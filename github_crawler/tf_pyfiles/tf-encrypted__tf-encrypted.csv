file_path,api_count,code
setup.py,0,"b'""""""Installing with setuptools.""""""\nimport setuptools\n\nwith open(""README.md"", ""r"", encoding=""utf8"") as fh:\n  long_description = fh.read()\n\nsetuptools.setup(\n    name=""tf-encrypted"",\n    version=""0.5.9"",\n    packages=setuptools.find_packages(),\n    package_data={\'tf_encrypted\': [\n        \'operations/secure_random/*.so\',\n        \'convert/*.yaml\',\n    ]},\n    python_requires="">=3.6"",\n    install_requires=[\n        ""tensorflow >=1.12.0, <2"",\n        ""numpy >=1.14.0"",\n        ""pyyaml >=5.1"",\n        ""tf-big ~=0.1.0"",\n    ],\n    extras_require={\n        ""tf"": [""tensorflow>=1.12.0,<2""],\n    },\n    license=""Apache License 2.0"",\n    url=""https://github.com/tf-encrypted/tf-encrypted"",\n    description=""A Framework for Machine Learning on Encrypted Data."",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    author=""The TF Encrypted Authors"",\n    author_email=""contact@tf-encrypted.io"",\n    classifiers=[\n        ""Programming Language :: Python :: 3"",\n        ""License :: OSI Approved :: Apache Software License"",\n        ""Development Status :: 2 - Pre-Alpha"",\n        ""Operating System :: OS Independent"",\n        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",\n        ""Topic :: Security :: Cryptography"",\n    ]\n)\n'"
bin/data_preprocessing.py,6,"b'""""""Extracting tf.data processing pipelines from full model GraphDefs.""""""\nimport os\n\nimport tensorflow as tf\n# tf.graph_util.extract_sub_graph will be removed in future tf version\ntry:\n  from tensorflow.compat.v1.graph_util import extract_sub_graph\nexcept ImportError:\n  from tensorflow.graph_util import extract_sub_graph\n\n\ndef data_prep_from_saved_model(\n    graph_def,\n    data_filenames,\n    batch_size,\n    data_prep_start_node=""serialized_example:0"",\n    data_prep_end_node=""DatasetToSingleElement:0""\n):\n  """"""Main function to extract data processing pipelines.""""""\n\n  # Trim graph to keep only the nodes related to data pre-processing\n  data_prep_end_node_name = data_prep_end_node.split("":"")[0]\n  gdef_trimmed = extract_sub_graph(\n      graph_def,\n      dest_nodes=[data_prep_end_node_name],\n  )\n\n  # Load TFRecord files then generate a Dataset of batch\n  dataset = tf.data.TFRecordDataset(data_filenames)\n  dataset = dataset.batch(batch_size)\n  iterator = dataset.make_one_shot_iterator()\n  dataset_b = iterator.get_next()\n\n  # Preprocess data\n  data_out, = tf.import_graph_def(\n      gdef_trimmed,\n      input_map={data_prep_start_node: dataset_b},\n      return_elements=[data_prep_end_node],\n  )\n\n  # TFE expects tensors with fully defined shape\n  fixed_shape = [batch_size] + data_out.get_shape().as_list()[1:]\n  data_out = tf.reshape(data_out, fixed_shape)\n  return data_out\n\n\ndef list_files_from_dir(directory):\n  file_names_list = tf.io.gfile.listdir(directory)\n  path_files_list = [os.path.join(directory, f) for f in file_names_list]\n  return path_files_list\n'"
operations/__init__.py,0,b''
primitives/setup.py,0,"b'""""""Installing with setuptools.""""""\nimport setuptools\n\nsetuptools.setup(\n    name=""tf-encrypted-primitives"",\n    version=""0.0.1"",\n    packages=setuptools.find_namespace_packages(include=[""tf_encrypted.*""]),\n    package_data={"""": [""*.so""]},\n    python_requires="">=3.6"",\n    install_requires=[""tensorflow ==2.1.0"", ""numpy >=1.14.0"", ""tf-big ~=0.2.0""],\n    license=""Apache License 2.0"",\n    url=""https://github.com/tf-encrypted/tf-encrypted"",\n    description=""A Framework for Machine Learning on Encrypted Data."",\n    long_description="""",\n    long_description_content_type=""text/markdown"",\n    author=""The TF Encrypted Authors"",\n    author_email=""contact@tf-encrypted.io"",\n    classifiers=[\n        ""Programming Language :: Python :: 3"",\n        ""License :: OSI Approved :: Apache Software License"",\n        ""Development Status :: 2 - Pre-Alpha"",\n        ""Operating System :: OS Independent"",\n        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",\n        ""Topic :: Security :: Cryptography"",\n    ],\n)\n'"
tf_encrypted/__init__.py,2,"b'""""""TF Encrypted namespace.""""""\nfrom __future__ import absolute_import\n\nimport inspect\nfrom typing import Any\nfrom typing import Optional\n\nimport tensorflow as tf\n\nfrom . import convert\nfrom . import keras\nfrom . import layers\nfrom . import operations\nfrom . import protocol\nfrom . import queue\nfrom . import serving\nfrom .config import Config\nfrom .config import LocalConfig\nfrom .config import RemoteConfig\nfrom .config import get_config\nfrom .player import player\nfrom .protocol import Pond\nfrom .session import Session\nfrom .session import set_log_directory\nfrom .session import set_tfe_events_flag\nfrom .session import set_tfe_trace_flag\n\n__protocol__ = None\n__all_prot_funcs__ = protocol.get_all_funcs()\n\n\ndef _prot_func_not_implemented(*args: Any, **kwargs: Any) -> None:\n    msg = ""This function is not implemented in protocol {}""\n    raise Exception(msg.format(inspect.stack()[1][3]))\n\n\ndef _update_protocol(prot):\n    """"""Update current protocol in scope.""""""\n    global __protocol__\n    __protocol__ = prot\n\n\ndef get_protocol():\n    """"""Return the current protocol in scope.\n\n    Note this should not be used for accessing public protocol methods, use\n    tfe.<public_protocol_method> instead.\n    """"""\n    return __protocol__\n\n\ndef set_protocol(prot: Optional[protocol.Protocol] = None) -> None:\n    """"""Sets the global protocol.\n\n    See :class:`~tf_encrypted.protocol.protocol.Protocol` for more info.\n\n    :param ~tf_encrypted.protocol.protocol.Protocol prot: A protocol instance.\n    """"""\n\n    # reset all names\n    for func_name in __all_prot_funcs__:\n        globals()[func_name] = _prot_func_not_implemented\n\n    # add global names according to new protocol\n    if prot is not None:\n        methods = inspect.getmembers(prot, predicate=inspect.ismethod)\n        public_methods = [method for method in methods if not method[0].startswith(""_"")]\n        for name, func in public_methods:\n            globals()[name] = func\n\n    # record new protocol\n    _update_protocol(prot)\n\n\ndef set_config(config: Config) -> None:\n    # pylint: disable=import-outside-toplevel\n    from .config import set_config as set_global_config\n\n    # pylint: enable=import-outside-toplevel\n\n    set_global_config(config)\n    set_protocol(None)\n\n\ndef global_variables_initializer() -> tf.Operation:\n    return tf.global_variables_initializer()\n\n\nset_protocol(Pond())\n\n__all__ = [\n    ""LocalConfig"",\n    ""RemoteConfig"",\n    ""set_tfe_events_flag"",\n    ""set_tfe_trace_flag"",\n    ""set_log_directory"",\n    ""get_config"",\n    ""set_config"",\n    ""set_protocol"",\n    ""Session"",\n    ""player"",\n    ""primitives"",\n    ""protocol"",\n    ""layers"",\n    ""convert"",\n    ""operations"",\n    ""global_variables_initializer"",\n    ""keras"",\n    ""queue"",\n    ""serving"",\n]\n'"
tf_encrypted/config.py,14,"b'""""""The TF Encrypted Config abstraction and its implementations.""""""\nimport json\nimport logging\nimport math\nfrom abc import ABC\nfrom abc import abstractmethod\nfrom collections import OrderedDict\nfrom pathlib import Path\n\nimport tensorflow as tf\nfrom tensorflow.core.protobuf import rewriter_config_pb2\n\nfrom .player import Player\n\nlogger = logging.getLogger(""tf_encrypted"")\n\n\ndef tensorflow_supports_int64():\n    """"""Test if int64 is supported by this build of TensorFlow. Hacky.""""""\n    with tf.Graph().as_default():\n        x = tf.constant([1], shape=(1, 1), dtype=tf.int64)\n        try:\n            tf.matmul(x, x)\n        except TypeError:\n            return False\n        return True\n\n\ndef _get_docker_cpu_quota():\n    """"""Checks for available cpu cores in a containerized environment.\n\n    If you witness memory leaks while doing multiple predictions using docker\n    see https://github.com/tensorflow/tensorflow/issues/22098.\n    """"""\n    cpu_cores = None\n\n    # Check for quotas if we are in a linux container\n    cfs_period = Path(""/sys/fs/cgroup/cpu/cpu.cfs_period_us"")\n    cfs_quota = Path(""/sys/fs/cgroup/cpu/cpu.cfs_quota_us"")\n\n    if cfs_period.exists() and cfs_quota.exists():\n        with cfs_period.open(""rb"") as p, cfs_quota.open(""rb"") as q:\n            p_int, q_int = int(p.read()), int(q.read())\n\n            # get the cores allocated by dividing the quota\n            # in microseconds by the period in microseconds\n            if q_int > 0 and p_int > 0:\n                cpu_cores = math.ceil(q_int / p_int)\n\n    return cpu_cores\n\n\nclass Config(ABC):\n    """"""The main tfe.Config abstraction.""""""\n\n    @property\n    @abstractmethod\n    def players(self):\n        """"""Returns the config\'s list of :class:`Player` objects.""""""\n\n    @abstractmethod\n    def get_player(self, name_or_player):\n        """"""Retrieve a specific :class:`Player` object by name.\n\n        For convenience it is also possible to pass in an existing Player object,\n        which will simply be returned as-is if the player is known already.\n        """"""\n\n    @abstractmethod\n    def get_tf_config(\n        self, log_device_placement=False, disable_optimizations=False,\n    ):\n        """"""Extract the underlying :class:`tf.ConfigProto`.""""""\n\n    @classmethod\n    def build_graph_options(cls, disable_optimizations):\n        if not disable_optimizations:\n            return tf.GraphOptions()\n\n        return tf.GraphOptions(\n            optimizer_options=tf.OptimizerOptions(\n                opt_level=tf.OptimizerOptions.L0,\n                do_common_subexpression_elimination=False,\n                do_constant_folding=False,\n                do_function_inlining=False,\n            ),\n            rewrite_options=rewriter_config_pb2.RewriterConfig(\n                arithmetic_optimization=rewriter_config_pb2.RewriterConfig.OFF\n            ),\n        )\n\n\nclass LocalConfig(Config):\n    """"""Configure TF Encrypted to use threads on the local CPU.\n\n    Each thread instantiates a different Player to simulate secure computations\n    without requiring networking. Mostly intended for development/debugging use.\n\n    By default new players will be added when looked up for the first time;\n    this is useful for  instance to get a complete list of players involved\n    in a particular computation (see `auto_add_unknown_players`).\n\n    :param (str) player_names: List of players to be used in the session.\n    :param str job_name: The name of the job.\n    :param bool auto_add_unknown_players: Auto-add player on first lookup.\n    """"""\n\n    def __init__(\n        self, player_names=None, job_name=""localhost"", auto_add_unknown_players=True,\n    ) -> None:\n        self._job_name = job_name\n        self._auto_add_unknown_players = auto_add_unknown_players\n        self._players = []\n        if player_names is None:\n            player_names = []\n        for name in player_names:\n            self.add_player(name)\n\n    def add_player(self, name):\n        index = len(self._players)\n        dv_str = ""/job:{job_name}/replica:0/task:0/device:CPU:{cpu_id}""\n        player = Player(\n            name=name,\n            index=index,\n            device_name=dv_str.format(job_name=self._job_name, cpu_id=index),\n        )\n        self._players.append(player)\n        return player\n\n    @property\n    def players(self):\n        return self._players\n\n    def get_player(self, name_or_player):\n        if isinstance(name_or_player, Player):\n            # we\'re passed a player\n            assert name_or_player in self._players\n            return name_or_player\n\n        # we\'re passed a name\n        player = next(\n            (player for player in self._players if player.name == name_or_player), None\n        )\n        if player is None and self._auto_add_unknown_players:\n            player = self.add_player(name_or_player)\n        return player\n\n    def get_players(self, names):\n        if isinstance(names, str):\n            names = [name.strip() for name in names.split("","")]\n        assert isinstance(names, list)\n        return [player for player in self._players if player.name in names]\n\n    def get_tf_config(\n        self, log_device_placement=False, disable_optimizations=False,\n    ):\n        logger.info(""Players: %s"", [player.name for player in self.players])\n        target = """"\n        config = tf.ConfigProto(\n            log_device_placement=log_device_placement,\n            allow_soft_placement=False,\n            device_count={""CPU"": len(self._players)},\n            graph_options=self.build_graph_options(disable_optimizations),\n        )\n        return (target, config)\n\n\nclass RemoteConfig(Config):\n    """"""Configure TF Encrypted to use network hosts for the different players.\n\n    :param (str,str),str->str hostmap: A mapping of hostnames to\n        their IP / domain.\n    :param str job_name: The name of the job.\n    """"""\n\n    def __init__(\n        self, hostmap, job_name=""tfe"",\n    ):\n        assert isinstance(hostmap, dict)\n        if not isinstance(hostmap, OrderedDict):\n            logger.warning(\n                ""Consider passing an ordered dictionary to RemoteConfig instead""\n                ""in order to preserve host mapping.""\n            )\n\n        self._job_name = job_name\n        self._players = OrderedDict(\n            (\n                name,\n                Player(\n                    name=name,\n                    index=index,\n                    device_name=""/job:{job_name}/replica:0/task:{task_id}/cpu:0"".format(\n                        job_name=job_name, task_id=index\n                    ),\n                    host=host,\n                ),\n            )\n            for index, (name, host) in enumerate(hostmap.items())\n        )\n\n    @staticmethod\n    def load(filename):\n        """"""Constructs a RemoteConfig object from a JSON hostmap file.\n\n        :param str filename: Name of file to load from.\n        """"""\n        with open(filename, ""r"") as f:\n            hostmap = json.load(f, object_pairs_hook=OrderedDict)\n        return RemoteConfig(hostmap)\n\n    def save(self, filename):\n        """"""Saves the configuration as a JSON hostmap file.\n\n        :param str filename: Name of file to save to.\n        """"""\n        with open(filename, ""w"") as f:\n            json.dump(self.hostmap, f)\n\n    @property\n    def hostmap(self):\n        return OrderedDict(\n            (player.name, player.host) for player in self._players.values()\n        )\n\n    @property\n    def hosts(self):\n        return [player.host for player in self._players.values()]\n\n    @property\n    def players(self):\n        return list(self._players.values())\n\n    def get_player(self, name_or_player):\n        if isinstance(name_or_player, Player):\n            # we\'re passed a player\n            assert name_or_player in self._players.values()\n            return name_or_player\n\n        # we\'re passed a name\n        return self._players.get(name_or_player)\n\n    def get_players(self, names):\n        if isinstance(names, str):\n            names = [name.strip() for name in names.split("","")]\n        assert isinstance(names, list)\n        return [player for name, player in self._players.items() if name in names]\n\n    def server(self, name, start=True):\n        """"""Construct a :class:`tf.train.Server` object for the corresponding\n        :class:`Player`.\n\n        :param str name: Name of player.\n        """"""\n        player = self.get_player(name)\n        assert player is not None, ""\'{}\' not found in configuration"".format(name)\n        cluster = tf.train.ClusterSpec({self._job_name: self.hosts})\n        logger.debug(""Creating server for \'%s\' using %s"", name, cluster)\n        server = tf.train.Server(\n            cluster, job_name=self._job_name, task_index=player.index, start=start\n        )\n        logger.info(\n            ""Created server for \'%s\' as device \'%s\'; own session target is \'%s\'"",\n            name,\n            player.device_name,\n            server.target,\n        )\n        return server\n\n    def get_tf_config(self, log_device_placement=False, disable_optimizations=False):\n        # always use the first host as master; change config to match\n        target = ""grpc://{}"".format(self.hosts[0])\n        cpu_cores = _get_docker_cpu_quota()\n        if cpu_cores is None:\n            config = tf.ConfigProto(\n                log_device_placement=log_device_placement,\n                allow_soft_placement=False,\n                graph_options=self.build_graph_options(disable_optimizations),\n            )\n        else:\n            config = tf.ConfigProto(\n                log_device_placement=log_device_placement,\n                allow_soft_placement=False,\n                inter_op_parallelism_threads=cpu_cores,\n                intra_op_parallelism_threads=cpu_cores,\n                graph_options=self.build_graph_options(disable_optimizations),\n            )\n        return (target, config)\n\n\n__config__ = LocalConfig()\n\n\ndef get_config():\n    """"""Returns the current config.""""""\n    return __config__\n\n\ndef set_config(config) -> None:\n    """"""Sets the current config.\n\n    :param Config config: Intended configuration.\n    """"""\n    global __config__\n    __config__ = config\n'"
tf_encrypted/private_model.py,6,"b'""""""An abstraction for private models.""""""\nimport os\nimport tempfile\n\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.python.framework import graph_util\nfrom tensorflow.python.framework.graph_util_impl import remove_training_nodes\nfrom tensorflow.python.platform import gfile\n\nimport tf_encrypted as tfe\n\n_TMPDIR = tempfile.gettempdir()\n\n\nclass PrivateModel:\n    """"""An implementation of private models.""""""\n\n    def __init__(self, output_node):\n        self.output_node = output_node\n\n    # TODO support multiple inputs\n    def private_predict(self, x, input_name=None, tag=""prediction""):\n        """"""Perform a private prediction.""""""\n        if input_name is None:\n            name = ""private-input/api/0:0""\n        else:\n            name = input_name\n\n        pl = tf.get_default_graph().get_tensor_by_name(name)\n\n        with tfe.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            if isinstance(self.output_node, list):\n                op = [n.reveal() for n in self.output_node]\n            else:\n                op = self.output_node.reveal()\n\n            output = sess.run(op, feed_dict={pl: x}, tag=tag)\n\n            return output\n\n\ndef load_graph(model_file, model_name=None, batch_size=1):\n    """"""Load a plaintext model from protobuf.""""""\n\n    input_spec = []\n    with gfile.GFile(model_file, ""rb"") as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n\n        for node in graph_def.node:\n            if node.op != ""Placeholder"":\n                continue\n\n            input_spec.append(\n                {\n                    ""name"": node.name,\n                    ""dtype"": node.attr[""dtype""].type,\n                    ""shape"": [batch_size]\n                    + [int(d.size) for d in node.attr[""shape""].shape.dim[1:]],\n                }\n            )\n\n    inputs = []\n    for i, spec in enumerate(input_spec):\n\n        def scope(i, spec):\n            def provide_input() -> tf.Tensor:\n                if model_name is None:\n                    name = ""api/{}"".format(i)\n                else:\n                    name = ""api/{}/{}"".format(model_name, i)\n\n                pl = tf.placeholder(tf.float32, shape=spec[""shape""], name=name)\n                return pl\n\n            return provide_input\n\n        inputs.append(scope(i, spec))\n\n    return graph_def, inputs\n\n\ndef secure_model(model, **kwargs):\n    """"""Secure a plaintext model from the current session.""""""\n    session = K.get_session()\n    min_graph = graph_util.convert_variables_to_constants(\n        session, session.graph_def, [node.op.name for node in model.outputs]\n    )\n    graph_fname = ""model.pb""\n    tf.train.write_graph(min_graph, _TMPDIR, graph_fname, as_text=False)\n\n    if ""batch_size"" in kwargs:\n        batch_size = kwargs.pop(""batch_size"")\n    else:\n        batch_size = 1\n\n    graph_def, inputs = load_graph(\n        os.path.join(_TMPDIR, graph_fname), batch_size=batch_size\n    )\n\n    c = tfe.convert.convert.Converter(tfe.convert.registry(), **kwargs)\n    y = c.convert(remove_training_nodes(graph_def), ""input-provider"", inputs)\n\n    return PrivateModel(y)\n'"
tf_encrypted/private_model_test.py,7,"b'# pylint: disable=missing-docstring\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.convert.convert_test import export_matmul\nfrom tf_encrypted.convert.convert_test import read_graph\nfrom tf_encrypted.private_model import PrivateModel\nfrom tf_encrypted.private_model import secure_model\n\n\nclass TestPrivateModel(unittest.TestCase):\n    def test_private_model(self):\n        def provide_input():\n            return tf.placeholder(dtype=tf.float32, shape=[1, 2], name=""api/0"")\n\n        export_matmul(""matmul.pb"", [1, 2])\n\n        graph_def = read_graph(""matmul.pb"")\n\n        with tfe.protocol.Pond():\n            c = tfe.convert.convert.Converter(tfe.convert.registry())\n            y = c.convert(graph_def, ""input-provider"", provide_input)\n\n            model = PrivateModel(y)\n\n            output = model.private_predict(np.ones([1, 2]))\n\n        np.testing.assert_array_equal(output, [[2.0]])\n\n\nclass TestSecureModel(unittest.TestCase):\n    def setUp(self):\n        K.clear_session()\n\n    def tearDown(self):\n        K.clear_session()\n\n    def test_secure_model(self):\n        with tfe.protocol.Pond():\n            tf.random.set_random_seed(42)\n\n            d = tf.keras.layers.Dense(1, input_shape=(10,), use_bias=False)\n            model = tf.keras.Sequential([d])\n\n            x = np.ones((1, 10))\n            y = model.predict(x)\n\n            s_model = secure_model(model)\n            s_y = s_model.private_predict(x)\n\n            np.testing.assert_array_almost_equal(s_y, y, 3)\n\n    def test_secure_model_batch(self):\n        with tfe.protocol.Pond():\n            tf.random.set_random_seed(42)\n\n            d = tf.keras.layers.Dense(1, input_shape=(10,), use_bias=False)\n            model = tf.keras.Sequential([d])\n\n            x = np.ones((2, 10))\n            y = model.predict(x)\n\n            s_model = secure_model(model, batch_size=2)\n            s_y = s_model.private_predict(x)\n\n            np.testing.assert_array_almost_equal(s_y, y, 3)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/session.py,12,"b'""""""TF Encrypted extension of tf.Session.""""""\nimport logging\nimport os\nfrom collections import defaultdict\n\nimport tensorflow as tf\nfrom tensorflow.python.client import timeline\n\nfrom .config import RemoteConfig\nfrom .config import get_config\nfrom .utils import unwrap_fetches\n\n# pylint: disable=invalid-name\n__TFE_EVENTS__ = bool(os.getenv(""TFE_EVENTS"", """"))\n__TFE_TRACE__ = bool(os.getenv(""TFE_TRACE"", """"))\n__TENSORBOARD_DIR__ = str(os.getenv(""TFE_EVENTS_DIR"", ""/tmp/tensorboard""))\n# pylint: enable=invalid-name\n\n_run_counter = defaultdict(int)\n\nlogger = logging.getLogger(""tf_encrypted"")\n\n\nclass Session(tf.compat.v1.Session):\n    """"""\n  Wrap a Tensorflow Session.\n\n  See the documentation of\n  `tf.Session <https://www.tensorflow.org/api_docs/python/tf/Session>`_\n  for more details.\n\n  :param Optional[tf.Graph] graph: A :class:`tf.Graph`.  Used similarly.\n    This is the graph to be launched.  If nothing is specified, then the\n    default graph in the session will be used.\n  :param Optional[~tensorflow_encrypted.config.Config] config:  A\n    :class:`Local <tf_encrypted.config.LocalConfig/>` or\n    :class:`Remote <tf_encrypted.config.RemoteConfig>` config to be supplied\n    when executing the graph.\n  """"""\n\n    def __init__(self, graph=None, config=None, target=None, **tf_config_kwargs):\n        if config is None:\n            config = get_config()\n\n        default_target, config_proto = config.get_tf_config(**tf_config_kwargs)\n        if target is None:\n            target = default_target\n        self.target = target\n        self.config_proto = config_proto\n\n        if isinstance(config, RemoteConfig):\n            logger.info(\n                ""Starting session on target \'%s\' using config %s"",\n                self.target,\n                self.config_proto,\n            )\n        super(Session, self).__init__(self.target, graph, self.config_proto)\n\n    def run(\n        self,\n        fetches,\n        feed_dict=None,\n        tag=None,\n        write_trace=False,\n        output_partition_graphs=False,\n    ):\n        # pylint: disable=arguments-differ\n        """"""\n    run(fetches, feed_dict, tag, write_trace) -> Any\n\n    See the documentation for `tf.Session.run` for more details.\n\n    This method functions just as the one from tensorflow.\n\n    The value returned by run() has the same shape as the fetches argument,\n    where the leaves are replaced by the corresponding values returned by\n    TensorFlow.\n\n    :param Any fetches: A single graph element, a list of graph elements, or a\n      dictionary whose values are graph elements or lists of graph elements\n      (described in tf.Session.run docs).\n    :param str->np.ndarray feed_dict: A dictionary that maps graph elements to\n      values (described in tf.Session.run docs).\n    :param str tag: An optional namespace to run the session under.\n    :param bool write_trace: If true, the session logs will be dumped for use\n      in Tensorboard.\n    """"""\n\n        sanitized_fetches = unwrap_fetches(fetches)\n\n        if not __TFE_EVENTS__ or tag is None:\n            fetches_out = super(Session, self).run(\n                sanitized_fetches, feed_dict=feed_dict,\n            )\n        else:\n            session_tag = ""{}{}"".format(tag, _run_counter[tag])\n            run_tag = os.path.join(__TENSORBOARD_DIR__, session_tag)\n            _run_counter[tag] += 1\n\n            writer = tf.summary.FileWriter(run_tag, self.graph)\n            run_options = tf.RunOptions(\n                trace_level=tf.RunOptions.FULL_TRACE,\n                output_partition_graphs=output_partition_graphs,\n            )\n            run_metadata = tf.RunMetadata()\n\n            fetches_out = super(Session, self).run(\n                sanitized_fetches,\n                feed_dict=feed_dict,\n                options=run_options,\n                run_metadata=run_metadata,\n            )\n\n            if output_partition_graphs:\n                for i, g in enumerate(run_metadata.partition_graphs):\n                    tf.io.write_graph(\n                        g,\n                        logdir=os.path.join(__TENSORBOARD_DIR__, session_tag),\n                        name=""partition{}.pbtxt"".format(i),\n                    )\n\n            writer.add_run_metadata(run_metadata, session_tag)\n            writer.close()\n\n            if __TFE_TRACE__ or write_trace:\n                tracer = timeline.Timeline(run_metadata.step_stats)\n                chrome_trace = tracer.generate_chrome_trace_format()\n                trace_fname = ""{}/{}.ctr"".format(__TENSORBOARD_DIR__, session_tag)\n                with open(trace_fname, ""w"") as f:\n                    f.write(chrome_trace)\n\n        return fetches_out\n\n\ndef set_tfe_events_flag(monitor_events: bool = False) -> None:\n    """"""\n  set_tfe_events_flag(monitor_events)\n\n  Set flag to enable or disable monitoring of runtime statistics for each call\n  to session.run().\n\n  :param bool monitor_events: Enable or disable stats, disabled by default.\n  """"""\n    global __TFE_EVENTS__  # pylint: disable=invalid-name\n    if monitor_events is True:\n        logger.info(""Writing event files for each run with a tag"")\n\n    __TFE_EVENTS__ = monitor_events\n\n\ndef set_tfe_trace_flag(trace: bool = False) -> None:\n    """"""\n  set_tfe_trace_flag(trace)\n\n  Set flag to enable or disable tracing in TF Encrypted.\n\n  :param bool trace: Enable or disable tracing, disabled by default.\n  """"""\n    global __TFE_TRACE__  # pylint: disable=invalid-name\n    if trace is True:\n        logger.info(""Writing trace files"")\n\n    __TFE_TRACE__ = trace\n\n\ndef set_log_directory(path):\n    """"""\n  set_log_directory(path)\n\n  Sets the directory to write TensorBoard event and trace files to.\n\n  :param str path: The TensorBoard logdir.\n  """"""\n    global __TENSORBOARD_DIR__  # pylint: disable=invalid-name\n    if path:\n        logger.info(""Writing event and trace files to \'%s\'"", path)\n\n    __TENSORBOARD_DIR__ = path\n'"
tf_encrypted/utils.py,6,"b'""""""TF Encrypted utilities.""""""\nimport tensorflow as tf\n\n\ndef wrap_in_variables(*tensors):\n    """"""Wrap a list of tensors in Variables""""""\n    variables = [\n        tensor.factory.variable(\n            tf.zeros(shape=tensor.shape, dtype=tensor.factory.native_type)\n        )\n        for tensor in tensors\n    ]\n    group_updater = tf.group(\n        *[var.assign_from_same(tensor) for var, tensor in zip(variables, tensors)]\n    )\n    return group_updater, variables\n\n\ndef flatten(xs):\n    """"""\n    Flatten any recursive list or tuple into a single list.\n\n    For instance:\n    - `flatten(x) => [x]`\n    - `flatten([x]) => [x]`\n    - `flatten([x, [y], [[z]]]) => `[x, y, z]`\n    """"""\n    if isinstance(xs, (list, tuple)):\n        return [y for ys in [flatten(x) for x in xs] for y in ys]\n    return [xs]\n\n\ndef reachable_nodes(*nodes):\n    """"""\n    Find all nodes reachable from `nodes` in the implicit tf.Graph\n    to which they belong.\n\n    Both tensors and their underlying operation is returned.\n    """"""\n\n    nodes = flatten(nodes)\n    reachable = set(nodes)\n    queue = list(nodes)\n\n    while queue:\n        node = queue.pop(0)\n\n        if isinstance(node, tf.Tensor):\n            subnode = node.op\n            if subnode not in reachable:\n                reachable.add(subnode)\n                queue.append(subnode)\n            continue\n\n        if isinstance(node, tf.Operation):\n            for subnode in list(node.inputs) + list(node.control_inputs):\n                if subnode not in reachable:\n                    reachable.add(subnode)\n                    queue.append(subnode)\n            continue\n\n        raise TypeError(\n            ""Don\'t know how to process {} of type {}"".format(node, type(node))\n        )\n\n    return reachable\n\n\ndef unwrap_fetches(fetches):\n    """"""\n    Unwraps TF Encrypted fetches into TensorFlow-compatible fetches.\n    """"""\n\n    if isinstance(fetches, (list, tuple)):\n        return [unwrap_fetches(fetch) for fetch in fetches]\n    if isinstance(fetches, (tf.Tensor, tf.Operation)):\n        return fetches\n    return fetches.to_native()\n'"
docs/source/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(\'../..\'))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'tf-encrypted\'\ncopyright = \'2019, The TF Encrypted Authors\'\nauthor = \'The TF Encrypted Authors\'\n\n# The short X.Y version\nversion = \'0.5.9\'\n# The full version, including alpha/beta/rc tags\nrelease = \'0.5.9\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.githubpages\',\n    \'sphinx.ext.napoleon\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\nsource_parsers = {\n    \'.md\': \'recommonmark.parser.CommonMarkParser\',\n}\n\nsource_suffix = [\'.rst\', \'.md\']\n# source_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = None\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = ""sphinx_rtd_theme""\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'tf-encrypteddoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'tf-encrypted.tex\', \'TF Encrypted Documentation\',\n     author, \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'tf-encrypted\', \'TF Encrypted Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'tf-encrypted\', \'TF Encrypted Documentation\',\n     author, \'tf-encrypted\', \'Private ML playground.\',\n     \'Miscellaneous\'),\n]\n\n\n# -- Options for Epub output -------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [\'search.html\']\n\n\n# -- Extension configuration -------------------------------------------------\n\n# -- Options for intersphinx extension ---------------------------------------\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\n    \'https://docs.python.org/\': None,\n    \'https://www.tensorflow.org/api_docs/python/\': \'tensorflow\'\n}\n'"
examples/bloom/__init__.py,0,b''
examples/bloom/__main__.py,2,"b'""""""Main entrypoint for running Bloom regression example.""""""\nimport logging\n\nimport tensorflow as tf\n\nfrom regressor import BloomRegressor\nfrom regressor import DataOwner\n\nNUM_FEATURES = 100\nTRAINING_SET_SIZE = 10000\nTEST_SET_SIZE = 1000\n\ngenebanks = [\n    DataOwner(""genebank-0"", NUM_FEATURES, TRAINING_SET_SIZE, TEST_SET_SIZE),\n    DataOwner(""genebank-1"", NUM_FEATURES, TRAINING_SET_SIZE, TEST_SET_SIZE),\n    DataOwner(""genebank-3"", NUM_FEATURES, TRAINING_SET_SIZE, TEST_SET_SIZE),\n    DataOwner(""genebank-4"", NUM_FEATURES, TRAINING_SET_SIZE, TEST_SET_SIZE),\n    DataOwner(""genebank-5"", NUM_FEATURES, TRAINING_SET_SIZE, TEST_SET_SIZE),\n    DataOwner(""genebank-6"", NUM_FEATURES, TRAINING_SET_SIZE, TEST_SET_SIZE),\n]\n\nlogging.basicConfig(level=logging.DEBUG)\n\nmodel = BloomRegressor()\nmodel.fit(genebanks)\n\n# report results of training\nwith tf.Graph().as_default() as g:\n    output = model.coefficients\n    print(tf.Session(graph=g).run(output))\n'"
examples/bloom/regressor.py,17,"b'""""""BloomRegressor implementation and DataOwner helper.""""""\nimport numpy as np\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\n\n\nclass BloomRegressor:\n    """"""Secure multi-party linear regression at plaintext speed.\n\n  Computes the necessary components of the normal equations solution to a\n  linear regression.""""""\n\n    def __init__(self):\n        self.components = [\n            ""label_square"",\n            ""covariate_label_product"",\n            ""covariate_square"",\n        ]\n\n    @classmethod\n    def estimator_fn(cls, x_p, y_p):\n        # Recall beta = np.inv(X.T @ X) * (X.T @ y)\n        yy_p = tf.matmul(y_p, y_p, transpose_a=True)  # per-party y.T @ y\n        xy_p = tf.matmul(x_p, y_p, transpose_a=True)  # per-party X.T @ y\n        xx_p = tf.matmul(x_p, x_p, transpose_a=True)  # per-party X.T @ X\n        return yy_p, xy_p, xx_p\n\n    def fit(self, training_players, summary=0, validation_split=None):\n        """"""Trains the linear regressor.\n\n    Arguments:\n      training_players: Data owners used for joint training. Must implement the\n          compute_estimators as a tfe.local_computation.\n      summary: Controls what kind of summary statistics are generated after the\n          linear regression fit.\n      validation_split: Mimics the behavior of the Keras validation_split kwarg.\n    """"""\n        if validation_split is not None:\n            raise NotImplementedError()\n\n        partial_estimators = [\n            player.compute_estimators(self.estimator_fn) for player in training_players\n        ]\n\n        for attr, partial_estimator in zip(self.components, zip(*partial_estimators)):\n            setattr(self, attr, tfe.add_n(partial_estimator))\n\n        with tfe.Session() as sess:\n            for k in self.components:\n                op = getattr(self, k)\n                setattr(self, k, sess.run(op.reveal()))\n\n        tf_graph = tf.Graph()\n        with tf_graph.as_default():\n            self._inverted_covariate_square = tf.linalg.inv(self.covariate_square)\n            self.coefficients = tf.matmul(\n                self._inverted_covariate_square, self.covariate_label_product\n            )\n\n        with tf.Session(graph=tf_graph) as sess:\n            for k in [""_inverted_covariate_square"", ""coefficients""]:\n                setattr(self, k, sess.run(getattr(self, k)))\n\n        if not summary:\n            return self\n\n        return self.summarize(summary_level=summary)\n\n    def predict(self, x):\n        raise NotImplementedError()\n\n    def evaluate(self, testing_players):\n        raise NotImplementedError()\n\n    def __getattribute__(self, attr):\n        # We will only use numpy arrays for storage, so we can safely lift them\n        # into a tf.Tensor whenever they\'re requested.\n        obj = super().__getattribute__(attr)\n        if isinstance(obj, np.ndarray):\n            return tf.constant(obj)\n        return obj\n\n    def summarize(self, summary_level):\n        # TODO: coefficient variance\n        # TODO: stderror\n        # TODO: p-vals\n        raise NotImplementedError()\n\n\nclass DataOwner:\n    """"""Contains code meant to be executed by a data owner Player.""""""\n\n    def __init__(\n        self, player_name, num_features, training_set_size, test_set_size,\n    ):\n        self.player_name = player_name\n        self.num_features = num_features\n        self.training_set_size = training_set_size\n        self.test_set_size = test_set_size\n\n    def _build_training_data(self):\n        """"""Preprocess training dataset\n\n    Return single batch of training dataset\n    """"""\n\n        def cast(x, y):\n            return tf.cast(x, tf.float32), y\n\n        x_raw = tf.random.uniform(\n            minval=-0.5, maxval=0.5, shape=[self.training_set_size, self.num_features]\n        )\n        target_coeffs = tf.random.normal(\n            shape=[self.num_features, 1], mean=3, stddev=2.0\n        )\n        y_raw = tf.matmul(x_raw, target_coeffs)\n\n        return cast(x_raw, y_raw)\n\n    def _build_testing_data(self):\n        """"""Preprocess testing dataset\n\n    Return single batch of testing dataset\n    """"""\n\n        def cast(x, y):\n            return tf.cast(x, tf.float32), tf.expand_dims(y, 0)\n\n        x_raw = tf.random.uniform(\n            minval=-0.5, maxval=0.5, shape=[self.training_set_size, self.num_features]\n        )\n        target_coeffs = tf.random.normal(shape=(self.num_features), mean=3, stddev=2.0)\n        y_raw = tf.matmul(x_raw, target_coeffs)\n\n        return cast(x_raw, y_raw)\n\n    @tfe.local_computation\n    def compute_estimators(self, estimator_fn):\n        x, y = self._build_training_data()\n        return estimator_fn(x, y)\n'"
examples/deprecated/bench_conv2d_sigmoid.py,59,"b'import sys\n\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.layers import Conv2D\nfrom tf_encrypted.layers import Dense\nfrom tf_encrypted.layers import Reshape\nfrom tf_encrypted.layers import Sigmoid\n\nconfig = tfe.LocalConfig(\n    [""server0"", ""server1"", ""crypto-producer"", ""weights-provider"", ""prediction-client""]\n)\n\n# config = tfe.RemoteConfig([\n#     (\'server0\', \'localhost:4440\'),\n#     (\'server1\', \'localhost:4441\'),\n#     (\'crypto-producer\', \'localhost:4442\'),\n#     (\'weights-provider\', \'localhost:4443\'),\n#     (\'prediction-client\', \'localhost:4444\')\n# ])\n\n\nif len(sys.argv) > 1:\n    if isinstance(config, tfe.LocalConfig):\n        raise Exception(\n            ""You can launch a configured server only with a remote configuration""\n        )\n    #\n    # assume we\'re running as a server\n    #\n\n    player_name = str(sys.argv[1])\n\n    server = config.server(player_name)\n    server.start()\n    server.join()\nelse:\n\n    #\n    # assume we\'re running as master\n    #\n\n    input_shape = [1, 3, 192, 192]\n    conv11_fshape = [3, 3, 3, 64]\n    conv12_fshape = [3, 3, 64, 64]\n    pool1_shape = [1, 1, 64, 64]\n\n    conv21_fshape = [3, 3, 64, 128]\n    conv22_fshape = [3, 3, 128, 128]\n    pool2_shape = [1, 1, 128, 128]\n\n    conv31_fshape = [3, 3, 128, 256]\n    conv32_fshape = [3, 3, 256, 256]\n    conv33_fshape = [3, 3, 256, 256]\n    pool3_shape = [1, 1, 256, 256]\n\n    conv41_fshape = [3, 3, 256, 512]\n    conv42_fshape = [3, 3, 512, 512]\n    conv43_fshape = [3, 3, 512, 512]\n    pool4_shape = [1, 1, 512, 512]\n\n    conv51_fshape = [3, 3, 512, 512]\n    conv52_fshape = [3, 3, 512, 512]\n    conv53_fshape = [3, 3, 512, 512]\n    pool5_shape = [1, 1, 512, 512]\n\n    def provide_input_conv11weights() -> tf.Tensor:\n        w = tf.random_normal(shape=conv11_fshape, dtype=tf.float32)\n        tf.print(w, [w], message=""w11:"")\n        return w\n\n    def provide_input_conv12weights() -> tf.Tensor:\n        w = tf.random_normal(shape=conv12_fshape, dtype=tf.float32)\n        tf.print(w, [w], message=""w12:"")\n        return w\n\n    def provide_input_pool1weights() -> tf.Tensor:\n        w = tf.random_normal(shape=pool1_shape, dtype=tf.float32)\n        tf.print(w, [w], message=""p1:"")\n        return w\n\n    def provide_input_conv21weights() -> tf.Tensor:\n        w = tf.random_normal(shape=conv21_fshape, dtype=tf.float32)\n        tf.print(w, [w], message=""w21:"")\n        return w\n\n    def provide_input_conv22weights() -> tf.Tensor:\n        w = tf.random_normal(shape=conv22_fshape, dtype=tf.float32)\n        tf.print(w, [w], message=""w22:"")\n        return w\n\n    def provide_input_pool2weights() -> tf.Tensor:\n        w = tf.random_normal(shape=pool2_shape, dtype=tf.float32)\n        tf.print(w, [w], message=""p2:"")\n        return w\n\n    def provide_input_conv31weights() -> tf.Tensor:\n        w = tf.random_normal(shape=conv31_fshape, dtype=tf.float32)\n        tf.print(w, [w], message=""w31:"")\n        return w\n\n    def provide_input_conv32weights() -> tf.Tensor:\n        w = tf.random_normal(shape=conv32_fshape, dtype=tf.float32)\n        tf.print(w, [w], message=""w32:"")\n        return w\n\n    def provide_input_conv33weights() -> tf.Tensor:\n        w = tf.random_normal(shape=conv33_fshape, dtype=tf.float32)\n        tf.print(w, [w], message=""w33:"")\n        return w\n\n    def provide_input_pool3weights() -> tf.Tensor:\n        w = tf.random_normal(shape=pool3_shape, dtype=tf.float32)\n        tf.print(w, [w], message=""p3:"")\n        return w\n\n    def provide_input_conv41weights() -> tf.Tensor:\n        w = tf.random_normal(shape=conv41_fshape, dtype=tf.float32)\n        tf.print(w, [w], message=""w41:"")\n        return w\n\n    def provide_input_conv42weights() -> tf.Tensor:\n        w = tf.random_normal(shape=conv42_fshape, dtype=tf.float32)\n        tf.print(w, [w], message=""w42:"")\n        return w\n\n    def provide_input_conv43weights() -> tf.Tensor:\n        w = tf.random_normal(shape=conv43_fshape, dtype=tf.float32)\n        tf.print(w, [w], message=""w43:"")\n        return w\n\n    def provide_input_pool4weights() -> tf.Tensor:\n        w = tf.random_normal(shape=pool4_shape, dtype=tf.float32)\n        tf.print(w, [w], message=""p4:"")\n        return w\n\n    def provide_input_conv51weights() -> tf.Tensor:\n        w = tf.random_normal(shape=conv51_fshape, dtype=tf.float32)\n        tf.print(w, [w], message=""w51:"")\n        return w\n\n    def provide_input_conv52weights() -> tf.Tensor:\n        w = tf.random_normal(shape=conv52_fshape, dtype=tf.float32)\n        tf.print(w, [w], message=""w52:"")\n        return w\n\n    def provide_input_conv53weights() -> tf.Tensor:\n        w = tf.random_normal(shape=conv53_fshape, dtype=tf.float32)\n        tf.print(w, [w], message=""w53:"")\n        return w\n\n    def provide_input_pool5weights() -> tf.Tensor:\n        w = tf.random_normal(shape=pool5_shape, dtype=tf.float32)\n        tf.print(w, [w], message=""p5:"")\n        return w\n\n    def provide_input_prediction() -> tf.Tensor:\n        x = tf.random_normal(shape=input_shape, dtype=tf.float32)\n        tf.print(x, [x], message=""x:"")\n        return x\n\n    def receive_output(tensor: tf.Tensor) -> tf.Operation:\n        tf.print(tensor, [tensor, tf.shape(tensor)], message=""output:"")\n        return tensor\n\n    with tfe.protocol.Pond(\n        *config.get_players(""server0, server1, crypto-producer"")\n    ) as prot:\n\n        print(""Define the distributed graph"")\n        print(""5 blocks of convolutions and a 2-layer FC"")\n        # load input for prediction\n        x = prot.define_private_input(""prediction-client"", provide_input_prediction)\n\n        print(""Define Block 1"")\n        # Block 1\n        conv11 = Conv2D(input_shape, conv11_fshape, 1, ""SAME"")\n        initial_w_conv11 = prot.define_private_input(\n            ""weights-provider"", provide_input_conv11weights\n        )\n        conv11.initialize(initial_w_conv11)\n        x = conv11.forward(x)\n        x = Sigmoid(conv11.get_output_shape()).forward(x)\n        conv12 = Conv2D(conv11.get_output_shape(), conv12_fshape, 1, ""SAME"")\n        initial_w_conv12 = prot.define_private_input(\n            ""weights-provider"", provide_input_conv12weights\n        )\n        conv12.initialize(initial_w_conv12)\n        x = conv12.forward(x)\n        x = Sigmoid(conv12.get_output_shape()).forward(x)\n        fake_pool1 = Conv2D(conv12.get_output_shape(), pool1_shape, 2, ""SAME"")\n        initial_w_pool1 = prot.define_private_input(\n            ""weights-provider"", provide_input_pool1weights\n        )\n        fake_pool1.initialize(initial_w_pool1)\n        x = fake_pool1.forward(x)\n\n        print(""Define Block 2"")\n        # Block 2\n        conv21 = Conv2D(fake_pool1.get_output_shape(), conv21_fshape, 1, ""SAME"")\n        initial_w_conv21 = prot.define_private_input(\n            ""weights-provider"", provide_input_conv21weights\n        )\n        conv21.initialize(initial_w_conv21)\n        x = conv21.forward(x)\n        x = Sigmoid(conv21.get_output_shape()).forward(x)\n        conv22 = Conv2D(conv21.get_output_shape(), conv22_fshape, 1, ""SAME"")\n        initial_w_conv22 = prot.define_private_input(\n            ""weights-provider"", provide_input_conv22weights\n        )\n        conv22.initialize(initial_w_conv22)\n        x = conv22.forward(x)\n        x = Sigmoid(conv22.get_output_shape()).forward(x)\n        fake_pool2 = Conv2D(conv22.get_output_shape(), pool2_shape, 2, ""SAME"")\n        initial_w_pool2 = prot.define_private_input(\n            ""weights-provider"", provide_input_pool2weights\n        )\n        fake_pool2.initialize(initial_w_pool2)\n        x = fake_pool2.forward(x)\n\n        print(""Define Block 3"")\n        # Block 3\n        conv31 = Conv2D(fake_pool2.get_output_shape(), conv31_fshape, 1, ""SAME"")\n        initial_w_conv31 = prot.define_private_input(\n            ""weights-provider"", provide_input_conv31weights\n        )\n        conv31.initialize(initial_w_conv31)\n        x = conv31.forward(x)\n        x = Sigmoid(conv31.get_output_shape()).forward(x)\n        conv32 = Conv2D(conv31.get_output_shape(), conv32_fshape, 1, ""SAME"")\n        initial_w_conv32 = prot.define_private_input(\n            ""weights-provider"", provide_input_conv32weights\n        )\n        conv32.initialize(initial_w_conv32)\n        x = conv32.forward(x)\n        x = Sigmoid(conv32.get_output_shape()).forward(x)\n        conv33 = Conv2D(conv32.get_output_shape(), conv33_fshape, 1, ""SAME"")\n        initial_w_conv33 = prot.define_private_input(\n            ""weights-provider"", provide_input_conv33weights\n        )\n        conv33.initialize(initial_w_conv33)\n        x = conv33.forward(x)\n        x = Sigmoid(conv33.get_output_shape()).forward(x)\n        fake_pool3 = Conv2D(conv33.get_output_shape(), pool3_shape, 2, ""SAME"")\n        initial_w_pool3 = prot.define_private_input(\n            ""weights-provider"", provide_input_pool3weights\n        )\n        fake_pool3.initialize(initial_w_pool3)\n        x = fake_pool3.forward(x)\n\n        print(""Define Block 4"")\n        # Block 4\n        conv41 = Conv2D(fake_pool3.get_output_shape(), conv41_fshape, 1, ""SAME"")\n        initial_w_conv41 = prot.define_private_input(\n            ""weights-provider"", provide_input_conv41weights\n        )\n        conv41.initialize(initial_w_conv41)\n        x = conv41.forward(x)\n        x = Sigmoid(conv41.get_output_shape()).forward(x)\n        conv42 = Conv2D(conv41.get_output_shape(), conv42_fshape, 1, ""SAME"")\n        initial_w_conv42 = prot.define_private_input(\n            ""weights-provider"", provide_input_conv42weights\n        )\n        conv42.initialize(initial_w_conv42)\n        x = conv42.forward(x)\n        x = Sigmoid(conv42.get_output_shape()).forward(x)\n        conv43 = Conv2D(conv42.get_output_shape(), conv43_fshape, 1, ""SAME"")\n        initial_w_conv43 = prot.define_private_input(\n            ""weights-provider"", provide_input_conv43weights\n        )\n        conv43.initialize(initial_w_conv43)\n        x = conv43.forward(x)\n        x = Sigmoid(conv43.get_output_shape()).forward(x)\n        fake_pool4 = Conv2D(conv43.get_output_shape(), pool4_shape, 2, ""SAME"")\n        initial_w_pool4 = prot.define_private_input(\n            ""weights-provider"", provide_input_pool4weights\n        )\n        fake_pool4.initialize(initial_w_pool4)\n        x = fake_pool4.forward(x)\n\n        print(""Define Block 5"")\n        # Block 5\n        conv51 = Conv2D(fake_pool4.get_output_shape(), conv51_fshape, 1, ""SAME"")\n        initial_w_conv51 = prot.define_private_input(\n            ""weights-provider"", provide_input_conv51weights\n        )\n        conv51.initialize(initial_w_conv51)\n        x = conv51.forward(x)\n        x = Sigmoid(conv51.get_output_shape()).forward(x)\n        conv52 = Conv2D(conv51.get_output_shape(), conv52_fshape, 1, ""SAME"")\n        initial_w_conv52 = prot.define_private_input(\n            ""weights-provider"", provide_input_conv52weights\n        )\n        conv52.initialize(initial_w_conv52)\n        x = conv52.forward(x)\n        x = Sigmoid(conv52.get_output_shape()).forward(x)\n        conv53 = Conv2D(conv52.get_output_shape(), conv53_fshape, 1, ""SAME"")\n        initial_w_conv53 = prot.define_private_input(\n            ""weights-provider"", provide_input_conv53weights\n        )\n        conv53.initialize(initial_w_conv53)\n        x = conv53.forward(x)\n        x = Sigmoid(conv53.get_output_shape()).forward(x)\n        fake_pool5 = Conv2D(conv53.get_output_shape(), pool5_shape, 2, ""SAME"")\n        initial_w_pool5 = prot.define_private_input(\n            ""weights-provider"", provide_input_pool5weights\n        )\n        fake_pool5.initialize(initial_w_pool5)\n        x = fake_pool5.forward(x)\n\n        print(""Define Reshape"")\n        reshape1 = Reshape(fake_pool5.get_output_shape(), [1, -1])\n        x = reshape1.forward(x)\n\n        print(""Define 2-layer FC"")\n        dense1 = Dense(reshape1.get_output_shape(), 512)\n        dense1.initialize()\n        x = dense1.forward(x)\n        x = Sigmoid(dense1.get_output_shape()).forward(x)\n        dense2 = Dense(dense1.get_output_shape(), 2)\n        dense2.initialize()\n        y = dense2.forward(x)\n\n        # send output\n        prediction_op = prot.define_output(y, receive_output)\n\n        with tfe.Session(config=config) as sess:\n            print(""Initialize tensors"")\n            sess.run(tfe.global_variables_initializer(), tag=""init"")\n\n            print(""Predict"")\n\n            sess.run(prediction_op, tag=""prediction"")\n'"
examples/deprecated/convert.py,13,"b'import os\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import graph_io\nfrom tensorflow.python.framework import graph_util\nfrom tensorflow.python.platform import gfile\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.convert import convert\nfrom tf_encrypted.convert.register import registry\n\n\ndef export_cnn() -> None:\n    input = tf.placeholder(tf.float32, shape=(1, 1, 3, 3))\n    filter = tf.constant(np.ones((3, 3, 1, 1)), dtype=tf.float32)\n    x = tf.nn.conv2d(input, filter, (1, 1, 1, 1), ""SAME"", data_format=""NCHW"")\n    x = tf.nn.sigmoid(x)\n    x = tf.nn.relu(x)\n\n    pred_node_names = [""output""]\n    tf.identity(x, name=pred_node_names[0])\n\n    with tf.Session() as sess:\n        constant_graph = graph_util.convert_variables_to_constants(\n            sess, sess.graph.as_graph_def(), pred_node_names\n        )\n\n    frozen = graph_util.remove_training_nodes(constant_graph)\n\n    output = ""cnn.pb""\n    graph_io.write_graph(frozen, ""."", output, as_text=False)\n\n\nexport_cnn()\n\ntf.reset_default_graph()\n\nmodel_filename = ""cnn.pb""\nwith gfile.GFile(model_filename, ""rb"") as f:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(f.read())\n\nconfig = tfe.LocalConfig(\n    [""server0"", ""server1"", ""crypto-producer"", ""prediction-client"", ""weights-provider""]\n)\n\n\ndef provide_input() -> tf.Tensor:\n    return tf.constant(np.random.normal(size=(1, 1, 28, 28)), tf.float32)\n\n\ndef receive_output(tensor: tf.Tensor) -> tf.Tensor:\n    tf.print(tensor, [tensor])\n    return tensor\n\n\nwith tfe.protocol.Pond(\n    *config.get_players(""server0, server1, crypto-producer"")\n) as prot:\n\n    c = convert.Converter(config, prot, config.get_player(""weights-provider""))\n    x = c.convert(\n        graph_def, registry(), config.get_player(""prediction-client""), provide_input\n    )\n\n    prediction_op = prot.define_output(\n        config.get_player(""prediction-client""), x, receive_output\n    )\n\n    with tfe.Session(config=config) as sess:\n        sess.run(tfe.global_variables_initializer(), tag=""init"")\n\n        sess.run(prediction_op, tag=""prediction"")\n\nos.remove(model_filename)\n'"
examples/deprecated/export_2hlayers_model.py,16,"b'import os\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import graph_io\nfrom tensorflow.python.framework import graph_util\n\nmodel_name = ""2lfc""\noutput_file = dir_path = os.path.join(\n    os.path.dirname(os.path.realpath(__file__)),\n    "".."",\n    ""models"",\n    ""{}.pb"".format(model_name),\n)\ninput_data_file = dir_path = os.path.join(\n    os.path.dirname(os.path.realpath(__file__)),\n    "".."",\n    ""models"",\n    ""{}_input_example.npy"".format(model_name),\n)\n\ninput_size = 16\nhidden_size = 512\ninput_shape = [1, input_size]\nx = tf.placeholder(tf.float32, shape=input_shape)\n\nw1 = tf.get_variable(\n    ""w1"", [input_size, hidden_size], initializer=tf.random_normal_initializer()\n)\nb1 = tf.get_variable(""b1"", [hidden_size], initializer=tf.random_normal_initializer())\nx = tf.matmul(x, w1) + b1\nx = tf.sigmoid(x)\n\nw2 = tf.get_variable(\n    ""w2"", [hidden_size, hidden_size], initializer=tf.random_normal_initializer()\n)\nb2 = tf.get_variable(""b2"", [hidden_size])\nx = tf.matmul(x, w2) + b2\nx = tf.sigmoid(x)\n\nw3 = tf.get_variable(""w3"", [hidden_size, 2], initializer=tf.random_normal_initializer())\ny = tf.matmul(x, w3)\n\npred_node_names = [""output""]\npred = [tf.identity(y, name=pred_node_names[0])]\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\nconstant_graph = graph_util.convert_variables_to_constants(\n    sess, sess.graph.as_graph_def(), pred_node_names\n)\n\nfrozen = graph_util.remove_training_nodes(constant_graph)\n\ngraph_io.write_graph(frozen, ""."", output_file, as_text=False)\nprint(""saved the frozen graph (ready for inference) at: "", output_file)\n\ndata = np.random.standard_normal(input_shape)\nnp.save(input_data_file, data)\n'"
examples/deprecated/export_2inputs_model.py,10,"b'import os\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import graph_io\nfrom tensorflow.python.framework import graph_util\n\nmodel_name = ""2inputs""\noutput_file = os.path.join(\n    os.path.dirname(os.path.realpath(__file__)),\n    "".."",\n    ""models"",\n    ""{}.pb"".format(model_name),\n)\ninput_data_file = os.path.join(\n    os.path.dirname(os.path.realpath(__file__)),\n    "".."",\n    ""models"",\n    ""{}_input_example.npy"".format(model_name),\n)\n\ninput_size = 16\noutput_size = 2\ninput_shape = [1, input_size]\nx1 = tf.placeholder(tf.float32, shape=input_shape)\nx2 = tf.placeholder(tf.float32, shape=input_shape)\n\nx = x1 + x2\nw1 = tf.get_variable(\n    ""w1"", [input_size, output_size], initializer=tf.random_normal_initializer()\n)\nb1 = tf.get_variable(""b1"", [output_size], initializer=tf.random_normal_initializer())\nx = tf.matmul(x, w1) + b1\ny = tf.sigmoid(x)\n\npred_node_names = [""output""]\npred = [tf.identity(y, name=pred_node_names[0])]\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\nconstant_graph = graph_util.convert_variables_to_constants(\n    sess, sess.graph.as_graph_def(), pred_node_names\n)\n\nfrozen = graph_util.remove_training_nodes(constant_graph)\n\ngraph_io.write_graph(frozen, ""."", output_file, as_text=False)\nprint(""saved the frozen graph (ready for inference) at: "", output_file)\n\ndata = np.random.standard_normal(input_shape)\nnp.save(input_data_file, data)\n'"
examples/deprecated/inputs.py,8,"b'import sys\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\n\nconfig = tfe.get_config()\n\nif len(sys.argv) > 1:\n\n    #\n    # assume we\'re running as a server\n    #\n\n    player_name = str(sys.argv[1])\n\n    server = config.server(player_name)\n    server.start()\n    server.join()\n\nelse:\n\n    #\n    # assume we\'re running as master\n    #\n\n    def provide_weights() -> tf.Tensor:\n        raw_w = np.array([5, 5, 5, 5]).reshape((2, 2))\n        w = tf.constant(raw_w)\n        tf.print(w, [w])\n        return w\n\n    def provide_input() -> tf.Tensor:\n        x = tf.constant([1, 2, 3, 4], shape=(2, 2), dtype=tf.float32)\n        tf.print(x, [x])\n        return x\n\n    def receive_output(prediction):\n\n        tf.print([], [prediction], summarize=4)\n        return []\n\n    with tfe.protocol.Pond() as prot:\n\n        # treat weights as private\n        w = prot.define_private_input(""model-provider"", provide_weights)\n\n        # load input for prediction\n        x = prot.define_private_input(""input-provider"", provide_input)\n\n        # compute prediction\n        y = prot.matmul(x, w)\n\n        # send output\n        prediction_op = prot.define_output(""input-provider"", y, receive_output)\n\n        with tfe.Session() as sess:\n            sess.run(tf.global_variables_initializer(), tag=""init"")\n\n            for _ in range(5):\n                sess.run(prediction_op, tag=""prediction"")\n'"
examples/deprecated/int100.py,0,"b'import numpy as np\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.tensor.int100 import int100factory as int100\n\nx = int100.tensor(np.array([1, 2, 3]))\ny = int100.tensor(np.array([1, 2, 3]))\n\nz = x + y\nprint(z)\n\nz = x - y\nprint(z)\n\nz = x * y\nprint(z)\n\nc = int100.constant(np.array([4, 4, 4]))\nv = int100.variable(np.array([1, 1, 1]))\np = int100.placeholder((3,))\n\nwith tfe.Session() as sess:\n\n    print(""Constant"")\n    print(sess.run(c))\n\n    print(""Variable"")\n    sess.run(v.initializer)\n    print(sess.run(v))\n\n    print(""Placeholder"")\n    print(sess.run(p, feed_dict=p.feed(np.array([5, 5, 5]))))\n\n    print(""Assignment"")\n    w = c - p\n    sess.run(v.assign_from_same(w), feed_dict=p.feed(np.array([5, 5, 5])))\n    print(sess.run(v))\n'"
examples/deprecated/int32.py,0,"b'import numpy as np\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.tensor import int32factory as int32\n\nx = int32.tensor(np.array([1, 2, 3]))\ny = int32.tensor(np.array([1, 2, 3]))\n\nz = x + y\n\nz = x - y\n\nz = x * y\n\nc = int32.constant(np.array([4, 4, 4]))\nv = int32.variable(np.array([1, 1, 1]))\np = int32.placeholder((3,))\n\nwith tfe.Session() as sess:\n\n    print(""Constant"")\n    print(sess.run(c))\n\n    print(""Variable"")\n    sess.run(v.initializer)\n    print(sess.run(v))\n\n    print(""Placeholder"")\n    print(sess.run(p, feed_dict=p.feed(np.array([5, 5, 5]))))\n\n    print(""Assignment"")\n    w = c - p\n    sess.run(v.assign_from_same(w), feed_dict=p.feed(np.array([5, 5, 5])))\n    print(sess.run(v))\n'"
examples/deprecated/matmul.py,0,"b'import numpy as np\n\nimport tf_encrypted as tfe\n\na = np.ones((10, 10))\n\nx = tfe.define_private_variable(a)\n\nb = a\ny = x\nfor _ in range(2):\n    b = np.dot(b, b)\n    y = y.matmul(y)\n\nwith tfe.Session() as sess:\n    sess.run(tfe.global_variables_initializer(), tag=""init"")\n    actual = sess.run(y.reveal(), tag=""reveal"")\n\n    expected = b\n    np.testing.assert_allclose(actual, expected)\n'"
examples/deprecated/mnist_deep_cnn.py,34,"b'# This code is based on the following example:\n# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_deep.py\n\n""""""A deep MNIST classifier using convolutional layers.\nSee extensive documentation at\nhttps://www.tensorflow.org/get_started/mnist/pros\n""""""\n# Disable linter warnings to maintain consistency with tutorial.\n# pylint: disable=invalid-name\n# pylint: disable=g-bad-import-order\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport sys\n\nimport numpy\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nfrom tensorflow.python.framework import graph_io\nfrom tensorflow.python.framework import graph_util\n\nFLAGS = None\n\n\ndef deepnn(x):\n    """"""deepnn builds the graph for a deep net for classifying digits.\n  Args:\n  x: an input tensor with the dimensions (N_examples, 784), where 784 is the\n  number of pixels in a standard MNIST image.\n  Returns:\n  A tuple (y, keep_prob). y is a tensor of shape (N_examples, 10), with values\n  equal to the logits of classifying the digit into one of 10 classes (the\n  digits 0-9).\n  """"""\n    # Reshape to use within a convolutional neural net.\n    # Last dimension is for ""features"" - there is only one here, since images are\n    # grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n    with tf.name_scope(""reshape""):\n        x_image = tf.reshape(x, [-1, 28, 28, 1])\n\n    # First convolutional layer - maps one grayscale image to 16 feature maps.\n    with tf.name_scope(""conv1""):\n        W_conv1 = weight_variable([5, 5, 1, 16])\n        b_conv1 = bias_variable([16])\n        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n\n    # Pooling layer - downsamples by 2X.\n    with tf.name_scope(""pool1""):\n        h_pool1 = avg_pool_2x2(h_conv1)\n\n    # Second convolutional layer -- maps 16 feature maps to 16.\n    with tf.name_scope(""conv2""):\n        W_conv2 = weight_variable([5, 5, 16, 16])\n        b_conv2 = bias_variable([16])\n        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n\n    # Second pooling layer.\n    with tf.name_scope(""pool2""):\n        h_pool2 = avg_pool_2x2(h_conv2)\n\n    # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n    # is down to 256 feature maps -- maps this to 1024 features.\n    with tf.name_scope(""fc1""):\n        W_fc1 = weight_variable([256, 100])\n        b_fc1 = bias_variable([100])\n\n        h_pool2_flat = tf.reshape(h_pool2, [-1, 256])\n        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n    # Map the 256 features to 10 classes, one for each digit\n    with tf.name_scope(""fc2""):\n        W_fc2 = weight_variable([100, 10])\n        b_fc2 = bias_variable([10])\n\n        y_conv = tf.matmul(h_fc1, W_fc2) + b_fc2\n    return y_conv\n\n\ndef conv2d(x, W):\n    """"""conv2d returns a 2d convolution layer with full stride.""""""\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=""VALID"")\n\n\ndef avg_pool_2x2(x):\n    """"""avg_pool_2x2 downsamples a feature map by 2X.""""""\n    return tf.nn.avg_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=""VALID"")\n\n\ndef weight_variable(shape):\n    """"""weight_variable generates a weight variable of a given shape.""""""\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\n\ndef bias_variable(shape):\n    """"""bias_variable generates a bias variable of a given shape.""""""\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\n\ndef main(_):\n    # Import data\n    mnist = input_data.read_data_sets(FLAGS.data_dir)\n\n    # Create the model\n    x = tf.placeholder(tf.float32, [None, 784])\n\n    # Define loss and optimizer\n    y_ = tf.placeholder(tf.int64, [None])\n\n    # Build the graph for the deep net\n    y_conv = deepnn(x)\n\n    with tf.name_scope(""loss""):\n        cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y_, logits=y_conv)\n        cross_entropy = tf.reduce_mean(cross_entropy)\n\n    with tf.name_scope(""adam_optimizer""):\n        train_step = tf.train.AdamOptimizer(1e-2).minimize(cross_entropy)\n\n    with tf.name_scope(""accuracy""):\n        correct_prediction = tf.equal(tf.argmax(y_conv, 1), y_)\n        correct_prediction = tf.cast(correct_prediction, tf.float32)\n        accuracy = tf.reduce_mean(correct_prediction)\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for i in range(2000):\n            batch = mnist.train.next_batch(50)\n            if i % 100 == 0:\n                train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1]})\n                print(""step %d, training accuracy %g"" % (i, train_accuracy))\n            train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n\n        # compute in batches to avoid OOM on GPUs\n        accuracy_l = []\n        for _ in range(20):\n            batch = mnist.test.next_batch(500, shuffle=False)\n\n            accuracy_l.append(accuracy.eval(feed_dict={x: batch[0], y_: batch[1]}))\n\n        print(""test accuracy %g"" % numpy.mean(accuracy_l))\n\n        # Export model to protobuf file\n        current_dir = os.getcwd()\n        pb_filename = ""/test_data/mnist_model.pb""\n        export_to_pb(sess, y_conv, current_dir + pb_filename)\n\n        # Export sample mnist data\n        np_filename = ""/test_data/mnist_input.npy""\n        numpy.save(current_dir + np_filename, mnist.test.images[0])\n\n\ndef export_to_pb(sess, x, filename):\n    pred_names = [""output""]\n    tf.identity(x, name=pred_names[0])\n\n    graph = graph_util.convert_variables_to_constants(\n        sess, sess.graph.as_graph_def(), pred_names\n    )\n\n    graph = graph_util.remove_training_nodes(graph)\n    path = graph_io.write_graph(graph, ""."", filename, as_text=False)\n    print(""saved the frozen graph (ready for inference) at: "", filename)\n\n    return path\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--data_dir"",\n        type=str,\n        default=""/tmp/tensorflow/mnist/input_data"",\n        help=""Directory for storing input data"",\n    )\n    FLAGS, unparsed = parser.parse_known_args()\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
examples/deprecated/pond-simple.py,0,"b'import numpy as np\n\nimport tf_encrypted as tfe\n\nprot = tfe.protocol.Pond()\n\n# a = prot.define_constant(np.array([4, 3, 2, 1]).reshape(2,2))\n# b = prot.define_constant(np.array([4, 3, 2, 1]).reshape(2,2))\n# c = a * b\n\na = prot.define_private_variable(np.array([1.0, 2.0, 3.0, 4.0]).reshape(2, 2))\nb = prot.define_private_variable(np.array([1.0, 2.0, 3.0, 4.0]).reshape(2, 2))\nc = prot.define_private_variable(np.array([1.0, 2.0, 3.0, 4.0]).reshape(2, 2))\n\nx = a * b\ny = a * c\nz = x + y\n\nw = prot.define_private_variable(np.zeros((2, 2)))\n\nwith tfe.Session() as sess:\n    # print(sess.run(c, tag=\'c\'))\n\n    sess.run(tfe.global_variables_initializer(), tag=""init"")\n    sess.run(prot.assign(w, z), tag=""assign"")\n    sess.run(prot.assign(w, z), tag=""assign"")\n\n    print(sess.run(w.reveal(), tag=""reveal""))\n'"
examples/deprecated/securenn-playground.py,0,"b'import numpy as np\n\nimport tf_encrypted as tfe\n\nconfig = tfe.get_config()\nwith tfe.protocol.SecureNN(\n    *config.get_players(""server0, server1, crypto-producer"")\n) as prot:\n\n    a = prot.define_constant(np.array([0, 0, 1, 1]), apply_scaling=False)\n    b = prot.define_constant(np.array([0, 1, 0, 1]), apply_scaling=False)\n    c = prot.bitwise_or(a, b)\n\n    x = prot.define_constant(np.array([0.0, 1.0, 2.0, 3.0]))\n    y = prot.define_constant(np.array([0.0, 1.0, 2.0, 3.0]))\n    z = (x * c) * y\n\n    with tfe.Session() as sess:\n        print(sess.run(z, tag=""res""))\n'"
examples/federated-learning/convert.py,18,"b'""""""Data processing helpers.""""""\n#\n# Based on:\n# - https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py  # noqa\n# - https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py  # noqa\n#\nimport tensorflow as tf\n\n\ndef encode_image(value):\n    """"""Encode images into a tf.train.Feature for a TFRecord.""""""\n    bytes_list = tf.train.BytesList(value=[value.tostring()])\n    return tf.train.Feature(bytes_list=bytes_list)\n\n\ndef decode_image(value):\n    """"""Decode the image from a tf.train.Feature in a TFRecord.""""""\n    image = tf.decode_raw(value, tf.uint8)\n    image.set_shape((28 * 28))\n    return image\n\n\ndef encode_label(value):\n    """"""Encode a label into a tf.train.Feature for a TFRecord.""""""\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef decode_label(value):\n    """"""Decode the label from a tf.train.Feature in a TFRecord.""""""\n    return tf.cast(value, tf.int32)\n\n\ndef encode(image, label):\n    """"""Encode an instance as a tf.train.Example for a TFRecord.""""""\n    feature_dict = {""image"": encode_image(image), ""label"": encode_label(label)}\n    features = tf.train.Features(feature=feature_dict)\n    return tf.train.Example(features=features)\n\n\ndef decode(serialized_example):\n    """"""Decode an instance from a tf.train.Example in a TFRecord.""""""\n    features = tf.parse_single_example(\n        serialized_example,\n        features={\n            ""image"": tf.FixedLenFeature([], tf.string),\n            ""label"": tf.FixedLenFeature([], tf.int64),\n        },\n    )\n    image = decode_image(features[""image""])\n    label = decode_label(features[""label""])\n    return image, label\n\n\ndef normalize(image, label):\n    """"""Standardization of MNIST images.""""""\n    x = tf.cast(image, tf.float32) / 255.0\n    image = (x - 0.1307) / 0.3081  # image = (x - mean) / std\n    return image, label\n\n\ndef get_data_from_tfrecord(filename, batch_size: int):\n    """"""Construct a TFRecordDataset iterator.""""""\n    return (\n        tf.data.TFRecordDataset([filename])\n        .map(decode)\n        .map(normalize)\n        .repeat()\n        .batch(batch_size)\n        .make_one_shot_iterator()\n    )\n'"
examples/federated-learning/download.py,1,"b'""""""Downloading the MNIST dataset and storing as TFRecords.""""""\n\nimport os\n\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\n\nfrom convert import encode\n\n\ndef save_training_data(images, labels, filename):\n    """"""Convert Keras MNIST data into TFRecords.""""""\n    assert images.shape[0] == labels.shape[0]\n    num_examples = images.shape[0]\n\n    with tf.python_io.TFRecordWriter(filename) as writer:\n\n        for index in range(num_examples):\n\n            image = images[index]\n            label = labels[index]\n            example = encode(image, label)\n            writer.write(example.SerializeToString())\n\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\ndata_dir = os.path.expanduser(""./data/"")\nif not os.path.exists(data_dir):\n    os.makedirs(data_dir)\n\nsave_training_data(x_train, y_train, os.path.join(data_dir, ""train.tfrecord""))\nsave_training_data(x_test, y_test, os.path.join(data_dir, ""test.tfrecord""))\n'"
examples/federated-learning/run.py,31,"b'""""""An example of the secure aggregation protocol for federated learning.""""""\n\nimport logging\nimport sys\n\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\nfrom convert import decode\n\nif len(sys.argv) > 1:\n    # config file was specified\n    config_file = sys.argv[1]\n    config = tfe.RemoteConfig.load(config_file)\n    tfe.set_config(config)\n    tfe.set_protocol(tfe.protocol.Pond())\n\nsession_target = sys.argv[2] if len(sys.argv) > 2 else None\n\n\nclass ModelOwner:\n    """"""Contains code meant to be executed by some `ModelOwner` Player.\n\n  Args:\n    player_name: `str`, name of the `tfe.player.Player`\n                 representing the model owner.\n  """"""\n\n    LEARNING_RATE = 0.1\n    ITERATIONS = 60000 // 30\n\n    def __init__(self, player_name):\n        self.player_name = player_name\n\n        with tf.device(tfe.get_config().get_player(player_name).device_name):\n            self._initialize_weights()\n\n    def _initialize_weights(self):\n        with tf.name_scope(""parameters""):\n            self.w0 = tf.Variable(tf.random_normal([28 * 28, 512]))\n            self.b0 = tf.Variable(tf.zeros([512]))\n            self.w1 = tf.Variable(tf.random_normal([512, 10]))\n            self.b1 = tf.Variable(tf.zeros([10]))\n\n    def _build_model(self, x, y):\n        """"""Build the model function for federated learning.\n\n    Includes loss calculation and backprop.\n    """"""\n        w0 = self.w0.read_value()\n        b0 = self.b0.read_value()\n        w1 = self.w1.read_value()\n        b1 = self.b1.read_value()\n        params = (w0, b0, w1, b1)\n\n        layer0 = tf.matmul(x, w0) + b0\n        layer1 = tf.nn.sigmoid(layer0)\n        layer2 = tf.matmul(layer1, w1) + b1\n        predictions = layer2\n\n        loss = tf.reduce_mean(\n            tf.losses.sparse_softmax_cross_entropy(logits=predictions, labels=y)\n        )\n        grads = tf.gradients(ys=loss, xs=params)\n        return predictions, loss, grads\n\n    def build_update_step(self, x, y):\n        """"""Build a graph representing a single update step.\n\n    This method will be called once by all data owners\n    to create a local gradient computation on their machine.\n    """"""\n        _, _, grads = self._build_model(x, y)\n        return grads\n\n    def _build_validation_step(self, x, y):\n        predictions, loss, _ = self._build_model(x, y)\n        most_likely = tf.argmax(predictions, axis=1)\n        return most_likely, loss\n\n    def _build_data_pipeline(self):\n        """"""Build data pipeline for validation by model owner.""""""\n\n        def normalize(image, label):\n            image = tf.cast(image, tf.float32) / 255.0\n            return image, label\n\n        dataset = tf.data.TFRecordDataset([""./data/train.tfrecord""])\n        dataset = dataset.map(decode)\n        dataset = dataset.map(normalize)\n        dataset = dataset.batch(50)\n        dataset = dataset.take(1)  # keep validating on the same items\n        dataset = dataset.repeat()\n\n        iterator = dataset.make_one_shot_iterator()\n        return iterator.get_next()\n\n    @tfe.local_computation\n    def update_model(self, *grads):\n        """"""Perform a single update step.\n\n    This will be performed on the ModelOwner device\n    after securely aggregating gradients.\n\n    Args:\n      *grads: `tf.Variables` representing the federally computed gradients.\n    """"""\n        params = [self.w0, self.b0, self.w1, self.b1]\n        grads = [tf.cast(grad, tf.float32) for grad in grads]\n        with tf.name_scope(""update""):\n            update_op = tf.group(\n                *[\n                    param.assign(param - grad * self.LEARNING_RATE)\n                    for param, grad in zip(params, grads)\n                ]\n            )\n\n        with tf.name_scope(""validate""):\n            x, y = self._build_data_pipeline()\n            y_hat, loss = self._build_validation_step(x, y)\n\n            with tf.control_dependencies([update_op]):\n                print_loss = tf.print(""loss"", loss)\n                print_expected = tf.print(""expect"", y, summarize=50)\n                print_result = tf.print(""result"", y_hat, summarize=50)\n                return tf.group(print_loss, print_expected, print_result)\n\n\nclass DataOwner:\n    """"""Contains methods meant to be executed by a data owner.\n\n  Args:\n    player_name: `str`, name of the `tfe.player.Player`\n                 representing the data owner\n    build_update_step: `Callable`, the function used to construct\n                       a local federated learning update.\n  """"""\n\n    BATCH_SIZE = 30\n\n    def __init__(self, player_name, local_data_file, build_update_step):\n        self.player_name = player_name\n        self.local_data_file = local_data_file\n        self._build_update_step = build_update_step\n\n    def _build_data_pipeline(self):\n        """"""Build local data pipeline for federated DataOwners.""""""\n\n        def normalize(image, label):\n            image = tf.cast(image, tf.float32) / 255.0\n            return image, label\n\n        dataset = tf.data.TFRecordDataset([self.local_data_file])\n        dataset = dataset.map(decode)\n        dataset = dataset.map(normalize)\n        dataset = dataset.repeat()\n        dataset = dataset.batch(self.BATCH_SIZE)\n\n        iterator = dataset.make_one_shot_iterator()\n        return iterator.get_next()\n\n    @tfe.local_computation\n    def compute_gradient(self):\n        """"""Compute gradient given current model parameters and local data.""""""\n        with tf.name_scope(""data_loading""):\n            x, y = self._build_data_pipeline()\n\n        with tf.name_scope(""gradient_computation""):\n            grads = self._build_update_step(x, y)\n\n        return grads\n\n\nif __name__ == ""__main__"":\n\n    logging.basicConfig(level=logging.DEBUG)\n\n    model_owner = ModelOwner(""model-owner"")\n    data_owners = [\n        DataOwner(\n            ""data-owner-0"", ""./data/train.tfrecord"", model_owner.build_update_step\n        ),\n        DataOwner(\n            ""data-owner-1"", ""./data/train.tfrecord"", model_owner.build_update_step\n        ),\n        DataOwner(\n            ""data-owner-2"", ""./data/train.tfrecord"", model_owner.build_update_step\n        ),\n    ]\n\n    model_grads = zip(*(data_owner.compute_gradient() for data_owner in data_owners))\n\n    with tf.name_scope(""secure_aggregation""):\n        aggregated_model_grads = [\n            tfe.add_n(grads) / len(grads) for grads in model_grads\n        ]\n\n    iteration_op = model_owner.update_model(*aggregated_model_grads)\n\n    with tfe.Session(target=session_target) as sess:\n        sess.run(tf.global_variables_initializer(), tag=""init"")\n\n        for i in range(model_owner.ITERATIONS):\n            if i % 100 == 0:\n                print(""Iteration {}"".format(i))\n                sess.run(iteration_op, tag=""iteration"")\n            else:\n                sess.run(iteration_op)\n'"
examples/house-credit-default/get_input.py,0,"b'""""""CLI for data preparation and processing.""""""\nimport argparse\n\nfrom utils import data_prep\nfrom utils import read_one_row\nfrom utils import save_input\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    ""--save_row"",\n    type=int,\n    default=""0"",\n    help=""Saves a single row to a file defaults to row 0"",\n)\nparser.add_argument(\n    ""--input_file"",\n    type=str,\n    default=""final_data_with_feature_engineered.csv"",\n    help=(\n        ""File to read the row from defaults to ""\n        ""final_data_with_feature_engineered.csv""\n    ),\n)\nparser.add_argument(\n    ""--output_file"",\n    type=str,\n    default=""input.npy"",\n    help=(""Output file with the input row defaults to "" ""input.npy""),\n)\nconfig = parser.parse_args()\n\ninput_file = config.input_file\noutput_file = config.output_file\nsave_row = config.save_row\n\ntrain_x_df, _ = data_prep(input_file)\nout = read_one_row(save_row, train_x_df)\nsave_input(output_file, out)\n'"
examples/house-credit-default/main.py,7,"b'""""""Plaintext benchmark for""""""\nimport argparse\nimport os\nimport time\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import layers\nfrom tensorflow.python import errors_impl as errors\nfrom tensorflow.python.framework import graph_io\nfrom tensorflow.python.framework import graph_util\n\nfrom utils import data_prep\nfrom utils import read_one_row\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    ""--predict"",\n    type=int,\n    default=""-1"",\n    help=""Do a prediction on specified row of input file"",\n)\nparser.add_argument(\n    ""--input_file"",\n    type=str,\n    default=""final_data_with_feature_engineered.csv"",\n    help=(""Location of input, defaults to "", ""final_data_with_feature_engineered.csv""),\n)\nparser.add_argument(\n    ""--bench_prediction"",\n    type=int,\n    default=-1,\n    help=(""Benchmark prediction by doing n iterations"", ""and taking the average""),\n)\nconfig = parser.parse_args()\n\nepochs = 20\nbatch_size = 256\n\ncheckpoint_path = ""./saved_models/train""\n\npredict_row = config.predict\ninput_file = config.input_file\nbench_prediction = config.bench_prediction\n\n\ndef export_to_pb(sess, x, filename):\n    pred_names = [""output""]\n    tf.identity(x, name=pred_names[0])\n\n    graph = graph_util.convert_variables_to_constants(\n        sess, sess.graph.as_graph_def(), pred_names\n    )\n\n    graph = graph_util.remove_training_nodes(graph)\n    path = graph_io.write_graph(graph, ""."", filename, as_text=False)\n    print(""saved the frozen graph (ready for inference) at: "", path)\n\n\ndef print_nodes(graph):\n    """"""Print a list of nodes from a tf.Graph.""""""\n    print([n.name for n in graph.as_graph_def().node])\n\n\ndef build_model(input_shape):\n    """"""Build a logistic regression model with tf.keras.""""""\n    model = keras.Sequential(\n        [\n            layers.Dense(\n                1, use_bias=False, activation=""sigmoid"", input_shape=[input_shape]\n            ),\n        ]\n    )\n\n    model.compile(\n        loss=""binary_crossentropy"",\n        optimizer=tf.train.AdamOptimizer(),\n        metrics=[""accuracy""],\n    )\n\n    return model\n\n\ndef train(train_x_df, train_y_df):\n    """"""Train a logistic regressor on the dataset.""""""\n    x = list(train_x_df.columns.values)\n    model = build_model(len(x))\n\n    os.makedirs(""./saved_models"", exist_ok=True)\n\n    cp_callback = keras.callbacks.ModelCheckpoint(\n        checkpoint_path, save_weights_only=True, save_best_only=True, verbose=1\n    )\n\n    # first 80 percent for training\n    train_x = train_x_df[1:246005]\n    train_y = train_y_df[1:246005]\n\n    # other 20 percent for evaluating\n    eval_x = train_x_df[246006 : len(train_x_df) - 1]\n    eval_y = train_y_df[246006 : len(train_y_df) - 1]\n\n    # train model\n    model.fit(\n        train_x,\n        train_y,\n        epochs=epochs,\n        validation_split=0.2,\n        verbose=0,\n        batch_size=batch_size,\n        callbacks=[cp_callback],\n    )\n\n    print(""done training"")\n\n    # get the default session and graph for exporting and calculating the AUC\n    sess = K.get_session()\n    graph = K.get_session().graph\n\n    # export the graph to a protobuf file for loading in tfe and secure enclave\n    export_to_pb(\n        K.get_session(),\n        graph.get_tensor_by_name(""dense/Sigmoid:0""),\n        ""house_credit_default.pb"",\n    )\n\n    # evaluate the model using AUC, the metric used in the kaggle competition\n    loss = model.evaluate(eval_x, eval_y, batch_size=batch_size)\n\n    predictions = model.predict(eval_x, batch_size=batch_size)\n    auc = tf.metrics.auc(eval_y, predictions)\n\n    print(""Evaluation Loss:"", loss[0])\n    print(""Accuracy:"", loss[1])\n\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.local_variables_initializer())\n\n    print(""AUC: "", sess.run([auc])[0][1])\n\n\ndef predict_preamble(train_x_df, train_y_df):\n    """"""Load the trained model and prepare a data point for prediction.""""""\n    x = list(train_x_df.columns.values)\n    model = build_model(len(x))\n\n    try:\n        model.load_weights(checkpoint_path)\n    except errors.InvalidArgumentError:\n        print(""Weights couldn\'t be found, training before predicting"")\n        train(train_x_df, train_y_df)\n        model = build_model(len(x))\n\n    x = read_one_row(predict_row, train_x_df)\n\n    return model, x\n\n\ndef predict(train_x_df, train_y_df):\n    model, x = predict_preamble(train_x_df, train_y_df)\n\n    print(""Prediction:"", model.predict(x)[0][0])\n\n\ndef benchmark(train_x_df, train_y_df):\n    """"""Benchmark the time required to predict on the `bench_prediction` data.""""""\n    model, x = predict_preamble(train_x_df, train_y_df)\n\n    total_duration = 0\n    for _ in range(0, bench_prediction):\n        start = time.time()\n        model.predict(x)\n        end = time.time()\n        duration = end - start\n\n        total_duration = total_duration + duration\n\n    print(""Total Duration:"", total_duration)\n    print(""Avg Runtime:"", total_duration / bench_prediction * 1000, ""ms"")\n\n\ndef main():\n    print(""Home Credit Default!"")\n\n    # TODO only load all data when training\n    train_x_df, train_y_df = data_prep(input_file)\n\n    if predict_row != -1:\n        predict(train_x_df, train_y_df)\n    elif bench_prediction != -1:\n        benchmark(train_x_df, train_y_df)\n    else:\n        train(train_x_df, train_y_df)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/house-credit-default/utils.py,0,"b'""""""Data preparation utilities.""""""\nimport numpy as np\n\nimport pandas as pd\n\n\ndef data_prep(filename):\n    """"""Preprocess data from CSV.""""""\n    df = pd.read_csv(filename)\n\n    # remove whitespace from pandas names\n    df.columns = df.columns.str.replace("" "", ""_"")\n    df.columns = df.columns.str.replace("":"", ""-"")\n    df.columns = df.columns.str.replace(""("", ""_"")\n    df.columns = df.columns.str.replace("")"", ""_"")\n    df.columns = df.columns.str.replace(""+"", ""."")\n    df.columns = df.columns.str.replace("","", ""_"")\n\n    train_df = df[df[""TARGET""].notnull()]\n    train_x_df = train_df.drop(columns=[""TARGET"", ""SK_ID_CURR"", ""index""])\n    train_y_df = train_df[""TARGET""]\n\n    return train_x_df, train_y_df\n\n\ndef save_input(filename, output):\n    np.save(filename, output)\n\n\ndef read_one_row(row, train_x_df):\n    return train_x_df[row : row + 1]\n'"
examples/logistic/common.py,29,"b'""""""Provide classes to perform private training and private prediction with\nlogistic regression""""""\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\n\n\nclass LogisticRegression:\n    """"""Contains methods to build and train logistic regression.""""""\n\n    def __init__(self, num_features):\n        self.w = tfe.define_private_variable(\n            tf.random_uniform([num_features, 1], -0.01, 0.01)\n        )\n        self.w_masked = tfe.mask(self.w)\n        self.b = tfe.define_private_variable(tf.zeros([1]))\n        self.b_masked = tfe.mask(self.b)\n\n    @property\n    def weights(self):\n        return self.w, self.b\n\n    def forward(self, x):\n        with tf.name_scope(""forward""):\n            out = tfe.matmul(x, self.w_masked) + self.b_masked\n            y = tfe.sigmoid(out)\n            return y\n\n    def backward(self, x, dy, learning_rate=0.01):\n        batch_size = x.shape.as_list()[0]\n        with tf.name_scope(""backward""):\n            dw = tfe.matmul(tfe.transpose(x), dy) / batch_size\n            db = tfe.reduce_sum(dy, axis=0) / batch_size\n            assign_ops = [\n                tfe.assign(self.w, self.w - dw * learning_rate),\n                tfe.assign(self.b, self.b - db * learning_rate),\n            ]\n            return assign_ops\n\n    def loss_grad(self, y, y_hat):\n        with tf.name_scope(""loss-grad""):\n            dy = y_hat - y\n            return dy\n\n    def fit_batch(self, x, y):\n        with tf.name_scope(""fit-batch""):\n            y_hat = self.forward(x)\n            dy = self.loss_grad(y, y_hat)\n            fit_batch_op = self.backward(x, dy)\n            return fit_batch_op\n\n    def fit(self, sess, x, y, num_batches):\n        fit_batch_op = self.fit_batch(x, y)\n        for batch in range(num_batches):\n            print(""Batch {0: >4d}"".format(batch))\n            sess.run(fit_batch_op, tag=""fit-batch"")\n\n    def evaluate(self, sess, x, y, data_owner):\n        """"""Return the accuracy""""""\n\n        def print_accuracy(y_hat, y) -> tf.Operation:\n            with tf.name_scope(""print-accuracy""):\n                correct_prediction = tf.equal(tf.round(y_hat), y)\n                accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n                print_op = tf.print(\n                    ""Accuracy on {}:"".format(data_owner.player_name), accuracy\n                )\n                return print_op\n\n        with tf.name_scope(""evaluate""):\n            y_hat = self.forward(x)\n            print_accuracy_op = tfe.define_output(\n                data_owner.player_name, [y_hat, y], print_accuracy\n            )\n\n        sess.run(print_accuracy_op, tag=""evaluate"")\n\n\nclass DataOwner:\n    """"""Contains code meant to be executed by a data owner Player.""""""\n\n    def __init__(\n        self, player_name, num_features, training_set_size, test_set_size, batch_size\n    ):\n        self.player_name = player_name\n        self.num_features = num_features\n        self.training_set_size = training_set_size\n        self.test_set_size = test_set_size\n        self.batch_size = batch_size\n        self.train_initializer = None\n        self.test_initializer = None\n\n    @property\n    def initializer(self):\n        return tf.group(self.train_initializer, self.test_initializer)\n\n    @tfe.local_computation\n    def provide_training_data(self):\n        """"""Preprocess training dataset\n\n    Return single batch of training dataset\n    """"""\n\n        def norm(x, y):\n            return tf.cast(x, tf.float32), tf.expand_dims(y, 0)\n\n        x_raw = tf.random.uniform(\n            minval=-0.5, maxval=0.5, shape=[self.training_set_size, self.num_features]\n        )\n\n        y_raw = tf.cast(tf.reduce_mean(x_raw, axis=1) > 0, dtype=tf.float32)\n\n        train_set = (\n            tf.data.Dataset.from_tensor_slices((x_raw, y_raw))\n            .map(norm)\n            .repeat()\n            .shuffle(buffer_size=self.batch_size)\n            .batch(self.batch_size)\n        )\n\n        train_set_iterator = train_set.make_initializable_iterator()\n        self.train_initializer = train_set_iterator.initializer\n\n        x, y = train_set_iterator.get_next()\n        x = tf.reshape(x, [self.batch_size, self.num_features])\n        y = tf.reshape(y, [self.batch_size, 1])\n\n        return x, y\n\n    @tfe.local_computation\n    def provide_testing_data(self):\n        """"""Preprocess testing dataset\n\n    Return single batch of testing dataset\n    """"""\n\n        def norm(x, y):\n            return tf.cast(x, tf.float32), tf.expand_dims(y, 0)\n\n        x_raw = tf.random.uniform(\n            minval=-0.5, maxval=0.5, shape=[self.test_set_size, self.num_features]\n        )\n\n        y_raw = tf.cast(tf.reduce_mean(x_raw, axis=1) > 0, dtype=tf.float32)\n\n        test_set = (\n            tf.data.Dataset.from_tensor_slices((x_raw, y_raw))\n            .map(norm)\n            .batch(self.test_set_size)\n        )\n\n        test_set_iterator = test_set.make_initializable_iterator()\n        self.test_initializer = test_set_iterator.initializer\n\n        x, y = test_set_iterator.get_next()\n        x = tf.reshape(x, [self.test_set_size, self.num_features])\n        y = tf.reshape(y, [self.test_set_size, 1])\n\n        return x, y\n\n\nclass ModelOwner:\n    """"""Contains code meant to be executed by a model owner Player.""""""\n\n    def __init__(self, player_name):\n        self.player_name = player_name\n\n    @tfe.local_computation\n    def receive_weights(self, *weights):\n        return tf.print(""Weights on {}:"".format(self.player_name), weights)\n\n\nclass PredictionClient:\n    """"""Contains methods meant to be executed by a prediction client.""""""\n\n    def __init__(self, player_name, num_features):\n        self.player_name = player_name\n        self.num_features = num_features\n\n    @tfe.local_computation\n    def provide_input(self):\n        return tf.random.uniform(\n            minval=-0.5, maxval=0.5, dtype=tf.float32, shape=[1, self.num_features]\n        )\n\n    @tfe.local_computation\n    def receive_output(self, result):\n        return tf.print(""Result on {}:"".format(self.player_name), result)\n'"
examples/logistic/data.py,9,"b'""""""Dummy data preparation for logistic regression training.""""""\nfrom typing import Tuple\n\nimport numpy as np\nimport tensorflow as tf\n\nnp.random.seed(1)\n\n\ndef norm(x: tf.Tensor, y: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n    return tf.cast(x, tf.float32), tf.expand_dims(y, 0)\n\n\ndef gen_training_input(total_size, nb_feats, batch_size):\n    """"""Generate random data for training.""""""\n    x_np = np.random.uniform(-0.5, 0.5, size=[total_size, nb_feats])\n    y_np = np.array(x_np.mean(axis=1) > 0, np.float32)\n    train_set = (\n        tf.data.Dataset.from_tensor_slices((x_np, y_np))\n        .map(norm)\n        .shuffle(buffer_size=100)\n        .repeat()\n        .batch(batch_size)\n    )\n    train_set_iterator = train_set.make_one_shot_iterator()\n    x, y = train_set_iterator.get_next()\n    x = tf.reshape(x, [batch_size, nb_feats])\n    y = tf.reshape(y, [batch_size, 1])\n\n    # tf.print(x, data=[x], message=""x: "", summarize=6)\n    return x, y\n\n\ndef gen_test_input(total_size, nb_feats, batch_size):\n    """"""Generate random data for evaluation.""""""\n    x_test_np = np.random.uniform(-0.5, 0.5, size=[total_size, nb_feats])\n    y_test_np = np.array(x_test_np.mean(axis=1) > 0, np.float32)\n    test_set = (\n        tf.data.Dataset.from_tensor_slices((x_test_np, y_test_np))\n        .map(norm)\n        .batch(batch_size)\n    )\n    test_set_iterator = test_set.make_one_shot_iterator()\n    x_test, y_test = test_set_iterator.get_next()\n    x_test = tf.reshape(x_test, [batch_size, nb_feats])\n    y_test = tf.reshape(y_test, [batch_size, 1])\n\n    return x_test, y_test\n'"
examples/logistic/prediction_joint.py,0,"b'""""""Private prediction on combined features from several clients""""""\nimport tf_encrypted as tfe\nfrom common import LogisticRegression\nfrom common import PredictionClient\n\nnum_features = 10\n\nmodel = LogisticRegression(num_features)\nprediction_client_0 = PredictionClient(""prediction-client-0"", num_features // 2)\nprediction_client_1 = PredictionClient(""prediction-client-1"", num_features // 2)\nresult_receiver = prediction_client_0\n\nx_0 = prediction_client_0.provide_input()\nx_1 = prediction_client_1.provide_input()\nx = tfe.concat([x_0, x_1], axis=1)\n\ny = model.forward(x)\n\n\nreveal_output = result_receiver.receive_output(y)\n\nwith tfe.Session() as sess:\n    sess.run(tfe.global_variables_initializer(), tag=""init"")\n\n    sess.run(reveal_output, tag=""predict"")\n'"
examples/logistic/prediction_single.py,0,"b'""""""Private prediction with a single clients""""""\nimport tf_encrypted as tfe\nfrom common import LogisticRegression\nfrom common import PredictionClient\n\nnum_features = 10\n\nmodel = LogisticRegression(num_features)\nprediction_client = PredictionClient(""prediction-client"", num_features)\n\nx = prediction_client.provide_input()\n\ny = model.forward(x)\n\nreveal_output = prediction_client.receive_output(y)\n\nwith tfe.Session() as sess:\n    sess.run(tfe.global_variables_initializer(), tag=""init"")\n\n    sess.run(reveal_output, tag=""predict"")\n'"
examples/logistic/training_joint.py,0,"b'""""""Private training on combined data from several data owners""""""\nimport tf_encrypted as tfe\nfrom common import DataOwner\nfrom common import LogisticRegression\nfrom common import ModelOwner\n\nnum_features = 10\ntraining_set_size = 2000\ntest_set_size = 100\nbatch_size = 100\nnum_batches = (training_set_size // batch_size) * 10\n\nmodel_owner = ModelOwner(""model-owner"")\ndata_owner_0 = DataOwner(\n    ""data-owner-0"", num_features, training_set_size, test_set_size, batch_size // 2\n)\ndata_owner_1 = DataOwner(\n    ""data-owner-1"", num_features, training_set_size, test_set_size, batch_size // 2\n)\n\ntfe.set_protocol(\n    tfe.protocol.Pond(\n        tfe.get_config().get_player(data_owner_0.player_name),\n        tfe.get_config().get_player(data_owner_1.player_name),\n    )\n)\n\nx_train_0, y_train_0 = data_owner_0.provide_training_data()\nx_train_1, y_train_1 = data_owner_1.provide_training_data()\n\nx_test_0, y_test_0 = data_owner_0.provide_testing_data()\nx_test_1, y_test_1 = data_owner_1.provide_testing_data()\n\nx_train = tfe.concat([x_train_0, x_train_1], axis=0)\ny_train = tfe.concat([y_train_0, y_train_1], axis=0)\n\nmodel = LogisticRegression(num_features)\nreveal_weights_op = model_owner.receive_weights(model.weights)\n\nwith tfe.Session() as sess:\n    sess.run(\n        [\n            tfe.global_variables_initializer(),\n            data_owner_0.initializer,\n            data_owner_1.initializer,\n        ],\n        tag=""init"",\n    )\n\n    model.fit(sess, x_train, y_train, num_batches)\n    # TODO(Morten)\n    # each evaluation results in nodes for a forward pass being added to the graph;\n    # maybe there\'s some way to avoid this, even if it means only if the shapes match\n    model.evaluate(sess, x_test_0, y_test_0, data_owner_0)\n    model.evaluate(sess, x_test_1, y_test_1, data_owner_1)\n\n    sess.run(reveal_weights_op, tag=""reveal"")\n'"
examples/logistic/training_single.py,0,"b'""""""Private training on data from a single owner""""""\nimport tf_encrypted as tfe\nfrom common import DataOwner\nfrom common import LogisticRegression\nfrom common import ModelOwner\n\nnum_features = 10\ntraining_set_size = 2000\ntest_set_size = 100\nbatch_size = 100\nnum_batches = (training_set_size // batch_size) * 10\n\nmodel = LogisticRegression(num_features)\nmodel_owner = ModelOwner(""model-owner"")\ndata_owner = DataOwner(\n    ""data-owner"", num_features, training_set_size, test_set_size, batch_size\n)\n\nx_train, y_train = data_owner.provide_training_data()\nx_test, y_test = data_owner.provide_testing_data()\n\nreveal_weights_op = model_owner.receive_weights(model.weights)\n\nwith tfe.Session() as sess:\n    sess.run([tfe.global_variables_initializer(), data_owner.initializer], tag=""init"")\n\n    model.fit(sess, x_train, y_train, num_batches)\n    model.evaluate(sess, x_test, y_test, data_owner)\n\n    sess.run(reveal_weights_op, tag=""reveal"")\n'"
examples/mnist/convert.py,18,"b'""""""Data processing helpers.""""""\n#\n# Based on:\n# - https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py  # noqa\n# - https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py  # noqa\n#\nimport tensorflow as tf\n\n\ndef encode_image(value):\n    """"""Encode images into a tf.train.Feature for a TFRecord.""""""\n    bytes_list = tf.train.BytesList(value=[value.tostring()])\n    return tf.train.Feature(bytes_list=bytes_list)\n\n\ndef decode_image(value):\n    """"""Decode the image from a tf.train.Feature in a TFRecord.""""""\n    image = tf.decode_raw(value, tf.uint8)\n    image.set_shape((28 * 28))\n    return image\n\n\ndef encode_label(value):\n    """"""Encode a label into a tf.train.Feature for a TFRecord.""""""\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef decode_label(value):\n    """"""Decode the label from a tf.train.Feature in a TFRecord.""""""\n    return tf.cast(value, tf.int32)\n\n\ndef encode(image, label):\n    """"""Encode an instance as a tf.train.Example for a TFRecord.""""""\n    feature_dict = {""image"": encode_image(image), ""label"": encode_label(label)}\n    features = tf.train.Features(feature=feature_dict)\n    return tf.train.Example(features=features)\n\n\ndef decode(serialized_example):\n    """"""Decode an instance from a tf.train.Example in a TFRecord.""""""\n    features = tf.parse_single_example(\n        serialized_example,\n        features={\n            ""image"": tf.FixedLenFeature([], tf.string),\n            ""label"": tf.FixedLenFeature([], tf.int64),\n        },\n    )\n    image = decode_image(features[""image""])\n    label = decode_label(features[""label""])\n    return image, label\n\n\ndef normalize(image, label):\n    """"""Standardization of MNIST images.""""""\n    x = tf.cast(image, tf.float32) / 255.0\n    image = (x - 0.1307) / 0.3081  # image = (x - mean) / std\n    return image, label\n\n\ndef get_data_from_tfrecord(filename, batch_size: int):\n    """"""Construct a TFRecordDataset iterator.""""""\n    return (\n        tf.data.TFRecordDataset([filename])\n        .map(decode)\n        .map(normalize)\n        .repeat()\n        .batch(batch_size)\n        .make_one_shot_iterator()\n    )\n'"
examples/mnist/download.py,1,"b'""""""Downloading the MNIST dataset and storing as TFRecords.""""""\n\nimport os\n\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\n\nfrom convert import encode\n\n\ndef save_training_data(images, labels, filename):\n    """"""Convert Keras MNIST data into TFRecords.""""""\n    assert images.shape[0] == labels.shape[0]\n    num_examples = images.shape[0]\n\n    with tf.python_io.TFRecordWriter(filename) as writer:\n\n        for index in range(num_examples):\n\n            image = images[index]\n            label = labels[index]\n            example = encode(image, label)\n            writer.write(example.SerializeToString())\n\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\ndata_dir = os.path.expanduser(""./data/"")\nif not os.path.exists(data_dir):\n    os.makedirs(data_dir)\n\nsave_training_data(x_train, y_train, os.path.join(data_dir, ""train.tfrecord""))\nsave_training_data(x_test, y_test, os.path.join(data_dir, ""test.tfrecord""))\n'"
examples/mnist/run.py,31,"b'# pylint:  disable=redefined-outer-name\n""""""An example of performing secure inference with MNIST.\n\nAlso performs plaintext training.\n""""""\n\nimport logging\nimport sys\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\n\nimport tf_encrypted as tfe\nfrom convert import decode\n\nif len(sys.argv) > 1:\n    # config file was specified\n    config_file = sys.argv[1]\n    config = tfe.RemoteConfig.load(config_file)\n    tfe.set_config(config)\n    tfe.set_protocol(tfe.protocol.Pond())\n\nsession_target = sys.argv[2] if len(sys.argv) > 2 else None\n\n\nclass ModelOwner:\n    """"""Contains code meant to be executed by the model owner.\n\n  Args:\n    player_name: `str`, name of the `tfe.player.Player`\n                 representing the model owner.\n    local_data_file: filepath to MNIST data.\n  """"""\n\n    BATCH_SIZE = 128\n    NUM_CLASSES = 10\n    EPOCHS = 1\n\n    ITERATIONS = 60000 // BATCH_SIZE\n\n    IMG_ROWS = 28\n    IMG_COLS = 28\n    FLATTENED_DIM = IMG_ROWS * IMG_COLS\n\n    def __init__(self, player_name, local_data_file):\n        self.player_name = player_name\n        self.local_data_file = local_data_file\n\n    def _build_data_pipeline(self):\n        """"""Build a reproducible tf.data iterator.""""""\n\n        def normalize(image, label):\n            image = tf.cast(image, tf.float32) / 255.0\n            return image, label\n\n        def flatten(image, label):\n            image = tf.reshape(image, shape=[self.FLATTENED_DIM])\n            return image, label\n\n        dataset = tf.data.TFRecordDataset([self.local_data_file])\n        dataset = dataset.map(decode)\n        dataset = dataset.map(normalize)\n        dataset = dataset.map(flatten)\n        dataset = dataset.repeat()\n        dataset = dataset.batch(self.BATCH_SIZE)\n\n        iterator = dataset.make_one_shot_iterator()\n        return iterator\n\n    def _build_training_graph(self, training_data):\n        """"""Build a graph for plaintext model training.""""""\n\n        model = keras.Sequential()\n        model.add(keras.layers.Dense(512, input_shape=[self.FLATTENED_DIM]))\n        model.add(keras.layers.Activation(""relu""))\n        model.add(keras.layers.Dense(self.NUM_CLASSES, activation=None))\n\n        # optimizer and data pipeline\n        optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n\n        def loss(model, inputs, targets):\n            logits = model(inputs)\n            per_element_loss = tf.losses.sparse_softmax_cross_entropy(\n                labels=targets, logits=logits\n            )\n            return tf.reduce_mean(per_element_loss)\n\n        def grad(model, inputs, targets):\n            loss_value = loss(model, inputs, targets)\n            return loss_value, tf.gradients(loss_value, model.trainable_variables)\n\n        def loop_body(i):\n            x, y = training_data.get_next()\n            _, grads = grad(model, x, y)\n            update_op = optimizer.apply_gradients(zip(grads, model.trainable_variables))\n            with tf.control_dependencies([update_op]):\n                return i + 1\n\n        loop = tf.while_loop(\n            lambda i: i < self.ITERATIONS * self.EPOCHS, loop_body, loop_vars=(0,)\n        )\n\n        with tf.control_dependencies([loop]):\n            print_op = tf.print(""Training complete"")\n        with tf.control_dependencies([print_op]):\n            return [tf.identity(x) for x in model.trainable_variables]\n\n    @tfe.local_computation\n    def provide_weights(self):\n        with tf.name_scope(""loading""):\n            training_data = self._build_data_pipeline()\n\n        with tf.name_scope(""training""):\n            parameters = self._build_training_graph(training_data)\n\n        return parameters\n\n\nclass PredictionClient:\n    """"""\n  Contains code meant to be executed by a prediction client.\n\n  Args:\n    player_name: `str`, name of the `tfe.player.Player`\n                 representing the data owner\n    build_update_step: `Callable`, the function used to construct\n                       a local federated learning update.\n  """"""\n\n    BATCH_SIZE = 20\n\n    def __init__(self, player_name, local_data_file):\n        self.player_name = player_name\n        self.local_data_file = local_data_file\n\n    def _build_data_pipeline(self):\n        """"""Build a reproducible tf.data iterator.""""""\n\n        def normalize(image, label):\n            image = tf.cast(image, tf.float32) / 255.0\n            return image, label\n\n        dataset = tf.data.TFRecordDataset([self.local_data_file])\n        dataset = dataset.map(decode)\n        dataset = dataset.map(normalize)\n        dataset = dataset.repeat()\n        dataset = dataset.batch(self.BATCH_SIZE)\n\n        iterator = dataset.make_one_shot_iterator()\n        return iterator\n\n    @tfe.local_computation\n    def provide_input(self) -> tf.Tensor:\n        """"""Prepare input data for prediction.""""""\n        with tf.name_scope(""loading""):\n            prediction_input, expected_result = self._build_data_pipeline().get_next()\n            print_op = tf.print(""Expect"", expected_result, summarize=self.BATCH_SIZE)\n            with tf.control_dependencies([print_op]):\n                prediction_input = tf.identity(prediction_input)\n\n        with tf.name_scope(""pre-processing""):\n            prediction_input = tf.reshape(\n                prediction_input, shape=(self.BATCH_SIZE, ModelOwner.FLATTENED_DIM)\n            )\n        return prediction_input\n\n    @tfe.local_computation\n    def receive_output(self, logits: tf.Tensor) -> tf.Operation:\n        with tf.name_scope(""post-processing""):\n            prediction = tf.argmax(logits, axis=1)\n            op = tf.print(""Result"", prediction, summarize=self.BATCH_SIZE)\n            return op\n\n\nif __name__ == ""__main__"":\n\n    logging.basicConfig(level=logging.DEBUG)\n\n    model_owner = ModelOwner(\n        player_name=""model-owner"", local_data_file=""./data/train.tfrecord""\n    )\n\n    prediction_client = PredictionClient(\n        player_name=""prediction-client"", local_data_file=""./data/test.tfrecord""\n    )\n\n    # get model parameters as private tensors from model owner\n    params = model_owner.provide_weights()\n\n    # we\'ll use the same parameters for each prediction so we cache them to\n    # avoid re-training each time\n    cache_updater, params = tfe.cache(params)\n\n    with tfe.protocol.SecureNN():\n        # get prediction input from client\n        x = prediction_client.provide_input()\n\n        model = tfe.keras.Sequential()\n        model.add(tfe.keras.layers.Dense(512, batch_input_shape=x.shape))\n        model.add(tfe.keras.layers.Activation(""relu""))\n        model.add(tfe.keras.layers.Dense(10, activation=None))\n\n        logits = model(x)\n\n    # send prediction output back to client\n    prediction_op = prediction_client.receive_output(logits)\n\n    with tfe.Session(target=session_target) as sess:\n        sess.run(tf.global_variables_initializer(), tag=""init"")\n\n        print(""Training"")\n        sess.run(cache_updater, tag=""training"")\n\n        print(""Set trained weights"")\n        model.set_weights(params, sess)\n\n        for _ in range(5):\n            print(""Predicting"")\n            sess.run(prediction_op, tag=""prediction"")\n'"
examples/notebooks/utils.py,1,"b'""""""\nVarious helpers that make using TensorFlow and TF Encrypted in notebooks\neasier.\n""""""\n\nimport tensorflow as tf\n\n\ndef print_in_notebook(x):\n    return tf.py_func(print, [x], Tout=[])\n'"
examples/securenn/conv_convert.py,19,"b'""""""Data processing helpers.""""""\n#\n# Based on:\n# - https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py  # noqa\n# - https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py  # noqa\n#\nfrom functools import partial\n\nimport tensorflow as tf\n\n\ndef encode_image(value):\n    """"""Encode images into a tf.train.Feature for a TFRecord.""""""\n    bytes_list = tf.train.BytesList(value=[value.tostring()])\n    return tf.train.Feature(bytes_list=bytes_list)\n\n\ndef decode_image(value, flattened):\n    """"""Decode the image from a tf.train.Feature in a TFRecord.""""""\n    image = tf.decode_raw(value, tf.uint8)\n    if not flattened:\n        image = tf.reshape(image, (1, 28, 28))\n    return image\n\n\ndef encode_label(value):\n    """"""Encode a label into a tf.train.Feature for a TFRecord.""""""\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef decode_label(value):\n    """"""Decode the label from a tf.train.Feature in a TFRecord.""""""\n    return tf.cast(value, tf.int32)\n\n\ndef encode(image, label):\n    """"""Encode an instance as a tf.train.Example for a TFRecord.""""""\n    feature_dict = {""image"": encode_image(image), ""label"": encode_label(label)}\n    features = tf.train.Features(feature=feature_dict)\n    return tf.train.Example(features=features)\n\n\ndef decode(serialized_example, flattened):\n    """"""Decode an instance from a tf.train.Example in a TFRecord.""""""\n    features = tf.parse_single_example(\n        serialized_example,\n        features={\n            ""image"": tf.FixedLenFeature([], tf.string),\n            ""label"": tf.FixedLenFeature([], tf.int64),\n        },\n    )\n    image = decode_image(features[""image""], flattened)\n    label = decode_label(features[""label""])\n    return image, label\n\n\ndef normalize(image, label):\n    """"""Standardization of MNIST images.""""""\n    x = tf.cast(image, tf.float32) / 255.0\n    image = (x - 0.1307) / 0.3081  # image = (x - mean) / std\n    return image, label\n\n\ndef get_data_from_tfrecord(filename, batch_size: int, flattened=False):\n    """"""Construct a TFRecordDataset iterator.""""""\n    decoder = partial(decode, flattened=flattened)\n    return (\n        tf.data.TFRecordDataset([filename])\n        .map(decoder)\n        .map(normalize)\n        .repeat()\n        .batch(batch_size)\n        .make_one_shot_iterator()\n    )\n'"
examples/securenn/download.py,1,"b'""""""Downloading the MNIST dataset and storing as TFRecords.""""""\n\nimport os\n\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\n\nfrom conv_convert import encode\n\n\ndef save_training_data(images, labels, filename):\n    """"""Convert Keras MNIST data into TFRecords.""""""\n    assert images.shape[0] == labels.shape[0]\n    num_examples = images.shape[0]\n\n    with tf.python_io.TFRecordWriter(filename) as writer:\n\n        for index in range(num_examples):\n\n            image = images[index]\n            label = labels[index]\n            example = encode(image, label)\n            writer.write(example.SerializeToString())\n\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\ndata_dir = os.path.expanduser(""./data/"")\nif not os.path.exists(data_dir):\n    os.makedirs(data_dir)\n\nsave_training_data(x_train, y_train, os.path.join(data_dir, ""train.tfrecord""))\nsave_training_data(x_test, y_test, os.path.join(data_dir, ""test.tfrecord""))\n'"
examples/securenn/network_a.py,38,"b'# pylint:  disable=redefined-outer-name\n""""""An example of performing secure inference with MNIST.\n\nReproduces Network A from SecureNN, Wagh et al.\n""""""\nfrom __future__ import absolute_import\n\nimport sys\nfrom typing import List\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\n\nimport tf_encrypted as tfe\nfrom conv_convert import get_data_from_tfrecord\n\n# tfe.set_tfe_events_flag(True)\n\nif len(sys.argv) >= 2:\n    # config file was specified\n    config_file = sys.argv[1]\n    config = tfe.config.load(config_file)\nelse:\n    # default to using local config\n    config = tfe.LocalConfig(\n        [""server0"", ""server1"", ""crypto-producer"", ""model-trainer"", ""prediction-client""]\n    )\ntfe.set_config(config)\nplayers = [""server0"", ""server1"", ""crypto-producer""]\nprot = tfe.protocol.SecureNN(*tfe.get_config().get_players(players))\ntfe.set_protocol(prot)\nsession_target = sys.argv[2] if len(sys.argv) > 2 else None\n\n\nclass ModelTrainer:\n    """"""Contains code meant to be executed by a model training Player.""""""\n\n    BATCH_SIZE = 256\n    ITERATIONS = 60000 // BATCH_SIZE\n    EPOCHS = 3\n    LEARNING_RATE = 3e-3\n    IN_N = 28 * 28\n    HIDDEN_N = 128\n    OUT_N = 10\n\n    def cond(\n        self, i: int, max_iter: tf.Tensor, nb_epochs: tf.Tensor, avg_loss: tf.Tensor\n    ):\n        """"""Check if training termination condition has been met.""""""\n        is_end_epoch = tf.equal(i % max_iter, 0)\n        to_continue = tf.cast(i < max_iter * nb_epochs, tf.bool)\n\n        def true_fn() -> tf.Tensor:\n            to_continue = tf.print(""avg_loss: "", avg_loss)\n            return to_continue\n\n        def false_fn() -> tf.Tensor:\n            return to_continue\n\n        return tf.cond(is_end_epoch, true_fn, false_fn)\n\n    def build_training_graph(self, training_data) -> List[tf.Tensor]:\n        """"""Build a graph for plaintext model training.\n\n    Returns a list of the trained model\'s parameters.\n    """"""\n        j = self.IN_N\n        k = self.HIDDEN_N\n        m = self.OUT_N\n\n        # model parameters and initial values\n        model = keras.Sequential()\n        model.add(keras.layers.Dense(k, input_shape=[j]))\n        model.add(keras.layers.Activation(""relu""))\n        model.add(keras.layers.Dense(k))\n        model.add(keras.layers.Activation(""relu""))\n        model.add(keras.layers.Dense(m))\n\n        # optimizer and data pipeline\n        optimizer = tf.train.AdamOptimizer(learning_rate=self.LEARNING_RATE)\n\n        def loss(model, inputs, targets):\n            logits = model(inputs)\n            per_element_loss = tf.losses.sparse_softmax_cross_entropy(\n                labels=targets, logits=logits\n            )\n            return tf.reduce_mean(per_element_loss)\n\n        def grad(model, inputs, targets):\n            loss_value = loss(model, inputs, targets)\n            return loss_value, tf.gradients(loss_value, model.trainable_variables)\n\n        # training loop\n        def loop_body(i, max_iter, nb_epochs, avg_loss):\n            x, y = training_data.get_next()\n            loss, grads = grad(model, x, y)\n            update_op = optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n            is_end_epoch = tf.equal(i % max_iter, 0)\n\n            def true_fn() -> tf.Tensor:\n                return loss\n\n            def false_fn() -> tf.Tensor:\n                prev_loss = tf.cast(i - 1, tf.float32) * avg_loss\n                return (prev_loss + loss) / tf.cast(i, tf.float32)\n\n            with tf.control_dependencies([update_op]):\n                terminal_cond = tf.cond(is_end_epoch, true_fn, false_fn)\n                return i + 1, max_iter, nb_epochs, terminal_cond\n\n        loop, _, _, _ = tf.while_loop(\n            self.cond, loop_body, [0, self.ITERATIONS, self.EPOCHS, 0.0]\n        )\n\n        # return model parameters after training\n        loop = tf.print(""Training complete"", loop)\n\n        with tf.control_dependencies([loop]):\n            return [tf.identity(x) for x in model.trainable_variables]\n\n    def provide_input(self) -> List[tf.Tensor]:\n        with tf.name_scope(""loading""):\n            training_data = get_data_from_tfrecord(\n                ""./data/train.tfrecord"", self.BATCH_SIZE, flattened=True\n            )\n\n        with tf.name_scope(""training""):\n            parameters = self.build_training_graph(training_data)\n\n        return parameters\n\n\nclass PredictionClient:\n    """"""Contains methods meant to be executed by a prediction client.\n\n  Args:\n    player_name: `str`, name of the `tfe.player.Player`\n                 representing the data owner\n    build_update_step: `Callable`, the function used to construct\n                       a local federated learning update.\n  """"""\n\n    BATCH_SIZE = 20\n\n    def provide_input(self) -> List[tf.Tensor]:\n        """"""Prepare input data for prediction.""""""\n        with tf.name_scope(""loading""):\n            prediction_input, expected_result = get_data_from_tfrecord(\n                ""./data/test.tfrecord"", self.BATCH_SIZE, flattened=True\n            ).get_next()\n\n        with tf.name_scope(""pre-processing""):\n            prediction_input = tf.reshape(\n                prediction_input, shape=(self.BATCH_SIZE, ModelTrainer.IN_N)\n            )\n            expected_result = tf.reshape(expected_result, shape=(self.BATCH_SIZE,))\n\n        return [prediction_input, expected_result]\n\n    def receive_output(self, likelihoods: tf.Tensor, y_true: tf.Tensor):\n        with tf.name_scope(""post-processing""):\n            prediction = tf.argmax(likelihoods, axis=1)\n            eq_values = tf.equal(prediction, tf.cast(y_true, tf.int64))\n            acc = tf.reduce_mean(tf.cast(eq_values, tf.float32))\n            op = tf.print(\n                ""Expected:"", y_true, ""\\nActual:"", prediction, ""\\nAccuracy:"", acc\n            )\n\n            return op\n\n\nif __name__ == ""__main__"":\n    model_trainer = ModelTrainer()\n    prediction_client = PredictionClient()\n\n    # get model parameters as private tensors from model owner\n    params = tfe.define_private_input(\n        ""model-trainer"", model_trainer.provide_input\n    )  # pylint: disable=E0632\n\n    # we\'ll use the same parameters for each prediction so we cache them to\n    # avoid re-training each time\n    cache_updater, params = tfe.cache(params)\n\n    # get prediction input from client\n    x, y = tfe.define_private_input(\n        ""prediction-client"", prediction_client.provide_input\n    )  # pylint: disable=E0632\n\n    with tfe.protocol.SecureNN():\n        batch_size = PredictionClient.BATCH_SIZE\n        flat_dim = ModelTrainer.IN_N\n        batch_input_shape = [batch_size, flat_dim]\n        # compute prediction\n        model = tfe.keras.Sequential()\n        model.add(\n            tfe.keras.layers.Dense(\n                ModelTrainer.HIDDEN_N, batch_input_shape=batch_input_shape\n            )\n        )\n        model.add(tfe.keras.layers.Activation(""relu""))\n        model.add(tfe.keras.layers.Dense(ModelTrainer.HIDDEN_N))\n        model.add(tfe.keras.layers.Activation(""relu""))\n        model.add(tfe.keras.layers.Dense(ModelTrainer.OUT_N))\n\n        logits = model(x)\n\n    # send prediction output back to client\n    prediction_op = tfe.define_output(\n        ""prediction-client"", [logits, y], prediction_client.receive_output\n    )\n\n    sess = tfe.Session(target=session_target)\n    sess.run(tf.global_variables_initializer(), tag=""init"")\n\n    print(""Training"")\n    sess.run(cache_updater, tag=""training"")\n\n    print(""Set trained weights"")\n    model.set_weights(params, sess)\n\n    for _ in range(5):\n        print(""Predicting"")\n        sess.run(prediction_op, tag=""prediction"")\n\n    sess.close()\n'"
examples/securenn/network_b.py,44,"b'# pylint:  disable=redefined-outer-name\n""""""An example of performing secure inference with MNIST.\n\nReproduces Network B from SecureNN, Wagh et al.\n""""""\nfrom __future__ import absolute_import\n\nimport sys\nfrom typing import List\nfrom typing import Tuple\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\n\nimport tf_encrypted as tfe\nfrom conv_convert import get_data_from_tfrecord\n\n# tfe.set_tfe_events_flag(True)\n\nif len(sys.argv) >= 2:\n    # config file was specified\n    config_file = sys.argv[1]\n    config = tfe.config.load(config_file)\nelse:\n    # default to using local config\n    config = tfe.LocalConfig(\n        [""server0"", ""server1"", ""crypto-producer"", ""model-trainer"", ""prediction-client""]\n    )\ntfe.set_config(config)\ntfe.set_protocol(\n    tfe.protocol.SecureNN(\n        *tfe.get_config().get_players([""server0"", ""server1"", ""crypto-producer""])\n    )\n)\n\nsession_target = sys.argv[2] if len(sys.argv) > 2 else None\n\n\nclass ModelTrainer:\n    """"""Contains code meant to be executed by a model training Player.""""""\n\n    BATCH_SIZE = 256\n    ITERATIONS = 60000 // BATCH_SIZE\n    EPOCHS = 2\n    LEARNING_RATE = 3e-3\n    IN_DIM = 28\n    KERNEL = 5\n    STRIDE = 1\n    IN_CHANNELS = 1\n    HIDDEN_CHANNELS = 16\n    HIDDEN_FC1 = 256\n    HIDDEN_FC2 = 100\n    OUT_N = 10\n\n    def cond(\n        self,\n        i: tf.Tensor,\n        max_iter: tf.Tensor,\n        nb_epochs: tf.Tensor,\n        avg_loss: tf.Tensor,\n    ):\n        """"""Check if training termination condition has been met.""""""\n        is_end_epoch = tf.equal(i % max_iter, 0)\n        to_continue = tf.cast(i < max_iter * nb_epochs, tf.bool)\n\n        def true_fn() -> tf.Tensor:\n            to_continue = tf.print(""avg_loss: "", avg_loss)\n            return to_continue\n\n        def false_fn() -> tf.Tensor:\n            return to_continue\n\n        return tf.cond(is_end_epoch, true_fn, false_fn)\n\n    def build_training_graph(self, training_data) -> List[tf.Tensor]:\n        """"""Build a graph for plaintext model training.\n\n    Returns a list of the trained model\'s parameters.\n    """"""\n        model = keras.Sequential()\n        model.add(\n            keras.layers.Conv2D(\n                self.HIDDEN_CHANNELS,\n                (self.KERNEL, self.KERNEL),\n                batch_input_shape=(\n                    self.BATCH_SIZE,\n                    self.IN_DIM,\n                    self.IN_DIM,\n                    self.IN_CHANNELS,\n                ),\n            )\n        )\n        model.add(keras.layers.Activation(""relu""))\n        model.add(keras.layers.AveragePooling2D())\n        model.add(keras.layers.Conv2D(self.HIDDEN_CHANNELS, (self.KERNEL, self.KERNEL)))\n        model.add(keras.layers.Activation(""relu""))\n        model.add(keras.layers.AveragePooling2D())\n        model.add(keras.layers.Flatten())\n        model.add(keras.layers.Dense(self.HIDDEN_FC2))\n        model.add(keras.layers.Activation(""relu""))\n        model.add(keras.layers.Dense(self.OUT_N))\n\n        # optimizer and data pipeline\n        optimizer = tf.train.AdamOptimizer(learning_rate=self.LEARNING_RATE)\n\n        def loss(model, inputs, targets):\n            logits = model(inputs)\n            per_element_loss = tf.losses.sparse_softmax_cross_entropy(\n                labels=targets, logits=logits\n            )\n            return tf.reduce_mean(per_element_loss)\n\n        def grad(model, inputs, targets):\n            loss_value = loss(model, inputs, targets)\n            return loss_value, tf.gradients(loss_value, model.trainable_variables)\n\n        # training loop\n        def loop_body(\n            i: tf.Tensor, max_iter: tf.Tensor, nb_epochs: tf.Tensor, avg_loss: tf.Tensor\n        ) -> Tuple[tf.Tensor, tf.Tensor]:\n            """"""Main model training loop.""""""\n            # get next batch\n            x, y = training_data.get_next()\n            x = tf.reshape(x, [-1, self.IN_DIM, self.IN_DIM, 1])\n            loss, grads = grad(model, x, y)\n            update_op = optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n            is_end_epoch = tf.equal(i % max_iter, 0)\n\n            def true_fn() -> tf.Tensor:\n                return loss\n\n            def false_fn() -> tf.Tensor:\n                prev_loss = tf.cast(i - 1, tf.float32) * avg_loss\n                return (prev_loss + loss) / tf.cast(i, tf.float32)\n\n            with tf.control_dependencies([update_op]):\n                terminal_cond = tf.cond(is_end_epoch, true_fn, false_fn)\n                return i + 1, max_iter, nb_epochs, terminal_cond\n\n        loop, _, _, _ = tf.while_loop(\n            self.cond, loop_body, [0, self.ITERATIONS, self.EPOCHS, 0.0]\n        )\n\n        # return model parameters after training\n        loop = tf.print(""Training complete"", loop)\n\n        with tf.control_dependencies([loop]):\n            return [tf.identity(x) for x in model.trainable_variables]\n\n    def provide_input(self) -> List[tf.Tensor]:\n        with tf.name_scope(""loading""):\n            training_data = get_data_from_tfrecord(\n                ""./data/train.tfrecord"", self.BATCH_SIZE\n            )\n\n        with tf.name_scope(""training""):\n            parameters = self.build_training_graph(training_data)\n\n        return parameters\n\n\nclass PredictionClient:\n    """"""Contains methods meant to be executed by a prediction client.\n\n  Args:\n    player_name: `str`, name of the `tfe.player.Player`\n                  representing the data owner\n    build_update_step: `Callable`, the function used to\n                        construct a local federated learning update.\n  """"""\n\n    BATCH_SIZE = 20\n    HIDDEN_FC1 = 256\n\n    def provide_input(self) -> List[tf.Tensor]:\n        """"""Prepare input data for prediction.""""""\n        with tf.name_scope(""loading""):\n            prediction_input, expected_result = get_data_from_tfrecord(\n                ""./data/test.tfrecord"", self.BATCH_SIZE\n            ).get_next()\n\n        with tf.name_scope(""pre-processing""):\n            prediction_input = tf.reshape(\n                prediction_input,\n                shape=(self.BATCH_SIZE, ModelTrainer.IN_DIM, ModelTrainer.IN_DIM, 1),\n            )\n            expected_result = tf.reshape(expected_result, shape=(self.BATCH_SIZE,))\n\n        return [prediction_input, expected_result]\n\n    def receive_output(self, likelihoods: tf.Tensor, y_true: tf.Tensor):\n        with tf.name_scope(""post-processing""):\n            prediction = tf.argmax(likelihoods, axis=1)\n            eq_values = tf.equal(prediction, tf.cast(y_true, tf.int64))\n            acc = tf.reduce_mean(tf.cast(eq_values, tf.float32))\n            op = tf.print(\n                ""Expected:"", y_true, ""\\nActual:"", prediction, ""\\nAccuracy:"", acc\n            )\n\n            return op\n\n\nif __name__ == ""__main__"":\n    model_trainer = ModelTrainer()\n    prediction_client = PredictionClient()\n\n    # get model parameters as private tensors from model owner\n    params = tfe.define_private_input(\n        ""model-trainer"", model_trainer.provide_input\n    )  # pylint: disable=E0632\n\n    # we\'ll use the same parameters for each prediction so we cache them to\n    # avoid re-training each time\n    cache_updater, params = tfe.cache(params)\n\n    # get prediction input from client\n    x, y = tfe.define_private_input(\n        ""prediction-client"", prediction_client.provide_input\n    )  # pylint: disable=E0632\n\n    with tfe.protocol.SecureNN():\n        model = tfe.keras.Sequential()\n        model.add(\n            tfe.keras.layers.Conv2D(\n                ModelTrainer.HIDDEN_CHANNELS,\n                (ModelTrainer.KERNEL, ModelTrainer.KERNEL),\n                batch_input_shape=(\n                    PredictionClient.BATCH_SIZE,\n                    ModelTrainer.IN_DIM,\n                    ModelTrainer.IN_DIM,\n                    ModelTrainer.IN_CHANNELS,\n                ),\n            )\n        )\n        model.add(tfe.keras.layers.Activation(""relu""))\n        model.add(tfe.keras.layers.AveragePooling2D())\n        model.add(\n            tfe.keras.layers.Conv2D(\n                ModelTrainer.HIDDEN_CHANNELS, (ModelTrainer.KERNEL, ModelTrainer.KERNEL)\n            )\n        )\n        model.add(tfe.keras.layers.Activation(""relu""))\n        model.add(tfe.keras.layers.AveragePooling2D())\n        model.add(tfe.keras.layers.Flatten())\n        model.add(tfe.keras.layers.Dense(ModelTrainer.HIDDEN_FC2))\n        model.add(tfe.keras.layers.Activation(""relu""))\n        model.add(tfe.keras.layers.Dense(ModelTrainer.OUT_N))\n\n        logits = model(x)\n\n    # send prediction output back to client\n    prediction_op = tfe.define_output(\n        ""prediction-client"", [logits, y], prediction_client.receive_output\n    )\n\n    sess = tfe.Session(target=session_target)\n    sess.run(tf.global_variables_initializer(), tag=""init"")\n\n    print(""Training"")\n    sess.run(cache_updater, tag=""training"")\n\n    print(""Set trained weights"")\n    model.set_weights(params, sess)\n\n    for _ in range(5):\n        print(""Predicting"")\n        sess.run(prediction_op, tag=""prediction"")\n\n    sess.close()\n'"
examples/securenn/network_c.py,44,"b'# pylint:  disable=redefined-outer-name\n""""""An example of performing secure inference with MNIST.\n\nReproduces Network C from SecureNN, Wagh et al.\n""""""\nfrom __future__ import absolute_import\n\nimport sys\nfrom typing import List\nfrom typing import Tuple\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\n\nimport tf_encrypted as tfe\nfrom conv_convert import get_data_from_tfrecord\n\nif len(sys.argv) >= 2:\n    # config file was specified\n    config_file = sys.argv[1]\n    config = tfe.config.load(config_file)\nelse:\n    # default to using local config\n    config = tfe.LocalConfig(\n        [""server0"", ""server1"", ""crypto-producer"", ""model-trainer"", ""prediction-client""]\n    )\ntfe.set_config(config)\ntfe.set_protocol(\n    tfe.protocol.SecureNN(\n        *tfe.get_config().get_players([""server0"", ""server1"", ""crypto-producer""])\n    )\n)\n\nsession_target = sys.argv[2] if len(sys.argv) > 2 else None\n\n\nclass ModelTrainer:\n    """"""Contains code meant to be executed by a model training Player.""""""\n\n    BATCH_SIZE = 256\n    ITERATIONS = 60000 // BATCH_SIZE\n    EPOCHS = 3\n    LEARNING_RATE = 3e-3\n    IN_DIM = 28\n    KERNEL = 5\n    STRIDE = 1\n    IN_CHANNELS = 1\n    HIDDEN_C1 = 20\n    HIDDEN_C2 = 50\n    HIDDEN_FC1 = 800\n    HIDDEN_FC2 = 500\n    OUT_N = 10\n\n    def cond(\n        self,\n        i: tf.Tensor,\n        max_iter: tf.Tensor,\n        nb_epochs: tf.Tensor,\n        avg_loss: tf.Tensor,\n    ):\n        """"""Check if training termination condition has been met.""""""\n        is_end_epoch = tf.equal(i % max_iter, 0)\n        to_continue = tf.cast(i < max_iter * nb_epochs, tf.bool)\n\n        def true_fn() -> tf.Tensor:\n            to_continue = tf.print(""avg_loss: "", avg_loss)\n            return to_continue\n\n        def false_fn() -> tf.Tensor:\n            return to_continue\n\n        return tf.cond(is_end_epoch, true_fn, false_fn)\n\n    def build_training_graph(self, training_data) -> List[tf.Tensor]:\n        """"""Build a graph for plaintext model training.\n\n    Returns a list of the trained model\'s parameters.\n    """"""\n        # model parameters and initial values\n        model = keras.Sequential()\n        model.add(\n            keras.layers.Conv2D(\n                self.HIDDEN_C1,\n                (self.KERNEL, self.KERNEL),\n                batch_input_shape=(\n                    self.BATCH_SIZE,\n                    self.IN_DIM,\n                    self.IN_DIM,\n                    self.IN_CHANNELS,\n                ),\n            )\n        )\n        model.add(keras.layers.Activation(""relu""))\n        model.add(keras.layers.AveragePooling2D())\n        model.add(keras.layers.Conv2D(self.HIDDEN_C2, (self.KERNEL, self.KERNEL)))\n        model.add(keras.layers.Activation(""relu""))\n        model.add(keras.layers.AveragePooling2D())\n        model.add(keras.layers.Flatten())\n        model.add(keras.layers.Dense(self.HIDDEN_FC2))\n        model.add(keras.layers.Activation(""relu""))\n        model.add(keras.layers.Dense(self.OUT_N))\n\n        # optimizer and data pipeline\n        optimizer = tf.train.AdamOptimizer(learning_rate=self.LEARNING_RATE)\n\n        def loss(model, inputs, targets):\n            logits = model(inputs)\n            per_element_loss = tf.losses.sparse_softmax_cross_entropy(\n                labels=targets, logits=logits\n            )\n            return tf.reduce_mean(per_element_loss)\n\n        def grad(model, inputs, targets):\n            loss_value = loss(model, inputs, targets)\n            return loss_value, tf.gradients(loss_value, model.trainable_variables)\n\n        # training loop\n        def loop_body(\n            i: tf.Tensor, max_iter: tf.Tensor, nb_epochs: tf.Tensor, avg_loss: tf.Tensor\n        ) -> Tuple[tf.Tensor, tf.Tensor]:\n            """"""Main model training loop.""""""\n            # get next batch\n            x, y = training_data.get_next()\n            x = tf.reshape(x, [-1, self.IN_DIM, self.IN_DIM, 1])\n            loss, grads = grad(model, x, y)\n            update_op = optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n            is_end_epoch = tf.equal(i % max_iter, 0)\n\n            def true_fn() -> tf.Tensor:\n                return loss\n\n            def false_fn() -> tf.Tensor:\n                prev_loss = tf.cast(i - 1, tf.float32) * avg_loss\n                return (prev_loss + loss) / tf.cast(i, tf.float32)\n\n            with tf.control_dependencies([update_op]):\n                terminal_cond = tf.cond(is_end_epoch, true_fn, false_fn)\n                return i + 1, max_iter, nb_epochs, terminal_cond\n\n        loop, _, _, _ = tf.while_loop(\n            self.cond, loop_body, [0, self.ITERATIONS, self.EPOCHS, 0.0]\n        )\n\n        # return model parameters after training\n        loop = tf.print(""Training complete"", loop)\n\n        with tf.control_dependencies([loop]):\n            return [tf.identity(x) for x in model.trainable_variables]\n\n    def provide_input(self) -> List[tf.Tensor]:\n        with tf.name_scope(""loading""):\n            training_data = get_data_from_tfrecord(\n                ""./data/train.tfrecord"", self.BATCH_SIZE\n            )\n\n        with tf.name_scope(""training""):\n            parameters = self.build_training_graph(training_data)\n\n        return parameters\n\n\nclass PredictionClient:\n    """"""Contains methods meant to be executed by a prediction client.\n\n  Args:\n    player_name: `str`, name of the `tfe.player.Player`\n                  representing the data owner\n    build_update_step: `Callable`, the function used to construct\n                        a local federated learning update.\n  """"""\n\n    BATCH_SIZE = 20\n\n    def provide_input(self) -> List[tf.Tensor]:\n        """"""Prepare input data for prediction.""""""\n        with tf.name_scope(""loading""):\n            prediction_input, expected_result = get_data_from_tfrecord(\n                ""./data/test.tfrecord"", self.BATCH_SIZE\n            ).get_next()\n\n        with tf.name_scope(""pre-processing""):\n            prediction_input = tf.reshape(\n                prediction_input,\n                shape=(self.BATCH_SIZE, ModelTrainer.IN_DIM, ModelTrainer.IN_DIM, 1),\n            )\n            expected_result = tf.reshape(expected_result, shape=(self.BATCH_SIZE,))\n\n        return [prediction_input, expected_result]\n\n    def receive_output(self, likelihoods: tf.Tensor, y_true: tf.Tensor):\n        with tf.name_scope(""post-processing""):\n            prediction = tf.argmax(likelihoods, axis=1)\n            eq_values = tf.equal(prediction, tf.cast(y_true, tf.int64))\n            acc = tf.reduce_mean(tf.cast(eq_values, tf.float32))\n            op = tf.print(\n                ""Expected:"", y_true, ""\\nActual:"", prediction, ""\\nAccuracy:"", acc\n            )\n\n            return op\n\n\nif __name__ == ""__main__"":\n    model_trainer = ModelTrainer()\n    prediction_client = PredictionClient()\n\n    # get model parameters as private tensors from model owner\n    params = tfe.define_private_input(\n        ""model-trainer"", model_trainer.provide_input\n    )  # pylint: disable=E0632\n\n    # we\'ll use the same parameters for each prediction so we cache them to\n    # avoid re-training each time\n    cache_updater, params = tfe.cache(params)\n\n    # get prediction input from client\n    x, y = tfe.define_private_input(\n        ""prediction-client"", prediction_client.provide_input\n    )  # pylint: disable=E0632\n\n    with tfe.protocol.SecureNN():\n        model = tfe.keras.Sequential()\n        model.add(\n            tfe.keras.layers.Conv2D(\n                ModelTrainer.HIDDEN_C1,\n                (ModelTrainer.KERNEL, ModelTrainer.KERNEL),\n                batch_input_shape=(\n                    PredictionClient.BATCH_SIZE,\n                    ModelTrainer.IN_DIM,\n                    ModelTrainer.IN_DIM,\n                    ModelTrainer.IN_CHANNELS,\n                ),\n            )\n        )\n        model.add(tfe.keras.layers.Activation(""relu""))\n        model.add(tfe.keras.layers.AveragePooling2D())\n        model.add(\n            tfe.keras.layers.Conv2D(\n                ModelTrainer.HIDDEN_C2, (ModelTrainer.KERNEL, ModelTrainer.KERNEL)\n            )\n        )\n        model.add(tfe.keras.layers.Activation(""relu""))\n        model.add(tfe.keras.layers.AveragePooling2D())\n        model.add(tfe.keras.layers.Flatten())\n        model.add(tfe.keras.layers.Dense(ModelTrainer.HIDDEN_FC2))\n        model.add(tfe.keras.layers.Activation(""relu""))\n        model.add(tfe.keras.layers.Dense(ModelTrainer.OUT_N))\n\n        logits = model(x)\n\n    # send prediction output back to client\n    prediction_op = tfe.define_output(\n        ""prediction-client"", [logits, y], prediction_client.receive_output\n    )\n\n    sess = tfe.Session(target=session_target)\n    sess.run(tf.global_variables_initializer(), tag=""init"")\n\n    print(""Training"")\n    sess.run(cache_updater, tag=""training"")\n\n    print(""Set trained weights"")\n    model.set_weights(params, sess)\n\n    for _ in range(5):\n        print(""Predicting"")\n        sess.run(prediction_op, tag=""prediction"")\n\n    sess.close()\n'"
examples/securenn/network_d.py,44,"b'# pylint:  disable=redefined-outer-name\n""""""An example of performing secure inference with MNIST.\n\nReproduces Network D from SecureNN, Wagh et al.\n""""""\nfrom __future__ import absolute_import\n\nimport sys\nfrom typing import List\nfrom typing import Tuple\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\n\nimport tf_encrypted as tfe\nfrom conv_convert import get_data_from_tfrecord\n\n# tfe.set_tfe_events_flag(True)\n\nif len(sys.argv) >= 2:\n    # config file was specified\n    config_file = sys.argv[1]\n    config = tfe.config.load(config_file)\nelse:\n    # default to using local config\n    config = tfe.LocalConfig(\n        [""server0"", ""server1"", ""crypto-producer"", ""model-trainer"", ""prediction-client""]\n    )\ntfe.set_config(config)\ntfe.set_protocol(\n    tfe.protocol.SecureNN(\n        *tfe.get_config().get_players([""server0"", ""server1"", ""crypto-producer""])\n    )\n)\n\nsession_target = sys.argv[2] if len(sys.argv) > 2 else None\n\n\nclass ModelTrainer:\n    """"""Contains code meant to be executed by a model training Player.""""""\n\n    BATCH_SIZE = 256\n    ITERATIONS = 60000 // BATCH_SIZE\n    EPOCHS = 3\n    LEARNING_RATE = 3e-3\n    IN_DIM = 28\n    KERNEL = 5\n    STRIDE = 2\n    IN_CHANNELS = 1\n    HIDDEN_CHANNELS = 5\n    HIDDEN_FC1 = 180\n    HIDDEN_FC2 = 100\n    OUT_N = 10\n\n    def cond(\n        self,\n        i: tf.Tensor,\n        max_iter: tf.Tensor,\n        nb_epochs: tf.Tensor,\n        avg_loss: tf.Tensor,\n    ):\n        """"""Check if training termination condition has been met.""""""\n        is_end_epoch = tf.equal(i % max_iter, 0)\n        to_continue = tf.cast(i < max_iter * nb_epochs, tf.bool)\n\n        def true_fn() -> tf.Tensor:\n            to_continue = tf.print(""avg_loss: "", avg_loss)\n            return to_continue\n\n        def false_fn() -> tf.Tensor:\n            return to_continue\n\n        return tf.cond(is_end_epoch, true_fn, false_fn)\n\n    def build_training_graph(self, training_data) -> List[tf.Tensor]:\n        """"""Build a graph for plaintext model training.\n\n    Returns a list of the trained model\'s parameters.\n    """"""\n        # model parameters and initial values\n        model = keras.Sequential()\n        model.add(\n            keras.layers.Conv2D(\n                self.HIDDEN_CHANNELS,\n                (self.KERNEL, self.KERNEL),\n                batch_input_shape=(\n                    self.BATCH_SIZE,\n                    self.IN_DIM,\n                    self.IN_DIM,\n                    self.IN_CHANNELS,\n                ),\n            )\n        )\n        model.add(keras.layers.Activation(""relu""))\n        model.add(keras.layers.AveragePooling2D())\n        model.add(keras.layers.Flatten())\n        model.add(keras.layers.Dense(self.HIDDEN_FC2))\n        model.add(keras.layers.Activation(""relu""))\n        model.add(keras.layers.Dense(self.OUT_N))\n\n        # optimizer and data pipeline\n        optimizer = tf.train.AdamOptimizer(learning_rate=self.LEARNING_RATE)\n\n        def loss(model, inputs, targets):\n            logits = model(inputs)\n            per_element_loss = tf.losses.sparse_softmax_cross_entropy(\n                labels=targets, logits=logits\n            )\n            return tf.reduce_mean(per_element_loss)\n\n        def grad(model, inputs, targets):\n            loss_value = loss(model, inputs, targets)\n            return loss_value, tf.gradients(loss_value, model.trainable_variables)\n\n        # training loop\n        # training loop\n        def loop_body(\n            i: int, max_iter: tf.Tensor, nb_epochs: tf.Tensor, avg_loss: tf.Tensor\n        ) -> Tuple[tf.Tensor, tf.Tensor]:\n            """"""Main model training loop.""""""\n            # get next batch\n            x, y = training_data.get_next()\n            x = tf.reshape(x, [-1, self.IN_DIM, self.IN_DIM, 1])\n            loss, grads = grad(model, x, y)\n            update_op = optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n            is_end_epoch = tf.equal(i % max_iter, 0)\n\n            def true_fn() -> tf.Tensor:\n                return loss\n\n            def false_fn() -> tf.Tensor:\n                prev_loss = tf.cast(i - 1, tf.float32) * avg_loss\n                return (prev_loss + loss) / tf.cast(i, tf.float32)\n\n            with tf.control_dependencies([update_op]):\n                terminal_cond = tf.cond(is_end_epoch, true_fn, false_fn)\n                return i + 1, max_iter, nb_epochs, terminal_cond\n\n        loop, _, _, _ = tf.while_loop(\n            self.cond, loop_body, [0, self.ITERATIONS, self.EPOCHS, 0.0]\n        )\n\n        # return model parameters after training\n        loop = tf.print(""Training complete"", loop)\n\n        with tf.control_dependencies([loop]):\n            return [tf.identity(x) for x in model.trainable_variables]\n\n    def provide_input(self) -> List[tf.Tensor]:\n        with tf.name_scope(""loading""):\n            training_data = get_data_from_tfrecord(\n                ""./data/train.tfrecord"", self.BATCH_SIZE\n            )\n\n        with tf.name_scope(""training""):\n            parameters = self.build_training_graph(training_data)\n\n        return parameters\n\n\nclass PredictionClient:\n    """"""Contains methods meant to be executed by a prediction client.\n\n  Args:\n          player_name: `str`, name of the `tfe.player.Player`\n          build_update_step: `Callable`, the function used to construct\n                             a local federated learning update.\n  """"""\n\n    BATCH_SIZE = 20\n\n    def provide_input(self) -> List[tf.Tensor]:\n        """"""Prepare input data for prediction.""""""\n        with tf.name_scope(""loading""):\n            prediction_input, expected_result = get_data_from_tfrecord(\n                ""./data/test.tfrecord"", self.BATCH_SIZE\n            ).get_next()\n\n        with tf.name_scope(""pre-processing""):\n            prediction_input = tf.reshape(\n                prediction_input,\n                shape=(self.BATCH_SIZE, ModelTrainer.IN_DIM, ModelTrainer.IN_DIM, 1),\n            )\n            expected_result = tf.reshape(expected_result, shape=(self.BATCH_SIZE,))\n\n        return [prediction_input, expected_result]\n\n    def receive_output(self, likelihoods: tf.Tensor, y_true: tf.Tensor):\n        with tf.name_scope(""post-processing""):\n            prediction = tf.argmax(likelihoods, axis=1)\n            eq_values = tf.equal(prediction, tf.cast(y_true, tf.int64))\n            acc = tf.reduce_mean(tf.cast(eq_values, tf.float32))\n            op = tf.print(\n                ""Expected:"", y_true, ""\\nActual:"", prediction, ""\\nAccuracy:"", acc\n            )\n\n            return op\n\n\nif __name__ == ""__main__"":\n    model_trainer = ModelTrainer()\n    prediction_client = PredictionClient()\n\n    # get model parameters as private tensors from model owner\n    params = tfe.define_private_input(\n        ""model-trainer"", model_trainer.provide_input\n    )  # pylint: disable=E0632\n\n    # we\'ll use the same parameters for each prediction so we cache\n    # them to avoid re-training each time\n    cache_updater, params = tfe.cache(params)\n\n    # get prediction input from client\n    x, y = tfe.define_private_input(\n        ""prediction-client"", prediction_client.provide_input\n    )  # pylint: disable=E0632\n\n    with tfe.protocol.SecureNN():\n        model = tfe.keras.Sequential()\n        model.add(\n            tfe.keras.layers.Conv2D(\n                ModelTrainer.HIDDEN_CHANNELS,\n                (ModelTrainer.KERNEL, ModelTrainer.KERNEL),\n                batch_input_shape=(\n                    ModelTrainer.BATCH_SIZE,\n                    ModelTrainer.IN_DIM,\n                    ModelTrainer.IN_DIM,\n                    ModelTrainer.IN_CHANNELS,\n                ),\n            )\n        )\n        model.add(tfe.keras.layers.Activation(""relu""))\n        model.add(tfe.keras.layers.AveragePooling2D())\n        model.add(tfe.keras.layers.Flatten())\n        model.add(tfe.keras.layers.Dense(ModelTrainer.HIDDEN_FC2))\n        model.add(tfe.keras.layers.Activation(""relu""))\n        model.add(tfe.keras.layers.Dense(ModelTrainer.OUT_N))\n        logits = model(x)\n\n    # send prediction output back to client\n    prediction_op = tfe.define_output(\n        ""prediction-client"", [logits, y], prediction_client.receive_output\n    )\n\n    sess = tfe.Session(target=session_target)\n    sess.run(tf.global_variables_initializer(), tag=""init"")\n\n    print(""Training"")\n    sess.run(cache_updater, tag=""training"")\n\n    print(""Set trained weights"")\n    model.set_weights(params, sess)\n\n    for _ in range(5):\n        print(""Predicting"")\n        sess.run(prediction_op, tag=""prediction"")\n\n    sess.close()\n'"
examples/simple-average/run.py,4,"b'""""""Example of a simple average using TF Encrypted.""""""\n\nimport logging\nimport sys\n\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\n\n# use configuration from file if specified\n# otherwise, fall back to default LocalConfig\nif len(sys.argv) >= 2:\n    # config file was specified\n    config_file = sys.argv[1]\n    config = tfe.RemoteConfig.load(config_file)\n    tfe.set_config(config)\n    tfe.set_protocol(tfe.protocol.Pond())\n\n\n@tfe.local_computation(name_scope=""provide_input"")\ndef provide_input() -> tf.Tensor:\n    # pick random tensor to be averaged\n    return tf.random_normal(shape=(10,))\n\n\n@tfe.local_computation(""result-receiver"", name_scope=""receive_output"")\ndef receive_output(average: tf.Tensor) -> tf.Operation:\n    # simply print average\n    return tf.print(""Average:"", average)\n\n\nif __name__ == ""__main__"":\n\n    logging.basicConfig(level=logging.DEBUG)\n\n    # get input from inputters as private values\n    inputs = [\n        provide_input(\n            player_name=""inputter-0""\n        ),  # pylint: disable=unexpected-keyword-arg\n        provide_input(\n            player_name=""inputter-1""\n        ),  # pylint: disable=unexpected-keyword-arg\n        provide_input(\n            player_name=""inputter-2""\n        ),  # pylint: disable=unexpected-keyword-arg\n        provide_input(\n            player_name=""inputter-3""\n        ),  # pylint: disable=unexpected-keyword-arg\n        provide_input(\n            player_name=""inputter-4""\n        ),  # pylint: disable=unexpected-keyword-arg\n    ]\n\n    # sum all inputs and divide by count\n    result = tfe.add_n(inputs) / len(inputs)\n\n    # send result to receiver\n    result_op = receive_output(result)\n\n    # run a few times\n    with tfe.Session() as sess:\n        sess.run(result_op, tag=""average"")\n'"
operations/secure_random/__init__.py,0,b''
operations/secure_random/test_secure_random.py,27,"b'# pylint: disable=missing-docstring\nimport os\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework.errors import NotFoundError\n\nimport tf_encrypted as tfe\n\ndirname = os.path.dirname(tfe.__file__)\nso_name = ""{dn}/operations/secure_random/secure_random_module_tf_{tfv}.so""\nshared_object = so_name.format(dn=dirname, tfv=tf.__version__)\nnotfound_msg = ""secure_random_module not found""\n\ntry:\n    secure_random_module = tf.load_op_library(shared_object)\n    seeded_random_uniform = secure_random_module.secure_seeded_random_uniform\n    random_uniform = secure_random_module.secure_random_uniform\n    seed = secure_random_module.secure_seed\nexcept NotFoundError:\n    secure_random_module = None\n\n\n@unittest.skipIf(secure_random_module is None, notfound_msg)\nclass TestSeededRandomUniform(unittest.TestCase):\n    def test_int32_return(self):\n        expected = [[608, 425, 925], [198, 891, 721]]\n\n        with tf.Session():\n            output = seeded_random_uniform(\n                [2, 3], [1, 1, 1, 1, 1, 1, 1, 2], 0, 1000\n            ).eval()\n\n            np.testing.assert_array_equal(output, expected)\n\n    def test_int64_return(self):\n        expected = [[425, 198, 721], [911, 617, 113]]\n\n        with tf.Session():\n            minval = tf.constant(0, dtype=tf.int64)\n            maxval = tf.constant(1000, dtype=tf.int64)\n\n            output = seeded_random_uniform(\n                [2, 3], [1, 1, 1, 1, 1, 1, 1, 2], minval, maxval\n            ).eval()\n\n            np.testing.assert_array_equal(output, expected)\n\n    def test_min_max_range(self):\n        with tf.Session():\n            minval = tf.constant(-100000000, dtype=tf.int32)\n            maxval = tf.constant(100000000, dtype=tf.int32)\n\n            output = seeded_random_uniform(\n                [10000], [1, 1, 1, 500, 1, 1, 1, 2], minval, maxval\n            ).eval()\n\n            for out in output:\n                assert -100000000 <= out < 100000000\n\n    def test_invalid_max_min(self):\n        with tf.Session():\n            minval = tf.constant(1000, dtype=tf.int64)\n            maxval = tf.constant(-1000, dtype=tf.int64)\n\n            with np.testing.assert_raises(errors.InvalidArgumentError):\n                seeded_random_uniform(\n                    [2, 3], [1, 1, 1, 1, 1, 1, 1, 2], minval, maxval\n                ).eval()\n\n    def test_negative_numbers(self):\n        expected = [[-1575, -1802, -1279], [-1089, -1383, -1887]]\n        with tf.Session():\n            minval = tf.constant(-2000, dtype=tf.int64)\n            maxval = tf.constant(-1000, dtype=tf.int64)\n\n            output = seeded_random_uniform(\n                [2, 3], [1, 1, 1, 1, 1, 1, 1, 2], minval, maxval\n            ).eval()\n\n            np.testing.assert_array_equal(output, expected)\n\n\n@unittest.skipIf(secure_random_module is None, notfound_msg)\nclass TestRandomUniform(unittest.TestCase):\n    def test_min_max_range(self):\n        with tf.Session():\n            minval = tf.constant(-10000000, dtype=tf.int32)\n            maxval = tf.constant(10000000, dtype=tf.int32)\n\n            output = random_uniform([1000], minval, maxval).eval()\n\n            for out in output:\n                assert -10000000 <= out < 10000000\n\n    def test_small_range(self):\n        with tf.Session():\n            minval = tf.constant(-10, dtype=tf.int32)\n            maxval = tf.constant(10, dtype=tf.int32)\n\n            output = random_uniform([1000], minval, maxval).eval()\n\n            for out in output:\n                assert -10 <= out < 10\n\n    def test_neg_range(self):\n        with tf.Session():\n            minval = tf.constant(-100, dtype=tf.int32)\n            maxval = tf.constant(0, dtype=tf.int32)\n\n            output = random_uniform([1000], minval, maxval).eval()\n\n            for out in output:\n                assert out < 0\n\n\n@unittest.skipIf(secure_random_module is None, notfound_msg)\nclass TestSeed(unittest.TestCase):\n    def test_seed(self):\n        with tf.Session():\n            s = seed()\n\n            minval = tf.constant(-2000, dtype=tf.int64)\n            maxval = tf.constant(0, dtype=tf.int64)\n\n            shape = [2, 3]\n\n            output = seeded_random_uniform(shape, s, minval, maxval).eval()\n\n            np.testing.assert_array_equal(output.shape, shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/convert/__init__.py,0,"b'""""""The TFE Converter.""""""\nfrom .convert import Converter\nfrom .inspect import export\nfrom .inspect import inspect_subgraph\nfrom .inspect import print_from_graphdef\nfrom .register import registry\n\n__all__ = [\n    ""Converter"",\n    ""export"",\n    ""inspect_subgraph"",\n    ""print_from_graphdef"",\n    ""registry"",\n]\n'"
tf_encrypted/convert/convert.py,2,"b'""""""Module for automatically building TFE graphs from\ntheir corresponding TF GraphDefs.\n\nSee README.md for details on usage and extension.""""""\nimport re\nfrom collections import OrderedDict\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\nfrom typing import Union\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.config import Config\nfrom tf_encrypted.config import get_config\nfrom tf_encrypted.convert.register import REGISTERED_SPECOPS\nfrom tf_encrypted.player import Player\nfrom tf_encrypted.protocol import Protocol\nfrom tf_encrypted.protocol.pond import TFEInputter\n\n\nclass Converter:\n    """"""The TFE Converter.\n\n    Args:\n        registry: An OrderedDict mapping from scope names to TFE conversion\n            functions. Idiomatic behavior is to use tf.convert.registry(), however\n            custom layers and operations can be added to create new conversion\n            registries.\n        config: `Config` to use when constructing the TFE graph.\n            Defaults to the current global TFE config.\n        protocol: `Protocol` to use when constructing the TFE graph.\n            Defaults to the current global TFE protocol.\n        model_provider: The Player who will act as the model provider, or a string\n            identifier for the Player.\n    """"""\n\n    def __init__(\n        self,\n        registry,\n        config: Optional[Config] = None,\n        protocol: Optional[Protocol] = None,\n        model_provider: Optional[Union[str, Player]] = None,\n    ) -> None:\n        self.config = config if config is not None else get_config()\n        if protocol is not None:\n            tfe.set_protocol(protocol)\n        if model_provider is None:\n            self.model_provider = self.config.get_player(""model-provider"")\n        elif isinstance(model_provider, str):\n            self.model_provider = self.config.get_player(model_provider)\n        else:\n            self.model_provider = model_provider\n        self.registry = registry\n        self.outputs = {}\n\n    def convert(\n        self,\n        graph_def: Any,\n        input_player: Union[str, Player],\n        inputter_fn: Optional[Union[TFEInputter, List[TFEInputter]]] = None,\n    ) -> Any:\n        """"""Convert a frozen GraphDef to a TFE Graph.""""""\n        if not graph_def.node:\n            raise ValueError(""An empty model was passed to the converter."")\n\n        if isinstance(input_player, str):\n            input_player = get_config().get_player(""input-provider"")\n        assert isinstance(input_player, Player)\n\n        if inputter_fn is None:\n            inputs = []\n        elif isinstance(inputter_fn, (list, tuple)):\n            inputs = inputter_fn\n        else:\n            inputs = [inputter_fn]\n        inputs_iterable = enumerate(inputs)\n\n        # Identify if there are special ops in pb file,\n        # e.g. required_space_to_batch_paddings\n        # If yes, identify the inputs and outputs of these special ops.\n        output_name = graph_def.node[-1].name  # Assume output is last node.\n        specop_dict, specop_inputs, specop_outputs = find_specops(\n            graph_def, output_name\n        )\n\n        # Create a dictionary excluding all the sub ops related to\n        # required_space_to_batch_paddings. Except the sub ops related to the input\n        # or output of this special ops.\n        pb_trimmed, graph_def = select_relevant_ops(\n            specop_inputs, specop_outputs, graph_def\n        )\n        node_list = pb_trimmed.values()\n\n        # If the ops are not related to the special ops, use the existing approach\n        # to register them. Otherwise for the special ops replace the output from\n        # the sub ops by the output from the high level operation then register.\n        for node in node_list:\n            if node.name not in specop_outputs:\n                self._register_op(node, inputs_iterable, input_player, graph_def)\n\n            else:\n                # Register high level special operations\n                for s in specop_dict:\n                    # If this node is the output of the current specop, register it\n                    if match_numbered_scope(\n                        s, node.name, return_group=False, numbered=False\n                    ):\n                        self._register_specop(node, specop_dict[s])\n\n        return self.outputs[output_name]\n\n    def _register_op(self, node, inputs_iterable, input_player, graph_def):\n        """"""Register single ops.""""""\n        output = strip_tensor_info(node.name)\n        if node.op == ""Placeholder"":\n            try:\n                _, item = inputs_iterable.__next__()\n            except StopIteration:\n                raise InvalidArgumentError(""Not enough placeholders supplied"")\n\n            x = tfe.define_private_input(input_player, item)\n            self.outputs[output] = x\n            return\n\n        out = self.registry[node.op](self, node, node.input)\n\n        # if the operation returns a list or tuple with several ouputs,\n        # identify the outputs node name\n        if isinstance(out, (list, tuple)):\n            output_name = find_output_names(graph_def, node.name, len(out))\n            # If output_name is empty, it means this node\n            # is the last one in the graph\n            if not output_name:\n                self.outputs[output] = out\n            else:\n                for i, _ in enumerate(out):\n                    self.outputs[output_name[i]] = out[i]\n        else:\n            self.outputs[output] = out\n\n    def _register_specop(self, node, specop_scope_dict):\n        """"""Handle special op registration.""""""\n        input_list = specop_scope_dict[""inputs""]\n        output_list = specop_scope_dict[""outputs""]\n\n        # Handle edge cases if the ops return multiple outputs\n        op_handler = self.registry[specop_scope_dict[""op""]]\n\n        nodes = specop_scope_dict[""interiors""]\n        if not nodes:\n            nodes = node\n        outs = op_handler(self, nodes, input_list)\n        if isinstance(outs, (list, tuple)):\n            for i, x in enumerate(outs):\n                self.outputs[output_list[i]] = x\n        else:\n            self.outputs[output_list[0]] = outs\n\n\ndef select_relevant_ops(all_specop_inputs, all_specop_outputs, graph_def):\n    """"""\n    Prune out subgraphs that have been identified as special ops from the\n    graph_def, and return the pruned graph_def.\n    """"""\n\n    trimmed_graph = OrderedDict()\n    ordered_graph = OrderedDict()\n\n    for n in graph_def.node:\n        ordered_graph[n.name] = n\n        for op in REGISTERED_SPECOPS:\n\n            matched = False\n            # If the node falls under a specop scope,\n            # only add if it\'s an input or output to the specop.\n            if match_numbered_scope(op, n.name, return_group=False):\n                matched = True\n                is_input = n.name in all_specop_inputs\n                is_output = n.name in all_specop_outputs\n                if is_input or is_output:\n                    trimmed_graph[n.name] = n\n                break\n        # Otherwise, just add it\n        if not matched:\n            trimmed_graph[n.name] = n\n\n    return trimmed_graph, ordered_graph\n\n\ndef find_specops(graph_def, output_name):\n    """"""\n    For special ops defined in REGISTERED_SPECOPS, assemble necessary info to\n    convert them and place into the specops_dict. Also returns these ops\'\n    collective inputs and outputs together in separate arrays.\n    """"""\n\n    specops_dict = OrderedDict()\n    all_specop_inputs = []\n    all_specop_outputs = []\n    namespace = specop_namespace(graph_def)\n\n    for scope, subscope_map in namespace.items():\n        specops_dict[scope] = {}\n        specops_dict[scope][""op""] = specop_from_numberedscope(scope)\n        specops_dict[scope][""interiors""] = get_interiors(scope, namespace)\n        inputs, outputs = find_leaves(scope, subscope_map)\n\n        specops_dict[scope][""inputs""] = inputs\n        all_specop_inputs += inputs\n        # if no outputs found assume output to model is the special op output\n        if not outputs:\n            outputs = [output_name]\n        specops_dict[scope][""outputs""] = outputs\n        all_specop_outputs += outputs\n\n    return specops_dict, all_specop_inputs, all_specop_outputs\n\n\ndef specop_namespace(graph_def):\n    """"""\n    Gathers all subgraphs corresponding to registered special ops.\n\n    For each specop scope matching `{specop}_[0-9]+/`, assemble all ops\n    falling under that scope into a map of op name to op node, and add the\n    map to the namespace keyed by its scope.\n\n    Returns an OrderedDict[scope --> ops_map],\n    where ops_map is an OrderedDict[NodeDef.name --> NodeDef].\n    """"""\n\n    namespace = OrderedDict()\n    for node in graph_def.node:\n        for specop in REGISTERED_SPECOPS:\n            node_name = node.name\n            this_scope = match_numbered_scope(specop, node_name)\n            if this_scope is None:\n                continue\n            if this_scope not in namespace:\n                namespace[this_scope] = OrderedDict()\n            namespace[this_scope][node_name] = node\n\n    return namespace\n\n\ndef get_interiors(specop_scope, subscope_map):\n    """"""\n    An interior (op/node) of a subgraph is an op that is neither\n    an input or an output for all ops outside of that subgraph.\n\n    Given a specop_scope, look for registered interior ops in the\n    corresponding value of subscope_map and collect their NodeDefs\n    into an OrderedDict keyed by the registered interior op name.\n    """"""\n    specop = specop_from_numberedscope(specop_scope)\n    if specop is None:\n        return None\n    interior_names = REGISTERED_SPECOPS[specop][""interiors""]\n    interiors = OrderedDict()\n    if interior_names is None:\n        return interiors\n    subscope_ops_map = subscope_map[specop_scope]\n    for op in interior_names:\n        for node_name in subscope_ops_map:\n            if match_numbered_leaf(op, node_name) is not None:\n                interiors[op] = subscope_ops_map[node_name]\n                break\n    return interiors\n\n\ndef find_leaves(scope, subscope_map):\n    """"""Assemble input and output leaf nodes of the subgraph represented by\n    subscope_map.\n    """"""\n\n    input_leaves = []\n    output_leaves = list(subscope_map.keys())\n    for _, node in subscope_map.items():\n        for inp in node.input:\n\n            if (\n                match_numbered_scope(scope, inp) is None\n                and re.search(""(.*/)*keras_learning_phase$"", inp) is None\n            ):\n                # edge case - the keras_learning_phase node is an input to other\n                # batchnorm nodes, but is irrelevent to the converter\n                # and therefore can be ignored\n                input_leaves.append(inp)\n\n            if re.search(r"":\\d+$"", inp) is not None:\n                inp = inp.split("":"")[0]\n            if inp in output_leaves:\n                output_leaves.remove(inp)\n\n    seen = set()\n    adder = seen.add\n    input_leaves = [x for x in input_leaves if not (x in seen or adder(x))]\n\n    return input_leaves, output_leaves\n\n\ndef match_numbered_scope(specop, search_string, return_group=True, numbered=True):\n    """"""\n    Find a numbered scope matching a specop from REGISTERED_SPECOPS,\n    and return it if found.\n        Example: \'conv2d\' will match \'...conv2d_345/...\' and return \'conv2d_345\'.\n\n    Args:\n        specop: the specop to match.\n        search_string: the op name to search.\n        return group: if True, returns the last matching group, otherwise return\n            the match object.\n        numbered: only match exact numbering  -\n            i.e. conv2d_1 will only match conv2d_1 and not conv2d etc.\n\n    """"""\n    if numbered:\n        expr = ""^(.*/)*({0})/|(^(.*/)*({0}_[0-9]+))/"".format(specop)\n    else:\n        expr = ""^(.*/)*({0})/"".format(specop)\n\n    match = re.search(expr, search_string)\n    if match is not None:\n        if not return_group:\n            return match\n        for grp in reversed(match.groups()):\n            if grp is not None:\n                return grp\n    return match\n\n\ndef match_numbered_leaf(leaf_to_match, search_string):\n    """"""\n    Find a numbered leaf matching a tf.Operation and return it if found.\n\n    Example: \'Conv2D\' will match \'.../Conv2D_5\' and return \'Conv2D_5\'\n    """"""\n    expr = ""/({0}$)|/({0}_[0-9]+$)"".format(leaf_to_match)\n    match = re.search(expr, search_string)\n    if match is not None:\n        return match.group(1)\n    return match\n\n\ndef specop_from_numberedscope(scope):\n    """"""\n    An inverse for `match_numbered_scope`.\n\n    Example: \'conv2d_4\' will produce \'conv2d\'.\n    """"""\n    expr = ""[_a-zA-Z0-9]+(?=_[0-9]+)""\n    match = re.search(expr, scope)\n    if match is not None:\n        return match.group(0)\n    return scope\n\n\ndef strip_tensor_info(node_name: str) -> str:\n    if node_name.startswith(""^""):\n        return node_name[1:]\n    return node_name.split("":"")[0]\n\n\nclass InvalidArgumentError(Exception):\n    pass\n\n\ndef find_output_names(graph_def, node_name, num_outputs):\n    """"""\n    List output names for a specific node.\n\n    Add to the output name list if the input name starts with the\n    node name (namespace) of the ops we are registering but\n    not included in the name of the node we are inspecting.\n\n    For example, for the op `split`, based on the node below, the\n    output names are `split` and `split:1`\n\n    node {\n        name: ""output/input""\n        op: ""Pack""\n        input: ""split""\n        input: ""split:1""\n        }\n    """"""\n    output_node = [None] * num_outputs\n    node_name_list = list(graph_def.keys())\n    n_i = node_name_list.index(node_name)\n    # Forward lookahead from the node we want register\n    for n in node_name_list[n_i + 1 :]:\n        if not n.startswith(node_name):\n            gdf = graph_def[n]\n            for x in gdf.input:\n                # we insert the names by their index,\n                # and not by the order of appearance in the graph\n                if x.startswith(node_name):\n                    if "":"" not in x:\n                        output_node[0] = x\n                    else:\n                        output_node[int(x.split("":"")[-1])] = x\n\n    # assert if not all output nodes appear in the graph\n    assert None not in output_node\n\n    return output_node\n'"
tf_encrypted/convert/convert_test.py,179,"b'# pylint: disable=missing-docstring\nimport logging\nimport os\nimport unittest\nfrom typing import List\nfrom typing import Tuple\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import DepthwiseConv2D\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nfrom tensorflow.keras.layers import GlobalMaxPooling2D\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.python.framework import graph_io\nfrom tensorflow.python.framework import graph_util\nfrom tensorflow.python.platform import gfile\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.convert import Converter\nfrom tf_encrypted.convert.register import registry\n\n_GLOBAL_FILENAME = """"\n_SEED = 826485786\nnp.random.seed(_SEED)\ntf.set_random_seed(_SEED)\n\n\nclass TestConvert(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n        self.previous_logging_level = logging.getLogger().level\n        logging.getLogger().setLevel(logging.ERROR)\n\n    def tearDown(self):\n        global _GLOBAL_FILENAME\n\n        tf.reset_default_graph()\n        K.clear_session()\n\n        logging.debug(""Cleaning file: %s"", _GLOBAL_FILENAME)\n        os.remove(_GLOBAL_FILENAME)\n\n        logging.getLogger().setLevel(self.previous_logging_level)\n\n    @staticmethod\n    def ndarray_input_fn(x):\n        def input_fn():\n            return tf.constant(x)\n\n        return input_fn\n\n    @staticmethod\n    def _assert_successful_conversion(\n        prot,\n        graph_def,\n        actual,\n        *input_fns,\n        decimals=3,\n        **kwargs,  # pylint: disable=unused-argument\n    ):\n        converter = Converter(\n            registry(),\n            config=tfe.get_config(),\n            protocol=prot,\n            model_provider=""model-provider"",\n        )\n        x = converter.convert(graph_def, ""input-provider"", list(input_fns))\n\n        with tfe.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            if not isinstance(x, (list, tuple)):\n                x = [x]\n                actual = [actual]\n            else:\n                assert isinstance(\n                    actual, (list, tuple)\n                ), ""expected output to be tensor sequence""\n            try:\n                output = sess.run([xi.reveal().decode() for xi in x], tag=""reveal"")\n            except AttributeError:\n                # assume all xi are all public\n                output = sess.run(x, tag=""reveal"")\n            for o_i, a_i in zip(output, actual):\n                np.testing.assert_array_almost_equal(o_i, a_i, decimal=decimals)\n\n    @staticmethod\n    def _construct_conversion_test(op_name, *test_inputs, **kwargs):\n        global _GLOBAL_FILENAME\n        _GLOBAL_FILENAME = ""{}.pb"".format(op_name)\n        exporter = globals()[""export_{}"".format(op_name)]\n        runner = globals()[""run_{}"".format(op_name)]\n        protocol = kwargs.pop(""protocol"")\n\n        path = exporter(_GLOBAL_FILENAME, test_inputs[0].shape, **kwargs)\n        tf.reset_default_graph()\n\n        graph_def = read_graph(path)\n        tf.reset_default_graph()\n\n        actual = runner(*test_inputs, **kwargs)\n        tf.reset_default_graph()\n\n        prot_class = getattr(tfe.protocol, protocol)\n\n        return graph_def, actual, prot_class\n\n    @staticmethod\n    def _construct_empty_conversion_test(op_name, **kwargs):\n        global _GLOBAL_FILENAME\n        _GLOBAL_FILENAME = ""{}.pb"".format(op_name)\n        open(_GLOBAL_FILENAME, ""w+"")\n        protocol = kwargs.pop(""protocol"")\n\n        path = _GLOBAL_FILENAME\n        tf.reset_default_graph()\n\n        graph_def = read_graph(path)\n        tf.reset_default_graph()\n\n        prot_class = getattr(tfe.protocol, protocol)\n\n        return graph_def, prot_class\n\n    @classmethod\n    def _test_with_ndarray_input_fn(\n        cls, op_name, test_input, protocol=""Pond"", decimals=3, **kwargs\n    ):\n        # Treat as an example of how to run tests with a particular kind of input\n        graph_def, actual, prot_class = cls._construct_conversion_test(\n            op_name, test_input, protocol=protocol, **kwargs\n        )\n        with prot_class() as prot:\n            input_fn = cls.ndarray_input_fn(test_input)\n            cls._assert_successful_conversion(\n                prot, graph_def, actual, input_fn, decimals=decimals, **kwargs\n            )\n\n    def test_empty_model(self):\n        test_input = np.ones([1, 8, 8, 1])\n        graph_def, prot_class = self._construct_empty_conversion_test(\n            ""empty_model"", protocol=""SecureNN""\n        )\n        with prot_class() as prot:\n            input_fn = self.ndarray_input_fn(test_input)\n            converter = Converter(\n                registry(),\n                config=tfe.get_config(),\n                protocol=prot,\n                model_provider=""model-provider"",\n            )\n            self.assertRaises(\n                ValueError, converter.convert, graph_def, ""input-provider"", input_fn,\n            )\n\n    def test_keras_multilayer(self):\n        test_input = np.ones([1, 8, 8, 1])\n        self._test_with_ndarray_input_fn(\n            ""keras_multilayer"", test_input, protocol=""SecureNN"",\n        )\n\n    def test_conv2d_convert(self):\n        test_input = np.ones([1, 1, 8, 8])\n        self._test_with_ndarray_input_fn(""conv2d"", test_input, protocol=""Pond"")\n\n        test_input = np.ones([1, 8, 8, 1])\n        self._test_with_ndarray_input_fn(\n            ""conv2d"", test_input, protocol=""Pond"", data_format=""NHWC""\n        )\n\n    def test_matmul_convert(self):\n        test_input = np.ones([1, 28])\n        self._test_with_ndarray_input_fn(""matmul"", test_input, protocol=""Pond"")\n\n    def test_neg_convert(self):\n        test_input = np.ones([2, 2])\n        self._test_with_ndarray_input_fn(""neg"", test_input, protocol=""Pond"")\n\n    def test_add_convert(self):\n        test_input = np.ones([28, 1])\n        self._test_with_ndarray_input_fn(""add"", test_input, protocol=""Pond"")\n\n    def test_transpose_convert(self):\n        test_input = np.ones([1, 2, 3, 4])\n        self._test_with_ndarray_input_fn(""transpose"", test_input, protocol=""Pond"")\n\n    def test_reshape_convert(self):\n        test_input = np.ones([1, 2, 3, 4])\n        self._test_with_ndarray_input_fn(""reshape"", test_input, protocol=""Pond"")\n\n    def test_expand_dims_convert(self):\n        test_input = np.ones([2, 3, 4])\n        self._test_with_ndarray_input_fn(""expand_dims"", test_input, protocol=""Pond"")\n\n    def test_pad_convert(self):\n        test_input = np.ones([2, 3])\n        self._test_with_ndarray_input_fn(""pad"", test_input, protocol=""Pond"")\n\n    def test_batch_to_space_nd_convert(self):\n        test_input = np.ones([8, 1, 3, 1])\n        self._test_with_ndarray_input_fn(\n            ""batch_to_space_nd"", test_input, protocol=""Pond""\n        )\n\n    def test_space_to_batch_nd_convert(self):\n        test_input = np.ones([2, 2, 4, 1])\n        self._test_with_ndarray_input_fn(\n            ""space_to_batch_nd"", test_input, protocol=""Pond""\n        )\n\n    def test_squeeze_convert(self):\n        test_input = np.ones([1, 2, 3, 1])\n        self._test_with_ndarray_input_fn(""squeeze"", test_input, protocol=""Pond"")\n\n    def test_split_convert(self):\n        test_input = np.random.random([1, 10, 10, 3])\n        self._test_with_ndarray_input_fn(""split"", test_input, protocol=""Pond"")\n\n    def test_split_edge_case_convert(self):\n        test_input = np.random.random([1, 10, 10, 4])\n        self._test_with_ndarray_input_fn(""split_edge_case"", test_input, protocol=""Pond"")\n\n    def test_split_v_convert(self):\n        test_input = np.random.random([1, 10, 10, 3])\n        self._test_with_ndarray_input_fn(""split_v"", test_input, protocol=""Pond"")\n\n    def test_concat_convert(self):\n        test_input = np.ones([1, 10, 10, 3])\n        self._test_with_ndarray_input_fn(""concat"", test_input, protocol=""Pond"")\n\n    def test_sub_convert(self):\n        test_input = np.ones([28, 1])\n        self._test_with_ndarray_input_fn(""sub"", test_input, protocol=""Pond"")\n\n    def test_mul_convert(self):\n        test_input = np.array([[1.0, 2.0, 3.0, 4.0]])\n        self._test_with_ndarray_input_fn(""mul"", test_input, protocol=""Pond"")\n\n    def test_strided_slice_convert(self):\n        test_input = np.ones((3, 2, 3))\n        # test_input = np.array([[[1., 1., 1.], [2., 2., 2.]],\n        #                        [[3., 3., 3.], [4., 4., 4.]],\n        #                        [[5., 5., 5.], [6., 6., 6.]]])\n        self._test_with_ndarray_input_fn(""strided_slice"", test_input, protocol=""Pond"")\n\n    def test_slice_convert(self):\n        test_input = np.array(\n            [\n                [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0]],\n                [[3.0, 3.0, 3.0], [4.0, 4.0, 4.0]],\n                [[5.0, 5.0, 5.0], [6.0, 6.0, 6.0]],\n            ]\n        )\n        self._test_with_ndarray_input_fn(""slice"", test_input, protocol=""Pond"")\n\n    def test_batchnorm_convert(self):\n        test_input = np.ones([1, 1, 28, 28])\n        self._test_with_ndarray_input_fn(""batchnorm"", test_input, protocol=""Pond"")\n\n    def test_avgpool_convert(self):\n        test_input = np.ones([1, 28, 28, 1])\n        self._test_with_ndarray_input_fn(""avgpool"", test_input, protocol=""Pond"")\n\n    @pytest.mark.convert_maxpool\n    def test_maxpool_convert(self):\n        test_input = np.ones([1, 4, 4, 1])\n        self._test_with_ndarray_input_fn(""maxpool"", test_input, protocol=""SecureNN"")\n\n    def test_stack_convert(self):\n        input1 = np.array([1, 4])\n        input2 = np.array([2, 5])\n        input3 = np.array([3, 6])\n        test_inputs = [input1, input2, input3]\n        graph_def, actual, prot_class = self._construct_conversion_test(\n            ""stack"", *test_inputs, protocol=""Pond""\n        )\n\n        with prot_class() as prot:\n            input_fns = [self.ndarray_input_fn(x) for x in test_inputs]\n            self._assert_successful_conversion(prot, graph_def, actual, *input_fns)\n\n    @unittest.skipUnless(\n        tfe.config.tensorflow_supports_int64(), ""Too slow on Circle CI otherwise""\n    )\n    def test_argmax_convert(self):\n        test_input = np.array([1.0, 2.0, 3.0, 4.0])\n        self._test_with_ndarray_input_fn(\n            ""argmax"", test_input, protocol=""SecureNN"", axis=0\n        )\n\n    def test_required_space_to_batch_paddings_convert(self):\n        test_input = np.array([4, 1, 3], dtype=np.int32)\n        self._test_with_ndarray_input_fn(\n            ""required_space_to_batch_paddings"", test_input, protocol=""Pond""\n        )\n\n    def test_flatten_convert(self):\n        test_input = np.random.uniform(size=(1, 3, 3, 2)).astype(np.float32)\n        self._test_with_ndarray_input_fn(\n            ""flatten"", test_input, decimals=2, protocol=""Pond""\n        )\n\n    def test_keras_conv2d_convert(self):\n        test_input = np.ones([1, 8, 8, 1])\n        self._test_with_ndarray_input_fn(""keras_conv2d"", test_input, protocol=""Pond"")\n\n    def test_keras_depthwise_conv2d_convert(self):\n        test_input = np.ones([1, 8, 8, 1])\n        self._test_with_ndarray_input_fn(\n            ""keras_depthwise_conv2d"", test_input, protocol=""Pond""\n        )\n\n    def test_keras_dense_convert(self):\n        test_input = np.ones([2, 10])\n        self._test_with_ndarray_input_fn(\n            ""keras_dense"", test_input, decimals=2, protocol=""Pond"",\n        )\n\n    # TODO(justin1121): This is a bug in tf 1.14.0. We can re-enable\n    #                   with tf > 1.14.0.\n    # def test_keras_batchnorm_convert(self):\n    #   test_input = np.ones([1, 28, 28, 1])\n    #   self._test_with_ndarray_input_fn(\'keras_batchnorm\',\n    #                                    test_input,\n    #                                    protocol=\'Pond\')\n\n    def test_keras_global_avgpool_convert(self):\n        test_input = np.ones([1, 10, 10, 3])\n        self._test_with_ndarray_input_fn(\n            ""keras_global_avgpool"", test_input, decimals=2, protocol=""Pond""\n        )\n\n    def test_keras_global_maxgpool_convert(self):\n        test_input = np.ones([1, 10, 10, 3])\n        self._test_with_ndarray_input_fn(\n            ""keras_global_maxpool"", test_input, protocol=""SecureNN""\n        )\n\n\ndef export_argmax(filename, input_shape, axis):\n    pl = tf.placeholder(tf.float32, shape=input_shape)\n\n    output = tf.argmax(pl, axis)\n\n    return export(output, filename)\n\n\ndef run_argmax(data, axis):\n    inp = tf.constant(data)\n\n    output = tf.argmax(inp, axis)\n\n    with tf.Session() as sess:\n        out = sess.run(output)\n\n    return out\n\n\ndef run_stack(input1, input2, input3):\n    x = tf.constant(input1)\n    y = tf.constant(input2)\n    z = tf.constant(input3)\n    out = tf.stack([x, y, z])\n\n    with tf.Session() as sess:\n        out = sess.run(out)\n\n    return out\n\n\ndef export_stack(filename: str, input_shape: Tuple[int]):\n    x = tf.placeholder(tf.float32, shape=input_shape)\n    y = tf.placeholder(tf.float32, shape=input_shape)\n    z = tf.placeholder(tf.float32, shape=input_shape)\n\n    out = tf.stack([x, y, z])\n\n    return export(out, filename)\n\n\ndef run_avgpool(data):\n    a = tf.placeholder(tf.float32, shape=data.shape, name=""input"")\n\n    x = tf.nn.avg_pool(a, [1, 2, 2, 1], [1, 2, 2, 1], ""VALID"")\n\n    with tf.Session() as sess:\n        output = sess.run(x, feed_dict={a: data})\n\n    return output\n\n\ndef export_avgpool(filename, input_shape):\n    pl = tf.placeholder(tf.float32, shape=input_shape, name=""input"")\n\n    x = tf.nn.avg_pool(pl, [1, 2, 2, 1], [1, 2, 2, 1], ""VALID"")\n\n    return export(x, filename)\n\n\ndef run_maxpool(data):\n    a = tf.placeholder(tf.float32, shape=data.shape, name=""input"")\n\n    x = tf.nn.max_pool(a, [1, 2, 2, 1], [1, 2, 2, 1], ""VALID"")\n\n    with tf.Session() as sess:\n        output = sess.run(x, feed_dict={a: data})\n\n    return output\n\n\ndef export_maxpool(filename, input_shape):\n    pl = tf.placeholder(tf.float32, shape=input_shape, name=""input"")\n\n    x = tf.nn.max_pool(pl, [1, 2, 2, 1], [1, 2, 2, 1], ""VALID"")\n\n    return export(x, filename)\n\n\ndef run_batchnorm(data):\n    x = tf.placeholder(tf.float32, shape=data.shape, name=""input"")\n\n    dim = data.shape[3]\n    mean = np.ones((1, 1, 1, dim)) * 1\n    variance = np.ones((1, 1, 1, dim)) * 2\n    offset = np.ones((1, 1, 1, dim)) * 3\n    scale = np.ones((1, 1, 1, dim)) * 4\n\n    y = tf.nn.batch_normalization(x, mean, variance, offset, scale, 0.00001)\n\n    with tf.Session() as sess:\n        output = sess.run(y, feed_dict={x: data})\n\n    return output\n\n\ndef export_batchnorm(filename: str, input_shape: List[int]):\n    pl = tf.placeholder(tf.float32, shape=input_shape, name=""input"")\n\n    mean = np.ones((1, 1, 1, input_shape[3]), dtype=np.float32) * 1\n    variance = np.ones((1, 1, 1, input_shape[3]), dtype=np.float32) * 2\n    offset = np.ones((1, 1, 1, input_shape[3]), dtype=np.float32) * 3\n    scale = np.ones((1, 1, 1, input_shape[3]), dtype=np.float32) * 4\n\n    x = tf.nn.batch_normalization(pl, mean, variance, offset, scale, 0.00001)\n\n    return export(x, filename)\n\n\ndef run_conv2d(data, data_format=""NCHW""):\n    feed_me = tf.placeholder(tf.float32, shape=data.shape, name=""input"")\n\n    x = feed_me\n    if data_format == ""NCHW"":\n        x = tf.transpose(x, (0, 2, 3, 1))\n\n    filtered = tf.constant(np.ones((3, 3, 1, 3)), dtype=tf.float32, name=""weights"")\n    x = tf.nn.conv2d(x, filtered, (1, 1, 1, 1), ""SAME"", name=""nn_conv2d"")\n\n    with tf.Session() as sess:\n        output = sess.run(x, feed_dict={feed_me: data})\n\n        if data_format == ""NCHW"":\n            output = output.transpose(0, 3, 1, 2)\n\n    return output\n\n\ndef export_conv2d(filename: str, input_shape: List[int], data_format=""NCHW""):\n    pl = tf.placeholder(tf.float32, shape=input_shape, name=""input"")\n\n    filtered = tf.constant(np.ones((3, 3, 1, 3)), dtype=tf.float32, name=""weights"")\n    x = tf.nn.conv2d(\n        pl, filtered, (1, 1, 1, 1), ""SAME"", data_format=data_format, name=""nn_conv2d""\n    )\n\n    return export(x, filename)\n\n\ndef run_matmul(data):\n    a = tf.placeholder(tf.float32, shape=data.shape, name=""input"")\n    b = tf.constant(np.ones((data.shape[1], 1)), dtype=tf.float32)\n\n    x = tf.matmul(a, b)\n\n    with tf.Session() as sess:\n        output = sess.run(x, feed_dict={a: data})\n\n    return output\n\n\ndef export_matmul(filename: str, input_shape: List[int]):\n    a = tf.placeholder(tf.float32, shape=input_shape, name=""input"")\n    b = tf.constant(np.ones((input_shape[1], 1)), dtype=tf.float32)\n\n    x = tf.matmul(a, b)\n\n    return export(x, filename)\n\n\ndef export_neg(filename: str, input_shape: List[int]):\n    a = tf.placeholder(tf.float32, shape=input_shape, name=""input"")\n\n    x = tf.negative(a)\n\n    return export(x, filename)\n\n\ndef run_neg(data):\n    a = tf.placeholder(tf.float32, shape=data.shape, name=""input"")\n\n    x = tf.negative(a)\n\n    with tf.Session() as sess:\n        output = sess.run(x, feed_dict={a: data})\n\n    return output\n\n\ndef run_add(data):\n    a = tf.placeholder(tf.float32, shape=data.shape, name=""input"")\n    b = tf.constant(np.ones((data.shape[1], 1)), dtype=tf.float32)\n\n    x = tf.add(a, b)\n\n    with tf.Session() as sess:\n        output = sess.run(x, feed_dict={a: data})\n\n    return output\n\n\ndef export_add(filename: str, input_shape: List[int]):\n    a = tf.placeholder(tf.float32, shape=input_shape, name=""input"")\n    b = tf.constant(np.ones((input_shape[0], 1)), dtype=tf.float32)\n\n    x = tf.add(a, b)\n\n    return export(x, filename)\n\n\ndef run_transpose(data):\n    a = tf.placeholder(tf.float32, shape=data.shape, name=""input"")\n\n    x = tf.transpose(a, perm=(0, 3, 1, 2))\n\n    with tf.Session() as sess:\n        output = sess.run(x, feed_dict={a: data})\n\n    return output\n\n\ndef export_transpose(filename: str, input_shape: List[int]):\n    a = tf.placeholder(tf.float32, shape=input_shape, name=""input"")\n\n    x = tf.transpose(a, perm=(0, 3, 1, 2))\n\n    return export(x, filename)\n\n\ndef run_reshape(data):\n    a = tf.placeholder(tf.float32, shape=data.shape, name=""input"")\n\n    last_size = 1\n    for i in data.shape[1:]:\n        last_size *= i\n\n    x = tf.reshape(a, [-1, last_size])\n\n    with tf.Session() as sess:\n        output = sess.run(x, feed_dict={a: data})\n\n    return output\n\n\ndef export_reshape(filename: str, input_shape: List[int]):\n    a = tf.placeholder(tf.float32, shape=input_shape, name=""input"")\n\n    last_size = 1\n    for i in input_shape[1:]:\n        last_size *= i\n\n    x = tf.reshape(a, [-1, last_size])\n\n    return export(x, filename)\n\n\ndef run_expand_dims(data):\n    a = tf.placeholder(tf.float32, shape=data.shape, name=""input"")\n\n    x = tf.expand_dims(a, axis=0)\n\n    with tf.Session() as sess:\n        output = sess.run(x, feed_dict={a: data})\n\n    return output\n\n\ndef export_expand_dims(filename: str, input_shape: List[int]):\n    a = tf.placeholder(tf.float32, shape=input_shape, name=""input"")\n\n    x = tf.expand_dims(a, axis=0)\n\n    return export(x, filename)\n\n\ndef run_pad(data):\n    a = tf.placeholder(tf.float32, shape=data.shape, name=""input"")\n\n    x = tf.pad(a, paddings=tf.constant([[2, 2], [3, 4]]), mode=""CONSTANT"")\n\n    with tf.Session() as sess:\n        output = sess.run(x, feed_dict={a: data})\n\n    return output\n\n\ndef export_pad(filename: str, input_shape: List[int]):\n    a = tf.placeholder(tf.float32, shape=input_shape, name=""input"")\n\n    x = tf.pad(a, paddings=tf.constant([[2, 2], [3, 4]]), mode=""CONSTANT"")\n\n    return export(x, filename)\n\n\ndef _construct_batch_to_space_nd(input_shape):\n    a = tf.placeholder(tf.float32, shape=input_shape, name=""input"")\n    block_shape = tf.constant([2, 2], dtype=tf.int32)\n    crops = tf.constant([[0, 0], [2, 0]], dtype=tf.int32)\n    x = tf.batch_to_space_nd(a, block_shape=block_shape, crops=crops)\n    return x, a\n\n\ndef export_batch_to_space_nd(filename, input_shape):\n    x, _ = _construct_batch_to_space_nd(input_shape)\n    return export(x, filename)\n\n\ndef run_batch_to_space_nd(data):\n    x, a = _construct_batch_to_space_nd(data.shape)\n    with tf.Session() as sess:\n        output = sess.run(x, feed_dict={a: data})\n    return output\n\n\ndef _construct_space_to_batch_nd(input_shape):\n    a = tf.placeholder(tf.float32, shape=input_shape, name=""input"")\n    block_shape = tf.constant([2, 2], dtype=tf.int32)\n    paddings = tf.constant([[0, 0], [2, 0]], dtype=tf.int32)\n    x = tf.space_to_batch_nd(a, block_shape=block_shape, paddings=paddings)\n    return x, a\n\n\ndef export_space_to_batch_nd(filename, input_shape):\n    x, _ = _construct_space_to_batch_nd(input_shape)\n    return export(x, filename)\n\n\ndef run_space_to_batch_nd(data):\n    x, a = _construct_space_to_batch_nd(data.shape)\n    with tf.Session() as sess:\n        output = sess.run(x, feed_dict={a: data})\n    return output\n\n\ndef run_squeeze(data):\n    a = tf.placeholder(tf.float32, shape=data.shape, name=""input"")\n    x = tf.squeeze(a, axis=[0, 3])\n    with tf.Session() as sess:\n        output = sess.run(x, feed_dict={a: data})\n    return output\n\n\ndef export_squeeze(filename, input_shape):\n    a = tf.placeholder(tf.float32, shape=input_shape, name=""input"")\n    x = tf.squeeze(a, axis=[0, 3])\n    return export(x, filename)\n\n\ndef run_gather(data):\n    a = tf.placeholder(tf.float32, shape=data.shape, name=""input"")\n    x = tf.gather(a, indices=[1, 3], axis=0)\n    with tf.Session() as sess:\n        output = sess.run(x, feed_dict={a: data})\n    return output\n\n\ndef export_gather(filename, input_shape):\n    a = tf.placeholder(tf.float32, shape=input_shape, name=""input"")\n    x = tf.gather(a, indices=[1, 3], axis=0)\n    return export(x, filename)\n\n\ndef run_split(data):\n    a = tf.placeholder(tf.float32, shape=data.shape, name=""input"")\n    x = tf.split(a, num_or_size_splits=3, axis=-1)\n    with tf.Session() as sess:\n        output = sess.run(x, feed_dict={a: data})\n    return output\n\n\ndef export_split(filename, input_shape):\n    a = tf.placeholder(tf.float32, shape=input_shape, name=""input"")\n    x = tf.split(a, num_or_size_splits=3, axis=-1)\n    return export(x, filename)\n\n\ndef split_edge_case_builder(input_shape, filters=2, kernel_size=3):\n    init = tf.keras.initializers.RandomNormal(seed=1)\n\n    x = tf.keras.Input(shape=input_shape[1:])\n    y1, y2 = tf.keras.layers.Lambda(\n        lambda tensor: tf.split(tensor, num_or_size_splits=2, axis=-1)\n    )(x)\n    y = tf.keras.layers.Conv2D(\n        filters, kernel_size, kernel_initializer=init, use_bias=True, padding=""same""\n    )(y2)\n    y = tf.keras.layers.Concatenate(axis=-1)([y1, y])\n\n    return tf.keras.Model(x, y)\n\n\ndef export_split_edge_case(filename, input_shape):\n    model, _ = _keras_model_core(split_edge_case_builder, shape=input_shape)\n\n    sess = K.get_session()\n    output = model.output\n    return export(output, filename, sess=sess)\n\n\ndef run_split_edge_case(data):\n    _, out = _keras_model_core(split_edge_case_builder, data=data)\n    return out\n\n\ndef run_split_v(data):\n    a = tf.placeholder(tf.float32, shape=data.shape, name=""input"")\n    x = tf.split(a, num_or_size_splits=[1, 1, 1], axis=-1)\n    with tf.Session() as sess:\n        output = sess.run(x, feed_dict={a: data})\n    return output\n\n\ndef export_split_v(filename, input_shape):\n    a = tf.placeholder(tf.float32, shape=input_shape, name=""input"")\n    x = tf.split(a, num_or_size_splits=[1, 1, 1], axis=-1)\n    return export(x, filename)\n\n\ndef run_concat(data):\n    a = tf.placeholder(tf.float32, shape=data.shape, name=""input"")\n    x = tf.concat([a, a, a], axis=-1)\n    with tf.Session() as sess:\n        output = sess.run(x, feed_dict={a: data})\n    return output\n\n\ndef export_concat(filename, input_shape):\n    a = tf.placeholder(tf.float32, shape=input_shape, name=""input"")\n    x = tf.concat([a, a, a], axis=-1)\n    return export(x, filename)\n\n\ndef run_sub(data):\n    a = tf.placeholder(tf.float32, shape=data.shape, name=""input"")\n    b = tf.constant(np.ones((data.shape[0], 1)), dtype=tf.float32)\n\n    x = tf.subtract(a, b)\n\n    with tf.Session() as sess:\n        output = sess.run(x, feed_dict={a: data})\n\n    return output\n\n\ndef export_sub(filename, input_shape):\n    a = tf.placeholder(tf.float32, shape=input_shape, name=""input"")\n    b = tf.constant(np.ones((input_shape[0], 1)), dtype=tf.float32)\n\n    x = tf.subtract(a, b)\n\n    return export(x, filename)\n\n\ndef run_mul(data):\n    a = tf.placeholder(tf.float32, shape=data.shape, name=""input"")\n    b = tf.constant(\n        np.array([1.0, 2.0, 3.0, 4.0]).reshape(data.shape), dtype=tf.float32\n    )\n\n    x = tf.multiply(a, b)\n\n    with tf.Session() as sess:\n        output = sess.run(x, feed_dict={a: data})\n\n    return output\n\n\ndef export_mul(filename, input_shape):\n    a = tf.placeholder(tf.float32, shape=input_shape, name=""input"")\n    b = tf.constant(\n        np.array([1.0, 2.0, 3.0, 4.0]).reshape(input_shape), dtype=tf.float32\n    )\n\n    x = tf.multiply(a, b)\n\n    return export(x, filename)\n\n\ndef export_strided_slice(filename, input_shape):\n    t = tf.placeholder(tf.float32, shape=input_shape, name=""input"")\n    out = tf.strided_slice(t, [1, 0, 0], [2, 1, 3], [1, 1, 1])\n\n    return export(out, filename)\n\n\ndef run_strided_slice(data):\n    a = tf.placeholder(tf.float32, shape=data.shape, name=""input"")\n    out = tf.strided_slice(a, [1, 0, 0], [2, 1, 3], [1, 1, 1])\n\n    with tf.Session() as sess:\n        output = sess.run(out, feed_dict={a: data})\n\n    return output\n\n\ndef export_slice(filename, input_shape):\n    t = tf.placeholder(tf.float32, shape=input_shape, name=""input"")\n    out = tf.slice(t, [1, 0, 0], [2, 1, -1])\n\n    return export(out, filename)\n\n\ndef run_slice(data):\n    a = tf.placeholder(tf.float32, shape=data.shape, name=""input"")\n    out = tf.slice(a, [1, 0, 0], [2, 1, -1])\n\n    with tf.Session() as sess:\n        output = sess.run(out, feed_dict={a: data})\n\n    return output\n\n\ndef export_flatten(filename, input_shape):\n    model = Sequential()\n    model.add(Flatten(input_shape=input_shape[1:]))\n    model.predict(np.random.uniform(size=input_shape))\n\n    sess = K.get_session()\n    output = model.get_layer(""flatten"").output\n\n    return export(output, filename, sess=sess)\n\n\ndef run_flatten(data):\n    model = Sequential()\n    model.add(Flatten(input_shape=data.shape[1:]))\n    return model.predict(data)\n\n\ndef keras_multilayer_builder(\n    input_shape, filters=2, kernel_size=3, pool_size=2, units=2,\n):\n    init = tf.keras.initializers.RandomNormal(seed=1)\n    x = tf.keras.Input(shape=input_shape[1:])\n    y = tf.keras.layers.Conv2D(filters, kernel_size, kernel_initializer=init)(x)\n    y = tf.keras.layers.ReLU()(y)\n    y = tf.keras.layers.MaxPooling2D(pool_size)(y)\n    y = tf.keras.layers.Flatten()(y)\n    y = tf.keras.layers.Dense(units, kernel_initializer=init)(y)\n\n    return tf.keras.Model(x, y)\n\n\ndef export_keras_multilayer(filename, input_shape):\n    model, _ = _keras_model_core(keras_multilayer_builder, shape=input_shape)\n\n    sess = K.get_session()\n    output = model.output\n    return export(output, filename, sess=sess)\n\n\ndef run_keras_multilayer(data):\n    _, out = _keras_model_core(keras_multilayer_builder, data=data)\n    return out\n\n\ndef _keras_model_core(model_builder, shape=None, data=None, **model_builder_kwargs):\n    assert shape is None or data is None\n    if shape is None:\n        shape = data.shape\n\n    model = model_builder(shape, **model_builder_kwargs)\n\n    if data is None:\n        data = np.random.uniform(size=shape)\n    out = model.predict(data)\n    return model, out\n\n\ndef export_keras_conv2d(filename, input_shape):\n    model, _ = _keras_conv2d_core(shape=input_shape)\n\n    sess = K.get_session()\n    output = model.get_layer(""conv2d"").output\n    return export(output, filename, sess=sess)\n\n\ndef run_keras_conv2d(data):\n    _, out = _keras_conv2d_core(data=data)\n    return out\n\n\ndef _keras_conv2d_core(shape=None, data=None):\n    assert shape is None or data is None\n    if shape is None:\n        shape = data.shape\n\n    init = tf.keras.initializers.RandomNormal(seed=1)\n\n    model = Sequential()\n    c2d = Conv2D(\n        2,\n        (3, 3),\n        data_format=""channels_last"",\n        use_bias=False,\n        kernel_initializer=init,\n        input_shape=shape[1:],\n    )\n    model.add(c2d)\n\n    if data is None:\n        data = np.random.uniform(size=shape)\n    out = model.predict(data)\n    return model, out\n\n\ndef export_keras_depthwise_conv2d(filename, input_shape):\n    model, _ = _keras_depthwise_conv2d_core(shape=input_shape)\n\n    sess = K.get_session()\n    output = model.get_layer(""depthwise_conv2d"").output\n    return export(output, filename, sess=sess)\n\n\ndef run_keras_depthwise_conv2d(data):\n    _, out = _keras_depthwise_conv2d_core(data=data)\n    return out\n\n\ndef _keras_depthwise_conv2d_core(shape=None, data=None):\n    assert shape is None or data is None\n    if shape is None:\n        shape = data.shape\n\n    init = tf.keras.initializers.RandomNormal(seed=1)\n\n    model = Sequential()\n    c2d = DepthwiseConv2D(\n        (3, 3),\n        depthwise_initializer=init,\n        data_format=""channels_last"",\n        use_bias=False,\n        input_shape=shape[1:],\n    )\n    model.add(c2d)\n\n    if data is None:\n        data = np.random.uniform(size=shape)\n    out = model.predict(data)\n    return model, out\n\n\ndef export_keras_dense(filename, input_shape):\n    model, _ = _keras_dense_core(shape=input_shape)\n\n    sess = K.get_session()\n    output = model.get_layer(""dense"").output\n    return export(output, filename, sess=sess)\n\n\ndef run_keras_dense(data):\n    _, out = _keras_dense_core(data=data)\n    return out\n\n\ndef _keras_dense_core(shape=None, data=None):\n    assert shape is None or data is None\n    if shape is None:\n        shape = data.shape\n\n    init = tf.keras.initializers.RandomNormal(seed=1)\n\n    model = Sequential()\n    d = Dense(2, kernel_initializer=init, use_bias=True, input_shape=shape[1:])\n    model.add(d)\n\n    if data is None:\n        data = np.random.uniform(size=shape)\n    out = model.predict(data)\n    return model, out\n\n\ndef export_keras_batchnorm(filename, input_shape):\n    model, _ = _keras_batchnorm_core(shape=input_shape)\n\n    sess = K.get_session()\n    output = model.get_layer(""batch_normalization"").output\n    return export(output, filename, sess=sess)\n\n\ndef run_keras_batchnorm(data):\n    _, out = _keras_batchnorm_core(data=data)\n    return out\n\n\ndef _keras_batchnorm_core(shape=None, data=None):\n    assert shape is None or data is None\n    if shape is None:\n        shape = data.shape\n\n    model = Sequential()\n    bn = tf.keras.layers.BatchNormalization(input_shape=shape[1:])\n    model.add(bn)\n\n    if data is None:\n        data = np.random.uniform(size=shape)\n    out = model.predict(data)\n    return model, out\n\n\ndef export_keras_global_avgpool(filename, input_shape):\n    model, _ = _keras_global_avgpool_core(shape=input_shape)\n\n    sess = K.get_session()\n    output = model.get_layer(""global_average_pooling2d"").output\n    return export(output, filename, sess=sess)\n\n\ndef run_keras_global_avgpool(data):\n    _, out = _keras_global_avgpool_core(data=data)\n    return out\n\n\ndef _keras_global_avgpool_core(shape=None, data=None):\n    assert shape is None or data is None\n    if shape is None:\n        shape = data.shape\n\n    model = Sequential()\n    layer = GlobalAveragePooling2D(input_shape=shape[1:], data_format=""channels_last"")\n    model.add(layer)\n\n    if data is None:\n        data = np.random.uniform(size=shape)\n    out = model.predict(data)\n    return model, out\n\n\ndef export_keras_global_maxpool(filename, input_shape):\n    model, _ = _keras_global_maxpool_core(shape=input_shape)\n\n    sess = K.get_session()\n    output = model.get_layer(""global_max_pooling2d"").output\n    return export(output, filename, sess=sess)\n\n\ndef run_keras_global_maxpool(data):\n    _, out = _keras_global_maxpool_core(data=data)\n    return out\n\n\ndef _keras_global_maxpool_core(shape=None, data=None):\n    assert shape is None or data is None\n    if shape is None:\n        shape = data.shape\n\n    model = Sequential()\n    layer = GlobalMaxPooling2D(input_shape=shape[1:], data_format=""channels_last"")\n    model.add(layer)\n\n    if data is None:\n        data = np.random.uniform(size=shape)\n    out = model.predict(data)\n    return model, out\n\n\ndef run_required_space_to_batch_paddings(data):\n\n    x = tf.placeholder(tf.int32, shape=data.shape, name=""input_shape"")\n    y = tf.constant(np.array([2, 3, 2]), dtype=tf.int32)\n    p = tf.constant(np.array([[2, 3], [4, 3], [5, 2]]), dtype=tf.int32)\n\n    out = tf.required_space_to_batch_paddings(x, y, base_paddings=p)\n\n    with tf.Session() as sess:\n        output = sess.run(out, feed_dict={x: data})\n\n    return output\n\n\ndef export_required_space_to_batch_paddings(filename, input_shape):\n\n    x = tf.placeholder(tf.int32, shape=input_shape, name=""input_shape"")\n    y = tf.constant(np.array([2, 3, 2]), dtype=tf.int32)\n    p = tf.constant(np.array([[2, 3], [4, 3], [5, 2]]), dtype=tf.int32)\n\n    out = tf.required_space_to_batch_paddings(x, y, base_paddings=p)\n\n    return export(out, filename)\n\n\ndef export(x: tf.Tensor, filename: str, sess=None):\n    should_close = False\n    if sess is None:\n        should_close = True\n        sess = tf.Session()\n\n    pred_node_names = [""output""]\n    tf.identity(x, name=pred_node_names[0])\n    graph = graph_util.convert_variables_to_constants(\n        sess, sess.graph.as_graph_def(), pred_node_names\n    )\n\n    graph = graph_util.remove_training_nodes(graph)\n\n    path = graph_io.write_graph(graph, ""."", filename, as_text=False)\n\n    if should_close:\n        sess.close()\n\n    return path\n\n\ndef read_graph(path: str):\n    with gfile.GFile(path, ""rb"") as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n\n    return graph_def\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/convert/inspect.py,9,"b'""""""Methods for inspecting TF and Keras graphs for""""""\nimport os\nimport tempfile\n\nimport tensorflow as tf\n\n\ndef inspect_subgraph(subgraph, input_shape, sess=None):\n    path = _gen_graph_def(subgraph, input_shape, sess)\n    graph = _read_graph(path)\n    print_from_graphdef(graph)\n\n\ndef _gen_graph_def(subgraph, input_shape, sess):\n    temp_dir = tempfile.gettempdir()\n    filename = ""inspect_{}.pb"".format(subgraph.__class__.__name__)\n    filepath = os.path.join(temp_dir, filename)\n\n    if isinstance(input_shape, tuple):\n        input_shape = [1] + list(input_shape)\n    x = tf.zeros(input_shape)\n    y = subgraph(x)\n    return export(y, filepath, sess=sess)\n\n\ndef _read_graph(path):\n    with tf.io.gfile.GFile(path, ""rb"") as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n\n    return graph_def\n\n\ndef print_from_graphdef(graphdef):\n    for node in graphdef.node:\n        print(""| Name:"", node.name, ""| Op:"", node.op, ""|"")\n\n\ndef export(x: tf.Tensor, filename, sess=None):\n    """"""Export a GraphDef for the subgraph connected to x.\n\n    Args:\n        x: The tensor whose parent Graph we want to export.\n        filename: The filename to use for the exported GraphDef.\n        sess: An optional Session object; useful if already operating within a\n            Session context.\n\n    Returns:\n        A filepath pointing to the exported GraphDef.\n    """"""\n    should_close = False\n    if sess is None:\n        should_close = True\n        sess = tf.Session()\n\n    pred_node_names = [""output""]\n    tf.identity(x, name=pred_node_names[0])\n    graph = tf.graph_util.convert_variables_to_constants(\n        sess, sess.graph.as_graph_def(), pred_node_names\n    )\n\n    graph = tf.graph_util.remove_training_nodes(graph)\n    path = tf.io.write_graph(graph, ""."", filename, as_text=False)\n\n    if should_close:\n        sess.close()\n\n    return path\n'"
tf_encrypted/convert/register.py,67,"b'""""""Registry for the TF Encrypted Converter.""""""\nimport array\nimport logging\nimport os\nfrom collections import OrderedDict\nfrom typing import Any\nfrom typing import List\n\nimport numpy as np\nimport tensorflow as tf\nimport yaml\n\nimport tf_encrypted as tfe\n\nfrom ..keras.layers import BatchNormalization\nfrom ..keras.layers import DepthwiseConv2D\nfrom ..keras.layers import GlobalAveragePooling2D\nfrom ..keras.layers import GlobalMaxPooling2D\nfrom ..layers import AveragePooling2D\nfrom ..layers import Conv2D\nfrom ..layers import Dense\nfrom ..layers import MaxPooling2D\nfrom ..layers import Relu\nfrom ..layers import Sigmoid\nfrom ..protocol.pond import PondMaskedTensor\nfrom ..protocol.pond import PondPrivateTensor\n\n\ndef registry():\n    """"""Map reserved names and scopes to their conversion functions.""""""\n    reg = {\n        ""Placeholder"": _placeholder,\n        ""Const"": _constant,\n        ""Conv2D"": _conv2d,\n        ""Relu"": _relu,\n        ""Sigmoid"": _sigmoid,\n        ""MatMul"": _matmul,\n        ""Shape"": _shape,\n        ""StridedSlice"": _strided_slice,\n        ""Add"": _add,\n        ""AddV2"": _add,\n        ""Sub"": _sub,\n        ""Transpose"": _transpose,\n        ""Reshape"": _reshape,\n        ""Pack"": _pack,\n        ""Rsqrt"": _rsqrt,\n        ""Mul"": _mul,\n        ""ExpandDims"": _expand_dims,\n        ""AvgPool"": _avgpool,\n        ""Squeeze"": _squeeze,\n        ""ConcatV2"": _concat,\n        ""BiasAdd"": _bias_add,\n        ""MaxPool"": _maxpool,\n        ""Pad"": _pad,\n        ""BatchToSpaceND"": _batch_to_space_nd,\n        ""SpaceToBatchND"": _space_to_batch_nd,\n        ""ArgMax"": _argmax,\n        ""required_space_to_batch_paddings"": _required_space_to_batch_paddings,\n        ""flatten"": _flatten,\n        ""conv2d"": _keras_conv2d,\n        ""Slice"": _slice,\n        ""Neg"": _negative,\n        ""Split"": _split,\n        ""SplitV"": _split,\n        ""Identity"": _identity,\n        ""GatherV2"": _gather,\n        ""dense"": _keras_dense,\n        ""batch_normalization_v1"": _keras_batchnorm,\n        ""depthwise_conv2d"": _keras_depthwise_conv2d,\n        ""Mean"": _keras_global_avgpool,\n        ""Max"": _keras_global_maxpool,\n    }\n\n    return reg\n\n\nconvert_dir = os.path.dirname(os.path.abspath(__file__))\nspecops_path = os.path.join(convert_dir, ""specops.yaml"")\nwith open(specops_path, ""r"") as stream:\n    loaded_yaml = yaml.load(stream, Loader=yaml.SafeLoader)\n    sorted_yaml = sorted(loaded_yaml.items(), key=lambda kv: kv[0])\n    REGISTERED_SPECOPS = OrderedDict(sorted_yaml)\n\n\n# pylint: disable=unused-argument\n# pylint: disable=missing-docstring\ndef _placeholder(converter, node: Any, inputs: List[str]) -> Any:\n    return tf.placeholder(node.attr[""dtype""].type, shape=node.attr[""shape""].shape)\n\n\ndef _constant(converter, node: Any, inputs: List[str]) -> Any:\n    # need to able to access the underlying weights return the node\n    return node\n\n\ndef _identity(converter, node: Any, inputs: List[str]) -> Any:\n    # need to able to access the underlying weights return the node\n    return converter.outputs[inputs[0]]\n\n\ndef _matmul(converter, node: Any, inputs: List[str]) -> Any:\n    a = converter.outputs[inputs[0]]\n    b = converter.outputs[inputs[1]]\n\n    tensor = b.attr[""value""].tensor\n\n    b_shape = [i.size for i in tensor.tensor_shape.dim]\n\n    transpose_a = node.attr[""transpose_a""].b\n    transpose_b = node.attr[""transpose_b""].b\n\n    layer = Dense(\n        a.shape.as_list(),\n        b_shape[1],\n        transpose_input=transpose_a,\n        transpose_weight=transpose_b,\n    )\n\n    dtype = tensor.dtype\n\n    if dtype == tf.float32:\n        nums = array.array(""f"", tensor.tensor_content)\n    elif dtype == tf.float64:\n        nums = array.array(""d"", tensor.tensor_content)\n    else:\n        raise TypeError(""Unsupported dtype for weights"")\n\n    def inputter_fn():\n        return tf.constant(np.array(nums).reshape(b_shape))\n\n    w = tfe.define_private_input(converter.model_provider, inputter_fn)\n\n    layer.initialize(initial_weights=w)\n\n    return layer.forward(a)\n\n\ndef _conv2d(converter, node, inputs):\n    x_in = converter.outputs[inputs[0]]\n    kernel = converter.outputs[inputs[1]]\n\n    if isinstance(kernel, tf.NodeDef):\n        shape = [i.size for i in kernel.attr[""value""].tensor.tensor_shape.dim]\n        w = _nodef_to_private_pond(converter, kernel)\n    else:\n        shape = kernel.shape.as_list()\n        w = kernel\n\n    fmt = node.attr[""data_format""].s.decode(""ascii"")\n\n    layer = Conv2D(\n        x_in.shape.as_list(),\n        shape,\n        strides=int(max(node.attr[""strides""].list.i)),\n        padding=node.attr[""padding""].s.decode(""ascii""),\n        channels_first=fmt == ""NCHW"",\n    )\n\n    layer.initialize(initial_weights=w)\n\n    out = layer.forward(x_in)\n\n    return out\n\n\ndef _keras_conv2d(converter, interiors, inputs):\n    x_in = converter.outputs[inputs[0]]\n\n    conv_op = interiors[""Conv2D""]\n    kernel = interiors[""kernel""]\n    k = _nodef_to_private_pond(converter, kernel)\n    try:\n        bias = interiors[""bias""]\n        b = _nodef_to_private_pond(converter, bias)\n        for ax in [0, -1, -1]:\n            b = b.expand_dims(axis=ax)\n    except KeyError:\n        b = None\n\n    input_shape = x_in.shape.as_list()\n    shape = [i.size for i in kernel.attr[""value""].tensor.tensor_shape.dim]\n    fmt = conv_op.attr[""data_format""].s.decode(""ascii"")\n    strides = int(max(conv_op.attr[""strides""].list.i))\n    padding = conv_op.attr[""padding""].s.decode(""ascii"")\n\n    layer = Conv2D(\n        input_shape,\n        shape,\n        strides=strides,\n        padding=padding,\n        channels_first=fmt == ""NCHW"",\n    )\n\n    layer.initialize(initial_weights=k, initial_bias=b)\n    out = layer.forward(x_in)\n\n    return out\n\n\ndef _keras_depthwise_conv2d(converter, interiors, inputs):\n    x_in = converter.outputs[inputs[0]]\n\n    conv_op = interiors[""depthwise""]\n\n    kernel = interiors[""depthwise_kernel""]\n    k = _nodef_to_numpy_array(kernel)\n    kernel_init = tf.keras.initializers.Constant(k)\n\n    try:\n        bias = interiors[""bias""]\n        b = _nodef_to_numpy_array(bias)\n        bias_init = tf.keras.initializers.Constant(b)\n        use_bias = True\n    except KeyError:\n        use_bias = False\n        bias_init = ""zeros""\n\n    shape = [i.size for i in kernel.attr[""value""].tensor.tensor_shape.dim]\n\n    fmt = conv_op.attr[""data_format""].s.decode(""ascii"")\n    fmt = ""channels_last"" if fmt == ""NHWC"" else ""channels_first""\n\n    strides = int(max(conv_op.attr[""strides""].list.i))\n    padding = conv_op.attr[""padding""].s.decode(""ascii"")\n\n    layer = DepthwiseConv2D(\n        kernel_size=(shape[0], shape[1]),\n        strides=strides,\n        padding=padding,\n        depth_multiplier=1,\n        data_format=fmt,\n        use_bias=use_bias,\n        depthwise_initializer=kernel_init,\n        bias_initializer=bias_init,\n    )\n\n    return layer(x_in)\n\n\ndef _keras_dense(converter, interiors, inputs):\n    x_in = converter.outputs[inputs[0]]\n\n    kernel = interiors[""kernel""]\n    k = _nodef_to_private_pond(converter, kernel)\n    try:\n        bias = interiors[""bias""]\n        b = _nodef_to_private_pond(converter, bias)\n    except KeyError:\n        b = None\n\n    input_shape = x_in.shape.as_list()\n    shape = [i.size for i in kernel.attr[""value""].tensor.tensor_shape.dim]\n\n    layer = Dense(input_shape, out_features=shape[1])\n\n    layer.initialize(initial_weights=k, initial_bias=b)\n    out = layer.forward(x_in)\n\n    return out\n\n\ndef _keras_batchnorm(converter, interiors, inputs):\n    x_in = converter.outputs[inputs[0]]\n\n    bn_op = interiors[""FusedBatchNorm""]\n    fmt = bn_op.attr[""data_format""].s.decode(""ascii"")\n\n    gamma = _nodef_to_numpy_array(interiors[""gamma""])\n    gamma_init = tf.keras.initializers.Constant(gamma)\n\n    beta = _nodef_to_numpy_array(interiors[""beta""])\n    beta_init = tf.keras.initializers.Constant(beta)\n\n    moving_mean = _nodef_to_numpy_array(interiors[""moving_mean""])\n    moving_mean_init = tf.keras.initializers.Constant(moving_mean)\n\n    moving_variance = _nodef_to_numpy_array(interiors[""moving_variance""])\n    moving_variance_init = tf.keras.initializers.Constant(moving_variance)\n\n    input_shape = x_in.shape.as_list()\n\n    layer = BatchNormalization(\n        input_shape=input_shape,\n        axis=(3 if fmt == ""NHWC"" else 1),\n        gamma_initializer=gamma_init,\n        beta_initializer=beta_init,\n        moving_mean_initializer=moving_mean_init,\n        moving_variance_initializer=moving_variance_init,\n    )\n\n    return layer(x_in)\n\n\ndef _relu(converter, node: Any, inputs: List[str]) -> Any:\n    x_in = converter.outputs[inputs[0]]\n\n    return Relu(x_in.shape.as_list()).forward(x_in)\n\n\ndef _sigmoid(converter, node: Any, inputs: List[str]) -> Any:\n    x_in = converter.outputs[inputs[0]]\n\n    return Sigmoid(x_in.shape.as_list()).forward(x_in)\n\n\ndef _strided_slice(converter, node: Any, inputs: List[str]) -> Any:\n    x_in = converter.outputs[inputs[0]]\n\n    if isinstance(x_in, tf.NodeDef):\n        input_out = _nodef_to_private_pond(converter, x_in)\n    else:\n        input_out = x_in\n\n    begin = converter.outputs[inputs[1]]\n    end = converter.outputs[inputs[2]]\n    strides = converter.outputs[inputs[3]]\n\n    begin_mask = node.attr[""begin_mask""].i\n    end_mask = node.attr[""end_mask""].i\n    ellipsis_mask = node.attr[""ellipsis_mask""].i\n    new_axis_mask = node.attr[""new_axis_mask""].i\n    shrink_axis_mask = node.attr[""shrink_axis_mask""].i\n\n    begin = tf.constant(begin.attr[""value""].tensor)\n    end = tf.constant(end.attr[""value""].tensor)\n    strides = tf.constant(strides.attr[""value""].tensor)\n\n    return tfe.strided_slice(\n        input_out,\n        begin,\n        end,\n        strides=strides,\n        begin_mask=begin_mask,\n        end_mask=end_mask,\n        ellipsis_mask=ellipsis_mask,\n        new_axis_mask=new_axis_mask,\n        shrink_axis_mask=shrink_axis_mask,\n    )\n\n\ndef _pack(converter, node: Any, inputs: List[str]) -> Any:\n    final_inputs = []\n\n    for x_in in inputs:\n        input_c = converter.outputs[x_in]\n        if isinstance(input_c, tf.NodeDef):\n            final_inputs.append(_nodef_to_private_pond(converter, input_c))\n        else:\n            final_inputs.append(input_c)\n\n    return tfe.stack(final_inputs, axis=node.attr[""axis""].i)\n\n\ndef _bias_add(converter, node: Any, inputs: List[str]) -> Any:\n    a = converter.outputs[inputs[0]]\n    b = converter.outputs[inputs[1]]\n\n    if isinstance(a, tf.NodeDef):\n        a_out = _nodef_to_private_pond(converter, a)\n    else:\n        a_out = a\n\n    if isinstance(b, tf.NodeDef):\n        b_out = _nodef_to_private_pond(converter, b)\n    else:\n        b_out = b\n\n    return tfe.add(a_out, b_out)\n\n\ndef _maxpool(converter, node: Any, inputs: List[str]) -> Any:\n    x_in = converter.outputs[inputs[0]]\n\n    ksize = node.attr[""ksize""].list.i\n    s = node.attr[""strides""].list.i\n\n    padding = node.attr[""padding""].s.decode(""ascii"")\n    pool_size = [ksize[1], ksize[2]]\n    strides = [s[1], s[2]]\n\n    shape = [int(i) for i in x_in.shape]\n\n    channels_first = node.attr[""data_format""].s.decode(""ascii"") == ""NCHW""\n\n    pooler = MaxPooling2D(shape, pool_size, strides, padding, channels_first)\n\n    out = pooler.forward(x_in)\n\n    return out\n\n\ndef _shape(converter, node: Any, inputs: List[str]) -> Any:\n    x_in = converter.outputs[inputs[0]]\n\n    return x_in.shape\n\n\ndef _reshape(converter, node: Any, inputs: List[str]) -> Any:\n    x_in = converter.outputs[inputs[0]]\n    shape = converter.outputs[inputs[1]]\n\n    tensor = shape.attr[""value""].tensor\n    dtype = shape.attr[""dtype""].type\n    if dtype == tf.int32:\n        nums = array.array(""i"", tensor.tensor_content)\n    elif dtype == tf.int64:\n        nums = array.array(""l"", tensor.tensor_content)\n    else:\n        raise TypeError(""Unsupported dtype for reshape shape"")\n\n    return tfe.reshape(x_in, list(nums))\n\n\ndef _transpose(converter, node: Any, inputs: List[str]) -> Any:\n    x_in = converter.outputs[inputs[0]]\n    perm = converter.outputs[inputs[1]]\n\n    tensor = perm.attr[""value""].tensor\n    shape = [i.size for i in tensor.tensor_shape.dim]\n\n    dtype = perm.attr[""dtype""].type\n    if dtype == tf.int32:\n        nums = array.array(""i"", tensor.tensor_content)\n    elif dtype == tf.int64:\n        nums = array.array(""l"", tensor.tensor_content)\n    else:\n        raise TypeError(""Unsupported dtype for transpose perm"")\n\n    return tfe.transpose(x_in, np.array(nums).reshape(shape))\n\n\ndef _expand_dims(converter, node: Any, inputs: List[str]) -> Any:\n    x_in = converter.outputs[inputs[0]]\n\n    if isinstance(x_in, tf.NodeDef):\n        input_out = _nodef_to_private_pond(converter, x_in)\n    else:\n        input_out = x_in\n\n    input_axis = converter.outputs[inputs[1]]\n    axis_attr = input_axis.attr[""value""].tensor.int_val\n    axis_val = array.array(""i"", axis_attr)[0]\n\n    return tfe.expand_dims(input_out, axis_val)\n\n\ndef _negative(converter, node: Any, inputs: List[str]) -> Any:\n    x_in = converter.outputs[inputs[0]]\n\n    if isinstance(x_in, tf.NodeDef):\n        input_out = _nodef_to_private_pond(converter, x_in)\n    else:\n        input_out = x_in\n\n    return tfe.negative(input_out)\n\n\ndef _gather(converter, node: Any, inputs: List[str]) -> Any:\n    x_in = converter.outputs[inputs[0]]\n    indices = converter.outputs[inputs[1]]\n    axis = converter.outputs[inputs[2]]\n\n    if isinstance(x_in, tf.NodeDef):\n        input_out = _nodef_to_private_pond(converter, x_in)\n    else:\n        input_out = x_in\n\n    indices_out = list(_nodef_to_numpy_array(indices))\n\n    axis_val = axis.attr[""value""].tensor.int_val[0]\n\n    return tfe.gather(input_out, indices_out, axis_val)\n\n\ndef _squeeze(converter, node: Any, inputs: List[str]) -> Any:\n    x_in = converter.outputs[inputs[0]]\n\n    axis = node.attr[""squeeze_dims""].list.i\n\n    return tfe.squeeze(x_in, list(axis))\n\n\ndef _split(converter, node: Any, inputs: List[str]) -> Any:\n    if node.op == ""SplitV"":\n        # node.op is SplitV when num_or_size_splits is a list\n        x_in = converter.outputs[inputs[0]]\n        size_splits = converter.outputs[inputs[1]]\n        axis = converter.outputs[inputs[2]]\n\n        size_splits = size_splits.attr[""value""].tensor\n        num_or_size_splits = list(array.array(""I"", size_splits.tensor_content))\n\n    else:\n        # node.op is Split when num_or_size_splits is an integer\n        axis = converter.outputs[inputs[0]]\n        x_in = converter.outputs[inputs[1]]\n\n        num_or_size_splits = node.attr[""num_split""].i\n\n    if isinstance(x_in, tf.NodeDef):\n        input_out = _nodef_to_private_pond(converter, x_in)\n    else:\n        input_out = x_in\n\n    axis_val = axis.attr[""value""].tensor.int_val[0]\n\n    return tfe.split(input_out, num_or_size_splits, axis_val)\n\n\ndef _pad(converter, node: Any, inputs: List[str]) -> Any:\n    x_in = converter.outputs[inputs[0]]\n    p = converter.outputs[inputs[1]]\n\n    paddings_t = p.attr[""value""].tensor\n\n    paddings_arr = list(array.array(""I"", paddings_t.tensor_content))\n    paddings_lst = [paddings_arr[i : i + 2] for i in range(0, len(paddings_arr), 2)]\n\n    return tfe.pad(x_in, paddings_lst)\n\n\ndef _rsqrt(converter, node: Any, inputs: List[str]) -> Any:\n    x_in = converter.outputs[inputs[0]]\n\n    if isinstance(x_in, tf.NodeDef):\n        tensor = x_in.attr[""value""].tensor\n        shape = [i.size for i in tensor.tensor_shape.dim]\n\n        dtype = x_in.attr[""dtype""].type\n        if dtype == tf.float32:\n            nums = array.array(""f"", tensor.tensor_content)\n        elif dtype == tf.float64:\n            nums = array.array(""d"", tensor.tensor_content)\n\n        else:\n            raise TypeError(""Unsupported dtype for rsqrt"")\n\n        def inputter_fn():\n            return tf.constant(1 / np.sqrt(np.array(nums).reshape(shape)))\n\n    else:\n        # XXX this is a little weird but the input into rsqrt is public and\n        # being used only for batchnorm at the moment\n        prot = tfe.get_protocol()\n        # pylint: disable=protected-access\n        decoded = prot._decode(x_in.value_on_0, True)\n\n        # pylint: enable=protected-access\n\n        def inputter_fn():\n            return tf.rsqrt(decoded)\n\n    x = tfe.define_public_input(converter.model_provider, inputter_fn)\n\n    return x\n\n\ndef _add(converter, node: Any, inputs: List[str]) -> Any:\n    a = converter.outputs[inputs[0]]\n    b = converter.outputs[inputs[1]]\n\n    if isinstance(a, tf.NodeDef):\n        a_out = _nodef_to_public_pond(converter, a)\n    else:\n        a_out = a\n\n    if isinstance(b, tf.NodeDef):\n        b_out = _nodef_to_public_pond(converter, b)\n    else:\n        b_out = b\n\n    return tfe.add(a_out, b_out)\n\n\ndef _sub(converter, node: Any, inputs: List[str]) -> Any:\n    a = converter.outputs[inputs[0]]\n    b = converter.outputs[inputs[1]]\n\n    if isinstance(a, tf.NodeDef):\n        a_out = _nodef_to_public_pond(converter, a)\n    else:\n        a_out = a\n\n    if isinstance(b, tf.NodeDef):\n        b_out = _nodef_to_public_pond(converter, b)\n    else:\n        b_out = b\n\n    return tfe.sub(a_out, b_out)\n\n\ndef _mul(converter, node: Any, inputs: List[str]) -> Any:\n    a = converter.outputs[inputs[0]]\n    b = converter.outputs[inputs[1]]\n\n    if isinstance(a, tf.NodeDef):\n        a_out = _nodef_to_public_pond(converter, a)\n    else:\n        a_out = a\n\n    if isinstance(b, tf.NodeDef):\n        b_out = _nodef_to_public_pond(converter, b)\n    else:\n        b_out = b\n\n    return tfe.mul(a_out, b_out)\n\n\ndef _avgpool(converter, node: Any, inputs: List[str]) -> Any:\n    x_in = converter.outputs[inputs[0]]\n\n    ksize = node.attr[""ksize""].list.i\n    s = node.attr[""strides""].list.i\n\n    padding = node.attr[""padding""].s.decode(""ascii"")\n    pool_size = [ksize[1], ksize[2]]\n    strides = [s[1], s[2]]\n\n    shape = [int(i) for i in x_in.shape]\n\n    channels_first = node.attr[""data_format""].s.decode(""ascii"") == ""NCHW""\n\n    avg = AveragePooling2D(shape, pool_size, strides, padding, channels_first)\n\n    out = avg.forward(x_in)\n\n    return out\n\n\ndef _keras_global_avgpool(converter, node: Any, inputs: List[str]) -> Any:\n    x_in = converter.outputs[inputs[0]]\n\n    content = converter.outputs[inputs[1]].attr[""value""].tensor.tensor_content\n    reduction_indices = array.array(""i"", content)\n\n    if reduction_indices == array.array(""i"", [1, 2]):\n        data_format = ""channels_last""\n    else:\n        data_format = ""channels_first""\n\n    layer = GlobalAveragePooling2D(data_format=data_format)\n\n    return layer(x_in)\n\n\ndef _keras_global_maxpool(converter, node: Any, inputs: List[str]) -> Any:\n    x_in = converter.outputs[inputs[0]]\n\n    content = converter.outputs[inputs[1]].attr[""value""].tensor.tensor_content\n    reduction_indices = array.array(""i"", content)\n\n    if reduction_indices == array.array(""i"", [1, 2]):\n        data_format = ""channels_last""\n    else:\n        data_format = ""channels_first""\n\n    layer = GlobalMaxPooling2D(data_format=data_format)\n\n    return layer(x_in)\n\n\ndef _concat(converter, node: Any, inputs: List[str]) -> Any:\n    input_list = [converter.outputs[inputs[i]] for i in range(len(inputs) - 1)]\n    axis = converter.outputs[inputs[-1]]\n    axis_int = axis.attr[""value""].tensor.int_val[0]\n\n    return tfe.concat(input_list, axis_int)\n\n\ndef _batch_to_space_nd(converter, node, inputs):\n    x_in = converter.outputs[inputs[0]]\n    block_shape = converter.outputs[inputs[1]].attr[""value""].tensor\n    crops = converter.outputs[inputs[2]].attr[""value""].tensor\n\n    return tfe.batch_to_space_nd(x_in, block_shape, crops)\n\n\ndef _space_to_batch_nd(converter, node, inputs):\n    x_in = converter.outputs[inputs[0]]\n    block_shape = converter.outputs[inputs[1]].attr[""value""].tensor\n    paddings = converter.outputs[inputs[2]].attr[""value""].tensor\n\n    return tfe.space_to_batch_nd(x_in, block_shape, paddings)\n\n\ndef _flatten(converter, node, inputs):\n    x_in = converter.outputs[inputs[0]]\n\n    shape = x_in.shape.as_list()\n    non_batch = 1\n    for dim in shape[1:]:\n        non_batch *= dim\n\n    return tfe.reshape(x_in, [-1, non_batch])\n\n\ndef _required_space_to_batch_paddings(converter, node, inputs: List[str]):\n\n    inputs_node = [converter.outputs[inputs[i]] for i in range(len(inputs))]\n    inputs_int32 = []\n    for x_in in inputs_node:\n        pvt_check = isinstance(x_in, PondPrivateTensor)\n        msk_check = isinstance(x_in, PondMaskedTensor)\n        if pvt_check or msk_check:\n            logging.warning(\n                (\n                    ""Revealing private input: ""\n                    ""required_space_to_batch_paddings assumes public ""\n                    ""input.""\n                )\n            )\n            inputs_int32.append(tf.cast(x_in.reveal().decode(), tf.int32))\n        elif isinstance(x_in, tf.NodeDef):\n            inputs_int32.append(_nodef_to_numpy_array(x_in))\n        else:\n            raise TypeError(""Unexpected input of type {}."".format(type(x_in)))\n\n    if len(inputs_int32) == 2:\n        input_shape, block_shape = inputs_int32\n\n        def inputter_pad():\n            pads, _ = tf.required_space_to_batch_paddings(input_shape, block_shape)\n            return tf.cast(pads, tf.float64)\n\n        def inputter_crop():\n            _, crops = tf.required_space_to_batch_paddings(input_shape, block_shape)\n            return tf.cast(crops, tf.float64)\n\n    else:\n        base_paddings, input_shape, block_shape = inputs_int32\n\n        def inputter_pad():\n            pads, _ = tf.required_space_to_batch_paddings(\n                input_shape, block_shape, base_paddings=base_paddings,\n            )\n            return tf.cast(pads, tf.float64)\n\n        def inputter_crop():\n            _, crops = tf.required_space_to_batch_paddings(\n                input_shape, block_shape, base_paddings=base_paddings,\n            )\n            return tf.cast(crops, tf.float64)\n\n    pad_private = tfe.define_public_input(converter.model_provider, inputter_pad,)\n    crop_private = tfe.define_public_input(converter.model_provider, inputter_crop,)\n\n    return (pad_private, crop_private)\n\n\ndef _argmax(converter, node, inputs):\n    x_in = converter.outputs[inputs[0]]\n    axis = converter.outputs[inputs[1]].attr[""value""].tensor.int_val[0]\n\n    return tfe.argmax(x_in, axis=axis)\n\n\ndef _slice(converter, node, inputs):\n    x_in = converter.outputs[inputs[0]]\n    begin = _nodef_to_numpy_array(converter.outputs[inputs[1]])\n    size = _nodef_to_numpy_array(converter.outputs[inputs[2]])\n\n    if isinstance(x_in, tf.NodeDef):\n        input_out = _nodef_to_private_pond(converter, x_in)\n    else:\n        input_out = x_in\n\n    # Slice is a special case of strided_slice. Slice takes size (the number of\n    # elements we want to slice) as an input. However strided_slice takes end\n    # (integer until which the slicing takes place) as input.\n    # We can infere the end parameter with : end[i] = begin[i] + size[i].\n    # If size is negative, the stepping go towards smaller indices.\n    # In this case we can infer the end parameter with:\n    # end[i] = input_shape[i] - size[i] + 1\n    end = np.zeros(len(begin))\n    input_shape = x_in.shape.as_list()\n\n    # if size is negative take the input dimension\n    for i in range(len(end)):  # pylint: disable=consider-using-enumerate\n        if size[i] < 0:\n            end[i] = input_shape[i] - size[i] + 1\n        else:\n            end[i] = begin[i] + size[i]\n\n    return tfe.strided_slice(input_out, begin, end)\n\n\n# pylint: enable=unused-argument\n# pylint: enable=missing-docstring\ndef _nodef_to_public_pond(converter, x):\n    """"""Map a NodeDef x to a PublicPondTensor.""""""\n    dtype = x.attr[""dtype""].type\n    x_shape = [i.size for i in x.attr[""value""].tensor.tensor_shape.dim]\n\n    if not x_shape:\n        if dtype == tf.float32:\n            nums = x.attr[""value""].tensor.float_val\n        elif dtype == tf.float64:\n            nums = x.attr[""value""].tensor.float_val\n        elif dtype == tf.int32:\n            nums = x.attr[""value""].tensor.int_val\n        else:\n            raise TypeError(""Unsupported dtype"")\n\n        def inputter_fn():\n            return tf.constant(np.array(nums).reshape(1, 1))\n\n    else:\n        if dtype == tf.float32:\n            nums = array.array(""f"", x.attr[""value""].tensor.tensor_content)\n        elif dtype == tf.float64:\n            nums = array.array(""d"", x.attr[""value""].tensor.tensor_content)\n        elif dtype == tf.int32:\n            nums = array.array(""i"", x.attr[""value""].tensor.tensor_content)\n        else:\n            raise TypeError(""Unsupported dtype"")\n\n        def inputter_fn():\n            return tf.constant(np.array(nums).reshape(x_shape))\n\n    x_public = tfe.define_public_input(converter.model_provider, inputter_fn)\n\n    return x_public\n\n\ndef _nodef_to_private_pond(converter, x):\n    """"""Map a NodeDef x to a PrivatePondTensor.""""""\n    dtype = x.attr[""dtype""].type\n    warn_msg = ""Unexpected dtype {} found at node {}""\n    err_msg = ""Unsupported dtype {} found at node {}""\n\n    x_shape = [i.size for i in x.attr[""value""].tensor.tensor_shape.dim]\n\n    if not x_shape:\n        if dtype == tf.float32:\n            nums = x.attr[""value""].tensor.float_val\n        elif dtype == tf.float64:\n            nums = x.attr[""value""].tensor.float_val\n        elif dtype == tf.int32:\n            logging.warning(warn_msg, dtype, x.name)\n            nums = x.attr[""value""].tensor.int_val\n        else:\n            raise TypeError(err_msg.format(dtype, x.name))\n\n        def inputter_fn():\n            return tf.constant(np.array(nums).reshape(1, 1))\n\n    else:\n        if dtype == tf.float32:\n            nums = array.array(""f"", x.attr[""value""].tensor.tensor_content)\n        elif dtype == tf.float64:\n            nums = array.array(""d"", x.attr[""value""].tensor.tensor_content)\n        elif dtype == tf.int32:\n            logging.warning(warn_msg, dtype, x.name)\n            nums = array.array(""i"", x.attr[""value""].tensor.tensor_content)\n        else:\n            raise TypeError(err_msg.format(dtype, x.name))\n\n        def inputter_fn():\n            return tf.constant(np.array(nums).reshape(x_shape))\n\n    x_private = tfe.define_private_input(converter.model_provider, inputter_fn)\n\n    return x_private\n\n\ndef _nodef_to_numpy_array(x):\n    """"""Map a NodeDef x to a np.array.""""""\n    dtype = x.attr[""dtype""].type\n    x_shape = [i.size for i in x.attr[""value""].tensor.tensor_shape.dim]\n\n    content = x.attr[""value""].tensor.tensor_content\n\n    if dtype == tf.float32:\n        type_code = ""f""\n        if not content:\n            content = x.attr[""value""].tensor.float_val\n    elif dtype == tf.float64:\n        type_code = ""d""\n        if not content:\n            content = x.attr[""value""].tensor.double_val\n    elif dtype == tf.int32:\n        type_code = ""i""\n        if not content:\n            content = x.attr[""value""].tensor.int_val\n    else:\n        raise TypeError(""Unsupported dtype"")\n\n    nums = array.array(type_code, content)\n\n    return np.array(nums).reshape(x_shape)\n'"
tf_encrypted/keras/__init__.py,0,"b'""""""Higher-level layer abstractions built on TF Encrypted.""""""\nfrom __future__ import absolute_import\n\nfrom tf_encrypted.keras import backend\nfrom tf_encrypted.keras import engine\nfrom tf_encrypted.keras import layers\nfrom tf_encrypted.keras import losses\nfrom tf_encrypted.keras import models\nfrom tf_encrypted.keras.models import Sequential\n\n__all__ = [\n    ""backend"",\n    ""engine"",\n    ""layers"",\n    ""losses"",\n    ""models"",\n    ""Sequential"",\n]\n'"
tf_encrypted/keras/activations.py,0,"b'# pylint: disable=inconsistent-return-statements\n""""""Provide activation functions""""""\nimport tf_encrypted as tfe\n\n\ndef relu(x):\n    """"""Computes relu of x element-wise""""""\n    return tfe.relu(x)\n\n\ndef sigmoid(x):\n    """"""Computes sigmoid of x element-wise""""""\n    return tfe.sigmoid(x)\n\n\ndef sigmoid_deriv(y, d_y):\n    """"""Computes derive sigmoid of y""""""\n    return d_y * y * (tfe.negative(y) + 1)\n\n\ndef tanh(x):\n    """"""Computes tanh of x element-wise""""""\n    return tfe.tanh(x)\n\n\ndef linear(x):\n    return x\n\n\ndef get(identifier):\n    """"""get the activation function""""""\n    if identifier is None:\n        return linear\n    if callable(identifier):\n        return identifier\n    if isinstance(identifier, str):\n        activations = {\n            ""relu"": relu,\n            ""sigmoid"": sigmoid,\n            ""tanh"": tanh,\n            ""linear"": linear,\n        }\n        return activations[identifier]\n\n\ndef get_deriv(identifier):\n    """"""get the activation derivative function""""""\n    if identifier is None:\n        return linear\n    if callable(identifier):\n        raise NotImplementedError(\n            ""During training, please use a string ""\n            \'(e.g ""relu"") to specify the activation \'\n            ""function instead of calling directly ""\n            ""the activation function.""\n        )\n    if isinstance(identifier, str):\n        activations = {""sigmoid"": sigmoid_deriv}\n        if identifier not in activations.keys():\n            raise NotImplementedError(\n                ""Activation function {} not yet implemented ""\n                ""during training"".format(identifier)\n            )\n        return activations[identifier]\n\n    raise ValueError(""Could not interpret activation function identifier:"", identifier)\n'"
tf_encrypted/keras/backend.py,1,"b'""""""TFE Keras backend.\nMost of the code was borrowed from the tf.keras codebase.\n""""""\n\nimport threading\n\nfrom tensorflow.python.framework import ops\n\nimport tf_encrypted as tfe\n\n# This is a thread local object that will hold the default internal TFE session\n# used by TFE Keras. It can be set manually via `set_session(sess)`.\n_SESSION = threading.local()\n\n\ndef get_session(op_input_list=()):\n    """"""Returns the session object for the current thread.""""""\n    global _SESSION\n\n    def valid_session(session):\n        if session is None:\n            return False\n        if not isinstance(session, tfe.Session):\n            return False\n        if session.graph is not _current_graph(op_input_list):\n            return False\n        return True\n\n    if ops.inside_function():\n        raise RuntimeError(""Cannot get session inside Tensorflow graph function."")\n\n    # return any suitable session already specified\n    session = getattr(_SESSION, ""session"", None)\n    if valid_session(session):\n        return session\n\n    # return default TF session if of right type\n    session = ops.get_default_session()\n    if valid_session(session):\n        return session\n\n    # we don\'t have a suitable session, create and cache a new one\n    _SESSION.session = tfe.Session()\n    assert valid_session(_SESSION.session)\n    return _SESSION.session\n\n\ndef _current_graph(op_input_list):\n    """"""Return the graph members of `op_input_list`, or the current graph.""""""\n    # pylint: disable=protected-access\n    return ops._get_graph_from_inputs(op_input_list)\n\n\ndef set_session(session):\n    """"""Sets the global TFE session.\n    Arguments:\n        session: A TFE Session.\n    """"""\n    global _SESSION\n    _SESSION.session = session\n\n\ndef clear_session():\n    """"""Destroys the current TFE graph and creates a new one""""""\n    _SESSION.session = None\n    ops.reset_default_graph()\n'"
tf_encrypted/keras/losses.py,0,"b'""""""TFE Keras loss function""""""\nimport tf_encrypted as tfe\n\n\nclass Loss:\n    """"""Loss base class.""""""\n\n    def __init__(self, loss_fn, **kwargs):\n\n        self.loss_fn = loss_fn\n        self._fn_kwargs = kwargs\n\n    def call(self, y_true, y_pred):\n        """"""Invokes the `LossFunctionWrapper` instance.\n        Args:\n            y_true: Ground truth values.\n            y_pred: The predicted values.\n        Returns:\n            Loss values per sample.\n        """"""\n        return self.loss_fn(y_true, y_pred, **self._fn_kwargs)\n\n    def __call__(self, y_true, y_pred):\n        """"""Invokes the `Loss` instance.\n\n    Args:\n      y_true: Ground truth values.\n      y_pred: The predicted values.\n    """"""\n        return self.call(y_true, y_pred)\n\n\nclass BinaryCrossentropy(Loss):\n    """"""Computes the cross-entropy loss between true labels and predicted labels.\n\n    Args:\n        from_logits: Whether to interpret `y_pred` as a tensor of\n        [logit](https://en.wikipedia.org/wiki/Logit) values. By default we assume\n            that `y_pred` contains probabilities (i.e., values in [0, 1]).\n    """"""\n\n    def __init__(self, from_logits=False):\n        self.from_logits = from_logits\n        if from_logits:\n            super(BinaryCrossentropy, self).__init__(binary_crossentropy_from_logits)\n        else:\n            super(BinaryCrossentropy, self).__init__(binary_crossentropy)\n\n    def grad(self, y_true, y_pred):\n        if self.from_logits:\n            grad = tfe.sigmoid(y_pred) - y_true\n        else:\n            grad = y_pred - y_true\n        return grad\n\n\ndef binary_crossentropy(y_true, y_pred):\n    batch_size = y_true.shape.as_list()[0]\n    batch_size_inv = 1 / batch_size\n    out = y_true * tfe.log(y_pred)\n    out += (1 - y_true) * tfe.log(1 - y_pred)\n    out = out.negative()\n    bce = out.reduce_sum(axis=0) * batch_size_inv\n    return bce\n\n\ndef binary_crossentropy_from_logits(y_true, y_pred):\n    y_pred = tfe.sigmoid(y_pred)\n    return binary_crossentropy(y_true, y_pred)\n\n\nclass MeanSquaredError(Loss):\n    """"""Computes the MSE loss between true labels and predicted labels.""""""\n\n    def __init__(self):\n        super(MeanSquaredError, self).__init__(mean_squared_error)\n\n    def grad(self, y_true, y_pred):\n        batch_size = y_true.shape.as_list()[0]\n        batch_size_inv = 1 / batch_size\n        return 2 * (y_pred - y_true) * batch_size_inv\n\n\ndef mean_squared_error(y_true, y_pred):\n    batch_size = y_true.shape.as_list()[0]\n    batch_size_inv = 1 / batch_size\n    out = y_true - y_pred\n    out = out.square()\n    mse_loss = out.reduce_sum(axis=0) * batch_size_inv\n    return mse_loss\n'"
tf_encrypted/keras/losses_test.py,24,"b'# pylint: disable=missing-docstring\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\n\nnp.random.seed(42)\ntf.random.set_random_seed(42)\n\n\nclass TestLosses(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_binary_crossentropy(self):\n\n        y_true_np = np.array([1, 1, 0, 0]).astype(float)\n        y_pred_np = np.array([0.9, 0.1, 0.9, 0.1]).astype(float)\n\n        y_true = tfe.define_private_variable(y_true_np)\n        y_pred = tfe.define_private_variable(y_pred_np)\n\n        loss = tfe.keras.losses.BinaryCrossentropy()\n        out = loss(y_true, y_pred)\n        der_for_y_pred = loss.grad(y_true, y_pred)\n\n        with tfe.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            actual = sess.run(out.reveal())\n            actual_der = sess.run(der_for_y_pred.reveal())\n\n        tf.reset_default_graph()\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            y_true = tf.convert_to_tensor(y_true_np)\n            y_pred = tf.convert_to_tensor(y_pred_np)\n            loss = tf.keras.losses.BinaryCrossentropy()\n            out = loss(y_true, y_pred)\n            der_for_y_pred = y_true * (y_pred - 1) + (1 - y_true) * y_pred\n\n            expected = sess.run(out)\n            expected_der = sess.run(der_for_y_pred)\n\n        np.testing.assert_allclose(actual, expected, rtol=1e-1, atol=1e-1)\n        np.testing.assert_allclose(actual_der, expected_der, rtol=1e-1, atol=1e-1)\n\n    def test_binary_crossentropy_from_logits(self):\n\n        y_true_np = np.array([1, 1, 0, 0]).astype(float)\n        y_pred_np = np.array([0.9, 0.1, 0.9, 0.1]).astype(float)\n\n        y_true = tfe.define_private_variable(y_true_np)\n        y_pred = tfe.define_private_variable(y_pred_np)\n\n        loss = tfe.keras.losses.BinaryCrossentropy(from_logits=True)\n        out = loss(y_true, y_pred)\n        der_for_y_pred = loss.grad(y_true, y_pred)\n\n        with tfe.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            actual = sess.run(out.reveal())\n            actual_der = sess.run(der_for_y_pred.reveal())\n\n        tf.reset_default_graph()\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            y_true = tf.convert_to_tensor(y_true_np)\n            y_pred = tf.convert_to_tensor(y_pred_np)\n            loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n            out = loss(y_true, y_pred)\n            der_for_y_pred = tf.sigmoid(y_pred) - y_true\n\n            expected = sess.run(out)\n            expected_der = sess.run(der_for_y_pred)\n\n        np.testing.assert_allclose(actual, expected, rtol=1e-1, atol=1e-1)\n        np.testing.assert_allclose(actual_der, expected_der, rtol=1e-1, atol=1e-1)\n\n    def test_mean_squared_error(self):\n        y_true_np = np.array([1, 2, 3, 4]).astype(float)\n        y_pred_np = np.array([0.9, 2.1, 3.2, 4.1]).astype(float)\n\n        y_true = tfe.define_private_variable(y_true_np)\n        y_pred = tfe.define_private_variable(y_pred_np)\n\n        loss = tfe.keras.losses.MeanSquaredError()\n        out = loss(y_true, y_pred)\n\n        with tfe.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            actual = sess.run(out.reveal())\n\n        tf.reset_default_graph()\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            y_true = tf.convert_to_tensor(y_true_np)\n            y_pred = tf.convert_to_tensor(y_pred_np)\n\n            loss = tf.keras.losses.MeanSquaredError()\n            out = loss(y_true, y_pred)\n            expected = sess.run(out)\n\n        np.testing.assert_allclose(actual, expected, rtol=1e-1, atol=1e-1)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/keras/optimizers.py,0,"b'""""""TFE Keras optimizers""""""\nimport tf_encrypted as tfe\nfrom tf_encrypted.keras import backend as KE\n\n\nclass SGD:\n    """"""Stochastic gradient descent optimizer.\n\n    Arguments:\n        lr: float >= 0. Learning rate.\n    """"""\n\n    def __init__(self, lr=0.01):\n        self.lr = lr\n\n    def apply_gradients(self, var, grad):\n        sess = KE.get_session()\n        for i, w in enumerate(var):\n            sess.run(tfe.assign(w, w - grad[i] * self.lr))\n\n\n_known_optimizers = {\n    ""sgd"": SGD,\n}\n\n\ndef get(identifier):\n    if isinstance(identifier, type):\n        return identifier()\n    if isinstance(identifier, str):\n        global _known_optimizers\n        optimizer = _known_optimizers.get(identifier, None)\n        if optimizer is not None:\n            return optimizer()\n    return identifier\n'"
tf_encrypted/keras/testing_utils.py,8,"b'""""""Testing utilities for tfe.keras.""""""\nimport numpy as np\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.keras.models import Sequential\n\n\ndef agreement_test(\n    tfe_layer_cls,\n    kwargs=None,\n    input_shape=None,\n    input_data=None,\n    rtol=1e-1,\n    atol=1e-8,\n    **tfe_kwargs,\n):\n    """"""Check agreement between a tf.keras layer and a tfe.keras layer.\n    Arguments:\n      tfe_layer_cls: Layer class object (from tfe.keras).\n      kwargs: Optional dictionary of keyword arguments for instantiating the\n          layers.\n      input_shape: Input shape tuple.\n      input_data: Numpy array of input data.\n      expected_output: Shape tuple for the expected shape of the output.\n      tfe_kwargs: Additional kwargs to supply the tfe.keras Layer not included in\n          the argspec of the original tf.keras Layer object.\n    Raises:\n        ValueError: if `input_data is None and input_shape is None`.\n    """"""\n    input_shape, input_data = _sanitize_testing_args(input_shape, input_data)\n\n    tf_layer_cls = getattr(tf.keras.layers, tfe_layer_cls.__name__)\n    tfe_kwargs = {**kwargs, **tfe_kwargs}\n\n    with tfe.protocol.SecureNN():\n        tfe_layer = tfe_layer_cls(**tfe_kwargs)\n        x = tfe.define_private_variable(input_data)\n        y = tfe_layer(x)\n\n        with tfe.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            actual = sess.run(y.reveal())\n\n    tf.reset_default_graph()\n\n    with tf.Session() as sess:\n        tf_layer = tf_layer_cls(**kwargs)\n        x = tf.Variable(input_data, dtype=tf.float32)\n        y = tf_layer(x)\n        sess.run(tf.global_variables_initializer())\n        expected = sess.run(y)\n\n    np.testing.assert_allclose(actual, expected, rtol=rtol, atol=atol)\n\n\ndef layer_test(layer_cls, kwargs=None, batch_input_shape=None, input_data=None):\n    """"""Test routine for a layer with a single input and single output.\n    Arguments:\n      layer_cls: Layer class object.\n      kwargs: Optional dictionary of keyword arguments for instantiating the\n        layer.\n      input_shape: Input shape tuple.\n      input_dtype: Data type of the input data.\n      input_data: Numpy array of input data.\n    Returns:\n      The output data (Numpy array) returned by the layer, for additional\n      checks to be done by the calling code.\n    Raises:\n      ValueError: if `input_data is None and input_shape is None`.\n    """"""\n    input_shape, input_data = _sanitize_testing_args(batch_input_shape, input_data)\n\n    # instantiation\n    kwargs = kwargs or {}\n\n    with tfe.protocol.SecureNN():\n        layer = layer_cls(batch_input_shape=input_shape, **kwargs)\n        model = Sequential()\n        model.add(layer)\n\n        x = tfe.define_private_variable(input_data)\n        model(x)\n\n\ndef _sanitize_testing_args(input_shape, input_data):\n    """"""Construct appropriate values for input_shape and input_data whenever one\n    is missing.""""""\n    if input_data is None:\n        if input_shape is None:\n            raise ValueError(""input_shape is None"")\n        input_data_shape = list(input_shape)\n        for i, e in enumerate(input_data_shape):\n            if e is None:\n                input_data_shape[i] = 2\n        input_data = 10 * np.random.random(input_data_shape)\n    elif input_shape is None:\n        input_shape = input_data.shape\n    return input_shape, input_data\n'"
tf_encrypted/layers/__init__.py,0,"b'""""""Higher-level layer abstractions built on TF Encrypted.""""""\nfrom __future__ import absolute_import\n\nfrom .activation import Relu\nfrom .activation import Sigmoid\nfrom .batchnorm import Batchnorm\nfrom .convolution import Conv2D\nfrom .dense import Dense\nfrom .pooling import AveragePooling2D\nfrom .pooling import MaxPooling2D\nfrom .reshape import Reshape\n\n__all__ = [\n    ""AveragePooling2D"",\n    ""MaxPooling2D"",\n    ""Conv2D"",\n    ""Dense"",\n    ""Sigmoid"",\n    ""Relu"",\n    ""Batchnorm"",\n    ""Reshape"",\n]\n'"
tf_encrypted/layers/activation.py,3,"b'# pylint: disable=arguments-differ\n""""""Various activation functions implemented as Layer objects.""""""\nfrom typing import List\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.layers.core import Layer\n\nbackward_msg = ""`backward` is not implemented for layer {}""\n\n\nclass Sigmoid(Layer):\n    """"""\n    Sigmoid Layer\n\n    :See: tf.nn.Sigmoid\n    """"""\n\n    def get_output_shape(self) -> List[int]:\n        return self.input_shape\n\n    def initialize(self):\n        pass\n\n    def forward(self, x):\n        y = tfe.sigmoid(x)\n        self.layer_output = y\n        return y\n\n    def backward(self, d_y):\n        y = self.layer_output\n        d_x = d_y * y * (y.neg() + 1)\n        return d_x\n\n\nclass Relu(Layer):\n    """"""\n    Relu Layer\n\n    :See: tf.nn.relu\n    """"""\n\n    def get_output_shape(self) -> List[int]:\n        return self.input_shape\n\n    def initialize(self, *args, **kwargs) -> None:\n        pass\n\n    def forward(self, x):\n        """"""\n        :param ~tf_encrypted.protocol.pond.PondTensor x: The input tensor\n        :rtype: ~tf_encrypted.protocol.pond.PondTensor\n        :returns: A pond tensor with the same backing type as the input tensor.\n        """"""\n        y = tfe.relu(x)\n        self.layer_output = y\n        return y\n\n    # TODO Approximate Relu derivate to implement backward\n    def backward(self, d_y, *args):\n        """"""\n        `backward` is not implemented for `Relu`\n\n        :raises: NotImplementedError\n        """"""\n        raise NotImplementedError(backward_msg.format(""Relu""))\n\n\nclass Tanh(Layer):\n    """"""\n    Tanh Layer\n\n    :See: tf.nn.tanh\n    """"""\n\n    def get_output_shape(self) -> List[int]:\n        return self.input_shape\n\n    def initialize(self, *args, **kwargs) -> None:\n        pass\n\n    def forward(self, x):\n        y = tfe.tanh(x)\n        self.layer_output = y\n        return y\n\n    # TODO Approximate Relu derivate to implement backward\n    def backward(self, d_y, *args):\n        """"""\n        `backward` is not implemented for `Tanh`\n\n        :raises: NotImplementedError\n        """"""\n        raise NotImplementedError(backward_msg.format(""Tanh""))\n'"
tf_encrypted/layers/activation_test.py,22,"b'# pylint: disable=missing-docstring\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.layers.activation import Relu\nfrom tf_encrypted.layers.activation import Sigmoid\nfrom tf_encrypted.layers.activation import Tanh\n\n\nclass TestRelu(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_forward(self):\n        input_shape = [2, 2, 2, 50]\n        input_relu = (\n            np.random.randn(np.prod(input_shape))\n            .astype(np.float32)\n            .reshape(input_shape)\n        )\n\n        with tfe.protocol.SecureNN() as prot:\n\n            tf.reset_default_graph()\n\n            relu_input = prot.define_private_variable(input_relu)\n            relu_layer = Relu(input_shape)\n            relu_out_pond = relu_layer.forward(relu_input)\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                out_pond = sess.run(relu_out_pond.reveal(), tag=""tfe"")\n\n            tf.reset_default_graph()\n\n            x = tf.Variable(input_relu, dtype=tf.float32)\n            relu_out_tf = tf.nn.relu(x)\n            with tf.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                out_tensorflow = sess.run(relu_out_tf)\n\n            np.testing.assert_allclose(out_pond, out_tensorflow, atol=0.01)\n\n\nclass TestSigmoid(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_forward(self):\n        input_shape = [4]\n        input_sigmoid = np.array([-1.0, -0.5, 0.5, 3.0]).astype(np.float32)\n\n        # sigmoid pond\n        with tfe.protocol.Pond() as prot:\n\n            sigmoid_input = prot.define_private_variable(input_sigmoid)\n            sigmoid_layer = Sigmoid(input_shape)\n\n            sigmoid_out_pond = sigmoid_layer.forward(sigmoid_input)\n\n            with tfe.Session() as sess:\n\n                sess.run(tf.global_variables_initializer())\n                # outputs\n                out_pond = sess.run(sigmoid_out_pond.reveal())\n\n            # reset graph\n            tf.reset_default_graph()\n\n            with tf.Session() as sess:\n                x = tf.Variable(input_sigmoid, dtype=tf.float32)\n\n                sigmoid_out_tf = tf.nn.sigmoid(x)\n\n                sess.run(tf.global_variables_initializer())\n\n                out_tensorflow = sess.run(sigmoid_out_tf)\n\n        assert np.isclose(out_pond, out_tensorflow, atol=0.6).all()\n\n\nclass TestTanh(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_forward(self):\n        input_shape = [4]\n        input_tanh = np.array([-1.0, -0.5, 0.5, 3.0]).astype(np.float32)\n\n        # tanh pond\n        with tfe.protocol.Pond() as prot:\n\n            tanh_input = prot.define_private_variable(input_tanh)\n            tanh_layer = Tanh(input_shape)\n\n            tanh_out_pond = tanh_layer.forward(tanh_input)\n\n            with tfe.Session() as sess:\n\n                sess.run(tf.global_variables_initializer())\n                # outputs\n                out_pond = sess.run(tanh_out_pond.reveal())\n\n            # reset graph\n            tf.reset_default_graph()\n\n            with tf.Session() as sess:\n                x = tf.Variable(input_tanh, dtype=tf.float32)\n\n                tanh_out_tf = tf.nn.tanh(x)\n\n                sess.run(tf.global_variables_initializer())\n\n                out_tensorflow = sess.run(tanh_out_tf)\n\n        assert np.isclose(out_pond, out_tensorflow, atol=0.2).all()\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/layers/batchnorm.py,0,"b'# pylint: disable=arguments-differ\n""""""Layer implementaiton for batch normalization.""""""\nfrom typing import List\n\nimport numpy as np\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.layers.core import Layer\n\n\nclass Batchnorm(Layer):\n    """"""\n    Batch Normalization Layer\n\n    :param List[int] input_shape: input shape of the data flowing into the layer\n    :param np.ndarray mean: ...\n    :param np.ndarray variance: ...\n    :param np.ndarray scale: ...\n    :param np.ndarray offset: ...\n    :param float variance_epsilon: ...\n    """"""\n\n    def __init__(\n        self,\n        input_shape: List[int],\n        mean: np.ndarray,\n        variance: np.ndarray,\n        scale: np.ndarray,\n        offset: np.ndarray,\n        variance_epsilon: float = 1e-8,\n        channels_first: bool = True,\n    ) -> None:\n        self.mean = mean\n        self.variance = variance\n        self.scale = scale\n        self.offset = offset\n        self.variance_epsilon = variance_epsilon\n        self.channels_first = channels_first\n        self.denom = None\n\n        super(Batchnorm, self).__init__(input_shape)\n\n    def get_output_shape(self) -> List[int]:\n        return self.input_shape\n\n    def initialize(self) -> None:\n        # Batchnorm after Dense layer\n        if len(self.input_shape) == 2:\n            n, _ = self.input_shape\n            self.mean = self.mean.reshape(1, n)\n            self.variance = self.variance.reshape(1, n)\n            self.scale = self.scale.reshape(1, n)\n            self.offset = self.offset.reshape(1, n)\n\n        # Batchnorm after Conv2D layer\n        elif len(self.input_shape) == 4:\n            if self.channels_first:\n                # NCHW format\n                _, c, _, _ = self.input_shape\n                self.mean = self.mean.reshape(1, c, 1, 1)\n                self.variance = self.variance.reshape(1, c, 1, 1)\n                self.scale = self.scale.reshape(1, c, 1, 1)\n                self.offset = self.offset.reshape(1, c, 1, 1)\n            else:\n                # NHWC format\n                _, _, _, c = self.input_shape\n                self.mean = self.mean.reshape(1, 1, 1, c)\n                self.variance = self.variance.reshape(1, 1, 1, c)\n                self.scale = self.scale.reshape(1, 1, 1, c)\n                self.offset = self.offset.reshape(1, 1, 1, c)\n\n        denomtemp = 1.0 / np.sqrt(self.variance + self.variance_epsilon)\n\n        self.denom = tfe.define_public_variable(denomtemp)\n        self.mean = tfe.define_public_variable(self.mean)\n        self.variance = tfe.define_public_variable(self.variance)\n        self.scale = tfe.define_public_variable(self.scale)\n        self.offset = tfe.define_public_variable(self.offset)\n\n    def forward(self, x):\n        if self.scale is None and self.offset is None:\n            out = (x - self.mean) * self.denom\n        elif self.offset is None:\n            out = self.scale * (x - self.mean) * self.denom\n        elif self.scale is None:\n            out = (x - self.mean) * self.denom + self.offset\n        else:\n            out = self.scale * (x - self.mean) * self.denom + self.offset\n        return out\n\n    def backward(self) -> None:\n        """"""\n        `backward` is not implemented for `batchnorm`\n\n        :raises: NotImplementedError\n        """"""\n        raise NotImplementedError\n'"
tf_encrypted/layers/batchnorm_test.py,15,"b'# pylint: disable=missing-docstring\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.layers import Batchnorm\n\n\nclass TestBatchnorm(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_channels_first(self) -> None:\n        """"""\n        Test batch norm layer with NCHW (channels first) format\n        """"""\n        channels_first = True\n\n        batch_size, channels_in, img_height, img_width = (32, 3, 28, 28)\n\n        input_shape = [batch_size, channels_in, img_height, img_width]\n        input_batchnorm = np.random.normal(size=input_shape).astype(np.float32)\n\n        # I reshaped the input because tf.nn.batch_normalization doesn\'t reshape it\n        # automatically However tf encrypted will reshape automatically the input\n        mean = (\n            np.array([2.0, 1.5, 20.8])\n            .reshape((1, channels_in, 1, 1))\n            .astype(np.float32)\n        )\n        variance = (\n            np.array([0.5, 0.3, 0.1]).reshape((1, channels_in, 1, 1)).astype(np.float32)\n        )\n        scale = (\n            np.array([0.3, 0.5, 0.8]).reshape((1, channels_in, 1, 1)).astype(np.float32)\n        )\n        offset = (\n            np.array([1.5, 1.2, 1.4]).reshape((1, channels_in, 1, 1)).astype(np.float32)\n        )\n        variance_epsilon = 1e-8\n\n        with tfe.protocol.Pond() as prot:\n            batchnorm_input = prot.define_private_variable(input_batchnorm)\n\n            batchnorm_layer = Batchnorm(\n                input_shape,\n                mean,\n                variance,\n                scale,\n                offset,\n                channels_first=channels_first,\n            )\n            batchnorm_layer.initialize()\n            batchnorm_out_pond = batchnorm_layer.forward(batchnorm_input)\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                out_pond = sess.run(batchnorm_out_pond.reveal())\n\n            # reset graph\n            tf.reset_default_graph()\n\n            with tf.Session() as sess:\n                x = tf.Variable(input_batchnorm, dtype=tf.float32)\n\n                batchnorm_out_tf = tf.nn.batch_normalization(\n                    x, mean, variance, offset, scale, variance_epsilon\n                )\n\n                sess.run(tf.global_variables_initializer())\n\n                out_tensorflow = sess.run(batchnorm_out_tf)\n\n                np.testing.assert_array_almost_equal(\n                    out_pond, out_tensorflow, decimal=1\n                )\n\n    def test_channels_last(self) -> None:\n        """"""\n        Test batch norm layer with NHWC (channels last) format\n        """"""\n        channels_first = False\n\n        batch_size, img_height, img_width, channels_in = (32, 28, 28, 3)\n\n        input_shape = [batch_size, img_height, img_width, channels_in]\n        input_batchnorm = np.random.normal(size=input_shape).astype(np.float32)\n\n        # I reshaped the input because tf.nn.batch_normalization doesn\'t reshape it\n        # automatically However tf encrypted will reshape automatically the input\n        mean = (\n            np.array([2.0, 1.5, 20.8])\n            .reshape((1, 1, 1, channels_in))\n            .astype(np.float32)\n        )\n        variance = (\n            np.array([0.5, 0.3, 0.1]).reshape((1, 1, 1, channels_in)).astype(np.float32)\n        )\n        scale = (\n            np.array([0.3, 0.5, 0.8]).reshape((1, 1, 1, channels_in)).astype(np.float32)\n        )\n        offset = (\n            np.array([1.5, 1.2, 1.4]).reshape((1, 1, 1, channels_in)).astype(np.float32)\n        )\n        variance_epsilon = 1e-8\n\n        with tfe.protocol.Pond() as prot:\n            batchnorm_input = prot.define_private_variable(input_batchnorm)\n\n            batchnorm_layer = Batchnorm(\n                input_shape,\n                mean,\n                variance,\n                scale,\n                offset,\n                channels_first=channels_first,\n            )\n            batchnorm_layer.initialize()\n            batchnorm_out_pond = batchnorm_layer.forward(batchnorm_input)\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                out_pond = sess.run(batchnorm_out_pond.reveal())\n\n            # reset graph\n            tf.reset_default_graph()\n\n            with tf.Session() as sess:\n                x = tf.Variable(input_batchnorm, dtype=tf.float32)\n\n                batchnorm_out_tf = tf.nn.batch_normalization(\n                    x, mean, variance, offset, scale, variance_epsilon,\n                )\n\n                sess.run(tf.global_variables_initializer())\n\n                out_tensorflow = sess.run(batchnorm_out_tf)\n\n                np.testing.assert_array_almost_equal(\n                    out_pond, out_tensorflow, decimal=1\n                )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/layers/convolution.py,0,"b'# pylint: disable=arguments-differ\n""""""Layer implementation for 2d convolution.""""""\nimport numpy as np\n\nimport tf_encrypted as tfe\n\nfrom ..protocol.pond import PondMaskedTensor\nfrom ..protocol.pond import PondPrivateTensor\nfrom .core import Layer\n\n\nclass Conv2D(Layer):\n    """"""\n    2 Dimensional convolutional layer, expects NCHW data format\n\n    :param List[int] input_shape: The shape of the convolution input. Rank 4.\n    :param List[int] filter_shape: The shape of the convolution filter. Rank 4.\n    :param int strides: The size of the stride\n    :param padding str: The type of padding (""SAME"" or ""VALID"")\n    :param lambda filter_init: lambda function with shape parameter\n\n      `Example`\n\n      .. code-block:: python\n\n              Conv2D((4, 4, 1, 20), strides=2, filter_init=lambda shp:\n                      np.random.normal(scale=0.01, size=shp))\n    """"""\n\n    def __init__(\n        self,\n        input_shape,\n        filter_shape,\n        strides=1,\n        padding=""SAME"",\n        filter_init=lambda shp: np.random.normal(scale=0.1, size=shp),\n        l2reg_lambda=0.0,\n        channels_first=True,\n    ):\n        self.fshape = filter_shape\n        self.strides = strides\n        self.padding = padding\n        self.filter_init = filter_init\n        self.l2reg_lambda = l2reg_lambda\n        self.cache = None\n        self.cached_x_col = None\n        self.cached_input_shape = None\n        self.initializer = None\n        self.weights = None\n        self.bias = None\n        self.model = None\n        self.channels_first = channels_first\n\n        super(Conv2D, self).__init__(input_shape)\n\n    def get_output_shape(self):\n        """"""Compute output_shape for the layer.""""""\n        h_filter, w_filter, _, n_filters = self.fshape\n\n        if self.channels_first:\n            n_x, _, h_x, w_x = self.input_shape\n        else:\n            n_x, h_x, w_x, _ = self.input_shape\n\n        if self.padding == ""SAME"":\n            h_out = int(np.ceil(float(h_x) / float(self.strides)))\n            w_out = int(np.ceil(float(w_x) / float(self.strides)))\n        if self.padding == ""VALID"":\n            h_out = int(np.ceil(float(h_x - h_filter + 1) / float(self.strides)))\n            w_out = int(np.ceil(float(w_x - w_filter + 1) / float(self.strides)))\n\n        return [n_x, n_filters, h_out, w_out]\n\n    def initialize(self, initial_weights=None, initial_bias=None):\n        """"""Initialize layer weights as needed.""""""\n\n        def is_secret(x):\n            is_pvt = isinstance(x, PondPrivateTensor)\n            is_msk = isinstance(x, PondMaskedTensor)\n            return is_pvt or is_msk\n\n        if initial_weights is None:\n            initial_weights = self.filter_init(self.fshape)\n\n        if is_secret(initial_weights):\n            self.weights = initial_weights\n        else:\n            self.weights = tfe.define_private_variable(initial_weights)\n\n        if initial_bias is None or is_secret(initial_bias):\n            self.bias = initial_bias\n        else:\n            self.bias = tfe.define_private_variable(initial_bias)\n\n    def forward(self, x):\n        """"""Compute the forward convolution.""""""\n        self.cached_input_shape = x.shape\n        self.cache = x\n\n        if not self.channels_first:\n            x = tfe.transpose(x, perm=[0, 3, 1, 2])\n\n        out = tfe.conv2d(x, self.weights, self.strides, self.padding)\n        if self.bias is not None:\n            out = out + self.bias\n\n        if not self.channels_first:\n            out = tfe.transpose(out, perm=[0, 2, 3, 1])\n\n        return out\n\n    def backward(self, d_y, learning_rate):\n        """"""Compute the convolution derivatives.""""""\n        if not self.channels_first:\n            raise TypeError(""channels must be first on the backward pass"")\n\n        x = self.cache\n        h_filter, w_filter, _, n_filter = map(int, self.weights.shape)\n\n        if self.model.layers.index(self) != 0:\n            w_reshaped = self.weights.reshape(n_filter, -1).transpose()\n            dout_reshaped = d_y.transpose(1, 2, 3, 0).reshape(n_filter, -1)\n            dx = w_reshaped.matmul(dout_reshaped).col2im(\n                imshape=self.cached_input_shape,\n                field_height=h_filter,\n                field_width=w_filter,\n                padding=self.padding,\n                stride=self.strides,\n            )\n\n        d_w = tfe.conv2d_bw(x, d_y, self.weights.shape, self.strides, self.padding)\n        d_bias = d_y.reduce_sum(axis=0)\n\n        self.weights.assign((d_w * learning_rate).neg() + self.weights)\n        self.bias.assign((d_bias * learning_rate).neg() + self.bias)\n\n        return dx\n'"
tf_encrypted/layers/core.py,0,"b'""""""Includes base classes used by all layer types.""""""\n\nfrom abc import ABC\nfrom abc import abstractmethod\nfrom typing import List\nfrom typing import Optional\n\nfrom tf_encrypted.protocol.pond import TFEVariable\n\n# TODO\n# Split backward function in compute_gradient and compute_backpropagated_error?\n\n\nclass Layer(ABC):\n    """"""\n  Base class for all layers.\n  """"""\n\n    def __init__(self, input_shape: List[int]) -> None:\n        self.input_shape = input_shape\n        self.output_shape = self.get_output_shape()\n        self.layer_output = None\n\n    @abstractmethod\n    def get_output_shape(self) -> List[int]:\n        """"""Returns the layer\'s output shape""""""\n\n    @abstractmethod\n    def initialize(self, *args, **kwargs) -> None:\n        """"""Initialize any necessary tensors.""""""\n\n    @abstractmethod\n    def forward(self, *args, **kwargs) -> Optional[TFEVariable]:\n        """"""Forward pass for inference""""""\n\n    # TODO[jason]: @abstractmethod\n    def backward(self, *args, **kwargs) -> Optional[TFEVariable]:\n        """"""Backward pass for training.""""""\n'"
tf_encrypted/layers/dense.py,1,"b'# pylint: disable=arguments-differ\n""""""Dense (i.e. fully connected) Layer implementation.""""""\nfrom typing import Optional\nfrom typing import Union\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.layers.core import Layer\nfrom tf_encrypted.protocol.pond import PondPrivateTensor\nfrom tf_encrypted.protocol.pond import PondPublicTensor\n\nInitialTensor = Optional[\n    Union[np.ndarray, tf.Tensor, PondPublicTensor, PondPrivateTensor]\n]\n\n\nclass Dense(Layer):\n    """"""Standard dense linear layer including bias.\n\n    :param int in_features: number of input features\n    :param int out_features: number of output neurons for the layer\n    """"""\n\n    def __init__(\n        self, input_shape, out_features, transpose_input=False, transpose_weight=False,\n    ):\n        self.in_features = input_shape[-1]\n        self.out_features = out_features\n\n        self.layer_input = None\n        self.weights = None\n        self.bias = None\n\n        self.transpose_input = transpose_input\n        self.transpose_weight = transpose_weight\n\n        super(Dense, self).__init__(input_shape)\n\n    def get_output_shape(self):\n        return [self.input_shape[0] + self.out_features]\n\n    def initialize(\n        self, initial_weights: InitialTensor = None, initial_bias: InitialTensor = None,\n    ) -> None:\n        if initial_weights is None:\n            initial_size = (self.in_features, self.out_features)\n            initial_weights = np.random.normal(scale=0.1, size=initial_size)\n        if initial_bias is not None:\n            self.bias = tfe.define_private_variable(initial_bias)\n\n        self.weights = tfe.define_private_variable(initial_weights)\n\n        if self.transpose_weight:\n            self.weights = self.weights.transpose()\n\n    def forward(self, x):\n        self.layer_input = x\n\n        if self.transpose_input:\n            self.layer_input = self.layer_input.transpose()\n\n        if self.bias:\n            y = x.matmul(self.weights) + self.bias\n        else:\n            y = x.matmul(self.weights)\n        return y\n\n    def backward(self, d_y, learning_rate):\n        x = self.layer_input\n        if self.transpose_input:\n            self.layer_input = self.layer_input.transpose()\n\n        d_x = d_y.matmul(self.weights.transpose())\n\n        d_weights = x.transpose().matmul(d_y)\n        self.weights.assign((d_weights * learning_rate).neg() + self.weights)\n\n        if self.bias:\n            d_bias = d_y.reduce_sum(axis=0)\n            self.bias -= d_bias * learning_rate\n\n        return d_x\n'"
tf_encrypted/layers/pooling.py,2,"b'# pylint: disable=arguments-differ\n""""""Pooling Layer implementations.""""""\nimport math\nfrom abc import abstractmethod\nfrom typing import List\nfrom typing import Optional\nfrom typing import Tuple\nfrom typing import Union\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.layers.core import Layer\n\nfrom ..protocol.pond import TFEVariable\n\nIntTuple = Union[int, Tuple[int, int], List[int]]\n\n\nclass Pooling2D(Layer):\n    """"""\n    Base class for AveragePooling and MaxPooling layers\n\n    Do not instantiate.\n    """"""\n\n    def __init__(\n        self,\n        input_shape: List[int],\n        pool_size: IntTuple,\n        strides: Optional[IntTuple] = None,\n        padding: str = ""SAME"",\n        channels_first: bool = True,\n    ) -> None:\n        if isinstance(pool_size, int):\n            pool_size = (pool_size, pool_size)  # type: ignore\n        self.pool_size = pool_size\n        if strides is None:\n            strides = pool_size\n        elif isinstance(strides, int):\n            strides = (strides, strides)  # type: ignore\n        self.strides = strides\n        if padding not in [""SAME"", ""VALID""]:\n            raise ValueError(""Don\'t know how to do padding of type {}"".format(padding))\n        self.padding = padding\n        self.channels_first = channels_first\n\n        super(Pooling2D, self).__init__(input_shape)\n        self.cache = None\n        self.cached_input_shape = None\n\n    def initialize(\n        self, input_shape: IntTuple, initializer: Optional[TFEVariable] = None\n    ) -> None:\n        pass\n\n    def get_output_shape(self) -> List[int]:\n        if self.channels_first:\n            _, _, h_in, w_in = self.input_shape\n        else:\n            _, h_in, w_in, _ = self.input_shape\n\n        if self.padding == ""SAME"":\n            h_out = math.ceil(h_in / self.strides[0])\n            w_out = math.ceil(w_in / self.strides[1])\n        else:\n            h_out = math.ceil((h_in - self.pool_size[0] + 1) / self.strides[0])\n            w_out = math.ceil((w_in - self.pool_size[1] + 1) / self.strides[1])\n        return [self.input_shape[0], self.input_shape[1], h_out, w_out]\n\n    @abstractmethod\n    def pool(self, x: TFEVariable, pool_size, strides, padding) -> TFEVariable:\n        raise NotImplementedError\n\n    def forward(self, x: TFEVariable) -> TFEVariable:\n        if not self.channels_first:\n            x = tfe.transpose(x, perm=[0, 3, 1, 2])\n\n        self.cached_input_shape = x.shape\n        self.cache = x\n\n        out = self.pool(x, self.pool_size, self.strides, self.padding)\n\n        if not self.channels_first:\n            out = tfe.transpose(out, perm=[0, 2, 3, 1])\n\n        return out\n\n    def backward(self, d_y, learning_rate):\n        raise NotImplementedError(""`backward` not yet supported for pooling layers"")\n\n\nclass AveragePooling2D(Pooling2D):  # pylint: disable=abstract-method\n    """"""\n    AveragePooling2D\n\n    :See: tf.nn.avg_pool\n    """"""\n\n    def pool(self, x, pool_size, strides, padding):\n        return tfe.avgpool2d(x, pool_size, strides, padding)\n\n\nclass MaxPooling2D(Pooling2D):  # pylint: disable=abstract-method\n    """"""\n    MaxPooling2D\n\n    :See: tf.nn.max_pool\n    """"""\n\n    # TODO -- throw an error duing init if the protocol is not secureNN\n\n    def pool(self, x, pool_size, strides, padding):\n        return tfe.maxpool2d(x, pool_size, strides, padding)\n'"
tf_encrypted/layers/pooling_test.py,10,"b'# pylint: disable=missing-docstring\nimport unittest\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.layers import AveragePooling2D\nfrom tf_encrypted.layers import MaxPooling2D\n\n\nclass TestAveragePooling2D(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def _get_fixtures(self, even=True):\n        if even:\n            batch_size, channels_in = 2, 2\n            img_height, img_width = 8, 8\n        else:\n            batch_size, channels_in = 1, 1\n            img_height, img_width = 5, 11\n        input_shape = (batch_size, channels_in, img_height, img_width)\n\n        n_elements = batch_size * channels_in * img_height * img_width\n        input_pool = np.ones(n_elements, dtype=np.float32).reshape(input_shape)\n\n        return input_pool, input_shape\n\n    def _tf_tiled_forward(self, input_pool: np.ndarray) -> np.ndarray:\n        x = tf.constant(input_pool, dtype=tf.float32)\n        x_nhwc = tf.transpose(x, (0, 2, 3, 1))\n        ksize = [1, 2, 2, 1]\n        pool_out_tf = tf.nn.avg_pool(\n            x_nhwc, ksize=ksize, strides=ksize, padding=""VALID"", data_format=""NHWC""\n        )\n\n        with tf.Session() as sess:\n            out_tf = sess.run(pool_out_tf).transpose(0, 3, 1, 2)\n\n        return out_tf\n\n    def _generic_tiled_forward(self, t_type: str, even: bool = True) -> None:\n        assert t_type in [""public"", ""private"", ""masked""]\n        input_pool, input_shape = self._get_fixtures(even)\n\n        # pooling in pond\n        with tfe.protocol.Pond() as prot:\n            if t_type == ""public"":\n                x_in = prot.define_public_variable(input_pool)\n            elif t_type in [""private"", ""masked""]:\n                x_in = prot.define_private_variable(input_pool)\n            if t_type == ""masked"":\n                x_in = prot.mask(x_in)\n            pool = AveragePooling2D(list(input_shape), pool_size=2, padding=""VALID"")\n            pool_out_pond = pool.forward(x_in)\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                if t_type in [""private"", ""masked""]:\n                    out_pond = sess.run(pool_out_pond.reveal())\n                else:\n                    out_pond = sess.run(pool_out_pond)\n\n        # reset tf graph\n        tf.reset_default_graph()\n\n        # pooling in tf\n        out_tf = self._tf_tiled_forward(input_pool)\n\n        np.testing.assert_array_almost_equal(out_pond, out_tf, decimal=3)\n\n    def test_public_tiled_forward(self):\n        self._generic_tiled_forward(""public"", True)\n\n    def test_public_forward(self):\n        self._generic_tiled_forward(""public"", False)\n\n    def test_private_tiled_forward(self):\n        self._generic_tiled_forward(""private"")\n\n    def test_private_forward(self):\n        self._generic_tiled_forward(""private"", False)\n\n    def test_masked_tiled_forward(self):\n        self._generic_tiled_forward(""masked"")\n\n    def test_masked_forward(self):\n        self._generic_tiled_forward(""masked"", False)\n\n\n@pytest.mark.slow\nclass TestMaxPooling2D(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def tearDown(self):\n        tf.reset_default_graph()\n\n    def test_maxpool2d(self):\n        with tfe.protocol.SecureNN() as prot:\n\n            # fmt: off\n            x_in = np.array(\n                [[[\n                    [1, 2, 3, 4],\n                    [3, 2, 4, 1],\n                    [1, 2, 3, 4],\n                    [3, 2, 4, 1],\n                ]]]\n            )\n            # fmt: on\n\n            expected = np.array([[[[3, 4], [3, 4]]]], dtype=np.float64)\n\n            x = prot.define_private_variable(x_in)\n            pool = MaxPooling2D([0, 1, 4, 4], pool_size=2, padding=""VALID"")\n            result = pool.forward(x)\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                answer = sess.run(result.reveal())\n\n        assert np.array_equal(answer, expected)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/layers/reshape.py,1,"b'# pylint: disable=arguments-differ\n""""""Reshape Layer object.""""""\nfrom typing import List\n\nimport numpy as np\n\nimport tf_encrypted as tfe\n\nfrom .core import Layer\n\n\nclass Reshape(Layer):\n    """"""\n  Reshape Layer\n\n  :See: tf.keras.layers.Reshape\n  """"""\n\n    def __init__(self, input_shape, output_shape=None) -> None:\n        if output_shape is None:\n            self.output_shape = [-1]\n        self.output_shape = output_shape\n\n        super(Reshape, self).__init__(input_shape)\n\n    def get_output_shape(self) -> List[int]:\n        """"""Returns the layer\'s output shape""""""\n        if -1 not in self.output_shape:\n            return self.output_shape\n\n        total_input_dims = np.prod(self.input_shape)\n\n        dim = 1\n        for i in self.output_shape:\n            if i != -1:\n                dim *= i\n        missing_dim = int(total_input_dims / dim)\n\n        output_shape = self.output_shape\n        for ix, dim in enumerate(output_shape):\n            if dim == -1:\n                output_shape[ix] = missing_dim\n\n        return output_shape\n\n    def initialize(self, *args, **kwargs) -> None:\n        pass\n\n    def forward(self, x):\n        y = tfe.reshape(x, self.output_shape)\n        self.layer_output = y\n        return y\n\n    def backward(self, *args, **kwargs):\n        raise NotImplementedError()\n'"
tf_encrypted/layers/reshape_test.py,7,"b'# pylint: disable=missing-docstring\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.layers import Reshape\n\n\nclass TestReshape(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_forward(self):\n        input_shape = [2, 3, 4, 5]\n        output_shape = [2, -1]\n        input_reshape = np.random.standard_normal(input_shape)\n\n        # reshape pond\n        with tfe.protocol.Pond() as prot:\n\n            reshape_input = prot.define_private_variable(input_reshape)\n            reshape_layer = Reshape(input_shape, output_shape)\n\n            reshape_out_pond = reshape_layer.forward(reshape_input)\n\n            with tfe.Session() as sess:\n\n                sess.run(tf.global_variables_initializer())\n                # outputs\n                out_pond = sess.run(reshape_out_pond.reveal())\n\n            # reset graph\n            tf.reset_default_graph()\n\n            with tf.Session() as sess:\n                x = tf.Variable(input_reshape, dtype=tf.float32)\n\n                reshape_out_tf = tf.reshape(x, output_shape)\n\n                sess.run(tf.global_variables_initializer())\n\n                out_tensorflow = sess.run(reshape_out_tf)\n\n        assert np.isclose(out_pond, out_tensorflow, atol=0.6).all()\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/operations/__init__.py,0,"b'""""""Python client API for custom Ops.""""""\nfrom . import secure_random\n\n__all__ = [\n    ""secure_random"",\n]\n'"
tf_encrypted/player/__init__.py,0,"b'""""""The TF Encrypted Player abstraction.""""""\nfrom __future__ import absolute_import\n\nfrom .player import Player\n\n__all__ = [\n    ""Player"",\n]\n'"
tf_encrypted/player/__main__.py,0,"b'""""""Executable for hosting a Player""""""\nimport logging\n\nfrom tf_encrypted.config import RemoteConfig\n\nif __name__ == ""__main__"":\n\n    logging.basicConfig()\n    logger = logging.getLogger(""tf_encrypted"")\n    logger.setLevel(logging.DEBUG)\n\n    import argparse\n\n    parser = argparse.ArgumentParser(description=""Run a TF Encrypted player"")\n    parser.add_argument(\n        ""name"",\n        metavar=""NAME"",\n        type=str,\n        help=""name of player as specified in the config file"",\n    )\n    parser.add_argument(\n        ""--config"",\n        metavar=""FILE"",\n        type=str,\n        default=""./config.json"",\n        help=""path to configuration file"",\n    )\n    args = parser.parse_args()\n\n    config = RemoteConfig.load(args.config)\n    server = config.server(args.name, start=True)\n    server.join()\n'"
tf_encrypted/player/player.py,2,"b'""""""Implementation of the Player abstraction.""""""\nimport tensorflow as tf\n\n\nclass Player:\n    """"""\n  An abstraction for players in the game-theoretic threat model of\n  a secure computation.\n\n  :param str name: Name of the player\n  :param int index: Index of the player (for ordering)\n  :param str device_name: Name of device (fully expanded)\n  :param str host: IP/domain address of the player\'s device, defaults to None\n  """"""\n\n    def __init__(self, name, index, device_name, host=None):\n        self.name = name\n        self.index = index\n        self.device_name = device_name\n        self.host = host\n\n\ndef player_device(player: Player):\n    """"""\n  Retrieves the tf.device associated with a :class:`Player` object.\n\n  :param Player player: The :class:`Player` object.\n  """"""\n    return tf.device(player.device_name)\n'"
tf_encrypted/protocol/__init__.py,0,"b'""""""Module containing implementations of secure protocols.""""""\n\nfrom __future__ import absolute_import\n\nimport inspect\n\nfrom .aby3 import ABY3\nfrom .pond import Pond\nfrom .pond import TFETensor\nfrom .pond import TFEVariable\nfrom .protocol import Protocol\nfrom .protocol import memoize\nfrom .securenn import SecureNN\n\n\ndef get_all_funcs():\n    """"""Assemble public method names from all protocols into a list.""""""\n    all_prot_method_names = set()\n\n    protocols = [Pond, SecureNN, ABY3]\n    for protocol in protocols:\n        members = inspect.getmembers(protocol, predicate=inspect.isfunction)\n        all_prot_method_names |= set(\n            func_name for func_name, _ in members if not func_name.startswith(""_"")\n        )\n\n    return all_prot_method_names\n\n\n__all__ = [\n    ""Protocol"",\n    ""memoize"",\n    ""Pond"",\n    ""SecureNN"",\n    ""TFEVariable"",\n    ""TFETensor"",\n]\n'"
tf_encrypted/protocol/ops_test.py,142,"b'# pylint: disable=missing-docstring\nimport unittest\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\n\n\nclass TestBatchToSpaceND(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n        tf.set_random_seed(4224)\n\n    def test_4d_no_crops(self):\n        backing = [\n            [[[1], [3]], [[9], [11]]],\n            [[[2], [4]], [[10], [12]]],\n            [[[5], [7]], [[13], [15]]],\n            [[[6], [8]], [[14], [16]]],\n        ]\n        t = tf.constant(backing)\n        block_shape = [2, 2]\n        crops = [[0, 0], [0, 0]]\n        self._generic_private_test(t, block_shape, crops)\n\n    def test_4d_single_crop(self):\n        backing = [\n            [[[0], [1], [3]]],\n            [[[0], [9], [11]]],\n            [[[0], [2], [4]]],\n            [[[0], [10], [12]]],\n            [[[0], [5], [7]]],\n            [[[0], [13], [15]]],\n            [[[0], [6], [8]]],\n            [[[0], [14], [16]]],\n        ]\n        t = tf.constant(backing)\n        block_shape = [2, 2]\n        crops = [[0, 0], [2, 0]]\n        self._generic_private_test(t, block_shape, crops)\n\n    def test_3d_no_crops(self):\n        t = tf.random_uniform([16, 20, 10])  # e.g. [batch, time, features]\n        block_shape = [4]\n        crops = [[0, 0]]\n        self._generic_private_test(t, block_shape, crops)\n\n    def test_3d_mirror_crops(self):\n        t = tf.random_uniform([16, 20, 10])  # e.g. [batch, time, features]\n        block_shape = [4]\n        crops = [[2, 2]]\n        self._generic_private_test(t, block_shape, crops)\n\n    def test_3d_uneven_crops(self):\n        t = tf.random_uniform([16, 20, 10])  # e.g. [batch, time, features]\n        block_shape = [4]\n        crops = [[2, 0]]\n        self._generic_private_test(t, block_shape, crops)\n\n    def test_3d_block_shape(self):\n        t = tf.random_uniform([16, 20, 10])  # e.g. [batch, time, features]\n        block_shape = [8]\n        crops = [[0, 0]]\n        self._generic_private_test(t, block_shape, crops)\n\n    def test_public(self):\n        t = tf.random_uniform([16, 20, 10])  # e.g. [batch, time, features]\n        block_shape = [4]\n        crops = [[2, 2]]\n        self._generic_public_test(t, block_shape, crops)\n\n    def test_masked(self):\n        t = tf.random_uniform([16, 20, 10])  # e.g. [batch, time, features]\n        block_shape = [4]\n        crops = [[2, 2]]\n        self._generic_masked_test(t, block_shape, crops)\n\n    @staticmethod\n    def _generic_public_test(t, block_shape, crops):\n        with tf.Session() as sess:\n            out = tf.batch_to_space_nd(t, block_shape=block_shape, crops=crops)\n            actual = sess.run(out)\n\n        with tfe.protocol.Pond() as prot:\n            b = prot.define_public_variable(t)\n            out = prot.batch_to_space_nd(b, block_shape=block_shape, crops=crops)\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                final = sess.run(out)\n\n        np.testing.assert_array_almost_equal(final, actual, decimal=3)\n\n    @staticmethod\n    def _generic_private_test(t, block_shape, crops):\n        with tf.Session() as sess:\n            out = tf.batch_to_space_nd(t, block_shape=block_shape, crops=crops)\n            actual = sess.run(out)\n\n        with tfe.protocol.Pond() as prot:\n            b = prot.define_private_variable(t)\n            out = prot.batch_to_space_nd(b, block_shape=block_shape, crops=crops)\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                final = sess.run(out.reveal())\n\n        np.testing.assert_array_almost_equal(final, actual, decimal=3)\n\n    @staticmethod\n    def _generic_masked_test(t, block_shape, crops):\n        with tf.Session() as sess:\n            out = tf.batch_to_space_nd(t, block_shape=block_shape, crops=crops)\n            actual = sess.run(out)\n\n        with tfe.protocol.Pond() as prot:\n            b = prot.mask(prot.define_private_variable(t))\n            out = prot.batch_to_space_nd(b, block_shape=block_shape, crops=crops)\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                final = sess.run(out.reveal())\n\n        np.testing.assert_array_almost_equal(final, actual, decimal=3)\n\n\nclass TestSpaceToBatchND(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n        tf.set_random_seed(4224)\n\n    def test_4d_no_crops(self):\n        backing = [\n            [[[1], [3]], [[9], [11]]],\n            [[[2], [4]], [[10], [12]]],\n            [[[5], [7]], [[13], [15]]],\n            [[[6], [8]], [[14], [16]]],\n        ]\n        t = tf.constant(backing)\n        block_shape = [2, 2]\n        paddings = [[0, 0], [0, 0]]\n        self._generic_private_test(t, block_shape, paddings)\n\n    def test_4d_single_crop(self):\n        backing = [\n            [[[1], [2], [3], [4]], [[5], [6], [7], [8]]],\n            [[[9], [10], [11], [12]], [[13], [14], [15], [16]]],\n        ]\n        t = tf.constant(backing)\n        block_shape = [2, 2]\n        paddings = [[0, 0], [2, 0]]\n        self._generic_private_test(t, block_shape, paddings)\n\n    def test_3d_no_crops(self):\n        t = tf.random_uniform([16, 20, 10])  # e.g. [batch, time, features]\n        block_shape = [4]\n        paddings = [[0, 0]]\n        self._generic_private_test(t, block_shape, paddings)\n\n    def test_3d_mirror_crops(self):\n        t = tf.random_uniform([16, 20, 10])  # e.g. [batch, time, features]\n        block_shape = [4]\n        paddings = [[2, 2]]\n        self._generic_private_test(t, block_shape, paddings)\n\n    def test_3d_uneven_crops(self):\n        t = tf.random_uniform([16, 20, 10])  # e.g. [batch, time, features]\n        block_shape = [2]\n        paddings = [[2, 0]]\n        self._generic_private_test(t, block_shape, paddings)\n\n    def test_3d_block_shape(self):\n        t = tf.random_uniform([16, 20, 10])  # e.g. [batch, time, features]\n        block_shape = [5]\n        paddings = [[0, 0]]\n        self._generic_private_test(t, block_shape, paddings)\n\n    def test_public(self):\n        t = tf.random_uniform([16, 20, 10])  # e.g. [batch, time, features]\n        block_shape = [4]\n        paddings = [[2, 2]]\n        self._generic_public_test(t, block_shape, paddings)\n\n    def test_masked(self):\n        t = tf.random_uniform([16, 20, 10])  # e.g. [batch, time, features]\n        block_shape = [4]\n        paddings = [[2, 2]]\n        self._generic_masked_test(t, block_shape, paddings)\n\n    @staticmethod\n    def _generic_public_test(t, block_shape, paddings):\n        with tf.Session() as sess:\n            out = tf.space_to_batch_nd(t, block_shape=block_shape, paddings=paddings)\n            actual = sess.run(out)\n\n        with tfe.protocol.Pond() as prot:\n            b = prot.define_public_variable(t)\n            out = prot.space_to_batch_nd(b, block_shape=block_shape, paddings=paddings)\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                final = sess.run(out)\n\n        np.testing.assert_array_almost_equal(final, actual, decimal=3)\n\n    @staticmethod\n    def _generic_private_test(t, block_shape, paddings):\n        with tf.Session() as sess:\n            out = tf.space_to_batch_nd(t, block_shape=block_shape, paddings=paddings)\n            actual = sess.run(out)\n\n        with tfe.protocol.Pond() as prot:\n            b = prot.define_private_variable(t)\n            out = prot.space_to_batch_nd(b, block_shape=block_shape, paddings=paddings)\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                final = sess.run(out.reveal())\n\n        np.testing.assert_array_almost_equal(final, actual, decimal=3)\n\n    @staticmethod\n    def _generic_masked_test(t, block_shape, paddings):\n        with tf.Session() as sess:\n            out = tf.space_to_batch_nd(t, block_shape=block_shape, paddings=paddings)\n            actual = sess.run(out)\n\n        with tfe.protocol.Pond() as prot:\n            b = prot.mask(prot.define_private_variable(t))\n            out = prot.space_to_batch_nd(b, block_shape=block_shape, paddings=paddings)\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                final = sess.run(out.reveal())\n\n        np.testing.assert_array_almost_equal(final, actual, decimal=3)\n\n\nclass Testconcat(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_concat(self):\n\n        with tf.Session() as sess:\n            t1 = [[1, 2, 3], [4, 5, 6]]\n            t2 = [[7, 8, 9], [10, 11, 12]]\n            out = tf.concat([t1, t2], 0)\n            actual = sess.run(out)\n\n        tf.reset_default_graph()\n\n        with tfe.protocol.Pond() as prot:\n            x = prot.define_private_variable(np.array(t1))\n            y = prot.define_private_variable(np.array(t2))\n\n            out = prot.concat([x, y], 0)\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                final = sess.run(out.reveal())\n\n        np.testing.assert_array_equal(final, actual)\n\n    def test_masked_concat(self):\n\n        with tf.Session() as sess:\n            t1 = [[1, 2, 3], [4, 5, 6]]\n            t2 = [[7, 8, 9], [10, 11, 12]]\n            out = tf.concat([t1, t2], 0)\n            actual = sess.run(out)\n\n        tf.reset_default_graph()\n\n        with tfe.protocol.Pond() as prot:\n            x = prot.mask(prot.define_private_variable(np.array(t1)))\n            y = prot.mask(prot.define_private_variable(np.array(t2)))\n\n            out = prot.concat([x, y], 0)\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                final = sess.run(out.unmasked.reveal())\n\n        np.testing.assert_array_equal(final, actual)\n\n\nclass TestConv2D(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_forward(self) -> None:\n        # input\n        batch_size, channels_in, channels_out = 32, 3, 64\n        img_height, img_width = 28, 28\n        input_shape = (batch_size, channels_in, img_height, img_width)\n        input_conv = np.random.normal(size=input_shape).astype(np.float32)\n\n        # filters\n        h_filter, w_filter, strides = 2, 2, 2\n        filter_shape = (h_filter, w_filter, channels_in, channels_out)\n        filter_values = np.random.normal(size=filter_shape)\n\n        # convolution pond\n        with tfe.protocol.Pond() as prot:\n\n            conv_input = prot.define_private_variable(input_conv)\n            conv_layer = tfe.layers.Conv2D(input_shape, filter_shape, strides=2)\n            conv_layer.initialize(initial_weights=filter_values)\n            conv_out_pond = conv_layer.forward(conv_input)\n\n            with tfe.Session() as sess:\n\n                sess.run(tf.global_variables_initializer())\n                # outputs\n                out_pond = sess.run(conv_out_pond.reveal())\n\n        # reset graph\n        tf.reset_default_graph()\n\n        # convolution tensorflow\n        with tf.Session() as sess:\n            # conv input\n            x = tf.Variable(input_conv, dtype=tf.float32)\n            x_nhwc = tf.transpose(x, (0, 2, 3, 1))\n\n            # convolution Tensorflow\n            filters_tf = tf.Variable(filter_values, dtype=tf.float32)\n\n            conv_out_tf = tf.nn.conv2d(\n                x_nhwc, filters_tf, strides=[1, strides, strides, 1], padding=""SAME"",\n            )\n\n            sess.run(tf.global_variables_initializer())\n            out_tensorflow = sess.run(conv_out_tf).transpose(0, 3, 1, 2)\n\n        np.testing.assert_allclose(out_pond, out_tensorflow, atol=0.01)\n\n    def test_forward_bias(self) -> None:\n        # input\n        batch_size, channels_in, channels_out = 32, 3, 64\n        img_height, img_width = 28, 28\n        input_shape = (batch_size, channels_in, img_height, img_width)\n        input_conv = np.random.normal(size=input_shape).astype(np.float32)\n\n        # filters\n        h_filter, w_filter, strides = 2, 2, 2\n        filter_shape = (h_filter, w_filter, channels_in, channels_out)\n        filter_values = np.random.normal(size=filter_shape)\n\n        # convolution pond\n        with tfe.protocol.Pond() as prot:\n\n            conv_input = prot.define_private_variable(input_conv)\n            conv_layer = tfe.layers.Conv2D(input_shape, filter_shape, strides=2)\n\n            output_shape = conv_layer.get_output_shape()\n\n            bias = np.random.uniform(size=output_shape[1:])\n\n            conv_layer.initialize(initial_weights=filter_values, initial_bias=bias)\n            conv_out_pond = conv_layer.forward(conv_input)\n\n            with tfe.Session() as sess:\n\n                sess.run(tf.global_variables_initializer())\n                # outputs\n                out_pond = sess.run(conv_out_pond.reveal())\n\n        # reset graph\n        tf.reset_default_graph()\n\n        # convolution tensorflow\n        with tf.Session() as sess:\n            # conv input\n            x = tf.Variable(input_conv, dtype=tf.float32)\n            x_nhwc = tf.transpose(x, (0, 2, 3, 1))\n\n            # convolution Tensorflow\n            filters_tf = tf.Variable(filter_values, dtype=tf.float32)\n\n            conv_out_tf = tf.nn.conv2d(\n                x_nhwc, filters_tf, strides=[1, strides, strides, 1], padding=""SAME"",\n            )\n\n            sess.run(tf.global_variables_initializer())\n            out_tensorflow = sess.run(conv_out_tf).transpose(0, 3, 1, 2)\n            out_tensorflow += bias\n\n        np.testing.assert_allclose(out_pond, out_tensorflow, atol=0.01)\n\n    def test_backward(self):\n        pass\n\n\nclass TestMatMul(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_matmul(self) -> None:\n\n        with tfe.protocol.Pond() as prot:\n\n            input_shape = [4, 5]\n            x_in = np.random.normal(size=input_shape)\n\n            filter_shape = [5, 4]\n            filter_values = np.random.normal(size=filter_shape)\n\n            input_input = prot.define_private_variable(x_in)\n            filter_filter = prot.define_private_variable(filter_values)\n\n            out = prot.matmul(input_input, filter_filter)\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n\n                out_pond = sess.run(out.reveal())\n\n        # reset graph\n        tf.reset_default_graph()\n\n        with tf.Session() as sess:\n            x = tf.Variable(x_in, dtype=tf.float32)\n            filters_tf = tf.Variable(filter_values, dtype=tf.float32)\n\n            out = tf.matmul(x, filters_tf)\n\n            sess.run(tf.global_variables_initializer())\n            out_tensorflow = sess.run(out)\n\n        np.testing.assert_array_almost_equal(out_pond, out_tensorflow, decimal=2)\n\n    def test_big_middle_matmul(self) -> None:\n        with tfe.protocol.Pond() as prot:\n\n            input_shape = [64, 4500]\n            x_in = np.random.normal(size=input_shape)\n\n            filter_shape = [4500, 64]\n            filter_values = np.random.normal(size=filter_shape)\n\n            input_input = prot.define_private_variable(x_in)\n            filter_filter = prot.define_private_variable(filter_values)\n\n            out = prot.matmul(input_input, filter_filter)\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n\n                out_pond = sess.run(out.reveal())\n\n        # reset graph\n        tf.reset_default_graph()\n\n        with tf.Session() as sess:\n            x = tf.Variable(x_in, dtype=tf.float32)\n            filters_tf = tf.Variable(filter_values, dtype=tf.float32)\n\n            out = tf.matmul(x, filters_tf)\n\n            sess.run(tf.global_variables_initializer())\n            out_tensorflow = sess.run(out)\n\n        np.testing.assert_allclose(out_pond, out_tensorflow, atol=0.1)\n\n\nclass TestNegative(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_negative(self):\n        input_shape = [2, 2]\n        input_neg = np.ones(input_shape)\n\n        # reshape pond\n        with tfe.protocol.Pond() as prot:\n\n            neg_input = prot.define_private_variable(input_neg)\n\n            neg_out_pond = prot.negative(neg_input)\n\n            with tfe.Session() as sess:\n\n                sess.run(tf.global_variables_initializer())\n                # outputs\n                out_pond = sess.run(neg_out_pond.reveal())\n\n            # reset graph\n            tf.reset_default_graph()\n\n            with tf.Session() as sess:\n                x = tf.Variable(input_neg, dtype=tf.float32)\n\n                neg_out_tf = tf.negative(x)\n\n                sess.run(tf.global_variables_initializer())\n\n                out_tensorflow = sess.run(neg_out_tf)\n\n        assert np.isclose(out_pond, out_tensorflow, atol=0.6).all()\n\n\nclass TestSqrt(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_sqrt(self):\n        input_shape = [2, 2]\n        input_sqrt = np.ones(input_shape)\n\n        # reshape pond\n        with tfe.protocol.Pond() as prot:\n\n            sqrt_input = prot.define_public_variable(input_sqrt)\n\n            sqrt_out_pond = prot.sqrt(sqrt_input)\n\n            with tfe.Session() as sess:\n\n                sess.run(tf.global_variables_initializer())\n                # outputs\n                out_pond = sess.run(sqrt_out_pond)\n\n            # reset graph\n            tf.reset_default_graph()\n\n            with tf.Session() as sess:\n                x = tf.Variable(input_sqrt, dtype=tf.float32)\n\n                sqrt_out_tf = tf.math.sqrt(x)\n\n                sess.run(tf.global_variables_initializer())\n\n                out_tensorflow = sess.run(sqrt_out_tf)\n\n        assert np.isclose(out_pond, out_tensorflow, atol=0.6).all()\n\n\nclass TestPad(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_pad(self):\n\n        with tfe.protocol.Pond() as prot:\n\n            tf.reset_default_graph()\n\n            x_in = np.array([[1, 2, 3], [4, 5, 6]])\n            input_input = prot.define_private_variable(x_in)\n\n            paddings = [[2, 2], [3, 4]]\n\n            out = prot.pad(input_input, paddings)\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                out_tfe = sess.run(out.reveal())\n\n            tf.reset_default_graph()\n\n            # TODO this is a bit weird\n            out_tensorflow = tfe.convert.convert_test.run_pad(x_in)\n\n            np.testing.assert_allclose(out_tfe, out_tensorflow, atol=0.01)\n\n\n@pytest.mark.slow\nclass TestReduceMax(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def tearDown(self):\n        tf.reset_default_graph()\n\n    def test_reduce_max_1d(self):\n\n        t = np.array([1, 2, 3, 4]).astype(float)\n\n        with tf.Session() as sess:\n            out_tf = tf.reduce_max(t)\n            expected = sess.run(out_tf)\n\n        with tfe.protocol.SecureNN() as prot:\n            b = prot.define_private_variable(tf.constant(t))\n            out_tfe = prot.reduce_max(b)\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                for _ in range(2):\n                    actual = sess.run(out_tfe.reveal(), tag=""test_1d"")\n\n        np.testing.assert_array_equal(actual, expected)\n\n    def test_reduce_max_2d_axis0(self):\n\n        t = np.array([1, 2, 3, 4, 5, 6, 7, 8]).reshape((2, 4)).astype(float)\n\n        with tf.Session() as sess:\n            out_tf = tf.reduce_max(t, axis=0)\n            expected = sess.run(out_tf)\n\n        with tfe.protocol.SecureNN() as prot:\n            b = prot.define_private_variable(tf.constant(t))\n            out_tfe = prot.reduce_max(b, axis=0)\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                for _ in range(2):\n                    actual = sess.run(out_tfe.reveal(), tag=""test_2d_axis0"")\n\n        np.testing.assert_array_equal(actual, expected)\n\n    def test_reduce_max_2d_axis1(self):\n\n        t = np.array([1, 2, 3, 4, 5, 6, 7, 8]).reshape((2, 4)).astype(float)\n\n        with tf.Session() as sess:\n            out_tf = tf.reduce_max(t, axis=1)\n            expected = sess.run(out_tf)\n\n        with tfe.protocol.SecureNN() as prot:\n            b = prot.define_private_variable(tf.constant(t))\n            out_tfe = prot.reduce_max(b, axis=1)\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                for _ in range(2):\n                    actual = sess.run(out_tfe.reveal(), tag=""test_2d_axis1"")\n\n        np.testing.assert_array_equal(actual, expected)\n\n    def test_reduce_max_3d_axis0(self):\n\n        t = np.array([1, 2, 3, 4, 5, 6, 7, 8]).reshape((2, 2, 2))\n\n        with tf.Session() as sess:\n            out = tf.reduce_max(t, axis=0)\n            expected = sess.run(out)\n\n        with tfe.protocol.SecureNN() as prot:\n            b = prot.define_private_variable(tf.constant(t))\n            out_tfe = prot.reduce_max(b, axis=0)\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                for _ in range(2):\n                    actual = sess.run(out_tfe.reveal(), tag=""test_3d_axis0"")\n\n        np.testing.assert_array_equal(actual, expected)\n\n\nclass TestReduceSum(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_reduce_sum_1d(self):\n\n        t = [1, 2]\n        with tf.Session() as sess:\n            out = tf.reduce_sum(t)\n            actual = sess.run(out)\n\n        with tfe.protocol.Pond() as prot:\n            b = prot.define_private_variable(tf.constant(t))\n            out = prot.reduce_sum(b)\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                final = sess.run(out.reveal())\n\n        np.testing.assert_array_equal(final, actual)\n\n    def test_reduce_sum_2d(self):\n\n        t = [[1, 2], [1, 3]]\n        with tf.Session() as sess:\n            out = tf.reduce_sum(t, axis=1)\n            actual = sess.run(out)\n\n        with tfe.protocol.Pond() as prot:\n            b = prot.define_private_variable(tf.constant(t))\n            out = prot.reduce_sum(b, axis=1)\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                final = sess.run(out.reveal())\n\n        np.testing.assert_array_equal(final, actual)\n\n    def test_reduce_sum_huge_vector(self):\n\n        t = [1] * 2 ** 13\n        with tf.Session() as sess:\n            out = tf.reduce_sum(t)\n            actual = sess.run(out)\n\n        with tfe.protocol.Pond() as prot:\n            b = prot.define_private_variable(tf.constant(t))\n            out = prot.reduce_sum(b)\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                final = sess.run(out.reveal())\n\n        np.testing.assert_array_equal(final, actual)\n\n\nclass TestStack(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_stack(self):\n\n        with tf.Session() as sess:\n            x = tf.constant([1, 4])\n            y = tf.constant([2, 5])\n            z = tf.constant([3, 6])\n            out = tf.stack([x, y, z])\n\n            actual = sess.run(out)\n\n        tf.reset_default_graph()\n\n        with tfe.protocol.Pond() as prot:\n            x = prot.define_private_variable(np.array([1, 4]))\n            y = prot.define_private_variable(np.array([2, 5]))\n            z = prot.define_private_variable(np.array([3, 6]))\n\n            out = prot.stack((x, y, z))\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                final = sess.run(out.reveal())\n\n        np.testing.assert_array_equal(final, actual)\n\n\nclass TestStridedSlice(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_strided_slice(self):\n\n        with tf.Session() as sess:\n            t = tf.constant(\n                [\n                    [[1, 1, 1], [2, 2, 2]],\n                    [[3, 3, 3], [4, 4, 4]],\n                    [[5, 5, 5], [6, 6, 6]],\n                ]\n            )\n            out = tf.strided_slice(t, [1, 0, 0], [2, 1, 3], [1, 1, 1])\n\n            actual = sess.run(out)\n\n        tf.reset_default_graph()\n\n        with tfe.protocol.Pond() as prot:\n            x = np.array(\n                [\n                    [[1, 1, 1], [2, 2, 2]],\n                    [[3, 3, 3], [4, 4, 4]],\n                    [[5, 5, 5], [6, 6, 6]],\n                ]\n            )\n\n            out = prot.define_private_variable(x)\n            out = prot.strided_slice(out, [1, 0, 0], [2, 1, 3], [1, 1, 1])\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                final = sess.run(out.reveal())\n\n        np.testing.assert_array_equal(final, actual)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/protocol/protocol.py,0,"b'""""""Base abstraction for a Protocol.""""""\nimport functools\nfrom abc import ABC\nfrom types import TracebackType\nfrom typing import Any\nfrom typing import Callable\nfrom typing import Optional\n\nimport tf_encrypted as tfe\n\nfrom ..tensor.factory import AbstractTensor\n\nnodes = dict()\n\n\nclass Protocol(ABC):\n    """"""\n  Protocol is the base class that other protocols in TF Encrypted will extend.\n\n  Do not directly instantiate this class.  You should use a subclass instead,\n  such as :class:`~tf_encrypted.protocol.protocol.SecureNN`\n  or :class:`~tf_encrypted.protocol.protocol.Pond`\n  """"""\n\n    def __enter__(self) -> ""Protocol"":\n        self.last_protocol = tfe.get_protocol()\n        tfe.set_protocol(self)\n        return self\n\n    def __exit__(\n        self,\n        exception_type,\n        exception_value: Optional[Exception],\n        traceback: Optional[TracebackType],\n    ) -> Optional[bool]:\n        tfe.set_protocol(self.last_protocol)\n\n\ndef memoize(func: Callable) -> Callable:\n    """"""\n  memoize(func) -> Callable\n\n  Decorates a function for memoization, which explicitly caches the function\'s\n  output.\n\n  :param Callable func: The function to memoize\n  """"""\n\n    @functools.wraps(func)\n    def cache_nodes(self: Protocol, *args: Any, **kwargs: Any) -> AbstractTensor:\n        args = tuple(tuple(x) if isinstance(x, list) else x for x in args)\n        node_key = (func.__name__, args, tuple(sorted(kwargs.items())))\n\n        cached_result = nodes.get(node_key, None)\n        if cached_result is not None:\n            return cached_result\n\n        result = func(self, *args, **kwargs)\n\n        nodes[node_key] = result\n        return result\n\n    return cache_nodes\n'"
tf_encrypted/queue/__init__.py,0,"b'""""""\nQueue data structures.\n""""""\n\nfrom __future__ import absolute_import\n\nfrom .fifo import AbstractFIFOQueue\nfrom .fifo import FIFOQueue\n\n__all__ = [\n    ""FIFOQueue"",\n    ""AbstractFIFOQueue"",\n]\n'"
tf_encrypted/queue/fifo.py,1,"b'""""""\nFIFO queue data structure.\n""""""\n\nfrom __future__ import absolute_import\n\nimport abc\n\nimport tf_encrypted as tfe\n\n\nclass AbstractFIFOQueue(abc.ABC):\n    """"""\n  FIFO queues mimicking `tf.queue.FIFOQueue`.\n  """"""\n\n    @abc.abstractmethod\n    def enqueue(self, tensor):\n        """"""\n    Push `tensor` onto queue.\n\n    Blocks if queue is full.\n    """"""\n\n    @abc.abstractmethod\n    def dequeue(self):\n        """"""\n    Pop tensor from queue.\n\n    Blocks if queue is empty.\n    """"""\n\n\ndef FIFOQueue(capacity, shape, shared_name=None):\n    return tfe.fifo_queue(capacity=capacity, shape=shape, shared_name=shared_name,)\n'"
tf_encrypted/serving/__init__.py,0,"b'""""""\nTODO\n""""""\nfrom __future__ import absolute_import\n\nfrom .queues import QueueClient\nfrom .queues import QueueServer\n\n__all__ = [\n    ""QueueServer"",\n    ""QueueClient"",\n]\n'"
tf_encrypted/serving/queues.py,0,"b'# pylint: disable=missing-docstring\n\nimport numpy as np\n\nimport tf_encrypted as tfe\n\n\nclass QueueServer:\n    """"""\n  Serving server based on `tfe.queue.FIFOQueue`.\n  """"""\n\n    def __init__(\n        self,\n        input_shape,\n        output_shape,\n        computation_fn,\n        input_queue_capacity=1,\n        input_queue_name=""input"",\n        output_queue_capacity=1,\n        output_queue_name=""output"",\n    ):\n        self.input_shape = input_shape\n        self.output_shape = output_shape\n\n        self.input_queue = tfe.queue.FIFOQueue(\n            capacity=input_queue_capacity,\n            shape=input_shape,\n            shared_name=input_queue_name,\n        )\n\n        self.output_queue = tfe.queue.FIFOQueue(\n            capacity=output_queue_capacity,\n            shape=output_shape,\n            shared_name=output_queue_name,\n        )\n\n        # computation step\n        x = self.input_queue.dequeue()\n        y = computation_fn(x)\n        self.step_op = self.output_queue.enqueue(y)\n\n    def run_step(self, sess, tag=""step""):\n        """"""\n    Serve single computation.\n    """"""\n        sess.run(self.step_op, tag=tag)\n\n    def run(self, sess, num_steps=None, step_fn=None):\n        """"""\n    Continuously serve computations, for `num_steps` if specified.\n\n    If specified, `step_fn` is called after each step.\n    """"""\n        if num_steps is not None:\n            for _ in range(num_steps):\n                self.run_step(sess)\n                if step_fn is not None:\n                    step_fn()\n        else:\n            while True:\n                self.run_step(sess)\n                if step_fn is not None:\n                    step_fn()\n\n\nclass QueueClient:\n    """"""\n  Serving client based on `tfe.queue.FIFOQueue`.\n\n  Must be set up with the same arguments as the server.\n  """"""\n\n    def __init__(\n        self,\n        input_shape,\n        output_shape,\n        input_queue_capacity=1,\n        input_queue_name=""input"",\n        output_queue_capacity=1,\n        output_queue_name=""output"",\n    ):\n        self.input_shape = input_shape\n        self.output_shape = output_shape\n\n        # input\n        input_queue = tfe.queue.FIFOQueue(\n            capacity=input_queue_capacity,\n            shape=input_shape,\n            shared_name=input_queue_name,\n        )\n        self.input_placeholder = tfe.define_private_placeholder(shape=input_shape)\n        self.input_op = input_queue.enqueue(self.input_placeholder)\n\n        # output\n        output_queue = tfe.queue.FIFOQueue(\n            capacity=output_queue_capacity,\n            shape=output_shape,\n            shared_name=output_queue_name,\n        )\n        output = output_queue.dequeue()\n        self.output0 = output.share0\n        self.output1 = output.share1\n\n        # fixedpoint config\n        # TODO[jvmncs]: how do we access fixedpoint_config without calling\n        # get_protocol?\n        self.modulus = output.backing_dtype.modulus\n        self.bound = tfe.get_protocol().fixedpoint_config.bound_single_precision\n        self.scaling_factor = tfe.get_protocol().fixedpoint_config.scaling_factor\n\n    def send_input(self, sess, x, tag=""input""):\n        """"""\n    Send `x` to servers for processing.\n    """"""\n        assert isinstance(x, np.ndarray)\n        assert list(x.shape) == list(self.input_shape)\n\n        # simply run the input op with\n        sess.run(\n            self.input_op,\n            tag=tag,\n            output_partition_graphs=True,\n            feed_dict=self.input_placeholder.feed(x),\n        )\n\n    def receive_output(self, sess, tag=""output""):\n        """"""\n    Receive result from servers, blocking until available.\n    """"""\n        res0, res1 = sess.run(\n            [self.output0, self.output1], tag=tag, output_partition_graphs=True,\n        )\n\n        res = (res0 + res1) % self.modulus\n        res = (res + self.bound) % self.modulus - self.bound\n        res = res / self.scaling_factor\n        return res\n\n    def run(self, sess, x):\n        """"""\n    Send `x` to servers and return result.\n    """"""\n        self.send_input(sess, x)\n        return self.receive_output(sess)\n'"
tf_encrypted/tensor/__init__.py,2,"b'""""""Tensors representing non-native data types (like fixed-point precision).""""""\nfrom __future__ import absolute_import\n\nimport tensorflow as tf\n\nfrom .fixed import _validate_fixedpoint_config\nfrom .fixed import fixed64\nfrom .fixed import fixed64_ni\nfrom .fixed import fixed100\nfrom .fixed import fixed100_ni\nfrom .int100 import int100factory\nfrom .native import native_factory\n\nint32factory = native_factory(tf.int32)\nint64factory = native_factory(tf.int64)\n\nassert _validate_fixedpoint_config(fixed100, int100factory)\nassert _validate_fixedpoint_config(fixed100_ni, int100factory)\nassert _validate_fixedpoint_config(fixed64, int64factory)\nassert _validate_fixedpoint_config(fixed64_ni, int64factory)\n\n__all__ = [\n    ""native_factory"",\n    ""int32factory"",\n    ""int64factory"",\n    ""int100factory"",\n]\n'"
tf_encrypted/tensor/boolfactory.py,45,"b'""""""\nUse TensorFlow\'s native bool type.\n""""""\nfrom __future__ import absolute_import\n\nimport abc\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Tuple\nfrom typing import Union\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom ..operations import secure_random as crypto\nfrom .factory import AbstractConstant\nfrom .factory import AbstractFactory\nfrom .factory import AbstractPlaceholder\nfrom .factory import AbstractTensor\nfrom .factory import AbstractVariable\n\n\ndef bool_factory():\n    """"""Constructs the native tensor Factory.""""""\n\n    class Factory(AbstractFactory):\n        """"""Native tensor factory.""""""\n\n        def tensor(self, value):\n            if isinstance(value, Tensor):\n                return value\n\n            if isinstance(value, tf.Tensor):\n                if value.dtype is not self.native_type:\n                    value = tf.cast(value, dtype=self.native_type)\n                return DenseTensor(value)\n\n            value = np.array(value)\n            value = tf.convert_to_tensor(value)\n            value = tf.cast(value, self.native_type)\n            return DenseTensor(value)\n\n        def constant(self, value):\n            value = tf.constant(value, dtype=self.native_type)\n            return Constant(value)\n\n        def variable(self, initial_value):\n            if isinstance(initial_value, (tf.Tensor, np.ndarray)):\n                return Variable(initial_value)\n\n            if isinstance(initial_value, Tensor):\n                return Variable(initial_value.value)\n\n            msg = ""Don\'t know how to handle {}""\n            raise TypeError(msg.format(type(initial_value)))\n\n        def placeholder(self, shape):\n            return Placeholder(shape)\n\n        @property\n        def native_type(self):\n            return tf.bool\n\n        @property\n        def modulus(self) -> int:\n            return 2\n\n        def sample_uniform(self, shape):  # pylint: disable=arguments-differ\n            minval = 0\n            maxval = 2\n\n            if crypto.supports_seeded_randomness():\n                seed = crypto.secure_seed()\n                return UniformTensor(\n                    shape=shape, seed=seed, minval=minval, maxval=maxval,\n                )\n\n            if crypto.supports_secure_randomness():\n                sampler = crypto.random_uniform\n            else:\n                sampler = tf.random_uniform\n            value = sampler(shape=shape, minval=minval, maxval=maxval, dtype=tf.int32,)\n            value = tf.cast(value, tf.bool)\n            return DenseTensor(value)\n\n        def sample_seeded_uniform(self, shape, seed):\n            """"""Seeded sample of a random tensor.\n\n            Arguments:\n                shape (tuple of ints), shape of the tensor to sample\n                seed (int), seed for the sampler to use\n\n            Returns a tensor of shape `shape` drawn from a uniform distribution over\n            the interval [0,2].\n            """"""\n            minval = 0\n            maxval = 2\n\n            if crypto.supports_seeded_randomness():\n                # Don\'t use UniformTensor for lazy sampling here, because the `seed`\n                # might be something (e.g., key) we want to protect, and we cannot\n                # send it to another party\n                value = crypto.seeded_random_uniform(\n                    shape=shape,\n                    dtype=tf.int32,\n                    minval=minval,\n                    maxval=maxval,\n                    seed=seed,\n                )\n                value = tf.cast(value, tf.bool)\n                return DenseTensor(value)\n\n            raise NotImplementedError(\n                ""Secure seeded randomness implementation is not available.""\n            )\n\n        def sample_bounded(self, shape, bitlength: int):\n            raise NotImplementedError(""No bounded sampling for boolean type."")\n\n        def stack(self, xs: list, axis: int = 0):\n            assert all(isinstance(x, Tensor) for x in xs)\n            value = tf.stack([x.value for x in xs], axis=axis)\n            return DenseTensor(value)\n\n        def concat(self, xs: list, axis: int):\n            assert all(isinstance(x, Tensor) for x in xs)\n            value = tf.concat([x.value for x in xs], axis=axis)\n            return DenseTensor(value)\n\n        def where(self, condition, x, y, v2=True):\n            if not isinstance(condition, tf.Tensor):\n                msg = ""Don\'t know how to handle `condition` of type {}""\n                raise TypeError(msg.format(type(condition)))\n            if not v2:\n                value = tf.where(condition, x.value, y.value)\n            else:\n                value = tf.where_v2(condition, x.value, y.value)\n            return DenseTensor(value)\n\n    def _lift(x, y) -> Tuple[""Tensor"", ""Tensor""]:\n\n        if isinstance(x, Tensor) and isinstance(y, Tensor):\n            return x, y\n\n        if isinstance(x, Tensor):\n            return x, x.factory.tensor(y)\n\n        if isinstance(y, Tensor):\n            return y.factory.tensor(x), y\n\n        raise TypeError(""Don\'t know how to lift {} {}"".format(type(x), type(y)))\n\n    class Tensor(AbstractTensor):\n        """"""Base class for other native tensor classes.""""""\n\n        @property\n        @abc.abstractproperty\n        def value(self):\n            pass\n\n        @property\n        @abc.abstractproperty\n        def shape(self):\n            pass\n\n        def identity(self):\n            value = tf.identity(self.value)\n            return DenseTensor(value)\n\n        def to_native(self) -> tf.Tensor:\n            return self.value\n\n        def __repr__(self) -> str:\n            return ""{}(shape={})"".format(type(self), self.shape)\n\n        @property\n        def factory(self):\n            return FACTORY\n\n        @property\n        def dtype(self):\n            return self.factory.native_type\n\n        def __getitem__(self, slc):\n            return DenseTensor(self.value[slc])\n\n        def transpose(self, perm):\n            return DenseTensor(tf.transpose(self.value, perm))\n\n        def strided_slice(self, args, kwargs):\n            return DenseTensor(tf.strided_slice(self.value, *args, **kwargs))\n\n        def gather(self, indices: list, axis: int = 0):\n            return DenseTensor(tf.gather(self.value, indices, axis=axis))\n\n        def split(self, num_split: int, axis: int = 0):\n            values = tf.split(self.value, num_split, axis=axis)\n            return [DenseTensor(value) for value in values]\n\n        def reshape(self, axes: Union[tf.Tensor, List[int]]):\n            return DenseTensor(tf.reshape(self.value, axes))\n\n        def equal(self, other, factory=None):\n            x, y = _lift(self, other)\n            factory = factory or FACTORY\n            return factory.tensor(\n                tf.cast(tf.equal(x.value, y.value), dtype=factory.native_type)\n            )\n\n        def expand_dims(self, axis: Optional[int] = None):\n            return DenseTensor(tf.expand_dims(self.value, axis))\n\n        def squeeze(self, axis: Optional[List[int]] = None):\n            return DenseTensor(tf.squeeze(self.value, axis=axis))\n\n        def cast(self, factory):\n            return factory.tensor(self.value)\n\n        def __xor__(self, other):\n            return self.logical_xor(other)\n\n        def logical_xor(self, other):\n            x, y = _lift(self, other)\n            value = tf.math.logical_xor(x.value, y.value)\n            return DenseTensor(value)\n\n        def __and__(self, other):\n            return self.logical_and(other)\n\n        def logical_and(self, other):\n            x, y = _lift(self, other)\n            value = tf.math.logical_and(x.value, y.value)\n            return DenseTensor(value)\n\n        def __or__(self, other):\n            return self.logical_or(other)\n\n        def logical_or(self, other):\n            x, y = _lift(self, other)\n            value = tf.math.logical_or(x.value, y.value)\n            return DenseTensor(value)\n\n        def __invert__(self):\n            return self.logical_not()\n\n        def logical_not(self):\n            value = tf.math.logical_not(self.value)\n            return DenseTensor(value)\n\n    class DenseTensor(Tensor):\n        """"""Public native Tensor class.""""""\n\n        def __init__(self, value):\n            self._value = value\n\n        @property\n        def shape(self):\n            return self._value.shape\n\n        @property\n        def value(self):\n            return self._value\n\n        @property\n        def support(self):\n            return [self._value]\n\n    class UniformTensor(Tensor):\n        """"""Class representing a uniform-random, lazily sampled tensor.\n\n        Lazy sampling optimizes communication by sending seeds in place of\n        fully-expanded tensors.""""""\n\n        def __init__(self, shape, seed, minval, maxval):\n            self._seed = seed\n            self._shape = shape\n            self._minval = minval\n            self._maxval = maxval\n\n        @property\n        def shape(self):\n            return self._shape\n\n        @property\n        def value(self):\n            with tf.name_scope(""expand-seed""):\n                return tf.cast(\n                    crypto.seeded_random_uniform(\n                        shape=self._shape,\n                        dtype=tf.int32,\n                        minval=self._minval,\n                        maxval=self._maxval,\n                        seed=self._seed,\n                    ),\n                    tf.bool,\n                )\n\n        @property\n        def support(self):\n            return [self._seed]\n\n    class Constant(DenseTensor, AbstractConstant):\n        """"""Native Constant class.""""""\n\n        def __init__(self, constant: tf.Tensor) -> None:\n            assert isinstance(constant, tf.Tensor)\n            super(Constant, self).__init__(constant)\n\n        def __repr__(self) -> str:\n            return ""Constant(shape={})"".format(self.shape)\n\n    class Placeholder(DenseTensor, AbstractPlaceholder):\n        """"""Native Placeholder class.""""""\n\n        def __init__(self, shape: List[int]) -> None:\n            self.placeholder = tf.placeholder(tf.bool, shape=shape)\n            super(Placeholder, self).__init__(self.placeholder)\n\n        def __repr__(self) -> str:\n            return ""Placeholder(shape={})"".format(self.shape)\n\n        def feed(self, value: np.ndarray) -> Dict[tf.Tensor, np.ndarray]:\n            assert isinstance(value, np.ndarray), type(value)\n            return {self.placeholder: value}\n\n    class Variable(DenseTensor, AbstractVariable):\n        """"""Native Variable class.""""""\n\n        def __init__(self, initial_value: Union[tf.Tensor, np.ndarray]) -> None:\n            self.variable = tf.Variable(initial_value, dtype=tf.bool, trainable=False)\n            self.initializer = self.variable.initializer\n            super(Variable, self).__init__(self.variable.read_value())\n\n        def __repr__(self) -> str:\n            return ""Variable(shape={})"".format(self.shape)\n\n        def assign_from_native(self, value: np.ndarray) -> tf.Operation:\n            assert isinstance(value, np.ndarray), type(value)\n            return self.assign_from_same(FACTORY.tensor(value))\n\n        def assign_from_same(self, value: Tensor) -> tf.Operation:\n            assert isinstance(value, Tensor), type(value)\n            return tf.assign(self.variable, value.value).op\n\n    FACTORY = Factory()  # pylint: disable=invalid-name\n\n    return FACTORY\n'"
tf_encrypted/tensor/factory.py,1,"b'""""""Abstract classes for factories and their components.""""""\nimport abc\nfrom typing import Optional\nfrom typing import Union\n\nimport numpy as np\nimport tensorflow as tf\n\n\nclass AbstractTensor(abc.ABC):\n    """"""\n  An abstraction to use when building tensor classes and subclasses\n  in factories.\n  """"""\n\n    @property\n    @abc.abstractproperty\n    def factory(self):\n        pass\n\n    @property\n    @abc.abstractproperty\n    def shape(self):\n        pass\n\n    @property\n    @abc.abstractproperty\n    def support(self):\n        pass\n\n    @abc.abstractmethod\n    def identity(self):\n        pass\n\n\n# pylint: disable=abstract-method\n\n\nclass AbstractConstant(AbstractTensor):\n    pass\n\n\nclass AbstractPlaceholder(AbstractTensor):\n    pass\n\n\nclass AbstractVariable(AbstractTensor):\n    pass\n\n\n# pylint: enable=abstract-method\n\n\nclass AbstractFactory(abc.ABC):\n    """"""An abstraction to use when building tensor factories.""""""\n\n    @property\n    @abc.abstractmethod\n    def modulus(self) -> int:\n        """"""The modulus used by this data type.""""""\n\n    @property\n    @abc.abstractmethod\n    def native_type(self):\n        """"""The underlying TensorFlow dtype used by this data type.""""""\n\n    @abc.abstractmethod\n    def tensor(self, value: Union[tf.Tensor, np.ndarray]):\n        """"""Wrap raw `value` in this data type as a tensor.""""""\n\n    @abc.abstractmethod\n    def constant(self, value: np.ndarray):\n        """"""Create a constant of this data type using raw `value`.""""""\n\n    @abc.abstractmethod\n    def variable(self, initial_value):\n        """"""Create a variable of this data type using raw `initial_value`.""""""\n\n    @abc.abstractmethod\n    def placeholder(self, shape):\n        """"""Create a placeholder of this data type.""""""\n\n    @abc.abstractmethod\n    def sample_uniform(\n        self, shape, minval: Optional[int] = None, maxval: Optional[int] = None\n    ):\n        """"""Sample uniform random value of this data type.""""""\n\n    @abc.abstractmethod\n    def sample_bounded(self, shape, bitlength: int):\n        """"""Sample uniform random value of this data type.""""""\n\n    @abc.abstractmethod\n    def stack(self, xs: list, axis: int = 0):\n        """"""Stack tensors of this data type together.""""""\n\n    @abc.abstractmethod\n    def concat(self, xs: list, axis: int):\n        """"""Concatenate tensors of this data type together.""""""\n'"
tf_encrypted/tensor/fixed.py,0,"b'""""""A fixed-point configuration to support various tensor types.""""""\nfrom __future__ import absolute_import\n\nfrom math import ceil\nfrom math import log2\n\nfrom .factory import AbstractFactory\n\n\n# NOTE the assumption in encoding/decoding is that encoded numbers will fit\n#      into signed int32\nclass FixedpointConfig:\n    """"""\n  Helper class containing various parameters of fixed-point precision\n  tensors.\n  """"""\n\n    def __init__(\n        self,\n        scaling_base: int,\n        precision_integral: int,\n        precision_fractional: int,\n        matmul_threshold: int,\n        truncation_gap: int,\n        use_noninteractive_truncation: bool,\n    ) -> None:\n        self.scaling_base = scaling_base\n        self.precision_integral = precision_integral\n        self.precision_fractional = precision_fractional\n        self.matmul_threshold = matmul_threshold\n        self.truncation_gap = truncation_gap\n        self.use_noninteractive_truncation = use_noninteractive_truncation\n\n    @property\n    def bound_single_precision(self) -> int:\n        total_precision = self.precision_integral + self.precision_fractional\n        return self.scaling_base ** (total_precision)\n\n    @property\n    def bound_double_precision(self) -> int:\n        total_precision = self.precision_integral + 2 * self.precision_fractional\n        return self.scaling_base ** (total_precision)\n\n    @property\n    def bound_intermediate_results(self) -> int:\n        return self.bound_double_precision * self.matmul_threshold\n\n    @property\n    def scaling_factor(self) -> int:\n        return self.scaling_base ** self.precision_fractional\n\n\nfixed100 = FixedpointConfig(\n    scaling_base=2,\n    precision_integral=14,\n    precision_fractional=16,\n    matmul_threshold=1024,\n    truncation_gap=40,\n    use_noninteractive_truncation=False,\n)\n\nfixed100_ni = FixedpointConfig(\n    scaling_base=2,\n    precision_integral=14,\n    precision_fractional=16,\n    matmul_threshold=1024,\n    truncation_gap=20,\n    use_noninteractive_truncation=True,\n)\n\n# TODO[Morten] make sure values in int64 configs make sense\n\nfixed64 = FixedpointConfig(\n    scaling_base=3,\n    precision_integral=7,\n    precision_fractional=8,\n    matmul_threshold=256,\n    truncation_gap=20,\n    use_noninteractive_truncation=False,\n)\n\nfixed64_ni = FixedpointConfig(\n    scaling_base=2,\n    precision_integral=10,\n    precision_fractional=13,\n    matmul_threshold=256,\n    truncation_gap=20,\n    use_noninteractive_truncation=True,\n)\n\n\ndef _validate_fixedpoint_config(\n    config: FixedpointConfig, tensor_factory: AbstractFactory\n) -> bool:\n    """"""\n  Ensure the given FixedpointConfig is compatible with the current\n  tensor_factory, preventing silent errors.\n  """"""\n    no_issues = True\n\n    check_32bit = ceil(log2(config.bound_single_precision)) > 31\n    check_64bit = ceil(log2(config.bound_single_precision)) > 63\n    trunc_over_mod = ceil(\n        log2(config.bound_double_precision)\n    ) + config.truncation_gap >= log2(tensor_factory.modulus)\n\n    if check_32bit:\n        print(""WARNING: Plaintext values won\'t fit in 32bit tensors"")\n        no_issues = False\n\n    if check_64bit:\n        print(""WARNING: Plaintext values won\'t fit in 64bit values"")\n        no_issues = False\n\n    if trunc_over_mod:\n        print(""WARNING: Modulus is too small for truncation"")\n        no_issues = False\n\n    # TODO[Morten] test for intermediate size wrt native type\n\n    # TODO[Morten] in decoding we assume that x + bound fits within the native\n    #              type of the backing tensor\n\n    # TODO[Morten] truncation gap is statistical security for interactive\n    #              truncation; write assertions\n\n    return no_issues\n'"
tf_encrypted/tensor/helpers.py,0,"b'""""""Useful math helper functions.""""""\nfrom functools import reduce\nfrom math import log\nfrom typing import Tuple\n\n\ndef egcd(a: int, b: int) -> Tuple[int, int, int]:\n    if a == 0:\n        return (b, 0, 1)\n    g, y, x = egcd(b % a, a)\n    return (g, x - (b // a) * y, y)\n\n\ndef gcd(a: int, b: int) -> int:\n    g, _, _ = egcd(a, b)\n    return g\n\n\ndef inverse(a: int, m: int) -> int:\n    g, b, _ = egcd(a, m)\n    assert g == 1\n    return b % m\n\n\ndef log2(x):\n    return log(x) / log(2)\n\n\ndef prod(xs):\n    return reduce(lambda x, y: x * y, xs)\n'"
tf_encrypted/tensor/int100.py,77,"b'# pylint: disable=missing-function-docstring\n""""""High-precision tensors that implement a fixed-point representation with\nan array of backing tensors with lower precision conforming to the Chinese\nremainder theorem.\n\nCurrently, we only use the CRT for an int100 tensor, although other\nhigh-precision tensor types are possible.""""""\nfrom __future__ import absolute_import\n\nimport abc\nimport math\nfrom functools import partial\nfrom functools import reduce\nfrom typing import List\nfrom typing import Optional\nfrom typing import Tuple\nfrom typing import Union\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom ..operations import secure_random\nfrom .factory import AbstractConstant\nfrom .factory import AbstractFactory\nfrom .factory import AbstractPlaceholder\nfrom .factory import AbstractTensor\nfrom .factory import AbstractVariable\nfrom .helpers import inverse\nfrom .helpers import prod\nfrom .shared import binarize\nfrom .shared import conv2d\nfrom .shared import im2col\n\n\ndef crt_factory(INT_TYPE, MODULI):  # pylint: disable=invalid-name\n    """"""Chinese remainder theorem tensor factory.""""""\n\n    # pylint: disable=invalid-name\n    MATMUL_THRESHOLD = 1024\n    MODULUS = prod(MODULI)\n    BITSIZE = math.ceil(math.log2(MODULUS))\n    # pylint: enable=invalid-name\n\n    # make sure we have room for lazy reductions:\n    # - 1 multiplication followed by 1024 additions\n    for mi in MODULI:\n        assert 2 * math.log2(mi) + math.log2(1024) < math.log2(INT_TYPE.max)\n\n    #\n    # methods benefitting from precomputation\n    #\n\n    def gen_crt_recombine_lagrange():\n\n        # precomputation\n        n = [MODULUS // mi for mi in MODULI]\n        lambdas = [ni * inverse(ni, mi) % MODULUS for ni, mi in zip(n, MODULI)]\n\n        def crt_recombine_lagrange(x):\n\n            with tf.name_scope(""crt_recombine_lagrange""):\n                res = sum(xi * li for xi, li in zip(x, lambdas)) % MODULUS\n                res = res.astype(object)\n                return res\n\n        return crt_recombine_lagrange\n\n    def gen_crt_recombine_explicit():\n\n        # precomputation\n        q = [inverse(MODULUS // mi, mi) for mi in MODULI]\n\n        def crt_recombine_explicit(x, bound):\n\n            big_b = MODULUS % bound\n            b = [np.array([(MODULUS // mi) % bound], dtype=np.int64) for mi in MODULI]\n\n            with tf.name_scope(""crt_recombine_explicit""):\n\n                if isinstance(x[0], np.ndarray):\n                    # backed by np.ndarray\n                    t = [(xi * qi) % mi for xi, qi, mi in zip(x, q, MODULI)]\n                    alpha = np.round(\n                        np.sum(\n                            [ti.astype(float) / mi for ti, mi in zip(t, MODULI)],\n                            axis=0,\n                        )\n                    )\n                    u = np.sum([ti * bi for ti, bi in zip(t, b)], axis=0).astype(\n                        np.int64\n                    )\n                    v = alpha.astype(np.int64) * big_b\n                    w = u - v\n                    res = w % bound\n                    res = res.astype(np.int32)\n                    return res\n\n                if isinstance(x[0], tf.Tensor):\n                    # backed by tf.Tensor\n                    t = [(xi * qi) % mi for xi, qi, mi in zip(x, q, MODULI)]\n                    alpha = tf.round(\n                        tf.reduce_sum(\n                            [tf.cast(ti, tf.float32) / mi for ti, mi in zip(t, MODULI)],\n                            axis=0,\n                        )\n                    )\n                    u = tf.cast(\n                        tf.reduce_sum([ti * bi for ti, bi in zip(t, b)], axis=0),\n                        tf.int64,\n                    )\n                    v = tf.cast(alpha, tf.int64) * big_b\n                    w = u - v\n                    res = w % bound\n                    res = tf.cast(res, INT_TYPE)\n                    return res\n\n                raise TypeError(""Don\'t know how to recombine {}"".format(type(x[0])))\n\n        return crt_recombine_explicit\n\n    def gen_crt_mod():\n\n        # outer precomputation\n        q = [inverse(MODULUS // mi, mi) for mi in MODULI]\n\n        def crt_mod(x, k):\n            assert isinstance(k, int), type(k)\n\n            # inner precomputations\n            big_b = MODULUS % k\n            b = [(MODULUS // mi) % k for mi in MODULI]\n\n            with tf.name_scope(""crt_mod""):\n                t = [(xi * qi) % mi for xi, qi, mi in zip(x, q, MODULI)]\n                alpha = tf.round(\n                    tf.reduce_sum(\n                        [tf.cast(ti, tf.float32) / mi for ti, mi in zip(t, MODULI)],\n                        axis=0,\n                    )\n                )\n                u = tf.reduce_sum([ti * bi for ti, bi in zip(t, b)], axis=0)\n                v = tf.cast(alpha, INT_TYPE) * big_b\n                w = u - v\n                return w % k\n\n        return crt_mod\n\n    crt_recombine_lagrange = gen_crt_recombine_lagrange()\n    crt_recombine_explicit = gen_crt_recombine_explicit()\n    crt_mod = gen_crt_mod()\n\n    #\n    # methods used in more than one place\n    #\n\n    def _crt_decompose(x):\n        return [x % mi for mi in MODULI]\n\n    def _crt_add(x, y):\n        return [(xi + yi) % mi for xi, yi, mi in zip(x, y, MODULI)]\n\n    def _crt_sub(x, y):\n        return [(xi - yi) % mi for xi, yi, mi in zip(x, y, MODULI)]\n\n    def _crt_mul(x, y):\n        return [(xi * yi) % mi for xi, yi, mi in zip(x, y, MODULI)]\n\n    def _crt_matmul(x, y):\n        return [tf.matmul(xi, yi) % mi for xi, yi, mi in zip(x, y, MODULI)]\n\n    def _construct_backing_from_chunks(chunk_sizes, chunk_values):\n        backing = _crt_decompose(0)\n        for chunk_size, chunk_value in zip(chunk_sizes, chunk_values):\n            scale = 2 ** chunk_size\n            backing = _crt_add(\n                _crt_mul(backing, _crt_decompose(scale)), _crt_decompose(chunk_value),\n            )\n        return backing\n\n    class Factory(AbstractFactory):\n        """"""CRT tensor factory.""""""\n\n        def zero(self):\n            backing = [tf.constant(0, dtype=INT_TYPE)] * len(MODULI)\n            return DenseTensor(backing)\n\n        def one(self):\n            backing = [tf.constant(1, dtype=INT_TYPE)] * len(MODULI)\n            return DenseTensor(backing)\n\n        def sample_uniform(\n            self, shape, minval: Optional[int] = None, maxval: Optional[int] = None\n        ):\n            assert minval is None\n            assert maxval is None\n\n            if secure_random.supports_seeded_randomness():\n                seeds = [secure_random.secure_seed() for _ in MODULI]\n                return UniformTensor(shape, seeds)\n\n            if secure_random.supports_secure_randomness():\n                backing = [\n                    secure_random.random_uniform(\n                        shape, minval=0, maxval=mi, dtype=INT_TYPE,\n                    )\n                    for mi in MODULI\n                ]\n                return DenseTensor(backing)\n\n            backing = [\n                tf.random_uniform(shape, minval=0, maxval=mi, dtype=INT_TYPE,)\n                for mi in MODULI\n            ]\n            return DenseTensor(backing)\n\n        def sample_bounded(self, shape, bitlength: int):\n\n            # TODO[Morten]\n            # bump to full range once signed numbers is settled (change minval etc)\n            chunk_max_bitlength = 30\n\n            q, r = bitlength // chunk_max_bitlength, bitlength % chunk_max_bitlength\n            chunk_sizes = [chunk_max_bitlength] * q + ([r] if r > 0 else [])\n\n            if secure_random.supports_seeded_randomness():\n                seeds = [secure_random.secure_seed() for _ in chunk_sizes]\n                return BoundedTensor(shape=shape, seeds=seeds, chunk_sizes=chunk_sizes)\n\n            if secure_random.supports_secure_randomness():\n                sampler = secure_random.random_uniform\n            else:\n                sampler = tf.random_uniform\n\n            chunk_values = [\n                sampler(shape=shape, minval=0, maxval=2 ** chunk_size, dtype=INT_TYPE)\n                for chunk_size in chunk_sizes\n            ]\n            backing = _construct_backing_from_chunks(chunk_sizes, chunk_values)\n            return DenseTensor(backing)\n\n        def stack(self, xs: list, axis: int = 0):\n            assert all(isinstance(x, Tensor) for x in xs)\n            backing = [\n                tf.stack([x.backing[i] for x in xs], axis=axis,)\n                for i in range(len(xs[0].backing))\n            ]\n            return DenseTensor(backing)\n\n        def concat(self, xs: list, axis: int = 0):\n            assert all(isinstance(x, Tensor) for x in xs)\n            backing = [\n                tf.concat([x.backing[i] for x in xs], axis=axis,)\n                for i in range(len(xs[0].backing))\n            ]\n            return DenseTensor(backing)\n\n        def tensor(self, value):\n\n            if isinstance(value, tf.Tensor):\n                backing = [\n                    tf.cast(component, dtype=INT_TYPE)\n                    for component in _crt_decompose(value)\n                ]\n                return DenseTensor(backing)\n\n            if isinstance(value, np.ndarray):\n                backing = [\n                    tf.convert_to_tensor(component, dtype=INT_TYPE)\n                    for component in _crt_decompose(value)\n                ]\n                return DenseTensor(backing)\n\n            raise TypeError((""Don\'t know how to handle "", ""{}"".format(type(value))))\n\n        def constant(self, value):\n\n            if isinstance(value, np.ndarray):\n                backing = [\n                    tf.constant(v, dtype=INT_TYPE) for v in _crt_decompose(value)\n                ]\n                return Constant(backing)\n\n            raise TypeError((""Don\'t know how to handle {}"".format(type(value))))\n\n        def variable(self, initial_value):\n\n            if isinstance(initial_value, (tf.Tensor, np.ndarray)):\n                return Variable(_crt_decompose(initial_value))\n\n            if isinstance(initial_value, Tensor):\n                return Variable(initial_value.backing)\n\n            raise TypeError((""Don\'t know how to handle {}"".format(type(initial_value))))\n\n        def placeholder(self, shape):\n            return Placeholder(shape)\n\n        @property\n        def min(self):\n            return 0\n\n        @property\n        def max(self):\n            return MODULUS\n\n        @property\n        def modulus(self) -> int:\n            return MODULUS\n\n        @property\n        def native_type(self):\n            return INT_TYPE\n\n    master_factory = Factory()\n\n    def _lift(x, y) -> Tuple[""Tensor"", ""Tensor""]:\n\n        if isinstance(x, Tensor) and isinstance(y, Tensor):\n            return x, y\n\n        if isinstance(x, Tensor):\n\n            if isinstance(y, int):\n                return x, x.factory.tensor(np.array([y]))\n\n        if isinstance(y, Tensor):\n\n            if isinstance(x, int):\n                return y.factory.tensor(np.array([x])), y\n\n        raise TypeError(""Don\'t know how to lift {} {}"".format(type(x), type(y)))\n\n    class Tensor(AbstractTensor):\n        """"""Base class for other CRT tensor classes.""""""\n\n        @abc.abstractproperty\n        @property\n        def backing(self):\n            pass\n\n        @abc.abstractproperty\n        @property\n        def shape(self):\n            pass\n\n        def identity(self):\n            backing = [tf.identity(x) for x in self.backing]\n            return DenseTensor(backing)\n\n        @property\n        def modulus(self):\n            return MODULUS\n\n        @property\n        def factory(self):\n            return master_factory\n\n        def to_native(self) -> Union[tf.Tensor, np.ndarray]:\n            return crt_recombine_explicit(self.backing, 2 ** 32)\n\n        def bits(\n            self,\n            factory: Optional[AbstractFactory] = None,\n            ensure_positive_interpretation: bool = False,\n        ) -> AbstractTensor:\n            """"""Convert to a pure bits representation.""""""\n\n            factory = factory or self.factory\n\n            with tf.name_scope(""to_bits""):\n\n                # we will extract the bits in chunks of 16 as that\'s reasonable for\n                # the explicit CRT\n                max_chunk_bitsize = 16\n                q, r = BITSIZE // max_chunk_bitsize, BITSIZE % max_chunk_bitsize\n                chunk_bitsizes = [max_chunk_bitsize] * q + ([r] if r > 0 else [])\n                chunks_modulus = [2 ** bitsize for bitsize in chunk_bitsizes]\n\n                remaining = self\n\n                if ensure_positive_interpretation:\n\n                    # To get the right bit pattern for negative numbers we need to apply\n                    # a correction to the first chunk. Unfortunately, this isn\'t known\n                    # until all bits have been extracted and hence we extract bits both\n                    # with and without the correction and select afterwards. Although\n                    # these two versions could be computed independently we here combine\n                    # them into a single tensor to keep the graph smaller.\n\n                    shape = self.shape.as_list()\n                    shape_value = [1] + shape\n                    shape_correction = [2] + [1] * len(shape)\n\n                    # this means that chunk[0] is uncorrected and chunk[1] is corrected\n                    correction_raw = [0, self.modulus % chunks_modulus[0]]\n                    correction = tf.constant(\n                        correction_raw,\n                        shape=shape_correction,\n                        dtype=self.factory.native_type,\n                    )\n\n                    remaining = remaining.reshape(shape_value)\n\n                # extract chunks\n                chunks = []\n                apply_correction = ensure_positive_interpretation\n                for chunk_modulus in chunks_modulus:\n\n                    # extract chunk from remaining\n                    chunk = crt_mod(remaining.backing, chunk_modulus)\n\n                    # apply correction only to the first chunk\n                    if apply_correction:\n                        chunk = (chunk + correction) % chunk_modulus\n                        apply_correction = False\n\n                    # save for below\n                    chunks.append(chunk)\n\n                    # perform right shift on remaining\n                    shifted = remaining - master_factory.tensor(chunk)\n                    remaining = shifted * inverse(chunk_modulus, self.modulus)\n\n                if ensure_positive_interpretation:\n                    # pick between corrected and uncorrected based on MSB\n                    msb = chunks[-1][0] >= (chunks_modulus[-1]) // 2\n                    chunks = [\n                        tf.where(msb, chunk[1], chunk[0],)  # corrected  # uncorrected\n                        for chunk in chunks\n                    ]\n\n                # extract bits from chunks\n                chunks_bits = [\n                    binarize(chunk, chunk_bitsize)\n                    for chunk, chunk_bitsize in zip(chunks, chunk_bitsizes)\n                ]\n\n                # combine bits of chunks\n                bits = tf.concat(chunks_bits, axis=-1)\n\n                return factory.tensor(bits)\n\n        def to_bigint(self) -> np.ndarray:\n            return crt_recombine_lagrange(self.backing)\n\n        def __getitem__(self, slc):\n            return DenseTensor([x[slc] for x in self.backing])\n\n        def __repr__(self) -> str:\n            return ""Tensor({})"".format(self.shape)\n\n        def __add__(self, other):\n            x, y = _lift(self, other)\n            return x.add(y)\n\n        def __radd__(self, other):\n            x, y = _lift(self, other)\n            return x.add(y)\n\n        def __sub__(self, other):\n            x, y = _lift(self, other)\n            return x.sub(y)\n\n        def __rsub__(self, other):\n            x, y = _lift(self, other)\n            return x.sub(y)\n\n        def __mul__(self, other):\n            x, y = _lift(self, other)\n            return x.mul(y)\n\n        def __rmul__(self, other):\n            x, y = _lift(self, other)\n            return x.mul(y)\n\n        def __mod__(self, k: int):\n            return self.mod(k)\n\n        def add(self, other):\n            x, y = _lift(self, other)\n            return DenseTensor(_crt_add(x.backing, y.backing))\n\n        def sub(self, other):\n            x, y = _lift(self, other)\n            return DenseTensor(_crt_sub(x.backing, y.backing))\n\n        def mul(self, other):\n            x, y = _lift(self, other)\n            return DenseTensor(_crt_mul(x.backing, y.backing))\n\n        def matmul(self, other):\n            """"""Matmul with other.""""""\n            x, y = _lift(self, other)\n\n            if x.shape[1] <= MATMUL_THRESHOLD:\n                # perform matmul directly (we have enough room)\n                z_backing = _crt_matmul(x.backing, y.backing)\n                return DenseTensor(z_backing)\n\n            # we need to split the tensors, process independently, and then recombine\n\n            with tf.name_scope(""split""):\n                z_split = []\n\n                num_columns = int(x.backing[0].shape[1])\n                num_split = int(math.ceil(num_columns / MATMUL_THRESHOLD))\n                for i in range(num_split):\n\n                    left = i * MATMUL_THRESHOLD\n                    right = (i + 1) * MATMUL_THRESHOLD\n\n                    inner_x = []  # type: List[Union[tf.Tensor, np.ndarray]]\n                    inner_y = []  # type: List[Union[tf.Tensor, np.ndarray]]\n\n                    for (xi, yi) in zip(x.backing, y.backing):\n                        inner_x.append(xi[:, left:right])\n                        inner_y.append(yi[left:right, :])\n\n                    z_split.append((inner_x, inner_y))\n\n            with tf.name_scope(""recombine""):\n                split_products = [_crt_matmul(xi, yi) for (xi, yi) in z_split]\n                z_backing = reduce(_crt_add, split_products)\n\n            return DenseTensor(z_backing)\n\n        def mod(self, k: int):\n            backing = _crt_decompose(crt_mod(self.backing, k))\n            return DenseTensor(backing)\n\n        def reduce_sum(self, axis, keepdims=None):\n            with tf.name_scope(""crt_reduce_sum""):\n                backing = [\n                    tf.reduce_sum(xi, axis, keepdims) % mi\n                    for (xi, mi) in zip(self.backing, MODULI)\n                ]\n                return DenseTensor(backing)\n\n        def cumsum(self, axis, exclusive, reverse):\n            with tf.name_scope(""crt_cumsum""):\n                backing = [\n                    tf.cumsum(xi, axis=axis, exclusive=exclusive, reverse=reverse) % mi\n                    for (xi, mi) in zip(self.backing, MODULI)\n                ]\n                return DenseTensor(backing)\n\n        def equal_zero(self, factory=None):\n            """"""Check equality with zero.""""""\n            factory = factory or master_factory\n\n            with tf.name_scope(""crt_equal_zero""):\n                zeros = [\n                    tf.cast(tf.equal(xi, 0), factory.native_type) for xi in self.backing\n                ]\n                number_of_zeros = tf.reduce_sum(zeros, axis=0)\n                backing = tf.equal(number_of_zeros, len(MODULI))\n                all_zeros = tf.cast(backing, factory.native_type)\n\n            return factory.tensor(all_zeros)\n\n        def equal(self, other, factory=None):\n            """"""Check equality with other.""""""\n            x, y = _lift(self, other)\n            factory = factory or x.factory\n\n            with tf.name_scope(""crt_equal""):\n                matches = [\n                    tf.cast(tf.equal(xi, yi), factory.native_type)\n                    for (xi, yi) in zip(x.backing, y.backing)\n                ]\n                number_of_matches = tf.reduce_sum(matches, axis=0)\n                backing = tf.equal(number_of_matches, len(MODULI))\n                all_matches = tf.cast(backing, factory.native_type)\n\n            return factory.tensor(all_matches)\n\n        def im2col(self, h_filter: int, w_filter: int, padding: str, stride: int):\n            with tf.name_scope(""crt_im2col""):\n                backing = [\n                    im2col(\n                        xi,\n                        h_filter=h_filter,\n                        w_filter=w_filter,\n                        padding=padding,\n                        stride=stride,\n                    )\n                    for xi in self.backing\n                ]\n                return DenseTensor(backing)\n\n        def conv2d(self, other, stride: int, padding: str = ""SAME""):\n            x, y = _lift(self, other)\n            return conv2d(x, y, stride, padding)  # type: ignore\n\n        def batch_to_space_nd(self, block_shape, crops):\n            with tf.name_scope(""crt_batch_to_space_nd""):\n                backing = [\n                    tf.batch_to_space_nd(xi, block_shape=block_shape, crops=crops)\n                    for xi in self.backing\n                ]\n                return DenseTensor(backing)\n\n        def space_to_batch_nd(self, block_shape, paddings):\n            with tf.name_scope(""crt_space_to_batch_nd""):\n                backing = [\n                    tf.space_to_batch_nd(xi, block_shape=block_shape, paddings=paddings)\n                    for xi in self.backing\n                ]\n                return DenseTensor(backing)\n\n        def transpose(self, perm):\n            backing = [tf.transpose(xi, perm=perm) for xi in self.backing]\n            return DenseTensor(backing)\n\n        def strided_slice(self, args, kwargs):\n            backing = [tf.strided_slice(xi, *args, **kwargs) for xi in self.backing]\n            return DenseTensor(backing)\n\n        def gather(self, indices: list, axis: int = 0):\n            backing = [tf.gather(xi, indices, axis=axis) for xi in self.backing]\n            return DenseTensor(backing)\n\n        def split(self, num_split: Union[int, list], axis: int = 0):\n            backings = zip(*[tf.split(xi, num_split, axis=axis) for xi in self.backing])\n            return [DenseTensor(backing) for backing in backings]\n\n        def reshape(self, axes: List[int]):\n            backing = [tf.reshape(xi, axes) for xi in self.backing]\n            return DenseTensor(backing)\n\n        def negative(self):\n            backing = [tf.negative(xi) % mi for xi, mi in zip(self.backing, MODULI)]\n            return DenseTensor(backing)\n\n        def expand_dims(self, axis: Optional[int] = None):\n            backing = [tf.expand_dims(xi, axis) for xi in self.backing]\n            return DenseTensor(backing)\n\n        def squeeze(self, axis: Optional[List[int]] = None):\n            backing = [tf.squeeze(xi, axis=axis) for xi in self.backing]\n            return DenseTensor(backing)\n\n        def truncate(self, amount, base=2):\n            factor = base ** amount\n            factor_inverse = inverse(factor, MODULUS)\n            return (self - (self % factor)) * factor_inverse\n\n        def right_shift(self, bitlength):\n            return self.truncate(bitlength, 2)\n\n        def cast(self, factory):\n            # NOTE(Morten) could add more convertion options\n            if factory is self.factory:\n                return self\n            raise ValueError(""Don\'t know how to cast into {}"".format(factory))\n\n    class DenseTensor(Tensor):\n        """"""Public CRT Tensor class.""""""\n\n        def __init__(self, backing):\n            self._backing = backing\n\n        @property\n        def shape(self):\n            return self._backing[0].shape\n\n        @property\n        def backing(self):\n            return self._backing\n\n        @property\n        def support(self):\n            return self._backing\n\n    class UniformTensor(Tensor):\n        """"""Class representing a uniform-random, lazily sampled tensor.\n\n    Lazy sampling optimizes communication by sending seeds in place of\n    fully-expanded tensors.""""""\n\n        def __init__(self, shape, seeds):\n            self._seeds = seeds\n            self._shape = shape\n\n        @property\n        def shape(self):\n            return self._shape\n\n        @property\n        def backing(self):\n            with tf.name_scope(""expand-seed""):\n                return [\n                    secure_random.seeded_random_uniform(\n                        self._shape, minval=0, maxval=mi, seed=seed, dtype=INT_TYPE\n                    )\n                    for (mi, seed) in zip(MODULI, self._seeds)\n                ]\n\n        @property\n        def support(self):\n            return self._seeds\n\n    class BoundedTensor(Tensor):\n        """"""CRT bounded-randomness tensor.""""""\n\n        def __init__(self, shape, seeds, chunk_sizes):\n            self._shape = shape\n            self._seeds = seeds\n            self._chunk_sizes = chunk_sizes\n\n        @property\n        def shape(self):\n            return self._shape\n\n        @property\n        def backing(self):\n            with tf.name_scope(""expand-seed""):\n                sampler = partial(\n                    secure_random.seeded_random_uniform,\n                    self._shape,\n                    minval=0,\n                    dtype=INT_TYPE,\n                )\n                zipped = zip(self._chunk_sizes, self._seeds)\n                chunk_values = [\n                    sampler(maxval=2 ** chunk_size, seed=seed_value)\n                    for (chunk_size, seed_value) in zipped\n                ]\n                return _construct_backing_from_chunks(self._chunk_sizes, chunk_values)\n\n        @property\n        def support(self):\n            return self._seeds\n\n    class Constant(DenseTensor, AbstractConstant):\n        """"""CRT Constant class.""""""\n\n        def __init__(self, backing) -> None:\n            assert all(isinstance(component, tf.Tensor) for component in backing)\n            super(Constant, self).__init__(backing)\n\n        def __repr__(self) -> str:\n            return ""Constant({})"".format(self.shape)\n\n    class Placeholder(DenseTensor, AbstractPlaceholder):\n        """"""CRT Placeholder class.""""""\n\n        def __init__(self, shape) -> None:\n            self.placeholders = [tf.placeholder(INT_TYPE, shape=shape) for _ in MODULI]\n            super(Placeholder, self).__init__(self.placeholders)\n\n        def __repr__(self):\n            return ""Placeholder({})"".format(self.shape)\n\n        def feed(self, value):\n            assert isinstance(value, np.ndarray), type(value)\n            backing = _crt_decompose(value)\n            return dict(zip(self.placeholders, backing))\n\n    class Variable(DenseTensor, AbstractVariable):\n        """"""CRT Variable class.""""""\n\n        def __init__(self, initial_backing) -> None:\n            self.variables = [\n                tf.Variable(val, dtype=INT_TYPE, trainable=False)\n                for val in initial_backing\n            ]\n            self.initializer = tf.group(*[var.initializer for var in self.variables])\n            backing = [var.read_value() for var in self.variables]\n            super(Variable, self).__init__(backing)\n\n        def __repr__(self):\n            return ""Variable({})"".format(self.shape)\n\n        def assign_from_native(self, value: np.ndarray):\n            assert isinstance(value, np.ndarray), type(value)\n            return self.assign_from_same(master_factory.tensor(value))\n\n        def assign_from_same(self, value: Tensor):\n            assert isinstance(value, Tensor), type(value)\n            assign_ops = [\n                tf.assign(xi, vi).op for (xi, vi) in zip(self.variables, value.backing)\n            ]\n            return tf.group(*assign_ops)\n\n    return master_factory\n\n\n#\n# 32 bit CRT\n# - we need this to do matmul as int32 is the only supported type for that\n# - tried tf.float64 but didn\'t work out of the box\n# - 10 components for modulus ~100 bits\n#\n\nint100factory = crt_factory(\n    INT_TYPE=tf.int32,\n    MODULI=[1201, 1433, 1217, 1237, 1321, 1103, 1129, 1367, 1093, 1039],\n)\n'"
tf_encrypted/tensor/int100_test.py,12,"b'# pylint: disable=missing-docstring\nimport math\nimport random\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.tensor import fixed100_ni\nfrom tf_encrypted.tensor import int100factory\n\n\nclass TestInt100Tensor(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_pond(self) -> None:\n\n        with tfe.protocol.Pond(\n            None, tensor_factory=int100factory, fixedpoint_config=fixed100_ni,\n        ) as prot:\n\n            x = prot.define_private_variable(np.array([2, 2]), apply_scaling=False)\n            y = prot.define_public_variable(np.array([2, 2]), apply_scaling=False)\n\n            z = x * y\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                actual = sess.run(z.reveal())\n\n            expected = np.array([4, 4])\n            np.testing.assert_array_almost_equal(actual, expected, decimal=3)\n\n    def core_test_binarize(\n        self, raw, shape, modulus, bitlen, ensure_positive_interpretation,\n    ) -> None:\n        def as_bits(x: int, min_bitlength):\n            bits = [int(b) for b in ""{0:b}"".format(x)]\n            bits = [0] * (min_bitlength - len(bits)) + bits\n            return list(reversed(bits))\n\n        expected = np.array([as_bits((modulus + x) % modulus, bitlen) for x in raw])\n        expected = expected.reshape(shape + (bitlen,))\n\n        x = int100factory.tensor(np.array(raw).reshape(shape))\n        epi = ensure_positive_interpretation\n        y = x.bits(ensure_positive_interpretation=epi).to_native()\n\n        with tf.Session() as sess:\n            actual = sess.run(y)\n\n        np.testing.assert_array_equal(actual, expected)\n\n    def test_binarize_positive(self) -> None:\n        lower = -int100factory.modulus // 2 + 1\n        upper = int100factory.modulus // 2\n\n        random.seed(1234)\n        raw = [-1, 0, 1] + [random.randint(lower, upper) for _ in range(256 - 3)]\n        shape = (2, 2, 2, -1)\n\n        bitlen = math.ceil(math.log2(int100factory.modulus))\n        modulus = int100factory.modulus\n        self.core_test_binarize(raw, shape, modulus, bitlen, True)\n\n    def test_binarize_symmetric(self) -> None:\n        lower = -int100factory.modulus // 2 + 1\n        upper = int100factory.modulus // 2\n\n        random.seed(1234)\n        raw = [-1, 0, 1] + [random.randint(lower, upper) for _ in range(256 - 3)]\n        shape = (2, 2, 2, -1)\n\n        bitlen = math.ceil(math.log2(int100factory.modulus))\n        modulus = 2 ** bitlen\n        self.core_test_binarize(raw, shape, modulus, bitlen, False)\n\n\nclass TestConv2D(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_forward(self) -> None:\n\n        # input\n        batch_size, channels_in, channels_out = 32, 3, 64\n        img_height, img_width = 28, 28\n        input_shape = (batch_size, channels_in, img_height, img_width)\n        input_conv = np.random.normal(size=input_shape).astype(np.int32)\n\n        # filters\n        h_filter, w_filter, strides = 2, 2, 2\n        filter_shape = (h_filter, w_filter, channels_in, channels_out)\n        filter_values = np.random.normal(size=filter_shape).astype(np.int32)\n\n        inp = int100factory.tensor(input_conv)\n        out = inp.conv2d(int100factory.tensor(filter_values), strides)\n        with tf.Session() as sess:\n            actual = sess.run(out.to_native())\n\n        # reset graph\n        tf.reset_default_graph()\n\n        # convolution tensorflow\n        with tf.Session() as sess:\n            # conv input\n            x = tf.Variable(input_conv, dtype=tf.float32)\n            x_nhwc = tf.transpose(x, (0, 2, 3, 1))\n\n            # convolution Tensorflow\n            filters_tf = tf.Variable(filter_values, dtype=tf.float32)\n\n            conv_out_tf = tf.nn.conv2d(\n                x_nhwc, filters_tf, strides=[1, strides, strides, 1], padding=""SAME""\n            )\n\n            sess.run(tf.global_variables_initializer())\n            expected = sess.run(conv_out_tf).transpose(0, 3, 1, 2)\n\n        np.testing.assert_array_almost_equal(actual, expected, decimal=3)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/tensor/native.py,54,"b'""""""Native tensors and their factory.\n\nThese use TensorFlow\'s native dtypes tf.int32 and tf.int64 for the given float\nencoding being used (fixed-point, etc.).""""""\nfrom __future__ import absolute_import\n\nimport abc\nimport math\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Tuple\nfrom typing import Union\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom ..operations import secure_random\nfrom .factory import AbstractConstant\nfrom .factory import AbstractFactory\nfrom .factory import AbstractPlaceholder\nfrom .factory import AbstractTensor\nfrom .factory import AbstractVariable\nfrom .helpers import inverse\nfrom .shared import binarize\nfrom .shared import conv2d\nfrom .shared import im2col\n\n\ndef native_factory(\n    NATIVE_TYPE, EXPLICIT_MODULUS=None,\n):  # pylint: disable=invalid-name\n    """"""Constructs the native tensor Factory.""""""\n\n    class Factory(AbstractFactory):\n        """"""Native tensor factory.""""""\n\n        def tensor(self, value):\n\n            if isinstance(value, tf.Tensor):\n                if value.dtype is not self.native_type:\n                    value = tf.cast(value, dtype=self.native_type)\n                return DenseTensor(value)\n\n            if isinstance(value, np.ndarray):\n                value = tf.convert_to_tensor(value, dtype=self.native_type)\n                return DenseTensor(value)\n\n            raise TypeError(""Don\'t know how to handle {}"".format(type(value)))\n\n        def constant(self, value):\n\n            if isinstance(value, np.ndarray):\n                value = tf.constant(value, dtype=self.native_type)\n                return Constant(value)\n\n            raise TypeError(""Don\'t know how to handle {}"".format(type(value)))\n\n        def variable(self, initial_value):\n\n            if isinstance(initial_value, (tf.Tensor, np.ndarray)):\n                return Variable(initial_value)\n\n            if isinstance(initial_value, Tensor):\n                return Variable(initial_value.value)\n\n            msg = ""Don\'t know how to handle {}""\n            raise TypeError(msg.format(type(initial_value)))\n\n        def placeholder(self, shape):\n            return Placeholder(shape)\n\n        @property\n        def min(self):\n            if EXPLICIT_MODULUS is not None:\n                return 0\n            return NATIVE_TYPE.min\n\n        @property\n        def max(self):\n            if EXPLICIT_MODULUS is not None:\n                return EXPLICIT_MODULUS\n            return NATIVE_TYPE.max\n\n        @property\n        def modulus(self) -> int:\n            if EXPLICIT_MODULUS is not None:\n                return EXPLICIT_MODULUS\n            return NATIVE_TYPE.max - NATIVE_TYPE.min + 1\n\n        @property\n        def native_type(self):\n            return NATIVE_TYPE\n\n        @property\n        def nbits(self):\n            return NATIVE_TYPE.size * 8\n\n        def sample_uniform(\n            self, shape, minval: Optional[int] = None, maxval: Optional[int] = None\n        ):\n            minval = self.min if minval is None else minval\n            # TODO(Morten) believe this should be native_type.max+1\n            maxval = self.max if maxval is None else maxval\n\n            if secure_random.supports_seeded_randomness():\n                seed = secure_random.secure_seed()\n                return UniformTensor(\n                    shape=shape, seed=seed, minval=minval, maxval=maxval\n                )\n\n            if secure_random.supports_secure_randomness():\n                sampler = secure_random.random_uniform\n            else:\n                sampler = tf.random_uniform\n            value = sampler(\n                shape=shape, minval=minval, maxval=maxval, dtype=NATIVE_TYPE\n            )\n            return DenseTensor(value)\n\n        def sample_seeded_uniform(\n            self,\n            shape,\n            seed,\n            minval: Optional[int] = None,\n            maxval: Optional[int] = None,\n        ):\n            """"""Seeded sample of a random tensor.\n\n            Arguments:\n                shape (tuple of ints), shape of the tensor to sample\n                seed (int), seed for the sampler to use\n                minval (int), the a in the interval [a,b]\n                maxval (int), the b in the interval [a,b]\n\n            Returns a tensor of shape `shape` drawn from a uniform distribution over\n            the interval [minval,maxval].\n            """"""\n            minval = self.min if minval is None else minval\n            maxval = self.max if maxval is None else maxval\n\n            if secure_random.supports_seeded_randomness():\n                # Don\'t use UniformTensor for lazy sampling here, because the `seed`\n                # might be something (e.g., key) we want to protect, and we cannot\n                # send it to another party\n                value = secure_random.seeded_random_uniform(\n                    shape=shape,\n                    dtype=NATIVE_TYPE,\n                    minval=minval,\n                    maxval=maxval,\n                    seed=seed,\n                )\n                return DenseTensor(value)\n\n            raise NotImplementedError(\n                ""Secure seeded randomness implementation is not available.""\n            )\n\n        def sample_bounded(self, shape, bitlength: int):\n            maxval = 2 ** bitlength\n            assert maxval <= self.max\n\n            if secure_random.supports_seeded_randomness():\n                seed = secure_random.secure_seed()\n                return UniformTensor(shape=shape, seed=seed, minval=0, maxval=maxval)\n\n            if secure_random.supports_secure_randomness():\n                sampler = secure_random.random_uniform\n            else:\n                sampler = tf.random_uniform\n            value = sampler(shape=shape, minval=0, maxval=maxval, dtype=NATIVE_TYPE)\n            return DenseTensor(value)\n\n        def sample_bits(self, shape):\n            return self.sample_bounded(shape, bitlength=1)\n\n        def stack(self, xs: list, axis: int = 0):\n            assert all(isinstance(x, Tensor) for x in xs)\n            value = tf.stack([x.value for x in xs], axis=axis)\n            return DenseTensor(value)\n\n        def concat(self, xs: list, axis: int):\n            assert all(isinstance(x, Tensor) for x in xs)\n            value = tf.concat([x.value for x in xs], axis=axis)\n            return DenseTensor(value)\n\n        def where(self, condition, x, y, v2=False):\n            if not isinstance(condition, tf.Tensor):\n                msg = ""Don\'t know how to handle `condition` of type {}""\n                raise TypeError(msg.format(type(condition)))\n            if not v2:\n                value = tf.where(condition, x.value, y.value)\n            else:\n                value = tf.compat.v2.where(condition, x.value, y.value)\n            return DenseTensor(value)\n\n    FACTORY = Factory()  # pylint: disable=invalid-name\n\n    def _lift(x, y) -> Tuple[""Tensor"", ""Tensor""]:\n\n        if isinstance(x, Tensor) and isinstance(y, Tensor):\n            return x, y\n\n        if isinstance(x, Tensor):\n\n            if isinstance(y, int):\n                return x, x.factory.tensor(np.array([y]))\n\n        if isinstance(y, Tensor):\n\n            if isinstance(x, int):\n                return y.factory.tensor(np.array([x])), y\n\n        raise TypeError(""Don\'t know how to lift {} {}"".format(type(x), type(y)))\n\n    class Tensor(AbstractTensor):\n        """"""Base class for other native tensor classes.""""""\n\n        @property\n        @abc.abstractproperty\n        def value(self):\n            pass\n\n        @property\n        @abc.abstractproperty\n        def shape(self):\n            pass\n\n        def identity(self):\n            value = tf.identity(self.value)\n            return DenseTensor(value)\n\n        def to_native(self) -> tf.Tensor:\n            return self.value\n\n        def bits(self, factory=None) -> AbstractTensor:\n            factory = factory or FACTORY\n            if EXPLICIT_MODULUS is None:\n                return factory.tensor(binarize(self.value))\n            bitsize = bitsize = math.ceil(math.log2(EXPLICIT_MODULUS))\n            return factory.tensor(binarize(self.value % EXPLICIT_MODULUS, bitsize))\n\n        def __repr__(self) -> str:\n            return ""{}(shape={})"".format(type(self), self.shape)\n\n        @property\n        def factory(self):\n            return FACTORY\n\n        def __add__(self, other):\n            x, y = _lift(self, other)\n            return x.add(y)\n\n        def __radd__(self, other):\n            x, y = _lift(self, other)\n            return y.add(x)\n\n        def __sub__(self, other):\n            x, y = _lift(self, other)\n            return x.sub(y)\n\n        def __rsub__(self, other):\n            x, y = _lift(self, other)\n            return y.sub(x)\n\n        def __mul__(self, other):\n            x, y = _lift(self, other)\n            return x.mul(y)\n\n        def __rmul__(self, other):\n            x, y = _lift(self, other)\n            return x.mul(y)\n\n        def __mod__(self, k: int):\n            return self.mod(k)\n\n        def __neg__(self):\n            return self.mul(-1)\n\n        def __getitem__(self, slc):\n            return DenseTensor(self.value[slc])\n\n        def add(self, other):\n            x, y = _lift(self, other)\n            value = x.value + y.value\n            if EXPLICIT_MODULUS is not None:\n                value %= EXPLICIT_MODULUS\n            return DenseTensor(value)\n\n        def sub(self, other):\n            x, y = _lift(self, other)\n            value = x.value - y.value\n            if EXPLICIT_MODULUS is not None:\n                value %= EXPLICIT_MODULUS\n            return DenseTensor(value)\n\n        def mul(self, other):\n            x, y = _lift(self, other)\n            value = x.value * y.value\n            if EXPLICIT_MODULUS is not None:\n                value %= EXPLICIT_MODULUS\n            return DenseTensor(value)\n\n        def matmul(self, other):\n            x, y = _lift(self, other)\n            value = tf.matmul(x.value, y.value)\n            if EXPLICIT_MODULUS is not None:\n                value %= EXPLICIT_MODULUS\n            return DenseTensor(value)\n\n        def im2col(self, h_filter, w_filter, padding, stride):\n            i2c = im2col(self.value, h_filter, w_filter, padding, stride)\n            return DenseTensor(i2c)\n\n        def conv2d(self, other, stride: int, padding: str = ""SAME""):\n            if EXPLICIT_MODULUS is not None:\n                # TODO(Morten) any good reason this wasn\'t implemented for PrimeTensor?\n                raise NotImplementedError()\n            x, y = _lift(self, other)\n            return conv2d(x, y, stride, padding)\n\n        def batch_to_space_nd(self, block_shape, crops):\n            value = tf.batch_to_space_nd(self.value, block_shape, crops)\n            return DenseTensor(value)\n\n        def space_to_batch_nd(self, block_shape, paddings):\n            value = tf.space_to_batch_nd(self.value, block_shape, paddings)\n            return DenseTensor(value)\n\n        def mod(self, k: int):\n            value = self.value % k\n            if EXPLICIT_MODULUS is not None:\n                value %= EXPLICIT_MODULUS\n            return DenseTensor(value)\n\n        def transpose(self, perm):\n            return DenseTensor(tf.transpose(self.value, perm))\n\n        def strided_slice(self, args, kwargs):\n            return DenseTensor(tf.strided_slice(self.value, *args, **kwargs))\n\n        def gather(self, indices: list, axis: int = 0):\n            return DenseTensor(tf.gather(self.value, indices, axis=axis))\n\n        def split(self, num_split: Union[int, list], axis: int = 0):\n            values = tf.split(self.value, num_split, axis=axis)\n            return [DenseTensor(value) for value in values]\n\n        def reshape(self, axes: Union[tf.Tensor, List[int]]):\n            return DenseTensor(tf.reshape(self.value, axes))\n\n        def negative(self):\n            value = tf.negative(self.value)\n            if EXPLICIT_MODULUS is not None:\n                value %= EXPLICIT_MODULUS\n            return DenseTensor(value)\n\n        def reduce_sum(self, axis, keepdims=None):\n            value = tf.reduce_sum(self.value, axis, keepdims)\n            if EXPLICIT_MODULUS is not None:\n                value %= EXPLICIT_MODULUS\n            return DenseTensor(value)\n\n        def cumsum(self, axis, exclusive, reverse):\n            value = tf.cumsum(\n                self.value, axis=axis, exclusive=exclusive, reverse=reverse\n            )\n            if EXPLICIT_MODULUS is not None:\n                value %= EXPLICIT_MODULUS\n            return DenseTensor(value)\n\n        def equal_zero(self, factory=None):\n            factory = factory or FACTORY\n            return factory.tensor(\n                tf.cast(tf.equal(self.value, 0), dtype=factory.native_type)\n            )\n\n        def equal(self, other, factory=None):\n            x, y = _lift(self, other)\n            factory = factory or FACTORY\n            return factory.tensor(\n                tf.cast(tf.equal(x.value, y.value), dtype=factory.native_type)\n            )\n\n        def truncate(self, amount, base=2):\n            if base == 2:\n                return self.right_shift(amount)\n            factor = base ** amount\n            factor_inverse = inverse(factor, self.factory.modulus)\n            return (self - (self % factor)) * factor_inverse\n\n        def right_shift(self, bitlength):\n            return DenseTensor(tf.bitwise.right_shift(self.value, bitlength))\n\n        def expand_dims(self, axis: Optional[int] = None):\n            return DenseTensor(tf.expand_dims(self.value, axis))\n\n        def squeeze(self, axis: Optional[List[int]] = None):\n            return DenseTensor(tf.squeeze(self.value, axis=axis))\n\n        def cast(self, factory):\n            return factory.tensor(self.value)\n\n        def __or__(self, other):\n            return self.bitwise_or(other)\n\n        def bitwise_or(self, other):\n            x, y = _lift(self, other)\n            value = tf.bitwise.bitwise_or(x.value, y.value)\n            return DenseTensor(value)\n\n        def __xor__(self, other):\n            return self.bitwise_xor(other)\n\n        def bitwise_xor(self, other):\n            x, y = _lift(self, other)\n            value = tf.bitwise.bitwise_xor(x.value, y.value)\n            return DenseTensor(value)\n\n        def __and__(self, other):\n            return self.bitwise_and(other)\n\n        def bitwise_and(self, other):\n            x, y = _lift(self, other)\n            value = tf.bitwise.bitwise_and(x.value, y.value)\n            return DenseTensor(value)\n\n        def __invert__(self):\n            return self.invert()\n\n        def invert(self):\n            value = tf.bitwise.invert(self.value)\n            return DenseTensor(value)\n\n        def __lshift__(self, bitlength):\n            return self.left_shift(bitlength)\n\n        def left_shift(self, bitlength):\n            return DenseTensor(tf.bitwise.left_shift(self.value, bitlength))\n\n        def __rshift__(self, bitlength):\n            """"""\n      Arithmetic shift.\n      Please refer to `self.logical_rshift` for a logical right shift.\n      """"""\n            return self.right_shift(bitlength)\n\n        def logical_rshift(self, bitlength):\n            """"""Computes a bitshift to the right.""""""\n            # There is some bug in TF when casting from int to uint: the uint result\n            # becomes 0 so the following code does not work.\n            # Bug report: https://github.com/tensorflow/tensorflow/issues/30215\n            #\n            # cast_map = {tf.int8: tf.uint8, tf.int16: tf.uint16,\n            #             tf.int32: tf.uint32, tf.int64: tf.uint64}\n            # x = tf.bitwise.right_shift(\n            #     tf.cast(self.value, dtype=cast_map[NATIVE_TYPE]), bitlength)\n            # x = tf.cast(x, NATIVE_TYPE)\n            #\n            # Instead, we have to do the following slightly more sophisticated stuff.\n            if bitlength < 0:\n                raise ValueError(""Unsupported shift steps."")\n            if bitlength == 0:\n                return self\n            total = NATIVE_TYPE.size * 8\n            mask = ~((-1) << (total - bitlength))\n            x = tf.bitwise.right_shift(self.value, bitlength)\n            x = tf.bitwise.bitwise_and(x, mask)\n            return DenseTensor(x)\n\n    class DenseTensor(Tensor):\n        """"""Public native Tensor class.""""""\n\n        def __init__(self, value):\n            self._value = value\n\n        @property\n        def shape(self):\n            return self._value.shape\n\n        @property\n        def value(self):\n            return self._value\n\n        @property\n        def support(self):\n            return [self._value]\n\n    class UniformTensor(Tensor):\n        """"""Class representing a uniform-random, lazily sampled tensor.\n\n    Lazy sampling optimizes communication by sending seeds in place of\n    fully-expanded tensors.""""""\n\n        def __init__(self, shape, seed, minval, maxval):\n            self._seed = seed\n            self._shape = shape\n            self._minval = minval\n            self._maxval = maxval\n\n        @property\n        def shape(self):\n            return self._shape\n\n        @property\n        def value(self):\n            with tf.name_scope(""expand-seed""):\n                return secure_random.seeded_random_uniform(\n                    shape=self._shape,\n                    dtype=NATIVE_TYPE,\n                    minval=self._minval,\n                    maxval=self._maxval,\n                    seed=self._seed,\n                )\n\n        @property\n        def support(self):\n            return [self._seed]\n\n    class Constant(DenseTensor, AbstractConstant):\n        """"""Native Constant class.""""""\n\n        def __init__(self, constant: tf.Tensor) -> None:\n            assert isinstance(constant, tf.Tensor)\n            super(Constant, self).__init__(constant)\n\n        def __repr__(self) -> str:\n            return ""Constant(shape={})"".format(self.shape)\n\n    class Placeholder(DenseTensor, AbstractPlaceholder):\n        """"""Native Placeholder class.""""""\n\n        def __init__(self, shape: List[int]) -> None:\n            self.placeholder = tf.placeholder(NATIVE_TYPE, shape=shape)\n            super(Placeholder, self).__init__(self.placeholder)\n\n        def __repr__(self) -> str:\n            return ""Placeholder(shape={})"".format(self.shape)\n\n        def feed(self, value: np.ndarray) -> Dict[tf.Tensor, np.ndarray]:\n            assert isinstance(value, np.ndarray), type(value)\n            return {self.placeholder: value}\n\n    class Variable(DenseTensor, AbstractVariable):\n        """"""Native Variable class.""""""\n\n        def __init__(self, initial_value: Union[tf.Tensor, np.ndarray]) -> None:\n            self.variable = tf.Variable(\n                initial_value, dtype=NATIVE_TYPE, trainable=False\n            )\n            self.initializer = self.variable.initializer\n            super(Variable, self).__init__(self.variable.read_value())\n\n        def __repr__(self) -> str:\n            return ""Variable(shape={})"".format(self.shape)\n\n        def assign_from_native(self, value: np.ndarray) -> tf.Operation:\n            assert isinstance(value, np.ndarray), type(value)\n            return self.assign_from_same(FACTORY.tensor(value))\n\n        def assign_from_same(self, value: Tensor) -> tf.Operation:\n            assert isinstance(value, Tensor), type(value)\n            return tf.assign(self.variable, value.value).op\n\n    return FACTORY\n'"
tf_encrypted/tensor/native_int32_test.py,12,"b'# pylint: disable=missing-docstring\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf_encrypted.tensor import int32factory\n\n\nclass TestInt32Tensor(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_binarize(self) -> None:\n        x = int32factory.tensor(\n            np.array(\n                [2 ** 32 + 3, 2 ** 31 - 1, 2 ** 31, -3]  # == 3  # max  # min\n            ).reshape(2, 2)\n        )\n\n        y = x.bits()\n\n        # fmt: off\n        expected = np.array([\n            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n            [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        ]).reshape([2, 2, 32])\n        # fmt: on\n\n        with tf.Session() as sess:\n            actual = sess.run(y.to_native())\n\n        np.testing.assert_array_equal(actual, expected)\n\n    def test_random_binarize(self) -> None:\n        x_in = np.random.uniform(low=2 ** 31 + 1, high=2 ** 31 - 1, size=2000,).astype(\n            ""int32""\n        )\n        x = int32factory.tensor(x_in)\n\n        y = x.bits()\n\n        with tf.Session() as sess:\n            actual = sess.run(y.to_native())\n\n        j = 0\n        for i in x_in.tolist():\n            if i < 0:\n                binary = bin(((1 << 32) - 1) & i)[2:][::-1]\n            else:\n                binary = bin(i)\n                binary = binary[2:].zfill(32)[::-1]\n            bin_list = np.array(list(binary)).astype(np.int32)\n            np.testing.assert_equal(actual[j], bin_list)\n            j += 1\n\n\nclass TestConv2D(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_forward(self) -> None:\n        # input\n        batch_size, channels_in, channels_out = 32, 3, 64\n        img_height, img_width = 28, 28\n        input_shape = (batch_size, channels_in, img_height, img_width)\n        input_conv = np.random.normal(size=input_shape).astype(np.int32)\n\n        # filters\n        h_filter, w_filter, strides = 2, 2, 2\n        filter_shape = (h_filter, w_filter, channels_in, channels_out)\n        filter_values = np.random.normal(size=filter_shape).astype(np.int32)\n\n        inp = int32factory.tensor(input_conv)\n        out = inp.conv2d(int32factory.tensor(filter_values), strides)\n        with tf.Session() as sess:\n            actual = sess.run(out.to_native())\n\n        # reset graph\n        tf.reset_default_graph()\n\n        # convolution tensorflow\n        with tf.Session() as sess:\n            # conv input\n            x = tf.Variable(input_conv, dtype=tf.float32)\n            x_nhwc = tf.transpose(x, (0, 2, 3, 1))\n\n            # convolution Tensorflow\n            filters_tf = tf.Variable(filter_values, dtype=tf.float32)\n\n            conv_out_tf = tf.nn.conv2d(\n                x_nhwc, filters_tf, strides=[1, strides, strides, 1], padding=""SAME"",\n            )\n\n            sess.run(tf.global_variables_initializer())\n            out_tensorflow = sess.run(conv_out_tf).transpose(0, 3, 1, 2)\n\n        np.testing.assert_array_almost_equal(actual, out_tensorflow, decimal=3)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/tensor/native_int64_test.py,16,"b'# pylint: disable=missing-docstring\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.tensor import fixed64\nfrom tf_encrypted.tensor import int64factory\n\n\nclass TestInt64Tensor(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_pond(self) -> None:\n\n        with tfe.protocol.Pond(\n            tensor_factory=int64factory, fixedpoint_config=fixed64,\n        ) as prot:\n\n            x = prot.define_private_variable(np.array([2, 2]), apply_scaling=False)\n            y = prot.define_public_variable(np.array([2, 2]), apply_scaling=False)\n\n            z = x * y\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                out = sess.run(z.reveal())\n                np.testing.assert_array_almost_equal(out, [4, 4], decimal=3)\n\n    def test_binarize(self) -> None:\n        x = int64factory.tensor(\n            tf.constant(\n                [2 ** 62 + 3, 2 ** 63 - 1, 2 ** 63 - 2, -3],\n                shape=[2, 2],\n                dtype=tf.int64,\n            )\n        )\n\n        y = x.bits()\n\n        # fmt: off\n        expected = np.array([\n            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n            [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n            [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        ]).reshape([2, 2, 64])\n        # fmt: on\n\n        with tf.Session() as sess:\n            actual = sess.run(y.to_native())\n\n        np.testing.assert_array_equal(actual, expected)\n\n    def test_random_binarize(self) -> None:\n        x_in = (\n            np.random.uniform(low=2 ** 63 + 1, high=2 ** 63 - 1, size=2000,)\n            .astype(np.int64)\n            .tolist()\n        )\n        x = int64factory.tensor(tf.constant(x_in, dtype=tf.int64))\n\n        y = x.bits()\n\n        with tf.Session() as sess:\n            actual = sess.run(y.to_native())\n\n        j = 0\n        for i in x_in:\n            if i < 0:\n                binary = bin(((1 << 64) - 1) & i)[2:][::-1]\n            else:\n                binary = bin(i)\n                binary = binary[2:].zfill(64)[::-1]\n            bin_list = np.array(list(binary)).astype(np.int64)\n            np.testing.assert_equal(actual[j], bin_list)\n            j += 1\n\n\nclass TestConv2D(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_forward(self) -> None:\n        # input\n        batch_size, channels_in, channels_out = 32, 3, 64\n        img_height, img_width = 28, 28\n        input_shape = (batch_size, channels_in, img_height, img_width)\n        input_conv = np.random.normal(size=input_shape).astype(np.int64)\n\n        # filters\n        h_filter, w_filter, strides = 2, 2, 2\n        filter_shape = (h_filter, w_filter, channels_in, channels_out)\n        filter_values = np.random.normal(size=filter_shape).astype(np.int64)\n\n        x_in = int64factory.tensor(input_conv)\n        out = x_in.conv2d(int64factory.tensor(filter_values), strides)\n        with tf.Session() as sess:\n            actual = sess.run(out.to_native())\n\n        # reset graph\n        tf.reset_default_graph()\n\n        # convolution tensorflow\n        with tf.Session() as sess:\n            # conv input\n            x = tf.Variable(input_conv, dtype=tf.float32)\n            x_nhwc = tf.transpose(x, (0, 2, 3, 1))\n\n            # convolution Tensorflow\n            filters_tf = tf.Variable(filter_values, dtype=tf.float32)\n\n            conv_out_tf = tf.nn.conv2d(\n                x_nhwc, filters_tf, strides=[1, strides, strides, 1], padding=""SAME"",\n            )\n\n            sess.run(tf.global_variables_initializer())\n            out_tensorflow = sess.run(conv_out_tf).transpose(0, 3, 1, 2)\n\n        np.testing.assert_array_almost_equal(actual, out_tensorflow, decimal=3)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/tensor/native_prime_test.py,9,"b'# pylint: disable=missing-docstring\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf_encrypted.tensor import native_factory\n\n\nclass TestPrimeTensor(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def set_up_indexing(self):\n        m = 2 ** 31\n\n        prime_factory = native_factory(tf.int32, m)\n\n        self.np_fix1d = np.arange(24)\n        self.np_fix2d = self.np_fix1d.reshape(8, 3)\n        self.np_fix3d = self.np_fix1d.reshape(2, 4, 3)\n        self.np_fix4d = self.np_fix1d.reshape(2, 2, 2, 3)\n        self.prime_fix1d = prime_factory.tensor(self.np_fix1d)\n        self.prime_fix2d = prime_factory.tensor(self.np_fix2d)\n        self.prime_fix3d = prime_factory.tensor(self.np_fix3d)\n        self.prime_fix4d = prime_factory.tensor(self.np_fix4d)\n        self.np_fixtures = [getattr(self, ""np_fix{}d"".format(i)) for i in range(1, 5)]\n        self.prime_fixtures = [\n            getattr(self, ""prime_fix{}d"".format(i)) for i in range(1, 5)\n        ]\n\n    @unittest.skip\n    def test_basic_indexing(self):\n        self.set_up_indexing()\n        for np_fix, prime_fix in zip(self.np_fixtures, self.prime_fixtures):\n            n = len(np_fix.shape)\n            for filler in [0, 1, -1]:\n                ixs = [filler for _ in range(n)]\n                np.testing.assert_equal(np_fix[ixs], prime_fix[ixs].value)\n\n    @unittest.skip\n    def test_slice_indexing(self):\n        self.set_up_indexing()\n        for np_fix, prime_fix in zip(self.np_fixtures, self.prime_fixtures):\n            ndim = len(np_fix.shape)\n            if ndim == 1:\n                np.testing.assert_equal(\n                    np_fix[2:5], prime_fix[2:5].value,\n                )\n                continue\n            np.testing.assert_equal(\n                np_fix[:, 0], prime_fix[:, 0].value,\n            )\n            np.testing.assert_equal(\n                np_fix[:, 1], prime_fix[:, 1].value,\n            )\n            np.testing.assert_equal(\n                np_fix[:, -1], prime_fix[:, -1].value,\n            )\n            if ndim > 2:\n                np.testing.assert_equal(\n                    np_fix[:, :-1, ...], prime_fix[:, :-1, ...].value,\n                )\n                np.testing.assert_equal(\n                    np_fix[:, :1, ...], prime_fix[:, :1, ...].value,\n                )\n                np.testing.assert_equal(\n                    np_fix[:, 1:, ...], prime_fix[:, 1:, ...].value,\n                )\n            elif ndim == 2:\n                np.testing.assert_equal(\n                    np_fix[:, :2], prime_fix[:, :-1].value,\n                )\n                np.testing.assert_equal(\n                    np_fix[:, 1:], prime_fix[:, 1:].value,\n                )\n\n    @unittest.skip\n    def test_ellipsis_indexing(self):\n        self.set_up_indexing()\n        for np_fix, prime_fix in zip(self.np_fixtures, self.prime_fixtures):\n            np.testing.assert_equal(\n                np_fix[0, ...], prime_fix[0, ...].value,\n            )\n            np.testing.assert_equal(\n                np_fix[1, ...], prime_fix[1, ...].value,\n            )\n            np.testing.assert_equal(\n                np_fix[..., -1], prime_fix[..., -1].value,\n            )\n\n    def test_arithmetic(self) -> None:\n        prime_factory = native_factory(tf.int32, 2 ** 16)\n\n        x = prime_factory.tensor(tf.constant([2 ** 16, 2 ** 16 + 1]))\n        y = prime_factory.tensor(tf.constant([2 ** 16 + 2, 2]))\n\n        with tf.Session() as sess:\n            z = (x * y).value\n            z0 = sess.run(z)\n\n            np.testing.assert_array_equal(z0, np.array([0, 2]))\n\n            z = (x + y).value\n            z1 = sess.run(z)\n\n            np.testing.assert_array_equal(z1, np.array([2, 3]))\n\n            z = (x - y).value\n            z2 = sess.run(z)\n\n            np.testing.assert_array_equal(z2, np.array([65534, 65535]))\n\n    def test_binarize(self) -> None:\n        prime_factory = native_factory(tf.int32, 1001)\n\n        x = prime_factory.tensor(\n            tf.constant(\n                [3, -1, 0], shape=[3], dtype=np.int32  # == 3  # == p-1 == max  # min\n            )\n        )\n\n        y = x.bits()\n\n        expected = np.array(\n            [\n                [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n                [0, 0, 0, 1, 0, 1, 1, 1, 1, 1],\n                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            ]\n        ).reshape([3, 10])\n\n        with tf.Session() as sess:\n            actual = sess.run(y.value)\n\n        np.testing.assert_array_equal(actual, expected)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/tensor/shared.py,23,"b'""""""Commonly used tensor functions.""""""\nimport math\nfrom typing import Optional\nfrom typing import Union\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom .factory import AbstractTensor\n\n\ndef binarize(tensor: tf.Tensor, bitsize: Optional[int] = None,) -> tf.Tensor:\n    """"""Extract bits of values in `tensor`, returning a `tf.Tensor` with same\n  dtype.""""""\n\n    with tf.name_scope(""binarize""):\n        bitsize = bitsize or (tensor.dtype.size * 8)\n\n        bit_indices_shape = [1] * len(tensor.shape) + [bitsize]\n        bit_indices = tf.range(bitsize, dtype=tensor.dtype)\n        bit_indices = tf.reshape(bit_indices, bit_indices_shape)\n\n        val = tf.expand_dims(tensor, -1)\n        val = tf.bitwise.bitwise_and(tf.bitwise.right_shift(val, bit_indices), 1)\n\n        assert val.dtype == tensor.dtype\n        return val\n\n\ndef bits(tensor: tf.Tensor, bitsize: Optional[int] = None,) -> list:\n    """"""Extract bits of values in `tensor`, returning a list of tensors.""""""\n\n    with tf.name_scope(""bits""):\n        bitsize = bitsize or (tensor.dtype.size * 8)\n        the_bits = [\n            tf.bitwise.bitwise_and(tf.bitwise.right_shift(tensor, i), 1)\n            for i in range(bitsize)\n        ]\n        return the_bits\n        # return tf.stack(bits, axis=-1)\n\n\ndef im2col(\n    x: Union[tf.Tensor, np.ndarray],\n    h_filter: int,\n    w_filter: int,\n    padding: str,\n    stride: int,\n) -> tf.Tensor:\n    """"""Generic implementation of im2col on tf.Tensors.""""""\n\n    with tf.name_scope(""im2col""):\n\n        # we need NHWC because tf.extract_image_patches expects this\n        nhwc_tensor = tf.transpose(x, [0, 2, 3, 1])\n        channels = int(nhwc_tensor.shape[3])\n\n        # extract patches\n        patch_tensor = tf.extract_image_patches(\n            nhwc_tensor,\n            ksizes=[1, h_filter, w_filter, 1],\n            strides=[1, stride, stride, 1],\n            rates=[1, 1, 1, 1],\n            padding=padding,\n        )\n\n        # change back to NCHW\n        patch_tensor_nchw = tf.reshape(\n            tf.transpose(patch_tensor, [3, 1, 2, 0]), (h_filter, w_filter, channels, -1)\n        )\n\n        # reshape to x_col\n        x_col_tensor = tf.reshape(\n            tf.transpose(patch_tensor_nchw, [2, 0, 1, 3]),\n            (channels * h_filter * w_filter, -1),\n        )\n\n        return x_col_tensor\n\n\ndef conv2d(x: AbstractTensor, y: AbstractTensor, stride, padding,) -> AbstractTensor:\n    """"""Generic convolution implementation with im2col over AbstractTensors.""""""\n\n    with tf.name_scope(""conv2d""):\n\n        h_filter, w_filter, in_filters, out_filters = map(int, y.shape)\n        n_x, c_x, h_x, w_x = map(int, x.shape)\n\n        if c_x != in_filters:\n            # in depthwise conv the filter\'s in and out dimensions are reversed\n            out_filters = in_filters\n\n        if padding == ""SAME"":\n            h_out = int(math.ceil(float(h_x) / float(stride)))\n            w_out = int(math.ceil(float(w_x) / float(stride)))\n        elif padding == ""VALID"":\n            h_out = int(math.ceil(float(h_x - h_filter + 1) / float(stride)))\n            w_out = int(math.ceil(float(w_x - w_filter + 1) / float(stride)))\n        else:\n            raise ValueError(""Don\'t know padding method \'{}\'"".format(padding))\n\n        x_col = x.im2col(h_filter, w_filter, padding, stride)\n        w_col = y.transpose([3, 2, 0, 1]).reshape([int(out_filters), -1])\n        out = w_col.matmul(x_col)\n\n        out = out.reshape([out_filters, h_out, w_out, n_x])\n        out = out.transpose([3, 0, 1, 2])\n\n        return out\n'"
examples/notebooks/keras-training/common.py,13,"b'""""""Provide classes to perform private training and private prediction with\nlogistic regression""""""\n\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\n\n\nclass DataOwner:\n    """"""Contains code meant to be executed by a data owner Player.""""""\n\n    def __init__(\n        self, player_name, num_features, training_set_size, test_set_size, batch_size\n    ):\n        self.player_name = player_name\n        self.num_features = num_features\n        self.training_set_size = training_set_size\n        self.test_set_size = test_set_size\n        self.batch_size = batch_size\n        self.train_initializer = None\n        self.test_initializer = None\n\n    @property\n    def initializer(self):\n        return tf.group(self.train_initializer, self.test_initializer)\n\n    @tfe.local_computation\n    def provide_training_data(self):\n        """"""Preprocess training dataset\n\n    Return single batch of training dataset\n    """"""\n\n        def norm(x, y):\n            return tf.cast(x, tf.float32), tf.expand_dims(y, 0)\n\n        x_raw = tf.random.uniform(\n            minval=-0.5, maxval=0.5, shape=[self.training_set_size, self.num_features]\n        )\n\n        y_raw = tf.cast(tf.reduce_mean(x_raw, axis=1) > 0, dtype=tf.float32)\n\n        train_set = (\n            tf.data.Dataset.from_tensor_slices((x_raw, y_raw))\n            .map(norm)\n            .repeat()\n            .shuffle(buffer_size=self.batch_size)\n            .batch(self.batch_size)\n        )\n\n        train_set_iterator = train_set.make_initializable_iterator()\n        self.train_initializer = train_set_iterator.initializer\n\n        x, y = train_set_iterator.get_next()\n        x = tf.reshape(x, [self.batch_size, self.num_features])\n        y = tf.reshape(y, [self.batch_size, 1])\n\n        return x, y\n\n    @tfe.local_computation\n    def provide_testing_data(self):\n        """"""Preprocess testing dataset\n\n    Return single batch of testing dataset\n    """"""\n\n        def norm(x, y):\n            return tf.cast(x, tf.float32), tf.expand_dims(y, 0)\n\n        x_raw = tf.random.uniform(\n            minval=-0.5, maxval=0.5, shape=[self.test_set_size, self.num_features]\n        )\n\n        y_raw = tf.cast(tf.reduce_mean(x_raw, axis=1) > 0, dtype=tf.float32)\n\n        test_set = (\n            tf.data.Dataset.from_tensor_slices((x_raw, y_raw))\n            .map(norm)\n            .batch(self.test_set_size)\n        )\n\n        test_set_iterator = test_set.make_initializable_iterator()\n        self.test_initializer = test_set_iterator.initializer\n\n        x, y = test_set_iterator.get_next()\n        x = tf.reshape(x, [self.test_set_size, self.num_features])\n        y = tf.reshape(y, [self.test_set_size, 1])\n\n        return x, y\n'"
primitives/tf_encrypted/primitives/__init__.py,0,"b'from . import paillier\nfrom . import sodium\n\n__all__ = [\n    ""paillier"",\n    ""sodium"",\n]\n'"
primitives/tf_encrypted/test/__init__.py,0,"b'from .execution_context import tf_execution_context\n\n__all__ = [\n    ""tf_execution_context"",\n]\n'"
primitives/tf_encrypted/test/execution_context.py,2,"b'import contextlib\n\nimport tensorflow as tf\n\n\nclass EagerExecutionContext:\n    def scope(self):\n        return contextlib.suppress()\n\n    def evaluate(self, value):\n        return value.numpy()\n\n\nclass GraphExecutionContext:\n    def __init__(self):\n        self._graph = None\n        self._session = None\n\n    @property\n    def graph(self):\n        if self._graph is None:\n            self._graph = tf.Graph()\n        return self._graph\n\n    @property\n    def session(self):\n        if self._session is None:\n            with self._graph.as_default():\n                self._session = tf.compat.v1.Session()\n        return self._session\n\n    def scope(self):\n        return self.graph.as_default()\n\n    def evaluate(self, value):\n        return self.session.run(value)\n\n\ndef tf_execution_context(run_eagerly):\n    if run_eagerly:\n        return EagerExecutionContext()\n    return GraphExecutionContext()\n'"
primitives/tf_encrypted/test/execution_context_test.py,3,"b'# pylint: disable=missing-docstring\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\nfrom absl.testing import parameterized\n\nfrom tf_encrypted.test import tf_execution_context\n\n\nclass TestExecutionContext(parameterized.TestCase):\n    @parameterized.parameters({""run_eagerly"": True}, {""run_eagerly"": False})\n    def test_tf_execution_mode(self, run_eagerly):\n        context = tf_execution_context(run_eagerly)\n        with context.scope():\n            x = tf.fill(dims=(2, 2), value=5.0)\n            assert tf.executing_eagerly() == run_eagerly\n\n        assert isinstance(x, tf.Tensor)\n        actual_result = context.evaluate(x)\n        assert isinstance(actual_result, np.ndarray)\n\n        expected_result = np.array([[5.0, 5.0], [5.0, 5.0]])\n        np.testing.assert_equal(actual_result, expected_result)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/convert/gen/__init__.py,0,b''
tf_encrypted/convert/gen/generate_reserved_scopes.py,0,"b'""""""Generates the Reserved Scopes table in tf_encrypted/convert/README.md\nfrom properly formatted key-values in `specops.yaml`.""""""\nimport os\n\nfrom tf_encrypted.convert.register import REGISTERED_SPECOPS\n\nHD = """"""Reserved Scope | TF Counterpart\\n---------------|---------------\\n""""""\n\n\ndef _table_from_registered_specops():\n    """"""Generate a Markdown table from REGISTERED_SPECOPS.""""""\n    table_string = HD\n    for scope, attr_dict in REGISTERED_SPECOPS.items():\n        tf_name = attr_dict[""tf-name""]\n        hyperlink = attr_dict[""hyperlink""]\n        table_string += """"""`{scope}`|[{name}]({hyperlink})\\n"""""".format(\n            scope=scope, name=tf_name, hyperlink=hyperlink,\n        )\n\n    return table_string\n\n\nreserved_scopes_table = _table_from_registered_specops()\n\nthis_file = os.path.realpath(__file__)\ngen_dir = os.path.dirname(this_file)\nconvert_dir = os.path.dirname(gen_dir)\n\ntemplate_path = os.path.join(gen_dir, ""readme_template.md"")\nreadme_path = os.path.join(convert_dir, ""README.md"")\n\nwith open(template_path, ""r"") as template_file:\n    template_str = template_file.read()\n    with open(readme_path, ""w"") as target_readme:\n        with_table = template_str.format(reserved_scopes=reserved_scopes_table)\n        target_readme.write(with_table)\n'"
tf_encrypted/keras/engine/__init__.py,0,"b'""""""Higher-level layer abstractions built on TF Encrypted.""""""\nfrom __future__ import absolute_import\n\nfrom .base_layer import Layer\n\n__all__ = [\n    ""Layer"",\n]\n'"
tf_encrypted/keras/engine/base_layer.py,0,"b'""""""Includes base classes used by all layer types.""""""\nimport logging\nfrom abc import ABC\n\nimport numpy as np\nfrom tensorflow.python.keras.utils import generic_utils\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.keras import backend as KE\nfrom tf_encrypted.keras.engine.base_layer_utils import unique_object_name\nfrom tf_encrypted.protocol.pond import PondMaskedTensor\nfrom tf_encrypted.protocol.pond import PondPrivateTensor\n\nlogger = logging.getLogger(""tf_encrypted"")\n\n\nclass Layer(ABC):\n    """"""\n    Base layer class.\n    This is the class from which all layers inherit.\n    A layer is a class implementing common neural networks operations, such\n    as convolution, batch norm, etc. These operations require managing weights,\n    losses, updates, and inter-layer connectivity.\n    Users will just instantiate a layer and then treat it as a callable.\n    We recommend that descendants of `Layer` implement the following methods:\n    * `__init__()`: Save configuration in member variables\n    * `build()`: Called once from `__call__`, when we know the shapes of inputs\n        and `dtype`.\n    * `call()`: Called in `__call__` after making sure `build()` has been called\n        once. Should actually perform the logic of applying the layer to the\n        input tensors (which should be passed in as the first argument).\n    """"""\n\n    def __init__(self, trainable=True, name=None, **kwargs):\n\n        allowed_kwargs = {\n            ""input_shape"",\n            ""batch_input_shape"",\n            ""batch_size"",\n            ""weights"",\n            ""activity_regularizer"",\n            ""dtype"",\n        }\n        # Validate optional keyword arguments.\n        for kwarg in kwargs:\n            if kwarg not in allowed_kwargs:\n                raise TypeError(""Keyword argument not understood:"", kwarg)\n\n        if ""input_shape"" in kwargs:\n            logger.warning(\n                ""Currently input_shape argument semantics include the ""\n                ""batch dimension. Please construct you model ""\n                ""accordingly.""\n            )\n            self._batch_input_shape = kwargs[""input_shape""]\n        if ""batch_input_shape"" in kwargs:\n            self._batch_input_shape = kwargs[""batch_input_shape""]\n\n        self.trainable = trainable\n        self._init_set_name(name)\n        self.built = False\n        self.weights = []\n\n    def build(self, input_shape):  # pylint: disable=unused-argument\n        """"""Creates the variables of the layer (optional, for subclass implementers).\n        This is a method that implementers of subclasses of `Layer`\n        can override if they need a state-creation step in-between\n        layer instantiation and layer call.\n        This is typically used to create the weights of `Layer` subclasses.\n        Arguments:\n        input_shape: Instance of `TensorShape`, or list of instances of\n            `TensorShape` if the layer expects a list of inputs\n            (one instance per input).\n        """"""\n        self.built = True\n\n    def call(self, inputs):\n        """"""This is where the layer\'s logic lives.\n        Arguments:\n            inputs: Input tensor, or list/tuple of input tensors.\n        Returns:\n            A tensor or list/tuple of tensors.\n        """"""\n        return inputs\n\n    def compute_output_shape(self, input_shape):\n        """"""Returns the layer\'s output shape""""""\n\n    def __call__(self, inputs, *args, **kargs):\n        """"""Wraps `call`, applying pre- and post-processing steps.\n        Arguments:\n        inputs: input tensor(s).\n        *args: additional positional arguments to be passed to `self.call`.\n        **kwargs: additional keyword arguments to be passed to `self.call`.\n        Returns:\n        Output tensor(s).\n        """"""\n        if not self.built:\n            input_shapes = inputs.shape\n            self.build(input_shapes)\n\n            self.built = True\n\n        outputs = self.call(inputs, *args, **kargs)\n\n        return outputs\n\n    def add_weight(self, variable, make_private=True):\n        if make_private:\n            variable = tfe.define_private_variable(variable)\n            self.weights.append(variable)\n        else:\n            variable = tfe.define_public_variable(variable)\n            self.weights.append(variable)\n\n        return variable\n\n    def set_weights(self, weights, sess=None):\n        """"""Sets the weights of the layer.\n        Arguments:\n        weights: A list of Numpy arrays with shapes and types\n            matching the output of layer.get_weights() or a list\n            of private variables\n        sess: tfe session""""""\n\n        weights_types = (np.ndarray, PondPrivateTensor, PondMaskedTensor)\n        assert isinstance(weights[0], weights_types), type(weights[0])\n\n        # Assign new keras weights to existing weights defined by\n        # default when tfe layer was instantiated\n        if not sess:\n            sess = KE.get_session()\n\n        if isinstance(weights[0], np.ndarray):\n            for i, w in enumerate(self.weights):\n                shape = w.shape.as_list()\n                tfe_weights_pl = tfe.define_private_placeholder(shape)\n                fd = tfe_weights_pl.feed(weights[i].reshape(shape))\n                sess.run(tfe.assign(w, tfe_weights_pl), feed_dict=fd)\n        elif isinstance(weights[0], PondPrivateTensor):\n            for i, w in enumerate(self.weights):\n                shape = w.shape.as_list()\n                sess.run(tfe.assign(w, weights[i].reshape(shape)))\n\n    @property\n    def name(self):\n        return self._name\n\n    def _init_set_name(self, name, zero_based=True):\n        if not name:\n            self._name = unique_object_name(\n                generic_utils.to_snake_case(self.__class__.__name__),\n                zero_based=zero_based,\n            )\n        else:\n            self._name = name\n'"
tf_encrypted/keras/engine/base_layer_utils.py,2,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Utilities borrowed from the tf.keras codebase.""""""\nimport collections\nimport weakref\n\nimport tensorflow as tf\n\n# A global dictionary mapping graph objects to an index of counters used\n# for various layer/optimizer names in each graph.\n# Allows to give unique autogenerated names to layers, in a graph-specific way.\nPER_GRAPH_OBJECT_NAME_UIDS = weakref.WeakKeyDictionary()\n\n\ndef _get_default_graph_uid_map():\n    graph = tf.get_default_graph()\n    name_uid_map = PER_GRAPH_OBJECT_NAME_UIDS.get(graph, None)\n    if name_uid_map is None:\n        name_uid_map = collections.defaultdict(int)\n        PER_GRAPH_OBJECT_NAME_UIDS[graph] = name_uid_map\n    return name_uid_map\n\n\ndef unique_object_name(\n    name, name_uid_map=None, avoid_names=None, namespace="""", zero_based=False,\n):\n    """"""Makes a object name (or arbitrary string) unique within a TensorFlow graph.\n  Arguments:\n    name: String name to make unique.\n    name_uid_map: An optional defaultdict(int) to use when creating unique\n      names. If None (default), uses a per-Graph dictionary.\n    avoid_names: An optional set or dict with names which should not be used. If\n      None (default) does not avoid any names.\n    namespace: Gets a name which is unique within the (graph, namespace). Layers\n      which are not Networks use a blank namespace and so get graph-global\n      names.\n    zero_based: If True, name sequences start with no suffix (e.g. ""dense"",\n      ""dense_1""). If False, naming is one-based (""dense_1"", ""dense_2"").\n  Returns:\n    Unique string name.\n  Example:\n  ```python\n  _unique_layer_name(\'dense\')  # dense_1\n  _unique_layer_name(\'dense\')  # dense_2\n  ```\n  """"""\n    if name_uid_map is None:\n        name_uid_map = _get_default_graph_uid_map()\n    if avoid_names is None:\n        avoid_names = set()\n    proposed_name = None\n    while proposed_name is None or proposed_name in avoid_names:\n        name_key = (namespace, name)\n        if zero_based:\n            number = name_uid_map[name_key]\n            if number:\n                proposed_name = name + ""_"" + str(number)\n            else:\n                proposed_name = name\n            name_uid_map[name_key] += 1\n        else:\n            name_uid_map[name_key] += 1\n            proposed_name = name + ""_"" + str(name_uid_map[name_key])\n    return proposed_name\n'"
tf_encrypted/keras/engine/input_layer.py,1,"b'""""""Providing Inputs to tfe models.  For internal use in Sequential only.""""""\nfrom tensorflow.keras import backend\nfrom tensorflow.python.framework import tensor_shape\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.keras.engine.base_layer import Layer\n\n\nclass InputLayer(Layer):\n    """"""Layer to be used as an entry point into a Network (a graph of layers).\n  It can either wrap an existing tensor (pass an `input_tensor` argument)\n  or create its a placeholder tensor (pass arguments `input_shape`, and\n  optionally, `dtype`).\n  It is generally recommend to use the functional layer API via `Input`,\n  (which creates an `InputLayer`) without directly using `InputLayer`.\n  Arguments:\n      input_shape: Shape tuple (not including the batch axis), or `TensorShape`\n        instance (not including the batch axis).\n      batch_size: Optional input batch size (integer or None).\n      dtype: Datatype of the input.\n      input_tensor: Optional tensor to use as layer input\n          instead of creating a placeholder.\n      sparse: Boolean, whether the placeholder created\n          is meant to be sparse.\n      name: Name of the layer (string).\n  """"""\n\n    def __init__(\n        self,\n        input_shape=None,\n        batch_size=None,\n        dtype=None,\n        input_tensor=None,\n        sparse=False,\n        name=None,\n        **kwargs,\n    ):\n        if ""batch_input_shape"" in kwargs:\n            batch_input_shape = kwargs.pop(""batch_input_shape"")\n            if input_shape and batch_input_shape:\n                raise ValueError(\n                    ""Only provide the input_shape OR ""\n                    ""batch_input_shape argument to ""\n                    ""InputLayer, not both at the same time.""\n                )\n            batch_size = batch_input_shape[0]\n            input_shape = batch_input_shape[1:]\n        if kwargs:\n            raise ValueError(""Unrecognized keyword arguments:"", kwargs.keys())\n\n        if not name:\n            prefix = ""input""\n            name = prefix + ""_"" + str(backend.get_uid(prefix))\n\n        if batch_size is None:\n            raise NotImplementedError()\n        if input_tensor is not None:\n            raise NotImplementedError()\n        if dtype is not None:\n            raise NotImplementedError()\n        if sparse:\n            raise NotImplementedError()\n\n        super(InputLayer, self).__init__()\n        self.built = True\n        self.batch_size = batch_size\n\n        if isinstance(input_shape, tensor_shape.TensorShape):\n            input_shape = tuple(input_shape.as_list())\n        elif isinstance(input_shape, int):\n            input_shape = (input_shape,)\n\n        if input_shape is not None:\n            self._batch_input_shape = (batch_size,) + tuple(input_shape)\n        else:\n            raise ValueError(""Input shape must be defined for the first layer."")\n\n        # Create a graph placeholder to call the layer on.\n        self.placeholder = tfe.define_private_placeholder(self._batch_input_shape)\n\n\ndef Input(  # pylint: disable=invalid-name\n    shape=None,\n    batch_size=None,\n    name=None,\n    dtype=None,\n    sparse=False,\n    tensor=None,\n    **kwargs,\n):\n    """"""`Input()` is used to instantiate a Keras tensor.\n  A Keras tensor is a tensor object from the underlying backend\n  (TF Encrypted), which we augment with certain\n  attributes that allow us to build a Keras model.\n\n  Arguments:\n      shape: A shape tuple (integers), not including the batch size.\n          For instance, `shape=(32,)` indicates that the expected input\n          will be batches of 32-dimensional vectors.\n      batch_size: optional static batch size (integer).\n      name: An optional name string for the layer.\n          Should be unique in a model (do not reuse the same name twice).\n          It will be autogenerated if it isn\'t provided.\n      dtype: The data type expected by the input, as a string\n          (`float32`, `float64`, `int32`...)\n      sparse: A boolean specifying whether the placeholder\n          to be created is sparse.\n      tensor: Optional existing tensor to wrap into the `Input` layer.\n          If set, the layer will not create a placeholder tensor.\n      **kwargs: deprecated arguments support.\n\n  Returns:\n    A `tensor`.\n\n  Example:\n  ```python\n  # this is a logistic regression in Keras\n  x = Input(shape=(32,))\n  y = Dense(16, activation=\'softmax\')(x)\n  model = Model(x, y)\n  ```\n  Note that even if eager execution is enabled,\n  `Input` produces a symbolic tensor (i.e. a placeholder).\n  This symbolic tensor can be used with other\n  TensorFlow ops, as such:\n  ```python\n  x = Input(shape=(32,))\n  y = tf.square(x)\n  ```\n\n  Raises:\n    ValueError: in case of invalid arguments.\n  """"""\n    batch_shape = None\n    if ""batch_shape"" in kwargs:\n        batch_shape = kwargs.pop(""batch_shape"")\n        if shape and batch_shape:\n            raise ValueError(\n                ""Only provide the shape OR ""\n                ""batch_shape argument to ""\n                ""Input, not both at the same time.""\n            )\n        batch_size = batch_shape[0]\n        shape = batch_shape[1:]\n    if kwargs:\n        raise ValueError(""Unrecognized keyword arguments:"", kwargs.keys())\n\n    if shape is None and tensor is None:\n        raise ValueError(\n            ""Please provide to Input either a `shape`""\n            "" or a `tensor` argument. Note that ""\n            ""`shape` does not include the batch ""\n            ""dimension.""\n        )\n\n    if sparse:\n        raise NotImplementedError()\n    if dtype is not None:\n        raise NotImplementedError()\n    if tensor is not None:\n        raise NotImplementedError()\n\n    if batch_shape:\n        input_layer = InputLayer(\n            batch_input_shape=batch_shape,\n            name=name,\n            dtype=dtype,\n            sparse=sparse,\n            input_tensor=tensor,\n        )\n    else:\n        input_layer = InputLayer(\n            input_shape=shape,\n            batch_size=batch_size,\n            name=name,\n            dtype=dtype,\n            sparse=sparse,\n            input_tensor=tensor,\n        )\n\n    # Return tensor including `_keras_history`.\n    # Note that in this case train_output and test_output are the same pointer.\n    return input_layer.placeholder\n'"
tf_encrypted/keras/engine/input_layer_test.py,1,"b'# pylint: disable=missing-docstring\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.keras.engine.input_layer import Input\n\nnp.random.seed(42)\n\n\nclass TestInput(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_input(self):\n        x = Input(shape=(2,), batch_size=1)\n        fd = x.feed(np.random.normal(size=(1, 2)))\n\n        with tfe.Session() as sess:\n            sess.run(x.reveal(), feed_dict=fd)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/keras/layers/__init__.py,0,"b'""""""Higher-level layer abstractions built on TF Encrypted.""""""\nfrom __future__ import absolute_import\n\nfrom tf_encrypted.keras.engine.input_layer import Input\nfrom tf_encrypted.keras.layers.activation import Activation\nfrom tf_encrypted.keras.layers.convolutional import Conv2D\nfrom tf_encrypted.keras.layers.convolutional import DepthwiseConv2D\nfrom tf_encrypted.keras.layers.core import Reshape\nfrom tf_encrypted.keras.layers.dense import Dense\nfrom tf_encrypted.keras.layers.flatten import Flatten\nfrom tf_encrypted.keras.layers.normalization import BatchNormalization\nfrom tf_encrypted.keras.layers.pooling import AveragePooling2D\nfrom tf_encrypted.keras.layers.pooling import GlobalAveragePooling2D\nfrom tf_encrypted.keras.layers.pooling import GlobalMaxPooling2D\nfrom tf_encrypted.keras.layers.pooling import MaxPooling2D\nfrom tf_encrypted.keras.layers.relu import ReLU\n\n__all__ = [\n    ""Input"",\n    ""Activation"",\n    ""Conv2D"",\n    ""Dense"",\n    ""Flatten"",\n    ""AveragePooling2D"",\n    ""MaxPooling2D"",\n    ""ReLU"",\n    ""BatchNormalization"",\n    ""Reshape"",\n    ""DepthwiseConv2D"",\n    ""GlobalAveragePooling2D"",\n    ""GlobalMaxPooling2D"",\n]\n'"
tf_encrypted/keras/layers/activation.py,0,"b'""""""Activation Layer implementation.""""""\nfrom tf_encrypted.keras import activations\nfrom tf_encrypted.keras.engine import Layer\n\n\nclass Activation(Layer):\n    """"""Applies an activation function to an output.\n  Arguments:\n      activation: name of activation function to use or\n          TF Encrypted operation.\n  Input shape:\n      Arbitrary. Use the keyword argument `input_shape`\n      (tuple of integers, does not include the samples axis)\n      when using this layer as the first layer in a model.\n  Output shape:\n      Same shape as input.\n  """"""\n\n    def __init__(self, activation, **kwargs):\n        super(Activation, self).__init__(**kwargs)\n        self.activation_identifier = activation\n        self.activation = activations.get(self.activation_identifier)\n\n    def build(self, input_shape):\n        pass\n\n    def call(self, inputs):\n        y = self.activation(inputs)\n        self._layer_output = y\n        return y\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def backward(self, d_y):\n        self._activation_deriv = activations.get_deriv(self.activation_identifier)\n        y = self._layer_output\n        grad_weights = []\n        d_x = self._activation_deriv(y, d_y)\n        return grad_weights, d_x\n'"
tf_encrypted/keras/layers/activation_test.py,9,"b'# pylint: disable=missing-docstring\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.keras.testing_utils import agreement_test\nfrom tf_encrypted.keras.testing_utils import layer_test\n\n\nclass TestActivation(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_activation_relu(self):\n        self._core_activation(activation=""relu"")\n\n    def _core_activation(self, **layer_kwargs):\n        agreement_test(\n            tfe.keras.layers.Activation, kwargs=layer_kwargs, input_shape=[1, 5],\n        )\n        layer_test(\n            tfe.keras.layers.Activation, kwargs=layer_kwargs, batch_input_shape=[1, 5],\n        )\n\n    def test_backward_sigmoid(self) -> None:\n\n        input_shape = [1, 4]\n        input_data = np.array([-1, -0.75, 0.75, 1]).reshape(input_shape)\n        weights_second_layer = np.ones(shape=[1])\n\n        with tfe.protocol.SecureNN() as prot:\n\n            private_input = prot.define_private_variable(input_data)\n            w = prot.define_private_variable(weights_second_layer)\n\n            tfe_layer = tfe.keras.layers.Activation(""sigmoid"", input_shape=[4])\n\n            dense_out_pond = tfe_layer(private_input)\n\n            loss = dense_out_pond * w\n\n            # backward\n            d_out = w\n            _, d_x = tfe_layer.backward(d_out)\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                tfe_loss = sess.run(loss.reveal())\n                tfe_d_x = sess.run(d_x.reveal())\n\n        # reset graph\n        tf.reset_default_graph()\n\n        with tf.Session() as sess:\n\n            tf_layer = tf.keras.layers.Activation(""sigmoid"", input_shape=[4])\n\n            x = tf.Variable(input_data, dtype=tf.float32)\n            y = tf_layer(x)\n            w = tf.Variable(weights_second_layer, dtype=tf.float32)\n            loss = y * w\n\n            # backward\n            d_x = tf.gradients(xs=x, ys=loss)\n\n            sess.run(tf.global_variables_initializer())\n            tf_loss, tf_d_x = sess.run([loss, d_x])\n\n            np.testing.assert_array_almost_equal(tfe_loss, tf_loss, decimal=1)\n            np.testing.assert_array_almost_equal(tfe_d_x, tf_d_x[0], decimal=2)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/keras/layers/convolutional.py,6,"b'""""""Convolutional Layer implementation.""""""\nimport logging\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.keras import initializers\nfrom tensorflow.python.keras.utils import conv_utils\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.keras import activations\nfrom tf_encrypted.keras import backend as KE\nfrom tf_encrypted.keras.engine import Layer\nfrom tf_encrypted.keras.layers.layers_utils import default_args_check\nfrom tf_encrypted.protocol.pond import PondPrivateTensor\n\nlogger = logging.getLogger(""tf_encrypted"")\n\n\nclass Conv2D(Layer):\n    """"""2D convolution layer (e.g. spatial convolution over images).\n  This layer creates a convolution kernel that is convolved\n  with the layer input to produce a tensor of\n  outputs. If `use_bias` is True,\n  a bias vector is created and added to the outputs. Finally, if\n  `activation` is not `None`, it is applied to the outputs as well.\n  When using this layer as the first layer in a model,\n  provide the keyword argument `input_shape`\n  (tuple of integers, does not include the sample axis),\n  e.g. `input_shape=(128, 128, 3)` for 128x128 RGB pictures\n  in `data_format=""channels_last""`.\n  Arguments:\n      filters: Integer, the dimensionality of the output space\n          (i.e. the number of output filters in the convolution).\n      kernel_size: An integer or tuple/list of 2 integers, specifying the\n          height and width of the 2D convolution window.\n          Can be a single integer to specify the same value for\n          all spatial dimensions.\n      strides: An integer or tuple/list of 2 integers,\n          specifying the strides of the convolution along the height and width.\n          Can be a single integer to specify the same value for\n          all spatial dimensions.\n          Specifying any stride value != 1 is incompatible with specifying\n          any `dilation_rate` value != 1.\n      padding: one of `""valid""` or `""same""` (case-insensitive).\n      data_format: A string,\n          one of `channels_last` (default) or `channels_first`.\n          The ordering of the dimensions in the inputs.\n          `channels_last` corresponds to inputs with shape\n          `(batch, height, width, channels)` while `channels_first`\n          corresponds to inputs with shape\n          `(batch, channels, height, width)`.\n          It defaults to the `image_data_format` value found in your\n          Keras config file at `~/.keras/keras.json`.\n          If you never set it, then it will be ""channels_last"".\n      dilation_rate: an integer or tuple/list of 2 integers, specifying\n          the dilation rate to use for dilated convolution.\n          Can be a single integer to specify the same value for\n          all spatial dimensions.\n          Currently, specifying any `dilation_rate` value != 1 is\n          incompatible with specifying any stride value != 1.\n      activation: Activation function to use.\n          If you don\'t specify anything, no activation is applied\n          (ie. ""linear"" activation: `a(x) = x`).\n      use_bias: Boolean, whether the layer uses a bias vector.\n      kernel_initializer: Initializer for the `kernel` weights matrix.\n      bias_initializer: Initializer for the bias vector.\n      kernel_regularizer: Regularizer function applied to\n          the `kernel` weights matrix.\n      bias_regularizer: Regularizer function applied to the bias vector.\n      activity_regularizer: Regularizer function applied to\n          the output of the layer (its ""activation"")..\n      kernel_constraint: Constraint function applied to the kernel matrix.\n      bias_constraint: Constraint function applied to the bias vector.\n  Input shape:\n      4D tensor with shape:\n      `(samples, channels, rows, cols)` if data_format=\'channels_first\'\n      or 4D tensor with shape:\n      `(samples, rows, cols, channels)` if data_format=\'channels_last\'.\n  Output shape:\n      4D tensor with shape:\n      `(samples, filters, new_rows, new_cols)` if data_format=\'channels_first\'\n      or 4D tensor with shape:\n      `(samples, new_rows, new_cols, filters)` if data_format=\'channels_last\'.\n      `rows` and `cols` values might have changed due to padding.\n  """"""\n\n    def __init__(\n        self,\n        filters,\n        kernel_size,\n        strides=(1, 1),\n        padding=""valid"",\n        data_format=None,\n        dilation_rate=(1, 1),\n        activation=None,\n        use_bias=True,\n        kernel_initializer=""glorot_uniform"",\n        bias_initializer=""zeros"",\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None,\n        kernel_constraint=None,\n        bias_constraint=None,\n        **kwargs,\n    ):\n\n        super(Conv2D, self).__init__(**kwargs)\n\n        self.rank = 2\n        self.filters = filters\n        self.kernel_size = conv_utils.normalize_tuple(\n            kernel_size, self.rank, ""kernel_size""\n        )\n        if self.kernel_size[0] != self.kernel_size[1]:\n            raise NotImplementedError(\n                ""TF Encrypted currently only supports same ""\n                ""stride along the height and the width.""\n                ""You gave: {}"".format(self.kernel_size)\n            )\n        self.strides = conv_utils.normalize_tuple(strides, self.rank, ""strides"")\n        self.padding = conv_utils.normalize_padding(padding).upper()\n        self.data_format = conv_utils.normalize_data_format(data_format)\n        if activation is not None:\n            logger.info(\n                ""Performing an activation before a pooling layer can result ""\n                ""in unnecessary performance loss. Check model definition in ""\n                ""case of missed optimization.""\n            )\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        # Not implemented arguments\n        default_args_check(dilation_rate, ""dilation_rate"", ""Conv2D"")\n        default_args_check(kernel_regularizer, ""kernel_regularizer"", ""Conv2D"")\n        default_args_check(bias_regularizer, ""bias_regularizer"", ""Conv2D"")\n        default_args_check(activity_regularizer, ""activity_regularizer"", ""Conv2D"")\n        default_args_check(kernel_constraint, ""kernel_constraint"", ""Conv2D"")\n        default_args_check(bias_constraint, ""bias_constraint"", ""Conv2D"")\n\n    def build(self, input_shape):\n        if self.data_format == ""channels_first"":\n            channel_axis = 1\n        else:\n            channel_axis = -1\n        if input_shape[channel_axis] is None:\n            raise ValueError(\n                ""The channel dimension of the inputs ""\n                ""should be defined. Found `None`.""\n            )\n        input_dim = int(input_shape[channel_axis])\n        self.kernel_shape = self.kernel_size + (input_dim, self.filters)\n\n        kernel = self.kernel_initializer(self.kernel_shape)\n        self.kernel = self.add_weight(kernel)\n\n        if self.use_bias:\n            # Expand bias shape dimensions. Bias needs to have\n            # a rank of 3 to be added to the output\n            bias_shape = [self.filters, 1, 1]\n            bias = self.bias_initializer(bias_shape)\n            self.bias = self.add_weight(bias)\n        else:\n            self.bias = None\n\n        self.built = True\n\n    def call(self, inputs):\n\n        if self.data_format != ""channels_first"":\n            inputs = tfe.transpose(inputs, perm=[0, 3, 1, 2])\n\n        outputs = tfe.conv2d(inputs, self.kernel, self.strides[0], self.padding)\n\n        if self.use_bias:\n            outputs = outputs + self.bias\n\n        if self.data_format != ""channels_first"":\n            outputs = tfe.transpose(outputs, perm=[0, 2, 3, 1])\n\n        if self.activation is not None:\n            return self.activation(outputs)\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        """"""Compute output_shape for the layer.""""""\n        h_filter, w_filter, _, n_filters = self.kernel_shape\n\n        if self.data_format == ""channels_first"":\n            n_x, _, h_x, w_x = input_shape.as_list()\n        else:\n            n_x, h_x, w_x, _ = input_shape.as_list()\n\n        if self.padding == ""SAME"":\n            h_out = int(np.ceil(float(h_x) / float(self.strides[0])))\n            w_out = int(np.ceil(float(w_x) / float(self.strides[0])))\n        if self.padding == ""VALID"":\n            h_out = int(np.ceil(float(h_x - h_filter + 1) / float(self.strides[0])))\n            w_out = int(np.ceil(float(w_x - w_filter + 1) / float(self.strides[0])))\n\n        return [n_x, n_filters, h_out, w_out]\n\n\nclass DepthwiseConv2D(Conv2D):\n    """"""Depthwise separable 2D convolution.\n\n  Depthwise Separable convolutions consists in performing\n  just the first step in a depthwise spatial convolution\n  (which acts on each input channel separately).\n  The `depth_multiplier` argument controls how many\n  output channels are generated per input channel in the depthwise step.\n\n  Arguments:\n    kernel_size: An integer or tuple/list of 2 integers, specifying the\n        height and width of the 2D convolution window.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n    strides: An integer or tuple/list of 2 integers,\n        specifying the strides of the convolution along the height and width.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Specifying any stride value != 1 is incompatible with specifying\n        any `dilation_rate` value != 1.\n    padding: one of `\'valid\'` or `\'same\'` (case-insensitive).\n    depth_multiplier: The number of depthwise convolution output channels\n        for each input channel.\n        The total number of depthwise convolution output\n        channels will be equal to `filters_in * depth_multiplier`.\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `channels_first`\n        corresponds to inputs with shape\n        `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \'channels_last\'.\n    activation: Activation function to use.\n        If you don\'t specify anything, no activation is applied\n        (ie. \'linear\' activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    depthwise_initializer: Initializer for the depthwise kernel matrix.\n    bias_initializer: Initializer for the bias vector.\n    depthwise_regularizer: Regularizer function applied to\n        the depthwise kernel matrix.\n    bias_regularizer: Regularizer function applied to the bias vector.\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \'activation\').\n    depthwise_constraint: Constraint function applied to\n        the depthwise kernel matrix.\n    bias_constraint: Constraint function applied to the bias vector.\n\n  Input shape:\n    4D tensor with shape:\n    `[batch, channels, rows, cols]` if data_format=\'channels_first\'\n    or 4D tensor with shape:\n    `[batch, rows, cols, channels]` if data_format=\'channels_last\'.\n\n  Output shape:\n    4D tensor with shape:\n    `[batch, filters, new_rows, new_cols]` if data_format=\'channels_first\'\n    or 4D tensor with shape:\n    `[batch, new_rows, new_cols, filters]` if data_format=\'channels_last\'.\n    `rows` and `cols` values might have changed due to padding.\n  """"""\n\n    def __init__(\n        self,\n        kernel_size,\n        strides=(1, 1),\n        padding=""valid"",\n        depth_multiplier=1,\n        data_format=None,\n        activation=None,\n        use_bias=True,\n        depthwise_initializer=""glorot_uniform"",\n        bias_initializer=""zeros"",\n        depthwise_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None,\n        depthwise_constraint=None,\n        bias_constraint=None,\n        **kwargs,\n    ):\n\n        super(DepthwiseConv2D, self).__init__(\n            filters=None,\n            kernel_size=kernel_size,\n            strides=strides,\n            padding=padding,\n            data_format=data_format,\n            activation=activation,\n            use_bias=use_bias,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            bias_constraint=bias_constraint,\n            **kwargs,\n        )\n\n        self.rank = 2\n        self.kernel_size = conv_utils.normalize_tuple(\n            kernel_size, self.rank, ""kernel_size""\n        )\n        if self.kernel_size[0] != self.kernel_size[1]:\n            raise NotImplementedError(\n                ""TF Encrypted currently only supports same ""\n                ""stride along the height and the width.""\n                ""You gave: {}"".format(self.kernel_size)\n            )\n        self.strides = conv_utils.normalize_tuple(strides, self.rank, ""strides"")\n        self.padding = conv_utils.normalize_padding(padding).upper()\n        self.depth_multiplier = depth_multiplier\n        self.data_format = conv_utils.normalize_data_format(data_format)\n        if activation is not None:\n            logger.info(\n                ""Performing an activation before a pooling layer can result ""\n                ""in unnecessary performance loss. Check model definition in ""\n                ""case of missed optimization.""\n            )\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n        self.depthwise_initializer = initializers.get(depthwise_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        # Not implemented arguments\n        default_args_check(\n            depthwise_regularizer, ""depthwise_regularizer"", ""DepthwiseConv2D"",\n        )\n        default_args_check(\n            bias_regularizer, ""bias_regularizer"", ""DepthwiseConv2D"",\n        )\n        default_args_check(\n            activity_regularizer, ""activity_regularizer"", ""DepthwiseConv2D"",\n        )\n        default_args_check(\n            depthwise_constraint, ""depthwise_constraint"", ""DepthwiseConv2D"",\n        )\n        default_args_check(\n            bias_constraint, ""bias_constraint"", ""DepthwiseConv2D"",\n        )\n\n    def build(self, input_shape):\n        if self.data_format == ""channels_first"":\n            channel_axis = 1\n        else:\n            channel_axis = -1\n        if input_shape[channel_axis] is None:\n            raise ValueError(\n                ""The channel dimension of the inputs ""\n                ""should be defined. Found `None`.""\n            )\n        self.input_dim = int(input_shape[channel_axis])\n        self.kernel_shape = self.kernel_size + (self.input_dim, self.depth_multiplier)\n\n        kernel = self.depthwise_initializer(self.kernel_shape)\n        kernel = self.rearrange_kernel(kernel)\n        self.kernel = self.add_weight(kernel)\n\n        if self.use_bias:\n            # Expand bias shape dimensions. Bias needs to have\n            # a rank of 3 to be added to the output\n            bias_shape = [self.input_dim * self.depth_multiplier, 1, 1]\n            bias = self.bias_initializer(bias_shape)\n            self.bias = self.add_weight(bias)\n        else:\n            self.bias = None\n\n        self.built = True\n\n    def rearrange_kernel(self, kernel):\n        """"""Rearrange kernel to match normal convolution kernels\n\n    Arguments:\n      kernel: kernel to be rearranged\n    """"""\n        mask = self.get_mask(self.input_dim)\n\n        if isinstance(kernel, tf.Tensor):\n            mask = tf.constant(\n                mask.tolist(),\n                dtype=tf.float32,\n                shape=(\n                    self.kernel_size[0],\n                    self.kernel_size[1],\n                    self.input_dim * self.depth_multiplier,\n                    self.input_dim,\n                ),\n            )\n\n            if self.depth_multiplier > 1:\n                # rearrange kernel\n                kernel = tf.transpose(kernel, [0, 1, 3, 2])\n                kernel = tf.reshape(\n                    kernel,\n                    shape=self.kernel_size\n                    + (self.input_dim * self.depth_multiplier, 1),\n                )\n\n            kernel = tf.multiply(kernel, mask)\n\n        elif isinstance(kernel, np.ndarray):\n            if self.depth_multiplier > 1:\n                # rearrange kernel\n                kernel = np.transpose(kernel, [0, 1, 3, 2])\n                kernel = np.reshape(\n                    kernel,\n                    newshape=self.kernel_size\n                    + (self.input_dim * self.depth_multiplier, 1),\n                )\n\n            kernel = np.multiply(kernel, mask)\n\n        elif isinstance(kernel, PondPrivateTensor):\n            mask = tfe.define_public_variable(mask)\n            if self.depth_multiplier > 1:\n                # rearrange kernel\n                kernel = tfe.transpose(kernel, [0, 1, 3, 2])\n                kernel = tfe.reshape(\n                    kernel,\n                    shape=self.kernel_size\n                    + (self.input_dim * self.depth_multiplier, 1),\n                )\n\n            kernel = tfe.mul(kernel, mask)\n\n        return kernel\n\n    def call(self, inputs):\n\n        if self.data_format != ""channels_first"":\n            inputs = tfe.transpose(inputs, perm=[0, 3, 1, 2])\n\n        outputs = tfe.conv2d(inputs, self.kernel, self.strides[0], self.padding)\n\n        if self.use_bias:\n            outputs = outputs + self.bias\n\n        if self.data_format != ""channels_first"":\n            outputs = tfe.transpose(outputs, perm=[0, 2, 3, 1])\n\n        if self.activation is not None:\n            return self.activation(outputs)\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        """"""Compute output_shape for the layer.""""""\n        h_filter, w_filter, _, n_filters = self.kernel_shape\n\n        if self.data_format == ""channels_first"":\n            n_x, _, h_x, w_x = input_shape.as_list()\n        else:\n            n_x, h_x, w_x, _ = input_shape.as_list()\n\n        if self.padding == ""SAME"":\n            h_out = int(np.ceil(float(h_x) / float(self.strides[0])))\n            w_out = int(np.ceil(float(w_x) / float(self.strides[0])))\n        if self.padding == ""VALID"":\n            h_out = int(np.ceil(float(h_x - h_filter + 1) / float(self.strides[0])))\n            w_out = int(np.ceil(float(w_x - w_filter + 1) / float(self.strides[0])))\n\n        return [n_x, n_filters, h_out, w_out]\n\n    def get_mask(self, in_channels):\n        """"""TODO""""""\n        mask = np.zeros(\n            (\n                self.kernel_size[0],\n                self.kernel_size[1],\n                in_channels,\n                in_channels * self.depth_multiplier,\n            )\n        )\n        for d in range(self.depth_multiplier):\n            for i in range(in_channels):\n                mask[:, :, i, i + (d * in_channels)] = 1.0\n        return np.transpose(mask, [0, 1, 3, 2])\n\n    def set_weights(self, weights, sess=None):\n        """"""\n    Sets the weights of the layer.\n\n    Arguments:\n      weights: A list of Numpy arrays with shapes and types\n          matching the output of layer.get_weights() or a list\n          of private variables\n      sess: tfe session""""""\n\n        weights_types = (np.ndarray, PondPrivateTensor)\n        assert isinstance(weights[0], weights_types), type(weights[0])\n\n        # Assign new keras weights to existing weights defined by\n        # default when tfe layer was instantiated\n        if not sess:\n            sess = KE.get_session()\n\n        if isinstance(weights[0], np.ndarray):\n            for i, w in enumerate(self.weights):\n                shape = w.shape.as_list()\n                tfe_weights_pl = tfe.define_private_placeholder(shape)\n\n                new_weight = weights[i]\n                if i == 0:\n                    # kernel\n                    new_weight = self.rearrange_kernel(new_weight)\n                else:\n                    # bias\n                    new_weight = new_weight.reshape(shape)\n\n                fd = tfe_weights_pl.feed(new_weight)\n                sess.run(tfe.assign(w, tfe_weights_pl), feed_dict=fd)\n\n        elif isinstance(weights[0], PondPrivateTensor):\n            for i, w in enumerate(self.weights):\n                shape = w.shape.as_list()\n\n                new_weight = weights[i]\n                if i == 0:\n                    # kernel\n                    new_weight = self.rearrange_kernel(new_weight)\n                else:\n                    # bias\n                    new_weight = new_weight.reshape(shape)\n\n                sess.run(tfe.assign(w, new_weight))\n'"
tf_encrypted/keras/layers/convolutional_test.py,8,"b'# pylint: disable=missing-docstring\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.keras import backend as KE\nfrom tf_encrypted.keras.testing_utils import agreement_test\nfrom tf_encrypted.keras.testing_utils import layer_test\n\nnp.random.seed(42)\n\n\nclass TestConv2d(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_conv2d_bias(self):\n        self._core_conv2d(kernel_size=2, use_bias=True)\n\n    def test_conv2d_nobias(self):\n        self._core_conv2d(kernel_size=2, use_bias=False)\n\n    def test_conv2d_same_padding(self):\n        self._core_conv2d(kernel_size=2, padding=""same"")\n\n    def test_conv2d_kernelsize_tuple(self):\n        self._core_conv2d(kernel_size=(2, 2))\n\n    def _core_conv2d(self, **layer_kwargs):\n        filters_in = 3\n        input_shape = [2, 6, 6, filters_in]  # channels last\n        filters = 5\n\n        if isinstance(layer_kwargs[""kernel_size""], int):\n            kernel_size_in = (layer_kwargs[""kernel_size""],) * 2\n        else:\n            kernel_size_in = layer_kwargs[""kernel_size""]\n\n        kernel = np.random.normal(kernel_size_in + (filters_in, filters))\n        initializer = tf.keras.initializers.Constant(kernel)\n\n        base_kwargs = {\n            ""filters"": filters,\n            ""strides"": 2,\n            ""kernel_initializer"": initializer,\n        }\n\n        kwargs = {**base_kwargs, **layer_kwargs}\n        agreement_test(tfe.keras.layers.Conv2D, kwargs=kwargs, input_shape=input_shape)\n        layer_test(\n            tfe.keras.layers.Conv2D, kwargs=kwargs, batch_input_shape=input_shape\n        )\n\n\nclass TestDepthwiseConv2d(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_depthwise_conv2d_bias(self):\n        self._core_depthwise_conv2d(kernel_size=2, use_bias=True)\n\n    def test_depthwise_conv2d_nobias(self):\n        self._core_depthwise_conv2d(kernel_size=2, use_bias=False)\n\n    def test_depthwise_conv2d_same_padding(self):\n        self._core_depthwise_conv2d(kernel_size=2, padding=""same"")\n\n    def test_depthwise_conv2d_kernelsize_tuple(self):\n        self._core_depthwise_conv2d(kernel_size=(2, 2))\n\n    def test_depthwise_conv2d_depth_multiplier(self):\n        self._core_depthwise_conv2d(kernel_size=2, depth_multiplier=2)\n\n    def test_depthwise_conv2d_set_weights(self):\n        input_shape = (1, 10, 10, 3)\n        input_data = np.random.normal(size=input_shape)\n\n        with tf.Session():\n            model = tf.keras.models.Sequential()\n\n            model.add(\n                tf.keras.layers.DepthwiseConv2D(\n                    kernel_size=(2, 2), batch_input_shape=input_shape,\n                )\n            )\n\n            expected = model.predict(input_data)\n            k_weights = model.get_weights()\n            k_config = model.get_config()\n\n        with tfe.protocol.SecureNN():\n            x = tfe.define_private_input(\n                ""inputter"", lambda: tf.convert_to_tensor(input_data)\n            )\n\n            tfe_model = tfe.keras.models.model_from_config(k_config)\n            tfe_model.set_weights(k_weights)\n            y = tfe_model(x)\n\n        with KE.get_session() as sess:\n            actual = sess.run(y.reveal())\n\n            np.testing.assert_allclose(actual, expected, rtol=1e-2, atol=1e-2)\n\n        KE.clear_session()\n\n    def _core_depthwise_conv2d(self, **layer_kwargs):\n        filters_in = 3\n        input_shape = [2, 6, 6, filters_in]  # channels last\n\n        if isinstance(layer_kwargs[""kernel_size""], int):\n            kernel_size_in = (layer_kwargs[""kernel_size""],) * 2\n        else:\n            kernel_size_in = layer_kwargs[""kernel_size""]\n\n        filters_out = layer_kwargs.get(""depth_multiplier"", 1)\n\n        kernel = np.random.normal(kernel_size_in + (filters_in, filters_out))\n\n        initializer = tf.keras.initializers.Constant(kernel)\n\n        base_kwargs = {\n            ""strides"": 2,\n            ""depthwise_initializer"": initializer,\n        }\n\n        kwargs = {**base_kwargs, **layer_kwargs}\n        agreement_test(\n            tfe.keras.layers.DepthwiseConv2D,\n            kwargs=kwargs,\n            input_shape=input_shape,\n            atol=1e-2,\n        )\n        layer_test(\n            tfe.keras.layers.DepthwiseConv2D,\n            kwargs=kwargs,\n            batch_input_shape=input_shape,\n        )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/keras/layers/core.py,0,"b'""""""Core layers such as Reshape""""""\n\nimport numpy as np\n\nfrom tf_encrypted.keras.engine import Layer\n\n\nclass Reshape(Layer):\n    """"""Reshapes an output to a certain shape.\n  Arguments:\n    target_shape: Target shape. Tuple of integers,\n      does not include the samples dimension (batch size).\n  Input shape:\n    Arbitrary, although all dimensions in the input shaped must be fixed.\n    Use the keyword argument `input_shape`\n    (tuple of integers, does not include the samples axis)\n    when using this layer as the first layer in a model.\n  Output shape:\n    `(batch_size,) + target_shape`\n  Example:\n  ```python\n  # as first layer in a Sequential model\n  model = Sequential()\n  model.add(Reshape((3, 4), input_shape=(12,)))\n  # now: model.output_shape == (None, 3, 4)\n  # note: `None` is the batch dimension\n  # as intermediate layer in a Sequential model\n  model.add(Reshape((6, 2)))\n  # now: model.output_shape == (None, 6, 2)\n  # also supports shape inference using `-1` as dimension\n  model.add(Reshape((-1, 2, 2)))\n  # now: model.output_shape == (None, 3, 2, 2)\n  ```\n  """"""\n\n    def __init__(self, target_shape, **kwargs):\n        super(Reshape, self).__init__(**kwargs)\n        self.target_shape = tuple(target_shape)\n\n    def _fix_unknown_dimension(self, input_shape, output_shape):\n        """"""Find and replace a missing dimension in an output shape.\n    This is a near direct port of the internal Numpy function\n    `_fix_unknown_dimension` in `numpy/core/src/multiarray/shape.c`\n    Arguments:\n      input_shape: Shape of array being reshaped\n      output_shape: Desired shape of the array with at most\n        a single -1 which indicates a dimension that should be\n        derived from the input shape.\n    Returns:\n      The new output shape with a -1 replaced with its computed value.\n    Raises:\n      ValueError: If the total array size of the output_shape is\n      different than the input_shape, or more than one unknown dimension\n      is specified.\n    """"""\n        output_shape = list(output_shape)\n        msg = ""total size of new array must be unchanged""\n\n        known, unknown = 1, None\n        for index, dim in enumerate(output_shape):\n            if dim < 0:\n                if unknown is None:\n                    unknown = index\n                else:\n                    raise ValueError(""Can only specify one unknown dimension."")\n            else:\n                known *= dim\n\n        original = np.prod(input_shape, dtype=int)\n        if unknown is not None:\n            if known == 0 or original % known != 0:\n                raise ValueError(msg)\n            output_shape[unknown] = original // known\n        elif original != known:\n            raise ValueError(msg)\n        return output_shape\n\n    def compute_output_shape(self, input_shape):\n        if None in input_shape[1:]:\n            output_shape = [input_shape[0]]\n            # input shape (partially) unknown? replace -1\'s with None\'s\n            output_shape += tuple(s if s != -1 else None for s in self.target_shape)\n        else:\n            output_shape = [input_shape[0]]\n            output_shape += self._fix_unknown_dimension(\n                input_shape[1:], self.target_shape\n            )\n        return output_shape\n\n    def call(self, inputs):\n        return inputs.reshape((int(inputs.shape[0]),) + self.target_shape)\n'"
tf_encrypted/keras/layers/core_test.py,1,"b'# pylint: disable=missing-docstring\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.keras.testing_utils import agreement_test\nfrom tf_encrypted.keras.testing_utils import layer_test\n\nnp.random.seed(42)\n\n\nclass TestReshape(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_reshape(self):\n        self._core_reshape((2, 2, 2, 2), target_shape=[8])\n\n    def test_reshape_unknown_dim(self):\n        self._core_reshape((2, 2, 2, 2), target_shape=[-1, 4])\n\n    def _core_reshape(self, input_shape, **layer_kwargs):\n\n        kwargs = {**layer_kwargs}\n        agreement_test(\n            tfe.keras.layers.Reshape, kwargs=kwargs, input_shape=input_shape,\n        )\n        layer_test(\n            tfe.keras.layers.Reshape, kwargs=kwargs, batch_input_shape=input_shape,\n        )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/keras/layers/dense.py,0,"b'# pylint: disable=arguments-differ\n""""""Dense (i.e. fully connected) Layer implementation.""""""\nfrom tensorflow.python.keras import initializers\n\nfrom tf_encrypted.keras import activations\nfrom tf_encrypted.keras.engine import Layer\nfrom tf_encrypted.keras.layers.layers_utils import default_args_check\n\n\nclass Dense(Layer):\n    """"""Just your regular densely-connected NN layer.\n  `Dense` implements the operation:\n  `output = activation(dot(input, kernel) + bias)`\n  where `activation` is the element-wise activation function\n  passed as the `activation` argument, `kernel` is a weights matrix\n  created by the layer, and `bias` is a bias vector created by the layer\n  (only applicable if `use_bias` is `True`).\n\n  Arguments:\n      units: Positive integer, dimensionality of the output space.\n      activation: Activation function to use.\n          If you don\'t specify anything, no activation is applied\n          (ie. ""linear"" activation: `a(x) = x`).\n      use_bias: Boolean, whether the layer uses a bias vector.\n      kernel_initializer: Initializer for the `kernel` weights matrix.\n      bias_initializer: Initializer for the bias vector.\n      kernel_regularizer: Regularizer function applied to\n          the `kernel` weights matrix.\n      bias_regularizer: Regularizer function applied to the bias vector.\n      activity_regularizer: Regularizer function applied to\n          the output of the layer (its ""activation"").\n      kernel_constraint: Constraint function applied to\n          the `kernel` weights matrix.\n      bias_constraint: Constraint function applied to the bias vector.\n\n  Input shape:\n      2D tensor with shape: `(batch_size, input_dim)`.\n\n  Output shape:\n      2D tensor with shape: `(batch_size, units)`.\n  """"""\n\n    def __init__(\n        self,\n        units,\n        activation=None,\n        use_bias=True,\n        kernel_initializer=""glorot_uniform"",\n        bias_initializer=""zeros"",\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None,\n        kernel_constraint=None,\n        bias_constraint=None,\n        **kwargs,\n    ):\n\n        super(Dense, self).__init__(**kwargs)\n\n        self.units = int(units)\n        self.activation_identifier = activation\n        self.activation = activations.get(self.activation_identifier)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        # Not implemented arguments\n        default_args_check(kernel_regularizer, ""kernel_regularizer"", ""Dense"")\n        default_args_check(bias_regularizer, ""bias_regularizer"", ""Dense"")\n        default_args_check(activity_regularizer, ""activity_regularizer"", ""Dense"")\n        default_args_check(kernel_constraint, ""kernel_constraint"", ""Dense"")\n        default_args_check(bias_constraint, ""bias_constraint"", ""Dense"")\n\n    def compute_output_shape(self, input_shape):\n        return [input_shape[0], self.units]\n\n    def build(self, input_shape):\n\n        rank = len(input_shape)\n\n        if rank > 2:\n            raise NotImplementedError(\n                ""For dense layer, TF Encrypted currently support only input with ""\n                ""a rank equal to 2 instead of {}."".format(len(input_shape))\n            )\n\n        units_in = int(input_shape[1])\n        kernel = self.kernel_initializer([units_in, self.units])\n        self.kernel = self.add_weight(kernel)\n\n        if self.use_bias:\n            bias = self.bias_initializer([self.units])\n            self.bias = self.add_weight(bias)\n        else:\n            self.bias = None\n\n        self.built = True\n\n    def call(self, inputs):\n\n        self._layer_input = inputs\n\n        if self.use_bias:\n            outputs = inputs.matmul(self.kernel) + self.bias\n        else:\n            outputs = inputs.matmul(self.kernel)\n\n        if self.activation_identifier is not None:\n            outputs = self.activation(outputs)\n\n        self._layer_output = outputs\n\n        return outputs\n\n    def backward(self, d_y):\n        """"""dense backward""""""\n        x = self._layer_input\n        y = self._layer_output\n        kernel = self.weights[0]\n        grad_weights = []\n\n        if self.activation_identifier is not None:\n            self._activation_deriv = activations.get_deriv(self.activation_identifier)\n            d_y = self._activation_deriv(y, d_y)\n\n        d_x = d_y.matmul(kernel.transpose())\n        d_weights = x.transpose().matmul(d_y)\n        grad_weights.append(d_weights)\n\n        if self.use_bias:\n            d_bias = d_y.reduce_sum(axis=0)\n            grad_weights.append(d_bias)\n\n        return grad_weights, d_x\n'"
tf_encrypted/keras/layers/dense_test.py,12,"b'# pylint: disable=missing-docstring\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.keras.testing_utils import agreement_test\nfrom tf_encrypted.keras.testing_utils import layer_test\n\nnp.random.seed(42)\n\n\nclass TestDense(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_dense_bias(self):\n        self._core_dense(use_bias=True)\n\n    def test_dense_nobias(self):\n        self._core_dense(use_bias=False)\n\n    def test_dense_relu(self):\n        self._core_dense(activation=""relu"")\n\n    def _core_dense(self, **layer_kwargs):\n        input_shape = [4, 5]\n        kernel = np.random.normal(input_shape[::-1])\n        initializer = tf.keras.initializers.Constant(kernel)\n\n        base_kwargs = {\n            ""units"": 4,\n            ""kernel_initializer"": initializer,\n        }\n        kwargs = {**base_kwargs, **layer_kwargs}\n        agreement_test(\n            tfe.keras.layers.Dense, kwargs=kwargs, input_shape=input_shape,\n        )\n        layer_test(\n            tfe.keras.layers.Dense, kwargs=kwargs, batch_input_shape=input_shape,\n        )\n\n    def test_backward(self):\n        input_shape = [1, 5]\n        input_data = np.ones(input_shape)\n        weights_second_layer = np.ones(shape=[1, 5])\n        kernel = np.ones([5, 5])\n        initializer = tf.keras.initializers.Constant(kernel)\n\n        with tfe.protocol.SecureNN() as prot:\n\n            private_input = prot.define_private_variable(input_data)\n            w = prot.define_private_variable(weights_second_layer)\n\n            tfe_layer = tfe.keras.layers.Dense(\n                5, input_shape=input_shape[1:], kernel_initializer=initializer,\n            )\n\n            dense_out_pond = tfe_layer(private_input)\n\n            loss = dense_out_pond * w\n\n            # backward\n            d_out = w\n            grad, d_x = tfe_layer.backward(d_out)\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                tfe_loss = sess.run(loss.reveal())\n                tfe_d_k = sess.run(grad[0].reveal())\n                tfe_d_b = sess.run(grad[1].reveal())\n                tfe_d_x = sess.run(d_x.reveal())\n\n        # reset graph\n        tf.reset_default_graph()\n\n        with tf.Session() as sess:\n\n            initializer = tf.keras.initializers.Constant(kernel)\n\n            tf_layer = tf.keras.layers.Dense(\n                5, input_shape=input_shape[1:], kernel_initializer=initializer,\n            )\n            x = tf.Variable(input_data, dtype=tf.float32)\n            y = tf_layer(x)\n\n            w = tf.Variable(weights_second_layer, dtype=tf.float32)\n            loss = y * w\n            k, b = tf_layer.trainable_weights\n\n            # backward\n            d_x, d_k, d_b = tf.gradients(xs=[x, k, b], ys=loss)\n\n            sess.run(tf.global_variables_initializer())\n            tf_loss, tf_d_x, tf_d_k, tf_d_b = sess.run([loss, d_x, d_k, d_b])\n\n            np.testing.assert_array_almost_equal(tfe_loss, tf_loss, decimal=2)\n            np.testing.assert_array_almost_equal(tfe_d_k, tf_d_k, decimal=2)\n            np.testing.assert_array_almost_equal(tfe_d_b, tf_d_b, decimal=2)\n            np.testing.assert_array_almost_equal(tfe_d_x, tf_d_x, decimal=2)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/keras/layers/flatten.py,0,"b'""""""Flatten Layer implementation.""""""\n\nimport numpy as np\nfrom tensorflow.python.keras.utils import conv_utils\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.keras.engine import Layer\n\n\nclass Flatten(Layer):\n    """"""Flattens the input. Does not affect the batch size.\n  If inputs are shaped `(batch,)` without a channel dimension, then flattening\n  adds an extra channel dimension and output shapes are `(batch, 1)`.\n  Arguments:\n      data_format: A string,\n          one of `channels_last` (default) or `channels_first`.\n          The ordering of the dimensions in the inputs.\n          `channels_last` corresponds to inputs with shape\n          `(batch, ..., channels)` while `channels_first` corresponds to\n          inputs with shape `(batch, channels, ...)`.\n          If you never set it, then it will be ""channels_last"".\n  """"""\n\n    def __init__(self, data_format=None, **kwargs):\n        super(Flatten, self).__init__(**kwargs)\n        self.data_format = conv_utils.normalize_data_format(data_format)\n\n    def build(self, input_shape):\n        self.built = True\n\n    def call(self, inputs):\n        input_shape = inputs.shape.as_list()\n        rank = len(input_shape)\n\n        if self.data_format == ""channels_first"" and rank > 1:\n            permutation = [0]\n            permutation.extend(i for i in range(2, rank))\n            permutation.append(1)\n            inputs = tfe.transpose(inputs, perm=permutation)\n\n        if rank == 1:\n            flatten_shape = [input_shape[0], 1]\n        else:\n            flatten_shape = [input_shape[0], -1]\n\n        outputs = tfe.reshape(inputs, flatten_shape)\n\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        if not input_shape:\n            raise ValueError(""input_shape shouldn\'t be empty or None"")\n        output_shape = [input_shape[0]]\n        if all(input_shape[1:]):\n            output_shape += [np.prod(input_shape[1:])]\n        else:\n            output_shape += [None]\n        return output_shape\n'"
tf_encrypted/keras/layers/flatten_test.py,1,"b'# pylint: disable=missing-docstring\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.keras.testing_utils import agreement_test\n\nnp.random.seed(42)\n\n\nclass TestFlatten(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_flatten_rank_four(self):\n        self._core_flatten(input_shape=[4, 5, 2, 2])\n\n    def test_flatten_rank_one(self):\n        self._core_flatten(input_shape=[4])\n\n    def test_flatten_channels_first(self):\n        self._core_flatten(input_shape=[4, 5, 2, 2], data_format=""channels_first"")\n\n    def _core_flatten(self, **layer_kwargs):\n        input_shape = layer_kwargs[""input_shape""]\n\n        agreement_test(\n            tfe.keras.layers.Flatten,\n            kwargs=layer_kwargs,\n            input_shape=input_shape,\n            atol=0.1,\n        )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/keras/layers/layers_utils.py,1,"b'""""""TF Encrypted Keras layers utils""""""\nimport inspect\n\nimport tensorflow as tf\n\n\nclass UnknownLayerArgError(ValueError):\n    """"""Raise error for unknown layer arguments.\n\n    Args:\n      arg_name: TF Encrypted Keras layer argument name (string)\n      layer_sign: TensorFlow Keras layer signature (dict)\n      tf_layer_name: TensorFlow Keras layer name (string)\n  """"""\n\n    def __init__(self, arg_name, layer_sign, layer_name):\n        super(UnknownLayerArgError, self).__init__()\n        self.arg_name = arg_name\n        self.layer_sign = layer_sign\n        self.layer_name = layer_name\n\n    def __str__(self):\n        msg = (\n            ""Argument \'{arg_name}\' is not part of the ""\n            ""signature for \'{layer_name}\' layers: {layer_sign}""\n        )\n        return msg.format(\n            arg_name=self.arg_name,\n            layer_name=self.layer_name,\n            layer_sign=self.layer_sign.keys(),\n        )\n\n\nclass LayerArgNotImplementedError(NotImplementedError):\n    """"""Raise error when layer argument is not yet supported in TFE.\n\n    Args:\n      arg: TFE layer argument\n      arg_name: TFE layer argument name (string)\n      tf_layer_name: Tensorflow keras layer name (string)\n  """"""\n\n    def __init__(self, arg_name, tf_layer_name, tf_default_arg):\n        super(LayerArgNotImplementedError, self).__init__()\n        self.arg_name = arg_name\n        self.tf_layer_name = tf_layer_name\n        self.tf_default_arg = tf_default_arg\n\n    def __str__(self):\n        arg_not_impl_msg = (\n            ""`{}` argument is not implemented for layer {}. ""\n            ""Please use the default value of {}.""\n        )\n        return arg_not_impl_msg.format(\n            self.arg_name, self.tf_layer_name, self.tf_default_arg\n        )\n\n\ndef default_args_check(arg, arg_name, tf_layer_name):\n    """"""Check if the layer is using the dfault argument\n\n    Args:\n      arg: TFE layer argument\n      arg_name: TFE layer argument name (string)\n      tf_layer_name: Tensorflow keras layer name (string)\n\n    Raises:\n      NotImplementedError: if this argument is not implemented for this `layer`.\n  """"""\n    tf_layer_cls = getattr(tf.keras.layers, tf_layer_name)\n    layer_sign = inspect.signature(tf_layer_cls.__init__).parameters\n    if arg_name not in layer_sign:\n        raise UnknownLayerArgError(arg_name, layer_sign, tf_layer_name)\n    tf_default_arg = layer_sign[arg_name].default\n    if arg != tf_default_arg:\n        raise LayerArgNotImplementedError(arg_name, tf_layer_name, tf_default_arg)\n'"
tf_encrypted/keras/layers/normalization.py,4,"b'""""""Normalization layers implementation.""""""\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.keras import initializers\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.keras import backend as KE\nfrom tf_encrypted.keras.engine import Layer\nfrom tf_encrypted.keras.layers.layers_utils import default_args_check\nfrom tf_encrypted.protocol.pond import PondPublicTensor\n\n\nclass BatchNormalization(Layer):\n    """"""Batch normalization layer (Ioffe and Szegedy, 2014).\n  Normalize the activations of the previous layer at each batch,\n  i.e. applies a transformation that maintains the mean activation\n  close to 0 and the activation standard deviation close to 1.\n  Arguments:\n    axis: Integer, the axis that should be normalized\n        (typically the features axis).\n        For instance, after a `Conv2D` layer with\n        `data_format=""channels_first""`,\n        set `axis=1` in `BatchNormalization`.\n    momentum: Momentum for the moving average.\n    epsilon: Small float added to variance to avoid dividing by zero.\n    center: If True, add offset of `beta` to normalized tensor.\n        If False, `beta` is ignored.\n    scale: If True, multiply by `gamma`.\n        If False, `gamma` is not used.\n        When the next layer is linear (also e.g. `nn.relu`),\n        this can be disabled since the scaling\n        will be done by the next layer.\n    beta_initializer: Initializer for the beta weight.\n    gamma_initializer: Initializer for the gamma weight.\n    moving_mean_initializer: Initializer for the moving mean.\n    moving_variance_initializer: Initializer for the moving variance.\n    beta_regularizer: Optional regularizer for the beta weight.\n    gamma_regularizer: Optional regularizer for the gamma weight.\n    beta_constraint: Optional constraint for the beta weight.\n    gamma_constraint: Optional constraint for the gamma weight.\n    renorm: Whether to use Batch Renormalization\n      (https://arxiv.org/abs/1702.03275). This adds extra variables during\n      training. The inference is the same for either value of this parameter.\n    renorm_clipping: A dictionary that may map keys \'rmax\', \'rmin\', \'dmax\' to\n      scalar `Tensors` used to clip the renorm correction. The correction\n      `(r, d)` is used as `corrected_value = normalized_value * r + d`, with\n      `r` clipped to [rmin, rmax], and `d` to [-dmax, dmax]. Missing rmax, rmin,\n      dmax are set to inf, 0, inf, respectively.\n    renorm_momentum: Momentum used to update the moving means and standard\n      deviations with renorm. Unlike `momentum`, this affects training\n      and should be neither too small (which would add noise) nor too large\n      (which would give stale estimates). Note that `momentum` is still applied\n      to get the means and variances for inference.\n    fused: if `True`, use a faster, fused implementation, or raise a ValueError\n      if the fused implementation cannot be used. If `None`, use the faster\n      implementation if possible. If False, do not used the fused\n      implementation.\n    trainable: Boolean, if `True` also add variables to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n    virtual_batch_size: An `int`. By default, `virtual_batch_size` is `None`,\n      which means batch normalization is performed across the whole batch. When\n      `virtual_batch_size` is not `None`, instead perform ""Ghost Batch\n      Normalization"", which creates virtual sub-batches which are each\n      normalized separately (with shared gamma, beta, and moving statistics).\n      Must divide the actual batch size during execution.\n    adjustment: A function taking the `Tensor` containing the (dynamic) shape of\n      the input tensor and returning a pair (scale, bias) to apply to the\n      normalized values (before gamma and beta), only during training. For\n      example, if axis==-1,\n        `adjustment = lambda shape: (\n          tf.random_uniform(shape[-1:], 0.93, 1.07),\n          tf.random_uniform(shape[-1:], -0.1, 0.1))`\n      will scale the normalized value by up to 7% up or down, then shift the\n      result by up to 0.1 (with independent scaling and bias for each feature\n      but shared across all examples), and finally apply gamma and/or beta. If\n      `None`, no adjustment is applied. Cannot be specified if\n      virtual_batch_size is specified.\n  Input shape:\n      Arbitrary. Use the keyword argument `input_shape`\n      (tuple of integers, does not include the samples axis)\n      when using this layer as the first layer in a model.\n  Output shape:\n      Same shape as input.\n  References:\n      - [Batch Normalization: Accelerating Deep Network Training by Reducing\n        Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n  """"""\n\n    def __init__(\n        self,\n        axis=3,\n        momentum=0.99,\n        epsilon=1e-3,\n        center=True,\n        scale=True,\n        beta_initializer=""zeros"",\n        gamma_initializer=""ones"",\n        moving_mean_initializer=""zeros"",\n        moving_variance_initializer=""ones"",\n        beta_regularizer=None,\n        gamma_regularizer=None,\n        beta_constraint=None,\n        gamma_constraint=None,\n        renorm=False,\n        renorm_clipping=None,\n        renorm_momentum=0.99,\n        fused=None,  # pylint: disable=unused-argument\n        trainable=False,\n        virtual_batch_size=None,\n        adjustment=None,\n        name=None,\n        **kwargs,\n    ):\n        super(BatchNormalization, self).__init__(\n            name=name, trainable=trainable, **kwargs\n        )\n\n        self.beta_initializer = initializers.get(beta_initializer)\n        self.gamma_initializer = initializers.get(gamma_initializer)\n        self.moving_mean_initializer = initializers.get(moving_mean_initializer)\n        self.moving_variance_initializer = initializers.get(moving_variance_initializer)\n\n        default_args_check(\n            beta_regularizer, ""beta_regularizer"", ""BatchNormalization"",\n        )\n        default_args_check(\n            gamma_regularizer, ""gamma_regularizer"", ""BatchNormalization"",\n        )\n        default_args_check(\n            beta_constraint, ""beta_constraint"", ""BatchNormalization"",\n        )\n        default_args_check(\n            gamma_constraint, ""gamma_constraint"", ""BatchNormalization"",\n        )\n        default_args_check(\n            renorm, ""renorm"", ""BatchNormalization"",\n        )\n        default_args_check(\n            renorm_clipping, ""renorm_clipping"", ""BatchNormalization"",\n        )\n        default_args_check(\n            virtual_batch_size, ""virtual_batch_size"", ""BatchNormalization"",\n        )\n        default_args_check(\n            adjustment, ""adjustment"", ""BatchNormalization"",\n        )\n\n        # Axis from get_config can be in ListWrapper format even if\n        # the layer is expecting an integer for the axis\n        if isinstance(axis, list):\n            axis = axis[0]\n\n        # Axis -3 is equivalent to 1, and axis -1 is equivalent to 3, because the\n        # input rank is required to be 4 (which is checked later).\n        if axis not in (1, 3):\n            raise ValueError(""Axis of 1 or 3 is currently only supported"")\n\n        self.axis = axis\n        self.scale = scale\n        self.center = center\n        self.epsilon = epsilon\n        self.momentum = momentum\n        self.renorm_momentum = renorm_momentum\n\n    def build(self, input_shape):\n        c = input_shape[self.axis]\n        if len(input_shape) == 2:\n            param_shape = [1, 1]\n        elif len(input_shape) == 4:\n            param_shape = [1, 1, 1, 1]\n\n        param_shape[self.axis] = int(c)\n\n        if self.scale:\n            gamma = self.gamma_initializer(param_shape)\n            self.gamma = self.add_weight(gamma, make_private=False)\n        else:\n            self.gamma = None\n\n        if self.center:\n            beta = self.beta_initializer(param_shape)\n            self.beta = self.add_weight(beta, make_private=False)\n        else:\n            self.beta = None\n\n        moving_mean = self.moving_mean_initializer(param_shape)\n        self.moving_mean = self.add_weight(moving_mean, make_private=False)\n\n        moving_variance_init = self.moving_variance_initializer(param_shape)\n        self.moving_variance = self.add_weight(moving_variance_init, make_private=False)\n\n        denomtemp = 1.0 / tf.sqrt(moving_variance_init + self.epsilon)\n\n        # We have two different public variables for moving_variance and\n        # denomtemp to avoid calling tfe.sqrt everytime denom is used\n        self.denom = tfe.define_public_variable(denomtemp)\n\n        self.built = True\n\n    def call(self, inputs):\n        if self.beta is None and self.gamma is None:\n            out = (inputs - self.moving_mean) * self.denom\n        elif self.gamma is None:\n            out = (inputs - self.moving_mean) * self.denom + self.beta\n        elif self.beta is None:\n            out = self.gamma * (inputs - self.moving_mean) * self.denom\n        else:\n            out = self.gamma * (inputs - self.moving_mean) * self.denom + self.beta\n        return out\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def set_weights(self, weights, sess=None):\n        """"""Update layer weights from numpy array or Public Tensors\n      including denom.\n\n    Arguments:\n      weights: A list of Numpy arrays with shapes and types\n          matching the output of layer.get_weights() or a list\n          of private variables\n      sess: tfe session""""""\n\n        if not sess:\n            sess = KE.get_session()\n\n        if isinstance(weights[0], np.ndarray):\n            for i, w in enumerate(self.weights):\n                if isinstance(w, PondPublicTensor):\n                    shape = w.shape.as_list()\n                    tfe_weights_pl = tfe.define_public_placeholder(shape)\n                    fd = tfe_weights_pl.feed(weights[i].reshape(shape))\n                    sess.run(tfe.assign(w, tfe_weights_pl), feed_dict=fd)\n                else:\n                    raise TypeError(\n                        (\n                            ""Don\'t know how to handle weights ""\n                            ""of type {}. Batchnorm expects public tensors""\n                            ""as weights""\n                        ).format(type(w))\n                    )\n\n        elif isinstance(weights[0], PondPublicTensor):\n            for i, w in enumerate(self.weights):\n                shape = w.shape.as_list()\n                sess.run(tfe.assign(w, weights[i].reshape(shape)))\n\n        # Compute denom on public tensors before being lifted to private tensor\n        denomtemp = tfe.reciprocal(\n            tfe.sqrt(tfe.add(self.moving_variance, self.epsilon))\n        )\n\n        # Update denom as well when moving variance gets updated\n        sess.run(tfe.assign(self.denom, denomtemp))\n'"
tf_encrypted/keras/layers/normalization_test.py,3,"b'# pylint: disable=missing-docstring\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.keras.testing_utils import agreement_test\nfrom tf_encrypted.keras.testing_utils import layer_test\n\nnp.random.seed(42)\n\n\nclass TestBatchNormalization(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_batchnorm_2d(self):\n        self._core_batchnorm([1, 4], axis=1)\n\n    def test_batchnorm_4d(self):\n        self._core_batchnorm([1, 1, 1, 3])\n\n    def test_batchnorm_channels_first(self):\n        self._core_batchnorm([1, 3, 1, 1], axis=1)\n\n    def test_batchnorm_no_scale(self):\n        self._core_batchnorm([1, 1, 1, 3], scale=False)\n\n    def test_batchnorm_no_center(self):\n        self._core_batchnorm([1, 1, 1, 3], center=False)\n\n    def test_batchnorm_non_default_mean_init(self):\n        input_shape = [1, 1, 3]\n        const = np.random.normal(input_shape)\n        initializer = tf.keras.initializers.Constant(const)\n\n        self._core_batchnorm(\n            [1] + input_shape, moving_mean_initializer=initializer,\n        )\n\n    def test_batchnorm_non_default_variance_init(self):\n        input_shape = [1, 1, 3]\n        const = np.random.uniform(input_shape)\n        initializer = tf.keras.initializers.Constant(const)\n\n        self._core_batchnorm(\n            [1] + input_shape, moving_variance_initializer=initializer,\n        )\n\n    def _core_batchnorm(self, input_shape, **layer_kwargs):\n        base_kwargs = {""fused"": False}\n\n        kwargs = {**base_kwargs, **layer_kwargs}\n\n        agreement_test(\n            tfe.keras.layers.BatchNormalization, kwargs=kwargs, input_shape=input_shape,\n        )\n        layer_test(\n            tfe.keras.layers.BatchNormalization,\n            kwargs=kwargs,\n            batch_input_shape=input_shape,\n        )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/keras/layers/pooling.py,0,"b'""""""Pooling Layer implementation.""""""\nimport math\nfrom abc import abstractmethod\n\nfrom tensorflow.python.keras.utils import conv_utils\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.keras.engine import Layer\n\n\nclass Pooling2D(Layer):\n    """"""Pooling layer for arbitrary pooling functions, for 2D inputs (e.g. images).\n  This class only exists for code reuse. It will never be an exposed API.\n  Arguments:\n    _pool_function: The pooling function to apply, e.g. `prot.max_pool2d`.\n    pool_size: An integer or tuple/list of 2 integers: (pool_height, pool_width)\n      specifying the size of the pooling window.\n      Can be a single integer to specify the same value for\n      all spatial dimensions.\n    strides: An integer or tuple/list of 2 integers,\n      specifying the strides of the pooling operation.\n      Can be a single integer to specify the same value for\n      all spatial dimensions.\n    padding: A string. The padding method, either \'valid\' or \'same\'.\n      Case-insensitive.\n    data_format: A string, one of `channels_last` (default) or `channels_first`.\n      The ordering of the dimensions in the inputs.\n      `channels_last` corresponds to inputs with shape\n      `(batch, height, width, channels)` while `channels_first` corresponds to\n      inputs with shape `(batch, channels, height, width)`.\n  """"""\n\n    def __init__(\n        self,\n        _pool_function,\n        pool_size,\n        strides,\n        padding=""valid"",\n        data_format=None,\n        **kwargs,\n    ):\n        super(Pooling2D, self).__init__(**kwargs)\n\n        if data_format is None:\n            data_format = ""channels_last""\n        if strides is None:\n            strides = pool_size\n        self._pool_function = _pool_function\n        self.pool_size = conv_utils.normalize_tuple(pool_size, 2, ""pool_size"")\n        self.strides = conv_utils.normalize_tuple(strides, 2, ""strides"")\n        self.padding = conv_utils.normalize_padding(padding).upper()\n        self.data_format = conv_utils.normalize_data_format(data_format)\n\n    def build(self, input_shape):\n        self.built = True\n\n    def call(self, inputs):\n\n        if self.data_format != ""channels_first"":\n            inputs = tfe.transpose(inputs, perm=[0, 3, 1, 2])\n\n        outputs = self._pool_function(\n            inputs, self.pool_size, self.strides, self.padding\n        )\n\n        if self.data_format != ""channels_first"":\n            outputs = tfe.transpose(outputs, perm=[0, 2, 3, 1])\n\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        if self.channels_first:\n            _, _, h_in, w_in = input_shape\n        else:\n            _, h_in, w_in, _ = input_shape\n\n        if self.padding == ""SAME"":\n            h_out = math.ceil(h_in / self.strides[0])\n            w_out = math.ceil(w_in / self.strides[1])\n        else:\n            h_out = math.ceil((h_in - self.pool_size[0] + 1) / self.strides[0])\n            w_out = math.ceil((w_in - self.pool_size[1] + 1) / self.strides[1])\n        return [input_shape[0], input_shape[1], h_out, w_out]\n\n\nclass MaxPooling2D(Pooling2D):\n    """"""Max pooling operation for spatial data.\n  Arguments:\n      pool_size: integer or tuple of 2 integers,\n          factors by which to downscale (vertical, horizontal).\n          (2, 2) will halve the input in both spatial dimension.\n          If only one integer is specified, the same window length\n          will be used for both dimensions.\n      strides: Integer, tuple of 2 integers, or None.\n          Strides values.\n          If None, it will default to `pool_size`.\n      padding: One of `""valid""` or `""same""` (case-insensitive).\n      data_format: A string,\n          one of `channels_last` (default) or `channels_first`.\n          The ordering of the dimensions in the inputs.\n          `channels_last` corresponds to inputs with shape\n          `(batch, height, width, channels)` while `channels_first`\n          corresponds to inputs with shape\n          `(batch, channels, height, width)`.\n  Input shape:\n      - If `data_format=\'channels_last\'`:\n          4D tensor with shape:\n          `(batch_size, rows, cols, channels)`\n      - If `data_format=\'channels_first\'`:\n          4D tensor with shape:\n          `(batch_size, channels, rows, cols)`\n  Output shape:\n      - If `data_format=\'channels_last\'`:\n          4D tensor with shape:\n          `(batch_size, pooled_rows, pooled_cols, channels)`\n      - If `data_format=\'channels_first\'`:\n          4D tensor with shape:\n          `(batch_size, channels, pooled_rows, pooled_cols)`\n  """"""\n\n    def __init__(\n        self,\n        pool_size=(2, 2),\n        strides=None,\n        padding=""valid"",\n        data_format=None,\n        **kwargs,\n    ):\n        super(MaxPooling2D, self).__init__(\n            tfe.maxpool2d,\n            pool_size=pool_size,\n            strides=strides,\n            padding=padding,\n            data_format=data_format,\n            **kwargs,\n        )\n\n\nclass AveragePooling2D(Pooling2D):\n    """"""Average pooling operation for spatial data.\n  Arguments:\n      pool_size: integer or tuple of 2 integers,\n          factors by which to downscale (vertical, horizontal).\n          (2, 2) will halve the input in both spatial dimension.\n          If only one integer is specified, the same window length\n          will be used for both dimensions.\n      strides: Integer, tuple of 2 integers, or None.\n          Strides values.\n          If None, it will default to `pool_size`.\n      padding: One of `""valid""` or `""same""` (case-insensitive).\n      data_format: A string,\n          one of `channels_last` (default) or `channels_first`.\n          The ordering of the dimensions in the inputs.\n          `channels_last` corresponds to inputs with shape\n          `(batch, height, width, channels)` while `channels_first`\n          corresponds to inputs with shape\n          `(batch, channels, height, width)`.\n  Input shape:\n      - If `data_format=\'channels_last\'`:\n          4D tensor with shape:\n          `(batch_size, rows, cols, channels)`\n      - If `data_format=\'channels_first\'`:\n          4D tensor with shape:\n          `(batch_size, channels, rows, cols)`\n  Output shape:\n      - If `data_format=\'channels_last\'`:\n          4D tensor with shape:\n          `(batch_size, pooled_rows, pooled_cols, channels)`\n      - If `data_format=\'channels_first\'`:\n          4D tensor with shape:\n          `(batch_size, channels, pooled_rows, pooled_cols)`\n  """"""\n\n    def __init__(\n        self,\n        pool_size=(2, 2),\n        strides=None,\n        padding=""valid"",\n        data_format=None,\n        **kwargs,\n    ):\n        super(AveragePooling2D, self).__init__(\n            tfe.avgpool2d,\n            pool_size=pool_size,\n            strides=strides,\n            padding=padding,\n            data_format=data_format,\n            **kwargs,\n        )\n\n\nclass GlobalPooling2D(Layer):\n    """"""Abstract class for different global pooling 2D layers.\n  """"""\n\n    def __init__(self, data_format=None, **kwargs):\n        super(GlobalPooling2D, self).__init__(**kwargs)\n        self.data_format = conv_utils.normalize_data_format(data_format)\n\n    def compute_output_shape(self, input_shape):\n        if self.data_format == ""channels_last"":\n            output_shape = [input_shape[0], input_shape[3]]\n        else:\n            output_shape = [input_shape[0], input_shape[1]]\n\n        return output_shape\n\n    @abstractmethod\n    def call(self, inputs):\n        raise NotImplementedError\n\n\nclass GlobalAveragePooling2D(GlobalPooling2D):\n    """"""Global average pooling operation for spatial data.\n\n  Arguments:\n      data_format: A string,\n          one of `channels_last` (default) or `channels_first`.\n          The ordering of the dimensions in the inputs.\n          `channels_last` corresponds to inputs with shape\n          `(batch, height, width, channels)` while `channels_first`\n          corresponds to inputs with shape\n          `(batch, channels, height, width)`.\n          It defaults to the `image_data_format` value found in your\n          Keras config file at `~/.keras/keras.json`.\n          If you never set it, then it will be ""channels_last"".\n\n  Input shape:\n      - If `data_format=\'channels_last\'`:\n          4D tensor with shape:\n          `(batch_size, rows, cols, channels)`\n      - If `data_format=\'channels_first\'`:\n          4D tensor with shape:\n          `(batch_size, channels, rows, cols)`\n\n  Output shape:\n      2D tensor with shape:\n      `(batch_size, channels)`\n  """"""\n\n    def build(self, input_shape):\n        if self.data_format == ""channels_last"":\n            _, h_in, w_in, _ = input_shape\n        else:\n            _, _, h_in, w_in = input_shape\n\n        self.scalar = 1 / int(h_in * w_in)\n\n    def call(self, inputs):\n        if self.data_format == ""channels_last"":\n            x_reduced = inputs.reduce_sum(axis=2).reduce_sum(axis=1)\n        else:\n            x_reduced = inputs.reduce_sum(axis=3).reduce_sum(axis=2)\n\n        return x_reduced * self.scalar\n\n\nclass GlobalMaxPooling2D(GlobalPooling2D):\n    """"""Global max pooling operation for spatial data.\n\n  Arguments:\n      data_format: A string,\n          one of `channels_last` (default) or `channels_first`.\n          The ordering of the dimensions in the inputs.\n          `channels_last` corresponds to inputs with shape\n          `(batch, height, width, channels)` while `channels_first`\n          corresponds to inputs with shape\n          `(batch, channels, height, width)`.\n          It defaults to the `image_data_format` value found in your\n          Keras config file at `~/.keras/keras.json`.\n          If you never set it, then it will be ""channels_last"".\n\n  Input shape:\n      - If `data_format=\'channels_last\'`:\n          4D tensor with shape:\n          `(batch_size, rows, cols, channels)`\n      - If `data_format=\'channels_first\'`:\n          4D tensor with shape:\n          `(batch_size, channels, rows, cols)`\n\n  Output shape:\n      2D tensor with shape:\n      `(batch_size, channels)`\n  """"""\n\n    def call(self, inputs):\n        if self.data_format == ""channels_last"":\n            x_reduced = tfe.reduce_max(inputs, axis=2)\n            x_reduced = tfe.reduce_max(x_reduced, axis=1)\n        else:\n            x_reduced = tfe.reduce_max(inputs, axis=3)\n            x_reduced = tfe.reduce_max(x_reduced, axis=2)\n\n        return x_reduced\n'"
tf_encrypted/keras/layers/pooling_test.py,2,"b'# pylint: disable=missing-docstring\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.keras.testing_utils import agreement_test\nfrom tf_encrypted.keras.testing_utils import layer_test\n\nnp.random.seed(42)\n\n\nclass TestPooling2d(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_maxpooling2d_valid(self):\n        self._core_maxpooling2d(strides=2, padding=""valid"")\n\n    def test_maxpooling2d_same(self):\n        self._core_maxpooling2d(strides=2, padding=""same"")\n\n    def test_maxpooling2d_strides_one(self):\n        self._core_maxpooling2d(strides=1, padding=""valid"")\n\n    def test_avgpooling2d_valid(self):\n        self._core_avgpooling2d(strides=2, padding=""valid"")\n\n    def test_avgpooling2d_same(self):\n        self._core_avgpooling2d(strides=2, padding=""same"")\n\n    def test_avgpooling2d_strides_one(self):\n        self._core_avgpooling2d(strides=1, padding=""valid"")\n\n    def _core_maxpooling2d(self, **layer_kwargs):\n        channel_in = 2\n        input_shape = [2, 8, 8, channel_in]  # channels last\n        pool_size_in = 2\n\n        base_kwargs = {\n            ""pool_size"": pool_size_in,\n        }\n\n        kwargs = {**base_kwargs, **layer_kwargs}\n        agreement_test(\n            tfe.keras.layers.MaxPooling2D, kwargs=kwargs, input_shape=input_shape,\n        )\n        layer_test(\n            tfe.keras.layers.MaxPooling2D, kwargs=kwargs, batch_input_shape=input_shape,\n        )\n\n    def _core_avgpooling2d(self, **layer_kwargs):\n        channel_in = 2\n        input_shape = [2, 8, 8, channel_in]  # channels last\n        pool_size_in = 2\n\n        base_kwargs = {\n            ""pool_size"": pool_size_in,\n        }\n\n        kwargs = {**base_kwargs, **layer_kwargs}\n        agreement_test(\n            tfe.keras.layers.AveragePooling2D, kwargs=kwargs, input_shape=input_shape,\n        )\n        layer_test(\n            tfe.keras.layers.AveragePooling2D,\n            kwargs=kwargs,\n            batch_input_shape=input_shape,\n        )\n\n\nclass TestGlobalPooling2d(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_global_maxpooling2d(self):\n        self._core_global_maxpooling2d()\n\n    def test_global_avgpooling2d(self):\n        self._core_global_avgpooling2d()\n\n    def _core_global_maxpooling2d(self, **layer_kwargs):\n        channel_in = 2\n        input_shape = [2, 8, 8, channel_in]  # channels last\n\n        base_kwargs = {}\n\n        kwargs = {**base_kwargs, **layer_kwargs}\n        agreement_test(\n            tfe.keras.layers.GlobalMaxPooling2D, kwargs=kwargs, input_shape=input_shape,\n        )\n        layer_test(\n            tfe.keras.layers.GlobalMaxPooling2D,\n            kwargs=kwargs,\n            batch_input_shape=input_shape,\n        )\n\n    def _core_global_avgpooling2d(self, **layer_kwargs):\n        channel_in = 2\n        input_shape = [2, 8, 8, channel_in]  # channels last\n\n        base_kwargs = {}\n\n        kwargs = {**base_kwargs, **layer_kwargs}\n        agreement_test(\n            tfe.keras.layers.GlobalAveragePooling2D,\n            kwargs=kwargs,\n            input_shape=input_shape,\n        )\n        layer_test(\n            tfe.keras.layers.GlobalAveragePooling2D,\n            kwargs=kwargs,\n            batch_input_shape=input_shape,\n        )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/keras/layers/relu.py,0,"b'""""""Activation Layer implementation.""""""\nfrom tf_encrypted.keras.activations import relu\nfrom tf_encrypted.keras.engine import Layer\nfrom tf_encrypted.keras.layers.layers_utils import default_args_check\n\n\nclass ReLU(Layer):\n    """"""Rectified Linear Unit activation function.\n  With default values, it returns element-wise `max(x, 0)`.\n  Otherwise, it follows:\n  `f(x) = max_value` for `x >= max_value`,\n  `f(x) = x` for `threshold <= x < max_value`,\n  `f(x) = negative_slope * (x - threshold)` otherwise.\n  Input shape:\n      Arbitrary. Use the keyword argument `input_shape`\n      (tuple of integers, does not include the samples axis)\n      when using this layer as the first layer in a model.\n  Output shape:\n      Same shape as the input.\n  Arguments:\n      max_value: float >= 0. Maximum activation value.\n      negative_slope: float >= 0. Negative slope coefficient.\n      threshold: float. Threshold value for thresholded activation.\n  """"""\n\n    def __init__(self, max_value=None, negative_slope=0, threshold=0, **kwargs):\n        super(ReLU, self).__init__(**kwargs)\n\n        # Not implemented arguments\n        default_args_check(max_value, ""max_value"", ""ReLU"")\n        default_args_check(negative_slope, ""negative_slope"", ""ReLU"")\n        default_args_check(threshold, ""threshold"", ""ReLU"")\n\n    def build(self, input_shape):\n        self.built = True\n\n    def call(self, inputs):\n        return relu(inputs)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n'"
tf_encrypted/keras/layers/relu_test.py,1,"b'# pylint: disable=missing-docstring\nimport unittest\n\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.keras.testing_utils import agreement_test\n\n\nclass TestActivation(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_activation_relu(self):\n        self._core_relu()\n\n    def _core_relu(self, **layer_kwargs):\n        agreement_test(\n            tfe.keras.layers.ReLU, kwargs=layer_kwargs, input_shape=[1, 5], rtol=0.1,\n        )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/keras/models/__init__.py,0,"b'""""""Keras models in TF Encrypted.""""""\nfrom __future__ import absolute_import\n\nfrom .sequential import Sequential\nfrom .sequential import clone_model\nfrom .sequential import model_from_config\n\n__all__ = [\n    ""Sequential"",\n    ""model_from_config"",\n    ""clone_model"",\n]\n'"
tf_encrypted/keras/models/sequential.py,4,"b'""""""Sequential model API.""""""\nimport tensorflow as tf\nfrom tensorflow.keras import utils\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.keras import backend as KE\nfrom tf_encrypted.keras import optimizers\nfrom tf_encrypted.keras.engine.base_layer import Layer\nfrom tf_encrypted.keras.engine.input_layer import Input\nfrom tf_encrypted.keras.engine.input_layer import InputLayer\nfrom tf_encrypted.protocol.pond import PondPrivateTensor\n\n\nclass Sequential(Layer):\n    """"""Model defined by a stack of layers in sequence.""""""\n\n    def __init__(self, layers=None, name=None):\n        super(Sequential, self).__init__(name=name)\n\n        self._layers = []\n\n        # Add to the model any layers passed to the constructor.\n        if layers:\n            for layer in layers:\n                self.add(layer)\n\n    def add(self, layer):\n        """"""Adds a layer instance on top of the layer stack.\n\n    Arguments:\n      layer: layer instance.\n\n    Raises:\n      TypeError: If `layer` is not a layer instance.\n      ValueError: In case the `layer` argument does not\n        know its input shape.\n      ValueError: In case the `layer` argument has\n        multiple output tensors, or is already connected\n        somewhere else (forbidden in `Sequential` models).\n    """"""\n        if not isinstance(layer, Layer):\n            raise TypeError(\n                ""The added layer must be ""\n                ""an instance of class Layer. ""\n                ""Found: "" + str(layer)\n            )\n        self.built = False\n        set_inputs = False\n        if not self._layers:\n            if isinstance(layer, InputLayer):\n                raise ValueError(\n                    ""Do not manually define an InputLayer in your ""\n                    ""tfe.keras.Sequential model.""\n                )\n\n            batch_shape = layer._batch_input_shape  # pylint: disable=protected-access\n\n            # Instantiate an input layer.\n            x = Input(batch_shape=batch_shape, name=layer.name + ""_input"")\n            # This will build the current layer\n            # and create the node connecting the current layer\n            # to the input layer we just created.\n            y = layer(x)\n\n            # If an input layer (placeholder) is available.\n            if isinstance(y, (tuple, list)):\n                raise ValueError(\n                    ""All layers in a Sequential model ""\n                    ""should have a single output tensor. ""\n                    ""For multi-output layers, ""\n                    ""use the functional API.""\n                )\n            self.outputs = [y]\n\n        elif self.outputs:\n            # If the model is being built continuously on top of an input layer:\n            # refresh its output.\n            output_tensor = layer(self.outputs[0])\n            if isinstance(output_tensor, list):\n                raise TypeError(\n                    ""All layers in a Sequential model ""\n                    ""should have a single output tensor. ""\n                    ""For multi-output layers, ""\n                    ""use the functional API.""\n                )\n            self.outputs = [output_tensor]\n        if set_inputs:\n            self.built = True\n        else:\n            self._layers.append(layer)\n\n    def call(\n        self, inputs, training=None, mask=None,\n    ):  # pylint: disable=arguments-differ\n        if training is not None:\n            raise NotImplementedError()\n        if mask is not None:\n            raise NotImplementedError()\n        outputs = inputs  # handle the corner case where self.layers is empty\n        for layer in self.layers:\n            # During each iteration, `inputs` are the inputs to `layer`, and `outputs`\n            # are the outputs of `layer` applied to `inputs`. At the end of each\n            # iteration `inputs` is set to `outputs` to prepare for the next layer.\n            outputs = layer(inputs)\n\n            # `outputs` will be the inputs to the next layer.\n            inputs = outputs\n\n        return outputs\n\n    @property\n    def layers(self):\n        """"""Historically, `sequential.layers` only returns layers that were added\n    via `add`, and omits the auto-generated `InputLayer` that comes at the\n    bottom of the stack.""""""\n        layers = self._layers\n        if layers and isinstance(layers[0], InputLayer):\n            return layers[1:]\n        return layers[:]\n\n    def backward(self, d_y):\n        for layer in reversed(self.layers):\n            grad_weights, d_y = layer.backward(d_y)\n            self._optimizer.apply_gradients(layer.weights, grad_weights)\n\n    def compile(self, optimizer, loss):\n        """"""Configures the model for training.\n\n      Arguments:\n        optimizer: Optimizer instance\n        loss: Objective function\n    """"""\n        self._optimizer = optimizers.get(optimizer)\n        self._loss = loss\n        assert self._optimizer is not None, ""An optimizer must be specified.""\n        assert self._loss is not None, ""A loss must be specified.""\n\n    def fit_batch(self, x, y):\n        """"""Trains the model on a single batch.\n\n    Arguments:\n      x: Private tensor of training data\n      y: Private tensor of target (label) data\n    """"""\n        y_pred = self.call(x)\n        dy = self._loss.grad(y, y_pred)\n        self.backward(dy)\n        loss = self._loss(y, y_pred)\n\n        sess = KE.get_session()\n        self._current_loss = sess.run(loss.reveal())\n\n    def fit(self, x, y, epochs=1, steps_per_epoch=1):\n        """"""Trains the model for a given number of epochs\n    (iterations on a dataset).\n\n    Arguments:\n      x: Private tensor of training data\n      y: Private tensor of target (label) data\n      epochs: Integer. Number of epochs to train the model.\n      steps_per_epoch: Integer. Total number of steps (batches of samples)\n        before declaring one epoch finished and starting the next epoch.\n    """"""\n        assert isinstance(x, PondPrivateTensor), type(x)\n        assert isinstance(y, PondPrivateTensor), type(y)\n\n        # Initialize variables before starting to train\n        sess = KE.get_session()\n        sess.run(tf.global_variables_initializer())\n\n        for e in range(epochs):\n            print(""Epoch {}/{}"".format(e + 1, epochs))\n            batch_size = x.shape.as_list()[0]\n            progbar = utils.Progbar(batch_size * steps_per_epoch)\n            for _ in range(steps_per_epoch):\n                self.fit_batch(x, y)\n                progbar.add(batch_size, values=[(""loss"", self._current_loss)])\n\n    def set_weights(self, weights, sess=None):\n        """"""Sets the weights of the model.\n\n    Arguments:\n      weights: A list of Numpy arrays with shapes and types\n        matching the output of model.get_weights()\n      sess: tfe.Session instance.\n    """"""\n\n        if not sess:\n            sess = KE.get_session()\n\n        # Updated weights for each layer\n        for layer in self.layers:\n            num_param = len(layer.weights)\n            if num_param == 0:\n                continue\n            layer_weights = weights[:num_param]\n\n            layer.set_weights(layer_weights, sess)\n\n            weights = weights[num_param:]\n\n    @classmethod\n    def from_config(cls, config):\n        """"""Instantiates a TFE Keras model from its config.\n\n    Arguments:\n      config: Configuration dictionary matching the output of\n        model.get_weights().\n\n    Returns:\n        A TFE Keras Sequential instance.\n    """"""\n        tfe_model = model_from_config(config)\n\n        return tfe_model\n\n\ndef model_from_config(config):\n    """"""Instantiates a TFE Keras model from its config.\n\n  Arguments:\n    config: Configuration dictionary matching the output of\n        model.get_weights().\n\n  Returns:\n    A TFE Keras Sequential instance.\n  """"""\n\n    tfe_model = tfe.keras.Sequential([])\n\n    for k_l_c in config[""layers""]:\n        tfe_layer = _instantiate_tfe_layer(k_l_c)\n        tfe_model.add(tfe_layer)\n\n    return tfe_model\n\n\ndef clone_model(model):\n    """"""Clone any tf.keras.Model into a tfe.keras.Sequenial model.\n\n  Arguments:\n    model: tf.keras.Sequential or tf.keras.Model instance.\n\n  Returns:\n    A TFE Keras model instance reproducing the behavior of the\n    original model using newly instantiated weights.\n  """"""\n\n    config = model.get_config()\n    weights = model.get_weights()\n\n    tfe_model = model_from_config(config)\n    tfe_model.set_weights(weights)\n\n    return tfe_model\n\n\ndef _instantiate_tfe_layer(keras_layer_config):\n    """"""instantiate TFE layer based on Keras layer config.\n\n  Arguments:\n    keras_layer_config: result of layer.get_config().\n\n  Returns:\n    A TFE Keras layer instance reproducing the behavior of the\n    original Keras layer.\n  """"""\n\n    # Identify tf.keras layer type, and grab the corresponding tfe.keras layer\n    keras_layer_type = keras_layer_config[""class_name""]\n    try:\n        tfe_layer_cls = getattr(tfe.keras.layers, keras_layer_type)\n    except AttributeError:\n        # TODO: rethink how we warn the user about this, maybe codegen a list of\n        #       supported layers in a doc somewhere\n        raise RuntimeError(\n            ""TF Encrypted does not yet support the {lcls} layer."".format(\n                lcls=keras_layer_type\n            )\n        )\n\n    # get layer config to instiate the tfe layer with the right parameters\n    config = keras_layer_config[""config""]\n\n    return tfe_layer_cls(**config)\n'"
tf_encrypted/keras/models/sequential_test.py,26,"b'# pylint: disable=missing-docstring\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.keras import Sequential\nfrom tf_encrypted.keras import backend as KE\nfrom tf_encrypted.keras.layers import Dense\n\nnp.random.seed(42)\ntf.random.set_random_seed(42)\n\n\nclass TestSequential(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_two_layers(self):\n        shape = (1, 3)\n        input_data = np.random.normal(size=shape)\n        with tfe.protocol.SecureNN():\n            model = Sequential()\n            model.add(Dense(2, input_shape=shape))\n            model.add(Dense(3))\n\n            x = tfe.define_private_variable(input_data)\n            model(x)\n\n    def test_model_from_config(self):\n        input_shape = (1, 3)\n        input_data = np.random.normal(size=input_shape)\n        expected, k_weights, k_config = _model_predict_keras(input_data, input_shape)\n\n        with tfe.protocol.SecureNN():\n            x = tfe.define_private_input(\n                ""inputter"", lambda: tf.convert_to_tensor(input_data)\n            )\n\n            tfe_model = tfe.keras.models.model_from_config(k_config)\n            tfe_model.set_weights(k_weights)\n            y = tfe_model(x)\n\n        with KE.get_session() as sess:\n            actual = sess.run(y.reveal())\n\n            np.testing.assert_allclose(actual, expected, rtol=1e-2, atol=1e-3)\n\n        KE.clear_session()\n\n    def test_from_config(self):\n        input_shape = (1, 3)\n        input_data = np.random.normal(size=input_shape)\n        expected, k_weights, k_config = _model_predict_keras(input_data, input_shape)\n\n        with tfe.protocol.SecureNN():\n            x = tfe.define_private_input(\n                ""inputter"", lambda: tf.convert_to_tensor(input_data)\n            )\n\n            tfe_model = Sequential.from_config(k_config)\n            tfe_model.set_weights(k_weights)\n            y = tfe_model(x)\n\n        with KE.get_session() as sess:\n            actual = sess.run(y.reveal())\n\n            np.testing.assert_allclose(actual, expected, rtol=1e-2, atol=1e-3)\n\n        KE.clear_session()\n\n    def test_clone_model(self):\n        input_shape = (1, 3)\n        input_data = np.random.normal(size=input_shape)\n\n        model = tf.keras.models.Sequential()\n        model.add(tf.keras.layers.Dense(2, batch_input_shape=input_shape))\n        model.add(tf.keras.layers.Dense(3))\n        expected = model.predict(input_data)\n\n        with tfe.protocol.SecureNN():\n            x = tfe.define_private_input(\n                ""inputter"", lambda: tf.convert_to_tensor(input_data)\n            )\n\n            tfe_model = tfe.keras.models.clone_model(model)\n\n        with KE.get_session() as sess:\n            y = tfe_model(x)\n            actual = sess.run(y.reveal())\n\n            np.testing.assert_allclose(actual, expected, rtol=1e-2, atol=1e-3)\n\n        KE.clear_session()\n\n    def test_weights_as_private_var(self):\n        input_shape = (1, 3)\n        input_data = np.random.normal(size=input_shape)\n        expected, k_weights, k_config = _model_predict_keras(input_data, input_shape)\n\n        with tfe.protocol.SecureNN():\n            x = tfe.define_private_input(\n                ""inputter"", lambda: tf.convert_to_tensor(input_data)\n            )\n\n            tfe_model = tfe.keras.models.model_from_config(k_config)\n            weights_private_var = [tfe.define_private_variable(w) for w in k_weights]\n\n            with tfe.Session() as sess:\n                for w in weights_private_var:\n                    sess.run(w.initializer)\n\n                tfe_model.set_weights(weights_private_var, sess)\n                y = tfe_model(x)\n\n                actual = sess.run(y.reveal())\n\n                np.testing.assert_allclose(actual, expected, rtol=1e-2, atol=1e-3)\n\n    def test_conv_model(self):\n        num_classes = 10\n        input_shape = (1, 28, 28, 1)\n        input_data = np.random.normal(size=input_shape)\n\n        with tf.Session():\n            model = tf.keras.models.Sequential()\n\n            model.add(tf.keras.layers.Conv2D(2, (3, 3), batch_input_shape=input_shape))\n            model.add(tf.keras.layers.ReLU())\n            model.add(tf.keras.layers.BatchNormalization())\n            model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n            model.add(tf.keras.layers.Conv2D(2, (3, 3)))\n            model.add(tf.keras.layers.ReLU())\n            model.add(tf.keras.layers.BatchNormalization())\n            model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n            model.add(tf.keras.layers.Flatten())\n            model.add(tf.keras.layers.Dense(num_classes, name=""logit""))\n\n            expected = model.predict(input_data)\n            k_weights = model.get_weights()\n            k_config = model.get_config()\n\n        with tfe.protocol.SecureNN():\n            x = tfe.define_private_input(\n                ""inputter"", lambda: tf.convert_to_tensor(input_data)\n            )\n\n            tfe_model = tfe.keras.models.model_from_config(k_config)\n\n            tfe_model.set_weights(k_weights)\n            y = tfe_model(x)\n\n        with KE.get_session() as sess:\n            actual = sess.run(y.reveal())\n\n            np.testing.assert_allclose(actual, expected, rtol=1e-2, atol=1e-2)\n\n        KE.clear_session()\n\n\ndef _model_predict_keras(input_data, input_shape):\n    with tf.Session():\n        model = tf.keras.models.Sequential()\n        model.add(tf.keras.layers.Dense(2, batch_input_shape=input_shape))\n        model.add(tf.keras.layers.Dense(3))\n\n        weights = model.get_weights()\n        config = model.get_config()\n        out = model.predict(input_data)\n\n    return out, weights, config\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/operations/secure_random/__init__.py,0,"b'""""""Secure random API.""""""\nfrom .secure_random import random_uniform\nfrom .secure_random import secure_seed\nfrom .secure_random import seeded_random_uniform\nfrom .secure_random import supports_secure_randomness\nfrom .secure_random import supports_seeded_randomness\n\n__all__ = [\n    ""supports_secure_randomness"",\n    ""supports_seeded_randomness"",\n    ""seeded_random_uniform"",\n    ""random_uniform"",\n    ""secure_seed"",\n]\n'"
tf_encrypted/operations/secure_random/secure_random.py,8,"b'""""""Secure random sampling.""""""\nimport logging\nimport os\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework.errors import NotFoundError\n\nimport tf_encrypted as tfe\n\nlogger = logging.getLogger(""tf_encrypted"")\n\nSO_PATH = ""{dn}/operations/secure_random/secure_random_module_tf_{tfv}.so""\n\n\ndef _try_load_secure_random_module():\n    """"""\n    Attempt to load and return secure random module; returns None if failed.\n    """"""\n    so_file = SO_PATH.format(dn=os.path.dirname(tfe.__file__), tfv=tf.__version__)\n    if not os.path.exists(so_file):\n        logger.warning(\n            (\n                ""Falling back to insecure randomness since the required custom op ""\n                ""could not be found for the installed version of TensorFlow. Fix ""\n                ""this by compiling custom ops. Missing file was \'%s\'""\n            ),\n            so_file,\n        )\n        return None\n\n    try:\n        return tf.load_op_library(so_file)\n\n    except NotFoundError as ex:\n        logger.warning(\n            (\n                ""Falling back to insecure randomness since the required custom op ""\n                ""could not be found for the installed version of TensorFlow. Fix ""\n                ""this by compiling custom ops. ""\n                ""Missing file was \'%s\', error was \\""%s\\"".""\n            ),\n            so_file,\n            ex,\n        )\n\n    except Exception as ex:  # pylint: disable=broad-except\n        logger.error(\n            (\n                ""Falling back to insecure randomness since an error occurred ""\n                \'loading the required custom op: ""%s"".\'\n            ),\n            ex,\n        )\n\n    return None\n\n\nsecure_random_module = _try_load_secure_random_module()\n\n\ndef supports_secure_randomness():\n    return secure_random_module is not None\n\n\ndef supports_seeded_randomness():\n    return secure_random_module is not None\n\n\ndef seeded_random_uniform(\n    shape, minval=0, maxval=None, dtype=tf.int32, seed=None, name=None,\n):\n    """"""\n    Returns cryptographically strong random numbers with a seed\n\n    .. code-block:: python\n\n        x = secure_random([2, 2], minval=-1000, maxval=1000)\n\n    :param list shape: Shape of the random tensor.\n    :param int minval: Minimum value to return, inclusive.\n    :param int maxval: Maximum value to return, exclusive.\n    :param dtype: Data type of the return random values. Either int32 or int64.\n    :param tf.Tensor seed: The seed to be used when generating the random numbers.\n    :param str name:\n\n    :rtype: tf.Tensor\n    """"""\n    dtype = dtypes.as_dtype(dtype)\n    if dtype not in (dtypes.int32, dtypes.int64):\n        raise ValueError(""Invalid dtype %r"" % dtype)\n\n    if maxval is None:\n        raise ValueError(""Must specify maxval for integer dtype %r"" % dtype)\n\n    if seed is None:\n        raise ValueError(""Seed must be passed"")\n\n    minval = ops.convert_to_tensor(minval, dtype=dtype, name=""min"")\n    maxval = ops.convert_to_tensor(maxval, dtype=dtype, name=""max"")\n\n    return secure_random_module.secure_seeded_random_uniform(\n        shape, seed, minval, maxval, name=name,\n    )\n\n\ndef random_uniform(\n    shape, minval=0, maxval=None, dtype=tf.int32, name=None,\n):\n    """"""\n    Returns cryptographically strong random numbers.\n\n    .. code-block:: python\n\n        x = secure_random([2, 2], minval=-1000, maxval=1000)\n\n    :param list shape: Shape of the random tensor.\n    :param int minval: Minimum value to return, inclusive.\n    :param int maxval: Maximum value to return, exclusive.\n    :param dtype: Data type of the return random values. Either int32 or int64.\n    :param tf.Tensor seed: A seed for generating the random numbers.\n    :param str name:\n\n    :rtype: tf.Tensor\n    """"""\n\n    dtype = dtypes.as_dtype(dtype)\n    if dtype not in (dtypes.int32, dtypes.int64):\n        raise ValueError(""Invalid dtype %r"" % dtype)\n\n    if maxval is None:\n        raise ValueError(""Must specify maxval for integer dtype %r"" % dtype)\n\n    minval = ops.convert_to_tensor(minval, dtype=dtype, name=""min"")\n    maxval = ops.convert_to_tensor(maxval, dtype=dtype, name=""max"")\n\n    return secure_random_module.secure_random_uniform(shape, minval, maxval, name=name,)\n\n\ndef secure_seed():\n    return secure_random_module.secure_seed()\n'"
tf_encrypted/operations/secure_random/secure_random_test.py,14,"b'# pylint: disable=missing-docstring\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf_encrypted.operations import secure_random\n\nSEED = [\n    87654321,\n    87654321,\n    87654321,\n    87654321,\n    87654321,\n    87654321,\n    87654321,\n    87654321,\n]\ndisabled_msg = ""Secure random disabled""\ndontskip = secure_random.supports_seeded_randomness()\n\n\n@unittest.skipUnless(dontskip, disabled_msg)\nclass TestSeededRandom(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def tearDown(self):\n        tf.reset_default_graph()\n\n    def test_wrapper(self):\n        expected = [\n            [6610, 5100, 676],\n            [6111, 9801, 5407],\n            [9678, 7188, 8280],\n        ]\n\n        with tf.Session():\n            output = secure_random.seeded_random_uniform(\n                [3, 3], seed=SEED, maxval=10000,\n            ).eval()\n\n            np.testing.assert_array_equal(output, expected)\n\n    def test_min_val(self):\n        expected = [\n            [3220, 200, -8648],\n            [2223, 9603, 815],\n            [9356, 4377, 6561],\n        ]\n\n        with tf.Session():\n            output = secure_random.seeded_random_uniform(\n                [3, 3], seed=SEED, minval=-10000, maxval=10000,\n            ).eval()\n\n            np.testing.assert_array_equal(output, expected)\n\n    def test_invalid_args(self):\n        with tf.Session():\n            # invalid seed\n            with np.testing.assert_raises(ValueError):\n                secure_random.seeded_random_uniform(\n                    [3, 3], maxval=10000, seed=[1],\n                ).eval()\n\n            # invalid maxval\n            with np.testing.assert_raises(ValueError):\n                secure_random.seeded_random_uniform([3, 3]).eval()\n\n            # invalid dtype\n            with np.testing.assert_raises(ValueError):\n                secure_random.seeded_random_uniform(\n                    [3, 3], seed=SEED, maxval=10000, dtype=tf.float32,\n                ).eval()\n\n    def test_rejection(self):\n        m = 1129\n\n        seed0 = [\n            2108217960,\n            -1340439062,\n            476173466,\n            -681389164,\n            -1502583120,\n            1663373136,\n            2144760032,\n            -1591917499,\n        ]\n\n        with tf.Session():\n            out0 = secure_random.seeded_random_uniform(\n                [64, 4500], seed=seed0, maxval=m, dtype=tf.int32,\n            ).eval()\n            out1 = secure_random.seeded_random_uniform(\n                [64, 4500], seed=seed0, maxval=m, dtype=tf.int32,\n            ).eval()\n\n            np.testing.assert_array_equal(out0, out1)\n\n\n@unittest.skipUnless(dontskip, disabled_msg)\nclass TestRandomUniform(unittest.TestCase):\n    def test_wrapper(self):\n        with tf.Session():\n            output = secure_random.random_uniform([3, 3], maxval=10000).eval()\n\n            np.testing.assert_array_equal(output.shape, [3, 3])\n\n    def test_min_val(self):\n        with tf.Session():\n            output = secure_random.random_uniform([6], minval=-10000, maxval=0).eval()\n\n            for out in output:\n                assert out < 0\n\n    def test_invalid_args(self):\n        with tf.Session():\n            # invalid maxval\n            with np.testing.assert_raises(ValueError):\n                secure_random.random_uniform([3, 3]).eval()\n\n            # invalid dtype\n            with np.testing.assert_raises(ValueError):\n                secure_random.random_uniform(\n                    [3, 3], maxval=10000, dtype=tf.float32,\n                ).eval()\n\n\n@unittest.skipUnless(dontskip, disabled_msg)\nclass TestSeed(unittest.TestCase):\n    def test_seed_generation(self):\n        with tf.Session():\n            s = secure_random.secure_seed()\n\n            minval = -2000\n            maxval = 0\n\n            shape = [2, 3]\n\n            output = secure_random.seeded_random_uniform(\n                shape, seed=s, minval=minval, maxval=maxval,\n            ).eval()\n\n            np.testing.assert_array_equal(output.shape, shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/protocol/aby3/__init__.py,0,"b'""""""Implementation of the ABY3 framework.""""""\n\nfrom .aby3 import ABY3\nfrom .aby3 import ARITHMETIC\nfrom .aby3 import BOOLEAN\nfrom .aby3 import ABY3PrivateTensor\nfrom .aby3 import ABY3PublicTensor\nfrom .aby3 import ABY3Tensor\n\n__all__ = [\n    ""ABY3"",\n    ""ABY3Tensor"",\n    ""ABY3PublicTensor"",\n    ""ABY3PrivateTensor"",\n    ""ARITHMETIC"",\n    ""BOOLEAN"",\n]\n'"
tf_encrypted/protocol/aby3/aby3.py,289,"b'# flake8: noqa\n# pylint: disable=all\n""""""\nImplementation of the ABY3 framework.\n""""""\nfrom __future__ import absolute_import\n\nimport abc\nimport sys\nfrom functools import reduce\nfrom math import ceil\nfrom math import log2\nfrom typing import Callable\nfrom typing import List\nfrom typing import Optional\nfrom typing import Tuple\nfrom typing import Union\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom ...config import get_config\nfrom ...operations import secure_random as crypto\nfrom ...player import Player\nfrom ...tensor import fixed64\nfrom ...tensor import fixed64_ni\nfrom ...tensor.boolfactory import bool_factory\nfrom ...tensor.factory import AbstractConstant\nfrom ...tensor.factory import AbstractFactory\nfrom ...tensor.factory import AbstractTensor\nfrom ...tensor.fixed import FixedpointConfig\nfrom ...tensor.fixed import _validate_fixedpoint_config\nfrom ...tensor.helpers import inverse\nfrom ...tensor.native import native_factory\nfrom ..protocol import Protocol\nfrom ..protocol import memoize\n\nTFEInputter = Callable[[], Union[List[tf.Tensor], tf.Tensor]]\nTF_NATIVE_TYPES = [tf.bool, tf.int8, tf.int16, tf.int32, tf.int64]\n\n_THISMODULE = sys.modules[__name__]\n\n# ===== Share types =====\nARITHMETIC = 0\nBOOLEAN = 1\n\n# ===== Factory =====\ni64_factory = native_factory(tf.int64)\nb_factory = bool_factory()\n\n\nclass ABY3(Protocol):\n    """"""ABY3 framework.""""""\n\n    def __init__(\n        self,\n        server_0=None,\n        server_1=None,\n        server_2=None,\n        use_noninteractive_truncation=True,\n    ):\n        self._initializers = list()\n        config = get_config()\n        self.servers = [None, None, None]\n        self.servers[0] = config.get_player(server_0 if server_0 else ""server0"")\n        self.servers[1] = config.get_player(server_1 if server_1 else ""server1"")\n        self.servers[2] = config.get_player(server_2 if server_2 else ""server2"")\n\n        int_factory = i64_factory\n\n        if use_noninteractive_truncation:\n            fixedpoint_config = fixed64_ni\n        else:\n            fixedpoint_config = fixed64\n\n        self.fixedpoint_config = fixedpoint_config\n        self.int_factory = int_factory\n        self.bool_factory = b_factory\n\n        self.pairwise_keys, self.pairwise_nonces = self.setup_pairwise_randomness()\n        self.b2a_keys_1, self.b2a_keys_2, self.b2a_nonce = self.setup_b2a_generator()\n\n    @property\n    def nbits(self):\n        return self.int_factory.nbits\n\n    def setup_pairwise_randomness(self):\n        """"""\n    Initial setup for pairwise randomness: Every two parties hold a shared key.\n    """"""\n        if not crypto.supports_seeded_randomness():\n            raise NotImplementedError(\n                ""Secure randomness implementation is not available.""\n            )\n\n        keys = [[None, None], [None, None], [None, None]]\n        with tf.device(self.servers[0].device_name):\n            seed_0 = crypto.secure_seed()\n        with tf.device(self.servers[1].device_name):\n            seed_1 = crypto.secure_seed()\n        with tf.device(self.servers[2].device_name):\n            seed_2 = crypto.secure_seed()\n\n        # Replicated keys\n        # NOTE: The following `with` contexts do NOT have any impact for the Python-only operations.\n        #       We use them here only for indicating ""which server has which seed"".\n        #       In other words, `keys[0][1] = seed_1` only stores the TF graph node `seed_1` in the\n        #       Python list `keys`, but does NOT actually ""send"" `seed_1` to server 0, which only happens\n        #       when a future TF operation on server 0 uses `keys[0][1]`.\n        # The same NOTE applies to other places where we use Python list to store TF graph nodes in the\n        # `with` context.\n        with tf.device(self.servers[0].device_name):\n            keys[0][0] = seed_0\n            keys[0][1] = seed_1\n        with tf.device(self.servers[1].device_name):\n            keys[1][0] = seed_1\n            keys[1][1] = seed_2\n        with tf.device(self.servers[2].device_name):\n            keys[2][0] = seed_2\n            keys[2][1] = seed_0\n\n        # nonces[0] for server 0 and 1, nonces[1] for server 1 and 2, nonces[2] for server 2 and 0\n        nonces = np.array([0, 0, 0], dtype=np.int)\n\n        return keys, nonces\n\n    def setup_b2a_generator(self):\n        """"""\n    Initial setup for generating shares during the conversion\n    from boolean sharing to arithmetic sharing\n    """"""\n\n        if not crypto.supports_seeded_randomness():\n            raise NotImplementedError(\n                ""Secure randomness implementation is not available.""\n            )\n\n        # Type 1: Server 0 and 1 hold three keys, while server 2 holds two\n        b2a_keys_1 = [[None, None, None], [None, None, None], [None, None, None]]\n        with tf.device(self.servers[0].device_name):\n            seed_0 = crypto.secure_seed()\n        with tf.device(self.servers[1].device_name):\n            seed_1 = crypto.secure_seed()\n        with tf.device(self.servers[2].device_name):\n            seed_2 = crypto.secure_seed()\n\n        with tf.device(self.servers[0].device_name):\n            b2a_keys_1[0][0] = seed_0\n            b2a_keys_1[0][1] = seed_1\n            b2a_keys_1[0][2] = seed_2\n        with tf.device(self.servers[1].device_name):\n            b2a_keys_1[1][0] = seed_0\n            b2a_keys_1[1][1] = seed_1\n            b2a_keys_1[1][2] = seed_2\n        with tf.device(self.servers[2].device_name):\n            b2a_keys_1[2][0] = seed_0\n            b2a_keys_1[2][2] = seed_2\n\n        # Type 2: Server 1 and 2 hold three keys, while server 0 holds two\n        b2a_keys_2 = [[None, None, None], [None, None, None], [None, None, None]]\n        with tf.device(self.servers[0].device_name):\n            seed_0 = crypto.secure_seed()\n        with tf.device(self.servers[1].device_name):\n            seed_1 = crypto.secure_seed()\n        with tf.device(self.servers[2].device_name):\n            seed_2 = crypto.secure_seed()\n\n        with tf.device(self.servers[0].device_name):\n            b2a_keys_2[0][0] = seed_0\n            b2a_keys_2[0][1] = seed_1\n        with tf.device(self.servers[1].device_name):\n            b2a_keys_2[1][0] = seed_0\n            b2a_keys_2[1][1] = seed_1\n            b2a_keys_2[1][2] = seed_2\n        with tf.device(self.servers[2].device_name):\n            b2a_keys_2[2][0] = seed_0\n            b2a_keys_2[2][1] = seed_1\n            b2a_keys_2[2][2] = seed_2\n\n        b2a_nonce = 0\n        return b2a_keys_1, b2a_keys_2, b2a_nonce\n\n    def define_constant(\n        self,\n        value: Union[np.ndarray, int, float],\n        apply_scaling: bool = True,\n        share_type=ARITHMETIC,\n        name: Optional[str] = None,\n        factory: Optional[AbstractFactory] = None,\n    ):\n        """"""\n    Define a constant to use in computation.\n\n    .. code-block:: python\n\n        x = prot.define_constant(np.array([1,2,3,4]), apply_scaling=False)\n\n    :See: tf.constant\n\n    :param bool apply_scaling: Whether or not to scale the value.\n    :param str name: What name to give to this node in the graph.\n    :param AbstractFactory factory: Which tensor type to represent this value with.\n    """"""\n        assert isinstance(value, (np.ndarray, int, float))\n\n        if isinstance(value, (int, float)):\n            value = np.array([value])\n\n        factory = factory or self.int_factory\n\n        value = self._encode(value, apply_scaling)\n        with tf.name_scope(""constant{}"".format(""-"" + name if name else """")):\n            with tf.device(self.servers[0].device_name):\n                x_on_0 = factory.constant(value)\n\n            with tf.device(self.servers[1].device_name):\n                x_on_1 = factory.constant(value)\n\n            with tf.device(self.servers[2].device_name):\n                x_on_2 = factory.constant(value)\n\n        return ABY3Constant(self, [x_on_0, x_on_1, x_on_2], apply_scaling, share_type)\n\n    def define_private_variable(\n        self,\n        initial_value,\n        apply_scaling: bool = True,\n        share_type=ARITHMETIC,\n        name: Optional[str] = None,\n        factory: Optional[AbstractFactory] = None,\n    ):\n        """"""\n    Define a private variable.\n\n    This will take the passed value and construct shares that will be split up\n    between those involved in the computation.\n\n    For example, in a three party replicated sharing, this will split the value into\n    three shares and transfer two shares to each party in a secure manner.\n\n    :see tf.Variable\n\n    :param Union[np.ndarray,tf.Tensor,ABY3PublicTensor] initial_value: The initial value.\n    :param bool apply_scaling: Whether or not to scale the value.\n    :param str name: What name to give to this node in the graph.\n    :param AbstractFactory factory: Which tensor type to represent this value with.\n    """"""\n        init_val_types = (np.ndarray, tf.Tensor, ABY3PrivateTensor)\n        assert isinstance(initial_value, init_val_types), type(initial_value)\n\n        factory = factory or self.int_factory\n        suffix = ""-"" + name if name else """"\n\n        with tf.name_scope(""private-var{}"".format(suffix)):\n\n            if isinstance(initial_value, np.ndarray):\n                initial_value = self._encode(initial_value, apply_scaling)\n                v = factory.tensor(initial_value)\n                shares = self._share(v, share_type=share_type)\n\n            elif isinstance(initial_value, tf.Tensor):\n                initial_value = self._encode(initial_value, apply_scaling)\n                v = factory.tensor(initial_value)\n                shares = self._share(v, share_type=share_type)\n\n            elif isinstance(initial_value, ABY3PrivateTensor):\n                shares = initial_value.unwrapped\n\n            else:\n                raise TypeError(\n                    (""Don\'t know how to turn {} "" ""into private variable"").format(\n                        type(initial_value)\n                    )\n                )\n\n            # The backing factory for the shares might have changed after the sharing step\n            factory = shares[0][0].factory\n            x = [[None, None], [None, None], [None, None]]\n            with tf.device(self.servers[0].device_name):\n                x[0][0] = factory.variable(shares[0][0])\n                x[0][1] = factory.variable(shares[0][1])\n\n            with tf.device(self.servers[1].device_name):\n                x[1][0] = factory.variable(shares[1][0])\n                x[1][1] = factory.variable(shares[1][1])\n\n            with tf.device(self.servers[2].device_name):\n                x[2][0] = factory.variable(shares[2][0])\n                x[2][1] = factory.variable(shares[2][1])\n\n        x = ABY3PrivateVariable(self, x, apply_scaling, share_type)\n        return x\n\n    def define_local_computation(\n        self,\n        player,\n        computation_fn,\n        arguments=None,\n        apply_scaling=True,\n        share_type=ARITHMETIC,\n        name=None,\n        factory=None,\n    ):\n        """"""\n    Define a local computation that happens on plaintext tensors.\n\n    :param player: Who performs the computation and gets to see the values in plaintext.\n    :param apply_scaling: Whether or not to scale the outputs.\n    :param name: Optional name to give to this node in the graph.\n    :param factory: Backing tensor type to use for outputs.\n    """"""\n\n        factory = factory or self.int_factory\n\n        if isinstance(player, str):\n            player = get_config().get_player(player)\n        assert isinstance(player, Player)\n\n        def share_output(v: tf.Tensor):\n            assert (\n                v.shape.is_fully_defined()\n            ), ""Shape of return value \'{}\' on \'{}\' not fully defined"".format(\n                name if name else """", player.name,\n            )\n\n            v = self._encode(v, apply_scaling)\n            w = factory.tensor(v)\n            x = self._share_and_wrap(w, apply_scaling, share_type, player)\n\n            return x\n\n        def reconstruct_input(x, player):\n\n            if isinstance(x, tf.Tensor):\n                return x\n\n            if isinstance(x, ABY3PublicTensor):\n                w, _ = x.unwrapped\n                v = self._decode(w, x.is_scaled)\n                return v\n\n            if isinstance(x, ABY3PrivateTensor):\n                shares = x.unwrapped\n                w = self._reconstruct(shares, player, share_type)\n                v = self._decode(w, x.is_scaled)\n                return v\n\n            raise TypeError(\n                (""Don\'t know how to process input argument "" ""of type {}"").format(\n                    type(x)\n                )\n            )\n\n        with tf.name_scope(name if name else ""local-computation""):\n\n            with tf.device(player.device_name):\n                if arguments is None:\n                    inputs = []\n                else:\n                    if not isinstance(arguments, (list, tuple)):\n                        arguments = [arguments]\n\n                    inputs = [reconstruct_input(x, player) for x in arguments]\n\n                outputs = computation_fn(*inputs)\n\n                if isinstance(outputs, tf.Operation):\n                    return outputs\n\n                if isinstance(outputs, tf.Tensor):\n                    return share_output(outputs)\n\n                if isinstance(outputs, (list, tuple)):\n                    return [share_output(output) for output in outputs]\n\n                raise TypeError(\n                    ""Don\'t know how to handle results of ""\n                    ""type {}"".format(type(outputs))\n                )\n\n    def define_private_input(\n        self,\n        player,\n        inputter_fn,\n        apply_scaling: bool = True,\n        share_type=ARITHMETIC,\n        name: Optional[str] = None,\n        factory: Optional[AbstractFactory] = None,\n    ):\n        """"""\n    Define a private input.\n\n    This represents a `private` input owned by the specified player into the graph.\n\n    :param Union[str,Player] player: Which player owns this input.\n    :param bool apply_scaling: Whether or not to scale the value.\n    :param str name: What name to give to this node in the graph.\n    :param AbstractFactory factory: Which backing type to use for this input\n        (e.g. `int100` or `int64`).\n    """"""\n        suffix = ""-"" + name if name else """"\n\n        return self.define_local_computation(\n            player=player,\n            computation_fn=inputter_fn,\n            arguments=[],\n            apply_scaling=apply_scaling,\n            share_type=share_type,\n            name=""private-input{}"".format(suffix),\n            factory=factory,\n        )\n\n    def define_public_input(\n        self,\n        player: Union[str, Player],\n        inputter_fn: TFEInputter,\n        apply_scaling: bool = True,\n        share_type=ARITHMETIC,\n        name: Optional[str] = None,\n        factory: Optional[AbstractFactory] = None,\n    ):\n        """"""\n    Define a public input.\n\n    This represents a `public` input owned by the specified player into the\n    graph.\n\n    :param Union[str,Player] player: Which player owns this input.\n    :param bool apply_scaling: Whether or not to scale the value.\n    :param str name: What name to give to this node in the graph.\n    """"""\n        if isinstance(player, str):\n            player = get_config().get_player(player)\n        assert isinstance(player, Player)\n\n        factory = factory or self.int_factory\n        suffix = ""-"" + name if name else """"\n\n        def helper(v: tf.Tensor) -> ""ABY3PublicTensor"":\n            assert (\n                v.shape.is_fully_defined()\n            ), ""Shape of input \'{}\' on \'{}\' is not fully defined"".format(\n                name if name else """", player.name,\n            )\n            v = self._encode(v, apply_scaling)\n            w = factory.tensor(v)\n            return ABY3PublicTensor(self, [w, w, w], apply_scaling, share_type)\n\n        with tf.name_scope(""public-input{}"".format(suffix)):\n\n            with tf.device(player.device_name):\n\n                inputs = inputter_fn()\n\n                if isinstance(inputs, tf.Tensor):\n                    # single input -> single output\n                    v = inputs\n                    return helper(v)\n\n                if isinstance(inputs, (list, tuple)):\n                    # multiple inputs -> multiple outputs\n                    return [helper(v) for v in inputs]\n\n                raise TypeError(\n                    (""Don\'t know how to handle inputs of type {}"").format(type(inputs))\n                )\n\n    def define_public_tensor(\n        self,\n        tensor: tf.Tensor,\n        apply_scaling: bool = True,\n        share_type=ARITHMETIC,\n        name: Optional[str] = None,\n        factory: Optional[AbstractFactory] = None,\n    ):\n        """"""\n    Convert a tf.Tensor to an ABY3PublicTensor.\n    """"""\n        assert isinstance(tensor, tf.Tensor)\n        assert (\n            tensor.shape.is_fully_defined()\n        ), ""Shape of input \'{}\' is not fully defined"".format(name if name else """")\n\n        factory = factory or self.int_factory\n\n        with tf.name_scope(""public-tensor""):\n            tensor = self._encode(tensor, apply_scaling)\n            w = factory.tensor(tensor)\n            return ABY3PublicTensor(self, [w, w, w], apply_scaling, share_type)\n\n    def define_output(\n        self, player, arguments, outputter_fn, name=None,\n    ):\n        """"""\n    Define an output for this graph.\n\n    :param player: Which player this output will be sent to.\n    """"""\n\n        def result_wrapper(*args):\n            op = outputter_fn(*args)\n            # wrap in tf.group to prevent sending back any tensors (which might hence\n            # be leaked)\n            return tf.group(op)\n\n        return self.define_local_computation(\n            player=player,\n            computation_fn=result_wrapper,\n            arguments=arguments,\n            name=""output{}"".format(""-"" + name if name else """"),\n        )\n\n    def _encode(\n        self,\n        rationals: Union[tf.Tensor, np.ndarray],\n        apply_scaling: bool,\n        factory=None,\n    ) -> Union[tf.Tensor, np.ndarray]:\n        """"""\n    Encode tensor of rational numbers into tensor of ring elements. Output is\n    of same type as input to allow function to be used for constants.\n    """"""\n\n        with tf.name_scope(""encode""):\n\n            # we first scale as needed\n            if apply_scaling:\n                scaled = rationals * self.fixedpoint_config.scaling_factor\n            else:\n                scaled = rationals\n\n            # and then we round to integers\n\n            if isinstance(scaled, np.ndarray):\n                integers = scaled.astype(int).astype(object)\n\n            elif isinstance(scaled, tf.Tensor):\n                factory = factory or self.int_factory\n                tf_native_type = factory.native_type\n                assert tf_native_type in TF_NATIVE_TYPES\n                integers = tf.cast(scaled, dtype=tf_native_type)\n\n            else:\n                raise TypeError(""Don\'t know how to encode {}"".format(type(rationals)))\n\n            assert type(rationals) == type(integers)\n            return integers\n\n    @memoize\n    def _decode(self, elements: AbstractTensor, is_scaled: bool) -> tf.Tensor:\n        """"""Decode tensor of ring elements into tensor of rational numbers.""""""\n\n        with tf.name_scope(""decode""):\n            scaled = elements.to_native()\n            if not is_scaled:\n                return scaled\n            return scaled / self.fixedpoint_config.scaling_factor\n\n    def _share(self, secret: AbstractTensor, share_type: str, player=None):\n        """"""Secret-share an AbstractTensor.\n\n    Args:\n      secret: `AbstractTensor`, the tensor to share.\n\n    Returns:\n      A pair of `AbstractTensor`, the shares.\n    """"""\n\n        with tf.name_scope(""share""):\n            if share_type == ARITHMETIC or share_type == BOOLEAN:\n                share0 = secret.factory.sample_uniform(secret.shape)\n                share1 = secret.factory.sample_uniform(secret.shape)\n                if share_type == ARITHMETIC:\n                    share2 = secret - share0 - share1\n                elif share_type == BOOLEAN:\n                    share2 = secret ^ share0 ^ share1\n                # Replicated sharing\n                shares = ((share0, share1), (share1, share2), (share2, share0))\n                return shares\n\n            else:\n                raise NotImplementedError(""Unknown share type."")\n\n    def _share_and_wrap(\n        self, secret: AbstractTensor, is_scaled: bool, share_type: str, player=None,\n    ) -> ""ABY3PrivateTensor"":\n        shares = self._share(secret, share_type, player)\n        return ABY3PrivateTensor(self, shares, is_scaled, share_type)\n\n    def _reconstruct(self, shares, player, share_type):\n        """"""\n    Reconstruct the plaintext value at a specified player.\n    The shares might locate at three different players, so we need the \'player\' argument\n    in order to optimally use two local shares and one (probably) remote share to\n    minimize communication.\n\n    :param shares:\n    :param player: Where to reconstruct\n    :return:\n    """"""\n\n        def helper(s0, s1, s2):\n            if share_type == ARITHMETIC:\n                return s0 + s1 + s2\n            elif share_type == BOOLEAN:\n                return s0 ^ s1 ^ s2\n            else:\n                raise NotImplementedError(\n                    ""Only arithmetic and boolean sharings are supported.""\n                )\n\n        with tf.name_scope(""reconstruct""):\n            if share_type == ARITHMETIC or share_type == BOOLEAN:\n                if player == self.servers[0]:\n                    return helper(shares[0][0], shares[0][1], shares[2][0])\n                elif player == self.servers[1]:\n                    return helper(shares[1][0], shares[1][1], shares[0][0])\n                elif player == self.servers[2]:\n                    return helper(shares[2][0], shares[2][1], shares[1][0])\n                else:\n                    # The player is not any of the three ABY3 servers, so\n                    # we just let each server give one share to this player\n                    # in order to have a fair communication cost for each server\n                    return helper(shares[0][0], shares[1][0], shares[2][0])\n\n            else:\n                raise NotImplementedError(""Unknown share type."")\n\n    def _gen_zero_sharing(self, shape, share_type=ARITHMETIC, factory=None):\n        def helper(f0, f1):\n            if share_type == ARITHMETIC:\n                return f0 - f1\n            elif share_type == BOOLEAN:\n                return f0 ^ f1\n            else:\n                raise NotImplementedError(\n                    ""Only arithmetic and boolean sharings are supported.""\n                )\n\n        factory = factory or self.int_factory\n        with tf.name_scope(""zero-sharing""):\n            with tf.device(self.servers[0].device_name):\n                f00 = factory.sample_seeded_uniform(\n                    shape=shape, seed=self.pairwise_keys[0][0] + self.pairwise_nonces[2]\n                )  # yapf: disable\n                f01 = factory.sample_seeded_uniform(\n                    shape=shape, seed=self.pairwise_keys[0][1] + self.pairwise_nonces[0]\n                )  # yapf: disable\n                a0 = helper(f00, f01)\n            with tf.device(self.servers[1].device_name):\n                f10 = factory.sample_seeded_uniform(\n                    shape=shape, seed=self.pairwise_keys[1][0] + self.pairwise_nonces[0]\n                )  # yapf: disable\n                f11 = factory.sample_seeded_uniform(\n                    shape=shape, seed=self.pairwise_keys[1][1] + self.pairwise_nonces[1]\n                )  # yapf: disable\n                a1 = helper(f10, f11)\n            with tf.device(self.servers[2].device_name):\n                f20 = factory.sample_seeded_uniform(\n                    shape=shape, seed=self.pairwise_keys[2][0] + self.pairwise_nonces[1]\n                )  # yapf: disable\n                f21 = factory.sample_seeded_uniform(\n                    shape=shape, seed=self.pairwise_keys[2][1] + self.pairwise_nonces[2]\n                )  # yapf: disable\n                a2 = helper(f20, f21)\n\n        self.pairwise_nonces = self.pairwise_nonces + 1\n        return a0, a1, a2\n\n    def _gen_random_sharing(self, shape, share_type=ARITHMETIC, factory=None):\n\n        r = [[None] * 2 for _ in range(3)]\n        factory = factory or self.int_factory\n        with tf.name_scope(""random-sharing""):\n            with tf.device(self.servers[0].device_name):\n                r[0][0] = factory.sample_seeded_uniform(\n                    shape=shape, seed=self.pairwise_keys[0][0] + self.pairwise_nonces[2]\n                )  # yapf: disable\n                r[0][1] = factory.sample_seeded_uniform(\n                    shape=shape, seed=self.pairwise_keys[0][1] + self.pairwise_nonces[0]\n                )  # yapf: disable\n            with tf.device(self.servers[1].device_name):\n                r[1][0] = factory.sample_seeded_uniform(\n                    shape=shape, seed=self.pairwise_keys[1][0] + self.pairwise_nonces[0]\n                )  # yapf: disable\n                r[1][1] = factory.sample_seeded_uniform(\n                    shape=shape, seed=self.pairwise_keys[1][1] + self.pairwise_nonces[1]\n                )  # yapf: disable\n            with tf.device(self.servers[2].device_name):\n                r[2][0] = factory.sample_seeded_uniform(\n                    shape=shape, seed=self.pairwise_keys[2][0] + self.pairwise_nonces[1]\n                )  # yapf: disable\n                r[2][1] = factory.sample_seeded_uniform(\n                    shape=shape, seed=self.pairwise_keys[2][1] + self.pairwise_nonces[2]\n                )  # yapf: disable\n\n        self.pairwise_nonces = self.pairwise_nonces + 1\n\n        return ABY3PrivateTensor(self, r, True, share_type)\n\n    def _gen_b2a_sharing(self, shape, b2a_keys):\n        shares = [[None, None], [None, None], [None, None]]\n        with tf.device(self.servers[0].device_name):\n            shares[0][0] = self.int_factory.sample_seeded_uniform(\n                shape=shape, seed=b2a_keys[0][0] + self.b2a_nonce\n            )  # yapf: disable\n            shares[0][1] = self.int_factory.sample_seeded_uniform(\n                shape=shape, seed=b2a_keys[0][1] + self.b2a_nonce\n            )  # yapf: disable\n            x_on_0 = None\n            if b2a_keys[0][2] is not None:\n                share_2 = self.int_factory.sample_seeded_uniform(\n                    shape=shape, seed=b2a_keys[0][2] + self.b2a_nonce\n                )  # yapf: disable\n                x_on_0 = shares[0][0] ^ shares[0][1] ^ share_2\n\n        with tf.device(self.servers[1].device_name):\n            shares[1][0] = self.int_factory.sample_seeded_uniform(\n                shape=shape, seed=b2a_keys[1][1] + self.b2a_nonce\n            )  # yapf: disable\n            shares[1][1] = self.int_factory.sample_seeded_uniform(\n                shape=shape, seed=b2a_keys[1][2] + self.b2a_nonce\n            )  # yapf: disable\n            x_on_1 = None\n            if b2a_keys[1][0] is not None:\n                share_0 = self.int_factory.sample_seeded_uniform(\n                    shape=shape, seed=b2a_keys[1][0] + self.b2a_nonce\n                )  # yapf: disable\n                x_on_1 = share_0 ^ shares[1][0] ^ shares[1][1]\n\n        with tf.device(self.servers[2].device_name):\n            shares[2][0] = self.int_factory.sample_seeded_uniform(\n                shape=shape, seed=b2a_keys[2][2] + self.b2a_nonce\n            )  # yapf: disable\n            shares[2][1] = self.int_factory.sample_seeded_uniform(\n                shape=shape, seed=b2a_keys[2][0] + self.b2a_nonce\n            )  # yapf: disable\n            x_on_2 = None\n            if b2a_keys[2][1] is not None:\n                share_1 = self.int_factory.sample_seeded_uniform(\n                    shape=shape, seed=b2a_keys[2][1] + self.b2a_nonce\n                )  # yapf: disable\n                x_on_2 = share_1 ^ shares[2][0] ^ shares[2][1]\n\n        self.b2a_nonce = self.b2a_nonce + 1\n        return x_on_0, x_on_1, x_on_2, shares\n\n    def _ot(\n        self,\n        sender,\n        receiver,\n        helper,\n        m0,\n        m1,\n        c_on_receiver,\n        c_on_helper,\n        key_on_sender,\n        key_on_helper,\n        nonce,\n    ):\n        """"""\n    Three-party OT protocol.\n\n    \'m0\' and \'m1\' are the two messages located on the sender.\n    \'c_on_receiver\' and \'c_on_helper\' should be the same choice bit, located on receiver and helper respectively.\n    \'key_on_sender\' and \'key_on_helper\' should be the same key, located on sender and helper respectively.\n    \'nonce\' is a non-repeating ID for this call of the OT protocol.\n    """"""\n        assert m0.shape == m1.shape, ""m0 shape {}, m1 shape {}"".format(\n            m0.shape, m1.shape\n        )\n        assert m0.factory == self.int_factory\n        assert m1.factory == self.int_factory\n        assert c_on_receiver.factory == self.bool_factory\n        assert c_on_helper.factory == self.bool_factory\n\n        with tf.name_scope(""OT""):\n            int_factory = self.int_factory\n            with tf.device(sender.device_name):\n                w_on_sender = int_factory.sample_seeded_uniform(\n                    shape=[2] + m0.shape.as_list(), seed=key_on_sender + nonce\n                )\n                masked_m0 = m0 ^ w_on_sender[0]\n                masked_m1 = m1 ^ w_on_sender[1]\n            with tf.device(helper.device_name):\n                w_on_helper = int_factory.sample_seeded_uniform(\n                    shape=[2] + m0.shape.as_list(), seed=key_on_helper + nonce\n                )\n                w_c = int_factory.where(\n                    c_on_helper.value, w_on_helper[1], w_on_helper[0], v2=False\n                )\n            with tf.device(receiver.device_name):\n                masked_m_c = int_factory.where(\n                    c_on_receiver.value, masked_m1, masked_m0, v2=False\n                )\n                m_c = masked_m_c ^ w_c\n\n        return m_c\n\n    @memoize\n    def assign(self, variable: ""ABY3PrivateVariable"", value) -> tf.Operation:\n        """"""See tf.assign.""""""\n        assert isinstance(variable, ABY3PrivateVariable), type(variable)\n        assert isinstance(value, ABY3PrivateTensor), type(value)\n        assert (\n            variable.is_scaled == value.is_scaled\n        ), ""Scaling must match: {}, {}"".format(variable.is_scaled, value.is_scaled,)\n\n        var_shares = variable.unwrapped\n        val_shares = value.unwrapped\n\n        with tf.name_scope(""assign""):\n\n            # Having this control_dependencies is important in order to avoid that\n            # computationally-dependent shares are updated in different pace\n            # (e.g., share0 is computed from share1, and we need to make sure that\n            # share1 is NOT already updated).\n            with tf.control_dependencies(\n                [\n                    val_shares[0][0].value,\n                    val_shares[0][1].value,\n                    val_shares[1][0].value,\n                    val_shares[1][1].value,\n                    val_shares[2][0].value,\n                    val_shares[2][1].value,\n                ]\n            ):\n\n                with tf.device(self.servers[0].device_name):\n                    op00 = var_shares[0][0].assign_from_same(val_shares[0][0])\n                    op01 = var_shares[0][1].assign_from_same(val_shares[0][1])\n\n                with tf.device(self.servers[1].device_name):\n                    op10 = var_shares[1][0].assign_from_same(val_shares[1][0])\n                    op11 = var_shares[1][1].assign_from_same(val_shares[1][1])\n\n                with tf.device(self.servers[2].device_name):\n                    op20 = var_shares[2][0].assign_from_same(val_shares[2][0])\n                    op21 = var_shares[2][1].assign_from_same(val_shares[2][1])\n\n                op = tf.group(op00, op01, op10, op11, op20, op21)\n\n        return op\n\n    @memoize\n    def add(self, x, y):\n        """"""\n    Adds two tensors `x` and `y`.\n\n    :param ABY3Tensor x: The first operand.\n    :param ABY3Tensor y: The second operand.\n    """"""\n        x, y = self.lift(x, y)\n        return self.dispatch(""add"", x, y)\n\n    def lift(self, x, y=None, share_type=ARITHMETIC):\n        """"""\n    Convenience method for working with mixed typed tensors in programs:\n    combining any of the ABY3 objects together with e.g. ints and floats\n    will automatically lift the latter into ABY3 objects.\n\n    Lifting will guarantee the two outputs are both scaled or unscaled if at\n    least one of them is lifted from int or float.\n    """"""\n\n        if y is None:\n\n            if isinstance(x, (np.ndarray, int, float)):\n                return self.define_constant(x, share_type=share_type)\n\n            if isinstance(x, tf.Tensor):\n                return self.define_public_tensor(x, share_type=share_type)\n\n            if isinstance(x, ABY3Tensor):\n                return x\n\n            raise TypeError(""Don\'t know how to lift {}"".format(type(x)))\n\n        if isinstance(x, (np.ndarray, int, float)):\n\n            if isinstance(y, (np.ndarray, int, float)):\n                x = self.define_constant(x, share_type=share_type)\n                y = self.define_constant(y, share_type=share_type)\n                return x, y\n\n            if isinstance(y, tf.Tensor):\n                x = self.define_constant(x, share_type=share_type)\n                y = self.define_public_tensor(y, share_type=share_type)\n                return x, y\n\n            if isinstance(y, ABY3Tensor):\n                x = self.define_constant(\n                    x,\n                    apply_scaling=y.is_scaled,\n                    share_type=share_type,\n                    factory=y.backing_dtype,\n                )\n                return x, y\n\n            raise TypeError(\n                (""Don\'t know how to lift "" ""{}, {}"").format(type(x), type(y))\n            )\n\n        if isinstance(x, tf.Tensor):\n\n            if isinstance(y, (np.ndarray, int, float)):\n                x = self.define_public_tensor(x, share_type=share_type)\n                y = self.define_constant(y, share_type=share_type)\n                return x, y\n\n            if isinstance(y, tf.Tensor):\n                x = self.define_public_tensor(x, share_type=share_type)\n                y = self.define_public_tensor(y, share_type=share_type)\n                return x, y\n\n            if isinstance(y, ABY3Tensor):\n                x = self.define_public_tensor(\n                    x,\n                    apply_scaling=y.is_scaled,\n                    share_type=share_type,\n                    factory=y.backing_dtype,\n                )\n                return x, y\n\n            raise TypeError(\n                (""Don\'t know how to lift "" ""{}, {}"").format(type(x), type(y))\n            )\n\n        if isinstance(x, ABY3Tensor):\n\n            if isinstance(y, (np.ndarray, int, float)):\n                y = self.define_constant(\n                    y,\n                    apply_scaling=x.is_scaled,\n                    share_type=share_type,\n                    factory=x.backing_dtype,\n                )\n                return x, y\n\n            if isinstance(y, tf.Tensor):\n                y = self.define_public_tensor(\n                    y,\n                    apply_scaling=x.is_scaled,\n                    share_type=share_type,\n                    factory=x.backing_dtype,\n                )\n                return x, y\n\n            if isinstance(y, ABY3Tensor):\n                return x, y\n\n        raise TypeError((""Don\'t know how to lift "" ""{}, {}"").format(type(x), type(y)))\n\n    @memoize\n    def add_n(self, tensors):\n        # TODO(Morten) we could optimize by doing lazy reductions, potentially\n        #              segmenting as needed\n        return reduce(lambda x, y: x + y, tensors)\n\n    @memoize\n    def sub(self, x, y):\n        x, y = self.lift(x, y)\n        return self.dispatch(""sub"", x, y)\n\n    @memoize\n    def negative(self, x):\n        x = self.lift(x)\n        return self.dispatch(""negative"", x)\n\n    @memoize\n    def mul(self, x, y):\n        x, y = self.lift(x, y)\n        return self.dispatch(""mul"", x, y)\n\n    @memoize\n    def mul_trunc2(self, x, y):\n        x, y = self.lift(x, y)\n        return self.dispatch(""mul_trunc2"", x, y)\n\n    @memoize\n    def div(self, x, y):\n        """"""\n    Performs a true division of `x` by `y` where `y` is public.\n\n    No flooring is performing if `y` is an integer type as it is implicitly\n    treated as a float.\n    """"""\n\n        assert isinstance(x, ABY3Tensor)\n\n        if isinstance(y, float):\n            y_inverse = 1.0 / y\n        elif isinstance(y, int):\n            y_inverse = 1.0 / float(y)\n        elif isinstance(y, ABY3PublicTensor):\n            y_inverse = 1.0 / y.decode()\n        else:\n            raise TypeError(""Don\'t know how to divide by type {}"".format(type(y)))\n\n        return self.mul(x, y_inverse)\n\n    @memoize\n    def pow(self, x, p):\n        x = self.lift(x)\n        return self.dispatch(""pow"", x, p)\n\n    @memoize\n    def matmul(self, x, y):\n        x, y = self.lift(x, y)\n        return self.dispatch(""matmul"", x, y)\n\n    def gather_bit(self, x, even):\n        assert x.share_type is BOOLEAN\n        return self.dispatch(""gather_bit"", x, even)\n\n    def xor_indices(self, x):\n        assert x.share_type is BOOLEAN\n        return self.dispatch(""xor_indices"", x)\n\n    @memoize\n    def transpose(self, x, perm=None):\n        x = self.lift(x)\n        return self.dispatch(""transpose"", x, perm)\n\n    def indexer(self, x: ""ABY3Tensor"", slc) -> ""ABY3Tensor"":\n        return self.dispatch(""indexer"", x, slc)\n\n    def reshape(self, x: ""ABY3Tensor"", axe) -> ""ABY3Tensor"":\n        return self.dispatch(""reshape"", x, axe)\n\n    @memoize\n    def concat(self, xs, axis):\n        if all(isinstance(x, ABY3PublicTensor) for x in xs):\n            return _concat_public(self, xs, axis=axis)\n\n        if all(isinstance(x, ABY3PrivateTensor) for x in xs):\n            return _concat_private(self, xs, axis=axis)\n\n        raise TypeError(""Don\'t know how to do a concat {}"".format(type(xs)))\n\n    @memoize\n    def reduce_sum(self, x, axis=None, keepdims=False):\n        x = self.lift(x)\n        return self.dispatch(""reduce_sum"", x, axis=axis, keepdims=keepdims)\n\n    @memoize\n    def truncate(self, x: ""ABY3Tensor""):\n        return self.dispatch(""truncate"", x)\n\n    @memoize\n    def reveal(self, x):\n        return self.dispatch(""reveal"", x)\n\n    @memoize\n    def B_xor(self, x, y):\n        x, y = self.lift(x, y, share_type=BOOLEAN)\n        return self.dispatch(""B_xor"", x, y)\n\n    @memoize\n    def B_and(self, x, y):\n        x, y = self.lift(x, y, share_type=BOOLEAN)\n        return self.dispatch(""B_and"", x, y)\n\n    @memoize\n    def B_or(self, x, y):\n        x, y = self.lift(x, y, share_type=BOOLEAN)\n        return self.dispatch(""B_or"", x, y)\n\n    @memoize\n    def B_not(self, x):\n        x = self.lift(x, share_type=BOOLEAN)\n        return self.dispatch(""B_not"", x)\n\n    @memoize\n    def B_ppa(self, x, y, n_bits=None, topology=""kogge_stone""):\n        x, y = self.lift(x, y, share_type=BOOLEAN)\n        return self.dispatch(""B_ppa"", x, y, n_bits, topology)\n\n    @memoize\n    def B_add(self, x, y):\n        x, y = self.lift(x, y, share_type=BOOLEAN)\n        return self.dispatch(""B_add"", x, y)\n\n    @memoize\n    def B_sub(self, x, y):\n        x, y = self.lift(x, y, share_type=BOOLEAN)\n        return self.dispatch(""B_sub"", x, y)\n\n    @memoize\n    def lshift(self, x, steps):\n        return self.dispatch(""lshift"", x, steps)\n\n    @memoize\n    def rshift(self, x, steps):\n        return self.dispatch(""rshift"", x, steps)\n\n    @memoize\n    def logical_rshift(self, x, steps):\n        return self.dispatch(""logical_rshift"", x, steps)\n\n    @memoize\n    def A2B(self, x, nbits=None):\n        return self.dispatch(""A2B"", x, nbits)\n\n    @memoize\n    def B2A(self, x, nbits=None):\n        return self.dispatch(""B2A"", x, nbits)\n\n    @memoize\n    def mul_AB(self, x, y):\n        """"""\n    Callers should make sure y is boolean sharing whose backing TF native type is `tf.bool`.\n    There is no automatic lifting for boolean sharing in the mixed-protocol multiplication.\n    """"""\n        x = self.lift(x)\n        return self.dispatch(""mul_AB"", x, y)\n\n    @memoize\n    def bit_extract(self, x, i):\n        if x.share_type == BOOLEAN or x.share_type == ARITHMETIC:\n            return self.dispatch(""bit_extract"", x, i)\n        else:\n            raise ValueError(""unsupported share type: {}"".format(x.share_type))\n\n    @memoize\n    def msb(self, x):\n        return self.bit_extract(x, self.nbits - 1)\n\n    @memoize\n    def polynomial(self, x, coeffs):\n        x = self.lift(x)\n        return self.dispatch(""polynomial"", x, coeffs)\n\n    @memoize\n    def polynomial_piecewise(self, x, c, coeffs):\n        return self.dispatch(""polynomial_piecewise"", x, c, coeffs)\n\n    @memoize\n    def sigmoid(self, x, approx_type=""piecewise_linear""):\n        return self.dispatch(""sigmoid"", x, approx_type)\n\n    @memoize\n    def gather(self, x, indices, axis):\n        raise NotImplementedError(""Unsupported share type: {}"".format(x.share_type))\n\n    @memoize\n    def stack(self, xs, axis):\n        raise TypeError(""Don\'t know how to do a stack {}"".format(type(xs)))\n\n    def write(self, x, filename_prefix):\n        if not isinstance(x, ABY3PrivateTensor):\n            raise TypeError(""Only support writing ABY3PrivateTensor to disk."")\n        return self.dispatch(""write"", x, filename_prefix)\n\n    def read(self, filename_prefix, batch_size, n_columns):\n        return self.dispatch(""read"", filename_prefix, batch_size, n_columns)\n\n    def iterate(\n        self,\n        tensor: ""ABY3PrivateTensor"",\n        batch_size: int,\n        repeat=True,\n        shuffle=True,\n        seed: int = None,\n    ):\n        if not isinstance(tensor, ABY3PrivateTensor):\n            raise TypeError(""Only support iterating ABY3PrivateTensor."")\n        return self.dispatch(""iterate"", tensor, batch_size, repeat, shuffle, seed)\n\n    def blinded_shuffle(self, tensor: ""ABY3PrivateTensor""):\n        """"""\n    Shuffle the rows of the given tenosr privately.\n    After the shuffle, none of the share holder could know the exact shuffle order.\n    """"""\n        if not isinstance(tensor, ABY3PrivateTensor):\n            raise TypeError(\n                (\n                    ""Only support blindly shuffle ABY3PrivateTensor. ""\n                    ""For public tensor, use the shuffle() method""\n                )\n            )\n        return self.dispatch(""blinded_shuffle"", tensor)\n\n    def dispatch(self, base_name, *args, container=None, **kwargs):\n        """"""\n    Finds the correct protocol logicto perform based on the dispatch_id\n    attribute of the input tensors in args.\n    """"""\n        suffix = ""_"".join(\n            [arg.dispatch_id for arg in args if hasattr(arg, ""dispatch_id"")]\n        )\n        func_name = ""_{}_{}"".format(base_name, suffix)\n\n        if container is None:\n            container = _THISMODULE\n\n        func = getattr(container, func_name, None)\n        if func is not None:\n            return func(self, *args, **kwargs)  # pylint: disable=not-callable\n        raise TypeError(\n            (""Don\'t know how to {}: {}"").format(base_name, [type(arg) for arg in args])\n        )\n\n\n#\n# Classes representing the base values in the ABY3 protocol.\n#\n\n\nclass ABY3Tensor(abc.ABC):\n    """"""\n  This class functions mostly as a convenient way of exposing operations\n  directly on the various tensor objects, ie allowing one to write `x + y`\n  instead of `prot.add(x, y)`. Since this functionality is shared among all\n  tensors we put it in this superclass.\n\n  This class should never be instantiated on its own.\n  Instead you should use your chosen protocols factory methods::\n\n      x = prot.define_private_input(tf.constant(np.array([1,2,3,4])))\n      y = prot.define_public_input(tf.constant(np.array([4,5,6,7])))\n\n      z = x + y\n\n      with config.Session() as sess:\n          answer = z.reveal().eval(sess)\n\n          print(answer) # => [5, 7, 9, 11]\n    """"""\n\n    def __init__(self, prot, is_scaled, share_type):\n        self.prot = prot\n        self.is_scaled = is_scaled\n        self.share_type = share_type\n\n    @property\n    @abc.abstractmethod\n    def shape(self) -> List[int]:\n        """"""\n    :rtype: List[int]\n    :returns: The shape of this tensor.\n    """"""\n        pass\n\n    @property\n    @abc.abstractmethod\n    def unwrapped(self) -> Tuple[AbstractTensor, ...]:\n        pass\n\n    def add(self, other):\n        """"""\n    Add `other` to this ABY3Tensor.  This can be another tensor with the same\n    backing or a primitive.\n\n    This function returns a new ABY3Tensor and does not modify this one.\n\n    :param ABY3Tensor other: a or primitive (e.g. a float)\n    :return: A new ABY3Tensor with `other` added.\n    :rtype: ABY3Tensor\n    """"""\n        if self.share_type == ARITHMETIC:\n            return self.prot.add(self, other)\n        else:\n            raise ValueError(\n                ""unsupported share type for add: {}"".format(self.share_type)\n            )\n\n    def __add__(self, other):\n        """"""\n    See :meth:`~tf_encrypted.protocol.aby3.ABY3Tensor.add`\n    """"""\n        return self.add(other)\n\n    def __radd__(self, other):\n        return self + other\n\n    def reduce_sum(self, axis=None, keepdims=False):\n        """"""\n    Like :meth:`tensorflow.reduce_sum`\n\n    :param int axis:  The axis to reduce along\n    :param bool keepdims: If true, retains reduced dimensions with length 1.\n    :return: A new ABY3Tensor\n    :rtype: ABY3Tensor\n    """"""\n        return self.prot.reduce_sum(self, axis, keepdims)\n\n    def sum(self, axis=None, keepdims=False):\n        """"""\n    See :meth:`ABY3Tensor.reduce_sum`\n    """"""\n        return self.reduce_sum(axis, keepdims)\n\n    def sub(self, other):\n        """"""\n    Subtract `other` from this tensor.\n\n    :param ABY3Tensor other: to subtract\n    :return: A new ABY3Tensor\n    :rtype: ABY3Tensor\n    """"""\n        if self.share_type == ARITHMETIC:\n            return self.prot.sub(self, other)\n        else:\n            raise ValueError(\n                ""unsupported share type for sub: {}"".format(self.share_type)\n            )\n\n    def __sub__(self, other):\n        return self.sub(other)\n\n    def __rsub__(self, other):\n        if self.share_type == ARITHMETIC:\n            return self.prot.sub(other, self)\n        else:\n            raise ValueError(\n                ""unsupported share type for sub: {}"".format(self.share_type)\n            )\n\n    def mul(self, other):\n        """"""\n    Multiply this tensor with `other`\n\n    :param ABY3Tensor other: to multiply\n    :return: A new ABY3Tensor\n    :rtype: ABY3Tensor\n    """"""\n        return self.prot.mul(self, other)\n\n    def __mul__(self, other):\n        return self.prot.mul(self, other)\n\n    def __rmul__(self, other):\n        return self.prot.mul(other, self)\n\n    def __truediv__(self, other):\n        return self.prot.div(self, other)\n\n    def __mod__(self, other):\n        return self.prot.mod(self, other)\n\n    def __pow__(self, p):\n        return self.prot.pow(self, p)\n\n    def square(self):\n        """"""\n    Square this tensor.\n\n    :return: A new ABY3Tensor\n    :rtype: ABY3Tensor\n    """"""\n        return self.prot.square(self)\n\n    def matmul(self, other):\n        """"""\n    MatMul this tensor with `other`.  This will perform matrix multiplication,\n    rather than elementwise like\n    :meth:`~tf_encrypted.protocol.aby3.ABY3Tensor.mul`\n\n    :param ABY3Tensor other: to mul\n    :return: A new ABY3Tensor\n    :rtype: ABY3Tensor\n    """"""\n        return self.prot.matmul(self, other)\n\n    def dot(self, other):\n        """"""\n    :return: A new ABY3Tensor\n    :rtype: ABY3Tensor\n    """"""\n        return self.matmul(other)\n\n    def __getitem__(self, slc):\n        return self.prot.indexer(self, slc)\n\n    def transpose(self, perm=None):\n        """"""\n    Transpose this tensor.\n\n    See :meth:`tensorflow.transpose`\n\n    :param List[int]: A permutation of the dimensions of this tensor.\n\n    :return: A new ABY3Tensor\n    :rtype: ABY3Tensor\n    """"""\n        return self.prot.transpose(self, perm)\n\n    def truncate(self):\n        """"""\n    Truncate this tensor.\n\n    `TODO`\n\n    :return: A new ABY3Tensor\n    :rtype: ABY3Tensor\n    """"""\n        return self.prot.truncate(self)\n\n    def expand_dims(self, axis=None):\n        """"""\n    :See: tf.expand_dims\n\n    :return: A new ABY3Tensor\n    :rtype: ABY3Tensor\n    """"""\n        return self.prot.expand_dims(self, axis=axis)\n\n    def reshape(self, shape: List[int]) -> ""ABY3Tensor"":\n        """"""\n    :See: tf.reshape\n\n    :param List[int] shape: The new shape of the tensor.\n    :rtype: ABY3Tensor\n    :returns: A new tensor with the contents of this tensor, but with the new\n        specified shape.\n    """"""\n        return self.prot.reshape(self, shape)\n\n    def __neg__(self):\n        return self.prot.negative(self)\n\n    def negative(self) -> ""ABY3Tensor"":\n        """"""\n    :See: tf.negative\n\n    :rtype: ABY3Tensor\n    :returns: A new tensor with numerical negative value element-wise computed.\n    """"""\n        return self.prot.negative(self)\n\n    def reduce_max(self, axis: int) -> ""ABY3Tensor"":\n        """"""\n    :See: tf.reduce_max\n\n    :param int axis: The axis to take the max along\n    :rtype: ABY3Tensor\n    :returns: A new ABY3 tensor with the max value from each axis.\n    """"""\n        return self.prot.reduce_max(self, axis)\n\n    def bitwise_xor(self, other):\n        if self.share_type == BOOLEAN:\n            return self.prot.B_xor(self, other)\n        else:\n            raise ValueError(\n                ""Unsupported share type for xor: {}"".format(self.share_type)\n            )\n\n    def __xor__(self, other):\n        return self.bitwise_xor(other)\n\n    def bitwise_and(self, other):\n        if self.share_type == BOOLEAN:\n            return self.prot.B_and(self, other)\n        else:\n            raise ValueError(\n                ""unsupported share type for and: {}"".format(self.share_type)\n            )\n\n    def __and__(self, other):\n        return self.bitwise_and(other)\n\n    def bitwise_or(self, other):\n        if self.share_type == BOOLEAN:\n            return self.prot.B_or(self, other)\n        else:\n            raise ValueError(\n                ""unsupported share type for and: {}"".format(self.share_type)\n            )\n\n    def __or__(self, other):\n        return self.bitwise_or(other)\n\n    def invert(self):\n        if self.share_type == BOOLEAN:\n            return self.prot.B_not(self)\n        else:\n            raise ValueError(\n                ""unsupported share type for and: {}"".format(self.share_type)\n            )\n\n    def __invert__(self):\n        return self.invert()\n\n    def __lshift__(self, steps):\n        return self.prot.lshift(self, steps)\n\n    def lshift(self, steps):\n        return self.prot.lshift(self, steps)\n\n    def __rshift__(self, steps):\n        return self.prot.rshift(self, steps)\n\n    def rshift(self, steps):\n        return self.prot.rshift(self, steps)\n\n    def arith_rshift(self, steps):\n        return self.rshift(steps)\n\n    def logical_rshift(self, steps):\n        return self.prot.logical_rshift(self, steps)\n\n    def write(self, filename_prefix):\n        return self.prot.write(self, filename_prefix)\n\n\nclass ABY3PublicTensor(ABY3Tensor):\n    """"""\n  This class represents a public tensor, known by at least by the three servers\n  but potentially known by more. Although there is only a single value we\n  replicate it on both servers to avoid sending it from one to the other\n  in the operations where it\'s needed by both (eg multiplication).\n  """"""\n\n    dispatch_id = ""public""\n\n    def __init__(\n        self, prot: ABY3, values: List[AbstractTensor], is_scaled: bool, share_type\n    ) -> None:\n        assert all(isinstance(v, AbstractTensor) for v in values)\n        assert all((v.shape == values[0].shape) for v in values)\n\n        super(ABY3PublicTensor, self).__init__(prot, is_scaled, share_type)\n        self.values = values\n\n    def __repr__(self) -> str:\n        return ""ABY3PublicTensor(shape={}, share_type={})"".format(\n            self.shape, self.share_type\n        )\n\n    @property\n    def shape(self) -> List[int]:\n        return self.values[0].shape\n\n    @property\n    def backing_dtype(self):\n        return self.values[0].factory\n\n    @property\n    def unwrapped(self) -> Tuple[AbstractTensor, ...]:\n        """"""\n    Unwrap the tensor.\n\n    This will return the value for each of the parties that collectively own\n    the tensor.\n\n    In most cases, this will be the same value on each device.\n\n    .. code-block:: python\n\n        x_0, y_0, z_0 = tensor.unwrapped\n        # x_0 == 10 with the value pinned to player_0\'s device.\n        # y_0 == 10 with the value pinned to player_1\'s device.\n        # z_0 == 10 with the value pinned to player_2\'s device.\n\n    In most cases you will want to work on this data on the specified device.\n\n    .. code-block:: python\n\n        x_0, y_0, z_0= tensor.unwrapped\n\n        with tf.device(prot.player_0.device_name):\n            # act on x_0\n\n        with tf.device(prot.player_1.device_name):\n            # act on y_0\n\n        with tf.device(prot.player_2.device_name):\n            # act on z_0\n\n    In most cases you will not need to use this method.  All funtions\n    will hide this functionality for you (e.g. `add`, `mul`, etc).\n    """"""\n        return self.values\n\n    def decode(self) -> Union[np.ndarray, tf.Tensor]:\n        return self.prot._decode(\n            self.values[0], self.is_scaled\n        )  # pylint: disable=protected-access\n\n    def to_native(self):\n        return self.decode()\n\n\nclass ABY3Constant(ABY3PublicTensor):\n    """"""\n  This class essentially represents a public value, however it additionally\n  records the fact that the underlying value was declared as a constant.\n  """"""\n\n    def __init__(self, prot, constants, is_scaled, share_type):\n        assert all(isinstance(c, AbstractConstant) for c in constants)\n        assert all((c.shape == constants[0].shape) for c in constants)\n\n        super(ABY3Constant, self).__init__(prot, constants, is_scaled, share_type)\n        self.constants = constants\n\n    def __repr__(self) -> str:\n        return ""ABY3Constant(shape={}, share_type={})"".format(\n            self.shape, self.share_type\n        )\n\n\nclass ABY3PrivateTensor(ABY3Tensor):\n    """"""\n  This class represents a private value that may be unknown to everyone.\n  """"""\n\n    dispatch_id = ""private""\n\n    def __init__(self, prot, shares, is_scaled, share_type):\n        assert len(shares) == 3\n        assert all(\n            (ss.shape == shares[0][0].shape) for s in shares for ss in s\n        ), ""Shares have different shapes.""\n\n        super(ABY3PrivateTensor, self).__init__(prot, is_scaled, share_type)\n        self.shares = shares\n\n    def __repr__(self) -> str:\n        return ""ABY3PrivateTensor(shape={}, share_type={})"".format(\n            self.shape, self.share_type\n        )\n\n    @property\n    def shape(self) -> List[int]:\n        return self.shares[0][0].shape\n\n    @property\n    def backing_dtype(self):\n        return self.shares[0][0].factory\n\n    @property\n    def unwrapped(self):\n        return self.shares\n\n    def reveal(self) -> ABY3PublicTensor:\n        return self.prot.reveal(self)\n\n\nclass ABY3PrivateVariable(ABY3PrivateTensor):\n    """"""\n  This class essentially represents a private value, however it additionally\n  records the fact that the backing tensor was declared as a variable in\n  order to allow treating it as a variable itself.\n  """"""\n\n    def __init__(self, prot, shares, is_scaled, share_type):\n        super(ABY3PrivateVariable, self).__init__(prot, shares, is_scaled, share_type)\n        self.shares = shares\n        self.initializer = tf.group(\n            *[var.initializer for share in shares for var in share]\n        )\n\n    def __repr__(self) -> str:\n        return ""ABY3PrivateVariable(shape={}, share_type={})"".format(\n            self.shape, self.share_type\n        )\n\n\n#\n# reveal helpers\n#\n\n\ndef _reveal_private(prot, x):\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n\n    with tf.name_scope(""reveal""):\n\n        shares = x.unwrapped\n\n        with tf.device(prot.servers[0].device_name):\n            z_on_0 = prot._reconstruct(shares, prot.servers[0], x.share_type)\n\n        with tf.device(prot.servers[1].device_name):\n            z_on_1 = prot._reconstruct(shares, prot.servers[1], x.share_type)\n\n        with tf.device(prot.servers[2].device_name):\n            z_on_2 = prot._reconstruct(shares, prot.servers[2], x.share_type)\n\n    return ABY3PublicTensor(prot, [z_on_0, z_on_1, z_on_2], x.is_scaled, x.share_type)\n\n\n#\n# add helpers\n#\n\n\ndef _add_private_private(prot, x, y):\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n    assert isinstance(y, ABY3PrivateTensor), type(y)\n\n    z = [[None] * 2 for _ in range(3)]\n    with tf.name_scope(""add""):\n        for i in range(3):\n            with tf.device(prot.servers[i].device_name):\n                z[i][0] = x.shares[i][0] + y.shares[i][0]\n                z[i][1] = x.shares[i][1] + y.shares[i][1]\n\n    return ABY3PrivateTensor(prot, z, x.is_scaled, x.share_type)\n\n\ndef _add_private_public(prot, x, y):\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n    assert isinstance(y, ABY3PublicTensor), type(y)\n    assert x.is_scaled == y.is_scaled, (\n        ""Cannot mix different encodings: "" ""{} {}""\n    ).format(x.is_scaled, y.is_scaled)\n\n    shares = x.unwrapped\n    y_on_0, _, y_on_2 = y.unwrapped\n\n    z = [[None] * 2 for _ in range(3)]\n    with tf.name_scope(""add""):\n\n        with tf.device(prot.servers[0].device_name):\n            z[0][0] = shares[0][0] + y_on_0\n            z[0][1] = shares[0][1]\n\n        with tf.device(prot.servers[1].device_name):\n            z[1][0] = shares[1][0]\n            z[1][1] = shares[1][1]\n\n        with tf.device(prot.servers[2].device_name):\n            z[2][0] = shares[2][0]\n            z[2][1] = shares[2][1] + y_on_2\n    return ABY3PrivateTensor(prot, z, x.is_scaled, x.share_type)\n\n\ndef _add_public_private(prot, x, y):\n    assert isinstance(x, ABY3PublicTensor), type(x)\n    assert isinstance(y, ABY3PrivateTensor), type(y)\n    assert x.is_scaled == y.is_scaled, (\n        ""Cannot mix different encodings: "" ""{} {}""\n    ).format(x.is_scaled, y.is_scaled)\n\n    x_on_0, _, x_on_2 = x.unwrapped\n    shares = y.unwrapped\n\n    z = [[None] * 2 for _ in range(3)]\n    with tf.name_scope(""add""):\n\n        with tf.device(prot.servers[0].device_name):\n            z[0][0] = shares[0][0] + x_on_0\n            z[0][1] = shares[0][1]\n\n        with tf.device(prot.servers[1].device_name):\n            z[1][0] = shares[1][0]\n            z[1][1] = shares[1][1]\n\n        with tf.device(prot.servers[2].device_name):\n            z[2][0] = shares[2][0]\n            z[2][1] = shares[2][1] + x_on_2\n\n    return ABY3PrivateTensor(prot, z, x.is_scaled, y.share_type)\n\n\ndef _add_public_public(prot, x, y):\n    assert isinstance(x, ABY3PublicTensor), type(x)\n    assert isinstance(y, ABY3PublicTensor), type(y)\n    assert x.is_scaled == y.is_scaled, ""Cannot add tensors with different scales""\n\n    x_shares = x.unwrapped\n    y_shares = y.unwrapped\n\n    z = [None] * 3\n    with tf.name_scope(""add""):\n        for i in range(3):\n            z[i] = x_shares[i] + y_shares[i]\n\n    return ABY3PublicTensor(prot, z, x.is_scaled, x.share_type)\n\n\n#\n# sub helpers\n#\n\n\ndef _sub_private_private(prot, x, y):\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n    assert isinstance(y, ABY3PrivateTensor), type(y)\n    assert x.is_scaled == y.is_scaled\n\n    z = [[None] * 2 for _ in range(3)]\n    with tf.name_scope(""sub""):\n        x_shares = x.unwrapped\n        y_shares = y.unwrapped\n        for i in range(3):\n            with tf.device(prot.servers[i].device_name):\n                z[i][0] = x_shares[i][0] - y_shares[i][0]\n                z[i][1] = x_shares[i][1] - y_shares[i][1]\n\n    return ABY3PrivateTensor(prot, z, x.is_scaled, x.share_type)\n\n\ndef _sub_private_public(prot, x, y):\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n    assert isinstance(y, ABY3PublicTensor), type(y)\n    assert x.is_scaled == y.is_scaled\n\n    shares = x.unwrapped\n    y_on_0, _, y_on_2 = y.unwrapped\n\n    z = [[None] * 2 for _ in range(3)]\n    with tf.name_scope(""sub""):\n\n        with tf.device(prot.servers[0].device_name):\n            z[0][0] = shares[0][0] - y_on_0\n            z[0][1] = shares[0][1]\n\n        with tf.device(prot.servers[1].device_name):\n            z[1][0] = shares[1][0]\n            z[1][1] = shares[1][1]\n\n        with tf.device(prot.servers[2].device_name):\n            z[2][0] = shares[2][0]\n            z[2][1] = shares[2][1] - y_on_2\n\n    return ABY3PrivateTensor(prot, z, x.is_scaled, x.share_type)\n\n\ndef _sub_public_private(prot, x, y):\n    assert isinstance(x, ABY3PublicTensor), type(x)\n    assert isinstance(y, ABY3PrivateTensor), type(y)\n\n    x_on_0, _, x_on_2 = x.unwrapped\n    shares = y.unwrapped\n\n    z = [[None] * 2 for _ in range(3)]\n    with tf.name_scope(""sub""):\n\n        with tf.device(prot.servers[0].device_name):\n            z[0][0] = x_on_0 - shares[0][0]\n            z[0][1] = -shares[0][1]\n\n        with tf.device(prot.servers[1].device_name):\n            z[1][0] = -shares[1][0]\n            z[1][1] = -shares[1][1]\n\n        with tf.device(prot.servers[2].device_name):\n            z[2][0] = -shares[2][0]\n            z[2][1] = x_on_2 - shares[2][1]\n\n    return ABY3PrivateTensor(prot, z, x.is_scaled, y.share_type)\n\n\n#\n# negative helpers\n#\n\n\ndef _negative_private(prot, x):\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n\n    x_shares = x.unwrapped\n\n    z = [[None, None], [None, None], [None, None]]\n    with tf.name_scope(""negative""):\n        for i in range(3):\n            with tf.device(prot.servers[i].device_name):\n                z[i][0] = -x_shares[i][0]\n                z[i][1] = -x_shares[i][1]\n\n        z = ABY3PrivateTensor(prot, z, x.is_scaled, x.share_type)\n    return z\n\n\ndef _negative_public(prot, x):\n    assert isinstance(x, ABY3PublicTensor), type(x)\n\n    x_on_0, x_on_1, x_on_2 = x.unwrapped\n\n    with tf.name_scope(""negative""):\n        with tf.device(prot.servers[0].device_name):\n            x_on_0_neg = -x_on_0\n        with tf.device(prot.servers[1].device_name):\n            x_on_1_neg = -x_on_1\n        with tf.device(prot.servers[2].device_name):\n            x_on_2_neg = -x_on_2\n        x_neg = ABY3PublicTensor(\n            prot, [x_on_0_neg, x_on_1_neg, x_on_2_neg], x.is_scaled, x.share_type\n        )\n    return x_neg\n\n\n#\n# mul helpers\n#\n\n\ndef _mul_public_private(prot, x, y):\n    assert isinstance(x, ABY3PublicTensor), type(x)\n    assert isinstance(y, ABY3PrivateTensor), type(y)\n\n    x_on_0, x_on_1, x_on_2 = x.unwrapped\n    shares = y.unwrapped\n\n    z = [[None, None], [None, None], [None, None]]\n    with tf.name_scope(""mul""):\n\n        with tf.device(prot.servers[0].device_name):\n            z[0][0] = shares[0][0] * x_on_0\n            z[0][1] = shares[0][1] * x_on_0\n\n        with tf.device(prot.servers[1].device_name):\n            z[1][0] = shares[1][0] * x_on_1\n            z[1][1] = shares[1][1] * x_on_1\n\n        with tf.device(prot.servers[2].device_name):\n            z[2][0] = shares[2][0] * x_on_2\n            z[2][1] = shares[2][1] * x_on_2\n\n        z = ABY3PrivateTensor(prot, z, x.is_scaled or y.is_scaled, y.share_type)\n        z = prot.truncate(z) if x.is_scaled and y.is_scaled else z\n        return z\n\n\ndef _mul_private_public(prot, x, y):\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n    assert isinstance(y, ABY3PublicTensor), type(y)\n\n    shares = x.unwrapped\n    y_on_0, y_on_1, y_on_2 = y.unwrapped\n\n    z = [[None, None], [None, None], [None, None]]\n    with tf.name_scope(""mul""):\n\n        with tf.device(prot.servers[0].device_name):\n            z[0][0] = shares[0][0] * y_on_0\n            z[0][1] = shares[0][1] * y_on_0\n\n        with tf.device(prot.servers[1].device_name):\n            z[1][0] = shares[1][0] * y_on_1\n            z[1][1] = shares[1][1] * y_on_1\n\n        with tf.device(prot.servers[2].device_name):\n            z[2][0] = shares[2][0] * y_on_2\n            z[2][1] = shares[2][1] * y_on_2\n\n        z = ABY3PrivateTensor(prot, z, x.is_scaled or y.is_scaled, x.share_type)\n        z = prot.truncate(z) if x.is_scaled and y.is_scaled else z\n        return z\n\n\ndef _mul_private_private(prot, x, y):\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n    assert isinstance(y, ABY3PrivateTensor), type(y)\n\n    x_shares = x.unwrapped\n    y_shares = y.unwrapped\n\n    z = [[None, None], [None, None], [None, None]]\n    with tf.name_scope(""mul""):\n        a0, a1, a2 = prot._gen_zero_sharing(x.shape)\n        with tf.device(prot.servers[0].device_name):\n            z0 = (\n                x_shares[0][0] * y_shares[0][0]\n                + x_shares[0][0] * y_shares[0][1]\n                + x_shares[0][1] * y_shares[0][0]\n                + a0\n            )\n\n        with tf.device(prot.servers[1].device_name):\n            z1 = (\n                x_shares[1][0] * y_shares[1][0]\n                + x_shares[1][0] * y_shares[1][1]\n                + x_shares[1][1] * y_shares[1][0]\n                + a1\n            )\n\n        with tf.device(prot.servers[2].device_name):\n            z2 = (\n                x_shares[2][0] * y_shares[2][0]\n                + x_shares[2][0] * y_shares[2][1]\n                + x_shares[2][1] * y_shares[2][0]\n                + a2\n            )\n        # Re-sharing\n        with tf.device(prot.servers[0].device_name):\n            z[0][0] = z0\n            z[0][1] = z1\n        with tf.device(prot.servers[1].device_name):\n            z[1][0] = z1\n            z[1][1] = z2\n        with tf.device(prot.servers[2].device_name):\n            z[2][0] = z2\n            z[2][1] = z0\n\n        z = ABY3PrivateTensor(prot, z, x.is_scaled or y.is_scaled, x.share_type)\n        z = prot.truncate(z) if x.is_scaled and y.is_scaled else z\n        return z\n\n\ndef _mul_trunc2_private_private(prot, x, y):\n    """"""\n  Multiplication with the Trunc2 protocol in the ABY3 paper.\n  This is more efficient (in terms of communication rounds)\n  than `mul` in the onlline phase only when pre-computation\n  is left out of consideration.\n  """"""\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n    assert isinstance(y, ABY3PrivateTensor), type(y)\n\n    # If there will not be any truncation, then just call the simple multiplication protocol.\n    if not (x.is_scaled and y.is_scaled):\n        return _mul_private_private(prot, x, y)\n\n    x_shares = x.unwrapped\n    y_shares = y.unwrapped\n    shape = x_shares[0][0].shape\n    amount = prot.fixedpoint_config.precision_fractional\n\n    with tf.name_scope(""mul_trunc2""):\n        # Step 1: Generate a Random Truncation Pair\n        # If TF is smart enough, this part is supposed to be pre-computation.\n        r = prot._gen_random_sharing(shape, share_type=BOOLEAN)\n        r_trunc = r.arith_rshift(amount)\n        r = prot.B2A(r)\n        r_trunc = prot.B2A(r_trunc)\n\n        # Step 2: Compute 3-out-of-3 sharing of (x*y - r)\n        a0, a1, a2 = prot._gen_zero_sharing(x.shape)\n        with tf.device(prot.servers[0].device_name):\n            z0 = (\n                x_shares[0][0] * y_shares[0][0]\n                + x_shares[0][0] * y_shares[0][1]\n                + x_shares[0][1] * y_shares[0][0]\n                + a0\n                - r.shares[0][0]\n            )\n\n        with tf.device(prot.servers[1].device_name):\n            z1 = (\n                x_shares[1][0] * y_shares[1][0]\n                + x_shares[1][0] * y_shares[1][1]\n                + x_shares[1][1] * y_shares[1][0]\n                + a1\n                - r.shares[1][0]\n            )\n\n        with tf.device(prot.servers[2].device_name):\n            z2 = (\n                x_shares[2][0] * y_shares[2][0]\n                + x_shares[2][0] * y_shares[2][1]\n                + x_shares[2][1] * y_shares[2][0]\n                + a2\n                - r.shares[2][0]\n            )\n\n        # Step 3: Reveal (x*y - r) / 2^d\n        # xy_minus_r = z0 + z1 + z2\n        # xy_minus_r_trunc = xy_minus_r.right_shift(amount)\n        # z = ABY3PublicTensor(prot, [xy_minus_r_trunc, xy_minus_r_trunc, xy_minus_r_trunc], True, ARITHMETIC)\n        xy_minus_r_trunc = [None] * 3\n        for i in range(3):\n            with tf.device(prot.servers[i].device_name):\n                xy_minus_r_trunc[i] = z0 + z1 + z2\n                xy_minus_r_trunc[i] = xy_minus_r_trunc[i].right_shift(amount)\n        z = ABY3PublicTensor(prot, xy_minus_r_trunc, True, ARITHMETIC)\n\n        # Step 4: Final addition\n        z = z + r_trunc\n\n        return z\n\n\ndef _matmul_public_private(prot, x, y):\n    assert isinstance(x, ABY3PublicTensor), type(x)\n    assert isinstance(y, ABY3PrivateTensor), type(y)\n\n    x_on_0, x_on_1, x_on_2 = x.unwrapped\n    shares = y.unwrapped\n\n    z = [[None, None], [None, None], [None, None]]\n    with tf.name_scope(""matmul""):\n\n        with tf.device(prot.servers[0].device_name):\n            z[0][0] = x_on_0.matmul(shares[0][0])\n            z[0][1] = x_on_0.matmul(shares[0][1])\n\n        with tf.device(prot.servers[1].device_name):\n            z[1][0] = x_on_1.matmul(shares[1][0])\n            z[1][1] = x_on_1.matmul(shares[1][1])\n\n        with tf.device(prot.servers[2].device_name):\n            z[2][0] = x_on_2.matmul(shares[2][0])\n            z[2][1] = x_on_2.matmul(shares[2][1])\n\n        z = ABY3PrivateTensor(prot, z, x.is_scaled or y.is_scaled, y.share_type)\n        z = prot.truncate(z) if x.is_scaled and y.is_scaled else z\n        return z\n\n\ndef _matmul_private_public(prot, x, y):\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n    assert isinstance(y, ABY3PublicTensor), type(y)\n\n    shares = x.unwrapped\n    y_on_0, y_on_1, y_on_2 = y.unwrapped\n\n    z = [[None, None], [None, None], [None, None]]\n    with tf.name_scope(""matmul""):\n\n        with tf.device(prot.servers[0].device_name):\n            z[0][0] = shares[0][0].matmul(y_on_0)\n            z[0][1] = shares[0][1].matmul(y_on_0)\n\n        with tf.device(prot.servers[1].device_name):\n            z[1][0] = shares[1][0].matmul(y_on_1)\n            z[1][1] = shares[1][1].matmul(y_on_1)\n\n        with tf.device(prot.servers[2].device_name):\n            z[2][0] = shares[2][0].matmul(y_on_2)\n            z[2][1] = shares[2][1].matmul(y_on_2)\n\n        z = ABY3PrivateTensor(prot, z, x.is_scaled or y.is_scaled, x.share_type)\n        z = prot.truncate(z) if x.is_scaled and y.is_scaled else z\n        return z\n\n\ndef _matmul_private_private(prot, x, y):\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n    assert isinstance(y, ABY3PrivateTensor), type(y)\n\n    x_shares = x.unwrapped\n    y_shares = y.unwrapped\n\n    # Tensorflow supports matmul for more than 2 dimensions,\n    # with the inner-most 2 dimensions specifying the 2-D matrix multiplication\n    result_shape = tf.TensorShape((*x.shape[:-1], y.shape[-1]))\n\n    z = [[None, None], [None, None], [None, None]]\n    with tf.name_scope(""matmul""):\n        a0, a1, a2 = prot._gen_zero_sharing(result_shape)\n\n        with tf.device(prot.servers[0].device_name):\n            z0 = (\n                x_shares[0][0].matmul(y_shares[0][0])\n                + x_shares[0][0].matmul(y_shares[0][1])\n                + x_shares[0][1].matmul(y_shares[0][0])\n                + a0\n            )\n\n        with tf.device(prot.servers[1].device_name):\n            z1 = (\n                x_shares[1][0].matmul(y_shares[1][0])\n                + x_shares[1][0].matmul(y_shares[1][1])\n                + x_shares[1][1].matmul(y_shares[1][0])\n                + a1\n            )\n\n        with tf.device(prot.servers[2].device_name):\n            z2 = (\n                x_shares[2][0].matmul(y_shares[2][0])\n                + x_shares[2][0].matmul(y_shares[2][1])\n                + x_shares[2][1].matmul(y_shares[2][0])\n                + a2\n            )\n        # Re-sharing\n        with tf.device(prot.servers[0].device_name):\n            z[0][0] = z0\n            z[0][1] = z1\n        with tf.device(prot.servers[1].device_name):\n            z[1][0] = z1\n            z[1][1] = z2\n        with tf.device(prot.servers[2].device_name):\n            z[2][0] = z2\n            z[2][1] = z0\n\n        z = ABY3PrivateTensor(prot, z, x.is_scaled or y.is_scaled, x.share_type)\n        z = prot.truncate(z) if x.is_scaled and y.is_scaled else z\n        return z\n\n\ndef _truncate_private(prot: ABY3, x: ABY3PrivateTensor) -> ABY3PrivateTensor:\n    assert isinstance(x, ABY3PrivateTensor)\n\n    if prot.fixedpoint_config.use_noninteractive_truncation:\n        return _truncate_private_noninteractive(prot, x)\n\n    return _truncate_private_interactive(prot, x)\n\n\ndef _truncate_private_noninteractive(\n    prot: ABY3, x: ABY3PrivateTensor,\n) -> ABY3PrivateTensor:\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n\n    base = prot.fixedpoint_config.scaling_base\n    amount = prot.fixedpoint_config.precision_fractional\n    shares = x.unwrapped\n\n    y = [[None, None], [None, None], [None, None]]\n    with tf.name_scope(""truncate""):\n\n        # First step: compute new shares\n\n        with tf.device(prot.servers[2].device_name):\n            r_on_2 = prot.int_factory.sample_seeded_uniform(\n                shares[2][0].shape, prot.pairwise_keys[2][0] + prot.pairwise_nonces[1]\n            )\n\n        with tf.device(prot.servers[0].device_name):\n            y0 = shares[0][0].truncate(amount, base)\n\n        with tf.device(prot.servers[1].device_name):\n            r_on_1 = prot.int_factory.sample_seeded_uniform(\n                shares[1][0].shape, prot.pairwise_keys[1][1] + prot.pairwise_nonces[1]\n            )\n            t = shares[1][0] + shares[1][1]\n            # tmp = 0 - (0 - t).truncate(amount, base)\n            tmp = t.truncate(amount, base)\n            y1 = tmp - r_on_1\n\n        prot.pairwise_nonces[1] = prot.pairwise_nonces[1] + 1\n\n        # Second step: replicate shares\n\n        with tf.device(prot.servers[0].device_name):\n            y[0][0] = y0\n            y[0][1] = y1\n        with tf.device(prot.servers[1].device_name):\n            y[1][0] = y1\n            y[1][1] = r_on_1\n        with tf.device(prot.servers[2].device_name):\n            y[2][0] = r_on_2\n            y[2][1] = y0\n\n    return ABY3PrivateTensor(prot, y, x.is_scaled, x.share_type)\n\n\ndef _truncate_private_interactive(\n    prot: ABY3, a: ABY3PrivateTensor\n) -> ABY3PrivateTensor:\n    """"""\n  See protocol TruncPr (3.1) in\n    ""Secure Computation With Fixed-Point Numbers"" by Octavian Catrina and Amitabh\n    Saxena, FC\'10.\n\n  We call it ""interactive"" to keep consistent with the 2pc setting,\n  but in fact, our protocol uses only one round communication, exactly the same as\n  that in the ""non-interactive"" one.\n  """"""\n    assert isinstance(a, ABY3PrivateTensor), type(a)\n\n    with tf.name_scope(""truncate-i""):\n        scaling_factor = prot.fixedpoint_config.scaling_factor\n        scaling_factor_inverse = inverse(\n            prot.fixedpoint_config.scaling_factor, prot.int_factory.modulus\n        )\n\n        # we first rotate `a` to make sure reconstructed values fall into\n        # a non-negative interval `[0, 2B)` for some bound B; this uses an\n        # assumption that the values originally lie in `[-B, B)`, and will\n        # leak private information otherwise\n\n        # \'a + bound\' will automatically lift \'bound\' by another scaling factor,\n        # so we should first divide bound by the scaling factor if we want to\n        # use this convenient \'+\' operation.\n        bound = prot.fixedpoint_config.bound_double_precision\n        b = a + (bound / scaling_factor)\n\n        # next step is for servers to add a statistical mask to `b`, reveal\n        # it to server1 and server2, and compute the lower part\n        trunc_gap = prot.fixedpoint_config.truncation_gap\n        mask_bitlength = ceil(log2(bound)) + 2 + trunc_gap\n\n        b_shares = b.unwrapped\n        a_shares = a.unwrapped\n        shape = a.shape\n\n        # NOTE: The following algorithm has an assumption to ensure the correctness:\n        # c = a + bound + r0 + r1  SHOULD be positively smaller than\n        # the max int64 number 2^{63} - 1. This is necessary to ensure the correctness of\n        # the modulo operation \'c % scaling_factor\'.\n        # As a simple example, consider a 4-bit number \'1111\', when we think of it as a signed\n        # number, it is \'-1\', and \'-1 % 3 = 2\'. But when we think of it as an unsigned number,\n        # then \'15 % 3 = 0\'. AND the following works only if c is a positive number that is within\n        # 63-bit, because 64-bit becomes a negative number.\n        # Therefore, \'mask_bitlength\' is better <= 61 if we use int64 as the underlying type, because\n        # r0 is 61-bit, r1 is 61-bit, bound is much smaller, and (assuming) a is much smaller than bound.\n\n        d = [[None] * 2 for _ in range(3)]\n        with tf.device(prot.servers[0].device_name):\n            r0_on_0 = prot.int_factory.sample_seeded_bounded(\n                shape,\n                prot.pairwise_keys[0][0] + prot.pairwise_nonces[2],\n                mask_bitlength,\n            )\n            r1_on_0 = prot.int_factory.sample_seeded_bounded(\n                shape,\n                prot.pairwise_keys[0][1] + prot.pairwise_nonces[0],\n                mask_bitlength,\n            )\n            c0_on_0 = b_shares[0][0] + r0_on_0\n            c1_on_0 = b_shares[0][1] + r1_on_0\n\n            r0_lower_on_0 = r0_on_0 % scaling_factor\n            r1_lower_on_0 = r1_on_0 % scaling_factor\n\n            a_lower0_on_0 = -r0_lower_on_0\n            a_lower1_on_0 = -r1_lower_on_0\n\n            d[0][0] = (a_shares[0][0] - a_lower0_on_0) * scaling_factor_inverse\n            d[0][1] = (a_shares[0][1] - a_lower1_on_0) * scaling_factor_inverse\n\n        with tf.device(prot.servers[1].device_name):\n            r1_on_1 = prot.int_factory.sample_seeded_bounded(\n                shape,\n                prot.pairwise_keys[1][0] + prot.pairwise_nonces[0],\n                mask_bitlength,\n            )\n            c1_on_1 = b_shares[1][0] + r1_on_1\n            c2_on_1 = b_shares[1][1]\n\n            # server0 sends c0 to server1, revealing c to server1\n            c_on_1 = c0_on_0 + c1_on_1 + c2_on_1\n\n            r1_lower_on_1 = r1_on_1 % scaling_factor\n\n            a_lower1_on_1 = -r1_lower_on_1\n            a_lower2_on_1 = c_on_1 % scaling_factor\n\n            d[1][0] = (a_shares[1][0] - a_lower1_on_1) * scaling_factor_inverse\n            d[1][1] = (a_shares[1][1] - a_lower2_on_1) * scaling_factor_inverse\n\n        with tf.device(prot.servers[2].device_name):\n            r0_on_2 = prot.int_factory.sample_seeded_bounded(\n                shape,\n                prot.pairwise_keys[2][1] + prot.pairwise_nonces[2],\n                mask_bitlength,\n            )\n            c0_on_2 = b_shares[2][1] + r0_on_2\n            c2_on_2 = b_shares[2][0]\n\n            # server1 sends c1 to server2, revealing c to server2\n            c_on_2 = c0_on_2 + c1_on_1 + c2_on_2\n\n            r0_lower_on_2 = r0_on_2 % scaling_factor\n\n            a_lower0_on_2 = -r0_lower_on_2\n            a_lower2_on_2 = c_on_2 % scaling_factor\n\n            d[2][0] = (a_shares[2][0] - a_lower2_on_2) * scaling_factor_inverse\n            d[2][1] = (a_shares[2][1] - a_lower0_on_2) * scaling_factor_inverse\n\n        prot.pairwise_nonces[0] += 1\n        prot.pairwise_nonces[2] += 1\n\n    return ABY3PrivateTensor(prot, d, a.is_scaled, a.share_type)\n\n\ndef _B_xor_private_private(prot: ABY3, x: ABY3PrivateTensor, y: ABY3PrivateTensor):\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n    assert isinstance(y, ABY3PrivateTensor), type(y)\n    assert x.backing_dtype == y.backing_dtype\n\n    z = [[None, None], [None, None], [None, None]]\n    with tf.name_scope(""b_xor""):\n\n        with tf.device(prot.servers[0].device_name):\n            z[0][0] = x.shares[0][0] ^ y.shares[0][0]\n            z[0][1] = x.shares[0][1] ^ y.shares[0][1]\n\n        with tf.device(prot.servers[1].device_name):\n            z[1][0] = x.shares[1][0] ^ y.shares[1][0]\n            z[1][1] = x.shares[1][1] ^ y.shares[1][1]\n\n        with tf.device(prot.servers[2].device_name):\n            z[2][0] = x.shares[2][0] ^ y.shares[2][0]\n            z[2][1] = x.shares[2][1] ^ y.shares[2][1]\n\n    return ABY3PrivateTensor(prot, z, x.is_scaled, x.share_type)\n\n\ndef _B_xor_private_public(prot: ABY3, x: ABY3PrivateTensor, y: ABY3PublicTensor):\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n    assert isinstance(y, ABY3PublicTensor), type(y)\n    assert x.backing_dtype == y.backing_dtype\n\n    z = [[None, None], [None, None], [None, None]]\n    with tf.name_scope(""b_xor""):\n        y_on_0, y_on_1, y_on_2 = y.unwrapped\n        with tf.device(prot.servers[0].device_name):\n            z[0][0] = x.shares[0][0] ^ y_on_0\n            z[0][1] = x.shares[0][1] ^ y_on_0\n\n        with tf.device(prot.servers[1].device_name):\n            z[1][0] = x.shares[1][0] ^ y_on_1\n            z[1][1] = x.shares[1][1] ^ y_on_1\n\n        with tf.device(prot.servers[2].device_name):\n            z[2][0] = x.shares[2][0] ^ y_on_2\n            z[2][1] = x.shares[2][1] ^ y_on_2\n\n    return ABY3PrivateTensor(prot, z, x.is_scaled, x.share_type)\n\n\ndef _B_and_private_private(prot: ABY3, x: ABY3PrivateTensor, y: ABY3PrivateTensor):\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n    assert isinstance(y, ABY3PrivateTensor), type(y)\n    assert x.backing_dtype == y.backing_dtype\n\n    x_shares = x.unwrapped\n    y_shares = y.unwrapped\n\n    z = [[None, None], [None, None], [None, None]]\n    with tf.name_scope(""b_and""):\n        a0, a1, a2 = prot._gen_zero_sharing(\n            x.shape, share_type=BOOLEAN, factory=x.backing_dtype\n        )\n\n        with tf.device(prot.servers[0].device_name):\n            tmp0 = x_shares[0][0] & y_shares[0][0]\n            tmp1 = x_shares[0][0] & y_shares[0][1]\n            tmp2 = x_shares[0][1] & y_shares[0][0]\n            z0 = tmp0 ^ tmp1 ^ tmp2 ^ a0\n\n        with tf.device(prot.servers[1].device_name):\n            tmp0 = x_shares[1][0] & y_shares[1][0]\n            tmp1 = x_shares[1][0] & y_shares[1][1]\n            tmp2 = x_shares[1][1] & y_shares[1][0]\n            z1 = tmp0 ^ tmp1 ^ tmp2 ^ a1\n\n        with tf.device(prot.servers[2].device_name):\n            tmp0 = x_shares[2][0] & y_shares[2][0]\n            tmp1 = x_shares[2][0] & y_shares[2][1]\n            tmp2 = x_shares[2][1] & y_shares[2][0]\n            z2 = tmp0 ^ tmp1 ^ tmp2 ^ a2\n\n        # Re-sharing\n        with tf.device(prot.servers[0].device_name):\n            z[0][0] = z0\n            z[0][1] = z1\n        with tf.device(prot.servers[1].device_name):\n            z[1][0] = z1\n            z[1][1] = z2\n        with tf.device(prot.servers[2].device_name):\n            z[2][0] = z2\n            z[2][1] = z0\n\n        z = ABY3PrivateTensor(prot, z, x.is_scaled or y.is_scaled, x.share_type)\n        return z\n\n\ndef _B_and_private_public(prot, x, y):\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n    assert isinstance(y, ABY3PublicTensor), type(x)\n    assert x.backing_dtype == y.backing_dtype\n\n    x_shares = x.unwrapped\n    y_on_0, y_on_1, y_on_2 = y.unwrapped\n\n    z = [[None, None], [None, None], [None, None]]\n    with tf.name_scope(""B_and""):\n        with tf.device(prot.servers[0].device_name):\n            z[0][0] = x_shares[0][0] & y_on_0\n            z[0][1] = x_shares[0][1] & y_on_0\n        with tf.device(prot.servers[1].device_name):\n            z[1][0] = x_shares[1][0] & y_on_1\n            z[1][1] = x_shares[1][1] & y_on_1\n        with tf.device(prot.servers[2].device_name):\n            z[2][0] = x_shares[2][0] & y_on_2\n            z[2][1] = x_shares[2][1] & y_on_2\n\n    z = ABY3PrivateTensor(prot, z, x.is_scaled, x.share_type)\n    return z\n\n\ndef _B_and_public_private(prot, x, y):\n    assert isinstance(x, ABY3PublicTensor), type(x)\n    assert isinstance(y, ABY3PrivateTensor), type(y)\n    assert x.backing_dtype == y.backing_dtype\n\n    x_on_0, x_on_1, x_on_2 = x.unwrapped\n    y_shares = y.unwrapped\n\n    z = [[None, None], [None, None], [None, None]]\n    with tf.name_scope(""B_and""):\n        with tf.device(prot.servers[0].device_name):\n            z[0][0] = x_on_0 & y_shares[0][0]\n            z[0][1] = x_on_0 & y_shares[0][1]\n        with tf.device(prot.servers[1].device_name):\n            z[1][0] = x_on_1 & y_shares[1][0]\n            z[1][1] = x_on_1 & y_shares[1][1]\n        with tf.device(prot.servers[2].device_name):\n            z[2][0] = x_on_2 & y_shares[2][0]\n            z[2][1] = x_on_2 & y_shares[2][1]\n\n    z = ABY3PrivateTensor(prot, z, y.is_scaled, y.share_type)\n    return z\n\n\ndef _B_or_private_private(prot, x, y):\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n    assert isinstance(y, ABY3PrivateTensor), type(y)\n\n    with tf.name_scope(""B_or""):\n        z = (x ^ y) ^ (x & y)\n\n    return z\n\n\ndef _B_not_private(prot, x):\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n\n    x_shares = x.unwrapped\n\n    z = [[None, None], [None, None], [None, None]]\n    with tf.name_scope(""B_not""):\n        with tf.device(prot.servers[0].device_name):\n            # We use the `~` operator instead of XORing a constant, because we want it to work for both\n            # the int_factory and the bool_factory\n            z[0][0] = ~x_shares[0][0]\n            z[0][1] = x_shares[0][1]\n        with tf.device(prot.servers[1].device_name):\n            z[1][0] = x_shares[1][0]\n            z[1][1] = x_shares[1][1]\n        with tf.device(prot.servers[2].device_name):\n            z[2][0] = x_shares[2][0]\n            z[2][1] = ~x_shares[2][1]\n        z = ABY3PrivateTensor(prot, z, x.is_scaled, x.share_type)\n    return z\n\n\ndef _lshift_private(prot, x, steps):\n    """"""\n  Left shift.\n  """"""\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n\n    x_shares = x.unwrapped\n\n    z = [[None, None], [None, None], [None, None]]\n    with tf.name_scope(""lshift""):\n        for i in range(3):\n            with tf.device(prot.servers[i].device_name):\n                z[i][0] = x_shares[i][0] << steps\n                z[i][1] = x_shares[i][1] << steps\n\n        z = ABY3PrivateTensor(prot, z, x.is_scaled, x.share_type)\n\n    return z\n\n\ndef _rshift_private(prot, x, steps):\n    """"""\n  Arithmetic right shift.\n  """"""\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n\n    x_shares = x.unwrapped\n\n    z = [[None, None], [None, None], [None, None]]\n    with tf.name_scope(""rshift""):\n        for i in range(3):\n            with tf.device(prot.servers[i].device_name):\n                z[i][0] = x_shares[i][0] >> steps\n                z[i][1] = x_shares[i][1] >> steps\n\n        z = ABY3PrivateTensor(prot, z, x.is_scaled, x.share_type)\n\n    return z\n\n\ndef _logical_rshift_private(prot, x, steps):\n    """"""\n  Logical right shift.\n  """"""\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n\n    x_shares = x.unwrapped\n\n    z = [[None, None], [None, None], [None, None]]\n    with tf.name_scope(""logical-rshift""):\n        for i in range(3):\n            with tf.device(prot.servers[i].device_name):\n                z[i][0] = x_shares[i][0].logical_rshift(steps)\n                z[i][1] = x_shares[i][1].logical_rshift(steps)\n\n        z = ABY3PrivateTensor(prot, z, x.is_scaled, x.share_type)\n\n    return z\n\n\ndef _B_add_private_private(prot, x, y):\n    raise NotImplementedError(\n        ""Addition with boolean sharing is not implemented, and not recommended.""\n    )\n\n\ndef _B_sub_private_private(prot, x, y):\n    raise NotImplementedError(\n        ""Sbustraction with boolean sharing is not implemented, and not recommended.""\n    )\n\n\ndef _B_ppa_private_private(prot, x, y, n_bits, topology=""kogge_stone""):\n    """"""\n  Parallel prefix adder (PPA). This adder can be used for addition of boolean sharings.\n\n  `n_bits` can be passed as an optimization to constrain the computation for least significant\n  `n_bits` bits.\n\n  AND Depth: log(k)\n  Total gates: klog(k)\n  """"""\n\n    if topology == ""kogge_stone"":\n        return _B_ppa_kogge_stone_private_private(prot, x, y, n_bits)\n    elif topology == ""sklansky"":\n        return _B_ppa_sklansky_private_private(prot, x, y, n_bits)\n    else:\n        raise NotImplementedError(""Unknown adder topology."")\n\n\ndef _B_ppa_sklansky_private_private(prot, x, y, n_bits):\n    """"""\n  Parallel prefix adder (PPA), using the Sklansky adder topology.\n  """"""\n\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n    assert isinstance(y, ABY3PrivateTensor), type(y)\n\n    if x.backing_dtype.native_type != tf.int64:\n        raise NotImplementedError(\n            ""Native type {} not supported"".format(x.backing_dtype.native_type)\n        )\n\n    with tf.name_scope(""B_ppa""):\n        keep_masks = [\n            0x5555555555555555,\n            0x3333333333333333,\n            0x0F0F0F0F0F0F0F0F,\n            0x00FF00FF00FF00FF,\n            0x0000FFFF0000FFFF,\n            0x00000000FFFFFFFF,\n        ]  # yapf: disable\n        copy_masks = [\n            0x5555555555555555,\n            0x2222222222222222,\n            0x0808080808080808,\n            0x0080008000800080,\n            0x0000800000008000,\n            0x0000000080000000,\n        ]  # yapf: disable\n\n        G = x & y\n        P = x ^ y\n\n        k = prot.nbits\n        if n_bits is not None:\n            k = n_bits\n        for i in range(ceil(log2(k))):\n            c_mask = prot.define_constant(\n                np.ones(x.shape, dtype=np.object) * copy_masks[i],\n                apply_scaling=False,\n                share_type=BOOLEAN,\n            )\n            k_mask = prot.define_constant(\n                np.ones(x.shape, dtype=np.object) * keep_masks[i],\n                apply_scaling=False,\n                share_type=BOOLEAN,\n            )\n            # Copy the selected bit to 2^i positions:\n            # For example, when i=2, the 4-th bit is copied to the (5, 6, 7, 8)-th bits\n            G1 = (G & c_mask) << 1\n            P1 = (P & c_mask) << 1\n            for j in range(i):\n                G1 = (G1 << (2 ** j)) ^ G1\n                P1 = (P1 << (2 ** j)) ^ P1\n            """"""\n      Two-round impl. using algo. that assume using OR gate is free, but in fact,\n      here using OR gate cost one round.\n      The PPA operator \'o\' is defined as:\n      (G, P) o (G1, P1) = (G + P*G1, P*P1), where \'+\' is OR, \'*\' is AND\n      """"""\n            # G1 and P1 are 0 for those positions that we do not copy the selected bit to.\n            # Hence for those positions, the result is: (G, P) = (G, P) o (0, 0) = (G, 0).\n            # In order to keep (G, P) for these positions so that they can be used in the future,\n            # we need to let (G1, P1) = (G, P) for these positions, because (G, P) o (G, P) = (G, P)\n            #\n            # G1 = G1 ^ (G & k_mask)\n            # P1 = P1 ^ (P & k_mask)\n            #\n            # G = G | (P & G1)\n            # P = P & P1\n            """"""\n      One-round impl. by modifying the PPA operator \'o\' as:\n      (G, P) o (G1, P1) = (G ^ (P*G1), P*P1), where \'^\' is XOR, \'*\' is AND\n      This is a valid definition: when calculating the carry bit c_i = g_i + p_i * c_{i-1},\n      the OR \'+\' can actually be replaced with XOR \'^\' because we know g_i and p_i will NOT take \'1\'\n      at the same time.\n      And this PPA operator \'o\' is also associative. BUT, it is NOT idempotent: (G, P) o (G, P) != (G, P).\n      This does not matter, because we can do (G, P) o (0, P) = (G, P), or (G, P) o (0, 1) = (G, P)\n      if we want to keep G and P bits.\n      """"""\n            # Option 1: Using (G, P) o (0, P) = (G, P)\n            # P1 = P1 ^ (P & k_mask)\n            # Option 2: Using (G, P) o (0, 1) = (G, P)\n            P1 = P1 ^ k_mask\n\n            G = G ^ (P & G1)\n            P = P & P1\n\n        # G stores the carry-in to the next position\n        C = G << 1\n        P = x ^ y\n        z = C ^ P\n\n    return z\n\n\ndef _B_ppa_kogge_stone_private_private(prot, x, y, n_bits):\n    """"""\n  Parallel prefix adder (PPA), using the Kogge-Stone adder topology.\n  """"""\n\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n    assert isinstance(y, ABY3PrivateTensor), type(y)\n\n    if x.backing_dtype.native_type != tf.int64:\n        raise NotImplementedError(\n            ""Native type {} not supported"".format(x.backing_dtype.native_type)\n        )\n\n    with tf.name_scope(""B_ppa""):\n        keep_masks = []\n        for i in range(ceil(log2(prot.nbits))):\n            keep_masks.append((1 << (2 ** i)) - 1)\n        """"""\n    For example, if prot.nbits = 64, then keep_masks is:\n    keep_masks = [0x0000000000000001, 0x0000000000000003, 0x000000000000000f,\n                  0x00000000000000ff, 0x000000000000ffff, 0x00000000ffffffff]\n    """"""\n\n        G = x & y\n        P = x ^ y\n        k = prot.nbits if n_bits is None else n_bits\n        for i in range(ceil(log2(k))):\n            k_mask = prot.define_constant(\n                np.ones(x.shape, dtype=np.object) * keep_masks[i],\n                apply_scaling=False,\n                share_type=BOOLEAN,\n            )\n\n            G1 = G << (2 ** i)\n            P1 = P << (2 ** i)\n            """"""\n      One-round impl. by modifying the PPA operator \'o\' as:\n      (G, P) o (G1, P1) = (G ^ (P*G1), P*P1), where \'^\' is XOR, \'*\' is AND\n      This is a valid definition: when calculating the carry bit c_i = g_i + p_i * c_{i-1},\n      the OR \'+\' can actually be replaced with XOR \'^\' because we know g_i and p_i will NOT take \'1\'\n      at the same time.\n      And this PPA operator \'o\' is also associative. BUT, it is NOT idempotent: (G, P) o (G, P) != (G, P).\n      This does not matter, because we can do (G, P) o (0, P) = (G, P), or (G, P) o (0, 1) = (G, P)\n      if we want to keep G and P bits.\n      """"""\n            # Option 1: Using (G, P) o (0, P) = (G, P)\n            # P1 = P1 ^ (P & k_mask)\n            # Option 2: Using (G, P) o (0, 1) = (G, P)\n            P1 = P1 ^ k_mask\n\n            G = G ^ (P & G1)\n            P = P & P1\n\n        # G stores the carry-in to the next position\n        C = G << 1\n        P = x ^ y\n        z = C ^ P\n    return z\n\n\ndef _A2B_private(prot, x, nbits):\n    """"""\n  Bit decomposition: Convert an arithmetic sharing to a boolean sharing.\n  """"""\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n    assert x.share_type == ARITHMETIC\n\n    x_shares = x.unwrapped\n    zero = prot.define_constant(\n        np.zeros(x.shape, dtype=np.int64), apply_scaling=False, share_type=BOOLEAN\n    )\n    zero_on_0, zero_on_1, zero_on_2 = zero.unwrapped\n    a0, a1, a2 = prot._gen_zero_sharing(x.shape, share_type=BOOLEAN)\n\n    operand1 = [[None, None], [None, None], [None, None]]\n    operand2 = [[None, None], [None, None], [None, None]]\n    with tf.name_scope(""A2B""):\n        # Step 1: We know x = ((x0, x1), (x1, x2), (x2, x0))\n        # We need to reshare it into two operands that will be fed into an addition circuit:\n        # operand1 = (((x0+x1) XOR a0, a1), (a1, a2), (a2, (x0+x1) XOR a0)), meaning boolean sharing of x0+x1\n        # operand2 = ((0, 0), (0, x2), (x2, 0)), meaning boolean sharing of x2\n        with tf.device(prot.servers[0].device_name):\n            x0_plus_x1 = x_shares[0][0] + x_shares[0][1]\n            operand1[0][0] = x0_plus_x1 ^ a0\n            operand1[0][1] = a1\n\n            operand2[0][0] = zero_on_0\n            operand2[0][1] = zero_on_0\n\n        with tf.device(prot.servers[1].device_name):\n            operand1[1][0] = a1\n            operand1[1][1] = a2\n\n            operand2[1][0] = zero_on_1\n            operand2[1][1] = x_shares[1][1]\n\n        with tf.device(prot.servers[2].device_name):\n            operand1[2][0] = a2\n            operand1[2][1] = operand1[0][0]\n\n            operand2[2][0] = x_shares[2][0]\n            operand2[2][1] = zero_on_2\n\n        operand1 = ABY3PrivateTensor(prot, operand1, x.is_scaled, BOOLEAN)\n        operand2 = ABY3PrivateTensor(prot, operand2, x.is_scaled, BOOLEAN)\n\n        # Step 2: Parallel prefix adder that requires log(k) rounds of communication\n        result = prot.B_ppa(operand1, operand2, nbits)\n\n    return result\n\n\ndef _bit_extract_private(prot, x, i):\n    """"""\n  Bit extraction: Extracts the `i`-th bit of an arithmetic sharing or boolean sharing\n  to a single-bit boolean sharing.\n  """"""\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n    assert x.backing_dtype == prot.int_factory\n\n    with tf.name_scope(""bit_extract""):\n        if x.share_type == ARITHMETIC:\n            with tf.name_scope(""A2B_partial""):\n                x_shares = x.unwrapped\n                zero = prot.define_constant(\n                    np.zeros(x.shape, dtype=np.int64),\n                    apply_scaling=False,\n                    share_type=BOOLEAN,\n                )\n                zero_on_0, zero_on_1, zero_on_2 = zero.unwrapped\n                a0, a1, a2 = prot._gen_zero_sharing(x.shape, share_type=BOOLEAN)\n\n                operand1 = [[None, None], [None, None], [None, None]]\n                operand2 = [[None, None], [None, None], [None, None]]\n                # Step 1: We know x = ((x0, x1), (x1, x2), (x2, x0))\n                # We need to reshare it into two operands that will be fed into an addition circuit:\n                # operand1 = (((x0+x1) XOR a0, a1), (a1, a2), (a2, (x0+x1) XOR a0)), meaning boolean sharing of x0+x1\n                # operand2 = ((0, 0), (0, x2), (x2, 0)), meaning boolean sharing of x2\n                with tf.device(prot.servers[0].device_name):\n                    x0_plus_x1 = x_shares[0][0] + x_shares[0][1]\n                    operand1[0][0] = x0_plus_x1 ^ a0\n                    operand1[0][1] = a1\n\n                    operand2[0][0] = zero_on_0\n                    operand2[0][1] = zero_on_0\n\n                with tf.device(prot.servers[1].device_name):\n                    operand1[1][0] = a1\n                    operand1[1][1] = a2\n\n                    operand2[1][0] = zero_on_1\n                    operand2[1][1] = x_shares[1][1]\n\n                with tf.device(prot.servers[2].device_name):\n                    operand1[2][0] = a2\n                    operand1[2][1] = operand1[0][0]\n\n                    operand2[2][0] = x_shares[2][0]\n                    operand2[2][1] = zero_on_2\n\n                operand1 = ABY3PrivateTensor(prot, operand1, x.is_scaled, BOOLEAN)\n                operand2 = ABY3PrivateTensor(prot, operand2, x.is_scaled, BOOLEAN)\n\n                # Step 2: Parallel prefix adder that requires log(i+1) rounds of communication\n                x = prot.B_ppa(operand1, operand2, i + 1)\n\n        # Take out the i-th bit\n        #\n        # NOTE: Don\'t use x = x & 0x1. Even though we support automatic lifting of 0x1\n        # to an ABY3Tensor, but it also includes automatic scaling to make the two operands have\n        # the same scale, which is not what want here.\n        #\n        mask = prot.define_constant(\n            np.array([0x1 << i]), apply_scaling=False, share_type=BOOLEAN\n        )\n        x = x & mask\n\n        x_shares = x.unwrapped\n        result = [[None, None], [None, None], [None, None]]\n        for i in range(3):\n            with tf.device(prot.servers[i].device_name):\n                result[i][0] = x_shares[i][0].cast(prot.bool_factory)\n                result[i][1] = x_shares[i][1].cast(prot.bool_factory)\n        result = ABY3PrivateTensor(prot, result, False, BOOLEAN)\n\n    return result\n\n\ndef _B2A_private(prot, x, nbits):\n    """"""\n  Bit composition: Convert a boolean sharing to an arithmetic sharing.\n  """"""\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n    assert x.share_type == BOOLEAN\n\n    # In semi-honest, the following two calls can be further optimized because we don\'t\n    # need the boolean shares of x1 and x2. We only need their original values on intended servers.\n    x1_on_0, x1_on_1, x1_on_2, x1_shares = prot._gen_b2a_sharing(\n        x.shape, prot.b2a_keys_1\n    )\n    assert x1_on_2 is None\n    x2_on_0, x2_on_1, x2_on_2, x2_shares = prot._gen_b2a_sharing(\n        x.shape, prot.b2a_keys_2\n    )\n    assert x2_on_0 is None\n\n    a0, a1, a2 = prot._gen_zero_sharing(x.shape, share_type=BOOLEAN)\n\n    with tf.name_scope(""B2A""):\n        # Server 1 reshares (-x1-x2) as private input\n        neg_x1_neg_x2 = [[None, None], [None, None], [None, None]]\n        with tf.device(prot.servers[1].device_name):\n            value = -x1_on_1 - x2_on_1\n            neg_x1_neg_x2[1][0] = value ^ a1\n            neg_x1_neg_x2[1][1] = a2\n        with tf.device(prot.servers[0].device_name):\n            neg_x1_neg_x2[0][0] = a0\n            neg_x1_neg_x2[0][1] = neg_x1_neg_x2[1][0]\n        with tf.device(prot.servers[2].device_name):\n            neg_x1_neg_x2[2][0] = a2\n            neg_x1_neg_x2[2][1] = a0\n        neg_x1_neg_x2 = ABY3PrivateTensor(prot, neg_x1_neg_x2, x.is_scaled, BOOLEAN)\n\n        # Compute x0 = x + (-x1-x2) using the parallel prefix adder\n        x0 = prot.B_ppa(x, neg_x1_neg_x2, nbits)\n\n        # Reveal x0 to server 0 and 2\n        with tf.device(prot.servers[0].device_name):\n            x0_on_0 = prot._reconstruct(x0.unwrapped, prot.servers[0], BOOLEAN)\n        with tf.device(prot.servers[2].device_name):\n            x0_on_2 = prot._reconstruct(x0.unwrapped, prot.servers[2], BOOLEAN)\n\n        # Construct the arithmetic sharing\n        result = [[None, None], [None, None], [None, None]]\n        with tf.device(prot.servers[0].device_name):\n            result[0][0] = x0_on_0\n            result[0][1] = x1_on_0\n        with tf.device(prot.servers[1].device_name):\n            result[1][0] = x1_on_1\n            result[1][1] = x2_on_1\n        with tf.device(prot.servers[2].device_name):\n            result[2][0] = x2_on_2\n            result[2][1] = x0_on_2\n        result = ABY3PrivateTensor(prot, result, x.is_scaled, ARITHMETIC)\n\n    return result\n\n\ndef _mul_AB_public_private(prot, x, y):\n    assert isinstance(x, ABY3PublicTensor), type(x)\n    assert isinstance(y, ABY3PrivateTensor), type(x)\n    assert x.share_type == ARITHMETIC\n    assert y.share_type == BOOLEAN\n\n    x_on_0, x_on_1, x_on_2 = x.unwrapped\n\n    with tf.name_scope(""mul_AB""):\n        z = __mul_AB_routine(prot, x_on_2, y, 2)\n        z = ABY3PrivateTensor(prot, z, x.is_scaled, ARITHMETIC)\n\n    return z\n\n\ndef _mul_AB_private_private(prot, x, y):\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n    assert isinstance(y, ABY3PrivateTensor), type(y)\n    assert x.share_type == ARITHMETIC\n    assert y.share_type == BOOLEAN\n\n    x_shares = x.unwrapped\n\n    with tf.name_scope(""mul_AB""):\n        with tf.name_scope(""term0""):\n            w = __mul_AB_routine(prot, x_shares[0][0], y, 0)\n            w = ABY3PrivateTensor(prot, w, x.is_scaled, ARITHMETIC)\n\n        with tf.name_scope(""term1""):\n            with tf.device(prot.servers[1].device_name):\n                a = x_shares[1][0] + x_shares[1][1]\n            z = __mul_AB_routine(prot, a, y, 1)\n            z = ABY3PrivateTensor(prot, z, x.is_scaled, ARITHMETIC)\n        z = w + z\n\n    return z\n\n\ndef __mul_AB_routine(prot, a, b, sender_idx):\n    """"""\n    A sub routine for multiplying a value \'a\' (located at servers[sender_idx]) with a boolean sharing \'b\'.\n    """"""\n    assert isinstance(a, AbstractTensor), type(a)\n    assert isinstance(b, ABY3PrivateTensor), type(b)\n\n    with tf.name_scope(""__mul_AB_routine""):\n        b_shares = b.unwrapped\n        s = [None, None, None]\n        s[0], s[1], s[2] = prot._gen_zero_sharing(a.shape, ARITHMETIC)\n\n        z = [[None, None], [None, None], [None, None]]\n        idx0 = sender_idx\n        idx1 = (sender_idx + 1) % 3\n        idx2 = (sender_idx + 2) % 3\n        with tf.device(prot.servers[idx0].device_name):\n            z[idx0][0] = s[idx2]\n            z[idx0][1] = s[idx1]\n            tmp = (b_shares[idx0][0] ^ b_shares[idx0][1]).cast(a.factory) * a\n            m0 = tmp + s[idx0]\n            m1 = -tmp + a + s[idx0]\n\n        with tf.device(prot.servers[idx1].device_name):\n            z[idx1][0] = s[idx1]\n            z[idx1][1] = prot._ot(\n                prot.servers[idx0],\n                prot.servers[idx1],\n                prot.servers[idx2],\n                m0,\n                m1,\n                b_shares[idx1][1],\n                b_shares[idx2][0],\n                prot.pairwise_keys[idx0][0],\n                prot.pairwise_keys[idx2][1],\n                prot.pairwise_nonces[idx2],\n            )\n            prot.pairwise_nonces[idx2] = prot.pairwise_nonces[idx2] + 1\n\n        with tf.device(prot.servers[idx2].device_name):\n            z[idx2][0] = prot._ot(\n                prot.servers[idx0],\n                prot.servers[idx2],\n                prot.servers[idx1],\n                m0,\n                m1,\n                b_shares[idx2][0],\n                b_shares[idx1][1],\n                prot.pairwise_keys[idx0][1],\n                prot.pairwise_keys[idx1][0],\n                prot.pairwise_nonces[idx0],\n            )\n            z[idx2][1] = s[idx2]\n            prot.pairwise_nonces[idx0] = prot.pairwise_nonces[idx0] + 1\n\n    return z\n\n\ndef _pow_private(prot, x, p):\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n    assert x.share_type == ARITHMETIC\n    assert p >= 1, ""Exponent should be >= 0""\n\n    # NOTE: pow should be able to use the `memoir` memoization\n\n    with tf.name_scope(""pow""):\n        result = 1\n        tmp = x\n        while p > 0:\n            bit = p & 0x1\n            if bit > 0:\n                result = result * tmp\n            p >>= 1\n            if p > 0:\n                tmp = tmp * tmp\n    return result\n\n\ndef _polynomial_private(prot, x, coeffs):\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n    assert x.share_type == ARITHMETIC\n\n    with tf.name_scope(""polynomial""):\n        result = prot.define_constant(np.zeros(x.shape), apply_scaling=x.is_scaled)\n        for i in range(len(coeffs)):\n            if i == 0:\n                result = result + coeffs[i]\n            elif coeffs[i] == 0:\n                continue\n            elif (coeffs[i] - int(coeffs[i])) == 0:\n                # Optimization when coefficient is integer: mulitplication can be performed\n                # locally without interactive truncation\n                tmp = prot.define_constant(np.array([coeffs[i]]), apply_scaling=False)\n                tmp = tmp * (x ** i)\n                result = result + tmp\n            else:\n                tmp = coeffs[i] * (x ** i)\n                result = result + tmp\n    return result\n\n\ndef _polynomial_piecewise_private(prot, x, c, coeffs):\n    """"""\n  :param prot:\n  :param x:\n  :param c: A list of splitting points between pieces\n  :param coeffs: Two-dimensional list: 1st dimension is the polynomial index, 2nd dimension is the coefficient index\n  :return:\n  """"""\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n    assert len(c) + 1 == len(coeffs), ""# of pieces do not match # of polynomials""\n\n    with tf.name_scope(""polynomial_piecewise""):\n        # Compute the selection bit for each polynomial\n        with tf.name_scope(""polynomial-selection-bit""):\n            msbs = [None] * len(c)\n            for i in range(len(c)):\n                msbs[i] = prot.msb(x - c[i])\n            b = [None] * len(coeffs)\n            b[0] = msbs[0]\n            for i in range(len(c) - 1):\n                b[i + 1] = ~msbs[i] & msbs[i + 1]\n            b[len(c)] = ~msbs[len(c) - 1]\n\n        # Compute the piecewise combination result\n        result = 0\n        for i in range(len(coeffs)):\n            fi = prot.polynomial(x, coeffs[i])\n            result = result + prot.mul_AB(fi, b[i])\n    return result\n\n\ndef _sigmoid_private(prot, x, approx_type):\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n\n    with tf.name_scope(""sigmoid""):\n        if approx_type == ""piecewise_linear"":\n            c = (-2.5, 2.5)\n            coeffs = ((1e-4,), (0.50, 0.17), (1 - 1e-4,))\n        else:\n            raise NotImplementedError(\n                ""Only support piecewise linear approximation of sigmoid.""\n            )\n\n        result = prot.polynomial_piecewise(x, c, coeffs)\n    return result\n\n\n#\n# transpose helpers\n#\n\n\ndef _transpose_private(prot, x, perm=None):\n    assert isinstance(x, ABY3PrivateTensor)\n\n    x_shares = x.unwrapped\n\n    with tf.name_scope(""transpose""):\n        z = [[None, None], [None, None], [None, None]]\n        for i in range(3):\n            with tf.device(prot.servers[i].device_name):\n                z[i][0] = x_shares[i][0].transpose(perm=perm)\n                z[i][1] = x_shares[i][1].transpose(perm=perm)\n\n        return ABY3PrivateTensor(prot, z, x.is_scaled, x.share_type)\n\n\ndef _transpose_public(prot, x, perm=None):\n    assert isinstance(x, ABY3PublicTensor)\n\n    x_on_0, x_on_1, x_on_2 = x.unwrapped\n\n    with tf.name_scope(""transpose""):\n\n        with tf.device(prot.servers[0].device_name):\n            x_on_0_t = x_on_0.transpose(perm=perm)\n\n        with tf.device(prot.servers[1].device_name):\n            x_on_1_t = x_on_1.transpose(perm=perm)\n\n        with tf.device(prot.servers[2].device_name):\n            x_on_2_t = x_on_2.transpose(perm=perm)\n\n        return ABY3PublicTensor(\n            prot, [x_on_0_t, x_on_1_t, x_on_2_t], x.is_scaled, x.share_type\n        )\n\n\n#\n# reduce_sum helpers\n#\n\n\ndef _reduce_sum_public(prot, x, axis=None, keepdims=False):\n\n    x_on_0, x_on_1, x_on_2 = x.unwrapped\n\n    with tf.name_scope(""reduce_sum""):\n\n        with tf.device(prot.servers[0].device_name):\n            y_on_0 = x_on_0.reduce_sum(axis, keepdims)\n\n        with tf.device(prot.servers[1].device_name):\n            y_on_1 = x_on_1.reduce_sum(axis, keepdims)\n\n        with tf.device(prot.servers[2].device_name):\n            y_on_2 = x_on_2.reduce_sum(axis, keepdims)\n\n    return ABY3PublicTensor(prot, [y_on_0, y_on_1, y_on_2], x.is_scaled, x.share_type)\n\n\ndef _reduce_sum_private(prot, x, axis=None, keepdims=False):\n\n    x_shares = x.unwrapped\n\n    with tf.name_scope(""reduce_sum""):\n        z = [[None, None], [None, None], [None, None]]\n        for i in range(3):\n            with tf.device(prot.servers[i].device_name):\n                z[i][0] = x_shares[i][0].reduce_sum(axis, keepdims)\n                z[i][1] = x_shares[i][1].reduce_sum(axis, keepdims)\n    return ABY3PrivateTensor(prot, z, x.is_scaled, x.share_type)\n\n\n#\n# concat helpers\n#\n\n\ndef _concat_public(prot, xs, axis):\n    assert all(x.is_scaled for x in xs) or all(not x.is_scaled for x in xs)\n\n    factory = xs[0].backing_dtype\n    is_scaled = xs[0].is_scaled\n    xs_on_0, xs_on_1, xs_on_2 = zip(*(x.unwrapped for x in xs))\n\n    with tf.name_scope(""concat""):\n\n        with tf.device(prot.servers[0].device_name):\n            x_on_0_concat = factory.concat(xs_on_0, axis=axis)\n\n        with tf.device(prot.servers[1].device_name):\n            x_on_1_concat = factory.concat(xs_on_1, axis=axis)\n\n        with tf.device(prot.servers[2].device_name):\n            x_on_2_concat = factory.concat(xs_on_2, axis=axis)\n\n        return ABY3PublicTensor(\n            prot,\n            [x_on_0_concat, x_on_1_concat, x_on_2_concat],\n            is_scaled,\n            xs[0].share_type,\n        )\n\n\ndef _concat_private(prot, xs, axis):\n    assert all(x.is_scaled for x in xs) or all(not x.is_scaled for x in xs)\n\n    factory = xs[0].backing_dtype\n    is_scaled = xs[0].is_scaled\n    share_type = xs[0].share_type\n\n    xs_shares = [x.unwrapped for x in xs]\n    z = [[None, None], [None, None], [None, None]]\n    for i in range(3):\n        z[i][0] = [x_shares[i][0] for x_shares in xs_shares]\n        z[i][1] = [x_shares[i][1] for x_shares in xs_shares]\n\n    with tf.name_scope(""concat""):\n\n        for i in range(3):\n            with tf.device(prot.servers[i].device_name):\n                z[i][0] = factory.concat(z[i][0], axis=axis)\n                z[i][1] = factory.concat(z[i][1], axis=axis)\n\n        return ABY3PrivateTensor(prot, z, is_scaled, share_type)\n\n\ndef _write_private(prot, x, filename_prefix):\n    assert isinstance(x, ABY3PrivateTensor), type(x)\n\n    def encode(feature_row):\n        # Converting a row to a string seems to be the only way of writing out\n        # the dataset in a distributed way\n        feature = tf.strings.reduce_join(\n            tf.dtypes.as_string(tf.reshape(feature_row, [-1])), separator="",""\n        )\n        return feature\n\n    x_shares = x.unwrapped\n    ops = []\n    for i in range(3):\n        with tf.device(prot.servers[i].device_name):\n            for j in range(2):\n                data = tf.data.Dataset.from_tensor_slices(x_shares[i][j].value).map(\n                    encode\n                )\n                writer = tf.data.experimental.TFRecordWriter(\n                    ""{}_share{}{}"".format(filename_prefix, i, j)\n                )\n                ops.append(writer.write(data))\n\n    return tf.group(*ops)\n\n\ndef _read_(prot, filename_prefix, batch_size, n_columns):\n\n    row_shape = [n_columns]\n\n    def decode(line):\n        fields = tf.string_split([line], "","").values\n        fields = tf.strings.to_number(fields, tf.int64)\n        fields = tf.reshape(fields, row_shape)\n        return fields\n\n    batch = [[None] * 2 for _ in range(3)]\n    for i in range(3):\n        with tf.device(prot.servers[i].device_name):\n            for j in range(2):\n                data = (\n                    tf.data.TFRecordDataset(\n                        [""{}_share{}{}"".format(filename_prefix, i, j)]\n                    )\n                    .map(decode)\n                    .repeat()\n                    .batch(batch_size=batch_size)\n                )\n                it = data.make_one_shot_iterator()\n                batch[i][j] = it.get_next()\n                batch[i][j] = tf.reshape(batch[i][j], [batch_size] + row_shape)\n                batch[i][j] = prot.int_factory.tensor(batch[i][j])\n\n    return ABY3PrivateTensor(prot, batch, True, ARITHMETIC)\n\n\ndef _iterate_private(\n    prot,\n    tensor: ""ABY3PrivateTensor"",\n    batch_size: int,\n    repeat=True,\n    shuffle=True,\n    seed: int = None,\n):\n\n    assert isinstance(tensor, ABY3PrivateTensor)\n    shares = tensor.unwrapped\n    iterators = [[None] * 2 for _ in range(3)]\n    results = [[None] * 2 for _ in range(3)]\n\n    if seed is None:\n        seed = np.random.randint(1, 1 << 32)  # this seed is publicly known.\n    batch_size = max(1, batch_size)\n\n    def helper(idx):\n        with tf.device(prot.servers[idx].device_name):\n            out_shape = shares[idx][0].value.shape.as_list()\n            out_shape[0] = batch_size\n            for i in range(2):\n                dataset = tf.data.Dataset.from_tensor_slices(shares[idx][i].value)\n\n                if repeat:\n                    dataset = dataset.repeat()\n\n                if shuffle:\n                    dataset = dataset.shuffle(buffer_size=512, seed=seed)\n\n                dataset = dataset.batch(batch_size)\n\n                # NOTE: initializable_iterator needs to run initializer.\n                iterators[idx][i] = tf.compat.v1.data.make_initializable_iterator(\n                    dataset\n                )\n                batch = iterators[idx][i].get_next()\n                # Wrap the tf.tensor as a dense tensor (no extra encoding is needed)\n                results[idx][i] = prot.int_factory.tensor(tf.reshape(batch, out_shape))\n\n            prot._initializers.append(\n                tf.group(iterators[idx][0].initializer, iterators[idx][1].initializer)\n            )\n\n    for idx in range(3):\n        helper(idx)\n\n    # Synchronize the reading of all 6 dataset iterators\n    with tf.control_dependencies(\n        [share.value for result in results for share in result]\n    ):\n        for i in range(3):\n            results[i][0] = results[i][0].identity()\n            results[i][1] = results[i][1].identity()\n\n    return ABY3PrivateTensor(prot, results, tensor.is_scaled, tensor.share_type)\n\n\ndef _indexer_private(prot: ABY3, tensor: ABY3PrivateTensor, slc) -> ""ABY3PrivateTensor"":\n    shares = tensor.unwrapped\n    results = [[None] * 2 for _ in range(3)]\n    with tf.name_scope(""index""):\n        for i in range(3):\n            with tf.device(prot.servers[i].device_name):\n                results[i][0] = shares[i][0][slc]\n                results[i][1] = shares[i][1][slc]\n    return ABY3PrivateTensor(prot, results, tensor.is_scaled, tensor.share_type)\n\n\ndef _reshape_private(prot: ABY3, tensor: ABY3PrivateTensor, axe):\n    shares = tensor.unwrapped\n    results = [[None] * 2 for _ in range(3)]\n    with tf.name_scope(""reshape""):\n        for i in range(3):\n            with tf.device(prot.servers[i].device_name):\n                results[i][0] = shares[i][0].reshape(axe)\n                results[i][1] = shares[i][1].reshape(axe)\n    return ABY3PrivateTensor(prot, results, tensor.is_scaled, tensor.share_type)\n'"
tf_encrypted/protocol/aby3/aby3_test.py,80,"b'# pylint: disable=all\n# pylint: disable=missing-docstring\n# flake8: noqa\n\nimport os\nimport tempfile\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.protocol.aby3 import ABY3\nfrom tf_encrypted.protocol.aby3 import ARITHMETIC\nfrom tf_encrypted.protocol.aby3 import BOOLEAN\n\n\nclass TestABY3(unittest.TestCase):\n    def test_add_private_private(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        def provide_input():\n            return tf.ones(shape=(2, 2)) * 1.3\n\n        # define inputs\n        x = tfe.define_private_variable(tf.ones(shape=(2, 2)))\n        y = tfe.define_private_input(""input-provider"", provide_input)\n\n        # define computation\n        z = x + y\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(z.reveal())\n            # Should be [[2.3, 2.3], [2.3, 2.3]]\n            expected = np.array([[2.3, 2.3], [2.3, 2.3]])\n            np.testing.assert_allclose(result, expected, rtol=0.0, atol=0.01)\n\n    def test_add_private_public(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        # define inputs\n        x = tfe.define_private_variable(tf.ones(shape=(2, 2)))\n        y = tfe.define_constant(np.array([[0.6, 0.7], [0.8, 0.9]]))\n\n        # define computation\n        z = x + y\n        z = y + z\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(z.reveal())\n            expected = np.array([[2.2, 2.4], [2.6, 2.8]])\n            np.testing.assert_allclose(result, expected, rtol=0.0, atol=0.01)\n\n    def test_sub_private_private(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        def provide_input():\n            return tf.ones(shape=(2, 2)) * 1.3\n\n        x = tfe.define_private_variable(tf.ones(shape=(2, 2)))\n        y = tfe.define_private_input(""input-provider"", provide_input)\n\n        z = x - y\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(z.reveal())\n            expected = np.array([[-0.3, -0.3], [-0.3, -0.3]])\n            np.testing.assert_allclose(result, expected, rtol=0.0, atol=0.01)\n\n    def test_sub_private_public(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        # define inputs\n        x = tfe.define_private_variable(tf.ones(shape=(2, 2)))\n        y = tfe.define_constant(np.array([[0.6, 0.7], [0.8, 0.9]]))\n\n        # define computation\n        z1 = x - y\n        z2 = y - x\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(z1.reveal())\n            z1_exp = np.array([[0.4, 0.3], [0.2, 0.1]])\n            np.testing.assert_allclose(result, z1_exp, rtol=0.0, atol=0.01)\n            result = sess.run(z2.reveal())\n            z2_exp = np.array([[-0.4, -0.3], [-0.2, -0.1]])\n            np.testing.assert_allclose(result, z2_exp, rtol=0.0, atol=0.01)\n\n    def test_neg(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        # define inputs\n        x = tfe.define_private_variable(np.array([[0.6, -0.7], [-0.8, 0.9]]))\n        y = tfe.define_constant(np.array([[0.6, -0.7], [-0.8, 0.9]]))\n\n        # define computation\n        z1 = -x\n        z2 = -y\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(z1.reveal())\n            z1_exp = np.array([[-0.6, 0.7], [0.8, -0.9]])\n            np.testing.assert_allclose(result, z1_exp, rtol=0.0, atol=0.01)\n            result = sess.run(z2)\n            z2_exp = np.array([[-0.6, 0.7], [0.8, -0.9]])\n            np.testing.assert_allclose(result, z2_exp, rtol=0.0, atol=0.01)\n\n    def test_mul_private_public(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        # define inputs\n        x = tfe.define_private_variable(tf.ones(shape=(2, 2)) * 2)\n        y = tfe.define_constant(np.array([[0.6, 0.7], [0.8, 0.9]]))\n        w = tfe.define_constant(np.array([[2, 2], [2, 2]]))\n\n        # define computation\n        z1 = y * x  # mul_public_private\n        z2 = z1 * w  # mul_private_public\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(z2.reveal())\n            np.testing.assert_allclose(\n                result, np.array([[2.4, 2.8], [3.2, 3.6]]), rtol=0.0, atol=0.01\n            )\n\n    def test_mul_private_private(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        def provide_input():\n            # normal TensorFlow operations can be run locally\n            # as part of defining a private input, in this\n            # case on the machine of the input provider\n            return tf.ones(shape=(2, 2)) * 1.3\n\n        # define inputs\n        x = tfe.define_private_variable(tf.ones(shape=(2, 2)) * 2)\n        y = tfe.define_private_input(""input-provider"", provide_input)\n\n        # define computation\n        z = y * x\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(z.reveal())\n            np.testing.assert_allclose(\n                result, np.array([[2.6, 2.6], [2.6, 2.6]]), rtol=0.0, atol=0.01\n            )\n\n    def test_matmul_public_private(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        def provide_input():\n            # normal TensorFlow operations can be run locally\n            # as part of defining a private input, in this\n            # case on the machine of the input provider\n            return tf.constant(np.array([[1.1, 1.2], [1.3, 1.4], [1.5, 1.6]]))\n\n        # define inputs\n        x = tfe.define_private_variable(tf.ones(shape=(2, 2)))\n        y = tfe.define_public_input(""input-provider"", provide_input)\n        v = tfe.define_constant(np.ones((2, 2)))\n\n        # define computation\n        w = y.matmul(x)  # matmul_public_private\n        z = w.matmul(v)  # matmul_private_public\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(w.reveal())\n            np.testing.assert_allclose(\n                result,\n                np.array([[2.3, 2.3], [2.7, 2.7], [3.1, 3.1]]),\n                rtol=0.0,\n                atol=0.01,\n            )\n            result = sess.run(z.reveal())\n            np.testing.assert_allclose(\n                result,\n                np.array([[4.6, 4.6], [5.4, 5.4], [6.2, 6.2]]),\n                rtol=0.0,\n                atol=0.01,\n            )\n\n    def test_matmul_private_private(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        # 2-D matrix mult\n        x = tfe.define_private_variable(tf.constant([[1, 2, 3], [4, 5, 6]]))\n        y = tfe.define_private_variable(tf.constant([[7, 8], [9, 10], [11, 12]]))\n\n        z = tfe.matmul(x, y)\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(z.reveal())\n            np.testing.assert_allclose(\n                result, np.array([[58, 64], [139, 154]]), rtol=0.0, atol=0.01\n            )\n\n    def test_3d_matmul_private(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        # 3-D matrix mult\n        x = tfe.define_private_variable(tf.constant(np.arange(1, 13), shape=[2, 2, 3]))\n        y = tfe.define_private_variable(tf.constant(np.arange(13, 25), shape=[2, 3, 2]))\n\n        z = tfe.matmul(x, y)\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(z.reveal())\n            np.testing.assert_allclose(\n                result,\n                np.array([[[94, 100], [229, 244]], [[508, 532], [697, 730]]]),\n                rtol=0.0,\n                atol=0.01,\n            )\n\n    def test_boolean_sharing(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        x = tfe.define_private_variable(\n            tf.constant([[1, 2, 3], [4, 5, 6]]), share_type=BOOLEAN\n        )\n        y = tfe.define_private_variable(\n            tf.constant([[7, 8, 9], [10, 11, 12]]), share_type=BOOLEAN\n        )\n\n        z1 = tfe.B_xor(x, y)\n\n        z2 = tfe.B_and(x, y)\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(z1.reveal())\n            np.testing.assert_allclose(\n                result, np.array([[6, 10, 10], [14, 14, 10]]), rtol=0.0, atol=0.01\n            )\n\n            result = sess.run(z2.reveal())\n            np.testing.assert_allclose(\n                result, np.array([[1, 0, 1], [0, 1, 4]]), rtol=0.0, atol=0.01\n            )\n\n    def test_not_private(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        x = tfe.define_private_variable(\n            tf.constant([[1, 2, 3], [4, 5, 6]]), share_type=BOOLEAN, apply_scaling=False\n        )\n        y = tfe.define_private_variable(\n            tf.constant([[1, 0, 0], [0, 1, 0]]),\n            apply_scaling=False,\n            share_type=BOOLEAN,\n            factory=prot.bool_factory,\n        )\n        z1 = ~x\n        z2 = ~y\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(z1.reveal())\n            np.testing.assert_allclose(\n                result, np.array([[-2, -3, -4], [-5, -6, -7]]), rtol=0.0, atol=0.01\n            )\n\n            result = sess.run(z2.reveal())\n            np.testing.assert_allclose(\n                result, np.array([[0, 1, 1], [1, 0, 1]]), rtol=0.0, atol=0.01\n            )\n\n    def test_native_ppa_sklansky(self):\n        from math import log2\n        from random import randint\n\n        n = 10\n        while n > 0:\n            n = n - 1\n\n            x = randint(1, 2 ** 31)\n            y = randint(1, 2 ** 31)\n            keep_masks = [\n                0x5555555555555555,\n                0x3333333333333333,\n                0x0F0F0F0F0F0F0F0F,\n                0x00FF00FF00FF00FF,\n                0x0000FFFF0000FFFF,\n                0x00000000FFFFFFFF,\n            ]  # yapf: disable\n            copy_masks = [\n                0x5555555555555555,\n                0x2222222222222222,\n                0x0808080808080808,\n                0x0080008000800080,\n                0x0000800000008000,\n                0x0000000080000000,\n            ]  # yapf: disable\n\n            G = x & y\n            P = x ^ y\n            k = 64\n            for i in range(int(log2(k))):\n                c_mask = copy_masks[i]\n                k_mask = keep_masks[i]\n                # Copy the selected bit to 2^i positions:\n                # For example, when i=2, the 4-th bit is copied to the (5, 6, 7, 8)-th bits\n                G1 = (G & c_mask) << 1\n                P1 = (P & c_mask) << 1\n                for j in range(i):\n                    G1 = (G1 << (2 ** j)) ^ G1\n                    P1 = (P1 << (2 ** j)) ^ P1\n                # Two-round impl. using algo. specified in the slides that assume using OR gate is free, but in fact,\n                # here using OR gate cost one round.\n                # The PPA operator \'o\' is defined as:\n                # (G, P) o (G1, P1) = (G + P*G1, P*P1), where \'+\' is OR, \'*\' is AND\n\n                # G1 and P1 are 0 for those positions that we do not copy the selected bit to.\n                # Hence for those positions, the result is: (G, P) = (G, P) o (0, 0) = (G, 0).\n                # In order to keep (G, P) for these positions so that they can be used in the future,\n                # we need to let (G1, P1) = (G, P) for these positions, because (G, P) o (G, P) = (G, P)\n\n                # G1 = G1 ^ (G & k_mask)\n                # P1 = P1 ^ (P & k_mask)\n\n                # G = G | (P & G1)\n                # P = P & P1\n\n                # One-round impl. by modifying the PPA operator \'o\' as:\n                # (G, P) o (G1, P1) = (G ^ (P*G1), P*P1), where \'^\' is XOR, \'*\' is AND\n                # This is a valid definition: when calculating the carry bit c_i = g_i + p_i * c_{i-1},\n                # the OR \'+\' can actually be replaced with XOR \'^\' because we know g_i and p_i will NOT take \'1\'\n                # at the same time.\n                # And this PPA operator \'o\' is also associative. BUT, it is NOT idempotent, hence (G, P) o (G, P) != (G, P).\n                # This does not matter, because we can do (G, P) o (0, P) = (G, P), or (G, P) o (0, 1) = (G, P)\n                # if we want to keep G and P bits.\n\n                # Option 1: Using (G, P) o (0, P) = (G, P)\n                # P1 = P1 ^ (P & k_mask)\n                # Option 2: Using (G, P) o (0, 1) = (G, P)\n                P1 = P1 ^ k_mask\n\n                G = G ^ (P & G1)\n                P = P & P1\n\n            # G stores the carry-in to the next position\n            C = G << 1\n            P = x ^ y\n            z = C ^ P\n\n            truth = x + y\n\n            assert z == truth\n\n    def test_native_ppa_kogge_stone(self):\n        from math import log2\n        from random import randint\n\n        n = 10\n        while n > 0:\n            n = n - 1\n            x = randint(1, 2 ** 31)\n            y = randint(1, 2 ** 31)\n            G = x & y\n            P = x ^ y\n            keep_masks = [\n                0x0000000000000001,\n                0x0000000000000003,\n                0x000000000000000F,\n                0x00000000000000FF,\n                0x000000000000FFFF,\n                0x00000000FFFFFFFF,\n            ]  # yapf: disable\n            k = 64\n            for i in range(int(log2(k))):\n                k_mask = keep_masks[i]\n                # Copy the selected bit to 2^i positions:\n                # For example, when i=2, the 4-th bit is copied to the (5, 6, 7, 8)-th bits\n                G1 = G << (2 ** i)\n                P1 = P << (2 ** i)\n\n                # One-round impl. by modifying the PPA operator \'o\' as:\n                # (G, P) o (G1, P1) = (G ^ (P*G1), P*P1), where \'^\' is XOR, \'*\' is AND\n                # This is a valid definition: when calculating the carry bit c_i = g_i + p_i * c_{i-1},\n                # the OR \'+\' can actually be replaced with XOR \'^\' because we know g_i and p_i will NOT take \'1\'\n                # at the same time.\n                # And this PPA operator \'o\' is also associative. BUT, it is NOT idempotent, hence (G, P) o (G, P) != (G, P).\n                # This does not matter, because we can do (G, P) o (0, P) = (G, P), or (G, P) o (0, 1) = (G, P)\n                # if we want to keep G and P bits.\n\n                # Option 1: Using (G, P) o (0, P) = (G, P)\n                # P1 = P1 ^ (P & k_mask)\n                # Option 2: Using (G, P) o (0, 1) = (G, P)\n                P1 = P1 ^ k_mask\n\n                G = G ^ (P & G1)\n                P = P & P1\n\n            # G stores the carry-in to the next position\n            C = G << 1\n            P = x ^ y\n            z = C ^ P\n\n            truth = x + y\n\n            assert z == truth\n\n    def test_lshift_private(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        x = tfe.define_private_variable(\n            tf.constant([[1, 2, 3], [4, 5, 6]]), share_type=BOOLEAN\n        )\n\n        z = x << 1\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(z.reveal())\n            np.testing.assert_allclose(\n                result, np.array([[2, 4, 6], [8, 10, 12]]), rtol=0.0, atol=0.01\n            )\n\n    def test_rshift_private(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        x = tfe.define_private_variable(\n            tf.constant([[1, 2, 3], [4, 5, 6]]), share_type=BOOLEAN\n        )\n        y = tfe.define_private_variable(\n            tf.constant([[-1, -2, -3], [-4, 5, 6]]),\n            share_type=BOOLEAN,\n            apply_scaling=False,\n        )\n\n        z = x >> 1\n        w = y >> 1\n        s = y.logical_rshift(1)\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(z.reveal())\n            np.testing.assert_allclose(\n                result,\n                np.array(\n                    [[0.5, 1, 1.5], [2, 2.5, 3]]\n                ),  # NOTE: x is scaled and treated as fixed-point number\n                rtol=0.0,\n                atol=0.01,\n            )\n            result = sess.run(w.reveal())\n            np.testing.assert_allclose(\n                result, np.array([[-1, -1, -2], [-2, 2, 3]]), rtol=0.0, atol=0.01\n            )\n            result = sess.run(s.reveal())\n            np.testing.assert_allclose(\n                result,\n                np.array(\n                    [\n                        [\n                            (-1 & ((1 << prot.nbits) - 1)) >> 1,\n                            (-2 & ((1 << prot.nbits) - 1)) >> 1,\n                            (-3 & ((1 << prot.nbits) - 1)) >> 1,\n                        ],\n                        [(-4 & ((1 << prot.nbits) - 1)) >> 1, 2, 3],\n                    ]\n                ),\n                rtol=0.0,\n                atol=0.01,\n            )\n\n    def test_ppa_private_private(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        x = tfe.define_private_variable(\n            tf.constant([[1, 2, 3], [4, 5, 6]]), share_type=BOOLEAN\n        )\n        y = tfe.define_private_variable(\n            tf.constant([[7, 8, 9], [10, 11, 12]]), share_type=BOOLEAN\n        )\n\n        # Parallel prefix adder. It is simply an adder for boolean sharing.\n        z1 = tfe.B_ppa(x, y, topology=""sklansky"")\n        z2 = tfe.B_ppa(x, y, topology=""kogge_stone"")\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(z1.reveal())\n            np.testing.assert_allclose(\n                result, np.array([[8, 10, 12], [14, 16, 18]]), rtol=0.0, atol=0.01\n            )\n\n            result = sess.run(z2.reveal())\n            np.testing.assert_allclose(\n                result, np.array([[8, 10, 12], [14, 16, 18]]), rtol=0.0, atol=0.01\n            )\n\n    def test_a2b_private(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        x = tfe.define_private_variable(\n            tf.constant([[1, 2, 3], [4, 5, 6]]), share_type=ARITHMETIC\n        )\n\n        z = tfe.A2B(x)\n        assert z.share_type == BOOLEAN\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(z.reveal())\n            np.testing.assert_allclose(\n                result, np.array([[1, 2, 3], [4, 5, 6]]), rtol=0.0, atol=0.01\n            )\n\n    def test_b2a_private(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        x = tfe.define_private_variable(\n            tf.constant([[1, 2, 3], [4, 5, 6]]), share_type=BOOLEAN\n        )\n\n        z = tfe.B2A(x)\n        assert z.share_type == ARITHMETIC\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(z.reveal())\n            np.testing.assert_allclose(\n                result, np.array([[1, 2, 3], [4, 5, 6]]), rtol=0.0, atol=0.01\n            )\n\n    def test_ot(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        m0 = prot.define_constant(\n            np.array([[1, 2, 3], [4, 5, 6]]), apply_scaling=False\n        ).unwrapped[0]\n        m1 = prot.define_constant(\n            np.array([[2, 3, 4], [5, 6, 7]]), apply_scaling=False\n        ).unwrapped[0]\n        c_on_receiver = prot.define_constant(\n            np.array([[1, 0, 1], [0, 1, 0]]),\n            apply_scaling=False,\n            factory=prot.bool_factory,\n        ).unwrapped[0]\n        c_on_helper = prot.define_constant(\n            np.array([[1, 0, 1], [0, 1, 0]]),\n            apply_scaling=False,\n            factory=prot.bool_factory,\n        ).unwrapped[0]\n\n        m_c = prot._ot(  # pylint: disable=protected-access\n            prot.servers[1],\n            prot.servers[2],\n            prot.servers[0],\n            m0,\n            m1,\n            c_on_receiver,\n            c_on_helper,\n            prot.pairwise_keys[1][0],\n            prot.pairwise_keys[0][1],\n            prot.pairwise_nonces[0],\n        )\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(\n                prot._decode(m_c, False)\n            )  # pylint: disable=protected-access\n            np.testing.assert_allclose(\n                result, np.array([[2, 2, 4], [4, 6, 6]]), rtol=0.0, atol=0.01\n            )\n\n    def test_mul_AB_public_private(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        x = tfe.define_constant(np.array([[1, 2, 3], [4, 5, 6]]), share_type=ARITHMETIC)\n        y = tfe.define_private_variable(\n            tf.constant([[1, 0, 0], [0, 1, 0]]),\n            apply_scaling=False,\n            share_type=BOOLEAN,\n            factory=prot.bool_factory,\n        )\n\n        z = tfe.mul_AB(x, y)\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(z.reveal())\n            np.testing.assert_allclose(\n                result, np.array([[1, 0, 0], [0, 5, 0]]), rtol=0.0, atol=0.01\n            )\n\n    def test_mul_AB_private_private(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        x = tfe.define_private_variable(\n            np.array([[1, 2, 3], [4, 5, 6]]), share_type=ARITHMETIC,\n        )\n        y = tfe.define_private_variable(\n            tf.constant([[1, 0, 0], [0, 1, 0]]),\n            apply_scaling=False,\n            share_type=BOOLEAN,\n            factory=prot.bool_factory,\n        )\n\n        z = tfe.mul_AB(x, y)\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(z.reveal())\n            np.testing.assert_allclose(\n                result, np.array([[1, 0, 0], [0, 5, 0]]), rtol=0.0, atol=0.01\n            )\n\n    def test_bit_extract(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        x = tfe.define_private_variable(\n            np.array([[1, -2, 3], [-4, -5, 6]]), share_type=ARITHMETIC,\n        )\n        y = tfe.define_private_variable(\n            np.array([[1, -2, 3], [-4, -5, 6]]),\n            share_type=ARITHMETIC,\n            apply_scaling=False,\n        )\n\n        z = tfe.bit_extract(\n            x, 63\n        )  # The sign bit. Since x is scaled, you should be more careful about extracting other bits.\n        w = tfe.bit_extract(y, 1)  # y is not scaled\n        s = tfe.msb(x)  # Sign bit\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(z.reveal())\n            np.testing.assert_allclose(\n                result.astype(int),\n                np.array([[0, 1, 0], [1, 1, 0]]),\n                rtol=0.0,\n                atol=0.01,\n            )\n            result = sess.run(w.reveal())\n            np.testing.assert_allclose(\n                result.astype(int),\n                np.array([[0, 1, 1], [0, 1, 1]]),\n                rtol=0.0,\n                atol=0.01,\n            )\n            result = sess.run(s.reveal())\n            np.testing.assert_allclose(\n                result.astype(int),\n                np.array([[0, 1, 0], [1, 1, 0]]),\n                rtol=0.0,\n                atol=0.01,\n            )\n\n    def test_pow_private(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        x = tfe.define_private_variable(tf.constant([[1, 2, 3], [4, 5, 6]]))\n\n        y = x ** 2\n        z = x ** 3\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(y.reveal())\n            np.testing.assert_allclose(\n                result, np.array([[1, 4, 9], [16, 25, 36]]), rtol=0.0, atol=0.01\n            )\n\n            result = sess.run(z.reveal())\n            np.testing.assert_allclose(\n                result, np.array([[1, 8, 27], [64, 125, 216]]), rtol=0.0, atol=0.01\n            )\n\n    def test_polynomial_private(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        x = tfe.define_private_variable(tf.constant([[1, 2, 3], [4, 5, 6]]))\n\n        # Friendly version\n        y = 1 + 1.2 * x + 3 * (x ** 2) + 0.5 * (x ** 3)\n        # More optimized version: No truncation for multiplying integer coefficients (e.g., \'3\' in this example)\n        z = tfe.polynomial(x, [1, 1.2, 3, 0.5])\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(y.reveal())\n            np.testing.assert_allclose(\n                result,\n                np.array([[5.7, 19.4, 45.1], [85.8, 144.5, 224.2]]),\n                rtol=0.0,\n                atol=0.01,\n            )\n\n            result = sess.run(z.reveal())\n            np.testing.assert_allclose(\n                result,\n                np.array([[5.7, 19.4, 45.1], [85.8, 144.5, 224.2]]),\n                rtol=0.0,\n                atol=0.01,\n            )\n\n    def test_polynomial_piecewise(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        x = tfe.define_private_variable(tf.constant([[-1, -0.5, -0.25], [0, 0.25, 2]]))\n\n        # This is the approximation of the sigmoid function by using a piecewise function:\n        # f(x) = (0 if x<-0.5), (x+0.5 if -0.5<=x<0.5), (1 if x>=0.5)\n        z1 = tfe.polynomial_piecewise(\n            x,\n            (-0.5, 0.5),\n            ((0,), (0.5, 1), (1,)),  # use tuple because list is not hashable\n        )\n        # Or, simply use the pre-defined sigmoid API which includes a different approximation\n        z2 = tfe.sigmoid(x)\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(z1.reveal())\n            np.testing.assert_allclose(\n                result, np.array([[0, 0, 0.25], [0.5, 0.75, 1]]), rtol=0.0, atol=0.01\n            )\n            result = sess.run(z2.reveal())\n            np.testing.assert_allclose(\n                result,\n                np.array([[0.33, 0.415, 0.4575], [0.5, 0.5425, 0.84]]),\n                rtol=0.0,\n                atol=0.01,\n            )\n\n    def test_transpose(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        x = tfe.define_private_variable(tf.constant([[1, 2, 3], [4, 5, 6]]))\n        y = tfe.define_constant(np.array([[1, 2, 3], [4, 5, 6]]))\n\n        z1 = x.transpose()\n        z2 = tfe.transpose(y)\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(z1.reveal())\n            np.testing.assert_allclose(\n                result, np.array([[1, 4], [2, 5], [3, 6]]), rtol=0.0, atol=0.01\n            )\n\n            result = sess.run(z2)\n            np.testing.assert_allclose(\n                result, np.array([[1, 4], [2, 5], [3, 6]]), rtol=0.0, atol=0.01\n            )\n\n    def test_reduce_sum(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        x = tfe.define_private_variable(tf.constant([[1, 2, 3], [4, 5, 6]]))\n        y = tfe.define_constant(np.array([[1, 2, 3], [4, 5, 6]]))\n\n        z1 = x.reduce_sum(axis=1, keepdims=True)\n        z2 = tfe.reduce_sum(y, axis=0, keepdims=False)\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(z1.reveal())\n            np.testing.assert_allclose(\n                result, np.array([[6], [15]]), rtol=0.0, atol=0.01\n            )\n\n            result = sess.run(z2)\n            np.testing.assert_allclose(result, np.array([5, 7, 9]), rtol=0.0, atol=0.01)\n\n    def test_concat(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        x1 = tfe.define_private_variable(tf.constant([[1, 2], [4, 5]]))\n        x2 = tfe.define_private_variable(tf.constant([[3], [6]]))\n        y1 = tfe.define_constant(np.array([[1, 2, 3]]))\n        y2 = tfe.define_constant(np.array([[4, 5, 6]]))\n\n        z1 = tfe.concat([x1, x2], axis=1)\n        z2 = tfe.concat([y1, y2], axis=0)\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(z1.reveal())\n            np.testing.assert_allclose(\n                result, np.array([[1, 2, 3], [4, 5, 6]]), rtol=0.0, atol=0.01\n            )\n\n            result = sess.run(z2)\n            np.testing.assert_allclose(\n                result, np.array([[1, 2, 3], [4, 5, 6]]), rtol=0.0, atol=0.01\n            )\n\n    def test_simple_lr_model(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        # define inputs\n        x_raw = tf.random.uniform(minval=-0.5, maxval=0.5, shape=[99, 10], seed=1000)\n        x = tfe.define_private_variable(x_raw, name=""x"")\n        y_raw = tf.cast(\n            tf.reduce_mean(x_raw, axis=1, keepdims=True) > 0, dtype=tf.float32\n        )\n        y = tfe.define_private_variable(y_raw, name=""y"")\n        w = tfe.define_private_variable(\n            tf.random_uniform([10, 1], -0.01, 0.01, seed=100), name=""w""\n        )\n        b = tfe.define_private_variable(tf.zeros([1]), name=""b"")\n        learning_rate = 0.01\n\n        with tf.name_scope(""forward""):\n            out = tfe.matmul(x, w) + b\n            y_hat = tfe.sigmoid(out)\n\n        with tf.name_scope(""loss-grad""):\n            dy = y_hat - y\n        batch_size = x.shape.as_list()[0]\n        with tf.name_scope(""backward""):\n            dw = tfe.matmul(tfe.transpose(x), dy) / batch_size\n            db = tfe.reduce_sum(dy, axis=0) / batch_size\n            upd1 = dw * learning_rate\n            upd2 = db * learning_rate\n            assign_ops = [tfe.assign(w, w - upd1), tfe.assign(b, b - upd2)]\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            sess.run(assign_ops)\n\n    def test_mul_trunc2_private_private(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        def provide_input():\n            # normal TensorFlow operations can be run locally\n            # as part of defining a private input, in this\n            # case on the machine of the input provider\n            return tf.ones(shape=(2, 2)) * 1.3\n\n        # define inputs\n        x = tfe.define_private_variable(tf.ones(shape=(2, 2)) * 2)\n        y = tfe.define_private_input(""input-provider"", provide_input)\n\n        # define computation\n        z = tfe.mul_trunc2(x, y)\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            result = sess.run(z.reveal(), tag=""mul_trunc2"")\n            np.testing.assert_allclose(\n                result, np.array([[2.6, 2.6], [2.6, 2.6]]), rtol=0.0, atol=0.01\n            )\n\n    def test_write_private(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        def provide_input():\n            # normal TensorFlow operations can be run locally\n            # as part of defining a private input, in this\n            # case on the machine of the input provider\n            return tf.ones(shape=(2, 2)) * 1.3\n\n        # define inputs\n        x = tfe.define_private_input(""input-provider"", provide_input)\n\n        _, tmp_filename = tempfile.mkstemp()\n        write_op = x.write(tmp_filename)\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            sess.run(write_op)\n\n        os.remove(tmp_filename)\n\n    def test_read_private(self):\n\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        def provide_input():\n            return tf.reshape(tf.range(0, 8), [4, 2])\n\n        # define inputs\n        x = tfe.define_private_input(""input-provider"", provide_input)\n\n        _, tmp_filename = tempfile.mkstemp()\n        write_op = x.write(tmp_filename)\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            sess.run(write_op)\n\n        x = tfe.read(tmp_filename, batch_size=5, n_columns=2)\n        with tfe.Session() as sess:\n            result = sess.run(x.reveal())\n            np.testing.assert_allclose(\n                result,\n                np.array(list(range(0, 8)) + [0, 1]).reshape([5, 2]),\n                rtol=0.0,\n                atol=0.01,\n            )\n\n        os.remove(tmp_filename)\n\n    @unittest.skip\n    def test_iterate_private(self):\n        tf.reset_default_graph()\n\n        prot = ABY3()\n        tfe.set_protocol(prot)\n\n        def provide_input():\n            return tf.reshape(tf.range(0, 8), [4, 2])\n\n        # define inputs\n        x = tfe.define_private_input(""input-provider"", provide_input)\n\n        _, tmp_filename = tempfile.mkstemp()\n        write_op = x.write(tmp_filename)\n\n        with tfe.Session() as sess:\n            # initialize variables\n            sess.run(tfe.global_variables_initializer())\n            # reveal result\n            sess.run(write_op)\n\n        x = tfe.read(tmp_filename, batch_size=5, n_columns=2)\n        y = tfe.iterate(x, batch_size=3, repeat=True, shuffle=False)\n        z = tfe.iterate(x, batch_size=3, repeat=True, shuffle=True)\n        with tfe.Session() as sess:\n            sess.run(tfe.global_variables_initializer())\n            # TODO: fix this test\n            print(sess.run(x.reveal()))\n            print(sess.run(y.reveal()))\n            print(sess.run(y.reveal()))\n            print(sess.run(x.reveal()))\n            print(sess.run(z.reveal()))\n\n        os.remove(tmp_filename)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/protocol/pond/__init__.py,0,"b'""""""The Pond protocol.""""""\nfrom __future__ import absolute_import\n\nfrom .pond import AdditiveFIFOQueue\nfrom .pond import Pond\nfrom .pond import PondMaskedTensor\nfrom .pond import PondPrivateTensor\nfrom .pond import PondPrivateVariable\nfrom .pond import PondPublicTensor\nfrom .pond import PondTensor\nfrom .pond import TFEInputter\nfrom .pond import TFETensor\nfrom .pond import TFEVariable\nfrom .pond import _type\nfrom .triple_sources import OnlineTripleSource\nfrom .triple_sources import QueuedOnlineTripleSource\n\n__all__ = [\n    ""Pond"",\n    ""PondPublicTensor"",\n    ""PondTensor"",\n    ""PondPublicTensor"",\n    ""PondPrivateTensor"",\n    ""PondPrivateVariable"",\n    ""PondMaskedTensor"",\n    ""TFEVariable"",\n    ""TFETensor"",\n    ""TFEInputter"",\n    ""_type"",\n    ""OnlineTripleSource"",\n    ""QueuedOnlineTripleSource"",\n    ""AdditiveFIFOQueue"",\n]\n'"
tf_encrypted/protocol/pond/pond.py,376,"b'# pylint: disable=protected-access\n""""""Implementation of the Pond protocol.\n\nPond is a vectorized two-party secret sharing protocol similar to SPDZ with a\ngeneralized implementation of Beaver triples that are produced by a third-party\nhelper.""""""\nfrom __future__ import absolute_import\n\nimport abc\nimport logging\nimport random\nimport sys\nfrom functools import reduce\nfrom functools import wraps\nfrom math import ceil\nfrom math import log2\nfrom typing import Any\nfrom typing import Callable\nfrom typing import List\nfrom typing import NewType\nfrom typing import Optional\nfrom typing import Tuple\nfrom typing import Union\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom ...config import get_config\nfrom ...config import tensorflow_supports_int64\nfrom ...player import Player\nfrom ...queue.fifo import AbstractFIFOQueue\nfrom ...tensor import fixed64\nfrom ...tensor import fixed100\nfrom ...tensor import int64factory\nfrom ...tensor import int100factory\nfrom ...tensor.factory import AbstractConstant\nfrom ...tensor.factory import AbstractFactory\nfrom ...tensor.factory import AbstractPlaceholder\nfrom ...tensor.factory import AbstractTensor\nfrom ...tensor.factory import AbstractVariable\nfrom ...tensor.fixed import FixedpointConfig\nfrom ...tensor.fixed import _validate_fixedpoint_config\nfrom ...tensor.helpers import inverse\nfrom ...utils import wrap_in_variables\nfrom ..protocol import Protocol\nfrom ..protocol import memoize\nfrom ..protocol import nodes\nfrom .triple_sources import OnlineTripleSource\nfrom .triple_sources import TripleSource\n\nTFEData = Union[np.ndarray, tf.Tensor]\nTFEVariable = Union[""PondPublicVariable"", ""PondPrivateVariable"", tf.Variable]\nTFEPublicTensor = NewType(""TFEPublicTensor"", ""PondPublicTensor"")\nTFETensor = Union[TFEPublicTensor, ""PondPrivateTensor"", ""PondMaskedTensor""]\nTFEInputter = Callable[[], Union[List[tf.Tensor], tf.Tensor]]\nTF_INT_TYPES = [tf.int8, tf.int16, tf.int32, tf.int64]\nTripleSourceOrPlayer = Union[TripleSource, Player]\n\n_THISMODULE = sys.modules[__name__]\n\n\nclass Pond(Protocol):\n    """"""Pond is similar to SPDZ except it has been vectorized plus a few more\n  optimizations.\n\n  Pond works with 2 parties for computation and one crypto producer for\n  triples.\n\n  :param Player server_0: The ""alice"" of MPC.\n  :param Player server_1: The ""bob"" of MPC.\n  :param Player triple_source: the entity responsible for producing triples in\n      `Pond` protocol. The valid values can be of type `TripleSource` or\n      `Player`. If a `Player` is passed, it will be the host that is used as an\n      `OnlineTripleSource` producer.\n  :param AbstractFactory tensor_factory: Which backing type of tensor you would\n      like to use, e.g. `int100` or `int64`\n  :param Player fixedpoint_config: Parameters for fixed-point precision tensors\n  """"""  # noqa:E501\n\n    def __init__(\n        self,\n        server_0=None,\n        server_1=None,\n        triple_source: Optional[TripleSourceOrPlayer] = None,\n        tensor_factory: Optional[AbstractFactory] = None,\n        fixedpoint_config: Optional[FixedpointConfig] = None,\n    ) -> None:\n        config = get_config()\n        self.server_0 = config.get_player(server_0 if server_0 else ""server0"")\n        self.server_1 = config.get_player(server_1 if server_1 else ""server1"")\n\n        if triple_source is None:\n            crypto_producer = config.get_player(""server2"")\n            crypto_producer = config.get_player(\n                crypto_producer if crypto_producer else ""crypto-producer""\n            )\n            self.triple_source = OnlineTripleSource(crypto_producer)\n        else:\n            if isinstance(triple_source, Player):\n                self.triple_source = OnlineTripleSource(triple_source)\n            else:\n                assert isinstance(triple_source, TripleSource)\n                self.triple_source = triple_source\n\n        if tensor_factory is None:\n            if tensorflow_supports_int64():\n                tensor_factory = int64factory\n            else:\n                logging.warning(\n                    ""Falling back to using int100 tensors due to lack of int64 ""\n                    ""support. Performance may be improved by installing a version of ""\n                    ""TensorFlow supporting this (1.13+ or custom build).""\n                )\n                tensor_factory = int100factory\n\n        if fixedpoint_config is None:\n            if tensor_factory is int64factory:\n                fixedpoint_config = fixed64\n            elif tensor_factory is int100factory:\n                fixedpoint_config = fixed100\n            else:\n                raise ValueError(\n                    (\n                        ""Don\'t know how to pick fixedpoint configuration ""\n                        ""for tensor type {}""\n                    ).format(tensor_factory)\n                )\n\n        _validate_fixedpoint_config(fixedpoint_config, tensor_factory)\n        self.fixedpoint_config = fixedpoint_config\n        self.tensor_factory = tensor_factory\n\n    def define_constant(\n        self,\n        value: np.ndarray,\n        apply_scaling: bool = True,\n        name: Optional[str] = None,\n        factory: Optional[AbstractFactory] = None,\n    ):\n        """"""\n    define_constant(value, apply_scaling, name, factory) -> PondConstant\n\n    Define a constant to use in computation.\n\n    .. code-block:: python\n\n        x = prot.define_constant(np.array([1,2,3,4]), apply_scaling=False)\n\n    :See: tf.constant\n\n    :param np.ndarray value: The value to define as a constant.\n    :param bool apply_scaling: Whether or not to scale the value.\n    :param str name: What name to give to this node in the graph.\n    :param AbstractFactory factory: Which tensor type to represent this value\n        with.\n    """"""\n        assert isinstance(value, np.ndarray), type(value)\n\n        factory = factory or self.tensor_factory\n\n        v = self._encode(value, apply_scaling)\n\n        with tf.name_scope(""constant{}"".format(""-"" + name if name else """")):\n\n            with tf.device(self.server_0.device_name):\n                x_on_0 = factory.constant(v)\n\n            with tf.device(self.server_1.device_name):\n                x_on_1 = factory.constant(v)\n\n        return PondConstant(self, x_on_0, x_on_1, apply_scaling)\n\n    def define_public_placeholder(\n        self,\n        shape,\n        apply_scaling: bool = True,\n        name: Optional[str] = None,\n        factory: Optional[AbstractFactory] = None,\n    ):\n        """"""Define a `public` placeholder to use in computation. This will be known\n    to both parties.\n\n    .. code-block:: python\n\n        x = prot.define_public_placeholder(shape=(1024, 1024))\n\n    :See: tf.placeholder\n\n    :param List[int] shape: The shape of the placeholder.\n    :param bool apply_scaling: Whether or not to scale the value.\n    :param str name: What name to give to this node in the graph.\n    :param AbstractFactory factory: Which tensor type to represent this value\n        with.\n    """"""\n\n        factory = factory or self.tensor_factory\n        suffix = ""-"" + name if name else """"\n\n        with tf.name_scope(""public-placeholder{}"".format(suffix)):\n\n            with tf.device(self.server_0.device_name):\n                x_on_0 = factory.placeholder(shape)\n\n            with tf.device(self.server_1.device_name):\n                x_on_1 = factory.placeholder(shape)\n\n        return PondPublicPlaceholder(self, x_on_0, x_on_1, apply_scaling)\n\n    def define_private_placeholder(\n        self,\n        shape,\n        apply_scaling: bool = True,\n        name: Optional[str] = None,\n        factory: Optional[AbstractFactory] = None,\n    ):\n        """"""Define a `private` placeholder to use in computation. This will only be\n    known by the party that defines it.\n\n    .. code-block:: python\n\n        x = prot.define_private_placeholder(shape=(1024, 1024))\n\n    :See: tf.placeholder\n\n    :param List[int] shape: The shape of the placeholder.\n    :param bool apply_scaling: Whether or not to scale the value.\n    :param str name: What name to give to this node in the graph.\n    :param AbstractFactory factory: Which tensor type to represent this value\n        with.\n    """"""\n\n        factory = factory or self.tensor_factory\n\n        suffix = ""-"" + name if name else """"\n        with tf.name_scope(""private-placeholder{}"".format(suffix)):\n\n            with tf.device(self.server_0.device_name):\n                x0 = factory.placeholder(shape)\n\n            with tf.device(self.server_1.device_name):\n                x1 = factory.placeholder(shape)\n\n        return PondPrivatePlaceholder(self, x0, x1, apply_scaling)\n\n    def define_public_variable(\n        self,\n        initial_value,\n        apply_scaling: bool = True,\n        name: Optional[str] = None,\n        factory: Optional[AbstractFactory] = None,\n    ):\n        """"""Define a public variable.\n\n    This is like defining a variable in tensorflow except it creates one that\n    can be used by the protocol.\n\n    For most cases, you can think of this as the same as the one from\n    TensorFlow and you don\'t generally need to consider the difference.\n\n    For those curious, under the hood, the major difference is that this\n    function will pin your data to a specific device which will be used to\n    optimize the graph later on.\n\n    :see: tf.Variable\n\n    :param Union[np.ndarray,tf.Tensor,PondPublicTensor] initial_value: The\n        initial value.\n    :param bool apply_scaling: Whether or not to scale the value.\n    :param str name: What name to give to this node in the graph.\n    :param AbstractFactory factory: Which tensor type to represent this value\n        with.\n    """"""\n        assert isinstance(\n            initial_value, (np.ndarray, tf.Tensor, PondPublicTensor)\n        ), type(initial_value)\n\n        factory = factory or self.tensor_factory\n\n        with tf.name_scope(""public-var{}"".format(""-"" + name if name else """")):\n\n            if isinstance(initial_value, np.ndarray):\n                v = self._encode(initial_value, apply_scaling)\n                v_on_0, v_on_1 = v, v\n\n            elif isinstance(initial_value, tf.Tensor):\n                inttype = factory.native_type\n                v = self._encode(initial_value, apply_scaling, tf_int_type=inttype)\n                v_on_0, v_on_1 = v, v\n\n            elif isinstance(initial_value, PondPublicTensor):\n                v_on_0, v_on_1 = initial_value.unwrapped\n\n            else:\n                raise TypeError(\n                    (""Don\'t know how to turn {} into a "" ""public variable"").format(\n                        type(initial_value)\n                    )\n                )\n\n            with tf.device(self.server_0.device_name):\n                x_on_0 = factory.variable(v_on_0)\n\n            with tf.device(self.server_1.device_name):\n                x_on_1 = factory.variable(v_on_1)\n\n        x = PondPublicVariable(self, x_on_0, x_on_1, apply_scaling)\n        return x\n\n    def define_private_variable(\n        self,\n        initial_value,\n        apply_scaling: bool = True,\n        name: Optional[str] = None,\n        factory: Optional[AbstractFactory] = None,\n    ):\n        """"""Define a private variable.\n\n    This will take the passed value and construct shares that will be split up\n    between those involved in the computation.\n\n    For example, in a two party architecture, this will split the value into\n    two sets of shares and transfer them between each party in a secure manner.\n\n    :see tf.Variable\n\n    :param Union[np.ndarray,tf.Tensor,PondPublicTensor] initial_value: The\n        initial value.\n    :param bool apply_scaling: Whether or not to scale the value.\n    :param str name: What name to give to this node in the graph.\n    :param AbstractFactory factory: Which tensor type to represent this value\n        with.\n    """"""\n        init_val_types = (np.ndarray, tf.Tensor, PondPublicTensor, PondPrivateTensor)\n        assert isinstance(initial_value, init_val_types), type(initial_value)\n\n        factory = factory or self.tensor_factory\n        suffix = ""-"" + name if name else """"\n\n        with tf.name_scope(""private-var{}"".format(suffix)):\n\n            if isinstance(initial_value, np.ndarray):\n                v = factory.tensor(self._encode(initial_value, apply_scaling))\n                v0, v1 = self._share(v)\n\n            elif isinstance(initial_value, tf.Tensor):\n                v = factory.tensor(\n                    self._encode(\n                        initial_value, apply_scaling, tf_int_type=factory.native_type,\n                    )\n                )\n                v0, v1 = self._share(v)\n\n            elif isinstance(initial_value, PondPublicTensor):\n                v_on_0, _ = initial_value.unwrapped\n                with tf.device(self.server_0.device_name):\n                    # NOTE[Morten]\n                    # we can alternatively avoid transfer of v1 from server0 and server1\n                    # by having the crypto producer (pre-)generate sharings of zero\n                    v0, v1 = self._share(v_on_0)\n\n            elif isinstance(initial_value, PondPrivateTensor):\n                v0, v1 = initial_value.unwrapped\n\n            else:\n                raise TypeError(\n                    (""Don\'t know how to turn {} "" ""into private variable"").format(\n                        type(initial_value)\n                    )\n                )\n\n            with tf.device(self.server_0.device_name):\n                x0 = factory.variable(v0)\n\n            with tf.device(self.server_1.device_name):\n                x1 = factory.variable(v1)\n\n        x = PondPrivateVariable(self, x0, x1, apply_scaling)\n        return x\n\n    def fifo_queue(self, capacity, shape, shared_name):\n        return AdditiveFIFOQueue(\n            protocol=self,\n            server_0=self.server_0,\n            server_1=self.server_1,\n            capacity=capacity,\n            dtype=self.tensor_factory,\n            shape=shape,\n            shared_name=shared_name,\n        )\n\n    def define_public_input(\n        self,\n        player: Union[str, Player],\n        inputter_fn: TFEInputter,\n        apply_scaling: bool = True,\n        name: Optional[str] = None,\n    ):\n        """"""Define a public input.\n\n    This represents a `public` input owned by the specified player into the\n    graph.\n\n    :param Union[str,Player] player: Which player owns this input.\n    :param bool apply_scaling: Whether or not to scale the value.\n    :param str name: What name to give to this node in the graph.\n    """"""\n        if isinstance(player, str):\n            player = get_config().get_player(player)\n        assert isinstance(player, Player)\n\n        factory = self.tensor_factory\n        suffix = ""-"" + name if name else """"\n\n        def helper(v: tf.Tensor) -> ""PondPublicTensor"":\n            assert (\n                v.shape.is_fully_defined()\n            ), ""Shape of input \'{}\' on \'{}\' is not fully defined"".format(\n                name if name else """", player.name\n            )\n            enc = self._encode(v, apply_scaling, tf_int_type=factory.native_type)\n            w = factory.tensor(enc)\n            return PondPublicTensor(self, w, w, apply_scaling)\n\n        with tf.name_scope(""public-input{}"".format(suffix)):\n\n            with tf.device(player.device_name):\n\n                inputs = inputter_fn()\n\n                if isinstance(inputs, tf.Tensor):\n                    # single input -> single output\n                    v = inputs\n                    return helper(v)\n\n                if isinstance(inputs, (list, tuple)):\n                    # multiple inputs -> multiple outputs\n                    return [helper(v) for v in inputs]\n\n                raise TypeError(\n                    (""Don\'t know how to handle inputs "" ""of type {}"").format(\n                        type(inputs)\n                    )\n                )\n\n    def local_computation(self, player_name=None, **kwargs):\n        """"""Annotate a function `compute_func` for local computation.\n\n    This decorator can be used to pin a function\'s code to a specific player\'s\n    device for remote execution.  This is useful when defining player-specific\n    handlers for e.g. providing model weights, or input and output tensors.\n\n    The decorator can handle global functions, normal object methods, or\n    classmethods. If wrapping a method, it\'s presumed that the method\'s object\n    has an attribute named `player_name`, or that the user will provide the\n    `player_name` later on as a kwarg to the `compute_func`.\n\n    Example:\n      ```\n      @tfe.local_computation(\'input-provider\')\n      def provide_input():\n        return tf.random.normal((3, 3))\n\n      @tfe.local_computation\n      def receive_output(logits):\n        return tf.print(tf.argmax(logits, axis=-1))\n\n      x = provide_input()\n      y = model(x)\n      receive_op = receive_output(y, player_name=\'output-receiver\')\n      with tfe.Session():\n        sess.run(receive_op)\n      ```\n\n    Arguments:\n      player_name: Name of the player who should execute the function.\n      kwargs: Keyword arguments to use when encoding or encrypting\n        inputs/outputs to compute_func: see tfe.define_local_computation for\n        details.\n\n    Returns:\n      The compute_func, but decorated for remote execution.\n    """"""\n        if callable(player_name):\n            # The user has called us as a standard decorator:\n            #\n            # @tfe.local_computation\n            # def provide_input():\n            #   return tf.zeros((2, 2))\n            actual_compute_func = player_name\n            player_name = None\n        else:\n            # The user has called us as a function, maybe with non-default args:\n            #\n            # @tfe.local_computation(\'input-provider\')\n            # def provide_input():\n            #   return tf.zeros((2, 2))\n            actual_compute_func = None\n\n        def decorator(compute_func):\n            @wraps(compute_func)\n            def compute_func_wrapper(*compute_func_args, **compute_func_kwargs):\n\n                # Assume player_name was passed to decorator. If not, try to recover.\n                actual_player_name = player_name\n                if actual_player_name is None:\n                    # Maybe user has passed player_name to compute_func as a kwarg\n                    actual_player_name = compute_func_kwargs.get(""player_name"", None)\n                if actual_player_name is None:\n                    # Assume compute_func is a method and its instance has some\n                    # attribute `player_name`\n                    if compute_func_args:\n                        parent_instance = compute_func_args[0]\n                        actual_player_name = getattr(\n                            parent_instance, ""player_name"", None\n                        )\n                if actual_player_name is None:\n                    # Fallback to error\n                    raise ValueError(\n                        ""\'player_name\' not provided. Please provide ""\n                        ""\'player_name\' as a keyword argument to this ""\n                        ""function, or as an argument to the ""\n                        ""tfe.local_computation decorator.""\n                    )\n\n                return self.define_local_computation(\n                    actual_player_name,\n                    compute_func,\n                    arguments=compute_func_args,\n                    **kwargs,\n                )\n\n            return compute_func_wrapper\n\n        if actual_compute_func is None:\n            # User has not yet passed a compute_func, so we\'ll expect them to\n            # pass it outside of this function\'s scope (e.g. as a decorator).\n            return decorator\n\n        # User has already passed a compute_func, so return the decorated version.\n        return decorator(actual_compute_func)\n\n    def define_local_computation(\n        self,\n        player,\n        computation_fn,\n        arguments=None,\n        apply_scaling=True,\n        name_scope=None,\n        masked=False,\n        factory=None,\n    ):\n        """"""\n    Define a local computation that happens on plaintext tensors.\n\n    :param player: Who performs the computation and gets to see the values in\n        plaintext.\n    :param apply_scaling: Whether or not to scale the outputs.\n    :param name_scope: Optional name to give to this node in the graph.\n    :param masked: Whether or not to produce masked outputs.\n    :param factory: Backing tensor type to use for outputs.\n    """"""  # noqa:E501\n\n        factory = factory or self.tensor_factory\n\n        if isinstance(player, str):\n            player = get_config().get_player(player)\n        assert isinstance(player, Player)\n\n        def share_output(v: tf.Tensor):\n            assert (\n                v.shape.is_fully_defined()\n            ), ""Shape of return value \'{}\' on \'{}\' not fully defined"".format(\n                name_scope if name_scope else """", player.name\n            )\n\n            enc = self._encode(v, apply_scaling, tf_int_type=factory.native_type)\n            w = factory.tensor(enc)\n            x = self._share_and_wrap(w, apply_scaling)\n\n            if not masked:\n                return x\n            with tf.name_scope(""local_mask""):\n                a0 = factory.sample_uniform(v.shape)\n                a1 = factory.sample_uniform(v.shape)\n                a = a0 + a1\n                alpha = w - a\n            return PondMaskedTensor(self, x, a, a0, a1, alpha, alpha, apply_scaling)\n\n        def reconstruct_input(x):\n\n            if not isinstance(x, (AbstractTensor, PondTensor)):\n                return x\n\n            if isinstance(x, PondPublicTensor):\n                w, _ = x.unwrapped\n                v = self._decode(w, x.is_scaled)\n                return v\n\n            if isinstance(x, PondPrivateTensor):\n                x0, x1 = x.unwrapped\n                w = self._reconstruct(x0, x1)\n                v = self._decode(w, x.is_scaled)\n                return v\n\n            if isinstance(x, PondMaskedTensor):\n                x0, x1 = x.unmasked.unwrapped\n                w = self._reconstruct(x0, x1)\n                v = self._decode(w, x.is_scaled)\n                return v\n\n            raise TypeError(\n                (""Don\'t know how to process input argument "" ""of type {}"").format(\n                    type(x)\n                )\n            )\n\n        with tf.name_scope(name_scope if name_scope else ""local-computation""):\n\n            with tf.device(player.device_name):\n                if arguments is None:\n                    inputs = []\n                else:\n                    if not isinstance(arguments, (list, tuple)):\n                        arguments = [arguments]\n\n                    inputs = tf.contrib.framework.nest.map_structure(\n                        reconstruct_input, arguments\n                    )\n\n                outputs = computation_fn(*inputs)\n\n                if isinstance(outputs, tf.Operation):\n                    return outputs\n\n                if isinstance(outputs, tf.Tensor):\n                    return share_output(outputs)\n\n                if isinstance(outputs, (list, tuple)):\n                    return [share_output(output) for output in outputs]\n\n                raise TypeError(\n                    ""Don\'t know how to handle results of ""\n                    ""type {}"".format(type(outputs))\n                )\n\n    def define_private_input(\n        self,\n        player,\n        inputter_fn,\n        apply_scaling: bool = True,\n        name_scope: Optional[str] = None,\n        masked: bool = False,\n        factory: Optional[AbstractFactory] = None,\n    ):\n        """"""\n    Define a private input.\n\n    This represents a `private` input owned by the specified player into the\n    graph.\n\n    :param Union[str,Player] player: Which player owns this input.\n    :param bool apply_scaling: Whether or not to scale the value.\n    :param str name_scope: What name to give to this node in the graph.\n    :param bool masked: Whether or not to mask the input.\n    :param AbstractFactory factory: Which backing type to use for this input\n        (e.g. `int100` or `int64`).\n    """"""\n        return self.define_local_computation(\n            player=player,\n            computation_fn=inputter_fn,\n            arguments=[],\n            apply_scaling=apply_scaling,\n            name_scope=name_scope if name_scope else ""private-input"",\n            masked=masked,\n            factory=factory,\n        )\n\n    def define_output(\n        self, player, arguments, outputter_fn, name_scope=None,\n    ):\n        """"""\n    Define an output for this graph.\n\n    :param player: Which player this output will be sent to.\n    """"""\n\n        def result_wrapper(*args):\n            op = outputter_fn(*args)\n            # wrap in tf.group to prevent sending back any tensors\n            # (which might hence be leaked)\n            return tf.group(op)\n\n        return self.define_local_computation(\n            player=player,\n            computation_fn=result_wrapper,\n            arguments=arguments,\n            name_scope=name_scope if name_scope else ""output"",\n        )\n\n    def _encode(\n        self,\n        rationals: Union[tf.Tensor, np.ndarray],\n        apply_scaling: bool,\n        tf_int_type=None,\n    ) -> Union[tf.Tensor, np.ndarray]:\n        """"""\n    Encode tensor of rational numbers into tensor of ring elements. Output is\n    of same type as input to allow function to be used for constants.\n    """"""\n\n        with tf.name_scope(""encode""):\n\n            # we first scale as needed\n\n            if apply_scaling:\n                scaled = rationals * self.fixedpoint_config.scaling_factor\n            else:\n                scaled = rationals\n\n            # and then we round to integers\n\n            if isinstance(scaled, np.ndarray):\n                integers = scaled.astype(np.int64)\n\n            elif isinstance(scaled, tf.Tensor):\n                tf_int_type = tf_int_type or (\n                    scaled.dtype\n                    if scaled.dtype in TF_INT_TYPES\n                    else self.tensor_factory.native_type\n                )\n                assert tf_int_type in TF_INT_TYPES\n                integers = tf.cast(scaled, dtype=tf_int_type)\n\n            else:\n                raise TypeError(""Don\'t know how to encode {}"".format(type(rationals)))\n\n            # pylint: disable=unidiomatic-typecheck\n            assert type(rationals) == type(integers), (type(rationals), type(integers))\n            # pylint: enable=unidiomatic-typecheck\n            return integers\n\n    @memoize\n    def _decode(\n        self, elements: AbstractTensor, is_scaled: bool,\n    ) -> Union[tf.Tensor, np.ndarray]:\n        """"""Decode tensor of ring elements into tensor of rational numbers.""""""\n\n        with tf.name_scope(""decode""):\n\n            bound = self.fixedpoint_config.bound_single_precision\n            scaled = (elements + bound).to_native() - bound\n\n            if not is_scaled:\n                return scaled\n\n            return scaled / self.fixedpoint_config.scaling_factor\n\n    def _share(self, secret: AbstractTensor,) -> Tuple[AbstractTensor, AbstractTensor]:\n        """"""Secret-share an AbstractTensor.\n\n    Args:\n      secret: `AbstractTensor`, the tensor to share.\n\n    Returns:\n      A pair of `AbstractTensor`, the shares.\n    """"""\n\n        with tf.name_scope(""share""):\n            share0 = secret.factory.sample_uniform(secret.shape)\n            share1 = secret - share0\n\n            # randomized swap to distribute load between two servers wrt who gets\n            # the seed\n            if random.random() < 0.5:\n                share0, share1 = share1, share0\n\n        return share0, share1\n\n    def _share_and_wrap(\n        self, secret: AbstractTensor, is_scaled: bool,\n    ) -> ""PondPrivateTensor"":\n        s0, s1 = self._share(secret)\n        return PondPrivateTensor(self, s0, s1, is_scaled)\n\n    def _reconstruct(self, share0, share1):\n        with tf.name_scope(""reconstruct""):\n            return share0 + share1\n\n    @memoize\n    def assign(\n        self, variable: Union[""PondPrivateVariable"", ""PondPublicVariable""], value,\n    ) -> tf.Operation:\n        """"""See tf.assign.""""""\n\n        if isinstance(variable, PondPrivateVariable):\n            assert isinstance(value, PondPrivateTensor), type(value)\n            assert variable.is_scaled == value.is_scaled, (\n                ""Scaling must match: {}, {}""\n            ).format(variable.is_scaled, value.is_scaled)\n\n            var0, var1 = variable.variable0, variable.variable1\n            val0, val1 = value.share0, value.share1\n\n            with tf.name_scope(""assign""):\n\n                # Having this control_dependencies is important in order to avoid that\n                # computationally-dependent shares are updated in different pace\n                # (e.g., share0 is computed from share1, and we need to make sure that\n                # share1 is NOT already updated).\n                # See https://github.com/tf-encrypted/tf-encrypted/pull/665 for details.\n                with tf.control_dependencies(val0.support + val1.support):\n\n                    with tf.device(self.server_0.device_name):\n                        op0 = var0.assign_from_same(val0)\n\n                    with tf.device(self.server_1.device_name):\n                        op1 = var1.assign_from_same(val1)\n\n                    op = tf.group(op0, op1)\n\n        elif isinstance(variable, PondPublicVariable):\n            assert isinstance(value, PondPublicTensor), type(value)\n            assert (\n                variable.is_scaled == value.is_scaled\n            ), ""Scaling must match: {}, {}"".format(variable.is_scaled, value.is_scaled,)\n\n            var0, var1 = variable.variable_on_0, variable.variable_on_1\n            val0, val1 = value.value_on_0, value.value_on_1\n\n            with tf.name_scope(""assign""):\n\n                with tf.control_dependencies(val0.support + val1.support):\n\n                    with tf.device(self.server_0.device_name):\n                        op0 = var0.assign_from_same(val0)\n\n                    with tf.device(self.server_1.device_name):\n                        op1 = var1.assign_from_same(val1)\n\n                    op = tf.group(op0, op1)\n\n        else:\n            raise TypeError(\n                (""Don\'t know how to handle variable "" ""of type {}"").format(\n                    type(variable)\n                )\n            )\n\n        return op\n\n    def identity(\n        self, x, control_dependencies_0=None, control_dependencies_1=None,\n    ):\n        return self.dispatch(\n            ""identity"",\n            x,\n            control_dependencies_0=control_dependencies_0,\n            control_dependencies_1=control_dependencies_1,\n        )\n\n    @memoize\n    def add(self, x, y):\n        """"""\n    add(x, y) -> PondTensor\n\n    Adds two tensors `x` and `y`.\n\n    :param PondTensor x: The first operand.\n    :param PondTensor y: The second operand.\n    """"""\n        x, y = self.lift(x, y)\n        return self.dispatch(""add"", x, y)\n\n    # pylint: disable=inconsistent-return-statements\n    def lift(self, x, y=None, apply_scaling: Optional[bool] = None):\n        """"""\n    lift(x, y=None, apply_scaling=None) -> PondTensor(s)\n\n    Convenience method for working with mixed typed tensors in programs:\n    combining any of the Pond objects together with e.g. ints and floats\n    will automatically lift the latter into Pond objects.\n\n    :param int,float,PondTensor x: Python object to lift.\n    :param int,float,PondTensor y: Second Python object to lift, optional.\n    :param bool apply_scaling: Whether to apply scaling to the input object(s).\n    """"""\n\n        if y is None:\n\n            if isinstance(x, (int, float)):\n                return self.define_constant(np.array([x]))\n\n            if isinstance(x, PondTensor):\n                return x\n\n            raise TypeError(""Don\'t know how to lift {}"".format(type(x)))\n\n        if isinstance(x, (int, float)):\n\n            if isinstance(y, (int, float)):\n                x = self.define_constant(np.array([x]))\n                y = self.define_constant(np.array([y]))\n                return x, y\n\n            if isinstance(y, PondTensor):\n                x = self.define_constant(\n                    np.array([x]),\n                    apply_scaling=apply_scaling or y.is_scaled,\n                    factory=y.backing_dtype,\n                )\n                return x, y\n\n            raise TypeError(\n                (""Don\'t know how to lift "" ""{}, {}"").format(type(x), type(y))\n            )\n\n        if isinstance(x, PondTensor):\n\n            if isinstance(y, (int, float)):\n                y = self.define_constant(\n                    np.array([y]),\n                    apply_scaling=apply_scaling or x.is_scaled,\n                    factory=x.backing_dtype,\n                )\n                return x, y\n\n            if isinstance(y, PondTensor):\n                return x, y\n\n        raise TypeError((""Don\'t know how to lift "" ""{}, {}"").format(type(x), type(y)))\n\n    # pylint: enable=inconsistent-return-statements\n\n    @memoize\n    def add_n(self, tensors):\n        # TODO(Morten) we could optimize by doing lazy reductions, potentially\n        #              segmenting as needed\n        return reduce(lambda x, y: x + y, tensors)\n\n    @memoize\n    def reduce_sum(self, x, axis=None, keepdims=None):\n        x = self.lift(x)\n        return self.dispatch(""reduce_sum"", x, axis=axis, keepdims=keepdims)\n\n    def sum(self, x, axis=None, keepdims=None):\n        return self.reduce_sum(x, axis, keepdims)\n\n    @memoize\n    def cumsum(self, x, axis=0, exclusive=False, reverse=False):\n        return self.dispatch(\n            ""cumsum"", x, axis=axis, exclusive=exclusive, reverse=reverse,\n        )\n\n    @memoize\n    def sub(self, x, y):\n        x, y = self.lift(x, y)\n        return self.dispatch(""sub"", x, y)\n\n    def mask(self, x):\n        """"""Convert to a PondMaskedTensor.""""""\n        if isinstance(x, (list, tuple)):\n            # apply recursively\n            return [self.mask(xi) for xi in x]\n\n        node_key = (""mask"", x)\n        x_masked = nodes.get(node_key, None)\n\n        if x_masked is not None:\n            return x_masked\n\n        if isinstance(x, PondPrivateTensor):\n            x_masked = _mask_private(self, x)\n\n        else:\n            raise TypeError(""Don\'t know how to mask {}"".format(type(x)))\n\n        nodes[node_key] = x_masked\n        return x_masked\n\n    @memoize\n    def mul(self, x, y):\n        x, y = self.lift(x, y)\n        return self.dispatch(""mul"", x, y)\n\n    @memoize\n    def square(self, x):\n        return self.dispatch(""square"", x)\n\n    @memoize\n    def matmul(self, x: ""PondTensor"", y: ""PondTensor"") -> ""PondTensor"":\n        return self.dispatch(""matmul"", x, y)\n\n    def dot(self, x, y):\n        return self.matmul(x, y)\n\n    def reciprocal(self, x):\n        return self.dispatch(""reciprocal"", x)\n\n    @memoize\n    def div(self, x, y):\n        """"""\n    Performs a true division of `x` by `y` where `y` is public.\n\n    No flooring is performing if `y` is an integer type as it is implicitly\n    treated as a float.\n    """"""\n\n        assert isinstance(x, PondTensor)\n\n        if isinstance(y, float):\n            y_inverse = 1.0 / y\n        if isinstance(y, int):\n            y_inverse = 1.0 / float(y)\n        elif isinstance(y, PondPublicTensor):\n            y_inverse = self.reciprocal(y)\n        else:\n            raise TypeError(""Don\'t know how to divide by type {}"".format(type(y)))\n\n        return self.mul(x, y_inverse)\n\n    @memoize\n    def sqrt(self, x):\n        """"""\n    sqrt(x) -> PondTensor\n\n    Computes numerical square root value element-wise.\n\n    :param PondTensor x: Input tensor.\n    """"""\n        return self.dispatch(""sqrt"", x)\n\n    @memoize\n    def truncate(self, x: ""PondTensor""):\n        return self.dispatch(""truncate"", x)\n\n    @memoize\n    def indexer(self, x: ""PondTensor"", slc) -> ""PondTensor"":\n        return self.dispatch(""indexer"", x, slc)\n\n    def transpose(self, x, perm=None) -> ""PondTensor"":\n        """"""\n    transpose(x, perm=None) -> PondTensor\n\n    Transposes the input `x`, or permutes the axes of `x` if `perm` is given.\n\n    :param PondTensor x: The tensor to transpose or permute.\n    :param List perm: A permutation of axis indices.\n    """"""\n\n        node_key = (""transpose"", x)\n        x_t = nodes.get(node_key, None)\n\n        if x_t is not None:\n            return x_t\n\n        if isinstance(x, PondPublicTensor):\n            x_t = _transpose_public(self, x, perm=perm)\n\n        elif isinstance(x, PondPrivateTensor):\n            x_t = _transpose_private(self, x, perm=perm)\n\n        elif isinstance(x, PondMaskedTensor):\n            x_t = _transpose_masked(self, x, perm=perm)\n\n        else:\n            raise TypeError(""Don\'t know how to transpose {}"".format(type(x)))\n\n        nodes[node_key] = x_t\n        return x_t\n\n    @memoize\n    def reshape(self, x: ""PondTensor"", shape: List[int]):\n        """"""\n    reshape(x, shape) -> PondTensor\n\n    Reshape `x` into a tensor with a new `shape`.\n\n    :param PondTensor x: Input tensor.\n    :param (int,...) shape: Shape of output tensor.\n    """"""\n\n        if isinstance(x, PondPublicTensor):\n            return _reshape_public(self, x, shape)\n\n        if isinstance(x, PondPrivateTensor):\n            return _reshape_private(self, x, shape)\n\n        if isinstance(x, PondMaskedTensor):\n            return _reshape_masked(self, x, shape)\n\n        raise TypeError(""Don\'t know how to reshape {}"".format(type(x)))\n\n    @memoize\n    def negative(self, x: ""PondTensor""):\n        """"""\n    negative(x) -> PondTensor\n\n    Computes numerical negative value element-wise.\n\n    :param PondTensor x: Input tensor.\n    """"""\n        return self.dispatch(""negative"", x)\n\n    @memoize\n    def expand_dims(self, x: ""PondTensor"", axis=None):\n        """"""See tf.expand_dims.""""""\n\n        if isinstance(x, PondPublicTensor):\n            return _expand_dims_public(self, x, axis=axis)\n\n        if isinstance(x, PondPrivateTensor):\n            return _expand_dims_private(self, x, axis=axis)\n\n        if isinstance(x, PondMaskedTensor):\n            return _expand_dims_masked(self, x, axis=axis)\n\n        raise TypeError(""Don\'t know how to expand dims {}"".format(type(x)))\n\n    @memoize\n    def squeeze(self, x: ""PondTensor"", axis: Optional[List[int]] = None):\n        """"""See tf.squeeze.""""""\n\n        if isinstance(x, PondPublicTensor):\n            return _squeeze_public(self, x, axis)\n\n        if isinstance(x, PondPrivateTensor):\n            return _squeeze_private(self, x, axis)\n\n        if isinstance(x, PondMaskedTensor):\n            return _squeeze_masked(self, x, axis)\n\n        raise TypeError(""Don\'t know how to squeeze {}"".format(type(x)))\n\n    def strided_slice(self, x: ""PondTensor"", *args: Any, **kwargs: Any):\n        """"""\n    strided_slice(x, *args, **kwargs) -> PondTensor\n\n    See https://www.tensorflow.org/api_docs/python/tf/strided_slice for further\n    documentation.\n    """"""\n\n        node_key = (""strided_slice"", x)\n\n        x_sliced = nodes.get(node_key, None)\n\n        if x_sliced is not None:\n            return x_sliced\n\n        if isinstance(x, PondPublicTensor):\n            x_sliced = _strided_slice_public(self, x, args, kwargs)\n        elif isinstance(x, PondPrivateTensor):\n            x_sliced = _strided_slice_private(self, x, args, kwargs)\n        elif isinstance(x, PondMaskedTensor):\n            x_sliced = _strided_slice_masked(self, x, args, kwargs)\n            nodes[(""strided_slice"", x.unmasked)] = x_sliced.unmasked\n        else:\n            raise TypeError(\n                (""Don\'t know how to do a strided slice on "" "" {}"").format(type(x))\n            )\n\n        nodes[node_key] = x_sliced\n\n        return x_sliced\n\n    @memoize\n    def gather(self, x: ""PondTensor"", indices: list, axis=0) -> ""PondTensor"":\n        return self.dispatch(""gather"", x, indices, axis=axis)\n\n    @memoize\n    def split(\n        self, x: ""PondTensor"", num_split: Union[int, list], axis=0\n    ) -> List[""PondTensor""]:\n        return self.dispatch(""split"", x, num_split, axis=axis)\n\n    def stack(self, xs: List[""PondTensor""], axis=0):\n        """"""See tf.stack.""""""\n\n        node_key = (""stack"", tuple(xs))\n        xs_stack = nodes.get(node_key, None)\n\n        if xs_stack is not None:\n            return xs_stack\n\n        if all([isinstance(x, PondPublicTensor) for x in xs]):\n            xs_stack = _stack_public(self, xs, axis=axis)\n\n        elif all([isinstance(x, PondPrivateTensor) for x in xs]):\n            xs_stack = _stack_private(self, xs, axis=axis)\n\n        elif all([isinstance(x, PondMaskedTensor) for x in xs]):\n            xs_stack = _stack_masked(self, xs, axis=axis)\n        else:\n            raise TypeError(""Don\'t know how to do a stack {}"".format(type(xs)))\n\n        nodes[node_key] = xs_stack\n\n        return xs_stack\n\n    @memoize\n    def concat(self, xs: List[""PondTensor""], axis):\n        """"""See tf.concat.""""""\n\n        if all(isinstance(x, PondPublicTensor) for x in xs):\n            return _concat_public(self, xs, axis=axis)\n\n        if all(isinstance(x, PondPrivateTensor) for x in xs):\n            return _concat_private(self, xs, axis=axis)\n\n        if all(isinstance(x, PondMaskedTensor) for x in xs):\n            return _concat_masked(self, xs, axis=axis)\n\n        raise TypeError(""Don\'t know how to do a concat {}"".format(type(xs)))\n\n    @memoize\n    def sigmoid(self, x: ""PondTensor""):\n        """"""A Chebyshev polynomial approximation of the sigmoid function.""""""\n        assert isinstance(x, PondTensor), type(x)\n\n        w0 = 0.5\n        w1 = 0.2159198015\n        w3 = -0.0082176259\n        w5 = 0.0001825597\n        w7 = -0.0000018848\n        w9 = 0.0000000072\n\n        with tf.name_scope(""sigmoid""):\n\n            # TODO[Morten] try in single round\n            x1 = x\n            x2 = x1.square()\n            x3 = x2 * x\n            x5 = x2 * x3\n            x7 = x2 * x5\n            x9 = x2 * x7\n\n            y1 = x1 * w1\n            y3 = x3 * w3\n            y5 = x5 * w5\n            y7 = x7 * w7\n            y9 = x9 * w9\n\n            z = y9 + y7 + y5 + y3 + y1 + w0\n            # z = y7 + y5 + y3 + y1 + w0\n\n        return z\n\n    @memoize\n    def relu(self, x: ""PondTensor"", **kwargs):  # pylint: disable=unused-argument\n        """"""A Chebyshev polynomial approximation of the ReLU function.""""""\n        assert isinstance(x, PondTensor), type(x)\n\n        w0 = 0.44015372000819103\n        w1 = 0.500000000\n        w2 = 0.11217537671414643\n        w4 = -0.0013660836712429923\n        w6 = 9.009136367360004e-06\n        w8 = -2.1097433984e-08\n\n        with tf.name_scope(""relu""):\n\n            x1 = x\n            x2 = x.square()\n            x4 = x2 * x2\n            x6 = x2 * x4\n            x8 = x2 * x6\n\n            y1 = x1 * w1\n            y2 = x2 * w2\n            y4 = x4 * w4\n            y6 = x6 * w6\n            y8 = x8 * w8\n\n            z = y8 + y6 + y4 + y2 + y1 + w0\n\n        return z\n\n    @memoize\n    def tanh(self, x: ""PondTensor""):\n        """"""\n    A Chebyshev polynomial approximation of the hyperbolic tangent function.\n    """"""\n        assert isinstance(x, PondTensor), type(x)\n\n        w0 = 0.0\n        w1 = 0.852721056\n        w3 = -0.12494112\n        w5 = 0.010654528\n        w7 = -0.000423424\n\n        with tf.name_scope(""relu""):\n\n            x1 = x\n            x2 = x.square()\n            x3 = x2 * x1\n            x5 = x2 * x3\n            x7 = x2 * x5\n\n            y1 = x1 * w1\n            y3 = x3 * w3\n            y5 = x5 * w5\n            y7 = x7 * w7\n\n            z = y7 + y5 + y3 + y1 + w0\n\n        return z\n\n    def log(self, x: ""PondTensor""):\n        """"""\n    A Chebyshev polynomial approximation of the hyperbolic tangent function.\n    """"""\n        assert isinstance(x, PondTensor), type(x)\n\n        w0 = -3.35674972\n        w1 = 12.79333646\n        w2 = -26.18955259\n        w3 = 30.24596692\n        w4 = -17.30367641\n        w5 = 3.82474222\n\n        with tf.name_scope(""log""):\n\n            x1 = x\n            x2 = x.square()\n            x3 = x2 * x1\n            x4 = x3 * x1\n            x5 = x2 * x3\n\n            y1 = x1 * w1\n            y2 = x2 * w2\n            y3 = x3 * w3\n            y4 = x4 * w4\n            y5 = x5 * w5\n\n            z = y5 + y4 + y3 + y2 + y1 + w0\n\n        return z\n\n    @memoize\n    def reveal(self, x):\n        return self.dispatch(""reveal"", x)\n\n    def cache(self, xs):\n        """"""\n    Wraps all input tensors, including private and masked, in variables so\n    that computation and masking of these can be reused between different\n    runs.\n\n    For private predictions this may be used to avoid re-masking model\n    weights between each run, thereby saving on communication.\n    For private training this may be used to avoid re-masked the traning\n    data between each iteration, again saving on communication.\n    """"""\n\n        if isinstance(xs, (list, tuple)):\n            # apply recursively\n            updaters, cached = zip(*[self.cache(x) for x in xs])\n            return tf.group(*updaters), cached\n\n        # base case\n        node_key = (""cache"", xs)\n        cached = nodes.get(node_key, None)\n\n        if cached is not None:\n            return cached\n\n        dispatch = {\n            PondPublicTensor: _cache_public,\n            PondPrivateTensor: _cache_private,\n            PondMaskedTensor: _cache_masked,\n        }\n        func = dispatch.get(_type(xs), None)\n        if func is None:\n            raise TypeError(""Don\'t know how to cache {}"".format(type(xs)))\n\n        updater, cached = func(self, xs)\n        nodes[node_key] = cached\n\n        return updater, cached\n\n    def conv2d(self, x, w, strides, padding):\n        """"""See tf.nn.conv2d.""""""\n\n        node_key = (""conv2d"", x, w, strides, padding)\n        z = nodes.get(node_key, None)\n\n        if z is not None:\n            return z\n\n        dispatch = {\n            (PondPublicTensor, PondPublicTensor): _conv2d_public_public,\n            (PondPublicTensor, PondPrivateTensor): _conv2d_public_private,\n            (PondPublicTensor, PondMaskedTensor): _conv2d_public_masked,\n            (PondPrivateTensor, PondPublicTensor): _conv2d_private_public,\n            (PondPrivateTensor, PondPrivateTensor): _conv2d_private_private,\n            (PondPrivateTensor, PondMaskedTensor): _conv2d_private_masked,\n            (PondMaskedTensor, PondPublicTensor): _conv2d_masked_public,\n            (PondMaskedTensor, PondPrivateTensor): _conv2d_masked_private,\n            (PondMaskedTensor, PondMaskedTensor): _conv2d_masked_masked,\n        }\n\n        func = dispatch.get((_type(x), _type(w)), None)\n        if func is None:\n            raise TypeError(\n                ""Don\'t know how to conv2d {} and {}"".format(type(x), type(w))\n            )\n\n        z = func(self, x, w, strides, padding)\n        nodes[node_key] = z\n\n        return z\n\n    def maxpool2d(self, x, pool_size, strides, padding):\n        raise NotImplementedError(""Only SecureNN supports Max Pooling"")\n\n    def avgpool2d(self, x, pool_size, strides, padding):\n        """"""See tf.nn.avgpool2d.""""""\n        node_key = (""avgpool2d"", x, tuple(pool_size), tuple(strides), padding)\n        z = nodes.get(node_key, None)\n\n        if z is not None:\n            return z\n\n        dispatch = {\n            PondPublicTensor: _avgpool2d_public,\n            PondPrivateTensor: _avgpool2d_private,\n            PondMaskedTensor: _avgpool2d_masked,\n        }\n\n        func = dispatch.get(_type(x), None)\n        if func is None:\n            raise TypeError(""Don\'t know how to avgpool2d {}"".format(type(x)))\n\n        z = func(self, x, pool_size, strides, padding)\n        nodes[node_key] = z\n\n        return z\n\n    def batch_to_space_nd(self, x, block_shape, crops):\n        return self.dispatch(""batch_to_space_nd"", x, block_shape, crops)\n\n    def space_to_batch_nd(self, x, block_shape, paddings):\n        return self.dispatch(""space_to_batch_nd"", x, block_shape, paddings)\n\n    @memoize\n    def equal(self, x, y):\n        x, y = self.lift(x, y)\n        return self.dispatch(""equal"", x, y)\n\n    def dispatch(self, base_name, *args, container=None, **kwargs):\n        """"""\n    Finds the correct protocol logic to perform based on the dispatch_id\n    attribute of the input tensors in args.\n    """"""\n        suffix = ""_"".join(\n            [arg.dispatch_id for arg in args if hasattr(arg, ""dispatch_id"")]\n        )\n        func_name = ""_{}_{}"".format(base_name, suffix)\n\n        if container is None:\n            container = _THISMODULE\n\n        func = getattr(container, func_name, None)\n        if func is not None:\n            return func(self, *args, **kwargs)  # pylint: disable=not-callable\n        raise TypeError(\n            (""Don\'t know how to {}: "" ""{}"").format(\n                base_name, [type(arg) for arg in args]\n            )\n        )\n\n    def pad(self, x: ""PondTensor"", paddings: list):\n        """"""See tf.pad.""""""\n\n        backing_type = x.backing_dtype\n\n        if isinstance(x, PondPublicTensor):\n            tensor_type = PondPublicTensor\n        elif isinstance(x, PondPrivateTensor):\n            tensor_type = PondPrivateTensor\n        else:\n            raise ValueError(""Don\'t know how to handle type {}"".format(type(x)))\n\n        def zeros(shape):\n            # NOTE\n            # this is a cheating way of getting zeros for the case where tensor_type\n            # is private, in particular because non-interactive truncation may fail\n            # if applied to these tensors only; for this reason we here use the\n            # assumption that truncation will only ever be applied after these zeros\n            # have been mix with proper shares\n\n            with tf.device(self.server_0.device_name):\n                zval0 = tf.zeros(shape, dtype=backing_type.native_type)\n                zeros0 = backing_type.tensor(zval0)\n\n            with tf.device(self.server_1.device_name):\n                zval1 = tf.zeros(shape, dtype=backing_type.native_type)\n                zeros1 = backing_type.tensor(zval1)\n\n            return tensor_type(self, zeros0, zeros1, True)\n\n        def prepend_zeros(tensor, pad_amt, axis):\n\n            with tf.name_scope(""prepend""):\n\n                if pad_amt == 0:\n                    return tensor\n\n                padshape = tuple(\n                    dim if i != axis else pad_amt\n                    for (i, dim) in enumerate(tensor.shape.as_list())\n                )\n\n                return self.concat([zeros(padshape), tensor], axis=axis)\n\n        def append_zeros(tensor, pad_amt, axis):\n\n            with tf.name_scope(""append""):\n\n                if pad_amt == 0:\n                    return tensor\n\n                padshape = tuple(\n                    dim if i != axis else pad_amt\n                    for (i, dim) in enumerate(tensor.shape.as_list())\n                )\n\n                return self.concat([tensor, zeros(padshape)], axis=axis)\n\n        with tf.name_scope(""pad""):\n            for axis, (pad_before, pad_after) in enumerate(paddings):\n                x = append_zeros(x, pad_after, axis)\n                x = prepend_zeros(x, pad_before, axis)\n\n        return x\n\n\n#\n# Queue classes\n#\n\n\nclass AdditiveFIFOQueue(AbstractFIFOQueue):\n    """"""\n  FIFOQueue for holding two-additive shared tensors.\n  """"""\n\n    def __init__(\n        self, protocol, server_0, server_1, capacity, dtype, shape, shared_name,\n    ):\n\n        self.protocol = protocol\n        self.server_0 = server_0\n        self.server_1 = server_1\n        self.capacity = capacity\n        self.dtype = dtype\n        self.shape = shape\n        self.is_scaled = True  # TODO(Morten) should get this from the dtype\n\n        # TODO(Morten) this is not taking eg int100 into account\n        native_dtype = dtype.native_type\n        native_shape = shape\n\n        with tf.device(self.server_0.device_name):\n            self.queue0 = tf.queue.FIFOQueue(\n                capacity=capacity,\n                dtypes=[native_dtype],\n                shapes=[native_shape],\n                shared_name=""{}-0"".format(shared_name) if shared_name else None,\n            )\n\n        with tf.device(self.server_1.device_name):\n            self.queue1 = tf.queue.FIFOQueue(\n                capacity=capacity,\n                dtypes=[native_dtype],\n                shapes=[native_shape],\n                shared_name=""{}-1"".format(shared_name) if shared_name else None,\n            )\n\n    def size(self):\n        return self.queue0.size()\n\n    def enqueue(self, tensor):\n        assert isinstance(tensor, PondPrivateTensor), type(tensor)\n        assert tensor.backing_dtype == self.dtype\n        assert tensor.shape == self.shape\n        assert tensor.is_scaled == self.is_scaled\n\n        tensor0, tensor1 = tensor.unwrapped\n        # TODO(Morten) this it not taking eg int100 into account\n        raw_tensor0 = tensor0.value\n        raw_tensor1 = tensor1.value\n\n        with tf.device(self.server_0.device_name):\n            enqueue_op0 = self.queue0.enqueue(raw_tensor0)\n\n        with tf.device(self.server_1.device_name):\n            enqueue_op1 = self.queue1.enqueue(raw_tensor1)\n\n        enqueue_op = tf.group(enqueue_op0, enqueue_op1)\n        return enqueue_op\n\n    def dequeue(self):\n\n        with tf.device(self.server_0.device_name):\n            raw_tensor0 = self.queue0.dequeue()\n            tensor_0 = self.dtype.tensor(raw_tensor0)\n\n        with tf.device(self.server_1.device_name):\n            raw_tensor1 = self.queue1.dequeue()\n            tensor_1 = self.dtype.tensor(raw_tensor1)\n\n        return PondPrivateTensor(self.protocol, tensor_0, tensor_1, self.is_scaled,)\n\n\n#\n# Classes representing the base values in the Pond protocol.\n#\n\n\nclass PondTensor(abc.ABC):\n    """"""\n  This class functions mostly as a convenient way of exposing operations\n  directly on the various tensor objects, ie allowing one to write `x + y`\n  instead of `prot.add(x, y)`. Since this functionality is shared among all\n  tensors we put it in this superclass.\n\n  This class should never be instantiated on its own.\n  Instead you should use your chosen protocols factory methods::\n\n      x = prot.define_private_input(tf.constant(np.array([1,2,3,4])))\n      y = prot.define_public_input(tf.constant(np.array([4,5,6,7])))\n\n      z = x + y\n\n      with config.Session() as sess:\n          answer = z.reveal().eval(sess)\n\n          print(answer) # => [5, 7, 9, 11]\n  """"""\n\n    def __init__(self, prot, is_scaled):\n        self.prot = prot\n        self.is_scaled = is_scaled\n\n    @property\n    @abc.abstractmethod\n    def shape(self) -> List[int]:\n        """"""\n    :rtype: List[int]\n    :returns: The shape of this tensor.\n    """"""\n\n    @property\n    @abc.abstractmethod\n    def unwrapped(self) -> Tuple[AbstractTensor, ...]:\n        pass\n\n    def add(self, other):\n        """"""\n    Add `other` to this PondTensor.  This can be another tensor with the same\n    backing or a primitive.\n\n    This function returns a new PondTensor and does not modify this one.\n\n    :param PondTensor other: a or primitive (e.g. a float)\n    :return: A new PondTensor with `other` added.\n    :rtype: PondTensor\n    """"""\n        return self.prot.add(self, other)\n\n    def __add__(self, other):\n        """"""\n    See :meth:`~tf_encrypted.protocol.pond.PondTensor.add`\n    """"""\n        return self.prot.add(self, other)\n\n    def __radd__(self, other):\n        return other.prot.add(other, self)\n\n    def reduce_sum(self, axis=None, keepdims=None):\n        """"""\n    Like :meth:`tensorflow.reduce_sum`\n\n    :param int axis:  The axis to reduce along\n    :param bool keepdims: If true, retains reduced dimensions with length 1.\n    :return: A new PondTensor\n    :rtype: PondTensor\n    """"""\n        return self.prot.reduce_sum(self, axis, keepdims)\n\n    def sum(self, axis=None, keepdims=None):\n        """"""\n    See :meth:`PondTensor.reduce_sum`\n    """"""\n        return self.reduce_sum(axis, keepdims)\n\n    def sub(self, other):\n        """"""\n    Subtract `other` from this tensor.\n\n    :param PondTensor other: to subtract\n    :return: A new PondTensor\n    :rtype: PondTensor\n    """"""\n        return self.prot.sub(self, other)\n\n    def __sub__(self, other):\n        return self.prot.sub(self, other)\n\n    def __rsub__(self, other):\n        return self.prot.sub(other, self)\n\n    def mul(self, other):\n        """"""\n    Multiply this tensor with `other`\n\n    :param PondTensor other: to multiply\n    :return: A new PondTensor\n    :rtype: PondTensor\n    """"""\n        return self.prot.mul(self, other)\n\n    def __mul__(self, other):\n        return self.prot.mul(self, other)\n\n    def __rmul__(self, other):\n        return self.prot.mul(self, other)\n\n    def __truediv__(self, other):\n        return self.prot.div(self, other)\n\n    def __mod__(self, other):\n        return self.prot.mod(self, other)\n\n    def square(self):\n        """"""\n    Square this tensor.\n\n    :return: A new PondTensor\n    :rtype: PondTensor\n    """"""\n        return self.prot.square(self)\n\n    def matmul(self, other):\n        """"""\n    MatMul this tensor with `other`.  This will perform matrix multiplication,\n    rather than elementwise like\n    :meth:`~tf_encrypted.protocol.pond.PondTensor.mul`\n\n    :param PondTensor other: to subtract\n    :return: A new PondTensor\n    :rtype: PondTensor\n    """"""\n        return self.prot.matmul(self, other)\n\n    def dot(self, other):\n        """"""\n    Alias for :meth:`~tf_encrypted.protocol.pond.PondTensor.matmul`\n\n    :return: A new PondTensor\n    :rtype: PondTensor\n    """"""\n        return self.matmul(other)\n\n    def __getitem__(self, slc):\n        return self.prot.indexer(self, slc)\n\n    def transpose(self, perm=None):\n        """"""\n    Transpose this tensor.\n\n    See :meth:`tensorflow.transpose`\n\n    :param List[int]: A permutation of the dimensions of this tensor.\n\n    :return: A new PondTensor\n    :rtype: PondTensor\n    """"""\n        return self.prot.transpose(self, perm)\n\n    def truncate(self):\n        """"""\n    Truncate this tensor.\n\n    `TODO`\n\n    :return: A new PondTensor\n    :rtype: PondTensor\n    """"""\n        return self.prot.truncate(self)\n\n    def expand_dims(self, axis=None):\n        """"""\n    :See: tf.expand_dims\n\n    :return: A new PondTensor\n    :rtype: PondTensor\n    """"""\n        return self.prot.expand_dims(self, axis=axis)\n\n    def reshape(self, shape: List[int]) -> ""PondTensor"":\n        """"""\n    :See: tf.reshape\n\n    :param List[int] shape: The new shape of the tensor.\n    :rtype: PondTensor\n    :returns: A new tensor with the contents of this tensor, but with the new\n        specified shape.\n    """"""\n        return self.prot.reshape(self, shape)\n\n    def negative(self) -> ""PondTensor"":\n        """"""\n    :See: tf.negative\n\n    :rtype: PondTensor\n    :returns: A new tensor with numerical negative value element-wise computed.\n    """"""\n        return self.prot.negative(self)\n\n    def reduce_max(self, axis: int) -> ""PondTensor"":\n        """"""\n    :See: tf.reduce_max\n\n    :param int axis: The axis to take the max along\n    :rtype: PondTensor\n    :returns: A new pond tensor with the max value from each axis.\n    """"""\n        return self.prot.reduce_max(self, axis)\n\n\nclass PondPublicTensor(PondTensor):\n    """"""\n  This class represents a public tensor, known by at least the two servers\n  but potentially known by more. Although there is only a single value we\n  replicate it on both servers to avoid sending it from one to the other\n  in the operations where it\'s needed by both (eg multiplication).\n  """"""\n\n    dispatch_id = ""public""\n\n    def __init__(\n        self,\n        prot: Pond,\n        value_on_0: AbstractTensor,\n        value_on_1: AbstractTensor,\n        is_scaled: bool,\n    ) -> None:\n        assert isinstance(value_on_0, AbstractTensor), type(value_on_0)\n        assert isinstance(value_on_1, AbstractTensor), type(value_on_1)\n        assert value_on_0.shape == value_on_1.shape\n\n        super(PondPublicTensor, self).__init__(prot, is_scaled)\n        self.value_on_0 = value_on_0\n        self.value_on_1 = value_on_1\n\n    def __repr__(self) -> str:\n        return ""PondPublicTensor(shape={})"".format(self.shape)\n\n    @property\n    def shape(self) -> List[int]:\n        return self.value_on_0.shape\n\n    @property\n    def backing_dtype(self):\n        return self.value_on_0.factory\n\n    @property\n    def unwrapped(self) -> Tuple[AbstractTensor, ...]:\n        """"""\n    Unwrap the tensor.\n\n    This will return the value for each of the parties that collectively own\n    the tensor.\n\n    In most cases, this will be the same value on each device.\n\n    .. code-block:: python\n\n        x_0, y_0 = tensor.unwrapped\n        # x_0 == 10 with the value pinned to player_0\'s device.\n        # y_0 == 10 with the value pinned to player_1\'s device.\n\n    In most cases you will want to work on this data on the specified device.\n\n    .. code-block:: python\n\n        x_0, y_0 = tensor.unwrapped\n\n        with tf.device(prot.player_0.device_name):\n            # act on x_0\n\n        with tf.device(prot.player_1.device_name):\n            # act on y_0\n\n    In most cases you will not need to use this method.  All functions\n    will hide this functionality for you (e.g. `add`, `mul`, etc).\n    """"""\n        return (self.value_on_0, self.value_on_1)\n\n    def decode(self) -> Union[np.ndarray, tf.Tensor]:\n        return self.prot._decode(self.value_on_0, self.is_scaled)\n\n    def to_native(self):\n        return self.decode()\n\n\nclass PondPrivateTensor(PondTensor):\n    """"""\n  This class represents a private value that may be unknown to everyone.\n  """"""\n\n    dispatch_id = ""private""\n\n    def __init__(\n        self,\n        prot: Pond,\n        share0: AbstractTensor,\n        share1: AbstractTensor,\n        is_scaled: bool,\n    ) -> None:\n        assert isinstance(share0, AbstractTensor), type(share0)\n        assert isinstance(share1, AbstractTensor), type(share1)\n        assert share0.shape == share1.shape\n\n        super(PondPrivateTensor, self).__init__(prot, is_scaled)\n        self.share0 = share0\n        self.share1 = share1\n\n    def __repr__(self) -> str:\n        return ""PondPrivateTensor(shape={})"".format(self.shape)\n\n    @property\n    def shape(self) -> List[int]:\n        return self.share0.shape\n\n    @property\n    def backing_dtype(self):\n        return self.share0.factory\n\n    @property\n    def unwrapped(self) -> Tuple[AbstractTensor, ...]:\n        """"""\n    Unwrap the tensor.\n\n    This will return the shares for each of the parties that collectively own\n    he tensor.\n\n    .. code-block:: python\n\n        x_0, y_0 = tensor.unwrapped\n        # x_0 == private shares of the value pinned to player_0\'s device.\n        # y_0 == private shares of the value pinned to player_1\'s device.\n\n    In most cases you will not need to use this method. All functions\n    will hide this functionality for you (e.g. `add`, `mul`, etc).\n    """"""\n        return (self.share0, self.share1)\n\n    def reveal(self) -> PondPublicTensor:\n        return self.prot.reveal(self)\n\n\nclass PondMaskedTensor(PondTensor):\n    """"""\n  This class is part of an optimization where values are only ever masked\n  once as opposed to for every operation in which they are used. As such\n  it represents a private value with additional data associated, namely\n  the masks used for the shares on the two servers as well as on the\n  crypto provider. For convenience it keeps a reference to the unmasked\n  value as well (in the form of a private tensor).\n  """"""\n\n    dispatch_id = ""masked""\n\n    def __init__(\n        self,\n        prot: Pond,\n        unmasked: PondPrivateTensor,\n        a: AbstractTensor,\n        a0: AbstractTensor,\n        a1: AbstractTensor,\n        alpha_on_0: AbstractTensor,\n        alpha_on_1: AbstractTensor,\n        is_scaled: bool,\n    ) -> None:\n        assert isinstance(unmasked, PondPrivateTensor)\n\n        super(PondMaskedTensor, self).__init__(prot, is_scaled)\n        self.unmasked = unmasked\n        self.a = a\n        self.a0 = a0\n        self.a1 = a1\n        self.alpha_on_0 = alpha_on_0\n        self.alpha_on_1 = alpha_on_1\n\n    def __repr__(self) -> str:\n        return ""PondMaskedTensor(shape={})"".format(self.shape)\n\n    @property\n    def shape(self) -> List[int]:\n        return self.a.shape\n\n    @property\n    def backing_dtype(self):\n        return self.a.factory\n\n    @property\n    def unwrapped(self) -> Tuple[AbstractTensor, ...]:\n        return (self.a, self.a0, self.a1, self.alpha_on_0, self.alpha_on_1)\n\n    def reveal(self) -> PondPublicTensor:\n        return self.prot.reveal(self.unmasked)\n\n\n#\n# Extentions of the base Pond classes that record extra information\n# relevant to how TensorFlow works.\n#\n\n\nclass PondConstant(PondPublicTensor):\n    """"""\n  This class essentially represents a public value, however it additionally\n  records the fact that the underlying value was declared as a constant.\n  """"""\n\n    def __init__(self, prot, constant_on_0, constant_on_1, is_scaled):\n        assert isinstance(constant_on_0, AbstractConstant), type(constant_on_0)\n        assert isinstance(constant_on_1, AbstractConstant), type(constant_on_1)\n        assert constant_on_0.shape == constant_on_1.shape\n\n        super(PondConstant, self).__init__(\n            prot, constant_on_0, constant_on_1, is_scaled,\n        )\n        self.constant_on_0 = constant_on_0\n        self.constant_on_1 = constant_on_1\n\n    def __repr__(self) -> str:\n        return ""PondConstant(shape={})"".format(self.shape)\n\n\nclass PondPublicPlaceholder(PondPublicTensor):\n    """"""\n  This class essentially represents a public value, however it additionally\n  records the fact that the backing tensor was declared as a placeholder in\n  order to allow treating it as a placeholder itself.\n  """"""\n\n    def __init__(self, prot, placeholder_on_0, placeholder_on_1, is_scaled):\n        assert isinstance(placeholder_on_0, AbstractPlaceholder), type(placeholder_on_0)\n        assert isinstance(placeholder_on_0, AbstractPlaceholder), type(placeholder_on_1)\n        assert placeholder_on_0.shape == placeholder_on_1.shape\n\n        super(PondPublicPlaceholder, self).__init__(\n            prot, placeholder_on_0, placeholder_on_1, is_scaled,\n        )\n        self.placeholder_on_0 = placeholder_on_0\n        self.placeholder_on_1 = placeholder_on_1\n\n    def __repr__(self) -> str:\n        return ""PondPublicPlaceholder(shape={})"".format(self.shape)\n\n    def feed(self, value):\n        """"""\n    Feed `value` to placeholder\n    """"""\n        assert isinstance(value, np.ndarray), type(value)\n        enc = self.prot._encode(value, self.is_scaled)\n        feed0 = self.placeholder_on_0.feed(enc)\n        feed1 = self.placeholder_on_1.feed(enc)\n        return {**feed0, **feed1}\n\n\nclass PondPrivatePlaceholder(PondPrivateTensor):\n    """"""\n  This class essentially represents a private value, however it additionally\n  records the fact that the backing tensor was declared as a placeholder in\n  order to allow treating it as a placeholder itself.\n  """"""\n\n    def __init__(self, prot, placeholder0, placeholder1, is_scaled):\n        assert isinstance(placeholder0, AbstractPlaceholder), type(placeholder0)\n        assert isinstance(placeholder1, AbstractPlaceholder), type(placeholder1)\n        assert placeholder0.shape == placeholder1.shape\n\n        super().__init__(prot, placeholder0, placeholder1, is_scaled)\n        self.placeholder0 = placeholder0\n        self.placeholder1 = placeholder1\n\n    def __repr__(self) -> str:\n        return ""PondPrivatePlaceholder(shape={})"".format(self.shape)\n\n    def feed(self, value):\n        """"""\n    Feed `value` to placeholder\n    """"""\n        assert isinstance(value, np.ndarray), type(value)\n        enc = self.prot._encode(value, self.is_scaled)\n        assert isinstance(enc, np.ndarray)\n\n        # x0, x1 = self.prot._share(enc)\n        # assert isinstance(x0, np.ndarray), type(x0)\n        # assert isinstance(x1, np.ndarray), type(x1)\n\n        # TODO(Morten)\n        #\n        # This is a huge hack and it would be better to use `_share` as above.\n        # However, _share currently expects its inputs to be TFE tensors backed\n        # by tf.Tensors in order to have extra information attached, and not sure\n        # we should change this until we\'ve least considered what will happen with\n        # TF2 and eager mode.\n        #\n        # So, to ensure that feeding can be done locally *outside* the TF graph,\n        # in the mean time we manually share values here, avoiding a call to\n        # `factory.tensor` as that\'s where tensors are converted to tf.Tensors.\n        shape = self.shape\n        minval = self.backing_dtype.min\n        maxval = self.backing_dtype.max\n        # TODO(Morten) not using secure randomness here; reconsider after TF2\n        x0 = np.array(\n            [random.randrange(minval, maxval) for _ in range(np.product(shape))]\n        ).reshape(shape)\n        x1 = enc - x0\n        assert isinstance(x0, np.ndarray)\n        assert isinstance(x1, np.ndarray)\n\n        feed0 = self.placeholder0.feed(x0)\n        feed1 = self.placeholder1.feed(x1)\n        return {**feed0, **feed1}\n\n\nclass PondPublicVariable(PondPublicTensor):\n    """"""\n  This class essentially represents a public value, however it additionally\n  records the fact that the backing tensor was declared as a variable in\n  order to allow treating it as a variable itself.\n  """"""\n\n    def __init__(self, prot, variable_on_0, variable_on_1, is_scaled):\n        assert isinstance(variable_on_0, AbstractVariable), type(variable_on_0)\n        assert isinstance(variable_on_1, AbstractVariable), type(variable_on_1)\n        assert variable_on_0.shape == variable_on_1.shape\n\n        super(PondPublicVariable, self).__init__(\n            prot, variable_on_0, variable_on_1, is_scaled,\n        )\n        self.variable_on_0 = variable_on_0\n        self.variable_on_1 = variable_on_1\n        self.initializer = tf.group(\n            *[var.initializer for var in [variable_on_0, variable_on_1]]\n        )\n\n    def __repr__(self) -> str:\n        return ""PondPublicVariable(shape={})"".format(self.shape)\n\n\nclass PondPrivateVariable(PondPrivateTensor):\n    """"""\n  This class essentially represents a private value, however it additionally\n  records the fact that the backing tensor was declared as a variable in\n  order to allow treating it as a variable itself.\n  """"""\n\n    def __init__(self, prot, variable0, variable1, is_scaled):\n        assert isinstance(variable0, AbstractVariable), type(variable0)\n        assert isinstance(variable1, AbstractVariable), type(variable1)\n        assert variable0.shape == variable1.shape\n\n        super(PondPrivateVariable, self).__init__(\n            prot, variable0, variable1, is_scaled,\n        )\n        self.variable0 = variable0\n        self.variable1 = variable1\n        self.initializer = tf.group(\n            *[var.initializer for var in [variable0, variable1]]\n        )\n\n    def __repr__(self) -> str:\n        return ""PondPrivateVariable(shape={})"".format(self.shape)\n\n\nclass PondCachedPublicTensor(PondPublicTensor):\n    """"""A PondPublicTensor that has been cached for reuse.""""""\n\n    def __init__(self, prot, x_on_0, x_on_1, is_scaled, updater):\n        assert isinstance(x_on_0, AbstractTensor), type(x_on_0)\n        assert isinstance(x_on_1, AbstractTensor), type(x_on_1)\n        assert isinstance(updater, tf.Operation), type(updater)\n\n        super(PondCachedPublicTensor, self).__init__(\n            prot, x_on_0, x_on_1, is_scaled,\n        )\n        self.updater = updater\n\n    def __repr__(self) -> str:\n        return ""PondCachedPublicTensor(shape={})"".format(self.shape)\n\n\nclass PondCachedPrivateTensor(PondPrivateTensor):\n    """"""A PondPrivateTensor that has been cached for reuse.""""""\n\n    def __init__(self, prot, x0, x1, is_scaled, updater):\n        assert isinstance(x0, AbstractTensor), type(x0)\n        assert isinstance(x1, AbstractTensor), type(x1)\n        assert isinstance(updater, tf.Operation), type(updater)\n\n        super(PondCachedPrivateTensor, self).__init__(prot, x0, x1, is_scaled)\n        self.updater = updater\n\n    def __repr__(self) -> str:\n        return ""PondCachedPrivateTensor(shape={})"".format(self.shape)\n\n\nclass PondCachedMaskedTensor(PondMaskedTensor):\n    """"""A PondMaskedTensor that has been cached for reuse.""""""\n\n    def __init__(\n        self, prot, unmasked, a, a0, a1, alpha_on_0, alpha_on_1, is_scaled, updater,\n    ):\n        assert isinstance(unmasked, PondPrivateTensor), type(unmasked)\n        assert isinstance(a, AbstractTensor), type(a)\n        assert isinstance(a0, AbstractTensor), type(a0)\n        assert isinstance(a1, AbstractTensor), type(a1)\n        assert isinstance(alpha_on_0, AbstractTensor), type(alpha_on_0)\n        assert isinstance(alpha_on_1, AbstractTensor), type(alpha_on_1)\n        assert isinstance(updater, tf.Operation), type(updater)\n\n        super(PondCachedMaskedTensor, self).__init__(\n            prot, unmasked, a, a0, a1, alpha_on_0, alpha_on_1, is_scaled,\n        )\n        self.updater = updater\n\n    def __repr__(self) -> str:\n        return ""PondCachedMaskedTensor(shape={})"".format(self.shape)\n\n\n#\n# helpers\n#\n\n\ndef _type(x):\n    """"""Helper to check and return PondTensor types.""""""\n\n    if isinstance(x, PondPublicTensor):\n        return PondPublicTensor\n\n    if isinstance(x, PondPrivateTensor):\n        return PondPrivateTensor\n\n    if isinstance(x, PondMaskedTensor):\n        return PondMaskedTensor\n\n    return type(x)\n\n\n# TODO[Morten] this is just a very first step; far from finished\ndef debug(x: PondTensor, summarize=None, message=""""):\n    """"""Print contents of a PondTensor for debugging purposes.""""""\n    if isinstance(x, PondPublicTensor):\n        tf.print(\n            x.value_on_0.value,\n            [x.value_on_0.value],\n            summarize=summarize,\n            message=message,\n        )\n\n    elif isinstance(x, PondPrivateTensor):\n        tf.print(\n            x.share0.value,\n            [x.reveal().value_on_0.value],\n            summarize=summarize,\n            message=message,\n        )\n\n    else:\n        raise TypeError(""Don\'t know how to debug {}"".format(type(x)))\n\n\n#\n# identity\n#\n\n\ndef _identity_public(\n    prot, x, control_dependencies_0, control_dependencies_1,\n):\n    assert isinstance(x, PondPublicTensor), type(x)\n\n    x_on_0, x_on_1 = x.unwrapped\n\n    with tf.name_scope(""identity""):\n\n        with tf.device(prot.server_0.device_name):\n            if control_dependencies_0:\n                with tf.control_dependencies(control_dependencies_0):\n                    y_on_0 = x_on_0.identity()\n            else:\n                y_on_0 = x_on_0.identity()\n\n        with tf.device(prot.server_1.device_name):\n            if control_dependencies_1:\n                with tf.control_dependencies(control_dependencies_1):\n                    y_on_1 = x_on_1.identity()\n            else:\n                y_on_1 = x_on_1.identity()\n\n        y = PondPublicTensor(prot, y_on_0, y_on_1, x.is_scaled)\n        return y\n\n\ndef _identity_private(\n    prot, x, control_dependencies_0, control_dependencies_1,\n):\n    assert isinstance(x, PondPrivateTensor), type(x)\n\n    x0, x1 = x.unwrapped\n\n    with tf.name_scope(""identity""):\n\n        with tf.device(prot.server_0.device_name):\n            if control_dependencies_0:\n                with tf.control_dependencies(control_dependencies_0):\n                    y0 = x0.identity()\n            else:\n                y0 = x0.identity()\n\n        with tf.device(prot.server_1.device_name):\n            if control_dependencies_1:\n                with tf.control_dependencies(control_dependencies_1):\n                    y1 = x1.identity()\n            else:\n                y1 = x1.identity()\n\n        y = PondPrivateTensor(prot, y0, y1, x.is_scaled)\n        return y\n\n\n#\n# cache\n#\n\n\ndef _cache_public(prot, x):\n    assert isinstance(x, PondPublicTensor), type(x)\n\n    x_on_0, x_on_1 = x.unwrapped\n\n    with tf.name_scope(""cache""):\n\n        with tf.device(prot.server_0.device_name):\n            updater0, [x_on_0_cached] = wrap_in_variables(x_on_0)\n\n        with tf.device(prot.server_1.device_name):\n            updater1, [x_on_1_cached] = wrap_in_variables(x_on_1)\n\n        combined_updater = tf.group(updater0, updater1)\n\n    return (\n        combined_updater,\n        PondCachedPublicTensor(\n            prot, x_on_0_cached, x_on_1_cached, x.is_scaled, combined_updater,\n        ),\n    )\n\n\ndef _cache_private(prot, x):\n    assert isinstance(x, PondPrivateTensor), type(x)\n\n    x0, x1 = x.unwrapped\n\n    with tf.name_scope(""cache""):\n\n        with tf.device(prot.server_0.device_name):\n            updater0, [x0_cached] = wrap_in_variables(x0)\n\n        with tf.device(prot.server_1.device_name):\n            updater1, [x1_cached] = wrap_in_variables(x1)\n\n        combined_updater = tf.group(updater0, updater1)\n\n    return (\n        combined_updater,\n        PondCachedPrivateTensor(\n            prot, x0_cached, x1_cached, x.is_scaled, combined_updater,\n        ),\n    )\n\n\ndef _cache_masked(prot, x):\n    assert isinstance(x, PondMaskedTensor), type(x)\n\n    a, a0, a1, alpha_on_0, alpha_on_1 = x.unwrapped\n\n    with tf.name_scope(""cache""):\n\n        with tf.device(prot.server_0.device_name):\n            updater0, [a0_cached, alpha_on_0_cached] = wrap_in_variables(a0, alpha_on_0)\n\n        with tf.device(prot.server_1.device_name):\n            updater1, [a1_cached, alpha_on_1_cached] = wrap_in_variables(a1, alpha_on_1)\n\n        unmasked_updater, unmasked_cached = prot.cache(x.unmasked)\n        online_updater = tf.group(updater0, updater1, unmasked_updater)\n\n        offline_updater, a_cached = prot.triple_source.cache(a, online_updater)\n        combined_updater = tf.group(online_updater, offline_updater)\n\n    return (\n        combined_updater,\n        PondCachedMaskedTensor(\n            prot,\n            unmasked_cached,\n            a_cached,\n            a0_cached,\n            a1_cached,\n            alpha_on_0_cached,\n            alpha_on_1_cached,\n            x.is_scaled,\n            combined_updater,\n        ),\n    )\n\n\n#\n# truncate\n#\n\n\ndef _truncate_public(prot: Pond, x: PondPublicTensor,) -> PondPublicTensor:\n    assert isinstance(x, PondPublicTensor)\n\n    base = prot.fixedpoint_config.scaling_base\n    amount = prot.fixedpoint_config.precision_fractional\n    x_on_0, x_on_1 = x.unwrapped\n\n    with tf.name_scope(""truncate""):\n\n        with tf.device(prot.server_0.device_name):\n            y_on_0 = x_on_0.truncate(amount, base)\n\n        with tf.device(prot.server_1.device_name):\n            y_on_1 = x_on_1.truncate(amount, base)\n\n    return PondPublicTensor(prot, y_on_0, y_on_1, x.is_scaled)\n\n\ndef _truncate_private(prot: Pond, x: PondPrivateTensor,) -> PondPrivateTensor:\n    assert isinstance(x, PondPrivateTensor)\n\n    if prot.fixedpoint_config.use_noninteractive_truncation:\n        return _truncate_private_noninteractive(prot, x)\n\n    return _truncate_private_interactive(prot, x)\n\n\ndef _truncate_private_noninteractive(\n    prot: Pond, x: PondPrivateTensor,\n) -> PondPrivateTensor:\n    assert isinstance(x, PondPrivateTensor)\n\n    base = prot.fixedpoint_config.scaling_base\n    amount = prot.fixedpoint_config.precision_fractional\n    x0, x1 = x.unwrapped\n\n    with tf.name_scope(""truncate-ni""):\n\n        with tf.device(prot.server_0.device_name):\n            y0 = x0.truncate(amount, base)\n\n        with tf.device(prot.server_1.device_name):\n            y1 = 0 - (0 - x1).truncate(amount, base)\n\n    return PondPrivateTensor(prot, y0, y1, x.is_scaled)\n\n\ndef _truncate_private_interactive(\n    prot: Pond, a: PondPrivateTensor,\n) -> PondPrivateTensor:\n    """"""See protocol TruncPr (3.1) in\n  ""Secure Computation With Fixed-Point Numbers"" by Octavian Catrina and Amitabh\n  Saxena, FC\'10.""""""\n\n    with tf.name_scope(""truncate-i""):\n\n        scaling_factor = prot.fixedpoint_config.scaling_factor\n        scaling_factor_inverse = inverse(\n            prot.fixedpoint_config.scaling_factor, prot.tensor_factory.modulus\n        )\n\n        # we first rotate `a` to make sure reconstructed values fall into\n        # a non-negative interval `[0, 2B)` for some bound B; this uses an\n        # assumption that the values originally lie in `[-B, B)`, and will\n        # leak private information otherwise\n\n        # \'a + bound\' will automatically lift \'bound\' by another scaling factor,\n        # so we should first divide bound by the scaling factor if we want to\n        # use this convenient \'+\' operation.\n        bound = prot.fixedpoint_config.bound_double_precision\n        b = a + (bound / scaling_factor)\n\n        # next step is for server0 to add a statistical mask to `b`, reveal\n        # it to server1, and compute the lower part\n        trunc_gap = prot.fixedpoint_config.truncation_gap\n        mask_bitlength = ceil(log2(bound)) + 1 + trunc_gap\n\n        b0, b1 = b.unwrapped\n        shape = a.shape\n\n        with tf.device(prot.server_0.device_name):\n            r = prot.tensor_factory.sample_bounded(shape, mask_bitlength)\n            c0 = b0 + r\n\n        with tf.device(prot.server_1.device_name):\n            c1 = b1\n            c_lower = prot._reconstruct(c0, c1) % scaling_factor\n\n        # then use the lower part of the masked value to compute lower part\n        # of original value\n\n        with tf.device(prot.server_0.device_name):\n            r_lower = r % scaling_factor\n            a_lower0 = r_lower * -1\n\n        with tf.device(prot.server_1.device_name):\n            a_lower1 = c_lower\n\n        # finally subtract and multiply by inverse\n\n        a0, a1 = a.unwrapped\n\n        with tf.device(prot.server_0.device_name):\n            d0 = (a0 - a_lower0) * scaling_factor_inverse\n\n        with tf.device(prot.server_1.device_name):\n            d1 = (a1 - a_lower1) * scaling_factor_inverse\n\n    return PondPrivateTensor(prot, d0, d1, a.is_scaled)\n\n\ndef _truncate_masked(prot: Pond, x: PondMaskedTensor,) -> PondMaskedTensor:\n    assert isinstance(x, PondMaskedTensor)\n    return prot.truncate(x.unmasked)\n\n\n#\n# reveal helpers\n#\n\n\ndef _reveal_private(prot, x):\n    assert isinstance(x, PondPrivateTensor), type(x)\n\n    with tf.name_scope(""reveal""):\n\n        x0, x1 = x.unwrapped\n\n        with tf.device(prot.server_0.device_name):\n            z_on_0 = x0 + x1\n\n        with tf.device(prot.server_1.device_name):\n            z_on_1 = x0 + x1\n\n    return PondPublicTensor(prot, z_on_0, z_on_1, x.is_scaled)\n\n\ndef _reveal_masked(prot, x):\n    assert isinstance(x, PondMaskedTensor), type(x)\n    return prot.reveal(x.unmasked)\n\n\n#\n# add helpers\n#\n\n\ndef _add_public_public(prot, x, y):\n    assert isinstance(x, PondPublicTensor), type(x)\n    assert isinstance(y, PondPublicTensor), type(y)\n    assert x.is_scaled == y.is_scaled, ""Cannot mix different encodings: {} {}"".format(\n        x.is_scaled, y.is_scaled\n    )\n\n    x_on_0, x_on_1 = x.unwrapped\n    y_on_0, y_on_1 = y.unwrapped\n\n    with tf.name_scope(""add""):\n\n        with tf.device(prot.server_0.device_name):\n            z_on_0 = x_on_0 + y_on_0\n\n        with tf.device(prot.server_1.device_name):\n            z_on_1 = x_on_1 + y_on_1\n\n    return PondPublicTensor(prot, z_on_0, z_on_1, x.is_scaled)\n\n\ndef _add_public_private(prot, x, y):\n    assert isinstance(x, PondPublicTensor), type(x)\n    assert isinstance(y, PondPrivateTensor), type(y)\n    assert x.is_scaled == y.is_scaled, ""Cannot mix different encodings: {} {}"".format(\n        x.is_scaled, y.is_scaled\n    )\n\n    x_on_0, _ = x.unwrapped\n    y0, y1 = y.unwrapped\n\n    with tf.name_scope(""add""):\n\n        with tf.device(prot.server_0.device_name):\n            z0 = x_on_0 + y0\n\n        with tf.device(prot.server_1.device_name):\n            z1 = y1\n\n    return PondPrivateTensor(prot, z0, z1, x.is_scaled)\n\n\ndef _add_public_masked(prot, x, y):\n    assert isinstance(x, PondPublicTensor), type(x)\n    assert isinstance(y, PondMaskedTensor), type(y)\n    return prot.add(x, y.unmasked)\n\n\ndef _add_private_public(prot, x, y):\n    assert isinstance(x, PondPrivateTensor), type(x)\n    assert isinstance(y, PondPublicTensor), type(y)\n    assert x.is_scaled == y.is_scaled, ""Cannot mix different encodings: {} {}"".format(\n        x.is_scaled, y.is_scaled\n    )\n\n    x0, x1 = x.unwrapped\n    y_on_0, _ = y.unwrapped\n\n    with tf.name_scope(""add""):\n\n        with tf.device(prot.server_0.device_name):\n            z0 = x0 + y_on_0\n\n        with tf.device(prot.server_1.device_name):\n            z1 = x1\n\n    return PondPrivateTensor(prot, z0, z1, x.is_scaled)\n\n\ndef _add_private_private(prot, x, y):\n    assert isinstance(x, PondPrivateTensor), type(x)\n    assert isinstance(y, PondPrivateTensor), type(y)\n    # TODO[Morten] fails due to use in masking in SecureNN; how do deal with\n    #              this?\n    # err = ""Cannot mix different encodings: {} {}"".format(x.is_scaled,\n    #                                                      y.is_scaled)\n    # assert x.is_scaled == y.is_scaled, err\n\n    x0, x1 = x.unwrapped\n    y0, y1 = y.unwrapped\n\n    with tf.name_scope(""add""):\n\n        with tf.device(prot.server_0.device_name):\n            z0 = x0 + y0\n\n        with tf.device(prot.server_1.device_name):\n            z1 = x1 + y1\n\n    return PondPrivateTensor(prot, z0, z1, x.is_scaled)\n\n\ndef _add_private_masked(prot, x, y):\n    assert isinstance(x, PondPrivateTensor), type(x)\n    assert isinstance(y, PondMaskedTensor), type(y)\n    return prot.add(x, y.unmasked)\n\n\ndef _add_masked_public(prot, x, y):\n    assert isinstance(x, PondMaskedTensor), type(x)\n    assert isinstance(y, PondPublicTensor), type(y)\n    return prot.add(x.unmasked, y)\n\n\ndef _add_masked_private(prot, x, y):\n    assert isinstance(x, PondMaskedTensor), type(x)\n    assert isinstance(y, PondPrivateTensor), type(y)\n    return prot.add(x.unmasked, y)\n\n\ndef _add_masked_masked(prot, x, y):\n    assert isinstance(x, PondMaskedTensor), type(x)\n    assert isinstance(y, PondMaskedTensor), type(y)\n    return prot.add(x.unmasked, y.unmasked)\n\n\n#\n# reduce_sum helpers\n#\n\n\ndef _reduce_sum_public(\n    prot: Pond,\n    x: PondPublicTensor,\n    axis: Optional[int] = None,\n    keepdims: Optional[bool] = None,\n) -> PondPublicTensor:\n\n    x_on_0, x_on_1 = x.unwrapped\n\n    with tf.name_scope(""reduce_sum""):\n\n        with tf.device(prot.server_0.device_name):\n            y_on_0 = x_on_0.reduce_sum(axis, keepdims)\n\n        with tf.device(prot.server_1.device_name):\n            y_on_1 = x_on_1.reduce_sum(axis, keepdims)\n\n    return PondPublicTensor(prot, y_on_0, y_on_1, x.is_scaled)\n\n\ndef _reduce_sum_private(\n    prot: Pond,\n    x: PondPrivateTensor,\n    axis: Optional[int] = None,\n    keepdims: Optional[bool] = None,\n) -> PondPrivateTensor:\n\n    x0, x1 = x.unwrapped\n\n    with tf.name_scope(""reduce_sum""):\n\n        with tf.device(prot.server_0.device_name):\n            y0 = x0.reduce_sum(axis, keepdims)\n\n        with tf.device(prot.server_1.device_name):\n            y1 = x1.reduce_sum(axis, keepdims)\n\n    return PondPrivateTensor(prot, y0, y1, x.is_scaled)\n\n\ndef _reduce_sum_masked(\n    prot: Pond,\n    x: PondMaskedTensor,\n    axis: Optional[int] = None,\n    keepdims: Optional[bool] = None,\n) -> PondPrivateTensor:\n    return prot.reduce_sum(x.unmasked, axis, keepdims)\n\n\n#\n# cumsum helpers\n#\n\n\ndef _cumsum_public(\n    prot: Pond,\n    x: PondPublicTensor,\n    axis: Optional[int] = None,\n    exclusive: Optional[bool] = None,\n    reverse: Optional[bool] = None,\n) -> PondPublicTensor:\n\n    x_on_0, x_on_1 = x.unwrapped\n\n    with tf.name_scope(""cumsum""):\n\n        with tf.device(prot.server_0.device_name):\n            y_on_0 = x_on_0.cumsum(axis=axis, exclusive=exclusive, reverse=reverse)\n\n        with tf.device(prot.server_1.device_name):\n            y_on_1 = x_on_1.cumsum(axis=axis, exclusive=exclusive, reverse=reverse)\n\n    return PondPublicTensor(prot, y_on_0, y_on_1, x.is_scaled)\n\n\ndef _cumsum_private(\n    prot: Pond,\n    x: PondPrivateTensor,\n    axis: Optional[int] = None,\n    exclusive: Optional[bool] = None,\n    reverse: Optional[bool] = None,\n) -> PondPrivateTensor:\n\n    x0, x1 = x.unwrapped\n\n    with tf.name_scope(""cumsum""):\n\n        with tf.device(prot.server_0.device_name):\n            y0 = x0.cumsum(axis=axis, exclusive=exclusive, reverse=reverse)\n\n        with tf.device(prot.server_1.device_name):\n            y1 = x1.cumsum(axis=axis, exclusive=exclusive, reverse=reverse)\n\n    return PondPrivateTensor(prot, y0, y1, x.is_scaled)\n\n\ndef _cumsum_masked(\n    prot: Pond,\n    x: PondMaskedTensor,\n    axis: Optional[int] = None,\n    exclusive: Optional[bool] = None,\n    reverse: Optional[bool] = None,\n) -> PondPrivateTensor:\n    return prot.cumsum(x.unmasked, axis=axis, exclusive=exclusive, reverse=reverse,)\n\n\n#\n# sub helpers\n#\n\n\ndef _sub_public_public(prot, x, y):\n    assert isinstance(x, PondPublicTensor), type(x)\n    assert isinstance(y, PondPublicTensor), type(y)\n    assert x.is_scaled == y.is_scaled, ""Cannot mix different encodings: {} {}"".format(\n        x.is_scaled, y.is_scaled\n    )\n\n    x_on_0, x_on_1 = x.unwrapped\n    y_on_0, y_on_1 = y.unwrapped\n\n    with tf.name_scope(""sub""):\n\n        with tf.device(prot.server_0.device_name):\n            z_on_0 = x_on_0 - y_on_0\n\n        with tf.device(prot.server_1.device_name):\n            z_on_1 = x_on_1 - y_on_1\n\n    return PondPublicTensor(prot, z_on_0, z_on_1, x.is_scaled)\n\n\ndef _sub_public_private(prot, x, y):\n    assert isinstance(x, PondPublicTensor), type(x)\n    assert isinstance(y, PondPrivateTensor), type(y)\n    assert x.is_scaled == y.is_scaled, ""Cannot mix different encodings: {} {}"".format(\n        x.is_scaled, y.is_scaled\n    )\n\n    x_on_0, _ = x.unwrapped\n    y0, y1 = y.unwrapped\n\n    with tf.name_scope(""sub""):\n\n        with tf.device(prot.server_0.device_name):\n            z0 = x_on_0 - y0\n\n        with tf.device(prot.server_1.device_name):\n            z1 = y1.negative()\n\n    return PondPrivateTensor(prot, z0, z1, x.is_scaled)\n\n\ndef _sub_public_masked(prot, x, y):\n    assert isinstance(x, PondPublicTensor), type(x)\n    assert isinstance(y, PondMaskedTensor), type(y)\n    return prot.sub(x, y.unmasked)\n\n\ndef _sub_private_public(prot, x, y):\n    assert isinstance(x, PondPrivateTensor), type(x)\n    assert isinstance(y, PondPublicTensor), type(y)\n    assert x.is_scaled == y.is_scaled, ""Cannot mix different encodings: {} {}"".format(\n        x.is_scaled, y.is_scaled\n    )\n\n    x0, x1 = x.unwrapped\n    y_on_0, _ = y.unwrapped\n\n    with tf.name_scope(""sub""):\n\n        with tf.device(prot.server_0.device_name):\n            z0 = x0 - y_on_0\n\n        with tf.device(prot.server_1.device_name):\n            z1 = x1\n\n    return PondPrivateTensor(prot, z0, z1, x.is_scaled)\n\n\ndef _sub_private_private(prot, x, y):\n    assert isinstance(x, PondPrivateTensor), type(x)\n    assert isinstance(y, PondPrivateTensor), type(y)\n    assert x.is_scaled == y.is_scaled, ""Cannot mix different encodings: {} {}"".format(\n        x.is_scaled, y.is_scaled\n    )\n\n    x0, x1 = x.unwrapped\n    y0, y1 = y.unwrapped\n\n    with tf.name_scope(""sub""):\n\n        with tf.device(prot.server_0.device_name):\n            z0 = x0 - y0\n\n        with tf.device(prot.server_1.device_name):\n            z1 = x1 - y1\n\n    return PondPrivateTensor(prot, z0, z1, x.is_scaled)\n\n\ndef _sub_private_masked(prot, x, y):\n    assert isinstance(x, PondPrivateTensor), type(x)\n    assert isinstance(y, PondMaskedTensor), type(y)\n    return prot.sub(x, y.unmasked)\n\n\ndef _sub_masked_public(prot, x, y):\n    assert isinstance(x, PondMaskedTensor), type(x)\n    assert isinstance(y, PondPublicTensor), type(y)\n    return prot.sub(x.unmasked, y)\n\n\ndef _sub_masked_private(prot, x, y):\n    assert isinstance(x, PondMaskedTensor), type(x)\n    assert isinstance(y, PondPrivateTensor), type(y)\n    return prot.sub(x.unmasked, y)\n\n\ndef _sub_masked_masked(prot, x, y):\n    assert isinstance(x, PondMaskedTensor), type(x)\n    assert isinstance(y, PondMaskedTensor), type(y)\n    return prot.sub(x.unmasked, y.unmasked)\n\n\n#\n# mul helpers\n#\n\n\ndef _mul_public_public(prot, x, y):\n    assert isinstance(x, PondPublicTensor), type(x)\n    assert isinstance(y, PondPublicTensor), type(y)\n\n    x_on_0, x_on_1 = x.unwrapped\n    y_on_0, y_on_1 = y.unwrapped\n\n    with tf.name_scope(""mul""):\n\n        with tf.device(prot.server_0.device_name):\n            z_on_0 = x_on_0 * y_on_0\n\n        with tf.device(prot.server_1.device_name):\n            z_on_1 = x_on_1 * y_on_1\n\n        z = PondPublicTensor(prot, z_on_0, z_on_1, x.is_scaled or y.is_scaled)\n        z = prot.truncate(z) if x.is_scaled and y.is_scaled else z\n        return z\n\n\ndef _mul_public_private(prot, x, y):\n    assert isinstance(x, PondPublicTensor), type(x)\n    assert isinstance(y, PondPrivateTensor), type(y)\n\n    x_on_0, x_on_1 = x.unwrapped\n    y0, y1 = y.unwrapped\n\n    with tf.name_scope(""mul""):\n\n        with tf.device(prot.server_0.device_name):\n            z0 = x_on_0 * y0\n\n        with tf.device(prot.server_1.device_name):\n            z1 = x_on_1 * y1\n\n        z = PondPrivateTensor(prot, z0, z1, x.is_scaled or y.is_scaled)\n        z = prot.truncate(z) if x.is_scaled and y.is_scaled else z\n        return z\n\n\ndef _mul_public_masked(prot, x, y):\n    assert isinstance(x, PondPublicTensor), type(x)\n    assert isinstance(y, PondMaskedTensor), type(y)\n    return prot.mul(x, y.unmasked)\n\n\ndef _mul_private_public(prot, x, y):\n    assert isinstance(x, PondPrivateTensor), type(x)\n    assert isinstance(y, PondPublicTensor), type(y)\n\n    x0, x1 = x.unwrapped\n    y_on_0, y_on_1 = y.unwrapped\n\n    with tf.name_scope(""mul""):\n\n        with tf.device(prot.server_0.device_name):\n            z0 = x0 * y_on_0\n\n        with tf.device(prot.server_1.device_name):\n            z1 = x1 * y_on_1\n\n        z = PondPrivateTensor(prot, z0, z1, x.is_scaled or y.is_scaled)\n        z = prot.truncate(z) if x.is_scaled and y.is_scaled else z\n        return z\n\n\ndef _mul_private_private(prot, x, y):\n    assert isinstance(x, PondPrivateTensor), type(x)\n    assert isinstance(y, PondPrivateTensor), type(y)\n    return prot.mul(prot.mask(x), prot.mask(y))\n\n\ndef _mul_private_masked(prot, x, y):\n    assert isinstance(x, PondPrivateTensor), type(x)\n    assert isinstance(y, PondMaskedTensor), type(y)\n    return prot.mul(prot.mask(x), y)\n\n\ndef _mul_masked_public(prot, x, y):\n    assert isinstance(x, PondMaskedTensor), type(x)\n    assert isinstance(y, PondPublicTensor), type(y)\n    return prot.mul(x.unmasked, y)\n\n\ndef _mul_masked_private(prot, x, y):\n    assert isinstance(x, PondMaskedTensor), type(x)\n    assert isinstance(y, PondPrivateTensor), type(y)\n    return prot.mul(x, prot.mask(y))\n\n\ndef _mul_masked_masked(prot, x, y):\n    assert isinstance(x, PondMaskedTensor), type(x)\n    assert isinstance(y, PondMaskedTensor), type(y)\n\n    a, a0, a1, alpha_on_0, alpha_on_1 = x.unwrapped\n    b, b0, b1, beta_on_0, beta_on_1 = y.unwrapped\n\n    with tf.name_scope(""mul""):\n\n        ab0, ab1 = prot.triple_source.mul_triple(a, b)\n\n        with tf.device(prot.server_0.device_name):\n            with tf.name_scope(""combine""):\n                alpha = alpha_on_0\n                beta = beta_on_0\n                z0 = ab0 + (a0 * beta) + (alpha * b0) + (alpha * beta)\n\n        with tf.device(prot.server_1.device_name):\n            with tf.name_scope(""combine""):\n                alpha = alpha_on_1\n                beta = beta_on_1\n                z1 = ab1 + (a1 * beta) + (alpha * b1)\n\n        z = PondPrivateTensor(prot, z0, z1, x.is_scaled or y.is_scaled)\n        z = prot.truncate(z) if x.is_scaled and y.is_scaled else z\n        return z\n\n\n#\n# reciprocal helpers\n#\n\n\ndef _reciprocal_public(prot, x):\n    assert isinstance(x, PondPublicTensor), type(x)\n\n    backing_dtype = x.backing_dtype\n    x_on_0, x_on_1 = x.unwrapped\n    is_scaled = x.is_scaled\n    assert is_scaled, ""Can only reciprocal of scaled numbers""\n\n    with tf.name_scope(""reciprocal""):\n\n        with tf.device(prot.server_0.device_name):\n            # decode value as ordinary tensor locally and compute reciprocal\n            x_on_0_decoded = prot._decode(x_on_0, is_scaled)\n            y_on_0_decoded = tf.math.reciprocal(x_on_0_decoded)\n            # re-encode and re-wrap\n            y_on_0 = backing_dtype.tensor(\n                prot._encode(\n                    y_on_0_decoded,\n                    apply_scaling=is_scaled,\n                    tf_int_type=backing_dtype.native_type,\n                )\n            )\n\n        with tf.device(prot.server_1.device_name):\n            # decode value as ordinary tensor locally and compute reciprocal\n            x_on_1_decoded = prot._decode(x_on_1, is_scaled)\n            y_on_1_decoded = tf.math.reciprocal(x_on_1_decoded)\n            # re-encode and re-wrap\n            y_on_1 = backing_dtype.tensor(\n                prot._encode(\n                    y_on_1_decoded,\n                    apply_scaling=is_scaled,\n                    tf_int_type=backing_dtype.native_type,\n                )\n            )\n\n        y = PondPublicTensor(prot, y_on_0, y_on_1, is_scaled)\n        return y\n\n\n#\n# square helpers\n#\n\n\ndef _square_public(prot, x):\n    assert isinstance(x, PondPublicTensor), type(x)\n\n    x_on_0, x_on_1 = x.unwrapped\n\n    with tf.name_scope(""square""):\n\n        with tf.device(prot.server_0.device_name):\n            y_on_0 = x_on_0 * x_on_0\n\n        with tf.device(prot.server_1.device_name):\n            y_on_1 = x_on_1 * x_on_1\n\n        y = PondPublicTensor(prot, y_on_0, y_on_1, x.is_scaled)\n        y = prot.truncate(y) if y.is_scaled else y\n        return y\n\n\ndef _square_private(prot, x):\n    assert isinstance(x, PondPrivateTensor), type(x)\n    return prot.square(prot.mask(x))\n\n\ndef _square_masked(prot, x):\n    assert isinstance(x, PondMaskedTensor), type(x)\n\n    a, a0, a1, alpha_on_0, alpha_on_1 = x.unwrapped\n\n    with tf.name_scope(""square""):\n\n        aa0, aa1 = prot.triple_source.square_triple(a)\n\n        with tf.device(prot.server_0.device_name):\n            with tf.name_scope(""combine""):\n                alpha = alpha_on_0\n                y0 = aa0 + (a0 * alpha) * 2 + (alpha * alpha)\n\n        with tf.device(prot.server_1.device_name):\n            with tf.name_scope(""combine""):\n                alpha = alpha_on_1\n                y1 = aa1 + (a1 * alpha) * 2\n\n        y = PondPrivateTensor(prot, y0, y1, x.is_scaled)\n        y = prot.truncate(y) if y.is_scaled else y\n        return y\n\n\n#\n# matmul helpers\n#\n\n\ndef _matmul_public_public(\n    prot, x: PondPublicTensor, y: PondPublicTensor,\n) -> PondPublicTensor:\n\n    x_on_0, x_on_1 = x.unwrapped\n    y_on_0, y_on_1 = y.unwrapped\n\n    with tf.name_scope(""matmul""):\n\n        with tf.device(prot.server_0.device_name):\n            z_on_0 = x_on_0.matmul(y_on_0)\n\n        with tf.device(prot.server_1.device_name):\n            z_on_1 = x_on_1.matmul(y_on_1)\n\n        z = PondPublicTensor(prot, z_on_0, z_on_1, x.is_scaled or y.is_scaled)\n        z = prot.truncate(z) if x.is_scaled and y.is_scaled else z\n        return z\n\n\ndef _matmul_public_private(prot, x, y):\n    assert isinstance(x, PondPublicTensor), type(x)\n    assert isinstance(y, PondPrivateTensor), type(y)\n\n    x_on_0, x_on_1 = x.unwrapped\n    y0, y1 = y.unwrapped\n\n    with tf.name_scope(""matmul""):\n\n        with tf.device(prot.server_0.device_name):\n            z0 = x_on_0.matmul(y0)\n\n        with tf.device(prot.server_1.device_name):\n            z1 = x_on_1.matmul(y1)\n\n        z = PondPrivateTensor(prot, z0, z1, x.is_scaled or y.is_scaled)\n        z = prot.truncate(z) if x.is_scaled and y.is_scaled else z\n        return z\n\n\ndef _matmul_public_masked(prot, x, y):\n    assert isinstance(x, PondPublicTensor), type(x)\n    assert isinstance(y, PondMaskedTensor), type(y)\n    return prot.matmul(x, y.unmasked)\n\n\ndef _matmul_private_public(prot, x, y):\n    assert isinstance(x, PondPrivateTensor), type(x)\n    assert isinstance(y, PondPublicTensor), type(y)\n\n    x0, x1 = x.unwrapped\n    y_on_0, y_on_1 = y.unwrapped\n\n    with tf.name_scope(""matmul""):\n\n        with tf.device(prot.server_0.device_name):\n            z0 = x0.matmul(y_on_0)\n\n        with tf.device(prot.server_0.device_name):\n            z1 = x1.matmul(y_on_1)\n\n        z = PondPrivateTensor(prot, z0, z1, x.is_scaled or y.is_scaled)\n        z = prot.truncate(z) if x.is_scaled and y.is_scaled else z\n        return z\n\n\ndef _matmul_private_private(prot, x, y):\n    assert isinstance(x, PondPrivateTensor), type(x)\n    assert isinstance(y, PondPrivateTensor), type(y)\n    return prot.matmul(prot.mask(x), prot.mask(y))\n\n\ndef _matmul_private_masked(prot, x, y):\n    assert isinstance(x, PondPrivateTensor), type(x)\n    assert isinstance(y, PondMaskedTensor), type(y)\n    return prot.matmul(prot.mask(x), y)\n\n\ndef _matmul_masked_public(prot, x, y):\n    assert isinstance(x, PondMaskedTensor), type(x)\n    assert isinstance(y, PondPublicTensor), type(y)\n    return prot.matmul(x.unmasked, y)\n\n\ndef _matmul_masked_private(prot, x, y):\n    assert isinstance(x, PondMaskedTensor), type(x)\n    assert isinstance(y, PondPrivateTensor), type(y)\n    return prot.matmul(x, prot.mask(y))\n\n\ndef _matmul_masked_masked(prot, x, y):\n    assert isinstance(x, PondMaskedTensor), type(x)\n    assert isinstance(y, PondMaskedTensor), type(y)\n\n    a, a0, a1, alpha_on_0, alpha_on_1 = x.unwrapped\n    b, b0, b1, beta_on_0, beta_on_1 = y.unwrapped\n\n    with tf.name_scope(""matmul""):\n\n        ab0, ab1 = prot.triple_source.matmul_triple(a, b)\n\n        with tf.device(prot.server_0.device_name):\n            with tf.name_scope(""combine""):\n                alpha = alpha_on_0\n                beta = beta_on_0\n                z0 = ab0 + a0.matmul(beta) + alpha.matmul(b0) + alpha.matmul(beta)\n\n        with tf.device(prot.server_1.device_name):\n            with tf.name_scope(""combine""):\n                alpha = alpha_on_1\n                beta = beta_on_1\n                z1 = ab1 + a1.matmul(beta) + alpha.matmul(b1)\n\n        z = PondPrivateTensor(prot, z0, z1, x.is_scaled or y.is_scaled)\n        z = prot.truncate(z) if x.is_scaled and y.is_scaled else z\n        return z\n\n\n#\n# Conv helpers\n#\n# TODO[koen] create operations for all possible combinations\n\n\ndef _conv2d_public_public(prot, x, y, strides, padding):\n    assert isinstance(x, PondPublicTensor), type(x)\n    assert isinstance(y, PondPublicTensor), type(y)\n\n    x_0, x_1 = x.unwrapped\n    y_0, y_1 = y.unwrapped\n\n    with tf.name_scope(""conv2d""):\n\n        with tf.device(prot.server_0.device_name):\n            z0 = x_0.conv2d(y_0, strides, padding)\n\n        with tf.device(prot.server_1.device_name):\n            z1 = x_1.conv2d(y_1, strides, padding)\n\n        z = PondPublicTensor(prot, z0, z1, x.is_scaled or y.is_scaled)\n        z = prot.truncate(z) if x.is_scaled and y.is_scaled else z\n        return z\n\n\ndef _conv2d_public_private(prot, x, y, strides, padding):\n    raise NotImplementedError()\n\n\ndef _conv2d_public_masked(prot, x, y, strides, padding):\n    raise NotImplementedError()\n\n\ndef _conv2d_private_public(prot, x, y, strides, padding):\n    raise NotImplementedError()\n\n\ndef _conv2d_private_masked(prot, x, y, strides, padding):\n    assert isinstance(x, PondPrivateTensor), type(x)\n    assert isinstance(y, PondMaskedTensor), type(y)\n    return prot.conv2d(prot.mask(x), y, strides, padding)\n\n\ndef _conv2d_private_private(prot, x, y, strides, padding):\n    assert isinstance(x, PondPrivateTensor), type(x)\n    assert isinstance(y, PondPrivateTensor), type(y)\n    return prot.conv2d(prot.mask(x), prot.mask(y), strides, padding)\n\n\ndef _conv2d_masked_public(prot, x, y, strides, padding):\n    raise NotImplementedError()\n\n\ndef _conv2d_masked_private(prot, x, y, strides, padding):\n    assert isinstance(x, PondMaskedTensor), type(x)\n    assert isinstance(y, PondPrivateTensor), type(y)\n    return prot.conv2d(x, prot.mask(y), strides, padding)\n\n\ndef _conv2d_masked_masked(prot, x, y, strides, padding):\n    assert isinstance(x, PondMaskedTensor), type(x)\n    assert isinstance(y, PondMaskedTensor), type(y)\n\n    a, a0, a1, alpha_on_0, alpha_on_1 = x.unwrapped\n    b, b0, b1, beta_on_0, beta_on_1 = y.unwrapped\n\n    with tf.name_scope(""conv2d""):\n\n        a_conv2d_b0, a_conv2d_b1 = prot.triple_source.conv2d_triple(\n            a, b, strides, padding,\n        )\n\n        with tf.device(prot.server_0.device_name):\n            with tf.name_scope(""combine""):\n                alpha = alpha_on_0\n                beta = beta_on_0\n                z0 = (\n                    a_conv2d_b0\n                    + a0.conv2d(beta, strides, padding)\n                    + alpha.conv2d(b0, strides, padding)\n                    + alpha.conv2d(beta, strides, padding)\n                )\n\n        with tf.device(prot.server_1.device_name):\n            with tf.name_scope(""combine""):\n                alpha = alpha_on_1\n                beta = beta_on_1\n                z1 = (\n                    a_conv2d_b1\n                    + a1.conv2d(beta, strides, padding)\n                    + alpha.conv2d(b1, strides, padding)\n                )\n\n        z = PondPrivateTensor(prot, z0, z1, x.is_scaled or y.is_scaled)\n        z = prot.truncate(z) if x.is_scaled and y.is_scaled else z\n        return z\n\n\n#\n# average pooling helpers\n#\n\n\ndef _avgpool2d_core(\n    prot: Pond,\n    x: PondTensor,\n    pool_size: Tuple[int, int],\n    strides: Tuple[int, int],\n    padding: str,\n) -> Tuple[AbstractTensor, AbstractTensor, float]:\n    x_on_0, x_on_1 = x.unwrapped\n    _, _, h, w = x.shape\n    scalar = 1 / (pool_size[0] * pool_size[1])\n    siamese = pool_size == strides and pool_size[0] == pool_size[1]\n    even = h.value % pool_size[0] == 0 and w.value % pool_size[1] == 0\n\n    if siamese and even:\n        pooler = _avgpool2d_reshape_reduce\n    else:\n        pooler = _avgpool2d_im2col_reduce\n\n    with tf.device(prot.server_0.device_name):\n        y_on_0 = pooler(x_on_0, pool_size, strides, padding)\n\n    with tf.device(prot.server_1.device_name):\n        y_on_1 = pooler(x_on_1, pool_size, strides, padding)\n\n    return y_on_0, y_on_1, scalar\n\n\ndef _avgpool2d_im2col_reduce(\n    x: AbstractTensor,\n    pool_size: Tuple[int, int],\n    strides: Tuple[int, int],\n    padding: str,\n) -> AbstractTensor:\n    """"""Perform 2D average pooling by the im2col method.""""""\n    batch, channels, height, width = x.shape\n    pool_height, pool_width = pool_size\n\n    if padding == ""SAME"":\n        out_height = ceil(int(height) / strides[0])\n        out_width = ceil(int(width) / strides[1])\n    else:\n        out_height = ceil((int(height) - pool_size[0] + 1) / strides[0])\n        out_width = ceil((int(width) - pool_size[1] + 1) / strides[1])\n\n    x_split = x.reshape((batch * channels, 1, height, width))\n    x_cols = x_split.im2col(pool_height, pool_width, padding, strides[0])\n    x_cols_sum = x_cols.reduce_sum(axis=0)\n    return x_cols_sum.reshape([out_height, out_width, batch, channels]).transpose(\n        [2, 3, 0, 1]\n    )\n\n\ndef _avgpool2d_reshape_reduce(x, pool_size: Tuple[int, int], *args):\n    """"""Perform 2D average pooling by the reshape method.""""""\n    del args\n    pool_height = tf.Dimension(pool_size[0])\n    pool_width = tf.Dimension(pool_size[1])\n    n, c, h, w = x.shape\n    return (\n        x.reshape([n, c, h // pool_height, pool_height, w // pool_width, pool_width])\n        .reduce_sum(axis=3)\n        .reduce_sum(axis=4)\n    )\n\n\ndef _avgpool2d_public(\n    prot: Pond,\n    x: PondPublicTensor,\n    pool_size: Tuple[int, int],\n    strides: Tuple[int, int],\n    padding: str,\n) -> PondPublicTensor:\n\n    with tf.name_scope(""avgpool2d""):\n        y_on_0, y_on_1, scalar = _avgpool2d_core(prot, x, pool_size, strides, padding)\n        return PondPublicTensor(prot, y_on_0, y_on_1, x.is_scaled) * scalar\n\n\ndef _avgpool2d_private(\n    prot: Pond,\n    x: PondPrivateTensor,\n    pool_size: Tuple[int, int],\n    strides: Tuple[int, int],\n    padding: str,\n) -> PondPrivateTensor:\n\n    with tf.name_scope(""avgpool2d""):\n        y_on_0, y_on_1, scalar = _avgpool2d_core(prot, x, pool_size, strides, padding,)\n        return PondPrivateTensor(prot, y_on_0, y_on_1, x.is_scaled) * scalar\n\n\ndef _avgpool2d_masked(\n    prot: Pond,\n    x: PondMaskedTensor,\n    pool_size: Tuple[int, int],\n    strides: Tuple[int, int],\n    padding: str,\n) -> PondPrivateTensor:\n\n    with tf.name_scope(""avgpool2d""):\n        y_on_0, y_on_1, scalar = _avgpool2d_core(\n            prot, x.unmasked, pool_size, strides, padding\n        )\n        return PondPrivateTensor(prot, y_on_0, y_on_1, x.is_scaled) * scalar\n\n\n#\n# batch_to_space_nd and space_to_batch_nd helpers\n#\n\n\ndef _batch_to_space_nd_core(prot, tensor, block_shape, crops):\n    tensor_on_0, tensor_on_1 = tensor.unwrapped\n\n    with tf.device(prot.server_0.device_name):\n        space_on_0 = tensor_on_0.batch_to_space_nd(block_shape, crops)\n\n    with tf.device(prot.server_1.device_name):\n        space_on_1 = tensor_on_1.batch_to_space_nd(block_shape, crops)\n\n    return space_on_0, space_on_1\n\n\ndef _batch_to_space_nd_public(prot, tensor, block_shape, crops):\n\n    with tf.name_scope(""batch_to_space_nd""):\n        space_on_0, space_on_1 = _batch_to_space_nd_core(\n            prot, tensor, block_shape, crops,\n        )\n\n    return PondPublicTensor(prot, space_on_0, space_on_1, tensor.is_scaled)\n\n\ndef _batch_to_space_nd_private(prot, tensor, block_shape, crops):\n\n    with tf.name_scope(""batch_to_space_nd""):\n        space_on_0, space_on_1 = _batch_to_space_nd_core(\n            prot, tensor, block_shape, crops,\n        )\n\n    return PondPrivateTensor(prot, space_on_0, space_on_1, tensor.is_scaled)\n\n\ndef _batch_to_space_nd_masked(prot, tensor, block_shape, crops):\n\n    with tf.name_scope(""batch_to_space_nd""):\n        space_on_0, space_on_1 = _batch_to_space_nd_core(\n            prot, tensor.unmasked, block_shape, crops,\n        )\n\n    return PondPrivateTensor(prot, space_on_0, space_on_1, tensor.is_scaled)\n\n\ndef _space_to_batch_nd_core(prot, tensor, block_shape, paddings):\n    tensor_on_0, tensor_on_1 = tensor.unwrapped\n\n    with tf.name_scope(""space_to_batch_nd""):\n\n        with tf.device(prot.server_0.device_name):\n            batch_on_0 = tensor_on_0.space_to_batch_nd(block_shape, paddings)\n\n        with tf.device(prot.server_1.device_name):\n            batch_on_1 = tensor_on_1.space_to_batch_nd(block_shape, paddings)\n\n    return batch_on_0, batch_on_1\n\n\ndef _space_to_batch_nd_public(prot, tensor, block_shape, paddings):\n\n    with tf.name_scope(""space_to_batch_nd""):\n        batch_on_0, batch_on_1 = _space_to_batch_nd_core(\n            prot, tensor, block_shape, paddings,\n        )\n\n    return PondPublicTensor(prot, batch_on_0, batch_on_1, tensor.is_scaled)\n\n\ndef _space_to_batch_nd_private(prot, tensor, block_shape, paddings):\n\n    with tf.name_scope(""space_to_batch_nd""):\n        batch_on_0, batch_on_1 = _space_to_batch_nd_core(\n            prot, tensor, block_shape, paddings,\n        )\n\n    return PondPrivateTensor(prot, batch_on_0, batch_on_1, tensor.is_scaled)\n\n\ndef _space_to_batch_nd_masked(prot, tensor, block_shape, paddings):\n\n    with tf.name_scope(""space_to_batch_nd""):\n        batch_on_0, batch_on_1 = _space_to_batch_nd_core(\n            prot, tensor.unmasked, block_shape, paddings,\n        )\n\n    return PondPrivateTensor(prot, batch_on_0, batch_on_1, tensor.is_scaled)\n\n\n#\n# indexing helpers\n#\n\n\ndef _indexer_public(prot: Pond, tensor: PondPublicTensor, slc) -> ""PondPublicTensor"":\n\n    with tf.name_scope(""index""):\n\n        with tf.device(prot.server_0.device_name):\n            v_on_0 = tensor.value_on_0[slc]\n\n        with tf.device(prot.server_1.device_name):\n            v_on_1 = tensor.value_on_1[slc]\n\n        return PondPublicTensor(prot, v_on_0, v_on_1, tensor.is_scaled)\n\n\ndef _indexer_private(prot: Pond, tensor: PondPrivateTensor, slc) -> ""PondPrivateTensor"":\n\n    with tf.name_scope(""index""):\n\n        with tf.device(prot.server_0.device_name):\n            s0 = tensor.share0[slc]\n\n        with tf.device(prot.server_1.device_name):\n            s1 = tensor.share1[slc]\n\n    return PondPrivateTensor(prot, s0, s1, tensor.is_scaled)\n\n\ndef _indexer_masked(prot: Pond, tensor: PondMaskedTensor, slc) -> ""PondMaskedTensor"":\n\n    with tf.name_scope(""index""):\n\n        # TODO(Morten)\n        # we could save a0 and a1 on disk as well; what\'s best performance wise?\n        a = prot.triple_source.indexer_mask(tensor.a, slc)\n\n        with tf.device(prot.server_0.device_name):\n            a0 = tensor.a0[slc]\n            alpha_on_0 = tensor.alpha_on_0[slc]\n\n        with tf.device(prot.server_1.device_name):\n            a1 = tensor.a1[slc]\n            alpha_on_1 = tensor.alpha_on_1[slc]\n\n        return PondMaskedTensor(\n            prot,\n            tensor.unmasked[slc],\n            a,\n            a0,\n            a1,\n            alpha_on_0,\n            alpha_on_1,\n            tensor.is_scaled,\n        )\n\n\n#\n# transpose helpers\n#\n\n\ndef _transpose_public(prot, x, perm=None):\n    assert isinstance(x, PondPublicTensor)\n\n    x_on_0, x_on_1 = x.unwrapped\n\n    with tf.name_scope(""transpose""):\n\n        with tf.device(prot.server_0.device_name):\n            x_on_0_t = x_on_0.transpose(perm=perm)\n\n        with tf.device(prot.server_1.device_name):\n            x_on_1_t = x_on_1.transpose(perm=perm)\n\n        return PondPublicTensor(prot, x_on_0_t, x_on_1_t, x.is_scaled)\n\n\ndef _transpose_private(prot, x, perm=None):\n    assert isinstance(x, PondPrivateTensor)\n\n    x0, x1 = x.unwrapped\n\n    with tf.name_scope(""transpose""):\n\n        with tf.device(prot.server_0.device_name):\n            x0_t = x0.transpose(perm=perm)\n\n        with tf.device(prot.server_1.device_name):\n            x1_t = x1.transpose(perm=perm)\n\n        return PondPrivateTensor(prot, x0_t, x1_t, x.is_scaled)\n\n\ndef _transpose_masked(prot, x, perm=None):\n    assert isinstance(x, PondMaskedTensor)\n\n    a, a0, a1, alpha_on_0, alpha_on_1 = x.unwrapped\n\n    with tf.name_scope(""transpose""):\n        # TODO(Arash) a is undefined, fix in another commit\n        a_t = prot.triple_source.transpose_mask(a, perm=perm)\n\n        with tf.device(prot.server_0.device_name):\n            a0_t = a0.transpose(perm=perm)\n            alpha_on_0_t = alpha_on_0.transpose(perm=perm)\n\n        with tf.device(prot.server_1.device_name):\n            a1_t = a1.transpose(perm=perm)\n            alpha_on_1_t = alpha_on_1.transpose(perm=perm)\n\n        return PondMaskedTensor(\n            prot,\n            prot.transpose(x.unmasked, perm=perm),\n            a_t,\n            a0_t,\n            a1_t,\n            alpha_on_0_t,\n            alpha_on_1_t,\n            x.is_scaled,\n        )\n\n\n#\n# strided slice helpers\n#\n\n\ndef _strided_slice_public(prot, x: PondPublicTensor, args: Any, kwargs: Any):\n    assert isinstance(x, PondPublicTensor)\n\n    x_on_0, x_on_1 = x.unwrapped\n\n    with tf.name_scope(""strided_slice""):\n\n        with tf.device(prot.server_0.device_name):\n            x_on_0_slice = x_on_0.strided_slice(args, kwargs)\n\n        with tf.device(prot.server_1.device_name):\n            x_on_1_slice = x_on_1.strided_slice(args, kwargs)\n\n        return PondPublicTensor(prot, x_on_0_slice, x_on_1_slice, x.is_scaled)\n\n\ndef _strided_slice_private(prot, x: PondPrivateTensor, args: Any, kwargs: Any):\n    assert isinstance(x, PondPrivateTensor)\n\n    x0, x1 = x.unwrapped\n\n    with tf.name_scope(""strided_slice""):\n\n        with tf.device(prot.server_0.device_name):\n            x0_slice = x0.strided_slice(args, kwargs)\n\n        with tf.device(prot.server_1.device_name):\n            x1_slice = x1.strided_slice(args, kwargs)\n\n        return PondPrivateTensor(prot, x0_slice, x1_slice, x.is_scaled)\n\n\ndef _strided_slice_masked(prot, x: PondMaskedTensor, args: Any, kwargs: Any):\n    assert isinstance(x, PondMaskedTensor)\n\n    a, a0, a1, alpha_on_0, alpha_on_1 = x.unwrapped\n\n    with tf.name_scope(""strided_slice""):\n\n        a_slice = prot.triple_source.strided_slice_mask(a, args, kwargs)\n\n        with tf.device(prot.server_0.device_name):\n            a0_slice = a0.strided_slice(args, kwargs)\n            alpha_on_0_slice = alpha_on_0.strided_slice(args, kwargs)\n\n        with tf.device(prot.server_1.device_name):\n            a1_slice = a1.strided_slice(args, kwargs)\n            alpha_on_1_slice = alpha_on_1.strided_slice(args, kwargs)\n\n        return PondMaskedTensor(\n            prot,\n            prot.strided_slice(x.unmasked, args, kwargs),\n            a_slice,\n            a0_slice,\n            a1_slice,\n            alpha_on_0_slice,\n            alpha_on_1_slice,\n            x.is_scaled,\n        )\n\n\n#\n# gather helpers\n#\n\n\ndef _gather_public(\n    prot: Pond, x: PondPublicTensor, indices: list, axis: int = 0,\n) -> PondPublicTensor:\n\n    x_on_0, x_on_1 = x.unwrapped\n\n    with tf.name_scope(""gather""):\n\n        with tf.device(prot.server_0.device_name):\n            y_on_0_g = x_on_0.gather(indices, axis=axis)\n\n        with tf.device(prot.server_1.device_name):\n            y_on_1_g = x_on_1.gather(indices, axis=axis)\n\n        return PondPublicTensor(prot, y_on_0_g, y_on_1_g, x.is_scaled)\n\n\ndef _gather_private(\n    prot: Pond, x: PondPrivateTensor, indices: list, axis: int = 0,\n) -> PondPrivateTensor:\n\n    x0, x1 = x.unwrapped\n\n    with tf.name_scope(""gather""):\n\n        with tf.device(prot.server_0.device_name):\n            y0_g = x0.gather(indices, axis=axis)\n\n        with tf.device(prot.server_1.device_name):\n            y1_g = x1.gather(indices, axis=axis)\n\n        return PondPrivateTensor(prot, y0_g, y1_g, x.is_scaled)\n\n\ndef _gather_masked(\n    prot: Pond, x: PondMaskedTensor, indices: list, axis: int = 0,\n) -> PondMaskedTensor:\n    assert isinstance(x, PondMaskedTensor)\n    a, a0, a1, alpha_on_0, alpha_on_1 = x.unwrapped\n\n    with tf.name_scope(""gather""):\n\n        a_g = prot.triple_source.gather_mask(a, indices, axis)\n\n        with tf.device(prot.server_0.device_name):\n            a0_g = a0.gather(indices, axis=axis)\n            alpha_on_0_g = alpha_on_0.gather(indices, axis=axis)\n\n        with tf.device(prot.server_1.device_name):\n            a1_g = a1.gather(indices, axis=axis)\n            alpha_on_1_g = alpha_on_1.gather(indices, axis=axis)\n\n        return PondMaskedTensor(\n            prot,\n            prot.gather(x.unmasked, indices, axis=axis),\n            a_g,\n            a0_g,\n            a1_g,\n            alpha_on_0_g,\n            alpha_on_1_g,\n            x.is_scaled,\n        )\n\n\n#\n# split helpers\n#\n\n\ndef _split_public(\n    prot: Pond, x: PondPublicTensor, num_split: Union[int, list], axis: int = 0,\n) -> List[PondPublicTensor]:\n\n    x_on_0, x_on_1 = x.unwrapped\n\n    with tf.name_scope(""split""):\n        with tf.device(prot.server_0.device_name):\n            ys_on_0 = x_on_0.split(num_split, axis=axis)\n\n        with tf.device(prot.server_1.device_name):\n            ys_on_1 = x_on_1.split(num_split, axis=axis)\n\n        return [\n            PondPublicTensor(prot, y_on_0, y_on_1, x.is_scaled)\n            for y_on_0, y_on_1 in zip(ys_on_0, ys_on_1)\n        ]\n\n\ndef _split_private(\n    prot: Pond, x: PondPrivateTensor, num_split: Union[int, list], axis: int = 0,\n) -> List[PondPrivateTensor]:\n\n    x0, x1 = x.unwrapped\n\n    with tf.name_scope(""split""):\n\n        with tf.device(prot.server_0.device_name):\n            ys0 = x0.split(num_split, axis=axis)\n\n        with tf.device(prot.server_1.device_name):\n            ys1 = x1.split(num_split, axis=axis)\n\n        return [\n            PondPrivateTensor(prot, y0, y1, x.is_scaled) for y0, y1 in zip(ys0, ys1)\n        ]\n\n\ndef _split_masked(\n    prot: Pond, x: PondMaskedTensor, num_split: Union[int, list], axis=0,\n) -> List[PondMaskedTensor]:\n\n    a, a0, a1, alpha_on_0, alpha_on_1 = x.unwrapped\n\n    with tf.name_scope(""split""):\n\n        bs = prot.triple_source.split_mask(a, num_split=num_split, axis=axis)\n\n        with tf.device(prot.server_0.device_name):\n            bs0 = a0.split(num_split, axis=axis)\n            betas_on_0 = alpha_on_0.split(num_split, axis=axis)\n\n        with tf.device(prot.server_1.device_name):\n            bs1 = a1.split(num_split, axis=axis)\n            betas_on_1 = alpha_on_1.split(num_split, axis=axis)\n\n            ys = prot.split(x.unmasked, num_split, axis=axis)\n\n        return [\n            PondMaskedTensor(prot, y, b, b0, b1, beta_on_0, beta_on_1, x.is_scaled)\n            for (y, b, b0, b1, beta_on_0, beta_on_1) in zip(\n                ys, bs, bs0, bs1, betas_on_0, betas_on_1,\n            )\n        ]\n\n\n#\n# stack helpers\n#\n\n\ndef _stack_public(prot: Pond, xs: List[PondPublicTensor], axis=0,) -> PondPublicTensor:\n    assert all(x.is_scaled for x in xs) or all(not x.is_scaled for x in xs)\n\n    factory = xs[0].backing_dtype\n    is_scaled = xs[0].is_scaled\n    xs_on_0, xs_on_1 = zip(*(x.unwrapped for x in xs))\n\n    with tf.name_scope(""stack""):\n\n        with tf.device(prot.server_0.device_name):\n            x_on_0_stacked = factory.stack(xs_on_0, axis=axis)\n\n        with tf.device(prot.server_1.device_name):\n            x_on_1_stacked = factory.stack(xs_on_1, axis=axis)\n\n        return PondPublicTensor(prot, x_on_0_stacked, x_on_1_stacked, is_scaled)\n\n\ndef _stack_private(\n    prot: Pond, xs: List[PondPrivateTensor], axis=0,\n) -> PondPrivateTensor:\n    assert all(x.is_scaled for x in xs) or all(not x.is_scaled for x in xs)\n\n    factory = xs[0].backing_dtype\n    is_scaled = xs[0].is_scaled\n    xs0, xs1 = zip(*(x.unwrapped for x in xs))\n\n    with tf.name_scope(""stack""):\n\n        with tf.device(prot.server_0.device_name):\n            x0_stacked = factory.stack(xs0, axis=axis)\n\n        with tf.device(prot.server_1.device_name):\n            x1_stacked = factory.stack(xs1, axis=axis)\n\n        return PondPrivateTensor(prot, x0_stacked, x1_stacked, is_scaled)\n\n\ndef _stack_masked(prot: Pond, xs: List[PondMaskedTensor], axis=0,) -> PondMaskedTensor:\n    assert all(x.is_scaled for x in xs) or all(not x.is_scaled for x in xs)\n\n    factory = xs[0].backing_dtype\n    is_scaled = xs[0].is_scaled\n    a, a0, a1, alpha_on_0, alpha_on_1 = zip(*(x.unwrapped for x in xs))\n\n    with tf.name_scope(""stack""):\n\n        a_stacked = prot.triple_source.stack_mask(a, axis=axis)\n\n        with tf.device(prot.server_0.device_name):\n            a0_stacked = factory.stack(a0, axis=axis)\n            alpha_on_0_stacked = factory.stack(alpha_on_0, axis=axis)\n\n        with tf.device(prot.server_1.device_name):\n            a1_stacked = factory.stack(a1, axis=axis)\n            alpha_on_1_stacked = factory.stack(alpha_on_1, axis=axis)\n\n        return PondMaskedTensor(\n            prot,\n            prot.stack([x.unmasked for x in xs], axis=axis),\n            a_stacked,\n            a0_stacked,\n            a1_stacked,\n            alpha_on_0_stacked,\n            alpha_on_1_stacked,\n            is_scaled,\n        )\n\n\n#\n# concat helpers\n#\n\n\ndef _concat_public(\n    prot: Pond, xs: List[PondPublicTensor], axis: int,\n) -> PondPublicTensor:\n    assert all(x.is_scaled for x in xs) or all(not x.is_scaled for x in xs)\n\n    factory = xs[0].backing_dtype\n    is_scaled = xs[0].is_scaled\n    xs_on_0, xs_on_1 = zip(*(x.unwrapped for x in xs))\n\n    with tf.name_scope(""concat""):\n\n        with tf.device(prot.server_0.device_name):\n            x_on_0_concat = factory.concat(xs_on_0, axis=axis)\n\n        with tf.device(prot.server_1.device_name):\n            x_on_1_concat = factory.concat(xs_on_1, axis=axis)\n\n        return PondPublicTensor(prot, x_on_0_concat, x_on_1_concat, is_scaled)\n\n\ndef _concat_private(\n    prot: Pond, xs: List[PondPrivateTensor], axis,\n) -> PondPrivateTensor:\n    assert all(x.is_scaled for x in xs) or all(not x.is_scaled for x in xs)\n\n    factory = xs[0].backing_dtype\n    is_scaled = xs[0].is_scaled\n    xs0, xs1 = zip(*(x.unwrapped for x in xs))\n\n    with tf.name_scope(""concat""):\n\n        with tf.device(prot.server_0.device_name):\n            x0_concat = factory.concat(xs0, axis=axis)\n\n        with tf.device(prot.server_1.device_name):\n            x1_concat = factory.concat(xs1, axis=axis)\n\n        return PondPrivateTensor(prot, x0_concat, x1_concat, is_scaled)\n\n\ndef _concat_masked(\n    prot: Pond, xs: List[PondMaskedTensor], axis: int,\n) -> PondMaskedTensor:\n    assert all(x.is_scaled for x in xs) or all(not x.is_scaled for x in xs)\n\n    factory = xs[0].backing_dtype\n    is_scaled = xs[0].is_scaled\n    a, a0, a1, alpha_on_0, alpha_on_1 = zip(*(x.unwrapped for x in xs))\n\n    with tf.name_scope(""concat""):\n\n        a_concat = prot.triple_source.concat_mask(a, axis=axis)\n\n        with tf.device(prot.server_0.device_name):\n            a0_concat = factory.concat(a0, axis=axis)\n            alpha_on_0_concat = factory.concat(alpha_on_0, axis=axis)\n\n        with tf.device(prot.server_1.device_name):\n            a1_concat = factory.concat(a1, axis=axis)\n            alpha_on_1_concat = factory.concat(alpha_on_1, axis=axis)\n\n        return PondMaskedTensor(\n            prot,\n            prot.concat([x.unmasked for x in xs], axis=axis),\n            a_concat,\n            a0_concat,\n            a1_concat,\n            alpha_on_0_concat,\n            alpha_on_1_concat,\n            is_scaled,\n        )\n\n\n#\n# mask helpers\n#\n\n\ndef _mask_private(prot: Pond, x: PondPrivateTensor) -> PondMaskedTensor:\n    assert isinstance(x, PondPrivateTensor)\n\n    x0, x1 = x.unwrapped\n\n    with tf.name_scope(""mask""):\n\n        a, a0, a1 = prot.triple_source.mask(x.backing_dtype, x.shape)\n\n        with tf.name_scope(""online""):\n\n            with tf.device(prot.server_0.device_name):\n                alpha0 = x0 - a0\n\n            with tf.device(prot.server_1.device_name):\n                alpha1 = x1 - a1\n\n            with tf.device(prot.server_0.device_name):\n                alpha_on_0 = alpha0 + alpha1\n\n            with tf.device(prot.server_1.device_name):\n                alpha_on_1 = alpha0 + alpha1\n\n    return PondMaskedTensor(prot, x, a, a0, a1, alpha_on_0, alpha_on_1, x.is_scaled,)\n\n\n#\n# sqrt helpers\n#\n\n\ndef _sqrt_public(prot, x):\n    assert isinstance(x, PondPublicTensor), type(x)\n\n    backing_dtype = x.backing_dtype\n    x_on_0, x_on_1 = x.unwrapped\n    is_scaled = x.is_scaled\n    assert is_scaled, ""Can only sqrt of scaled numbers""\n\n    with tf.name_scope(""sqrt""):\n\n        with tf.device(prot.server_0.device_name):\n            # decode value as ordinary tensor locally and compute sqrt\n            x_on_0_decoded = prot._decode(x_on_0, is_scaled)\n            y_on_0_decoded = tf.math.sqrt(x_on_0_decoded)\n            # re-encode and re-wrap\n            y_on_0 = backing_dtype.tensor(\n                prot._encode(\n                    y_on_0_decoded,\n                    apply_scaling=is_scaled,\n                    tf_int_type=backing_dtype.native_type,\n                )\n            )\n\n        with tf.device(prot.server_1.device_name):\n            # decode value as ordinary tensor locally and compute sqrt\n            x_on_1_decoded = prot._decode(x_on_1, is_scaled)\n            y_on_1_decoded = tf.math.sqrt(x_on_1_decoded)\n            # re-encode and re-wrap\n            y_on_1 = backing_dtype.tensor(\n                prot._encode(\n                    y_on_1_decoded,\n                    apply_scaling=is_scaled,\n                    tf_int_type=backing_dtype.native_type,\n                )\n            )\n\n        y = PondPublicTensor(prot, y_on_0, y_on_1, is_scaled)\n        return y\n\n\ndef _reshape_public(\n    prot: Pond, x: PondPublicTensor, shape: List[int],\n) -> PondPublicTensor:\n    assert isinstance(x, PondPublicTensor)\n\n    x_on_0, x_on_1 = x.unwrapped\n\n    with tf.name_scope(""reshape""):\n\n        with tf.device(prot.server_0.device_name):\n            x_on_0_reshaped = x_on_0.reshape(shape)\n\n        with tf.device(prot.server_1.device_name):\n            x_on_1_reshaped = x_on_1.reshape(shape)\n\n        return PondPublicTensor(prot, x_on_0_reshaped, x_on_1_reshaped, x.is_scaled,)\n\n\ndef _reshape_private(\n    prot: Pond, x: PondPrivateTensor, shape: List[int],\n) -> PondPrivateTensor:\n    assert isinstance(x, PondPrivateTensor)\n\n    x0, x1 = x.unwrapped\n\n    with tf.name_scope(""reshape""):\n\n        with tf.device(prot.server_0.device_name):\n            x0_reshaped = x0.reshape(shape)\n\n        with tf.device(prot.server_1.device_name):\n            x1_reshaped = x1.reshape(shape)\n\n        return PondPrivateTensor(prot, x0_reshaped, x1_reshaped, x.is_scaled)\n\n\ndef _reshape_masked(\n    prot: Pond, x: PondMaskedTensor, shape: List[int],\n) -> PondMaskedTensor:\n    assert isinstance(x, PondMaskedTensor)\n    a, a0, a1, alpha_on_0, alpha_on_1 = x.unwrapped\n\n    with tf.name_scope(""reshape""):\n\n        a_reshaped = prot.triple_source.reshape_mask(a, shape=shape)\n\n        with tf.device(prot.server_0.device_name):\n            a0_reshaped = a0.reshape(shape)\n            alpha_on_0_reshaped = alpha_on_0.reshape(shape)\n\n        with tf.device(prot.server_1.device_name):\n            a1_reshaped = a1.reshape(shape)\n            alpha_on_1_reshaped = alpha_on_1.reshape(shape)\n\n        return PondMaskedTensor(\n            prot,\n            prot.reshape(x.unmasked, shape),\n            a_reshaped,\n            a0_reshaped,\n            a1_reshaped,\n            alpha_on_0_reshaped,\n            alpha_on_1_reshaped,\n            x.is_scaled,\n        )\n\n\n#\n# negative helpers\n#\n\n\ndef _negative_public(prot: Pond, x: PondPublicTensor) -> PondPublicTensor:\n    assert isinstance(x, PondPublicTensor)\n\n    x_on_0, x_on_1 = x.unwrapped\n\n    with tf.name_scope(""negative""):\n\n        with tf.device(prot.server_0.device_name):\n            x_on_0_negative = x_on_0.negative()\n\n        with tf.device(prot.server_1.device_name):\n            x_on_1_negative = x_on_1.negative()\n\n        return PondPublicTensor(prot, x_on_0_negative, x_on_1_negative, x.is_scaled,)\n\n\ndef _negative_private(prot: Pond, x: PondPrivateTensor) -> PondPrivateTensor:\n    assert isinstance(x, PondPrivateTensor)\n\n    x0, x1 = x.unwrapped\n\n    with tf.name_scope(""negative""):\n\n        with tf.device(prot.server_0.device_name):\n            x0_negative = x0.negative()\n\n        with tf.device(prot.server_1.device_name):\n            x1_negative = x1.negative()\n\n        return PondPrivateTensor(prot, x0_negative, x1_negative, x.is_scaled)\n\n\ndef _negative_masked(prot: Pond, x: PondMaskedTensor) -> PondMaskedTensor:\n    assert isinstance(x, PondMaskedTensor)\n    a, a0, a1, alpha_on_0, alpha_on_1 = x.unwrapped\n\n    with tf.name_scope(""negative""):\n\n        a_negative = prot.triple_source.negative_mask(a)\n\n        with tf.device(prot.server_0.device_name):\n            a0_negative = a0.negative()\n            alpha_on_0_negative = alpha_on_0.negative()\n\n        with tf.device(prot.server_1.device_name):\n            a1_negative = a1.negative()\n            alpha_on_1_negative = alpha_on_1.negative()\n\n        return PondMaskedTensor(\n            prot,\n            prot.negative(x.unmasked),\n            a_negative,\n            a0_negative,\n            a1_negative,\n            alpha_on_0_negative,\n            alpha_on_1_negative,\n            x.is_scaled,\n        )\n\n\n#\n# expand dims helpers\n#\n\n\ndef _expand_dims_public(\n    prot: Pond, x: PondPublicTensor, axis: Optional[int] = None,\n) -> PondPublicTensor:\n    assert isinstance(x, PondPublicTensor)\n\n    x_on_0, x_on_1 = x.unwrapped\n\n    with tf.name_scope(""expand""):\n\n        with tf.device(prot.server_0.device_name):\n            x_on_0_e = x_on_0.expand_dims(axis=axis)\n\n        with tf.device(prot.server_1.device_name):\n            x_on_1_e = x_on_1.expand_dims(axis=axis)\n\n        return PondPublicTensor(prot, x_on_0_e, x_on_1_e, x.is_scaled)\n\n\ndef _expand_dims_private(\n    prot: Pond, x: PondPrivateTensor, axis: Optional[int] = None,\n) -> PondPrivateTensor:\n    assert isinstance(x, PondPrivateTensor)\n\n    x0, x1 = x.unwrapped\n\n    with tf.name_scope(""expand""):\n\n        with tf.device(prot.server_0.device_name):\n            x0_e = x0.expand_dims(axis=axis)\n\n        with tf.device(prot.server_1.device_name):\n            x1_e = x1.expand_dims(axis=axis)\n\n        return PondPrivateTensor(prot, x0_e, x1_e, x.is_scaled)\n\n\ndef _expand_dims_masked(\n    prot: Pond, x: PondMaskedTensor, axis: Optional[int] = None,\n) -> PondMaskedTensor:\n    assert isinstance(x, PondMaskedTensor)\n    a, a0, a1, alpha_on_0, alpha_on_1 = x.unwrapped\n\n    with tf.name_scope(""expand""):\n\n        a_e = prot.triple_source.expand_dims_mask(a, axis=axis)\n\n        with tf.device(prot.server_0.device_name):\n            a0_e = a0.expand_dims(axis=axis)\n            alpha_on_0_e = alpha_on_0.expand_dims(axis=axis)\n\n        with tf.device(prot.server_1.device_name):\n            a1_e = a1.expand_dims(axis=axis)\n            alpha_on_1_e = alpha_on_1.expand_dims(axis=axis)\n\n        return PondMaskedTensor(\n            prot,\n            prot.expand_dims(x.unmasked, axis=axis),\n            a_e,\n            a0_e,\n            a1_e,\n            alpha_on_0_e,\n            alpha_on_1_e,\n            x.is_scaled,\n        )\n\n\n#\n# squeeze helpers\n#\n\n\ndef _squeeze_public(\n    prot: Pond, x: PondPublicTensor, axis: Optional[int] = None,\n) -> PondPublicTensor:\n    assert isinstance(x, PondPublicTensor)\n\n    x_on_0, x_on_1 = x.unwrapped\n\n    with tf.name_scope(""squeeze""):\n\n        with tf.device(prot.server_0.device_name):\n            x_on_0_squeezed = x_on_0.squeeze(axis)\n\n        with tf.device(prot.server_1.device_name):\n            x_on_1_squeezed = x_on_1.squeeze(axis)\n\n        return PondPublicTensor(prot, x_on_0_squeezed, x_on_1_squeezed, x.is_scaled,)\n\n\ndef _squeeze_private(\n    prot: Pond, x: PondPrivateTensor, axis: Optional[int] = None,\n) -> PondPrivateTensor:\n    assert isinstance(x, PondPrivateTensor)\n\n    x0, x1 = x.unwrapped\n\n    with tf.name_scope(""squeeze""):\n\n        with tf.device(prot.server_0.device_name):\n            x0_squeezed = x0.squeeze(axis)\n\n        with tf.device(prot.server_1.device_name):\n            x1_squeezed = x1.squeeze(axis)\n\n        return PondPrivateTensor(prot, x0_squeezed, x1_squeezed, x.is_scaled)\n\n\ndef _squeeze_masked(prot: Pond, x: PondMaskedTensor, axis=None,) -> PondMaskedTensor:\n    assert isinstance(x, PondMaskedTensor)\n\n    _, a0, a1, alpha_on_0, alpha_on_1 = x.unwrapped\n\n    with tf.name_scope(""squeeze""):\n\n        a_squeezed = prot.triple_source.squeeze_mask(axis=axis)\n\n        with tf.device(prot.server_0.device_name):\n            a0_squeezed = a0.squeeze(axis)\n            alpha_on_0_squeezed = alpha_on_0.squeeze(axis)\n\n        with tf.device(prot.server_1.device_name):\n            a1_squeezed = a1.squeeze(axis)\n            alpha_on_1_squeezed = alpha_on_1.squeeze(axis)\n\n        return PondMaskedTensor(\n            prot,\n            prot.squeeze(x.unmasked),\n            a_squeezed,\n            a0_squeezed,\n            a1_squeezed,\n            alpha_on_0_squeezed,\n            alpha_on_1_squeezed,\n            x.is_scaled,\n        )\n\n\n#\n# equal helpers\n#\n\n\ndef _equal_public_public(\n    prot: Pond, x: PondPublicTensor, y: PondPublicTensor,\n) -> PondPublicTensor:\n\n    x_on_0, x_on_1 = x.unwrapped\n    y_on_0, y_on_1 = y.unwrapped\n\n    with tf.name_scope(""equal""):\n\n        with tf.device(prot.server_0.device_name):\n            z_on_0 = x_on_0.equal(y_on_0)\n\n        with tf.device(prot.server_0.device_name):\n            z_on_1 = x_on_1.equal(y_on_1)\n\n        return PondPublicTensor(prot, z_on_0, z_on_1, False)\n\n\n#\n# zeros helpers\n#\n\n\ndef _zeros_private(\n    prot,\n    shape,\n    apply_scaling: bool = True,\n    name: Optional[str] = None,\n    factory: Optional[AbstractFactory] = None,\n) -> PondPrivateTensor:\n\n    zeros_array = np.zeros(shape)\n\n    factory = factory or prot.tensor_factory\n\n    with tf.name_scope(""private-zeros{}"".format(""-"" + name if name else """")):\n\n        v = factory.tensor(prot._encode(zeros_array, apply_scaling))\n        v0, v1 = prot._share(v)\n\n        with tf.device(prot.server_0.device_name):\n            x0 = factory.variable(v0)\n\n        with tf.device(prot.server_1.device_name):\n            x1 = factory.variable(v1)\n\n    x = PondPrivateTensor(prot, x0, x1, apply_scaling)\n    return x\n\n\ndef _zeros_public(\n    prot,\n    shape,\n    apply_scaling: bool = True,\n    name: Optional[str] = None,\n    factory: Optional[AbstractFactory] = None,\n) -> PondPublicTensor:\n\n    zeros_array = np.zeros(shape)\n\n    factory = factory or prot.tensor_factory\n\n    with tf.name_scope(""private-zeros{}"".format(""-"" + name if name else """")):\n\n        enc = prot._encode(zeros_array, apply_scaling)\n        v = factory.tensor(enc)\n\n        with tf.device(prot.server_0.device_name):\n            x0 = factory.variable(v)\n\n        with tf.device(prot.server_1.device_name):\n            x1 = factory.variable(v)\n\n    x = PondPublicTensor(prot, x0, x1, apply_scaling)\n    return x\n'"
tf_encrypted/protocol/pond/pond_test.py,21,"b'# pylint: disable=missing-docstring\n# pylint: disable=protected-access\n\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.protocol.pond import PondMaskedTensor\nfrom tf_encrypted.protocol.pond import PondPublicTensor\nfrom tf_encrypted.tensor import fixed100\nfrom tf_encrypted.tensor import fixed100_ni\nfrom tf_encrypted.tensor import int64factory\nfrom tf_encrypted.tensor import int100factory\nfrom tf_encrypted.tensor import native_factory\n\nfrom .pond import _gather_masked\nfrom .pond import _indexer_masked\nfrom .pond import _negative_masked\n\n\nclass TestPond(unittest.TestCase):\n    def test_encode(self):\n\n        with tf.Graph().as_default():\n            prot = tfe.protocol.Pond()\n\n            expected = np.array([1234567.9875])\n            x = prot.define_constant(expected)\n\n            with tfe.Session() as sess:\n                actual = sess.run(x)\n                np.testing.assert_array_almost_equal(actual, expected, decimal=3)\n\n\nclass TestTruncate(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_interactive_truncate(self):\n\n        prot = tfe.protocol.Pond(\n            tensor_factory=int100factory, fixedpoint_config=fixed100,\n        )\n\n        # TODO[Morten] remove this condition\n        if prot.tensor_factory not in [tfe.tensor.int64factory]:\n\n            expected = np.array([12345.6789])\n\n            w = prot.define_private_variable(\n                expected * prot.fixedpoint_config.scaling_factor\n            )  # double precision\n            v = prot.truncate(w)  # single precision\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                actual = sess.run(v.reveal())\n\n            np.testing.assert_allclose(actual, expected)\n\n    def test_noninteractive_truncate(self):\n\n        prot = tfe.protocol.Pond(\n            tensor_factory=int100factory, fixedpoint_config=fixed100_ni,\n        )\n\n        with tfe.Session() as sess:\n\n            expected = np.array([12345.6789])\n\n            w = prot.define_private_variable(\n                expected * prot.fixedpoint_config.scaling_factor\n            )  # double precision\n            v = prot.truncate(w)  # single precision\n\n            sess.run(tf.global_variables_initializer())\n            actual = sess.run(v.reveal())\n\n            np.testing.assert_allclose(actual, expected)\n\n\nclass TestPondPublicEqual(unittest.TestCase):\n    def test_public_compare(self):\n\n        expected = np.array([1, 0, 1, 0])\n\n        with tfe.protocol.Pond() as prot:\n\n            x_raw = prot.tensor_factory.constant(np.array([100, 200, 100, 300]))\n            x = PondPublicTensor(\n                prot, value_on_0=x_raw, value_on_1=x_raw, is_scaled=False\n            )\n\n            res = prot.equal(x, 100)\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                answer = sess.run(res)\n\n            assert np.array_equal(answer, expected)\n\n\nclass TestPondPublicDivision(unittest.TestCase):\n    def test_public_division(self):\n\n        x_raw = np.array([10.0, 20.0, 30.0, 40.0])\n        y_raw = np.array([1.0, 2.0, 3.0, 4.0])\n        expected = x_raw / y_raw\n\n        with tfe.protocol.Pond() as prot:\n\n            x = prot.define_private_variable(x_raw)\n            y = prot.define_constant(y_raw)\n            z = x / y\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                actual = sess.run(z.reveal())\n\n            np.testing.assert_array_almost_equal(actual, expected, decimal=2)\n\n    def test_public_reciprocal(self):\n\n        x_raw = np.array([10.0, 20.0, 30.0, 40.0])\n        expected = 1.0 / x_raw\n\n        with tfe.protocol.Pond() as prot:\n\n            x = prot.define_constant(x_raw)\n            y = prot.reciprocal(x)\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                actual = sess.run(y)\n\n            np.testing.assert_array_almost_equal(actual, expected, decimal=3)\n\n\nclass TestShare(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def _core_test_sharing(self, dtype):\n\n        expected = np.array([[1, 2, 3], [4, 5, 6]])\n\n        with tfe.protocol.Pond() as prot:\n\n            with tfe.Session() as sess:\n                shares = prot._share(dtype.tensor(expected))\n                actual = sess.run(prot._reconstruct(*shares).to_native())\n\n        np.testing.assert_array_equal(actual, expected)\n\n    def test_int64(self):\n        self._core_test_sharing(int64factory)\n\n    def test_int100(self):\n        self._core_test_sharing(int100factory)\n\n    def test_prime(self):\n        self._core_test_sharing(native_factory(tf.int32, 67))\n\n\nclass TestMasked(unittest.TestCase):\n    def _setup(self, dtype):\n        prot = tfe.protocol.Pond()\n        plain_tensor = dtype.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\n        # pylint: disable=protected-access\n        unmasked = prot._share_and_wrap(plain_tensor, False)\n        # pylint: enable=protected-access\n        a0 = dtype.sample_uniform(plain_tensor.shape)\n        a1 = dtype.sample_uniform(plain_tensor.shape)\n        a = a0 + a1\n        alpha = plain_tensor - a\n        x = PondMaskedTensor(self, unmasked, a, a0, a1, alpha, alpha, False)\n        return prot, x\n\n    def test_transpose_masked(self):\n\n        with tf.Graph().as_default():\n\n            prot, x = self._setup(int64factory)\n            transpose = prot.transpose(x)\n\n            with tfe.Session() as sess:\n                actual = sess.run(transpose.reveal().to_native())\n                expected = np.array([[1, 4], [2, 5], [3, 6]])\n                np.testing.assert_array_equal(actual, expected)\n\n    def test_indexer(self):\n\n        with tf.Graph().as_default():\n\n            prot, x = self._setup(int64factory)\n            indexed = _indexer_masked(prot, x, 0)\n\n            with tfe.Session() as sess:\n                actual = sess.run(indexed.reveal().to_native())\n                expected = np.array([1, 2, 3])\n                np.testing.assert_array_equal(actual, expected)\n\n    def test_gather(self):\n\n        with tf.Graph().as_default():\n\n            prot, x = self._setup(int64factory)\n            gathered = _gather_masked(prot, x, [0, 2], axis=1)\n\n            with tfe.Session() as sess:\n                actual = sess.run(gathered.reveal().to_native())\n                expected = np.array([[1, 3], [4, 6]])\n                np.testing.assert_array_equal(actual, expected)\n\n    def test_negative_masked(self):\n\n        with tf.Graph().as_default():\n\n            prot, x = self._setup(int64factory)\n            negative = _negative_masked(prot, x)\n\n            with tfe.Session() as sess:\n                actual = sess.run(negative.reveal().to_native())\n                expected = np.array([[-1, -2, -3], [-4, -5, -6]])\n                np.testing.assert_array_equal(actual, expected)\n\n\nclass TestIdentity(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_same_value_different_instance(self):\n\n        expected = np.array([[1, 2, 3], [4, 5, 6]])\n\n        with tfe.protocol.Pond() as prot:\n\n            x = prot.define_private_variable(expected)\n            y = prot.identity(x)\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                actual = sess.run(y.reveal())\n\n        assert x is not y\n        np.testing.assert_array_equal(actual, expected)\n\n\nclass TestPondAssign(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_assign_synchronization(self):\n        # from https://github.com/tf-encrypted/tf-encrypted/pull/665\n\n        prot = tfe.protocol.Pond()\n        tfe.set_protocol(prot)\n\n        def poc(x, y):\n            x_shares = x.unwrapped\n            y_shares = y.unwrapped\n            z_shares = [None, None]\n\n            with tf.name_scope(""fabricated_test""):\n                with tf.device(prot.server_0.device_name):\n                    z_shares[0] = x_shares[1] + y_shares[1]\n                with tf.device(prot.server_1.device_name):\n                    z_shares[1] = x_shares[0] + y_shares[0]\n\n            return tfe.protocol.pond.PondPrivateTensor(\n                prot, z_shares[0], z_shares[1], x.is_scaled\n            )\n\n        a = prot.define_private_variable(tf.ones(shape=(1, 1)))\n        b = prot.define_private_variable(tf.ones(shape=(1, 1)))\n\n        op = prot.assign(a, poc(a, b))\n\n        with tfe.Session() as sess:\n            sess.run(tfe.global_variables_initializer())\n\n            for _ in range(100):\n                sess.run(op)\n\n            result = sess.run(a.reveal())\n            assert result == np.array([101.0])\n\n    def test_public_assign(self):\n\n        with tfe.protocol.Pond() as prot:\n            x_var = prot.define_public_variable(np.zeros(shape=(2, 2)))\n            data = np.ones((2, 2))\n            x_pl = tfe.define_public_placeholder(shape=(2, 2))\n            fd = x_pl.feed(data.reshape((2, 2)))\n\n            with tfe.Session() as sess:\n                sess.run(tfe.assign(x_var, x_pl), feed_dict=fd)\n                result = sess.run(x_var)\n                np.testing.assert_array_equal(result, np.ones([2, 2]))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/protocol/pond/queues_test.py,2,"b'# pylint: disable=missing-docstring\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\n\n\nclass TestFIFO(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_fifo(self):\n\n        shape = (10, 10)\n\n        with tfe.protocol.Pond():\n\n            q = tfe.queue.FIFOQueue(capacity=10, shape=shape,)\n            assert isinstance(q, tfe.protocol.pond.AdditiveFIFOQueue)\n\n            raw = np.full(shape, 5)\n\n            x = tfe.define_private_input(""inputter"", lambda: tf.convert_to_tensor(raw))\n            assert isinstance(x, tfe.protocol.pond.PondPrivateTensor)\n\n            enqueue_op = q.enqueue(x)\n\n            y = q.dequeue()\n            assert isinstance(y, tfe.protocol.pond.PondPrivateTensor)\n            assert y.backing_dtype == x.backing_dtype\n            assert y.shape == x.shape\n\n        with tfe.Session() as sess:\n            sess.run(enqueue_op)\n            res = sess.run(y.reveal())\n\n            np.testing.assert_array_equal(res, raw)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/protocol/pond/triple_sources.py,62,"b'""""""Various sources for providing generalized Beaver triples for the Pond\nprotocol.""""""\nimport abc\nimport logging\nimport random\n\nimport tensorflow as tf\n\nfrom ...config import get_config\nfrom ...utils import reachable_nodes\nfrom ...utils import unwrap_fetches\nfrom ...utils import wrap_in_variables\n\nlogger = logging.getLogger(""tf_encrypted"")\n\n\nclass TripleSource(abc.ABC):\n    """"""Base class for triples sources.""""""\n\n    @abc.abstractmethod\n    def cache(self, a, cache_updater):\n        pass\n\n    @abc.abstractmethod\n    def initializer(self):\n        pass\n\n    @abc.abstractmethod\n    def generate_triples(self, fetches):\n        pass\n\n\nclass BaseTripleSource(TripleSource):\n    """"""\n  Partial triple source adding graph nodes for constructing and keeping track\n  of triples and their use. Subclasses must implement `_build_queues`.\n  """"""\n\n    def __init__(self, player0, player1, producer):\n        config = get_config()\n        self.player0 = config.get_player(player0) if player0 else None\n        self.player1 = config.get_player(player1) if player1 else None\n        self.producer = config.get_player(producer) if producer else None\n\n    def mask(self, backing_dtype, shape):\n\n        with tf.name_scope(""triple-generation""):\n            with tf.device(self.producer.device_name):\n                a0 = backing_dtype.sample_uniform(shape)\n                a1 = backing_dtype.sample_uniform(shape)\n                a = a0 + a1\n\n        d0, d1 = self._build_queues(a0, a1)\n        return a, d0, d1\n\n    def mul_triple(self, a, b):\n\n        with tf.name_scope(""triple-generation""):\n            with tf.device(self.producer.device_name):\n                ab = a * b\n                ab0, ab1 = self._share(ab)\n\n        return self._build_queues(ab0, ab1)\n\n    def square_triple(self, a):\n\n        with tf.name_scope(""triple-generation""):\n            with tf.device(self.producer.device_name):\n                aa = a * a\n                aa0, aa1 = self._share(aa)\n\n        return self._build_queues(aa0, aa1)\n\n    def matmul_triple(self, a, b):\n\n        with tf.name_scope(""triple-generation""):\n            with tf.device(self.producer.device_name):\n                ab = a.matmul(b)\n                ab0, ab1 = self._share(ab)\n\n        return self._build_queues(ab0, ab1)\n\n    def conv2d_triple(self, a, b, strides, padding):\n\n        with tf.device(self.producer.device_name):\n            with tf.name_scope(""triple""):\n                ab = a.conv2d(b, strides, padding)\n                ab0, ab1 = self._share(ab)\n\n        return self._build_queues(ab0, ab1)\n\n    def indexer_mask(self, a, slc):\n\n        with tf.name_scope(""mask-transformation""):\n            with tf.device(self.producer.device_name):\n                a_sliced = a[slc]\n\n        return a_sliced\n\n    def transpose_mask(self, a, perm):\n\n        with tf.name_scope(""mask-transformation""):\n            with tf.device(self.producer.device_name):\n                a_t = a.transpose(perm=perm)\n\n        return a_t\n\n    def gather_mask(self, a, indices, axis):\n\n        with tf.name_scope(""mask-transformation""):\n            with tf.device(self.producer.device_name):\n                a_g = a.gather(indices, axis=axis)\n\n        return a_g\n\n    def negative_mask(self, a):\n\n        with tf.name_scope(""mask-transformation""):\n            with tf.device(self.producer.device_name):\n                a_negative = a.negative()\n\n        return a_negative\n\n    def strided_slice_mask(self, a, args, kwargs):\n\n        with tf.name_scope(""mask-transformation""):\n            with tf.device(self.producer.device_name):\n                a_slice = a.strided_slice(args, kwargs)\n\n        return a_slice\n\n    def split_mask(self, a, num_split, axis):\n\n        with tf.name_scope(""mask-transformation""):\n            with tf.device(self.producer.device_name):\n                bs = a.split(num_split=num_split, axis=axis)\n\n        return bs\n\n    def stack_mask(self, bs, axis):\n\n        factory = bs[0].factory\n\n        with tf.name_scope(""mask-transformation""):\n            with tf.device(self.producer.device_name):\n                b_stacked = factory.stack(bs, axis=axis)\n\n        return b_stacked\n\n    def concat_mask(self, bs, axis):\n\n        factory = bs[0].factory\n\n        with tf.name_scope(""mask-transformation""):\n            with tf.device(self.producer.device_name):\n                b_stacked = factory.concat(bs, axis=axis)\n\n        return b_stacked\n\n    def reshape_mask(self, a, shape):\n\n        with tf.name_scope(""mask-transformation""):\n            with tf.device(self.producer.device_name):\n                a_reshaped = a.reshape(shape)\n\n        return a_reshaped\n\n    def expand_dims_mask(self, a, axis):\n\n        with tf.name_scope(""mask-transformation""):\n            with tf.device(self.producer.device_name):\n                a_e = a.expand_dims(axis=axis)\n\n        return a_e\n\n    def squeeze_mask(self, a, axis):\n\n        with tf.name_scope(""mask-transformation""):\n            with tf.device(self.producer.device_name):\n                a_squeezed = a.squeeze(axis=axis)\n\n        return a_squeezed\n\n    def _share(self, secret):\n        with tf.name_scope(""share""):\n            share0 = secret.factory.sample_uniform(secret.shape)\n            share1 = secret - share0\n            # randomized swap to distribute who gets the seed\n            if random.random() < 0.5:\n                share0, share1 = share1, share0\n        return share0, share1\n\n    @abc.abstractmethod\n    def _build_queues(self, c0, c1):\n        """"""\n    Method used to inject buffers between mask generating and use\n    (ie online vs offline). `c0` and `c1` represent the generated\n    masks and the method is expected to return a similar pair of\n    of tensors.\n    """"""\n\n\nclass OnlineTripleSource(BaseTripleSource):\n    """"""\n  This triple source will generate triples as part of the online phase\n  using a dedicated third-party `producer`.\n\n  There is no need to call `generate_triples` nor `initialize`.\n  """"""\n\n    def __init__(self, producer):\n        super().__init__(None, None, producer)\n\n    def cache(self, a, cache_updater):\n        with tf.device(self.producer.device_name):\n            updater, [a_cached] = wrap_in_variables(a)\n        return updater, a_cached\n\n    def initializer(self):\n        return tf.no_op()\n\n    def generate_triples(self, fetches):\n        return []\n\n    def _build_queues(self, c0, c1):\n        return c0, c1\n\n\nclass QueuedOnlineTripleSource(BaseTripleSource):\n    """"""\n  Similar to `OnlineTripleSource` but with in-memory buffering backed by\n  `tf.FIFOQueue`s.\n  """"""\n\n    def __init__(self, player0, player1, producer, capacity=10):\n        super().__init__(player0, player1, producer)\n        self.capacity = capacity\n        self.queues = list()\n        self.triggers = dict()\n\n    def cache(self, a, cache_updater):\n        with tf.device(self.producer.device_name):\n            offline_updater, [a_cached] = wrap_in_variables(a)\n        self.triggers[cache_updater] = offline_updater\n        return tf.no_op(), a_cached\n\n    def initializer(self):\n        return tf.no_op()\n\n    def generate_triples(self, fetches):\n        if isinstance(fetches, (list, tuple)) and len(fetches) > 1:\n            logger.warning(\n                ""Generating triples for a run involving more than ""\n                ""one fetch may introduce non-determinism that can ""\n                ""break the correspondence between the two phases ""\n                ""of the computation.""\n            )\n\n        unwrapped_fetches = unwrap_fetches(fetches)\n        reachable_operations = [\n            node\n            for node in reachable_nodes(unwrapped_fetches)\n            if isinstance(node, tf.Operation)\n        ]\n        reachable_triggers = [\n            self.triggers[op] for op in reachable_operations if op in self.triggers\n        ]\n        return reachable_triggers\n\n    def _build_triple_store(self, mask, player_id):\n        """"""\n    Adds a tf.FIFOQueue to store mask locally on player.\n    """"""\n\n        # TODO(Morten) taking `value` doesn\'t work for int100\n        raw_mask = mask.value\n        factory = mask.factory\n        dtype = mask.factory.native_type\n        shape = mask.shape\n\n        with tf.name_scope(""triple-store-{}"".format(player_id)):\n\n            q = tf.queue.FIFOQueue(\n                capacity=self.capacity, dtypes=[dtype], shapes=[shape],\n            )\n            e = q.enqueue(raw_mask)\n            d = q.dequeue()\n            d_wrapped = factory.tensor(d)\n\n        self.queues += [q]\n        self.triggers[d.op] = e\n        return d_wrapped\n\n    def _build_queues(self, c0, c1):\n\n        with tf.device(self.player0.device_name):\n            d0 = self._build_triple_store(c0, ""0"")\n\n        with tf.device(self.player1.device_name):\n            d1 = self._build_triple_store(c1, ""1"")\n\n        return d0, d1\n\n\n""""""\nclass PlaceholderTripleSource(BaseTripleSource):\n\n    # TODO(Morten) manually unwrap and re-wrap of values, should be hidden away\n\n    def __init__(self, player0, player1, producer):\n        super().__init__(player0, player1, producer)\n        self.placeholders = list()\n\n    def _build_queues(self, c0, c1):\n\n        with tf.device(self.player0.device_name):\n            r0 = tf.placeholder(\n                dtype=c0.factory.native_type,\n                shape=c0.shape,\n            )\n            d0 = c0.factory.tensor(r0)\n\n        with tf.device(self.player1.device_name):\n            r1 = tf.placeholder(\n                dtype=c1.factory.native_type,\n                shape=c1.shape,\n            )\n            d1 = c1.factory.tensor(r1)\n\n        self.placeholders += [r0, r1]\n        return d0, d1\n""""""  # pylint: disable=pointless-string-statement\n\n\n""""""\nclass DatasetTripleSource(BaseTripleSource):\n\n    # TODO(Morten) manually unwrap and re-wrap of values, should be hidden away\n\n    def __init__(\n        self,\n        player0,\n        player1,\n        producer,\n        capacity=10,\n        directory=""/tmp/triples/"",\n        support_online_running=False,\n    ):\n        super().__init__(player0, player1, producer)\n        self.capacity = capacity\n        self.dequeuers = list()\n        self.enqueuers = list()\n        self.initializers = list()\n        self.directory = directory\n        self.support_online_running = support_online_running\n        if support_online_running:\n            self.dequeue_from_file = tf.placeholder_with_default(True,\n                                                                 shape=[])\n\n    def _build_queues(self, c0, c1):\n\n        def dataset_from_file(filename, dtype, shape):\n            def parse(x):\n                res = tf.parse_tensor(x, out_type=dtype)\n                res = tf.reshape(res, shape)\n                return res\n            iterator = tf.data.TFRecordDataset(filename) \\\n                .map(parse) \\\n                .make_initializable_iterator()\n            return iterator.get_next(), iterator.initializer\n\n        def dataset_from_queue(queue, dtype, shape):\n            dummy = tf.data.Dataset.from_tensors(0).repeat(None)\n            iterator = (dummy.map(lambda _: queue.dequeue())\n                             .make_initializable_iterator())\n            return iterator.get_next(), iterator.initializer\n            # gen = lambda: queue.dequeue()\n            # dataset = tf.data.Dataset.from_generator(gen, [dtype], [shape])\n            # iterator = dataset.make_one_shot_iterator()\n            # return iterator.get_next(), iterator.initializer\n\n        def sanitize_filename(filename):\n            return filename.replace(\'/\', \'__\')\n\n        def build_triple_store(mask):\n\n            raw_mask = mask.value\n            factory = mask.factory\n            dtype = mask.factory.native_type\n            shape = mask.shape\n\n            with tf.name_scope(""triple-store""):\n\n                q = tf.queue.FIFOQueue(\n                    capacity=self.capacity,\n                    dtypes=[dtype],\n                    shapes=[shape],\n                )\n\n                e = q.enqueue(raw_mask)\n                f = os.path.join(self.directory, sanitize_filename(q.name))\n\n                if self.support_online_running:\n                    r, i = tf.cond(\n                        self.dequeue_from_file,\n                        true_fn=lambda: dataset_from_file(f, dtype, shape),\n                        false_fn=lambda: dataset_from_queue(q, dtype, shape),\n                    )\n                else:\n                    r, i = dataset_from_file(f, dtype, shape)\n                d = factory.tensor(r)\n\n            return f, q, e, d, i\n\n        with tf.device(self.player0.device_name):\n            f0, q0, e0, d0, i0 = build_triple_store(c0)\n\n        with tf.device(self.player1.device_name):\n            f1, q1, e1, d1, i1 = build_triple_store(c1)\n\n        self.dequeuers += [(f0, q0.dequeue()), (f1, q1.dequeue())]\n        self.enqueuers += [(e0, e1)]\n        self.initializers += [(i0, i1)]\n        return d0, d1\n\n    def initialize(self, sess, tag=None):\n        sess.run(self.initializers, tag=tag)\n\n    def generate_triples(self, sess, num=1, tag=None, save_to_file=True):\n        for _ in range(num):\n            sess.run(self.enqueuers, tag=tag)\n\n        if save_to_file:\n            self.save_triples_to_file(sess, num=num, tag=tag)\n\n    def save_triples_to_file(self, sess, num, tag=None):\n        if not os.path.exists(self.directory):\n            os.makedirs(self.directory)\n\n        for filename, dequeue in self.dequeuers:\n            with tf.io.TFRecordWriter(filename) as writer:\n                # size = sess.run(queue.size(), tag=tag)\n                for _ in range(num):\n                    serialized = tf.io.serialize_tensor(dequeue)\n                    triple = sess.run(serialized, tag=tag)\n                    writer.write(triple)\n""""""  # pylint: disable=pointless-string-statement\n'"
tf_encrypted/protocol/securenn/__init__.py,0,"b'""""""\nImplementation of SecureNN from Wagh et al.\n\nSee https://eprint.iacr.org/2018/442/\n""""""\nfrom __future__ import absolute_import\n\nfrom .securenn import SecureNN\n\n__all__ = [\n    ""SecureNN"",\n]\n'"
tf_encrypted/protocol/securenn/odd_tensor.py,30,"b'""""""Odd tensors abstraction. For internal use with SecureNN subprotocols.""""""\nfrom __future__ import absolute_import\n\nimport abc\nimport math\nfrom functools import partial\nfrom typing import Optional\nfrom typing import Tuple\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom ...operations import secure_random\nfrom ...tensor.factory import AbstractTensor\nfrom ...tensor.shared import binarize\n\n\ndef odd_factory(NATIVE_TYPE):  # pylint: disable=invalid-name\n    """"""\n  Produces a Factory for OddTensors with underlying tf.dtype NATIVE_TYPE.\n  """"""\n\n    assert NATIVE_TYPE in (tf.int32, tf.int64)\n\n    class Factory:\n        """"""\n    Represents a native integer data type. It is currently not considered for\n    general use, but only to support subprotocols of SecureNN.\n\n    One value of the native dtype is removed in order to obtain an odd modulus.\n    More concretely, this data type wraps either tf.int32 or tf.int64 but\n    removes -1, which is instead mapped to 0.\n    """"""\n\n        def tensor(self, value):\n            """"""\n      Wrap `value` in this data type, performing type conversion as needed.\n      Internal use should consider explicit construction as an optimization that\n      avoids redundant correction.\n      """"""\n\n            if isinstance(value, tf.Tensor):\n                if value.dtype is not NATIVE_TYPE:\n                    value = tf.cast(value, dtype=NATIVE_TYPE)\n                # no assumptions are made about the tensor here and hence we need to\n                # apply our mapping for invalid values\n                value = _map_minusone_to_zero(value, NATIVE_TYPE)\n                return OddDenseTensor(value)\n\n            raise TypeError(""Don\'t know how to handle {}"".format(type(value)))\n\n        def constant(self, value):\n            raise NotImplementedError()\n\n        def variable(self, initial_value):\n            raise NotImplementedError()\n\n        def placeholder(self, shape):\n            raise NotImplementedError()\n\n        @property\n        def modulus(self):\n\n            if NATIVE_TYPE is tf.int32:\n                return 2 ** 32 - 1\n\n            if NATIVE_TYPE is tf.int64:\n                return 2 ** 64 - 1\n\n            raise NotImplementedError(""Incorrect native type {}."".format(NATIVE_TYPE))\n\n        @property\n        def native_type(self):\n            return NATIVE_TYPE\n\n        def sample_uniform(\n            self, shape, minval: Optional[int] = None, maxval: Optional[int] = None\n        ):\n            """"""Sample a tensor from a uniform distribution.""""""\n            assert minval is None\n            assert maxval is None\n\n            if secure_random.supports_seeded_randomness():\n                seed = secure_random.secure_seed()\n                return OddUniformTensor(shape=shape, seed=seed)\n\n            if secure_random.supports_secure_randomness():\n                sampler = secure_random.random_uniform\n            else:\n                sampler = tf.random_uniform\n\n            value = _construct_value_from_sampler(sampler=sampler, shape=shape)\n            return OddDenseTensor(value)\n\n        def sample_bounded(self, shape, bitlength: int):\n            raise NotImplementedError()\n\n        def stack(self, xs: list, axis: int = 0):\n            raise NotImplementedError()\n\n        def concat(self, xs: list, axis: int):\n            raise NotImplementedError()\n\n    master_factory = Factory()\n\n    class OddTensor(AbstractTensor):\n        """"""\n    Base class for the concrete odd tensors types.\n\n    Implements basic functionality needed by SecureNN subprotocols from a few\n    abstract properties implemented by concrete types below.\n    """"""\n\n        @property\n        def factory(self):\n            return master_factory\n\n        @property\n        @abc.abstractproperty\n        def value(self) -> tf.Tensor:\n            pass\n\n        @property\n        @abc.abstractproperty\n        def shape(self):\n            pass\n\n        def identity(self):\n            value = tf.identity(self.value)\n            return OddDenseTensor(value)\n\n        def __repr__(self) -> str:\n            return ""{}(shape={}, NATIVE_TYPE={})"".format(\n                type(self), self.shape, NATIVE_TYPE,\n            )\n\n        def __getitem__(self, slc):\n            return OddDenseTensor(self.value[slc])\n\n        def __add__(self, other):\n            return self.add(other)\n\n        def __sub__(self, other):\n            return self.sub(other)\n\n        def add(self, other):\n            """"""Add other to this tensor.""""""\n            x, y = _lift(self, other)\n            bitlength = math.ceil(math.log2(master_factory.modulus))\n\n            with tf.name_scope(""add""):\n\n                # the below avoids redundant seed expansion; can be removed once\n                # we have a (per-device) caching mechanism in place\n                x_value = x.value\n                y_value = y.value\n\n                z = x_value + y_value\n\n                with tf.name_scope(""correct_wrap""):\n\n                    # we want to compute whether we wrapped around, ie\n                    # `pos(x) + pos(y) >= m - 1`, for correction purposes which,\n                    # since `m - 1 == 1` for signed integers, can be rewritten as:\n                    #  -> `pos(x) >= m - 1 - pos(y)`\n                    #  -> `m - 1 - pos(y) - 1 < pos(x)`\n                    #  -> `-1 - pos(y) - 1 < pos(x)`\n                    #  -> `-2 - pos(y) < pos(x)`\n                    wrapped_around = _lessthan_as_unsigned(\n                        -2 - y_value, x_value, bitlength\n                    )\n                    z += wrapped_around\n\n            return OddDenseTensor(z)\n\n        def sub(self, other):\n            """"""Subtract other from this tensor.""""""\n            x, y = _lift(self, other)\n            bitlength = math.ceil(math.log2(master_factory.modulus))\n\n            with tf.name_scope(""sub""):\n\n                # the below avoids redundant seed expansion; can be removed once\n                # we have a (per-device) caching mechanism in place\n                x_value = x.value\n                y_value = y.value\n\n                z = x_value - y_value\n\n                with tf.name_scope(""correct-wrap""):\n\n                    # we want to compute whether we wrapped around, ie\n                    # `pos(x) - pos(y) < 0`, for correction purposes which can be\n                    # rewritten as -> `pos(x) < pos(y)`\n                    wrapped_around = _lessthan_as_unsigned(x_value, y_value, bitlength)\n                    z -= wrapped_around\n\n            return OddDenseTensor(z)\n\n        def bits(self, factory=None):\n            if factory is None:\n                return OddDenseTensor(binarize(self.value))\n            return factory.tensor(binarize(self.value))\n\n        def cast(self, factory):\n            if factory is master_factory:\n                return self\n            return factory.tensor(self.value)\n\n    class OddDenseTensor(OddTensor):\n        """"""\n    Represents a tensor with explicit values, as opposed to OddUniformTensor\n    with implicit values.\n\n    Internal use only and assume that invalid values have already been mapped.\n    """"""\n\n        def __init__(self, value):\n            assert isinstance(value, tf.Tensor)\n            self._value = value\n\n        @property\n        def value(self) -> tf.Tensor:\n            return self._value\n\n        @property\n        def shape(self):\n            return self._value.shape\n\n        @property\n        def support(self):\n            return [self._value]\n\n    class OddUniformTensor(OddTensor):\n        """"""\n    Represents a tensor with uniform values defined implicitly through a seed.\n\n    Internal use only.\n    """"""\n\n        def __init__(self, shape, seed):\n            self._seed = seed\n            self._shape = shape\n\n        @property\n        def shape(self):\n            return self._shape\n\n        @property\n        def value(self) -> tf.Tensor:\n            # TODO(Morten) result should be stored in a (per-device) cache\n            with tf.name_scope(""expand-seed""):\n                sampler = partial(secure_random.seeded_random_uniform, seed=self._seed)\n                value = _construct_value_from_sampler(\n                    sampler=sampler, shape=self._shape\n                )\n                return value\n\n        @property\n        def support(self):\n            return [self._seed]\n\n    def _lift(x, y) -> Tuple[OddTensor, OddTensor]:\n        """"""\n    Attempts to lift x and y to compatible OddTensors for further processing.\n    """"""\n\n        if isinstance(x, OddTensor) and isinstance(y, OddTensor):\n            assert x.factory == y.factory, ""Incompatible types: {} and {}"".format(\n                x.factory, y.factory\n            )\n            return x, y\n\n        if isinstance(x, OddTensor):\n\n            if isinstance(y, int):\n                return x, x.factory.tensor(np.array([y]))\n\n        if isinstance(y, OddTensor):\n\n            if isinstance(x, int):\n                return y.factory.tensor(np.array([x])), y\n\n        raise TypeError(""Don\'t know how to lift {} {}"".format(type(x), type(y)))\n\n    def _construct_value_from_sampler(sampler, shape):\n        """"""Sample from sampler and correct for the modified dtype.""""""\n        # to get uniform distribution over [min, max] without -1 we sample\n        # [min+1, max] and shift negative values down by one\n        unshifted_value = sampler(\n            shape=shape,\n            dtype=NATIVE_TYPE,\n            minval=NATIVE_TYPE.min + 1,\n            maxval=NATIVE_TYPE.max,\n        )\n        shifted_values = unshifted_value + tf.ones(\n            shape=unshifted_value.shape, dtype=unshifted_value.dtype\n        )\n        value = tf.where(unshifted_value < 0, shifted_values, unshifted_value)\n        return value\n\n    def _lessthan_as_unsigned(x, y, bitlength):\n        """"""\n    Performs comparison `x < y` on signed integers *as if* they were unsigned,\n    e.g. `1 < -1`. Taken from Section 2-12, page 23, of\n    [Hacker\'s Delight](https://www.hackersdelight.org/).\n    """"""\n        with tf.name_scope(""unsigned-compare""):\n            not_x = tf.bitwise.invert(x)\n            lhs = tf.bitwise.bitwise_and(not_x, y)\n            rhs = tf.bitwise.bitwise_and(tf.bitwise.bitwise_or(not_x, y), x - y)\n            z = tf.bitwise.right_shift(tf.bitwise.bitwise_or(lhs, rhs), bitlength - 1)\n            # turn 0/-1 into 0/1 before returning\n            return tf.bitwise.bitwise_and(z, tf.ones(shape=z.shape, dtype=z.dtype))\n\n    def _map_minusone_to_zero(value, native_type):\n        """"""Maps all -1 values to zero.""""""\n        zeros = tf.zeros(shape=value.shape, dtype=native_type)\n        return tf.where(tf.equal(value, -1), zeros, value)\n\n    return master_factory\n\n\noddint32_factory = odd_factory(tf.int32)\noddint64_factory = odd_factory(tf.int64)\n'"
tf_encrypted/protocol/securenn/odd_tensor_test.py,9,"b'# pylint: disable=missing-docstring\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf_encrypted.protocol.securenn.odd_tensor import oddint64_factory\n\n\nclass TestOddImplicitTensor(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_tensor(self) -> None:\n        # regular, overflow, underflow\n        x = oddint64_factory.tensor(tf.constant([-2, -1, 0, 1], dtype=tf.int64))\n        expected = np.array([-2, 0, 0, 1])\n\n        with tf.Session() as sess:\n            actual = sess.run(x.value)\n\n        np.testing.assert_array_almost_equal(actual, expected, decimal=3)\n\n    def test_add(self) -> None:\n\n        # regular, overflow, underflow\n        x = oddint64_factory.tensor(tf.constant([2, -2], dtype=tf.int64))\n        y = oddint64_factory.tensor(tf.constant([3, 3], dtype=tf.int64))\n\n        z = x + y\n\n        expected = np.array([5, 2])\n\n        with tf.Session() as sess:\n            actual = sess.run(z.value)\n\n        np.testing.assert_array_almost_equal(actual, expected, decimal=3)\n\n    def test_sub(self) -> None:\n\n        # regular, overflow, underflow\n        x = oddint64_factory.tensor(tf.constant([2, -2], dtype=tf.int64))\n        y = oddint64_factory.tensor(tf.constant([3, 3], dtype=tf.int64))\n\n        z = x - y\n\n        expected = np.array([-2, -5])\n\n        with tf.Session() as sess:\n            actual = sess.run(z.value)\n\n        np.testing.assert_array_almost_equal(actual, expected, decimal=3)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_encrypted/protocol/securenn/securenn.py,50,"b'# pylint: disable=protected-access\n""""""\nThe SecureNN protocol implementation.\n\nSee https://eprint.iacr.org/2018/442/ for more.\n""""""\nfrom __future__ import absolute_import\n\nimport math\nimport random\nimport sys\nfrom typing import Optional\nfrom typing import Tuple\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf_encrypted.config import get_config\nfrom tf_encrypted.player import Player\nfrom tf_encrypted.protocol import memoize\nfrom tf_encrypted.protocol.pond import Pond\nfrom tf_encrypted.protocol.pond import PondMaskedTensor\nfrom tf_encrypted.protocol.pond import PondPrivateTensor\nfrom tf_encrypted.protocol.pond import PondPublicTensor\nfrom tf_encrypted.protocol.pond import PondTensor\nfrom tf_encrypted.protocol.pond import _type\nfrom tf_encrypted.protocol.protocol import nodes\nfrom tf_encrypted.protocol.securenn.odd_tensor import oddint64_factory\nfrom tf_encrypted.tensor import int64factory\nfrom tf_encrypted.tensor import native_factory\nfrom tf_encrypted.tensor.factory import AbstractFactory\nfrom tf_encrypted.tensor.factory import AbstractTensor\n\n_thismodule = sys.modules[__name__]\n\n\nclass SecureNN(Pond):\n    """"""\n  SecureNN(server_0, server_1, server_2, prime_factory, odd_factory, **kwargs)\n\n  Implementation of SecureNN from\n  `Wagh et al <https://eprint.iacr.org/2018/442/>`_.\n  """"""\n\n    def __init__(\n        self,\n        server_0: Optional[Player] = None,\n        server_1: Optional[Player] = None,\n        server_2: Optional[Player] = None,\n        tensor_factory: Optional[AbstractFactory] = None,\n        prime_factory: Optional[AbstractFactory] = None,\n        odd_factory: Optional[AbstractFactory] = None,\n        **kwargs,\n    ) -> None:\n        server_0 = server_0 or get_config().get_player(""server0"")\n        server_1 = server_1 or get_config().get_player(""server1"")\n        server_2 = (\n            server_2\n            or get_config().get_player(""server2"")\n            or get_config().get_player(""crypto-producer"")\n        )\n\n        assert server_0 is not None\n        assert server_1 is not None\n        assert server_2 is not None\n\n        super(SecureNN, self).__init__(\n            server_0=server_0,\n            server_1=server_1,\n            triple_source=server_2,\n            tensor_factory=tensor_factory,\n            **kwargs,\n        )\n        self.server_2 = server_2\n\n        if odd_factory is None:\n            if self.tensor_factory is int64factory:\n                odd_factory = oddint64_factory\n            else:\n                odd_factory = self.tensor_factory\n\n        if prime_factory is None:\n            prime = 107\n            assert prime > math.ceil(math.log2(self.tensor_factory.modulus))\n            prime_factory = native_factory(self.tensor_factory.native_type, prime)\n\n        self.prime_factory = prime_factory\n        self.odd_factory = odd_factory\n        assert self.prime_factory.native_type == self.tensor_factory.native_type\n        assert self.odd_factory.native_type == self.tensor_factory.native_type\n\n    @memoize\n    def bitwise_not(self, x: PondTensor) -> PondTensor:\n        """"""\n    bitwise_not(x) -> PondTensor\n\n    Computes the bitwise `NOT` of the input, i.e. :math:`f(x) = 1 - x`.\n\n    :param PondTensor x: Input tensor.\n    """"""\n        assert not x.is_scaled, ""Input is not supposed to be scaled""\n        with tf.name_scope(""bitwise_not""):\n            return self.sub(1, x)\n\n    @memoize\n    def bitwise_and(self, x: ""PondTensor"", y: ""PondTensor"") -> ""PondTensor"":\n        """"""\n    bitwise_and(x, y) -> PondTensor\n\n    Computes the bitwise `AND` of the given inputs, :math:`f(x,y) = xy`.\n\n    :param PondTensor x: Input tensor.\n    :param PondTensor y: Input tensor.\n    """"""\n        assert not x.is_scaled, ""Input is not supposed to be scaled""\n        assert not y.is_scaled, ""Input is not supposed to be scaled""\n        with tf.name_scope(""bitwise_and""):\n            return x * y\n\n    @memoize\n    def bitwise_or(self, x: ""PondTensor"", y: ""PondTensor"") -> ""PondTensor"":\n        """"""\n    bitwise_or(x, y) -> PondTensor\n\n    Computes the bitwise `OR` of the given inputs, :math:`f(x,y) = x + y - xy`.\n\n    :param PondTensor x: Input tensor.\n    :param PondTensor y: Input tensor.\n    """"""\n        assert not x.is_scaled, ""Input is not supposed to be scaled""\n        assert not y.is_scaled, ""Input is not supposed to be scaled""\n        with tf.name_scope(""bitwise_or""):\n            return x + y - self.bitwise_and(x, y)\n\n    @memoize\n    def bitwise_xor(self, x: ""PondTensor"", y: ""PondTensor"") -> ""PondTensor"":\n        """"""\n    bitwise_xor(x, y) -> PondTensor\n\n    Compute the bitwise `XOR` of the given inputs, :math:`f(x,y) = x + y - 2xy`\n\n    :param PondTensor x: Input tensor.\n    :param PondTensor y: Input tensor.\n    """"""\n        assert not x.is_scaled, ""Input is not supposed to be scaled""\n        assert not y.is_scaled, ""Input is not supposed to be scaled""\n        with tf.name_scope(""bitwise_xor""):\n            return x + y - self.bitwise_and(x, y) * 2\n\n    @memoize\n    def msb(self, x: ""PondTensor"") -> ""PondTensor"":\n        """"""\n    msb(x) -> PondTensor\n\n    Computes the most significant bit of the provided tensor.\n\n    :param PondTensor x: The tensor to take the most significant bit of\n    """"""\n        with tf.name_scope(""msb""):\n            # when the modulus is odd msb reduces to lsb via x -> 2*x\n            x = self.cast_backing(x, self.odd_factory)\n            return self.lsb(x + x)\n\n    @memoize\n    def lsb(self, x: PondTensor) -> PondTensor:\n        """"""\n    lsb(x) -> PondTensor\n\n    Computes the least significant bit of the provided tensor.\n\n    :param PondTensor x: The tensor to take the least significant bit of.\n    """"""\n        return self.dispatch(""lsb"", x, container=_thismodule)\n\n    @memoize\n    def bits(\n        self, x: PondPublicTensor, factory: Optional[AbstractFactory] = None\n    ) -> ""PondPublicTensor"":\n        """"""\n    bits(x, factory) -> PondPublicTensor\n\n    Convert a fixed-point precision tensor into its bitwise representation.\n\n    :param PondPublicTensor x: A fixed-point tensor to extract into a bitwise\n      representation.\n    """"""\n        return self.dispatch(""bits"", x, container=_thismodule, factory=factory)\n\n    @memoize\n    def is_negative(self, x: PondTensor) -> PondTensor:\n        """"""\n    Returns :math:`x < 0`.\n\n    .. code-block:: python\n\n        >>> negative([-1, 0, 1])\n        [1, 0, 0]\n\n    :param PondTensor x: The tensor to check.\n    """"""\n        with tf.name_scope(""is_negative""):\n            # NOTE MSB is 1 iff xi < 0\n            return self.msb(x)\n\n    @memoize\n    def non_negative(self, x: PondTensor) -> PondTensor:\n        """"""\n    non_negative(x) -> PondTensor\n\n    Returns :math:`x >= 0`.\n\n    .. code-block:: python\n\n        >>> non_negative([-1, 0, 1])\n        [0, 1, 1]\n\n    Note this is the derivative of the ReLU function.\n\n    :param PondTensor x: The tensor to check.\n    """"""\n        with tf.name_scope(""non_negative""):\n            return self.bitwise_not(self.msb(x))\n\n    @memoize\n    def less(self, x: PondTensor, y: PondTensor) -> PondTensor:\n        """"""\n    less(x, y) -> PondTensor\n\n    Returns :math:`x < y`.\n\n    .. code-block:: python\n\n        >>> less([1,2,3], [0,1,5])\n        [0, 0, 1]\n\n    :param PondTensor x: The tensor to check.\n    :param PondTensor y: The tensor to check against.\n    """"""\n        with tf.name_scope(""less""):\n            return self.is_negative(x - y)\n\n    @memoize\n    def less_equal(self, x: PondTensor, y: PondTensor) -> PondTensor:\n        """"""\n    less_equal(x, y) -> PondTensor\n\n    Returns :math:`x <= y`.\n\n    .. code-block:: python\n\n        >>> less_equal([1,2,3], [0,1,3])\n        [0, 0, 1]\n\n    :param PondTensor x: The tensor to check.\n    :param PondTensor y: The tensor to check against.\n    """"""\n        with tf.name_scope(""less_equal""):\n            return self.bitwise_not(self.greater(x, y))\n\n    @memoize\n    def greater(self, x: PondTensor, y: PondTensor) -> PondTensor:\n        """"""\n    greater(x, y) -> PondTensor\n\n    Returns :math:`x > y`.\n\n    .. code-block:: python\n\n        >>> greater([1,2,3], [0,1,5])\n        [1, 1, 0]\n\n    :param PondTensor x: The tensor to check.\n    :param PondTensor y: The tensor to check against.\n    """"""\n        with tf.name_scope(""greater""):\n            return self.is_negative(y - x)\n\n    @memoize\n    def greater_equal(self, x: PondTensor, y: PondTensor) -> PondTensor:\n        """"""\n    greater_equal(x, y) -> PondTensor\n\n    Returns :math:`x >= y`.\n\n    .. code-block:: python\n\n        >>> greater_equal([1,2,3], [0,1,3])\n        [1, 1, 1]\n\n    :param PondTensor x: The tensor to check.\n    :param PondTensor y: The tensor to check against.\n    """"""\n        with tf.name_scope(""greater_equal""):\n            return self.bitwise_not(self.less(x, y))\n\n    @memoize\n    def select(self, choice_bit, x, y):\n        """"""\n    select(choice_bit, x, y) -> PondTensor\n\n    The `select` protocol from Wagh et al.  Secretly selects and returns\n    elements from two candidate tensors.\n\n    .. code-block:: python\n\n        >>> option_x = [10, 20, 30, 40]\n        >>> option_y = [1, 2, 3, 4]\n        >>> select(choice_bit=1, x=option_x, y=option_y)\n        [1, 2, 3, 4]\n        >>> select(choice_bit=[0,1,0,1], x=option_x, y=option_y)\n        [10, 2, 30, 4]\n\n    `NOTE:` Inputs to this function in real use will not look like above.\n    In practice these will be secret shares.\n\n    :param PondTensor choice_bit: The bits representing which tensor to choose.\n      If `choice_bit = 0` then choose elements from `x`, otherwise choose\n      from `y`.\n    :param PondTensor x: Candidate tensor 0.\n    :param PondTensor y: Candidate tensor 1.\n    """"""  # noqa:E501\n\n        # TODO[Morten] optimize select when choice_bit is a public tensor\n\n        # TODO[Morten]\n        # these assertions should ideally be enabled but requires lifting to be\n        # applied to the inputs first; make sure that\'s fixed during refactoring\n        #\n        # assert x.backing_dtype == y.backing_dtype\n        # assert x.is_scaled == y.is_scaled\n        # assert not choice_bit.is_scaled\n\n        with tf.name_scope(""select""):\n            return (y - x) * choice_bit + x\n\n    @memoize\n    def equal_zero(self, x, dtype: Optional[AbstractFactory] = None):\n        """"""\n    equal_zero(x, dtype) -> PondTensor\n\n    Evaluates the Boolean expression :math:`x = 0`.\n\n    .. code-block:: python\n\n        >>> equal_zero([1,0,1])\n        [0, 1, 0]\n\n    :param PondTensor x: The tensor to evaluate.\n    :param AbstractFactory dtype: An optional tensor factory.\n      Defaults to dtype of `x`.\n    """"""\n        return self.dispatch(""equal_zero"", x, container=_thismodule, dtype=dtype)\n\n    @memoize\n    def relu(self, x, **kwargs):\n        """"""\n    relu(x) -> PondTensor\n\n    Returns the exact `ReLU` by computing `ReLU(x) = x * nonnegative(x)`.\n\n    .. code-block:: python\n\n        >>> relu([-12, -3, 1, 3, 3])\n        [0, 0, 1, 3, 3]\n\n    :param PondTensor x: Input tensor.\n    """"""\n\n        def actual_relu(x, name_scope):\n            with tf.name_scope(name_scope):\n                drelu = self.non_negative(x)\n                return drelu * x\n\n        shape = x.shape.as_list()\n\n        total_size = np.prod(shape)\n\n        max_size = kwargs.get(""max_size"", 2500000)\n        if not max_size or total_size <= max_size:\n            return actual_relu(x, ""relu"")\n\n        # tensor is too big and might raise OOM; split it along the last\n        # dimension and process subtensors individually, using sizes as\n        # close as possible to `maxsize` (but possibly larger)\n\n        # compute how large the last dimension can be while being at least 1\n        max_last_dimension = max(max_size // np.prod(shape[:-1]), 1)\n        # compute split vector\n        last_dimension = shape[-1]\n        number_of_max_last_dimension = last_dimension // max_last_dimension\n        leftover = last_dimension % max_last_dimension\n        split_vector = [max_last_dimension] * number_of_max_last_dimension\n        split_vector += [leftover] if leftover > 0 else []\n        assert np.sum(split_vector) == last_dimension\n\n        with tf.name_scope(""relu""):\n            xs = self.split(x, split_vector, axis=-1)\n\n            for i, _ in enumerate(xs):\n                xs[i] = actual_relu(xs[i], ""subrelu"")\n\n            return self.concat(xs, axis=-1)\n\n    def maxpool2d(self, x, pool_size, strides, padding):\n        """"""\n    maxpool2d(x, pool_size, strides, padding) -> PondTensor\n\n    Performs a `MaxPooling2d` operation on `x`.\n\n    :param PondTensor x: Input tensor.\n    :param List[int] pool_size: The size of the pool.\n    :param List[int] strides: A list describing how to stride over the\n      convolution.\n    :param str padding: Which type of padding to use (""SAME"" or ""VALID"").\n    """"""\n        node_key = (""maxpool2d"", x, tuple(pool_size), tuple(strides), padding)\n        z = nodes.get(node_key, None)\n\n        if z is not None:\n            return z\n\n        dispatch = {\n            PondPublicTensor: _maxpool2d_public,\n            PondPrivateTensor: _maxpool2d_private,\n            PondMaskedTensor: _maxpool2d_masked,\n        }\n\n        func = dispatch.get(_type(x), None)\n        if func is None:\n            raise TypeError(""Don\'t know how to avgpool2d {}"".format(type(x)))\n\n        z = func(self, x, pool_size, strides, padding)\n        nodes[node_key] = z\n\n        return z\n\n    @memoize\n    def maximum(self, x, y):\n        """"""\n    maximum(x, y) -> PondTensor\n\n    Computes :math:`max(x,y)`.\n\n    Returns the greater value of each tensor per index.\n\n    .. code-block:: python\n\n        >>> maximum([10, 20, 30], [11, 19, 31])\n        [11, 20, 31]\n\n    :param PondTensor x: Input tensor.\n    :param PondTensor y: Input tensor.\n    """"""\n        with tf.name_scope(""maximum""):\n            indices_of_maximum = self.greater(x, y)\n            return self.select(indices_of_maximum, y, x)\n\n    @memoize\n    def reduce_max(self, x, axis=0):\n        """"""\n    reduce_max(x, axis) -> PondTensor\n\n    Find the max value along an axis.\n\n    .. code-block:: python\n\n        >>> reduce_max([[10, 20, 30], [11, 13, 12], [15, 16, 17]], axis=0)\n        [[30], [13], [17]]\n\n    :See: tf.reduce_max\n    :param PondTensor x: Input tensor.\n    :param int axis: The tensor axis to reduce along.\n    :rtype: PondTensor\n    :returns: A new tensor with the specified axis reduced to the max value in\n      that axis.\n    """"""\n        with tf.name_scope(""reduce_max""):\n\n            def build_comparison_tree(ts):\n                if len(ts) == 1:\n                    return ts[0]\n                halfway = len(ts) // 2\n                ts_left, ts_right = ts[:halfway], ts[halfway:]\n                maximum_left = build_comparison_tree(ts_left)\n                maximum_right = build_comparison_tree(ts_right)\n                return self.maximum(maximum_left, maximum_right)\n\n            tensors = self.split(x, int(x.shape[axis]), axis=axis)\n            maximum = build_comparison_tree(tensors)\n            return self.squeeze(maximum, axis=(axis,))\n\n    @memoize\n    def argmax(self, x, axis=0):\n        """"""\n    argmax(x, axis) -> PondTensor\n\n    Find the index of the max value along an axis.\n\n    .. code-block:: python\n\n        >>> argmax([[10, 20, 30], [11, 13, 12], [15, 16, 17]], axis=0)\n        [[2], [1], [2]]\n\n    :See: tf.argmax\n    :param PondTensor x: Input tensor.\n    :param int axis: The tensor axis to reduce along.\n    :rtype: PondTensor\n    :returns: A new tensor with the indices of the max values along specified\n      axis.\n    """"""\n        with tf.name_scope(""argmax""):\n\n            def build_comparison_tree(tensors, indices):\n                assert len(tensors) == len(indices)\n                if len(indices) == 1:\n                    return tensors[0], indices[0]\n\n                halfway = len(tensors) // 2\n                tensors_left, tensors_right = tensors[:halfway], tensors[halfway:]\n                indices_left, indices_right = indices[:halfway], indices[halfway:]\n\n                maximum_left, argmax_left = build_comparison_tree(\n                    tensors_left, indices_left\n                )\n                maximum_right, argmax_right = build_comparison_tree(\n                    tensors_right, indices_right\n                )\n\n                # compute binary tensor indicating which side is greater\n                greater = self.greater(maximum_left, maximum_right)\n\n                # use above binary tensor to select maximum and argmax\n                maximum = self.select(greater, maximum_right, maximum_left)\n                argmax = self.select(greater, argmax_right, argmax_left)\n\n                return maximum, argmax\n\n            tensors = self.split(x, int(x.shape[axis]), axis=axis)\n            indices = [\n                self.define_constant(np.array([i])) for i, _ in enumerate(tensors)\n            ]\n\n            with tf.name_scope(""comparison-tree""):\n                maximum, argmax = build_comparison_tree(tensors, indices)\n\n            maximum = self.squeeze(maximum, axis=(axis,))\n            argmax = self.squeeze(argmax, axis=(axis,))\n            return argmax\n\n    @memoize\n    def cast_backing(self, x, backing_dtype):\n        return self.dispatch(""cast_backing"", x, backing_dtype, container=_thismodule)\n\n\ndef _bits_public(prot, x: PondPublicTensor, factory=None) -> PondPublicTensor:\n    """"""Converts a public tensor to its binary tensor representation.""""""\n\n    factory = factory or prot.tensor_factory\n\n    with tf.name_scope(""bits""):\n\n        x_on_0, x_on_1 = x.unwrapped\n\n        with tf.device(prot.server_0.device_name):\n            bits_on_0 = x_on_0.bits(factory)\n\n        with tf.device(prot.server_1.device_name):\n            bits_on_1 = x_on_1.bits(factory)\n\n        return PondPublicTensor(prot, bits_on_0, bits_on_1, False)\n\n\ndef _lsb_public(prot, x: PondPublicTensor):\n    # TODO[Morten]\n    # we could call through and ask the underlying dtype for its lsb instead as\n    # there might be more efficient ways of getting it for some types (ie without\n    # getting all bits)\n    x_bits = prot.bits(x)\n    x_lsb = x_bits[..., 0]\n    return x_lsb\n\n\ndef _lsb_private(prot, x: PondPrivateTensor):\n    """"""\n  Logic for finding the least significant bit of a private tensor\n  in binary representation.\n  """"""\n\n    # TODO[Morten] in the refactor these could be type parameters\n    odd_dtype = x.backing_dtype\n    out_dtype = prot.tensor_factory\n    prime_dtype = prot.prime_factory\n\n    assert odd_dtype.modulus % 2 == 1\n\n    # needed for security because of `r` masking\n    assert x.backing_dtype.native_type == odd_dtype.native_type\n\n    with tf.name_scope(""lsb""):\n\n        with tf.name_scope(""blind""):\n\n            # ask server2 to generate r mask and its bits\n            with tf.device(prot.server_2.device_name):\n                r0 = odd_dtype.sample_uniform(x.shape)\n                r1 = odd_dtype.sample_uniform(x.shape)\n                r = PondPrivateTensor(prot, r0, r1, False)\n\n                r_raw = r0 + r1\n                rbits_raw = r_raw.bits(factory=prime_dtype)\n                rbits = prot._share_and_wrap(rbits_raw, False)\n\n                # TODO[Morten] once .bits() is cached then call .lsb() here instead\n                rlsb_raw = rbits_raw[..., 0].cast(out_dtype)\n                rlsb = prot._share_and_wrap(rlsb_raw, False)\n\n            # blind and reveal\n            c = (x + r).reveal()\n            c = prot.cast_backing(c, out_dtype)\n            c.is_scaled = False\n\n        with tf.name_scope(""compare""):\n\n            # ask either server0 and server1 to generate beta (distributing load)\n            server = random.choice([prot.server_0, prot.server_1])\n            with tf.device(server.device_name):\n                beta_raw = prime_dtype.sample_bits(x.shape)\n                beta = PondPublicTensor(prot, beta_raw, beta_raw, is_scaled=False)\n\n            greater_xor_beta = _private_compare(prot, rbits, c, beta)\n            clsb = prot.lsb(c)\n\n        with tf.name_scope(""unblind""):\n            gamma = prot.bitwise_xor(\n                greater_xor_beta, prot.cast_backing(beta, out_dtype)\n            )\n            delta = prot.bitwise_xor(rlsb, clsb)\n            alpha = prot.bitwise_xor(gamma, delta)\n\n        assert alpha.backing_dtype is out_dtype\n        return alpha\n\n\ndef _lsb_masked(prot, x: PondMaskedTensor):\n    return prot.lsb(x.unmasked)\n\n\ndef _private_compare(\n    prot, x_bits: PondPrivateTensor, r: PondPublicTensor, beta: PondPublicTensor,\n):\n    """"""Logic for private comparison.""""""\n    # TODO[Morten] no need to check this (should be free)\n    assert x_bits.backing_dtype == prot.prime_factory\n    assert r.backing_dtype.native_type == prot.tensor_factory.native_type\n\n    out_shape = r.shape\n    out_dtype = r.backing_dtype\n    prime_dtype = x_bits.backing_dtype\n    bit_length = x_bits.shape[-1]\n\n    assert r.shape == out_shape\n    assert r.backing_dtype.native_type == out_dtype.native_type\n    assert not r.is_scaled\n\n    assert x_bits.shape[:-1] == out_shape\n    assert x_bits.backing_dtype == prime_dtype\n    assert not x_bits.is_scaled\n\n    assert beta.shape == out_shape\n    assert beta.backing_dtype == prime_dtype\n    assert not beta.is_scaled\n\n    with tf.name_scope(""private_compare""):\n\n        with tf.name_scope(""bit_comparisons""):\n\n            # use either r or t = r + 1 according to beta\n            s = prot.select(prot.cast_backing(beta, out_dtype), r, r + 1)\n            s_bits = prot.bits(s, factory=prime_dtype)\n            assert s_bits.shape[-1] == bit_length\n\n            # compute w_sum\n            w_bits = prot.bitwise_xor(x_bits, s_bits)\n            w_sum = prot.cumsum(w_bits, axis=-1, reverse=True, exclusive=True)\n            assert w_sum.backing_dtype == prime_dtype\n\n            # compute c, ignoring edge cases at first\n            sign = prot.select(beta, 1, -1)\n            sign = prot.expand_dims(sign, axis=-1)\n            c_except_edge_case = (s_bits - x_bits) * sign + 1 + w_sum\n\n            assert c_except_edge_case.backing_dtype == prime_dtype\n\n        with tf.name_scope(""edge_cases""):\n\n            # adjust for edge cases, i.e. where beta is 1 and s is zero\n            # (meaning r was -1)\n\n            # identify edge cases\n            edge_cases = prot.bitwise_and(beta, prot.equal_zero(s, prime_dtype))\n            edge_cases = prot.expand_dims(edge_cases, axis=-1)\n\n            # tensor for edge cases: one zero and the rest ones\n            c_edge_vals = [0] + [1] * (bit_length - 1)\n            c_const = tf.constant(\n                c_edge_vals, dtype=prime_dtype.native_type, shape=(1, bit_length)\n            )\n            c_edge_case_raw = prime_dtype.tensor(c_const)\n            c_edge_case = prot._share_and_wrap(c_edge_case_raw, False)\n\n            c = prot.select(edge_cases, c_except_edge_case, c_edge_case)\n            assert c.backing_dtype == prime_dtype\n\n        with tf.name_scope(""zero_search""):\n\n            # generate multiplicative mask to hide non-zero values\n            with tf.device(prot.server_0.device_name):\n                mask_raw = prime_dtype.sample_uniform(c.shape, minval=1)\n                mask = PondPublicTensor(prot, mask_raw, mask_raw, False)\n\n            # mask non-zero values; this is safe when we\'re in a prime dtype\n            # (since it\'s a field)\n            c_masked = c * mask\n            assert c_masked.backing_dtype == prime_dtype\n\n            # TODO[Morten] permute\n\n            # reconstruct masked values on server 2 to find entries with zeros\n            with tf.device(prot.server_2.device_name):\n                d = prot._reconstruct(*c_masked.unwrapped)\n                # find all zero entries\n                zeros = d.equal_zero(out_dtype)\n                # for each bit sequence, determine whether it has one or no zero in it\n                rows_with_zeros = zeros.reduce_sum(axis=-1, keepdims=False)\n                # reshare result\n                result = prot._share_and_wrap(rows_with_zeros, False)\n\n        assert result.backing_dtype.native_type == out_dtype.native_type\n        return result\n\n\ndef _equal_zero_public(prot, x: PondPublicTensor, dtype=None):\n    """"""Check if a public tensor\'s values equal zero.""""""\n\n    with tf.name_scope(""equal_zero""):\n\n        x_on_0, x_on_1 = x.unwrapped\n\n        with tf.device(prot.server_0.device_name):\n            equal_zero_on_0 = x_on_0.equal_zero(dtype)\n\n        with tf.device(prot.server_1.device_name):\n            equal_zero_on_1 = x_on_1.equal_zero(dtype)\n\n        return PondPublicTensor(prot, equal_zero_on_0, equal_zero_on_1, False)\n\n\n#\n# max pooling helpers\n#\n\n\ndef _im2col(\n    prot: Pond,\n    x: PondTensor,\n    pool_size: Tuple[int, int],\n    strides: Tuple[int, int],\n    padding: str,\n) -> Tuple[AbstractTensor, AbstractTensor]:\n    """"""Compute im2col on a tensor.""""""\n\n    x_on_0, x_on_1 = x.unwrapped\n    batch, channels, height, width = x.shape\n\n    if padding == ""SAME"":\n        out_height = math.ceil(int(height) / strides[0])\n        out_width = math.ceil(int(width) / strides[1])\n    else:\n        out_height = math.ceil((int(height) - pool_size[0] + 1) / strides[0])\n        out_width = math.ceil((int(width) - pool_size[1] + 1) / strides[1])\n\n    batch, channels, height, width = x.shape\n    pool_height, pool_width = pool_size\n\n    with tf.device(prot.server_0.device_name):\n        x_split = x_on_0.reshape((batch * channels, 1, height, width))\n        y_on_0 = x_split.im2col(pool_height, pool_width, padding, strides[0])\n\n    with tf.device(prot.server_1.device_name):\n        x_split = x_on_1.reshape((batch * channels, 1, height, width))\n        y_on_1 = x_split.im2col(pool_height, pool_width, padding, strides[0])\n\n    return y_on_0, y_on_1, [out_height, out_width, int(batch), int(channels)]\n\n\ndef _maxpool2d_public(\n    prot: Pond,\n    x: PondPublicTensor,\n    pool_size: Tuple[int, int],\n    strides: Tuple[int, int],\n    padding: str,\n) -> PondPublicTensor:\n    """"""Logic for performing maxpool2d on public input.""""""\n    with tf.name_scope(""maxpool2d""):\n        y_on_0, y_on_1, reshape_to = _im2col(prot, x, pool_size, strides, padding)\n        im2col = PondPublicTensor(prot, y_on_0, y_on_1, x.is_scaled)\n        i2c_max = im2col.reduce_max(axis=0)\n        result = i2c_max.reshape(reshape_to).transpose([2, 3, 0, 1])\n        return result\n\n\ndef _maxpool2d_private(\n    prot: Pond,\n    x: PondPrivateTensor,\n    pool_size: Tuple[int, int],\n    strides: Tuple[int, int],\n    padding: str,\n) -> PondPrivateTensor:\n    """"""Logic for performing maxpool2d on private input.""""""\n    with tf.name_scope(""maxpool2d""):\n        y_on_0, y_on_1, reshape_to = _im2col(prot, x, pool_size, strides, padding)\n        im2col = PondPrivateTensor(prot, y_on_0, y_on_1, x.is_scaled)\n        i2c_max = im2col.reduce_max(axis=0)\n        result = i2c_max.reshape(reshape_to).transpose([2, 3, 0, 1])\n        return result\n\n\ndef _maxpool2d_masked(\n    prot: Pond,\n    x: PondMaskedTensor,\n    pool_size: Tuple[int, int],\n    strides: Tuple[int, int],\n    padding: str,\n) -> PondPrivateTensor:\n    with tf.name_scope(""maxpool2d""):\n        return prot.maxpool2d(x.unwrapped, pool_size, strides, padding)\n\n\n#\n# cast helpers\n#\n\n\ndef _cast_backing_public(prot, x: PondPublicTensor, backing_dtype):\n    """"""Cast a public tensor\'s backing dtype.""""""\n    # See refactoring comment below under private version.\n\n    x_on_0, x_on_1 = x.unwrapped\n\n    with tf.name_scope(""cast_backing""):\n\n        with tf.device(prot.server_0.device_name):\n            y_on_0 = x_on_0.cast(backing_dtype)\n\n        with tf.device(prot.server_1.device_name):\n            y_on_1 = x_on_1.cast(backing_dtype)\n\n        return PondPublicTensor(prot, y_on_0, y_on_1, x.is_scaled)\n\n\ndef _cast_backing_private(prot, x: PondPrivateTensor, backing_dtype):\n    """"""Cast a private tensor\'s backing dtype.""""""\n    # TODO[Morten]\n    # this method is risky as it differs from what the user might expect, which\n    # would normally require more advanced conversion protocols accounting for\n    # wrap-around etc; for this reason we should consider hiding it during\n    # refactoring\n\n    x0, x1 = x.unwrapped\n\n    with tf.name_scope(""cast_backing""):\n\n        with tf.device(prot.server_0.device_name):\n            y0 = x0.cast(backing_dtype)\n\n        with tf.device(prot.server_1.device_name):\n            y1 = x1.cast(backing_dtype)\n\n        return PondPrivateTensor(prot, y0, y1, x.is_scaled)\n'"
tf_encrypted/protocol/securenn/securenn_test.py,24,"b'# pylint: disable=missing-docstring\nimport random\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\nfrom tf_encrypted.protocol.pond import PondPrivateTensor\nfrom tf_encrypted.protocol.pond import PondPublicTensor\nfrom tf_encrypted.protocol.securenn.securenn import _private_compare\nfrom tf_encrypted.tensor import int100factory\n\n\nclass TestPrivateCompare(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_int64(self):\n        self._core_test(tfe.tensor.int64factory)\n\n    def test_int100(self):\n        self._core_test(tfe.tensor.int100factory)\n\n    def _core_test(self, tensor_factory):\n\n        prot = tfe.protocol.SecureNN(tensor_factory=tensor_factory)\n\n        bit_dtype = prot.prime_factory\n        val_dtype = prot.tensor_factory\n\n        x = np.array([21, 21, 21, 21, 21, 21, 21, 21], dtype=np.int32,).reshape(\n            (2, 2, 2)\n        )\n\n        r = np.array([36, 20, 21, 22, 36, 20, 21, 22], dtype=np.int32,).reshape(\n            (2, 2, 2)\n        )\n\n        beta = np.array([0, 0, 0, 0, 1, 1, 1, 1], dtype=np.int32,).reshape((2, 2, 2))\n\n        expected = np.bitwise_xor(x > r, beta.astype(bool)).astype(np.int32)\n        x_native = tf.convert_to_tensor(x, dtype=val_dtype.native_type)\n        x_bits_preshare = val_dtype.tensor(x_native).bits(bit_dtype)\n        x_bits = prot._share(x_bits_preshare)  # pylint: disable=protected-access\n\n        r_native = tf.convert_to_tensor(r, dtype=val_dtype.native_type)\n        r0 = r1 = val_dtype.tensor(r_native)\n\n        beta_native = tf.convert_to_tensor(beta, dtype=bit_dtype.native_type)\n        beta0 = beta1 = bit_dtype.tensor(beta_native)\n\n        res = _private_compare(\n            prot,\n            x_bits=PondPrivateTensor(prot, *x_bits, False),\n            r=PondPublicTensor(prot, r0, r1, False),\n            beta=PondPublicTensor(prot, beta0, beta1, False),\n        )\n\n        with tfe.Session() as sess:\n            actual = sess.run(res.reveal().value_on_0.to_native())\n            np.testing.assert_array_equal(actual, expected)\n\n\nclass TestSelectShare(unittest.TestCase):\n    def test_select_share(self):\n\n        alice = np.array([1, 1, 1, 1]).astype(np.float32)\n        bob = np.array([2, 2, 2, 2]).astype(np.float32)\n        bit = np.array([1, 0, 1, 0]).astype(np.float32)\n\n        expected = np.array([2, 1, 2, 1]).astype(np.float32)\n\n        with tfe.protocol.SecureNN() as prot:\n            alice_input = prot.define_private_variable(alice, apply_scaling=True)\n            bob_input = prot.define_private_variable(bob, apply_scaling=True)\n            bit_input = prot.define_private_variable(bit, apply_scaling=False)\n\n            select = prot.select(bit_input, alice_input, bob_input)\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                chosen = sess.run(select.reveal())\n\n            np.testing.assert_equal(expected, chosen)\n\n\nclass TestLSB(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def _core_lsb(self, tensor_factory, prime_factory):\n\n        f_bin = np.vectorize(np.binary_repr)\n        f_get = np.vectorize(lambda x, ix: x[ix])\n\n        raw = np.array([random.randrange(0, 10000000000) for _ in range(20)])\n        raw = raw.reshape((2, 2, 5))\n        expected_lsb = f_get(f_bin(raw), -1).astype(np.int32)\n\n        with tfe.protocol.SecureNN(\n            tensor_factory=tensor_factory, prime_factory=prime_factory,\n        ) as prot:\n\n            x_in = prot.define_private_variable(\n                raw, apply_scaling=False, name=""test_lsb_input""\n            )\n            x_lsb = prot.lsb(x_in)\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                actual_lsb = sess.run(x_lsb.reveal(), tag=""lsb"")\n\n                np.testing.assert_array_equal(actual_lsb, expected_lsb)\n\n    def test_lsb_int100(self):\n        self._core_lsb(int100factory, None)\n\n\nclass TestArgMax(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    @unittest.skipUnless(\n        tfe.config.tensorflow_supports_int64(), ""Too slow on Circle CI otherwise""\n    )\n    def test_argmax_1d(self):\n\n        t = np.array([1, 2, 3, 4, 5, 6, 7, 8]).astype(float)\n\n        with tf.Session() as sess:\n            out_tf = tf.argmax(t)\n            expected = sess.run(out_tf)\n\n        with tfe.protocol.SecureNN() as prot:\n            out_tfe = prot.argmax(prot.define_private_variable(tf.constant(t)))\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                actual = sess.run(out_tfe.reveal())\n\n        np.testing.assert_array_equal(actual, expected)\n\n    @unittest.skipUnless(\n        tfe.config.tensorflow_supports_int64(), ""Too slow on Circle CI otherwise""\n    )\n    def test_argmax_2d_axis0(self):\n\n        t = np.array([1, 2, 3, 4, 5, 6, 7, 8]).reshape(2, 4).astype(float)\n\n        with tf.Session() as sess:\n            out_tf = tf.argmax(t, axis=0)\n            expected = sess.run(out_tf)\n\n        with tfe.protocol.SecureNN() as prot:\n            out_tfe = prot.argmax(prot.define_private_variable(tf.constant(t)), axis=0)\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                actual = sess.run(out_tfe.reveal())\n\n        np.testing.assert_array_equal(actual, expected)\n\n    @unittest.skipUnless(\n        tfe.config.tensorflow_supports_int64(), ""Too slow on Circle CI otherwise""\n    )\n    def test_argmax_2d_axis1(self):\n\n        t = np.array([1, 2, 3, 4, 5, 6, 7, 8]).reshape(2, 4).astype(float)\n\n        with tf.Session() as sess:\n            out_tf = tf.argmax(t, axis=1)\n            expected = sess.run(out_tf)\n\n        with tfe.protocol.SecureNN() as prot:\n            out_tfe = prot.argmax(prot.define_private_variable(tf.constant(t)), axis=1)\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                actual = sess.run(out_tfe.reveal())\n\n        np.testing.assert_array_equal(actual, expected)\n\n    @unittest.skipUnless(\n        tfe.config.tensorflow_supports_int64(), ""Too slow on Circle CI otherwise""\n    )\n    def test_argmax_3d_axis0(self):\n\n        t = np.array(np.arange(128)).reshape((8, 2, 2, 2, 2))\n\n        with tf.Session() as sess:\n            out = tf.argmax(t, axis=0)\n            expected = sess.run(out)\n\n        with tfe.protocol.SecureNN() as prot:\n            out_tfe = prot.argmax(prot.define_private_variable(tf.constant(t)), axis=0)\n\n            with tfe.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                actual = sess.run(out_tfe.reveal())\n\n        np.testing.assert_array_equal(actual, expected)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
primitives/tf_encrypted/primitives/paillier/__init__.py,0,"b'from .primitives import Ciphertext\nfrom .primitives import DecryptionKey\nfrom .primitives import EncryptionKey\nfrom .primitives import Randomness\nfrom .primitives import add\nfrom .primitives import decrypt\nfrom .primitives import encrypt\nfrom .primitives import gen_keypair\nfrom .primitives import gen_randomness\nfrom .primitives import mul\nfrom .primitives import refresh\n\n__all__ = [\n    ""Ciphertext"",\n    ""DecryptionKey"",\n    ""EncryptionKey"",\n    ""Randomness"",\n    ""add"",\n    ""decrypt"",\n    ""encrypt"",\n    ""gen_keypair"",\n    ""gen_randomness"",\n    ""mul"",\n    ""refresh"",\n]\n'"
primitives/tf_encrypted/primitives/paillier/primitives.py,8,"b'from typing import Optional\n\nimport tensorflow as tf\nimport tf_big\n\ntf_big.set_secure_default(True)\n\n\nclass EncryptionKey:\n    """"""Paillier encryption key.\n\n    Note that the generator `g` has been fixed to `1 + n`.\n    """"""\n\n    def __init__(self, n):\n        n = tf_big.convert_to_tensor(n)\n\n        self.n = n\n        self.nn = n * n\n\n    def export(self, dtype: tf.DType = tf.string):\n        return tf_big.convert_from_tensor(self.n, dtype=dtype)\n\n\nclass DecryptionKey:\n    def __init__(self, p, q):\n        self.p = tf_big.convert_to_tensor(p)\n        self.q = tf_big.convert_to_tensor(q)\n\n        self.n = self.p * self.q\n        self.nn = self.n * self.n\n\n        order_of_n = (self.p - 1) * (self.q - 1)\n        self.d1 = order_of_n\n        self.d2 = tf_big.inv(order_of_n, self.n)\n        self.e = tf_big.inv(self.n, order_of_n)\n\n    def export(self, dtype: tf.DType = tf.string):\n        return (\n            tf_big.convert_from_tensor(self.p, dtype=dtype),\n            tf_big.convert_from_tensor(self.q, dtype=dtype),\n        )\n\n\ndef gen_keypair(bitlength=2048):\n    p, q, n = tf_big.random_rsa_modulus(bitlength=bitlength)\n    ek = EncryptionKey(n)\n    dk = DecryptionKey(p, q)\n    return ek, dk\n\n\nclass Randomness:\n    def __init__(self, raw_randomness):\n        self.raw = tf_big.convert_to_tensor(raw_randomness)\n\n    def export(self, dtype: tf.DType = tf.string):\n        return tf_big.convert_from_tensor(self.raw, dtype=dtype)\n\n\ndef gen_randomness(ek, shape):\n    return Randomness(tf_big.random_uniform(shape=shape, maxval=ek.n))\n\n\nclass Ciphertext:\n    def __init__(self, ek: EncryptionKey, raw_ciphertext):\n        self.ek = ek\n        self.raw = tf_big.convert_to_tensor(raw_ciphertext)\n\n    def export(self, dtype: tf.DType = tf.string):\n        return tf_big.convert_from_tensor(self.raw, dtype=dtype)\n\n    def __add__(self, other):\n        assert self.ek == other.ek\n        return add(self.ek, self, other)\n\n\ndef encrypt(\n    ek: EncryptionKey, plaintext: tf.Tensor, randomness: Optional[Randomness] = None,\n):\n    x = tf_big.convert_to_tensor(plaintext)\n\n    randomness = randomness or gen_randomness(ek=ek, shape=x.shape)\n    r = randomness.raw\n    assert r.shape == x.shape\n\n    gx = 1 + (ek.n * x) % ek.nn\n    rn = tf_big.pow(r, ek.n, ek.nn)\n    c = gx * rn % ek.nn\n    return Ciphertext(ek, c)\n\n\ndef decrypt(dk: DecryptionKey, ciphertext: Ciphertext, dtype: tf.DType = tf.int32):\n    c = ciphertext.raw\n\n    gxd = tf_big.pow(c, dk.d1, dk.nn)\n    xd = (gxd - 1) // dk.n\n    x = (xd * dk.d2) % dk.n\n\n    if dtype == tf.variant:\n        return x\n\n    return tf_big.convert_from_tensor(x, dtype=dtype)\n\n\ndef refresh(ek: EncryptionKey, ciphertext: Ciphertext):\n    c = ciphertext.raw\n    s = gen_randomness(ek=ek, shape=c.shape).raw\n    sn = tf_big.pow(s, ek.n, ek.nn)\n    d = (c * sn) % ek.nn\n    return Ciphertext(ek, d)\n\n\ndef add(\n    ek: EncryptionKey, lhs: Ciphertext, rhs: Ciphertext, do_refresh: bool = True,\n):\n    c0 = tf_big.convert_to_tensor(lhs.raw)\n    c1 = tf_big.convert_to_tensor(rhs.raw)\n    c = (c0 * c1) % ek.nn\n    res = Ciphertext(ek, c)\n\n    if not do_refresh:\n        return res\n    return refresh(ek, res)\n\n\ndef mul(\n    ek: EncryptionKey, lhs: Ciphertext, rhs: tf.Tensor, do_refresh: bool = True,\n):\n    c = lhs.raw\n    k = tf_big.convert_to_tensor(rhs)\n    d = tf_big.pow(c, k) % ek.nn\n    res = Ciphertext(ek, d)\n\n    if not do_refresh:\n        return res\n    return refresh(ek, res)\n'"
primitives/tf_encrypted/primitives/paillier/primitives_test.py,12,"b'# pylint: disable=missing-docstring\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\nfrom absl.testing import parameterized\n\nfrom tf_encrypted.primitives import paillier\nfrom tf_encrypted.test import tf_execution_context\n\n\nclass EncryptionTest(parameterized.TestCase):\n    @parameterized.parameters(\n        {\n            ""run_eagerly"": run_eagerly,\n            ""export_dtype"": export_dtype,\n            ""export_expansion"": export_expansion,\n        }\n        for run_eagerly in [True, False]\n        for export_dtype, export_expansion in [(tf.string, ())]\n    )\n    def test_export(self, run_eagerly, export_dtype, export_expansion):\n        x = np.array([[12345, 34342]])\n\n        context = tf_execution_context(run_eagerly)\n        with context.scope():\n\n            ek, dk = paillier.gen_keypair()\n            assert isinstance(ek, paillier.EncryptionKey)\n            assert isinstance(dk, paillier.DecryptionKey)\n            n_exported = ek.export(export_dtype)\n            assert isinstance(n_exported, tf.Tensor)\n            assert n_exported.dtype == export_dtype\n            assert n_exported.shape == ()\n            p_exported, q_exported = dk.export(export_dtype)\n            assert isinstance(p_exported, tf.Tensor)\n            assert p_exported.dtype == export_dtype\n            assert p_exported.shape == ()\n            assert isinstance(q_exported, tf.Tensor)\n            assert q_exported.dtype == export_dtype\n            assert q_exported.shape == ()\n\n            r = paillier.gen_randomness(ek, shape=x.shape)\n            assert isinstance(r, paillier.Randomness)\n            r_exported = r.export(export_dtype)\n            assert isinstance(r_exported, tf.Tensor)\n            assert r_exported.dtype == export_dtype\n            assert r_exported.shape == x.shape + export_expansion\n\n            c = paillier.encrypt(ek, x, r)\n            assert isinstance(c, paillier.Ciphertext)\n            c_exported = c.export(export_dtype)\n            assert isinstance(c_exported, tf.Tensor)\n            assert c_exported.dtype == export_dtype\n            assert c_exported.shape == x.shape + export_expansion\n\n    @parameterized.parameters(\n        {""run_eagerly"": run_eagerly} for run_eagerly in (True, False)\n    )\n    def test_correctness(self, run_eagerly):\n\n        p = 100000015333\n        q = 100000015021\n        n = p * q\n        nn = n * n\n        g = 1 + n\n        x = 123456789\n        r = 5083216764521909821749\n        c = pow(g, x, nn) * pow(r, n, nn) % nn\n\n        context = tf_execution_context(run_eagerly)\n        with context.scope():\n\n            ek = paillier.EncryptionKey(str(n))\n            plaintext = np.array([[x]]).astype(str)\n            randomness = paillier.Randomness(np.array([[r]]).astype(str))\n            ciphertext = paillier.encrypt(ek, plaintext, randomness)\n\n            expected = np.array([[c]]).astype(str)\n            actual = ciphertext.export(tf.string)\n\n        np.testing.assert_equal(context.evaluate(actual).astype(str), expected)\n\n    @parameterized.parameters(\n        {""run_eagerly"": run_eagerly, ""x"": x, ""dtype"": dtype}\n        for run_eagerly in [True, False]\n        for x, dtype in [\n            (np.array([[12345, 34342]]).astype(np.int32), tf.int32),\n            (np.array([[""12345"", ""34342""]]).astype(str), tf.string),\n            (\n                np.array(\n                    [\n                        [\n                            ""123456789123456789123456789123456789"",\n                            ""987654321987654321987654321987654321"",\n                        ]\n                    ]\n                ).astype(str),\n                tf.string,\n            ),\n        ]\n    )\n    def test_encrypt_decrypt(self, run_eagerly, x, dtype):\n        context = tf_execution_context(run_eagerly)\n        with context.scope():\n\n            ek, dk = paillier.gen_keypair()\n            r = paillier.gen_randomness(ek, shape=x.shape)\n            c = paillier.encrypt(ek, x, r)\n            y = paillier.decrypt(dk, c, dtype=dtype)\n            assert isinstance(y, tf.Tensor)\n            assert y.dtype == dtype\n\n        np.testing.assert_equal(context.evaluate(y).astype(x.dtype), x)\n\n    @parameterized.parameters(\n        {""run_eagerly"": run_eagerly, ""dtype"": dtype, ""x0"": x0, ""x1"": x1}\n        for run_eagerly in (True, False)\n        for dtype in (tf.int32, tf.string)\n        for x0 in (np.array([[12345, 123243]]), np.array([[12345]]))\n        for x1 in (np.array([[12656, 434234]]),)\n    )\n    def test_add(self, run_eagerly, dtype, x0, x1):\n\n        expected = x0 + x1\n\n        context = tf_execution_context(run_eagerly)\n        with context.scope():\n            ek, dk = paillier.gen_keypair()\n\n            r0 = paillier.gen_randomness(ek, shape=x0.shape)\n            c0 = paillier.encrypt(ek, x0, r0)\n\n            r1 = paillier.gen_randomness(ek, shape=x1.shape)\n            c1 = paillier.encrypt(ek, x1, r1)\n\n            c = paillier.add(ek, c0, c1)\n            y = paillier.decrypt(dk, c, dtype=dtype)\n\n        np.testing.assert_equal(\n            context.evaluate(y).astype(np.int32), expected.astype(np.int32)\n        )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
primitives/tf_encrypted/primitives/sodium/__init__.py,0,"b'from .python import easy_box\n\n__all__ = [\n    ""easy_box"",\n]\n'"
tf_encrypted/protocol/aby3/model/logistic_regression.py,57,"b'# flake8: noqa\n\nimport tensorflow as tf\n\nimport tf_encrypted as tfe\n\n\nclass LogisticRegression:\n    def __init__(self, num_features, init_learning_rate=0.01):\n        self.w = tfe.define_private_variable(\n            tf.random_uniform([num_features, 1], -0.01, 0.01)\n        )\n        self.b = tfe.define_private_variable(tf.zeros([1]))\n        self.init_learning_rate = init_learning_rate\n\n    @property\n    def weights(self):\n        return self.w, self.b\n\n    def forward(self, x):\n        with tf.name_scope(""forward""):\n            out = tfe.matmul(x, self.w) + self.b\n            y_hat = tfe.sigmoid(out)\n            return y_hat\n\n    def backward(self, x, dy, learning_rate):\n        batch_size = x.shape.as_list()[0]\n        with tf.name_scope(""backward""):\n            dw = tfe.matmul(tfe.transpose(x), dy) / batch_size\n            db = tfe.reduce_sum(dy, axis=0) / batch_size\n            assign_ops = [\n                tfe.assign(self.w, self.w - dw * learning_rate),\n                tfe.assign(self.b, self.b - db * learning_rate),\n            ]\n            return assign_ops\n\n    def loss_grad(self, y, y_hat):\n        with tf.name_scope(""loss-grad""):\n            dy = y_hat - y\n            return dy\n\n    def fit_batch(self, x, y):\n        with tf.name_scope(""fit-batch""):\n            y_hat = self.forward(x)\n            dy = self.loss_grad(y, y_hat)\n            fit_batch_op = self.backward(x, dy, self.init_learning_rate)\n            return fit_batch_op\n\n    def fit(self, sess, x, y, num_iters):\n        fit_batch_op = self.fit_batch(x, y)\n        for batch in range(num_iters):\n            sess.run(fit_batch_op, tag=""fit-batch"")\n\n    """"""\n    Revealing and printing loss in the middle of training should only be used for debugging.\n    It might leak information in a normal run.\n    """"""\n\n    def loss(self, sess, x, y, player_name):\n        def print_loss(y_hat, y):\n            with tf.name_scope(""print-loss""):\n                loss = -y * tf.log(y_hat) - (1 - y) * tf.log(1 - y_hat)\n                print_op = tf.print(""Loss on {}:"".format(player_name), loss)\n                return print_op\n\n        with tf.name_scope(""loss""):\n            y_hat = self.forward(x)\n            print_loss_op = tfe.define_output(player_name, [y_hat, y], print_loss)\n        sess.run(print_loss_op, tag=""loss"")\n\n    def evaluate(self, sess, x, y, data_owner):\n        """"""Return the accuracy""""""\n\n        def print_accuracy(y_hat, y) -> tf.Operation:\n            with tf.name_scope(""print-accuracy""):\n                correct_prediction = tf.equal(tf.round(y_hat), y)\n                accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n                print_op = tf.print(\n                    ""Accuracy on {}:"".format(data_owner.player_name), accuracy\n                )\n                return print_op\n\n        with tf.name_scope(""evaluate""):\n            y_hat = self.forward(x)\n            print_accuracy_op = tfe.define_output(\n                data_owner.player_name, [y_hat, y], print_accuracy\n            )\n\n        sess.run(print_accuracy_op, tag=""evaluate"")\n\n\nclass FakeDataOwner:\n    def __init__(\n        self, player_name, num_features, train_set_size, test_set_size, batch_size\n    ):\n        self.player_name = player_name\n        self.num_features = num_features\n        self.train_set_size = train_set_size\n        self.test_set_size = test_set_size\n        self.batch_size = batch_size\n        self.train_initilizer = None\n        self.test_initializer = None\n\n    @property\n    def initializer(self):\n        return self.train_initilizer, self.test_initializer\n\n    def provide_train_data_fake(self):\n        x_raw = tf.random.uniform(\n            minval=-0.5, maxval=0.5, shape=[self.train_set_size, self.num_features]\n        )\n        # y_raw is created as a simple linear combination of x_raw\'s feature values\n        y_raw = tf.cast(\n            tf.reduce_mean(x_raw, axis=1, keepdims=True) > 0, dtype=tf.float32\n        )\n\n        train_set = (\n            tf.data.Dataset.from_tensor_slices((x_raw, y_raw))\n            .repeat()\n            .shuffle(buffer_size=self.batch_size)\n            .batch(self.batch_size)\n        )\n\n        train_set_iterator = train_set.make_initializable_iterator()\n        self.train_initilizer = train_set_iterator.initializer\n\n        x, y = train_set_iterator.get_next()\n        x = tf.reshape(x, [self.batch_size, self.num_features])\n        y = tf.reshape(y, [self.batch_size, 1])\n\n        return x, y\n\n    def provide_train_features_fake(self):\n        x_raw = tf.random.uniform(\n            minval=-0.5, maxval=0.5, shape=[self.train_set_size, self.num_features]\n        )\n\n        train_set = (\n            tf.data.Dataset.from_tensor_slices(x_raw)\n            .repeat()\n            .shuffle(buffer_size=self.batch_size)\n            .batch(self.batch_size)\n        )\n\n        train_set_iterator = train_set.make_initializable_iterator()\n        self.train_initilizer = train_set_iterator.initializer\n\n        x = train_set_iterator.get_next()\n        x = tf.reshape(x, [self.batch_size, self.num_features])\n\n        return x\n\n    def provide_train_targets_fake(self, *train_feature_sets):\n        x = tf.concat(train_feature_sets, axis=1)\n        y = tf.cast(tf.reduce_mean(x, axis=1, keepdims=True) > 0, dtype=tf.float32)\n        y = tf.reshape(y, [self.batch_size, 1])\n        return y\n\n    def provide_test_data_fake(self):\n        x_raw = tf.random.uniform(\n            minval=-0.5, maxval=0.5, shape=[self.test_set_size, self.num_features]\n        )\n\n        y_raw = tf.cast(tf.reduce_mean(x_raw, axis=1) > 0, dtype=tf.float32)\n\n        test_set = (\n            tf.data.Dataset.from_tensor_slices((x_raw, y_raw))\n            .repeat()\n            .batch(self.test_set_size)\n        )\n\n        test_set_iterator = test_set.make_initializable_iterator()\n        self.test_initializer = test_set_iterator.initializer\n\n        x, y = test_set_iterator.get_next()\n        x = tf.reshape(x, [self.test_set_size, self.num_features])\n        y = tf.reshape(y, [self.test_set_size, 1])\n\n        return x, y\n\n    def provide_test_features_fake(self):\n        x_raw = tf.random.uniform(\n            minval=-0.5, maxval=0.5, shape=[self.test_set_size, self.num_features]\n        )\n\n        test_set = (\n            tf.data.Dataset.from_tensor_slices(x_raw).repeat().batch(self.test_set_size)\n        )\n\n        test_set_iterator = test_set.make_initializable_iterator()\n        self.test_initializer = test_set_iterator.initializer\n\n        x = test_set_iterator.get_next()\n        x = tf.reshape(x, [self.test_set_size, self.num_features])\n\n        return x\n\n    def provide_test_targets_fake(self, *test_feature_sets):\n        x = tf.concat(test_feature_sets, axis=1)\n        y = tf.cast(tf.reduce_mean(x, axis=1, keepdims=True) > 0, dtype=tf.float32)\n        y = tf.reshape(y, [self.test_set_size, 1])\n        return y\n\n\nclass DataSchema:\n    def __init__(self, field_types, field_defaults):\n        self.field_types = field_types\n        self.field_defaults = field_defaults\n\n    @property\n    def field_num(self):\n        return len(self.field_types)\n\n\nclass DataOwner:\n    def __init__(\n        self,\n        player_name,\n        local_data_file,\n        data_schema,\n        header=False,\n        index=False,\n        field_delim="","",\n        na_values=[""nan""],\n        batch_size=128,\n    ):\n        self.player_name = player_name\n        self.local_data_file = local_data_file\n        self.data_schema = data_schema\n        self.batch_size = batch_size\n        self.header = header\n        self.index = index\n        self.na_values = na_values\n        self.field_delim = field_delim\n\n    def provide_data_experimental(self):\n        """"""\n        Use TF\'s CsvDataset to load local data, but it is too slow.\n        Please use `self.provide_data` instead.\n        """"""\n        dataset = (\n            tf.data.experimental.CsvDataset(\n                self.local_data_file,\n                [\n                    tf.constant(\n                        [self.data_schema.field_defaults[i]],\n                        dtype=self.data_schema.field_types[i],\n                    )\n                    for i in range(self.data_schema.field_num)\n                ],\n                header=self.header,\n                field_delim=self.field_delim,\n                select_cols=list(range(self.data_schema.field_num))\n                if not self.index\n                else list(range(1, self.data_schema.field_num + 1)),\n            )\n            .repeat()\n            .batch(self.batch_size)\n        )\n        iterator = dataset.make_one_shot_iterator()\n        batch = iterator.get_next()\n        batch = tf.transpose(tf.stack(batch, axis=0))\n        return batch\n\n    def provide_data(self):\n        def decode(line):\n            fields = tf.string_split([line], self.field_delim).values\n            if self.index:  # Skip index\n                fields = fields[1:]\n            fields = tf.regex_replace(fields, ""|"".join(self.na_values), ""nan"")\n            fields = tf.string_to_number(fields, tf.float32)\n            return fields\n\n        def fill_na(fields, fill_values):\n            fields = tf.where(tf.is_nan(fields), fill_values, fields)\n            return fields\n\n        dataset = tf.data.TextLineDataset(self.local_data_file)\n        if self.header:  # Skip header\n            dataset = dataset.skip(1)\n        dataset = (\n            dataset.map(decode)\n            .map(lambda x: fill_na(x, self.data_schema.field_defaults))\n            .repeat()\n            .batch(self.batch_size)\n        )\n\n        iterator = dataset.make_one_shot_iterator()\n        batch = iterator.get_next()\n        batch = tf.reshape(batch, [self.batch_size, self.data_schema.field_num])\n        return batch\n\n\nclass ModelOwner:\n    def __init__(self, player_name):\n        self.player_name = player_name\n\n    def receive_weights(self, *weights):\n        return tf.print(""Weights on {}:\\n"".format(self.player_name), weights)\n\n\nclass PredictionClient:\n    def __init__(self, player_name, num_features):\n        self.player_name = player_name\n        self.num_features = num_features\n\n    def provide_input_fake(self):\n        return tf.random.uniform(\n            minval=-0.5, maxval=0.5, dtype=tf.float32, shape=[1, self.num_features]\n        )\n\n    def receive_output(self, result):\n        return tf.print(""Result on {}:"".format(self.player_name), result)\n\n\nclass LossDebugger:\n    def __init__(self, player_name):\n        self.player_name = player_name\n\n    def loss(self, model, x, y):\n        def print_loss(y_hat, y):\n            with tf.name_scope(""print-loss""):\n                loss = -y * tf.log(y_hat) - (1 - y) * tf.log(1 - y_hat)\n                loss = tf.reduce_mean(loss)\n                print_op = tf.print(""Loss on {}:"".format(self.player_name), loss)\n                return print_op\n\n        with tf.name_scope(""loss""):\n            y_hat = model.forward(x)\n            print_loss_op = tfe.define_output(self.player_name, [y_hat, y], print_loss)\n        return print_loss_op\n'"
primitives/tf_encrypted/primitives/sodium/python/__init__.py,0,b''
primitives/tf_encrypted/primitives/sodium/python/easy_box.py,4,"b'import os\nfrom typing import Tuple\n\nimport tensorflow as tf\n\nSO_FILE = ""{current_dir}/_sodium_module.so"".format(\n    current_dir=os.path.dirname(__file__),\n)\nassert os.path.exists(SO_FILE), (\n    ""Could not find required \'sodium\' module, \'{so_file}\'""\n).format(so_file=SO_FILE)\n\nsodium_module = tf.load_op_library(SO_FILE)\nassert sodium_module, ""Could not load required \'sodium\' module""\n\n\nclass PublicKey:\n    def __init__(self, raw_pk):\n        self.raw = raw_pk\n\n\nclass SecretKey:\n    def __init__(self, raw_sk):\n        self.raw = raw_sk\n\n\ndef gen_keypair() -> Tuple[PublicKey, SecretKey]:\n    raw_pk, raw_sk = sodium_module.sodium_easy_box_gen_keypair()\n    return PublicKey(raw_pk), SecretKey(raw_sk)\n\n\nclass Nonce:\n    def __init__(self, raw_nonce):\n        self.raw = raw_nonce\n\n\ndef gen_nonce() -> Nonce:\n    raw_nonce = sodium_module.sodium_easy_box_gen_nonce()\n    return Nonce(raw_nonce)\n\n\nclass Ciphertext:\n    def __init__(self, raw_ciphertext):\n        self.raw = raw_ciphertext\n\n\nclass Mac:\n    def __init__(self, raw_mac):\n        self.raw = raw_mac\n\n\ndef seal_detached(\n    plaintext: tf.Tensor,\n    nonce: Nonce,\n    publickey_receiver: PublicKey,\n    secretkey_sender: SecretKey,\n) -> Tuple[Ciphertext, Mac]:\n    ciphertext, mac = sodium_module.sodium_easy_box_seal_detached(\n        plaintext, nonce.raw, publickey_receiver.raw, secretkey_sender.raw\n    )\n    return Ciphertext(ciphertext), Mac(mac)\n\n\ndef open_detached(\n    ciphertext: Ciphertext,\n    mac: Mac,\n    nonce: Nonce,\n    public_sender: PublicKey,\n    secretkey_receiver: SecretKey,\n    plaintext_dtype: tf.DType,\n) -> tf.Tensor:\n    plaintext = sodium_module.sodium_easy_box_open_detached(\n        ciphertext.raw,\n        mac.raw,\n        nonce.raw,\n        public_sender.raw,\n        secretkey_receiver.raw,\n        plaintext_dtype,\n    )\n    return plaintext\n'"
primitives/tf_encrypted/primitives/sodium/python/easy_box_test.py,23,"b'# pylint: disable=missing-docstring\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\nfrom absl.testing import parameterized\n\nfrom tf_encrypted.primitives.sodium.python import easy_box\nfrom tf_encrypted.test import tf_execution_context\n\n\nclass TestEasyBox(parameterized.TestCase):\n    @parameterized.parameters(\n        {""run_eagerly"": True}, {""run_eagerly"": False},\n    )\n    def test_gen_keypair(self, run_eagerly):\n        context = tf_execution_context(run_eagerly)\n        with context.scope():\n            pk, sk = easy_box.gen_keypair()\n\n        assert isinstance(pk, easy_box.PublicKey), type(pk)\n        assert isinstance(pk.raw, tf.Tensor)\n        assert pk.raw.dtype == tf.uint8\n        assert pk.raw.shape == (32,)\n\n        assert isinstance(sk, easy_box.SecretKey), type(sk)\n        assert isinstance(sk.raw, tf.Tensor)\n        assert sk.raw.dtype == tf.uint8\n        assert sk.raw.shape == (32,)\n\n    @parameterized.parameters(\n        {""run_eagerly"": True}, {""run_eagerly"": False},\n    )\n    def test_gen_nonce(self, run_eagerly):\n        context = tf_execution_context(run_eagerly)\n        with context.scope():\n            nonce = easy_box.gen_nonce()\n\n        assert isinstance(nonce, easy_box.Nonce), type(nonce)\n        assert isinstance(nonce.raw, tf.Tensor)\n        assert nonce.raw.dtype == tf.uint8\n        assert nonce.raw.shape == (24,)\n\n    @parameterized.parameters(\n        {""run_eagerly"": run_eagerly, ""m"": m, ""dtype"": dtype, ""dtype_size"": dtype_size}\n        for run_eagerly in (True, False)\n        for m in (5, [5], [[1, 2], [3, 4]])\n        for dtype, dtype_size in [\n            (tf.uint8, 1),\n            (tf.uint16, 2),\n            (tf.uint32, 4),\n            (tf.uint64, 8),\n            (tf.int8, 1),\n            (tf.int16, 2),\n            (tf.int32, 4),\n            (tf.int64, 8),\n            (tf.bfloat16, 2),\n            (tf.float32, 4),\n            (tf.float64, 8),\n        ]\n    )\n    def test_seal_and_open(self, run_eagerly, m, dtype, dtype_size):\n        context = tf_execution_context(run_eagerly)\n        with context.scope():\n            pk_s, sk_s = easy_box.gen_keypair()\n            pk_r, sk_r = easy_box.gen_keypair()\n\n            plaintext = tf.constant(m, dtype=dtype)\n\n            nonce = easy_box.gen_nonce()\n            ciphertext, mac = easy_box.seal_detached(plaintext, nonce, pk_r, sk_s)\n\n            plaintext_recovered = easy_box.open_detached(\n                ciphertext, mac, nonce, pk_s, sk_r, plaintext.dtype\n            )\n\n        assert isinstance(ciphertext, easy_box.Ciphertext)\n        assert isinstance(ciphertext.raw, tf.Tensor)\n        assert ciphertext.raw.dtype == tf.uint8\n        assert ciphertext.raw.shape == plaintext.shape + (dtype_size,)\n\n        assert isinstance(mac, easy_box.Mac)\n        assert isinstance(mac.raw, tf.Tensor)\n        assert mac.raw.dtype == tf.uint8\n        assert mac.raw.shape == (16,)\n\n        assert isinstance(plaintext_recovered, tf.Tensor)\n        assert plaintext_recovered.dtype == plaintext.dtype\n        assert plaintext_recovered.shape == plaintext.shape\n\n        plaintext_recovered = context.evaluate(plaintext_recovered)\n        np.testing.assert_equal(plaintext_recovered, np.array(m))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
