file_path,api_count,code
01_linear_regression.py,0,"b""from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nimport numpy as np\n\n# Generate dataset\ntrX = np.linspace(-1, 1, 101)\ntrY = 2 * trX + np.random.randn(*trX.shape) * 0.33 # create a y value which is approximately linear but with some random noise\n\n# Linear regression model\nmodel = Sequential()\nmodel.add(Dense(output_dim=1, input_dim=1, init='normal', activation='linear'))\nmodel.compile(optimizer=SGD(lr=0.01), loss='mean_squared_error', metrics=['accuracy'])\n\n# Train\nmodel.fit(trX, trY, nb_epoch=100, verbose=1)"""
02_logistic_regression.py,0,"b""from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\n\nbatch_size = 128\nnb_classes = 10\nnb_epoch = 100\n\n# Load MNIST dataset\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = X_train.reshape(60000, 784)\nX_test = X_test.reshape(10000, 784)\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train /= 255\nX_test /= 255\nY_Train = np_utils.to_categorical(y_train, nb_classes)\nY_Test = np_utils.to_categorical(y_test, nb_classes)\n\n# Logistic regression model\nmodel = Sequential()\nmodel.add(Dense(output_dim=10, input_shape=(784,), init='normal', activation='softmax'))\nmodel.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n\n# Train\nhistory = model.fit(X_train, Y_Train, nb_epoch=nb_epoch, batch_size=batch_size, verbose=1)\n\n# Evaluate\nevaluation = model.evaluate(X_test, Y_Test, verbose=1)\nprint('Summary: Loss over the test dataset: %.2f, Accuracy: %.2f' % (evaluation[0], evaluation[1]))\n"""
03_net.py,0,"b""from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\n\nbatch_size = 128\nnb_classes = 10\nnb_epoch = 100\n\n# Load MNIST dataset\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = X_train.reshape(60000, 784)\nX_test = X_test.reshape(10000, 784)\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train /= 255\nX_test /= 255\nY_Train = np_utils.to_categorical(y_train, nb_classes)\nY_Test = np_utils.to_categorical(y_test, nb_classes)\n\n# Multilayer Perceptron model\nmodel = Sequential()\nmodel.add(Dense(output_dim=625, input_dim=784, init='normal', activation='sigmoid'))\nmodel.add(Dense(output_dim=625, input_dim=625, init='normal', activation='sigmoid'))\nmodel.add(Dense(output_dim=10, input_dim=625, init='normal', activation='softmax'))\nmodel.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n\n# Train\nhistory = model.fit(X_train, Y_Train, nb_epoch=nb_epoch, batch_size=batch_size, verbose=1)\n\n# Evaluate\nevaluation = model.evaluate(X_test, Y_Test, verbose=1)\nprint('Summary: Loss over the test dataset: %.2f, Accuracy: %.2f' % (evaluation[0], evaluation[1]))\n"""
04_modern_net.py,0,"b""from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.optimizers import RMSprop\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\n\nbatch_size = 128\nnb_classes = 10\nnb_epoch = 100\n\n# Load MNIST dataset\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = X_train.reshape(60000, 784)\nX_test = X_test.reshape(10000, 784)\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train /= 255\nX_test /= 255\nY_train = np_utils.to_categorical(y_train, nb_classes)\nY_test = np_utils.to_categorical(y_test, nb_classes)\n\n# Deep Multilayer Perceptron model\nmodel = Sequential()\nmodel.add(Dense(output_dim=625, input_dim=784, init='normal'))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(output_dim=625, input_dim=625, init='normal'))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(output_dim=10, input_dim=625, init='normal'))\nmodel.add(Activation('softmax'))\n\nmodel.compile(optimizer=RMSprop(lr=0.001, rho=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n\n# Train\nhistory = model.fit(X_train, Y_train, nb_epoch=nb_epoch, batch_size=batch_size, verbose=1)\n\n# Evaluate\nevaluation = model.evaluate(X_test, Y_test, verbose=1)\nprint('Summary: Loss over the test dataset: %.2f, Accuracy: %.2f' % (evaluation[0], evaluation[1]))\n"""
05_convolution_net_by_DeepCognition.ai.py,0,"b'from keras.layers.convolutional import Conv2D\r\nfrom keras.layers.pooling import MaxPooling2D\r\nfrom keras.layers.core import Dropout\r\nfrom keras.layers.core import Flatten\r\nfrom keras.layers.core import Dense\r\nfrom keras.layers import Input\r\nfrom keras.models import Model\r\nfrom keras.regularizers import *\r\n\r\n\r\ndef get_model():\r\n\tInput_1 = Input(shape=(28, 28, 1))\r\n\tConv2D_6 = Conv2D(activation= \'relu\' ,nb_col= 3,nb_filter= 32,nb_row= 3,border_mode= \'same\' )(Input_1)\r\n\tMaxPooling2D_4 = MaxPooling2D(strides= (2,2),border_mode= \'same\' ,pool_size= (2,2))(Conv2D_6)\r\n\tDropout_1 = Dropout(p= 0.3)(MaxPooling2D_4)\r\n\tConv2D_8 = Conv2D(nb_col= 3,nb_filter= 64,nb_row= 3,border_mode= \'same\' ,init= \'glorot_normal\' ,activation= \'relu\' )(Dropout_1)\r\n\tMaxPooling2D_6 = MaxPooling2D(strides= (2,2),border_mode= \'same\' ,pool_size= (2,2))(Conv2D_8)\r\n\tDropout_2 = Dropout(p= 0.3)(MaxPooling2D_6)\r\n\tConv2D_9 = Conv2D(nb_col= 3,nb_filter= 128,nb_row= 3,border_mode= \'same\' ,init= \'glorot_normal\' ,activation= \'relu\' )(Dropout_2)\r\n\tMaxPooling2D_7 = MaxPooling2D(border_mode= \'same\' ,pool_size= (2,2))(Conv2D_9)\r\n\tFlatten_2 = Flatten()(MaxPooling2D_7)\r\n\tDropout_3 = Dropout(p= 0.3)(Flatten_2)\r\n\tDense_4 = Dense(activation= \'relu\' ,init= \'glorot_normal\' ,output_dim= 625)(Dropout_3)\r\n\tDropout_4 = Dropout(p= 0.5)(Dense_4)\r\n\tDense_5 = Dense(activation= \'softmax\' ,output_dim= 10)(Dropout_4)\r\n\r\n\treturn Model([Input_1],[Dense_5])\r\n\r\n\r\nfrom keras.optimizers import *\r\n\r\ndef get_optimizer():\r\n\treturn Adadelta()\r\n\r\ndef get_loss_function():\r\n\treturn \'categorical_crossentropy\'\r\n\r\ndef get_batch_size():\r\n\treturn 64\r\n\r\ndef get_num_epoch():\r\n\treturn 10\r\n\r\ndef get_data_config():\r\n\t return {""mapping"": {""Image"": {""port"": ""InputPort0"", ""type"": ""Image""}, ""Digit Label"": {""port"": ""OutputPort0"", ""type"": ""Categorical""}}, ""samples"": {""split"": 1, ""validation"": 14000, ""test"": 14000, ""training"": 42000}, ""dataset"": {""samples"": 70000, ""type"": ""public"", ""name"": ""mnist""}, ""datasetLoadOption"": ""batch"", ""numPorts"": 1}'"
05_convolutional_net.py,0,"b""from keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Convolution2D, MaxPooling2D, Dropout\nfrom keras.optimizers import RMSprop\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\nfrom keras import initializations\nfrom keras import backend as K\n\nbatch_size = 128\nnb_classes = 10\nnb_epoch = 100\n\nimg_rows, img_cols = 28, 28         # input image dimensions\npool_size = (2, 2)                  # size of pooling area for max pooling\nprob_drop_conv = 0.2                # drop probability for dropout @ conv layer\nprob_drop_hidden = 0.5              # drop probability for dropout @ fc layer\n\ndef init_weights(shape, name=None):\n    return initializations.normal(shape, scale=0.01, name=name)\n\n# Load MNIST dataset\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nprint('X_train original shape:', X_train.shape)\n\nif K.image_dim_ordering() == 'th':\n    # For Theano backend\n    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    # For TensorFlow backend\n    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\n\nX_train = X_train.astype('float32') / 255.\nX_test = X_test.astype('float32') / 255.\nY_train = np_utils.to_categorical(y_train, nb_classes)\nY_test = np_utils.to_categorical(y_test, nb_classes)\n\nprint('X_train shape:', X_train.shape)\nprint(X_train.shape[0], 'train samples')\nprint(X_test.shape[0], 'test samples')\n\n\n# Convolutional model\nmodel = Sequential()\n\n# conv1 layer\nmodel.add(Convolution2D(32, 3, 3, border_mode='same', activation='relu', input_shape=input_shape, init=init_weights))\nmodel.add(MaxPooling2D(pool_size=pool_size, strides=(2,2), border_mode='same'))\nmodel.add(Dropout(prob_drop_conv))\n\n# conv2 layer\nmodel.add(Convolution2D(64, 3, 3, border_mode='same', activation='relu', init=init_weights))\nmodel.add(MaxPooling2D(pool_size=pool_size, strides=(2,2), border_mode='same'))\nmodel.add(Dropout(prob_drop_conv))\n\n# conv3 layer\nmodel.add(Convolution2D(128, 3, 3, border_mode='same', activation='relu', init=init_weights))\nmodel.add(MaxPooling2D(pool_size=pool_size, strides=(2,2), border_mode='same'))\nmodel.add(Flatten())\nmodel.add(Dropout(prob_drop_conv))\n\n# fc1 layer\nmodel.add(Dense(625, activation='relu', init=init_weights))\nmodel.add(Dropout(prob_drop_hidden))\n\n# fc2 layer\nmodel.add(Dense(10, activation='softmax', init=init_weights))\n\nopt = RMSprop(lr=0.001, rho=0.9)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n\n# Train\nhistory = model.fit(X_train, Y_train, nb_epoch=nb_epoch, batch_size=batch_size, shuffle=True, verbose=1)\n\n# Evaluate\nevaluation = model.evaluate(X_test, Y_test, batch_size=256, verbose=1)\nprint('Summary: Loss over the test dataset: %.2f, Accuracy: %.2f' % (evaluation[0], evaluation[1]))"""
06_autoencoder.py,0,"b""from keras.models import Model\nfrom keras.layers import Dense, Input\nfrom keras.datasets import mnist\nimport numpy as np\n\n# Hyper parameters\nbatch_size = 128\nnb_epoch = 100\n\n# Parameters for MNIST dataset\nimg_rows, img_cols = 28, 28\n\n# Parameters for denoising autoencoder\nnb_visible = img_rows * img_cols\nnb_hidden = 500\ncorruption_level = 0.3\n\n# Load MNIST Dataset\n(x_train, _), (x_test, _) = mnist.load_data()\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\nprint(x_train.shape)\nprint(x_test.shape)\n\n# Add random noise\nx_train_noisy = x_train + corruption_level * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\nx_test_noisy = x_test + corruption_level * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\nx_test_noisy = np.clip(x_test_noisy, 0., 1.)\nprint(x_train_noisy.shape)\nprint(x_test_noisy.shape)\n\n# Build autoencoder model\ninput_img = Input(shape=(nb_visible,))\nencoded = Dense(nb_hidden, activation='relu')(input_img)\ndecoded = Dense(nb_visible, activation='sigmoid')(encoded)\n\nautoencoder = Model(input=input_img, output=decoded)\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\nautoencoder.summary()\n\n# Train\nautoencoder.fit(x_train_noisy, x_train,\n                nb_epoch=nb_epoch, batch_size=batch_size, shuffle=True, verbose=1,\n                validation_data=(x_test_noisy, x_test))\n\n# Evaluate\nevaluation = autoencoder.evaluate(x_test_noisy, x_test, batch_size=batch_size, verbose=1)\nprint('\\nSummary: Loss over the test dataset: %.2f' % evaluation)"""
07_lstm.py,0,"b""from keras.models import Sequential\nfrom keras.layers import LSTM, Dense\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\nfrom keras import initializations\n\ndef init_weights(shape, name=None):\n    return initializations.normal(shape, scale=0.01, name=name)\n\n# Hyper parameters\nbatch_size = 128\nnb_epoch = 50\n\n# Parameters for MNIST dataset\nimg_rows, img_cols = 28, 28\nnb_classes = 10\n\n# Parameters for LSTM network\nnb_lstm_outputs = 30\nnb_time_steps = img_rows\ndim_input_vector = img_cols\n\n\n# Load MNIST dataset\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nprint('X_train original shape:', X_train.shape)\ninput_shape = (nb_time_steps, dim_input_vector)\n\nX_train = X_train.astype('float32') / 255.\nX_test = X_test.astype('float32') / 255.\nY_train = np_utils.to_categorical(y_train, nb_classes)\nY_test = np_utils.to_categorical(y_test, nb_classes)\n\nprint('X_train shape:', X_train.shape)\nprint(X_train.shape[0], 'train samples')\nprint(X_test.shape[0], 'test samples')\n\n# Build LSTM network\nmodel = Sequential()\nmodel.add(LSTM(nb_lstm_outputs, input_shape=input_shape))\nmodel.add(Dense(nb_classes, activation='softmax', init=init_weights))\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n\n# Train\nhistory = model.fit(X_train, Y_train, nb_epoch=nb_epoch, batch_size=batch_size, shuffle=True, verbose=1)\n\n# Evaluate\nevaluation = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=1)\nprint('Summary: Loss over the test dataset: %.2f, Accuracy: %.2f' % (evaluation[0], evaluation[1]))"""
09_tensorboard.py,0,"b""from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import RMSprop\nfrom keras.datasets import mnist\nfrom keras import initializations\nfrom keras.utils import np_utils\nfrom keras.callbacks import TensorBoard\n\n# Hyper parameters\nbatch_size = 128\nnb_epoch = 100\n\n# Parameters for MNIST dataset\nnb_classes = 10\n\n# Parameters for MLP\nprob_drop_input = 0.2               # drop probability for dropout @ input layer\nprob_drop_hidden = 0.5              # drop probability for dropout @ fc layer\n\n\ndef init_weights(shape, name=None):\n    return initializations.normal(shape, scale=0.01, name=name)\n\n# Load MNIST dataset\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = X_train.reshape(60000, 784)\nX_test = X_test.reshape(10000, 784)\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train /= 255\nX_test /= 255\nY_Train = np_utils.to_categorical(y_train, nb_classes)\nY_Test = np_utils.to_categorical(y_test, nb_classes)\n\n# Multilayer Perceptron model\nmodel = Sequential()\nmodel.add(Dense(output_dim=625, input_dim=784, init=init_weights, activation='sigmoid', name='dense1'))\nmodel.add(Dropout(prob_drop_input, name='dropout1'))\nmodel.add(Dense(output_dim=625, input_dim=625, init=init_weights, activation='sigmoid', name='dense2'))\nmodel.add(Dropout(prob_drop_hidden, name='dropout2'))\nmodel.add(Dense(output_dim=10, input_dim=625, init=init_weights, activation='softmax', name='dense3'))\nmodel.compile(optimizer=RMSprop(lr=0.001, rho=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n\n# Train\nhistory = model.fit(X_train, Y_Train, nb_epoch=nb_epoch, batch_size=batch_size, verbose=1,\n                    callbacks=[TensorBoard(log_dir='./logs/09_tensorboard', histogram_freq=1)])\n\n# Evaluate\nevaluation = model.evaluate(X_test, Y_Test, verbose=1)\nprint('Summary: Loss over the test dataset: %.2f, Accuracy: %.2f' % (evaluation[0], evaluation[1]))"""
10_save_restore_net.py,0,"b""from keras.models import Sequential, save_model, load_model\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import RMSprop\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\nfrom keras.callbacks import ModelCheckpoint\n\n# Hyper parameters\nbatch_size = 128\nnb_epoch = 10\n\n# Parameters for MNIST dataset\nnb_classes = 10\n\n# Parameters for MLP\nprob_drop_input = 0.2               # drop probability for dropout @ input layer\nprob_drop_hidden = 0.5              # drop probability for dropout @ fc layer\n\n\n# Load MNIST dataset\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = X_train.reshape(60000, 784)\nX_test = X_test.reshape(10000, 784)\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train /= 255\nX_test /= 255\nY_Train = np_utils.to_categorical(y_train, nb_classes)\nY_Test = np_utils.to_categorical(y_test, nb_classes)\n\n# Multilayer Perceptron model\nmodel = Sequential()\nmodel.add(Dense(output_dim=625, input_dim=784, init='normal', activation='sigmoid', name='dense1'))\nmodel.add(Dropout(prob_drop_input, name='dropout1'))\nmodel.add(Dense(output_dim=625, input_dim=625, init='normal', activation='sigmoid', name='dense2'))\nmodel.add(Dropout(prob_drop_hidden, name='dropout2'))\nmodel.add(Dense(output_dim=10, input_dim=625, init='normal', activation='softmax', name='dense3'))\nmodel.compile(optimizer=RMSprop(lr=0.001, rho=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n\n# Train\nsave_model(model, './logs/model_mlp')\ncheckpoint = ModelCheckpoint(filepath='./logs/weights.epoch.{epoch:02d}-val_loss.{val_loss:.2f}.hdf5', verbose=0)\nhistory = model.fit(X_train, Y_Train, nb_epoch=nb_epoch, batch_size=batch_size, verbose=1,\n                    callbacks=[checkpoint], validation_data=(X_test, Y_Test))\n\n# Evaluate\nevaluation = model.evaluate(X_test, Y_Test, verbose=1)\nprint('\\nSummary: Loss over the test dataset: %.2f, Accuracy: %.2f' % (evaluation[0], evaluation[1]))\n\n# Restore trained model\nloaded_model = load_model('./logs/model_mlp')\nloaded_model.load_weights('./logs/weights.epoch.09-val_loss.0.08.hdf5')\nloaded_model.summary()\n\n# Evaluate with loaded model\nevaluation = loaded_model.evaluate(X_test, Y_Test, verbose=1)\nprint('\\nSummary: Loss over the test dataset: %.2f, Accuracy: %.2f' % (evaluation[0], evaluation[1]))"""
