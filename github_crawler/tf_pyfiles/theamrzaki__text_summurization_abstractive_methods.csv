file_path,api_count,code
Hindi/2_process (local).py,0,"b'#pip install langdetect\nimport json\nimport os\nimport pandas\nfrom langdetect import detect\nimport pickle\n\nfrom __future__ import print_function\nimport io\nimport sys\nimport re\n\nclass ProgressBar(object):\n    DEFAULT = \'Progress: %(bar)s %(percent)3d%%\'\n    FULL = \'%(bar)s %(current)d/%(total)d (%(percent)3d%%) %(remaining)d to go\'\n\n    def __init__(self, total, width=40, fmt=DEFAULT, symbol=\'=\',\n                 output=sys.stderr):\n        assert len(symbol) == 1\n\n        self.total = total\n        self.width = width\n        self.symbol = symbol\n        self.output = output\n        self.fmt = re.sub(r\'(?P<name>%\\(.+?\\))d\',\n            r\'\\g<name>%dd\' % len(str(total)), fmt)\n\n        self.current = 0\n\n    def __call__(self):\n        percent = self.current / float(self.total)\n        size = int(self.width * percent)\n        remaining = self.total - self.current\n        bar = \'[\' + self.symbol * size + \' \' * (self.width - size) + \']\'\n\n        args = {\n            \'total\': self.total,\n            \'bar\': bar,\n            \'current\': self.current,\n            \'percent\': percent * 100,\n            \'remaining\': remaining\n        }\n        print(\'\\r\' + self.fmt % args, file=self.output, end=\'\')\n\n    def done(self):\n        self.current = self.total\n        self()\n        print(\'\', file=self.output)\n\n\n\nfull_path = r""E:\\Projects\\Python\\Hindi_News\\data\\data""\ndirs = os.listdir(full_path)\nresultdict = []\n\n#with open(\'hindi.pkl\', \'rb\') as handle:\n#\tb = pickle.load(handle)\n\nfor d in dirs:\n\tlist_files = [file_json for file_json in os.listdir(full_path+""\\\\""+d) if file_json.endswith(\'.json\')]\n\tif len(list_files)==0:\n\t\tcontinue\n\tprogress = ProgressBar(len(list_files), fmt=ProgressBar.FULL)\n\tfor fili in list_files:\n\t\twith open(os.path.join(full_path+""\\\\""+d, fili), encoding=""utf-8"") as inputjson:\n\t\t\tobjj = json.load(inputjson) \n\t\t\trow = {}\n\t\t\trow[""title""] = objj[""title""]\n\t\t\trow[""text""] = objj[""text""]\n\t\t\ttry:\n\t\t\t\tif detect(objj[""text""]) == ""hi"" and objj[\'text\'] != """":\n\t\t\t\t\tresultdict.append( row )\n\t\t\texcept:\n\t\t\t\tresultdict.append( row )\n\t\t\tprogress.current += 1\n\t\t\tprogress()\n\tprogress.done()\n\nwith open(\'hindi.pkl\', \'wb\') as handle:\n\tpickle.dump(resultdict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\nprint(""converting to dataframe .."")\ndataframe = pandas.DataFrame(resultdict)\n\nprint(""saving .."")\nwith open(""HindiNewsBook.csv"", ""a"" ,encoding=\'utf-8\') as csvout:\n\tdataframe.to_csv(csvout, encoding=\'utf-8\', index=False)\n\nprint(""done el7"")\t\n'"
Implementation B (Pointer Generator seq2seq network)/PreProcessData/process_English.py,1,"b'\xef\xbb\xbfimport ProgressBar\n\nimport sys\nimport os\nimport hashlib\nimport struct\nimport subprocess\nimport collections\nimport tensorflow as tf\nfrom tensorflow.core.example import example_pb2\nimport nltk\nimport pandas as pd\n\n#for cleaning text \ndef clean_text(text, remove_stopwords = True):\n    \'\'\'Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings\'\'\'\n    \n    # Convert words to lower case\n    text = text.lower()\n    \n    # Replace contractions with their longer forms \n    if True:\n        text = text.split()\n        new_text = []\n        for word in text:\n            if word in contractions:\n                new_text.append(contractions[word])\n            else:\n                new_text.append(word)\n        text = "" "".join(new_text)\n    \n    # Format words and remove unwanted characters\n    text = re.sub(r\'https?:\\/\\/.*[\\r\\n]*\', \'\', text, flags=re.MULTILINE)\n    text = re.sub(r\'\\<a href\', \' \', text)\n    text = re.sub(r\'&amp;\', \'\', text) \n    text = re.sub(r\'[_""\\-;%()|+&=*%.,!?:#$@\\[\\]/]\', \' \', text)\n    text = re.sub(r\'<br />\', \' \', text)\n    text = re.sub(r\'\\\'\', \' \', text)\n    \n    # Optionally, remove stop words\n    if remove_stopwords:\n        text = text.split()\n        stops = set(stopwords.words(""english""))\n        text = [w for w in text if not w in stops]\n        text = "" "".join(text)\n\n    return text\n\n\ndm_single_close_quote = u\'\\u2019\' # unicode\ndm_double_close_quote = u\'\\u201d\'\nEND_TOKENS = [\'.\', \'!\', \'?\', \'...\', ""\'"", ""`"", \'""\', dm_single_close_quote, dm_double_close_quote, "")""] # acceptable ways to end a sentence\n\n# We use these to separate the summary sentences in the .bin datafiles\nSENTENCE_START = \'<s>\'\nSENTENCE_END = \'</s>\'\n\nall_train_urls = """"\nall_val_urls = """"\nall_test_urls = """"\n\ncnn_tokenized_stories_dir = ""cnn_stories_tokenized"" #location of folder to tokenize text\ndm_tokenized_stories_dir = ""dm_stories_tokenized"" #not used\nfinished_files_dir = ""arabic_finished_files"" #final ouput\nchunks_dir = os.path.join(finished_files_dir, ""chunked"")\n\n\n\nVOCAB_SIZE = 200000\nCHUNK_SIZE = 1000 # num examples per chunk, for the chunked data\n\n\ndef chunk_file(set_name):\n  in_file = finished_files_dir + \'/%s.bin\' % set_name\n  reader = open(in_file, ""rb"")\n  chunk = 0\n  finished = False\n  while not finished:\n    chunk_fname = os.path.join(chunks_dir, \'%s_%03d.bin\' % (set_name, chunk)) # new chunk\n    with open(chunk_fname, \'wb\') as writer:\n      for _ in range(CHUNK_SIZE):\n        len_bytes = reader.read(8)\n        if not len_bytes:\n          finished = True\n          break\n        str_len = struct.unpack(\'q\', len_bytes)[0]\n        example_str = struct.unpack(\'%ds\' % str_len, reader.read(str_len))[0]\n        writer.write(struct.pack(\'q\', str_len))\n        writer.write(struct.pack(\'%ds\' % str_len, example_str))\n      chunk += 1\n\n\ndef chunk_all():\n  # Make a dir to hold the chunks\n  if not os.path.isdir(chunks_dir):\n    os.mkdir(chunks_dir)\n  # Chunk the data\n  for set_name in [\'train\', \'val\', \'test\']:\n    print (""Splitting %s data into chunks..."" % set_name)\n    chunk_file(set_name)\n  print (""Saved chunked data in %s"" % chunks_dir)\n\n\ndef tokenize_stories(reviews, tokenized_stories_dir):\n  """"""Maps a whole directory of .story files to a tokenized version using Stanford CoreNLP Tokenizer""""""\n  progress = ProgressBar.ProgressBar(len(reviews), fmt=ProgressBar.ProgressBar.FULL)\n\n  for i, row in reviews.iterrows():\n        #if i==20:\n        #    break\n        filename = str(i) + \'.tok\'\n        with open(os.path.join(tokenized_stories_dir, filename), \'w\', encoding=""utf-8"") as temp_file:\n            text = row[""content""]\n            text = clean_text(text , remove_stopwords = True)\n            tok = nltk.word_tokenize(text)\n            tok.append(""@highlight"")\n            Summary = row[""title""]\n            Summary = clean_text(Summary ,remove_stopwords = False)\n            tok.extend(nltk.word_tokenize(Summary))\n            list = tok.copy()\n\n            for i in tok:\n                if(i==\'``\' or i==""\'\'"" ):\n                    list.remove(i)\n            tok_string = ""\\n"".join(str(x) for x in list)\n            temp_file.write(tok_string)\n\n        progress.current += 1\n        progress()\n  print (""Successfully finished tokenizing to %s .\\n"" % (tokenized_stories_dir))\n\n\ndef fix_missing_period(line):\n  """"""Adds a period to a line that is missing a period""""""\n  if ""@highlight"" in line: return line\n  if line=="""": return line\n  if line[-1] in END_TOKENS: return line\n  # print line[-1]\n  return line + "" .""\n\ndef read_text_file(text_file):\n  lines = []\n  with open(text_file, ""r"", encoding=""utf-8"") as f:\n    for line in f:\n      lines.append(line.strip())\n  return lines\n\ndef get_art_abs(story_file):\n  lines = read_text_file(story_file)\n\n  # Lowercase everything\n  lines = [line.lower() for line in lines]\n\n  # Put periods on the ends of lines that are missing them (this is a problem in the dataset because many image captions don\'t end in periods; consequently they end up in the body of the article as run-on sentences)\n  lines = [fix_missing_period(line) for line in lines]\n\n  # Separate out article and abstract sentences\n  article_lines = []\n  highlights = []\n  next_is_highlight = False\n  for idx,line in enumerate(lines):\n    if line == """":\n      continue # empty line\n    elif line.startswith(""@highlight""):\n      next_is_highlight = True\n    elif next_is_highlight:\n      highlights.append(line)\n    else:\n      article_lines.append(line)\n\n  # Make article into a single string\n  article = \' \'.join(article_lines)\n\n  # Make abstract into a signle string, putting <s> and </s> tags around the sentences\n  abstract = \' \'.join([""%s %s %s"" % (SENTENCE_START, sent, SENTENCE_END) for sent in highlights])\n\n  return article, abstract\n\n\ndef write_to_bin(file_names, out_file, makevocab=False):\n  """"""Reads the tokenized .story files corresponding to the urls listed in the url_file and writes them to a out_file.""""""\n \n  story_fnames = [str(s)+"".tok"" for s in file_names]\n  num_stories = len(story_fnames)\n\n  if makevocab:\n    vocab_counter = collections.Counter()\n\n  with open(out_file, \'wb\') as writer:\n    for idx,s in enumerate(story_fnames):\n      if idx % 1000 == 0:\n        print( ""Writing story %i of %i; %.2f percent done"" % (idx, num_stories, float(idx)*100.0/float(num_stories)))\n\n      # Look in the tokenized story dirs to find the .story file corresponding to this url\n      if os.path.isfile(os.path.join(cnn_tokenized_stories_dir, s)):\n        story_file = os.path.join(cnn_tokenized_stories_dir, s)\n      elif os.path.isfile(os.path.join(dm_tokenized_stories_dir, s)):\n        story_file = os.path.join(dm_tokenized_stories_dir, s)\n      else:\n        print (""Error: Couldn\'t find tokenized story file %s in either tokenized story directories %s and %s. Was there an error during tokenization?"" % (s, cnn_tokenized_stories_dir, dm_tokenized_stories_dir))\n        # Check again if tokenized stories directories contain correct number of files\n        print (""Checking that the tokenized stories directories %s and %s contain correct number of files..."" % (cnn_tokenized_stories_dir, dm_tokenized_stories_dir))\n        #check_num_stories(cnn_tokenized_stories_dir, num_expected_cnn_stories)\n        #check_num_stories(dm_tokenized_stories_dir, num_expected_dm_stories)\n        #raise Exception(""Tokenized stories directories %s and %s contain correct number of files but story file %s found in neither."" % (cnn_tokenized_stories_dir, dm_tokenized_stories_dir, s))\n        \n      # Get the strings to write to .bin file\n      article, abstract = get_art_abs(story_file)\n\n      \n      # Write to tf.Example\n      tf_example = example_pb2.Example()\n      tf_example.features.feature[\'article\'].bytes_list.value.extend([article.encode(\'utf-8\')])\n      tf_example.features.feature[\'abstract\'].bytes_list.value.extend([abstract.encode(\'utf-8\')])\n      tf_example_str = tf_example.SerializeToString()\n      str_len = len(tf_example_str)\n      writer.write(struct.pack(\'q\', str_len))\n      writer.write(struct.pack(\'%ds\' % str_len, tf_example_str))\n   \n\n      # Write the vocab to file, if applicable\n      if makevocab:\n        art_tokens = article.split(\' \')\n        abs_tokens = abstract.split(\' \')\n        abs_tokens = [t for t in abs_tokens if t not in [SENTENCE_START, SENTENCE_END]] # remove these tags from vocab\n        tokens = art_tokens + abs_tokens\n        tokens = [t.strip() for t in tokens] # strip\n        tokens = [t for t in tokens if t!=""""] # remove empty\n        vocab_counter.update(tokens)\n\n  print (""Finished writing file %s\\n"" % out_file)\n\n  # write vocab to file\n  if makevocab:\n    print (""Writing vocab file..."")\n    with open(os.path.join(finished_files_dir, ""vocab""), \'w\', encoding=""utf-8"") as writer:\n      for word, count in vocab_counter.most_common(VOCAB_SIZE):\n        writer.write(word + \' \' + str(count) + \'\\n\')\n    print (""Finished writing vocab file"")\n\n\ndef check_num_stories(stories_dir, num_expected):\n  num_stories = len(os.listdir(stories_dir))\n  if num_stories != num_expected:\n    raise Exception(""stories directory %s contains %i files but should contain %i"" % (stories_dir, num_stories, num_expected))\n\n\n\n\n\n""""""\nthe requirements are , having\n    1- csv of your data set having 2 columbs \n        content(text) |  summary \n        by modifying \n        cnn_stories_dir to pointtto your main directory \n        and then replacing \\ArabicBook00.csv\n        with your csv\n\n\noutput would be \n    1- folder (cnn_stories_tokenized) used internally here \n    2- finished files (the folder that we would use)\n        |--> (folder) chunks ==> (used in upload)\n        |--> test.bin  |\n        |--> train.bin |--> not used in upload\n        |--> val.bin   |\n        |--> vocab  ==> (used in upload)\n\n    then \n    put both \n      |--> (folder) chunks ==> (used in upload)\n      |--> vocab  ==> (used in upload)\n      in a zip and upload online\n""""""\n\n\n\nif __name__ == \'__main__\':\n  #main directory\n  cnn_stories_dir =  r""E:\\Handasa\\Majester\\thesis\\python\\DataProcessing\\DataProcessing\\arhelpers""\n\n  # Create some new directories\n  if not os.path.exists(cnn_tokenized_stories_dir): os.makedirs(cnn_tokenized_stories_dir)\n  if not os.path.exists(finished_files_dir): os.makedirs(finished_files_dir)\n\n  #data needed is in a csv format\n  #containg 2 columbs (content , title)\n  reviews_csv =cnn_stories_dir + ""\\ArabicBook00.csv""\n  reviews = pd.read_csv(reviews_csv)\n  reviews = reviews.filter([\'content\', \'title\'])\n  reviews = reviews.dropna()\n  reviews = reviews.reset_index(drop=True)\n  reviews.head()\n\n  # Run nltk tokenizer on both text and summary , outputting to tokenized stories directories\n  tokenize_stories(reviews, cnn_tokenized_stories_dir)\n\n  #to get the length of your dataset\n  num_expected_cnn_stories =reviews.shape[0]\n\n  #testing len = 2000\n  #validation lenght = 2000\n  all_train_urls = range(0,num_expected_cnn_stories-2000)\n  all_val_urls = range(num_expected_cnn_stories-2000,num_expected_cnn_stories-1000)\n  all_test_urls = range(num_expected_cnn_stories-1000,num_expected_cnn_stories)\n\n  #for testing\n  ##############all_train_urls= range(0,80)\n  ##############all_val_urls = range(80,90)\n  ##############all_test_urls = range(90,100)\n\n  # Read the tokenized stories, do a little postprocessing then write to bin files\n  write_to_bin(all_test_urls, os.path.join(finished_files_dir, ""test.bin""))\n  write_to_bin(all_val_urls, os.path.join(finished_files_dir, ""val.bin""))\n  write_to_bin(all_train_urls, os.path.join(finished_files_dir, ""train.bin""), makevocab=True)\n\n  # Chunk the data. This splits each of train.bin, val.bin and test.bin into smaller chunks, each containing e.g. 1000 examples, and saves them in finished_files/chunks\n  chunk_all()\n\n'"
