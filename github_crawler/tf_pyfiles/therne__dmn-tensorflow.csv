file_path,api_count,code
main.py,4,"b""#!/usr/bin/env python3\nimport os\n\nimport tensorflow as tf\n\nfrom read_data import read_babi, get_max_sizes\nfrom utils.data_utils import load_glove, WordTable\n\nflags = tf.app.flags\n\n# directories\nflags.DEFINE_string('model', 'dmn+', 'Model type - dmn+, dmn, dmn_embed [Default: DMN+]')\nflags.DEFINE_boolean('test', False, 'true for testing, false for training [False]')\nflags.DEFINE_string('data_dir', 'data/tasks_1-20_v1-2/en-10k', 'Data directory [data/tasks_1-20_v1-2/en-10k]')\nflags.DEFINE_string('save_dir', 'save', 'Save path [save]')\n\n# training options\nflags.DEFINE_bool('gpu', True, 'Use GPU? [True]')\nflags.DEFINE_integer('batch_size', 128, 'Batch size during training and testing [128]')\nflags.DEFINE_integer('num_epochs', 256, 'Number of epochs for training [256]')\nflags.DEFINE_float('learning_rate', 0.002, 'Learning rate [0.002]')\nflags.DEFINE_boolean('load', False, 'Start training from saved model? [False]')\nflags.DEFINE_integer('acc_period', 10, 'Training accuracy display period [10]')\nflags.DEFINE_integer('val_period', 40, 'Validation period (for display purpose) [40]')\nflags.DEFINE_integer('save_period', 80, 'Save period [80]')\n\n# model params\nflags.DEFINE_integer('memory_step', 3, 'Episodic Memory steps [3]')\nflags.DEFINE_string('memory_update', 'relu', 'Episodic meory update method - relu or gru [relu]')\n# flags.DEFINE_bool('memory_tied', False, 'Share memory update weights among the layers? [False]')\nflags.DEFINE_integer('glove_size', 50, 'GloVe size - Only used in dmn [50]')\nflags.DEFINE_integer('embed_size', 80, 'Word embedding size - Used in dmn+, dmn_embed [80]')\nflags.DEFINE_integer('hidden_size', 80, 'Size of hidden units [80]')\n\n# train hyperparameters\nflags.DEFINE_float('weight_decay', 0.001, 'Weight decay - 0 to turn off L2 regularization [0.001]')\nflags.DEFINE_float('keep_prob', 1., 'Dropout rate - 1.0 to turn off [1.0]')\nflags.DEFINE_bool('batch_norm', True, 'Use batch normalization? [True]')\n\n# bAbi dataset params\nflags.DEFINE_integer('task', 1, 'bAbi Task number [1]')\nflags.DEFINE_float('val_ratio', 0.1, 'Validation data ratio to training data [0.1]')\n\nFLAGS = flags.FLAGS\n\n\ndef main(_):\n    if FLAGS.model == 'dmn':\n        word2vec = load_glove(FLAGS.glove_size)\n        words = WordTable(word2vec, FLAGS.glove_size)\n        from models.old.dmn import DMN\n\n    elif FLAGS.model == 'dmn+':\n        words = WordTable()\n        from models.new.dmn_plus import DMN\n\n    elif FLAGS.model == 'dmn_embed':\n        words = WordTable()\n        from models.old.dmn_embedding import DMN\n    else:\n        print('Unknown model type: %s' % FLAGS.model)\n        return\n\n    # Read data\n    train = read_babi(FLAGS.data_dir, FLAGS.task, 'train', FLAGS.batch_size, words)\n    test = read_babi(FLAGS.data_dir, FLAGS.task, 'test', FLAGS.batch_size, words)\n    val = train.split_dataset(FLAGS.val_ratio)\n\n    FLAGS.max_sent_size, FLAGS.max_ques_size, FLAGS.max_fact_count = get_max_sizes(train, test, val)\n    print('Word count: %d, Max sentence len : %d' % (words.vocab_size, FLAGS.max_sent_size))\n\n    # Modify save dir\n    FLAGS.save_dir += '/task_%d/' % FLAGS.task\n    if not os.path.exists(FLAGS.save_dir):\n        os.makedirs(FLAGS.save_dir, exist_ok=True)\n\n    with tf.Session() as sess:\n        model = DMN(FLAGS, words)\n        sess.run(tf.initialize_all_variables())\n\n        if FLAGS.test:\n            model.load(sess)\n            model.eval(sess, test, name='Test')\n        else:\n            if FLAGS.load: model.load(sess)\n            model.train(sess, train, val)\n\nif __name__ == '__main__':\n    tf.app.run()\n"""
read_data.py,0,"b'"""""" a neat code from https://github.com/YerevaNN/Dynamic-memory-networks-in-Theano/ """"""\nimport os\n\nfrom utils.data_utils import DataSet\nfrom copy import deepcopy\n\n\ndef load_babi(data_dir, task_id, type=\'train\'):\n    """""" Load bAbi Dataset.\n    :param data_dir\n    :param task_id: bAbI Task ID\n    :param type: ""train"" or ""test""\n    :return: dict\n    """"""\n    files = os.listdir(data_dir)\n    files = [os.path.join(data_dir, f) for f in files]\n    s = \'qa{}_\'.format(task_id)\n    file_name = [f for f in files if s in f and type in f][0]\n\n    # Parsing\n    tasks = []\n    skip = False\n    curr_task = None\n    for i, line in enumerate(open(file_name)):\n        id = int(line[0:line.find(\' \')])\n        if id == 1:\n            skip = False\n            curr_task = {""C"": [], ""Q"": """", ""A"": """"}\n\n        # Filter tasks that are too large\n        if skip: continue\n        if task_id == 3 and id > 130:\n            skip = True\n            continue\n\n        elif task_id != 3 and id > 70:\n            skip = True\n            continue\n\n        line = line.strip()\n        line = line.replace(\'.\', \' . \')\n        line = line[line.find(\' \') + 1:]\n        if line.find(\'?\') == -1:\n            curr_task[""C""].append(line)\n        else:\n            idx = line.find(\'?\')\n            tmp = line[idx + 1:].split(\'\\t\')\n            curr_task[""Q""] = line[:idx]\n            curr_task[""A""] = tmp[1].strip()\n            tasks.append(deepcopy(curr_task))\n\n    print(""Loaded {} data from bAbI {} task {}"".format(len(tasks), type, task_id))\n    return tasks\n\n\ndef process_babi(raw, word_table):\n    """""" Tokenizes sentences.\n    :param raw: dict returned from load_babi\n    :param word_table: WordTable\n    :return:\n    """"""\n    questions = []\n    inputs = []\n    answers = []\n    fact_counts = []\n\n    for x in raw:\n        inp = []\n        for fact in x[""C""]:\n            sent = [w for w in fact.lower().split(\' \') if len(w) > 0]\n            inp.append(sent)\n            word_table.add_vocab(*sent)\n\n        q = [w for w in x[""Q""].lower().split(\' \') if len(w) > 0]\n\n        word_table.add_vocab(*q, x[""A""])\n\n        inputs.append(inp)\n        questions.append(q)\n        answers.append(x[""A""])  # NOTE: here we assume the answer is one word!\n        fact_counts.append(len(inp))\n\n    return inputs, questions, answers, fact_counts\n\n\ndef read_babi(data_dir, task_id, type, batch_size, word_table):\n    """""" Reads bAbi data set.\n    :param data_dir: bAbi data directory\n    :param task_id: task no. (int)\n    :param type: \'train\' or \'test\'\n    :param batch_size: how many examples in a minibatch?\n    :param word_table: WordTable\n    :return: DataSet\n    """"""\n    data = load_babi(data_dir, task_id, type)\n    x, q, y, fc = process_babi(data, word_table)\n    return DataSet(batch_size, x, q, y, fc, name=type)\n\n\ndef get_max_sizes(*data_sets):\n    max_sent_size = max_ques_size = max_fact_count = 0\n    for data in data_sets:\n        for x, q, fc in zip(data.xs, data.qs, data.fact_counts):\n            for fact in x: max_sent_size = max(max_sent_size, len(fact))\n            max_ques_size = max(max_ques_size, len(q))\n            max_fact_count = max(max_fact_count, fc)\n\n    return max_sent_size, max_ques_size, max_fact_count\n'"
models/__init__.py,0,b''
models/base_model.py,4,"b'import sys\n\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import tqdm\n\n\nclass BaseModel(object):\n    """""" Code from mem2nn-tensorflow. """"""\n    def __init__(self, params, words):\n        self.params = params\n        self.words = words\n        self.save_dir = params.save_dir\n\n        with tf.variable_scope(\'DMN\'):\n            print(""Building DMN..."")\n            self.global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n            self.build()\n            self.saver = tf.train.Saver()\n\n    def build(self):\n        raise NotImplementedError()\n\n    def get_feed_dict(self, batch, is_train):\n        raise NotImplementedError()\n\n    def train_batch(self, sess, batch):\n        feed_dict = self.get_feed_dict(batch, is_train=True)\n        return sess.run([self.opt_op, self.global_step], feed_dict=feed_dict)\n\n    def test_batch(self, sess, batch):\n        feed_dict = self.get_feed_dict(batch, is_train=False)\n        return sess.run([self.num_corrects, self.total_loss, self.global_step], feed_dict=feed_dict)\n\n    def train(self, sess, train_data, val_data):\n        params = self.params\n        num_epochs = params.num_epochs\n        num_batches = train_data.num_batches\n\n        print(""Training %d epochs ..."" % num_epochs)\n        for epoch_no in tqdm(range(num_epochs), desc=\'Epoch\', maxinterval=86400, ncols=100):\n            for _ in range(num_batches):\n                batch = train_data.next_batch()\n                _, global_step = self.train_batch(sess, batch)\n\n            train_data.reset()\n\n            if (epoch_no + 1) % params.acc_period == 0:\n                print()  # Newline for TQDM\n                self.eval(sess, train_data, name=\'Training\')\n\n            if val_data and (epoch_no + 1) % params.val_period == 0:\n                self.eval(sess, val_data, name=\'Validation\')\n\n            if (epoch_no + 1) % params.save_period == 0:\n                self.save(sess)\n\n        print(""Training completed."")\n\n    def eval(self, sess, data, name):\n        num_batches = data.num_batches\n        num_corrects = total = 0\n        losses = []\n        for _ in range(num_batches):\n            batch = data.next_batch()\n            cur_num_corrects, cur_loss, global_step = self.test_batch(sess, batch)\n            num_corrects += cur_num_corrects\n            total += len(batch[0])\n            losses.append(cur_loss)\n        data.reset()\n        loss = np.mean(losses)\n\n        print(""[%s] step %d: Accuracy = %.2f%% (%d / %d), Loss = %.4f"" % \\\n              (name, global_step, 100 * float(num_corrects) / total, num_corrects, total, loss))\n        return loss\n\n    def save(self, sess):\n        print(""Saving model to %s"" % self.save_dir)\n        self.saver.save(sess, self.save_dir, self.global_step)\n\n    def load(self, sess):\n        print(""Loading model ..."")\n        checkpoint = tf.train.get_checkpoint_state(self.save_dir)\n        if checkpoint is None:\n            print(""Error: No saved model found. Please train first."")\n            sys.exit(0)\n        self.saver.restore(sess, checkpoint.model_checkpoint_path)\n'"
utils/__init__.py,0,b''
utils/attn_gru.py,9,"b'import tensorflow as tf\nfrom tensorflow.python.ops.nn import tanh\nfrom utils.nn import weight, bias, batch_norm\n\n\nclass AttnGRU:\n    """"""Attention-based Gated Recurrent Unit cell (cf. https://arxiv.org/abs/1603.01417).""""""\n\n    def __init__(self, num_units, is_training, bn):\n        self._num_units = num_units\n        self.is_training = is_training\n        self.batch_norm = bn\n\n    def __call__(self, inputs, state, attention, scope=None):\n        """"""Gated recurrent unit (GRU) with nunits cells.""""""\n        with tf.variable_scope(scope or \'AttrGRU\'):\n            with tf.variable_scope(""Gates""):  # Reset gate and update gate.\n                # We start with bias of 1.0 to not reset.\n                r = tf.nn.sigmoid(self._linear(inputs, state, bias_default=1.0))\n            with tf.variable_scope(""Candidate""):\n                c = tanh(self._linear(inputs, r * state))\n\n            new_h = attention * c + (1 - attention) * state\n        return new_h\n\n    def _linear(self, x, h, bias_default=0.0):\n        I, D = x.get_shape().as_list()[1], self._num_units\n        w = weight(\'W\', [I, D])\n        u = weight(\'U\', [D, D])\n        b = bias(\'b\', D, bias_default)\n\n        if self.batch_norm:\n            with tf.variable_scope(\'Linear1\'):\n                x_w = batch_norm(tf.matmul(x, w), is_training=self.is_training)\n            with tf.variable_scope(\'Linear2\'):\n                h_u = batch_norm(tf.matmul(h, u), is_training=self.is_training)\n            return x_w + h_u + b\n        else:\n            return tf.matmul(x, w) + tf.matmul(h, u) + b\n'"
utils/data_utils.py,0,"b'# Common data loading utilities.\nimport pickle\nimport copy\nimport os\nimport numpy as np\n\n\nclass DataSet:\n    def __init__(self, batch_size, xs, qs, ys, fact_counts, shuffle=True, name=""dataset""):\n        assert batch_size <= len(xs), ""batch size cannot be greater than data size.""\n        self.name = name\n        self.xs = xs\n        self.qs = qs\n        self.ys = ys\n        self.fact_counts = fact_counts\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.count = len(self.xs)\n        self.setup()\n\n    def setup(self):\n        self.indexes = list(range(self.count))  # used in shuffling\n        self.current_index = 0\n        self.num_batches = int(self.count / self.batch_size)\n        self.reset()\n\n    def next_batch(self):\n        assert self.has_next_batch(), ""End of epoch. Call \'complete_epoch()\' to reset.""\n        from_, to = self.current_index, self.current_index + self.batch_size\n        cur_idxs = self.indexes[from_:to]\n        xs, qs, ys = zip(*[[self.xs[i], self.qs[i], self.ys[i]] for i in cur_idxs])\n        self.current_index += self.batch_size\n        return xs, qs, ys\n\n    def has_next_batch(self):\n        return self.current_index + self.batch_size <= self.count\n\n    def split_dataset(self, split_ratio):\n        """""" Splits a data set by split_ratio.\n        (ex: split_ratio = 0.3 -> this set (70%) and splitted (30%))\n        :param split_ratio: ratio of train data\n        :return: val_set\n        """"""\n        end_index = int(self.count * (1 - split_ratio))\n\n        # do not (deep) copy data - just modify index list!\n        val_set = copy.copy(self)\n        val_set.count = self.count - end_index\n        val_set.indexes = list(range(end_index, self.count))\n        val_set.num_batches = int(val_set.count / val_set.batch_size)\n        self.count = end_index\n        self.setup()\n        return val_set\n\n    def reset(self):\n        self.current_index = 0\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n\n\nclass WordTable:\n    def __init__(self, word2vec=None, embed_size=0):\n        self.word2vec = word2vec\n        self.word2idx = {}\n        self.idx2word = [\'<eos>\']  # zero padding will be <eos>\n        self.embed_size = embed_size\n\n    def add_vocab(self, *words):\n        """""" Add vocabularies to dictionary. """"""\n        for word in words:\n            if self.word2vec and word not in self.word2vec:\n                self._create_vector(word)\n\n            if word not in self.word2idx:\n                index = len(self.idx2word)\n                self.word2idx[word] = index\n                self.idx2word.append(word)\n\n    def vectorize(self, word):\n        """""" Converts word to vector.\n        :param word: string\n        :return: 1-D array (vector)\n        """"""\n        self.add_vocab(word)\n        return self.word2vec[word]\n\n    def _create_vector(self, word):\n        # if the word is missing from Glove, create some fake vector and store in glove!\n        vector = np.random.uniform(0.0, 1.0, (self.embed_size,))\n        self.word2vec[word] = vector\n        print(""create_vector => %s is missing"" % word)\n        return vector\n\n    def word_to_index(self, word):\n        self.add_vocab(word)\n        return self.word2idx[word]\n\n    def index_to_word(self, index):\n        return self.idx2word[index]\n\n    @property\n    def vocab_size(self):\n        return len(self.idx2word)\n\n\ndef load_glove(dim):\n    """""" Loads GloVe data.\n    :param dim: word vector size (50, 100, 200)\n    :return: GloVe word table\n    """"""\n    word2vec = {}\n\n    path = ""data/glove/glove.6B."" + str(dim) + \'d\'\n    if os.path.exists(path + \'.cache\'):\n        with open(path + \'.cache\', \'rb\') as cache_file:\n            word2vec = pickle.load(cache_file)\n\n    else:\n        # Load n create cache\n        with open(path + \'.txt\') as f:\n            for line in f:\n                l = line.split()\n                word2vec[l[0]] = [float(x) for x in l[1:]]\n\n        with open(path + \'.cache\', \'wb\') as cache_file:\n            pickle.dump(word2vec, cache_file)\n\n    print(""Loaded Glove data"")\n    return word2vec\n'"
utils/nn.py,26,"b'import math\nimport tensorflow as tf\nimport numpy as np\n\n\ndef weight(name, shape, init=\'he\', range=None):\n    """""" Initializes weight.\n    :param name: Variable name\n    :param shape: Tensor shape\n    :param init: Init mode. xavier / normal / uniform / he (default is \'he\')\n    :param range:\n    :return: Variable\n    """"""\n    initializer = tf.constant_initializer()\n    if init == \'xavier\':\n        fan_in, fan_out = _get_dims(shape)\n        range = math.sqrt(6.0 / (fan_in + fan_out))\n        initializer = tf.random_uniform_initializer(-range, range)\n\n    elif init == \'he\':\n        fan_in, _ = _get_dims(shape)\n        std = math.sqrt(2.0 / fan_in)\n        initializer = tf.random_normal_initializer(stddev=std)\n\n    elif init == \'normal\':\n        initializer = tf.random_normal_initializer(stddev=0.1)\n\n    elif init == \'uniform\':\n        if range is None:\n            raise ValueError(""range must not be None if uniform init is used."")\n        initializer = tf.random_uniform_initializer(-range, range)\n\n    var = tf.get_variable(name, shape, initializer=initializer)\n    tf.add_to_collection(\'l2\', tf.nn.l2_loss(var))  # Add L2 Loss\n    return var\n\n\ndef _get_dims(shape):\n    fan_in = shape[0] if len(shape) == 2 else np.prod(shape[:-1])\n    fan_out = shape[1] if len(shape) == 2 else shape[-1]\n    return fan_in, fan_out\n\n\ndef bias(name, dim, initial_value=0.0):\n    """""" Initializes bias parameter.\n    :param name: Variable name\n    :param dim: Tensor size (list or int)\n    :param initial_value: Initial bias term\n    :return: Variable\n    """"""\n    dims = dim if isinstance(dim, list) else [dim]\n    return tf.get_variable(name, dims, initializer=tf.constant_initializer(initial_value))\n\n\ndef batch_norm(x, is_training):\n    """""" Batch normalization.\n    :param x: Tensor\n    :param is_training: boolean tf.Variable, true indicates training phase\n    :return: batch-normalized tensor\n    """"""\n    with tf.variable_scope(\'BatchNorm\'):\n        # calculate dimensions (from tf.contrib.layers.batch_norm)\n        inputs_shape = x.get_shape()\n        axis = list(range(len(inputs_shape) - 1))\n        param_shape = inputs_shape[-1:]\n\n        beta = tf.get_variable(\'beta\', param_shape, initializer=tf.constant_initializer(0.))\n        gamma = tf.get_variable(\'gamma\', param_shape, initializer=tf.constant_initializer(1.))\n        batch_mean, batch_var = tf.nn.moments(x, axis)\n        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n\n        def mean_var_with_update():\n            ema_apply_op = ema.apply([batch_mean, batch_var])\n            with tf.control_dependencies([ema_apply_op]):\n                return tf.identity(batch_mean), tf.identity(batch_var)\n\n        mean, var = tf.cond(is_training,\n                            mean_var_with_update,\n                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n    return normed\n\n\ndef dropout(x, keep_prob, is_training):\n    """""" Apply dropout.\n    :param x: Tensor\n    :param keep_prob: float, Dropout rate.\n    :param is_training: boolean tf.Varialbe, true indicates training phase\n    :return: dropout applied tensor\n    """"""\n    return tf.cond(is_training, lambda: tf.nn.dropout(x, keep_prob), lambda: x)\n\n\ndef conv(x, filter, is_training):\n    l = tf.nn.conv2d(x, filter, strides=[1, 1, 1, 1], padding=\'SAME\')\n    l = batch_norm(l, is_training)\n    return tf.nn.relu(l)\n\n\ndef flatten(x):\n    return tf.reshape(x, [-1])\n\n\ndef fully_connected(input, num_neurons, name, is_training):\n    input_size = input.get_shape()[1]\n    w = weight(name, [input_size, num_neurons], init=\'he\')\n    l = tf.matmul(input, w)\n    l = batch_norm(l, is_training)\n    return tf.nn.relu(l)'"
models/new/__init__.py,0,b''
models/new/dmn_plus.py,41,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.ops import rnn_cell\n\nfrom models.base_model import BaseModel\nfrom models.new.episode_module import EpisodeModule\nfrom utils.nn import weight, bias, dropout, batch_norm\n\n\nclass DMN(BaseModel):\n    """""" Dynamic Memory Networks (March 2016 Version - https://arxiv.org/abs/1603.01417)\n        Improved End-To-End version.""""""\n    def build(self):\n        params = self.params\n        N, L, Q, F = params.batch_size, params.max_sent_size, params.max_ques_size, params.max_fact_count\n        V, d, A = params.embed_size, params.hidden_size, self.words.vocab_size\n\n        # initialize self\n        # placeholders\n        input = tf.placeholder(\'int32\', shape=[N, F, L], name=\'x\')  # [num_batch, fact_count, sentence_len]\n        question = tf.placeholder(\'int32\', shape=[N, Q], name=\'q\')  # [num_batch, question_len]\n        answer = tf.placeholder(\'int32\', shape=[N], name=\'y\')  # [num_batch] - one word answer\n        fact_counts = tf.placeholder(\'int64\', shape=[N], name=\'fc\')\n        input_mask = tf.placeholder(\'float32\', shape=[N, F, L, V], name=\'xm\')\n        is_training = tf.placeholder(tf.bool)\n        self.att = tf.constant(0.)\n\n        # Prepare parameters\n        gru = rnn_cell.GRUCell(d)\n        l = self.positional_encoding()\n        embedding = weight(\'embedding\', [A, V], init=\'uniform\', range=3**(1/2))\n\n        with tf.name_scope(\'SentenceReader\'):\n            input_list = tf.unpack(tf.transpose(input))  # L x [F, N]\n            input_embed = []\n            for facts in input_list:\n                facts = tf.unpack(facts)\n                embed = tf.pack([tf.nn.embedding_lookup(embedding, w) for w in facts])  # [F, N, V]\n                input_embed.append(embed)\n\n            # apply positional encoding\n            input_embed = tf.transpose(tf.pack(input_embed), [2, 1, 0, 3])  # [N, F, L, V]\n            encoded = l * input_embed * input_mask\n            facts = tf.reduce_sum(encoded, 2)  # [N, F, V]\n\n        # dropout time\n        facts = dropout(facts, params.keep_prob, is_training)\n\n        with tf.name_scope(\'InputFusion\'):\n            # Bidirectional RNN\n            with tf.variable_scope(\'Forward\'):\n                forward_states, _ = tf.nn.dynamic_rnn(gru, facts, fact_counts, dtype=tf.float32)\n\n            with tf.variable_scope(\'Backward\'):\n                facts_reverse = tf.reverse_sequence(facts, fact_counts, 1)\n                backward_states, _ = tf.nn.dynamic_rnn(gru, facts_reverse, fact_counts, dtype=tf.float32)\n\n            # Use forward and backward states both\n            facts = forward_states + backward_states  # [N, F, d]\n\n        with tf.variable_scope(\'Question\'):\n            ques_list = tf.unpack(tf.transpose(question))\n            ques_embed = [tf.nn.embedding_lookup(embedding, w) for w in ques_list]\n            _, question_vec = tf.nn.rnn(gru, ques_embed, dtype=tf.float32)\n\n        # Episodic Memory\n        with tf.variable_scope(\'Episodic\'):\n            episode = EpisodeModule(d, question_vec, facts, is_training, params.batch_norm)\n            memory = tf.identity(question_vec)\n\n            for t in range(params.memory_step):\n                with tf.variable_scope(\'Layer%d\' % t) as scope:\n                    if params.memory_update == \'gru\':\n                        memory = gru(episode.new(memory), memory)[0]\n                    else:\n                        # ReLU update\n                        c = episode.new(memory)\n                        concated = tf.concat(1, [memory, c, question_vec])\n\n                        w_t = weight(\'w_t\', [3 * d, d])\n                        z = tf.matmul(concated, w_t)\n                        if params.batch_norm:\n                            z = batch_norm(z, is_training)\n                        else:\n                            b_t = bias(\'b_t\', d)\n                            z = z + b_t\n                        memory = tf.nn.relu(z)  # [N, d]\n\n                    scope.reuse_variables()\n\n        # Regularizations\n        if params.batch_norm:\n            memory = batch_norm(memory, is_training=is_training)\n        memory = dropout(memory, params.keep_prob, is_training)\n\n        with tf.name_scope(\'Answer\'):\n            # Answer module : feed-forward version (for it is one word answer)\n            w_a = weight(\'w_a\', [d, A], init=\'xavier\')\n            logits = tf.matmul(memory, w_a)  # [N, A]\n\n        with tf.name_scope(\'Loss\'):\n            # Cross-Entropy loss\n            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, answer)\n            loss = tf.reduce_mean(cross_entropy)\n            total_loss = loss + params.weight_decay * tf.add_n(tf.get_collection(\'l2\'))\n\n        with tf.variable_scope(\'Accuracy\'):\n            # Accuracy\n            predicts = tf.cast(tf.argmax(logits, 1), \'int32\')\n            corrects = tf.equal(predicts, answer)\n            num_corrects = tf.reduce_sum(tf.cast(corrects, tf.float32))\n            accuracy = tf.reduce_mean(tf.cast(corrects, tf.float32))\n\n        # Training\n        optimizer = tf.train.AdamOptimizer(params.learning_rate)\n        opt_op = optimizer.minimize(total_loss, global_step=self.global_step)\n\n        # placeholders\n        self.x = input\n        self.xm = input_mask\n        self.q = question\n        self.y = answer\n        self.fc = fact_counts\n        self.is_training = is_training\n\n        # tensors\n        self.total_loss = total_loss\n        self.num_corrects = num_corrects\n        self.accuracy = accuracy\n        self.opt_op = opt_op\n\n    def positional_encoding(self):\n        D, M, N = self.params.embed_size, self.params.max_sent_size, self.params.batch_size\n        encoding = np.zeros([M, D])\n        for j in range(M):\n            for d in range(D):\n                encoding[j, d] = (1 - float(j)/M) - (float(d)/D)*(1 - 2.0*j/M)\n\n        return encoding\n\n    def preprocess_batch(self, batches):\n        """""" Make padding and masks last word of sentence. (EOS token)\n        :param batches: A tuple (input, question, label, mask)\n        :return A tuple (input, question, label, mask)\n        """"""\n        params = self.params\n        input, question, label = batches\n        N, L, Q, F = params.batch_size, params.max_sent_size, params.max_ques_size, params.max_fact_count\n        V = params.embed_size\n\n        # make input and question fixed size\n        new_input = np.zeros([N, F, L])  # zero padding\n        input_masks = np.zeros([N, F, L, V])\n        new_question = np.zeros([N, Q])\n        new_labels = []\n        fact_counts = []\n\n        for n in range(N):\n            for i, sentence in enumerate(input[n]):\n                sentence_len = len(sentence)\n                new_input[n, i, :sentence_len] = [self.words.word_to_index(w) for w in sentence]\n                input_masks[n, i, :sentence_len, :] = 1.  # mask words\n\n            fact_counts.append(len(input[n]))\n\n            sentence_len = len(question[n])\n            new_question[n, :sentence_len] = [self.words.word_to_index(w) for w in question[n]]\n\n            new_labels.append(self.words.word_to_index(label[n]))\n\n        return new_input, new_question, new_labels, fact_counts, input_masks\n\n    def get_feed_dict(self, batches, is_train):\n        input, question, label, fact_counts, mask = self.preprocess_batch(batches)\n        return {\n            self.x: input,\n            self.xm: mask,\n            self.q: question,\n            self.y: label,\n            self.fc: fact_counts,\n            self.is_training: is_train\n        }\n'"
models/new/episode_module.py,13,"b'import tensorflow as tf\n\nfrom utils.nn import weight, bias\nfrom utils.attn_gru import AttnGRU\n\n\nclass EpisodeModule:\n    """""" Inner GRU module in episodic memory that creates episode vector. """"""\n    def __init__(self, num_hidden, question, facts, is_training, bn):\n        self.question = question\n        self.facts = tf.unpack(tf.transpose(facts, [1, 2, 0]))  # F x [d, N]\n\n        # transposing for attention\n        self.question_transposed = tf.transpose(question)\n        self.facts_transposed = [tf.transpose(f) for f in self.facts]  # F x [N, d]\n\n        # parameters\n        self.w1 = weight(\'w1\', [num_hidden, 4 * num_hidden])\n        self.b1 = bias(\'b1\', [num_hidden, 1])\n        self.w2 = weight(\'w2\', [1, num_hidden])\n        self.b2 = bias(\'b2\', [1, 1])\n        self.gru = AttnGRU(num_hidden, is_training, bn)\n\n    @property\n    def init_state(self):\n        return tf.zeros_like(self.facts_transposed[0])\n\n    def new(self, memory):\n        """""" Creates new episode vector (will feed into Episodic Memory GRU)\n        :param memory: Previous memory vector\n        :return: episode vector\n        """"""\n        state = self.init_state\n        memory = tf.transpose(memory)  # [N, D]\n\n        with tf.variable_scope(\'AttnGate\') as scope:\n            for f, f_t in zip(self.facts, self.facts_transposed):\n                g = self.attention(f, memory)\n                state = self.gru(f_t, state, g)\n                scope.reuse_variables()  # share params\n\n        return state\n\n    def attention(self, f, m):\n        """""" Attention mechanism. For details, see paper.\n        :param f: A fact vector [N, D] at timestep\n        :param m: Previous memory vector [N, D]\n        :return: attention vector at timestep\n        """"""\n        with tf.variable_scope(\'attention\'):\n            # NOTE THAT instead of L1 norm we used L2\n            q = self.question_transposed\n            vec = tf.concat(0, [f * q, f * m, tf.abs(f - q), tf.abs(f - m)])  # [4*d, N]\n\n            # attention learning\n            l1 = tf.matmul(self.w1, vec) + self.b1  # [N, d]\n            l1 = tf.nn.tanh(l1)\n            l2 = tf.matmul(self.w2, l1) + self.b2\n            l2 = tf.nn.softmax(l2)\n            return tf.transpose(l2)\n\n        return att\n'"
models/old/__init__.py,0,b''
models/old/dmn.py,30,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.ops import seq2seq, rnn_cell\n\nfrom models.base_model import BaseModel\nfrom models.old.episode_module import EpisodeModule\nfrom utils.nn import weight, batch_norm, dropout\n\n\nclass DMN(BaseModel):\n    """""" Dynamic Memory Networks (http://arxiv.org/abs/1506.07285)\n        Semantic Memory version: Instead of implementing embedding layer,\n        it uses GloVe instead. (First version of DMN paper.)\n    """"""\n    def build(self):\n        params = self.params\n        N, L, Q, F = params.batch_size, params.max_sent_size, params.max_ques_size, params.max_fact_count\n        V, d, A = params.glove_size, params.hidden_size, self.words.vocab_size\n\n        # initialize self\n        # placeholders\n        input = tf.placeholder(tf.float32, shape=[N, L, V], name=\'x\')  # [num_batch, sentence_len, glove_dim]\n        question = tf.placeholder(tf.float32, shape=[N, Q, V], name=\'q\')  # [num_batch, sentence_len, glove_dim]\n        answer = tf.placeholder(tf.int64, shape=[N], name=\'y\')  # [num_batch] - one word answer\n        input_mask = tf.placeholder(tf.bool, shape=[N, L], name=\'x_mask\')  # [num_batch, sentence_len]\n        is_training = tf.placeholder(tf.bool)\n\n        # Prepare parameters\n        gru = rnn_cell.GRUCell(d)\n\n        # Input module\n        with tf.variable_scope(\'input\') as scope:\n            input_list = self.make_decoder_batch_input(input)\n            input_states, _ = seq2seq.rnn_decoder(input_list, gru.zero_state(N, tf.float32), gru)\n\n            # Question module\n            scope.reuse_variables()\n\n            ques_list = self.make_decoder_batch_input(question)\n            questions, _ = seq2seq.rnn_decoder(ques_list, gru.zero_state(N, tf.float32), gru)\n            question_vec = questions[-1]  # use final state\n\n        # Masking: to extract fact vectors at end of sentence. (details in paper)\n        input_states = tf.transpose(tf.pack(input_states), [1, 0, 2])  # [N, L, D]\n        facts = []\n        for n in range(N):\n            filtered = tf.boolean_mask(input_states[n, :, :], input_mask[n, :])  # [?, D]\n            padding = tf.zeros(tf.pack([F - tf.shape(filtered)[0], d]))\n            facts.append(tf.concat(0, [filtered, padding]))  # [F, D]\n\n        facked = tf.pack(facts)  # packing for transpose... I hate TF so much\n        facts = tf.unpack(tf.transpose(facked, [1, 0, 2]), num=F)  # F x [N, D]\n\n        # Episodic Memory\n        with tf.variable_scope(\'episodic\') as scope:\n            episode = EpisodeModule(d, question_vec, facts)\n\n            memory = tf.identity(question_vec)\n            for t in range(params.memory_step):\n                memory = gru(episode.new(memory), memory)[0]\n                scope.reuse_variables()\n\n        # Regularizations\n        if params.batch_norm:\n            memory = batch_norm(memory, is_training=is_training)\n        memory = dropout(memory, params.keep_prob, is_training)\n\n        with tf.name_scope(\'Answer\'):\n            # Answer module : feed-forward version (for it is one word answer)\n            w_a = weight(\'w_a\', [d, A])\n            logits = tf.matmul(memory, w_a)  # [N, A]\n\n        with tf.name_scope(\'Loss\'):\n            # Cross-Entropy loss\n            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, answer)\n            loss = tf.reduce_mean(cross_entropy)\n            total_loss = loss + params.weight_decay * tf.add_n(tf.get_collection(\'l2\'))\n\n        with tf.variable_scope(\'Accuracy\'):\n            # Accuracy\n            predicts = tf.cast(tf.argmax(logits, 1), \'int32\')\n            corrects = tf.equal(predicts, answer)\n            num_corrects = tf.reduce_sum(tf.cast(corrects, tf.float32))\n            accuracy = tf.reduce_mean(tf.cast(corrects, tf.float32))\n\n        # Training\n        optimizer = tf.train.AdadeltaOptimizer(params.learning_rate)\n        opt_op = optimizer.minimize(total_loss, global_step=self.global_step)\n\n        # placeholders\n        self.x = input\n        self.q = question\n        self.y = answer\n        self.mask = input_mask\n        self.is_training = is_training\n\n        # tensors\n        self.total_loss = total_loss\n        self.num_corrects = num_corrects\n        self.accuracy = accuracy\n        self.opt_op = opt_op\n\n    def make_decoder_batch_input(self, input):\n        """""" Reshape batch data to be compatible with Seq2Seq RNN decoder.\n        :param input: Input 3D tensor that has shape [num_batch, sentence_len, wordvec_dim]\n        :return: list of 2D tensor that has shape [num_batch, wordvec_dim]\n        """"""\n        input_transposed = tf.transpose(input, [1, 0, 2])  # [L, N, V]\n        return tf.unpack(input_transposed)\n\n    def preprocess_batch(self, batches):\n        """""" Vectorizes padding and masks last word of sentence. (EOS token)\n        :param batches: A tuple (input, question, label, mask)\n        :return A tuple (input, question, label, mask)\n        """"""\n        params = self.params\n        input, question, label = batches\n        N, Q, F, V = params.batch_size, params.max_ques_size, params.max_fact_count, params.embed_size\n\n        # calculate max sentence size\n        L = 0\n        for n in range(N):\n            sent_len = np.sum([len(sentence) for sentence in input[n]])\n            L = max(L, sent_len)\n        params.max_sent_size = L\n\n        # make input and question fixed size\n        new_input = np.zeros([N, L, V])  # zero padding\n        new_question = np.zeros([N, Q, V])\n        new_mask = np.full([N, L], False, dtype=bool)\n        new_labels = []\n\n        for n in range(N):\n            sentence = np.array(input[n]).flatten()  # concat all sentences\n            sentence_len = len(sentence)\n\n            input_mask = [index for index, w in enumerate(sentence) if w == \'.\']\n            new_input[n, :sentence_len] = [self.words.vectorize(w) for w in sentence]\n\n            sentence_len = len(question[n])\n            new_question[n, :sentence_len] = [self.words.vectorize(w) for w in question[n]]\n\n            new_labels.append(self.words.word_to_index(label[n]))\n\n            # mask on\n            for eos_index in input_mask:\n                new_mask[n, eos_index] = True\n\n        return new_input, new_question, new_labels, new_mask\n\n    def get_feed_dict(self, batches, is_train):\n        input, question, label, mask = self.preprocess_batch(batches)\n        return {\n            self.x: input,\n            self.q: question,\n            self.y: label,\n            self.mask: mask,\n            self.is_training: is_train\n        }\n'"
models/old/dmn_embedding.py,30,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.ops import seq2seq, rnn_cell\n\nfrom models.base_model import BaseModel\nfrom models.old.episode_module import EpisodeModule\nfrom utils.nn import weight, batch_norm, dropout\n\n\nclass DMN(BaseModel):\n    """""" Dynamic Memory Networks (http://arxiv.org/abs/1506.07285)\n        Non-semantic memory version: Implements embedding layer instead of using GloVe.\n    """"""\n    def build(self):\n        params = self.params\n        N, L, Q, F = params.batch_size, params.max_sent_size, params.max_ques_size, params.max_fact_count\n        V, d, A = params.embed_size, params.hidden_size, self.words.vocab_size\n\n        # initialize self\n        # placeholders\n        input = tf.placeholder(\'int32\', shape=[N, L], name=\'x\')  # [num_batch, sentence_len]\n        question = tf.placeholder(\'int32\', shape=[N, Q], name=\'q\')  # [num_batch, sentence_len]\n        answer = tf.placeholder(\'int32\', shape=[N], name=\'y\')  # [num_batch] - one word answer\n        input_mask = tf.placeholder(tf.bool, shape=[N, L], name=\'x_mask\')  # [num_batch, sentence_len]\n        is_training = tf.placeholder(tf.bool)\n\n        # Prepare parameters\n        gru = rnn_cell.GRUCell(d)\n\n        # Input module\n        with tf.variable_scope(\'input\') as scope:\n            input_list = tf.unpack(tf.transpose(input))\n            input_states, _ = seq2seq.embedding_rnn_decoder(input_list, gru.zero_state(N, tf.float32), gru, A, V)\n\n            # Question module\n            scope.reuse_variables()\n\n            ques_list = tf.unpack(tf.transpose(question))\n            questions, _ = seq2seq.embedding_rnn_decoder(ques_list, gru.zero_state(N, tf.float32), gru, A, V)\n            question_vec = questions[-1]  # use final state\n\n        # Masking: to extract fact vectors at end of sentence. (details in paper)\n        input_states = tf.transpose(tf.pack(input_states), [1, 0, 2])  # [N, L, D]\n        facts = []\n        for n in range(N):\n            filtered = tf.boolean_mask(input_states[n, :, :], input_mask[n, :])  # [?, D]\n            padding = tf.zeros(tf.pack([F - tf.shape(filtered)[0], d]))\n            facts.append(tf.concat(0, [filtered, padding]))  # [F, D]\n\n        facked = tf.pack(facts)  # packing for transpose... I hate TF so much\n        facts = tf.unpack(tf.transpose(facked, [1, 0, 2]), num=F)  # F x [N, D]\n\n        # Episodic Memory\n        with tf.variable_scope(\'episodic\') as scope:\n            episode = EpisodeModule(d, question_vec, facts)\n\n            memory = tf.identity(question_vec)\n            for t in range(params.memory_step):\n                memory = gru(episode.new(memory), memory)[0]\n                scope.reuse_variables()\n\n        # Regularizations\n        if params.batch_norm:\n            memory = batch_norm(memory, is_training=is_training)\n        memory = dropout(memory, params.keep_prob, is_training)\n\n        with tf.name_scope(\'Answer\'):\n            # Answer module : feed-forward version (for it is one word answer)\n            w_a = weight(\'w_a\', [d, A])\n            logits = tf.matmul(memory, w_a)  # [N, A]\n\n        with tf.name_scope(\'Loss\'):\n            # Cross-Entropy loss\n            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, answer)\n            loss = tf.reduce_mean(cross_entropy)\n            total_loss = loss + params.weight_decay * tf.add_n(tf.get_collection(\'l2\'))\n\n        with tf.variable_scope(\'Accuracy\'):\n            # Accuracy\n            predicts = tf.cast(tf.argmax(logits, 1), \'int32\')\n            corrects = tf.equal(predicts, answer)\n            num_corrects = tf.reduce_sum(tf.cast(corrects, tf.float32))\n            accuracy = tf.reduce_mean(tf.cast(corrects, tf.float32))\n\n        # Training\n        optimizer = tf.train.AdadeltaOptimizer(params.learning_rate)\n        opt_op = optimizer.minimize(total_loss, global_step=self.global_step)\n\n        # placeholders\n        self.x = input\n        self.q = question\n        self.y = answer\n        self.mask = input_mask\n        self.is_training = is_training\n\n        # tensors\n        self.total_loss = total_loss\n        self.num_corrects = num_corrects\n        self.accuracy = accuracy\n        self.opt_op = opt_op\n\n    def preprocess_batch(self, batches):\n        """""" Make padding and masks last word of sentence. (EOS token)\n        :param batches: A tuple (input, question, label, mask)\n        :return A tuple (input, question, label, mask)\n        """"""\n        params = self.params\n        input, question, label = batches\n        N, Q, F, V = params.batch_size, params.max_ques_size, params.max_fact_count, params.embed_size\n\n        # calculate max sentence size\n        L = 0\n        for n in range(N):\n            sent_len = np.sum([len(sentence) for sentence in input[n]])\n            L = max(L, sent_len)\n        params.max_sent_size = L\n\n        # make input and question fixed size\n        new_input = np.zeros([N, L])  # zero padding\n        new_question = np.zeros([N, Q])\n        new_mask = np.full([N, L], False, dtype=bool)\n        new_labels = []\n\n        for n in range(N):\n            sentence = np.array(input[n]).flatten()  # concat all sentences\n            sentence_len = len(sentence)\n\n            input_mask = [index for index, w in enumerate(sentence) if w == \'.\']\n            new_input[n, :sentence_len] = [self.words.word_to_index(w) for w in sentence]\n\n            sentence_len = len(question[n])\n            new_question[n, :sentence_len] = [self.words.word_to_index(w) for w in question[n]]\n\n            new_labels.append(self.words.word_to_index(label[n]))\n\n            # mask on\n            for eos_index in input_mask:\n                new_mask[n, eos_index] = True\n\n        return new_input, new_question, new_labels, new_mask\n\n    def get_feed_dict(self, batches, is_train):\n        input, question, label, mask = self.preprocess_batch(batches)\n        return {\n            self.x: input,\n            self.q: question,\n            self.y: label,\n            self.mask: mask,\n            self.is_training: is_train\n        }\n'"
models/old/episode_module.py,13,"b'import tensorflow as tf\n\nfrom utils.nn import weight, bias\n\n\nclass EpisodeModule:\n    """""" Inner GRU module in episodic memory that creates episode vector. """"""\n    def __init__(self, num_hidden, question, facts):\n        self.question = question\n        self.facts = facts\n\n        # transposing for attention\n        self.question_transposed = tf.transpose(question)\n        self.facts_transposed = [tf.transpose(c) for c in facts]\n\n        # parameters\n        self.w1 = weight(\'w1\', [num_hidden, 7 * num_hidden])\n        self.b1 = bias(\'b1\', [num_hidden, 1])\n        self.w2 = weight(\'w2\', [1, num_hidden])\n        self.b2 = bias(\'b2\', [1, 1])\n        self.gru = tf.nn.rnn_cell.GRUCell(num_hidden)\n\n    @property\n    def init_state(self):\n        return tf.zeros_like(self.facts[0])\n\n    def new(self, memory):\n        """""" Creates new episode vector (will feed into Episodic Memory GRU)\n        :param memory: Previous memory vector\n        :return: episode vector\n        """"""\n        state = self.init_state\n        memory = tf.transpose(memory)  # [N, D]\n        for c, c_t in zip(self.facts, self.facts_transposed):\n            g = self.attention(c_t, memory)\n            state = g * self.gru(c, state)[0] + (1 - g) * state\n            tf.get_variable_scope().reuse_variables()  # share params\n\n        return state\n\n    def attention(self, c, m):\n        """""" Attention mechanism. For details, see paper.\n        :param c: A fact vector [N, D] at timestep\n        :param m: Previous memory vector [N, D]\n        :return: attention vector at timestep\n        """"""\n        with tf.variable_scope(\'attention\'):\n            # NOTE THAT instead of L1 norm we used L2\n            q = self.question_transposed\n            vec = tf.concat(0, [c, m, q, c*q, c*m, (c-q)**2, (c-m)**2])  # (7*d, N)\n\n            # attention learning\n            l1 = tf.matmul(self.w1, vec) + self.b1  # (N, d)\n            l1 = tf.nn.tanh(l1)\n            l2 = tf.matmul(self.w2, l1) + self.b2\n            l2 = tf.nn.sigmoid(l2)\n            return tf.transpose(l2)\n\n        return att\n\n'"
