file_path,api_count,code
SpeechRecognizer.py,66,"b'import os\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.python.layers.core import Dense\n\nimport sr_model_utils\n\n\nclass SpeechRecognizer:\n\n    def __init__(self,\n                 char2ind,\n                 ind2char,\n                 save_path,\n                 mode=\'TRAIN\',\n                 num_layers_encoder=1,\n                 num_layers_decoder=1,\n                 embedding_dim=300,\n                 rnn_size_encoder=256,\n                 rnn_size_decoder=256,\n                 learning_rate=0.001,\n                 learning_rate_decay=0.9,\n                 learning_rate_decay_steps=100,\n                 max_lr=0.01,\n                 keep_probability_i=0.825,\n                 keep_probability_o=0.895,\n                 keep_probability_h=0.86,\n                 keep_probability_e=0.986,\n                 batch_size=64,\n                 beam_width=10,\n                 epochs=20,\n                 eos=""<EOS>"",\n                 sos=""<SOS>"",\n                 pad=\'<PAD>\',\n                 clip=5,\n                 inference_targets=False,\n                 summary_dir=None,\n                 use_cyclic_lr=False):\n        """"""\n\n        Args:\n            char2ind: lookup dict from char to index.\n            ind2char: lookup dict from index to char.\n            save_path: path to save the tf model to.\n            mode: String. \'TRAIN\' or \'INFER\'. depending on which mode we use\n                  a different graph is created.\n            num_layers_encoder: Float. Number of encoder layers. defaults to 1.\n            num_layers_decoder: Float. Number of decoder layers. defaults to 1.\n            embedding_dim: dimension of the embedding vectors in the embedding matrix.\n                           every word has an embedding_dim \'long\' vector.\n            rnn_size_encoder: Integer. number of hidden units in encoder. defaults to 256.\n            rnn_size_decoder: Integer. number of hidden units in decoder. defaults to 256.\n            learning_rate: Float.\n            learning_rate_decay: only if exponential learning rate is used.\n            learning_rate_decay_steps: Integer.\n            max_lr: only if cyclic learning rate is used.\n            keep_probability_i: Float. Values inspired by Jeremy Howard\'s fast.ai course.\n            keep_probability_o: Float. Values inspired by Jeremy Howard\'s fast.ai course.\n            keep_probability_e: Float. Values inspired by Jeremy Howard\'s fast.ai course.\n            keep_probability_h: Float. Values inspired by Jeremy Howard\'s fast.ai course.\n            batch_size: Integer. Size of mini batches.\n            beam_width: Integer. Only used in inference, for Beam Search.(\'INFER\'-mode)\n            epochs: Integer. Number of times the training is conducted\n                    on the whole training data.\n            eos: EndOfSentence tag.\n            sos: StartOfSentence tag.\n            pad: Padding tag.\n            clip: Value to clip the gradients to in training process.\n            inference_targets:\n            summary_dir: Directory the summaries are written to for tensorboard.\n            use_cyclic_lr: Boolean.\n        """"""\n\n        self.char2ind = char2ind\n        self.ind2char = ind2char\n        self.vocab_size = len(char2ind)\n        self.num_layers_encoder = num_layers_encoder\n        self.num_layers_decoder = num_layers_decoder\n        self.rnn_size_encoder = rnn_size_encoder\n        self.rnn_size_decoder = rnn_size_decoder\n        self.save_path = save_path\n        self.embedding_dim = embedding_dim\n        self.mode = mode.upper()\n        self.learning_rate = learning_rate\n        self.learning_rate_decay = learning_rate_decay\n        self.learning_rate_decay_steps = learning_rate_decay_steps\n        self.keep_probability_i = keep_probability_i\n        self.keep_probability_o = keep_probability_o\n        self.keep_probability_h = keep_probability_h\n        self.keep_probability_e = keep_probability_e\n        self.batch_size = batch_size\n        self.beam_width = beam_width\n        self.eos = eos\n        self.sos = sos\n        self.clip = clip\n        self.pad = pad\n        self.epochs = epochs\n        self.inference_targets = inference_targets\n        self.use_cyclic_lr = use_cyclic_lr\n        self.max_lr = max_lr\n        self.summary_dir = summary_dir\n\n    def build_graph(self):\n        self.add_placeholders()\n        self.add_embeddings()\n        self.add_lookup_ops()\n        self.add_seq2seq()\n        self.saver = tf.train.Saver()\n        print(\'Graph built.\')\n\n    def add_placeholders(self):\n        self.audios = tf.placeholder(tf.float32,\n                                     shape=[None, None, 494])\n        self.audio_sequence_lengths = tf.placeholder(tf.int32,\n                                                     shape=[None],\n                                                     name=\'sequence_length_source\')\n\n        self.char_ids = tf.placeholder(tf.int32,\n                                       shape=[None, None],\n                                       name=\'ids_target\')\n\n        self.char_sequence_lengths = tf.placeholder(tf.int32,\n                                                    shape=[None],\n                                                    name=\'sequence_length_target\')\n        self.maximum_iterations = tf.reduce_max(self.char_sequence_lengths,\n                                                name=\'max_dec_len\')\n\n    def create_word_embedding(self, embed_name, vocab_size, embed_dim):\n        """"""Creates embedding matrix in given shape - [vocab_size, embed_dim].\n        """"""\n        embedding = tf.get_variable(embed_name,\n                                    shape=[vocab_size+1, embed_dim],\n                                    dtype=tf.float32)\n        return embedding\n\n    def add_embeddings(self):\n        """"""Creates the embedding matrix.\n        """"""\n        self.embedding = self.create_word_embedding(\'embedding\',\n                                                    self.vocab_size,\n                                                    self.embedding_dim)\n\n    def add_lookup_ops(self):\n        """"""Performs the lookup operation.\n        """"""\n\n        char_embedding = tf.nn.embedding_lookup(self.embedding,\n                                                self.char_ids,\n                                                name=\'char_embedding\')\n        self.char_embedding = tf.nn.dropout(char_embedding,\n                                            self.keep_probability_e,\n                                            name=\'char_embedding_dropout\')\n    def make_rnn_cell(self, rnn_size):\n        """"""Creates LSTM cell wrapped with dropout.\n        """"""\n        cell = tf.nn.rnn_cell.LSTMCell(rnn_size)\n        cell = tf.nn.rnn_cell.DropoutWrapper(cell,\n                                             input_keep_prob=self.keep_probability_i,\n                                             output_keep_prob=self.keep_probability_o,\n                                             state_keep_prob=self.keep_probability_h)\n        return cell\n\n    def make_attention_cell(self, dec_cell, rnn_size, enc_output, lengths):\n        """"""Wraps the given cell with Bahdanau Attention.\n        """"""\n        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units=rnn_size,\n                                                                   memory=enc_output,\n                                                                   memory_sequence_length=lengths,\n                                                                   name=\'BahdanauAttention\')\n\n        return tf.contrib.seq2seq.AttentionWrapper(cell=dec_cell,\n                                                   attention_mechanism=attention_mechanism,\n                                                   attention_layer_size=None,\n                                                   output_attention=False)\n\n    def blstm(self,\n              inputs,\n              seq_length,\n              n_hidden,\n              scope=None,\n              initial_state_fw=None,\n              initial_state_bw=None):\n        """"""\n        Creates a bidirectional lstm.\n        Args:\n            inputs: Array of input points.\n            seq_length: Array of integers. Sequence lengths of the\n                        input points.\n            n_hidden: Integer. Number of hidden units to use for\n                      rnn cell.\n            scope: String.\n            initial_state_fw: Initial state of foward cell.\n            initial_state_bw: Initial state of backward cell.\n\n        Returns: Tuple of fw and bw output.\n                 Tuple of fw and bw state.\n\n        """"""\n        fw_cell = self.make_rnn_cell(n_hidden)\n        bw_cell = self.make_rnn_cell(n_hidden)\n\n        (out_fw, out_bw), (state_fw, state_bw) = tf.nn.bidirectional_dynamic_rnn(\n            cell_fw=fw_cell,\n            cell_bw=bw_cell,\n            inputs=inputs,\n            sequence_length=seq_length,\n            initial_state_fw=initial_state_fw,\n            initial_state_bw=initial_state_bw,\n            dtype=tf.float32,\n            scope=scope\n        )\n\n        return (out_fw, out_bw), (state_fw, state_bw)\n\n\n    def reshape_pyramidal(self, outputs, sequence_length):\n        """"""\n        Reshapes the given outputs, i.e. reduces the\n        time resolution by 2.\n\n        Similar to ""Listen Attend Spell"".\n        https://arxiv.org/pdf/1508.01211.pdf\n        """"""\n        # [batch_size, max_time, num_units]\n        shape = tf.shape(outputs)\n        batch_size, max_time = shape[0], shape[1]\n        num_units = outputs.get_shape().as_list()[-1]\n\n        pads = [[0, 0], [0, tf.floormod(max_time, 2)], [0, 0]]\n        outputs = tf.pad(outputs, pads)\n\n        concat_outputs = tf.reshape(outputs, (batch_size, -1, num_units * 2))\n        return concat_outputs, tf.floordiv(sequence_length, 2) + tf.floormod(sequence_length, 2)\n\n\n    def triangular_lr(self, current_step):\n        """"""cyclic learning rate - exponential range.""""""\n        step_size = self.learning_rate_decay_steps\n        base_lr = self.learning_rate\n        max_lr = self.max_lr\n\n        cycle = tf.floor(1 + current_step / (2 * step_size))\n        x = tf.abs(current_step / step_size - 2 * cycle + 1)\n        lr = base_lr + (max_lr - base_lr) * tf.maximum(0.0, tf.cast((1.0 - x), dtype=tf.float32)) * (0.99999 ** tf.cast(\n            current_step,\n            dtype=tf.float32))\n        return lr\n\n\n    def add_seq2seq(self):\n        """"""Creates the sequence to sequence architecture.""""""\n        with tf.variable_scope(\'dynamic_seq2seq\', dtype=tf.float32):\n            # Encoder\n            encoder_outputs, encoder_state = self.build_encoder()\n\n            # Decoder\n            logits, sample_id, final_context_state = self.build_decoder(encoder_outputs,\n                                                                        encoder_state)\n            if self.mode == \'TRAIN\':\n\n                # Loss\n                loss = self.compute_loss(logits)\n                self.train_loss = loss\n                self.eval_loss = loss\n                self.global_step = tf.Variable(0, trainable=False)\n\n\n                # cyclic learning rate\n                if self.use_cyclic_lr:\n                    self.learning_rate = self.triangular_lr(self.global_step)\n\n                # exponential learning rate\n                else:\n                    self.learning_rate = tf.train.exponential_decay(\n                        self.learning_rate,\n                        self.global_step,\n                        decay_steps=self.learning_rate_decay_steps,\n                        decay_rate=self.learning_rate_decay,\n                        staircase=True)\n\n                # Optimizer\n                opt = tf.train.AdamOptimizer(self.learning_rate, beta1=0.7, beta2=0.99)\n\n                # Gradients\n                if self.clip > 0:\n                    grads, vs = zip(*opt.compute_gradients(self.train_loss))\n                    grads, _ = tf.clip_by_global_norm(grads, self.clip)\n                    self.train_op = opt.apply_gradients(zip(grads, vs),\n                                                        global_step=self.global_step)\n                else:\n                    self.train_op = opt.minimize(self.train_loss,\n                                                 global_step=self.global_step)\n\n\n\n            elif self.mode == \'INFER\':\n                loss = None\n                self.infer_logits, _, self.final_context_state, self.sample_id = logits, loss, final_context_state, sample_id\n                self.sample_words = self.sample_id\n\n    def build_encoder(self):\n        """"""The encoder. Bidirectional LSTM.\n           Similar architecture as in: ""Listen, Attend and Spell""\n           https://arxiv.org/pdf/1508.01211.pdf\n        """"""\n\n        with tf.variable_scope(""encoder""):\n            # Pyramidal bidirectional LSTM(s)\n            inputs = self.audios\n            seq_lengths = self.audio_sequence_lengths\n\n            initial_state_fw = None\n            initial_state_bw = None\n\n            for n in range(self.num_layers_encoder):\n                scope = \'pBLSTM\' + str(n)\n                (out_fw, out_bw), (state_fw, state_bw) = self.blstm(\n                    inputs,\n                    seq_lengths,\n                    self.rnn_size_encoder // 2,\n                    scope=scope,\n                    initial_state_fw=initial_state_fw,\n                    initial_state_bw=initial_state_bw\n                )\n\n                inputs = tf.concat([out_fw, out_bw], -1)\n                inputs, seq_lengths = self.reshape_pyramidal(inputs, seq_lengths)\n                initial_state_fw = state_fw\n                initial_state_bw = state_bw\n\n\n            bi_state_c = tf.concat((initial_state_fw.c, initial_state_fw.c), -1)\n            bi_state_h = tf.concat((initial_state_fw.h, initial_state_fw.h), -1)\n            bi_lstm_state = tf.nn.rnn_cell.LSTMStateTuple(c=bi_state_c, h=bi_state_h)\n            encoder_state = tuple([bi_lstm_state] * self.num_layers_encoder)\n\n            return inputs, encoder_state\n\n\n\n    def build_decoder(self, encoder_outputs, encoder_state):\n\n        sos_id_2 = tf.cast(self.char2ind[self.sos], tf.int32)\n        eos_id_2 = tf.cast(self.char2ind[self.eos], tf.int32)\n        self.output_layer = Dense(self.vocab_size, name=\'output_projection\')\n\n        # Decoder.\n        with tf.variable_scope(""decoder"") as decoder_scope:\n\n            cell, decoder_initial_state = self.build_decoder_cell(\n                encoder_outputs,\n                encoder_state,\n                self.audio_sequence_lengths)\n\n            # Train\n            if self.mode != \'INFER\':\n\n                helper = tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper(\n                    inputs=self.char_embedding,\n                    sequence_length=self.char_sequence_lengths,\n                    embedding=self.embedding,\n                    sampling_probability=0.5,\n                    time_major=False)\n\n                # Decoder\n                my_decoder = tf.contrib.seq2seq.BasicDecoder(cell,\n                                                             helper,\n                                                             decoder_initial_state,\n                                                             output_layer=self.output_layer)\n\n                # Dynamic decoding\n                outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\n                    my_decoder,\n                    output_time_major=False,\n                    maximum_iterations=self.maximum_iterations,\n                    swap_memory=False,\n                    impute_finished=True,\n                    scope=decoder_scope\n                )\n\n                sample_id = outputs.sample_id\n                logits = outputs.rnn_output\n\n\n            # Inference\n            else:\n                start_tokens = tf.fill([self.batch_size], sos_id_2)\n                end_token = eos_id_2\n\n                # Beam search\n                if self.beam_width > 0:\n                    my_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n                        cell=cell,\n                        embedding=self.embedding,\n                        start_tokens=start_tokens,\n                        end_token=end_token,\n                        initial_state=decoder_initial_state,\n                        beam_width=self.beam_width,\n                        output_layer=self.output_layer,\n                    )\n\n                # Greedy\n                else:\n                    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(self.embedding,\n                                                                      start_tokens,\n                                                                      end_token)\n\n                    my_decoder = tf.contrib.seq2seq.BasicDecoder(cell,\n                                                                 helper,\n                                                                 decoder_initial_state,\n                                                                 output_layer=self.output_layer)\n                if self.inference_targets:\n                    maximum_iterations = self.maximum_iterations\n                else:\n                    maximum_iterations = None\n\n                # Dynamic decoding\n                outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\n                    my_decoder,\n                    maximum_iterations=maximum_iterations,\n                    output_time_major=False,\n                    impute_finished=False,\n                    swap_memory=False,\n                    scope=decoder_scope)\n\n                if self.beam_width > 0:\n                    logits = tf.no_op()\n                    sample_id = outputs.predicted_ids\n                else:\n                    logits = tf.no_op()\n                    sample_id = outputs.sample_id\n\n        return logits, sample_id, final_context_state\n\n    def build_decoder_cell(self, encoder_outputs, encoder_state,\n                           audio_sequence_lengths):\n        """"""Builds the attention decoder cell. If mode is inference performs tiling\n           Passes last encoder state.\n        """"""\n\n        memory = encoder_outputs\n\n        if self.mode == \'INFER\' and self.beam_width > 0:\n            memory = tf.contrib.seq2seq.tile_batch(memory,\n                                                   multiplier=self.beam_width)\n            encoder_state = tf.contrib.seq2seq.tile_batch(encoder_state,\n                                                          multiplier=self.beam_width)\n            audio_sequence_lengths = tf.contrib.seq2seq.tile_batch(audio_sequence_lengths,\n                                                                   multiplier=self.beam_width)\n            batch_size = self.batch_size * self.beam_width\n\n        else:\n            batch_size = self.batch_size\n\n        if self.num_layers_decoder is not None:\n            lstm_cell = tf.nn.rnn_cell.MultiRNNCell(\n                [self.make_rnn_cell(self.rnn_size_decoder) for _ in\n                 range(self.num_layers_decoder)])\n\n        else:\n            lstm_cell = self.make_rnn_cell(self.rnn_size_decoder)\n\n        # attention cell\n        cell = self.make_attention_cell(lstm_cell,\n                                        self.rnn_size_decoder,\n                                        memory,\n                                        audio_sequence_lengths)\n\n        decoder_initial_state = cell.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state)\n\n        return cell, decoder_initial_state\n\n\n    def compute_loss(self, logits):\n        """"""Compute the loss during optimization.""""""\n        target_output = self.char_ids\n        max_time = self.maximum_iterations\n\n        target_weights = tf.sequence_mask(self.char_sequence_lengths,\n                                          max_time,\n                                          dtype=tf.float32,\n                                          name=\'mask\')\n\n        loss = tf.contrib.seq2seq.sequence_loss(logits=logits,\n                                                targets=target_output,\n                                                weights=target_weights,\n                                                average_across_timesteps=True,\n                                                average_across_batch=True, )\n        return loss\n\n\n    def train(self,\n              inputs,\n              targets,\n              restore_path=None,\n              validation_inputs=None,\n              validation_targets=None):\n        """"""Performs the training process. Runs training step in every epoch.\n           Shuffles input data before every epoch.\n           Optionally: - add tensorboard summaries.\n                       - restoring previous model and retraining on top.\n                       - evaluation step.\n        """"""\n        assert len(inputs) == len(targets)\n\n        if self.summary_dir is not None:\n            self.add_summary()\n\n        self.initialize_session()\n        if restore_path is not None:\n            self.restore_session(restore_path)\n\n        best_score = np.inf\n        nepoch_no_imprv = 0\n\n        # inputs = np.array(inputs)\n        # targets = np.array(targets)\n\n        for epoch in range(self.epochs + 1):\n            print(\'-------------------- Epoch {} of {} --------------------\'.format(epoch,\n                                                                                    self.epochs))\n            # # # shuffle\n            # indices = np.random.permutation(len(inputs))\n            # inputs = inputs[indices]\n            # targets = targets[indices]\n\n            # run training epoch\n            score = self.run_epoch(inputs, targets, epoch)\n\n            # evaluate model\n            if validation_inputs is not None and validation_targets is not None:\n                self.run_evaluate(validation_inputs, validation_targets, epoch)\n\n\n            if score <= best_score:\n                nepoch_no_imprv = 0\n                if not os.path.exists(self.save_path):\n                    os.makedirs(self.save_path)\n                self.saver.save(self.sess, self.save_path)\n                best_score = score\n                print(""--- new best score ---\\n\\n"")\n            else:\n                # warm up\n                if epoch > 10:\n                    nepoch_no_imprv += 1\n                # early stopping\n                if nepoch_no_imprv >= 5:\n                    print(""- early stopping {} epochs without improvement"".format(nepoch_no_imprv))\n                    break\n\n    def infer(self, inputs, restore_path, targets=None):\n        """"""Runs inference process. No training takes place.\n           Returns the predicted ids for every sentence.\n        """"""\n        self.initialize_session()\n        self.restore_session(restore_path)\n\n        prediction_ids = []\n\n        if targets is not None:\n\n            for (inps, trgts) in sr_model_utils.minibatches(inputs,\n                                                            targets=targets,\n                                                            minibatch_size=self.batch_size):\n                feed, _, char_sequence_lengths = self.get_feed_dict(inps,\n                                                                    trgts=trgts)\n\n                s_ids = self.sess.run([self.sample_words],\n                                      feed_dict=feed)\n\n\n                for s in s_ids:\n                    prediction_ids.append(s)\n\n        else:\n\n            for inps in sr_model_utils.minibatches(inputs,\n                                                   targets=None,\n                                                   minibatch_size=self.batch_size):\n                feed, _ = self.get_feed_dict(inps)\n                s_ids = self.sess.run([self.sample_words],\n                                      feed_dict=feed)\n                for s in s_ids:\n                    prediction_ids.append(s)\n\n        return prediction_ids\n\n    def run_epoch(self, inputs, targets, epoch):\n        """"""Runs a single epoch.\n           Returns the average loss value on the epoch.""""""\n\n        nbatches = (len(inputs) + self.batch_size - 1) // self.batch_size\n        losses = []\n\n        for i, (inps, trgts) in enumerate(sr_model_utils.minibatches(inputs,\n                                                                     targets,\n                                                                     self.batch_size)):\n            fd, sl, s2 = self.get_feed_dict(inps,\n                                            trgts=trgts)\n\n            if i % 10 == 0 and self.summary_dir is not None:\n                _, train_loss, training_summ = self.sess.run([self.train_op,\n                                                              self.train_loss,\n                                                              self.training_summary],\n                                                             feed_dict=fd)\n                self.training_writer.add_summary(training_summ, epoch*nbatches + i)\n\n            else:\n                _, train_loss = self.sess.run([self.train_op, self.train_loss],\n                                              feed_dict=fd)\n\n            if i % 5 == 0 or i == (nbatches - 1):\n                print(\'Iteration: {} of {}\\ttrain_loss: {:.4f}\'.format(i, nbatches - 1, train_loss))\n            losses.append(train_loss)\n\n\n        avg_loss = self.sess.run(tf.reduce_mean(losses))\n        print(\'Average Score for this Epoch: {}\'.format(avg_loss))\n\n        return avg_loss\n\n    def run_evaluate(self, inputs, targets, epoch):\n        """"""Runs evaluation on validation inputs and targets.\n        Optionally: - writes summary to Tensorboard.\n        """"""\n        if self.summary_dir is not None:\n            eval_losses = []\n            for inps, trgts in sr_model_utils.minibatches(inputs, targets, self.batch_size):\n                fd, sl, s2 = self.get_feed_dict(inps, trgts)\n                eval_loss = self.sess.run([self.eval_loss], feed_dict=fd)\n                eval_losses.append(eval_loss)\n\n            avg_eval_loss = self.sess.run(tf.reduce_mean(eval_losses))\n\n            print(\'Eval_loss: {}\\n\'.format(avg_eval_loss))\n            eval_summ = self.sess.run([self.eval_summary], feed_dict=fd)\n            self.eval_writer.add_summary(eval_summ, epoch)\n\n        else:\n            eval_losses = []\n            for inps, trgts in sr_model_utils.minibatches(inputs, targets, self.batch_size):\n                fd, sl, s2 = self.get_feed_dict(inps, trgts)\n                eval_loss = self.sess.run([self.eval_loss], feed_dict=fd)\n                eval_losses.append(eval_loss)\n\n            avg_eval_loss = self.sess.run(tf.reduce_mean(eval_losses))\n\n            print(\'Eval_loss: {}\\n\'.format(avg_eval_loss))\n\n\n\n    def get_feed_dict(self, inps, trgts=None):\n        """"""Creates the feed_dict that is fed into training or inference network.\n           Pads inputs and targets.\n           Returns feed_dict and sequence_length(s) depending on training mode.\n        """"""\n        if self.mode != \'INFER\':\n\n            inp_ids, audio_sequence_lengths = sr_model_utils.pad_audio_sequences(inps)\n            feed = {\n                self.audios: inp_ids,\n                self.audio_sequence_lengths: audio_sequence_lengths,\n            }\n\n            if trgts is not None:\n                trgt_ids, char_sequence_lengths = sr_model_utils.pad_txt_sequences(\n                    trgts,\n                    self.char2ind[self.pad]\n                )\n                feed[self.char_ids] = trgt_ids\n                feed[self.char_sequence_lengths] = char_sequence_lengths\n\n                return feed, audio_sequence_lengths, char_sequence_lengths\n\n            else:\n                return feed, audio_sequence_lengths\n\n        else:\n            inp_ids, audio_sequence_lengths = sr_model_utils.pad_audio_sequences(inps)\n\n            feed = {\n                self.audios: inp_ids,\n                self.audio_sequence_lengths: audio_sequence_lengths\n            }\n\n            if trgts is not None:\n                _, char_sequence_lengths = sr_model_utils.pad_txt_sequences(\n                    trgts,\n                    self.char2ind[self.pad],\n                )\n\n                feed[self.char_sequence_lengths] = char_sequence_lengths\n\n                return feed, audio_sequence_lengths, char_sequence_lengths\n            else:\n                return feed, audio_sequence_lengths\n\n    def initialize_session(self):\n        self.sess = tf.Session()\n        self.sess.run(tf.global_variables_initializer())\n\n    def restore_session(self, restore_path):\n        self.saver.restore(self.sess, restore_path)\n        print(\'Done restoring.\')\n\n    def add_summary(self):\n        """"""Summaries for Tensorboard.""""""\n        self.training_summary = tf.summary.scalar(\'training_loss\', self.train_loss)\n        self.eval_summary = tf.summary.scalar(\'evaluation_loss\', self.eval_loss)\n        self.training_writer = tf.summary.FileWriter(self.summary_dir,\n                                                     tf.get_default_graph())\n        self.eval_writer = tf.summary.FileWriter(self.summary_dir)\n'"
sr_data_utils.py,0,"b'import os\nimport re\nimport sys\nimport urllib\nfrom pathlib import Path\nimport pickle\n\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nfrom python_speech_features import mfcc\n\n\n\n# http://www.openslr.org/resources/12/dev-clean.tar.gz\ndef report_hook(count, block_size, total_size):\n    """"""\n    Shows the downloading progress.\n    """"""\n    percent = count * block_size * 100 // total_size\n    sys.stdout.write(""\\rDownloading: {}%"".format(percent))\n    sys.stdout.flush()\n\n# TODO: use pathlib instead of os.\ndef download_file(download_path, save_path, file_name):\n    """"""\n    Downloads data and created dirs if necessary.\n    Args:\n        download_path: String. path to download data from.\n        save_path: String. path to save data to.\n        file_name: String. name to save tdata with.\n\n    """"""\n    if os.path.exists(save_path):\n        if file_name in os.listdir(save_path):\n            return\n        else:\n            file_path = os.path.join(save_path, file_name)\n            urllib.request.urlretrieve(download_path, file_path, report_hook)\n    else:\n        os.makedirs(save_path)\n        file_path = os.path.join(save_path, file_name)\n        urllib.request.urlretrieve(download_path, file_path, report_hook)\n\n\ndef plot_wave(path):\n    """"""\n    Args:\n        path: Path to the audio file we want to plot\n    """"""\n    samples, sample_rate = librosa.load(path, mono=True, sr=None)\n    plt.figure(figsize=[15, 5])\n    librosa.display.waveplot(samples, sr=sample_rate)\n    plt.show()\n\n\ndef plot_melspectogram(path, n_mels=128):\n    """"""\n    Args:\n        path: The path to to the audiofile we want to plot.\n    """"""\n    samples, sample_rate = librosa.load(path, mono=True, sr=None)\n    plt.figure(figsize=[20, 5])\n    S = librosa.feature.melspectrogram(samples, sr=sample_rate, n_mels=n_mels)\n    log_S = librosa.power_to_db(S, ref=np.max)\n    librosa.display.specshow(log_S)\n    plt.show()\n\n\n\ndef audioToInputVector(audio_filename, numcep, numcontext):\n    """"""\n    Given a WAV audio file at ``audio_filename``, calculates ``numcep`` MFCC features\n    at every 0.01s time step with a window length of 0.025s. Appends ``numcontext``\n    context frames to the left and right of each time step, and returns this data\n    in a numpy array.\n\n    Borrowed from Mozilla\'s Deep Speech and slightly modified.\n    https://github.com/mozilla/DeepSpeech\n    """"""\n\n    audio, fs = librosa.load(audio_filename)\n\n    # # Get mfcc coefficients\n    features = mfcc(audio, samplerate=fs, numcep=numcep, nfft=551)\n    # features = librosa.feature.mfcc(y=audio,\n    #                                 sr=fs,\n    #                                 n_fft=551,\n    #                                 n_mfcc=numcep).T\n\n    # We only keep every second feature (BiRNN stride = 2)\n    features = features[::2]\n\n    # One stride per time step in the input\n    num_strides = len(features)\n\n    # Add empty initial and final contexts\n    empty_context = np.zeros((numcontext, numcep), dtype=features.dtype)\n\n    features = np.concatenate((empty_context, features, empty_context))\n\n    # Create a view into the array with overlapping strides of size\n    # numcontext (past) + 1 (present) + numcontext (future)\n    window_size = 2 * numcontext + 1\n    train_inputs = np.lib.stride_tricks.as_strided(\n        features,\n        (num_strides, window_size, numcep),\n        (features.strides[0], features.strides[0], features.strides[1]),\n        writeable=False)\n\n    # Flatten the second and third dimensions\n    train_inputs = np.reshape(train_inputs, [num_strides, -1])\n\n    # Whiten inputs (TODO: Should we whiten?)\n    # Copy the strided array so that we can write to it safely\n    train_inputs = np.copy(train_inputs)\n    train_inputs = (train_inputs - np.mean(train_inputs)) / np.std(train_inputs)\n\n    # Return results\n    return train_inputs\n\n\ndef load_data(dir_path, how_many=0):\n    """"""\n\n    Args:\n        dir_path: path to the directory with txt and audio files.\n        how_many: Integer. Number of directories we want to iterate,\n                  that contain the audio files and transcriptions.\n\n    Returns:\n        txts: The spoken texts extracted from the .txt files,\n              which correspond to the .flac files in audios.\n              Text version.\n        audios: The .flac file paths corresponding to the\n                sentences in txts. Spoken version.\n\n    """"""\n    dir_path = Path(dir_path)\n    txt_list = [f for f in dir_path.glob(\'**/*.txt\') if f.is_file()]\n    audio_list = [f for f in dir_path.glob(\'**/*.flac\') if f.is_file()]\n\n    print(\'Number of audio txt paths:\', len(txt_list))\n    print(\'Number of audio file paths:\', len(audio_list))\n\n    txts = []\n    audios = []\n    audio_paths = []\n\n    # for development we want to reduce the numbers of files we read in.\n    if how_many == 0:\n        how_many = len(txt_list)\n\n    for i, txt in enumerate(txt_list[:how_many]):\n        print(\'Text#:\', i+1)\n        with open(txt) as f:\n            for line in f.readlines():\n                for audio in audio_list:\n                    if audio.stem in line:\n                        line = re.sub(r\'[^A-Za-z]\', \' \', line)\n                        line = line.strip()\n                        txts.append(line)\n                        audios.append(audioToInputVector(audio, 26, 9))\n                        audio_paths.append(audio)\n                        break\n    return txts, audios, audio_paths\n\n\ndef split_txts(txts):\n    """"""\n    Args:\n        txts: The texts that will be split\n              into single characters\n\n    Returns:\n        The splitted texts and array of all unique characters\n        in those texts.\n\n    """"""\n    txts_splitted = []\n    unique_chars = set()\n\n    for txt in txts:\n        splitted = list(txt)\n        splitted = [ch if ch != \' \' else \'<SPACE>\' for ch in splitted]\n        txts_splitted.append(splitted)\n        unique_chars.update(splitted)\n    return txts_splitted, sorted(unique_chars)\n\n\ndef create_lookup_dicts(unique_chars, specials=None):\n    """"""\n\n    Args:\n        unique_chars: Set of unique chars appearning in texts.\n        specials: Special characters we want to add to the dict,\n                  such as <PAD>, <SOS> or <EOS>\n\n    Returns:\n        char2ind: look updict from character to index\n        ind2char: lookup dict from index to character\n\n    """"""\n    char2ind = {}\n    ind2char = {}\n    i = 0\n\n    if specials is not None:\n        for sp in specials:\n            char2ind[sp] = i\n            ind2char[i] = sp\n            i += 1\n    for ch in unique_chars:\n        char2ind[ch] = i\n        ind2char[i] = ch\n        i += 1\n    return char2ind, ind2char\n\n\ndef convert_txt_to_inds(txt, char2ind, eos=False, sos=False):\n    """"""\n\n    Args:\n        txt: Array of chars to convert to inds.\n        char2ind: Lookup dict from chars to inds.\n\n    Returns: The converted chars, i.e. array of ints.\n\n    """"""\n    txt_to_inds = [char2ind[ch] for ch in txt]\n    if eos:\n        txt_to_inds.append(char2ind[\'<EOS>\'])\n    if sos:\n        txt_to_inds.insert(0, char2ind[\'<SOS>\'])\n    return txt_to_inds\n\n\ndef convert_inds_to_txt(inds, ind2char):\n    """"""\n\n    Args:\n        inds: Array of ints to convert to chars\n        ind2char: Lookup dict from ind to chars\n\n    Returns: The converted inds, i.e. array of chars.\n\n    """"""\n    inds_to_txt = [ind2char[ind] for ind in inds]\n    return inds_to_txt\n\n\ndef process_txts(txts, specials):\n    """"""\n    Processes the texts. Calls the functions split_txts,\n    create_lookup_dicts and uses convert_txt_to_inds.\n\n    Args:\n        txts: Array of strings. Input texts.\n        specials: Specials tokens we want to include in the\n                  lookup dicts\n\n    Returns:\n        txts_splitted: Array of the input texts splitted up into\n                       characters\n        unique_chars: Set of Unique chars appearing in input texts.\n        char2ind: Lookup dict from character to index.\n        ind2char: Lookup dict from index to character.\n        txts_converted: txts splitted converted to indices of\n                        word2ind. i.e. array of arrays of ints.\n\n    """"""\n    txts_splitted, unique_chars = split_txts(txts)\n    char2ind, ind2char = create_lookup_dicts(unique_chars, specials)\n    txts_converted = [convert_txt_to_inds(txt, char2ind, eos=True, sos=True)\n                      for txt in txts_splitted]\n\n    return txts_splitted, unique_chars, char2ind, ind2char, txts_converted\n\n\ndef sort_by_length(audios,\n                   txts,\n                   audio_paths,\n                   txts_splitted,\n                   txts_converted,\n                   by_text_length=True):\n    """"""\n    Sort texts by text length from shortest to longest.\n    To keep everything in order we also sort the rest of the data.\n\n    Args:\n        by_text_length: Boolean. Sort either by text lengths or\n                        by length of audios.\n\n    Returns:\n\n    """"""\n\n    # check if that works. if not audios isn\'t a  numpy array.\n    # in that case we could convert beforehand.\n    if by_text_length:\n        indices = [txt[0] for txt in sorted(enumerate(txts_converted), key=lambda x: len(x[1]))]\n    else:\n        indices = [a[0] for a in sorted(enumerate(audios), key=lambda x: x[1].shape[0])]\n    txts_sorted = np.array(txts)[indices]\n    audios_sorted = np.array(audios)[indices]\n    audio_paths_sorted = np.array(audio_paths)[indices]\n    txts_splitted_sorted = np.array(txts_splitted)[indices]\n    txts_converted_sorted = np.array(txts_converted)[indices]\n\n    return txts_sorted, audios_sorted, audio_paths_sorted, txts_splitted_sorted, txts_converted_sorted\n\n\ndef preds2txt(preds,\n              ind2char,\n              beam=False):\n    """"""\n    Converts the predictions to text and removes\n    <SOS> and <EOS> tokens\n    Args:\n        preds: the predictions. output of either\n               greedy or beam search decoding.\n        ind2char: Lookup dict from index to character.\n        beam: Boolean. Wheter preds is the output\n              of greedy or beam search decoding.\n\n    Returns:\n        p2t: The converted predictions,\n             i.e. the predicted text.\n\n    """"""\n    if beam:\n        p2t = []\n        for batch in preds:\n            for sentence in batch:\n                converted_sentence = []\n                for p in sentence:\n                    converted_ch = ind2char[p[0]]\n                    if converted_ch != \'<EOS>\' and converted_ch != \'<SOS>\':\n                        converted_sentence.append(converted_ch)\n                p2t.append(converted_sentence)\n                converted_sentence = []\n    else:\n        p2t = []\n        for batch in preds:\n            for sentence in batch:\n                converted_sentence = convert_inds_to_txt(sentence, ind2char)\n                converted_sentence = [ch for ch in converted_sentence\n                                      if ch != \'<EOS>\' and ch != \'<SOS>\']\n                p2t.append(converted_sentence)\n\n    return p2t\n\n\ndef print_samples(preds, targets):\n    """"""\n    Print predicted text and actual text side by side\n    and measures the accuracy of the predicted\n    characters. If we produce shorter texts than\n    the actual ones we penalize the acc score.\n\n    Args:\n        preds: Array of converted sentences.\n        targets: The actual sentences.\n    """"""\n    accs = []\n    for p, t in zip(preds, targets):\n        if len(p) >= len(t):\n            acc_score = accuracy_score(p[:len(t)], t)\n        else:\n            acc_score = accuracy_score(p, t[:len(p)]) - 0.3\n\n        accs.append(acc_score)\n        print(\'Created:\', p)\n        print(\'Actual:\', t)\n        print(\'Accuracy score:\', acc_score, \'\\n\\n\')\n\n    print(\'Mean acc score:\', np.mean(accs))\n\n\ndef save_as_pickled_object(obj, filepath):\n    """"""\n    This is a defensive way to write pickle.write, allowing for very large files on all platforms\n    https://stackoverflow.com/questions/31468117/python-3-can-pickle-handle-byte-objects-larger-than-4gb\n    """"""\n    max_bytes = 2 ** 31 - 1\n    bytes_out = pickle.dumps(obj)\n    n_bytes = sys.getsizeof(bytes_out)\n    with open(filepath, \'wb\') as f_out:\n        for idx in range(0, n_bytes, max_bytes):\n            f_out.write(bytes_out[idx:idx + max_bytes])\n\n\ndef write_pkl(path, data):\n    """"""\n    Writes the given data to .pkl file.\n    """"""\n    with open(path, \'wb\') as f:\n        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n\n\ndef load_pkl(path):\n    """"""\n    Loads data from given path to .pkl file.\n    """"""\n    with open(path, \'rb\') as f:\n        data = pickle.load(f)\n    return data\n\n# TODO: implement mean edit distance (levensthein distance)\n# https://www.tensorflow.org/api_docs/python/tf/edit_distance'"
sr_model_utils.py,2,"b'import numpy as np\nimport tensorflow as tf\n\ndef minibatches(inputs, targets=None, minibatch_size=64):\n    """"""batch generator. yields x and y batch.\n    """"""\n    x_batch, y_batch = [], []\n\n    if targets is not None:\n        for inp, tgt in zip(inputs, targets):\n            if len(x_batch) == minibatch_size and len(y_batch) == minibatch_size:\n                yield x_batch, y_batch\n                x_batch, y_batch = [], []\n            x_batch.append(inp)\n            y_batch.append(tgt)\n\n        if len(x_batch) != 0:\n            for inp, tgt in zip(inputs, targets):\n                if len(x_batch) != minibatch_size:\n                    x_batch.append(inp)\n                    y_batch.append(tgt)\n                else:\n                    break\n            yield x_batch, y_batch\n    else:\n        for inp in inputs:\n            if len(x_batch) == minibatch_size:\n                yield x_batch\n                x_batch = []\n            x_batch.append(inp)\n\n        if len(x_batch) != 0:\n            for inp in inputs:\n                if len(x_batch) != minibatch_size:\n                    x_batch.append(inp)\n                else:\n                    break\n            yield x_batch\n\n\ndef pad_txt_sequences(sequences, pad_tok):\n    """"""Pads the sentences, so that all sentences in a batch have the same length.\n    """"""\n\n    max_length = max(len(x) for x in sequences)\n\n    sequence_padded, sequence_length = [], []\n\n    for seq in sequences:\n        seq = list(seq)\n        seq_ = seq + [pad_tok] * max(max_length - len(seq), 0)\n\n        sequence_padded += [seq_]\n        sequence_length += [len(seq)]\n\n    return sequence_padded, sequence_length\n\ndef pad_audio_sequences(sequences, tail=True):\n    """"""\n\n    Args:\n        sequences: Array of audio sequences\n        tail: Boolean. Append silence to end or beginning\n\n    Returns: Padded array with audio sequences, padded with\n             silence.\n\n    """"""\n\n    max_length = max(seq.shape[0] for seq in sequences)\n\n    sequences_padded, sequence_length = [], []\n\n    for seq in sequences:\n        if tail:\n            seq_shape = seq.shape\n            pad_vector = [0] * seq_shape[1]\n            n_vectors_to_add = max_length - seq_shape[0]\n\n            for _ in range(n_vectors_to_add):\n                seq = np.append(seq, [pad_vector], axis=0)\n\n        sequences_padded.append(seq)\n        sequence_length.append(seq_shape[0])\n\n\n    return sequences_padded, sequence_length\n\n\ndef reset_graph(seed=97):\n    """"""helper function to reset the default graph. this often\n       comes handy when using jupyter noteboooks.\n    """"""\n    tf.reset_default_graph()\n    tf.set_random_seed(seed)\n    np.random.seed(seed)\n'"
