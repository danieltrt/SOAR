file_path,api_count,code
Summarizer.py,61,"b'import os\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.python.layers.core import Dense\n\nimport summarizer_model_utils\n\n\nclass Summarizer:\n\n    def __init__(self,\n                 word2ind,\n                 ind2word,\n                 save_path,\n                 mode=\'TRAIN\',\n                 num_layers_encoder=1,\n                 num_layers_decoder=1,\n                 embedding_dim=300,\n                 rnn_size_encoder=256,\n                 rnn_size_decoder=256,\n                 learning_rate=0.001,\n                 learning_rate_decay=0.9,\n                 learning_rate_decay_steps=100,\n                 max_lr=0.01,\n                 keep_probability=0.8,\n                 batch_size=64,\n                 beam_width=10,\n                 epochs=20,\n                 eos=""<EOS>"",\n                 sos=""<SOS>"",\n                 pad=\'<PAD>\',\n                 clip=5,\n                 inference_targets=False,\n                 pretrained_embeddings_path=None,\n                 summary_dir=None,\n                 use_cyclic_lr=False):\n        """"""\n\n        Args:\n            word2ind: lookup dict from word to index.\n            ind2word: lookup dict from index to word.\n            save_path: path to save the tf model to in the end.\n            mode: String. \'TRAIN\' or \'INFER\'. depending on which mode we use\n                  a different graph is created.\n            num_layers_encoder: Float. Number of encoder layers. defaults to 1.\n            num_layers_decoder: Float. Number of decoder layers. defaults to 1.\n            embedding_dim: dimension of the embedding vectors in the embedding matrix.\n                           every word has a embedding_dim \'long\' vector.\n            rnn_size_encoder: Integer. number of hidden units in encoder. defaults to 256.\n            rnn_size_decoder: Integer. number of hidden units in decoder. defaults to 256.\n            learning_rate: Float.\n            learning_rate_decay: only if exponential learning rate is used.\n            learning_rate_decay_steps: Integer.\n            max_lr: only used if cyclic learning rate is used.\n            keep_probability: Float.\n            batch_size: Integer. Size of minibatches.\n            beam_width: Integer. Only used in inference, for Beam Search.(\'INFER\'-mode)\n            epochs: Integer. Number of times the training is conducted\n                    on the whole training data.\n            eos: EndOfSentence tag.\n            sos: StartOfSentence tag.\n            pad: Padding tag.\n            clip: Value to clip the gradients to in training process.\n            inference_targets:\n            pretrained_embeddings_path: Path to pretrained embeddings. Has to be .npy\n            summary_dir: Directory the summaries are written to for tensorboard.\n            use_cyclic_lr: Boolean.\n        """"""\n\n        self.word2ind = word2ind\n        self.ind2word = ind2word\n        self.vocab_size = len(word2ind)\n        self.num_layers_encoder = num_layers_encoder\n        self.num_layers_decoder = num_layers_decoder\n        self.rnn_size_encoder = rnn_size_encoder\n        self.rnn_size_decoder = rnn_size_decoder\n        self.save_path = save_path\n        self.embedding_dim = embedding_dim\n        self.mode = mode.upper()\n        self.learning_rate = learning_rate\n        self.learning_rate_decay = learning_rate_decay\n        self.learning_rate_decay_steps = learning_rate_decay_steps\n        self.keep_probability = keep_probability\n        self.batch_size = batch_size\n        self.beam_width = beam_width\n        self.eos = eos\n        self.sos = sos\n        self.clip = clip\n        self.pad = pad\n        self.epochs = epochs\n        self.inference_targets = inference_targets\n        self.pretrained_embeddings_path = pretrained_embeddings_path\n        self.use_cyclic_lr = use_cyclic_lr\n        self.max_lr = max_lr\n        self.summary_dir = summary_dir\n\n    def build_graph(self):\n        self.add_placeholders()\n        self.add_embeddings()\n        self.add_lookup_ops()\n        self.initialize_session()\n        self.add_seq2seq()\n        self.saver = tf.train.Saver()\n        print(\'Graph built.\')\n\n    def add_placeholders(self):\n        self.ids_1 = tf.placeholder(tf.int32,\n                                    shape=[None, None],\n                                    name=\'ids_source\')\n        self.ids_2 = tf.placeholder(tf.int32,\n                                    shape=[None, None],\n                                    name=\'ids_target\')\n        self.sequence_lengths_1 = tf.placeholder(tf.int32,\n                                                 shape=[None],\n                                                 name=\'sequence_length_source\')\n        self.sequence_lengths_2 = tf.placeholder(tf.int32,\n                                                 shape=[None],\n                                                 name=\'sequence_length_target\')\n        self.maximum_iterations = tf.reduce_max(self.sequence_lengths_2,\n                                                name=\'max_dec_len\')\n\n    def create_word_embedding(self, embed_name, vocab_size, embed_dim):\n        """"""Creates embedding matrix in given shape - [vocab_size, embed_dim].\n        """"""\n        embedding = tf.get_variable(embed_name,\n                                    shape=[vocab_size, embed_dim],\n                                    dtype=tf.float32)\n        return embedding\n\n    def add_embeddings(self):\n        """"""Creates the embedding matrix. In case path to pretrained embeddings is given,\n           that embedding is loaded. Otherwise created.\n        """"""\n        if self.pretrained_embeddings_path is not None:\n            self.embedding = tf.Variable(np.load(self.pretrained_embeddings_path),\n                                         name=\'embedding\')\n            print(\'Loaded pretrained embeddings.\')\n        else:\n            self.embedding = self.create_word_embedding(\'embedding\',\n                                                        self.vocab_size,\n                                                        self.embedding_dim)\n\n    def add_lookup_ops(self):\n        """"""Additional lookup operation for both source embedding and target embedding matrix.\n        """"""\n        self.word_embeddings_1 = tf.nn.embedding_lookup(self.embedding,\n                                                        self.ids_1,\n                                                        name=\'word_embeddings_1\')\n        self.word_embeddings_2 = tf.nn.embedding_lookup(self.embedding,\n                                                        self.ids_2,\n                                                        name=\'word_embeddings_2\')\n\n    def make_rnn_cell(self, rnn_size, keep_probability):\n        """"""Creates LSTM cell wrapped with dropout.\n        """"""\n        cell = tf.nn.rnn_cell.LSTMCell(rnn_size)\n        cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=keep_probability)\n        return cell\n\n    def make_attention_cell(self, dec_cell, rnn_size, enc_output, lengths, alignment_history=False):\n        """"""Wraps the given cell with Bahdanau Attention.\n        """"""\n        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units=rnn_size,\n                                                                   memory=enc_output,\n                                                                   memory_sequence_length=lengths,\n                                                                   name=\'BahdanauAttention\')\n\n        return tf.contrib.seq2seq.AttentionWrapper(cell=dec_cell,\n                                                   attention_mechanism=attention_mechanism,\n                                                   attention_layer_size=None,\n                                                   output_attention=False,\n                                                   alignment_history=alignment_history)\n\n    def triangular_lr(self, current_step):\n        """"""cyclic learning rate - exponential range.""""""\n        step_size = self.learning_rate_decay_steps\n        base_lr = self.learning_rate\n        max_lr = self.max_lr\n\n        cycle = tf.floor(1 + current_step / (2 * step_size))\n        x = tf.abs(current_step / step_size - 2 * cycle + 1)\n        lr = base_lr + (max_lr - base_lr) * tf.maximum(0.0, tf.cast((1.0 - x), dtype=tf.float32)) * (0.99999 ** tf.cast(\n            current_step,\n            dtype=tf.float32))\n        return lr\n\n\n    def add_seq2seq(self):\n        """"""Creates the sequence to sequence architecture.""""""\n        with tf.variable_scope(\'dynamic_seq2seq\', dtype=tf.float32):\n            # Encoder\n            encoder_outputs, encoder_state = self.build_encoder()\n\n            # Decoder\n            logits, sample_id, final_context_state = self.build_decoder(encoder_outputs,\n                                                                        encoder_state)\n            if self.mode == \'TRAIN\':\n\n                # Loss\n                loss = self.compute_loss(logits)\n                self.train_loss = loss\n                self.eval_loss = loss\n                self.global_step = tf.Variable(0, trainable=False)\n\n\n                # cyclic learning rate\n                if self.use_cyclic_lr:\n                    self.learning_rate = self.triangular_lr(self.global_step)\n\n                # exponential learning rate\n                else:\n                    self.learning_rate = tf.train.exponential_decay(\n                        self.learning_rate,\n                        self.global_step,\n                        decay_steps=self.learning_rate_decay_steps,\n                        decay_rate=self.learning_rate_decay,\n                        staircase=True)\n\n                # Optimizer\n                opt = tf.train.AdamOptimizer(self.learning_rate)\n\n\n                # Gradients\n                if self.clip > 0:\n                    grads, vs = zip(*opt.compute_gradients(self.train_loss))\n                    grads, _ = tf.clip_by_global_norm(grads, self.clip)\n                    self.train_op = opt.apply_gradients(zip(grads, vs),\n                                                        global_step=self.global_step)\n                else:\n                    self.train_op = opt.minimize(self.train_loss,\n                                                 global_step=self.global_step)\n\n\n\n            elif self.mode == \'INFER\':\n                loss = None\n                self.infer_logits, _, self.final_context_state, self.sample_id = logits, loss, final_context_state, sample_id\n                self.sample_words = self.sample_id\n\n    def build_encoder(self):\n        """"""The encoder. Bidirectional LSTM.""""""\n\n        with tf.variable_scope(""encoder""):\n            fw_cell = self.make_rnn_cell(self.rnn_size_encoder // 2, self.keep_probability)\n            bw_cell = self.make_rnn_cell(self.rnn_size_encoder // 2, self.keep_probability)\n\n            for _ in range(self.num_layers_encoder):\n                (out_fw, out_bw), (state_fw, state_bw) = tf.nn.bidirectional_dynamic_rnn(\n                    cell_fw=fw_cell,\n                    cell_bw=bw_cell,\n                    inputs=self.word_embeddings_1,\n                    sequence_length=self.sequence_lengths_1,\n                    dtype=tf.float32)\n                encoder_outputs = tf.concat((out_fw, out_bw), -1)\n\n            bi_state_c = tf.concat((state_fw.c, state_bw.c), -1)\n            bi_state_h = tf.concat((state_fw.h, state_bw.h), -1)\n            bi_lstm_state = tf.nn.rnn_cell.LSTMStateTuple(c=bi_state_c, h=bi_state_h)\n            encoder_state = tuple([bi_lstm_state] * self.num_layers_encoder)\n\n            return encoder_outputs, encoder_state\n\n\n    def build_decoder(self, encoder_outputs, encoder_state):\n\n        sos_id_2 = tf.cast(self.word2ind[self.sos], tf.int32)\n        eos_id_2 = tf.cast(self.word2ind[self.eos], tf.int32)\n        self.output_layer = Dense(self.vocab_size, name=\'output_projection\')\n\n        # Decoder.\n        with tf.variable_scope(""decoder"") as decoder_scope:\n\n            cell, decoder_initial_state = self.build_decoder_cell(\n                encoder_outputs,\n                encoder_state,\n                self.sequence_lengths_1)\n\n            # Train\n            if self.mode != \'INFER\':\n\n                helper = tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper(\n                    inputs=self.word_embeddings_2,\n                    sequence_length=self.sequence_lengths_2,\n                    embedding=self.embedding,\n                    sampling_probability=0.5,\n                    time_major=False)\n\n                # Decoder\n                my_decoder = tf.contrib.seq2seq.BasicDecoder(cell,\n                                                             helper,\n                                                             decoder_initial_state,\n                                                             output_layer=self.output_layer)\n\n                # Dynamic decoding\n                outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\n                    my_decoder,\n                    output_time_major=False,\n                    maximum_iterations=self.maximum_iterations,\n                    swap_memory=False,\n                    impute_finished=True,\n                    scope=decoder_scope\n                )\n\n                sample_id = outputs.sample_id\n                logits = outputs.rnn_output\n\n\n            # Inference\n            else:\n                start_tokens = tf.fill([self.batch_size], sos_id_2)\n                end_token = eos_id_2\n\n                # beam search\n                if self.beam_width > 0:\n                    my_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n                        cell=cell,\n                        embedding=self.embedding,\n                        start_tokens=start_tokens,\n                        end_token=end_token,\n                        initial_state=decoder_initial_state,\n                        beam_width=self.beam_width,\n                        output_layer=self.output_layer,\n                    )\n\n                # greedy\n                else:\n                    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(self.embedding,\n                                                                      start_tokens,\n                                                                      end_token)\n\n                    my_decoder = tf.contrib.seq2seq.BasicDecoder(cell,\n                                                                 helper,\n                                                                 decoder_initial_state,\n                                                                 output_layer=self.output_layer)\n                if self.inference_targets:\n                    maximum_iterations = self.maximum_iterations\n                else:\n                    maximum_iterations = None\n\n                # Dynamic decoding\n                outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\n                    my_decoder,\n                    maximum_iterations=maximum_iterations,\n                    output_time_major=False,\n                    impute_finished=False,\n                    swap_memory=False,\n                    scope=decoder_scope)\n\n                if self.beam_width > 0:\n                    logits = tf.no_op()\n                    sample_id = outputs.predicted_ids\n                else:\n                    logits = outputs.rnn_output\n                    sample_id = outputs.sample_id\n\n        return logits, sample_id, final_context_state\n\n    def build_decoder_cell(self, encoder_outputs, encoder_state,\n                           sequence_lengths_1):\n        """"""Builds the attention decoder cell. If mode is inference performs tiling\n           Passes last encoder state.\n        """"""\n\n        memory = encoder_outputs\n\n        if self.mode == \'INFER\' and self.beam_width > 0:\n            memory = tf.contrib.seq2seq.tile_batch(memory,\n                                                   multiplier=self.beam_width)\n            encoder_state = tf.contrib.seq2seq.tile_batch(encoder_state,\n                                                          multiplier=self.beam_width)\n            sequence_lengths_1 = tf.contrib.seq2seq.tile_batch(sequence_lengths_1,\n                                                               multiplier=self.beam_width)\n            batch_size = self.batch_size * self.beam_width\n\n        else:\n            batch_size = self.batch_size\n\n        # MY APPROACH\n        if self.num_layers_decoder is not None:\n            lstm_cell = tf.nn.rnn_cell.MultiRNNCell(\n                [self.make_rnn_cell(self.rnn_size_decoder, self.keep_probability) for _ in\n                 range(self.num_layers_decoder)])\n\n        else:\n            lstm_cell = self.make_rnn_cell(self.rnn_size_decoder, self.keep_probability)\n\n        # attention cell\n        cell = self.make_attention_cell(lstm_cell,\n                                        self.rnn_size_decoder,\n                                        memory,\n                                        sequence_lengths_1)\n\n        decoder_initial_state = cell.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state)\n\n        return cell, decoder_initial_state\n\n\n    def compute_loss(self, logits):\n        """"""Compute the loss during optimization.""""""\n        target_output = self.ids_2\n        max_time = self.maximum_iterations\n\n        target_weights = tf.sequence_mask(self.sequence_lengths_2,\n                                          max_time,\n                                          dtype=tf.float32,\n                                          name=\'mask\')\n\n        loss = tf.contrib.seq2seq.sequence_loss(logits=logits,\n                                                targets=target_output,\n                                                weights=target_weights,\n                                                average_across_timesteps=True,\n                                                average_across_batch=True, )\n        return loss\n\n\n    def train(self,\n              inputs,\n              targets,\n              restore_path=None,\n              validation_inputs=None,\n              validation_targets=None):\n        """"""Performs the training process. Runs training step in every epoch.\n           Shuffles input data before every epoch.\n           Optionally: - add tensorboard summaries.\n                       - restoring previous model and retraining on top.\n                       - evaluation step.\n        """"""\n        assert len(inputs) == len(targets)\n\n        if self.summary_dir is not None:\n            self.add_summary()\n\n        self.initialize_session()\n        if restore_path is not None:\n            self.restore_session(restore_path)\n\n        best_score = np.inf\n        nepoch_no_imprv = 0\n\n        inputs = np.array(inputs)\n        targets = np.array(targets)\n\n        for epoch in range(self.epochs + 1):\n            print(\'-------------------- Epoch {} of {} --------------------\'.format(epoch,\n                                                                                    self.epochs))\n\n            # shuffle the input data before every epoch.\n            shuffle_indices = np.random.permutation(len(inputs))\n            inputs = inputs[shuffle_indices]\n            targets = targets[shuffle_indices]\n\n            # run training epoch\n            score = self.run_epoch(inputs, targets, epoch)\n\n            # evaluate model\n            if validation_inputs is not None and validation_targets is not None:\n                self.run_evaluate(validation_inputs, validation_targets, epoch)\n\n\n            if score <= best_score:\n                nepoch_no_imprv = 0\n                if not os.path.exists(self.save_path):\n                    os.makedirs(self.save_path)\n                self.saver.save(self.sess, self.save_path)\n                best_score = score\n                print(""--- new best score ---\\n\\n"")\n            else:\n                # warm up epochs for the model\n                if epoch > 10:\n                    nepoch_no_imprv += 1\n                # early stopping\n                if nepoch_no_imprv >= 5:\n                    print(""- early stopping {} epochs without improvement"".format(nepoch_no_imprv))\n                    break\n\n    def infer(self, inputs, restore_path, targets=None):\n        """"""Runs inference process. No training takes place.\n           Returns the predicted ids for every sentence.\n        """"""\n        self.initialize_session()\n        self.restore_session(restore_path)\n\n        prediction_ids = []\n        if targets is not None:\n            feed, _, sequence_lengths_2 = self.get_feed_dict(inputs, trgts=targets)\n        else:\n            feed, _ = self.get_feed_dict(inputs)\n\n        infer_logits, s_ids = self.sess.run([self.infer_logits, self.sample_words], feed_dict=feed)\n        prediction_ids.append(s_ids)\n\n        # for (inps, trgts) in summarizer_model_utils.minibatches(inputs, targets, self.batch_size):\n        #     feed, _, sequence_lengths= self.get_feed_dict(inps, trgts=trgts)\n        #     infer_logits, s_ids = self.sess.run([self.infer_logits, self.sample_words], feed_dict = feed)\n        #     prediction_ids.append(s_ids)\n\n        return prediction_ids\n\n    def run_epoch(self, inputs, targets, epoch):\n        """"""Runs a single epoch.\n           Returns the average loss value on the epoch.""""""\n        batch_size = self.batch_size\n        nbatches = (len(inputs) + batch_size - 1) // batch_size\n        losses = []\n\n        for i, (inps, trgts) in enumerate(summarizer_model_utils.minibatches(inputs,\n                                                                             targets,\n                                                                             batch_size)):\n            if inps is not None and trgts is not None:\n                fd, sl, s2 = self.get_feed_dict(inps,\n                                                trgts=trgts)\n\n                if i % 10 == 0 and self.summary_dir is not None:\n                    _, train_loss, training_summ = self.sess.run([self.train_op,\n                                                                  self.train_loss,\n                                                                  self.training_summary],\n                                                                 feed_dict=fd)\n                    self.training_writer.add_summary(training_summ, epoch*nbatches + i)\n\n                else:\n                    _, train_loss = self.sess.run([self.train_op, self.train_loss],\n                                                  feed_dict=fd)\n\n                if i % 2 == 0 or i == (nbatches - 1):\n                    print(\'Iteration: {} of {}\\ttrain_loss: {:.4f}\'.format(i, nbatches - 1, train_loss))\n                losses.append(train_loss)\n\n            else:\n                print(\'Minibatch empty.\')\n                continue\n\n        avg_loss = self.sess.run(tf.reduce_mean(losses))\n        print(\'Average Score for this Epoch: {}\'.format(avg_loss))\n\n        return avg_loss\n\n    def run_evaluate(self, inputs, targets, epoch):\n        """"""Runs evaluation on validation inputs and targets.\n        Optionally: - writes summary to Tensorboard.\n        """"""\n        if self.summary_dir is not None:\n            eval_losses = []\n            for inps, trgts in summarizer_model_utils.minibatches(inputs, targets, self.batch_size):\n                fd, sl, s2 = self.get_feed_dict(inps, trgts)\n                eval_loss = self.sess.run([self.eval_loss], feed_dict=fd)\n                eval_losses.append(eval_loss)\n\n            avg_eval_loss = self.sess.run(tf.reduce_mean(eval_losses))\n\n            print(\'Eval_loss: {}\\n\'.format(avg_eval_loss))\n            eval_summ = self.sess.run([self.eval_summary], feed_dict=fd)\n            self.eval_writer.add_summary(eval_summ, epoch)\n\n        else:\n            eval_losses = []\n            for inps, trgts in summarizer_model_utils.minibatches(inputs, targets, self.batch_size):\n                fd, sl, s2 = self.get_feed_dict(inps, trgts)\n                eval_loss = self.sess.run([self.eval_loss], feed_dict=fd)\n                eval_losses.append(eval_loss)\n\n            avg_eval_loss = self.sess.run(tf.reduce_mean(eval_losses))\n\n            print(\'Eval_loss: {}\\n\'.format(avg_eval_loss))\n\n\n\n    def get_feed_dict(self, inps, trgts=None):\n        """"""Creates the feed_dict that is fed into training or inference network.\n           Pads inputs and targets.\n           Returns feed_dict and sequence_length(s) depending on training mode.\n        """"""\n        if self.mode != \'INFER\':\n            inp_ids, sequence_lengths_1 = summarizer_model_utils.pad_sequences(inps,\n                                                                               self.word2ind[self.pad],\n                                                                               tail=False)\n\n            feed = {\n                self.ids_1: inp_ids,\n                self.sequence_lengths_1: sequence_lengths_1\n            }\n\n            if trgts is not None:\n                trgt_ids, sequence_lengths_2 = summarizer_model_utils.pad_sequences(trgts,\n                                                                                    self.word2ind[self.pad],\n                                                                                    tail=True)\n                feed[self.ids_2] = trgt_ids\n                feed[self.sequence_lengths_2] = sequence_lengths_2\n\n                return feed, sequence_lengths_1, sequence_lengths_2\n\n        else:\n\n            inp_ids, sequence_lengths_1 = summarizer_model_utils.pad_sequences(inps,\n                                                                               self.word2ind[self.pad],\n                                                                               tail=False)\n\n            feed = {\n                self.ids_1: inp_ids,\n                self.sequence_lengths_1: sequence_lengths_1\n            }\n\n            if trgts is not None:\n                trgt_ids, sequence_lengths_2 = summarizer_model_utils.pad_sequences(trgts,\n                                                                                    self.word2ind[self.pad],\n                                                                                    tail=True)\n\n                feed[self.sequence_lengths_2] = sequence_lengths_2\n\n                return feed, sequence_lengths_1, sequence_lengths_2\n            else:\n                return feed, sequence_lengths_1\n\n    def initialize_session(self):\n        self.sess = tf.Session()\n        self.sess.run(tf.global_variables_initializer())\n\n    def restore_session(self, restore_path):\n        self.saver.restore(self.sess, restore_path)\n        print(\'Done.\')\n\n    def add_summary(self):\n        """"""Summaries for Tensorboard.""""""\n        self.training_summary = tf.summary.scalar(\'training_loss\', self.train_loss)\n        self.eval_summary = tf.summary.scalar(\'evaluation_loss\', self.eval_loss)\n        self.training_writer = tf.summary.FileWriter(self.summary_dir,\n                                                     tf.get_default_graph())\n        self.eval_writer = tf.summary.FileWriter(self.summary_dir)\n'"
summarizer_data_utils.py,0,"b'import os\nimport time\nimport re\nimport html\nfrom collections import Counter\n\nimport nltk\nimport numpy as np\n\n\ndef preprocess_sentence(text, keep_most=False):\n    """"""\n    Helper function to remove html, unneccessary spaces and punctuation.\n    Args:\n        text: String.\n        keep_most: Boolean. depending if True or False, we either\n                   keep only letters and numbers or also other characters.\n\n    Returns:\n        processed text.\n\n    """"""\n    text = text.lower()\n    text = fixup(text)\n    text = re.sub(r""<br />"", "" "", text)\n    if keep_most:\n        text = re.sub(r""[^a-z0-9%!?.,:()/]"", "" "", text)\n    else:\n        text = re.sub(r""[^a-z0-9]"", "" "", text)\n    text = re.sub(r""    "", "" "", text)\n    text = re.sub(r""   "", "" "", text)\n    text = re.sub(r""  "", "" "", text)\n    text = text.strip()\n    return text\n\n\ndef fixup(x):\n    re1 = re.compile(r\'  +\')\n    x = x.replace(\'#39;\', ""\'"").replace(\'amp;\', \'&\').replace(\'#146;\', ""\'"").replace(\n        \'nbsp;\', \' \').replace(\'#36;\', \'$\').replace(\'\\\\n\', ""\\n"").replace(\'quot;\', ""\'"").replace(\n        \'<br />\', ""\\n"").replace(\'\\\\""\', \'""\').replace(\'<unk>\', \'u_n\').replace(\' @.@ \', \'.\').replace(\n        \' @-@ \', \'-\').replace(\'\\\\\', \' \\\\ \')\n    return re1.sub(\' \', html.unescape(x))\n\n\ndef preprocess(text, keep_most=False):\n    """"""\n    Splits the text into sentences, preprocesses\n       and tokenizes each sentence.\n    Args:\n        text: String. multiple sentences.\n        keep_most: Boolean. depending if True or False, we either\n                   keep only letters and numbers or also other characters.\n\n    Returns:\n        preprocessed and tokenized text.\n\n    """"""\n    tokenized = []\n    for sentence in nltk.sent_tokenize(text):\n        sentence = preprocess_sentence(sentence, keep_most)\n        sentence = nltk.word_tokenize(sentence)\n        for token in sentence:\n            tokenized.append(token)\n\n    return tokenized\n\n\ndef preprocess_texts_and_summaries(texts,\n                                   summaries,\n                                   keep_most=False):\n    """"""iterates given list of texts and given list of summaries and tokenizes every\n       review using the tokenize_review() function.\n       apart from that we count up all the words in the texts and summaries.\n       returns: - processed texts\n                - processed summaries\n                - array containing all the unique words together with their counts\n                  sorted by counts.\n    """"""\n\n    start_time = time.time()\n    processed_texts = []\n    processed_summaries = []\n    words = []\n\n    for text in texts:\n        text = preprocess(text, keep_most)\n        for word in text:\n            words.append(word)\n        processed_texts.append(text)\n    for summary in summaries:\n        summary = preprocess(summary, keep_most)\n        for word in summary:\n            words.append(word)\n\n        processed_summaries.append(summary)\n    words_counted = Counter(words).most_common()\n    print(\'Processing Time: \', time.time() - start_time)\n\n    return processed_texts, processed_summaries, words_counted\n\n\ndef create_word_inds_dicts(words_counted,\n                           specials=None,\n                           min_occurences=0):\n    """""" creates lookup dicts from word to index and back.\n        returns the lookup dicts and an array of words that were not used,\n        due to rare occurence.\n    """"""\n    missing_words = []\n    word2ind = {}\n    ind2word = {}\n    i = 0\n\n    if specials is not None:\n        for sp in specials:\n            word2ind[sp] = i\n            ind2word[i] = sp\n            i += 1\n\n    for (word, count) in words_counted:\n        if count >= min_occurences:\n            word2ind[word] = i\n            ind2word[i] = word\n            i += 1\n        else:\n            missing_words.append(word)\n\n    return word2ind, ind2word, missing_words\n\n\ndef convert_sentence(review, word2ind):\n    """""" converts the given sent to int values corresponding to the given word2ind""""""\n    inds = []\n    unknown_words = []\n\n    for word in review:\n        if word in word2ind.keys():\n            inds.append(int(word2ind[word]))\n        else:\n            inds.append(int(word2ind[\'<UNK>\']))\n            unknown_words.append(word)\n\n    return inds, unknown_words\n\n\ndef convert_to_inds(input, word2ind, eos=False, sos=False):\n    converted_input = []\n    all_unknown_words = set()\n\n    for inp in input:\n        converted_inp, unknown_words = convert_sentence(inp, word2ind)\n        if eos:\n            converted_inp.append(word2ind[\'<EOS>\'])\n        if sos:\n            converted_inp.insert(0, word2ind[\'<SOS>\'])\n        converted_input.append(converted_inp)\n        all_unknown_words.update(unknown_words)\n\n    return converted_input, all_unknown_words\n\n\ndef convert_inds_to_text(inds, ind2word, preprocess=False):\n    """""" convert the given indexes back to text """"""\n    words = [ind2word[word] for word in inds]\n    return words\n\n\ndef load_pretrained_embeddings(path):\n    """"""loads pretrained embeddings. stores each embedding in a\n       dictionary with its corresponding word\n    """"""\n    embeddings = {}\n    with open(path, \'r\', encoding=\'utf-8\') as f:\n        for line in f:\n            values = line.split(\' \')\n            word = values[0]\n            embedding_vector = np.array(values[1:], dtype=\'float32\')\n            embeddings[word] = embedding_vector\n    return embeddings\n\n\ndef create_and_save_embedding_matrix(word2ind,\n                                     pretrained_embeddings_path,\n                                     save_path,\n                                     embedding_dim=300):\n    """"""creates embedding matrix for each word in word2ind. if that words is in\n       pretrained_embeddings, that vector is used. otherwise initialized\n       randomly.\n    """"""\n    pretrained_embeddings = load_pretrained_embeddings(pretrained_embeddings_path)\n    embedding_matrix = np.zeros((len(word2ind), embedding_dim), dtype=np.float32)\n    for word, i in word2ind.items():\n        if word in pretrained_embeddings.keys():\n            embedding_matrix[i] = pretrained_embeddings[word]\n        else:\n            embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n            embedding_matrix[i] = embedding\n    if not os.path.exists(os.path.dirname(save_path)):\n        os.makedirs(os.path.dirname(save_path))\n    np.save(save_path, embedding_matrix)\n    return np.array(embedding_matrix)\n'"
summarizer_model_utils.py,2,"b'import numpy as np\nimport tensorflow as tf\nfrom nltk.translate.bleu_score import sentence_bleu\n\n\ndef minibatches(inputs, targets, minibatch_size):\n    """"""batch generator. yields x and y batch.\n    """"""\n    x_batch, y_batch = [], []\n    for inp, tgt in zip(inputs, targets):\n        if len(x_batch) == minibatch_size and len(y_batch) == minibatch_size:\n            yield x_batch, y_batch\n            x_batch, y_batch = [], []\n        x_batch.append(inp)\n        y_batch.append(tgt)\n\n    if len(x_batch) != 0:\n        for inp, tgt in zip(inputs, targets):\n            if len(x_batch) != minibatch_size:\n                x_batch.append(inp)\n                y_batch.append(tgt)\n            else:\n                break\n        yield x_batch, y_batch\n\n\ndef pad_sequences(sequences, pad_tok, tail=True):\n    """"""Pads the sentences, so that all sentences in a batch have the same length.\n    """"""\n\n    max_length = max(len(x) for x in sequences)\n\n    sequence_padded, sequence_length = [], []\n\n    for seq in sequences:\n        seq = list(seq)\n        if tail:\n            seq_ = seq[:max_length] + [pad_tok] * max(max_length - len(seq), 0)\n        else:\n            seq_ = [pad_tok] * max(max_length - len(seq), 0) + seq[:max_length]\n\n        sequence_padded += [seq_]\n        sequence_length += [min(len(seq), max_length)]\n\n    return sequence_padded, sequence_length\n\n\ndef sample_results(preds, ind2word, word2ind, converted_summaries, converted_texts, use_bleu=False):\n    """"""Plots the actual text and summary and the corresponding created summary.\n    takes care of whether beam search or greedy decoder was used.\n    """"""\n    beam = False\n\n    if len(np.array(preds).shape) == 4:\n        beam = True\n\n    \'\'\'Bleu score is not used correctly here, but serves as reference.\n    \'\'\'\n    if use_bleu:\n        bleu_scores = []\n\n    for pred, summary, text, seq_length in zip(preds[0],\n                                               converted_summaries,\n                                               converted_texts,\n                                               [len(inds) for inds in converted_summaries]):\n        print(\'\\n\\n\\n\', 100 * \'-\')\n        if beam:\n            actual_text = [ind2word[word] for word in text if\n                           word != word2ind[""<SOS>""] and word != word2ind[""<EOS>""]]\n            actual_summary = [ind2word[word] for word in summary if\n                              word != word2ind[\'<EOS>\'] and word != word2ind[\'<SOS>\']]\n\n            created_summary = []\n            for word in pred:\n                if word[0] != word2ind[\'<SOS>\'] and word[0] != word2ind[\'<EOS>\']:\n                    created_summary.append(ind2word[word[0]])\n                    continue\n                else:\n                    continue\n\n            print(\'Actual Text:\\n{}\\n\'.format(\' \'.join(actual_text)))\n            print(\'Actual Summary:\\n{}\\n\'.format(\' \'.join(actual_summary)))\n            print(\'Created Summary:\\n{}\\n\'.format(\' \'.join(created_summary)))\n            if use_bleu:\n                bleu_score = sentence_bleu([actual_summary], created_summary)\n                bleu_scores.append(bleu_score)\n                print(\'Bleu-score:\', bleu_score)\n\n            print()\n\n\n        else:\n            actual_text = [ind2word[word] for word in text if\n                           word != word2ind[""<SOS>""] and word != word2ind[""<EOS>""]]\n            actual_summary = [ind2word[word] for word in summary if\n                              word != word2ind[\'<EOS>\'] and word != word2ind[\'<SOS>\']]\n            created_summary = [ind2word[word] for word in pred if\n                               word != word2ind[\'<EOS>\'] and word != word2ind[\'<SOS>\']]\n\n            print(\'Actual Text:\\n{}\\n\'.format(\' \'.join(actual_text)))\n            print(\'Actual Summary:\\n{}\\n\'.format(\' \'.join(actual_summary)))\n            print(\'Created Summary:\\n{}\\n\'.format(\' \'.join(created_summary)))\n            if use_bleu:\n                bleu_score = sentence_bleu([actual_summary], created_summary)\n                bleu_scores.append(bleu_score)\n                print(\'Bleu-score:\', bleu_score)\n\n    if use_bleu:\n        bleu_score = np.mean(bleu_scores)\n        print(\'\\n\\n\\nTotal Bleu Score:\', bleu_score)\n\n\ndef reset_graph(seed=97):\n    """"""helper function to reset the default graph. this often\n       comes handy when using jupyter noteboooks.\n    """"""\n    tf.reset_default_graph()\n    tf.set_random_seed(seed)\n    np.random.seed(seed)\n'"
