file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\nfrom setuptools.extension import Extension\nfrom Cython.Build import cythonize\nimport numpy\nimport os\nimport imp\n\nVERSION = imp.load_source(\'version\', os.path.join(\'.\', \'darkflow\', \'version.py\'))\nVERSION = VERSION.__version__\n\nif os.name ==\'nt\' :\n    ext_modules=[\n        Extension(""darkflow.cython_utils.nms"",\n            sources=[""darkflow/cython_utils/nms.pyx""],\n            #libraries=[""m""] # Unix-like specific\n            include_dirs=[numpy.get_include()]\n        ),        \n        Extension(""darkflow.cython_utils.cy_yolo2_findboxes"",\n            sources=[""darkflow/cython_utils/cy_yolo2_findboxes.pyx""],\n            #libraries=[""m""] # Unix-like specific\n            include_dirs=[numpy.get_include()]\n        ),\n        Extension(""darkflow.cython_utils.cy_yolo_findboxes"",\n            sources=[""darkflow/cython_utils/cy_yolo_findboxes.pyx""],\n            #libraries=[""m""] # Unix-like specific\n            include_dirs=[numpy.get_include()]\n        )\n    ]\n\nelif os.name ==\'posix\' :\n    ext_modules=[\n        Extension(""darkflow.cython_utils.nms"",\n            sources=[""darkflow/cython_utils/nms.pyx""],\n            libraries=[""m""], # Unix-like specific\n            include_dirs=[numpy.get_include()]\n        ),        \n        Extension(""darkflow.cython_utils.cy_yolo2_findboxes"",\n            sources=[""darkflow/cython_utils/cy_yolo2_findboxes.pyx""],\n            libraries=[""m""], # Unix-like specific\n            include_dirs=[numpy.get_include()]\n        ),\n        Extension(""darkflow.cython_utils.cy_yolo_findboxes"",\n            sources=[""darkflow/cython_utils/cy_yolo_findboxes.pyx""],\n            libraries=[""m""], # Unix-like specific\n            include_dirs=[numpy.get_include()]\n        )\n    ]\n\nelse :\n    ext_modules=[\n        Extension(""darkflow.cython_utils.nms"",\n            sources=[""darkflow/cython_utils/nms.pyx""],\n            libraries=[""m""] # Unix-like specific\n        ),        \n        Extension(""darkflow.cython_utils.cy_yolo2_findboxes"",\n            sources=[""darkflow/cython_utils/cy_yolo2_findboxes.pyx""],\n            libraries=[""m""] # Unix-like specific\n        ),\n        Extension(""darkflow.cython_utils.cy_yolo_findboxes"",\n            sources=[""darkflow/cython_utils/cy_yolo_findboxes.pyx""],\n            libraries=[""m""] # Unix-like specific\n        )\n    ]\n\nsetup(\n    version=VERSION,\n\tname=\'darkflow\',\n    description=\'Darkflow\',\n    license=\'GPLv3\',\n    url=\'https://github.com/thtrieu/darkflow\',\n    packages = find_packages(),\n\tscripts = [\'flow\'],\n    ext_modules = cythonize(ext_modules)\n)'"
darkflow/__init__.py,0,b''
darkflow/cli.py,0,"b""from .defaults import argHandler #Import the default arguments\nimport os\nfrom .net.build import TFNet\n\ndef cliHandler(args):\n    FLAGS = argHandler()\n    FLAGS.setDefaults()\n    FLAGS.parseArgs(args)\n\n    # make sure all necessary dirs exist\n    def _get_dir(dirs):\n        for d in dirs:\n            this = os.path.abspath(os.path.join(os.path.curdir, d))\n            if not os.path.exists(this): os.makedirs(this)\n    \n    requiredDirectories = [FLAGS.imgdir, FLAGS.binary, FLAGS.backup, os.path.join(FLAGS.imgdir,'out')]\n    if FLAGS.summary:\n        requiredDirectories.append(FLAGS.summary)\n\n    _get_dir(requiredDirectories)\n\n    # fix FLAGS.load to appropriate type\n    try: FLAGS.load = int(FLAGS.load)\n    except: pass\n\n    tfnet = TFNet(FLAGS)\n    \n    if FLAGS.demo:\n        tfnet.camera()\n        exit('Demo stopped, exit.')\n\n    if FLAGS.train:\n        print('Enter training ...'); tfnet.train()\n        if not FLAGS.savepb: \n            exit('Training finished, exit.')\n\n    if FLAGS.savepb:\n        print('Rebuild a constant version ...')\n        tfnet.savepb(); exit('Done')\n\n    tfnet.predict()\n"""
darkflow/defaults.py,0,"b""class argHandler(dict):\n    #A super duper fancy custom made CLI argument handler!!\n    __getattr__ = dict.get\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\n    _descriptions = {'help, --h, -h': 'show this super helpful message and exit'}\n    \n    def setDefaults(self):\n        self.define('imgdir', './sample_img/', 'path to testing directory with images')\n        self.define('binary', './bin/', 'path to .weights directory')\n        self.define('config', './cfg/', 'path to .cfg directory')\n        self.define('dataset', '../pascal/VOCdevkit/IMG/', 'path to dataset directory')\n        self.define('labels', 'labels.txt', 'path to labels file')\n        self.define('backup', './ckpt/', 'path to backup folder')\n        self.define('summary', '', 'path to TensorBoard summaries directory')\n        self.define('annotation', '../pascal/VOCdevkit/ANN/', 'path to annotation directory')\n        self.define('threshold', -0.1, 'detection threshold')\n        self.define('model', '', 'configuration of choice')\n        self.define('trainer', 'rmsprop', 'training algorithm')\n        self.define('momentum', 0.0, 'applicable for rmsprop and momentum optimizers')\n        self.define('verbalise', True, 'say out loud while building graph')\n        self.define('train', False, 'train the whole net')\n        self.define('load', '', 'how to initialize the net? Either from .weights or a checkpoint, or even from scratch')\n        self.define('savepb', False, 'save net and weight to a .pb file')\n        self.define('gpu', 0.0, 'how much gpu (from 0.0 to 1.0)')\n        self.define('gpuName', '/gpu:0', 'GPU device name')\n        self.define('lr', 1e-5, 'learning rate')\n        self.define('keep',20,'Number of most recent training results to save')\n        self.define('batch', 16, 'batch size')\n        self.define('epoch', 1000, 'number of epoch')\n        self.define('save', 2000, 'save checkpoint every ? training examples')\n        self.define('demo', '', 'demo on webcam')\n        self.define('queue', 1, 'process demo in batch')\n        self.define('json', False, 'Outputs bounding box information in json format.')\n        self.define('saveVideo', False, 'Records video from input video or camera')\n        self.define('pbLoad', '', 'path to .pb protobuf file (metaLoad must also be specified)')\n        self.define('metaLoad', '', 'path to .meta file generated during --savepb that corresponds to .pb file')\n\n    def define(self, argName, default, description):\n        self[argName] = default\n        self._descriptions[argName] = description\n    \n    def help(self):\n        print('Example usage: flow --imgdir sample_img/ --model cfg/yolo.cfg --load bin/yolo.weights')\n        print('')\n        print('Arguments:')\n        spacing = max([len(i) for i in self._descriptions.keys()]) + 2\n        for item in self._descriptions:\n            currentSpacing = spacing - len(item)\n            print('  --' + item + (' ' * currentSpacing) + self._descriptions[item])\n        print('')\n        exit()\n\n    def parseArgs(self, args):\n        print('')\n        i = 1\n        while i < len(args):\n            if args[i] == '-h' or args[i] == '--h' or args[i] == '--help':\n                self.help() #Time for some self help! :)\n            if len(args[i]) < 2:\n                print('ERROR - Invalid argument: ' + args[i])\n                print('Try running flow --help')\n                exit()\n            argumentName = args[i][2:]\n            if isinstance(self.get(argumentName), bool):\n                if not (i + 1) >= len(args) and (args[i + 1].lower() != 'false' and args[i + 1].lower() != 'true') and not args[i + 1].startswith('--'):\n                    print('ERROR - Expected boolean value (or no value) following argument: ' + args[i])\n                    print('Try running flow --help')\n                    exit()\n                elif not (i + 1) >= len(args) and (args[i + 1].lower() == 'false' or args[i + 1].lower() == 'true'):\n                    self[argumentName] = (args[i + 1].lower() == 'true')\n                    i += 1\n                else:\n                    self[argumentName] = True\n            elif args[i].startswith('--') and not (i + 1) >= len(args) and not args[i + 1].startswith('--') and argumentName in self:\n                if isinstance(self[argumentName], float):\n                    try:\n                        args[i + 1] = float(args[i + 1])\n                    except:\n                        print('ERROR - Expected float for argument: ' + args[i])\n                        print('Try running flow --help')\n                        exit()\n                elif isinstance(self[argumentName], int):\n                    try:\n                        args[i + 1] = int(args[i + 1])\n                    except:\n                        print('ERROR - Expected int for argument: ' + args[i])\n                        print('Try running flow --help')\n                        exit()\n                self[argumentName] = args[i + 1]\n                i += 1\n            else:\n                print('ERROR - Invalid argument: ' + args[i])\n                print('Try running flow --help')\n                exit()\n            i += 1\n"""
darkflow/version.py,0,"b'__version__ = \'1.0.0\'\n""""""Current version of darkflow.""""""'"
test/test_darkflow.py,0,"b'from darkflow.net.build import TFNet\nfrom darkflow.cli import cliHandler\nimport json\nimport requests\nimport cv2\nimport os\nimport sys\nimport pytest\n\n#NOTE: This file is designed to be run in the TravisCI environment. If you want to run it locally set the environment variable TRAVIS_BUILD_DIR to the base\n#      directory of the cloned darkflow repository. WARNING: This file delete images from sample_img/ that won\'t be used for testing (so don\'t run it\n#      locally if you don\'t want this happening!)\n\n#Settings\nbuildPath = os.environ.get(""TRAVIS_BUILD_DIR"")\n\nif buildPath is None:\n    print()\n    print(""TRAVIS_BUILD_DIR environment variable was not found - is this running on TravisCI?"")\n    print(""If you want to test this locally, set TRAVIS_BUILD_DIR to the base directory of the cloned darkflow repository."")\n    exit()\n\ntestImg = {""path"": os.path.join(buildPath, ""sample_img"", ""sample_person.jpg""), ""width"": 640, ""height"": 424,\n           ""expected-objects"": {""yolo-small"": [{""label"": ""dog"", ""confidence"": 0.46, ""topleft"": {""x"": 84, ""y"": 249}, ""bottomright"": {""x"": 208, ""y"": 367}},\n                                                  {""label"": ""person"", ""confidence"": 0.60, ""topleft"": {""x"": 159, ""y"": 102}, ""bottomright"": {""x"": 304, ""y"": 365}}],\n                                ""yolo"":       [{""label"": ""person"", ""confidence"": 0.82, ""topleft"": {""x"": 189, ""y"": 96}, ""bottomright"": {""x"": 271, ""y"": 380}},\n                                                  {""label"": ""dog"", ""confidence"": 0.79, ""topleft"": {""x"": 69, ""y"": 258}, ""bottomright"": {""x"": 209, ""y"": 354}},\n                                                  {""label"": ""horse"", ""confidence"": 0.89, ""topleft"": {""x"": 397, ""y"": 127}, ""bottomright"": {""x"": 605, ""y"": 352}}]}}\n\ntrainImgBikePerson = {""path"": os.path.join(buildPath, ""test"", ""training"", ""images"", ""1.jpg""), ""width"": 500, ""height"": 375,\n                      ""expected-objects"": {""tiny-yolo-voc"": [{""label"":""bicycle"",""confidence"":0.35,""topleft"":{""x"":121,""y"":126},""bottomright"":{""x"":233,""y"":244}},\n                                                             {""label"":""person"",""confidence"":0.60,""topleft"":{""x"":132,""y"":35},""bottomright"":{""x"":232,""y"":165}}]}}\n\ntrainImgHorsePerson = {""path"": os.path.join(buildPath, ""test"", ""training"", ""images"", ""2.jpg""), ""width"": 500, ""height"": 332,\n                       ""expected-objects"": {""tiny-yolo-voc"": [{""label"":""horse"",""confidence"":0.99,""topleft"":{""x"":156,""y"":108},""bottomright"":{""x"":410,""y"":281}},\n                                                              {""label"":""person"",""confidence"":0.89,""topleft"":{""x"":258,""y"":52},""bottomright"":{""x"":300,""y"":218}}]}}\n\n\nposCompareThreshold = 0.05 #Comparisons must match be within 5% of width/height when compared to expected value\nthreshCompareThreshold = 0.1 #Comparisons must match within 0.1 of expected threshold for each prediction\n\nyolo_small_Download = ""https://pjreddie.com/media/files/yolo-small.weights"" #YOLOv1\nyolo_Download = ""https://pjreddie.com/media/files/yolo.weights"" #YOLOv2\ntiny_yolo_voc_Download = ""https://pjreddie.com/media/files/tiny-yolo-voc.weights"" #YOLOv2\n\ndef download_file(url, savePath):\n    fileName = savePath.split(""/"")[-1]\n    if not os.path.isfile(savePath):\n        os.makedirs(os.path.dirname(savePath), exist_ok=True) #Make directories nessecary for file incase they don\'t exist\n        print(""Downloading "" + fileName + "" file..."")\n        r = requests.get(url, stream=True)\n        with open(savePath, \'wb\') as f:\n            for chunk in r.iter_content(chunk_size=1024): \n                if chunk: # filter out keep-alive new chunks\n                    f.write(chunk)\n        r.close()\n    else:\n        print(""Found existing "" + fileName + "" file."")\n\nyolo_small_WeightPath = os.path.join(buildPath, ""bin"", yolo_small_Download.split(""/"")[-1])\nyolo_small_CfgPath = os.path.join(buildPath, ""cfg"", ""v1"", ""{0}.cfg"".format(os.path.splitext(os.path.basename(yolo_small_WeightPath))[0]))\n\nyolo_WeightPath = os.path.join(buildPath, ""bin"", yolo_Download.split(""/"")[-1])\nyolo_CfgPath = os.path.join(buildPath, ""cfg"", ""{0}.cfg"".format(os.path.splitext(os.path.basename(yolo_WeightPath))[0]))\n\ntiny_yolo_voc_WeightPath = os.path.join(buildPath, ""bin"", tiny_yolo_voc_Download.split(""/"")[-1])\ntiny_yolo_voc_CfgPath = os.path.join(buildPath, ""cfg"", ""{0}.cfg"".format(os.path.splitext(os.path.basename(tiny_yolo_voc_WeightPath))[0]))\n\npbPath = os.path.join(buildPath, ""built_graph"", os.path.splitext(os.path.basename(yolo_WeightPath))[0] + "".pb"")\nmetaPath = os.path.join(buildPath, ""built_graph"", os.path.splitext(os.path.basename(yolo_WeightPath))[0] + "".meta"")\n\ngeneralConfigPath = os.path.join(buildPath, ""cfg"")\n\ndownload_file(yolo_small_Download, yolo_small_WeightPath) #Check if we need to download (and if so download) the yolo-small weights (YOLOv1)\ndownload_file(yolo_Download, yolo_WeightPath) #Check if we need to download (and if so download) the yolo weights (YOLOv2)\ndownload_file(tiny_yolo_voc_Download, tiny_yolo_voc_WeightPath) #Check if we need to download (and if so download) the tiny-yolo-voc weights (YOLOv2)\n\ndef executeCLI(commandString):\n    print()\n    print(""Executing: {0}"".format(commandString))\n    print()\n    splitArgs = [item.strip() for item in commandString.split("" "")]\n    cliHandler(splitArgs) #Run the command\n    print()\n\ndef compareSingleObjects(firstObject, secondObject, width, height, threshCompare, posCompare):\n    if(firstObject[""label""] != secondObject[""label""]):\n        return False\n    if(abs(firstObject[""topleft""][""x""] - secondObject[""topleft""][""x""]) > width * posCompare):\n        return False\n    if(abs(firstObject[""topleft""][""y""] - secondObject[""topleft""][""y""]) > height * posCompare):\n        return False\n    if(abs(firstObject[""bottomright""][""x""] - secondObject[""bottomright""][""x""]) > width * posCompare):\n        return False\n    if(abs(firstObject[""bottomright""][""y""] - secondObject[""bottomright""][""y""]) > height * posCompare):\n        return False\n    if(abs(firstObject[""confidence""] - secondObject[""confidence""]) > threshCompare):\n        return False\n    return True\n\ndef compareObjectData(defaultObjects, newObjects, width, height, threshCompare, posCompare):\n    currentlyFound = False\n    for firstObject in defaultObjects:\n        currentlyFound = False\n        for secondObject in newObjects:\n            if compareSingleObjects(firstObject, secondObject, width, height, threshCompare, posCompare):\n                currentlyFound = True\n                break\n        if not currentlyFound:\n            return False\n    return True\n\n#Delete all images that won\'t be tested on so forwarding the whole folder doesn\'t take forever\nfilelist = [f for f in os.listdir(os.path.dirname(testImg[""path""])) if os.path.isfile(os.path.join(os.path.dirname(testImg[""path""]), f)) and f != os.path.basename(testImg[""path""])]\nfor f in filelist:\n    os.remove(os.path.join(os.path.dirname(testImg[""path""]), f))\n\n\n#TESTS FOR INFERENCE\ndef test_CLI_IMG_YOLOv2():\n    #Test predictions outputted to an image using the YOLOv2 model through CLI\n    #NOTE: This test currently does not verify anything about the image created (i.e. proper labeling, proper positioning of prediction boxes, etc.)\n    #      it simply verifies that the code executes properly and that the expected output image is indeed created in ./test/img/out\n\n    testString = ""flow --imgdir {0} --model {1} --load {2} --config {3} --threshold 0.4"".format(os.path.dirname(testImg[""path""]), yolo_CfgPath, yolo_WeightPath, generalConfigPath)\n    executeCLI(testString)\n\n    outputImgPath = os.path.join(os.path.dirname(testImg[""path""]), ""out"", os.path.basename(testImg[""path""]))\n    assert os.path.exists(outputImgPath), ""Expected output image: {0} was not found."".format(outputImgPath)\n    os.remove(outputImgPath) #Remove the image so that it does not affect subsequent tests\n\ndef test_CLI_JSON_YOLOv2():\n    #Test predictions outputted to a JSON file using the YOLOv2 model through CLI\n    #NOTE: This test verifies that the code executes properly, the JSON file is created properly and the predictions generated are within a certain\n    #      margin of error when compared to the expected predictions.\n\n    testString = ""flow --imgdir {0} --model {1} --load {2} --config {3} --threshold 0.4 --json"".format(os.path.dirname(testImg[""path""]), yolo_CfgPath, yolo_WeightPath, generalConfigPath)\n    executeCLI(testString)\n\n    outputJSONPath = os.path.join(os.path.dirname(testImg[""path""]), ""out"", os.path.splitext(os.path.basename(testImg[""path""]))[0] + "".json"")\n    assert os.path.exists(outputJSONPath), ""Expected output JSON file: {0} was not found."".format(outputJSONPath)\n\n    with open(outputJSONPath) as json_file:\n        loadedPredictions = json.load(json_file)\n\n    assert compareObjectData(testImg[""expected-objects""][""yolo""], loadedPredictions, testImg[""width""], testImg[""height""], threshCompareThreshold, posCompareThreshold), ""Generated object predictions from JSON were not within margin of error compared to expected values.""\n    os.remove(outputJSONPath) #Remove the JSON file so that it does not affect subsequent tests\n\ndef test_CLI_JSON_YOLOv1():\n    #Test predictions outputted to a JSON file using the YOLOv1 model through CLI\n    #NOTE: This test verifies that the code executes properly, the JSON file is created properly and the predictions generated are within a certain\n    #      margin of error when compared to the expected predictions.\n\n    testString = ""flow --imgdir {0} --model {1} --load {2} --config {3} --threshold 0.4 --json"".format(os.path.dirname(testImg[""path""]), yolo_small_CfgPath, yolo_small_WeightPath, generalConfigPath)\n    executeCLI(testString)\n\n    outputJSONPath = os.path.join(os.path.dirname(testImg[""path""]), ""out"", os.path.splitext(os.path.basename(testImg[""path""]))[0] + "".json"")\n    assert os.path.exists(outputJSONPath), ""Expected output JSON file: {0} was not found."".format(outputJSONPath)\n\n    with open(outputJSONPath) as json_file:\n        loadedPredictions = json.load(json_file)\n\n    assert compareObjectData(testImg[""expected-objects""][""yolo-small""], loadedPredictions, testImg[""width""], testImg[""height""], threshCompareThreshold, posCompareThreshold), ""Generated object predictions from JSON were not within margin of error compared to expected values.""\n    os.remove(outputJSONPath) #Remove the JSON file so that it does not affect subsequent tests\n\ndef test_CLI_SAVEPB_YOLOv2():\n    #Save .pb and .meta as generated from the YOLOv2 model through CLI\n    #NOTE: This test verifies that the code executes properly, and the .pb and .meta files are successfully created. The subsequent test will verify the\n    #      contents of those files.\n\n    testString = ""flow --model {0} --load {1} --config {2} --threshold 0.4 --savepb"".format(yolo_CfgPath, yolo_WeightPath, generalConfigPath)\n    \n    with pytest.raises(SystemExit):\n            executeCLI(testString)\n\n    assert os.path.exists(pbPath), ""Expected output .pb file: {0} was not found."".format(pbPath)\n    assert os.path.exists(metaPath), ""Expected output .meta file: {0} was not found."".format(metaPath)\n\ndef test_RETURNPREDICT_PBLOAD_YOLOv2():\n    #Test the .pb and .meta files generated in the previous step\n    #NOTE: This test verifies that the code executes properly, and the .pb and .meta files that were created are able to be loaded and used for inference.\n    #      The predictions that are generated will be compared against expected predictions.\n\n    options = {""pbLoad"": pbPath, ""metaLoad"": metaPath, ""threshold"": 0.4}\n    tfnet = TFNet(options)\n    imgcv = cv2.imread(testImg[""path""])\n    loadedPredictions = tfnet.return_predict(imgcv)\n\n    assert compareObjectData(testImg[""expected-objects""][""yolo""], loadedPredictions, testImg[""width""], testImg[""height""], threshCompareThreshold, posCompareThreshold), ""Generated object predictions from return_predict() were not within margin of error compared to expected values.""\n\n#TESTS FOR TRAINING\ndef test_TRAIN_FROM_WEIGHTS_CLI__LOAD_CHECKPOINT_RETURNPREDICT_YOLOv2():\n    #Test training using pre-generated weights for tiny-yolo-voc\n    #NOTE: This test verifies that the code executes properly, and that the expected checkpoint file (tiny-yolo-voc-20.meta in this case) is generated.\n    #      In addition, predictions are generated using the checkpoint file to verify that training completed successfully.\n\n    testString = ""flow --model {0} --load {1} --train --dataset {2} --annotation {3} --epoch 20"".format(tiny_yolo_voc_CfgPath, tiny_yolo_voc_WeightPath, os.path.join(buildPath, ""test"", ""training"", ""images""), os.path.join(buildPath, ""test"", ""training"", ""annotations""))\n    with pytest.raises(SystemExit):\n        executeCLI(testString)\n\n    checkpointPath = os.path.join(buildPath, ""ckpt"", ""tiny-yolo-voc-20.meta"")\n    assert os.path.exists(checkpointPath), ""Expected output checkpoint file: {0} was not found."".format(checkpointPath)\n\n    #Using trained weights\n    options = {""model"": tiny_yolo_voc_CfgPath, ""load"": 20, ""config"": generalConfigPath, ""threshold"": 0.1}\n    tfnet = TFNet(options)\n\n    #Make sure predictions very roughly match the expected values for image with bike and person\n    imgcv = cv2.imread(trainImgBikePerson[""path""])\n    loadedPredictions = tfnet.return_predict(imgcv)\n    assert compareObjectData(trainImgBikePerson[""expected-objects""][""tiny-yolo-voc""], loadedPredictions, trainImgBikePerson[""width""], trainImgBikePerson[""height""], 0.7, 0.25), ""Generated object predictions from training (for image with person on the bike) were not anywhere close to what they are expected to be.\\nTraining may not have completed successfully.""\n    differentThanExpectedBike = compareObjectData(trainImgBikePerson[""expected-objects""][""tiny-yolo-voc""], loadedPredictions, trainImgBikePerson[""width""], trainImgBikePerson[""height""], 0.01, 0.001)\n\n    #Make sure predictions very roughly match the expected values for image with horse and person\n    imgcv = cv2.imread(trainImgHorsePerson[""path""])\n    loadedPredictions = tfnet.return_predict(imgcv)\n    assert compareObjectData(trainImgHorsePerson[""expected-objects""][""tiny-yolo-voc""], loadedPredictions, trainImgHorsePerson[""width""], trainImgHorsePerson[""height""], 0.7, 0.25), ""Generated object predictions from training (for image with person on the horse) were not anywhere close to what they are expected to be.\\nTraining may not have completed successfully.""\n    differentThanExpectedHorse = compareObjectData(trainImgHorsePerson[""expected-objects""][""tiny-yolo-voc""], loadedPredictions, trainImgHorsePerson[""width""], trainImgHorsePerson[""height""], 0.01, 0.001)\n\n    assert not (differentThanExpectedBike and differentThanExpectedHorse), ""The generated object predictions for both images appear to be exactly the same as the ones generated with the original weights.\\nTraining may not have completed successfully.\\n\\nNOTE: It is possible this is a fluke error and training did complete properly (try running this build again to confirm) - but most likely something is wrong.""'"
darkflow/cython_utils/__init__.py,0,b''
darkflow/dark/__init__.py,0,b''
darkflow/dark/connected.py,0,"b""from .layer import Layer\nimport numpy as np\n\nclass extract_layer(Layer):\n    def setup(self, old_inp, old_out,\n              activation, inp, out):\n        if inp is None: inp = range(old_inp)\n        self.activation = activation\n        self.old_inp = old_inp\n        self.old_out = old_out\n        self.inp = inp\n        self.out = out\n        self.wshape = {\n            'biases': [len(self.out)],\n            'weights': [len(self.inp), len(self.out)]\n        }\n\n    @property\n    def signature(self):\n        sig = ['connected']\n        sig += self._signature[1:-2]\n        return sig\n\n    def present(self):\n        args = self.signature\n        self.presenter = connected_layer(*args)\n\n    def recollect(self, val):\n        w = val['weights']\n        b = val['biases']\n        if w is None: self.w = val; return\n        w = np.take(w, self.inp, 0)\n        w = np.take(w, self.out, 1)\n        b = np.take(b, self.out)\n        assert1 = w.shape == tuple(self.wshape['weights'])\n        assert2 = b.shape == tuple(self.wshape['biases'])\n        assert assert1 and assert2, \\\n        'Dimension does not match in {} recollect'.format(\n            self._signature)\n        \n        self.w['weights'] = w\n        self.w['biases'] = b\n    \n\n\nclass select_layer(Layer):\n    def setup(self, inp, old, \n              activation, inp_idx,\n              out, keep, train):\n        self.old = old\n        self.keep = keep\n        self.train = train\n        self.inp_idx = inp_idx\n        self.activation = activation\n        inp_dim = inp\n        if inp_idx is not None:\n            inp_dim = len(inp_idx)\n        self.inp = inp_dim\n        self.out = out\n        self.wshape = {\n            'biases': [out],\n            'weights': [inp_dim, out]\n        }\n\n    @property\n    def signature(self):\n        sig = ['connected']\n        sig += self._signature[1:-4]\n        return sig\n\n    def present(self):\n        args = self.signature\n        self.presenter = connected_layer(*args)\n\n    def recollect(self, val):\n        w = val['weights']\n        b = val['biases']\n        if w is None: self.w = val; return\n        if self.inp_idx is not None:\n            w = np.take(w, self.inp_idx, 0)\n            \n        keep_b = np.take(b, self.keep)\n        keep_w = np.take(w, self.keep, 1)\n        train_b = b[self.train:]\n        train_w = w[:, self.train:]\n        self.w['biases'] = np.concatenate(\n            (keep_b, train_b), axis = 0)\n        self.w['weights'] = np.concatenate(\n            (keep_w, train_w), axis = 1)\n\n\nclass connected_layer(Layer):\n    def setup(self, input_size, \n              output_size, activation):\n        self.activation = activation\n        self.inp = input_size\n        self.out = output_size\n        self.wshape = {\n            'biases': [self.out],\n            'weights': [self.inp, self.out]\n        }\n\n    def finalize(self, transpose):\n        weights = self.w['weights']\n        if weights is None: return\n        shp = self.wshape['weights']\n        if not transpose:\n            weights = weights.reshape(shp[::-1])\n            weights = weights.transpose([1,0])\n        else: weights = weights.reshape(shp)\n        self.w['weights'] = weights"""
darkflow/dark/convolution.py,0,"b'from .layer import Layer\nimport numpy as np\n\nclass local_layer(Layer):\n    def setup(self, ksize, c, n, stride, \n              pad, w_, h_, activation):\n        self.pad = pad * int(ksize / 2)\n        self.activation = activation\n        self.stride = stride\n        self.ksize = ksize\n        self.h_out = h_\n        self.w_out = w_\n\n        self.dnshape = [h_ * w_, n, c, ksize, ksize]\n        self.wshape = dict({\n            \'biases\': [h_ * w_ * n],\n            \'kernels\': [h_ * w_, ksize, ksize, c, n]\n        })\n\n    def finalize(self, _):\n        weights = self.w[\'kernels\']\n        if weights is None: return\n        weights = weights.reshape(self.dnshape)\n        weights = weights.transpose([0,3,4,2,1])\n        self.w[\'kernels\'] = weights\n\nclass conv_extract_layer(Layer):\n    def setup(self, ksize, c, n, stride, \n              pad, batch_norm, activation,\n              inp, out):\n        if inp is None: inp = range(c)\n        self.activation = activation\n        self.batch_norm = batch_norm\n        self.stride = stride\n        self.ksize = ksize\n        self.pad = pad\n        self.inp = inp\n        self.out = out\n        self.wshape = dict({\n            \'biases\': [len(out)], \n            \'kernel\': [ksize, ksize, len(inp), len(out)]\n        })\n\n    @property\n    def signature(self):\n        sig = [\'convolutional\']\n        sig += self._signature[1:-2]\n        return sig\n\n    def present(self):\n        args = self.signature\n        self.presenter = convolutional_layer(*args)\n\n    def recollect(self, w):\n        if w is None:\n            self.w = w\n            return\n        k = w[\'kernel\']\n        b = w[\'biases\']\n        k = np.take(k, self.inp, 2)\n        k = np.take(k, self.out, 3)\n        b = np.take(b, self.out)\n        assert1 = k.shape == tuple(self.wshape[\'kernel\'])\n        assert2 = b.shape == tuple(self.wshape[\'biases\'])\n        assert assert1 and assert2, \\\n        \'Dimension not matching in {} recollect\'.format(\n            self._signature)\n        self.w[\'kernel\'] = k\n        self.w[\'biases\'] = b\n\n\nclass conv_select_layer(Layer):\n    def setup(self, ksize, c, n, stride, \n              pad, batch_norm, activation,\n              keep_idx, real_n):\n        self.batch_norm = bool(batch_norm)\n        self.activation = activation\n        self.keep_idx = keep_idx\n        self.stride = stride\n        self.ksize = ksize\n        self.pad = pad\n        self.wshape = dict({\n            \'biases\': [real_n], \n            \'kernel\': [ksize, ksize, c, real_n]\n        })\n        if self.batch_norm:\n            self.wshape.update({\n                \'moving_variance\'  : [real_n], \n                \'moving_mean\': [real_n], \n                \'gamma\' : [real_n]\n            })\n            self.h[\'is_training\'] = {\n                \'shape\': (),\n                \'feed\': True,\n                \'dfault\': False\n            }\n\n    @property\n    def signature(self):\n        sig = [\'convolutional\']\n        sig += self._signature[1:-2]\n        return sig\n    \n    def present(self):\n        args = self.signature\n        self.presenter = convolutional_layer(*args)\n\n    def recollect(self, w):\n        if w is None:\n            self.w = w\n            return\n        idx = self.keep_idx\n        k = w[\'kernel\']\n        b = w[\'biases\']\n        self.w[\'kernel\'] = np.take(k, idx, 3) \n        self.w[\'biases\'] = np.take(b, idx)\n        if self.batch_norm:\n            m = w[\'moving_mean\']\n            v = w[\'moving_variance\']\n            g = w[\'gamma\']\n            self.w[\'moving_mean\'] = np.take(m, idx)\n            self.w[\'moving_variance\'] = np.take(v, idx)\n            self.w[\'gamma\'] = np.take(g, idx)\n\nclass convolutional_layer(Layer):\n    def setup(self, ksize, c, n, stride, \n              pad, batch_norm, activation):\n        self.batch_norm = bool(batch_norm)\n        self.activation = activation\n        self.stride = stride\n        self.ksize = ksize\n        self.pad = pad\n        self.dnshape = [n, c, ksize, ksize] # darknet shape\n        self.wshape = dict({\n            \'biases\': [n], \n            \'kernel\': [ksize, ksize, c, n]\n        })\n        if self.batch_norm:\n            self.wshape.update({\n                \'moving_variance\'  : [n], \n                \'moving_mean\': [n], \n                \'gamma\' : [n]\n            })\n            self.h[\'is_training\'] = {\n                \'feed\': True,\n                \'dfault\': False,\n                \'shape\': ()\n            }\n\n    def finalize(self, _):\n        """"""deal with darknet""""""\n        kernel = self.w[\'kernel\']\n        if kernel is None: return\n        kernel = kernel.reshape(self.dnshape)\n        kernel = kernel.transpose([2,3,1,0])\n        self.w[\'kernel\'] = kernel'"
darkflow/dark/darknet.py,0,"b'from ..utils.process import cfg_yielder\nfrom .darkop import create_darkop\nfrom ..utils import loader\nimport warnings\nimport time\nimport os\n\nclass Darknet(object):\n\n    _EXT = \'.weights\'\n\n    def __init__(self, FLAGS):\n        self.get_weight_src(FLAGS)\n        self.modify = False\n\n        print(\'Parsing {}\'.format(self.src_cfg))\n        src_parsed = self.parse_cfg(self.src_cfg, FLAGS)\n        self.src_meta, self.src_layers = src_parsed\n        \n        if self.src_cfg == FLAGS.model:\n            self.meta, self.layers = src_parsed\n        else: \n        \tprint(\'Parsing {}\'.format(FLAGS.model))\n        \tdes_parsed = self.parse_cfg(FLAGS.model, FLAGS)\n        \tself.meta, self.layers = des_parsed\n\n        self.load_weights()\n\n    def get_weight_src(self, FLAGS):\n        """"""\n        analyse FLAGS.load to know where is the \n        source binary and what is its config.\n        can be: None, FLAGS.model, or some other\n        """"""\n        self.src_bin = FLAGS.model + self._EXT\n        self.src_bin = FLAGS.binary + self.src_bin\n        self.src_bin = os.path.abspath(self.src_bin)\n        exist = os.path.isfile(self.src_bin)\n\n        if FLAGS.load == str(): FLAGS.load = int()\n        if type(FLAGS.load) is int:\n            self.src_cfg = FLAGS.model\n            if FLAGS.load: self.src_bin = None\n            elif not exist: self.src_bin = None\n        else:\n            assert os.path.isfile(FLAGS.load), \\\n            \'{} not found\'.format(FLAGS.load)\n            self.src_bin = FLAGS.load\n            name = loader.model_name(FLAGS.load)\n            cfg_path = os.path.join(FLAGS.config, name + \'.cfg\')\n            if not os.path.isfile(cfg_path):\n                warnings.warn(\n                    \'{} not found, use {} instead\'.format(\n                    cfg_path, FLAGS.model))\n                cfg_path = FLAGS.model\n            self.src_cfg = cfg_path\n            FLAGS.load = int()\n\n\n    def parse_cfg(self, model, FLAGS):\n        """"""\n        return a list of `layers` objects (darkop.py)\n        given path to binaries/ and configs/\n        """"""\n        args = [model, FLAGS.binary]\n        cfg_layers = cfg_yielder(*args)\n        meta = dict(); layers = list()\n        for i, info in enumerate(cfg_layers):\n            if i == 0: meta = info; continue\n            else: new = create_darkop(*info)\n            layers.append(new)\n        return meta, layers\n\n    def load_weights(self):\n        """"""\n        Use `layers` and Loader to load .weights file\n        """"""\n        print(\'Loading {} ...\'.format(self.src_bin))\n        start = time.time()\n\n        args = [self.src_bin, self.src_layers]\n        wgts_loader = loader.create_loader(*args)\n        for layer in self.layers: layer.load(wgts_loader)\n        \n        stop = time.time()\n        print(\'Finished in {}s\'.format(stop - start))'"
darkflow/dark/darkop.py,0,"b'from .layer import Layer\nfrom .convolution import *\nfrom .connected import *\n\nclass avgpool_layer(Layer):\n    pass\n\nclass crop_layer(Layer):\n    pass\n\nclass maxpool_layer(Layer):\n    def setup(self, ksize, stride, pad):\n        self.stride = stride\n        self.ksize = ksize\n        self.pad = pad\n\nclass softmax_layer(Layer):\n    def setup(self, groups):\n        self.groups = groups\n\nclass dropout_layer(Layer):\n    def setup(self, p):\n        self.h[\'pdrop\'] = dict({\n            \'feed\': p, # for training\n            \'dfault\': 1.0, # for testing\n            \'shape\': ()\n        })\n\nclass route_layer(Layer):\n    def setup(self, routes):\n        self.routes = routes\n\nclass reorg_layer(Layer):\n    def setup(self, stride):\n        self.stride = stride\n\n""""""\nDarkop Factory\n""""""\n\ndarkops = {\n    \'dropout\': dropout_layer,\n    \'connected\': connected_layer,\n    \'maxpool\': maxpool_layer,\n    \'convolutional\': convolutional_layer,\n    \'avgpool\': avgpool_layer,\n    \'softmax\': softmax_layer,\n    \'crop\': crop_layer,\n    \'local\': local_layer,\n    \'select\': select_layer,\n    \'route\': route_layer,\n    \'reorg\': reorg_layer,\n    \'conv-select\': conv_select_layer,\n    \'conv-extract\': conv_extract_layer,\n    \'extract\': extract_layer\n}\n\ndef create_darkop(ltype, num, *args):\n    op_class = darkops.get(ltype, Layer)\n    return op_class(ltype, num, *args)'"
darkflow/dark/layer.py,0,"b""from ..utils import loader\nimport numpy as np\n\nclass Layer(object):\n\n    def __init__(self, *args):\n        self._signature = list(args)\n        self.type = list(args)[0]\n        self.number = list(args)[1]\n\n        self.w = dict() # weights\n        self.h = dict() # placeholders\n        self.wshape = dict() # weight shape\n        self.wsize = dict() # weight size\n        self.setup(*args[2:]) # set attr up\n        self.present()\n        for var in self.wshape:\n            shp = self.wshape[var]\n            size = np.prod(shp)\n            self.wsize[var] = size\n\n    def load(self, src_loader):\n        var_lay = src_loader.VAR_LAYER\n        if self.type not in var_lay: return\n\n        src_type = type(src_loader)\n        if src_type is loader.weights_loader:\n            wdict = self.load_weights(src_loader)\n        else: \n            wdict = self.load_ckpt(src_loader)\n        if wdict is not None:\n            self.recollect(wdict)\n\n    def load_weights(self, src_loader):\n        val = src_loader([self.presenter])\n        if val is None: return None\n        else: return val.w\n\n    def load_ckpt(self, src_loader):\n        result = dict()\n        presenter = self.presenter\n        for var in presenter.wshape:\n            name = presenter.varsig(var)\n            shape = presenter.wshape[var]\n            key = [name, shape]\n            val = src_loader(key)\n            result[var] = val\n        return result\n\n    @property\n    def signature(self):\n        return self._signature\n\n    # For comparing two layers\n    def __eq__(self, other):\n        return self.signature == other.signature\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def varsig(self, var):\n        if var not in self.wshape:\n            return None\n        sig = str(self.number)\n        sig += '-' + self.type\n        sig += '/' + var\n        return sig\n\n    def recollect(self, w): self.w = w\n    def present(self): self.presenter = self\n    def setup(self, *args): pass\n    def finalize(self): pass """
darkflow/net/__init__.py,0,b''
darkflow/net/build.py,27,"b'import tensorflow as tf\nimport time\nfrom . import help\nfrom . import flow\nfrom .ops import op_create, identity\nfrom .ops import HEADER, LINE\nfrom .framework import create_framework\nfrom ..dark.darknet import Darknet\nimport json\nimport os\n\nclass TFNet(object):\n\n\t_TRAINER = dict({\n\t\t\'rmsprop\': tf.train.RMSPropOptimizer,\n\t\t\'adadelta\': tf.train.AdadeltaOptimizer,\n\t\t\'adagrad\': tf.train.AdagradOptimizer,\n\t\t\'adagradDA\': tf.train.AdagradDAOptimizer,\n\t\t\'momentum\': tf.train.MomentumOptimizer,\n\t\t\'adam\': tf.train.AdamOptimizer,\n\t\t\'ftrl\': tf.train.FtrlOptimizer,\n\t\t\'sgd\': tf.train.GradientDescentOptimizer\n\t})\n\n\t# imported methods\n\t_get_fps = help._get_fps\n\tsay = help.say\n\ttrain = flow.train\n\tcamera = help.camera\n\tpredict = flow.predict\n\treturn_predict = flow.return_predict\n\tto_darknet = help.to_darknet\n\tbuild_train_op = help.build_train_op\n\tload_from_ckpt = help.load_from_ckpt\n\n\tdef __init__(self, FLAGS, darknet = None):\n\t\tself.ntrain = 0\n\n\t\tif isinstance(FLAGS, dict):\n\t\t\tfrom ..defaults import argHandler\n\t\t\tnewFLAGS = argHandler()\n\t\t\tnewFLAGS.setDefaults()\n\t\t\tnewFLAGS.update(FLAGS)\n\t\t\tFLAGS = newFLAGS\n\n\t\tself.FLAGS = FLAGS\n\t\tif self.FLAGS.pbLoad and self.FLAGS.metaLoad:\n\t\t\tself.say(\'\\nLoading from .pb and .meta\')\n\t\t\tself.graph = tf.Graph()\n\t\t\tdevice_name = FLAGS.gpuName \\\n\t\t\t\tif FLAGS.gpu > 0.0 else None\n\t\t\twith tf.device(device_name):\n\t\t\t\twith self.graph.as_default() as g:\n\t\t\t\t\tself.build_from_pb()\n\t\t\treturn\n\n\t\tif darknet is None:\t\n\t\t\tdarknet = Darknet(FLAGS)\n\t\t\tself.ntrain = len(darknet.layers)\n\n\t\tself.darknet = darknet\n\t\targs = [darknet.meta, FLAGS]\n\t\tself.num_layer = len(darknet.layers)\n\t\tself.framework = create_framework(*args)\n\t\t\n\t\tself.meta = darknet.meta\n\n\t\tself.say(\'\\nBuilding net ...\')\n\t\tstart = time.time()\n\t\tself.graph = tf.Graph()\n\t\tdevice_name = FLAGS.gpuName \\\n\t\t\tif FLAGS.gpu > 0.0 else None\n\t\twith tf.device(device_name):\n\t\t\twith self.graph.as_default() as g:\n\t\t\t\tself.build_forward()\n\t\t\t\tself.setup_meta_ops()\n\t\tself.say(\'Finished in {}s\\n\'.format(\n\t\t\ttime.time() - start))\n\t\n\tdef build_from_pb(self):\n\t\twith tf.gfile.FastGFile(self.FLAGS.pbLoad, ""rb"") as f:\n\t\t\tgraph_def = tf.GraphDef()\n\t\t\tgraph_def.ParseFromString(f.read())\n\t\t\n\t\ttf.import_graph_def(\n\t\t\tgraph_def,\n\t\t\tname=""""\n\t\t)\n\t\twith open(self.FLAGS.metaLoad, \'r\') as fp:\n\t\t\tself.meta = json.load(fp)\n\t\tself.framework = create_framework(self.meta, self.FLAGS)\n\n\t\t# Placeholders\n\t\tself.inp = tf.get_default_graph().get_tensor_by_name(\'input:0\')\n\t\tself.feed = dict() # other placeholders\n\t\tself.out = tf.get_default_graph().get_tensor_by_name(\'output:0\')\n\t\t\n\t\tself.setup_meta_ops()\n\t\n\tdef build_forward(self):\n\t\tverbalise = self.FLAGS.verbalise\n\n\t\t# Placeholders\n\t\tinp_size = [None] + self.meta[\'inp_size\']\n\t\tself.inp = tf.placeholder(tf.float32, inp_size, \'input\')\n\t\tself.feed = dict() # other placeholders\n\n\t\t# Build the forward pass\n\t\tstate = identity(self.inp)\n\t\troof = self.num_layer - self.ntrain\n\t\tself.say(HEADER, LINE)\n\t\tfor i, layer in enumerate(self.darknet.layers):\n\t\t\tscope = \'{}-{}\'.format(str(i),layer.type)\n\t\t\targs = [layer, state, i, roof, self.feed]\n\t\t\tstate = op_create(*args)\n\t\t\tmess = state.verbalise()\n\t\t\tself.say(mess)\n\t\tself.say(LINE)\n\n\t\tself.top = state\n\t\tself.out = tf.identity(state.out, name=\'output\')\n\n\tdef setup_meta_ops(self):\n\t\tcfg = dict({\n\t\t\t\'allow_soft_placement\': False,\n\t\t\t\'log_device_placement\': False\n\t\t})\n\n\t\tutility = min(self.FLAGS.gpu, 1.)\n\t\tif utility > 0.0:\n\t\t\tself.say(\'GPU mode with {} usage\'.format(utility))\n\t\t\tcfg[\'gpu_options\'] = tf.GPUOptions(\n\t\t\t\tper_process_gpu_memory_fraction = utility)\n\t\t\tcfg[\'allow_soft_placement\'] = True\n\t\telse: \n\t\t\tself.say(\'Running entirely on CPU\')\n\t\t\tcfg[\'device_count\'] = {\'GPU\': 0}\n\n\t\tif self.FLAGS.train: self.build_train_op()\n\t\t\n\t\tif self.FLAGS.summary:\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tself.writer = tf.summary.FileWriter(self.FLAGS.summary + \'train\')\n\t\t\n\t\tself.sess = tf.Session(config = tf.ConfigProto(**cfg))\n\t\tself.sess.run(tf.global_variables_initializer())\n\n\t\tif not self.ntrain: return\n\t\tself.saver = tf.train.Saver(tf.global_variables(), \n\t\t\tmax_to_keep = self.FLAGS.keep)\n\t\tif self.FLAGS.load != 0: self.load_from_ckpt()\n\t\t\n\t\tif self.FLAGS.summary:\n\t\t\tself.writer.add_graph(self.sess.graph)\n\n\tdef savepb(self):\n\t\t""""""\n\t\tCreate a standalone const graph def that \n\t\tC++\tcan load and run.\n\t\t""""""\n\t\tdarknet_pb = self.to_darknet()\n\t\tflags_pb = self.FLAGS\n\t\tflags_pb.verbalise = False\n\t\t\n\t\tflags_pb.train = False\n\t\t# rebuild another tfnet. all const.\n\t\ttfnet_pb = TFNet(flags_pb, darknet_pb)\t\t\n\t\ttfnet_pb.sess = tf.Session(graph = tfnet_pb.graph)\n\t\t# tfnet_pb.predict() # uncomment for unit testing\n\t\tname = \'built_graph/{}.pb\'.format(self.meta[\'name\'])\n\t\tos.makedirs(os.path.dirname(name), exist_ok=True)\n\t\t#Save dump of everything in meta\n\t\twith open(\'built_graph/{}.meta\'.format(self.meta[\'name\']), \'w\') as fp:\n\t\t\tjson.dump(self.meta, fp)\n\t\tself.say(\'Saving const graph def to {}\'.format(name))\n\t\tgraph_def = tfnet_pb.sess.graph_def\n\t\ttf.train.write_graph(graph_def,\'./\', name, False)'"
darkflow/net/flow.py,0,"b'import os\nimport time\nimport numpy as np\nimport tensorflow as tf\nimport pickle\nfrom multiprocessing.pool import ThreadPool\n\ntrain_stats = (\n    \'Training statistics: \\n\'\n    \'\\tLearning rate : {}\\n\'\n    \'\\tBatch size    : {}\\n\'\n    \'\\tEpoch number  : {}\\n\'\n    \'\\tBackup every  : {}\'\n)\npool = ThreadPool()\n\ndef _save_ckpt(self, step, loss_profile):\n    file = \'{}-{}{}\'\n    model = self.meta[\'name\']\n\n    profile = file.format(model, step, \'.profile\')\n    profile = os.path.join(self.FLAGS.backup, profile)\n    with open(profile, \'wb\') as profile_ckpt: \n        pickle.dump(loss_profile, profile_ckpt)\n\n    ckpt = file.format(model, step, \'\')\n    ckpt = os.path.join(self.FLAGS.backup, ckpt)\n    self.say(\'Checkpoint at step {}\'.format(step))\n    self.saver.save(self.sess, ckpt)\n\n\ndef train(self):\n    loss_ph = self.framework.placeholders\n    loss_mva = None; profile = list()\n\n    batches = self.framework.shuffle()\n    loss_op = self.framework.loss\n\n    for i, (x_batch, datum) in enumerate(batches):\n        if not i: self.say(train_stats.format(\n            self.FLAGS.lr, self.FLAGS.batch,\n            self.FLAGS.epoch, self.FLAGS.save\n        ))\n\n        feed_dict = {\n            loss_ph[key]: datum[key] \n                for key in loss_ph }\n        feed_dict[self.inp] = x_batch\n        feed_dict.update(self.feed)\n\n        fetches = [self.train_op, loss_op]\n\n        if self.FLAGS.summary:\n            fetches.append(self.summary_op)\n\n        fetched = self.sess.run(fetches, feed_dict)\n        loss = fetched[1]\n\n        if loss_mva is None: loss_mva = loss\n        loss_mva = .9 * loss_mva + .1 * loss\n        step_now = self.FLAGS.load + i + 1\n\n        if self.FLAGS.summary:\n            self.writer.add_summary(fetched[2], step_now)\n\n        form = \'step {} - loss {} - moving ave loss {}\'\n        self.say(form.format(step_now, loss, loss_mva))\n        profile += [(loss, loss_mva)]\n\n        ckpt = (i+1) % (self.FLAGS.save // self.FLAGS.batch)\n        args = [step_now, profile]\n        if not ckpt: _save_ckpt(self, *args)\n\n    if ckpt: _save_ckpt(self, *args)\n\ndef return_predict(self, im):\n    assert isinstance(im, np.ndarray), \\\n\t\t\t\t\'Image is not a np.ndarray\'\n    h, w, _ = im.shape\n    im = self.framework.resize_input(im)\n    this_inp = np.expand_dims(im, 0)\n    feed_dict = {self.inp : this_inp}\n\n    out = self.sess.run(self.out, feed_dict)[0]\n    boxes = self.framework.findboxes(out)\n    threshold = self.FLAGS.threshold\n    boxesInfo = list()\n    for box in boxes:\n        tmpBox = self.framework.process_box(box, h, w, threshold)\n        if tmpBox is None:\n            continue\n        boxesInfo.append({\n            ""label"": tmpBox[4],\n            ""confidence"": tmpBox[6],\n            ""topleft"": {\n                ""x"": tmpBox[0],\n                ""y"": tmpBox[2]},\n            ""bottomright"": {\n                ""x"": tmpBox[1],\n                ""y"": tmpBox[3]}\n        })\n    return boxesInfo\n\nimport math\n\ndef predict(self):\n    inp_path = self.FLAGS.imgdir\n    all_inps = os.listdir(inp_path)\n    all_inps = [i for i in all_inps if self.framework.is_inp(i)]\n    if not all_inps:\n        msg = \'Failed to find any images in {} .\'\n        exit(\'Error: {}\'.format(msg.format(inp_path)))\n\n    batch = min(self.FLAGS.batch, len(all_inps))\n\n    # predict in batches\n    n_batch = int(math.ceil(len(all_inps) / batch))\n    for j in range(n_batch):\n        from_idx = j * batch\n        to_idx = min(from_idx + batch, len(all_inps))\n\n        # collect images input in the batch\n        this_batch = all_inps[from_idx:to_idx]\n        inp_feed = pool.map(lambda inp: (\n            np.expand_dims(self.framework.preprocess(\n                os.path.join(inp_path, inp)), 0)), this_batch)\n\n        # Feed to the net\n        feed_dict = {self.inp : np.concatenate(inp_feed, 0)}    \n        self.say(\'Forwarding {} inputs ...\'.format(len(inp_feed)))\n        start = time.time()\n        out = self.sess.run(self.out, feed_dict)\n        stop = time.time(); last = stop - start\n        self.say(\'Total time = {}s / {} inps = {} ips\'.format(\n            last, len(inp_feed), len(inp_feed) / last))\n\n        # Post processing\n        self.say(\'Post processing {} inputs ...\'.format(len(inp_feed)))\n        start = time.time()\n        pool.map(lambda p: (lambda i, prediction:\n            self.framework.postprocess(\n               prediction, os.path.join(inp_path, this_batch[i])))(*p),\n            enumerate(out))\n        stop = time.time(); last = stop - start\n\n        # Timing\n        self.say(\'Total time = {}s / {} inps = {} ips\'.format(\n            last, len(inp_feed), len(inp_feed) / last))\n'"
darkflow/net/framework.py,0,"b'from . import yolo\nfrom . import yolov2\nfrom . import vanilla\nfrom os.path import basename\n\nclass framework(object):\n    constructor = vanilla.constructor\n    loss = vanilla.train.loss\n    \n    def __init__(self, meta, FLAGS):\n        model = basename(meta[\'model\'])\n        model = \'.\'.join(model.split(\'.\')[:-1])\n        meta[\'name\'] = model\n        \n        self.constructor(meta, FLAGS)\n\n    def is_inp(self, file_name):\n        return True\n\nclass YOLO(framework):\n    constructor = yolo.constructor\n    parse = yolo.data.parse\n    shuffle = yolo.data.shuffle\n    preprocess = yolo.predict.preprocess\n    postprocess = yolo.predict.postprocess\n    loss = yolo.train.loss\n    is_inp = yolo.misc.is_inp\n    profile = yolo.misc.profile\n    _batch = yolo.data._batch\n    resize_input = yolo.predict.resize_input\n    findboxes = yolo.predict.findboxes\n    process_box = yolo.predict.process_box\n\nclass YOLOv2(framework):\n    constructor = yolo.constructor\n    parse = yolo.data.parse\n    shuffle = yolov2.data.shuffle\n    preprocess = yolo.predict.preprocess\n    loss = yolov2.train.loss\n    is_inp = yolo.misc.is_inp\n    postprocess = yolov2.predict.postprocess\n    _batch = yolov2.data._batch\n    resize_input = yolo.predict.resize_input\n    findboxes = yolov2.predict.findboxes\n    process_box = yolo.predict.process_box\n\n""""""\nframework factory\n""""""\n\ntypes = {\n    \'[detection]\': YOLO,\n    \'[region]\': YOLOv2\n}\n\ndef create_framework(meta, FLAGS):\n    net_type = meta[\'type\']\n    this = types.get(net_type, framework)\n    return this(meta, FLAGS)'"
darkflow/net/help.py,4,"b'""""""\ntfnet secondary (helper) methods\n""""""\nfrom ..utils.loader import create_loader\nfrom time import time as timer\nimport tensorflow as tf\nimport numpy as np\nimport sys\nimport cv2\nimport os\n\nold_graph_msg = \'Resolving old graph def {} (no guarantee)\'\n\ndef build_train_op(self):\n    self.framework.loss(self.out)\n    self.say(\'Building {} train op\'.format(self.meta[\'model\']))\n    optimizer = self._TRAINER[self.FLAGS.trainer](self.FLAGS.lr)\n    gradients = optimizer.compute_gradients(self.framework.loss)\n    self.train_op = optimizer.apply_gradients(gradients)\n\ndef load_from_ckpt(self):\n    if self.FLAGS.load < 0: # load lastest ckpt\n        with open(os.path.join(self.FLAGS.backup, \'checkpoint\'), \'r\') as f:\n            last = f.readlines()[-1].strip()\n            load_point = last.split(\' \')[1]\n            load_point = load_point.split(\'""\')[1]\n            load_point = load_point.split(\'-\')[-1]\n            self.FLAGS.load = int(load_point)\n    \n    load_point = os.path.join(self.FLAGS.backup, self.meta[\'name\'])\n    load_point = \'{}-{}\'.format(load_point, self.FLAGS.load)\n    self.say(\'Loading from {}\'.format(load_point))\n    try: self.saver.restore(self.sess, load_point)\n    except: load_old_graph(self, load_point)\n\ndef say(self, *msgs):\n    if not self.FLAGS.verbalise:\n        return\n    msgs = list(msgs)\n    for msg in msgs:\n        if msg is None: continue\n        print(msg)\n\ndef load_old_graph(self, ckpt): \n    ckpt_loader = create_loader(ckpt)\n    self.say(old_graph_msg.format(ckpt))\n    \n    for var in tf.global_variables():\n        name = var.name.split(\':\')[0]\n        args = [name, var.get_shape()]\n        val = ckpt_loader(args)\n        assert val is not None, \\\n        \'Cannot find and load {}\'.format(var.name)\n        shp = val.shape\n        plh = tf.placeholder(tf.float32, shp)\n        op = tf.assign(var, plh)\n        self.sess.run(op, {plh: val})\n\ndef _get_fps(self, frame):\n    elapsed = int()\n    start = timer()\n    preprocessed = self.framework.preprocess(frame)\n    feed_dict = {self.inp: [preprocessed]}\n    net_out = self.sess.run(self.out, feed_dict)[0]\n    processed = self.framework.postprocess(net_out, frame, False)\n    return timer() - start\n\ndef camera(self):\n    file = self.FLAGS.demo\n    SaveVideo = self.FLAGS.saveVideo\n    \n    if file == \'camera\':\n        file = 0\n    else:\n        assert os.path.isfile(file), \\\n        \'file {} does not exist\'.format(file)\n        \n    camera = cv2.VideoCapture(file)\n    \n    if file == 0:\n        self.say(\'Press [ESC] to quit demo\')\n        \n    assert camera.isOpened(), \\\n    \'Cannot capture source\'\n    \n    if file == 0:#camera window\n        cv2.namedWindow(\'\', 0)\n        _, frame = camera.read()\n        height, width, _ = frame.shape\n        cv2.resizeWindow(\'\', width, height)\n    else:\n        _, frame = camera.read()\n        height, width, _ = frame.shape\n\n    if SaveVideo:\n        fourcc = cv2.VideoWriter_fourcc(*\'XVID\')\n        if file == 0:#camera window\n          fps = 1 / self._get_fps(frame)\n          if fps < 1:\n            fps = 1\n        else:\n            fps = round(camera.get(cv2.CAP_PROP_FPS))\n        videoWriter = cv2.VideoWriter(\n            \'video.avi\', fourcc, fps, (width, height))\n\n    # buffers for demo in batch\n    buffer_inp = list()\n    buffer_pre = list()\n    \n    elapsed = int()\n    start = timer()\n    self.say(\'Press [ESC] to quit demo\')\n    # Loop through frames\n    while camera.isOpened():\n        elapsed += 1\n        _, frame = camera.read()\n        if frame is None:\n            print (\'\\nEnd of Video\')\n            break\n        preprocessed = self.framework.preprocess(frame)\n        buffer_inp.append(frame)\n        buffer_pre.append(preprocessed)\n        \n        # Only process and imshow when queue is full\n        if elapsed % self.FLAGS.queue == 0:\n            feed_dict = {self.inp: buffer_pre}\n            net_out = self.sess.run(self.out, feed_dict)\n            for img, single_out in zip(buffer_inp, net_out):\n                postprocessed = self.framework.postprocess(\n                    single_out, img, False)\n                if SaveVideo:\n                    videoWriter.write(postprocessed)\n                if file == 0: #camera window\n                    cv2.imshow(\'\', postprocessed)\n            # Clear Buffers\n            buffer_inp = list()\n            buffer_pre = list()\n\n        if elapsed % 5 == 0:\n            sys.stdout.write(\'\\r\')\n            sys.stdout.write(\'{0:3.3f} FPS\'.format(\n                elapsed / (timer() - start)))\n            sys.stdout.flush()\n        if file == 0: #camera window\n            choice = cv2.waitKey(1)\n            if choice == 27: break\n\n    sys.stdout.write(\'\\n\')\n    if SaveVideo:\n        videoWriter.release()\n    camera.release()\n    if file == 0: #camera window\n        cv2.destroyAllWindows()\n\ndef to_darknet(self):\n    darknet_ckpt = self.darknet\n\n    with self.graph.as_default() as g:\n        for var in tf.global_variables():\n            name = var.name.split(\':\')[0]\n            var_name = name.split(\'-\')\n            l_idx = int(var_name[0])\n            w_sig = var_name[1].split(\'/\')[-1]\n            l = darknet_ckpt.layers[l_idx]\n            l.w[w_sig] = var.eval(self.sess)\n\n    for layer in darknet_ckpt.layers:\n        for ph in layer.h:\n            layer.h[ph] = None\n\n    return darknet_ckpt\n'"
darkflow/utils/__init__.py,0,b''
darkflow/utils/box.py,0,"b'import numpy as np\n\nclass BoundBox:\n    def __init__(self, classes):\n        self.x, self.y = float(), float()\n        self.w, self.h = float(), float()\n        self.c = float()\n        self.class_num = classes\n        self.probs = np.zeros((classes,))\n\ndef overlap(x1,w1,x2,w2):\n    l1 = x1 - w1 / 2.;\n    l2 = x2 - w2 / 2.;\n    left = max(l1, l2)\n    r1 = x1 + w1 / 2.;\n    r2 = x2 + w2 / 2.;\n    right = min(r1, r2)\n    return right - left;\n\ndef box_intersection(a, b):\n    w = overlap(a.x, a.w, b.x, b.w);\n    h = overlap(a.y, a.h, b.y, b.h);\n    if w < 0 or h < 0: return 0;\n    area = w * h;\n    return area;\n\ndef box_union(a, b):\n    i = box_intersection(a, b);\n    u = a.w * a.h + b.w * b.h - i;\n    return u;\n\ndef box_iou(a, b):\n    return box_intersection(a, b) / box_union(a, b);\n\ndef prob_compare(box):\n    return box.probs[box.class_num]\n\ndef prob_compare2(boxa, boxb):\n    if (boxa.pi < boxb.pi):\n        return 1\n    elif(boxa.pi == boxb.pi):\n        return 0\n    else:\n        return -1'"
darkflow/utils/im_transform.py,0,"b'import numpy as np\nimport cv2\n\ndef imcv2_recolor(im, a = .1):\n\tt = [np.random.uniform()]\n\tt += [np.random.uniform()]\n\tt += [np.random.uniform()]\n\tt = np.array(t) * 2. - 1.\n\n\t# random amplify each channel\n\tim = im * (1 + t * a)\n\tmx = 255. * (1 + a)\n\tup = np.random.uniform() * 2 - 1\n# \tim = np.power(im/mx, 1. + up * .5)\n\tim = cv2.pow(im/mx, 1. + up * .5)\n\treturn np.array(im * 255., np.uint8)\n\ndef imcv2_affine_trans(im):\n\t# Scale and translate\n\th, w, c = im.shape\n\tscale = np.random.uniform() / 10. + 1.\n\tmax_offx = (scale-1.) * w\n\tmax_offy = (scale-1.) * h\n\toffx = int(np.random.uniform() * max_offx)\n\toffy = int(np.random.uniform() * max_offy)\n\t\n\tim = cv2.resize(im, (0,0), fx = scale, fy = scale)\n\tim = im[offy : (offy + h), offx : (offx + w)]\n\tflip = np.random.binomial(1, .5)\n\tif flip: im = cv2.flip(im, 1)\n\treturn im, [w, h, c], [scale, [offx, offy], flip]\n'"
darkflow/utils/loader.py,4,"b'import tensorflow as tf\nimport os\nfrom .. import dark\nimport numpy as np\nfrom os.path import basename\n\nclass loader(object):\n    """"""\n    interface to work with both .weights and .ckpt files\n    in loading / recollecting / resolving mode\n    """"""\n    VAR_LAYER = [\'convolutional\', \'connected\', \'local\', \n                 \'select\', \'conv-select\',\n                 \'extract\', \'conv-extract\']\n\n    def __init__(self, *args):\n        self.src_key = list()\n        self.vals = list()\n        self.load(*args)\n\n    def __call__(self, key):\n        for idx in range(len(key)):\n            val = self.find(key, idx)\n            if val is not None: return val\n        return None\n    \n    def find(self, key, idx):\n        up_to = min(len(self.src_key), 4)\n        for i in range(up_to):\n            key_b = self.src_key[i]\n            if key_b[idx:] == key[idx:]:\n                return self.yields(i)\n        return None\n\n    def yields(self, idx):\n        del self.src_key[idx]\n        temp = self.vals[idx]\n        del self.vals[idx]\n        return temp\n\nclass weights_loader(loader):\n    """"""one who understands .weights files""""""\n    \n    _W_ORDER = dict({ # order of param flattened into .weights file\n        \'convolutional\': [\n            \'biases\',\'gamma\',\'moving_mean\',\'moving_variance\',\'kernel\'\n        ],\n        \'connected\': [\'biases\', \'weights\'],\n        \'local\': [\'biases\', \'kernels\']\n    })\n\n    def load(self, path, src_layers):\n        self.src_layers = src_layers\n        walker = weights_walker(path)\n\n        for i, layer in enumerate(src_layers):\n            if layer.type not in self.VAR_LAYER: continue\n            self.src_key.append([layer])\n            \n            if walker.eof: new = None\n            else: \n                args = layer.signature\n                new = dark.darknet.create_darkop(*args)\n            self.vals.append(new)\n\n            if new is None: continue\n            order = self._W_ORDER[new.type]\n            for par in order:\n                if par not in new.wshape: continue\n                val = walker.walk(new.wsize[par])\n                new.w[par] = val\n            new.finalize(walker.transpose)\n\n        if walker.path is not None:\n            assert walker.offset == walker.size, \\\n            \'expect {} bytes, found {}\'.format(\n                walker.offset, walker.size)\n            print(\'Successfully identified {} bytes\'.format(\n                walker.offset))\n\nclass checkpoint_loader(loader):\n    """"""\n    one who understands .ckpt files, very much\n    """"""\n    def load(self, ckpt, ignore):\n        meta = ckpt + \'.meta\'\n        with tf.Graph().as_default() as graph:\n            with tf.Session().as_default() as sess:\n                saver = tf.train.import_meta_graph(meta)\n                saver.restore(sess, ckpt)\n                for var in tf.global_variables():\n                    name = var.name.split(\':\')[0]\n                    packet = [name, var.get_shape().as_list()]\n                    self.src_key += [packet]\n                    self.vals += [var.eval(sess)]\n\ndef create_loader(path, cfg = None):\n    if path is None:\n        load_type = weights_loader\n    elif \'.weights\' in path:\n        load_type = weights_loader\n    else: \n        load_type = checkpoint_loader\n    \n    return load_type(path, cfg)\n\nclass weights_walker(object):\n    """"""incremental reader of float32 binary files""""""\n    def __init__(self, path):\n        self.eof = False # end of file\n        self.path = path  # current pos\n        if path is None: \n            self.eof = True\n            return\n        else: \n            self.size = os.path.getsize(path)# save the path\n            major, minor, revision, seen = np.memmap(path,\n                shape = (), mode = \'r\', offset = 0,\n                dtype = \'({})i4,\'.format(4))\n            self.transpose = major > 1000 or minor > 1000\n            self.offset = 16\n\n    def walk(self, size):\n        if self.eof: return None\n        end_point = self.offset + 4 * size\n        assert end_point <= self.size, \\\n        \'Over-read {}\'.format(self.path)\n\n        float32_1D_array = np.memmap(\n            self.path, shape = (), mode = \'r\', \n            offset = self.offset,\n            dtype=\'({})float32,\'.format(size)\n        )\n\n        self.offset = end_point\n        if end_point == self.size: \n            self.eof = True\n        return float32_1D_array\n\ndef model_name(file_path):\n    file_name = basename(file_path)\n    ext = str()\n    if \'.\' in file_name: # exclude extension\n        file_name = file_name.split(\'.\')\n        ext = file_name[-1]\n        file_name = \'.\'.join(file_name[:-1])\n    if ext == str() or ext == \'meta\': # ckpt file\n        file_name = file_name.split(\'-\')\n        num = int(file_name[-1])\n        return \'-\'.join(file_name[:-1])\n    if ext == \'weights\':\n        return file_name'"
darkflow/utils/pascal_voc_clean_xml.py,0,"b'""""""\nparse PASCAL VOC xml annotations\n""""""\n\nimport os\nimport sys\nimport xml.etree.ElementTree as ET\nimport glob\n\n\ndef _pp(l): # pretty printing \n    for i in l: print(\'{}: {}\'.format(i,l[i]))\n\ndef pascal_voc_clean_xml(ANN, pick, exclusive = False):\n    print(\'Parsing for {} {}\'.format(\n            pick, \'exclusively\' * int(exclusive)))\n\n    dumps = list()\n    cur_dir = os.getcwd()\n    os.chdir(ANN)\n    annotations = os.listdir(\'.\')\n    annotations = glob.glob(str(annotations)+\'*.xml\')\n    size = len(annotations)\n\n    for i, file in enumerate(annotations):\n        # progress bar      \n        sys.stdout.write(\'\\r\')\n        percentage = 1. * (i+1) / size\n        progress = int(percentage * 20)\n        bar_arg = [progress*\'=\', \' \'*(19-progress), percentage*100]\n        bar_arg += [file]\n        sys.stdout.write(\'[{}>{}]{:.0f}%  {}\'.format(*bar_arg))\n        sys.stdout.flush()\n        \n        # actual parsing \n        in_file = open(file)\n        tree=ET.parse(in_file)\n        root = tree.getroot()\n        jpg = str(root.find(\'filename\').text)\n        imsize = root.find(\'size\')\n        w = int(imsize.find(\'width\').text)\n        h = int(imsize.find(\'height\').text)\n        all = list()\n\n        for obj in root.iter(\'object\'):\n                current = list()\n                name = obj.find(\'name\').text\n                if name not in pick:\n                        continue\n\n                xmlbox = obj.find(\'bndbox\')\n                xn = int(float(xmlbox.find(\'xmin\').text))\n                xx = int(float(xmlbox.find(\'xmax\').text))\n                yn = int(float(xmlbox.find(\'ymin\').text))\n                yx = int(float(xmlbox.find(\'ymax\').text))\n                current = [name,xn,yn,xx,yx]\n                all += [current]\n\n        add = [[jpg, [w, h, all]]]\n        dumps += add\n        in_file.close()\n\n    # gather all stats\n    stat = dict()\n    for dump in dumps:\n        all = dump[1][2]\n        for current in all:\n            if current[0] in pick:\n                if current[0] in stat:\n                    stat[current[0]]+=1\n                else:\n                    stat[current[0]] =1\n\n    print(\'\\nStatistics:\')\n    _pp(stat)\n    print(\'Dataset size: {}\'.format(len(dumps)))\n\n    os.chdir(cur_dir)\n    return dumps'"
darkflow/utils/process.py,0,"b'""""""\nWARNING: spaghetti code.\n""""""\n\nimport numpy as np\nimport pickle\nimport os\n\ndef parser(model):\n\t""""""\n\tRead the .cfg file to extract layers into `layers`\n\tas well as model-specific parameters into `meta`\n\t""""""\n\tdef _parse(l, i = 1):\n\t\treturn l.split(\'=\')[i].strip()\n\n\twith open(model, \'rb\') as f:\n\t\tlines = f.readlines()\n\n\tlines = [line.decode() for line in lines]\t\n\t\n\tmeta = dict(); layers = list() # will contains layers\' info\n\th, w, c = [int()] * 3; layer = dict()\n\tfor line in lines:\n\t\tline = line.strip()\n\t\tline = line.split(\'#\')[0]\n\t\tif \'[\' in line:\n\t\t\tif layer != dict(): \n\t\t\t\tif layer[\'type\'] == \'[net]\': \n\t\t\t\t\th = layer[\'height\']\n\t\t\t\t\tw = layer[\'width\']\n\t\t\t\t\tc = layer[\'channels\']\n\t\t\t\t\tmeta[\'net\'] = layer\n\t\t\t\telse:\n\t\t\t\t\tif layer[\'type\'] == \'[crop]\':\n\t\t\t\t\t\th = layer[\'crop_height\']\n\t\t\t\t\t\tw = layer[\'crop_width\']\n\t\t\t\t\tlayers += [layer]\t\t\t\t\n\t\t\tlayer = {\'type\': line}\n\t\telse:\n\t\t\ttry:\n\t\t\t\ti = float(_parse(line))\n\t\t\t\tif i == int(i): i = int(i)\n\t\t\t\tlayer[line.split(\'=\')[0].strip()] = i\n\t\t\texcept:\n\t\t\t\ttry:\n\t\t\t\t\tkey = _parse(line, 0)\n\t\t\t\t\tval = _parse(line, 1)\n\t\t\t\t\tlayer[key] = val\n\t\t\t\texcept:\n\t\t\t\t\t\'banana ninja yadayada\'\n\n\tmeta.update(layer) # last layer contains meta info\n\tif \'anchors\' in meta:\n\t\tsplits = meta[\'anchors\'].split(\',\')\n\t\tanchors = [float(x.strip()) for x in splits]\n\t\tmeta[\'anchors\'] = anchors\n\tmeta[\'model\'] = model # path to cfg, not model name\n\tmeta[\'inp_size\'] = [h, w, c]\n\treturn layers, meta\n\ndef cfg_yielder(model, binary):\n\t""""""\n\tyielding each layer information to initialize `layer`\n\t""""""\n\tlayers, meta = parser(model); yield meta;\n\th, w, c = meta[\'inp_size\']; l = w * h * c\n\n\t# Start yielding\n\tflat = False # flag for 1st dense layer\n\tconv = \'.conv.\' in model\n\tfor i, d in enumerate(layers):\n\t\t#-----------------------------------------------------\n\t\tif d[\'type\'] == \'[crop]\':\n\t\t\tyield [\'crop\', i]\n\t\t#-----------------------------------------------------\n\t\telif d[\'type\'] == \'[local]\':\n\t\t\tn = d.get(\'filters\', 1)\n\t\t\tsize = d.get(\'size\', 1)\n\t\t\tstride = d.get(\'stride\', 1)\n\t\t\tpad = d.get(\'pad\', 0)\n\t\t\tactivation = d.get(\'activation\', \'logistic\')\n\t\t\tw_ = (w - 1 - (1 - pad) * (size - 1)) // stride + 1\n\t\t\th_ = (h - 1 - (1 - pad) * (size - 1)) // stride + 1\n\t\t\tyield [\'local\', i, size, c, n, stride, \n\t\t\t\t\tpad, w_, h_, activation]\n\t\t\tif activation != \'linear\': yield [activation, i]\n\t\t\tw, h, c = w_, h_, n\n\t\t\tl = w * h * c\n\t\t#-----------------------------------------------------\n\t\telif d[\'type\'] == \'[convolutional]\':\n\t\t\tn = d.get(\'filters\', 1)\n\t\t\tsize = d.get(\'size\', 1)\n\t\t\tstride = d.get(\'stride\', 1)\n\t\t\tpad = d.get(\'pad\', 0)\n\t\t\tpadding = d.get(\'padding\', 0)\n\t\t\tif pad: padding = size // 2\n\t\t\tactivation = d.get(\'activation\', \'logistic\')\n\t\t\tbatch_norm = d.get(\'batch_normalize\', 0) or conv\n\t\t\tyield [\'convolutional\', i, size, c, n, \n\t\t\t\t   stride, padding, batch_norm,\n\t\t\t\t   activation]\n\t\t\tif activation != \'linear\': yield [activation, i]\n\t\t\tw_ = (w + 2 * padding - size) // stride + 1\n\t\t\th_ = (h + 2 * padding - size) // stride + 1\n\t\t\tw, h, c = w_, h_, n\n\t\t\tl = w * h * c\n\t\t#-----------------------------------------------------\n\t\telif d[\'type\'] == \'[maxpool]\':\n\t\t\tstride = d.get(\'stride\', 1)\n\t\t\tsize = d.get(\'size\', stride)\n\t\t\tpadding = d.get(\'padding\', (size-1) // 2)\n\t\t\tyield [\'maxpool\', i, size, stride, padding]\n\t\t\tw_ = (w + 2*padding) // d[\'stride\'] \n\t\t\th_ = (h + 2*padding) // d[\'stride\']\n\t\t\tw, h = w_, h_\n\t\t\tl = w * h * c\n\t\t#-----------------------------------------------------\n\t\telif d[\'type\'] == \'[avgpool]\':\n\t\t\tflat = True; l = c\n\t\t\tyield [\'avgpool\', i]\n\t\t#-----------------------------------------------------\n\t\telif d[\'type\'] == \'[softmax]\':\n\t\t\tyield [\'softmax\', i, d[\'groups\']]\n\t\t#-----------------------------------------------------\n\t\telif d[\'type\'] == \'[connected]\':\n\t\t\tif not flat:\n\t\t\t\tyield [\'flatten\', i]\n\t\t\t\tflat = True\n\t\t\tactivation = d.get(\'activation\', \'logistic\')\n\t\t\tyield [\'connected\', i, l, d[\'output\'], activation]\n\t\t\tif activation != \'linear\': yield [activation, i]\n\t\t\tl = d[\'output\']\n\t\t#-----------------------------------------------------\n\t\telif d[\'type\'] == \'[dropout]\': \n\t\t\tyield [\'dropout\', i, d[\'probability\']]\n\t\t#-----------------------------------------------------\n\t\telif d[\'type\'] == \'[select]\':\n\t\t\tif not flat:\n\t\t\t\tyield [\'flatten\', i]\n\t\t\t\tflat = True\n\t\t\tinp = d.get(\'input\', None)\n\t\t\tif type(inp) is str:\n\t\t\t\tfile = inp.split(\',\')[0]\n\t\t\t\tlayer_num = int(inp.split(\',\')[1])\n\t\t\t\twith open(file, \'rb\') as f:\n\t\t\t\t\tprofiles = pickle.load(f, encoding = \'latin1\')[0]\n\t\t\t\tlayer = profiles[layer_num]\n\t\t\telse: layer = inp\n\t\t\tactivation = d.get(\'activation\', \'logistic\')\n\t\t\td[\'keep\'] = d[\'keep\'].split(\'/\')\n\t\t\tclasses = int(d[\'keep\'][-1])\n\t\t\tkeep = [int(c) for c in d[\'keep\'][0].split(\',\')]\n\t\t\tkeep_n = len(keep)\n\t\t\ttrain_from = classes * d[\'bins\']\n\t\t\tfor count in range(d[\'bins\']-1):\n\t\t\t\tfor num in keep[-keep_n:]:\n\t\t\t\t\tkeep += [num + classes]\n\t\t\tk = 1\n\t\t\twhile layers[i-k][\'type\'] not in [\'[connected]\', \'[extract]\']:\n\t\t\t\tk += 1\n\t\t\t\tif i-k < 0:\n\t\t\t\t\tbreak\n\t\t\tif i-k < 0: l_ = l\n\t\t\telif layers[i-k][\'type\'] == \'connected\':\n\t\t\t\tl_ = layers[i-k][\'output\']\n\t\t\telse:\n\t\t\t\tl_ = layers[i-k].get(\'old\',[l])[-1]\n\t\t\tyield [\'select\', i, l_, d[\'old_output\'],\n\t\t\t\t   activation, layer, d[\'output\'], \n\t\t\t\t   keep, train_from]\n\t\t\tif activation != \'linear\': yield [activation, i]\n\t\t\tl = d[\'output\']\n\t\t#-----------------------------------------------------\n\t\telif d[\'type\'] == \'[conv-select]\':\n\t\t\tn = d.get(\'filters\', 1)\n\t\t\tsize = d.get(\'size\', 1)\n\t\t\tstride = d.get(\'stride\', 1)\n\t\t\tpad = d.get(\'pad\', 0)\n\t\t\tpadding = d.get(\'padding\', 0)\n\t\t\tif pad: padding = size // 2\n\t\t\tactivation = d.get(\'activation\', \'logistic\')\n\t\t\tbatch_norm = d.get(\'batch_normalize\', 0) or conv\n\t\t\td[\'keep\'] = d[\'keep\'].split(\'/\')\n\t\t\tclasses = int(d[\'keep\'][-1])\n\t\t\tkeep = [int(x) for x in d[\'keep\'][0].split(\',\')]\n\n\t\t\tsegment = classes + 5\n\t\t\tassert n % segment == 0, \\\n\t\t\t\'conv-select: segment failed\'\n\t\t\tbins = n // segment\n\t\t\tkeep_idx = list()\n\t\t\tfor j in range(bins):\n\t\t\t\toffset = j * segment\n\t\t\t\tfor k in range(5):\n\t\t\t\t\tkeep_idx += [offset + k]\n\t\t\t\tfor k in keep:\n\t\t\t\t\tkeep_idx += [offset + 5 + k]\n\t\t\tw_ = (w + 2 * padding - size) // stride + 1\n\t\t\th_ = (h + 2 * padding - size) // stride + 1\n\t\t\tc_ = len(keep_idx)\n\t\t\tyield [\'conv-select\', i, size, c, n, \n\t\t\t\t   stride, padding, batch_norm,\n\t\t\t\t   activation, keep_idx, c_]\n\t\t\tw, h, c = w_, h_, c_\n\t\t\tl = w * h * c\n\t\t#-----------------------------------------------------\n\t\telif d[\'type\'] == \'[conv-extract]\':\n\t\t\tfile = d[\'profile\']\n\t\t\twith open(file, \'rb\') as f:\n\t\t\t\tprofiles = pickle.load(f, encoding = \'latin1\')[0]\n\t\t\tinp_layer = None\n\t\t\tinp = d[\'input\']\n\t\t\tout = d[\'output\']\n\t\t\tinp_layer = None\n\t\t\tif inp >= 0:\n\t\t\t\tinp_layer = profiles[inp]\n\t\t\tif inp_layer is not None:\n\t\t\t\tassert len(inp_layer) == c, \\\n\t\t\t\t\'Conv-extract does not match input dimension\'\n\t\t\tout_layer = profiles[out]\n\n\t\t\tn = d.get(\'filters\', 1)\n\t\t\tsize = d.get(\'size\', 1)\n\t\t\tstride = d.get(\'stride\', 1)\n\t\t\tpad = d.get(\'pad\', 0)\n\t\t\tpadding = d.get(\'padding\', 0)\n\t\t\tif pad: padding = size // 2\n\t\t\tactivation = d.get(\'activation\', \'logistic\')\n\t\t\tbatch_norm = d.get(\'batch_normalize\', 0) or conv\n\t\t\t\n\t\t\tk = 1\n\t\t\tfind = [\'[convolutional]\',\'[conv-extract]\']\n\t\t\twhile layers[i-k][\'type\'] not in find:\n\t\t\t\tk += 1\n\t\t\t\tif i-k < 0: break\n\t\t\tif i-k >= 0:\n\t\t\t\tprevious_layer = layers[i-k]\n\t\t\t\tc_ = previous_layer[\'filters\']\n\t\t\telse:\n\t\t\t\tc_ = c\n\t\t\t\n\t\t\tyield [\'conv-extract\', i, size, c_, n, \n\t\t\t\t   stride, padding, batch_norm,\n\t\t\t\t   activation, inp_layer, out_layer]\n\t\t\tif activation != \'linear\': yield [activation, i]\n\t\t\tw_ = (w + 2 * padding - size) // stride + 1\n\t\t\th_ = (h + 2 * padding - size) // stride + 1\n\t\t\tw, h, c = w_, h_, len(out_layer)\n\t\t\tl = w * h * c\n\t\t#-----------------------------------------------------\n\t\telif d[\'type\'] == \'[extract]\':\n\t\t\tif not flat:\n\t\t\t\tyield[\'flatten\', i]\n\t\t\t\tflat = True\n\t\t\tactivation = d.get(\'activation\', \'logistic\')\n\t\t\tfile = d[\'profile\']\n\t\t\twith open(file, \'rb\') as f:\n\t\t\t\tprofiles = pickle.load(f, encoding = \'latin1\')[0]\n\t\t\tinp_layer = None\n\t\t\tinp = d[\'input\']\n\t\t\tout = d[\'output\']\n\t\t\tif inp >= 0:\n\t\t\t\tinp_layer = profiles[inp]\n\t\t\tout_layer = profiles[out]\n\t\t\told = d[\'old\']\n\t\t\told = [int(x) for x in old.split(\',\')]\n\t\t\tif inp_layer is not None:\n\t\t\t\tif len(old) > 2: \n\t\t\t\t\th_, w_, c_, n_ = old\n\t\t\t\t\tnew_inp = list()\n\t\t\t\t\tfor p in range(c_):\n\t\t\t\t\t\tfor q in range(h_):\n\t\t\t\t\t\t\tfor r in range(w_):\n\t\t\t\t\t\t\t\tif p not in inp_layer:\n\t\t\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t\t\tnew_inp += [r + w*(q + h*p)]\n\t\t\t\t\tinp_layer = new_inp\n\t\t\t\t\told = [h_ * w_ * c_, n_]\n\t\t\t\tassert len(inp_layer) == l, \\\n\t\t\t\t\'Extract does not match input dimension\'\n\t\t\td[\'old\'] = old\n\t\t\tyield [\'extract\', i] + old + [activation] + [inp_layer, out_layer]\n\t\t\tif activation != \'linear\': yield [activation, i]\n\t\t\tl = len(out_layer)\n\t\t#-----------------------------------------------------\n\t\telif d[\'type\'] == \'[route]\': # add new layer here\n\t\t\troutes = d[\'layers\']\n\t\t\tif type(routes) is int:\n\t\t\t\troutes = [routes]\n\t\t\telse:\n\t\t\t\troutes = [int(x.strip()) for x in routes.split(\',\')]\n\t\t\troutes = [i + x if x < 0 else x for x in routes]\n\t\t\tfor j, x in enumerate(routes):\n\t\t\t\tlx = layers[x]; \n\t\t\t\txtype = lx[\'type\']\n\t\t\t\t_size = lx[\'_size\'][:3]\n\t\t\t\tif j == 0:\n\t\t\t\t\th, w, c = _size\n\t\t\t\telse: \n\t\t\t\t\th_, w_, c_ = _size\n\t\t\t\t\tassert w_ == w and h_ == h, \\\n\t\t\t\t\t\'Routing incompatible conv sizes\'\n\t\t\t\t\tc += c_\n\t\t\tyield [\'route\', i, routes]\n\t\t\tl = w * h * c\n\t\t#-----------------------------------------------------\n\t\telif d[\'type\'] == \'[reorg]\':\n\t\t\tstride = d.get(\'stride\', 1)\n\t\t\tyield [\'reorg\', i, stride]\n\t\t\tw = w // stride; h = h // stride; \n\t\t\tc = c * (stride ** 2)\n\t\t\tl = w * h * c\n\t\t#-----------------------------------------------------\n\t\telse:\n\t\t\texit(\'Layer {} not implemented\'.format(d[\'type\']))\n\n\t\td[\'_size\'] = list([h, w, c, l, flat])\n\n\tif not flat: meta[\'out_size\'] = [h, w, c]\n\telse: meta[\'out_size\'] = l'"
darkflow/net/mnist/run.py,0,b''
darkflow/net/ops/__init__.py,0,"b""from .simple import *\nfrom .convolution import *\nfrom .baseop import HEADER, LINE\n\nop_types = {\n\t'convolutional': convolutional,\n\t'conv-select': conv_select,\n\t'connected': connected,\n\t'maxpool': maxpool,\n\t'leaky': leaky,\n\t'dropout': dropout,\n\t'flatten': flatten,\n\t'avgpool': avgpool,\n\t'softmax': softmax,\n\t'identity': identity,\n\t'crop': crop,\n\t'local': local,\n\t'select': select,\n\t'route': route,\n\t'reorg': reorg,\n\t'conv-extract': conv_extract,\n\t'extract': extract\n}\n\ndef op_create(*args):\n\tlayer_type = list(args)[0].type\n\treturn op_types[layer_type](*args)"""
darkflow/net/ops/baseop.py,8,"b'import tensorflow as tf\nimport numpy as np\n\nFORM = \'{:>6} | {:>6} | {:<32} | {}\'\nFORM_ = \'{}+{}+{}+{}\'\nLINE = FORM_.format(\'-\'*7, \'-\'*8, \'-\'*34, \'-\'*15) \nHEADER = FORM.format(\n    \'Source\', \'Train?\',\'Layer description\', \'Output size\')\n\ndef _shape(tensor): # work for both tf.Tensor & np.ndarray\n    if type(tensor) in [tf.Variable, tf.Tensor]: \n        return tensor.get_shape()\n    else: return tensor.shape\n\ndef _name(tensor):\n    return tensor.name.split(\':\')[0]\n\nclass BaseOp(object):\n    """"""\n    BaseOp objects initialise with a darknet\'s `layer` object\n    and input tensor of that layer `inp`, it calculates the \n    output of this layer and place the result in self.out\n    """"""\n\n    # let slim take care of the following vars\n    _SLIM = [\'gamma\', \'moving_mean\', \'moving_variance\']\n\n    def __init__(self, layer, inp, num, roof, feed):\n        self.inp = inp # BaseOp\n        self.num = num # int\n        self.out = None # tf.Tensor\n        self.lay = layer\n\n        self.scope = \'{}-{}\'.format(\n            str(self.num), self.lay.type)\n        self.gap = roof - self.num\n        self.var = not self.gap > 0\n        self.act = \'Load \'\n        self.convert(feed)\n        if self.var: self.train_msg = \'Yep! \'\n        else: self.train_msg = \'Nope \'\n        self.forward()\n\n    def convert(self, feed):\n        """"""convert self.lay to variables & placeholders""""""\n        for var in self.lay.wshape:\n            self.wrap_variable(var)\n        for ph in self.lay.h:\n            self.wrap_pholder(ph, feed)\n\n    def wrap_variable(self, var):\n        """"""wrap layer.w into variables""""""\n        val = self.lay.w.get(var, None)\n        if val is None:\n            shape = self.lay.wshape[var]\n            args = [0., 1e-2, shape]\n            if \'moving_mean\' in var:\n                val = np.zeros(shape)\n            elif \'moving_variance\' in var:\n                val = np.ones(shape)\n            else:\n                val = np.random.normal(*args)\n            self.lay.w[var] = val.astype(np.float32)\n            self.act = \'Init \'\n        if not self.var: return\n\n        val = self.lay.w[var]\n        self.lay.w[var] = tf.constant_initializer(val)\n        if var in self._SLIM: return\n        with tf.variable_scope(self.scope):\n            self.lay.w[var] = tf.get_variable(var,\n                shape = self.lay.wshape[var],\n                dtype = tf.float32,\n                initializer = self.lay.w[var])\n\n    def wrap_pholder(self, ph, feed):\n        """"""wrap layer.h into placeholders""""""\n        phtype = type(self.lay.h[ph])\n        if phtype is not dict: return\n\n        sig = \'{}/{}\'.format(self.scope, ph)\n        val = self.lay.h[ph]\n\n        self.lay.h[ph] = tf.placeholder_with_default(\n            val[\'dfault\'], val[\'shape\'], name = sig)\n        feed[self.lay.h[ph]] = val[\'feed\']\n\n    def verbalise(self): # console speaker\n        msg = str()\n        inp = _name(self.inp.out)\n        if inp == \'input\': \\\n        msg = FORM.format(\n            \'\', \'\', \'input\',\n            _shape(self.inp.out)) + \'\\n\'\n        if not self.act: return msg\n        return msg + FORM.format(\n            self.act, self.train_msg, \n            self.speak(), _shape(self.out))\n    \n    def speak(self): pass'"
darkflow/net/ops/convolution.py,11,"b""import tensorflow.contrib.slim as slim\nfrom .baseop import BaseOp\nimport tensorflow as tf\nimport numpy as np\n\nclass reorg(BaseOp):\n    def _forward(self):\n        inp = self.inp.out\n        shape = inp.get_shape().as_list()\n        _, h, w, c = shape\n        s = self.lay.stride\n        out = list()\n        for i in range(int(h/s)):\n            row_i = list()\n            for j in range(int(w/s)):\n                si, sj = s * i, s * j\n                boxij = inp[:, si: si+s, sj: sj+s,:]\n                flatij = tf.reshape(boxij, [-1,1,1,c*s*s])\n                row_i += [flatij]\n            out += [tf.concat(row_i, 2)]\n\n        self.out = tf.concat(out, 1)\n\n    def forward(self):\n        inp = self.inp.out\n        s = self.lay.stride\n        self.out = tf.extract_image_patches(\n            inp, [1,s,s,1], [1,s,s,1], [1,1,1,1], 'VALID')\n\n    def speak(self):\n        args = [self.lay.stride] * 2\n        msg = 'local flatten {}x{}'\n        return msg.format(*args)\n\n\nclass local(BaseOp):\n    def forward(self):\n        pad = [[self.lay.pad, self.lay.pad]] * 2;\n        temp = tf.pad(self.inp.out, [[0, 0]] + pad + [[0, 0]])\n\n        k = self.lay.w['kernels']\n        ksz = self.lay.ksize\n        half = int(ksz / 2)\n        out = list()\n        for i in range(self.lay.h_out):\n            row_i = list()\n            for j in range(self.lay.w_out):\n                kij = k[i * self.lay.w_out + j]\n                i_, j_ = i + 1 - half, j + 1 - half\n                tij = temp[:, i_ : i_ + ksz, j_ : j_ + ksz,:]\n                row_i.append(\n                    tf.nn.conv2d(tij, kij, \n                        padding = 'VALID', \n                        strides = [1] * 4))\n            out += [tf.concat(row_i, 2)]\n\n        self.out = tf.concat(out, 1)\n\n    def speak(self):\n        l = self.lay\n        args = [l.ksize] * 2 + [l.pad] + [l.stride]\n        args += [l.activation]\n        msg = 'loca {}x{}p{}_{}  {}'.format(*args)\n        return msg\n\nclass convolutional(BaseOp):\n    def forward(self):\n        pad = [[self.lay.pad, self.lay.pad]] * 2;\n        temp = tf.pad(self.inp.out, [[0, 0]] + pad + [[0, 0]])\n        temp = tf.nn.conv2d(temp, self.lay.w['kernel'], padding = 'VALID', \n            name = self.scope, strides = [1] + [self.lay.stride] * 2 + [1])\n        if self.lay.batch_norm: \n            temp = self.batchnorm(self.lay, temp)\n        self.out = tf.nn.bias_add(temp, self.lay.w['biases'])\n\n    def batchnorm(self, layer, inp):\n        if not self.var:\n            temp = (inp - layer.w['moving_mean'])\n            temp /= (np.sqrt(layer.w['moving_variance']) + 1e-5)\n            temp *= layer.w['gamma']\n            return temp\n        else:\n            args = dict({\n                'center' : False, 'scale' : True,\n                'epsilon': 1e-5, 'scope' : self.scope,\n                'updates_collections' : None,\n                'is_training': layer.h['is_training'],\n                'param_initializers': layer.w\n                })\n            return slim.batch_norm(inp, **args)\n\n    def speak(self):\n        l = self.lay\n        args = [l.ksize] * 2 + [l.pad] + [l.stride]\n        args += [l.batch_norm * '+bnorm']\n        args += [l.activation]\n        msg = 'conv {}x{}p{}_{}  {}  {}'.format(*args)\n        return msg\n\nclass conv_select(convolutional):\n    def speak(self):\n        l = self.lay\n        args = [l.ksize] * 2 + [l.pad] + [l.stride]\n        args += [l.batch_norm * '+bnorm']\n        args += [l.activation]\n        msg = 'sele {}x{}p{}_{}  {}  {}'.format(*args)\n        return msg\n\nclass conv_extract(convolutional):\n    def speak(self):\n        l = self.lay\n        args = [l.ksize] * 2 + [l.pad] + [l.stride]\n        args += [l.batch_norm * '+bnorm']\n        args += [l.activation]\n        msg = 'extr {}x{}p{}_{}  {}  {}'.format(*args)\n        return msg"""
darkflow/net/ops/simple.py,8,"b'import tensorflow.contrib.slim as slim\nfrom .baseop import BaseOp\nimport tensorflow as tf\nfrom distutils.version import StrictVersion\n\nclass route(BaseOp):\n\tdef forward(self):\n\t\troutes = self.lay.routes\n\t\troutes_out = list()\n\t\tfor r in routes:\n\t\t\tthis = self.inp\n\t\t\twhile this.lay.number != r:\n\t\t\t\tthis = this.inp\n\t\t\t\tassert this is not None, \\\n\t\t\t\t\'Routing to non-existence {}\'.format(r)\n\t\t\troutes_out += [this.out]\n\t\tself.out = tf.concat(routes_out, 3)\n\n\tdef speak(self):\n\t\tmsg = \'concat {}\'\n\t\treturn msg.format(self.lay.routes)\n\nclass connected(BaseOp):\n\tdef forward(self):\n\t\tself.out = tf.nn.xw_plus_b(\n\t\t\tself.inp.out,\n\t\t\tself.lay.w[\'weights\'], \n\t\t\tself.lay.w[\'biases\'], \n\t\t\tname = self.scope)\n\n\tdef speak(self):\n\t\tlayer = self.lay\n\t\targs = [layer.inp, layer.out]\n\t\targs += [layer.activation]\n\t\tmsg = \'full {} x {}  {}\'\n\t\treturn msg.format(*args)\n\nclass select(connected):\n\t""""""a weird connected layer""""""\n\tdef speak(self):\n\t\tlayer = self.lay\n\t\targs = [layer.inp, layer.out]\n\t\targs += [layer.activation]\n\t\tmsg = \'sele {} x {}  {}\'\n\t\treturn msg.format(*args)\n\nclass extract(connected):\n\t""""""a weird connected layer""""""\n\tdef speak(self):\n\t\tlayer = self.lay\n\t\targs = [len(layer.inp), len(layer.out)]\n\t\targs += [layer.activation]\n\t\tmsg = \'extr {} x {}  {}\'\n\t\treturn msg.format(*args)\n\nclass flatten(BaseOp):\n\tdef forward(self):\n\t\ttemp = tf.transpose(\n\t\t\tself.inp.out, [0,3,1,2])\n\t\tself.out = slim.flatten(\n\t\t\ttemp, scope = self.scope)\n\n\tdef speak(self): return \'flat\'\n\n\nclass softmax(BaseOp):\n\tdef forward(self):\n\t\tself.out = tf.nn.softmax(self.inp.out)\n\n\tdef speak(self): return \'softmax()\'\n\n\nclass avgpool(BaseOp):\n\tdef forward(self):\n\t\tself.out = tf.reduce_mean(\n\t\t\tself.inp.out, [1, 2], \n\t\t\tname = self.scope\n\t\t)\n\n\tdef speak(self): return \'avgpool()\'\n\n\nclass dropout(BaseOp):\n\tdef forward(self):\n\t\tif self.lay.h[\'pdrop\'] is None:\n\t\t\tself.lay.h[\'pdrop\'] = 1.0\n\t\tself.out = tf.nn.dropout(\n\t\t\tself.inp.out, \n\t\t\tself.lay.h[\'pdrop\'], \n\t\t\tname = self.scope\n\t\t)\n\n\tdef speak(self): return \'drop\'\n\n\nclass crop(BaseOp):\n\tdef forward(self):\n\t\tself.out =  self.inp.out * 2. - 1.\n\n\tdef speak(self):\n\t\treturn \'scale to (-1, 1)\'\n\n\nclass maxpool(BaseOp):\n\tdef forward(self):\n\t\tself.out = tf.nn.max_pool(\n\t\t\tself.inp.out, padding = \'SAME\',\n\t        ksize = [1] + [self.lay.ksize]*2 + [1], \n\t        strides = [1] + [self.lay.stride]*2 + [1],\n\t        name = self.scope\n\t    )\n\t\n\tdef speak(self):\n\t\tl = self.lay\n\t\treturn \'maxp {}x{}p{}_{}\'.format(\n\t\t\tl.ksize, l.ksize, l.pad, l.stride)\n\n\nclass leaky(BaseOp):\n\tdef forward(self):\n\t\tself.out = tf.maximum(\n\t\t\t.1 * self.inp.out, \n\t\t\tself.inp.out, \n\t\t\tname = self.scope\n\t\t)\n\n\tdef verbalise(self): pass\n\n\nclass identity(BaseOp):\n\tdef __init__(self, inp):\n\t\tself.inp = None\n\t\tself.out = inp\n'"
darkflow/net/vanilla/__init__.py,0,"b'from . import train\n\ndef constructor(self, meta, FLAGS):\n\tself.meta, self.FLAGS = meta, FLAGS'"
darkflow/net/vanilla/train.py,8,"b""import tensorflow as tf\n\n_LOSS_TYPE = ['sse','l2', 'smooth',\n\t\t\t  'sparse', 'l1', 'softmax',\n\t\t\t  'svm', 'fisher']\n\ndef loss(self, net_out):\n\tm = self.meta\n\tloss_type = self.meta['type']\n\tassert loss_type in _LOSS_TYPE, \\\n\t'Loss type {} not implemented'.format(loss_type)\n\n\tout = net_out\n\tout_shape = out.get_shape()\n\tout_dtype = out.dtype.base_dtype\n\t_truth = tf.placeholders(out_dtype, out_shape)\n\n\tself.placeholders = dict({\n\t\t\t'truth': _truth\n\t\t})\n\n\tdiff = _truth - out\n\tif loss_type in ['sse','12']:\n\t\tloss = tf.nn.l2_loss(diff)\n\n\telif loss_type == ['smooth']:\n\t\tsmall = tf.cast(diff < 1, tf.float32)\n\t\tlarge = 1. - small\n\t\tl1_loss = tf.nn.l1_loss(tf.multiply(diff, large))\n\t\tl2_loss = tf.nn.l2_loss(tf.multiply(diff, small))\n\t\tloss = l1_loss + l2_loss\n\n\telif loss_type in ['sparse', 'l1']:\n\t\tloss = l1_loss(diff)\n\n\telif loss_type == 'softmax':\n\t\tloss = tf.nn.softmax_cross_entropy_with_logits(logits, y)\n\t\tloss = tf.reduce_mean(loss)\n\n\telif loss_type == 'svm':\n\t\tassert 'train_size' in m, \\\n\t\t'Must specify'\n\t\tsize = m['train_size']\n\t\tself.nu = tf.Variable(tf.ones([train_size, num_classes]))\n"""
darkflow/net/yolo/__init__.py,0,"b'from . import train\nfrom . import predict\nfrom . import data\nfrom . import misc\nimport numpy as np\n\n\n"""""" YOLO framework __init__ equivalent""""""\n\ndef constructor(self, meta, FLAGS):\n\n\tdef _to_color(indx, base):\n\t\t"""""" return (b, r, g) tuple""""""\n\t\tbase2 = base * base\n\t\tb = 2 - indx / base2\n\t\tr = 2 - (indx % base2) / base\n\t\tg = 2 - (indx % base2) % base\n\t\treturn (b * 127, r * 127, g * 127)\n\tif \'labels\' not in meta:\n\t\tmisc.labels(meta, FLAGS) #We\'re not loading from a .pb so we do need to load the labels\n\tassert len(meta[\'labels\']) == meta[\'classes\'], (\n\t\t\'labels.txt and {} indicate\' + \' \'\n\t\t\'inconsistent class numbers\'\n\t).format(meta[\'model\'])\n\n\t# assign a color for each label\n\tcolors = list()\n\tbase = int(np.ceil(pow(meta[\'classes\'], 1./3)))\n\tfor x in range(len(meta[\'labels\'])): \n\t\tcolors += [_to_color(x, base)]\n\tmeta[\'colors\'] = colors\n\tself.fetch = list()\n\tself.meta, self.FLAGS = meta, FLAGS\n\n\t# over-ride the threshold in meta if FLAGS has it.\n\tif FLAGS.threshold > 0.0:\n\t\tself.meta[\'thresh\'] = FLAGS.threshold'"
darkflow/net/yolo/data.py,0,"b'from ...utils.pascal_voc_clean_xml import pascal_voc_clean_xml\nfrom numpy.random import permutation as perm\nfrom .predict import preprocess\n# from .misc import show\nfrom copy import deepcopy\nimport pickle\nimport numpy as np\nimport os \n\ndef parse(self, exclusive = False):\n    meta = self.meta\n    ext = \'.parsed\'\n    ann = self.FLAGS.annotation\n    if not os.path.isdir(ann):\n        msg = \'Annotation directory not found {} .\'\n        exit(\'Error: {}\'.format(msg.format(ann)))\n    print(\'\\n{} parsing {}\'.format(meta[\'model\'], ann))\n    dumps = pascal_voc_clean_xml(ann, meta[\'labels\'], exclusive)\n    return dumps\n\n\ndef _batch(self, chunk):\n    """"""\n    Takes a chunk of parsed annotations\n    returns value for placeholders of net\'s \n    input & loss layer correspond to this chunk\n    """"""\n    meta = self.meta\n    S, B = meta[\'side\'], meta[\'num\']\n    C, labels = meta[\'classes\'], meta[\'labels\']\n\n    # preprocess\n    jpg = chunk[0]; w, h, allobj_ = chunk[1]\n    allobj = deepcopy(allobj_)\n    path = os.path.join(self.FLAGS.dataset, jpg)\n    img = self.preprocess(path, allobj)\n\n    # Calculate regression target\n    cellx = 1. * w / S\n    celly = 1. * h / S\n    for obj in allobj:\n        centerx = .5*(obj[1]+obj[3]) #xmin, xmax\n        centery = .5*(obj[2]+obj[4]) #ymin, ymax\n        cx = centerx / cellx\n        cy = centery / celly\n        if cx >= S or cy >= S: return None, None\n        obj[3] = float(obj[3]-obj[1]) / w\n        obj[4] = float(obj[4]-obj[2]) / h\n        obj[3] = np.sqrt(obj[3])\n        obj[4] = np.sqrt(obj[4])\n        obj[1] = cx - np.floor(cx) # centerx\n        obj[2] = cy - np.floor(cy) # centery\n        obj += [int(np.floor(cy) * S + np.floor(cx))]\n\n    # show(im, allobj, S, w, h, cellx, celly) # unit test\n\n    # Calculate placeholders\' values\n    probs = np.zeros([S*S,C])\n    confs = np.zeros([S*S,B])\n    coord = np.zeros([S*S,B,4])\n    proid = np.zeros([S*S,C])\n    prear = np.zeros([S*S,4])\n    for obj in allobj:\n        probs[obj[5], :] = [0.] * C\n        probs[obj[5], labels.index(obj[0])] = 1.\n        proid[obj[5], :] = [1] * C\n        coord[obj[5], :, :] = [obj[1:5]] * B\n        prear[obj[5],0] = obj[1] - obj[3]**2 * .5 * S # xleft\n        prear[obj[5],1] = obj[2] - obj[4]**2 * .5 * S # yup\n        prear[obj[5],2] = obj[1] + obj[3]**2 * .5 * S # xright\n        prear[obj[5],3] = obj[2] + obj[4]**2 * .5 * S # ybot\n        confs[obj[5], :] = [1.] * B\n\n    # Finalise the placeholders\' values\n    upleft   = np.expand_dims(prear[:,0:2], 1)\n    botright = np.expand_dims(prear[:,2:4], 1)\n    wh = botright - upleft; \n    area = wh[:,:,0] * wh[:,:,1]\n    upleft   = np.concatenate([upleft] * B, 1)\n    botright = np.concatenate([botright] * B, 1)\n    areas = np.concatenate([area] * B, 1)\n\n    # value for placeholder at input layer\n    inp_feed_val = img\n    # value for placeholder at loss layer \n    loss_feed_val = {\n        \'probs\': probs, \'confs\': confs, \n        \'coord\': coord, \'proid\': proid,\n        \'areas\': areas, \'upleft\': upleft, \n        \'botright\': botright\n    }\n\n    return inp_feed_val, loss_feed_val\n\ndef shuffle(self):\n    batch = self.FLAGS.batch\n    data = self.parse()\n    size = len(data)\n\n    print(\'Dataset of {} instance(s)\'.format(size))\n    if batch > size: self.FLAGS.batch = batch = size\n    batch_per_epoch = int(size / batch)\n\n    for i in range(self.FLAGS.epoch):\n        shuffle_idx = perm(np.arange(size))\n        for b in range(batch_per_epoch):\n            # yield these\n            x_batch = list()\n            feed_batch = dict()\n\n            for j in range(b*batch, b*batch+batch):\n                train_instance = data[shuffle_idx[j]]\n                try:\n                    inp, new_feed = self._batch(train_instance)\n                except ZeroDivisionError:\n                    print(""This image\'s width or height are zeros: "", train_instance[0])\n                    print(\'train_instance:\', train_instance)\n                    print(\'Please remove or fix it then try again.\')\n                    raise\n\n                if inp is None: continue\n                x_batch += [np.expand_dims(inp, 0)]\n\n                for key in new_feed:\n                    new = new_feed[key]\n                    old_feed = feed_batch.get(key, \n                        np.zeros((0,) + new.shape))\n                    feed_batch[key] = np.concatenate([ \n                        old_feed, [new] \n                    ])      \n            \n            x_batch = np.concatenate(x_batch, 0)\n            yield x_batch, feed_batch\n        \n        print(\'Finish {} epoch(es)\'.format(i + 1))\n\n'"
darkflow/net/yolo/misc.py,0,"b'import pickle\nimport numpy as np\nimport cv2\nimport os\n\nlabels20 = [""aeroplane"", ""bicycle"", ""bird"", ""boat"", ""bottle"",\n    ""bus"", ""car"", ""cat"", ""chair"", ""cow"", ""diningtable"", ""dog"",\n    ""horse"", ""motorbike"", ""person"", ""pottedplant"", ""sheep"", ""sofa"",\n    ""train"", ""tvmonitor""]\n\n# 8, 14, 15, 19\n\nvoc_models = [\'yolo-full\', \'yolo-tiny\', \'yolo-small\',  # <- v1\n              \'yolov1\', \'tiny-yolov1\', # <- v1.1 \n              \'tiny-yolo-voc\', \'yolo-voc\'] # <- v2\n\ncoco_models = [\'tiny-coco\', \'yolo-coco\',  # <- v1.1\n               \'yolo\', \'tiny-yolo\'] # <- v2\n\ncoco_names = \'coco.names\'\nnine_names = \'9k.names\'\n\ndef labels(meta, FLAGS):    \n    model = os.path.basename(meta[\'name\'])\n    if model in voc_models: \n        print(""Model has a VOC model name, loading VOC labels."")\n        meta[\'labels\'] = labels20\n    else:\n        file = FLAGS.labels\n        if model in coco_models:\n            print(""Model has a coco model name, loading coco labels."")\n            file = os.path.join(FLAGS.config, coco_names)\n        elif model == \'yolo9000\':\n            print(""Model has name yolo9000, loading yolo9000 labels."")\n            file = os.path.join(FLAGS.config, nine_names)\n        with open(file, \'r\') as f:\n            meta[\'labels\'] = list()\n            labs = [l.strip() for l in f.readlines()]\n            for lab in labs:\n                if lab == \'----\': break\n                meta[\'labels\'] += [lab]\n    if len(meta[\'labels\']) == 0: \n        meta[\'labels\'] = labels20\n\ndef is_inp(self, name): \n    return name.lower().endswith((\'.jpg\', \'.jpeg\', \'.png\'))\n\ndef show(im, allobj, S, w, h, cellx, celly):\n    for obj in allobj:\n        a = obj[5] % S\n        b = obj[5] // S\n        cx = a + obj[1]\n        cy = b + obj[2]\n        centerx = cx * cellx\n        centery = cy * celly\n        ww = obj[3]**2 * w\n        hh = obj[4]**2 * h\n        cv2.rectangle(im,\n            (int(centerx - ww/2), int(centery - hh/2)),\n            (int(centerx + ww/2), int(centery + hh/2)),\n            (0,0,255), 2)\n    cv2.imshow(\'result\', im)\n    cv2.waitKey()\n    cv2.destroyAllWindows()\n\ndef show2(im, allobj):\n    for obj in allobj:\n        cv2.rectangle(im,\n            (obj[1], obj[2]), \n            (obj[3], obj[4]), \n            (0,0,255),2)\n    cv2.imshow(\'result\', im)\n    cv2.waitKey()\n    cv2.destroyAllWindows()\n\n\n_MVA = .05\n\ndef profile(self, net):\n    pass\n#     data = self.parse(exclusive = True)\n#     size = len(data); batch = self.FLAGS.batch\n#     all_inp_ = [x[0] for x in data]\n#     net.say(\'Will cycle through {} examples {} times\'.format(\n#         len(all_inp_), net.FLAGS.epoch))\n\n#     fetch = list(); mvave = list(); names = list();\n#     this = net.top\n#     conv_lay = [\'convolutional\', \'connected\', \'local\', \'conv-select\']\n#     while this.inp is not None:\n#         if this.lay.type in conv_lay:\n#             fetch = [this.out] + fetch\n#             names = [this.lay.signature] + names\n#             mvave = [None] + mvave \n#         this = this.inp\n#     print(names)\n\n#     total = int(); allofthem = len(all_inp_) * net.FLAGS.epoch\n#     batch = min(net.FLAGS.batch, len(all_inp_))\n#     for count in range(net.FLAGS.epoch):\n#         net.say(\'EPOCH {}\'.format(count))\n#         for j in range(len(all_inp_)/batch):\n#             inp_feed = list(); new_all = list()\n#             all_inp = all_inp_[j*batch: (j*batch+batch)]\n#             for inp in all_inp:\n#                 new_all += [inp]\n#                 this_inp = os.path.join(net.FLAGS.dataset, inp)\n#                 this_inp = net.framework.preprocess(this_inp)\n#                 expanded = np.expand_dims(this_inp, 0)\n#                 inp_feed.append(expanded)\n#             all_inp = new_all\n#             feed_dict = {net.inp : np.concatenate(inp_feed, 0)}\n#             out = net.sess.run(fetch, feed_dict)\n\n#             for i, o in enumerate(out):\n#                 oi = out[i];\n#                 dim = len(oi.shape) - 1\n#                 ai = mvave[i]; \n#                 mi = np.mean(oi, tuple(range(dim)))\n#                 vi = np.var(oi, tuple(range(dim)))\n#                 if ai is None: mvave[i] = [mi, vi]\n#                 elif \'banana ninja yada yada\':\n#                     ai[0] = (1 - _MVA) * ai[0] + _MVA * mi\n#                     ai[1] = (1 - _MVA) * ai[1] + _MVA * vi\n#             total += len(inp_feed)\n#             net.say(\'{} / {} = {}%\'.format(\n#                 total, allofthem, 100. * total / allofthem))\n\n#         with open(\'profile\', \'wb\') as f:\n#             pickle.dump([mvave], f, protocol = -1)\n'"
darkflow/net/yolo/predict.py,1,"b'from ...utils.im_transform import imcv2_recolor, imcv2_affine_trans\nfrom ...utils.box import BoundBox, box_iou, prob_compare\nimport numpy as np\nimport cv2\nimport os\nimport json\nfrom ...cython_utils.cy_yolo_findboxes import yolo_box_constructor\n\ndef _fix(obj, dims, scale, offs):\n\tfor i in range(1, 5):\n\t\tdim = dims[(i + 1) % 2]\n\t\toff = offs[(i + 1) % 2]\n\t\tobj[i] = int(obj[i] * scale - off)\n\t\tobj[i] = max(min(obj[i], dim), 0)\n\ndef resize_input(self, im):\n\th, w, c = self.meta[\'inp_size\']\n\timsz = cv2.resize(im, (w, h))\n\timsz = imsz / 255.\n\timsz = imsz[:,:,::-1]\n\treturn imsz\n\ndef process_box(self, b, h, w, threshold):\n\tmax_indx = np.argmax(b.probs)\n\tmax_prob = b.probs[max_indx]\n\tlabel = self.meta[\'labels\'][max_indx]\n\tif max_prob > threshold:\n\t\tleft  = int ((b.x - b.w/2.) * w)\n\t\tright = int ((b.x + b.w/2.) * w)\n\t\ttop   = int ((b.y - b.h/2.) * h)\n\t\tbot   = int ((b.y + b.h/2.) * h)\n\t\tif left  < 0    :  left = 0\n\t\tif right > w - 1: right = w - 1\n\t\tif top   < 0    :   top = 0\n\t\tif bot   > h - 1:   bot = h - 1\n\t\tmess = \'{}\'.format(label)\n\t\treturn (left, right, top, bot, mess, max_indx, max_prob)\n\treturn None\n\ndef findboxes(self, net_out):\n\tmeta, FLAGS = self.meta, self.FLAGS\n\tthreshold = FLAGS.threshold\n\t\n\tboxes = []\n\tboxes = yolo_box_constructor(meta, net_out, threshold)\n\t\n\treturn boxes\n\ndef preprocess(self, im, allobj = None):\n\t""""""\n\tTakes an image, return it as a numpy tensor that is readily\n\tto be fed into tfnet. If there is an accompanied annotation (allobj),\n\tmeaning this preprocessing is serving the train process, then this\n\timage will be transformed with random noise to augment training data,\n\tusing scale, translation, flipping and recolor. The accompanied\n\tparsed annotation (allobj) will also be modified accordingly.\n\t""""""\n\tif type(im) is not np.ndarray:\n\t\tim = cv2.imread(im)\n\n\tif allobj is not None: # in training mode\n\t\tresult = imcv2_affine_trans(im)\n\t\tim, dims, trans_param = result\n\t\tscale, offs, flip = trans_param\n\t\tfor obj in allobj:\n\t\t\t_fix(obj, dims, scale, offs)\n\t\t\tif not flip: continue\n\t\t\tobj_1_ =  obj[1]\n\t\t\tobj[1] = dims[0] - obj[3]\n\t\t\tobj[3] = dims[0] - obj_1_\n\t\tim = imcv2_recolor(im)\n\n\tim = self.resize_input(im)\n\tif allobj is None: return im\n\treturn im#, np.array(im) # for unit testing\n\ndef postprocess(self, net_out, im, save = True):\n\t""""""\n\tTakes net output, draw predictions, save to disk\n\t""""""\n\tmeta, FLAGS = self.meta, self.FLAGS\n\tthreshold = FLAGS.threshold\n\tcolors, labels = meta[\'colors\'], meta[\'labels\']\n\n\tboxes = self.findboxes(net_out)\n\n\tif type(im) is not np.ndarray:\n\t\timgcv = cv2.imread(im)\n\telse: imgcv = im\n\n\th, w, _ = imgcv.shape\n\tresultsForJSON = []\n\tfor b in boxes:\n\t\tboxResults = self.process_box(b, h, w, threshold)\n\t\tif boxResults is None:\n\t\t\tcontinue\n\t\tleft, right, top, bot, mess, max_indx, confidence = boxResults\n\t\tthick = int((h + w) // 300)\n\t\tif self.FLAGS.json:\n\t\t\tresultsForJSON.append({""label"": mess, ""confidence"": float(\'%.2f\' % confidence), ""topleft"": {""x"": left, ""y"": top}, ""bottomright"": {""x"": right, ""y"": bot}})\n\t\t\tcontinue\n\n\t\tcv2.rectangle(imgcv,\n\t\t\t(left, top), (right, bot),\n\t\t\tself.meta[\'colors\'][max_indx], thick)\n\t\tcv2.putText(\n\t\t\timgcv, mess, (left, top - 12),\n\t\t\t0, 1e-3 * h, self.meta[\'colors\'][max_indx],\n\t\t\tthick // 3)\n\n\n\tif not save: return imgcv\n\n\toutfolder = os.path.join(self.FLAGS.imgdir, \'out\')\n\timg_name = os.path.join(outfolder, os.path.basename(im))\n\tif self.FLAGS.json:\n\t\ttextJSON = json.dumps(resultsForJSON)\n\t\ttextFile = os.path.splitext(img_name)[0] + "".json""\n\t\twith open(textFile, \'w\') as f:\n\t\t\tf.write(textJSON)\n\t\treturn\t\n\n\tcv2.imwrite(img_name, imgcv)\n'"
darkflow/net/yolo/train.py,25,"b'import tensorflow.contrib.slim as slim\nimport pickle\nimport tensorflow as tf\nfrom .misc import show\nimport numpy as np\nimport os\n\ndef loss(self, net_out):\n    """"""\n    Takes net.out and placeholders value\n    returned in batch() func above,\n    to build train_op and loss\n    """"""\n    # meta\n    m = self.meta\n    sprob = float(m[\'class_scale\'])\n    sconf = float(m[\'object_scale\'])\n    snoob = float(m[\'noobject_scale\'])\n    scoor = float(m[\'coord_scale\'])\n    S, B, C = m[\'side\'], m[\'num\'], m[\'classes\']\n    SS = S * S # number of grid cells\n\n    print(\'{} loss hyper-parameters:\'.format(m[\'model\']))\n    print(\'\\tside    = {}\'.format(m[\'side\']))\n    print(\'\\tbox     = {}\'.format(m[\'num\']))\n    print(\'\\tclasses = {}\'.format(m[\'classes\']))\n    print(\'\\tscales  = {}\'.format([sprob, sconf, snoob, scoor]))\n\n    size1 = [None, SS, C]\n    size2 = [None, SS, B]\n\n    # return the below placeholders\n    _probs = tf.placeholder(tf.float32, size1)\n    _confs = tf.placeholder(tf.float32, size2)\n    _coord = tf.placeholder(tf.float32, size2 + [4])\n    # weights term for L2 loss\n    _proid = tf.placeholder(tf.float32, size1)\n    # material calculating IOU\n    _areas = tf.placeholder(tf.float32, size2)\n    _upleft = tf.placeholder(tf.float32, size2 + [2])\n    _botright = tf.placeholder(tf.float32, size2 + [2])\n\n    self.placeholders = {\n        \'probs\':_probs, \'confs\':_confs, \'coord\':_coord, \'proid\':_proid,\n        \'areas\':_areas, \'upleft\':_upleft, \'botright\':_botright\n    }\n\n    # Extract the coordinate prediction from net.out\n    coords = net_out[:, SS * (C + B):]\n    coords = tf.reshape(coords, [-1, SS, B, 4])\n    wh = tf.pow(coords[:,:,:,2:4], 2) * S # unit: grid cell\n    area_pred = wh[:,:,:,0] * wh[:,:,:,1] # unit: grid cell^2\n    centers = coords[:,:,:,0:2] # [batch, SS, B, 2]\n    floor = centers - (wh * .5) # [batch, SS, B, 2]\n    ceil  = centers + (wh * .5) # [batch, SS, B, 2]\n\n    # calculate the intersection areas\n    intersect_upleft   = tf.maximum(floor, _upleft)\n    intersect_botright = tf.minimum(ceil , _botright)\n    intersect_wh = intersect_botright - intersect_upleft\n    intersect_wh = tf.maximum(intersect_wh, 0.0)\n    intersect = tf.multiply(intersect_wh[:,:,:,0], intersect_wh[:,:,:,1])\n\n    # calculate the best IOU, set 0.0 confidence for worse boxes\n    iou = tf.truediv(intersect, _areas + area_pred - intersect)\n    best_box = tf.equal(iou, tf.reduce_max(iou, [2], True))\n    best_box = tf.to_float(best_box)\n    confs = tf.multiply(best_box, _confs)\n\n    # take care of the weight terms\n    conid = snoob * (1. - confs) + sconf * confs\n    weight_coo = tf.concat(4 * [tf.expand_dims(confs, -1)], 3)\n    cooid = scoor * weight_coo\n    proid = sprob * _proid\n\n    # flatten \'em all\n    probs = slim.flatten(_probs)\n    proid = slim.flatten(proid)\n    confs = slim.flatten(confs)\n    conid = slim.flatten(conid)\n    coord = slim.flatten(_coord)\n    cooid = slim.flatten(cooid)\n\n    self.fetch += [probs, confs, conid, cooid, proid]\n    true = tf.concat([probs, confs, coord], 1)\n    wght = tf.concat([proid, conid, cooid], 1)\n    print(\'Building {} loss\'.format(m[\'model\']))\n    loss = tf.pow(net_out - true, 2)\n    loss = tf.multiply(loss, wght)\n    loss = tf.reduce_sum(loss, 1)\n    self.loss = .5 * tf.reduce_mean(loss)\n    tf.summary.scalar(\'{} loss\'.format(m[\'model\']), self.loss)\n'"
darkflow/net/yolov2/__init__.py,0,b'from . import train\nfrom . import predict\nfrom . import data\nfrom ..yolo import misc\nimport numpy as np\n'
darkflow/net/yolov2/data.py,0,"b'from ...utils.pascal_voc_clean_xml import pascal_voc_clean_xml\nfrom numpy.random import permutation as perm\nfrom ..yolo.predict import preprocess\nfrom ..yolo.data import shuffle\nfrom copy import deepcopy\nimport pickle\nimport numpy as np\nimport os\n\ndef _batch(self, chunk):\n    """"""\n    Takes a chunk of parsed annotations\n    returns value for placeholders of net\'s \n    input & loss layer correspond to this chunk\n    """"""\n    meta = self.meta\n    labels = meta[\'labels\']\n    \n    H, W, _ = meta[\'out_size\']\n    C, B = meta[\'classes\'], meta[\'num\']\n    anchors = meta[\'anchors\']\n\n    # preprocess\n    jpg = chunk[0]; w, h, allobj_ = chunk[1]\n    allobj = deepcopy(allobj_)\n    path = os.path.join(self.FLAGS.dataset, jpg)\n    img = self.preprocess(path, allobj)\n\n    # Calculate regression target\n    cellx = 1. * w / W\n    celly = 1. * h / H\n    for obj in allobj:\n        centerx = .5*(obj[1]+obj[3]) #xmin, xmax\n        centery = .5*(obj[2]+obj[4]) #ymin, ymax\n        cx = centerx / cellx\n        cy = centery / celly\n        if cx >= W or cy >= H: return None, None\n        obj[3] = float(obj[3]-obj[1]) / w\n        obj[4] = float(obj[4]-obj[2]) / h\n        obj[3] = np.sqrt(obj[3])\n        obj[4] = np.sqrt(obj[4])\n        obj[1] = cx - np.floor(cx) # centerx\n        obj[2] = cy - np.floor(cy) # centery\n        obj += [int(np.floor(cy) * W + np.floor(cx))]\n\n    # show(im, allobj, S, w, h, cellx, celly) # unit test\n\n    # Calculate placeholders\' values\n    probs = np.zeros([H*W,B,C])\n    confs = np.zeros([H*W,B])\n    coord = np.zeros([H*W,B,4])\n    proid = np.zeros([H*W,B,C])\n    prear = np.zeros([H*W,4])\n    for obj in allobj:\n        probs[obj[5], :, :] = [[0.]*C] * B\n        probs[obj[5], :, labels.index(obj[0])] = 1.\n        proid[obj[5], :, :] = [[1.]*C] * B\n        coord[obj[5], :, :] = [obj[1:5]] * B\n        prear[obj[5],0] = obj[1] - obj[3]**2 * .5 * W # xleft\n        prear[obj[5],1] = obj[2] - obj[4]**2 * .5 * H # yup\n        prear[obj[5],2] = obj[1] + obj[3]**2 * .5 * W # xright\n        prear[obj[5],3] = obj[2] + obj[4]**2 * .5 * H # ybot\n        confs[obj[5], :] = [1.] * B\n\n    # Finalise the placeholders\' values\n    upleft   = np.expand_dims(prear[:,0:2], 1)\n    botright = np.expand_dims(prear[:,2:4], 1)\n    wh = botright - upleft; \n    area = wh[:,:,0] * wh[:,:,1]\n    upleft   = np.concatenate([upleft] * B, 1)\n    botright = np.concatenate([botright] * B, 1)\n    areas = np.concatenate([area] * B, 1)\n\n    # value for placeholder at input layer\n    inp_feed_val = img\n    # value for placeholder at loss layer \n    loss_feed_val = {\n        \'probs\': probs, \'confs\': confs, \n        \'coord\': coord, \'proid\': proid,\n        \'areas\': areas, \'upleft\': upleft, \n        \'botright\': botright\n    }\n\n    return inp_feed_val, loss_feed_val\n\n'"
darkflow/net/yolov2/predict.py,1,"b'import numpy as np\nimport math\nimport cv2\nimport os\nimport json\n#from scipy.special import expit\n#from utils.box import BoundBox, box_iou, prob_compare\n#from utils.box import prob_compare2, box_intersection\nfrom ...utils.box import BoundBox\nfrom ...cython_utils.cy_yolo2_findboxes import box_constructor\n\ndef expit(x):\n\treturn 1. / (1. + np.exp(-x))\n\ndef _softmax(x):\n    e_x = np.exp(x - np.max(x))\n    out = e_x / e_x.sum()\n    return out\n\ndef findboxes(self, net_out):\n\t# meta\n\tmeta = self.meta\n\tboxes = list()\n\tboxes=box_constructor(meta,net_out)\n\treturn boxes\n\ndef postprocess(self, net_out, im, save = True):\n\t""""""\n\tTakes net output, draw net_out, save to disk\n\t""""""\n\tboxes = self.findboxes(net_out)\n\n\t# meta\n\tmeta = self.meta\n\tthreshold = meta[\'thresh\']\n\tcolors = meta[\'colors\']\n\tlabels = meta[\'labels\']\n\tif type(im) is not np.ndarray:\n\t\timgcv = cv2.imread(im)\n\telse: imgcv = im\n\th, w, _ = imgcv.shape\n\t\n\tresultsForJSON = []\n\tfor b in boxes:\n\t\tboxResults = self.process_box(b, h, w, threshold)\n\t\tif boxResults is None:\n\t\t\tcontinue\n\t\tleft, right, top, bot, mess, max_indx, confidence = boxResults\n\t\tthick = int((h + w) // 300)\n\t\tif self.FLAGS.json:\n\t\t\tresultsForJSON.append({""label"": mess, ""confidence"": float(\'%.2f\' % confidence), ""topleft"": {""x"": left, ""y"": top}, ""bottomright"": {""x"": right, ""y"": bot}})\n\t\t\tcontinue\n\n\t\tcv2.rectangle(imgcv,\n\t\t\t(left, top), (right, bot),\n\t\t\tcolors[max_indx], thick)\n\t\tcv2.putText(imgcv, mess, (left, top - 12),\n\t\t\t0, 1e-3 * h, colors[max_indx],thick//3)\n\n\tif not save: return imgcv\n\n\toutfolder = os.path.join(self.FLAGS.imgdir, \'out\')\n\timg_name = os.path.join(outfolder, os.path.basename(im))\n\tif self.FLAGS.json:\n\t\ttextJSON = json.dumps(resultsForJSON)\n\t\ttextFile = os.path.splitext(img_name)[0] + "".json""\n\t\twith open(textFile, \'w\') as f:\n\t\t\tf.write(textJSON)\n\t\treturn\n\n\tcv2.imwrite(img_name, imgcv)\n'"
darkflow/net/yolov2/train.py,35,"b'import tensorflow.contrib.slim as slim\nimport pickle\nimport tensorflow as tf\nfrom ..yolo.misc import show\nimport numpy as np\nimport os\nimport math\n\ndef expit_tensor(x):\n\treturn 1. / (1. + tf.exp(-x))\n\ndef loss(self, net_out):\n    """"""\n    Takes net.out and placeholders value\n    returned in batch() func above,\n    to build train_op and loss\n    """"""\n    # meta\n    m = self.meta\n    sprob = float(m[\'class_scale\'])\n    sconf = float(m[\'object_scale\'])\n    snoob = float(m[\'noobject_scale\'])\n    scoor = float(m[\'coord_scale\'])\n    H, W, _ = m[\'out_size\']\n    B, C = m[\'num\'], m[\'classes\']\n    HW = H * W # number of grid cells\n    anchors = m[\'anchors\']\n\n    print(\'{} loss hyper-parameters:\'.format(m[\'model\']))\n    print(\'\\tH       = {}\'.format(H))\n    print(\'\\tW       = {}\'.format(W))\n    print(\'\\tbox     = {}\'.format(m[\'num\']))\n    print(\'\\tclasses = {}\'.format(m[\'classes\']))\n    print(\'\\tscales  = {}\'.format([sprob, sconf, snoob, scoor]))\n\n    size1 = [None, HW, B, C]\n    size2 = [None, HW, B]\n\n    # return the below placeholders\n    _probs = tf.placeholder(tf.float32, size1)\n    _confs = tf.placeholder(tf.float32, size2)\n    _coord = tf.placeholder(tf.float32, size2 + [4])\n    # weights term for L2 loss\n    _proid = tf.placeholder(tf.float32, size1)\n    # material calculating IOU\n    _areas = tf.placeholder(tf.float32, size2)\n    _upleft = tf.placeholder(tf.float32, size2 + [2])\n    _botright = tf.placeholder(tf.float32, size2 + [2])\n\n    self.placeholders = {\n        \'probs\':_probs, \'confs\':_confs, \'coord\':_coord, \'proid\':_proid,\n        \'areas\':_areas, \'upleft\':_upleft, \'botright\':_botright\n    }\n\n    # Extract the coordinate prediction from net.out\n    net_out_reshape = tf.reshape(net_out, [-1, H, W, B, (4 + 1 + C)])\n    coords = net_out_reshape[:, :, :, :, :4]\n    coords = tf.reshape(coords, [-1, H*W, B, 4])\n    adjusted_coords_xy = expit_tensor(coords[:,:,:,0:2])\n    adjusted_coords_wh = tf.sqrt(tf.exp(coords[:,:,:,2:4]) * np.reshape(anchors, [1, 1, B, 2]) / np.reshape([W, H], [1, 1, 1, 2]))\n    coords = tf.concat([adjusted_coords_xy, adjusted_coords_wh], 3)\n\n    adjusted_c = expit_tensor(net_out_reshape[:, :, :, :, 4])\n    adjusted_c = tf.reshape(adjusted_c, [-1, H*W, B, 1])\n\n    adjusted_prob = tf.nn.softmax(net_out_reshape[:, :, :, :, 5:])\n    adjusted_prob = tf.reshape(adjusted_prob, [-1, H*W, B, C])\n\n    adjusted_net_out = tf.concat([adjusted_coords_xy, adjusted_coords_wh, adjusted_c, adjusted_prob], 3)\n\n    wh = tf.pow(coords[:,:,:,2:4], 2) * np.reshape([W, H], [1, 1, 1, 2])\n    area_pred = wh[:,:,:,0] * wh[:,:,:,1]\n    centers = coords[:,:,:,0:2]\n    floor = centers - (wh * .5)\n    ceil  = centers + (wh * .5)\n\n    # calculate the intersection areas\n    intersect_upleft   = tf.maximum(floor, _upleft)\n    intersect_botright = tf.minimum(ceil , _botright)\n    intersect_wh = intersect_botright - intersect_upleft\n    intersect_wh = tf.maximum(intersect_wh, 0.0)\n    intersect = tf.multiply(intersect_wh[:,:,:,0], intersect_wh[:,:,:,1])\n\n    # calculate the best IOU, set 0.0 confidence for worse boxes\n    iou = tf.truediv(intersect, _areas + area_pred - intersect)\n    best_box = tf.equal(iou, tf.reduce_max(iou, [2], True))\n    best_box = tf.to_float(best_box)\n    confs = tf.multiply(best_box, _confs)\n\n    # take care of the weight terms\n    conid = snoob * (1. - confs) + sconf * confs\n    weight_coo = tf.concat(4 * [tf.expand_dims(confs, -1)], 3)\n    cooid = scoor * weight_coo\n    weight_pro = tf.concat(C * [tf.expand_dims(confs, -1)], 3)\n    proid = sprob * weight_pro\n\n    self.fetch += [_probs, confs, conid, cooid, proid]\n    true = tf.concat([_coord, tf.expand_dims(confs, 3), _probs ], 3)\n    wght = tf.concat([cooid, tf.expand_dims(conid, 3), proid ], 3)\n\n    print(\'Building {} loss\'.format(m[\'model\']))\n    loss = tf.pow(adjusted_net_out - true, 2)\n    loss = tf.multiply(loss, wght)\n    loss = tf.reshape(loss, [-1, H*W*B*(4 + 1 + C)])\n    loss = tf.reduce_sum(loss, 1)\n    self.loss = .5 * tf.reduce_mean(loss)\n    tf.summary.scalar(\'{} loss\'.format(m[\'model\']), self.loss)'"
