file_path,api_count,code
data/__init__.py,0,b''
layers/__init__.py,0,b''
layers/attention.py,5,"b'import tensorflow as tf\nimport os\nfrom tensorflow.python.keras.layers import Layer\nfrom tensorflow.python.keras import backend as K\n\n\nclass AttentionLayer(Layer):\n    """"""\n    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n    There are three sets of weights introduced W_a, U_a, and V_a\n     """"""\n\n    def __init__(self, **kwargs):\n        super(AttentionLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert isinstance(input_shape, list)\n        # Create a trainable weight variable for this layer.\n\n        self.W_a = self.add_weight(name=\'W_a\',\n                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n                                   initializer=\'uniform\',\n                                   trainable=True)\n        self.U_a = self.add_weight(name=\'U_a\',\n                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n                                   initializer=\'uniform\',\n                                   trainable=True)\n        self.V_a = self.add_weight(name=\'V_a\',\n                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n                                   initializer=\'uniform\',\n                                   trainable=True)\n\n        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n\n    def call(self, inputs, verbose=False):\n        """"""\n        inputs: [encoder_output_sequence, decoder_output_sequence]\n        """"""\n        assert type(inputs) == list\n        encoder_out_seq, decoder_out_seq = inputs\n        if verbose:\n            print(\'encoder_out_seq>\', encoder_out_seq.shape)\n            print(\'decoder_out_seq>\', decoder_out_seq.shape)\n\n        def energy_step(inputs, states):\n            """""" Step function for computing energy for a single decoder state """"""\n\n            assert_msg = ""States must be a list. However states {} is of type {}"".format(states, type(states))\n            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n\n            """""" Some parameters required for shaping tensors""""""\n            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n            de_hidden = inputs.shape[-1]\n\n            """""" Computing S.Wa where S=[s0, s1, ..., si]""""""\n            # <= batch_size*en_seq_len, latent_dim\n            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n            # <= batch_size*en_seq_len, latent_dim\n            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n            if verbose:\n                print(\'wa.s>\',W_a_dot_s.shape)\n\n            """""" Computing hj.Ua """"""\n            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n            if verbose:\n                print(\'Ua.h>\',U_a_dot_h.shape)\n\n            """""" tanh(S.Wa + hj.Ua) """"""\n            # <= batch_size*en_seq_len, latent_dim\n            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n            if verbose:\n                print(\'Ws+Uh>\', reshaped_Ws_plus_Uh.shape)\n\n            """""" softmax(va.tanh(S.Wa + hj.Ua)) """"""\n            # <= batch_size, en_seq_len\n            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n            # <= batch_size, en_seq_len\n            e_i = K.softmax(e_i)\n\n            if verbose:\n                print(\'ei>\', e_i.shape)\n\n            return e_i, [e_i]\n\n        def context_step(inputs, states):\n            """""" Step function for computing ci using ei """"""\n            # <= batch_size, hidden_size\n            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n            if verbose:\n                print(\'ci>\', c_i.shape)\n            return c_i, [c_i]\n\n        def create_inital_state(inputs, hidden_size):\n            # We are not using initial states, but need to pass something to K.rnn funciton\n            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n            return fake_state\n\n        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n\n        """""" Computing energy outputs """"""\n        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n        last_out, e_outputs, _ = K.rnn(\n            energy_step, decoder_out_seq, [fake_state_e],\n        )\n\n        """""" Computing context vectors """"""\n        last_out, c_outputs, _ = K.rnn(\n            context_step, e_outputs, [fake_state_c],\n        )\n\n        return c_outputs, e_outputs\n\n    def compute_output_shape(self, input_shape):\n        """""" Outputs produced by the layer """"""\n        return [\n            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n        ]'"
examples/nmt/__init__.py,0,b''
examples/nmt/model.py,0,"b'from tensorflow.python.keras.layers import Input, GRU, Dense, Concatenate, TimeDistributed\nfrom tensorflow.python.keras.models import Model\nfrom layers.attention import AttentionLayer\n\n\ndef define_nmt(hidden_size, batch_size, en_timesteps, en_vsize, fr_timesteps, fr_vsize):\n    """""" Defining a NMT model """"""\n\n    # Define an input sequence and process it.\n    if batch_size:\n        encoder_inputs = Input(batch_shape=(batch_size, en_timesteps, en_vsize), name=\'encoder_inputs\')\n        decoder_inputs = Input(batch_shape=(batch_size, fr_timesteps - 1, fr_vsize), name=\'decoder_inputs\')\n    else:\n        encoder_inputs = Input(shape=(en_timesteps, en_vsize), name=\'encoder_inputs\')\n        decoder_inputs = Input(shape=(fr_timesteps - 1, fr_vsize), name=\'decoder_inputs\')\n\n    # Encoder GRU\n    encoder_gru = GRU(hidden_size, return_sequences=True, return_state=True, name=\'encoder_gru\')\n    encoder_out, encoder_state = encoder_gru(encoder_inputs)\n\n    # Set up the decoder GRU, using `encoder_states` as initial state.\n    decoder_gru = GRU(hidden_size, return_sequences=True, return_state=True, name=\'decoder_gru\')\n    decoder_out, decoder_state = decoder_gru(decoder_inputs, initial_state=encoder_state)\n\n    # Attention layer\n    attn_layer = AttentionLayer(name=\'attention_layer\')\n    attn_out, attn_states = attn_layer([encoder_out, decoder_out])\n\n    # Concat attention input and decoder GRU output\n    decoder_concat_input = Concatenate(axis=-1, name=\'concat_layer\')([decoder_out, attn_out])\n\n    # Dense layer\n    dense = Dense(fr_vsize, activation=\'softmax\', name=\'softmax_layer\')\n    dense_time = TimeDistributed(dense, name=\'time_distributed_layer\')\n    decoder_pred = dense_time(decoder_concat_input)\n\n    # Full model\n    full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n    full_model.compile(optimizer=\'adam\', loss=\'categorical_crossentropy\')\n\n    full_model.summary()\n\n    """""" Inference model """"""\n    batch_size = 1\n\n    """""" Encoder (Inference) model """"""\n    encoder_inf_inputs = Input(batch_shape=(batch_size, en_timesteps, en_vsize), name=\'encoder_inf_inputs\')\n    encoder_inf_out, encoder_inf_state = encoder_gru(encoder_inf_inputs)\n    encoder_model = Model(inputs=encoder_inf_inputs, outputs=[encoder_inf_out, encoder_inf_state])\n\n    """""" Decoder (Inference) model """"""\n    decoder_inf_inputs = Input(batch_shape=(batch_size, 1, fr_vsize), name=\'decoder_word_inputs\')\n    encoder_inf_states = Input(batch_shape=(batch_size, en_timesteps, hidden_size), name=\'encoder_inf_states\')\n    decoder_init_state = Input(batch_shape=(batch_size, hidden_size), name=\'decoder_init\')\n\n    decoder_inf_out, decoder_inf_state = decoder_gru(decoder_inf_inputs, initial_state=decoder_init_state)\n    attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n    decoder_inf_concat = Concatenate(axis=-1, name=\'concat\')([decoder_inf_out, attn_inf_out])\n    decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n    decoder_model = Model(inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],\n                          outputs=[decoder_inf_pred, attn_inf_states, decoder_inf_state])\n\n    return full_model, encoder_model, decoder_model\n\n\nif __name__ == \'__main__\':\n\n    """""" Checking nmt model for toy examples """"""\n    define_nmt(64, None, 20, 30, 20, 20)'"
examples/nmt/train.py,0,"b'import tensorflow.keras as keras\n\nfrom tensorflow.python.keras.utils import to_categorical\nimport numpy as np\nimport os, sys\n\nproject_path = os.path.sep.join(os.path.abspath(__file__).split(os.path.sep)[:-3])\nif project_path not in sys.path:\n    sys.path.append(project_path)\n\nfrom examples.utils.data_helper import read_data, sents2sequences\nfrom examples.nmt.model import define_nmt\nfrom examples.utils.model_helper import plot_attention_weights\nfrom examples.utils.logger import get_logger\n\nbase_dir = os.path.sep.join(os.path.abspath(__file__).split(os.path.sep)[:-3])\nlogger = get_logger(""examples.nmt.train"",os.path.join(base_dir, \'logs\'))\n\nbatch_size = 64\nhidden_size = 96\nen_timesteps, fr_timesteps = 20, 20\n\ndef get_data(train_size, random_seed=100):\n\n    """""" Getting randomly shuffled training / testing data """"""\n    en_text = read_data(os.path.join(project_path, \'data\', \'small_vocab_en.txt\'))\n    fr_text = read_data(os.path.join(project_path, \'data\', \'small_vocab_fr.txt\'))\n    logger.info(\'Length of text: {}\'.format(len(en_text)))\n\n    fr_text = [\'sos \' + sent[:-1] + \'eos .\'  if sent.endswith(\'.\') else \'sos \' + sent + \' eos .\' for sent in fr_text]\n\n    np.random.seed(random_seed)\n    inds = np.arange(len(en_text))\n    np.random.shuffle(inds)\n\n    train_inds = inds[:train_size]\n    test_inds = inds[train_size:]\n    tr_en_text = [en_text[ti] for ti in train_inds]\n    tr_fr_text = [fr_text[ti] for ti in train_inds]\n\n    ts_en_text = [en_text[ti] for ti in test_inds]\n    ts_fr_text = [fr_text[ti] for ti in test_inds]\n\n    return tr_en_text, tr_fr_text, ts_en_text, ts_fr_text\n\n\ndef preprocess_data(en_tokenizer, fr_tokenizer, en_text, fr_text, en_timesteps, fr_timesteps):\n    """""" Preprocessing data and getting a sequence of word indices """"""\n\n    en_seq = sents2sequences(en_tokenizer, en_text, reverse=False, padding_type=\'pre\', pad_length=en_timesteps)\n    fr_seq = sents2sequences(fr_tokenizer, fr_text, pad_length=fr_timesteps)\n    logger.info(\'Vocabulary size (English): {}\'.format(np.max(en_seq)+1))\n    logger.info(\'Vocabulary size (French): {}\'.format(np.max(fr_seq)+1))\n    logger.debug(\'En text shape: {}\'.format(en_seq.shape))\n    logger.debug(\'Fr text shape: {}\'.format(fr_seq.shape))\n\n    return en_seq, fr_seq\n\n\ndef train(full_model, en_seq, fr_seq, batch_size, n_epochs=10):\n    """""" Training the model """"""\n\n    for ep in range(n_epochs):\n        losses = []\n        for bi in range(0, en_seq.shape[0] - batch_size, batch_size):\n\n            en_onehot_seq = to_categorical(en_seq[bi:bi + batch_size, :], num_classes=en_vsize)\n            fr_onehot_seq = to_categorical(fr_seq[bi:bi + batch_size, :], num_classes=fr_vsize)\n\n            full_model.train_on_batch([en_onehot_seq, fr_onehot_seq[:, :-1, :]], fr_onehot_seq[:, 1:, :])\n\n            l = full_model.evaluate([en_onehot_seq, fr_onehot_seq[:, :-1, :]], fr_onehot_seq[:, 1:, :],\n                                    batch_size=batch_size, verbose=0)\n\n            losses.append(l)\n        if (ep + 1) % 1 == 0:\n            logger.info(""Loss in epoch {}: {}"".format(ep + 1, np.mean(losses)))\n\n\ndef infer_nmt(encoder_model, decoder_model, test_en_seq, en_vsize, fr_vsize):\n    """"""\n    Infer logic\n    :param encoder_model: keras.Model\n    :param decoder_model: keras.Model\n    :param test_en_seq: sequence of word ids\n    :param en_vsize: int\n    :param fr_vsize: int\n    :return:\n    """"""\n\n    test_fr_seq = sents2sequences(fr_tokenizer, [\'sos\'], fr_vsize)\n    test_en_onehot_seq = to_categorical(test_en_seq, num_classes=en_vsize)\n    test_fr_onehot_seq = np.expand_dims(to_categorical(test_fr_seq, num_classes=fr_vsize), 1)\n\n    enc_outs, enc_last_state = encoder_model.predict(test_en_onehot_seq)\n    dec_state = enc_last_state\n    attention_weights = []\n    fr_text = \'\'\n    for i in range(20):\n\n        dec_out, attention, dec_state = decoder_model.predict([enc_outs, dec_state, test_fr_onehot_seq])\n        dec_ind = np.argmax(dec_out, axis=-1)[0, 0]\n\n        if dec_ind == 0:\n            break\n        test_fr_seq = sents2sequences(fr_tokenizer, [fr_index2word[dec_ind]], fr_vsize)\n        test_fr_onehot_seq = np.expand_dims(to_categorical(test_fr_seq, num_classes=fr_vsize), 1)\n\n        attention_weights.append((dec_ind, attention))\n        fr_text += fr_index2word[dec_ind] + \' \'\n\n    return fr_text, attention_weights\n\n\nif __name__ == \'__main__\':\n\n    debug = True\n    """""" Hyperparameters """"""\n\n    train_size = 100000 if not debug else 10000\n    filename = \'\'\n\n    tr_en_text, tr_fr_text, ts_en_text, ts_fr_text = get_data(train_size=train_size)\n\n    """""" Defining tokenizers """"""\n    en_tokenizer = keras.preprocessing.text.Tokenizer(oov_token=\'UNK\')\n    en_tokenizer.fit_on_texts(tr_en_text)\n\n    fr_tokenizer = keras.preprocessing.text.Tokenizer(oov_token=\'UNK\')\n    fr_tokenizer.fit_on_texts(tr_fr_text)\n\n    """""" Getting preprocessed data """"""\n    en_seq, fr_seq = preprocess_data(en_tokenizer, fr_tokenizer, tr_en_text, tr_fr_text, en_timesteps, fr_timesteps)\n\n    en_vsize = max(en_tokenizer.index_word.keys()) + 1\n    fr_vsize = max(fr_tokenizer.index_word.keys()) + 1\n\n    """""" Defining the full model """"""\n    full_model, infer_enc_model, infer_dec_model = define_nmt(\n        hidden_size=hidden_size, batch_size=batch_size,\n        en_timesteps=en_timesteps, fr_timesteps=fr_timesteps,\n        en_vsize=en_vsize, fr_vsize=fr_vsize)\n\n    n_epochs = 10 if not debug else 3\n    train(full_model, en_seq, fr_seq, batch_size, n_epochs)\n\n    """""" Save model """"""\n    if not os.path.exists(os.path.join(\'..\', \'h5.models\')):\n        os.mkdir(os.path.join(\'..\', \'h5.models\'))\n    full_model.save(os.path.join(\'..\', \'h5.models\', \'nmt.h5\'))\n\n    """""" Index2word """"""\n    en_index2word = dict(zip(en_tokenizer.word_index.values(), en_tokenizer.word_index.keys()))\n    fr_index2word = dict(zip(fr_tokenizer.word_index.values(), fr_tokenizer.word_index.keys()))\n\n    """""" Inferring with trained model """"""\n    test_en = ts_en_text[0]\n    logger.info(\'Translating: {}\'.format(test_en))\n\n    test_en_seq = sents2sequences(en_tokenizer, [test_en], pad_length=en_timesteps)\n    test_fr, attn_weights = infer_nmt(\n        encoder_model=infer_enc_model, decoder_model=infer_dec_model,\n        test_en_seq=test_en_seq, en_vsize=en_vsize, fr_vsize=fr_vsize)\n    logger.info(\'\\tFrench: {}\'.format(test_fr))\n\n    """""" Attention plotting """"""\n    plot_attention_weights(test_en_seq, attn_weights, en_index2word, fr_index2word, base_dir=base_dir)'"
examples/nmt_bidirectional/__init__.py,0,b''
examples/nmt_bidirectional/model.py,0,"b'from tensorflow.python.keras.layers import Input, GRU, Dense, Concatenate, TimeDistributed, Bidirectional\nfrom tensorflow.python.keras.models import Model\nfrom layers.attention import AttentionLayer\n\n\ndef define_nmt(hidden_size, batch_size, en_timesteps, en_vsize, fr_timesteps, fr_vsize):\n    """""" Defining a NMT model """"""\n\n    # Define an input sequence and process it.\n    if batch_size:\n        encoder_inputs = Input(batch_shape=(batch_size, en_timesteps, en_vsize), name=\'encoder_inputs\')\n        decoder_inputs = Input(batch_shape=(batch_size, fr_timesteps - 1, fr_vsize), name=\'decoder_inputs\')\n    else:\n        encoder_inputs = Input(shape=(en_timesteps, en_vsize), name=\'encoder_inputs\')\n        decoder_inputs = Input(shape=(fr_timesteps - 1, fr_vsize), name=\'decoder_inputs\')\n\n    # Encoder GRU\n    encoder_gru = Bidirectional(GRU(hidden_size, return_sequences=True, return_state=True, name=\'encoder_gru\'), name=\'bidirectional_encoder\')\n    encoder_out, encoder_fwd_state, encoder_back_state = encoder_gru(encoder_inputs)\n\n    # Set up the decoder GRU, using `encoder_states` as initial state.\n    decoder_gru = GRU(hidden_size*2, return_sequences=True, return_state=True, name=\'decoder_gru\')\n    decoder_out, decoder_state = decoder_gru(\n        decoder_inputs, initial_state=Concatenate(axis=-1)([encoder_fwd_state, encoder_back_state])\n    )\n\n    # Attention layer\n    attn_layer = AttentionLayer(name=\'attention_layer\')\n    attn_out, attn_states = attn_layer([encoder_out, decoder_out])\n\n    # Concat attention input and decoder GRU output\n    decoder_concat_input = Concatenate(axis=-1, name=\'concat_layer\')([decoder_out, attn_out])\n\n    # Dense layer\n    dense = Dense(fr_vsize, activation=\'softmax\', name=\'softmax_layer\')\n    dense_time = TimeDistributed(dense, name=\'time_distributed_layer\')\n    decoder_pred = dense_time(decoder_concat_input)\n\n    # Full model\n    full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n    full_model.compile(optimizer=\'adam\', loss=\'categorical_crossentropy\')\n\n    full_model.summary()\n\n    """""" Inference model """"""\n    batch_size = 1\n\n    """""" Encoder (Inference) model """"""\n    encoder_inf_inputs = Input(batch_shape=(batch_size, en_timesteps, en_vsize), name=\'encoder_inf_inputs\')\n    encoder_inf_out, encoder_inf_fwd_state, encoder_inf_back_state = encoder_gru(encoder_inf_inputs)\n    encoder_model = Model(inputs=encoder_inf_inputs, outputs=[encoder_inf_out, encoder_inf_fwd_state, encoder_inf_back_state])\n\n    """""" Decoder (Inference) model """"""\n    decoder_inf_inputs = Input(batch_shape=(batch_size, 1, fr_vsize), name=\'decoder_word_inputs\')\n    encoder_inf_states = Input(batch_shape=(batch_size, en_timesteps, 2*hidden_size), name=\'encoder_inf_states\')\n    decoder_init_state = Input(batch_shape=(batch_size, 2*hidden_size), name=\'decoder_init\')\n\n    decoder_inf_out, decoder_inf_state = decoder_gru(\n        decoder_inf_inputs, initial_state=decoder_init_state)\n    attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n    decoder_inf_concat = Concatenate(axis=-1, name=\'concat\')([decoder_inf_out, attn_inf_out])\n    decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n    decoder_model = Model(inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],\n                          outputs=[decoder_inf_pred, attn_inf_states, decoder_inf_state])\n\n    return full_model, encoder_model, decoder_model\n\n\nif __name__ == \'__main__\':\n\n    """""" Checking nmt model for toy examples """"""\n    define_nmt(64, None, 20, 30, 20, 20)'"
examples/nmt_bidirectional/train.py,0,"b'import tensorflow.keras as keras\n\nfrom tensorflow.python.keras.utils import to_categorical\nimport numpy as np\nimport os, sys\n\nproject_path = os.path.sep.join(os.path.abspath(__file__).split(os.path.sep)[:-3])\nif project_path not in sys.path:\n    sys.path.append(project_path)\n\nfrom examples.utils.data_helper import read_data, sents2sequences\nfrom examples.nmt_bidirectional.model import define_nmt\nfrom examples.utils.model_helper import plot_attention_weights\nfrom examples.utils.logger import get_logger\n\nbase_dir = os.path.sep.join(os.path.abspath(__file__).split(os.path.sep)[:-3])\nlogger = get_logger(""examples.nmt_bidirectional.train"", os.path.join(base_dir, \'logs\'))\n\nbatch_size = 64\nhidden_size = 96\nen_timesteps, fr_timesteps = 20, 20\n\n\ndef get_data(train_size, random_seed=100):\n\n    """""" Getting randomly shuffled training / testing data """"""\n    en_text = read_data(os.path.join(project_path, \'data\', \'small_vocab_en.txt\'))\n    fr_text = read_data(os.path.join(project_path, \'data\', \'small_vocab_fr.txt\'))\n    logger.info(\'Length of text: {}\'.format(len(en_text)))\n\n    fr_text = [\'sos \' + sent[:-1] + \'eos\' if sent.endswith(\'.\') else \'sos \' + sent + \' eos\' for sent in fr_text]\n\n    np.random.seed(random_seed)\n    inds = np.arange(len(en_text))\n    np.random.shuffle(inds)\n\n    train_inds = inds[:train_size]\n    test_inds = inds[train_size:]\n    tr_en_text = [en_text[ti] for ti in train_inds]\n    tr_fr_text = [fr_text[ti] for ti in train_inds]\n\n    ts_en_text = [en_text[ti] for ti in test_inds]\n    ts_fr_text = [fr_text[ti] for ti in test_inds]\n\n    logger.info(""Average length of an English sentence: {}"".format(\n        np.mean([len(en_sent.split("" "")) for en_sent in tr_en_text])))\n    logger.info(""Average length of a French sentence: {}"".format(\n        np.mean([len(fr_sent.split("" "")) for fr_sent in tr_fr_text])))\n    return tr_en_text, tr_fr_text, ts_en_text, ts_fr_text\n\n\ndef preprocess_data(en_tokenizer, fr_tokenizer, en_text, fr_text, en_timesteps, fr_timesteps):\n    """""" Preprocessing data and getting a sequence of word indices """"""\n\n    en_seq = sents2sequences(en_tokenizer, en_text, reverse=False, padding_type=\'pre\', pad_length=en_timesteps)\n    fr_seq = sents2sequences(fr_tokenizer, fr_text, pad_length=fr_timesteps)\n    logger.info(\'Vocabulary size (English): {}\'.format(np.max(en_seq)+1))\n    logger.info(\'Vocabulary size (French): {}\'.format(np.max(fr_seq)+1))\n    logger.debug(\'En text shape: {}\'.format(en_seq.shape))\n    logger.debug(\'Fr text shape: {}\'.format(fr_seq.shape))\n\n    return en_seq, fr_seq\n\n\ndef train(full_model, en_seq, fr_seq, batch_size, n_epochs=10):\n    """""" Training the model """"""\n\n    for ep in range(n_epochs):\n        losses = []\n        for bi in range(0, en_seq.shape[0] - batch_size, batch_size):\n\n            en_onehot_seq = to_categorical(en_seq[bi:bi + batch_size, :], num_classes=en_vsize)\n            fr_onehot_seq = to_categorical(fr_seq[bi:bi + batch_size, :], num_classes=fr_vsize)\n\n            full_model.train_on_batch([en_onehot_seq, fr_onehot_seq[:, :-1, :]], fr_onehot_seq[:, 1:, :])\n\n            l = full_model.evaluate([en_onehot_seq, fr_onehot_seq[:, :-1, :]], fr_onehot_seq[:, 1:, :],\n                                    batch_size=batch_size, verbose=0)\n\n            losses.append(l)\n        if (ep + 1) % 1 == 0:\n            logger.info(""Loss in epoch {}: {}"".format(ep + 1, np.mean(losses)))\n\n\ndef infer_nmt(encoder_model, decoder_model, test_en_seq, en_vsize, fr_vsize):\n    """"""\n    Infer logic\n    :param encoder_model: keras.Model\n    :param decoder_model: keras.Model\n    :param test_en_seq: sequence of word ids\n    :param en_vsize: int\n    :param fr_vsize: int\n    :return:\n    """"""\n\n    test_fr_seq = sents2sequences(fr_tokenizer, [\'sos\'], fr_vsize)\n    test_en_onehot_seq = to_categorical(test_en_seq, num_classes=en_vsize)\n    test_fr_onehot_seq = np.expand_dims(to_categorical(test_fr_seq, num_classes=fr_vsize), 1)\n\n    enc_outs, enc_fwd_state, enc_back_state = encoder_model.predict(test_en_onehot_seq)\n    dec_state = np.concatenate([enc_fwd_state, enc_back_state], axis=-1)\n    attention_weights = []\n    fr_text = \'\'\n\n    for i in range(fr_timesteps):\n\n        dec_out, attention, dec_state = decoder_model.predict(\n            [enc_outs, dec_state, test_fr_onehot_seq])\n        dec_ind = np.argmax(dec_out, axis=-1)[0, 0]\n\n        if dec_ind == 0:\n            break\n        test_fr_seq = sents2sequences(fr_tokenizer, [fr_index2word[dec_ind]], fr_vsize)\n        test_fr_onehot_seq = np.expand_dims(to_categorical(test_fr_seq, num_classes=fr_vsize), 1)\n\n        attention_weights.append((dec_ind, attention))\n        fr_text += fr_index2word[dec_ind] + \' \'\n\n    return fr_text, attention_weights\n\n\nif __name__ == \'__main__\':\n    debug = False\n\n    """""" Hyperparameters """"""\n\n    train_size = 100000 if not debug else 10000\n    filename = \'\'\n    tr_en_text, tr_fr_text, ts_en_text, ts_fr_text = get_data(train_size=train_size)\n\n    """""" Defining tokenizers """"""\n    en_tokenizer = keras.preprocessing.text.Tokenizer(oov_token=\'UNK\')\n    en_tokenizer.fit_on_texts(tr_en_text)\n\n    fr_tokenizer = keras.preprocessing.text.Tokenizer(oov_token=\'UNK\')\n    fr_tokenizer.fit_on_texts(tr_fr_text)\n\n    """""" Getting preprocessed data """"""\n    en_seq, fr_seq = preprocess_data(en_tokenizer, fr_tokenizer, tr_en_text, tr_fr_text, en_timesteps, fr_timesteps)\n\n    en_vsize = max(en_tokenizer.index_word.keys()) + 1\n    fr_vsize = max(fr_tokenizer.index_word.keys()) + 1\n\n    """""" Defining the full model """"""\n    full_model, infer_enc_model, infer_dec_model = define_nmt(\n        hidden_size=hidden_size, batch_size=batch_size,\n        en_timesteps=en_timesteps, fr_timesteps=fr_timesteps,\n        en_vsize=en_vsize, fr_vsize=fr_vsize)\n\n    n_epochs = 10 if not debug else 3\n    train(full_model, en_seq, fr_seq, batch_size, n_epochs)\n\n    """""" Save model """"""\n    if not os.path.exists(os.path.join(\'..\', \'h5.models\')):\n        os.mkdir(os.path.join(\'..\', \'h5.models\'))\n    full_model.save(os.path.join(\'..\', \'h5.models\', \'nmt.h5\'))\n\n    """""" Index2word """"""\n    en_index2word = dict(zip(en_tokenizer.word_index.values(), en_tokenizer.word_index.keys()))\n    fr_index2word = dict(zip(fr_tokenizer.word_index.values(), fr_tokenizer.word_index.keys()))\n\n    """""" Inferring with trained model """"""\n\n    np.random.seed(100)\n    rand_test_ids = np.random.randint(0, len(ts_en_text), size=10)\n    for rid in rand_test_ids:\n        test_en = ts_en_text[rid]\n        logger.info(\'\\nTranslating: {}\'.format(test_en))\n\n        test_en_seq = sents2sequences(en_tokenizer, [test_en], pad_length=en_timesteps)\n        test_fr, attn_weights = infer_nmt(\n            encoder_model=infer_enc_model, decoder_model=infer_dec_model,\n            test_en_seq=test_en_seq, en_vsize=en_vsize, fr_vsize=fr_vsize)\n        logger.info(\'\\tFrench: {}\'.format(test_fr))\n\n        """""" Attention plotting """"""\n        plot_attention_weights(test_en_seq, attn_weights, en_index2word, fr_index2word,\n                               base_dir=base_dir, filename=\'attention_{}.png\'.format(rid))'"
examples/utils/__init__.py,0,b''
examples/utils/data_helper.py,0,"b'import os\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\nimport numpy as np\n\n\ndef read_data(filename):\n    """""" Reading the zip file to extract text """"""\n    text = []\n    with open(filename, \'r\', encoding=\'utf-8\') as f:\n        i = 0\n        for row in f:\n            text.append(row)\n            i += 1\n    return text\n\n\ndef sents2sequences(tokenizer, sentences, reverse=False, pad_length=None, padding_type=\'post\'):\n    encoded_text = tokenizer.texts_to_sequences(sentences)\n    preproc_text = pad_sequences(encoded_text, padding=padding_type, maxlen=pad_length)\n    if reverse:\n        preproc_text = np.flip(preproc_text, axis=1)\n\n    return preproc_text\n\n'"
examples/utils/logger.py,0,"b'import logging\nimport logging.config\nimport yaml\nimport os\n\n\ndef get_logger(mod_name, log_dir):\n    if not os.path.exists(log_dir):\n        os.mkdir(log_dir)\n\n    config_filepath = os.path.join(os.path.realpath(os.path.dirname(__file__)), \'logger_config.yml\')\n    if os.path.exists(config_filepath):\n        with open(config_filepath, \'r\') as f:\n            config = yaml.safe_load(f.read())\n            config[""handlers""][""file""][""filename""] = os.path.join(log_dir, mod_name+\'.log\')\n            logging.config.dictConfig(config)\n    else:\n        logging.basicConfig(level=logging.INFO)\n\n    logger = logging.getLogger(mod_name)\n    logger.info(""Started log {}"".format(os.path.join(log_dir, mod_name)))\n    return logger'"
examples/utils/model_helper.py,0,"b'import os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef plot_attention_weights(encoder_inputs, attention_weights, en_id2word, fr_id2word, base_dir, filename=None):\n    """"""\n    Plots attention weights\n    :param encoder_inputs: Sequence of word ids (list/numpy.ndarray)\n    :param attention_weights: Sequence of (<word_id_at_decode_step_t>:<attention_weights_at_decode_step_t>)\n    :param en_id2word: dict\n    :param fr_id2word: dict\n    :return:\n    """"""\n\n    if len(attention_weights) == 0:\n        print(\'Your attention weights was empty. No attention map saved to the disk. \' +\n              \'\\nPlease check if the decoder produced  a proper translation\')\n        return\n\n    mats = []\n    dec_inputs = []\n    for dec_ind, attn in attention_weights:\n        mats.append(attn.reshape(-1))\n        dec_inputs.append(dec_ind)\n    attention_mat = np.transpose(np.array(mats))\n\n    fig, ax = plt.subplots(figsize=(32, 32))\n    ax.imshow(attention_mat)\n\n    ax.set_xticks(np.arange(attention_mat.shape[1]))\n    ax.set_yticks(np.arange(attention_mat.shape[0]))\n\n    ax.set_xticklabels([fr_id2word[inp] if inp != 0 else ""<Res>"" for inp in dec_inputs])\n    ax.set_yticklabels([en_id2word[inp] if inp != 0 else ""<Res>"" for inp in encoder_inputs.ravel()])\n\n    ax.tick_params(labelsize=32)\n    ax.tick_params(axis=\'x\', labelrotation=90)\n\n    if not os.path.exists(os.path.join(base_dir, \'results\')):\n        os.mkdir(os.path.join(base_dir, \'results\'))\n    if filename is None:\n        plt.savefig(os.path.join(base_dir, \'results\', \'attention.png\'))\n    else:\n        plt.savefig(os.path.join(base_dir, \'results\', \'{}\'.format(filename)))'"
