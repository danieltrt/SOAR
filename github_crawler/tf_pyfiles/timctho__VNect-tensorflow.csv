file_path,api_count,code
caffe_weights_to_pickle.py,0,"b""import caffe\nimport numpy as np\nimport pickle\nimport argparse\nfrom collections import OrderedDict\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--prototxt',\n                    default='models/vnect_net.prototxt')\nparser.add_argument('--caffemodel',\n                    default='models/vnect_model.caffemodel')\nparser.add_argument('--output_file',\n                    default='vnect.pkl')\nargs = parser.parse_args()\n\nif __name__ == '__main__':\n\n    pkl_weights = OrderedDict()\n\n    net = caffe.Net(args.prototxt,\n                    caffe.TEST,\n                    weights=args.caffemodel)\n\n    for layer in net.params.keys():\n        print(layer)\n\n    print('======')\n    cur_bn_name = ''\n    for layer in net.params.keys():\n        print(layer, len(net.params[layer]))\n\n        for i in range(len(net.params[layer])):\n            print(net.params[layer][i].data.shape)\n\n        if layer.startswith('bn'):\n            cur_bn_name = layer\n            pkl_weights[layer+'/moving_mean'] = np.asarray(net.params[layer][0].data) / net.params[layer][2].data\n            pkl_weights[layer+'/moving_variance'] = np.asarray(net.params[layer][1].data) / net.params[layer][2].data\n        elif layer.startswith('scale'):\n            pkl_weights[cur_bn_name+'/gamma'] = np.asarray(net.params[layer][0].data)\n            pkl_weights[cur_bn_name+'/beta'] = np.asarray(net.params[layer][1].data)\n        elif len(net.params[layer]) == 2:\n            pkl_weights[layer+'/weights'] = np.asarray(net.params[layer][0].data).transpose((2,3,1,0))\n            pkl_weights[layer+'/biases'] = np.asarray(net.params[layer][1].data)\n        elif len(net.params[layer]) == 1:\n            pkl_weights[layer+'/kernel'] = np.asarray(net.params[layer][0].data).transpose((2,3,1,0))\n\n    for layer in pkl_weights.keys():\n        print(layer, pkl_weights[layer].shape)\n\n    with open(args.output_file, 'wb') as f:\n        pickle.dump(pkl_weights, f)\n"""
demo.py,0,"b""import caffe\nimport argparse\nimport os\nimport cv2\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nimport utils\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--device', default='gpu')\nparser.add_argument('--model_dir', default='')\nparser.add_argument('--input_size', default=368)\nparser.add_argument('--num_of_joints', default=21)\nparser.add_argument('--pool_scale', default=8)\nparser.add_argument('--plot_2d', default=True)\nparser.add_argument('--plot_3d', default=True)\nargs = parser.parse_args()\n\njoint_color_code = [[139, 53, 255],\n                    [0, 56, 255],\n                    [43, 140, 237],\n                    [37, 168, 36],\n                    [147, 147, 0],\n                    [70, 17, 145]]\n\n# Limb parents of each joint\nlimb_parents = [1, 15, 1, 2, 3, 1, 5, 6, 14, 8, 9, 14, 11, 12, 14, 14, 1, 4, 7, 10, 13]\n\n# input scales\nscales = [1.0, 0.7]\n\n\ndef demo():\n    joints_2d = np.zeros(shape=(args.num_of_joints, 2), dtype=np.int32)\n    joints_3d = np.zeros(shape=(args.num_of_joints, 3), dtype=np.float32)\n\n    if args.plot_3d:\n        plt.ion()\n        fig = plt.figure()\n        ax = fig.add_subplot(121, projection='3d')\n        ax2 = fig.add_subplot(122)\n        plt.show()\n\n    if args.device == 'cpu':\n        caffe.set_mode_cpu()\n    elif args.device == 'gpu':\n        caffe.set_mode_gpu()\n        caffe.set_device(0)\n    else:\n        raise ValueError('No such device')\n\n    model_prototxt_path = os.path.join(args.model_dir, 'vnect_net.prototxt')\n    model_weight_path = os.path.join(args.model_dir, 'vnect_model.caffemodel')\n\n    # Load model\n    model = caffe.Net(model_prototxt_path,\n                      model_weight_path,\n                      caffe.TEST)\n\n    # Show network structure and shape\n    for layer_name in model.params.keys():\n        print(layer_name, model.params[layer_name][0].data.shape)\n    print('')\n\n    for i in model.blobs.keys():\n        print(i, model.blobs[i].data.shape)\n\n    cam = cv2.VideoCapture(0)\n    is_tracking = False\n    # for img_name in os.listdir('test_imgs'):\n    while True:\n        # if not is_tracking:\n\n        img_path = 'test_imgs/{}'.format('dance.jpg')\n        t1 = time.time()\n        input_batch = []\n\n        cam_img = utils.read_square_image('', cam, args.input_size, 'WEBCAM')\n        # cam_img = utils.read_square_image(img_path, '', args.input_size, 'IMAGE')\n        # cv2.imshow('', cam_img)\n        # cv2.waitKey(0)\n        orig_size_input = cam_img.astype(np.float32)\n\n        for scale in scales:\n            resized_img = utils.resize_pad_img(orig_size_input, scale, args.input_size)\n            input_batch.append(resized_img)\n\n        input_batch = np.asarray(input_batch, dtype=np.float32)\n        input_batch = np.transpose(input_batch, (0, 3, 1, 2))\n        input_batch /= 255.0\n        input_batch -= 0.4\n\n        model.blobs['data'].data[...] = input_batch\n\n        # Forward\n        model.forward()\n\n        # Get output data\n        x_hm = model.blobs['x_heatmap'].data\n        y_hm = model.blobs['y_heatmap'].data\n        z_hm = model.blobs['z_heatmap'].data\n        hm = model.blobs['heatmap'].data\n\n        # Trans coordinates\n        x_hm = x_hm.transpose([0, 2, 3, 1])\n        y_hm = y_hm.transpose([0, 2, 3, 1])\n        z_hm = z_hm.transpose([0, 2, 3, 1])\n        hm = hm.transpose([0, 2, 3, 1])\n\n        # Average scale outputs\n        hm_size = args.input_size // args.pool_scale\n        hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n        x_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n        y_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n        z_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n        for i in range(len(scales)):\n            rescale = 1.0 / scales[i]\n            scaled_hm = cv2.resize(hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n            scaled_x_hm = cv2.resize(x_hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n            scaled_y_hm = cv2.resize(y_hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n            scaled_z_hm = cv2.resize(z_hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n            mid = [scaled_hm.shape[0] // 2, scaled_hm.shape[1] // 2]\n            hm_avg += scaled_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                      mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n            x_hm_avg += scaled_x_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                        mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n            y_hm_avg += scaled_y_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                        mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n            z_hm_avg += scaled_z_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                        mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n        hm_avg /= len(scales)\n        x_hm_avg /= len(scales)\n        y_hm_avg /= len(scales)\n        z_hm_avg /= len(scales)\n\n        t2 = time.time()\n        # Get 2d joints\n        joints_2d = utils.extract_2d_joint_from_heatmap(hm_avg, args.input_size, joints_2d)\n\n        # Get 3d joints\n        joints_3d = utils.extract_3d_joints_from_heatmap(joints_2d, x_hm_avg, y_hm_avg, z_hm_avg, args.input_size,\n                                                         joints_3d)\n        print('Post FPS', 1/(time.time()-t2))\n\n        # Plot 2d location heatmap\n        joint_map = np.zeros(shape=(args.input_size, args.input_size, 3))\n        for joint_num in range(joints_2d.shape[0]):\n            cv2.circle(joint_map, center=(joints_2d[joint_num][1], joints_2d[joint_num][0]), radius=3,\n                       color=(255, 0, 0), thickness=-1)\n\n        # Plot 2d limbs\n        limb_img = utils.draw_limbs_2d(cam_img, joints_2d, limb_parents)\n\n        # Plot 3d limbs\n        if args.plot_3d:\n            ax.clear()\n            ax.view_init(azim=0, elev=90)\n            ax.set_xlim(-700, 700)\n            ax.set_ylim(-800, 800)\n            ax.set_zlim(-700, 700)\n            ax.set_xlabel('x')\n            ax.set_ylabel('y')\n            ax.set_zlabel('z')\n            utils.draw_limbs_3d(joints_3d, limb_parents, ax)\n\n        # draw heatmap\n        # hm_img = utils.draw_predicted_heatmap(hm_avg*200, args.input_size)\n        # cv2.imshow('hm', hm_img.astype(np.uint8))\n        # cv2.waitKey(0)\n\n\n        concat_img = np.concatenate((limb_img, joint_map), axis=1)\n\n        # ax2.imshow(concat_img[..., ::-1].astype(np.uint8))\n        cv2.imshow('2d', concat_img.astype(np.uint8))\n        cv2.waitKey(1)\n        # ax2.imshow(concat_img.astype(np.uint8))\n        # plt.pause(0.0001)\n        # plt.show(block=False)\n        print('Forward FPS', 1 / (time.time() - t1))\n\n\nif __name__ == '__main__':\n    demo()\n"""
demo_gl.py,0,"b""import caffe\nimport argparse\nimport os\nimport cv2\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport pygame\nfrom pygame.locals import *\nfrom OpenGL.GL import *\nfrom OpenGL.GLU import *\n\nimport utils\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--device', default='gpu')\nparser.add_argument('--model_dir', default='/media/tim_ho/HDD1/Projects/VNect-tensorflow/models')\nparser.add_argument('--input_size', default=368)\nparser.add_argument('--num_of_joints', default=21)\nparser.add_argument('--pool_scale', default=8)\nparser.add_argument('--plot_2d', default=False)\nparser.add_argument('--plot_3d', default=False)\nargs = parser.parse_args()\n\njoint_color_code = [[139, 53, 255],\n                    [0, 56, 255],\n                    [43, 140, 237],\n                    [37, 168, 36],\n                    [147, 147, 0],\n                    [70, 17, 145]]\n\n# Limb parents of each joint\nlimb_parents = [1, 15, 1, 2, 3, 1, 5, 6, 14, 8, 9, 14, 11, 12, 14, 14, 1, 4, 7, 10, 13]\n\n# input scales\nscales = [1.0, 0.7]\n\n\ndef demo():\n    joints_2d = np.zeros(shape=(args.num_of_joints, 2), dtype=np.int32)\n    joints_3d = np.zeros(shape=(args.num_of_joints, 3), dtype=np.float32)\n\n    if args.plot_3d:\n        plt.ion()\n        fig = plt.figure()\n        ax = fig.add_subplot(121, projection='3d')\n        ax2 = fig.add_subplot(122)\n        plt.show()\n\n    if args.device == 'cpu':\n        caffe.set_mode_cpu()\n    elif args.device == 'gpu':\n        caffe.set_mode_gpu()\n        caffe.set_device(0)\n    else:\n        raise ValueError('No such device')\n\n    model_prototxt_path = os.path.join(args.model_dir, 'vnect_net.prototxt')\n    model_weight_path = os.path.join(args.model_dir, 'vnect_model.caffemodel')\n\n    # Load model\n    model = caffe.Net(model_prototxt_path,\n                      model_weight_path,\n                      caffe.TEST)\n\n    # Show network structure and shape\n    for layer_name in model.params.keys():\n        print(layer_name, model.params[layer_name][0].data.shape)\n    print('')\n\n    for i in model.blobs.keys():\n        print(i, model.blobs[i].data.shape)\n\n    cam = cv2.VideoCapture(0)\n    is_tracking = False\n    # for img_name in os.listdir('test_imgs'):\n    while True:\n        # if not is_tracking:\n\n        img_path = 'test_imgs/{}'.format('dance.jpg')\n        t1 = time.time()\n        input_batch = []\n\n        cam_img = utils.read_square_image('', cam, args.input_size, 'WEBCAM')\n        # cam_img = utils.read_square_image(img_path, '', args.input_size, 'IMAGE')\n        # cv2.imshow('', cam_img)\n        # cv2.waitKey(0)\n        orig_size_input = cam_img.astype(np.float32)\n\n        for scale in scales:\n            resized_img = utils.resize_pad_img(orig_size_input, scale, args.input_size)\n            input_batch.append(resized_img)\n\n        input_batch = np.asarray(input_batch, dtype=np.float32)\n        input_batch = np.transpose(input_batch, (0, 3, 1, 2))\n        input_batch /= 255.0\n        input_batch -= 0.4\n\n        model.blobs['data'].data[...] = input_batch\n\n        # Forward\n        model.forward()\n\n        # Get output data\n        x_hm = model.blobs['x_heatmap'].data\n        y_hm = model.blobs['y_heatmap'].data\n        z_hm = model.blobs['z_heatmap'].data\n        hm = model.blobs['heatmap'].data\n\n        # Trans coordinates\n        x_hm = x_hm.transpose([0, 2, 3, 1])\n        y_hm = y_hm.transpose([0, 2, 3, 1])\n        z_hm = z_hm.transpose([0, 2, 3, 1])\n        hm = hm.transpose([0, 2, 3, 1])\n\n        # Average scale outputs\n        hm_size = args.input_size // args.pool_scale\n        hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n        x_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n        y_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n        z_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n        for i in range(len(scales)):\n            rescale = 1.0 / scales[i]\n            scaled_hm = cv2.resize(hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n            scaled_x_hm = cv2.resize(x_hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n            scaled_y_hm = cv2.resize(y_hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n            scaled_z_hm = cv2.resize(z_hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n            mid = [scaled_hm.shape[0] // 2, scaled_hm.shape[1] // 2]\n            hm_avg += scaled_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                      mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n            x_hm_avg += scaled_x_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                        mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n            y_hm_avg += scaled_y_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                        mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n            z_hm_avg += scaled_z_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                        mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n        hm_avg /= len(scales)\n        x_hm_avg /= len(scales)\n        y_hm_avg /= len(scales)\n        z_hm_avg /= len(scales)\n\n        t2 = time.time()\n        # Get 2d joints\n        joints_2d = utils.extract_2d_joint_from_heatmap(hm_avg, args.input_size, joints_2d)\n\n        # Get 3d joints\n        joints_3d = utils.extract_3d_joints_from_heatmap(joints_2d, x_hm_avg, y_hm_avg, z_hm_avg, args.input_size,\n                                                         joints_3d)\n        print('Post FPS', 1/(time.time()-t2))\n\n        # Plot 2d location heatmap\n        joint_map = np.zeros(shape=(args.input_size, args.input_size, 3))\n        for joint_num in range(joints_2d.shape[0]):\n            cv2.circle(joint_map, center=(joints_2d[joint_num][1], joints_2d[joint_num][0]), radius=3,\n                       color=(255, 0, 0), thickness=-1)\n\n        # Plot 2d limbs\n        limb_img = utils.draw_limbs_2d(cam_img, joints_2d, limb_parents)\n\n        # Plot 3d limbs\n        if args.plot_3d:\n            ax.clear()\n            ax.view_init(azim=0, elev=90)\n            ax.set_xlim(-700, 700)\n            ax.set_ylim(-800, 800)\n            ax.set_zlim(-700, 700)\n            ax.set_xlabel('x')\n            ax.set_ylabel('y')\n            ax.set_zlabel('z')\n            utils.draw_limbs_3d(joints_3d, limb_parents, ax)\n\n        # draw heatmap\n        # hm_img = utils.draw_predicted_heatmap(hm_avg*200, args.input_size)\n        # cv2.imshow('hm', hm_img.astype(np.uint8))\n        # cv2.waitKey(0)\n\n\n        glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)\n        utils.draw_limb_3d_gl(joints_3d, limb_parents)\n        pygame.display.flip()\n        pygame.time.wait(1)\n\n\n        concat_img = np.concatenate((limb_img, joint_map), axis=1)\n\n        # ax2.imshow(concat_img[..., ::-1].astype(np.uint8))\n        cv2.imshow('2d', concat_img.astype(np.uint8))\n        cv2.waitKey(1)\n        # ax2.imshow(concat_img.astype(np.uint8))\n        # plt.pause(0.0001)\n        # plt.show(block=False)\n        print('Forward FPS', 1 / (time.time() - t1))\n\n\nif __name__ == '__main__':\n    pygame.init()\n    display = (800, 600)\n    pygame.display.set_mode(display, DOUBLEBUF | OPENGL)\n\n    gluPerspective(70, (display[0] / display[1]), 0.1, 200.0)\n    view_range = 800\n    # glOrtho(-view_range, view_range,\n    #         -view_range, view_range,\n    #         -view_range, view_range)\n\n    glTranslatef(0.0, 0.0, 100)\n\n    demo()\n"""
demo_multithread.py,0,"b""import caffe\nimport argparse\nimport os\nimport cv2\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\nimport threading\nimport Queue\nfrom mpl_toolkits.mplot3d import Axes3D\n\nimport utils\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--device', default='gpu')\nparser.add_argument('--model_dir', default='/media/tim_ho/HDD1/Projects/VNect-tensorflow/models')\nparser.add_argument('--input_size', default=368)\nparser.add_argument('--num_of_joints', default=21)\nparser.add_argument('--pool_scale', default=8)\nparser.add_argument('--plot_2d', default=False)\nparser.add_argument('--plot_3d', default=True)\nargs = parser.parse_args()\n\njoint_color_code = [[139, 53, 255],\n                    [0, 56, 255],\n                    [43, 140, 237],\n                    [37, 168, 36],\n                    [147, 147, 0],\n                    [70, 17, 145]]\n\n# Limb parents of each joint\nlimb_parents = [1, 15, 1, 2, 3, 1, 5, 6, 14, 8, 9, 14, 11, 12, 14, 14, 1, 4, 7, 10, 13]\n\n# Input scales\nscales = [1.0, 0.7]\n\n\n# Global vars for threads\n# joints_2d = np.zeros(shape=(args.num_of_joints, 2), dtype=np.int32)\n# joints_3d = np.zeros(shape=(args.num_of_joints, 3), dtype=np.float32)\n# cam_img = np.zeros(shape=(args.input_size, args.input_size, 3), dtype=np.uint8)\n# hm_size = args.input_size // args.pool_scale\n# hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n# x_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n# y_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n# z_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n\n# Create queue between threads\ncam_model_q = Queue.Queue(1)\nmodel_post_q = Queue.Queue(1)\npost_render_q = Queue.Queue(1)\n\n\n\ndef camera_reader():\n    cam = cv2.VideoCapture(0)\n    while True:\n        t1 = time.time()\n        cam_img = utils.read_square_image('', cam, args.input_size, 'WEBCAM')\n        if not cam_model_q.full():\n            cam_model_q.put(cam_img)\n            # print('cam put')\n        print('Cam FPS', 1/(time.time()-t1))\n\n\n\ndef forward():\n    # global hm_avg, x_hm_avg, y_hm_avg, z_hm_avg\n    cam_img = np.zeros(shape=(args.input_size, args.input_size, 3), dtype=np.uint8)\n    joints_2d = np.zeros(shape=(args.num_of_joints, 2), dtype=np.int32)\n    joints_3d = np.zeros(shape=(args.num_of_joints, 3), dtype=np.float32)\n\n    if args.device == 'cpu':\n        caffe.set_mode_cpu()\n    elif args.device == 'gpu':\n        caffe.set_mode_gpu()\n        caffe.set_device(1)\n    else:\n        raise ValueError('No such device')\n\n    model_prototxt_path = os.path.join(args.model_dir, 'vnect_net.prototxt')\n    model_weight_path = os.path.join(args.model_dir, 'vnect_model.caffemodel')\n\n    # Load model\n    model = caffe.Net(model_prototxt_path,\n                      model_weight_path,\n                      caffe.TEST)\n\n    # Show network structure and shape\n    print('##################################################')\n    print('################Network Structures################')\n    print('##################################################')\n    for layer_name in model.params.keys():\n        print(layer_name, model.params[layer_name][0].data.shape)\n    print('')\n    print('##################################################')\n    print('##################################################')\n    print('##################################################')\n    print('\\n\\n\\n\\n')\n\n    print('##################################################')\n    print('################Input Output Blobs################')\n    print('##################################################')\n    for i in model.blobs.keys():\n        print(i, model.blobs[i].data.shape)\n    print('##################################################')\n    print('##################################################')\n    print('##################################################')\n\n    # cam = cv2.VideoCapture(0)\n    is_tracking = False\n    # for img_name in os.listdir('test_imgs'):\n    while True:\n        # if not is_tracking:\n\n        img_path = 'test_imgs/{}'.format('dance.jpg')\n        t1 = time.time()\n        input_batch = []\n\n        if not cam_model_q.empty():\n            cam_img = cam_model_q.get()\n            # print('forward get')\n        # cam_img = utils.read_square_image('', cam, args.input_size, 'WEBCAM')\n        # cam_img = utils.read_square_image(img_path, '', args.input_size, 'IMAGE')\n        # cv2.imshow('', cam_img)\n        # cv2.waitKey(0)\n        orig_size_input = cam_img.astype(np.float32)\n\n        for scale in scales:\n            resized_img = utils.resize_pad_img(orig_size_input, scale, args.input_size)\n            input_batch.append(resized_img)\n\n        input_batch = np.asarray(input_batch, dtype=np.float32)\n        input_batch = np.transpose(input_batch, (0, 3, 1, 2))\n        input_batch /= 255.0\n        input_batch -= 0.4\n\n        model.blobs['data'].data[...] = input_batch\n\n        # Forward\n        model.forward()\n\n        # Get output data\n        x_hm = model.blobs['x_heatmap'].data\n        y_hm = model.blobs['y_heatmap'].data\n        z_hm = model.blobs['z_heatmap'].data\n        hm = model.blobs['heatmap'].data\n\n        # Trans coordinates\n        x_hm = x_hm.transpose([0, 2, 3, 1])\n        y_hm = y_hm.transpose([0, 2, 3, 1])\n        z_hm = z_hm.transpose([0, 2, 3, 1])\n        hm = hm.transpose([0, 2, 3, 1])\n\n        # Average scale outputs\n        hm_size = args.input_size // args.pool_scale\n        hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n        x_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n        y_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n        z_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n        for i in range(len(scales)):\n            rescale = 1.0 / scales[i]\n            scaled_hm = cv2.resize(hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n            scaled_x_hm = cv2.resize(x_hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n            scaled_y_hm = cv2.resize(y_hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n            scaled_z_hm = cv2.resize(z_hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n            mid = [scaled_hm.shape[0] // 2, scaled_hm.shape[1] // 2]\n            hm_avg += scaled_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                      mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n            x_hm_avg += scaled_x_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                        mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n            y_hm_avg += scaled_y_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                        mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n            z_hm_avg += scaled_z_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                        mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n        hm_avg /= len(scales)\n        x_hm_avg /= len(scales)\n        y_hm_avg /= len(scales)\n        z_hm_avg /= len(scales)\n\n        t2 = time.time()\n        # Get 2d joints\n        joints_2d = utils.extract_2d_joint_from_heatmap(hm_avg, args.input_size, joints_2d)\n\n        # Get 3d joints\n        joints_3d = utils.extract_3d_joints_from_heatmap(joints_2d, x_hm_avg, y_hm_avg, z_hm_avg, args.input_size,\n                                                         joints_3d)\n        print('Post FPS', 1/(time.time()-t2))\n\n        if not model_post_q.full():\n            # model_post_q.put([hm_avg, x_hm_avg, y_hm_avg, z_hm_avg, cam_img])\n            model_post_q.put([joints_2d, joints_3d, cam_img])\n            # print('forward put')\n        print('Forward FPS', 1 / (time.time() - t1))\n\n\n\n\n            # Get 2d joints\n        # joints_2d = utils.extract_2d_joint_from_heatmap(hm_avg, args.input_size, joints_2d)\n\n        # Get 3d joints\n        # joints_3d = utils.extract_3d_joints_from_heatmap(joints_2d, x_hm_avg, y_hm_avg, z_hm_avg, args.input_size,\n        #                                                  joints_3d)\n\n\n        # plt.show(block=False)\n\n\n\n\ndef post_process():\n    # global joints_2d, joints_3d\n    joints_2d = np.zeros(shape=(args.num_of_joints, 2), dtype=np.int32)\n    joints_3d = np.zeros(shape=(args.num_of_joints, 3), dtype=np.float32)\n    hm_size = args.input_size // args.pool_scale\n    hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n    x_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n    y_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n    z_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n    cam_img = np.zeros(shape=(args.input_size, args.input_size, 3), dtype=np.uint8)\n\n\n    while True:\n        if not model_post_q.empty():\n            [hm_avg, x_hm_avg, y_hm_avg, z_hm_avg, cam_img] = model_post_q.get(False)\n            # print('post get')\n\n\n        t1 = time.time()\n        # Get 2d joints\n        joints_2d = utils.extract_2d_joint_from_heatmap(hm_avg, args.input_size, joints_2d)\n\n        # Get 3d joints\n        if args.plot_3d:\n            joints_3d = utils.extract_3d_joints_from_heatmap(joints_2d, x_hm_avg, y_hm_avg, z_hm_avg, args.input_size,\n                                                         joints_3d)\n        print('Post FPS', 1/(time.time()-t1))\n\n        if not post_render_q.full():\n            post_render_q.put([joints_2d, joints_3d, cam_img])\n            # print('post put')\n\n\n\n\ndef render_plt():\n    joints_2d = np.zeros(shape=(args.num_of_joints, 2), dtype=np.int32)\n    joints_3d = np.zeros(shape=(args.num_of_joints, 3), dtype=np.float32)\n    cam_img = np.zeros(shape=(args.input_size, args.input_size, 3), dtype=np.uint8)\n\n    if args.plot_3d and args.plot_2d:\n        plt.ion()\n        fig = plt.figure(figsize=(10,10))\n        ax = fig.add_subplot(121, projection='3d')\n        ax2 = fig.add_subplot(122)\n        plt.show()\n    elif args.plot_3d:\n        plt.ion()\n        fig = plt.figure(figsize=(10,10))\n        ax = fig.add_subplot(111, projection='3d')\n\n\n    while True:\n\n        if model_post_q.qsize() != 0:\n            [joints_2d, joints_3d, cam_img] = model_post_q.get(False)\n        else:\n            print('render old')\n\n        t1 = time.time()\n        # Plot 2d location heatmap\n        if args.plot_2d:\n            joint_map = np.zeros(shape=(args.input_size, args.input_size, 3))\n            for joint_num in range(joints_2d.shape[0]):\n                cv2.circle(joint_map, center=(joints_2d[joint_num][1], joints_2d[joint_num][0]), radius=3,\n                           color=(255, 0, 0), thickness=-1)\n\n            # Plot 2d limbs\n            limb_img = utils.draw_limbs_2d(cam_img, joints_2d, limb_parents)\n\n        # Plot 3d limbs\n        if args.plot_3d:\n            ax.clear()\n            ax.view_init(azim=0, elev=90)\n            ax.set_xlim(-700, 700)\n            ax.set_ylim(-800, 800)\n            ax.set_zlim(-700, 700)\n            ax.set_xlabel('x')\n            ax.set_ylabel('y')\n            ax.set_zlabel('z')\n            utils.draw_limbs_3d(joints_3d, limb_parents, ax)\n\n            # draw heatmap\n            # hm_img = utils.draw_predicted_heatmap(hm_avg*200, args.input_size)\n            # cv2.imshow('hm', hm_img.astype(np.uint8))\n            # cv2.waitKey(0)\n\n        if args.plot_2d and args.plot_3d:\n            concat_img = np.concatenate((limb_img, joint_map), axis=1)\n            ax2.imshow(concat_img[..., ::-1].astype(np.uint8))\n            plt.pause(1e-10)\n        elif args.plot_3d:\n            plt.pause(1e-10)\n        else:\n            concat_img = np.concatenate((limb_img, joint_map), axis=1)\n            cv2.imshow('2d', concat_img.astype(np.uint8))\n            cv2.waitKey(1)\n        # ax2.imshow(concat_img.astype(np.uint8))\n        print('Render FPS', 1 / (time.time() - t1))\n\n\n\n\n\nif __name__ == '__main__':\n    t1 = threading.Thread(target=camera_reader, name='cam_thread')\n    t2 = threading.Thread(target=forward, name='model_thread')\n    # t3 = threading.Thread(target=post_process, name='post_process_thread')\n    t4 = threading.Thread(target=render_plt, name='render_thread')\n\n    t1.start()\n    t2.start()\n    # t3.start()\n    t4.start()\n"""
demo_tf.py,10,"b""import argparse\nimport time\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom models.nets import vnect_model_bn_folded as vnect_model\nimport utils.utils as utils\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--demo_type', default='image')\nparser.add_argument('--device', default='cpu')\nparser.add_argument('--model_file', default='models/weights/vnect_tf')\nparser.add_argument('--test_img', default='test_imgs/yuniko.jpg')\nparser.add_argument('--input_size', default=368)\nparser.add_argument('--num_of_joints', default=21)\nparser.add_argument('--pool_scale', default=8)\nparser.add_argument('--plot_2d', default=True)\nparser.add_argument('--plot_3d', default=True)\nargs = parser.parse_args()\n\njoint_color_code = [[139, 53, 255],\n                    [0, 56, 255],\n                    [43, 140, 237],\n                    [37, 168, 36],\n                    [147, 147, 0],\n                    [70, 17, 145]]\n\n# Limb parents of each joint\nlimb_parents = [1, 15, 1, 2, 3, 1, 5, 6, 14, 8, 9, 14, 11, 12, 14, 14, 1, 4, 7, 10, 13]\n\n# input scales\nscales = [1.0, 0.7]\n\n# Use gpu or cpu\ngpu_count = {'GPU':1} if args.device == 'gpu' else {'GPU':0}\n\n\ndef demo_single_image():\n    if args.plot_3d:\n        plt.ion()\n        fig = plt.figure()\n        ax = fig.add_subplot(121, projection='3d')\n        ax2 = fig.add_subplot(122)\n        plt.show()\n\n    # Create model\n    model_tf = vnect_model.VNect(args.input_size)\n\n    # Create session\n    sess_config = tf.ConfigProto(device_count=gpu_count)\n    sess = tf.Session(config=sess_config)\n\n    # Restore weights\n    saver = tf.train.Saver()\n    saver.restore(sess, args.model_file)\n\n    # Joints placeholder\n    joints_2d = np.zeros(shape=(args.num_of_joints, 2), dtype=np.int32)\n    joints_3d = np.zeros(shape=(args.num_of_joints, 3), dtype=np.float32)\n\n\n    img_path = args.test_img\n    t1 = time.time()\n    input_batch = []\n\n    cam_img = utils.read_square_image(img_path, '', args.input_size, 'IMAGE')\n    orig_size_input = cam_img.astype(np.float32)\n\n    # Create multi-scale inputs\n    for scale in scales:\n        resized_img = utils.resize_pad_img(orig_size_input, scale, args.input_size)\n        input_batch.append(resized_img)\n\n    input_batch = np.asarray(input_batch, dtype=np.float32)\n    input_batch /= 255.0\n    input_batch -= 0.4\n\n    # Inference\n    [hm, x_hm, y_hm, z_hm] = sess.run(\n        [model_tf.heapmap, model_tf.x_heatmap, model_tf.y_heatmap, model_tf.z_heatmap],\n        feed_dict={model_tf.input_holder: input_batch})\n\n    # Average scale outputs\n    hm_size = args.input_size // args.pool_scale\n    hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n    x_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n    y_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n    z_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n    for i in range(len(scales)):\n        rescale = 1.0 / scales[i]\n        scaled_hm = cv2.resize(hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n        scaled_x_hm = cv2.resize(x_hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n        scaled_y_hm = cv2.resize(y_hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n        scaled_z_hm = cv2.resize(z_hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n        mid = [scaled_hm.shape[0] // 2, scaled_hm.shape[1] // 2]\n        hm_avg += scaled_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                  mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n        x_hm_avg += scaled_x_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                    mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n        y_hm_avg += scaled_y_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                    mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n        z_hm_avg += scaled_z_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                    mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n    hm_avg /= len(scales)\n    x_hm_avg /= len(scales)\n    y_hm_avg /= len(scales)\n    z_hm_avg /= len(scales)\n\n    # Get 2d joints\n    utils.extract_2d_joint_from_heatmap(hm_avg, args.input_size, joints_2d)\n\n    # Get 3d joints\n    utils.extract_3d_joints_from_heatmap(joints_2d, x_hm_avg, y_hm_avg, z_hm_avg, args.input_size, joints_3d)\n\n    if args.plot_2d:\n        # Plot 2d joint location\n        joint_map = np.zeros(shape=(args.input_size, args.input_size, 3))\n        for joint_num in range(joints_2d.shape[0]):\n            cv2.circle(joint_map, center=(joints_2d[joint_num][1], joints_2d[joint_num][0]), radius=3,\n                       color=(255, 0, 0), thickness=-1)\n        # Draw 2d limbs\n        utils.draw_limbs_2d(cam_img, joints_2d, limb_parents)\n\n\n    print('FPS: {:>2.2f}'.format(1 / (time.time() - t1)))\n\n    if args.plot_3d:\n        ax.clear()\n        ax.view_init(azim=0, elev=90)\n        ax.set_xlim(-50, 50)\n        ax.set_ylim(-50, 50)\n        ax.set_zlim(-50, 50)\n        ax.set_xlabel('x')\n        ax.set_ylabel('y')\n        ax.set_zlabel('z')\n        utils.draw_limbs_3d(joints_3d, limb_parents, ax)\n\n        if args.plot_2d:\n            # Display 2d results\n            concat_img = np.concatenate((cam_img[:, :, ::-1], joint_map), axis=1)\n            ax2.imshow(concat_img.astype(np.uint8))\n        plt.pause(100000)\n        plt.show(block=False)\n\n    elif args.plot_2d:\n        concat_img = np.concatenate((cam_img, joint_map), axis=1)\n        cv2.imshow('2D img', concat_img.astype(np.uint8))\n        cv2.waitKey(1)\n\n\n\ndef demo_webcam():\n    if args.plot_3d:\n        plt.ion()\n        fig = plt.figure()\n        ax = fig.add_subplot(121, projection='3d')\n        ax2 = fig.add_subplot(122)\n        plt.show()\n\n    # Create model\n    model_tf = vnect_model.VNect(args.input_size)\n\n    # Create session\n    sess_config = tf.ConfigProto(device_count=gpu_count)\n    sess = tf.Session(config=sess_config)\n\n    # Restore weights\n    saver = tf.train.Saver()\n    saver.restore(sess, args.model_file)\n\n    # Joints placeholder\n    joints_2d = np.zeros(shape=(args.num_of_joints, 2), dtype=np.int32)\n    joints_3d = np.zeros(shape=(args.num_of_joints, 3), dtype=np.float32)\n\n    cam = cv2.VideoCapture(0)\n\n    while True:\n        t1 = time.time()\n        input_batch = []\n\n        cam_img = utils.read_square_image('', cam, args.input_size, 'WEBCAM')\n        orig_size_input = cam_img.astype(np.float32)\n\n        # Create multi-scale inputs\n        for scale in scales:\n            resized_img = utils.resize_pad_img(orig_size_input, scale, args.input_size)\n            input_batch.append(resized_img)\n\n        input_batch = np.asarray(input_batch, dtype=np.float32)\n        input_batch /= 255.0\n        input_batch -= 0.4\n\n        # Inference\n        [hm, x_hm, y_hm, z_hm] = sess.run(\n            [model_tf.heapmap, model_tf.x_heatmap, model_tf.y_heatmap, model_tf.z_heatmap],\n            feed_dict={model_tf.input_holder: input_batch})\n\n        # Average scale outputs\n        hm_size = args.input_size // args.pool_scale\n        hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n        x_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n        y_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n        z_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n        for i in range(len(scales)):\n            rescale = 1.0 / scales[i]\n            scaled_hm = cv2.resize(hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n            scaled_x_hm = cv2.resize(x_hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n            scaled_y_hm = cv2.resize(y_hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n            scaled_z_hm = cv2.resize(z_hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n            mid = [scaled_hm.shape[0] // 2, scaled_hm.shape[1] // 2]\n            hm_avg += scaled_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                      mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n            x_hm_avg += scaled_x_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                        mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n            y_hm_avg += scaled_y_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                        mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n            z_hm_avg += scaled_z_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                        mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n        hm_avg /= len(scales)\n        x_hm_avg /= len(scales)\n        y_hm_avg /= len(scales)\n        z_hm_avg /= len(scales)\n\n        # Get 2d joints\n        utils.extract_2d_joint_from_heatmap(hm_avg, args.input_size, joints_2d)\n\n        # Get 3d joints\n        utils.extract_3d_joints_from_heatmap(joints_2d, x_hm_avg, y_hm_avg, z_hm_avg, args.input_size, joints_3d)\n\n        if args.plot_2d:\n            # Plot 2d joint location\n            joint_map = np.zeros(shape=(args.input_size, args.input_size, 3))\n            for joint_num in range(joints_2d.shape[0]):\n                cv2.circle(joint_map, center=(joints_2d[joint_num][1], joints_2d[joint_num][0]), radius=3,\n                           color=(255, 0, 0), thickness=-1)\n            # Draw 2d limbs\n            utils.draw_limbs_2d(cam_img, joints_2d, limb_parents)\n\n        if args.plot_3d:\n            ax.clear()\n            ax.view_init(azim=0, elev=90)\n            ax.set_xlim(-50, 50)\n            ax.set_ylim(-50, 50)\n            ax.set_zlim(-50, 50)\n            ax.set_xlabel('x')\n            ax.set_ylabel('y')\n            ax.set_zlabel('z')\n            utils.draw_limbs_3d(joints_3d, limb_parents, ax)\n\n            if args.plot_2d:\n                # Display 2d results\n                concat_img = np.concatenate((cam_img[:, :, ::-1], joint_map), axis=1)\n                ax2.imshow(concat_img.astype(np.uint8))\n            plt.pause(0.00001)\n            plt.show(block=False)\n\n        elif args.plot_2d:\n            concat_img = np.concatenate((cam_img, joint_map), axis=1)\n            cv2.imshow('2D img', concat_img.astype(np.uint8))\n            if cv2.waitKey(1) == ord('q'): break\n\n        print('FPS: {:>2.2f}'.format(1 / (time.time() - t1)))\n\n\n\nif __name__ == '__main__':\n\n    if args.demo_type == 'image':\n        demo_single_image()\n    elif args.demo_type == 'webcam':\n        demo_webcam()\n"""
demo_tf_gl.py,10,"b""import argparse\nimport time\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport pygame\nfrom pygame.locals import *\nfrom OpenGL.GL import *\nfrom OpenGL.GLU import *\n\nfrom models.nets import vnect_model_bn_folded as vnect_model\nimport utils.utils as utils\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--device', default='gpu')\nparser.add_argument('--demo_type', default='image')\nparser.add_argument('--model_file', default='models/weights/vnect_tf')\nparser.add_argument('--test_img', default='test_imgs/yuniko.jpg')\nparser.add_argument('--input_size', default=368)\nparser.add_argument('--num_of_joints', default=21)\nparser.add_argument('--pool_scale', default=8)\nparser.add_argument('--plot_2d', default=True)\nparser.add_argument('--plot_3d', default=True)\nargs = parser.parse_args()\n\njoint_color_code = [[139, 53, 255],\n                    [0, 56, 255],\n                    [43, 140, 237],\n                    [37, 168, 36],\n                    [147, 147, 0],\n                    [70, 17, 145]]\n\n# Limb parents of each joint\nlimb_parents = [1, 15, 1, 2, 3, 1, 5, 6, 14, 8, 9, 14, 11, 12, 14, 14, 1, 4, 7, 10, 13]\n\n# input scales\nscales = [1.0, 0.7]\n\n# Use gpu or cpu\ngpu_count = {'GPU':1} if args.device == 'gpu' else {'GPU':0}\n\n\ndef demo_single_image():\n    # Create model\n    model_tf = vnect_model.VNect(args.input_size)\n\n    # Create session\n    sess_config = tf.ConfigProto(device_count=gpu_count)\n    sess = tf.Session(config=sess_config)\n\n    # Restore weights\n    saver = tf.train.Saver()\n    saver.restore(sess, args.model_file)\n\n    # Joints placeholder\n    joints_2d = np.zeros(shape=(args.num_of_joints, 2), dtype=np.int32)\n    joints_3d = np.zeros(shape=(args.num_of_joints, 3), dtype=np.float32)\n\n\n    img_path = args.test_img\n    t1 = time.time()\n    input_batch = []\n\n    cam_img = utils.read_square_image(img_path, '', args.input_size, 'IMAGE')\n    orig_size_input = cam_img.astype(np.float32)\n\n    # Create multi-scale inputs\n    for scale in scales:\n        resized_img = utils.resize_pad_img(orig_size_input, scale, args.input_size)\n        input_batch.append(resized_img)\n\n    input_batch = np.asarray(input_batch, dtype=np.float32)\n    input_batch /= 255.0\n    input_batch -= 0.4\n\n    # Inference\n    [hm, x_hm, y_hm, z_hm] = sess.run(\n        [model_tf.heapmap, model_tf.x_heatmap, model_tf.y_heatmap, model_tf.z_heatmap],\n        feed_dict={model_tf.input_holder: input_batch})\n\n    # Average scale outputs\n    hm_size = args.input_size // args.pool_scale\n    hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n    x_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n    y_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n    z_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n    for i in range(len(scales)):\n        rescale = 1.0 / scales[i]\n        scaled_hm = cv2.resize(hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n        scaled_x_hm = cv2.resize(x_hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n        scaled_y_hm = cv2.resize(y_hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n        scaled_z_hm = cv2.resize(z_hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n        mid = [scaled_hm.shape[0] // 2, scaled_hm.shape[1] // 2]\n        hm_avg += scaled_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                  mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n        x_hm_avg += scaled_x_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                    mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n        y_hm_avg += scaled_y_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                    mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n        z_hm_avg += scaled_z_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                    mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n    hm_avg /= len(scales)\n    x_hm_avg /= len(scales)\n    y_hm_avg /= len(scales)\n    z_hm_avg /= len(scales)\n\n    # Get 2d joints\n    utils.extract_2d_joint_from_heatmap(hm_avg, args.input_size, joints_2d)\n\n    # Get 3d joints\n    utils.extract_3d_joints_from_heatmap(joints_2d, x_hm_avg, y_hm_avg, z_hm_avg, args.input_size, joints_3d)\n\n    if args.plot_2d:\n        # Plot 2d joint location\n        joint_map = np.zeros(shape=(args.input_size, args.input_size, 3))\n        for joint_num in range(joints_2d.shape[0]):\n            cv2.circle(joint_map, center=(joints_2d[joint_num][1], joints_2d[joint_num][0]), radius=3,\n                       color=(255, 0, 0), thickness=-1)\n        # Draw 2d limbs\n        utils.draw_limbs_2d(cam_img, joints_2d, limb_parents)\n\n\n    print('FPS: {:>2.2f}'.format(1 / (time.time() - t1)))\n\n    if args.plot_3d:\n        # Draw 3d limbs\n        glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)\n        utils.draw_limbs_3d_gl(joints_3d, limb_parents)\n        pygame.display.flip()\n        pygame.time.wait(1)\n\n    if args.plot_2d:\n        # Display 2d results\n        concat_img = np.concatenate((cam_img, joint_map), axis=1)\n        cv2.imshow('2D', concat_img.astype(np.uint8))\n        cv2.waitKey(0)\n\n\n\n\ndef demo_webcam():\n    # Create model\n    model_tf = vnect_model.VNect(args.input_size)\n\n    # Create session\n    sess_config = tf.ConfigProto(device_count=gpu_count)\n    sess = tf.Session(config=sess_config)\n\n    # Restore weights\n    saver = tf.train.Saver()\n    saver.restore(sess, args.model_file)\n\n    # Joints placeholder\n    joints_2d = np.zeros(shape=(args.num_of_joints, 2), dtype=np.int32)\n    joints_3d = np.zeros(shape=(args.num_of_joints, 3), dtype=np.float32)\n\n    cam = cv2.VideoCapture(0)\n\n    while True:\n        t1 = time.time()\n        input_batch = []\n\n        cam_img = utils.read_square_image('', cam, args.input_size, 'WEBCAM')\n        orig_size_input = cam_img.astype(np.float32)\n\n        # Create multi-scale inputs\n        for scale in scales:\n            resized_img = utils.resize_pad_img(orig_size_input, scale, args.input_size)\n            input_batch.append(resized_img)\n\n        input_batch = np.asarray(input_batch, dtype=np.float32)\n        input_batch /= 255.0\n        input_batch -= 0.4\n\n        # Inference\n        [hm, x_hm, y_hm, z_hm] = sess.run(\n            [model_tf.heapmap, model_tf.x_heatmap, model_tf.y_heatmap, model_tf.z_heatmap],\n            feed_dict={model_tf.input_holder: input_batch})\n\n        # Average scale outputs\n        hm_size = args.input_size // args.pool_scale\n        hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n        x_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n        y_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n        z_hm_avg = np.zeros(shape=(hm_size, hm_size, args.num_of_joints))\n        for i in range(len(scales)):\n            rescale = 1.0 / scales[i]\n            scaled_hm = cv2.resize(hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n            scaled_x_hm = cv2.resize(x_hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n            scaled_y_hm = cv2.resize(y_hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n            scaled_z_hm = cv2.resize(z_hm[i, :, :, :], (0, 0), fx=rescale, fy=rescale, interpolation=cv2.INTER_LINEAR)\n            mid = [scaled_hm.shape[0] // 2, scaled_hm.shape[1] // 2]\n            hm_avg += scaled_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                      mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n            x_hm_avg += scaled_x_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                        mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n            y_hm_avg += scaled_y_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                        mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n            z_hm_avg += scaled_z_hm[mid[0] - hm_size // 2: mid[0] + hm_size // 2,\n                        mid[1] - hm_size // 2: mid[1] + hm_size // 2, :]\n        hm_avg /= len(scales)\n        x_hm_avg /= len(scales)\n        y_hm_avg /= len(scales)\n        z_hm_avg /= len(scales)\n\n        # Get 2d joints\n        utils.extract_2d_joint_from_heatmap(hm_avg, args.input_size, joints_2d)\n\n        # Get 3d joints\n        utils.extract_3d_joints_from_heatmap(joints_2d, x_hm_avg, y_hm_avg, z_hm_avg, args.input_size, joints_3d)\n\n        if args.plot_2d:\n            # Plot 2d joint location\n            joint_map = np.zeros(shape=(args.input_size, args.input_size, 3))\n            for joint_num in range(joints_2d.shape[0]):\n                cv2.circle(joint_map, center=(joints_2d[joint_num][1], joints_2d[joint_num][0]), radius=3,\n                           color=(255, 0, 0), thickness=-1)\n            # Draw 2d limbs\n            utils.draw_limbs_2d(cam_img, joints_2d, limb_parents)\n\n        print('FPS: {:>2.2f}'.format(1 / (time.time() - t1)))\n\n        if args.plot_3d:\n            # Draw 3d limbs\n            glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)\n            utils.draw_limbs_3d_gl(joints_3d, limb_parents)\n            pygame.display.flip()\n            pygame.time.wait(1)\n\n        if args.plot_2d:\n            # Display 2d results\n            concat_img = np.concatenate((cam_img, joint_map), axis=1)\n            cv2.imshow('2D', concat_img.astype(np.uint8))\n            if cv2.waitKey(1) == ord('q'): break\n\n\n\nif __name__ == '__main__':\n    # GL initiation\n    pygame.init()\n    display = (800, 600)\n\n    glMatrixMode(GL_PROJECTION)\n    glLoadIdentity()\n    pygame.display.set_mode(display, DOUBLEBUF | OPENGL)\n    gluPerspective(70, (display[0] / display[1]), 0.1, 800.0)\n    glMatrixMode(GL_MODELVIEW)\n    gluLookAt(-.0, 0.0, -200.0,\n              5.0, 0.0, 0.0,\n              -5.0, -1.0, -10.0)\n\n    if args.demo_type == 'image':\n        demo_single_image()\n    elif args.demo_type == 'webcam':\n        demo_webcam()\n"""
plotly_test.py,0,"b""import sys\nimport time\nfrom pyqtgraph.Qt import QtCore, QtGui\nimport numpy as np\nimport pyqtgraph as pg\n\n\nclass App(QtGui.QMainWindow):\n    def __init__(self, parent=None):\n        super(App, self).__init__(parent)\n\n        #### Create Gui Elements ###########\n        self.mainbox = QtGui.QWidget()\n        self.setCentralWidget(self.mainbox)\n        self.mainbox.setLayout(QtGui.QVBoxLayout())\n\n        self.canvas = pg.GraphicsLayoutWidget()\n        self.mainbox.layout().addWidget(self.canvas)\n\n        self.label = QtGui.QLabel()\n        self.mainbox.layout().addWidget(self.label)\n\n        self.view = self.canvas.addViewBox()\n        self.view.setAspectLocked(True)\n        self.view.setRange(QtCore.QRectF(0,0, 100, 100))\n\n        #  image plot\n        self.img = pg.ImageItem(border='w')\n        self.view.addItem(self.img)\n\n        self.canvas.nextRow()\n        #  line plot\n        self.otherplot = self.canvas.addPlot()\n        self.h2 = self.otherplot.plot(pen='y')\n\n\n        #### Set Data  #####################\n\n        self.x = np.linspace(0,50., num=100)\n        self.X,self.Y = np.meshgrid(self.x,self.x)\n\n        self.counter = 0\n        self.fps = 0.\n        self.lastupdate = time.time()\n\n        #### Start  #####################\n        self._update()\n\n    def _update(self):\n\n        self.data = np.sin(self.X/3.+self.counter/9.)*np.cos(self.Y/3.+self.counter/9.)\n        self.ydata = np.sin(self.x/3.+ self.counter/9.)\n\n        self.img.setImage(self.data)\n        self.h2.setData(self.ydata)\n\n        now = time.time()\n        dt = (now-self.lastupdate)\n        if dt <= 0:\n            dt = 0.000000000001\n        fps2 = 1.0 / dt\n        self.lastupdate = now\n        self.fps = self.fps * 0.9 + fps2 * 0.1\n        tx = 'Mean Frame Rate:  {fps:.3f} FPS'.format(fps=self.fps )\n        self.label.setText(tx)\n        QtCore.QTimer.singleShot(1, self._update)\n        self.counter += 1\n\n\nif __name__ == '__main__':\n\n    app = QtGui.QApplication(sys.argv)\n    thisapp = App()\n    thisapp.show()\n    sys.exit(app.exec_())"""
pyqt_test.py,0,"b""import pygame\nfrom pygame.locals import *\n\nfrom OpenGL.GL import *\nfrom OpenGL.GLU import *\nimport time\n\nverticies = (\n    (1, -1, -1),\n    (1, 1, -1),\n    (-1, 1, -1),\n    (-1, -1, -1),\n    (1, -1, 1),\n    (1, 1, 1),\n    (-1, -1, 1),\n    (-1, 1, 1)\n    )\n\nedges = (\n    (0,2),\n    (0,3),\n    (0,4),\n    (2,1),\n    (2,3),\n    (2,7),\n    (6,3),\n    (6,4),\n    (6,7),\n    (5,1),\n    (5,4),\n    (5,7)\n    )\n\n\ndef Cube():\n    glBegin(GL_LINES)\n    for edge in edges:\n        for vertex in edge:\n            glVertex3fv(verticies[vertex])\n    glEnd()\n\n\ndef main():\n    pygame.init()\n    display = (800,600)\n    pygame.display.set_mode(display, DOUBLEBUF|OPENGL)\n\n    gluPerspective(45, (display[0]/display[1]), 0.1, 500.0)\n\n    glTranslatef(0.0,0.0, -5)\n\n    while True:\n        t1 = time.time()\n        for event in pygame.event.get():\n            if event.type == pygame.QUIT:\n                pygame.quit()\n                quit()\n\n        glRotatef(1, 3, 1, 1)\n        glClear(GL_COLOR_BUFFER_BIT|GL_DEPTH_BUFFER_BIT)\n        Cube()\n        pygame.display.flip()\n        print('FPS', 1/(time.time()-t1))\n\n        pygame.time.wait(1)\n\nmain()"""
utils.py,0,"b""import cv2\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom OpenGL.GL import *\nfrom OpenGL.GLU import *\n\n\n\ndef read_square_image(file, cam, boxsize, type):\n    # from file\n    if type == 'IMAGE':\n        oriImg = cv2.imread(file)\n    # from webcam\n    elif type == 'WEBCAM':\n        _, oriImg = cam.read()\n\n    scale = boxsize / (oriImg.shape[0] * 1.0)\n    imageToTest = cv2.resize(oriImg, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_LANCZOS4)\n\n    output_img = np.ones((boxsize, boxsize, 3)) * 128\n\n    if imageToTest.shape[1] < boxsize:\n        offset = imageToTest.shape[1] % 2\n        output_img[:, int(boxsize/2-math.ceil(imageToTest.shape[1]/2)):int(boxsize/2+math.ceil(imageToTest.shape[1]/2)+offset), :] = imageToTest\n    else:\n        output_img = imageToTest[:, int(imageToTest.shape[1]/2-boxsize/2):int(imageToTest.shape[1]/2+boxsize/2), :]\n    return output_img\n\ndef resize_pad_img(img, scale, output_size):\n    resized_img = cv2.resize(img, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n    pad_h = (output_size - resized_img.shape[0]) // 2\n    pad_w = (output_size - resized_img.shape[1]) // 2\n    pad_h_offset = (output_size - resized_img.shape[0]) % 2\n    pad_w_offset = (output_size - resized_img.shape[1]) % 2\n    resized_pad_img = np.pad(resized_img, ((pad_w, pad_w+pad_w_offset), (pad_h, pad_h+pad_h_offset), (0, 0)),\n                             mode='constant', constant_values=128)\n\n    return resized_pad_img\n\n\ndef draw_predicted_heatmap(heatmap, input_size):\n    heatmap_resized = cv2.resize(heatmap, (input_size, input_size))\n\n    output_img = None\n    tmp_concat_img = None\n    h_count = 0\n    for joint_num in range(heatmap_resized.shape[2]):\n        if h_count < 4:\n            tmp_concat_img = np.concatenate((tmp_concat_img, heatmap_resized[:, :, joint_num]), axis=1) \\\n                if tmp_concat_img is not None else heatmap_resized[:, :, joint_num]\n            h_count += 1\n        else:\n            output_img = np.concatenate((output_img, tmp_concat_img), axis=0) if output_img is not None else tmp_concat_img\n            tmp_concat_img = None\n            h_count = 0\n    # last row img\n    if h_count != 0:\n        while h_count < 4:\n            tmp_concat_img = np.concatenate((tmp_concat_img, np.zeros(shape=(input_size, input_size), dtype=np.float32)), axis=1)\n            h_count += 1\n        output_img = np.concatenate((output_img, tmp_concat_img), axis=0)\n\n    # adjust heatmap color\n    output_img = output_img.astype(np.uint8)\n    output_img = cv2.applyColorMap(output_img, cv2.COLORMAP_JET)\n    return output_img\n\ndef extract_2d_joint_from_heatmap(heatmap, input_size, joints_2d):\n    heatmap_resized = cv2.resize(heatmap, (input_size, input_size))\n\n    for joint_num in range(heatmap_resized.shape[2]):\n        joint_coord = np.unravel_index(np.argmax(heatmap_resized[:, :, joint_num]), (input_size, input_size))\n        joints_2d[joint_num, :] = joint_coord\n\n    return joints_2d\n\n\ndef extract_3d_joints_from_heatmap(joints_2d, x_hm, y_hm, z_hm, input_size, joints_3d):\n\n    for joint_num in range(x_hm.shape[2]):\n        coord_2d_x = joints_2d[joint_num][0]\n        coord_2d_y = joints_2d[joint_num][1]\n\n        # x_hm_resized = cv2.resize(x_hm, (input_size, input_size))\n        # y_hm_resized = cv2.resize(y_hm, (input_size, input_size))\n        # z_hm_resized = cv2.resize(z_hm, (input_size, input_size))\n        # joint_x = x_hm_resized[max(int(coord_2d_x), 1), max(int(coord_2d_y), 1), joint_num] * 100\n        # joint_y = y_hm_resized[max(int(coord_2d_x), 1), max(int(coord_2d_y), 1), joint_num] * 100\n        # joint_z = z_hm_resized[max(int(coord_2d_x), 1), max(int(coord_2d_y), 1), joint_num] * 100\n\n\n        joint_x = x_hm[max(int(coord_2d_x/8), 1), max(int(coord_2d_y/8), 1), joint_num] * 10\n        joint_y = y_hm[max(int(coord_2d_x/8), 1), max(int(coord_2d_y/8), 1), joint_num] * 10\n        joint_z = z_hm[max(int(coord_2d_x/8), 1), max(int(coord_2d_y/8), 1), joint_num] * 10\n        joints_3d[joint_num, 0] = joint_x\n        joints_3d[joint_num, 1] = joint_y\n        joints_3d[joint_num, 2] = joint_z\n    joints_3d -= joints_3d[14, :]\n\n    return joints_3d\n\ndef draw_limbs_2d(img, joints_2d, limb_parents):\n    for limb_num in range(len(limb_parents)-1):\n        x1 = joints_2d[limb_num, 0]\n        y1 = joints_2d[limb_num, 1]\n        x2 = joints_2d[limb_parents[limb_num], 0]\n        y2 = joints_2d[limb_parents[limb_num], 1]\n        length = ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5\n        # if length < 10000 and length > 5:\n        deg = math.degrees(math.atan2(x1 - x2, y1 - y2))\n        polygon = cv2.ellipse2Poly((int((y1 + y2) / 2), int((x1 + x2) / 2)),\n                                   (int(length / 2), 3),\n                                   int(deg),\n                                   0, 360, 1)\n        cv2.fillConvexPoly(img, polygon, color=(0,255,0))\n    return img\n\ndef draw_limbs_3d(joints_3d, limb_parents, ax):\n\n    for i in range(joints_3d.shape[0]):\n        x_pair = [joints_3d[i, 0], joints_3d[limb_parents[i], 0]]\n        y_pair = [joints_3d[i, 1], joints_3d[limb_parents[i], 1]]\n        z_pair = [joints_3d[i, 2], joints_3d[limb_parents[i], 2]]\n        ax.plot(x_pair, y_pair, zs=z_pair, linewidth=3)\n\n\ndef draw_limb_3d_gl(joints_3d, limb_parents):\n    glBegin(GL_LINES)\n    for i in range(joints_3d.shape[0]):\n        print(joints_3d[i, :])\n        glVertex3fv((joints_3d[i, 0], joints_3d[i, 1], joints_3d[i, 2]))\n        glVertex3fv((joints_3d[limb_parents[i], 0], joints_3d[limb_parents[i], 1], joints_3d[limb_parents[i], 2]))\n    glEnd()\n\n\n\n\n\n\n\n\n\n\n\n\n"""
vispy_test.py,0,"b'# -*- coding: utf-8 -*-\n# -----------------------------------------------------------------------------\n# Copyright (c) 2015, Vispy Development Team. All Rights Reserved.\n# Distributed under the (new) BSD License. See LICENSE.txt for more info.\n# -----------------------------------------------------------------------------\n# Author: Nicolas P .Rougier\n# Date:   04/03/2014\n# -----------------------------------------------------------------------------\nimport math\nimport numpy as np\n\nfrom vispy import app\nfrom vispy.gloo import gl\n\n\ndef checkerboard(grid_num=8, grid_size=32):\n    row_even = grid_num // 2 * [0, 1]\n    row_odd = grid_num // 2 * [1, 0]\n    Z = np.row_stack(grid_num // 2 * (row_even, row_odd)).astype(np.uint8)\n    return 255 * Z.repeat(grid_size, axis=0).repeat(grid_size, axis=1)\n\n\ndef rotate(M, angle, x, y, z, point=None):\n    angle = math.pi * angle / 180\n    c, s = math.cos(angle), math.sin(angle)\n    n = math.sqrt(x * x + y * y + z * z)\n    x /= n\n    y /= n\n    z /= n\n    cx, cy, cz = (1 - c) * x, (1 - c) * y, (1 - c) * z\n    R = np.array([[cx * x + c, cy * x - z * s, cz * x + y * s, 0],\n                  [cx * y + z * s, cy * y + c, cz * y - x * s, 0],\n                  [cx * z - y * s, cy * z + x * s, cz * z + c, 0],\n                  [0, 0, 0, 1]], dtype=M.dtype).T\n    M[...] = np.dot(M, R)\n    return M\n\n\ndef translate(M, x, y=None, z=None):\n    y = x if y is None else y\n    z = x if z is None else z\n    T = np.array([[1.0, 0.0, 0.0, x],\n                  [0.0, 1.0, 0.0, y],\n                  [0.0, 0.0, 1.0, z],\n                  [0.0, 0.0, 0.0, 1.0]], dtype=M.dtype).T\n    M[...] = np.dot(M, T)\n    return M\n\n\ndef frustum(left, right, bottom, top, znear, zfar):\n    M = np.zeros((4, 4), dtype=np.float32)\n    M[0, 0] = +2.0 * znear / (right - left)\n    M[2, 0] = (right + left) / (right - left)\n    M[1, 1] = +2.0 * znear / (top - bottom)\n    M[3, 1] = (top + bottom) / (top - bottom)\n    M[2, 2] = -(zfar + znear) / (zfar - znear)\n    M[3, 2] = -2.0 * znear * zfar / (zfar - znear)\n    M[2, 3] = -1.0\n    return M\n\n\ndef perspective(fovy, aspect, znear, zfar):\n    h = math.tan(fovy / 360.0 * math.pi) * znear\n    w = h * aspect\n    return frustum(-w, w, -h, h, znear, zfar)\n\n\ndef makecube():\n    """""" Generate vertices & indices for a filled cube """"""\n\n    vtype = [(\'a_position\', np.float32, 3),\n             (\'a_texcoord\', np.float32, 2)]\n    itype = np.uint32\n\n    # Vertices positions\n    p = np.array([[1, 1, 1], [-1, 1, 1], [-1, -1, 1], [1, -1, 1],\n                  [1, -1, -1], [1, 1, -1], [-1, 1, -1], [-1, -1, -1]])\n\n    # Texture coords\n    t = np.array([[0, 0], [0, 1], [1, 1], [1, 0]])\n\n    faces_p = [0, 1, 2, 3, 0, 3, 4, 5, 0, 5, 6,\n               1, 1, 6, 7, 2, 7, 4, 3, 2, 4, 7, 6, 5]\n    faces_t = [0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2,\n               3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3]\n\n    vertices = np.zeros(24, vtype)\n    vertices[\'a_position\'] = p[faces_p]\n    vertices[\'a_texcoord\'] = t[faces_t]\n\n    indices = np.resize(\n        np.array([0, 1, 2, 0, 2, 3], dtype=itype), 6 * (2 * 3))\n    indices += np.repeat(4 * np.arange(6), 6).astype(np.uint32)\n\n    return vertices, indices\n\n\ncube_vertex = """"""\nuniform mat4 u_model;\nuniform mat4 u_view;\nuniform mat4 u_projection;\nattribute vec3 a_position;\nattribute vec2 a_texcoord;\nvarying vec2 v_texcoord;\nvoid main()\n{\n    gl_Position = u_projection * u_view * u_model * vec4(a_position,1.0);\n    v_texcoord = a_texcoord;\n}\n""""""\n\ncube_fragment = """"""\nuniform sampler2D u_texture;\nvarying vec2 v_texcoord;\nvoid main()\n{\n    gl_FragColor = texture2D(u_texture, v_texcoord);\n}\n""""""\n\n\nclass Canvas(app.Canvas):\n    def __init__(self):\n        app.Canvas.__init__(self, size=(512, 512),\n                            title=\'Rotating cube (GL version)\',\n                            keys=\'interactive\')\n\n    def on_initialize(self, event):\n        # Build & activate cube program\n        self.cube = gl.glCreateProgram()\n        vertex = gl.glCreateShader(gl.GL_VERTEX_SHADER)\n        fragment = gl.glCreateShader(gl.GL_FRAGMENT_SHADER)\n        gl.glShaderSource(vertex, cube_vertex)\n        gl.glShaderSource(fragment, cube_fragment)\n        gl.glCompileShader(vertex)\n        gl.glCompileShader(fragment)\n        gl.glAttachShader(self.cube, vertex)\n        gl.glAttachShader(self.cube, fragment)\n        gl.glLinkProgram(self.cube)\n        gl.glDetachShader(self.cube, vertex)\n        gl.glDetachShader(self.cube, fragment)\n        gl.glUseProgram(self.cube)\n\n        # Get data & build cube buffers\n        vcube_data, self.icube_data = makecube()\n        vcube = gl.glCreateBuffer()\n        gl.glBindBuffer(gl.GL_ARRAY_BUFFER, vcube)\n        gl.glBufferData(gl.GL_ARRAY_BUFFER, vcube_data, gl.GL_STATIC_DRAW)\n        icube = gl.glCreateBuffer()\n        gl.glBindBuffer(gl.GL_ELEMENT_ARRAY_BUFFER, icube)\n        gl.glBufferData(gl.GL_ELEMENT_ARRAY_BUFFER,\n                        self.icube_data, gl.GL_STATIC_DRAW)\n\n        # Bind cube attributes\n        stride = vcube_data.strides[0]\n        offset = 0\n        loc = gl.glGetAttribLocation(self.cube, ""a_position"")\n        gl.glEnableVertexAttribArray(loc)\n        gl.glVertexAttribPointer(loc, 3, gl.GL_FLOAT, False, stride, offset)\n\n        offset = vcube_data.dtype[""a_position""].itemsize\n        loc = gl.glGetAttribLocation(self.cube, ""a_texcoord"")\n        gl.glEnableVertexAttribArray(loc)\n        gl.glVertexAttribPointer(loc, 2, gl.GL_FLOAT, False, stride, offset)\n\n        # Create & bind cube texture\n        crate = checkerboard()\n        texture = gl.glCreateTexture()\n        gl.glTexParameterf(gl.GL_TEXTURE_2D, gl.GL_TEXTURE_MIN_FILTER,\n                           gl.GL_LINEAR)\n        gl.glTexParameterf(gl.GL_TEXTURE_2D, gl.GL_TEXTURE_MAG_FILTER,\n                           gl.GL_LINEAR)\n        gl.glTexParameterf(gl.GL_TEXTURE_2D, gl.GL_TEXTURE_WRAP_S,\n                           gl.GL_CLAMP_TO_EDGE)\n        gl.glTexParameterf(gl.GL_TEXTURE_2D, gl.GL_TEXTURE_WRAP_T,\n                           gl.GL_CLAMP_TO_EDGE)\n        gl.glTexImage2D(gl.GL_TEXTURE_2D, 0, gl.GL_LUMINANCE, gl.GL_LUMINANCE,\n                        gl.GL_UNSIGNED_BYTE, crate.shape[:2])\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, 0, 0, gl.GL_LUMINANCE,\n                           gl.GL_UNSIGNED_BYTE, crate)\n        loc = gl.glGetUniformLocation(self.cube, ""u_texture"")\n        gl.glUniform1i(loc, texture)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, 0)\n\n        # Create & bind cube matrices\n        view = np.eye(4, dtype=np.float32)\n        model = np.eye(4, dtype=np.float32)\n        projection = np.eye(4, dtype=np.float32)\n        translate(view, 0, 0, -7)\n        self.phi, self.theta = 60, 20\n        rotate(model, self.theta, 0, 0, 1)\n        rotate(model, self.phi, 0, 1, 0)\n        loc = gl.glGetUniformLocation(self.cube, ""u_model"")\n        gl.glUniformMatrix4fv(loc, 1, False, model)\n        loc = gl.glGetUniformLocation(self.cube, ""u_view"")\n        gl.glUniformMatrix4fv(loc, 1, False, view)\n        loc = gl.glGetUniformLocation(self.cube, ""u_projection"")\n        gl.glUniformMatrix4fv(loc, 1, False, projection)\n\n        # OpenGL initalization\n        gl.glClearColor(0.30, 0.30, 0.35, 1.00)\n        gl.glEnable(gl.GL_DEPTH_TEST)\n        self._resize(*(self.size + self.physical_size))\n        self.timer = app.Timer(\'auto\', self.on_timer, start=True)\n\n    def on_draw(self, event):\n        gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)\n        gl.glDrawElements(gl.GL_TRIANGLES, self.icube_data.size,\n                          gl.GL_UNSIGNED_INT, None)\n\n    def on_resize(self, event):\n        self._resize(*(event.size + event.physical_size))\n\n    def _resize(self, width, height, physical_width, physical_height):\n        gl.glViewport(0, 0, physical_width, physical_height)\n        projection = perspective(35.0, width / float(height), 2.0, 10.0)\n        loc = gl.glGetUniformLocation(self.cube, ""u_projection"")\n        gl.glUniformMatrix4fv(loc, 1, False, projection)\n\n    def on_timer(self, event):\n        self.theta += .5\n        self.phi += .5\n        model = np.eye(4, dtype=np.float32)\n        rotate(model, self.theta, 0, 0, 1)\n        rotate(model, self.phi, 0, 1, 0)\n        loc = gl.glGetUniformLocation(self.cube, ""u_model"")\n        gl.glUniformMatrix4fv(loc, 1, False, model)\n        self.update()\n\nif __name__ == \'__main__\':\n    c = Canvas()\n    c.show()\n    app.run()'"
models/__init__.py,0,b'\n'
models/vnect_model.py,51,"b""import tensorflow as tf\nimport tensorflow.contrib as tc\n\nimport pickle\nimport numpy as np\n\n\nclass VNect():\n    def __init__(self, input_size):\n        self.is_training = False\n        self.input_holder = tf.placeholder(dtype=tf.float32,\n                                           shape=(None, input_size, input_size, 3))\n        self._create_network()\n\n    def _create_network(self):\n        # Conv\n        self.conv1 = tc.layers.conv2d(self.input_holder, kernel_size=7, num_outputs=64, stride=2, scope='conv1')\n        self.pool1 = tc.layers.max_pool2d(self.conv1, kernel_size=3, padding='same', scope='pool1')\n\n        # Residual block 2a\n        self.res2a_branch2a = tc.layers.conv2d(self.pool1, kernel_size=1, num_outputs=64, scope='res2a_branch2a')\n        self.res2a_branch2b = tc.layers.conv2d(self.res2a_branch2a, kernel_size=3, num_outputs=64, scope='res2a_branch2b')\n        self.res2a_branch2c = tc.layers.conv2d(self.res2a_branch2b, kernel_size=1, num_outputs=256, activation_fn=None, scope='res2a_branch2c')\n        self.res2a_branch1 = tc.layers.conv2d(self.pool1, kernel_size=1, num_outputs=256, activation_fn=None, scope='res2a_branch1')\n        self.res2a = tf.add(self.res2a_branch2c, self.res2a_branch1, name='res2a_add')\n        self.res2a = tf.nn.relu(self.res2a, name='res2a')\n\n        # Residual block 2b\n        self.res2b_branch2a = tc.layers.conv2d(self.res2a, kernel_size=1, num_outputs=64, scope='res2b_branch2a')\n        self.res2b_branch2b = tc.layers.conv2d(self.res2b_branch2a, kernel_size=3, num_outputs=64, scope='res2b_branch2b')\n        self.res2b_branch2c = tc.layers.conv2d(self.res2b_branch2b, kernel_size=1, num_outputs=256, activation_fn=None, scope='res2b_branch2c')\n        self.res2b = tf.add(self.res2b_branch2c, self.res2a, name='res2b_add')\n        self.res2b = tf.nn.relu(self.res2b, name='res2b')\n\n        # Residual block 2c\n        self.res2c_branch2a = tc.layers.conv2d(self.res2b, kernel_size=1, num_outputs=64, scope='res2c_branch2a')\n        self.res2c_branch2b = tc.layers.conv2d(self.res2c_branch2a, kernel_size=3, num_outputs=64, scope='res2c_branch2b')\n        self.res2c_branch2c = tc.layers.conv2d(self.res2c_branch2b, kernel_size=1, num_outputs=256, activation_fn=None, scope='res2c_branch2c')\n        self.res2c = tf.add(self.res2c_branch2c, self.res2b, name='res2c_add')\n        self.res2c = tf.nn.relu(self.res2c, name='res2c')\n\n        # Residual block 3a\n        self.res3a_branch2a = tc.layers.conv2d(self.res2c, kernel_size=1, num_outputs=128, stride=2, scope='res3a_branch2a')\n        self.res3a_branch2b = tc.layers.conv2d(self.res3a_branch2a, kernel_size=3, num_outputs=128, scope='res3a_branch2b')\n        self.res3a_branch2c = tc.layers.conv2d(self.res3a_branch2b, kernel_size=1, num_outputs=512, activation_fn=None,scope='res3a_branch2c')\n        self.res3a_branch1 = tc.layers.conv2d(self.res2c, kernel_size=1, num_outputs=512, activation_fn=None, stride=2, scope='res3a_branch1')\n        self.res3a = tf.add(self.res3a_branch2c, self.res3a_branch1, name='res3a_add')\n        self.res3a = tf.nn.relu(self.res3a, name='res3a')\n\n        # Residual block 3b\n        self.res3b_branch2a = tc.layers.conv2d(self.res3a, kernel_size=1, num_outputs=128, scope='res3b_branch2a')\n        self.res3b_branch2b = tc.layers.conv2d(self.res3b_branch2a, kernel_size=3, num_outputs=128,scope='res3b_branch2b')\n        self.res3b_branch2c = tc.layers.conv2d(self.res3b_branch2b, kernel_size=1, num_outputs=512, activation_fn=None,scope='res3b_branch2c')\n        self.res3b = tf.add(self.res3b_branch2c, self.res3a, name='res3b_add')\n        self.res3b = tf.nn.relu(self.res3b, name='res3b')\n\n        # Residual block 3c\n        self.res3c_branch2a = tc.layers.conv2d(self.res3b, kernel_size=1, num_outputs=128, scope='res3c_branch2a')\n        self.res3c_branch2b = tc.layers.conv2d(self.res3c_branch2a, kernel_size=3, num_outputs=128,scope='res3c_branch2b')\n        self.res3c_branch2c = tc.layers.conv2d(self.res3c_branch2b, kernel_size=1, num_outputs=512, activation_fn=None,scope='res3c_branch2c')\n        self.res3c = tf.add(self.res3c_branch2c, self.res3b, name='res3c_add')\n        self.res3c = tf.nn.relu(self.res3c, name='res3c')\n\n        # Residual block 3d\n        self.res3d_branch2a = tc.layers.conv2d(self.res3c, kernel_size=1, num_outputs=128, scope='res3d_branch2a')\n        self.res3d_branch2b = tc.layers.conv2d(self.res3d_branch2a, kernel_size=3, num_outputs=128,scope='res3d_branch2b')\n        self.res3d_branch2c = tc.layers.conv2d(self.res3d_branch2b, kernel_size=1, num_outputs=512, activation_fn=None,scope='res3d_branch2c')\n        self.res3d = tf.add(self.res3d_branch2c, self.res3c, name='res3d_add')\n        self.res3d = tf.nn.relu(self.res3d, name='res3d')\n\n        # Residual block 4a\n        self.res4a_branch2a = tc.layers.conv2d(self.res3d, kernel_size=1, num_outputs=256, stride=2, scope='res4a_branch2a')\n        self.res4a_branch2b = tc.layers.conv2d(self.res4a_branch2a, kernel_size=3, num_outputs=256,scope='res4a_branch2b')\n        self.res4a_branch2c = tc.layers.conv2d(self.res4a_branch2b, kernel_size=1, num_outputs=1024, activation_fn=None,scope='res4a_branch2c')\n        self.res4a_branch1 = tc.layers.conv2d(self.res3d, kernel_size=1, num_outputs=1024, activation_fn=None, stride=2, scope='res4a_branch1')\n        self.res4a = tf.add(self.res4a_branch2c, self.res4a_branch1, name='res4a_add')\n        self.res4a = tf.nn.relu(self.res4a, name='res4a')\n\n        # Residual block 4b\n        self.res4b_branch2a = tc.layers.conv2d(self.res4a, kernel_size=1, num_outputs=256, scope='res4b_branch2a')\n        self.res4b_branch2b = tc.layers.conv2d(self.res4b_branch2a, kernel_size=3, num_outputs=256, scope='res4b_branch2b')\n        self.res4b_branch2c = tc.layers.conv2d(self.res4b_branch2b, kernel_size=1, num_outputs=1024, activation_fn=None, scope='res4b_branch2c')\n        self.res4b = tf.add(self.res4b_branch2c, self.res4a, name='res4b_add')\n        self.res4b = tf.nn.relu(self.res4b, name='res4b')\n\n        # Residual block 4c\n        self.res4c_branch2a = tc.layers.conv2d(self.res4b, kernel_size=1, num_outputs=256, scope='res4c_branch2a')\n        self.res4c_branch2b = tc.layers.conv2d(self.res4c_branch2a, kernel_size=3, num_outputs=256, scope='res4c_branch2b')\n        self.res4c_branch2c = tc.layers.conv2d(self.res4c_branch2b, kernel_size=1, num_outputs=1024, activation_fn=None, scope='res4c_branch2c')\n        self.res4c = tf.add(self.res4c_branch2c, self.res4b, name='res4c_add')\n        self.res4c = tf.nn.relu(self.res4c, name='res4c')\n\n        # Residual block 4d\n        self.res4d_branch2a = tc.layers.conv2d(self.res4c, kernel_size=1, num_outputs=256, scope='res4d_branch2a')\n        self.res4d_branch2b = tc.layers.conv2d(self.res4d_branch2a, kernel_size=3, num_outputs=256, scope='res4d_branch2b')\n        self.res4d_branch2c = tc.layers.conv2d(self.res4d_branch2b, kernel_size=1, num_outputs=1024, activation_fn=None, scope='res4d_branch2c')\n        self.res4d = tf.add(self.res4d_branch2c, self.res4c, name='res4d_add')\n        self.res4d = tf.nn.relu(self.res4d, name='res4d')\n\n        # Residual block 4e\n        self.res4e_branch2a = tc.layers.conv2d(self.res4d, kernel_size=1, num_outputs=256, scope='res4e_branch2a')\n        self.res4e_branch2b = tc.layers.conv2d(self.res4e_branch2a, kernel_size=3, num_outputs=256, scope='res4e_branch2b')\n        self.res4e_branch2c = tc.layers.conv2d(self.res4e_branch2b, kernel_size=1, num_outputs=1024, activation_fn=None, scope='res4e_branch2c')\n        self.res4e = tf.add(self.res4e_branch2c, self.res4d, name='res4e_add')\n        self.res4e = tf.nn.relu(self.res4e, name='res4e')\n\n        # Residual block 4f\n        self.res4f_branch2a = tc.layers.conv2d(self.res4e, kernel_size=1, num_outputs=256, scope='res4f_branch2a')\n        self.res4f_branch2b = tc.layers.conv2d(self.res4f_branch2a, kernel_size=3, num_outputs=256, scope='res4f_branch2b')\n        self.res4f_branch2c = tc.layers.conv2d(self.res4f_branch2b, kernel_size=1, num_outputs=1024, activation_fn=None, scope='res4f_branch2c')\n        self.res4f = tf.add(self.res4f_branch2c, self.res4e, name='res4f_add')\n        self.res4f = tf.nn.relu(self.res4f, name='res4f')\n\n        # Residual block 5a\n        self.res5a_branch2a_new = tc.layers.conv2d(self.res4f, kernel_size=1, num_outputs=512, scope='res5a_branch2a_new')\n        self.res5a_branch2b_new = tc.layers.conv2d(self.res5a_branch2a_new, kernel_size=3, num_outputs=512, scope='res5a_branch2b_new')\n        self.res5a_branch2c_new = tc.layers.conv2d(self.res5a_branch2b_new, kernel_size=1, num_outputs=1024, activation_fn=None, scope='res5a_branch2c_new')\n        self.res5a_branch1_new = tc.layers.conv2d(self.res4f, kernel_size=1, num_outputs=1024, activation_fn=None, scope='res5a_branch1_new')\n        self.res5a = tf.add(self.res5a_branch2c_new, self.res5a_branch1_new, name='res5a_add')\n        self.res5a = tf.nn.relu(self.res5a, name='res5a')\n\n        # Residual block 5b\n        self.res5b_branch2a_new = tc.layers.conv2d(self.res5a, kernel_size=1, num_outputs=256, scope='res5b_branch2a_new')\n        self.res5b_branch2b_new = tc.layers.conv2d(self.res5b_branch2a_new, kernel_size=3, num_outputs=128, scope='res5b_branch2b_new')\n        self.res5b_branch2c_new = tc.layers.conv2d(self.res5b_branch2b_new, kernel_size=1, num_outputs=256, scope='res5b_branch2c_new')\n\n        # Transpose Conv\n        self.res5c_branch1a = tf.layers.conv2d_transpose(self.res5b_branch2c_new, kernel_size=4, filters=63, activation=None, strides=2, padding='same', use_bias=False, name='res5c_branch1a')\n        self.res5c_branch2a = tf.layers.conv2d_transpose(self.res5b_branch2c_new, kernel_size=4, filters=128, activation=None, strides=2, padding='same', use_bias=False, name='res5c_branch2a')\n        self.bn5c_branch2a = tc.layers.batch_norm(self.res5c_branch2a, scale=True, is_training=self.is_training, scope='bn5c_branch2a')\n        self.bn5c_branch2a = tf.nn.relu(self.bn5c_branch2a)\n\n        self.res5c_delta_x, self.res5c_delta_y, self.res5c_delta_z = tf.split(self.res5c_branch1a, num_or_size_splits=3, axis=3)\n        self.res5c_branch1a_sqr = tf.multiply(self.res5c_branch1a, self.res5c_branch1a, name='res5c_branch1a_sqr')\n        self.res5c_delta_x_sqr, self.res5c_delta_y_sqr, self.res5c_delta_z_sqr = tf.split(self.res5c_branch1a_sqr, num_or_size_splits=3, axis=3)\n        self.res5c_bone_length_sqr = tf.add(tf.add(self.res5c_delta_x_sqr, self.res5c_delta_y_sqr), self.res5c_delta_z_sqr)\n        self.res5c_bone_length = tf.sqrt(self.res5c_bone_length_sqr)\n\n        self.res5c_branch2a_feat = tf.concat([self.bn5c_branch2a, self.res5c_delta_x, self.res5c_delta_y, self.res5c_delta_z, self.res5c_bone_length],\n                                             axis=3, name='res5c_branch2a_feat')\n\n        self.res5c_branch2b = tc.layers.conv2d(self.res5c_branch2a_feat, kernel_size=3, num_outputs=128, scope='res5c_branch2b')\n        self.res5c_branch2c = tf.layers.conv2d(self.res5c_branch2b, kernel_size=1, filters=84, activation=None, use_bias=False, name='res5c_branch2c')\n        self.heapmap, self.x_heatmap, self.y_heatmap, self.z_heatmap = tf.split(self.res5c_branch2c, num_or_size_splits=4, axis=3)\n\n\n    @property\n    def all_vars(self):\n        return tf.global_variables()\n\n\n    def load_weights(self, sess, weight_file):\n        # Read pretrained model file\n        model_weights = pickle.load(open(weight_file, 'rb'))\n\n        # For each layer each var\n        with tf.variable_scope('', reuse=True):\n            for variable in tf.global_variables():\n                var_name = variable.name.split(':')[0]\n                self._assign_weights_from_dict(var_name, model_weights, sess)\n\n\n    def _assign_weights_from_dict(self, var_name, model_weights, sess):\n        with tf.variable_scope('', reuse=True):\n            var_tf = tf.get_variable(var_name)\n            # print(var_tf)\n            sess.run(tf.assign(var_tf, model_weights[var_name]))\n            np.testing.assert_allclose(var_tf.eval(sess), model_weights[var_name])\n\n\n\n\nif __name__ == '__main__':\n    model_file = 'vnect.pkl'\n    model = VNect(368)\n\n\n    with tf.Session() as sess:\n        saver = tf.train.Saver()\n        tf_writer = tf.summary.FileWriter(logdir='./', graph=sess.graph)\n\n        sess.run(tf.global_variables_initializer())\n        print(model.res5b_branch2c_new)\n        print(model.heapmap, model.x_heatmap, model.y_heatmap, model.z_heatmap)\n\n\n"""
utils/__init__.py,0,b''
utils/utils.py,0,"b""import cv2\nimport numpy as np\nimport math\nfrom OpenGL.GL import *\n\n\n\ndef read_square_image(file, cam, boxsize, type):\n    # from file\n    if type == 'IMAGE':\n        oriImg = cv2.imread(file)\n    # from webcam\n    elif type == 'WEBCAM':\n        _, oriImg = cam.read()\n\n    scale = boxsize / (oriImg.shape[0] * 1.0)\n    imageToTest = cv2.resize(oriImg, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_LANCZOS4)\n\n    output_img = np.ones((boxsize, boxsize, 3)) * 128\n\n    if imageToTest.shape[1] < boxsize:\n        offset = imageToTest.shape[1] % 2\n        output_img[:, int(boxsize/2-math.ceil(imageToTest.shape[1]/2)):int(boxsize/2+math.ceil(imageToTest.shape[1]/2)+offset), :] = imageToTest\n    else:\n        output_img = imageToTest[:, int(imageToTest.shape[1]/2-boxsize/2):int(imageToTest.shape[1]/2+boxsize/2), :]\n    return output_img\n\ndef resize_pad_img(img, scale, output_size):\n    resized_img = cv2.resize(img, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n    pad_h = (output_size - resized_img.shape[0]) // 2\n    pad_w = (output_size - resized_img.shape[1]) // 2\n    pad_h_offset = (output_size - resized_img.shape[0]) % 2\n    pad_w_offset = (output_size - resized_img.shape[1]) % 2\n    resized_pad_img = np.pad(resized_img, ((pad_w, pad_w+pad_w_offset), (pad_h, pad_h+pad_h_offset), (0, 0)),\n                             mode='constant', constant_values=128)\n\n    return resized_pad_img\n\n\ndef draw_predicted_heatmap(heatmap, input_size):\n    heatmap_resized = cv2.resize(heatmap, (input_size, input_size))\n\n    output_img = None\n    tmp_concat_img = None\n    h_count = 0\n    for joint_num in range(heatmap_resized.shape[2]):\n        if h_count < 4:\n            tmp_concat_img = np.concatenate((tmp_concat_img, heatmap_resized[:, :, joint_num]), axis=1) \\\n                if tmp_concat_img is not None else heatmap_resized[:, :, joint_num]\n            h_count += 1\n        else:\n            output_img = np.concatenate((output_img, tmp_concat_img), axis=0) if output_img is not None else tmp_concat_img\n            tmp_concat_img = None\n            h_count = 0\n    # last row img\n    if h_count != 0:\n        while h_count < 4:\n            tmp_concat_img = np.concatenate((tmp_concat_img, np.zeros(shape=(input_size, input_size), dtype=np.float32)), axis=1)\n            h_count += 1\n        output_img = np.concatenate((output_img, tmp_concat_img), axis=0)\n\n    # adjust heatmap color\n    output_img = output_img.astype(np.uint8)\n    output_img = cv2.applyColorMap(output_img, cv2.COLORMAP_JET)\n    return output_img\n\ndef extract_2d_joint_from_heatmap(heatmap, input_size, joints_2d):\n    heatmap_resized = cv2.resize(heatmap, (input_size, input_size))\n\n    for joint_num in range(heatmap_resized.shape[2]):\n        joint_coord = np.unravel_index(np.argmax(heatmap_resized[:, :, joint_num]), (input_size, input_size))\n        joints_2d[joint_num, :] = joint_coord\n\n    return\n\n\ndef extract_3d_joints_from_heatmap(joints_2d, x_hm, y_hm, z_hm, input_size, joints_3d):\n\n    for joint_num in range(x_hm.shape[2]):\n        coord_2d_x = joints_2d[joint_num][0]\n        coord_2d_y = joints_2d[joint_num][1]\n\n        joint_x = x_hm[max(int(coord_2d_x/8), 1), max(int(coord_2d_y/8), 1), joint_num] * 10\n        joint_y = y_hm[max(int(coord_2d_x/8), 1), max(int(coord_2d_y/8), 1), joint_num] * 10\n        joint_z = z_hm[max(int(coord_2d_x/8), 1), max(int(coord_2d_y/8), 1), joint_num] * 10\n        joints_3d[joint_num, 0] = joint_x\n        joints_3d[joint_num, 1] = joint_y\n        joints_3d[joint_num, 2] = joint_z\n    joints_3d -= joints_3d[14, :]\n\n    return\n\ndef draw_limbs_2d(img, joints_2d, limb_parents):\n    for limb_num in range(len(limb_parents)-1):\n        x1 = joints_2d[limb_num, 0]\n        y1 = joints_2d[limb_num, 1]\n        x2 = joints_2d[limb_parents[limb_num], 0]\n        y2 = joints_2d[limb_parents[limb_num], 1]\n        length = ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5\n        # if length < 10000 and length > 5:\n        deg = math.degrees(math.atan2(x1 - x2, y1 - y2))\n        polygon = cv2.ellipse2Poly((int((y1 + y2) / 2), int((x1 + x2) / 2)),\n                                   (int(length / 2), 3),\n                                   int(deg),\n                                   0, 360, 1)\n        cv2.fillConvexPoly(img, polygon, color=(0,255,0))\n    return\n\ndef draw_limbs_3d(joints_3d, limb_parents, ax):\n\n    for i in range(joints_3d.shape[0]):\n        x_pair = [joints_3d[i, 0], joints_3d[limb_parents[i], 0]]\n        y_pair = [joints_3d[i, 1], joints_3d[limb_parents[i], 1]]\n        z_pair = [joints_3d[i, 2], joints_3d[limb_parents[i], 2]]\n        ax.plot(x_pair, y_pair, zs=z_pair, linewidth=3)\n\n\ndef draw_limbs_3d_gl(joints_3d, limb_parents):\n\n    glLineWidth(2)\n    glBegin(GL_LINES)\n    glColor3f(1,0,0)\n    glVertex3fv((0,0,0))\n    glVertex3fv((100,0,0))\n    glColor3f(0,1,0)\n    glVertex3fv((0,0,0))\n    glVertex3fv((0,100,0))\n    glColor3f(0,0,1)\n    glVertex3fv((0,0,0))\n    glVertex3fv((0,0,100))\n    glEnd()\n\n    glColor3f(1,1,1)\n    glBegin(GL_LINES)\n    for i in range(joints_3d.shape[0]):\n        glVertex3fv((joints_3d[i, 0], joints_3d[i, 1], joints_3d[i, 2]))\n        glVertex3fv((joints_3d[limb_parents[i], 0], joints_3d[limb_parents[i], 1], joints_3d[limb_parents[i], 2]))\n    glEnd()\n\n\n\n\n\n\n\n\n\n\n\n\n\n"""
models/nets/__init__.py,0,b''
models/nets/vnect_model_bn_folded.py,51,"b""import tensorflow as tf\nimport tensorflow.contrib as tc\n\nimport pickle\nimport numpy as np\n\n\nclass VNect():\n    def __init__(self, input_size):\n        self.is_training = False\n        self.input_holder = tf.placeholder(dtype=tf.float32,\n                                           shape=(None, input_size, input_size, 3))\n        self._create_network()\n\n    def _create_network(self):\n        # Conv\n        self.conv1 = tc.layers.conv2d(self.input_holder, kernel_size=7, num_outputs=64, stride=2, scope='conv1')\n        self.pool1 = tc.layers.max_pool2d(self.conv1, kernel_size=3, padding='same', scope='pool1')\n\n        # Residual block 2a\n        self.res2a_branch2a = tc.layers.conv2d(self.pool1, kernel_size=1, num_outputs=64, scope='res2a_branch2a')\n        self.res2a_branch2b = tc.layers.conv2d(self.res2a_branch2a, kernel_size=3, num_outputs=64, scope='res2a_branch2b')\n        self.res2a_branch2c = tc.layers.conv2d(self.res2a_branch2b, kernel_size=1, num_outputs=256, activation_fn=None, scope='res2a_branch2c')\n        self.res2a_branch1 = tc.layers.conv2d(self.pool1, kernel_size=1, num_outputs=256, activation_fn=None, scope='res2a_branch1')\n        self.res2a = tf.add(self.res2a_branch2c, self.res2a_branch1, name='res2a_add')\n        self.res2a = tf.nn.relu(self.res2a, name='res2a')\n\n        # Residual block 2b\n        self.res2b_branch2a = tc.layers.conv2d(self.res2a, kernel_size=1, num_outputs=64, scope='res2b_branch2a')\n        self.res2b_branch2b = tc.layers.conv2d(self.res2b_branch2a, kernel_size=3, num_outputs=64, scope='res2b_branch2b')\n        self.res2b_branch2c = tc.layers.conv2d(self.res2b_branch2b, kernel_size=1, num_outputs=256, activation_fn=None, scope='res2b_branch2c')\n        self.res2b = tf.add(self.res2b_branch2c, self.res2a, name='res2b_add')\n        self.res2b = tf.nn.relu(self.res2b, name='res2b')\n\n        # Residual block 2c\n        self.res2c_branch2a = tc.layers.conv2d(self.res2b, kernel_size=1, num_outputs=64, scope='res2c_branch2a')\n        self.res2c_branch2b = tc.layers.conv2d(self.res2c_branch2a, kernel_size=3, num_outputs=64, scope='res2c_branch2b')\n        self.res2c_branch2c = tc.layers.conv2d(self.res2c_branch2b, kernel_size=1, num_outputs=256, activation_fn=None, scope='res2c_branch2c')\n        self.res2c = tf.add(self.res2c_branch2c, self.res2b, name='res2c_add')\n        self.res2c = tf.nn.relu(self.res2c, name='res2c')\n\n        # Residual block 3a\n        self.res3a_branch2a = tc.layers.conv2d(self.res2c, kernel_size=1, num_outputs=128, stride=2, scope='res3a_branch2a')\n        self.res3a_branch2b = tc.layers.conv2d(self.res3a_branch2a, kernel_size=3, num_outputs=128, scope='res3a_branch2b')\n        self.res3a_branch2c = tc.layers.conv2d(self.res3a_branch2b, kernel_size=1, num_outputs=512, activation_fn=None,scope='res3a_branch2c')\n        self.res3a_branch1 = tc.layers.conv2d(self.res2c, kernel_size=1, num_outputs=512, activation_fn=None, stride=2, scope='res3a_branch1')\n        self.res3a = tf.add(self.res3a_branch2c, self.res3a_branch1, name='res3a_add')\n        self.res3a = tf.nn.relu(self.res3a, name='res3a')\n\n        # Residual block 3b\n        self.res3b_branch2a = tc.layers.conv2d(self.res3a, kernel_size=1, num_outputs=128, scope='res3b_branch2a')\n        self.res3b_branch2b = tc.layers.conv2d(self.res3b_branch2a, kernel_size=3, num_outputs=128,scope='res3b_branch2b')\n        self.res3b_branch2c = tc.layers.conv2d(self.res3b_branch2b, kernel_size=1, num_outputs=512, activation_fn=None,scope='res3b_branch2c')\n        self.res3b = tf.add(self.res3b_branch2c, self.res3a, name='res3b_add')\n        self.res3b = tf.nn.relu(self.res3b, name='res3b')\n\n        # Residual block 3c\n        self.res3c_branch2a = tc.layers.conv2d(self.res3b, kernel_size=1, num_outputs=128, scope='res3c_branch2a')\n        self.res3c_branch2b = tc.layers.conv2d(self.res3c_branch2a, kernel_size=3, num_outputs=128,scope='res3c_branch2b')\n        self.res3c_branch2c = tc.layers.conv2d(self.res3c_branch2b, kernel_size=1, num_outputs=512, activation_fn=None,scope='res3c_branch2c')\n        self.res3c = tf.add(self.res3c_branch2c, self.res3b, name='res3c_add')\n        self.res3c = tf.nn.relu(self.res3c, name='res3c')\n\n        # Residual block 3d\n        self.res3d_branch2a = tc.layers.conv2d(self.res3c, kernel_size=1, num_outputs=128, scope='res3d_branch2a')\n        self.res3d_branch2b = tc.layers.conv2d(self.res3d_branch2a, kernel_size=3, num_outputs=128,scope='res3d_branch2b')\n        self.res3d_branch2c = tc.layers.conv2d(self.res3d_branch2b, kernel_size=1, num_outputs=512, activation_fn=None,scope='res3d_branch2c')\n        self.res3d = tf.add(self.res3d_branch2c, self.res3c, name='res3d_add')\n        self.res3d = tf.nn.relu(self.res3d, name='res3d')\n\n        # Residual block 4a\n        self.res4a_branch2a = tc.layers.conv2d(self.res3d, kernel_size=1, num_outputs=256, stride=2, scope='res4a_branch2a')\n        self.res4a_branch2b = tc.layers.conv2d(self.res4a_branch2a, kernel_size=3, num_outputs=256,scope='res4a_branch2b')\n        self.res4a_branch2c = tc.layers.conv2d(self.res4a_branch2b, kernel_size=1, num_outputs=1024, activation_fn=None,scope='res4a_branch2c')\n        self.res4a_branch1 = tc.layers.conv2d(self.res3d, kernel_size=1, num_outputs=1024, activation_fn=None, stride=2, scope='res4a_branch1')\n        self.res4a = tf.add(self.res4a_branch2c, self.res4a_branch1, name='res4a_add')\n        self.res4a = tf.nn.relu(self.res4a, name='res4a')\n\n        # Residual block 4b\n        self.res4b_branch2a = tc.layers.conv2d(self.res4a, kernel_size=1, num_outputs=256, scope='res4b_branch2a')\n        self.res4b_branch2b = tc.layers.conv2d(self.res4b_branch2a, kernel_size=3, num_outputs=256, scope='res4b_branch2b')\n        self.res4b_branch2c = tc.layers.conv2d(self.res4b_branch2b, kernel_size=1, num_outputs=1024, activation_fn=None, scope='res4b_branch2c')\n        self.res4b = tf.add(self.res4b_branch2c, self.res4a, name='res4b_add')\n        self.res4b = tf.nn.relu(self.res4b, name='res4b')\n\n        # Residual block 4c\n        self.res4c_branch2a = tc.layers.conv2d(self.res4b, kernel_size=1, num_outputs=256, scope='res4c_branch2a')\n        self.res4c_branch2b = tc.layers.conv2d(self.res4c_branch2a, kernel_size=3, num_outputs=256, scope='res4c_branch2b')\n        self.res4c_branch2c = tc.layers.conv2d(self.res4c_branch2b, kernel_size=1, num_outputs=1024, activation_fn=None, scope='res4c_branch2c')\n        self.res4c = tf.add(self.res4c_branch2c, self.res4b, name='res4c_add')\n        self.res4c = tf.nn.relu(self.res4c, name='res4c')\n\n        # Residual block 4d\n        self.res4d_branch2a = tc.layers.conv2d(self.res4c, kernel_size=1, num_outputs=256, scope='res4d_branch2a')\n        self.res4d_branch2b = tc.layers.conv2d(self.res4d_branch2a, kernel_size=3, num_outputs=256, scope='res4d_branch2b')\n        self.res4d_branch2c = tc.layers.conv2d(self.res4d_branch2b, kernel_size=1, num_outputs=1024, activation_fn=None, scope='res4d_branch2c')\n        self.res4d = tf.add(self.res4d_branch2c, self.res4c, name='res4d_add')\n        self.res4d = tf.nn.relu(self.res4d, name='res4d')\n\n        # Residual block 4e\n        self.res4e_branch2a = tc.layers.conv2d(self.res4d, kernel_size=1, num_outputs=256, scope='res4e_branch2a')\n        self.res4e_branch2b = tc.layers.conv2d(self.res4e_branch2a, kernel_size=3, num_outputs=256, scope='res4e_branch2b')\n        self.res4e_branch2c = tc.layers.conv2d(self.res4e_branch2b, kernel_size=1, num_outputs=1024, activation_fn=None, scope='res4e_branch2c')\n        self.res4e = tf.add(self.res4e_branch2c, self.res4d, name='res4e_add')\n        self.res4e = tf.nn.relu(self.res4e, name='res4e')\n\n        # Residual block 4f\n        self.res4f_branch2a = tc.layers.conv2d(self.res4e, kernel_size=1, num_outputs=256, scope='res4f_branch2a')\n        self.res4f_branch2b = tc.layers.conv2d(self.res4f_branch2a, kernel_size=3, num_outputs=256, scope='res4f_branch2b')\n        self.res4f_branch2c = tc.layers.conv2d(self.res4f_branch2b, kernel_size=1, num_outputs=1024, activation_fn=None, scope='res4f_branch2c')\n        self.res4f = tf.add(self.res4f_branch2c, self.res4e, name='res4f_add')\n        self.res4f = tf.nn.relu(self.res4f, name='res4f')\n\n        # Residual block 5a\n        self.res5a_branch2a_new = tc.layers.conv2d(self.res4f, kernel_size=1, num_outputs=512, scope='res5a_branch2a_new')\n        self.res5a_branch2b_new = tc.layers.conv2d(self.res5a_branch2a_new, kernel_size=3, num_outputs=512, scope='res5a_branch2b_new')\n        self.res5a_branch2c_new = tc.layers.conv2d(self.res5a_branch2b_new, kernel_size=1, num_outputs=1024, activation_fn=None, scope='res5a_branch2c_new')\n        self.res5a_branch1_new = tc.layers.conv2d(self.res4f, kernel_size=1, num_outputs=1024, activation_fn=None, scope='res5a_branch1_new')\n        self.res5a = tf.add(self.res5a_branch2c_new, self.res5a_branch1_new, name='res5a_add')\n        self.res5a = tf.nn.relu(self.res5a, name='res5a')\n\n        # Residual block 5b\n        self.res5b_branch2a_new = tc.layers.conv2d(self.res5a, kernel_size=1, num_outputs=256, scope='res5b_branch2a_new')\n        self.res5b_branch2b_new = tc.layers.conv2d(self.res5b_branch2a_new, kernel_size=3, num_outputs=128, scope='res5b_branch2b_new')\n        self.res5b_branch2c_new = tc.layers.conv2d(self.res5b_branch2b_new, kernel_size=1, num_outputs=256, scope='res5b_branch2c_new')\n\n        # Transpose Conv\n        self.res5c_branch1a = tf.layers.conv2d_transpose(self.res5b_branch2c_new, kernel_size=4, filters=63, activation=None, strides=2, padding='same', use_bias=False, name='res5c_branch1a')\n        self.res5c_branch2a = tf.layers.conv2d_transpose(self.res5b_branch2c_new, kernel_size=4, filters=128, activation=None, strides=2, padding='same', use_bias=False, name='res5c_branch2a')\n        self.bn5c_branch2a = tc.layers.batch_norm(self.res5c_branch2a, scale=True, is_training=self.is_training, scope='bn5c_branch2a')\n        self.bn5c_branch2a = tf.nn.relu(self.bn5c_branch2a)\n\n        self.res5c_delta_x, self.res5c_delta_y, self.res5c_delta_z = tf.split(self.res5c_branch1a, num_or_size_splits=3, axis=3)\n        self.res5c_branch1a_sqr = tf.multiply(self.res5c_branch1a, self.res5c_branch1a, name='res5c_branch1a_sqr')\n        self.res5c_delta_x_sqr, self.res5c_delta_y_sqr, self.res5c_delta_z_sqr = tf.split(self.res5c_branch1a_sqr, num_or_size_splits=3, axis=3)\n        self.res5c_bone_length_sqr = tf.add(tf.add(self.res5c_delta_x_sqr, self.res5c_delta_y_sqr), self.res5c_delta_z_sqr)\n        self.res5c_bone_length = tf.sqrt(self.res5c_bone_length_sqr)\n\n        self.res5c_branch2a_feat = tf.concat([self.bn5c_branch2a, self.res5c_delta_x, self.res5c_delta_y, self.res5c_delta_z, self.res5c_bone_length],\n                                             axis=3, name='res5c_branch2a_feat')\n\n        self.res5c_branch2b = tc.layers.conv2d(self.res5c_branch2a_feat, kernel_size=3, num_outputs=128, scope='res5c_branch2b')\n        self.res5c_branch2c = tf.layers.conv2d(self.res5c_branch2b, kernel_size=1, filters=84, activation=None, use_bias=False, name='res5c_branch2c')\n        self.heapmap, self.x_heatmap, self.y_heatmap, self.z_heatmap = tf.split(self.res5c_branch2c, num_or_size_splits=4, axis=3)\n\n\n    @property\n    def all_vars(self):\n        return tf.global_variables()\n\n\n    def load_weights(self, sess, weight_file):\n        # Read pretrained model file\n        model_weights = pickle.load(open(weight_file, 'rb'))\n\n        # For each layer each var\n        with tf.variable_scope('', reuse=True):\n            for variable in tf.global_variables():\n                var_name = variable.name.split(':')[0]\n                self._assign_weights_from_dict(var_name, model_weights, sess)\n\n\n    def _assign_weights_from_dict(self, var_name, model_weights, sess):\n        with tf.variable_scope('', reuse=True):\n            var_tf = tf.get_variable(var_name)\n            # print(var_tf)\n            sess.run(tf.assign(var_tf, model_weights[var_name]))\n            np.testing.assert_allclose(var_tf.eval(sess), model_weights[var_name])\n\n\n\n\nif __name__ == '__main__':\n    model_file = 'vnect.pkl'\n    model = VNect(368)\n\n\n    with tf.Session() as sess:\n        saver = tf.train.Saver()\n        tf_writer = tf.summary.FileWriter(logdir='./', graph=sess.graph)\n\n        sess.run(tf.global_variables_initializer())\n        print(model.res5b_branch2c_new)\n        print(model.heapmap, model.x_heatmap, model.y_heatmap, model.z_heatmap)\n\n\n"""
