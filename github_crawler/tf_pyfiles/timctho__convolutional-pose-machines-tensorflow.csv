file_path,api_count,code
config.py,0,"b'class FLAGS(object):\n    """""" """"""\n    """"""\n    General settings\n    """"""\n    input_size = 256\n    heatmap_size = 32\n    cpm_stages = 3\n    joint_gaussian_variance = 1.0\n    center_radius = 21\n    num_of_joints = 21\n    color_channel = \'RGB\'\n    normalize_img = True\n    use_gpu = True\n    gpu_id = 0\n\n\n    """"""\n    Demo settings\n    """"""\n    # \'MULTI\': show multiple stage heatmaps\n    # \'SINGLE\': show last stage heatmap\n    # \'Joint_HM\': show last stage heatmap for each joint\n    # \'image or video path\': show detection on single image or video\n    DEMO_TYPE = \'SINGLE\'\n\n    model_path = \'cpm_hand\'\n    cam_id = 0\n\n    webcam_height = 480\n    webcam_width = 640\n\n    use_kalman = True\n    kalman_noise = 0.03\n\n\n    """"""\n    Training settings\n    """"""\n    network_def = \'cpm_hand\'\n    train_img_dir = \'\'\n    val_img_dir = \'\'\n    bg_img_dir = \'\'\n    pretrained_model = \'cpm_hand\'\n    batch_size = 5\n    init_lr = 0.001\n    lr_decay_rate = 0.5\n    lr_decay_step = 10000\n    training_iters = 300000\n    verbose_iters = 10\n    validation_iters = 1000\n    model_save_iters = 5000\n    augmentation_config = {\'hue_shift_limit\': (-5, 5),\n                           \'sat_shift_limit\': (-10, 10),\n                           \'val_shift_limit\': (-15, 15),\n                           \'translation_limit\': (-0.15, 0.15),\n                           \'scale_limit\': (-0.3, 0.5),\n                           \'rotate_limit\': (-90, 90)}\n    hnm = True  # Make sure generate hnm files first\n    do_cropping = True\n\n    """"""\n    For Freeze graphs\n    """"""\n    output_node_names = \'stage_3/mid_conv7/BiasAdd:0\'\n\n\n    """"""\n    For Drawing\n    """"""\n    # Default Pose\n    default_hand = [[259, 335],\n                    [245, 311],\n                    [226, 288],\n                    [206, 270],\n                    [195, 261],\n                    [203, 308],\n                    [165, 290],\n                    [139, 287],\n                    [119, 284],\n                    [199, 328],\n                    [156, 318],\n                    [128, 314],\n                    [104, 318],\n                    [204, 341],\n                    [163, 340],\n                    [133, 347],\n                    [108, 349],\n                    [206, 359],\n                    [176, 368],\n                    [164, 370],\n                    [144, 377]]\n\n    # Limb connections\n    limbs = [[0, 1],\n             [1, 2],\n             [2, 3],\n             [3, 4],\n             [0, 5],\n             [5, 6],\n             [6, 7],\n             [7, 8],\n             [0, 9],\n             [9, 10],\n             [10, 11],\n             [11, 12],\n             [0, 13],\n             [13, 14],\n             [14, 15],\n             [15, 16],\n             [0, 17],\n             [17, 18],\n             [18, 19],\n             [19, 20]\n             ]\n\n    # Finger colors\n    joint_color_code = [[139, 53, 255],\n                        [0, 56, 255],\n                        [43, 140, 237],\n                        [37, 168, 36],\n                        [147, 147, 0],\n                        [70, 17, 145]]\n\n    # My hand joint order\n    # FLAGS.limbs = [[0, 1],\n    #          [1, 2],\n    #          [2, 3],\n    #          [3, 20],\n    #          [4, 5],\n    #          [5, 6],\n    #          [6, 7],\n    #          [7, 20],\n    #          [8, 9],\n    #          [9, 10],\n    #          [10, 11],\n    #          [11, 20],\n    #          [12, 13],\n    #          [13, 14],\n    #          [14, 15],\n    #          [15, 20],\n    #          [16, 17],\n    #          [17, 18],\n    #          [18, 19],\n    #          [19, 20]\n    #          ]\n\n\n\n\n\n\n\n\n\n\n'"
demo_cpm_body.py,24,"b'import numpy as np\nfrom utils import cpm_utils\nimport cv2\nimport time\nimport math\nimport sys\nimport os\nimport imageio\nimport tensorflow as tf\nfrom models.nets import cpm_body_slim\n\n\n""""""Parameters\n""""""\nFLAGS = tf.app.flags.FLAGS\ntf.app.flags.DEFINE_string(\'DEMO_TYPE\',\n                           # default_value=\'test_imgs/roger.png\',\n                           default_value=\'test_imgs/single_gym.mp4\',\n                           # default_value=\'SINGLE\',\n                           docstring=\'MULTI: show multiple stage,\'\n                                     \'SINGLE: only last stage,\'\n                                     \'HM: show last stage heatmap,\'\n                                     \'paths to .jpg or .png image\'\n                                     \'paths to .avi or .flv or .mp4 video\')\ntf.app.flags.DEFINE_string(\'model_path\',\n                           default_value=\'models/weights/cpm_body.pkl\',\n                           docstring=\'Your model\')\ntf.app.flags.DEFINE_integer(\'input_size\',\n                            default_value=368,\n                            docstring=\'Input image size\')\ntf.app.flags.DEFINE_integer(\'hmap_size\',\n                            default_value=46,\n                            docstring=\'Output heatmap size\')\ntf.app.flags.DEFINE_integer(\'cmap_radius\',\n                            default_value=21,\n                            docstring=\'Center map gaussian variance\')\ntf.app.flags.DEFINE_integer(\'joints\',\n                            default_value=14,\n                            docstring=\'Number of joints\')\ntf.app.flags.DEFINE_integer(\'stages\',\n                            default_value=6,\n                            docstring=\'How many CPM stages\')\ntf.app.flags.DEFINE_integer(\'cam_num\',\n                            default_value=0,\n                            docstring=\'Webcam device number\')\ntf.app.flags.DEFINE_bool(\'KALMAN_ON\',\n                         default_value=False,\n                         docstring=\'enalbe kalman filter\')\ntf.app.flags.DEFINE_integer(\'kalman_noise\',\n                            default_value=3e-2,\n                            docstring=\'Kalman filter noise value\')\ntf.app.flags.DEFINE_string(\'color_channel\',\n                           default_value=\'RGB\',\n                           docstring=\'\')\n\n# Set color for each finger\njoint_color_code = [[139, 53, 255],\n                    [0, 56, 255],\n                    [43, 140, 237],\n                    [37, 168, 36],\n                    [147, 147, 0],\n                    [70, 17, 145]]\n\n\nlimbs = [[0, 1],\n         [2, 3],\n         [3, 4],\n         [5, 6],\n         [6, 7],\n         [8, 9],\n         [9, 10],\n         [11, 12],\n         [12, 13]]\n\nif sys.version_info.major == 3:\n    PYTHON_VERSION = 3\nelse:\n    PYTHON_VERSION = 2\n\n\ndef mgray(test_img_resize, test_img):\n    test_img_resize = np.dot(test_img_resize[..., :3], [0.299, 0.587, 0.114]).reshape(\n                    (FLAGS.input_size, FLAGS.input_size, 1))\n    cv2.imshow(\'color\', test_img.astype(np.uint8))\n    cv2.imshow(\'gray\', test_img_resize.astype(np.uint8))\n    cv2.waitKey(1)\n    return test_img_resize\n\n\ndef main(argv):\n    tf_device = \'/gpu:0\'\n    with tf.device(tf_device):\n        """"""Build graph\n        """"""\n        if FLAGS.color_channel == \'RGB\':\n            input_data = tf.placeholder(dtype=tf.float32,\n                                        shape=[None, FLAGS.input_size, FLAGS.input_size, 3],\n                                        name=\'input_image\')\n        else:\n            input_data = tf.placeholder(dtype=tf.float32,\n                                        shape=[None, FLAGS.input_size, FLAGS.input_size, 1],\n                                        name=\'input_image\')\n\n        center_map = tf.placeholder(dtype=tf.float32,\n                                    shape=[None, FLAGS.input_size, FLAGS.input_size, 1],\n                                    name=\'center_map\')\n\n        model = cpm_body_slim.CPM_Model(FLAGS.stages, FLAGS.joints + 1)\n        model.build_model(input_data, center_map, 1)\n\n    saver = tf.train.Saver()\n\n    """"""Create session and restore weights\n    """"""\n    sess = tf.Session()\n\n    sess.run(tf.global_variables_initializer())\n    if FLAGS.model_path.endswith(\'pkl\'):\n        model.load_weights_from_file(FLAGS.model_path, sess, False)\n    else:\n        saver.restore(sess, FLAGS.model_path)\n\n    test_center_map = cpm_utils.gaussian_img(FLAGS.input_size,\n                                             FLAGS.input_size,\n                                             FLAGS.input_size / 2,\n                                             FLAGS.input_size / 2,\n                                             FLAGS.cmap_radius)\n    test_center_map = np.reshape(test_center_map, [1, FLAGS.input_size,\n                                                   FLAGS.input_size, 1])\n\n    # Check weights\n    for variable in tf.trainable_variables():\n        with tf.variable_scope(\'\', reuse=True):\n            var = tf.get_variable(variable.name.split(\':0\')[0])\n            print(variable.name, np.mean(sess.run(var)))\n\n    # Create kalman filters\n    if FLAGS.KALMAN_ON:\n        kalman_filter_array = [cv2.KalmanFilter(4, 2) for _ in range(FLAGS.joints)]\n        for _, joint_kalman_filter in enumerate(kalman_filter_array):\n            joint_kalman_filter.transitionMatrix = np.array([[1, 0, 1, 0],\n                                                             [0, 1, 0, 1],\n                                                             [0, 0, 1, 0],\n                                                             [0, 0, 0, 1]],\n                                                            np.float32)\n            joint_kalman_filter.measurementMatrix = np.array([[1, 0, 0, 0],\n                                                              [0, 1, 0, 0]],\n                                                             np.float32)\n            joint_kalman_filter.processNoiseCov = np.array([[1, 0, 0, 0],\n                                                            [0, 1, 0, 0],\n                                                            [0, 0, 1, 0],\n                                                            [0, 0, 0, 1]],\n                                                           np.float32) * FLAGS.kalman_noise\n    else:\n        kalman_filter_array = None\n\n    # read in video / flow frames\n    if FLAGS.DEMO_TYPE.endswith((\'avi\', \'flv\', \'mp4\')):\n        # OpenCV can only read in \'.avi\' files\n        cam = imageio.get_reader(FLAGS.DEMO_TYPE)\n    else:\n        cam = cv2.VideoCapture(FLAGS.cam_num)\n\n    # iamge processing\n    with tf.device(tf_device):\n        if FLAGS.DEMO_TYPE.endswith((\'avi\', \'flv\', \'mp4\')):\n            ori_fps = cam.get_meta_data()[\'fps\']\n            print(\'This video fps is %f\' % ori_fps)\n            video_length = cam.get_length()\n            writer_path = os.path.join(\'results\', os.path.basename(FLAGS.DEMO_TYPE))\n            # !! OpenCV can only write in .avi\n            cv_writer = cv2.VideoWriter(writer_path + \'.avi\',\n                                        # cv2.cv.CV_FOURCC(\'M\', \'J\', \'P\', \'G\'),\n                                        cv2.VideoWriter_fourcc(\'M\', \'J\', \'P\', \'G\'),\n                                        ori_fps,\n                                        (FLAGS.input_size, FLAGS.input_size))\n            # imageio_writer = imageio.get_writer(writer_path, fps=ori_fps)\n\n            try:\n                for it, im in enumerate(cam):\n                    test_img_t = time.time()\n\n                    test_img = cpm_utils.read_image(im, [], FLAGS.input_size, \'VIDEO\')\n                    test_img_resize = cv2.resize(test_img, (FLAGS.input_size, FLAGS.input_size))\n                    print(\'img read time %f\' % (time.time() - test_img_t))\n\n                    if FLAGS.color_channel == \'GRAY\':\n                        test_img_resize = mgray(test_img_resize, test_img)\n\n                    test_img_input = test_img_resize / 256.0 - 0.5\n                    test_img_input = np.expand_dims(test_img_input, axis=0)\n\n                    # Inference\n                    fps_t = time.time()\n                    predict_heatmap, stage_heatmap_np = sess.run([model.current_heatmap,\n                                                                  model.stage_heatmap,\n                                                                  ],\n                                                                 feed_dict={\'input_image:0\': test_img_input,\n                                                                            \'center_map:0\': test_center_map})\n\n                    # Show visualized image\n                    demo_img = visualize_result(test_img, FLAGS, stage_heatmap_np, kalman_filter_array)\n                    cv2.imshow(\'demo_img\', demo_img.astype(np.uint8))\n                    if (cv2.waitKey(1) == ord(\'q\')): break\n                    print(\'fps: %.2f\' % (1 / (time.time() - fps_t)))\n\n                    cv_writer.write(demo_img.astype(np.uint8))\n                    # imageio_writer.append_data(demo_img[:, :, 1])\n            except KeyboardInterrupt:\n                print(\'Stopped! {}/{} frames captured!\'.format(it, video_length))\n            finally:\n                cv_writer.release()\n                # imageio_writer.close()\n        else:\n            while True:\n                test_img_t = time.time()\n\n                if FLAGS.DEMO_TYPE.endswith((\'png\', \'jpg\')):\n                    test_img = cpm_utils.read_image(FLAGS.DEMO_TYPE, [], FLAGS.input_size, \'IMAGE\')\n                else:\n                    test_img = cpm_utils.read_image([], cam, FLAGS.input_size, \'WEBCAM\')\n\n                test_img_resize = cv2.resize(test_img, (FLAGS.input_size, FLAGS.input_size))\n                print(\'img read time %f\' % (time.time() - test_img_t))\n\n                if FLAGS.color_channel == \'GRAY\':\n                    test_img_resize = mgray(test_img_resize, test_img)\n\n                test_img_input = test_img_resize / 256.0 - 0.5\n                test_img_input = np.expand_dims(test_img_input, axis=0)\n\n                if FLAGS.DEMO_TYPE.endswith((\'png\', \'jpg\')):\n                    # Inference\n                    fps_t = time.time()\n                    predict_heatmap, stage_heatmap_np = sess.run([model.current_heatmap,\n                                                                  model.stage_heatmap, ],\n                                                                 feed_dict={\'input_image:0\': test_img_input,\n                                                                            \'center_map:0\': test_center_map})\n\n                    # Show visualized image\n                    demo_img = visualize_result(test_img, FLAGS, stage_heatmap_np, kalman_filter_array)\n                    cv2.imshow(\'demo_img\', demo_img.astype(np.uint8))\n                    if cv2.waitKey(0) == ord(\'q\'): break\n                    print(\'fps: %.2f\' % (1 / (time.time() - fps_t)))\n\n                elif FLAGS.DEMO_TYPE == \'MULTI\':\n\n                    # Inference\n                    fps_t = time.time()\n                    predict_heatmap, stage_heatmap_np = sess.run([model.current_heatmap,\n                                                                  model.stage_heatmap,\n                                                                  ],\n                                                                 feed_dict={\'input_image:0\': test_img_input,\n                                                                            \'center_map:0\': test_center_map})\n\n                    # Show visualized image\n                    demo_img = visualize_result(test_img, FLAGS, stage_heatmap_np, kalman_filter_array)\n                    cv2.imshow(\'demo_img\', demo_img.astype(np.uint8))\n                    if cv2.waitKey(1) == ord(\'q\'): break\n                    print(\'fps: %.2f\' % (1 / (time.time() - fps_t)))\n\n\n                elif FLAGS.DEMO_TYPE == \'SINGLE\':\n\n                    # Inference\n                    fps_t = time.time()\n                    stage_heatmap_np = sess.run([model.stage_heatmap[5]],\n                                                feed_dict={\'input_image:0\': test_img_input,\n                                                           \'center_map:0\': test_center_map})\n\n                    # Show visualized image\n                    demo_img = visualize_result(test_img, FLAGS, stage_heatmap_np, kalman_filter_array)\n                    cv2.imshow(\'current heatmap\', (demo_img).astype(np.uint8))\n                    if cv2.waitKey(1) == ord(\'q\'): break\n                    print(\'fps: %.2f\' % (1 / (time.time() - fps_t)))\n\n\n                elif FLAGS.DEMO_TYPE == \'HM\':\n\n                    # Inference\n                    fps_t = time.time()\n                    stage_heatmap_np = sess.run([model.stage_heatmap[FLAGS.stages - 1]],\n                                                feed_dict={\'input_image:0\': test_img_input,\n                                                           \'center_map:0\': test_center_map})\n                    print(\'fps: %.2f\' % (1 / (time.time() - fps_t)))\n\n                    # demo_stage_heatmap = stage_heatmap_np[len(stage_heatmap_np) - 1][0, :, :, 0:FLAGS.joints].reshape(\n                    #     (FLAGS.hmap_size, FLAGS.hmap_size, FLAGS.joints))\n                    demo_stage_heatmap = stage_heatmap_np[-1][0, :, :, 0:FLAGS.joints].reshape(\n                        (FLAGS.hmap_size, FLAGS.hmap_size, FLAGS.joints))\n                    demo_stage_heatmap = cv2.resize(demo_stage_heatmap, (FLAGS.input_size, FLAGS.input_size))\n\n                    vertical_imgs = []\n                    tmp_img = None\n                    joint_coord_set = np.zeros((FLAGS.joints, 2))\n\n                    for joint_num in range(FLAGS.joints):\n                        # Concat until 4 img\n                        if (joint_num % 4) == 0 and joint_num != 0:\n                            vertical_imgs.append(tmp_img)\n                            tmp_img = None\n\n                        demo_stage_heatmap[:, :, joint_num] *= (255 / np.max(demo_stage_heatmap[:, :, joint_num]))\n\n                        # Plot color joints\n                        if np.min(demo_stage_heatmap[:, :, joint_num]) > -50:\n                            joint_coord = np.unravel_index(np.argmax(demo_stage_heatmap[:, :, joint_num]),\n                                                           (FLAGS.input_size, FLAGS.input_size))\n                            joint_coord_set[joint_num, :] = joint_coord\n                            color_code_num = (joint_num // 4)\n\n                            if joint_num in [0, 4, 8, 12, 16]:\n                                if PYTHON_VERSION == 3:\n                                    joint_color = list(\n                                        map(lambda x: x + 35 * (joint_num % 4), joint_color_code[color_code_num]))\n                                else:\n                                    joint_color = map(lambda x: x + 35 * (joint_num % 4), joint_color_code[color_code_num])\n\n                                cv2.circle(test_img, center=(joint_coord[1], joint_coord[0]), radius=3, color=joint_color,\n                                           thickness=-1)\n                            else:\n                                if PYTHON_VERSION == 3:\n                                    joint_color = list(\n                                        map(lambda x: x + 35 * (joint_num % 4), joint_color_code[color_code_num]))\n                                else:\n                                    joint_color = map(lambda x: x + 35 * (joint_num % 4), joint_color_code[color_code_num])\n\n                                cv2.circle(test_img, center=(joint_coord[1], joint_coord[0]), radius=3, color=joint_color,\n                                           thickness=-1)\n\n                        # Put text\n                        tmp = demo_stage_heatmap[:, :, joint_num].astype(np.uint8)\n                        tmp = cv2.putText(tmp, \'Min:\' + str(np.min(demo_stage_heatmap[:, :, joint_num])),\n                                          org=(5, 20), fontFace=cv2.FONT_HERSHEY_COMPLEX, fontScale=0.3, color=150)\n                        tmp = cv2.putText(tmp, \'Mean:\' + str(np.mean(demo_stage_heatmap[:, :, joint_num])),\n                                          org=(5, 30), fontFace=cv2.FONT_HERSHEY_COMPLEX, fontScale=0.3, color=150)\n                        tmp_img = np.concatenate((tmp_img, tmp), axis=0) \\\n                            if tmp_img is not None else tmp\n\n                    # Plot limbs\n                    for limb_num in range(len(limbs)):\n                        if np.min(demo_stage_heatmap[:, :, limbs[limb_num][0]]) > -2000 and np.min(\n                                demo_stage_heatmap[:, :, limbs[limb_num][1]]) > -2000:\n                            x1 = joint_coord_set[limbs[limb_num][0], 0]\n                            y1 = joint_coord_set[limbs[limb_num][0], 1]\n                            x2 = joint_coord_set[limbs[limb_num][1], 0]\n                            y2 = joint_coord_set[limbs[limb_num][1], 1]\n                            length = ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5\n                            if length < 10000 and length > 5:\n                                deg = math.degrees(math.atan2(x1 - x2, y1 - y2))\n                                polygon = cv2.ellipse2Poly((int((y1 + y2) / 2), int((x1 + x2) / 2)),\n                                                           (int(length / 2), 3),\n                                                           int(deg),\n                                                           0, 360, 1)\n                                color_code_num = limb_num // 4\n                                if PYTHON_VERSION == 3:\n                                    limb_color = list(\n                                        map(lambda x: x + 35 * (limb_num % 4), joint_color_code[color_code_num]))\n                                else:\n                                    limb_color = map(lambda x: x + 35 * (limb_num % 4), joint_color_code[color_code_num])\n\n                                cv2.fillConvexPoly(test_img, polygon, color=limb_color)\n\n                    if tmp_img is not None:\n                        tmp_img = np.lib.pad(tmp_img, ((0, vertical_imgs[0].shape[0] - tmp_img.shape[0]), (0, 0)),\n                                             \'constant\', constant_values=(0, 0))\n                        vertical_imgs.append(tmp_img)\n\n                    # Concat horizontally\n                    output_img = None\n                    for col in range(len(vertical_imgs)):\n                        output_img = np.concatenate((output_img, vertical_imgs[col]), axis=1) if output_img is not None else \\\n                            vertical_imgs[col]\n\n                    output_img = output_img.astype(np.uint8)\n                    output_img = cv2.applyColorMap(output_img, cv2.COLORMAP_JET)\n                    test_img = cv2.resize(test_img, (300, 300), cv2.INTER_LANCZOS4)\n                    cv2.imshow(\'hm\', output_img)\n                    cv2.moveWindow(\'hm\', 2000, 200)\n                    cv2.imshow(\'rgb\', test_img)\n                    cv2.moveWindow(\'rgb\', 2000, 750)\n                    if cv2.waitKey(1) == ord(\'q\'): break\n\n\ndef visualize_result(test_img, FLAGS, stage_heatmap_np, kalman_filter_array):\n    hm_t = time.time()\n    demo_stage_heatmaps = []\n    if FLAGS.DEMO_TYPE == \'MULTI\':\n        for stage in range(len(stage_heatmap_np)):\n            demo_stage_heatmap = stage_heatmap_np[stage][0, :, :, 0:FLAGS.joints].reshape(\n                (FLAGS.hmap_size, FLAGS.hmap_size, FLAGS.joints))\n            demo_stage_heatmap = cv2.resize(demo_stage_heatmap, (test_img.shape[1], test_img.shape[0]))\n            demo_stage_heatmap = np.amax(demo_stage_heatmap, axis=2)\n            demo_stage_heatmap = np.reshape(demo_stage_heatmap, (test_img.shape[1], test_img.shape[0], 1))\n            demo_stage_heatmap = np.repeat(demo_stage_heatmap, 3, axis=2)\n            demo_stage_heatmap *= 255\n            demo_stage_heatmaps.append(demo_stage_heatmap)\n\n        # last_heatmap = stage_heatmap_np[len(stage_heatmap_np) - 1][0, :, :, 0:FLAGS.joints].reshape(\n        #     (FLAGS.hmap_size, FLAGS.hmap_size, FLAGS.joints))\n        last_heatmap = stage_heatmap_np[-1][0, :, :, 0:FLAGS.joints].reshape(\n            (FLAGS.hmap_size, FLAGS.hmap_size, FLAGS.joints))\n        last_heatmap = cv2.resize(last_heatmap, (test_img.shape[1], test_img.shape[0]))\n    else:\n        # last_heatmap = stage_heatmap_np[len(stage_heatmap_np) - 1][0, :, :, 0:FLAGS.joints].reshape(\n        #     (FLAGS.hmap_size, FLAGS.hmap_size, FLAGS.joints))\n        last_heatmap = stage_heatmap_np[-1][0, :, :, 0:FLAGS.joints].reshape(\n            (FLAGS.hmap_size, FLAGS.hmap_size, FLAGS.joints))\n        last_heatmap = cv2.resize(last_heatmap, (test_img.shape[1], test_img.shape[0]))\n    print(\'hm resize time %f\' % (time.time() - hm_t))\n\n    joint_t = time.time()\n    joint_coord_set = np.zeros((FLAGS.joints, 2))\n\n    # Plot joint colors\n    if kalman_filter_array is not None:\n        for joint_num in range(FLAGS.joints):\n            joint_coord = np.unravel_index(np.argmax(last_heatmap[:, :, joint_num]),\n                                           (test_img.shape[0], test_img.shape[1]))\n            # add a dimension for kalman filter\n            joint_coord = np.array(joint_coord).reshape((2, 1)).astype(np.float32)\n            kalman_filter_array[joint_num].correct(joint_coord)\n            kalman_pred = kalman_filter_array[joint_num].predict()\n            joint_coord_set[joint_num, :] = np.array([kalman_pred[0], kalman_pred[1]]).reshape((2))\n\n            color_code_num = (joint_num // 4)\n            if joint_num in [0, 4, 8, 12, 16]:\n                if PYTHON_VERSION == 3:\n                    joint_color = list(map(lambda x: x + 35 * (joint_num % 4), joint_color_code[color_code_num]))\n                else:\n                    joint_color = map(lambda x: x + 35 * (joint_num % 4), joint_color_code[color_code_num])\n\n                cv2.circle(test_img, center=(joint_coord[1], joint_coord[0]), radius=3, color=joint_color, thickness=-1)\n            else:\n                if PYTHON_VERSION == 3:\n                    joint_color = list(map(lambda x: x + 35 * (joint_num % 4), joint_color_code[color_code_num]))\n                else:\n                    joint_color = map(lambda x: x + 35 * (joint_num % 4), joint_color_code[color_code_num])\n\n                cv2.circle(test_img, center=(joint_coord[1], joint_coord[0]), radius=3, color=joint_color, thickness=-1)\n    else:\n        for joint_num in range(FLAGS.joints):\n            joint_coord = np.unravel_index(np.argmax(last_heatmap[:, :, joint_num]),\n                                           (test_img.shape[0], test_img.shape[1]))\n            joint_coord_set[joint_num, :] = [joint_coord[0], joint_coord[1]]\n\n            color_code_num = (joint_num // 4)\n            if joint_num in [0, 4, 8, 12, 16]:\n                if PYTHON_VERSION == 3:\n                    joint_color = list(map(lambda x: x + 35 * (joint_num % 4), joint_color_code[color_code_num]))\n                else:\n                    joint_color = map(lambda x: x + 35 * (joint_num % 4), joint_color_code[color_code_num])\n\n                cv2.circle(test_img, center=(joint_coord[1], joint_coord[0]), radius=3, color=joint_color, thickness=-1)\n            else:\n                if PYTHON_VERSION == 3:\n                    joint_color = list(map(lambda x: x + 35 * (joint_num % 4), joint_color_code[color_code_num]))\n                else:\n                    joint_color = map(lambda x: x + 35 * (joint_num % 4), joint_color_code[color_code_num])\n\n                cv2.circle(test_img, center=(joint_coord[1], joint_coord[0]), radius=3, color=joint_color, thickness=-1)\n    print(\'plot joint time %f\' % (time.time() - joint_t))\n\n    limb_t = time.time()\n    # Plot limb colors\n    for limb_num in range(len(limbs)):\n\n        x1 = joint_coord_set[limbs[limb_num][0], 0]\n        y1 = joint_coord_set[limbs[limb_num][0], 1]\n        x2 = joint_coord_set[limbs[limb_num][1], 0]\n        y2 = joint_coord_set[limbs[limb_num][1], 1]\n        length = ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5\n        if length < 200 and length > 5:\n            deg = math.degrees(math.atan2(x1 - x2, y1 - y2))\n            polygon = cv2.ellipse2Poly((int((y1 + y2) / 2), int((x1 + x2) / 2)),\n                                       (int(length / 2), 6),\n                                       int(deg),\n                                       0, 360, 1)\n            color_code_num = limb_num // 4\n            if PYTHON_VERSION == 3:\n                limb_color = list(map(lambda x: x + 35 * (limb_num % 4), joint_color_code[color_code_num]))\n            else:\n                limb_color = map(lambda x: x + 35 * (limb_num % 4), joint_color_code[color_code_num])\n\n            cv2.fillConvexPoly(test_img, polygon, color=limb_color)\n    print(\'plot limb time %f\' % (time.time() - limb_t))\n\n    if FLAGS.DEMO_TYPE == \'MULTI\':\n        upper_img = np.concatenate((demo_stage_heatmaps[0], demo_stage_heatmaps[1], demo_stage_heatmaps[2]), axis=1)\n        lower_img = np.concatenate((demo_stage_heatmaps[3], demo_stage_heatmaps[len(stage_heatmap_np) - 1], test_img),\n                                   axis=1)\n        demo_img = np.concatenate((upper_img, lower_img), axis=0)\n        return demo_img\n    else:\n        return test_img\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
demo_cpm_hand.py,24,"b'# For single hand and no body part in the picture\n# ======================================================\n\nimport tensorflow as tf\nfrom models.nets import cpm_hand_slim\nimport numpy as np\nfrom utils import cpm_utils\nimport cv2\nimport time\nimport math\nimport sys\n\n""""""Parameters\n""""""\nFLAGS = tf.app.flags.FLAGS\ntf.app.flags.DEFINE_string(\'DEMO_TYPE\',\n                           default_value=\'test_imgs/longhand.jpg\',\n                           # default_value=\'SINGLE\',\n                           docstring=\'MULTI: show multiple stage,\'\n                                     \'SINGLE: only last stage,\'\n                                     \'HM: show last stage heatmap,\'\n                                     \'paths to .jpg or .png image\')\ntf.app.flags.DEFINE_string(\'model_path\',\n                           default_value=\'models/weights/cpm_hand.pkl\',\n                           docstring=\'Your model\')\ntf.app.flags.DEFINE_integer(\'input_size\',\n                            default_value=368,\n                            docstring=\'Input image size\')\ntf.app.flags.DEFINE_integer(\'hmap_size\',\n                            default_value=46,\n                            docstring=\'Output heatmap size\')\ntf.app.flags.DEFINE_integer(\'cmap_radius\',\n                            default_value=21,\n                            docstring=\'Center map gaussian variance\')\ntf.app.flags.DEFINE_integer(\'joints\',\n                            default_value=21,\n                            docstring=\'Number of joints\')\ntf.app.flags.DEFINE_integer(\'stages\',\n                            default_value=6,\n                            docstring=\'How many CPM stages\')\ntf.app.flags.DEFINE_integer(\'cam_num\',\n                            default_value=0,\n                            docstring=\'Webcam device number\')\ntf.app.flags.DEFINE_bool(\'KALMAN_ON\',\n                         default_value=True,\n                         docstring=\'enalbe kalman filter\')\ntf.app.flags.DEFINE_float(\'kalman_noise\',\n                            default_value=3e-2,\n                            docstring=\'Kalman filter noise value\')\ntf.app.flags.DEFINE_string(\'color_channel\',\n                           default_value=\'RGB\',\n                           docstring=\'\')\n\n# Set color for each finger\njoint_color_code = [[139, 53, 255],\n                    [0, 56, 255],\n                    [43, 140, 237],\n                    [37, 168, 36],\n                    [147, 147, 0],\n                    [70, 17, 145]]\n\n\nlimbs = [[0, 1],\n         [1, 2],\n         [2, 3],\n         [3, 4],\n         [0, 5],\n         [5, 6],\n         [6, 7],\n         [7, 8],\n         [0, 9],\n         [9, 10],\n         [10, 11],\n         [11, 12],\n         [0, 13],\n         [13, 14],\n         [14, 15],\n         [15, 16],\n         [0, 17],\n         [17, 18],\n         [18, 19],\n         [19, 20]\n         ]\n\nif sys.version_info.major == 3:\n    PYTHON_VERSION = 3\nelse:\n    PYTHON_VERSION = 2\n\n\ndef main(argv):\n    tf_device = \'/gpu:0\'\n    with tf.device(tf_device):\n        """"""Build graph\n        """"""\n        if FLAGS.color_channel == \'RGB\':\n            input_data = tf.placeholder(dtype=tf.float32, shape=[None, FLAGS.input_size, FLAGS.input_size, 3],\n                                        name=\'input_image\')\n        else:\n            input_data = tf.placeholder(dtype=tf.float32, shape=[None, FLAGS.input_size, FLAGS.input_size, 1],\n                                        name=\'input_image\')\n\n        center_map = tf.placeholder(dtype=tf.float32, shape=[None, FLAGS.input_size, FLAGS.input_size, 1],\n                                    name=\'center_map\')\n\n        model = cpm_hand_slim.CPM_Model(FLAGS.stages, FLAGS.joints + 1)\n        model.build_model(input_data, center_map, 1)\n\n    saver = tf.train.Saver()\n\n    """"""Create session and restore weights\n    """"""\n    sess = tf.Session()\n\n    sess.run(tf.global_variables_initializer())\n    if FLAGS.model_path.endswith(\'pkl\'):\n        model.load_weights_from_file(FLAGS.model_path, sess, False)\n    else:\n        saver.restore(sess, FLAGS.model_path)\n\n    test_center_map = cpm_utils.gaussian_img(FLAGS.input_size, FLAGS.input_size, FLAGS.input_size / 2,\n                                             FLAGS.input_size / 2,\n                                             FLAGS.cmap_radius)\n    test_center_map = np.reshape(test_center_map, [1, FLAGS.input_size, FLAGS.input_size, 1])\n\n    # Check weights\n    for variable in tf.trainable_variables():\n        with tf.variable_scope(\'\', reuse=True):\n            var = tf.get_variable(variable.name.split(\':0\')[0])\n            print(variable.name, np.mean(sess.run(var)))\n\n    if not FLAGS.DEMO_TYPE.endswith((\'png\', \'jpg\')):\n        cam = cv2.VideoCapture(FLAGS.cam_num)\n\n    # Create kalman filters\n    if FLAGS.KALMAN_ON:\n        kalman_filter_array = [cv2.KalmanFilter(4, 2) for _ in range(FLAGS.joints)]\n        for _, joint_kalman_filter in enumerate(kalman_filter_array):\n            joint_kalman_filter.transitionMatrix = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]],\n                                                            np.float32)\n            joint_kalman_filter.measurementMatrix = np.array([[1, 0, 0, 0], [0, 1, 0, 0]], np.float32)\n            joint_kalman_filter.processNoiseCov = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]],\n                                                           np.float32) * FLAGS.kalman_noise\n    else:\n        kalman_filter_array = None\n\n    with tf.device(tf_device):\n\n        while True:\n            t1 = time.time()\n            if FLAGS.DEMO_TYPE.endswith((\'png\', \'jpg\')):\n                test_img = cpm_utils.read_image(FLAGS.DEMO_TYPE, [], FLAGS.input_size, \'IMAGE\')\n            else:\n                test_img = cpm_utils.read_image([], cam, FLAGS.input_size, \'WEBCAM\')\n\n            test_img_resize = cv2.resize(test_img, (FLAGS.input_size, FLAGS.input_size))\n            print(\'img read time %f\' % (time.time() - t1))\n\n            if FLAGS.color_channel == \'GRAY\':\n                test_img_resize = np.dot(test_img_resize[..., :3], [0.299, 0.587, 0.114]).reshape(\n                    (FLAGS.input_size, FLAGS.input_size, 1))\n                cv2.imshow(\'color\', test_img.astype(np.uint8))\n                cv2.imshow(\'gray\', test_img_resize.astype(np.uint8))\n                cv2.waitKey(1)\n\n            test_img_input = test_img_resize / 256.0 - 0.5\n            test_img_input = np.expand_dims(test_img_input, axis=0)\n\n\n            if FLAGS.DEMO_TYPE.endswith((\'png\', \'jpg\')):\n                # Inference\n                t1 = time.time()\n                predict_heatmap, stage_heatmap_np = sess.run([model.current_heatmap,\n                                                              model.stage_heatmap,\n                                                              ],\n                                                             feed_dict={\'input_image:0\': test_img_input,\n                                                                        \'center_map:0\': test_center_map})\n\n                # Show visualized image\n                demo_img = visualize_result(test_img, FLAGS, stage_heatmap_np, kalman_filter_array)\n                cv2.imshow(\'demo_img\', demo_img.astype(np.uint8))\n                if cv2.waitKey(0) == ord(\'q\'): break\n                print(\'fps: %.2f\' % (1 / (time.time() - t1)))\n            elif FLAGS.DEMO_TYPE == \'MULTI\':\n\n                # Inference\n                t1 = time.time()\n                predict_heatmap, stage_heatmap_np = sess.run([model.current_heatmap,\n                                                              model.stage_heatmap,\n                                                              ],\n                                                             feed_dict={\'input_image:0\': test_img_input,\n                                                                        \'center_map:0\': test_center_map})\n\n                # Show visualized image\n                demo_img = visualize_result(test_img, FLAGS, stage_heatmap_np, kalman_filter_array)\n                cv2.imshow(\'demo_img\', demo_img.astype(np.uint8))\n                if cv2.waitKey(1) == ord(\'q\'): break\n                print(\'fps: %.2f\' % (1 / (time.time() - t1)))\n\n\n            elif FLAGS.DEMO_TYPE == \'SINGLE\':\n\n                # Inference\n                t1 = time.time()\n                stage_heatmap_np = sess.run([model.stage_heatmap[5]],\n                                            feed_dict={\'input_image:0\': test_img_input,\n                                                       \'center_map:0\': test_center_map})\n\n                # Show visualized image\n                demo_img = visualize_result(test_img, FLAGS, stage_heatmap_np, kalman_filter_array)\n                cv2.imshow(\'current heatmap\', (demo_img).astype(np.uint8))\n                if cv2.waitKey(1) == ord(\'q\'): break\n                print(\'fps: %.2f\' % (1 / (time.time() - t1)))\n\n\n            elif FLAGS.DEMO_TYPE == \'HM\':\n\n                # Inference\n                t1 = time.time()\n                stage_heatmap_np = sess.run([model.stage_heatmap[FLAGS.stages - 1]],\n                                            feed_dict={\'input_image:0\': test_img_input,\n                                                       \'center_map:0\': test_center_map})\n                print(\'fps: %.2f\' % (1 / (time.time() - t1)))\n\n                demo_stage_heatmap = stage_heatmap_np[len(stage_heatmap_np) - 1][0, :, :, 0:FLAGS.joints].reshape(\n                    (FLAGS.hmap_size, FLAGS.hmap_size, FLAGS.joints))\n                demo_stage_heatmap = cv2.resize(demo_stage_heatmap, (FLAGS.input_size, FLAGS.input_size))\n\n                vertical_imgs = []\n                tmp_img = None\n                joint_coord_set = np.zeros((FLAGS.joints, 2))\n\n                for joint_num in range(FLAGS.joints):\n                    # Concat until 4 img\n                    if (joint_num % 4) == 0 and joint_num != 0:\n                        vertical_imgs.append(tmp_img)\n                        tmp_img = None\n\n                    demo_stage_heatmap[:, :, joint_num] *= (255 / np.max(demo_stage_heatmap[:, :, joint_num]))\n\n                    # Plot color joints\n                    if np.min(demo_stage_heatmap[:, :, joint_num]) > -50:\n                        joint_coord = np.unravel_index(np.argmax(demo_stage_heatmap[:, :, joint_num]),\n                                                       (FLAGS.input_size, FLAGS.input_size))\n                        joint_coord_set[joint_num, :] = joint_coord\n                        color_code_num = (joint_num // 4)\n\n                        if joint_num in [0, 4, 8, 12, 16]:\n                            if PYTHON_VERSION == 3:\n                                joint_color = list(\n                                    map(lambda x: x + 35 * (joint_num % 4), joint_color_code[color_code_num]))\n                            else:\n                                joint_color = map(lambda x: x + 35 * (joint_num % 4), joint_color_code[color_code_num])\n\n                            cv2.circle(test_img, center=(joint_coord[1], joint_coord[0]), radius=3, color=joint_color,\n                                       thickness=-1)\n                        else:\n                            if PYTHON_VERSION == 3:\n                                joint_color = list(\n                                    map(lambda x: x + 35 * (joint_num % 4), joint_color_code[color_code_num]))\n                            else:\n                                joint_color = map(lambda x: x + 35 * (joint_num % 4), joint_color_code[color_code_num])\n\n                            cv2.circle(test_img, center=(joint_coord[1], joint_coord[0]), radius=3, color=joint_color,\n                                       thickness=-1)\n\n                    # Put text\n                    tmp = demo_stage_heatmap[:, :, joint_num].astype(np.uint8)\n                    tmp = cv2.putText(tmp, \'Min:\' + str(np.min(demo_stage_heatmap[:, :, joint_num])),\n                                      org=(5, 20), fontFace=cv2.FONT_HERSHEY_COMPLEX, fontScale=0.3, color=150)\n                    tmp = cv2.putText(tmp, \'Mean:\' + str(np.mean(demo_stage_heatmap[:, :, joint_num])),\n                                      org=(5, 30), fontFace=cv2.FONT_HERSHEY_COMPLEX, fontScale=0.3, color=150)\n                    tmp_img = np.concatenate((tmp_img, tmp), axis=0) \\\n                        if tmp_img is not None else tmp\n\n                # Plot limbs\n                for limb_num in range(len(limbs)):\n                    if np.min(demo_stage_heatmap[:, :, limbs[limb_num][0]]) > -2000 and np.min(\n                            demo_stage_heatmap[:, :, limbs[limb_num][1]]) > -2000:\n                        x1 = joint_coord_set[limbs[limb_num][0], 0]\n                        y1 = joint_coord_set[limbs[limb_num][0], 1]\n                        x2 = joint_coord_set[limbs[limb_num][1], 0]\n                        y2 = joint_coord_set[limbs[limb_num][1], 1]\n                        length = ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5\n                        if length < 10000 and length > 5:\n                            deg = math.degrees(math.atan2(x1 - x2, y1 - y2))\n                            polygon = cv2.ellipse2Poly((int((y1 + y2) / 2), int((x1 + x2) / 2)),\n                                                       (int(length / 2), 3),\n                                                       int(deg),\n                                                       0, 360, 1)\n                            color_code_num = limb_num // 4\n                            if PYTHON_VERSION == 3:\n                                limb_color = list(\n                                    map(lambda x: x + 35 * (limb_num % 4), joint_color_code[color_code_num]))\n                            else:\n                                limb_color = map(lambda x: x + 35 * (limb_num % 4), joint_color_code[color_code_num])\n\n                            cv2.fillConvexPoly(test_img, polygon, color=limb_color)\n\n                if tmp_img is not None:\n                    tmp_img = np.lib.pad(tmp_img, ((0, vertical_imgs[0].shape[0] - tmp_img.shape[0]), (0, 0)),\n                                         \'constant\', constant_values=(0, 0))\n                    vertical_imgs.append(tmp_img)\n\n                # Concat horizontally\n                output_img = None\n                for col in range(len(vertical_imgs)):\n                    output_img = np.concatenate((output_img, vertical_imgs[col]), axis=1) if output_img is not None else \\\n                        vertical_imgs[col]\n\n                output_img = output_img.astype(np.uint8)\n                output_img = cv2.applyColorMap(output_img, cv2.COLORMAP_JET)\n                test_img = cv2.resize(test_img, (300, 300), cv2.INTER_LANCZOS4)\n                cv2.imshow(\'hm\', output_img)\n                cv2.moveWindow(\'hm\', 2000, 200)\n                cv2.imshow(\'rgb\', test_img)\n                cv2.moveWindow(\'rgb\', 2000, 750)\n                if cv2.waitKey(1) == ord(\'q\'): break\n\n\ndef visualize_result(test_img, FLAGS, stage_heatmap_np, kalman_filter_array):\n    t1 = time.time()\n    demo_stage_heatmaps = []\n    if FLAGS.DEMO_TYPE == \'MULTI\':\n        for stage in range(len(stage_heatmap_np)):\n            demo_stage_heatmap = stage_heatmap_np[stage][0, :, :, 0:FLAGS.joints].reshape(\n                (FLAGS.hmap_size, FLAGS.hmap_size, FLAGS.joints))\n            demo_stage_heatmap = cv2.resize(demo_stage_heatmap, (test_img.shape[1], test_img.shape[0]))\n            demo_stage_heatmap = np.amax(demo_stage_heatmap, axis=2)\n            demo_stage_heatmap = np.reshape(demo_stage_heatmap, (test_img.shape[1], test_img.shape[0], 1))\n            demo_stage_heatmap = np.repeat(demo_stage_heatmap, 3, axis=2)\n            demo_stage_heatmap *= 255\n            demo_stage_heatmaps.append(demo_stage_heatmap)\n\n        last_heatmap = stage_heatmap_np[len(stage_heatmap_np) - 1][0, :, :, 0:FLAGS.joints].reshape(\n            (FLAGS.hmap_size, FLAGS.hmap_size, FLAGS.joints))\n        last_heatmap = cv2.resize(last_heatmap, (test_img.shape[1], test_img.shape[0]))\n    else:\n        last_heatmap = stage_heatmap_np[len(stage_heatmap_np) - 1][0, :, :, 0:FLAGS.joints].reshape(\n            (FLAGS.hmap_size, FLAGS.hmap_size, FLAGS.joints))\n        last_heatmap = cv2.resize(last_heatmap, (test_img.shape[1], test_img.shape[0]))\n    print(\'hm resize time %f\' % (time.time() - t1))\n\n    t1 = time.time()\n    joint_coord_set = np.zeros((FLAGS.joints, 2))\n\n    # Plot joint colors\n    if kalman_filter_array is not None:\n        for joint_num in range(FLAGS.joints):\n            joint_coord = np.unravel_index(np.argmax(last_heatmap[:, :, joint_num]),\n                                           (test_img.shape[0], test_img.shape[1]))\n            joint_coord = np.array(joint_coord).reshape((2, 1)).astype(np.float32)\n            kalman_filter_array[joint_num].correct(joint_coord)\n            kalman_pred = kalman_filter_array[joint_num].predict()\n            joint_coord_set[joint_num, :] = np.array([kalman_pred[0], kalman_pred[1]]).reshape((2))\n\n            color_code_num = (joint_num // 4)\n            if joint_num in [0, 4, 8, 12, 16]:\n                if PYTHON_VERSION == 3:\n                    joint_color = list(map(lambda x: x + 35 * (joint_num % 4), joint_color_code[color_code_num]))\n                else:\n                    joint_color = map(lambda x: x + 35 * (joint_num % 4), joint_color_code[color_code_num])\n\n                cv2.circle(test_img, center=(joint_coord[1], joint_coord[0]), radius=3, color=joint_color, thickness=-1)\n            else:\n                if PYTHON_VERSION == 3:\n                    joint_color = list(map(lambda x: x + 35 * (joint_num % 4), joint_color_code[color_code_num]))\n                else:\n                    joint_color = map(lambda x: x + 35 * (joint_num % 4), joint_color_code[color_code_num])\n\n                cv2.circle(test_img, center=(joint_coord[1], joint_coord[0]), radius=3, color=joint_color, thickness=-1)\n    else:\n        for joint_num in range(FLAGS.joints):\n            joint_coord = np.unravel_index(np.argmax(last_heatmap[:, :, joint_num]),\n                                           (test_img.shape[0], test_img.shape[1]))\n            joint_coord_set[joint_num, :] = [joint_coord[0], joint_coord[1]]\n\n            color_code_num = (joint_num // 4)\n            if joint_num in [0, 4, 8, 12, 16]:\n                if PYTHON_VERSION == 3:\n                    joint_color = list(map(lambda x: x + 35 * (joint_num % 4), joint_color_code[color_code_num]))\n                else:\n                    joint_color = map(lambda x: x + 35 * (joint_num % 4), joint_color_code[color_code_num])\n\n                cv2.circle(test_img, center=(joint_coord[1], joint_coord[0]), radius=3, color=joint_color, thickness=-1)\n            else:\n                if PYTHON_VERSION == 3:\n                    joint_color = list(map(lambda x: x + 35 * (joint_num % 4), joint_color_code[color_code_num]))\n                else:\n                    joint_color = map(lambda x: x + 35 * (joint_num % 4), joint_color_code[color_code_num])\n\n                cv2.circle(test_img, center=(joint_coord[1], joint_coord[0]), radius=3, color=joint_color, thickness=-1)\n    print(\'plot joint time %f\' % (time.time() - t1))\n\n    t1 = time.time()\n    # Plot limb colors\n    for limb_num in range(len(limbs)):\n\n        x1 = joint_coord_set[limbs[limb_num][0], 0]\n        y1 = joint_coord_set[limbs[limb_num][0], 1]\n        x2 = joint_coord_set[limbs[limb_num][1], 0]\n        y2 = joint_coord_set[limbs[limb_num][1], 1]\n        length = ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5\n        if length < 150 and length > 5:\n            deg = math.degrees(math.atan2(x1 - x2, y1 - y2))\n            polygon = cv2.ellipse2Poly((int((y1 + y2) / 2), int((x1 + x2) / 2)),\n                                       (int(length / 2), 3),\n                                       int(deg),\n                                       0, 360, 1)\n            color_code_num = limb_num // 4\n            if PYTHON_VERSION == 3:\n                limb_color = list(map(lambda x: x + 35 * (limb_num % 4), joint_color_code[color_code_num]))\n            else:\n                limb_color = map(lambda x: x + 35 * (limb_num % 4), joint_color_code[color_code_num])\n\n            cv2.fillConvexPoly(test_img, polygon, color=limb_color)\n    print(\'plot limb time %f\' % (time.time() - t1))\n\n    if FLAGS.DEMO_TYPE == \'MULTI\':\n        upper_img = np.concatenate((demo_stage_heatmaps[0], demo_stage_heatmaps[1], demo_stage_heatmaps[2]), axis=1)\n        lower_img = np.concatenate((demo_stage_heatmaps[3], demo_stage_heatmaps[len(stage_heatmap_np) - 1], test_img),\n                                   axis=1)\n        demo_img = np.concatenate((upper_img, lower_img), axis=0)\n        return demo_img\n    else:\n        return test_img\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
hnm.py,3,"b""import tensorflow as tf\nimport numpy as np\nfrom utils import cpm_utils\nimport cv2\nimport time\nimport math\nimport importlib\nimport sys\nimport os\nimport json\nfrom Tim_utils import utils\nimport Ensemble_data_generator\n\n\nfrom config import FLAGS\ncpm_model = importlib.import_module('models.nets.'+FLAGS.network_def)\n\nif sys.version_info.major == 3:\n    PYTHON_VERSION = 3\nelse:\n    PYTHON_VERSION = 2\n\n\ndef scale_square_data(img, points, box_size):\n    # Resize and pad image to fit output image size\n    output_image = np.ones(shape=(box_size, box_size, 3)) * 128.0\n    img_h = img.shape[0]\n    img_w = img.shape[1]\n    if img_h > img_w:\n        scale = box_size / (img_h * 1.0)\n\n        # Relocalize points\n        points[:, 0] *= scale\n        points[:, 1] *= scale\n\n        # Resize image\n        image = cv2.resize(img, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_LANCZOS4)\n        resized_img_h, resized_img_w = image.shape[0], image.shape[1]\n\n        offset = resized_img_w % 2\n\n        output_image[:, int(box_size / 2 - math.floor(resized_img_w / 2)): int(\n            box_size / 2 + math.floor(resized_img_w / 2) + offset), :] = image\n        points[:, 0] += (box_size / 2 - math.floor(resized_img_w / 2))\n\n    else:\n        scale = box_size / (img_w * 1.0)\n\n        # Relocalize points\n        points[:, 0] *= scale\n        points[:, 1] *= scale\n\n        # Resize image\n        image = cv2.resize(img, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_LANCZOS4)\n        resized_img_h, resized_img_w = image.shape[0], image.shape[1]\n\n        offset = resized_img_h % 2\n\n        output_image[int(box_size / 2 - math.floor(resized_img_h / 2)): int(\n            box_size / 2 + math.floor(resized_img_h / 2) + offset), :, :] = image\n        points[:, 1] += (box_size / 2 - math.floor(resized_img_h / 2))\n\n    return output_image, points\n\n\ndef main():\n    model = cpm_model.CPM_Model(input_size=FLAGS.input_size,\n                                heatmap_size=FLAGS.heatmap_size,\n                                stages=FLAGS.cpm_stages,\n                                joints=FLAGS.num_of_joints,\n                                img_type=FLAGS.color_channel,\n                                is_training=False)\n    model.build_loss(FLAGS.init_lr, FLAGS.lr_decay_rate, FLAGS.lr_decay_step, optimizer='RMSProp')\n    saver = tf.train.Saver()\n\n    g = Ensemble_data_generator.ensemble_data_generator(FLAGS.train_img_dir,\n                                                        None,\n                                                        FLAGS.batch_size, FLAGS.input_size, True, False,\n                                                        FLAGS.augmentation_config, False)\n\n    device_count = {'GPU': 1} if FLAGS.use_gpu else {'GPU': 0}\n    sess_config = tf.ConfigProto(device_count=device_count)\n    sess_config.gpu_options.per_process_gpu_memory_fraction = 0.5\n    sess_config.gpu_options.allow_growth = True\n    sess_config.allow_soft_placement = True\n    with tf.Session(config=sess_config) as sess:\n\n        model_path_suffix = os.path.join(FLAGS.network_def,\n                                         'input_{}_output_{}'.format(FLAGS.input_size, FLAGS.heatmap_size),\n                                         'joints_{}'.format(FLAGS.num_of_joints),\n                                         'stages_{}'.format(FLAGS.cpm_stages),\n                                         'init_{}_rate_{}_step_{}'.format(FLAGS.init_lr, FLAGS.lr_decay_rate,\n                                                                          FLAGS.lr_decay_step)\n                                         )\n        model_save_dir = os.path.join('models',\n                                      'weights',\n                                      model_path_suffix)\n        print('Load model from [{}]'.format(os.path.join(model_save_dir, FLAGS.model_path)))\n        if FLAGS.model_path.endswith('pkl'):\n            model.load_weights_from_file(FLAGS.model_path, sess, False)\n        else:\n            saver.restore(sess, os.path.join(model_save_dir, FLAGS.model_path))\n        print('Load model done')\n\n\n        bbox_offset = 100\n        for person_dir in os.listdir(FLAGS.train_img_dir):\n            json_file_path = os.path.join(FLAGS.train_img_dir, person_dir, 'attr_data.json')\n            hnm_json_list = [[] for _ in range(11)]\n\n            with open(json_file_path, 'r') as f:\n                json_file = json.load(f)\n\n            loss_cnt = 0\n            img_cnt = 0\n            hnm_cnt = 0\n            for cam_id in range(11):\n                for img_id in range(len(json_file[cam_id])):\n                    img_path = os.path.join(FLAGS.train_img_dir,\n                                            person_dir,\n                                            'undistorted_img',\n                                            json_file[cam_id][img_id]['name'])\n                    img = cv2.imread(img_path)\n\n                    # Read joints\n                    hand_2d_joints = np.zeros(shape=(21, 2))\n                    bbox = json_file[cam_id][img_id]['bbox']\n                    bbox[0] = max(bbox[0] - bbox_offset, 0)\n                    bbox[1] = max(bbox[1] - bbox_offset, 0)\n                    bbox[2] = min(bbox[2] + bbox_offset, img.shape[0])\n                    bbox[3] = min(bbox[3] + bbox_offset, img.shape[1])\n                    img = img[bbox[1]:bbox[3],\n                          bbox[0]:bbox[2]]\n\n                    for i, finger_name in enumerate(['thumb', 'index', 'middle', 'ring', 'pinky']):\n                        for j, joint_name in enumerate(['tip', 'dip', 'pip', 'mcp']):\n                            hand_2d_joints[i * 4 + j, :] = \\\n                            json_file[cam_id][img_id][finger_name][joint_name]['pose2']\n                    hand_2d_joints[20, :] = json_file[cam_id][img_id]['wrist']['pose2']\n                    hand_2d_joints[:, 0] -= bbox[0]\n                    hand_2d_joints[:, 1] -= bbox[1]\n\n                    # for i in range(hand_2d_joints.shape[0]):\n                    #     cv2.circle(img, (int(hand_2d_joints[i][0]), int(hand_2d_joints[i][1])), 5, (0, 255, 0), -1)\n                    # print(img_path)\n                    img = img / 255.0 - 0.5\n\n                    img, hand_2d_joints = scale_square_data(img, hand_2d_joints, FLAGS.input_size)\n                    # for i in range(hand_2d_joints.shape[0]):\n                    #     cv2.circle(img, (int(hand_2d_joints[i][0]), int(hand_2d_joints[i][1])), 5, (0, 255, 0), -1)\n                    # cv2.imshow('', img)\n                    # cv2.waitKey(0)\n\n                    img = np.expand_dims(img, axis=0)\n                    hand_2d_joints = np.expand_dims(hand_2d_joints, axis=0)\n\n                    gt_heatmap_np = cpm_utils.make_heatmaps_from_joints(FLAGS.input_size,\n                                                                              FLAGS.heatmap_size,\n                                                                              FLAGS.joint_gaussian_variance,\n                                                                              hand_2d_joints)\n\n\n                    loss, = sess.run([model.total_loss], feed_dict={model.input_images: img,\n                                                                    model.gt_hmap_placeholder: gt_heatmap_np})\n\n                    # loss_cnt += loss\n                    img_cnt += 1\n                    # print(img_path, float(loss_cnt)/ img_cnt)\n\n                    if loss > 150.0:\n                        hnm_json_list[cam_id].append(json_file[cam_id][img_id])\n                        hnm_cnt += 1\n                        print('hnm cnt {} / {}'.format(hnm_cnt, img_cnt))\n\n            with open(os.path.join(FLAGS.train_img_dir, person_dir, 'attr_data_hnm.json'), 'wb') as f:\n                json.dump(hnm_json_list, f)\n                print('write done with {}'.format(person_dir))\n\n\nif __name__ == '__main__':\n    main()\n"""
run_demo_hand_with_tracker.py,8,"b'import tensorflow as tf\nimport numpy as np\nfrom utils import cpm_utils, tracking_module, utils\nimport cv2\nimport time\nimport math\nimport importlib\nimport os\n\nfrom config import FLAGS\n\ncpm_model = importlib.import_module(\'models.nets.\' + FLAGS.network_def)\n\njoint_detections = np.zeros(shape=(21, 2))\n\n\ndef main(argv):\n    global joint_detections\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = str(FLAGS.gpu_id)\n\n    """""" Initial tracker\n    """"""\n    tracker = tracking_module.SelfTracker([FLAGS.webcam_height, FLAGS.webcam_width], FLAGS.input_size)\n\n    """""" Build network graph\n    """"""\n    model = cpm_model.CPM_Model(input_size=FLAGS.input_size,\n                                heatmap_size=FLAGS.heatmap_size,\n                                stages=FLAGS.cpm_stages,\n                                joints=FLAGS.num_of_joints,\n                                img_type=FLAGS.color_channel,\n                                is_training=False)\n    saver = tf.train.Saver()\n\n    """""" Get output node\n    """"""\n    output_node = tf.get_default_graph().get_tensor_by_name(name=FLAGS.output_node_names)\n\n    device_count = {\'GPU\': 1} if FLAGS.use_gpu else {\'GPU\': 0}\n    sess_config = tf.ConfigProto(device_count=device_count)\n    sess_config.gpu_options.per_process_gpu_memory_fraction = 0.2\n    sess_config.gpu_options.allow_growth = True\n    sess_config.allow_soft_placement = True\n    with tf.Session(config=sess_config) as sess:\n\n        model_path_suffix = os.path.join(FLAGS.network_def,\n                                         \'input_{}_output_{}\'.format(FLAGS.input_size, FLAGS.heatmap_size),\n                                         \'joints_{}\'.format(FLAGS.num_of_joints),\n                                         \'stages_{}\'.format(FLAGS.cpm_stages),\n                                         \'init_{}_rate_{}_step_{}\'.format(FLAGS.init_lr, FLAGS.lr_decay_rate,\n                                                                          FLAGS.lr_decay_step)\n                                         )\n        model_save_dir = os.path.join(\'models\',\n                                      \'weights\',\n                                      model_path_suffix)\n        print(\'Load model from [{}]\'.format(os.path.join(model_save_dir, FLAGS.model_path)))\n        if FLAGS.model_path.endswith(\'pkl\'):\n            model.load_weights_from_file(FLAGS.model_path, sess, False)\n        else:\n            saver.restore(sess, \'models/weights/cpm_hand\')\n\n        # Check weights\n        for variable in tf.global_variables():\n            with tf.variable_scope(\'\', reuse=True):\n                var = tf.get_variable(variable.name.split(\':0\')[0])\n                print(variable.name, np.mean(sess.run(var)))\n\n        # Create webcam instance\n        if FLAGS.DEMO_TYPE in [\'MULTI\', \'SINGLE\', \'Joint_HM\']:\n            cam = cv2.VideoCapture(FLAGS.cam_id)\n\n        # Create kalman filters\n        if FLAGS.use_kalman:\n            kalman_filter_array = [cv2.KalmanFilter(4, 2) for _ in range(FLAGS.num_of_joints)]\n            for _, joint_kalman_filter in enumerate(kalman_filter_array):\n                joint_kalman_filter.transitionMatrix = np.array(\n                    [[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]],\n                    np.float32)\n                joint_kalman_filter.measurementMatrix = np.array([[1, 0, 0, 0], [0, 1, 0, 0]], np.float32)\n                joint_kalman_filter.processNoiseCov = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]],\n                                                               np.float32) * FLAGS.kalman_noise\n        else:\n            kalman_filter_array = None\n\n        if FLAGS.DEMO_TYPE.endswith((\'png\', \'jpg\')):\n            test_img = cpm_utils.read_image(FLAGS.DEMO_TYPE, [], FLAGS.input_size, \'IMAGE\')\n            test_img_resize = cv2.resize(test_img, (FLAGS.input_size, FLAGS.input_size))\n\n            test_img_input = normalize_and_centralize_img(test_img_resize)\n\n            t1 = time.time()\n            predict_heatmap, stage_heatmap_np = sess.run([model.current_heatmap,\n                                                          output_node,\n                                                          ],\n                                                         feed_dict={model.input_images: test_img_input}\n                                                         )\n            print(\'fps: %.2f\' % (1 / (time.time() - t1)))\n\n            correct_and_draw_hand(test_img,\n                                  cv2.resize(stage_heatmap_np[0], (FLAGS.input_size, FLAGS.input_size)),\n                                  kalman_filter_array, tracker, tracker.input_crop_ratio, test_img)\n\n            # Show visualized image\n            # demo_img = visualize_result(test_img, stage_heatmap_np, kalman_filter_array)\n            cv2.imshow(\'demo_img\', test_img.astype(np.uint8))\n            cv2.waitKey(0)\n\n        elif FLAGS.DEMO_TYPE in [\'SINGLE\', \'MULTI\']:\n            while True:\n                # # Prepare input image\n                _, full_img = cam.read()\n\n                test_img = tracker.tracking_by_joints(full_img, joint_detections=joint_detections)\n                crop_full_scale = tracker.input_crop_ratio\n                test_img_copy = test_img.copy()\n\n                # White balance\n                test_img_wb = utils.img_white_balance(test_img, 5)\n                test_img_input = normalize_and_centralize_img(test_img_wb)\n\n                # Inference\n                t1 = time.time()\n                stage_heatmap_np = sess.run([output_node],\n                                            feed_dict={model.input_images: test_img_input})\n                print(\'FPS: %.2f\' % (1 / (time.time() - t1)))\n\n                local_img = visualize_result(full_img, stage_heatmap_np, kalman_filter_array, tracker, crop_full_scale,\n                                             test_img_copy)\n\n                cv2.imshow(\'local_img\', local_img.astype(np.uint8))\n                cv2.imshow(\'global_img\', full_img.astype(np.uint8))\n                if cv2.waitKey(1) == ord(\'q\'): break\n\n\n        elif FLAGS.DEMO_TYPE == \'Joint_HM\':\n            while True:\n                # Prepare input image\n                test_img = cpm_utils.read_image([], cam, FLAGS.input_size, \'WEBCAM\')\n                test_img_resize = cv2.resize(test_img, (FLAGS.input_size, FLAGS.input_size))\n\n                test_img_input = normalize_and_centralize_img(test_img_resize)\n\n                # Inference\n                t1 = time.time()\n                stage_heatmap_np = sess.run([output_node],\n                                            feed_dict={model.input_images: test_img_input})\n                print(\'FPS: %.2f\' % (1 / (time.time() - t1)))\n\n                demo_stage_heatmap = stage_heatmap_np[len(stage_heatmap_np) - 1][0, :, :,\n                                     0:FLAGS.num_of_joints].reshape(\n                    (FLAGS.heatmap_size, FLAGS.heatmap_size, FLAGS.num_of_joints))\n                demo_stage_heatmap = cv2.resize(demo_stage_heatmap, (FLAGS.input_size, FLAGS.input_size))\n\n                vertical_imgs = []\n                tmp_img = None\n                joint_coord_set = np.zeros((FLAGS.num_of_joints, 2))\n\n                for joint_num in range(FLAGS.num_of_joints):\n                    # Concat until 4 img\n                    if (joint_num % 4) == 0 and joint_num != 0:\n                        vertical_imgs.append(tmp_img)\n                        tmp_img = None\n\n                    demo_stage_heatmap[:, :, joint_num] *= (255 / np.max(demo_stage_heatmap[:, :, joint_num]))\n\n                    # Plot color joints\n                    if np.min(demo_stage_heatmap[:, :, joint_num]) > -50:\n                        joint_coord = np.unravel_index(np.argmax(demo_stage_heatmap[:, :, joint_num]),\n                                                       (FLAGS.input_size, FLAGS.input_size))\n                        joint_coord_set[joint_num, :] = joint_coord\n                        color_code_num = (joint_num // 4)\n\n                        if joint_num in [0, 4, 8, 12, 16]:\n                            joint_color = list(\n                                map(lambda x: x + 35 * (joint_num % 4), FLAGS.joint_color_code[color_code_num]))\n                            cv2.circle(test_img, center=(joint_coord[1], joint_coord[0]), radius=3, color=joint_color,\n                                       thickness=-1)\n                        else:\n                            joint_color = list(\n                                map(lambda x: x + 35 * (joint_num % 4), FLAGS.joint_color_code[color_code_num]))\n                            cv2.circle(test_img, center=(joint_coord[1], joint_coord[0]), radius=3, color=joint_color,\n                                       thickness=-1)\n\n                    # Put text\n                    tmp = demo_stage_heatmap[:, :, joint_num].astype(np.uint8)\n                    tmp = cv2.putText(tmp, \'Min:\' + str(np.min(demo_stage_heatmap[:, :, joint_num])),\n                                      org=(5, 20), fontFace=cv2.FONT_HERSHEY_COMPLEX, fontScale=0.3, color=150)\n                    tmp = cv2.putText(tmp, \'Mean:\' + str(np.mean(demo_stage_heatmap[:, :, joint_num])),\n                                      org=(5, 30), fontFace=cv2.FONT_HERSHEY_COMPLEX, fontScale=0.3, color=150)\n                    tmp_img = np.concatenate((tmp_img, tmp), axis=0) \\\n                        if tmp_img is not None else tmp\n\n                # Plot FLAGS.limbs\n                for limb_num in range(len(FLAGS.limbs)):\n                    if np.min(demo_stage_heatmap[:, :, FLAGS.limbs[limb_num][0]]) > -2000 and np.min(\n                            demo_stage_heatmap[:, :, FLAGS.limbs[limb_num][1]]) > -2000:\n                        x1 = joint_coord_set[FLAGS.limbs[limb_num][0], 0]\n                        y1 = joint_coord_set[FLAGS.limbs[limb_num][0], 1]\n                        x2 = joint_coord_set[FLAGS.limbs[limb_num][1], 0]\n                        y2 = joint_coord_set[FLAGS.limbs[limb_num][1], 1]\n                        length = ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5\n                        if length < 10000 and length > 5:\n                            deg = math.degrees(math.atan2(x1 - x2, y1 - y2))\n                            polygon = cv2.ellipse2Poly((int((y1 + y2) / 2), int((x1 + x2) / 2)),\n                                                       (int(length / 2), 3),\n                                                       int(deg),\n                                                       0, 360, 1)\n                            color_code_num = limb_num // 4\n                            limb_color = list(\n                                map(lambda x: x + 35 * (limb_num % 4), FLAGS.joint_color_code[color_code_num]))\n\n                            cv2.fillConvexPoly(test_img, polygon, color=limb_color)\n\n                if tmp_img is not None:\n                    tmp_img = np.lib.pad(tmp_img, ((0, vertical_imgs[0].shape[0] - tmp_img.shape[0]), (0, 0)),\n                                         \'constant\', constant_values=(0, 0))\n                    vertical_imgs.append(tmp_img)\n\n                # Concat horizontally\n                output_img = None\n                for col in range(len(vertical_imgs)):\n                    output_img = np.concatenate((output_img, vertical_imgs[col]), axis=1) if output_img is not None else \\\n                        vertical_imgs[col]\n\n                output_img = output_img.astype(np.uint8)\n                output_img = cv2.applyColorMap(output_img, cv2.COLORMAP_JET)\n                test_img = cv2.resize(test_img, (300, 300), cv2.INTER_LANCZOS4)\n                cv2.imshow(\'hm\', output_img)\n                cv2.moveWindow(\'hm\', 2000, 200)\n                cv2.imshow(\'rgb\', test_img)\n                cv2.moveWindow(\'rgb\', 2000, 750)\n                if cv2.waitKey(1) == ord(\'q\'): break\n\n\ndef normalize_and_centralize_img(img):\n    if FLAGS.color_channel == \'GRAY\':\n        img = np.dot(img[..., :3], [0.299, 0.587, 0.114]).reshape((FLAGS.input_size, FLAGS.input_size, 1))\n\n    if FLAGS.normalize_img:\n        test_img_input = img / 256.0 - 0.5\n        test_img_input = np.expand_dims(test_img_input, axis=0)\n    else:\n        test_img_input = img - 128.0\n        test_img_input = np.expand_dims(test_img_input, axis=0)\n    return test_img_input\n\n\ndef visualize_result(test_img, stage_heatmap_np, kalman_filter_array, tracker, crop_full_scale, crop_img):\n    demo_stage_heatmaps = []\n    if FLAGS.DEMO_TYPE == \'MULTI\':\n        for stage in range(len(stage_heatmap_np)):\n            demo_stage_heatmap = stage_heatmap_np[stage][0, :, :, 0:FLAGS.num_of_joints].reshape(\n                (FLAGS.heatmap_size, FLAGS.heatmap_size, FLAGS.num_of_joints))\n            demo_stage_heatmap = cv2.resize(demo_stage_heatmap, (FLAGS.input_size, FLAGS.input_size))\n            demo_stage_heatmap = np.amax(demo_stage_heatmap, axis=2)\n            demo_stage_heatmap = np.reshape(demo_stage_heatmap, (FLAGS.input_size, FLAGS.input_size, 1))\n            demo_stage_heatmap = np.repeat(demo_stage_heatmap, 3, axis=2)\n            demo_stage_heatmap *= 255\n            demo_stage_heatmaps.append(demo_stage_heatmap)\n\n        last_heatmap = stage_heatmap_np[len(stage_heatmap_np) - 1][0, :, :, 0:FLAGS.num_of_joints].reshape(\n            (FLAGS.heatmap_size, FLAGS.heatmap_size, FLAGS.num_of_joints))\n        last_heatmap = cv2.resize(last_heatmap, (FLAGS.input_size, FLAGS.input_size))\n    else:\n        last_heatmap = stage_heatmap_np[len(stage_heatmap_np) - 1][0, :, :, 0:FLAGS.num_of_joints].reshape(\n            (FLAGS.heatmap_size, FLAGS.heatmap_size, FLAGS.num_of_joints))\n        last_heatmap = cv2.resize(last_heatmap, (FLAGS.input_size, FLAGS.input_size))\n\n    correct_and_draw_hand(test_img, last_heatmap, kalman_filter_array, tracker, crop_full_scale, crop_img)\n\n    if FLAGS.DEMO_TYPE == \'MULTI\':\n        if len(demo_stage_heatmaps) > 3:\n            upper_img = np.concatenate((demo_stage_heatmaps[0], demo_stage_heatmaps[1], demo_stage_heatmaps[2]), axis=1)\n            lower_img = np.concatenate(\n                (demo_stage_heatmaps[3], demo_stage_heatmaps[len(stage_heatmap_np) - 1], crop_img),\n                axis=1)\n            demo_img = np.concatenate((upper_img, lower_img), axis=0)\n            return demo_img\n        else:\n            return np.concatenate((demo_stage_heatmaps[0], demo_stage_heatmaps[len(stage_heatmap_np) - 1], crop_img),\n                                  axis=1)\n\n    else:\n        return crop_img\n\n\ndef correct_and_draw_hand(full_img, stage_heatmap_np, kalman_filter_array, tracker, crop_full_scale, crop_img):\n    global joint_detections\n    joint_coord_set = np.zeros((FLAGS.num_of_joints, 2))\n    local_joint_coord_set = np.zeros((FLAGS.num_of_joints, 2))\n\n    mean_response_val = 0.0\n\n    # Plot joint colors\n    if kalman_filter_array is not None:\n        for joint_num in range(FLAGS.num_of_joints):\n            tmp_heatmap = stage_heatmap_np[:, :, joint_num]\n            joint_coord = np.unravel_index(np.argmax(tmp_heatmap),\n                                           (FLAGS.input_size, FLAGS.input_size))\n            mean_response_val += tmp_heatmap[joint_coord[0], joint_coord[1]]\n            joint_coord = np.array(joint_coord).reshape((2, 1)).astype(np.float32)\n            kalman_filter_array[joint_num].correct(joint_coord)\n            kalman_pred = kalman_filter_array[joint_num].predict()\n            correct_coord = np.array([kalman_pred[0], kalman_pred[1]]).reshape((2))\n            local_joint_coord_set[joint_num, :] = correct_coord\n\n            # Resize back\n            correct_coord /= crop_full_scale\n\n            # Substract padding border\n            correct_coord[0] -= (tracker.pad_boundary[0] / crop_full_scale)\n            correct_coord[1] -= (tracker.pad_boundary[2] / crop_full_scale)\n            correct_coord[0] += tracker.bbox[0]\n            correct_coord[1] += tracker.bbox[2]\n            joint_coord_set[joint_num, :] = correct_coord\n\n    else:\n        for joint_num in range(FLAGS.num_of_joints):\n            tmp_heatmap = stage_heatmap_np[:, :, joint_num]\n            joint_coord = np.unravel_index(np.argmax(tmp_heatmap),\n                                           (FLAGS.input_size, FLAGS.input_size))\n            mean_response_val += tmp_heatmap[joint_coord[0], joint_coord[1]]\n            joint_coord = np.array(joint_coord).astype(np.float32)\n\n            local_joint_coord_set[joint_num, :] = joint_coord\n\n            # Resize back\n            joint_coord /= crop_full_scale\n\n            # Substract padding border\n            joint_coord[0] -= (tracker.pad_boundary[2] / crop_full_scale)\n            joint_coord[1] -= (tracker.pad_boundary[0] / crop_full_scale)\n            joint_coord[0] += tracker.bbox[0]\n            joint_coord[1] += tracker.bbox[2]\n            joint_coord_set[joint_num, :] = joint_coord\n\n    draw_hand(full_img, joint_coord_set, tracker.loss_track)\n    draw_hand(crop_img, local_joint_coord_set, tracker.loss_track)\n    joint_detections = joint_coord_set\n\n    if mean_response_val >= 1:\n        tracker.loss_track = False\n    else:\n        tracker.loss_track = True\n\n    cv2.putText(full_img, \'Response: {:<.3f}\'.format(mean_response_val),\n                org=(20, 20), fontFace=cv2.FONT_HERSHEY_PLAIN, fontScale=1, color=(255, 0, 0))\n\n\ndef draw_hand(full_img, joint_coords, is_loss_track):\n    if is_loss_track:\n        joint_coords = FLAGS.default_hand\n\n    # Plot joints\n    for joint_num in range(FLAGS.num_of_joints):\n        color_code_num = (joint_num // 4)\n        if joint_num in [0, 4, 8, 12, 16]:\n            joint_color = list(map(lambda x: x + 35 * (joint_num % 4), FLAGS.joint_color_code[color_code_num]))\n            cv2.circle(full_img, center=(int(joint_coords[joint_num][1]), int(joint_coords[joint_num][0])), radius=3,\n                       color=joint_color, thickness=-1)\n        else:\n            joint_color = list(map(lambda x: x + 35 * (joint_num % 4), FLAGS.joint_color_code[color_code_num]))\n            cv2.circle(full_img, center=(int(joint_coords[joint_num][1]), int(joint_coords[joint_num][0])), radius=3,\n                       color=joint_color, thickness=-1)\n\n    # Plot limbs\n    for limb_num in range(len(FLAGS.limbs)):\n        x1 = int(joint_coords[int(FLAGS.limbs[limb_num][0])][0])\n        y1 = int(joint_coords[int(FLAGS.limbs[limb_num][0])][1])\n        x2 = int(joint_coords[int(FLAGS.limbs[limb_num][1])][0])\n        y2 = int(joint_coords[int(FLAGS.limbs[limb_num][1])][1])\n        length = ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5\n        if length < 150 and length > 5:\n            deg = math.degrees(math.atan2(x1 - x2, y1 - y2))\n            polygon = cv2.ellipse2Poly((int((y1 + y2) / 2), int((x1 + x2) / 2)),\n                                       (int(length / 2), 3),\n                                       int(deg),\n                                       0, 360, 1)\n            color_code_num = limb_num // 4\n            limb_color = list(map(lambda x: x + 35 * (limb_num % 4), FLAGS.joint_color_code[color_code_num]))\n            cv2.fillConvexPoly(full_img, polygon, color=limb_color)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
run_freeze_graph.py,2,"b""import tensorflow as tf\nimport os\nfrom tensorflow.python.tools.freeze_graph import freeze_graph\n\nfrom config import FLAGS\n\n\ndef freeze_model():\n    model_path_suffix = os.path.join(FLAGS.network_def,\n                                     'input_{}_output_{}'.format(FLAGS.input_size, FLAGS.heatmap_size),\n                                     'joints_{}'.format(FLAGS.num_of_joints),\n                                     'stages_{}'.format(FLAGS.cpm_stages),\n                                     'init_{}_rate_{}_step_{}'.format(FLAGS.init_lr, FLAGS.lr_decay_rate,\n                                                                      FLAGS.lr_decay_step)\n                                     )\n    model_save_dir = os.path.join('models',\n                                  'weights',\n                                  model_path_suffix)\n    model_path = os.path.join(model_save_dir, FLAGS.model_path)\n    model_path = 'models/weights/cpm_hand'\n\n    # Load graph and dump to protobuf\n    meta_graph = tf.train.import_meta_graph(model_path + '.meta')\n    tf.train.write_graph(tf.get_default_graph(), 'frozen_models/', 'graph_proto.pb')\n\n    output_graph_path = os.path.join('frozen_models', '{}_frozen.pb'.format('cpm_hand'))\n    freeze_graph(input_graph='frozen_models/graph_proto.pb',\n                 input_saver='',\n                 input_checkpoint=model_path,\n                 output_graph=output_graph_path,\n                 output_node_names=FLAGS.output_node_names,\n                 restore_op_name='save/restore_all',\n                 clear_devices=True,\n                 initializer_nodes='',\n                 variable_names_blacklist='',\n                 input_binary=False,\n                 filename_tensor_name='save/Const:0')\n\n\nif __name__ == '__main__':\n    freeze_model()\n"""
run_tensorboard.py,0,"b'import os\nimport argparse\n\nfrom config import FLAGS\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--train\', default=False, action=\'store_true\')\nparser.add_argument(\'--test\', default=False, action=\'store_true\')\nargs = parser.parse_args()\n\nif __name__ == \'__main__\':\n\n    """""" Create dirs for saving models and logs\n    """"""\n    if args.train and args.test:\n        raise ValueError(\'Can\\\'t open train and test log and same time.\')\n    elif args.train:\n        log_type = \'train\'\n    else:\n        log_type = \'test\'\n    log_path = os.path.join(\'models\',\n                            \'logs\',\n                            FLAGS.network_def,\n                            \'input_{}_output_{}\'.format(FLAGS.input_size, FLAGS.heatmap_size),\n                            \'joints_{}\'.format(FLAGS.num_of_joints),\n                            \'stages_{}\'.format(FLAGS.cpm_stages),\n                            \'init_{}_rate_{}_step_{}\'.format(FLAGS.init_lr, FLAGS.lr_decay_rate, FLAGS.lr_decay_step),\n                            log_type\n                            )\n    print(\'Show tensorboard log in [{}]\'.format(log_path))\n    os.system(\'tensorboard --logdir={}\'.format(log_path))\n'"
run_training.py,13,"b'import tensorflow as tf\nimport numpy as np\nimport cv2\nimport os\nimport importlib\nimport time\n\nfrom utils import cpm_utils\nfrom config import FLAGS\nimport Ensemble_data_generator\n\ncpm_model = importlib.import_module(\'models.nets.\' + FLAGS.network_def)\n\n\ndef main(argv):\n    """"""\n\n    :param argv:\n    :return:\n    """"""\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'0\'\n\n    """""" Create dirs for saving models and logs\n    """"""\n    model_path_suffix = os.path.join(FLAGS.network_def,\n                                     \'input_{}_output_{}\'.format(FLAGS.input_size, FLAGS.heatmap_size),\n                                     \'joints_{}\'.format(FLAGS.num_of_joints),\n                                     \'stages_{}\'.format(FLAGS.cpm_stages),\n                                     \'init_{}_rate_{}_step_{}\'.format(FLAGS.init_lr, FLAGS.lr_decay_rate,\n                                                                      FLAGS.lr_decay_step)\n                                     )\n    model_save_dir = os.path.join(\'models\',\n                                  \'weights\',\n                                  model_path_suffix)\n    train_log_save_dir = os.path.join(\'models\',\n                                      \'logs\',\n                                      model_path_suffix,\n                                      \'train\')\n    test_log_save_dir = os.path.join(\'models\',\n                                     \'logs\',\n                                     model_path_suffix,\n                                     \'test\')\n    os.system(\'mkdir -p {}\'.format(model_save_dir))\n    os.system(\'mkdir -p {}\'.format(train_log_save_dir))\n    os.system(\'mkdir -p {}\'.format(test_log_save_dir))\n\n    """""" Create data generator\n    """"""\n    g = Ensemble_data_generator.ensemble_data_generator(FLAGS.train_img_dir,\n                                                        FLAGS.bg_img_dir,\n                                                        FLAGS.batch_size, FLAGS.input_size, True, True,\n                                                        FLAGS.augmentation_config, FLAGS.hnm, FLAGS.do_cropping)\n    g_eval = Ensemble_data_generator.ensemble_data_generator(FLAGS.val_img_dir,\n                                                             FLAGS.bg_img_dir,\n                                                             FLAGS.batch_size, FLAGS.input_size, True, True,\n                                                             FLAGS.augmentation_config, FLAGS.hnm, FLAGS.do_cropping)\n\n    """""" Build network graph\n    """"""\n    model = cpm_model.CPM_Model(input_size=FLAGS.input_size,\n                                heatmap_size=FLAGS.heatmap_size,\n                                stages=FLAGS.cpm_stages,\n                                joints=FLAGS.num_of_joints,\n                                img_type=FLAGS.color_channel,\n                                is_training=True)\n    model.build_loss(FLAGS.init_lr, FLAGS.lr_decay_rate, FLAGS.lr_decay_step, optimizer=\'RMSProp\')\n    print(\'=====Model Build=====\\n\')\n\n    merged_summary = tf.summary.merge_all()\n\n    """""" Training\n    """"""\n    device_count = {\'GPU\': 1} if FLAGS.use_gpu else {\'GPU\': 0}\n    with tf.Session(config=tf.ConfigProto(device_count=device_count,\n                                          allow_soft_placement=True)) as sess:\n        # Create tensorboard\n        train_writer = tf.summary.FileWriter(train_log_save_dir, sess.graph)\n        test_writer = tf.summary.FileWriter(test_log_save_dir, sess.graph)\n\n        # Create model saver\n        saver = tf.train.Saver(max_to_keep=None)\n\n        # Init all vars\n        init_op = tf.global_variables_initializer()\n        sess.run(init_op)\n\n        # Restore pretrained weights\n        if FLAGS.pretrained_model != \'\':\n            if FLAGS.pretrained_model.endswith(\'.pkl\'):\n                model.load_weights_from_file(FLAGS.pretrained_model, sess, finetune=True)\n\n                # Check weights\n                for variable in tf.trainable_variables():\n                    with tf.variable_scope(\'\', reuse=True):\n                        var = tf.get_variable(variable.name.split(\':0\')[0])\n                        print(variable.name, np.mean(sess.run(var)))\n\n            else:\n                saver.restore(sess, os.path.join(model_save_dir, FLAGS.pretrained_model))\n\n                # check weights\n                for variable in tf.trainable_variables():\n                    with tf.variable_scope(\'\', reuse=True):\n                        var = tf.get_variable(variable.name.split(\':0\')[0])\n                        print(variable.name, np.mean(sess.run(var)))\n\n        for training_itr in range(FLAGS.training_iters):\n            t1 = time.time()\n\n            # Read one batch data\n            batch_x_np, batch_joints_np = g.next()\n\n            if FLAGS.normalize_img:\n                # Normalize images\n                batch_x_np = batch_x_np / 255.0 - 0.5\n            else:\n                batch_x_np -= 128.0\n\n            # Generate heatmaps from joints\n            batch_gt_heatmap_np = cpm_utils.make_heatmaps_from_joints(FLAGS.input_size,\n                                                                      FLAGS.heatmap_size,\n                                                                      FLAGS.joint_gaussian_variance,\n                                                                      batch_joints_np)\n\n            # Forward and update weights\n            stage_losses_np, total_loss_np, _, summaries, current_lr, \\\n            stage_heatmap_np, global_step = sess.run([model.stage_loss,\n                                                      model.total_loss,\n                                                      model.train_op,\n                                                      merged_summary,\n                                                      model.cur_lr,\n                                                      model.stage_heatmap,\n                                                      model.global_step\n                                                      ],\n                                                     feed_dict={model.input_images: batch_x_np,\n                                                                model.gt_hmap_placeholder: batch_gt_heatmap_np})\n\n            # Show training info\n            print_current_training_stats(global_step, current_lr, stage_losses_np, total_loss_np, time.time() - t1)\n\n            # Write logs\n            train_writer.add_summary(summaries, global_step)\n\n            # Draw intermediate results\n            if (global_step + 1) % 10 == 0:\n                if FLAGS.color_channel == \'GRAY\':\n                    demo_img = np.repeat(batch_x_np[0], 3, axis=2)\n                    if FLAGS.normalize_img:\n                        demo_img += 0.5\n                    else:\n                        demo_img += 128.0\n                        demo_img /= 255.0\n                elif FLAGS.color_channel == \'RGB\':\n                    if FLAGS.normalize_img:\n                        demo_img = batch_x_np[0] + 0.5\n                    else:\n                        demo_img += 128.0\n                        demo_img /= 255.0\n                else:\n                    raise ValueError(\'Non support image type.\')\n\n                demo_stage_heatmaps = []\n                for stage in range(FLAGS.cpm_stages):\n                    demo_stage_heatmap = stage_heatmap_np[stage][0, :, :, 0:FLAGS.num_of_joints].reshape(\n                        (FLAGS.heatmap_size, FLAGS.heatmap_size, FLAGS.num_of_joints))\n                    demo_stage_heatmap = cv2.resize(demo_stage_heatmap, (FLAGS.input_size, FLAGS.input_size))\n                    demo_stage_heatmap = np.amax(demo_stage_heatmap, axis=2)\n                    demo_stage_heatmap = np.reshape(demo_stage_heatmap, (FLAGS.input_size, FLAGS.input_size, 1))\n                    demo_stage_heatmap = np.repeat(demo_stage_heatmap, 3, axis=2)\n                    demo_stage_heatmaps.append(demo_stage_heatmap)\n\n                demo_gt_heatmap = batch_gt_heatmap_np[0, :, :, 0:FLAGS.num_of_joints].reshape(\n                    (FLAGS.heatmap_size, FLAGS.heatmap_size, FLAGS.num_of_joints))\n                demo_gt_heatmap = cv2.resize(demo_gt_heatmap, (FLAGS.input_size, FLAGS.input_size))\n                demo_gt_heatmap = np.amax(demo_gt_heatmap, axis=2)\n                demo_gt_heatmap = np.reshape(demo_gt_heatmap, (FLAGS.input_size, FLAGS.input_size, 1))\n                demo_gt_heatmap = np.repeat(demo_gt_heatmap, 3, axis=2)\n\n                if FLAGS.cpm_stages > 4:\n                    upper_img = np.concatenate((demo_stage_heatmaps[0], demo_stage_heatmaps[1], demo_stage_heatmaps[2]),\n                                               axis=1)\n                    if FLAGS.normalize_img:\n                        blend_img = 0.5 * demo_img + 0.5 * demo_gt_heatmap\n                    else:\n                        blend_img = 0.5 * demo_img / 255.0 + 0.5 * demo_gt_heatmap\n                    lower_img = np.concatenate((demo_stage_heatmaps[FLAGS.cpm_stages - 1], demo_gt_heatmap, blend_img),\n                                               axis=1)\n                    demo_img = np.concatenate((upper_img, lower_img), axis=0)\n                    cv2.imshow(\'current heatmap\', (demo_img * 255).astype(np.uint8))\n                    cv2.waitKey(1000)\n                else:\n                    upper_img = np.concatenate((demo_stage_heatmaps[FLAGS.cpm_stages - 1], demo_gt_heatmap, demo_img),\n                                               axis=1)\n                    cv2.imshow(\'current heatmap\', (upper_img * 255).astype(np.uint8))\n                    cv2.waitKey(1000)\n\n            if (global_step + 1) % FLAGS.validation_iters == 0:\n                mean_val_loss = 0\n                cnt = 0\n\n                while cnt < 10:\n                    batch_x_np, batch_joints_np = g_eval.next()\n                    # Normalize images\n                    batch_x_np = batch_x_np / 255.0 - 0.5\n\n                    batch_gt_heatmap_np = cpm_utils.make_heatmaps_from_joints(FLAGS.input_size,\n                                                                              FLAGS.heatmap_size,\n                                                                              FLAGS.joint_gaussian_variance,\n                                                                              batch_joints_np)\n                    total_loss_np, summaries = sess.run([model.total_loss, merged_summary],\n                                                        feed_dict={model.input_images: batch_x_np,\n                                                                   model.gt_hmap_placeholder: batch_gt_heatmap_np})\n                    mean_val_loss += total_loss_np\n                    cnt += 1\n\n                print(\'\\nValidation loss: {:>7.2f}\\n\'.format(mean_val_loss / cnt))\n                test_writer.add_summary(summaries, global_step)\n\n            # Save models\n            if (global_step + 1) % FLAGS.model_save_iters == 0:\n                saver.save(sess=sess, save_path=model_save_dir + \'/\' + FLAGS.network_def.split(\'.py\')[0],\n                           global_step=(global_step + 1))\n                print(\'\\nModel checkpoint saved...\\n\')\n\n            # Finish training\n            if global_step == FLAGS.training_iters:\n                break\n    print(\'Training done.\')\n\n\ndef print_current_training_stats(global_step, cur_lr, stage_losses, total_loss, time_elapsed):\n    stats = \'Step: {}/{} ----- Cur_lr: {:1.7f} ----- Time: {:>2.2f} sec.\'.format(global_step, FLAGS.training_iters,\n                                                                                 cur_lr, time_elapsed)\n    losses = \' | \'.join(\n        [\'S{} loss: {:>7.2f}\'.format(stage_num + 1, stage_losses[stage_num]) for stage_num in range(FLAGS.cpm_stages)])\n    losses += \' | Total loss: {}\'.format(total_loss)\n    print(stats)\n    print(losses + \'\\n\')\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
run_training_distillation.py,34,"b""import os\nimport time\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib as tc\n\nimport models.nets.cpm_hand as teacher_model\nimport models.nets.cpm_hand_v2 as student_model\nfrom utils import cpm_utils, utils\nimport Ensemble_data_generator\nfrom config import FLAGS\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '1'\n\n\nclass Teacher(object):\n    def __init__(self, input_size, output_size):\n        self.graph = tf.Graph()\n        with self.graph.as_default():\n            self.model = teacher_model.CPM_Model(input_size, output_size, 6, 21, img_type='RGB', is_training=False)\n\n            sess_config = tf.ConfigProto()\n            sess_config.gpu_options.per_process_gpu_memory_fraction = 0.7\n            sess_config.gpu_options.allow_growth = True\n            sess_config.allow_soft_placement = True\n            self.sess = tf.Session(config=sess_config, graph=self.graph)\n            self._init_vars()\n            self.saver = tf.train.Saver()\n\n    def _init_vars(self):\n        self.sess.run(tf.global_variables_initializer())\n\n    @property\n    def all_graph_nodes(self):\n        with self.graph.as_default() as graph:\n            return [n.name for n in graph.as_graph_def().node]\n\n\nclass Student(object):\n    def __init__(self, input_size, output_size):\n        self.graph = tf.Graph()\n        with self.graph.as_default():\n            self.model = student_model.CPM_Model(input_size, output_size, 3, 21, img_type='RGB', is_training=True)\n\n            sess_config = tf.ConfigProto()\n            sess_config.gpu_options.per_process_gpu_memory_fraction = 0.7\n            sess_config.gpu_options.allow_growth = True\n            sess_config.allow_soft_placement = True\n            self.sess = tf.Session(config=sess_config, graph=self.graph)\n            self._init_vars()\n            self.saver = tf.train.Saver()\n\n    def _init_vars(self):\n        self.sess.run(tf.global_variables_initializer())\n\n    @property\n    def all_graph_nodes(self):\n        with self.graph.as_default() as graph:\n            return [n.name for n in graph.as_graph_def().node]\n\n\ndef guided_loss(student_stages_output_tensor, teacher_stages_output_tensor):\n    if len(student_stages_output_tensor) != len(teacher_stages_output_tensor):\n        raise ValueError('Length must be equal between teacher and student nodes')\n\n    batch_size = tf.cast(tf.shape(student_stages_output_tensor[0])[0], dtype=tf.float32)\n\n    stages = len(student_stages_output_tensor)\n    stage_loss = [0 for _ in range(stages)]\n    total_loss = 0\n    for stage in range(stages):\n        with tf.variable_scope('stage_' + str(stage + 1) + '_loss'):\n            stage_loss[stage] = tf.nn.l2_loss(student_stages_output_tensor[stage] -\n                                              teacher_stages_output_tensor[stage], name='L2_loss') / batch_size\n            tf.summary.scalar('stage_' + str(stage + 1) + '_loss', stage_loss[stage])\n        with tf.variable_scope('total_loss'):\n            total_loss += stage_loss[stage]\n    tf.summary.scalar('total loss', total_loss)\n    return total_loss, stage_loss\n\n\ndef gt_loss(student_stages_output_tensor, gt_output):\n    batch_size = tf.cast(tf.shape(student_stages_output_tensor[0])[0], dtype=tf.float32)\n\n    stages = len(student_stages_output_tensor)\n    stage_loss_gt = [0 for _ in range(stages)]\n    total_loss_gt = 0\n    for stage in range(stages):\n        with tf.variable_scope('stage_' + str(stage + 1) + '_gt_loss'):\n            stage_loss_gt[stage] = tf.nn.l2_loss(student_stages_output_tensor[stage] -\n                                                 gt_output, name='L2_loss') / batch_size\n            tf.summary.scalar('stage_' + str(stage + 1) + '_gt_loss', stage_loss_gt[stage])\n        with tf.variable_scope('total_gt_loss'):\n            total_loss_gt += stage_loss_gt[stage]\n    tf.summary.scalar('gt loss', total_loss_gt)\n    return total_loss_gt, stage_loss_gt\n\n\ndef get_train_op(total_loss, init_lr, lr_decay_rate, lr_decay_step, optimizer):\n    with tf.variable_scope('train'):\n        global_step = tc.framework.get_or_create_global_step()\n\n        cur_lr = tf.train.exponential_decay(init_lr,\n                                            global_step=global_step,\n                                            decay_steps=lr_decay_step,\n                                            decay_rate=lr_decay_rate\n                                            )\n        tf.summary.scalar('learning rate', cur_lr)\n        train_op = tf.contrib.layers.optimize_loss(loss=total_loss,\n                                                   global_step=global_step,\n                                                   learning_rate=cur_lr,\n                                                   optimizer=optimizer)\n        return {'train_op': train_op, 'global_step': global_step, 'cur_lr': cur_lr}\n\n\ndef print_current_training_stats(global_step, cur_lr, stage_losses, total_loss, stage_gt_losses, total_gt_loss,\n                                 time_elapsed):\n    nStages = len(stage_losses)\n    stats = 'Step: {}/{} ----- Cur_lr: {:1.7f} ----- Time: {:>2.2f} sec.'.format(global_step, FLAGS.training_iters,\n                                                                                 cur_lr, time_elapsed)\n    losses = ' | '.join(\n        ['S{} loss: {:>7.2f}'.format(stage_num + 1, stage_losses[stage_num]) for stage_num in range(nStages)])\n    losses += ' | Total loss: {}'.format(total_loss)\n    gt_losses = ' | '.join(\n        ['S{} gt_loss: {:>7.2f}'.format(stage_num + 1, stage_gt_losses[stage_num]) for stage_num in\n         range(len(stage_gt_losses))])\n    gt_losses += ' | Total gt_loss: {}'.format(total_gt_loss)\n    print(stats)\n    print(losses)\n    print(gt_losses + '\\n')\n\n\ndef train():\n    LEARN_TARGET = 'full_tensor'  # [full_tensor, response]\n\n    guide_node_names = [  # 'sub_stages/sub_stage_img_feature/BiasAdd:0',\n        'stage_2/mid_conv7/BiasAdd:0',\n        'stage_4/mid_conv7/BiasAdd:0',\n        'stage_6/mid_conv7/BiasAdd:0']\n    studied_node_names = [  # 'sub_stages/sub_stage_img_feature/BiasAdd:0',\n        'stage_1/stage_heatmap/BiasAdd:0',\n        'stage_2/mid_conv7/BiasAdd:0',\n        'stage_3/mid_conv7/BiasAdd:0']\n\n    teacher_input_size = 368\n    student_input_size = 128\n    down_sample = 4\n    alpha = 0.0\n    resize_scale = float(student_input_size) / teacher_input_size\n    teacher = Teacher(teacher_input_size, teacher_input_size // down_sample)\n    student = Student(student_input_size, student_input_size // down_sample)\n\n    g = Ensemble_data_generator.ensemble_data_generator(FLAGS.train_img_dir,\n                                                        FLAGS.bg_img_dir,\n                                                        FLAGS.batch_size, 368, True, True,\n                                                        FLAGS.augmentation_config, FLAGS.hnm, FLAGS.do_cropping)\n\n    # Get teacher middle network output nodes\n    guide_nodes = []\n    with teacher.graph.as_default() as cur_graph:\n        if LEARN_TARGET == 'full_tensor':\n            for name in guide_node_names:\n                node = cur_graph.get_tensor_by_name(name=name)\n                guide_nodes.append(node)\n        elif LEARN_TARGET == 'response':\n            for name in guide_node_names:\n                node = cur_graph.get_tensor_by_name(name=name)\n                node = tf.abs(node)\n                node = tf.reduce_mean(node, axis=3)\n                guide_nodes.append(node)\n\n    studied_nodes = []\n    with student.graph.as_default() as cur_graph:\n        if LEARN_TARGET == 'full_tensor':\n            for name in studied_node_names:\n                node = cur_graph.get_tensor_by_name(name=name)\n                studied_nodes.append(node)\n        elif LEARN_TARGET == 'response':\n            for name in studied_node_names:\n                node = cur_graph.get_tensor_by_name(name=name)\n                node = tf.abs(node)\n                node = tf.reduce_mean(node, axis=3)\n                studied_nodes.append(node)\n\n    with student.graph.as_default():\n        teacher_output_list = [tf.placeholder(dtype=tf.float32,\n                                              shape=[None, 32, 32, 22],\n                                              name='guided_output_{}'.format(i))\n                               for i in range(len(guide_nodes))]\n        student_loss, student_stage_loss = guided_loss(studied_nodes, teacher_output_list)\n        gt_loss_part, gt_stage_loss_part = gt_loss(student.model.stage_heatmap, student.model.gt_hmap_placeholder)\n        total_loss = student_loss + alpha * gt_loss_part\n\n        train_ops_vars = get_train_op(total_loss, init_lr=FLAGS.init_lr,\n                                      lr_decay_rate=FLAGS.lr_decay_rate,\n                                      lr_decay_step=FLAGS.lr_decay_step,\n                                      optimizer='RMSProp')\n\n    with student.graph.as_default():\n        for v in tf.global_variables():\n            print('in student', v.name)\n        student.sess.run(tf.global_variables_initializer())\n        # student.saver.restore(student.sess, 'guided_cpm-10000')\n\n        # gs = student.graph.get_tensor_by_name('train/global_step:0')\n        # student.sess.run(tf.assign(gs, value=10000))\n        # print(student.sess.run(gs))\n\n    teacher.saver.restore(teacher.sess, 'cpm_hand')\n\n    train_iters = 300000\n    for train_iter in range(train_iters):\n        t1 = time.time()\n\n        # Size 368\n        batch_imgs_large, batch_joints_large = g.next()\n\n        # Size 128\n        batch_size = batch_imgs_large.shape[0]\n        batch_imgs_small = np.zeros(shape=(batch_size, student_input_size, student_input_size, 3))\n        batch_joints_small = np.zeros(shape=(batch_size, 21, 2))\n        for i in range(batch_size):\n            batch_imgs_small[i] = cv2.resize(batch_imgs_large[i], (student_input_size, student_input_size))\n            batch_joints_small[i] = batch_joints_large[i] * resize_scale\n        # Generate heatmaps from joints\n        hm_size = student_input_size / down_sample\n        batch_gt_heatmap_np = cpm_utils.make_heatmaps_from_joints_openpose(student_input_size,\n                                                                           hm_size,\n                                                                           FLAGS.joint_gaussian_variance,\n                                                                           batch_joints_small)\n\n        # Normalize\n        batch_imgs_large = batch_imgs_large / 255.0 - 0.5\n        batch_imgs_small = batch_imgs_small / 255.0 - 0.5\n\n        teacher_output_heatmaps, \\\n        guide_output = teacher.sess.run([teacher.model.stage_heatmap,\n                                         guide_nodes],\n                                        feed_dict={teacher.model.input_images: batch_imgs_large})\n\n        teacher_resized_heatmaps = [\n            np.zeros(shape=(batch_size, student_input_size / down_sample, student_input_size / down_sample, 22))\n            for _ in range(len(guide_output))]\n        for i in range(len(guide_output)):\n            for batch_num in range(batch_size):\n                teacher_resized_heatmaps[i][batch_num] = cv2.resize(guide_output[i][batch_num], (\n                student_input_size / down_sample, student_input_size / down_sample))\n\n        feed_dict = {student.model.input_images: batch_imgs_small,\n                     student.model.gt_hmap_placeholder: batch_gt_heatmap_np}\n        for k, v in zip(teacher_output_list, teacher_resized_heatmaps):\n            feed_dict.update({k: v})\n\n        student_output_heatmaps, \\\n        loss, \\\n        stage_loss_np, \\\n        gt_loss_np, \\\n        stage_gt_loss_np, \\\n        _, \\\n        global_step_np, \\\n        cur_lr_np = student.sess.run([student.model.stage_heatmap,\n                                      student_loss,\n                                      student_stage_loss,\n                                      gt_loss_part,\n                                      gt_stage_loss_part,\n                                      train_ops_vars['train_op'],\n                                      train_ops_vars['global_step'],\n                                      train_ops_vars['cur_lr']],\n                                     feed_dict=feed_dict)\n\n        if (train_iter + 1) % 10 == 0:\n            color_img = (batch_imgs_large[0] + 0.5) * 255.0\n            hm_img_teacher = utils.draw_stages_heatmaps(teacher_output_heatmaps, student_input_size)\n            hm_img_student = utils.draw_stages_heatmaps(student_output_heatmaps, student_input_size)\n            cv2.imshow('hm teacher', hm_img_teacher)\n            cv2.imshow('hm student', hm_img_student)\n            cv2.imshow('color', color_img.astype(np.uint8))\n            cv2.waitKey(10)\n\n        if (train_iter + 1) % 10000 == 0:\n            with student.graph.as_default():\n                student.saver.save(sess=student.sess, save_path='distillation', global_step=global_step_np)\n\n        print_current_training_stats(global_step_np, cur_lr_np, stage_loss_np, loss, stage_gt_loss_np, gt_loss_np,\n                                     time.time() - t1)\n\n\nif __name__ == '__main__':\n    train()\n"""
models/__init__.py,0,b''
utils/__init__.py,0,b''
utils/cpm_utils.py,0,"b'import numpy as np\nimport math\nimport cv2\n\n\nM_PI = 3.14159\n\n\n# Compute gaussian kernel for input image\ndef gaussian_img(img_height, img_width, c_x, c_y, variance):\n    gaussian_map = np.zeros((img_height, img_width))\n    for x_p in range(img_width):\n        for y_p in range(img_height):\n            dist_sq = (x_p - c_x) * (x_p - c_x) + \\\n                      (y_p - c_y) * (y_p - c_y)\n            exponent = dist_sq / 2.0 / variance / variance\n            gaussian_map[y_p, x_p] = np.exp(-exponent)\n    return gaussian_map\n\n\ndef read_image(file, cam, boxsize, type):\n    # from file\n    if type == \'IMAGE\':\n        oriImg = cv2.imread(file)\n    # from webcam\n    elif type == \'WEBCAM\':\n        _, oriImg = cam.read()\n    # from video\n    elif type == \'VIDEO\':\n        oriImg = cv2.cvtColor(file, cv2.COLOR_BGR2RGB)\n\n    if oriImg is None:\n        print(\'oriImg is None\')\n        return None\n\n    scale = boxsize / (oriImg.shape[0] * 1.0)\n    imageToTest = cv2.resize(oriImg, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_LANCZOS4)\n\n    output_img = np.ones((boxsize, boxsize, 3)) * 128\n\n    img_h = imageToTest.shape[0]\n    img_w = imageToTest.shape[1]\n    if img_w < boxsize:\n        offset = img_w % 2\n        # make the origin image be the center\n        output_img[:, int(boxsize / 2 - math.floor(img_w / 2)):int(\n            boxsize / 2 + math.floor(img_w / 2) + offset), :] = imageToTest\n    else:\n        # crop the center of the origin image\n        output_img = imageToTest[:,\n                     int(img_w / 2 - boxsize / 2):int(img_w / 2 + boxsize / 2), :]\n    return output_img\n\n\ndef make_gaussian(size, fwhm=3, center=None):\n    """""" Make a square gaussian kernel.\n    size is the length of a side of the square\n    fwhm is full-width-half-maximum, which\n    can be thought of as an effective radius.\n    """"""\n\n    x = np.arange(0, size, 1, float)\n    y = x[:, np.newaxis]\n\n    if center is None:\n        x0 = y0 = size // 2\n    else:\n        x0 = center[0]\n        y0 = center[1]\n\n    return np.exp(-((x - x0) ** 2 + (y - y0) ** 2) / 2.0 / fwhm / fwhm)\n\n\ndef make_gaussian_batch(heatmaps, size, fwhm):\n    """""" Make a square gaussian kernel.\n    size is the length of a side of the square\n    fwhm is full-width-half-maximum, which\n    can be thought of as an effective radius.\n    """"""\n    stride = heatmaps.shape[1] // size\n\n    batch_datum = np.zeros(shape=(heatmaps.shape[0], size, size, heatmaps.shape[3]))\n\n    for data_num in range(heatmaps.shape[0]):\n        for joint_num in range(heatmaps.shape[3] - 1):\n            heatmap = heatmaps[data_num, :, :, joint_num]\n            center = np.unravel_index(np.argmax(heatmap), (heatmap.shape[0], heatmap.shape[1]))\n\n            x = np.arange(0, size, 1, float)\n            y = x[:, np.newaxis]\n\n            if center is None:\n                x0 = y0 = size * stride // 2\n            else:\n                x0 = center[1]\n                y0 = center[0]\n\n            batch_datum[data_num, :, :, joint_num] = np.exp(\n                -((x * stride - x0) ** 2 + (y * stride - y0) ** 2) / 2.0 / fwhm / fwhm)\n        batch_datum[data_num, :, :, heatmaps.shape[3] - 1] = np.ones((size, size)) - np.amax(\n            batch_datum[data_num, :, :, 0:heatmaps.shape[3] - 1], axis=2)\n\n    return batch_datum\n\n\ndef make_heatmaps_from_joints(input_size, heatmap_size, gaussian_variance, batch_joints):\n    # Generate ground-truth heatmaps from ground-truth 2d joints\n    scale_factor = input_size // heatmap_size\n    batch_gt_heatmap_np = []\n    for i in range(batch_joints.shape[0]):\n        gt_heatmap_np = []\n        invert_heatmap_np = np.ones(shape=(heatmap_size, heatmap_size))\n        for j in range(batch_joints.shape[1]):\n            cur_joint_heatmap = make_gaussian(heatmap_size,\n                                              gaussian_variance,\n                                              center=(batch_joints[i][j] // scale_factor))\n            gt_heatmap_np.append(cur_joint_heatmap)\n            invert_heatmap_np -= cur_joint_heatmap\n        gt_heatmap_np.append(invert_heatmap_np)\n        batch_gt_heatmap_np.append(gt_heatmap_np)\n    batch_gt_heatmap_np = np.asarray(batch_gt_heatmap_np)\n    batch_gt_heatmap_np = np.transpose(batch_gt_heatmap_np, (0, 2, 3, 1))\n\n    return batch_gt_heatmap_np\n\n\ndef make_heatmaps_from_joints_openpose(input_size, heatmap_size, gaussian_variance, batch_joints):\n    joint_map = [4, 3, 2, 1, 8, 7, 6, 5, 12, 11, 10, 9, 16, 15, 14, 13, 20, 19, 18, 17, 0]\n    # Generate ground-truth heatmaps from ground-truth 2d joints\n    scale_factor = input_size // heatmap_size\n    batch_gt_heatmap_np = []\n    for i in range(batch_joints.shape[0]):\n        gt_heatmap_np = []\n        invert_heatmap_np = np.ones(shape=(heatmap_size, heatmap_size))\n        for j in range(batch_joints.shape[1]):\n            cur_joint_heatmap = make_gaussian(heatmap_size,\n                                              gaussian_variance,\n                                              center=(batch_joints[i][joint_map[j]] // scale_factor))\n            gt_heatmap_np.append(cur_joint_heatmap)\n            invert_heatmap_np -= cur_joint_heatmap\n        gt_heatmap_np.append(invert_heatmap_np)\n        batch_gt_heatmap_np.append(gt_heatmap_np)\n    batch_gt_heatmap_np = np.asarray(batch_gt_heatmap_np)\n    batch_gt_heatmap_np = np.transpose(batch_gt_heatmap_np, (0, 2, 3, 1))\n\n    return batch_gt_heatmap_np\n\n\ndef rad2Deg(rad):\n    return rad * (180 / M_PI)\n\n\ndef deg2Rad(deg):\n    return deg * (M_PI / 180)\n\n\ndef warpMatrix(sw, sh, theta, phi, gamma, scale, fovy):\n    st = math.sin(deg2Rad(theta))\n    ct = math.cos(deg2Rad(theta))\n    sp = math.sin(deg2Rad(phi))\n    cp = math.cos(deg2Rad(phi))\n    sg = math.sin(deg2Rad(gamma))\n    cg = math.cos(deg2Rad(gamma))\n\n    halfFovy = fovy * 0.5\n    d = math.hypot(sw, sh)\n    sideLength = scale * d / math.cos(deg2Rad(halfFovy))\n    h = d / (2.0 * math.sin(deg2Rad(halfFovy)))\n    n = h - (d / 2.0)\n    f = h + (d / 2.0)\n\n    Rtheta = np.identity(4)\n    Rphi = np.identity(4)\n    Rgamma = np.identity(4)\n\n    T = np.identity(4)\n    P = np.zeros((4, 4))\n\n    Rtheta[0, 0] = Rtheta[1, 1] = ct\n    Rtheta[0, 1] = -st\n    Rtheta[1, 0] = st\n\n    Rphi[1, 1] = Rphi[2, 2] = cp\n    Rphi[1, 2] = -sp\n    Rphi[2, 1] = sp\n\n    Rgamma[0, 0] = cg\n    Rgamma[2, 2] = cg\n    Rgamma[0, 2] = sg\n    Rgamma[2, 0] = sg\n\n    T[2, 3] = -h\n\n    P[0, 0] = P[1, 1] = 1.0 / math.tan(deg2Rad(halfFovy))\n    P[2, 2] = -(f + n) / (f - n)\n    P[2, 3] = -(2.0 * f * n) / (f - n)\n    P[3, 2] = -1.0\n\n    F = np.matmul(Rtheta, Rgamma)\n    F = np.matmul(Rphi, F)\n    F = np.matmul(T, F)\n    F = np.matmul(P, F)\n\n    ptsIn = np.zeros(12)\n    ptsOut = np.zeros(12)\n    halfW = sw / 2\n    halfH = sh / 2\n\n    ptsIn[0] = -halfW\n    ptsIn[1] = halfH\n    ptsIn[3] = halfW\n    ptsIn[4] = halfH\n    ptsIn[6] = halfW\n    ptsIn[7] = -halfH\n    ptsIn[9] = -halfW\n    ptsIn[10] = -halfH\n    ptsIn[2] = ptsIn[5] = ptsIn[8] = ptsIn[11] = 0\n\n    ptsInMat = np.array([[ptsIn[0], ptsIn[1], ptsIn[2]], [ptsIn[3], ptsIn[4], ptsIn[5]], [ptsIn[6], ptsIn[7], ptsIn[8]],\n                         [ptsIn[9], ptsIn[10], ptsIn[11]]], dtype=np.float32)\n    ptsOutMat = np.array(\n        [[ptsOut[0], ptsOut[1], ptsOut[2]], [ptsOut[3], ptsOut[4], ptsOut[5]], [ptsOut[6], ptsOut[7], ptsOut[8]],\n         [ptsOut[9], ptsOut[10], ptsOut[11]]], dtype=np.float32)\n    ptsInMat = np.array([ptsInMat])\n    ptsOutMat = cv2.perspectiveTransform(ptsInMat, F)\n\n    ptsInPt2f = np.array([[0, 0], [0, 0], [0, 0], [0, 0]], dtype=np.float32)\n    ptsOutPt2f = np.array([[0, 0], [0, 0], [0, 0], [0, 0]], dtype=np.float32)\n\n    i = 0\n\n    while i < 4:\n        ptsInPt2f[i][0] = ptsIn[i * 3 + 0] + halfW\n        ptsInPt2f[i][1] = ptsIn[i * 3 + 1] + halfH\n        ptsOutPt2f[i][0] = (ptsOutMat[0][i][0] + 1) * sideLength * 0.5\n        ptsOutPt2f[i][1] = (ptsOutMat[0][i][1] + 1) * sideLength * 0.5\n        i = i + 1\n\n    M = cv2.getPerspectiveTransform(ptsInPt2f, ptsOutPt2f)\n    return M\n\n\ndef warpImage(src, theta, phi, gamma, scale, fovy):\n    halfFovy = fovy * 0.5\n    d = math.hypot(src.shape[1], src.shape[0])\n    sideLength = scale * d / math.cos(deg2Rad(halfFovy))\n    sideLength = np.int32(sideLength)\n\n    M = warpMatrix(src.shape[1], src.shape[0], theta, phi, gamma, scale, fovy)\n    dst = cv2.warpPerspective(src, M, (sideLength, sideLength))\n    mid_x = mid_y = dst.shape[0] // 2\n    target_x = target_y = src.shape[0] // 2\n    offset = (target_x % 2)\n\n    if len(dst.shape) == 3:\n        dst = dst[mid_y - target_y:mid_y + target_y + offset,\n              mid_x - target_x:mid_x + target_x + offset,\n              :]\n    else:\n        dst = dst[mid_y - target_y:mid_y + target_y + offset,\n              mid_x - target_x:mid_x + target_x + offset]\n\n    return dst\n'"
utils/create_cpm_tfr_fulljoints.py,5,"b""import cv2\nimport cpm_utils\nimport numpy as np\nimport math\nimport tensorflow as tf\nimport time\nimport random\nimport os\n\n\ntfr_file = 'cpm_sample_dataset.tfrecords'\ndataset_dir = ''\n\nSHOW_INFO = False\nbox_size = 64\nnum_of_joints = 21\ngaussian_radius = 2\n\n\ndef _bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef _int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef _float64_feature(value):\n    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\n\n# Create writer\ntfr_writer = tf.python_io.TFRecordWriter(tfr_file)\n\nimg_count = 0\nt1 = time.time()\n# Loop each dir\nfor person_dir in os.listdir(dataset_dir):\n    if not os.path.isdir(dataset_dir + person_dir): continue\n\n    gt_file = dataset_dir + person_dir + '/labels.txt'\n    gt_content = open(gt_file, 'rb').readlines()\n\n    for idx, line in enumerate(gt_content):\n        line = line.split()\n\n        # Check if it is a valid img file\n        if not line[0].endswith(('jpg', 'png')):\n            continue\n        cur_img_path = dataset_dir + person_dir + '/imgs/' + line[0]\n        cur_img = cv2.imread(cur_img_path)\n\n        # Read in bbox and joints coords\n        tmp = [float(x) for x in line[1:5]]\n        cur_hand_bbox = [min([tmp[0], tmp[2]]),\n                         min([tmp[1], tmp[3]]),\n                         max([tmp[0], tmp[2]]),\n                         max([tmp[1], tmp[3]])\n                         ]\n        if cur_hand_bbox[0] < 0: cur_hand_bbox[0] = 0\n        if cur_hand_bbox[1] < 0: cur_hand_bbox[1] = 0\n        if cur_hand_bbox[2] > cur_img.shape[1]: cur_hand_bbox[2] = cur_img.shape[1]\n        if cur_hand_bbox[3] > cur_img.shape[0]: cur_hand_bbox[3] = cur_img.shape[0]\n\n        cur_hand_joints_x = [float(i) for i in line[9:49:2]]\n        cur_hand_joints_x.append(float(line[7]))\n        cur_hand_joints_y = [float(i) for i in line[10:49:2]]\n        cur_hand_joints_y.append(float(line[8]))\n\n        # Crop image and adjust joint coords\n        cur_img = cur_img[int(float(cur_hand_bbox[1])):int(float(cur_hand_bbox[3])),\n                  int(float(cur_hand_bbox[0])):int(float(cur_hand_bbox[2])),\n                  :]\n        cur_hand_joints_x = [x - cur_hand_bbox[0] for x in cur_hand_joints_x]\n        cur_hand_joints_y = [x - cur_hand_bbox[1] for x in cur_hand_joints_y]\n\n        # # Display joints\n        # for i in range(len(cur_hand_joints_x)):\n        #     cv2.circle(cur_img, center=(int(cur_hand_joints_x[i]), int(cur_hand_joints_y[i])),radius=3, color=(255,0,0), thickness=-1)\n        #     cv2.imshow('', cur_img)\n        #     cv2.waitKey(500)\n        # cv2.imshow('', cur_img)\n        # cv2.waitKey(1)\n\n        output_image = np.ones(shape=(box_size, box_size, 3)) * 128\n        output_heatmaps = np.zeros((box_size, box_size, num_of_joints))\n\n        # Resize and pad image to fit output image size\n        if cur_img.shape[0] > cur_img.shape[1]:\n            scale = box_size / (cur_img.shape[0] * 1.0)\n\n            # Relocalize points\n            cur_hand_joints_x = map(lambda x: x * scale, cur_hand_joints_x)\n            cur_hand_joints_y = map(lambda x: x * scale, cur_hand_joints_y)\n\n            # Resize image \n            image = cv2.resize(cur_img, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_LANCZOS4)\n            offset = image.shape[1] % 2\n\n            output_image[:, int(box_size / 2 - math.floor(image.shape[1] / 2)): int(\n                box_size / 2 + math.floor(image.shape[1] / 2) + offset), :] = image\n            cur_hand_joints_x = map(lambda x: x + (box_size / 2 - math.floor(image.shape[1] / 2)),\n                                    cur_hand_joints_x)\n\n            cur_hand_joints_x = np.asarray(cur_hand_joints_x)\n            cur_hand_joints_y = np.asarray(cur_hand_joints_y)\n\n            if SHOW_INFO:\n                hmap = np.zeros((box_size, box_size))\n                # Plot joints\n                for i in range(num_of_joints):\n                    cv2.circle(output_image, (int(cur_hand_joints_x[i]), int(cur_hand_joints_y[i])), 3, (0, 255, 0), 2)\n\n                    # Generate joint gaussian map\n                    part_heatmap= cpm_utils.gaussian_img(box_size,box_size,cur_hand_joints_x[i],cur_hand_joints_y[i],1)\n                    #part_heatmap = utils.make_gaussian(output_image.shape[0], gaussian_radius,\n                     #                                  [cur_hand_joints_x[i], cur_hand_joints_y[i]])\n                    hmap += part_heatmap * 50\n            else:\n                for i in range(num_of_joints):\n                    #output_heatmaps[:, :, i] = utils.make_gaussian(box_size, gaussian_radius,\n                    #                                               [cur_hand_joints_x[i], cur_hand_joints_y[i]])\n                    output_heatmaps[:, :, i]= cpm_utils.gaussian_img(box_size,box_size,cur_hand_joints_x[i],cur_hand_joints_y[i],1)\n\n        else:\n            scale = box_size / (cur_img.shape[1] * 1.0)\n\n            # Relocalize points\n            cur_hand_joints_x = map(lambda x: x * scale, cur_hand_joints_x)\n            cur_hand_joints_y = map(lambda x: x * scale, cur_hand_joints_y)\n\n            # Resize image\n            image = cv2.resize(cur_img, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_LANCZOS4)\n            offset = image.shape[0] % 2\n\n            output_image[int(box_size / 2 - math.floor(image.shape[0] / 2)): int(\n                box_size / 2 + math.floor(image.shape[0] / 2) + offset), :, :] = image\n            cur_hand_joints_y = map(lambda x: x + (box_size / 2 - math.floor(image.shape[0] / 2)),\n                                    cur_hand_joints_y)\n\n            cur_hand_joints_x = np.asarray(cur_hand_joints_x)\n            cur_hand_joints_y = np.asarray(cur_hand_joints_y)\n\n            if SHOW_INFO:\n                hmap = np.zeros((box_size, box_size))\n                # Plot joints\n                for i in range(num_of_joints):\n                    cv2.circle(output_image, (int(cur_hand_joints_x[i]), int(cur_hand_joints_y[i])), 3, (0, 255, 0), 2)\n\n                    # Generate joint gaussian map\n                    part_heatmap = utils.make_gaussian(output_image.shape[0], gaussian_radius,\n                                                       [cur_hand_joints_x[i], cur_hand_joints_y[i]])\n                    hmap += part_heatmap * 50\n            else:\n                for i in range(num_of_joints):\n                    output_heatmaps[:, :, i] = utils.make_gaussian(box_size, gaussian_radius,\n                                                                   [cur_hand_joints_x[i], cur_hand_joints_y[i]])\n        if SHOW_INFO:\n            cv2.imshow('', hmap.astype(np.uint8))\n            cv2.imshow('i', output_image.astype(np.uint8))\n            cv2.waitKey(0)\n\n        # Create background map\n        output_background_map = np.ones((box_size, box_size)) - np.amax(output_heatmaps, axis=2)\n        output_heatmaps = np.concatenate((output_heatmaps, output_background_map.reshape((box_size, box_size, 1))),\n                                         axis=2)\n        # cv2.imshow('', (output_background_map*255).astype(np.uint8))\n        # cv2.imshow('h', (np.amax(output_heatmaps[:, :, 0:21], axis=2)*255).astype(np.uint8))\n        # cv2.waitKey(1000)\n\n\n        coords_set = np.concatenate((np.reshape(cur_hand_joints_x, (num_of_joints, 1)),\n                                     np.reshape(cur_hand_joints_y, (num_of_joints, 1))),\n                                    axis=1)\n\n        output_image_raw = output_image.astype(np.uint8).tostring()\n        output_heatmaps_raw = output_heatmaps.flatten().tolist()\n        output_coords_raw = coords_set.flatten().tolist()\n\n        raw_sample = tf.train.Example(features=tf.train.Features(feature={\n            'image': _bytes_feature(output_image_raw),\n            'heatmaps': _float64_feature(output_heatmaps_raw)\n        }))\n\n        tfr_writer.write(raw_sample.SerializeToString())\n\n        img_count += 1\n        if img_count % 50 == 0:\n            print('Processed %d images, took %f seconds' % (img_count, time.time() - t1))\n            t1 = time.time()\n\ntfr_writer.close()\n"""
utils/tf_utils.py,87,"b'import tensorflow as tf\nimport utils.cpm_utils as cpm_utils\n\n\ndef read_and_decode_cpm(tfr_queue, img_size, num_joints, center_radius):\n    tfr_reader = tf.TFRecordReader()\n    _, serialized_example = tfr_reader.read(tfr_queue)\n\n    queue_images = []\n    queue_center_maps = []\n    queue_labels = []\n    queue_orig_images = []\n\n    for i in range(2):\n        features = tf.parse_single_example(serialized_example,\n                                           features={\n                                               \'image\': tf.FixedLenFeature([], tf.string),\n                                               \'heatmaps\': tf.FixedLenFeature(\n                                                   [int(img_size * img_size * (num_joints + 1))], tf.float32)\n                                           })\n\n        # img_size = 128\n        # center_radius = 11\n        img = tf.decode_raw(features[\'image\'], tf.uint8)\n        img = tf.reshape(img, [img_size, img_size, 3])\n        img = tf.cast(img, tf.float32)\n\n        img = img[..., ::-1]\n        img = tf.image.random_contrast(img, 0.7, 1)\n        img = tf.image.random_brightness(img, max_delta=0.9)\n        img = tf.image.random_hue(img, 0.05)\n        img = tf.image.random_saturation(img, 0.7, 1.1)\n        img = img[..., ::-1]\n\n        # heatmap = tf.decode_raw(features[\'heatmaps\'], tf.float32)\n        heatmap = tf.reshape(features[\'heatmaps\'], [img_size, img_size, (num_joints + 1)])\n\n        # create centermap\n        center_map = tf.constant((cpm_utils.make_gaussian(img_size, center_radius,\n                                                          [int(img_size / 2), int(img_size / 2)])).reshape(\n            (img_size, img_size, 1)), name=\'center_map\')\n        center_map = tf.cast(center_map, tf.float32)\n\n        # merge img + centermap + heatmap\n        merged_img_heatmap = tf.concat([img, center_map, heatmap], axis=2)\n\n        # subtract mean before pad\n        mean_volume = tf.concat((128 * tf.ones(shape=(img_size, img_size, 3)),\n                                 tf.zeros(shape=(img_size, img_size, (num_joints + 1))),\n                                 tf.ones(shape=(img_size, img_size, 1))), axis=2)\n\n        merged_img_heatmap -= mean_volume\n\n        # preprocessing\n        preprocessed_merged_img_c_heatmap, _, _ = preprocess(merged_img_heatmap,\n                                                             label=None,\n                                                             crop_off_ratio=0.05,\n                                                             rotation_angle=0.8,\n                                                             has_bbox=False,\n                                                             do_flip_lr=True,\n                                                             do_flip_ud=False,\n                                                             low_sat=None,\n                                                             high_sat=None,\n                                                             max_bright_delta=None,\n                                                             max_hue_delta=None)\n\n        padded_img_size = img_size  # * (1 + tf.random_uniform([], minval=0.0, maxval=0.3))\n        padded_img_size = tf.cast(padded_img_size, tf.int32)\n\n        # resize pad\n        preprocessed_merged_img_c_heatmap = tf.image.resize_image_with_crop_or_pad(preprocessed_merged_img_c_heatmap,\n                                                                                   padded_img_size, padded_img_size)\n        preprocessed_merged_img_c_heatmap += tf.concat((128 * tf.ones(shape=(padded_img_size, padded_img_size, 3)),\n                                                        tf.zeros(\n                                                            shape=(padded_img_size, padded_img_size, (num_joints + 1))),\n                                                        tf.ones(shape=(padded_img_size, padded_img_size, 1))), axis=2)\n        preprocessed_merged_img_c_heatmap = tf.image.resize_images(preprocessed_merged_img_c_heatmap,\n                                                                   size=[img_size, img_size])\n\n        with tf.control_dependencies([preprocessed_merged_img_c_heatmap]):\n            # preprocessed_img = tf.slice(preprocessed_merged_img_c_heatmap, [0,0,0], [368,368,3])\n            # preprocessed_center_maps = tf.slice(preprocessed_merged_img_c_heatmap, [0,0,3], [368,368,1])\n            # preprocessed_heatmaps = tf.slice(preprocessed_merged_img_c_heatmap, [0,0,4], [368,368,13])\n\n            preprocessed_img, preprocessed_center_maps, preprocessed_heatmaps = tf.split(\n                preprocessed_merged_img_c_heatmap, [3, 1, (num_joints + 1)], axis=2)\n\n            # Normalize image value\n            preprocessed_img /= 256\n            preprocessed_img -= 0.5\n\n            queue_images.append(preprocessed_img)\n            queue_center_maps.append(preprocessed_center_maps)\n            queue_labels.append(preprocessed_heatmaps)\n            queue_orig_images.append(img)\n\n    return queue_images, queue_center_maps, queue_labels, queue_orig_images\n    # return preprocessed_img, preprocessed_center_maps, preprocessed_heatmaps, img\n\n\ndef read_batch_cpm(tfr_path, img_size, hmap_size, num_joints, center_radius, batch_size=16, num_epochs=None):\n    """"""Read batch images as the input to the network\n\n        tfr_path: path to tfrecord file\n        num_epochs: None=iteratively read forever\n                    other number=iterate whole tfr_file how many times\n        """"""\n\n    with tf.name_scope(\'Batch_Inputs\'):\n        tfr_queue = tf.train.string_input_producer(tfr_path, num_epochs=num_epochs, shuffle=True)\n\n        # images, centers, labels, image_orig = read_and_decode_cpm(tfr_queue, img_size, num_joints, center_radius)\n\n        data_list = [read_and_decode_cpm(tfr_queue, img_size, num_joints, center_radius) for _ in\n                     range(2 * len(tfr_path))]\n\n        batch_images, batch_centers, batch_labels, batch_images_orig = tf.train.shuffle_batch_join(data_list,\n                                                                                                   batch_size=batch_size,\n                                                                                                   capacity=100 + 6 * batch_size,\n                                                                                                   min_after_dequeue=100,\n                                                                                                   enqueue_many=True,\n                                                                                                   name=\'batch_data_read\')\n\n        # batch_labels = tf.image.resize_bilinear(batch_labels, size=tf.constant((hmap_size,hmap_size), name=\'shape\'))\n\n    return batch_images, batch_centers, batch_labels, batch_images_orig\n\n\ndef rotate_points(orig_points, angle, w, h):\n    """"""Return rotated points\n\n    Args:\n        orig_points: \'Tensor\' with shape [N,2], each entry is point (x,y)\n        angle: rotate radians\n\n    Returns:\n        \'Tensor\' with shape [N,2], with rotated points\n    """"""\n\n    # rotation\n    rotate_mat = tf.stack([[tf.cos(angle) / w, tf.sin(angle) / h],\n                           [-tf.sin(angle) / w, tf.cos(angle) / h]])\n\n    # shift coord\n    orig_points = tf.subtract(orig_points, 0.5)\n\n    orig_points = tf.stack([orig_points[:, 0] * w,\n                            orig_points[:, 1] * h], axis=1)\n    print(orig_points)\n    rotated_points = tf.matmul(orig_points, rotate_mat) + 0.5\n\n    return rotated_points\n\n\ndef preprocess(image,\n               label,\n               has_bbox=True,\n               rotation_angle=1.5,\n               crop_off_ratio=0.2,\n               do_flip_lr=True,\n               do_flip_ud=True,\n               max_hue_delta=0.15,\n               low_sat=0.5,\n               high_sat=2.0,\n               max_bright_delta=0.3):\n    """"""Do some processes for input image\n\n    Args:\n        image: A \'Tensor\' of RGB image\n        label: vector of floats with even length (be pair of (x,y))\n        has_bbox: if \'True\', Assume first 4 numbers of \'label\' are [top-left, bot-right] coords\n        rotation_angle: maximum allowed rotation radians\n        crop_off_ratio: maximum cropping offset of top-left corner\n                        1-crop_off_ratio be maximum cropping offset of cropped bot-right corner\n        do_flip_lr: with half chance flip the image left right\n        do_flip_ud: with half chance flip the image upper down\n        max_hue_delta: allowed random adjust hue range\n        low_sat: lowest range of saturation\n        high_sat: highest range of saturation\n        max_bright_delta: allowed random adjust brightness range\n\n    Returns:\n        image: processed image \'Tensor\'\n        new_bbox: \'Tensor\' of processed bbox coords if \'has_bbox\' == True\n        total_points: \'Tensor\' of processed points coords\n    """"""\n\n    new_bbox = []\n    total_points = []\n\n    # [height, width, channel] of input image\n    img_shape_list = image.get_shape().as_list()\n\n    if max_hue_delta is not None:\n        # random hue\n        image = tf.image.random_hue(image, max_delta=max_hue_delta)\n\n    if low_sat is not None and high_sat is not None:\n        # random saturation\n        image = tf.image.random_saturation(image, lower=low_sat, upper=high_sat)\n\n    if max_bright_delta is not None:\n        # random brightness\n        image = tf.image.random_brightness(image, max_delta=max_bright_delta)\n\n    if label is not None:\n        total_points = tf.stack([label[i] for i in range(label.shape[0])])\n\n    # crop image\n    new_top_left_x = crop_off_ratio * tf.random_uniform([], minval=-1.0, maxval=1.0)\n    off_w_ratio = tf.cond(tf.less(new_top_left_x, 0), lambda: tf.zeros([]), lambda: new_top_left_x)\n\n    new_top_left_y = crop_off_ratio * tf.random_uniform([], minval=-1.0, maxval=1.0)\n    off_h_ratio = tf.cond(tf.less(new_top_left_y, 0), lambda: tf.zeros([]), lambda: new_top_left_y)\n\n    new_bot_right_x = crop_off_ratio * tf.random_uniform([], minval=-1.0, maxval=1.0)\n    tar_w_ratio = tf.cond(tf.less(new_bot_right_x, 0), lambda: tf.ones([]) - off_w_ratio,\n                          lambda: 1 - new_bot_right_x - off_w_ratio)\n\n    new_bot_right_y = crop_off_ratio * tf.random_uniform([], minval=-1.0, maxval=1.0)\n    tar_h_ratio = tf.cond(tf.less(new_bot_right_y, 0), lambda: tf.ones([]) - off_h_ratio,\n                          lambda: 1 - new_bot_right_y - off_h_ratio)\n\n    pad_image_height = (1 - new_top_left_y - new_bot_right_y) * img_shape_list[0]\n    pad_image_width = (1 - new_top_left_x - new_bot_right_x) * img_shape_list[1]\n    cropped_image = tf.image.crop_to_bounding_box(image,\n                                                  offset_width=tf.cast(off_w_ratio * img_shape_list[1], tf.int32),\n                                                  offset_height=tf.cast(off_h_ratio * img_shape_list[0], tf.int32),\n                                                  target_height=tf.cast(tar_h_ratio * img_shape_list[0], tf.int32),\n                                                  target_width=tf.cast(tar_w_ratio * img_shape_list[1], tf.int32))\n\n    image = tf.image.pad_to_bounding_box(cropped_image,\n                                         offset_width=tf.cast((off_w_ratio - new_top_left_x) * img_shape_list[1],\n                                                              tf.int32),\n                                         offset_height=tf.cast((off_h_ratio - new_top_left_y) * img_shape_list[0],\n                                                               tf.int32),\n                                         target_height=tf.cast(pad_image_height, tf.int32),\n                                         target_width=tf.cast(pad_image_width, tf.int32))\n\n    # random rotation angle\n    angle = rotation_angle * tf.random_uniform([])\n\n    # rotate image\n    image = tf.contrib.image.rotate(image, -angle, interpolation=\'BILINEAR\')\n\n    if label is not None:\n        if has_bbox:\n            # include 4 bbox points\n            bbox_points = tf.stack([[total_points[0][0], total_points[0][1]],\n                                    [total_points[1][0], total_points[0][1]],\n                                    [total_points[0][0], total_points[1][1]],\n                                    [total_points[1][0], total_points[1][1]]], axis=0)\n            if label.shape[0] == 4:\n                total_points = bbox_points\n            else:\n                total_points = tf.concat([bbox_points, total_points[2:]], axis=0)\n\n        # rotate points\n        total_points = rotate_points(total_points, angle, pad_image_width, pad_image_height)\n\n        if has_bbox:\n            # new bbox [top_left, bot_right]\n            new_bbox = tf.stack([[total_points[2][0], total_points[0][1]],\n                                 [total_points[1][0], total_points[3][1]]], axis=0)\n            total_points = tf.concat([new_bbox, total_points[4:]], axis=0)\n\n    if label is not None:\n        # adjust points\' coords for cropped image\n        total_points = tf.reshape(total_points[:], shape=[-1, 2])\n        total_points = tf.stack([(total_points[:, 0] - new_top_left_x) / (1 - new_top_left_x - new_bot_right_x),\n                                 (total_points[:, 1] - new_top_left_y) / (1 - new_top_left_y - new_bot_right_y)],\n                                axis=1)\n\n    if label is not None:\n        # chance flip left right\n        def flip_lr():\n            i = tf.image.flip_left_right(image)\n            l = tf.stack([1 - total_points[:, 0],\n                          total_points[:, 1]], axis=1)\n            return i, l\n\n        def no_flip_lr():\n            i = image\n            l = total_points\n            return i, l\n\n        if do_flip_lr:\n            image, total_points = tf.cond(tf.greater(tf.random_uniform([]), 0.5), flip_lr, no_flip_lr)\n\n        # chance flip upside down\n        def flip_ud():\n            i = tf.image.flip_up_down(image)\n            l = tf.stack([total_points[:, 0],\n                          1 - total_points[:, 1]], axis=1)\n            return i, l\n\n        def no_flip_ud():\n            i = image\n            l = total_points\n            return i, l\n\n        if do_flip_ud:\n            image, total_points = tf.cond(tf.greater(tf.random_uniform([]), 0.5), flip_ud, no_flip_ud)\n\n        if has_bbox:\n            new_bbox = tf.stack([(total_points[0, 0] + total_points[1, 0]) / 2,\n                                 (total_points[0, 1] + total_points[1, 1]) / 2,\n                                 tf.abs(total_points[1, 0] - total_points[0, 0]),\n                                 tf.abs(total_points[1, 1] - total_points[0, 1])], axis=0)\n\n        total_points = tf.reshape(total_points, shape=[-1, ])\n\n    else:\n        # chance flip left right\n        def flip_lr():\n            i = tf.image.flip_left_right(image)\n            return i\n\n        def no_flip_lr():\n            i = image\n            return i\n\n        if do_flip_lr:\n            image = tf.cond(tf.greater(tf.random_uniform([]), 0.5), flip_lr, no_flip_lr)\n\n        # chance flip upside down\n        def flip_ud():\n            i = tf.image.flip_up_down(image)\n            return i\n\n        def no_flip_ud():\n            i = image\n            return i\n\n        if do_flip_ud:\n            image = tf.cond(tf.greater(tf.random_uniform([]), 0.5), flip_ud, no_flip_ud)\n\n    return image, new_bbox, total_points\n'"
utils/tracking_module.py,0,"b""import numpy as np\nimport cv2\n\n\nclass SelfTracker(object):\n    def __init__(self, img_shape, model_input_size):\n        self.img_shape = img_shape\n        self.loss_track = False\n        self.prev_bbox = [0, 0, img_shape[0], img_shape[1]]\n        self.init_center = [img_shape[0]//2, img_shape[1]//2]\n        self.cur_center = [img_shape[0]//2, img_shape[1]//2]\n        self._default_crop_size = 368\n        self.bbox = [0, 0, 0, 0]\n        self.pad_boundary = [0, 0, 0, 0]\n        self.prev_crop_h = self._default_crop_size\n        self.prev_crop_w = self._default_crop_size\n        self.alpha = 0.2\n        self.input_crop_ratio = 1.0\n        self.input_size = float(model_input_size)\n\n\n    def tracking_by_joints(self, full_img, joint_detections=None):\n        if self.loss_track or joint_detections is None:\n            cropped_img = self._crop_image(full_img, self.init_center, (self._default_crop_size, self._default_crop_size))\n            self.input_crop_ratio = self.input_size / max(cropped_img.shape[0], cropped_img.shape[1])\n            resize_img = self._resize_image(cropped_img, self.input_size)\n            return self._pad_image(resize_img, max(resize_img.shape[0], resize_img.shape[1]))\n        else:\n            self.cur_center = np.mean(joint_detections, axis=0, dtype=np.int)\n            crop_h = np.max(joint_detections[:, 0]) - np.min(joint_detections[:, 0])\n            crop_w = np.max(joint_detections[:, 1]) - np.min(joint_detections[:, 1])\n            crop_h = max(int(crop_h), 96)\n            crop_w = max(int(crop_w), 96)\n            crop_h *= 2.0\n            crop_w *= 2.0\n            self.prev_crop_h = self.alpha * crop_h + (1-self.alpha) * self.prev_crop_h\n            self.prev_crop_w = self.alpha * crop_w + (1-self.alpha) * self.prev_crop_w\n\n            cropped_img = self._crop_image(full_img, self.cur_center, (int(self.prev_crop_h), int(self.prev_crop_w)))\n            self.input_crop_ratio = self.input_size / max(cropped_img.shape[0], cropped_img.shape[1])\n            resize_img = self._resize_image(cropped_img, self.input_size)\n\n            pad_size = max(resize_img.shape[0], resize_img.shape[1])\n            return self._pad_image(resize_img, pad_size)\n\n\n    def _resize_image(self, cropped_img, size):\n        h, w, _ = cropped_img.shape\n        if h > w:\n            scale = size / h\n            return cv2.resize(cropped_img, None, fx=scale, fy=scale)\n        else:\n            scale = size / w\n            return cv2.resize(cropped_img, None, fx=scale, fy=scale)\n\n\n    def _crop_image(self, full_img, center, size):\n        h_offset = size[0] % 2\n        w_offset = size[1] % 2\n        self.bbox = [max(0, center[0]-size[0]//2), min(self.img_shape[0], center[0]+size[0]//2+h_offset),\n                max(0, center[1]-size[1]//2), min(self.img_shape[1], center[1]+size[1]//2+w_offset)]\n        return full_img[self.bbox[0]:self.bbox[1], self.bbox[2]:self.bbox[3], :]\n\n\n    def _pad_image(self, img, size):\n        h, w, _ = img.shape\n        if size < h or size < w:\n            raise ValueError('Pad size cannot smaller than original image size')\n\n        pad_h_offset = (size - h) % 2\n        pad_w_offset = (size - w) % 2\n        self.pad_boundary = [(size-h)//2+pad_h_offset, (size-h)//2, (size-w)//2+pad_w_offset, (size-w)//2]\n        return cv2.copyMakeBorder(img, top=self.pad_boundary[0],\n                                  bottom=self.pad_boundary[1],\n                                  left=self.pad_boundary[2],\n                                  right=self.pad_boundary[3], borderType=cv2.BORDER_CONSTANT, value=(128, 128, 128))\n"""
utils/utils.py,0,"b""import cv2\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n# from OpenGL.GL import *\n# from OpenGL.GLU import *\n\n\n\ndef read_square_image(file, cam, boxsize, type):\n    # from file\n    if type == 'IMAGE':\n        oriImg = cv2.imread(file)\n    # from webcam\n    elif type == 'WEBCAM':\n        _, oriImg = cam.read()\n\n    scale = boxsize / (oriImg.shape[0] * 1.0)\n    imageToTest = cv2.resize(oriImg, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_LANCZOS4)\n\n    output_img = np.ones((boxsize, boxsize, 3)) * 128\n\n    if imageToTest.shape[1] < boxsize:\n        offset = imageToTest.shape[1] % 2\n        output_img[:, int(boxsize/2-math.ceil(imageToTest.shape[1]/2)):int(boxsize/2+math.ceil(imageToTest.shape[1]/2)+offset), :] = imageToTest\n    else:\n        output_img = imageToTest[:, int(imageToTest.shape[1]/2-boxsize/2):int(imageToTest.shape[1]/2+boxsize/2), :]\n    return output_img\n\n\ndef resize_pad_img(img, scale, output_size):\n    resized_img = cv2.resize(img, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n    pad_h = (output_size - resized_img.shape[0]) // 2\n    pad_w = (output_size - resized_img.shape[1]) // 2\n    pad_h_offset = (output_size - resized_img.shape[0]) % 2\n    pad_w_offset = (output_size - resized_img.shape[1]) % 2\n    resized_pad_img = np.pad(resized_img, ((pad_w, pad_w+pad_w_offset), (pad_h, pad_h+pad_h_offset), (0, 0)),\n                             mode='constant', constant_values=128)\n\n    return resized_pad_img\n\n\ndef img_white_balance(img, white_ratio):\n    for channel in range(img.shape[2]):\n        channel_max = np.percentile(img[:, :, channel], 100-white_ratio)\n        channel_min = np.percentile(img[:, :, channel], white_ratio)\n        img[:, :, channel] = (channel_max-channel_min) * (img[:, :, channel] / 255.0)\n    return img\n\n\ndef img_white_balance_with_bg(img, bg, white_ratio):\n    for channel in range(img.shape[2]):\n        channel_max = np.percentile(bg[:, :, channel], 100-white_ratio)\n        channel_min = np.percentile(bg[:, :, channel], white_ratio)\n        img[:, :, channel] = (channel_max-channel_min) * (img[:, :, channel] / 255.0)\n    return img\n\n\ndef draw_predicted_heatmap(heatmap, input_size):\n    heatmap_resized = cv2.resize(heatmap, (input_size, input_size))\n\n    output_img = None\n    tmp_concat_img = None\n    h_count = 0\n    for joint_num in range(heatmap_resized.shape[2]):\n        if h_count < 4:\n            tmp_concat_img = np.concatenate((tmp_concat_img, heatmap_resized[:, :, joint_num]), axis=1) \\\n                if tmp_concat_img is not None else heatmap_resized[:, :, joint_num]\n            h_count += 1\n        else:\n            output_img = np.concatenate((output_img, tmp_concat_img), axis=0) if output_img is not None else tmp_concat_img\n            tmp_concat_img = None\n            h_count = 0\n    # last row img\n    if h_count != 0:\n        while h_count < 4:\n            tmp_concat_img = np.concatenate((tmp_concat_img, np.zeros(shape=(input_size, input_size), dtype=np.float32)), axis=1)\n            h_count += 1\n        output_img = np.concatenate((output_img, tmp_concat_img), axis=0)\n\n    # adjust heatmap color\n    output_img = output_img.astype(np.uint8)\n    output_img = cv2.applyColorMap(output_img, cv2.COLORMAP_JET)\n    return output_img\n\n\ndef draw_stages_heatmaps(stage_heatmap_list, orig_img_size):\n\n    output_img = None\n    nStages = len(stage_heatmap_list)\n    nJoints = stage_heatmap_list[0].shape[3]\n    for stage in range(nStages):\n        cur_heatmap = np.squeeze(stage_heatmap_list[0][0, :, :, 0:nJoints-1])\n        cur_heatmap = cv2.resize(cur_heatmap, (orig_img_size, orig_img_size))\n\n        channel_max = np.percentile(cur_heatmap, 99)\n        channel_min = np.percentile(cur_heatmap, 1)\n        cur_heatmap = 255.0 / (channel_max - channel_min) * (cur_heatmap - channel_min)\n        cur_heatmap = np.clip(cur_heatmap, 0, 255)\n\n        cur_heatmap = np.repeat(np.expand_dims(np.amax(cur_heatmap, axis=2), axis=2), 3, axis=2)\n        output_img = np.concatenate((output_img, cur_heatmap), axis=1) if output_img is not None else cur_heatmap\n    return output_img.astype(np.uint8)\n\n\ndef extract_2d_joint_from_heatmap(heatmap, input_size, joints_2d):\n    heatmap_resized = cv2.resize(heatmap, (input_size, input_size))\n\n    for joint_num in range(heatmap_resized.shape[2]):\n        joint_coord = np.unravel_index(np.argmax(heatmap_resized[:, :, joint_num]), (input_size, input_size))\n        joints_2d[joint_num, :] = joint_coord\n\n    return joints_2d\n\n\ndef extract_3d_joints_from_heatmap(joints_2d, x_hm, y_hm, z_hm, input_size, joints_3d):\n\n    for joint_num in range(x_hm.shape[2]):\n        coord_2d_y = joints_2d[joint_num][0]\n        coord_2d_x = joints_2d[joint_num][1]\n\n        # x_hm_resized = cv2.resize(x_hm, (input_size, input_size))\n        # y_hm_resized = cv2.resize(y_hm, (input_size, input_size))\n        # z_hm_resized = cv2.resize(z_hm, (input_size, input_size))\n        # joint_x = x_hm_resized[max(int(coord_2d_x), 1), max(int(coord_2d_y), 1), joint_num] * 100\n        # joint_y = y_hm_resized[max(int(coord_2d_x), 1), max(int(coord_2d_y), 1), joint_num] * 100\n        # joint_z = z_hm_resized[max(int(coord_2d_x), 1), max(int(coord_2d_y), 1), joint_num] * 100\n\n\n        joint_x = x_hm[max(int(coord_2d_x/8), 1), max(int(coord_2d_y/8), 1), joint_num] * 10\n        joint_y = y_hm[max(int(coord_2d_x/8), 1), max(int(coord_2d_y/8), 1), joint_num] * 10\n        joint_z = z_hm[max(int(coord_2d_x/8), 1), max(int(coord_2d_y/8), 1), joint_num] * 10\n        joints_3d[joint_num, 0] = joint_x\n        joints_3d[joint_num, 1] = joint_y\n        joints_3d[joint_num, 2] = joint_z\n    joints_3d -= joints_3d[14, :]\n\n    return joints_3d\n\ndef draw_limbs_2d(img, joints_2d, limb_parents):\n    for limb_num in range(len(limb_parents)-1):\n        x1 = joints_2d[limb_num, 0]\n        y1 = joints_2d[limb_num, 1]\n        x2 = joints_2d[limb_parents[limb_num], 0]\n        y2 = joints_2d[limb_parents[limb_num], 1]\n        length = ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5\n        # if length < 10000 and length > 5:\n        deg = math.degrees(math.atan2(x1 - x2, y1 - y2))\n        polygon = cv2.ellipse2Poly((int((y1 + y2) / 2), int((x1 + x2) / 2)),\n                                   (int(length / 2), 3),\n                                   int(deg),\n                                   0, 360, 1)\n        cv2.fillConvexPoly(img, polygon, color=(0,255,0))\n    return img\n\ndef draw_limbs_3d(joints_3d, limb_parents, ax):\n\n    for i in range(joints_3d.shape[0]):\n        x_pair = [joints_3d[i, 0], joints_3d[limb_parents[i], 0]]\n        y_pair = [joints_3d[i, 1], joints_3d[limb_parents[i], 1]]\n        z_pair = [joints_3d[i, 2], joints_3d[limb_parents[i], 2]]\n        ax.plot(x_pair, y_pair, zs=z_pair, linewidth=3)\n\n\ndef draw_limb_3d_gl(joints_3d, limb_parents):\n\n    glLineWidth(2)\n    glBegin(GL_LINES)\n    glColor3f(1,0,0)\n    glVertex3fv((0,0,0))\n    glVertex3fv((100,0,0))\n    glColor3f(0,1,0)\n    glVertex3fv((0,0,0))\n    glVertex3fv((0,100,0))\n    glColor3f(0,0,1)\n    glVertex3fv((0,0,0))\n    glVertex3fv((0,0,100))\n    glEnd()\n\n    glColor3f(1,1,1)\n    glBegin(GL_LINES)\n    for i in range(joints_3d.shape[0]):\n        glVertex3fv((joints_3d[i, 0], joints_3d[i, 1], joints_3d[i, 2]))\n        glVertex3fv((joints_3d[limb_parents[i], 0], joints_3d[limb_parents[i], 1], joints_3d[limb_parents[i], 2]))\n    glEnd()\n\n    # glBegin(GL_TRIANGLES)\n    # glVertex3f(0, 100, 0)\n    # glVertex3f(100, 0, 50)\n    # glVertex3f(0, -100, 100)\n    # glEnd()\n\n\ndef draw_float_range_img(img):\n    tmp_min = np.min(img)\n    tmp_max = np.max(img)\n    img = cv2.convertScaleAbs(img, None, 255.0 / (tmp_max - tmp_min))\n    img = cv2.applyColorMap(img, cv2.COLORMAP_JET)\n    return img.astype(np.uint8)\n\n\n\n\n\n\n\n\n\n\n\n\n"""
models/nets/CPM.py,0,"b""from abc import ABCMeta, abstractmethod, abstractproperty\n\nclass CPM(object):\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def __init__(self, input_size, heatmap_size, stages, joints, img_type='RGB'):\n        pass\n\n    @abstractmethod\n    def _build_model(self):\n        pass\n\n    @abstractmethod\n    def build_loss(self, lr, lr_decay_rate, lr_decay_step, optimizer='Adam'):\n        pass\n\n\n\n"""
models/nets/__init__.py,0,b''
models/nets/cpm_body.py,127,"b""import tensorflow as tf\nimport pickle\n\n\nclass CPM_Model(object):\n    def __init__(self, stages, joints):\n        self.stages = stages\n        self.stage_heatmap = []\n        self.stage_loss = [0] * stages\n        self.total_loss = 0\n        self.input_image = None\n        self.center_map = None\n        self.gt_heatmap = None\n        self.learning_rate = 0\n        self.merged_summary = None\n        self.joints = joints\n        self.batch_size = 0\n\n    def build_model(self, input_image, center_map, batch_size):\n        self.batch_size = batch_size\n        self.input_image = input_image\n        self.center_map = center_map\n        with tf.variable_scope('pooled_center_map'):\n            self.center_map = tf.layers.average_pooling2d(inputs=self.center_map,\n                                                          pool_size=[9, 9],\n                                                          strides=[8, 8],\n                                                          padding='same',\n                                                          name='center_map')\n        with tf.variable_scope('sub_stages'):\n            sub_conv1 = tf.layers.conv2d(inputs=input_image,\n                                         filters=64,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv1')\n            sub_conv2 = tf.layers.conv2d(inputs=sub_conv1,\n                                         filters=64,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv2')\n            sub_pool1 = tf.layers.max_pooling2d(inputs=sub_conv2,\n                                                pool_size=[2, 2],\n                                                strides=2,\n                                                padding='same',\n                                                name='sub_pool1')\n            sub_conv3 = tf.layers.conv2d(inputs=sub_pool1,\n                                         filters=128,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv3')\n            sub_conv4 = tf.layers.conv2d(inputs=sub_conv3,\n                                         filters=128,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv4')\n            sub_pool2 = tf.layers.max_pooling2d(inputs=sub_conv4,\n                                                pool_size=[2, 2],\n                                                strides=2,\n                                                padding='same',\n                                                name='sub_pool2')\n            sub_conv5 = tf.layers.conv2d(inputs=sub_pool2,\n                                         filters=256,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv5')\n            sub_conv6 = tf.layers.conv2d(inputs=sub_conv5,\n                                         filters=256,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv6')\n            sub_conv7 = tf.layers.conv2d(inputs=sub_conv6,\n                                         filters=256,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv7')\n            sub_conv8 = tf.layers.conv2d(inputs=sub_conv7,\n                                         filters=256,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv8')\n            sub_pool3 = tf.layers.max_pooling2d(inputs=sub_conv8,\n                                                pool_size=[2, 2],\n                                                strides=2,\n                                                padding='same',\n                                                name='sub_pool3')\n            sub_conv9 = tf.layers.conv2d(inputs=sub_pool3,\n                                         filters=512,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv9')\n            sub_conv10 = tf.layers.conv2d(inputs=sub_conv9,\n                                          filters=512,\n                                          kernel_size=[3, 3],\n                                          strides=[1, 1],\n                                          padding='same',\n                                          activation=tf.nn.relu,\n                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                          name='sub_conv10')\n            sub_conv11 = tf.layers.conv2d(inputs=sub_conv10,\n                                          filters=256,\n                                          kernel_size=[3, 3],\n                                          strides=[1, 1],\n                                          padding='same',\n                                          activation=tf.nn.relu,\n                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                          name='sub_conv11')\n            sub_conv12 = tf.layers.conv2d(inputs=sub_conv11,\n                                          filters=256,\n                                          kernel_size=[3, 3],\n                                          strides=[1, 1],\n                                          padding='same',\n                                          activation=tf.nn.relu,\n                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                          name='sub_conv12')\n            sub_conv13 = tf.layers.conv2d(inputs=sub_conv12,\n                                          filters=256,\n                                          kernel_size=[3, 3],\n                                          strides=[1, 1],\n                                          padding='same',\n                                          activation=tf.nn.relu,\n                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                          name='sub_conv13')\n            sub_conv14 = tf.layers.conv2d(inputs=sub_conv13,\n                                          filters=256,\n                                          kernel_size=[3, 3],\n                                          strides=[1, 1],\n                                          padding='same',\n                                          activation=tf.nn.relu,\n                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                          name='sub_conv14')\n\n            self.sub_stage_img_feature = tf.layers.conv2d(inputs=sub_conv14,\n                                                          filters=128,\n                                                          kernel_size=[3, 3],\n                                                          strides=[1, 1],\n                                                          padding='same',\n                                                          activation=tf.nn.relu,\n                                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                                          name='sub_stage_img_feature')\n\n        with tf.variable_scope('stage_1'):\n            conv1 = tf.layers.conv2d(inputs=self.sub_stage_img_feature,\n                                     filters=512,\n                                     kernel_size=[1, 1],\n                                     strides=[1, 1],\n                                     padding='same',\n                                     activation=tf.nn.relu,\n                                     kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                     name='conv1')\n            self.stage_heatmap.append(tf.layers.conv2d(inputs=conv1,\n                                                       filters=self.joints,\n                                                       kernel_size=[1, 1],\n                                                       strides=[1, 1],\n                                                       padding='same',\n                                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                                       name='stage_heatmap'))\n        for stage in range(2, self.stages + 1):\n            self._middle_conv(stage)\n\n    def _middle_conv(self, stage):\n        with tf.variable_scope('stage_' + str(stage)):\n            self.current_featuremap = tf.concat([self.stage_heatmap[stage - 2],\n                                                 self.sub_stage_img_feature,\n                                                 self.center_map,\n                                                 ],\n                                                axis=3)\n            mid_conv1 = tf.layers.conv2d(inputs=self.current_featuremap,\n                                         filters=128,\n                                         kernel_size=[7, 7],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='mid_conv1')\n            mid_conv2 = tf.layers.conv2d(inputs=mid_conv1,\n                                         filters=128,\n                                         kernel_size=[7, 7],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='mid_conv2')\n            mid_conv3 = tf.layers.conv2d(inputs=mid_conv2,\n                                         filters=128,\n                                         kernel_size=[7, 7],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='mid_conv3')\n            mid_conv4 = tf.layers.conv2d(inputs=mid_conv3,\n                                         filters=128,\n                                         kernel_size=[7, 7],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='mid_conv4')\n            mid_conv5 = tf.layers.conv2d(inputs=mid_conv4,\n                                         filters=128,\n                                         kernel_size=[7, 7],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='mid_conv5')\n            mid_conv6 = tf.layers.conv2d(inputs=mid_conv5,\n                                         filters=128,\n                                         kernel_size=[1, 1],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='mid_conv6')\n            self.current_heatmap = tf.layers.conv2d(inputs=mid_conv6,\n                                                    filters=self.joints,\n                                                    kernel_size=[1, 1],\n                                                    strides=[1, 1],\n                                                    padding='same',\n                                                    kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                                    name='mid_conv7')\n            self.stage_heatmap.append(self.current_heatmap)\n\n    def build_loss(self, gt_heatmap, lr, lr_decay_rate, lr_decay_step):\n        self.gt_heatmap = gt_heatmap\n        self.total_loss = 0\n        self.learning_rate = lr\n        self.lr_decay_rate = lr_decay_rate\n        self.lr_decay_step = lr_decay_step\n\n        for stage in range(self.stages):\n            with tf.variable_scope('stage' + str(stage + 1) + '_loss'):\n                self.stage_loss[stage] = tf.nn.l2_loss(self.stage_heatmap[stage] - self.gt_heatmap,\n                                                       name='l2_loss') / self.batch_size\n            tf.summary.scalar('stage' + str(stage + 1) + '_loss', self.stage_loss[stage])\n\n        with tf.variable_scope('total_loss'):\n            for stage in range(self.stages):\n                self.total_loss += self.stage_loss[stage]\n            tf.summary.scalar('total loss', self.total_loss)\n\n        with tf.variable_scope('train'):\n            self.global_step = tf.contrib.framework.get_or_create_global_step()\n\n            self.lr = tf.train.exponential_decay(self.learning_rate,\n                                                 global_step=self.global_step,\n                                                 decay_rate=self.lr_decay_rate,\n                                                 decay_steps=self.lr_decay_step)\n            tf.summary.scalar('learning rate', self.lr)\n\n            self.train_op = tf.contrib.layers.optimize_loss(loss=self.total_loss,\n                                                            global_step=self.global_step,\n                                                            learning_rate=self.lr,\n                                                            optimizer='Adam')\n        self.merged_summary = tf.summary.merge_all()\n\n    def load_weights_from_file(self, weight_file_path, sess, finetune=True):\n        weights = pickle.load(open(weight_file_path, 'rb'), encoding='latin1')\n\n        with tf.variable_scope('', reuse=True):\n            ## Pre stage conv\n            # conv1\n            for layer in range(1, 3):\n                conv_kernel = tf.get_variable('sub_stages/sub_conv' + str(layer) + '/kernel')\n                conv_bias = tf.get_variable('sub_stages/sub_conv' + str(layer) + '/bias')\n\n                loaded_kernel = weights['conv1_' + str(layer)]\n                loaded_bias = weights['conv1_' + str(layer) + '_b']\n\n                sess.run(tf.assign(conv_kernel, loaded_kernel))\n                sess.run(tf.assign(conv_bias, loaded_bias))\n\n            # conv2\n            for layer in range(1, 3):\n                conv_kernel = tf.get_variable('sub_stages/sub_conv' + str(layer + 2) + '/kernel')\n                conv_bias = tf.get_variable('sub_stages/sub_conv' + str(layer + 2) + '/bias')\n\n                loaded_kernel = weights['conv2_' + str(layer)]\n                loaded_bias = weights['conv2_' + str(layer) + '_b']\n\n                sess.run(tf.assign(conv_kernel, loaded_kernel))\n                sess.run(tf.assign(conv_bias, loaded_bias))\n\n            # conv3\n            for layer in range(1, 5):\n                conv_kernel = tf.get_variable('sub_stages/sub_conv' + str(layer + 4) + '/kernel')\n                conv_bias = tf.get_variable('sub_stages/sub_conv' + str(layer + 4) + '/bias')\n\n                loaded_kernel = weights['conv3_' + str(layer)]\n                loaded_bias = weights['conv3_' + str(layer) + '_b']\n\n                sess.run(tf.assign(conv_kernel, loaded_kernel))\n                sess.run(tf.assign(conv_bias, loaded_bias))\n\n            # conv4\n            for layer in range(1, 3):\n                conv_kernel = tf.get_variable('sub_stages/sub_conv' + str(layer + 8) + '/kernel')\n                conv_bias = tf.get_variable('sub_stages/sub_conv' + str(layer + 8) + '/bias')\n\n                loaded_kernel = weights['conv4_' + str(layer)]\n                loaded_bias = weights['conv4_' + str(layer) + '_b']\n\n                sess.run(tf.assign(conv_kernel, loaded_kernel))\n                sess.run(tf.assign(conv_bias, loaded_bias))\n\n            # conv4_CPM\n            for layer in range(1, 5):\n                conv_kernel = tf.get_variable('sub_stages/sub_conv' + str(layer + 10) + '/kernel')\n                conv_bias = tf.get_variable('sub_stages/sub_conv' + str(layer + 10) + '/bias')\n\n                loaded_kernel = weights['conv4_' + str(2 + layer) + '_CPM']\n                loaded_bias = weights['conv4_' + str(2 + layer) + '_CPM_b']\n\n                sess.run(tf.assign(conv_kernel, loaded_kernel))\n                sess.run(tf.assign(conv_bias, loaded_bias))\n\n            # conv5_3_CPM\n            conv_kernel = tf.get_variable('sub_stages/sub_stage_img_feature/kernel')\n            conv_bias = tf.get_variable('sub_stages/sub_stage_img_feature/bias')\n\n            loaded_kernel = weights['conv4_7_CPM']\n            loaded_bias = weights['conv4_7_CPM_b']\n\n            sess.run(tf.assign(conv_kernel, loaded_kernel))\n            sess.run(tf.assign(conv_bias, loaded_bias))\n\n            ## stage 1\n            conv_kernel = tf.get_variable('stage_1/conv1/kernel')\n            conv_bias = tf.get_variable('stage_1/conv1/bias')\n\n            loaded_kernel = weights['conv5_1_CPM']\n            loaded_bias = weights['conv5_1_CPM_b']\n\n            sess.run(tf.assign(conv_kernel, loaded_kernel))\n            sess.run(tf.assign(conv_bias, loaded_bias))\n\n            if finetune != True:\n                conv_kernel = tf.get_variable('stage_1/stage_heatmap/kernel')\n                conv_bias = tf.get_variable('stage_1/stage_heatmap/bias')\n\n                loaded_kernel = weights['conv5_2_CPM']\n                loaded_bias = weights['conv5_2_CPM_b']\n\n                sess.run(tf.assign(conv_kernel, loaded_kernel))\n                sess.run(tf.assign(conv_bias, loaded_bias))\n\n                ## stage 2 and behind\n                for stage in range(2, self.stages + 1):\n                    for layer in range(1, 8):\n                        conv_kernel = tf.get_variable('stage_' + str(stage) + '/mid_conv' + str(layer) + '/kernel')\n                        conv_bias = tf.get_variable('stage_' + str(stage) + '/mid_conv' + str(layer) + '/bias')\n\n                        loaded_kernel = weights['Mconv' + str(layer) + '_stage' + str(stage)]\n                        loaded_bias = weights['Mconv' + str(layer) + '_stage' + str(stage) + '_b']\n\n                        sess.run(tf.assign(conv_kernel, loaded_kernel))\n                        sess.run(tf.assign(conv_bias, loaded_bias))\n"""
models/nets/cpm_body_slim.py,61,"b""import pickle\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\n\nclass CPM_Model(object):\n    def __init__(self, stages, joints):\n        self.stages = stages\n        self.stage_heatmap = []\n        self.stage_loss = [0] * stages\n        self.total_loss = 0\n        self.input_image = None\n        self.center_map = None\n        self.gt_heatmap = None\n        self.learning_rate = 0\n        self.merged_summary = None\n        self.joints = joints\n        self.batch_size = 0\n\n    def build_model(self, input_image, center_map, batch_size):\n        self.batch_size = batch_size\n        self.input_image = input_image\n        self.center_map = center_map\n        with tf.variable_scope('pooled_center_map'):\n            self.center_map = slim.avg_pool2d(self.center_map,\n                                              [9, 9], stride=8,\n                                              padding='SAME',\n                                              scope='center_map')\n        with slim.arg_scope([slim.conv2d],\n                            padding='SAME',\n                            activation_fn=tf.nn.relu,\n                            weights_initializer=tf.contrib.layers.xavier_initializer()):\n            with tf.variable_scope('sub_stages'):\n                net = slim.conv2d(input_image, 64, [3, 3], scope='sub_conv1')\n                net = slim.conv2d(net, 64, [3, 3], scope='sub_conv2')\n                net = slim.max_pool2d(net, [2, 2], padding='SAME', scope='sub_pool1')\n                net = slim.conv2d(net, 128, [3, 3], scope='sub_conv3')\n                net = slim.conv2d(net, 128, [3, 3], scope='sub_conv4')\n                net = slim.max_pool2d(net, [2, 2], padding='SAME', scope='sub_pool2')\n                net = slim.conv2d(net, 256, [3, 3], scope='sub_conv5')\n                net = slim.conv2d(net, 256, [3, 3], scope='sub_conv6')\n                net = slim.conv2d(net, 256, [3, 3], scope='sub_conv7')\n                net = slim.conv2d(net, 256, [3, 3], scope='sub_conv8')\n                net = slim.max_pool2d(net, [2, 2], padding='SAME', scope='sub_pool3')\n                net = slim.conv2d(net, 512, [3, 3], scope='sub_conv9')\n                net = slim.conv2d(net, 512, [3, 3], scope='sub_conv10')\n                net = slim.conv2d(net, 256, [3, 3], scope='sub_conv11')\n                net = slim.conv2d(net, 256, [3, 3], scope='sub_conv12')\n                net = slim.conv2d(net, 256, [3, 3], scope='sub_conv13')\n                net = slim.conv2d(net, 256, [3, 3], scope='sub_conv14')\n                self.sub_stage_img_feature = slim.conv2d(net, 128, [3, 3],\n                                                         scope='sub_stage_img_feature')\n\n            with tf.variable_scope('stage_1'):\n                conv1 = slim.conv2d(self.sub_stage_img_feature, 512, [1, 1],\n                                    scope='conv1')\n                self.stage_heatmap.append(slim.conv2d(conv1, self.joints, [1, 1],\n                                                      scope='stage_heatmap'))\n\n            for stage in range(2, self.stages+1):\n                self._middle_conv(stage)\n\n    def _middle_conv(self, stage):\n        with tf.variable_scope('stage_' + str(stage)):\n            self.current_featuremap = tf.concat([self.stage_heatmap[stage-2],\n                                                 self.sub_stage_img_feature,\n                                                 self.center_map],\n                                                axis=3)\n            with slim.arg_scope([slim.conv2d],\n                                padding='SAME',\n                                activation_fn=tf.nn.relu,\n                                weights_initializer=tf.contrib.layers.xavier_initializer()):\n                mid_net = slim.conv2d(self.current_featuremap, 128, [7, 7], scope='mid_conv1')\n                mid_net = slim.conv2d(mid_net, 128, [7, 7], scope='mid_conv2')\n                mid_net = slim.conv2d(mid_net, 128, [7, 7], scope='mid_conv3')\n                mid_net = slim.conv2d(mid_net, 128, [7, 7], scope='mid_conv4')\n                mid_net = slim.conv2d(mid_net, 128, [7, 7], scope='mid_conv5')\n                mid_net = slim.conv2d(mid_net, 128, [1, 1], scope='mid_conv6')\n                self.current_heatmap = slim.conv2d(mid_net, self.joints, [1, 1],\n                                                   scope='mid_conv7')\n                self.stage_heatmap.append(self.current_heatmap)\n\n    def build_loss(self, gt_heatmap, lr, lr_decay_rate, lr_decay_step):\n        self.gt_heatmap = gt_heatmap\n        self.total_loss = 0\n        self.learning_rate = lr\n        self.lr_decay_rate = lr_decay_rate\n        self.lr_decay_step = lr_decay_step\n\n        for stage in range(self.stages):\n            with tf.variable_scope('stage' + str(stage+1) + '_loss'):\n                self.stage_loss[stage] = tf.nn.l2_loss(self.stage_heatmap[stage] - self.gt_heatmap,\n                                                       name='l2_loss') / self.batch_size\n            tf.summary.scalar('stage' + str(stage+1) + '_loss', self.stage_loss[stage])\n\n        with tf.variable_scope('total_loss'):\n            for stage in range(self.stages):\n                self.total_loss += self.stage_loss[stage]\n            tf.summary.scalar('total loss', self.total_loss)\n\n        with tf.variable_scope('train'):\n            self.global_step = tf.contrib.framework.get_or_creat_global_step()\n\n            self.lr = tf.train.exponential_decay(self.learning_rate,\n                                                 global_step=self.global_step,\n                                                 decay_rate=self.lr_decay_rate,\n                                                 decay_steps=self.lr_decay_step)\n            tf.summary.scalar('learning rate', self.lr)\n\n            self.train_op = tf.contrib.layers.optimize_loss(loss=self.total_loss,\n                                                            global_step=self.global_step,\n                                                            learning_rate=self.lr,\n                                                            optimizer='Adam')\n        self.merged_summary = tf.summary.merge_all()\n\n    def load_weights_from_file(self, weight_file_path, sess, finetune=True):\n        weights = pickle.load(open(weight_file_path, 'rb'), encoding='latin1')\n\n        with tf.variable_scope('', reuse=True):\n            ## Pre stage conv\n            # for layer in range(1, 15):\n            #     conv_weights = tf.get_variable('sub_stages/sub_conv' + str(layer) + '/weights')\n            #     conv_biases = tf.get_variable('sub_stages/sub_conv' + str(layer) + '/biases')\n            #\n            #     loaded_weights = weights['sub_conv' + str(layer)]\n            #     loaded_biases = weights['sub_conv' + str(layer)]\n            #\n            #     sess.run(tf.assign(conv_weights, loaded_weights))\n            #     sess.run(tf.assign(conv_biases, loaded_biases))\n\n            # conv1\n            for layer in range(1, 3):\n                conv_weights = tf.get_variable('sub_stages/sub_conv' + str(layer) + '/weights')\n                conv_biases = tf.get_variable('sub_stages/sub_conv' + str(layer) + '/biases')\n\n                loaded_weights = weights['conv1_' + str(layer)]\n                loaded_biases = weights['conv1_' + str(layer) + '_b']\n\n                sess.run(tf.assign(conv_weights, loaded_weights))\n                sess.run(tf.assign(conv_biases, loaded_biases))\n\n            # conv2\n            for layer in range(1, 3):\n                conv_weights = tf.get_variable('sub_stages/sub_conv' + str(layer + 2) + '/weights')\n                conv_biases = tf.get_variable('sub_stages/sub_conv' + str(layer + 2) + '/biases')\n\n                loaded_weights = weights['conv2_' + str(layer)]\n                loaded_biases = weights['conv2_' + str(layer) + '_b']\n\n                sess.run(tf.assign(conv_weights, loaded_weights))\n                sess.run(tf.assign(conv_biases, loaded_biases))\n\n            # conv3\n            for layer in range(1, 5):\n                conv_weights = tf.get_variable('sub_stages/sub_conv' + str(layer + 4) + '/weights')\n                conv_biases = tf.get_variable('sub_stages/sub_conv' + str(layer + 4) + '/biases')\n\n                loaded_weights = weights['conv3_' + str(layer)]\n                loaded_biases = weights['conv3_' + str(layer) + '_b']\n\n                sess.run(tf.assign(conv_weights, loaded_weights))\n                sess.run(tf.assign(conv_biases, loaded_biases))\n\n            # conv4\n            for layer in range(1, 3):\n                conv_weights = tf.get_variable('sub_stages/sub_conv' + str(layer + 8) + '/weights')\n                conv_biases = tf.get_variable('sub_stages/sub_conv' + str(layer + 8) + '/biases')\n\n                loaded_weights = weights['conv4_' + str(layer)]\n                loaded_biases = weights['conv4_' + str(layer) + '_b']\n\n                sess.run(tf.assign(conv_weights, loaded_weights))\n                sess.run(tf.assign(conv_biases, loaded_biases))\n\n            # conv4_CPM\n            for layer in range(1, 5):\n                conv_weights = tf.get_variable('sub_stages/sub_conv' + str(layer + 10) + '/weights')\n                conv_biases = tf.get_variable('sub_stages/sub_conv' + str(layer + 10) + '/biases')\n\n                loaded_weights = weights['conv4_' + str(2 + layer) + '_CPM']\n                loaded_biases = weights['conv4_' + str(2 + layer) + '_CPM_b']\n\n                sess.run(tf.assign(conv_weights, loaded_weights))\n                sess.run(tf.assign(conv_biases, loaded_biases))\n\n            # conv5_3_CPM\n            conv_weights = tf.get_variable('sub_stages/sub_stage_img_feature/weights')\n            conv_biases = tf.get_variable('sub_stages/sub_stage_img_feature/biases')\n\n            loaded_weights = weights['conv4_7_CPM']\n            loaded_biases = weights['conv4_7_CPM_b']\n\n            sess.run(tf.assign(conv_weights, loaded_weights))\n            sess.run(tf.assign(conv_biases, loaded_biases))\n\n            ## stage 1\n            conv_weights = tf.get_variable('stage_1/conv1/weights')\n            conv_biases = tf.get_variable('stage_1/conv1/biases')\n\n            loaded_weights = weights['conv5_1_CPM']\n            loaded_biases = weights['conv5_1_CPM_b']\n\n            sess.run(tf.assign(conv_weights, loaded_weights))\n            sess.run(tf.assign(conv_biases, loaded_biases))\n\n            if finetune != True:\n                conv_weights = tf.get_variable('stage_1/stage_heatmap/weights')\n                conv_biases = tf.get_variable('stage_1/stage_heatmap/biases')\n\n                loaded_weights = weights['conv5_2_CPM']\n                loaded_biases = weights['conv5_2_CPM_b']\n\n                sess.run(tf.assign(conv_weights, loaded_weights))\n                sess.run(tf.assign(conv_biases, loaded_biases))\n\n                ## stage 2 and behind\n                for stage in range(2, self.stages + 1):\n                    for layer in range(1, 8):\n                        conv_weights = tf.get_variable('stage_' + str(stage) + '/mid_conv' + str(layer) + '/weights')\n                        conv_biases = tf.get_variable('stage_' + str(stage) + '/mid_conv' + str(layer) + '/biases')\n\n                        loaded_weights = weights['Mconv' + str(layer) + '_stage' + str(stage)]\n                        loaded_biases = weights['Mconv' + str(layer) + '_stage' + str(stage) + '_b']\n\n                        sess.run(tf.assign(conv_weights, loaded_weights))\n                        sess.run(tf.assign(conv_biases, loaded_biases))\n"""
models/nets/cpm_hand.py,133,"b""import tensorflow as tf\nimport pickle\nfrom models.nets.CPM import CPM\n\n\nclass CPM_Model(CPM):\n    def __init__(self, input_size, heatmap_size, stages, joints, img_type='RGB', is_training=True):\n        self.stages = stages\n        self.stage_heatmap = []\n        self.stage_loss = [0 for _ in range(stages)]\n        self.total_loss = 0\n        self.input_image = None\n        self.center_map = None\n        self.gt_heatmap = None\n        self.init_lr = 0\n        self.merged_summary = None\n        self.joints = joints\n        self.batch_size = 0\n        self.inference_type = 'Train'\n\n        if img_type == 'RGB':\n            self.input_images = tf.placeholder(dtype=tf.float32,\n                                               shape=(None, input_size, input_size, 3),\n                                               name='input_placeholder')\n        elif img_type == 'GRAY':\n            self.input_images = tf.placeholder(dtype=tf.float32,\n                                               shape=(None, input_size, input_size, 1),\n                                               name='input_placeholder')\n        self.cmap_placeholder = tf.placeholder(dtype=tf.float32,\n                                               shape=(None, input_size, input_size, 1),\n                                               name='cmap_placeholder')\n        self.gt_hmap_placeholder = tf.placeholder(dtype=tf.float32,\n                                                  shape=(None, heatmap_size, heatmap_size, joints + 1),\n                                                  name='gt_hmap_placeholder')\n        self._build_model()\n\n    def _build_model(self):\n        with tf.variable_scope('pooled_center_map'):\n            self.center_map = tf.layers.average_pooling2d(inputs=self.cmap_placeholder,\n                                                          pool_size=[9, 9],\n                                                          strides=[8, 8],\n                                                          padding='same',\n                                                          name='center_map')\n        with tf.variable_scope('sub_stages'):\n            sub_conv1 = tf.layers.conv2d(inputs=self.input_images,\n                                         filters=64,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv1')\n\n            sub_conv2 = tf.layers.conv2d(inputs=sub_conv1,\n                                         filters=64,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv2')\n            sub_pool1 = tf.layers.max_pooling2d(inputs=sub_conv2,\n                                                pool_size=[2, 2],\n                                                strides=2,\n                                                padding='valid',\n                                                name='sub_pool1')\n            sub_conv3 = tf.layers.conv2d(inputs=sub_pool1,\n                                         filters=128,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv3')\n            sub_conv4 = tf.layers.conv2d(inputs=sub_conv3,\n                                         filters=128,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv4')\n            sub_pool2 = tf.layers.max_pooling2d(inputs=sub_conv4,\n                                                pool_size=[2, 2],\n                                                strides=2,\n                                                padding='valid',\n                                                name='sub_pool2')\n            sub_conv5 = tf.layers.conv2d(inputs=sub_pool2,\n                                         filters=256,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv5')\n            sub_conv6 = tf.layers.conv2d(inputs=sub_conv5,\n                                         filters=256,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv6')\n            sub_conv7 = tf.layers.conv2d(inputs=sub_conv6,\n                                         filters=256,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv7')\n            sub_conv8 = tf.layers.conv2d(inputs=sub_conv7,\n                                         filters=256,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv8')\n            sub_pool3 = tf.layers.max_pooling2d(inputs=sub_conv8,\n                                                pool_size=[2, 2],\n                                                strides=2,\n                                                padding='valid',\n                                                name='sub_pool3')\n            sub_conv9 = tf.layers.conv2d(inputs=sub_pool3,\n                                         filters=512,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv9')\n            sub_conv10 = tf.layers.conv2d(inputs=sub_conv9,\n                                          filters=512,\n                                          kernel_size=[3, 3],\n                                          strides=[1, 1],\n                                          padding='same',\n                                          activation=tf.nn.relu,\n                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                          name='sub_conv10')\n            sub_conv11 = tf.layers.conv2d(inputs=sub_conv10,\n                                          filters=512,\n                                          kernel_size=[3, 3],\n                                          strides=[1, 1],\n                                          padding='same',\n                                          activation=tf.nn.relu,\n                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                          name='sub_conv11')\n            sub_conv12 = tf.layers.conv2d(inputs=sub_conv11,\n                                          filters=512,\n                                          kernel_size=[3, 3],\n                                          strides=[1, 1],\n                                          padding='same',\n                                          activation=tf.nn.relu,\n                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                          name='sub_conv12')\n            sub_conv13 = tf.layers.conv2d(inputs=sub_conv12,\n                                          filters=512,\n                                          kernel_size=[3, 3],\n                                          strides=[1, 1],\n                                          padding='same',\n                                          activation=tf.nn.relu,\n                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                          name='sub_conv13')\n            sub_conv14 = tf.layers.conv2d(inputs=sub_conv13,\n                                          filters=512,\n                                          kernel_size=[3, 3],\n                                          strides=[1, 1],\n                                          padding='same',\n                                          activation=tf.nn.relu,\n                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                          name='sub_conv14')\n            self.sub_stage_img_feature = tf.layers.conv2d(inputs=sub_conv14,\n                                                          filters=128,\n                                                          kernel_size=[3, 3],\n                                                          strides=[1, 1],\n                                                          padding='same',\n                                                          activation=tf.nn.relu,\n                                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                                          name='sub_stage_img_feature')\n\n        with tf.variable_scope('stage_1'):\n            conv1 = tf.layers.conv2d(inputs=self.sub_stage_img_feature,\n                                     filters=512,\n                                     kernel_size=[1, 1],\n                                     strides=[1, 1],\n                                     padding='valid',\n                                     activation=tf.nn.relu,\n                                     kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                     name='conv1')\n            self.stage_heatmap.append(tf.layers.conv2d(inputs=conv1,\n                                                       filters=self.joints+1,\n                                                       kernel_size=[1, 1],\n                                                       strides=[1, 1],\n                                                       padding='valid',\n                                                       activation=None,\n                                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                                       name='stage_heatmap'))\n        for stage in range(2, self.stages + 1):\n            self._middle_conv(stage)\n\n    def _middle_conv(self, stage):\n        with tf.variable_scope('stage_' + str(stage)):\n            self.current_featuremap = tf.concat([self.stage_heatmap[stage - 2],\n                                                 self.sub_stage_img_feature,\n                                                 # self.center_map],\n                                                 ],\n                                                axis=3)\n            mid_conv1 = tf.layers.conv2d(inputs=self.current_featuremap,\n                                         filters=128,\n                                         kernel_size=[7, 7],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='mid_conv1')\n            mid_conv2 = tf.layers.conv2d(inputs=mid_conv1,\n                                         filters=128,\n                                         kernel_size=[7, 7],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='mid_conv2')\n            mid_conv3 = tf.layers.conv2d(inputs=mid_conv2,\n                                         filters=128,\n                                         kernel_size=[7, 7],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='mid_conv3')\n            mid_conv4 = tf.layers.conv2d(inputs=mid_conv3,\n                                         filters=128,\n                                         kernel_size=[7, 7],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='mid_conv4')\n            mid_conv5 = tf.layers.conv2d(inputs=mid_conv4,\n                                         filters=128,\n                                         kernel_size=[7, 7],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='mid_conv5')\n            mid_conv6 = tf.layers.conv2d(inputs=mid_conv5,\n                                         filters=128,\n                                         kernel_size=[1, 1],\n                                         strides=[1, 1],\n                                         padding='valid',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='mid_conv6')\n            self.current_heatmap = tf.layers.conv2d(inputs=mid_conv6,\n                                                    filters=self.joints+1,\n                                                    kernel_size=[1, 1],\n                                                    strides=[1, 1],\n                                                    padding='valid',\n                                                    activation=None,\n                                                    kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                                    name='mid_conv7')\n            self.stage_heatmap.append(self.current_heatmap)\n\n    def build_loss(self, lr, lr_decay_rate, lr_decay_step, optimizer='Adam'):\n        self.total_loss = 0\n        self.total_loss_eval = 0\n        self.init_lr = lr\n        self.lr_decay_rate = lr_decay_rate\n        self.lr_decay_step = lr_decay_step\n        self.optimizer = optimizer\n        self.batch_size = tf.cast(tf.shape(self.input_images)[0], dtype=tf.float32)\n\n\n        for stage in range(self.stages):\n            with tf.variable_scope('stage' + str(stage + 1) + '_loss'):\n                self.stage_loss[stage] = tf.nn.l2_loss(self.stage_heatmap[stage] - self.gt_hmap_placeholder,\n                                                       name='l2_loss') / self.batch_size\n            tf.summary.scalar('stage' + str(stage + 1) + '_loss', self.stage_loss[stage])\n\n        with tf.variable_scope('total_loss'):\n            for stage in range(self.stages):\n                self.total_loss += self.stage_loss[stage]\n            tf.summary.scalar('total loss train', self.total_loss)\n\n        with tf.variable_scope('total_loss_eval'):\n            for stage in range(self.stages):\n                self.total_loss_eval += self.stage_loss[stage]\n            tf.summary.scalar('total loss eval', self.total_loss)\n\n        with tf.variable_scope('train'):\n            self.global_step = tf.contrib.framework.get_or_create_global_step()\n\n            self.cur_lr = tf.train.exponential_decay(self.init_lr,\n                                                 global_step=self.global_step,\n                                                 decay_rate=self.lr_decay_rate,\n                                                 decay_steps=self.lr_decay_step)\n            tf.summary.scalar('global learning rate', self.cur_lr)\n\n            self.train_op = tf.contrib.layers.optimize_loss(loss=self.total_loss,\n                                                            global_step=self.global_step,\n                                                            learning_rate=self.cur_lr,\n                                                            optimizer=self.optimizer)\n\n    def load_weights_from_file(self, weight_file_path, sess, finetune=True):\n        # weight_file_object = open(weight_file_path, 'rb')\n        weights = pickle.load(open(weight_file_path, 'rb'))#, encoding='latin1')\n\n        with tf.variable_scope('', reuse=True):\n            ## Pre stage conv\n            # conv1\n            for layer in range(1, 3):\n                conv_kernel = tf.get_variable('sub_stages/sub_conv' + str(layer) + '/kernel')\n                conv_bias = tf.get_variable('sub_stages/sub_conv' + str(layer) + '/bias')\n\n                loaded_kernel = weights['conv1_' + str(layer)]\n                loaded_bias = weights['conv1_' + str(layer) + '_b']\n\n                sess.run(tf.assign(conv_kernel, loaded_kernel))\n                sess.run(tf.assign(conv_bias, loaded_bias))\n\n            # conv2\n            for layer in range(1, 3):\n                conv_kernel = tf.get_variable('sub_stages/sub_conv' + str(layer + 2) + '/kernel')\n                conv_bias = tf.get_variable('sub_stages/sub_conv' + str(layer + 2) + '/bias')\n\n                loaded_kernel = weights['conv2_' + str(layer)]\n                loaded_bias = weights['conv2_' + str(layer) + '_b']\n\n                sess.run(tf.assign(conv_kernel, loaded_kernel))\n                sess.run(tf.assign(conv_bias, loaded_bias))\n\n            # conv3\n            for layer in range(1, 5):\n                conv_kernel = tf.get_variable('sub_stages/sub_conv' + str(layer + 4) + '/kernel')\n                conv_bias = tf.get_variable('sub_stages/sub_conv' + str(layer + 4) + '/bias')\n\n                loaded_kernel = weights['conv3_' + str(layer)]\n                loaded_bias = weights['conv3_' + str(layer) + '_b']\n\n                sess.run(tf.assign(conv_kernel, loaded_kernel))\n                sess.run(tf.assign(conv_bias, loaded_bias))\n\n            # conv4\n            for layer in range(1, 5):\n                conv_kernel = tf.get_variable('sub_stages/sub_conv' + str(layer + 8) + '/kernel')\n                conv_bias = tf.get_variable('sub_stages/sub_conv' + str(layer + 8) + '/bias')\n\n                loaded_kernel = weights['conv4_' + str(layer)]\n                loaded_bias = weights['conv4_' + str(layer) + '_b']\n\n                sess.run(tf.assign(conv_kernel, loaded_kernel))\n                sess.run(tf.assign(conv_bias, loaded_bias))\n\n            # conv5\n            for layer in range(1, 3):\n                conv_kernel = tf.get_variable('sub_stages/sub_conv' + str(layer + 12) + '/kernel')\n                conv_bias = tf.get_variable('sub_stages/sub_conv' + str(layer + 12) + '/bias')\n\n                loaded_kernel = weights['conv5_' + str(layer)]\n                loaded_bias = weights['conv5_' + str(layer) + '_b']\n\n                sess.run(tf.assign(conv_kernel, loaded_kernel))\n                sess.run(tf.assign(conv_bias, loaded_bias))\n\n            # conv5_3_CPM\n            conv_kernel = tf.get_variable('sub_stages/sub_stage_img_feature/kernel')\n            conv_bias = tf.get_variable('sub_stages/sub_stage_img_feature/bias')\n\n            loaded_kernel = weights['conv5_3_CPM']\n            loaded_bias = weights['conv5_3_CPM_b']\n\n            sess.run(tf.assign(conv_kernel, loaded_kernel))\n            sess.run(tf.assign(conv_bias, loaded_bias))\n\n            ## stage 1\n            conv_kernel = tf.get_variable('stage_1/conv1/kernel')\n            conv_bias = tf.get_variable('stage_1/conv1/bias')\n\n            loaded_kernel = weights['conv6_1_CPM']\n            loaded_bias = weights['conv6_1_CPM_b']\n\n            sess.run(tf.assign(conv_kernel, loaded_kernel))\n            sess.run(tf.assign(conv_bias, loaded_bias))\n\n            if finetune != True:\n                conv_kernel = tf.get_variable('stage_1/stage_heatmap/kernel')\n                conv_bias = tf.get_variable('stage_1/stage_heatmap/bias')\n\n                loaded_kernel = weights['conv6_2_CPM']\n                loaded_bias = weights['conv6_2_CPM_b']\n\n                sess.run(tf.assign(conv_kernel, loaded_kernel))\n                sess.run(tf.assign(conv_bias, loaded_bias))\n\n                ## Stage 2 and behind\n                for stage in range(2, self.stages + 1):\n                    for layer in range(1, 8):\n                        conv_kernel = tf.get_variable('stage_' + str(stage) + '/mid_conv' + str(layer) + '/kernel')\n                        conv_bias = tf.get_variable('stage_' + str(stage) + '/mid_conv' + str(layer) + '/bias')\n\n                        loaded_kernel = weights['Mconv' + str(layer) + '_stage' + str(stage)]\n                        loaded_bias = weights['Mconv' + str(layer) + '_stage' + str(stage) + '_b']\n\n                        sess.run(tf.assign(conv_kernel, loaded_kernel))\n                        sess.run(tf.assign(conv_bias, loaded_bias))\n"""
models/nets/cpm_hand_slim.py,57,"b""import pickle\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\n\nclass CPM_Model(object):\n    def __init__(self, stages, joints):\n        self.stages = stages\n        self.stage_heatmap = []\n        self.stage_loss = [0] * stages\n        self.total_loss = 0\n        self.input_image = None\n        self.center_map = None\n        self.gt_heatmap = None\n        self.learning_rate = 0\n        self.merged_summary = None\n        self.joints = joints\n        self.batch_size = 0\n\n    def build_model(self, input_image, center_map, batch_size):\n        self.batch_size = batch_size\n        self.input_image = input_image\n        self.center_map = center_map\n        with tf.variable_scope('pooled_center_map'):\n            # center map is a gaussion template which gather the respose\n            self.center_map = slim.avg_pool2d(self.center_map,\n                                              [9, 9], stride=8,\n                                              padding='SAME',\n                                              scope='center_map')\n\n        with slim.arg_scope([slim.conv2d],\n                            padding='SAME',\n                            activation_fn=tf.nn.relu,\n                            weights_initializer=tf.contrib.layers.xavier_initializer()):\n            with tf.variable_scope('sub_stages'):\n                net = slim.conv2d(input_image, 64, [3, 3], scope='sub_conv1')\n                net = slim.conv2d(net, 64, [3, 3], scope='sub_conv2')\n                net = slim.max_pool2d(net, [2, 2], padding='SAME', scope='sub_pool1')\n                net = slim.conv2d(net, 128, [3, 3], scope='sub_conv3')\n                net = slim.conv2d(net, 128, [3, 3], scope='sub_conv4')\n                net = slim.max_pool2d(net, [2, 2], padding='SAME', scope='sub_pool2')\n                net = slim.conv2d(net, 256, [3, 3], scope='sub_conv5')\n                net = slim.conv2d(net, 256, [3, 3], scope='sub_conv6')\n                net = slim.conv2d(net, 256, [3, 3], scope='sub_conv7')\n                net = slim.conv2d(net, 256, [3, 3], scope='sub_conv8')\n                net = slim.max_pool2d(net, [2, 2], padding='SAME', scope='sub_pool3')\n                net = slim.conv2d(net, 512, [3, 3], scope='sub_conv9')\n                net = slim.conv2d(net, 512, [3, 3], scope='sub_conv10')\n                net = slim.conv2d(net, 512, [3, 3], scope='sub_conv11')\n                net = slim.conv2d(net, 512, [3, 3], scope='sub_conv12')\n                net = slim.conv2d(net, 512, [3, 3], scope='sub_conv13')\n                net = slim.conv2d(net, 512, [3, 3], scope='sub_conv14')\n\n                self.sub_stage_img_feature = slim.conv2d(net, 128, [3, 3],\n                                                         scope='sub_stage_img_feature')\n\n            with tf.variable_scope('stage_1'):\n                conv1 = slim.conv2d(self.sub_stage_img_feature, 512, [1, 1],\n                                    scope='conv1')\n                self.stage_heatmap.append(slim.conv2d(conv1, self.joints, [1, 1],\n                                                      scope='stage_heatmap'))\n\n            for stage in range(2, self.stages + 1):\n                self._middle_conv(stage)\n\n    def _middle_conv(self, stage):\n        with tf.variable_scope('stage_' + str(stage)):\n            self.current_featuremap = tf.concat([self.stage_heatmap[stage-2],\n                                                 self.sub_stage_img_feature,\n                                                 # self.center_map,\n                                                 ],\n                                                axis=3)\n            with slim.arg_scope([slim.conv2d],\n                                padding='SAME',\n                                activation_fn=tf.nn.relu,\n                                weights_initializer=tf.contrib.layers.xavier_initializer()):\n                mid_net = slim.conv2d(self.current_featuremap, 128, [7, 7], scope='mid_conv1')\n                mid_net = slim.conv2d(mid_net, 128, [7, 7], scope='mid_conv2')\n                mid_net = slim.conv2d(mid_net, 128, [7, 7], scope='mid_conv3')\n                mid_net = slim.conv2d(mid_net, 128, [7, 7], scope='mid_conv4')\n                mid_net = slim.conv2d(mid_net, 128, [7, 7], scope='mid_conv5')\n                mid_net = slim.conv2d(mid_net, 128, [1, 1], scope='mid_conv6')\n                self.current_heatmap = slim.conv2d(mid_net, self.joints, [1, 1],\n                                                   scope='mid_conv7')\n                self.stage_heatmap.append(self.current_heatmap)\n\n    def build_loss(self, gt_heatmap, lr, lr_decay_rate, lr_decay_step):\n        self.gt_heatmap = gt_heatmap\n        self.total_loss = 0\n        self.learning_rate = lr\n        self.lr_decay_rate = lr_decay_rate\n        self.lr_decay_step = lr_decay_step\n\n        for stage in range(self.stages):\n            with tf.variable_scope('stage' + str(stage + 1) + '_loss'):\n                self.stage_loss[stage] = tf.nn.l2_loss(self.stage_heatmap[stage] - self.gt_heatmap,\n                                                       name='l2_loss') / self.batch_size\n            tf.summary.scalar('stage' + str(stage + 1) + '_loss', self.stage_loss[stage])\n\n        with tf.variable_scope('total_loss'):\n            for stage in range(self.stages):\n                self.total_loss += self.stage_loss[stage]\n            tf.summary.scalar('total loss', self.total_loss)\n\n        with tf.variable_scope('train'):\n            self.global_step = tf.contrib.framework.get_or_create_global_step()\n\n            self.lr = tf.train.exponential_decay(self.learning_rate,\n                                                 global_step=self.global_step,\n                                                 decay_rate=self.lr_decay_rate,\n                                                 decay_steps=self.lr_decay_step)\n            tf.summary.scalar('learning rate', self.lr)\n\n            self.train_op = tf.contrib.layers.optimize_loss(loss=self.total_loss,\n                                                            global_step=self.global_step,\n                                                            learning_rate=self.lr,\n                                                            optimizer='Adam')\n        self.merged_summary = tf.summary.merge_all()\n\n    def load_weights_from_file(self, weight_file_path, sess, finetune=True):\n        # weight_file_object = open(weight_file_path, 'rb')\n        weights = pickle.load(open(weight_file_path, 'rb'), encoding='latin1')\n\n        with tf.variable_scope('', reuse=True):\n            ## Pre stage conv\n            # conv1\n            for layer in range(1, 3):\n                conv_kernel = tf.get_variable('sub_stages/sub_conv' + str(layer) + '/weights')\n                conv_bias = tf.get_variable('sub_stages/sub_conv' + str(layer) + '/biases')\n\n                loaded_kernel = weights['conv1_' + str(layer)]\n                loaded_bias = weights['conv1_' + str(layer) + '_b']\n\n                sess.run(tf.assign(conv_kernel, loaded_kernel))\n                sess.run(tf.assign(conv_bias, loaded_bias))\n\n            # conv2\n            for layer in range(1, 3):\n                conv_kernel = tf.get_variable('sub_stages/sub_conv' + str(layer + 2) + '/weights')\n                conv_bias = tf.get_variable('sub_stages/sub_conv' + str(layer + 2) + '/biases')\n\n                loaded_kernel = weights['conv2_' + str(layer)]\n                loaded_bias = weights['conv2_' + str(layer) + '_b']\n\n                sess.run(tf.assign(conv_kernel, loaded_kernel))\n                sess.run(tf.assign(conv_bias, loaded_bias))\n\n            # conv3\n            for layer in range(1, 5):\n                conv_kernel = tf.get_variable('sub_stages/sub_conv' + str(layer + 4) + '/weights')\n                conv_bias = tf.get_variable('sub_stages/sub_conv' + str(layer + 4) + '/biases')\n\n                loaded_kernel = weights['conv3_' + str(layer)]\n                loaded_bias = weights['conv3_' + str(layer) + '_b']\n\n                sess.run(tf.assign(conv_kernel, loaded_kernel))\n                sess.run(tf.assign(conv_bias, loaded_bias))\n\n            # conv4\n            for layer in range(1, 5):\n                conv_kernel = tf.get_variable('sub_stages/sub_conv' + str(layer + 8) + '/weights')\n                conv_bias = tf.get_variable('sub_stages/sub_conv' + str(layer + 8) + '/biases')\n\n                loaded_kernel = weights['conv4_' + str(layer)]\n                loaded_bias = weights['conv4_' + str(layer) + '_b']\n\n                sess.run(tf.assign(conv_kernel, loaded_kernel))\n                sess.run(tf.assign(conv_bias, loaded_bias))\n\n            # conv5\n            for layer in range(1, 3):\n                conv_kernel = tf.get_variable('sub_stages/sub_conv' + str(layer + 12) + '/weights')\n                conv_bias = tf.get_variable('sub_stages/sub_conv' + str(layer + 12) + '/biases')\n\n                loaded_kernel = weights['conv5_' + str(layer)]\n                loaded_bias = weights['conv5_' + str(layer) + '_b']\n\n                sess.run(tf.assign(conv_kernel, loaded_kernel))\n                sess.run(tf.assign(conv_bias, loaded_bias))\n\n            # conv5_3_CPM\n            conv_kernel = tf.get_variable('sub_stages/sub_stage_img_feature/weights')\n            conv_bias = tf.get_variable('sub_stages/sub_stage_img_feature/biases')\n\n            loaded_kernel = weights['conv5_3_CPM']\n            loaded_bias = weights['conv5_3_CPM_b']\n\n            sess.run(tf.assign(conv_kernel, loaded_kernel))\n            sess.run(tf.assign(conv_bias, loaded_bias))\n\n            ## stage 1\n            conv_kernel = tf.get_variable('stage_1/conv1/weights')\n            conv_bias = tf.get_variable('stage_1/conv1/biases')\n\n            loaded_kernel = weights['conv6_1_CPM']\n            loaded_bias = weights['conv6_1_CPM_b']\n\n            sess.run(tf.assign(conv_kernel, loaded_kernel))\n            sess.run(tf.assign(conv_bias, loaded_bias))\n\n            if finetune != True:\n                conv_kernel = tf.get_variable('stage_1/stage_heatmap/weights')\n                conv_bias = tf.get_variable('stage_1/stage_heatmap/biases')\n\n                loaded_kernel = weights['conv6_2_CPM']\n                loaded_bias = weights['conv6_2_CPM_b']\n\n                sess.run(tf.assign(conv_kernel, loaded_kernel))\n                sess.run(tf.assign(conv_bias, loaded_bias))\n\n                ## stage 2 and behind\n                for stage in range(2, self.stages + 1):\n                    for layer in range(1, 8):\n                        conv_kernel = tf.get_variable('stage_' + str(stage) + '/mid_conv' + str(layer) + '/weights')\n                        conv_bias = tf.get_variable('stage_' + str(stage) + '/mid_conv' + str(layer) + '/biases')\n\n                        loaded_kernel = weights['Mconv' + str(layer) + '_stage' + str(stage)]\n                        loaded_bias = weights['Mconv' + str(layer) + '_stage' + str(stage) + '_b']\n\n                        sess.run(tf.assign(conv_kernel, loaded_kernel))\n                        sess.run(tf.assign(conv_bias, loaded_bias))\n"""
models/nets/cpm_hand_v2.py,94,"b""import tensorflow as tf\nimport pickle\nfrom models.nets.CPM import CPM\n\n\n\nclass CPM_Model(CPM):\n    def __init__(self, input_size, heatmap_size, stages, joints, img_type='RGB', is_training=True):\n        self.stages = stages\n        self.stage_heatmap = []\n        self.stage_loss = [0 for _ in range(stages)]\n        self.total_loss = 0\n        self.input_image = None\n        self.center_map = None\n        self.gt_heatmap = None\n        self.init_lr = 0\n        self.merged_summary = None\n        self.joints = joints\n        self.batch_size = 0\n        self.inference_type = 'Train'\n\n        if img_type == 'RGB':\n            self.input_images = tf.placeholder(dtype=tf.float32,\n                                               shape=(None, input_size, input_size, 3),\n                                               name='input_placeholder')\n        elif img_type == 'GRAY':\n            self.input_images = tf.placeholder(dtype=tf.float32,\n                                               shape=(None, input_size, input_size, 1),\n                                               name='input_placeholder')\n        # self.cmap_placeholder = tf.placeholder(dtype=tf.float32,\n        #                                        shape=(None, input_size, input_size, 1),\n        #                                        name='cmap_placeholder')\n        self.gt_hmap_placeholder = tf.placeholder(dtype=tf.float32,\n                                                  shape=(None, heatmap_size, heatmap_size, joints + 1),\n                                                  name='gt_hmap_placeholder')\n        self._build_model()\n\n    def _build_model(self):\n        # with tf.variable_scope('pooled_center_map'):\n        #     self.center_map = tf.layers.average_pooling2d(inputs=self.cmap_placeholder,\n        #                                                   pool_size=[9, 9],\n        #                                                   strides=[8, 8],\n        #                                                   padding='same',\n        #                                                   name='center_map')\n        with tf.variable_scope('sub_stages'):\n            sub_conv1 = tf.layers.conv2d(inputs=self.input_images,\n                                         filters=64,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv1')\n            sub_conv2 = tf.layers.conv2d(inputs=sub_conv1,\n                                         filters=64,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv2')\n            sub_pool1 = tf.layers.max_pooling2d(inputs=sub_conv2,\n                                                pool_size=[2, 2],\n                                                strides=2,\n                                                padding='valid',\n                                                name='sub_pool1')\n            sub_conv3 = tf.layers.conv2d(inputs=sub_pool1,\n                                         filters=128,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv3')\n            sub_conv4 = tf.layers.conv2d(inputs=sub_conv3,\n                                         filters=128,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv4')\n            # sub_pool2 = tf.layers.max_pooling2d(inputs=sub_conv4,\n            #                                     pool_size=[2, 2],\n            #                                     strides=2,\n            #                                     padding='valid',\n            #                                     name='sub_pool2')\n            sub_conv5 = tf.layers.conv2d(inputs=sub_conv4,\n                                         filters=256,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv5')\n            sub_conv6 = tf.layers.conv2d(inputs=sub_conv5,\n                                         filters=256,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv6')\n            sub_conv7 = tf.layers.conv2d(inputs=sub_conv6,\n                                         filters=256,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv7')\n            sub_conv8 = tf.layers.conv2d(inputs=sub_conv7,\n                                         filters=256,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv8')\n            sub_pool3 = tf.layers.max_pooling2d(inputs=sub_conv8,\n                                                pool_size=[2, 2],\n                                                strides=2,\n                                                padding='valid',\n                                                name='sub_pool3')\n            sub_conv9 = tf.layers.conv2d(inputs=sub_pool3,\n                                         filters=512,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv9')\n            sub_conv10 = tf.layers.conv2d(inputs=sub_conv9,\n                                          filters=512,\n                                          kernel_size=[3, 3],\n                                          strides=[1, 1],\n                                          padding='same',\n                                          activation=tf.nn.relu,\n                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                          name='sub_conv10')\n            sub_conv11 = tf.layers.conv2d(inputs=sub_conv10,\n                                          filters=512,\n                                          kernel_size=[3, 3],\n                                          strides=[1, 1],\n                                          padding='same',\n                                          activation=tf.nn.relu,\n                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                          name='sub_conv11')\n            sub_conv12 = tf.layers.conv2d(inputs=sub_conv11,\n                                          filters=512,\n                                          kernel_size=[3, 3],\n                                          strides=[1, 1],\n                                          padding='same',\n                                          activation=tf.nn.relu,\n                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                          name='sub_conv12')\n            sub_conv13 = tf.layers.conv2d(inputs=sub_conv12,\n                                          filters=512,\n                                          kernel_size=[3, 3],\n                                          strides=[1, 1],\n                                          padding='same',\n                                          activation=tf.nn.relu,\n                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                          name='sub_conv13')\n            sub_conv14 = tf.layers.conv2d(inputs=sub_conv13,\n                                          filters=512,\n                                          kernel_size=[3, 3],\n                                          strides=[1, 1],\n                                          padding='same',\n                                          activation=tf.nn.relu,\n                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                          name='sub_conv14')\n            self.sub_stage_img_feature = tf.layers.conv2d(inputs=sub_conv14,\n                                                          filters=128,\n                                                          kernel_size=[3, 3],\n                                                          strides=[1, 1],\n                                                          padding='same',\n                                                          activation=tf.nn.relu,\n                                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                                          name='sub_stage_img_feature')\n\n        with tf.variable_scope('stage_1'):\n            conv1 = tf.layers.conv2d(inputs=self.sub_stage_img_feature,\n                                     filters=512,\n                                     kernel_size=[1, 1],\n                                     strides=[1, 1],\n                                     padding='valid',\n                                     activation=tf.nn.relu,\n                                     kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                     name='conv1')\n            self.stage_heatmap.append(tf.layers.conv2d(inputs=conv1,\n                                                       filters=self.joints+1,\n                                                       kernel_size=[1, 1],\n                                                       strides=[1, 1],\n                                                       padding='valid',\n                                                       activation=None,\n                                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                                       name='stage_heatmap'))\n        for stage in range(2, self.stages + 1):\n            self._middle_conv(stage)\n\n    def _middle_conv(self, stage):\n        with tf.variable_scope('stage_' + str(stage)):\n            self.current_featuremap = tf.concat([self.stage_heatmap[stage - 2],\n                                                 self.sub_stage_img_feature,\n                                                 # self.center_map],\n                                                 ],\n                                                axis=3)\n            mid_conv1 = tf.layers.conv2d(inputs=self.current_featuremap,\n                                         filters=128,\n                                         kernel_size=[7, 7],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='mid_conv1')\n            mid_conv2 = tf.layers.conv2d(inputs=mid_conv1,\n                                         filters=128,\n                                         kernel_size=[7, 7],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='mid_conv2')\n            mid_conv3 = tf.layers.conv2d(inputs=mid_conv2,\n                                         filters=128,\n                                         kernel_size=[7, 7],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='mid_conv3')\n            mid_conv4 = tf.layers.conv2d(inputs=mid_conv3,\n                                         filters=128,\n                                         kernel_size=[7, 7],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='mid_conv4')\n            mid_conv5 = tf.layers.conv2d(inputs=mid_conv4,\n                                         filters=128,\n                                         kernel_size=[7, 7],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='mid_conv5')\n            mid_conv6 = tf.layers.conv2d(inputs=mid_conv5,\n                                         filters=128,\n                                         kernel_size=[1, 1],\n                                         strides=[1, 1],\n                                         padding='valid',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='mid_conv6')\n            self.current_heatmap = tf.layers.conv2d(inputs=mid_conv6,\n                                                    filters=self.joints+1,\n                                                    kernel_size=[1, 1],\n                                                    strides=[1, 1],\n                                                    padding='valid',\n                                                    activation=None,\n                                                    kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                                    name='mid_conv7')\n            self.stage_heatmap.append(self.current_heatmap)\n\n    def build_loss(self, lr, lr_decay_rate, lr_decay_step, optimizer='Adam'):\n        self.total_loss = 0\n        self.init_lr = lr\n        self.lr_decay_rate = lr_decay_rate\n        self.lr_decay_step = lr_decay_step\n        self.optimizer = optimizer\n        self.batch_size = tf.cast(tf.shape(self.input_images)[0], dtype=tf.float32)\n\n\n        for stage in range(self.stages):\n            with tf.variable_scope('stage' + str(stage + 1) + '_loss'):\n                self.stage_loss[stage] = tf.nn.l2_loss(self.stage_heatmap[stage] - self.gt_hmap_placeholder,\n                                                       name='l2_loss') / self.batch_size\n            tf.summary.scalar('stage' + str(stage + 1) + '_loss', self.stage_loss[stage])\n\n        with tf.variable_scope('total_loss'):\n            for stage in range(self.stages):\n                self.total_loss += self.stage_loss[stage]\n            tf.summary.scalar('total loss'.format(self.inference_type), self.total_loss)\n\n        with tf.variable_scope('train'):\n            self.global_step = tf.contrib.framework.get_or_create_global_step()\n\n            self.cur_lr = tf.train.exponential_decay(self.init_lr,\n                                                 global_step=self.global_step,\n                                                 decay_rate=self.lr_decay_rate,\n                                                 decay_steps=self.lr_decay_step)\n            tf.summary.scalar('global learning rate', self.cur_lr)\n\n            self.train_op = tf.contrib.layers.optimize_loss(loss=self.total_loss,\n                                                            global_step=self.global_step,\n                                                            learning_rate=self.cur_lr,\n                                                            optimizer=self.optimizer)\n"""
