file_path,api_count,code
action_3d_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 3\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    \'\'\' sabsample timesteps to prevent OOM due to Attention LSTM \'\'\'\n    stride = 2\n\n    x = Permute((2, 1))(ip)\n    x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n               kernel_initializer=\'he_uniform\')(x) # (None, variables / stride, timesteps)\n    x = Permute((2, 1))(x)\n\n    x = Masking()(x)\n    x = AttentionLSTM(128)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    #train_model(model, DATASET_INDEX, dataset_prefix=\'action_3d\', epochs=600, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'action_3d\', batch_size=128)\n'"
acvitivity_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 4\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    \'\'\' sabsample timesteps to prevent OOM due to Attention LSTM \'\'\'\n    stride = 3\n\n    x = Permute((2, 1))(ip)\n    x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n               kernel_initializer=\'he_uniform\')(x) # (None, variables / stride, timesteps)\n\n    x = Masking()(x)\n    x = AttentionLSTM(384, unroll=True)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    train_model(model, DATASET_INDEX, dataset_prefix=\'activity_attention\', epochs=1000, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'activity_attention\', batch_size=128)\n'"
arabic_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 27\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    # train_model(model, DATASET_INDEX, dataset_prefix=\'arabic_voice_\', epochs=1000, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'arabic_voice_\', batch_size=128)\n'"
arabic_voice_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\nfrom keras.regularizers import l2\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 5\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\nregularization_weight = 5e-4\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    # train_model(model, DATASET_INDEX, dataset_prefix=\'arabic_voice\', epochs=600, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'arabic_voice\', batch_size=128)\n'"
aram_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 7\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    #train_model(model, DATASET_INDEX, dataset_prefix=\'AReM\', epochs=600, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'AReM\', batch_size=128)\n'"
arem_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 7\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    #train_model(model, DATASET_INDEX, dataset_prefix=\'AReM\', epochs=600, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'AReM\', batch_size=128)\n'"
auslan_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 28\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(128)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(64)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    # train_model(model, DATASET_INDEX, dataset_prefix=\'auslan_\', epochs=1000, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'auslan_\', batch_size=128)\n'"
character_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 2\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    from keras import backend as K\n    import json\n\n    \'\'\' Train portion \'\'\'\n    scores = []\n\n    for i in range(10):\n        K.clear_session()\n\n        print(""Begin iteration %d"" % (i + 1))\n        print(""*"" * 80)\n        print()\n\n        model = generate_model() # change to generate_model_2()\n        #train_model(model, DATASET_INDEX, dataset_prefix=\'character_attention\', dataset_fold_id=(i + 1), epochs=600, batch_size=128)\n        score = evaluate_model(model, DATASET_INDEX, dataset_prefix=\'character_attention\', dataset_fold_id=(i + 1), batch_size=128)\n        scores.append(score)\n\n    with open(\'data/character/scores.json\', \'w\') as f:\n        json.dump({\'scores\': scores}, f)\n\n    \'\'\' evaluate average score \'\'\'\n    with open(\'data/character/scores.json\', \'r\') as f:\n        results = json.load(f)\n\n    scores = results[\'scores\']\n    avg_score = sum(scores) / len(scores)\n    print(""Scores : "", scores)\n    print(""Average score over 10 epochs : "", avg_score)\n\n'"
character_trajectories_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 29\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(64)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(64)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    # train_model(model, DATASET_INDEX, dataset_prefix=\'character_trajectories_\', epochs=1000, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'character_trajectories_\', batch_size=128)\n'"
ck_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 1\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    from keras import backend as K\n    import json\n\n    \'\'\' Train portion \'\'\'\n    scores = []\n\n    for i in range(10):\n        K.clear_session()\n\n        print(""Begin iteration %d"" % (i + 1))\n        print(""*"" * 80)\n        print()\n\n        model = generate_model() # change to generate_model_2()\n        #train_model(model, DATASET_INDEX, dataset_prefix=\'ck\', dataset_fold_id=(i + 1), epochs=600, batch_size=128)\n        score = evaluate_model(model, DATASET_INDEX, dataset_prefix=\'ck\', dataset_fold_id=(i + 1), batch_size=128)\n        scores.append(score)\n\n    with open(\'data/CK/scores.json\', \'w\') as f:\n        json.dump({\'scores\': scores}, f)\n\n    \'\'\' evaluate average score \'\'\'\n    with open(\'data/CK/scores.json\', \'r\') as f:\n        results = json.load(f)\n\n    scores = results[\'scores\']\n    avg_score = sum(scores) / len(scores)\n    print(""Scores : "", scores)\n    print(""Average score over 10 epochs : "", avg_score)\n'"
cmu_subject_16_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 30\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    # train_model(model, DATASET_INDEX, dataset_prefix=\'cmu_subject_16_\', epochs=1000, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'cmu_subject_16_\', batch_size=128)\n'"
daily_sport.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 15\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model()\n\n    train_model(model, DATASET_INDEX, dataset_prefix=\'daily_sport_no_attention\', epochs=500, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'daily_sport_no_attention\', batch_size=128)\n'"
digitshape_random_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 40\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    # train_model(model, DATASET_INDEX, dataset_prefix=\'digitshape_random_\', epochs=1000, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'digitshape_random_\', batch_size=128)\n'"
ecg_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 31\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(128)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(128)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    # train_model(model, DATASET_INDEX, dataset_prefix=\'ecg_\', epochs=1000, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'ecg_\', batch_size=128)\n'"
eeg2_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 16\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    train_model(model, DATASET_INDEX, dataset_prefix=\'eeg2_attention\', epochs=500, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'eeg2_attention\', batch_size=128)\n'"
eeg_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 13\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    train_model(model, DATASET_INDEX, dataset_prefix=\'eeg\', epochs=500, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'eeg\', batch_size=128)\n'"
gesture_phase_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 8\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model()\n\n    train_model(model, DATASET_INDEX, dataset_prefix=\'gesture_phase\', epochs=1000, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'gesture_phase\', batch_size=128)\n'"
har_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 11\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    train_model(model, DATASET_INDEX, dataset_prefix=\'har\', epochs=600, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'har\', batch_size=128)\n'"
ht_sensor_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 9\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    # train_model(model, DATASET_INDEX, dataset_prefix=\'ht_sensor\', epochs=600, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'ht_sensor\', batch_size=128)\n'"
japanese_vowels_dataset.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 6\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    train_model(model, DATASET_INDEX, dataset_prefix=\'japanese_vowels\', epochs=600, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'japanese_vowels\', batch_size=128)\n'"
japanese_vowels_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 32\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    # train_model(model, DATASET_INDEX, dataset_prefix=\'japanese_vowels_\', epochs=1000, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'japanese_vowels_\', batch_size=128)\n'"
kick_vs_punch_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 33\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    # train_model(model, DATASET_INDEX, dataset_prefix=\'kick_vs_punch_\', epochs=1000, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'kick_vs_punch_\', batch_size=128)\n'"
libras_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 34\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    # train_model(model, DATASET_INDEX, dataset_prefix=\'libras_\', epochs=1000, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'libras_\', batch_size=128)\n'"
lp1_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 41\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    # train_model(model, DATASET_INDEX, dataset_prefix=\'lp1_\', epochs=1000, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'lp1_\', batch_size=128)\n'"
lp2_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 42\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    # train_model(model, DATASET_INDEX, dataset_prefix=\'lp2_\', epochs=1000, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'lp2_\', batch_size=128)\n'"
lp3_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 43\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    # train_model(model, DATASET_INDEX, dataset_prefix=\'lp3_\', epochs=1000, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'lp3_\', batch_size=128)\n'"
lp4_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 44\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(64)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    # train_model(model, DATASET_INDEX, dataset_prefix=\'lp4_\', epochs=1000, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'lp4_\', batch_size=128)\n'"
lp5_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 45\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(64)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(64)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(128)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    # train_model(model, DATASET_INDEX, dataset_prefix=\'lp5_\', epochs=1000, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'lp5_\', batch_size=128)\n'"
movement_aal_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 10\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    train_model(model, DATASET_INDEX, dataset_prefix=\'movement_aal\', epochs=1000, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'movement_aal\', batch_size=128)\n'"
net_flow_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 35\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    # train_model(model, DATASET_INDEX, dataset_prefix=\'net_flow_\', epochs=1000, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'net_flow_\', batch_size=128)\n'"
occupancy_detect_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable, f1_score\nfrom utils.layer_utils import AttentionLSTM\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nmpl.style.use(\'seaborn-paper\')\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport warnings\nwarnings.simplefilter(\'ignore\', category=DeprecationWarning)\n\nfrom keras.models import Model\nfrom keras.layers import Permute\nfrom keras.optimizers import Adam\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras import backend as K\n\nfrom utils.generic_utils import load_dataset_at, calculate_dataset_metrics, cutoff_choice, \\\n                                cutoff_sequence\nfrom utils.constants import MAX_NB_VARIABLES, MAX_TIMESTEPS_LIST\nimport numpy as np\nfrom sklearn import metrics\n\n\nDATASET_INDEX = 25\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\ndef classifaction_report_csv(report):\n    report_data = []\n    lines = report.split(\'\\n\')\n    for line in lines[2:-3]:\n        row = {}\n        row_data = line.split(\'      \')\n        row[\'class\'] = row_data[0]\n        row[\'precision\'] = float(row_data[1])\n        row[\'recall\'] = float(row_data[2])\n        row[\'f1_score\'] = float(row_data[3])\n        row[\'support\'] = float(row_data[4])\n        report_data.append(row)\n    dataframe = pd.DataFrame.from_dict(report_data)\n    return(dataframe)\n\ndef predict_model(model:Model, dataset_id, dataset_prefix, dataset_fold_id=None, batch_size=128, test_data_subset=None,\n                   cutoff=None, normalize_timeseries=False):\n    _, _, X_test, y_test, is_timeseries = load_dataset_at(dataset_id,\n                                                          fold_index=dataset_fold_id,\n                                                          normalize_timeseries=normalize_timeseries)\n    max_timesteps, max_nb_variables = calculate_dataset_metrics(X_test)\n\n    if not is_timeseries:\n        X_test = pad_sequences(X_test, maxlen=MAX_NB_VARIABLES[dataset_id], padding=\'post\', truncating=\'post\')\n    y_test = to_categorical(y_test, len(np.unique(y_test)))\n\n    optm = Adam(lr=1e-3)\n    model.compile(optimizer=optm, loss=\'categorical_crossentropy\', metrics=[\'accuracy\', f1_score])\n\n    if dataset_fold_id is None:\n        weight_fn = ""./weights/%s_weights.h5"" % dataset_prefix\n    else:\n        weight_fn = ""./weights/%s_fold_%d_weights.h5"" % (dataset_prefix, dataset_fold_id)\n    model.load_weights(weight_fn)\n\n    if test_data_subset is not None:\n        X_test = X_test[:test_data_subset]\n        y_test = y_test[:test_data_subset]\n\n    print(""\\nPredicting : "")\n\n    # metrics = model.evaluate(X_test, y_test, batch_size=batch_size)\n    # print()\n    # print(""Final metrics %s: "" % model.metrics_names, metrics)\n\n    y_pred = model.predict(X_test, batch_size=batch_size)\n    y_test = (np.argmax(y_test, axis=1)).tolist()\n    y_pred = (np.argmax(y_pred, axis=1)).tolist()\n    cfmatrix = metrics.confusion_matrix(y_test, y_pred)\n    report = metrics.classification_report(y_test, y_pred)\n    averagef_mahi = np.mean(classifaction_report_csv(report).iloc[:, 1])\n    microf =  metrics.f1_score(y_test, y_pred, average=\'micro\')\n    averagef = metrics.f1_score(y_test, y_pred, average=\'macro\')\n    weightedf_mahi = sum((classifaction_report_csv(report).iloc[:,1])*(cfmatrix.sum(axis=1)/sum(cfmatrix.sum(axis=1))))\n    weightedf = metrics.f1_score(y_test, y_pred, average=\'weighted\')\n\n    print()\n    print(""Binary F-Score : "", microf)\n    print(""Final F-Score : "", averagef)\n    print(""Weighted F-Score : "", weightedf)\n\n    print(""Mahis F-Score : "", averagef_mahi)\n    print(""Mahis W F-Score : "", weightedf_mahi)\n    return averagef\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n    model.compile(\'adam\', \'categorical_crossentropy\', metrics=[\'accuracy\', f1_score])\n\n    train_model(model, DATASET_INDEX, dataset_prefix=\'opportunity_new\', epochs=1000, batch_size=128,\n                 monitor=\'val_f1_score\', optimization_mode=\'max\', compile_model=False)\n\n    #evaluate_model(model, DATASET_INDEX, dataset_prefix=\'opportunity\', batch_size=128)\n    #predict_model(model, DATASET_INDEX, dataset_prefix=\'opportunity_weights_attention_9208_512_lstm_128\', batch_size=512)\n'"
opportunity_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 12\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    # train_model(model, DATASET_INDEX, dataset_prefix=\'occupancy_detect\', epochs=1000, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'occupancy_detect\', batch_size=128)\n'"
ozone_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 14\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    train_model(model, DATASET_INDEX, dataset_prefix=\'ozone\', epochs=600, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'ozone\', batch_size=128)\n'"
pendigits_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 46\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    # train_model(model, DATASET_INDEX, dataset_prefix=\'pendigits_\', epochs=1000, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'pendigits_\', batch_size=128)\n'"
shapes_random_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 47\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    # train_model(model, DATASET_INDEX, dataset_prefix=\'shapes_random_\', epochs=1000, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'shapes_random_\', batch_size=128)\n'"
u_wave_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 37\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(128)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(128)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    # train_model(model, DATASET_INDEX, dataset_prefix=\'u_wave_\', epochs=1000, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'u_wave_\', batch_size=128)\n'"
uwave_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 24\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(128)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = AttentionLSTM(128)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    train_model(model, DATASET_INDEX, dataset_prefix=\'uwave_attention\', epochs=500, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'uwave_attention\', batch_size=128)\n'"
wafer_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 38\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    # train_model(model, DATASET_INDEX, dataset_prefix=\'wafer_\', epochs=1000, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'wafer_\', batch_size=128)\n'"
walk_vs_run_model.py,0,"b'from keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n\nfrom utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\nfrom utils.keras_utils import train_model, evaluate_model, set_trainable\nfrom utils.layer_utils import AttentionLSTM\n\nDATASET_INDEX = 39\n\nMAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\nMAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\nNB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n\nTRAINABLE = True\n\n\ndef generate_model():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_2():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 10\n\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    #x = Permute((2, 1))(ip)\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef generate_model_3():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n\n    x = Masking()(ip)\n    x = LSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\n\ndef generate_model_4():\n    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n    # stride = 3\n    #\n    # x = Permute((2, 1))(ip)\n    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding=\'same\', activation=\'relu\', use_bias=False,\n    #            kernel_initializer=\'he_uniform\')(x)  # (None, variables / stride, timesteps)\n    # x = Permute((2, 1))(x)\n\n    x = Masking()(ip)\n    x = AttentionLSTM(8)(x)\n    x = Dropout(0.8)(x)\n\n    y = Permute((2, 1))(ip)\n    y = Conv1D(128, 8, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(256, 5, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n    #y = squeeze_excite_block(y)\n\n    y = Conv1D(128, 3, padding=\'same\', kernel_initializer=\'he_uniform\')(y)\n    y = BatchNormalization()(y)\n    y = Activation(\'relu\')(y)\n\n    y = GlobalAveragePooling1D()(y)\n\n    x = concatenate([x, y])\n\n    out = Dense(NB_CLASS, activation=\'softmax\')(x)\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\ndef squeeze_excite_block(input):\n    \'\'\' Create a squeeze-excite block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        k: width factor\n\n    Returns: a keras tensor\n    \'\'\'\n    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n\n    se = GlobalAveragePooling1D()(input)\n    se = Reshape((1, filters))(se)\n    se = Dense(filters // 16,  activation=\'relu\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', use_bias=False)(se)\n    se = multiply([input, se])\n    return se\n\n\nif __name__ == ""__main__"":\n    model = generate_model_2()\n\n    # train_model(model, DATASET_INDEX, dataset_prefix=\'walk_vs_run_\', epochs=1000, batch_size=128)\n\n    evaluate_model(model, DATASET_INDEX, dataset_prefix=\'walk_vs_run_\', batch_size=128)\n'"
utils/constants.py,0,"b""\nTRAIN_FILES = ['../data/arabic/', # 0\n               '../data/CK/', # 1\n               '../data/character/', # 2\n               '../data/Action3D/', # 3\n               '../data/Activity/', # 4\n               '../data/arabic_voice/', # 5\n               '../data/JapaneseVowels/', # 6\n\n               # New benchmark datasets\n               '../data/AREM/', # 7\n               '../data/gesture_phase/', # 8\n               '../data/HT_Sensor/', # 9\n               '../data/MovementAAL/', # 10\n               '../data/HAR/', # 11\n               '../data/occupancy_detect/', # 12\n               '../data/eeg/',  # 13\n               '../data/ozone/', # 14\n               '../data/daily_sport/',  # 15\n               '../data/eeg2/',  # 16\n               '../data/MHEALTH/',  # 17\n               '../data/EEG_Comp2_data1a/',  # 18\n               '../data/EEG_Comp2_data1b/',  # 19\n               '../data/EEG_Comp2_data3/',  # 20\n               '../data/EEG_Comp2_data4/',  # 21\n               '../data/EEG_Comp3_data2/',  # 22\n               '../data/EEG_Comp3_data1/',  # 23\n               '../data/uwave/',  # 24\n               '../data/opportunity/',  # 25\n               '../data/pamap2/',  # 26\n               '../data/WEASEL_MUSE_DATASETS/ArabicDigits/',  # 27\n               '../data/WEASEL_MUSE_DATASETS/AUSLAN/',  # 28\n               '../data/WEASEL_MUSE_DATASETS/CharacterTrajectories/',  # 29\n               '../data/WEASEL_MUSE_DATASETS/CMUsubject16/',  # 30\n               '../data/WEASEL_MUSE_DATASETS/ECG/',  # 31\n               '../data/WEASEL_MUSE_DATASETS/JapaneseVowels/',  # 32\n               '../data/WEASEL_MUSE_DATASETS/KickvsPunch/',  # 33\n               '../data/WEASEL_MUSE_DATASETS/Libras/',  # 34\n               '../data/WEASEL_MUSE_DATASETS/NetFlow/',  # 35\n               '../data/WEASEL_MUSE_DATASETS/PEMS/',  # 36\n               '../data/WEASEL_MUSE_DATASETS/UWave/',  # 37\n               '../data/WEASEL_MUSE_DATASETS/Wafer/',  # 38\n               '../data/WEASEL_MUSE_DATASETS/WalkvsRun/',  # 39\n               '../data/WEASEL_MUSE_DATASETS/digitshape_random/',  # 40\n               '../data/WEASEL_MUSE_DATASETS/lp1/',  # 41\n               '../data/WEASEL_MUSE_DATASETS/lp2/',  # 42\n               '../data/WEASEL_MUSE_DATASETS/lp3/',  # 43\n               '../data/WEASEL_MUSE_DATASETS/lp4/',  # 44\n               '../data/WEASEL_MUSE_DATASETS/lp5/',  # 45\n               '../data/WEASEL_MUSE_DATASETS/pendigits/',  # 46\n               '../data/WEASEL_MUSE_DATASETS/shapes_random/',  # 47\n               ]\n\nTEST_FILES = ['../data/arabic/', # 0\n              '../data/CK/', # 1\n              '../data/character/', # 2\n              '../data/Action3D/', # 3\n              '../data/Activity/', # 4\n              '../data/arabic_voice/', # 5\n              '../data/JapaneseVowels/', # 6\n\n              # New benchmark datasets\n              '../data/AREM/', # 7\n              '../data/gesture_phase/', # 8\n              '../data/HT_Sensor/',  # 9\n              '../data/MovementAAL/',  # 10\n              '../data/HAR/',  # 11\n              '../data/occupancy_detect/',  # 12\n              '../data/eeg/', # 13\n              '../data/ozone/',  # 14\n              '../data/daily_sport/',  # 15\n              '../data/eeg2/',  # 16\n              '../data/MHEALTH/',  # 17\n              '../data/EEG_Comp2_data1a/',  # 18\n              '../data/EEG_Comp2_data1b/',  # 19\n              '../data/EEG_Comp2_data3/',  # 20\n              '../data/EEG_Comp2_data4/',  # 21\n              '../data/EEG_Comp3_data2/',  # 22\n              '../data/EEG_Comp3_data1/',  # 23\n              '../data/uwave/',  # 24\n              '../data/opportunity/',  # 25\n              '../data/pamap2/',  # 26\n              '../data/WEASEL_MUSE_DATASETS/ArabicDigits/',  # 27\n              '../data/WEASEL_MUSE_DATASETS/AUSLAN/',  # 28\n              '../data/WEASEL_MUSE_DATASETS/CharacterTrajectories/',  # 29\n              '../data/WEASEL_MUSE_DATASETS/CMUsubject16/',  # 30\n              '../data/WEASEL_MUSE_DATASETS/ECG/',  # 31\n              '../data/WEASEL_MUSE_DATASETS/JapaneseVowels/',  # 32\n              '../data/WEASEL_MUSE_DATASETS/KickvsPunch/',  # 33\n              '../data/WEASEL_MUSE_DATASETS/Libras/',  # 34\n              '../data/WEASEL_MUSE_DATASETS/NetFlow/',  # 35\n              '../data/WEASEL_MUSE_DATASETS/PEMS/',  # 36\n              '../data/WEASEL_MUSE_DATASETS/UWave/',  # 37\n              '../data/WEASEL_MUSE_DATASETS/Wafer/',  # 38\n              '../data/WEASEL_MUSE_DATASETS/WalkvsRun/',  # 39\n              '../data/WEASEL_MUSE_DATASETS/digitshape_random/',  # 40\n              '../data/WEASEL_MUSE_DATASETS/lp1/',  # 41\n              '../data/WEASEL_MUSE_DATASETS/lp2/',  # 42\n              '../data/WEASEL_MUSE_DATASETS/lp3/',  # 43\n              '../data/WEASEL_MUSE_DATASETS/lp4/',  # 44\n              '../data/WEASEL_MUSE_DATASETS/lp5/',  # 45\n              '../data/WEASEL_MUSE_DATASETS/pendigits/',  # 46\n              '../data/WEASEL_MUSE_DATASETS/shapes_random/',  # 47\n\n              ]\n\nMAX_NB_VARIABLES = [13,  # 0\n                    136,  # 1\n                    30,  # 2\n                    570,  # 3\n                    570,  # 4\n                    39,  # 5\n                    12,  # 6\n\n                    # New benchmark datasets\n                    7,  # 7\n                    18,  # 8\n                    11,  # 9\n                    4,  # 10\n                    9,  # 11\n                    5,  # 12\n                    13,  # 13\n                    72,  # 14\n                    45,  #15\n                    64,  #16\n                    23,  #17\n                    6,  #18\n                    7,  #19\n                    3,  #20\n                    28,  #21\n                    64,  #22\n                    64,  #23\n                    3,  #24\n                    77,  #25\n                    52,  #26\n                    13,  #27\n                    22,  #28\n                    3,  #29\n                    62,  #30\n                    2,  #31\n                    12,  #32\n                    62,  #33\n                    2,  #34\n                    4,  #35\n                    963,  #36\n                    3,  #37\n                    6,  #38\n                    62,  #39\n                    2,  #40\n                    6,  #41\n                    6,  # 42\n                    6,  # 43\n                    6,  # 44\n                    6,  # 45\n                    2, #46\n                    2, #46\n\n                    ]\n\nMAX_TIMESTEPS_LIST = [93,  # 0\n                      71,  # 1\n                      173,  # 2\n                      100, # 3\n                      337, # 4\n                      91, # 5\n                      26, # 6\n\n                      # New benchmark datasets\n                      480, # 7\n                      214, # 8\n                      5396, # 9\n                      119, # 10\n                      128, # 11\n                      3758, # 12\n                      117, # 13\n                      291, # 14\n                      125, #15\n                      256, #16\n                      42701, #17\n                      896, #18\n                      1152, #19\n                      1152, #20\n                      500,#21\n                      7794, #22\n                      3000, #23\n                      315, #24\n                      24, #25\n                      34, #26\n                      93, #27\n                      96, #28\n                      205, #29\n                      534, #30\n                      147, #31\n                      26, #32\n                      761, #33\n                      45, #34\n                      994, #35\n                      144, #36\n                      315, #37\n                      198, #38\n                      1918, #39\n                      97, #40\n                      15, #41\n                      15, #42\n                      15, #43\n                      15, #44\n                      15, #45\n                      8, #46\n                      97, #47\n\n                      ]\n\n\nNB_CLASSES_LIST = [10, # 0\n                   7, # 1\n                   20, # 2\n                   20, # 3\n                   16, # 4\n                   88, # 5\n                   9, # 6\n\n                   # New benchmark datasets\n                   7, # 7\n                   5, # 8\n                   3, # 9\n                   2, # 10\n                   6, # 11\n                   2, # 12\n                   2, # 13\n                   2, # 14\n                   19, #15\n                   2, #16\n                   13, #17\n                   2, #18\n                   2, #19\n                   2, #20\n                   2,#21\n                   29, #22\n                   2, #23\n                   8, #24\n                   18, #25\n                   12, #26\n                   10, #27\n                   95, #28\n                   20, #29\n                   2, #30\n                   2, #31\n                   9, #32\n                   2, #33\n                   15, #34\n                   2, #35\n                   7, #36\n                   8, #37\n                   2, #38\n                   2, #39\n                   4, #40\n                   4, #41\n                   5, #42\n                   4, #43\n                   3, #44\n                   5, #45\n                   10, #46\n                   3,#47\n\n                   ]"""
utils/embedding_utils.py,0,"b'import os\nimport pickle\nimport numpy as np\nnp.random.seed(1000)\n\nfrom utils.generic_utils import load_dataset_at\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n\ndef load_embedding_matrix(embedding_path, word_index, max_nb_words, embedding_dim, print_error_words=True):\n    if not os.path.exists(\'data/embedding_matrix max words %d embedding dim %d.npy\' % (max_nb_words, embedding_dim)):\n        embeddings_index = {}\n        error_words = []\n\n        f = open(embedding_path, encoding=\'utf8\')\n        for line in f:\n            values = line.split()\n            word = values[0]\n            try:\n                coefs = np.asarray(values[1:], dtype=\'float32\')\n                embeddings_index[word] = coefs\n            except Exception:\n                error_words.append(word)\n\n        f.close()\n\n        if len(error_words) > 0:\n            print(""%d words were not added."" % (len(error_words)))\n            if print_error_words:\n                print(""Words are : \\n"", error_words)\n\n        print(\'Preparing embedding matrix.\')\n\n        # prepare embedding matrix\n        nb_words = min(max_nb_words, len(word_index))\n        embedding_matrix = np.zeros((nb_words, embedding_dim))\n        for word, i in word_index.items():\n            if i >= nb_words:\n                continue\n            embedding_vector = embeddings_index.get(word)\n            if embedding_vector is not None:\n                # words not found in embedding index will be all-zeros.\n                embedding_matrix[i] = embedding_vector\n\n        np.save(\'data/embedding_matrix max words %d embedding dim %d.npy\' % (max_nb_words,\n                                                                             embedding_dim),\n                embedding_matrix)\n\n        print(\'Saved embedding matrix\')\n\n    else:\n        embedding_matrix = np.load(\'data/embedding_matrix max words %d embedding dim %d.npy\' % (max_nb_words,\n                                                                                                embedding_dim))\n\n        print(\'Loaded embedding matrix\')\n\n    return embedding_matrix\n\n\ndef create_ngram_set(input_list, ngram_value=2):\n    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n\n\ndef add_ngram(sequences, token_indice, ngram_range=2):\n    new_sequences = []\n    for input_list in sequences:\n        new_list = input_list[:]\n        for i in range(len(new_list) - ngram_range + 1):\n            for ngram_value in range(2, ngram_range + 1):\n                ngram = tuple(new_list[i:i + ngram_value])\n                if ngram in token_indice:\n                    new_list.append(token_indice[ngram])\n        new_sequences.append(new_list)\n\n    return new_sequences\n\n\ndef prepare_tokenized_data(texts, max_nb_words, max_sequence_length, ngram_range=1):\n    if not os.path.exists(\'data/tokenizer.pkl\'):\n        tokenizer = Tokenizer(nb_words=max_nb_words)\n        tokenizer.fit_on_texts(texts)\n\n        with open(\'data/tokenizer.pkl\', \'wb\') as f:\n            pickle.dump(tokenizer, f)\n\n        print(\'Saved tokenizer.pkl\')\n    else:\n        with open(\'data/tokenizer.pkl\', \'rb\') as f:\n            tokenizer = pickle.load(f)\n            print(\'Loaded tokenizer.pkl\')\n\n    sequences = tokenizer.texts_to_sequences(texts)\n    word_index = tokenizer.word_index\n    print(\'Found %s unique 1-gram tokens.\' % len(word_index))\n\n    ngram_set = set()\n    for input_list in sequences:\n        for i in range(2, ngram_range + 1):\n            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n            ngram_set.update(set_of_ngram)\n\n    # Dictionary mapping n-gram token to a unique integer.\n    # Integer values are greater than max_features in order\n    # to avoid collision with existing features.\n    start_index = max_nb_words + 1\n    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n    indice_token = {token_indice[k]: k for k in token_indice}\n    word_index.update(token_indice)\n\n    max_features = np.max(list(indice_token.keys())) + 1\n    print(\'Now there are:\', max_features, \'features\')\n\n    # Augmenting X_train and X_test_mat with n-grams features\n    sequences = add_ngram(sequences, token_indice, ngram_range)\n    print(\'Average sequence length: {}\'.format(np.mean(list(map(len, sequences)), dtype=int)))\n    print(\'Max sequence length: {}\'.format(np.max(list(map(len, sequences)))))\n\n    data = pad_sequences(sequences, maxlen=max_sequence_length)\n\n\n    return (data, word_index)\n\n\n# def __load_embeddings(dataset_prefix, verbose=False):\n#\n#     embedding_path = npy_path # change to numpy data format (which contains the preloaded embedding matrix)\n#     if os.path.exists(embedding_path):\n#         # embedding matrix exists, no need to create again.\n#         print(""Loading embedding matrix for dataset \\\'%s\\\'"" % (dataset_prefix))\n#         embedding_matrix = np.load(embedding_path)\n#         return embedding_matrix\n#\n#     with open(txt_path, \'r\', encoding=\'utf8\') as f:\n#         header = f.readline()\n#         splits = header.split(\' \')\n#\n#         vocab_size = int(splits[0])\n#         embedding_size = int(splits[1])\n#\n#         embeddings_index = {}\n#         error_words = []\n#\n#         for line in f:\n#             values = line.split()\n#             word = values[0]\n#             try:\n#                 coefs = np.asarray(values[1:], dtype=\'float32\')\n#                 embeddings_index[word] = coefs\n#             except Exception:\n#                 error_words.append(word)\n#\n#         if len(error_words) > 0:\n#             print(""%d words were not added."" % (len(error_words)))\n#             if verbose:\n#                 print(""Words are : \\n"", error_words)\n#\n#         if verbose: print(\'Preparing embedding matrix.\')\n#\n#         embedding_matrix = np.zeros((vocab_size, embedding_size))\n#\n#         for key, vector in embeddings_index.items():\n#             if vector is not None:\n#                 # words not found in embedding index will be all-zeros.\n#                 key = int(key)\n#                 embedding_matrix[key] = vector\n#\n#         if verbose: print(\'Saving embedding matrix for dataset \\\'%s\\\'\' % (dataset_prefix))\n#\n#         np.save(embedding_path, embedding_matrix)\n#         return embedding_matrix\n'"
utils/generic_utils.py,0,"b'import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib as mpl\nimport matplotlib.pylab as plt\n\nmpl.style.use(\'seaborn-paper\')\n\nfrom utils.constants import TRAIN_FILES, TEST_FILES, MAX_NB_VARIABLES, NB_CLASSES_LIST\n\n\ndef load_dataset_at(index, fold_index=None, normalize_timeseries=False, verbose=True) -> (np.array, np.array):\n    if verbose: print(""Loading train / test dataset : "", TRAIN_FILES[index], TEST_FILES[index])\n\n    if fold_index is None:\n        x_train_path = TRAIN_FILES[index] + ""X_train.npy""\n        y_train_path = TRAIN_FILES[index] + ""y_train.npy""\n        x_test_path = TEST_FILES[index] + ""X_test.npy""\n        y_test_path = TEST_FILES[index] + ""y_test.npy""\n    else:\n        x_train_path = TRAIN_FILES[index] + ""X_train_%d.npy"" % fold_index\n        y_train_path = TRAIN_FILES[index] + ""y_train_%d.npy"" % fold_index\n        x_test_path = TEST_FILES[index] + ""X_test_%d.npy"" % fold_index\n        y_test_path = TEST_FILES[index] + ""y_test_%d.npy"" % fold_index\n\n    if os.path.exists(x_train_path):\n        X_train = np.load(x_train_path)\n        y_train = np.load(y_train_path)\n        X_test = np.load(x_test_path)\n        y_test = np.load(y_test_path)\n    elif os.path.exists(x_train_path[1:]):\n        X_train = np.load(x_train_path[1:])\n        y_train = np.load(y_train_path[1:])\n        X_test = np.load(x_test_path[1:])\n        y_test = np.load(y_test_path[1:])\n    else:\n        raise FileNotFoundError(\'File %s not found!\' % (TRAIN_FILES[index]))\n\n    is_timeseries = True\n\n    # extract labels Y and normalize to [0 - (MAX - 1)] range\n    nb_classes = len(np.unique(y_train))\n    y_train = (y_train - y_train.min()) / (y_train.max() - y_train.min()) * (nb_classes - 1)\n\n    if is_timeseries:\n        # scale the values\n        if normalize_timeseries:\n            X_train_mean = X_train.mean()\n            X_train_std = X_train.std()\n            X_train = (X_train - X_train_mean) / (X_train_std + 1e-8)\n\n    if verbose: print(""Finished processing train dataset.."")\n\n    # extract labels Y and normalize to [0 - (MAX - 1)] range\n    nb_classes = len(np.unique(y_test))\n    y_test = (y_test - y_test.min()) / (y_test.max() - y_test.min()) * (nb_classes - 1)\n\n    if is_timeseries:\n        # scale the values\n        if normalize_timeseries:\n            X_test = (X_test - X_train_mean) / (X_train_std + 1e-8)\n\n    if verbose:\n        print(""Finished loading test dataset.."")\n        print()\n        print(""Number of train samples : "", X_train.shape[0], ""Number of test samples : "", X_test.shape[0])\n        print(""Number of classes : "", nb_classes)\n        print(""Sequence length : "", X_train.shape[-1])\n\n    return X_train, y_train, X_test, y_test, is_timeseries\n\n\ndef calculate_dataset_metrics(X_train):\n    max_nb_variables = X_train.shape[1]\n    max_timesteps = X_train.shape[-1]\n\n    return max_timesteps, max_nb_variables\n\n\ndef cutoff_choice(dataset_id, sequence_length):\n    print(""Original sequence length was :"", sequence_length, ""New sequence Length will be : "",\n          MAX_NB_VARIABLES[dataset_id])\n    choice = input(\'Options : \\n\'\n                   \'`pre` - cut the sequence from the beginning\\n\'\n                   \'`post`- cut the sequence from the end\\n\'\n                   \'`anything else` - stop execution\\n\'\n                   \'To automate choice: add flag `cutoff` = choice as above\\n\'\n                   \'Choice = \')\n\n    choice = str(choice).lower()\n    return choice\n\n\ndef cutoff_sequence(X_train, X_test, choice, dataset_id, sequence_length):\n    assert MAX_NB_VARIABLES[dataset_id] < sequence_length, ""If sequence is to be cut, max sequence"" \\\n                                                                   ""length must be less than original sequence length.""\n    cutoff = sequence_length - MAX_NB_VARIABLES[dataset_id]\n    if choice == \'pre\':\n        if X_train is not None:\n            X_train = X_train[:, :, cutoff:]\n        if X_test is not None:\n            X_test = X_test[:, :, cutoff:]\n    else:\n        if X_train is not None:\n            X_train = X_train[:, :, :-cutoff]\n        if X_test is not None:\n            X_test = X_test[:, :, :-cutoff]\n    print(""New sequence length :"", MAX_NB_VARIABLES[dataset_id])\n    return X_train, X_test\n\n\nif __name__ == ""__main__"":\n    pass'"
utils/keras_utils.py,0,"b'import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nmpl.style.use(\'seaborn-paper\')\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport warnings\nwarnings.simplefilter(\'ignore\', category=DeprecationWarning)\n\nfrom keras.models import Model\nfrom keras.layers import Permute\nfrom keras.optimizers import Adam\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras import backend as K\n\nfrom utils.generic_utils import load_dataset_at, calculate_dataset_metrics, cutoff_choice, \\\n                                cutoff_sequence\nfrom utils.constants import MAX_NB_VARIABLES, MAX_TIMESTEPS_LIST\n\n\ndef multi_label_log_loss(y_pred, y_true):\n    return K.sum(K.binary_crossentropy(y_pred, y_true), axis=-1)\n\n\ndef _average_gradient_norm(model, X_train, y_train, batch_size):\n    # just checking if the model was already compiled\n    if not hasattr(model, ""train_function""):\n        raise RuntimeError(""You must compile your model before using it."")\n\n    weights = model.trainable_weights  # weight tensors\n\n    get_gradients = model.optimizer.get_gradients(model.total_loss, weights)  # gradient tensors\n\n    input_tensors = [\n        # input data\n        model.inputs[0],\n        # how much to weight each sample by\n        model.sample_weights[0],\n        # labels\n        model.targets[0],\n        # train or test mode\n        K.learning_phase()\n    ]\n\n    grad_fct = K.function(inputs=input_tensors, outputs=get_gradients)\n\n    steps = 0\n    total_norm = 0\n    s_w = None\n\n    nb_steps = X_train.shape[0] // batch_size\n\n    if X_train.shape[0] % batch_size == 0:\n        pad_last = False\n    else:\n        pad_last = True\n\n    def generator(X_train, y_train, pad_last):\n        for i in range(nb_steps):\n            X = X_train[i * batch_size: (i + 1) * batch_size, ...]\n            y = y_train[i * batch_size: (i + 1) * batch_size, ...]\n\n            yield (X, y)\n\n        if pad_last:\n            X = X_train[nb_steps * batch_size:, ...]\n            y = y_train[nb_steps * batch_size:, ...]\n\n            yield (X, y)\n\n    datagen = generator(X_train, y_train, pad_last)\n\n    while steps < nb_steps:\n        X, y = next(datagen)\n        # set sample weights to one\n        # for every input\n        if s_w is None:\n            s_w = np.ones(X.shape[0])\n\n        gradients = grad_fct([X, s_w, y, 0])\n        total_norm += np.sqrt(np.sum([np.sum(np.square(g)) for g in gradients]))\n        steps += 1\n\n    if pad_last:\n        X, y = next(datagen)\n        # set sample weights to one\n        # for every input\n        if s_w is None:\n            s_w = np.ones(X.shape[0])\n\n        gradients = grad_fct([X, s_w, y, 0])\n        total_norm += np.sqrt(np.sum([np.sum(np.square(g)) for g in gradients]))\n        steps += 1\n\n    return total_norm / float(steps)\n\n\n\ndef train_model(model:Model, dataset_id, dataset_prefix, dataset_fold_id=None, epochs=50, batch_size=128, val_subset=None,\n                cutoff=None, normalize_timeseries=False, learning_rate=1e-3, monitor=\'loss\', optimization_mode=\'auto\', compile_model=True):\n    X_train, y_train, X_test, y_test, is_timeseries = load_dataset_at(dataset_id,\n                                                                      fold_index=dataset_fold_id,\n                                                                      normalize_timeseries=normalize_timeseries)\n    max_timesteps, max_nb_variables = calculate_dataset_metrics(X_train)\n\n    if max_nb_variables != MAX_NB_VARIABLES[dataset_id]:\n        if cutoff is None:\n            choice = cutoff_choice(dataset_id, max_nb_variables)\n        else:\n            assert cutoff in [\'pre\', \'post\'], \'Cutoff parameter value must be either ""pre"" or ""post""\'\n            choice = cutoff\n\n        if choice not in [\'pre\', \'post\']:\n            return\n        else:\n            X_train, X_test = cutoff_sequence(X_train, X_test, choice, dataset_id, max_nb_variables)\n\n    classes = np.unique(y_train)\n    le = LabelEncoder()\n    y_ind = le.fit_transform(y_train.ravel())\n    recip_freq = len(y_train) / (len(le.classes_) *\n                           np.bincount(y_ind).astype(np.float64))\n    class_weight = recip_freq[le.transform(classes)]\n\n    print(""Class weights : "", class_weight)\n\n    y_train = to_categorical(y_train, len(np.unique(y_train)))\n    y_test = to_categorical(y_test, len(np.unique(y_test)))\n\n    if is_timeseries:\n        factor = 1. / np.cbrt(2)\n    else:\n        factor = 1. / np.sqrt(2)\n\n    if dataset_fold_id is None:\n        weight_fn = ""./weights/%s_weights.h5"" % dataset_prefix\n    else:\n        weight_fn = ""./weights/%s_fold_%d_weights.h5"" % (dataset_prefix, dataset_fold_id)\n\n    model_checkpoint = ModelCheckpoint(weight_fn, verbose=1, mode=optimization_mode,\n                                       monitor=monitor, save_best_only=True, save_weights_only=True)\n    reduce_lr = ReduceLROnPlateau(monitor=monitor, patience=100, mode=optimization_mode,\n                                  factor=factor, cooldown=0, min_lr=1e-4, verbose=2)\n    callback_list = [model_checkpoint, reduce_lr]\n\n    optm = Adam(lr=learning_rate)\n\n    if compile_model:\n        model.compile(optimizer=optm, loss=\'categorical_crossentropy\', metrics=[\'accuracy\'])\n\n    if val_subset is not None:\n        X_test = X_test[:val_subset]\n        y_test = y_test[:val_subset]\n\n    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=callback_list,\n              class_weight=class_weight, verbose=2, validation_data=(X_test, y_test))\n\n\ndef evaluate_model(model:Model, dataset_id, dataset_prefix, dataset_fold_id=None, batch_size=128, test_data_subset=None,\n                   cutoff=None, normalize_timeseries=False):\n    _, _, X_test, y_test, is_timeseries = load_dataset_at(dataset_id,\n                                                          fold_index=dataset_fold_id,\n                                                          normalize_timeseries=normalize_timeseries)\n    max_timesteps, max_nb_variables = calculate_dataset_metrics(X_test)\n\n    if max_nb_variables != MAX_NB_VARIABLES[dataset_id]:\n        if cutoff is None:\n            choice = cutoff_choice(dataset_id, max_nb_variables)\n        else:\n            assert cutoff in [\'pre\', \'post\'], \'Cutoff parameter value must be either ""pre"" or ""post""\'\n            choice = cutoff\n\n        if choice not in [\'pre\', \'post\']:\n            return\n        else:\n            _, X_test = cutoff_sequence(None, X_test, choice, dataset_id, max_nb_variables)\n\n    if not is_timeseries:\n        X_test = pad_sequences(X_test, maxlen=MAX_NB_VARIABLES[dataset_id], padding=\'post\', truncating=\'post\')\n    y_test = to_categorical(y_test, len(np.unique(y_test)))\n\n    optm = Adam(lr=1e-3)\n    model.compile(optimizer=optm, loss=\'categorical_crossentropy\', metrics=[\'accuracy\'])\n\n    if dataset_fold_id is None:\n        weight_fn = ""./weights/%s_weights.h5"" % dataset_prefix\n    else:\n        weight_fn = ""./weights/%s_fold_%d_weights.h5"" % (dataset_prefix, dataset_fold_id)\n    model.load_weights(weight_fn)\n\n    if test_data_subset is not None:\n        X_test = X_test[:test_data_subset]\n        y_test = y_test[:test_data_subset]\n\n    print(""\\nEvaluating : "")\n    loss, accuracy = model.evaluate(X_test, y_test, batch_size=batch_size)\n    print()\n    print(""Final Accuracy : "", accuracy)\n\n    return accuracy, loss\n\ndef set_trainable(layer, value):\n   layer.trainable = value\n\n   # case: container\n   if hasattr(layer, \'layers\'):\n       for l in layer.layers:\n           set_trainable(l, value)\n\n   # case: wrapper (which is a case not covered by the PR)\n   if hasattr(layer, \'layer\'):\n        set_trainable(layer.layer, value)\n\n\ndef compute_average_gradient_norm(model:Model, dataset_id, dataset_fold_id=None, batch_size=128,\n                cutoff=None, normalize_timeseries=False, learning_rate=1e-3):\n    X_train, y_train, X_test, y_test, is_timeseries = load_dataset_at(dataset_id,\n                                                                      fold_index=dataset_fold_id,\n                                                                      normalize_timeseries=normalize_timeseries)\n    max_timesteps, sequence_length = calculate_dataset_metrics(X_train)\n\n    if sequence_length != MAX_NB_VARIABLES[dataset_id]:\n        if cutoff is None:\n            choice = cutoff_choice(dataset_id, sequence_length)\n        else:\n            assert cutoff in [\'pre\', \'post\'], \'Cutoff parameter value must be either ""pre"" or ""post""\'\n            choice = cutoff\n\n        if choice not in [\'pre\', \'post\']:\n            return\n        else:\n            X_train, X_test = cutoff_sequence(X_train, X_test, choice, dataset_id, sequence_length)\n\n    y_train = to_categorical(y_train, len(np.unique(y_train)))\n\n    optm = Adam(lr=learning_rate)\n    model.compile(optimizer=optm, loss=\'categorical_crossentropy\', metrics=[\'accuracy\'])\n\n    average_gradient = _average_gradient_norm(model, X_train, y_train, batch_size)\n    print(""Average gradient norm : "", average_gradient)\n\n\nclass MaskablePermute(Permute):\n\n    def __init__(self, dims, **kwargs):\n        super(MaskablePermute, self).__init__(dims, **kwargs)\n        self.supports_masking = True\n\n\ndef f1_score(y_true, y_pred):\n    def recall(y_true, y_pred):\n        """"""Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        """"""\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        """"""Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        """"""\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n\n    return 2 * ((precision * recall) / (precision + recall))\n\n\n'"
utils/layer_utils.py,0,"b'from __future__ import absolute_import\nimport numpy as np\n\nfrom keras import backend as K\nfrom keras import activations\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras.engine import Layer\nfrom keras.engine import InputSpec\nfrom keras.legacy import interfaces\nfrom keras.layers import Recurrent\n\n\ndef _time_distributed_dense(x, w, b=None, dropout=None,\n                            input_dim=None, output_dim=None,\n                            timesteps=None, training=None):\n    """"""Apply `y . w + b` for every temporal slice y of x.\n\n    # Arguments\n        x: input tensor.\n        w: weight matrix.\n        b: optional bias vector.\n        dropout: wether to apply dropout (same dropout mask\n            for every temporal slice of the input).\n        input_dim: integer; optional dimensionality of the input.\n        output_dim: integer; optional dimensionality of the output.\n        timesteps: integer; optional number of timesteps.\n        training: training phase tensor or boolean.\n\n    # Returns\n        Output tensor.\n    """"""\n    if not input_dim:\n        input_dim = K.shape(x)[2]\n    if not timesteps:\n        timesteps = K.shape(x)[1]\n    if not output_dim:\n        output_dim = K.int_shape(w)[1]\n\n    if dropout is not None and 0. < dropout < 1.:\n        # apply the same dropout pattern at every timestep\n        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n        dropout_matrix = K.dropout(ones, dropout)\n        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n\n    # collapse time dimension and batch dimension together\n    x = K.reshape(x, (-1, input_dim))\n    x = K.dot(x, w)\n    if b is not None:\n        x = K.bias_add(x, b)\n    # reshape to 3D tensor\n    if K.backend() == \'tensorflow\':\n        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n        x.set_shape([None, None, output_dim])\n    else:\n        x = K.reshape(x, (-1, timesteps, output_dim))\n    return x\n\n\nclass AttentionLSTM(Recurrent):\n    """"""Long-Short Term Memory unit - with Attention.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](keras/activations.md)).\n            If you pass None, no activation is applied\n            (ie. ""linear"" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](keras/activations.md)).\n        attention_activation: Activation function to use\n            for the attention step. If you pass None, no activation is applied\n            (ie. ""linear"" activation: `a(x) = x`).\n            (see [activations](keras/activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        attention_initializer: Initializer for the `attention_kernel` weights\n            matrix, used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        unit_forget_bias: Boolean.\n            If True, add 1 to the bias of the forget gate at initialization.\n            Setting it to true will also force `bias_initializer=""zeros""`.\n            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its ""activation"").\n            (see [regularizer](../regularizers.md)).\n        attention_regularizer: Regularizer function applied to\n            the `attention_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        attention_constraint: Constraint function applied to\n            the `attention_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n        return_attention: Returns the attention vector instead of\n            the internal state.\n\n    # References\n        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n        - [Bahdanau, Cho & Bengio (2014), ""Neural Machine Translation by Jointly Learning to Align and Translate""](https://arxiv.org/pdf/1409.0473.pdf)\n        - [Xu, Ba, Kiros, Cho, Courville, Salakhutdinov, Zemel & Bengio (2016), ""Show, Attend and Tell: Neural Image Caption Generation with Visual Attention""](http://arxiv.org/pdf/1502.03044.pdf)\n    """"""\n    @interfaces.legacy_recurrent_support\n    def __init__(self, units,\n                 activation=\'tanh\',\n                 recurrent_activation=\'hard_sigmoid\',\n                 attention_activation=\'tanh\',\n                 use_bias=True,\n                 kernel_initializer=\'glorot_uniform\',\n                 recurrent_initializer=\'orthogonal\',\n                 attention_initializer=\'orthogonal\',\n                 bias_initializer=\'zeros\',\n                 unit_forget_bias=True,\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 attention_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 attention_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 return_attention=False,\n                 implementation=1,\n                 **kwargs):\n        super(AttentionLSTM, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.attention_activation = activations.get(attention_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.attention_initializer = initializers.get(attention_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.unit_forget_bias = unit_forget_bias\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n        self.attention_regularizer = regularizers.get(attention_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n        self.attention_constraint = constraints.get(attention_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n        self.return_attention = return_attention\n        self.state_spec = [InputSpec(shape=(None, self.units)),\n                           InputSpec(shape=(None, self.units))]\n        self.implementation = implementation\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        batch_size = input_shape[0] if self.stateful else None\n        self.timestep_dim = input_shape[1]\n        self.input_dim = input_shape[2]\n        self.input_spec[0] = InputSpec(shape=(batch_size, None, self.input_dim))\n\n        self.states = [None, None]\n        if self.stateful:\n            self.reset_states()\n\n        self.kernel = self.add_weight(shape=(self.input_dim, self.units * 4),\n                                      name=\'kernel\',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            shape=(self.units, self.units * 4),\n            name=\'recurrent_kernel\',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        # add attention kernel\n        self.attention_kernel = self.add_weight(\n            shape=(self.input_dim, self.units * 4),\n            name=\'attention_kernel\',\n            initializer=self.attention_initializer,\n            regularizer=self.attention_regularizer,\n            constraint=self.attention_constraint)\n\n        # add attention weights\n        # weights for attention model\n        self.attention_weights = self.add_weight(shape=(self.input_dim, self.units),\n                                                 name=\'attention_W\',\n                                                 initializer=self.attention_initializer,\n                                                 regularizer=self.attention_regularizer,\n                                                 constraint=self.attention_constraint)\n\n        self.attention_recurrent_weights = self.add_weight(shape=(self.units, self.units),\n                                                           name=\'attention_U\',\n                                                           initializer=self.recurrent_initializer,\n                                                           regularizer=self.recurrent_regularizer,\n                                                           constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            if self.unit_forget_bias:\n                def bias_initializer(shape, *args, **kwargs):\n                    return K.concatenate([\n                        self.bias_initializer((self.units,), *args, **kwargs),\n                        initializers.Ones()((self.units,), *args, **kwargs),\n                        self.bias_initializer((self.units * 2,), *args, **kwargs),\n                    ])\n            else:\n                bias_initializer = self.bias_initializer\n            self.bias = self.add_weight(shape=(self.units * 4,),\n                                        name=\'bias\',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n\n            self.attention_bias = self.add_weight(shape=(self.units,),\n                                                  name=\'attention_b\',\n                                                  initializer=self.bias_initializer,\n                                                  regularizer=self.bias_regularizer,\n                                                  constraint=self.bias_constraint)\n\n            self.attention_recurrent_bias = self.add_weight(shape=(self.units, 1),\n                                                            name=\'attention_v\',\n                                                            initializer=self.bias_initializer,\n                                                            regularizer=self.bias_regularizer,\n                                                            constraint=self.bias_constraint)\n        else:\n            self.bias = None\n            self.attention_bias = None\n            self.attention_recurrent_bias = None\n\n        self.kernel_i = self.kernel[:, :self.units]\n        self.kernel_f = self.kernel[:, self.units: self.units * 2]\n        self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n        self.kernel_o = self.kernel[:, self.units * 3:]\n\n        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n        self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n\n        self.attention_i = self.attention_kernel[:, :self.units]\n        self.attention_f = self.attention_kernel[:, self.units: self.units * 2]\n        self.attention_c = self.attention_kernel[:, self.units * 2: self.units * 3]\n        self.attention_o = self.attention_kernel[:, self.units * 3:]\n\n        if self.use_bias:\n            self.bias_i = self.bias[:self.units]\n            self.bias_f = self.bias[self.units: self.units * 2]\n            self.bias_c = self.bias[self.units * 2: self.units * 3]\n            self.bias_o = self.bias[self.units * 3:]\n        else:\n            self.bias_i = None\n            self.bias_f = None\n            self.bias_c = None\n            self.bias_o = None\n\n        self.built = True\n\n    def preprocess_input(self, inputs, training=None):\n        return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation != 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(4)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(4)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n\n        # append the input as well for use later\n        constants.append(inputs)\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]\n        c_tm1 = states[1]\n        dp_mask = states[2]\n        rec_dp_mask = states[3]\n        x_input = states[4]\n\n        # alignment model\n        h_att = K.repeat(h_tm1, self.timestep_dim)\n        att = _time_distributed_dense(x_input, self.attention_weights, self.attention_bias,\n                                      output_dim=K.int_shape(self.attention_weights)[1])\n        attention_ = self.attention_activation(K.dot(h_att, self.attention_recurrent_weights) + att)\n        attention_ = K.squeeze(K.dot(attention_, self.attention_recurrent_bias), 2)\n\n        alpha = K.exp(attention_)\n\n        if dp_mask is not None:\n            alpha *= dp_mask[0]\n\n        alpha /= K.sum(alpha, axis=1, keepdims=True)\n        alpha_r = K.repeat(alpha, self.input_dim)\n        alpha_r = K.permute_dimensions(alpha_r, (0, 2, 1))\n\n        # make context vector (soft attention after Bahdanau et al.)\n        z_hat = x_input * alpha_r\n        context_sequence = z_hat\n        z_hat = K.sum(z_hat, axis=1)\n\n        if self.implementation == 2:\n            z = K.dot(inputs * dp_mask[0], self.kernel)\n            z += K.dot(h_tm1 * rec_dp_mask[0], self.recurrent_kernel)\n            z += K.dot(z_hat, self.attention_kernel)\n\n            if self.use_bias:\n                z = K.bias_add(z, self.bias)\n\n            z0 = z[:, :self.units]\n            z1 = z[:, self.units: 2 * self.units]\n            z2 = z[:, 2 * self.units: 3 * self.units]\n            z3 = z[:, 3 * self.units:]\n\n            i = self.recurrent_activation(z0)\n            f = self.recurrent_activation(z1)\n            c = f * c_tm1 + i * self.activation(z2)\n            o = self.recurrent_activation(z3)\n        else:\n            if self.implementation == 0:\n                x_i = inputs[:, :self.units]\n                x_f = inputs[:, self.units: 2 * self.units]\n                x_c = inputs[:, 2 * self.units: 3 * self.units]\n                x_o = inputs[:, 3 * self.units:]\n            elif self.implementation == 1:\n                x_i = K.dot(inputs * dp_mask[0], self.kernel_i) + self.bias_i\n                x_f = K.dot(inputs * dp_mask[1], self.kernel_f) + self.bias_f\n                x_c = K.dot(inputs * dp_mask[2], self.kernel_c) + self.bias_c\n                x_o = K.dot(inputs * dp_mask[3], self.kernel_o) + self.bias_o\n            else:\n                raise ValueError(\'Unknown `implementation` mode.\')\n\n            i = self.recurrent_activation(x_i + K.dot(h_tm1 * rec_dp_mask[0], self.recurrent_kernel_i)\n                                              + K.dot(z_hat, self.attention_i))\n            f = self.recurrent_activation(x_f + K.dot(h_tm1 * rec_dp_mask[1], self.recurrent_kernel_f)\n                                          + K.dot(z_hat, self.attention_f))\n            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * rec_dp_mask[2], self.recurrent_kernel_c)\n                                                + K.dot(z_hat, self.attention_c))\n            o = self.recurrent_activation(x_o + K.dot(h_tm1 * rec_dp_mask[3], self.recurrent_kernel_o)\n                                          + K.dot(z_hat, self.attention_o))\n        h = o * self.activation(c)\n        if 0 < self.dropout + self.recurrent_dropout:\n            h._uses_learning_phase = True\n\n        if self.return_attention:\n            return context_sequence, [h, c]\n        else:\n            return h, [h, c]\n\n    def get_config(self):\n        config = {\'units\': self.units,\n                  \'activation\': activations.serialize(self.activation),\n                  \'recurrent_activation\': activations.serialize(self.recurrent_activation),\n                  \'attention_activation\': activations.serialize(self.attention_activation),\n                  \'use_bias\': self.use_bias,\n                  \'kernel_initializer\': initializers.serialize(self.kernel_initializer),\n                  \'recurrent_initializer\': initializers.serialize(self.recurrent_initializer),\n                  \'bias_initializer\': initializers.serialize(self.bias_initializer),\n                  \'attention_initializer\': initializers.serialize(self.attention_initializer),\n                  \'unit_forget_bias\': self.unit_forget_bias,\n                  \'kernel_regularizer\': regularizers.serialize(self.kernel_regularizer),\n                  \'recurrent_regularizer\': regularizers.serialize(self.recurrent_regularizer),\n                  \'bias_regularizer\': regularizers.serialize(self.bias_regularizer),\n                  \'activity_regularizer\': regularizers.serialize(self.activity_regularizer),\n                  \'attention_regularizer\': regularizers.serialize(self.attention_regularizer),\n                  \'kernel_constraint\': constraints.serialize(self.kernel_constraint),\n                  \'recurrent_constraint\': constraints.serialize(self.recurrent_constraint),\n                  \'bias_constraint\': constraints.serialize(self.bias_constraint),\n                  \'attention_constraint\': constraints.serialize(self.attention_constraint),\n                  \'dropout\': self.dropout,\n                  \'recurrent_dropout\': self.recurrent_dropout,\n                  \'return_attention\': self.return_attention}\n        base_config = super(AttentionLSTM, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n'"
data/AREM/generate_arem_dataset.py,0,"b'import numpy as np\nnp.random.seed(1000)\n\nfrom scipy.io import loadmat\n\narem_path = r""""\n\n\'\'\' Load train set \'\'\'\ndata_dict = loadmat(arem_path + ""AReM.mat"")\nX_train_mat = data_dict[\'X_train\'][0]\ny_train_mat = data_dict[\'Y_train\'][0]\nX_test_mat = data_dict[\'X_test\'][0]\ny_test_mat = data_dict[\'Y_test\'][0]\n\ny_train = y_train_mat.reshape(-1, 1)\ny_test = y_test_mat.reshape(-1, 1)\n\nvar_list = []\nfor i in range(X_train_mat.shape[0]):\n    var_count = X_train_mat[i].shape[-1]\n    var_list.append(var_count)\n\nvar_list = np.array(var_list)\nmax_nb_timesteps = var_list.max()\nmin_nb_timesteps = var_list.min()\nmedian_nb_timesteps = np.median(var_list)\n\nprint(\'max nb timesteps train : \', max_nb_timesteps)\nprint(\'min nb timesteps train : \', min_nb_timesteps)\nprint(\'median_nb_timesteps nb timesteps train : \', median_nb_timesteps)\n\nX_train = np.zeros((X_train_mat.shape[0], X_train_mat[0].shape[0], max_nb_timesteps))\n\n# pad ending with zeros to get numpy arrays\nfor i in range(X_train_mat.shape[0]):\n    var_count = X_train_mat[i].shape[-1]\n    #print(i, X_train_mat[i])\n    X_train[i, :, :var_count] = X_train_mat[i]\n\n# \'\'\' Load test set \'\'\'\n\nX_test = np.zeros((X_test_mat.shape[0], X_test_mat[0].shape[0], max_nb_timesteps))\n\n# pad ending with zeros to get numpy arrays\nfor i in range(X_test_mat.shape[0]):\n    var_count = X_test_mat[i].shape[-1]\n    X_test[i, :, :var_count] = X_test_mat[i]\n\n# \'\'\' Save the datasets \'\'\'\nprint(""Train dataset : "", X_train.shape, y_train.shape)\nprint(""Test dataset : "", X_test.shape, y_test.shape)\nprint(""Train dataset metrics : "", X_train.mean(), X_train.std())\nprint(""Test dataset : "", X_test.mean(), X_test.std())\nprint(""Nb classes : "", len(np.unique(y_train)))\n\nnp.save(arem_path + \'X_train.npy\', X_train)\nnp.save(arem_path + \'y_train.npy\', y_train)\nnp.save(arem_path + \'X_test.npy\', X_test)\nnp.save(arem_path + \'y_test.npy\', y_test)\n'"
data/Action3D/generate_action_3d_dataset.py,0,"b'import scipy.io as sio\nimport numpy as np\n\n\ndef find1(a, func):\n    qqq = [i for (i, val) in enumerate(a) if func(val)]\n    return (np.array(qqq) + 1)\n\n\naction_3d_path = r\'\'\n\nDATA = sio.loadmat(action_3d_path + \'joint_feat_coordinate.mat\')\n\nfeat = DATA[\'feat\'][0]\nif_contain = DATA[\'if_contain\'][0]\nlabels = DATA[\'labels\'][0]\n\ndata = feat\n\nK = 20\ntrain_ind = []\ntest_ind = []\ntestActors = [6, 7, 8, 9, 10]\ni = 1\ntrue_i = 0\n\nfor a in range(1, 21):\n    for j in range(1, 11):\n        for e in range(1, 4):\n            if (if_contain[i - 1] == 0):\n                i = i + 1\n                continue\n            true_i = true_i + 1\n            if not (np.all((find1(testActors, lambda x: x == j)) == 0)):\n                test_ind.append(true_i)\n            else:\n                train_ind.append(true_i)\n            i = i + 1\n\n\'\'\' Load train set \'\'\'\nX = data[(np.array(train_ind) - 1)]\n\nvar_list = []\nfor i in range(X.shape[0]):\n    var_count = X[i].shape[-1]\n    var_list.append(var_count)\n\nvar_list = np.array(var_list)\nmax_nb_timesteps = var_list.max()\nmin_nb_timesteps = var_list.min()\nmedian_nb_timesteps = np.median(var_list)\n\nprint(\'max nb timesteps train : \', max_nb_timesteps)\nprint(\'min nb timesteps train : \', min_nb_timesteps)\nprint(\'median_nb_timesteps nb timesteps train : \', median_nb_timesteps)\n\nX_train = np.zeros((X.shape[0], X[0].shape[0], max_nb_timesteps))\ny_train = labels[(np.array(train_ind) - 1)]\n\n# pad ending with zeros to get numpy arrays\nfor i in range(X_train.shape[0]):\n    var_count = X[i].shape[-1]\n    X_train[i, :, :var_count] = X[i]\n\nX_train_mean = X_train.mean()\nX_train_std = X_train.std()\nprint(""Train Mean +- std : "", X_train_mean, X_train_std)\n#X_train_mat = (X_train_mat - X_train_mean) / (X_train_std + 1e-8)\n\n\'\'\' Load test set \'\'\'\nX = data[(np.array(test_ind) - 1)]\n\nX_test = np.zeros((X.shape[0], X[0].shape[0], max_nb_timesteps))\ny_test = labels[(np.array(test_ind) - 1)]\n\nmax_variables_test = -np.inf\ncount = 0\n\nfor i in range(X.shape[0]):\n    var_count = X[i].shape[-1]\n\n    if var_count > max_nb_timesteps:\n        max_variables_test = var_count\n        count += 1\n\nprint(\'max nb variables test : \', max_variables_test)\nprint(""# of instances where test vars > %d : "" % max_nb_timesteps, count)\n\nprint(""\\nSince there is only %d instance where test # variables > %d (max # of variables in train), ""\n      ""we clip the specific instance to match %d variables\\n"" % (count, max_nb_timesteps, max_nb_timesteps))\n\n# pad ending with zeros to get numpy arrays\nfor i in range(X_test.shape[0]):\n    var_count = X[i].shape[-1]\n    X_test[i, :, :var_count] = X[i][:, :max_nb_timesteps]\n\n#X_test_mat = (X_test_mat - X_train_mean) / (X_train_std + 1e-8)\n\n\'\'\' Save the datasets \'\'\'\nprint(""Train dataset : "", X_train.shape, y_train.shape)\nprint(""Test dataset : "", X_test.shape, y_test.shape)\nprint(""Train dataset metrics : "", X_train.mean(), X_train.std())\nprint(""Test dataset : "", X_test.mean(), X_test.std())\nprint(""Nb classes : "", len(np.unique(y_train)))\n\nnp.save(action_3d_path + \'X_train_mat.npy\', X_train)\nnp.save(action_3d_path + \'y_train_mat.npy\', y_train)\nnp.save(action_3d_path + \'X_test_mat.npy\', X_test)\nnp.save(action_3d_path + \'y_test_mat.npy\', y_test)'"
data/Activity/generate_activity_dataset.py,0,"b'import scipy.io as sio\nimport numpy as np\n\n\ndef find1(a, func):\n    qqq = [i for (i, val) in enumerate(a) if func(val)]\n    return (np.array(qqq) + 1)\n\n\nactivity_path = r""""\n\nDATA = sio.loadmat(activity_path + \'joint3D_feature_noFFT.mat\')\n\nJoint3D_feature = DATA[\'Joint3D_feature\'][0]\nlabels = DATA[\'labels\'][0]\n\ndata = Joint3D_feature\nK = 16\ntrain_ind = []\ntest_ind = []\ntestActors = [1, 2, 3, 4, 5]\ntestClass = range(1, 17)\ntrue_i = 0\n\nfor a in range(1, 17):\n    for j in range(1, 11):\n        for e in range(1, 3):\n            true_i = true_i + 1\n            if not (np.all((find1(testActors, lambda x: x == j)) == 0)):\n                test_ind.append(true_i)\n            else:\n                train_ind.append(true_i)\n\n\'\'\' Load train set \'\'\'\nX = data[(np.array(train_ind) - 1)]\n\nvar_list = []\nfor i in range(X.shape[0]):\n    var_count = X[i].shape[-1]\n    var_list.append(var_count)\n\nvar_list = np.array(var_list)\nmax_nb_timesteps = var_list.max()\nmin_nb_timesteps = var_list.min()\nmedian_nb_timesteps = np.median(var_list)\n\nprint(\'max nb timesteps train : \', max_nb_timesteps)\nprint(\'min nb timesteps train : \', min_nb_timesteps)\nprint(\'median_nb_timesteps nb timesteps train : \', median_nb_timesteps)\n\n\nX_train = np.zeros((X.shape[0], X[0].shape[0], max_nb_timesteps))\ny_train = labels[(np.array(train_ind) - 1)]\n\n# pad ending with zeros to get numpy arrays\nfor i in range(X_train.shape[0]):\n    var_count = X[i].shape[-1]\n    X_train[i, :, :var_count] = X[i]\n\n\'\'\' Load test set \'\'\'\nX = data[(np.array(test_ind) - 1)]\n\nX_test = np.zeros((X.shape[0], X[0].shape[0], max_nb_timesteps))\ny_test = labels[(np.array(test_ind) - 1)]\n\nmax_variables_test = -np.inf\ncount = 0\n\nfor i in range(X.shape[0]):\n    var_count = X[i].shape[-1]\n\n    if var_count > max_nb_timesteps:\n        max_variables_test = var_count\n        count += 1\n\nprint(\'max nb variables test : \', max_variables_test)\nprint(""# of instances where test vars > %d : "" % max_nb_timesteps, count)\n\nprint(""\\nSince there is only %d instance where test # variables > %d (max # of variables in train), ""\n      ""we clip the specific instance to match %d variables\\n"" % (count, max_nb_timesteps, max_nb_timesteps))\n\n# pad ending with zeros to get numpy arrays\nfor i in range(X_test.shape[0]):\n    var_count = X[i].shape[-1]\n    X_test[i, :, :var_count] = X[i][:, :max_nb_timesteps]\n\n\'\'\' Save the datasets \'\'\'\nprint(""Train dataset : "", X_train.shape, y_train.shape)\nprint(""Test dataset : "", X_test.shape, y_test.shape)\nprint(""Train dataset metrics : "", X_train.mean(), X_train.std())\nprint(""Test dataset : "", X_test.mean(), X_test.std())\nprint(""Nb classes : "", len(np.unique(y_train)))\n\nnp.save(activity_path + \'X_train.npy\', X_train)\nnp.save(activity_path + \'y_train.npy\', y_train)\nnp.save(activity_path + \'X_test.npy\', X_test)\nnp.save(activity_path + \'y_test.npy\', y_test)\n'"
data/CK/generate_ck_dataset.py,0,"b'import scipy.io as sio\n\nimport numpy as np\n\nck_path = r\'\'\nDATA = sio.loadmat(ck_path + \'randomperm_CK.mat\')\n\nvar_list = None\nno_folds = 10\n\nfor k in range(10):\n    index_fold = k + 1 # index fold\n    pre = 0\n\n    train_ind = []\n    test_ind = []\n    each_label_number = DATA[\'each_label_number\'][0]\n    labels_n = DATA[\'labels_n\'][0]\n\n    for i in range(0, labels_n[0]):\n        fold_size = np.floor(each_label_number[i] / no_folds)\n        ind = (index_fold - 1) * fold_size + 1\n        train_ind.append(list(range(pre + 1, int(pre + ind - 1 + 1))))\n        train_ind.append(list(range(int(pre + ind + fold_size), int(pre + 1 + each_label_number[i]))))\n        test_ind.append(list(range(int(pre + ind), int(pre + ind + fold_size - 1 + 1))))\n        pre = pre + each_label_number[i]\n\n    trainind = [item for sublist in train_ind for item in sublist]\n    testind = [item for sublist in test_ind for item in sublist]\n\n    X = DATA[\'new_X\'][0, (np.array(trainind) - 1)]  ###Train data\n    y = DATA[\'new_labels\'][0, (np.array(trainind) - 1)]  ###Train Labels\n\n    \'\'\' Load train Dataset \'\'\'\n    if var_list is None:\n        var_list = []\n        for i in range(X.shape[0]):\n            var_count = X[i].shape[-1]\n            var_list.append(var_count)\n\n        var_list = np.array(var_list)\n        max_nb_timesteps = var_list.max()\n        min_nb_timesteps = var_list.min()\n        median_nb_timesteps = np.median(var_list)\n\n        print(\'max nb timesteps train : \', max_nb_timesteps)\n        print(\'min nb timesteps train : \', min_nb_timesteps)\n        print(\'median_nb_timesteps nb timesteps train : \', median_nb_timesteps)\n\n        print(\'-\' * 80)\n        print()\n\n    X_train = np.zeros((X.shape[0], X[0].shape[0], max_nb_timesteps))\n    y_train = y\n\n    # pad ending with zeros to get numpy arrays\n    for i in range(X_train.shape[0]):\n        var_count = X[i].shape[-1]\n        X_train[i, :, :var_count] = X[i]\n\n    \'\'\' Load test dataset\'\'\'\n    X = DATA[\'new_X\'][0, (np.array(testind) - 1)]  ####Test Data\n    y = DATA[\'new_labels\'][0, (np.array(testind) - 1)]  ####Test Labels\n\n    X_test = np.zeros((X.shape[0], X[0].shape[0], max_nb_timesteps))\n    y_test = y\n\n    # pad ending with zeros to get numpy arrays\n    for i in range(X_test.shape[0]):\n        var_count = X[i].shape[-1]\n        X_test[i, :, :var_count] = X[i]\n\n    \'\'\' Save the datasets \'\'\'\n    print(""Dataset Fold #%d"" % index_fold)\n    print(""Train dataset : "", X_train.shape, y_train.shape)\n    print(""Test dataset : "", X_test.shape, y_test.shape)\n    print(""Train dataset metrics : "", X_train.mean(), X_train.std())\n    print(""Test dataset : "", X_test.mean(), X_test.std())\n    print(""Nb classes : "", len(np.unique(y_train)), ""Classes : "", np.unique(y_train))\n\n    np.save(ck_path + \'X_train_%d.npy\' % index_fold, X_train)\n    np.save(ck_path + \'y_train_%d.npy\' % index_fold, y_train)\n    np.save(ck_path + \'X_test_%d.npy\' % index_fold, X_test)\n    np.save(ck_path + \'y_test_%d.npy\' % index_fold, y_test)\n\n    print()\n'"
data/HAR/generate_har_dataset.py,0,"b'import numpy as np\nnp.random.seed(1000)\n\nfrom scipy.io import loadmat\n\nhar_dataset = r""""\n\n\'\'\' Load train set \'\'\'\ndata_dict = loadmat(har_dataset + ""UCI_HAR_DATASET.mat"")\nX_train_mat = data_dict[\'X_train\'][0]\ny_train_mat = data_dict[\'Y_train\'][0]\nX_test_mat = data_dict[\'X_test\'][0]\ny_test_mat = data_dict[\'Y_test\'][0]\n\ny_train = y_train_mat.reshape(-1, 1)\ny_test = y_test_mat.reshape(-1, 1)\n\nvar_list = []\nfor i in range(X_train_mat.shape[0]):\n    var_count = X_train_mat[i].shape[-1]\n    var_list.append(var_count)\n\nvar_list = np.array(var_list)\nmax_nb_timesteps = var_list.max()\nmin_nb_timesteps = var_list.min()\nmedian_nb_timesteps = np.median(var_list)\n\nprint(\'max nb timesteps train : \', max_nb_timesteps)\nprint(\'min nb timesteps train : \', min_nb_timesteps)\nprint(\'median_nb_timesteps nb timesteps train : \', median_nb_timesteps)\n\nX_train = np.zeros((X_train_mat.shape[0], X_train_mat[0].shape[0], max_nb_timesteps))\n\n# pad ending with zeros to get numpy arrays\nfor i in range(X_train_mat.shape[0]):\n    var_count = X_train_mat[i].shape[-1]\n    #print(i, X_train_mat[i])\n    X_train[i, :, :var_count] = X_train_mat[i]\n\n# \'\'\' Load test set \'\'\'\n\nX_test = np.zeros((X_test_mat.shape[0], X_test_mat[0].shape[0], max_nb_timesteps))\n\n# pad ending with zeros to get numpy arrays\nfor i in range(X_test_mat.shape[0]):\n    var_count = X_test_mat[i].shape[-1]\n    X_test[i, :, :var_count] = X_test_mat[i][:, :max_nb_timesteps]\n\n\n# \'\'\' Save the datasets \'\'\'\nprint(""Train dataset : "", X_train.shape, y_train.shape)\nprint(""Test dataset : "", X_test.shape, y_test.shape)\nprint(""Train dataset metrics : "", X_train.mean(), X_train.std())\nprint(""Test dataset : "", X_test.mean(), X_test.std())\nprint(""Nb classes : "", len(np.unique(y_train)))\n\nnp.save(har_dataset + \'X_train.npy\', X_train)\nnp.save(har_dataset + \'y_train.npy\', y_train)\nnp.save(har_dataset + \'X_test.npy\', X_test)\nnp.save(har_dataset + \'y_test.npy\', y_test)\n'"
data/HT_Sensor/generate_ht_sensor_dataset.py,0,"b'import numpy as np\nnp.random.seed(1000)\n\nfrom scipy.io import loadmat\n\nht_sensor_path = r""""\n\n\'\'\' Load train set \'\'\'\ndata_dict = loadmat(ht_sensor_path + ""HT_Sensor_dataset.mat"")\nX_train_mat = data_dict[\'X_train\'][0]\ny_train_mat = data_dict[\'Y_train\'][0]\nX_test_mat = data_dict[\'X_test\'][0]\ny_test_mat = data_dict[\'Y_test\'][0]\n\nprint(X_train_mat.shape, X_test_mat.shape)\n\ny_train = y_train_mat.reshape(-1, 1)\ny_test = y_test_mat.reshape(-1, 1)\n\nvar_list = []\nfor i in range(X_train_mat.shape[0]):\n    var_count = X_train_mat[i].shape[-1]\n    var_list.append(var_count)\n\nvar_list = np.array(var_list)\nmax_nb_timesteps = var_list.max()\nmin_nb_timesteps = var_list.min()\nmedian_nb_timesteps = int(np.median(var_list))\n\nprint(\'max nb timesteps train : \', max_nb_timesteps)\nprint(\'min nb timesteps train : \', min_nb_timesteps)\nprint(\'median nb timesteps train : \', median_nb_timesteps)\n\n\'\'\' \nWe use the min_nb_timesteps in this dataset as our number of variables,\nsince the maximum is nearly 15000 timesteps and cannot be trained due to\nGPU memory constraints.\n\nHowever, comparison of models on this dataset should be against either\nfull (max_nb_timesteps) or limited (min_nb_timesteps). \n\nBaseline provided for limited (min_nb_timesteps) data only.\n\'\'\'\nX_train = np.zeros((X_train_mat.shape[0], X_train_mat[0].shape[0], min_nb_timesteps))\n\n# pad ending with zeros to get numpy arrays\nfor i in range(X_train_mat.shape[0]):\n    var_count = X_train_mat[i].shape[-1]\n    var_count = min(var_count, min_nb_timesteps)\n    X_train[i, :, :var_count] = X_train_mat[i][:, :min_nb_timesteps]\n\n# \'\'\' Load test set \'\'\'\n\nX_test = np.zeros((X_test_mat.shape[0], X_test_mat[0].shape[0], min_nb_timesteps))\n\n# pad ending with zeros to get numpy arrays\nfor i in range(X_test_mat.shape[0]):\n    var_count = X_test_mat[i].shape[-1]\n\n    # skip empty tuples\n    if var_count == 0:\n        continue\n\n    var_count = min(var_count, min_nb_timesteps)\n    X_test[i, :, :var_count] = X_test_mat[i][:, :min_nb_timesteps]\n\n\n# \'\'\' Save the datasets \'\'\'\nprint(""Train dataset : "", X_train.shape, y_train.shape)\nprint(""Test dataset : "", X_test.shape, y_test.shape)\nprint(""Train dataset metrics : "", X_train.mean(), X_train.std())\nprint(""Test dataset : "", X_test.mean(), X_test.std())\nprint(""Nb classes : "", len(np.unique(y_train)))\n\nnp.save(ht_sensor_path + \'X_train.npy\', X_train)\nnp.save(ht_sensor_path + \'y_train.npy\', y_train)\nnp.save(ht_sensor_path + \'X_test.npy\', X_test)\nnp.save(ht_sensor_path + \'y_test.npy\', y_test)\n'"
data/JapaneseVowels/generate_japanese_vowels_dataset.py,0,"b'import numpy as np\nfrom sklearn.model_selection import KFold\n\njapanese_vowels_path = \'\'\ntrain_data_path = japanese_vowels_path + \'ae.train\'\ntest_data_path = japanese_vowels_path + \'ae.test\'\ntrain_speaker_rows_path = japanese_vowels_path + \'size_ae.train\'\ntest_speaker_rows_path = japanese_vowels_path + \'size_ae.test\'\n\n\'\'\' TRAIN \'\'\'\n\ntrain_arrays = []\nwith open(train_data_path, \'r\') as f:\n    array_buffer = []\n\n    for line in f:\n        if line == \'\\n\':\n            train_arrays.append(array_buffer)\n            array_buffer = []\n        else:\n            array = np.array(line.split(\' \')[:-1], dtype=\'float32\')\n            array_buffer.append(array)\n\ntrain_array_lengths = []\nfor i in range(len(train_arrays)):\n    train_array_lengths.append(len(train_arrays[i]))\n\ntrain_array_lengths = np.asarray(train_array_lengths)\nmax_train_length = train_array_lengths.max()\nnb_variables = len(train_arrays[0][0])\n\nmax_nb_timesteps = train_array_lengths.max()\nmin_nb_timesteps = train_array_lengths.min()\nmedian_nb_timesteps = np.median(train_array_lengths)\n\nprint(\'max nb timesteps train : \', max_nb_timesteps)\nprint(\'min nb timesteps train : \', min_nb_timesteps)\nprint(\'median_nb_timesteps nb timesteps train : \', median_nb_timesteps)\n\nX_train = np.zeros((len(train_arrays), nb_variables, max_train_length))\ny_train = np.zeros((len(train_arrays),))\n\nfor i in range(len(train_arrays)):\n    x = np.asarray(train_arrays[i])\n    x = x.transpose((1, 0))\n    max_len = x.shape[-1]\n    X_train[i, :, :max_len] = x\n\nwith open(train_speaker_rows_path, \'r\') as f:\n    lengths = f.readline().split(\' \')\n    try:\n        test = int(lengths[-1])\n    except ValueError:\n        lengths = lengths[:-1]\n    lengths = np.array(lengths, dtype=\'int\')\n\n    index = 0\n    label = 0\n    for length in lengths:\n        for _ in range(length):\n            y_train[index] = label\n            index += 1\n        label += 1\n\n\'\'\' TEST \'\'\'\n\ntest_arrays = []\nwith open(test_data_path, \'r\') as f:\n    array_buffer = []\n\n    for line in f:\n        if line == \'\\n\':\n            test_arrays.append(array_buffer)\n            array_buffer = []\n        else:\n            array = np.array(line.split(\' \')[:-1], dtype=\'float32\')\n            array_buffer.append(array)\n\nX_test = np.zeros((len(test_arrays), nb_variables, max_train_length))\ny_test = np.zeros((len(test_arrays),))\n\nfor i in range(len(test_arrays)):\n    x = np.asarray(test_arrays[i])\n    x = x.transpose((1, 0))\n    max_len = x.shape[-1]\n    X_test[i, :, :max_len] = x[:, :max_train_length]\n\nwith open(test_speaker_rows_path, \'r\') as f:\n    lengths = f.readline().split(\' \')\n    try:\n        test = int(lengths[-1])\n    except ValueError:\n        lengths = lengths[:-1]\n    lengths = np.array(lengths, dtype=\'int\')\n\n    index = 0\n    label = 0\n    for length in lengths:\n        for _ in range(length):\n            y_test[index] = label\n            index += 1\n        label += 1\n\n\'\'\' Save datasets \'\'\'\nprint(""Train dataset : "", X_train.shape, y_train.shape)\nprint(""Test dataset : "", X_test.shape, y_test.shape)\nprint(""Train dataset metrics : "", X_train.mean(), X_train.std())\nprint(""Test dataset : "", X_test.mean(), X_test.std())\nprint(""Nb classes : "", len(np.unique(y_train)))\n\n#print(""\\nPerforming 10 fold crossvalidation split now"")\n#kf = KFold(n_splits=10, shuffle=False, random_state=1000)\n\nnp.save(japanese_vowels_path + \'X_train_mat.npy\', X_train)\nnp.save(japanese_vowels_path + \'y_train_mat.npy\', y_train)\nnp.save(japanese_vowels_path + \'X_test_mat.npy\', X_test) # full test dataset\nnp.save(japanese_vowels_path + \'y_test_mat.npy\', y_test) # full test dataset\n\nnp.save(japanese_vowels_path + \'X_train.npy\', X_train)\nnp.save(japanese_vowels_path + \'y_train.npy\', y_train)\nnp.save(japanese_vowels_path + \'X_test.npy\', X_test) # full test dataset\nnp.save(japanese_vowels_path + \'y_test.npy\', y_test) # full test dataset'"
data/MovementAAL/generate_movement_aal_dataset.py,0,"b'import numpy as np\nnp.random.seed(1000)\n\nfrom scipy.io import loadmat\n\nmovement_aal = r""""\n\n\'\'\' Load train set \'\'\'\ndata_dict = loadmat(movement_aal + ""MovementAAL.mat"")\nX_train_mat = data_dict[\'X_train\'][0]\ny_train_mat = data_dict[\'Y_train\'][0]\nX_test_mat = data_dict[\'X_test\'][0]\ny_test_mat = data_dict[\'Y_test\'][0]\n\ny_train = y_train_mat.reshape(-1, 1)\ny_test = y_test_mat.reshape(-1, 1)\n\nvar_list = []\nfor i in range(X_train_mat.shape[0]):\n    var_count = X_train_mat[i].shape[-1]\n    var_list.append(var_count)\n\nvar_list = np.array(var_list)\nmax_nb_timesteps = var_list.max()\nmin_nb_timesteps = var_list.min()\nmedian_nb_timesteps = np.median(var_list)\n\nprint(\'max nb timesteps train : \', max_nb_timesteps)\nprint(\'min nb timesteps train : \', min_nb_timesteps)\nprint(\'median_nb_timesteps nb timesteps train : \', median_nb_timesteps)\n\nX_train = np.zeros((X_train_mat.shape[0], X_train_mat[0].shape[0], max_nb_timesteps))\n\n# pad ending with zeros to get numpy arrays\nfor i in range(X_train_mat.shape[0]):\n    var_count = X_train_mat[i].shape[-1]\n    #print(i, X_train_mat[i])\n    X_train[i, :, :var_count] = X_train_mat[i]\n\n# \'\'\' Load test set \'\'\'\n\nX_test = np.zeros((X_test_mat.shape[0], X_test_mat[0].shape[0], max_nb_timesteps))\n\n# pad ending with zeros to get numpy arrays\nfor i in range(X_test_mat.shape[0]):\n    var_count = X_test_mat[i].shape[-1]\n    X_test[i, :, :var_count] = X_test_mat[i][:, :max_nb_timesteps]\n\n# Normalize the data\n# train_mean = X_train.mean()\n# train_std = X_train.std()\n#\n# X_train = (X_train - train_mean) / (train_std + 1e-8)\n# X_test = (X_test - train_mean) / (train_std + 1e-8)\n\n# \'\'\' Save the datasets \'\'\'\nprint(""Train dataset : "", X_train.shape, y_train.shape)\nprint(""Test dataset : "", X_test.shape, y_test.shape)\nprint(""Train dataset metrics : "", X_train.mean(), X_train.std())\nprint(""Test dataset : "", X_test.mean(), X_test.std())\nprint(""Nb classes : "", len(np.unique(y_train)))\n\nnp.save(movement_aal + \'X_train.npy\', X_train)\nnp.save(movement_aal + \'y_train.npy\', y_train)\nnp.save(movement_aal + \'X_test.npy\', X_test)\nnp.save(movement_aal + \'y_test.npy\', y_test)\n'"
data/arabic/generate_arabic_dataset.py,0,"b'import numpy as np\nfrom scipy.io import loadmat\n\narabic_path = r""""\n\n\'\'\' Load train set \'\'\'\narabic_training_dict = loadmat(arabic_path + ""training_set_arabic.mat"")\nX = arabic_training_dict[\'training_set\']\ny = arabic_training_dict[\'train_labels\']\n\nX = X[0]\n\nvar_list = []\nfor i in range(X.shape[0]):\n    var_count = X[i].shape[-1]\n    var_list.append(var_count)\n\nvar_list = np.array(var_list)\nmax_nb_timesteps = var_list.max()\nmin_nb_timesteps = var_list.min()\nmedian_nb_timesteps = np.median(var_list)\n\nprint(\'max nb timesteps train : \', max_nb_timesteps)\nprint(\'min nb timesteps train : \', min_nb_timesteps)\nprint(\'median_nb_timesteps nb timesteps train : \', median_nb_timesteps)\n\n\nX_train = np.zeros((X.shape[0], X[0].shape[0], max_nb_timesteps))\ny_train = y[0]\n\n# pad ending with zeros to get numpy arrays\nfor i in range(X_train.shape[0]):\n    var_count = X[i].shape[-1]\n    X_train[i, :, :var_count] = X[i]\n\n\'\'\' Load test set \'\'\'\narabic_test_dict = loadmat(arabic_path + ""test_set_arabic.mat"")\nX = arabic_test_dict[\'test_set\']\ny = arabic_test_dict[\'test_labels\']\n\nX = X[0]\ny = y[0]\n\nX_test = np.zeros((X.shape[0], X[0].shape[0], max_nb_timesteps))\ny_test = y\n\n# pad ending with zeros to get numpy arrays\nfor i in range(X_test.shape[0]):\n    var_count = X[i].shape[-1]\n    X_test[i, :, :var_count] = X[i]\n\n\'\'\' Save the datasets \'\'\'\nprint(""Train dataset : "", X_train.shape, y_train.shape)\nprint(""Test dataset : "", X_test.shape, y_test.shape)\nprint(""Train dataset metrics : "", X_train.mean(), X_train.std())\nprint(""Test dataset : "", X_test.mean(), X_test.std())\nprint(""Nb classes : "", len(np.unique(y_train)))\n\nnp.save(arabic_path + \'X_train.npy\', X_train)\nnp.save(arabic_path + \'y_train.npy\', y_train)\nnp.save(arabic_path + \'X_test.npy\', X_test)\nnp.save(arabic_path + \'y_test.npy\', y_test)'"
data/arabic_voice/generate_arabic_voice_dataset.py,0,"b'import numpy as np\nfrom scipy.io import loadmat\n\narabic_path = r""""\n\n\'\'\' Load train set \'\'\'\nDATA = loadmat(arabic_path + ""arabic_voice_window_3_ifperm_1.mat"")\n\nX_data = DATA[\'new_X\'][0]\ny_data = DATA[\'new_labels\'][0]\n\nlabels_n = 88\neach_label_number = 100\n\npre = 0\ntrain_ind = []\ntest_ind = []\ntraining_size = 75\ntest_size = 25\n\nfor i in range (1,(labels_n+1)):\n    ti = list(range(pre+1,pre+training_size+1))\n    train_ind.append(ti)\n    test_ind.append(list(range(pre+training_size+1,pre+training_size+test_size+1)))\n    pre = pre + each_label_number\n\n\ntrainind = [item for sublist in train_ind for item in sublist]\ntestind = [item for sublist in test_ind for item in sublist]\n\n\'\'\' Load train set \'\'\'\nX = X_data[np.array(trainind)-1] ###Train data\n\nvar_list = []\nfor i in range(X.shape[0]):\n    var_count = X[i].shape[-1]\n    var_list.append(var_count)\n\nvar_list = np.array(var_list)\nmax_nb_timesteps = var_list.max()\nmin_nb_timesteps = var_list.min()\nmedian_nb_timesteps = np.median(var_list)\n\nprint(\'max nb timesteps train : \', max_nb_timesteps)\nprint(\'min nb timesteps train : \', min_nb_timesteps)\nprint(\'median_nb_timesteps nb timesteps train : \', median_nb_timesteps)\n\n\nX_train = np.zeros((X.shape[0], X[0].shape[0], max_nb_timesteps))\ny_train = y_data[(np.array(trainind)-1)] ###Train Labels\n\n# pad ending with zeros to get numpy arrays\nfor i in range(X_train.shape[0]):\n    var_count = X[i].shape[-1]\n    X_train[i, :, :var_count] = X[i]\n\nX_train_mean = X_train.mean()\nX_train_std = X_train.std()\nprint(""Train Mean +- std : "", X_train_mean, X_train_std)\n#X_train_mat = (X_train_mat - X_train_mean) / (X_train_std + 1e-8)\n\n\'\'\' Load test set \'\'\'\n\nX = X_data[(np.array(testind)-1)] ####Test Data\n\nX_test = np.zeros((X.shape[0], X[0].shape[0], max_nb_timesteps))\ny_test = y_data[(np.array(testind)-1)] ####Test Labels\xe2\x80\x8b\n\n# pad ending with zeros to get numpy arrays\nfor i in range(X_test.shape[0]):\n    var_count = X[i].shape[-1]\n    X_test[i, :, :var_count] = X[i]\n\n#X_test_mat = (X_test_mat - X_train_mean) / (X_train_std + 1e-8)\n\n\'\'\' Save the datasets \'\'\'\nprint(""Train dataset : "", X_train.shape, y_train.shape)\nprint(""Test dataset : "", X_test.shape, y_test.shape)\nprint(""Train dataset metrics : "", X_train.mean(), X_train.std())\nprint(""Test dataset : "", X_test.mean(), X_test.std())\nprint(""Nb classes : "", len(np.unique(y_train)))\n\nnp.save(arabic_path + \'X_train_mat.npy\', X_train)\nnp.save(arabic_path + \'y_train_mat.npy\', y_train)\nnp.save(arabic_path + \'X_test_mat.npy\', X_test)\nnp.save(arabic_path + \'y_test_mat.npy\', y_test)\n\n\n'"
data/character/generate_character_dataset.py,0,"b'import scipy.io as sio\n\nimport numpy as np\n\nck_path = r\'\'\nDATA = sio.loadmat(ck_path + \'randomperm_character_overlap.mat\')\n\nvar_list = None\nno_folds = 10\n\nfor k in range(10):\n    index_fold = k + 1 # index fold\n    pre = 0\n\n    train_ind = []\n    test_ind = []\n    each_label_number = DATA[\'each_label_number\'][0]\n    labels_n = DATA[\'labels_n\'][0]\n\n    for i in range(0, labels_n[0]):\n        fold_size = np.floor(each_label_number[i] / no_folds)\n        ind = (index_fold - 1) * fold_size + 1\n        train_ind.append(list(range(pre + 1, int(pre + ind - 1 + 1))))\n        train_ind.append(list(range(int(pre + ind + fold_size), int(pre + 1 + each_label_number[i]))))\n        test_ind.append(list(range(int(pre + ind), int(pre + ind + fold_size - 1 + 1))))\n        pre = pre + each_label_number[i]\n\n    trainind = [item for sublist in train_ind for item in sublist]\n    testind = [item for sublist in test_ind for item in sublist]\n\n    X = DATA[\'new_X\'][0, (np.array(trainind) - 1)]  ###Train data\n    y = DATA[\'new_labels\'][0, (np.array(trainind) - 1)]  ###Train Labels\n\n    \'\'\' Load train Dataset \'\'\'\n    if var_list is None:\n        var_list = []\n        for i in range(X.shape[0]):\n            var_count = X[i].shape[-1]\n            var_list.append(var_count)\n\n        var_list = np.array(var_list)\n        max_nb_timesteps = var_list.max()\n        min_nb_timesteps = var_list.min()\n        median_nb_timesteps = np.median(var_list)\n\n        print(\'max nb timesteps train : \', max_nb_timesteps)\n        print(\'min nb timesteps train : \', min_nb_timesteps)\n        print(\'median_nb_timesteps nb timesteps train : \', median_nb_timesteps)\n\n        print(\'-\' * 80)\n        print()\n\n    X_train = np.zeros((X.shape[0], X[0].shape[0], max_nb_timesteps))\n    y_train = y\n\n    # pad ending with zeros to get numpy arrays\n    for i in range(X_train.shape[0]):\n        var_count = X[i].shape[-1]\n        X_train[i, :, :var_count] = X[i]\n\n    \'\'\' Load test dataset\'\'\'\n    X = DATA[\'new_X\'][0, (np.array(testind) - 1)]  ####Test Data\n    y = DATA[\'new_labels\'][0, (np.array(testind) - 1)]  ####Test Labels\n\n    X_test = np.zeros((X.shape[0], X[0].shape[0], max_nb_timesteps))\n    y_test = y\n\n    # pad ending with zeros to get numpy arrays\n    for i in range(X_test.shape[0]):\n        var_count = X[i].shape[-1]\n        X_test[i, :, :var_count] = X[i]\n\n    \'\'\' Save the datasets \'\'\'\n    print(""Dataset Fold #%d"" % index_fold)\n    print(""Train dataset : "", X_train.shape, y_train.shape)\n    print(""Test dataset : "", X_test.shape, y_test.shape)\n    print(""Train dataset metrics : "", X_train.mean(), X_train.std())\n    print(""Test dataset : "", X_test.mean(), X_test.std())\n    print(""Nb classes : "", len(np.unique(y_train)), ""Classes : "", np.unique(y_train))\n\n    np.save(ck_path + \'X_train_%d.npy\' % index_fold, X_train)\n    np.save(ck_path + \'y_train_%d.npy\' % index_fold, y_train)\n    np.save(ck_path + \'X_test_%d.npy\' % index_fold, X_test)\n    np.save(ck_path + \'y_test_%d.npy\' % index_fold, y_test)\n\n    print()\n'"
data/daily_sport/generate_daily_sport_dataset.py,0,"b'import numpy as np\nnp.random.seed(1000)\n\nfrom scipy.io import loadmat\n\ndaily_sport = r""""\n\n\'\'\' Load train set \'\'\'\ndata_dict = loadmat(daily_sport + ""daily_sport.mat"")\nX_train_mat = data_dict[\'X_train\'][0]\ny_train_mat = data_dict[\'Y_train\'][0]\nX_test_mat = data_dict[\'X_test\'][0]\ny_test_mat = data_dict[\'Y_test\'][0]\n\ny_train = y_train_mat.reshape(-1, 1)\ny_test = y_test_mat.reshape(-1, 1)\n\nvar_list = []\nfor i in range(X_train_mat.shape[0]):\n    var_count = X_train_mat[i].shape[-1]\n    var_list.append(var_count)\n\nvar_list = np.array(var_list)\nmax_nb_timesteps = var_list.max()\nmin_nb_timesteps = var_list.min()\nmedian_nb_timesteps = np.median(var_list)\n\nprint(\'max nb timesteps train : \', max_nb_timesteps)\nprint(\'min nb timesteps train : \', min_nb_timesteps)\nprint(\'median_nb_timesteps nb timesteps train : \', median_nb_timesteps)\n\nX_train = np.zeros((X_train_mat.shape[0], X_train_mat[0].shape[0], max_nb_timesteps))\n\n# pad ending with zeros to get numpy arrays\nfor i in range(X_train_mat.shape[0]):\n    var_count = X_train_mat[i].shape[-1]\n    X_train[i, :, :var_count] = X_train_mat[i]\n\n# \'\'\' Load test set \'\'\'\n\nX_test = np.zeros((X_test_mat.shape[0], X_test_mat[0].shape[0], max_nb_timesteps))\n\n# pad ending with zeros to get numpy arrays\nfor i in range(X_test_mat.shape[0]):\n    var_count = X_test_mat[i].shape[-1]\n    X_test[i, :, :var_count] = X_test_mat[i][:, :max_nb_timesteps]\n\n# \'\'\' Save the datasets \'\'\'\nprint(""Train dataset : "", X_train.shape, y_train.shape)\nprint(""Test dataset : "", X_test.shape, y_test.shape)\nprint(""Train dataset metrics : "", X_train.mean(), X_train.std())\nprint(""Test dataset : "", X_test.mean(), X_test.std())\nprint(""Nb classes : "", len(np.unique(y_train)))\n\nnp.save(daily_sport + \'X_train.npy\', X_train)\nnp.save(daily_sport + \'y_train.npy\', y_train)\nnp.save(daily_sport + \'X_test.npy\', X_test)\nnp.save(daily_sport + \'y_test.npy\', y_test)\n'"
data/eeg/generate_eeg_dataset.py,0,"b'import numpy as np\nnp.random.seed(1000)\n\nfrom scipy.io import loadmat\n\neeg = r""""\n\n\'\'\' Load train set \'\'\'\ndata_dict = loadmat(eeg + ""eeg.mat"")\nX_train_mat = data_dict[\'X_train\'][0]\ny_train_mat = data_dict[\'Y_train\'][0]\nX_test_mat = data_dict[\'X_test\'][0]\ny_test_mat = data_dict[\'Y_test\'][0]\n\ny_train = y_train_mat.reshape(-1, 1)\ny_test = y_test_mat.reshape(-1, 1)\n\nvar_list = []\nfor i in range(X_train_mat.shape[0]):\n    var_count = X_train_mat[i].shape[-1]\n    var_list.append(var_count)\n\nvar_list = np.array(var_list)\nmax_nb_timesteps = var_list.max()\nmin_nb_timesteps = var_list.min()\nmedian_nb_timesteps = np.median(var_list)\n\nprint(\'max nb timesteps train : \', max_nb_timesteps)\nprint(\'min nb timesteps train : \', min_nb_timesteps)\nprint(\'median_nb_timesteps nb timesteps train : \', median_nb_timesteps)\n\nX_train = np.zeros((X_train_mat.shape[0], X_train_mat[0].shape[0], max_nb_timesteps))\n\n# pad ending with zeros to get numpy arrays\nfor i in range(X_train_mat.shape[0]):\n    var_count = X_train_mat[i].shape[-1]\n    #print(i, X_train_mat[i])\n    X_train[i, :, :var_count] = X_train_mat[i]\n\n# \'\'\' Load test set \'\'\'\n\nX_test = np.zeros((X_test_mat.shape[0], X_test_mat[0].shape[0], max_nb_timesteps))\n\n# pad ending with zeros to get numpy arrays\nfor i in range(X_test_mat.shape[0]):\n    var_count = X_test_mat[i].shape[-1]\n    X_test[i, :, :var_count] = X_test_mat[i][:, :max_nb_timesteps]\n\n\n# \'\'\' Save the datasets \'\'\'\nprint(""Train dataset : "", X_train.shape, y_train.shape)\nprint(""Test dataset : "", X_test.shape, y_test.shape)\nprint(""Train dataset metrics : "", X_train.mean(), X_train.std())\nprint(""Test dataset : "", X_test.mean(), X_test.std())\nprint(""Nb classes : "", len(np.unique(y_train)))\n\nnp.save(eeg + \'X_train.npy\', X_train)\nnp.save(eeg + \'y_train.npy\', y_train)\nnp.save(eeg + \'X_test.npy\', X_test)\nnp.save(eeg + \'y_test.npy\', y_test)\n'"
data/gesture_phase/generate_gesture_phase_dataset.py,0,"b'import numpy as np\nnp.random.seed(1000)\n\nfrom scipy.io import loadmat\n\ngesture_phase = r""""\n\n\'\'\' Load train set \'\'\'\ndata_dict = loadmat(gesture_phase + ""gesture_phase_dataset.mat"")\nX_train_mat = data_dict[\'X_train\'][0]\ny_train_mat = data_dict[\'Y_train\'][0]\nX_test_mat = data_dict[\'X_test\'][0]\ny_test_mat = data_dict[\'Y_test\'][0]\n\ny_train = y_train_mat.reshape(-1, 1)\ny_test = y_test_mat.reshape(-1, 1)\n\nvar_list = []\nfor i in range(X_train_mat.shape[0]):\n    var_count = X_train_mat[i].shape[-1]\n    var_list.append(var_count)\n\nvar_list = np.array(var_list)\nmax_nb_timesteps = var_list.max()\nmin_nb_timesteps = var_list.min()\nmedian_nb_timesteps = np.median(var_list)\n\nprint(\'max nb timesteps train : \', max_nb_timesteps)\nprint(\'min nb timesteps train : \', min_nb_timesteps)\nprint(\'median_nb_timesteps nb timesteps train : \', median_nb_timesteps)\n\nX_train = np.zeros((X_train_mat.shape[0], X_train_mat[0].shape[0], max_nb_timesteps))\n\n# pad ending with zeros to get numpy arrays\nfor i in range(X_train_mat.shape[0]):\n    var_count = X_train_mat[i].shape[-1]\n    #print(i, X_train_mat[i])\n    X_train[i, :, :var_count] = X_train_mat[i]\n\n# \'\'\' Load test set \'\'\'\n\nX_test = np.zeros((X_test_mat.shape[0], X_test_mat[0].shape[0], max_nb_timesteps))\n\n# pad ending with zeros to get numpy arrays\nfor i in range(X_test_mat.shape[0]):\n    var_count = X_test_mat[i].shape[-1]\n    X_test[i, :, :var_count] = X_test_mat[i][:, :max_nb_timesteps]\n\n# Normalize the data\ntrain_mean = X_train.mean()\ntrain_std = X_train.std()\n\nX_train = (X_train - train_mean) / (train_std + 1e-8)\nX_test = (X_test - train_mean) / (train_std + 1e-8)\n\n# \'\'\' Save the datasets \'\'\'\nprint(""Train dataset : "", X_train.shape, y_train.shape)\nprint(""Test dataset : "", X_test.shape, y_test.shape)\nprint(""Train dataset metrics : "", X_train.mean(), X_train.std())\nprint(""Test dataset : "", X_test.mean(), X_test.std())\nprint(""Nb classes : "", len(np.unique(y_train)))\n\nnp.save(gesture_phase + \'X_train.npy\', X_train)\nnp.save(gesture_phase + \'y_train.npy\', y_train)\nnp.save(gesture_phase + \'X_test.npy\', X_test)\nnp.save(gesture_phase + \'y_test.npy\', y_test)\n'"
data/occupancy_detect/generate_occupancy_detect_dataset.py,0,"b'import numpy as np\nnp.random.seed(1000)\n\nfrom scipy.io import loadmat\n\noccupancy_detect = r""""\n\n\'\'\' Load train set \'\'\'\ndata_dict = loadmat(occupancy_detect + ""occupancy_detect.mat"")\nX_train_mat = data_dict[\'X_train\'][0]\ny_train_mat = data_dict[\'Y_train\'][0]\nX_test_mat = data_dict[\'X_test\'][0]\ny_test_mat = data_dict[\'Y_test\'][0]\n\ny_train = y_train_mat.reshape(-1, 1)\ny_test = y_test_mat.reshape(-1, 1)\n\nvar_list = []\nfor i in range(X_train_mat.shape[0]):\n    var_count = X_train_mat[i].shape[-1]\n    var_list.append(var_count)\n\nvar_list = np.array(var_list)\nmax_nb_timesteps = var_list.max()\nmin_nb_timesteps = var_list.min()\nmedian_nb_timesteps = np.median(var_list)\n\nprint(\'max nb timesteps train : \', max_nb_timesteps)\nprint(\'min nb timesteps train : \', min_nb_timesteps)\nprint(\'median_nb_timesteps nb timesteps train : \', median_nb_timesteps)\n\nX_train = np.zeros((X_train_mat.shape[0], X_train_mat[0].shape[0], max_nb_timesteps))\n\n# pad ending with zeros to get numpy arrays\nfor i in range(X_train_mat.shape[0]):\n    var_count = X_train_mat[i].shape[-1]\n    #print(i, X_train_mat[i])\n    X_train[i, :, :var_count] = X_train_mat[i]\n\n# \'\'\' Load test set \'\'\'\n\nX_test = np.zeros((X_test_mat.shape[0], X_test_mat[0].shape[0], max_nb_timesteps))\n\n# pad ending with zeros to get numpy arrays\nfor i in range(X_test_mat.shape[0]):\n    var_count = X_test_mat[i].shape[-1]\n    X_test[i, :, :var_count] = X_test_mat[i][:, :max_nb_timesteps]\n\n\n# \'\'\' Save the datasets \'\'\'\nprint(""Train dataset : "", X_train.shape, y_train.shape)\nprint(""Test dataset : "", X_test.shape, y_test.shape)\nprint(""Train dataset metrics : "", X_train.mean(), X_train.std())\nprint(""Test dataset : "", X_test.mean(), X_test.std())\nprint(""Nb classes : "", len(np.unique(y_train)))\n\nnp.save(occupancy_detect + \'X_train.npy\', X_train)\nnp.save(occupancy_detect + \'y_train.npy\', y_train)\nnp.save(occupancy_detect + \'X_test.npy\', X_test)\nnp.save(occupancy_detect + \'y_test.npy\', y_test)\n'"
data/ozone/generate_ozone_dataset.py,0,"b'import numpy as np\nnp.random.seed(1000)\n\nfrom scipy.io import loadmat\n\nozone = r""""\n\n\'\'\' Load train set \'\'\'\ndata_dict = loadmat(ozone + ""ozone.mat"")\nX_train_mat = data_dict[\'X_train\'][0]\ny_train_mat = data_dict[\'Y_train\'][0]\nX_test_mat = data_dict[\'X_test\'][0]\ny_test_mat = data_dict[\'Y_test\'][0]\n\ny_train = y_train_mat.reshape(-1, 1)\ny_test = y_test_mat.reshape(-1, 1)\n\nvar_list = []\nfor i in range(X_train_mat.shape[0]):\n    var_count = X_train_mat[i].shape[-1]\n    var_list.append(var_count)\n\nvar_list = np.array(var_list)\nmax_nb_timesteps = var_list.max()\nmin_nb_timesteps = var_list.min()\nmedian_nb_timesteps = np.median(var_list)\n\nprint(\'max nb timesteps train : \', max_nb_timesteps)\nprint(\'min nb timesteps train : \', min_nb_timesteps)\nprint(\'median_nb_timesteps nb timesteps train : \', median_nb_timesteps)\n\nX_train = np.zeros((X_train_mat.shape[0], X_train_mat[0].shape[0], max_nb_timesteps))\n\n# pad ending with zeros to get numpy arrays\nfor i in range(X_train_mat.shape[0]):\n    var_count = X_train_mat[i].shape[-1]\n    if var_count == 0:\n        continue\n    X_train[i, :, :var_count] = X_train_mat[i]\n\n# \'\'\' Load test set \'\'\'\n\nX_test = np.zeros((X_test_mat.shape[0], X_test_mat[0].shape[0], max_nb_timesteps))\n\n# pad ending with zeros to get numpy arrays\nfor i in range(X_test_mat.shape[0]):\n    var_count = X_test_mat[i].shape[-1]\n    X_test[i, :, :var_count] = X_test_mat[i][:, :max_nb_timesteps]\n\n\n# \'\'\' Save the datasets \'\'\'\nprint(""Train dataset : "", X_train.shape, y_train.shape)\nprint(""Test dataset : "", X_test.shape, y_test.shape)\nprint(""Train dataset metrics : "", X_train.mean(), X_train.std())\nprint(""Test dataset : "", X_test.mean(), X_test.std())\nprint(""Nb classes : "", len(np.unique(y_train)))\n\nnp.save(ozone + \'X_train.npy\', X_train)\nnp.save(ozone + \'y_train.npy\', y_train)\nnp.save(ozone + \'X_test.npy\', X_test)\nnp.save(ozone + \'y_test.npy\', y_test)\n'"
