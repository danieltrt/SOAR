file_path,api_count,code
controller.py,48,"b'import numpy as np\nimport time\nimport pprint\nfrom collections import OrderedDict\n\nfrom keras import backend as K\nimport tensorflow as tf\n\nimport os\nif not os.path.exists(\'weights/\'):\n    os.makedirs(\'weights/\')\n\n\nclass StateSpace:\n    \'\'\'\n    State Space manager\n\n    Provides utilit functions for holding ""states"" / ""actions"" that the controller\n    must use to train and predict.\n\n    Also provides a more convenient way to define the search space\n    \'\'\'\n    def __init__(self):\n        self.states = OrderedDict()\n        self.state_count_ = 0\n\n    def add_state(self, name, values):\n        \'\'\'\n        Adds a ""state"" to the state manager, along with some metadata for efficient\n        packing and unpacking of information required by the RNN Controller.\n\n        Stores metadata such as:\n        -   Global ID\n        -   Name\n        -   Valid Values\n        -   Number of valid values possible\n        -   Map from value ID to state value\n        -   Map from state value to value ID\n\n        Args:\n            name: name of the state / action\n            values: valid values that this state can take\n\n        Returns:\n            Global ID of the state. Can be used to refer to this state later.\n        \'\'\'\n        index_map = {}\n        for i, val in enumerate(values):\n            index_map[i] = val\n\n        value_map = {}\n        for i, val in enumerate(values):\n            value_map[val] = i\n\n        metadata = {\n            \'id\': self.state_count_,\n            \'name\': name,\n            \'values\': values,\n            \'size\': len(values),\n            \'index_map_\': index_map,\n            \'value_map_\': value_map,\n        }\n        self.states[self.state_count_] = metadata\n        self.state_count_ += 1\n\n        return self.state_count_ - 1\n\n    def embedding_encode(self, id, value):\n        \'\'\'\n        Embedding index encode the specific state value\n\n        Args:\n            id: global id of the state\n            value: state value\n\n        Returns:\n            embedding encoded representation of the state value\n        \'\'\'\n        state = self[id]\n        size = state[\'size\']\n        value_map = state[\'value_map_\']\n        value_idx = value_map[value]\n\n        one_hot = np.zeros((1, size), dtype=np.float32)\n        one_hot[np.arange(1), value_idx] = value_idx + 1\n        return one_hot\n\n    def get_state_value(self, id, index):\n        \'\'\'\n        Retrieves the state value from the state value ID\n\n        Args:\n            id: global id of the state\n            index: index of the state value (usually from argmax)\n\n        Returns:\n            The actual state value at given value index\n        \'\'\'\n        state = self[id]\n        index_map = state[\'index_map_\']\n\n        if (type(index) == list or type(index) == np.ndarray) and len(index) == 1:\n            index = index[0]\n\n        value = index_map[index]\n        return value\n\n    def get_random_state_space(self, num_layers):\n        \'\'\'\n        Constructs a random initial state space for feeding as an initial value\n        to the Controller RNN\n\n        Args:\n            num_layers: number of layers to duplicate the search space\n\n        Returns:\n            A list of one hot encoded states\n        \'\'\'\n        states = []\n\n        for id in range(self.size * num_layers):\n            state = self[id]\n            size = state[\'size\']\n\n            sample = np.random.choice(size, size=1)\n            sample = state[\'index_map_\'][sample[0]]\n            state = self.embedding_encode(id, sample)\n            states.append(state)\n        return states\n\n    def parse_state_space_list(self, state_list):\n        \'\'\'\n        Parses a list of one hot encoded states to retrieve a list of state values\n\n        Args:\n            state_list: list of one hot encoded states\n\n        Returns:\n            list of state values\n        \'\'\'\n        state_values = []\n        for id, state_one_hot in enumerate(state_list):\n            state_val_idx = np.argmax(state_one_hot, axis=-1)[0]\n            value = self.get_state_value(id, state_val_idx)\n            state_values.append(value)\n\n        return state_values\n\n    def print_state_space(self):\n        \'\'\' Pretty print the state space \'\'\'\n        print(\'*\' * 40, \'STATE SPACE\', \'*\' * 40)\n\n        pp = pprint.PrettyPrinter(indent=2, width=100)\n        for id, state in self.states.items():\n            pp.pprint(state)\n            print()\n\n    def print_actions(self, actions):\n        \'\'\' Print the action space properly \'\'\'\n        print(\'Actions :\')\n\n        for id, action in enumerate(actions):\n            if id % self.size == 0:\n                print(""*"" * 20, ""Layer %d"" % (((id + 1) // self.size) + 1), ""*"" * 20)\n\n            state = self[id]\n            name = state[\'name\']\n            vals = [(n, p) for n, p in zip(state[\'values\'], *action)]\n            print(""%s : "" % name, vals)\n        print()\n\n    def __getitem__(self, id):\n        return self.states[id % self.size]\n\n    @property\n    def size(self):\n        return self.state_count_\n\n\nclass Controller:\n    \'\'\'\n    Utility class to manage the RNN Controller\n    \'\'\'\n    def __init__(self, policy_session, num_layers, state_space,\n                 reg_param=0.001,\n                 discount_factor=0.99,\n                 exploration=0.8,\n                 controller_cells=32,\n                 embedding_dim=20,\n                 clip_norm=0.0,\n                 restore_controller=False):\n        self.policy_session = policy_session  # type: tf.Session\n\n        self.num_layers = num_layers\n        self.state_space = state_space  # type: StateSpace\n        self.state_size = self.state_space.size\n\n        self.controller_cells = controller_cells\n        self.embedding_dim = embedding_dim\n        self.reg_strength = reg_param\n        self.discount_factor = discount_factor\n        self.exploration = exploration\n        self.restore_controller = restore_controller\n        self.clip_norm = clip_norm\n\n        self.reward_buffer = []\n        self.state_buffer = []\n\n        self.cell_outputs = []\n        self.policy_classifiers = []\n        self.policy_actions = []\n        self.policy_labels = []\n\n        self.build_policy_network()\n\n    def get_action(self, state):\n        \'\'\'\n        Gets a one hot encoded action list, either from random sampling or from\n        the Controller RNN\n\n        Args:\n            state: a list of one hot encoded states, whose first value is used as initial\n                state for the controller RNN\n\n        Returns:\n            A one hot encoded action list\n        \'\'\'\n        if np.random.random() < self.exploration:\n            print(""Generating random action to explore"")\n            actions = []\n\n            for i in range(self.state_size * self.num_layers):\n                state_ = self.state_space[i]\n                size = state_[\'size\']\n\n                sample = np.random.choice(size, size=1)\n                sample = state_[\'index_map_\'][sample[0]]\n                action = self.state_space.embedding_encode(i, sample)\n                actions.append(action)\n            return actions\n\n        else:\n            print(""Prediction action from Controller"")\n            initial_state = self.state_space[0]\n            size = initial_state[\'size\']\n\n            if state[0].shape != (1, size):\n                state = state[0].reshape((1, size)).astype(\'int32\')\n            else:\n                state = state[0]\n\n            print(""State input to Controller for Action : "", state.flatten())\n\n            with self.policy_session.as_default():\n                K.set_session(self.policy_session)\n\n                with tf.name_scope(\'action_prediction\'):\n                    pred_actions = self.policy_session.run(self.policy_actions, feed_dict={self.state_input: state})\n\n                return pred_actions\n\n\n    def build_policy_network(self):\n        with self.policy_session.as_default():\n            K.set_session(self.policy_session)\n\n            with tf.name_scope(\'controller\'):\n                with tf.variable_scope(\'policy_network\'):\n\n                    # state input is the first input fed into the controller RNN.\n                    # the rest of the inputs are fed to the RNN internally\n                    with tf.name_scope(\'state_input\'):\n                        state_input = tf.placeholder(dtype=tf.int32, shape=(1, None), name=\'state_input\')\n\n                    self.state_input = state_input\n\n                    # we can use LSTM as the controller as well\n                    nas_cell = tf.nn.rnn_cell.LSTMCell(self.controller_cells)\n                    cell_state = nas_cell.zero_state(batch_size=1, dtype=tf.float32)\n\n                    embedding_weights = []\n\n                    # for each possible state, create a new embedding. Reuse the weights for multiple layers.\n                    with tf.variable_scope(\'embeddings\', reuse=tf.AUTO_REUSE):\n                        for i in range(self.state_size):\n                            state_ = self.state_space[i]\n                            size = state_[\'size\']\n\n                            # size + 1 is used so that 0th index is never updated and is ""default"" value\n                            weights = tf.get_variable(\'state_embeddings_%d\' % i,\n                                                      shape=[size + 1, self.embedding_dim],\n                                                      initializer=tf.initializers.random_uniform(-1., 1.))\n\n                            embedding_weights.append(weights)\n\n                        # initially, cell input will be 1st state input\n                        embeddings = tf.nn.embedding_lookup(embedding_weights[0], state_input)\n\n                    cell_input = embeddings\n\n                    # we provide a flat list of chained input-output to the RNN\n                    for i in range(self.state_size * self.num_layers):\n                        state_id = i % self.state_size\n                        state_space = self.state_space[i]\n                        size = state_space[\'size\']\n\n                        with tf.name_scope(\'controller_output_%d\' % i):\n                            # feed the ith layer input (i-1 layer output) to the RNN\n                            outputs, final_state = tf.nn.dynamic_rnn(nas_cell,\n                                                                     cell_input,\n                                                                     initial_state=cell_state,\n                                                                     dtype=tf.float32)\n\n                            # add a new classifier for each layers output\n                            classifier = tf.layers.dense(outputs[:, -1, :], units=size, name=\'classifier_%d\' % (i),\n                                                         reuse=False)\n                            preds = tf.nn.softmax(classifier)\n\n                            # feed the previous layer (i-1 layer output) to the next layers input, along with state\n                            # take the class label\n                            cell_input = tf.argmax(preds, axis=-1)\n                            cell_input = tf.expand_dims(cell_input, -1, name=\'pred_output_%d\' % (i))\n                            cell_input = tf.cast(cell_input, tf.int32)\n                            cell_input = tf.add(cell_input, 1)  # we avoid using 0 so as to have a ""default"" embedding at 0th index\n\n                            # embedding lookup of this state using its state weights ; reuse weights\n                            cell_input = tf.nn.embedding_lookup(embedding_weights[state_id], cell_input,\n                                                           name=\'cell_output_%d\' % (i))\n\n                            cell_state = final_state\n\n                        # store the tensors for later loss computation\n                        self.cell_outputs.append(cell_input)\n                        self.policy_classifiers.append(classifier)\n                        self.policy_actions.append(preds)\n\n            policy_net_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'policy_network\')\n\n            with tf.name_scope(\'optimizer\'):\n                self.global_step = tf.Variable(0, trainable=False)\n                starter_learning_rate = 0.1\n                learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step,\n                                                           500, 0.95, staircase=True)\n\n                tf.summary.scalar(\'learning_rate\', learning_rate)\n\n                self.optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n\n            with tf.name_scope(\'losses\'):\n                self.discounted_rewards = tf.placeholder(tf.float32, shape=(None,), name=\'discounted_rewards\')\n                tf.summary.scalar(\'discounted_reward\', tf.reduce_sum(self.discounted_rewards))\n\n                # calculate sum of all the individual classifiers\n                cross_entropy_loss = 0\n                for i in range(self.state_size * self.num_layers):\n                    classifier = self.policy_classifiers[i]\n                    state_space = self.state_space[i]\n                    size = state_space[\'size\']\n\n                    with tf.name_scope(\'state_%d\' % (i + 1)):\n                        labels = tf.placeholder(dtype=tf.float32, shape=(None, size), name=\'cell_label_%d\' % i)\n                        self.policy_labels.append(labels)\n\n                        ce_loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=classifier, labels=labels)\n                        tf.summary.scalar(\'state_%d_ce_loss\' % (i + 1), tf.reduce_mean(ce_loss))\n\n                    cross_entropy_loss += ce_loss\n\n                policy_gradient_loss = tf.reduce_mean(cross_entropy_loss)\n                reg_loss = tf.reduce_sum([tf.reduce_sum(tf.square(x)) for x in policy_net_variables])  # Regularization\n\n                # sum up policy gradient and regularization loss\n                self.total_loss = policy_gradient_loss + self.reg_strength * reg_loss\n                tf.summary.scalar(\'total_loss\', self.total_loss)\n\n                self.gradients = self.optimizer.compute_gradients(self.total_loss)\n\n                with tf.name_scope(\'policy_gradients\'):\n                    # normalize gradients so that they dont explode if argument passed\n                    if self.clip_norm is not None and self.clip_norm != 0.0:\n                        norm = tf.constant(self.clip_norm, dtype=tf.float32)\n                        gradients, vars = zip(*self.gradients)  # unpack the two lists of gradients and the variables\n                        gradients, _ = tf.clip_by_global_norm(gradients, norm)  # clip by the norm\n                        self.gradients = list(zip(gradients, vars))  # we need to set values later, convert to list\n\n                    # compute policy gradients\n                    for i, (grad, var) in enumerate(self.gradients):\n                        if grad is not None:\n                            self.gradients[i] = (grad * self.discounted_rewards, var)\n\n                # training update\n                with tf.name_scope(""train_policy_network""):\n                    # apply gradients to update policy network\n                    self.train_op = self.optimizer.apply_gradients(self.gradients, global_step=self.global_step)\n\n            self.summaries_op = tf.summary.merge_all()\n\n            timestr = time.strftime(""%Y-%m-%d-%H-%M-%S"")\n            filename = \'logs/%s\' % timestr\n\n            self.summary_writer = tf.summary.FileWriter(filename, graph=self.policy_session.graph)\n\n            self.policy_session.run(tf.global_variables_initializer())\n            self.saver = tf.train.Saver(max_to_keep=1)\n\n            if self.restore_controller:\n                path = tf.train.latest_checkpoint(\'weights/\')\n\n                if path is not None and tf.train.checkpoint_exists(path):\n                    print(""Loading Controller Checkpoint !"")\n                    self.saver.restore(self.policy_session, path)\n\n    def store_rollout(self, state, reward):\n        self.reward_buffer.append(reward)\n        self.state_buffer.append(state)\n\n        # dump buffers to file if it grows larger than 50 items\n        if len(self.reward_buffer) > 20:\n            with open(\'buffers.txt\', mode=\'a+\') as f:\n                for i in range(20):\n                    state_ = self.state_buffer[i]\n                    state_list = self.state_space.parse_state_space_list(state_)\n                    state_list = \',\'.join(str(v) for v in state_list)\n\n                    f.write(""%0.4f,%s\\n"" % (self.reward_buffer[i], state_list))\n\n                print(""Saved buffers to file `buffers.txt` !"")\n\n            self.reward_buffer = [self.reward_buffer[-1]]\n            self.state_buffer = [self.state_buffer[-1]]\n\n    def discount_rewards(self):\n        \'\'\'\n        Compute discounted rewards over the entire reward buffer\n\n        Returns:\n            Discounted reward value\n        \'\'\'\n        rewards = np.asarray(self.reward_buffer)\n        discounted_rewards = np.zeros_like(rewards)\n        running_add = 0\n        for t in reversed(range(0, rewards.size)):\n            if rewards[t] != 0:\n                running_add = 0\n            running_add = running_add * self.discount_factor + rewards[t]\n            discounted_rewards[t] = running_add\n        return discounted_rewards[-1]\n\n    def train_step(self):\n        \'\'\'\n        Perform a single train step on the Controller RNN\n\n        Returns:\n            the training loss\n        \'\'\'\n        states = self.state_buffer[-1]\n        label_list = []\n\n        # parse the state space to get real value of the states,\n        # then one hot encode them for comparison with the predictions\n        state_list = self.state_space.parse_state_space_list(states)\n        for id, state_value in enumerate(state_list):\n            state_one_hot = self.state_space.embedding_encode(id, state_value)\n            label_list.append(state_one_hot)\n\n        # the initial input to the controller RNN\n        state_input_size = self.state_space[0][\'size\']\n        state_input = states[0].reshape((1, state_input_size)).astype(\'int32\')\n        print(""State input to Controller for training : "", state_input.flatten())\n\n        # the discounted reward value\n        reward = self.discount_rewards()\n        reward = np.asarray([reward]).astype(\'float32\')\n\n        feed_dict = {\n            self.state_input: state_input,\n            self.discounted_rewards: reward\n        }\n\n        # prepare the feed dict with the values of all the policy labels for each\n        # of the Controller outputs\n        for i, label in enumerate(label_list):\n            feed_dict[self.policy_labels[i]] = label\n\n        with self.policy_session.as_default():\n            K.set_session(self.policy_session)\n\n            print(""Training RNN (States ip) : "", state_list)\n            print(""Training RNN (Reward ip) : "", reward.flatten())\n            _, loss, summary, global_step = self.policy_session.run([self.train_op, self.total_loss, self.summaries_op,\n                                                                     self.global_step],\n                                                                     feed_dict=feed_dict)\n\n            self.summary_writer.add_summary(summary, global_step)\n            self.saver.save(self.policy_session, save_path=\'weights/controller.ckpt\', global_step=self.global_step)\n\n            # reduce exploration after many train steps\n            if global_step != 0 and global_step % 20 == 0 and self.exploration > 0.5:\n                self.exploration *= 0.99\n\n        return loss\n\n    def remove_files(self):\n        files = [\'train_history.csv\', \'buffers.txt\']\n\n        for file in files:\n            if os.path.exists(file):\n                os.remove(file)\n'"
manager.py,1,"b'import numpy as np\n\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint\nimport tensorflow as tf\n\nclass NetworkManager:\n    \'\'\'\n    Helper class to manage the generation of subnetwork training given a dataset\n    \'\'\'\n    def __init__(self, dataset, epochs=5, child_batchsize=128, acc_beta=0.8, clip_rewards=0.0):\n        \'\'\'\n        Manager which is tasked with creating subnetworks, training them on a dataset, and retrieving\n        rewards in the term of accuracy, which is passed to the controller RNN.\n\n        Args:\n            dataset: a tuple of 4 arrays (X_train, y_train, X_val, y_val)\n            epochs: number of epochs to train the subnetworks\n            child_batchsize: batchsize of training the subnetworks\n            acc_beta: exponential weight for the accuracy\n            clip_rewards: float - to clip rewards in [-range, range] to prevent\n                large weight updates. Use when training is highly unstable.\n        \'\'\'\n        self.dataset = dataset\n        self.epochs = epochs\n        self.batchsize = child_batchsize\n        self.clip_rewards = clip_rewards\n\n        self.beta = acc_beta\n        self.beta_bias = acc_beta\n        self.moving_acc = 0.0\n\n    def get_rewards(self, model_fn, actions):\n        \'\'\'\n        Creates a subnetwork given the actions predicted by the controller RNN,\n        trains it on the provided dataset, and then returns a reward.\n\n        Args:\n            model_fn: a function which accepts one argument, a list of\n                parsed actions, obtained via an inverse mapping from the\n                StateSpace.\n            actions: a list of parsed actions obtained via an inverse mapping\n                from the StateSpace. It is in a specific order as given below:\n\n                Consider 4 states were added to the StateSpace via the `add_state`\n                method. Then the `actions` array will be of length 4, with the\n                values of those states in the order that they were added.\n\n                If number of layers is greater than one, then the `actions` array\n                will be of length `4 * number of layers` (in the above scenario).\n                The index from [0:4] will be for layer 0, from [4:8] for layer 1,\n                etc for the number of layers.\n\n                These action values are for direct use in the construction of models.\n\n        Returns:\n            a reward for training a model with the given actions\n        \'\'\'\n        with tf.Session(graph=tf.Graph()) as network_sess:\n            K.set_session(network_sess)\n\n            # generate a submodel given predicted actions\n            model = model_fn(actions)  # type: Model\n            model.compile(\'adam\', \'categorical_crossentropy\', metrics=[\'accuracy\'])\n\n            # unpack the dataset\n            X_train, y_train, X_val, y_val = self.dataset\n\n            # train the model using Keras methods\n            model.fit(X_train, y_train, batch_size=self.batchsize, epochs=self.epochs,\n                      verbose=1, validation_data=(X_val, y_val),\n                      callbacks=[ModelCheckpoint(\'weights/temp_network.h5\',\n                                                 monitor=\'val_acc\', verbose=1,\n                                                 save_best_only=True,\n                                                 save_weights_only=True)])\n\n            # load best performance epoch in this training session\n            model.load_weights(\'weights/temp_network.h5\')\n\n            # evaluate the model\n            loss, acc = model.evaluate(X_val, y_val, batch_size=self.batchsize)\n\n            # compute the reward\n            reward = (acc - self.moving_acc)\n\n            # if rewards are clipped, clip them in the range -0.05 to 0.05\n            if self.clip_rewards:\n                reward = np.clip(reward, -0.05, 0.05)\n\n            # update moving accuracy with bias correction for 1st update\n            if self.beta > 0.0 and self.beta < 1.0:\n                self.moving_acc = self.beta * self.moving_acc + (1 - self.beta) * acc\n                self.moving_acc = self.moving_acc / (1 - self.beta_bias)\n                self.beta_bias = 0\n\n                reward = np.clip(reward, -0.1, 0.1)\n\n            print()\n            print(""Manager: EWA Accuracy = "", self.moving_acc)\n\n        # clean up resources and GPU memory\n        network_sess.close()\n\n        return reward, acc'"
model.py,0,"b""from keras.models import Model\nfrom keras.layers import Input, Dense, Conv2D, GlobalAveragePooling2D\n\n# generic model design\ndef model_fn(actions):\n    # unpack the actions from the list\n    kernel_1, filters_1, kernel_2, filters_2, kernel_3, filters_3, kernel_4, filters_4 = actions\n\n    ip = Input(shape=(32, 32, 3))\n    x = Conv2D(filters_1, (kernel_1, kernel_1), strides=(2, 2), padding='same', activation='relu')(ip)\n    x = Conv2D(filters_2, (kernel_2, kernel_2), strides=(1, 1), padding='same', activation='relu')(x)\n    x = Conv2D(filters_3, (kernel_3, kernel_3), strides=(2, 2), padding='same', activation='relu')(x)\n    x = Conv2D(filters_4, (kernel_4, kernel_4), strides=(1, 1), padding='same', activation='relu')(x)\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(10, activation='softmax')(x)\n\n    model = Model(ip, x)\n    return model\n"""
nascell.py,0,"b'from keras.engine import Layer\nfrom keras import activations\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras import backend as K\nfrom keras.layers import RNN\nfrom keras.layers.recurrent import _generate_dropout_mask, _generate_dropout_ones\n\nimport warnings\n\n#import tensorflow as tf\n#import tensorflow.contrib.rnn as rnn\n\nclass NASCell(Layer):\n    """"""Neural Architecture Search (NAS) recurrent network cell.\n\n    This implements the recurrent cell from the paper:\n    https://arxiv.org/abs/1611.01578\n    Barret Zoph and Quoc V. Le.\n    ""Neural Architecture Search with Reinforcement Learning"" Proc. ICLR 2017.\n\n    The class uses an optional projection layer.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        projection_units: (optional) Positive integer, The output dimensionality\n            for the projection matrices.  If None, no projection is performed.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. ""linear"" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        projection_activation: Activation function to use\n            for the projection step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        projection_initializer: Initializer for the `projection_kernel`\n            weights matrix,\n            used for the linear transformation of the projection step.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        unit_forget_bias: Boolean.\n            If True, add 1 to the bias of the forget gate at initialization.\n            Setting it to true will also force `bias_initializer=""zeros""`.\n            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        projection_regularizer: Regularizer function applied to\n            the `projection_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        projection_constraint: Constraint function applied to\n            the `projection_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n        implementation: Implementation mode, either 1 or 2.\n            Mode 1 will structure its operations as a larger number of\n            smaller dot products and additions, whereas mode 2 will\n            batch them into fewer, larger operations. These modes will\n            have different performance profiles on different hardware and\n            for different applications.\n    """"""\n\n    def __init__(self, units,\n                 projection_units=None,\n                 activation=\'tanh\',\n                 recurrent_activation=\'sigmoid\',\n                 projection_activation=\'linear\',\n                 use_bias=True,\n                 kernel_initializer=\'glorot_uniform\',\n                 recurrent_initializer=\'orthogonal\',\n                 projection_initializer=\'glorot_uniform\',\n                 bias_initializer=\'zeros\',\n                 unit_forget_bias=False,\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 projection_regularizer=None,\n                 bias_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 projection_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 implementation=2,\n                 **kwargs):\n        super(NASCell, self).__init__(**kwargs)\n        self.units = units\n        self.projection_units = projection_units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.projection_activation = activations.get(projection_activation)\n        self.cell_activation = activations.get(\'relu\')\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.projection_initializer = initializers.get(projection_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.unit_forget_bias = unit_forget_bias\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.projection_regularizer = regularizers.get(projection_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.projection_constraint = constraints.get(projection_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n        self.implementation = implementation\n\n        if self.projection_units is not None:\n            self.state_size = (self.projection_units, self.units)\n        else:\n            self.state_size = (self.units, self.units)\n\n        self._dropout_mask = None\n        self._recurrent_dropout_mask = None\n\n    def build(self, input_shape):\n        input_dim = input_shape[-1]\n\n        if self.projection_units is not None:\n            recurrent_output_dim = self.projection_units\n        else:\n            recurrent_output_dim = self.units\n\n        self.kernel = self.add_weight(shape=(input_dim, self.units * 8),\n                                      name=\'kernel\',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n\n        self.recurrent_kernel = self.add_weight(\n            shape=(recurrent_output_dim, self.units * 8),\n            name=\'recurrent_kernel\',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.projection_units is not None:\n            self.projection_kernel = self.add_weight(\n                shape=(self.units, self.projection_units),\n                name=\'projection_kernel\',\n                initializer=self.projection_initializer,\n                regularizer=self.projection_regularizer,\n                constraint=self.projection_constraint)\n\n        if self.use_bias:\n            if self.unit_forget_bias:\n                def bias_initializer(shape, *args, **kwargs):\n                    return K.concatenate([\n                        self.bias_initializer((self.units,), *args, **kwargs),\n                        initializers.Ones()((self.units,), *args, **kwargs),\n                        self.bias_initializer((self.units * 6,), *args, **kwargs),\n                    ])\n            else:\n                bias_initializer = self.bias_initializer\n            self.bias = self.add_weight(shape=(self.units * 8,),\n                                        name=\'bias\',\n                                        initializer=bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        self.kernel_0 = self.kernel[:, :self.units]\n        self.kernel_1 = self.kernel[:, self.units: self.units * 2]\n        self.kernel_2 = self.kernel[:, self.units * 2: self.units * 3]\n        self.kernel_3 = self.kernel[:, self.units * 3: self.units * 4]\n        self.kernel_4 = self.kernel[:, self.units * 4: self.units * 5]\n        self.kernel_5 = self.kernel[:, self.units * 5: self.units * 6]\n        self.kernel_6 = self.kernel[:, self.units * 6: self.units * 7]\n        self.kernel_7 = self.kernel[:, self.units * 7:]\n\n        self.recurrent_kernel_0 = self.recurrent_kernel[:, :self.units]\n        self.recurrent_kernel_1 = self.recurrent_kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_2 = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n        self.recurrent_kernel_3 = self.recurrent_kernel[:, self.units * 3: self.units * 4]\n        self.recurrent_kernel_4 = self.recurrent_kernel[:, self.units * 4: self.units * 5]\n        self.recurrent_kernel_5 = self.recurrent_kernel[:, self.units * 5: self.units * 6]\n        self.recurrent_kernel_6 = self.recurrent_kernel[:, self.units * 6: self.units * 7]\n        self.recurrent_kernel_7 = self.recurrent_kernel[:, self.units * 7:]\n\n        if self.use_bias:\n            self.bias_0 = self.bias[:self.units]\n            self.bias_1 = self.bias[self.units: self.units * 2]\n            self.bias_2 = self.bias[self.units * 2: self.units * 3]\n            self.bias_3 = self.bias[self.units * 3: self.units * 4]\n            self.bias_4 = self.bias[self.units * 4: self.units * 5]\n            self.bias_5 = self.bias[self.units * 5: self.units * 6]\n            self.bias_6 = self.bias[self.units * 6: self.units * 7]\n            self.bias_7 = self.bias[self.units * 7:]\n        else:\n            self.bias_0 = None\n            self.bias_1 = None\n            self.bias_2 = None\n            self.bias_3 = None\n            self.bias_4 = None\n            self.bias_5 = None\n            self.bias_6 = None\n            self.bias_7 = None\n        self.built = True\n\n    def call(self, inputs, states, training=None):\n        if 0 < self.dropout < 1 and self._dropout_mask is None:\n            self._dropout_mask = _generate_dropout_mask(\n                _generate_dropout_ones(inputs, K.shape(inputs)[-1]),\n                self.dropout,\n                training=training,\n                count=8)\n        if (0 < self.recurrent_dropout < 1 and\n                self._recurrent_dropout_mask is None):\n            _recurrent_dropout_mask = _generate_dropout_mask(\n                _generate_dropout_ones(inputs, self.units),\n                self.recurrent_dropout,\n                training=training,\n                count=8)\n            self._recurrent_dropout_mask = _recurrent_dropout_mask\n\n        # dropout matrices for input units\n        dp_mask = self._dropout_mask\n        # dropout matrices for recurrent units\n        rec_dp_mask = self._recurrent_dropout_mask\n\n        h_tm1 = states[0]  # previous memory state\n        c_tm1 = states[1]  # previous carry state\n\n        if self.implementation == 1:\n            if 0 < self.dropout < 1.:\n                inputs_0 = inputs * dp_mask[0]\n                inputs_1 = inputs * dp_mask[1]\n                inputs_2 = inputs * dp_mask[2]\n                inputs_3 = inputs * dp_mask[3]\n                inputs_4 = inputs * dp_mask[4]\n                inputs_5 = inputs * dp_mask[5]\n                inputs_6 = inputs * dp_mask[6]\n                inputs_7 = inputs * dp_mask[7]\n            else:\n                inputs_0 = inputs\n                inputs_1 = inputs\n                inputs_2 = inputs\n                inputs_3 = inputs\n                inputs_4 = inputs\n                inputs_5 = inputs\n                inputs_6 = inputs\n                inputs_7 = inputs\n\n            x_0 = K.dot(inputs_0, self.kernel_0)\n            x_1 = K.dot(inputs_1, self.kernel_1)\n            x_2 = K.dot(inputs_2, self.kernel_2)\n            x_3 = K.dot(inputs_3, self.kernel_3)\n            x_4 = K.dot(inputs_4, self.kernel_4)\n            x_5 = K.dot(inputs_5, self.kernel_5)\n            x_6 = K.dot(inputs_6, self.kernel_6)\n            x_7 = K.dot(inputs_7, self.kernel_7)\n\n            if self.use_bias:\n                x_0 = K.bias_add(x_0, self.bias_0)\n                x_1 = K.bias_add(x_1, self.bias_1)\n                x_2 = K.bias_add(x_2, self.bias_2)\n                x_3 = K.bias_add(x_3, self.bias_3)\n                x_4 = K.bias_add(x_4, self.bias_4)\n                x_5 = K.bias_add(x_5, self.bias_5)\n                x_6 = K.bias_add(x_6, self.bias_6)\n                x_7 = K.bias_add(x_7, self.bias_7)\n\n            if 0 < self.recurrent_dropout < 1.:\n                h_tm1_0 = h_tm1 * rec_dp_mask[0]\n                h_tm1_1 = h_tm1 * rec_dp_mask[1]\n                h_tm1_2 = h_tm1 * rec_dp_mask[2]\n                h_tm1_3 = h_tm1 * rec_dp_mask[3]\n                h_tm1_4 = h_tm1 * rec_dp_mask[4]\n                h_tm1_5 = h_tm1 * rec_dp_mask[5]\n                h_tm1_6 = h_tm1 * rec_dp_mask[6]\n                h_tm1_7 = h_tm1 * rec_dp_mask[7]\n            else:\n                h_tm1_0 = h_tm1\n                h_tm1_1 = h_tm1\n                h_tm1_2 = h_tm1\n                h_tm1_3 = h_tm1\n                h_tm1_4 = h_tm1\n                h_tm1_5 = h_tm1\n                h_tm1_6 = h_tm1\n                h_tm1_7 = h_tm1\n\n            # First Layer\n            layer1_0 = self.recurrent_activation(x_0 + K.dot(h_tm1_0, self.recurrent_kernel_0))\n            layer1_1 = self.cell_activation(x_1 + K.dot(h_tm1_1, self.recurrent_kernel_1))\n            layer1_2 = self.recurrent_activation(x_2 + K.dot(h_tm1_2, self.recurrent_kernel_2))\n            layer1_3 = self.cell_activation(x_3 * K.dot(h_tm1_3, self.recurrent_kernel_3))\n            layer1_4 = self.activation(x_4 + K.dot(h_tm1_4, self.recurrent_kernel_4))\n            layer1_5 = self.recurrent_activation(x_5 + K.dot(h_tm1_5, self.recurrent_kernel_5))\n            layer1_6 = self.activation(x_6 + K.dot(h_tm1_6, self.recurrent_kernel_6))\n            layer1_7 = self.recurrent_activation(x_7 + K.dot(h_tm1_7, self.recurrent_kernel_7))\n\n            # Second Layer\n            layer2_0 = self.activation(layer1_0 * layer1_1)\n            layer2_1 = self.activation(layer1_2 + layer1_3)\n            layer2_2 = self.activation(layer1_4 * layer1_5)\n            layer2_3 = self.recurrent_activation(layer1_6 + layer1_7)\n\n            # Inject the Cell\n            layer2_0 = self.activation(layer2_0 + c_tm1)\n\n            # Third Layer\n            layer3_0_pre = layer2_0 * layer2_1\n            c = layer3_0_pre  # create a new cell\n            layer3_0 = layer3_0_pre\n            layer3_1 = self.activation(layer2_2 + layer2_3)\n\n            # Final Layer\n            h = self.activation(layer3_0 * layer3_1)\n\n            if self.projection_units is not None:\n                h = self.projection_activation(K.dot(h, self.projection_kernel))\n\n        else:\n            if 0. < self.dropout < 1.:\n                inputs *= dp_mask[0]\n            z = K.dot(inputs, self.kernel)\n            if 0. < self.recurrent_dropout < 1.:\n                h_tm1 *= rec_dp_mask[0]\n            zr = K.dot(h_tm1, self.recurrent_kernel)\n            if self.use_bias:\n                zr = K.bias_add(zr, self.bias)\n\n            z0 = z[:, :self.units]\n            z1 = z[:, self.units: 2 * self.units]\n            z2 = z[:, 2 * self.units: 3 * self.units]\n            z3 = z[:, 3 * self.units: 4 * self.units]\n            z4 = z[:, 4 * self.units: 5 * self.units]\n            z5 = z[:, 5 * self.units: 6 * self.units]\n            z6 = z[:, 6 * self.units: 7 * self.units]\n            z7 = z[:, 7 * self.units:]\n\n            zr0 = zr[:, :self.units]\n            zr1 = zr[:, self.units: 2 * self.units]\n            zr2 = zr[:, 2 * self.units: 3 * self.units]\n            zr3 = zr[:, 3 * self.units: 4 * self.units]\n            zr4 = zr[:, 4 * self.units: 5 * self.units]\n            zr5 = zr[:, 5 * self.units: 6 * self.units]\n            zr6 = zr[:, 6 * self.units: 7 * self.units]\n            zr7 = zr[:, 7 * self.units:]\n\n            # First Layer\n            layer1_0 = self.recurrent_activation(z0 + zr0)\n            layer1_1 = self.cell_activation(z1 + zr1)\n            layer1_2 = self.recurrent_activation(z2 + zr2)\n            layer1_3 = self.cell_activation(z3 * zr3)\n            layer1_4 = self.activation(z4 + zr4)\n            layer1_5 = self.recurrent_activation(z5 + zr5)\n            layer1_6 = self.activation(z6 + zr6)\n            layer1_7 = self.recurrent_activation(z7 + zr7)\n\n            # Second Layer\n            layer2_0 = self.activation(layer1_0 * layer1_1)\n            layer2_1 = self.activation(layer1_2 + layer1_3)\n            layer2_2 = self.activation(layer1_4 * layer1_5)\n            layer2_3 = self.recurrent_activation(layer1_6 + layer1_7)\n\n            # Inject the Cell\n            layer2_0 = self.activation(layer2_0 + c_tm1)\n\n            # Third Layer\n            layer3_0_pre = layer2_0 * layer2_1\n            c = layer3_0_pre\n            layer3_0 = layer3_0_pre\n            layer3_1 = self.activation(layer2_2 + layer2_3)\n\n            # Final Layer\n            h = self.activation(layer3_0 * layer3_1)\n\n            if self.projection_units is not None:\n                h = self.projection_activation(K.dot(h, self.projection_kernel))\n\n        if 0 < self.dropout + self.recurrent_dropout:\n            if training is None:\n                h._uses_learning_phase = True\n        return h, [h, c]\n\n    def get_config(self):\n        config = {\'units\': self.units,\n                  \'projection_units\': self.projection_units,\n                  \'activation\': activations.serialize(self.activation),\n                  \'recurrent_activation\': activations.serialize(self.recurrent_activation),\n                  \'projection_activation\': activations.serialize(self.projection_activation),\n                  \'use_bias\': self.use_bias,\n                  \'kernel_initializer\': initializers.serialize(self.kernel_initializer),\n                  \'recurrent_initializer\': initializers.serialize(self.recurrent_initializer),\n                  \'projection_initializer\': initializers.serialize(self.projection_initializer),\n                  \'bias_initializer\': initializers.serialize(self.bias_initializer),\n                  \'unit_forget_bias\': self.unit_forget_bias,\n                  \'kernel_regularizer\': regularizers.serialize(self.kernel_regularizer),\n                  \'recurrent_regularizer\': regularizers.serialize(self.recurrent_regularizer),\n                  \'projection_regularizer\': regularizers.serialize(self.projection_regularizer),\n                  \'bias_regularizer\': regularizers.serialize(self.bias_regularizer),\n                  \'kernel_constraint\': constraints.serialize(self.kernel_constraint),\n                  \'recurrent_constraint\': constraints.serialize(self.recurrent_constraint),\n                  \'projection_constraint\': constraints.serialize(self.projection_constraint),\n                  \'bias_constraint\': constraints.serialize(self.bias_constraint),\n                  \'dropout\': self.dropout,\n                  \'recurrent_dropout\': self.recurrent_dropout,\n                  \'implementation\': self.implementation}\n        base_config = super(NASCell, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass NASRNN(RNN):\n    """"""Neural Architecture Search (NAS) recurrent network cell.\n\n    This implements the recurrent cell from the paper:\n    https://arxiv.org/abs/1611.01578\n    Barret Zoph and Quoc V. Le.\n    ""Neural Architecture Search with Reinforcement Learning"" Proc. ICLR 2017.\n\n    The class uses an optional projection layer.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        projection_units: (optional) Positive integer, The output dimensionality\n            for the projection matrices.  If None, no projection is performed.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. ""linear"" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        projection_activation: Activation function to use\n            for the projection step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        projection_initializer: Initializer for the `projection_kernel`\n            weights matrix,\n            used for the linear transformation of the projection step.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        unit_forget_bias: Boolean.\n            If True, add 1 to the bias of the forget gate at initialization.\n            Setting it to true will also force `bias_initializer=""zeros""`.\n            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        projection_regularizer: Regularizer function applied to\n            the `projection_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        projection_constraint: Constraint function applied to\n            the `projection_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n        implementation: Implementation mode, either 1 or 2.\n            Mode 1 will structure its operations as a larger number of\n            smaller dot products and additions, whereas mode 2 will\n            batch them into fewer, larger operations. These modes will\n            have different performance profiles on different hardware and\n            for different applications.\n        return_sequences: Boolean. Whether to return the last output.\n            in the output sequence, or the full sequence.\n        return_state: Boolean. Whether to return the last state\n            in addition to the output.\n        go_backwards: Boolean (default False).\n            If True, process the input sequence backwards and return the\n            reversed sequence.\n        stateful: Boolean (default False). If True, the last state\n            for each sample at index i in a batch will be used as initial\n            state for the sample of index i in the following batch.\n        unroll: Boolean (default False).\n            If True, the network will be unrolled,\n            else a symbolic loop will be used.\n            Unrolling can speed-up a RNN,\n            although it tends to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n\n    # References\n        - [Long short-term memory](http://www.bioinf.jku.at/publications/older/2604.pdf) (original 1997 paper)\n        - [Learning to forget: Continual prediction with NestedLSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n        - [Nested LSTMs](https://arxiv.org/abs/1801.10308)\n    """"""\n\n    def __init__(self, units,\n                 projection_units=None,\n                 activation=\'tanh\',\n                 recurrent_activation=\'sigmoid\',\n                 projection_activation=\'linear\',\n                 use_bias=True,\n                 kernel_initializer=\'glorot_uniform\',\n                 recurrent_initializer=\'orthogonal\',\n                 projection_initializer=\'glorot_uniform\',\n                 bias_initializer=\'zeros\',\n                 unit_forget_bias=False,\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 projection_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 projection_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 implementation=2,\n                 return_sequences=False,\n                 return_state=False,\n                 go_backwards=False,\n                 stateful=False,\n                 unroll=False,\n                 **kwargs):\n        if implementation == 0:\n            warnings.warn(\'`implementation=0` has been deprecated, \'\n                          \'and now defaults to `implementation=2`.\'\n                          \'Please update your layer call.\')\n        if K.backend() == \'theano\':\n            warnings.warn(\n                \'RNN dropout is no longer supported with the Theano backend \'\n                \'due to technical limitations. \'\n                \'You can either set `dropout` and `recurrent_dropout` to 0, \'\n                \'or use the TensorFlow backend.\')\n            dropout = 0.\n            recurrent_dropout = 0.\n\n        cell = NASCell(units, projection_units,\n                       activation=activation,\n                       recurrent_activation=recurrent_activation,\n                       projection_activation=projection_activation,\n                       use_bias=use_bias,\n                       kernel_initializer=kernel_initializer,\n                       recurrent_initializer=recurrent_initializer,\n                       projection_initializer=projection_initializer,\n                       unit_forget_bias=unit_forget_bias,\n                       bias_initializer=bias_initializer,\n                       kernel_regularizer=kernel_regularizer,\n                       recurrent_regularizer=recurrent_regularizer,\n                       bias_regularizer=bias_regularizer,\n                       projection_regularizer=projection_regularizer,\n                       kernel_constraint=kernel_constraint,\n                       recurrent_constraint=recurrent_constraint,\n                       bias_constraint=bias_constraint,\n                       projection_constraint=projection_constraint,\n                       dropout=dropout,\n                       recurrent_dropout=recurrent_dropout,\n                       implementation=implementation)\n        super(NASRNN, self).__init__(cell,\n                                      return_sequences=return_sequences,\n                                      return_state=return_state,\n                                      go_backwards=go_backwards,\n                                      stateful=stateful,\n                                      unroll=unroll,\n                                      **kwargs)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n    def call(self, inputs, mask=None, training=None, initial_state=None, constants=None):\n        self.cell._dropout_mask = None\n        self.cell._recurrent_dropout_mask = None\n        return super(NASRNN, self).call(inputs,\n                                            mask=mask,\n                                            training=training,\n                                            initial_state=initial_state,\n                                            constants=constants)\n\n    @property\n    def units(self):\n        return self.cell.units\n\n    @property\n    def projection_units(self):\n        return self.cell.projection_units\n\n    @property\n    def activation(self):\n        return self.cell.activation\n\n    @property\n    def recurrent_activation(self):\n        return self.cell.recurrent_activation\n\n    @property\n    def projection_activation(self):\n        return self.cell.projection_activation\n\n    @property\n    def use_bias(self):\n        return self.cell.use_bias\n\n    @property\n    def kernel_initializer(self):\n        return self.cell.kernel_initializer\n\n    @property\n    def recurrent_initializer(self):\n        return self.cell.recurrent_initializer\n\n    @property\n    def bias_initializer(self):\n        return self.cell.bias_initializer\n\n    @property\n    def projection_initializer(self):\n        return self.cell.projection_initializer\n\n    @property\n    def unit_forget_bias(self):\n        return self.cell.unit_forget_bias\n\n    @property\n    def kernel_regularizer(self):\n        return self.cell.kernel_regularizer\n\n    @property\n    def recurrent_regularizer(self):\n        return self.cell.recurrent_regularizer\n\n    @property\n    def bias_regularizer(self):\n        return self.cell.bias_regularizer\n\n    @property\n    def projection_regularizer(self):\n        return self.cell.projection_regularizer\n\n    @property\n    def kernel_constraint(self):\n        return self.cell.kernel_constraint\n\n    @property\n    def recurrent_constraint(self):\n        return self.cell.recurrent_constraint\n\n    @property\n    def bias_constraint(self):\n        return self.cell.bias_constraint\n\n    @property\n    def projection_constraint(self):\n        return self.cell.projection_constraint\n\n    @property\n    def dropout(self):\n        return self.cell.dropout\n\n    @property\n    def recurrent_dropout(self):\n        return self.cell.recurrent_dropout\n\n    @property\n    def implementation(self):\n        return self.cell.implementation\n\n    def get_config(self):\n        config = {\'units\': self.units,\n                  \'projection_units\': self.projection_units,\n                  \'activation\': activations.serialize(self.activation),\n                  \'recurrent_activation\': activations.serialize(self.recurrent_activation),\n                  \'projection_activation\': activations.serialize(self.projection_activation),\n                  \'use_bias\': self.use_bias,\n                  \'kernel_initializer\': initializers.serialize(self.kernel_initializer),\n                  \'recurrent_initializer\': initializers.serialize(self.recurrent_initializer),\n                  \'bias_initializer\': initializers.serialize(self.bias_initializer),\n                  \'projection_initializer\': initializers.serialize(self.projection_initializer),\n                  \'unit_forget_bias\': self.unit_forget_bias,\n                  \'kernel_regularizer\': regularizers.serialize(self.kernel_regularizer),\n                  \'recurrent_regularizer\': regularizers.serialize(self.recurrent_regularizer),\n                  \'bias_regularizer\': regularizers.serialize(self.bias_regularizer),\n                  \'projection_regularizer\': regularizers.serialize(self.projection_regularizer),\n                  \'activity_regularizer\': regularizers.serialize(self.activity_regularizer),\n                  \'kernel_constraint\': constraints.serialize(self.kernel_constraint),\n                  \'recurrent_constraint\': constraints.serialize(self.recurrent_constraint),\n                  \'bias_constraint\': constraints.serialize(self.bias_constraint),\n                  \'projection_constraint\': constraints.serialize(self.projection_constraint),\n                  \'dropout\': self.dropout,\n                  \'recurrent_dropout\': self.recurrent_dropout,\n                  \'implementation\': self.implementation}\n        base_config = super(NASRNN, self).get_config()\n        del base_config[\'cell\']\n        return dict(list(base_config.items()) + list(config.items()))\n\n    @classmethod\n    def from_config(cls, config):\n        if \'implementation\' in config and config[\'implementation\'] == 0:\n            config[\'implementation\'] = 2\n        return cls(**config)\n'"
train.py,1,"b'import numpy as np\nimport csv\n\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras.datasets import cifar10\nfrom keras.utils import to_categorical\n\nfrom controller import Controller, StateSpace\nfrom manager import NetworkManager\nfrom model import model_fn\n\n# create a shared session between Keras and Tensorflow\npolicy_sess = tf.Session()\nK.set_session(policy_sess)\n\nNUM_LAYERS = 4  # number of layers of the state space\nMAX_TRIALS = 250  # maximum number of models generated\n\nMAX_EPOCHS = 10  # maximum number of epochs to train\nCHILD_BATCHSIZE = 128  # batchsize of the child models\nEXPLORATION = 0.8  # high exploration for the first 1000 steps\nREGULARIZATION = 1e-3  # regularization strength\nCONTROLLER_CELLS = 32  # number of cells in RNN controller\nEMBEDDING_DIM = 20  # dimension of the embeddings for each state\nACCURACY_BETA = 0.8  # beta value for the moving average of the accuracy\nCLIP_REWARDS = 0.0  # clip rewards in the [-0.05, 0.05] range\nRESTORE_CONTROLLER = True  # restore controller to continue training\n\n# construct a state space\nstate_space = StateSpace()\n\n# add states\nstate_space.add_state(name=\'kernel\', values=[1, 3])\nstate_space.add_state(name=\'filters\', values=[16, 32, 64])\n\n# print the state space being searched\nstate_space.print_state_space()\n\n# prepare the training data for the NetworkManager\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nx_train = x_train.astype(\'float32\') / 255.\nx_test = x_test.astype(\'float32\') / 255.\ny_train = to_categorical(y_train, 10)\ny_test = to_categorical(y_test, 10)\n\ndataset = [x_train, y_train, x_test, y_test]  # pack the dataset for the NetworkManager\n\nprevious_acc = 0.0\ntotal_reward = 0.0\n\nwith policy_sess.as_default():\n    # create the Controller and build the internal policy network\n    controller = Controller(policy_sess, NUM_LAYERS, state_space,\n                            reg_param=REGULARIZATION,\n                            exploration=EXPLORATION,\n                            controller_cells=CONTROLLER_CELLS,\n                            embedding_dim=EMBEDDING_DIM,\n                            restore_controller=RESTORE_CONTROLLER)\n\n# create the Network Manager\nmanager = NetworkManager(dataset, epochs=MAX_EPOCHS, child_batchsize=CHILD_BATCHSIZE, clip_rewards=CLIP_REWARDS,\n                         acc_beta=ACCURACY_BETA)\n\n# get an initial random state space if controller needs to predict an\n# action from the initial state\nstate = state_space.get_random_state_space(NUM_LAYERS)\nprint(""Initial Random State : "", state_space.parse_state_space_list(state))\nprint()\n\n# clear the previous files\ncontroller.remove_files()\n\n# train for number of trails\nfor trial in range(MAX_TRIALS):\n    with policy_sess.as_default():\n        K.set_session(policy_sess)\n        actions = controller.get_action(state)  # get an action for the previous state\n\n    # print the action probabilities\n    state_space.print_actions(actions)\n    print(""Predicted actions : "", state_space.parse_state_space_list(actions))\n\n    # build a model, train and get reward and accuracy from the network manager\n    reward, previous_acc = manager.get_rewards(model_fn, state_space.parse_state_space_list(actions))\n    print(""Rewards : "", reward, ""Accuracy : "", previous_acc)\n\n    with policy_sess.as_default():\n        K.set_session(policy_sess)\n\n        total_reward += reward\n        print(""Total reward : "", total_reward)\n\n        # actions and states are equivalent, save the state and reward\n        state = actions\n        controller.store_rollout(state, reward)\n\n        # train the controller on the saved state and the discounted rewards\n        loss = controller.train_step()\n        print(""Trial %d: Controller loss : %0.6f"" % (trial + 1, loss))\n\n        # write the results of this trial into a file\n        with open(\'train_history.csv\', mode=\'a+\') as f:\n            data = [previous_acc, reward]\n            data.extend(state_space.parse_state_space_list(state))\n            writer = csv.writer(f)\n            writer.writerow(data)\n    print()\n\nprint(""Total Reward : "", total_reward)'"
