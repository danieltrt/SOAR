file_path,api_count,code
setup.py,0,"b'# Update the code and upload the package to pypi\n# 1. python ./setup.py bdist_wheel --universal\n# 2. twine upload dist/simple_tensorflow_serving-x.x.x-py2.py3-none-any.whl\n\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=""simple_tensorflow_serving"",\n    version=""0.8.1.1"",\n    author=""tobe"",\n    author_email=""tobeg3oogle@gmail.com"",\n    url=""https://github.com/tobegit3hub/simple_tensorflow_serving"",\n    description=\n    ""The simpler and easy-to-use serving service for TensorFlow models"",\n    packages=find_packages(),\n    install_requires=[\n        \'configparser\', \'pandas\', \'protobuf\', \'flask\', \'jinja2\', \'flask-cors\',\n        \'requests\', \'pillow\', \'uwsgi\'\n    ],\n    include_package_data=True,\n    zip_safe=False,\n    entry_points={\n        ""console_scripts"": [\n            # TODO: Do not use wsgi by default\n            #""simple_tensorflow_serving=simple_tensorflow_serving.command:main"",\n            ""simple_tensorflow_serving=simple_tensorflow_serving.server:main"",\n            ""stfs=simple_tensorflow_serving.server:main""\n        ],\n    })\n'"
wsgi.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom simple_tensorflow_serving.server import application\n\n\ndef main():\n  application.run()\n\n\nif __name__ == ""__main__"":\n  main()\n'"
benchmark/benchmark_simplest_client.py,0,"b'#!/usr/bin/env python\n\nimport time\nimport requests\n\ndef main():\n  endpoint = ""http://127.0.0.1:8500""\n  json_data = {""model_name"": ""default"", ""data"": {""keys"": [1, 1]} }\n\n  iteration = 1000\n  start_time = time.time()\n\n  for i in range(iteration):\n    result = requests.post(endpoint, json=json_data)\n\n  end_time = time.time()\n  print(result.json())\n\n  print(""Benchmark iteration: {}, time: {}"".format(iteration, end_time - start_time))\n\n\nif __name__ == ""__main__"":\n  main()\n  \n'"
benchmark/benchmark_template_client.py,0,"b'#!/usr/bin/env python\n\nimport time\nimport requests\n\ndef main():\n  endpoint = ""http://127.0.0.1:8500""\n  json_data = {""model_name"": ""default"", ""data"": {""keys"": [[1], [1]], ""features"": [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]]} }\n\n  iteration = 1000\n  start_time = time.time()\n\n  for i in range(iteration):\n    result = requests.post(endpoint, json=json_data)\n\n  end_time = time.time()\n  print(result.json())\n\n  print(""Benchmark iteration: {}, time: {}"".format(iteration, end_time - start_time))\n\nif __name__ == ""__main__"":\n  main()\n  \n'"
check_saved_model/check.py,3,"b'#!/usr/bin/env python\n\nimport tensorflow as tf\n\n\ndef main():\n  test_file_path = ""../models/tensorflow_template_application_model/1""\n  is_saved_model = check_saved_model(test_file_path)\n  if is_saved_model:\n    print(""{} is the saved model"".format(test_file_path))\n  else:\n    print(""{} is NOT the saved model"".format(test_file_path))\n\n\ndef check_saved_model(model_file_path):\n  try:\n    session = tf.Session(graph=tf.Graph())\n    meta_graph = tf.saved_model.loader.load(\n        session, [tf.saved_model.tag_constants.SERVING], model_file_path)\n    return True\n  except IOError as ioe:\n    print(""Catch exception: "".foramt(ioe))\n    return False\n\n\nif __name__ == ""__main__"":\n  main()\n'"
simple_tensorflow_serving/__init__.py,0,b''
simple_tensorflow_serving/abstract_inference_service.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom abc import ABCMeta, abstractmethod\n\n\nclass AbstractInferenceService(object):\n  """"""\n  The abstract class for inference service which should implement the method.\n  """"""\n\n  __metaclass__ = ABCMeta\n\n  def __init__(self):\n    self.model_name = None\n    self.model_base_path = """"\n    self.model_version_list = []\n    self.model_graph_signature = None\n    self.model_graph_signature_dict = {}\n    self.platform = """"\n\n  @abstractmethod\n  def inference(self, json_data):\n    """"""\n    Args:\n      json_data: The JSON serialized object with key and array data.\n    Return:\n      The JSON serialized object with key and array data.\n    """"""\n    pass\n\n  def get_detail(self):\n    detail = {}\n    detail[""model_name""] = self.model_name\n    detail[""model_base_path""] = self.model_base_path\n    detail[""model_version_list""] = self.model_version_list\n    detail[""model_signature""] = self.model_graph_signature_dict\n    detail[""platform""] = self.platform\n    return detail'"
simple_tensorflow_serving/base64_util.py,0,"b'import base64\n\n\ndef replace_b64_in_dict(item):\n  """"""\n  Replace base64 string in python dictionary of inference data. Refer to https://github.com/tensorflow/serving/blob/master/tensorflow_serving/g3doc/api_rest.md#encoding-binary-values .\n  \n  For example: {\'inputs\': {\'images\': {\'b64\': \'YWJjZGVmZ2hpMTIz\'}, \'foo\': \'bar\'}} to {\'inputs\': {\'images\': \'abcdefghi123\', \'foo\': \'bar\'}}.\n  """"""\n\n  if isinstance(item, dict):\n    # Use items for Python 3 instead of iteritems\n    for key, value in item.items():\n      if isinstance(value, dict) and list(value.keys())[0] == ""b64"":\n        # Use list to wrap .keys() and .values() for Python 3\n        b64_string = list(value.values())[0]\n        # TODO: unicode string to string\n        b64_string = str(b64_string)\n        bytearray_string = base64.urlsafe_b64decode(b64_string)\n        item[key] = bytearray_string\n      else:\n        replace_b64_in_dict(value)\n\n  elif isinstance(item, list):\n    for index, value in enumerate(item):\n      if isinstance(value, dict) and list(value.keys())[0] == ""b64"":\n        b64_string = list(value.values())[0]\n        b64_string = str(b64_string)\n        bytearray_string = base64.urlsafe_b64decode(b64_string)\n        item[index] = bytearray_string\n      else:\n        replace_b64_in_dict(value)\n'"
simple_tensorflow_serving/base64_util_test.py,0,"b'import base64\n\nfrom . import base64_util\n\ntest_data1 = {""inputs"": {""images"": {""b64"": ""YWJjZGVmZ2hpMTIz""}, ""foo"": ""bar""}}\nprint(test_data1)\nbase64_util.replace_b64_in_dict(test_data1)\nprint(test_data1)\n\ntest_data1 = {\n    ""inputs"": {\n        ""images"": [{\n            ""b64"": ""YWJjZGVmZ2hpMTIz""\n        }, ""foo"", ""bar""]\n    }\n}\nprint(test_data1)\nbase64_util.replace_b64_in_dict(test_data1)\nprint(test_data1)\n\ntest_data1 = {\n    ""inputs"": {\n        ""images"": {\n            ""b64"": ""YWJjZGVmZ2hpMTIz""\n        },\n        ""foo"": [{\n            ""images"": {\n                ""b64"": ""YWJjZGVmZ2hpMTIz""\n            }\n        }]\n    }\n}\nprint(test_data1)\nbase64_util.replace_b64_in_dict(test_data1)\nprint(test_data1)\n\ntest_data1 = {\n    ""inputs"": {\n        ""images"": [[{\n            ""b64"": ""YWJjZGVmZ2hpMTIz""\n        }, ""foo""], [{\n            ""b64"": ""YWJjZGVmZ2hpMTIz""\n        }, ""bar""]]\n    }\n}\nprint(test_data1)\nbase64_util.replace_b64_in_dict(test_data1)\nprint(test_data1)\n\ninput_data = {\n    ""model_name"": ""tensorflow_template_application_model"",\n    ""model_version"": 1,\n    ""signature_name"": ""serving_default"",\n    ""data"": {\n        ""keys"": [[1.0], [2.0]],\n        ""b64_inputs"": [[{\n            ""b64"": ""YWJjZGVmZ2hpMTIz""\n        }], [""hello""]],\n        ""features"": [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]]\n    }\n}\ntest_data1 = input_data\nprint(test_data1)\nbase64_util.replace_b64_in_dict(test_data1)\nprint(test_data1)\n\ntest_data1 = {\n    \'data\': {\n        \'keys\': [[1.0], [2.0]],\n        \'features\': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]],\n        \'key\': [[{\n            \'b64\': u\'YWJjZGVmZ2hpMTIz\'\n        }]]\n    }\n}\nprint(test_data1)\nbase64_util.replace_b64_in_dict(test_data1)\nprint(test_data1)\n\n\ndef generate_b64_string():\n  test_string = ""abcdefghi123""\n  b64_string = base64.urlsafe_b64encode(test_string)\n  print(b64_string)\n\n  test_string = base64.urlsafe_b64decode(b64_string)\n  import ipdb\n  ipdb.set_trace()\n  print(test_string)\n'"
simple_tensorflow_serving/command.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport configparser\nimport subprocess\n\n\ndef print_usage():\n  """"""\n  Print the usage of this class.\n  """"""\n\n  print(\'Usage: simple_tensorflow_serving --model_base_path=""./model""\')\n\n\ndef update_uwsgi_conf(args, uwsgi_conf):\n  host = os.environ.get(""STFS_HOST"", ""0.0.0.0"")\n  port = int(os.environ.get(""STFS_PORT"", ""8500""))\n\n  for arg in args:\n    if arg.startswith(""-h"") or arg.startswith(""--help""):\n      print_usage()\n      return\n    if arg.startswith(""--host""):\n      host = arg[7:]\n      print(""Use the host: {}"".format(host))\n    if arg.startswith(""--port""):\n      port = int(arg[7:])\n      print(""Use the port: {}"".format(port))\n\n    worker_number = int(os.environ.get(""STFS_WORKERS"", ""1""))\n    if arg.startswith(""--workers""):\n      worker_number = int(arg[10:])\n\n    thread_number = int(os.environ.get(""STFS_THREADS"", ""1""))\n    if arg.startswith(""--threads""):\n        thread_number = int(arg[10:])\n\n  uwsgi_conf[""uwsgi""][""http""] = ""{}:{}"".format(host, port)\n  uwsgi_conf[""uwsgi""][""workers""] = worker_number\n  uwsgi_conf[""uwsgi""][""threads""] = thread_number\n\n\ndef main():\n  """"""\n  Start new uwsgi progress for simple tensorflow serving.\n  """"""\n\n  # 1. Parse command-line parameter to generate uwsgi conf\n  args = sys.argv[1:]\n  uwsgi_conf = {\n      ""uwsgi"": {\n          ""module"": ""simple_tensorflow_serving.server:app"",\n          ""pyargv"": "" "".join(args),\n          ""http"": ""0.0.0.0:8500"",\n          ""socket"": ""/tmp/uwsgi.sock"",\n          ""pidfile"": ""/tmp/uwsgi.pid"",\n          ""master"": True,\n          ""close-on-exec"": True,\n          ""enable-threads"": True,\n          ""http-keepalive"": 1,\n          ""http-auto-chunked"": 1,\n          ""workers"": 1,\n          ""threads"": 1,\n          # TODO: Log format refers to https://uwsgi-docs.readthedocs.io/en/latest/LogFormat.html\n          #""log-format"": \'%(ltime) ""%(method) %(uri) %(proto)"" %(status) %(size) ""%(referer)"" ""%(uagent)""\'\n      }\n  }\n  update_uwsgi_conf(args, uwsgi_conf)\n  print(""Uwsgi config: {}"".format(uwsgi_conf))\n\n  # 2. Save config file of uwsgi.ini\n  uwsgi_ini_file = ""/tmp/uwsgi.ini""\n  with open(uwsgi_ini_file, ""w"") as f:\n    uwsgi_confparser = configparser.ConfigParser()\n    uwsgi_confparser.read_dict(uwsgi_conf)\n    uwsgi_confparser.write(f)\n  print(""Save uwsgi config in: {}"".format(uwsgi_ini_file))\n\n  # 3. Start uwsgi command\n  uwsgi_command = ""uwsgi --ini {}"".format(uwsgi_ini_file)\n  print(""Try to run command: {}"".format(uwsgi_command))\n  subprocess.call(uwsgi_command, shell=True)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
simple_tensorflow_serving/filesystem_util.py,12,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport logging\nimport tensorflow as tf\n\nlogger = logging.getLogger(""simple_tensorflow_serving"")\n\n\ndef download_hdfs_moels(model_base_path):\n  """"""\n  Download the model file from HDFS into local filesystem.\n\n  Args:\n    model_base_path: The model path which is in local or HDFS.\n  \n  Return:\n    The new path in local of model file.\n  """"""\n\n  if not model_base_path.startswith(""hdfs://""):\n    return model_base_path\n  else:\n    libhdfs_model_base_path = update_hdfs_prefix_for_libhdfs(model_base_path)\n    new_model_base_path = os.path.join(""/tmp/"",\n                                       libhdfs_model_base_path.split(""/"")[-1])\n    logging.info(""Copy model file from {} to {}"".format(\n            libhdfs_model_base_path, new_model_base_path))\n    if tf.__version__.startswith(""1""):\n      tf.gfile.Copy(libhdfs_model_base_path, new_model_base_path, overwrite=True)\n    else:\n      tf.io.gfile.copy(libhdfs_model_base_path, new_model_base_path, overwrite=True)\n\n    return new_model_base_path\n\n\ndef update_hdfs_prefix_for_libhdfs(model_base_path):\n  """"""\n  Update the hdfs path with Java prefix into Libhdfs prefix.\n  \n  Args:\n    model_base_path: The model path which is in local or HDFS.\n\n  Return:\n    The new path with Libhdfs prefix.\n  """"""\n\n  if model_base_path.startswith(""hdfs:///""):\n    # Change ""hdfs:///..."" to ""hdfs://default/...""\n    new_model_base_path = model_base_path[:7] + ""default"" + model_base_path[7:]\n    logging.info(""Update hdfs prefix from {} to {}"".format(\n        model_base_path, new_model_base_path))\n    return new_model_base_path\n  else:\n    return model_base_path\n\n\ndef down_mxnet_model_from_hdfs(model_base_path):\n  """"""\n  Download the MXNet model file from HDFS into local filesystem.\n\n  Args:\n    model_base_path: The model path which is in local or HDFS.\n\n  Return:\n    The new path in local of model file.\n  """"""\n\n  if not model_base_path.startswith(""hdfs://""):\n    return model_base_path\n  else:\n    libhdfs_model_base_path = update_hdfs_prefix_for_libhdfs(model_base_path)\n\n    model_prefix = libhdfs_model_base_path.split(""/"")[-1]\n    # Example: ""./models/mxnet_mlp/mx_mlp"" to ""./models/mxnet_mlp/""\n    parent_model_base_path = ""/"".join(libhdfs_model_base_path.split(""/"")[:-1])\n\n    # Example: /tmp/mxnet_mlp/\n    new_parent_model_base_path = os.path.join(\n        ""/tmp/"", parent_model_base_path.split(""/"")[-1])\n\n    if tf.__version__.startswith(""1""):\n      tf.gfile.MakeDirs(new_parent_model_base_path)\n    else:\n      tf.io.gfile.mkdir(new_parent_model_base_path)\n\n    # Example:\n    new_model_base_path = os.path.join(new_parent_model_base_path,\n                                       model_prefix)\n\n    if tf.__version__.startswith(""1""):\n      dir_files = tf.gfile.ListDirectory(parent_model_base_path)\n    else:\n      dir_files = tf.io.gfile.listdir(parent_model_base_path)\n\n    for file_name in dir_files:\n      if file_name.startswith(model_prefix):\n        old_file_path = parent_model_base_path + ""/"" + file_name\n        new_file_path = os.path.join(new_parent_model_base_path, file_name)\n        logging.info(""Copy model file from {} to {}"".format(\n            old_file_path, new_file_path))\n\n        if tf.__version__.startswith(""1""):\n          tf.gfile.Copy(old_file_path, new_file_path, overwrite=True)\n        else:\n          tf.io.gfile.copy(old_file_path, new_file_path, overwrite=True)\n\n    return new_model_base_path\n'"
simple_tensorflow_serving/h2o_inference_service.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport logging\nimport time\nimport json\nimport subprocess\n\nfrom abstract_inference_service import AbstractInferenceService\nimport filesystem_util\nimport preprocess_util\n\nlogger = logging.getLogger(""simple_tensorflow_serving"")\n\n\nclass H2oInferenceService(AbstractInferenceService):\n  """"""\n  The H2O service to load H2O model and make inference.\n  """"""\n\n  def __init__(self, model_name, model_base_path):\n    """"""\n    Initialize the service.\n        \n    Args:\n      model_name: The name of the model.\n      model_base_path: The file path of the model.\n    Return:\n      None\n    """"""\n    super(H2oInferenceService, self).__init__()\n\n    # Start the h2o server\n    if os.path.isfile(""/tmp/h2o.jar""):\n      logging.info(""Run to run command \'java -jar /tmp/h2o.jar\'"")\n      subprocess.Popen([""java"", ""-jar"", ""/tmp/h2o.jar""])\n\n      logging.info(""Sleep 10s to wait for h2o server"")\n      time.sleep(10)\n\n    local_model_base_path = filesystem_util.download_hdfs_moels(\n        model_base_path)\n\n    self.model_name = model_name\n    self.model_base_path = local_model_base_path\n    self.model_version_list = [1]\n    self.model_graph_signature = """"\n    self.platform = ""H2o""\n\n    self.preprocess_function, self.postprocess_function = preprocess_util.get_preprocess_postprocess_function_from_model_path(\n        self.model_base_path)\n\n    import h2o\n\n    logger.info(""Try to initialize and connect the h2o server"")\n    h2o.init()\n\n    logger.info(""Try to load the h2o model"")\n    model = h2o.load_model(self.model_base_path)\n\n    self.model = model\n    # TODO: Update the signature with readable string\n    self.model_graph_signature = ""{}"".format(self.model.full_parameters)\n\n  def inference(self, json_data):\n    """"""\n    Make inference with the current Session object and JSON request data.\n        \n    Args:\n      json_data: The JSON serialized object with key and array data.\n                 Example is {""model_version"": 1, ""data"": {""keys"": [[1.0], [2.0]], ""features"": [[10, 10, 10, 8, 6, 1, 8, 9, 1], [6, 2, 1, 1, 1, 1, 7, 1, 1]]}}.\n    Return:\n      The dictionary with key and array data.\n      Example is {""keys"": [[11], [2]], ""softmax"": [[0.61554497, 0.38445505], [0.61554497, 0.38445505]], ""prediction"": [0, 0]}.\n    """"""\n\n    # 1. Import libraries\n    import h2o\n    import pandas as pd\n\n    # 2. Do inference\n    request_ndarray_data = json_data[""data""][""data""]\n\n    if json_data.get(""preprocess"", ""false"") != ""false"":\n      if self.preprocess_function != None:\n        request_ndarray_data = self.preprocess_function(request_ndarray_data)\n        logger.debug(\n            ""Preprocess to generate data: {}"".format(request_ndarray_data))\n      else:\n        logger.warning(""No preprocess function in model"")\n\n    start_time = time.time()\n\n    df = pd.read_json(json.dumps(request_ndarray_data), orient=""index"")\n\n    #test = h2o.H2OFrame(df.values.tolist())\n    test = h2o.H2OFrame(df)\n\n    predictions = self.model.predict(test)\n    predictions.show()\n\n    #performance = self.model.model_performance(test)\n    #performance.show()\n\n    logger.debug(""Inference time: {} s"".format(time.time() - start_time))\n\n    result_df = predictions.as_data_frame()\n    result_string = result_df.to_json(orient=\'index\')\n\n    # 3. Build return data\n    result = json.loads(result_string)\n    logger.debug(""Inference result: {}"".format(result))\n\n    if json_data.get(""postprocess"", ""false"") != ""false"":\n      if self.postprocess_function != None:\n        result = self.postprocess_function(result)\n        logger.debug(""Postprocess to generate data: {}"".format(result))\n      else:\n        logger.warning(""No postprocess function in model"")\n\n    return result\n'"
simple_tensorflow_serving/manager.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'.\'))\n\nimport json\nimport logging\n\nimport base64_util\nfrom tensorflow_inference_service import TensorFlowInferenceService\nfrom mxnet_inference_service import MxnetInferenceService\nfrom onnx_inference_service import OnnxInferenceService\nfrom pytorch_onnx_inference_service import PytorchOnnxInferenceService\nfrom h2o_inference_service import H2oInferenceService\nfrom scikitlearn_inference_service import ScikitlearnInferenceService\nfrom xgboost_inference_service import XgboostInferenceService\nfrom pmml_inference_service import PmmlInferenceService\nfrom spark_inference_service import SparkInferenceService\n\nlogger = logging.getLogger(\'simple_tensorflow_serving\')\n\n\nclass InferenceServiceManager:\n  def __init__(self, args):\n    self.args = args\n\n    # Example: {""default"": TensorFlowInferenceService}\n    self.model_name_service_map = {}\n\n  def init(self):\n\n    if self.args.model_config_file != """":\n      # Read from configuration file\n      with open(self.args.model_config_file) as data_file:\n        model_config_file_dict = json.load(data_file)\n        # Example: [{u\'platform\': u\'tensorflow\', u\'name\': u\'tensorflow_template_application\', u\'base_path\': u\'/Users/tobe/code/simple_tensorflow_serving/models/tensorflow_template_application_model/\'}, {u\'platform\': u\'tensorflow\', u\'name\': u\'deep_image_model\', u\'base_path\': u\'/Users/tobe/code/simple_tensorflow_serving/models/deep_image_model/\'}]\n        model_config_list = model_config_file_dict[""model_config_list""]\n\n        for model_config in model_config_list:\n          # Example: {""name"": ""tensorflow_template_application"", ""base_path"": ""/"", ""platform"": ""tensorflow""}\n          model_name = model_config[""name""]\n          model_base_path = model_config[""base_path""]\n          model_platform = model_config.get(""platform"", ""tensorflow"")\n          custom_op_paths = model_config.get(""custom_op_paths"", """")\n          session_config = model_config.get(""session_config"", {})\n\n          if model_platform == ""tensorflow"":\n            inference_service = TensorFlowInferenceService(\n                model_name, model_base_path, custom_op_paths, session_config)\n          elif model_platform == ""mxnet"":\n            inference_service = MxnetInferenceService(model_name,\n                                                      model_base_path)\n          elif model_platform == ""onnx"":\n            inference_service = OnnxInferenceService(model_name,\n                                                     model_base_path)\n          elif model_platform == ""pytorch_onnx"":\n            inference_service = PytorchOnnxInferenceService(\n                model_name, model_base_path)\n          elif model_platform == ""h2o"":\n            inference_service = H2oInferenceService(model_name,\n                                                    model_base_path)\n          elif model_platform == ""scikitlearn"":\n            inference_service = ScikitlearnInferenceService(\n                model_name, model_base_path)\n          elif model_platform == ""xgboost"":\n            inference_service = XgboostInferenceService(\n                model_name, model_base_path)\n          elif model_platform == ""pmml"":\n            inference_service = PmmlInferenceService(model_name,\n                                                     model_base_path)\n          elif model_platform == ""spark"":\n            inference_service = SparkInferenceService(model_name,\n                                                      model_base_path)\n\n          self.model_name_service_map[model_name] = inference_service\n    else:\n\n      # Read from command-line parameter\n      if self.args.model_platform == ""tensorflow"":\n        session_config = json.loads(self.args.session_config)\n        inference_service = TensorFlowInferenceService(\n            self.args.model_name, self.args.model_base_path,\n            self.args.custom_op_paths, session_config)\n      elif self.args.model_platform == ""mxnet"":\n        inference_service = MxnetInferenceService(self.args.model_name,\n                                                  self.args.model_base_path)\n      elif self.args.model_platform == ""h2o"":\n        inference_service = H2oInferenceService(self.args.model_name,\n                                                self.args.model_base_path)\n      elif self.args.model_platform == ""onnx"":\n        inference_service = OnnxInferenceService(self.args.model_name,\n                                                 self.args.model_base_path)\n      elif self.args.model_platform == ""pytorch_onnx"":\n        inference_service = PytorchOnnxInferenceService(\n            self.args.model_name, self.args.model_base_path)\n      elif self.args.model_platform == ""scikitlearn"":\n        inference_service = ScikitlearnInferenceService(\n            self.args.model_name, self.args.model_base_path)\n      elif self.args.model_platform == ""xgboost"":\n        inference_service = XgboostInferenceService(self.args.model_name,\n                                                    self.args.model_base_path)\n      elif self.args.model_platform == ""pmml"":\n        inference_service = PmmlInferenceService(self.args.model_name,\n                                                 self.args.model_base_path)\n      elif self.args.model_platform == ""spark"":\n        inference_service = SparkInferenceService(self.args.model_name,\n                                                  self.args.model_base_path)\n\n      self.model_name_service_map[self.args.model_name] = inference_service\n\n    # Start thread to periodically reload models or not\n    if self.args.reload_models == ""True"" or self.args.reload_models == ""true"":\n      for model_name, inference_service in self.model_name_service_map.items():\n        if inference_service.platform == ""tensorflow"":\n          inference_service.dynamically_reload_models()\n\n  def inference(self, model_name, json_data):\n    """"""\n    Args:\n      save_file_dir: Path to save data.\n  \n    Return:\n      json_data: The inference result or error message.\n      status code: The HTTP response code.\n    """"""\n\n    inferenceService = self.model_name_service_map[model_name]\n\n    if self.args.enable_b64_autoconvert:\n      # Decode base64 string and modify request json data\n      base64_util.replace_b64_in_dict(json_data)\n\n    result = inferenceService.inference(json_data)\n    return result\n'"
simple_tensorflow_serving/mxnet_inference_service.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport os\nimport time\nimport json\nfrom collections import namedtuple\n\nfrom abstract_inference_service import AbstractInferenceService\nimport filesystem_util\nimport preprocess_util\n\nlogger = logging.getLogger(""simple_tensorflow_serving"")\n\n\nclass MxnetInferenceService(AbstractInferenceService):\n  """"""\n  The MXNet service to load MXNet checkpoint and make inference.\n  """"""\n\n  def __init__(self, model_name, model_base_path):\n    """"""\n    Initialize the service.\n        \n    Args:\n      model_name: The name of the model.\n      model_base_path: The file path of the model.\n    Return:\n      None\n    """"""\n\n    super(MxnetInferenceService, self).__init__()\n\n    local_model_base_path = filesystem_util.down_mxnet_model_from_hdfs(\n        model_base_path)\n\n    self.model_name = model_name\n    self.model_base_path = local_model_base_path\n    self.model_version_list = [1]\n    self.model_graph_signature = """"\n    self.platform = ""MXNet""\n\n    self.preprocess_function, self.postprocess_function = preprocess_util.get_preprocess_postprocess_function_from_model_path(\n        self.model_base_path)\n\n    # TODO: Import as needed and only once\n    import mxnet as mx\n\n    # TODO: Select the available version\n    epoch_number = 1\n\n    # Load model\n    sym, arg_params, aux_params = mx.model.load_checkpoint(\n        self.model_base_path, epoch_number)\n    self.mod = mx.mod.Module(symbol=sym, context=mx.cpu(), label_names=None)\n    self.has_signature_file = False\n    self.signature_input_names = []\n    self.signature_output_names = []\n\n    # Load inputs from signature file\n    signature_file_path = self.model_base_path + ""-signature.json""\n    if os.path.exists(signature_file_path) and os.path.isfile(\n        signature_file_path):\n      self.has_signature_file = True\n      data_shapes = []\n\n      with open(signature_file_path) as signature_file:\n        signature_dict = json.load(signature_file)\n        inputs = signature_dict[""inputs""]\n        outputs = signature_dict[""outputs""]\n\n        for input in inputs:\n          input_data_name = input[""data_name""]\n          input_data_shape = input[""data_shape""]\n\n          self.signature_input_names.append(input_data_name)\n          data_shapes.append((input_data_name, tuple(input_data_shape)))\n\n        for output in outputs:\n          output_data_name = output[""data_name""]\n          ouput_data_shape = output[""data_shape""]\n\n          self.signature_output_names.append(output_data_name)\n\n    else:\n      data_shapes = [(\'data\', (1, 2))]\n\n    self.mod.bind(for_training=False, data_shapes=data_shapes)\n    self.mod.set_params(arg_params, aux_params, allow_missing=True)\n    if self.has_signature_file:\n      self.model_graph_signature = ""Inputs: {}\\nOutputs: {}\\n{}"".format(\n          self.signature_input_names, self.signature_output_names,\n          self.mod.symbol.tojson())\n    else:\n      self.model_graph_signature = ""{}"".format(self.mod.symbol.tojson())\n\n  def inference(self, json_data):\n    """"""\n    Make inference with the current Session object and JSON request data.\n        \n    Args:\n      json_data: The JSON serialized object with key and array data.\n                 Example is {""model_version"": 1, ""data"": {""keys"": [[1.0], [2.0]], ""features"": [[10, 10, 10, 8, 6, 1, 8, 9, 1], [6, 2, 1, 1, 1, 1, 7, 1, 1]]}}.\n    Return:\n      The dictionary with key and array data.\n      Example is {""keys"": [[11], [2]], ""softmax"": [[0.61554497, 0.38445505], [0.61554497, 0.38445505]], ""prediction"": [0, 0]}.\n    """"""\n\n    import mxnet as mx\n    from mxnet.io import DataBatch\n\n    # 1. Build inference data\n\n    # TODO: Should use DataBatch or not\n    # from mxnet.io import DataBatch\n    # request_ndarray_data = json_data[""data""][""data""]\n    # request_mxnet_ndarray_data = [mx.nd.array(request_ndarray_data)]\n    # batch_data = DataBatch(request_mxnet_ndarray_data)\n\n    if self.has_signature_file:\n      request_mxnet_ndarray_data = []\n      batch_tuple_list = []\n\n      for input_name in self.signature_input_names:\n        # Example: [[7.0, 2.0]]\n        request_ndarray_data = json_data[""data""][input_name]\n        batch_tuple_list.append(input_name)\n        request_mxnet_ndarray_data.append(mx.nd.array(request_ndarray_data))\n\n      # namedtuple(\'Batch\', [\'data\'])\n      Batch = namedtuple(\'Batch\', batch_tuple_list)\n      batch_data = Batch(request_mxnet_ndarray_data)\n    else:\n      Batch = namedtuple(\'Batch\', [\'data\'])\n      request_ndarray_data = json_data[""data""][""data""]\n      request_mxnet_ndarray_data = [mx.nd.array(request_ndarray_data)]\n      batch_data = Batch(request_mxnet_ndarray_data)\n\n    if json_data.get(""preprocess"", ""false"") != ""false"":\n      if self.preprocess_function != None:\n        batch_data = self.preprocess_function(batch_data)\n        logger.debug(""Preprocess to generate data: {}"".format(batch_data))\n      else:\n        logger.warning(""No preprocess function in model"")\n\n    # 2. Do inference\n    start_time = time.time()\n    self.mod.forward(batch_data)\n    logging.debug(""Inference time: {} s"".format(time.time() - start_time))\n\n    model_outputs = self.mod.get_outputs()\n    prob = self.mod.get_outputs()[0].asnumpy()\n    print(prob)\n\n    # 3. Build return data\n    result = {}\n    for i, model_output in enumerate(model_outputs):\n      result[self.signature_output_names[i]] = model_output.asnumpy()\n    logging.debug(""Inference result: {}"".format(result))\n\n    if json_data.get(""postprocess"", ""false"") != ""false"":\n      if self.postprocess_function != None:\n        result = self.postprocess_function(result)\n        logger.debug(""Postprocess to generate data: {}"".format(result))\n      else:\n        logger.warning(""No postprocess function in model"")\n\n    return result\n'"
simple_tensorflow_serving/onnx_inference_service.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport os\nimport time\nimport json\nimport numpy as np\nfrom collections import namedtuple\n\nfrom abstract_inference_service import AbstractInferenceService\nimport filesystem_util\nimport preprocess_util\n\nlogger = logging.getLogger(""simple_tensorflow_serving"")\n\n\nclass OnnxInferenceService(AbstractInferenceService):\n  """"""\n  The service to load ONNX model and make inference.\n  """"""\n\n  def __init__(self, model_name, model_base_path):\n    """"""\n    Initialize the service.\n        \n    Args:\n      model_name: The name of the model.\n      model_base_path: The file path of the model.\n    Return:\n      None\n    """"""\n\n    super(OnnxInferenceService, self).__init__()\n\n    local_model_base_path = filesystem_util.download_hdfs_moels(\n        model_base_path)\n\n    self.model_name = model_name\n    self.model_base_path = local_model_base_path\n    self.model_version_list = [1]\n    self.model_graph_signature = """"\n    self.platform = ""ONNX""\n\n    self.preprocess_function, self.postprocess_function = preprocess_util.get_preprocess_postprocess_function_from_model_path(\n        self.model_base_path)\n\n    # TODO: Import as needed and only once\n    import mxnet as mx\n    #import onnx_mxnet\n\n    # TODO: Select the available version\n    epoch_number = 1\n\n    # Load model\n    #sym, arg_params, aux_params = mx.model.load_checkpoint(self.model_base_path, epoch_number)\n    #sym, params = onnx_mxnet.import_model(self.model_base_path)\n    sym, arg_params, aux_params = mx.contrib.onnx.import_model(\n        self.model_base_path)\n\n    # TODO: Support other inputs\n    # self.mod = mx.mod.Module(symbol=sym, context=mx.cpu(), label_names=None)\n    #self.mod = mx.mod.Module(symbol=sym, data_names=[\'input_0\'], context=mx.cpu(), label_names=None)\n    self.mod = mx.mod.Module(\n        symbol=sym, data_names=[\'1\'], context=mx.cpu(), label_names=None)\n\n    self.has_signature_file = False\n    self.signature_input_names = []\n    self.signature_output_names = []\n\n    # Load inputs from signature file\n    signature_file_path = self.model_base_path + ""-signature.json""\n    if os.path.exists(signature_file_path) and os.path.isfile(\n        signature_file_path):\n      self.has_signature_file = True\n      data_shapes = []\n\n      with open(signature_file_path) as signature_file:\n        signature_dict = json.load(signature_file)\n        inputs = signature_dict[""inputs""]\n        outputs = signature_dict[""outputs""]\n\n        for input in inputs:\n          input_data_name = input[""data_name""]\n          input_data_shape = input[""data_shape""]\n\n          self.signature_input_names.append(input_data_name)\n          data_shapes.append((input_data_name, tuple(input_data_shape)))\n\n        for output in outputs:\n          output_data_name = output[""data_name""]\n          ouput_data_shape = output[""data_shape""]\n\n          self.signature_output_names.append(output_data_name)\n\n    else:\n      data_shapes = [(\'data\', (1, 2))]\n      test_image = np.random.randn(1, 1, 28, 28)\n      data_shapes = [(\'1\', test_image.shape)]\n\n    self.mod.bind(for_training=False, data_shapes=data_shapes)\n    self.mod.set_params(\n        arg_params, aux_params, allow_missing=True, allow_extra=True)\n    if self.has_signature_file:\n      self.model_graph_signature = ""Inputs: {}\\nOutputs: {}\\n{}"".format(\n          self.signature_input_names, self.signature_output_names,\n          self.mod.symbol.tojson())\n    else:\n      self.model_graph_signature = ""{}"".format(self.mod.symbol.tojson())\n\n  def inference(self, json_data):\n    """"""\n    Make inference with the current Session object and JSON request data.\n        \n    Args:\n      json_data: The JSON serialized object with key and array data.\n                 Example is {""model_version"": 1, ""data"": {""keys"": [[1.0], [2.0]], ""features"": [[10, 10, 10, 8, 6, 1, 8, 9, 1], [6, 2, 1, 1, 1, 1, 7, 1, 1]]}}.\n    Return:\n      The dictionary with key and array data.\n      Example is {""keys"": [[11], [2]], ""softmax"": [[0.61554497, 0.38445505], [0.61554497, 0.38445505]], ""prediction"": [0, 0]}.\n    """"""\n\n    import mxnet as mx\n    from mxnet.io import DataBatch\n\n    # 1. Build inference data\n\n    # TODO: Should use DataBatch or not\n    # from mxnet.io import DataBatch\n    # request_ndarray_data = json_data[""data""][""data""]\n    # request_mxnet_ndarray_data = [mx.nd.array(request_ndarray_data)]\n    # batch_data = DataBatch(request_mxnet_ndarray_data)\n\n    if self.has_signature_file:\n      request_mxnet_ndarray_data = []\n      batch_tuple_list = []\n\n      for input_name in self.signature_input_names:\n        # Example: [[7.0, 2.0]]\n        request_ndarray_data = json_data[""data""][input_name]\n        batch_tuple_list.append(input_name)\n        request_mxnet_ndarray_data.append(mx.nd.array(request_ndarray_data))\n\n      # namedtuple(\'Batch\', [\'data\'])\n      Batch = namedtuple(\'Batch\', batch_tuple_list)\n      batch_data = Batch(request_mxnet_ndarray_data)\n    else:\n      Batch = namedtuple(\'Batch\', [\'data\'])\n      request_ndarray_data = json_data[""data""][""data""]\n      request_mxnet_ndarray_data = [mx.nd.array(request_ndarray_data)]\n      batch_data = Batch(request_mxnet_ndarray_data)\n\n    if json_data.get(""preprocess"", ""false"") != ""false"":\n      if self.preprocess_function != None:\n        batch_data = self.preprocess_function(batch_data)\n        logger.debug(""Preprocess to generate data: {}"".format(batch_data))\n      else:\n        logger.warning(""No preprocess function in model"")\n\n    # 2. Do inference\n    start_time = time.time()\n    self.mod.forward(batch_data)\n    logger.debug(""Inference time: {} s"".format(time.time() - start_time))\n\n    model_outputs = self.mod.get_outputs()\n\n    prob = self.mod.get_outputs()[0].asnumpy()\n    print(prob)\n\n    # 3. Build return data\n    result = {}\n    for i, model_output in enumerate(model_outputs):\n      # TODO: Get the real output name from ONNX model\n      #result[self.signature_output_names[i]] = model_output.asnumpy()\n      result[""output_{}"".format(i)] = model_output.asnumpy()\n    logger.debug(""Inference result: {}"".format(result))\n\n    if json_data.get(""postprocess"", ""false"") != ""false"":\n      if self.postprocess_function != None:\n        result = self.postprocess_function(result)\n        logger.debug(""Postprocess to generate data: {}"".format(result))\n      else:\n        logger.warning(""No postprocess function in model"")\n\n    return result\n'"
simple_tensorflow_serving/pmml_inference_service.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport logging\nimport time\nimport subprocess\nimport requests\n\nfrom abstract_inference_service import AbstractInferenceService\nimport filesystem_util\nimport preprocess_util\n\nlogger = logging.getLogger(""simple_tensorflow_serving"")\n\n\nclass PmmlInferenceService(AbstractInferenceService):\n  """"""\n  The service to load PMML model and make inference.\n  """"""\n\n  def __init__(self, model_name, model_base_path):\n    """"""\n    Initialize the service.\n        \n    Args:\n      model_name: The name of the model.\n      model_base_path: The file path of the model.\n    Return:\n      None\n    """"""\n\n    super(PmmlInferenceService, self).__init__()\n\n    # Start the pmml server\n    if os.path.isfile(""/tmp/openscoring-server-executable-1.4-SNAPSHOT.jar""):\n      logging.info(\n          ""Run to run command \'java -jar /tmp/openscoring-server-executable-1.4-SNAPSHOT.jar\'""\n      )\n      subprocess.Popen([\n          ""java"", ""-jar"", ""/tmp/openscoring-server-executable-1.4-SNAPSHOT.jar""\n      ])\n\n      logging.info(""Sleep 10s to wait for pmml server"")\n      time.sleep(10)\n\n    local_model_base_path = filesystem_util.download_hdfs_moels(\n        model_base_path)\n\n    self.model_name = model_name\n    self.model_base_path = local_model_base_path\n    self.model_version_list = [1]\n    self.model_graph_signature = """"\n    self.platform = ""PMML""\n\n    self.preprocess_function, self.postprocess_function = preprocess_util.get_preprocess_postprocess_function_from_model_path(\n        self.model_base_path)\n\n    # Load model\n    """"""\n    from openscoring import Openscoring\n    # TODO: Support to config the port of Openscoring server\n    openscoring_server_endpoint = ""localhost:8080""\n    kwargs = {""auth"": (""admin"", ""adminadmin"")}\n    self.openscoring = Openscoring(\n        ""http://{}/openscoring"".format(openscoring_server_endpoint))\n    self.openscoring.deployFile(self.model_name, self.model_base_path,\n                                **kwargs)\n    """"""\n\n    endpoint = ""http://localhost:8080/openscoring/model/{}"".format(\n        self.model_name)\n\n    with open(self.model_base_path, ""rb"") as f:\n      kwargs = {\n          \'headers\': {\n              \'content-type\': \'application/xml\'\n          },\n          \'json\': None,\n          \'data\': f,\n          \'auth\': (\'admin\', \'adminadmin\')\n      }\n      result = requests.put(endpoint, **kwargs)\n      logging.info(""Deploy the model to Openscoring: {}"".format(result.text))\n\n    self.model_graph_signature = ""No signature for PMML models""\n\n  def inference(self, json_data):\n    """"""\n    Make inference with the current Session object and JSON request data.\n        \n    Args:\n      json_data: The JSON serialized object with key and array data.\n                 Example is {""model_version"": 1, ""data"": {""keys"": [[1.0], [2.0]], ""features"": [[10, 10, 10, 8, 6, 1, 8, 9, 1], [6, 2, 1, 1, 1, 1, 7, 1, 1]]}}.\n    Return:\n      The dictionary with key and array data.\n      Example is {""keys"": [[11], [2]], ""softmax"": [[0.61554497, 0.38445505], [0.61554497, 0.38445505]], ""prediction"": [0, 0]}.\n    """"""\n\n    # 1. Build inference data\n    # Example: arguments = {""Sepal_Length"" : 5.1, ""Sepal_Width"" : 3.5, ""Petal_Length"" : 1.4, ""Petal_Width"" : 0.2}\n    request_json_data = json_data[""data""]\n\n    if json_data.get(""preprocess"", ""false"") != ""false"":\n      if self.preprocess_function != None:\n        request_json_data = self.preprocess_function(request_json_data)\n        logger.debug(\n            ""Preprocess to generate data: {}"".format(request_json_data))\n      else:\n        logger.warning(""No preprocess function in model"")\n\n    # 2. Do inference\n    start_time = time.time()\n    # Example: {\'Probability_setosa\': 1.0, \'Probability_versicolor\': 0.0, \'Node_Id\': \'2\', \'Species\': \'setosa\', \'Probability_virginica\': 0.0}\n    #predict_result = self.openscoring.evaluate(self.model_name, request_json_data)\n\n    endpoint = ""http://localhost:8080/openscoring/model/{}"".format(\n        self.model_name)\n    input_data = {""id"": None, ""arguments"": request_json_data}\n    result = requests.post(endpoint, json=input_data)\n    predict_result = result.json()\n\n    logger.debug(""Inference time: {} s"".format(time.time() - start_time))\n    logger.debug(""Inference result: {}"".format(predict_result))\n\n    if json_data.get(""postprocess"", ""false"") != ""false"":\n      if self.postprocess_function != None:\n        predict_result = self.postprocess_function(predict_result)\n        logger.debug(""Postprocess to generate data: {}"".format(predict_result))\n      else:\n        logger.warning(""No postprocess function in model"")\n\n    return predict_result\n'"
simple_tensorflow_serving/preprocess_util.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport marshal\nimport types\nimport logging\n\nlogger = logging.getLogger(""simple_tensorflow_serving"")\n\n\ndef get_function_from_marshal_file(function_file_path,\n                                    function_name=""function""):\n  logging.info(""Try to get function from file: {}"".format(function_file_path))\n\n  function_object = None\n\n  if os.path.exists(function_file_path):\n    with open(function_file_path, ""rb"") as f:\n      preprocess_function_string = f.read()\n      loaded_function = marshal.loads(preprocess_function_string)\n      function_object = types.FunctionType(loaded_function,\n                                           globals(), function_name)\n\n  return function_object\n\n\ndef get_preprocess_postprocess_function_from_model_path(model_base_path):\n  # Example: ./models/scikitlearn_iris/model.joblib\n  model_parent_path = ""/"".join(model_base_path.split(""/"")[:-1])\n\n  preprocess_file_path = os.path.join(model_parent_path,\n                                      ""preprocess_function.marshal"")\n  preprocess_function = get_function_from_marshal_file(preprocess_file_path)\n\n  postprocess_file_path = os.path.join(model_parent_path,\n                                       ""postprocess_function.marshal"")\n  postprocess_function = get_function_from_marshal_file(postprocess_file_path)\n\n  return preprocess_function, postprocess_function\n'"
simple_tensorflow_serving/python_predict_client.py,0,"b'#!/usr/bin/env python\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport json\nimport requests\nimport logging\nfrom PIL import Image\nimport numpy as np\n\n\nlogging.basicConfig(\n        format=\'%(asctime)s %(levelname)-8s %(message)s\',\n        level=logging.INFO,\n        datefmt=\'%Y-%m-%d %H:%M:%S\')\n\n# TODO: Remove logging make debug log level not work\n#logger = logging.getLogger(""simple_tensorflow_serving"")\n\n\ndef predict_image(image_file_path, channel_layout=""RGB"", run_profile="""", port=8500):\n  endpoint = ""http://127.0.0.1:"" + str(port)\n\n  img = Image.open(image_file_path)\n  img = img.convert(channel_layout)\n  img.load()\n  image_ndarray = np.asarray(img, dtype=""int32"")\n  # Shape is [48, 400, 3] -> [400, 48, 3]\n  image_ndarray = image_ndarray.transpose((1, 0, 2))\n  image_array = [image_ndarray.tolist()]\n  # TODO: Support specified model name\n  json_data = {""model_name"": ""default"",\n               ""data"": {""image"": image_array},\n               ""run_profile"": run_profile}\n\n  result = requests.post(endpoint, json=json_data)\n\n  # Shape is [-1, -1]\n  predict_result = json.loads(result.text)\n  print(""Get predict result:{}"".format(predict_result))\n\n  return predict_result\n\n\ndef predict_json(json_data, port=8500):\n  # TODO: Support for other endpoint\n  endpoint = ""http://127.0.0.1:"" + str(port)\n  predict_result = ""Error""\n\n  try:\n    result = requests.post(endpoint, json=json_data)\n    predict_result = result.json()\n    logging.debug(""Get predict result:{}"".format(predict_result))\n  except Exception as e:\n    logging.error(""Get result: {} and exception: {}"".format(result, e.message))\n\n  return predict_result\n\n\ndef get_gen_json_and_clients(model_name=""default"", signature_name=""serving_default"", language=""json"", port=8500):\n  return_result = ""Error""\n\n  try:\n\n    if language == ""json"" or language == ""Json"" or language == ""JSON"":\n      endpoint = ""http://127.0.0.1:{}/v1/models/{}/gen_json"".format(port, model_name)\n      result = requests.get(endpoint)\n      return_result = result.json()\n    else:\n      endpoint = ""http://127.0.0.1:{}/v1/models/{}/gen_client?language={}"".format(port, model_name, language)\n      result = requests.get(endpoint)\n      return_result = result.text\n\n    logging.debug(""Get predict result:{}"".format(return_result))\n  except Exception as e:\n    logging.error(""Get exception: {}"".format(e.message))\n\n  return return_result\n\n\n\n# TODO: Only support testing with images\ndef parse_args():\n  parser = argparse.ArgumentParser(description=\'Predict image\')\n  parser.add_argument(\n      \'--image\', required=False, type=str, default=""./0.jpg"", help=\'The image\')\n  args = parser.parse_args()\n  return args\n\n\ndef main(args):\n  image_file_path = args.image\n  predict_image(image_file_path)\n\n\nif __name__ == ""__main__"":\n  main(parse_args())\n'"
simple_tensorflow_serving/pytorch_onnx_inference_service.py,0,"b'\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport traceback\nimport logging\nimport os\nimport time\nimport json\nimport numpy as np\nfrom collections import namedtuple\n\nfrom abstract_inference_service import AbstractInferenceService\n\n\n# Lazy init\nNUMPY_DTYPE_MAP = {}\n\n\nclass PytorchOnnxInferenceService(AbstractInferenceService):\n  """"""\n  The service to load ONNX model and make inference with pytorch-caffe2 backend.\n  """"""\n\n  def __init__(self, model_name, model_base_path, verbose=False):\n    """"""\n    Initialize the service.\n        \n    Args:\n      model_name: The name of the model.\n      model_base_path: The file path of the model.\n    Return:\n      None\n    """"""\n    super(PytorchOnnxInferenceService, self).__init__()\n\n    # Init onnx datatype map to numpy\n    self.init_dtype_map()\n\n    self.model_name = model_name\n    self.model_base_path = model_base_path\n    self.model_version_list = []\n\n    self.model_version_dict = {}\n    self.model_dict = {}\n    self.executor_dict = {}\n    self.model_graph_signature_dict = {}\n\n    # This property is for index.html\n    self.model_graph_signature = self.model_graph_signature_dict\n\n    self.platform = ""PyTorch_ONNX""\n    self.verbose = verbose\n\n    # Find available models\n    model_path_list = []\n    if os.path.isdir(self.model_base_path):\n      for filename in os.path.listdir(self.model_base_path):\n        if filename.endswith("".onnx""):\n          path = os.path.join(self.model_base_path, filename)\n          logging.info(""Found onnx model: {}"".format(path))\n          model_path_list.append(path)\n      if len(model_path_list) == 0:\n        logging.error(""No onnx model found in {}"".format(self.model_base_path))\n    elif os.path.isfile(self.model_base_path):\n      logging.info(""Found onnx model: {}"".format(self.model_base_path))\n      model_path_list.append(self.model_base_path)\n    else:\n      raise Exception(""Invalid model_base_path: {}"".format(self.model_base_path))\n\n    # Load available models\n    count = 1\n    for model_path in model_path_list:\n      try:\n        version = str(count)\n        model, executor, signature = self.load_model(model_path)\n        logging.info(""Load onnx model: {}, signature: {}"".format(\n          model_path, json.dumps(signature)))\n        self.model_version_dict[version] = model_path\n        self.model_dict[version] = model\n        self.executor_dict[version] = executor\n        self.model_graph_signature_dict[version] = signature\n        self.model_version_list.append(version)\n        count += 1\n      except Exception as e:\n        traceback.print_exc()\n\n  def init_dtype_map(self):\n    from onnx import TensorProto as tp\n    global NUMPY_DTYPE_MAP\n    NUMPY_DTYPE_MAP.update({\n      tp.FLOAT: ""float32"",\n      tp.UINT8: ""uint8"",\n      tp.INT8: ""int8"",\n      tp.INT32: ""int32"",\n      tp.INT64: ""int64"",\n      tp.DOUBLE: ""float64"",\n      tp.UINT32: ""uint32"",\n      tp.UINT64: ""uint64""\n    })\n\n  def load_model(self, model_path):\n    # TODO: Import as needed and only once\n    import onnx\n    import caffe2.python.onnx.backend as backend\n    \n    # Load model\n    model = onnx.load(model_path)\n    onnx.checker.check_model(model)\n    \n    # Genetate signature\n    signature = {""inputs"": [], ""outputs"": []}\n    for input in model.graph.input:\n      if isinstance(input, str) or isinstance(input, unicode):\n        # maybe old version onnx proto\n        signature[""inputs""].append({""name"": input})\n      else:\n        info = input.type.tensor_type\n        input_meta = {\n          ""name"": input.name,\n          ""dtype"": int(info.elem_type),\n          ""shape"": [(d.dim_value if d.HasField(""dim_value"") else -1) \n                    for d in info.shape.dim]\n        }\n        signature[""inputs""].append(input_meta)\n    for output in model.graph.output:\n      if isinstance(output, str) or isinstance(output, unicode):\n        # maybe old version onnx proto\n        signature[""outputs""].append({""name"": output})\n      else:\n        info = output.type.tensor_type\n        output_meta = {\n          ""name"": output.name,\n          ""dtype"": int(info.elem_type),\n          ""shape"": [(d.dim_value if d.HasField(""dim_value"") else -1) \n                    for d in info.shape.dim]\n        }\n        signature[""outputs""].append(output_meta)\n\n    # Build model executor\n    executor = backend.prepare(model)\n    return model, executor, signature\n\n  def inference(self, json_data):\n    # Get version\n    model_version = str(json_data.get(""model_version"", """"))\n    if model_version.strip() == """":\n      model_version = self.model_version_list[-1]\n\n    input_data = json_data.get(""data"", """")\n    if str(model_version) not in self.model_version_dict or input_data == """":\n      logging.error(""No model version: {} to serve"".format(model_version))\n      return ""Fail to request the model version: {} with data: {}"".format(\n          model_version, input_data)\n    else:\n      logging.debug(""Inference with json data: {}"".format(json_data))\n\n    signature = self.model_graph_signature_dict[model_version]\n    inputs_signature = signature[""inputs""]\n    inputs = []\n    if isinstance(input_data, dict):\n      for input_meta in inputs_signature:\n        name = input_meta[""name""]\n        onnx_type = input_meta[""dtype""]\n        if name not in input_data:\n          logging.error(""Cannot find input name: {}"".format(name))\n        else:\n          data_item = input_data[name]\n          if not isinstance(data_item, np.ndarray):\n            data_item = np.asarray(data_item)\n          if onnx_type in NUMPY_DTYPE_MAP:\n            numpy_type = NUMPY_DTYPE_MAP[onnx_type]\n            if numpy_type != data_item.dtype:\n              data_item = data_item.astype(numpy_type)\n          inputs.append(data_item)\n    else:\n      raise Exception(""Invalid json input data"")        \n\n    start_time = time.time()\n    executor = self.executor_dict[model_version]\n    outputs = executor.run(inputs)\n    \n    result = {}\n    for idx, output_meta in enumerate(signature[""outputs""]):\n      name = output_meta[""name""]\n      result[name] = outputs[idx]\n    logging.debug(""Inference result: {}"".format(result))\n    return result\n\n'"
simple_tensorflow_serving/scikitlearn_inference_service.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport time\nimport numpy as np\nimport pickle\n\nfrom abstract_inference_service import AbstractInferenceService\nimport filesystem_util\nimport preprocess_util\n\nlogger = logging.getLogger(""simple_tensorflow_serving"")\n\n\nclass ScikitlearnInferenceService(AbstractInferenceService):\n  """"""\n  The service to load Scikit-learn model and make inference.\n  """"""\n\n  def __init__(self, model_name, model_base_path):\n    """"""\n    Initialize the service.\n        \n    Args:\n      model_name: The name of the model.\n      model_base_path: The file path of the model.\n    Return:\n      None\n    """"""\n\n    super(ScikitlearnInferenceService, self).__init__()\n\n    local_model_base_path = filesystem_util.download_hdfs_moels(\n        model_base_path)\n\n    self.model_name = model_name\n    self.model_base_path = local_model_base_path\n    self.model_version_list = [1]\n    self.model_graph_signature = """"\n    self.platform = ""Scikit-learn""\n\n    # TODO: Download function files from HDFS if needed\n    self.preprocess_function, self.postprocess_function = preprocess_util.get_preprocess_postprocess_function_from_model_path(\n        self.model_base_path)\n\n    # TODO: Import as needed and only once\n    from sklearn.externals import joblib\n\n    # Load model\n    if self.model_base_path.endswith("".joblib""):\n      self.pipeline = joblib.load(self.model_base_path)\n    elif self.model_base_path.endswith(\n        "".pkl"") or self.model_base_path.endswith("".pickle""):\n      with open(self.model_base_path, \'rb\') as f:\n        self.pipeline = pickle.load(f)\n    else:\n      logger.error(\n          ""Unsupported model file format: {}"".format(self.model_base_path))\n\n    self.model_graph_signature = str(self.pipeline.get_params())\n\n  def inference(self, json_data):\n    """"""\n    Make inference with the current Session object and JSON request data.\n        \n    Args:\n      json_data: The JSON serialized object with key and array data.\n                 Example is {""model_version"": 1, ""data"": {""keys"": [[1.0], [2.0]], ""features"": [[10, 10, 10, 8, 6, 1, 8, 9, 1], [6, 2, 1, 1, 1, 1, 7, 1, 1]]}}.\n    Return:\n      The dictionary with key and array data.\n      Example is {""keys"": [[11], [2]], ""softmax"": [[0.61554497, 0.38445505], [0.61554497, 0.38445505]], ""prediction"": [0, 0]}.\n    """"""\n\n    # 1. Build inference data\n    input_data = json_data[""data""]\n\n    if json_data.get(""preprocess"", ""false"") != ""false"":\n      if self.preprocess_function != None:\n        input_data = self.preprocess_function(input_data)\n        logger.debug(""Preprocess to generate data: {}"".format(input_data))\n      else:\n        logger.warning(""No preprocess function in model"")\n\n    request_ndarray_data = np.array(input_data)\n\n    # 2. Do inference\n    start_time = time.time()\n\n    predict_result = self.pipeline.predict(request_ndarray_data)\n    predict_proba_result = self.pipeline.predict_proba(request_ndarray_data)\n    predict_log_proba_result = self.pipeline.predict_log_proba(\n        request_ndarray_data)\n\n    logger.debug(""Inference time: {} s"".format(time.time() - start_time))\n\n    # 3. Build return data\n    result = {\n        ""predict"": predict_result,\n        ""predict_proba"": predict_proba_result,\n        ""predict_log_proba"": predict_log_proba_result\n    }\n    logger.debug(""Inference result: {}"".format(result))\n\n    if json_data.get(""postprocess"", ""false"") != ""false"":\n      if self.postprocess_function != None:\n        result = self.postprocess_function(result)\n        logger.debug(""Postprocess to generate data: {}"".format(result))\n      else:\n        logger.warning(""No postprocess function in model"")\n\n    return result\n'"
simple_tensorflow_serving/server.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \'.\'))\n\nimport json\nimport logging\nfrom functools import wraps\nfrom flask import Flask, Response, jsonify, render_template, request\nfrom flask_cors import CORS\nimport argparse\n\nfrom manager import InferenceServiceManager\nfrom service_utils import request_util\nimport python_predict_client\nfrom gen_client import gen_client\n\nlogger = logging.getLogger(""simple_tensorflow_serving"")\n\n# Define parameters\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\n    ""--host"",\n    default=os.environ.get(""STFS_HOST"", ""0.0.0.0""),\n    help=""The host of the server(eg. 0.0.0.0)"")\nparser.add_argument(\n    ""--port"",\n    default=int(os.environ.get(""STFS_PORT"", ""8500"")),\n    help=""The port of the server(eg. 8500)"",\n    type=int)\nparser.add_argument(\n    ""--enable_ssl"",\n    default=bool(os.environ.get(""STFS_ENABLE_SSL"", """")),\n    help=""If enable RESTfull API over https"")\nparser.add_argument(\n    ""--secret_pem"",\n    default=os.environ.get(""STFS_SECRET_PEM"", ""secret.pem""),\n    help=""SSL pem file"")\nparser.add_argument(\n    ""--secret_key"",\n    default=os.environ.get(""STFS_SECRET_KEY"", ""secret.key""),\n    help=""SSL key file"")\nparser.add_argument(\n    ""--model_name"",\n    default=os.environ.get(""STFS_MODEL_NAME"", ""default""),\n    help=""The name of the model(eg. default)"")\nparser.add_argument(\n    ""--model_base_path"",\n    default=os.environ.get(""STFS_MODEL_BASE_PATH"", ""./model""),\n    help=""The file path of the model(eg. 8500)"")\nparser.add_argument(\n    ""--model_platform"",\n    default=os.environ.get(""STFS_MODEL_PLATFORM"", ""tensorflow""),\n    help=""The platform of model(eg. tensorflow)"")\nparser.add_argument(\n    ""--model_config_file"",\n    default=os.environ.get(""STFS_MODEL_CONFIG_FILE"", """"),\n    help=""The file of the model config(eg. \'\')"")\n# TODO: type=bool not works, make it true by default if fixing exit bug\nparser.add_argument(\n    ""--reload_models"",\n    default=os.environ.get(""STFS_RELOAD_MODELS"", """"),\n    help=""Reload models or not(eg. True)"")\nparser.add_argument(\n    ""--custom_op_paths"",\n    default=os.environ.get(""STFS_CUSTOM_OP_PATHS"", """"),\n    help=""The path of custom op so files(eg. ./)"")\nparser.add_argument(\n    ""--session_config"",\n    default=os.environ.get(""STFS_SESSION_CONFIG"", ""{}""),\n    help=""The json of session config"")\nparser.add_argument(\n    ""--debug"",\n    default=os.environ.get(""STFS_DEBUG"", """"),\n    help=""Enable debug for flask or not(eg. False)"",\n    type=bool)\nparser.add_argument(\n    ""--log_level"",\n    default=os.environ.get(""STFS_LOG_LEVEL"", ""info""),\n    help=""The log level(eg. info)"")\nparser.add_argument(\n    ""--enable_auth"",\n    default=os.environ.get(""STFS_ENABLE_AUTH"", """"),\n    help=""Enable basic auth or not(eg. False)"",\n    type=bool)\nparser.add_argument(\n    ""--auth_username"",\n    default=os.environ.get(""STFS_AUTH_USERNAME"", ""admin""),\n    help=""The username of basic auth(eg. admin)"")\nparser.add_argument(\n    ""--auth_password"",\n    default=os.environ.get(""STFS_AUTH_PASSWORD"", ""admin""),\n    help=""The password of basic auth(eg. admin)"")\nparser.add_argument(\n    ""--enable_cors"",\n    default=os.environ.get(""STFS_ENABLE_CORS"", ""True""),\n    help=""Enable cors(eg. True)"",\n    type=bool)\nparser.add_argument(\n    ""--enable_b64_autoconvert"",\n    default=os.environ.get(""STFS_B64_AUTOCONVERT"", """"),\n    help=""Enable auto convert b64 string(eg. False)"",\n    type=bool)\nparser.add_argument(\n    ""--download_inference_images"",\n    default=os.environ.get(""STFS_DOWNLOAD_INFERENCE_IMAGES"", ""True""),\n    help=""Download inference images(eg. True)"",\n    type=bool)\n\n# Parse and check parameters\nargs = parser.parse_args(sys.argv[1:])\n\nfor arg in vars(args):\n  logger.info(""{}: {}"".format(arg, getattr(args, arg)))\n\nif args.log_level == ""info"" or args.log_level == ""INFO"":\n  logger.setLevel(logging.INFO)\nelif args.log_level == ""debug"" or args.log_level == ""DEBUG"":\n  logger.setLevel(logging.DEBUG)\nelif args.log_level == ""error"" or args.log_level == ""ERROR"":\n  logger.setLevel(logging.ERROR)\nelif args.log_level == ""warning"" or args.log_level == ""WARNING"":\n  logger.setLevel(logging.WARNING)\nelif args.log_level == ""critical"" or args.log_level == ""CRITICAL"":\n  logger.setLevel(logging.CRITICAL)\n\nif args.debug == True:\n  logger.setLevel(logging.DEBUG)\n\n\nclass WsgiApp:\n  """"""\n  The class has Flask app to run by Flask server or uwsgi server.\n  """"""\n\n  def __init__(self, args):\n    self.args = args\n    self.app = Flask(""simple_tensorlow_serving"", template_folder=\'templates\')\n    self.manager = InferenceServiceManager(args)\n\n    # Initialize Flask app with parameters\n    self.app.before_first_request(self.manager.init)\n    # TODO: Init the manager before first request\n    #self.manager.init()\n\n    if self.args.enable_cors:\n      CORS(self.app)\n\n    UPLOAD_FOLDER = os.path.basename(\'static\')\n    self.app.config[\'UPLOAD_FOLDER\'] = UPLOAD_FOLDER\n    if (os.path.isdir(UPLOAD_FOLDER)):\n      pass\n    else:\n      logging.warn(\n          ""Create path to host static files: {}"".format(UPLOAD_FOLDER))\n      os.mkdir(UPLOAD_FOLDER)\n\n    # The API to render the dashboard page\n    @self.app.route(""/"", methods=[""GET""])\n    @self.requires_auth\n    def index():\n      return render_template(\n          ""index.html"",\n          model_name_service_map=self.manager.model_name_service_map)\n\n    # The API to process inference request\n    @self.app.route(""/"", methods=[""POST""])\n    @self.requires_auth\n    def inference():\n      json_result, status_code = self.do_inference()\n      response = jsonify(json_result)\n      response.status_code = status_code\n      return response\n\n    @self.app.route(\'/health\', methods=[""GET""])\n    def health():\n      return Response(""healthy"")\n\n    @self.app.route(\'/image_inference\', methods=[""GET""])\n    def image_inference():\n      return render_template(\'image_inference.html\')\n\n    @self.app.route(\'/run_image_inference\', methods=[\'POST\'])\n    def run_image_inference():\n      predict_result = self.do_inference(\n          save_file_dir=self.app.config[\'UPLOAD_FOLDER\'])\n      json_result = json.dumps(predict_result)\n\n      file = request.files[\'image\']\n      image_file_path = os.path.join(app.config[\'UPLOAD_FOLDER\'],\n                                     file.filename)\n\n      return render_template(\n          \'image_inference.html\',\n          image_file_path=image_file_path,\n          predict_result=json_result)\n\n    @self.app.route(\'/json_inference\', methods=[""GET""])\n    def json_inference():\n      return render_template(\'json_inference.html\')\n\n    @self.app.route(\'/run_json_inference\', methods=[\'POST\'])\n    def run_json_inference():\n      # TODO: Fail to parse u\'{\\r\\n  ""inputs"": [\\\'\\\\n\\\\x1f\\\\n\\\\x0e\\\\n\\\\x01a\\\\x12\\\\t\\\\n\\\\x07\\\\n\\\\x05hello\\\\n\\\\r\\\\n\\\\x01b\\\\x12\\\\x08\\\\x12\\\\x06\\\\n\\\\x04\\\\x00\\\\x00\\\\x00?\\\']\\r\\n}\\r\\n          \'\n      # {\n      # ""inputs"": [\'\\n\\x1f\\n\\x0e\\n\\x01a\\x12\\t\\n\\x07\\n\\x05hello\\n\\r\\n\\x01b\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00\\x00?\']\n      #}\n      json_data_string = request.form[""json_data""]\n      json_data = json.loads(json_data_string)\n      model_name = request.form[""model_name""]\n      model_version = request.form[""model_version""]\n      signature_name = request.form[""signature_name""]\n\n      request_json_data = {\n          ""model_name"": model_name,\n          ""model_version"": model_version,\n          ""signature_name"": signature_name,\n          ""data"": json_data\n      }\n\n      predict_result = python_predict_client.predict_json(\n          request_json_data, port=args.port)\n\n      return render_template(\n          \'json_inference.html\', predict_result=predict_result)\n\n    # The API to get all models\n    @self.app.route(""/v1/models"", methods=[""GET""])\n    @self.requires_auth\n    def get_models():\n      result = [\n          inference_service.get_detail() for inference_service in self.manager.\n          model_name_service_map.values()\n      ]\n      return json.dumps(result)\n\n    # The API to get default of the model\n    @self.app.route(""/v1/models/<model_name>"", methods=[""GET""])\n    @self.requires_auth\n    def get_model_detail(model_name):\n\n      if model_name not in self.manager.model_name_service_map:\n        return ""Model not found: {}"".format(model_name)\n\n      inference_service = self.manager.model_name_service_map[model_name]\n      return json.dumps(inference_service.get_detail())\n\n    # The API to get example json for client\n    @self.app.route(""/v1/models/<model_name>/gen_json"", methods=[""GET""])\n    @self.requires_auth\n    def gen_example_json(model_name):\n      inference_service = self.manager.model_name_service_map[model_name]\n      data_json_dict = gen_client.gen_tensorflow_client(\n          inference_service, ""json"", model_name)\n      return json.dumps(data_json_dict)\n\n    # The API to get example json for client\n    @self.app.route(""/v1/models/<model_name>/gen_client"", methods=[""GET""])\n    @self.requires_auth\n    def gen_example_client(model_name):\n      client_type = request.args.get(""language"", default=""bash"", type=str)\n      inference_service = self.manager.model_name_service_map[model_name]\n      example_client_string = gen_client.gen_tensorflow_client(\n          inference_service, client_type, model_name)\n\n      return example_client_string\n\n    @self.app.route(""/generate_clients"", methods=[""GET""])\n    def generate_clients():\n      return render_template(\'generate_clients.html\')\n\n    @self.app.route(""/run_generate_clients"", methods=[\'POST\'])\n    def run_generate_clients():\n      model_name = request.form[""model_name""]\n      signature_name = request.form[""signature_name""]\n      language = request.form[""language""]\n\n      result = python_predict_client.get_gen_json_and_clients(\n          model_name, signature_name, language, port=args.port)\n\n      return render_template(""generate_clients.html"", result=result)\n\n  def do_inference(self):\n    # 1. Check request data format\n    if request.content_type.startswith(""application/json""):\n      # Process requests with json data\n      try:\n        json_data = request.json\n        if not isinstance(json_data, dict):\n          result = {""error"": ""Invalid json data: {}"".format(request.data)}\n          return result, 400\n      except Exception as e:\n        result = {""error"": ""Invalid json data: {}"".format(request.data)}\n        return result, 400\n\n    elif request.content_type.startswith(""multipart/form-data""):\n      # Process requests with raw image\n      try:\n        json_data = request_util.create_json_from_formdata_request(\n            request,\n            self.args.download_inference_images,\n            save_file_dir=self.app.config[\'UPLOAD_FOLDER\'])\n      except Exception as e:\n        result = {""error"": ""Invalid form-data: {}"".format(e)}\n        return result, 400\n\n    else:\n      logging.error(""Unsupported content type: {}"".format(\n          request.content_type))\n      return {""error"": ""Error, unsupported content type""}, 400\n\n    # 2. Get model or use default one\n    model_name = ""default""\n    if ""model_name"" in json_data:\n      model_name = json_data.get(""model_name"")\n\n    if model_name not in self.manager.model_name_service_map:\n      return {\n          ""error"":\n          ""Invalid model name: {}, available models: {}"".format(\n              model_name, self.manager.model_name_service_map.keys())\n      }, 400\n\n    # 3. Use initialized manager for inference\n    try:\n      result = self.manager.inference(model_name, json_data)\n      return result, 200\n    except Exception as e:\n      result = {""error"": e.message}\n      return result, 400\n\n  def verify_authentication(self, username, password):\n    """"""\n      Verify if this user should be authenticated or not.\n    \n      Args:\n        username: The user name.\n        password: The password.\n    \n      Return:\n        True if it passes and False if it does not pass.\n      """"""\n    if self.args.enable_auth:\n      if username == self.args.auth_username and password == self.args.auth_password:\n        return True\n      else:\n        return False\n    else:\n      return True\n\n  def requires_auth(self, f):\n    """"""\n      The decorator to enable basic auth.\n      """"""\n\n    @wraps(f)\n    def decorated(*decorator_args, **decorator_kwargs):\n\n      auth = request.authorization\n\n      if self.args.enable_auth:\n        if not auth or not self.verify_authentication(auth.username,\n                                                      auth.password):\n          response = Response(\n              ""Need basic auth to request the resources"", 401,\n              {""WWW-Authenticate"": \'""Basic realm=""Login Required""\'})\n          return response\n\n      return f(*decorator_args, **decorator_kwargs)\n\n    return decorated\n\n\ndef main():\n  app = WsgiApp(args).app\n\n  # Run with Flask HTTP server\n  if args.enable_ssl:\n    # Can pass ssl_context=""adhoc"" for auto-sign certification\n    app.run(\n        host=args.host,\n        port=args.port,\n        threaded=True,\n        debug=args.debug,\n        ssl_context=(args.secret_pem, args.secret_key))\n  else:\n    # TODO: Use single thread by default\n    app.run(host=args.host, port=args.port, threaded=False, debug=args.debug)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
simple_tensorflow_serving/spark_inference_service.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport logging\nimport time\nimport subprocess\nimport requests\n\nfrom abstract_inference_service import AbstractInferenceService\nimport filesystem_util\nimport preprocess_util\n\nlogger = logging.getLogger(""simple_tensorflow_serving"")\n\n\nclass SparkInferenceService(AbstractInferenceService):\n  """"""\n  The service to load Spark MLlib model and make inference.\n  """"""\n\n  def __init__(self, model_name, model_base_path):\n    """"""\n    Initialize the service.\n        \n    Args:\n      model_name: The name of the model.\n      model_base_path: The file path of the model.\n    Return:\n      None\n    """"""\n\n    super(SparkInferenceService, self).__init__()\n\n    # TODO: Download the model files\n    #local_model_base_path = filesystem_util.download_hdfs_moels(\n    #    model_base_path)\n\n    self.model_name = model_name\n    self.model_base_path = model_base_path\n    self.model_version_list = [1]\n    self.model_graph_signature = """"\n    self.platform = ""Spark""\n\n    self.preprocess_function, self.postprocess_function = preprocess_util.get_preprocess_postprocess_function_from_model_path(\n        self.model_base_path)\n\n    # Load model\n    from pyspark.sql import SparkSession\n    from pyspark.ml.classification import LogisticRegressionModel\n\n    self.spark_session = SparkSession.builder.appName(""libsvm_lr"").getOrCreate()\n    # TODO: Support other model\n    self.spark_model = LogisticRegressionModel.load(self.model_base_path)\n\n    # TODO: Add signature for Spark model\n    self.model_graph_signature = ""No signature for Spark MLlib models""\n\n  def inference(self, json_data):\n    """"""\n    Make inference with the current Session object and JSON request data.\n        \n    Args:\n      json_data: The JSON serialized object with key and array data.\n                 Example is {""model_version"": 1, ""data"": {""keys"": [[1.0], [2.0]], ""features"": [[10, 10, 10, 8, 6, 1, 8, 9, 1], [6, 2, 1, 1, 1, 1, 7, 1, 1]]}}.\n    Return:\n      The dictionary with key and array data.\n      Example is {""keys"": [[11], [2]], ""softmax"": [[0.61554497, 0.38445505], [0.61554497, 0.38445505]], ""prediction"": [0, 0]}.\n    """"""\n\n    # 1. Build inference data\n    # Example: arguments = {""Sepal_Length"" : 5.1, ""Sepal_Width"" : 3.5, ""Petal_Length"" : 1.4, ""Petal_Width"" : 0.2}\n    request_json_data = json_data[""data""]\n\n    if json_data.get(""preprocess"", ""false"") != ""false"":\n      if self.preprocess_function != None:\n        request_json_data = self.preprocess_function(request_json_data)\n        logger.debug(\n            ""Preprocess to generate data: {}"".format(request_json_data))\n      else:\n        logger.warning(""No preprocess function in model"")\n\n    # 2. Do inference\n    start_time = time.time()\n    # Example: {\'Probability_setosa\': 1.0, \'Probability_versicolor\': 0.0, \'Node_Id\': \'2\', \'Species\': \'setosa\', \'Probability_virginica\': 0.0}\n    #predict_result = self.openscoring.evaluate(self.model_name, request_json_data)\n\n    # Construct data\n    from pyspark.ml.linalg import SparseVector\n\n    # TODO: Support other data format\n    if request_json_data[""format""] == ""libsvm"":\n      # Example: self.spark_session.createDataFrame([(1.0, SparseVector(692, [128, 129, 130], [51, 159, 20]))], [\'label\', \'features\'])\n      # TODO: Support Spark dataframe schema from model signature\n      testset = self.spark_session.createDataFrame([(1.0, SparseVector(request_json_data[""max_ids""], request_json_data[""ids""], request_json_data[""values""]))], [\'label\', \'features\'])\n\n    # Make inference\n    result = self.spark_model.transform(testset)\n    result = result.first()\n    #print(""Prediction: {}, probability_of_0: {}, probability_of_1: {}"".format(result.label, result.probability[0], result.probability[1]))\n    predict_result = {""prediction"": result.label, ""probability_of_0"": result.probability[0], ""probability_of_1"": result.probability[1]}\n\n\n    logger.debug(""Inference time: {} s"".format(time.time() - start_time))\n    logger.debug(""Inference result: {}"".format(predict_result))\n\n    if json_data.get(""postprocess"", ""false"") != ""false"":\n      if self.postprocess_function != None:\n        predict_result = self.postprocess_function(predict_result)\n        logger.debug(""Postprocess to generate data: {}"".format(predict_result))\n      else:\n        logger.warning(""No postprocess function in model"")\n\n    return predict_result\n'"
simple_tensorflow_serving/tensorflow_inference_service.py,26,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport base64\nimport logging\nimport os\nimport signal\nimport threading\n\nimport time\nimport tensorflow as tf\nimport marshal\nimport types\n\nfrom abstract_inference_service import AbstractInferenceService\nimport filesystem_util\n\nlogger = logging.getLogger(\'simple_tensorflow_serving\')\n\n\nclass TensorFlowInferenceService(AbstractInferenceService):\n  """"""\n  The TensorFlow service to load TensorFlow SavedModel and make inference.\n  """"""\n\n  def __init__(self,\n               model_name,\n               model_base_path,\n               custom_op_paths="""",\n               session_config={}):\n    """"""\n    Initialize the TensorFlow service by loading SavedModel to the Session.\n        \n    Args:\n      model_name: The name of the model.\n      model_base_path: The file path of the model.\n    Return:\n      None\n    """"""\n\n    super(TensorFlowInferenceService, self).__init__()\n\n    self.model_name = model_name\n    libhdfs_model_base_path = filesystem_util.update_hdfs_prefix_for_libhdfs(\n        model_base_path)\n    self.model_base_path = libhdfs_model_base_path\n    self.model_version_list = []\n    self.model_graph_signature = None\n    self.model_graph_signature_dict = {}\n\n    self.signature_input_tensor_names_map = {}\n    self.signature_input_op_names_map = {}\n    self.signature_output_tensor_names_map = {}\n    self.signature_output_op_names_map = {}\n\n    self.platform = ""TensorFlow""\n    self.session_config = session_config\n\n    self.name_signature_map = {}\n    self.preprocess_function = None\n    self.postprocess_function = None\n\n    if self.model_base_path == """":\n      raise Exception(""The model base path is empty"")\n    if self.model_base_path.startswith(""/"") or self.model_base_path.startswith(\n        ""./""):\n      if not os.path.exists(self.model_base_path):\n        raise Exception(""Local model path does not exist: {}"".format(\n            self.model_base_path))\n\n    if custom_op_paths != """":\n      self.load_custom_op(custom_op_paths)\n\n    self.version_session_map = {}\n    self.profiler_map = {}\n\n    self.should_stop_all_threads = False\n\n    # Register the signals to exist\n    signal.signal(signal.SIGTERM, self.stop_all_threads)\n    signal.signal(signal.SIGINT, self.stop_all_threads)\n\n    model_versions = self.get_all_model_versions()\n    for model_version in model_versions:\n      self.load_saved_model_version(model_version)\n\n    self.init_model_signature()\n\n  def load_custom_op(self, custom_op_paths):\n\n    custom_op_path_list = custom_op_paths.split("","")\n\n    for custom_op_path in custom_op_path_list:\n      if os.path.isdir(custom_op_path):\n        for filename in os.listdir(custom_op_path):\n          if filename.endswith("".so""):\n\n            op_filepath = os.path.join(custom_op_path, filename)\n            logger.info(""Load the so file from: {}"".format(op_filepath))\n            tf.load_op_library(op_filepath)\n\n      else:\n        logger.error(""The path does not exist: {}"".format(custom_op_path))\n\n  def dynamically_reload_models(self):\n    """"""\n    Start new thread to load models periodically.\n\n    Return:\n      None\n    """"""\n\n    logger.info(""Start the new thread to periodically reload model versions"")\n    load_savedmodels_thread = threading.Thread(\n        target=self.load_savedmodels_thread, args=())\n    load_savedmodels_thread.start()\n    # dynamically_load_savedmodels_thread.join()\n\n  def stop_all_threads(self, signum, frame):\n    logger.info(""Catch signal {} and exit all threads"".format(signum))\n    self.should_stop_all_threads = True\n    exit(0)\n\n  def load_savedmodels_thread(self):\n    """"""\n    Load the SavedModel, update the Session object and return the Graph object.\n\n    Return:\n      None\n    """"""\n\n    while self.should_stop_all_threads == False:\n      # TODO: Add lock if needed\n      # TODO: Support HDFS with TensorFlow API\n      current_model_versions_string = os.listdir(self.model_base_path)\n      current_model_versions = set(\n          [version_string for version_string in current_model_versions_string])\n\n      old_model_versions_string = self.version_session_map.keys()\n      old_model_versions = set(\n          [version_string for version_string in old_model_versions_string])\n\n      if current_model_versions == old_model_versions:\n        # No version change, just sleep\n        logger.debug(""Watch the model path: {} and sleep {} seconds"".format(\n            self.model_base_path, 10))\n        time.sleep(10)\n\n      else:\n        # Versions change, load the new models and offline the deprecated ones\n        logger.info(\n            ""Model path updated, change model versions from: {} to: {}"".format(\n                old_model_versions, current_model_versions))\n\n        # Put old model versions offline\n        offline_model_versions = old_model_versions - current_model_versions\n        for model_version in offline_model_versions:\n          logger.info(""Put the model version: {} offline"".format(\n              str(model_version)))\n          del self.version_session_map[str(model_version)]\n          self.version_session_map.remove(model_version)\n\n      # Create Session for new model version\n        online_model_versions = current_model_versions - old_model_versions\n        for model_version in online_model_versions:\n          self.load_saved_model_version(model_version)\n\n  def load_saved_model_version(self, model_version):\n\n    gpu_options = tf.GPUOptions(allow_growth=True)\n\n    if tf.__version__.startswith(""1""):\n      config = tf.ConfigProto(gpu_options=gpu_options)\n    else:\n      config = tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n\n    if ""log_device_placement"" in self.session_config:\n      config.log_device_placement = self.session_config[""log_device_placement""]\n    if ""allow_soft_placement"" in self.session_config:\n      config.allow_soft_placement = self.session_config[""allow_soft_placement""]\n    if ""allow_growth"" in self.session_config:\n      config.gpu_options.allow_growth = self.session_config[""allow_growth""]\n    if ""per_process_gpu_memory_fraction"" in self.session_config:\n      config.gpu_options.per_process_gpu_memory_fraction = self.session_config[\n          ""per_process_gpu_memory_fraction""]\n\n    if tf.__version__.startswith(""1""):\n      session = tf.Session(graph=tf.Graph(), config=config)\n    else:\n      session = tf.compat.v1.Session(graph=tf.Graph(), config=config)\n\n    self.version_session_map[str(model_version)] = session\n    self.model_version_list.append(model_version)\n\n    model_file_path = os.path.join(self.model_base_path, str(model_version))\n    logger.info(""Put the model version: {} online, path: {}"".format(\n        model_version, model_file_path))\n\n    if tf.__version__.startswith(""1""):\n      meta_graph = tf.saved_model.loader.load(\n          session, [tf.saved_model.tag_constants.SERVING], model_file_path)\n    else:\n      meta_graph = tf.compat.v1.saved_model.loader.load(\n          session, [tf.compat.v1.saved_model.tag_constants.SERVING],\n          model_file_path)\n\n    # Get preprocess and postprocess function from collection_def\n    if ""preprocess_function"" in meta_graph.collection_def:\n      logging.info(""Load the preprocess function in graph"")\n      preprocess_function_string = meta_graph.collection_def[\n          ""preprocess_function""].bytes_list.value[0]\n      loaded_function = marshal.loads(preprocess_function_string)\n      self.preprocess_function = types.FunctionType(loaded_function, globals(),\n                                                    ""preprocess_function"")\n\n    if ""postprocess_function"" in meta_graph.collection_def:\n      logging.info(""Load the postprocess function in graph"")\n      postrocess_function_string = meta_graph.collection_def[\n          ""postprocess_function""].bytes_list.value[0]\n      loaded_function = marshal.loads(postrocess_function_string)\n      self.postprocess_function = types.FunctionType(\n          loaded_function, globals(), ""postprocess_function"")\n\n  def init_model_signature(self):\n\n    latest_model_version = self.model_version_list[-1]\n    sess = self.version_session_map[str(latest_model_version)]\n\n    model_file_path = os.path.join(self.model_base_path,\n                                   str(latest_model_version))\n\n    if tf.__version__.startswith(""1""):\n      meta_graph = tf.saved_model.loader.load(\n          sess, [tf.saved_model.tag_constants.SERVING], model_file_path)\n    else:\n      meta_graph = tf.compat.v1.saved_model.loader.load(\n          sess, [tf.compat.v1.saved_model.tag_constants.SERVING],\n          model_file_path)\n\n    # Update ItemsView to list for Python 3\n    signature_items = list(meta_graph.signature_def.items())\n    items_num = len(signature_items)\n\n    for i in range(items_num):\n      item = signature_items[i]\n      signature_name = item[0]\n      self.name_signature_map[signature_name] = item[1]\n\n      # tf.python.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n      #if signature_name == ""serving_default"":\n      if signature_name == ""serving_default"" or signature_name == ""predict"":\n        self.model_graph_signature = item[1]\n        self.model_graph_signature_dict = tensorflow_model_graph_to_dict(\n            self.model_graph_signature)\n      elif self.model_graph_signature == None and i == (items_num - 1):\n        self.model_graph_signature = item[1]\n        self.model_graph_signature_dict = tensorflow_model_graph_to_dict(\n            self.model_graph_signature)\n\n      input_tensor_names, input_op_names = get_input_tensor_names_by_signature(\n          self.model_graph_signature)\n      output_tensor_names, output_op_names = get_output_tensor_names_by_signature(\n          self.model_graph_signature)\n\n      self.signature_input_tensor_names_map[\n          signature_name] = input_tensor_names\n      self.signature_input_op_names_map[signature_name] = input_op_names\n      self.signature_output_tensor_names_map[\n          signature_name] = output_tensor_names\n      self.signature_output_op_names_map[signature_name] = output_op_names\n\n  def get_one_model_version(self):\n    all_model_versions = self.get_all_model_versions()\n    # current_model_versions_string = os.listdir(self.model_base_path)\n\n    if len(all_model_versions) > 0:\n      return all_model_versions[0]\n    else:\n      logger.error(""No model version found"")\n\n  def get_all_model_versions(self):\n    # Be compatible for TensorFlow 1.x and 2.x\n    if tf.__version__.startswith(""1""):\n      model_versions = tf.gfile.ListDirectory(self.model_base_path)\n    else:\n      model_versions = tf.io.gfile.listdir(self.model_base_path)\n    return model_versions\n\n  def run_with_profiler(self, session, version, output_tensors, feed_dict):\n    if version not in self.profiler_map:\n      if len(self.profiler_map) > 0:\n        logger.warn(\n            ""Only support one profiler per process, run without profiler"")\n        return session.run(output_tensors, feed_dict), None\n      profiler = tf.profiler.Profiler(session.graph)\n      self.profiler_map[version] = profiler\n    else:\n      profiler = self.profiler_map[version]\n    run_meta = tf.RunMetadata()\n    result = session.run(\n        output_tensors,\n        feed_dict=feed_dict,\n        options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),\n        run_metadata=run_meta)\n    profiler.add_step(0, run_meta)\n    profiler_out_file = ""/tmp/.simple_tensorflow_serving_prof-"" + str(\n        int(time.time()))\n    opts = tf.profiler.ProfileOptionBuilder.time_and_memory()\n    opts[""output""] = ""file:outfile=%s"" % profiler_out_file\n    profiler.profile_operations(options=opts)\n    profile_result = None\n    try:\n      with open(profiler_out_file) as f:\n        profile_result = f.read()\n    except Exception as e:\n      logger.error(e.message)\n    return result, profile_result\n\n  def inference(self, json_data):\n    """"""\n    Make inference with the current Session object and JSON request data.\n        \n    Args:\n      json_data: The JSON serialized object with key and array data.\n                 Example is {""model_version"": 1, ""data"": {""keys"": [[1.0], [2.0]], ""features"": [[10, 10, 10, 8, 6, 1, 8, 9, 1], [6, 2, 1, 1, 1, 1, 7, 1, 1]]}}.\n    Return:\n      The dictionary with key and array data.\n      Example is {""keys"": [[11], [2]], ""softmax"": [[0.61554497, 0.38445505], [0.61554497, 0.38445505]], ""prediction"": [0, 0]}.\n    """"""\n\n    if ""model_version"" in json_data:\n      model_version = json_data.get(""model_version"")\n    else:\n      # Use the latest model version if not specified\n      if len(self.version_session_map) > 0:\n        # TODO: Make sure it use the latest model version\n        model_version = list(self.version_session_map.keys())[-1]\n      else:\n        raise Exception(\n            ""No model version found, please check the TensorFlow model files"")\n\n    if ""data"" not in json_data:\n      raise Exception(\n          ""Inference with empty data, please check the request JSON"")\n    input_data = json_data.get(""data"")\n    logger.debug(""Inference with json data: {}"".format(json_data))\n\n    if ""preprocess"" in json_data:\n      if self.preprocess_function != None:\n        input_data = self.preprocess_function(input_data)\n        logger.debug(""Preprocess to generate data: {}"".format(input_data))\n      else:\n        logger.warning(""No preprocess function in model"")\n\n    if ""signature_name"" in json_data:\n      signature_name = json_data.get(""signature_name"")\n      if signature_name not in self.name_signature_map:\n        raise Exception(\n            ""Fail to request the signature name: {}, please check the request JSON""\n            .format(signature_name))\n    else:\n      signature_name = ""serving_default""\n\n    # 1. Build feed dict for input data\n    feed_dict_map = {}\n\n    input_op_names = self.signature_input_op_names_map[signature_name]\n    input_tensor_names = self.signature_input_tensor_names_map[signature_name]\n\n    for i in range(len(input_op_names)):\n      input_op_name = input_op_names[i]\n      input_tensor_name = input_tensor_names[i]\n\n      # Example: {""Placeholder_0"": [[1.0], [2.0]], ""Placeholder_1:0"": [[10, 10, 10, 8, 6, 1, 8, 9, 1], [6, 2, 1, 1, 1, 1, 7, 1, 1]]}\n      if input_op_name not in input_data:\n        raise Exception(\n            ""Input op name \'{}\' does not exist in input data: {}, please check the request JSON""\n            .format(input_op_name, input_data))\n      feed_dict_map[input_tensor_name] = input_data[input_op_name]\n\n    # 2. Build inference operators\n    output_op_names = self.signature_output_op_names_map[signature_name]\n    output_tensor_names = self.signature_output_tensor_names_map[\n        signature_name]\n\n    # 3. Inference with Session run\n    sess = self.version_session_map[str(model_version)]\n\n    if ""run_profile"" in json_data:\n      if json_data.get(""run_profile"") == ""true"":\n        logger.info(""run_profile=true, running with tfprof"")\n        result_ndarrays, result_profile = self.run_with_profiler(\n            sess, str(model_version), output_tensor_names, feed_dict_map)\n    else:\n      # Update input data by decoding base64 string for esitmator model\n      should_decode_base64 = json_data.get(""base64_decode"", False)\n      if should_decode_base64 and ""input_example_tensor:0"" in feed_dict_map:\n        final_example_strings = []\n        base64_example_strings = feed_dict_map[""input_example_tensor:0""]\n        for base64_example_string in base64_example_strings:\n          final_example_string = base64.urlsafe_b64decode(\n              base64_example_string.encode(""utf-8""))\n          final_example_strings.append(final_example_string)\n        feed_dict_map[""input_example_tensor:0""] = final_example_strings\n\n      try:\n        start_time = time.time()\n        result_ndarrays = sess.run(\n            output_tensor_names, feed_dict=feed_dict_map)\n        logger.debug(""Inference time: {} s"".format(time.time() - start_time))\n      except Exception as e:\n        logging.warn(\n            ""Fail to run with output_tensor_names: {}, feed_dict_map: {}"".\n            format(output_tensor_names, feed_dict_map))\n        raise Exception(\n            ""Sess.run() fail because of {}, please check shape of input"".\n            format(e.message))\n\n    # 4. Build return result\n    result = {}\n    for i in range(len(output_op_names)):\n      result[output_op_names[i]] = result_ndarrays[i].tolist()\n    logger.debug(""Inference result: {}"".format(result))\n\n    if ""postprocess"" in json_data:\n      if self.postprocess_function != None:\n        result = self.postprocess_function(result)\n        logger.debug(""Postprocess to generate data: {}"".format(result))\n      else:\n        logger.warning(""No postprocess function in model"")\n\n    # 5. Build extra return information\n    if ""run_profile"" in json_data:\n      if result_profile is not None and ""__PROFILE__"" not in output_tensor_names:\n        result[""__PROFILE__""] = result_profile\n\n    return result\n\n\ndef tensorflow_model_graph_to_dict(model_graph_signature):\n  model_graph_signature_dict = {}\n  model_graph_signature_dict[""inputs""] = []\n  model_graph_signature_dict[""outputs""] = []\n\n  for input_item in model_graph_signature.inputs.items():\n    # Example: {""name: ""keys"", ""dtype"": 1(DT_INT32), ""shape"": [-1, 1]}\n    input_map = {""name"": """", ""dtype"": 0, ""shape"": []}\n\n    # Example: ""keys""\n    input_opname = input_item[0]\n    input_map[""name""] = input_opname\n\n    dtype = input_item[1].dtype\n    input_map[""dtype""] = dtype\n\n    # Example: [-1, 1]\n    shape_dims = input_item[1].tensor_shape.dim\n\n    for dim in shape_dims:\n      input_map[""shape""].append(int(dim.size))\n\n    model_graph_signature_dict[""inputs""].append(input_map)\n\n  for output_item in model_graph_signature.outputs.items():\n\n    if output_item[1].name != """":\n      # Example: {""name: ""keys"", ""dtype"": 1(DT_INT32), ""shape"": [-1, 1]}\n      output_map = {""name"": """", ""dtype"": 0, ""shape"": []}\n\n      # Example: ""keys""\n      output_op_name = output_item[0]\n      output_map[""name""] = output_op_name\n\n      dtype = output_item[1].dtype\n      output_map[""dtype""] = dtype\n\n      # Example: [-1, 1]\n      shape_dims = output_item[1].tensor_shape.dim\n\n      for dim in shape_dims:\n        output_map[""shape""].append(int(dim.size))\n\n      model_graph_signature_dict[""outputs""].append(output_map)\n\n    elif output_item[1].coo_sparse != None:\n      # For SparseTensor op, Example: values_tensor_name: ""CTCBeamSearchDecoder_1:1"", indices_tensor_name: ""CTCBeamSearchDecoder_1:0"", dense_shape_tensor_name: ""CTCBeamSearchDecoder_1:2""\n      output_map1 = {""name"": """", ""dtype"": 0, ""shape"": []}\n      output_map2 = {""name"": """", ""dtype"": 0, ""shape"": []}\n      output_map3 = {""name"": """", ""dtype"": 0, ""shape"": []}\n\n      #values_tensor_name = output_item[1].coo_sparse.values_tensor_name\n      #indices_tensor_name = output_item[1].coo_sparse.indices_tensor_name\n      #dense_shape_tensor_name = output_item[1].coo_sparse.dense_shape_tensor_name\n\n      values_op_name = ""{}_{}"".format(output_item[0], ""values"")\n      indices_op_name = ""{}_{}"".format(output_item[0], ""indices"")\n      shape_op_name = ""{}_{}"".format(output_item[0], ""shape"")\n      output_map1[""name""] = values_op_name\n      output_map2[""name""] = indices_op_name\n      output_map3[""name""] = shape_op_name\n\n      # TODO: Add dtype and shape for sparse model\n      model_graph_signature_dict[""outputs""].append(output_map1)\n      model_graph_signature_dict[""outputs""].append(output_map2)\n      model_graph_signature_dict[""outputs""].append(output_map3)\n\n  return model_graph_signature_dict\n\n\ndef get_input_tensor_names_by_signature(model_graph_signature):\n  """"""\n  Get the input tensor/op names by the model signature.\n  """"""\n  input_op_names = []\n  input_tensor_names = []\n\n  for input_item in model_graph_signature.inputs.items():\n    # Example: ""keys""\n    input_op_name = input_item[0]\n    input_op_names.append(input_op_name)\n\n    # Example: ""Placeholder_0""\n    input_tensor_name = input_item[1].name\n    input_tensor_names.append(input_tensor_name)\n\n  return input_tensor_names, input_op_names\n\n\ndef get_output_tensor_names_by_signature(model_graph_signature):\n  """"""\n  Get the output tensor/op names by the model signature.\n  """"""\n\n  output_tensor_names = []\n  output_op_names = []\n\n  for output_item in model_graph_signature.outputs.items():\n    if output_item[1].name != """":\n      # Example: ""keys""\n      output_op_name = output_item[0]\n      output_op_names.append(output_op_name)\n      # Example: ""Identity:0""\n      output_tensor_name = output_item[1].name\n      output_tensor_names.append(output_tensor_name)\n    elif output_item[1].coo_sparse != None:\n      # For SparseTensor op, Example: values_tensor_name: ""CTCBeamSearchDecoder_1:1"", indices_tensor_name: ""CTCBeamSearchDecoder_1:0"", dense_shape_tensor_name: ""CTCBeamSearchDecoder_1:2""\n      values_tensor_name = output_item[1].coo_sparse.values_tensor_name\n      indices_tensor_name = output_item[1].coo_sparse.indices_tensor_name\n      dense_shape_tensor_name = output_item[\n          1].coo_sparse.dense_shape_tensor_name\n      output_op_names.append(""{}_{}"".format(output_item[0], ""values""))\n      output_op_names.append(""{}_{}"".format(output_item[0], ""indices""))\n      output_op_names.append(""{}_{}"".format(output_item[0], ""shape""))\n      output_tensor_names.append(values_tensor_name)\n      output_tensor_names.append(indices_tensor_name)\n      output_tensor_names.append(dense_shape_tensor_name)\n\n  return output_tensor_names, output_op_names\n'"
simple_tensorflow_serving/xgboost_inference_service.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport time\nimport numpy as np\nimport pickle\n\nfrom abstract_inference_service import AbstractInferenceService\nimport filesystem_util\nimport preprocess_util\n\nlogger = logging.getLogger(""simple_tensorflow_serving"")\n\n\nclass XgboostInferenceService(AbstractInferenceService):\n  """"""\n  The service to load XGBoost model and make inference.\n  """"""\n\n  def __init__(self, model_name, model_base_path):\n    """"""\n    Initialize the service.\n        \n    Args:\n      model_name: The name of the model.\n      model_base_path: The file path of the model.\n    Return:\n      None\n    """"""\n\n    super(XgboostInferenceService, self).__init__()\n\n    local_model_base_path = filesystem_util.download_hdfs_moels(\n        model_base_path)\n\n    self.model_name = model_name\n    self.model_base_path = local_model_base_path\n    self.model_version_list = [1]\n    self.model_graph_signature = """"\n    self.platform = ""XGBoost""\n\n    self.preprocess_function, self.postprocess_function = preprocess_util.get_preprocess_postprocess_function_from_model_path(\n        self.model_base_path)\n\n    # TODO: Import as needed and only once\n    import xgboost as xgb\n    from sklearn.externals import joblib\n\n    self.bst = xgb.Booster()\n\n    # Load model\n    if self.model_base_path.endswith("".joblib""):\n      self.bst = joblib.load(self.model_base_path)\n    elif self.model_base_path.endswith(\n        "".pkl"") or self.model_base_path.endswith("".pickle""):\n      with open(self.model_base_path, \'rb\') as f:\n        self.bst = pickle.load(f)\n    elif self.model_base_path.endswith(\n        "".bst"") or self.model_base_path.endswith("".bin""):\n      self.bst.load_model(self.model_base_path)\n    else:\n      logger.error(\n          ""Unsupported model file format: {}"".format(self.model_base_path))\n\n    self.model_graph_signature = ""score: {}\\nfscore: {}"".format(\n        self.bst.get_score(), self.bst.get_fscore())\n\n  def inference(self, json_data):\n    """"""\n    Make inference with the current Session object and JSON request data.\n        \n    Args:\n      json_data: The JSON serialized object with key and array data.\n                 Example is {""model_version"": 1, ""data"": {""keys"": [[1.0], [2.0]], ""features"": [[10, 10, 10, 8, 6, 1, 8, 9, 1], [6, 2, 1, 1, 1, 1, 7, 1, 1]]}}.\n    Return:\n      The dictionary with key and array data.\n      Example is {""keys"": [[11], [2]], ""softmax"": [[0.61554497, 0.38445505], [0.61554497, 0.38445505]], ""prediction"": [0, 0]}.\n    """"""\n\n    # 1. Build inference data\n    import xgboost as xgb\n\n    input_data = json_data[""data""]\n    if json_data.get(""preprocess"", ""false"") != ""false"":\n      if self.preprocess_function != None:\n        input_data = self.preprocess_function(input_data)\n        logger.debug(""Preprocess to generate data: {}"".format(input_data))\n      else:\n        logger.warning(""No preprocess function in model"")\n\n    request_ndarray_data = xgb.DMatrix(np.array(input_data))\n\n    # 2. Do inference\n    start_time = time.time()\n    predict_result = self.bst.predict(request_ndarray_data)\n    logger.debug(""Inference time: {} s"".format(time.time() - start_time))\n\n    if json_data.get(""postprocess"", ""false"") != ""false"":\n      if self.postprocess_function != None:\n        predict_result = self.postprocess_function(predict_result)\n        logger.debug(""Postprocess to generate data: {}"".format(predict_result))\n      else:\n        logger.warning(""No postprocess function in model"")\n\n    # 3. Build return data\n    result = {\n        ""result"": predict_result,\n    }\n    logger.debug(""Inference result: {}"".format(result))\n\n    return result\n'"
tools/check_model.py,3,"b'#!/usr/bin/env python\n\nimport tensorflow as tf\n\nimport logging\n\nlogging.basicConfig(level=logging.DEBUG)\n\n\ndef check_saved_model(model_file_path):\n  logging.info(""Try to load the model in: {}"".format(model_file_path))\n\n  try:\n    session = tf.Session(graph=tf.Graph())\n    meta_graph = tf.saved_model.loader.load(\n        session, [tf.saved_model.tag_constants.SERVING], model_file_path)\n    logging.info(""Succeed to load model in: {}"".format(model_file_path))\n  except IOError as ioe:\n    logging.info(""Fail to load model and catch exception: {}"".format(ioe))\n\n\ndef main():\n  model_file_path = ""../models/tensorflow_template_application_model/1""\n  check_saved_model(model_file_path)\n\n  model_file_path = ""../models/tensorflow_template_application_model/100""\n  check_saved_model(model_file_path)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
tools/local_inference.py,3,"b'#!/usr/bin/env python\n\nimport os\nimport time\nimport tensorflow as tf\nimport logging\n\nlogging.basicConfig(level=logging.DEBUG)\n\n\nclass LocalInferenceService(object):\n  def __init__(self, model_base_path, model_version):\n\n    self.model_base_path = model_base_path\n    self.model_version = model_version\n    self.model_graph_signature = None\n    self.session = tf.Session(graph=tf.Graph())\n\n    self.load_model()\n\n  def load_model(self):\n\n    model_file_path = os.path.join(self.model_base_path,\n                                   str(self.model_version))\n    logging.info(""Try to load the model in: {}"".format(model_file_path))\n\n    try:\n      meta_graph = tf.saved_model.loader.load(self.session, [\n          tf.saved_model.tag_constants.SERVING\n      ], model_file_path)\n      logging.info(""Succeed to load model in: {}"".format(model_file_path))\n\n      self.model_graph_signature = list(meta_graph.signature_def.items())[0][1]\n\n    except IOError as ioe:\n      logging.info(""Fail to load model and catch exception: {}"".format(ioe))\n\n  def inference(self, json_data):\n    # 1. Build feed dict for input data\n    feed_dict_map = {}\n    input_data = json_data.get(""data"", """")\n\n    for input_item in self.model_graph_signature.inputs.items():\n      input_op_name = input_item[0]\n      input_tensor_name = input_item[1].name\n      feed_dict_map[input_tensor_name] = input_data[input_op_name]\n\n    # 2. Build inference operators\n    output_tensor_names = []\n    output_op_names = []\n    for output_item in self.model_graph_signature.outputs.items():\n\n      if output_item[1].name != """":\n        output_op_name = output_item[0]\n        output_op_names.append(output_op_name)\n        output_tensor_name = output_item[1].name\n        output_tensor_names.append(output_tensor_name)\n      elif output_item[1].coo_sparse != None:\n        # For SparseTensor op, Example: values_tensor_name: ""CTCBeamSearchDecoder_1:1"", indices_tensor_name: ""CTCBeamSearchDecoder_1:0"", dense_shape_tensor_name: ""CTCBeamSearchDecoder_1:2""\n        values_tensor_name = output_item[1].coo_sparse.values_tensor_name\n        indices_tensor_name = output_item[1].coo_sparse.indices_tensor_name\n        dense_shape_tensor_name = output_item[\n            1].coo_sparse.dense_shape_tensor_name\n        output_op_names.append(""{}_{}"".format(output_item[0], ""values""))\n        output_op_names.append(""{}_{}"".format(output_item[0], ""indices""))\n        output_op_names.append(""{}_{}"".format(output_item[0], ""shape""))\n        output_tensor_names.append(values_tensor_name)\n        output_tensor_names.append(indices_tensor_name)\n        output_tensor_names.append(dense_shape_tensor_name)\n\n    # 3. Inference with Session run\n    start_time = time.time()\n    result_ndarrays = self.session.run(\n        output_tensor_names, feed_dict=feed_dict_map)\n    logging.debug(""Inference time: {} s"".format(time.time() - start_time))\n\n    # 4. Build return result\n    result = {}\n    for i in range(len(output_op_names)):\n      result[output_op_names[i]] = result_ndarrays[i]\n    logging.debug(""Inference result: {}"".format(result))\n    return result\n\n\ndef main():\n\n  model_base_path = ""../models/tensorflow_template_application_model/""\n  model_version = ""1""\n  json_data = {\n      ""data"": {\n          ""keys"": [[1.0], [2.0]],\n          ""features"": [[1, 1, 1, 1, 1, 1, 1, 1, 1],\n                       [1, 1, 1, 1, 1, 1, 1, 1, 1]]\n      }\n  }\n\n  service = LocalInferenceService(model_base_path, model_version)\n  service.inference(json_data)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
benchmark/inception_v4/benchmark_grpc_client.py,2,"b'#!/usr/bin/env python\n\nimport base64\nimport time\nimport numpy\nimport tensorflow as tf\nfrom grpc.beta import implementations\nfrom tensorflow_serving.apis import predict_pb2, prediction_service_pb2\n\n\ndef main():\n  host = ""0.0.0.0""\n  port = 8502\n  model_name = ""default""\n  model_version = -1\n  signature_name = """"\n  request_timeout = 10.0\n\n  # Generate inference data\n  image_b64_string = base64.urlsafe_b64encode(open(""./0.jpg"", ""rb"").read())\n  images_tensor_proto = tf.contrib.util.make_tensor_proto(\n      [image_b64_string], dtype=tf.string)\n\n  # Create gRPC client\n  channel = implementations.insecure_channel(host, port)\n  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n  request = predict_pb2.PredictRequest()\n  request.model_spec.name = model_name\n  if model_version > 0:\n    request.model_spec.version.value = model_version\n  if signature_name != """":\n    request.model_spec.signature_name = signature_name\n  request.inputs[""images""].CopyFrom(images_tensor_proto)\n\n  # Send request\n  start_time = time.time()\n  for i in range(10):\n    result = stub.Predict(request, request_timeout)\n  end_time = time.time()\n  print(""Cost time: {}"".format(end_time - start_time))\n\n  print(result)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
benchmark/inception_v4/benchmark_http_client.py,0,"b'#!/usr/bin/env python\n\nimport requests\nimport time\nimport base64\n\n\ndef main():\n  benchmark(""simple_tensorflow_serving_flask"")\n\n  time.sleep(3)\n  benchmark(""simple_tensorflow_serving_uwsgi"")\n\n  time.sleep(3)\n  benchmark(""tensorflow_serving_restful"")\n\n\ndef benchmark(benchmark_type):\n  print(""Start benchmark for {}"".format(benchmark_type))\n\n  if benchmark_type == ""simple_tensorflow_serving_flask"":\n    endpoint = ""http://127.0.0.1:8500""\n    image_b64_string = base64.urlsafe_b64encode(open(""./0.jpg"", ""rb"").read())\n    input_data = {""data"": {""images"": [image_b64_string]}}\n\n  elif benchmark_type == ""simple_tensorflow_serving_uwsgi"":\n    endpoint = ""http://127.0.0.1:8501""\n    image_b64_string = base64.urlsafe_b64encode(open(""./0.jpg"", ""rb"").read())\n    input_data = {""data"": {""images"": [image_b64_string]}}\n\n  elif benchmark_type == ""tensorflow_serving_restful"":\n    endpoint = ""http://127.0.0.1:8503/v1/models/default/versions/1:predict""\n    image_b64_string = base64.urlsafe_b64encode(open(""./0.jpg"", ""rb"").read())\n    input_data = {""instances"": [{""images"": image_b64_string}]}\n\n  start_time = time.time()\n  for i in range(10):\n    result = requests.post(endpoint, json=input_data)\n  end_time = time.time()\n  print(""Cost time: {}"".format(end_time - start_time))\n  print(result)\n  #print(result.text)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
benchmark/inception_v4/benchmark_http_client_batch.py,0,"b'#!/usr/bin/env python\n\nimport requests\nimport time\nimport base64\n\n\ndef main():\n  benchmark(""simple_tensorflow_serving_flask"")\n\n  time.sleep(3)\n  benchmark(""simple_tensorflow_serving_uwsgi"")\n\n  time.sleep(3)\n  benchmark(""tensorflow_serving_restful"")\n\n\ndef benchmark(benchmark_type):\n  print(""Start benchmark for {}"".format(benchmark_type))\n\n  if benchmark_type == ""simple_tensorflow_serving_flask"":\n    endpoint = ""http://127.0.0.1:8500""\n    image_b64_string = base64.urlsafe_b64encode(open(""./0.jpg"", ""rb"").read())\n    input_data = {""data"": {""images"": [image_b64_string]}}\n\n  elif benchmark_type == ""simple_tensorflow_serving_uwsgi"":\n    endpoint = ""http://127.0.0.1:8501""\n    image_b64_string = base64.urlsafe_b64encode(open(""./0.jpg"", ""rb"").read())\n    input_data = {""data"": {""images"": [image_b64_string]}}\n\n  elif benchmark_type == ""tensorflow_serving_restful"":\n    endpoint = ""http://127.0.0.1:8503/v1/models/default/versions/1:predict""\n    image_b64_string = base64.urlsafe_b64encode(open(""./0.jpg"", ""rb"").read())\n    input_data = {""instances"": [{""images"": image_b64_string}]}\n\n  start_time = time.time()\n  for i in range(10):\n    result = requests.post(endpoint, json=input_data)\n  end_time = time.time()\n  print(""Cost time: {}"".format(end_time - start_time))\n  print(result)\n  #print(result.text)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
benchmark/simplest_model/benchmark_grpc_client.py,3,"b'#!/usr/bin/env python\n\nimport time\nimport numpy\nimport tensorflow as tf\nfrom grpc.beta import implementations\nfrom tensorflow_serving.apis import predict_pb2, prediction_service_pb2\n\n\ndef main():\n  host = ""0.0.0.0""\n  port = 8501\n  model_name = ""default""\n  model_version = -1\n  signature_name = """"\n  request_timeout = 10.0\n\n  # Generate inference data\n  keys = numpy.asarray([[1]])\n  keys_tensor_proto = tf.contrib.util.make_tensor_proto(keys, dtype=tf.int32)\n  features = numpy.asarray([[1, 2, 3, 4, 5, 6, 7, 8, 9]])\n  features_tensor_proto = tf.contrib.util.make_tensor_proto(\n      features, dtype=tf.float32)\n\n  # Create gRPC client\n  channel = implementations.insecure_channel(host, port)\n  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n  request = predict_pb2.PredictRequest()\n  request.model_spec.name = model_name\n  if model_version > 0:\n    request.model_spec.version.value = model_version\n  if signature_name != """":\n    request.model_spec.signature_name = signature_name\n  request.inputs[""keys""].CopyFrom(keys_tensor_proto)\n  #request.inputs[""features""].CopyFrom(features_tensor_proto)\n\n  # Send request\n  start_time = time.time()\n  for i in range(100):\n    result = stub.Predict(request, request_timeout)\n  end_time = time.time()\n  print(""Cost time: {}"".format(end_time - start_time))\n\n  print(result)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
benchmark/simplest_model/benchmark_http_client.py,0,"b'#!/usr/bin/env python\n\nimport requests\nimport time\n\n\ndef main():\n  benchmark(""simple_tensorflow_serving_flask"")\n\n  time.sleep(3)\n  benchmark(""simple_tensorflow_serving_uwsgi"")\n\n  time.sleep(3)\n  benchmark(""tensorflow_serving_restful"")\n\n\ndef benchmark(benchmark_type):\n  print(""Start benchmark for {}"".format(benchmark_type))\n\n  if benchmark_type == ""simple_tensorflow_serving_flask"":\n    endpoint = ""http://127.0.0.1:8500""\n    input_data = {""data"": {""keys"": [[1]]}}\n\n  elif benchmark_type == ""simple_tensorflow_serving_uwsgi"":\n    endpoint = ""http://127.0.0.1:8501""\n    input_data = {""data"": {""keys"": [[1]]}}\n\n  elif benchmark_type == ""tensorflow_serving_restful"":\n    endpoint = ""http://127.0.0.1:8503/v1/models/default/versions/1:predict""\n    input_data = {""instances"": [{""keys"": 1}]}\n\n  start_time = time.time()\n  for i in range(100):\n    result = requests.post(endpoint, json=input_data)\n  end_time = time.time()\n  print(""Cost time: {}"".format(end_time - start_time))\n  print(result)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
benchmark/simplest_model/benchmark_http_client_batch.py,0,"b'#!/usr/bin/env python\n\nimport requests\nimport time\n\n\ndef main():\n  benchmark(1)\n\n  time.sleep(3)\n  benchmark(16)\n\n  time.sleep(3)\n  benchmark(128)\n\n  time.sleep(3)\n  benchmark(1024)\n\n  time.sleep(3)\n  benchmark(8192)\n\n  time.sleep(3)\n  benchmark(65536)\n\n  time.sleep(3)\n  benchmark(524288)\n\n  time.sleep(3)\n  benchmark(4194304)\n\n\ndef benchmark(batch_size):\n  print(""Start benchmark for batch size: {}"".format(batch_size))\n\n  endpoint = ""http://127.0.0.1:8500""\n\n  batch_data = [1 for i in range(batch_size)]\n  input_data = {""data"": {""keys"": [batch_data]}}\n\n  if batch_size == 4194304:\n    start_time = time.time()\n    for i in range(10):\n      result = requests.post(endpoint, json=input_data)\n    end_time = time.time()\n  elif batch_size == 524288 or batch_size == 65536:\n    start_time = time.time()\n    for i in range(100):\n      result = requests.post(endpoint, json=input_data)\n    end_time = time.time()\n  else:\n    start_time = time.time()\n    for i in range(1000):\n      result = requests.post(endpoint, json=input_data)\n    end_time = time.time()\n\n  print(""Cost time: {}"".format(end_time - start_time))\n  print(result)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
benchmark/tensorflow_template_application_model/benchmark_grpc_client.py,3,"b'#!/usr/bin/env python\n\nimport time\nimport numpy\nimport tensorflow as tf\nfrom grpc.beta import implementations\nfrom tensorflow_serving.apis import predict_pb2, prediction_service_pb2\n\n\ndef main():\n  host = ""0.0.0.0""\n  port = 8502\n  model_name = ""default""\n  model_version = -1\n  signature_name = """"\n  request_timeout = 10.0\n\n  # Generate inference data\n  keys = numpy.asarray([[1]])\n  keys_tensor_proto = tf.contrib.util.make_tensor_proto(keys, dtype=tf.int32)\n  features = numpy.asarray([[1, 2, 3, 4, 5, 6, 7, 8, 9]])\n  features_tensor_proto = tf.contrib.util.make_tensor_proto(\n      features, dtype=tf.float32)\n\n  # Create gRPC client\n  channel = implementations.insecure_channel(host, port)\n  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n  request = predict_pb2.PredictRequest()\n  request.model_spec.name = model_name\n  if model_version > 0:\n    request.model_spec.version.value = model_version\n  if signature_name != """":\n    request.model_spec.signature_name = signature_name\n  request.inputs[""keys""].CopyFrom(keys_tensor_proto)\n  request.inputs[""features""].CopyFrom(features_tensor_proto)\n\n  # Send request\n  start_time = time.time()\n  for i in range(100):\n    result = stub.Predict(request, request_timeout)\n  end_time = time.time()\n  print(""Cost time: {}"".format(end_time - start_time))\n\n  print(result)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
benchmark/tensorflow_template_application_model/benchmark_http_client.py,0,"b'#!/usr/bin/env python\n\nimport requests\nimport time\n\n\ndef main():\n  benchmark(""simple_tensorflow_serving_flask"")\n\n  time.sleep(3)\n  benchmark(""simple_tensorflow_serving_uwsgi"")\n\n  time.sleep(3)\n  benchmark(""tensorflow_serving_restful"")\n\n\ndef benchmark(benchmark_type):\n  print(""Start benchmark for {}"".format(benchmark_type))\n\n  if benchmark_type == ""simple_tensorflow_serving_flask"":\n    endpoint = ""http://127.0.0.1:8500""\n    input_data = {\n        ""data"": {\n            ""keys"": [[1]],\n            ""features"": [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]]\n        }\n    }\n\n  elif benchmark_type == ""simple_tensorflow_serving_uwsgi"":\n    endpoint = ""http://127.0.0.1:8501""\n    input_data = {\n        ""data"": {\n            ""keys"": [[1]],\n            ""features"": [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]]\n        }\n    }\n\n  elif benchmark_type == ""tensorflow_serving_restful"":\n    endpoint = ""http://127.0.0.1:8503/v1/models/default/versions/1:predict""\n    input_data = {\n        ""instances"": [{\n            ""keys"": 1,\n            ""features"": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n        }]\n    }\n\n  start_time = time.time()\n  for i in range(100):\n    result = requests.post(endpoint, json=input_data)\n  end_time = time.time()\n  print(""Cost time: {}"".format(end_time - start_time))\n  print(result)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
clients/python_client/auth_client.py,0,"b'#!/usr/bin/env python\n\nimport requests\n\n\ndef main():\n\n  endpoint = ""http://127.0.0.1:8500""\n  input_data = {\n      ""data"": {\n          ""keys"": [[11.0], [2.0]],\n          ""features"": [[1, 1, 1, 1, 1, 1, 1, 1, 1],\n                       [1, 1, 1, 1, 1, 1, 1, 1, 1]]\n      }\n  }\n  auth = requests.auth.HTTPBasicAuth(""admin"", ""admin"")\n  result = requests.post(endpoint, json=input_data, auth=auth)\n  print(result.text)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
clients/python_client/client.py,0,"b'#!/usr/bin/env python\n\nimport requests\n\n\ndef main():\n\n  endpoint = ""http://127.0.0.1:8500""\n  input_data = {\n      #""model_name"": ""tensorflow_template_application_model"",\n      #""model_version"": 1,\n      #""signature_name"": ""serving_default"",\n      ""data"": {\n          ""keys"": [[1.0], [2.0]],\n          ""features"": [[1, 1, 1, 1, 1, 1, 1, 1, 1],\n                       [1, 1, 1, 1, 1, 1, 1, 1, 1]]\n      }\n  }\n  result = requests.post(endpoint, json=input_data)\n  print(result.text)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
clients/python_client/estimator_client.py,0,"b'#!/usr/bin/env python\n\nimport requests\nimport base64\n\n\ndef main():\n\n  string_data1 = \'\\n\\x1f\\n\\x0e\\n\\x01a\\x12\\t\\n\\x07\\n\\x05hello\\n\\r\\n\\x01b\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00\\x00?\'\n  string_data2 = \'\\n \\n\\x0f\\n\\x01a\\x12\\n\\n\\x08\\n\\x06\\xe4\\xbd\\xa0\\xe5\\xa5\\xbd\\n\\r\\n\\x01b\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00\\x80\\xbf\'\n  # Example: Ch8KDgoBYRIJCgcKBWhlbGxvCg0KAWISCBIGCgQAAAA_\n  string_data1_b64 = base64.urlsafe_b64encode(string_data1)\n  string_data2_b64 = base64.urlsafe_b64encode(string_data2)\n  string_datas = [{""b64"": string_data1_b64}, {""b64"": string_data2_b64}]\n\n  endpoint = ""http://127.0.0.1:8500""\n  input_data = {""data"": {""inputs"": string_datas}}\n  result = requests.post(endpoint, json=input_data)\n  print(result.text)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
clients/python_client/h2o_prostate_client.py,0,"b'#!/usr/bin/env python\n\nimport json\nimport requests\n\n\ndef main():\n\n  endpoint = ""http://127.0.0.1:8500""\n\n  # Load json file\n  json_filename = ""prostate_test_data.json""\n\n  with open(json_filename, ""r"") as f:\n    pandas_json_data = json.load(f)\n    json_data = json.loads(pandas_json_data)\n\n  input_data = {\n      ""model_name"": ""default"",\n      ""model_version"": 1,\n      ""data"": {\n          ""data"": json_data\n      }\n  }\n  result = requests.post(endpoint, json=input_data)\n  print(result.text)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
clients/python_client/image_client.py,0,"b'#!/usr/bin/env python\n\nimport requests\nimport base64\n\n\ndef main():\n\n  image_file_name = ""../images/mew.jpg""\n  image_b64_string = base64.urlsafe_b64encode(\n      open(image_file_name, ""rb"").read())\n\n  endpoint = ""http://127.0.0.1:8500""\n  input_data = {\n      #""model_name"": ""tensorflow_template_application_model"",\n      #""model_version"": 2,\n      #""signature_name"": ""serving_base64"",\n      ""data"": {\n          ""images"": [image_b64_string]\n      }\n  }\n  result = requests.post(endpoint, json=input_data)\n  print(result.text)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
clients/python_client/mllib_lr_client.py,0,"b'#!/usr/bin/env python\n\nimport requests\n\n\ndef main():\n\n  endpoint = ""http://127.0.0.1:8500""\n  input_data = {\n      ""model_name"": ""default"",\n      ""model_version"": 1,\n      ""data"": {\n          ""format"": ""libsvm"",\n          ""max_ids"": 692,\n          ""ids"": [128, 129, 130],\n          ""values"": [51, 159, 20]\n      }\n  }\n  result = requests.post(endpoint, json=input_data)\n  print(result.text)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
clients/python_client/mxnet_mlp_client.py,0,"b'#!/usr/bin/env python\n\nimport requests\n\n\ndef main():\n\n  endpoint = ""http://127.0.0.1:8500""\n  input_data = {\n      ""model_name"": ""mxnet_mlp_model"",\n      ""model_version"": 1,\n      ""data"": {\n          ""data"": [[1.0, 2.0]]\n      }\n  }\n  result = requests.post(endpoint, json=input_data)\n  print(result.text)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
clients/python_client/onnx_mnist_client.py,0,"b'#!/usr/bin/env python\n\nimport requests\n\n\ndef main():\n\n  endpoint = ""http://127.0.0.1:8500""\n  input_data = {\n      #""model_name"": ""onnx_mnist_model"",\n      #""model_version"": 1,\n      ""data"": {\n          ""data"": [[[[\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ], [\n              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n              1, 1, 1, 1, 1, 1\n          ]]]]\n      }\n  }\n  result = requests.post(endpoint, json=input_data)\n  print(result.text)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
clients/python_client/pmml_iris_client.py,0,"b'#!/usr/bin/env python\n\nimport requests\n\n\ndef main():\n\n  endpoint = ""http://127.0.0.1:8500""\n  input_data = {\n      ""model_name"": ""default"",\n      ""model_version"": 1,\n      ""data"": {\n          ""Sepal_Length"": 5.1,\n          ""Sepal_Width"": 3.5,\n          ""Petal_Length"": 1.4,\n          ""Petal_Width"": 0.2\n      }\n  }\n  result = requests.post(endpoint, json=input_data)\n  print(result.text)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
clients/python_client/scikitlearn_iris_client.py,0,"b'#!/usr/bin/env python\n\nimport requests\n\n\ndef main():\n\n  endpoint = ""http://127.0.0.1:8500""\n\n  input_data = {\n      ""model_name"": ""default"",\n      ""model_version"": 1,\n      ""data"": [[1.0, 2.0, 3.0, 4.0]]\n  }\n  result = requests.post(endpoint, json=input_data)\n  print(result.text)\n\n  input_data = {\n      ""preprocess"": True,\n      ""postprocess"": True,\n      ""data"": [[1.0, 2.0, 3.0, 4.0]]\n  }\n  result = requests.post(endpoint, json=input_data)\n  print(result.text)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
clients/python_client/simplest_client.py,0,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\nimport requests\n\n\ndef main():\n\n  endpoint = ""http://127.0.0.1:8500""\n\n  print(""Request for raw model signature"")\n  input_data = {""data"": {""keys"": [1, 2]}}\n  result = requests.post(endpoint, json=input_data)\n  print(result.text)\n\n  print(""Request with preprocess"")\n  input_data = {""preprocess"": True, ""data"": {""keys"": [""\xe4\xbd\xa0\xe5\xa5\xbd\xe4\xb8\x96\xe7\x95\x8c"", ""\xe6\x9c\xba\xe5\x99\xa8\xe5\xad\xa6\xe4\xb9\xa0\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xe6\xa8\xa1\xe5\x9e\x8b""]}}\n  result = requests.post(endpoint, json=input_data)\n  print(result.text)\n\n  print(""Request with preprocess and postprocess"")\n  input_data = {\n      ""preprocess"": True,\n      ""postprocess"": True,\n      ""data"": {\n          ""keys"": [""\xe4\xbd\xa0\xe5\xa5\xbd\xe4\xb8\x96\xe7\x95\x8c"", ""\xe6\x9c\xba\xe5\x99\xa8\xe5\xad\xa6\xe4\xb9\xa0\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xe6\xa8\xa1\xe5\x9e\x8b""]\n      }\n  }\n  result = requests.post(endpoint, json=input_data)\n  print(result.text)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
clients/python_client/xgboost_iris_client.py,0,"b'#!/usr/bin/env python\n\nimport requests\n\n\ndef main():\n\n  endpoint = ""http://127.0.0.1:8500""\n  input_data = {\n      ""model_name"": ""default"",\n      ""model_version"": 1,\n      ""data"": [[1.0, 2.0, 3.0, 4.0]]\n  }\n  result = requests.post(endpoint, json=input_data)\n  print(result.text)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
docs/source/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\nimport recommonmark\nfrom recommonmark.transform import AutoStructify\n\n\n# -- Project information -----------------------------------------------------\n\nproject = u\'simple-tensorflow-serving\'\ncopyright = u\'2018, tobe\'\nauthor = u\'tobe\'\n\n# The short X.Y version\nversion = u\'\'\n# The full version, including alpha/beta/rc tags\nrelease = u\'\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\n#source_suffix = \'.rst\'\nfrom recommonmark.parser import CommonMarkParser\nsource_parsers = {\n    \'.md\': CommonMarkParser,\n}\nsource_suffix = [\'.rst\', \'.md\']\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = None\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\n#html_theme = \'alabaster\'\nimport sphinx_rtd_theme\nhtml_theme = ""sphinx_rtd_theme""\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'simple-tensorflow-servingdoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'simple-tensorflow-serving.tex\', u\'simple-tensorflow-serving Documentation\',\n     u\'tobe\', \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'simple-tensorflow-serving\', u\'simple-tensorflow-serving Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'simple-tensorflow-serving\', u\'simple-tensorflow-serving Documentation\',\n     author, \'simple-tensorflow-serving\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n# -- Options for Epub output -------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [\'search.html\']\n\ngithub_doc_root = \'https://github.com/tobegit3hub/simple_tensorflow_serving/tree/master/docs/\'\ndef setup(app):\n    app.add_config_value(\'recommonmark_config\', {\n            \'url_resolver\': lambda url: github_doc_root + url,\n            \'auto_toc_tree_section\': \'Contents\',\n            }, True)\n    app.add_transform(AutoStructify)\n'"
examples/custom_op/train.py,5,"b'#!/usr/bin/env python\n\nimport os\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.saved_model import builder as saved_model_builder\nfrom tensorflow.python.saved_model import (\n    signature_constants, signature_def_utils, tag_constants, utils)\nfrom tensorflow.python.util import compat\n\n\ndef main():\n  # Load custom op\n  filename = os.path.join(os.path.dirname(__file__), ""zero_out.so"")\n  zero_out_module = tf.load_op_library(filename)\n  zero_out = zero_out_module.zero_out\n\n  # Prepare train data\n  train_data = np.ones((2, 2))\n  print(""Input data: {}"".format(train_data))\n\n  # Define the model\n  input = tf.placeholder(tf.int32, shape=(None, 2))\n  output = zero_out(input)\n\n  # Export the model\n  model_path = ""model""\n  model_version = 1\n  model_signature = signature_def_utils.build_signature_def(\n      inputs={\n          ""input"": utils.build_tensor_info(input),\n      },\n      outputs={\n          ""output"": utils.build_tensor_info(output),\n      },\n      method_name=signature_constants.PREDICT_METHOD_NAME)\n  export_path = os.path.join(\n      compat.as_bytes(model_path), compat.as_bytes(str(model_version)))\n  legacy_init_op = tf.group(tf.tables_initializer(), name=\'legacy_init_op\')\n\n  # Create session to run\n  with tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n\n    output_data = sess.run(output, feed_dict={input: train_data})\n    print(""Output data: {}"".format(output_data))\n\n    builder = saved_model_builder.SavedModelBuilder(export_path)\n    builder.add_meta_graph_and_variables(\n        sess, [tag_constants.SERVING],\n        clear_devices=True,\n        signature_def_map={\n            signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n            model_signature,\n        },\n        legacy_init_op=legacy_init_op)\n\n    builder.save()\n\n\nif __name__ == ""__main__"":\n  main()\n'"
simple_tensorflow_serving/gen_client/__init__.py,0,b''
simple_tensorflow_serving/gen_client/gen_bash.py,0,"b'\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport logging\n\nfrom jinja2 import Template\n\n\ndef gen_tensorflow_client_string(generated_tensor_data, model_name):\n  """"""\n  Generate TensorFlow SDK in Bash.\n\n  Args:\n    generated_tensor_data: Example is {""keys"": [[1.0], [2.0]], ""features"": [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n  """"""\n\n  code_template = """"""#!/bin/bash\n\ncurl -H ""Content-Type: application/json"" -X POST -d \'{""model_name"": ""{{ model_name }}"", ""data"": {{ tensor_data }} }\' http://127.0.0.1:8500\n  """"""\n\n  generated_tensor_data_string = json.dumps(generated_tensor_data)\n  template = Template(code_template)\n  generate_code = template.render(\n      model_name=model_name, tensor_data=generated_tensor_data_string)\n  logging.debug(""Generate the code in Bash:\\n{}"".format(generate_code))\n\n  return generate_code\n\n\ndef gen_tensorflow_client(generated_tensor_data, model_name):\n\n  generate_code = gen_tensorflow_client_string(generated_tensor_data, model_name)\n\n  generated_code_filename = ""client.sh""\n  with open(generated_code_filename, ""w"") as f:\n    f.write(generate_code)\n\n  logging.info(\'Save the generated code in {}, try ""bash {}""\'.format(\n      generated_code_filename, generated_code_filename))\n'"
simple_tensorflow_serving/gen_client/gen_client.py,6,"b'\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport tensorflow as tf\n\nfrom . import gen_bash\nfrom . import gen_golang\nfrom . import gen_javascript\nfrom . import gen_python\n\nlogger = logging.getLogger(""simple_tensorflow_serving"")\n\n\ndef gen_tensorflow_client(tensorflow_inference_service, language, model_name):\n  """"""\n  Generate the TensorFlow client for programming languages.\n  \n  Args:\n    tensorflow_inference_service: The tensorflow service object.\n    language: The client in this programming language to generate.\n    model_name: The name of the model.\n    \n  Return:\n    None\n  """"""\n\n  if language not in [""json"", ""bash"", ""python"", ""golang"", ""javascript""]:\n    logger.error(\n        ""Language: {} is not supported to gen client"".format(language))\n    return\n\n  # Example: {""keys"": [-1, 1], ""features"": [-1, 9]}\n  input_opname_shape_map = {}\n  input_opname_dtype_map = {}\n\n  for input_item in tensorflow_inference_service.model_graph_signature.inputs.items(\n  ):\n    # Example: ""keys""\n    input_opname = input_item[0]\n    input_opname_shape_map[input_opname] = []\n    input_opname_dtype_map[input_opname] = input_item[1].dtype\n\n    # Example: [-1, 1]\n    shape_dims = input_item[1].tensor_shape.dim\n\n    for dim in shape_dims:\n      input_opname_shape_map[input_opname].append(int(dim.size))\n\n  logger.debug(\n      ""The input operator and shape: {}"".format(input_opname_shape_map))\n\n  # Example: {""keys"": [[1.0], [2.0]], ""features"": [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n  generated_tensor_data = {}\n\n  batch_size = 2\n  for opname, shapes in input_opname_shape_map.items():\n\n    # Use to generated the nested array\n    internal_array = None\n\n    # Travel all the dims in reverse order\n    for i in range(len(shapes)):\n      dim = shapes[len(shapes) - 1 - i]\n\n      if dim == -1:\n        dim = batch_size\n\n      if internal_array == None:\n        # Fill with default values by the types, refer to https://www.tensorflow.org/api_docs/python/tf/DType\n        default_value = 1.0\n        dtype = input_opname_dtype_map[opname]\n\n\n        # TODO: TensorFlow 1.14+ does not support converting to int number\n        """"""\n        if dtype == int(tf.int8) or dtype == int(tf.uint8) or dtype == int(\n            tf.int16) or dtype == int(tf.uint16) or dtype == int(\n                tf.int32) or dtype == int(tf.uint32):\n          default_value = 1\n        elif dtype == int(tf.int64) or dtype == int(tf.uint64):\n          default_value = 1\n        elif dtype == int(tf.bool):\n          default_value = True\n        elif dtype == int(tf.string):\n          default_value = """"\n        """"""\n\n        if dtype in [6, 4, 5, 17, 3, 22, 9, 23]:\n          default_value = 1\n        elif dtype == 10:\n          default_value = True\n        elif dtype == 7:\n          default_value = """"\n        else:\n          default_value = 1\n\n        internal_array = [default_value for i in range(dim)]\n\n      else:\n        internal_array = [internal_array for i in range(dim)]\n\n    generated_tensor_data[opname] = internal_array\n\n  if language == ""json"":\n    return {""data"": generated_tensor_data}\n  elif language == ""bash"":\n    return gen_bash.gen_tensorflow_client_string(generated_tensor_data, model_name)\n  elif language == ""python"":\n    return gen_python.gen_tensorflow_client_string(generated_tensor_data, model_name)\n  elif language == ""golang"":\n    return gen_golang.gen_tensorflow_client_string(generated_tensor_data, model_name)\n  elif language == ""javascript"":\n    return gen_javascript.gen_tensorflow_client_string(generated_tensor_data, model_name)\n\n  # TODO: Return the string instead of writing in local files\n  """"""\n  if language == ""bash"":\n    gen_bash.gen_tensorflow_client(generated_tensor_data, model_name)\n  elif language == ""python"":\n    gen_python.gen_tensorflow_client(generated_tensor_data, model_name)\n  elif language == ""golang"":\n    gen_golang.gen_tensorflow_client(generated_tensor_data, model_name)\n  elif language == ""javascript"":\n    gen_javascript.gen_tensorflow_client(generated_tensor_data, model_name)\n  """"""\n'"
simple_tensorflow_serving/gen_client/gen_golang.py,0,"b'\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport logging\nfrom jinja2 import Template\n\n\ndef gen_tensorflow_client_string(generated_tensor_data, model_name):\n  """"""\n  Generate TensorFlow SDK in Golang.\n\n  Args:\n    generated_tensor_data: Example is {""keys"": [[1.0], [2.0]], ""features"": [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n  """"""\n\n  code_template = """"""\npackage main\n\nimport (\n    ""encoding/json""\n    ""fmt""\n    ""log""\n    ""bytes""\n    ""net/http""\n    ""io/ioutil""\n)\n\nfunc main() {\n    endpoint := ""http://127.0.0.1:8500""\n    log.Print(""Request tensorflow serving in "" + endpoint)\n\n    // Construct request data\n    dataByte := []byte(`{""model_name"": ""{{ model_name }}"", ""data"": {{ tensor_data }} }`)\n    var dataInterface map[string]interface{}\n    json.Unmarshal(dataByte, &dataInterface)\n    dataJson, _ := json.Marshal(dataInterface)\n\n    // Send POST request\n    resp, err := http.Post(endpoint, ""application/json"", bytes.NewBuffer(dataJson))\n\n    // Print the response\n    if err != nil {\n        log.Print(""Error to request server"")\n        return\n\n    }\n    defer resp.Body.Close()\n    body, err := ioutil.ReadAll(resp.Body)\n    fmt.Println(string(body))\n}\n  """"""\n\n  generated_tensor_data_string = json.dumps(generated_tensor_data)\n  template = Template(code_template)\n  generate_code = template.render(\n      model_name=model_name, tensor_data=generated_tensor_data_string)\n  logging.debug(""Generate the code in Golang:\\n{}"".format(generate_code))\n\n  return generate_code\n\n\ndef gen_tensorflow_client(generated_tensor_data, model_name):\n\n  generate_code = gen_tensorflow_client_string(generated_tensor_data, model_name)\n\n  generated_code_filename = ""client.go""\n  with open(generated_code_filename, ""w"") as f:\n    f.write(generate_code)\n\n  logging.info(\'Save the generated code in {}, try ""go run {}""\'.format(\n      generated_code_filename, generated_code_filename))\n'"
simple_tensorflow_serving/gen_client/gen_javascript.py,0,"b'\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport logging\nfrom jinja2 import Template\n\n\ndef gen_tensorflow_client_string(generated_tensor_data, model_name):\n  """"""\n  Generate TensorFlow SDK in JavaScript.\n\n  Args:\n    generated_tensor_data: Example is {""keys"": [[1.0], [2.0]], ""features"": [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n  """"""\n\n  code_template = """"""#!/usr/bin/env node\n\nvar request = require(""request"");\n\nvar options = {\n    uri: ""http://127.0.0.1:8500"",\n    method: ""POST"",\n    json: {""model_name"": ""{{ model_name }}"", ""data"": {{ tensor_data }} }\n};\n\nrequest(options, function (error, response, body) {\n    if (!error && response.statusCode == 200) {\n        console.log(body)\n    } else {\n        console.log(error)\n    }\n});\n  """"""\n\n  generated_tensor_data_string = json.dumps(generated_tensor_data)\n  template = Template(code_template)\n  generate_code = template.render(\n      model_name=model_name, tensor_data=generated_tensor_data_string)\n  logging.debug(""Generate the code in JavaScript:\\n{}"".format(generate_code))\n\n  return generate_code\n\n\ndef gen_tensorflow_client(generated_tensor_data, model_name):\n\n  generate_code = gen_tensorflow_client_string(generated_tensor_data, model_name)\n\n  generated_code_filename = ""client.js""\n  with open(generated_code_filename, ""w"") as f:\n    f.write(generate_code)\n\n  logging.info(\n      \'Save the generated code in {}, try ""npm install request && node {}""\'.\n      format(generated_code_filename, generated_code_filename))\n'"
simple_tensorflow_serving/gen_client/gen_python.py,0,"b'\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport logging\nfrom jinja2 import Template\n\n\ndef gen_tensorflow_client_string(generated_tensor_data, model_name):\n  """"""\n  Generate TensorFlow SDK in Python.\n\n  Args:\n    generated_tensor_data: Example is {""keys"": [[1.0], [2.0]], ""features"": [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n  """"""\n\n  code_template = """"""#!/usr/bin/env python\n\nimport requests\n\ndef main():\n  endpoint = ""http://127.0.0.1:8500""\n  json_data = {""model_name"": ""{{ model_name }}"", ""data"": {{ tensor_data }} }\n  result = requests.post(endpoint, json=json_data)\n  print(result.json())\n\nif __name__ == ""__main__"":\n  main()\n  """"""\n\n  generated_tensor_data_string = json.dumps(generated_tensor_data)\n  template = Template(code_template)\n  generate_code = template.render(\n      model_name=model_name, tensor_data=generated_tensor_data_string)\n  logging.debug(""Generate the code in Python:\\n{}"".format(generate_code))\n\n  return generate_code\n\n\ndef gen_tensorflow_client(generated_tensor_data, model_name):\n\n  generate_code = gen_tensorflow_client_string(generated_tensor_data, model_name)\n\n  generated_code_filename = ""client.py""\n  with open(generated_code_filename, ""w"") as f:\n    f.write(generate_code)\n\n  logging.info(\'Save the generated code in {}, try ""python {}""\'.format(\n      generated_code_filename, generated_code_filename))\n'"
simple_tensorflow_serving/service_utils/__init__.py,0,b''
simple_tensorflow_serving/service_utils/request_util.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport io\nimport os\nimport logging\nimport base64\nimport numpy as np\nfrom PIL import Image\n\nlogger = logging.getLogger(""simple_tensorflow_serving"")\n\n\ndef get_image_channel_layout(request_layout, support_signatures=None):\n  """"""\n    Try select a channel layout by requested layout and model signatures\n    """"""\n  layout = request_layout\n  if support_signatures is not None:\n    # get input tensor ""image""\n    for item in support_signatures[""inputs""]:\n      if item[""name""] == ""image"":\n        shape = item[""shape""]\n        if len(shape) == 4:\n          channel_size = shape[-1]\n          if channel_size == 1:\n            layout = ""L""\n          elif channel_size == 3:\n            layout = ""RGB""\n          elif channel_size == 4:\n            layout = ""RGBA""\n        break\n  return layout\n\n\n# Deprecatecd: Get image content to 3-D tensor for form-date request\ndef get_image_request_data_and_options(request,\n                                       support_signatures=None,\n                                       save_file_dir=None):\n  """"""\n    Get data items for image application\n    :return: (numpy.ndarray, image specific options dict)\n    """"""\n  options = {}\n  image_file = request.files[""image""]\n  image_content = image_file.read()\n  image_string = np.fromstring(image_content, np.uint8)\n  if sys.version_info[0] < 3:\n    import cStringIO\n    image_string_io = cStringIO.StringIO(image_string)\n  else:\n    image_string_io = io.BytesIO(image_string)\n\n  if save_file_dir is not None:\n    with open(os.path.join(save_file_dir, image_file.filename),\n              ""wb"") as save_file:\n      save_file.write(image_content)\n\n  image_file = Image.open(image_string_io)\n\n  channel_layout = request.form.get(""channel_layout"", ""RGB"")\n  channel_layout = get_image_channel_layout(channel_layout, support_signatures)\n  if channel_layout in [""RGB"", ""RGBA""]:\n    if channel_layout != str(image_file.mode):\n      logger.info(""Convert image from %s to %s"" % (image_file.mode,\n                                                    channel_layout))\n      image_file = image_file.convert(channel_layout)\n  else:\n    logger.error(""Illegal image_layout: {}"".format(channel_layout))\n\n  image_array = np.array(image_file)\n\n  # TODO: Support multiple images without reshaping\n  if ""shape"" in request.form:\n    # Example: ""32,32,1,3"" -> (32, 32, 1, 3)\n    shape = tuple([int(item) for item in request.form[""shape""].split("","")])\n    image_array = image_array.reshape(shape)\n  else:\n    image_array = image_array.reshape(1, *image_array.shape)\n\n  return image_array, options\n\n\ndef create_json_from_formdata_request(request,\n                                      download_inference_images=False,\n                                      save_file_dir=None):\n  json_data = {}\n\n  # General arguments\n  if ""model_version"" in request.form:\n    json_data[""model_version""] = request.form[""model_version""]\n  if ""signature_name"" in request.form:\n    json_data[""signature_name""] = request.form[""signature_name""]\n  if ""run_profile"" in request.form:\n    json_data[""run_profile""] = request.form[""run_profile""]\n\n  if ""image"" not in request.files and ""images"" not in request.files:\n    logger.error(""Need to set image or images for form-data"")\n    return None\n\n  image_b64_strings = []\n  if ""image"" in request.files:\n\n    image_file = request.files[""image""]\n    image_content = image_file.read()\n    image_b64_string = base64.urlsafe_b64encode(image_content)\n    image_b64_strings.append(image_b64_string)\n\n    if download_inference_images and save_file_dir is not None:\n      with open(os.path.join(save_file_dir, image_file.filename),\n                ""wb"") as save_file:\n        save_file.write(image_content)\n\n  if ""images"" in request.files:\n\n    for image_file in request.files.getlist(""images""):\n      image_content = image_file.read()\n      image_b64_string = base64.urlsafe_b64encode(image_content)\n      image_b64_strings.append(image_b64_string)\n\n      if download_inference_images and save_file_dir is not None:\n        with open(os.path.join(save_file_dir, image_file.filename),\n                  ""wb"") as save_file:\n          save_file.write(image_content)\n\n  json_data[""data""] = {""images"": image_b64_strings}\n\n  return json_data\n'"
tools/mllib_model_tool/load_lr_model.py,0,"b'#!/usr/bin/env python\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.classification import LogisticRegressionModel\nfrom pyspark.ml.linalg import SparseVector\n\nspark = SparkSession.builder.appName(""libsvm_lr"").getOrCreate()\n\n# Load model\nmodel_path = ""./lr_model/""\nlrModel = LogisticRegressionModel.load(model_path)\nprint(""Coefficients: "" + str(lrModel.coefficients))\nprint(""Intercept: "" + str(lrModel.intercept))\n\n# Construct data\n#testset = spark.read.format(""libsvm"").load(""./sample_libsvm_data.txt"")\ntestset = spark.createDataFrame(\n    [(1.0, SparseVector(692, [128, 129, 130], [51, 159, 20]))],\n    [\'label\', \'features\'])\n\n# Make inference\nresult = lrModel.transform(testset)\nresult = result.first()\nprint(""Prediction: {}, probability_of_0: {}, probability_of_1: {}"".format(\n    result.label, result.probability[0], result.probability[1]))\n'"
tools/pmml_tool/http_client.py,0,"b'#!/usr/bin/env python\n\nimport requests\n\n\ndef main():\n\n  endpoint = \'http://localhost:8080/openscoring/model/PmmlModel\'\n\n  input_data = {""id"": None, ""arguments"": {""Sepal_Width"": 3.5, ""Petal_Width"": 0.2, ""Sepal_Length"": 5.1, ""Petal_Length"": 1.4}}\n\n  result = requests.post(endpoint, json=input_data)\n  print(result.text)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
tools/pmml_tool/http_deploy_model.py,0,"b'#!/usr/bin/env python\n\nimport requests\n\n\ndef main():\n  model_name = ""PmmlModel""\n  endpoint = ""http://localhost:8080/openscoring/model/{}"".format(model_name)\n  model_file_path = ""../../models/pmml_iris/DecisionTreeIris.pmml""\n\n  with open(model_file_path, ""rb"") as f:\n    kwargs = {\n        \'headers\': {\n            \'content-type\': \'application/xml\'\n        },\n        \'json\': None,\n        \'data\': f,\n        \'auth\': (\'admin\', \'adminadmin\')\n    }\n    result = requests.put(endpoint, **kwargs)\n    print(""Deploy the model to Openscoring: {}"".format(result))\n    print(""Deploy the model to Openscoring: {}"".format(result.text))\n\n\nif __name__ == ""__main__"":\n  main()\n'"
tools/pmml_tool/pmml_sdk_client.py,0,"b'#!/usr/bin/env python\n\nfrom openscoring import Openscoring\n\nos = Openscoring(""http://localhost:8080/openscoring"")\n\nkwargs = {""auth"": (""admin"", ""adminadmin"")}\n\nmodel_file_path = ""../../models/pmml_iris/DecisionTreeIris.pmml""\nmodel_name = ""PmmlModel""\nos.deployFile(model_name, model_file_path, **kwargs)\n\narguments = {\n    ""Sepal_Length"": 5.1,\n    ""Sepal_Width"": 3.5,\n    ""Petal_Length"": 1.4,\n    ""Petal_Width"": 0.2\n}\n\nresult = os.evaluate(model_name, arguments)\nprint(result)\n'"
tools/tensorflow_estimator_tool/convert_estimator_savedmodel.py,9,"b'#!/usr/bin/env python\n\nimport tensorflow as tf\nfrom tensorflow.contrib import graph_editor\n\n\ndef main():\n  with tf.Session(graph=tf.Graph()) as sess:\n\n    tag_name = ""serve""\n    signature_name = ""serving_default""\n    input_model_path = ""./savedmodel/1531987620""\n    output_model_path = ""./converted_savedmodel/1531987620""\n\n    graph = tf.get_default_graph()\n    meta_graph_def = tf.saved_model.loader.load(sess, [tag_name],\n                                                input_model_path)\n    signature_def = meta_graph_def.signature_def[signature_name]\n\n    old_input_tensor = graph.get_tensor_by_name(\n        signature_def.inputs.values()[0].name)\n    print ""Old model input: "", old_input_tensor.dtype, old_input_tensor.shape\n\n    new_base64_placeholder = tf.placeholder(shape=[None], dtype=tf.string)\n    new_input_tensor = tf.map_fn(tf.decode_base64, new_base64_placeholder)\n    print ""New model input: "", old_input_tensor.dtype, old_input_tensor.shape\n\n    output_keys = [_.name for _ in signature_def.outputs.values()]\n    old_output_tensors = [graph.get_tensor_by_name(_) for _ in output_keys]\n\n    new_output_tensors = graph_editor.graph_replace(\n        target_ts=old_output_tensors,\n        replacement_ts={old_input_tensor: new_input_tensor})\n    new_output_tensor_infos = [\n        tf.saved_model.utils.build_tensor_info(_) for _ in new_output_tensors\n    ]\n\n    new_signature = tf.saved_model.signature_def_utils.build_signature_def(\n        inputs={\n            signature_def.inputs.keys()[0]:\n            tf.saved_model.utils.build_tensor_info(new_base64_placeholder)\n        },\n        outputs=dict(zip(output_keys, new_output_tensor_infos)),\n        method_name=signature_def.method_name)\n    builder = tf.saved_model.builder.SavedModelBuilder(output_model_path)\n    builder.add_meta_graph_and_variables(\n        sess,\n        [tag_name],\n        clear_devices=True,\n        signature_def_map={\n            signature_name: new_signature,\n        },\n        main_op=None,  # avoid duplicate\n        legacy_init_op=None  # avoid duplicate\n    )\n    builder.save()\n\n\nif __name__ == ""__main__"":\n  main()\n'"
tools/tensorflow_estimator_tool/generate_estimator_string.py,5,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport base64\nimport tensorflow as tf\n\n\ndef _float_feature(value):\n  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n\n\ndef _bytes_feature(value):\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef main():\n  input_file_name = ""data.txt""\n  seperator_symbol = "" ""\n\n  # Example: {\'age\': _float_feature(value=25), \'workclass\': _bytes_feature(value=\'Private\'.encode())}\n  feature_dict = {}\n  serialized_strings = []\n\n  with open(input_file_name, ""r"") as f:\n    lines = f.readlines()\n\n    keys = [item.strip() for item in lines[0].split(seperator_symbol)]\n    types = [item.strip() for item in lines[1].split(seperator_symbol)]\n\n    for i in range(2, len(lines)):\n      items = [item.strip() for item in lines[i].split(seperator_symbol)]\n\n      for j in range(len(items)):\n        item = items[j]\n        if types[j] == ""float"":\n          item = float(item)\n          feature_dict[keys[j]] = _float_feature(value=item)\n        elif types[j] == ""string"":\n          feature_dict[keys[j]] = _bytes_feature(value=item)\n\n      example = tf.train.Example(features=tf.train.Features(\n          feature=feature_dict))\n      serialized = example.SerializeToString()\n      serialized_strings.append(serialized)\n\n    serialized_proto = tf.contrib.util.make_tensor_proto(\n        serialized_strings, dtype=tf.string)\n    serialized_proto_handle = serialized_proto.string_val\n\n    # Example: ""\\n\\x1f\\n\\x0e\\n\\x01a\\x12\\t\\n\\x07\\n\\x05hello\\n\\r\\n\\x01b\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00\\x00?""\n    proto_string = serialized_proto_handle.pop()\n    base64_proto_string = base64.urlsafe_b64encode(proto_string)\n    print(""Base64 string: {}"".format(base64_proto_string))\n\n\nif __name__ == ""__main__"":\n  main()\n'"
tools/tensorflow_estimator_tool/local_inference.py,3,"b'#!/usr/bin/env python\n\nimport os\nimport time\nimport tensorflow as tf\nimport logging\n\nlogging.basicConfig(level=logging.DEBUG)\n\n\nclass LocalInferenceService(object):\n  def __init__(self, model_base_path, model_version):\n\n    self.model_base_path = model_base_path\n    self.model_version = model_version\n    self.model_graph_signature = None\n    self.session = tf.Session(graph=tf.Graph())\n\n    self.load_model()\n\n  def load_model(self):\n\n    model_file_path = os.path.join(self.model_base_path,\n                                   str(self.model_version))\n    logging.info(""Try to load the model in: {}"".format(model_file_path))\n\n    try:\n      meta_graph = tf.saved_model.loader.load(self.session, [\n          tf.saved_model.tag_constants.SERVING\n      ], model_file_path)\n      logging.info(""Succeed to load model in: {}"".format(model_file_path))\n\n      self.model_graph_signature = list(meta_graph.signature_def.items())[0][1]\n\n    except IOError as ioe:\n      logging.info(""Fail to load model and catch exception: {}"".format(ioe))\n\n  def inference(self, json_data):\n    # 1. Build feed dict for input data\n    feed_dict_map = {}\n    input_data = json_data.get(""data"", """")\n\n    for input_item in self.model_graph_signature.inputs.items():\n      input_op_name = input_item[0]\n      input_tensor_name = input_item[1].name\n      feed_dict_map[input_tensor_name] = input_data[input_op_name]\n\n    # 2. Build inference operators\n    output_tensor_names = []\n    output_op_names = []\n    for output_item in self.model_graph_signature.outputs.items():\n\n      if output_item[1].name != """":\n        output_op_name = output_item[0]\n        output_op_names.append(output_op_name)\n        output_tensor_name = output_item[1].name\n        output_tensor_names.append(output_tensor_name)\n      elif output_item[1].coo_sparse != None:\n        # For SparseTensor op, Example: values_tensor_name: ""CTCBeamSearchDecoder_1:1"", indices_tensor_name: ""CTCBeamSearchDecoder_1:0"", dense_shape_tensor_name: ""CTCBeamSearchDecoder_1:2""\n        values_tensor_name = output_item[1].coo_sparse.values_tensor_name\n        indices_tensor_name = output_item[1].coo_sparse.indices_tensor_name\n        dense_shape_tensor_name = output_item[\n            1].coo_sparse.dense_shape_tensor_name\n        output_op_names.append(""{}_{}"".format(output_item[0], ""values""))\n        output_op_names.append(""{}_{}"".format(output_item[0], ""indices""))\n        output_op_names.append(""{}_{}"".format(output_item[0], ""shape""))\n        output_tensor_names.append(values_tensor_name)\n        output_tensor_names.append(indices_tensor_name)\n        output_tensor_names.append(dense_shape_tensor_name)\n\n    # 3. Inference with Session run\n    start_time = time.time()\n    result_ndarrays = self.session.run(\n        output_tensor_names, feed_dict=feed_dict_map)\n    logging.debug(""Inference time: {} s"".format(time.time() - start_time))\n\n    # 4. Build return result\n    result = {}\n    for i in range(len(output_op_names)):\n      result[output_op_names[i]] = result_ndarrays[i]\n    logging.debug(""Inference result: {}"".format(result))\n    return result\n\n\ndef main():\n\n  string_data = [\n      \'\\n \\n\\x0f\\n\\x01a\\x12\\n\\n\\x08\\n\\x06\\xe4\\xbd\\xa0\\xe5\\xa5\\xbd\\n\\r\\n\\x01b\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00\\x80\\xbf\'\n  ]\n\n  endpoint = ""http://127.0.0.1:8500""\n  input_data = {""model_version"": ""1531881325"", ""data"": {""inputs"": string_data}}\n  json_data = input_data\n\n  model_base_path = ""../../models/tensorflow_estimator_model/""\n  model_version = ""1531881325""\n\n  service = LocalInferenceService(model_base_path, model_version)\n  service.inference(json_data)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
tools/tensorflow_serving_tool/python_client.py,0,"b'#!/usr/bin/env python\n\nimport requests\n\n\ndef main():\n\n  endpoint = ""http://127.0.0.1:8501/v1/models/default:predict""\n\n  input_data = {\n      #""signature_name"": ""serving_default"",\n      # ""keys"": [[1.0], [2.0]],\n      # ""features"": [[0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1]]\n      ""instances"": [{\n          ""keys"": 1,\n          ""features"": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n      }, {\n          ""keys"": 2,\n          ""features"": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n      }]\n  }\n\n  result = requests.post(endpoint, json=input_data)\n  print(result.text)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
tools/tensorflow_serving_tool/python_grpc_client.py,10,"b'#!/usr/bin/env python\n\nimport numpy\nimport tensorflow as tf\nfrom grpc.beta import implementations\nfrom tensorflow_serving.apis import predict_pb2, prediction_service_pb2\n\ntf.app.flags.DEFINE_string(""host"", ""0.0.0.0"", ""TensorFlow Serving server ip"")\ntf.app.flags.DEFINE_integer(""port"", 8500, ""TensorFlow Serving server port"")\ntf.app.flags.DEFINE_string(""model_name"", ""default"", ""The model name"")\ntf.app.flags.DEFINE_integer(""model_version"", -1, ""The model version"")\ntf.app.flags.DEFINE_string(""signature_name"", """", ""The model signature name"")\ntf.app.flags.DEFINE_float(""request_timeout"", 10.0, ""Timeout of gRPC request"")\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main():\n\n  # Generate inference data\n  keys = numpy.asarray([1, 2, 3, 4])\n  keys_tensor_proto = tf.contrib.util.make_tensor_proto(keys, dtype=tf.int32)\n  features = numpy.asarray(\n      [[1, 2, 3, 4, 5, 6, 7, 8, 9], [1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [9, 8, 7, 6, 5, 4, 3, 2, 1], [9, 9, 9, 9, 9, 9, 9, 9, 9]])\n  features_tensor_proto = tf.contrib.util.make_tensor_proto(\n      features, dtype=tf.float32)\n\n  # Create gRPC client\n  channel = implementations.insecure_channel(FLAGS.host, FLAGS.port)\n  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n  request = predict_pb2.PredictRequest()\n  request.model_spec.name = FLAGS.model_name\n  if FLAGS.model_version > 0:\n    request.model_spec.version.value = FLAGS.model_version\n  if FLAGS.signature_name != """":\n    request.model_spec.signature_name = FLAGS.signature_name\n  request.inputs[""keys""].CopyFrom(keys_tensor_proto)\n  request.inputs[""features""].CopyFrom(features_tensor_proto)\n\n  # Send request\n  result = stub.Predict(request, FLAGS.request_timeout)\n  print(result)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
