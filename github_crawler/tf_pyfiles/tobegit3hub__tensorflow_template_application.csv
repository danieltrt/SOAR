file_path,api_count,code
__init__.py,0,b''
dense_classifier.py,40,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\nfrom __future__ import absolute_import, division, print_function\n\nimport datetime\nimport logging\nimport os\nimport pprint\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn import metrics\nfrom tensorflow.python.saved_model import (signature_constants,\n                                           signature_def_utils, utils)\n\nimport util\nimport model\n\nlogging.basicConfig(\n    format=\'%(asctime)s %(levelname)-8s %(message)s\',\n    level=logging.INFO,\n    datefmt=\'%Y-%m-%d %H:%M:%S\')\n\n\ndef define_flags():\n  """"""\n  Define all the command-line parameters.\n  \n  Return:\n    The FLAGS object.\n  """"""\n\n  flags = tf.app.flags\n  flags.DEFINE_string(""mode"", ""train"", ""Support train, inference, savedmodel"")\n  flags.DEFINE_boolean(""enable_benchmark"", False, ""Enable benchmark"")\n  flags.DEFINE_boolean(""resume_from_checkpoint"", False, ""Resume or not"")\n  flags.DEFINE_string(""scenario"", ""classification"",\n                      ""Support classification, regression"")\n  flags.DEFINE_string(\n      ""loss"", ""sparse_cross_entropy"",\n      ""Support sparse_cross_entropy, cross_entropy, mean_square"")\n  flags.DEFINE_integer(""feature_size"", 9, ""Number of feature size"")\n  flags.DEFINE_integer(""label_size"", 2, ""Number of label size"")\n  flags.DEFINE_string(""file_format"", ""tfrecords"", ""Support tfrecords, csv"")\n  flags.DEFINE_string(""train_files"",\n                      ""./data/cancer/cancer_train.csv.tfrecords"",\n                      ""Train files which supports glob pattern"")\n  flags.DEFINE_string(""validation_files"",\n                      ""./data/cancer/cancer_test.csv.tfrecords"",\n                      ""Validate files which supports glob pattern"")\n  flags.DEFINE_string(""inference_data_file"", ""./data/cancer/cancer_test.csv"",\n                      ""Data file for inference"")\n  flags.DEFINE_string(""inference_result_file"", ""./inference_result.txt"",\n                      ""Result file from inference"")\n  flags.DEFINE_string(""optimizer"", ""adagrad"",\n                      ""Support sgd, adadelta, adagrad, adam, ftrl, rmsprop"")\n  flags.DEFINE_float(""learning_rate"", 0.01, ""Learning rate"")\n  flags.DEFINE_string(\n      ""model"", ""dnn"",\n      ""Support dnn, lr, wide_and_deep, customized, cnn, lstm, bidirectional_lstm, gru""\n  )\n  flags.DEFINE_string(""dnn_struct"", ""128 32 8"", ""DNN struct"")\n  flags.DEFINE_integer(""epoch_number"", 100, ""Number of epoches"")\n  flags.DEFINE_integer(""train_batch_size"", 64, ""Batch size"")\n  flags.DEFINE_integer(""validation_batch_size"", 64,\n                       ""Batch size for validation"")\n  flags.DEFINE_boolean(""enable_bn"", False, ""Enable batch normalization"")\n  flags.DEFINE_float(""bn_epsilon"", 0.001, ""Epsilon of batch normalization"")\n  flags.DEFINE_boolean(""enable_dropout"", False, ""Enable dropout"")\n  flags.DEFINE_float(""dropout_keep_prob"", 0.5, ""Keep prob of dropout"")\n  flags.DEFINE_boolean(""enable_lr_decay"", False, ""Enable learning rate decay"")\n  flags.DEFINE_float(""lr_decay_rate"", 0.96, ""Learning rate decay rate"")\n  flags.DEFINE_integer(""steps_to_validate"", 10, ""Steps to validate"")\n  flags.DEFINE_string(""checkpoint_path"", ""./checkpoint/"",\n                      ""Path for checkpoint"")\n  flags.DEFINE_string(""output_path"", ""./tensorboard/"", ""Path for tensorboard"")\n  flags.DEFINE_string(""model_path"", ""./model/"", ""Path of the model"")\n  flags.DEFINE_integer(""model_version"", 1, ""Version of the model"")\n  FLAGS = flags.FLAGS\n\n  # Check parameters\n  assert (FLAGS.mode in [""train"", ""inference"", ""savedmodel""])\n  assert (FLAGS.scenario in [""classification"", ""regression""])\n  assert (FLAGS.loss in [\n      ""sparse_cross_entropy"", ""cross_entropy"", ""mean_square""\n  ])\n  assert (FLAGS.file_format in [""tfrecords"", ""csv""])\n  assert (FLAGS.optimizer in [\n      ""sgd"", ""adadelta"", ""adagrad"", ""adam"", ""ftrl"", ""rmsprop""\n  ])\n  assert (FLAGS.model in [\n      ""dnn"", ""lr"", ""wide_and_deep"", ""customized"", ""cnn"", ""customized_cnn"",\n      ""lstm"", ""bidirectional_lstm"", ""gru""\n  ])\n\n  # Print flags\n  FLAGS.mode\n  parameter_value_map = {}\n  for key in FLAGS.__flags.keys():\n    parameter_value_map[key] = FLAGS.__flags[key].value\n  pprint.PrettyPrinter().pprint(parameter_value_map)\n  return FLAGS\n\n\nFLAGS = define_flags()\n\n\ndef parse_tfrecords_function(example_proto):\n  """"""\n  Decode TFRecords for Dataset.\n  \n  Args:\n    example_proto: TensorFlow ExampleProto object. \n  \n  Return:\n    The op of features and labels\n  """"""\n  features = {\n      ""features"": tf.FixedLenFeature([FLAGS.feature_size], tf.float32),\n      ""label"": tf.FixedLenFeature([], tf.int64, default_value=0)\n  }\n  parsed_features = tf.parse_single_example(example_proto, features)\n  return parsed_features[""features""], parsed_features[""label""]\n\n\ndef parse_csv_function(line):\n  """"""\n  Decode CSV for Dataset.\n  \n  Args:\n    line: One line data of the CSV.\n  \n  Return:\n    The op of features and labels\n  """"""\n\n  FIELD_DEFAULTS = [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],\n                    [0.0], [0]]\n\n  fields = tf.decode_csv(line, FIELD_DEFAULTS)\n\n  label = fields[-1]\n  label = tf.cast(label, tf.int64)\n  features = tf.stack(fields[0:-1])\n\n  return features, label\n\n\ndef inference(inputs, input_units, output_units, is_train=True):\n  """"""\n  Define the model by model name.\n  \n  Return:\n    The logit of the model output.\n  """"""\n\n  if FLAGS.model == ""dnn"":\n    return model.dnn_inference(inputs, input_units, output_units, is_train,\n                               FLAGS)\n  elif FLAGS.model == ""lr"":\n    return model.lr_inference(inputs, input_units, output_units, is_train,\n                              FLAGS)\n  elif FLAGS.model == ""wide_and_deep"":\n    return model.wide_and_deep_inference(inputs, input_units, output_units,\n                                         is_train, FLAGS)\n  elif FLAGS.model == ""customized"":\n    return model.customized_inference(inputs, input_units, output_units,\n                                      is_train, FLAGS)\n  elif FLAGS.model == ""cnn"":\n    return model.cnn_inference(inputs, input_units, output_units, is_train,\n                               FLAGS)\n  elif FLAGS.model == ""customized_cnn"":\n    return model.customized_cnn_inference(inputs, input_units, output_units,\n                                          is_train, FLAGS)\n  elif FLAGS.model == ""lstm"":\n    return model.lstm_inference(inputs, input_units, output_units, is_train,\n                                FLAGS)\n  elif FLAGS.model == ""bidirectional_lstm"":\n    return model.bidirectional_lstm_inference(inputs, input_units,\n                                              output_units, is_train, FLAGS)\n  elif FLAGS.model == ""gru"":\n    return model.gru_inference(inputs, input_units, output_units, is_train,\n                               FLAGS)\n\n\ndef main():\n  """"""\n  Train the TensorFlow models.\n  """"""\n\n  # Get hyper-parameters\n  if os.path.exists(FLAGS.checkpoint_path) == False:\n    os.makedirs(FLAGS.checkpoint_path)\n  checkpoint_file_path = FLAGS.checkpoint_path + ""/checkpoint.ckpt""\n  latest_checkpoint_file_path = tf.train.latest_checkpoint(\n      FLAGS.checkpoint_path)\n\n  if os.path.exists(FLAGS.output_path) == False:\n    os.makedirs(FLAGS.output_path)\n\n  # Step 1: Construct the dataset op\n  epoch_number = FLAGS.epoch_number\n  if epoch_number <= 0:\n    epoch_number = -1\n  train_buffer_size = FLAGS.train_batch_size * 3\n  validation_buffer_size = FLAGS.train_batch_size * 3\n\n  train_filename_list = [filename for filename in FLAGS.train_files.split("","")]\n  train_filename_placeholder = tf.placeholder(tf.string, shape=[None])\n  if FLAGS.file_format == ""tfrecords"":\n    train_dataset = tf.data.TFRecordDataset(train_filename_placeholder)\n    train_dataset = train_dataset.map(parse_tfrecords_function).repeat(\n        epoch_number).batch(FLAGS.train_batch_size).shuffle(\n            buffer_size=train_buffer_size)\n  elif FLAGS.file_format == ""csv"":\n    # Skip the header or not\n    train_dataset = tf.data.TextLineDataset(train_filename_placeholder)\n    train_dataset = train_dataset.map(parse_csv_function).repeat(\n        epoch_number).batch(FLAGS.train_batch_size).shuffle(\n            buffer_size=train_buffer_size)\n  train_dataset_iterator = train_dataset.make_initializable_iterator()\n  train_features_op, train_label_op = train_dataset_iterator.get_next()\n\n  validation_filename_list = [\n      filename for filename in FLAGS.validation_files.split("","")\n  ]\n  validation_filename_placeholder = tf.placeholder(tf.string, shape=[None])\n  if FLAGS.file_format == ""tfrecords"":\n    validation_dataset = tf.data.TFRecordDataset(\n        validation_filename_placeholder)\n    validation_dataset = validation_dataset.map(\n        parse_tfrecords_function).repeat(epoch_number).batch(\n            FLAGS.validation_batch_size).shuffle(\n                buffer_size=validation_buffer_size)\n  elif FLAGS.file_format == ""csv"":\n    validation_dataset = tf.data.TextLineDataset(\n        validation_filename_placeholder)\n    validation_dataset = validation_dataset.map(parse_csv_function).repeat(\n        epoch_number).batch(FLAGS.validation_batch_size).shuffle(\n            buffer_size=validation_buffer_size)\n  validation_dataset_iterator = validation_dataset.make_initializable_iterator(\n  )\n  validation_features_op, validation_label_op = validation_dataset_iterator.get_next(\n  )\n\n  # Step 2: Define the model\n  input_units = FLAGS.feature_size\n  output_units = FLAGS.label_size\n  logits = inference(train_features_op, input_units, output_units, True)\n\n  if FLAGS.loss == ""sparse_cross_entropy"":\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        logits=logits, labels=train_label_op)\n    loss = tf.reduce_mean(cross_entropy, name=""loss"")\n  elif FLAGS.loss == ""cross_entropy"":\n    cross_entropy = tf.nn.cross_entropy_with_logits(\n        logits=logits, labels=train_label_op)\n    loss = tf.reduce_mean(cross_entropy, name=""loss"")\n  elif FLAGS.loss == ""mean_square"":\n    msl = tf.square(logits - train_label_op, name=""msl"")\n    loss = tf.reduce_mean(msl, name=""loss"")\n\n  global_step = tf.Variable(0, name=""global_step"", trainable=False)\n  learning_rate = FLAGS.learning_rate\n\n  if FLAGS.enable_lr_decay:\n    logging.info(\n        ""Enable learning rate decay rate: {}"".format(FLAGS.lr_decay_rate))\n    starter_learning_rate = FLAGS.learning_rate\n    learning_rate = tf.train.exponential_decay(\n        starter_learning_rate,\n        global_step,\n        100000,\n        FLAGS.lr_decay_rate,\n        staircase=True)\n\n  optimizer = util.get_optimizer_by_name(FLAGS.optimizer, learning_rate)\n  train_op = optimizer.minimize(loss, global_step=global_step)\n\n  # Need to re-use the Variables for training and validation\n  tf.get_variable_scope().reuse_variables()\n\n  # Define accuracy op and auc op for train\n  train_accuracy_logits = inference(train_features_op, input_units,\n                                    output_units, False)\n  train_softmax_op, train_accuracy_op = model.compute_softmax_and_accuracy(\n      train_accuracy_logits, train_label_op)\n  train_auc_op = model.compute_auc(train_softmax_op, train_label_op,\n                                   FLAGS.label_size)\n\n  # Define accuracy op and auc op for validation\n  validation_accuracy_logits = inference(validation_features_op, input_units,\n                                         output_units, False)\n  validation_softmax_op, validation_accuracy_op = model.compute_softmax_and_accuracy(\n      validation_accuracy_logits, validation_label_op)\n  validation_auc_op = model.compute_auc(validation_softmax_op,\n                                        validation_label_op, FLAGS.label_size)\n\n  # Define inference op\n  inference_features = tf.placeholder(\n      ""float"", [None, FLAGS.feature_size], name=""features"")\n  inference_logits = inference(inference_features, input_units, output_units,\n                               False)\n  inference_softmax_op = tf.nn.softmax(\n      inference_logits, name=""inference_softmax"")\n  inference_prediction_op = tf.argmax(\n      inference_softmax_op, 1, name=""inference_prediction"")\n  keys_placeholder = tf.placeholder(tf.int32, shape=[None, 1], name=""keys"")\n  keys_identity = tf.identity(keys_placeholder, name=""inference_keys"")\n\n  signature_def_map = {\n      signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n      signature_def_utils.build_signature_def(\n          inputs={\n              ""keys"": utils.build_tensor_info(keys_placeholder),\n              ""features"": utils.build_tensor_info(inference_features)\n          },\n          outputs={\n              ""keys"": utils.build_tensor_info(keys_identity),\n              ""prediction"": utils.build_tensor_info(inference_prediction_op),\n          },\n          method_name=""tensorflow/serving/predictss""),\n      ""serving_detail"":\n      signature_def_utils.build_signature_def(\n          inputs={\n              ""keys"": utils.build_tensor_info(keys_placeholder),\n              ""features"": utils.build_tensor_info(inference_features)\n          },\n          outputs={\n              ""keys"": utils.build_tensor_info(keys_identity),\n              ""prediction"": utils.build_tensor_info(inference_prediction_op),\n              ""softmax"": utils.build_tensor_info(inference_softmax_op),\n          },\n          method_name=""sdfas"")\n  }\n\n  # Initialize saver and summary\n  saver = tf.train.Saver()\n  tf.summary.scalar(""loss"", loss)\n  if FLAGS.scenario == ""classification"":\n    tf.summary.scalar(""train_accuracy"", train_accuracy_op)\n    tf.summary.scalar(""train_auc"", train_auc_op)\n    tf.summary.scalar(""validate_accuracy"", validation_accuracy_op)\n    tf.summary.scalar(""validate_auc"", validation_auc_op)\n  summary_op = tf.summary.merge_all()\n  init_op = [\n      tf.global_variables_initializer(),\n      tf.local_variables_initializer()\n  ]\n\n  # Step 3: Create session to run\n  with tf.Session() as sess:\n    writer = tf.summary.FileWriter(FLAGS.output_path, sess.graph)\n    sess.run(init_op)\n    sess.run(\n        [\n            train_dataset_iterator.initializer,\n            validation_dataset_iterator.initializer\n        ],\n        feed_dict={\n            train_filename_placeholder: train_filename_list,\n            validation_filename_placeholder: validation_filename_list\n        })\n\n    if FLAGS.mode == ""train"":\n      if FLAGS.resume_from_checkpoint:\n        util.restore_from_checkpoint(sess, saver, latest_checkpoint_file_path)\n\n      try:\n        start_time = datetime.datetime.now()\n\n        while True:\n          if FLAGS.enable_benchmark:\n            sess.run(train_op)\n          else:\n\n            _, global_step_value = sess.run([train_op, global_step])\n\n            # Step 4: Display training metrics after steps\n            if global_step_value % FLAGS.steps_to_validate == 0:\n              if FLAGS.scenario == ""classification"":\n                loss_value, train_accuracy_value, train_auc_value, validate_accuracy_value, validate_auc_value, summary_value = sess.run(\n                    [\n                        loss, train_accuracy_op, train_auc_op,\n                        validation_accuracy_op, validation_auc_op, summary_op\n                    ])\n                end_time = datetime.datetime.now()\n\n                logging.info(\n                    ""[{}] Step: {}, loss: {}, train_acc: {}, train_auc: {}, valid_acc: {}, valid_auc: {}"".\n                    format(end_time - start_time, global_step_value,\n                           loss_value, train_accuracy_value, train_auc_value,\n                           validate_accuracy_value, validate_auc_value))\n\n              elif FLAGS.scenario == ""regression"":\n                loss_value, summary_value = sess.run([loss, summary_op])\n                end_time = datetime.datetime.now()\n                logging.info(""[{}] Step: {}, loss: {}"".format(\n                    end_time - start_time, global_step_value, loss_value))\n\n              writer.add_summary(summary_value, global_step_value)\n              saver.save(\n                  sess, checkpoint_file_path, global_step=global_step_value)\n\n              start_time = end_time\n\n      except tf.errors.OutOfRangeError:\n        if FLAGS.enable_benchmark:\n          logging.info(""Finish training for benchmark"")\n        else:\n          # Step 5: Export the model after training\n          util.save_model(\n              FLAGS.model_path,\n              FLAGS.model_version,\n              sess,\n              signature_def_map,\n              is_save_graph=False)\n\n    elif FLAGS.mode == ""savedmodel"":\n      if util.restore_from_checkpoint(sess, saver,\n                                      latest_checkpoint_file_path) == False:\n        logging.error(""No checkpoint for exporting model, exit now"")\n        return\n\n      util.save_model(\n          FLAGS.model_path,\n          FLAGS.model_version,\n          sess,\n          signature_def_map,\n          is_save_graph=False)\n\n    elif FLAGS.mode == ""inference"":\n      if util.restore_from_checkpoint(sess, saver,\n                                      latest_checkpoint_file_path) == False:\n        logging.error(""No checkpoint for inference, exit now"")\n        return\n\n      # Load test data\n      inference_result_file_name = FLAGS.inference_result_file\n      inference_test_file_name = FLAGS.inference_data_file\n      inference_data = np.genfromtxt(inference_test_file_name, delimiter="","")\n      inference_data_features = inference_data[:, 0:9]\n      inference_data_labels = inference_data[:, 9]\n\n      # Run inference\n      start_time = datetime.datetime.now()\n      prediction, prediction_softmax = sess.run(\n          [inference_prediction_op, inference_softmax_op],\n          feed_dict={inference_features: inference_data_features})\n      end_time = datetime.datetime.now()\n\n      # Compute accuracy\n      label_number = len(inference_data_labels)\n      correct_label_number = 0\n      for i in range(label_number):\n        if inference_data_labels[i] == prediction[i]:\n          correct_label_number += 1\n      accuracy = float(correct_label_number) / label_number\n\n      # Compute auc\n      y_true = np.array(inference_data_labels)\n      y_score = prediction_softmax[:, 1]\n      fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score, pos_label=1)\n      auc = metrics.auc(fpr, tpr)\n      logging.info(""[{}] Inference accuracy: {}, auc: {}"".format(\n          end_time - start_time, accuracy, auc))\n\n      # Save result into the file\n      np.savetxt(inference_result_file_name, prediction_softmax, delimiter="","")\n      logging.info(\n          ""Save result to file: {}"".format(inference_result_file_name))\n\n\nif __name__ == ""__main__"":\n  main()\n'"
dense_classifier_use_queue.py,136,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\nimport datetime\nimport logging\nimport os\nimport pprint\n\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn import metrics\nfrom tensorflow.python.saved_model import builder as saved_model_builder\nfrom tensorflow.python.saved_model import (\n    signature_constants, signature_def_utils, tag_constants, utils)\nfrom tensorflow.python.util import compat\n\n\ndef define_flags():\n  flags = tf.app.flags\n  flags.DEFINE_boolean(""enable_colored_log"", False, ""Enable colored log"")\n  flags.DEFINE_string(""mode"", ""train"", ""Support train, inference, savedmodel"")\n  flags.DEFINE_boolean(""enable_benchmark"", False, ""Enable benchmark"")\n  flags.DEFINE_string(""scenario"", ""classification"",\n                      ""Support classification, regression"")\n  flags.DEFINE_integer(""feature_size"", 9, ""Number of feature size"")\n  flags.DEFINE_integer(""label_size"", 2, ""Number of label size"")\n  flags.DEFINE_string(""train_file_format"", ""tfrecords"",\n                      ""Support tfrecords, csv"")\n  flags.DEFINE_string(""train_file"", ""./data/cancer/cancer_train.csv.tfrecords"",\n                      ""Train files which supports glob pattern"")\n  flags.DEFINE_string(""validate_file"",\n                      ""./data/cancer/cancer_test.csv.tfrecords"",\n                      ""Validate files which supports glob pattern"")\n  flags.DEFINE_string(""inference_data_file"", ""./data/cancer/cancer_test.csv"",\n                      ""Data file for inference"")\n  flags.DEFINE_string(""inference_result_file"", ""./inference_result.txt"",\n                      ""Result file from inference"")\n  flags.DEFINE_string(""optimizer"", ""adagrad"",\n                      ""Support sgd, adadelta, adagrad, adam, ftrl, rmsprop"")\n  flags.DEFINE_float(""learning_rate"", 0.01, ""Learning rate"")\n  flags.DEFINE_string(""model"", ""dnn"",\n                      ""Support dnn, lr, wide_and_deep, customized, cnn"")\n  flags.DEFINE_string(""dnn_struct"", ""128 32 8"", ""DNN struct"")\n  flags.DEFINE_integer(""epoch_number"", 1000, ""Number of epoches"")\n  flags.DEFINE_integer(""batch_size"", 1024, ""Batch size"")\n  flags.DEFINE_integer(""validate_batch_size"", 1024,\n                       ""Batch size for validation"")\n  flags.DEFINE_integer(""batch_thread_number"", 1, ""Batch thread number"")\n  flags.DEFINE_integer(""min_after_dequeue"", 100, ""Min after dequeue"")\n  flags.DEFINE_boolean(""enable_bn"", False, ""Enable batch normalization"")\n  flags.DEFINE_float(""bn_epsilon"", 0.001, ""Epsilon of batch normalization"")\n  flags.DEFINE_boolean(""enable_dropout"", False, ""Enable dropout"")\n  flags.DEFINE_float(""dropout_keep_prob"", 0.5, ""Keep prob of dropout"")\n  flags.DEFINE_boolean(""enable_lr_decay"", False, ""Enable learning rate decay"")\n  flags.DEFINE_float(""lr_decay_rate"", 0.96, ""Learning rate decay rate"")\n  flags.DEFINE_integer(""steps_to_validate"", 10, ""Steps to validate"")\n  flags.DEFINE_string(""checkpoint_path"", ""./checkpoint/"",\n                      ""Path for checkpoint"")\n  flags.DEFINE_string(""output_path"", ""./tensorboard/"", ""Path for tensorboard"")\n  flags.DEFINE_string(""model_path"", ""./model/"", ""Path of the model"")\n  flags.DEFINE_integer(""model_version"", 1, ""Version of the model"")\n  FLAGS = flags.FLAGS\n  return FLAGS\n\n\ndef assert_flags(FLAGS):\n  if FLAGS.mode in [""train"", ""inference"", ""savedmodel""]:\n    if FLAGS.scenario in [""classification"", ""regression""]:\n      if FLAGS.train_file_format in [""tfrecords"", ""csv""]:\n        if FLAGS.optimizer in [\n            ""sgd"", ""adadelta"", ""adagrad"", ""adam"", ""ftrl"", ""rmsprop""\n        ]:\n          if FLAGS.model in [\n              ""dnn"", ""lr"", ""wide_and_deep"", ""customized"", ""cnn""\n          ]:\n            return\n\n  logging.error(""Get the unsupported parameters, exit now"")\n  exit(1)\n\n\ndef get_optimizer_by_name(optimizer_name, learning_rate):\n  logging.info(""Use the optimizer: {}"".format(optimizer_name))\n  if optimizer_name == ""sgd"":\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n  elif optimizer_name == ""adadelta"":\n    optimizer = tf.train.AdadeltaOptimizer(learning_rate)\n  elif optimizer_name == ""adagrad"":\n    optimizer = tf.train.AdagradOptimizer(learning_rate)\n  elif optimizer_name == ""adam"":\n    optimizer = tf.train.AdamOptimizer(learning_rate)\n  elif optimizer_name == ""ftrl"":\n    optimizer = tf.train.FtrlOptimizer(learning_rate)\n  elif optimizer_name == ""rmsprop"":\n    optimizer = tf.train.RMSPropOptimizer(learning_rate)\n  else:\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n  return optimizer\n\n\ndef restore_from_checkpoint(sess, saver, checkpoint):\n  if checkpoint:\n    logging.info(""Restore session from checkpoint: {}"".format(checkpoint))\n    saver.restore(sess, checkpoint)\n    return True\n  else:\n    logging.warn(""Checkpoint not found: {}"".format(checkpoint))\n    return False\n\n\ndef read_and_decode_tfrecords(filename_queue):\n  reader = tf.TFRecordReader()\n  _, serialized_example = reader.read(filename_queue)\n  examples = tf.parse_single_example(\n      serialized_example,\n      features={\n          ""label"": tf.FixedLenFeature([], tf.float32),\n          ""features"": tf.FixedLenFeature([FLAGS.feature_size], tf.float32),\n      })\n  label = examples[""label""]\n  features = examples[""features""]\n  return label, features\n\n\ndef read_and_decode_csv(filename_queue):\n  # Notice that it supports label in the last column only\n  reader = tf.TextLineReader()\n  key, value = reader.read(filename_queue)\n  record_defaults = [[1.0] for i in range(FLAGS.feature_size)] + [[0]]\n  columns = tf.decode_csv(value, record_defaults=record_defaults)\n  label = columns[-1]\n  features = tf.stack(columns[0:-1])\n  return label, features\n\n\ndef full_connect(inputs, weights_shape, biases_shape, is_train=True):\n  weights = tf.get_variable(\n      ""weights"", weights_shape, initializer=tf.random_normal_initializer())\n  biases = tf.get_variable(\n      ""biases"", biases_shape, initializer=tf.random_normal_initializer())\n  layer = tf.matmul(inputs, weights) + biases\n\n  if FLAGS.enable_bn and is_train:\n    mean, var = tf.nn.moments(layer, axes=[0])\n    scale = tf.get_variable(\n        ""scale"", biases_shape, initializer=tf.random_normal_initializer())\n    shift = tf.get_variable(\n        ""shift"", biases_shape, initializer=tf.random_normal_initializer())\n    layer = tf.nn.batch_normalization(layer, mean, var, shift, scale,\n                                      FLAGS.bn_epsilon)\n  return layer\n\n\ndef full_connect_relu(inputs, weights_shape, biases_shape, is_train=True):\n  layer = full_connect(inputs, weights_shape, biases_shape, is_train)\n  layer = tf.nn.relu(layer)\n  return layer\n\n\ndef customized_inference(inputs, input_units, output_units, is_train=True):\n  hidden1_units = 128\n  hidden2_units = 32\n  hidden3_units = 8\n\n  with tf.variable_scope(""input""):\n    layer = full_connect_relu(inputs, [input_units, hidden1_units],\n                              [hidden1_units], is_train)\n  with tf.variable_scope(""layer0""):\n    layer = full_connect_relu(layer, [hidden1_units, hidden2_units],\n                              [hidden2_units], is_train)\n  with tf.variable_scope(""layer1""):\n    layer = full_connect_relu(layer, [hidden2_units, hidden3_units],\n                              [hidden3_units], is_train)\n  if FLAGS.enable_dropout and is_train:\n    layer = tf.nn.dropout(layer, FLAGS.dropout_keep_prob)\n  with tf.variable_scope(""output""):\n    layer = full_connect(layer, [hidden3_units, output_units], [output_units],\n                         is_train)\n  return layer\n\n\ndef dnn_inference(inputs, input_units, output_units, is_train=True):\n  model_network_hidden_units = [int(i) for i in FLAGS.dnn_struct.split()]\n  with tf.variable_scope(""input""):\n    layer = full_connect_relu(inputs,\n                              [input_units, model_network_hidden_units[0]],\n                              [model_network_hidden_units[0]], is_train)\n\n  for i in range(len(model_network_hidden_units) - 1):\n    with tf.variable_scope(""layer{}"".format(i)):\n      layer = full_connect_relu(layer, [\n          model_network_hidden_units[i], model_network_hidden_units[i + 1]\n      ], [model_network_hidden_units[i + 1]], is_train)\n\n  with tf.variable_scope(""output""):\n    layer = full_connect(layer, [model_network_hidden_units[-1], output_units],\n                         [output_units], is_train)\n  return layer\n\n\ndef lr_inference(inputs, input_units, output_units, is_train=True):\n  with tf.variable_scope(""lr""):\n    layer = full_connect(inputs, [input_units, output_units], [output_units])\n  return layer\n\n\ndef wide_and_deep_inference(inputs, input_units, output_units, is_train=True):\n  return lr_inference(inputs, input_units,\n                      output_units, is_train) + dnn_inference(\n                          inputs, input_units, output_units, is_train)\n\n\ndef cnn_inference(inputs, input_units, output_units, is_train=True):\n  # TODO: Change if validate_batch_size is different\n  # [BATCH_SIZE, 512 * 512 * 1] -> [BATCH_SIZE, 512, 512, 1]\n  inputs = tf.reshape(inputs, [FLAGS.batch_size, 512, 512, 1])\n\n  # [BATCH_SIZE, 512, 512, 1] -> [BATCH_SIZE, 128, 128, 8]\n  with tf.variable_scope(""conv0""):\n    weights = tf.get_variable(\n        ""weights"", [3, 3, 1, 8], initializer=tf.random_normal_initializer())\n    bias = tf.get_variable(\n        ""bias"", [8], initializer=tf.random_normal_initializer())\n\n    layer = tf.nn.conv2d(inputs, weights, strides=[1, 1, 1, 1], padding=""SAME"")\n    layer = tf.nn.bias_add(layer, bias)\n    layer = tf.nn.relu(layer)\n    layer = tf.nn.max_pool(\n        layer, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1], padding=""SAME"")\n\n  # [BATCH_SIZE, 128, 128, 8] -> [BATCH_SIZE, 32, 32, 8]\n  with tf.variable_scope(""conv1""):\n    weights = tf.get_variable(\n        ""weights"", [3, 3, 8, 8], initializer=tf.random_normal_initializer())\n    bias = tf.get_variable(\n        ""bias"", [8], initializer=tf.random_normal_initializer())\n\n    layer = tf.nn.conv2d(layer, weights, strides=[1, 1, 1, 1], padding=""SAME"")\n    layer = tf.nn.bias_add(layer, bias)\n    layer = tf.nn.relu(layer)\n    layer = tf.nn.max_pool(\n        layer, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1], padding=""SAME"")\n\n  # [BATCH_SIZE, 32, 32, 8] -> [BATCH_SIZE, 8, 8, 8]\n  with tf.variable_scope(""conv2""):\n    weights = tf.get_variable(\n        ""weights"", [3, 3, 8, 8], initializer=tf.random_normal_initializer())\n    bias = tf.get_variable(\n        ""bias"", [8], initializer=tf.random_normal_initializer())\n\n    layer = tf.nn.conv2d(layer, weights, strides=[1, 1, 1, 1], padding=""SAME"")\n    layer = tf.nn.bias_add(layer, bias)\n    layer = tf.nn.relu(layer)\n    layer = tf.nn.max_pool(\n        layer, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1], padding=""SAME"")\n\n  # [BATCH_SIZE, 8, 8, 8] -> [BATCH_SIZE, 8 * 8 * 8]\n  layer = tf.reshape(layer, [-1, 8 * 8 * 8])\n\n  # [BATCH_SIZE, 8 * 8 * 8] -> [BATCH_SIZE, LABEL_SIZE]\n  with tf.variable_scope(""output""):\n    weights = tf.get_variable(\n        ""weights"", [8 * 8 * 8, FLAGS.label_size],\n        initializer=tf.random_normal_initializer())\n    bias = tf.get_variable(\n        ""bias"", [FLAGS.label_size], initializer=tf.random_normal_initializer())\n    layer = tf.add(tf.matmul(layer, weights), bias)\n\n  return layer\n\n\ndef inference(inputs, input_units, output_units, is_train=True):\n  if FLAGS.model == ""dnn"":\n    return dnn_inference(inputs, input_units, output_units, is_train)\n  elif FLAGS.model == ""lr"":\n    return lr_inference(inputs, input_units, output_units, is_train)\n  elif FLAGS.model == ""wide_and_deep"":\n    return wide_and_deep_inference(inputs, input_units, output_units, is_train)\n  elif FLAGS.model == ""customized"":\n    return customized_inference(inputs, input_units, output_units, is_train)\n  elif FLAGS.model == ""cnn"":\n    return cnn_inference(inputs, input_units, output_units, is_train)\n\n\nlogging.basicConfig(level=logging.INFO)\nFLAGS = define_flags()\nassert_flags(FLAGS)\n#import ipdb;ipdb.set_trace()\npprint.PrettyPrinter().pprint(FLAGS.__flags)\nif FLAGS.enable_colored_log:\n  import coloredlogs\n  coloredlogs.install()\n\n\ndef main():\n  # Get hyper-parameters\n  if os.path.exists(FLAGS.checkpoint_path) == False:\n    os.makedirs(FLAGS.checkpoint_path)\n  CHECKPOINT_FILE = FLAGS.checkpoint_path + ""/checkpoint.ckpt""\n  LATEST_CHECKPOINT = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n\n  if os.path.exists(FLAGS.output_path) == False:\n    os.makedirs(FLAGS.output_path)\n\n  EPOCH_NUMBER = FLAGS.epoch_number\n  if EPOCH_NUMBER <= 0:\n    EPOCH_NUMBER = None\n\n  BATCH_CAPACITY = FLAGS.batch_thread_number * FLAGS.batch_size + FLAGS.min_after_dequeue\n\n  if FLAGS.train_file_format == ""tfrecords"":\n    read_and_decode_function = read_and_decode_tfrecords\n  elif FLAGS.train_file_format == ""csv"":\n    read_and_decode_function = read_and_decode_csv\n\n  train_filename_queue = tf.train.string_input_producer(\n      tf.train.match_filenames_once(FLAGS.train_file), num_epochs=EPOCH_NUMBER)\n  train_label, train_features = read_and_decode_function(train_filename_queue)\n  batch_labels, batch_features = tf.train.shuffle_batch(\n      [train_label, train_features],\n      batch_size=FLAGS.batch_size,\n      num_threads=FLAGS.batch_thread_number,\n      capacity=BATCH_CAPACITY,\n      min_after_dequeue=FLAGS.min_after_dequeue)\n\n  validate_filename_queue = tf.train.string_input_producer(\n      tf.train.match_filenames_once(FLAGS.validate_file),\n      num_epochs=EPOCH_NUMBER)\n  validate_label, validate_features = read_and_decode_function(\n      validate_filename_queue)\n  validate_batch_labels, validate_batch_features = tf.train.shuffle_batch(\n      [validate_label, validate_features],\n      batch_size=FLAGS.validate_batch_size,\n      num_threads=FLAGS.batch_thread_number,\n      capacity=BATCH_CAPACITY,\n      min_after_dequeue=FLAGS.min_after_dequeue)\n\n  # Define the model\n  input_units = FLAGS.feature_size\n  output_units = FLAGS.label_size\n\n  logging.info(""Use the model: {}, model network: {}"".format(\n      FLAGS.model, FLAGS.dnn_struct))\n  logits = inference(batch_features, input_units, output_units, True)\n\n  if FLAGS.scenario == ""classification"":\n    batch_labels = tf.to_int64(batch_labels)\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        logits=logits, labels=batch_labels)\n    loss = tf.reduce_mean(cross_entropy, name=""loss"")\n  elif FLAGS.scenario == ""regression"":\n    msl = tf.square(logits - batch_labels, name=""msl"")\n    loss = tf.reduce_mean(msl, name=""loss"")\n\n  global_step = tf.Variable(0, name=""global_step"", trainable=False)\n  if FLAGS.enable_lr_decay:\n    logging.info(\n        ""Enable learning rate decay rate: {}"".format(FLAGS.lr_decay_rate))\n    starter_learning_rate = FLAGS.learning_rate\n    learning_rate = tf.train.exponential_decay(\n        starter_learning_rate,\n        global_step,\n        100000,\n        FLAGS.lr_decay_rate,\n        staircase=True)\n  else:\n    learning_rate = FLAGS.learning_rate\n  optimizer = get_optimizer_by_name(FLAGS.optimizer, learning_rate)\n  train_op = optimizer.minimize(loss, global_step=global_step)\n  tf.get_variable_scope().reuse_variables()\n\n  # Avoid error when not using acc and auc op\n  if FLAGS.scenario == ""regression"":\n    batch_labels = tf.to_int64(batch_labels)\n\n  # Define accuracy op for train data\n  train_accuracy_logits = inference(batch_features, input_units, output_units,\n                                    False)\n  train_softmax = tf.nn.softmax(train_accuracy_logits)\n  train_correct_prediction = tf.equal(\n      tf.argmax(train_softmax, 1), batch_labels)\n  train_accuracy = tf.reduce_mean(\n      tf.cast(train_correct_prediction, tf.float32))\n\n  # Define auc op for train data\n  batch_labels = tf.cast(batch_labels, tf.int32)\n  sparse_labels = tf.reshape(batch_labels, [-1, 1])\n  derived_size = tf.shape(batch_labels)[0]\n  indices = tf.reshape(tf.range(0, derived_size, 1), [-1, 1])\n  concated = tf.concat(axis=1, values=[indices, sparse_labels])\n  outshape = tf.stack([derived_size, FLAGS.label_size])\n  new_batch_labels = tf.sparse_to_dense(concated, outshape, 1.0, 0.0)\n  _, train_auc = tf.contrib.metrics.streaming_auc(train_softmax,\n                                                  new_batch_labels)\n\n  # Define accuracy op for validate data\n  validate_accuracy_logits = inference(validate_batch_features, input_units,\n                                       output_units, False)\n  validate_softmax = tf.nn.softmax(validate_accuracy_logits)\n  validate_batch_labels = tf.to_int64(validate_batch_labels)\n  validate_correct_prediction = tf.equal(\n      tf.argmax(validate_softmax, 1), validate_batch_labels)\n  validate_accuracy = tf.reduce_mean(\n      tf.cast(validate_correct_prediction, tf.float32))\n\n  # Define auc op for validate data\n  validate_batch_labels = tf.cast(validate_batch_labels, tf.int32)\n  sparse_labels = tf.reshape(validate_batch_labels, [-1, 1])\n  derived_size = tf.shape(validate_batch_labels)[0]\n  indices = tf.reshape(tf.range(0, derived_size, 1), [-1, 1])\n  concated = tf.concat(axis=1, values=[indices, sparse_labels])\n  outshape = tf.stack([derived_size, FLAGS.label_size])\n  new_validate_batch_labels = tf.sparse_to_dense(concated, outshape, 1.0, 0.0)\n  _, validate_auc = tf.contrib.metrics.streaming_auc(validate_softmax,\n                                                     new_validate_batch_labels)\n\n  # Define inference op\n  inference_features = tf.placeholder(\n      ""float"", [None, FLAGS.feature_size], name=""features"")\n  inference_logits = inference(inference_features, input_units, output_units,\n                               False)\n  inference_softmax = tf.nn.softmax(inference_logits, name=""output_softmax"")\n  inference_op = tf.argmax(inference_softmax, 1, name=""output_prediction"")\n  keys_placeholder = tf.placeholder(tf.int32, shape=[None, 1], name=""keys"")\n  keys_identity = tf.identity(keys_placeholder, name=""output_keys"")\n  model_signature = signature_def_utils.build_signature_def(\n      inputs={\n          ""keys"": utils.build_tensor_info(keys_placeholder),\n          ""features"": utils.build_tensor_info(inference_features)\n      },\n      outputs={\n          ""keys"": utils.build_tensor_info(keys_identity),\n          ""prediction"": utils.build_tensor_info(inference_op),\n          ""softmax"": utils.build_tensor_info(inference_softmax),\n      },\n      method_name=signature_constants.PREDICT_METHOD_NAME)\n\n  # Initialize saver and summary\n  saver = tf.train.Saver()\n  tf.summary.scalar(""loss"", loss)\n  if FLAGS.scenario == ""classification"":\n    tf.summary.scalar(""train_accuracy"", train_accuracy)\n    tf.summary.scalar(""train_auc"", train_auc)\n    tf.summary.scalar(""validate_accuracy"", validate_accuracy)\n    tf.summary.scalar(""validate_auc"", validate_auc)\n  summary_op = tf.summary.merge_all()\n  init_op = [\n      tf.global_variables_initializer(),\n      tf.local_variables_initializer()\n  ]\n\n  # Create session to run\n  with tf.Session() as sess:\n    writer = tf.summary.FileWriter(FLAGS.output_path, sess.graph)\n    sess.run(init_op)\n\n    if FLAGS.mode == ""train"":\n      # Restore session and start queue runner\n      restore_from_checkpoint(sess, saver, LATEST_CHECKPOINT)\n      coord = tf.train.Coordinator()\n      threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n      start_time = datetime.datetime.now()\n\n      try:\n        while not coord.should_stop():\n          if FLAGS.enable_benchmark:\n            sess.run(train_op)\n          else:\n            _, step = sess.run([train_op, global_step])\n\n            # Print state while training\n            if step % FLAGS.steps_to_validate == 0:\n              if FLAGS.scenario == ""classification"":\n                loss_value, train_accuracy_value, train_auc_value, validate_accuracy_value, validate_auc_value, summary_value = sess.run(\n                    [\n                        loss, train_accuracy, train_auc, validate_accuracy,\n                        validate_auc, summary_op\n                    ])\n                end_time = datetime.datetime.now()\n                logging.info(\n                    ""[{}] Step: {}, loss: {}, train_acc: {}, train_auc: {}, valid_acc: {}, valid_auc: {}"".\n                    format(end_time - start_time, step, loss_value,\n                           train_accuracy_value, train_auc_value,\n                           validate_accuracy_value, validate_auc_value))\n              elif FLAGS.scenario == ""regression"":\n                loss_value, summary_value = sess.run([loss, summary_op])\n                end_time = datetime.datetime.now()\n                logging.info(""[{}] Step: {}, loss: {}"".format(\n                    end_time - start_time, step, loss_value))\n\n              writer.add_summary(summary_value, step)\n              saver.save(sess, CHECKPOINT_FILE, global_step=step)\n              #saver.save(sess, CHECKPOINT_FILE)\n              start_time = end_time\n      except tf.errors.OutOfRangeError:\n        if FLAGS.enable_benchmark:\n          print(""Finish training for benchmark"")\n          exit(0)\n        else:\n          # Export the model after training\n          print(""Do not export the model yet"")\n\n      finally:\n        coord.request_stop()\n      coord.join(threads)\n\n    elif FLAGS.mode == ""savedmodel"":\n      if restore_from_checkpoint(sess, saver, LATEST_CHECKPOINT) == False:\n        logging.error(""No checkpoint for exporting model, exit now"")\n        exit(1)\n\n      graph_file_name = ""graph.pb""\n      logging.info(""Export the graph to: {}"".format(FLAGS.model_path))\n      tf.train.write_graph(\n          sess.graph_def, FLAGS.model_path, graph_file_name, as_text=False)\n\n      export_path = os.path.join(\n          compat.as_bytes(FLAGS.model_path),\n          compat.as_bytes(str(FLAGS.model_version)))\n      logging.info(""Export the model to {}"".format(export_path))\n\n      try:\n        legacy_init_op = tf.group(\n            tf.tables_initializer(), name=\'legacy_init_op\')\n        builder = saved_model_builder.SavedModelBuilder(export_path)\n        builder.add_meta_graph_and_variables(\n            sess, [tag_constants.SERVING],\n            clear_devices=True,\n            signature_def_map={\n                signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n                model_signature,\n            },\n            legacy_init_op=legacy_init_op)\n\n        builder.save()\n      except Exception as e:\n        logging.error(""Fail to export saved model, exception: {}"".format(e))\n\n    elif FLAGS.mode == ""inference"":\n      if restore_from_checkpoint(sess, saver, LATEST_CHECKPOINT) == False:\n        logging.error(""No checkpoint for inferencing, exit now"")\n        exit(1)\n\n      # Load inference test data\n      inference_result_file_name = FLAGS.inference_result_file\n      inference_test_file_name = FLAGS.inference_data_file\n      inference_data = np.genfromtxt(inference_test_file_name, delimiter="","")\n      inference_data_features = inference_data[:, 0:9]\n      inference_data_labels = inference_data[:, 9]\n\n      # Run inference\n      start_time = datetime.datetime.now()\n      prediction, prediction_softmax = sess.run(\n          [inference_op, inference_softmax],\n          feed_dict={inference_features: inference_data_features})\n      end_time = datetime.datetime.now()\n\n      # Compute accuracy\n      label_number = len(inference_data_labels)\n      correct_label_number = 0\n      for i in range(label_number):\n        if inference_data_labels[i] == prediction[i]:\n          correct_label_number += 1\n      accuracy = float(correct_label_number) / label_number\n\n      # Compute auc\n      y_true = np.array(inference_data_labels)\n      y_score = prediction_softmax[:, 1]\n      fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score, pos_label=1)\n      auc = metrics.auc(fpr, tpr)\n      logging.info(""[{}] Inference accuracy: {}, auc: {}"".format(\n          end_time - start_time, accuracy, auc))\n\n      # Save result into the file\n      np.savetxt(inference_result_file_name, prediction_softmax, delimiter="","")\n      logging.info(\n          ""Save result to file: {}"".format(inference_result_file_name))\n\n\nif __name__ == ""__main__"":\n  main()\n\n'"
model.py,106,"b'from __future__ import absolute_import, division, print_function\n\nimport tensorflow as tf\n\n\ndef full_connect(inputs,\n                 weights_shape,\n                 biases_shape,\n                 is_train=True,\n                 FLAGS=None):\n  """"""\n    Define full-connect layer with reused Variables.\n    """"""\n\n  weights = tf.get_variable(\n      ""weights"", weights_shape, initializer=tf.random_normal_initializer())\n  biases = tf.get_variable(\n      ""biases"", biases_shape, initializer=tf.random_normal_initializer())\n  layer = tf.matmul(inputs, weights) + biases\n\n  if FLAGS.enable_bn and is_train:\n    mean, var = tf.nn.moments(layer, axes=[0])\n    scale = tf.get_variable(\n        ""scale"", biases_shape, initializer=tf.random_normal_initializer())\n    shift = tf.get_variable(\n        ""shift"", biases_shape, initializer=tf.random_normal_initializer())\n    layer = tf.nn.batch_normalization(layer, mean, var, shift, scale,\n                                      FLAGS.bn_epsilon)\n  return layer\n\n\ndef full_connect_relu(inputs,\n                      weights_shape,\n                      biases_shape,\n                      is_train=True,\n                      FLAGS=None):\n  """"""\n    Define full-connect layer and activation function with reused Variables.\n    """"""\n\n  layer = full_connect(inputs, weights_shape, biases_shape, is_train, FLAGS)\n  layer = tf.nn.relu(layer)\n  return layer\n\n\ndef customized_inference(inputs,\n                         input_units,\n                         output_units,\n                         is_train=True,\n                         FLAGS=None):\n  """"""\n    Define the customed model.\n    """"""\n\n  hidden1_units = 128\n  hidden2_units = 32\n  hidden3_units = 8\n\n  with tf.variable_scope(""input_layer""):\n    layer = full_connect_relu(inputs, [input_units, hidden1_units],\n                              [hidden1_units], is_train, FLAGS)\n  with tf.variable_scope(""layer_0""):\n    layer = full_connect_relu(layer, [hidden1_units, hidden2_units],\n                              [hidden2_units], is_train, FLAGS)\n  with tf.variable_scope(""layer_1""):\n    layer = full_connect_relu(layer, [hidden2_units, hidden3_units],\n                              [hidden3_units], is_train, FLAGS)\n  if FLAGS.enable_dropout and is_train:\n    layer = tf.nn.dropout(layer, FLAGS.dropout_keep_prob)\n  with tf.variable_scope(""output_layer""):\n    layer = full_connect(layer, [hidden3_units, output_units], [output_units],\n                         is_train, FLAGS)\n  return layer\n\n\ndef dnn_inference(inputs, input_units, output_units, is_train=True,\n                  FLAGS=None):\n  """"""\n    Define the DNN model.\n    """"""\n\n  # Example: [128, 64, 32, 16]\n  model_network_hidden_units = [int(i) for i in FLAGS.dnn_struct.split()]\n  with tf.variable_scope(""input_layer""):\n    layer = full_connect_relu(inputs,\n                              [input_units, model_network_hidden_units[0]],\n                              [model_network_hidden_units[0]], is_train, FLAGS)\n\n  for i in range(len(model_network_hidden_units) - 1):\n    with tf.variable_scope(""layer_{}"".format(i)):\n      layer = full_connect_relu(layer, [\n          model_network_hidden_units[i], model_network_hidden_units[i + 1]\n      ], [model_network_hidden_units[i + 1]], is_train, FLAGS)\n\n  with tf.variable_scope(""output_layer""):\n    layer = full_connect(layer, [model_network_hidden_units[-1], output_units],\n                         [output_units], is_train, FLAGS)\n  return layer\n\n\ndef lr_inference(inputs, input_units, output_units, is_train=True, FLAGS=None):\n  """"""\n    Define the linear regression model.\n    """"""\n\n  with tf.variable_scope(""lr""):\n    layer = full_connect(inputs, [input_units, output_units], [output_units],\n                         FLAGS)\n  return layer\n\n\ndef wide_and_deep_inference(inputs,\n                            input_units,\n                            output_units,\n                            is_train=True,\n                            FLAGS=None):\n  """"""\n    Define the wide-and-deep model.\n    """"""\n\n  return lr_inference(inputs, input_units,\n                      output_units, is_train, FLAGS) + dnn_inference(\n                          inputs, input_units, output_units, is_train, FLAGS)\n\n\ndef cnn_inference(inputs, input_units, output_units, is_train=True,\n                  FLAGS=None):\n  """"""\n    Define the CNN model.\n    """"""\n\n  # [BATCH_SIZE, 9] -> [BATCH_SIZE, 3, 3, 1]\n  inputs = tf.reshape(inputs, [-1, 3, 3, 1])\n\n  # [BATCH_SIZE, 3, 3, 1] -> [BATCH_SIZE, 3, 3, 8]\n  with tf.variable_scope(""conv_0""):\n    weights = tf.get_variable(\n        ""weights"", [3, 3, 1, 8], initializer=tf.random_normal_initializer())\n    bias = tf.get_variable(\n        ""bias"", [8], initializer=tf.random_normal_initializer())\n\n    layer = tf.nn.conv2d(inputs, weights, strides=[1, 1, 1, 1], padding=""SAME"")\n    layer = tf.nn.bias_add(layer, bias)\n    layer = tf.nn.relu(layer)\n\n  # [BATCH_SIZE, 3, 3, 8] -> [BATCH_SIZE, 3 * 3 * 8]\n  layer = tf.reshape(layer, [-1, 3 * 3 * 8])\n\n  # [BATCH_SIZE, 3 * 3 * 8] -> [BATCH_SIZE, LABEL_SIZE]\n  with tf.variable_scope(""output_layer""):\n    weights = tf.get_variable(\n        ""weights"", [3 * 3 * 8, FLAGS.label_size],\n        initializer=tf.random_normal_initializer())\n    bias = tf.get_variable(\n        ""bias"", [FLAGS.label_size], initializer=tf.random_normal_initializer())\n    layer = tf.add(tf.matmul(layer, weights), bias)\n\n  return layer\n\n\ndef customized_cnn_inference(inputs,\n                             input_units,\n                             output_units,\n                             is_train=True,\n                             FLAGS=None):\n  """"""\n    Define the CNN model.\n    """"""\n\n  # TODO: Change if validate_batch_size is different\n  # [BATCH_SIZE, 512 * 512 * 1] -> [BATCH_SIZE, 512, 512, 1]\n  inputs = tf.reshape(inputs, [FLAGS.train_batch_size, 512, 512, 1])\n\n  # [BATCH_SIZE, 512, 512, 1] -> [BATCH_SIZE, 128, 128, 8]\n  with tf.variable_scope(""conv0""):\n    weights = tf.get_variable(\n        ""weights"", [3, 3, 1, 8], initializer=tf.random_normal_initializer())\n    bias = tf.get_variable(\n        ""bias"", [8], initializer=tf.random_normal_initializer())\n\n    layer = tf.nn.conv2d(inputs, weights, strides=[1, 1, 1, 1], padding=""SAME"")\n    layer = tf.nn.bias_add(layer, bias)\n    layer = tf.nn.relu(layer)\n    layer = tf.nn.max_pool(\n        layer, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1], padding=""SAME"")\n\n  # [BATCH_SIZE, 128, 128, 8] -> [BATCH_SIZE, 32, 32, 8]\n  with tf.variable_scope(""conv1""):\n    weights = tf.get_variable(\n        ""weights"", [3, 3, 8, 8], initializer=tf.random_normal_initializer())\n    bias = tf.get_variable(\n        ""bias"", [8], initializer=tf.random_normal_initializer())\n\n    layer = tf.nn.conv2d(layer, weights, strides=[1, 1, 1, 1], padding=""SAME"")\n    layer = tf.nn.bias_add(layer, bias)\n    layer = tf.nn.relu(layer)\n    layer = tf.nn.max_pool(\n        layer, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1], padding=""SAME"")\n\n  # [BATCH_SIZE, 32, 32, 8] -> [BATCH_SIZE, 8, 8, 8]\n  with tf.variable_scope(""conv2""):\n    weights = tf.get_variable(\n        ""weights"", [3, 3, 8, 8], initializer=tf.random_normal_initializer())\n    bias = tf.get_variable(\n        ""bias"", [8], initializer=tf.random_normal_initializer())\n\n    layer = tf.nn.conv2d(layer, weights, strides=[1, 1, 1, 1], padding=""SAME"")\n    layer = tf.nn.bias_add(layer, bias)\n    layer = tf.nn.relu(layer)\n    layer = tf.nn.max_pool(\n        layer, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1], padding=""SAME"")\n\n  # [BATCH_SIZE, 8, 8, 8] -> [BATCH_SIZE, 8 * 8 * 8]\n  layer = tf.reshape(layer, [-1, 8 * 8 * 8])\n\n  # [BATCH_SIZE, 8 * 8 * 8] -> [BATCH_SIZE, LABEL_SIZE]\n  with tf.variable_scope(""output""):\n    weights = tf.get_variable(\n        ""weights"", [8 * 8 * 8, FLAGS.label_size],\n        initializer=tf.random_normal_initializer())\n    bias = tf.get_variable(\n        ""bias"", [FLAGS.label_size], initializer=tf.random_normal_initializer())\n    layer = tf.add(tf.matmul(layer, weights), bias)\n\n  return layer\n\n\ndef lstm_inference(inputs,\n                   input_units,\n                   output_units,\n                   is_train=True,\n                   FLAGS=None):\n\n  RNN_HIDDEN_UNITS = 128\n  timesteps = 3\n  number_input = 3\n\n  weights = tf.Variable(tf.random_normal([RNN_HIDDEN_UNITS, output_units]))\n  biases = tf.Variable(tf.random_normal([output_units]))\n\n  #  [BATCH_SIZE, 9] -> [BATCH_SIZE, 3, 3]\n  x = tf.reshape(inputs, [-1, timesteps, number_input])\n\n  # [BATCH_SIZE, 3, 3] -> 3 * [BATCH_SIZE, 3]\n  x = tf.unstack(x, timesteps, 1)\n\n  # output size is 128, state size is (c=128, h=128)\n  lstm_cell = tf.contrib.rnn.BasicLSTMCell(RNN_HIDDEN_UNITS, forget_bias=1.0)\n\n  # outputs is array of 3 * [BATCH_SIZE, 3]\n  outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n\n  # outputs[-1] is [BATCH_SIZE, 3]\n  layer = tf.matmul(outputs[-1], weights) + biases\n  return layer\n\n\ndef bidirectional_lstm_inference(inputs,\n                                 input_units,\n                                 output_units,\n                                 is_train=True,\n                                 FLAGS=None):\n\n  RNN_HIDDEN_UNITS = 128\n  timesteps = 3\n  number_input = 3\n\n  weights = tf.Variable(tf.random_normal([RNN_HIDDEN_UNITS, output_units]))\n  biases = tf.Variable(tf.random_normal([output_units]))\n\n  #  [BATCH_SIZE, 9] -> [BATCH_SIZE, 3, 3]\n  x = tf.reshape(inputs, [-1, timesteps, number_input])\n\n  # [BATCH_SIZE, 3, 3] -> 3 * [BATCH_SIZE, 3]\n  x = tf.unstack(x, timesteps, 1)\n\n  # Update the hidden units for bidirection-rnn\n  fw_lstm_cell = tf.contrib.rnn.BasicLSTMCell(\n      RNN_HIDDEN_UNITS / 2, forget_bias=1.0)\n  bw_lstm_cell = tf.contrib.rnn.BasicLSTMCell(\n      RNN_HIDDEN_UNITS / 2, forget_bias=1.0)\n\n  outputs, _, _ = tf.contrib.rnn.static_bidirectional_rnn(\n      fw_lstm_cell, bw_lstm_cell, x, dtype=tf.float32)\n\n  # outputs[-1] is [BATCH_SIZE, 3]\n  layer = tf.matmul(outputs[-1], weights) + biases\n  return layer\n\n\ndef gru_inference(inputs, input_units, output_units, is_train=True,\n                  FLAGS=None):\n\n  RNN_HIDDEN_UNITS = 128\n  timesteps = 3\n  number_input = 3\n\n  weights = tf.Variable(tf.random_normal([RNN_HIDDEN_UNITS, output_units]))\n  biases = tf.Variable(tf.random_normal([output_units]))\n\n  #  [BATCH_SIZE, 9] -> [BATCH_SIZE, 3, 3]\n  x = tf.reshape(inputs, [-1, timesteps, number_input])\n\n  # [BATCH_SIZE, 3, 3] -> 3 * [BATCH_SIZE, 3]\n  x = tf.unstack(x, timesteps, 1)\n\n  # output size is 128, state size is (c=128, h=128)\n  lstm_cell = tf.contrib.rnn.GRUCell(RNN_HIDDEN_UNITS)\n\n  # outputs is array of 3 * [BATCH_SIZE, 3]\n  outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n\n  # outputs[-1] is [BATCH_SIZE, 3]\n  layer = tf.matmul(outputs[-1], weights) + biases\n  return layer\n\n\ndef compute_softmax_and_accuracy(logits, labels):\n  """"""\n  Compute the softmax and accuracy of the logits and labels.\n  \n  Args:\n    logits: The logits from the model.\n    labels: The labels.\n  \n  Return:\n    The softmax op and accuracy op.\n  """"""\n  softmax_op = tf.nn.softmax(logits)\n  correct_prediction_op = tf.equal(tf.argmax(softmax_op, 1), labels)\n  accuracy_op = tf.reduce_mean(tf.cast(correct_prediction_op, tf.float32))\n\n  return softmax_op, accuracy_op\n\n\ndef compute_auc(softmax_op, label_op, label_size):\n  """"""\n  Compute the auc of the softmax result and labels.\n  \n  Args:\n    softmax_op: The softmax op.\n    label_op: The label op.\n    label_size: The label size.\n   \n  Return:\n    The auc op.\n  """"""\n\n  batch_labels = tf.cast(label_op, tf.int32)\n  sparse_labels = tf.reshape(batch_labels, [-1, 1])\n  derived_size = tf.shape(batch_labels)[0]\n  indices = tf.reshape(tf.range(0, derived_size, 1), [-1, 1])\n  concated = tf.concat(axis=1, values=[indices, sparse_labels])\n  outshape = tf.stack([derived_size, label_size])\n  new_batch_labels = tf.sparse_to_dense(concated, outshape, 1.0, 0.0)\n  _, auc_op = tf.contrib.metrics.streaming_auc(softmax_op, new_batch_labels)\n\n  return auc_op\n'"
setup.py,0,"b""import setuptools\n\nsetuptools.setup(name='trainer', version='1.0', packages=['trainer'])\n"""
sparse_classifier.py,74,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\nfrom __future__ import absolute_import, division, print_function\n\nimport datetime\nimport logging\nimport os\nimport pprint\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn import metrics\nfrom tensorflow.python.saved_model import (\n    signature_constants, signature_def_utils, tag_constants, utils)\n\nimport sparse_model\nimport util\n\nlogging.basicConfig(\n    format=\'%(asctime)s %(levelname)-8s %(message)s\',\n    level=logging.INFO,\n    datefmt=\'%Y-%m-%d %H:%M:%S\')\n\n\ndef define_flags():\n  """"""\n    Define all the command-line parameters.\n    \n    Return:\n      The FLAGS object.\n    """"""\n\n  flags = tf.app.flags\n  flags.DEFINE_string(""train_files"", ""./data/a8a/a8a_train.libsvm.tfrecords"",\n                      ""The glob pattern of train TFRecords files"")\n  flags.DEFINE_string(""validation_files"",\n                      ""./data/a8a/a8a_test.libsvm.tfrecords"",\n                      ""The glob pattern of train TFRecords files"")\n  flags.DEFINE_integer(""feature_size"", 124, ""Number of feature size"")\n  flags.DEFINE_integer(""label_size"", 2, ""Number of label size"")\n  flags.DEFINE_string(""label_type"", ""int"", ""The type of label"")\n  flags.DEFINE_float(""learning_rate"", 0.01, ""The learning rate"")\n  flags.DEFINE_integer(""epoch_number"", 10, ""Number of epochs to train"")\n  flags.DEFINE_integer(""batch_size"", 1024, ""The batch size of training"")\n  flags.DEFINE_integer(""train_batch_size"", 64, ""The batch size of training"")\n  flags.DEFINE_integer(""validation_batch_size"", 64,\n                       ""The batch size of training"")\n  flags.DEFINE_integer(""validate_batch_size"", 1024,\n                       ""The batch size of validation"")\n  flags.DEFINE_integer(""batch_thread_number"", 1,\n                       ""Number of threads to read data"")\n  flags.DEFINE_integer(""min_after_dequeue"", 100,\n                       ""The minimal number after dequeue"")\n  flags.DEFINE_string(""checkpoint_path"", ""./sparse_checkpoint/"",\n                      ""The path of checkpoint"")\n  flags.DEFINE_string(""output_path"", ""./sparse_tensorboard/"",\n                      ""The path of tensorboard event files"")\n  flags.DEFINE_string(""model"", ""dnn"", ""Support dnn, lr, wide_and_deep"")\n  flags.DEFINE_string(""model_network"", ""128 32 8"",\n                      ""The neural network of model"")\n  flags.DEFINE_boolean(""enable_bn"", False, ""Enable batch normalization or not"")\n  flags.DEFINE_float(""bn_epsilon"", 0.001, ""The epsilon of batch normalization"")\n  flags.DEFINE_boolean(""enable_dropout"", False, ""Enable dropout or not"")\n  flags.DEFINE_float(""dropout_keep_prob"", 0.5, ""The dropout keep prob"")\n  flags.DEFINE_boolean(""enable_lr_decay"", False, ""Enable learning rate decay"")\n  flags.DEFINE_float(""lr_decay_rate"", 0.96, ""Learning rate decay rate"")\n  flags.DEFINE_string(""optimizer"", ""adagrad"", ""The optimizer to train"")\n  flags.DEFINE_integer(""steps_to_validate"", 10,\n                       ""Steps to validate and print state"")\n  flags.DEFINE_string(""mode"", ""train"", ""Support train, export, inference"")\n  flags.DEFINE_string(""saved_model_path"", ""./sparse_saved_model/"",\n                      ""The path of the saved model"")\n  flags.DEFINE_string(""model_path"", ""./sparse_model/"", ""The path of the model"")\n  flags.DEFINE_integer(""model_version"", 1, ""The version of the model"")\n  flags.DEFINE_string(""inference_test_file"", ""./data/a8a_test.libsvm"",\n                      ""The test file for inference"")\n  flags.DEFINE_string(""inference_result_file"", ""./inference_result.txt"",\n                      ""The result file from inference"")\n  flags.DEFINE_boolean(""benchmark_mode"", False,\n                       ""Reduce extra computation in benchmark mode"")\n  FLAGS = flags.FLAGS\n\n  # Check parameters\n  assert (FLAGS.optimizer in [\n      ""sgd"", ""adadelta"", ""adagrad"", ""adam"", ""ftrl"", ""rmsprop""\n  ])\n\n  # Print flags\n  FLAGS.mode\n  parameter_value_map = {}\n  for key in FLAGS.__flags.keys():\n    parameter_value_map[key] = FLAGS.__flags[key].value\n  pprint.PrettyPrinter().pprint(parameter_value_map)\n  return FLAGS\n\n\nFLAGS = define_flags()\n\n\ndef parse_tfrecords_function(example_proto):\n  """"""\n    Decode TFRecords for Dataset.\n    \n    Args:\n      example_proto: TensorFlow ExampleProto object. \n    \n    Return:\n      The op of features and labels\n    """"""\n\n  if FLAGS.label_type == ""int"":\n    features = {\n        ""ids"": tf.VarLenFeature(tf.int64),\n        ""values"": tf.VarLenFeature(tf.float32),\n        ""label"": tf.FixedLenFeature([], tf.int64, default_value=0)\n    }\n\n    parsed_features = tf.parse_single_example(example_proto, features)\n    labels = parsed_features[""label""]\n    ids = parsed_features[""ids""]\n    values = parsed_features[""values""]\n\n  elif FLAGS.label_type == ""float"":\n    features = {\n        ""ids"": tf.VarLenFeature(tf.int64),\n        ""values"": tf.VarLenFeature(tf.float32),\n        ""label"": tf.FixedLenFeature([], tf.float32, default_value=0)\n    }\n\n    parsed_features = tf.parse_single_example(example_proto, features)\n    labels = tf.cast(parsed_features[""label""], tf.int32)\n    ids = parsed_features[""ids""]\n    values = parsed_features[""values""]\n\n  return labels, ids, values\n\n\ndef inference(sparse_ids, sparse_values, is_train=True):\n  """"""\n    Define the model by model name.\n    \n    Return:\n      The logit of the model output.\n    """"""\n\n  if FLAGS.model == ""dnn"":\n    return sparse_model.dnn_inference(sparse_ids, sparse_values, is_train,\n                                      FLAGS)\n  elif FLAGS.model == ""lr"":\n    return sparse_model.lr_inference(sparse_ids, sparse_values, is_train,\n                                     FLAGS)\n  elif FLAGS.model == ""wide_and_deep"":\n    return sparse_model.wide_and_deep_inference(sparse_ids, sparse_values,\n                                                is_train, FLAGS)\n  elif FLAGS.model == ""customized"":\n    return sparse_model.customized_inference(sparse_ids, sparse_values,\n                                             is_train, FLAGS)\n\n\ndef main():\n\n  if os.path.exists(FLAGS.checkpoint_path) == False:\n    os.makedirs(FLAGS.checkpoint_path)\n  checkpoint_file_path = FLAGS.checkpoint_path + ""/checkpoint.ckpt""\n  latest_checkpoint_file_path = tf.train.latest_checkpoint(\n      FLAGS.checkpoint_path)\n\n  if os.path.exists(FLAGS.output_path) == False:\n    os.makedirs(FLAGS.output_path)\n\n  # Step 1: Construct the dataset op\n  epoch_number = FLAGS.epoch_number\n  if epoch_number <= 0:\n    epoch_number = -1\n  train_buffer_size = FLAGS.train_batch_size * 3\n  validation_buffer_size = FLAGS.train_batch_size * 3\n\n  train_filename_list = [filename for filename in FLAGS.train_files.split("","")]\n  train_filename_placeholder = tf.placeholder(tf.string, shape=[None])\n  train_dataset = tf.data.TFRecordDataset(train_filename_placeholder)\n  train_dataset = train_dataset.map(parse_tfrecords_function).repeat(\n      epoch_number).batch(FLAGS.train_batch_size).shuffle(\n          buffer_size=train_buffer_size)\n  train_dataset_iterator = train_dataset.make_initializable_iterator()\n  batch_labels, batch_ids, batch_values = train_dataset_iterator.get_next()\n\n  validation_filename_list = [\n      filename for filename in FLAGS.validation_files.split("","")\n  ]\n  validation_filename_placeholder = tf.placeholder(tf.string, shape=[None])\n  validation_dataset = tf.data.TFRecordDataset(validation_filename_placeholder)\n  validation_dataset = validation_dataset.map(parse_tfrecords_function).repeat(\n  ).batch(FLAGS.validation_batch_size).shuffle(\n      buffer_size=validation_buffer_size)\n  validation_dataset_iterator = validation_dataset.make_initializable_iterator(\n  )\n  validation_labels, validation_ids, validation_values = validation_dataset_iterator.get_next(\n  )\n\n  # Define the model\n  logits = inference(batch_ids, batch_values, True)\n  batch_labels = tf.to_int64(batch_labels)\n  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n      logits=logits, labels=batch_labels)\n  loss = tf.reduce_mean(cross_entropy, name=""loss"")\n  global_step = tf.Variable(0, name=""global_step"", trainable=False)\n  if FLAGS.enable_lr_decay:\n    logging.info(\n        ""Enable learning rate decay rate: {}"".format(FLAGS.lr_decay_rate))\n    starter_learning_rate = FLAGS.learning_rate\n    learning_rate = tf.train.exponential_decay(\n        starter_learning_rate,\n        global_step,\n        100000,\n        FLAGS.lr_decay_rate,\n        staircase=True)\n  else:\n    learning_rate = FLAGS.learning_rate\n  optimizer = util.get_optimizer_by_name(FLAGS.optimizer, learning_rate)\n  train_op = optimizer.minimize(loss, global_step=global_step)\n  tf.get_variable_scope().reuse_variables()\n\n  # Define accuracy op for train data\n  train_accuracy_logits = inference(batch_ids, batch_values, False)\n  train_softmax = tf.nn.softmax(train_accuracy_logits)\n  train_correct_prediction = tf.equal(\n      tf.argmax(train_softmax, 1), batch_labels)\n  train_accuracy = tf.reduce_mean(\n      tf.cast(train_correct_prediction, tf.float32))\n\n  # Define auc op for train data\n  batch_labels = tf.cast(batch_labels, tf.int32)\n  sparse_labels = tf.reshape(batch_labels, [-1, 1])\n  derived_size = tf.shape(batch_labels)[0]\n  indices = tf.reshape(tf.range(0, derived_size, 1), [-1, 1])\n  concated = tf.concat(axis=1, values=[indices, sparse_labels])\n  outshape = tf.stack([derived_size, FLAGS.label_size])\n  new_train_batch_labels = tf.sparse_to_dense(concated, outshape, 1.0, 0.0)\n  _, train_auc = tf.contrib.metrics.streaming_auc(train_softmax,\n                                                  new_train_batch_labels)\n\n  # Define accuracy op for validate data\n  validate_accuracy_logits = inference(validation_ids, validation_values,\n                                       False)\n  validate_softmax = tf.nn.softmax(validate_accuracy_logits)\n  validate_batch_labels = tf.to_int64(validation_labels)\n  validate_correct_prediction = tf.equal(\n      tf.argmax(validate_softmax, 1), validate_batch_labels)\n  validate_accuracy = tf.reduce_mean(\n      tf.cast(validate_correct_prediction, tf.float32))\n\n  # Define auc op for validate data\n  validate_batch_labels = tf.cast(validate_batch_labels, tf.int32)\n  sparse_labels = tf.reshape(validate_batch_labels, [-1, 1])\n  derived_size = tf.shape(validate_batch_labels)[0]\n  indices = tf.reshape(tf.range(0, derived_size, 1), [-1, 1])\n  concated = tf.concat(axis=1, values=[indices, sparse_labels])\n  outshape = tf.stack([derived_size, FLAGS.label_size])\n  new_validate_batch_labels = tf.sparse_to_dense(concated, outshape, 1.0, 0.0)\n  _, validate_auc = tf.contrib.metrics.streaming_auc(validate_softmax,\n                                                     new_validate_batch_labels)\n\n  # Define inference op\n  sparse_index = tf.placeholder(tf.int64, [None, 2])\n  sparse_ids = tf.placeholder(tf.int64, [None])\n  sparse_values = tf.placeholder(tf.float32, [None])\n  sparse_shape = tf.placeholder(tf.int64, [2])\n  inference_ids = tf.SparseTensor(sparse_index, sparse_ids, sparse_shape)\n  inference_values = tf.SparseTensor(sparse_index, sparse_values, sparse_shape)\n  inference_logits = inference(inference_ids, inference_values, False)\n  inference_softmax = tf.nn.softmax(inference_logits)\n  inference_op = tf.argmax(inference_softmax, 1)\n  keys_placeholder = tf.placeholder(tf.int32, shape=[None, 1])\n  keys = tf.identity(keys_placeholder)\n\n  signature_def_map = {\n      signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n      signature_def_utils.build_signature_def(\n          inputs={\n              ""keys"": utils.build_tensor_info(keys_placeholder),\n              ""indexs"": utils.build_tensor_info(sparse_index),\n              ""ids"": utils.build_tensor_info(sparse_ids),\n              ""values"": utils.build_tensor_info(sparse_values),\n              ""shape"": utils.build_tensor_info(sparse_shape)\n          },\n          outputs={\n              ""keys"": utils.build_tensor_info(keys),\n              ""softmax"": utils.build_tensor_info(inference_softmax),\n              ""prediction"": utils.build_tensor_info(inference_op)\n          },\n          method_name=signature_constants.PREDICT_METHOD_NAME)\n  }\n\n  # Initialize saver and summary\n  saver = tf.train.Saver()\n  tf.summary.scalar(""loss"", loss)\n  tf.summary.scalar(""train_accuracy"", train_accuracy)\n  tf.summary.scalar(""train_auc"", train_auc)\n  tf.summary.scalar(""validate_accuracy"", validate_accuracy)\n  tf.summary.scalar(""validate_auc"", validate_auc)\n  summary_op = tf.summary.merge_all()\n  init_op = [\n      tf.global_variables_initializer(),\n      tf.local_variables_initializer()\n  ]\n\n  # Create session to run\n  with tf.Session() as sess:\n    writer = tf.summary.FileWriter(FLAGS.output_path, sess.graph)\n    sess.run(init_op)\n    sess.run(\n        train_dataset_iterator.initializer,\n        feed_dict={train_filename_placeholder: train_filename_list})\n    sess.run(\n        validation_dataset_iterator.initializer,\n        feed_dict={validation_filename_placeholder: validation_filename_list})\n\n    if FLAGS.mode == ""train"":\n      # Restore session and start queue runner\n      util.restore_from_checkpoint(sess, saver, latest_checkpoint_file_path)\n      coord = tf.train.Coordinator()\n      threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n      start_time = datetime.datetime.now()\n\n      try:\n        while not coord.should_stop():\n          if FLAGS.benchmark_mode:\n            sess.run(train_op)\n          else:\n            _, step = sess.run([train_op, global_step])\n\n            # Print state while training\n            if step % FLAGS.steps_to_validate == 0:\n              loss_value, train_accuracy_value, train_auc_value, validate_accuracy_value, auc_value, summary_value = sess.run(\n                  [\n                      loss, train_accuracy, train_auc, validate_accuracy,\n                      validate_auc, summary_op\n                  ])\n              end_time = datetime.datetime.now()\n\n              logging.info(\n                  ""[{}] Step: {}, loss: {}, train_acc: {}, train_auc: {}, valid_acc: {}, valid_auc: {}"".\n                  format(end_time - start_time, step, loss_value,\n                         train_accuracy_value, train_auc_value,\n                         validate_accuracy_value, auc_value))\n              writer.add_summary(summary_value, step)\n              saver.save(sess, checkpoint_file_path, global_step=step)\n              start_time = end_time\n      except tf.errors.OutOfRangeError:\n        if FLAGS.benchmark_mode:\n          print(""Finish training for benchmark"")\n          exit(0)\n        else:\n          # Export the model after training\n          util.save_model(\n              FLAGS.model_path,\n              FLAGS.model_version,\n              sess,\n              signature_def_map,\n              is_save_graph=False)\n      finally:\n        coord.request_stop()\n      coord.join(threads)\n\n    elif FLAGS.mode == ""save_model"":\n      if not util.restore_from_checkpoint(sess, saver,\n                                          latest_checkpoint_file_path):\n        logging.error(""No checkpoint found, exit now"")\n        exit(1)\n\n      util.save_model(\n          FLAGS.model_path,\n          FLAGS.model_version,\n          sess,\n          signature_def_map,\n          is_save_graph=False)\n\n    elif FLAGS.mode == ""inference"":\n      if not util.restore_from_checkpoint(sess, saver,\n                                          latest_checkpoint_file_path):\n        logging.error(""No checkpoint found, exit now"")\n        exit(1)\n\n      # Load inference test data\n      inference_result_file_name = ""./inference_result.txt""\n      inference_test_file_name = ""./data/a8a_test.libsvm""\n      labels = []\n      feature_ids = []\n      feature_values = []\n      feature_index = []\n      ins_num = 0\n      for line in open(inference_test_file_name, ""r""):\n        tokens = line.split("" "")\n        labels.append(int(tokens[0]))\n        feature_num = 0\n        for feature in tokens[1:]:\n          feature_id, feature_value = feature.split("":"")\n          feature_ids.append(int(feature_id))\n          feature_values.append(float(feature_value))\n          feature_index.append([ins_num, feature_num])\n          feature_num += 1\n        ins_num += 1\n\n      # Run inference\n      start_time = datetime.datetime.now()\n      prediction, prediction_softmax = sess.run(\n          [inference_op, inference_softmax],\n          feed_dict={\n              sparse_index: feature_index,\n              sparse_ids: feature_ids,\n              sparse_values: feature_values,\n              sparse_shape: [ins_num, FLAGS.feature_size]\n          })\n\n      end_time = datetime.datetime.now()\n\n      # Compute accuracy\n      label_number = len(labels)\n      correct_label_number = 0\n      for i in range(label_number):\n        if labels[i] == prediction[i]:\n          correct_label_number += 1\n      accuracy = float(correct_label_number) / label_number\n\n      # Compute auc\n      expected_labels = np.array(labels)\n      predict_labels = prediction_softmax[:, 0]\n      fpr, tpr, thresholds = metrics.roc_curve(\n          expected_labels, predict_labels, pos_label=0)\n      auc = metrics.auc(fpr, tpr)\n      logging.info(""[{}] Inference accuracy: {}, auc: {}"".format(\n          end_time - start_time, accuracy, auc))\n\n      # Save result into the file\n      np.savetxt(inference_result_file_name, prediction_softmax, delimiter="","")\n      logging.info(\n          ""Save result to file: {}"".format(inference_result_file_name))\n\n    elif FLAGS.mode == ""inference_with_tfrecords"":\n      if not util.restore_from_checkpoint(sess, saver,\n                                          latest_checkpoint_file_path):\n        logging.error(""No checkpoint found, exit now"")\n        exit(1)\n\n      # Load inference test data\n      inference_result_file_name = ""./inference_result.txt""\n      inference_test_file_name = ""./data/a8a/a8a_test.libsvm.tfrecords""\n\n      batch_feature_index = []\n      batch_labels = []\n      batch_ids = []\n      batch_values = []\n      ins_num = 0\n\n      # Read from TFRecords files\n      for serialized_example in tf.python_io.tf_record_iterator(\n          inference_test_file_name):\n        # Get serialized example from file\n        example = tf.train.Example()\n        example.ParseFromString(serialized_example)\n        label = example.features.feature[""label""].float_list.value\n        ids = example.features.feature[""ids""].int64_list.value\n        values = example.features.feature[""values""].float_list.value\n        #print(""label: {}, features: {}"".format(label, "" "".join([str(id) + "":"" + str(value) for id, value in zip(ids, values)])))\n        batch_labels.append(label)\n        # Notice that using extend() instead of append() to flatten the values\n        batch_ids.extend(ids)\n        batch_values.extend(values)\n        for i in xrange(len(ids)):\n          batch_feature_index.append([ins_num, i])\n\n        ins_num += 1\n\n      # Run inference\n      start_time = datetime.datetime.now()\n      prediction, prediction_softmax = sess.run(\n          [inference_op, inference_softmax],\n          feed_dict={\n              sparse_index: batch_feature_index,\n              sparse_ids: batch_ids,\n              sparse_values: batch_values,\n              sparse_shape: [ins_num, FLAGS.feature_size]\n          })\n\n      end_time = datetime.datetime.now()\n\n      # Compute accuracy\n      label_number = len(batch_labels)\n      correct_label_number = 0\n      for i in range(label_number):\n        if batch_labels[i] == prediction[i]:\n          correct_label_number += 1\n      accuracy = float(correct_label_number) / label_number\n\n      # Compute auc\n      expected_labels = np.array(batch_labels)\n      predict_labels = prediction_softmax[:, 0]\n      fpr, tpr, thresholds = metrics.roc_curve(\n          expected_labels, predict_labels, pos_label=0)\n      auc = metrics.auc(fpr, tpr)\n      logging.info(""[{}] Inference accuracy: {}, auc: {}"".format(\n          end_time - start_time, accuracy, auc))\n\n      # Save result into the file\n      np.savetxt(inference_result_file_name, prediction_softmax, delimiter="","")\n      logging.info(\n          ""Save result to file: {}"".format(inference_result_file_name))\n\n\nif __name__ == ""__main__"":\n  main()\n'"
sparse_model.py,29,"b'from __future__ import absolute_import, division, print_function\n\nimport tensorflow as tf\n\n\ndef full_connect(inputs,\n                 weights_shape,\n                 biases_shape,\n                 is_train=True,\n                 FLAGS=None):\n  with tf.device(""/cpu:0""):\n    weights = tf.get_variable(\n        ""weights"", weights_shape, initializer=tf.random_normal_initializer())\n    biases = tf.get_variable(\n        ""biases"", biases_shape, initializer=tf.random_normal_initializer())\n    layer = tf.matmul(inputs, weights) + biases\n\n    if FLAGS.enable_bn and is_train:\n      mean, var = tf.nn.moments(layer, axes=[0])\n      scale = tf.get_variable(\n          ""scale"", biases_shape, initializer=tf.random_normal_initializer())\n      shift = tf.get_variable(\n          ""shift"", biases_shape, initializer=tf.random_normal_initializer())\n      layer = tf.nn.batch_normalization(layer, mean, var, shift, scale,\n                                        FLAGS.bn_epsilon)\n  return layer\n\n\ndef sparse_full_connect(sparse_ids,\n                        sparse_values,\n                        weights_shape,\n                        biases_shape,\n                        is_train=True,\n                        FLAGS=None):\n\n  weights = tf.get_variable(\n      ""weights"", weights_shape, initializer=tf.random_normal_initializer())\n  biases = tf.get_variable(\n      ""biases"", biases_shape, initializer=tf.random_normal_initializer())\n  return tf.nn.embedding_lookup_sparse(\n      weights, sparse_ids, sparse_values, combiner=""sum"") + biases\n\n\ndef full_connect_relu(inputs,\n                      weights_shape,\n                      biases_shape,\n                      is_train=True,\n                      FLAGS=None):\n  return tf.nn.relu(\n      full_connect(inputs, weights_shape, biases_shape, is_train, FLAGS))\n\n\ndef customized_inference(sparse_ids, sparse_values, is_train=True, FLAGS=None):\n  hidden1_units = 128\n  hidden2_units = 32\n  hidden3_units = 8\n\n  with tf.variable_scope(""input""):\n    sparse_layer = sparse_full_connect(sparse_ids, sparse_values,\n                                       [FLAGS.feature_size, hidden1_units],\n                                       [hidden1_units], is_train, FLAGS)\n    layer = tf.nn.relu(sparse_layer)\n  with tf.variable_scope(""layer0""):\n    layer = full_connect_relu(layer, [hidden1_units, hidden2_units],\n                              [hidden2_units], is_train, FLAGS)\n  with tf.variable_scope(""layer1""):\n    layer = full_connect_relu(layer, [hidden2_units, hidden3_units],\n                              [hidden3_units], is_train, FLAGS)\n  if FLAGS.enable_dropout and is_train:\n    layer = tf.nn.dropout(layer, FLAGS.dropout_keep_prob)\n  with tf.variable_scope(""output""):\n    layer = full_connect(layer, [hidden3_units, FLAGS.label_size],\n                         [FLAGS.label_size], is_train, FLAGS)\n  return layer\n\n\ndef dnn_inference(sparse_ids, sparse_values, is_train=True, FLAGS=None):\n  model_network_hidden_units = [int(i) for i in FLAGS.model_network.split()]\n\n  with tf.variable_scope(""input""):\n    sparse_layer = sparse_full_connect(sparse_ids, sparse_values, [\n        FLAGS.feature_size, model_network_hidden_units[0]\n    ], [model_network_hidden_units[0]], is_train, FLAGS)\n    layer = tf.nn.relu(sparse_layer)\n\n  for i in range(len(model_network_hidden_units) - 1):\n    with tf.variable_scope(""layer{}"".format(i)):\n      layer = full_connect_relu(layer, [\n          model_network_hidden_units[i], model_network_hidden_units[i + 1]\n      ], [model_network_hidden_units[i + 1]], is_train, FLAGS)\n\n  with tf.variable_scope(""output""):\n    layer = full_connect(layer,\n                         [model_network_hidden_units[-1], FLAGS.label_size],\n                         [FLAGS.label_size], is_train, FLAGS)\n  return layer\n\n\ndef lr_inference(sparse_ids, sparse_values, is_train=True, FLAGS=None):\n  with tf.variable_scope(""logistic_regression""):\n    layer = sparse_full_connect(sparse_ids, sparse_values,\n                                [FLAGS.input_units, FLAGS.label_size],\n                                [FLAGS.label_size], is_train, FLAGS)\n  return layer\n\n\ndef wide_and_deep_inference(sparse_ids,\n                            sparse_values,\n                            is_train=True,\n                            FLAGS=None):\n  return lr_inference(sparse_ids,\n                      sparse_values, is_train, FLAGS) + dnn_inference(\n                          sparse_ids, sparse_values, is_train, FLAGS)\n'"
util.py,9,"b'from __future__ import absolute_import, division, print_function\n\nimport logging\nimport os\nimport tensorflow as tf\nfrom tensorflow.python.saved_model import builder as saved_model_builder\nfrom tensorflow.python.saved_model import (signature_constants, tag_constants)\n\n\ndef get_optimizer_by_name(optimizer_name, learning_rate):\n  """"""\n    Get optimizer object by the optimizer name.\n    \n    Args:\n      optimizer_name: Name of the optimizer. \n      learning_rate: The learning rate.\n    \n    Return:\n      The optimizer object.\n    """"""\n\n  logging.info(""Use the optimizer: {}"".format(optimizer_name))\n  if optimizer_name == ""sgd"":\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n  elif optimizer_name == ""adadelta"":\n    optimizer = tf.train.AdadeltaOptimizer(learning_rate)\n  elif optimizer_name == ""adagrad"":\n    optimizer = tf.train.AdagradOptimizer(learning_rate)\n  elif optimizer_name == ""adam"":\n    optimizer = tf.train.AdamOptimizer(learning_rate)\n  elif optimizer_name == ""ftrl"":\n    optimizer = tf.train.FtrlOptimizer(learning_rate)\n  elif optimizer_name == ""rmsprop"":\n    optimizer = tf.train.RMSPropOptimizer(learning_rate)\n  else:\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n  return optimizer\n\n\ndef save_model(model_path,\n               model_version,\n               sess,\n               signature_def_map,\n               is_save_graph=False):\n  """"""\n    Save the model in standard SavedModel format.\n    \n    Args:\n      model_path: The path to model.\n      model_version: The version of model.\n      sess: The TensorFlow Session object.\n      signature_def_map: The map of TensorFlow SignatureDef object.\n      is_save_graph: Should save graph file of not.\n    \n    Return:\n      None\n    """"""\n\n  export_path = os.path.join(model_path, str(model_version))\n  if os.path.isdir(export_path) == True:\n    logging.error(""The model exists in path: {}"".format(export_path))\n    return\n\n  try:\n    # Save the SavedModel\n    legacy_init_op = tf.group(tf.tables_initializer(), name=\'legacy_init_op\')\n    builder = saved_model_builder.SavedModelBuilder(export_path)\n    builder.add_meta_graph_and_variables(\n        sess, [tag_constants.SERVING],\n        clear_devices=True,\n        signature_def_map=signature_def_map,\n        legacy_init_op=legacy_init_op)\n    logging.info(""Save the model in: {}"".format(export_path))\n    builder.save()\n\n    # Save the GraphDef\n    if is_save_graph == True:\n      graph_file_name = ""graph.pb""\n      logging.info(""Save the graph file in: {}"".format(model_path))\n      tf.train.write_graph(\n          sess.graph_def, model_path, graph_file_name, as_text=False)\n\n  except Exception as e:\n    logging.error(""Fail to export saved model, exception: {}"".format(e))\n\n\ndef restore_from_checkpoint(sess, saver, checkpoint_file_path):\n  """"""\n    Restore session from checkpoint files.\n    \n    Args:\n      sess: TensorFlow Session object.\n      saver: TensorFlow Saver object.\n      checkpoint_file_path: The checkpoint file path.\n    \n    Return:\n      True if restore successfully and False if fail\n    """"""\n  if checkpoint_file_path:\n    logging.info(\n        ""Restore session from checkpoint: {}"".format(checkpoint_file_path))\n    saver.restore(sess, checkpoint_file_path)\n    return True\n  else:\n    logging.error(""Checkpoint not found: {}"".format(checkpoint_file_path))\n    return False\n'"
distributed/dense_classifier.py,78,"b'#!/usr/bin/env python\n\nimport tensorflow as tf\nimport math\nimport os\nimport numpy as np\n\n# Define parameters\nflags = tf.app.flags\nFLAGS = flags.FLAGS\nflags.DEFINE_float(\'learning_rate\', 0.01, \'Initial learning rate.\')\nflags.DEFINE_integer(\'epoch_number\', None, \'Number of epochs to run trainer.\')\nflags.DEFINE_integer(""batch_size"", 1024,\n                     ""indicates batch size in a single gpu, default is 1024"")\nflags.DEFINE_integer(""thread_number"", 1, ""Number of thread to read data"")\nflags.DEFINE_integer(""min_after_dequeue"", 100,\n                     ""indicates min_after_dequeue of shuffle queue"")\nflags.DEFINE_string(""output_dir"", ""./tensorboard/"",\n                    ""indicates training output"")\nflags.DEFINE_string(""model"", ""deep"",\n                    ""Model to train, option model: deep, linear"")\nflags.DEFINE_string(""optimizer"", ""sgd"", ""optimizer to import"")\nflags.DEFINE_integer(\'hidden1\', 10, \'Number of units in hidden layer 1.\')\nflags.DEFINE_integer(\'hidden2\', 20, \'Number of units in hidden layer 2.\')\nflags.DEFINE_integer(\'steps_to_validate\', 10,\n                     \'Steps to validate and print loss\')\nflags.DEFINE_string(""mode"", ""train"",\n                    ""Option mode: train, train_from_scratch, inference"")\n# For distributed\ntf.app.flags.DEFINE_string(""ps_hosts"", """",\n                           ""Comma-separated list of hostname:port pairs"")\ntf.app.flags.DEFINE_string(""worker_hosts"", """",\n                           ""Comma-separated list of hostname:port pairs"")\ntf.app.flags.DEFINE_string(""job_name"", """", ""One of \'ps\', \'worker\'"")\ntf.app.flags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")\n\n# Hyperparameters\nlearning_rate = FLAGS.learning_rate\nepoch_number = FLAGS.epoch_number\nthread_number = FLAGS.thread_number\nbatch_size = FLAGS.batch_size\nmin_after_dequeue = FLAGS.min_after_dequeue\ncapacity = thread_number * batch_size + min_after_dequeue\nFEATURE_SIZE = 9\n\n\n# Read serialized examples from filename queue\ndef read_and_decode(filename_queue):\n    reader = tf.TFRecordReader()\n    _, serialized_example = reader.read(filename_queue)\n    features = tf.parse_single_example(\n        serialized_example,\n        features={\n            ""label"": tf.FixedLenFeature([], tf.float32),\n            ""features"": tf.FixedLenFeature([FEATURE_SIZE], tf.float32),\n        })\n\n    label = features[""label""]\n    features = features[""features""]\n\n    return label, features\n\n\ndef main(_):\n    ps_hosts = FLAGS.ps_hosts.split("","")\n    worker_hosts = FLAGS.worker_hosts.split("","")\n    cluster = tf.train.ClusterSpec({""ps"": ps_hosts, ""worker"": worker_hosts})\n    server = tf.train.Server(cluster,\n                             job_name=FLAGS.job_name,\n                             task_index=FLAGS.task_index)\n\n    if FLAGS.job_name == ""ps"":\n        server.join()\n    elif FLAGS.job_name == ""worker"":\n\n        with tf.device(tf.train.replica_device_setter(\n                worker_device=""/job:worker/task:%d"" % FLAGS.task_index,\n                cluster=cluster)):\n\n            # Read TFRecords files\n            filename_queue = tf.train.string_input_producer(\n                tf.train.match_filenames_once(""../data/cancer/cancer_train.csv.tfrecords""),\n                num_epochs=epoch_number)\n            label, features = read_and_decode(filename_queue)\n            batch_labels, batch_features = tf.train.shuffle_batch(\n                [label, features],\n                batch_size=batch_size,\n                num_threads=thread_number,\n                capacity=capacity,\n                min_after_dequeue=min_after_dequeue)\n\n            validate_filename_queue = tf.train.string_input_producer(\n                tf.train.match_filenames_once(\n                    ""../data/cancer/cancer_test.csv.tfrecords""),\n                num_epochs=epoch_number)\n            validate_label, validate_features = read_and_decode(\n                validate_filename_queue)\n            validate_batch_labels, validate_batch_features = tf.train.shuffle_batch(\n                [validate_label, validate_features],\n                batch_size=batch_size,\n                num_threads=thread_number,\n                capacity=capacity,\n                min_after_dequeue=min_after_dequeue)\n\n            # Define the model\n            input_units = FEATURE_SIZE\n            hidden1_units = FLAGS.hidden1\n            hidden2_units = FLAGS.hidden2\n            output_units = 2\n\n            # Hidden 1\n            weights1 = tf.Variable(\n                tf.truncated_normal([input_units, hidden1_units]),\n                dtype=tf.float32,\n                name=\'weights\')\n            biases1 = tf.Variable(\n                tf.truncated_normal([hidden1_units]),\n                name=\'biases\',\n                dtype=tf.float32)\n            hidden1 = tf.nn.relu(tf.matmul(batch_features, weights1) + biases1)\n\n            # Hidden 2\n            weights2 = tf.Variable(\n                tf.truncated_normal([hidden1_units, hidden2_units]),\n                dtype=tf.float32,\n                name=\'weights\')\n            biases2 = tf.Variable(\n                tf.truncated_normal([hidden2_units]),\n                name=\'biases\',\n                dtype=tf.float32)\n            hidden2 = tf.nn.relu(tf.matmul(hidden1, weights2) + biases2)\n\n            # Linear\n            weights3 = tf.Variable(\n                tf.truncated_normal([hidden2_units, output_units]),\n                dtype=tf.float32,\n                name=\'weights\')\n            biases3 = tf.Variable(\n                tf.truncated_normal([output_units]),\n                name=\'biases\',\n                dtype=tf.float32)\n            logits = tf.matmul(hidden2, weights3) + biases3\n\n            batch_labels = tf.to_int64(batch_labels)\n            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n                logits=logits, labels=batch_labels)\n            loss = tf.reduce_mean(cross_entropy, name=\'xentropy_mean\')\n            if FLAGS.optimizer == ""sgd"":\n                optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n            else:\n                optimizer = tf.train.MomentumOptimizer(learning_rate)\n            global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n            train_op = optimizer.minimize(loss, global_step=global_step)\n\n            # Compute accuracy\n            accuracy_hidden1 = tf.nn.relu(tf.matmul(validate_batch_features,\n                                                    weights1) + biases1)\n            accuracy_hidden2 = tf.nn.relu(tf.matmul(accuracy_hidden1, weights2)\n                                          + biases2)\n            accuracy_logits = tf.matmul(accuracy_hidden2, weights3) + biases3\n            validate_softmax = tf.nn.softmax(accuracy_logits)\n\n            validate_batch_labels = tf.to_int64(validate_batch_labels)\n            correct_prediction = tf.equal(\n                tf.argmax(validate_softmax, 1), validate_batch_labels)\n            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n            # Compute auc\n            validate_batch_labels = tf.cast(validate_batch_labels, tf.int32)\n            num_labels = 2\n            sparse_labels = tf.reshape(validate_batch_labels, [-1, 1])\n            derived_size = tf.shape(validate_batch_labels)[0]\n            indices = tf.reshape(tf.range(0, derived_size, 1), [-1, 1])\n            concated = tf.concat(axis=1, values=[indices, sparse_labels])\n            outshape = tf.stack([derived_size, num_labels])\n            new_validate_batch_labels = tf.sparse_to_dense(concated, outshape,\n                                                           1.0, 0.0)\n            _, auc_op = tf.contrib.metrics.streaming_auc(\n                validate_softmax, new_validate_batch_labels)\n\n            # Define inference op\n            inference_features = tf.placeholder(""float"", [None, 9])\n            inference_hidden1 = tf.nn.relu(tf.matmul(inference_features,\n                                                     weights1) + biases1)\n            inference_hidden2 = tf.nn.relu(tf.matmul(inference_hidden1,\n                                                     weights2) + biases2)\n            inference_logits = tf.matmul(inference_hidden2, weights3) + biases3\n            inference_softmax = tf.nn.softmax(inference_logits)\n            inference_op = tf.argmax(inference_softmax, 1)\n\n            saver = tf.train.Saver()\n            steps_to_validate = FLAGS.steps_to_validate\n            init_op = tf.global_variables_initializer()\n\n            tf.summary.scalar(\'loss\', loss)\n            tf.summary.scalar(\'accuracy\', accuracy)\n            tf.summary.scalar(\'auc\', auc_op)\n\n            summary_op = tf.summary.merge_all()\n\n        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                                 logdir=""./checkpoint/"",\n                                 init_op=init_op,\n                                 summary_op=summary_op,\n                                 saver=saver,\n                                 global_step=global_step,\n                                 save_model_secs=60)\n\n        with sv.managed_session(server.target) as sess:\n            step = 0\n            while not sv.should_stop() and step < 1000000:\n\n                # Get coordinator and run queues to read data\n                coord = tf.train.Coordinator()\n                threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n\n                try:\n                    while not coord.should_stop():\n                        # Run train op\n                        _, loss_value, step = sess.run([train_op, loss,\n                                                        global_step])\n\n                        if step % steps_to_validate == 0:\n                            accuracy_value, auc_value, summary_value = sess.run(\n                                [accuracy, auc_op, summary_op])\n                            print(\n                                ""Step: {}, loss: {}, accuracy: {}, auc: {}"".format(\n                                    step, loss_value, accuracy_value,\n                                    auc_value))\n\n                except tf.errors.OutOfRangeError:\n                    print(""Done training after reading all data"")\n                finally:\n                    coord.request_stop()\n\n                # Wait for threads to exit\n                coord.join(threads)\n\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
http_service/manage.py,0,"b'#!/usr/bin/env python\nimport os\nimport sys\n\nif __name__ == ""__main__"":\n    os.environ.setdefault(""DJANGO_SETTINGS_MODULE"",\n                          ""restful_server.settings"")\n\n    from django.core.management import execute_from_command_line\n\n    execute_from_command_line(sys.argv)\n'"
minimal_model/benchmark_predict.py,12,"b'#!/usr/bin/env python\n\nimport numpy as np\nimport os\nimport tensorflow as tf\nfrom tensorflow.contrib.session_bundle import exporter\nimport time\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\nflags.DEFINE_integer(""batch_size"", 10, ""The batch size to train"")\nflags.DEFINE_integer(""epoch_number"", 10, ""Number of epochs to run trainer"")\nflags.DEFINE_integer(""steps_to_validate"", 1,\n                     ""Steps to validate and print loss"")\nflags.DEFINE_string(""checkpoint_dir"", ""./checkpoint/"",\n                    ""indicates the checkpoint dirctory"")\nflags.DEFINE_string(""model_path"", ""./model/"", ""The export path of the model"")\nflags.DEFINE_integer(""export_version"", 1, ""The version number of the model"")\nflags.DEFINE_integer(""benchmark_batch_size"", 1, """")\nflags.DEFINE_integer(""benchmark_test_number"", 10000, """")\n\ndef main():\n  # Define training data\n  x = np.ones(FLAGS.batch_size)\n  y = np.ones(FLAGS.batch_size)\n\n  # Define the model\n  X = tf.placeholder(tf.float32, shape=[None])\n  Y = tf.placeholder(tf.float32, shape=[None])\n  w = tf.Variable(1.0, name=""weight"")\n  b = tf.Variable(1.0, name=""bias"")\n  loss = tf.square(Y - tf.mul(X, w) - b)\n  train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n  predict_op  = tf.mul(X, w) + b\n\n  saver = tf.train.Saver()\n  checkpoint_dir = FLAGS.checkpoint_dir\n  checkpoint_file = checkpoint_dir + ""/checkpoint.ckpt""\n  if not os.path.exists(checkpoint_dir):\n    os.makedirs(checkpoint_dir)\n\n  # Start the session\n  with tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n\n    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n    if ckpt and ckpt.model_checkpoint_path:\n      print(""Continue training from the model {}"".format(ckpt.model_checkpoint_path))\n      saver.restore(sess, ckpt.model_checkpoint_path)\n\n    # Start training\n    start_time = time.time()\n\n    request_number = FLAGS.benchmark_test_number\n    batch_size = FLAGS.benchmark_batch_size\n    predict_x = np.ones(batch_size)\n\n    start_time = time.time()\n    for i in range(request_number):\n      sess.run(predict_op, feed_dict={X: predict_x})\n\n    end_time = time.time()\n    print(""Average latency is: {} ms"".format((end_time - start_time) * 1000 / request_number))\n\n\nif __name__ == ""__main__"":\n  main()\n'"
minimal_model/train.py,13,"b'#!/usr/bin/env python\n\nimport numpy as np\nimport os\nimport tensorflow as tf\nfrom tensorflow.contrib.session_bundle import exporter\nimport time\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\nflags.DEFINE_integer(""batch_size"", 10, ""The batch size to train"")\nflags.DEFINE_integer(""epoch_number"", 10, ""Number of epochs to run trainer"")\nflags.DEFINE_integer(""steps_to_validate"", 1,\n                     ""Steps to validate and print loss"")\nflags.DEFINE_string(""checkpoint_dir"", ""./checkpoint/"",\n                    ""indicates the checkpoint dirctory"")\nflags.DEFINE_string(""model_path"", ""./model/"", ""The export path of the model"")\nflags.DEFINE_integer(""export_version"", 1, ""The version number of the model"")\n\ndef main():\n  # Define training data\n  x = np.ones(FLAGS.batch_size)\n  y = np.ones(FLAGS.batch_size)\n\n  # Define the model\n  X = tf.placeholder(tf.float32, shape=[None])\n  Y = tf.placeholder(tf.float32, shape=[None])\n  w = tf.Variable(1.0, name=""weight"")\n  b = tf.Variable(1.0, name=""bias"")\n  loss = tf.square(Y - tf.mul(X, w) - b)\n  train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n  predict_op  = tf.mul(X, w) + b\n\n  saver = tf.train.Saver()\n  checkpoint_dir = FLAGS.checkpoint_dir\n  checkpoint_file = checkpoint_dir + ""/checkpoint.ckpt""\n  if not os.path.exists(checkpoint_dir):\n    os.makedirs(checkpoint_dir)\n\n  # Start the session\n  with tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n\n    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n    if ckpt and ckpt.model_checkpoint_path:\n      print(""Continue training from the model {}"".format(ckpt.model_checkpoint_path))\n      saver.restore(sess, ckpt.model_checkpoint_path)\n\n    # Start training\n    start_time = time.time()\n    for epoch in range(FLAGS.epoch_number):\n      sess.run(train_op, feed_dict={X: x, Y: y})\n\n      # Start validating\n      if epoch % FLAGS.steps_to_validate == 0:\n        end_time = time.time()\n        print(""[{}] Epoch: {}"".format(end_time - start_time, epoch))\n\n        saver.save(sess, checkpoint_file)\n        start_time = end_time\n\n    # Print model variables\n    w_value, b_value = sess.run([w, b])\n    print(""The model of w: {}, b: {}"".format(w_value, b_value))\n\n    # Export the model\n    print(""Exporting trained model to {}"".format(FLAGS.model_path))\n    model_exporter = exporter.Exporter(saver)\n    model_exporter.init(\n      sess.graph.as_graph_def(),\n      named_graph_signatures={\n        \'inputs\': exporter.generic_signature({""features"": X}),\n        \'outputs\': exporter.generic_signature({""prediction"": predict_op})\n      })\n    model_exporter.export(FLAGS.model_path, tf.constant(FLAGS.export_version), sess)\n    print \'Done exporting!\'\n\nif __name__ == ""__main__"":\n  main()\n'"
python_predict_client/predict_client.py,10,"b'#!/usr/bin/env python\n\nimport numpy\nimport tensorflow as tf\nfrom grpc.beta import implementations\nfrom tensorflow_serving.apis import predict_pb2, prediction_service_pb2\n\ntf.app.flags.DEFINE_string(""host"", ""0.0.0.0"", ""TensorFlow Serving server ip"")\ntf.app.flags.DEFINE_integer(""port"", 8500, ""TensorFlow Serving server port"")\ntf.app.flags.DEFINE_string(""model_name"", ""default"", ""The model name"")\ntf.app.flags.DEFINE_integer(""model_version"", -1, ""The model version"")\ntf.app.flags.DEFINE_string(""signature_name"", """", ""The model signature name"")\ntf.app.flags.DEFINE_float(""request_timeout"", 10.0, ""Timeout of gRPC request"")\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main():\n  # Generate inference data\n  keys = numpy.asarray([1, 2, 3, 4])\n  keys_tensor_proto = tf.contrib.util.make_tensor_proto(keys, dtype=tf.int32)\n  features = numpy.asarray(\n      [[1, 2, 3, 4, 5, 6, 7, 8, 9], [1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [9, 8, 7, 6, 5, 4, 3, 2, 1], [9, 9, 9, 9, 9, 9, 9, 9, 9]])\n  features_tensor_proto = tf.contrib.util.make_tensor_proto(\n      features, dtype=tf.float32)\n\n  # Create gRPC client\n  channel = implementations.insecure_channel(FLAGS.host, FLAGS.port)\n  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n  request = predict_pb2.PredictRequest()\n  request.model_spec.name = FLAGS.model_name\n  if FLAGS.model_version > 0:\n    request.model_spec.version.value = FLAGS.model_version\n  if FLAGS.signature_name != """":\n    request.model_spec.signature_name = FLAGS.signature_name\n  request.inputs[""keys""].CopyFrom(keys_tensor_proto)\n  request.inputs[""features""].CopyFrom(features_tensor_proto)\n\n  # Send request\n  result = stub.Predict(request, FLAGS.request_timeout)\n  print(result)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
python_predict_client/sparse_predict_client.py,13,"b'#!/usr/bin/env python\n\nimport numpy\n\nfrom grpc.beta import implementations\nimport tensorflow as tf\n\nimport predict_pb2\nimport prediction_service_pb2\n\ntf.app.flags.DEFINE_string(""host"", ""127.0.0.1"", ""gRPC server host"")\ntf.app.flags.DEFINE_integer(""port"", 9000, ""gRPC server port"")\ntf.app.flags.DEFINE_string(""model_name"", ""cancer"", ""TensorFlow model name"")\ntf.app.flags.DEFINE_integer(""model_version"", 1, ""TensorFlow model version"")\ntf.app.flags.DEFINE_float(""request_timeout"", 10.0, ""Timeout of gRPC request"")\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main():\n  host = FLAGS.host\n  port = FLAGS.port\n  model_name = FLAGS.model_name\n  model_version = FLAGS.model_version\n  request_timeout = FLAGS.request_timeout\n  \'\'\'\n  Example data:\n    0 5:1 6:1 17:1 21:1 35:1 40:1 53:1 63:1 71:1 73:1 74:1 76:1 80:1 83:1\n    1 5:1 7:1 17:1 22:1 36:1 40:1 51:1 63:1 67:1 73:1 74:1 76:1 81:1 83:1\n  \'\'\'\n\n  # Generate keys TensorProto\n  keys = numpy.asarray([1, 2])\n  keys_tensor_proto = tf.contrib.util.make_tensor_proto(keys, dtype=tf.int32)\n\n  # Generate indexs TensorProto\n  indexs = numpy.asarray([[0, 0], [0, 1], [0, 2], [0, 3], [0, 4], [0, 5],\n                          [0, 6], [0, 7], [0, 8], [0, 9], [0, 10], [0, 11],\n                          [0, 12], [0, 13], [1, 0], [1, 1], [1, 2], [1, 3],\n                          [1, 4], [1, 5], [1, 6], [1, 7], [1, 8], [1, 9],\n                          [1, 10], [1, 11], [1, 12], [1, 13]])\n  indexs_tensor_proto = tf.contrib.util.make_tensor_proto(indexs,\n                                                          dtype=tf.int64)\n\n  # Generate ids TensorProto\n  ids = numpy.asarray([5, 6, 17, 21, 35, 40, 53, 63, 71, 73, 74, 76, 80, 83, 5,\n                       7, 17, 22, 36, 40, 51, 63, 67, 73, 74, 76, 81, 83])\n  ids_tensor_proto = tf.contrib.util.make_tensor_proto(ids, dtype=tf.int64)\n\n  # Generate values TensorProto\n  values = numpy.asarray([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n                          1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n                          1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n  values_tensor_proto = tf.contrib.util.make_tensor_proto(values,\n                                                          dtype=tf.float32)\n\n  # Generate values TensorProto\n  shape = numpy.asarray([2, 124])\n  shape_tensor_proto = tf.contrib.util.make_tensor_proto(shape, dtype=tf.int64)\n\n  # Create gRPC client and request\n  channel = implementations.insecure_channel(host, port)\n  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n  request = predict_pb2.PredictRequest()\n  request.model_spec.name = model_name\n  if model_version > 0:\n    request.model_spec.version.value = model_version\n\n  request.inputs[""keys""].CopyFrom(keys_tensor_proto)\n  request.inputs[""indexs""].CopyFrom(indexs_tensor_proto)\n  request.inputs[""ids""].CopyFrom(ids_tensor_proto)\n  request.inputs[""values""].CopyFrom(values_tensor_proto)\n  request.inputs[""shape""].CopyFrom(shape_tensor_proto)\n\n  # Send request\n  result = stub.Predict(request, request_timeout)\n  print(result)\n\n\nif __name__ == \'__main__\':\n  main()\n'"
sklearn_exmaples/cancer_classifier.py,0,"b'#!/usr/bin/env python\n\nimport sys\nimport numpy as np\nfrom sklearn import metrics\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nFEATURE_NUMBER = 9\n\n# Read train and test data\nwith open(""../data/cancer_train.csv"", ""r"") as f:\n  train_dataset = np.loadtxt(f, delimiter="","")\n  train_labels = train_dataset[:, FEATURE_NUMBER]\n  train_features = train_dataset[:, 0:FEATURE_NUMBER]\n\nwith open(""../data/cancer_test.csv"", ""r"") as f:\n  test_dataset = np.loadtxt(f, delimiter="","")\n  test_labels = test_dataset[:, FEATURE_NUMBER]\n  test_features = test_dataset[:, 0:FEATURE_NUMBER]\n\n# Define the model\nclassifiers = [\n  DecisionTreeClassifier(max_depth=5),\n  MLPClassifier(algorithm=\'sgd\', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1, learning_rate_init=0.001, batch_size=64, max_iter=100, verbose=False),\n  MLPClassifier(algorithm=\'l-bfgs\', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1),\n  MLPClassifier(algorithm=\'adam\', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1),\n  KNeighborsClassifier(2),\n  SVC(kernel=""linear"", C=0.025),\n  SVC(gamma=2, C=1),\n  RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n  AdaBoostClassifier(),\n  GaussianNB(),\n  LinearDiscriminantAnalysis(),\n  QuadraticDiscriminantAnalysis()]\n\nif len(sys.argv) > 1:\n  classifier_index = int(sys.argv[1])\nelse:\n  classifier_index = 0\nclassifier = classifiers[classifier_index]\nprint(""Use the classifier: {}"".format(classifier))\n\n# Train the model\nprint(""Start to train"")\nmodel = classifier.fit(train_features, train_labels)\n\nprint(""Start to validate"")\npredict_labels = model.predict(test_features)\nauc = metrics.roc_auc_score(test_labels, predict_labels)\naccuracy = metrics.accuracy_score(test_labels, predict_labels)\n\n# Print the metrics\nprint(""Accuracy: {}, acu: {}"".format(accuracy, auc))\n'"
tensorboard_tools/read_event_files.py,1,"b'#!/usr/bin/env python\n\nimport tensorflow as tf\n\n\ndef main():\n  event_file_path = ""/Users/tobe/code/tensorflow_template_application/tensorboard/events.out.tfevents.1527218992.mbp-2.local""\n  for event in tf.train.summary_iterator(event_file_path):\n\n    print(""--------------------"")\n    print(""Time: {}"".format(event.wall_time))\n\n    for v in event.summary.value:\n      if v.tag == ""loss_1"":\n        print(""Loss: {}"".format(v.simple_value))\n      elif v.tag == ""train_accuracy"":\n        print(""Train accuracy: {}"".format(v.simple_value))\n      elif v.tag == ""train_auc"":\n        print(""Train auc: {}"".format(v.simple_value))\n      elif v.tag == ""validate_accuracy"":\n        print(""Validate accuracy: {}"".format(v.simple_value))\n      elif v.tag == ""validate_auc"":\n        print(""Validate auc: {}"".format(v.simple_value))\n\n\nif __name__ == ""__main__"":\n  main()\n'"
trainer/__init__.py,0,b''
data/a8a/generate_libsvm_tfrecord.py,5,"b'#!/usr/bin/env python\n\nimport tensorflow as tf\nimport os\n\n\ndef generate_tfrecords(input_filename, output_filename):\n  print(""Start to convert {} to {}"".format(input_filename, output_filename))\n  writer = tf.python_io.TFRecordWriter(output_filename)\n\n  for line in open(input_filename, ""r""):\n    data = line.split("" "")\n    label = float(data[0])\n    ids = []\n    values = []\n    for fea in data[1:]:\n      id, value = fea.split("":"")\n      ids.append(int(id))\n      values.append(float(value))\n\n    # Write each example one by one\n    example = tf.train.Example(features=tf.train.Features(feature={\n        ""label"":\n        tf.train.Feature(float_list=tf.train.FloatList(value=[label])),\n        ""ids"": tf.train.Feature(int64_list=tf.train.Int64List(value=ids)),\n        ""values"": tf.train.Feature(float_list=tf.train.FloatList(value=values))\n    }))\n\n    writer.write(example.SerializeToString())\n\n  writer.close()\n  print(""Successfully convert {} to {}"".format(input_filename,\n                                               output_filename))\n\n\ndef main():\n  current_path = os.getcwd()\n  for filename in os.listdir(current_path):\n    if filename.startswith("""") and filename.endswith("".libsvm""):\n      generate_tfrecords(filename, filename + "".tfrecords"")\n\n\nif __name__ == ""__main__"":\n  main()\n'"
data/a8a/generate_tfrecords_from_libsvm.py,5,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\n\ndef generate_tfrecords(input_filename, output_filename):\n  print(""Start to convert {} to {}"".format(input_filename, output_filename))\n\n  writer = tf.python_io.TFRecordWriter(output_filename)\n\n  for line in open(input_filename, ""r""):\n    data = line.split("" "")\n    label = int(data[0])\n    ids = []\n    values = []\n    for fea in data[1:]:\n      id, value = fea.split("":"")\n      ids.append(int(id))\n      values.append(float(value))\n\n    # Write each example one by one\n    example = tf.train.Example(features=tf.train.Features(\n        feature={\n            ""label"":\n            tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n            ""ids"":\n            tf.train.Feature(int64_list=tf.train.Int64List(value=ids)),\n            ""values"":\n            tf.train.Feature(float_list=tf.train.FloatList(value=values))\n        }))\n\n    writer.write(example.SerializeToString())\n\n  writer.close()\n  print(\n      ""Successfully convert {} to {}"".format(input_filename, output_filename))\n\n\ndef main():\n  current_path = os.getcwd()\n  for filename in os.listdir(current_path):\n    if filename.startswith("""") and filename.endswith("".libsvm""):\n      generate_tfrecords(filename, filename + "".tfrecords"")\n\n\nif __name__ == ""__main__"":\n  main()\n'"
data/a8a/print_libsvm_tfrecords.py,2,"b'#!/usr/bin/env python\n\nimport tensorflow as tf\nimport os\n\n\ndef print_tfrecords(input_filename):\n  max_print_number = 100\n  current_print_number = 0\n\n  for serialized_example in tf.python_io.tf_record_iterator(input_filename):\n    # Get serialized example from file\n    example = tf.train.Example()\n    example.ParseFromString(serialized_example)\n    label = example.features.feature[""label""].float_list.value\n    ids = example.features.feature[""ids""].int64_list.value\n    values = example.features.feature[""values""].float_list.value\n    print(""Number: {}, label: {}, features: {}"".format(\n        current_print_number, label, "" "".join([str(id) + "":"" + str(\n            value) for id, value in zip(ids, values)])))\n\n    # Return when reaching max print number\n    current_print_number += 1\n    if current_print_number > max_print_number:\n      exit()\n\n\ndef main():\n  current_path = os.getcwd()\n  tfrecords_file_name = ""a8a_train.libsvm.tfrecords""\n  input_filename = os.path.join(current_path, tfrecords_file_name)\n  print_tfrecords(input_filename)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
data/a8a/print_tfrecords_files.py,2,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\n\ndef print_tfrecords_file(input_filename):\n  print(""Try to print the tfrecords file: {}"".format(input_filename))\n\n  max_print_number = 10\n  current_print_index = 0\n\n  for serialized_example in tf.python_io.tf_record_iterator(input_filename):\n    # Get serialized example from file\n    example = tf.train.Example()\n    example.ParseFromString(serialized_example)\n    label = example.features.feature[""label""].int64_list.value\n    ids = example.features.feature[""ids""].int64_list.value\n    values = example.features.feature[""values""].float_list.value\n    print(""Index: {}, label: {}, features: {}"".format(\n            current_print_index, label, "" "".join(\n            [str(id) + "":"" + str(value) for id, value in zip(ids, values)])))\n\n    # Return when reaching max print number\n    current_print_index += 1\n    if current_print_index > max_print_number - 1:\n      return\n\n\ndef main():\n  current_path = os.getcwd()\n  for filename in os.listdir(current_path):\n    if filename.startswith("""") and filename.endswith("".tfrecords""):\n      tfrecords_file_path = os.path.join(current_path, filename)\n      print_tfrecords_file(tfrecords_file_path)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
data/boston_housing/generate_csv_tfrecords.py,4,"b'#!/usr/bin/env python\n\nimport tensorflow as tf\nimport os\n\n\ndef generate_tfrecords(input_filename, output_filename):\n  print(""Start to convert {} to {}"".format(input_filename, output_filename))\n  writer = tf.python_io.TFRecordWriter(output_filename)\n\n  index = 0\n  for line in open(input_filename, ""r""):\n    index += 1\n\n    # Ignore the first line\n    if index == 1:\n      continue\n\n    data = line.split("","")\n    label = float(data[14])\n    features = [float(i) for i in data[1:14]]\n\n    example = tf.train.Example(features=tf.train.Features(\n        feature={\n            ""label"":\n            tf.train.Feature(float_list=tf.train.FloatList(value=[label])),\n            ""features"":\n            tf.train.Feature(float_list=tf.train.FloatList(value=features)),\n        }))\n    writer.write(example.SerializeToString())\n\n  writer.close()\n  print(\n      ""Successfully convert {} to {}"".format(input_filename, output_filename))\n\n\ndef main():\n  current_path = os.getcwd()\n  for filename in os.listdir(current_path):\n    if filename.startswith("""") and filename.endswith("".csv""):\n      generate_tfrecords(filename, filename + "".tfrecords"")\n\n\nif __name__ == ""__main__"":\n  main()\n'"
data/boston_housing/print_csv_tfrecords.py,2,"b'#!/usr/bin/env python\n\nimport tensorflow as tf\nimport os\n\n\ndef print_tfrecords(input_filename):\n  max_print_number = 100\n  current_print_number = 0\n\n  for serialized_example in tf.python_io.tf_record_iterator(input_filename):\n    # Get serialized example from file\n    example = tf.train.Example()\n    example.ParseFromString(serialized_example)\n    label = example.features.feature[""label""].float_list.value\n    features = example.features.feature[""features""].float_list.value\n    print(""Number: {}, label: {}, features: {}"".format(current_print_number,\n                                                       label, features))\n\n    # Return when reaching max print number\n    current_print_number += 1\n    if current_print_number > max_print_number:\n      exit()\n\n\ndef main():\n  current_path = os.getcwd()\n  tfrecords_file_name = ""train.csv.tfrecords""\n  input_filename = os.path.join(current_path, tfrecords_file_name)\n  print_tfrecords(input_filename)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
data/cancer/generate_tfrecords_from_csv.py,4,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\n\ndef generate_tfrecords_file(input_filename, output_filename):\n  print(""Start to convert {} to {}"".format(input_filename, output_filename))\n\n  writer = tf.python_io.TFRecordWriter(output_filename)\n\n  for line in open(input_filename, ""r""):\n    data = line.split("","")\n    label = int(data[-1])\n    features = [float(i) for i in data[:-1]]\n\n    example = tf.train.Example(features=tf.train.Features(\n        feature={\n            ""label"":\n            tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n            ""features"":\n            tf.train.Feature(float_list=tf.train.FloatList(value=features)),\n        }))\n    writer.write(example.SerializeToString())\n\n  writer.close()\n\n  print(\n      ""Successfully convert {} to {}"".format(input_filename, output_filename))\n\n\ndef main():\n  current_path = os.getcwd()\n  for filename in os.listdir(current_path):\n    if filename.startswith("""") and filename.endswith("".csv""):\n      generate_tfrecords_file(filename, filename + "".tfrecords"")\n\n\nif __name__ == ""__main__"":\n  main()\n'"
data/cancer/print_tfrecords_files.py,2,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\n\ndef print_tfrecords_file(input_filename):\n  print(""Try to print the tfrecords file: {}"".format(input_filename))\n\n  max_print_number = 10\n  current_print_index = 0\n\n  for serialized_example in tf.python_io.tf_record_iterator(input_filename):\n    # Get serialized example from file\n    example = tf.train.Example()\n    example.ParseFromString(serialized_example)\n    label = example.features.feature[""label""].int64_list.value\n    features = example.features.feature[""features""].float_list.value\n    print(""Index: {}, label: {}, features: {}"".format(current_print_index,\n                                                      label, features))\n\n    # Return when reaching max print number\n    current_print_index += 1\n    if current_print_index > max_print_number - 1:\n      return\n\n\ndef main():\n  current_path = os.getcwd()\n  for filename in os.listdir(current_path):\n    if filename.startswith("""") and filename.endswith("".tfrecords""):\n      tfrecords_file_path = os.path.join(current_path, filename)\n      print_tfrecords_file(tfrecords_file_path)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
data/iris/download_iris.py,0,"b'#!/usr/bin/env python\n\nimport numpy as np\nimport random\nfrom sklearn import datasets\n\n\ndef main():\n  # Load data set\n  print(""Download the iris dataset"")\n  iris = datasets.load_iris()\n\n  # Split into train and test\n  TRAIN_FILE_NAME = ""iris_train.csv""\n  TEST_FILE_NAME = ""iris_test.csv""\n  file_content_array = []\n  train_file_content = """"\n  test_file_content = """"\n  total_number = len(iris.data)\n  train_number = int(total_number * 0.9)\n  test_number = total_number - train_number\n\n  # Generate text content\n  for i in range(total_number):\n    line_content = str(iris.target[i]) + "",""\n    for j in iris.data[i]:\n      line_content += str(j) + "",""\n    file_content_array.append(line_content[:-1] + ""\\n"")\n\n  random.shuffle(file_content_array)\n  for i in file_content_array[:train_number]:\n    train_file_content += i\n  for i in file_content_array[train_number:]:\n    test_file_content += i\n\n  # Write into files\n  print(""Write content into files: {} and {}"".format(TRAIN_FILE_NAME,\n                                                     TEST_FILE_NAME))\n  with open(TRAIN_FILE_NAME, ""w"") as f:\n    f.write(train_file_content)\n  with open(TEST_FILE_NAME, ""w"") as f:\n    f.write(test_file_content)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
data/iris/generate_csv_tfrecords.py,4,"b'#!/usr/bin/env python\n\nimport tensorflow as tf\nimport os\n\n\ndef generate_tfrecords(input_filename, output_filename):\n  print(""Start to convert {} to {}"".format(input_filename, output_filename))\n  writer = tf.python_io.TFRecordWriter(output_filename)\n\n  for line in open(input_filename, ""r""):\n    data = line.split("","")\n    label = float(data[0])\n    features = [float(i) for i in data[1:]]\n\n    example = tf.train.Example(features=tf.train.Features(feature={\n        ""label"":\n        tf.train.Feature(float_list=tf.train.FloatList(value=[label])),\n        ""features"":\n        tf.train.Feature(float_list=tf.train.FloatList(value=features)),\n    }))\n    writer.write(example.SerializeToString())\n\n  writer.close()\n  print(""Successfully convert {} to {}"".format(input_filename,\n                                               output_filename))\n\n\ndef main():\n  current_path = os.getcwd()\n  for filename in os.listdir(current_path):\n    if filename.startswith("""") and filename.endswith("".csv""):\n      generate_tfrecords(filename, filename + "".tfrecords"")\n\n\nif __name__ == ""__main__"":\n  main()\n'"
data/iris/print_csv_tfrecords.py,2,"b'#!/usr/bin/env python\n\nimport tensorflow as tf\nimport os\n\n\ndef print_tfrecords(input_filename):\n  max_print_number = 100\n  current_print_number = 0\n\n  for serialized_example in tf.python_io.tf_record_iterator(input_filename):\n    # Get serialized example from file\n    example = tf.train.Example()\n    example.ParseFromString(serialized_example)\n    label = example.features.feature[""label""].float_list.value\n    features = example.features.feature[""features""].float_list.value\n    print(""Number: {}, label: {}, features: {}"".format(current_print_number,\n                                                       label, features))\n\n    # Return when reaching max print number\n    current_print_number += 1\n    if current_print_number > max_print_number:\n      exit()\n\n\ndef main():\n  current_path = os.getcwd()\n  tfrecords_file_name = ""iris_train.csv.tfrecords""\n  input_filename = os.path.join(current_path, tfrecords_file_name)\n  print_tfrecords(input_filename)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
data/lung/convert_dcm_to_csv.py,0,"b'#!/usr/bin/env python\n\nfrom pydicom import dicomio\nimport numpy as np\n\n\ndef main():\n  # Read label file\n  label_filename = ""./raw_data/stage1_labels.csv""\n  id_label_map = {}\n  for i in open(label_filename, ""r""):\n    if i.startswith(""id,cancer""):\n      continue\n    id_label_pair = i.split("","")\n    id_label_map[id_label_pair[0]] = int(id_label_pair[1].rstrip())\n\n  # Read dcm file\n  input_filename = ""fa7a21165ae152b13def786e6afc3edf.dcm""\n  output_filename = input_filename + "".csv""\n  convert_dcm_to_csv(id_label_map, input_filename, output_filename)\n\n\ndef convert_dcm_to_csv(id_label_map, input_filename, output_filename):\n  print(""Start to convert dcm: {} to csv: {}"".format(input_filename,\n                                                     output_filename))\n\n  ds = dicomio.read_file(input_filename)\n  image_ndarray = ds.pixel_array\n  label = id_label_map[ds.PatientID]\n  csv_content = ""{},"".format(label)\n\n  # Example: 512 * 512\n  for i in image_ndarray:\n    for j in i:\n      csv_content += ""{},"".format(j)\n  csv_content = csv_content[:-1] + ""\\n""\n\n  with open(output_filename, ""w"") as f:\n    f.write(csv_content)\n\n  print(""Successfully convert dcm: {} to csv: {}"".format(input_filename,\n                                                         output_filename))\n\n\nif __name__ == ""__main__"":\n  main()\n'"
data/lung/generate_csv_tfrecords.py,4,"b'#!/usr/bin/env python\n\nimport tensorflow as tf\nimport os\n\n\ndef generate_tfrecords(input_filename, output_filename):\n  print(""Start to convert {} to {}"".format(input_filename, output_filename))\n  writer = tf.python_io.TFRecordWriter(output_filename)\n\n  for line in open(input_filename, ""r""):\n    data = line.split("","")\n    label = float(data[0])\n    features = [float(i) for i in data[1:]]\n\n    example = tf.train.Example(features=tf.train.Features(feature={\n        ""label"":\n        tf.train.Feature(float_list=tf.train.FloatList(value=[label])),\n        ""features"":\n        tf.train.Feature(float_list=tf.train.FloatList(value=features)),\n    }))\n    writer.write(example.SerializeToString())\n\n  writer.close()\n  print(""Successfully convert {} to {}"".format(input_filename,\n                                               output_filename))\n\n\ndef main():\n  current_path = os.getcwd()\n  for filename in os.listdir(current_path):\n    if filename.startswith("""") and filename.endswith("".csv""):\n      generate_tfrecords(filename, filename + "".tfrecords"")\n\n\nif __name__ == ""__main__"":\n  main()\n'"
data/lung/print_csv_tfrecords.py,2,"b'#!/usr/bin/env python\n\nimport tensorflow as tf\nimport os\n\n\ndef print_tfrecords(input_filename):\n  max_print_number = 100\n  current_print_number = 0\n\n  for serialized_example in tf.python_io.tf_record_iterator(input_filename):\n    # Get serialized example from file\n    example = tf.train.Example()\n    example.ParseFromString(serialized_example)\n    label = example.features.feature[""label""].float_list.value\n    features = example.features.feature[""features""].float_list.value\n    print(""Number: {}, label: {}, features: {}"".format(current_print_number,\n                                                       label, features))\n\n    # Return when reaching max print number\n    current_print_number += 1\n    if current_print_number > max_print_number:\n      exit()\n\n\ndef main():\n  current_path = os.getcwd()\n  tfrecords_file_name = ""fa7a21165ae152b13def786e6afc3edf.dcm.csv.tfrecords""\n  input_filename = os.path.join(current_path, tfrecords_file_name)\n  print_tfrecords(input_filename)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
http_service/cancer_predict/__init__.py,0,b''
http_service/cancer_predict/admin.py,0,b'from django.contrib import admin\n\n# Register your models here.\n'
http_service/cancer_predict/apps.py,0,"b""from __future__ import unicode_literals\n\nfrom django.apps import AppConfig\n\n\nclass CancerPredictConfig(AppConfig):\n    name = 'cancer_predict'\n"""
http_service/cancer_predict/models.py,0,b'from __future__ import unicode_literals\n\nfrom django.db import models\n\n# Create your models here.\n'
http_service/cancer_predict/tests.py,0,b'from django.test import TestCase\n\n# Create your tests here.\n'
http_service/cancer_predict/urls.py,0,"b""from django.conf.urls import url\nfrom . import views\n\nurlpatterns = [\n    url(r'^predict/', views.predict,\n        name='predict'),\n    url(r'^$', views.index, name='index'),\n]\n"""
http_service/cancer_predict/views.py,5,"b'from django.shortcuts import render\nfrom django.http import HttpResponse\nfrom django.views.decorators.csrf import csrf_exempt\n\nimport tensorflow as tf\nimport numpy as np\nimport json\n\n\nclass PredictService(object):\n    def __init__(self, checkpoint_path, checkpoint_file):\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_file = checkpoint_file\n        self.sess = None\n        self.inputs = None\n        self.outputs = None\n\n        self.init_session_handler()\n\n    def init_session_handler(self):\n        self.sess = tf.Session()\n\n        ckpt = tf.train.get_checkpoint_state(self.checkpoint_path)\n        if ckpt and ckpt.model_checkpoint_path:\n            print(""Use the model {}"".format(ckpt.model_checkpoint_path))\n            saver = tf.train.import_meta_graph(self.checkpoint_file)\n            saver.restore(self.sess, ckpt.model_checkpoint_path)\n\n            self.inputs = json.loads(tf.get_collection(\'inputs\')[0])\n            self.outputs = json.loads(tf.get_collection(\'outputs\')[0])\n        else:\n            print(""No model found, exit now"")\n            exit()\n\n    def predict(self, examples):\n        feed_dict = {}\n        for k, v in self.inputs.items():\n            feed_dict[v] = np.array(examples[k])\n        result = self.sess.run(self.outputs, feed_dict=feed_dict)\n        print(""Request examples: {}, inference result: {}"".format(examples,\n                                                                  result))\n        return result\n\n\ncheckpoint_path = ""../checkpoint/""\ncheckpoint_file = ""../checkpoint/checkpoint.ckpt-10.meta""\npredict_service = PredictService(checkpoint_path, checkpoint_file)\n\n\ndef index(request):\n    return HttpResponse(\n        ""You should POST /cancer_predict/predict/ ."")\n\n\n# Disable CSRF, refer to https://docs.djangoproject.com/en/dev/ref/csrf/#edge-cases\n@csrf_exempt\ndef predict(request):\n    if request.method == \'POST\':\n        # The post body should be json, such as {""key"": [1.0, 2.0], ""features"": [[10,10,10,8,6,1,8,9,1], [6,2,1,1,1,1,7,1,1]]}\n        body = json.loads(request.body)\n\n        result = predict_service.predict(body)\n        return HttpResponse(""Success to predict cancer, result: {}"".format(\n            result))\n    else:\n        return HttpResponse(""Please use POST to request with data"")\n'"
http_service/restful_server/__init__.py,0,b''
http_service/restful_server/settings.py,0,"b'""""""\nDjango settings for restful_server project.\n\nGenerated by \'django-admin startproject\' using Django 1.9.8.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/1.9/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/1.9/ref/settings/\n""""""\n\nimport os\n\n# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/1.9/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = \'5jeg*%=e7r7*=z*f-5+uz(l3wbe3&1_#306wc6iry!u4shd7)-\'\n\n# SECURITY WARNING: don\'t run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = []\n\n# Application definition\n\nINSTALLED_APPS = [\n    \'django.contrib.admin\',\n    \'django.contrib.auth\',\n    \'django.contrib.contenttypes\',\n    \'django.contrib.sessions\',\n    \'django.contrib.messages\',\n    \'django.contrib.staticfiles\',\n]\n\nMIDDLEWARE_CLASSES = [\n    \'django.middleware.security.SecurityMiddleware\',\n    \'django.contrib.sessions.middleware.SessionMiddleware\',\n    \'django.middleware.common.CommonMiddleware\',\n    \'django.middleware.csrf.CsrfViewMiddleware\',\n    \'django.contrib.auth.middleware.AuthenticationMiddleware\',\n    \'django.contrib.auth.middleware.SessionAuthenticationMiddleware\',\n    \'django.contrib.messages.middleware.MessageMiddleware\',\n    \'django.middleware.clickjacking.XFrameOptionsMiddleware\',\n]\n\nROOT_URLCONF = \'restful_server.urls\'\n\nTEMPLATES = [\n    {\n        \'BACKEND\': \'django.template.backends.django.DjangoTemplates\',\n        \'DIRS\': [],\n        \'APP_DIRS\': True,\n        \'OPTIONS\': {\n            \'context_processors\': [\n                \'django.template.context_processors.debug\',\n                \'django.template.context_processors.request\',\n                \'django.contrib.auth.context_processors.auth\',\n                \'django.contrib.messages.context_processors.messages\',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = \'restful_server.wsgi.application\'\n\n# Database\n# https://docs.djangoproject.com/en/1.9/ref/settings/#databases\n\nDATABASES = {\n    \'default\': {\n        \'ENGINE\': \'django.db.backends.sqlite3\',\n        \'NAME\': os.path.join(BASE_DIR, \'db.sqlite3\'),\n    }\n}\n\n# Password validation\n# https://docs.djangoproject.com/en/1.9/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        \'NAME\':\n        \'django.contrib.auth.password_validation.UserAttributeSimilarityValidator\',\n    },\n    {\n        \'NAME\':\n        \'django.contrib.auth.password_validation.MinimumLengthValidator\',\n    },\n    {\n        \'NAME\':\n        \'django.contrib.auth.password_validation.CommonPasswordValidator\',\n    },\n    {\n        \'NAME\':\n        \'django.contrib.auth.password_validation.NumericPasswordValidator\',\n    },\n]\n\n# Internationalization\n# https://docs.djangoproject.com/en/1.9/topics/i18n/\n\nLANGUAGE_CODE = \'en-us\'\n\nTIME_ZONE = \'UTC\'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/1.9/howto/static-files/\n\nSTATIC_URL = \'/static/\'\n'"
http_service/restful_server/urls.py,0,"b'""""""restful_server URL Configuration\n\nThe `urlpatterns` list routes URLs to views. For more information please see:\n    https://docs.djangoproject.com/en/1.9/topics/http/urls/\nExamples:\nFunction views\n    1. Add an import:  from my_app import views\n    2. Add a URL to urlpatterns:  url(r\'^$\', views.home, name=\'home\')\nClass-based views\n    1. Add an import:  from other_app.views import Home\n    2. Add a URL to urlpatterns:  url(r\'^$\', Home.as_view(), name=\'home\')\nIncluding another URLconf\n    1. Import the include() function: from django.conf.urls import url, include\n    2. Add a URL to urlpatterns:  url(r\'^blog/\', include(\'blog.urls\'))\n""""""\nfrom django.conf.urls import url\nfrom django.contrib import admin\n\nfrom django.conf.urls import include, url\n\nurlpatterns = [\n    url(r\'^cancer_predict/\', include(\'cancer_predict.urls\')),\n    url(r\'^admin/\', admin.site.urls),\n]\n'"
http_service/restful_server/wsgi.py,0,"b'""""""\nWSGI config for restful_server project.\n\nIt exposes the WSGI callable as a module-level variable named ``application``.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/1.9/howto/deployment/wsgi/\n""""""\n\nimport os\n\nfrom django.core.wsgi import get_wsgi_application\n\nos.environ.setdefault(""DJANGO_SETTINGS_MODULE"",\n                      ""restful_server.settings"")\n\napplication = get_wsgi_application()\n'"
minimal_model/python_predict_client/benchmark_latency.py,10,"b'#!/usr/bin/env python\n\nimport numpy\nimport time\n\nfrom grpc.beta import implementations\nimport tensorflow as tf\n\nimport predict_pb2\nimport prediction_service_pb2\n\ntf.app.flags.DEFINE_string(""host"", ""127.0.0.1"", ""gRPC server host"")\ntf.app.flags.DEFINE_integer(""port"", 9000, ""gRPC server port"")\ntf.app.flags.DEFINE_string(""model_name"", ""cancer"", ""TensorFlow model name"")\ntf.app.flags.DEFINE_integer(""model_version"", 1, ""TensorFlow model version"")\ntf.app.flags.DEFINE_float(""request_timeout"", 10.0, ""Timeout of gRPC request"")\ntf.app.flags.DEFINE_integer(""benchmark_batch_size"", 1, """")\ntf.app.flags.DEFINE_integer(""benchmark_test_number"", 10000, """")\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main():\n  host = FLAGS.host\n  port = FLAGS.port\n  model_name = FLAGS.model_name\n  model_version = FLAGS.model_version\n  request_timeout = FLAGS.request_timeout\n\n  request_batch = FLAGS.benchmark_batch_size\n  request_data = [ i for i in range(request_batch)]\n  # Generate inference data\n  features = numpy.asarray(\n      request_data)\n  features_tensor_proto = tf.contrib.util.make_tensor_proto(features,\n                                                            dtype=tf.float32)\n\n  # Create gRPC client and request\n  channel = implementations.insecure_channel(host, port)\n  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n  request = predict_pb2.PredictRequest()\n  request.model_spec.name = model_name\n  request.model_spec.version.value = model_version\n  request.inputs[\'features\'].CopyFrom(features_tensor_proto)\n\n  # Send request\n\n  request_number = FLAGS.benchmark_test_number\n  start_time = time.time()\n  for i in range(request_number):\n    result = stub.Predict(request, request_timeout)\n\n  end_time = time.time()\n  print(""Average latency is: {} ms"".format((end_time - start_time) * 1000 / request_number))\n\n  #print(result)\n\n\nif __name__ == \'__main__\':\n  main()\n'"
minimal_model/python_predict_client/benchmark_qps.py,11,"b'#!/usr/bin/env python\n\nimport numpy\nimport multiprocessing\nimport threading\nimport time\n\nfrom grpc.beta import implementations\nimport tensorflow as tf\n\nimport predict_pb2\nimport prediction_service_pb2\n\ntf.app.flags.DEFINE_string(""host"", ""127.0.0.1"", ""gRPC server host"")\ntf.app.flags.DEFINE_integer(""port"", 9000, ""gRPC server port"")\ntf.app.flags.DEFINE_string(""model_name"", ""cancer"", ""TensorFlow model name"")\ntf.app.flags.DEFINE_integer(""model_version"", 1, ""TensorFlow model version"")\ntf.app.flags.DEFINE_float(""request_timeout"", 100.0, ""Timeout of gRPC request"")\ntf.app.flags.DEFINE_integer(""benchmark_batch_size"", 1, """")\ntf.app.flags.DEFINE_integer(""benchmark_test_number"", 10000, """")\ntf.app.flags.DEFINE_integer(""benchmark_thread_number"", 10, """")\nFLAGS = tf.app.flags.FLAGS\n\n\ndef _create_rpc_callback(event):\n  def _callback(result_future):\n    event.set()\n    exception = result_future.exception()\n    if exception:\n      print(exception)\n    else:\n      pass\n      #print(""No exception after callback"")\n\n  return _callback\n\n\ndef test_one_process(i):\n  host = FLAGS.host\n  port = FLAGS.port\n  model_name = FLAGS.model_name\n  model_version = FLAGS.model_version\n  request_timeout = FLAGS.request_timeout\n\n  request_batch = FLAGS.benchmark_batch_size\n  request_data = [i for i in range(request_batch)]\n  # Generate inference data\n  features = numpy.asarray(request_data)\n  features_tensor_proto = tf.contrib.util.make_tensor_proto(features,\n                                                            dtype=tf.float32)\n\n  # Create gRPC client and request\n  channel = implementations.insecure_channel(host, port)\n  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n  request = predict_pb2.PredictRequest()\n  request.model_spec.name = model_name\n  request.model_spec.version.value = model_version\n  request.inputs[\'features\'].CopyFrom(features_tensor_proto)\n\n  # Send request\n  request_number = FLAGS.benchmark_test_number\n  #start_time = time.time()\n\n  events = []\n  for i in range(request_number):\n    event = threading.Event()\n    result_future = stub.Predict.future(request, request_timeout)\n    #result_future = stub.Predict.future(request, 0.00000001)\n    result_future.add_done_callback(_create_rpc_callback(event))\n    events.append(event)\n    #result = stub.Predict(request, request_timeout)\n\n    #end_time = time.time()\n    #print(""Average latency is: {} ms"".format((end_time - start_time) * 1000 / request_number))\n    #print(""Average qps is: {}"".format(request_number / (end_time - start_time)))\n\n  for event in events:\n    event.wait()\n\n\ndef main():\n  thread_number = FLAGS.benchmark_thread_number\n\n  p = multiprocessing.Pool(thread_number)\n\n  start_time = time.time()\n\n  for i in range(thread_number):\n    p.apply_async(test_one_process, args=(i, ))\n\n  p.close()\n  p.join()\n\n  end_time = time.time()\n  request_number = FLAGS.benchmark_test_number * thread_number\n  print(""Average qps is: {}"".format(request_number / (end_time - start_time)))\n\n\nif __name__ == \'__main__\':\n  main()\n'"
minimal_model/python_predict_client/model_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: model.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom google.protobuf import wrappers_pb2 as google_dot_protobuf_dot_wrappers__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'model.proto\',\n  package=\'tensorflow.serving\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n\\x0bmodel.proto\\x12\\x12tensorflow.serving\\x1a\\x1egoogle/protobuf/wrappers.proto\\""_\\n\\tModelSpec\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12,\\n\\x07version\\x18\\x02 \\x01(\\x0b\\x32\\x1b.google.protobuf.Int64Value\\x12\\x16\\n\\x0esignature_name\\x18\\x03 \\x01(\\tB\\x03\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[google_dot_protobuf_dot_wrappers__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_MODELSPEC = _descriptor.Descriptor(\n  name=\'ModelSpec\',\n  full_name=\'tensorflow.serving.ModelSpec\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorflow.serving.ModelSpec.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'version\', full_name=\'tensorflow.serving.ModelSpec.version\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'signature_name\', full_name=\'tensorflow.serving.ModelSpec.signature_name\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=67,\n  serialized_end=162,\n)\n\n_MODELSPEC.fields_by_name[\'version\'].message_type = google_dot_protobuf_dot_wrappers__pb2._INT64VALUE\nDESCRIPTOR.message_types_by_name[\'ModelSpec\'] = _MODELSPEC\n\nModelSpec = _reflection.GeneratedProtocolMessageType(\'ModelSpec\', (_message.Message,), dict(\n  DESCRIPTOR = _MODELSPEC,\n  __module__ = \'model_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.ModelSpec)\n  ))\n_sym_db.RegisterMessage(ModelSpec)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\370\\001\\001\'))\nimport grpc\nfrom grpc.beta import implementations as beta_implementations\nfrom grpc.beta import interfaces as beta_interfaces\nfrom grpc.framework.common import cardinality\nfrom grpc.framework.interfaces.face import utilities as face_utilities\n# @@protoc_insertion_point(module_scope)\n'"
minimal_model/python_predict_client/predict_client.py,8,"b'#!/usr/bin/env python\n\nimport numpy\n\nfrom grpc.beta import implementations\nimport tensorflow as tf\n\nimport predict_pb2\nimport prediction_service_pb2\n\ntf.app.flags.DEFINE_string(""host"", ""127.0.0.1"", ""gRPC server host"")\ntf.app.flags.DEFINE_integer(""port"", 9000, ""gRPC server port"")\ntf.app.flags.DEFINE_string(""model_name"", ""cancer"", ""TensorFlow model name"")\ntf.app.flags.DEFINE_integer(""model_version"", 1, ""TensorFlow model version"")\ntf.app.flags.DEFINE_float(""request_timeout"", 10.0, ""Timeout of gRPC request"")\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main():\n  host = FLAGS.host\n  port = FLAGS.port\n  model_name = FLAGS.model_name\n  model_version = FLAGS.model_version\n  request_timeout = FLAGS.request_timeout\n\n  # Generate inference data\n  features = numpy.asarray(\n      [1, 2, 3, 4, 5, 6, 7, 8, 9])\n  features_tensor_proto = tf.contrib.util.make_tensor_proto(features,\n                                                            dtype=tf.float32)\n\n  # Create gRPC client and request\n  channel = implementations.insecure_channel(host, port)\n  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n  request = predict_pb2.PredictRequest()\n  request.model_spec.name = model_name\n  request.model_spec.version.value = model_version\n  request.inputs[\'features\'].CopyFrom(features_tensor_proto)\n\n  # Send request\n  result = stub.Predict(request, request_timeout)\n  print(result)\n\n\nif __name__ == \'__main__\':\n  main()\n'"
minimal_model/python_predict_client/predict_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: predict.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\nimport model_pb2 as model__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'predict.proto\',\n  package=\'tensorflow.serving\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n\\rpredict.proto\\x12\\x12tensorflow.serving\\x1a&tensorflow/core/framework/tensor.proto\\x1a\\x0bmodel.proto\\""\\xe2\\x01\\n\\x0ePredictRequest\\x12\\x31\\n\\nmodel_spec\\x18\\x01 \\x01(\\x0b\\x32\\x1d.tensorflow.serving.ModelSpec\\x12>\\n\\x06inputs\\x18\\x02 \\x03(\\x0b\\x32..tensorflow.serving.PredictRequest.InputsEntry\\x12\\x15\\n\\routput_filter\\x18\\x03 \\x03(\\t\\x1a\\x46\\n\\x0bInputsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12&\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x17.tensorflow.TensorProto:\\x02\\x38\\x01\\""\\x9d\\x01\\n\\x0fPredictResponse\\x12\\x41\\n\\x07outputs\\x18\\x01 \\x03(\\x0b\\x32\\x30.tensorflow.serving.PredictResponse.OutputsEntry\\x1aG\\n\\x0cOutputsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12&\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x17.tensorflow.TensorProto:\\x02\\x38\\x01\\x42\\x03\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_framework_dot_tensor__pb2.DESCRIPTOR,model__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_PREDICTREQUEST_INPUTSENTRY = _descriptor.Descriptor(\n  name=\'InputsEntry\',\n  full_name=\'tensorflow.serving.PredictRequest.InputsEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.serving.PredictRequest.InputsEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.serving.PredictRequest.InputsEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=247,\n  serialized_end=317,\n)\n\n_PREDICTREQUEST = _descriptor.Descriptor(\n  name=\'PredictRequest\',\n  full_name=\'tensorflow.serving.PredictRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'model_spec\', full_name=\'tensorflow.serving.PredictRequest.model_spec\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'inputs\', full_name=\'tensorflow.serving.PredictRequest.inputs\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'output_filter\', full_name=\'tensorflow.serving.PredictRequest.output_filter\', index=2,\n      number=3, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_PREDICTREQUEST_INPUTSENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=91,\n  serialized_end=317,\n)\n\n\n_PREDICTRESPONSE_OUTPUTSENTRY = _descriptor.Descriptor(\n  name=\'OutputsEntry\',\n  full_name=\'tensorflow.serving.PredictResponse.OutputsEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.serving.PredictResponse.OutputsEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.serving.PredictResponse.OutputsEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=406,\n  serialized_end=477,\n)\n\n_PREDICTRESPONSE = _descriptor.Descriptor(\n  name=\'PredictResponse\',\n  full_name=\'tensorflow.serving.PredictResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'outputs\', full_name=\'tensorflow.serving.PredictResponse.outputs\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_PREDICTRESPONSE_OUTPUTSENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=320,\n  serialized_end=477,\n)\n\n_PREDICTREQUEST_INPUTSENTRY.fields_by_name[\'value\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__pb2._TENSORPROTO\n_PREDICTREQUEST_INPUTSENTRY.containing_type = _PREDICTREQUEST\n_PREDICTREQUEST.fields_by_name[\'model_spec\'].message_type = model__pb2._MODELSPEC\n_PREDICTREQUEST.fields_by_name[\'inputs\'].message_type = _PREDICTREQUEST_INPUTSENTRY\n_PREDICTRESPONSE_OUTPUTSENTRY.fields_by_name[\'value\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__pb2._TENSORPROTO\n_PREDICTRESPONSE_OUTPUTSENTRY.containing_type = _PREDICTRESPONSE\n_PREDICTRESPONSE.fields_by_name[\'outputs\'].message_type = _PREDICTRESPONSE_OUTPUTSENTRY\nDESCRIPTOR.message_types_by_name[\'PredictRequest\'] = _PREDICTREQUEST\nDESCRIPTOR.message_types_by_name[\'PredictResponse\'] = _PREDICTRESPONSE\n\nPredictRequest = _reflection.GeneratedProtocolMessageType(\'PredictRequest\', (_message.Message,), dict(\n\n  InputsEntry = _reflection.GeneratedProtocolMessageType(\'InputsEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _PREDICTREQUEST_INPUTSENTRY,\n    __module__ = \'predict_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.serving.PredictRequest.InputsEntry)\n    ))\n  ,\n  DESCRIPTOR = _PREDICTREQUEST,\n  __module__ = \'predict_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.PredictRequest)\n  ))\n_sym_db.RegisterMessage(PredictRequest)\n_sym_db.RegisterMessage(PredictRequest.InputsEntry)\n\nPredictResponse = _reflection.GeneratedProtocolMessageType(\'PredictResponse\', (_message.Message,), dict(\n\n  OutputsEntry = _reflection.GeneratedProtocolMessageType(\'OutputsEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _PREDICTRESPONSE_OUTPUTSENTRY,\n    __module__ = \'predict_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.serving.PredictResponse.OutputsEntry)\n    ))\n  ,\n  DESCRIPTOR = _PREDICTRESPONSE,\n  __module__ = \'predict_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.PredictResponse)\n  ))\n_sym_db.RegisterMessage(PredictResponse)\n_sym_db.RegisterMessage(PredictResponse.OutputsEntry)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\370\\001\\001\'))\n_PREDICTREQUEST_INPUTSENTRY.has_options = True\n_PREDICTREQUEST_INPUTSENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\n_PREDICTRESPONSE_OUTPUTSENTRY.has_options = True\n_PREDICTRESPONSE_OUTPUTSENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\nimport grpc\nfrom grpc.beta import implementations as beta_implementations\nfrom grpc.beta import interfaces as beta_interfaces\nfrom grpc.framework.common import cardinality\nfrom grpc.framework.interfaces.face import utilities as face_utilities\n# @@protoc_insertion_point(module_scope)\n'"
minimal_model/python_predict_client/prediction_service_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: prediction_service.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nimport predict_pb2 as predict__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'prediction_service.proto\',\n  package=\'tensorflow.serving\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n\\x18prediction_service.proto\\x12\\x12tensorflow.serving\\x1a\\rpredict.proto2g\\n\\x11PredictionService\\x12R\\n\\x07Predict\\x12\\"".tensorflow.serving.PredictRequest\\x1a#.tensorflow.serving.PredictResponseB\\x03\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[predict__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\370\\001\\001\'))\nimport grpc\nfrom grpc.beta import implementations as beta_implementations\nfrom grpc.beta import interfaces as beta_interfaces\nfrom grpc.framework.common import cardinality\nfrom grpc.framework.interfaces.face import utilities as face_utilities\n\n\nclass PredictionServiceStub(object):\n  """"""PredictionService provides access to machine-learned models loaded by\n  model_servers.\n  """"""\n\n  def __init__(self, channel):\n    """"""Constructor.\n\n    Args:\n      channel: A grpc.Channel.\n    """"""\n    self.Predict = channel.unary_unary(\n        \'/tensorflow.serving.PredictionService/Predict\',\n        request_serializer=predict__pb2.PredictRequest.SerializeToString,\n        response_deserializer=predict__pb2.PredictResponse.FromString,\n        )\n\n\nclass PredictionServiceServicer(object):\n  """"""PredictionService provides access to machine-learned models loaded by\n  model_servers.\n  """"""\n\n  def Predict(self, request, context):\n    """"""Predict -- provides access to loaded TensorFlow model.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n\ndef add_PredictionServiceServicer_to_server(servicer, server):\n  rpc_method_handlers = {\n      \'Predict\': grpc.unary_unary_rpc_method_handler(\n          servicer.Predict,\n          request_deserializer=predict__pb2.PredictRequest.FromString,\n          response_serializer=predict__pb2.PredictResponse.SerializeToString,\n      ),\n  }\n  generic_handler = grpc.method_handlers_generic_handler(\n      \'tensorflow.serving.PredictionService\', rpc_method_handlers)\n  server.add_generic_rpc_handlers((generic_handler,))\n\n\nclass BetaPredictionServiceServicer(object):\n  """"""PredictionService provides access to machine-learned models loaded by\n  model_servers.\n  """"""\n  def Predict(self, request, context):\n    """"""Predict -- provides access to loaded TensorFlow model.\n    """"""\n    context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n\n\nclass BetaPredictionServiceStub(object):\n  """"""PredictionService provides access to machine-learned models loaded by\n  model_servers.\n  """"""\n  def Predict(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n    """"""Predict -- provides access to loaded TensorFlow model.\n    """"""\n    raise NotImplementedError()\n  Predict.future = None\n\n\ndef beta_create_PredictionService_server(servicer, pool=None, pool_size=None, default_timeout=None, maximum_timeout=None):\n  request_deserializers = {\n    (\'tensorflow.serving.PredictionService\', \'Predict\'): predict__pb2.PredictRequest.FromString,\n  }\n  response_serializers = {\n    (\'tensorflow.serving.PredictionService\', \'Predict\'): predict__pb2.PredictResponse.SerializeToString,\n  }\n  method_implementations = {\n    (\'tensorflow.serving.PredictionService\', \'Predict\'): face_utilities.unary_unary_inline(servicer.Predict),\n  }\n  server_options = beta_implementations.server_options(request_deserializers=request_deserializers, response_serializers=response_serializers, thread_pool=pool, thread_pool_size=pool_size, default_timeout=default_timeout, maximum_timeout=maximum_timeout)\n  return beta_implementations.server(method_implementations, options=server_options)\n\n\ndef beta_create_PredictionService_stub(channel, host=None, metadata_transformer=None, pool=None, pool_size=None):\n  request_serializers = {\n    (\'tensorflow.serving.PredictionService\', \'Predict\'): predict__pb2.PredictRequest.SerializeToString,\n  }\n  response_deserializers = {\n    (\'tensorflow.serving.PredictionService\', \'Predict\'): predict__pb2.PredictResponse.FromString,\n  }\n  cardinalities = {\n    \'Predict\': cardinality.Cardinality.UNARY_UNARY,\n  }\n  stub_options = beta_implementations.stub_options(host=host, metadata_transformer=metadata_transformer, request_serializers=request_serializers, response_deserializers=response_deserializers, thread_pool=pool, thread_pool_size=pool_size)\n  return beta_implementations.dynamic_stub(channel, \'tensorflow.serving.PredictionService\', cardinalities, options=stub_options)\n# @@protoc_insertion_point(module_scope)\n'"
http_service/cancer_predict/migrations/__init__.py,0,b''
