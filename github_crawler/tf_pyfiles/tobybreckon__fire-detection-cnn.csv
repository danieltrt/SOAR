file_path,api_count,code
firenet.py,1,"b'################################################################################\n\n# Example : perform live fire detection in video using FireNet CNN\n\n# Copyright (c) 2017/18 - Andrew Dunnings / Toby Breckon, Durham University, UK\n\n# License : https://github.com/tobybreckon/fire-detection-cnn/blob/master/LICENSE\n\n################################################################################\n\nimport cv2\nimport os\nimport sys\nimport math\n\n################################################################################\n\nimport tflearn\nfrom tflearn.layers.core import *\nfrom tflearn.layers.conv import *\nfrom tflearn.layers.normalization import *\nfrom tflearn.layers.estimator import regression\n\n################################################################################\n\ndef construct_firenet (x,y, training=False):\n\n    # Build network as per architecture in [Dunnings/Breckon, 2018]\n\n    network = tflearn.input_data(shape=[None, y, x, 3], dtype=tf.float32)\n\n    network = conv_2d(network, 64, 5, strides=4, activation=\'relu\')\n\n    network = max_pool_2d(network, 3, strides=2)\n    network = local_response_normalization(network)\n\n    network = conv_2d(network, 128, 4, activation=\'relu\')\n\n    network = max_pool_2d(network, 3, strides=2)\n    network = local_response_normalization(network)\n\n    network = conv_2d(network, 256, 1, activation=\'relu\')\n\n    network = max_pool_2d(network, 3, strides=2)\n    network = local_response_normalization(network)\n\n    network = fully_connected(network, 4096, activation=\'tanh\')\n    if(training):\n        network = dropout(network, 0.5)\n\n    network = fully_connected(network, 4096, activation=\'tanh\')\n    if(training):\n        network = dropout(network, 0.5)\n\n    network = fully_connected(network, 2, activation=\'softmax\')\n\n    # if training then add training hyperparameters\n\n    if(training):\n        network = regression(network, optimizer=\'momentum\',\n                            loss=\'categorical_crossentropy\',\n                            learning_rate=0.001)\n\n    # constuct final model\n\n    model = tflearn.DNN(network, checkpoint_path=\'firenet\',\n                        max_checkpoints=1, tensorboard_verbose=2)\n\n    return model\n\n################################################################################\n\nif __name__ == \'__main__\':\n\n################################################################################\n\n    # construct and display model\n\n    model = construct_firenet (224, 224, training=False)\n    print(""Constructed FireNet ..."")\n\n    model.load(os.path.join(""models/FireNet"", ""firenet""),weights_only=True)\n    print(""Loaded CNN network weights ..."")\n\n################################################################################\n\n    # network input sizes\n\n    rows = 224\n    cols = 224\n\n    # display and loop settings\n\n    windowName = ""Live Fire Detection - FireNet CNN"";\n    keepProcessing = True;\n\n################################################################################\n\n    if len(sys.argv) == 2:\n\n        # load video file from first command line argument\n\n        video = cv2.VideoCapture(sys.argv[1])\n        print(""Loaded video ..."")\n\n        # create window\n\n        cv2.namedWindow(windowName, cv2.WINDOW_NORMAL);\n\n        # get video properties\n\n        width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH));\n        height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        fps = video.get(cv2.CAP_PROP_FPS)\n        frame_time = round(1000/fps);\n\n        while (keepProcessing):\n\n            # start a timer (to see how long processing and display takes)\n\n            start_t = cv2.getTickCount();\n\n            # get video frame from file, handle end of file\n\n            ret, frame = video.read()\n            if not ret:\n                print(""... end of video file reached"");\n                break;\n\n            # re-size image to network input size and perform prediction\n\n            small_frame = cv2.resize(frame, (rows, cols), cv2.INTER_AREA)\n\n            # perform prediction on the image frame which is:\n            # - an image (tensor) of dimension 224 x 224 x 3\n            # - a 3 channel colour image with channel ordering BGR (not RGB)\n            # - un-normalised (i.e. pixel range going into network is 0->255)\n\n            output = model.predict([small_frame])\n\n            # label image based on prediction\n\n            if round(output[0][0]) == 1:\n                cv2.rectangle(frame, (0,0), (width,height), (0,0,255), 50)\n                cv2.putText(frame,\'FIRE\',(int(width/16),int(height/4)),\n                    cv2.FONT_HERSHEY_SIMPLEX, 4,(255,255,255),10,cv2.LINE_AA);\n            else:\n                cv2.rectangle(frame, (0,0), (width,height), (0,255,0), 50)\n                cv2.putText(frame,\'CLEAR\',(int(width/16),int(height/4)),\n                    cv2.FONT_HERSHEY_SIMPLEX, 4,(255,255,255),10,cv2.LINE_AA);\n\n            # stop the timer and convert to ms. (to see how long processing and display takes)\n\n            stop_t = ((cv2.getTickCount() - start_t)/cv2.getTickFrequency()) * 1000;\n\n            # image display and key handling\n\n            cv2.imshow(windowName, frame);\n\n            # wait fps time or less depending on processing time taken (e.g. 1000ms / 25 fps = 40 ms)\n\n            key = cv2.waitKey(max(2, frame_time - int(math.ceil(stop_t)))) & 0xFF;\n            if (key == ord(\'x\')):\n                keepProcessing = False;\n            elif (key == ord(\'f\')):\n                cv2.setWindowProperty(windowName, cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN);\n    else:\n        print(""usage: python firenet.py videofile.ext"");\n\n################################################################################\n'"
inceptionVxOnFire.py,0,"b'################################################################################\n\n# Example : perform live fire detection in video using InceptionV1-OnFire,\n# InceptionV3-OnFire and InceptionV4-OnFire CNN models\n\n# Copyright (c) 2017/18 - Andrew Dunnings / Toby Breckon, Durham University, UK\n# Copyright (c) 2019/20 - Ganesh Samarth / Toby Breckon, Durham University, UK\n\n# License : https://github.com/tobybreckon/fire-detection-cnn/blob/master/LICENSE\n\n################################################################################\n\nimport cv2\nimport os\nimport sys\nimport math\n\n################################################################################\n\nimport tflearn\nfrom tflearn.layers.core import input_data, dropout, fully_connected\nfrom tflearn.layers.conv import conv_2d, max_pool_2d, avg_pool_2d, global_avg_pool\nfrom tflearn.layers.normalization import local_response_normalization, batch_normalization\nfrom tflearn.layers.merge_ops import merge\nfrom tflearn.layers.estimator import regression\n\n################################################################################\n\ndef construct_inceptionv1onfire (x,y, training=False):\n\n    # Build network as per architecture in [Dunnings/Breckon, 2018]\n\n    network = input_data(shape=[None, y, x, 3])\n\n    conv1_7_7 = conv_2d(network, 64, 5, strides=2, activation=\'relu\', name = \'conv1_7_7_s2\')\n\n    pool1_3_3 = max_pool_2d(conv1_7_7, 3,strides=2)\n    pool1_3_3 = local_response_normalization(pool1_3_3)\n\n    conv2_3_3_reduce = conv_2d(pool1_3_3, 64,1, activation=\'relu\',name = \'conv2_3_3_reduce\')\n    conv2_3_3 = conv_2d(conv2_3_3_reduce, 128,3, activation=\'relu\', name=\'conv2_3_3\')\n\n    conv2_3_3 = local_response_normalization(conv2_3_3)\n    pool2_3_3 = max_pool_2d(conv2_3_3, kernel_size=3, strides=2, name=\'pool2_3_3_s2\')\n\n    inception_3a_1_1 = conv_2d(pool2_3_3, 64, 1, activation=\'relu\', name=\'inception_3a_1_1\')\n\n    inception_3a_3_3_reduce = conv_2d(pool2_3_3, 96,1, activation=\'relu\', name=\'inception_3a_3_3_reduce\')\n    inception_3a_3_3 = conv_2d(inception_3a_3_3_reduce, 128,filter_size=3,  activation=\'relu\', name = \'inception_3a_3_3\')\n    inception_3a_5_5_reduce = conv_2d(pool2_3_3,16, filter_size=1,activation=\'relu\', name =\'inception_3a_5_5_reduce\' )\n    inception_3a_5_5 = conv_2d(inception_3a_5_5_reduce, 32, filter_size=5, activation=\'relu\', name= \'inception_3a_5_5\')\n    inception_3a_pool = max_pool_2d(pool2_3_3, kernel_size=3, strides=1, )\n    inception_3a_pool_1_1 = conv_2d(inception_3a_pool, 32, filter_size=1, activation=\'relu\', name=\'inception_3a_pool_1_1\')\n\n    # merge the inception_3a__\n    inception_3a_output = merge([inception_3a_1_1, inception_3a_3_3, inception_3a_5_5, inception_3a_pool_1_1], mode=\'concat\', axis=3)\n\n    inception_3b_1_1 = conv_2d(inception_3a_output, 128,filter_size=1,activation=\'relu\', name= \'inception_3b_1_1\' )\n    inception_3b_3_3_reduce = conv_2d(inception_3a_output, 128, filter_size=1, activation=\'relu\', name=\'inception_3b_3_3_reduce\')\n    inception_3b_3_3 = conv_2d(inception_3b_3_3_reduce, 192, filter_size=3,  activation=\'relu\',name=\'inception_3b_3_3\')\n    inception_3b_5_5_reduce = conv_2d(inception_3a_output, 32, filter_size=1, activation=\'relu\', name = \'inception_3b_5_5_reduce\')\n    inception_3b_5_5 = conv_2d(inception_3b_5_5_reduce, 96, filter_size=5,  name = \'inception_3b_5_5\')\n    inception_3b_pool = max_pool_2d(inception_3a_output, kernel_size=3, strides=1,  name=\'inception_3b_pool\')\n    inception_3b_pool_1_1 = conv_2d(inception_3b_pool, 64, filter_size=1,activation=\'relu\', name=\'inception_3b_pool_1_1\')\n\n    #merge the inception_3b_*\n    inception_3b_output = merge([inception_3b_1_1, inception_3b_3_3, inception_3b_5_5, inception_3b_pool_1_1], mode=\'concat\',axis=3,name=\'inception_3b_output\')\n\n    pool3_3_3 = max_pool_2d(inception_3b_output, kernel_size=3, strides=2, name=\'pool3_3_3\')\n    inception_4a_1_1 = conv_2d(pool3_3_3, 192, filter_size=1, activation=\'relu\', name=\'inception_4a_1_1\')\n    inception_4a_3_3_reduce = conv_2d(pool3_3_3, 96, filter_size=1, activation=\'relu\', name=\'inception_4a_3_3_reduce\')\n    inception_4a_3_3 = conv_2d(inception_4a_3_3_reduce, 208, filter_size=3,  activation=\'relu\', name=\'inception_4a_3_3\')\n    inception_4a_5_5_reduce = conv_2d(pool3_3_3, 16, filter_size=1, activation=\'relu\', name=\'inception_4a_5_5_reduce\')\n    inception_4a_5_5 = conv_2d(inception_4a_5_5_reduce, 48, filter_size=5,  activation=\'relu\', name=\'inception_4a_5_5\')\n    inception_4a_pool = max_pool_2d(pool3_3_3, kernel_size=3, strides=1,  name=\'inception_4a_pool\')\n    inception_4a_pool_1_1 = conv_2d(inception_4a_pool, 64, filter_size=1, activation=\'relu\', name=\'inception_4a_pool_1_1\')\n\n    inception_4a_output = merge([inception_4a_1_1, inception_4a_3_3, inception_4a_5_5, inception_4a_pool_1_1], mode=\'concat\', axis=3, name=\'inception_4a_output\')\n\n    pool5_7_7 = avg_pool_2d(inception_4a_output, kernel_size=5, strides=1)\n    if(training):\n        pool5_7_7 = dropout(pool5_7_7, 0.4)\n    loss = fully_connected(pool5_7_7, 2,activation=\'softmax\')\n\n    # if training then add training hyperparameters\n\n    if(training):\n        network = regression(loss, optimizer=\'momentum\',\n                            loss=\'categorical_crossentropy\',\n                            learning_rate=0.001)\n    else:\n        network = loss;\n\n    model = tflearn.DNN(network, checkpoint_path=\'inceptiononv1onfire\',\n                        max_checkpoints=1, tensorboard_verbose=2)\n\n    return model\n\n################################################################################\n\ndef construct_inceptionv3onfire(x,y, training=False, enable_batch_norm=True):\n\n    # build network as per architecture\n\n    network = input_data(shape=[None, y, x, 3])\n\n    conv1_3_3 = conv_2d(network, 32, 3, strides=2, activation=\'relu\', name = \'conv1_3_3\',padding=\'valid\')\n    conv2_3_3 = conv_2d(conv1_3_3, 32, 3, strides=1, activation=\'relu\', name = \'conv2_3_3\',padding=\'valid\')\n    conv3_3_3 = conv_2d(conv2_3_3, 64, 3, strides=2, activation=\'relu\', name = \'conv3_3_3\')\n\n    pool1_3_3 = max_pool_2d(conv3_3_3, 3,strides=2)\n    if enable_batch_norm:\n        pool1_3_3 = batch_normalization(pool1_3_3)\n    conv1_7_7 = conv_2d(pool1_3_3, 80,3, strides=1, activation=\'relu\', name=\'conv2_7_7_s2\',padding=\'valid\')\n    conv2_7_7 = conv_2d(conv1_7_7, 96,3, strides=1, activation=\'relu\', name=\'conv2_7_7_s2\',padding=\'valid\')\n    pool2_3_3= max_pool_2d(conv2_7_7,3,strides=2)\n\n    inception_3a_1_1 = conv_2d(pool2_3_3,64, filter_size=1, activation=\'relu\', name=\'inception_3a_1_1\')\n\n    inception_3a_3_3_reduce = conv_2d(pool2_3_3, 48, filter_size=1, activation=\'relu\', name=\'inception_3a_3_3_reduce\')\n    inception_3a_3_3 = conv_2d(inception_3a_3_3_reduce, 64, filter_size=[5,5],  activation=\'relu\',name=\'inception_3a_3_3\')\n\n\n    inception_3a_5_5_reduce = conv_2d(pool2_3_3, 64, filter_size=1, activation=\'relu\', name = \'inception_3a_5_5_reduce\')\n    inception_3a_5_5_asym_1 = conv_2d(inception_3a_5_5_reduce, 96, filter_size=[3,3],  name = \'inception_3a_5_5_asym_1\')\n    inception_3a_5_5 = conv_2d(inception_3a_5_5_asym_1, 96, filter_size=[3,3],  name = \'inception_3a_5_5\')\n\n\n    inception_3a_pool = avg_pool_2d(pool2_3_3, kernel_size=3, strides=1,  name=\'inception_3a_pool\')\n    inception_3a_pool_1_1 = conv_2d(inception_3a_pool, 32, filter_size=1, activation=\'relu\', name=\'inception_3a_pool_1_1\')\n\n    # merge the inception_3a\n\n    inception_3a_output = merge([inception_3a_1_1, inception_3a_3_3, inception_3a_5_5, inception_3a_pool_1_1], mode=\'concat\', axis=3, name=\'inception_3a_output\')\n\n\n    inception_5a_1_1 = conv_2d(inception_3a_output, 96, 1, activation=\'relu\', name=\'inception_5a_1_1\')\n\n    inception_5a_3_3_reduce = conv_2d(inception_3a_output, 64, filter_size=1, activation=\'relu\', name=\'inception_5a_3_3_reduce\')\n    inception_5a_3_3_asym_1 = conv_2d(inception_5a_3_3_reduce, 64, filter_size=[1,7],  activation=\'relu\',name=\'inception_5a_3_3_asym_1\')\n    inception_5a_3_3 = conv_2d(inception_5a_3_3_asym_1,96, filter_size=[7,1],  activation=\'relu\',name=\'inception_5a_3_3\')\n\n\n    inception_5a_5_5_reduce = conv_2d(inception_3a_output, 64, filter_size=1, activation=\'relu\', name = \'inception_5a_5_5_reduce\')\n    inception_5a_5_5_asym_1 = conv_2d(inception_5a_5_5_reduce, 64, filter_size=[7,1],  name = \'inception_5a_5_5_asym_1\')\n    inception_5a_5_5_asym_2 = conv_2d(inception_5a_5_5_asym_1, 64, filter_size=[1,7],  name = \'inception_5a_5_5_asym_2\')\n    inception_5a_5_5_asym_3 = conv_2d(inception_5a_5_5_asym_2, 64, filter_size=[7,1],  name = \'inception_5a_5_5_asym_3\')\n    inception_5a_5_5 = conv_2d(inception_5a_5_5_asym_3, 96, filter_size=[1,7],  name = \'inception_5a_5_5\')\n\n\n    inception_5a_pool = avg_pool_2d(inception_3a_output, kernel_size=3, strides=1 )\n    inception_5a_pool_1_1 = conv_2d(inception_5a_pool, 96, filter_size=1, activation=\'relu\', name=\'inception_5a_pool_1_1\')\n\n    # merge the inception_5a__\n    inception_5a_output = merge([inception_5a_1_1, inception_5a_3_3, inception_5a_5_5, inception_5a_pool_1_1], mode=\'concat\', axis=3)\n\n\n\n    inception_7a_1_1 = conv_2d(inception_5a_output, 80, 1, activation=\'relu\', name=\'inception_7a_1_1\')\n    inception_7a_3_3_reduce = conv_2d(inception_5a_output, 96, filter_size=1, activation=\'relu\', name=\'inception_7a_3_3_reduce\')\n    inception_7a_3_3_asym_1 = conv_2d(inception_7a_3_3_reduce, 96, filter_size=[1,3],  activation=\'relu\',name=\'inception_7a_3_3_asym_1\')\n    inception_7a_3_3_asym_2 = conv_2d(inception_7a_3_3_reduce, 96, filter_size=[3,1],  activation=\'relu\',name=\'inception_7a_3_3_asym_2\')\n    inception_7a_3_3=merge([inception_7a_3_3_asym_1,inception_7a_3_3_asym_2],mode=\'concat\',axis=3)\n\n    inception_7a_5_5_reduce = conv_2d(inception_5a_output, 66, filter_size=1, activation=\'relu\', name = \'inception_7a_5_5_reduce\')\n    inception_7a_5_5_asym_1 = conv_2d(inception_7a_5_5_reduce, 96, filter_size=[3,3],  name = \'inception_7a_5_5_asym_1\')\n    inception_7a_5_5_asym_2 = conv_2d(inception_7a_3_3_asym_1, 96, filter_size=[1,3],  activation=\'relu\',name=\'inception_7a_5_5_asym_2\')\n    inception_7a_5_5_asym_3 = conv_2d(inception_7a_3_3_asym_1, 96, filter_size=[3,1],  activation=\'relu\',name=\'inception_7a_5_5_asym_3\')\n    inception_7a_5_5=merge([inception_7a_5_5_asym_2,inception_7a_5_5_asym_3],mode=\'concat\',axis=3)\n\n\n    inception_7a_pool = avg_pool_2d(inception_5a_output, kernel_size=3, strides=1 )\n    inception_7a_pool_1_1 = conv_2d(inception_7a_pool, 96, filter_size=1, activation=\'relu\', name=\'inception_7a_pool_1_1\')\n\n    # merge the inception_7a__\n    inception_7a_output = merge([inception_7a_1_1, inception_7a_3_3, inception_7a_5_5, inception_7a_pool_1_1], mode=\'concat\', axis=3)\n\n\n\n    pool5_7_7=global_avg_pool(inception_7a_output)\n    if(training):\n        pool5_7_7=dropout(pool5_7_7,0.4)\n    loss = fully_connected(pool5_7_7, 2,activation=\'softmax\')\n\n    if(training):\n        network = regression(loss, optimizer=\'rmsprop\',\n                             loss=\'categorical_crossentropy\',\n                             learning_rate=0.001)\n    else:\n        network=loss\n\n    model = tflearn.DNN(network, checkpoint_path=\'inceptionv3\',\n                        max_checkpoints=1, tensorboard_verbose=0)\n\n    return model\n\n################################################################################\n\n# InceptionV4 : definition of inception_block_a\n\ndef inception_block_a(input_a):\n\n    inception_a_conv1_1_1 = conv_2d(input_a,96,1,activation=\'relu\',name=\'inception_a_conv1_1_1\')\n\n    inception_a_conv1_3_3_reduce = conv_2d(input_a,64,1,activation=\'relu\',name=\'inception_a_conv1_3_3_reduce\')\n    inception_a_conv1_3_3 = conv_2d(inception_a_conv1_3_3_reduce,96,3,activation=\'relu\',name=\'inception_a_conv1_3_3\')\n\n    inception_a_conv2_3_3_reduce = conv_2d(input_a,64,1,activation=\'relu\',name=\'inception_a_conv2_3_3_reduce\')\n    inception_a_conv2_3_3_sym_1 = conv_2d(inception_a_conv2_3_3_reduce,96,3,activation=\'relu\',name=\'inception_a_conv2_3_3\')\n    inception_a_conv2_3_3 = conv_2d(inception_a_conv2_3_3_sym_1,96,3,activation=\'relu\',name=\'inception_a_conv2_3_3\')\n\n    inception_a_pool = avg_pool_2d(input_a,kernel_size=3,name=\'inception_a_pool\',strides=1)\n    inception_a_pool_1_1 = conv_2d(inception_a_pool,96,1,activation=\'relu\',name=\'inception_a_pool_1_1\')\n\n    # merge inception_a\n\n    inception_a = merge([inception_a_conv1_1_1,inception_a_conv1_3_3,inception_a_conv2_3_3,inception_a_pool_1_1],mode=\'concat\',axis=3)\n\n    return inception_a\n\n\n################################################################################\n\n# InceptionV4 : definition of reduction_block_a\n\ndef reduction_block_a(reduction_input_a):\n\n    reduction_a_conv1_1_1 = conv_2d(reduction_input_a,384,3,strides=2,padding=\'valid\',activation=\'relu\',name=\'reduction_a_conv1_1_1\')\n\n    reduction_a_conv2_1_1 = conv_2d(reduction_input_a,192,1,activation=\'relu\',name=\'reduction_a_conv2_1_1\')\n    reduction_a_conv2_3_3 = conv_2d(reduction_a_conv2_1_1,224,3,activation=\'relu\',name=\'reduction_a_conv2_3_3\')\n    reduction_a_conv2_3_3_s2 = conv_2d(reduction_a_conv2_3_3,256,3,strides=2,padding=\'valid\',activation=\'relu\',name=\'reduction_a_conv2_3_3_s2\')\n\n    reduction_a_pool = max_pool_2d(reduction_input_a,strides=2,padding=\'valid\',kernel_size=3,name=\'reduction_a_pool\')\n\n    # merge reduction_a\n\n    reduction_a = merge([reduction_a_conv1_1_1,reduction_a_conv2_3_3_s2,reduction_a_pool],mode=\'concat\',axis=3)\n\n    return reduction_a\n\n################################################################################\n\n# InceptionV4 : definition of inception_block_b\n\ndef inception_block_b(input_b):\n\n    inception_b_1_1 = conv_2d(input_b, 384, 1, activation=\'relu\', name=\'inception_b_1_1\')\n\n    inception_b_3_3_reduce = conv_2d(input_b, 192, filter_size=1, activation=\'relu\', name=\'inception_b_3_3_reduce\')\n    inception_b_3_3_asym_1 = conv_2d(inception_b_3_3_reduce, 224, filter_size=[1,7],  activation=\'relu\',name=\'inception_b_3_3_asym_1\')\n    inception_b_3_3 = conv_2d(inception_b_3_3_asym_1, 256, filter_size=[7,1],  activation=\'relu\',name=\'inception_b_3_3\')\n\n\n    inception_b_5_5_reduce = conv_2d(input_b, 192, filter_size=1, activation=\'relu\', name = \'inception_b_5_5_reduce\')\n    inception_b_5_5_asym_1 = conv_2d(inception_b_5_5_reduce, 192, filter_size=[7,1],  name = \'inception_b_5_5_asym_1\')\n    inception_b_5_5_asym_2 = conv_2d(inception_b_5_5_asym_1, 224, filter_size=[1,7],  name = \'inception_b_5_5_asym_2\')\n    inception_b_5_5_asym_3 = conv_2d(inception_b_5_5_asym_2, 224, filter_size=[7,1],  name = \'inception_b_5_5_asym_3\')\n    inception_b_5_5 = conv_2d(inception_b_5_5_asym_3, 256, filter_size=[1,7],  name = \'inception_b_5_5\')\n\n\n    inception_b_pool = avg_pool_2d(input_b, kernel_size=3, strides=1 )\n    inception_b_pool_1_1 = conv_2d(inception_b_pool, 128, filter_size=1, activation=\'relu\', name=\'inception_b_pool_1_1\')\n\n    # merge the inception_b\n\n    inception_b_output = merge([inception_b_1_1, inception_b_3_3, inception_b_5_5, inception_b_pool_1_1], mode=\'concat\', axis=3)\n\n    return inception_b_output\n\n################################################################################\n\n# InceptionV4 : definition of reduction_block_b\n\ndef reduction_block_b(reduction_input_b):\n\n    reduction_b_1_1 = conv_2d(reduction_input_b,192,1,activation=\'relu\',name=\'reduction_b_1_1\')\n    reduction_b_1_3 = conv_2d(reduction_b_1_1,192,3,strides=2,padding=\'valid\',name=\'reduction_b_1_3\')\n\n    reduction_b_3_3_reduce = conv_2d(reduction_input_b, 256, filter_size=1, activation=\'relu\', name=\'reduction_b_3_3_reduce\')\n    reduction_b_3_3_asym_1 = conv_2d(reduction_b_3_3_reduce, 256, filter_size=[1,7],  activation=\'relu\',name=\'reduction_b_3_3_asym_1\')\n    reduction_b_3_3_asym_2 = conv_2d(reduction_b_3_3_asym_1, 320, filter_size=[7,1],  activation=\'relu\',name=\'reduction_b_3_3_asym_2\')\n    reduction_b_3_3=conv_2d(reduction_b_3_3_asym_2,320,3,strides=2,activation=\'relu\',padding=\'valid\',name=\'reduction_b_3_3\')\n\n    reduction_b_pool = max_pool_2d(reduction_input_b,kernel_size=3,strides=2,padding=\'valid\')\n\n    # merge the reduction_b\n\n    reduction_b_output = merge([reduction_b_1_3,reduction_b_3_3,reduction_b_pool],mode=\'concat\',axis=3)\n\n    return reduction_b_output\n\n################################################################################\n\n# InceptionV4 : defintion of inception_block_c\n\ndef inception_block_c(input_c):\n    inception_c_1_1 = conv_2d(input_c, 256, 1, activation=\'relu\', name=\'inception_c_1_1\')\n    inception_c_3_3_reduce = conv_2d(input_c, 384, filter_size=1, activation=\'relu\', name=\'inception_c_3_3_reduce\')\n    inception_c_3_3_asym_1 = conv_2d(inception_c_3_3_reduce, 256, filter_size=[1,3],  activation=\'relu\',name=\'inception_c_3_3_asym_1\')\n    inception_c_3_3_asym_2 = conv_2d(inception_c_3_3_reduce, 256, filter_size=[3,1],  activation=\'relu\',name=\'inception_c_3_3_asym_2\')\n    inception_c_3_3=merge([inception_c_3_3_asym_1,inception_c_3_3_asym_2],mode=\'concat\',axis=3)\n\n    inception_c_5_5_reduce = conv_2d(input_c, 384, filter_size=1, activation=\'relu\', name = \'inception_c_5_5_reduce\')\n    inception_c_5_5_asym_1 = conv_2d(inception_c_5_5_reduce, 448, filter_size=[1,3],  name = \'inception_c_5_5_asym_1\')\n    inception_c_5_5_asym_2 = conv_2d(inception_c_5_5_asym_1, 512, filter_size=[3,1],  activation=\'relu\',name=\'inception_c_5_5_asym_2\')\n    inception_c_5_5_asym_3 = conv_2d(inception_c_5_5_asym_2, 256, filter_size=[1,3],  activation=\'relu\',name=\'inception_c_5_5_asym_3\')\n\n    inception_c_5_5_asym_4 = conv_2d(inception_c_5_5_asym_2, 256, filter_size=[3,1],  activation=\'relu\',name=\'inception_c_5_5_asym_4\')\n    inception_c_5_5=merge([inception_c_5_5_asym_4,inception_c_5_5_asym_3],mode=\'concat\',axis=3)\n\n\n    inception_c_pool = avg_pool_2d(input_c, kernel_size=3, strides=1 )\n    inception_c_pool_1_1 = conv_2d(inception_c_pool, 256, filter_size=1, activation=\'relu\', name=\'inception_c_pool_1_1\')\n\n    # merge the inception_c\n\n    inception_c_output = merge([inception_c_1_1, inception_c_3_3, inception_c_5_5, inception_c_pool_1_1], mode=\'concat\', axis=3)\n\n    return inception_c_output\n\n################################################################################\n\ndef construct_inceptionv4onfire(x,y, training=True, enable_batch_norm=True):\n\n    network = input_data(shape=[None, y, x, 3])\n\n    #stem of inceptionV4\n\n    conv1_3_3 = conv_2d(network,32,3,strides=2,activation=\'relu\',name=\'conv1_3_3_s2\',padding=\'valid\')\n    conv2_3_3 = conv_2d(conv1_3_3,32,3,activation=\'relu\',name=\'conv2_3_3\')\n    conv3_3_3 = conv_2d(conv2_3_3,64,3,activation=\'relu\',name=\'conv3_3_3\')\n    b_conv_1_pool = max_pool_2d(conv3_3_3,kernel_size=3,strides=2,padding=\'valid\',name=\'b_conv_1_pool\')\n    if enable_batch_norm:\n        b_conv_1_pool = batch_normalization(b_conv_1_pool)\n    b_conv_1_conv = conv_2d(conv3_3_3,96,3,strides=2,padding=\'valid\',activation=\'relu\',name=\'b_conv_1_conv\')\n    b_conv_1 = merge([b_conv_1_conv,b_conv_1_pool],mode=\'concat\',axis=3)\n\n    b_conv4_1_1 = conv_2d(b_conv_1,64,1,activation=\'relu\',name=\'conv4_3_3\')\n    b_conv4_3_3 = conv_2d(b_conv4_1_1,96,3,padding=\'valid\',activation=\'relu\',name=\'conv5_3_3\')\n\n    b_conv4_1_1_reduce = conv_2d(b_conv_1,64,1,activation=\'relu\',name=\'b_conv4_1_1_reduce\')\n    b_conv4_1_7 = conv_2d(b_conv4_1_1_reduce,64,[1,7],activation=\'relu\',name=\'b_conv4_1_7\')\n    b_conv4_7_1 = conv_2d(b_conv4_1_7,64,[7,1],activation=\'relu\',name=\'b_conv4_7_1\')\n    b_conv4_3_3_v = conv_2d(b_conv4_7_1,96,3,padding=\'valid\',name=\'b_conv4_3_3_v\')\n    b_conv_4 = merge([b_conv4_3_3_v, b_conv4_3_3],mode=\'concat\',axis=3)\n\n    b_conv5_3_3 = conv_2d(b_conv_4,192,3,padding=\'valid\',activation=\'relu\',name=\'b_conv5_3_3\',strides=2)\n    b_pool5_3_3 = max_pool_2d(b_conv_4,kernel_size=3,padding=\'valid\',strides=2,name=\'b_pool5_3_3\')\n    if enable_batch_norm:\n        b_pool5_3_3 = batch_normalization(b_pool5_3_3)\n    b_conv_5 = merge([b_conv5_3_3,b_pool5_3_3],mode=\'concat\',axis=3)\n    net = b_conv_5\n\n    # inceptionV4 modules\n\n    net=inception_block_a(net)\n\n    net=inception_block_b(net)\n\n    net=inception_block_c(net)\n\n    pool5_7_7=global_avg_pool(net)\n    if(training):\n        pool5_7_7=dropout(pool5_7_7,0.4)\n    loss = fully_connected(pool5_7_7, 2,activation=\'softmax\')\n\n    if(training):\n        network = regression(loss, optimizer=\'rmsprop\',\n                             loss=\'categorical_crossentropy\',\n                             learning_rate=0.001)\n    else:\n        network=loss\n\n    model = tflearn.DNN(network, checkpoint_path=\'inceptionv4onfire\',\n                        max_checkpoints=1, tensorboard_verbose=0)\n\n    return model\n\n################################################################################\n\nif __name__ == \'__main__\':\n\n################################################################################\n\n    # parse command line arguments\n\n    import argparse\n\n    parser = argparse.ArgumentParser(description=\'Perform InceptionV1/V3/V4 fire detection on incoming video\')\n    parser.add_argument(""-m"", ""--model_to_use"", type=int, help=""specify model to use"", default=1, choices={1, 3, 4})\n    parser.add_argument(\'video_file\', metavar=\'video_file\', type=str, help=\'specify video file\')\n    args = parser.parse_args()\n\n    #   construct and display model\n\n    print(""Constructing InceptionV"" + str(args.model_to_use) + ""-OnFire ..."")\n\n    if (args.model_to_use == 1):\n\n        # use InceptionV1-OnFire CNN model - [Dunning/Breckon, 2018]\n\n        model = construct_inceptionv1onfire (224, 224, training=False)\n        # also work around typo in naming of original models for V1 models [Dunning/Breckon, 2018] ""...iononv ...""\n        model.load(os.path.join(""models/InceptionV1-OnFire"", ""inceptiononv1onfire""),weights_only=True)\n\n    elif (args.model_to_use == 3):\n\n        # use InceptionV3-OnFire CNN model - [Samarth/Bhowmik/Breckon, 2019]\n        # N.B. weights_only=False as we are using Batch Normalization, and need those weights loaded also\n\n        model = construct_inceptionv3onfire (224, 224, training=False)\n        model.load(os.path.join(""models/InceptionV3-OnFire"", ""inceptionv3onfire""),weights_only=False)\n\n    elif (args.model_to_use == 4):\n\n        # use InceptionV4-OnFire CNN model - [Samarth/Bhowmik/Breckon, 2019]\n        # N.B. weights_only=False as we are using Batch Normalization, and need those weights loaded also\n\n        model = construct_inceptionv4onfire (224, 224, training=False)\n        model.load(os.path.join(""models/InceptionV4-OnFire"", ""inceptionv4onfire""),weights_only=False)\n\n    print(""Loaded CNN network weights ..."")\n\n################################################################################\n\n    # network input sizes\n\n    rows = 224\n    cols = 224\n\n    # display and loop settings\n\n    windowName = ""Live Fire Detection - InceptionV"" + str(args.model_to_use) + ""-OnFire"";\n    keepProcessing = True;\n\n################################################################################\n\n    # load video file from first command line argument\n\n    video = cv2.VideoCapture(args.video_file)\n    print(""Loaded video ..."")\n\n    # create window\n\n    cv2.namedWindow(windowName, cv2.WINDOW_NORMAL);\n\n    # get video properties\n\n    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH));\n    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = video.get(cv2.CAP_PROP_FPS)\n    frame_time = round(1000/fps);\n\n    while (keepProcessing):\n\n        # start a timer (to see how long processing and display takes)\n\n        start_t = cv2.getTickCount();\n\n        # get video frame from file, handle end of file\n\n        ret, frame = video.read()\n        if not ret:\n            print(""... end of video file reached"");\n            break;\n\n        # re-size image to network input size and perform prediction\n\n        small_frame = cv2.resize(frame, (rows, cols), cv2.INTER_AREA)\n\n        # perform prediction on the image frame which is:\n        # - an image (tensor) of dimension 224 x 224 x 3\n        # - a 3 channel colour image with channel ordering BGR (not RGB)\n        # - un-normalised (i.e. pixel range going into network is 0->255)\n\n        output = model.predict([small_frame])\n\n        # label image based on prediction\n\n        if round(output[0][0]) == 1: # equiv. to 0.5 threshold in [Dunnings / Breckon, 2018],  [Samarth/Bhowmik/Breckon, 2019] test code\n            cv2.rectangle(frame, (0,0), (width,height), (0,0,255), 50)\n            cv2.putText(frame,\'FIRE\',(int(width/16),int(height/4)),\n                cv2.FONT_HERSHEY_SIMPLEX, 4,(255,255,255),10,cv2.LINE_AA);\n        else:\n            cv2.rectangle(frame, (0,0), (width,height), (0,255,0), 50)\n            cv2.putText(frame,\'CLEAR\',(int(width/16),int(height/4)),\n                cv2.FONT_HERSHEY_SIMPLEX, 4,(255,255,255),10,cv2.LINE_AA);\n\n        # stop the timer and convert to ms. (to see how long processing and display takes)\n\n        stop_t = ((cv2.getTickCount() - start_t)/cv2.getTickFrequency()) * 1000;\n\n        # image display and key handling\n\n        cv2.imshow(windowName, frame);\n\n        # wait fps time or less depending on processing time taken (e.g. 1000ms / 25 fps = 40 ms)\n\n        key = cv2.waitKey(max(2, frame_time - int(math.ceil(stop_t)))) & 0xFF;\n        if (key == ord(\'x\')):\n            keepProcessing = False;\n        elif (key == ord(\'f\')):\n            cv2.setWindowProperty(windowName, cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN);\n\n################################################################################\n'"
superpixel-inceptionVxOnFire.py,0,"b'################################################################################\n\n# Example : perform live fire detection in video using superpixel localization\n# and the superpixel trained version of the InceptionV1-OnFire,\n# InceptionV3-OnFire and InceptionV4-OnFire CNN models\n\n# Copyright (c) 2017/18 - Andrew Dunnings / Toby Breckon, Durham University, UK\n# Copyright (c) 2019/20 - Ganesh Samarth / Toby Breckon, Durham University, UK\n\n# License : https://github.com/tobybreckon/fire-detection-cnn/blob/master/LICENSE\n\n################################################################################\n\nimport cv2\nimport os\nimport sys\nimport math\nimport numpy as np\nimport argparse\n\n################################################################################\n\nimport tflearn\nfrom tflearn.layers.core import input_data, dropout, fully_connected\nfrom tflearn.layers.conv import conv_2d, max_pool_2d, avg_pool_2d, global_avg_pool\nfrom tflearn.layers.normalization import local_response_normalization, batch_normalization\nfrom tflearn.layers.merge_ops import merge\nfrom tflearn.layers.estimator import regression\n\n################################################################################\n\nfrom inceptionVxOnFire import construct_inceptionv1onfire, construct_inceptionv3onfire, construct_inceptionv4onfire\n\n################################################################################\n\n# extract non-zero region of interest (ROI) in an otherwise zero\'d image\n\ndef extract_bounded_nonzero(input):\n\n    # take the first channel only (for speed)\n\n    gray = input[:, :, 0];\n\n    # find bounding rectangle of a non-zero region in an numpy array\n    # credit: https://stackoverflow.com/questions/31400769/bounding-box-of-numpy-array\n\n    rows = np.any(gray, axis=1)\n    cols = np.any(gray, axis=0)\n    rmin, rmax = np.where(rows)[0][[0, -1]]\n    cmin, cmax = np.where(cols)[0][[0, -1]]\n\n    # cropping the non zero image\n\n    return input[cmin:cmax,rmin:rmax]\n\n################################################################################\n\n# pad a supplied multi-channel image to the required [X,Y,C] size\n\ndef pad_image(image, new_width, new_height, pad_value = 0):\n\n    # create an image of zeros, the same size as padding target size\n\n    padded = np.zeros((new_width, new_height, image.shape[2]), dtype=np.uint8)\n\n    # compute where our input image will go to centre it within the padded image\n\n    pos_x = int(np.round((new_width / 2) - (image.shape[1] / 2)))\n    pos_y = int(np.round((new_height / 2) - (image.shape[0] / 2)))\n\n    # copy across the data from the input to the position centred within the padded image\n\n    padded[pos_y:image.shape[0]+pos_y,pos_x:image.shape[1]+pos_x] = image\n\n    return padded\n\n################################################################################\n\n# parse command line arguments\n\nparser = argparse.ArgumentParser(description=\'Perform superpixel based InceptionV1/V3/V4 fire detection on incoming video\')\nparser.add_argument(""-m"", ""--model_to_use"", type=int, help=""specify model to use"", default=1, choices={1, 3, 4})\nparser.add_argument(\'video_file\', metavar=\'video_file\', type=str, help=\'specify video file\')\nargs = parser.parse_args()\n\n#   construct and display model\n\nprint(""Constructing SP-InceptionV"" + str(args.model_to_use) + ""-OnFire ..."")\n\nif (args.model_to_use == 1):\n\n    # use InceptionV1-OnFire CNN model - [Dunning/Breckon, 2018]\n\n    model = construct_inceptionv1onfire (224, 224, training=False)\n    # also work around typo in naming of original models for V1 models [Dunning/Breckon, 2018] ""...iononv ...""\n    model.load(os.path.join(""models/SP-InceptionV1-OnFire"", ""sp-inceptiononv1onfire""),weights_only=True)\n\nelif (args.model_to_use == 3):\n\n    # use InceptionV3-OnFire CNN model -  [Samarth/Bhowmik/Breckon, 2019]\n    # N.B. weights_only=False as we are using Batch Normalization, and need those weights loaded also\n\n    model = construct_inceptionv3onfire (224, 224, training=False)\n    model.load(os.path.join(""models/SP-InceptionV3-OnFire"", ""sp-inceptionv3onfire""),weights_only=False)\n\nelif (args.model_to_use == 4):\n\n    # use InceptionV4-OnFire CNN model -  [Samarth/Bhowmik/Breckon, 2019]\n    # N.B. weights_only=False as we are using Batch Normalization, and need those weights loaded also\n\n    model = construct_inceptionv4onfire (224, 224, training=False)\n    model.load(os.path.join(""models/SP-InceptionV4-OnFire"", ""sp-inceptionv4onfire""),weights_only=False)\n\nprint(""Loaded CNN network weights ..."")\n\n################################################################################\n\n# network input sizes\n\nrows = 224\ncols = 224\n\n# display and loop settings\n\nwindowName = ""Live Fire Detection - Superpixels with SP-InceptionV"" + str(args.model_to_use) + ""-OnFire""\nkeepProcessing = True\n\n################################################################################\n\n# load video file from first command line argument\n\nvideo = cv2.VideoCapture(args.video_file)\nprint(""Loaded video ..."")\n\n# create window\n\ncv2.namedWindow(windowName, cv2.WINDOW_NORMAL)\n\n# get video properties\n\nwidth = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = video.get(cv2.CAP_PROP_FPS)\nframe_time = round(1000/fps)\n\nwhile (keepProcessing):\n\n    # start a timer (to see how long processing and display takes)\n\n    start_t = cv2.getTickCount()\n\n    # get video frame from file, handle end of file\n\n    ret, frame = video.read()\n    if not ret:\n        print(""... end of video file reached"")\n        break\n\n    # re-size image to network input size and perform prediction\n\n    small_frame = cv2.resize(frame, (rows, cols), cv2.INTER_AREA)\n\n    # OpenCV imgproc SLIC superpixels implementation below\n\n    slic = cv2.ximgproc.createSuperpixelSLIC(small_frame, region_size=22)\n    slic.iterate(10)\n\n    # getLabels method returns the different superpixel segments\n    segments = slic.getLabels()\n\n    # print(len(np.unique(segments)))\n\n    # loop over the unique segment values\n    for (i, segVal) in enumerate(np.unique(segments)):\n\n        # Construct a mask for the segment\n        mask = np.zeros(small_frame.shape[:2], dtype = ""uint8"")\n        mask[segments == segVal] = 255\n\n        # get contours (first checking if OPENCV >= 4.x)\n\n        if (int(cv2.__version__.split(""."")[0]) >= 4):\n            contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n        else:\n            im2, contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n        # create the superpixel by applying the mask\n\n        # N.B. this creates an image of the full frame with this superpixel being the only non-zero\n        # (i.e. not black) region. CNN training/testing classification is performed using these\n        # full frame size images, rather than isolated small superpixel images.\n        # Using the approach, we re-use the same InceptionV1-OnFire architecture as described in\n        # the paper [Dunnings / Breckon, 2018] with no changes trained on full frame images each\n        # containing an isolated superpixel with the rest of the image being zero/black.\n\n        superpixel = cv2.bitwise_and(small_frame, small_frame, mask = mask)\n\n        # N.B. ... but for the later work using the InceptionV3-OnFire and InceptionV4-OnFire architecture\n        # as described in the paper [Samarth / Breckon, 2019] we instead centre and pad the resulting\n        # image with zeros\n\n        if ((args.model_to_use == 3) or (args.model_to_use == 4)):\n\n            # convert the superpixel from BGR to RGB space\n\n            superpixel = cv2.cvtColor(superpixel, cv2.COLOR_BGR2RGB)\n\n            # center and pad the superpixel in the centre of a (224 x 244 x 3) RGB image\n\n            superpixel = pad_image(extract_bounded_nonzero(superpixel), 224, 224)\n\n        # use loaded model to make prediction on given superpixel segment\n        # which is now:\n        # - an image (tensor) of dimension 224 x 224 x 3 (constructed from the superpixel as per above)\n        # - for InceptionV1-OnFire: a 3 channel colour image with channel ordering BGR (not RGB)\n        # - for InceptionV3-OnFire / InceptionV4-OnFire: a 3 channel colour image with channel ordering RGB\n        # - un-normalised (i.e. pixel range going into network is 0->255)\n\n        output = model.predict([superpixel])\n\n        # we know the green/red label seems back-to-front here (i.e.\n        # green means fire, red means no fire) but this is how we did it\n        # in the paper (?!) so we\'ll just keep the same crazyness for\n        # consistency with the paper figures\n\n        if round(output[0][0]) == 1: # equiv. to 0.5 threshold in [Dunnings / Breckon, 2018],  [Samarth/Bhowmik/Breckon, 2019] test code\n            # draw the contour\n            # if prediction for FIRE was TRUE (round to 1), draw GREEN contour for superpixel\n            cv2.drawContours(small_frame, contours, -1, (0,255,0), 1)\n\n        else:\n            # if prediction for FIRE was FALSE, draw RED contour for superpixel\n            cv2.drawContours(small_frame, contours, -1, (0,0,255), 1)\n\n    # stop the timer and convert to ms. (to see how long processing and display takes)\n\n    stop_t = ((cv2.getTickCount() - start_t)/cv2.getTickFrequency()) * 1000\n\n    # image display and key handling\n\n    cv2.imshow(windowName, small_frame)\n\n    # wait fps time or less depending on processing time taken (e.g. 1000ms / 25 fps = 40 ms)\n\n    key = cv2.waitKey(max(2, frame_time - int(math.ceil(stop_t)))) & 0xFF\n    if (key == ord(\'x\')):\n        keepProcessing = False\n    elif (key == ord(\'f\')):\n        cv2.setWindowProperty(windowName, cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n\n################################################################################\n'"
converter/converter.py,7,"b'################################################################################\n\n# Example : perform conversion from tflearn checkpoint format to TensorFlow\n# protocol buffer (.pb) binary format and also .tflite files (for import into other tools)\n\n# Copyright (c) 2019 Toby Breckon, Durham University, UK\n\n# License : https://github.com/tobybreckon/fire-detection-cnn/blob/master/LICENSE\n\n# Acknowledgements: some portions - tensorflow tutorial examples and URL below\n\n################################################################################\n\nimport glob,os\n\n################################################################################\n\n# import tensorflow api\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import graph_util\nfrom tensorflow.compat.v1.graph_util import convert_variables_to_constants\nfrom tensorflow.tools.graph_transforms import TransformGraph\nfrom tensorflow.python.tools import optimize_for_inference_lib\nfrom tensorflow.compat.v1.lite import TFLiteConverter\n\n################################################################################\n\n# import tflearn api\n\nimport tflearn\nfrom tflearn.layers.core import *\nfrom tflearn.layers.conv import *\nfrom tflearn.layers.normalization import *\nfrom tflearn.layers.estimator import regression\n\n################################################################################\n# convert a loaded model definition by loading a checkpoint from a given path\n# retaining the network between the specified input and output layers\n# outputs to pbfilename as a binary .pb protocol buffer format file\n\n# e.g. for FireNet\n#    model = construct_firenet (224, 224, False)\n#    path = ""models/FireNet/firenet""; # path to tflearn checkpoint including filestem\n#    input_layer_name = \'InputData/X\'                  # input layer of network\n#    output_layer_name= \'FullyConnected_2/Softmax\'     # output layer of network\n#    filename = ""firenet.pb""                              # output filename\n\ndef convert_to_pb(model, path, input_layer_name,  output_layer_name, pbfilename, verbose=False):\n\n  model.load(path,weights_only=True)\n  print(""[INFO] Loaded CNN network weights from "" + path + "" ..."")\n\n  print(""[INFO] Re-export model ..."")\n  del tf.get_collection_ref(tf.GraphKeys.TRAIN_OPS)[:]\n  model.save(""model-tmp.tfl"")\n\n  # taken from: https://stackoverflow.com/questions/34343259/is-there-an-example-on-how-to-generate-protobuf-files-holding-trained-tensorflow\n\n  print(""[INFO] Re-import model ..."")\n\n  input_checkpoint = ""model-tmp.tfl""\n  saver = tf.train.import_meta_graph(input_checkpoint + \'.meta\', True)\n  sess = tf.Session();\n  saver.restore(sess, input_checkpoint)\n\n  # print out all layers to find name of output\n\n  if (verbose):\n      op = sess.graph.get_operations()\n      [print(m.values()) for m in op][1]\n\n  print(""[INFO] Freeze model to "" +  pbfilename + "" ..."")\n\n  # freeze and removes nodes which are not related to feedforward prediction\n\n  minimal_graph = convert_variables_to_constants(sess, sess.graph.as_graph_def(), [output_layer_name])\n\n  graph_def = optimize_for_inference_lib.optimize_for_inference(minimal_graph, [input_layer_name], [output_layer_name], tf.float32.as_datatype_enum)\n  graph_def = TransformGraph(graph_def, [input_layer_name], [output_layer_name], [""sort_by_execution_order""])\n\n  with tf.gfile.GFile(pbfilename, \'wb\') as f:\n      f.write(graph_def.SerializeToString())\n\n  # write model to logs dir so we can visualize it as:\n  # tensorboard --logdir=""logs""\n\n  if (verbose):\n      writer = tf.summary.FileWriter(\'logs\', graph_def)\n      writer.close()\n\n  # tidy up tmp files\n\n  for f in glob.glob(""model-tmp.tfl*""):\n      os.remove(f)\n\n  os.remove(\'checkpoint\')\n\n################################################################################\n# convert a  binary .pb protocol buffer format model to tflite format\n\n# e.g. for FireNet\n#    pbfilename = ""firenet.pb""\n#    input_layer_name = \'InputData/X\'                  # input layer of network\n#    output_layer_name= \'FullyConnected_2/Softmax\'     # output layer of network\n\ndef convert_to_tflite(pbfilename, input_layer_name,  output_layer_name,\n                        input_tensor_dim_x=224, input_tensor_dim_y=224, input_tensor_channels=3):\n\n  input_tensor={input_layer_name:[1,input_tensor_dim_x,input_tensor_dim_y,input_tensor_channels]}\n\n  print(""[INFO] tflite model to "" +  pbfilename.replace("".pb"","".tflite"") + "" ..."")\n\n  converter = tf.lite.TFLiteConverter.from_frozen_graph(pbfilename, [input_layer_name], [output_layer_name], input_tensor)\n  tflite_model = converter.convert()\n  open(pbfilename.replace("".pb"","".tflite""), ""wb"").write(tflite_model)\n\n################################################################################\n'"
converter/firenet-conversion.py,0,"b'################################################################################\n\n# Example : perform conversion of FireNet tflearn model to TensorFlow protocol\n# buffer (.pb) binary format and tflife format files (for import into other tools, example OpenCV)\n\n# Copyright (c) 2019 Toby Breckon, Durham University, UK\n\n# License : https://github.com/tobybreckon/fire-detection-cnn/blob/master/LICENSE\n\n# Acknowledgements: some portions - tensorflow tutorial examples and URL below\n\n################################################################################\n\nimport glob,os\nimport sys\nsys.path.append(\'..\')\n\n################################################################################\n\nfrom firenet import construct_firenet\nfrom converter import convert_to_pb\nfrom converter import convert_to_tflite\n\n################################################################################\n\nif __name__ == \'__main__\':\n\n    # construct and re-export model (so that is excludes the training layers)\n\n    model = construct_firenet (224, 224, False)\n    print(""[INFO] Constructed FireNet ..."")\n\n    path = ""../models/FireNet/firenet""; # path to tflearn checkpoint including filestem\n    input_layer_name = \'InputData/X\'                  # input layer of network\n    output_layer_name= \'FullyConnected_2/Softmax\'     # output layer of network\n    filename = ""firenet.pb""                           # output pb format filename\n\n    convert_to_pb(model, path, input_layer_name,  output_layer_name, filename)\n    convert_to_tflite(filename, input_layer_name,  output_layer_name)\n\n################################################################################\n'"
converter/firenet-validation.py,1,"b'################################################################################\n\n# Example : perform validation of FireNet models in TFLearn, PB and TFLite formats\n\n# Copyright (c) 2019 - Toby Breckon, Durham University, UK\n\n# License : https://github.com/tobybreckon/fire-detection-cnn/blob/master/LICENSE\n\n################################################################################\n\nimport cv2\nimport os\nimport sys\nimport math\n\n################################################################################\n\nimport tflearn\nfrom tflearn.layers.core import *\nfrom tflearn.layers.conv import *\nfrom tflearn.layers.normalization import *\nfrom tflearn.layers.estimator import regression\n\n################################################################################\n\nVALIDATE_TO_PRECISION_N = 5\n\n################################################################################\n\nsys.path.append(\'..\')\nfrom firenet import construct_firenet\n\n################################################################################\n\n# tflearn - load model\n\nprint(""Load tflearn model from: ../models/FireNet ..."", end = \'\')\nmodel_tflearn = construct_firenet (224, 224, training=False)\nmodel_tflearn.load(os.path.join(""../models/FireNet"", ""firenet""),weights_only=True)\nprint(""OK"")\n\n################################################################################\n\n# tf protocol buffer - load model (into opencv)\n\nprint(""Load protocolbuf (pb) model from: firenet.pb ..."", end = \'\')\ntensorflow_pb_model = cv2.dnn.readNetFromTensorflow(\'firenet.pb\')\nprint(""OK"")\n\n################################################################################\n\n# tflite - load model\n\nprint(""Load tflite model from: firenet.tflite ..."", end = \'\')\ntflife_model = tf.lite.Interpreter(model_path=""firenet.tflite"")\ntflife_model.allocate_tensors()\nprint(""OK"")\n\n# Get input and output tensors.\ntflife_input_details = tflife_model.get_input_details()\ntflife_output_details = tflife_model.get_output_details()\n\n################################################################################\n\n# load video file\n\nvideo = cv2.VideoCapture(""../models/test.mp4"")\nprint(""Load test video from ../models/test.mp4 ..."")\n\n# get video properties\n\nwidth = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nframe_counter = 0\nfail_counter = 0\n\nwhile (True):\n\n    # get video frame from file, handle end of file\n\n    ret, frame = video.read()\n    if not ret:\n        print(""... end of video file reached"")\n        break\n\n    print(""frame: "" + str(frame_counter),  end = \'\')\n    frame_counter = frame_counter + 1\n\n    # re-size image to network input size and perform prediction\n\n    # input to networks is: 224x224x3 colour image with channel ordering as {B,G,R}\n    # as is the opencv norm, not {R,G,B} and pixel value range 0->255 for each channel\n\n    small_frame = cv2.resize(frame, (224, 224), cv2.INTER_AREA)\n\n    ############################################################################\n\n    np.set_printoptions(precision=6)\n\n    # perform predictiion with tflearn model\n\n    output_tflearn = model_tflearn.predict([small_frame])\n    print(""\\t: TFLearn (original): "", end = \'\')\n    print(output_tflearn, end = \'\')\n\n    # perform prediction with protocolbuf model via opencv\n\n    tensorflow_pb_model.setInput(cv2.dnn.blobFromImage(small_frame, size=(224, 224), swapRB=False, crop=False))\n    output_tensorflow_pb = tensorflow_pb_model.forward()\n\n    print(""\\t: Tensorflow .pb (via opencv): "", end = \'\')\n    print(output_tensorflow_pb, end = \'\')\n\n    # perform prediction with tflite model via TensorFlow\n\n    tflife_input_data = np.reshape(np.float32(small_frame), (1, 224, 224, 3))\n    tflife_model.set_tensor(tflife_input_details[0][\'index\'], tflife_input_data)\n\n    tflife_model.invoke()\n\n    output_tflite = tflife_model.get_tensor(tflife_output_details[0][\'index\'])\n    print(""\\t: TFLite (via tensorflow): "", end = \'\')\n    print(output_tflite, end = \'\')\n\n    try:\n        np.testing.assert_almost_equal(output_tflearn, output_tensorflow_pb, VALIDATE_TO_PRECISION_N)\n        np.testing.assert_almost_equal(output_tflearn, output_tflite, 3)\n        print("": all equal test - PASS"")\n    except AssertionError:\n        print("" all equal test - FAIL"")\n        fail_counter = fail_counter +1\n\n################################################################################\nprint(""*** FINAL cross-model validation FAILS (for precision of "" + str(VALIDATE_TO_PRECISION_N) + "") = "" + str(fail_counter))\n################################################################################\n'"
converter/inceptionVxOnFire-conversion.py,1,"b'################################################################################\n\n# Example : perform conversion of inceptionVxOnFire tflearn models to TensorFlow protocol\n# buffer (.pb) binary format and tflife format files (for import into other tools, example OpenCV)\n\n# Copyright (c) 2019 Toby Breckon, Durham University, UK\n\n# License : https://github.com/tobybreckon/fire-detection-cnn/blob/master/LICENSE\n\n# Acknowledgements: some portions - tensorflow tutorial examples and URL below\n\n################################################################################\n\nimport glob,os\nimport tensorflow as tf\nimport sys\nimport argparse\nsys.path.append(\'..\')\n\n################################################################################\n\nfrom inceptionVxOnFire import construct_inceptionv1onfire, construct_inceptionv3onfire, construct_inceptionv4onfire\nfrom converter import convert_to_pb\nfrom converter import convert_to_tflite\n\n################################################################################\n\nparser = argparse.ArgumentParser(description=\'Perform InceptionV1/V3/V4 model conversion\')\nparser.add_argument(""-m"", ""--model_to_use"", type=int, help=""specify model to use"", default=1, choices={1, 3, 4})\nargs = parser.parse_args()\n\n################################################################################\n\n# perform conversion of the specified binary detection model\n\nif (args.model_to_use == 1):\n\n    # use InceptionV1-OnFire CNN model - [Dunning/Breckon, 2018]\n\n    model = construct_inceptionv1onfire (224, 224, False)\n    print(""[INFO] Constructed InceptionV1-OnFire (binary, full-frame)..."")\n\n    path = ""../models/InceptionV1-OnFire/inceptiononv1onfire""; # path to tflearn checkpoint including filestem\n    input_layer_name = \'InputData/X\'            # input layer of network\n    output_layer_name= \'FullyConnected/Softmax\' # output layer of network\n    filename = ""inceptionv1onfire.pb""           # output pb format filename\n\nelif (args.model_to_use == 3):\n\n    # use InceptionV3-OnFire CNN model -  [Samarth/Bhowmik/Breckon, 2019]\n    # N.B. for conversion to .pb + .tflite format we disable batch norm, which is a hack but seems to work\n    # (if we don\'t then this issue occurs - https://github.com/tensorflow/tensorflow/issues/3628)\n\n    model = construct_inceptionv3onfire (224, 224, training=False, enable_batch_norm=False)\n    print(""[INFO] Constructed InceptionV3-OnFire (binary, full-frame)..."")\n\n    path = ""../models/InceptionV3-OnFire/inceptionv3onfire""; # path to tflearn checkpoint including filestem\n    input_layer_name = \'InputData/X\'            # input layer of network\n    output_layer_name= \'FullyConnected/Softmax\' # output layer of network\n    filename = ""inceptionv3onfire.pb""           # output pb format filename\n\nelif (args.model_to_use == 4):\n\n    # use InceptionV4-OnFire CNN model - [Samarth/Bhowmik/Breckon, 2019]\n    # N.B. for conversion to .pb + .tflite format we disable batch norm, which is a hack but seems to work\n    # (if we don\'t then this issue occurs - https://github.com/tensorflow/tensorflow/issues/3628)\n\n    model = construct_inceptionv4onfire (224, 224, training=False, enable_batch_norm=False)\n    print(""[INFO] Constructed InceptionV4-OnFire (binary, full-frame)..."")\n\n    path = ""../models/InceptionV4-OnFire/inceptionv4onfire""; # path to tflearn checkpoint including filestem\n    input_layer_name = \'InputData/X\'            # input layer of network\n    output_layer_name= \'FullyConnected/Softmax\' # output layer of network\n    filename = ""inceptionv4onfire.pb""           # output pb format filename\n\nconvert_to_pb(model, path, input_layer_name,  output_layer_name, filename)\nconvert_to_tflite(filename, input_layer_name,  output_layer_name)\n\n################################################################################\n\n# reset TensorFlow before next conversion\n\ntf.reset_default_graph()\n\n################################################################################\n\n# perform conversion of the specified superpixel based detection model\n\nif (args.model_to_use == 1):\n\n    # use InceptionV1-OnFire CNN model - [Dunning/Breckon, 2018]\n\n    model_sp = construct_inceptionv1onfire (224, 224, False)\n    print(""[INFO] Constructed InceptionV1-OnFire (superpixel)..."")\n\n    input_layer_name = \'InputData/X\'            # input layer of network\n    output_layer_name= \'FullyConnected/Softmax\' # output layer of network\n    path_sp = ""../models/SP-InceptionV1-OnFire/sp-inceptiononv1onfire""; # path to tflearn checkpoint including filestem\n    filename_sp = ""sp-inceptionv1onfire.pb""         # output filename\n\nelif (args.model_to_use == 3):\n\n    # use InceptionV3-OnFire CNN model - [Samarth/Bhowmik/Breckon, 2019]\n    # N.B. for conversion to .pb + .tflite format we disable batch norm, which is a hack but seems to work\n    # (if we don\'t then this issue occurs - https://github.com/tensorflow/tensorflow/issues/3628)\n\n    model_sp = construct_inceptionv3onfire (224, 224, training=False, enable_batch_norm=False)\n    print(""[INFO] Constructed InceptionV3-OnFire (superpixel)..."")\n\n    input_layer_name = \'InputData/X\'            # input layer of network\n    output_layer_name= \'FullyConnected/Softmax\' # output layer of network\n    path_sp = ""../models/SP-InceptionV3-OnFire/sp-inceptionv3onfire""; # path to tflearn checkpoint including filestem\n    filename_sp = ""sp-inceptionv3onfire.pb""         # output filename\n\nelif (args.model_to_use == 4):\n\n    # use InceptionV4-OnFire CNN model - [Samarth/Bhowmik/Breckon, 2019]\n    # N.B. for conversion to .pb + .tflite format we disable batch norm, which is a hack but seems to work\n    # (if we don\'t then this issue occurs - https://github.com/tensorflow/tensorflow/issues/3628)\n\n    model_sp = construct_inceptionv4onfire (224, 224, training=False, enable_batch_norm=False)\n    print(""[INFO] Constructed InceptionV4-OnFire (superpixel)..."")\n\n    input_layer_name = \'InputData/X\'            # input layer of network\n    output_layer_name= \'FullyConnected/Softmax\' # output layer of network\n    path_sp = ""../models/SP-InceptionV4-OnFire/sp-inceptionv4onfire""; # path to tflearn checkpoint including filestem\n    filename_sp = ""sp-inceptionv4onfire.pb""         # output filename\n\nconvert_to_pb(model_sp, path_sp, input_layer_name,  output_layer_name, filename_sp)\nconvert_to_tflite(filename_sp, input_layer_name,  output_layer_name)\n\n################################################################################\n'"
converter/inceptionVxOnFire-validation.py,1,"b'################################################################################\n\n# Example : perform validation of InceptionVx-OnFire models in TFLearn, PB and TFLite formats\n\n# Copyright (c) 2019 - Toby Breckon, Durham University, UK\n\n# License : https://github.com/tobybreckon/fire-detection-cnn/blob/master/LICENSE\n\n################################################################################\n\nimport cv2\nimport os\nimport sys\nimport math\nimport argparse\n\n################################################################################\n\nimport tflearn\nfrom tflearn.layers.core import *\nfrom tflearn.layers.conv import *\nfrom tflearn.layers.normalization import *\nfrom tflearn.layers.estimator import regression\n\n################################################################################\n\nVALIDATE_TO_PRECISION_N = 3\n\n################################################################################\n\nsys.path.append(\'..\')\nfrom inceptionVxOnFire import construct_inceptionv1onfire, construct_inceptionv3onfire, construct_inceptionv4onfire\n\n################################################################################\n\nparser = argparse.ArgumentParser(description=\'Perform InceptionV1/V3/V4 model validation\')\nparser.add_argument(""-m"", ""--model_to_use"", type=int, help=""specify model to use"", default=1, choices={1, 3, 4})\nparser.add_argument(""-sp"", ""--superpixel_model"", action=\'store_true\', help=""use superpixel version  of model"")\nparser.add_argument(""-i"", ""--input_video"", type=str, help=""specify test video to use"", default=""../models/test.mp4"")\nparser.add_argument(""-d"", ""--display"", action=\'store_true\', help=""use superpixel version  of model"")\nargs = parser.parse_args()\n\n################################################################################\n\n# perform conversion of the specified binary or superpixel detection model\n\nif (args.superpixel_model):\n    pre_string = ""SP-""\nelse:\n    pre_string = """"\n\nif (args.model_to_use == 1):\n\n    # use InceptionV1-OnFire CNN model - [Dunning/Breckon, 2018]\n\n    model_tflearn = construct_inceptionv1onfire (224, 224, training=False)\n    print(""[INFO] Constructed "" + pre_string + ""InceptionV1-OnFire ..."")\n    path = ""../models/"" + pre_string + ""InceptionV1-OnFire/"" + pre_string.lower() + ""inceptiononv1onfire""; # path to tflearn checkpoint including filestem\n\nelif (args.model_to_use == 3):\n\n    # use InceptionV3-OnFire CNN model -  [Samarth/Bhowmik/Breckon, 2019]\n    # N.B. here we enable batch norm as this is for the TFLearn model only, which needs it activated to work\n\n    model_tflearn = construct_inceptionv3onfire (224, 224, training=False, enable_batch_norm=True)\n    print(""[INFO] Constructed "" + pre_string + ""InceptionV3-OnFire ..."")\n    path = ""../models/"" + pre_string + ""InceptionV3-OnFire/"" + pre_string.lower() + ""inceptionv3onfire""; # path to tflearn checkpoint including filestem\n\nelif (args.model_to_use == 4):\n\n    # use InceptionV4-OnFire CNN model - [Samarth/Bhowmik/Breckon, 2019]\n    # N.B. here we enable batch norm as this is for the TFLearn model only, which needs it activated to work\n\n    model_tflearn = construct_inceptionv4onfire (224, 224, training=False, enable_batch_norm=True)\n    print(""[INFO] Constructed "" + pre_string + ""InceptionV4-OnFire ..."")\n    path = ""../models/"" + pre_string + ""InceptionV4-OnFire/"" + pre_string.lower() + ""inceptionv4onfire""; # path to tflearn checkpoint including filestem\n\n################################################################################\n\n# tflearn - load model\n\nprint(""Load tflearn model from: "" + path + "" ..."", end = \'\')\n\n# only use wieghts_only for V1 model, due to use of batch norm\nmodel_tflearn.load(path,weights_only=(args.model_to_use == 1))\n\nprint(""OK"")\n\n################################################################################\n\n# tf protocol buffer - load model (into opencv)\n\ntry:\n    print(""Load protocolbuf (pb) model from: "" + pre_string.lower() + ""inceptiononv"" + str(args.model_to_use) + ""onfire.pb ..."", end = \'\')\n    tensorflow_pb_model = cv2.dnn.readNetFromTensorflow(pre_string.lower() + ""inceptionv"" + str(args.model_to_use) + ""onfire.pb"")\n    print(""OK"")\nexcept:\n    print(""FAIL"")\n    print(""ERROR: file "" +  (pre_string.lower() + ""inceptionv"" + str(args.model_to_use) + ""onfire.pb"") + "" missing, ensure you run the correct convertor to generate it first!"")\n    exit(1)\n\n################################################################################\n\n# tflite - load model\n\nprint(""Load tflite model from: "" + pre_string.lower() + ""inceptiononv"" + str(args.model_to_use) + ""onfire.tflite ..."", end = \'\')\ntflife_model = tf.lite.Interpreter(model_path=pre_string.lower() + ""inceptionv"" + str(args.model_to_use) + ""onfire.tflite"")\ntflife_model.allocate_tensors()\nprint(""OK"")\n\n# Get input and output tensors.\ntflife_input_details = tflife_model.get_input_details()\ntflife_output_details = tflife_model.get_output_details()\n\n################################################################################\n\n# load video file\n\nvideo = cv2.VideoCapture(args.input_video)\nprint(""Load test video from "" + args.input_video + "" ..."")\n\n# get video properties\n\nwidth = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nframe_counter = 0\nfail_counter = 0\n\nwhile (True):\n\n    # get video frame from file, handle end of file\n\n    ret, frame = video.read()\n    if not ret:\n        print(""... end of video file reached"")\n        break\n\n    if (args.display):\n        cv2.imshow(""Validation Image"", frame)\n        cv2.waitKey(40)\n\n    print(""frame: "" + str(frame_counter),  end = \'\')\n    frame_counter = frame_counter + 1\n\n    # re-size image to network input size and perform prediction\n\n    small_frame = cv2.resize(frame, (224, 224), cv2.INTER_AREA)\n\n    ############################################################################\n\n    np.set_printoptions(precision=6)\n\n    # perform predictiion with tflearn model\n\n    output_tflearn = model_tflearn.predict([small_frame])\n    print(""\\t: TFLearn (original): "", end = \'\')\n    print(output_tflearn, end = \'\')\n\n    # perform prediction with protocolbuf model via opencv\n\n    tensorflow_pb_model.setInput(cv2.dnn.blobFromImage(small_frame, size=(224, 224), swapRB=False, crop=False))\n    output_tensorflow_pb = tensorflow_pb_model.forward()\n\n    print(""\\t: Tensorflow .pb (via opencv): "", end = \'\')\n    print(output_tensorflow_pb, end = \'\')\n\n    # perform prediction with tflite model via TensorFlow\n\n    tflife_input_data = np.reshape(np.float32(small_frame), (1, 224, 224, 3))\n    tflife_model.set_tensor(tflife_input_details[0][\'index\'], tflife_input_data)\n\n    tflife_model.invoke()\n\n    output_tflite = tflife_model.get_tensor(tflife_output_details[0][\'index\'])\n    print(""\\t: TFLite (via tensorflow): "", end = \'\')\n    print(output_tflite, end = \'\')\n\n    try:\n        np.testing.assert_almost_equal(output_tflearn, output_tensorflow_pb, VALIDATE_TO_PRECISION_N)\n        np.testing.assert_almost_equal(output_tflearn, output_tflite, VALIDATE_TO_PRECISION_N)\n        print("": all equal test - PASS"")\n    except AssertionError:\n        print("" all equal test - FAIL"")\n        fail_counter = fail_counter +1\n\n################################################################################\nprint(""*** FINAL cross-model validation FAILS (for precision of "" + str(VALIDATE_TO_PRECISION_N) + "") = "" + str(fail_counter))\n################################################################################\n'"
