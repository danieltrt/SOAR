file_path,api_count,code
run_tracker_evaluation.py,1,"b'from __future__ import division\nimport sys\nimport os\nimport numpy as np\nfrom PIL import Image\nimport src.siamese as siam\nfrom src.tracker import tracker\nfrom src.parse_arguments import parse_arguments\nfrom src.region_to_bbox import region_to_bbox\n\n\ndef main():\n    # avoid printing TF debugging information\n    os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n    # TODO: allow parameters from command line or leave everything in json files?\n    hp, evaluation, run, env, design = parse_arguments()\n    # Set size for use with tf.image.resize_images with align_corners=True.\n    # For example,\n    #   [1 4 7] =>   [1 2 3 4 5 6 7]    (length 3*(3-1)+1)\n    # instead of\n    # [1 4 7] => [1 1 2 3 4 5 6 7 7]  (length 3*3)\n    final_score_sz = hp.response_up * (design.score_sz - 1) + 1\n    # build TF graph once for all\n    filename, image, templates_z, scores = siam.build_tracking_graph(final_score_sz, design, env)\n\n    # iterate through all videos of evaluation.dataset\n    if evaluation.video == \'all\':\n        dataset_folder = os.path.join(env.root_dataset, evaluation.dataset)\n        videos_list = [v for v in os.listdir(dataset_folder)]\n        videos_list.sort()\n        nv = np.size(videos_list)\n        speed = np.zeros(nv * evaluation.n_subseq)\n        precisions = np.zeros(nv * evaluation.n_subseq)\n        precisions_auc = np.zeros(nv * evaluation.n_subseq)\n        ious = np.zeros(nv * evaluation.n_subseq)\n        lengths = np.zeros(nv * evaluation.n_subseq)\n        for i in range(nv):\n            gt, frame_name_list, frame_sz, n_frames = _init_video(env, evaluation, videos_list[i])\n            starts = np.rint(np.linspace(0, n_frames - 1, evaluation.n_subseq + 1))\n            starts = starts[0:evaluation.n_subseq]\n            for j in range(evaluation.n_subseq):\n                start_frame = int(starts[j])\n                gt_ = gt[start_frame:, :]\n                frame_name_list_ = frame_name_list[start_frame:]\n                pos_x, pos_y, target_w, target_h = region_to_bbox(gt_[0])\n                idx = i * evaluation.n_subseq + j\n                bboxes, speed[idx] = tracker(hp, run, design, frame_name_list_, pos_x, pos_y,\n                                                                     target_w, target_h, final_score_sz, filename,\n                                                                     image, templates_z, scores, start_frame)\n                lengths[idx], precisions[idx], precisions_auc[idx], ious[idx] = _compile_results(gt_, bboxes, evaluation.dist_threshold)\n                print str(i) + \' -- \' + videos_list[i] + \\\n                \' -- Precision: \' + ""%.2f"" % precisions[idx] + \\\n                \' -- Precisions AUC: \' + ""%.2f"" % precisions_auc[idx] + \\\n                \' -- IOU: \' + ""%.2f"" % ious[idx] + \\\n                \' -- Speed: \' + ""%.2f"" % speed[idx] + \' --\'\n                print\n\n        tot_frames = np.sum(lengths)\n        mean_precision = np.sum(precisions * lengths) / tot_frames\n        mean_precision_auc = np.sum(precisions_auc * lengths) / tot_frames\n        mean_iou = np.sum(ious * lengths) / tot_frames\n        mean_speed = np.sum(speed * lengths) / tot_frames\n        print \'-- Overall stats (averaged per frame) on \' + str(nv) + \' videos (\' + str(tot_frames) + \' frames) --\'\n        print \' -- Precision \' + ""(%d px)"" % evaluation.dist_threshold + \': \' + ""%.2f"" % mean_precision +\\\n              \' -- Precisions AUC: \' + ""%.2f"" % mean_precision_auc +\\\n              \' -- IOU: \' + ""%.2f"" % mean_iou +\\\n              \' -- Speed: \' + ""%.2f"" % mean_speed + \' --\'\n        print\n\n    else:\n        gt, frame_name_list, _, _ = _init_video(env, evaluation, evaluation.video)\n        pos_x, pos_y, target_w, target_h = region_to_bbox(gt[evaluation.start_frame])\n        bboxes, speed = tracker(hp, run, design, frame_name_list, pos_x, pos_y, target_w, target_h, final_score_sz,\n                                filename, image, templates_z, scores, evaluation.start_frame)\n        _, precision, precision_auc, iou = _compile_results(gt, bboxes, evaluation.dist_threshold)\n        print evaluation.video + \\\n              \' -- Precision \' + ""(%d px)"" % evaluation.dist_threshold + \': \' + ""%.2f"" % precision +\\\n              \' -- Precision AUC: \' + ""%.2f"" % precision_auc + \\\n              \' -- IOU: \' + ""%.2f"" % iou + \\\n              \' -- Speed: \' + ""%.2f"" % speed + \' --\'\n        print\n\n\ndef _compile_results(gt, bboxes, dist_threshold):\n    l = np.size(bboxes, 0)\n    gt4 = np.zeros((l, 4))\n    new_distances = np.zeros(l)\n    new_ious = np.zeros(l)\n    n_thresholds = 50\n    precisions_ths = np.zeros(n_thresholds)\n\n    for i in range(l):\n        gt4[i, :] = region_to_bbox(gt[i, :], center=False)\n        new_distances[i] = _compute_distance(bboxes[i, :], gt4[i, :])\n        new_ious[i] = _compute_iou(bboxes[i, :], gt4[i, :])\n\n    # what\'s the percentage of frame in which center displacement is inferior to given threshold? (OTB metric)\n    precision = sum(new_distances < dist_threshold)/np.size(new_distances) * 100\n\n    # find above result for many thresholds, then report the AUC\n    thresholds = np.linspace(0, 25, n_thresholds+1)\n    thresholds = thresholds[-n_thresholds:]\n    # reverse it so that higher values of precision goes at the beginning\n    thresholds = thresholds[::-1]\n    for i in range(n_thresholds):\n        precisions_ths[i] = sum(new_distances < thresholds[i])/np.size(new_distances)\n\n    # integrate over the thresholds\n    precision_auc = np.trapz(precisions_ths)    \n\n    # per frame averaged intersection over union (OTB metric)\n    iou = np.mean(new_ious) * 100\n\n    return l, precision, precision_auc, iou\n\n\ndef _init_video(env, evaluation, video):\n    video_folder = os.path.join(env.root_dataset, evaluation.dataset, video)\n    frame_name_list = [f for f in os.listdir(video_folder) if f.endswith("".jpg"")]\n    frame_name_list = [os.path.join(env.root_dataset, evaluation.dataset, video, \'\') + s for s in frame_name_list]\n    frame_name_list.sort()\n    with Image.open(frame_name_list[0]) as img:\n        frame_sz = np.asarray(img.size)\n        frame_sz[1], frame_sz[0] = frame_sz[0], frame_sz[1]\n\n    # read the initialization from ground truth\n    gt_file = os.path.join(video_folder, \'groundtruth.txt\')\n    gt = np.genfromtxt(gt_file, delimiter=\',\')\n    n_frames = len(frame_name_list)\n    assert n_frames == len(gt), \'Number of frames and number of GT lines should be equal.\'\n\n    return gt, frame_name_list, frame_sz, n_frames\n\n\ndef _compute_distance(boxA, boxB):\n    a = np.array((boxA[0]+boxA[2]/2, boxA[1]+boxA[3]/2))\n    b = np.array((boxB[0]+boxB[2]/2, boxB[1]+boxB[3]/2))\n    dist = np.linalg.norm(a - b)\n\n    assert dist >= 0\n    assert dist != float(\'Inf\')\n\n    return dist\n\n\ndef _compute_iou(boxA, boxB):\n    # determine the (x, y)-coordinates of the intersection rectangle\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])\n    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])\n\n    if xA < xB and yA < yB:\n        # compute the area of intersection rectangle\n        interArea = (xB - xA) * (yB - yA)\n        # compute the area of both the prediction and ground-truth\n        # rectangles\n        boxAArea = boxA[2] * boxA[3]\n        boxBArea = boxB[2] * boxB[3]\n        # compute the intersection over union by taking the intersection\n        # area and dividing it by the sum of prediction + ground-truth\n        # areas - the intersection area\n        iou = interArea / float(boxAArea + boxBArea - interArea)\n    else:\n        iou = 0\n\n    assert iou >= 0\n    assert iou <= 1.01\n\n    return iou\n\n\nif __name__ == \'__main__\':\n    sys.exit(main())\n'"
src/__init__.py,0,b''
src/convolutional.py,14,"b'import tensorflow as tf\n\n\ndef set_convolutional(X, W, b, stride, bn_beta, bn_gamma, bn_mm, bn_mv, filtergroup=False, batchnorm=True,\n                      activation=True, scope=None, reuse=False):\n    # use the input scope or default to ""conv""\n    with tf.variable_scope(scope or \'conv\', reuse=reuse):\n        # sanity check    \n        W = tf.get_variable(""W"", W.shape, trainable=False, initializer=tf.constant_initializer(W))\n        b = tf.get_variable(""b"", b.shape, trainable=False, initializer=tf.constant_initializer(b))\n\n        if filtergroup:\n            X0, X1 = tf.split(X, 2, 3)\n            W0, W1 = tf.split(W, 2, 3)\n            h0 = tf.nn.conv2d(X0, W0, strides=[1, stride, stride, 1], padding=\'VALID\')\n            h1 = tf.nn.conv2d(X1, W1, strides=[1, stride, stride, 1], padding=\'VALID\')\n            h = tf.concat([h0, h1], 3) + b\n        else:\n            h = tf.nn.conv2d(X, W, strides=[1, stride, stride, 1], padding=\'VALID\') + b\n\n        if batchnorm:\n            h = tf.layers.batch_normalization(h, beta_initializer=tf.constant_initializer(bn_beta),\n                                              gamma_initializer=tf.constant_initializer(bn_gamma),\n                                              moving_mean_initializer=tf.constant_initializer(bn_mm),\n                                              moving_variance_initializer=tf.constant_initializer(bn_mv),\n                                              training=False, trainable=False)\n\n        if activation:\n            h = tf.nn.relu(h)\n\n        return h\n'"
src/crops.py,46,"b""from __future__ import division\nimport numpy as np\nimport tensorflow as tf\nfrom PIL import Image\nimport functools\n\n\ndef resize_images(images, size, resample):\n    '''Alternative to tf.image.resize_images that uses PIL.'''\n    fn = functools.partial(_resize_images, size=size, resample=resample)\n    return tf.py_func(fn, [images], images.dtype)\n\n\ndef _resize_images(x, size, resample):\n    # TODO: Use tf.map_fn?\n    if len(x.shape) == 3:\n        return _resize_image(x, size, resample)\n    assert len(x.shape) == 4\n    y = []\n    for i in range(x.shape[0]):\n        y.append(_resize_image(x[i]))\n    y = np.stack(y, axis=0)\n    return y\n\n\ndef _resize_image(x, size, resample):\n    assert len(x.shape) == 3\n    y = []\n    for j in range(x.shape[2]):\n        f = x[:, :, j]\n        f = Image.fromarray(f)\n        f = f.resize((size[1], size[0]), resample=resample)\n        f = np.array(f)\n        y.append(f)\n    return np.stack(y, axis=2)\n\n\ndef pad_frame(im, frame_sz, pos_x, pos_y, patch_sz, avg_chan):\n    c = patch_sz / 2\n    xleft_pad = tf.maximum(0, -tf.cast(tf.round(pos_x - c), tf.int32))\n    ytop_pad = tf.maximum(0, -tf.cast(tf.round(pos_y - c), tf.int32))\n    xright_pad = tf.maximum(0, tf.cast(tf.round(pos_x + c), tf.int32) - frame_sz[1])\n    ybottom_pad = tf.maximum(0, tf.cast(tf.round(pos_y + c), tf.int32) - frame_sz[0])\n    npad = tf.reduce_max([xleft_pad, ytop_pad, xright_pad, ybottom_pad])\n    paddings = [[npad, npad], [npad, npad], [0, 0]]\n    im_padded = im\n    if avg_chan is not None:\n        im_padded = im_padded - avg_chan\n    im_padded = tf.pad(im_padded, paddings, mode='CONSTANT')\n    if avg_chan is not None:\n        im_padded = im_padded + avg_chan\n    return im_padded, npad\n\n\ndef extract_crops_z(im, npad, pos_x, pos_y, sz_src, sz_dst):\n    c = sz_src / 2\n    # get top-right corner of bbox and consider padding\n    tr_x = npad + tf.cast(tf.round(pos_x - c), tf.int32)\n    # Compute size from rounded co-ords to ensure rectangle lies inside padding.\n    tr_y = npad + tf.cast(tf.round(pos_y - c), tf.int32)\n    width = tf.round(pos_x + c) - tf.round(pos_x - c)\n    height = tf.round(pos_y + c) - tf.round(pos_y - c)\n    crop = tf.image.crop_to_bounding_box(im,\n                                         tf.cast(tr_y, tf.int32),\n                                         tf.cast(tr_x, tf.int32),\n                                         tf.cast(height, tf.int32),\n                                         tf.cast(width, tf.int32))\n    crop = tf.image.resize_images(crop, [sz_dst, sz_dst], method=tf.image.ResizeMethod.BILINEAR)\n    # crops = tf.stack([crop, crop, crop])\n    crops = tf.expand_dims(crop, axis=0)\n    return crops\n\n\ndef extract_crops_x(im, npad, pos_x, pos_y, sz_src0, sz_src1, sz_src2, sz_dst):\n    # take center of the biggest scaled source patch\n    c = sz_src2 / 2\n    # get top-right corner of bbox and consider padding\n    tr_x = npad + tf.cast(tf.round(pos_x - c), tf.int32)\n    tr_y = npad + tf.cast(tf.round(pos_y - c), tf.int32)\n    # Compute size from rounded co-ords to ensure rectangle lies inside padding.\n    width = tf.round(pos_x + c) - tf.round(pos_x - c)\n    height = tf.round(pos_y + c) - tf.round(pos_y - c)\n    search_area = tf.image.crop_to_bounding_box(im,\n                                                tf.cast(tr_y, tf.int32),\n                                                tf.cast(tr_x, tf.int32),\n                                                tf.cast(height, tf.int32),\n                                                tf.cast(width, tf.int32))\n    # TODO: Use computed width and height here?\n    offset_s0 = (sz_src2 - sz_src0) / 2\n    offset_s1 = (sz_src2 - sz_src1) / 2\n\n    crop_s0 = tf.image.crop_to_bounding_box(search_area,\n                                            tf.cast(offset_s0, tf.int32),\n                                            tf.cast(offset_s0, tf.int32),\n                                            tf.cast(tf.round(sz_src0), tf.int32),\n                                            tf.cast(tf.round(sz_src0), tf.int32))\n    crop_s0 = tf.image.resize_images(crop_s0, [sz_dst, sz_dst], method=tf.image.ResizeMethod.BILINEAR)\n    crop_s1 = tf.image.crop_to_bounding_box(search_area,\n                                            tf.cast(offset_s1, tf.int32),\n                                            tf.cast(offset_s1, tf.int32),\n                                            tf.cast(tf.round(sz_src1), tf.int32),\n                                            tf.cast(tf.round(sz_src1), tf.int32))\n    crop_s1 = tf.image.resize_images(crop_s1, [sz_dst, sz_dst], method=tf.image.ResizeMethod.BILINEAR)\n    crop_s2 = tf.image.resize_images(search_area, [sz_dst, sz_dst], method=tf.image.ResizeMethod.BILINEAR)\n    crops = tf.stack([crop_s0, crop_s1, crop_s2])\n    return crops\n\n# Can't manage to use tf.crop_and_resize, which would be ideal!\n# im:  A 4-D tensor of shape [batch, image_height, image_width, depth]\n# boxes: the i-th row of the tensor specifies the coordinates of a box in the box_ind[i] image and is\n# specified in normalized coordinates [y1, x1, y2, x2]\n# box_ind: specify image to which each box refers to\n# crop = tf.image.crop_and_resize(im, boxes, box_ind, sz_dst)\n"""
src/parse_arguments.py,0,"b""import json\nfrom collections import namedtuple\n\n\ndef parse_arguments(in_hp={}, in_evaluation={}, in_run={}):\n\n    with open('parameters/hyperparams.json') as json_file:\n        hp = json.load(json_file)\n    with open('parameters/evaluation.json') as json_file:\n        evaluation = json.load(json_file)\n    with open('parameters/run.json') as json_file:\n        run = json.load(json_file)\n    with open('parameters/environment.json') as json_file:\n        env = json.load(json_file)\n    with open('parameters/design.json') as json_file:\n        design = json.load(json_file)                \n\n    for name,value in in_hp.iteritems():\n        hp[name] = value\n    for name,value in in_evaluation.iteritems():\n        evaluation[name] = value\n    for name,value in in_run.iteritems():\n        run[name] = value\n    \n    hp = namedtuple('hp', hp.keys())(**hp)\n    evaluation = namedtuple('evaluation', evaluation.keys())(**evaluation)\n    run = namedtuple('run', run.keys())(**run)\n    env = namedtuple('env', env.keys())(**env)\n    design = namedtuple('design', design.keys())(**design)\n\n    return hp, evaluation, run, env, design\n"""
src/pprint_params.py,0,"b""from pprint import pprint\n\n\ndef pprint_params(params_list, group_names):\n    print '\\n#### SETTINGS'\n    for idx, p in enumerate(params_list):\n        print '\\n## ' + group_names[idx]\n        pprint(p)\n"""
src/region_to_bbox.py,0,"b""import numpy as np\n\ndef region_to_bbox(region, center=True):\n\n    n = len(region)\n    assert n==4 or n==8, ('GT region format is invalid, should have 4 or 8 entries.')\n\n    if n==4:\n        return _rect(region, center)\n    else:\n        return _poly(region, center)\n\n# we assume the grountruth bounding boxes are saved with 0-indexing\ndef _rect(region, center):\n    \n    if center:\n        x = region[0]\n        y = region[1]\n        w = region[2]\n        h = region[3]\n        cx = x+w/2\n        cy = y+h/2\n        return cx, cy, w, h\n    else:\n        #region[0] -= 1\n        #region[1] -= 1\n        return region\n\n\ndef _poly(region, center):\n    cx = np.mean(region[::2])\n    cy = np.mean(region[1::2])\n    x1 = np.min(region[::2])\n    x2 = np.max(region[::2])\n    y1 = np.min(region[1::2])\n    y2 = np.max(region[1::2])\n    A1 = np.linalg.norm(region[0:2] - region[2:4]) * np.linalg.norm(region[2:4] - region[4:6])\n    A2 = (x2 - x1) * (y2 - y1)\n    s = np.sqrt(A1/A2)\n    w = s * (x2 - x1) + 1\n    h = s * (y2 - y1) + 1\n\n    if center:\n        return cx, cy, w, h\n    else:\n        return cx-w/2, cy-h/2, w, h\n\n"""
src/siamese.py,35,"b'import tensorflow as tf\nimport numpy as np\nimport scipy.io\nimport sys\nimport os.path\nfrom src.convolutional import set_convolutional\nfrom src.crops import extract_crops_z, extract_crops_x, pad_frame, resize_images\nsys.path.append(\'../\')\n\npos_x_ph = tf.placeholder(tf.float64)\npos_y_ph = tf.placeholder(tf.float64)\nz_sz_ph = tf.placeholder(tf.float64)\nx_sz0_ph = tf.placeholder(tf.float64)\nx_sz1_ph = tf.placeholder(tf.float64)\nx_sz2_ph = tf.placeholder(tf.float64)\n\n# the follow parameters *have to* reflect the design of the network to be imported\n_conv_stride = np.array([2,1,1,1,1])\n_filtergroup_yn = np.array([0,1,0,1,1], dtype=bool)\n_bnorm_yn = np.array([1,1,1,1,0], dtype=bool)\n_relu_yn = np.array([1,1,1,1,0], dtype=bool)\n_pool_stride = np.array([2,1,0,0,0]) # 0 means no pool\n_pool_sz = 3\n_bnorm_adjust = True\nassert len(_conv_stride) == len(_filtergroup_yn) == len(_bnorm_yn) == len(_relu_yn) == len(_pool_stride), (\'These arrays of flags should have same length\')\nassert all(_conv_stride) >= True, (\'The number of conv layers is assumed to define the depth of the network\')\n_num_layers = len(_conv_stride)\n\n\ndef build_tracking_graph(final_score_sz, design, env):\n    # Make a queue of file names\n    # filename_queue = tf.train.string_input_producer(frame_name_list, shuffle=False, capacity=num_frames)\n    # image_reader = tf.WholeFileReader()\n    # # Read a whole file from the queue\n    # image_name, image_file = image_reader.read(filename_queue)\n\n    filename = tf.placeholder(tf.string, [], name=\'filename\')\n    image_file = tf.read_file(filename)\n    # Decode the image as a JPEG file, this will turn it into a Tensor\n    image = tf.image.decode_jpeg(image_file)\n    image = 255.0 * tf.image.convert_image_dtype(image, tf.float32)\n    frame_sz = tf.shape(image)\n    # used to pad the crops\n    if design.pad_with_image_mean:\n        avg_chan = tf.reduce_mean(image, axis=(0,1), name=\'avg_chan\')\n    else:\n        avg_chan = None\n    # pad with if necessary\n    frame_padded_z, npad_z = pad_frame(image, frame_sz, pos_x_ph, pos_y_ph, z_sz_ph, avg_chan)\n    frame_padded_z = tf.cast(frame_padded_z, tf.float32)\n    # extract tensor of z_crops\n    z_crops = extract_crops_z(frame_padded_z, npad_z, pos_x_ph, pos_y_ph, z_sz_ph, design.exemplar_sz)\n    frame_padded_x, npad_x = pad_frame(image, frame_sz, pos_x_ph, pos_y_ph, x_sz2_ph, avg_chan)\n    frame_padded_x = tf.cast(frame_padded_x, tf.float32)\n    # extract tensor of x_crops (3 scales)\n    x_crops = extract_crops_x(frame_padded_x, npad_x, pos_x_ph, pos_y_ph, x_sz0_ph, x_sz1_ph, x_sz2_ph, design.search_sz)\n    # use crops as input of (MatConvnet imported) pre-trained fully-convolutional Siamese net\n    template_z, templates_x, p_names_list, p_val_list = _create_siamese(os.path.join(env.root_pretrained,design.net), x_crops, z_crops)\n    template_z = tf.squeeze(template_z)\n    templates_z = tf.stack([template_z, template_z, template_z])\n    # compare templates via cross-correlation\n    scores = _match_templates(templates_z, templates_x, p_names_list, p_val_list)\n    # upsample the score maps\n    scores_up = tf.image.resize_images(scores, [final_score_sz, final_score_sz],\n        method=tf.image.ResizeMethod.BICUBIC, align_corners=True)\n    return filename, image, templates_z, scores_up\n\n\n# import pretrained Siamese network from matconvnet\ndef _create_siamese(net_path, net_x, net_z):\n    # read mat file from net_path and start TF Siamese graph from placeholders X and Z\n    params_names_list, params_values_list = _import_from_matconvnet(net_path)\n\n    # loop through the flag arrays and re-construct network, reading parameters of conv and bnorm layers\n    for i in xrange(_num_layers):\n        print \'> Layer \'+str(i+1)\n        # conv\n        conv_W_name = _find_params(\'conv\'+str(i+1)+\'f\', params_names_list)[0]\n        conv_b_name = _find_params(\'conv\'+str(i+1)+\'b\', params_names_list)[0]\n        print \'\\t\\tCONV: setting \'+conv_W_name+\' \'+conv_b_name\n        print \'\\t\\tCONV: stride \'+str(_conv_stride[i])+\', filter-group \'+str(_filtergroup_yn[i])\n        conv_W = params_values_list[params_names_list.index(conv_W_name)]\n        conv_b = params_values_list[params_names_list.index(conv_b_name)]\n        # batchnorm\n        if _bnorm_yn[i]:\n            bn_beta_name = _find_params(\'bn\'+str(i+1)+\'b\', params_names_list)[0]\n            bn_gamma_name = _find_params(\'bn\'+str(i+1)+\'m\', params_names_list)[0]\n            bn_moments_name = _find_params(\'bn\'+str(i+1)+\'x\', params_names_list)[0]\n            print \'\\t\\tBNORM: setting \'+bn_beta_name+\' \'+bn_gamma_name+\' \'+bn_moments_name\n            bn_beta = params_values_list[params_names_list.index(bn_beta_name)]\n            bn_gamma = params_values_list[params_names_list.index(bn_gamma_name)]\n            bn_moments = params_values_list[params_names_list.index(bn_moments_name)]\n            bn_moving_mean = bn_moments[:,0]\n            bn_moving_variance = bn_moments[:,1]**2 # saved as std in matconvnet\n        else:\n            bn_beta = bn_gamma = bn_moving_mean = bn_moving_variance = []\n        \n        # set up conv ""block"" with bnorm and activation \n        net_x = set_convolutional(net_x, conv_W, np.swapaxes(conv_b,0,1), _conv_stride[i], \\\n                            bn_beta, bn_gamma, bn_moving_mean, bn_moving_variance, \\\n                            filtergroup=_filtergroup_yn[i], batchnorm=_bnorm_yn[i], activation=_relu_yn[i], \\\n                            scope=\'conv\'+str(i+1), reuse=False)\n        \n        # notice reuse=True for Siamese parameters sharing\n        net_z = set_convolutional(net_z, conv_W, np.swapaxes(conv_b,0,1), _conv_stride[i], \\\n                            bn_beta, bn_gamma, bn_moving_mean, bn_moving_variance, \\\n                            filtergroup=_filtergroup_yn[i], batchnorm=_bnorm_yn[i], activation=_relu_yn[i], \\\n                            scope=\'conv\'+str(i+1), reuse=True)    \n        \n        # add max pool if required\n        if _pool_stride[i]>0:\n            print \'\\t\\tMAX-POOL: size \'+str(_pool_sz)+ \' and stride \'+str(_pool_stride[i])\n            net_x = tf.nn.max_pool(net_x, [1,_pool_sz,_pool_sz,1], strides=[1,_pool_stride[i],_pool_stride[i],1], padding=\'VALID\', name=\'pool\'+str(i+1))\n            net_z = tf.nn.max_pool(net_z, [1,_pool_sz,_pool_sz,1], strides=[1,_pool_stride[i],_pool_stride[i],1], padding=\'VALID\', name=\'pool\'+str(i+1))\n\n    print\n\n    return net_z, net_x, params_names_list, params_values_list\n\n\ndef _import_from_matconvnet(net_path):\n    mat = scipy.io.loadmat(net_path)\n    net_dot_mat = mat.get(\'net\')\n    # organize parameters to import\n    params = net_dot_mat[\'params\']\n    params = params[0][0]\n    params_names = params[\'name\'][0]\n    params_names_list = [params_names[p][0] for p in xrange(params_names.size)]\n    params_values = params[\'value\'][0]\n    params_values_list = [params_values[p] for p in xrange(params_values.size)]\n    return params_names_list, params_values_list\n\n\n# find all parameters matching the codename (there should be only one)\ndef _find_params(x, params):\n    matching = [s for s in params if x in s]\n    assert len(matching)==1, (\'Ambiguous param name found\')    \n    return matching\n\n\ndef _match_templates(net_z, net_x, params_names_list, params_values_list):\n    # finalize network\n    # z, x are [B, H, W, C]\n    net_z = tf.transpose(net_z, perm=[1,2,0,3])\n    net_x = tf.transpose(net_x, perm=[1,2,0,3])\n    # z, x are [H, W, B, C]\n    Hz, Wz, B, C = tf.unstack(tf.shape(net_z))\n    Hx, Wx, Bx, Cx = tf.unstack(tf.shape(net_x))\n    # assert B==Bx, (\'Z and X should have same Batch size\')\n    # assert C==Cx, (\'Z and X should have same Channels number\')\n    net_z = tf.reshape(net_z, (Hz, Wz, B*C, 1))\n    net_x = tf.reshape(net_x, (1, Hx, Wx, B*C))\n    net_final = tf.nn.depthwise_conv2d(net_x, net_z, strides=[1,1,1,1], padding=\'VALID\')\n    # final is [1, Hf, Wf, BC]\n    net_final = tf.concat(tf.split(net_final, 3, axis=3), axis=0)\n    # final is [B, Hf, Wf, C]\n    net_final = tf.expand_dims(tf.reduce_sum(net_final, axis=3), axis=3)\n    # final is [B, Hf, Wf, 1]\n    if _bnorm_adjust:\n        bn_beta = params_values_list[params_names_list.index(\'fin_adjust_bnb\')]\n        bn_gamma = params_values_list[params_names_list.index(\'fin_adjust_bnm\')]\n        bn_moments = params_values_list[params_names_list.index(\'fin_adjust_bnx\')]\n        bn_moving_mean = bn_moments[:,0]\n        bn_moving_variance = bn_moments[:,1]**2\n        net_final = tf.layers.batch_normalization(net_final, beta_initializer=tf.constant_initializer(bn_beta),\n                                                gamma_initializer=tf.constant_initializer(bn_gamma),\n                                                moving_mean_initializer=tf.constant_initializer(bn_moving_mean),\n                                                moving_variance_initializer=tf.constant_initializer(bn_moving_variance),\n                                                training=False, trainable=False)\n\n    return net_final\n'"
src/tracker.py,9,"b""\nimport tensorflow as tf\nprint('Using Tensorflow '+tf.__version__)\nimport matplotlib.pyplot as plt\nimport sys\n# sys.path.append('../')\nimport os\nimport csv\nimport numpy as np\nfrom PIL import Image\nimport time\n\nimport src.siamese as siam\nfrom src.visualization import show_frame, show_crops, show_scores\n\n\n# gpu_device = 2\n# os.environ['CUDA_VISIBLE_DEVICES'] = '{}'.format(gpu_device)\n\n# read default parameters and override with custom ones\ndef tracker(hp, run, design, frame_name_list, pos_x, pos_y, target_w, target_h, final_score_sz, filename, image, templates_z, scores, start_frame):\n    num_frames = np.size(frame_name_list)\n    # stores tracker's output for evaluation\n    bboxes = np.zeros((num_frames,4))\n\n    scale_factors = hp.scale_step**np.linspace(-np.ceil(hp.scale_num/2), np.ceil(hp.scale_num/2), hp.scale_num)\n    # cosine window to penalize large displacements    \n    hann_1d = np.expand_dims(np.hanning(final_score_sz), axis=0)\n    penalty = np.transpose(hann_1d) * hann_1d\n    penalty = penalty / np.sum(penalty)\n\n    context = design.context*(target_w+target_h)\n    z_sz = np.sqrt(np.prod((target_w+context)*(target_h+context)))\n    x_sz = float(design.search_sz) / design.exemplar_sz * z_sz\n\n    # thresholds to saturate patches shrinking/growing\n    min_z = hp.scale_min * z_sz\n    max_z = hp.scale_max * z_sz\n    min_x = hp.scale_min * x_sz\n    max_x = hp.scale_max * x_sz\n\n    # run_metadata = tf.RunMetadata()\n    # run_opts = {\n    #     'options': tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),\n    #     'run_metadata': run_metadata,\n    # }\n\n    run_opts = {}\n\n    # with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n    with tf.Session() as sess:\n        tf.global_variables_initializer().run()\n        # Coordinate the loading of image files.\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(coord=coord)\n        \n        # save first frame position (from ground-truth)\n        bboxes[0,:] = pos_x-target_w/2, pos_y-target_h/2, target_w, target_h                \n\n        image_, templates_z_ = sess.run([image, templates_z], feed_dict={\n                                                                        siam.pos_x_ph: pos_x,\n                                                                        siam.pos_y_ph: pos_y,\n                                                                        siam.z_sz_ph: z_sz,\n                                                                        filename: frame_name_list[0]})\n        new_templates_z_ = templates_z_\n\n        t_start = time.time()\n\n        # Get an image from the queue\n        for i in range(1, num_frames):        \n            scaled_exemplar = z_sz * scale_factors\n            scaled_search_area = x_sz * scale_factors\n            scaled_target_w = target_w * scale_factors\n            scaled_target_h = target_h * scale_factors\n            image_, scores_ = sess.run(\n                [image, scores],\n                feed_dict={\n                    siam.pos_x_ph: pos_x,\n                    siam.pos_y_ph: pos_y,\n                    siam.x_sz0_ph: scaled_search_area[0],\n                    siam.x_sz1_ph: scaled_search_area[1],\n                    siam.x_sz2_ph: scaled_search_area[2],\n                    templates_z: np.squeeze(templates_z_),\n                    filename: frame_name_list[i],\n                }, **run_opts)\n            scores_ = np.squeeze(scores_)\n            # penalize change of scale\n            scores_[0,:,:] = hp.scale_penalty*scores_[0,:,:]\n            scores_[2,:,:] = hp.scale_penalty*scores_[2,:,:]\n            # find scale with highest peak (after penalty)\n            new_scale_id = np.argmax(np.amax(scores_, axis=(1,2)))\n            # update scaled sizes\n            x_sz = (1-hp.scale_lr)*x_sz + hp.scale_lr*scaled_search_area[new_scale_id]        \n            target_w = (1-hp.scale_lr)*target_w + hp.scale_lr*scaled_target_w[new_scale_id]\n            target_h = (1-hp.scale_lr)*target_h + hp.scale_lr*scaled_target_h[new_scale_id]\n            # select response with new_scale_id\n            score_ = scores_[new_scale_id,:,:]\n            score_ = score_ - np.min(score_)\n            score_ = score_/np.sum(score_)\n            # apply displacement penalty\n            score_ = (1-hp.window_influence)*score_ + hp.window_influence*penalty\n            pos_x, pos_y = _update_target_position(pos_x, pos_y, score_, final_score_sz, design.tot_stride, design.search_sz, hp.response_up, x_sz)\n            # convert <cx,cy,w,h> to <x,y,w,h> and save output\n            bboxes[i,:] = pos_x-target_w/2, pos_y-target_h/2, target_w, target_h\n            # update the target representation with a rolling average\n            if hp.z_lr>0:\n                new_templates_z_ = sess.run([templates_z], feed_dict={\n                                                                siam.pos_x_ph: pos_x,\n                                                                siam.pos_y_ph: pos_y,\n                                                                siam.z_sz_ph: z_sz,\n                                                                image: image_\n                                                                })\n\n                templates_z_=(1-hp.z_lr)*np.asarray(templates_z_) + hp.z_lr*np.asarray(new_templates_z_)\n            \n            # update template patch size\n            z_sz = (1-hp.scale_lr)*z_sz + hp.scale_lr*scaled_exemplar[new_scale_id]\n            \n            if run.visualization:\n                show_frame(image_, bboxes[i,:], 1)        \n\n        t_elapsed = time.time() - t_start\n        speed = num_frames/t_elapsed\n\n        # Finish off the filename queue coordinator.\n        coord.request_stop()\n        coord.join(threads) \n\n        # from tensorflow.python.client import timeline\n        # trace = timeline.Timeline(step_stats=run_metadata.step_stats)\n        # trace_file = open('timeline-search.ctf.json', 'w')\n        # trace_file.write(trace.generate_chrome_trace_format())\n\n    plt.close('all')\n\n    return bboxes, speed\n\n\ndef _update_target_position(pos_x, pos_y, score, final_score_sz, tot_stride, search_sz, response_up, x_sz):\n    # find location of score maximizer\n    p = np.asarray(np.unravel_index(np.argmax(score), np.shape(score)))\n    # displacement from the center in search area final representation ...\n    center = float(final_score_sz - 1) / 2\n    disp_in_area = p - center\n    # displacement from the center in instance crop\n    disp_in_xcrop = disp_in_area * float(tot_stride) / response_up\n    # displacement from the center in instance crop (in frame coordinates)\n    disp_in_frame = disp_in_xcrop *  x_sz / search_sz\n    # *position* within frame in frame coordinates\n    pos_y, pos_x = pos_y + disp_in_frame[0], pos_x + disp_in_frame[1]\n    return pos_x, pos_y\n\n\n"""
src/visualization.py,0,"b""import numpy as np\nimport matplotlib.patches as patches\nimport matplotlib.pyplot as plt\n\n\ndef show_frame(frame, bbox, fig_n):\n    fig = plt.figure(fig_n)\n    ax = fig.add_subplot(111)\n    r = patches.Rectangle((bbox[0],bbox[1]), bbox[2], bbox[3], linewidth=2, edgecolor='r', fill=False)\n    ax.imshow(np.uint8(frame))\n    ax.add_patch(r)\n    plt.ion()\n    plt.show()\n    plt.pause(0.001)\n    plt.clf()\n\n\ndef show_crops(crops, fig_n):\n    fig = plt.figure(fig_n)\n    ax1 = fig.add_subplot(131)\n    ax2 = fig.add_subplot(132)\n    ax3 = fig.add_subplot(133)\n    ax1.imshow(np.uint8(crops[0,:,:,:]))\n    ax2.imshow(np.uint8(crops[1,:,:,:]))\n    ax3.imshow(np.uint8(crops[2,:,:,:]))\n    plt.ion()\n    plt.show()\n    plt.pause(0.001)\n\n\ndef show_scores(scores, fig_n):\n    fig = plt.figure(fig_n)\n    ax1 = fig.add_subplot(131)\n    ax2 = fig.add_subplot(132)\n    ax3 = fig.add_subplot(133)\n    ax1.imshow(scores[0,:,:], interpolation='none', cmap='hot')\n    ax2.imshow(scores[1,:,:], interpolation='none', cmap='hot')\n    ax3.imshow(scores[2,:,:], interpolation='none', cmap='hot')\n    plt.ion()\n    plt.show()\n    plt.pause(0.001)"""
