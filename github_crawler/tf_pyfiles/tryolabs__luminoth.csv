file_path,api_count,code
setup.py,0,"b'import codecs\nimport os\nimport re\nimport sys\n\nfrom setuptools import find_packages, setup\n\n\n# -------------------------------------------------------------\n\nNAME = \'luminoth\'\nPACKAGES = find_packages()\nMETA_PATH = os.path.join(\'luminoth\', \'__init__.py\')\nKEYWORDS = [\n    \'tensorflow\', \'computer vision\', \'object detection\', \'toolkit\', \'deep learning\',\n    \'faster rcnn\'\n]\nCLASSIFIERS = [\n    \'Development Status :: 2 - Pre-Alpha\',\n    \'Intended Audience :: Developers\',\n    \'Intended Audience :: Education\',\n    \'Intended Audience :: Science/Research\',\n    \'Natural Language :: English\',\n    \'License :: OSI Approved :: BSD License\',\n    \'Topic :: Scientific/Engineering :: Mathematics\',\n    \'Topic :: Software Development :: Libraries :: Python Modules\',\n    \'Topic :: Software Development :: Libraries\',\n    \'Programming Language :: Python\',\n    \'Programming Language :: Python :: 2\',\n    \'Programming Language :: Python :: 2.7\',\n    \'Programming Language :: Python :: 3\',\n    \'Programming Language :: Python :: 3.4\',\n    \'Programming Language :: Python :: 3.5\',\n    \'Programming Language :: Python :: 3.6\',\n]\n\nINSTALL_REQUIRES = [\n    \'Pillow\',\n    \'lxml\',\n    \'numpy\',\n    \'requests\',\n    \'scikit-video\',\n    \'Flask>=0.12\',\n    \'PyYAML>=3.12,<4\',\n    \'click>=6.7,<7\',\n    # Sonnet 1.25+ requires tensorflow_probability which we do not need here.\n    \'dm-sonnet>=1.12,<=1.23\',\n    # Can remove easydict <=1.8 pin after\n    # https://github.com/makinacorpus/easydict/pull/14 is merged.\n    \'easydict>=1.7,<=1.8\',\n    \'six>=1.11\',\n]\nTEST_REQUIRES = []\n\n# -------------------------------------------------------------\n\nHERE = os.path.abspath(os.path.dirname(__file__))\n\n\ndef read(*parts):\n    """"""\n    Build an absolute path from *parts* and return the contents of the\n    resulting file.  Assume UTF-8 encoding.\n    """"""\n    with codecs.open(os.path.join(HERE, *parts), \'rb\', \'utf-8\') as f:\n        return f.read()\n\n\nMETA_FILE = read(META_PATH)\n\n\ndef find_meta(meta):\n    """"""\n    Extract __*meta*__ from META_FILE.\n    """"""\n    meta_match = re.search(\n        r""^__{meta}__ = [\'\\""]([^\'\\""]*)[\'\\""]"".format(meta=meta),\n        META_FILE, re.M\n    )\n    if meta_match:\n        return meta_match.group(1)\n    raise RuntimeError(\'Unable to find __{meta}__ string.\'.format(meta=meta))\n\n\nMIN_TF_VERSION = find_meta(\'min_tf_version\')\n\n\nsetup(\n    name=NAME,\n    version=find_meta(\'version\'),\n    description=find_meta(\'description\'),\n    long_description=read(\'README.md\'),\n    license=find_meta(\'license\'),\n    author=find_meta(\'author\'),\n    author_email=find_meta(\'email\'),\n    maintainer=find_meta(\'author\'),\n    maintainer_email=find_meta(\'email\'),\n    url=find_meta(\'uri\'),\n    packages=PACKAGES,\n    classifiers=CLASSIFIERS,\n    include_package_data=True,\n    setup_requires=[\n    ],\n    install_requires=INSTALL_REQUIRES,\n    test_require=TEST_REQUIRES,\n    extras_require={\n        \'tf\': [\'tensorflow>={}\'.format(MIN_TF_VERSION)],\n        \'tf-gpu\': [\'tensorflow-gpu>=\'.format(MIN_TF_VERSION)],\n        \'gcloud\': [\n            \'google-api-python-client>=1.6.2,<2\',\n            \'google-cloud-storage>=1.2.0\',\n            \'oauth2client>=4.1.2\',\n            # See https://github.com/tryolabs/luminoth/issues/147\n            \'pyasn1>=0.4.2\',\n        ]\n    },\n    entry_points=""""""\n        [console_scripts]\n        lumi=luminoth:cli\n    """""",\n    python_requires=\'>=2.7, !=3.0.*, !=3.1.*, !=3.2.*\',\n)\n'"
docs/conf.py,0,"b'#\n# Configuration file for the Sphinx documentation builder.\n#\nimport pkg_resources\nimport sys\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'Luminoth\'\ncopyright = \'2018, Tryolabs\'\nauthor = \'Tryolabs\'\n\ntry:\n    # The full version, including alpha/beta/rc tags.\n    release = pkg_resources.get_distribution(\'luminoth\').version\nexcept pkg_resources.DistributionNotFound:\n    print(\'Luminoth must be installed to build the documentation.\')\n    sys.exit(1)\n\nif \'dev\' in release:\n    # Trim everything after `dev`, if present.\n    release = \'\'.join(release.partition(\'dev\')[:2])\n\n# The short X.Y version.\nversion = \'.\'.join(release.split(\'.\')[:2])\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.viewcode\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path .\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'alabaster\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\nhtml_sidebars = {\n    \'**\': [\n        \'globaltoc.html\', \'relations.html\', \'searchbox.html\'\n    ],\n}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'Luminothdoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'Luminoth.tex\', \'Luminoth Documentation\',\n     \'Tryolabs\', \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'luminoth\', \'Luminoth Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'Luminoth\', \'Luminoth Documentation\',\n     author, \'Luminoth\', \'Computer vision toolkit.\',\n     \'Miscellaneous\'),\n]\n\n\n# -- Extension configuration -------------------------------------------------\n'"
luminoth/__init__.py,0,"b'__version__ = \'0.2.4dev0\'\n\n__title__ = \'Luminoth\'\n__description__ = \'Computer vision toolkit based on TensorFlow\'\n__uri__ = \'https://luminoth.ai\'\n__doc__ = __description__ + \' <\' + __uri__ + \'>\'\n\n__author__ = \'Tryolabs\'\n__email__ = \'luminoth@tryolabs.com\'\n\n__license__ = \'BSD 3-Clause License\'\n__copyright__ = \'Copyright (c) 2018 Tryolabs S.A.\'\n\n__min_tf_version__ = \'1.5\'\n\n\nimport sys\n\n# Check for a current TensorFlow installation.\ntry:\n    import tensorflow  # noqa: F401\nexcept ImportError:\n    sys.exit(""""""Luminoth requires a TensorFlow >= {} installation.\n\nDepending on your use case, you should install either `tensorflow` or\n`tensorflow-gpu` packages manually or via PyPI."""""".format(__min_tf_version__))\n\n\n# Import functions that are part of Luminoth\'s public interface.\nfrom luminoth.cli import cli  # noqa\nfrom luminoth.io import read_image  # noqa\nfrom luminoth.tasks import Detector  # noqa\nfrom luminoth.vis import vis_objects  # noqa\n'"
luminoth/cli.py,0,"b'""""""Simple command line utility called `lumi`.\n\nThe cli is composed of subcommands that are able to handle different tasks\nneeded for training and using deep learning models.\n\nIt\'s base subcommands are:\n    train: For training locally.\n    cloud: For traning and monitoring in the cloud.\n    dataset: For modifying and transforming datasets.\n""""""\n\nimport click\n\nfrom luminoth.eval import eval\nfrom luminoth.predict import predict\nfrom luminoth.tools import checkpoint, cloud, dataset, server\nfrom luminoth.train import train\n\n\nCONTEXT_SETTINGS = dict(help_option_names=[\'-h\', \'--help\'])\n\n\n@click.group(context_settings=CONTEXT_SETTINGS)\ndef cli():\n    pass\n\n\ncli.add_command(checkpoint)\ncli.add_command(cloud)\ncli.add_command(dataset)\ncli.add_command(eval)\ncli.add_command(predict)\ncli.add_command(server)\ncli.add_command(train)\n'"
luminoth/eval.py,45,"b'import click\nimport json\nimport numpy as np\nimport os\nimport tensorflow as tf\nimport time\n\nfrom luminoth.datasets import get_dataset\nfrom luminoth.models import get_model\nfrom luminoth.utils.bbox_overlap import bbox_overlap\nfrom luminoth.utils.config import get_config\nfrom luminoth.utils.image_vis import image_vis_summaries\n\n\n@click.command(help=\'Evaluate trained (or training) models\')\n@click.option(\'dataset_split\', \'--split\', default=\'val\', help=\'Dataset split to use.\')  # noqa\n@click.option(\'config_files\', \'--config\', \'-c\', required=True, multiple=True, help=\'Config to use.\')  # noqa\n@click.option(\'--watch/--no-watch\', default=True, help=\'Keep watching checkpoint directory for new files.\')  # noqa\n@click.option(\'--from-global-step\', type=int, default=None, help=\'Consider only checkpoints after this global step\')  # noqa\n@click.option(\'override_params\', \'--override\', \'-o\', multiple=True, help=\'Override model config params.\')  # noqa\n@click.option(\'--files-per-class\', type=int, default=10, help=\'How many files per class display in every epoch.\')  # noqa\n@click.option(\'--max-detections\', type=int, default=100, help=\'Max detections to consider.\')  # noqa\ndef eval(dataset_split, config_files, watch, from_global_step, override_params,\n         files_per_class, max_detections):\n    """"""Evaluate models using dataset.""""""\n\n    # If the config file is empty, our config will be the base_config for the\n    # default model.\n    try:\n        config = get_config(config_files, override_params=override_params)\n    except KeyError:\n        raise KeyError(\'model.type should be set on the custom config.\')\n\n    if not config.train.job_dir:\n        raise KeyError(\'`job_dir` should be set.\')\n    if not config.train.run_name:\n        raise KeyError(\'`run_name` should be set.\')\n\n    # `run_dir` is where the actual checkpoint and logs are located.\n    run_dir = os.path.join(config.train.job_dir, config.train.run_name)\n\n    # Only activate debug for if needed for debug visualization mode.\n    if not config.train.debug:\n        config.train.debug = config.eval.image_vis == \'debug\'\n\n    if config.train.debug or config.train.tf_debug:\n        tf.logging.set_verbosity(tf.logging.DEBUG)\n    else:\n        tf.logging.set_verbosity(tf.logging.INFO)\n\n    # Build the dataset tensors, overriding the default dataset split.\n    config.dataset.split = dataset_split\n\n    # Disable data augmentation.\n    config.dataset.data_augmentation = []\n\n    # Attempt to get class names, if available.\n    classes_file = os.path.join(config.dataset.dir, \'classes.json\')\n    if tf.gfile.Exists(classes_file):\n        class_labels = json.load(tf.gfile.GFile(classes_file))\n    else:\n        class_labels = None\n\n    if config.model.type == \'fasterrcnn\':\n        # Override max detections with specified value.\n        if config.model.network.with_rcnn:\n            config.model.rcnn.proposals.total_max_detections = max_detections\n        else:\n            config.model.rpn.proposals.post_nms_top_n = max_detections\n\n        # Also overwrite `min_prob_threshold` in order to use all detections.\n        config.model.rcnn.proposals.min_prob_threshold = 0.0\n    elif config.model.type == \'ssd\':\n        config.model.proposals.total_max_detections = max_detections\n        config.model.proposals.min_prob_threshold = 0.0\n    else:\n        raise ValueError(\n            ""Model type \'{}\' not supported"".format(config.model.type)\n        )\n\n    # Only a single run over the dataset to calculate metrics.\n    config.train.num_epochs = 1\n\n    # Seed setup.\n    if config.train.seed:\n        tf.set_random_seed(config.train.seed)\n\n    # Set pretrained as not training.\n    config.model.base_network.trainable = False\n\n    model_class = get_model(config.model.type)\n    model = model_class(config)\n    dataset_class = get_dataset(config.dataset.type)\n    dataset = dataset_class(config)\n    train_dataset = dataset()\n\n    train_image = train_dataset[\'image\']\n    train_objects = train_dataset[\'bboxes\']\n    train_filename = train_dataset[\'filename\']\n\n    # Build the graph of the model to evaluate, retrieving required\n    # intermediate tensors.\n    prediction_dict = model(train_image, train_objects)\n\n    if config.model.type == \'ssd\' or config.model.network.with_rcnn:\n        pred = prediction_dict[\'classification_prediction\']\n        pred_objects = pred[\'objects\']\n        pred_objects_classes = pred[\'labels\']\n        pred_objects_scores = pred[\'probs\']\n    else:\n        # Force the num_classes to 1.\n        config.model.network.num_classes = 1\n\n        pred = prediction_dict[\'rpn_prediction\']\n        pred_objects = pred[\'proposals\']\n        pred_objects_scores = pred[\'scores\']\n        # When using only RPN all classes are 0.\n        pred_objects_classes = tf.zeros(\n            (tf.shape(pred_objects_scores)[0],), dtype=tf.int32\n        )\n\n    # Retrieve *all* the losses from the model and calculate their streaming\n    # means, so we get the loss over the whole dataset.\n    batch_losses = model.loss(prediction_dict, return_all=True)\n    losses = {}\n    for loss_name, loss_tensor in batch_losses.items():\n        loss_mean, _ = tf.metrics.mean(\n            loss_tensor, name=loss_name,\n            metrics_collections=\'metrics\',\n            updates_collections=\'metric_ops\',\n        )\n        full_loss_name = \'{}_losses/{}\'.format(dataset_split, loss_name)\n        losses[full_loss_name] = loss_mean\n\n    metric_ops = tf.get_collection(\'metric_ops\')\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    # Using a global saver instead of the one for the model.\n    saver = tf.train.Saver(sharded=True, allow_empty=True)\n\n    # Aggregate the required ops to evaluate into a dict.\n    ops = {\n        \'init_op\': init_op,\n        \'metric_ops\': metric_ops,\n        \'pred_objects\': pred_objects,\n        \'pred_objects_classes\': pred_objects_classes,\n        \'pred_objects_scores\': pred_objects_scores,\n        \'train_objects\': train_objects,\n        \'losses\': losses,\n        \'prediction_dict\': prediction_dict,\n        \'filename\': train_filename,\n        \'train_image\': train_image\n    }\n\n    metrics_scope = \'{}_metrics\'.format(dataset_split)\n\n    # Use global writer for all checkpoints. We don\'t want to write different\n    # files for each checkpoint.\n    writer = tf.summary.FileWriter(run_dir)\n\n    files_to_visualize = {}\n\n    last_global_step = from_global_step\n    while True:\n        # Get the checkpoint files to evaluate.\n        try:\n            checkpoints = get_checkpoints(\n                run_dir, last_global_step, last_only=not watch\n            )\n        except ValueError as e:\n            if not watch:\n                tf.logging.error(\'Missing checkpoint.\')\n                raise e\n\n            tf.logging.warning(\n                \'Missing checkpoint; Checking again in a moment\')\n            time.sleep(5)\n            continue\n\n        for checkpoint in checkpoints:\n            # Always returned in order, so it\'s safe to assign directly.\n            tf.logging.info(\n                \'Evaluating global_step {} using checkpoint \\\'{}\\\'\'.format(\n                    checkpoint[\'global_step\'], checkpoint[\'file\']\n                )\n            )\n            try:\n                start = time.time()\n                evaluate_once(\n                    config, writer, saver, ops, checkpoint,\n                    class_labels=class_labels,\n                    metrics_scope=metrics_scope,\n                    image_vis=config.eval.image_vis,\n                    files_per_class=files_per_class,\n                    files_to_visualize=files_to_visualize,\n                )\n                last_global_step = checkpoint[\'global_step\']\n                tf.logging.info(\'Evaluated in {:.2f}s\'.format(\n                    time.time() - start\n                ))\n            except tf.errors.NotFoundError:\n                # The checkpoint is not ready yet. It was written in the\n                # checkpoints file, but it still hasn\'t been completely saved.\n                tf.logging.info(\n                    \'Checkpoint {} is not ready yet. \'\n                    \'Checking again in a moment.\'.format(\n                        checkpoint[\'file\']\n                    )\n                )\n                time.sleep(5)\n                continue\n\n        # If no watching was requested, finish the execution.\n        if not watch:\n            return\n\n        # Sleep for a moment and check for new checkpoints.\n        tf.logging.info(\'All checkpoints evaluated; sleeping for a moment\')\n        time.sleep(5)\n\n\ndef get_checkpoints(run_dir, from_global_step=None, last_only=False):\n    """"""Return all available checkpoints.\n\n    Args:\n        run_dir: Directory where the checkpoints are located.\n        from_global_step (int): Only return checkpoints after this global step.\n            The comparison is *strict*. If ``None``, returns all available\n            checkpoints.\n\n    Returns:\n        List of dicts (with keys ``global_step``, ``file``) with all the\n        checkpoints found.\n\n    Raises:\n        ValueError: If there are no checkpoints in ``run_dir``.\n    """"""\n    # The latest checkpoint file should be the last item of\n    # `all_model_checkpoint_paths`, according to the CheckpointState protobuf\n    # definition.\n    # TODO: Must check if the checkpoints are complete somehow.\n    ckpt = tf.train.get_checkpoint_state(run_dir)\n    if not ckpt or not ckpt.all_model_checkpoint_paths:\n        raise ValueError(\'Could not find checkpoint in {}.\'.format(run_dir))\n\n    # TODO: Any other way to get the global_step? (Same as in `checkpoints`.)\n    checkpoints = sorted([\n        {\'global_step\': int(path.split(\'-\')[-1]), \'file\': path}\n        for path in ckpt.all_model_checkpoint_paths\n    ], key=lambda c: c[\'global_step\'])\n\n    if last_only:\n        checkpoints = checkpoints[-1:]\n        tf.logging.info(\n            \'Using last checkpoint in run_dir, global_step = {}\'.format(\n                checkpoints[0][\'global_step\']\n            )\n        )\n    elif from_global_step is not None:\n        checkpoints = [\n            c for c in checkpoints\n            if c[\'global_step\'] > from_global_step\n        ]\n\n        tf.logging.info(\n            \'Found %s checkpoints in run_dir with global_step > %s\',\n            len(checkpoints), from_global_step,\n        )\n\n    else:\n        tf.logging.info(\n            \'Found {} checkpoints in run_dir\'.format(len(checkpoints))\n        )\n\n    return checkpoints\n\n\ndef evaluate_once(config, writer, saver, ops, checkpoint,\n                  class_labels, metrics_scope=\'metrics\', image_vis=None,\n                  files_per_class=None, files_to_visualize=None):\n    """"""Run the evaluation once.\n\n    Create a new session with the previously-built graph, run it through the\n    dataset, calculate the evaluation metrics and write the corresponding\n    summaries.\n\n    Args:\n        config: Config object for the model.\n        writer: Summary writers.\n        saver: Saver object to restore checkpoint parameters.\n        ops (dict): All the operations needed to successfully run the model.\n            Expects the following keys: ``init_op``, ``metric_ops``,\n            ``pred_objects``, ``pred_objects_classes``,\n            ``pred_objects_scores``, ``train_objects``, ``losses``,\n            ``train_image``.\n        checkpoint (dict): Checkpoint-related data.\n            Expects the following keys: ``global_step``, ``file``.\n    """"""\n    # Output of the detector, per batch.\n    output_per_batch = {\n        \'bboxes\': [],  # Bounding boxes detected.\n        \'classes\': [],  # Class associated to each bounding box.\n        \'scores\': [],  # Score for each detection.\n        \'gt_bboxes\': [],  # Ground-truth bounding boxes for the batch.\n        \'gt_classes\': [],  # Ground-truth classes for each bounding box.\n    }\n\n    with tf.Session() as sess:\n        sess.run(ops[\'init_op\'])\n        saver.restore(sess, checkpoint[\'file\'])\n\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n        total_evaluated = 0\n        start_time = time.time()\n\n        try:\n            track_start = start_time\n            track_count = 0\n            while not coord.should_stop():\n                fetches = {\n                    \'metric_ops\': ops[\'metric_ops\'],\n                    \'bboxes\': ops[\'pred_objects\'],\n                    \'classes\': ops[\'pred_objects_classes\'],\n                    \'scores\': ops[\'pred_objects_scores\'],\n                    \'gt_bboxes\': ops[\'train_objects\'],\n                    \'losses\': ops[\'losses\'],\n                    \'filename\': ops[\'filename\'],\n                }\n                if image_vis is not None:\n                    fetches[\'prediction_dict\'] = ops[\'prediction_dict\']\n                    fetches[\'train_image\'] = ops[\'train_image\']\n\n                batch_fetched = sess.run(fetches)\n                output_per_batch[\'bboxes\'].append(batch_fetched.get(\'bboxes\'))\n                output_per_batch[\'classes\'].append(batch_fetched[\'classes\'])\n                output_per_batch[\'scores\'].append(batch_fetched[\'scores\'])\n\n                batch_gt_objects = batch_fetched[\'gt_bboxes\']\n                output_per_batch[\'gt_bboxes\'].append(batch_gt_objects[:, :4])\n                batch_gt_classes = batch_gt_objects[:, 4]\n                output_per_batch[\'gt_classes\'].append(batch_gt_classes)\n\n                val_losses = batch_fetched[\'losses\']\n\n                if image_vis is not None:\n                    filename = batch_fetched[\'filename\'].decode(\'utf-8\')\n                    visualize_file = False\n                    for gt_class in batch_gt_classes:\n                        cls_files = files_to_visualize.get(\n                            gt_class, set()\n                        )\n                        if len(cls_files) < files_per_class:\n                            files_to_visualize.setdefault(\n                                gt_class, set()\n                            ).add(filename)\n                            visualize_file = True\n                            break\n                        elif filename in cls_files:\n                            visualize_file = True\n                            break\n\n                    if visualize_file:\n                        image_summaries = image_vis_summaries(\n                            batch_fetched[\'prediction_dict\'],\n                            config=config.model,\n                            extra_tag=filename,\n                            image_visualization_mode=image_vis,\n                            image=batch_fetched[\'train_image\'],\n                            gt_bboxes=batch_fetched[\'gt_bboxes\']\n                        )\n                        for image_summary in image_summaries:\n                            writer.add_summary(\n                                image_summary, checkpoint[\'global_step\']\n                            )\n\n                total_evaluated += 1\n                track_count += 1\n\n                track_end = time.time()\n                if track_end - track_start > 20.:\n                    click.echo(\n                        \'{} processed in {:.2f}s (global {:.2f} images/s, \'\n                        \'period {:.2f} images/s)\'.format(\n                            total_evaluated, track_end - start_time,\n                            total_evaluated / (track_end - start_time),\n                            track_count / (track_end - track_start),\n                        ))\n                    track_count = 0\n                    track_start = track_end\n\n        except tf.errors.OutOfRangeError:\n\n            # Save final evaluation stats into summary under the checkpoint\'s\n            # global step.\n            ap_per_class, ar_per_class = calculate_metrics(\n                output_per_batch, config.model.network.num_classes\n            )\n\n            map_at_50 = np.mean(ap_per_class[:, 0])\n            map_at_75 = np.mean(ap_per_class[:, 5])\n            map_at_range = np.mean(ap_per_class)\n            mar_at_range = np.mean(ar_per_class)\n\n            tf.logging.info(\'Finished evaluation at step {}.\'.format(\n                checkpoint[\'global_step\']))\n            tf.logging.info(\'Evaluated {} images.\'.format(total_evaluated))\n\n            tf.logging.info(\n                \'Average Precision (AP) @ [0.50] = {:.3f}\'.format(map_at_50)\n            )\n            tf.logging.info(\n                \'Average Precision (AP) @ [0.75] = {:.3f}\'.format(map_at_75)\n            )\n            tf.logging.info(\n                \'Average Precision (AP) @ [0.50:0.95] = {:.3f}\'.format(\n                    map_at_range\n                )\n            )\n            tf.logging.info(\n                \'Average Recall (AR) @ [0.50:0.95] = {:.3f}\'.format(\n                    mar_at_range\n                )\n            )\n\n            for idx, val in enumerate(ap_per_class[:, 0]):\n                class_label = \'{} ({})\'.format(\n                    class_labels[idx], idx\n                ) if class_labels else idx\n                tf.logging.debug(\n                    \'Average Precision (AP) @ [0.50] for {} = {:.3f}\'.format(\n                        class_label, val\n                    )\n                )\n\n            summary = [\n                tf.Summary.Value(\n                    tag=\'{}/AP@0.50\'.format(metrics_scope),\n                    simple_value=map_at_50\n                ),\n                tf.Summary.Value(\n                    tag=\'{}/AP@0.75\'.format(metrics_scope),\n                    simple_value=map_at_75\n                ),\n                tf.Summary.Value(\n                    tag=\'{}/AP@[0.50:0.95]\'.format(metrics_scope),\n                    simple_value=map_at_range\n                ),\n                tf.Summary.Value(\n                    tag=\'{}/AR@[0.50:0.95]\'.format(metrics_scope),\n                    simple_value=mar_at_range\n                ),\n                tf.Summary.Value(\n                    tag=\'{}/total_evaluated\'.format(metrics_scope),\n                    simple_value=total_evaluated\n                ),\n                tf.Summary.Value(\n                    tag=\'{}/evaluation_time\'.format(metrics_scope),\n                    simple_value=time.time() - start_time\n                ),\n            ]\n\n            for loss_name, loss_value in val_losses.items():\n                tf.logging.debug(\'{} loss = {:.4f}\'.format(\n                    loss_name, loss_value))\n                summary.append(tf.Summary.Value(\n                    tag=loss_name,\n                    simple_value=loss_value\n                ))\n\n            writer.add_summary(\n                tf.Summary(value=summary), checkpoint[\'global_step\']\n            )\n\n        finally:\n            coord.request_stop()\n\n        # Wait for all threads to stop.\n        coord.join(threads)\n\n\ndef calculate_metrics(output_per_batch, num_classes):\n    """"""Calculates mAP and mAR from the detector\'s output.\n\n    The procedure for calculating the average precision for class ``C`` is as\n    follows (see `VOC mAP metric`_ for more details):\n\n    Start by ranking all the predictions (for a given image and said class) in\n    order of confidence.  Each of these predictions is marked as correct (true\n    positive, when it has a IoU-threshold greater or equal to `iou_thresholds`)\n    or incorrect (false positive, in the other case).  This matching is\n    performed greedily over the confidence scores, so a higher-confidence\n    prediction will be matched over another lower-confidence one even if the\n    latter has better IoU.  Also, each prediction is matched at most once, so\n    repeated detections are counted as false positives.\n\n    We then integrate over the interpolated PR curve, thus obtaining the value\n    for the class\' average precision.  This interpolation makes sure the\n    precision curve is monotonically decreasing; for this, we go through the\n    precisions and make sure it\'s always decreasing.  The integration is\n    performed over 101 fixed points over the curve (``[0.0, 0.01, ..., 1.0]``).\n\n    Average the result among all the classes to obtain the final, ``mAP``,\n    value.\n\n    Args:\n        output_per_batch (dict): Output of the detector to calculate mAP.\n            Expects the following keys: ``bboxes``, ``classes``, ``scores``,\n            ``gt_bboxes``, ``gt_classes``. Under each key, there should be a\n            list of the results per batch as returned by the detector.\n        num_classes (int): Number of classes on the dataset.\n\n    Returns:\n        (``np.ndarray``, ``ndarray``) tuple. The first value is an array of\n        size (`num_classes`,), with the AP value per class, while the second\n        one is an array for the AR.\n\n    .. _VOC mAP metric:\n        http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.pdf\n    """"""\n    iou_thresholds = np.linspace(\n        0.50, 0.95, np.round((0.95 - 0.50) / 0.05) + 1\n    )\n    # 101 recall levels, same as COCO evaluation.\n    rec_thresholds = np.linspace(\n        0.00, 1.00, np.round((1.00 - 0.00) / 0.01) + 1\n    )\n\n    # List; first by class, then by example. Each entry is a tuple of ndarrays\n    # of size (D_{c,i},), for tp/fp labels and for score, where D_{c,i} is the\n    # number of detected boxes for class `c` on image `i`.\n    tp_fp_labels_by_class = [[] for _ in range(num_classes)]\n    num_examples_per_class = [0 for _ in range(num_classes)]\n\n    # For each image, order predictions by score and classify each as a true\n    # positive or a false positive.\n    num_batches = len(output_per_batch[\'bboxes\'])\n    for idx in range(num_batches):\n\n        # Get the results of the batch.\n        classes = output_per_batch[\'classes\'][idx]  # (D_{c,i},)\n        bboxes = output_per_batch[\'bboxes\'][idx]  # (D_{c,i}, 4)\n        scores = output_per_batch[\'scores\'][idx]  # (D_{c,i},)\n\n        gt_classes = output_per_batch[\'gt_classes\'][idx]\n        gt_bboxes = output_per_batch[\'gt_bboxes\'][idx]\n\n        # Analysis must be made per-class.\n        for cls in range(num_classes):\n            # Get the bounding boxes of `cls` only.\n            cls_bboxes = bboxes[classes == cls, :]\n            cls_scores = scores[classes == cls]\n            cls_gt_bboxes = gt_bboxes[gt_classes == cls, :]\n\n            num_gt = cls_gt_bboxes.shape[0]\n            num_examples_per_class[cls] += num_gt\n\n            # Sort by score descending, so we prioritize higher-confidence\n            # results when matching.\n            sorted_indices = np.argsort(-cls_scores)\n\n            # Whether the ground-truth has been previously detected.\n            is_detected = np.zeros((num_gt, len(iou_thresholds)))\n\n            # TP/FP labels for detected bboxes of (class, image).\n            tp_fp_labels = np.zeros((len(sorted_indices), len(iou_thresholds)))\n\n            if num_gt == 0:\n                # If no ground truth examples for class, all predictions must\n                # be false positives.\n                tp_fp_labels_by_class[cls].append(\n                    (tp_fp_labels, cls_scores[sorted_indices])\n                )\n                continue\n\n            # Get the IoUs for the class\' bboxes.\n            ious = bbox_overlap(cls_bboxes, cls_gt_bboxes)\n\n            # Greedily assign bboxes to ground truths (highest score first).\n            for bbox_idx in sorted_indices:\n                gt_match = np.argmax(ious[bbox_idx, :])\n                # TODO: Try to vectorize.\n                for iou_idx, iou_threshold in enumerate(iou_thresholds):\n                    if ious[bbox_idx, gt_match] >= iou_threshold:\n                        # Over IoU threshold.\n                        if not is_detected[gt_match, iou_idx]:\n                            # And first detection: it\'s a true positive.\n                            tp_fp_labels[bbox_idx, iou_idx] = True\n                            is_detected[gt_match, iou_idx] = True\n\n            tp_fp_labels_by_class[cls].append(\n                (tp_fp_labels, cls_scores[sorted_indices])\n            )\n\n    # Calculate average precision per class.\n    ap_per_class = np.zeros((num_classes, len(iou_thresholds)))\n    ar_per_class = np.zeros((num_classes, len(iou_thresholds)))\n    for cls in range(num_classes):\n        tp_fp_labels = tp_fp_labels_by_class[cls]\n        num_examples = num_examples_per_class[cls]\n\n        # Flatten the tp/fp labels into a single ndarray.\n        labels, scores = zip(*tp_fp_labels)\n        labels = np.concatenate(labels)\n        scores = np.concatenate(scores)\n\n        # Sort the tp/fp labels by decreasing confidence score and calculate\n        # precision and recall at every position of this ranked output.\n        sorted_indices = np.argsort(-scores)\n        true_positives = labels[sorted_indices, :]\n        false_positives = 1 - true_positives\n\n        cum_true_positives = np.cumsum(true_positives, axis=0)\n        cum_false_positives = np.cumsum(false_positives, axis=0)\n\n        recall = cum_true_positives.astype(float) / num_examples\n        precision = np.divide(\n            cum_true_positives.astype(float),\n            cum_true_positives + cum_false_positives\n        )\n\n        # Find AP by integrating over PR curve, with interpolated precision.\n        for iou_idx in range(len(iou_thresholds)):\n            p = precision[:, iou_idx]\n            r = recall[:, iou_idx]\n\n            # Interpolate the precision. (Make it monotonically-increasing.)\n            for i in range(len(p) - 1, 0, -1):\n                if p[i] > p[i-1]:\n                    p[i-1] = p[i]\n\n            ap = 0\n            inds = np.searchsorted(r, rec_thresholds)\n            for ridx, pidx in enumerate(inds):\n                if pidx >= len(r):\n                    # Out of bounds, no recall higher than threshold for any of\n                    # the remaining thresholds (as they\'re ordered).\n                    break\n\n                ap += p[pidx] / len(rec_thresholds)\n\n            ap_per_class[cls, iou_idx] = ap\n            if len(r):\n                ar_per_class[cls, iou_idx] = r[-1]\n            else:\n                ar_per_class[cls, iou_idx] = 0\n\n    return ap_per_class, ar_per_class\n\n\nif __name__ == \'__main__\':\n    eval()\n'"
luminoth/io.py,0,"b'import numpy as np\nimport os\n\nfrom PIL import Image\n\n\ndef read_image(path):\n    """"""Reads an image located at `path` into an array.\n\n    Arguments:\n        path (str): Path to a valid image file in the filesystem.\n\n    Returns:\n        `numpy.ndarray` of size `(height, width, channels)`.\n    """"""\n    full_path = os.path.expanduser(path)\n    return np.array(Image.open(full_path).convert(\'RGB\'))\n'"
luminoth/predict.py,10,"b'import click\nimport json\nimport numpy as np\nimport os\nimport skvideo.io\nimport sys\nimport time\nimport tensorflow as tf\n\nfrom PIL import Image\nfrom luminoth.tools.checkpoint import get_checkpoint_config\nfrom luminoth.utils.config import get_config, override_config_params\nfrom luminoth.utils.predicting import PredictorNetwork\nfrom luminoth.vis import build_colormap, vis_objects\n\nIMAGE_FORMATS = [\'jpg\', \'jpeg\', \'png\']\nVIDEO_FORMATS = [\'mov\', \'mp4\', \'avi\']  # TODO: check if more formats work\n\n\ndef get_file_type(filename):\n    extension = filename.split(\'.\')[-1].lower()\n    if extension in IMAGE_FORMATS:\n        return \'image\'\n    elif extension in VIDEO_FORMATS:\n        return \'video\'\n\n\ndef resolve_files(path_or_dir):\n    """"""Returns the file paths for `path_or_dir`.\n\n    Args:\n        path_or_dir: String or list of strings for the paths or directories to\n            run predictions in. For directories, will return all the files\n            within.\n\n    Returns:\n        List of strings with the full path for each file.\n    """"""\n    if not isinstance(path_or_dir, tuple):\n        path_or_dir = (path_or_dir,)\n\n    paths = []\n    for entry in path_or_dir:\n        if tf.gfile.IsDirectory(entry):\n            paths.extend([\n                os.path.join(entry, f)\n                for f in tf.gfile.ListDirectory(entry)\n                if get_file_type(f) in (\'image\', \'video\')\n            ])\n        elif get_file_type(entry) in (\'image\', \'video\'):\n            if not tf.gfile.Exists(entry):\n                click.echo(\'Input {} not found, skipping.\'.format(entry))\n                continue\n            paths.append(entry)\n\n    return paths\n\n\ndef filter_classes(objects, only_classes=None, ignore_classes=None):\n    if ignore_classes:\n        objects = [o for o in objects if o[\'label\'] not in ignore_classes]\n\n    if only_classes:\n        objects = [o for o in objects if o[\'label\'] in only_classes]\n\n    return objects\n\n\ndef predict_image(network, path, only_classes=None, ignore_classes=None,\n                  save_path=None):\n    click.echo(\'Predicting {}...\'.format(path), nl=False)\n\n    # Open and read the image to predict.\n    with tf.gfile.Open(path, \'rb\') as f:\n        try:\n            image = Image.open(f).convert(\'RGB\')\n        except (tf.errors.OutOfRangeError, OSError) as e:\n            click.echo()\n            click.echo(\'Error while processing {}: {}\'.format(path, e))\n            return\n\n    # Run image through the network.\n    objects = network.predict_image(image)\n\n    # Filter the results according to the user input.\n    objects = filter_classes(\n        objects,\n        only_classes=only_classes,\n        ignore_classes=ignore_classes\n    )\n\n    # Save predicted image.\n    if save_path:\n        vis_objects(np.array(image), objects).save(save_path)\n\n    click.echo(\' done.\')\n    return objects\n\n\ndef predict_video(network, path, only_classes=None, ignore_classes=None,\n                  save_path=None):\n    if save_path:\n        # We hardcode the video output to mp4 for the time being.\n        save_path = os.path.splitext(save_path)[0] + \'.mp4\'\n        try:\n            writer = skvideo.io.FFmpegWriter(save_path)\n        except AssertionError as e:\n            tf.logging.error(e)\n            tf.logging.error(\n                \'Please install ffmpeg before making video predictions.\'\n            )\n            exit()\n    else:\n        click.echo(\n            \'Video not being saved. Note that for the time being, no JSON \'\n            \'output is being generated. Did you mean to specify `--save-path`?\'\n        )\n\n    num_of_frames = int(skvideo.io.ffprobe(path)[\'video\'][\'@nb_frames\'])\n\n    video_progress_bar = click.progressbar(\n        skvideo.io.vreader(path),\n        length=num_of_frames,\n        label=\'Predicting {}\'.format(path)\n    )\n\n    colormap = build_colormap()\n\n    objects_per_frame = []\n    with video_progress_bar as bar:\n        try:\n            start_time = time.time()\n            for idx, frame in enumerate(bar):\n                # Run image through network.\n                objects = network.predict_image(frame)\n\n                # Filter the results according to the user input.\n                objects = filter_classes(\n                    objects,\n                    only_classes=only_classes,\n                    ignore_classes=ignore_classes\n                )\n\n                objects_per_frame.append({\n                    \'frame\': idx,\n                    \'objects\': objects\n                })\n\n                # Draw the image and write it to the video file.\n                if save_path:\n                    image = vis_objects(frame, objects, colormap=colormap)\n                    writer.writeFrame(np.array(image))\n\n            stop_time = time.time()\n            click.echo(\n                \'fps: {0:.1f}\'.format(num_of_frames / (stop_time - start_time))\n            )\n        except RuntimeError as e:\n            click.echo()  # Error prints next to progress bar otherwise.\n            click.echo(\'Error while processing {}: {}\'.format(path, e))\n            if save_path:\n                click.echo(\n                    \'Partially processed video file saved in {}\'.format(\n                        save_path\n                    )\n                )\n\n    if save_path:\n        writer.close()\n\n    return objects_per_frame\n\n\n@click.command(help=""Obtain a model\'s predictions."")\n@click.argument(\'path-or-dir\', nargs=-1)\n@click.option(\'config_files\', \'--config\', \'-c\', multiple=True, help=\'Config to use.\')  # noqa\n@click.option(\'--checkpoint\', help=\'Checkpoint to use.\')\n@click.option(\'override_params\', \'--override\', \'-o\', multiple=True, help=\'Override model config params.\')  # noqa\n@click.option(\'output_path\', \'--output\', \'-f\', default=\'-\', help=\'Output file with the predictions (for example, JSON bounding boxes).\')  # noqa\n@click.option(\'--save-media-to\', \'-d\', help=\'Directory to store media to.\')\n@click.option(\'--min-prob\', default=0.5, type=float, help=\'When drawing, only draw bounding boxes with probability larger than.\')  # noqa\n@click.option(\'--max-detections\', default=100, type=int, help=\'Maximum number of detections per image.\')  # noqa\n@click.option(\'--only-class\', \'-k\', default=None, multiple=True, help=\'Class to ignore when predicting.\')  # noqa\n@click.option(\'--ignore-class\', \'-K\', default=None, multiple=True, help=\'Class to ignore when predicting.\')  # noqa\n@click.option(\'--debug\', is_flag=True, help=\'Set debug level logging.\')\ndef predict(path_or_dir, config_files, checkpoint, override_params,\n            output_path, save_media_to, min_prob, max_detections, only_class,\n            ignore_class, debug):\n    """"""Obtain a model\'s predictions.\n\n    Receives either `config_files` or `checkpoint` in order to load the correct\n    model. Afterwards, runs the model through the inputs specified by\n    `path-or-dir`, returning predictions according to the format specified by\n    `output`.\n\n    Additional model behavior may be modified with `min-prob`, `only-class` and\n    `ignore-class`.\n    """"""\n    if debug:\n        tf.logging.set_verbosity(tf.logging.DEBUG)\n    else:\n        tf.logging.set_verbosity(tf.logging.ERROR)\n\n    if only_class and ignore_class:\n        click.echo(\n            ""Only one of `only-class` or `ignore-class` may be specified.""\n        )\n        return\n\n    # Process the input and get the actual files to predict.\n    files = resolve_files(path_or_dir)\n    if not files:\n        error = \'No files to predict found. Accepted formats are: {}.\'.format(\n            \', \'.join(IMAGE_FORMATS + VIDEO_FORMATS)\n        )\n        click.echo(error)\n        return\n    else:\n        click.echo(\'Found {} files to predict.\'.format(len(files)))\n\n    # Build the `Formatter` based on the outputs, which automatically writes\n    # the formatted output to all the requested output files.\n    if output_path == \'-\':\n        output = sys.stdout\n    else:\n        output = open(output_path, \'w\')\n\n    # Create `save_media_to` if specified and it doesn\'t exist.\n    if save_media_to:\n        tf.gfile.MakeDirs(save_media_to)\n\n    # Resolve the config to use and initialize the model.\n    if checkpoint:\n        config = get_checkpoint_config(checkpoint)\n    elif config_files:\n        config = get_config(config_files)\n    else:\n        click.echo(\n            \'Neither checkpoint not config specified, assuming `accurate`.\'\n        )\n        config = get_checkpoint_config(\'accurate\')\n\n    if override_params:\n        config = override_config_params(config, override_params)\n\n    # Filter bounding boxes according to `min_prob` and `max_detections`.\n    if config.model.type == \'fasterrcnn\':\n        if config.model.network.with_rcnn:\n            config.model.rcnn.proposals.total_max_detections = max_detections\n        else:\n            config.model.rpn.proposals.post_nms_top_n = max_detections\n        config.model.rcnn.proposals.min_prob_threshold = min_prob\n    elif config.model.type == \'ssd\':\n        config.model.proposals.total_max_detections = max_detections\n        config.model.proposals.min_prob_threshold = min_prob\n    else:\n        raise ValueError(\n            ""Model type \'{}\' not supported"".format(config.model.type)\n        )\n\n    # Instantiate the model indicated by the config.\n    network = PredictorNetwork(config)\n\n    # Iterate over files and run the model on each.\n    for file in files:\n\n        # Get the media output path, if media storage is requested.\n        save_path = os.path.join(\n            save_media_to, \'pred_{}\'.format(os.path.basename(file))\n        ) if save_media_to else None\n\n        file_type = get_file_type(file)\n        predictor = predict_image if file_type == \'image\' else predict_video\n\n        objects = predictor(\n            network, file,\n            only_classes=only_class,\n            ignore_classes=ignore_class,\n            save_path=save_path,\n        )\n\n        # TODO: Not writing jsons for video files for now.\n        if objects is not None and file_type == \'image\':\n            output.write(\n                json.dumps({\n                    \'file\': file,\n                    \'objects\': objects,\n                }) + \'\\n\'\n            )\n\n    output.close()\n'"
luminoth/tasks.py,0,"b'""""""Module where all generic task models are located.\n\nEach class corresponds to a different task, providing a task-specific API for\nease of use. This API is common among different implementations, abstracting\naway the pecularities of each task model. Thus, no knowledge of the inner\nworkings of said models should be needed to use any of these classes.\n""""""\nfrom luminoth.tools.checkpoint import get_checkpoint_config\nfrom luminoth.utils.predicting import PredictorNetwork\n\n\nclass Detector(object):\n    """"""Encapsulates an object detection model behavior.\n\n    In order to perform object detection with a model implemented within\n    Luminoth, this class should be used.\n\n    Attributes:\n        classes (list of str): Ordered class names for the detector.\n        prob (float): Default probability threshold for predictions.\n\n    TODO:\n        - Don\'t create a TF session internally (or make its creation optional)\n          in order to be compatible with both TF-Eager and Jupyter Notebooks.\n        - Manage multiple instantiations correctly in order to avoid creating\n          the same TF objects over and over (which appends the `_N` suffix to\n          the graph and makes the checkpoint loading fail).\n    """"""\n\n    DEFAULT_CHECKPOINT = \'accurate\'\n\n    def __init__(self, checkpoint=None, config=None, prob=0.7, classes=None):\n        """"""Instantiate a detector object with the appropriate config.\n\n        Arguments:\n            checkpoint (str): Checkpoint id or alias to instantiate the\n                detector as.\n            config (dict): Configuration parameters describing the desired\n                model. See `get_config` to load a config file.\n\n        Note:\n            Only one of the parameters must be specified. If none is, we\n            default to loading the checkpoint indicated by\n            `DEFAULT_CHECKPOINT`.\n        """"""\n        if checkpoint is not None and config is not None:\n            raise ValueError(\n                \'Only one of `checkpoint` or `config` must be specified in \'\n                \'order to instantiate a Detector.\'\n            )\n\n        if checkpoint is None and config is None:\n            # Neither checkpoint no config specified, default to\n            # `DEFAULT_CHECKPOINT`.\n            checkpoint = self.DEFAULT_CHECKPOINT\n\n        if checkpoint:\n            config = get_checkpoint_config(checkpoint)\n\n        # Prevent the model itself from filtering its proposals (default\n        # value of 0.5 is in use in the configs).\n        # TODO: A model should always return all of its predictions. The\n        # filtering should be done (if at all) by PredictorNetwork.\n        if config.model.type == \'fasterrcnn\':\n            config.model.rcnn.proposals.min_prob_threshold = 0.0\n        elif config.model.type == \'ssd\':\n            config.model.proposals.min_prob_threshold = 0.0\n\n        # TODO: Remove dependency on `PredictorNetwork` or clearly separate\n        # responsibilities.\n        self._network = PredictorNetwork(config)\n\n        self.prob = prob\n\n        # Use the labels when available, integers when not.\n        self._model_classes = (\n            self._network.class_labels if self._network.class_labels\n            else list(range(config.model.network.num_classes))\n        )\n        if classes:\n            self.classes = set(classes)\n            if not set(self._model_classes).issuperset(self.classes):\n                raise ValueError(\n                    \'`classes` must be contained in the detector\\\'s classes. \'\n                    \'Available classes are: {}.\'.format(self._model_classes)\n                )\n        else:\n            self.classes = set(self._model_classes)\n\n    def predict(self, images, prob=None, classes=None):\n        """"""Run the detector through a set of images.\n\n        Arguments:\n            images (numpy.ndarray or list): Either array of dimensions\n                `(height, width, channels)` (single image) or array of\n                dimensions `(number_of_images, height, width, channels)`\n                (multiple images). If a list, must be a list of rank 3 arrays.\n            prob (float): Override configured probability threshold for\n                predictions.\n            classes (set of str): Override configured class names to consider.\n\n        Returns:\n            Either list of objects detected in the image (single image case) or\n            list of list of objects detected (multiple images case).\n\n            In the multiple images case, the outer list has `number_of_images`\n            elements, while the inner ones have the number of objects detected\n            in each image.\n\n            Each object has the format::\n\n                {\n                    \'bbox\': [x_min, y_min, x_max, y_max],\n                    \'label\': \'<cat|dog|person|...>\' | 0..C,\n                    \'prob\': prob\n                }\n\n            The coordinates are integers, where `(x_min, y_min)` are the\n            coordinates of the top-left corner of the bounding box, while\n            `(x_max, y_max)` the bottom-right. By convention, the top-left\n            corner of the image is coordinate `(0, 0)`.\n\n            The probability, `prob`, is a float between 0 and 1, indicating the\n            confidence of the detection being correct.\n\n            The label of the object, `label`, may be either a string if the\n            classes file for the model is found or an integer between 0 and the\n            number of classes `C`.\n\n        """"""\n        # If it\'s a single image (ndarray of rank 3), turn into a list.\n        single_image = False\n        if not isinstance(images, list):\n            if len(images.shape) == 3:\n                images = [images]\n                single_image = True\n\n        if prob is None:\n            prob = self.prob\n\n        if classes is None:\n            classes = self.classes\n        else:\n            classes = set(classes)\n\n        # TODO: Remove the loop once (and if) we implement batch sizes. Neither\n        # Faster R-CNN nor SSD support batch size yet, so it\'s the same for\n        # now.\n        predictions = []\n        for image in images:\n            predictions.append([\n                pred for pred in self._network.predict_image(image)\n                if pred[\'prob\'] >= prob and pred[\'label\'] in classes\n            ])\n\n        if single_image:\n            predictions = predictions[0]\n\n        return predictions\n'"
luminoth/train.py,32,"b'import click\nimport json\nimport os\nimport sys\nimport tensorflow as tf\nimport time\n\nfrom tensorflow.python import debug as tf_debug\n\nfrom luminoth.datasets import get_dataset\nfrom luminoth.datasets.exceptions import InvalidDataDirectory\nfrom luminoth.models import get_model\nfrom luminoth.utils.config import get_config\nfrom luminoth.utils.hooks import ImageVisHook, VarVisHook\nfrom luminoth.utils.training import get_optimizer, clip_gradients_by_norm\nfrom luminoth.utils.experiments import save_run\n\n\ndef run(config, target=\'\', cluster_spec=None, is_chief=True, job_name=None,\n        task_index=None, get_model_fn=get_model, get_dataset_fn=get_dataset,\n        environment=None):\n    model_class = get_model_fn(config.model.type)\n\n    image_vis = config.train.get(\'image_vis\')\n    var_vis = config.train.get(\'var_vis\')\n\n    if config.train.get(\'seed\') is not None:\n        tf.set_random_seed(config.train.seed)\n\n    log_prefix = \'[{}-{}] - \'.format(job_name, task_index) \\\n        if job_name is not None and task_index is not None else \'\'\n\n    if config.train.debug or config.train.tf_debug:\n        tf.logging.set_verbosity(tf.logging.DEBUG)\n    else:\n        tf.logging.set_verbosity(tf.logging.INFO)\n\n    model = model_class(config)\n\n    # Placement of ops on devices using replica device setter\n    # which automatically places the parameters on the `ps` server\n    # and the `ops` on the workers\n    #\n    # See:\n    # https://www.tensorflow.org/api_docs/python/tf/train/replica_device_setter\n    with tf.device(tf.train.replica_device_setter(cluster=cluster_spec)):\n        try:\n            config[\'dataset\'][\'type\']\n        except KeyError:\n            raise KeyError(\'dataset.type should be set on the custom config.\')\n\n        try:\n            dataset_class = get_dataset_fn(config.dataset.type)\n            dataset = dataset_class(config)\n            train_dataset = dataset()\n        except InvalidDataDirectory as exc:\n            tf.logging.error(\n                ""Error while reading dataset, {}"".format(exc)\n            )\n            sys.exit(1)\n\n        train_image = train_dataset[\'image\']\n        train_filename = train_dataset[\'filename\']\n        train_bboxes = train_dataset[\'bboxes\']\n\n        prediction_dict = model(train_image, train_bboxes, is_training=True)\n        total_loss = model.loss(prediction_dict)\n\n        global_step = tf.train.get_or_create_global_step()\n\n        optimizer = get_optimizer(config.train, global_step)\n\n        # TODO: Is this necesarry? Couldn\'t we just get them from the\n        # trainable vars collection? We should probably improve our\n        # usage of collections.\n        trainable_vars = model.get_trainable_vars()\n\n        # Compute, clip and apply gradients\n        with tf.name_scope(\'gradients\'):\n            grads_and_vars = optimizer.compute_gradients(\n                total_loss, trainable_vars\n            )\n\n            if config.train.clip_by_norm:\n                grads_and_vars = clip_gradients_by_norm(grads_and_vars)\n\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n            train_op = optimizer.apply_gradients(\n                grads_and_vars, global_step=global_step\n            )\n\n        # Create custom init for slots in optimizer, as we don\'t save them to\n        # our checkpoints. An example of slots in an optimizer are the Momentum\n        # variables in MomentumOptimizer. We do this because slot variables can\n        # effectively duplicate the size of your checkpoint!\n        slot_variables = [\n            optimizer.get_slot(var, name)\n            for name in optimizer.get_slot_names()\n            for var in trainable_vars\n        ]\n        slot_init = tf.variables_initializer(\n            slot_variables,\n            name=\'optimizer_slots_initializer\'\n        )\n\n        # Create saver for saving/restoring model\n        model_saver = tf.train.Saver(\n            set(tf.global_variables()) - set(slot_variables),\n            name=\'model_saver\',\n            max_to_keep=config.train.get(\'checkpoints_max_keep\', 1),\n        )\n\n        # Create saver for loading pretrained checkpoint into base network\n        base_checkpoint_vars = model.get_base_network_checkpoint_vars()\n        checkpoint_file = model.get_checkpoint_file()\n        if base_checkpoint_vars and checkpoint_file:\n            base_net_checkpoint_saver = tf.train.Saver(\n                base_checkpoint_vars,\n                name=\'base_net_checkpoint_saver\'\n            )\n\n            # We\'ll send this fn to Scaffold init_fn\n            def load_base_net_checkpoint(_, session):\n                base_net_checkpoint_saver.restore(\n                    session, checkpoint_file\n                )\n        else:\n            load_base_net_checkpoint = None\n\n    tf.logging.info(\'{}Starting training for {}\'.format(log_prefix, model))\n\n    run_options = None\n    if config.train.full_trace:\n        run_options = tf.RunOptions(\n            trace_level=tf.RunOptions.FULL_TRACE\n        )\n\n    # Create custom Scaffold to make sure we run our own init_op when model\n    # is not restored from checkpoint.\n    summary_op = [model.summary]\n    summaries = tf.summary.merge_all()\n    if summaries is not None:\n        summary_op.append(summaries)\n    summary_op = tf.summary.merge(summary_op)\n\n    # `ready_for_local_init_op` is hardcoded to \'ready\' as local init doesn\'t\n    # depend on global init and `local_init_op` only runs when it is set as\n    # \'ready\' (an empty string tensor sets it as ready).\n    scaffold = tf.train.Scaffold(\n        saver=model_saver,\n        init_op=tf.global_variables_initializer() if is_chief else tf.no_op(),\n        local_init_op=tf.group(tf.initialize_local_variables(), slot_init),\n        ready_for_local_init_op=tf.constant([], dtype=tf.string),\n        summary_op=summary_op,\n        init_fn=load_base_net_checkpoint,\n    )\n\n    # Custom hooks for our session\n    hooks = []\n    chief_only_hooks = []\n\n    if config.train.tf_debug:\n        debug_hook = tf_debug.LocalCLIDebugHook()\n        debug_hook.add_tensor_filter(\n            \'has_inf_or_nan\', tf_debug.has_inf_or_nan\n        )\n        hooks.extend([debug_hook])\n\n    if not config.train.job_dir:\n        tf.logging.warning(\n            \'`job_dir` is not defined. Checkpoints and logs will not be saved.\'\n        )\n        checkpoint_dir = None\n    elif config.train.run_name:\n        # Use run_name when available\n        checkpoint_dir = os.path.join(\n            config.train.job_dir, config.train.run_name\n        )\n    else:\n        checkpoint_dir = config.train.job_dir\n\n    should_add_hooks = (\n        config.train.display_every_steps\n        or config.train.display_every_secs\n        and checkpoint_dir is not None\n    )\n    if should_add_hooks:\n        if not config.train.debug and image_vis == \'debug\':\n            tf.logging.warning(\'ImageVisHook will not run without debug mode.\')\n        elif image_vis is not None:\n            # ImageVis only runs on the chief.\n            chief_only_hooks.append(\n                ImageVisHook(\n                    prediction_dict,\n                    image=train_dataset[\'image\'],\n                    gt_bboxes=train_dataset[\'bboxes\'],\n                    config=config.model,\n                    output_dir=checkpoint_dir,\n                    every_n_steps=config.train.display_every_steps,\n                    every_n_secs=config.train.display_every_secs,\n                    image_visualization_mode=image_vis\n                )\n            )\n\n        if var_vis is not None:\n            # VarVis only runs on the chief.\n            chief_only_hooks.append(\n                VarVisHook(\n                    every_n_steps=config.train.display_every_steps,\n                    every_n_secs=config.train.display_every_secs,\n                    mode=var_vis,\n                    output_dir=checkpoint_dir,\n                    vars_summary=model.vars_summary,\n                )\n            )\n\n    step = -1\n    with tf.train.MonitoredTrainingSession(\n        master=target,\n        is_chief=is_chief,\n        checkpoint_dir=checkpoint_dir,\n        scaffold=scaffold,\n        hooks=hooks,\n        chief_only_hooks=chief_only_hooks,\n        save_checkpoint_secs=config.train.save_checkpoint_secs,\n        save_summaries_steps=config.train.save_summaries_steps,\n        save_summaries_secs=config.train.save_summaries_secs,\n    ) as sess:\n\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n        try:\n            while not coord.should_stop():\n                before = time.time()\n                _, train_loss, step, filename = sess.run([\n                    train_op, total_loss, global_step, train_filename\n                ], options=run_options)\n\n                # TODO: Add image summary every once in a while.\n\n                tf.logging.info(\n                    \'{}step: {}, file: {}, train_loss: {}, in {:.2f}s\'.format(\n                        log_prefix, step, filename, train_loss,\n                        time.time() - before\n                    ))\n\n                if is_chief and step == 1:\n                    # We save the run after first batch to make sure everything\n                    # works properly.\n                    save_run(config, environment=environment)\n\n        except tf.errors.OutOfRangeError:\n            tf.logging.info(\n                \'{}finished training after {} epoch limit\'.format(\n                    log_prefix, config.train.num_epochs\n                )\n            )\n\n            # TODO: Print summary\n        finally:\n            coord.request_stop()\n\n        # Wait for all threads to stop.\n        coord.join(threads)\n\n        return step\n\n\n@click.command(help=\'Train models\')\n@click.option(\'config_files\', \'--config\', \'-c\', required=True, multiple=True, help=\'Config to use.\')  # noqa\n@click.option(\'--job-dir\', help=\'Job directory.\')\n@click.option(\'override_params\', \'--override\', \'-o\', multiple=True, help=\'Override model config params.\')  # noqa\ndef train(config_files, job_dir, override_params):\n    """"""\n    Parse TF_CONFIG to cluster_spec and call run() function\n    """"""\n    # TF_CONFIG environment variable is available when running using gcloud\n    # either locally or on cloud. It has all the information required to create\n    # a ClusterSpec which is important for running distributed code.\n    tf_config_val = os.environ.get(\'TF_CONFIG\')\n\n    if tf_config_val:\n        tf_config = json.loads(tf_config_val)\n    else:\n        tf_config = {}\n\n    cluster = tf_config.get(\'cluster\')\n    job_name = tf_config.get(\'task\', {}).get(\'type\')\n    task_index = tf_config.get(\'task\', {}).get(\'index\')\n    environment = tf_config.get(\'environment\', \'local\')\n\n    # Get the user config and the model type from it.\n    try:\n        config = get_config(config_files, override_params=override_params)\n    except KeyError:\n        # Without mode type defined we can\'t use the default config settings.\n        raise KeyError(\'model.type should be set on the custom config.\')\n\n    if job_dir:\n        override_params += (\'train.job_dir={}\'.format(job_dir), )\n\n    # If cluster information is empty or TF_CONFIG is not available, run local\n    if job_name is None or task_index is None:\n        return run(\n            config, environment=environment\n        )\n\n    cluster_spec = tf.train.ClusterSpec(cluster)\n    server = tf.train.Server(\n        cluster_spec, job_name=job_name, task_index=task_index)\n\n    # Wait for incoming connections forever\n    # Worker ships the graph to the ps server\n    # The ps server manages the parameters of the model.\n    if job_name == \'ps\':\n        server.join()\n        return\n    elif job_name in [\'master\', \'worker\']:\n        is_chief = job_name == \'master\'\n        return run(\n            config, target=server.target, cluster_spec=cluster_spec,\n            is_chief=is_chief, job_name=job_name, task_index=task_index,\n            environment=environment\n        )\n\n\nif __name__ == \'__main__\':\n    train()\n'"
luminoth/train_test.py,18,"b'import sonnet as snt\nimport tempfile\nimport tensorflow as tf\n\nfrom easydict import EasyDict\nfrom luminoth.train import run\nfrom luminoth.models import get_model\nfrom luminoth.utils.config import (\n    get_model_config, load_config_files, get_base_config\n)\n\n\nclass MockFasterRCNN(snt.AbstractModule):\n    """"""\n    Mocks Faster RCNN Network\n    """"""\n    def __init__(self, config, name=\'mockfasterrcnn\'):\n        super(MockFasterRCNN, self).__init__(name=name)\n        self._config = config\n\n    def _build(self, image, gt_boxes=None, is_training=False):\n        w = tf.get_variable(\'w\', initializer=[2.5, 3.0], trainable=True)\n        return {\'w\': w}\n\n    def loss(self, pred_dict, return_all=False):\n        return tf.reduce_sum(pred_dict[\'w\'], 0)\n\n    def get_trainable_vars(self):\n        return snt.get_variables_in_module(self)\n\n    def get_base_network_checkpoint_vars(self):\n        return None\n\n    def get_checkpoint_file(self):\n        return None\n\n    @property\n    def summary(self):\n        return tf.summary.scalar(\'dummy\', 1, collections=[\'rcnn\'])\n\n\nclass TrainTest(tf.test.TestCase):\n    """"""\n    Basic test to train module\n    """"""\n    def setUp(self):\n        self.total_epochs = 2\n        self.config = EasyDict({\n            \'model_type\': \'fasterrcnn\',\n            \'dataset_type\': \'\',\n            \'config_files\': (),\n            \'override_params\': [],\n            \'base_network\': {\n                \'download\': False\n            }\n        })\n        tf.reset_default_graph()\n\n    def get_dataset(self, dataset_type):\n        """"""\n        Mocks luminoth.datasets.datasets.get_dataset\n        """"""\n        def dataset_class(arg2):\n            def build():\n                queue_dtypes = [tf.float32, tf.int32, tf.string]\n                queue_names = [\'image\', \'bboxes\', \'filename\']\n\n                queue = tf.FIFOQueue(\n                    capacity=3,\n                    dtypes=queue_dtypes,\n                    names=queue_names,\n                    name=\'fifo_queue\'\n                )\n                filename = tf.cast(\'filename_test\', tf.string)\n                filename = tf.train.limit_epochs([filename], num_epochs=2)\n\n                data = {\n                    \'image\': tf.random_uniform([600, 800, 3], maxval=255),\n                    \'bboxes\': tf.constant([[0, 0, 30, 30, 0]]),\n                    \'filename\': filename\n                }\n                enqueue_ops = [queue.enqueue(data)] * 2\n                tf.train.add_queue_runner(\n                    tf.train.QueueRunner(queue, enqueue_ops))\n\n                return queue.dequeue()\n            return build\n        return dataset_class\n\n    def get_model(self, model_type):\n        """"""\n        Mocks from luminoth.models.get_model\n        """"""\n        return MockFasterRCNN\n\n    def get_config(self, model_type, override_params=None):\n        custom_config = load_config_files(self.config.config_files)\n        model_class = get_model(\'fasterrcnn\')\n        model_base_config = get_base_config(model_class)\n        config = get_model_config(\n            model_base_config, custom_config, override_params\n        )\n\n        config.model.type = model_type\n\n        return config\n\n    def testTrain(self):\n        model_type = \'mockfasterrcnn\'\n\n        override_params = [\n            \'train.num_epochs={}\'.format(self.total_epochs),\n            \'train.job_dir=\',\n        ]\n\n        config = self.get_config(model_type, override_params=override_params)\n\n        # This should not fail\n        run(\n            config, get_dataset_fn=self.get_dataset,\n            get_model_fn=self.get_model\n        )\n\n    def testTrainSave(self):\n        model_type = \'mockfasterrcnn\'\n\n        # Save checkpoints to a temp directory.\n        tmp_job_dir = tempfile.mkdtemp()\n        override_params = [\n            \'train.num_epochs={}\'.format(self.total_epochs),\n            \'train.job_dir={}\'.format(tmp_job_dir),\n            \'train.run_name=test_runname\',\n        ]\n\n        config = self.get_config(model_type, override_params=override_params)\n\n        run(config, get_dataset_fn=self.get_dataset,\n            get_model_fn=self.get_model)\n\n        # Create new graph which will load previously saved checkpoint\n        tf.reset_default_graph()\n        new_session = tf.Session()\n        new_saver = tf.train.import_meta_graph(\n            tmp_job_dir + \'/test_runname/model.ckpt-3.meta\'\n        )\n        new_saver.restore(\n            new_session, tmp_job_dir + \'/test_runname/model.ckpt-3\'\n        )\n\n        # Get tensor from graph and run it in session\n        w_tensor = tf.get_default_graph().get_tensor_by_name(\n            ""mockfasterrcnn/w:0""\n        )\n        w_numpy = new_session.run(w_tensor)\n\n        # Assert we correctly loaded the weight\n        self.assertArrayNear(w_numpy, [2.5, 3.0], err=0.01)\n\n\nif __name__ == \'__main__\':\n    tf.test.main()\n'"
luminoth/vis.py,0,"b'""""""Provides various visualization-specific functions.""""""\nimport numpy as np\nimport sys\n\nfrom PIL import Image, ImageDraw, ImageFont\n\n\ndef get_font():\n    """"""Attempts to retrieve a reasonably-looking TTF font from the system.\n\n    We don\'t make much of an effort, but it\'s what we can reasonably do without\n    incorporating additional dependencies for this task.\n    """"""\n    if sys.platform == \'win32\':\n        font_names = [\'Arial\']\n    elif sys.platform in [\'linux\', \'linux2\']:\n        font_names = [\'DejaVuSans-Bold\', \'DroidSans-Bold\']\n    elif sys.platform == \'darwin\':\n        font_names = [\'Menlo\', \'Helvetica\']\n\n    font = None\n    for font_name in font_names:\n        try:\n            font = ImageFont.truetype(font_name)\n            break\n        except IOError:\n            continue\n\n    return font\n\n\nSYSTEM_FONT = get_font()\n\n\ndef hex_to_rgb(x):\n    """"""Turns a color hex representation into a tuple representation.""""""\n    return tuple([int(x[i:i + 2], 16) for i in (0, 2, 4)])\n\n\ndef build_colormap():\n    """"""Builds a colormap function that maps labels to colors.\n\n    Returns:\n        Function that receives a label and returns a color tuple `(R, G, B)`\n        for said label.\n    """"""\n    # Build the 10-color palette to be used for all classes. The following are\n    # the hex-codes for said colors (taken the default 10-categorical d3 color\n    # palette).\n    palette = (\n        \'1f77b4ff7f0e2ca02cd627289467bd8c564be377c27f7f7fbcbd2217becf\'\n    )\n    colors = [hex_to_rgb(palette[i:i + 6]) for i in range(0, len(palette), 6)]\n\n    seen_labels = {}\n\n    def colormap(label):\n        # If label not yet seen, get the next value in the palette sequence.\n        if label not in seen_labels:\n            seen_labels[label] = colors[len(seen_labels) % len(colors)]\n\n        return seen_labels[label]\n\n    return colormap\n\n\ndef draw_rectangle(draw, coordinates, color, width=1, fill=30):\n    """"""Draw a rectangle with an optional width.""""""\n    # Add alphas to the color so we have a small overlay over the object.\n    fill = color + (fill,)\n    outline = color + (255,)\n\n    # Pillow doesn\'t support width in rectangles, so we must emulate it with a\n    # loop.\n    for i in range(width):\n        coords = [\n            coordinates[0] - i,\n            coordinates[1] - i,\n            coordinates[2] + i,\n            coordinates[3] + i,\n        ]\n\n        # Fill must be drawn only for the first rectangle, or the alphas will\n        # add up.\n        if i == 0:\n            draw.rectangle(coords, fill=fill, outline=outline)\n        else:\n            draw.rectangle(coords, outline=outline)\n\n\ndef draw_label(draw, coords, label, prob, color, scale=1):\n    """"""Draw a box with the label and probability.""""""\n    # Attempt to get a native TTF font. If not, use the default bitmap font.\n    global SYSTEM_FONT\n    if SYSTEM_FONT:\n        label_font = SYSTEM_FONT.font_variant(size=int(round(16 * scale)))\n        prob_font = SYSTEM_FONT.font_variant(size=int(round(12 * scale)))\n    else:\n        label_font = ImageFont.load_default()\n        prob_font = ImageFont.load_default()\n\n    label = str(label)  # `label` may not be a string.\n    prob = \'({:.2f})\'.format(prob)  # Turn `prob` into a string.\n\n    # We want the probability font to be smaller, so we\'ll write the label in\n    # two steps.\n    label_w, label_h = label_font.getsize(label)\n    prob_w, prob_h = prob_font.getsize(prob)\n\n    # Get margins to manually adjust the spacing. The margin goes between each\n    # segment (i.e. margin, label, margin, prob, margin).\n    margin_w, margin_h = label_font.getsize(\'M\')\n    margin_w *= 0.2\n    _, full_line_height = label_font.getsize(\'Mq\')\n\n    # Draw the background first, considering all margins and the full line\n    # height.\n    background_coords = [\n        coords[0],\n        coords[1],\n        coords[0] + label_w + prob_w + 3 * margin_w,\n        coords[1] + full_line_height * 1.15,\n    ]\n    draw.rectangle(background_coords, fill=color + (255,))\n\n    # Then write the two pieces of text.\n    draw.text([\n        coords[0] + margin_w,\n        coords[1],\n    ], label, font=label_font)\n\n    draw.text([\n        coords[0] + label_w + 2 * margin_w,\n        coords[1] + (margin_h - prob_h),\n    ], prob, font=prob_font)\n\n\ndef vis_objects(image, objects, colormap=None, labels=True, scale=1, fill=30):\n    """"""Visualize objects as returned by `Detector`.\n\n    Arguments:\n        image (numpy.ndarray): Image to draw the bounding boxes on.\n        objects (list of dicts or dict): List of objects as returned by a\n            `Detector` instance.\n        colormap (function): Colormap function to use for the objects.\n        labels (boolean): Whether to draw labels.\n        scale (float): Scale factor for the box sizes, which will enlarge or\n            shrink the width of the boxes and the fonts.\n        fill (int): Integer between 0..255 to use as fill for the bounding\n            boxes.\n\n    Returns:\n        A PIL image with the detected objects\' bounding boxes and labels drawn.\n        Can be casted to a `numpy.ndarray` by using `numpy.array` on the\n        returned object.\n    """"""\n    if not isinstance(objects, list):\n        objects = [objects]\n\n    if colormap is None:\n        colormap = build_colormap()\n\n    image = Image.fromarray(image.astype(np.uint8))\n\n    draw = ImageDraw.Draw(image, \'RGBA\')\n    for obj in objects:\n        # TODO: Can we do image resolution-agnostic?\n        color = colormap(obj[\'label\'])\n\n        # Cast `width` to int so it also works in Python 2\n        draw_rectangle(\n            draw, obj[\'bbox\'], color, width=int(round(3 * scale)),\n            fill=fill\n        )\n        if labels:\n            draw_label(\n                draw, obj[\'bbox\'][:2], obj[\'label\'], obj[\'prob\'], color,\n                scale=scale\n            )\n\n    return image\n'"
luminoth/datasets/__init__.py,0,b'from .datasets import get_dataset  # noqa\nfrom .object_detection_dataset import ObjectDetectionDataset  # noqa\n'
luminoth/datasets/base_dataset.py,7,"b'import os\nimport tensorflow as tf\nimport sonnet as snt\n\nfrom luminoth.datasets.exceptions import InvalidDataDirectory\n\n\nclass BaseDataset(snt.AbstractModule):\n    def __init__(self, config, **kwargs):\n        super(BaseDataset, self).__init__(**kwargs)\n        self._dataset_dir = config.dataset.dir\n        self._num_epochs = config.train.num_epochs\n        self._batch_size = config.train.batch_size\n        self._split = config.dataset.split\n        self._random_shuffle = config.train.random_shuffle\n        self._seed = config.train.seed\n\n        self._fixed_resize = (\n            \'fixed_height\' in config.dataset.image_preprocessing and\n            \'fixed_width\' in config.dataset.image_preprocessing\n        )\n        if self._fixed_resize:\n            self._image_fixed_height = (\n                config.dataset.image_preprocessing.fixed_height\n            )\n            self._image_fixed_width = (\n                config.dataset.image_preprocessing.fixed_width\n            )\n\n        self._total_queue_ops = 20\n\n    def _build(self):\n        # Find split file from which we are going to read.\n        split_path = os.path.join(\n            self._dataset_dir, \'{}.tfrecords\'.format(self._split)\n        )\n        if not tf.gfile.Exists(split_path):\n            raise InvalidDataDirectory(\n                \'""{}"" does not exist.\'.format(split_path)\n            )\n        # String input producer allows for a variable number of files to read\n        # from. We just know we have a single file.\n        filename_queue = tf.train.string_input_producer(\n            [split_path], num_epochs=self._num_epochs, seed=self._seed\n        )\n\n        # Define reader to parse records.\n        reader = tf.TFRecordReader()\n        _, raw_record = reader.read(filename_queue)\n\n        values, dtypes, names = self.read_record(raw_record)\n\n        if self._random_shuffle:\n            queue = tf.RandomShuffleQueue(\n                capacity=100,\n                min_after_dequeue=0,\n                dtypes=dtypes,\n                names=names,\n                name=\'tfrecord_random_queue\',\n                seed=self._seed\n            )\n        else:\n            queue = tf.FIFOQueue(\n                capacity=100,\n                dtypes=dtypes,\n                names=names,\n                name=\'tfrecord_fifo_queue\'\n            )\n\n        # Generate queueing ops for QueueRunner.\n        enqueue_ops = [queue.enqueue(values)] * self._total_queue_ops\n        self.queue_runner = tf.train.QueueRunner(queue, enqueue_ops)\n\n        tf.train.add_queue_runner(self.queue_runner)\n\n        return queue.dequeue()\n'"
luminoth/datasets/datasets.py,1,"b'import tensorflow as tf\n\nfrom luminoth.datasets.object_detection_dataset import ObjectDetectionDataset\n\nDATASETS = {\n    \'tfrecord\': ObjectDetectionDataset,\n    \'object_detection\': ObjectDetectionDataset,\n}\n\n\ndef get_dataset(dataset_type):\n    dataset_type = dataset_type.lower()\n    if dataset_type not in DATASETS:\n        raise ValueError(\'""{}"" is not a valid dataset_type\'\n                         .format(dataset_type))\n\n    if dataset_type == \'tfrecord\':\n        tf.logging.warning(\n            \'Dataset `tfrecord` is deprecated. Use `object_detection` instead.\'\n        )\n\n    return DATASETS[dataset_type]\n'"
luminoth/datasets/exceptions.py,0,"b'\nclass InvalidDataDirectory(Exception):\n    """"""\n    Error raised when the chosen intput directory for the dataset is not valid.\n    """"""\n'"
luminoth/datasets/object_detection_dataset.py,30,"b'import tensorflow as tf\n\nfrom luminoth.datasets.base_dataset import BaseDataset\nfrom luminoth.utils.image import (\n    resize_image_fixed, resize_image, flip_image, random_patch, random_resize,\n    random_distortion, expand\n)\n\nDATA_AUGMENTATION_STRATEGIES = {\n    \'flip\': flip_image,\n    \'patch\': random_patch,\n    \'resize\': random_resize,\n    \'distortion\': random_distortion,\n    \'expand\': expand\n}\n\n\nclass ObjectDetectionDataset(BaseDataset):\n    """"""Abstract object detector dataset module.\n\n    This module implements some of the basic functionalities every object\n    detector dataset usually needs.\n\n    Object detection datasets are datasets that have ground-truth information\n    consisting of rectangular bounding boxes.\n\n    Attributes:\n        dataset_dir (str): Base directory of the dataset.\n        num_epochs (int): Number of epochs the dataset should iterate over.\n        batch_size (int): Batch size the module should return.\n        split (str): Split to consume the data from (usually ""train"", ""val"" or\n            ""test"").\n        image_min_size (int): Image minimum size, used for resizing images if\n            needed.\n        image_max_size (int): Image maximum size.\n        random_shuffle (bool): To consume the dataset using random shuffle or\n            to just use a regular FIFO queue.\n    """"""\n\n    CONTEXT_FEATURES = {\n        \'image_raw\': tf.FixedLenFeature([], tf.string),\n        \'filename\': tf.FixedLenFeature([], tf.string),\n        \'width\': tf.FixedLenFeature([], tf.int64),\n        \'height\': tf.FixedLenFeature([], tf.int64),\n        \'depth\': tf.FixedLenFeature([], tf.int64),\n    }\n\n    SEQUENCE_FEATURES = {\n        \'label\': tf.VarLenFeature(tf.int64),\n        \'xmin\': tf.VarLenFeature(tf.int64),\n        \'xmax\': tf.VarLenFeature(tf.int64),\n        \'ymin\': tf.VarLenFeature(tf.int64),\n        \'ymax\': tf.VarLenFeature(tf.int64),\n    }\n\n    def __init__(self, config, name=\'object_detection_dataset\', **kwargs):\n        """"""\n        Save general purpose attributes for Dataset module.\n\n        Args:\n            config: Config object with all the session properties.\n        """"""\n        super(ObjectDetectionDataset, self).__init__(config, **kwargs)\n        self._image_min_size = config.dataset.image_preprocessing.get(\n            \'min_size\')\n        self._image_max_size = config.dataset.image_preprocessing.get(\n            \'max_size\')\n        # In case no keys are defined, default to empty list.\n        self._data_augmentation = config.dataset.data_augmentation or []\n\n    def preprocess(self, image, bboxes=None):\n        """"""Apply transformations to image and bboxes (if available).\n\n        Transformations are applied according to the config values.\n        """"""\n        # Resize images (if needed)\n        image, bboxes, applied_augmentations = self._augment(image, bboxes)\n        image, bboxes, scale_factor = self._resize_image(image, bboxes)\n\n        return image, bboxes, {\n            \'scale_factor\': scale_factor,\n            \'applied_augmentations\': applied_augmentations,\n        }\n\n    def read_record(self, record):\n        """"""Parse record TFRecord into a set a set of values, names and types\n        that can be queued and then read.\n\n        Returns:\n            - queue_values: Dict with tensor values.\n            - queue_names: Names for each tensor.\n            - queue_types: Types for each tensor.\n        """"""\n        # We parse variable length features (bboxes in a image) as sequence\n        # features\n        context_example, sequence_example = tf.parse_single_sequence_example(\n            record,\n            context_features=self.CONTEXT_FEATURES,\n            sequence_features=self.SEQUENCE_FEATURES\n        )\n\n        # Decode image\n        image_raw = tf.image.decode_image(\n            context_example[\'image_raw\'], channels=3\n        )\n\n        image = tf.cast(image_raw, tf.float32)\n\n        height = tf.cast(context_example[\'height\'], tf.int32)\n        width = tf.cast(context_example[\'width\'], tf.int32)\n        image_shape = tf.stack([height, width, 3])\n        image = tf.reshape(image, image_shape)\n\n        label = self._sparse_to_tensor(sequence_example[\'label\'])\n        xmin = self._sparse_to_tensor(sequence_example[\'xmin\'])\n        xmax = self._sparse_to_tensor(sequence_example[\'xmax\'])\n        ymin = self._sparse_to_tensor(sequence_example[\'ymin\'])\n        ymax = self._sparse_to_tensor(sequence_example[\'ymax\'])\n\n        # Stack parsed tensors to define bounding boxes of shape (num_boxes, 5)\n        bboxes = tf.stack([xmin, ymin, xmax, ymax, label], axis=1)\n\n        image, bboxes, preprocessing_details = self.preprocess(image, bboxes)\n\n        filename = tf.cast(context_example[\'filename\'], tf.string)\n\n        # TODO: Send additional metadata through the queue (scale_factor,\n        # applied_augmentations)\n\n        queue_dtypes = [tf.float32, tf.int32, tf.string, tf.float32]\n        queue_names = [\'image\', \'bboxes\', \'filename\', \'scale_factor\']\n        queue_values = {\n            \'image\': image,\n            \'bboxes\': bboxes,\n            \'filename\': filename,\n            \'scale_factor\': preprocessing_details[\'scale_factor\'],\n        }\n\n        return queue_values, queue_dtypes, queue_names\n\n    def _augment(self, image, bboxes=None, default_prob=0.5):\n        """"""Applies different data augmentation techniques.\n\n        Uses the list of data augmentation configurations, each data\n        augmentation technique has a probability assigned to it (or just uses\n        the default value for the dataset).\n\n        Procedures are applied sequentially on top of each other according to\n        the order defined in the config.\n\n        TODO: We need a better way to ensure order using YAML config without\n        ending up with a list of single-key dictionaries.\n\n        Args:\n            image: A Tensor of shape (height, width, 3).\n            bboxes: A Tensor of shape (total_bboxes, 5).\n\n        Returns:\n            image: A Tensor of shape (height, width, 3).\n            bboxes: A Tensor of shape (total_bboxes, 5) of type tf.int32.\n        """"""\n        applied_data_augmentation = []\n        for aug_config in self._data_augmentation:\n            if len(aug_config.keys()) != 1:\n                raise ValueError(\n                    \'Invalid data_augmentation definition: ""{}""\'.format(\n                        aug_config))\n\n            aug_type = list(aug_config.keys())[0]\n            if aug_type not in DATA_AUGMENTATION_STRATEGIES:\n                tf.logging.warning(\n                    \'Invalid data augmentation strategy ""{}"". Ignoring\'.format(\n                        aug_type))\n                continue\n\n            aug_config = aug_config[aug_type]\n            aug_fn = DATA_AUGMENTATION_STRATEGIES[aug_type]\n\n            random_number = tf.random_uniform([], seed=self._seed)\n            prob = tf.to_float(aug_config.pop(\'prob\', default_prob))\n            apply_aug_strategy = tf.less(random_number, prob)\n\n            augmented = aug_fn(image, bboxes, **aug_config)\n\n            image = tf.cond(\n                apply_aug_strategy,\n                lambda: augmented[\'image\'],\n                lambda: image\n            )\n\n            if bboxes is not None:\n                bboxes = tf.cond(\n                    apply_aug_strategy,\n                    lambda: augmented.get(\'bboxes\'),\n                    lambda: bboxes\n                )\n\n            applied_data_augmentation.append({aug_type: apply_aug_strategy})\n\n        return image, bboxes, applied_data_augmentation\n\n    def _resize_image(self, image, bboxes=None):\n        """"""\n        We need to resize image and bounding boxes when the biggest side\n        dimension is bigger than `self._image_max_size` or when the smaller\n        side is smaller than `self._image_min_size`.\n\n        Then, using the ratio we used, we need to properly scale the bounding\n        boxes.\n\n        Args:\n            image: Tensor with image of shape (H, W, 3).\n            bboxes: Tensor with bounding boxes with shape (num_bboxes, 5).\n                where we have (x_min, y_min, x_max, y_max, label) for each one.\n\n        Returns:\n            image: Tensor with scaled image.\n            bboxes: Tensor with scaled (using the same factor as the image)\n                bounding boxes with shape (num_bboxes, 5).\n            scale_factor: Scale factor used to modify the image (1.0 means no\n                change).\n        """"""\n        if self._fixed_resize:\n            resized = resize_image_fixed(\n                image, self._image_fixed_height, self._image_fixed_width,\n                bboxes=bboxes\n            )\n        else:\n            resized = resize_image(\n                image, bboxes=bboxes, min_size=self._image_min_size,\n                max_size=self._image_max_size\n            )\n\n        return resized[\'image\'], resized.get(\'bboxes\'), resized[\'scale_factor\']\n\n    def _sparse_to_tensor(self, sparse_tensor, dtype=tf.int32, axis=[1]):\n        return tf.squeeze(\n            tf.cast(tf.sparse_tensor_to_dense(sparse_tensor), dtype), axis=axis\n        )\n'"
luminoth/datasets/object_detection_dataset_test.py,5,"b'import tensorflow as tf\nimport numpy as np\n\nfrom easydict import EasyDict\n\nfrom luminoth.datasets.object_detection_dataset import ObjectDetectionDataset\n\n\nclass ObjectDetectionDatasetTest(tf.test.TestCase):\n    def setUp(self):\n        self.base_config = EasyDict({\n            \'dataset\': {\n                \'dir\': \'\',\n                \'split\': \'train\',\n                \'image_preprocessing\': {\n                    \'min_size\': 600,\n                    \'max_size\': 1024,\n                },\n                \'data_augmentation\': {},\n            },\n            \'train\': {\n                \'num_epochs\': 1,\n                \'batch_size\': 1,\n                \'random_shuffle\': False,\n                \'seed\': None,\n            }\n        })\n        tf.reset_default_graph()\n\n    def _run_augment(self, augment_config, image, bboxes):\n        self.base_config[\'dataset\'][\'data_augmentation\'] = augment_config\n\n        bboxes_tf = tf.placeholder(tf.int32, shape=bboxes.shape)\n        image_tf = tf.placeholder(tf.int32, shape=image.shape)\n\n        model = ObjectDetectionDataset(self.base_config)\n        image_aug, bboxes_aug, applied_data_augmentation = model._augment(\n            image_tf, bboxes_tf)\n\n        with self.test_session() as sess:\n            image_aug, bboxes_aug, applied_data_augmentation = sess.run(\n                [image_aug, bboxes_aug, applied_data_augmentation], feed_dict={\n                    bboxes_tf: bboxes,\n                    image_tf: image,\n                })\n            return image_aug, bboxes_aug, applied_data_augmentation\n\n    def testSortedAugmentation(self):\n        """"""\n        Tests that the augmentation is applied in order\n        """"""\n        image = np.random.randint(low=0, high=255, size=(600, 800, 3))\n        bboxes = np.array([\n            [10, 10, 26, 28, 1],\n            [10, 10, 20, 22, 1],\n            [10, 11, 20, 21, 1],\n            [19, 30, 31, 33, 1],\n        ])\n        config = [{\'flip\': {\'prob\': 0}}, {\'flip\': {\'prob\': 1}}]\n\n        image_aug, bboxes_aug, aug = self._run_augment(config, image, bboxes)\n        self.assertEqual(aug[0], {\'flip\': False})\n        self.assertEqual(aug[1], {\'flip\': True})\n\n        config = [{\'flip\': {\'prob\': 1}}, {\'flip\': {\'prob\': 0}}]\n\n        image_aug, bboxes_aug, aug = self._run_augment(config, image, bboxes)\n        self.assertEqual(aug[0], {\'flip\': True})\n        self.assertEqual(aug[1], {\'flip\': False})\n\n    def testIdentityAugmentation(self):\n        """"""\n        Tests that to apply flip twice to an image and bboxes returns the same\n        image and bboxes\n        """"""\n        image = np.random.randint(low=0, high=255, size=(600, 800, 3))\n        bboxes = np.array([\n            [10, 10, 26, 28, 1],\n            [19, 30, 31, 33, 1],\n        ])\n        config = [{\'flip\': {\'prob\': 1}}, {\'flip\': {\'prob\': 1}}]\n\n        image_aug, bboxes_aug, aug = self._run_augment(config, image, bboxes)\n        self.assertEqual(aug[0], {\'flip\': True})\n        self.assertEqual(aug[1], {\'flip\': True})\n\n        self.assertAllEqual(image, image_aug)\n        self.assertAllEqual(bboxes, bboxes_aug)\n\n\nif __name__ == \'__main__\':\n    tf.test.main()\n'"
luminoth/models/__init__.py,0,b'from .models import get_model  # noqa\n'
luminoth/models/models.py,0,"b'from luminoth.models.fasterrcnn import FasterRCNN\nfrom luminoth.models.ssd import SSD\n\n\n# TODO: More models :)\nMODELS = {\n    \'fasterrcnn\': FasterRCNN,\n    \'ssd\': SSD\n}\n\n\ndef get_model(model_type):\n    model_type = model_type.lower()\n    if model_type not in MODELS:\n        raise ValueError(\'""{}"" is not a valid model_type\'.format(model_type))\n\n    return MODELS[model_type]\n'"
luminoth/tools/__init__.py,0,b'from .checkpoint import checkpoint  # noqa\nfrom .cloud import cloud  # noqa\nfrom .dataset import dataset  # noqa\nfrom .server import server  # noqa\n'
luminoth/utils/__init__.py,0,b''
luminoth/utils/anchors.py,0,"b'import numpy as np\n\n\ndef generate_anchors_reference(base_size, aspect_ratios, scales):\n    """"""Generate base anchor to be used as reference of generating all anchors.\n\n    Anchors vary only in width and height. Using the base_size and the\n    different ratios we can calculate the wanted widths and heights.\n\n    Scales apply to area of object.\n\n    Args:\n        base_size (int): Base size of the base anchor (square).\n        aspect_ratios: Ratios to use to generate different anchors. The ratio\n            is the value of height / width.\n        scales: Scaling ratios applied to area.\n\n    Returns:\n        anchors: Numpy array with shape (total_aspect_ratios * total_scales, 4)\n            with the corner points of the reference base anchors using the\n            convention (x_min, y_min, x_max, y_max).\n    """"""\n    scales_grid, aspect_ratios_grid = np.meshgrid(scales, aspect_ratios)\n    base_scales = scales_grid.reshape(-1)\n    base_aspect_ratios = aspect_ratios_grid.reshape(-1)\n\n    aspect_ratio_sqrts = np.sqrt(base_aspect_ratios)\n    heights = base_scales * aspect_ratio_sqrts * base_size\n    widths = base_scales / aspect_ratio_sqrts * base_size\n\n    # Center point has the same X, Y value.\n    center_xy = 0\n\n    # Create anchor reference.\n    anchors = np.column_stack([\n        center_xy - (widths - 1) / 2,\n        center_xy - (heights - 1) / 2,\n        center_xy + (widths - 1) / 2,\n        center_xy + (heights - 1) / 2,\n    ])\n\n    real_heights = (anchors[:, 3] - anchors[:, 1]).astype(np.int)\n    real_widths = (anchors[:, 2] - anchors[:, 0]).astype(np.int)\n\n    if (real_widths == 0).any() or (real_heights == 0).any():\n        raise ValueError(\n            \'base_size {} is too small for aspect_ratios and scales.\'.format(\n                base_size\n            )\n        )\n\n    return anchors\n'"
luminoth/utils/anchors_test.py,3,"b""import numpy as np\nimport tensorflow as tf\n\nfrom luminoth.utils.anchors import generate_anchors_reference\n\n\nclass AnchorsTest(tf.test.TestCase):\n    def tearDown(self):\n        tf.reset_default_graph()\n\n    def _get_widths_heights(self, anchor_reference):\n        return np.column_stack((\n            (anchor_reference[:, 2] - anchor_reference[:, 0] + 1),\n            (anchor_reference[:, 3] - anchor_reference[:, 1] + 1)\n        ))\n\n    def testAnchorReference(self):\n        # Test simple case with one aspect ratio and one scale.\n        base_size = 256\n        aspect_ratios = [1.]\n        scales = [1.]\n        anchor_reference = generate_anchors_reference(\n            base_size=base_size,\n            aspect_ratios=aspect_ratios,\n            scales=scales\n        )\n\n        # Should return a single anchor.\n        self.assertEqual(anchor_reference.shape, (1, 4))\n        self.assertAllEqual(\n            anchor_reference[0],\n            [\n                -(base_size - 1) / 2.0, -(base_size - 1) / 2.0,\n                (base_size - 1) / 2.0, (base_size - 1) / 2.0\n            ]\n        )\n\n        # Test with fixed ratio and different scales.\n        scales = np.array([0.5, 1., 2., 4.])\n        anchor_reference = generate_anchors_reference(\n            base_size=base_size,\n            aspect_ratios=aspect_ratios,\n            scales=scales\n        )\n\n        # Check that we have the correct number of anchors.\n        self.assertEqual(anchor_reference.shape, (4, 4))\n        width_heights = self._get_widths_heights(anchor_reference)\n        # Check that anchors are squares (aspect_ratio = [1.0]).\n        self.assertTrue((width_heights[:, 0] == width_heights[:, 1]).all())\n        # Check that widths are consistent with scales times base_size.\n        self.assertAllEqual(width_heights[:, 0], base_size * scales)\n        # Check exact values.\n        self.assertAllEqual(\n            anchor_reference,\n            np.array([\n                [-63.5, -63.5, 63.5, 63.5],\n                [-127.5, -127.5, 127.5, 127.5],\n                [-255.5, -255.5, 255.5, 255.5],\n                [-511.5, -511.5, 511.5, 511.5]\n            ])\n        )\n\n        # Test with different ratios and scales.\n        scales = np.array([0.5, 1., 2.])\n        aspect_ratios = np.array([0.5, 1., 2.])\n        anchor_reference = generate_anchors_reference(\n            base_size=base_size,\n            aspect_ratios=aspect_ratios,\n            scales=scales\n        )\n\n        # Check we have the correct number of anchors.\n        self.assertEqual(\n            anchor_reference.shape, (len(scales) * len(aspect_ratios), 4)\n        )\n\n        width_heights = self._get_widths_heights(anchor_reference)\n\n        # Check ratios of height / widths\n        anchor_ratios = width_heights[:, 1] / width_heights[:, 0]\n        # Check scales (applied to )\n        anchor_scales = np.sqrt(\n            (width_heights[:, 1] * width_heights[:, 0]) / (base_size ** 2)\n        )\n\n        # Test that all ratios are used in the correct order.\n        self.assertAllClose(\n            anchor_ratios, [0.5, 0.5, 0.5, 1., 1., 1., 2., 2., 2.]\n        )\n        # Test that all scales are used in the correct order.\n        self.assertAllClose(\n            anchor_scales, [0.5, 1., 2., 0.5, 1., 2., 0.5, 1., 2.]\n        )\n\n    def testInvalidValues(self):\n        # Should fail because base_size is too small to for that scale and\n        # ratio.\n        base_size = 1\n        aspect_ratios = [0.5]\n        scales = [0.5]\n        try:\n            generate_anchors_reference(\n                base_size=base_size,\n                aspect_ratios=aspect_ratios,\n                scales=scales\n            )\n        except ValueError:\n            return\n\n        self.fail('Should have thrown an exception.')\n\n\nif __name__ == '__main__':\n    tf.test.main()\n"""
luminoth/utils/bbox_overlap.py,11,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef bbox_overlap_tf(bboxes1, bboxes2):\n    """"""Calculate Intersection over Union (IoU) between two sets of bounding\n    boxes.\n\n    Args:\n        bboxes1: shape (total_bboxes1, 4)\n            with x1, y1, x2, y2 point order.\n        bboxes2: shape (total_bboxes2, 4)\n            with x1, y1, x2, y2 point order.\n\n        p1 *-----\n           |     |\n           |_____* p2\n\n    Returns:\n        Tensor with shape (total_bboxes1, total_bboxes2)\n        with the IoU (intersection over union) of bboxes1[i] and bboxes2[j]\n        in [i, j].\n    """"""\n    with tf.name_scope(\'bbox_overlap\'):\n        x11, y11, x12, y12 = tf.split(bboxes1, 4, axis=1)\n        x21, y21, x22, y22 = tf.split(bboxes2, 4, axis=1)\n\n        xI1 = tf.maximum(x11, tf.transpose(x21))\n        yI1 = tf.maximum(y11, tf.transpose(y21))\n\n        xI2 = tf.minimum(x12, tf.transpose(x22))\n        yI2 = tf.minimum(y12, tf.transpose(y22))\n\n        intersection = (\n            tf.maximum(xI2 - xI1 + 1., 0.) *\n            tf.maximum(yI2 - yI1 + 1., 0.)\n        )\n\n        bboxes1_area = (x12 - x11 + 1) * (y12 - y11 + 1)\n        bboxes2_area = (x22 - x21 + 1) * (y22 - y21 + 1)\n\n        union = (bboxes1_area + tf.transpose(bboxes2_area)) - intersection\n\n        iou = tf.maximum(intersection / union, 0)\n\n        return iou\n\n\ndef bbox_overlap(bboxes1, bboxes2):\n    """"""Calculate Intersection of Union between two sets of bounding boxes.\n\n    Intersection over Union (IoU) of two bounding boxes A and B is calculated\n    doing: (A \xe2\x88\xa9 B) / (A \xe2\x88\xaa B).\n\n    Args:\n        bboxes1: numpy array of shape (total_bboxes1, 4).\n        bboxes2: numpy array of shape (total_bboxes2, 4).\n\n    Returns:\n        iou: numpy array of shape (total_bboxes1, total_bboxes1) a matrix with\n            the intersection over union of bboxes1[i] and bboxes2[j] in\n            iou[i][j].\n    """"""\n    xI1 = np.maximum(bboxes1[:, [0]], bboxes2[:, [0]].T)\n    yI1 = np.maximum(bboxes1[:, [1]], bboxes2[:, [1]].T)\n\n    xI2 = np.minimum(bboxes1[:, [2]], bboxes2[:, [2]].T)\n    yI2 = np.minimum(bboxes1[:, [3]], bboxes2[:, [3]].T)\n\n    intersection = (\n        np.maximum(xI2 - xI1 + 1, 0.) *\n        np.maximum(yI2 - yI1 + 1, 0.)\n    )\n\n    bboxes1_area = (\n        (bboxes1[:, [2]] - bboxes1[:, [0]] + 1) *\n        (bboxes1[:, [3]] - bboxes1[:, [1]] + 1)\n    )\n    bboxes2_area = (\n        (bboxes2[:, [2]] - bboxes2[:, [0]] + 1) *\n        (bboxes2[:, [3]] - bboxes2[:, [1]] + 1)\n    )\n\n    # Calculate the union as the sum of areas minus intersection\n    union = (bboxes1_area + bboxes2_area.T) - intersection\n\n    # We start we an empty array of zeros.\n    iou = np.zeros((bboxes1.shape[0], bboxes2.shape[0]))\n\n    # Only divide where the intersection is > 0\n    np.divide(intersection, union, out=iou, where=intersection > 0.)\n    return iou\n'"
luminoth/utils/bbox_overlap_test.py,5,"b'import numpy as np\nimport tensorflow as tf\n\nfrom luminoth.utils.bbox_overlap import bbox_overlap_tf, bbox_overlap\n\n\nclass BBoxOverlapTest(tf.test.TestCase):\n    """"""Tests for bbox_overlap\n    bbox_overlap has a TensorFlow and a Numpy implementation.\n\n    We test both at the same time by getting both values and making sure they\n    are both equal before doing any assertions.\n    """"""\n    def tearDown(self):\n        tf.reset_default_graph()\n\n    def _get_iou(self, bbox1_val, bbox2_val):\n        """"""Get IoU for two sets of bounding boxes.\n\n        It also checks that both implementations return the same before\n        returning.\n\n        Args:\n            bbox1_val: Array of shape (total_bbox1, 4).\n            bbox2_val: Array of shape (total_bbox2, 4).\n\n        Returns:\n            iou: Array of shape (total_bbox1, total_bbox2)\n        """"""\n        bbox1 = tf.placeholder(tf.float32, (None, 4))\n        bbox2 = tf.placeholder(tf.float32, (None, 4))\n        iou = bbox_overlap_tf(bbox1, bbox2)\n\n        with self.test_session() as sess:\n            iou_val_tf = sess.run(iou, feed_dict={\n                bbox1: np.array(bbox1_val),\n                bbox2: np.array(bbox2_val),\n            })\n\n        iou_val_np = bbox_overlap(np.array(bbox1_val), np.array(bbox2_val))\n        self.assertAllClose(iou_val_np, iou_val_tf)\n        return iou_val_tf\n\n    def testNoOverlap(self):\n        # Single box test\n        iou = self._get_iou([[0, 0, 10, 10]], [[11, 11, 20, 20]])\n        self.assertAllEqual(iou, [[0.]])\n\n        # Multiple boxes.\n        iou = self._get_iou(\n            [[0, 0, 10, 10], [5, 5, 10, 10]],\n            [[11, 11, 20, 20], [15, 15, 20, 20]]\n        )\n        self.assertAllEqual(iou, [[0., 0.], [0., 0.]])\n\n    def testAllOverlap(self):\n        # Equal boxes\n        iou = self._get_iou([[0, 0, 10, 10]], [[0, 0, 10, 10]])\n        self.assertAllEqual(iou, [[1.]])\n\n        # Crossed equal boxes.\n        iou = self._get_iou(\n            [[0, 0, 10, 10], [11, 11, 20, 20]],\n            [[0, 0, 10, 10], [11, 11, 20, 20]]\n        )\n        # We should get an identity matrix.\n        self.assertAllEqual(iou, [[1., 0.], [0., 1.]])\n\n    def testInvalidBoxes(self):\n        # Zero area, bbox1 has x_min == x_max\n        iou = self._get_iou([[10, 0, 10, 10]], [[0, 0, 10, 10]])\n        # self.assertAllEqual(iou, [[0.]]) TODO: Fails\n\n        # Negative area, bbox1 has x_min > x_max (only by one)\n        iou = self._get_iou([[10, 0, 9, 10]], [[0, 0, 10, 10]])\n        self.assertAllEqual(iou, [[0.]])\n\n        # Negative area, bbox1 has x_min > x_max\n        iou = self._get_iou([[10, 0, 7, 10]], [[0, 0, 10, 10]])\n        self.assertAllEqual(iou, [[0.]])\n\n        # Negative area in both cases, both boxes equal but negative\n        iou = self._get_iou([[10, 0, 7, 10]], [[10, 0, 7, 10]])\n        self.assertAllEqual(iou, [[0.]])\n\n\nif __name__ == \'__main__\':\n    tf.test.main()\n'"
luminoth/utils/bbox_transform.py,0,"b'import numpy as np\n\n\ndef get_bbox_properties(bboxes):\n    """"""Get bounding boxes width, height and center point.\n\n    Args:\n        bboxes: Numpy array with bounding boxes of shape (total_boxes, 4).\n\n    Returns:\n        widths: Numpy array with the width of each bbox.\n        heights: Numpy array with the height of each bbox.\n        center_x: X-coordinate for center point of each bbox.\n        center_y: Y-coordinate for center point of each bbox.\n    """"""\n    bboxes_widths = bboxes[:, 2] - bboxes[:, 0] + 1.0\n    bboxes_heights = bboxes[:, 3] - bboxes[:, 1] + 1.0\n    bboxes_center_x = bboxes[:, 0] + 0.5 * bboxes_widths\n    bboxes_center_y = bboxes[:, 1] + 0.5 * bboxes_heights\n    return bboxes_widths, bboxes_heights, bboxes_center_x, bboxes_center_y\n\n\ndef encode(proposals, gt_boxes):\n    """"""Encode the different adjustments needed to transform it to its\n    corresponding ground truth box.\n\n    Args:\n        proposals: Numpy array of shape (total_proposals, 4). Having the\n            bbox encoding in the (x_min, y_min, x_max, y_max) order.\n        gt_boxes: Numpy array of shape (total_proposals, 4). With the same\n            bbox encoding.\n\n    Returns:\n        targets: Numpy array of shape (total_proposals, 4) with the different\n            deltas needed to transform the proposal to the gt_boxes. These\n            deltas are with regards to the center, width and height of the\n            two boxes.\n    """"""\n\n    (proposal_widths, proposal_heights,\n     proposal_center_x, proposal_center_y) = get_bbox_properties(proposals)\n    (gt_widths, gt_heights,\n     gt_center_x, gt_center_y) = get_bbox_properties(gt_boxes)\n\n    # We need to apply targets as specified by the paper parametrization\n    # Faster RCNN 3.1.2\n    targets_x = (gt_center_x - proposal_center_x) / proposal_widths\n    targets_y = (gt_center_y - proposal_center_y) / proposal_heights\n    targets_w = np.log(gt_widths / proposal_widths)\n    targets_h = np.log(gt_heights / proposal_heights)\n\n    targets = np.column_stack((targets_x, targets_y, targets_w, targets_h))\n\n    return targets\n\n\ndef decode(bboxes, deltas):\n    """"""\n    Args:\n        boxes: numpy array of bounding boxes of shape: (num_boxes, 4) following\n            the encoding (x_min, y_min, x_max, y_max).\n        deltas: numpy array of bounding box deltas, one for each bounding box.\n            Its shape is (num_boxes, 4), where the deltas are encoded as\n            (dx, dy, dw, dh).\n\n    Returns:\n        bboxes: bounding boxes transformed to (x1, y1, x2, y2) coordinates. It\n            has the same shape as bboxes.\n    """"""\n    widths, heights, ctr_x, ctr_y = get_bbox_properties(bboxes)\n\n    # The dx, dy deltas are relative while the dw, dh deltas are ""log relative""\n    # d[:, x::y] is used for having a `(num_boxes, 1)` shape instead of\n    # `(num_boxes,)`\n\n    # Split deltas columns into flat array\n    dx = deltas[:, 0]\n    dy = deltas[:, 1]\n    dw = deltas[:, 2]\n    dh = deltas[:, 3]\n\n    # We get the center of the real box as center anchor + relative width\n    pred_ctr_x = dx * widths + ctr_x\n    pred_ctr_y = dy * heights + ctr_y\n\n    # New width and height using exp\n    pred_w = np.exp(dw) * widths\n    pred_h = np.exp(dh) * heights\n\n    # Calculate (x_min, y_min, x_max, y_max) and pack them together.\n    pred_boxes = np.column_stack((\n        pred_ctr_x - 0.5 * pred_w,\n        pred_ctr_y - 0.5 * pred_h,\n        pred_ctr_x + 0.5 * pred_w - 1.0,\n        pred_ctr_y + 0.5 * pred_h - 1.0,\n    ))\n\n    return pred_boxes\n\n\ndef clip_points(points, max_val, min_val):\n    return np.maximum(np.minimum(points, max_val), min_val)\n\n\ndef clip_boxes(boxes, image_shape):\n    """"""Clip boxes to image boundaries.\n\n    Args:\n        boxes: A numpy array of bounding boxes.\n        image_shape: Image shape (height, width).\n    """"""\n    max_width = image_shape[1] - 1\n    max_height = image_shape[0] - 1\n    min_width = 0\n    min_height = 0\n\n    boxes[:, 0] = clip_points(boxes[:, 0], max_width, min_width)\n    boxes[:, 1] = clip_points(boxes[:, 1], max_height, min_height)\n    boxes[:, 2] = clip_points(boxes[:, 2], max_width, min_width)\n    boxes[:, 3] = clip_points(boxes[:, 3], max_height, min_height)\n\n    return boxes\n\n\ndef unmap(data, count, inds, fill=0):\n    """"""Unmap a subset of item (data) back to the original set of items (of size\n    count)\n    """"""\n    if len(data.shape) == 1:\n        ret = np.empty((count, ), dtype=np.float32)\n        ret.fill(fill)\n        ret[inds] = data\n    else:\n        ret = np.empty((count, ) + data.shape[1:], dtype=np.float32)\n        ret.fill(fill)\n        ret[inds, :] = data\n    return ret\n'"
luminoth/utils/bbox_transform_test.py,11,"b'import numpy as np\nimport tensorflow as tf\n\nfrom luminoth.utils.bbox_transform import (\n    encode as encode_np, decode as decode_np, clip_boxes as clip_boxes_np\n)\nfrom luminoth.utils.bbox_transform_tf import (\n    encode as encode_tf, decode as decode_tf, clip_boxes as clip_boxes_tf\n)\nfrom luminoth.utils.test.gt_boxes import generate_gt_boxes\n\n\nclass BBoxTransformTest(tf.test.TestCase):\n    def tearDown(self):\n        tf.reset_default_graph()\n\n    def _encode(self, proposals, gt_boxes):\n        """"""\n        Encodes the adjustment from proposals to GT boxes using both the\n        TensorFlow and the Numpy implementation.\n\n        Asserts that both results are equal.\n        """"""\n        proposals_tf = tf.placeholder(tf.float32, shape=proposals.shape)\n        gt_boxes_tf = tf.placeholder(tf.float32, shape=gt_boxes.shape)\n\n        encoded_tf = encode_tf(proposals_tf, gt_boxes_tf)\n        with self.test_session() as sess:\n            encoded_tf_val = sess.run(encoded_tf, feed_dict={\n                proposals_tf: proposals,\n                gt_boxes_tf: gt_boxes,\n            })\n\n        encoded_np = encode_np(proposals, gt_boxes)\n\n        self.assertAllClose(encoded_np, encoded_tf_val)\n        return encoded_np\n\n    def _decode(self, proposals, deltas):\n        """"""\n        Encodes the final boxes from proposals with deltas, using both the\n        TensorFlow and the Numpy implementation.\n\n        Asserts that both results are equal.\n        """"""\n        proposals_tf = tf.placeholder(tf.float32, shape=proposals.shape)\n        deltas_tf = tf.placeholder(tf.float32, shape=deltas.shape)\n\n        decoded_tf = decode_tf(proposals_tf, deltas_tf)\n        with self.test_session() as sess:\n            decoded_tf_val = sess.run(decoded_tf, feed_dict={\n                proposals_tf: proposals,\n                deltas_tf: deltas,\n            })\n\n        decoded_np = decode_np(proposals, deltas)\n\n        self.assertAllClose(decoded_np, decoded_tf_val)\n        return decoded_np\n\n    def _encode_decode(self, proposals, gt_boxes):\n        """"""\n        Encode and decode to check inverse.\n        """"""\n        proposals_tf = tf.placeholder(tf.float32, shape=proposals.shape)\n        gt_boxes_tf = tf.placeholder(tf.float32, shape=gt_boxes.shape)\n        deltas_tf = encode_tf(proposals_tf, gt_boxes_tf)\n        decoded_gt_boxes_tf = decode_tf(proposals_tf, deltas_tf)\n\n        with self.test_session() as sess:\n            decoded_gt_boxes = sess.run(decoded_gt_boxes_tf, feed_dict={\n                proposals_tf: proposals,\n                gt_boxes_tf: gt_boxes,\n            })\n            self.assertAllClose(decoded_gt_boxes, gt_boxes, atol=1e-04)\n\n    def _clip_boxes(self, proposals, image_shape):\n        """"""\n        Clips boxes to image shape using both the TensorFlow and the Numpy\n        implementation.\n\n        Asserts that both results are equal.\n        """"""\n        proposals_tf = tf.placeholder(tf.float32, shape=proposals.shape)\n        image_shape_tf = tf.placeholder(tf.int32, shape=(2,))\n        clipped_tf = clip_boxes_tf(proposals, image_shape_tf)\n        with self.test_session() as sess:\n            clipped_tf_val = sess.run(clipped_tf, feed_dict={\n                proposals_tf: proposals,\n                image_shape_tf: image_shape,\n            })\n\n        clipped_np_val = clip_boxes_np(proposals, image_shape)\n        self.assertAllClose(clipped_np_val, clipped_tf_val)\n        return clipped_np_val\n\n    def testEncodeDecode(self):\n        # 1 vs 1 already equal encode and decode.\n        proposal = generate_gt_boxes(1, image_size=100)\n        gt_boxes = proposal\n\n        deltas = self._encode(proposal, gt_boxes)\n        decoded_gt_boxes = self._decode(proposal, deltas)\n\n        self.assertAllEqual(deltas, np.zeros((1, 4)))\n        self.assertAllClose(gt_boxes, decoded_gt_boxes)\n\n        # 3 vs 3 already equal encode and decode\n        proposal = generate_gt_boxes(3, image_size=100)\n        gt_boxes = proposal\n\n        deltas = self._encode(proposal, gt_boxes)\n        decoded_gt_boxes = self._decode(proposal, deltas)\n\n        self.assertAllEqual(deltas, np.zeros((3, 4)))\n        self.assertAllClose(gt_boxes, decoded_gt_boxes)\n\n        # 3 vs 4 different encode and decode\n        proposal = generate_gt_boxes(3, image_size=100)\n        gt_boxes = generate_gt_boxes(3, image_size=100)\n\n        deltas = self._encode(proposal, gt_boxes)\n        decoded_gt_boxes = self._decode(proposal, deltas)\n\n        self.assertAllEqual(deltas.shape, (3, 4))\n        self.assertAllClose(gt_boxes, decoded_gt_boxes)\n\n    def testClipBboxes(self):\n        image_shape = (50, 60)  # height, width\n        boxes = np.array([\n            [-1, 10, 20, 20],  # x_min is left of the image.\n            [10, -1, 20, 20],  # y_min is above the image.\n            [10, 10, 60, 20],  # x_max is right of the image.\n            [10, 10, 20, 50],  # y_max is below the image.\n            [10, 10, 20, 20],  # everything is in place\n            [60, 50, 60, 50],  # complete box is outside the image.\n        ])\n        clipped_bboxes = self._clip_boxes(boxes, image_shape)\n        self.assertAllEqual(clipped_bboxes, [\n            [0, 10, 20, 20],\n            [10, 0, 20, 20],\n            [10, 10, 59, 20],\n            [10, 10, 20, 49],\n            [10, 10, 20, 20],\n            [59, 49, 59, 49],\n        ])\n\n    def testEncodeDecodeRandomizedValues(self):\n        for i in range(1, 2000, 117):\n            gt_boxes = generate_gt_boxes(i, image_size=800)\n            proposals = generate_gt_boxes(i, image_size=800)\n            self._encode_decode(proposals, gt_boxes)\n\n\nif __name__ == \'__main__\':\n    tf.test.main()\n'"
luminoth/utils/bbox_transform_tf.py,28,"b'import tensorflow as tf\n\n\ndef get_width_upright(bboxes):\n    with tf.name_scope(\'BoundingBoxTransform/get_width_upright\'):\n        bboxes = tf.cast(bboxes, tf.float32)\n        x1, y1, x2, y2 = tf.split(bboxes, 4, axis=1)\n        width = x2 - x1 + 1.\n        height = y2 - y1 + 1.\n\n        # Calculate up right point of bbox (urx = up right x)\n        urx = x1 + .5 * width\n        ury = y1 + .5 * height\n\n        return width, height, urx, ury\n\n\ndef encode(bboxes, gt_boxes, variances=None):\n    with tf.name_scope(\'BoundingBoxTransform/encode\'):\n        (bboxes_width, bboxes_height,\n         bboxes_urx, bboxes_ury) = get_width_upright(bboxes)\n\n        (gt_boxes_width, gt_boxes_height,\n         gt_boxes_urx, gt_boxes_ury) = get_width_upright(gt_boxes)\n\n        if variances is None:\n            variances = [1., 1.]\n\n        targets_dx = (gt_boxes_urx - bboxes_urx)/(bboxes_width * variances[0])\n        targets_dy = (gt_boxes_ury - bboxes_ury)/(bboxes_height * variances[0])\n\n        targets_dw = tf.log(gt_boxes_width / bboxes_width) / variances[1]\n        targets_dh = tf.log(gt_boxes_height / bboxes_height) / variances[1]\n\n        targets = tf.concat(\n            [targets_dx, targets_dy, targets_dw, targets_dh], axis=1)\n\n        return targets\n\n\ndef decode(roi, deltas, variances=None):\n    with tf.name_scope(\'BoundingBoxTransform/decode\'):\n        (roi_width, roi_height,\n         roi_urx, roi_ury) = get_width_upright(roi)\n\n        dx, dy, dw, dh = tf.split(deltas, 4, axis=1)\n\n        if variances is None:\n            variances = [1., 1.]\n\n        pred_ur_x = dx * roi_width * variances[0] + roi_urx\n        pred_ur_y = dy * roi_height * variances[0] + roi_ury\n        pred_w = tf.exp(dw * variances[1]) * roi_width\n        pred_h = tf.exp(dh * variances[1]) * roi_height\n\n        bbox_x1 = pred_ur_x - 0.5 * pred_w\n        bbox_y1 = pred_ur_y - 0.5 * pred_h\n\n        # This -1. extra is different from reference implementation.\n        bbox_x2 = pred_ur_x + 0.5 * pred_w - 1.\n        bbox_y2 = pred_ur_y + 0.5 * pred_h - 1.\n\n        bboxes = tf.concat(\n            [bbox_x1, bbox_y1, bbox_x2, bbox_y2], axis=1)\n\n        return bboxes\n\n\ndef clip_boxes(bboxes, imshape):\n    """"""\n    Clips bounding boxes to image boundaries based on image shape.\n\n    Args:\n        bboxes: Tensor with shape (num_bboxes, 4)\n            where point order is x1, y1, x2, y2.\n\n        imshape: Tensor with shape (2, )\n            where the first value is height and the next is width.\n\n    Returns\n        Tensor with same shape as bboxes but making sure that none\n        of the bboxes are outside the image.\n    """"""\n    with tf.name_scope(\'BoundingBoxTransform/clip_bboxes\'):\n        bboxes = tf.cast(bboxes, dtype=tf.float32)\n        imshape = tf.cast(imshape, dtype=tf.float32)\n\n        x1, y1, x2, y2 = tf.split(bboxes, 4, axis=1)\n        width = imshape[1]\n        height = imshape[0]\n        x1 = tf.maximum(tf.minimum(x1, width - 1.0), 0.0)\n        x2 = tf.maximum(tf.minimum(x2, width - 1.0), 0.0)\n\n        y1 = tf.maximum(tf.minimum(y1, height - 1.0), 0.0)\n        y2 = tf.maximum(tf.minimum(y2, height - 1.0), 0.0)\n\n        bboxes = tf.concat([x1, y1, x2, y2], axis=1)\n\n        return bboxes\n\n\ndef change_order(bboxes):\n    """"""Change bounding box encoding order.\n\n    TensorFlow works with the (y_min, x_min, y_max, x_max) order while we work\n    with the (x_min, y_min, x_max, y_min).\n\n    While both encoding options have its advantages and disadvantages we\n    decided to use the (x_min, y_min, x_max, y_min), forcing use to switch to\n    TensorFlow\'s every time we want to use a std function that handles bounding\n    boxes.\n\n    Args:\n        bboxes: A Tensor of shape (total_bboxes, 4)\n\n    Returns:\n        bboxes: A Tensor of shape (total_bboxes, 4) with the order swaped.\n    """"""\n    with tf.name_scope(\'BoundingBoxTransform/change_order\'):\n        first_min, second_min, first_max, second_max = tf.unstack(\n            bboxes, axis=1\n        )\n        bboxes = tf.stack(\n            [second_min, first_min, second_max, first_max], axis=1\n        )\n        return bboxes\n\n\nif __name__ == \'__main__\':\n    import numpy as np\n\n    bboxes = tf.placeholder(tf.float32)\n    bboxes_val = [[10, 10, 20, 22]]\n\n    gt_boxes = tf.placeholder(tf.float32)\n    gt_boxes_val = [[11, 13, 34, 31]]\n\n    imshape = tf.placeholder(tf.int32)\n    imshape_val = (100, 100)\n\n    deltas = encode(bboxes, gt_boxes)\n    decoded_bboxes = decode(bboxes, deltas)\n    final_decoded_bboxes = clip_boxes(decoded_bboxes, imshape)\n\n    with tf.Session() as sess:\n        final_decoded_bboxes = sess.run(final_decoded_bboxes, feed_dict={\n            bboxes: bboxes_val,\n            gt_boxes: gt_boxes_val,\n            imshape: imshape_val,\n        })\n\n        assert np.all(gt_boxes_val == final_decoded_bboxes)\n'"
luminoth/utils/checkpoint_downloader.py,10,"b'import click\nimport json\nimport os\nimport requests\nimport tarfile\nimport tensorflow as tf\n\nfrom luminoth.utils.homedir import get_luminoth_home\n\n\nTENSORFLOW_OFFICIAL_ENDPOINT = \'http://download.tensorflow.org/models/\'\n\nBASE_NETWORK_FILENAMES = {\n    \'inception_v3\': \'inception_v3_2016_08_28.tar.gz\',\n    \'resnet_v1_50\': \'resnet_v1_50_2016_08_28.tar.gz\',\n    \'resnet_v1_101\': \'resnet_v1_101_2016_08_28.tar.gz\',\n    \'resnet_v1_152\': \'resnet_v1_152_2016_08_28.tar.gz\',\n    \'resnet_v2_50\': \'resnet_v2_50_2017_04_14.tar.gz\',\n    \'resnet_v2_101\': \'resnet_v2_101_2017_04_14.tar.gz\',\n    \'resnet_v2_152\': \'resnet_v2_152_2017_04_14.tar.gz\',\n    \'vgg_16\': \'vgg_16_2016_08_28.tar.gz\',\n    \'truncated_vgg_16\': \'vgg_16_2016_08_28.tar.gz\',\n}\n\n\ndef get_default_path():\n    if \'TF_CONFIG\' in os.environ:\n        tf_config = json.loads(os.environ[\'TF_CONFIG\'])\n        job_dir = tf_config.get(\'job\', {}).get(\'job_dir\')\n        if job_dir:\n            # Instead of using the job_dir we create a folder inside.\n            job_dir = os.path.join(job_dir, \'pretrained_checkpoints/\')\n            return job_dir\n\n    return get_luminoth_home()\n\n\nDEFAULT_PATH = get_default_path()\n\n\ndef get_checkpoint_path(path=DEFAULT_PATH):\n    # Expand user if path is relative to user home.\n    path = os.path.expanduser(path)\n\n    if not path.startswith(\'gs://\'):\n        # We don\'t need to create Google cloud storage ""folders""\n        path = os.path.abspath(path)\n\n    if not tf.gfile.Exists(path):\n        tf.logging.info(\n            \'Creating folder ""{}"" to save checkpoints.\'.format(path))\n        tf.gfile.MakeDirs(path)\n\n    return path\n\n\ndef download_checkpoint(network, network_filename, checkpoint_path,\n                        checkpoint_filename):\n    tarball_filename = BASE_NETWORK_FILENAMES[network]\n    url = TENSORFLOW_OFFICIAL_ENDPOINT + tarball_filename\n    response = requests.get(url, stream=True)\n    total_size = int(response.headers.get(\'Content-Length\'))\n    tarball_path = os.path.join(checkpoint_path, tarball_filename)\n    tmp_tarball = tf.gfile.Open(tarball_path, \'wb\')\n    tf.logging.info(\'Downloading {} checkpoint.\'.format(network_filename))\n    with click.progressbar(length=total_size) as bar:\n        for data in response.iter_content(chunk_size=4096):\n            tmp_tarball.write(data)\n            bar.update(len(data))\n    tmp_tarball.flush()\n\n    tf.logging.info(\'Saving checkpoint to {}\'.format(checkpoint_path))\n    # Open saved tarball as readable binary\n    tmp_tarball = tf.gfile.Open(tarball_path, \'rb\')\n    # Open tarfile object\n    tar_obj = tarfile.open(fileobj=tmp_tarball)\n    # Get checkpoint file name\n    checkpoint_file_name = tar_obj.getnames()[0]\n    # Create buffer with extracted network checkpoint\n    checkpoint_fp = tar_obj.extractfile(checkpoint_file_name)\n    # Define where to save.\n    checkpoint_file = tf.gfile.Open(checkpoint_filename, \'wb\')\n    # Write extracted checkpoint to file\n    checkpoint_file.write(checkpoint_fp.read())\n    checkpoint_file.flush()\n    checkpoint_file.close()\n    tmp_tarball.close()\n    # Remove temp tarball\n    tf.gfile.Remove(tarball_path)\n\n\ndef get_checkpoint_file(network, checkpoint_path=DEFAULT_PATH):\n    if checkpoint_path is None:\n        checkpoint_path = DEFAULT_PATH\n    checkpoint_path = get_checkpoint_path(path=checkpoint_path)\n    files = tf.gfile.ListDirectory(checkpoint_path)\n    network_filename = \'{}.ckpt\'.format(network)\n    checkpoint_file = os.path.join(checkpoint_path, network_filename)\n    if network_filename not in files:\n        download_checkpoint(\n            network, network_filename, checkpoint_path, checkpoint_file\n        )\n\n    return checkpoint_file\n'"
luminoth/utils/config.py,3,"b'import inspect\nimport os.path\nimport tensorflow as tf\nimport yaml\n\nfrom easydict import EasyDict\n\nfrom luminoth.models import get_model\n\n\nREPLACE_KEY = \'_replace\'\n\n\ndef get_config(config_files, override_params=None):\n    custom_config = load_config_files(config_files)\n    model_class = get_model(custom_config[\'model\'][\'type\'])\n    model_base_config = get_base_config(model_class)\n    config = get_model_config(\n        model_base_config, custom_config, override_params\n    )\n\n    return config\n\n\ndef load_config_files(filename_or_filenames, warn_overwrite=True):\n    if (isinstance(filename_or_filenames, list) or\n       isinstance(filename_or_filenames, tuple)):\n        filenames = filename_or_filenames\n    else:\n        filenames = [filename_or_filenames]\n\n    if len(filenames) <= 0:\n        tf.logging.error(""Tried to load 0 config files."")\n\n    config = EasyDict({})\n    for filename in filenames:\n        with tf.gfile.GFile(filename) as f:\n            new_config = EasyDict(yaml.load(f))\n        config = merge_into(\n            new_config,\n            config, overwrite=True, warn_overwrite=warn_overwrite\n        )\n    return config\n\n\ndef to_dict(config):\n    if type(config) is list:\n        return [to_dict(c) for c in config]\n    elif type(config) is EasyDict:\n        return dict([(k, to_dict(v)) for k, v in config.items()])\n    else:\n        return config\n\n\ndef dump_config(config):\n    config = to_dict(config)\n    return yaml.dump(config, default_flow_style=False)\n\n\ndef get_base_config(model_class, base_config_filename=\'base_config.yml\'):\n    path = inspect.getfile(model_class)\n    config_path = os.path.join(os.path.dirname(path), base_config_filename)\n    return load_config_files([config_path])\n\n\ndef is_basestring(value):\n    """"""\n    Checks if value is string in both Python2.7 and Python3+\n    """"""\n    return isinstance(value, (type(u\'\'), str))\n\n\ndef types_compatible(new_config_value, base_config_value):\n    """"""\n    Checks that config value types are compatible.\n    """"""\n    # Allow to overwrite None values (explicit or just missing)\n    if base_config_value is None:\n        return True\n    # Allow overwrite all None and False values.\n    # TODO: reconsider this.\n    if new_config_value is None or new_config_value is False:\n        return True\n\n    # Checking strings is different because in Python2 we could get different\n    # types str vs unicode.\n    if is_basestring(new_config_value) and is_basestring(base_config_value):\n        return True\n\n    return isinstance(new_config_value, type(base_config_value))\n\n\ndef should_replace(new_config, base_config, key):\n    """"""Find out whether we should replace a key when merging.\n    """"""\n    try:\n        base_replace = base_config[key][REPLACE_KEY]\n    except KeyError:\n        base_replace = None\n    try:\n        new_replace = new_config[key][REPLACE_KEY]\n    except KeyError:\n        new_replace = None\n\n    if new_replace:\n        return True\n    elif new_replace is None and base_replace:\n        return True\n\n    return False\n\n\ndef merge_into(new_config, base_config, overwrite=False, warn_overwrite=False):\n    """"""Merge one easy dict into another.\n\n    If `overwrite` is set to true, conflicting keys will get their values from\n    new_config. Else, the value will be taken from base_config.\n    """"""\n    if type(new_config) is not EasyDict:\n        return\n\n    for key, value in new_config.items():\n        # Since we already have the values of base_config we check against them\n        if not types_compatible(value, base_config.get(key)):\n            raise ValueError(\n                \'Incorrect type ""{}"" for key ""{}"". Must be ""{}""\'.format(\n                    type(value), key, type(base_config.get(key))))\n\n        # Recursively merge dicts\n        if isinstance(value, dict):\n            # Sometimes we want to completely replace the original key (i.e.\n            # deleting all keys that aren\'t in new_config).\n            if (should_replace(new_config, base_config, key)):\n                base_config[key] = value\n            else:\n                base_config[key] = merge_into(\n                    new_config[key], base_config.get(key, EasyDict({})),\n                    overwrite=overwrite, warn_overwrite=warn_overwrite\n                )\n        else:\n            if base_config.get(key) is None:\n                base_config[key] = value\n            elif overwrite:\n                base_config[key] = value\n                if warn_overwrite:\n                    tf.logging.warn(\'Overwrote key ""{}""\'.format(key))\n\n    return base_config\n\n\ndef parse_override(override_options):\n    if not override_options:\n        return {}\n\n    override_dict = {}\n    for option in override_options:\n        key_value = option.split(\'=\')\n        if len(key_value) != 2:\n            raise ValueError(\'Invalid override option ""{}""\'.format(option))\n        key, value = key_value\n        nested_keys = key.split(\'.\')\n\n        local_override_dict = override_dict\n        for nested_key in nested_keys[:-1]:\n            if nested_key not in local_override_dict:\n                local_override_dict[nested_key] = {}\n            local_override_dict = local_override_dict[nested_key]\n\n        local_override_dict[nested_keys[-1]] = parse_config_value(value)\n\n    return override_dict\n\n\ndef parse_config_value(value):\n    """"""\n    Try to parse the config value to boolean, integer, float or string.\n    We assume all values are strings.\n    """"""\n    if value.lower() == \'none\':\n        return None\n    elif value.lower() == \'true\':\n        return True\n    elif value.lower() == \'false\':\n        return False\n\n    try:\n        return int(value)\n    except ValueError:\n        pass\n\n    try:\n        return float(value)\n    except ValueError:\n        pass\n\n    return value\n\n\ndef cleanup_config(config):\n    """"""Delete meta-keys from the config file.\n    """"""\n    cleanup_keys = [REPLACE_KEY]\n    for cleanup_key in cleanup_keys:\n        config.pop(cleanup_key, None)\n\n    for config_key in config:\n        if isinstance(config[config_key], dict):\n            cleanup_config(config[config_key])\n\n    return config\n\n\ndef get_model_config(base_config, custom_config, override_params):\n    config = EasyDict(base_config.copy())\n\n    if custom_config:\n        # If we have a custom config file overwriting default settings\n        # then we merge those values to the base_config.\n        config = merge_into(custom_config, config, overwrite=True)\n    if override_params:\n        override_config = EasyDict(parse_override(override_params))\n        config = merge_into(override_config, config, overwrite=True)\n\n    # Delete meta-keys before returning.\n    return cleanup_config(config)\n\n\ndef override_config_params(config, params):\n    """"""Overrides `config` with `params` and returns it.""""""\n    override_config = EasyDict(parse_override(params))\n    config = merge_into(override_config, config, overwrite=True)\n    return config\n'"
luminoth/utils/dataset.py,8,"b""import tensorflow as tf\n\nfrom lxml import etree\n\n\ndef node2dict(root):\n    if root.getchildren():\n        val = {}\n        for node in root.getchildren():\n            chkey, chval = node2dict(node)\n            val[chkey] = chval\n    else:\n        val = root.text\n\n    return root.tag, val\n\n\ndef read_xml(path):\n    with tf.gfile.GFile(path) as f:\n        root = etree.fromstring(f.read())\n\n    annotations = {}\n    for node in root.getchildren():\n        key, val = node2dict(node)\n        # If `key` is object, it's actually a list.\n        if key == 'object':\n            annotations.setdefault(key, []).append(val)\n        else:\n            annotations[key] = val\n\n    return annotations\n\n\ndef read_image(path):\n    with tf.gfile.GFile(path, 'rb') as f:\n        image = f.read()\n    return image\n\n\ndef to_int64(value):\n    value = [int(value)] if not isinstance(value, list) else value\n    return tf.train.Feature(\n        int64_list=tf.train.Int64List(value=value)\n    )\n\n\ndef to_bytes(value):\n    value = [value] if not isinstance(value, list) else value\n    return tf.train.Feature(\n        bytes_list=tf.train.BytesList(value=value)\n    )\n\n\ndef to_string(value):\n    value = [value] if not isinstance(value, list) else value\n    value = [v.encode('utf-8') for v in value]\n    return tf.train.Feature(\n        bytes_list=tf.train.BytesList(value=value)\n    )\n"""
luminoth/utils/debug.py,2,"b'import tensorflow as tf\n\n# flake8: noqa\n\n\ndef debug(*args, **kwargs):\n    def call_ipdb(*args, **kwargs):\n        print(args)\n        print(kwargs)\n        import ipdb; ipdb.set_trace()\n        return 0\n\n    return tf.py_func(call_ipdb,\n        [list(args) + list(kwargs.values())],\n        tf.int32\n    )'"
luminoth/utils/experiments.py,2,"b""import datetime\nimport json\nimport os.path\nimport subprocess\nimport tensorflow as tf\n\nfrom luminoth.utils.homedir import get_luminoth_home\n\n\nDEFAULT_FILENAME = 'runs.json'\nCURRENT_DIR = os.path.dirname(os.path.realpath(__file__))\n\n\ndef get_diff():\n    try:\n        return subprocess.check_output(\n            ['git', 'diff'], cwd=CURRENT_DIR\n        ).strip().decode('utf-8')\n    except:  # noqa\n        # Never fail, we don't care about the error.\n        return None\n\n\ndef get_luminoth_version():\n    try:\n        return subprocess.check_output(\n            ['git', 'rev-parse', 'HEAD'], cwd=CURRENT_DIR\n        ).strip().decode('utf-8')\n    except:  # noqa\n        # Never fail, we don't care about the error.\n        pass\n\n    try:\n        from luminoth import __version__ as lumi_version\n        return lumi_version\n    except ImportError:\n        pass\n\n\ndef get_tensorflow_version():\n    try:\n        from tensorflow import __version__ as tf_version\n        return tf_version\n    except ImportError:\n        pass\n\n\ndef save_run(config, environment=None, comment=None, extra_config=None,\n             filename=DEFAULT_FILENAME):\n    if environment == 'cloud':\n        # We don't write runs inside Google Cloud, we run it before.\n        return\n\n    diff = get_diff()\n    lumi_version = get_luminoth_version()\n    tf_version = get_tensorflow_version()\n\n    experiment = {\n        'environment': environment,\n        'datetime': str(datetime.datetime.utcnow()) + 'Z',\n        'diff': diff,\n        'luminoth_version': lumi_version,\n        'tensorflow_version': tf_version,\n        'config': config,\n        'extra_config': extra_config,\n    }\n\n    path = get_luminoth_home()\n    file_path = os.path.join(path, filename)\n    tf.gfile.MakeDirs(path)\n\n    with tf.gfile.Open(file_path, 'a') as log:\n        log.write(json.dumps(experiment) + '\\n')\n"""
luminoth/utils/homedir.py,1,"b'""""""Luminoth home (~/.luminoth) management utilities.""""""\nimport os\nimport tensorflow as tf\n\n\nDEFAULT_LUMINOTH_HOME = os.path.expanduser(\'~/.luminoth\')\n\n\ndef get_luminoth_home(create_if_missing=True):\n    """"""Returns Luminoth\'s homedir.""""""\n    # Get Luminoth\'s home directory (the default one or the overridden).\n    path = os.path.abspath(\n        os.environ.get(\'LUMI_HOME\', DEFAULT_LUMINOTH_HOME)\n    )\n\n    # Create the directory if it doesn\'t exist.\n    if create_if_missing and not os.path.exists(path):\n        tf.gfile.MakeDirs(path)\n\n    return path\n'"
luminoth/utils/image.py,111,"b'import tensorflow as tf\n\nfrom luminoth.utils.bbox_transform_tf import clip_boxes\n\n\ndef adjust_bboxes(bboxes, old_height, old_width, new_height, new_width):\n    """"""Adjusts the bboxes of an image that has been resized.\n\n    Args:\n        bboxes: Tensor with shape (num_bboxes, 5). Last element is the label.\n        old_height: Float. Height of the original image.\n        old_width: Float. Width of the original image.\n        new_height: Float. Height of the image after resizing.\n        new_width: Float. Width of the image after resizing.\n    Returns:\n        Tensor with shape (num_bboxes, 5), with the adjusted bboxes.\n    """"""\n    # We normalize bounding boxes points.\n    bboxes_float = tf.to_float(bboxes)\n    x_min, y_min, x_max, y_max, label = tf.unstack(bboxes_float, axis=1)\n\n    x_min = x_min / old_width\n    y_min = y_min / old_height\n    x_max = x_max / old_width\n    y_max = y_max / old_height\n\n    # Use new size to scale back the bboxes points to absolute values.\n    x_min = tf.to_int32(x_min * new_width)\n    y_min = tf.to_int32(y_min * new_height)\n    x_max = tf.to_int32(x_max * new_width)\n    y_max = tf.to_int32(y_max * new_height)\n    label = tf.to_int32(label)  # Cast back to int.\n\n    # Concat points and label to return a [num_bboxes, 5] tensor.\n    return tf.stack([x_min, y_min, x_max, y_max, label], axis=1)\n\n\ndef resize_image(image, bboxes=None, min_size=None, max_size=None):\n    """"""\n    We need to resize image and (optionally) bounding boxes when the biggest\n    side dimension is bigger than `max_size` or when the smaller side is\n    smaller than `min_size`. If no max_size defined it won\'t scale down and if\n    no min_size defined it won\'t scale up.\n\n    Then, using the ratio we used, we need to properly scale the bounding\n    boxes.\n\n    Args:\n        image: Tensor with image of shape (H, W, 3).\n        bboxes: Optional Tensor with bounding boxes with shape (num_bboxes, 5).\n            where we have (x_min, y_min, x_max, y_max, label) for each one.\n        min_size: Min size of width or height.\n        max_size: Max size of width or height.\n\n    Returns:\n        Dictionary containing:\n            image: Tensor with scaled image.\n            bboxes: Tensor with scaled (using the same factor as the image)\n                bounding boxes with shape (num_bboxes, 5).\n            scale_factor: Scale factor used to modify the image (1.0 means no\n                change).\n    """"""\n    image_shape = tf.to_float(tf.shape(image))\n    height = image_shape[0]\n    width = image_shape[1]\n\n    if min_size is not None:\n        # We calculate the upscale factor, the rate we need to use to end up\n        # with an image with it\'s lowest dimension at least `image_min_size`.\n        # In case of being big enough the scale factor is 1. (no change)\n        min_size = tf.to_float(min_size)\n        min_dimension = tf.minimum(height, width)\n        upscale_factor = tf.maximum(min_size / min_dimension, 1.)\n    else:\n        upscale_factor = tf.constant(1.)\n\n    if max_size is not None:\n        # We do the same calculating the downscale factor, to end up with an\n        # image where the biggest dimension is less than `image_max_size`.\n        # When the image is small enough the scale factor is 1. (no change)\n        max_size = tf.to_float(max_size)\n        max_dimension = tf.maximum(height, width)\n        downscale_factor = tf.minimum(max_size / max_dimension, 1.)\n    else:\n        downscale_factor = tf.constant(1.)\n\n    scale_factor = upscale_factor * downscale_factor\n\n    # New size is calculate using the scale factor and rounding to int.\n    new_height = height * scale_factor\n    new_width = width * scale_factor\n\n    # Resize image using TensorFlow\'s own `resize_image` utility.\n    image = tf.image.resize_images(\n        image, tf.stack(tf.to_int32([new_height, new_width])),\n        method=tf.image.ResizeMethod.BILINEAR\n    )\n\n    if bboxes is not None:\n        bboxes = adjust_bboxes(\n            bboxes,\n            old_height=height, old_width=width,\n            new_height=new_height, new_width=new_width\n        )\n        return {\n            \'image\': image,\n            \'bboxes\': bboxes,\n            \'scale_factor\': scale_factor,\n        }\n\n    return {\n        \'image\': image,\n        \'scale_factor\': scale_factor,\n    }\n\n\ndef resize_image_fixed(image, new_height, new_width, bboxes=None):\n\n    image_shape = tf.to_float(tf.shape(image))\n    height = image_shape[0]\n    width = image_shape[1]\n\n    scale_factor_height = new_height / height\n    scale_factor_width = new_width / width\n\n    # Resize image using TensorFlow\'s own `resize_image` utility.\n    image = tf.image.resize_images(\n        image, tf.stack(tf.to_int32([new_height, new_width])),\n        method=tf.image.ResizeMethod.BILINEAR\n    )\n\n    if bboxes is not None:\n        bboxes = adjust_bboxes(\n            bboxes,\n            old_height=height, old_width=width,\n            new_height=new_height, new_width=new_width\n        )\n        return {\n            \'image\': image,\n            \'bboxes\': bboxes,\n            \'scale_factor\': (scale_factor_height, scale_factor_width),\n        }\n\n    return {\n        \'image\': image,\n        \'scale_factor\': (scale_factor_height, scale_factor_width),\n    }\n\n\ndef patch_image(image, bboxes=None, offset_height=0, offset_width=0,\n                target_height=None, target_width=None):\n    """"""Gets a patch using tf.image.crop_to_bounding_box and adjusts bboxes\n\n    If patching would leave us with zero bboxes, we return the image and bboxes\n    unchanged.\n\n    Args:\n        image: Float32 Tensor with shape (H, W, 3).\n        bboxes: Tensor with the ground-truth boxes. Shaped (total_boxes, 5).\n            The last element in each box is the category label.\n        offset_height: Height of the upper-left corner of the patch with\n            respect to the original image. Non-negative.\n        offset_width: Width of the upper-left corner of the patch with respect\n            to the original image. Non-negative.\n        target_height: Height of the patch. If set to none, it will be the\n            maximum (tf.shape(image)[0] - offset_height - 1). Positive.\n        target_width: Width of the patch. If set to none, it will be the\n            maximum (tf.shape(image)[1] - offset_width - 1). Positive.\n\n    Returns:\n        image: Patch of the original image.\n        bboxes: Adjusted bboxes (only those whose centers are inside the\n            patch). The key isn\'t set if bboxes is None.\n    """"""\n    # TODO: make this function safe with respect to senseless inputs (i.e\n    # having an offset_height that\'s larger than tf.shape(image)[0], etc.)\n    # As of now we only use it inside random_patch, which already makes sure\n    # the arguments are legal.\n    im_shape = tf.shape(image)\n    if target_height is None:\n        target_height = (im_shape[0] - offset_height - 1)\n    if target_width is None:\n        target_width = (im_shape[1] - offset_width - 1)\n\n    new_image = tf.image.crop_to_bounding_box(\n        image,\n        offset_height=offset_height, offset_width=offset_width,\n        target_height=target_height, target_width=target_width\n    )\n    patch_shape = tf.shape(new_image)\n\n    # Return if we didn\'t have bboxes.\n    if bboxes is None:\n        # Resize the patch to the original image\'s size. This is to make sure\n        # we respect restrictions in image size in the models.\n        new_image_resized = tf.image.resize_images(\n            new_image, im_shape[:2],\n            method=tf.image.ResizeMethod.BILINEAR\n        )\n        return_dict = {\'image\': new_image_resized}\n        return return_dict\n\n    # Now we will remove all bboxes whose centers are not inside the cropped\n    # image.\n\n    # First get the x  and y coordinates of the center of each of the\n    # bboxes.\n    bboxes_center_x = tf.reduce_mean(\n        tf.concat(\n            [\n                # bboxes[:, 0] gets a Tensor with shape (20,).\n                # We do this to get a Tensor with shape (20, 1).\n                bboxes[:, 0:1],\n                bboxes[:, 2:3]\n            ],\n            axis=1\n        )\n    )\n    bboxes_center_y = tf.reduce_mean(\n        tf.concat(\n            [\n                bboxes[:, 1:2],\n                bboxes[:, 3:4]\n            ],\n            axis=1\n        ),\n        axis=1\n    )\n\n    # Now we get a boolean tensor holding for each of the bboxes\' centers\n    # wheter they are inside the patch.\n    center_x_is_inside = tf.logical_and(\n        tf.greater(\n            bboxes_center_x,\n            offset_width\n        ),\n        tf.less(\n            bboxes_center_x,\n            tf.add(target_width, offset_width)\n        )\n    )\n    center_y_is_inside = tf.logical_and(\n        tf.greater(\n            bboxes_center_y,\n            offset_height\n        ),\n        tf.less(\n            bboxes_center_y,\n            tf.add(target_height, offset_height)\n        )\n    )\n    center_is_inside = tf.logical_and(\n        center_x_is_inside,\n        center_y_is_inside\n    )\n\n    # Now we mask the bboxes, removing all those whose centers are outside\n    # the patch.\n    masked_bboxes = tf.boolean_mask(bboxes, center_is_inside)\n    # We move the bboxes to the right place, clipping them if\n    # necessary.\n    new_bboxes_unclipped = tf.concat(\n        [\n            tf.subtract(masked_bboxes[:, 0:1], offset_width),\n            tf.subtract(masked_bboxes[:, 1:2], offset_height),\n            tf.subtract(masked_bboxes[:, 2:3], offset_width),\n            tf.subtract(masked_bboxes[:, 3:4], offset_height),\n        ],\n        axis=1,\n    )\n    # Finally, we clip the boxes and add back the labels.\n    new_bboxes = tf.concat(\n        [\n            tf.to_int32(\n                clip_boxes(\n                    new_bboxes_unclipped,\n                    imshape=patch_shape[:2]\n                ),\n            ),\n            masked_bboxes[:, 4:]\n        ],\n        axis=1\n    )\n    # Now resize the image to the original size and adjust bboxes accordingly\n    new_image_resized = tf.image.resize_images(\n        new_image, im_shape[:2],\n        method=tf.image.ResizeMethod.BILINEAR\n    )\n    # adjust_bboxes requires height and width values with dtype=float32\n    new_bboxes_resized = adjust_bboxes(\n        new_bboxes,\n        old_height=tf.to_float(patch_shape[0]),\n        old_width=tf.to_float(patch_shape[1]),\n        new_height=tf.to_float(im_shape[0]),\n        new_width=tf.to_float(im_shape[1])\n    )\n\n    # Finally, set up the return dict, but only update the image and bboxes if\n    # our patch has at least one bbox in it.\n    update_condition = tf.greater_equal(\n        tf.shape(new_bboxes_resized)[0],\n        1\n    )\n    return_dict = {}\n    return_dict[\'image\'] = tf.cond(\n        update_condition,\n        lambda: new_image_resized,\n        lambda: image\n    )\n    return_dict[\'bboxes\'] = tf.cond(\n        update_condition,\n        lambda: new_bboxes_resized,\n        lambda: bboxes\n    )\n    return return_dict\n\n\ndef flip_image(image, bboxes=None, left_right=True, up_down=False):\n    """"""Flips image on its axis for data augmentation.\n\n    Args:\n        image: Tensor with image of shape (H, W, 3).\n        bboxes: Optional Tensor with bounding boxes with shape\n            (total_bboxes, 5).\n        left_right: Boolean flag to flip the image horizontally\n            (left to right).\n        up_down: Boolean flag to flip the image vertically (upside down)\n    Returns:\n        image: Flipped image with the same shape.\n        bboxes: Tensor with the same shape.\n    """"""\n    image_shape = tf.shape(image)\n    height = image_shape[0]\n    width = image_shape[1]\n\n    if bboxes is not None:\n        # bboxes usually come from dataset as ints, but just in case we are\n        # using flip for preprocessing, where bboxes usually are represented as\n        # floats, we cast them.\n        bboxes = tf.to_int32(bboxes)\n\n    if left_right:\n        image = tf.image.flip_left_right(image)\n        if bboxes is not None:\n            x_min, y_min, x_max, y_max, label = tf.unstack(bboxes, axis=1)\n            new_x_min = width - x_max - 1\n            new_y_min = y_min\n            new_x_max = new_x_min + (x_max - x_min)\n            new_y_max = y_max\n            bboxes = tf.stack(\n                [new_x_min, new_y_min, new_x_max, new_y_max, label], axis=1\n            )\n\n    if up_down:\n        image = tf.image.flip_up_down(image)\n        if bboxes is not None:\n            x_min, y_min, x_max, y_max, label = tf.unstack(bboxes, axis=1)\n            new_x_min = x_min\n            new_y_min = height - y_max - 1\n            new_x_max = x_max\n            new_y_max = new_y_min + (y_max - y_min)\n            bboxes = tf.stack(\n                [new_x_min, new_y_min, new_x_max, new_y_max, label], axis=1\n            )\n\n    return_dict = {\'image\': image}\n    if bboxes is not None:\n        return_dict[\'bboxes\'] = bboxes\n\n    return return_dict\n\n\ndef random_patch(image, bboxes=None, min_height=600, min_width=600,\n                 seed=None):\n    """"""Gets a random patch from an image.\n\n    min_height and min_width values will be normalized if they are not possible\n    given the input image\'s shape. See also patch_image.\n\n    Args:\n        image: Tensor with shape (H, W, 3).\n        bboxes: Tensor with the ground-truth boxes. Shaped (total_boxes, 5).\n            The last element in each box is the category label.\n        min_height: Minimum height of the patch.\n        min_width: Minimum width of the patch.\n        seed: Seed to be used in randomizing functions.\n\n    Returns:\n        image: Tensor with shape (H\', W\', 3), with H\' <= H and W\' <= W. A\n            random patch of the input image.\n        bboxes: Tensor with shape (new_total_boxes, 5), where we keep\n            bboxes that have their center inside the patch, cropping\n            them to the patch boundaries. If we didn\'t get any bboxes, the\n            return dict will not have the \'bboxes\' key defined.\n    """"""\n    # Start by normalizing the arguments.\n    # Our patch can\'t be larger than the original image.\n    im_shape = tf.shape(image)\n    min_height = tf.minimum(min_height, im_shape[0] - 1)\n    min_width = tf.minimum(min_width, im_shape[1] - 1)\n\n    # Now get the patch using tf.image.crop_to_bounding_box.\n    # See the documentation on tf.image.crop_to_bounding_box or the explanation\n    # in patch_image for the meaning of these variables.\n    offset_width = tf.random_uniform(\n        shape=[],\n        minval=0,\n        maxval=tf.subtract(\n            im_shape[1],\n            min_width\n        ),\n        dtype=tf.int32,\n        seed=seed\n    )\n    offset_height = tf.random_uniform(\n        shape=[],\n        minval=0,\n        maxval=tf.subtract(\n            im_shape[0],\n            min_height\n        ),\n        dtype=tf.int32,\n        seed=seed\n    )\n    target_width = tf.random_uniform(\n        shape=[],\n        minval=min_width,\n        maxval=tf.subtract(\n            im_shape[1],\n            offset_width\n        ),\n        dtype=tf.int32,\n        seed=seed\n    )\n    target_height = tf.random_uniform(\n        shape=[],\n        minval=min_height,\n        maxval=tf.subtract(\n            im_shape[0],\n            offset_height\n        ),\n        dtype=tf.int32,\n        seed=seed\n    )\n    return patch_image(\n        image, bboxes=bboxes,\n        offset_height=offset_height, offset_width=offset_width,\n        target_height=target_height, target_width=target_width\n    )\n\n\ndef random_resize(image, bboxes=None, min_size=600, max_size=980,\n                  seed=None):\n    """"""Randomly resizes an image within limits.\n\n    Args:\n        image: Tensor with shape (H, W, 3)\n        bboxes: Tensor with the ground-truth boxes. Shaped (total_boxes, 5).\n            The last element in each box is the category label.\n        min_size: minimum side-size of the resized image.\n        max_size: maximum side-size of the resized image.\n        seed: Seed to be used in randomizing functions.\n\n    Returns:\n        image: Tensor with shape (H\', W\', 3), satisfying the following.\n            min_size <= H\' <= H\n            min_size <= W\' <= W\n        bboxes: Tensor with the same shape as the input bboxes, if we had them.\n            Else, this key will not be set.\n    """"""\n    im_shape = tf.to_float(tf.shape(image))\n    new_size = tf.random_uniform(\n        shape=[2],\n        minval=min_size,\n        maxval=max_size,\n        dtype=tf.int32,\n        seed=seed,\n    )\n    image = tf.image.resize_images(\n        image, new_size,\n        method=tf.image.ResizeMethod.BILINEAR\n    )\n    # Our returned dict needs to have a fixed size. So we can\'t\n    # return the scale_factor that resize_image returns.\n    if bboxes is not None:\n        new_size = tf.to_float(new_size)\n        bboxes = adjust_bboxes(\n            bboxes,\n            old_height=im_shape[0], old_width=im_shape[1],\n            new_height=new_size[0], new_width=new_size[1]\n        )\n        return_dict = {\n            \'image\': image,\n            \'bboxes\': bboxes\n        }\n    else:\n        return_dict = {\'image\': image}\n    return return_dict\n\n\ndef random_distortion(image, bboxes=None, brightness=None, contrast=None,\n                      hue=None, saturation=None, seed=None):\n    """"""Photometrically distorts an image.\n\n    This includes changing the brightness, contrast and hue.\n\n    Args:\n        image: Tensor with shape (H, W, 3)\n        brightness:\n            max_delta: non-negative float\n        contrast:\n            lower: non-negative float\n            upper: non-negative float\n        hue:\n            max_delta: float in [0, 0.5]\n        saturation:\n            lower: non-negative float\n            upper: non-negative float\n        seed: Seed to be used in randomizing functions.\n\n    Returns:\n        image: Distorted image with the same shape as the input image.\n        bboxes: Unchanged bboxes.\n    """"""\n    # Following Andrew Howard (2013). ""Some improvements on deep convolutional\n    # neural network based image classification.""\n    if brightness is not None:\n        if \'max_delta\' not in brightness:\n            brightness.max_delta = 0.3\n        image = tf.image.random_brightness(\n            image, max_delta=brightness.max_delta, seed=seed\n        )\n    # Changing contrast, even with parameters close to 1, can lead to\n    # excessively distorted images. Use with care.\n    if contrast is not None:\n        if \'lower\' not in contrast:\n            contrast.lower = 0.8\n        if \'upper\' not in contrast:\n            contrast.upper = 1.2\n        image = tf.image.random_contrast(\n            image, lower=contrast.lower, upper=contrast.upper,\n            seed=seed\n        )\n    if hue is not None:\n        if \'max_delta\' not in hue:\n            hue.max_delta = 0.2\n        image = tf.image.random_hue(\n            image, max_delta=hue.max_delta, seed=seed\n        )\n    if saturation is not None:\n        if \'lower\' not in saturation:\n            saturation.lower = 0.8\n        if \'upper\' not in saturation:\n            saturation.upper = 1.2\n        image = tf.image.random_saturation(\n            image, lower=saturation.lower, upper=saturation.upper,\n            seed=seed\n        )\n    if bboxes is None:\n        return_dict = {\'image\': image}\n    else:\n        return_dict = {\n            \'image\': image,\n            \'bboxes\': bboxes,\n        }\n    return return_dict\n\n\ndef expand(image, bboxes=None, fill=0, min_ratio=1, max_ratio=4, seed=None):\n    """"""\n    Increases the image size by adding large padding around the image\n\n    Acts as a zoom out of the image, and when the image is later resized to\n    the input size the network expects, it provides smaller size object\n    examples.\n\n    Args:\n        image: Tensor with image of shape (H, W, 3).\n        bboxes: Optional Tensor with bounding boxes with shape (num_bboxes, 5).\n            where we have (x_min, y_min, x_max, y_max, label) for each one.\n\n    Returns:\n        Dictionary containing:\n            image: Tensor with zoomed out image.\n            bboxes: Tensor with zoomed out bounding boxes with shape\n                (num_bboxes, 5).\n    """"""\n    image_shape = tf.to_float(tf.shape(image))\n    height = image_shape[0]\n    width = image_shape[1]\n    size_multiplier = tf.random_uniform([1], minval=min_ratio,\n                                        maxval=max_ratio, seed=seed)\n\n    # Expand image\n    new_height = height * size_multiplier\n    new_width = width * size_multiplier\n    pad_left = tf.random_uniform([1], minval=0,\n                                 maxval=new_width-width, seed=seed)\n    pad_right = new_width - width - pad_left\n    pad_top = tf.random_uniform([1], minval=0,\n                                maxval=new_height-height, seed=seed)\n    pad_bottom = new_height - height - pad_top\n\n    # TODO: use mean instead of 0 for filling the paddings\n    paddings = tf.stack([tf.concat([pad_top, pad_bottom], axis=0),\n                         tf.concat([pad_left, pad_right], axis=0),\n                         tf.constant([0., 0.])])\n    expanded_image = tf.pad(image, tf.to_int32(paddings), constant_values=fill)\n\n    # Adjust bboxes\n    shift_bboxes_by = tf.concat([pad_left, pad_top, pad_left, pad_top], axis=0)\n    bbox_labels = tf.reshape(bboxes[:, 4], (-1, 1))\n    bbox_adjusted_coords = bboxes[:, :4] + tf.to_int32(shift_bboxes_by)\n    bbox_adjusted = tf.concat([bbox_adjusted_coords, bbox_labels], axis=1)\n\n    # Return results\n    return_dict = {\'image\': expanded_image}\n    if bboxes is not None:\n        return_dict[\'bboxes\'] = bbox_adjusted\n    return return_dict\n'"
luminoth/utils/image_test.py,24,"b'import numpy as np\nimport tensorflow as tf\n\nfrom easydict import EasyDict\n\nfrom luminoth.utils.image import (\n    resize_image, flip_image, random_patch, random_resize, random_distortion,\n    patch_image\n)\nfrom luminoth.utils.test.gt_boxes import generate_gt_boxes\n\n\nclass ImageTest(tf.test.TestCase):\n    def setUp(self):\n        self._random_resize_config = EasyDict({\n            \'min_size\': 400,\n            \'max_size\': 980,\n        })\n        self._random_distort_config = EasyDict({\n            \'brightness\': {\n                \'max_delta\': 0.3,\n            },\n            \'contrast\': {\n                \'lower\': 0.4,\n                \'upper\': 0.8,\n            },\n            \'hue\': {\n                \'max_delta\': 0.2,\n            },\n            \'saturation\': {\n                \'lower\': 0.5,\n                \'upper\': 1.5,\n            }\n        })\n        self._random_patch_config = EasyDict({\n            \'min_height\': 600,\n            \'min_width\': 600,\n        })\n        tf.reset_default_graph()\n\n    def _gen_image(self, *shape):\n        return np.random.rand(*shape)\n\n    def _get_image_with_boxes(self, image_size, total_boxes):\n        image = self._gen_image(*image_size)\n        bboxes = generate_gt_boxes(\n            total_boxes, image_size[:2],\n        )\n        return image, bboxes\n\n    def _resize_image(self, image_array, boxes_array=None, min_size=None,\n                      max_size=None):\n        image = tf.placeholder(tf.float32, image_array.shape)\n        feed_dict = {\n            image: image_array,\n        }\n        if boxes_array is not None:\n            boxes = tf.placeholder(tf.float32, boxes_array.shape)\n            feed_dict[boxes] = boxes_array\n        else:\n            boxes = None\n        resized = resize_image(\n            image, bboxes=boxes, min_size=min_size, max_size=max_size\n        )\n        with self.test_session() as sess:\n            resized_dict = sess.run(resized, feed_dict=feed_dict)\n            return (\n                resized_dict[\'image\'],\n                resized_dict.get(\'bboxes\'),\n                resized_dict.get(\'scale_factor\'),\n            )\n\n    def _flip_image(self, image_array, boxes_array=None, left_right=False,\n                    up_down=False, bboxes_dtype=tf.int32):\n        image = tf.placeholder(tf.float32, image_array.shape)\n        feed_dict = {\n            image: image_array,\n        }\n        if boxes_array is not None:\n            boxes = tf.placeholder(bboxes_dtype, boxes_array.shape)\n            feed_dict[boxes] = boxes_array\n        else:\n            boxes = None\n        flipped = flip_image(\n            image, bboxes=boxes, left_right=left_right, up_down=up_down\n        )\n        with self.test_session() as sess:\n            flipped_dict = sess.run(flipped, feed_dict=feed_dict)\n            return flipped_dict[\'image\'], flipped_dict.get(\'bboxes\')\n\n    def _random_patch(self, image, config, bboxes=None):\n        with self.test_session() as sess:\n            image = tf.cast(image, tf.float32)\n            if bboxes is not None:\n                bboxes = tf.cast(bboxes, tf.int32)\n            patch = random_patch(image, bboxes=bboxes, seed=0, **config)\n            return_dict = sess.run(patch)\n            ret_bboxes = return_dict.get(\'bboxes\')\n            return return_dict[\'image\'], ret_bboxes\n\n    def _random_resize(self, image, config, bboxes=None):\n        config = self._random_resize_config\n        with self.test_session() as sess:\n            resize = random_resize(image, bboxes=bboxes, seed=0, **config)\n            return_dict = sess.run(resize)\n            ret_bboxes = return_dict.get(\'bboxes\')\n            return return_dict[\'image\'], ret_bboxes\n\n    def _random_distort(self, image, config, bboxes=None):\n        with self.test_session() as sess:\n            distort = random_distortion(\n                image, bboxes=bboxes, seed=0, **config\n            )\n            return_dict = sess.run(distort)\n            ret_bboxes = return_dict.get(\'bboxes\')\n            return return_dict[\'image\'], ret_bboxes\n\n    def testResizeOnlyImage(self):\n        # No min or max size, it doesn\'t change the image.\n        resized_image, _, scale = self._resize_image(\n            self._gen_image(100, 1024, 3)\n        )\n        self.assertAllEqual(\n            resized_image.shape,\n            (100, 1024, 3)\n        )\n        self.assertAlmostEqual(scale, 1.)\n\n        # min and max sizes smaller and larger that original image size.\n        resized_image, _, scal = self._resize_image(\n            self._gen_image(100, 1024, 3), min_size=0, max_size=2000\n        )\n        self.assertAllEqual(\n            resized_image.shape,\n            (100, 1024, 3)\n        )\n        self.assertAlmostEqual(scale, 1.)\n\n        # max_size only, slightly smaller than origin image size.\n        resized_image, _, scale = self._resize_image(\n            self._gen_image(100, 1024, 3), max_size=1000\n        )\n        self.assertAllEqual(\n            resized_image.shape,\n            (97, 1000, 3)\n        )\n        self.assertAlmostEqual(int(scale * 100), 97)\n\n        # min_size only, slightly bigger than origin image size.\n        resized_image, _, scale = self._resize_image(\n            self._gen_image(100, 1024, 3), min_size=120\n        )\n        self.assertAllEqual(\n            resized_image.shape,\n            (120, 1228, 3)\n        )\n        self.assertAlmostEqual(int(scale * 100), 120)\n\n        # max_size only, half image size\n        resized_image, _, scale = self._resize_image(\n            self._gen_image(100, 1024, 3), max_size=512\n        )\n        self.assertAllEqual(\n            resized_image.shape,\n            (50, 512, 3)\n        )\n        self.assertAlmostEqual(scale, 0.5)\n\n        # min_size only, double image size\n        resized_image, _, scale = self._resize_image(\n            self._gen_image(100, 1024, 3), min_size=200\n        )\n        self.assertAllEqual(\n            resized_image.shape,\n            (200, 2048, 3)\n        )\n        self.assertAlmostEqual(scale, 2.)\n\n        # min and max invariant changes, both changes negate themselves.\n        change = 1.1\n        resized_image, _, scale = self._resize_image(\n            self._gen_image(100, 200, 3), min_size=int(100 * change),\n            max_size=round(200 / change)\n        )\n        self.assertAllEqual(\n            resized_image.shape,\n            (100, 200, 3)\n        )\n        self.assertAlmostEqual(int(scale), 1)\n\n        # min and max invariant changes, both changes negate themselves.\n        change = 1.6  # Not all values work because of rounding issues.\n        resized_image, _, scale = self._resize_image(\n            self._gen_image(100, 200, 3), min_size=int(100 * change),\n            max_size=round(200 / change)\n        )\n        self.assertAllEqual(\n            resized_image.shape,\n            (100, 200, 3)  # No change\n        )\n        self.assertAlmostEqual(int(scale), 1)\n\n        resized_image, _, scale = self._resize_image(\n            self._gen_image(100, 200, 3), min_size=600,\n            max_size=1000\n        )\n        self.assertAllEqual(\n            resized_image.shape,\n            (100 * 6, 200 * 6, 3)  # We end up bigger than maximum.\n        )\n        self.assertAlmostEqual(scale, 6.)\n\n        resized_image, _, scale = self._resize_image(\n            self._gen_image(2000, 600, 3), min_size=600,\n            max_size=1000\n        )\n        self.assertAllEqual(\n            resized_image.shape,\n            (1000, 300, 3)  # We end up smaller than minimum image.\n        )\n        self.assertAlmostEqual(scale, 0.5)\n\n    def testResizeImageBoxes(self):\n        image = self._gen_image(100, 100, 3)\n        # Half size, corner box\n        boxes = np.array([[0, 0, 10, 10, -1]])\n        _, resized_boxes, scale = self._resize_image(\n            image, boxes, max_size=50\n        )\n        self.assertAllEqual(resized_boxes, [[0, 0, 5, 5, -1]])\n\n        # Half size, middle box\n        boxes = np.array([[10, 10, 90, 90, -1]])\n        _, resized_boxes, scale = self._resize_image(\n            image, boxes, max_size=50\n        )\n        self.assertAllEqual(resized_boxes, [[5, 5, 45, 45, -1]])\n\n        # Quarter size, maximum box\n        boxes = np.array([[0, 0, 99, 99, -1]])\n        _, resized_boxes, scale = self._resize_image(\n            image, boxes, max_size=25\n        )\n        self.assertAllEqual(resized_boxes, [[0, 0, 24, 24, -1]])\n\n    def testPatchImageUpdateCondition(self):\n        """"""Tests we\'re not patching if we would lose all gt_boxes.\n        """"""\n        im_shape = (600, 800, 3)\n        label = 3\n        image_ph = tf.placeholder(shape=(None, None, 3), dtype=tf.float32)\n        bboxes_ph = tf.placeholder(shape=(None, 5), dtype=tf.int32)\n\n        with self.test_session() as sess:\n            # Generate image and bboxes.\n            image = self._gen_image(*im_shape)\n            bboxes = [(0, 0, 40, 40, label), (430, 200, 480, 250, label)]\n            # Get patch_image so that the proposed patch has no gt_box center\n            # in it.\n            patch = patch_image(\n                image_ph, bboxes_ph,\n                offset_height=45, offset_width=45,\n                target_height=100, target_width=200\n            )\n            feed_dict = {\n                image_ph: image,\n                bboxes_ph: bboxes\n            }\n            # Run patch in a test session.\n            ret_dict = sess.run(patch, feed_dict)\n\n            ret_image = ret_dict[\'image\']\n            ret_bboxes = ret_dict.get(\'bboxes\')\n        # Assertions\n        self.assertAllClose(ret_image, image)\n        self.assertAllClose(ret_bboxes, bboxes)\n\n    def testFlipOnlyImage(self):\n        # No changes to image or boxes when no flip is specified.\n        image = self._gen_image(100, 100, 3)\n        not_flipped_image, _ = self._flip_image(image)\n        self.assertAllClose(not_flipped_image, image)\n\n        # Test flipped image that original first column is equal to the last\n        # column of the flipped one.\n        image = self._gen_image(100, 100, 3)\n        flipped_image, _ = self._flip_image(image, left_right=True)\n        self.assertAllClose(flipped_image[:, 0], image[:, -1])\n        self.assertAllClose(flipped_image[:, 1], image[:, -2])\n\n    def testFlipImageBoxes(self):\n        image, boxes = self._get_image_with_boxes((500, 500, 3), 10)\n        not_flipped_image, not_flipped_boxes = self._flip_image(image, boxes)\n        self.assertAllClose(not_flipped_image, image)\n        self.assertAllClose(not_flipped_boxes, boxes)\n\n        # Test box same size as image, left_right flip should not change.\n        image = self._gen_image(100, 100, 3)\n        boxes = np.array([[0, 0, 99, 99, -1]])\n        flipped_image, flipped_boxes = self._flip_image(\n            image, boxes, left_right=True\n        )\n        self.assertAllClose(boxes, flipped_boxes)\n        # Check that sum of columns is not modified, just the order.\n        self.assertAllClose(image.sum(axis=1), flipped_image.sum(axis=1))\n\n        # Test box same size as image, up_down flip, should not change.\n        image = self._gen_image(100, 100, 3)\n        boxes = np.array([[0, 0, 99, 99, -1]])\n        flipped_image, flipped_boxes = self._flip_image(\n            image, boxes, up_down=True\n        )\n        self.assertAllClose(boxes, flipped_boxes)\n        # Check that sum of columns is not modified, just the order.\n        self.assertAllClose(image.sum(axis=0), flipped_image.sum(axis=0))\n\n        # Test box same size as image, up_down flip, should not change.\n        image = self._gen_image(100, 100, 3)\n        boxes = np.array([[0, 0, 99, 99, -1]])\n        flipped_image, flipped_boxes = self._flip_image(\n            image, boxes, up_down=True, left_right=True\n        )\n        # Check that sum of columns is not modified, just the order.\n        self.assertAllClose(boxes, flipped_boxes)\n\n        # Test box same size as image, up_down flip, should not change.\n        image = self._gen_image(100, 100, 3)\n        boxes = np.array([[0, 0, 10, 10, -1]])\n        flipped_image, flipped_boxes = self._flip_image(\n            image, boxes, up_down=True, left_right=True\n        )\n        # Check that sum of columns is not modified, just the order.\n        self.assertAllClose(\n            np.array([[89.,  89.,  99.,  99.,  -1.]]), flipped_boxes\n        )\n\n    def testFlipBboxesDiffDtype(self):\n        image = self._gen_image(100, 100, 3)\n        boxes = np.array([[25, 14, 63, 41, -1]])\n        _, flipped_boxes_float = self._flip_image(\n            image, boxes, up_down=True, left_right=True,\n            bboxes_dtype=tf.float32\n        )\n\n        _, flipped_boxes_int = self._flip_image(\n            image, boxes, up_down=True, left_right=True,\n            bboxes_dtype=tf.int32\n        )\n\n        # Check that using different types is exactly the same.\n        self.assertAllClose(\n            flipped_boxes_float, flipped_boxes_int\n        )\n\n    def testRandomPatchImageBboxes(self):\n        """"""Tests the integrity of the return values of random_patch\n\n        When bboxes is not None.\n        """"""\n        im_shape = (800, 600, 3)\n        total_boxes = 5\n        # We don\'t care about the label\n        label = 3\n        # First test case, we use randomly generated image and bboxes.\n        image, bboxes = self._get_image_with_boxes(im_shape, total_boxes)\n        # Add a label to each bbox.\n        bboxes_w_label = tf.concat(\n            [\n                bboxes,\n                tf.fill((bboxes.shape[0], 1), label)\n            ],\n            axis=1\n        )\n        config = self._random_patch_config\n        ret_image, ret_bboxes = self._random_patch(\n            image, config, bboxes_w_label\n        )\n        # Assertions\n        self.assertLessEqual(ret_bboxes.shape[0], total_boxes)\n        self.assertGreater(ret_bboxes.shape[0], 0)\n        self.assertTrue(np.all(ret_bboxes >= 0))\n        self.assertTrue(np.all(\n            ret_bboxes[:, 0] <= ret_image.shape[1]\n        ))\n        self.assertTrue(np.all(\n            ret_bboxes[:, 1] <= ret_image.shape[0]\n        ))\n        self.assertTrue(np.all(\n            ret_bboxes[:, 2] <= ret_image.shape[1]\n        ))\n        self.assertTrue(np.all(\n            ret_bboxes[:, 3] <= ret_image.shape[0]\n        ))\n        self.assertTrue(np.all(ret_image.shape <= im_shape))\n\n    def testRandomPatchLargerThanImage(self):\n        """"""Tests random_patch normalizes the minimum sizes.\n        """"""\n        im_shape = (600, 800, 3)\n        total_boxes = 5\n        config = EasyDict({\n            \'min_height\': 900,\n            \'min_width\': 900\n        })\n        label = 3\n        image, bboxes = self._get_image_with_boxes(im_shape, total_boxes)\n        # Add a label to each bbox.\n        bboxes_w_label = tf.concat(\n            [\n                bboxes,\n                tf.fill((bboxes.shape[0], 1), label)\n            ],\n            axis=1\n        )\n        ret_image, ret_bboxes = self._random_patch(\n            image, config, bboxes_w_label\n        )\n        # Assertions\n        self.assertLessEqual(ret_bboxes.shape[0], total_boxes)\n        self.assertGreater(ret_bboxes.shape[0], 0)\n        self.assertTrue(np.all(ret_bboxes >= 0))\n        self.assertTrue(np.all(\n            ret_bboxes[:, 0] <= ret_image.shape[1]\n        ))\n        self.assertTrue(np.all(\n            ret_bboxes[:, 1] <= ret_image.shape[0]\n        ))\n        self.assertTrue(np.all(\n            ret_bboxes[:, 2] <= ret_image.shape[1]\n        ))\n        self.assertTrue(np.all(\n            ret_bboxes[:, 3] <= ret_image.shape[0]\n        ))\n        self.assertTrue(np.all(ret_image.shape <= im_shape))\n\n    def testRandomPatchOnlyImage(self):\n        """"""Tests the integrity of the return values of random_patch\n\n        When bboxes is None.\n        """"""\n        im_shape = (600, 800, 3)\n        image = self._gen_image(*im_shape)\n        config = self._random_patch_config\n        ret_image, ret_bboxes = self._random_patch(image, config)\n        # Assertions\n        self.assertTrue(np.all(ret_image.shape <= im_shape))\n        # We ran return_dict.get(\'bboxes\') on the dict returned by\n        # random_patch. That\'s why we should get a None in this case.\n        self.assertIs(ret_bboxes, None)\n\n    def testRandomResizeImageBboxes(self):\n        """"""Tests the integrity of the return values of random_resize\n\n        This tests the case when bboxes is not None.\n        """"""\n        im_shape = (600, 800, 3)\n        config = self._random_resize_config\n        total_boxes = 5\n        label = 3\n\n        image, bboxes = self._get_image_with_boxes(im_shape, total_boxes)\n        # Add a label to each bbox.\n        bboxes_w_label = tf.concat(\n            [\n                bboxes,\n                tf.fill((bboxes.shape[0], 1), label)\n            ],\n            axis=1\n        )\n        ret_image, ret_bboxes = self._random_resize(\n            image, config, bboxes_w_label\n        )\n        # Assertions\n        self.assertEqual(ret_bboxes.shape[0], total_boxes)\n        self.assertTrue(np.all(\n            np.asarray(ret_image.shape[:2]) >= config.min_size\n        ))\n        self.assertTrue(np.all(\n            np.asarray(ret_image.shape[:2]) <= config.max_size\n        ))\n\n    def testRandomResizeOnlyImage(self):\n        """"""Tests the integrity of the return values of random_resize\n\n        This tests the case when bboxes is None.\n        """"""\n        im_shape = (600, 800, 3)\n        image = self._gen_image(*im_shape)\n        config = self._random_resize_config\n        ret_image, ret_bboxes = self._random_resize(image, config)\n        # Assertions\n        self.assertEqual(ret_bboxes, None)\n        self.assertTrue(np.all(\n            np.asarray(ret_image.shape[:2]) >= config.min_size\n        ))\n        self.assertTrue(np.all(\n            np.asarray(ret_image.shape[:2]) <= config.max_size\n        ))\n\n    def testRandomDistort(self):\n        """"""Tests the integrity of the return values of random_distortion.\n        """"""\n        im_shape = (600, 900, 3)\n        config = self._random_distort_config\n        total_boxes = 5\n        label = 3\n\n        image, bboxes = self._get_image_with_boxes(im_shape, total_boxes)\n        # Add a label to each bbox.\n        bboxes_w_label = tf.concat(\n            [\n                bboxes,\n                tf.fill((bboxes.shape[0], 1), label)\n            ],\n            axis=1\n        )\n\n        ret_image, ret_bboxes = self._random_distort(\n            image, config, bboxes_w_label\n        )\n        # Assertions\n        self.assertEqual(im_shape, ret_image.shape)\n        self.assertAllEqual(\n            bboxes, ret_bboxes[:, :4]\n        )\n\n    def testSmallRandomDistort(self):\n        """"""Tests random_distort with small-change arguments.\n\n        We pass parameters to random_distort that make it so that it should\n        change the image relatively little, and then check that in fact it\n        changed relatively little.\n        """"""\n        total_boxes = 3\n        im_shape = (600, 900, 3)\n        config = EasyDict({\n            \'brightness\': {\n                \'max_delta\': 0.00001,\n            },\n            \'hue\': {\n                \'max_delta\': 0.00001,\n            },\n            \'saturation\': {\n                \'lower\': 0.99999,\n                \'upper\': 1.00001,\n            },\n            \'contrast\': {\n                \'lower\': 0.99999,\n                \'upper\': 1.00001\n            }\n        })\n        label = 3\n        image, bboxes = self._get_image_with_boxes(im_shape, total_boxes)\n        # Add a label to each bbox.\n        bboxes_w_label = tf.concat(\n            [\n                bboxes,\n                tf.fill((bboxes.shape[0], 1), label)\n            ],\n            axis=1\n        )\n        ret_image, ret_bboxes = self._random_distort(\n            image, config, bboxes_w_label\n        )\n        # Assertions\n        large_number = 0.1\n        self.assertAllClose(image, ret_image, rtol=0.05, atol=large_number)\n\n\nif __name__ == \'__main__\':\n    tf.test.main()\n'"
luminoth/utils/image_vis.py,7,"b'import io\nimport numpy as np\nimport os\nimport logging\nimport PIL.Image as Image\nimport PIL.ImageDraw as ImageDraw\nimport PIL.ImageFont as ImageFont\nimport tensorflow as tf\n\nfrom .bbox_overlap import bbox_overlap\nfrom .bbox_transform import decode\nfrom base64 import b64encode\nfrom sys import stdout\n\n# flake8: noqa\n\nfont = ImageFont.load_default()\nlogger = logging.getLogger(\'luminoth-vis\')\n\n\nsummaries_fn = {\n    \'fasterrcnn\': {\n        \'train\': {\n            \'rpn\': {\n                \'draw_top_nms_proposals\': None,\n                \'draw_gt_boxes\': None,\n            },\n            \'rcnn\': {\n                \'draw_object_prediction\': None\n            }\n        },\n        \'eval\': {\n            \'rpn\': {\n                \'draw_top_nms_proposals\': None,\n                \'draw_gt_boxes\': None,\n            },\n            \'rcnn\': {\n                \'draw_object_prediction\': None\n            }\n        },\n        \'debug\': {\n            \'rpn\': {\n                \'draw_anchors\': [\n                    None, {\'anchor_num\': 0}\n                ],\n                \'draw_anchor_centers\': None,\n                \'draw_anchor_batch\': None,\n                \'draw_positive_anchors\': None,\n                \'draw_top_proposals\': [\n                    None, {\'top_k\': False}, {\'max_display\': 50},\n                ],\n                \'draw_top_nms_proposals\': [\n                    None, {\'min_score\': 0.9},\n                    {\'min_score\': 0.75}, {\'min_score\': 0.05}\n                ],\n                \'draw_batch_proposals\': [\n                    {\'display\': \'anchor\'}, {\'display\': \'proposal\'},\n                    {\'display\': \'proposal\', \'draw_all\': False},\n                    {\'display\': \'proposal\', \'top_k\': 10, \'draw_all\': False},\n                    {\'display\': \'proposal\', \'top_k\': 20, \'draw_all\': False},\n                    {\'display\': \'anchor\', \'top_k\': 10, \'draw_all\': False},\n                    {\'display\': \'anchor\', \'top_k\': 20, \'draw_all\': False},\n                ],\n                \'draw_rpn_cls_loss\': [\n                    {\'foreground\': True, \'topn\': 10, \'worst\': True},\n                    {\'foreground\': True, \'topn\': 10, \'worst\': False},\n                    {\'foreground\': False, \'topn\': 10, \'worst\': True},\n                    {\'foreground\': False, \'topn\': 10, \'worst\': False},\n                    {\'foreground\': True, \'topn\': 20, \'worst\': True},\n                    {\'foreground\': True, \'topn\': 20, \'worst\': False},\n                    {\'foreground\': False, \'topn\': 20, \'worst\': True},\n                    {\'foreground\': False, \'topn\': 20, \'worst\': False},\n                ],\n                \'draw_rpn_bbox_targets\': None,\n                \'draw_rpn_bbox_pred\': [\n                    {\'top_k\': 1}, {\'top_k\': 5}, {\'top_k\': 10}, {\'top_k\': 20},\n                    {\'top_k\': 40}, {\'top_k\': 80}\n                ],\n                \'draw_rpn_bbox_pred_with_target\': [\n                    {\'worst\': True}, {\'worst\': False}\n                ],\n                \'draw_gt_boxes\': None,\n                \'draw_correct_rpn_proposals_anchors\': None,\n                \'draw_rpn_pred_combined_loss\': [\n                    {\'top_k\': 1}, {\'top_k\': 5}, {\'top_k\': 10}, {\'top_k\': 20},\n                    {\'top_k\': 50}\n                ],\n            },\n            \'rcnn\': {\n                \'draw_rcnn_cls_batch\': None,\n                \'draw_rcnn_input_proposals\': None,\n                \'draw_rcnn_cls_batch_errors\': [\n                    {\'worst\': True}, {\'worst\': False}],\n                \'draw_object_prediction\': None\n            }\n        },\n    },\n    \'ssd\': {\n        \'train\': {},\n        \'eval\': {},\n        \'debug\': {\n            \'draw_object_prediction\': None,\n            \'draw_ssd_target_proposals\': None,\n            \'draw_ssd_cls_loss\': [\n                {\'foreground\': True, \'topn\': 10, \'worst\': True},\n                {\'foreground\': True, \'topn\': 10, \'worst\': False},\n                {\'foreground\': False, \'topn\': 10, \'worst\': True},\n                {\'foreground\': False, \'topn\': 10, \'worst\': False},\n            ],\n            \'draw_ssd_final_pred_anchors\': None,\n            \'draw_ssd_bbox_pred\': [{\'top_k\': 1}, {\'top_k\': 5}, {\'top_k\': 10}],\n            \'draw_ssd_top_k_anchors_per_gt\': None\n        }\n    }\n}\n\n\ndef get_image_summaries(summaries_fn, pred_dict, image,\n                        gt_bboxes=None, extra_tag=None):\n    summaries = []\n    if gt_bboxes is not None:\n        pred_dict[\'gt_bboxes\'] = gt_bboxes\n\n    for fn_name, arguments in summaries_fn.items():\n        if not arguments:\n            arguments = [{}]\n\n        for argument in arguments:\n            if not argument:\n                argument = {}\n                tag = fn_name\n            else:\n                tag = os.path.join(fn_name, \',\'.join(\n                    \'{}={}\'.format(k, v) for k, v in argument.items()\n                ))\n            if extra_tag:\n                tag = \'{}/{}\'.format(tag, extra_tag)\n            try:\n                summary = image_to_summary(\n                    globals()[fn_name](\n                        pred_dict, image,\n                        **argument), tag)\n                summaries.append(summary)\n            except KeyError as err:\n                tf.logging.warning(\n                    \'Function {} failed with KeyError. Key value: {}\'.format(\n                        fn_name, err))\n    return summaries\n\n\ndef image_vis_summaries(pred_dict, config=None, extra_tag=None,\n                        image_visualization_mode=None, image=None,\n                        gt_bboxes=None):\n    summaries = []\n    if config.type == \'fasterrcnn\':\n        summaries.extend(\n            get_image_summaries(\n                summaries_fn[\n                    \'fasterrcnn\'][image_visualization_mode][\'rpn\'],\n                pred_dict, image, gt_bboxes,\n                extra_tag=extra_tag\n            )\n        )\n\n        if config.network.with_rcnn and summaries_fn[\n                \'fasterrcnn\'][image_visualization_mode].get(\'rcnn\'):\n            summaries.extend(\n                get_image_summaries(\n                    summaries_fn[\n                        \'fasterrcnn\'][image_visualization_mode][\'rcnn\'],\n                    pred_dict, image, gt_bboxes, extra_tag=extra_tag\n                )\n            )\n    elif config.type == \'ssd\':\n        summaries.extend(\n            get_image_summaries(\n                summaries_fn[\'ssd\'][image_visualization_mode],\n                pred_dict, image, gt_bboxes,\n                extra_tag=extra_tag\n            )\n        )\n\n    return summaries\n\n\ndef image_to_summary(image_pil, tag):\n    summary = tf.Summary(value=[\n        tf.Summary.Value(tag=tag, image=tf.Summary.Image(\n            encoded_image_string=imagepil_to_str(image_pil)))\n    ])\n    return summary\n\n\ndef imagepil_to_str(image_pil):\n    output = io.BytesIO()\n    image_pil.save(output, format=\'PNG\')\n    png_string = output.getvalue()\n    output.close()\n    return png_string\n\n\ndef imgcat(data, width=\'auto\', height=\'auto\', preserveAspectRatio=False,\n           inline=True, filename=\'\'):\n    """"""\n    The width and height are given as a number followed by a unit, or the\n            word ""auto"".\n        N: N character cells.\n        Npx: N pixels.\n        N%: N percent of the session\'s width or height.\n        auto: The image\'s inherent size will be used to determine an\n            appropriate dimension.\n    """"""\n    buf = bytes()\n    enc = \'utf-8\'\n\n    is_tmux = os.environ[\'TERM\'].startswith(\'screen\')\n\n    # OSC\n    buf += b\'\\033\'\n    if is_tmux:\n        buf += b\'Ptmux;\\033\\033\'\n    buf += b\']\'\n\n    buf += b\'1337;File=\'\n\n    if filename:\n        buf += b\'name=\'\n        buf += b64encode(filename.encode(enc))\n\n    buf += b\';size=%d\' % len(data)\n    buf += b\';inline=%d\' % int(inline)\n    buf += b\';width=%s\' % width.encode(enc)\n    buf += b\';height=%s\' % height.encode(enc)\n    buf += b\';preserveAspectRatio=%d\' % int(preserveAspectRatio)\n    buf += b\':\'\n    buf += b64encode(data)\n\n    # ST\n    buf += b\'\\a\'\n    if is_tmux:\n        buf += b\'\\033\\\\\'\n\n    buf += b\'\\n\'\n\n    stdout.buffer.write(buf)\n    stdout.flush()\n\n\ndef imgcat_pil(image_pil):\n    image_bytes = io.BytesIO()\n    image_pil.save(image_bytes, format=\'PNG\')\n    imgcat(image_bytes.getvalue())\n\n\ndef get_image_draw(image):\n    image_pil = Image.fromarray(\n        np.uint8(np.squeeze(image))).convert(\'RGB\')\n    draw = ImageDraw.Draw(image_pil, \'RGBA\')\n    return image_pil, draw\n\n\ndef draw_positive_anchors(pred_dict, image):\n    """"""\n    Draws positive anchors used as ""correct"" in RPN\n    """"""\n    anchors = pred_dict[\'all_anchors\']\n    correct_labels = pred_dict[\'rpn_prediction\'][\'rpn_cls_target\']\n    correct_labels = np.squeeze(correct_labels.reshape(anchors.shape[0], 1))\n    positive_indices = np.nonzero(correct_labels > 0)[0]\n    positive_anchors = anchors[positive_indices]\n    correct_labels = correct_labels[positive_indices]\n    gt_bboxes = pred_dict[\'gt_bboxes\']\n    max_overlap = pred_dict[\'rpn_prediction\'][\'rpn_max_overlap\']\n    max_overlap = np.squeeze(max_overlap.reshape(anchors.shape[0], 1))\n    overlap_iou = max_overlap[positive_indices]\n\n    image_pil, draw = get_image_draw(image)\n\n    logger.debug(\n        \'We have {} positive_anchors\'.format(positive_anchors.shape[0]))\n    logger.debug(\'GT boxes: {}\'.format(gt_bboxes))\n\n    for label, positive_anchor in zip(list(overlap_iou), positive_anchors):\n        draw.rectangle(\n            list(positive_anchor), fill=(255, 0, 0, 40),\n            outline=(0, 255, 0, 100))\n        x, y = positive_anchor[:2]\n        x = max(x, 0)\n        y = max(y, 0)\n        draw.text(\n            tuple([x, y]), text=str(label), font=font,\n            fill=(0, 255, 0, 255))\n\n    for gt_box in gt_bboxes:\n        draw.rectangle(\n            list(gt_box[:4]), fill=(0, 0, 255, 60),\n            outline=(0, 0, 255, 150))\n\n    return image_pil\n\n\ndef draw_gt_boxes(pred_dict, image):\n    """"""\n    Draws GT boxes.\n    """"""\n\n    image_pil, draw = get_image_draw(image)\n    gt_bboxes = pred_dict[\'gt_bboxes\']\n    for gt_box in gt_bboxes:\n        draw.rectangle(\n            list(gt_box[:4]),\n            fill=(0, 0, 255, 60),\n            outline=(0, 0, 255, 150)\n        )\n\n    return image_pil\n\n\ndef draw_anchor_centers(pred_dict, image):\n    anchors = pred_dict[\'all_anchors\']\n    x_min = anchors[:, 0]\n    y_min = anchors[:, 1]\n    x_max = anchors[:, 2]\n    y_max = anchors[:, 3]\n\n    center_x = x_min + (x_max - x_min) / 2.\n    center_y = y_min + (y_max - y_min) / 2.\n\n    image_pil, draw = get_image_draw(image)\n\n    for x, y in zip(center_x, center_y):\n        draw.rectangle(\n            [x - 1, y - 1, x + 1, y + 1],\n            fill=(255, 0, 0, 150), outline=(0, 255, 0, 200)\n        )\n\n    return image_pil\n\n\ndef draw_anchors(pred_dict, image, anchor_num=None):\n    """"""\n    Draws positive anchors used as ""correct"" in RPN\n    """"""\n    anchors = pred_dict[\'all_anchors\']\n    x_min = anchors[:, 0]\n    y_min = anchors[:, 1]\n    x_max = anchors[:, 2]\n    y_max = anchors[:, 3]\n\n    height = pred_dict[\'image_shape\'][0]\n    width = pred_dict[\'image_shape\'][1]\n\n    areas = np.unique(np.round((x_max - x_min) * (y_max - y_min)))\n\n    inside_filter = np.logical_and.reduce((\n        (x_min >= 0),\n        (y_min >= 0),\n        (x_max < width),\n        (y_max < height)\n    ))\n\n    x_negative = x_min < 0\n    y_negative = y_min < 0\n    x_outof = x_max >= width\n    y_outof = y_max >= height\n\n    if anchor_num is None:\n        logger.debug(\'{} unique areas: {}\'.format(len(areas), areas))\n        logger.debug(\'{:.2f}% valid anchors\'.format(\n            100.0 * np.count_nonzero(inside_filter) /\n            inside_filter.shape[0]\n        ))\n        logger.debug(\'\'\'\n{} anchors with X_min negative.\n{} anchors with Y_min negative.\n{} anchors with X_max above limit.\n{} anchors with Y_max above limit.\n        \'\'\'.format(\n            np.count_nonzero(x_negative),\n            np.count_nonzero(y_negative),\n            np.count_nonzero(x_outof),\n            np.count_nonzero(y_outof),\n        ))\n\n    moved_anchors = anchors.copy()\n    min_x = -x_min.min()\n    min_y = -y_min.min()\n\n    moved_anchors += [[min_x, min_y, min_x, min_y]]\n\n    max_x = int(moved_anchors[:, 2].max())\n    max_y = int(moved_anchors[:, 3].max())\n\n    image_pil, _ = get_image_draw(image)\n    back = Image.new(\'RGB\', [max_x, max_y], \'white\')\n    back.paste(image_pil, [int(min_x), int(min_y)])\n\n    draw = ImageDraw.Draw(back, \'RGBA\')\n\n    anchor_id_to_draw = anchor_num\n    draw_every = pred_dict[\'anchor_reference\'].shape[0]\n    first = True\n    for anchor_id, anchor in enumerate(moved_anchors):\n        if anchor_num is not None:\n            if anchor_id != anchor_id_to_draw:\n                continue\n            else:\n                anchor_id_to_draw += draw_every\n\n        if first and anchor_num is not None:\n            draw.rectangle(\n                list(anchor), fill=(255, 0, 0, 40), outline=(0, 255, 0, 120)\n            )\n            first = False\n        elif anchor_num is None:\n            draw.rectangle(\n                list(anchor), fill=(255, 0, 0, 1), outline=(0, 255, 0, 2)\n            )\n        else:\n            draw.rectangle(\n                list(anchor), fill=(255, 0, 0, 2), outline=(0, 255, 0, 4)\n            )\n\n    draw.text(\n        tuple([min_x, min_y - 10]),\n        text=\'{}w x {}h\'.format(width, height),\n        font=font, fill=(0, 0, 0, 160)\n    )\n\n    return back\n\n\ndef draw_anchor_batch(pred_dict, image):\n    """"""\n    Draw anchors used in the batch for RPN.\n    """"""\n    anchors = pred_dict[\'all_anchors\']\n    targets = pred_dict[\'rpn_prediction\'][\'rpn_cls_target\']\n\n    in_batch_idx = targets >= 0\n    anchors = anchors[in_batch_idx]\n    targets = targets[in_batch_idx]\n\n    image_pil, draw = get_image_draw(image)\n\n    for anchor, target in zip(anchors, targets):\n        if target == 1:\n            draw.rectangle(\n                list(anchor), fill=(20, 200, 10, 15),\n                outline=(20, 200, 10, 30))\n        else:\n            draw.rectangle(\n                list(anchor), fill=(200, 10, 170, 10),\n                outline=(200, 10, 170, 30))\n\n    return image_pil\n\n\ndef draw_bbox(image, bbox):\n    """"""\n    bbox: x1,y1,x2,y2\n    image: h,w,rgb\n    """"""\n    image_pil = Image.fromarray(np.uint8(image)).convert(\'RGB\')\n\n    draw = ImageDraw.Draw(image_pil, \'RGBA\')\n    draw.rectangle(bbox, fill=(255, 0, 0, 60), outline=(0, 255, 0, 200))\n\n    return image_pil\n\n\ndef draw_top_proposals(pred_dict, image, min_score=0.8, max_display=20,\n                       top_k=True, used_in_batch=False):\n    tf.logging.debug(\n        \'Top proposals (blue = matches target in batch, \'\n        \'green = matches background in batch, red = ignored in batch)\')\n    proposal_prediction = pred_dict[\'rpn_prediction\'][\'proposal_prediction\']\n    if top_k:\n        scores = proposal_prediction[\'sorted_top_scores\']\n        proposals = proposal_prediction[\'sorted_top_proposals\']\n    else:\n        scores = proposal_prediction[\'scores\']\n        proposals = proposal_prediction[\'proposals\']\n\n    min_score_idx = scores > min_score\n    scores = scores[min_score_idx]\n    proposals = proposals[min_score_idx]\n\n    top_scores_idx = np.argsort(scores)[::-1][:max_display]\n    scores = scores[top_scores_idx]\n    proposals = proposals[top_scores_idx]\n\n    image_pil, draw = get_image_draw(image)\n\n    for proposal, score in zip(proposals, scores):\n        bbox = list(proposal)\n        if (bbox[2] - bbox[0] <= 0) or (bbox[3] - bbox[1] <= 0):\n            logger.debug(\n                \'Ignoring top proposal without positive area: \'\n                \'{}, score: {}\'.format(proposal, score))\n            continue\n\n        draw.rectangle(\n            list(bbox), fill=(0, 255, 0, 20),\n            outline=(0, 255, 0, 80))\n        x, y = bbox[:2]\n        x = max(x, 0)\n        y = max(y, 0)\n\n        draw.text(\n            tuple([x, y]), text=str(score), font=font,\n            fill=(30, 30, 30, 80))\n\n    return image_pil\n\n\ndef draw_batch_proposals(pred_dict, image, display=\'proposal\', top_k=None,\n                         draw_all=True):\n    tf.logging.debug(\n        \'Batch proposals (background or foreground) \'\n        \'(score is classification, blue = foreground, \'\n        \'red = background, green = GT)\')\n    tf.logging.debug(\n        \'This only displays the images on the batch (256). \'\n        \'The number displayed is the classification score \'\n        \'(green is > 0.5, red <= 0.5)\')\n    tf.logging.debug(\n        \'{} are displayed\'.format(\n            \'Anchors\' if display == \'anchor\' else \'Final proposals\'))\n    scores = pred_dict[\'rpn_prediction\'][\'rpn_cls_prob\']\n    scores = scores[:, 1]\n    bbox_pred = pred_dict[\'rpn_prediction\'][\'rpn_bbox_pred\']\n    targets = pred_dict[\'rpn_prediction\'][\'rpn_cls_target\']\n    max_overlaps = pred_dict[\'rpn_prediction\'][\'rpn_max_overlap\']\n    all_anchors = pred_dict[\'all_anchors\']\n    gt_bboxes = pred_dict[\'gt_bboxes\']\n\n    batch_idx = targets >= 0\n    scores = scores[batch_idx]\n    bbox_pred = bbox_pred[batch_idx]\n    max_overlaps = max_overlaps[batch_idx]\n    all_anchors = all_anchors[batch_idx]\n    targets = targets[batch_idx]\n\n    if top_k:\n        top_scores_idx = np.argsort(scores)[::-1][:top_k]\n        scores = scores[top_scores_idx]\n        bbox_pred = bbox_pred[top_scores_idx]\n        max_overlaps = max_overlaps[top_scores_idx]\n        all_anchors = all_anchors[top_scores_idx]\n        targets = targets[top_scores_idx]\n\n    if not draw_all:\n        # Only batch foreground.\n        foreground_idx = targets > 0\n        scores = scores[foreground_idx]\n        bbox_pred = bbox_pred[foreground_idx]\n        max_overlaps = max_overlaps[foreground_idx]\n        all_anchors = all_anchors[foreground_idx]\n        targets = targets[foreground_idx]\n\n    bboxes = decode(all_anchors, bbox_pred)\n\n    image_pil, draw = get_image_draw(image)\n\n    for score, proposal, target, max_overlap, anchor in zip(\n            scores, bboxes, targets, max_overlaps, all_anchors):\n\n        if ((proposal[2] - proposal[0] <= 0) or\n                (proposal[3] - proposal[1] <= 0)):\n            logger.debug(\n                \'Ignoring proposal for target {} \'\n                \'because of negative area => {}\'.format(\n                    target, proposal))\n            continue\n\n        if target == 1:\n            fill = (0, 0, 255, 30)\n            if score > 0.8:\n                font_fill = (0, 0, 255, 160)\n            else:\n                font_fill = (0, 255, 255, 180)\n        else:\n            fill = (255, 0, 0, 5)\n            if score > 0.8:\n                font_fill = (255, 0, 255, 160)\n            else:\n                font_fill = (255, 0, 0, 100)\n\n        if score > 0.5:\n            outline_fill = (0, 0, 255, 50)\n        else:\n            outline_fill = (255, 0, 0, 50)\n\n        if np.abs(score - 1.) < 0.05:\n            font_txt = \'1\'\n        else:\n            font_txt = \'{:.2f}\'.format(score)[1:]\n\n        if display == \'anchor\':\n            box = list(anchor)\n        else:\n            box = list(proposal)\n\n        draw.rectangle(box, fill=fill, outline=outline_fill)\n        x, y = box[:2]\n        x = max(x, 0)\n        y = max(y, 0)\n\n        score = float(score)\n        draw.text(tuple([x, y]), text=font_txt, font=font, fill=font_fill)\n\n    for gt_box in gt_bboxes:\n        box = list(gt_box[:4])\n        draw.rectangle(box, fill=(0, 255, 0, 60), outline=(0, 255, 0, 70))\n\n    return image_pil\n\n\ndef draw_top_nms_proposals(pred_dict, image, min_score=0.8, draw_gt=False):\n    logger.debug(\'Top NMS proposals (min_score = {})\'.format(min_score))\n    scores = pred_dict[\'rpn_prediction\'][\'scores\']\n    proposals = pred_dict[\'rpn_prediction\'][\'proposals\']\n    top_scores_mask = scores > min_score\n    scores = scores[top_scores_mask]\n    proposals = proposals[top_scores_mask]\n\n    sorted_idx = scores.argsort()[::-1]\n    scores = scores[sorted_idx]\n    proposals = proposals[sorted_idx]\n\n    image_pil, draw = get_image_draw(image)\n\n    fill_alpha = 70\n\n    for topn, (score, proposal) in enumerate(zip(scores, proposals)):\n        bbox = list(proposal)\n        if (bbox[2] - bbox[0] <= 0) or (bbox[3] - bbox[1] <= 0):\n            logger.debug(\'Proposal has negative area: {}\'.format(bbox))\n            continue\n\n        draw.rectangle(\n            bbox, fill=(0, 255, 0, fill_alpha), outline=(0, 255, 0, 50))\n\n        if np.abs(score - 1.0) <= 0.01:\n            font_txt = \'1\'\n        else:\n            font_txt = \'{:.2f}\'.format(score)[1:]\n\n        draw.text(\n            tuple([bbox[0], bbox[1]]), text=font_txt,\n            font=font, fill=(0, 255, 0, 150))\n\n        fill_alpha -= 5\n\n    if draw_gt:\n        gt_bboxes = pred_dict[\'gt_bboxes\']\n        for gt_box in gt_bboxes:\n            draw.rectangle(\n                list(gt_box[:4]), fill=(0, 0, 255, 60),\n                outline=(0, 0, 255, 150))\n\n    return image_pil\n\n\ndef draw_rpn_cls_loss(pred_dict, image, foreground=True, topn=10,\n                      worst=True):\n    """"""\n    For each bounding box labeled object. We want to display the softmax score.\n\n    We display the anchors, and not the adjusted bounding boxes.\n    """"""\n    loss = pred_dict[\'rpn_prediction\'][\'cross_entropy_per_anchor\']\n    type_str = \'foreground\' if foreground else \'background\'\n    logger.debug(\n        \'RPN classification loss (anchors, with the softmax score) \'\n        \'(mean softmax loss (all): {})\'.format(loss.mean()))\n    logger.debug(\'Showing {} only\'.format(type_str))\n    logger.debug(\'{} {} performers\'.format(\'Worst\' if worst else \'Best\', topn))\n    prob = pred_dict[\'rpn_prediction\'][\'rpn_cls_prob\']\n    prob = prob.reshape([-1, 2])[:, 1]\n    target = pred_dict[\'rpn_prediction\'][\'rpn_cls_target\']\n    anchors = pred_dict[\'all_anchors\']\n    gt_bboxes = pred_dict[\'gt_bboxes\']\n\n    non_ignored_indices = target >= 0\n    target = target[non_ignored_indices]\n    prob = prob[non_ignored_indices]\n    anchors = anchors[non_ignored_indices]\n\n    # Get anchors with positive label.\n    if foreground:\n        positive_indices = np.nonzero(target > 0)[0]\n    else:\n        positive_indices = np.nonzero(target == 0)[0]\n\n    loss = loss[positive_indices]\n    prob = prob[positive_indices]\n    anchors = anchors[positive_indices]\n\n    logger.debug(\'Mean loss for {}: {}\'.format(type_str, loss.mean()))\n\n    sorted_idx = loss.argsort()\n    if worst:\n        sorted_idx = sorted_idx[::-1]\n\n    sorted_idx = sorted_idx[:topn]\n\n    loss = loss[sorted_idx]\n    prob = prob[sorted_idx]\n    anchors = anchors[sorted_idx]\n\n    logger.debug(\n        \'Mean loss for displayed {}: {}\'.format(type_str, loss.mean()))\n\n    image_pil, draw = get_image_draw(image)\n\n    for anchor_prob, anchor, anchor_loss in zip(prob, anchors, loss):\n        anchor = list(anchor)\n        draw.rectangle(anchor, fill=(0, 255, 0, 20), outline=(0, 255, 0, 100))\n        draw.text(\n            tuple([anchor[0], anchor[1]]), text=\'{:.2f}\'.format(anchor_loss),\n            font=font, fill=(0, 0, 0, 255))\n\n    for gt_box in gt_bboxes:\n        draw.rectangle(\n            list(gt_box[:4]), fill=(0, 0, 255, 60), outline=(0, 0, 255, 150))\n\n    return image_pil\n\n\ndef draw_rpn_pred_combined_loss(pred_dict, image, top_k=10):\n    target = pred_dict[\'rpn_prediction\'][\'rpn_cls_target\']\n    in_target = target >= 0\n    bbox_pred = pred_dict[\'rpn_prediction\'][\'rpn_bbox_pred\'][in_target]\n    all_anchors = pred_dict[\'all_anchors\'][in_target]\n    target = target[in_target]\n\n    in_foreground = target > 0\n\n    ce_loss = pred_dict[\'rpn_prediction\'][\n        \'cross_entropy_per_anchor\'][in_foreground]\n\n    bbox_pred = bbox_pred[in_foreground]\n    all_anchors = all_anchors[in_foreground]\n\n    reg_loss = pred_dict[\'rpn_prediction\'][\'reg_loss_per_anchor\']\n\n    combined_loss = ce_loss + reg_loss\n\n    bbox_final = decode(all_anchors, bbox_pred)\n\n    image_pil, draw = get_image_draw(image)\n\n    for bbox, loss in zip(bbox_final, combined_loss):\n        bbox = list(bbox)\n        draw.rectangle(bbox, fill=(30, 0, 240, 20), outline=(30, 0, 240, 100))\n        draw.text(\n            tuple([bbox[0], bbox[1]]), text=\'{:.2f}\'.format(loss),\n            font=font, fill=(0, 0, 0, 255))\n\n    return image_pil\n\n\ndef draw_rpn_bbox_pred(pred_dict, image, top_k=5):\n    """"""\n    For each bounding box labeled object. We want to display bbox_reg_error.\n\n    We display the final bounding box and the anchor. Drawing lines between the\n    corners.\n    """"""\n    logger.debug(\n        \'RPN regression loss (bbox to original anchors, \'\n        \'with the smoothL1Loss)\')\n    target = pred_dict[\'rpn_prediction\'][\'rpn_cls_target\']\n    bbox_pred = pred_dict[\'rpn_prediction\'][\'rpn_bbox_pred\']\n    all_anchors = pred_dict[\'all_anchors\']\n    # Get anchors with positive label.\n    positive_indices = target > 0\n\n    bbox_pred = bbox_pred[positive_indices]\n    all_anchors = all_anchors[positive_indices]\n\n    loss_per_anchor = pred_dict[\'rpn_prediction\'][\'reg_loss_per_anchor\']\n\n    top_losses_idx = np.argsort(loss_per_anchor)[::-1][:top_k]\n\n    loss_per_anchor = loss_per_anchor[top_losses_idx]\n    bbox_pred = bbox_pred[top_losses_idx]\n    all_anchors = all_anchors[top_losses_idx]\n\n    bbox_final = decode(all_anchors, bbox_pred)\n\n    image_pil, draw = get_image_draw(image)\n\n    for anchor, bbox, loss in zip(all_anchors, bbox_final, loss_per_anchor):\n        anchor = list(anchor)\n        bbox = list(bbox)\n        draw.rectangle(anchor, fill=(0, 255, 0, 20), outline=(0, 255, 0, 100))\n        draw.rectangle(\n            bbox, fill=(255, 0, 255, 20), outline=(255, 0, 255, 100))\n        draw.text(\n            tuple([anchor[0], anchor[1]]), text=\'{:.2f}\'.format(loss),\n            font=font, fill=(0, 0, 0, 255))\n        draw.line(\n            [(anchor[0], anchor[1]), (bbox[0], bbox[1])],\n            fill=(0, 0, 0, 170), width=1)\n        draw.line(\n            [(anchor[2], anchor[1]), (bbox[2], bbox[1])],\n            fill=(0, 0, 0, 170), width=1)\n        draw.line(\n            [(anchor[2], anchor[3]), (bbox[2], bbox[3])],\n            fill=(0, 0, 0, 170), width=1)\n        draw.line(\n            [(anchor[0], anchor[3]), (bbox[0], bbox[3])],\n            fill=(0, 0, 0, 170), width=1)\n\n    return image_pil\n\n\ndef draw_ssd_cls_loss(pred_dict, image, foreground=True, topn=10, worst=True):\n    """"""\n    For each bounding box labeled object. We want to display the softmax score.\n\n    We display the anchors, and not the adjusted bounding boxes.\n    """"""\n    loss = pred_dict[\'cls_loss_per_proposal\']\n\n    prob = pred_dict[\'cls_prob\']\n    target = pred_dict[\'target\'][\'cls\']\n    anchors = pred_dict[\'target\'][\'anchors\']\n    gt_bboxes = pred_dict[\'gt_bboxes\']\n\n    non_ignored_indices = target >= 0\n    target = target[non_ignored_indices]\n    prob = prob[non_ignored_indices]\n    anchors = anchors[non_ignored_indices]\n\n    # Get anchors with positive label.\n    if foreground:\n        positive_indices = np.nonzero(target > 0)[0]\n    else:\n        positive_indices = np.nonzero(target == 0)[0]\n\n    loss = loss[positive_indices]\n    prob = prob[positive_indices]\n    anchors = anchors[positive_indices]\n\n    sorted_idx = loss.argsort()\n    if worst:\n        sorted_idx = sorted_idx[::-1]\n\n    sorted_idx = sorted_idx[:topn]\n\n    loss = loss[sorted_idx]\n    prob = prob[sorted_idx]\n    anchors = anchors[sorted_idx]\n\n    image_pil, draw = get_image_draw(image)\n\n    for anchor_prob, anchor, anchor_loss in zip(prob, anchors, loss):\n        anchor = list(anchor)\n        draw.rectangle(anchor, fill=(0, 255, 0, 20), outline=(0, 255, 0, 100))\n        draw.text(\n            tuple([anchor[0], anchor[1]]), text=\'{:.2f}\'.format(anchor_loss),\n            font=font, fill=(0, 0, 0, 255))\n\n    for gt_box in gt_bboxes:\n        draw.rectangle(\n            list(gt_box[:4]), fill=(0, 0, 255, 60), outline=(0, 0, 255, 150))\n\n    return image_pil\n\n\ndef draw_rpn_bbox_targets(pred_dict, image):\n    target = pred_dict[\'rpn_prediction\'][\'rpn_cls_target\']\n    bbox_target = pred_dict[\'rpn_prediction\'][\'rpn_bbox_target\']\n    all_anchors = pred_dict[\'all_anchors\']\n\n    batch_foreground_idxs = target == 1\n    bbox_target = bbox_target[batch_foreground_idxs]\n    all_anchors = all_anchors[batch_foreground_idxs]\n\n    gt_boxes = decode(all_anchors, bbox_target)\n\n    image_pil, draw = get_image_draw(image)\n\n    for gt_box in gt_boxes:\n        draw.rectangle(\n            list(gt_box), fill=(30, 0, 240, 20), outline=(30, 0, 240, 100))\n\n    return image_pil\n\n\ndef draw_rpn_bbox_pred_with_target(pred_dict, image, worst=True):\n    if worst:\n        draw_desc = \'worst\'\n    else:\n        draw_desc = \'best\'\n\n    logger.debug(\n        \'Display prediction vs original for \'\n        \'{} performer or batch.\'.format(draw_desc))\n    logger.debug(\n        \'green = anchor, magenta = prediction, \'\n        \'red = anchor * target (should be GT)\')\n    target = pred_dict[\'rpn_prediction\'][\'rpn_cls_target\']\n    target = target.reshape([-1, 1])\n    # Get anchors with positive label.\n    positive_indices = np.nonzero(np.squeeze(target) > 0)[0]\n    random_indices = np.random.choice(np.arange(len(positive_indices)), 5)\n\n    loss_per_anchor = pred_dict[\'rpn_prediction\'][\'reg_loss_per_anchor\']\n\n    # Get only n random to avoid overloading image.\n    positive_indices = positive_indices[random_indices]\n    loss_per_anchor = loss_per_anchor[random_indices]\n    target = target[positive_indices]\n\n    bbox_pred = pred_dict[\'rpn_prediction\'][\'rpn_bbox_pred\']\n    bbox_pred = bbox_pred.reshape([-1, 4])\n    bbox_pred = bbox_pred[positive_indices]\n\n    bbox_target = pred_dict[\'rpn_prediction\'][\'rpn_bbox_target\']\n    bbox_target = bbox_target.reshape([-1, 4])\n    bbox_target = bbox_target[positive_indices]\n\n    all_anchors = pred_dict[\'all_anchors\']\n    all_anchors = all_anchors[positive_indices]\n\n    if worst:\n        loss_idx = loss_per_anchor.argmax()\n    else:\n        loss_idx = loss_per_anchor.argmin()\n\n    loss = loss_per_anchor[loss_idx]\n    anchor = all_anchors[loss_idx]\n    bbox_pred = bbox_pred[loss_idx]\n    bbox_target = bbox_target[loss_idx]\n\n    bbox = decode(np.array([anchor]), np.array([bbox_pred]))[0]\n    gt_box = decode(np.array([anchor]), np.array([bbox_target]))[0]\n\n    image_pil, draw = get_image_draw(image)\n\n    anchor = list(anchor)\n    bbox = list(bbox)\n    gt_box = list(gt_box)\n    draw.rectangle(anchor, fill=(0, 255, 0, 20), outline=(0, 255, 0, 100))\n    draw.rectangle(bbox, fill=(255, 0, 255, 20), outline=(255, 0, 255, 100))\n    draw.rectangle(gt_box, fill=(255, 0, 0, 20), outline=(255, 0, 0, 100))\n\n    logger.debug(\'Loss is {}\'.format(loss))\n    return image_pil\n\n\ndef draw_rcnn_cls_batch(pred_dict, image, foreground=True, background=True):\n    logger.debug(\n        \'Show the proposals used for training classifier. \'\n        \'(GT labels are -1 from cls targets)\')\n    logger.debug(\'blue => GT, green => foreground, red => background\')\n\n    proposals = pred_dict[\'rpn_prediction\'][\'proposals\']\n    cls_targets = pred_dict[\'classification_prediction\'][\'target\'][\'cls\']\n    gt_bboxes = pred_dict[\'gt_bboxes\']\n\n    batch_idx = np.where(cls_targets != -1)[0]\n    bboxes = proposals[batch_idx]\n    cls_targets = cls_targets[batch_idx]\n\n    image_pil, draw = get_image_draw(image)\n\n    for bbox, cls_target in zip(bboxes, cls_targets):\n        bbox = list(bbox.astype(int))\n        if cls_target > 0:\n            fill = (0, 255, 0, 20)\n            outline = (0, 255, 0, 100)\n        else:\n            fill = (255, 0, 0, 20)\n            outline = (255, 0, 0, 100)\n\n        draw.rectangle(bbox, fill=fill, outline=outline)\n        draw.text(\n            tuple(bbox[:2]), text=str(int(cls_target)), font=font, fill=fill)\n\n    for gt_box in gt_bboxes:\n        draw.rectangle(\n            list(gt_box[:4]), fill=(0, 0, 255, 20), outline=(0, 0, 255, 100))\n        draw.text(\n            tuple(gt_box[:2]), text=str(gt_box[4]),\n            font=font, fill=(0, 0, 255, 255))\n\n    return image_pil\n\n\ndef draw_rcnn_cls_batch_errors(pred_dict, image, foreground=True,\n                               background=True, worst=True, n=10):\n    logger.debug(\n        \'Show the {} classification errors in batch \'\n        \'used for training classifier.\'.format(\'worst\' if worst else \'best\'))\n    logger.debug(\'blue => GT, green => foreground, red => background\')\n\n    proposals = pred_dict[\'rpn_prediction\'][\'proposals\']\n    cls_targets = pred_dict[\'classification_prediction\'][\'target\'][\'cls\']\n    bbox_offsets_targets = pred_dict[\n        \'classification_prediction\'][\'target\'][\'bbox_offsets\']\n    gt_bboxes = pred_dict[\'gt_bboxes\']\n    batch_idx = np.where(cls_targets != -1)[0]\n\n    proposals = proposals[batch_idx]\n    cls_targets = cls_targets[batch_idx]\n    bbox_offsets_targets = bbox_offsets_targets[batch_idx]\n\n    # Cross entropy per proposal already has >= 0 target\n    # batches (not ignored proposals)\n    cross_entropy_per_proposal = pred_dict[\n        \'classification_prediction\'][\'_debug\'][\n        \'losses\'][\'cross_entropy_per_proposal\']\n\n    if worst:\n        selected_idx = cross_entropy_per_proposal.argsort()[::-1][:n]\n    else:\n        selected_idx = cross_entropy_per_proposal.argsort()[:n]\n\n    cross_entropy_per_proposal = cross_entropy_per_proposal[selected_idx]\n    proposals = proposals[selected_idx]\n    cls_targets = cls_targets[selected_idx]\n    bbox_offsets_targets = bbox_offsets_targets[selected_idx]\n\n    bboxes = decode(proposals, bbox_offsets_targets)\n\n    image_pil, draw = get_image_draw(image)\n\n    for bbox, cls_target, error in zip(\n            bboxes, cls_targets, cross_entropy_per_proposal):\n        bbox = list(bbox.astype(int))\n        if cls_target > 0:\n            fill = (0, 255, 0, 20)\n            outline = (0, 255, 0, 100)\n        else:\n            fill = (255, 0, 0, 20)\n            outline = (255, 0, 0, 100)\n\n        draw.rectangle(bbox, fill=fill, outline=outline)\n        draw.text(\n            tuple(bbox[:2]), text=\'{:.2f}\'.format(error), font=font, fill=fill)\n\n    for gt_box in gt_bboxes:\n        draw.rectangle(\n            list(gt_box[:4]), fill=(0, 0, 255, 20), outline=(0, 0, 255, 100))\n        # draw.text(tuple(gt_box[:2]), text=str(gt_box[4]),\n        #       font=font, fill=(0, 0, 255, 255))\n\n    return image_pil\n\n\ndef draw_rcnn_reg_batch_errors(pred_dict, image):\n    logger.debug(\n        \'Show errors in batch used for training classifier regressor.\')\n    logger.debug(\n        \'blue => GT, green => foreground, \'\n        \'r`regression_loss` - c`classification_loss`.\')\n\n    proposals = pred_dict[\'rpn_prediction\'][\'proposals\']\n    cls_targets = pred_dict[\'classification_prediction\'][\'target\'][\'cls\']\n    bbox_offsets_targets = pred_dict[\n        \'classification_prediction\'][\'target\'][\'bbox_offsets\']\n    bbox_offsets = pred_dict[\'classification_prediction\'][\'bbox_offsets\']\n    gt_bboxes = pred_dict[\'gt_bboxes\']\n\n    batch_idx = np.where(cls_targets >= 0)[0]\n\n    proposals = proposals[batch_idx]\n    cls_targets = cls_targets[batch_idx]\n    bbox_offsets_targets = bbox_offsets_targets[batch_idx]\n    bbox_offsets = bbox_offsets[batch_idx]\n    cross_entropy_per_proposal = pred_dict[\n        \'classification_prediction\'][\'_debug\'][\n        \'losses\'][\'cross_entropy_per_proposal\']\n\n    foreground_batch_idx = np.where(cls_targets > 0)[0]\n\n    proposals = proposals[foreground_batch_idx]\n    cls_targets = cls_targets[foreground_batch_idx]\n    bbox_offsets_targets = bbox_offsets_targets[foreground_batch_idx]\n    bbox_offsets = bbox_offsets[foreground_batch_idx]\n    cross_entropy_per_proposal = cross_entropy_per_proposal[\n        foreground_batch_idx]\n    reg_loss_per_proposal = pred_dict[\n        \'classification_prediction\'][\'_debug\'][\n        \'losses\'][\'reg_loss_per_proposal\']\n\n    cls_targets = cls_targets - 1\n\n    bbox_offsets_idx_pairs = np.stack(\n        np.array([\n            cls_targets * 4, cls_targets * 4 + 1,\n            cls_targets * 4 + 2, cls_targets * 4 + 3]\n        ), axis=1)\n    bbox_offsets = np.take(bbox_offsets, bbox_offsets_idx_pairs.astype(np.int))\n\n    bboxes = decode(proposals, bbox_offsets)\n\n    image_pil, draw = get_image_draw(image)\n\n    for proposal, bbox, cls_target, reg_error, cls_error in zip(\n            proposals, bboxes, cls_targets,\n            reg_loss_per_proposal, cross_entropy_per_proposal):\n        bbox = list(bbox.astype(int))\n        proposal = list(proposal.astype(int))\n\n        if cls_target > 0:\n            fill = (0, 255, 0, 20)\n            outline = (0, 255, 0, 100)\n            proposal_fill = (255, 255, 30, 20)\n            proposal_outline = (255, 255, 30, 100)\n        else:\n            fill = (255, 0, 0, 20)\n            outline = (255, 0, 0, 100)\n            proposal_fill = (255, 30, 255, 20)\n            proposal_outline = (255, 30, 255, 100)\n\n        draw.rectangle(bbox, fill=fill, outline=outline)\n        draw.rectangle(proposal, fill=proposal_fill, outline=proposal_outline)\n        draw.text(\n            tuple(bbox[:2]), text=\'r{:.3f} - c{:.2f}\'.format(\n                reg_error, cls_error), font=font, fill=(0, 0, 0, 150))\n\n        draw.line(\n            [(proposal[0], proposal[1]), (bbox[0], bbox[1])],\n            fill=(0, 0, 0, 170), width=1)\n        draw.line(\n            [(proposal[2], proposal[1]), (bbox[2], bbox[1])],\n            fill=(0, 0, 0, 170), width=1)\n        draw.line(\n            [(proposal[2], proposal[3]), (bbox[2], bbox[3])],\n            fill=(0, 0, 0, 170), width=1)\n        draw.line(\n            [(proposal[0], proposal[3]), (bbox[0], bbox[3])],\n            fill=(0, 0, 0, 170), width=1)\n\n    for gt_box in gt_bboxes:\n        draw.rectangle(\n            list(gt_box[:4]), fill=(0, 0, 255, 20), outline=(0, 0, 255, 100))\n\n    return image_pil\n\n\ndef recalculate_objects(pred_dict, image):\n    proposals = pred_dict[\'rpn_prediction\'][\'proposals\']\n    proposals_prob = pred_dict[\'classification_prediction\'][\'rcnn\'][\'cls_prob\']\n    proposals_target = proposals_prob.argmax(axis=1) - 1\n    bbox_offsets = pred_dict[\n        \'classification_prediction\'][\'rcnn\'][\'bbox_offsets\']\n\n    bbox_offsets = bbox_offsets[proposals_target >= 0]\n    proposals = proposals[proposals_target >= 0]\n    proposals_target = proposals_target[proposals_target >= 0]\n\n    bbox_offsets_idx_pairs = np.stack(\n        np.array([\n            proposals_target * 4, proposals_target * 4 + 1,\n            proposals_target * 4 + 2, proposals_target * 4 + 3]), axis=1)\n    bbox_offsets = np.take(bbox_offsets, bbox_offsets_idx_pairs.astype(np.int))\n\n    bboxes = decode(proposals, bbox_offsets)\n\n    return bboxes, proposals_target\n\n\ndef draw_object_prediction(pred_dict, image, topn=50):\n    logger.debug(\'Display top scored objects with label.\')\n    objects = pred_dict[\'classification_prediction\'][\'objects\']\n    objects_labels = pred_dict[\'classification_prediction\'][\'labels\']\n    objects_labels_prob = pred_dict[\'classification_prediction\'][\'probs\']\n\n    if len(objects_labels) == 0:\n        logger.debug(\n            \'No objects detected. Probably all classified as background.\')\n\n    image_pil, draw = get_image_draw(image)\n\n    for num_object, (object_, label, prob) in enumerate(\n            zip(objects, objects_labels, objects_labels_prob)):\n        bbox = list(object_)\n        draw.rectangle(bbox, fill=(0, 255, 0, 20), outline=(0, 255, 0, 100))\n        draw.text(\n            tuple([bbox[0], bbox[1]]), text=\'{} - {:.2f}\'.format(label, prob),\n            font=font, fill=(0, 0, 0, 255))\n\n    # bboxes, classes = recalculate_objects(pred_dict)\n    # for bbox, label in zip(bboxes, classes):\n    #     bbox = list(bbox)\n    #     draw.rectangle(bbox, fill=(0, 255, 0, 20), outline=(0, 255, 0, 100))\n    #     draw.text(tuple([bbox[0], bbox[1]]), text=\'{}\'.format(label),\n    #           font=font, fill=(0, 0, 0, 255))\n\n    return image_pil\n\n\ndef draw_correct_rpn_proposals_anchors(pred_dict, image, top_k=5):\n    scores = pred_dict[\'rpn_prediction\'][\'rpn_cls_prob\']\n    scores = scores[:, 1]\n    bbox_pred = pred_dict[\'rpn_prediction\'][\'rpn_bbox_pred\']\n    all_anchors = pred_dict[\'all_anchors\']\n    gt_bboxes = pred_dict[\'gt_bboxes\']\n    bboxes = decode(all_anchors, bbox_pred)\n\n    gt_overlap = bbox_overlap(bboxes, gt_bboxes[:, :4])\n    top_idxs = gt_overlap.max(axis=1).argsort()[::-1][:top_k]\n    bboxes = bboxes[top_idxs]\n    all_anchors = all_anchors[top_idxs]\n    scores = scores[top_idxs]\n\n    image_pil, draw = get_image_draw(image)\n\n    for bbox, anchor, score in zip(bboxes, all_anchors, scores):\n        bbox = list(bbox)\n        anchor = list(anchor)\n        draw.rectangle(bbox, fill=(0, 255, 50, 20), outline=(0, 255, 50, 100))\n        draw.rectangle(\n            anchor, fill=(0, 50, 255, 20), outline=(0, 50, 255, 100))\n        draw.text(\n            tuple([bbox[0], bbox[1]]), text=\'{:.2f}\'.format(score)[1:],\n            font=font, fill=(0, 0, 0, 255)\n        )\n\n    return image_pil\n\n\ndef draw_rpn_correct_proposals(pred_dict, image):\n    proposals = pred_dict[\'rpn_prediction\'][\'proposals\']\n    gt_bboxes = pred_dict[\'gt_bboxes\']\n    gt_boxes = gt_bboxes[:, :4]\n\n    overlaps = bbox_overlap(proposals, gt_boxes)\n\n    top_overlap = overlaps.max(axis=1)\n\n    top_overlap_idx = top_overlap >= 0.95\n\n    proposals = proposals[top_overlap_idx]\n\n    image_pil, draw = get_image_draw(image)\n\n    for proposal in proposals:\n        proposal = list(proposal)\n        draw.rectangle(\n            proposal, fill=(0, 255, 50, 20), outline=(0, 255, 50, 100))\n\n\ndef draw_rcnn_input_proposals(pred_dict, image):\n    logger.debug(\n        \'Display RPN proposals used in training classification. \'\n        \'Top IoU with GT is displayed.\')\n    proposals = pred_dict[\'rpn_prediction\'][\'proposals\']\n    gt_bboxes = pred_dict[\'gt_bboxes\']\n    gt_boxes = gt_bboxes[:, :4]\n\n    overlaps = bbox_overlap(proposals, gt_boxes)\n\n    top_overlap = overlaps.max(axis=1)\n\n    top_overlap_idx = top_overlap >= 0.5\n\n    proposals = proposals[top_overlap_idx]\n    top_overlap = top_overlap[top_overlap_idx]\n\n    image_pil, draw = get_image_draw(image)\n\n    for proposal, overlap in zip(proposals, top_overlap):\n        proposal = list(proposal)\n        draw.rectangle(\n            proposal, fill=(0, 255, 0, 20), outline=(0, 255, 0, 100))\n        draw.text(\n            tuple([proposal[0], proposal[1]]),\n            text=\'{:.2f}\'.format(overlap)[1:],\n            font=font, fill=(0, 0, 0, 255))\n\n    return image_pil\n\n\ndef draw_ssd_target_proposals(pred_dict, image):\n    image_pil, draw = get_image_draw(image)\n\n    all_anchors = pred_dict[\'target\'][\'anchors\']\n    bbox_pred = pred_dict[\'loc_pred\']\n    bbox_targets = pred_dict[\'target\'][\'cls\']\n\n    bboxes = decode(all_anchors, bbox_pred)\n\n    fill_alpha = 70\n\n    for proposal, target in zip(bboxes, bbox_targets):\n        bbox = list(proposal)\n        if (bbox[2] - bbox[0] <= 0) or (bbox[3] - bbox[1] <= 0):\n            logger.debug(\'Proposal has negative area: {}\'.format(bbox))\n            continue\n\n        if target == 0:\n            # Background.\n            fill = (255, 0, 0, fill_alpha)\n            outline = (255, 0, 0, 100)\n        elif target > 0:\n            # Foreground.\n            fill = (0, 255, 0, fill_alpha)\n            outline = (0, 255, 0, 100)\n        else:\n            # Unknown.\n            continue\n\n        draw.rectangle(bbox, fill=fill, outline=outline)\n\n        fill_alpha -= 5\n\n    gt_bboxes = pred_dict[\'gt_bboxes\']\n    for gt_box in gt_bboxes:\n        draw.rectangle(\n            list(gt_box[:4]), fill=(0, 0, 255, 60), outline=(0, 0, 255, 150))\n\n    return image_pil\n\n\ndef draw_ssd_bbox_pred(pred_dict, image, top_k=5):\n    """"""\n    For each bounding box labeled object.\n\n    We display the final bounding box and the anchor. Drawing lines between the\n    corners.\n    """"""\n\n    target = pred_dict[\'target\'][\'cls\']\n    all_anchors = pred_dict[\'target\'][\'anchors\']\n    bbox_pred = pred_dict[\'loc_pred\']\n    # Get anchors with positive label.\n    positive_indices = target > 0\n\n    bbox_pred = bbox_pred[positive_indices]\n    all_anchors = all_anchors[positive_indices]\n\n    loss_per_proposal = pred_dict[\'reg_loss_per_proposal\']\n\n    top_losses_idx = np.argsort(loss_per_proposal)[::-1][:top_k]\n\n    loss_per_proposal = loss_per_proposal[top_losses_idx]\n    bbox_pred = bbox_pred[top_losses_idx]\n    all_anchors = all_anchors[top_losses_idx]\n\n    bbox_final = decode(all_anchors, bbox_pred)\n\n    image_pil, draw = get_image_draw(image)\n\n    for anchor, bbox, loss in zip(all_anchors, bbox_final, loss_per_proposal):\n        anchor = list(anchor)\n        bbox = list(bbox)\n        draw.rectangle(anchor, fill=(0, 255, 0, 20), outline=(0, 255, 0, 100))\n        draw.rectangle(\n            bbox, fill=(255, 0, 255, 20), outline=(255, 0, 255, 100))\n        draw.text(\n            tuple([anchor[0], anchor[1]]), text=\'{:.2f}\'.format(loss),\n            font=font, fill=(0, 0, 0, 255))\n        draw.line(\n            [(anchor[0], anchor[1]), (bbox[0], bbox[1])],\n            fill=(0, 0, 0, 170), width=1)\n        draw.line(\n            [(anchor[2], anchor[1]), (bbox[2], bbox[1])],\n            fill=(0, 0, 0, 170), width=1)\n        draw.line(\n            [(anchor[2], anchor[3]), (bbox[2], bbox[3])],\n            fill=(0, 0, 0, 170), width=1)\n        draw.line(\n            [(anchor[0], anchor[3]), (bbox[0], bbox[3])],\n            fill=(0, 0, 0, 170), width=1)\n\n    return image_pil\n\n\ndef draw_ssd_final_pred_anchors(pred_dict, image):\n    """"""\n    Draw the anchors used for the final prediction.\n    """"""\n\n    objects = pred_dict[\'classification_prediction\'][\'objects\']\n    objects_labels = pred_dict[\'classification_prediction\'][\'labels\']\n    objects_labels_prob = pred_dict[\'classification_prediction\'][\'probs\']\n    objects_anchors = pred_dict[\'classification_prediction\'][\'anchors\']\n\n    if len(objects_labels) == 0:\n        logger.debug(\n            \'No objects detected. Probably all classified as background.\')\n\n    image_pil, draw = get_image_draw(image)\n\n    for num_object, (object_, label, prob, anchor) in enumerate(zip(\n            objects, objects_labels, objects_labels_prob, objects_anchors)):\n        anchor = list(anchor)\n        bbox = list(object_)\n        draw.rectangle(anchor, fill=(0, 255, 0, 20), outline=(0, 255, 0, 100))\n        draw.rectangle(\n            bbox, fill=(255, 0, 255, 20), outline=(255, 0, 255, 100))\n        draw.text(\n            tuple([bbox[0], bbox[1]]), text=\'{} - {:.2f}\'.format(label, prob),\n            font=font, fill=(0, 0, 0, 255))\n        draw.line(\n            [(anchor[0], anchor[1]), (bbox[0], bbox[1])],\n            fill=(0, 0, 0, 170), width=1)\n        draw.line(\n            [(anchor[2], anchor[1]), (bbox[2], bbox[1])],\n            fill=(0, 0, 0, 170), width=1)\n        draw.line(\n            [(anchor[2], anchor[3]), (bbox[2], bbox[3])],\n            fill=(0, 0, 0, 170), width=1)\n        draw.line(\n            [(anchor[0], anchor[3]), (bbox[0], bbox[3])],\n            fill=(0, 0, 0, 170), width=1)\n\n    return image_pil\n\n\ndef draw_ssd_top_k_anchors_per_gt(pred_dict, image, top_k=5):\n    """"""Generates the top_k anchors for each gt_bbox according to iou""""""\n    image_pil, draw = get_image_draw(image)\n    anchors = pred_dict[\'all_anchors\']\n    gt_bboxes = pred_dict[\'gt_bboxes\']\n    overlaps = bbox_overlap(anchors, gt_bboxes[:, :4])\n\n    # We iterate over the columns (gt_boxes) of overlaps, thats why we transpose\n    for overlaps_per_gt_box, gt_box in zip(overlaps.T, gt_bboxes):\n        # Draw gt box\n        draw.rectangle(\n            list(gt_box[:4]), fill=(0, 0, 255, 60), outline=(0, 0, 255, 150)\n        )\n\n        if top_k < len(overlaps):\n            # Get indices of the top_k anchors\n            partition_edge = top_k * -1\n            top_k_anchors_idx = np.argpartition(\n                overlaps_per_gt_box, partition_edge\n            )[partition_edge:]\n        \n            # Get top_k anchors\n            top_k_anchors = anchors[top_k_anchors_idx]\n\n            # Get the iou for the top_k anchors\n            top_k_overlaps = overlaps_per_gt_box[top_k_anchors_idx]\n        else:\n            top_k_anchors = anchors\n            top_k_overlaps = overlaps_per_gt_box\n\n        # Draw best k anchors with regards to overlap (iou)\n        for anchor, iou in zip(top_k_anchors, top_k_overlaps):\n            # TODO: We should get the iou tresholds from the configfile\n            iou_treshold = 0.5\n            rectangle_outline = (0, 255, 0, 100) if iou > iou_treshold else (255, 0, 0, 100)\n            draw.rectangle(\n                list(anchor), fill=(0, 0, 0, 0), outline=rectangle_outline\n            )\n            x, y = anchor[:2]\n            x = max(x, 0)\n            y = max(y, 0)\n            text_fill = (30, 30, 30, 100) if iou > iou_treshold else (255, 0, 0, 100)\n            draw.text(tuple([x, y]), text=str(iou)[:4], font=font,\n                      fill=text_fill)\n    return image_pil\n'"
luminoth/utils/losses.py,8,"b'import tensorflow as tf\n\n\ndef smooth_l1_loss(bbox_prediction, bbox_target, sigma=3.0):\n    """"""\n    Return Smooth L1 Loss for bounding box prediction.\n\n    Args:\n        bbox_prediction: shape (1, H, W, num_anchors * 4)\n        bbox_target:     shape (1, H, W, num_anchors * 4)\n\n\n    Smooth L1 loss is defined as:\n\n    0.5 * x^2                  if |x| < d\n    abs(x) - 0.5               if |x| >= d\n\n    Where d = 1 and x = prediction - target\n\n    """"""\n    sigma2 = sigma ** 2\n    diff = bbox_prediction - bbox_target\n    abs_diff = tf.abs(diff)\n    abs_diff_lt_sigma2 = tf.less(abs_diff, 1.0 / sigma2)\n    bbox_loss = tf.reduce_sum(\n        tf.where(\n            abs_diff_lt_sigma2,\n            0.5 * sigma2 * tf.square(abs_diff),\n            abs_diff - 0.5 / sigma2\n        ), [1]\n    )\n    return bbox_loss\n\n\nif __name__ == \'__main__\':\n    bbox_prediction_tf = tf.placeholder(tf.float32)\n    bbox_target_tf = tf.placeholder(tf.float32)\n    loss_tf = smooth_l1_loss(bbox_prediction_tf, bbox_target_tf)\n    with tf.Session() as sess:\n        loss = sess.run(\n            loss_tf,\n            feed_dict={\n                bbox_prediction_tf: [\n                    [0.47450006, -0.80413032, -0.26595005, 0.17124325]\n                ],\n                bbox_target_tf: [\n                    [0.10058594, 0.07910156, 0.10555581, -0.1224325]\n                ],\n            })\n'"
luminoth/utils/predicting.py,15,"b'import json\nimport numpy as np\nimport os\nimport tensorflow as tf\n\nfrom luminoth.models import get_model\nfrom luminoth.datasets import get_dataset\n\n\nclass PredictorNetwork(object):\n    """"""Instantiates a network in order to get predictions from it.\n\n    If a checkpoint exists in the job\'s directory, load it.  The names of the\n    classes will be obtained from the dataset directory.\n\n    Returns a list of objects detected, which is a dict of its coordinates,\n    label and probability, ordered by probability.\n    """"""\n\n    def __init__(self, config):\n\n        if config.dataset.dir:\n            # Gets the names of the classes\n            classes_file = os.path.join(config.dataset.dir, \'classes.json\')\n            if tf.gfile.Exists(classes_file):\n                self.class_labels = json.load(tf.gfile.GFile(classes_file))\n            else:\n                self.class_labels = None\n\n        # Don\'t use data augmentation in predictions\n        config.dataset.data_augmentation = None\n\n        dataset_class = get_dataset(config.dataset.type)\n        model_class = get_model(config.model.type)\n        dataset = dataset_class(config)\n        model = model_class(config)\n\n        graph = tf.Graph()\n        self.session = tf.Session(graph=graph)\n\n        with graph.as_default():\n            self.image_placeholder = tf.placeholder(\n                tf.float32, (None, None, 3)\n            )\n            image_tf, _, process_meta = dataset.preprocess(\n                self.image_placeholder\n            )\n            pred_dict = model(image_tf)\n\n            # Restore checkpoint\n            if config.train.job_dir:\n                job_dir = config.train.job_dir\n                if config.train.run_name:\n                    job_dir = os.path.join(job_dir, config.train.run_name)\n                ckpt = tf.train.get_checkpoint_state(job_dir)\n                if not ckpt or not ckpt.all_model_checkpoint_paths:\n                    raise ValueError(\'Could not find checkpoint in {}.\'.format(\n                        job_dir\n                    ))\n                ckpt = ckpt.all_model_checkpoint_paths[-1]\n                saver = tf.train.Saver(sharded=True, allow_empty=True)\n                saver.restore(self.session, ckpt)\n                tf.logging.info(\'Loaded checkpoint.\')\n            else:\n                # A prediction without checkpoint is just used for testing\n                tf.logging.warning(\n                    \'Could not load checkpoint. Using initialized model.\')\n                init_op = tf.group(\n                    tf.global_variables_initializer(),\n                    tf.local_variables_initializer()\n                )\n                self.session.run(init_op)\n\n            if config.model.type == \'ssd\':\n                cls_prediction = pred_dict[\'classification_prediction\']\n                objects_tf = cls_prediction[\'objects\']\n                objects_labels_tf = cls_prediction[\'labels\']\n                objects_labels_prob_tf = cls_prediction[\'probs\']\n            elif config.model.type == \'fasterrcnn\':\n                if config.model.network.get(\'with_rcnn\', False):\n                    cls_prediction = pred_dict[\'classification_prediction\']\n                    objects_tf = cls_prediction[\'objects\']\n                    objects_labels_tf = cls_prediction[\'labels\']\n                    objects_labels_prob_tf = cls_prediction[\'probs\']\n                else:\n                    rpn_prediction = pred_dict[\'rpn_prediction\']\n                    objects_tf = rpn_prediction[\'proposals\']\n                    objects_labels_prob_tf = rpn_prediction[\'scores\']\n                    # All labels without RCNN are zero\n                    objects_labels_tf = tf.zeros(\n                        tf.shape(objects_labels_prob_tf), dtype=tf.int32\n                    )\n            else:\n                raise ValueError(\n                    ""Model type \'{}\' not supported"".format(config.model.type)\n                )\n\n            self.fetches = {\n                \'objects\': objects_tf,\n                \'labels\': objects_labels_tf,\n                \'probs\': objects_labels_prob_tf,\n                \'scale_factor\': process_meta[\'scale_factor\']\n            }\n\n            # If in debug mode, return the full prediction dictionary.\n            if config.train.debug:\n                self.fetches[\'_debug\'] = pred_dict\n\n    def predict_image(self, image):\n        fetched = self.session.run(self.fetches, feed_dict={\n            self.image_placeholder: np.array(image)\n        })\n\n        objects = fetched[\'objects\']\n        labels = fetched[\'labels\'].tolist()\n        probs = fetched[\'probs\'].tolist()\n        scale_factor = fetched[\'scale_factor\']\n\n        if self.class_labels is not None:\n            labels = [self.class_labels[label] for label in labels]\n\n        # Scale objects to original image dimensions\n        if isinstance(scale_factor, tuple):\n            # If scale factor is a tuple, it means we need to scale height and\n            # width by a different amount. In that case scale factor is:\n            # (scale_factor_height, scale_factor_width)\n            objects /= [scale_factor[1], scale_factor[0],\n                        scale_factor[1], scale_factor[0]]\n        else:\n            # If scale factor is a scalar, height and width get scaled by the\n            # same amount\n            objects /= scale_factor\n\n        # Cast to int to consistently return the same type in Python 2 and 3\n        objects = [\n            [int(round(coord)) for coord in obj]\n            for obj in objects.tolist()\n        ]\n\n        predictions = sorted([\n            {\n                \'bbox\': obj,\n                \'label\': label,\n                \'prob\': round(prob, 4),\n            } for obj, label, prob in zip(objects, labels, probs)\n        ], key=lambda x: x[\'prob\'], reverse=True)\n\n        return predictions\n'"
luminoth/utils/training.py,14,"b'import tensorflow as tf\n\nfrom luminoth.utils.vars import variable_summaries\n\n\nOPTIMIZERS = {\n    \'adam\': tf.train.AdamOptimizer,\n    \'momentum\': tf.train.MomentumOptimizer,\n    \'gradient_descent\': tf.train.GradientDescentOptimizer,\n    \'rmsprop\': tf.train.RMSPropOptimizer,\n}\n\nLEARNING_RATE_DECAY_METHODS = {\n    \'polynomial_decay\': tf.train.polynomial_decay,\n    \'piecewise_constant\': tf.train.piecewise_constant,\n    \'exponential_decay\': tf.train.exponential_decay,\n}\n\n\ndef get_learning_rate(train_config, global_step=None):\n    """"""\n    Get learning rate from train config.\n\n    TODO: Better config usage.\n\n    Returns:\n        learning_rate: TensorFlow variable.\n\n    Raises:\n        ValueError: When the method used is not available.\n    """"""\n    lr_config = train_config.learning_rate.copy()\n    decay_method = lr_config.pop(\'decay_method\', None)\n\n    if not decay_method or decay_method == \'none\':\n        return lr_config.get(\'value\') or lr_config.get(\'learning_rate\')\n\n    if decay_method not in LEARNING_RATE_DECAY_METHODS:\n        raise ValueError(\'Invalid learning_rate method ""{}""\'.format(\n            decay_method\n        ))\n\n    if decay_method == \'piecewise_constant\':\n        lr_config[\'x\'] = global_step\n    else:\n        lr_config[\'global_step\'] = global_step\n\n    # boundaries, when used, must be the same type as global_step (int64).\n    if \'boundaries\' in lr_config:\n        lr_config[\'boundaries\'] = [\n            tf.cast(b, tf.int64) for b in lr_config[\'boundaries\']\n        ]\n\n    decay_function = LEARNING_RATE_DECAY_METHODS[decay_method]\n    learning_rate = decay_function(\n        **lr_config\n    )\n\n    tf.summary.scalar(\'losses/learning_rate\', learning_rate)\n\n    return learning_rate\n\n\ndef get_optimizer(train_config, global_step=None):\n    """"""\n    Get optimizer from train config.\n\n    Raises:\n        ValueError: When the optimizer type or learning_rate method are not\n            valid.\n    """"""\n    learning_rate = get_learning_rate(train_config, global_step)\n    optimizer_config = train_config.optimizer.copy()\n    optimizer_type = optimizer_config.pop(\'type\')\n    if optimizer_type not in OPTIMIZERS:\n        raise ValueError(\n            \'Invalid optimizer type ""{}""\'.format(optimizer_type)\n        )\n\n    optimizer_cls = OPTIMIZERS[optimizer_type]\n    return optimizer_cls(learning_rate, **optimizer_config)\n\n\ndef clip_gradients_by_norm(grads_and_vars, add_to_summary=False):\n    if add_to_summary:\n        for grad, var in grads_and_vars:\n            if grad is not None:\n                variable_summaries(\n                    grad, \'grad/{}\'.format(var.name[:-2]), \'full\'\n                )\n                variable_summaries(\n                    tf.abs(grad), \'grad/abs/{}\'.format(var.name[:-2]), \'full\'\n                )\n\n    # Clip by norm. Grad can be null when not training some modules.\n    with tf.name_scope(\'clip_gradients_by_norm\'):\n        grads_and_vars = [\n            (\n                tf.check_numerics(\n                    tf.clip_by_norm(gv[0], 10.),\n                    \'Invalid gradient\'\n                ), gv[1]\n            )\n            if gv[0] is not None else gv\n            for gv in grads_and_vars\n        ]\n\n    if add_to_summary:\n        for grad, var in grads_and_vars:\n            if grad is not None:\n                variable_summaries(\n                    grad, \'clipped_grad/{}\'.format(var.name[:-2]), \'full\'\n                )\n                variable_summaries(\n                    tf.abs(grad),\n                    \'clipped_grad/{}\'.format(var.name[:-2]),\n                    \'full\'\n                )\n\n    return grads_and_vars\n'"
luminoth/utils/vars.py,17,"b'import tensorflow as tf\n\n\nVALID_INITIALIZERS = {\n    \'truncated_normal_initializer\': tf.truncated_normal_initializer,\n    \'variance_scaling_initializer\': (\n        tf.contrib.layers.variance_scaling_initializer\n    ),\n    \'random_normal_initializer\': tf.random_normal_initializer,\n    \'xavier_initializer\': tf.contrib.layers.xavier_initializer,\n}\n\n\nVAR_LOG_LEVELS = {\n    \'full\': [\'variable_summaries_full\'],\n    \'reduced\': [\'variable_summaries_reduced\', \'variable_summaries_full\'],\n}\n\n\ndef variable_summaries(var, name, collection_key):\n    """"""Attach a lot of summaries to a Tensor (for TensorBoard visualization).\n\n    Args:\n        - var: Tensor for variable from which we want to log.\n        - name: Variable name.\n        - collection_key: Collection to save the summary to, can be any key of\n          `VAR_LOG_LEVELS`.\n    """"""\n    if collection_key not in VAR_LOG_LEVELS.keys():\n        raise ValueError(\'""{}"" not in `VAR_LOG_LEVELS`\'.format(collection_key))\n    collections = VAR_LOG_LEVELS[collection_key]\n\n    with tf.name_scope(name):\n        mean = tf.reduce_mean(var)\n        tf.summary.scalar(\'mean\', mean, collections)\n        num_params = tf.reduce_prod(tf.shape(var))\n        tf.summary.scalar(\'num_params\', num_params, collections)\n        with tf.name_scope(\'stddev\'):\n            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n        tf.summary.scalar(\'stddev\', stddev, collections)\n        tf.summary.scalar(\'max\', tf.reduce_max(var), collections)\n        tf.summary.scalar(\'min\', tf.reduce_min(var), collections)\n        tf.summary.histogram(\'histogram\', var, collections)\n        tf.summary.scalar(\'sparsity\', tf.nn.zero_fraction(var), collections)\n\n\ndef layer_summaries(layer, collection_key):\n    layer_name = layer.module_name\n    if hasattr(layer, \'_w\'):\n        variable_summaries(layer._w, \'{}/W\'.format(layer_name), collection_key)\n\n    if hasattr(layer, \'_b\'):\n        variable_summaries(layer._b, \'{}/b\'.format(layer_name), collection_key)\n\n\ndef get_initializer(initializer_config, seed=None):\n    """"""Get variable initializer.\n\n    Args:\n        - initializer_config: Configuration for initializer.\n\n    Returns:\n        initializer: Instantiated variable initializer.\n    """"""\n\n    if \'type\' not in initializer_config:\n        raise ValueError(\'Initializer missing type.\')\n\n    if initializer_config.type not in VALID_INITIALIZERS:\n        raise ValueError(\'Initializer ""{}"" is not valid.\'.format(\n            initializer_config.type))\n\n    config = initializer_config.copy()\n    initializer = VALID_INITIALIZERS[config.pop(\'type\')]\n    config[\'seed\'] = seed\n\n    return initializer(**config)\n\n\ndef get_activation_function(activation_function):\n    if not activation_function:\n        return lambda a: a\n\n    try:\n        return getattr(tf.nn, activation_function)\n    except AttributeError:\n        raise ValueError(\n            \'Invalid activation function ""{}""\'.format(activation_function))\n'"
luminoth/models/base/__init__.py,0,b'from .base_network import BaseNetwork  # noqa\nfrom .truncated_base_network import TruncatedBaseNetwork  # noqa\n'
luminoth/models/base/base_network.py,5,"b'import functools\n\nimport sonnet as snt\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom tensorflow.contrib.slim.nets import resnet_v2, resnet_v1, vgg\n\nfrom luminoth.models.base import truncated_vgg\nfrom luminoth.utils.checkpoint_downloader import get_checkpoint_file\n\n\n# Default RGB means used commonly.\n_R_MEAN = 123.68\n_G_MEAN = 116.78\n_B_MEAN = 103.94\n\nVALID_ARCHITECTURES = set([\n    \'resnet_v1_50\',\n    \'resnet_v1_101\',\n    \'resnet_v1_152\',\n    \'resnet_v2_50\',\n    \'resnet_v2_101\',\n    \'resnet_v2_152\',\n    \'vgg_16\',\n    \'truncated_vgg_16\',\n])\n\n\nclass BaseNetwork(snt.AbstractModule):\n    """"""\n    Convolutional Neural Network used for image classification, whose\n    architecture can be any of the `VALID_ARCHITECTURES`.\n\n    This class wraps the `tf.slim` implementations of these models, with some\n    helpful additions.\n    """"""\n\n    def __init__(self, config, name=\'base_network\'):\n        super(BaseNetwork, self).__init__(name=name)\n        if config.get(\'architecture\') not in VALID_ARCHITECTURES:\n            raise ValueError(\'Invalid architecture: ""{}""\'.format(\n                config.get(\'architecture\')\n            ))\n\n        self._architecture = config.get(\'architecture\')\n        self._config = config\n\n        self.pretrained_weights_scope = None\n\n    @property\n    def arg_scope(self):\n        arg_scope_kwargs = self._config.get(\'arg_scope\', {})\n\n        if self.vgg_type:\n            return vgg.vgg_arg_scope(**arg_scope_kwargs)\n\n        if self.truncated_vgg_type:\n            return truncated_vgg.vgg_arg_scope(**arg_scope_kwargs)\n\n        if self.resnet_type:\n            # It\'s the same arg_scope for v1 or v2.\n            return resnet_v2.resnet_utils.resnet_arg_scope(**arg_scope_kwargs)\n\n        raise ValueError(\'Invalid architecture: ""{}""\'.format(\n            self._config.get(\'architecture\')\n        ))\n\n    def network(self, is_training=False):\n        if self.vgg_type:\n            return functools.partial(\n                getattr(vgg, self._architecture),\n                is_training=is_training,\n                spatial_squeeze=self._config.get(\'spatial_squeeze\', False),\n            )\n        elif self.truncated_vgg_type:\n            return functools.partial(\n                getattr(truncated_vgg, self._architecture),\n                is_training=is_training,\n            )\n\n        elif self.resnet_v1_type:\n            output_stride = self._config.get(\'output_stride\')\n            train_batch_norm = (\n                is_training and self._config.get(\'train_batch_norm\')\n            )\n            return functools.partial(\n                getattr(resnet_v1, self._architecture),\n                is_training=train_batch_norm,\n                num_classes=None,\n                global_pool=False,\n                output_stride=output_stride\n            )\n        elif self.resnet_v2_type:\n            output_stride = self._config.get(\'output_stride\')\n            return functools.partial(\n                getattr(resnet_v2, self._architecture),\n                is_training=is_training,\n                num_classes=self._config.get(\'num_classes\'),\n                output_stride=output_stride,\n            )\n\n    @property\n    def vgg_type(self):\n        return self._architecture.startswith(\'vgg\')\n\n    @property\n    def vgg_16_type(self):\n        return self._architecture.startswith(\'vgg_16\')\n\n    @property\n    def truncated_vgg_type(self):\n        return self._architecture.startswith(\'truncated_vgg\')\n\n    @property\n    def truncated_vgg_16_type(self):\n        return self._architecture.startswith(\'truncated_vgg_16\')\n\n    @property\n    def resnet_type(self):\n        return self._architecture.startswith(\'resnet\')\n\n    @property\n    def resnet_v1_type(self):\n        return self._architecture.startswith(\'resnet_v1\')\n\n    @property\n    def resnet_v2_type(self):\n        return self._architecture.startswith(\'resnet_v2\')\n\n    @property\n    def default_image_size(self):\n        # Usually 224, but depends on the architecture.\n        if self.vgg_16_type:\n            return vgg.vgg_16.default_image_size\n        if self.truncated_vgg_16_type:\n            return vgg.truncated_vgg_16.default_image_size\n        if self.resnet_v1_type:\n            return resnet_v1.resnet_v1.default_image_size\n        if self.resnet_v2_type:\n            return resnet_v2.resnet_v2.default_image_size\n\n    def _build(self, inputs, is_training=False):\n        inputs = self.preprocess(inputs)\n        with slim.arg_scope(self.arg_scope):\n            net, end_points = self.network(is_training=is_training)(inputs)\n\n            return {\n                \'net\': net,\n                \'end_points\': end_points,\n            }\n\n    def preprocess(self, inputs):\n        if self.vgg_type or self.resnet_type:\n            inputs = self._subtract_channels(inputs)\n\n        return inputs\n\n    def _subtract_channels(self, inputs, means=[_R_MEAN, _G_MEAN, _B_MEAN]):\n        """"""Subtract channels from images.\n\n        It is common for CNNs to subtract the mean of all images from each\n        channel. In the case of RGB images we first calculate the mean from\n        each of the channels (Red, Green, Blue) and subtract those values\n        for training and for inference.\n\n        Args:\n            inputs: A Tensor of images we want to normalize. Its shape is\n                (1, height, width, num_channels).\n            means: A Tensor of shape (num_channels,) with the means to be\n                subtracted from each channels on the inputs.\n\n        Returns:\n            outputs: A Tensor of images normalized with the means.\n                Its shape is the same as the input.\n        """"""\n        return inputs - [means]\n\n    def _normalize(self, inputs):\n        """"""Normalize between -1.0 to 1.0.\n\n        Args:\n            inputs: A Tensor of images we want to normalize. Its shape is\n                (1, height, width, num_channels).\n        Returns:\n            outputs: A Tensor of images normalized between -1 and 1.\n                Its shape is the same as the input.\n        """"""\n        inputs = inputs / 255.\n        inputs = (inputs - 0.5) * 2.\n        return inputs\n\n    def get_checkpoint_file(self):\n        return get_checkpoint_file(self._architecture)\n\n    def _get_base_network_vars(self):\n        """"""Returns a list of all the base network\'s variables.""""""\n        if self.pretrained_weights_scope:\n            # We may have defined the base network in a particular scope\n            module_variables = tf.get_collection(\n                tf.GraphKeys.MODEL_VARIABLES,\n                scope=self.pretrained_weights_scope\n            )\n        else:\n            module_variables = snt.get_variables_in_module(\n                self, tf.GraphKeys.MODEL_VARIABLES\n            )\n        assert len(module_variables) > 0\n        return module_variables\n\n    def get_trainable_vars(self):\n        """"""\n        Returns a list of the variables that are trainable.\n\n        If a value for `fine_tune_from` is specified in the config, only the\n        variables starting from the first that contains this string in its name\n        will be trainable. For example, specifying `vgg_16/fc6` for a VGG16\n        will set only the variables in the fully connected layers to be\n        trainable.\n        If `fine_tune_from` is None, then all the variables will be trainable.\n\n        Returns:\n            trainable_variables: a tuple of `tf.Variable`.\n        """"""\n        all_variables = snt.get_variables_in_module(self)\n\n        fine_tune_from = self._config.get(\'fine_tune_from\')\n        if fine_tune_from is None:\n            return all_variables\n\n        # Get the index of the first trainable variable\n        var_iter = enumerate(v.name for v in all_variables)\n        try:\n            index = next(i for i, name in var_iter if fine_tune_from in name)\n        except StopIteration:\n            raise ValueError(\n                \'""{}"" is an invalid value of fine_tune_from for this \'\n                \'architecture.\'.format(fine_tune_from)\n            )\n\n        return all_variables[index:]\n\n    def get_base_network_checkpoint_vars(self):\n        """"""Returns the vars which the base network checkpoint will load into.\n\n        We return a dict which maps a variable name to a variable object. This\n        is needed because the base network may be created inside a particular\n        scope, which the checkpoint may not contain. Therefore we must map\n        each variable to its unscoped name in order to be able to find them in\n        the checkpoint file.\n        """"""\n        variable_scope_len = len(self.variable_scope.name) + 1\n        var_list = self._get_base_network_vars()\n        var_map = {}\n        for var in var_list:\n            var_name = var.op.name\n            checkpoint_var_name = var_name[variable_scope_len:]\n            var_map[checkpoint_var_name] = var\n        return var_map\n'"
luminoth/models/base/base_network_test.py,7,"b""import gc\nimport easydict\nimport numpy as np\nimport tensorflow as tf\n\nfrom luminoth.models.base.base_network import (\n    BaseNetwork, _R_MEAN, _G_MEAN, _B_MEAN, VALID_ARCHITECTURES\n)\n\n\nclass BaseNetworkTest(tf.test.TestCase):\n\n    def setUp(self):\n        self.config = easydict.EasyDict({\n            'architecture': 'vgg_16',\n        })\n        tf.reset_default_graph()\n\n    def testDefaultImageSize(self):\n        m = BaseNetwork(easydict.EasyDict({'architecture': 'vgg_16'}))\n        self.assertEqual(m.default_image_size, 224)\n\n        m = BaseNetwork(easydict.EasyDict({'architecture': 'resnet_v1_50'}))\n        self.assertEqual(m.default_image_size, 224)\n\n    def testSubtractChannels(self):\n        m = BaseNetwork(self.config)\n        inputs = tf.placeholder(tf.float32, [1, 2, 2, 3])\n        subtracted_inputs = m._subtract_channels(inputs)\n        # White image\n        r = 255. - _R_MEAN\n        g = 255. - _G_MEAN\n        b = 255. - _B_MEAN\n        with self.test_session() as sess:\n            res = sess.run(subtracted_inputs, feed_dict={\n                inputs: np.ones([1, 2, 2, 3]) * 255\n            })\n            # Assert close and not equals because of floating point\n            # differences between TF and numpy\n            self.assertAllClose(\n                res,\n                # numpy broadcast multiplication\n                np.ones([1, 2, 2, 3]) * [r, g, b]\n            )\n\n    def testAllArchitectures(self):\n        for architecture in VALID_ARCHITECTURES:\n            self.config.architecture = architecture\n            m = BaseNetwork(self.config)\n            inputs = tf.placeholder(tf.float32, [1, None, None, 3])\n            # Should not fail.\n            m(inputs)\n            # Free up memory for Travis\n            tf.reset_default_graph()\n            gc.collect(generation=2)\n\n    def testTrainableVariables(self):\n        inputs = tf.placeholder(tf.float32, [1, 224, 224, 3])\n\n        model = BaseNetwork(easydict.EasyDict({'architecture': 'vgg_16'}))\n        model(inputs)\n        # Variables in VGG16:\n        #   0 conv1/conv1_1/weights:0\n        #   1 conv1/conv1_1/biases:0\n        #   (...)\n        #   30 fc8/weights:0\n        #   31 fc8/biases:0\n\n        self.assertEqual(len(model.get_trainable_vars()), 32)\n\n        model = BaseNetwork(\n            easydict.EasyDict(\n                {'architecture': 'vgg_16', 'fine_tune_from': 'conv5/conv5_3'}\n            )\n        )\n        model(inputs)\n        # Variables from `conv5/conv5_3` to the end:\n        #   conv5/conv5_3/weights:0\n        #   conv5/conv5_3/biases:0\n        #   fc6/weights:0\n        #   fc6/biases:0\n        #   fc7/weights:0\n        #   fc7/biases:0\n        #   fc8/weights:0\n        #   fc8/biases:0\n        self.assertEqual(len(model.get_trainable_vars()), 8)\n\n        #\n        # Check invalid fine_tune_from raises proper exception\n        #\n        model = BaseNetwork(\n            easydict.EasyDict(\n                {'architecture': 'vgg_16', 'fine_tune_from': 'conv5/conv99'}\n            )\n        )\n        model(inputs)\n        with self.assertRaises(ValueError):\n            model.get_trainable_vars()\n\n\nif __name__ == '__main__':\n    tf.test.main()\n"""
luminoth/models/base/truncated_base_network.py,2,"b'import tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom tensorflow.contrib.slim.nets import resnet_utils, resnet_v1\nfrom luminoth.models.base import BaseNetwork\n\n\nDEFAULT_ENDPOINTS = {\n    \'resnet_v1_50\': \'block3\',\n    \'resnet_v1_101\': \'block3\',\n    \'resnet_v1_152\': \'block3\',\n    \'resnet_v2_50\': \'block3\',\n    \'resnet_v2_101\': \'block3\',\n    \'resnet_v2_152\': \'block3\',\n    \'vgg_16\': \'conv5/conv5_3\',\n}\n\n\nclass TruncatedBaseNetwork(BaseNetwork):\n    """"""\n    Feature extractor for images using a regular CNN.\n\n    By using the notion of an ""endpoint"", we truncate a classification CNN at\n    a certain layer output, and return this partial feature map to be used as\n    a good image representation for other ML tasks.\n    """"""\n\n    def __init__(self, config, name=\'truncated_base_network\', **kwargs):\n        super(TruncatedBaseNetwork, self).__init__(config, name=name, **kwargs)\n        self._endpoint = (\n            config.endpoint or DEFAULT_ENDPOINTS[config.architecture]\n        )\n        self._scope_endpoint = \'{}/{}/{}\'.format(\n            self.module_name, config.architecture, self._endpoint\n        )\n        self._freeze_tail = config.freeze_tail\n        self._use_tail = config.use_tail\n\n    def _build(self, inputs, is_training=False):\n        """"""\n        Args:\n            inputs: A Tensor of shape `(batch_size, height, width, channels)`.\n\n        Returns:\n            feature_map: A Tensor of shape\n                `(batch_size, feature_map_height, feature_map_width, depth)`.\n                The resulting dimensions depend on the CNN architecture, the\n                endpoint used, and the dimensions of the input images.\n        """"""\n        pred = super(TruncatedBaseNetwork, self)._build(\n            inputs, is_training=is_training\n        )\n\n        return self._get_endpoint(dict(pred[\'end_points\']))\n\n    def _build_tail(self, inputs, is_training=False):\n        if not self._use_tail:\n            return inputs\n\n        if self._architecture == \'resnet_v1_101\':\n            train_batch_norm = (\n                is_training and self._config.get(\'train_batch_norm\')\n            )\n            with self._enter_variable_scope():\n                weight_decay = (\n                    self._config.get(\'arg_scope\', {}).get(\'weight_decay\', 0)\n                )\n                with tf.variable_scope(self._architecture, reuse=True):\n                    resnet_arg_scope = resnet_utils.resnet_arg_scope(\n                            batch_norm_epsilon=1e-5,\n                            batch_norm_scale=True,\n                            weight_decay=weight_decay\n                        )\n                    with slim.arg_scope(resnet_arg_scope):\n                        with slim.arg_scope(\n                            [slim.batch_norm], is_training=train_batch_norm\n                        ):\n                            blocks = [\n                                resnet_utils.Block(\n                                    \'block4\',\n                                    resnet_v1.bottleneck,\n                                    [{\n                                        \'depth\': 2048,\n                                        \'depth_bottleneck\': 512,\n                                        \'stride\': 1\n                                    }] * 3\n                                )\n                            ]\n                            proposal_classifier_features = (\n                                resnet_utils.stack_blocks_dense(inputs, blocks)\n                            )\n        else:\n            proposal_classifier_features = inputs\n\n        return proposal_classifier_features\n\n    def get_trainable_vars(self):\n        """"""\n        Returns a list of the variables that are trainable.\n\n        Returns:\n            trainable_variables: a tuple of `tf.Variable`.\n        """"""\n        all_trainable = super(TruncatedBaseNetwork, self).get_trainable_vars()\n\n        # Get the index of the last endpoint scope variable.\n        # For example, if the endpoint for ResNet-50 is set as\n        # ""block4/unit_3/bottleneck_v1/conv2"", then it will get 155,\n        # because the variables (with their indexes) are:\n        #   153 block4/unit_3/bottleneck_v1/conv2/weights:0\n        #   154 block4/unit_3/bottleneck_v1/conv2/BatchNorm/beta:0\n        #   155 block4/unit_3/bottleneck_v1/conv2/BatchNorm/gamma:0\n        var_iter = enumerate(v.name for v in all_trainable)\n        scope_var_index_iter = (\n            i for i, name in var_iter if self._endpoint in name\n        )\n        index = None\n        for index in scope_var_index_iter:\n            pass\n\n        if index is None:\n            # Resulting `trainable_vars` is empty, possibly due to the\n            # `fine_tune_from` starting after the endpoint.\n            trainable_vars = tuple()\n        else:\n            trainable_vars = all_trainable[:index + 1]\n\n        if self._use_tail and not self._freeze_tail:\n            if self._architecture == \'resnet_v1_101\':\n                # Retrieve the trainable vars out of the tail.\n                # TODO: Tail should be configurable too, to avoid hard-coding\n                # the trainable portion to `block4` and allow using something\n                # in block4 as endpoint.\n                var_iter = enumerate(v.name for v in all_trainable)\n                try:\n                    index = next(i for i, name in var_iter if \'block4\' in name)\n                except StopIteration:\n                    raise ValueError(\n                        \'""block4"" not present in the trainable vars retrieved \'\n                        \'from base network.\'\n                    )\n                trainable_vars += all_trainable[index:]\n\n        return trainable_vars\n\n    def _get_endpoint(self, endpoints):\n        """"""\n        Returns the endpoint tensor from the list of possible endpoints.\n\n        Since we already have a dictionary with variable names we should be\n        able to get the desired tensor directly. Unfortunately the variable\n        names change with scope and the scope changes between TensorFlow\n        versions. We opted to just select the tensor for which the variable\n        name ends with the endpoint name we want (it should be just one).\n\n        Args:\n            endpoints: a dictionary with {variable_name: tensor}.\n\n        Returns:\n            endpoint_value: a tensor.\n        """"""\n        for endpoint_key, endpoint_value in endpoints.items():\n            if endpoint_key.endswith(self._scope_endpoint):\n                return endpoint_value\n\n        raise ValueError(\n            \'""{}"" is an invalid value of endpoint for this \'\n            \'architecture.\'.format(self._scope_endpoint)\n        )\n'"
luminoth/models/base/truncated_base_network_test.py,9,"b""import easydict\nimport tensorflow as tf\nimport gc\n\nfrom luminoth.models.base.truncated_base_network import (\n    TruncatedBaseNetwork, DEFAULT_ENDPOINTS\n)\n\n\nclass TruncatedBaseNetworkTest(tf.test.TestCase):\n\n    def setUp(self):\n        self.config = easydict.EasyDict({\n            'architecture': None,\n            'endpoint': None,\n            'freeze_tail': False,\n            'use_tail': True,\n            'output_stride': 16,\n        })\n        tf.reset_default_graph()\n\n    def testAllArchitectures(self):\n        for architecture, endpoint in DEFAULT_ENDPOINTS.items():\n            self.config.architecture = architecture\n            self.config.endpoint = endpoint\n            model = TruncatedBaseNetwork(self.config)\n            image = tf.placeholder(tf.float32, [1, 320, 320, 3])\n            # This should not fail.\n            out = model(image)\n            self.assertEqual(out.get_shape()[:3], (1, 20, 20))\n\n            # Free up memory for travis\n            tf.reset_default_graph()\n            gc.collect(generation=2)\n\n    # TODO: This test fails in Travis because of OOM error.\n    # def testVGG16Output(self):\n    #     self.config.architecture = 'vgg_16'\n    #     self.config.endpoint = None\n    #     model = TruncatedBaseNetwork(self.config)\n\n    #     batch_image_placeholder = tf.placeholder(\n    #         tf.float32, shape=[1, None, None, 3])\n    #     feature_map_tensor = model(batch_image_placeholder)\n\n    #     with self.test_session() as sess:\n    #         # As in the case of a real session we need to initialize\n    #         # variables.\n    #         sess.run(tf.global_variables_initializer())\n    #         width = 192\n    #         height = 192\n    #         feature_map = sess.run(feature_map_tensor, feed_dict={\n    #             batch_image_placeholder: np.random.rand(1, width, height, 3)\n    #         })\n    #         # with width and height between 200 and 200 we should have this\n    #         # output\n    #         self.assertEqual(\n    #             feature_map.shape, (1, width / 16, height / 16, 512)\n    #         )\n\n    def testTrainableVariables(self):\n        inputs = tf.placeholder(tf.float32, [1, 224, 224, 3])\n\n        model = TruncatedBaseNetwork(\n            easydict.EasyDict({\n                'architecture': 'resnet_v1_50',\n                'endpoint': 'block4/unit_3/bottleneck_v1/conv2',\n                'freeze_tail': False,\n                'use_tail': True,\n            })\n        )\n        model(inputs)\n        # Variables in ResNet-50:\n        # (the order of beta and gamma depends on the TensorFlow's version)\n        #   0 conv1/weights:0\n        #   1 conv1/BatchNorm/(beta|gamma):0\n        #   2 conv1/BatchNorm/(beta|gamma):0\n        #   3 block1/unit_1/bottleneck_v1/shortcut/weights:0\n        #   (...)\n        #   153 block4/unit_3/bottleneck_v1/conv2/weights:0\n        #   154 block4/unit_3/bottleneck_v1/conv2/BatchNorm/(beta|gamma):0\n        #   155 block4/unit_3/bottleneck_v1/conv2/BatchNorm/(beta|gamma):0\n        #   --- endpoint ---\n        #   156 block4/unit_3/bottleneck_v1/conv3/weights:0\n        #   157 block4/unit_3/bottleneck_v1/conv3/BatchNorm/(beta|gamma):0\n        #   158 block4/unit_3/bottleneck_v1/conv3/BatchNorm/(beta|gamma):0\n        #   159 logits/weights:0\n        #   160 logits/biases:0\n        trainable_vars = model.get_trainable_vars()\n        self.assertEqual(len(trainable_vars), 156)\n        self.assertEqual(\n            trainable_vars[-3].name,\n            'truncated_base_network/resnet_v1_50/' +\n            'block4/unit_3/bottleneck_v1/conv2/weights:0'\n        )\n\n        model = TruncatedBaseNetwork(\n            easydict.EasyDict({\n                'architecture': 'resnet_v1_50',\n                'endpoint': 'block4/unit_2/bottleneck_v1/conv2',\n                'fine_tune_from': 'block4/unit_2/bottleneck_v1/conv1',\n                'freeze_tail': False,\n                'use_tail': True,\n            })\n        )\n        model(inputs)\n        trainable_vars = model.get_trainable_vars()\n        # Now there should be only 6 trainable vars:\n        #   141 block4/unit_2/bottleneck_v1/conv1/weights:0\n        #   142 block4/unit_2/bottleneck_v1/conv1/BatchNorm/beta:0\n        #   143 block4/unit_2/bottleneck_v1/conv1/BatchNorm/gamma:0\n        #   144 block4/unit_2/bottleneck_v1/conv2/weights:0\n        #   145 block4/unit_2/bottleneck_v1/conv2/BatchNorm/beta:0\n        #   146 block4/unit_2/bottleneck_v1/conv2/BatchNorm/gamma:0\n        self.assertEqual(len(trainable_vars), 6)\n\n        #\n        # Check that we return no vars if fine_tune_from is after the chosen\n        # endpoint (there is nothing to fine-tune!) and tail is frozen.\n        #\n        model = TruncatedBaseNetwork(\n            easydict.EasyDict(\n                {\n                    'architecture': 'resnet_v1_50',\n                    'endpoint': 'block4/unit_2/bottleneck_v1/conv1',\n                    'fine_tune_from': 'block4/unit_2/bottleneck_v1/conv2',\n                    'freeze_tail': True,\n                    'use_tail': True,\n                }\n            )\n        )\n        model(inputs)\n        self.assertEqual(len(model.get_trainable_vars()), 0)\n\n\nif __name__ == '__main__':\n    tf.test.main()\n"""
luminoth/models/base/truncated_vgg.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n# ==============================================================================\n# Taken from tensorboard repository:\n# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/vgg.py\n# Modified to remove the fully connected layers from vgg16.\n# ==============================================================================\n""""""Contains model definitions for versions of the Oxford VGG network.\n\nThese model definitions were introduced in the following technical report:\n\n  Very Deep Convolutional Networks For Large-Scale Image Recognition\n  Karen Simonyan and Andrew Zisserman\n  arXiv technical report, 2015\n  PDF: http://arxiv.org/pdf/1409.1556.pdf\n  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n  CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n\nUsage:\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_a(inputs)\n\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_16(inputs)\n\n@@vgg_a\n@@vgg_16\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.contrib import layers\nfrom tensorflow.contrib.framework.python.ops import arg_scope\nfrom tensorflow.contrib.layers.python.layers import layers as layers_lib\nfrom tensorflow.contrib.layers.python.layers import regularizers\nfrom tensorflow.contrib.layers.python.layers import utils\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import variable_scope\n\n\ndef vgg_arg_scope(weight_decay=0.0005):\n    """"""Defines the VGG arg scope.\n\n    Args:\n      weight_decay: The l2 regularization coefficient.\n\n    Returns:\n      An arg_scope.\n    """"""\n    with arg_scope(\n        [layers.conv2d, layers_lib.fully_connected],\n        activation_fn=nn_ops.relu,\n        weights_regularizer=regularizers.l2_regularizer(weight_decay),\n        biases_initializer=init_ops.zeros_initializer()\n    ):\n        with arg_scope([layers.conv2d], padding=\'SAME\') as arg_sc:\n            return arg_sc\n\n\ndef truncated_vgg_16(inputs, is_training=True, scope=\'vgg_16\'):\n    """"""Oxford Net VGG 16-Layers version D Example.\n\n    For use in SSD object detection network, which has this particular\n    truncated version of VGG16 detailed in its paper.\n\n    Args:\n      inputs: a tensor of size [batch_size, height, width, channels].\n      scope: Optional scope for the variables.\n\n    Returns:\n      the last op containing the conv5 tensor and end_points dict.\n    """"""\n    with variable_scope.variable_scope(scope, \'vgg_16\', [inputs]) as sc:\n        end_points_collection = sc.original_name_scope + \'_end_points\'\n        # Collect outputs for conv2d, fully_connected and max_pool2d.\n        with arg_scope(\n            [layers.conv2d, layers_lib.fully_connected, layers_lib.max_pool2d],\n            outputs_collections=end_points_collection\n        ):\n            net = layers_lib.repeat(\n                inputs, 2, layers.conv2d, 64, [3, 3], scope=\'conv1\')\n            net = layers_lib.max_pool2d(net, [2, 2], scope=\'pool1\')\n            net = layers_lib.repeat(\n                net, 2, layers.conv2d, 128, [3, 3], scope=\'conv2\'\n            )\n            net = layers_lib.max_pool2d(net, [2, 2], scope=\'pool2\')\n            net = layers_lib.repeat(\n                net, 3, layers.conv2d, 256, [3, 3], scope=\'conv3\'\n            )\n            net = layers_lib.max_pool2d(net, [2, 2], scope=\'pool3\')\n            net = layers_lib.repeat(\n                net, 3, layers.conv2d, 512, [3, 3], scope=\'conv4\'\n            )\n            net = layers_lib.max_pool2d(net, [2, 2], scope=\'pool4\')\n            net = layers_lib.repeat(\n                net, 3, layers.conv2d, 512, [3, 3], scope=\'conv5\'\n            )\n            # Convert end_points_collection into a end_point dict.\n            end_points = utils.convert_collection_to_dict(\n                end_points_collection\n            )\n            return net, end_points\n\n\ntruncated_vgg_16.default_image_size = 224\n'"
luminoth/models/fasterrcnn/__init__.py,0,b'from .fasterrcnn import FasterRCNN  # noqa\n'
luminoth/models/fasterrcnn/fasterrcnn.py,33,"b'import numpy as np\nimport sonnet as snt\nimport tensorflow as tf\n\nfrom luminoth.models.fasterrcnn.rcnn import RCNN\nfrom luminoth.models.fasterrcnn.rpn import RPN\nfrom luminoth.models.base import TruncatedBaseNetwork\nfrom luminoth.utils.anchors import generate_anchors_reference\nfrom luminoth.utils.vars import VAR_LOG_LEVELS, variable_summaries\n\n\nclass FasterRCNN(snt.AbstractModule):\n    """"""Faster RCNN Network module\n\n    Builds the Faster RCNN network architecture using different submodules.\n    Calculates the total loss of the model based on the different losses by\n    each of the submodules.\n\n    It is also responsible for building the anchor reference which is used in\n    graph for generating the dynamic anchors.\n    """"""\n    def __init__(self, config, name=\'fasterrcnn\'):\n        super(FasterRCNN, self).__init__(name=name)\n\n        # Main configuration object, it holds not only the necessary\n        # information for this module but also configuration for each of the\n        # different submodules.\n        self._config = config\n\n        # Total number of classes to classify. If not using RCNN then it is not\n        # used. TODO: Make it *more* optional.\n        self._num_classes = config.model.network.num_classes\n\n        # Generate network with RCNN thus allowing for classification of\n        # objects and not just finding them.\n        self._with_rcnn = config.model.network.with_rcnn\n\n        # Turn on debug mode with returns more Tensors which can be used for\n        # better visualization and (of course) debugging.\n        self._debug = config.train.debug\n        self._seed = config.train.seed\n\n        # Anchor config, check out the docs of base_config.yml for a better\n        # understanding of how anchors work.\n        self._anchor_base_size = config.model.anchors.base_size\n        self._anchor_scales = np.array(config.model.anchors.scales)\n        self._anchor_ratios = np.array(config.model.anchors.ratios)\n        self._anchor_stride = config.model.anchors.stride\n\n        # Anchor reference for building dynamic anchors for each image in the\n        # computation graph.\n        self._anchor_reference = generate_anchors_reference(\n            self._anchor_base_size, self._anchor_ratios, self._anchor_scales\n        )\n\n        # Total number of anchors per point.\n        self._num_anchors = self._anchor_reference.shape[0]\n\n        # Weights used to sum each of the losses of the submodules\n        self._rpn_cls_loss_weight = config.model.loss.rpn_cls_loss_weight\n        self._rpn_reg_loss_weight = config.model.loss.rpn_reg_loss_weights\n\n        self._rcnn_cls_loss_weight = config.model.loss.rcnn_cls_loss_weight\n        self._rcnn_reg_loss_weight = config.model.loss.rcnn_reg_loss_weights\n        self._losses_collections = [\'fastercnn_losses\']\n\n        # We want the pretrained model to be outside the FasterRCNN name scope.\n        self.base_network = TruncatedBaseNetwork(config.model.base_network)\n\n    def _build(self, image, gt_boxes=None, is_training=False):\n        """"""\n        Returns bounding boxes and classification probabilities.\n\n        Args:\n            image: A tensor with the image.\n                Its shape should be `(height, width, 3)`.\n            gt_boxes: A tensor with all the ground truth boxes of that image.\n                Its shape should be `(num_gt_boxes, 5)`\n                Where for each gt box we have (x1, y1, x2, y2, label),\n                in that order.\n            is_training: A boolean to whether or not it is used for training.\n\n        Returns:\n            classification_prob: A tensor with the softmax probability for\n                each of the bounding boxes found in the image.\n                Its shape should be: (num_bboxes, num_categories + 1)\n            classification_bbox: A tensor with the bounding boxes found.\n                It\'s shape should be: (num_bboxes, 4). For each of the bboxes\n                we have (x1, y1, x2, y2)\n        """"""\n        if gt_boxes is not None:\n            gt_boxes = tf.cast(gt_boxes, tf.float32)\n        # A Tensor with the feature map for the image,\n        # its shape should be `(feature_height, feature_width, 512)`.\n        # The shape depends of the pretrained network in use.\n\n        # Set rank and last dimension before using base network\n        # TODO: Why does it loose information when using queue?\n        image.set_shape((None, None, 3))\n\n        conv_feature_map = self.base_network(\n            tf.expand_dims(image, 0), is_training=is_training\n        )\n\n        # The RPN submodule which generates proposals of objects.\n        self._rpn = RPN(\n            self._num_anchors, self._config.model.rpn,\n            debug=self._debug, seed=self._seed\n        )\n        if self._with_rcnn:\n            # The RCNN submodule which classifies RPN\'s proposals and\n            # classifies them as background or a specific class.\n            self._rcnn = RCNN(\n                self._num_classes, self._config.model.rcnn,\n                debug=self._debug, seed=self._seed\n            )\n\n        image_shape = tf.shape(image)[0:2]\n\n        variable_summaries(\n            conv_feature_map, \'conv_feature_map\', \'reduced\'\n        )\n\n        # Generate anchors for the image based on the anchor reference.\n        all_anchors = self._generate_anchors(tf.shape(conv_feature_map))\n        rpn_prediction = self._rpn(\n            conv_feature_map, image_shape, all_anchors,\n            gt_boxes=gt_boxes, is_training=is_training\n        )\n\n        prediction_dict = {\n            \'rpn_prediction\': rpn_prediction,\n        }\n\n        if self._debug:\n            prediction_dict[\'image\'] = image\n            prediction_dict[\'image_shape\'] = image_shape\n            prediction_dict[\'all_anchors\'] = all_anchors\n            prediction_dict[\'anchor_reference\'] = tf.convert_to_tensor(\n                self._anchor_reference\n            )\n            if gt_boxes is not None:\n                prediction_dict[\'gt_boxes\'] = gt_boxes\n            prediction_dict[\'conv_feature_map\'] = conv_feature_map\n\n        if self._with_rcnn:\n            proposals = tf.stop_gradient(rpn_prediction[\'proposals\'])\n            classification_pred = self._rcnn(\n                conv_feature_map, proposals,\n                image_shape, self.base_network,\n                gt_boxes=gt_boxes, is_training=is_training\n            )\n\n            prediction_dict[\'classification_prediction\'] = classification_pred\n\n        return prediction_dict\n\n    def loss(self, prediction_dict, return_all=False):\n        """"""Compute the joint training loss for Faster RCNN.\n\n        Args:\n            prediction_dict: The output dictionary of the _build method from\n                which we use two different main keys:\n\n                rpn_prediction: A dictionary with the output Tensors from the\n                    RPN.\n                classification_prediction: A dictionary with the output Tensors\n                    from the RCNN.\n\n        Returns:\n            If `return_all` is False, a tensor for the total loss. If True, a\n            dict with all the internal losses (RPN\'s, RCNN\'s, regularization\n            and total loss).\n        """"""\n\n        with tf.name_scope(\'losses\'):\n            rpn_loss_dict = self._rpn.loss(\n                prediction_dict[\'rpn_prediction\']\n            )\n\n            # Losses have a weight assigned, we multiply by them before saving\n            # them.\n            rpn_loss_dict[\'rpn_cls_loss\'] = (\n                rpn_loss_dict[\'rpn_cls_loss\'] * self._rpn_cls_loss_weight)\n            rpn_loss_dict[\'rpn_reg_loss\'] = (\n                rpn_loss_dict[\'rpn_reg_loss\'] * self._rpn_reg_loss_weight)\n\n            prediction_dict[\'rpn_loss_dict\'] = rpn_loss_dict\n\n            if self._with_rcnn:\n                rcnn_loss_dict = self._rcnn.loss(\n                    prediction_dict[\'classification_prediction\']\n                )\n\n                rcnn_loss_dict[\'rcnn_cls_loss\'] = (\n                    rcnn_loss_dict[\'rcnn_cls_loss\'] *\n                    self._rcnn_cls_loss_weight\n                )\n                rcnn_loss_dict[\'rcnn_reg_loss\'] = (\n                    rcnn_loss_dict[\'rcnn_reg_loss\'] *\n                    self._rcnn_reg_loss_weight\n                )\n\n                prediction_dict[\'rcnn_loss_dict\'] = rcnn_loss_dict\n            else:\n                rcnn_loss_dict = {}\n\n            all_losses_items = (\n                list(rpn_loss_dict.items()) + list(rcnn_loss_dict.items()))\n\n            for loss_name, loss_tensor in all_losses_items:\n                tf.summary.scalar(\n                    loss_name, loss_tensor,\n                    collections=self._losses_collections\n                )\n                # We add losses to the losses collection instead of manually\n                # summing them just in case somebody wants to use it in another\n                # place.\n                tf.losses.add_loss(loss_tensor)\n\n            # Regularization loss is automatically saved by TensorFlow, we log\n            # it differently so we can visualize it independently.\n            regularization_loss = tf.losses.get_regularization_loss()\n            # Total loss without regularization\n            no_reg_loss = tf.losses.get_total_loss(\n                add_regularization_losses=False\n            )\n            total_loss = tf.losses.get_total_loss()\n\n            tf.summary.scalar(\n                \'total_loss\', total_loss,\n                collections=self._losses_collections\n            )\n            tf.summary.scalar(\n                \'no_reg_loss\', no_reg_loss,\n                collections=self._losses_collections\n            )\n            tf.summary.scalar(\n                \'regularization_loss\', regularization_loss,\n                collections=self._losses_collections\n            )\n\n            if return_all:\n                loss_dict = {\n                    \'total_loss\': total_loss,\n                    \'no_reg_loss\': no_reg_loss,\n                    \'regularization_loss\': regularization_loss,\n                }\n\n                for loss_name, loss_tensor in all_losses_items:\n                    loss_dict[loss_name] = loss_tensor\n\n                return loss_dict\n\n            # We return the total loss, which includes:\n            # - rpn loss\n            # - rcnn loss (if activated)\n            # - regularization loss\n            return total_loss\n\n    def _generate_anchors(self, feature_map_shape):\n        """"""Generate anchor for an image.\n\n        Using the feature map, the output of the pretrained network for an\n        image, and the anchor_reference generated using the anchor config\n        values. We generate a list of anchors.\n\n        Anchors are just fixed bounding boxes of different ratios and sizes\n        that are uniformly generated throught the image.\n\n        Args:\n            feature_map_shape: Shape of the convolutional feature map used as\n                input for the RPN. Should be (batch, height, width, depth).\n\n        Returns:\n            all_anchors: A flattened Tensor with all the anchors of shape\n                `(num_anchors_per_points * feature_width * feature_height, 4)`\n                using the (x1, y1, x2, y2) convention.\n        """"""\n        with tf.variable_scope(\'generate_anchors\'):\n            grid_width = feature_map_shape[2]  # width\n            grid_height = feature_map_shape[1]  # height\n            shift_x = tf.range(grid_width) * self._anchor_stride\n            shift_y = tf.range(grid_height) * self._anchor_stride\n            shift_x, shift_y = tf.meshgrid(shift_x, shift_y)\n\n            shift_x = tf.reshape(shift_x, [-1])\n            shift_y = tf.reshape(shift_y, [-1])\n\n            shifts = tf.stack(\n                [shift_x, shift_y, shift_x, shift_y],\n                axis=0\n            )\n\n            shifts = tf.transpose(shifts)\n            # Shifts now is a (H x W, 4) Tensor\n\n            # Expand dims to use broadcasting sum.\n            all_anchors = (\n                np.expand_dims(self._anchor_reference, axis=0) +\n                tf.expand_dims(shifts, axis=1)\n            )\n\n            # Flatten\n            all_anchors = tf.reshape(\n                all_anchors, (-1, 4)\n            )\n            return all_anchors\n\n    @property\n    def summary(self):\n        """"""\n        Generate merged summary of all the sub-summaries used inside the\n        Faster R-CNN network.\n        """"""\n        summaries = [\n            tf.summary.merge_all(key=\'rpn\'),\n        ]\n\n        summaries.append(\n            tf.summary.merge_all(key=self._losses_collections[0])\n        )\n\n        if self._with_rcnn:\n            summaries.append(tf.summary.merge_all(key=\'rcnn\'))\n\n        return tf.summary.merge(summaries)\n\n    @property\n    def vars_summary(self):\n        return {\n            key: tf.summary.merge_all(key=collection)\n            for key, collections in VAR_LOG_LEVELS.items()\n            for collection in collections\n        }\n\n    def get_trainable_vars(self):\n        """"""Get trainable vars included in the module.\n        """"""\n        trainable_vars = snt.get_variables_in_module(self)\n        if self._config.model.base_network.trainable:\n            pretrained_trainable_vars = self.base_network.get_trainable_vars()\n            if len(pretrained_trainable_vars):\n                tf.logging.info(\n                    \'Training {} vars from pretrained module; \'\n                    \'from ""{}"" to ""{}"".\'.format(\n                        len(pretrained_trainable_vars),\n                        pretrained_trainable_vars[0].name,\n                        pretrained_trainable_vars[-1].name,\n                    )\n                )\n            else:\n                tf.logging.info(\'No vars from pretrained module to train.\')\n            trainable_vars += pretrained_trainable_vars\n        else:\n            tf.logging.info(\'Not training variables from pretrained module\')\n\n        return trainable_vars\n\n    def get_base_network_checkpoint_vars(self):\n        return self.base_network.get_base_network_checkpoint_vars()\n\n    def get_checkpoint_file(self):\n        return self.base_network.get_checkpoint_file()\n'"
luminoth/models/fasterrcnn/fasterrcnn_test.py,48,"b'import numpy as np\nimport tensorflow as tf\n\nfrom easydict import EasyDict\nfrom luminoth.models.fasterrcnn import FasterRCNN\nfrom luminoth.utils.bbox_transform import clip_boxes\n\n\nclass FasterRCNNNetworkTest(tf.test.TestCase):\n\n    def setUp(self):\n        super(FasterRCNNNetworkTest, self).setUp()\n        # Setup\n        self.config = EasyDict({\n            \'train\': {\n                \'debug\': True,\n                \'seed\': None,\n            },\n            \'model\': {\n                \'network\': {\n                    \'num_classes\': 20,\n                    \'with_rcnn\': True\n                },\n                \'anchors\': {\n                    \'base_size\': 256,\n                    \'scales\': [0.5, 1, 2],\n                    \'ratios\': [0.5, 1, 2],\n                    \'stride\': 16\n                },\n                \'loss\': {\n                    \'rpn_cls_loss_weight\': 1.0,\n                    \'rpn_reg_loss_weights\': 2.0,\n                    \'rcnn_cls_loss_weight\': 1.0,\n                    \'rcnn_reg_loss_weights\': 2.0,\n                },\n                \'base_network\': {\n                    \'architecture\': \'vgg_16\',\n                    \'trainable\': True,\n                    \'endpoint\': \'conv5/conv5_1\',\n                    \'download\': False,\n                    \'fine_tune_from\': \'conv4/conv4_2\',\n                    \'freeze_tail\': False,\n                    \'use_tail\': True,\n                    \'arg_scope\': {\n                        \'weight_decay\': 0.0005,\n                    }\n                },\n                \'rcnn\': {\n                    \'enabled\': True,\n                    \'layer_sizes\': [4096, 4096],\n                    \'dropout_keep_prob\': 1.0,\n                    \'activation_function\': \'relu6\',\n                    \'l2_regularization_scale\': 0.0005,\n                    \'l1_sigma\': 3.0,\n                    \'use_mean\': False,\n                    \'target_normalization_variances\': [1., 1.],\n                    \'rcnn_initializer\': {\n                        \'type\': \'variance_scaling_initializer\',\n                        \'factor\': 1.0,\n                        \'uniform\': True,\n                        \'mode\': \'FAN_AVG\'\n                    },\n                    \'bbox_initializer\': {\n                        \'type\': \'variance_scaling_initializer\',\n                        \'factor\': 1.0,\n                        \'uniform\': True,\n                        \'mode\': \'FAN_AVG\'\n                    },\n                    \'cls_initializer\': {\n                        \'type\': \'variance_scaling_initializer\',\n                        \'factor\': 1.0,\n                        \'uniform\': True,\n                        \'mode\': \'FAN_AVG\'\n                    },\n                    \'roi\': {\n                        \'pooling_mode\': \'crop\',\n                        \'pooled_width\': 7,\n                        \'pooled_height\': 7,\n                        \'padding\': \'VALID\'\n                    },\n                    \'proposals\': {\n                        \'class_max_detections\': 100,\n                        \'class_nms_threshold\': 0.6,\n                        \'total_max_detections\': 300,\n                        \'min_prob_threshold\': 0.0\n                    },\n                    \'target\': {\n                        \'foreground_fraction\': 0.25,\n                        \'minibatch_size\': 64,\n                        \'foreground_threshold\': 0.5,\n                        \'background_threshold_high\': 0.5,\n                        \'background_threshold_low\': 0.1,\n                    }\n                },\n                \'rpn\': {\n                    \'num_channels\': 512,\n                    \'kernel_shape\': [3, 3],\n                    \'rpn_initializer\': {\n                        \'type\': \'variance_scaling_initializer\',\n                        \'factor\': 1.0,\n                        \'mode\': \'FAN_AVG\',\n                        \'uniform\': True,\n                    },\n                    \'cls_initializer\': {\n                        \'type\': \'truncated_normal_initializer\',\n                        \'mean\': 0.0,\n                        \'stddev\': 0.01,\n                    },\n                    \'bbox_initializer\': {\n                        \'type\': \'truncated_normal_initializer\',\n                        \'mean\': 0.0,\n                        \'stddev\': 0.01,\n                    },\n                    \'activation_function\': \'relu6\',\n                    \'l2_regularization_scale\': 0.0005,\n                    \'l1_sigma\': 3.0,\n                    \'proposals\': {\n                        \'pre_nms_top_n\': 12000,\n                        \'post_nms_top_n\': 2000,\n                        \'nms_threshold\': 0.6,\n                        \'min_size\': 0,\n                        \'clip_after_nms\': False,\n                        \'filter_outside_anchors\': False,\n                        \'apply_nms\': True,\n                        \'min_prob_threshold\': 0.0,\n                    },\n                    \'target\': {\n                        \'allowed_border\': 0,\n                        \'clobber_positives\': False,\n                        \'foreground_threshold\': 0.7,\n                        \'background_threshold_high\': 0.3,\n                        \'background_threshold_low\': 0.,\n                        \'foreground_fraction\': 0.5,\n                        \'minibatch_size\': 256,\n                    }\n                }\n            }\n        })\n\n        self.image_size = (600, 800)\n        self.image = np.random.randint(low=0, high=255, size=(600, 800, 3))\n        self.gt_boxes = np.array([\n            [10, 10, 26, 28, 1],\n            [10, 10, 20, 22, 1],\n            [10, 11, 20, 21, 1],\n            [19, 30, 31, 33, 1],\n        ])\n        tf.reset_default_graph()\n\n    def _run_network(self):\n        image = tf.placeholder(\n            tf.float32, shape=self.image.shape)\n        gt_boxes = tf.placeholder(\n            tf.float32, shape=self.gt_boxes.shape)\n        model = FasterRCNN(self.config)\n\n        results = model(image, gt_boxes)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            results = sess.run(results, feed_dict={\n                gt_boxes: self.gt_boxes,\n                image: self.image,\n            })\n            return results\n\n    def _gen_anchors(self, config, feature_map_shape):\n        model = FasterRCNN(config)\n        results = model._generate_anchors(feature_map_shape)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            results = sess.run(results)\n            return results\n\n    def _get_losses(self, config, prediction_dict, image_size):\n        image = tf.placeholder(\n            tf.float32, shape=self.image.shape)\n        gt_boxes = tf.placeholder(\n            tf.float32, shape=self.gt_boxes.shape)\n        model = FasterRCNN(config)\n        model(image, gt_boxes, is_training=True)\n        all_losses = model.loss(prediction_dict, return_all=True)\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            all_losses = sess.run(all_losses)\n            return all_losses\n\n    def testBasic(self):\n        """"""\n        Test basic output of the FasterCnnNetwork\n        """"""\n        results = self._run_network()\n        class_prediction = results[\'classification_prediction\']\n        rpn_prediction = results[\'rpn_prediction\']\n\n        # Check that every object is defined by 4 coordinates\n        self.assertEqual(\n            class_prediction[\'objects\'].shape[1],\n            4\n        )\n\n        # Check we get objects clipped to the image.\n        self.assertAllEqual(\n            clip_boxes(class_prediction[\'objects\'], self.image_size),\n            class_prediction[\'objects\']\n        )\n\n        self.assertEqual(\n            class_prediction[\'labels\'].shape[0],\n            class_prediction[\'objects\'].shape[0]\n        )\n\n        # Check that every object label is less or equal than \'num_classes\'\n        self.assertTrue(\n            np.less_equal(class_prediction[\'labels\'],\n                          self.config.model.network.num_classes).all()\n        )\n\n        # Check that the sum of class probabilities is 1\n        self.assertAllClose(\n            np.sum(class_prediction[\'rcnn\'][\'cls_prob\'], axis=1),\n            np.ones((class_prediction[\'rcnn\'][\'cls_prob\'].shape[0]))\n        )\n\n        # Check that the sum of rpn class probabilities is 1\n        self.assertAllClose(\n            np.sum(rpn_prediction[\'rpn_cls_prob\'], axis=1),\n            np.ones((rpn_prediction[\'rpn_cls_prob\'].shape[0]))\n        )\n\n        # Check that every rpn proposal has 4 coordinates\n        self.assertEqual(\n            rpn_prediction[\'proposals\'].shape[1],\n            4\n        )\n\n        # Check we get rpn proposals clipped to the image.\n        self.assertAllEqual(\n            clip_boxes(rpn_prediction[\'proposals\'], self.image_size),\n            rpn_prediction[\'proposals\']\n        )\n\n    def testAnchors(self):\n        """"""\n        Tests about the anchors generated by the FasterRCNN\n        """"""\n        results = self._run_network()\n\n        # Check we get anchors clipped to the image.\n        self.assertAllEqual(\n            clip_boxes(results[\'all_anchors\'], self.image_size),\n            results[\'all_anchors\']\n        )\n\n        feature_map = np.random.randint(low=0, high=255, size=(1, 32, 32, 1))\n        config = self.config\n        config.model.anchors.base_size = 16\n        config.model.anchors.scales = [0.5, 1, 2]\n        config.model.anchors.ratios = [0.5, 1, 2]\n        config.model.anchors.stride = 1  # image is 32 x 32\n\n        anchors = self._gen_anchors(config, feature_map.shape)\n\n        # Check the amount of anchors generated is correct:\n        # 9216 = 32^2 * config.anchor.scales * config.anchor.ratios = 1024 * 9\n        self.assertEqual(anchors.shape, (9216, 4))\n\n        anchor_widths = anchors[:, 2] - anchors[:, 0]\n        anchor_heights = anchors[:, 3] - anchors[:, 1]\n\n        # Since we are using equal scales and ratios, the set of unique heights\n        # and widths must be the same.\n        self.assertAllEqual(\n            np.unique(anchor_widths), np.unique(anchor_heights)\n        )\n\n        anchor_areas = anchor_widths * anchor_heights\n\n        # We have 9 possible anchors areas, minus 3 repeated ones. 6 unique.\n        self.assertAllEqual(np.unique(anchor_areas).shape[0], 6)\n\n        # Check the anchors cover all the image.\n        # TODO: Check with values calculated from config.\n        self.assertEqual(np.min(anchors[:, 0]), -22)\n        self.assertEqual(np.max(anchors[:, 0]), 29)\n\n        self.assertEqual(np.min(anchors[:, 1]), -22)\n        self.assertEqual(np.max(anchors[:, 1]), 29)\n\n        self.assertEqual(np.min(anchors[:, 2]), 2)\n        self.assertEqual(np.max(anchors[:, 2]), 53)\n\n        self.assertEqual(np.min(anchors[:, 3]), 2)\n        self.assertEqual(np.max(anchors[:, 3]), 53)\n\n        stride = config.model.anchors.stride\n        # Check values are sequential.\n        self._assert_sequential_values(anchors[:, 0], stride)\n        self._assert_sequential_values(anchors[:, 1], stride)\n        self._assert_sequential_values(anchors[:, 2], stride)\n        self._assert_sequential_values(anchors[:, 3], stride)\n\n    def testLoss(self):\n        """"""\n        Tests the loss of the FasterRCNN\n        """"""\n\n        # Create prediction_dict\'s structure\n        prediction_dict_random = {\n            \'rpn_prediction\': {},\n            \'classification_prediction\': {\n                \'rcnn\': {\n                    \'cls_score\': None,\n                    \'bbox_offsets\': None\n                },\n                \'target\': {},\n                \'_debug\': {\n                    \'losses\': {}\n                }\n            }\n        }\n        prediction_dict_perf = {\n            \'rpn_prediction\': {},\n            \'classification_prediction\': {\n                \'rcnn\': {\n                    \'cls_score\': None,\n                    \'bbox_offsets\': None\n                },\n                \'target\': {},\n                \'_debug\': {\n                    \'losses\': {}\n                }\n            }\n        }\n\n        # Set seeds for stable results\n        rand_seed = 13\n        target_seed = 43\n        image_size = (60, 80)\n        num_anchors = 1000\n\n        config = EasyDict(self.config)\n        config.model.rpn.l2_regularization_scale = 0.0\n        config.model.rcnn.l2_regularization_scale = 0.0\n        config.model.base_network.arg_scope.weight_decay = 0.0\n\n        #   RPN\n\n        # Random generation of cls_targets for rpn\n        # where:\n        #       {-1}:   Ignore\n        #       { 0}:   Background\n        #       { 1}:   Object\n        rpn_cls_target = tf.floor(tf.random_uniform(\n            [num_anchors],\n            minval=-1,\n            maxval=2,\n            dtype=tf.float32,\n            seed=target_seed,\n            name=None\n        ))\n\n        # Creation of cls_scores with:\n        #   score 100 in correct class\n        #   score 0 in wrong class\n\n        # Generation of opposite cls_score for rpn\n        rpn_cls_score = tf.cast(\n            tf.one_hot(\n                tf.cast(\n                    tf.mod(tf.identity(rpn_cls_target) + 1, 2),\n                    tf.int32),\n                depth=2,\n                on_value=10),\n            tf.float32\n        )\n        # Generation of correct cls_score for rpn\n        rpn_cls_perf_score = tf.cast(\n            tf.one_hot(\n                tf.cast(\n                    tf.identity(rpn_cls_target),\n                    tf.int32),\n                depth=2,\n                on_value=100),\n            tf.float32\n        )\n\n        # Random generation of target bbox deltas\n        rpn_bbox_target = tf.floor(tf.random_uniform(\n            [num_anchors, 4],\n            minval=-1,\n            maxval=1,\n            dtype=tf.float32,\n            seed=target_seed,\n            name=None\n        ))\n\n        # Random generation of predicted bbox deltas\n        rpn_bbox_predictions = tf.floor(tf.random_uniform(\n            [num_anchors, 4],\n            minval=-1,\n            maxval=1,\n            dtype=tf.float32,\n            seed=rand_seed,\n            name=None\n        ))\n\n        prediction_dict_random[\'rpn_prediction\'][\n            \'rpn_cls_score\'] = rpn_cls_score\n        prediction_dict_random[\'rpn_prediction\'][\n            \'rpn_cls_target\'] = rpn_cls_target\n        prediction_dict_random[\'rpn_prediction\'][\n            \'rpn_bbox_target\'] = rpn_bbox_target\n        prediction_dict_random[\'rpn_prediction\'][\n            \'rpn_bbox_pred\'] = rpn_bbox_predictions\n\n        prediction_dict_perf[\'rpn_prediction\'][\n            \'rpn_cls_score\'] = rpn_cls_perf_score\n        prediction_dict_perf[\'rpn_prediction\'][\n            \'rpn_cls_target\'] = rpn_cls_target\n        prediction_dict_perf[\'rpn_prediction\'][\n            \'rpn_bbox_target\'] = rpn_bbox_target\n        prediction_dict_perf[\'rpn_prediction\'][\n            \'rpn_bbox_pred\'] = rpn_bbox_target\n\n        #   RCNN\n\n        # Set the number of classes\n        num_classes = config.model.network.num_classes\n\n        # Randomly generate the bbox_offsets for the correct class = 1\n        prediction_dict_random[\'classification_prediction\'][\'target\'] = {\n            \'bbox_offsets\': tf.random_uniform(\n                [1, 4],\n                minval=-1,\n                maxval=1,\n                dtype=tf.float32,\n                seed=target_seed,\n                name=None\n            ),\n            \'cls\': [1]\n        }\n\n        # Set the same bbox_offsets and cls for the perfect prediction\n        prediction_dict_perf[\n            \'classification_prediction\'][\'target\'] = prediction_dict_random[\n                \'classification_prediction\'][\'target\'].copy()\n\n        # Generate random scores for the num_classes + the background class\n        rcnn_cls_score = tf.random_uniform(\n            [1, num_classes + 1],\n            minval=-100,\n            maxval=100,\n            dtype=tf.float32,\n            seed=rand_seed,\n            name=None\n        )\n\n        # Generate a perfect prediction with the correct class score = 100\n        # and the rest set to 0\n        rcnn_cls_perf_score = tf.cast(\n            tf.one_hot(\n                [1], depth=num_classes + 1,\n                on_value=100\n            ),\n            tf.float32\n        )\n\n        # Generate the random delta prediction for each class\n        rcnn_bbox_offsets = tf.random_uniform(\n            [1, num_classes * 4],\n            minval=-1,\n            maxval=1,\n            dtype=tf.float32,\n            seed=rand_seed,\n            name=None\n        )\n\n        # Copy the random prediction and set the correct class prediction\n        # as the target one\n        target_bbox_offsets = prediction_dict_random[\n            \'classification_prediction\'][\'target\'][\'bbox_offsets\']\n        initial_val = 1 * 4  # cls value * 4\n        rcnn_bbox_perf_offsets = tf.Variable(tf.reshape(\n            tf.random_uniform(\n                [1, num_classes * 4],\n                minval=-1,\n                maxval=1,\n                dtype=tf.float32,\n                seed=target_seed,\n                name=None\n            ), [-1]))\n        rcnn_bbox_perf_offsets = tf.reshape(\n            tf.scatter_update(\n                rcnn_bbox_perf_offsets,\n                tf.range(initial_val, initial_val + 4),\n                tf.reshape(target_bbox_offsets, [-1])\n            ),\n            [1, -1])\n\n        prediction_dict_random[\'classification_prediction\'][\n            \'rcnn\'][\'cls_score\'] = rcnn_cls_score\n        prediction_dict_random[\'classification_prediction\'][\n            \'rcnn\'][\'bbox_offsets\'] = rcnn_bbox_offsets\n\n        prediction_dict_perf[\'classification_prediction\'][\n            \'rcnn\'][\'cls_score\'] = rcnn_cls_perf_score\n        prediction_dict_perf[\'classification_prediction\'][\n            \'rcnn\'][\'bbox_offsets\'] = rcnn_bbox_perf_offsets\n\n        loss_perfect = self._get_losses(\n            config, prediction_dict_perf, image_size)\n        loss_random = self._get_losses(\n            config, prediction_dict_random, image_size)\n\n        loss_random_compare = {\n            \'rcnn_cls_loss\': 5,\n            \'rcnn_reg_loss\': 3,\n            \'rpn_cls_loss\': 5,\n            \'rpn_reg_loss\': 3,\n            \'no_reg_loss\': 16,\n            \'regularization_loss\': 0,\n            \'total_loss\': 22,\n        }\n        for loss in loss_random:\n            self.assertGreaterEqual(\n                loss_random[loss],\n                loss_random_compare[loss],\n                loss\n            )\n            self.assertEqual(\n                loss_perfect[loss],\n                0, loss\n            )\n\n    def _assert_sequential_values(self, values, delta=1):\n        unique_values = np.unique(values)\n        paired_values = np.column_stack(\n            (unique_values[:-1], unique_values[1:])\n        )\n        self.assertAllEqual(\n            paired_values[:, 1] - paired_values[:, 0],\n            np.ones((paired_values.shape[0], ), np.int)\n        )\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
luminoth/models/fasterrcnn/rcnn.py,38,"b'import sonnet as snt\nimport tensorflow as tf\n\nfrom luminoth.models.fasterrcnn.rcnn_proposal import RCNNProposal\nfrom luminoth.models.fasterrcnn.rcnn_target import RCNNTarget\nfrom luminoth.models.fasterrcnn.roi_pool import ROIPoolingLayer\nfrom luminoth.utils.losses import smooth_l1_loss\nfrom luminoth.utils.vars import (\n    get_initializer, layer_summaries, variable_summaries,\n    get_activation_function\n)\n\n\nclass RCNN(snt.AbstractModule):\n    """"""RCNN: Region-based Convolutional Neural Network.\n\n    Given region proposals (bounding boxes on an image) and a feature map of\n    that image, RCNN adjusts the bounding boxes and classifies each region as\n    either background or a specific object class.\n\n    Steps:\n        1. Region of Interest Pooling. Extract features from the feature map\n           (based on the proposals) and convert into fixed size tensors\n           (applying extrapolation).\n        2. Two fully connected layers generate a smaller tensor for each\n           region.\n        3. A fully conected layer outputs the probability distribution over the\n           classes (plus a background class), and another fully connected layer\n           outputs the bounding box regressions (one 4-d regression for each of\n           the possible classes).\n\n    Using the class probability, filter regions classified as background. For\n    the remaining regions, use the class probability together with the\n    corresponding bounding box regression offsets to generate the final object\n    bounding boxes, with classes and probabilities assigned.\n    """"""\n\n    def __init__(self, num_classes, config, debug=False, seed=None,\n                 name=\'rcnn\'):\n        super(RCNN, self).__init__(name=name)\n        self._num_classes = num_classes\n        # List of the fully connected layer sizes used before classifying and\n        # adjusting the bounding box.\n        self._layer_sizes = config.layer_sizes\n        self._activation = get_activation_function(config.activation_function)\n        self._dropout_keep_prob = config.dropout_keep_prob\n        self._use_mean = config.use_mean\n        self._variances = config.target_normalization_variances\n\n        self._rcnn_initializer = get_initializer(\n            config.rcnn_initializer, seed=seed\n        )\n        self._cls_initializer = get_initializer(\n            config.cls_initializer, seed=seed\n        )\n        self._bbox_initializer = get_initializer(\n            config.bbox_initializer, seed=seed\n        )\n        self.regularizer = tf.contrib.layers.l2_regularizer(\n            scale=config.l2_regularization_scale)\n\n        self._l1_sigma = config.l1_sigma\n\n        # Debug mode makes the module return more detailed Tensors which can be\n        # useful for debugging.\n        self._debug = debug\n        self._config = config\n        self._seed = seed\n\n    def _instantiate_layers(self):\n        # We define layers as an array since they are simple fully connected\n        # ones and it should be easy to tune it from the network config.\n        self._layers = [\n            snt.Linear(\n                layer_size,\n                name=\'fc_{}\'.format(i),\n                initializers={\'w\': self._rcnn_initializer},\n                regularizers={\'w\': self.regularizer},\n            )\n            for i, layer_size in enumerate(self._layer_sizes)\n        ]\n        # We define the classifier layer having a num_classes + 1 background\n        # since we want to be able to predict if the proposal is background as\n        # well.\n        self._classifier_layer = snt.Linear(\n            self._num_classes + 1, name=\'fc_classifier\',\n            initializers={\'w\': self._cls_initializer},\n            regularizers={\'w\': self.regularizer},\n        )\n\n        # The bounding box adjustment layer has 4 times the number of classes\n        # We choose which to use depending on the output of the classifier\n        # layer\n        self._bbox_layer = snt.Linear(\n            self._num_classes * 4, name=\'fc_bbox\',\n            initializers={\'w\': self._bbox_initializer},\n            regularizers={\'w\': self.regularizer}\n        )\n\n        # ROIPoolingLayer is used to extract the feature from the feature map\n        # using the proposals.\n        self._roi_pool = ROIPoolingLayer(self._config.roi, debug=self._debug)\n        # RCNNTarget is used to define a minibatch and the correct values for\n        # each of the proposals.\n        self._rcnn_target = RCNNTarget(\n            self._num_classes, self._config.target, variances=self._variances,\n            seed=self._seed\n        )\n        # RCNNProposal generates the final bounding boxes and tries to remove\n        # duplicates.\n        self._rcnn_proposal = RCNNProposal(\n            self._num_classes, self._config.proposals,\n            variances=self._variances\n        )\n\n    def _build(self, conv_feature_map, proposals, im_shape, base_network,\n               gt_boxes=None, is_training=False):\n        """"""\n        Classifies & refines proposals based on the pooled feature map.\n\n        Args:\n            conv_feature_map: The feature map of the image, extracted\n                using the pretrained network.\n                Shape: (num_proposals, pool_height, pool_width, 512).\n            proposals: A Tensor with the bounding boxes proposed by the RPN.\n                Shape: (total_num_proposals, 4).\n                Encoding: (x1, y1, x2, y2).\n            im_shape: A Tensor with the shape of the image in the form of\n                (image_height, image_width).\n            gt_boxes (optional): A Tensor with the ground truth boxes of the\n                image.\n                Shape: (total_num_gt, 5).\n                Encoding: (x1, y1, x2, y2, label).\n            is_training (optional): A boolean to determine if we are just using\n                the module for training or just inference.\n\n        Returns:\n            prediction_dict: a dict with the object predictions.\n                It should have the keys:\n                objects:\n                labels:\n                probs:\n\n                rcnn:\n                target:\n\n        """"""\n        self._instantiate_layers()\n\n        prediction_dict = {\'_debug\': {}}\n\n        if gt_boxes is not None:\n            proposals_target, bbox_offsets_target = self._rcnn_target(\n                proposals, gt_boxes)\n\n            if is_training:\n                with tf.name_scope(\'prepare_batch\'):\n                    # We flatten to set shape, but it is already a flat Tensor.\n                    in_batch_proposals = tf.reshape(\n                        tf.greater_equal(proposals_target, 0), [-1]\n                    )\n                    proposals = tf.boolean_mask(\n                        proposals, in_batch_proposals)\n                    bbox_offsets_target = tf.boolean_mask(\n                        bbox_offsets_target, in_batch_proposals)\n                    proposals_target = tf.boolean_mask(\n                        proposals_target, in_batch_proposals)\n\n            prediction_dict[\'target\'] = {\n                \'cls\': proposals_target,\n                \'bbox_offsets\': bbox_offsets_target,\n            }\n\n        roi_prediction = self._roi_pool(proposals, conv_feature_map, im_shape)\n\n        if self._debug:\n            # Save raw roi prediction in debug mode.\n            prediction_dict[\'_debug\'][\'roi\'] = roi_prediction\n\n        pooled_features = roi_prediction[\'roi_pool\']\n        features = base_network._build_tail(\n            pooled_features, is_training=is_training\n        )\n\n        if self._use_mean:\n            # We avg our height and width dimensions for a more\n            # ""memory-friendly"" Tensor.\n            features = tf.reduce_mean(features, [1, 2])\n\n        # We treat num proposals as batch number so that when flattening we\n        # get a (num_proposals, flatten_pooled_feature_map_size) Tensor.\n        flatten_features = tf.contrib.layers.flatten(features)\n        net = tf.identity(flatten_features)\n\n        if is_training:\n            net = tf.nn.dropout(net, keep_prob=self._dropout_keep_prob)\n\n        if self._debug:\n            prediction_dict[\'_debug\'][\'flatten_net\'] = net\n\n        # After flattening we are left with a Tensor of shape\n        # (num_proposals, pool_height * pool_width * 512).\n        # The first dimension works as batch size when applied to snt.Linear.\n        for i, layer in enumerate(self._layers):\n            # Through FC layer.\n            net = layer(net)\n\n            # Apply activation and dropout.\n            variable_summaries(\n                net, \'fc_{}_preactivationout\'.format(i), \'reduced\'\n            )\n            net = self._activation(net)\n            if self._debug:\n                prediction_dict[\'_debug\'][\'layer_{}_out\'.format(i)] = net\n\n            variable_summaries(net, \'fc_{}_out\'.format(i), \'reduced\')\n            if is_training:\n                net = tf.nn.dropout(net, keep_prob=self._dropout_keep_prob)\n\n        cls_score = self._classifier_layer(net)\n        cls_prob = tf.nn.softmax(cls_score, axis=1)\n        bbox_offsets = self._bbox_layer(net)\n\n        prediction_dict[\'rcnn\'] = {\n            \'cls_score\': cls_score,\n            \'cls_prob\': cls_prob,\n            \'bbox_offsets\': bbox_offsets,\n        }\n\n        # Get final objects proposals based on the probabilty, the offsets and\n        # the original proposals.\n        proposals_pred = self._rcnn_proposal(\n            proposals, bbox_offsets, cls_prob, im_shape)\n\n        # objects, objects_labels, and objects_labels_prob are the only keys\n        # that matter for drawing objects.\n        prediction_dict[\'objects\'] = proposals_pred[\'objects\']\n        prediction_dict[\'labels\'] = proposals_pred[\'proposal_label\']\n        prediction_dict[\'probs\'] = proposals_pred[\'proposal_label_prob\']\n\n        if self._debug:\n            prediction_dict[\'_debug\'][\'proposal\'] = proposals_pred\n\n        # Calculate summaries for results\n        variable_summaries(cls_prob, \'cls_prob\', \'reduced\')\n        variable_summaries(bbox_offsets, \'bbox_offsets\', \'reduced\')\n\n        if self._debug:\n            variable_summaries(pooled_features, \'pooled_features\', \'full\')\n            layer_summaries(self._classifier_layer, \'full\')\n            layer_summaries(self._bbox_layer, \'full\')\n\n        return prediction_dict\n\n    def loss(self, prediction_dict):\n        """"""\n        Returns cost for RCNN based on:\n\n        Args:\n            prediction_dict with keys:\n                rcnn:\n                    cls_score: shape (num_proposals, num_classes + 1)\n                        Has the class scoring for each the proposals. Classes\n                        are 1-indexed with 0 being the background.\n\n                    cls_prob: shape (num_proposals, num_classes + 1)\n                        Application of softmax on cls_score.\n\n                    bbox_offsets: shape (num_proposals, num_classes * 4)\n                        Has the offset for each proposal for each class.\n                        We have to compare only the proposals labeled with the\n                        offsets for that label.\n\n                target:\n                    cls_target: shape (num_proposals,)\n                        Has the correct label for each of the proposals.\n                        0 => background\n                        1..n => 1-indexed classes\n\n                    bbox_offsets_target: shape (num_proposals, 4)\n                        Has the true offset of each proposal for the true\n                        label.\n                        In case of not having a true label (non-background)\n                        then it\'s just zeroes.\n\n        Returns:\n            loss_dict with keys:\n                rcnn_cls_loss: The cross-entropy or log-loss of the\n                    classification tasks between then num_classes + background.\n                rcnn_reg_loss: The smooth L1 loss for the bounding box\n                    regression task to adjust correctly labeled boxes.\n\n        """"""\n        with tf.name_scope(\'RCNNLoss\'):\n            cls_score = prediction_dict[\'rcnn\'][\'cls_score\']\n            # cls_prob = prediction_dict[\'rcnn\'][\'cls_prob\']\n            # Cast target explicitly as int32.\n            cls_target = tf.cast(\n                prediction_dict[\'target\'][\'cls\'], tf.int32\n            )\n\n            # First we need to calculate the log loss betweetn cls_prob and\n            # cls_target\n\n            # We only care for the targets that are >= 0\n            not_ignored = tf.reshape(tf.greater_equal(\n                cls_target, 0), [-1], name=\'not_ignored\')\n            # We apply boolean mask to score, prob and target.\n            cls_score_labeled = tf.boolean_mask(\n                cls_score, not_ignored, name=\'cls_score_labeled\')\n            # cls_prob_labeled = tf.boolean_mask(\n            #    cls_prob, not_ignored, name=\'cls_prob_labeled\')\n            cls_target_labeled = tf.boolean_mask(\n                cls_target, not_ignored, name=\'cls_target_labeled\')\n\n            tf.summary.scalar(\n                \'batch_size\',\n                tf.shape(cls_score_labeled)[0], [\'rcnn\']\n            )\n\n            # Transform to one-hot vector\n            cls_target_one_hot = tf.one_hot(\n                cls_target_labeled, depth=self._num_classes + 1,\n                name=\'cls_target_one_hot\'\n            )\n\n            # We get cross entropy loss of each proposal.\n            cross_entropy_per_proposal = (\n                tf.nn.softmax_cross_entropy_with_logits_v2(\n                    labels=tf.stop_gradient(cls_target_one_hot),\n                    logits=cls_score_labeled\n                )\n            )\n\n            if self._debug:\n                prediction_dict[\'_debug\'][\'losses\'] = {}\n                # Save the cross entropy per proposal to be able to\n                # visualize proposals with high and low error.\n                prediction_dict[\'_debug\'][\'losses\'][\n                    \'cross_entropy_per_proposal\'\n                ] = (\n                    cross_entropy_per_proposal\n                )\n\n            # Second we need to calculate the smooth l1 loss between\n            # `bbox_offsets` and `bbox_offsets_target`.\n            bbox_offsets = prediction_dict[\'rcnn\'][\'bbox_offsets\']\n            bbox_offsets_target = (\n                prediction_dict[\'target\'][\'bbox_offsets\']\n            )\n\n            # We only want the non-background labels bounding boxes.\n            not_ignored = tf.reshape(tf.greater(cls_target, 0), [-1])\n            bbox_offsets_labeled = tf.boolean_mask(\n                bbox_offsets, not_ignored, name=\'bbox_offsets_labeled\')\n            bbox_offsets_target_labeled = tf.boolean_mask(\n                bbox_offsets_target, not_ignored,\n                name=\'bbox_offsets_target_labeled\'\n            )\n\n            cls_target_labeled = tf.boolean_mask(\n                cls_target, not_ignored, name=\'cls_target_labeled\')\n            # `cls_target_labeled` is based on `cls_target` which has\n            # `num_classes` + 1 classes.\n            # for making `one_hot` with depth `num_classes` to work we need\n            # to lower them to make them 0-index.\n            cls_target_labeled = cls_target_labeled - 1\n\n            cls_target_one_hot = tf.one_hot(\n                cls_target_labeled, depth=self._num_classes,\n                name=\'cls_target_one_hot\'\n            )\n\n            # cls_target now is (num_labeled, num_classes)\n            bbox_flatten = tf.reshape(\n                bbox_offsets_labeled, [-1, 4], name=\'bbox_flatten\')\n\n            # We use the flatten cls_target_one_hot as boolean mask for the\n            # bboxes.\n            cls_flatten = tf.cast(tf.reshape(\n                cls_target_one_hot, [-1]), tf.bool, \'cls_flatten_as_bool\')\n\n            bbox_offset_cleaned = tf.boolean_mask(\n                bbox_flatten, cls_flatten, \'bbox_offset_cleaned\')\n\n            # Calculate the smooth l1 loss between the ""cleaned"" bboxes\n            # offsets (that means, the useful results) and the labeled\n            # targets.\n            reg_loss_per_proposal = smooth_l1_loss(\n                bbox_offset_cleaned, bbox_offsets_target_labeled,\n                sigma=self._l1_sigma\n            )\n\n            tf.summary.scalar(\n                \'rcnn_foreground_samples\',\n                tf.shape(bbox_offset_cleaned)[0], [\'rcnn\']\n            )\n\n            if self._debug:\n                # Also save reg loss per proposals to be able to visualize\n                # good and bad proposals in debug mode.\n                prediction_dict[\'_debug\'][\'losses\'][\n                    \'reg_loss_per_proposal\'\n                ] = (\n                    reg_loss_per_proposal\n                )\n\n            return {\n                \'rcnn_cls_loss\': tf.reduce_mean(cross_entropy_per_proposal),\n                \'rcnn_reg_loss\': tf.reduce_mean(reg_loss_per_proposal),\n            }\n'"
luminoth/models/fasterrcnn/rcnn_proposal.py,22,"b'import sonnet as snt\nimport tensorflow as tf\n\nfrom luminoth.utils.bbox_transform_tf import decode, clip_boxes, change_order\n\n\nclass RCNNProposal(snt.AbstractModule):\n    """"""Create final object detection proposals.\n\n    RCNNProposals takes the proposals generated by the RPN and the predictions\n    of the RCNN (both classification and boundin box adjusting) and generates\n    a list of object proposals with assigned class.\n\n    In the process it tries to remove duplicated suggestions by applying non\n    maximum suppresion (NMS).\n\n    We apply NMS because the way object detectors are usually scored is by\n    treating duplicated detections (multiple detections that overlap the same\n    ground truth value) as false positive. It is resonable to assume that there\n    may exist such case that applying NMS is completly unnecesary.\n\n    Besides applying NMS it also filters the top N results, both for classes\n    and in general. These values are easily modifiable in the configuration\n    files.\n    """"""\n    def __init__(self, num_classes, config, variances=None,\n                 name=\'rcnn_proposal\'):\n        """"""\n        Args:\n            num_classes: Total number of classes RCNN is classifying.\n            config: Configuration object.\n        """"""\n        super(RCNNProposal, self).__init__(name=name)\n        self._num_classes = num_classes\n        self._variances = variances\n\n        # Max number of object detections per class.\n        self._class_max_detections = config.class_max_detections\n        # NMS intersection over union threshold to be used for classes.\n        self._class_nms_threshold = float(config.class_nms_threshold)\n        # Maximum number of detections to return.\n        self._total_max_detections = config.total_max_detections\n        # Threshold probability\n        self._min_prob_threshold = config.min_prob_threshold or 0.0\n\n    def _build(self, proposals, bbox_pred, cls_prob, im_shape):\n        """"""\n        Args:\n            proposals: Tensor with the RPN proposals bounding boxes.\n                Shape (num_proposals, 4). Where num_proposals is less than\n                POST_NMS_TOP_N (We don\'t know exactly beforehand)\n            bbox_pred: Tensor with the RCNN delta predictions for each proposal\n                for each class. Shape (num_proposals, 4 * num_classes)\n            cls_prob: A softmax probability for each proposal where the idx = 0\n                is the background class (which we should ignore).\n                Shape (num_proposals, num_classes + 1)\n\n        Returns:\n            objects:\n                Shape (final_num_proposals, 4)\n                Where final_num_proposals is unknown before-hand (it depends on\n                NMS). The 4-length Tensor for each corresponds to:\n                (x_min, y_min, x_max, y_max).\n            objects_label:\n                Shape (final_num_proposals,)\n            objects_label_prob:\n                Shape (final_num_proposals,)\n\n        """"""\n        selected_boxes = []\n        selected_probs = []\n        selected_labels = []\n\n        # For each class, take the proposals with the class-specific\n        # predictions (class scores and bbox regression) and filter accordingly\n        # (valid area, min probability score and NMS).\n        for class_id in range(self._num_classes):\n            # Apply the class-specific transformations to the proposals to\n            # obtain the current class\' prediction.\n            class_prob = cls_prob[:, class_id + 1]  # 0 is background class.\n            class_bboxes = bbox_pred[:, (4 * class_id):(4 * class_id + 4)]\n            raw_class_objects = decode(\n                proposals,\n                class_bboxes,\n                variances=self._variances,\n            )\n\n            # Clip bboxes so they don\'t go out of the image.\n            class_objects = clip_boxes(raw_class_objects, im_shape)\n\n            # Filter objects based on the min probability threshold and on them\n            # having a valid area.\n            prob_filter = tf.greater_equal(\n                class_prob, self._min_prob_threshold\n            )\n\n            (x_min, y_min, x_max, y_max) = tf.unstack(class_objects, axis=1)\n            area_filter = tf.greater(\n                tf.maximum(x_max - x_min, 0.0)\n                * tf.maximum(y_max - y_min, 0.0),\n                0.0\n            )\n\n            object_filter = tf.logical_and(area_filter, prob_filter)\n\n            class_objects = tf.boolean_mask(class_objects, object_filter)\n            class_prob = tf.boolean_mask(class_prob, object_filter)\n\n            # We have to use the TensorFlow\'s bounding box convention to use\n            # the included function for NMS.\n            class_objects_tf = change_order(class_objects)\n\n            # Apply class NMS.\n            class_selected_idx = tf.image.non_max_suppression(\n                class_objects_tf, class_prob, self._class_max_detections,\n                iou_threshold=self._class_nms_threshold\n            )\n\n            # Using NMS resulting indices, gather values from Tensors.\n            class_objects_tf = tf.gather(class_objects_tf, class_selected_idx)\n            class_prob = tf.gather(class_prob, class_selected_idx)\n\n            # Revert to our bbox convention.\n            class_objects = change_order(class_objects_tf)\n\n            # We append values to a regular list which will later be\n            # transformed to a proper Tensor.\n            selected_boxes.append(class_objects)\n            selected_probs.append(class_prob)\n            # In the case of the class_id, since it is a loop on classes, we\n            # already have a fixed class_id. We use `tf.tile` to create that\n            # Tensor with the total number of indices returned by the NMS.\n            selected_labels.append(\n                tf.tile([class_id], [tf.shape(class_selected_idx)[0]])\n            )\n\n        # We use concat (axis=0) to generate a Tensor where the rows are\n        # stacked on top of each other\n        objects = tf.concat(selected_boxes, axis=0)\n        proposal_label = tf.concat(selected_labels, axis=0)\n        proposal_label_prob = tf.concat(selected_probs, axis=0)\n\n        tf.summary.histogram(\n            \'proposal_cls_scores\', proposal_label_prob, [\'rcnn\']\n        )\n\n        # Get top-k detections of all classes.\n        k = tf.minimum(\n            self._total_max_detections,\n            tf.shape(proposal_label_prob)[0]\n        )\n        top_k = tf.nn.top_k(proposal_label_prob, k=k)\n        top_k_proposal_label_prob = top_k.values\n        top_k_objects = tf.gather(objects, top_k.indices)\n        top_k_proposal_label = tf.gather(proposal_label, top_k.indices)\n\n        return {\n            \'objects\': top_k_objects,\n            \'proposal_label\': top_k_proposal_label,\n            \'proposal_label_prob\': top_k_proposal_label_prob,\n            \'selected_boxes\': selected_boxes,\n            \'selected_probs\': selected_probs,\n            \'selected_labels\': selected_labels,\n        }\n'"
luminoth/models/fasterrcnn/rcnn_proposal_test.py,24,"b'import tensorflow as tf\n\nfrom easydict import EasyDict\nfrom luminoth.models.fasterrcnn.rcnn_proposal import RCNNProposal\nfrom luminoth.utils.bbox_transform_tf import encode\n\n\nclass RCNNProposalTest(tf.test.TestCase):\n\n    def setUp(self):\n        super(RCNNProposalTest, self).setUp()\n\n        self._num_classes = 3\n        self._image_shape = (900, 1440)\n        self._config = EasyDict({\n            \'class_max_detections\': 100,\n            \'class_nms_threshold\': 0.6,\n            \'total_max_detections\': 300,\n            \'min_prob_threshold\': 0.0,\n        })\n\n        self._equality_delta = 1e-03\n\n        self._shared_model = RCNNProposal(self._num_classes, self._config)\n        tf.reset_default_graph()\n\n    def _run_rcnn_proposal(self, model, proposals, bbox_pred, cls_prob,\n                           image_shape=None):\n        if image_shape is None:\n            image_shape = self._image_shape\n        rcnn_proposal_net = model(proposals, bbox_pred, cls_prob, image_shape)\n        with self.test_session() as sess:\n            return sess.run(rcnn_proposal_net)\n\n    def _compute_tf_graph(self, graph):\n        with self.test_session() as sess:\n            return sess.run(graph)\n\n    def _get_bbox_pred(self, proposed_boxes, gt_boxes_per_class):\n        """"""Computes valid bbox_pred from proposals and gt_boxes for each class.\n\n        Args:\n            proposed_boxes: Tensor with shape (num_proposals, 4).\n            gt_boxes_per_class: Tensor holding the ground truth boxes for each\n                class. Has shape (num_classes, num_gt_boxes_per_class, 4).\n\n        Returns:\n            A tensor with shape (num_proposals, num_classes * 4), holding the\n            correct bbox_preds.\n        """"""\n\n        def bbox_encode(gt_boxes):\n            return encode(\n                proposed_boxes, gt_boxes\n            )\n        bbox_pred_tensor = tf.map_fn(\n            bbox_encode, gt_boxes_per_class,\n            dtype=tf.float32\n        )\n        # We need to explicitly unstack the tensor so that tf.concat works\n        # properly.\n        bbox_pred_list = tf.unstack(bbox_pred_tensor)\n        return tf.concat(bbox_pred_list, 1)\n\n    def _check_proposals_are_clipped(self, proposals, image_shape):\n        """"""Asserts that no proposals exceed the image boundaries.""""""\n        for proposal in proposals:\n            self.assertLess(proposal[0], image_shape[1])\n            self.assertLess(proposal[1], image_shape[0])\n            self.assertLess(proposal[2], image_shape[1])\n            self.assertLess(proposal[3], image_shape[0])\n            for i in range(4):\n                self.assertGreaterEqual(proposal[i], 0)\n\n    def testNoBackgroundClass(self):\n        """"""Tests that we\'re not returning an object with background class.\n\n        For this, we make sure all predictions have a class between 0 and 2.\n        That is, even though we have four classes (three plus background),\n        background is completely ignored.\n        """"""\n\n        proposed_boxes = tf.constant([\n            (85, 500, 730, 590),\n            (50, 500, 70, 530),\n            (700, 570, 740, 598),\n        ])\n        gt_boxes_per_class = tf.constant([\n            [(101, 101, 201, 249)],\n            [(200, 502, 209, 532)],\n            [(86, 571, 743, 599)],\n        ])\n        bbox_pred = self._get_bbox_pred(proposed_boxes, gt_boxes_per_class)\n\n        # We build one prediction for each class.\n        cls_prob = tf.constant([\n            (0., .3, .3, .4),\n            (.8, 0., 0., 2.),\n            (.35, .3, .2, .15),\n        ])\n\n        proposal_prediction = self._run_rcnn_proposal(\n            self._shared_model,\n            proposed_boxes,\n            bbox_pred,\n            cls_prob,\n        )\n\n        # Make sure we get 3 predictions, one per class (as they\'re NMSed int\n        # a single one).\n        self.assertEqual(len(proposal_prediction[\'objects\']), 3)\n        self.assertIn(0, proposal_prediction[\'proposal_label\'])\n        self.assertIn(1, proposal_prediction[\'proposal_label\'])\n        self.assertIn(2, proposal_prediction[\'proposal_label\'])\n\n    def testNMSFilter(self):\n        """"""Tests that we\'re applying NMS correctly.""""""\n\n        proposed_boxes = tf.constant([\n            (85, 500, 730, 590),\n            (50, 500, 740, 570),\n            (700, 570, 740, 598),\n        ])\n        gt_boxes_per_class = tf.constant([\n            [(101, 101, 201, 249)],\n            [(200, 502, 209, 532)],\n            [(86, 571, 743, 599)],\n        ])\n        bbox_pred = self._get_bbox_pred(proposed_boxes, gt_boxes_per_class)\n        cls_prob = tf.constant([\n            (0., .1, .3, .6),\n            (.1, .2, .25, .45),\n            (.2, .3, .25, .25),\n        ])\n\n        proposal_prediction = self._run_rcnn_proposal(\n            self._shared_model,\n            proposed_boxes,\n            bbox_pred,\n            cls_prob,\n        )\n\n        # All proposals are mapped perfectly into each GT box, so we should\n        # have 3 resulting objects after applying NMS.\n        self.assertEqual(len(proposal_prediction[\'objects\']), 3)\n\n    def testImageClipping(self):\n        """"""Tests that we\'re clipping images correctly.\n\n        We test two image shapes, (1440, 900) and (900, 1440). Note we pass\n        shapes as (height, width).\n        """"""\n\n        proposed_boxes = tf.constant([\n            (1300, 800, 1435, 870),\n            (10, 1, 30, 7),\n            (2, 870, 80, 898),\n        ])\n        gt_boxes_per_class = tf.constant([\n            [(1320, 815, 1455, 912)],\n            [(5, -8, 31, 8)],\n            [(-120, 910, 78, 1040)],\n        ])\n        bbox_pred = self._get_bbox_pred(proposed_boxes, gt_boxes_per_class)\n        cls_prob = tf.constant([\n            (0., 1., 0., 0.),\n            (.2, .25, .3, .25),\n            (.45, 0., 0., .55),\n        ])\n\n        shape1 = (1440, 900)\n        shape2 = (900, 1440)\n\n        proposal_prediction_shape1 = self._run_rcnn_proposal(\n            self._shared_model,\n            proposed_boxes,\n            bbox_pred,\n            cls_prob,\n            image_shape=shape1,\n        )\n        proposal_prediction_shape2 = self._run_rcnn_proposal(\n            self._shared_model,\n            proposed_boxes,\n            bbox_pred,\n            cls_prob,\n            image_shape=shape2,\n        )\n        # Assertions\n        self._check_proposals_are_clipped(\n            proposal_prediction_shape1[\'objects\'],\n            shape1,\n        )\n        self._check_proposals_are_clipped(\n            proposal_prediction_shape2[\'objects\'],\n            shape2,\n        )\n\n    def testBboxPred(self):\n        """"""Tests that we\'re using bbox_pred correctly.""""""\n\n        proposed_boxes = tf.constant([\n            (200, 315, 400, 370),\n            (56, 0, 106, 4),\n            (15, 15, 20, 20),\n        ])\n\n        gt_boxes_per_class = tf.constant([\n            [(0, 0, 1, 1)],\n            [(5, 5, 10, 10)],\n            [(15, 15, 20, 20)],\n        ])\n        bbox_pred = self._get_bbox_pred(proposed_boxes, gt_boxes_per_class)\n\n        cls_prob = tf.constant([\n            (0., 1., 0., 0.),\n            (.2, .25, .3, .25),\n            (.45, 0., 0., .55),\n        ])\n\n        proposal_prediction = self._run_rcnn_proposal(\n            self._shared_model,\n            proposed_boxes,\n            bbox_pred,\n            cls_prob,\n        )\n\n        objects = self._compute_tf_graph(\n            tf.squeeze(gt_boxes_per_class, axis=1)\n        )\n        # We need to sort the objects by `cls_prob` from high to low score.\n        cls_prob = self._compute_tf_graph(cls_prob)\n        # Ignoring background prob get the reverse argsort for the max of each\n        # object.\n        decreasing_idxs = cls_prob[:, 1:].max(axis=1).argsort()[::-1]\n        # Sort by indexing.\n        objects_sorted = objects[decreasing_idxs]\n\n        self.assertAllClose(\n            proposal_prediction[\'objects\'],\n            objects_sorted,\n            atol=self._equality_delta\n        )\n\n    def testLimits(self):\n        """"""Tests that we\'re respecting the limits imposed by the config.""""""\n\n        limits_config = self._config.copy()\n        limits_config[\'class_max_detections\'] = 2\n        limits_config[\'total_max_detections\'] = 3\n        limits_config = EasyDict(limits_config)\n        limits_num_classes = 2\n        limits_model = RCNNProposal(limits_num_classes, limits_config)\n\n        proposed_boxes = tf.constant([\n            (0, 0, 1, 1),  # class 0\n            (5, 5, 10, 10),  # class 1\n            (15, 15, 20, 20),  # class 1\n            (25, 25, 30, 30),  # class 0\n            (35, 35, 40, 40),\n            (38, 40, 65, 65),\n            (70, 50, 90, 90),  # class 0\n            (95, 95, 100, 100),\n            (105, 105, 110, 110),  # class 1\n        ])\n        # All zeroes for our bbox_pred.\n        bbox_pred = tf.constant([[0.] * limits_num_classes * 4] * 9)\n        cls_prob = tf.constant([\n            (0., 1., 0.),\n            (0., .2, .8),\n            (0., .45, .55),\n            (0., .55, .45),\n            (1., 0., 0.),\n            (1., 0., 0.),\n            (0., .95, .05),\n            (1., 0., 0.),\n            (0., .495, .505),\n        ])\n\n        proposal_prediction = self._run_rcnn_proposal(\n            limits_model,\n            proposed_boxes,\n            bbox_pred,\n            cls_prob,\n        )\n        labels = proposal_prediction[\'proposal_label\']\n        num_class0 = labels[labels == 0].shape[0]\n        num_class1 = labels[labels == 1].shape[0]\n\n        self.assertLessEqual(num_class0, limits_config.class_max_detections)\n        self.assertLessEqual(num_class1, limits_config.class_max_detections)\n        num_total = labels.shape[0]\n        self.assertLessEqual(num_total, limits_config.total_max_detections)\n\n\nif __name__ == \'__main__\':\n    tf.test.main()\n'"
luminoth/models/fasterrcnn/rcnn_target.py,57,"b'import tensorflow as tf\nimport sonnet as snt\n\nfrom luminoth.utils.bbox_transform_tf import encode\nfrom luminoth.utils.bbox_overlap import bbox_overlap_tf\n\n\nclass RCNNTarget(snt.AbstractModule):\n    """"""Generate RCNN target tensors for both probabilities and bounding boxes.\n\n    Targets for RCNN are based upon the results of the RPN, this can get tricky\n    in the sense that RPN results might not be the best and it might not be\n    possible to have the ideal amount of targets for all the available ground\n    truth boxes.\n\n    There are two types of targets, class targets and bounding box targets.\n\n    Class targets are used both for background and foreground, while bounding\n    box targets are only used for foreground (since it\'s not possible to create\n    a bounding box of ""background objects"").\n\n    A minibatch size determines how many targets are going to be generated and\n    how many are going to be ignored. RCNNTarget is responsible for choosing\n    which proposals and corresponding targets are included in the minibatch and\n    which ones are completely ignored.\n    """"""\n    def __init__(self, num_classes, config, seed=None, variances=None,\n                 name=\'rcnn_proposal\'):\n        """"""\n        Args:\n            num_classes: Number of possible classes.\n            config: Configuration object for RCNNTarget.\n        """"""\n        super(RCNNTarget, self).__init__(name=name)\n        self._num_classes = num_classes\n        self._variances = variances\n        # Ratio of foreground vs background for the minibatch.\n        self._foreground_fraction = config.foreground_fraction\n        self._minibatch_size = config.minibatch_size\n        # IoU lower threshold with a ground truth box to be considered that\n        # specific class.\n        self._foreground_threshold = config.foreground_threshold\n        # High and low treshold to be considered background.\n        self._background_threshold_high = config.background_threshold_high\n        self._background_threshold_low = config.background_threshold_low\n        self._seed = seed\n\n    def _build(self, proposals, gt_boxes):\n        """"""\n        Args:\n            proposals: A Tensor with the RPN bounding boxes proposals.\n                The shape of the Tensor is (num_proposals, 4).\n            gt_boxes: A Tensor with the ground truth boxes for the image.\n                The shape of the Tensor is (num_gt, 5), having the truth label\n                as the last value for each box.\n        Returns:\n            proposals_label: Either a truth value of the proposals (a value\n                between 0 and num_classes, with 0 being background), or -1 when\n                the proposal is to be ignored in the minibatch.\n                The shape of the Tensor is (num_proposals, 1).\n            bbox_targets: A bounding box regression target for each of the\n                proposals that have and greater than zero label. For every\n                other proposal we return zeros.\n                The shape of the Tensor is (num_proposals, 4).\n        """"""\n        overlaps = bbox_overlap_tf(proposals, gt_boxes[:, :4])\n        # overlaps now contains (num_proposals, num_gt_boxes) with the IoU of\n        # proposal P and ground truth box G in overlaps[P, G]\n\n        # We are going to label each proposal based on the IoU with\n        # `gt_boxes`. Start by filling the labels with -1, marking them as\n        # ignored.\n        proposals_label_shape = tf.gather(tf.shape(proposals), [0])\n        proposals_label = tf.fill(\n            dims=proposals_label_shape,\n            value=-1.\n        )\n        # For each overlap there is three possible outcomes for labelling:\n        #  if max(iou) < config.background_threshold_low then we ignore.\n        #  elif max(iou) <= config.background_threshold_high then we label\n        #      background.\n        #  elif max(iou) > config.foreground_threshold then we label with\n        #      the highest IoU in overlap.\n        #\n        # max_overlaps gets, for each proposal, the index in which we can\n        # find the gt_box with which it has the highest overlap.\n        max_overlaps = tf.reduce_max(overlaps, axis=1)\n\n        iou_is_high_enough_for_bg = tf.greater_equal(\n            max_overlaps, self._background_threshold_low\n        )\n        iou_is_not_too_high_for_bg = tf.less(\n            max_overlaps, self._background_threshold_high\n        )\n        bg_condition = tf.logical_and(\n            iou_is_high_enough_for_bg, iou_is_not_too_high_for_bg\n        )\n        proposals_label = tf.where(\n            condition=bg_condition,\n            x=tf.zeros_like(proposals_label, dtype=tf.float32),\n            y=proposals_label\n        )\n\n        # Get the index of the best gt_box for each proposal.\n        overlaps_best_gt_idxs = tf.argmax(overlaps, axis=1)\n        # Having the index of the gt bbox with the best label we need to get\n        # the label for each gt box and sum it one because 0 is used for\n        # background.\n        best_fg_labels_for_proposals = tf.add(\n            tf.gather(gt_boxes[:, 4], overlaps_best_gt_idxs),\n            1.\n        )\n        iou_is_fg = tf.greater_equal(\n            max_overlaps, self._foreground_threshold\n        )\n        best_proposals_idxs = tf.argmax(overlaps, axis=0)\n\n        # Set the indices in best_proposals_idxs to True, and the rest to\n        # false.\n        # tf.sparse_to_dense is used because we know the set of indices which\n        # we want to set to True, and we know the rest of the indices\n        # should be set to False. That\'s exactly the use case of\n        # tf.sparse_to_dense.\n        is_best_box = tf.sparse_to_dense(\n            sparse_indices=tf.reshape(best_proposals_idxs, [-1]),\n            sparse_values=True, default_value=False,\n            output_shape=tf.cast(proposals_label_shape, tf.int64),\n            validate_indices=False\n        )\n        # We update proposals_label with the value in\n        # best_fg_labels_for_proposals only when the box is foreground.\n        proposals_label = tf.where(\n            condition=iou_is_fg,\n            x=best_fg_labels_for_proposals,\n            y=proposals_label\n        )\n        # Now we need to find the proposals that are the best for each of the\n        # gt_boxes. We overwrite the previous proposals_label with this\n        # because setting the best proposal for each gt_box has priority.\n        best_proposals_gt_labels = tf.sparse_to_dense(\n            sparse_indices=tf.reshape(best_proposals_idxs, [-1]),\n            sparse_values=gt_boxes[:, 4] + 1,\n            default_value=0.,\n            output_shape=tf.cast(proposals_label_shape, tf.int64),\n            validate_indices=False,\n            name=""get_right_labels_for_bestboxes""\n        )\n        proposals_label = tf.where(\n            condition=is_best_box,\n            x=best_proposals_gt_labels,\n            y=proposals_label,\n            name=""update_labels_for_bestbox_proposals""\n        )\n\n        # proposals_label now has a value in [0, num_classes + 1] for\n        # proposals we are going to use and -1 for the ones we should ignore.\n        # But we still need to make sure we don\'t have a number of proposals\n        # higher than minibatch_size * foreground_fraction.\n        max_fg = int(self._foreground_fraction * self._minibatch_size)\n        fg_condition = tf.logical_or(\n            iou_is_fg, is_best_box\n        )\n        fg_inds = tf.where(\n            condition=fg_condition\n        )\n\n        def disable_some_fgs():\n            # We want to delete a randomly-selected subset of fg_inds of\n            # size `fg_inds.shape[0] - max_fg`.\n            # We shuffle along the dimension 0 and then we get the first\n            # num_fg_inds - max_fg indices and we disable them.\n            shuffled_inds = tf.random_shuffle(fg_inds, seed=self._seed)\n            disable_place = (tf.shape(fg_inds)[0] - max_fg)\n            # This function should never run if num_fg_inds <= max_fg, so we\n            # add an assertion to catch the wrong behaviour if it happens.\n            integrity_assertion = tf.assert_positive(\n                disable_place,\n                message=""disable_place in disable_some_fgs is negative.""\n            )\n            with tf.control_dependencies([integrity_assertion]):\n                disable_inds = shuffled_inds[:disable_place]\n            is_disabled = tf.sparse_to_dense(\n                sparse_indices=disable_inds,\n                sparse_values=True, default_value=False,\n                output_shape=tf.cast(proposals_label_shape, tf.int64),\n                # We are shuffling the indices, so they may not be ordered.\n                validate_indices=False\n            )\n            return tf.where(\n                condition=is_disabled,\n                # We set it to -label for debugging purposes.\n                x=tf.negative(proposals_label),\n                y=proposals_label\n            )\n        # Disable some fgs if we have too many foregrounds.\n        proposals_label = tf.cond(\n            tf.greater(tf.shape(fg_inds)[0], max_fg),\n            true_fn=disable_some_fgs,\n            false_fn=lambda: proposals_label\n        )\n\n        total_fg_in_batch = tf.shape(\n            tf.where(\n                condition=tf.greater(proposals_label, 0)\n            )\n        )[0]\n\n        # Now we want to do the same for backgrounds.\n        # We calculate up to how many backgrounds we desire based on the\n        # final number of foregrounds and the total desired batch size.\n        max_bg = self._minibatch_size - total_fg_in_batch\n\n        # We can\'t use bg_condition because some of the proposals that satisfy\n        # the IoU conditions to be background may have been labeled as\n        # foreground due to them being the best proposal for a certain gt_box.\n        bg_mask = tf.equal(proposals_label, 0)\n        bg_inds = tf.where(\n            condition=bg_mask,\n        )\n\n        def disable_some_bgs():\n            # Mutatis mutandis, all comments from disable_some_fgs apply.\n            shuffled_inds = tf.random_shuffle(bg_inds, seed=self._seed)\n            disable_place = (tf.shape(bg_inds)[0] - max_bg)\n            integrity_assertion = tf.assert_non_negative(\n                disable_place,\n                message=""disable_place in disable_some_bgs is negative.""\n            )\n            with tf.control_dependencies([integrity_assertion]):\n                disable_inds = shuffled_inds[:disable_place]\n            is_disabled = tf.sparse_to_dense(\n                sparse_indices=disable_inds,\n                sparse_values=True, default_value=False,\n                output_shape=tf.cast(proposals_label_shape, tf.int64),\n                validate_indices=False\n            )\n            return tf.where(\n                condition=is_disabled,\n                x=tf.fill(\n                    dims=proposals_label_shape,\n                    value=-1.\n                ),\n                y=proposals_label\n            )\n\n        proposals_label = tf.cond(\n            tf.greater_equal(tf.shape(bg_inds)[0], max_bg),\n            true_fn=disable_some_bgs,\n            false_fn=lambda: proposals_label\n        )\n\n        """"""\n        Next step is to calculate the proper targets for the proposals labeled\n        based on the values of the ground-truth boxes.\n        We have to use only the proposals labeled >= 1, each matching with\n        the proper gt_boxes\n        """"""\n\n        # Get the ids of the proposals that matter for bbox_target comparisson.\n        is_proposal_with_target = tf.greater(\n            proposals_label, 0\n        )\n        proposals_with_target_idx = tf.where(\n            condition=is_proposal_with_target\n        )\n        # Get the corresponding ground truth box only for the proposals with\n        # target.\n        gt_boxes_idxs = tf.gather(\n            overlaps_best_gt_idxs,\n            proposals_with_target_idx\n        )\n        # Get the values of the ground truth boxes.\n        proposals_gt_boxes = tf.gather_nd(\n            gt_boxes[:, :4], gt_boxes_idxs\n        )\n        # We create the same array but with the proposals\n        proposals_with_target = tf.gather_nd(\n            proposals,\n            proposals_with_target_idx\n        )\n        # We create our targets with bbox_transform.\n        bbox_targets_nonzero = encode(\n            proposals_with_target,\n            proposals_gt_boxes,\n            variances=self._variances,\n        )\n\n        # We unmap targets to proposal_labels (containing the length of\n        # proposals)\n        bbox_targets = tf.scatter_nd(\n            indices=proposals_with_target_idx,\n            updates=bbox_targets_nonzero,\n            shape=tf.cast(tf.shape(proposals), tf.int64)\n        )\n\n        proposals_label = proposals_label\n        bbox_targets = bbox_targets\n\n        return proposals_label, bbox_targets\n'"
luminoth/models/fasterrcnn/rcnn_target_test.py,39,"b'import numpy as np\nimport tensorflow as tf\n\nfrom easydict import EasyDict\nfrom luminoth.models.fasterrcnn.rcnn_target import RCNNTarget\nfrom luminoth.utils.test.gt_boxes import generate_gt_boxes\n\n\nclass RCNNTargetTest(tf.test.TestCase):\n\n    def setUp(self):\n        super(RCNNTargetTest, self).setUp()\n\n        # We don\'t care about the class labels or the batch number in most of\n        # these tests.\n        self._num_classes = 5\n        self._placeholder_label = 3.\n\n        self._config = EasyDict({\n            \'foreground_threshold\': 0.5,\n            \'background_threshold_high\': 0.5,\n            \'background_threshold_low\': 0.1,\n            \'foreground_fraction\': 0.5,\n            \'minibatch_size\': 2,\n        })\n        # We check for a difference smaller than this numbers in our tests\n        # instead of checking for exact equality.\n        self._equality_delta = 1e-03\n\n        self._shared_model = RCNNTarget(\n            self._num_classes, self._config, seed=0\n        )\n        tf.reset_default_graph()\n\n    def _run_rcnn_target(self, model, gt_boxes, proposed_boxes):\n        """"""Runs an instance of RCNNTarget\n\n        Args:\n            model: an RCNNTarget model.\n            gt_boxes: a Tensor holding the ground truth boxes, with shape\n                (num_gt, 5). The last value is the class label.\n            proposed_boxes: a Tensor holding the proposed boxes. Its shape is\n                (num_proposals, 4). The first value is the batch number.\n\n        Returns:\n            The tuple returned by RCNNTarget._build().\n        """"""\n        rcnn_target_net = model(proposed_boxes, gt_boxes)\n        with self.test_session() as sess:\n            return sess.run(rcnn_target_net)\n\n    def testBasic(self):\n        """"""Tests a basic case.\n\n        We have one ground truth box and three proposals. One should be\n        background, one foreground, and one should be an ignored background\n        (i.e. less IoU than whatever value is set as\n        config.background_threshold_low).\n        """"""\n\n        gt_boxes = tf.constant(\n            [(20, 20, 80, 100, self._placeholder_label)],\n            dtype=tf.float32\n        )\n\n        proposed_boxes = tf.constant(\n            [\n                (55, 75, 85, 105),  # Background\n                                    # IoU ~0.1293\n                (25, 21, 85, 105),  # Foreground\n                                    # IoU ~0.7934\n                (78, 98, 99, 135),  # Ignored\n                                    # IoU ~0.0015\n            ],\n            dtype=tf.float32\n        )\n\n        proposals_label, bbox_targets = self._run_rcnn_target(\n            self._shared_model, gt_boxes, proposed_boxes\n        )\n        # We test that all values are \'close\' (up to self._equality_delta)\n        # instead of equal to avoid failing due to a floating point rounding\n        # error.\n        # We sum 1 to the placeholder label because rcnn_target does the same\n        # due to the fact that it uses 0 to signal \'background\'.\n        self.assertAllClose(\n            proposals_label,\n            np.array([0., self._placeholder_label + 1, -1.]),\n            atol=self._equality_delta\n        )\n\n        self.assertEqual(\n            proposals_label[proposals_label >= 0].shape[0],\n            self._config.minibatch_size\n        )\n\n    def testEmptyCase(self):\n        """"""Tests we\'re choosing the best box when none are above the\n        foreground threshold.\n        """"""\n\n        gt_boxes = tf.constant(\n            [(423, 30, 501, 80, self._placeholder_label)],\n            dtype=tf.float32\n        )\n\n        proposed_boxes = tf.constant(\n            [\n                (491, 70, 510, 92),  # IoU 0.0277\n                (400, 60, 450, 92),  # IoU 0.1147\n                (413, 40, 480, 77),  # IoU 0.4998: highest\n                (411, 40, 480, 77),  # IoU 0.4914\n            ],\n            dtype=tf.float32\n        )\n\n        proposals_label, bbox_targets = self._run_rcnn_target(\n            self._shared_model, gt_boxes, proposed_boxes\n        )\n\n        # Assertions\n        self.assertAlmostEqual(\n            proposals_label[2], self._placeholder_label + 1,\n            delta=self._equality_delta\n        )\n\n        for i, label in enumerate(proposals_label):\n            if i != 2:\n                self.assertLess(label, 1)\n\n        self.assertEqual(\n            proposals_label[proposals_label >= 0].shape[0],\n            self._config.minibatch_size\n        )\n\n    def testAbsolutelyEmptyCase(self):\n        """"""Tests the code doesn\'t break when there\'s no proposals with IoU > 0.\n        """"""\n\n        gt_boxes = tf.constant(\n            [(40, 90, 100, 105, self._placeholder_label)],\n            dtype=tf.float32\n        )\n\n        proposed_boxes = tf.constant(\n            [\n                (0, 0, 39, 89),\n                (101, 106, 300, 450),\n                (340, 199, 410, 420),\n            ],\n            dtype=tf.float32\n        )\n\n        (proposals_label, bbox_targets) = self._run_rcnn_target(\n            self._shared_model,\n            gt_boxes,\n            proposed_boxes\n        )\n        foreground_fraction = self._config.foreground_fraction\n        minibatch_size = self._config.minibatch_size\n\n        foreground_number = proposals_label[proposals_label >= 1].shape[0]\n        background_number = proposals_label[proposals_label == 0].shape[0]\n\n        self.assertGreater(foreground_number, 0)\n        self.assertLessEqual(\n            foreground_number,\n            np.floor(foreground_fraction * minibatch_size)\n        )\n        self.assertLessEqual(\n            background_number,\n            minibatch_size - foreground_number\n        )\n\n    def testMultipleOverlap(self):\n        """"""Tests that we\'re choosing a foreground box when there\'s several for\n        the same gt box.\n        """"""\n\n        gt_boxes = tf.constant(\n            [(200, 300, 250, 390, self._placeholder_label)],\n            dtype=tf.float32\n        )\n\n        proposed_boxes = tf.constant(\n            [\n                (12, 70, 350, 540),  # noise\n                (190, 310, 240, 370),  # IoU: 0.4763\n                (197, 300, 252, 389),  # IoU: 0.9015\n                (196, 300, 252, 389),  # IoU: 0.8859\n                (197, 303, 252, 394),  # IoU: 0.8459\n                (180, 310, 235, 370),  # IoU: 0.3747\n                (0, 0, 400, 400),  # noise\n                (197, 302, 252, 389),  # IoU: 0.8832\n                (0, 0, 400, 400),  # noise\n            ],\n            dtype=tf.float32\n        )\n\n        proposals_label, bbox_targets = self._run_rcnn_target(\n            self._shared_model, gt_boxes, proposed_boxes\n        )\n\n        # Assertions\n        foreground_number = proposals_label[proposals_label >= 1].shape[0]\n        background_number = proposals_label[proposals_label == 0].shape[0]\n\n        foreground_fraction = self._config.foreground_fraction\n\n        self.assertEqual(\n            foreground_number,\n            np.floor(self._config.minibatch_size * foreground_fraction),\n        )\n        self.assertEqual(\n            background_number,\n            self._config.minibatch_size - foreground_number,\n        )\n\n        foreground_idxs = np.nonzero(proposals_label >= 1)\n        for foreground_idx in foreground_idxs:\n            self.assertIn(foreground_idx, [2, 3, 4, 7])\n\n        self.assertEqual(\n            proposals_label[proposals_label >= 0].shape[0],\n            self._config.minibatch_size\n        )\n\n    def testOddMinibatchSize(self):\n        """"""Tests we\'re getting the right results when there\'s an odd minibatch\n        size.\n        """"""\n\n        config = EasyDict({\n            \'foreground_threshold\': 0.5,\n            \'background_threshold_high\': 0.5,\n            \'background_threshold_low\': 0.1,\n            \'foreground_fraction\': 0.5,\n            \'minibatch_size\': 5,\n        })\n\n        model = RCNNTarget(self._num_classes, config, seed=0)\n\n        gt_boxes = tf.constant(\n            [(200, 300, 250, 390, self._placeholder_label)],\n            dtype=tf.float32\n        )\n\n        proposed_boxes = tf.constant(\n            [\n                (12, 70, 350, 540),  # noise\n                (190, 310, 240, 370),  # IoU: 0.4763\n                (197, 300, 252, 389),  # IoU: 0.9015\n                (196, 300, 252, 389),  # IoU: 0.8859\n                (197, 303, 252, 394),  # IoU: 0.8459\n                (180, 310, 235, 370),  # IoU: 0.3747\n                (0, 0, 400, 400),  # noise\n                (197, 302, 252, 389),  # IoU: 0.8832\n                (180, 310, 235, 370),  # IoU: 0.3747\n                (180, 310, 235, 370),  # IoU: 0.3747\n                (0, 0, 400, 400),  # noise\n            ],\n            dtype=tf.float32\n        )\n\n        (proposals_label, bbox_targets) = self._run_rcnn_target(\n            model,\n            gt_boxes,\n            proposed_boxes\n        )\n\n        foreground_number = proposals_label[proposals_label >= 1].shape[0]\n        background_number = proposals_label[proposals_label == 0].shape[0]\n\n        foreground_fraction = config.foreground_fraction\n        minibatch_size = config.minibatch_size\n\n        self.assertLessEqual(\n            foreground_number,\n            np.floor(foreground_fraction * minibatch_size)\n        )\n        self.assertGreater(foreground_number, 0)\n        self.assertLessEqual(\n            background_number,\n            minibatch_size - foreground_number\n        )\n\n        self.assertEqual(\n            proposals_label[proposals_label >= 0].shape[0],\n            config.minibatch_size\n        )\n\n    def testBboxTargetConsistency(self):\n        """"""Tests that bbox_targets is consistent with proposals_label.\n\n        That means we test that we have the same number of elements in\n        bbox_targets and proposals_label, and that only the proposals\n        marked with a class are assigned a non-zero bbox_target.\n        """"""\n\n        config = EasyDict({\n            \'foreground_threshold\': 0.5,\n            \'background_threshold_high\': 0.5,\n            \'background_threshold_low\': 0,  # use 0 to get complete batch\n            \'foreground_fraction\': 0.5,\n            # We change the minibatch_size the catch all our foregrounds\n            \'minibatch_size\': 8,\n        })\n\n        model = RCNNTarget(self._num_classes, config, seed=0)\n\n        gt_boxes = tf.constant(\n            [(200, 300, 250, 390, self._placeholder_label)],\n            dtype=tf.float32\n        )\n\n        proposed_boxes = tf.constant(\n            [\n                (12, 70, 350, 540),  # noise\n                (190, 310, 240, 370),  # IoU: 0.4763\n                (197, 300, 252, 389),  # IoU: 0.9015\n                (196, 300, 252, 389),  # IoU: 0.8859\n                (197, 303, 252, 394),  # IoU: 0.8459\n                (180, 310, 235, 370),  # IoU: 0.3747\n                (0, 0, 400, 400),  # noise\n                (197, 302, 252, 389),  # IoU: 0.8832\n                (0, 0, 400, 400),  # noise\n            ],\n            dtype=tf.float32\n        )\n\n        (proposals_label, bbox_targets) = self._run_rcnn_target(\n            model,\n            gt_boxes,\n            proposed_boxes\n        )\n\n        foreground_idxs = np.nonzero(proposals_label >= 1)\n        non_empty_bbox_target_idxs = np.nonzero(np.any(bbox_targets, axis=1))\n\n        self.assertAllEqual(\n            foreground_idxs, non_empty_bbox_target_idxs\n        )\n        self.assertGreater(proposals_label[proposals_label >= 1].shape[0], 0)\n        self.assertEqual(\n            proposals_label[proposals_label >= 0].shape[0],\n            config.minibatch_size\n        )\n\n    def testMultipleGtBoxes(self):\n        """"""Tests we\'re getting the right labels when there\'s several gt_boxes.\n        """"""\n\n        num_classes = 3\n        config = EasyDict({\n            \'foreground_threshold\': 0.5,\n            \'background_threshold_high\': 0.5,\n            \'background_threshold_low\': 0.1,\n            \'foreground_fraction\': 0.5,\n            # We change the minibatch_size the catch all our foregrounds\n            \'minibatch_size\': 18,\n        })\n        model = RCNNTarget(num_classes, config, seed=0)\n\n        gt_boxes = tf.constant(\n            [\n                (10, 0, 398, 399, 0),\n                (200, 300, 250, 390, 1),\n                (185, 305, 235, 372, 2),\n            ],\n            dtype=tf.float32\n        )\n        proposed_boxes = tf.constant(\n            [\n                (12, 70, 350, 540),  # noise\n                (190, 310, 240, 370),  # 2\n                (197, 300, 252, 389),  # 1\n                (196, 300, 252, 389),  # 1\n                (197, 303, 252, 394),  # 1\n                (180, 310, 235, 370),  # 2\n                (0, 0, 400, 400),  # 0\n                (197, 302, 252, 389),  # 1\n                (0, 0, 400, 400),  # 0\n            ],\n            dtype=tf.float32\n        )\n\n        (proposals_label, bbox_targets) = self._run_rcnn_target(\n            model,\n            gt_boxes,\n            proposed_boxes\n        )\n        # We don\'t care much about the first value.\n        self.assertAllClose(\n            proposals_label[1:],\n            # We sum one to normalize for RCNNTarget\'s results.\n            np.add([2., 1., 1., 1., 2., 0., 1., 0.], 1),\n            self._equality_delta\n        )\n\n    def testNonZeroForegrounds(self):\n        """"""Tests we never get zero foregrounds.\n\n        We\'re doing iterations with random gt_boxes and proposals under\n        conditions that make it likely we would get zero foregrounds if there\n        is a bug in the code. (Very few gt_boxes and a small minibatch_size).\n        """"""\n        number_of_iterations = 50\n        for _ in range(number_of_iterations):\n            im_shape = np.random.randint(\n                low=600, high=980, size=2, dtype=np.int32\n            )\n            total_boxes = np.random.randint(\n                low=1, high=4, dtype=np.int32\n            )\n            total_proposals = np.random.randint(\n                low=4, high=8, dtype=np.int32\n            )\n            # Generate gt_boxes and then add a label.\n            gt_boxes = generate_gt_boxes(\n                total_boxes, im_shape\n            )\n            gt_boxes_w_label = np.concatenate(\n                [gt_boxes, [[self._placeholder_label]] * total_boxes],\n                axis=1\n            ).astype(np.float32)\n            # Generate the proposals and add the batch number.\n            proposed_boxes = generate_gt_boxes(\n                total_proposals, im_shape\n            )\n            # Run RCNNTarget.\n            (proposals_label, _) = self._run_rcnn_target(\n                self._shared_model, gt_boxes_w_label,\n                proposed_boxes.astype(np.float32)\n            )\n            # Assertion\n            foreground_number = proposals_label[proposals_label > 0].shape[0]\n            self.assertGreater(foreground_number, 0)\n\n    def testCorrectBatchSize(self):\n        config = EasyDict({\n            \'foreground_threshold\': 0.5,\n            \'background_threshold_high\': 0.5,\n            # Use zero to get all non matching as backgrounds.\n            \'background_threshold_low\': 0.0,\n            \'foreground_fraction\': 0.5,\n            # We change the minibatch_size the catch all our foregrounds\n            \'minibatch_size\': 64,\n        })\n\n        gt_boxes = tf.constant([\n            [10, 10, 20, 20, 0]\n        ], dtype=tf.float32)\n\n        proposed_boxes_backgrounds = [[21, 21, 30, 30]] * 100\n        proposed_boxes_foreground = [[11, 11, 19, 19]] * 100\n\n        proposed_boxes = tf.constant(\n            proposed_boxes_backgrounds + proposed_boxes_foreground,\n            dtype=tf.float32\n        )\n\n        model = RCNNTarget(self._num_classes, config, seed=0)\n\n        (proposals_label, bbox_targets) = self._run_rcnn_target(\n            model,\n            gt_boxes,\n            proposed_boxes\n        )\n\n        self.assertEqual(\n            proposals_label[proposals_label >= 0].shape[0],\n            config.minibatch_size\n        )\n\n    def testLabelPriority(self):\n        """"""Tests we\'re prioritizing being the best proposal for a gt_box in\n        label selection.\n        """"""\n\n        first_label = self._placeholder_label\n        second_label = self._placeholder_label + 1\n\n        num_classes = second_label + 10\n\n        # We need a custom config to have a larger minibatch_size.\n        config = EasyDict({\n            \'foreground_threshold\': 0.5,\n            \'background_threshold_high\': 0.5,\n            # Use zero to get all non matching as backgrounds.\n            \'background_threshold_low\': 0.0,\n            \'foreground_fraction\': 0.5,\n            # We change the minibatch_size the catch all our foregrounds\n            \'minibatch_size\': 64,\n        })\n        model = RCNNTarget(num_classes, config, seed=0)\n\n        gt_boxes = tf.constant([\n            [10, 10, 20, 20, first_label],\n            [10, 10, 30, 30, second_label]\n        ], dtype=tf.float32)\n\n        # Both proposals have the first gt_box as the best match, but one of\n        # them should be assigned to the label of the second gt_box anyway.\n        proposed_boxes = tf.constant([\n            [10, 10, 20, 20],\n            [12, 10, 20, 20],\n        ], dtype=tf.float32)\n\n        (proposals_label, _) = self._run_rcnn_target(\n            model,\n            gt_boxes,\n            proposed_boxes\n        )\n\n        num_first_label = len(\n            proposals_label[proposals_label == first_label + 1]\n        )\n        num_second_label = len(\n            proposals_label[proposals_label == second_label + 1]\n        )\n\n        # Assertions\n        self.assertEqual(num_first_label, 1)\n        self.assertEqual(num_second_label, 1)\n\n\nif __name__ == \'__main__\':\n    tf.test.main()\n'"
luminoth/models/fasterrcnn/rcnn_test.py,22,"b'import numpy as np\nimport tensorflow as tf\n\nfrom easydict import EasyDict\nfrom luminoth.models.fasterrcnn.rcnn import RCNN\n\n\nclass MockBaseNetwork():\n\n    def _build_tail(self, features, **kargs):\n        return features\n\n\nclass RCNNTest(tf.test.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n        self._num_classes = 5\n        self._num_proposals = 256\n        self._total_num_gt = 128\n        self._image_shape = (600, 800)\n        # The score we\'ll give to the true labels when testing for perfect\n        # score generation.\n        self._high_score = 100\n\n        self._equality_delta = 1e-03\n\n        self._config = EasyDict({\n            \'enabled\': True,\n            \'layer_sizes\': [4096, 4096],\n            \'dropout_keep_prob\': 1.0,\n            \'activation_function\': \'relu6\',\n            \'use_mean\': False,\n            \'target_normalization_variances\': [1., 1.],\n            \'rcnn_initializer\': {\n                \'type\': \'variance_scaling_initializer\',\n                \'factor\': 1.0,\n                \'uniform\': True,\n                \'mode\': \'FAN_AVG\',\n            },\n            \'bbox_initializer\': {\n                \'type\': \'variance_scaling_initializer\',\n                \'factor\': 1.0,\n                \'uniform\': True,\n                \'mode\': \'FAN_AVG\',\n            },\n            \'cls_initializer\': {\n                \'type\': \'variance_scaling_initializer\',\n                \'factor\': 1.0,\n                \'uniform\': True,\n                \'mode\': \'FAN_AVG\',\n            },\n            \'l2_regularization_scale\': 0.0005,\n            \'l1_sigma\': 3.0,\n            \'roi\': {\n                \'pooling_mode\': \'crop\',\n                \'pooled_width\': 7,\n                \'pooled_height\': 7,\n                \'padding\': \'VALID\',\n            },\n            \'proposals\': {\n                \'class_max_detections\': 100,\n                \'class_nms_threshold\': 0.6,\n                \'total_max_detections\': 300,\n                \'min_prob_threshold\': 0.0,\n            },\n            \'target\': {\n                \'foreground_fraction\': 0.25,\n                \'minibatch_size\': 64,\n                \'foreground_threshold\': 0.5,\n                \'background_threshold_high\': 0.5,\n                \'background_threshold_low\': 0.1,\n                \'target_normalization_variances\': [1.0, 1.0],\n            },\n\n        })\n\n        self._base_network = MockBaseNetwork()\n        self._shared_model = RCNN(self._num_classes, self._config)\n\n        # Declare placeholders\n        # We use the \'_ph\' suffix for placeholders.\n        self._pretrained_feature_map_shape = (\n            self._num_proposals,\n            self._config.roi.pooled_width,\n            self._config.roi.pooled_height,\n            4\n        )\n        self._pretrained_feature_map_ph = tf.placeholder(\n            tf.float32, shape=self._pretrained_feature_map_shape\n        )\n\n        self._proposals_shape = (self._num_proposals, 4)\n        self._proposals_ph = tf.placeholder(\n            tf.float32, shape=self._proposals_shape\n        )\n\n        self._image_shape_shape = (2,)\n        self._image_shape_ph = tf.placeholder(\n            tf.float32, shape=self._image_shape_shape\n        )\n\n        self._gt_boxes_shape = (self._total_num_gt, 5)\n        self._gt_boxes_ph = tf.placeholder(\n            tf.float32, shape=self._gt_boxes_shape\n        )\n\n    def _run_net_with_feed_dict(self, net, feed_dict):\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            return sess.run(net, feed_dict=feed_dict)\n\n    def _check_returning_shapes(self, prediction_dict, training=False):\n        """"""Asserts a prediction_dict has the right shapes.\n\n        This includes testing that:\n            - objects, objects_labels and objects_labels_prob have the same\n                shape in the first dimension. (i.e. the same number of\n                objects).\n            - objects has shape (_, 4). objects_labels and objects_labels_prob\n                have shape (_,).\n            - cls_score and cls_prob have shape (num_proposals,\n                num_classes + 1).\n            - bbox_offsets has shape (num_proposals, num_classes * 4).\n\n        And, if training:\n            - cls_target has shape (num_proposals,).\n            - bbox_offsets_target has shape (num_proposals, 4).\n        """"""\n\n        objects_shape = prediction_dict[\'objects\'].shape\n        objects_labels_shape = prediction_dict[\'labels\'].shape\n        objects_labels_prob_shape = prediction_dict[\'probs\'] \\\n            .shape\n\n        cls_score_shape = prediction_dict[\'rcnn\'][\'cls_score\'].shape\n        cls_prob_shape = prediction_dict[\'rcnn\'][\'cls_prob\'].shape\n\n        bbox_offsets_shape = prediction_dict[\'rcnn\'][\'bbox_offsets\'].shape\n\n        # We choose cls_score as the \'standard\' num_proposals which we will\n        # compare to the other shapes that should include num_proposals. We\n        # could have chosen a different one.\n        num_proposals = cls_score_shape[0]\n\n        self.assertEqual(objects_shape[0], objects_labels_shape[0])\n        self.assertEqual(objects_shape[0], objects_labels_prob_shape[0])\n\n        self.assertEqual(objects_shape[1], 4)\n        self.assertEqual(len(objects_labels_shape), 1)\n        self.assertEqual(len(objects_labels_prob_shape), 1)\n\n        self.assertEqual(cls_score_shape, cls_prob_shape)\n        self.assertEqual(\n            cls_prob_shape,\n            (num_proposals, self._num_classes + 1)\n        )\n\n        self.assertEqual(\n            bbox_offsets_shape,\n            (num_proposals, self._num_classes * 4)\n        )\n\n        if training:\n            cls_target_shape = prediction_dict[\'target\'][\'cls\'].shape\n            self.assertEqual(cls_target_shape, (num_proposals,))\n\n            bbox_offsets_trgt_shape = (\n                prediction_dict[\'target\'][\'bbox_offsets\'].shape\n            )\n            self.assertEqual(\n                bbox_offsets_trgt_shape,\n                (num_proposals, 4)\n            )\n\n    def testReturningShapes(self):\n        """"""Tests we\'re returning consistent shapes.\n\n        We test both the case where we\'re training and the case where we are\n        not.\n        """"""\n\n        # Prediction session (not training)\n        rcnn_net_not_training = self._shared_model(\n            self._pretrained_feature_map_ph, self._proposals_ph,\n            self._image_shape_ph, self._base_network\n        )\n\n        prediction_dict_not_training = self._run_net_with_feed_dict(\n            rcnn_net_not_training,\n            feed_dict={\n                self._pretrained_feature_map_ph: np.random.rand(\n                    *self._pretrained_feature_map_shape\n                ),\n                self._proposals_ph: np.random.randint(\n                    low=0,\n                    high=np.amin(self._image_shape),\n                    size=self._proposals_shape,\n                ),\n                self._image_shape_ph: self._image_shape,\n            }\n        )\n        # Training session\n        rcnn_net_training = self._shared_model(\n            self._pretrained_feature_map_ph, self._proposals_ph,\n            self._image_shape_ph, self._base_network, self._gt_boxes_ph\n        )\n        prediction_dict_training = self._run_net_with_feed_dict(\n            rcnn_net_training,\n            feed_dict={\n                self._pretrained_feature_map_ph: np.random.rand(\n                    *self._pretrained_feature_map_shape\n                ),\n                self._proposals_ph: np.random.randint(\n                    low=0,\n                    high=np.amin(self._image_shape),\n                    size=self._proposals_shape,\n                ),\n                self._image_shape_ph: self._image_shape,\n                self._gt_boxes_ph: np.random.randint(\n                    low=0,\n                    high=np.amin(self._image_shape),\n                    size=self._gt_boxes_shape,\n                ),\n            }\n        )\n        # Assertions\n        self._check_returning_shapes(\n            prediction_dict_not_training\n        )\n        self._check_returning_shapes(\n            prediction_dict_training, training=True\n        )\n\n    def testMinibatchBehaviour(self):\n        """"""Tests we\'re using minibatch_size correctly when testing.\n        """"""\n\n        rcnn_net = self._shared_model(\n            self._pretrained_feature_map_ph, self._proposals_ph,\n            self._image_shape_ph, self._base_network, self._gt_boxes_ph\n        )\n\n        prediction_dict = self._run_net_with_feed_dict(\n            rcnn_net,\n            feed_dict={\n                self._pretrained_feature_map_ph: np.random.rand(\n                    *self._pretrained_feature_map_shape\n                ),\n                self._proposals_ph: np.random.randint(\n                    low=0,\n                    high=np.amin(self._image_shape),\n                    size=self._proposals_shape,\n                ),\n                self._image_shape_ph: self._image_shape,\n                self._gt_boxes_ph: np.random.randint(\n                    low=0,\n                    high=np.amin(self._image_shape),\n                    size=self._gt_boxes_shape,\n                ),\n            }\n        )\n        # Assertions\n        self.assertLessEqual(\n            prediction_dict[\'target\'][\'cls\'][\n                prediction_dict[\'target\'][\'cls\'] >= 0\n            ].shape[0],\n            self._config.target.minibatch_size,\n        )\n\n    def testNumberOfObjects(self):\n        """"""Tests we\'re not returning too many objects.\n\n        The number of objects returned should be lower than the number of\n        proposals received times the number of classes.\n        """"""\n\n        rcnn_net = self._shared_model(\n            self._pretrained_feature_map_ph, self._proposals_ph,\n            self._image_shape_ph, self._base_network\n        )\n\n        prediction_dict = self._run_net_with_feed_dict(\n            rcnn_net,\n            feed_dict={\n                self._pretrained_feature_map_ph: np.random.rand(\n                    *self._pretrained_feature_map_shape\n                ),\n                self._proposals_ph: np.random.randint(\n                    0,\n                    high=np.amin(self._image_shape),\n                    size=self._proposals_shape,\n                ),\n                self._image_shape_ph: self._image_shape,\n            }\n        )\n        # Assertions\n        self.assertLessEqual(\n            prediction_dict[\'objects\'].shape[0],\n            self._num_proposals * self._num_classes\n        )\n\n    def testLoss(self):\n        """"""Tests we\'re computing loss correctly.\n\n        In particular, we\'re testing whether computing a perfect score when we\n        have to.\n        """"""\n\n        # Generate placeholders and loss_graph\n        cls_score_shape = (self._num_proposals, self._num_classes + 1)\n        cls_score_ph = tf.placeholder(\n            tf.float32,\n            cls_score_shape\n        )\n\n        cls_prob_shape = (self._num_proposals, self._num_classes + 1)\n        cls_prob_ph = tf.placeholder(\n            tf.float32,\n            cls_prob_shape\n        )\n\n        cls_target_shape = (self._num_proposals,)\n        cls_target_ph = tf.placeholder(\n            tf.float32,\n            cls_target_shape\n        )\n\n        bbox_offsets_shape = (self._num_proposals, self._num_classes * 4)\n        bbox_offsets_ph = tf.placeholder(\n            tf.float32,\n            bbox_offsets_shape\n        )\n\n        bbox_offsets_target_shape = (self._num_proposals, 4)\n        bbox_offsets_target_ph = tf.placeholder(\n            tf.float32,\n            bbox_offsets_target_shape\n        )\n\n        loss_graph = self._shared_model.loss({\n            \'rcnn\': {\n                \'cls_score\': cls_score_ph,\n                \'cls_prob\': cls_prob_ph,\n                \'bbox_offsets\': bbox_offsets_ph,\n            },\n            \'target\': {\n                \'cls\': cls_target_ph,\n                \'bbox_offsets\': bbox_offsets_target_ph,\n            }\n        })\n\n        # Generate values that ensure a perfect score\n        # We first initialize all our values to zero.\n        cls_score = np.zeros(cls_score_shape, dtype=np.float32)\n        cls_prob = np.zeros(cls_prob_shape, dtype=np.float32)\n        cls_target = np.zeros(cls_target_shape, dtype=np.float32)\n        bbox_offsets = np.zeros(bbox_offsets_shape, dtype=np.float32)\n        bbox_offsets_target = np.zeros(\n            bbox_offsets_target_shape,\n            dtype=np.float32\n        )\n        for i in range(self._num_proposals):\n            this_class = np.random.randint(low=1, high=self._num_classes + 1)\n\n            cls_score[i][this_class] = self._high_score\n            cls_prob[i][this_class] = 1.\n            cls_target[i] = this_class\n\n            # Find out where in the axis 1 in bbox_offsets we should\n            # put the offsets, because the shape is\n            # (num_proposals, num_classes * 4), and we\'re using\n            # 1-indexed classes.\n            class_place = (this_class - 1) * 4\n            for j in range(4):\n                this_coord = np.random.randint(\n                    low=0,\n                    high=np.amax(self._image_shape)\n                )\n\n                bbox_offsets[i][class_place + j] = this_coord\n                bbox_offsets_target[i][j] = this_coord\n        # Now get the loss dict using the values we just generated.\n        loss_dict = self._run_net_with_feed_dict(\n            loss_graph,\n            feed_dict={\n                cls_score_ph: cls_score,\n                cls_prob_ph: cls_prob,\n                cls_target_ph: cls_target,\n                bbox_offsets_ph: bbox_offsets,\n                bbox_offsets_target_ph: bbox_offsets_target,\n            }\n        )\n        # Assertions\n        self.assertAlmostEqual(\n            loss_dict[\'rcnn_cls_loss\'], 0,\n            delta=self._equality_delta\n        )\n        self.assertAlmostEqual(\n            loss_dict[\'rcnn_reg_loss\'], 0,\n            delta=self._equality_delta\n        )\n\n\nif __name__ == \'__main__\':\n    tf.test.main()\n'"
luminoth/models/fasterrcnn/roi_pool.py,9,"b'import sonnet as snt\nimport tensorflow as tf\n\n# Types of RoI ""pooling""\nCROP = \'crop\'\nROI_POOLING = \'roi_pooling\'\n\n\nclass ROIPoolingLayer(snt.AbstractModule):\n    """"""ROIPoolingLayer applies ROI Pooling (or tf.crop_and_resize).\n\n    RoI pooling or RoI extraction is used to extract fixed size features from a\n    variable sized feature map using variabled sized bounding boxes. Since we\n    have proposals of different shapes and sizes, we need a way to transform\n    them into a fixed size Tensor for using FC layers.\n\n    There are two basic ways to do this, the original one in the FasterRCNN\'s\n    paper is RoI Pooling, which as the name suggests, it maxpools directly from\n    the region of interest, or proposal, into a fixed size Tensor.\n\n    The alternative way uses TensorFlow\'s image utility operation called,\n    `crop_and_resize` which first crops an Tensor using a normalized proposal,\n    and then applies extrapolation to resize it to the desired size,\n    generating a fixed size Tensor.\n\n    Since there isn\'t a std support implemenation of RoIPooling, we apply the\n    easier but still proven alternatve way.\n    """"""\n    def __init__(self, config, debug=False, name=\'roi_pooling\'):\n        super(ROIPoolingLayer, self).__init__(name=name)\n        self._pooling_mode = config.pooling_mode.lower()\n        self._pooled_width = config.pooled_width\n        self._pooled_height = config.pooled_height\n        self._pooled_padding = config.padding\n        self._debug = debug\n\n    def _get_bboxes(self, roi_proposals, im_shape):\n        """"""\n        Gets normalized coordinates for RoIs (between 0 and 1 for cropping)\n        in TensorFlow\'s order (y1, x1, y2, x2).\n\n        Args:\n            roi_proposals: A Tensor with the bounding boxes of shape\n                (total_proposals, 5), where the values for each proposal are\n                (x_min, y_min, x_max, y_max).\n            im_shape: A Tensor with the shape of the image (height, width).\n\n        Returns:\n            bboxes: A Tensor with normalized bounding boxes in TensorFlow\'s\n                format order. Its should is (total_proposals, 4).\n        """"""\n        with tf.name_scope(\'get_bboxes\'):\n            im_shape = tf.cast(im_shape, tf.float32)\n\n            x1, y1, x2, y2 = tf.unstack(\n                roi_proposals, axis=1\n            )\n\n            x1 = x1 / im_shape[1]\n            y1 = y1 / im_shape[0]\n            x2 = x2 / im_shape[1]\n            y2 = y2 / im_shape[0]\n\n            bboxes = tf.stack([y1, x1, y2, x2], axis=1)\n\n            return bboxes\n\n    def _roi_crop(self, roi_proposals, conv_feature_map, im_shape):\n        # Get normalized bounding boxes.\n        bboxes = self._get_bboxes(roi_proposals, im_shape)\n        # Generate fake batch ids\n        bboxes_shape = tf.shape(bboxes)\n        batch_ids = tf.zeros((bboxes_shape[0], ), dtype=tf.int32)\n        # Apply crop and resize with extracting a crop double the desired size.\n        crops = tf.image.crop_and_resize(\n            conv_feature_map, bboxes, batch_ids,\n            [self._pooled_width * 2, self._pooled_height * 2], name=""crops""\n        )\n\n        # Applies max pool with [2,2] kernel to reduce the crops to half the\n        # size, and thus having the desired output.\n        prediction_dict = {\n            \'roi_pool\': tf.nn.max_pool(\n                crops, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n                padding=self._pooled_padding\n            ),\n        }\n\n        if self._debug:\n            prediction_dict[\'bboxes\'] = bboxes\n            prediction_dict[\'crops\'] = crops\n            prediction_dict[\'batch_ids\'] = batch_ids\n            prediction_dict[\'conv_feature_map\'] = conv_feature_map\n\n        return prediction_dict\n\n    def _roi_pooling(self, roi_proposals, conv_feature_map, im_shape):\n        raise NotImplementedError()\n\n    def _build(self, roi_proposals, conv_feature_map, im_shape):\n        if self._pooling_mode == CROP:\n            return self._roi_crop(roi_proposals, conv_feature_map, im_shape)\n        elif self._pooling_mode == ROI_POOLING:\n            return self._roi_pooling(roi_proposals, conv_feature_map, im_shape)\n        else:\n            raise NotImplementedError(\n                \'Pooling mode {} does not exist.\'.format(self._pooling_mode))\n'"
luminoth/models/fasterrcnn/roi_pool_test.py,9,"b'import numpy as np\nimport tensorflow as tf\n\nfrom easydict import EasyDict\nfrom luminoth.models.fasterrcnn.roi_pool import ROIPoolingLayer\n\n\nclass ROIPoolingTest(tf.test.TestCase):\n\n    def setUp(self):\n        super(ROIPoolingTest, self).setUp()\n        # Setup\n        self.im_shape = (10, 10)\n        self.config = EasyDict({\n            \'pooling_mode\': \'crop\',\n            \'pooled_width\': 2,\n            \'pooled_height\': 2,\n            \'padding\': \'VALID\',\n        })\n        # Construct the pretrained map with four matrix.\n        self.multiplier_a = 1\n        self.multiplier_b = 2\n        self.multiplier_c = 3\n        self.multiplier_d = 4\n        mat_a = np.ones((5, 5)) * self.multiplier_a\n        mat_b = np.ones((5, 5)) * self.multiplier_b\n        mat_c = np.ones((5, 5)) * self.multiplier_c\n        mat_d = np.ones((5, 5)) * self.multiplier_d\n        self.pretrained = np.bmat([[mat_a, mat_b], [mat_c, mat_d]])\n        # Expand the dimensions to be compatible with ROIPoolingLayer.\n        self.pretrained = np.expand_dims(self.pretrained, axis=0)\n        self.pretrained = np.expand_dims(self.pretrained, axis=3)\n        # pretrained:\n        #           mat_a | mat_b\n        #           -------------\n        #           mat_c | mat_d\n        tf.reset_default_graph()\n\n    def _run_roi_pooling(self, roi_proposals, pretrained, config):\n        roi_proposals_tf = tf.placeholder(\n            tf.float32, shape=roi_proposals.shape)\n        pretrained_tf = tf.placeholder(tf.float32, shape=pretrained.shape)\n        im_shape_tf = tf.placeholder(tf.float32, shape=(2,))\n\n        model = ROIPoolingLayer(config, debug=True)\n        results = model(roi_proposals_tf, pretrained_tf, im_shape_tf)\n\n        with self.test_session() as sess:\n            results = sess.run(results, feed_dict={\n                roi_proposals_tf: roi_proposals,\n                pretrained_tf: pretrained,\n                im_shape_tf: self.im_shape,\n            })\n            return results\n\n    def testBasic(self):\n        """"""\n        Test basic max pooling. We have 4 \'roi_proposals\' and use a \'pool size\'\n        of 2x2 (\'pooled_width\': 2, \'pooled_height\': 2), then we will get as\n        result a \'roi_pool\' of 2x2.\n        """"""\n        roi_proposals = np.array([\n            [1, 1, 4, 4],  # Inside mat_A\n            [6, 1, 9, 4],  # Inside mat_B\n            [1, 6, 4, 9],  # Inside mat_C\n            [6, 6, 9, 9],  # Inside mat_D\n        ])\n\n        results = self._run_roi_pooling(\n            roi_proposals, self.pretrained, self.config)\n\n        # Check that crops has the correct shape. This is (4, 4, 4, 1)\n        # because we have 4 proposals, \'pool size\' = 2x2 then the\n        # tf.image.crop_and_resize function duplicates that size.\n        self.assertEqual(\n            results[\'crops\'].shape,\n            (4, 4, 4, 1)\n        )\n\n        # Check that roi_pool has the correct shape. This is (4, 2, 2, 1)\n        # because we have 4 proposals, \'pool size\' = 2x2.\n        self.assertEqual(\n            results[\'roi_pool\'].shape,\n            (4, 2, 2, 1)\n        )\n\n        results[\'roi_pool\'] = np.squeeze(results[\'roi_pool\'], axis=3)\n        # Check that max polling returns only \'multiplier_a\'\n        self.assertAllEqual(\n            results[\'roi_pool\'][0],\n            np.ones((2, 2)) * self.multiplier_a\n        )\n\n        # Check that max polling returns only \'multiplier_b\'\n        self.assertAllEqual(\n            results[\'roi_pool\'][1],\n            np.ones((2, 2)) * self.multiplier_b\n        )\n\n        # Check that max polling returns only \'multiplier_c\'\n        self.assertAllEqual(\n            results[\'roi_pool\'][2],\n            np.ones((2, 2)) * self.multiplier_c\n        )\n\n        # Check that max polling returns only \'multiplier_d\'\n        self.assertAllEqual(\n            results[\'roi_pool\'][3],\n            np.ones((2, 2)) * self.multiplier_d\n        )\n\n    def testMaxPoolingWithoutInterpolation(self):\n        """"""\n        Test max pooling with a little bit more complex \'roi_proposals\'.\n        We have 4 \'roi_proposals\' and use a \'pool size\'\n        of 2x2 (\'pooled_width\': 2, \'pooled_height\': 2), then we will get as\n        result a \'roi_pool\' of 2x2. with\n        """"""\n        roi_proposals = np.array([\n            [3, 1, 6, 4],  # Across mat_A and mat_B (half-half)\n            [1, 3, 4, 7],  # Across mat_A and mat_C (half-half)\n            [5, 3, 9, 7],  # Inside mat_B and mat_D (half-half)\n            [3, 6, 6, 9],  # Inside mat_C and mat_D (half-half)\n        ])\n        a = self.multiplier_a\n        b = self.multiplier_b\n        c = self.multiplier_c\n        d = self.multiplier_d\n\n        results = self._run_roi_pooling(\n            roi_proposals, self.pretrained, self.config)\n\n        # Check that crops has the correct shape. This is (4, 4, 4, 1)\n        # because we have 4 proposals, \'pool size\' = 2x2 then the\n        # tf.image.crop_and_resize function duplicates that size.\n        self.assertEqual(\n            results[\'crops\'].shape,\n            (4, 4, 4, 1)\n        )\n\n        # Check that roi_pool has the correct shape. This is (4, 2, 2, 1)\n        # because we have 4 proposals, \'pool size\' = 2x2.\n        self.assertEqual(\n            results[\'roi_pool\'].shape,\n            (4, 2, 2, 1)\n        )\n\n        results[\'roi_pool\'] = np.squeeze(results[\'roi_pool\'], axis=3)\n        # Check that max polling returns a column of \'one\' and\n        # a column of \'two\'.\n        self.assertAllEqual(\n            results[\'roi_pool\'][0],\n            np.array([[a, b], [a, b]])\n        )\n\n        # Check that max polling returns a row of \'one\' and\n        # a row of \'three\'.\n        self.assertAllEqual(\n            results[\'roi_pool\'][1],\n            np.array([[a, a], [c, c]])\n        )\n\n        # Check that max polling returns a row of \'one\' and\n        # a row of \'three\'.\n        self.assertAllEqual(\n            results[\'roi_pool\'][2],\n            np.array([[b, b], [d, d]])\n        )\n\n        # Check that max polling returns a column of \'three\' and\n        # a column of \'four\'.\n        self.assertAllEqual(\n            results[\'roi_pool\'][3],\n            np.array([[c, d], [c, d]])\n        )\n\n    def testMaxPoolingWithInterpolation(self):\n        """"""\n        Test max pooling with bilinear interpolation.\n        We have 4 \'roi_proposals\' and use a \'pool size\'\n        of 2x2 (\'pooled_width\': 2, \'pooled_height\': 2), then we will get as\n        result a \'roi_pool\' of 2x2.\n        """"""\n\n        roi_proposals = np.array([\n            [4, 1, 7, 4],  # Across mat_A and mat_B (1/4 - 3/4)\n            [1, 4, 4, 8],  # Across mat_A and mat_C (1/4 - 3/4)\n            [5, 4, 9, 8],  # Inside mat_B and mat_D (1/4 - 3/4)\n            [4, 6, 7, 9],  # Inside mat_C and mat_D (1/4 - 3/4)\n        ])\n        a = self.multiplier_a\n        b = self.multiplier_b\n        c = self.multiplier_c\n        d = self.multiplier_d\n\n        results = self._run_roi_pooling(\n            roi_proposals, self.pretrained, self.config)\n\n        results[\'roi_pool\'] = np.squeeze(results[\'roi_pool\'], axis=3)\n\n        # Check that max polling returns values greater or equal\n        # than \'a\' and crops returns values lower or equal than \'b\'\n        self.assertTrue(\n            np.greater_equal(results[\'roi_pool\'][0], a).all()\n        )\n\n        self.assertTrue(\n            np.less_equal(results[\'crops\'][0], b).all()\n        )\n\n        # Check that max polling returns values greater or equal\n        # than \'a\' and crops returns values lower or equal than \'c\'\n        self.assertTrue(\n            np.greater_equal(results[\'roi_pool\'][1], a).all()\n        )\n\n        self.assertTrue(\n            np.less_equal(results[\'crops\'][1], c).all()\n        )\n\n        # Check that max polling returns values greater or equal\n        # than \'b\' and crops returns values lower or equal than \'d\'\n        self.assertTrue(\n            np.greater_equal(results[\'roi_pool\'][2], b).all()\n        )\n\n        self.assertTrue(\n            np.less_equal(results[\'crops\'][2], d).all()\n        )\n\n        # Check that max polling returns values greater or equal\n        # than \'c\' and crops returns values lower or equal than \'d\'\n        self.assertTrue(\n            np.greater_equal(results[\'roi_pool\'][3], c).all()\n        )\n\n        self.assertTrue(\n            np.less_equal(results[\'crops\'][3], d).all()\n        )\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
luminoth/models/fasterrcnn/rpn.py,32,"b'""""""\nRPN - Region Proposal Network\n""""""\n\nimport sonnet as snt\nimport tensorflow as tf\n\nfrom sonnet.python.modules.conv import Conv2D\n\nfrom .rpn_target import RPNTarget\nfrom .rpn_proposal import RPNProposal\nfrom luminoth.utils.losses import smooth_l1_loss\nfrom luminoth.utils.vars import (\n    get_initializer, layer_summaries, variable_summaries,\n    get_activation_function\n)\n\n\nclass RPN(snt.AbstractModule):\n\n    def __init__(self, num_anchors, config, debug=False, seed=None,\n                 name=\'rpn\'):\n        """"""RPN - Region Proposal Network.\n\n        Given an image (as feature map) and a fixed set of anchors, the RPN\n        will learn weights to adjust those anchors so they better look like the\n        ground truth objects, as well as scoring them by ""objectness"" (ie. how\n        likely they are to be an object vs background).\n\n        The final result will be a set of rectangular boxes (""proposals""),\n        each associated with an objectness score.\n\n        Note: this module can be used independently of Faster R-CNN.\n        """"""\n        super(RPN, self).__init__(name=name)\n        self._num_anchors = num_anchors\n        self._num_channels = config.num_channels\n        self._kernel_shape = config.kernel_shape\n\n        self._debug = debug\n        self._seed = seed\n\n        self._rpn_initializer = get_initializer(\n            config.rpn_initializer, seed=seed\n        )\n        # According to Faster RCNN paper we need to initialize layers with\n        # ""from a zero-mean Gaussian distribution with standard deviation 0.01\n        self._cls_initializer = get_initializer(\n            config.cls_initializer, seed=seed\n        )\n        self._bbox_initializer = get_initializer(\n            config.bbox_initializer, seed=seed\n        )\n        self._regularizer = tf.contrib.layers.l2_regularizer(\n            scale=config.l2_regularization_scale\n        )\n\n        self._l1_sigma = config.l1_sigma\n\n        # We could use normal relu without any problems.\n        self._rpn_activation = get_activation_function(\n            config.activation_function\n        )\n\n        self._config = config\n\n    def _instantiate_layers(self):\n        """"""Instantiates all convolutional modules used in the RPN.""""""\n        self._rpn = Conv2D(\n            output_channels=self._num_channels,\n            kernel_shape=self._kernel_shape,\n            initializers={\'w\': self._rpn_initializer},\n            regularizers={\'w\': self._regularizer},\n            name=\'conv\'\n        )\n\n        self._rpn_cls = Conv2D(\n            output_channels=self._num_anchors * 2, kernel_shape=[1, 1],\n            initializers={\'w\': self._cls_initializer},\n            regularizers={\'w\': self._regularizer},\n            padding=\'VALID\', name=\'cls_conv\'\n        )\n\n        # BBox prediction is 4 values * number of anchors.\n        self._rpn_bbox = Conv2D(\n            output_channels=self._num_anchors * 4, kernel_shape=[1, 1],\n            initializers={\'w\': self._bbox_initializer},\n            regularizers={\'w\': self._regularizer},\n            padding=\'VALID\', name=\'bbox_conv\'\n        )\n\n    def _build(self, conv_feature_map, im_shape, all_anchors,\n               gt_boxes=None, is_training=False):\n        """"""Builds the RPN model subgraph.\n\n        Args:\n            conv_feature_map: A Tensor with the output of some pretrained\n                network. Its dimensions should be\n                `[1, feature_map_height, feature_map_width, depth]` where depth\n                is 512 for the default layer in VGG and 1024 for the default\n                layer in ResNet.\n            im_shape: A Tensor with the shape of the original image.\n            all_anchors: A Tensor with all the anchor bounding boxes. Its shape\n                should be\n                [feature_map_height * feature_map_width * total_anchors, 4]\n            gt_boxes: A Tensor with the ground-truth boxes for the image.\n                Its dimensions should be `[total_gt_boxes, 5]`, and it should\n                consist of [x1, y1, x2, y2, label], being (x1, y1) -> top left\n                point, and (x2, y2) -> bottom right point of the bounding box.\n\n        Returns:\n            prediction_dict: A dict with the following keys:\n                proposals: A Tensor with a variable number of proposals for\n                    objects on the image.\n                scores: A Tensor with a ""objectness"" probability for each\n                    proposal. The score should be the output of the softmax for\n                    object.\n\n                If training is True, then some more Tensors are added to the\n                prediction dictionary to be used for calculating the loss.\n\n                rpn_cls_prob: A Tensor with the probability of being\n                    background and foreground for each anchor.\n                rpn_cls_score: A Tensor with the cls score of being background\n                    and foreground for each anchor (the input for the softmax).\n                rpn_bbox_pred: A Tensor with the bounding box regression for\n                    each anchor.\n                rpn_cls_target: A Tensor with the target for each of the\n                    anchors. The shape is [num_anchors,].\n                rpn_bbox_target: A Tensor with the target for each of the\n                    anchors. In case of ignoring the anchor for the target then\n                    we still have a bbox target for each anchors, and it\'s\n                    filled with zeroes when ignored.\n        """"""\n        # We start with a common conv layer applied to the feature map.\n        self._instantiate_layers()\n        self._proposal = RPNProposal(\n            self._num_anchors, self._config.proposals, debug=self._debug\n        )\n        self._anchor_target = RPNTarget(\n            self._num_anchors, self._config.target, seed=self._seed\n        )\n\n        prediction_dict = {}\n\n        # Get the RPN feature using a simple conv net. Activation function\n        # can be set to empty.\n        rpn_conv_feature = self._rpn(conv_feature_map)\n        rpn_feature = self._rpn_activation(rpn_conv_feature)\n\n        # Then we apply separate conv layers for classification and regression.\n        rpn_cls_score_original = self._rpn_cls(rpn_feature)\n        rpn_bbox_pred_original = self._rpn_bbox(rpn_feature)\n        # rpn_cls_score_original has shape (1, H, W, num_anchors * 2)\n        # rpn_bbox_pred_original has shape (1, H, W, num_anchors * 4)\n        # where H, W are height and width of the pretrained feature map.\n\n        # Convert (flatten) `rpn_cls_score_original` which has two scalars per\n        # anchor per location to be able to apply softmax.\n        rpn_cls_score = tf.reshape(rpn_cls_score_original, [-1, 2])\n        # Now that `rpn_cls_score` has shape (H * W * num_anchors, 2), we apply\n        # softmax to the last dim.\n        rpn_cls_prob = tf.nn.softmax(rpn_cls_score)\n\n        prediction_dict[\'rpn_cls_prob\'] = rpn_cls_prob\n        prediction_dict[\'rpn_cls_score\'] = rpn_cls_score\n\n        # Flatten bounding box delta prediction for easy manipulation.\n        # We end up with `rpn_bbox_pred` having shape (H * W * num_anchors, 4).\n        rpn_bbox_pred = tf.reshape(rpn_bbox_pred_original, [-1, 4])\n\n        prediction_dict[\'rpn_bbox_pred\'] = rpn_bbox_pred\n\n        # We have to convert bbox deltas to usable bounding boxes and remove\n        # redundant ones using Non Maximum Suppression (NMS).\n        proposal_prediction = self._proposal(\n            rpn_cls_prob, rpn_bbox_pred, all_anchors, im_shape)\n\n        prediction_dict[\'proposals\'] = proposal_prediction[\'proposals\']\n        prediction_dict[\'scores\'] = proposal_prediction[\'scores\']\n\n        if self._debug:\n            prediction_dict[\'proposal_prediction\'] = proposal_prediction\n\n        if gt_boxes is not None:\n            # When training we use a separate module to calculate the target\n            # values we want to output.\n            (rpn_cls_target, rpn_bbox_target,\n             rpn_max_overlap) = self._anchor_target(\n                all_anchors, gt_boxes, im_shape\n            )\n\n            prediction_dict[\'rpn_cls_target\'] = rpn_cls_target\n            prediction_dict[\'rpn_bbox_target\'] = rpn_bbox_target\n\n            if self._debug:\n                prediction_dict[\'rpn_max_overlap\'] = rpn_max_overlap\n                variable_summaries(rpn_bbox_target, \'rpn_bbox_target\', \'full\')\n\n        # Variables summaries.\n        variable_summaries(prediction_dict[\'scores\'], \'rpn_scores\', \'reduced\')\n        variable_summaries(rpn_cls_prob, \'rpn_cls_prob\', \'reduced\')\n        variable_summaries(rpn_bbox_pred, \'rpn_bbox_pred\', \'reduced\')\n\n        if self._debug:\n            variable_summaries(rpn_feature, \'rpn_feature\', \'full\')\n            variable_summaries(\n               rpn_cls_score_original, \'rpn_cls_score_original\', \'full\')\n            variable_summaries(\n               rpn_bbox_pred_original, \'rpn_bbox_pred_original\', \'full\')\n\n            # Layer summaries.\n            layer_summaries(self._rpn, \'full\')\n            layer_summaries(self._rpn_cls, \'full\')\n            layer_summaries(self._rpn_bbox, \'full\')\n\n        return prediction_dict\n\n    def loss(self, prediction_dict):\n        """"""\n        Returns cost for Region Proposal Network based on:\n\n        Args:\n            rpn_cls_score: Score for being an object or not for each anchor\n                in the image. Shape: (num_anchors, 2)\n            rpn_cls_target: Ground truth labeling for each anchor. Should be\n                * 1: for positive labels\n                * 0: for negative labels\n                * -1: for labels we should ignore.\n                Shape: (num_anchors, )\n            rpn_bbox_target: Bounding box output delta target for rpn.\n                Shape: (num_anchors, 4)\n            rpn_bbox_pred: Bounding box output delta prediction for rpn.\n                Shape: (num_anchors, 4)\n        Returns:\n            Multiloss between cls probability and bbox target.\n        """"""\n\n        rpn_cls_score = prediction_dict[\'rpn_cls_score\']\n        rpn_cls_target = prediction_dict[\'rpn_cls_target\']\n\n        rpn_bbox_target = prediction_dict[\'rpn_bbox_target\']\n        rpn_bbox_pred = prediction_dict[\'rpn_bbox_pred\']\n\n        with tf.variable_scope(\'RPNLoss\'):\n            # Flatten already flat Tensor for usage as boolean mask filter.\n            rpn_cls_target = tf.cast(tf.reshape(\n                rpn_cls_target, [-1]), tf.int32, name=\'rpn_cls_target\')\n            # Transform to boolean tensor mask for not ignored.\n            labels_not_ignored = tf.not_equal(\n                rpn_cls_target, -1, name=\'labels_not_ignored\')\n\n            # Now we only have the labels we are going to compare with the\n            # cls probability.\n            labels = tf.boolean_mask(rpn_cls_target, labels_not_ignored)\n            cls_score = tf.boolean_mask(rpn_cls_score, labels_not_ignored)\n\n            # We need to transform `labels` to `cls_score` shape.\n            # convert [1, 0] to [[0, 1], [1, 0]] for ce with logits.\n            cls_target = tf.one_hot(labels, depth=2)\n\n            # Equivalent to log loss\n            ce_per_anchor = tf.nn.softmax_cross_entropy_with_logits_v2(\n                labels=cls_target, logits=cls_score\n            )\n            prediction_dict[\'cross_entropy_per_anchor\'] = ce_per_anchor\n\n            # Finally, we need to calculate the regression loss over\n            # `rpn_bbox_target` and `rpn_bbox_pred`.\n            # We use SmoothL1Loss.\n            rpn_bbox_target = tf.reshape(rpn_bbox_target, [-1, 4])\n            rpn_bbox_pred = tf.reshape(rpn_bbox_pred, [-1, 4])\n\n            # We only care for positive labels (we ignore backgrounds since\n            # we don\'t have any bounding box information for it).\n            positive_labels = tf.equal(rpn_cls_target, 1)\n            rpn_bbox_target = tf.boolean_mask(rpn_bbox_target, positive_labels)\n            rpn_bbox_pred = tf.boolean_mask(rpn_bbox_pred, positive_labels)\n\n            # We apply smooth l1 loss as described by the Fast R-CNN paper.\n            reg_loss_per_anchor = smooth_l1_loss(\n                rpn_bbox_pred, rpn_bbox_target, sigma=self._l1_sigma\n            )\n\n            prediction_dict[\'reg_loss_per_anchor\'] = reg_loss_per_anchor\n\n            # Loss summaries.\n            tf.summary.scalar(\'batch_size\', tf.shape(labels)[0], [\'rpn\'])\n            foreground_cls_loss = tf.boolean_mask(\n                ce_per_anchor, tf.equal(labels, 1))\n            background_cls_loss = tf.boolean_mask(\n                ce_per_anchor, tf.equal(labels, 0))\n            tf.summary.scalar(\n                \'foreground_cls_loss\',\n                tf.reduce_mean(foreground_cls_loss), [\'rpn\'])\n            tf.summary.histogram(\n                \'foreground_cls_loss\', foreground_cls_loss, [\'rpn\'])\n            tf.summary.scalar(\n                \'background_cls_loss\',\n                tf.reduce_mean(background_cls_loss), [\'rpn\'])\n            tf.summary.histogram(\n                \'background_cls_loss\', background_cls_loss, [\'rpn\'])\n            tf.summary.scalar(\n                \'foreground_samples\', tf.shape(rpn_bbox_target)[0], [\'rpn\'])\n\n            return {\n                \'rpn_cls_loss\': tf.reduce_mean(ce_per_anchor),\n                \'rpn_reg_loss\': tf.reduce_mean(reg_loss_per_anchor),\n            }\n'"
luminoth/models/fasterrcnn/rpn_proposal.py,37,"b'import sonnet as snt\nimport tensorflow as tf\n\nfrom luminoth.utils.bbox_transform_tf import decode, clip_boxes, change_order\n\n\nclass RPNProposal(snt.AbstractModule):\n    """"""Transforms anchors and RPN predictions into object proposals.\n\n    Using the fixed anchors and the RPN predictions for both classification\n    and regression (adjusting the bounding box), we return a list of objects\n    sorted by relevance.\n\n    Besides applying the transformations (or adjustments) from the prediction,\n    it tries to get rid of duplicate proposals by using non maximum supression\n    (NMS).\n    """"""\n    def __init__(self, num_anchors, config, debug=False,\n                 name=\'proposal_layer\'):\n        super(RPNProposal, self).__init__(name=name)\n        self._num_anchors = num_anchors\n\n        # Filtering config\n        # Before applying NMS we filter the top N anchors.\n        self._pre_nms_top_n = config.pre_nms_top_n\n        self._apply_nms = config.apply_nms\n        # After applying NMS we filter the top M anchors.\n        # It\'s important to understand that because of NMS, it is not certain\n        # we will have this many output proposals. This is just the upper\n        # bound.\n        self._post_nms_top_n = config.post_nms_top_n\n        # Threshold to use for NMS.\n        self._nms_threshold = float(config.nms_threshold)\n        # Currently we do not filter out proposals by size.\n        self._min_size = config.min_size\n        self._filter_outside_anchors = config.filter_outside_anchors\n        self._clip_after_nms = config.clip_after_nms\n        self._min_prob_threshold = float(config.min_prob_threshold)\n        self._debug = debug\n\n    def _build(self, rpn_cls_prob, rpn_bbox_pred, all_anchors, im_shape):\n        """"""\n\n        Args:\n            rpn_cls_prob: A Tensor with the softmax output for each anchor.\n                Its shape should be (total_anchors, 2), with the probability of\n                being background and the probability of being foreground for\n                each anchor.\n            rpn_bbox_pred: A Tensor with the regression output for each anchor.\n                Its shape should be (total_anchors, 4).\n            all_anchors: A Tensor with the anchors bounding boxes of shape\n                (total_anchors, 4), having (x_min, y_min, x_max, y_max) for\n                each anchor.\n            im_shape: A Tensor with the image shape in format (height, width).\n\n        Returns:\n            prediction_dict with the following keys:\n                proposals: A Tensor with the final selected proposed\n                    bounding boxes. Its shape should be\n                    (total_proposals, 4).\n                scores: A Tensor with the probability of being an\n                    object for that proposal. Its shape should be\n                    (total_proposals, 1)\n        """"""\n        # Scores are extracted from the second scalar of the cls probability.\n        # cls_probability is a softmax of (background, foreground).\n        all_scores = rpn_cls_prob[:, 1]\n        # Force flatten the scores (it should be already be flatten).\n        all_scores = tf.reshape(all_scores, [-1])\n\n        if self._filter_outside_anchors:\n            with tf.name_scope(\'filter_outside_anchors\'):\n                (x_min_anchor, y_min_anchor,\n                 x_max_anchor, y_max_anchor) = tf.unstack(all_anchors, axis=1)\n\n                anchor_filter = tf.logical_and(\n                    tf.logical_and(\n                        tf.greater_equal(x_min_anchor, 0),\n                        tf.greater_equal(y_min_anchor, 0)\n                    ),\n                    tf.logical_and(\n                        tf.less(x_max_anchor, im_shape[1]),\n                        tf.less(y_max_anchor, im_shape[0])\n                    )\n                )\n                anchor_filter = tf.reshape(anchor_filter, [-1])\n                all_anchors = tf.boolean_mask(\n                    all_anchors, anchor_filter, name=\'filter_anchors\')\n                rpn_bbox_pred = tf.boolean_mask(rpn_bbox_pred, anchor_filter)\n                all_scores = tf.boolean_mask(all_scores, anchor_filter)\n\n        # Decode boxes\n        all_proposals = decode(all_anchors, rpn_bbox_pred)\n\n        # Filter proposals with less than threshold probability.\n        min_prob_filter = tf.greater_equal(\n            all_scores, self._min_prob_threshold\n        )\n\n        # Filter proposals with negative or zero area.\n        (x_min, y_min, x_max, y_max) = tf.unstack(all_proposals, axis=1)\n        zero_area_filter = tf.greater(\n            tf.maximum(x_max - x_min, 0.0) * tf.maximum(y_max - y_min, 0.0),\n            0.0\n        )\n        proposal_filter = tf.logical_and(zero_area_filter, min_prob_filter)\n\n        # Filter proposals and scores.\n        all_proposals_total = tf.shape(all_scores)[0]\n        unsorted_scores = tf.boolean_mask(\n            all_scores, proposal_filter,\n            name=\'filtered_scores\'\n        )\n        unsorted_proposals = tf.boolean_mask(\n            all_proposals, proposal_filter,\n            name=\'filtered_proposals\'\n        )\n        if self._debug:\n            proposals_unclipped = tf.identity(unsorted_proposals)\n\n        if not self._clip_after_nms:\n            # Clip proposals to the image.\n            unsorted_proposals = clip_boxes(unsorted_proposals, im_shape)\n\n        filtered_proposals_total = tf.shape(unsorted_scores)[0]\n\n        tf.summary.scalar(\n            \'valid_proposals_ratio\',\n            (\n                tf.cast(filtered_proposals_total, tf.float32) /\n                tf.cast(all_proposals_total, tf.float32)\n            ), [\'rpn\'])\n\n        tf.summary.scalar(\n            \'invalid_proposals\',\n            all_proposals_total - filtered_proposals_total, [\'rpn\'])\n\n        # Get top `pre_nms_top_n` indices by sorting the proposals by score.\n        k = tf.minimum(self._pre_nms_top_n, tf.shape(unsorted_scores)[0])\n        top_k = tf.nn.top_k(unsorted_scores, k=k)\n\n        sorted_top_proposals = tf.gather(unsorted_proposals, top_k.indices)\n        sorted_top_scores = top_k.values\n\n        if self._apply_nms:\n            with tf.name_scope(\'nms\'):\n                # We reorder the proposals into TensorFlows bounding box order\n                # for `tf.image.non_max_supression` compatibility.\n                proposals_tf_order = change_order(sorted_top_proposals)\n                # We cut the pre_nms filter in pure TF version and go straight\n                # into NMS.\n                selected_indices = tf.image.non_max_suppression(\n                    proposals_tf_order, tf.reshape(\n                        sorted_top_scores, [-1]\n                    ),\n                    self._post_nms_top_n, iou_threshold=self._nms_threshold\n                )\n\n                # Selected_indices is a smaller tensor, we need to extract the\n                # proposals and scores using it.\n                nms_proposals_tf_order = tf.gather(\n                    proposals_tf_order, selected_indices,\n                    name=\'gather_nms_proposals\'\n                )\n\n                # We switch back again to the regular bbox encoding.\n                proposals = change_order(nms_proposals_tf_order)\n                scores = tf.gather(\n                    sorted_top_scores, selected_indices,\n                    name=\'gather_nms_proposals_scores\'\n                )\n        else:\n            proposals = sorted_top_proposals\n            scores = sorted_top_scores\n\n        if self._clip_after_nms:\n            # Clip proposals to the image after NMS.\n            proposals = clip_boxes(proposals, im_shape)\n\n        pred = {\n            \'proposals\': proposals,\n            \'scores\': scores,\n        }\n\n        if self._debug:\n            pred.update({\n                \'sorted_top_scores\': sorted_top_scores,\n                \'sorted_top_proposals\': sorted_top_proposals,\n                \'unsorted_proposals\': unsorted_proposals,\n                \'unsorted_scores\': unsorted_scores,\n                \'all_proposals\': all_proposals,\n                \'all_scores\': all_scores,\n                # proposals_unclipped has the unsorted_scores scores\n                \'proposals_unclipped\': proposals_unclipped,\n            })\n\n        return pred\n'"
luminoth/models/fasterrcnn/rpn_proposal_test.py,15,"b'import numpy as np\nimport tensorflow as tf\n\nfrom easydict import EasyDict\nfrom luminoth.models.fasterrcnn.rpn_proposal import RPNProposal\nfrom luminoth.utils.bbox_transform_tf import encode, clip_boxes\n\n\nclass RPNProposalTest(tf.test.TestCase):\n\n    def setUp(self):\n        super(RPNProposalTest, self).setUp()\n        # Setup\n        self.im_size = (40, 40)\n        self.config = EasyDict({\n            \'pre_nms_top_n\': 4,\n            \'post_nms_top_n\': 3,\n            \'nms_threshold\': 1,\n            \'min_size\': 0,\n            \'clip_after_nms\': False,\n            \'filter_outside_anchors\': False,\n            \'apply_nms\': True,\n            \'min_prob_threshold\': 0.0,\n        })\n        tf.reset_default_graph()\n\n    def _run_rpn_proposal(self, all_anchors, rpn_cls_prob, config,\n                          gt_boxes=None, rpn_bbox_pred=None):\n        """"""\n        Define one of gt_boxes or rpn_bbox_pred.\n\n        If using gt_boxes, the correct rpn_bbox_pred for those gt_boxes will\n        be used.\n        """"""\n        feed_dict = {}\n        rpn_cls_prob_tf = tf.placeholder(\n            tf.float32, shape=(all_anchors.shape[0], 2))\n        feed_dict[rpn_cls_prob_tf] = rpn_cls_prob\n        im_size_tf = tf.placeholder(tf.float32, shape=(2,))\n        feed_dict[im_size_tf] = self.im_size\n        all_anchors_tf = tf.placeholder(tf.float32, shape=all_anchors.shape)\n        feed_dict[all_anchors_tf] = all_anchors\n        if rpn_bbox_pred is None and gt_boxes is not None:\n            # Here we encode \'all_anchors\' and \'gt_boxes\' to get corrects\n            # predictions that RPNProposal can decodes.\n            rpn_bbox_pred_tf = encode(all_anchors, gt_boxes)\n        else:\n            rpn_bbox_pred_tf = tf.placeholder(\n                tf.float32, shape=rpn_bbox_pred.shape\n            )\n            feed_dict[rpn_bbox_pred_tf] = rpn_bbox_pred\n\n        model = RPNProposal(all_anchors.shape[0], config, debug=True)\n        results = model(\n            rpn_cls_prob_tf, rpn_bbox_pred_tf, all_anchors_tf, im_size_tf)\n\n        with self.test_session() as sess:\n            results = sess.run(results, feed_dict=feed_dict)\n            return results\n\n    def testNMSThreshold(self):\n        """"""\n        Test nms threshold\n        """"""\n        gt_boxes = np.array([\n            [10, 10, 26, 36],\n            [10, 10, 20, 22],\n            [10, 11, 20, 21],\n            [19, 30, 33, 38],\n        ])\n        """"""\n        IoU Matrix of gt_boxes\n        [[ 1.          0.31154684  0.26361656  0.10408922]\n         [ 0.31154684  1.          0.84615385  0.        ]\n         [ 0.26361656  0.84615385  1.          0.        ]\n         [ 0.10408922  0.          0.          1.        ]]\n        """"""\n        all_anchors = np.array([\n            [11, 13, 34, 31],\n            [10, 10, 20, 22],\n            [11, 13, 34, 28],\n            [21, 29, 34, 37],\n        ])\n        rpn_cls_prob = np.array([\n            [0.8, 0.2],\n            [0.1, 0.9],\n            [0.4, 0.6],\n            [0.2, 0.8]\n        ])\n        config = self.config\n        config[\'post_nms_top_n\'] = 4\n        config[\'nms_threshold\'] = 0.0\n\n        results = self._run_rpn_proposal(\n            all_anchors, rpn_cls_prob, self.config, gt_boxes=gt_boxes)\n\n        # Check we get exactly 2 \'nms proposals\' because 2 IoU equals to 0.\n        # Also check that we get the corrects scores.\n        self.assertEqual(\n            results[\'proposals\'].shape,\n            (2, 4)\n        )\n\n        self.assertAllClose(\n            results[\'scores\'],\n            [0.9, 0.8]\n        )\n\n        config[\'nms_threshold\'] = 0.3\n\n        results = self._run_rpn_proposal(\n            all_anchors, rpn_cls_prob, self.config, gt_boxes=gt_boxes)\n\n        # Check we get exactly 3 \'nms proposals\' because 3 IoU lowers than 0.3.\n        # Also check that we get the corrects scores.\n        self.assertEqual(\n            results[\'proposals\'].shape,\n            (3, 4)\n        )\n\n        self.assertAllClose(\n            results[\'scores\'],\n            [0.9, 0.8, 0.2]\n        )\n\n        config[\'nms_threshold\'] = 0.6\n\n        results = self._run_rpn_proposal(\n            all_anchors, rpn_cls_prob, self.config, gt_boxes=gt_boxes)\n\n        # Check we get exactly 3 \'nms proposals\' because 3 IoU lowers than 0.3.\n        # Also check that we get the corrects scores.\n        self.assertEqual(\n            results[\'proposals\'].shape,\n            (3, 4)\n        )\n\n        self.assertAllClose(\n            results[\'scores\'],\n            [0.9, 0.8, 0.2]\n        )\n\n        config[\'nms_threshold\'] = 0.8\n\n        results = self._run_rpn_proposal(\n            all_anchors, rpn_cls_prob, self.config, gt_boxes=gt_boxes)\n\n        # Check we get exactly 3 \'nms proposals\' because 3 IoU lowers than 0.8.\n        # Also check that we get the corrects scores.\n        self.assertEqual(\n            results[\'proposals\'].shape,\n            (3, 4)\n        )\n\n        self.assertAllClose(\n            results[\'scores\'],\n            [0.9, 0.8, 0.2]\n        )\n\n        config[\'nms_threshold\'] = 1.0\n\n        results = self._run_rpn_proposal(\n            all_anchors, rpn_cls_prob, self.config, gt_boxes=gt_boxes)\n\n        # Check we get \'post_nms_top_n\' nms proposals because\n        # \'nms_threshold\' = 1 and this only removes duplicates.\n        self.assertEqual(\n            results[\'proposals\'].shape,\n            (4, 4)\n        )\n\n    def testOutsidersAndTopN(self):\n        """"""\n        Test outside anchors and topN filters\n        """"""\n        gt_boxes = np.array([\n            [10, 10, 20, 22],\n            [10, 10, 20, 22],\n            [10, 10, 20, 50],  # Outside anchor\n            [10, 10, 20, 22],\n        ])\n        all_anchors = np.array([\n            [11, 13, 34, 31],\n            [10, 10, 20, 22],\n            [11, 13, 34, 40],\n            [7, 13, 34, 30],\n        ])\n        rpn_cls_prob = np.array([\n            [0.3, 0.7],\n            [0.4, 0.6],\n            [0.9, 0.1],\n            [0.8, 0.2]\n        ])\n\n        results = self._run_rpn_proposal(\n            all_anchors, rpn_cls_prob, self.config, gt_boxes=gt_boxes)\n\n        # Check we get exactly 3 \'nms proposals\' and 3 \'unsorted_proposals\'\n        # because we have 4 gt_boxes, but 1 outsider (and nms_threshold = 1).\n        self.assertEqual(\n            results[\'proposals\'].shape,\n            (3, 4)\n        )\n\n        # We don\'t remove proposals outside, we just clip them.\n        self.assertEqual(\n            results[\'unsorted_proposals\'].shape,\n            (4, 4)\n        )\n\n        # Also check that we get the corrects scores.\n        self.assertAllClose(\n            results[\'scores\'],\n            [0.7, 0.6, 0.2]\n        )\n\n        config = self.config\n        config[\'post_nms_top_n\'] = 2\n\n        results = self._run_rpn_proposal(\n            all_anchors, rpn_cls_prob, config, gt_boxes=gt_boxes)\n\n        # Check that with a post_nms_top_n = 2 we have only 2 \'nms proposals\'\n        # but 3 \'unsorted_proposals\'.\n        self.assertAllEqual(\n            results[\'proposals\'].shape,\n            (2, 4)\n        )\n\n        self.assertEqual(\n            results[\'unsorted_proposals\'].shape,\n            (4, 4)\n        )\n\n        # Also check that we get the corrects scores.\n        self.assertAllClose(\n            results[\'scores\'],\n            [0.7, 0.6]\n        )\n\n        # Sorted\n        self.assertAllClose(\n            results[\'sorted_top_scores\'],\n            [0.7, 0.6, 0.2, 0.1]\n        )\n\n        # Check that we only filter by pre_nms_top_n\n        config[\'post_nms_top_n\'] = 3\n        config[\'pre_nms_top_n\'] = 2\n\n        results = self._run_rpn_proposal(\n            all_anchors, rpn_cls_prob, config, gt_boxes=gt_boxes)\n\n        # Check that with a post_nms_top_n = 3 and pre_nms_top = 2\n        # we have only 2 \'nms proposals\' and 2 \'unsorted_proposals\'.\n\n        self.assertAllEqual(\n            results[\'proposals\'].shape,\n            (2, 4)\n        )\n\n        # Filter pre nms\n        self.assertEqual(\n            results[\'sorted_top_proposals\'].shape,\n            (2, 4)\n        )\n\n        # Also check that we get the corrects scores.\n        self.assertAllClose(\n            results[\'scores\'],\n            [0.7, 0.6]\n        )\n\n        self.assertAllClose(\n            results[\'sorted_top_scores\'],\n            [0.7, 0.6]\n        )\n\n        config[\'post_nms_top_n\'] = 1\n        config[\'pre_nms_top_n\'] = 2\n\n        results = self._run_rpn_proposal(\n            all_anchors, rpn_cls_prob, config, gt_boxes=gt_boxes)\n\n        # Check that with a post_nms_top_n = 1 and pre_nms_top = 2\n        # we have only 1 \'nms proposals\' and 2 \'unsorted_proposals\'.\n        self.assertAllEqual(\n            results[\'proposals\'].shape,\n            (1, 4)\n        )\n\n        self.assertEqual(\n            results[\'sorted_top_proposals\'].shape,\n            (2, 4)\n        )\n\n        # Also check that we get the corrects scores.\n        self.assertAllClose(\n            results[\'scores\'],\n            [0.7]\n        )\n\n        self.assertAllClose(\n            results[\'sorted_top_scores\'],\n            [0.7, 0.6]\n        )\n\n    def testNegativeArea(self):\n        """"""\n        Test negative area filters\n        """"""\n        gt_boxes = np.array([\n            [10, 10, 20, 3],  # Negative area\n            [10, 10, 20, 22],\n            [10, 10, 8, 22],  # Negative area\n            [10, 10, 20, 22],\n        ])\n        all_anchors = np.array([\n            [11, 13, 12, 16],\n            [10, 10, 20, 22],\n            [11, 13, 12, 19],\n            [7, 13, 34, 30],\n        ])\n        rpn_cls_prob = np.array([\n            [0.3, 0.7],\n            [0.4, 0.6],\n            [0.9, 0.1],\n            [0.8, 0.2]\n        ])\n\n        results = self._run_rpn_proposal(\n            all_anchors, rpn_cls_prob, self.config, gt_boxes=gt_boxes)\n\n        # Check we get exactly 2 \'proposals\' and 2 \'unsorted_proposals\' because\n        # we have 4 gt_boxes, but 2 with negative area (and nms_threshold = 1).\n        self.assertEqual(\n            results[\'proposals\'].shape,\n            (2, 4)\n        )\n\n        self.assertEqual(\n            results[\'unsorted_proposals\'].shape,\n            (2, 4)\n        )\n\n    def testNegativeAreaProposals(self):\n        all_anchors = np.array([\n            [11, 13, 12, 16],\n            [10, 10, 9, 9],  # invalid anchor will transform to an invalid\n            [11, 13, 12, 28],  # proposal. we are cheating here but it\'s almost\n            [7, 13, 34, 30],  # the same.\n        ])\n        rpn_cls_prob = np.array([\n            [0.3, 0.7],\n            [0.4, 0.6],\n            [0.9, 0.1],\n            [0.8, 0.2]\n        ])\n        rpn_bbox_pred = np.array([\n            [0, 0, 0, 0],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0],\n        ])\n\n        results = self._run_rpn_proposal(\n            all_anchors, rpn_cls_prob, self.config,\n            rpn_bbox_pred=rpn_bbox_pred\n        )\n\n        self.assertEqual(\n            results[\'unsorted_proposals\'].shape,\n            (3, 4)\n        )\n\n    def testClippingOfProposals(self):\n        """"""\n        Test clipping of proposals before and after NMS\n        """"""\n        # Before NMS\n        gt_boxes = np.array([\n            [0, 0, 10, 12],\n            [10, 10, 20, 22],\n            [10, 10, 20, 22],\n            [30, 25, 39, 39],\n        ])\n        all_anchors = np.array([\n            [-20, -10, 12, 6],\n            [2, -10, 20, 20],\n            [0, 0, 12, 16],\n            [2, -10, 20, 2],\n        ])\n        rpn_cls_prob = np.array([\n            [0.3, 0.7],\n            [0.4, 0.6],\n            [0.3, 0.7],\n            [0.1, 0.9],\n        ])\n\n        rpn_bbox_pred = np.array([  # This is set to zeros so when decode is\n            [0, 0, 0, 0],           # applied in RPNProposal the anchors don\'t\n            [0, 0, 0, 0],           # change, leaving us with unclipped\n            [0, 0, 0, 0],           # proposals.\n            [0, 0, 0, 0],\n        ])\n        config = EasyDict(self.config)\n        config[\'clip_after_nms\'] = False\n        results_before = self._run_rpn_proposal(\n            all_anchors, rpn_cls_prob, config, gt_boxes=gt_boxes,\n            rpn_bbox_pred=rpn_bbox_pred)\n        im_size = tf.placeholder(tf.float32, shape=(2,))\n        proposals_unclipped = tf.placeholder(\n            tf.float32, shape=(results_before[\'proposals_unclipped\'].shape))\n        clip_bboxes_tf = clip_boxes(proposals_unclipped, im_size)\n\n        with self.test_session() as sess:\n            clipped_proposals = sess.run(clip_bboxes_tf, feed_dict={\n                proposals_unclipped: results_before[\'proposals_unclipped\'],\n                im_size: self.im_size\n            })\n\n        # Check we clip proposals right after filtering the invalid area ones.\n        self.assertAllEqual(\n            results_before[\'unsorted_proposals\'],\n            clipped_proposals\n        )\n\n        # Checks all NMS proposals have values inside the image boundaries\n        proposals = results_before[\'proposals\']\n        self.assertTrue((proposals >= 0).all())\n        self.assertTrue(\n            (proposals < np.array(self.im_size + self.im_size)).all()\n        )\n\n        # After NMS\n        config[\'clip_after_nms\'] = True\n        results_after = self._run_rpn_proposal(\n            all_anchors, rpn_cls_prob, config, gt_boxes=gt_boxes,\n            rpn_bbox_pred=rpn_bbox_pred)\n        im_size = tf.placeholder(tf.float32, shape=(2,))\n        proposals_unclipped = tf.placeholder(\n            tf.float32, shape=(results_after[\'proposals_unclipped\'].shape))\n        clip_bboxes_tf = clip_boxes(proposals_unclipped, im_size)\n\n        with self.test_session() as sess:\n            clipped_proposals = sess.run(clip_bboxes_tf, feed_dict={\n                proposals_unclipped: results_after[\'proposals_unclipped\'],\n                im_size: self.im_size\n            })\n\n        # Check we don\'t clip proposals in the beginning of the function.\n        self.assertAllEqual(\n            results_after[\'unsorted_proposals\'],\n            results_after[\'proposals_unclipped\']\n        )\n\n        proposals = results_after[\'proposals\']\n        # Checks all NMS proposals have values inside the image boundaries\n        self.assertTrue((proposals >= 0).all())\n        self.assertTrue(\n            (proposals < np.array(self.im_size + self.im_size)).all()\n        )\n\n    def testFilterOutsideAnchors(self):\n        """"""\n        Test clipping of proposals before and after NMS\n        """"""\n        gt_boxes = np.array([\n            [0, 0, 10, 12],\n            [10, 10, 20, 22],\n            [10, 10, 20, 22],\n            [30, 25, 39, 39],\n            [30, 25, 39, 39],\n        ])\n        all_anchors = np.array([    # Img_size (40, 40)\n            [-20, -10, 12, 6],      # Should be filtered\n            [2, 10, 20, 20],        # Valid anchor\n            [0, 0, 50, 16],         # Should be filtered\n            [2, -10, 20, 50],       # Should be filtered\n            [25, 30, 27, 33],       # Valid anchor\n        ])\n        rpn_cls_prob = np.array([\n            [0.3, 0.7],\n            [0.4, 0.6],\n            [0.3, 0.7],\n            [0.1, 0.9],\n            [0.2, 0.8],\n        ])\n        config = EasyDict(self.config)\n        config[\'filter_outside_anchors\'] = False\n        results_without_filter = self._run_rpn_proposal(\n            all_anchors, rpn_cls_prob, config, gt_boxes=gt_boxes)\n\n        # Check that all_proposals contain the outside anchors\n        self.assertAllEqual(\n            results_without_filter[\'all_proposals\'].shape,\n            all_anchors.shape)\n\n        config[\'filter_outside_anchors\'] = True\n        results_with_filter = self._run_rpn_proposal(\n            all_anchors, rpn_cls_prob, config, gt_boxes=gt_boxes)\n        self.assertAllEqual(\n            results_with_filter[\'all_proposals\'].shape,\n            (2, 4))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
luminoth/models/fasterrcnn/rpn_target.py,87,"b'import sonnet as snt\nimport tensorflow as tf\n\nfrom luminoth.utils.bbox_overlap import bbox_overlap_tf\nfrom luminoth.utils.bbox_transform_tf import encode as encode_tf\n\n\nclass RPNTarget(snt.AbstractModule):\n    """"""RPNTarget: Get RPN\'s classification and regression targets.\n\n    RPNTarget is responsible for:\n      * calculating the correct values for both classification and regression\n        problems.\n      * defining which anchors and target values are going to be used for the\n        RPN minibatch.\n\n    For calculating the correct values for classification (ie. the question of\n    ""does this anchor refer to an object?"") and returning an objectness score,\n    we calculate the intersection over union (IoU) between the anchors boxes\n    and the ground truth boxes, and use this to categorize anchors. When the\n    intersection between anchors and groundtruth is above a threshold, we can\n    mark the anchor as an object or as being foreground. In case of not having\n    any intersection or having a low IoU value, then we say that the anchor\n    refers to background.\n\n    For calculating the correct values for the regression, the problem of\n    transforming the fixed size anchor into a more suitable bounding box (equal\n    to the ground truth box) only applies to the anchors that we consider to be\n    foreground.\n\n    RPNTarget is also responsible for selecting which of the anchors are going\n    to be used for the minibatch. This is a random process with some\n    restrictions on the ratio between foreground and background samples.\n\n    For selecting the minibatch, labels are not only set to 0 or 1 (for the\n    cases of being background and foreground respectively), but also to -1 for\n    the anchors we just want to ignore and not include in the minibatch.\n\n    In summary:\n      * 1 is positive\n          when GT overlap is >= 0.7 (configurable) or for GT max overlap (one\n          anchor)\n      * 0 is negative\n          when GT overlap is < 0.3 (configurable)\n      * -1 is don\'t care\n          useful for subsampling negative labels\n\n    Returns:\n        labels: label for each anchor\n        bbox_targets: bbox regresion values for each anchor\n    """"""\n    def __init__(self, num_anchors, config, seed=None, name=\'anchor_target\'):\n        super(RPNTarget, self).__init__(name=name)\n        self._num_anchors = num_anchors\n\n        self._allowed_border = config.allowed_border\n        # We set clobber positive to False to make sure that there is always at\n        # least one positive anchor per GT box.\n        self._clobber_positives = config.clobber_positives\n        # We set anchors as positive when the IoU is greater than\n        # `positive_overlap`.\n        self._positive_overlap = config.foreground_threshold\n        # We set anchors as negative when the IoU is less than\n        # `negative_overlap`.\n        self._negative_overlap = config.background_threshold_high\n        # Fraction of the batch to be foreground labeled anchors.\n        self._foreground_fraction = config.foreground_fraction\n        self._minibatch_size = config.minibatch_size\n\n        # When choosing random targets use `seed` to replicate behaviour.\n        self._seed = seed\n\n    def _build(self, all_anchors, gt_boxes, im_shape):\n        """"""\n        We compare anchors to GT and using the minibatch size and the different\n        config settings (clobber, foreground fraction, etc), we end up with\n        training targets *only* for the elements we want to use in the batch,\n        while everything else is ignored.\n\n        Basically what it does is, first generate the targets for all (valid)\n        anchors, and then start subsampling the positive (foreground) and the\n        negative ones (background) based on the number of samples of each type\n        that we want.\n\n        Args:\n            all_anchors:\n                A Tensor with all the bounding boxes coords of the anchors.\n                Its shape should be (num_anchors, 4).\n            gt_boxes:\n                A Tensor with the ground truth bounding boxes of the image of\n                the batch being processed. Its shape should be (num_gt, 5).\n                The last dimension is used for the label.\n            im_shape:\n                Shape of original image (height, width) in order to define\n                anchor targers in respect with gt_boxes.\n\n        Returns:\n            Tuple of the tensors of:\n                labels: (1, 0, -1) for each anchor.\n                    Shape (num_anchors, 1)\n                bbox_targets: 4d bbox targets as specified by paper.\n                    Shape (num_anchors, 4)\n                max_overlaps: Max IoU overlap with ground truth boxes.\n                    Shape (num_anchors, 1)\n        """"""\n        # Keep only the coordinates of gt_boxes\n        gt_boxes = gt_boxes[:, :4]\n        all_anchors = all_anchors[:, :4]\n\n        # Only keep anchors inside the image\n        (x_min_anchor, y_min_anchor,\n         x_max_anchor, y_max_anchor) = tf.unstack(all_anchors, axis=1)\n\n        anchor_filter = tf.logical_and(\n            tf.logical_and(\n                tf.greater_equal(x_min_anchor, -self._allowed_border),\n                tf.greater_equal(y_min_anchor, -self._allowed_border)\n            ),\n            tf.logical_and(\n                tf.less(x_max_anchor, im_shape[1] + self._allowed_border),\n                tf.less(y_max_anchor, im_shape[0] + self._allowed_border)\n            )\n        )\n\n        # We (force) reshape the filter so that we can use it as a boolean mask\n        anchor_filter = tf.reshape(anchor_filter, [-1])\n        # Filter anchors.\n        anchors = tf.boolean_mask(\n            all_anchors, anchor_filter, name=\'filter_anchors\')\n\n        # Generate array with the labels for all_anchors.\n        labels = tf.fill((tf.gather(tf.shape(all_anchors), [0])), -1)\n        labels = tf.boolean_mask(labels, anchor_filter, name=\'filter_labels\')\n\n        # Intersection over union (IoU) overlap between the anchors and the\n        # ground truth boxes.\n        overlaps = bbox_overlap_tf(tf.to_float(anchors), tf.to_float(gt_boxes))\n\n        # Generate array with the IoU value of the closest GT box for each\n        # anchor.\n        max_overlaps = tf.reduce_max(overlaps, axis=1)\n        if not self._clobber_positives:\n            # Assign bg labels first so that positive labels can clobber them.\n            # First we get an array with True where IoU is less than\n            # self._negative_overlap\n            negative_overlap_nonzero = tf.less(\n                max_overlaps, self._negative_overlap)\n\n            # Finally we set 0 at True indices\n            labels = tf.where(\n                condition=negative_overlap_nonzero,\n                x=tf.zeros(tf.shape(labels)), y=tf.to_float(labels)\n            )\n        # Get the value of the max IoU for the closest anchor for each gt.\n        gt_max_overlaps = tf.reduce_max(overlaps, axis=0)\n\n        # Find all the indices that match (at least one, but could be more).\n        gt_argmax_overlaps = tf.squeeze(tf.equal(overlaps, gt_max_overlaps))\n        gt_argmax_overlaps = tf.where(gt_argmax_overlaps)[:, 0]\n        # Eliminate duplicates indices.\n        gt_argmax_overlaps, _ = tf.unique(gt_argmax_overlaps)\n        # Order the indices for sparse_to_dense compatibility\n        gt_argmax_overlaps, _ = tf.nn.top_k(\n            gt_argmax_overlaps, k=tf.shape(gt_argmax_overlaps)[-1])\n        gt_argmax_overlaps = tf.reverse(gt_argmax_overlaps, [0])\n\n        # Foreground label: for each ground-truth, anchor with highest overlap.\n        # When the argmax is many items we use all of them (for consistency).\n        # We set 1 at gt_argmax_overlaps_cond indices\n        gt_argmax_overlaps_cond = tf.sparse_to_dense(\n            gt_argmax_overlaps, tf.shape(labels, out_type=tf.int64),\n            True, default_value=False\n        )\n\n        labels = tf.where(\n            condition=gt_argmax_overlaps_cond,\n            x=tf.ones(tf.shape(labels)), y=tf.to_float(labels)\n        )\n\n        # Foreground label: above threshold Intersection over Union (IoU)\n        # First we get an array with True where IoU is greater or equal than\n        # self._positive_overlap\n        positive_overlap_inds = tf.greater_equal(\n            max_overlaps, self._positive_overlap)\n        # Finally we set 1 at True indices\n        labels = tf.where(\n            condition=positive_overlap_inds,\n            x=tf.ones(tf.shape(labels)), y=labels\n        )\n\n        if self._clobber_positives:\n            # Assign background labels last so that negative labels can clobber\n            # positives. First we get an array with True where IoU is less than\n            # self._negative_overlap\n            negative_overlap_nonzero = tf.less(\n                max_overlaps, self._negative_overlap)\n            # Finally we set 0 at True indices\n            labels = tf.where(\n                condition=negative_overlap_nonzero,\n                x=tf.zeros(tf.shape(labels)), y=labels\n            )\n\n        # Subsample positive labels if we have too many\n        def subsample_positive():\n            # Shuffle the foreground indices\n            disable_fg_inds = tf.random_shuffle(fg_inds, seed=self._seed)\n            # Select the indices that we have to ignore, this is\n            # `tf.shape(fg_inds)[0] - num_fg` because we want to get only\n            # `num_fg` foreground labels.\n            disable_place = (tf.shape(fg_inds)[0] - num_fg)\n            disable_fg_inds = disable_fg_inds[:disable_place]\n            # Order the indices for sparse_to_dense compatibility\n            disable_fg_inds, _ = tf.nn.top_k(\n                disable_fg_inds, k=tf.shape(disable_fg_inds)[-1])\n            disable_fg_inds = tf.reverse(disable_fg_inds, [0])\n            disable_fg_inds = tf.sparse_to_dense(\n                disable_fg_inds, tf.shape(labels, out_type=tf.int64),\n                True, default_value=False\n            )\n            # Put -1 to ignore the anchors in the selected indices\n            return tf.where(\n                condition=tf.squeeze(disable_fg_inds),\n                x=tf.to_float(tf.fill(tf.shape(labels), -1)), y=labels\n            )\n\n        num_fg = tf.to_int32(self._foreground_fraction * self._minibatch_size)\n        # Get foreground indices, get True in the indices where we have a one.\n        fg_inds = tf.equal(labels, 1)\n        # We get only the indices where we have True.\n        fg_inds = tf.squeeze(tf.where(fg_inds), axis=1)\n        fg_inds_size = tf.size(fg_inds)\n        # Condition for check if we have too many positive labels.\n        subsample_positive_cond = fg_inds_size > num_fg\n        # Check the condition and subsample positive labels.\n        labels = tf.cond(\n            subsample_positive_cond,\n            true_fn=subsample_positive, false_fn=lambda: labels\n        )\n\n        # Subsample negative labels if we have too many\n        def subsample_negative():\n            # Shuffle the background indices\n            disable_bg_inds = tf.random_shuffle(bg_inds, seed=self._seed)\n\n            # Select the indices that we have to ignore, this is\n            # `tf.shape(bg_inds)[0] - num_bg` because we want to get only\n            # `num_bg` background labels.\n            disable_place = (tf.shape(bg_inds)[0] - num_bg)\n            disable_bg_inds = disable_bg_inds[:disable_place]\n            # Order the indices for sparse_to_dense compatibility\n            disable_bg_inds, _ = tf.nn.top_k(\n                disable_bg_inds, k=tf.shape(disable_bg_inds)[-1])\n            disable_bg_inds = tf.reverse(disable_bg_inds, [0])\n            disable_bg_inds = tf.sparse_to_dense(\n                disable_bg_inds, tf.shape(labels, out_type=tf.int64),\n                True, default_value=False\n            )\n            # Put -1 to ignore the anchors in the selected indices\n            return tf.where(\n                condition=tf.squeeze(disable_bg_inds),\n                x=tf.to_float(tf.fill(tf.shape(labels), -1)), y=labels\n            )\n\n        # Recalculate the foreground indices after (maybe) disable some of them\n\n        # Get foreground indices, get True in the indices where we have a one.\n        fg_inds = tf.equal(labels, 1)\n        # We get only the indices where we have True.\n        fg_inds = tf.squeeze(tf.where(fg_inds), axis=1)\n        fg_inds_size = tf.size(fg_inds)\n\n        num_bg = tf.to_int32(self._minibatch_size - fg_inds_size)\n        # Get background indices, get True in the indices where we have a zero.\n        bg_inds = tf.equal(labels, 0)\n        # We get only the indices where we have True.\n        bg_inds = tf.squeeze(tf.where(bg_inds), axis=1)\n        bg_inds_size = tf.size(bg_inds)\n        # Condition for check if we have too many positive labels.\n        subsample_negative_cond = bg_inds_size > num_bg\n        # Check the condition and subsample positive labels.\n        labels = tf.cond(\n            subsample_negative_cond,\n            true_fn=subsample_negative, false_fn=lambda: labels\n        )\n\n        # Return bbox targets with shape (anchors.shape[0], 4).\n\n        # Find the closest gt box for each anchor.\n        argmax_overlaps = tf.argmax(overlaps, axis=1)\n        # Eliminate duplicates.\n        argmax_overlaps_unique, _ = tf.unique(argmax_overlaps)\n        # Filter the gt_boxes.\n        # We get only the indices where we have ""inside anchors"".\n        anchor_filter_inds = tf.where(anchor_filter)\n        gt_boxes = tf.gather(gt_boxes, argmax_overlaps)\n\n        bbox_targets = encode_tf(anchors, gt_boxes)\n\n        # For the anchors that arent foreground, we ignore the bbox_targets.\n        anchor_foreground_filter = tf.equal(labels, 1)\n        bbox_targets = tf.where(\n            condition=anchor_foreground_filter,\n            x=bbox_targets, y=tf.zeros_like(bbox_targets)\n        )\n\n        # We unroll ""inside anchors"" value for all anchors (for shape\n        # compatibility).\n\n        # We complete the missed indices with zeros\n        # (because scatter_nd has zeros as default).\n        bbox_targets = tf.scatter_nd(\n            indices=tf.to_int32(anchor_filter_inds),\n            updates=bbox_targets,\n            shape=tf.shape(all_anchors)\n        )\n\n        labels_scatter = tf.scatter_nd(\n            indices=tf.to_int32(anchor_filter_inds),\n            updates=labels,\n            shape=[tf.shape(all_anchors)[0]]\n        )\n        # We have to put -1 to ignore the indices with 0 generated by\n        # scatter_nd, otherwise it will be considered as background.\n        labels = tf.where(\n            condition=anchor_filter, x=labels_scatter,\n            y=tf.to_float(tf.fill(tf.shape(labels_scatter), -1))\n        )\n\n        max_overlaps = tf.scatter_nd(\n            indices=tf.to_int32(anchor_filter_inds),\n            updates=max_overlaps,\n            shape=[tf.shape(all_anchors)[0]]\n        )\n\n        return labels, bbox_targets, max_overlaps\n'"
luminoth/models/fasterrcnn/rpn_target_test.py,7,"b'import numpy as np\nimport tensorflow as tf\n\nfrom easydict import EasyDict\nfrom luminoth.models.fasterrcnn.rpn_target import RPNTarget\n\n\nclass RPNTargetTest(tf.test.TestCase):\n\n    def setUp(self):\n        super(RPNTargetTest, self).setUp()\n        # Setup\n        self.gt_boxes = np.array([[200, 0, 400, 400]])\n        self.im_size = (600, 600)\n        self.config = EasyDict({\n            \'allowed_border\': 0,\n            \'clobber_positives\': False,\n            \'foreground_threshold\': 0.7,\n            \'background_threshold_high\': 0.3,\n            \'foreground_fraction\': 0.5,\n            \'minibatch_size\': 2\n        })\n        tf.reset_default_graph()\n\n    def _run_rpn_target(self, anchors, gt_boxes, config):\n        gt_boxes_tf = tf.placeholder(tf.float32, shape=gt_boxes.shape)\n        im_size = tf.placeholder(tf.float32, shape=(2,))\n        all_anchors = tf.placeholder(tf.float32, shape=anchors.shape)\n\n        model = RPNTarget(anchors.shape[0], config, seed=0)\n        labels, bbox_targets, max_overlaps = model(\n            all_anchors, gt_boxes_tf, im_size)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            labels_val, bbox_targets_val, max_overlaps_val = sess.run(\n                [labels, bbox_targets, max_overlaps], feed_dict={\n                    gt_boxes_tf: gt_boxes,\n                    im_size: self.im_size,\n                    all_anchors: anchors,\n                })\n            return labels_val, bbox_targets_val, max_overlaps_val\n\n    def testBaseCase(self):\n        """"""\n        Tests a basic case that includes foreground and backgrounds\n        """"""\n        all_anchors = np.array([\n            [200, 100, 400, 400],  # foreground\n            [300, 300, 400, 400],  # background\n            [200, 380, 300, 500],  # background\n        ], dtype=np.float32)\n        labels, bbox_targets, max_overlaps = self._run_rpn_target(\n            all_anchors, self.gt_boxes, self.config\n        )\n\n        # Check we get exactly a foreground, an ignored background (because of\n        # minibatch_size = 2) and a background. Also checks they are in the\n        # correct order\n        self.assertAllEqual(\n            labels,\n            np.array([1, 0, -1])\n        )\n\n        # Assert bbox targets we are ignoring are zero.\n        self.assertAllEqual(\n            bbox_targets[1:],\n            [[0, 0, 0, 0], [0, 0, 0, 0]]\n        )\n\n        # Assert bbox target is not zero\n        self.assertTrue((bbox_targets[0] != 0).any())\n\n        # Check max_overlaps shape\n        self.assertEqual(\n            max_overlaps.shape,\n            (3,)\n        )\n\n        # Check that the foreground has overlap > 0.7.\n        # Only the first value is checked because it\'s the only anchor assigned\n        # to foreground.\n        self.assertGreaterEqual(\n            max_overlaps[0],\n            0.7\n        )\n\n        # Check that the backgrounds have overlaps < 0.3.\n        self.assertEqual(\n            np.less_equal(max_overlaps[1:], 0.3).all(),\n            True\n        )\n\n    def testBorderOutsiders(self):\n        """"""\n        Test with anchors that fall partially outside the image.\n        """"""\n        all_anchors = np.array([\n            [200, 100, 400, 400],  # foreground\n            [300, 300, 400, 400],  # background\n            [200, 380, 300, 500],  # background\n            [500, 500, 600, 650],  # border outsider\n            [200, 100, 400, 400],  # foreground\n        ])\n\n        config = self.config\n        config[\'minibatch_size\'] = 5\n        labels, bbox_targets, max_overlaps = self._run_rpn_target(\n            all_anchors, self.gt_boxes, config\n        )\n\n        # Checks that the order of labels for anchors is correct.\n        # The fourth anchor is always ignored because it has a point outside\n        # the image.\n        self.assertAllEqual(\n            labels,\n            np.array([1, 0, 0, -1, 1])\n        )\n\n        # Check that foreground bbox targets are partially zero because\n        # the X1 and width are already the same\n        self.assertEqual(bbox_targets[0][0], 0)\n        self.assertEqual(bbox_targets[0][2], 0)\n        self.assertNotEqual(bbox_targets[0][1], 0)\n        self.assertNotEqual(bbox_targets[0][3], 0)\n        self.assertAllEqual(bbox_targets[0], bbox_targets[-1])\n\n        # Assert bbox targets we are ignoring are zero.\n        self.assertAllEqual(\n            bbox_targets[1:4],\n            np.zeros((3, 4))\n        )\n\n        # Test with a different foreground_fraction value.\n        config[\'foreground_fraction\'] = 0.2\n        labels, bbox_targets, max_overlaps = self._run_rpn_target(\n            all_anchors, self.gt_boxes, config\n        )\n\n        # Checks that the order of labels for anchors is correct.\n        # The fourth anchor is always ignored because it has a point outside\n        # the image.\n        self.assertAllEqual(\n            labels,\n            np.array([1, 0, 0, -1, -1])\n        )\n\n        # Assert bbox targets we are ignoring are zero.\n        self.assertAllEqual(\n            bbox_targets[1:],\n            np.zeros((4, 4))\n        )\n\n    def testWithNoClearMatch(self):\n        """"""\n        Tests that despite a foreground doesn\'t exist, an anchor is always\n        assigned.\n        Also tests the positive clobbering behaviour setting.\n        """"""\n        all_anchors = np.array([\n            [300, 300, 400, 400],  # background\n            [200, 380, 300, 500],  # background\n        ])\n\n        labels, bbox_targets, max_overlaps = self._run_rpn_target(\n            all_anchors, self.gt_boxes, self.config\n        )\n\n        # Check we get only one foreground anchor.\n        self.assertAllEqual(\n            labels,\n            np.array([1, 0])\n        )\n        self.assertTrue((bbox_targets[0] != 0).all())\n        self.assertAllEqual(bbox_targets[1], np.zeros(4))\n\n        config = self.config\n        config[\'clobber_positives\'] = True\n\n        labels, bbox_targets, max_overlaps = self._run_rpn_target(\n            all_anchors, self.gt_boxes, config\n        )\n\n        # Check we don\'t get a foreground anchor because of possitive\n        # clobbering enabled.\n        self.assertAllEqual(\n            labels,\n            np.array([0, 0])\n        )\n        self.assertAllEqual(bbox_targets, np.zeros((2, 4)))\n\n    def testWithMultipleGTBoxes(self):\n        """"""\n        Tests a basic case that includes two gt_boxes. What is going to happen\n        is that for each gt_box its fixed at least a foreground. After if there\n        are too many foreground, some of them will be disabled.\n        """"""\n        all_anchors = np.array([\n            [300, 300, 400, 390],  # background IoU < 0.3\n            [300, 300, 400, 400],  # foreground for the first gt_box\n            [100, 310, 120, 380],  # foreground for the second gt_box\n        ], dtype=np.float32)\n        config = self.config\n        config[\'minibatch_size\'] = 3\n\n        gt_boxes = np.array([[200, 0, 400, 400], [100, 300, 120, 375]])\n        labels, bbox_targets, max_overlaps = self._run_rpn_target(\n            all_anchors, gt_boxes, config\n        )\n\n        # Check we get exactly a foreground, in this case the first one,\n        # a background and an ignored foreground.\n        # Also checks they are in the correct order.\n        self.assertAllEqual(\n            labels,\n            np.array([0, 1, -1])\n        )\n\n        # Assert bbox targets we are ignoring are zero.\n        self.assertAllEqual(\n            bbox_targets[0],\n            [0, 0, 0, 0]\n        )\n\n        self.assertAllEqual(\n             bbox_targets[2],\n             [0, 0, 0, 0]\n         )\n\n        # Assert bbox target is not zero\n        self.assertTrue((bbox_targets[1] != 0).any())\n\n        # Check max_overlaps shape\n        self.assertEqual(\n            max_overlaps.shape,\n            (3,)\n        )\n\n    def testWithManyGTBoxes(self):\n        all_anchors = np.array([\n            # Foregrounds\n            [0, 0, 10, 10],\n            [0, 0, 10, 10],\n            [10, 10, 20, 20],\n            [10, 10, 20, 20],\n            [20, 20, 30, 30],\n            [20, 20, 30, 30],\n            [30, 30, 40, 40],\n            [30, 30, 40, 40],\n            # Backgrounds\n            [100, 100, 110, 110],\n            [100, 100, 120, 120],\n            [110, 110, 120, 120],\n            [110, 110, 130, 130],\n            [110, 110, 120, 120],\n            [110, 110, 130, 130],\n            [110, 110, 120, 120],\n            [110, 110, 130, 130],\n        ], dtype=np.float32)\n        config = self.config\n        config[\'minibatch_size\'] = 8  # 4 foregrounds and 4 backgrounds\n\n        gt_boxes = np.array([\n            [2, 2, 8, 8], [12, 12, 18, 18], [22, 22, 28, 28], [32, 32, 38, 38]\n        ])\n        labels, bbox_targets, max_overlaps = self._run_rpn_target(\n            all_anchors, gt_boxes, config\n        )\n\n        # 4 foreground\n        self.assertEqual(labels[labels == 1].shape[0], 4)\n        # 4 background\n        self.assertEqual(labels[labels == 0].shape[0], 4)\n\n        # Check all 4 foregrounds are in the first 8 places of the label.\n        # TODO: Ideally it should be one for each GT.\n        self.assertTrue((labels.argsort()[-4:] < 8).all())\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
luminoth/models/fasterrcnn/rpn_test.py,24,"b'import numpy as np\nimport tensorflow as tf\n\nfrom easydict import EasyDict\nfrom luminoth.models.fasterrcnn.rpn import RPN\nfrom luminoth.utils.anchors import generate_anchors_reference\nfrom luminoth.utils.test import generate_gt_boxes, generate_anchors\n\n\nclass RPNTest(tf.test.TestCase):\n\n    def setUp(self):\n        super(RPNTest, self).setUp()\n        self.num_anchors = 9\n        # Use default settings.\n        self.config = EasyDict({\n            \'num_channels\': 512,\n            \'kernel_shape\': [3, 3],\n            \'rpn_initializer\': {\n                \'type\': \'variance_scaling_initializer\',\n                \'factor\': 1.0,\n                \'mode\': \'FAN_AVG\',\n                \'uniform\': True,\n            },\n            \'cls_initializer\': {\n                \'type\': \'truncated_normal_initializer\',\n                \'mean\': 0.0,\n                \'stddev\': 0.01,\n            },\n            \'bbox_initializer\': {\n                \'type\': \'truncated_normal_initializer\',\n                \'mean\': 0.0,\n                \'stddev\': 0.01,\n            },\n            \'l2_regularization_scale\': 0.0005,\n            \'l1_sigma\': 3.0,\n            \'activation_function\': \'relu6\',\n            \'proposals\': {\n                \'pre_nms_top_n\': 12000,\n                \'post_nms_top_n\': 2000,\n                \'nms_threshold\': 0.6,\n                \'min_size\': 0,\n                \'clip_after_nms\': False,\n                \'filter_outside_anchors\': False,\n                \'apply_nms\': True,\n                \'min_prob_threshold\': 0.0,\n            },\n            \'target\': {\n                \'allowed_border\': 0,\n                \'clobber_positives\': False,\n                \'foreground_threshold\': 0.7,\n                \'background_threshold_high\': 0.3,\n                \'background_threshold_low\': 0.,\n                \'foreground_fraction\': 0.5,\n                \'minibatch_size\': 256,\n            }\n        })\n\n        # Use default anchor configuration values.\n        self.base_size = 256\n        self.scales = np.array([0.5, 1, 2])\n        self.ratios = np.array([0.5, 1, 2])\n        self.stride = 16\n        tf.reset_default_graph()\n\n    def testBasic(self):\n        """"""Tests shapes are consistent with anchor generation.\n        """"""\n        model = RPN(\n            self.num_anchors, self.config, debug=True\n        )\n        # (plus the batch number)\n        pretrained_output_shape = (1, 32, 32, 512)\n        pretrained_output = tf.placeholder(\n            tf.float32, shape=pretrained_output_shape)\n\n        # Estimate image shape from the pretrained output and the anchor stride\n        image_shape_val = (\n            int(pretrained_output_shape[1] * self.stride),\n            int(pretrained_output_shape[2] * self.stride),\n        )\n\n        # Use 4 ground truth boxes.\n        gt_boxes_shape = (4, 4)\n        gt_boxes = tf.placeholder(tf.float32, shape=gt_boxes_shape)\n        image_shape_shape = (2,)\n        image_shape = tf.placeholder(tf.float32, shape=image_shape_shape)\n        # Total anchors depends on the pretrained output shape and the total\n        # number of anchors per point.\n        total_anchors = (\n            pretrained_output_shape[1] * pretrained_output_shape[2] *\n            self.num_anchors\n        )\n        all_anchors_shape = (total_anchors, 4)\n        all_anchors = tf.placeholder(tf.float32, shape=all_anchors_shape)\n        layers = model(\n            pretrained_output, image_shape, all_anchors, gt_boxes=gt_boxes\n        )\n\n        with self.test_session() as sess:\n            # As in the case of a real session we need to initialize the\n            # variables.\n            sess.run(tf.global_variables_initializer())\n            layers_inst = sess.run(layers, feed_dict={\n                # We don\'t really care about the value of the pretrained output\n                # only that has the correct shape.\n                pretrained_output: np.random.rand(\n                    *pretrained_output_shape\n                ),\n                # Generate random but valid ground truth boxes.\n                gt_boxes: generate_gt_boxes(\n                    gt_boxes_shape[0], image_shape_val\n                ),\n                # Generate anchors from a reference and the shape of the\n                # pretrained_output.\n                all_anchors: generate_anchors(\n                    generate_anchors_reference(\n                        self.base_size, self.ratios, self.scales\n                    ),\n                    16,\n                    pretrained_output_shape[1:3]\n                ),\n                image_shape: image_shape_val,\n            })\n\n        # Class score generates 2 values per anchor.\n        rpn_cls_score_shape = layers_inst[\'rpn_cls_score\'].shape\n        rpn_cls_score_true_shape = (total_anchors, 2)\n        self.assertEqual(rpn_cls_score_shape, rpn_cls_score_true_shape)\n\n        # Probs have the same shape as cls scores.\n        rpn_cls_prob_shape = layers_inst[\'rpn_cls_prob\'].shape\n        self.assertEqual(rpn_cls_prob_shape, rpn_cls_score_true_shape)\n\n        # We check softmax with the sum of the output.\n        rpn_cls_prob_sum = layers_inst[\'rpn_cls_prob\'].sum(axis=1)\n        self.assertAllClose(rpn_cls_prob_sum, np.ones(total_anchors))\n\n        # Proposals and scores are related to the output of the NMS with\n        # limits.\n        total_proposals = layers_inst[\'proposals\'].shape[0]\n        total_scores = layers_inst[\'scores\'].shape[0]\n\n        # Check we don\'t get more than top_n proposals.\n        self.assertGreaterEqual(\n            self.config.proposals.post_nms_top_n, total_proposals\n        )\n\n        # Check we get a score for each proposal.\n        self.assertEqual(total_proposals, total_scores)\n\n        # Check that we get a regression for each anchor.\n        self.assertEqual(\n            layers_inst[\'rpn_bbox_pred\'].shape,\n            (total_anchors, 4)\n        )\n\n        # Check that we get a target for each regression for each anchor.\n        self.assertEqual(\n            layers_inst[\'rpn_bbox_target\'].shape,\n            (total_anchors, 4)\n        )\n\n        # Check that we get a target class for each anchor.\n        self.assertEqual(\n            layers_inst[\'rpn_cls_target\'].shape,\n            (total_anchors,)\n        )\n\n        # Check that targets are composed of [-1, 0, 1] only.\n        rpn_cls_target = layers_inst[\'rpn_cls_target\']\n        self.assertEqual(\n            tuple(np.sort(np.unique(rpn_cls_target))),\n            (-1, 0., 1.)\n        )\n\n        batch_cls_target = rpn_cls_target[\n            (rpn_cls_target == 0.) | (rpn_cls_target == 1.)\n        ]\n\n        # Check that the non negative target class are exactly the size\n        # as the minibatch\n        self.assertEqual(\n            batch_cls_target.shape,\n            (self.config.target.minibatch_size, )\n        )\n\n        # Check that we get upto foreground_fraction of positive anchors.\n        self.assertLessEqual(\n            batch_cls_target[batch_cls_target == 1.].shape[0] /\n            batch_cls_target.shape[0],\n            self.config.target.foreground_fraction\n        )\n\n    def testTypes(self):\n        """"""Tests that return types are the expected ones.\n        """"""\n        # We repeat testBasic\'s setup.\n        model = RPN(\n            self.num_anchors, self.config, debug=True\n        )\n        pretrained_output_shape = (1, 32, 32, 512)\n        pretrained_output = tf.placeholder(\n            tf.float32, shape=pretrained_output_shape)\n\n        image_shape_val = (\n            int(pretrained_output_shape[1] * self.stride),\n            int(pretrained_output_shape[2] * self.stride),\n        )\n\n        gt_boxes_shape = (4, 4)\n        gt_boxes = tf.placeholder(tf.float32, shape=gt_boxes_shape)\n        image_shape_shape = (2,)\n        image_shape = tf.placeholder(tf.float32, shape=image_shape_shape)\n\n        total_anchors = (\n            pretrained_output_shape[1] * pretrained_output_shape[2] *\n            self.num_anchors\n        )\n        all_anchors_shape = (total_anchors, 4)\n        all_anchors = tf.placeholder(tf.float32, shape=all_anchors_shape)\n        layers = model(\n            pretrained_output, image_shape, all_anchors, gt_boxes=gt_boxes\n        )\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            layers_inst = sess.run(layers, feed_dict={\n                pretrained_output: np.random.rand(\n                    *pretrained_output_shape\n                ),\n                gt_boxes: generate_gt_boxes(\n                    gt_boxes_shape[0], image_shape_val\n                ),\n                all_anchors: generate_anchors(\n                    generate_anchors_reference(\n                        self.base_size, self.ratios, self.scales\n                    ),\n                    16,\n                    pretrained_output_shape[1:3]\n                ),\n                image_shape: image_shape_val,\n            })\n\n        # Assertions\n        proposals = layers_inst[\'proposals\']\n        scores = layers_inst[\'scores\']\n        rpn_cls_prob = layers_inst[\'rpn_cls_prob\']\n        rpn_cls_score = layers_inst[\'rpn_cls_score\']\n        rpn_bbox_pred = layers_inst[\'rpn_bbox_pred\']\n        rpn_cls_target = layers_inst[\'rpn_cls_target\']\n        rpn_bbox_target = layers_inst[\'rpn_bbox_target\']\n        # Everything should have dtype=tf.float32\n        self.assertAllEqual(\n            # We have 7 values we want to compare to tf.float32.\n            [tf.float32] * 7,\n            [\n                proposals.dtype, scores.dtype, rpn_cls_prob.dtype,\n                rpn_cls_score.dtype, rpn_bbox_pred.dtype,\n                rpn_cls_target.dtype, rpn_bbox_target.dtype,\n            ]\n\n        )\n\n    def testLoss(self):\n        """"""Tests that loss returns reasonable values in simple cases.\n        """"""\n        model = RPN(\n            self.num_anchors, self.config, debug=True\n        )\n\n        # Define placeholders that are used inside the loss method.\n        rpn_cls_prob = tf.placeholder(tf.float32)\n        rpn_cls_target = tf.placeholder(tf.float32)\n        rpn_cls_score = tf.placeholder(tf.float32)\n        rpn_bbox_target = tf.placeholder(tf.float32)\n        rpn_bbox_pred = tf.placeholder(tf.float32)\n\n        loss = model.loss({\n            \'rpn_cls_prob\': rpn_cls_prob,\n            \'rpn_cls_target\': rpn_cls_target,\n            \'rpn_cls_score\': rpn_cls_score,\n            \'rpn_bbox_target\': rpn_bbox_target,\n            \'rpn_bbox_pred\': rpn_bbox_pred,\n        })\n\n        # Test perfect score.\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            loss_dict = sess.run(loss, feed_dict={\n                # Probability is (background_prob, foreground_prob)\n                rpn_cls_prob: [[0, 1], [1., 0]],\n                # Target: 1 being foreground, 0 being background.\n                rpn_cls_target: [1, 0],\n                # Class scores before applying softmax. Since using cross\n                # entropy, we need a big difference between values.\n                rpn_cls_score: [[-100., 100.], [100., -100.]],\n                # Targets and predictions are exactly equal.\n                rpn_bbox_target: [[0.1, 0.1, 0.1, 0.1], [0.1, 0.1, 0.1, 0.1]],\n                rpn_bbox_pred: [[0.1, 0.1, 0.1, 0.1], [0.1, 0.1, 0.1, 0.1]],\n            })\n\n            # Assert close since cross-entropy could return very small value.\n            self.assertAllClose(tuple(loss_dict.values()), (0, 0))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
luminoth/models/ssd/__init__.py,0,b'from .ssd import SSD  # noqa\n'
luminoth/models/ssd/feature_extractor.py,23,"b'import sonnet as snt\nimport tensorflow as tf\n\nfrom sonnet.python.modules.conv import Conv2D\nfrom tensorflow.contrib.layers.python.layers import utils\n\nfrom luminoth.models.base import BaseNetwork\n\n\nVALID_SSD_ARCHITECTURES = set([\n    \'truncated_vgg_16\',\n])\n\n\nclass SSDFeatureExtractor(BaseNetwork):\n\n    def __init__(self, config, parent_name=None, name=\'ssd_feature_extractor\',\n                 **kwargs):\n        super(SSDFeatureExtractor, self).__init__(config, name=name, **kwargs)\n        if self._architecture not in VALID_SSD_ARCHITECTURES:\n            raise ValueError(\'Invalid architecture ""{}""\'.format(\n                self._architecture\n            ))\n        self.parent_name = parent_name\n        self.activation_fn = tf.nn.relu\n\n    def _init_vgg16_extra_layers(self):\n        self.conv6 = Conv2D(1024, [3, 3], rate=6, name=\'conv6\')\n        self.conv7 = Conv2D(1024, [1, 1], name=\'conv7\')\n        self.conv8_1 = Conv2D(256, [1, 1], name=\'conv8_1\')\n        self.conv8_2 = Conv2D(512, [3, 3], stride=2, name=\'conv8_2\')\n        self.conv9_1 = Conv2D(128, [1, 1], name=\'conv9_1\')\n        self.conv9_2 = Conv2D(256, [3, 3], stride=2, name=\'conv9_2\')\n        self.conv10_1 = Conv2D(128, [1, 1], name=\'conv10_1\')\n        self.conv10_2 = Conv2D(256, [3, 3], padding=\'VALID\', name=\'conv10_2\')\n        self.conv11_1 = Conv2D(128, [1, 1], name=\'conv11_1\')\n        self.conv11_2 = Conv2D(256, [3, 3], padding=\'VALID\', name=\'conv11_2\')\n\n    def _build(self, inputs, is_training=True):\n        """"""\n        Args:\n            inputs: A Tensor of shape `(batch_size, height, width, channels)`.\n\n        Returns:\n            A dict of feature maps to be consumed by an SSD network\n        """"""\n        # TODO: Is there a better way to manage scoping in these cases?\n        scope = self.module_name\n        if self.parent_name:\n            scope = self.parent_name + \'/\' + scope\n\n        base_net_endpoints = super(SSDFeatureExtractor, self)._build(\n            inputs, is_training=is_training)[\'end_points\']\n\n        if self.truncated_vgg_16_type:\n            # As it is pointed out in SSD and ParseNet papers, `conv4_3` has a\n            # different features scale compared to other layers, to adjust it\n            # we need to add a spatial normalization before adding the\n            # predictors.\n            vgg_conv4_3 = base_net_endpoints[scope + \'/vgg_16/conv4/conv4_3\']\n            tf.summary.histogram(\'conv4_3_hist\', vgg_conv4_3)\n            with tf.variable_scope(\'conv_4_3_norm\'):\n                # Normalize through channels dimension (dim=3)\n                vgg_conv4_3_norm = tf.nn.l2_normalize(\n                    vgg_conv4_3, 3, epsilon=1e-12\n                )\n                # Scale.\n                scale_initializer = tf.ones(\n                    [1, 1, 1, vgg_conv4_3.shape[3]]\n                ) * 20.0  # They initialize to 20.0 in paper\n                scale = tf.get_variable(\n                    \'gamma\',\n                    dtype=vgg_conv4_3.dtype.base_dtype,\n                    initializer=scale_initializer\n                )\n                vgg_conv4_3_norm = tf.multiply(vgg_conv4_3_norm, scale)\n                tf.summary.histogram(\'conv4_3_normalized_hist\', vgg_conv4_3)\n            tf.add_to_collection(\'FEATURE_MAPS\', vgg_conv4_3_norm)\n\n            # The original SSD paper uses a modified version of the vgg16\n            # network, which we\'ll modify here\n            vgg_network_truncation_endpoint = base_net_endpoints[\n                scope + \'/vgg_16/conv5/conv5_3\']\n            tf.summary.histogram(\n                \'conv5_3_hist\',\n                vgg_network_truncation_endpoint\n            )\n\n            # Extra layers for vgg16 as detailed in paper\n            with tf.variable_scope(\'extra_feature_layers\'):\n                self._init_vgg16_extra_layers()\n                net = tf.nn.max_pool(\n                    vgg_network_truncation_endpoint, [1, 3, 3, 1],\n                    padding=\'SAME\', strides=[1, 1, 1, 1], name=\'pool5\'\n                )\n                net = self.conv6(net)\n                net = self.activation_fn(net)\n                net = self.conv7(net)\n                net = self.activation_fn(net)\n                tf.summary.histogram(\'conv7_hist\', net)\n                tf.add_to_collection(\'FEATURE_MAPS\', net)\n                net = self.conv8_1(net)\n                net = self.activation_fn(net)\n                net = self.conv8_2(net)\n                net = self.activation_fn(net)\n                tf.summary.histogram(\'conv8_hist\', net)\n                tf.add_to_collection(\'FEATURE_MAPS\', net)\n                net = self.conv9_1(net)\n                net = self.activation_fn(net)\n                net = self.conv9_2(net)\n                net = self.activation_fn(net)\n                tf.summary.histogram(\'conv9_hist\', net)\n                tf.add_to_collection(\'FEATURE_MAPS\', net)\n                net = self.conv10_1(net)\n                net = self.activation_fn(net)\n                net = self.conv10_2(net)\n                net = self.activation_fn(net)\n                tf.summary.histogram(\'conv10_hist\', net)\n                tf.add_to_collection(\'FEATURE_MAPS\', net)\n                net = self.conv11_1(net)\n                net = self.activation_fn(net)\n                net = self.conv11_2(net)\n                net = self.activation_fn(net)\n                tf.summary.histogram(\'conv11_hist\', net)\n                tf.add_to_collection(\'FEATURE_MAPS\', net)\n\n            # This parameter determines onto which variables we try to load the\n            # pretrained weights\n            self.pretrained_weights_scope = scope + \'/vgg_16\'\n\n        # It\'s actually an ordered dict\n        return utils.convert_collection_to_dict(\'FEATURE_MAPS\')\n\n    def get_trainable_vars(self):\n        """"""\n        Returns a list of the variables that are trainable.\n\n        Returns:\n            trainable_variables: a tuple of `tf.Variable`.\n        """"""\n        return snt.get_variables_in_module(self)\n'"
luminoth/models/ssd/proposal.py,33,"b'import sonnet as snt\nimport tensorflow as tf\n\nfrom luminoth.utils.bbox_transform_tf import decode, clip_boxes, change_order\n\n\nclass SSDProposal(snt.AbstractModule):\n    """"""Transforms anchors and SSD predictions into object proposals.\n\n    Using the fixed anchors and the SSD predictions for both classification\n    and regression (adjusting the bounding box), we return a list of proposals\n    with assigned class.\n\n    In the process it tries to remove duplicated suggestions by applying non\n    maximum suppresion (NMS).\n\n    We apply NMS because the way object detectors are usually scored is by\n    treating duplicated detections (multiple detections that overlap the same\n    ground truth value) as false positive. It is resonable to assume that there\n    may exist such case that applying NMS is completely unnecesary.\n\n    Besides applying NMS it also filters the top N results, both for classes\n    and in general. These values are easily modifiable in the configuration\n    files.\n    """"""\n    def __init__(self, num_classes, config, variances, name=\'proposal_layer\'):\n        super(SSDProposal, self).__init__(name=name)\n        self._num_classes = num_classes\n\n        # Threshold to use for NMS.\n        self._class_nms_threshold = config.class_nms_threshold\n        # Max number of proposals detections per class.\n        self._class_max_detections = config.class_max_detections\n        # Maximum number of detections to return.\n        self._total_max_detections = config.total_max_detections\n        self._min_prob_threshold = config.min_prob_threshold or 0.0\n\n        self._filter_outside_anchors = config.filter_outside_anchors\n        self._variances = variances\n\n    def _build(self, cls_prob, loc_pred, all_anchors, im_shape):\n        """"""\n        Args:\n            cls_prob: A softmax probability for each anchor where the idx = 0\n                is the background class (which we should ignore).\n                Shape (total_anchors, num_classes + 1)\n            loc_pred: A Tensor with the regression output for each anchor.\n                Its shape should be (total_anchors, 4).\n            all_anchors: A Tensor with the anchors bounding boxes of shape\n                (total_anchors, 4), having (x_min, y_min, x_max, y_max) for\n                each anchor.\n            im_shape: A Tensor with the image shape in format (height, width).\n        Returns:\n            prediction_dict with the following keys:\n                raw_proposals: The raw proposals i.e. the anchors adjusted\n                    using loc_pred.\n                proposals: The proposals of the network after appling some\n                    filters like negative area; and NMS. It\'s shape is\n                    (final_num_proposals, 4), where final_num_proposals is\n                    unknown before-hand (it depends on NMS).\n                    The 4-length Tensor for each corresponds to:\n                    (x_min, y_min, x_max, y_max).\n                proposal_label: It\'s shape is (final_num_proposals,)\n                proposal_label_prob: It\'s shape is (final_num_proposals,)\n        """"""\n        selected_boxes = []\n        selected_probs = []\n        selected_labels = []\n        selected_anchors = []  # For debugging\n\n        for class_id in range(self._num_classes):\n            # Get the confidences for this class (+ 1 is to ignore background)\n            class_cls_prob = cls_prob[:, class_id + 1]\n\n            # Filter by min_prob_threshold\n            min_prob_filter = tf.greater_equal(\n                class_cls_prob, self._min_prob_threshold)\n            class_cls_prob = tf.boolean_mask(class_cls_prob, min_prob_filter)\n            class_loc_pred = tf.boolean_mask(loc_pred, min_prob_filter)\n            anchors = tf.boolean_mask(all_anchors, min_prob_filter)\n\n            # Using the loc_pred and the anchors, we generate the proposals.\n            raw_proposals = decode(anchors, class_loc_pred, self._variances)\n            # Clip boxes to image.\n            clipped_proposals = clip_boxes(raw_proposals, im_shape)\n\n            # Filter proposals that have an non-valid area.\n            (x_min, y_min, x_max, y_max) = tf.unstack(\n                clipped_proposals, axis=1)\n            proposal_filter = tf.greater(\n                tf.maximum(x_max - x_min, 0.) * tf.maximum(y_max - y_min, 0.),\n                0.\n            )\n            class_proposals = tf.boolean_mask(\n                clipped_proposals, proposal_filter)\n            class_loc_pred = tf.boolean_mask(\n                class_loc_pred, proposal_filter)\n            class_cls_prob = tf.boolean_mask(\n                class_cls_prob, proposal_filter)\n            proposal_anchors = tf.boolean_mask(\n                anchors, proposal_filter)\n\n            # Log results of filtering non-valid area proposals\n            total_anchors = tf.shape(all_anchors)[0]\n            total_proposals = tf.shape(class_proposals)[0]\n            total_raw_proposals = tf.shape(raw_proposals)[0]\n            tf.summary.scalar(\n                \'invalid_proposals\',\n                total_proposals - total_raw_proposals, [\'ssd\']\n            )\n            tf.summary.scalar(\n                \'valid_proposals_ratio\',\n                tf.cast(total_anchors, tf.float32) /\n                tf.cast(total_proposals, tf.float32), [\'ssd\']\n            )\n\n            # We have to use the TensorFlow\'s bounding box convention to use\n            # the included function for NMS.\n            # After gathering results we should normalize it back.\n            class_proposal_tf = change_order(class_proposals)\n\n            # Apply class NMS.\n            class_selected_idx = tf.image.non_max_suppression(\n                class_proposal_tf, class_cls_prob, self._class_max_detections,\n                iou_threshold=self._class_nms_threshold\n            )\n\n            # Using NMS resulting indices, gather values from Tensors.\n            class_proposal_tf = tf.gather(\n                class_proposal_tf, class_selected_idx)\n            class_cls_prob = tf.gather(class_cls_prob, class_selected_idx)\n\n            # We append values to a regular list which will later be\n            # transformed to a proper Tensor.\n            selected_boxes.append(class_proposal_tf)\n            selected_probs.append(class_cls_prob)\n            # In the case of the class_id, since it is a loop on classes, we\n            # already have a fixed class_id. We use `tf.tile` to create that\n            # Tensor with the total number of indices returned by the NMS.\n            selected_labels.append(\n                tf.tile([class_id], [tf.shape(class_selected_idx)[0]])\n            )\n            selected_anchors.append(proposal_anchors)\n\n        # We use concat (axis=0) to generate a Tensor where the rows are\n        # stacked on top of each other\n        proposals_tf = tf.concat(selected_boxes, axis=0)\n        # Return to the original convention.\n        proposals = change_order(proposals_tf)\n        proposal_label = tf.concat(selected_labels, axis=0)\n        proposal_label_prob = tf.concat(selected_probs, axis=0)\n        proposal_anchors = tf.concat(selected_anchors, axis=0)\n\n        # Get topK detections of all classes.\n        k = tf.minimum(\n            self._total_max_detections,\n            tf.shape(proposal_label_prob)[0]\n        )\n        top_k = tf.nn.top_k(proposal_label_prob, k=k)\n        top_k_proposal_label_prob = top_k.values\n        top_k_proposals = tf.gather(proposals, top_k.indices)\n        top_k_proposal_label = tf.gather(proposal_label, top_k.indices)\n        top_k_proposal_anchors = tf.gather(proposal_anchors, top_k.indices)\n\n        return {\n            \'objects\': top_k_proposals,\n            \'labels\': top_k_proposal_label,\n            \'probs\': top_k_proposal_label_prob,\n            \'raw_proposals\': raw_proposals,\n            \'anchors\': top_k_proposal_anchors,\n        }\n'"
luminoth/models/ssd/ssd.py,41,"b'import numpy as np\nimport sonnet as snt\nimport tensorflow as tf\n\nfrom sonnet.python.modules.conv import Conv2D\n\nfrom luminoth.models.ssd.feature_extractor import SSDFeatureExtractor\nfrom luminoth.models.ssd.proposal import SSDProposal\nfrom luminoth.models.ssd.target import SSDTarget\nfrom luminoth.models.ssd.utils import (\n    generate_raw_anchors, adjust_bboxes\n)\nfrom luminoth.utils.losses import smooth_l1_loss\nfrom luminoth.utils.bbox_transform import clip_boxes\n\n\nclass SSD(snt.AbstractModule):\n    """"""SSD: Single Shot MultiBox Detector\n    """"""\n\n    def __init__(self, config, name=\'ssd\'):\n        super(SSD, self).__init__(name=name)\n        self._config = config.model\n        self._num_classes = config.model.network.num_classes\n        self._debug = config.train.debug\n        self._seed = config.train.seed\n        self._anchor_max_scale = config.model.anchors.max_scale\n        self._anchor_min_scale = config.model.anchors.min_scale\n        self._anchor_ratios = np.array(config.model.anchors.ratios)\n        self.image_shape = [config.dataset.image_preprocessing.fixed_height,\n                            config.dataset.image_preprocessing.fixed_width]\n        self._anchors_per_point = config.model.anchors.anchors_per_point\n        self._loc_loss_weight = config.model.loss.localization_loss_weight\n        # TODO: Why not use the default LOSSES collection?\n        self._losses_collections = [\'ssd_losses\']\n\n    def _build(self, image, gt_boxes=None, is_training=False):\n        """"""\n        Returns bounding boxes and classification probabilities.\n\n        Args:\n            image: A tensor with the image.\n                Its shape should be `(height, width, 3)`.\n            gt_boxes: A tensor with all the ground truth boxes of that image.\n                Its shape should be `(num_gt_boxes, 5)`\n                Where for each gt box we have (x1, y1, x2, y2, label),\n                in that order.\n            is_training: A boolean to whether or not it is used for training.\n\n        Returns:\n            A dictionary with the following keys:\n            predictions:\n            proposal_prediction: A dictionary with:\n                proposals: The proposals of the network after appling some\n                    filters like negative area; and NMS\n                proposals_label: A tensor with the label for each proposal.\n                proposals_label_prob: A tensor with the softmax probability\n                    for the label of each proposal.\n            bbox_offsets: A tensor with the predicted bbox_offsets\n            class_scores: A tensor with the predicted classes scores\n        """"""\n        # Reshape image\n        self.image_shape.append(3)  # Add channels to shape\n        image.set_shape(self.image_shape)\n        image = tf.expand_dims(image, 0, name=\'hardcode_batch_size_to_1\')\n\n        # Generate feature maps from image\n        self.feature_extractor = SSDFeatureExtractor(\n            self._config.base_network, parent_name=self.module_name\n        )\n        feature_maps = self.feature_extractor(image, is_training=is_training)\n\n        # Build a MultiBox predictor on top of each feature layer and collect\n        # the bounding box offsets and the category score logits they produce\n        bbox_offsets_list = []\n        class_scores_list = []\n        for i, feat_map in enumerate(feature_maps.values()):\n            multibox_predictor_name = \'MultiBox_{}\'.format(i)\n            with tf.name_scope(multibox_predictor_name):\n                num_anchors = self._anchors_per_point[i]\n\n                # Predict bbox offsets\n                bbox_offsets_layer = Conv2D(\n                    num_anchors * 4, [3, 3],\n                    name=multibox_predictor_name + \'_offsets_conv\'\n                )(feat_map)\n                bbox_offsets_flattened = tf.reshape(\n                    bbox_offsets_layer, [-1, 4]\n                )\n                bbox_offsets_list.append(bbox_offsets_flattened)\n\n                # Predict class scores\n                class_scores_layer = Conv2D(\n                    num_anchors * (self._num_classes + 1), [3, 3],\n                    name=multibox_predictor_name + \'_classes_conv\',\n                )(feat_map)\n                class_scores_flattened = tf.reshape(\n                    class_scores_layer, [-1, self._num_classes + 1]\n                )\n                class_scores_list.append(class_scores_flattened)\n        bbox_offsets = tf.concat(\n            bbox_offsets_list, axis=0, name=\'concatenate_all_bbox_offsets\'\n        )\n        class_scores = tf.concat(\n            class_scores_list, axis=0, name=\'concatenate_all_class_scores\'\n        )\n        class_probabilities = tf.nn.softmax(\n            class_scores, axis=-1, name=\'class_probabilities_softmax\'\n        )\n\n        # Generate anchors (generated only once, therefore we use numpy)\n        raw_anchors_per_featmap = generate_raw_anchors(\n            feature_maps, self._anchor_min_scale, self._anchor_max_scale,\n            self._anchor_ratios, self._anchors_per_point\n        )\n        anchors_list = []\n        for i, (feat_map_name, feat_map) in enumerate(feature_maps.items()):\n            # TODO: Anchor generation should be simpler. We should create\n            #       them in image scale from the start instead of scaling\n            #       them to their feature map size.\n            feat_map_shape = feat_map.shape.as_list()[1:3]\n            scaled_bboxes = adjust_bboxes(\n                raw_anchors_per_featmap[feat_map_name], feat_map_shape[0],\n                feat_map_shape[1], self.image_shape[0], self.image_shape[1]\n            )\n            clipped_bboxes = clip_boxes(scaled_bboxes, self.image_shape)\n            anchors_list.append(clipped_bboxes)\n        anchors = np.concatenate(anchors_list, axis=0)\n        anchors = tf.convert_to_tensor(anchors, dtype=tf.float32)\n\n        # This is the dict we\'ll return after filling it with SSD\'s results\n        prediction_dict = {}\n\n        # Generate targets for training\n        if gt_boxes is not None:\n            gt_boxes = tf.cast(gt_boxes, tf.float32)\n\n            # Generate targets\n            target_creator = SSDTarget(\n                self._num_classes, self._config.target, self._config.variances\n            )\n            class_targets, bbox_offsets_targets = target_creator(\n                class_probabilities, anchors, gt_boxes\n            )\n\n            # Filter the predictions and targets that we will ignore during\n            # training due to hard negative mining. We use class_targets to\n            # know which ones to ignore (they are marked as -1 if they are to\n            # be ignored)\n            with tf.name_scope(\'hard_negative_mining_filter\'):\n                predictions_filter = tf.greater_equal(class_targets, 0)\n\n                anchors = tf.boolean_mask(\n                    anchors, predictions_filter)\n                bbox_offsets_targets = tf.boolean_mask(\n                    bbox_offsets_targets, predictions_filter)\n                class_targets = tf.boolean_mask(\n                    class_targets, predictions_filter)\n                class_scores = tf.boolean_mask(\n                    class_scores, predictions_filter)\n                class_probabilities = tf.boolean_mask(\n                    class_probabilities, predictions_filter)\n                bbox_offsets = tf.boolean_mask(\n                    bbox_offsets, predictions_filter)\n\n            # Add target tensors to prediction dict\n            prediction_dict[\'target\'] = {\n                \'cls\': class_targets,\n                \'bbox_offsets\': bbox_offsets_targets,\n                \'anchors\': anchors\n            }\n\n        # Add network\'s raw output to prediction dict\n        prediction_dict[\'cls_pred\'] = class_scores\n        prediction_dict[\'loc_pred\'] = bbox_offsets\n\n        # We generate proposals when predicting, or when debug=True for\n        # generating visualizations during training.\n        if not is_training or self._debug:\n            proposals_creator = SSDProposal(\n                self._num_classes, self._config.proposals,\n                self._config.variances\n            )\n            proposals = proposals_creator(\n                class_probabilities, bbox_offsets, anchors,\n                tf.cast(tf.shape(image)[1:3], tf.float32)\n            )\n            prediction_dict[\'classification_prediction\'] = proposals\n\n        # Add some non essential metrics for debugging\n        if self._debug:\n            prediction_dict[\'all_anchors\'] = anchors\n            prediction_dict[\'cls_prob\'] = class_probabilities\n\n        return prediction_dict\n\n    def loss(self, prediction_dict, return_all=False):\n        """"""Compute the loss for SSD.\n\n        Args:\n            prediction_dict: The output dictionary of the _build method from\n                which we use different main keys:\n\n                cls_pred: A dictionary with the classes classification.\n                loc_pred: A dictionary with the localization predictions\n                target: A dictionary with the targets for both classes and\n                    localizations.\n\n        Returns:\n            A tensor for the total loss.\n        """"""\n\n        with tf.name_scope(\'losses\'):\n\n            cls_pred = prediction_dict[\'cls_pred\']\n            cls_target = tf.cast(\n                prediction_dict[\'target\'][\'cls\'], tf.int32\n            )\n            # Transform to one-hot vector\n            cls_target_one_hot = tf.one_hot(\n                cls_target, depth=self._num_classes + 1,\n                name=\'cls_target_one_hot\'\n            )\n\n            # We get cross entropy loss of each proposal.\n            # TODO: Optimization opportunity: We calculate the probabilities\n            #       earlier in the program, so if we used those instead of the\n            #       logits we would not have the need to do softmax here too.\n            cross_entropy_per_proposal = (\n                tf.nn.softmax_cross_entropy_with_logits(\n                    labels=cls_target_one_hot, logits=cls_pred\n                )\n            )\n            # Second we need to calculate the smooth l1 loss between\n            # `bbox_offsets` and `bbox_offsets_targets`.\n            bbox_offsets = prediction_dict[\'loc_pred\']\n            bbox_offsets_targets = (\n                prediction_dict[\'target\'][\'bbox_offsets\']\n            )\n\n            # We only want the non-background labels bounding boxes.\n            not_ignored = tf.reshape(tf.greater(cls_target, 0), [-1])\n            bbox_offsets_positives = tf.boolean_mask(\n                bbox_offsets, not_ignored, name=\'bbox_offsets_positives\')\n            bbox_offsets_target_positives = tf.boolean_mask(\n                bbox_offsets_targets, not_ignored,\n                name=\'bbox_offsets_target_positives\'\n            )\n\n            # Calculate the smooth l1 regression loss between the flatten\n            # bboxes offsets  and the labeled targets.\n            reg_loss_per_proposal = smooth_l1_loss(\n                bbox_offsets_positives, bbox_offsets_target_positives)\n\n            cls_loss = tf.reduce_sum(cross_entropy_per_proposal)\n            bbox_loss = tf.reduce_sum(reg_loss_per_proposal)\n\n            # Following the paper, set loss to 0 if there are 0 bboxes\n            # assigned as foreground targets.\n            safety_condition = tf.not_equal(\n                tf.shape(bbox_offsets_positives)[0], 0\n            )\n            final_loss = tf.cond(\n                safety_condition,\n                true_fn=lambda: (\n                    (cls_loss + bbox_loss * self._loc_loss_weight) /\n                    tf.cast(tf.shape(bbox_offsets_positives)[0], tf.float32)\n                ),\n                false_fn=lambda: 0.0\n            )\n            tf.losses.add_loss(final_loss)\n            total_loss = tf.losses.get_total_loss()\n\n            prediction_dict[\'reg_loss_per_proposal\'] = reg_loss_per_proposal\n            prediction_dict[\'cls_loss_per_proposal\'] = (\n                cross_entropy_per_proposal\n            )\n\n            tf.summary.scalar(\n                \'cls_loss\', cls_loss,\n                collections=self._losses_collections\n            )\n\n            tf.summary.scalar(\n                \'bbox_loss\', bbox_loss,\n                collections=self._losses_collections\n            )\n\n            tf.summary.scalar(\n                \'total_loss\', total_loss,\n                collections=self._losses_collections\n            )\n            if return_all:\n                return {\n                    \'total_loss\': total_loss,\n                    \'cls_loss\': cls_loss,\n                    \'bbox_loss\': bbox_loss\n                }\n            else:\n                return total_loss\n\n    @property\n    def summary(self):\n        """"""\n        Generate merged summary of all the sub-summaries used inside the\n        ssd network.\n        """"""\n        summaries = [\n            tf.summary.merge_all(key=self._losses_collections[0])\n        ]\n\n        return tf.summary.merge(summaries)\n\n    def get_trainable_vars(self):\n        """"""Get trainable vars included in the module.\n        """"""\n        trainable_vars = snt.get_variables_in_module(self)\n        if self._config.base_network.trainable:\n            pretrained_trainable_vars = (\n                self.feature_extractor.get_trainable_vars()\n            )\n            tf.logging.info(\'Training {} vars from pretrained module.\'.format(\n                len(pretrained_trainable_vars)))\n            trainable_vars += pretrained_trainable_vars\n        else:\n            tf.logging.info(\'Not training variables from pretrained module\')\n\n        return trainable_vars\n\n    def get_base_network_checkpoint_vars(self):\n        return self.feature_extractor.get_base_network_checkpoint_vars()\n\n    def get_checkpoint_file(self):\n        return self.feature_extractor.get_checkpoint_file()\n'"
luminoth/models/ssd/target.py,36,"b'import tensorflow as tf\nimport sonnet as snt\n\nfrom luminoth.utils.bbox_transform_tf import encode\nfrom luminoth.utils.bbox_overlap import bbox_overlap_tf\n\n\nclass SSDTarget(snt.AbstractModule):\n    """"""Generate SSD target tensors for both probabilities and bounding boxes.\n\n    There are two types of targets, anchor_label and bounding box targets.\n\n    Anchor labels are just the label which best fits each anchor, and therefore\n    are the target for that anchor, they are used both for background and\n    foreground labels.\n\n    Bounding box targets are just the encoded coordinates that anchors labeled\n    as foreground should target.\n    """"""\n    def __init__(self, num_classes, config, variances, seed=None,\n                 name=\'ssd_target\'):\n        """"""\n        Args:\n            num_classes: Number of possible classes.\n            config: Configuration object for RCNNTarget.\n        """"""\n        super(SSDTarget, self).__init__(name=name)\n        self._num_classes = num_classes\n        self._hard_negative_ratio = config.hard_negative_ratio\n        self._foreground_threshold = config.foreground_threshold\n        self._background_threshold_high = config.background_threshold_high\n        self._variances = variances\n        self._seed = seed\n\n    def _build(self, probs, all_anchors, gt_boxes):\n        """"""\n        Args:\n            all_anchors: A Tensor with anchors for all of SSD\'s features.\n                The shape of the Tensor is (num_anchors, 4).\n            gt_boxes: A Tensor with the ground truth boxes for the image.\n                The shape of the Tensor is (num_gt, 5), having the truth label\n                as the last value for each box.\n        Returns:\n            class_targets: Either a truth value of the anchor (a value\n                between 0 and num_classes, with 0 being background), or -1 when\n                the anchor is to be ignored in the minibatch.\n                The shape of the Tensor is (num_anchors, 1).\n            bbox_offsets_targets: A bounding box regression target for each of\n                the anchors that have a greater than zero label. For every\n                other anchors we return zeros.\n                The shape of the Tensor is (num_anchors, 4).\n        """"""\n\n        all_anchors = tf.cast(all_anchors, tf.float32)\n        gt_boxes = tf.cast(gt_boxes, tf.float32)\n\n        # We are going to label each anchor based on the IoU with\n        # `gt_boxes`. Start by filling the labels with -1, marking them as\n        # unknown.\n        anchors_label_shape = tf.gather(tf.shape(all_anchors), [0])\n        anchors_label = tf.fill(\n            dims=anchors_label_shape,\n            value=-1.\n        )\n\n        overlaps = bbox_overlap_tf(all_anchors, gt_boxes[:, :4])\n        max_overlaps = tf.reduce_max(overlaps, axis=1)\n\n        # Get the index of the best gt_box for each anchor.\n        best_gtbox_for_anchors_idx = tf.argmax(overlaps, axis=1)\n\n        # Having the index of the gt bbox with the best label we need to get\n        # the label for each gt box and sum 1 to it because 0 is used for\n        # background.\n        best_fg_labels_for_anchors = tf.add(\n            tf.gather(gt_boxes[:, 4], best_gtbox_for_anchors_idx),\n            1.\n        )\n        iou_is_fg = tf.greater_equal(\n            max_overlaps, self._foreground_threshold\n        )\n        # We update anchors_label with the value in\n        # best_fg_labels_for_anchors only when the box is foreground.\n        # TODO: Replace with a sparse_to_dense with -1 default_value\n        anchors_label = tf.where(\n            condition=iou_is_fg,\n            x=best_fg_labels_for_anchors,\n            y=anchors_label\n        )\n\n        best_anchor_idxs = tf.argmax(overlaps, axis=0)\n        is_best_box = tf.sparse_to_dense(\n            sparse_indices=best_anchor_idxs,\n            sparse_values=True, default_value=False,\n            output_shape=tf.cast(anchors_label_shape, tf.int64),\n            validate_indices=False\n        )\n\n        # Now we need to find the anchors that are the best for each of the\n        # gt_boxes. We overwrite the previous anchors_label with this\n        # because setting the best anchor for each gt_box has priority.\n        best_anchors_gt_labels = tf.sparse_to_dense(\n            sparse_indices=best_anchor_idxs,\n            sparse_values=gt_boxes[:, 4] + 1,\n            default_value=-1,\n            output_shape=tf.cast(anchors_label_shape, tf.int64),\n            validate_indices=False,\n            name=""get_right_labels_for_bestboxes""\n        )\n        anchors_label = tf.where(\n            condition=is_best_box,\n            x=best_anchors_gt_labels,\n            y=anchors_label,\n            name=""update_labels_for_bestbox_anchors""\n        )\n\n        # Use the worst backgrounds (the bgs whose probability of being fg is\n        # the greatest).\n        cls_probs = probs[:, 1:]\n        max_cls_probs = tf.reduce_max(cls_probs, axis=1)\n\n        # Exclude boxes with IOU > `background_threshold_high` with any GT.\n        iou_less_than_bg_tresh_high_filter = tf.less_equal(\n            max_overlaps, self._background_threshold_high\n        )\n        bg_anchors = tf.less_equal(anchors_label, 0)\n        bg_overlaps_filter = tf.logical_and(\n            iou_less_than_bg_tresh_high_filter, bg_anchors\n        )\n\n        max_cls_probs = tf.where(\n            condition=bg_overlaps_filter,\n            x=max_cls_probs,\n            y=tf.fill(dims=anchors_label_shape, value=-1.),\n        )\n\n        # We calculate up to how many backgrounds we desire based on the\n        # final number of foregrounds and the hard minning ratio.\n        num_fg_mask = tf.greater(anchors_label, 0.0)\n        num_fg = tf.cast(tf.count_nonzero(num_fg_mask), tf.float32)\n\n        num_bg = tf.cast(num_fg * self._hard_negative_ratio, tf.int32)\n        top_k_bg = tf.nn.top_k(max_cls_probs, k=num_bg)\n\n        set_bg = tf.sparse_to_dense(\n            sparse_indices=top_k_bg.indices,\n            sparse_values=True, default_value=False,\n            output_shape=anchors_label_shape,\n            validate_indices=False\n        )\n\n        anchors_label = tf.where(\n            condition=set_bg,\n            x=tf.fill(dims=anchors_label_shape, value=0.),\n            y=anchors_label\n        )\n\n        # Next step is to calculate the proper bbox targets for the labeled\n        # anchors based on the values of the ground-truth boxes.\n        # We have to use only the anchors labeled >= 1, each matching with\n        # the proper gt_boxes\n\n        # Get the ids of the anchors that mater for bbox_target comparison.\n        is_anchor_with_target = tf.greater(\n            anchors_label, 0\n        )\n        anchors_with_target_idx = tf.where(\n            condition=is_anchor_with_target\n        )\n        # Get the corresponding ground truth box only for the anchors with\n        # target.\n        gt_boxes_idxs = tf.gather(\n            best_gtbox_for_anchors_idx,\n            anchors_with_target_idx\n        )\n        # Get the values of the ground truth boxes.\n        anchors_gt_boxes = tf.gather_nd(\n            gt_boxes[:, :4], gt_boxes_idxs\n        )\n        # We create the same array but with the anchors\n        anchors_with_target = tf.gather_nd(\n            all_anchors,\n            anchors_with_target_idx\n        )\n        # We create our targets with bbox_transform\n        bbox_targets = encode(\n            anchors_with_target,\n            anchors_gt_boxes,\n            variances=self._variances\n        )\n\n        # We unmap targets to anchor_labels (containing the length of\n        # anchors)\n        bbox_targets = tf.scatter_nd(\n            indices=anchors_with_target_idx,\n            updates=bbox_targets,\n            shape=tf.cast(tf.shape(all_anchors), tf.int64)\n        )\n\n        return anchors_label, bbox_targets\n'"
luminoth/models/ssd/utils.py,1,"b'import numpy as np\nimport tensorflow as tf\n\n\ndef adjust_bboxes(bboxes, old_height, old_width, new_height, new_width):\n    """"""Adjusts the bboxes of an image that has been resized.\n\n    Args:\n        bboxes: Tensor with shape (num_bboxes, 4).\n        old_height: Float. Height of the original image.\n        old_width: Float. Width of the original image.\n        new_height: Float. Height of the image after resizing.\n        new_width: Float. Width of the image after resizing.\n    Returns:\n        Tensor with shape (num_bboxes, 4), with the adjusted bboxes.\n    """"""\n    # x_min, y_min, x_max, y_max = np.split(bboxes, 4, axis=1)\n    x_min = bboxes[:, 0] / old_width\n    y_min = bboxes[:, 1] / old_height\n    x_max = bboxes[:, 2] / old_width\n    y_max = bboxes[:, 3] / old_height\n\n    # Use new size to scale back the bboxes points to absolute values.\n    x_min = x_min * new_width\n    y_min = y_min * new_height\n    x_max = x_max * new_width\n    y_max = y_max * new_height\n\n    # Concat points and label to return a [num_bboxes, 4] tensor.\n    return np.stack([x_min, y_min, x_max, y_max], axis=1)\n\n\ndef generate_anchors_reference(ratios, scales, num_anchors, feature_map_shape):\n    """"""\n    Generate the default anchor for one feat map which we will later convolve\n    to generate all the anchors of that feat map.\n    """"""\n    heights = np.zeros(num_anchors)\n    widths = np.zeros(num_anchors)\n\n    if len(scales) > 1:\n        widths[0] = heights[0] = (np.sqrt(scales[0] * scales[1]) *\n                                  feature_map_shape[0])\n    # The last endpoint\n    else:\n        # The last layer doesn\'t have a subsequent layer with which\n        # to generate the second scale from their geometric mean,\n        # so we hard code it to 0.99.\n        # We should add this parameter to the config eventually.\n        heights[0] = scales[0] * feature_map_shape[0] * 0.99\n        widths[0] = scales[0] * feature_map_shape[1] * 0.99\n\n    ratios = ratios[:num_anchors - 1]\n    heights[1:] = scales[0] / np.sqrt(ratios) * feature_map_shape[0]\n    widths[1:] = scales[0] * np.sqrt(ratios) * feature_map_shape[1]\n\n    # Each feature layer forms a grid on image space, so we\n    # calculate the center point on the first cell of this grid.\n    # Which we\'ll use as the center for our anchor reference.\n    # The center will be the midpoint of the top left cell,\n    # given that each cell is of 1x1 size, its center will be 0.5x0.5\n    x_center = y_center = 0.5\n\n    # Create anchor reference.\n    anchors = np.column_stack([\n        x_center - widths / 2,\n        y_center - heights / 2,\n        x_center + widths / 2,\n        y_center + heights / 2,\n    ])\n\n    return anchors\n\n\ndef generate_raw_anchors(feature_maps, anchor_min_scale, anchor_max_scale,\n                         anchor_ratios, anchors_per_point):\n    """"""\n    Returns a dictionary containing the anchors per feature map.\n\n    Returns:\n    anchors: A dictionary with feature maps as keys and an array of anchors\n        as values (\'[[x_min, y_min, x_max, y_max], ...]\') with shape\n        (anchors_per_point[i] * endpoints_outputs[i][0]\n         * endpoints_outputs[i][1], 4)\n    """"""\n    # TODO: Anchor generation needs heavy refactor\n\n    # We interpolate the scales of the anchors from a min and a max scale\n    scales = np.linspace(anchor_min_scale, anchor_max_scale, len(feature_maps))\n\n    anchors = {}\n    for i, (feat_map_name, feat_map) in enumerate(feature_maps.items()):\n        feat_map_shape = feat_map.shape.as_list()[1:3]\n        anchor_reference = generate_anchors_reference(\n            anchor_ratios, scales[i: i + 2],\n            anchors_per_point[i], feat_map_shape\n        )\n        anchors[feat_map_name] = generate_anchors_per_feat_map(\n            feat_map_shape, anchor_reference)\n\n    return anchors\n\n\ndef generate_anchors_per_feat_map(feature_map_shape, anchor_reference):\n    """"""Generate anchor for an image.\n\n    Using the feature map, the output of the pretrained network for an\n    image, and the anchor_reference generated using the anchor config\n    values. We generate a list of anchors.\n\n    Anchors are just fixed bounding boxes of different ratios and sizes\n    that are uniformly generated throught the image.\n\n    Args:\n        feature_map_shape: Shape of the convolutional feature map used as\n            input for the RPN. Should be (batch, height, width, depth).\n\n    Returns:\n        all_anchors: A flattened Tensor with all the anchors of shape\n            `(num_anchors_per_points * feature_width * feature_height, 4)`\n            using the (x1, y1, x2, y2) convention.\n    """"""\n    with tf.variable_scope(\'generate_anchors\'):\n        shift_x = np.arange(feature_map_shape[1])\n        shift_y = np.arange(feature_map_shape[0])\n        shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n\n        shift_x = np.reshape(shift_x, [-1])\n        shift_y = np.reshape(shift_y, [-1])\n\n        shifts = np.stack(\n            [shift_x, shift_y, shift_x, shift_y],\n            axis=0\n        )\n\n        shifts = np.transpose(shifts)\n        # Shifts now is a (H x W, 4) Tensor\n\n        # Expand dims to use broadcasting sum.\n        all_anchors = (\n            np.expand_dims(anchor_reference, axis=0) +\n            np.expand_dims(shifts, axis=1)\n        )\n        # Flatten\n        return np.reshape(all_anchors, (-1, 4))\n'"
luminoth/tools/checkpoint/__init__.py,3,"b'import click\nimport json\nimport os\nimport shutil\nimport six\nimport requests\nimport tarfile\nimport tempfile\nimport tensorflow as tf\nimport uuid\n\nfrom datetime import datetime\n\nfrom luminoth import __version__ as lumi_version\nfrom luminoth.utils.config import get_config\nfrom luminoth.utils.homedir import get_luminoth_home\n\n\nCHECKPOINT_INDEX = \'checkpoints.json\'\nCHECKPOINT_PATH = \'checkpoints\'\nREMOTE_INDEX_URL = (\n    \'https://github.com/tryolabs/luminoth/releases/download/v0.0.3/\'\n    \'checkpoints.json\'\n)\n\n\n# Definition of path management functions.\n\ndef get_checkpoints_directory():\n    """"""Returns checkpoint directory within Luminoth\'s homedir.""""""\n    # Checkpoint directory, `$LUMI_HOME/checkpoints/`. Create if not present.\n    path = os.path.join(get_luminoth_home(), CHECKPOINT_PATH)\n    if not os.path.exists(path):\n        tf.gfile.MakeDirs(path)\n    return path\n\n\ndef get_checkpoint_path(checkpoint_id):\n    """"""Returns checkpoint\'s directory.""""""\n    return os.path.join(get_checkpoints_directory(), checkpoint_id)\n\n\n# Index-related functions: access and mutation.\n\ndef read_checkpoint_db():\n    """"""Reads the checkpoints database file from disk.""""""\n    path = os.path.join(get_checkpoints_directory(), CHECKPOINT_INDEX)\n    if not os.path.exists(path):\n        return {\'checkpoints\': []}\n\n    with open(path) as f:\n        index = json.load(f)\n\n    return index\n\n\ndef save_checkpoint_db(checkpoints):\n    """"""Overwrites the database file in disk with `checkpoints`.""""""\n    path = os.path.join(get_checkpoints_directory(), CHECKPOINT_INDEX)\n    with open(path, \'w\') as f:\n        json.dump(checkpoints, f)\n\n\ndef merge_index(local_index, remote_index):\n    """"""Merge the `remote_index` into `local_index`.\n\n    The merging process is only applied over the checkpoints in `local_index`\n    marked as ``remote``.\n    """"""\n\n    non_remotes_in_local = [\n        c for c in local_index[\'checkpoints\']\n        if c[\'source\'] != \'remote\'\n    ]\n    remotes_in_local = {\n        c[\'id\']: c for c in local_index[\'checkpoints\']\n        if c[\'source\'] == \'remote\'\n    }\n\n    to_add = []\n    seen_ids = set()\n    for checkpoint in remote_index[\'checkpoints\']:\n        seen_ids.add(checkpoint[\'id\'])\n        local = remotes_in_local.get(checkpoint[\'id\'])\n        if local:\n            # Checkpoint is in local index. Overwrite all the fields.\n            local.update(**checkpoint)\n        elif not local:\n            # Checkpoint not found, it\'s an addition. Transform into our schema\n            # before appending to `to_add`. (The remote index schema is exactly\n            # the same except for the ``source`` and ``status`` keys.)\n            checkpoint[\'source\'] = \'remote\'\n            checkpoint[\'status\'] = \'NOT_DOWNLOADED\'\n            to_add.append(checkpoint)\n\n    # Out of the removed checkpoints, only keep those with status\n    # ``DOWNLOADED`` and turn them into local checkpoints.\n    missing_ids = set(remotes_in_local.keys()) - seen_ids\n    already_downloaded = [\n        c for c in remotes_in_local.values()\n        if c[\'id\'] in missing_ids and c[\'status\'] == \'DOWNLOADED\'\n    ]\n    for checkpoint in already_downloaded:\n        checkpoint[\'status\'] = \'LOCAL\'\n        checkpoint[\'source\'] = \'local\'\n\n    new_remotes = [\n        c for c in remotes_in_local.values()\n        if not c[\'id\'] in missing_ids  # Checkpoints to remove.\n    ] + to_add + already_downloaded\n\n    if len(to_add):\n        click.echo(\'{} new remote checkpoints added.\'.format(len(to_add)))\n    if len(missing_ids):\n        if len(already_downloaded):\n            click.echo(\'{} remote checkpoints transformed to local.\'.format(\n                len(already_downloaded)\n            ))\n        click.echo(\'{} remote checkpoints removed.\'.format(\n            len(missing_ids) - len(already_downloaded)\n        ))\n    if not len(to_add) and not len(missing_ids):\n        click.echo(\'No changes in remote index.\')\n\n    local_index[\'checkpoints\'] = non_remotes_in_local + new_remotes\n\n    return local_index\n\n\ndef get_checkpoint(db, id_or_alias):\n    """"""Returns checkpoint entry in `db` indicated by `id_or_alias`.\n\n    First tries to match an ID, then an alias. For the case of repeated\n    aliases, will first match local checkpoints and then remotes. In both\n    cases, matching will be newest first.\n    """"""\n    # Go through the checkpoints ordered by creation date. There shouldn\'t be\n    # repeated aliases, but if there are, prioritize the newest one.\n    local_checkpoints = sorted(\n        [c for c in db[\'checkpoints\'] if c[\'source\'] == \'local\'],\n        key=lambda c: c[\'created_at\'], reverse=True\n    )\n    remote_checkpoints = sorted(\n        [c for c in db[\'checkpoints\'] if c[\'source\'] == \'remote\'],\n        key=lambda c: c[\'created_at\'], reverse=True\n    )\n\n    selected = []\n    for cp in local_checkpoints:\n        if cp[\'id\'] == id_or_alias or cp[\'alias\'] == id_or_alias:\n            selected.append(cp)\n\n    for cp in remote_checkpoints:\n        if cp[\'id\'] == id_or_alias or cp[\'alias\'] == id_or_alias:\n            selected.append(cp)\n\n    if len(selected) < 1:\n        return None\n\n    if len(selected) > 1:\n        click.echo(\n            ""Multiple checkpoints found for \'{}\' ({}). Returning \'{}\'."".format(\n                id_or_alias, len(selected), selected[0][\'id\']\n            )\n        )\n\n    return selected[0]\n\n\ndef get_checkpoint_config(id_or_alias, prompt=True):\n    """"""Returns the checkpoint config object in order to load the model.\n\n    If `prompt` is ``True`` and the checkpoint is not present in the index,\n    prompt the user to refresh the index. If the checkpoint is present in the\n    index but is remote and not yet downloaded, prompt to download.\n    """"""\n    db = read_checkpoint_db()\n    checkpoint = get_checkpoint(db, id_or_alias)\n\n    if prompt and not checkpoint:\n        # Checkpoint not found in database. Prompt for refreshing the index and\n        # try again.\n        click.confirm(\n            \'Checkpoint not found. Check remote repository?\', abort=True\n        )\n        db = refresh_remote_index()\n        checkpoint = get_checkpoint(db, id_or_alias)\n        if not checkpoint:\n            # Still not found, abort.\n            click.echo(\n                ""Checkpoint isn\'t available in remote repository either.""\n            )\n            raise ValueError(\'Checkpoint not found.\')\n    elif not checkpoint:\n        # No checkpoint but didn\'t prompt.\n        raise ValueError(\'Checkpoint not found.\')\n\n    if prompt and checkpoint[\'status\'] == \'NOT_DOWNLOADED\':\n        # Checkpoint hasn\'t been downloaded yet. Prompt for downloading it\n        # before continuing.\n        click.confirm(\n            \'Checkpoint not present locally. Want to download it?\', abort=True\n        )\n        download_remote_checkpoint(db, checkpoint)\n    elif checkpoint[\'status\'] == \'NOT_DOWNLOADED\':\n        # Not downloaded but didn\'t prompt.\n        raise ValueError(\'Checkpoint not downloaded.\')\n\n    path = get_checkpoint_path(checkpoint[\'id\'])\n    config = get_config(os.path.join(path, \'config.yml\'))\n\n    # Config paths should point to the path where the checkpoint files are\n    # stored.\n    config.dataset.dir = path\n    config.train.job_dir = get_checkpoints_directory()\n\n    return config\n\n\ndef field_allowed(field):\n    return field in [\n        \'name\', \'description\', \'alias\', \'dataset.name\', \'dataset.num_classes\',\n    ]\n\n\ndef parse_entries(entries, editing=False):\n    """"""Parses and validates `entries`.""""""\n    # Parse the entries (list of `<field>=<value>`).\n    values = [tuple(entry.split(\'=\')) for entry in entries]\n\n    # Validate allowed field values to modify.\n    disallowed = [k for k, _ in values if not field_allowed(k)]\n    if disallowed:\n        click.echo(""The following fields may not be set: {}"".format(\n            \', \'.join(disallowed)\n        ))\n        return\n\n    # Make sure there are no repeated values.\n    unique = set([k for k, _ in values])\n    if len(values) - len(unique) > 0:\n        click.echo(""Repeated fields. Each field may be passed exactly once."")\n        return\n\n    return dict(values)\n\n\ndef apply_entries(checkpoint, entries):\n    """"""Recursively modifies `checkpoint` with `entries` values.""""""\n    for field, value in entries.items():\n        # Access the last dict (if nested) and modify it.\n        to_edit = checkpoint\n        bits = field.split(\'.\')\n        for bit in bits[:-1]:\n            to_edit = to_edit[bit]\n        to_edit[bits[-1]] = value\n\n    return checkpoint\n\n\n# Network-related IO functions.\n\ndef get_remote_index_url():\n    url = REMOTE_INDEX_URL\n    if \'LUMI_REMOTE_URL\' in os.environ:\n        url = os.environ[\'LUMI_REMOTE_URL\']\n    return url\n\n\ndef fetch_remote_index():\n    url = get_remote_index_url()\n    index = requests.get(url).json()\n    return index\n\n\ndef refresh_remote_index():\n    click.echo(\'Retrieving remote index... \', nl=False)\n    remote = fetch_remote_index()\n    click.echo(\'done.\')\n\n    db = read_checkpoint_db()\n    db = merge_index(db, remote)\n\n    save_checkpoint_db(db)\n\n    return db\n\n\ndef download_remote_checkpoint(db, checkpoint):\n    # Check if output directory doesn\'t exist already to fail early.\n    output = get_checkpoint_path(checkpoint[\'id\'])\n    if os.path.exists(output):\n        click.echo(\n            ""Checkpoint directory \'{}\' for checkpoint_id \'{}\' already exists. ""\n            ""Try issuing a `lumi checkpoint delete` or delete the directory ""\n            ""manually."".format(output, checkpoint[\'id\'])\n        )\n        return\n\n    # Create a temporary directory to download the tar into.\n    tempdir = tempfile.mkdtemp()\n    path = os.path.join(tempdir, \'{}.tar\'.format(checkpoint[\'id\']))\n\n    # Start the actual tar file download.\n    response = requests.get(checkpoint[\'url\'], stream=True)\n    length = int(response.headers.get(\'Content-Length\'))\n    chunk_size = 16 * 1024\n    progressbar = click.progressbar(\n        response.iter_content(chunk_size=chunk_size),\n        length=length / chunk_size, label=\'Downloading checkpoint...\',\n    )\n\n    with open(path, \'wb\') as f:\n        with progressbar as content:\n            for chunk in content:\n                f.write(chunk)\n\n    # Import the checkpoint from the tar.\n    click.echo(""Importing checkpoint... "", nl=False)\n    with tarfile.open(path) as f:\n        members = [m for m in f.getmembers() if m.name != \'metadata.json\']\n        f.extractall(output, members)\n    click.echo(""done."")\n\n    # Update the checkpoint status and persist database.\n    checkpoint[\'status\'] = \'DOWNLOADED\'\n    save_checkpoint_db(db)\n\n    # And finally make sure to delete the temp dir.\n    shutil.rmtree(tempdir)\n\n    click.echo(""Checkpoint imported successfully."")\n\n\n# Actual command definition.\n\n@click.command(help=\'List available checkpoints.\')\ndef list():\n    db = read_checkpoint_db()\n\n    if not db[\'checkpoints\']:\n        click.echo(\'No checkpoints available.\')\n        return\n\n    template = \'| {:>12} | {:>21} | {:>11} | {:>6} | {:>14} |\'\n\n    header = template.format(\n        \'id\', \'name\', \'alias\', \'source\', \'status\'\n    )\n    click.echo(\'=\' * len(header))\n    click.echo(header)\n    click.echo(\'=\' * len(header))\n\n    for checkpoint in db[\'checkpoints\']:\n        line = template.format(\n            checkpoint[\'id\'],\n            checkpoint[\'name\'],\n            checkpoint[\'alias\'],\n            checkpoint[\'source\'],\n            checkpoint[\'status\'],\n        )\n        click.echo(line)\n\n    click.echo(\'=\' * len(header))\n\n\n@click.command(help=\'Display detailed information on checkpoint.\')\n@click.argument(\'id_or_alias\')\ndef info(id_or_alias):\n    db = read_checkpoint_db()\n\n    checkpoint = get_checkpoint(db, id_or_alias)\n    if not checkpoint:\n        click.echo(\n            ""Checkpoint \'{}\' not found in index."".format(id_or_alias)\n        )\n        return\n\n    if checkpoint[\'alias\']:\n        click.echo(\'{} ({}, {})\'.format(\n            checkpoint[\'name\'], checkpoint[\'id\'], checkpoint[\'alias\']\n        ))\n    else:\n        click.echo(\'{} ({})\'.format(checkpoint[\'name\'], checkpoint[\'id\']))\n\n    if checkpoint[\'description\']:\n        click.echo(checkpoint[\'description\'])\n\n    click.echo()\n\n    click.echo(\'Model used: {}\'.format(checkpoint[\'model\']))\n    click.echo(\'Dataset information\')\n    click.echo(\'    Name: {}\'.format(checkpoint[\'dataset\'][\'name\']))\n    click.echo(\'    Number of classes: {}\'.format(\n        checkpoint[\'dataset\'][\'num_classes\']\n    ))\n\n    click.echo()\n\n    click.echo(\'Creation date: {}\'.format(checkpoint[\'created_at\']))\n    click.echo(\'Luminoth version: {}\'.format(checkpoint[\'luminoth_version\']))\n\n    click.echo()\n\n    if checkpoint[\'source\'] == \'remote\':\n        click.echo(\'Source: {} ({})\'.format(\n            checkpoint[\'source\'], checkpoint[\'status\']\n        ))\n        click.echo(\'URL: {}\'.format(checkpoint[\'url\']))\n    else:\n        click.echo(\'Source: {}\'.format(checkpoint[\'source\']))\n\n\n@click.command(help=\'Create a checkpoint from a configuration file.\')\n@click.argument(\'config_files\', nargs=-1)\n@click.option(\n    \'override_params\', \'--override\', \'-o\', multiple=True,\n    help=\'Override model config params.\'\n)\n@click.option(\n    \'entries\', \'--entry\', \'-e\', multiple=True,\n    help=""Specify checkpoint\'s metadata field value.""\n)\ndef create(config_files, override_params, entries):\n    # Parse the entries passed as options.\n    entries = parse_entries(entries)\n    if entries is None:\n        return\n\n    click.echo(\'Creating checkpoint for given configuration...\')\n    # Get and build the configuration file for the model.\n    config = get_config(config_files, override_params=override_params)\n\n    # Retrieve the files for the last checkpoint available.\n    run_dir = os.path.join(config.train.job_dir, config.train.run_name)\n    ckpt = tf.train.get_checkpoint_state(run_dir)\n    if not ckpt or not ckpt.all_model_checkpoint_paths:\n        click.echo(""Couldn\'t find checkpoint in \'{}\'."".format(run_dir))\n        return\n\n    last_checkpoint = sorted([\n        {\'global_step\': int(path.split(\'-\')[-1]), \'file\': path}\n        for path in ckpt.all_model_checkpoint_paths\n    ], key=lambda c: c[\'global_step\'])[-1][\'file\']\n\n    checkpoint_prefix = os.path.basename(last_checkpoint)\n    checkpoint_paths = [\n        os.path.join(run_dir, file)\n        for file in os.listdir(run_dir)\n        if file.startswith(checkpoint_prefix)\n    ]\n\n    # Find the `classes.json` file.\n    classes_path = os.path.join(config.dataset.dir, \'classes.json\')\n    if not os.path.exists(classes_path):\n        classes_path = None\n\n    # Create an checkpoint_id to identify the checkpoint.\n    checkpoint_id = str(uuid.uuid4()).replace(\'-\', \'\')[:12]\n\n    # Update the directory paths for the configuration file. Since it\'s going\n    # to be packed into a single tar file, we set them to the current directoy.\n    config.dataset.dir = \'.\'\n    config.train.job_dir = \'.\'\n    config.train.run_name = checkpoint_id\n\n    # Create the directory that will contain the model.\n    path = get_checkpoint_path(checkpoint_id)\n    tf.gfile.MakeDirs(path)\n\n    with open(os.path.join(path, \'config.yml\'), \'w\') as f:\n        json.dump(config, f)\n\n    # Add the checkpoint files.\n    for checkpoint_path in checkpoint_paths:\n        shutil.copy2(checkpoint_path, path)\n\n    # Add `checkpoint` file to indicate where the checkpoint is located. We\n    # need to create it manually instead of just copying as it may contain\n    # absolute paths.\n    with open(os.path.join(path, \'checkpoint\'), \'w\') as f:\n        f.write(\n            """"""\n            model_checkpoint_path: ""{0}""\n            all_model_checkpoint_paths: ""{0}""\n            """""".format(checkpoint_prefix)\n        )\n\n    # Add the `classes.json` file. Also get the number of classes, if\n    # available.\n    num_classes = None\n    if classes_path:\n        shutil.copy2(classes_path, path)\n        with open(classes_path) as f:\n            num_classes = len(json.load(f))\n\n    # Store the new checkpoint into the checkpoint index.\n    metadata = {\n        \'id\': checkpoint_id,\n        \'name\': entries.get(\'name\', \'\'),\n        \'description\': entries.get(\'description\', \'\'),\n        \'alias\': entries.get(\'alias\', \'\'),\n\n        \'model\': config.model.type,\n        \'dataset\': {\n            \'name\': entries.get(\'dataset.name\', \'\'),\n            \'num_classes\': (\n                num_classes or entries.get(\'dataset.num_classes\', None)\n            ),\n        },\n\n        \'luminoth_version\': lumi_version,\n        \'created_at\': datetime.utcnow().isoformat(),\n\n        \'status\': \'LOCAL\',\n        \'source\': \'local\',\n        \'url\': None,  # Only for remotes.\n    }\n\n    db = read_checkpoint_db()\n    db[\'checkpoints\'].append(metadata)\n    save_checkpoint_db(db)\n\n    click.echo(\'Checkpoint {} created successfully.\'.format(checkpoint_id))\n\n\n@click.command(help=""Edit a checkpoint\'s metadata."")\n@click.argument(\'id_or_alias\')\n@click.option(\n    \'entries\', \'--entry\', \'-e\', multiple=True,\n    help=""Overwrite checkpoint\'s metadata field value.""\n)\ndef edit(id_or_alias, entries):\n    db = read_checkpoint_db()\n    checkpoint = get_checkpoint(db, id_or_alias)\n    if not checkpoint:\n        click.echo(\n            ""Checkpoint \'{}\' not found in index."".format(id_or_alias)\n        )\n        return\n\n    # Parse the entries to modify and apply them.\n    entries = parse_entries(entries, editing=True)\n    if entries is None:\n        return\n\n    apply_entries(checkpoint, entries)\n\n    save_checkpoint_db(db)\n\n    click.echo(\'Checkpoint edited successfully.\')\n\n\n@click.command(help=\'Remove a checkpoint from the index and delete its files.\')\n@click.argument(\'id_or_alias\')\ndef delete(id_or_alias):\n    db = read_checkpoint_db()\n    checkpoint = get_checkpoint(db, id_or_alias)\n    if not checkpoint:\n        click.echo(\n            ""Checkpoint \'{}\' not found in index."".format(id_or_alias)\n        )\n        return\n\n    # If checkpoint is local, remove entry from index. If it\'s remote, only\n    # mark as ``NOT_DOWNLOADED``.\n    if checkpoint[\'source\'] == \'local\':\n        db[\'checkpoints\'] = [\n            cp for cp in db[\'checkpoints\']\n            if not cp[\'id\'] == checkpoint[\'id\']\n        ]\n    else:  # Remote.\n        if checkpoint[\'status\'] == \'NOT_DOWNLOADED\':\n            click.echo(""Checkpoint isn\'t downloaded. Nothing to delete."")\n            return\n        checkpoint[\'status\'] = \'NOT_DOWNLOADED\'\n    save_checkpoint_db(db)\n\n    # Delete tar file associated to checkpoint.\n    path = get_checkpoint_path(checkpoint[\'id\'])\n    try:\n        shutil.rmtree(path)\n    except OSError:\n        # The tar is not present, warn the user just in case.\n        click.echo(\n            \'Skipping files deletion; not present in {}.\'.format(path)\n        )\n\n    click.echo(\'Checkpoint {} deleted successfully.\'.format(checkpoint[\'id\']))\n\n\n@click.command(help=\'Export a checkpoint to a tar file for easy sharing.\')\n@click.argument(\'id_or_alias\')\n@click.option(\'--output\', default=\'.\', help=\'Specify the output location.\')\ndef export(id_or_alias, output):\n    db = read_checkpoint_db()\n    checkpoint = get_checkpoint(db, id_or_alias)\n    if not checkpoint:\n        click.echo(\n            ""Checkpoint \'{}\' not found in index."".format(id_or_alias)\n        )\n        return\n\n    # Create the tar that will contain the checkpoint.\n    tar_path = os.path.join(\n        os.path.abspath(os.path.expanduser(output)),\n        \'{}.tar\'.format(checkpoint[\'id\'])\n    )\n    checkpoint_path = get_checkpoint_path(checkpoint[\'id\'])\n    with tarfile.open(tar_path, \'w\') as f:\n        # Add the config file. Dump the dict into a BytesIO, go to the\n        # beginning of the file and pass it as a file to the tar.\n        metadata_file = six.BytesIO()\n        metadata_file.write(json.dumps(checkpoint).encode(\'utf-8\'))\n        metadata_file.seek(0)\n\n        tarinfo = tarfile.TarInfo(name=\'metadata.json\')\n        tarinfo.size = len(metadata_file.getvalue())\n        f.addfile(tarinfo=tarinfo, fileobj=metadata_file)\n\n        metadata_file.close()\n\n        # Add the files present in the checkpoint\'s directory.\n        for filename in os.listdir(checkpoint_path):\n            path = os.path.join(checkpoint_path, filename)\n            f.add(path, filename)\n\n    click.echo(\'Checkpoint {} exported successfully.\'.format(checkpoint[\'id\']))\n\n\n@click.command(help=\'Import a checkpoint tar into the local index.\')\n@click.argument(\'path\')\ndef import_(path):\n    # Load the checkpoint metadata first.\n    path = os.path.expanduser(path)\n    try:\n        with tarfile.open(path) as f:\n            # This way of loading the JSON works in all supported versions of\n            # Python 2 and 3.\n            # See: https://github.com/tryolabs/luminoth/issues/222\n            metadata = json.loads(\n                f.extractfile(\'metadata.json\').read().decode(\'utf-8\')\n            )\n    except tarfile.ReadError:\n        click.echo(""Invalid file. Is it an exported checkpoint?"")\n        return\n    except KeyError:\n        click.echo(\n            ""Tar file doesn\'t contain `metadata.json`. ""\n            ""Is it an exported checkpoint?""\n        )\n        return\n\n    # Check if checkpoint isn\'t present already.\n    db = read_checkpoint_db()\n    checkpoint = get_checkpoint(db, metadata[\'id\'])\n    if checkpoint:\n        click.echo(\n            ""Checkpoint \'{}\' already found in index."".format(metadata[\'id\'])\n        )\n        return\n\n    # Check if the output directory doesn\'t exist already.\n    output_path = get_checkpoint_path(metadata[\'id\'])\n    if os.path.exists(output_path):\n        click.echo(\n            ""Checkpoint directory \'{}\' for checkpoint_id \'{}\' already exists. ""\n            ""Try issuing a `lumi checkpoint delete` or delete the directory ""\n            ""manually."".format(output_path, metadata[\'id\'])\n        )\n        return\n\n    # Extract all the files except `metadata.json` into the checkpoint\n    # directory.\n    with tarfile.open(path) as f:\n        members = [m for m in f.getmembers() if m.name != \'metadata.json\']\n        f.extractall(output_path, members)\n\n    # Store metadata into the checkpoint index.\n    db[\'checkpoints\'].append(metadata)\n    save_checkpoint_db(db)\n\n    click.echo(\'Checkpoint {} imported successfully.\'.format(metadata[\'id\']))\n\n\n@click.command(help=\'Refresh the remote checkpoint index.\')\ndef refresh():\n    refresh_remote_index()\n\n\n@click.command(help=\'Download a remote checkpoint.\')\n@click.argument(\'id_or_alias\')\ndef download(id_or_alias):\n    db = read_checkpoint_db()\n    checkpoint = get_checkpoint(db, id_or_alias)\n    if not checkpoint:\n        click.echo(\n            ""Checkpoint \'{}\' not found in index."".format(id_or_alias)\n        )\n        return\n\n    if checkpoint[\'source\'] != \'remote\':\n        click.echo(\n            ""Checkpoint is not remote. If you intended to download a remote ""\n            ""checkpoint and used an alias, try using the id directly.""\n        )\n        return\n\n    if checkpoint[\'status\'] != \'NOT_DOWNLOADED\':\n        click.echo(""Checkpoint is already downloaded."")\n        return\n\n    download_remote_checkpoint(db, checkpoint)\n\n\n@click.group(help=\'Groups of commands to manage checkpoints\')\ndef checkpoint():\n    pass\n\n\ncheckpoint.add_command(create)\ncheckpoint.add_command(delete)\ncheckpoint.add_command(download)\ncheckpoint.add_command(edit)\ncheckpoint.add_command(export)\ncheckpoint.add_command(import_, name=\'import\')\ncheckpoint.add_command(info)\ncheckpoint.add_command(list)\ncheckpoint.add_command(refresh)\n'"
luminoth/tools/cloud/__init__.py,0,b'from .cli import cloud  # noqa\n'
luminoth/tools/cloud/cli.py,0,"b""import click\n\nfrom .gcloud import gc\n\n\n@click.group(help='Groups of commands to train models in the cloud')\ndef cloud():\n    pass\n\n\ncloud.add_command(gc)\n"""
luminoth/tools/cloud/gcloud.py,5,"b'import click\nimport json\nimport luminoth\nimport os\nimport shutil\nimport subprocess\nimport sys\nimport tempfile\nimport tensorflow as tf\nimport time\n\nfrom datetime import datetime\nfrom functools import wraps\n\nfrom luminoth.utils.config import get_config, dump_config\nfrom luminoth.utils.experiments import save_run\n\nMISSING_DEPENDENCIES = False\ntry:\n    from google.cloud import storage\n    from googleapiclient import discovery, errors\n    from oauth2client import service_account\nexcept ImportError:\n    MISSING_DEPENDENCIES = True\n\n\nRUNTIME_VERSION = \'1.10\'\n\n# We cannot use Python 3 yet, unless we figure out a way to set environment\n# variables before Luminoth is launched (set locale).\n# See Click issue: https://bit.ly/2zxEkmM\nPYTHON_VERSION = \'2.7\'\n\nSCALE_TIERS = [\n    \'BASIC\', \'STANDARD_1\', \'PREMIUM_1\', \'BASIC_GPU\', \'BASIC_TPU\', \'CUSTOM\'\n]\nMACHINE_TYPES = [\n    \'standard\', \'large_model\', \'complex_model_s\', \'complex_model_m\',\n    \'complex_model_l\', \'standard_gpu\', \'complex_model_m_gpu\',\n    \'complex_model_l_gpu\', \'standard_p100\', \'complex_model_m_p100\',\n    \'cloud_tpu\'\n]\n\nDEFAULT_SCALE_TIER = \'BASIC_GPU\'\nDEFAULT_MASTER_TYPE = \'standard_gpu\'\nDEFAULT_WORKER_TYPE = \'standard_gpu\'\nDEFAULT_WORKER_COUNT = 2\nDEFAULT_PS_TYPE = \'large_model\'\nDEFAULT_PS_COUNT = 0\n\nDEFAULT_CONFIG_FILENAME = \'config.yml\'\nDEFAULT_PACKAGES_PATH = \'packages\'\n\n# We will create this Python package with only a `setup.py` file, in order to\n# install the current version of Luminoth as a dependency in GCP.\nGCP_TRAINER_PACKAGE_SETUP = """"""\nfrom setuptools import find_packages\nfrom setuptools import setup\n\nREQUIRED_PACKAGES = [\'luminoth=={version}\']\n\nsetup(\n    name=\'luminoth-gcp-setup\',\n    version=\'{version}\',\n    install_requires=REQUIRED_PACKAGES,\n    packages=find_packages(),\n    include_package_data=True,\n    description=\'Installs Luminoth in GCP.\'\n)\n"""""".format(version=luminoth.__version__)\n\n\ndef check_dependencies(f):\n    """"""\n    Decorator for commands that will check if they have\n    """"""\n    @wraps(f)\n    def decorated_function(*args, **kwargs):\n        if MISSING_DEPENDENCIES:\n            raise click.ClickException(\n                \'To use Google Cloud functionalities, you must install \'\n                \'Luminoth with the `gcloud` extras.\\n\\n\'\n                \'\'\n                \'Ie. `pip install luminoth[gcloud]`\'\n            )\n\n        return f(*args, **kwargs)\n\n    return decorated_function\n\n\ndef build_package(bucket, base_path):\n    temp_dir = tempfile.mkdtemp()\n    output_dir = os.path.join(temp_dir, \'output\')\n\n    #\n    # Option 1: try to find `setup.py` file in package dir.\n    #\n    # Will only work if the package is installed with -e (useful for\n    # development).\n    #\n    tentative_dir = os.path.abspath(\n        os.path.join(os.path.realpath(__file__), \'..\', \'..\', \'..\', \'..\')\n    )\n    setup_file = os.path.join(tentative_dir, \'setup.py\')\n    if os.path.isfile(setup_file):\n        package_dir = tentative_dir\n        click.echo(\n            \'Found `setup.py` file in ""{}"". \'\n            \'Using it instead of shim.\'.format(tentative_dir))\n    else:\n        #\n        # Option 2: generate a `setup.py` file that can install current version\n        #           of Luminoth.\n        #\n        package_dir = temp_dir\n        setup_file = os.path.join(package_dir, \'setup.py\')\n        with open(setup_file, \'w\') as f:\n            f.write(GCP_TRAINER_PACKAGE_SETUP)\n\n        click.echo(\'Generating ""{}"" for installing luminoth=={}.\'.format(\n            setup_file, luminoth.__version__\n        ))\n\n    devnull = open(os.devnull, \'w\')\n    subprocess.call(\n        [\'python\', \'setup.py\', \'sdist\', \'--dist-dir\', output_dir],\n        cwd=package_dir, stdout=devnull, stderr=devnull,\n    )\n\n    tarball_filename = os.listdir(output_dir)[0]\n    tarball_path = os.path.join(output_dir, tarball_filename)\n    click.echo(\'Built tarball for GCP: ""{}"".\'.format(tarball_path))\n\n    path = upload_file(\n        bucket, \'{}/{}\'.format(base_path, DEFAULT_PACKAGES_PATH), tarball_path\n    )\n    shutil.rmtree(temp_dir)\n\n    return path\n\n\nclass ServiceAccount(object):\n    """"""\n    Wrapper for handling Google services via the Service Account.\n    """"""\n\n    def __init__(self):\n        try:\n            data = json.load(\n                tf.gfile.GFile(\n                    os.environ.get(\'GOOGLE_APPLICATION_CREDENTIALS\', \'\'), \'r\'\n                )\n            )\n\n            self.project_id = data[\'project_id\']\n            self.client_id = data[\'client_id\']\n\n            self.credentials = service_account.ServiceAccountCredentials.\\\n                from_json_keyfile_dict(data)\n        except (ValueError, tf.errors.NotFoundError):\n            click.echo(\n                \'Error: could not read service account credentials.\\n\\n\'\n                \'Make sure the GOOGLE_APPLICATION_CREDENTIALS environment \'\n                \'variable is set and points to a valid service account JSON \'\n                \'file.\',\n                err=True\n            )\n            sys.exit(1)\n\n    def cloud_service(self, service, version=\'v1\'):\n        return discovery.build(service, version, credentials=self.credentials)\n\n    def get_bucket(self, bucket_name):\n        # If not passed, will get credentials from env. This library (storage)\n        # doesn\'t work with self.credentials.\n        cli = storage.Client()\n        bucket = cli.lookup_bucket(bucket_name)\n        if not bucket:\n            bucket = cli.create_bucket(bucket_name)\n        return bucket\n\n    def validate_region(self, region):\n        regionrequest = self.cloud_service(\'compute\').regions().get(\n            region=region, project=self.project_id\n        )\n        try:\n            regionrequest.execute()\n        except errors.HttpError as err:\n            if err.resp.status == 404:\n                click.echo(\n                    \'Error: Couldn\\\'t find region ""{}"" for \'\n                    \'project ""{}"".\'.format(region, self.project_id),\n                    err=True)\n            elif err.resp.status == 403:\n                click.echo(\'Error: Forbidden access to resources.\')\n                click.echo(\'Raw response:\\n{}\\n\'.format(err.content))\n                click.echo(\n                    \'Make sure to enable the following APIs for the project:\\n\'\n                    \'  * Compute Engine\\n\'\n                    \'  * Cloud Machine Learning Engine\\n\'\n                    \'  * Google Cloud Storage\\n\'\n                    \'You can do it with the following command:\\n\'\n                    \'  gcloud services enable compute.googleapis.com \'\n                    \'ml.googleapis.com storage-component.googleapis.com\\n\\n\'\n                    \'For information on how to enable these APIs, see here: \'\n                    \'https://support.google.com/cloud/answer/6158841\',\n                    err=True\n                )\n            else:\n                click.echo(\'Unknown error: {}\'.format(err.resp), err=True)\n            sys.exit(1)\n\n\ndef upload_data(bucket, file_path, data):\n    blob = bucket.blob(file_path)\n    blob.upload_from_string(data)\n    return blob\n\n\ndef upload_file(bucket, base_path, file_path):\n    filename = os.path.basename(file_path)\n    path = \'{}/{}\'.format(base_path, filename)\n    click.echo(\'Uploading file: ""{}""\\n\\t-> to ""gs://{}/{}""\'.format(\n        filename, bucket.name, path))\n    blob = bucket.blob(path)\n    blob.upload_from_file(tf.gfile.GFile(file_path, \'rb\'))\n    return path\n\n\n@click.group(help=\'Train models in Google Cloud ML\')\ndef gc():\n    pass\n\n\n@gc.command(help=\'Start a training job\')\n@click.option(\'--job-id\', help=\'Identifies the training job in Google Cloud. Will use it to name the folder where checkpoints and logs will be stored, except when resuming a previous training job.\')  # noqa\n@click.option(\'--resume\', \'resume_job_id\', help=\'Id of the previous job to resume (start from last stored checkpoint). In case you are resuming multiple times, must always point to the first job (ie. the one that first created the checkpoint).\')  # noqa\n@click.option(\'--bucket\', \'bucket_name\', help=\'Bucket where to create the folder to save checkpoints and logs. If resuming a job, it must match the bucket used for the original job.\')  # noqa\n@click.option(\'--region\', default=\'us-central1\', help=\'Region in which to run the job.\')  # noqa\n@click.option(\'--dataset\', help=\'Complete path (bucket included) to the folder where the dataset is located (TFRecord files).\')  # noqa\n@click.option(\'config_files\', \'--config\', \'-c\', required=True, multiple=True, help=\'Path to config to use in training.\')  # noqa\n@click.option(\'--scale-tier\', default=DEFAULT_SCALE_TIER, type=click.Choice(SCALE_TIERS))  # noqa\n@click.option(\'--master-type\', default=DEFAULT_MASTER_TYPE, type=click.Choice(MACHINE_TYPES))  # noqa\n@click.option(\'--worker-type\', default=DEFAULT_WORKER_TYPE, type=click.Choice(MACHINE_TYPES))  # noqa\n@click.option(\'--worker-count\', default=DEFAULT_WORKER_COUNT, type=int)\n@click.option(\'--parameter-server-type\', default=DEFAULT_PS_TYPE, type=click.Choice(MACHINE_TYPES))  # noqa\n@click.option(\'--parameter-server-count\', default=DEFAULT_PS_COUNT, type=int)\n@check_dependencies\ndef train(job_id, resume_job_id, bucket_name, region, config_files, dataset,\n          scale_tier, master_type, worker_type, worker_count,\n          parameter_server_type, parameter_server_count):\n    account = ServiceAccount()\n    account.validate_region(region)\n\n    if bucket_name is None:\n        bucket_name = \'luminoth-{}\'.format(account.client_id)\n        click.echo(\n            \'Bucket name not specified. Using ""{}"".\'.format(bucket_name))\n\n    # Creates bucket for logs and models if it doesn\'t exist\n    bucket = account.get_bucket(bucket_name)\n\n    if not job_id:\n        job_id = \'train_{}\'.format(datetime.now().strftime(""%Y%m%d_%H%M%S""))\n\n    # Path in bucket to store job\'s config, logs, etc.\n    # If we are resuming a previous job, then we will use the same path\n    # that job used, so Luminoth will load the checkpoint from there.\n    base_path = \'lumi_{}\'.format(resume_job_id if resume_job_id else job_id)\n\n    package_path = build_package(bucket, base_path)\n    job_dir = \'gs://{}/{}\'.format(bucket_name, base_path)\n\n    override_params = [\n        \'train.job_dir={}\'.format(job_dir),\n    ]\n\n    if dataset:\n        # Check if absolute or relative dataset path\n        if not dataset.startswith(\'gs://\'):\n            dataset = \'gs://{}\'.format(dataset)\n        override_params.append(\'dataset.dir={}\'.format(dataset))\n\n    # Even if we are resuming job, we will use a new config. Thus, we will\n    # overwrite the config in the old job\'s dir if it existed.\n    config = get_config(config_files, override_params=override_params)\n\n    # Update final config file to job bucket\n    config_path = \'{}/{}\'.format(base_path, DEFAULT_CONFIG_FILENAME)\n    upload_data(bucket, config_path, dump_config(config))\n\n    args = [\'--config\', \'{}/{}\'.format(job_dir, DEFAULT_CONFIG_FILENAME)]\n\n    cloudml = account.cloud_service(\'ml\')\n\n    training_inputs = {\n        \'scaleTier\': scale_tier,\n        \'packageUris\': [\n            \'gs://{}/{}\'.format(bucket_name, package_path)\n        ],\n        \'pythonModule\': \'luminoth.train\',\n        \'args\': args,\n        \'region\': region,\n        \'jobDir\': job_dir,\n        \'runtimeVersion\': RUNTIME_VERSION,\n        \'pythonVersion\': PYTHON_VERSION,\n    }\n\n    if scale_tier == \'CUSTOM\':\n        training_inputs[\'masterType\'] = master_type\n        if worker_count > 0:\n            training_inputs[\'workerCount\'] = worker_count\n            training_inputs[\'workerType\'] = worker_type\n\n        if parameter_server_count > 0:\n            training_inputs[\'parameterServerCount\'] = parameter_server_count\n            training_inputs[\'parameterServerType\'] = parameter_server_type\n\n    job_spec = {\n        \'jobId\': job_id,\n        \'trainingInput\': training_inputs\n    }\n\n    jobrequest = cloudml.projects().jobs().create(\n        body=job_spec, parent=\'projects/{}\'.format(account.project_id))\n\n    try:\n        click.echo(\'Submitting training job.\')\n        res = jobrequest.execute()\n        click.echo(\'Job submitted successfully.\')\n        click.echo(\'state = {}, createTime = {}\'.format(\n            res.get(\'state\'), res.get(\'createTime\')))\n        if resume_job_id:\n            click.echo(\n                \'\\nNote: this job is resuming job {}.\\n\'.format(resume_job_id))\n        click.echo(\'Job id: {}\'.format(job_id))\n        click.echo(\'Job directory: {}\'.format(job_dir))\n\n        save_run(config, environment=\'gcloud\', extra_config=job_spec)\n\n    except Exception as err:\n        click.echo(\n            \'There was an error creating the training job. \'\n            \'Check the details: \\n{}\'.format(err._get_reason())\n        )\n\n\n@gc.command(help=\'Start a evaluation job\')\n@click.option(\'--job-id\', help=\'Job Id for naming the folder where the results of the evaluation will be stored.\')  # noqa\n@click.option(\'--train-folder\', \'train_folder\', required=True, help=\'Complete path (bucket included) where the training results are stored (config.yml should live here).\')  # noqa\n@click.option(\'--bucket\', \'bucket_name\', help=\'The bucket where the evaluation results were stored.\')  # noqa\n@click.option(\'dataset_split\', \'--split\', default=\'val\', help=\'Dataset split to use.\')  # noqa\n@click.option(\'--region\', default=\'us-central1\', help=\'Region in which to run the job.\')  # noqa\n@click.option(\'--machine-type\', default=DEFAULT_MASTER_TYPE, type=click.Choice(MACHINE_TYPES))  # noqa\n@click.option(\'--rebuild\', default=False, is_flag=True, help=\'Rebuild Luminoth package for evaluation. If not, will use the same package that was used for training.\')  # noqa\n@check_dependencies\ndef evaluate(job_id, train_folder, bucket_name, dataset_split, region,\n             machine_type, rebuild):\n    account = ServiceAccount()\n    account.validate_region(region)\n\n    if not train_folder.startswith(\'gs://\'):\n        train_folder = \'gs://{}\'.format(train_folder)\n\n    if not job_id:\n        job_id = \'eval_{}\'.format(datetime.now().strftime(""%Y%m%d_%H%M%S""))\n\n    if bucket_name is None:\n        bucket_name = \'luminoth-{}\'.format(account.client_id)\n        click.echo(\n            \'Bucket name not specified. Using ""{}"".\'.format(bucket_name),\n            err=True)\n\n    if rebuild:\n        job_folder = \'lumi_{}\'.format(job_id)\n\n        # Make new package in the bucket for eval results\n        bucket = account.get_bucket(bucket_name)\n        package_path = build_package(bucket, job_folder)\n        full_package_path = \'gs://{}/{}\'.format(bucket_name, package_path)\n    else:\n        # Get old training package from the training folder.\n        # There should only be one file ending in `.tar.gz`.\n        train_packages_dir = \'{}/{}\'.format(\n            train_folder, DEFAULT_PACKAGES_PATH\n        )\n\n        try:\n            package_files = tf.gfile.ListDirectory(train_packages_dir)\n            package_filename = [\n                n for n in package_files if n.endswith(\'tar.gz\')\n            ][0]\n            full_package_path = \'{}/{}\'.format(\n                train_packages_dir, package_filename)\n        except (IndexError, tf.errors.NotFoundError):\n            click.echo(\n                \'Could not find a `.tar.gz` Python package of Luminoth in \'\n                \'{}.\\n\\nCheck that the --train-folder parameter is \'\n                \'correct.\'.format(train_packages_dir),\n                err=True\n            )\n            sys.exit(1)\n\n    train_config_path = \'{}/{}\'.format(train_folder, DEFAULT_CONFIG_FILENAME)\n    cloudml = account.cloud_service(\'ml\')\n\n    args = [\n        [\'--config\', train_config_path],\n        [\'--split\', dataset_split],\n    ]\n\n    training_inputs = {\n        \'scaleTier\': \'CUSTOM\',\n        \'masterType\': machine_type,\n        \'packageUris\': [full_package_path],\n        \'pythonModule\': \'luminoth.eval\',\n        \'args\': args,\n        \'region\': region,\n        # Don\'t need to pass jobDir since it will read it from config file.\n        \'runtimeVersion\': RUNTIME_VERSION,\n    }\n\n    job_spec = {\n        \'jobId\': job_id,\n        \'trainingInput\': training_inputs\n    }\n\n    jobrequest = cloudml.projects().jobs().create(\n        body=job_spec, parent=\'projects/{}\'.format(account.project_id))\n\n    try:\n        click.echo(\'Submitting evaluation job.\')\n        res = jobrequest.execute()\n        click.echo(\'Job submitted successfully.\')\n        click.echo(\'state = {}, createTime = {}\\n\'.format(\n            res.get(\'state\'), res.get(\'createTime\')))\n        click.echo(\'Job id: {}\'.format(job_id))\n        click.echo(\'Job directory: {}\'.format(\'gs://{}\'.format(bucket_name)))\n\n    except Exception as err:\n        click.echo(\n            \'There was an error creating the evaluation job. \'\n            \'Check the details: \\n{}\'.format(err._get_reason()),\n            err=True\n        )\n\n\n@gc.command(help=\'List project jobs\')\n@click.option(\'--running\', is_flag=True, help=\'List only jobs that are running.\')  # noqa\n@check_dependencies\ndef jobs(running):\n    account = ServiceAccount()\n    cloudml = account.cloud_service(\'ml\')\n    request = cloudml.projects().jobs().list(\n        parent=\'projects/{}\'.format(account.project_id))\n\n    try:\n        response = request.execute()\n        jobs = response[\'jobs\']\n\n        if not jobs:\n            click.echo(\'There are no jobs for this project.\')\n            return\n\n        if running:\n            jobs = [j for j in jobs if j[\'state\'] == \'RUNNING\']\n            if not jobs:\n                click.echo(\'There are no running jobs.\')\n                return\n\n        for job in jobs:\n            click.echo(\'Id: {} Created: {} State: {}\'.format(\n                job[\'jobId\'], job[\'createTime\'], job[\'state\']))\n    except Exception as err:\n        click.echo(\n            \'There was an error fetching jobs. \'\n            \'Check the details: \\n{}\'.format(err._get_reason()),\n            err=True\n        )\n\n\n@gc.command(help=\'Show logs from a running job\')\n@click.option(\'--job-id\', required=True)\n@click.option(\'--polling-interval\', default=30, help=\'Polling interval in seconds.\')  # noqa\n@check_dependencies\ndef logs(job_id, polling_interval):\n    account = ServiceAccount()\n    cloudlog = account.cloud_service(\'logging\', \'v2\')\n\n    job_filter = \'resource.labels.job_id = ""{}""\'.format(job_id)\n    last_timestamp = None\n    while True:\n        filters = [job_filter]\n        if last_timestamp:\n            filters.append(\'timestamp > ""{}""\'.format(last_timestamp))\n\n        # Fetch all pages.\n        entries = []\n        next_page = None\n        while True:\n            request = cloudlog.entries().list(body={\n                \'resourceNames\': \'projects/{}\'.format(account.project_id),\n                \'filter\': \' AND \'.join(filters),\n                \'pageToken\': next_page,\n            })\n\n            try:\n                response = request.execute()\n                next_page = response.get(\'nextPageToken\', None)\n                entries.extend(response.get(\'entries\', []))\n                if not next_page:\n                    break\n            except Exception as err:\n                click.echo(\n                    \'There was an error fetching the logs. \'\n                    \'Check the details: \\n{}\'.format(err._get_reason()),\n                    err=True\n                )\n                break\n\n        for entry in entries:\n            last_timestamp = entry[\'timestamp\']\n\n            if \'jsonPayload\' in entry:\n                message = entry[\'jsonPayload\'][\'message\']\n            elif \'textPayload\' in entry:\n                message = entry[\'textPayload\']\n            else:\n                continue\n\n            click.echo(\'{:30} :: {:7} :: {}\'.format(\n                entry[\'timestamp\'], entry[\'severity\'], message.strip()\n            ))\n\n        time.sleep(polling_interval)\n'"
luminoth/tools/dataset/__init__.py,0,b'from .cli import dataset  # noqa\nfrom .readers import InvalidDataDirectory  # noqa\n'
luminoth/tools/dataset/cli.py,0,"b""import click\n\nfrom .merge import merge\nfrom .transform import transform\n\n\n@click.group(help='Groups of commands to manage datasets')\ndef dataset():\n    pass\n\n\ndataset.add_command(merge)\ndataset.add_command(transform)\n"""
luminoth/tools/dataset/merge.py,7,"b'import click\nimport tensorflow as tf\n\n\n@click.command()\n@click.argument(\'src\', nargs=-1)\n@click.argument(\'dst\', nargs=1)\n@click.option(\'--debug\', is_flag=True, help=\'Set level logging to DEBUG.\')\ndef merge(src, dst, debug):\n    """"""\n    Merges existing datasets into a single one.\n    """"""\n\n    if debug:\n        tf.logging.set_verbosity(tf.logging.DEBUG)\n    else:\n        tf.logging.set_verbosity(tf.logging.INFO)\n\n    tf.logging.info(\'Saving records to ""{}""\'.format(dst))\n    writer = tf.python_io.TFRecordWriter(dst)\n\n    total_records = 0\n\n    for src_file in src:\n        total_src_records = 0\n        for record in tf.python_io.tf_record_iterator(src_file):\n            writer.write(record)\n            total_src_records += 1\n            total_records += 1\n\n        tf.logging.info(\'Saved {} records from ""{}""\'.format(\n            total_src_records, src_file))\n\n    tf.logging.info(\'Saved {} to ""{}""\'.format(total_records, dst))\n\n    writer.close()\n'"
luminoth/tools/dataset/transform.py,6,"b'import click\nimport tensorflow as tf\n\nfrom luminoth.datasets.exceptions import InvalidDataDirectory\nfrom luminoth.utils.config import parse_override\nfrom .readers import get_reader, READERS\nfrom .writers import ObjectDetectionWriter\n\n\n@click.command()\n@click.option(\'dataset_reader\', \'--type\', type=click.Choice(READERS.keys()), required=True)  # noqa\n@click.option(\'--data-dir\', required=True, help=\'Where to locate the original data.\')  # noqa\n@click.option(\'--output-dir\', required=True, help=\'Where to save the transformed data.\')  # noqa\n@click.option(\'splits\', \'--split\', required=True, multiple=True, help=\'The splits to transform (ie. train, test, val).\')  # noqa\n@click.option(\'--only-classes\', help=\'Keep only examples of these classes. Comma separated list.\')  # noqa\n@click.option(\'--only-images\', help=\'Create dataset with specific examples. Useful to test model if your model has the ability to overfit.\')  # noqa\n@click.option(\'--limit-examples\', type=int, help=\'Limit the dataset to the first `N` examples.\')  # noqa\n@click.option(\'--class-examples\', type=int, help=\'Finish when every class has at least `N` number of samples. This will be the attempted lower bound; more examples might be added or a class might finish with fewer samples depending on the dataset.\')  # noqa\n@click.option(\'overrides\', \'--override\', \'-o\', multiple=True, help=\'Custom parameters for readers.\')  # noqa\n@click.option(\'--debug\', is_flag=True, help=\'Set level logging to DEBUG.\')\ndef transform(dataset_reader, data_dir, output_dir, splits, only_classes,\n              only_images, limit_examples, class_examples, overrides, debug):\n    """"""\n    Prepares dataset for ingestion.\n\n    Converts the dataset into different (one per split) TFRecords files.\n    """"""\n    tf.logging.set_verbosity(tf.logging.INFO)\n    if debug:\n        tf.logging.set_verbosity(tf.logging.DEBUG)\n\n    try:\n        reader = get_reader(dataset_reader)\n    except ValueError as e:\n        tf.logging.error(\'Error getting reader: {}\'.format(e))\n        return\n\n    # All splits must have a consistent set of classes.\n    classes = None\n\n    reader_kwargs = parse_override(overrides)\n\n    try:\n        for split in splits:\n            # Create instance of reader.\n            split_reader = reader(\n                data_dir, split,\n                only_classes=only_classes, only_images=only_images,\n                limit_examples=limit_examples, class_examples=class_examples,\n                **reader_kwargs\n            )\n\n            if classes is None:\n                # ""Save"" classes from the first split reader\n                classes = split_reader.classes\n            else:\n                # Overwrite classes after first split for consistency.\n                split_reader.classes = classes\n\n            # We assume we are saving object detection objects, but it should\n            # be easy to modify once we have different types of objects.\n            writer = ObjectDetectionWriter(split_reader, output_dir, split)\n            writer.save()\n\n            tf.logging.info(\'Composition per class ({}):\'.format(split))\n            for label, count in split_reader._per_class_counter.most_common():\n                tf.logging.info(\n                    \'\\t%s: %d\', split_reader.pretty_name(label), count\n                )\n\n    except InvalidDataDirectory as e:\n        tf.logging.error(\'Error reading dataset: {}\'.format(e))\n'"
luminoth/tools/server/__init__.py,0,b'from .cli import server  # noqa\n'
luminoth/tools/server/cli.py,0,"b""import click\n\nfrom .web import web\n\n\n@click.group(help='Groups of commands to serve models')\ndef server():\n    pass\n\n\nserver.add_command(web)\n"""
luminoth/tools/server/web.py,3,"b'import click\nimport tensorflow as tf\n\nfrom flask import Flask, jsonify, request, render_template\nfrom threading import Thread\nfrom PIL import Image\nfrom six.moves import _thread\n\nfrom luminoth.tools.checkpoint import get_checkpoint_config\nfrom luminoth.utils.config import get_config, override_config_params\nfrom luminoth.utils.predicting import PredictorNetwork\n\n\napp = Flask(__name__)\n\n\ndef get_image():\n    image = request.files.get(\'image\')\n    if not image:\n        raise ValueError\n\n    image = Image.open(image.stream).convert(\'RGB\')\n    return image\n\n\n@app.route(\'/\')\ndef index():\n    return render_template(\'index.html\')\n\n\n@app.route(\'/api/<model_name>/predict/\', methods=[\'GET\', \'POST\'])\ndef predict(model_name):\n    if request.method == \'GET\':\n        return jsonify(error=\'Use POST method to send image.\'), 400\n\n    try:\n        image_array = get_image()\n    except ValueError:\n        return jsonify(error=\'Missing image\'), 400\n    except OSError:\n        return jsonify(error=\'Incompatible file type\'), 400\n\n    total_predictions = request.args.get(\'total\')\n    if total_predictions is not None:\n        try:\n            total_predictions = int(total_predictions)\n        except ValueError:\n            total_predictions = None\n\n    # Wait for the model to finish loading.\n    NETWORK_START_THREAD.join()\n\n    objects = PREDICTOR_NETWORK.predict_image(image_array)\n    objects = objects[:total_predictions]\n\n    return jsonify({\'objects\': objects})\n\n\ndef start_network(config):\n    global PREDICTOR_NETWORK\n    try:\n        PREDICTOR_NETWORK = PredictorNetwork(config)\n    except Exception as e:\n        # An error occurred loading the model; interrupt the whole server.\n        tf.logging.error(e)\n        _thread.interrupt_main()\n\n\n@click.command(help=\'Start basic web application.\')\n@click.option(\'config_files\', \'--config\', \'-c\', multiple=True, help=\'Config to use.\')  # noqa\n@click.option(\'--checkpoint\', help=\'Checkpoint to use.\')\n@click.option(\'override_params\', \'--override\', \'-o\', multiple=True, help=\'Override model config params.\')  # noqa\n@click.option(\'--host\', default=\'127.0.0.1\', help=\'Hostname to listen on. Set this to ""0.0.0.0"" to have the server available externally.\')  # noqa\n@click.option(\'--port\', default=5000, help=\'Port to listen to.\')\n@click.option(\'--debug\', is_flag=True, help=\'Set debug level logging.\')\ndef web(config_files, checkpoint, override_params, host, port, debug):\n    if debug:\n        tf.logging.set_verbosity(tf.logging.DEBUG)\n    else:\n        tf.logging.set_verbosity(tf.logging.INFO)\n\n    if checkpoint:\n        config = get_checkpoint_config(checkpoint)\n    elif config_files:\n        config = get_config(config_files)\n    else:\n        click.echo(\n            \'Neither checkpoint not config specified, assuming `accurate`.\'\n        )\n        config = get_checkpoint_config(\'accurate\')\n\n    if override_params:\n        config = override_config_params(config, override_params)\n\n    # Bounding boxes will be filtered by frontend (using slider), so we set a\n    # low threshold.\n    if config.model.type == \'fasterrcnn\':\n        config.model.rcnn.proposals.min_prob_threshold = 0.01\n    elif config.model.type == \'ssd\':\n        config.model.proposals.min_prob_threshold = 0.01\n    else:\n        raise ValueError(\n            ""Model type \'{}\' not supported"".format(config.model.type)\n        )\n\n    # Initialize model\n    global NETWORK_START_THREAD\n    NETWORK_START_THREAD = Thread(target=start_network, args=(config,))\n    NETWORK_START_THREAD.start()\n\n    app.run(host=host, port=port, debug=debug)\n'"
luminoth/tools/server/web_test.py,3,"b'# import numpy as np\nimport tensorflow as tf\n\nfrom easydict import EasyDict\nfrom luminoth.models import get_model\nfrom luminoth.utils.config import get_base_config\n\n\nclass WebTest(tf.test.TestCase):\n    # TODO When the image size has big dimensions like (1024, 1024, 3),\n    # Travis fails during this test, probably ran out of memory. Using an build\n    # environment with more memory all works fine.\n    def setUp(self):\n        tf.reset_default_graph()\n        model_class = get_model(\'fasterrcnn\')\n        base_config = get_base_config(model_class)\n        image_resize = base_config.dataset.image_preprocessing\n        self.config = EasyDict({\n            \'image_resize_min\': image_resize.min_size,\n            \'image_resize_max\': image_resize.max_size\n        })\n\n    # # This test fails with Travis\' build environment\n    # def testWithoutResize(self):\n    #     """"""\n    #     Tests the FasterRCNN\'s predict without resize an image\n    #     """"""\n    #     # Does a prediction without resizing the image\n    #     image = Image.fromarray(\n    #         np.random.randint(\n    #             low=0, high=255,\n    #             size=(self.config.image_resize_min,\n    #                   self.config.image_resize_max, 3)\n    #         ).astype(np.uint8)\n    #     )\n\n    #     results = get_prediction(\'fasterrcnn\', image)\n\n    #     # Check that scale_factor and inference_time are corrects values\n    #     self.assertEqual(results[\'scale_factor\'], 1.0)\n    #     self.assertGreaterEqual(results[\'inference_time\'], 0)\n\n    #     # Check that objects, labels and probs aren\'t None\n    #     self.assertIsNotNone(results[\'objects\'])\n    #     self.assertIsNotNone(results[\'objects_labels\'])\n    #     self.assertIsNotNone(results[\'objects_labels_prob\'])\n\n    # This test fails with Travis\' build environment\n    # def testWithResize(self):\n    #     """"""\n    #     Tests the FasterRCNN\'s predict without resize an image\n    #     """"""\n    #     # Does a prediction resizing the image\n    #     image = Image.fromarray(\n    #         np.random.randint(\n    #             low=0, high=255,\n    #             size=(self.config.image_resize_min,\n    #                   self.config.image_resize_max + 1, 3)\n    #         ).astype(np.uint8)\n    #     )\n    #\n    #     results = get_prediction(\'fasterrcnn\', image)\n    #\n    #     # Check that scale_factor and inference_time are corrects values\n    #     self.assertNotEqual(1.0, results[\'scale_factor\'])\n    #     self.assertGreaterEqual(results[\'inference_time\'], 0)\n    #\n    #     # Check that objects, labels and probs aren\'t None\n    #     self.assertIsNotNone(results[\'objects\'])\n    #     self.assertIsNotNone(results[\'objects_labels\'])\n    #     self.assertIsNotNone(results[\'objects_labels_prob\'])\n\n\nif __name__ == \'__main__\':\n    tf.test.main()\n'"
luminoth/utils/hooks/__init__.py,0,b'from .image_vis_hook import ImageVisHook  # noqa\nfrom .var_vis_hook import VarVisHook  # noqa\n'
luminoth/utils/hooks/image_vis_hook.py,6,"b'import tensorflow as tf\n\nfrom tensorflow.python.training.summary_io import SummaryWriterCache\nfrom luminoth.utils.image_vis import image_vis_summaries\n\n\nclass ImageVisHook(tf.train.SessionRunHook):\n    def __init__(self, prediction_dict, image, config=None, gt_bboxes=None,\n                 every_n_steps=None, every_n_secs=None, output_dir=None,\n                 summary_writer=None, image_visualization_mode=None):\n        super(ImageVisHook, self).__init__()\n        if (every_n_secs is None) == (every_n_steps is None):\n            raise ValueError(\n                \'Only one of ""every_n_secs"" and ""every_n_steps"" must be \'\n                \'provided.\')\n        if output_dir is None and summary_writer is None:\n            tf.logging.warning(\n                \'ImageVisHook is not saving summaries. One of ""output_dir"" \'\n                \'and ""summary_writer"" must be provided\')\n        self._timer = tf.train.SecondOrStepTimer(\n            every_steps=every_n_steps, every_secs=every_n_secs)\n\n        self._prediction_dict = prediction_dict\n        self._config = config\n        self._output_dir = output_dir\n        self._summary_writer = summary_writer\n        self._image_visualization_mode = image_visualization_mode\n        self._image = image\n        self._gt_bboxes = gt_bboxes\n\n        tf.logging.info(\'ImageVisHook was created with mode = ""{}""\'.format(\n            image_visualization_mode\n        ))\n\n    def begin(self):\n        if self._summary_writer is None and self._output_dir:\n            self._summary_writer = SummaryWriterCache.get(self._output_dir)\n        self._next_step = None\n        self._global_step = tf.train.get_global_step()\n        if self._global_step is None:\n            raise RuntimeError(\'Global step must be created for ImageVisHook.\')\n\n    def before_run(self, run_context):\n\n        fetches = {\'global_step\': self._global_step}\n        self._draw_images = (\n            self._next_step is None or\n            self._timer.should_trigger_for_step(self._next_step)\n        )\n\n        if self._draw_images:\n            fetches[\'prediction_dict\'] = self._prediction_dict\n            fetches[\'image\'] = self._image\n            if self._gt_bboxes is not None:\n                fetches[\'gt_bboxes\'] = self._gt_bboxes\n\n        return tf.train.SessionRunArgs(fetches)\n\n    def after_run(self, run_context, run_values):\n        results = run_values.results\n        global_step = results.get(\'global_step\')\n\n        if self._draw_images:\n            self._timer.update_last_triggered_step(global_step)\n            prediction_dict = results.get(\'prediction_dict\')\n            if prediction_dict is not None:\n                summaries = image_vis_summaries(\n                    prediction_dict, config=self._config,\n                    image_visualization_mode=self._image_visualization_mode,\n                    image=results.get(\'image\'),\n                    gt_bboxes=results.get(\'gt_bboxes\')\n                )\n                if self._summary_writer is not None:\n                    for summary in summaries:\n                        self._summary_writer.add_summary(summary, global_step)\n\n        self._next_step = global_step + 1\n\n    def end(self, session=None):\n        if self._summary_writer:\n            self._summary_writer.flush()\n'"
luminoth/utils/hooks/var_vis_hook.py,6,"b'import tensorflow as tf\n\nfrom tensorflow.python.training.summary_io import SummaryWriterCache\n\n\nclass VarVisHook(tf.train.SessionRunHook):\n\n    def __init__(self, every_n_steps=None, every_n_secs=None, mode=None,\n                 output_dir=None, vars_summary=None):\n        super(VarVisHook, self).__init__()\n\n        if (every_n_secs is None) == (every_n_steps is None):\n            raise ValueError(\n                \'Only one of ""every_n_secs"" and ""every_n_steps"" must be \'\n                \'provided.\'\n            )\n\n        if output_dir is None:\n            tf.logging.warning(\n                \'`output_dir` not provided, VarVisHook is not saving \'\n                \'summaries.\'\n            )\n\n        self._timer = tf.train.SecondOrStepTimer(\n            every_steps=every_n_steps,\n            every_secs=every_n_secs\n        )\n\n        self._mode = mode\n        self._output_dir = output_dir\n        self._summary_writer = None\n        self._vars_summary = vars_summary\n\n        tf.logging.info(\'VarVisHook was created with mode = ""{}""\'.format(mode))\n\n    def begin(self):\n        if self._output_dir:\n            self._summary_writer = SummaryWriterCache.get(self._output_dir)\n\n        self._next_step = None\n        self._global_step = tf.train.get_global_step()\n        if self._global_step is None:\n            raise RuntimeError(\'Global step must be created for VarVisHook.\')\n\n    def before_run(self, run_context):\n        fetches = {\n            \'global_step\': self._global_step,\n        }\n\n        self._write_summaries = (\n            self._next_step is None or\n            self._timer.should_trigger_for_step(self._next_step)\n        )\n\n        if self._write_summaries:\n            fetches[\'summary\'] = self._vars_summary[self._mode]\n\n        return tf.train.SessionRunArgs(fetches)\n\n    def after_run(self, run_context, run_values):\n        results = run_values.results\n        global_step = results.get(\'global_step\')\n\n        if self._write_summaries:\n            self._timer.update_last_triggered_step(global_step)\n            summary = results.get(\'summary\')\n            if summary is not None:\n                if self._summary_writer is not None:\n                    self._summary_writer.add_summary(summary, global_step)\n\n        self._next_step = global_step + 1\n\n    def end(self, session=None):\n        if self._summary_writer:\n            self._summary_writer.flush()\n'"
luminoth/utils/test/__init__.py,0,b'from .gt_boxes import generate_gt_boxes  # noqa\nfrom .anchors import generate_anchors  # noqa\n'
luminoth/utils/test/anchors.py,0,"b'import numpy as np\n\n\ndef generate_anchors(anchors_reference, anchor_stride, feature_map_size):\n    """"""\n    Generate anchors from an anchor_reference using the anchor_stride for an\n    image with a feature map size of `feature_map_size`.\n\n    This code is based on the TensorFlow code for generating the same thing\n    on the computation graph.\n\n    Args:\n        anchors_reference (np.array): with shape (total_anchors, 4), the\n            relative distance between the center and the top left X,Y and\n            bottom right X, Y of the anchor.\n        anchor_stride (int): stride for generation of anchors.\n        feature_map_size (np.array): with shape (2,)\n\n    Returns:\n        anchors (np.array): array with anchors.\n            with shape (height_feature * width_feature * total_anchors, 4)\n\n    TODO: We should create a test for comparing this function vs the one\n          actually used in the computation graph.\n    """"""\n\n    grid_width = feature_map_size[1]\n    grid_height = feature_map_size[0]\n\n    shift_x = np.arange(grid_width) * anchor_stride\n    shift_y = np.arange(grid_height) * anchor_stride\n\n    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n\n    shift_x = shift_x.reshape([-1])\n    shift_y = shift_y.reshape([-1])\n\n    shifts = np.stack(\n        [shift_x, shift_y, shift_x, shift_y],\n        axis=0\n    )\n\n    shifts = shifts.T\n\n    num_anchors = anchors_reference.shape[0]\n    num_anchor_points = shifts.shape[0]\n\n    all_anchors = (\n        anchors_reference.reshape((1, num_anchors, 4)) +\n        np.transpose(\n            shifts.reshape((1, num_anchor_points, 4)),\n            axes=(1, 0, 2)\n        )\n    )\n\n    all_anchors = np.reshape(\n        all_anchors, (num_anchors * num_anchor_points, 4)\n    )\n\n    return all_anchors\n\n\nif __name__ == \'__main__\':\n    from luminoth.utils.anchors import generate_anchors_reference\n\n    ref = generate_anchors_reference(\n        base_size=16, ratios=[0.5, 1, 2], scales=2**np.arange(3, 6)\n    )\n'"
luminoth/utils/test/gt_boxes.py,0,"b'import numpy as np\n\n\ndef generate_gt_boxes(total_boxes, image_size, min_size=10,\n                      total_classes=None):\n    """"""\n    Generate `total_boxes` fake (but consistent) ground-truth boxes for an\n    image of size `image_size` (height, width).\n\n    Args:\n        total_boxes (int): The total number of boxes.\n        image_size (tuple): Size of the fake image.\n\n    Returns:\n        gt_boxes (np.array): With shape [total_boxes, 4].\n    """"""\n\n    image_size = np.array(image_size)\n\n    assert (image_size > min_size).all(), \\\n        \'Can\\\'t generate gt_boxes that small for that image size\'\n\n    # Generate random sizes for each boxes.\n    max_size = np.min(image_size) - min_size\n    random_sizes = np.random.randint(\n        low=min_size, high=max_size,\n        size=(total_boxes, 2)\n    )\n\n    # Generate random starting points for boundind boxes (left top point)\n    random_leftop = np.random.randint(\n        low=0, high=max_size, size=(total_boxes, 2)\n    )\n\n    rightbottom = np.minimum(\n        random_sizes + random_leftop,\n        np.array(image_size) - 1\n    )\n\n    gt_boxes = np.column_stack((random_leftop, rightbottom))\n\n    # TODO: Remove asserts after writing tests for this function.\n    assert (gt_boxes[:, 0] < gt_boxes[:, 2]).all(), \\\n        \'Gt boxes without consistent Xs\'\n    assert (gt_boxes[:, 1] < gt_boxes[:, 3]).all(), \\\n        \'Gt boxes without consistent Ys\'\n\n    if total_classes:\n        random_classes = np.random.randint(\n            low=0, high=total_classes - 1, size=(total_boxes, 1))\n        gt_boxes = np.column_stack((gt_boxes, random_classes))\n\n        assert (gt_boxes[:, 1] < total_classes).all(), \\\n            \'Gt boxes without consistent classes\'\n\n    return gt_boxes\n'"
luminoth/tools/dataset/readers/__init__.py,0,"b'from .base_reader import BaseReader, InvalidDataDirectory  # noqa\nfrom .object_detection import ObjectDetectionReader  # noqa\nfrom .object_detection import (\n    COCOReader, CSVReader, FlatReader, ImageNetReader, OpenImagesReader,\n    PascalVOCReader, TaggerineReader\n)\n\nREADERS = {\n    \'coco\': COCOReader,\n    \'csv\': CSVReader,\n    \'flat\': FlatReader,\n    \'imagenet\': ImageNetReader,\n    \'openimages\': OpenImagesReader,\n    \'pascal\': PascalVOCReader,\n    \'taggerine\': TaggerineReader,\n}\n\n\ndef get_reader(reader):\n    reader = reader.lower()\n    if reader not in READERS:\n        raise ValueError(\'""{}"" is not a valid reader\'.format(reader))\n\n    return READERS[reader]\n'"
luminoth/tools/dataset/readers/base_reader.py,0,"b'import abc\n\n\nclass InvalidDataDirectory(Exception):\n    """"""\n    Error raised when the chosen intput directory for the dataset is not valid.\n    """"""\n\n\nclass BaseReader(object):\n    """"""Base reader for reading different types of data\n    """"""\n    def __init__(self, **kwargs):\n        super(BaseReader, self).__init__()\n\n    @property\n    @abc.abstractproperty\n    def total(self):\n        """"""Returns the total amount of records in the dataset.\n        """"""\n\n    @abc.abstractmethod\n    def iterate(self):\n        """"""Iterates over the records in the dataset.\n        """"""\n'"
luminoth/tools/dataset/writers/__init__.py,0,b'from .base_writer import BaseWriter  # noqa\nfrom .object_detection_writer import ObjectDetectionWriter  # noqa'
luminoth/tools/dataset/writers/base_writer.py,0,"b'\nclass BaseWriter(object):\n    """"""BaseWriter for saving tfrecords.\n    """"""\n    def __init__(self):\n        super(BaseWriter, self).__init__()\n'"
luminoth/tools/dataset/writers/object_detection_writer.py,21,"b'import click\nimport json\nimport os\nimport tensorflow as tf\n\nfrom .base_writer import BaseWriter\n\nfrom luminoth.tools.dataset.readers import ObjectDetectionReader\nfrom luminoth.utils.dataset import to_int64, to_string, to_bytes\n\nREQUIRED_KEYS = set(\n    [\'width\', \'height\', \'depth\', \'filename\', \'image_raw\', \'gt_boxes\']\n)\nREQUIRED_GT_KEYS = set([\'label\', \'xmin\', \'ymin\', \'xmax\', \'ymax\'])\n\nCLASSES_FILENAME = \'classes.json\'\n\n\nclass InvalidRecord(Exception):\n    pass\n\n\nclass ObjectDetectionWriter(BaseWriter):\n    """"""Writes object detection dataset into tfrecords.\n\n    Reads dataset from a subclass of ObjectDetectionReader and saves it using\n    the default format for tfrecords.\n    """"""\n    def __init__(self, reader, output_dir, split=\'data\'):\n        """"""\n        Args:\n            reader:\n            output_dir: Directory to save the resulting tfrecords.\n            split: Split being save, which is used as a filename for the\n                resulting file.\n        """"""\n        super(ObjectDetectionWriter, self).__init__()\n        if not isinstance(reader, ObjectDetectionReader):\n            raise ValueError(\n                \'Saver needs a valid ObjectDetectionReader subclass\'\n            )\n\n        self._reader = reader\n        self._output_dir = output_dir\n        self._split = split\n\n    def save(self):\n        """"""\n        """"""\n        tf.logging.info(\'Saving split ""{}"" in output_dir = {}\'.format(\n            self._split, self._output_dir))\n        if not tf.gfile.Exists(self._output_dir):\n            tf.gfile.MakeDirs(self._output_dir)\n\n        # Save classes in simple json format for later use.\n        classes_file = os.path.join(self._output_dir, CLASSES_FILENAME)\n        json.dump([\n            self._reader.pretty_name(label)\n            for label in self._reader.classes\n        ], tf.gfile.GFile(classes_file, \'w\'))\n\n        record_file = os.path.join(\n            self._output_dir, \'{}.tfrecords\'.format(self._split))\n        writer = tf.python_io.TFRecordWriter(record_file)\n\n        tf.logging.debug(\'Found {} images.\'.format(self._reader.total))\n\n        with click.progressbar(self._reader.iterate(),\n                               length=self._reader.total) as record_list:\n            for record_idx, record in enumerate(record_list):\n                tf_record = self._record_to_tf(record)\n                if tf_record is not None:\n                    writer.write(tf_record.SerializeToString())\n\n            if self._output_dir.startswith(\'gs://\'):\n                tf.logging.info(\'Saving tfrecord to Google Cloud Storage. \'\n                                \'It may take a while.\')\n            writer.close()\n\n        if self._reader.yielded_records == 0:\n            tf.logging.error(\n                \'Data is missing. Removing record file. \'\n                \'(Use ""--debug"" flag to display all logs)\')\n            tf.gfile.Remove(record_file)\n            return\n        elif self._reader.errors > 0:\n            tf.logging.warning(\n                \'Failed on {} records. \'\n                \'(Use ""--debug"" flag to display all logs)\'.format(\n                    self._reader.errors, self._reader.yielded_records\n                )\n            )\n\n        tf.logging.info(\'Saved {} records to ""{}""\'.format(\n            self._reader.yielded_records, record_file))\n\n    def _validate_record(self, record):\n        """"""\n        Checks that the record is valid before saving it.\n\n        Args:\n            record: `dict`\n\n        Raises:\n            InvalidRecord when required keys are missing from the record.\n        """"""\n        record_keys = set(record.keys())\n        if record_keys != REQUIRED_KEYS:\n            raise InvalidRecord(\'Missing keys: {}\'.format(\n                REQUIRED_KEYS - record_keys\n            ))\n\n        if len(record[\'gt_boxes\']) == 0:\n            raise InvalidRecord(\'Record should have at least one `gt_boxes`\')\n\n        for gt_box in record[\'gt_boxes\']:\n            gt_keys = set(gt_box.keys())\n            if gt_keys != REQUIRED_GT_KEYS:\n                raise InvalidRecord(\'Missing gt boxes keys {}\'.format(\n                    REQUIRED_GT_KEYS - gt_keys\n                ))\n\n    def _record_to_tf(self, record):\n        """"""Creates tf.train.SequenceExample object from records.\n        """"""\n        try:\n            self._validate_record(record)\n        except InvalidRecord as e:\n            # Pop image before displaying record.\n            record.pop(\'image_raw\')\n            tf.logging.warning(\'Invalid record: {} - {}\'.format(\n                e, record\n            ))\n            return\n\n        sequence_vals = {\n            \'label\': [],\n            \'xmin\': [],\n            \'ymin\': [],\n            \'xmax\': [],\n            \'ymax\': [],\n        }\n\n        for b in record[\'gt_boxes\']:\n            sequence_vals[\'label\'].append(to_int64(b[\'label\']))\n            sequence_vals[\'xmin\'].append(to_int64(b[\'xmin\']))\n            sequence_vals[\'ymin\'].append(to_int64(b[\'ymin\']))\n            sequence_vals[\'xmax\'].append(to_int64(b[\'xmax\']))\n            sequence_vals[\'ymax\'].append(to_int64(b[\'ymax\']))\n\n        object_feature_lists = {\n            \'label\': tf.train.FeatureList(feature=sequence_vals[\'label\']),\n            \'xmin\': tf.train.FeatureList(feature=sequence_vals[\'xmin\']),\n            \'ymin\': tf.train.FeatureList(feature=sequence_vals[\'ymin\']),\n            \'xmax\': tf.train.FeatureList(feature=sequence_vals[\'xmax\']),\n            \'ymax\': tf.train.FeatureList(feature=sequence_vals[\'ymax\']),\n        }\n\n        object_features = tf.train.FeatureLists(\n            feature_list=object_feature_lists\n        )\n\n        feature = {\n            \'width\': to_int64(record[\'width\']),\n            \'height\': to_int64(record[\'height\']),\n            \'depth\': to_int64(record[\'depth\']),\n            \'filename\': to_string(record[\'filename\']),\n            \'image_raw\': to_bytes(record[\'image_raw\']),\n        }\n\n        # Now build an `Example` protobuf object and save with the writer.\n        context = tf.train.Features(feature=feature)\n        example = tf.train.SequenceExample(\n            feature_lists=object_features, context=context\n        )\n\n        return example\n'"
luminoth/tools/dataset/readers/object_detection/__init__.py,0,b'from .object_detection_reader import ObjectDetectionReader  # noqa\n\nfrom .coco import COCOReader  # noqa\nfrom .csv_reader import CSVReader  # noqa\nfrom .flat_reader import FlatReader  # noqa\nfrom .imagenet import ImageNetReader  # noqa\nfrom .openimages import OpenImagesReader  # noqa\nfrom .pascalvoc import PascalVOCReader  # noqa\nfrom .taggerine import TaggerineReader  # noqa\n'
luminoth/tools/dataset/readers/object_detection/coco.py,6,"b'import json\nimport os\nimport tensorflow as tf\n\nfrom luminoth.tools.dataset.readers import InvalidDataDirectory\nfrom luminoth.tools.dataset.readers.object_detection import (\n    ObjectDetectionReader\n)\nfrom luminoth.utils.dataset import read_image\n\n\nDEFAULT_YEAR = \'2017\'\n\n\nclass COCOReader(ObjectDetectionReader):\n    def __init__(self, data_dir, split, year=DEFAULT_YEAR,\n                 use_supercategory=False, **kwargs):\n        super(COCOReader, self).__init__(**kwargs)\n        self._data_dir = data_dir\n        self._split = split\n        self._year = year\n\n        try:\n            if self._split == \'train\':\n                tf.logging.debug(\'Loading annotation json (may take a while).\')\n\n            annotations_json = json.load(\n                tf.gfile.Open(self._get_annotations_path())\n            )\n        except tf.errors.NotFoundError:\n            raise InvalidDataDirectory(\n                \'Could not find COCO annotations in path\'\n            )\n\n        self._total_records = len(annotations_json[\'images\'])\n\n        category_to_name = {\n            c[\'id\']: (c[\'supercategory\'] if use_supercategory else c[\'name\'])\n            for c in annotations_json[\'categories\']\n        }\n\n        self._total_classes = sorted(set(category_to_name.values()))\n\n        self._image_to_bboxes = {}\n        for annotation in annotations_json[\'annotations\']:\n            image_id = annotation[\'image_id\']\n            x, y, width, height = annotation[\'bbox\']\n\n            # If the class is not in `classes`, it was filtered.\n            try:\n                annotation_class = self.classes.index(\n                    category_to_name[annotation[\'category_id\']]\n                )\n            except ValueError:\n                continue\n\n            self._image_to_bboxes.setdefault(image_id, []).append({\n                \'xmin\': x,\n                \'ymin\': y,\n                \'xmax\': x + width,\n                \'ymax\': y + height,\n                \'label\': annotation_class,\n            })\n\n        self._image_to_details = {}\n        for image in annotations_json[\'images\']:\n            self._image_to_details[image[\'id\']] = {\n                \'file_name\': image[\'file_name\'],\n                \'width\': image[\'width\'],\n                \'height\': image[\'height\'],\n            }\n\n        del annotations_json\n\n        self.yielded_records = 0\n        self.errors = 0\n\n    def get_total(self):\n        return self._total_records\n\n    def get_classes(self):\n        return self._total_classes\n\n    def iterate(self):\n        for image_id, image_details in self._image_to_details.items():\n\n            if self._stop_iteration():\n                return\n\n            filename = image_details[\'file_name\']\n            width = image_details[\'width\']\n            height = image_details[\'height\']\n\n            gt_boxes = self._image_to_bboxes.get(image_id, [])\n            if len(gt_boxes) == 0:\n                continue\n\n            if self._should_skip(image_id):\n                continue\n\n            # Read the image *after* checking whether any ground truth box is\n            # present.\n            try:\n                image_path = self._get_image_path(filename)\n                image = read_image(image_path)\n            except tf.errors.NotFoundError:\n                tf.logging.debug(\n                    \'Error reading image or annotation for ""{}"".\'.format(\n                        image_id))\n                self.errors += 1\n                continue\n\n            record = {\n                \'width\': width,\n                \'height\': height,\n                \'depth\': 3,\n                \'filename\': filename,\n                \'image_raw\': image,\n                \'gt_boxes\': gt_boxes,\n            }\n            self._will_add_record(record)\n            self.yielded_records += 1\n\n            yield record\n\n    def _get_annotations_path(self):\n        filename = \'instances_{}{}.json\'.format(self._split, self._year)\n        base_dir = os.path.join(self._data_dir, filename)\n        if tf.gfile.Exists(base_dir):\n            return base_dir\n\n        return os.path.join(self._data_dir, \'annotations\', filename)\n\n    def _get_image_path(self, image):\n        return os.path.join(\n            self._data_dir,\n            \'{}{}\'.format(self._split, self._year),\n            image\n        )\n'"
luminoth/tools/dataset/readers/object_detection/csv_reader.py,6,"b'# -*- coding: utf-8 -*-\nimport csv\nimport os\nimport six\nimport tensorflow as tf\n\nfrom PIL import Image\n\nfrom luminoth.tools.dataset.readers import InvalidDataDirectory\nfrom luminoth.tools.dataset.readers.object_detection import (\n    ObjectDetectionReader\n)\nfrom luminoth.utils.config import is_basestring\nfrom luminoth.utils.dataset import read_image\n\n\nclass CSVReader(ObjectDetectionReader):\n    """"""CSVReader supports reading annotations out of a CSV file.\n\n    The reader requires the following directory structure within `data_dir`:\n    * Annotation data (bounding boxes) per split, under the name `{split}.csv`\n      on the root directory.\n    * Dataset images per split, under the `{split}/` directory on the root\n      directory.\n\n    Thus, a CSV dataset directory structure may look as follows::\n\n        .\n        \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 train\n        \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 image_1.jpg\n        \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 image_2.jpg\n        \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 image_3.jpg\n        \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 val\n        \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 image_4.jpg\n        \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 image_5.jpg\n        \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 image_6.jpg\n        \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 train.csv\n        \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 val.csv\n\n    The CSV file itself must have the following format::\n\n        image_id,xmin,ymin,xmax,ymax,label\n        image_1.jpg,26,594,86,617,cat\n        image_1.jpg,599,528,612,541,car\n        image_2.jpg,393,477,430,552,dog\n\n    You can skip the header by overriding the `headers` parameter, in which\n    case the `columns` option will be used (specified as either a string or a\n    comma-separated list of fields). If this is done, the above six columns\n    *must* be present. Extra columns will be ignored.\n    """"""\n\n    DEFAULT_COLUMNS = [\'image_id\', \'xmin\', \'ymin\', \'xmax\', \'ymax\', \'label\']\n\n    def __init__(self, data_dir, split, headers=True, columns=None, **kwargs):\n        """"""Initializes the reader, allowing to override internal settings.\n\n        Arguments:\n            data_dir: Path to base directory where all the files are\n                located. See class docstring for a description on the expected\n                structure.\n            split: Split to read. Possible values depend on the dataset itself.\n            headers (boolean): Whether the CSV file has headers indicating\n                field names, in which case those will be considered.\n            columns (list or str): Column names for when `headers` is `False`\n                (i.e. the CSV file has no headers). Will be ignored if\n                `headers` is `True`.\n        """"""\n        super(CSVReader, self).__init__(**kwargs)\n\n        self._data_dir = data_dir\n        self._split = split\n\n        self._annotations_path = os.path.join(\n            self._data_dir, \'{}.csv\'.format(self._split)\n        )\n        if not tf.gfile.Exists(self._annotations_path):\n            raise InvalidDataDirectory(\n                \'CSV annotation file not found. Should be located at \'\n                \'`{}`\'.format(self._annotations_path)\n            )\n\n        self._images_dir = os.path.join(self._data_dir, self._split)\n        if not tf.gfile.Exists(self._images_dir):\n            raise InvalidDataDirectory(\n                \'Image directory not found. Should be located at \'\n                \'`{}`\'.format(self._images_dir)\n            )\n\n        if columns is not None:\n            if is_basestring(columns):\n                columns = columns.split(\',\')\n        else:\n            columns = self.DEFAULT_COLUMNS\n        self._columns = columns\n        self._column_names = set(self._columns)\n\n        self._has_headers = headers\n\n        # Cache for the records.\n        # TODO: Don\'t read it all upfront.\n        self._records = None\n\n        # Whether the structure of the CSV file has been checked already.\n        self._csv_checked = False\n\n        self.errors = 0\n        self.yielded_records = 0\n\n    def get_total(self):\n        return len(self._get_records())\n\n    def get_classes(self):\n        return sorted(set([\n            a[\'label\']\n            for annotations in self._get_records().values()\n            for a in annotations\n        ]))\n\n    def iterate(self):\n        records = self._get_records()\n        for image_id, annotations in records.items():\n            if self._stop_iteration():\n                return\n\n            if self._should_skip(image_id):\n                continue\n\n            image_path = os.path.join(self._images_dir, image_id)\n            try:\n                image = read_image(image_path)\n            except tf.errors.NotFoundError:\n                tf.logging.warning(\n                    \'Image `{}` at `{}` couldn\\\'t be opened.\'.format(\n                        image_id, image_path\n                    )\n                )\n                self.errors += 1\n                continue\n\n            image_pil = Image.open(six.BytesIO(image))\n            width = image_pil.width\n            height = image_pil.height\n\n            gt_boxes = []\n            for annotation in annotations:\n                try:\n                    label_id = self.classes.index(annotation[\'label\'])\n                except ValueError:\n                    tf.logging.warning(\n                        \'Error finding id for image `{}`, label `{}`.\'.format(\n                            image_id, annotation[\'label\']\n                        )\n                    )\n                    continue\n\n                gt_boxes.append({\n                    \'label\': label_id,\n                    \'xmin\': annotation[\'xmin\'],\n                    \'ymin\': annotation[\'ymin\'],\n                    \'xmax\': annotation[\'xmax\'],\n                    \'ymax\': annotation[\'ymax\'],\n                })\n\n            if len(gt_boxes) == 0:\n                continue\n\n            record = {\n                \'width\': width,\n                \'height\': height,\n                \'depth\': 3,\n                \'filename\': image_id,\n                \'image_raw\': image,\n                \'gt_boxes\': gt_boxes,\n            }\n            self._will_add_record(record)\n            self.yielded_records += 1\n\n            yield record\n\n    def _get_records(self):\n        """"""Read all the records out of the CSV file.\n\n        If they\'ve been previously read, just return the records.\n\n        Returns:\n            Dictionary mapping `image_id`s to a list of annotations.\n        """"""\n        if self._records is None:\n            with tf.gfile.Open(self._annotations_path) as annotations:\n                if self._has_headers:\n                    reader = csv.DictReader(annotations)\n                else:\n                    # If file has no headers, pass the field names to the CSV\n                    # reader.\n                    reader = csv.DictReader(\n                        annotations, fieldnames=self._columns\n                    )\n\n            images_gt_boxes = {}\n\n            for row in reader:\n                # When reading the first row, make sure the CSV is correct.\n                self._check_csv(row)\n\n                # Then proceed as normal, reading each row and aggregating\n                # bounding boxes by image.\n                label = self._normalize_row(row)\n                image_id = label.pop(\'image_id\')\n                images_gt_boxes.setdefault(image_id, []).append(label)\n\n            self._records = images_gt_boxes\n\n        return self._records\n\n    def _check_csv(self, row):\n        """"""Checks whether the CSV has all the necessary columns.\n\n        The actual check is done on the first row only, once the CSV has been\n        finally opened and read.\n        """"""\n        if not self._csv_checked:\n            missing_keys = self._column_names - set(row.keys())\n            if missing_keys:\n                raise InvalidDataDirectory(\n                    \'Columns missing from CSV: {}\'.format(missing_keys)\n                )\n            self._csv_checked = True\n\n    def _normalize_row(self, row):\n        """"""Normalizes a row from the CSV file by removing extra keys.""""""\n        return {\n            key: value for key, value in row.items()\n            if key in self._column_names\n        }\n'"
luminoth/tools/dataset/readers/object_detection/flat_reader.py,8,"b'import json\nimport os\nimport six\nimport tensorflow as tf\n\nfrom PIL import Image\n\nfrom luminoth.tools.dataset.readers import InvalidDataDirectory\nfrom luminoth.tools.dataset.readers.object_detection import (\n    ObjectDetectionReader\n)\nfrom luminoth.utils.dataset import read_image\n\nDEFAULT_ANNOTATION_TYPE = \'json\'\nDEFAULT_CLASS = 0\nDEFAULT_OBJECTS_KEY = \'rects\'\nX_MIN_KEY = \'x1\'\nY_MIN_KEY = \'y1\'\nX_MAX_KEY = \'x2\'\nY_MAX_KEY = \'y2\'\n\n\nclass FlatReader(ObjectDetectionReader):\n    def __init__(self, data_dir, split,\n                 annotation_type=DEFAULT_ANNOTATION_TYPE,\n                 default_class=DEFAULT_CLASS, objects_key=DEFAULT_OBJECTS_KEY,\n                 x_min_key=X_MIN_KEY, y_min_key=Y_MIN_KEY, x_max_key=X_MAX_KEY,\n                 y_max_key=Y_MAX_KEY, **kwargs):\n        super(FlatReader, self).__init__(**kwargs)\n        self._data_dir = data_dir\n        self._split = split\n        self._annotation_type = annotation_type\n        self._default_class = default_class\n        self._objects_key = objects_key\n        self._x_min_key = x_min_key\n        self._y_min_key = y_min_key\n        self._x_max_key = x_max_key\n        self._y_max_key = y_max_key\n\n        self._annotated_files = None\n        self._annotations = None\n\n        self.errors = 0\n        self.yielded_records = 0\n\n    def get_total(self):\n        return len(self.annotated_files)\n\n    def get_classes(self):\n        return sorted(set([\n            b.get(\'label\', self._default_class)\n            for r in self.annotations\n            for b in r.get(self._objects_key, [])\n        ]))\n\n    @property\n    def annotated_files(self):\n        if self._annotated_files is None:\n            split_path = self._get_split_path()\n            try:\n                all_files = tf.gfile.ListDirectory(split_path)\n            except tf.errors.NotFoundError:\n                raise InvalidDataDirectory(\n                    \'Directory for split ""{}"" does not exist\'.format(\n                        self._split))\n\n            self._annotated_files = []\n            for filename in all_files:\n                if filename.endswith(\'.{}\'.format(self._annotation_type)):\n                    self._annotated_files.append(\n                        filename[:-(len(self._annotation_type) + 1)]\n                    )\n            if len(self._annotated_files) == 0:\n                raise InvalidDataDirectory(\n                    \'Could not find any annotations in {}\'.format(split_path))\n\n        return self._annotated_files\n\n    def iterate(self):\n        for annotation in self.annotations:\n            if self._stop_iteration():\n                return\n\n            image_id = annotation[\'image_id\']\n\n            if self._should_skip(image_id):\n                continue\n\n            try:\n                image_path = self._get_image_path(image_id)\n                image = read_image(image_path)\n            except tf.errors.NotFoundError:\n                tf.logging.debug(\n                    \'Error reading image or annotation for ""{}"".\'.format(\n                        image_id))\n                self.errors += 1\n                continue\n\n            image_pil = Image.open(six.BytesIO(image))\n            width = image_pil.width\n            height = image_pil.height\n\n            gt_boxes = []\n            for b in annotation[self._objects_key]:\n                try:\n                    label_id = self.classes.index(\n                        b.get(\'label\', self._default_class)\n                    )\n                except ValueError:\n                    continue\n\n                gt_boxes.append({\n                    \'label\': label_id,\n                    \'xmin\': b[self._x_min_key],\n                    \'ymin\': b[self._y_min_key],\n                    \'xmax\': b[self._x_max_key],\n                    \'ymax\': b[self._y_max_key],\n                })\n\n            if len(gt_boxes) == 0:\n                tf.logging.debug(\'Image ""{}"" has zero valid gt_boxes.\'.format(\n                    image_id))\n                self.errors += 1\n                continue\n\n            record = {\n                \'width\': width,\n                \'height\': height,\n                \'depth\': 3,\n                \'filename\': image_id,\n                \'image_raw\': image,\n                \'gt_boxes\': gt_boxes,\n            }\n            self._will_add_record(record)\n            self.yielded_records += 1\n\n            yield record\n\n    @property\n    def annotations(self):\n        if self._annotations is None:\n            self._annotations = []\n            for annotation_id in self.annotated_files:\n                annotation = self._read_annotation(\n                    self._get_annotation_path(annotation_id)\n                )\n                annotation[\'image_id\'] = annotation_id\n                self._annotations.append(annotation)\n        return self._annotations\n\n    def _get_annotation_path(self, annotation_id):\n        return os.path.join(\n            self._get_split_path(),\n            \'{}.{}\'.format(annotation_id, self._annotation_type)\n        )\n\n    def _get_split_path(self):\n        return os.path.join(self._data_dir, self._split)\n\n    def _get_image_path(self, image_id):\n        default_path = os.path.join(self._get_split_path(), image_id)\n        if tf.gfile.Exists(default_path):\n            return default_path\n\n        # Lets assume it doesn\'t have extension\n        possible_files = [\n            f for f in self.files if f.startswith(\'{}.\'.format(image_id))\n        ]\n        if len(possible_files) == 0:\n            return\n        elif len(possible_files) > 1:\n            tf.logging.warning(\n                \'Image {} matches with {} files ({}).\'.format(\n                    image_id, len(possible_files)))\n\n        return os.path.join(self._get_split_path(), possible_files[0])\n\n    def _read_annotation(self, annotation_path):\n        if self._annotation_type == \'json\':\n            with tf.gfile.Open(annotation_path) as annotation_file:\n                return json.load(annotation_file)\n        else:\n            raise ValueError(\n                \'Annotation type {} not supported\'.format(\n                    self._annotation_type))\n'"
luminoth/tools/dataset/readers/object_detection/imagenet.py,9,"b'import json\nimport os\nimport six\nimport tensorflow as tf\n\nfrom PIL import Image\n\nfrom luminoth.tools.dataset.readers import InvalidDataDirectory\nfrom luminoth.tools.dataset.readers.object_detection import (\n    ObjectDetectionReader\n)\nfrom luminoth.utils.dataset import read_xml, read_image\n\nWNIDS_FILE = \'data/imagenet_wnids.json\'\n\n\nclass ImageNetReader(ObjectDetectionReader):\n    def __init__(self, data_dir, split, **kwargs):\n        super(ImageNetReader, self).__init__(**kwargs)\n        self._split = split\n        self._data_dir = data_dir\n        self._imagesets_path = os.path.join(self._data_dir, \'ImageSets\', \'DET\')\n        self._images_path = os.path.join(self._data_dir, \'Data\', \'DET\',)\n        self._annotations_path = os.path.join(\n            self._data_dir, \'Annotations\', \'DET\'\n        )\n\n        self.yielded_records = 0\n        self.errors = 0\n\n        # Validate Imagenet structure in `data_dir`.\n        self._validate_structure()\n\n        # Load wnids from file.\n        wnids_path = os.path.join(\n            os.path.dirname(os.path.realpath(__file__)),\n            WNIDS_FILE\n        )\n        with tf.gfile.GFile(wnids_path) as wnidsjson:\n            self._wnids = json.load(wnidsjson)\n\n    def get_total(self):\n        return sum(1 for _ in self._get_record_names())\n\n    def get_classes(self):\n        return sorted(list(self._wnids.values()))\n\n    def iterate(self):\n        for image_id in self._get_record_names():\n            if self._stop_iteration():\n                return\n\n            if self._should_skip(image_id):\n                continue\n\n            try:\n                annotation_path = self._get_image_annotation(image_id)\n                image_path = self._get_image_path(image_id)\n                # Read both the image and the annotation into memory.\n                annotation = read_xml(annotation_path)\n                image = read_image(image_path)\n            except tf.errors.NotFoundError:\n                tf.logging.debug(\n                    \'Error reading image or annotation for ""{}"".\'.format(\n                        image_id))\n                self.errors += 1\n                continue\n\n            objects = annotation.get(\'object\')\n            if objects is None:\n                # If there\'s no bounding boxes, we don\'t want it.\n                continue\n\n            image_pil = Image.open(six.BytesIO(image))\n            width = image_pil.width\n            height = image_pil.height\n\n            gt_boxes = []\n            for b in annotation[\'object\']:\n                try:\n                    label_id = self.classes.index(self._wnids[b[\'name\']])\n                except ValueError:\n                    continue\n\n                (xmin, ymin, xmax, ymax) = self._adjust_bbox(\n                    xmin=int(b[\'bndbox\'][\'xmin\']),\n                    ymin=int(b[\'bndbox\'][\'ymin\']),\n                    xmax=int(b[\'bndbox\'][\'xmax\']),\n                    ymax=int(b[\'bndbox\'][\'ymax\']),\n                    old_width=int(annotation[\'size\'][\'width\']),\n                    old_height=int(annotation[\'size\'][\'height\']),\n                    new_width=width, new_height=height\n                )\n\n                gt_boxes.append({\n                    \'label\': label_id,\n                    \'xmin\': xmin,\n                    \'ymin\': ymin,\n                    \'xmax\': xmax,\n                    \'ymax\': ymax,\n                })\n\n            if len(gt_boxes) == 0:\n                continue\n\n            record = {\n                \'width\': width,\n                \'height\': height,\n                \'depth\': 3,\n                \'filename\': annotation[\'filename\'],\n                \'image_raw\': image,\n                \'gt_boxes\': gt_boxes,\n            }\n\n            self._will_add_record(record)\n            self.yielded_records += 1\n\n            yield record\n\n    def _validate_structure(self):\n        if not tf.gfile.Exists(self._data_dir):\n            raise InvalidDataDirectory(\n                \'""{}"" does not exist.\'.format(self._data_dir)\n            )\n\n        if not tf.gfile.Exists(self._imagesets_path):\n            raise InvalidDataDirectory(\'ImageSets path is missing\')\n\n        if not tf.gfile.Exists(self._images_path):\n            raise InvalidDataDirectory(\'Images path is missing\')\n\n        if not tf.gfile.Exists(self._annotations_path):\n            raise InvalidDataDirectory(\'Annotations path is missing\')\n\n    def _get_split_path(self):\n        return os.path.join(\n            self._imagesets_path, \'{}.txt\'.format(self._split)\n        )\n\n    def _get_image_path(self, image_id):\n        return os.path.join(\n            self._images_path, \'{}.JPEG\'.format(image_id)\n        )\n\n    def _get_image_annotation(self, image_id):\n        return os.path.join(self._annotations_path, \'{}.xml\'.format(image_id))\n\n    def _get_record_names(self):\n        split_path = self._get_split_path()\n\n        if not tf.gfile.Exists(split_path):\n            raise ValueError(\'""{}"" not found\'.format(self._split))\n\n        with tf.gfile.GFile(split_path) as f:\n            for line in f:\n                # The images in \'extra\' directories don\'t have annotations.\n                if \'extra\' in line:\n                    continue\n                filename = line.split()[0]\n                filename = os.path.join(self._split, filename)\n                yield filename.strip()\n\n    def _adjust_bbox(self, xmin, ymin, xmax, ymax, old_width, old_height,\n                     new_width, new_height):\n        # TODO: consider reusing luminoth.utils.image.adjust_bboxes instead of\n        # this, but note it uses tensorflow, and using tf and np here may\n        # introduce too many problems.\n        xmin = (xmin / old_width) * new_width\n        ymin = (ymin / old_height) * new_height\n        xmax = (xmax / old_width) * new_width\n        ymax = (ymax / old_height) * new_height\n\n        return xmin, ymin, xmax, ymax\n'"
luminoth/tools/dataset/readers/object_detection/object_detection_reader.py,1,"b'import abc\nimport six\nimport tensorflow as tf\n\nfrom collections import Counter\n\nfrom luminoth.tools.dataset.readers import BaseReader\n\n\nclass ObjectDetectionReader(BaseReader):\n    """"""\n    Reads data suitable for object detection.\n\n    The object detection task needs the following information:\n        - images.\n        - ground truth, rectangular bounding boxes with associated labels.\n\n    For implementing a subclass of object detection, one needs to implement\n    the following methods:\n        - __init__\n        - get_total\n        - get_classes\n        - iterate\n\n    Optionally:\n        - pretty_name\n\n    Additionally, must use the `_per_class_counter` variable to honor the max\n    number of examples per class in an efficient way.\n    """"""\n    def __init__(self, only_classes=None, only_images=None,\n                 limit_examples=None, class_examples=None, **kwargs):\n        """"""\n        Args:\n            - only_classes: string or list of strings used as a class\n                whitelist.\n            - only_images: string or list of strings used as a image_id\n                whitelist.\n            - limit_examples: max number of examples (images) to use.\n            - class_examples: finish when every class has this approximate\n                number of examples.\n        """"""\n        super(ObjectDetectionReader, self).__init__()\n        if isinstance(only_classes, six.string_types):\n            # We can get one class as string.\n            only_classes = only_classes.split(\',\')\n        self._only_classes = only_classes\n\n        if isinstance(only_images, six.string_types):\n            # We can get one class as string.\n            only_images = only_images.split(\',\')\n        self._only_images = only_images\n\n        self._total = None\n        self._classes = None\n\n        self._limit_examples = limit_examples\n        self._class_examples = class_examples\n        self._per_class_counter = Counter()\n        self._maxed_out_classes = set()\n\n    @property\n    def total(self):\n        if self._total is None:\n            self._total = self._filter_total(self.get_total())\n        return self._total\n\n    @property\n    def classes(self):\n        if self._classes is None:\n            self._classes = self.get_classes()\n        return self._classes\n\n    @classes.setter\n    def classes(self, classes):\n        self._classes = classes\n\n    @abc.abstractmethod\n    def get_total(self):\n        """"""\n        Returns the total number of records in the dataset.\n        """"""\n\n    @abc.abstractmethod\n    def get_classes(self):\n        """"""\n        Returns a list of all the classes available in the dataset.\n        """"""\n\n    def pretty_name(self, label):\n        """"""\n        Return the ""pretty"" name for easy human identification of a given\n        label.\n        """"""\n        return label\n\n    def _filter_total(self, original_total_records):\n        """"""\n        Filters total number of records we will need to iterate over in dataset\n        based on reader options used.\n        """"""\n        if self._only_images:  # not None and not empty\n            return len(self._only_images)\n\n        if self._limit_examples is not None and self._limit_examples > 0:\n            return min(self._limit_examples, original_total_records)\n\n        # With _class_examples we potentially have to iterate over every\n        # record, so we don\'t know the total ahead of time.\n\n        return original_total_records\n\n    def _filter_classes(self, original_classes):\n        """"""\n        Filters classes based on reader options used.\n        """"""\n        if self._only_classes:  # not None and not empty\n            new_classes = sorted(self._only_classes)\n        else:\n            new_classes = list(original_classes) if original_classes else None\n\n        return new_classes\n\n    def _should_skip(self, image_id):\n        """"""\n        Determine if we should skip the current image, based on the options\n        used for the reader.\n\n        Args:\n            - image_id: String with id of image.\n\n        Returns:\n            bool: True if record should be skipped, False if not.\n        """"""\n        if self._only_images is not None and image_id is not None:\n            # Skip because the current image_id was not asked for.\n            if image_id not in self._only_images:\n                return True\n\n        return False\n\n    def _all_maxed_out(self):\n        # Every class is maxed out\n        if self._class_examples is not None:\n            return len(self._maxed_out_classes) == len(self.classes)\n\n        return False\n\n    def _stop_iteration(self):\n        if self.yielded_records == self.total:\n            return True\n\n        if self._all_maxed_out():\n            return True\n\n        return False\n\n    def _will_add_record(self, record):\n        """"""\n        Called whenever a new record is to be added.\n        """"""\n        # Adjust per-class counter from totals from current record\n        for box in record[\'gt_boxes\']:\n            self._per_class_counter[self.classes[box[\'label\']]] += 1\n\n        if self._class_examples is not None:\n            # Check which classes we have maxed out.\n            old_maxed_out = self._maxed_out_classes.copy()\n\n            self._maxed_out_classes |= set([\n                label\n                for label, count in self._per_class_counter.items()\n                if count >= self._class_examples\n            ])\n\n            for label in self._maxed_out_classes - old_maxed_out:\n                tf.logging.debug(\'Maxed out ""{}"" at {} examples\'.format(\n                    self.pretty_name(label),\n                    self._per_class_counter[label]\n                ))\n\n    @abc.abstractmethod\n    def iterate(self):\n        """"""\n        Iterate over object detection records read from the dataset source.\n\n        Returns:\n            iterator of records of type `dict` with the following keys:\n                width (int): Image width.\n                height (int): Image height.\n                depth (int): Number of channels in the image.\n                filename (str): Filename (or image id)\n                image_raw (bytes): Read image as bytes\n                gt_boxes (list of dicts):\n                    label (int): Label index over the possible classes.\n                    xmin (int): x value for top-left point.\n                    ymin (int): y value for top-left point.\n                    xmax (int): x value for bottom-right point.\n                    ymax (int): y value for bottom-right point.\n        """"""\n'"
luminoth/tools/dataset/readers/object_detection/openimages.py,13,"b'import csv\nimport os\nimport signal\nimport six\nimport sys\nimport tensorflow as tf\nimport threading\n\nfrom six.moves import queue\nfrom PIL import Image\n\nfrom luminoth.tools.dataset.readers import InvalidDataDirectory\nfrom luminoth.tools.dataset.readers.object_detection import (\n    ObjectDetectionReader\n)\nfrom luminoth.utils.dataset import read_image\n\n# Compatible with OpenImages V4\n# Files available at: https://storage.googleapis.com/openimages/web/index.html\nCLASSES_TRAINABLE = \'{split}-annotations-human-imagelabels-boxable.csv\'\nANNOTATIONS_FILENAME = \'{split}-annotations-bbox.csv\'\nCLASSES_DESC = \'class-descriptions-boxable.csv\'\nIMAGES_LOCATION = \'s3://open-images-dataset\'\n\n\nclass OpenImagesReader(ObjectDetectionReader):\n    """"""OpenImagesReader supports reading the images directly from the original\n    data source which is hosted in Google Cloud Storage.\n\n    Before using it you have to request and configure access following the\n    instructions here: https://github.com/cvdfoundation/open-images-dataset\n    """"""\n    def __init__(self, data_dir, split, download_threads=25, **kwargs):\n        """"""\n        Args:\n            data_dir: Path to base directory where to find all the necessary\n                files and folders.\n            split: Split to use, it is used for reading the appropiate\n                annotations.\n            download_threads: Number of threads to use for downloading\n                images.\n            only_classes: String with classes ids to be used as filter for\n                all the available classes. If the string contains \',\' it will\n                split the string using them.\n        """"""\n        super(OpenImagesReader, self).__init__(**kwargs)\n        self._data_dir = data_dir\n        self._split = split\n        self._download_threads = download_threads\n\n        self._image_ids = None\n        self.desc_by_label = {}\n\n        self.yielded_records = 0\n        self.errors = 0\n        self._total_queued = 0\n        # Flag to notify threads if the execution is halted.\n        self._alive = True\n\n    def _get_classes_path(self):\n        """"""\n        Return the full path to the CLASSES_TRAINABLE for the current split\n        in the data directory.\n\n        We expect this file to be located in a directory corresponding to the\n        split, ie. ""train"", ""validation"", ""test"".\n        """"""\n        return os.path.join(\n            self._data_dir, self._split, CLASSES_TRAINABLE\n        ).format(split=self._split)\n\n    def _get_annotations_path(self):\n        """"""\n        Return the full path to the ANNOTATIONS_FILENAME for the current split\n        in the data directory.\n\n        We expect this file to be located in a directory corresponding to the\n        split, ie. ""train"", ""validation"", ""test"".\n        """"""\n        return os.path.join(\n            self._data_dir, self._split, ANNOTATIONS_FILENAME\n        ).format(split=self._split)\n\n    def _get_image_path(self, image_id):\n        return os.path.join(\n            IMAGES_LOCATION, self._split, \'{}.jpg\'.format(image_id)\n        ).format(split=self._split)\n\n    def get_classes(self):\n        trainable_labels_file = self._get_classes_path()\n        trainable_labels = set()\n        try:\n            with tf.gfile.Open(trainable_labels_file) as tl:\n                reader = csv.reader(tl)\n                # Skip header\n                next(reader, None)\n                for line in reader:\n                    trainable_labels.add(line[2])\n        except tf.errors.NotFoundError:\n            raise InvalidDataDirectory(\n                \'The label file ""{}"" must be in the root data \'\n                \'directory: {}\'.format(\n                    os.path.split(trainable_labels_file)[1], self._data_dir\n                )\n            )\n\n        self.trainable_labels = self._filter_classes(trainable_labels)\n\n        # Build the map from classes to description for pretty printing their\n        # names.\n        labels_descriptions_file = os.path.join(self._data_dir, CLASSES_DESC)\n        try:\n            with tf.gfile.Open(labels_descriptions_file) as ld:\n                reader = csv.reader(ld)\n                for line in reader:\n                    if line[0] in self.trainable_labels:\n                        self.desc_by_label[line[0]] = line[1]\n        except tf.errors.NotFoundError:\n            raise InvalidDataDirectory(\n                \'Missing label description file ""{}"" from root data \'\n                \'directory: {}\'.format(CLASSES_DESC, self._data_dir)\n            )\n\n        return self.trainable_labels\n\n    def pretty_name(self, label):\n        return \'{} ({})\'.format(self.desc_by_label[label], label)\n\n    def get_total(self):\n        return len(self.image_ids)\n\n    @property\n    def image_ids(self):\n        if self._image_ids is None:\n            annotations_file = self._get_annotations_path()\n            with tf.gfile.Open(annotations_file) as af:\n                reader = csv.DictReader(af)\n                image_ids = set()\n                for l in reader:\n                    image_ids.add(l[\'ImageID\'])\n            self._image_ids = image_ids\n        return self._image_ids\n\n    def _queue_record(self, queue, record):\n        if not record[\'gt_boxes\']:\n            tf.logging.debug(\n                \'Dropping record {} without gt_boxes.\'.format(record))\n            return\n\n        # If asking for a limited number per class, only yield if the current\n        # example adds at least 1 new class that hasn\'t been maxed out. For\n        # example, if ""Persons"" has been maxed out but ""Bus"" has not, a new\n        # image containing only instances of ""Person"" will not be yielded,\n        # while an image containing both ""Person"" and ""Bus"" instances will.\n        if self._class_examples:\n            labels_in_image = set([\n                self.classes[bbox[\'label\']] for bbox in record[\'gt_boxes\']\n            ])\n            not_maxed_out = labels_in_image - self._maxed_out_classes\n\n            if not not_maxed_out:\n                tf.logging.debug(\n                    \'Dropping record {} with maxed-out labels: {}\'.format(\n                        record[\'filename\'], labels_in_image))\n                return\n\n            tf.logging.debug(\n                \'Queuing record {} with labels: {}\'.format(\n                    record[\'filename\'], labels_in_image))\n\n        self._will_add_record(record)\n        queue.put(record)\n\n    def _queue_partial_records(self, partial_records_queue, records_queue):\n        """"""\n        Read annotations from file and queue them.\n\n        Annotations are stored in a CSV file where each line has one\n        annotation. Since images can have multiple annotations (boxes), we read\n        lines and merge all the annotations for one image into a single record.\n        We do it this way to avoid loading the complete file in memory.\n\n        It is VERY important that the annotation file is sorted by image_id,\n        otherwise this way of reading them will not work.\n        """"""\n        annotations_file = self._get_annotations_path()\n\n        with tf.gfile.Open(annotations_file) as af:\n            reader = csv.DictReader(af)\n\n            current_image_id = None\n            partial_record = {}\n\n            # Number of records we have queued so far, which should be\n            # completed by another thread.\n            num_queued_records = 0\n\n            for line in reader:\n                if num_queued_records == self.total:\n                    # Reached the max number of records we can or want to\n                    # process.\n                    break\n\n                if self._all_maxed_out():\n                    break\n\n                if self._should_skip(line[\'ImageID\']):\n                    continue\n\n                # Filter group annotations (we only want single instances)\n                if line[\'IsGroupOf\'] == \'1\':\n                    continue\n\n                # Append annotation to current record.\n                try:\n                    # LabelName may not exist because not all labels are\n                    # trainable\n                    label = self.trainable_labels.index(line[\'LabelName\'])\n                except ValueError:\n                    continue\n\n                if line[\'ImageID\'] != current_image_id:\n                    # Yield if image changes and we have current image.\n                    if current_image_id is not None:\n                        num_queued_records += 1\n                        self._queue_record(\n                            partial_records_queue,\n                            partial_record\n                        )\n\n                    # Start new record.\n                    current_image_id = line[\'ImageID\']\n                    partial_record = {\n                        \'filename\': current_image_id,\n                        \'gt_boxes\': []\n                    }\n\n                partial_record[\'gt_boxes\'].append({\n                    \'xmin\': float(line[\'XMin\']),\n                    \'ymin\': float(line[\'YMin\']),\n                    \'xmax\': float(line[\'XMax\']),\n                    \'ymax\': float(line[\'YMax\']),\n                    \'label\': label,\n                })\n\n            else:\n                # No data we care about in dataset -- nothing to queue\n                if partial_record:\n                    num_queued_records += 1\n                    self._queue_record(\n                        partial_records_queue,\n                        partial_record\n                    )\n\n        tf.logging.debug(\'Stopped queuing records.\')\n\n        # Wait for all records to be consumed by the threads that complete them\n        partial_records_queue.join()\n\n        tf.logging.debug(\'All records consumed!\')\n\n        # Signal the main thread that we have finished producing and every\n        # record in the the queues has been consumed.\n        records_queue.put(None)\n\n    def _complete_records(self, input_queue, output_queue):\n        """"""\n        Daemon thread that will complete queued records from `input_queue` and\n        put them in `output_queue`, where they will be read and yielded by the\n        main thread.\n\n        This is the thread that will actually download the images of the\n        dataset.\n        """"""\n        while True:\n            try:\n                partial_record = input_queue.get()\n\n                image_id = partial_record[\'filename\']\n                image_raw = read_image(self._get_image_path(image_id))\n                image = Image.open(six.BytesIO(image_raw))\n\n                for gt_box in partial_record[\'gt_boxes\']:\n                    gt_box[\'xmin\'] *= image.width\n                    gt_box[\'ymin\'] *= image.height\n                    gt_box[\'xmax\'] *= image.width\n                    gt_box[\'ymax\'] *= image.height\n\n                partial_record[\'width\'] = image.width\n                partial_record[\'height\'] = image.height\n                partial_record[\'depth\'] = 3 if image.mode == \'RGB\' else 1\n                partial_record[\'image_raw\'] = image_raw\n\n                output_queue.put(partial_record)\n            except Exception as e:\n                tf.logging.error(\n                    \'Error processing record: {}\'.format(partial_record))\n                tf.logging.error(e)\n                self.errors += 1\n            finally:\n                input_queue.task_done()\n\n    def iterate(self):\n        """"""\n        We have a generator/consumer-generator/consumer setup where we have:\n        - one thread to read the file without the images\n        - multiple threads to download images and complete the records\n        - the main thread to yield the completed records\n        """"""\n        signal.signal(signal.SIGINT, self._stop_reading)\n\n        # Which records to complete (missing image)\n        partial_records_queue = queue.Queue()\n\n        # Limit records queue to 250 because we don\'t want to end up with all\n        # the images in memory.\n        records_queue = queue.Queue(maxsize=250)\n\n        generator = threading.Thread(\n            target=self._queue_partial_records,\n            args=(partial_records_queue, records_queue)\n        )\n        generator.start()\n\n        for _ in range(self._download_threads):\n            t = threading.Thread(\n                target=self._complete_records,\n                args=(partial_records_queue, records_queue)\n            )\n            t.daemon = True\n            t.start()\n\n        while not self._stop_iteration():\n            record = records_queue.get()\n\n            if record is None:\n                break\n\n            self.yielded_records += 1\n            yield record\n\n        # In case we were killed by signal\n        self._empty_queue(partial_records_queue)\n        self._empty_queue(records_queue)\n\n        # Wait for generator to finish peacefuly...\n        generator.join()\n\n    def _empty_queue(self, queue_to_empty):\n        while not queue_to_empty.empty():\n            try:\n                queue_to_empty.get(False)\n            except queue.Empty:\n                continue\n            queue_to_empty.task_done()\n\n    def _stop_iteration(self):\n        """"""\n        Override the parent implementation, because we deal with this in the\n        producer thread.\n        """"""\n        if not self._alive:\n            return True\n\n    def _stop_reading(self, signal, frame):\n        self._alive = False\n        sys.exit(1)\n'"
luminoth/tools/dataset/readers/object_detection/pascalvoc.py,9,"b'import os\n\nimport tensorflow as tf\n\nfrom luminoth.tools.dataset.readers import InvalidDataDirectory\nfrom luminoth.tools.dataset.readers.object_detection import (\n    ObjectDetectionReader\n)\nfrom luminoth.utils.dataset import read_xml, read_image\n\n\nclass PascalVOCReader(ObjectDetectionReader):\n    def __init__(self, data_dir, split, **kwargs):\n        super(PascalVOCReader, self).__init__(**kwargs)\n        self._data_dir = data_dir\n        self._split = split\n        self._labels_path = os.path.join(self._data_dir, \'ImageSets\', \'Main\')\n        self._images_path = os.path.join(self._data_dir, \'JPEGImages\')\n        self._annots_path = os.path.join(self._data_dir, \'Annotations\')\n\n        self.yielded_records = 0\n        self.errors = 0\n\n        # Validate PascalVoc structure in `data_dir`.\n        self._validate_structure()\n\n    def get_total(self):\n        return sum(1 for _ in self._get_record_names())\n\n    def get_classes(self):\n        classes_set = set()\n        for entry in tf.gfile.ListDirectory(self._labels_path):\n            if ""_"" not in entry:\n                continue\n            class_name, _ = entry.split(\'_\')\n            classes_set.add(class_name)\n        all_classes = list(sorted(classes_set))\n        return all_classes\n\n    def _validate_structure(self):\n        if not tf.gfile.Exists(self._data_dir):\n            raise InvalidDataDirectory(\n                \'""{}"" does not exist.\'.format(self._data_dir)\n            )\n\n        if not tf.gfile.Exists(self._labels_path):\n            raise InvalidDataDirectory(\'Labels path is missing\')\n\n        if not tf.gfile.Exists(self._images_path):\n            raise InvalidDataDirectory(\'Images path is missing\')\n\n        if not tf.gfile.Exists(self._annots_path):\n            raise InvalidDataDirectory(\'Annotations path is missing\')\n\n    def _get_split_path(self):\n        return os.path.join(self._labels_path, \'{}.txt\'.format(self._split))\n\n    def _get_record_names(self):\n        split_path = self._get_split_path()\n\n        if not tf.gfile.Exists(split_path):\n            raise ValueError(\'""{}"" not found.\'.format(split_path))\n\n        with tf.gfile.GFile(split_path) as f:\n            for line in f:\n                yield line.strip()\n\n    def _get_image_annotation(self, image_id):\n        return os.path.join(self._annots_path, \'{}.xml\'.format(image_id))\n\n    def _get_image_path(self, image_id):\n        return os.path.join(self._images_path, \'{}.jpg\'.format(image_id))\n\n    def iterate(self):\n        for image_id in self._get_record_names():\n            if self._stop_iteration():\n                # Finish iteration.\n                return\n\n            if self._should_skip(image_id):\n                continue\n\n            try:\n                annotation_path = self._get_image_annotation(image_id)\n                image_path = self._get_image_path(image_id)\n\n                # Read both the image and the annotation into memory.\n                annotation = read_xml(annotation_path)\n                image = read_image(image_path)\n            except tf.errors.NotFoundError:\n                tf.logging.debug(\n                    \'Error reading image or annotation for ""{}"".\'.format(\n                        image_id))\n                self.errors += 1\n                continue\n\n            gt_boxes = []\n\n            for b in annotation[\'object\']:\n                try:\n                    label_id = self.classes.index(b[\'name\'])\n                except ValueError:\n                    continue\n\n                gt_boxes.append({\n                    \'label\': label_id,\n                    \'xmin\': b[\'bndbox\'][\'xmin\'],\n                    \'ymin\': b[\'bndbox\'][\'ymin\'],\n                    \'xmax\': b[\'bndbox\'][\'xmax\'],\n                    \'ymax\': b[\'bndbox\'][\'ymax\'],\n                })\n\n            if len(gt_boxes) == 0:\n                continue\n\n            record = {\n                \'width\': annotation[\'size\'][\'width\'],\n                \'height\': annotation[\'size\'][\'height\'],\n                \'depth\': annotation[\'size\'][\'depth\'],\n                \'filename\': annotation[\'filename\'],\n                \'image_raw\': image,\n                \'gt_boxes\': gt_boxes,\n            }\n            self._will_add_record(record)\n            self.yielded_records += 1\n\n            yield record\n'"
luminoth/tools/dataset/readers/object_detection/taggerine.py,6,"b'import collections\nimport json\nimport os\n\nimport six\nimport tensorflow as tf\n\nfrom PIL import Image\n\nfrom luminoth.tools.dataset.readers import InvalidDataDirectory\nfrom luminoth.tools.dataset.readers.object_detection import (\n    ObjectDetectionReader\n)\nfrom luminoth.utils.dataset import read_image\n\nVALID_KEYS = [\n    (\'x\', \'y\', \'width\', \'height\', \'label\'),\n    (\'x_min\', \'y_min\', \'x_max\', \'y_max\', \'label\')\n]\n\n\nclass TaggerineReader(ObjectDetectionReader):\n    """"""\n    Object detection reader for data tagged using Taggerine:\n    https://github.com/tryolabs/taggerine/\n    """"""\n    def __init__(self, data_dir, split, default_class=0, **kwargs):\n        super(TaggerineReader, self).__init__(**kwargs)\n        self._data_dir = data_dir\n        self._split = split\n        self._split_path = os.path.join(self._data_dir, self._split)\n        self._default_class = default_class\n\n        self.annotations = []\n        # Find, read and ""parse"" annotations from files.\n        self._read_annotations(self._split_path)\n\n        self.errors = 0\n        self.yielded_records = 0\n\n    def get_total(self):\n        """"""Returns the number of files annotated.\n        """"""\n        return len(self.annotations)\n\n    def get_classes(self):\n        """"""Returns the sorted list of possible labels.\n        """"""\n        return sorted(set([\n            b.get(\'label\', self._default_class)\n            for r in self.annotations\n            for b in r.get(\'gt_boxes\')\n        ]))\n\n    def _read_annotations(self, directory):\n        """"""\n        Finds and parses Taggerine\'s JSON files.\n        """"""\n        try:\n            all_files = tf.gfile.ListDirectory(self._split_path)\n        except tf.errors.NotFoundError:\n            raise InvalidDataDirectory(\n                \'Directory for split ""{}"" does not exist\'.format(\n                    self._split))\n\n        annotation_file_candidates = []\n        for filename in all_files:\n            if filename.lower().endswith(\'.json\'):\n                annotation_file_candidates.append(filename)\n\n        if len(annotation_file_candidates) == 0:\n            raise InvalidDataDirectory(\n                \'Could not find any annotations in {}.\'.format(\n                    self._split_path) +\n                \'Check that there is a .json file with Taggerine\\\'s \' +\n                \'annotations.\')\n\n        self.annotations = []\n        # Open, validate and extract label information.\n        for filename in annotation_file_candidates:\n            annotation_path = os.path.join(self._split_path, filename)\n            with tf.gfile.Open(annotation_path) as annotation_file:\n                annotations = json.load(annotation_file)\n\n            if not isinstance(annotations, dict):\n                # JSON file with invalid format.\n                continue\n\n            file_annotations = []\n\n            invalid_label = False\n            for image_filename, labels in annotations.items():\n                if not isinstance(labels, collections.Iterable):\n                    # Ignore labels that are not lists. Ignore file.\n                    invalid_label = True\n                    break\n\n                # Validate labels\n                for label in labels:\n                    for valid_keyset in VALID_KEYS:\n                        if all(key in label for key in valid_keyset):\n                            break\n                    else:\n                        # There is not valid_keyset that can parse the label.\n                        # Ignore all labels from this file.\n                        invalid_label = True\n                        break\n\n                # Early stop for labels inside file when there is an invalid\n                # label.\n                if invalid_label:\n                    break\n\n                # Save annotations.\n                file_annotations.append({\n                    \'image_id\': os.path.basename(image_filename),\n                    \'filename\': image_filename,\n                    \'path\': os.path.join(self._split_path, image_filename),\n                    \'gt_boxes\': labels,\n                })\n\n            if invalid_label:\n                # Ignore file that have invalid labels.\n                continue\n\n            # If we have a valid file with data in it then we use it.\n            self.annotations.extend(file_annotations)\n\n    def iterate(self):\n        for annotation in self.annotations:\n            # Checks that we don\'t yield more records than necessary.\n            if self._stop_iteration():\n                return\n\n            image_id = annotation[\'image_id\']\n\n            if self._should_skip(image_id):\n                continue\n\n            try:\n                image = read_image(annotation[\'path\'])\n            except tf.errors.NotFoundError:\n                tf.logging.debug(\n                    \'Error reading image or annotation for ""{}"".\'.format(\n                        image_id))\n                self.errors += 1\n                continue\n\n            # Parse image bytes with PIL to get width and height.\n            image_pil = Image.open(six.BytesIO(image))\n            img_width = image_pil.width\n            img_height = image_pil.height\n\n            gt_boxes = []\n            for b in annotation[\'gt_boxes\']:\n                try:\n                    label_id = self.classes.index(\n                        b.get(\'label\', self._default_class)\n                    )\n                except ValueError:\n                    continue\n\n                if \'height\' in b and \'width\' in b and \'x\' in b and \'y\' in b:\n                    gt_boxes.append({\n                        \'label\': label_id,\n                        \'xmin\': b[\'x\'] * img_width,\n                        \'ymin\': b[\'y\'] * img_height,\n                        \'xmax\': b[\'x\'] * img_width + b[\'width\'] * img_width,\n                        \'ymax\': b[\'y\'] * img_height + b[\'height\'] * img_height,\n                    })\n                else:\n                    gt_boxes.append({\n                        \'label\': label_id,\n                        \'xmin\': b[\'x_min\'] * img_width,\n                        \'ymin\': b[\'y_min\'] * img_height,\n                        \'xmax\': b[\'x_max\'] * img_width,\n                        \'ymax\': b[\'y_max\'] * img_height,\n                    })\n\n            if len(gt_boxes) == 0:\n                tf.logging.debug(\'Image ""{}"" has zero valid gt_boxes.\'.format(\n                    image_id))\n                self.errors += 1\n                continue\n\n            record = {\n                \'width\': img_width,\n                \'height\': img_height,\n                \'depth\': 3,\n                \'filename\': image_id,\n                \'image_raw\': image,\n                \'gt_boxes\': gt_boxes,\n            }\n\n            self._will_add_record(record)\n            self.yielded_records += 1\n\n            yield record\n'"
