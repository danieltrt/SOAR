file_path,api_count,code
config.py,0,"b'#!/usr/bin/env python\n# -*- coding:UTF-8 -*-\n\n# File Name : config.py\n# Purpose :\n# Creation Date : 09-12-2017\n# Last Modified : Fri 19 Jan 2018 01:11:28 PM CST\n# Created By : Jeasine Ma [jeasinema[at]gmail[dot]com]\n\n\n""""""VoxelNet config system.\n""""""\n\nimport os\nimport os.path as osp\nimport numpy as np\nfrom time import strftime, localtime\nfrom easydict import EasyDict as edict\nimport math\n\n__C = edict()\n# Consumers can get config by:\n#    import config as cfg\ncfg = __C\n\n# for gpu allocation\n__C.GPU_AVAILABLE = \'3,1,2,0\'\n__C.GPU_USE_COUNT = len(__C.GPU_AVAILABLE.split(\',\'))\n__C.GPU_MEMORY_FRACTION = 1\n\n# selected object\n__C.DETECT_OBJ = \'Car\'  # Pedestrian/Cyclist\nif __C.DETECT_OBJ == \'Car\':\n    __C.Y_MIN = -40\n    __C.Y_MAX = 40\n    __C.X_MIN = 0\n    __C.X_MAX = 70.4\n    __C.VOXEL_X_SIZE = 0.2\n    __C.VOXEL_Y_SIZE = 0.2\n    __C.VOXEL_POINT_COUNT = 35\n    __C.INPUT_WIDTH = int((__C.X_MAX - __C.X_MIN) / __C.VOXEL_X_SIZE)\n    __C.INPUT_HEIGHT = int((__C.Y_MAX - __C.Y_MIN) / __C.VOXEL_Y_SIZE)\n    __C.FEATURE_RATIO = 2\n    __C.FEATURE_WIDTH = int(__C.INPUT_WIDTH / __C.FEATURE_RATIO)\n    __C.FEATURE_HEIGHT = int(__C.INPUT_HEIGHT / __C.FEATURE_RATIO)\nelse:\n    __C.Y_MIN = -20\n    __C.Y_MAX = 20\n    __C.X_MIN = 0\n    __C.X_MAX = 48\n    __C.VOXEL_X_SIZE = 0.2\n    __C.VOXEL_Y_SIZE = 0.2\n    __C.VOXEL_POINT_COUNT = 45\n    __C.INPUT_WIDTH = int((__C.X_MAX - __C.X_MIN) / __C.VOXEL_X_SIZE)\n    __C.INPUT_HEIGHT = int((__C.Y_MAX - __C.Y_MIN) / __C.VOXEL_Y_SIZE)\n    __C.FEATURE_RATIO = 2\n    __C.FEATURE_WIDTH = int(__C.INPUT_WIDTH / __C.FEATURE_RATIO)\n    __C.FEATURE_HEIGHT = int(__C.INPUT_HEIGHT / __C.FEATURE_RATIO)\n\n# set the log image scale factor\n__C.BV_LOG_FACTOR = 8\n\n# for data set type\n__C.DATA_SETS_TYPE = \'kitti\'\n\n# Root directory of project\n__C.CHECKPOINT_DIR = osp.join(\'checkpoint\')\n__C.LOG_DIR = osp.join(\'log\')\n\n# for data preprocess\n# sensors\n__C.VELODYNE_ANGULAR_RESOLUTION = 0.08 / 180 * math.pi\n__C.VELODYNE_VERTICAL_RESOLUTION = 0.4 / 180 * math.pi\n__C.VELODYNE_HEIGHT = 1.73\n# rgb\nif __C.DATA_SETS_TYPE == \'kitti\':\n    __C.IMAGE_WIDTH = 1242\n    __C.IMAGE_HEIGHT = 375\n    __C.IMAGE_CHANNEL = 3\n# top\nif __C.DATA_SETS_TYPE == \'kitti\':\n    __C.TOP_Y_MIN = -30\n    __C.TOP_Y_MAX = +30\n    __C.TOP_X_MIN = 0\n    __C.TOP_X_MAX = 80\n    __C.TOP_Z_MIN = -4.2\n    __C.TOP_Z_MAX = 0.8\n\n    __C.TOP_X_DIVISION = 0.1\n    __C.TOP_Y_DIVISION = 0.1\n    __C.TOP_Z_DIVISION = 0.2\n\n    __C.TOP_WIDTH = (__C.TOP_X_MAX - __C.TOP_X_MIN) // __C.TOP_X_DIVISION\n    __C.TOP_HEIGHT = (__C.TOP_Y_MAX - __C.TOP_Y_MIN) // __C.TOP_Y_DIVISION\n    __C.TOP_CHANNEL = (__C.TOP_Z_MAX - __C.TOP_Z_MIN) // __C.TOP_Z_DIVISION\n\n# for 2d proposal to 3d proposal\n__C.PROPOSAL3D_Z_MIN = -2.3  # -2.52\n__C.PROPOSAL3D_Z_MAX = 1.5  # -1.02\n\n# for RPN basenet choose\n__C.USE_VGG_AS_RPN = 0\n__C.USE_RESNET_AS_RPN = 0\n__C.USE_RESNEXT_AS_RPN = 0\n\n# for camera and lidar coordination convert\nif __C.DATA_SETS_TYPE == \'kitti\':\n    # cal mean from train set\n    __C.MATRIX_P2 = ([[719.787081,    0.,            608.463003, 44.9538775],\n                      [0.,            719.787081,    174.545111, 0.1066855],\n                      [0.,            0.,            1.,         3.0106472e-03],\n                      [0.,            0.,            0.,         0]])\n\n    # cal mean from train set\n    __C.MATRIX_T_VELO_2_CAM = ([\n        [7.49916597e-03, -9.99971248e-01, -8.65110297e-04, -6.71807577e-03],\n        [1.18652889e-02, 9.54520517e-04, -9.99910318e-01, -7.33152811e-02],\n        [9.99882833e-01, 7.49141178e-03, 1.18719929e-02, -2.78557062e-01],\n        [0, 0, 0, 1]\n    ])\n    # cal mean from train set\n    __C.MATRIX_R_RECT_0 = ([\n        [0.99992475, 0.00975976, -0.00734152, 0],\n        [-0.0097913, 0.99994262, -0.00430371, 0],\n        [0.00729911, 0.0043753, 0.99996319, 0],\n        [0, 0, 0, 1]\n    ])\n\n# Faster-RCNN/SSD Hyper params\nif __C.DETECT_OBJ == \'Car\':\n    # car anchor\n    __C.ANCHOR_L = 3.9\n    __C.ANCHOR_W = 1.6\n    __C.ANCHOR_H = 1.56\n    __C.ANCHOR_Z = -1.0 - cfg.ANCHOR_H/2\n    __C.RPN_POS_IOU = 0.6\n    __C.RPN_NEG_IOU = 0.45\n\nelif __C.DETECT_OBJ == \'Pedestrian\':\n    # pedestrian anchor\n    __C.ANCHOR_L = 0.8\n    __C.ANCHOR_W = 0.6\n    __C.ANCHOR_H = 1.73\n    __C.ANCHOR_Z = -0.6 - cfg.ANCHOR_H/2\n    __C.RPN_POS_IOU = 0.5\n    __C.RPN_NEG_IOU = 0.35\n\nif __C.DETECT_OBJ == \'Cyclist\':\n    # cyclist anchor\n    __C.ANCHOR_L = 1.76\n    __C.ANCHOR_W = 0.6\n    __C.ANCHOR_H = 1.73\n    __C.ANCHOR_Z = -0.6 - cfg.ANCHOR_H/2\n    __C.RPN_POS_IOU = 0.5\n    __C.RPN_NEG_IOU = 0.35\n\n# for rpn nms\n__C.RPN_NMS_POST_TOPK = 20\n__C.RPN_NMS_THRESH = 0.3\n__C.RPN_SCORE_THRESH = 0.96\n\n\n# utils\n__C.CORNER2CENTER_AVG = True  # average version or max version\n\nif __name__ == \'__main__\':\n    print(\'__C.ROOT_DIR = \' + __C.ROOT_DIR)\n    print(\'__C.DATA_SETS_DIR = \' + __C.DATA_SETS_DIR)\n'"
setup.py,0,"b""#!/usr/bin/env python\n# -*- coding:UTF-8 -*-\n\n# File Name : setup.py\n# Purpose :\n# Creation Date : 11-12-2017\n# Last Modified : Sat 23 Dec 2017 03:18:37 PM CST\n# Created By : Jeasine Ma [jeasinema[at]gmail[dot]com]\n\n\nfrom distutils.core import setup\nfrom Cython.Build import cythonize\n\nsetup(\n    name='box overlaps',\n    ext_modules=cythonize('./utils/box_overlaps.pyx')\n)\n"""
test.py,6,"b'#!/usr/bin/env python\n# -*- coding:UTF-8 -*-\n\n# File Name : train.py\n# Purpose :\n# Creation Date : 09-12-2017\n# Last Modified : Fri 05 Jan 2018 09:35:00 PM CST\n# Created By : Jeasine Ma [jeasinema[at]gmail[dot]com]\n\nimport glob\nimport argparse\nimport os\nimport time\nimport tensorflow as tf\n\nfrom model import RPN3D\nfrom config import cfg\nfrom utils import *\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'testing\')\n\n    parser.add_argument(\'-n\', \'--tag\', type=str, nargs=\'?\', default=\'default\',\n                        help=\'set log tag\')\n    parser.add_argument(\'--output-path\', type=str, nargs=\'?\',\n                        default=\'./data/results/data\', help=\'results output dir\')\n    parser.add_argument(\'-b\', \'--single-batch-size\', type=int, nargs=\'?\', default=1,\n                        help=\'set batch size for each gpu\')\n\n    args = parser.parse_args()\n\n    dataset_dir = \'./data/object\'\n    save_model_dir = os.path.join(\'./save_model\', args.tag)\n\n    with tf.Graph().as_default():\n        with KittiLoader(object_dir=os.path.join(dataset_dir, \'testing_real\'), queue_size=100, require_shuffle=False, is_testset=True, batch_size=args.single_batch_size * cfg.GPU_USE_COUNT, use_multi_process_num=8, multi_gpu_sum=cfg.GPU_USE_COUNT) as test_loader:\n            gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=cfg.GPU_MEMORY_FRACTION,\n                                        visible_device_list=cfg.GPU_AVAILABLE,\n                                        allow_growth=True)\n            config = tf.ConfigProto(\n                gpu_options=gpu_options,\n                device_count={\n                    ""GPU"": cfg.GPU_USE_COUNT,\n                },\n                allow_soft_placement=True,\n            )\n\n            with tf.Session(config=config) as sess:\n                model = RPN3D(\n                    cls=cfg.DETECT_OBJ,\n                    single_batch_size=args.single_batch_size,\n                    is_train=True,\n                    avail_gpus=cfg.GPU_AVAILABLE.split(\',\')\n                )\n                if tf.train.get_checkpoint_state(save_model_dir):\n                    print(""Reading model parameters from %s"" % save_model_dir)\n                    model.saver.restore(\n                        sess, tf.train.latest_checkpoint(save_model_dir))\n                while True:\n                    data = test_loader.load()\n                    if data is None:\n                        print(\'test done.\')\n                        break\n                    ret = model.predict_step(sess, data, summary=False)\n                    # ret: A, B\n                    # A: (N) tag\n                    # B: (N, N\') (class, x, y, z, h, w, l, rz, score)\n                    for tag, result in zip(*ret):\n                        of_path = os.path.join(args.output_path, tag + \'.txt\')\n                        with open(of_path, \'w+\') as f:\n                            labels = box3d_to_label([result[:, 1:8]], [result[:, 0]], [result[:, -1]], coordinate=\'lidar\')[0]\n                            for line in labels:\n                                f.write(line)\n                            print(\'write out {} objects to {}\'.format(len(labels), tag))\n'"
train.py,9,"b'#!/usr/bin/env python\n# -*- coding:UTF-8 -*-\n\n# File Name : train.py\n# Purpose :\n# Creation Date : 09-12-2017\n# Last Modified : Fri 19 Jan 2018 10:38:47 AM CST\n# Created By : Jeasine Ma [jeasinema[at]gmail[dot]com]\n\nimport glob\nimport argparse\nimport os\nimport time\nimport sys\nimport tensorflow as tf\nfrom itertools import count\n\nfrom config import cfg\nfrom model import RPN3D\nfrom utils.kitti_loader import KittiLoader\nfrom train_hook import check_if_should_pause\n\nparser = argparse.ArgumentParser(description=\'training\')\nparser.add_argument(\'-i\', \'--max-epoch\', type=int, nargs=\'?\', default=10,\n                    help=\'max epoch\')\nparser.add_argument(\'-n\', \'--tag\', type=str, nargs=\'?\', default=\'default\',\n                    help=\'set log tag\')\nparser.add_argument(\'-b\', \'--single-batch-size\', type=int, nargs=\'?\', default=1,\n                    help=\'set batch size for each gpu\')\nparser.add_argument(\'-l\', \'--lr\', type=float, nargs=\'?\', default=0.001,\n                    help=\'set learning rate\')\nargs = parser.parse_args()\n\ndataset_dir = \'./data/object\'\nlog_dir = os.path.join(\'./log\', args.tag)\nsave_model_dir = os.path.join(\'./save_model\', args.tag)\nos.makedirs(log_dir, exist_ok=True)\nos.makedirs(save_model_dir, exist_ok=True)\n\n\ndef main(_):\n    # TODO: split file support\n    with tf.Graph().as_default():\n        global save_model_dir\n        with KittiLoader(object_dir=os.path.join(dataset_dir, \'training\'), queue_size=50, require_shuffle=True,\n                         is_testset=False, batch_size=args.single_batch_size * cfg.GPU_USE_COUNT, use_multi_process_num=8, multi_gpu_sum=cfg.GPU_USE_COUNT, aug=True) as train_loader, \\\n            KittiLoader(object_dir=os.path.join(dataset_dir, \'testing\'), queue_size=50, require_shuffle=True,\n                        is_testset=False, batch_size=args.single_batch_size * cfg.GPU_USE_COUNT, use_multi_process_num=8, multi_gpu_sum=cfg.GPU_USE_COUNT, aug=False) as valid_loader:\n\n            gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=cfg.GPU_MEMORY_FRACTION,\n                                        visible_device_list=cfg.GPU_AVAILABLE,\n                                        allow_growth=True)\n            config = tf.ConfigProto(\n                gpu_options=gpu_options,\n                device_count={\n                    ""GPU"": cfg.GPU_USE_COUNT,\n                },\n                allow_soft_placement=True,\n            )\n            with tf.Session(config=config) as sess:\n                model = RPN3D(\n                    cls=cfg.DETECT_OBJ,\n                    single_batch_size=args.single_batch_size,\n                    learning_rate=args.lr,\n                    max_gradient_norm=5.0,\n                    is_train=True,\n                    alpha=1.5,\n                    beta=1,\n                    avail_gpus=cfg.GPU_AVAILABLE.split(\',\')\n                )\n                # param init/restore\n                if tf.train.get_checkpoint_state(save_model_dir):\n                    print(""Reading model parameters from %s"" % save_model_dir)\n                    model.saver.restore(\n                        sess, tf.train.latest_checkpoint(save_model_dir))\n                else:\n                    print(""Created model with fresh parameters."")\n                    tf.global_variables_initializer().run()\n\n                # train and validate\n                iter_per_epoch = int(\n                    len(train_loader) / (args.single_batch_size * cfg.GPU_USE_COUNT))\n                is_summary, is_summary_image, is_validate = False, False, False\n\n                summary_interval = 5\n                summary_image_interval = 20\n                save_model_interval = int(iter_per_epoch / 3)\n                validate_interval = 60\n\n                summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n                while model.epoch.eval() < args.max_epoch:\n                    is_summary, is_summary_image, is_validate = False, False, False\n                    iter = model.global_step.eval()\n                    if not iter % summary_interval:\n                        is_summary = True\n                    if not iter % summary_image_interval:\n                        is_summary_image = True\n                    if not iter % save_model_interval:\n                        model.saver.save(sess, os.path.join(\n                            save_model_dir, \'checkpoint\'), global_step=model.global_step)\n                    if not iter % validate_interval:\n                        is_validate = True\n                    if not iter % iter_per_epoch:\n                        sess.run(model.epoch_add_op)\n                        print(\'train {} epoch, total: {}\'.format(\n                            model.epoch.eval(), args.max_epoch))\n\n                    ret = model.train_step(\n                        sess, train_loader.load(), train=True, summary=is_summary)\n                    print(\'train: {}/{} @ epoch:{}/{} loss: {} reg_loss: {} cls_loss: {} {}\'.format(iter,\n                                                                                                    iter_per_epoch * args.max_epoch, model.epoch.eval(), args.max_epoch, ret[0], ret[1], ret[2], args.tag))\n\n                    if is_summary:\n                        summary_writer.add_summary(ret[-1], iter)\n\n                    if is_summary_image:\n                        ret = model.predict_step(\n                                sess, valid_loader.load(), summary=True)\n                        summary_writer.add_summary(ret[-1], iter)\n\n                    if is_validate:\n                        ret = model.validate_step(\n                                sess, valid_loader.load(), summary=True)\n                        summary_writer.add_summary(ret[-1], iter)\n\n                    if check_if_should_pause(args.tag):\n                        model.saver.save(sess, os.path.join(\n                            save_model_dir, \'checkpoint\'), global_step=model.global_step)\n                        print(\'pause and save model @ {} steps:{}\'.format(\n                            save_model_dir, model.global_step.eval()))\n                        sys.exit(0)\n\n                print(\'train done. total epoch:{} iter:{}\'.format(\n                    model.epoch.eval(), model.global_step.eval()))\n\n                # finallly save model\n                model.saver.save(sess, os.path.join(\n                    save_model_dir, \'checkpoint\'), global_step=model.global_step)\n\n\nif __name__ == \'__main__\':\n    tf.app.run(main)\n'"
train_hook.py,0,"b""#!/usr/bin/env python\n# -*- coding:UTF-8 -*-\n\n# File Name : train_hook.py\n# Purpose :\n# Creation Date : 14-12-2017\n# Last Modified : Sat 23 Dec 2017 11:45:38 PM CST\n# Created By : Jeasine Ma [jeasinema[at]gmail[dot]com]\n\nimport os\nimport argparse\nimport pickle\nimport numpy as np\n\n\ndef check_if_should_pause(tag):\n    fname = tag + '.pause.pkl'\n    ret = False\n    if os.path.exists(fname):\n        s = pickle.load(open(tag + '.pause.pkl', 'rb'))\n        if s == 'pause':\n            ret = True\n        os.remove(fname)\n    return ret\n\n\ndef pause_trainer(args):\n    fname = args.tag + '.pause.pkl'\n    if os.path.exists(fname):\n        os.remove(fname)\n    pickle.dump('pause', open(fname, 'wb'))\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='training')\n    parser.add_argument('--tag', type=str, nargs='?', default='default')\n    args = parser.parse_args()\n\n    pause_trainer(args)\n"""
model/__init__.py,0,b'#!/usr/bin/env python\n# -*- coding:UTF-8 -*-\n\n# File Name :\n# Purpose :\n# Creation Date : 21-12-2017\n# Last Modified : Thu 21 Dec 2017 08:03:57 PM CST\n# Created By : Jeasine Ma [jeasinema[at]gmail[dot]com]\n\nfrom model.group_pointcloud import *\nfrom model.rpn import *\nfrom model.model import *\n'
model/group_pointcloud.py,27,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# File Name : rpn.py\n# Purpose :\n# Creation Date : 10-12-2017\n# Last Modified : Thu 21 Dec 2017 07:48:05 PM CST\n# Created By : Wei Zhang\n\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport time\n\nfrom config import cfg\n\n\nclass VFELayer(object):\n\n    def __init__(self, out_channels, name):\n        super(VFELayer, self).__init__()\n        self.units = int(out_channels / 2)\n        with tf.variable_scope(name, reuse=tf.AUTO_REUSE) as scope:\n            self.dense = tf.layers.Dense(\n                self.units, tf.nn.relu, name=\'dense\', _reuse=tf.AUTO_REUSE, _scope=scope)\n            self.batch_norm = tf.layers.BatchNormalization(\n                name=\'batch_norm\', fused=True, _reuse=tf.AUTO_REUSE, _scope=scope)\n\n    def apply(self, inputs, mask, training):\n        # [K, T, 7] tensordot [7, units] = [K, T, units]\n        pointwise = self.batch_norm.apply(self.dense.apply(inputs), training)\n\n        #n [K, 1, units]\n        aggregated = tf.reduce_max(pointwise, axis=1, keep_dims=True)\n\n        # [K, T, units]\n        repeated = tf.tile(aggregated, [1, cfg.VOXEL_POINT_COUNT, 1])\n\n        # [K, T, 2 * units]\n        concatenated = tf.concat([pointwise, repeated], axis=2)\n\n        mask = tf.tile(mask, [1, 1, 2 * self.units])\n\n        concatenated = tf.multiply(concatenated, tf.cast(mask, tf.float32))\n\n        return concatenated\n\n\nclass FeatureNet(object):\n\n    def __init__(self, training, batch_size, name=\'\'):\n        super(FeatureNet, self).__init__()\n        self.training = training\n\n        # scalar\n        self.batch_size = batch_size\n        # [\xce\xa3K, 35/45, 7]\n        self.feature = tf.placeholder(\n            tf.float32, [None, cfg.VOXEL_POINT_COUNT, 7], name=\'feature\')\n        # [\xce\xa3K]\n        self.number = tf.placeholder(tf.int64, [None], name=\'number\')\n        # [\xce\xa3K, 4], each row stores (batch, d, h, w)\n        self.coordinate = tf.placeholder(\n            tf.int64, [None, 4], name=\'coordinate\')\n\n        with tf.variable_scope(name, reuse=tf.AUTO_REUSE) as scope:\n            self.vfe1 = VFELayer(32, \'VFE-1\')\n            self.vfe2 = VFELayer(128, \'VFE-2\')\n            self.dense = tf.layers.Dense(\n                128, tf.nn.relu, name=\'dense\', _reuse=tf.AUTO_REUSE, _scope=scope)\n            self.batch_norm = tf.layers.BatchNormalization(\n                name=\'batch_norm\', fused=True, _reuse=tf.AUTO_REUSE, _scope=scope)\n        # boolean mask [K, T, 2 * units]\n        mask = tf.not_equal(tf.reduce_max(\n            self.feature, axis=2, keep_dims=True), 0)\n        x = self.vfe1.apply(self.feature, mask, self.training)\n        x = self.vfe2.apply(x, mask, self.training)\n        x = self.dense.apply(x)\n        x = self.batch_norm.apply(x, self.training)\n\n        # [\xce\xa3K, 128]\n        voxelwise = tf.reduce_max(x, axis=1)\n\n        # car: [N * 10 * 400 * 352 * 128]\n        # pedestrian/cyclist: [N * 10 * 200 * 240 * 128]\n        self.outputs = tf.scatter_nd(\n            self.coordinate, voxelwise, [self.batch_size, 10, cfg.INPUT_HEIGHT, cfg.INPUT_WIDTH, 128])\n\n\ndef build_input(voxel_dict_list):\n    batch_size = len(voxel_dict_list)\n\n    feature_list = []\n    number_list = []\n    coordinate_list = []\n    for i, voxel_dict in zip(range(batch_size), voxel_dict_list):\n        feature_list.append(voxel_dict[\'feature_buffer\'])\n        number_list.append(voxel_dict[\'number_buffer\'])\n        coordinate = voxel_dict[\'coordinate_buffer\']\n        coordinate_list.append(\n            np.pad(coordinate, ((0, 0), (1, 0)),\n                   mode=\'constant\', constant_values=i))\n\n    feature = np.concatenate(feature_list)\n    number = np.concatenate(number_list)\n    coordinate = np.concatenate(coordinate_list)\n    return batch_size, feature, number, coordinate\n\n\ndef run(batch_size, feature, number, coordinate):\n    """"""\n    Input:\n        batch_size: scalar, the batch size\n        feature: [\xce\xa3K, T, 7], voxel input feature buffer\n        number: [\xce\xa3K], number of points in each voxel\n        coordinate: [\xce\xa3K, 4], voxel coordinate buffer\n\n        A feature tensor feature[i] has number[i] points in it and is located in\n        coordinate[i] (a 1-D tensor reprents [batch, d, h, w]) in the output\n\n        Input format is similiar to what\'s described in section 2.3 of the paper\n\n        Suppose the batch size is 3, the 3 point cloud is loaded as\n        1. feature: [K1, T, 7] (K1 is the number of non-empty voxels)\n           number: [K1] (number of points in the corresponding voxel)\n           coordinate: [K1, 3] (each row is a tensor reprents [d, h, w])\n        2. feature: [K2, T, 7]\n           number: [K2]\n           coordinate: [K2, 3]\n        3. feature: [K3, T, 7]\n           number: [K3]\n           coordinate: [K3, 3]\n        Then the corresponding input is\n        batch_size: 3\n        feature: [K1 + K2 + K3, T, 7]\n        number: [K1 + K2 + K3]\n        coordinate: [K1 + K2 + K3, 4] (need to append the batch index of the\n                                       corresponding voxel in front of each row)\n    Output:\n        outputs: [batch_size, 10, 400, 352, 128]\n    """"""\n    gpu_options = tf.GPUOptions(visible_device_list=\'0,2,3\')\n    config = tf.ConfigProto(\n        gpu_options=gpu_options,\n        device_count={\'GPU\': 3}\n    )\n\n    with tf.Session(config=config) as sess:\n        model = FeatureNet(training=False, batch_size=batch_size)\n        tf.global_variables_initializer().run()\n        for i in range(10):\n            time_start = time.time()\n            feed = {model.feature: feature,\n                    model.number: number,\n                    model.coordinate: coordinate}\n            outputs = sess.run([model.outputs], feed)\n            print(outputs[0].shape)\n            time_end = time.time()\n            print(time_end - time_start)\n\n\ndef main():\n    data_dir = \'./data/object/training/voxel\'\n    batch_size = 32\n\n    filelist = [f for f in os.listdir(data_dir) if f.endswith(\'npz\')]\n\n    import time\n    voxel_dict_list = []\n    for id in range(0, len(filelist), batch_size):\n        pre_time = time.time()\n        batch_file = [f for f in filelist[id:id + batch_size]]\n        voxel_dict_list = []\n        for file in batch_file:\n            voxel_dict_list.append(np.load(os.path.join(data_dir, file)))\n\n        # example input with batch size 16\n        batch_size, feature, number, coordinate = build_input(voxel_dict_list)\n        print(time.time() - pre_time)\n\n    run(batch_size, feature, number, coordinate)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
model/model.py,41,"b""#!/usr/bin/env python\n# -*- coding:UTF-8 -*-\n\n# File Name : model.py\n# Purpose :\n# Creation Date : 09-12-2017\n# Last Modified : Fri 05 Jan 2018 09:34:48 PM CST\n# Created By : Jeasine Ma [jeasinema[at]gmail[dot]com]\n\nimport sys\nimport os\nimport tensorflow as tf\nimport cv2\nfrom numba import jit\n\nfrom config import cfg\nfrom utils import *\nfrom model.group_pointcloud import FeatureNet\nfrom model.rpn import MiddleAndRPN\n\n\nclass RPN3D(object):\n\n    def __init__(self,\n                 cls='Car',\n                 single_batch_size=2,  # batch_size_per_gpu\n                 learning_rate=0.001,\n                 max_gradient_norm=5.0,\n                 alpha=1.5,\n                 beta=1,\n                 is_train=True,\n                 avail_gpus=['0']):\n        # hyper parameters and status\n        self.cls = cls\n        self.single_batch_size = single_batch_size\n        self.learning_rate = tf.Variable(\n            float(learning_rate), trainable=False, dtype=tf.float32)\n        self.global_step = tf.Variable(1, trainable=False)\n        self.epoch = tf.Variable(0, trainable=False)\n        self.epoch_add_op = self.epoch.assign(self.epoch + 1)\n        self.alpha = alpha\n        self.beta = beta\n        self.avail_gpus = avail_gpus\n\n        lr = tf.train.exponential_decay(\n            self.learning_rate, self.global_step, 10000, 0.96)\n\n        # build graph\n        # input placeholders\n        self.vox_feature = []\n        self.vox_number = []\n        self.vox_coordinate = []\n        self.targets = []\n        self.pos_equal_one = []\n        self.pos_equal_one_sum = []\n        self.pos_equal_one_for_reg = []\n        self.neg_equal_one = []\n        self.neg_equal_one_sum = []\n\n        self.delta_output = []\n        self.prob_output = []\n        self.opt = tf.train.AdamOptimizer(lr)\n        self.gradient_norm = []\n        self.tower_grads = []\n        with tf.variable_scope(tf.get_variable_scope()):\n            for idx, dev in enumerate(self.avail_gpus):\n                with tf.device('/gpu:{}'.format(dev)), tf.name_scope('gpu_{}'.format(dev)):\n                    # must use name scope here since we do not want to create new variables\n                    # graph\n                    feature = FeatureNet(\n                        training=is_train, batch_size=self.single_batch_size)\n                    rpn = MiddleAndRPN(\n                        input=feature.outputs, alpha=self.alpha, beta=self.beta, training=is_train)\n                    tf.get_variable_scope().reuse_variables()\n                    # input\n                    self.vox_feature.append(feature.feature)\n                    self.vox_number.append(feature.number)\n                    self.vox_coordinate.append(feature.coordinate)\n                    self.targets.append(rpn.targets)\n                    self.pos_equal_one.append(rpn.pos_equal_one)\n                    self.pos_equal_one_sum.append(rpn.pos_equal_one_sum)\n                    self.pos_equal_one_for_reg.append(\n                        rpn.pos_equal_one_for_reg)\n                    self.neg_equal_one.append(rpn.neg_equal_one)\n                    self.neg_equal_one_sum.append(rpn.neg_equal_one_sum)\n                    # output\n                    feature_output = feature.outputs\n                    delta_output = rpn.delta_output\n                    prob_output = rpn.prob_output\n                    # loss and grad\n                    self.loss = rpn.loss\n                    self.reg_loss = rpn.reg_loss\n                    self.cls_loss = rpn.cls_loss\n                    self.params = tf.trainable_variables()\n                    gradients = tf.gradients(self.loss, self.params)\n                    clipped_gradients, gradient_norm = tf.clip_by_global_norm(\n                        gradients, max_gradient_norm)\n\n                    self.delta_output.append(delta_output)\n                    self.prob_output.append(prob_output)\n                    self.tower_grads.append(clipped_gradients)\n                    self.gradient_norm.append(gradient_norm)\n                    self.rpn_output_shape = rpn.output_shape\n\n        # loss and optimizer\n        # self.xxxloss is only the loss for the lowest tower\n        with tf.device('/gpu:{}'.format(self.avail_gpus[0])):\n            self.grads = average_gradients(self.tower_grads)\n            self.update = self.opt.apply_gradients(\n                zip(self.grads, self.params), global_step=self.global_step)\n            self.gradient_norm = tf.group(*self.gradient_norm)\n\n        self.delta_output = tf.concat(self.delta_output, axis=0)\n        self.prob_output = tf.concat(self.prob_output, axis=0)\n\n        self.anchors = cal_anchors()\n        # for predict and image summary\n        self.rgb = tf.placeholder(\n            tf.uint8, [None, cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH, 3])\n        self.bv = tf.placeholder(tf.uint8, [\n                                 None, cfg.BV_LOG_FACTOR * cfg.INPUT_HEIGHT, cfg.BV_LOG_FACTOR * cfg.INPUT_WIDTH, 3])\n        self.bv_heatmap = tf.placeholder(tf.uint8, [\n            None, cfg.BV_LOG_FACTOR * cfg.FEATURE_HEIGHT, cfg.BV_LOG_FACTOR * cfg.FEATURE_WIDTH, 3])\n        self.boxes2d = tf.placeholder(tf.float32, [None, 4])\n        self.boxes2d_scores = tf.placeholder(tf.float32, [None])\n\n        # NMS(2D)\n        with tf.device('/gpu:{}'.format(self.avail_gpus[0])):\n            self.box2d_ind_after_nms = tf.image.non_max_suppression(\n                self.boxes2d, self.boxes2d_scores, max_output_size=cfg.RPN_NMS_POST_TOPK, iou_threshold=cfg.RPN_NMS_THRESH)\n\n        # summary and saver\n        self.saver = tf.train.Saver(write_version=tf.train.SaverDef.V2,\n                                    max_to_keep=10, pad_step_number=True, keep_checkpoint_every_n_hours=1.0)\n\n        self.train_summary = tf.summary.merge([\n            tf.summary.scalar('train/loss', self.loss),\n            tf.summary.scalar('train/reg_loss', self.reg_loss),\n            tf.summary.scalar('train/cls_loss', self.cls_loss),\n            *[tf.summary.histogram(each.name, each) for each in self.params]\n        ])\n\n        self.validate_summary = tf.summary.merge([\n            tf.summary.scalar('validate/loss', self.loss),\n            tf.summary.scalar('validate/reg_loss', self.reg_loss),\n            tf.summary.scalar('validate/cls_loss', self.cls_loss)\n        ])\n\n        # TODO: bird_view_summary and front_view_summary\n\n        self.predict_summary = tf.summary.merge([\n            tf.summary.image('predict/bird_view_lidar', self.bv),\n            tf.summary.image('predict/bird_view_heatmap', self.bv_heatmap),\n            tf.summary.image('predict/front_view_rgb', self.rgb),\n        ])\n\n    def train_step(self, session, data, train=False, summary=False):\n        # input:\n        #     (N) tag\n        #     (N, N') label\n        #     vox_feature\n        #     vox_number\n        #     vox_coordinate\n        tag = data[0]\n        label = data[1]\n        vox_feature = data[2]\n        vox_number = data[3]\n        vox_coordinate = data[4]\n        print('train', tag)\n        pos_equal_one, neg_equal_one, targets = cal_rpn_target(\n            label, self.rpn_output_shape, self.anchors, cls=cfg.DETECT_OBJ, coordinate='lidar')\n        pos_equal_one_for_reg = np.concatenate(\n            [np.tile(pos_equal_one[..., [0]], 7), np.tile(pos_equal_one[..., [1]], 7)], axis=-1)\n        pos_equal_one_sum = np.clip(np.sum(pos_equal_one, axis=(\n            1, 2, 3)).reshape(-1, 1, 1, 1), a_min=1, a_max=None)\n        neg_equal_one_sum = np.clip(np.sum(neg_equal_one, axis=(\n            1, 2, 3)).reshape(-1, 1, 1, 1), a_min=1, a_max=None)\n\n        input_feed = {}\n        for idx in range(len(self.avail_gpus)):\n            input_feed[self.vox_feature[idx]] = vox_feature[idx]\n            input_feed[self.vox_number[idx]] = vox_number[idx]\n            input_feed[self.vox_coordinate[idx]] = vox_coordinate[idx]\n            input_feed[self.targets[idx]] = targets[idx *\n                                                    self.single_batch_size:(idx + 1) * self.single_batch_size]\n            input_feed[self.pos_equal_one[idx]] = pos_equal_one[idx *\n                                                                self.single_batch_size:(idx + 1) * self.single_batch_size]\n            input_feed[self.pos_equal_one_sum[idx]] = pos_equal_one_sum[idx *\n                                                                        self.single_batch_size:(idx + 1) * self.single_batch_size]\n            input_feed[self.pos_equal_one_for_reg[idx]] = pos_equal_one_for_reg[idx *\n                                                                                self.single_batch_size:(idx + 1) * self.single_batch_size]\n            input_feed[self.neg_equal_one[idx]] = neg_equal_one[idx *\n                                                                self.single_batch_size:(idx + 1) * self.single_batch_size]\n            input_feed[self.neg_equal_one_sum[idx]] = neg_equal_one_sum[idx *\n                                                                        self.single_batch_size:(idx + 1) * self.single_batch_size]\n        if train:\n            output_feed = [self.loss, self.reg_loss,\n                           self.cls_loss, self.gradient_norm, self.update]\n        else:\n            output_feed = [self.loss, self.reg_loss, self.cls_loss]\n        if summary:\n            output_feed.append(self.train_summary)\n        # TODO: multi-gpu support for test and predict step\n        return session.run(output_feed, input_feed)\n\n    def validate_step(self, session, data, summary=False):\n        # input:\n        #     (N) tag\n        #     (N, N') label\n        #     vox_feature\n        #     vox_number\n        #     vox_coordinate\n        tag = data[0]\n        label = data[1]\n        vox_feature = data[2]\n        vox_number = data[3]\n        vox_coordinate = data[4]\n        print('valid', tag)\n        pos_equal_one, neg_equal_one, targets = cal_rpn_target(\n            label, self.rpn_output_shape, self.anchors)\n        pos_equal_one_for_reg = np.concatenate(\n            [np.tile(pos_equal_one[..., [0]], 7), np.tile(pos_equal_one[..., [1]], 7)], axis=-1)\n        pos_equal_one_sum = np.clip(np.sum(pos_equal_one, axis=(\n            1, 2, 3)).reshape(-1, 1, 1, 1), a_min=1, a_max=None)\n        neg_equal_one_sum = np.clip(np.sum(neg_equal_one, axis=(\n            1, 2, 3)).reshape(-1, 1, 1, 1), a_min=1, a_max=None)\n\n        input_feed = {}\n        for idx in range(len(self.avail_gpus)):\n            input_feed[self.vox_feature[idx]] = vox_feature[idx]\n            input_feed[self.vox_number[idx]] = vox_number[idx]\n            input_feed[self.vox_coordinate[idx]] = vox_coordinate[idx]\n            input_feed[self.targets[idx]] = targets[idx *\n                                                    self.single_batch_size:(idx + 1) * self.single_batch_size]\n            input_feed[self.pos_equal_one[idx]] = pos_equal_one[idx *\n                                                                self.single_batch_size:(idx + 1) * self.single_batch_size]\n            input_feed[self.pos_equal_one_sum[idx]] = pos_equal_one_sum[idx *\n                                                                        self.single_batch_size:(idx + 1) * self.single_batch_size]\n            input_feed[self.pos_equal_one_for_reg[idx]] = pos_equal_one_for_reg[idx *\n                                                                                self.single_batch_size:(idx + 1) * self.single_batch_size]\n            input_feed[self.neg_equal_one[idx]] = neg_equal_one[idx *\n                                                                self.single_batch_size:(idx + 1) * self.single_batch_size]\n            input_feed[self.neg_equal_one_sum[idx]] = neg_equal_one_sum[idx *\n                                                                        self.single_batch_size:(idx + 1) * self.single_batch_size]\n\n        output_feed = [self.loss, self.reg_loss, self.cls_loss]\n        if summary:\n            output_feed.append(self.validate_summary)\n        return session.run(output_feed, input_feed)\n\n    def predict_step(self, session, data, summary=False):\n        # input:\n        #     (N) tag\n        #     (N, N') label(can be empty)\n        #     vox_feature\n        #     vox_number\n        #     vox_coordinate\n        #     img (N, w, l, 3)\n        #     lidar (N, N', 4)\n        # output: A, B, C\n        #     A: (N) tag\n        #     B: (N, N') (class, x, y, z, h, w, l, rz, score)\n        #     C; summary(optional)\n        tag = data[0]\n        label = data[1]\n        vox_feature = data[2]\n        vox_number = data[3]\n        vox_coordinate = data[4]\n        img = data[5]\n        lidar = data[6]\n\n        if summary:\n            batch_gt_boxes3d = label_to_gt_box3d(\n                label, cls=self.cls, coordinate='lidar')\n        print('predict', tag)\n        input_feed = {}\n        for idx in range(len(self.avail_gpus)):\n            input_feed[self.vox_feature[idx]] = vox_feature[idx]\n            input_feed[self.vox_number[idx]] = vox_number[idx]\n            input_feed[self.vox_coordinate[idx]] = vox_coordinate[idx]\n\n        output_feed = [self.prob_output, self.delta_output]\n        probs, deltas = session.run(output_feed, input_feed)\n        # BOTTLENECK\n        batch_boxes3d = delta_to_boxes3d(\n            deltas, self.anchors, coordinate='lidar')\n        batch_boxes2d = batch_boxes3d[:, :, [0, 1, 4, 5, 6]]\n        batch_probs = probs.reshape(\n            (len(self.avail_gpus) * self.single_batch_size, -1))\n        # NMS\n        ret_box3d = []\n        ret_score = []\n        for batch_id in range(len(self.avail_gpus) * self.single_batch_size):\n            # remove box with low score\n            ind = np.where(batch_probs[batch_id, :] >= cfg.RPN_SCORE_THRESH)[0]\n            tmp_boxes3d = batch_boxes3d[batch_id, ind, ...]\n            tmp_boxes2d = batch_boxes2d[batch_id, ind, ...]\n            tmp_scores = batch_probs[batch_id, ind]\n\n            # TODO: if possible, use rotate NMS\n            boxes2d = corner_to_standup_box2d(\n                center_to_corner_box2d(tmp_boxes2d, coordinate='lidar'))\n            ind = session.run(self.box2d_ind_after_nms, {\n                self.boxes2d: boxes2d,\n                self.boxes2d_scores: tmp_scores\n            })\n            tmp_boxes3d = tmp_boxes3d[ind, ...]\n            tmp_scores = tmp_scores[ind]\n            ret_box3d.append(tmp_boxes3d)\n            ret_score.append(tmp_scores)\n\n        ret_box3d_score = []\n        for boxes3d, scores in zip(ret_box3d, ret_score):\n            ret_box3d_score.append(np.concatenate([np.tile(self.cls, len(boxes3d))[:, np.newaxis],\n                                                   boxes3d, scores[:, np.newaxis]], axis=-1))\n\n        if summary:\n            # only summry 1 in a batch\n            front_image = draw_lidar_box3d_on_image(img[0], ret_box3d[0], ret_score[0],\n                                                    batch_gt_boxes3d[0])\n            bird_view = lidar_to_bird_view_img(\n                lidar[0], factor=cfg.BV_LOG_FACTOR)\n            bird_view = draw_lidar_box3d_on_birdview(bird_view, ret_box3d[0], ret_score[0],\n                                                     batch_gt_boxes3d[0], factor=cfg.BV_LOG_FACTOR)\n            heatmap = colorize(probs[0, ...], cfg.BV_LOG_FACTOR)\n            ret_summary = session.run(self.predict_summary, {\n                self.rgb: front_image[np.newaxis, ...],\n                self.bv: bird_view[np.newaxis, ...],\n                self.bv_heatmap: heatmap[np.newaxis, ...]\n            })\n\n            return tag, ret_box3d_score, ret_summary\n\n        return tag, ret_box3d_score\n\n\ndef average_gradients(tower_grads):\n    # ref:\n    # https://github.com/tensorflow/models/blob/6db9f0282e2ab12795628de6200670892a8ad6ba/tutorials/image/cifar10/cifar10_multi_gpu_train.py#L103\n    # but only contains grads, no vars\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        grads = []\n        for g in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a 'tower' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the 'tower' dimension.\n        grad = tf.concat(axis=0, values=grads)\n        grad = tf.reduce_mean(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower's pointer to\n        # the Variable.\n        grad_and_var = grad\n        average_grads.append(grad_and_var)\n    return average_grads\n\n\nif __name__ == '__main__':\n    pass\n"""
model/rpn.py,45,"b'#!/usr/bin/env python\n# -*- coding:UTF-8 -*-\n\n# File Name : rpn.py\n# Purpose :\n# Creation Date : 10-12-2017\n# Last Modified : Thu 08 Mar 2018 02:20:43 PM CST\n# Created By : Jialin Zhao\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom config import cfg\n\n\nsmall_addon_for_BCE = 1e-6\n\n\nclass MiddleAndRPN:\n    def __init__(self, input, alpha=1.5, beta=1, sigma=3, training=True, name=\'\'):\n        # scale = [batchsize, 10, 400/200, 352/240, 128] should be the output of feature learning network\n        self.input = input\n        self.training = training\n        # groundtruth(target) - each anchor box, represent as \xe2\x96\xb3x, \xe2\x96\xb3y, \xe2\x96\xb3z, \xe2\x96\xb3l, \xe2\x96\xb3w, \xe2\x96\xb3h, rotation\n        self.targets = tf.placeholder(\n            tf.float32, [None, cfg.FEATURE_HEIGHT, cfg.FEATURE_WIDTH, 14])\n        # postive anchors equal to one and others equal to zero(2 anchors in 1 position)\n        self.pos_equal_one = tf.placeholder(\n            tf.float32, [None, cfg.FEATURE_HEIGHT, cfg.FEATURE_WIDTH, 2])\n        self.pos_equal_one_sum = tf.placeholder(tf.float32, [None, 1, 1, 1])\n        self.pos_equal_one_for_reg = tf.placeholder(\n            tf.float32, [None, cfg.FEATURE_HEIGHT, cfg.FEATURE_WIDTH, 14])\n        # negative anchors equal to one and others equal to zero\n        self.neg_equal_one = tf.placeholder(\n            tf.float32, [None, cfg.FEATURE_HEIGHT, cfg.FEATURE_WIDTH, 2])\n        self.neg_equal_one_sum = tf.placeholder(tf.float32, [None, 1, 1, 1])\n\n        with tf.variable_scope(\'MiddleAndRPN_\' + name):\n            # convolutinal middle layers\n            temp_conv = ConvMD(3, 128, 64, 3, (2, 1, 1),\n                               (1, 1, 1), self.input, name=\'conv1\')\n            temp_conv = ConvMD(3, 64, 64, 3, (1, 1, 1),\n                               (0, 1, 1), temp_conv, name=\'conv2\')\n            temp_conv = ConvMD(3, 64, 64, 3, (2, 1, 1),\n                               (1, 1, 1), temp_conv, name=\'conv3\')\n            temp_conv = tf.transpose(temp_conv, perm=[0, 2, 3, 4, 1])\n            temp_conv = tf.reshape(\n                temp_conv, [-1, cfg.INPUT_HEIGHT, cfg.INPUT_WIDTH, 128])\n\n            # rpn\n            # block1:\n            temp_conv = ConvMD(2, 128, 128, 3, (2, 2), (1, 1),\n                               temp_conv, training=self.training, name=\'conv4\')\n            temp_conv = ConvMD(2, 128, 128, 3, (1, 1), (1, 1),\n                               temp_conv, training=self.training, name=\'conv5\')\n            temp_conv = ConvMD(2, 128, 128, 3, (1, 1), (1, 1),\n                               temp_conv, training=self.training, name=\'conv6\')\n            temp_conv = ConvMD(2, 128, 128, 3, (1, 1), (1, 1),\n                               temp_conv, training=self.training, name=\'conv7\')\n            deconv1 = Deconv2D(128, 256, 3, (1, 1), (0, 0),\n                               temp_conv, training=self.training, name=\'deconv1\')\n\n            # block2:\n            temp_conv = ConvMD(2, 128, 128, 3, (2, 2), (1, 1),\n                               temp_conv, training=self.training, name=\'conv8\')\n            temp_conv = ConvMD(2, 128, 128, 3, (1, 1), (1, 1),\n                               temp_conv, training=self.training, name=\'conv9\')\n            temp_conv = ConvMD(2, 128, 128, 3, (1, 1), (1, 1),\n                               temp_conv, training=self.training, name=\'conv10\')\n            temp_conv = ConvMD(2, 128, 128, 3, (1, 1), (1, 1),\n                               temp_conv, training=self.training, name=\'conv11\')\n            temp_conv = ConvMD(2, 128, 128, 3, (1, 1), (1, 1),\n                               temp_conv, training=self.training, name=\'conv12\')\n            temp_conv = ConvMD(2, 128, 128, 3, (1, 1), (1, 1),\n                               temp_conv, training=self.training, name=\'conv13\')\n            deconv2 = Deconv2D(128, 256, 2, (2, 2), (0, 0),\n                               temp_conv, training=self.training, name=\'deconv2\')\n\n            # block3:\n            temp_conv = ConvMD(2, 128, 256, 3, (2, 2), (1, 1),\n                               temp_conv, training=self.training, name=\'conv14\')\n            temp_conv = ConvMD(2, 256, 256, 3, (1, 1), (1, 1),\n                               temp_conv, training=self.training, name=\'conv15\')\n            temp_conv = ConvMD(2, 256, 256, 3, (1, 1), (1, 1),\n                               temp_conv, training=self.training, name=\'conv16\')\n            temp_conv = ConvMD(2, 256, 256, 3, (1, 1), (1, 1),\n                               temp_conv, training=self.training, name=\'conv17\')\n            temp_conv = ConvMD(2, 256, 256, 3, (1, 1), (1, 1),\n                               temp_conv, training=self.training, name=\'conv18\')\n            temp_conv = ConvMD(2, 256, 256, 3, (1, 1), (1, 1),\n                               temp_conv, training=self.training, name=\'conv19\')\n            deconv3 = Deconv2D(256, 256, 4, (4, 4), (0, 0),\n                               temp_conv, training=self.training, name=\'deconv3\')\n\n            # final:\n            temp_conv = tf.concat([deconv3, deconv2, deconv1], -1)\n            # Probability score map, scale = [None, 200/100, 176/120, 2]\n            p_map = ConvMD(2, 768, 2, 1, (1, 1), (0, 0), temp_conv, activation=False,\n                           training=self.training, name=\'conv20\')\n            # Regression(residual) map, scale = [None, 200/100, 176/120, 14]\n            r_map = ConvMD(2, 768, 14, 1, (1, 1), (0, 0),\n                           temp_conv, training=self.training, activation=False, name=\'conv21\')\n            # softmax output for positive anchor and negative anchor, scale = [None, 200/100, 176/120, 1]\n            self.p_pos = tf.sigmoid(p_map)\n            self.output_shape = [cfg.FEATURE_HEIGHT, cfg.FEATURE_WIDTH]\n\n            self.cls_loss = alpha * (-self.pos_equal_one * tf.log(self.p_pos + small_addon_for_BCE)) / self.pos_equal_one_sum \\\n                + beta * (-self.neg_equal_one * tf.log(1 - self.p_pos +\n                                                       small_addon_for_BCE)) / self.neg_equal_one_sum\n            self.cls_loss = tf.reduce_sum(self.cls_loss)\n\n            self.reg_loss = smooth_l1(r_map * self.pos_equal_one_for_reg, self.targets *\n                                      self.pos_equal_one_for_reg, sigma) / self.pos_equal_one_sum\n            self.reg_loss = tf.reduce_sum(self.reg_loss)\n\n            self.loss = tf.reduce_sum(self.cls_loss + self.reg_loss)\n\n            self.delta_output = r_map\n            self.prob_output = self.p_pos\n\n\ndef smooth_l1(deltas, targets, sigma=3.0):\n    sigma2 = sigma * sigma\n    diffs = tf.subtract(deltas, targets)\n    smooth_l1_signs = tf.cast(tf.less(tf.abs(diffs), 1.0 / sigma2), tf.float32)\n\n    smooth_l1_option1 = tf.multiply(diffs, diffs) * 0.5 * sigma2\n    smooth_l1_option2 = tf.abs(diffs) - 0.5 / sigma2\n    smooth_l1_add = tf.multiply(smooth_l1_option1, smooth_l1_signs) + \\\n        tf.multiply(smooth_l1_option2, 1 - smooth_l1_signs)\n    smooth_l1 = smooth_l1_add\n\n    return smooth_l1\n\n\ndef ConvMD(M, Cin, Cout, k, s, p, input, training=True, activation=True, name=\'conv\'):\n    temp_p = np.array(p)\n    temp_p = np.lib.pad(temp_p, (1, 1), \'constant\', constant_values=(0, 0))\n    with tf.variable_scope(name) as scope:\n        if(M == 2):\n            paddings = (np.array(temp_p)).repeat(2).reshape(4, 2)\n            pad = tf.pad(input, paddings, ""CONSTANT"")\n            temp_conv = tf.layers.conv2d(\n                pad, Cout, k, strides=s, padding=""valid"", reuse=tf.AUTO_REUSE, name=scope)\n        if(M == 3):\n            paddings = (np.array(temp_p)).repeat(2).reshape(5, 2)\n            pad = tf.pad(input, paddings, ""CONSTANT"")\n            temp_conv = tf.layers.conv3d(\n                pad, Cout, k, strides=s, padding=""valid"", reuse=tf.AUTO_REUSE, name=scope)\n        temp_conv = tf.layers.batch_normalization(\n            temp_conv, axis=-1, fused=True, training=training, reuse=tf.AUTO_REUSE, name=scope)\n        if activation:\n            return tf.nn.relu(temp_conv)\n        else:\n            return temp_conv\n\ndef Deconv2D(Cin, Cout, k, s, p, input, training=True, name=\'deconv\'):\n    temp_p = np.array(p)\n    temp_p = np.lib.pad(temp_p, (1, 1), \'constant\', constant_values=(0, 0))\n    paddings = (np.array(temp_p)).repeat(2).reshape(4, 2)\n    pad = tf.pad(input, paddings, ""CONSTANT"")\n    with tf.variable_scope(name) as scope:\n        temp_conv = tf.layers.conv2d_transpose(\n            pad, Cout, k, strides=s, padding=""SAME"", reuse=tf.AUTO_REUSE, name=scope)\n        temp_conv = tf.layers.batch_normalization(\n            temp_conv, axis=-1, fused=True, training=training, reuse=tf.AUTO_REUSE, name=scope)\n        return tf.nn.relu(temp_conv)\n\n\nif(__name__ == ""__main__""):\n    m = MiddleAndRPN(tf.placeholder(\n        tf.float32, [None, 10, cfg.INPUT_HEIGHT, cfg.INPUT_WIDTH, 128]))\n'"
utils/__init__.py,0,b'#!/usr/bin/env python\n# -*- coding:UTF-8 -*-\n\n# File Name : __init__.py\n# Purpose :\n# Creation Date : 21-12-2017\n# Last Modified : Fri 19 Jan 2018 10:15:06 AM CST\n# Created By : Jeasine Ma [jeasinema[at]gmail[dot]com]\n\nfrom utils.box_overlaps import *\nfrom utils.colorize import *\nfrom utils.kitti_loader import *\nfrom utils.utils import *\nfrom utils.preprocess import *\nfrom utils.data_aug import *\n'
utils/colorize.py,10,"b'#!/usr/bin/env python\n# -*- coding:UTF-8 -*-\n\n# File Name : colorize.py\n# Purpose :\n# Creation Date : 21-12-2017\n# Last Modified : Thu 21 Dec 2017 09:02:22 PM CST\n# Created By : Jeasine Ma [jeasinema[at]gmail[dot]com]\n\n# ref: https://gist.github.com/jimfleming/c1adfdb0f526465c99409cc143dea97b\n\nimport matplotlib\nimport matplotlib.cm\nimport cv2\nimport numpy as np\n\nimport tensorflow as tf\n\n\ndef colorize(value, factor=1, vmin=None, vmax=None):\n    """"""\n    A utility function for TensorFlow that maps a grayscale image to a matplotlib\n    colormap for use with TensorBoard image summaries.\n\n    By default it will normalize the input value to the range 0..1 before mapping\n    to a grayscale colormap.\n\n    Arguments:\n      - value: 2D Tensor of shape [height, width] or 3D Tensor of shape\n        [height, width, 1].\n      - factor: resize factor, scalar\n      - vmin: the minimum value of the range used for normalization.\n        (Default: value minimum)\n      - vmax: the maximum value of the range used for normalization.\n        (Default: value maximum)\n\n    Example usage:\n\n    ```\n    output = tf.random_uniform(shape=[256, 256, 1])\n    output_color = colorize(output, vmin=0.0, vmax=1.0, cmap=\'viridis\')\n    tf.summary.image(\'output\', output_color)\n    ```\n\n    Returns a 3D tensor of shape [height, width, 3].\n    """"""\n\n    # normalize\n    value = np.sum(value, axis=-1)\n    vmin = np.min(value) if vmin is None else vmin\n    vmax = np.max(value) if vmax is None else vmax\n    value = (value - vmin) / (vmax - vmin)  # vmin..vmax\n\n    value = (value * 255).astype(np.uint8)\n    value = cv2.applyColorMap(value, cv2.COLORMAP_JET)\n    value = cv2.cvtColor(value, cv2.COLOR_BGR2RGB)\n    x, y, _ = value.shape\n    value = cv2.resize(value, (y * factor, x * factor))\n\n    return value\n\n\ndef tf_colorize(value, factor=1, vmin=None, vmax=None, cmap=None):\n    """"""\n    A utility function for TensorFlow that maps a grayscale image to a matplotlib\n    colormap for use with TensorBoard image summaries.\n\n    By default it will normalize the input value to the range 0..1 before mapping\n    to a grayscale colormap.\n\n    Arguments:\n      - value: 2D Tensor of shape [height, width] or 3D Tensor of shape\n        [height, width, 1].\n      - factor: resize factor, scalar\n      - vmin: the minimum value of the range used for normalization.\n        (Default: value minimum)\n      - vmax: the maximum value of the range used for normalization.\n        (Default: value maximum)\n      - cmap: a valid cmap named for use with matplotlib\'s `get_cmap`.\n        (Default: \'gray\')\n\n    Example usage:\n\n    ```\n    output = tf.random_uniform(shape=[256, 256, 1])\n    output_color = colorize(output, vmin=0.0, vmax=1.0, cmap=\'viridis\')\n    tf.summary.image(\'output\', output_color)\n    ```\n\n    Returns a 3D tensor of shape [height, width, 3].\n    """"""\n\n    # normalize\n    vmin = tf.reduce_min(value) if vmin is None else vmin\n    vmax = tf.reduce_max(value) if vmax is None else vmax\n    value = (value - vmin) / (vmax - vmin)  # vmin..vmax\n\n    # squeeze last dim if it exists\n    value = tf.squeeze(value)\n\n    # quantize\n    indices = tf.to_int32(tf.round(value * 255))\n\n    # gather\n    cm = matplotlib.cm.get_cmap(cmap if cmap is not None else \'gray\')\n    colors = tf.constant(cm.colors, dtype=tf.float32)\n    value = tf.gather(colors, indices)\n\n    return value\n'"
utils/data_aug.py,0,"b""#!/usr/bin/env python\n# -*- coding:UTF-8 -*-\n\n# File Name : data_aug.py\n# Purpose :\n# Creation Date : 21-12-2017\n# Last Modified : Fri 19 Jan 2018 01:06:35 PM CST\n# Created By : Jeasine Ma [jeasinema[at]gmail[dot]com]\n\nimport numpy as np\nimport cv2\nimport os\nimport multiprocessing as mp\nimport argparse\nimport glob\n\nfrom utils.utils import *\nfrom utils.preprocess import *\n\nobject_dir = './data/object'\n\n\ndef aug_data(tag, object_dir):\n    np.random.seed()\n    rgb = cv2.resize(cv2.imread(os.path.join(object_dir,\n                                             'image_2', tag + '.png')), (cfg.IMAGE_WIDTH, cfg.IMAGE_HEIGHT))\n    lidar = np.fromfile(os.path.join(object_dir,\n                                     'velodyne', tag + '.bin'), dtype=np.float32).reshape(-1, 4)\n    label = np.array([line for line in open(os.path.join(\n        object_dir, 'label_2', tag + '.txt'), 'r').readlines()])  # (N')\n    cls = np.array([line.split()[0] for line in label])  # (N')\n    gt_box3d = label_to_gt_box3d(np.array(label)[np.newaxis, :], cls='', coordinate='camera')[\n        0]  # (N', 7) x, y, z, h, w, l, r\n\n    choice = np.random.randint(1, 10)\n    if choice >= 7:\n        lidar_center_gt_box3d = camera_to_lidar_box(gt_box3d)\n        lidar_corner_gt_box3d = center_to_corner_box3d(\n            lidar_center_gt_box3d, coordinate='lidar')\n        for idx in range(len(lidar_corner_gt_box3d)):\n            # TODO: precisely gather the point\n            is_collision = True\n            _count = 0\n            while is_collision and _count < 100:\n                t_rz = np.random.uniform(-np.pi / 10, np.pi / 10)\n                t_x = np.random.normal()\n                t_y = np.random.normal()\n                t_z = np.random.normal()\n                # check collision\n                tmp = box_transform(\n                    lidar_center_gt_box3d[[idx]], t_x, t_y, t_z, t_rz, 'lidar')\n                is_collision = False\n                for idy in range(idx):\n                    x1, y1, w1, l1, r1 = tmp[0][[0, 1, 4, 5, 6]]\n                    x2, y2, w2, l2, r2 = lidar_center_gt_box3d[idy][[\n                        0, 1, 4, 5, 6]]\n                    iou = cal_iou2d(np.array([x1, y1, w1, l1, r1], dtype=np.float32),\n                                    np.array([x2, y2, w2, l2, r2], dtype=np.float32))\n                    if iou > 0:\n                        is_collision = True\n                        _count += 1\n                        break\n            if not is_collision:\n                box_corner = lidar_corner_gt_box3d[idx]\n                minx = np.min(box_corner[:, 0])\n                miny = np.min(box_corner[:, 1])\n                minz = np.min(box_corner[:, 2])\n                maxx = np.max(box_corner[:, 0])\n                maxy = np.max(box_corner[:, 1])\n                maxz = np.max(box_corner[:, 2])\n                bound_x = np.logical_and(\n                    lidar[:, 0] >= minx, lidar[:, 0] <= maxx)\n                bound_y = np.logical_and(\n                    lidar[:, 1] >= miny, lidar[:, 1] <= maxy)\n                bound_z = np.logical_and(\n                    lidar[:, 2] >= minz, lidar[:, 2] <= maxz)\n                bound_box = np.logical_and(\n                    np.logical_and(bound_x, bound_y), bound_z)\n                lidar[bound_box, 0:3] = point_transform(\n                    lidar[bound_box, 0:3], t_x, t_y, t_z, rz=t_rz)\n                lidar_center_gt_box3d[idx] = box_transform(\n                    lidar_center_gt_box3d[[idx]], t_x, t_y, t_z, t_rz, 'lidar')\n\n        gt_box3d = lidar_to_camera_box(lidar_center_gt_box3d)\n        newtag = 'aug_{}_1_{}'.format(\n            tag, np.random.randint(1, 1024))\n    elif choice < 7 and choice >= 4:\n        # global rotation\n        angle = np.random.uniform(-np.pi / 4, np.pi / 4)\n        lidar[:, 0:3] = point_transform(lidar[:, 0:3], 0, 0, 0, rz=angle)\n        lidar_center_gt_box3d = camera_to_lidar_box(gt_box3d)\n        lidar_center_gt_box3d = box_transform(lidar_center_gt_box3d, 0, 0, 0, r=angle, coordinate='lidar')\n        gt_box3d = lidar_to_camera_box(lidar_center_gt_box3d)\n        newtag = 'aug_{}_2_{:.4f}'.format(tag, angle).replace('.', '_')\n    else:\n        # global scaling\n        factor = np.random.uniform(0.95, 1.05)\n        lidar[:, 0:3] = lidar[:, 0:3] * factor\n        lidar_center_gt_box3d = camera_to_lidar_box(gt_box3d)\n        lidar_center_gt_box3d[:, 0:6] = lidar_center_gt_box3d[:, 0:6] * factor\n        gt_box3d = lidar_to_camera_box(lidar_center_gt_box3d)\n        newtag = 'aug_{}_3_{:.4f}'.format(tag, factor).replace('.', '_')\n\n    label = box3d_to_label(gt_box3d[np.newaxis, ...], cls[np.newaxis, ...], coordinate='camera')[0]  # (N')\n    voxel_dict = process_pointcloud(lidar)\n    return newtag, rgb, lidar, voxel_dict, label \n\n\ndef worker(tag):\n    new_tag, rgb, lidar, voxel_dict, label = aug_data(tag)\n    output_path = os.path.join(object_dir, 'training_aug')\n\n    cv2.imwrite(os.path.join(output_path, 'image_2', newtag + '.png'), rgb)\n    lidar.reshape(-1).tofile(os.path.join(output_path,\n                                          'velodyne', newtag + '.bin'))\n    np.savez_compressed(os.path.join(\n        output_path, 'voxel' if cfg.DETECT_OBJ == 'Car' else 'voxel_ped', newtag), **voxel_dict)\n    with open(os.path.join(output_path, 'label_2', newtag + '.txt'), 'w+') as f:\n        for line in label:\n            f.write(line)\n    print(newtag)\n\n\ndef main():\n    fl = glob.glob(os.path.join(object_dir, 'training', 'calib', '*.txt'))\n    candidate = [f.split('/')[-1].split('.')[0] for f in fl]\n    tags = []\n    for _ in range(args.aug_amount):\n        tags.append(candidate[np.random.randint(0, len(candidate))])\n    print('generate {} tags'.format(len(tags)))\n    pool = mp.Pool(args.num_workers)\n    pool.map(worker, tags)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='')\n    parser.add_argument('-i', '--aug-amount', type=int, nargs='?', default=1000)\n    parser.add_argument('-n', '--num-workers', type=int, nargs='?', default=10)\n    args = parser.parse_args()\n\n    main()\n"""
utils/kitti_loader.py,0,"b'#!/usr/bin/env python\n# -*- coding:UTF-8 -*-\n\n# File Name : kitti_loader.py\n# Purpose :\n# Creation Date : 09-12-2017\n# Last Modified : Fri 19 Jan 2018 03:11:15 PM CST\n# Created By : Jeasine Ma [jeasinema[at]gmail[dot]com]\n\nimport cv2\nimport numpy as np\nimport os\nimport sys\nimport glob\nimport threading\nimport time\nimport math\nimport random\nfrom sklearn.utils import shuffle\nfrom multiprocessing import Lock, Process, Queue as Queue, Value, Array, cpu_count\n\nfrom config import cfg\nfrom utils.data_aug import aug_data\nfrom utils.preprocess import process_pointcloud\n\n# for non-raw dataset\n\n\nclass KittiLoader(object):\n\n    # return:\n    # tag (N)\n    # label (N) (N\')\n    # rgb (N, H, W, C)\n    # raw_lidar (N) (N\', 4)\n    # vox_feature\n    # vox_number\n    # vox_coordinate\n\n    def __init__(self, object_dir=\'.\', queue_size=20, require_shuffle=False, is_testset=True, batch_size=1, use_multi_process_num=0, split_file=\'\', multi_gpu_sum=1, aug=False):\n        assert(use_multi_process_num >= 0)\n        self.object_dir = object_dir\n        self.is_testset = is_testset\n        self.use_multi_process_num = use_multi_process_num if not self.is_testset else 1\n        self.require_shuffle = require_shuffle if not self.is_testset else False\n        self.batch_size = batch_size\n        self.split_file = split_file\n        self.multi_gpu_sum = multi_gpu_sum\n        self.aug = aug\n\n        if self.split_file != \'\':\n            # use split file\n            _tag = []\n            self.f_rgb, self.f_lidar, self.f_label = [], [], []\n            for line in open(self.split_file, \'r\').readlines():\n                line = line[:-1]  # remove \'\\n\'\n                _tag.append(line)\n                self.f_rgb.append(os.path.join(\n                    self.object_dir, \'image_2\', line + \'.png\'))\n                self.f_lidar.append(os.path.join(\n                    self.object_dir, \'velodyne\', line + \'.bin\'))\n                self.f_label.append(os.path.join(\n                    self.object_dir, \'label_2\', line + \'.txt\'))\n        else:\n            self.f_rgb = glob.glob(os.path.join(\n                self.object_dir, \'image_2\', \'*.png\'))\n            self.f_rgb.sort()\n            self.f_lidar = glob.glob(os.path.join(\n                self.object_dir, \'velodyne\', \'*.bin\'))\n            self.f_lidar.sort()\n            self.f_label = glob.glob(os.path.join(\n                self.object_dir, \'label_2\', \'*.txt\'))\n            self.f_label.sort()\n\n        self.data_tag = [name.split(\'/\')[-1].split(\'.\')[-2]\n                         for name in self.f_rgb]\n        assert(len(self.data_tag) == len(self.f_rgb) == len(self.f_lidar))\n        self.dataset_size = len(self.f_rgb)\n        self.already_extract_data = 0\n        self.cur_frame_info = \'\'\n\n        print(""Dataset total length: {}"".format(self.dataset_size))\n        if self.require_shuffle:\n            self.shuffle_dataset()\n\n        self.queue_size = queue_size\n        self.require_shuffle = require_shuffle\n        # must use the queue provided by multiprocessing module(only this can be shared)\n        self.dataset_queue = Queue()\n\n        self.load_index = 0\n        if self.use_multi_process_num == 0:\n            self.loader_worker = [threading.Thread(\n                target=self.loader_worker_main, args=(self.batch_size,))]\n        else:\n            self.loader_worker = [Process(target=self.loader_worker_main, args=(\n                self.batch_size,)) for i in range(self.use_multi_process_num)]\n        self.work_exit = Value(\'i\', 0)\n        [i.start() for i in self.loader_worker]\n\n        # This operation is not thread-safe\n        self.rgb_shape = (cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH, 3)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.work_exit.value = True\n\n    def __len__(self):\n        return self.dataset_size\n\n    def fill_queue(self, batch_size=0):\n        load_index = self.load_index\n        self.load_index += batch_size\n        if self.load_index >= self.dataset_size:\n            if not self.is_testset:  # test set just end\n                if self.require_shuffle:\n                    self.shuffle_dataset()\n                load_index = 0\n                self.load_index = load_index + batch_size\n            else:\n                self.work_exit.value = True\n\n        labels, tag, voxel, rgb, raw_lidar = [], [], [], [], []\n        for _ in range(batch_size):\n            try:\n                if self.aug:\n                    ret = aug_data(self.data_tag[load_index], self.object_dir)\n                    tag.append(ret[0])\n                    rgb.append(ret[1])\n                    raw_lidar.append(ret[2])\n                    voxel.append(ret[3])\n                    labels.append(ret[4])\n                else:\n                    rgb.append(cv2.resize(cv2.imread(\n                        self.f_rgb[load_index]), (cfg.IMAGE_WIDTH, cfg.IMAGE_HEIGHT)))\n                    raw_lidar.append(np.fromfile(\n                        self.f_lidar[load_index], dtype=np.float32).reshape((-1, 4)))\n                    if not self.is_testset:\n                        labels.append([line for line in open(\n                            self.f_label[load_index], \'r\').readlines()])\n                    else:\n                        labels.append([\'\'])\n                    tag.append(self.data_tag[load_index])\n                    voxel.append(process_pointcloud(raw_lidar[-1]))\n\n                load_index += 1\n            except:\n                if not self.is_testset:  # test set just end\n                    self.load_index = 0\n                    if self.require_shuffle:\n                        self.shuffle_dataset()\n                else:\n                    self.work_exit.value = True\n\n        # only for voxel -> [gpu, k_single_batch, ...]\n        vox_feature, vox_number, vox_coordinate = [], [], []\n        single_batch_size = int(self.batch_size / self.multi_gpu_sum)\n        for idx in range(self.multi_gpu_sum):\n            _, per_vox_feature, per_vox_number, per_vox_coordinate = build_input(\n                voxel[idx * single_batch_size:(idx + 1) * single_batch_size])\n            vox_feature.append(per_vox_feature)\n            vox_number.append(per_vox_number)\n            vox_coordinate.append(per_vox_coordinate)\n\n        self.dataset_queue.put_nowait(\n            (labels, (vox_feature, vox_number, vox_coordinate), rgb, raw_lidar, tag))\n\n    def load(self):\n        try:\n            if self.is_testset and self.already_extract_data >= self.dataset_size:\n                return None\n\n            buff = self.dataset_queue.get()\n            label = buff[0]\n            vox_feature = buff[1][0]\n            vox_number = buff[1][1]\n            vox_coordinate = buff[1][2]\n            rgb = buff[2]\n            raw_lidar = buff[3]\n            tag = buff[4]\n            self.cur_frame_info = buff[4]\n\n            self.already_extract_data += self.batch_size\n\n            ret = (\n                np.array(tag),\n                np.array(label),\n                np.array(vox_feature),\n                np.array(vox_number),\n                np.array(vox_coordinate),\n                np.array(rgb),\n                np.array(raw_lidar)\n\n            )\n        except:\n            print(""Dataset empty!"")\n            ret = None\n        return ret\n\n    def load_specified(self, index=0):\n        rgb = cv2.resize(cv2.imread(\n            self.f_rgb[index]), (cfg.IMAGE_WIDTH, cfg.IMAGE_HEIGHT))\n        raw_lidar = np.fromfile(\n            self.f_lidar[index], dtype=np.float32).reshape((-1, 4))\n        labels = [line for line in open(self.f_label[index], \'r\').readlines()]\n        tag = self.data_tag[index]\n\n        if self.is_testset:\n            ret = (\n                np.array([tag]),\n                np.array([rgb]),\n                np.array([raw_lidar]),\n            )\n        else:\n            ret = (\n                np.array([tag]),\n                np.array([labels]),\n                np.array([rgb]),\n                np.array([raw_lidar]),\n            )\n        return ret\n\n    def loader_worker_main(self, batch_size):\n        if self.require_shuffle:\n            self.shuffle_dataset()\n        while not self.work_exit.value:\n            if self.dataset_queue.qsize() >= self.queue_size // 2:\n                time.sleep(1)\n            else:\n                # since we use multiprocessing, 1 is ok\n                self.fill_queue(batch_size)\n\n    def get_shape(self):\n        return self.rgb_shape\n\n    def shuffle_dataset(self):\n        # to prevent diff loader load same data\n        index = shuffle([i for i in range(len(self.f_label))],\n                        random_state=random.randint(0, self.use_multi_process_num**5))\n        self.f_label = [self.f_label[i] for i in index]\n        self.f_rgb = [self.f_rgb[i] for i in index]\n        self.f_lidar = [self.f_lidar[i] for i in index]\n        self.data_tag = [self.data_tag[i] for i in index]\n\n    def get_frame_info(self):\n        return self.cur_frame_info\n\n\ndef build_input(voxel_dict_list):\n    batch_size = len(voxel_dict_list)\n\n    feature_list = []\n    number_list = []\n    coordinate_list = []\n    for i, voxel_dict in zip(range(batch_size), voxel_dict_list):\n        feature_list.append(voxel_dict[\'feature_buffer\'])\n        number_list.append(voxel_dict[\'number_buffer\'])\n        coordinate = voxel_dict[\'coordinate_buffer\']\n        coordinate_list.append(\n            np.pad(coordinate, ((0, 0), (1, 0)),\n                   mode=\'constant\', constant_values=i))\n\n    feature = np.concatenate(feature_list)\n    number = np.concatenate(number_list)\n    coordinate = np.concatenate(coordinate_list)\n    return batch_size, feature, number, coordinate\n\n\nif __name__ == \'__main__\':\n    pass\n'"
utils/preprocess.py,0,"b""#!/usr/bin/env python\n# -*- coding:UTF-8 -*-\n\n# File Name : preprocess.py\n# Purpose :\n# Creation Date : 10-12-2017\n# Last Modified : Thu 18 Jan 2018 05:34:42 PM CST\n# Created By : Jeasine Ma [jeasinema[at]gmail[dot]com]\n\nimport os\nimport multiprocessing\nimport numpy as np\n\nfrom config import cfg\n\ndata_dir = 'velodyne'\n\ndef process_pointcloud(point_cloud, cls=cfg.DETECT_OBJ):\n    # Input:\n    #   (N, 4)\n    # Output:\n    #   voxel_dict\n    if cls == 'Car':\n        scene_size = np.array([4, 80, 70.4], dtype=np.float32)\n        voxel_size = np.array([0.4, 0.2, 0.2], dtype=np.float32)\n        grid_size = np.array([10, 400, 352], dtype=np.int64)\n        lidar_coord = np.array([0, 40, 3], dtype=np.float32)\n        max_point_number = 35\n    else:\n        scene_size = np.array([4, 40, 48], dtype=np.float32)\n        voxel_size = np.array([0.4, 0.2, 0.2], dtype=np.float32)\n        grid_size = np.array([10, 200, 240], dtype=np.int64)\n        lidar_coord = np.array([0, 20, 3], dtype=np.float32)\n        max_point_number = 45\n\n    np.random.shuffle(point_cloud)\n\n    shifted_coord = point_cloud[:, :3] + lidar_coord\n    # reverse the point cloud coordinate (X, Y, Z) -> (Z, Y, X)\n    voxel_index = np.floor(\n        shifted_coord[:, ::-1] / voxel_size).astype(np.int)\n\n    bound_x = np.logical_and(\n        voxel_index[:, 2] >= 0, voxel_index[:, 2] < grid_size[2])\n    bound_y = np.logical_and(\n        voxel_index[:, 1] >= 0, voxel_index[:, 1] < grid_size[1])\n    bound_z = np.logical_and(\n        voxel_index[:, 0] >= 0, voxel_index[:, 0] < grid_size[0])\n\n    bound_box = np.logical_and(np.logical_and(bound_x, bound_y), bound_z)\n\n    point_cloud = point_cloud[bound_box]\n    voxel_index = voxel_index[bound_box]\n\n    # [K, 3] coordinate buffer as described in the paper\n    coordinate_buffer = np.unique(voxel_index, axis=0)\n\n    K = len(coordinate_buffer)\n    T = max_point_number\n\n    # [K, 1] store number of points in each voxel grid\n    number_buffer = np.zeros(shape=(K), dtype=np.int64)\n\n    # [K, T, 7] feature buffer as described in the paper\n    feature_buffer = np.zeros(shape=(K, T, 7), dtype=np.float32)\n\n    # build a reverse index for coordinate buffer\n    index_buffer = {}\n    for i in range(K):\n        index_buffer[tuple(coordinate_buffer[i])] = i\n\n    for voxel, point in zip(voxel_index, point_cloud):\n        index = index_buffer[tuple(voxel)]\n        number = number_buffer[index]\n        if number < T:\n            feature_buffer[index, number, :4] = point\n            number_buffer[index] += 1\n\n    feature_buffer[:, :, -3:] = feature_buffer[:, :, :3] - \\\n        feature_buffer[:, :, :3].sum(axis=1, keepdims=True)/number_buffer.reshape(K, 1, 1)\n\n    voxel_dict = {'feature_buffer': feature_buffer,\n                  'coordinate_buffer': coordinate_buffer,\n                  'number_buffer': number_buffer}\n    return voxel_dict\n\n\ndef worker(filelist):\n    for file in filelist:\n        point_cloud = np.fromfile(\n            os.path.join(data_dir, file), dtype=np.float32).reshape(-1, 4)\n\n        name, extension = os.path.splitext(file)\n        voxel_dict = process_pointcloud(point_cloud)\n        output_dir = 'voxel' if cfg.DETECT_OBJ == 'Car' else 'voxel_ped'\n        np.savez_compressed(os.path.join(output_dir, name), **voxel_dict)\n\n\nif __name__ == '__main__':\n    filelist = [f for f in os.listdir(data_dir) if f.endswith('bin')]\n    num_worker = 8\n    for sublist in np.array_split(filelist, num_worker):\n        p = multiprocessing.Process(target=worker, args=(sublist,))\n        p.start()\n"""
utils/setup.py,0,"b""#!/usr/bin/env python\n# -*- coding:UTF-8 -*-\n\n# File Name : setup.py\n# Purpose :\n# Creation Date : 11-12-2017\n# Last Modified : Sat 23 Dec 2017 03:19:46 PM CST\n# Created By : Jeasine Ma [jeasinema[at]gmail[dot]com]\n\n\nfrom distutils.core import setup\nfrom Cython.Build import cythonize\n\nsetup(\n    name='box overlaps',\n    ext_modules=cythonize('./utils/box_overlaps.pyx')\n)\n"""
utils/utils.py,0,"b""\n# -*- cooing:UTF-8 -*-\n\n# File Name : utils.py\n# Purpose :\n# Creation Date : 09-12-2017\n# Last Modified : Thu 08 Mar 2018 02:30:56 PM CST\n# Created By : Jeasine Ma [jeasinema[at]gmail[dot]com]\n\nimport cv2\nimport numpy as np\nimport shapely.geometry\nimport shapely.affinity\nimport math\nfrom numba import jit\n\nfrom config import cfg\nfrom utils.box_overlaps import *\n\n\ndef lidar_to_bird_view(x, y, factor=1):\n    # using the cfg.INPUT_XXX\n    a = (x - cfg.X_MIN) / cfg.VOXEL_X_SIZE * factor\n    b = (y - cfg.Y_MIN) / cfg.VOXEL_Y_SIZE * factor\n    a = np.clip(a, a_max=(cfg.X_MAX - cfg.X_MIN) / cfg.VOXEL_X_SIZE * factor, a_min=0)\n    b = np.clip(b, a_max=(cfg.Y_MAX - cfg.Y_MIN) / cfg.VOXEL_Y_SIZE * factor, a_min=0)\n    return a, b\n\ndef batch_lidar_to_bird_view(points, factor=1):\n    # Input:\n    #   points (N, 2)\n    # Outputs:\n    #   points (N, 2)\n    # using the cfg.INPUT_XXX\n    a = (points[:, 0] - cfg.X_MIN) / cfg.VOXEL_X_SIZE * factor\n    b = (points[:, 1] - cfg.Y_MIN) / cfg.VOXEL_Y_SIZE * factor\n    a = np.clip(a, a_max=(cfg.X_MAX - cfg.X_MIN) / cfg.VOXEL_X_SIZE * factor, a_min=0)\n    b = np.clip(b, a_max=(cfg.Y_MAX - cfg.Y_MIN) / cfg.VOXEL_Y_SIZE * factor, a_min=0)\n    return np.concatenate([a[:, np.newaxis], b[:, np.newaxis]], axis=-1)\n\n\ndef angle_in_limit(angle):\n    # To limit the angle in -pi/2 - pi/2\n    limit_degree = 5\n    while angle >= np.pi / 2:\n        angle -= np.pi\n    while angle < -np.pi / 2:\n        angle += np.pi\n    if abs(angle + np.pi / 2) < limit_degree / 180 * np.pi:\n        angle = np.pi / 2\n    return angle\n\n\ndef camera_to_lidar(x, y, z):\n    p = np.array([x, y, z, 1])\n    p = np.matmul(np.linalg.inv(np.array(cfg.MATRIX_R_RECT_0)), p)\n    p = np.matmul(np.linalg.inv(np.array(cfg.MATRIX_T_VELO_2_CAM)), p)\n    p = p[0:3]\n    return tuple(p)\n\n\ndef lidar_to_camera(x, y, z):\n    p = np.array([x, y, z, 1])\n    p = np.matmul(np.array(cfg.MATRIX_T_VELO_2_CAM), p)\n    p = np.matmul(np.array(cfg.MATRIX_R_RECT_0), p)\n    p = p[0:3]\n    return tuple(p)\n\n\ndef camera_to_lidar_point(points):\n    # (N, 3) -> (N, 3)\n    N = points.shape[0]\n    points = np.hstack([points, np.ones((N, 1))]).T  # (N,4) -> (4,N)\n\n    points = np.matmul(np.linalg.inv(np.array(cfg.MATRIX_R_RECT_0)), points)\n    points = np.matmul(np.linalg.inv(\n        np.array(cfg.MATRIX_T_VELO_2_CAM)), points).T  # (4, N) -> (N, 4)\n    points = points[:, 0:3]\n    return points.reshape(-1, 3)\n\n\ndef lidar_to_camera_point(points):\n    # (N, 3) -> (N, 3)\n    N = points.shape[0]\n    points = np.hstack([points, np.ones((N, 1))]).T\n\n    points = np.matmul(np.array(cfg.MATRIX_T_VELO_2_CAM), points)\n    points = np.matmul(np.array(cfg.MATRIX_R_RECT_0), points).T\n    points = points[:, 0:3]\n    return points.reshape(-1, 3)\n\n\ndef camera_to_lidar_box(boxes):\n    # (N, 7) -> (N, 7) x,y,z,h,w,l,r\n    ret = []\n    for box in boxes:\n        x, y, z, h, w, l, ry = box\n        (x, y, z), h, w, l, rz = camera_to_lidar(\n            x, y, z), h, w, l, -ry - np.pi / 2\n        rz = angle_in_limit(rz)\n        ret.append([x, y, z, h, w, l, rz])\n    return np.array(ret).reshape(-1, 7)\n\n\ndef lidar_to_camera_box(boxes):\n    # (N, 7) -> (N, 7) x,y,z,h,w,l,r\n    ret = []\n    for box in boxes:\n        x, y, z, h, w, l, rz = box\n        (x, y, z), h, w, l, ry = lidar_to_camera(\n            x, y, z), h, w, l, -rz - np.pi / 2\n        ry = angle_in_limit(ry)\n        ret.append([x, y, z, h, w, l, ry])\n    return np.array(ret).reshape(-1, 7)\n\n\ndef center_to_corner_box2d(boxes_center, coordinate='lidar'):\n    # (N, 5) -> (N, 4, 2)\n    N = boxes_center.shape[0]\n    boxes3d_center = np.zeros((N, 7))\n    boxes3d_center[:, [0, 1, 4, 5, 6]] = boxes_center\n    boxes3d_corner = center_to_corner_box3d(\n        boxes3d_center, coordinate=coordinate)\n\n    return boxes3d_corner[:, 0:4, 0:2]\n\n\ndef center_to_corner_box3d(boxes_center, coordinate='lidar'):\n    # (N, 7) -> (N, 8, 3)\n    N = boxes_center.shape[0]\n    ret = np.zeros((N, 8, 3), dtype=np.float32)\n\n    if coordinate == 'camera':\n        boxes_center = camera_to_lidar_box(boxes_center)\n\n    for i in range(N):\n        box = boxes_center[i]\n        translation = box[0:3]\n        size = box[3:6]\n        rotation = [0, 0, box[-1]]\n\n        h, w, l = size[0], size[1], size[2]\n        trackletBox = np.array([  # in velodyne coordinates around zero point and without orientation yet\n            [-l / 2, -l / 2, l / 2, l / 2, -l / 2, -l / 2, l / 2, l / 2], \\\n            [w / 2, -w / 2, -w / 2, w / 2, w / 2, -w / 2, -w / 2, w / 2], \\\n            [0, 0, 0, 0, h, h, h, h]])\n\n        # re-create 3D bounding box in velodyne coordinate system\n        yaw = rotation[2]\n        rotMat = np.array([\n            [np.cos(yaw), -np.sin(yaw), 0.0],\n            [np.sin(yaw), np.cos(yaw), 0.0],\n            [0.0, 0.0, 1.0]])\n        cornerPosInVelo = np.dot(rotMat, trackletBox) + \\\n            np.tile(translation, (8, 1)).T\n        box3d = cornerPosInVelo.transpose()\n        ret[i] = box3d\n\n    if coordinate == 'camera':\n        for idx in range(len(ret)):\n            ret[idx] = lidar_to_camera_point(ret[idx])\n\n    return ret\n\n\ndef corner_to_center_box2d(boxes_corner, coordinate='lidar'):\n    # (N, 4, 2) -> (N, 5)  x,y,w,l,r\n    N = boxes_corner.shape[0]\n    boxes3d_corner = np.zeros((N, 8, 3))\n    boxes3d_corner[:, 0:4, 0:2] = boxes_corner\n    boxes3d_corner[:, 4:8, 0:2] = boxes_corner\n    boxes3d_center = corner_to_center_box3d(\n        boxes3d_corner, coordinate=coordinate)\n\n    return boxes3d_center[:, [0, 1, 4, 5, 6]]\n\n\ndef corner_to_standup_box2d(boxes_corner):\n    # (N, 4, 2) -> (N, 4) x1, y1, x2, y2\n    N = boxes_corner.shape[0]\n    standup_boxes2d = np.zeros((N, 4))\n    standup_boxes2d[:, 0] = np.min(boxes_corner[:, :, 0], axis=1)\n    standup_boxes2d[:, 1] = np.min(boxes_corner[:, :, 1], axis=1)\n    standup_boxes2d[:, 2] = np.max(boxes_corner[:, :, 0], axis=1)\n    standup_boxes2d[:, 3] = np.max(boxes_corner[:, :, 1], axis=1)\n\n    return standup_boxes2d\n\n\n# TODO: 0/90 may be not correct\ndef anchor_to_standup_box2d(anchors):\n    # (N, 4) -> (N, 4) x,y,w,l -> x1,y1,x2,y2\n    anchor_standup = np.zeros_like(anchors)\n    # r == 0\n    anchor_standup[::2, 0] = anchors[::2, 0] - anchors[::2, 3] / 2\n    anchor_standup[::2, 1] = anchors[::2, 1] - anchors[::2, 2] / 2\n    anchor_standup[::2, 2] = anchors[::2, 0] + anchors[::2, 3] / 2\n    anchor_standup[::2, 3] = anchors[::2, 1] + anchors[::2, 2] / 2\n    # r == pi/2\n    anchor_standup[1::2, 0] = anchors[1::2, 0] - anchors[1::2, 2] / 2\n    anchor_standup[1::2, 1] = anchors[1::2, 1] - anchors[1::2, 3] / 2\n    anchor_standup[1::2, 2] = anchors[1::2, 0] + anchors[1::2, 2] / 2\n    anchor_standup[1::2, 3] = anchors[1::2, 1] + anchors[1::2, 3] / 2\n\n    return anchor_standup\n\n\ndef corner_to_center_box3d(boxes_corner, coordinate='camera'):\n    # (N, 8, 3) -> (N, 7) x,y,z,h,w,l,ry/z\n    if coordinate == 'lidar':\n        for idx in range(len(boxes_corner)):\n            boxes_corner[idx] = lidar_to_camera_point(boxes_corner[idx])\n    ret = []\n    for roi in boxes_corner:\n        if cfg.CORNER2CENTER_AVG:  # average version\n            roi = np.array(roi)\n            h = abs(np.sum(roi[:4, 1] - roi[4:, 1]) / 4)\n            w = np.sum(\n                np.sqrt(np.sum((roi[0, [0, 2]] - roi[3, [0, 2]])**2)) +\n                np.sqrt(np.sum((roi[1, [0, 2]] - roi[2, [0, 2]])**2)) +\n                np.sqrt(np.sum((roi[4, [0, 2]] - roi[7, [0, 2]])**2)) +\n                np.sqrt(np.sum((roi[5, [0, 2]] - roi[6, [0, 2]])**2))\n            ) / 4\n            l = np.sum(\n                np.sqrt(np.sum((roi[0, [0, 2]] - roi[1, [0, 2]])**2)) +\n                np.sqrt(np.sum((roi[2, [0, 2]] - roi[3, [0, 2]])**2)) +\n                np.sqrt(np.sum((roi[4, [0, 2]] - roi[5, [0, 2]])**2)) +\n                np.sqrt(np.sum((roi[6, [0, 2]] - roi[7, [0, 2]])**2))\n            ) / 4\n            x = np.sum(roi[:, 0], axis=0) / 8\n            y = np.sum(roi[0:4, 1], axis=0) / 4\n            z = np.sum(roi[:, 2], axis=0) / 8\n            ry = np.sum(\n                math.atan2(roi[2, 0] - roi[1, 0], roi[2, 2] - roi[1, 2]) +\n                math.atan2(roi[6, 0] - roi[5, 0], roi[6, 2] - roi[5, 2]) +\n                math.atan2(roi[3, 0] - roi[0, 0], roi[3, 2] - roi[0, 2]) +\n                math.atan2(roi[7, 0] - roi[4, 0], roi[7, 2] - roi[4, 2]) +\n                math.atan2(roi[0, 2] - roi[1, 2], roi[1, 0] - roi[0, 0]) +\n                math.atan2(roi[4, 2] - roi[5, 2], roi[5, 0] - roi[4, 0]) +\n                math.atan2(roi[3, 2] - roi[2, 2], roi[2, 0] - roi[3, 0]) +\n                math.atan2(roi[7, 2] - roi[6, 2], roi[6, 0] - roi[7, 0])\n            ) / 8\n            if w > l:\n                w, l = l, w\n                ry = angle_in_limit(ry + np.pi / 2)\n        else:  # max version\n            h = max(abs(roi[:4, 1] - roi[4:, 1]))\n            w = np.max(\n                np.sqrt(np.sum((roi[0, [0, 2]] - roi[3, [0, 2]])**2)) +\n                np.sqrt(np.sum((roi[1, [0, 2]] - roi[2, [0, 2]])**2)) +\n                np.sqrt(np.sum((roi[4, [0, 2]] - roi[7, [0, 2]])**2)) +\n                np.sqrt(np.sum((roi[5, [0, 2]] - roi[6, [0, 2]])**2))\n            )\n            l = np.max(\n                np.sqrt(np.sum((roi[0, [0, 2]] - roi[1, [0, 2]])**2)) +\n                np.sqrt(np.sum((roi[2, [0, 2]] - roi[3, [0, 2]])**2)) +\n                np.sqrt(np.sum((roi[4, [0, 2]] - roi[5, [0, 2]])**2)) +\n                np.sqrt(np.sum((roi[6, [0, 2]] - roi[7, [0, 2]])**2))\n            )\n            x = np.sum(roi[:, 0], axis=0) / 8\n            y = np.sum(roi[0:4, 1], axis=0) / 4\n            z = np.sum(roi[:, 2], axis=0) / 8\n            ry = np.sum(\n                math.atan2(roi[2, 0] - roi[1, 0], roi[2, 2] - roi[1, 2]) +\n                math.atan2(roi[6, 0] - roi[5, 0], roi[6, 2] - roi[5, 2]) +\n                math.atan2(roi[3, 0] - roi[0, 0], roi[3, 2] - roi[0, 2]) +\n                math.atan2(roi[7, 0] - roi[4, 0], roi[7, 2] - roi[4, 2]) +\n                math.atan2(roi[0, 2] - roi[1, 2], roi[1, 0] - roi[0, 0]) +\n                math.atan2(roi[4, 2] - roi[5, 2], roi[5, 0] - roi[4, 0]) +\n                math.atan2(roi[3, 2] - roi[2, 2], roi[2, 0] - roi[3, 0]) +\n                math.atan2(roi[7, 2] - roi[6, 2], roi[6, 0] - roi[7, 0])\n            ) / 8\n            if w > l:\n                w, l = l, w\n                ry = angle_in_limit(ry + np.pi / 2)\n        ret.append([x, y, z, h, w, l, ry])\n    if coordinate == 'lidar':\n        ret = camera_to_lidar_box(np.array(ret))\n\n    return np.array(ret)\n\n\n# this just for visulize and testing\ndef lidar_box3d_to_camera_box(boxes3d, cal_projection=False):\n    # (N, 7) -> (N, 4)/(N, 8, 2)  x,y,z,h,w,l,rz -> x1,y1,x2,y2/8*(x, y)\n    num = len(boxes3d)\n    boxes2d = np.zeros((num, 4), dtype=np.int32)\n    projections = np.zeros((num, 8, 2), dtype=np.float32)\n\n    lidar_boxes3d_corner = center_to_corner_box3d(boxes3d, coordinate='lidar')\n    P2 = np.array(cfg.MATRIX_P2)\n\n    for n in range(num):\n        box3d = lidar_boxes3d_corner[n]\n        box3d = lidar_to_camera_point(box3d)\n        points = np.hstack((box3d, np.ones((8, 1)))).T  # (8, 4) -> (4, 8)\n        points = np.matmul(P2, points).T\n        points[:, 0] /= points[:, 2]\n        points[:, 1] /= points[:, 2]\n\n        projections[n] = points[:, 0:2]\n        minx = int(np.min(points[:, 0]))\n        maxx = int(np.max(points[:, 0]))\n        miny = int(np.min(points[:, 1]))\n        maxy = int(np.max(points[:, 1]))\n\n        boxes2d[n, :] = minx, miny, maxx, maxy\n\n    return projections if cal_projection else boxes2d\n\n\ndef lidar_to_bird_view_img(lidar, factor=1):\n    # Input:\n    #   lidar: (N', 4)\n    # Output:\n    #   birdview: (w, l, 3)\n    birdview = np.zeros(\n        (cfg.INPUT_HEIGHT * factor, cfg.INPUT_WIDTH * factor, 1))\n    for point in lidar:\n        x, y = point[0:2]\n        if cfg.X_MIN < x < cfg.X_MAX and cfg.Y_MIN < y < cfg.Y_MAX:\n            x, y = int((x - cfg.X_MIN) / cfg.VOXEL_X_SIZE *\n                       factor), int((y - cfg.Y_MIN) / cfg.VOXEL_Y_SIZE * factor)\n            birdview[y, x] += 1\n    birdview = birdview - np.min(birdview)\n    divisor = np.max(birdview) - np.min(birdview)\n    # TODO: adjust this factor\n    birdview = np.clip((birdview / divisor * 255) *\n                       5 * factor, a_min=0, a_max=255)\n    birdview = np.tile(birdview, 3).astype(np.uint8)\n\n    return birdview\n\n\ndef draw_lidar_box3d_on_image(img, boxes3d, scores, gt_boxes3d=np.array([]),\n                              color=(0, 255, 255), gt_color=(255, 0, 255), thickness=1):\n    # Input:\n    #   img: (h, w, 3)\n    #   boxes3d (N, 7) [x, y, z, h, w, l, r]\n    #   scores\n    #   gt_boxes3d (N, 7) [x, y, z, h, w, l, r]\n    img = img.copy()\n    projections = lidar_box3d_to_camera_box(boxes3d, cal_projection=True)\n    gt_projections = lidar_box3d_to_camera_box(gt_boxes3d, cal_projection=True)\n\n    # draw projections\n    for qs in projections:\n        for k in range(0, 4):\n            i, j = k, (k + 1) % 4\n            cv2.line(img, (qs[i, 0], qs[i, 1]), (qs[j, 0],\n                                                 qs[j, 1]), color, thickness, cv2.LINE_AA)\n\n            i, j = k + 4, (k + 1) % 4 + 4\n            cv2.line(img, (qs[i, 0], qs[i, 1]), (qs[j, 0],\n                                                 qs[j, 1]), color, thickness, cv2.LINE_AA)\n\n            i, j = k, k + 4\n            cv2.line(img, (qs[i, 0], qs[i, 1]), (qs[j, 0],\n                                                 qs[j, 1]), color, thickness, cv2.LINE_AA)\n\n    # draw gt projections\n    for qs in gt_projections:\n        for k in range(0, 4):\n            i, j = k, (k + 1) % 4\n            cv2.line(img, (qs[i, 0], qs[i, 1]), (qs[j, 0],\n                                                 qs[j, 1]), gt_color, thickness, cv2.LINE_AA)\n\n            i, j = k + 4, (k + 1) % 4 + 4\n            cv2.line(img, (qs[i, 0], qs[i, 1]), (qs[j, 0],\n                                                 qs[j, 1]), gt_color, thickness, cv2.LINE_AA)\n\n            i, j = k, k + 4\n            cv2.line(img, (qs[i, 0], qs[i, 1]), (qs[j, 0],\n                                                 qs[j, 1]), gt_color, thickness, cv2.LINE_AA)\n\n    return cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_BGR2RGB)\n\n\ndef draw_lidar_box3d_on_birdview(birdview, boxes3d, scores, gt_boxes3d=np.array([]),\n                                 color=(0, 255, 255), gt_color=(255, 0, 255), thickness=1, factor=1):\n    # Input:\n    #   birdview: (h, w, 3)\n    #   boxes3d (N, 7) [x, y, z, h, w, l, r]\n    #   scores\n    #   gt_boxes3d (N, 7) [x, y, z, h, w, l, r]\n    img = birdview.copy()\n    corner_boxes3d = center_to_corner_box3d(boxes3d, coordinate='lidar')\n    corner_gt_boxes3d = center_to_corner_box3d(gt_boxes3d, coordinate='lidar')\n    # draw gt\n    for box in corner_gt_boxes3d:\n        x0, y0 = lidar_to_bird_view(*box[0, 0:2], factor=factor)\n        x1, y1 = lidar_to_bird_view(*box[1, 0:2], factor=factor)\n        x2, y2 = lidar_to_bird_view(*box[2, 0:2], factor=factor)\n        x3, y3 = lidar_to_bird_view(*box[3, 0:2], factor=factor)\n\n        cv2.line(img, (int(x0), int(y0)), (int(x1), int(y1)),\n                 gt_color, thickness, cv2.LINE_AA)\n        cv2.line(img, (int(x1), int(y1)), (int(x2), int(y2)),\n                 gt_color, thickness, cv2.LINE_AA)\n        cv2.line(img, (int(x2), int(y2)), (int(x3), int(y3)),\n                 gt_color, thickness, cv2.LINE_AA)\n        cv2.line(img, (int(x3), int(y3)), (int(x0), int(y0)),\n                 gt_color, thickness, cv2.LINE_AA)\n\n    # draw detections\n    for box in corner_boxes3d:\n        x0, y0 = lidar_to_bird_view(*box[0, 0:2], factor=factor)\n        x1, y1 = lidar_to_bird_view(*box[1, 0:2], factor=factor)\n        x2, y2 = lidar_to_bird_view(*box[2, 0:2], factor=factor)\n        x3, y3 = lidar_to_bird_view(*box[3, 0:2], factor=factor)\n\n        cv2.line(img, (int(x0), int(y0)), (int(x1), int(y1)),\n                 color, thickness, cv2.LINE_AA)\n        cv2.line(img, (int(x1), int(y1)), (int(x2), int(y2)),\n                 color, thickness, cv2.LINE_AA)\n        cv2.line(img, (int(x2), int(y2)), (int(x3), int(y3)),\n                 color, thickness, cv2.LINE_AA)\n        cv2.line(img, (int(x3), int(y3)), (int(x0), int(y0)),\n                 color, thickness, cv2.LINE_AA)\n\n    return cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_BGR2RGB)\n\n\ndef label_to_gt_box3d(labels, cls='Car', coordinate='camera'):\n    # Input:\n    #   label: (N, N')\n    #   cls: 'Car' or 'Pedestrain' or 'Cyclist'\n    #   coordinate: 'camera' or 'lidar'\n    # Output:\n    #   (N, N', 7)\n    boxes3d = []\n    if cls == 'Car':\n        acc_cls = ['Car', 'Van']\n    elif cls == 'Pedestrian':\n        acc_cls = ['Pedestrian']\n    elif cls == 'Cyclist':\n        acc_cls = ['Cyclist']\n    else: # all\n        acc_cls = []\n\n    for label in labels:\n        boxes3d_a_label = []\n        for line in label:\n            ret = line.split()\n            if ret[0] in acc_cls or acc_cls == []:\n                h, w, l, x, y, z, r = [float(i) for i in ret[-7:]]\n                box3d = np.array([x, y, z, h, w, l, r])\n                boxes3d_a_label.append(box3d)\n        if coordinate == 'lidar':\n            boxes3d_a_label = camera_to_lidar_box(np.array(boxes3d_a_label))\n\n        boxes3d.append(np.array(boxes3d_a_label).reshape(-1, 7))\n    return boxes3d\n\n\ndef box3d_to_label(batch_box3d, batch_cls, batch_score=[], coordinate='camera'):\n    # Input:\n    #   (N, N', 7) x y z h w l r\n    #   (N, N')\n    #   cls: (N, N') 'Car' or 'Pedestrain' or 'Cyclist'\n    #   coordinate(input): 'camera' or 'lidar'\n    # Output:\n    #   label: (N, N') N batches and N lines\n    batch_label = []\n    if batch_score:\n        template = '{} ' + ' '.join(['{:.4f}' for i in range(15)]) + '\\n'\n        for boxes, scores, clses in zip(batch_box3d, batch_score, batch_cls):\n            label = []\n            for box, score, cls in zip(boxes, scores, clses):\n                if coordinate == 'camera':\n                    box3d = box\n                    box2d = lidar_box3d_to_camera_box(\n                        camera_to_lidar_box(box[np.newaxis, :].astype(np.float32)), cal_projection=False)[0]\n                else:\n                    box3d = lidar_to_camera_box(\n                        box[np.newaxis, :].astype(np.float32))[0]\n                    box2d = lidar_box3d_to_camera_box(\n                        box[np.newaxis, :].astype(np.float32), cal_projection=False)[0]\n                x, y, z, h, w, l, r = box3d\n                box3d = [h, w, l, x, y, z, r]\n                label.append(template.format(\n                    cls, 0, 0, 0, *box2d, *box3d, float(score)))\n            batch_label.append(label)\n    else:\n        template = '{} ' + ' '.join(['{:.4f}' for i in range(14)]) + '\\n'\n        for boxes, clses in zip(batch_box3d, batch_cls):\n            label = []\n            for box, cls in zip(boxes, clses):\n                if coordinate == 'camera':\n                    box3d = box\n                    box2d = lidar_box3d_to_camera_box(\n                        camera_to_lidar_box(box[np.newaxis, :].astype(np.float32)), cal_projection=False)[0]\n                else:\n                    box3d = lidar_to_camera_box(\n                        box[np.newaxis, :].astype(np.float32))[0]\n                    box2d = lidar_box3d_to_camera_box(\n                        box[np.newaxis, :].astype(np.float32), cal_projection=False)[0]\n                x, y, z, h, w, l, r = box3d\n                box3d = [h, w, l, x, y, z, r]\n                label.append(template.format(cls, 0, 0, 0, *box2d, *box3d))\n            batch_label.append(label)\n\n    return np.array(batch_label)\n\n\ndef cal_anchors():\n    # Output:\n    #   anchors: (w, l, 2, 7) x y z h w l r\n    x = np.linspace(cfg.X_MIN, cfg.X_MAX, cfg.FEATURE_WIDTH)\n    y = np.linspace(cfg.Y_MIN, cfg.Y_MAX, cfg.FEATURE_HEIGHT)\n    cx, cy = np.meshgrid(x, y)\n    # all is (w, l, 2)\n    cx = np.tile(cx[..., np.newaxis], 2)\n    cy = np.tile(cy[..., np.newaxis], 2)\n    cz = np.ones_like(cx) * cfg.ANCHOR_Z\n    w = np.ones_like(cx) * cfg.ANCHOR_W\n    l = np.ones_like(cx) * cfg.ANCHOR_L\n    h = np.ones_like(cx) * cfg.ANCHOR_H\n    r = np.ones_like(cx)\n    r[..., 0] = 0  # 0\n    r[..., 1] = 90 / 180 * np.pi  # 90\n\n    # 7*(w,l,2) -> (w, l, 2, 7)\n    anchors = np.stack([cx, cy, cz, h, w, l, r], axis=-1)\n\n    return anchors\n\n\ndef cal_rpn_target(labels, feature_map_shape, anchors, cls='Car', coordinate='lidar'):\n    # Input:\n    #   labels: (N, N')\n    #   feature_map_shape: (w, l)\n    #   anchors: (w, l, 2, 7)\n    # Output:\n    #   pos_equal_one (N, w, l, 2)\n    #   neg_equal_one (N, w, l, 2)\n    #   targets (N, w, l, 14)\n    # attention: cal IoU on birdview\n    batch_size = labels.shape[0]\n    batch_gt_boxes3d = label_to_gt_box3d(labels, cls=cls, coordinate=coordinate)\n    # defined in eq(1) in 2.2\n    anchors_reshaped = anchors.reshape(-1, 7)\n    anchors_d = np.sqrt(anchors_reshaped[:, 4]**2 + anchors_reshaped[:, 5]**2)\n    pos_equal_one = np.zeros((batch_size, *feature_map_shape, 2))\n    neg_equal_one = np.zeros((batch_size, *feature_map_shape, 2))\n    targets = np.zeros((batch_size, *feature_map_shape, 14))\n\n    for batch_id in range(batch_size):\n        # BOTTLENECK\n        anchors_standup_2d = anchor_to_standup_box2d(\n            anchors_reshaped[:, [0, 1, 4, 5]])\n        # BOTTLENECK\n        gt_standup_2d = corner_to_standup_box2d(center_to_corner_box2d(\n            batch_gt_boxes3d[batch_id][:, [0, 1, 4, 5, 6]], coordinate=coordinate))\n\n        iou = bbox_overlaps(\n            np.ascontiguousarray(anchors_standup_2d).astype(np.float32),\n            np.ascontiguousarray(gt_standup_2d).astype(np.float32),\n        )\n        # iou = cal_box3d_iou(\n        #     anchors_reshaped,\n        #     batch_gt_boxes3d[batch_id]\n        # )\n\n        # find anchor with highest iou(iou should also > 0)\n        id_highest = np.argmax(iou.T, axis=1)\n        id_highest_gt = np.arange(iou.T.shape[0])\n        mask = iou.T[id_highest_gt, id_highest] > 0\n        id_highest, id_highest_gt = id_highest[mask], id_highest_gt[mask]\n\n        # find anchor iou > cfg.XXX_POS_IOU\n        id_pos, id_pos_gt = np.where(iou > cfg.RPN_POS_IOU)\n\n        # find anchor iou < cfg.XXX_NEG_IOU\n        id_neg = np.where(np.sum(iou < cfg.RPN_NEG_IOU,\n                                 axis=1) == iou.shape[1])[0]\n\n        id_pos = np.concatenate([id_pos, id_highest])\n        id_pos_gt = np.concatenate([id_pos_gt, id_highest_gt])\n\n        # TODO: uniquify the array in a more scientific way\n        id_pos, index = np.unique(id_pos, return_index=True)\n        id_pos_gt = id_pos_gt[index]\n        id_neg.sort()\n\n        # cal the target and set the equal one\n        index_x, index_y, index_z = np.unravel_index(\n            id_pos, (*feature_map_shape, 2))\n        pos_equal_one[batch_id, index_x, index_y, index_z] = 1\n\n        # ATTENTION: index_z should be np.array\n        targets[batch_id, index_x, index_y, np.array(index_z) * 7] = (\n            batch_gt_boxes3d[batch_id][id_pos_gt, 0] - anchors_reshaped[id_pos, 0]) / anchors_d[id_pos]\n        targets[batch_id, index_x, index_y, np.array(index_z) * 7 + 1] = (\n            batch_gt_boxes3d[batch_id][id_pos_gt, 1] - anchors_reshaped[id_pos, 1]) / anchors_d[id_pos]\n        targets[batch_id, index_x, index_y, np.array(index_z) * 7 + 2] = (\n            batch_gt_boxes3d[batch_id][id_pos_gt, 2] - anchors_reshaped[id_pos, 2]) / cfg.ANCHOR_H\n        targets[batch_id, index_x, index_y, np.array(index_z) * 7 + 3] = np.log(\n            batch_gt_boxes3d[batch_id][id_pos_gt, 3] / anchors_reshaped[id_pos, 3])\n        targets[batch_id, index_x, index_y, np.array(index_z) * 7 + 4] = np.log(\n            batch_gt_boxes3d[batch_id][id_pos_gt, 4] / anchors_reshaped[id_pos, 4])\n        targets[batch_id, index_x, index_y, np.array(index_z) * 7 + 5] = np.log(\n            batch_gt_boxes3d[batch_id][id_pos_gt, 5] / anchors_reshaped[id_pos, 5])\n        targets[batch_id, index_x, index_y, np.array(index_z) * 7 + 6] = (\n            batch_gt_boxes3d[batch_id][id_pos_gt, 6] - anchors_reshaped[id_pos, 6])\n\n        index_x, index_y, index_z = np.unravel_index(\n            id_neg, (*feature_map_shape, 2))\n        neg_equal_one[batch_id, index_x, index_y, index_z] = 1\n        # to avoid a box be pos/neg in the same time\n        index_x, index_y, index_z = np.unravel_index(\n            id_highest, (*feature_map_shape, 2))\n        neg_equal_one[batch_id, index_x, index_y, index_z] = 0\n\n    return pos_equal_one, neg_equal_one, targets\n\n\n# BOTTLENECK\ndef delta_to_boxes3d(deltas, anchors, coordinate='lidar'):\n    # Input:\n    #   deltas: (N, w, l, 14)\n    #   feature_map_shape: (w, l)\n    #   anchors: (w, l, 2, 7)\n\n    # Ouput:\n    #   boxes3d: (N, w*l*2, 7)\n    anchors_reshaped = anchors.reshape(-1, 7)\n    deltas = deltas.reshape(deltas.shape[0], -1, 7)\n    anchors_d = np.sqrt(anchors_reshaped[:, 4]**2 + anchors_reshaped[:, 5]**2)\n    boxes3d = np.zeros_like(deltas)\n    boxes3d[..., [0, 1]] = deltas[..., [0, 1]] * \\\n        anchors_d[:, np.newaxis] + anchors_reshaped[..., [0, 1]]\n    boxes3d[..., [2]] = deltas[..., [2]] * \\\n        cfg.ANCHOR_H + anchors_reshaped[..., [2]]\n    boxes3d[..., [3, 4, 5]] = np.exp(\n        deltas[..., [3, 4, 5]]) * anchors_reshaped[..., [3, 4, 5]]\n    boxes3d[..., 6] = deltas[..., 6] + anchors_reshaped[..., 6]\n\n    return boxes3d\n\n\ndef point_transform(points, tx, ty, tz, rx=0, ry=0, rz=0):\n    # Input:\n    #   points: (N, 3)\n    #   rx/y/z: in radians\n    # Output:\n    #   points: (N, 3)\n    N = points.shape[0]\n    points = np.hstack([points, np.ones((N, 1))])\n\n    mat1 = np.eye(4)\n    mat1[3, 0:3] = tx, ty, tz\n    points = np.matmul(points, mat1)\n\n    if rx != 0:\n        mat = np.zeros((4, 4))\n        mat[0, 0] = 1\n        mat[3, 3] = 1\n        mat[1, 1] = np.cos(rx)\n        mat[1, 2] = -np.sin(rx)\n        mat[2, 1] = np.sin(rx)\n        mat[2, 2] = np.cos(rx)\n        points = np.matmul(points, mat)\n\n    if ry != 0:\n        mat = np.zeros((4, 4))\n        mat[1, 1] = 1\n        mat[3, 3] = 1\n        mat[0, 0] = np.cos(ry)\n        mat[0, 2] = np.sin(ry)\n        mat[2, 0] = -np.sin(ry)\n        mat[2, 2] = np.cos(ry)\n        points = np.matmul(points, mat)\n\n    if rz != 0:\n        mat = np.zeros((4, 4))\n        mat[2, 2] = 1\n        mat[3, 3] = 1\n        mat[0, 0] = np.cos(rz)\n        mat[0, 1] = -np.sin(rz)\n        mat[1, 0] = np.sin(rz)\n        mat[1, 1] = np.cos(rz)\n        points = np.matmul(points, mat)\n\n    return points[:, 0:3]\n\n\ndef box_transform(boxes, tx, ty, tz, r=0, coordinate='lidar'):\n    # Input:\n    #   boxes: (N, 7) x y z h w l rz/y\n    # Output:\n    #   boxes: (N, 7) x y z h w l rz/y\n    boxes_corner = center_to_corner_box3d(\n        boxes, coordinate=coordinate)  # (N, 8, 3)\n    for idx in range(len(boxes_corner)):\n        if coordinate == 'lidar':\n            boxes_corner[idx] = point_transform(\n                boxes_corner[idx], tx, ty, tz, rz=r)\n        else:\n            boxes_corner[idx] = point_transform(\n                boxes_corner[idx], tx, ty, tz, ry=r)\n\n    return corner_to_center_box3d(boxes_corner, coordinate=coordinate)\n\n\ndef cal_iou2d(box1, box2):\n    # Input: \n    #   box1/2: x, y, w, l, r\n    # Output :\n    #   iou\n    buf1 = np.zeros((cfg.INPUT_HEIGHT, cfg.INPUT_WIDTH, 3))\n    buf2 = np.zeros((cfg.INPUT_HEIGHT, cfg.INPUT_WIDTH, 3))\n    tmp = center_to_corner_box2d(np.array([box1, box2]), coordinate='lidar')\n    box1_corner = batch_lidar_to_bird_view(tmp[0]).astype(np.int32)\n    box2_corner = batch_lidar_to_bird_view(tmp[1]).astype(np.int32)\n    buf1 = cv2.fillConvexPoly(buf1, box1_corner, color=(1,1,1))[..., 0]\n    buf2 = cv2.fillConvexPoly(buf2, box2_corner, color=(1,1,1))[..., 0]\n    indiv = np.sum(np.absolute(buf1-buf2))\n    share = np.sum((buf1 + buf2) == 2)\n    if indiv == 0:\n        return 0.0 # when target is out of bound\n    return share / (indiv + share)\n\ndef cal_z_intersect(cz1, h1, cz2, h2):\n    b1z1, b1z2 = cz1 - h1 / 2, cz1 + h1 / 2\n    b2z1, b2z2 = cz2 - h2 / 2, cz2 + h2 / 2\n    if b1z1 > b2z2 or b2z1 > b1z2:\n        return 0\n    elif b2z1 <= b1z1 <= b2z2:\n        if b1z2 <= b2z2:\n            return h1 / h2\n        else:\n            return (b2z2 - b1z1) / (b1z2 - b2z1)\n    elif b1z1 < b2z1 < b1z2:\n        if b2z2 <= b1z2:\n            return h2 / h1\n        else:\n            return (b1z2 - b2z1) / (b2z2 - b1z1)\n\n\ndef cal_iou3d(box1, box2):\n    # Input:\n    #   box1/2: x, y, z, h, w, l, r\n    # Output:\n    #   iou\n    buf1 = np.zeros((cfg.INPUT_HEIGHT, cfg.INPUT_WIDTH, 3))\n    buf2 = np.zeros((cfg.INPUT_HEIGHT, cfg.INPUT_WIDTH, 3))\n    tmp = center_to_corner_box2d(np.array([box1[[0,1,4,5,6]], box2[[0,1,4,5,6]]]), coordinate='lidar')\n    box1_corner = batch_lidar_to_bird_view(tmp[0]).astype(np.int32)\n    box2_corner = batch_lidar_to_bird_view(tmp[1]).astype(np.int32)\n    buf1 = cv2.fillConvexPoly(buf1, box1_corner, color=(1,1,1))[..., 0]\n    buf2 = cv2.fillConvexPoly(buf2, box2_corner, color=(1,1,1))[..., 0]\n    share = np.sum((buf1 + buf2) == 2)\n    area1 = np.sum(buf1)\n    area2 = np.sum(buf2)\n    \n    z1, h1, z2, h2 = box1[2], box1[3], box2[2], box2[3]\n    z_intersect = cal_z_intersect(z1, h1, z2, h2)\n\n    return share * z_intersect / (area1 * h1 + area2 * h2 - share * z_intersect)\n\n\ndef cal_box3d_iou(boxes3d, gt_boxes3d, cal_3d=0):\n    # Inputs:\n    #   boxes3d: (N1, 7) x,y,z,h,w,l,r\n    #   gt_boxed3d: (N2, 7) x,y,z,h,w,l,r\n    # Outputs:\n    #   iou: (N1, N2)\n    N1 = len(boxes3d)\n    N2 = len(gt_boxes3d)\n    output = np.zeros((N1, N2), dtype=np.float32)\n\n    for idx in range(N1):\n        for idy in range(N2):\n            if cal_3d:\n                output[idx, idy] = float(\n                    cal_iou3d(boxes3d[idx], gt_boxes3d[idy]))\n            else:\n                output[idx, idy] = float(\n                    cal_iou2d(boxes3d[idx, [0, 1, 4, 5, 6]], gt_boxes3d[idy, [0, 1, 4, 5, 6]]))\n\n    return output\n\n\ndef cal_box2d_iou(boxes2d, gt_boxes2d):\n    # Inputs:\n    #   boxes2d: (N1, 5) x,y,w,l,r\n    #   gt_boxes2d: (N2, 5) x,y,w,l,r\n    # Outputs:\n    #   iou: (N1, N2)\n    N1 = len(boxes2d)\n    N2 = len(gt_boxes2d)\n    output = np.zeros((N1, N2), dtype=np.float32)\n    for idx in range(N1):\n        for idy in range(N2):\n            output[idx, idy] = cal_iou2d(boxes2d[idx], gt_boxes2d[idy])\n\n    return output\n\n\nif __name__ == '__main__':\n    pass\n"""
