file_path,api_count,code
visual_search/extractor.py,5,"b'import os\nimport sys\n\n# Add lib to PYTHONPATH\nlib_path = os.path.join(os.path.dirname(__file__), \'lib\')\nif lib_path not in sys.path:\n    sys.path.append(lib_path)\n\nfrom model.test import extract_regions_and_feats as _extract_regions_and_feats\nfrom model.test import extract_imfea as _extract_imfea\nfrom math import ceil\nimport numpy as np\nimport cv2\n\nimport tensorflow as tf\nfrom nets.vgg16 import vgg16\nfrom utils.im_util import read_img_base64\n\n# import logging\n# logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(message)s\', level=logging.INFO)\n# logger = logging.getLogger(__name__)\n\n\ndef _binarize_fea(x, thresh):\n    """"""binary and pack feature vector""""""\n    binary_vec = np.where(x >= thresh, 1, 0)\n    f_len = binary_vec.shape[0]\n    if f_len % 32 != 0:\n        new_size = int(ceil(f_len / 32.) * 32)\n        num_pad = new_size - f_len\n        binary_vec = np.pad(binary_vec, (num_pad, 0), \'constant\')\n\n    return np.packbits(binary_vec).view(\'uint32\')\n\n\nclass Extractor:\n    """""" Feature extractor\n\n    Parameter:\n        - model_path: path to the model\n        - weight_path: weight path\n        - sess: tensorflow session\n        - num_classes: number of classes\n    """"""\n    def __init__(self, model_path, weight_path, sess=None,\n                 anchors=[4, 8, 16, 32], num_classes=81):\n        if not sess:\n            tfconfig = tf.ConfigProto(allow_soft_placement=True)\n            tfconfig.gpu_options.allow_growth = True\n            # init session\n            sess = tf.Session(config=tfconfig)\n        # load network\n        self.sess = sess\n        self.net = vgg16(batch_size=1)\n        self.num_classes = num_classes\n        # load model\n        self.net.create_architecture(sess, ""TEST"", num_classes, weight_path,\n                                     tag=\'default\', anchor_scales=anchors)\n\n        print (\'Loading model check point from {:s}\').format(model_path)\n        saver = tf.train.Saver()\n        saver.restore(sess, model_path)\n\n    def extract_regions_and_feats(self, img):\n        """"""Extract regions and feature corresponding to\n        the each box""""""\n        boxes = _extract_regions_and_feats(self.sess,\n                                           self.net, img, max_per_image=10,\n                                           max_per_class=3, thresh=0.1)\n        return boxes\n\n    def extract_imfea(self, img):\n        ""Extract feature for image""\n        if type(img) == str:\n            img = self.read_img(img)\n\n        fea = _extract_imfea(self.sess, self.net, img)\n        return fea\n\n    def binarize_fea(self, fea, thres=0.1):\n        ""Binarize and pack feature vector""\n        return _binarize_fea(fea, thres)\n\n    def get_tags(self, img):\n        boxes = self.extract_regions_and_feats(img)\n        out = {}\n        for cl, bb in boxes.iteritems():\n            best_score = max([b[\'score\'] for b in bb])\n            out[cl] = float(best_score)\n        return out\n\n    def read_img(self, path):\n        "" Read image from file ""\n        im = cv2.imread(path)\\\n            .astype(np.float32, copy=True)\n        return im\n\n\nif __name__ == \'__main__\':\n    model_path = \'output/vgg16/coco_2014_train+coco_2014_valminusminival/\'\\\n        \'default/vgg16_faster_rcnn_iter_490000.ckpt\'\n    weight_path = \'./data/imagenet_weights/vgg16.weights\'\n\n    tfconfig = tf.ConfigProto(allow_soft_placement=True)\n    tfconfig.gpu_options.allow_growth = True\n    # init session\n    sess = tf.Session(config=tfconfig)\n    extractor = Extractor(model_path, weight_path, sess=sess)\n\n    test_img_path = \'tumblr/tumblr_o56polZl2L1v0xzvvo1_500.jpg\'\n    with open(\'test.txt\') as f:\n        text = f.read().strip()\n\n    img = read_img_base64(text)\n    extractor.get_tags(img)\n    import pdb; pdb.set_trace()\n    fea = extractor.extract_imfea(img)\n    print(fea.shape)\n    bin_fea = extractor.binarize_fea(fea)\n    print(bin_fea.shape)\n    sess.close()\n'"
visual_search/indexer/indexer.py,2,"b'#!/usr/bin/env python\n\nimport cv2\nimport glob\nimport numpy as np\nimport argparse\nimport tensorflow as tf\n\nfrom base64 import b64encode\n\nfrom elasticsearch import Elasticsearch\nfrom elasticsearch import helpers\n\nfrom extractor import Extractor\nfrom es.ImFea_pb2 import ImFea, ImFeaArr, \\\n    ImFeaBinArr, ImFeaBin\n\nimport logging\nlogging.basicConfig(\n    format=\'%(asctime)s : %(levelname)s : %(message)s\', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nMODEL_PATH = \'./models/vgg16_faster_rcnn_iter_490000.ckpt\'\nWEIGHT_PATH = \'./models/vgg16.weights\'\n\n\ndef parse_args():\n    """"""\n    Parse input arguments\n    """"""\n    parser = argparse.ArgumentParser(\n        description=\'Index image to elasticsearch\')\n    parser.add_argument(\'--weight\', dest=\'weight\',\n                        help=\'weight to test\',\n                        default=WEIGHT_PATH, type=str)\n    parser.add_argument(\'--model_path\', dest=\'model_path\',\n                        help=\'path to the model\',\n                        default=MODEL_PATH, type=str)\n\n    parser.add_argument(\'--input\', dest=\'input\',\n                        help=\'Input image folder\',\n                        default=None, type=str)\n\n    parser.add_argument(\'--es_host\', dest=\'es_host\',\n                        help=\'es sever host\',\n                        default=\'localhost\', type=str)\n    parser.add_argument(\'--es_index\', dest=\'es_index\',\n                        help=\'index name\',\n                        default=\'im_data\', type=str)\n    parser.add_argument(\'--es_type\', dest=\'es_type\',\n                        help=\'index type\',\n                        default=\'obj\', type=str)\n    parser.add_argument(\'--es_port\', dest=\'es_port\',\n                        help=\'es server port\',\n                        default=9200, type=int)\n\n    args = parser.parse_args()\n\n    if not args.input:\n        parser.error(\'Input folder not given\')\n    return args\n\n\ndef create_doc(im_src, tag, coords, fea_arr, fea_bin_arr):\n    """"""\n    Create elasticsearch doc\n\n    Params:\n        im_src: image file name\n        tag: tag or class for image\n        coords: list of boxes corresponding to a tag\n        fea_arr: list of ImFea objects\n        fea_bin_arr: list of ImFeaBin objects\n    """"""\n    doc = {}\n    doc[\'coords\'] = coords\n    f_bin = ImFeaBinArr()\n    f = ImFeaArr()\n    f.arr.extend(fea_arr)\n    f_bin.arr.extend(fea_bin_arr)\n    obj_bin_str = b64encode(f_bin.SerializeToString())\n    obj_str = b64encode(f.SerializeToString())\n    doc[\'sigs\'] = obj_str\n    doc[\'bin_sigs\'] = obj_bin_str\n    doc[\'im_src\'] = im_name\n    doc[\'cl\'] = tag\n    return doc\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n\n    # init session\n    tfconfig = tf.ConfigProto(allow_soft_placement=True)\n    tfconfig.gpu_options.allow_growth = True\n    sess = tf.Session(config=tfconfig)\n\n    # create feature extractor\n    print args\n    extractor = Extractor(args.model_path,\n                          args.weight,\n                          sess=sess)\n\n    # create elasticsearch client\n    es = Elasticsearch(hosts=\'{}:{}\'.format(args.es_host, args.es_port))\n\n    # load images\n    images = glob.glob(args.input + ""/*"")\n\n    bulk = []\n    actions = []\n    num_docs = 0\n    count = 0\n    es_index = args.es_index\n    es_type = args.es_type\n    num_imgs = len(images)\n    for im_path in images:\n        # read image\n        im = cv2.imread(im_path) \\\n            .astype(np.float32, copy=True)\n\n        boxes = extractor.extract_regions_and_feats(im)\n\n        count += 1\n        if count % 100 == 0:\n            logger.info(\'Processing image {}/{}\'.format(count, num_imgs))\n\n        for cl, cl_boxes in boxes.iteritems():\n            coords = []\n            im_name = im_path.split(\'/\')[-1]\n            ar = []\n            ar_bin = []\n            for b in cl_boxes:\n                coord_box = {}\n                coord_box[\'c\'] = b[\'lt\'] + b[\'rb\']\n                coord_box[\'score\'] = float(b[\'score\'])\n                coords.append(coord_box)\n                f = b[\'f\']\n                f_bin = extractor.binarize_fea(f)\n\n                im_fea = ImFea()\n                im_fea_bin = ImFeaBin()\n                im_fea.f.extend(f)\n                im_fea_bin.f.extend(f_bin)\n                ar.append(im_fea)\n                ar_bin.append(im_fea_bin)\n\n            doc = create_doc(im_name, cl, coords, ar, ar_bin)\n            num_docs += 1\n\n            # create index action\n            action = {\n                ""_index"": es_index,\n                ""_type"": es_type,\n                ""_source"": doc\n            }\n            actions.append(action)\n            if len(actions) == 1000:\n                logger.info(\'Bulking {} docs to sever, indexed: {}\'\n                            .format(len(actions), num_docs))\n                helpers.bulk(es, actions)\n                del actions[:]\n\n        # index document ifself\n        im_fea = extractor.extract_imfea(im)\n        im_fea_bin = extractor.binarize_fea(im_fea)\n        doc = {}\n        (w, h, _) = im.shape\n        coords = [{\'c\': [0, 0, h, w], \'score\': 1.0}]\n        fea_bin = ImFeaBin()\n        fea = ImFea()\n        fea_bin.f.extend(im_fea_bin)\n        fea.f.extend(im_fea)\n        doc = create_doc(im_name, \'whole\', coords, [fea], [fea_bin])\n        num_docs += 1\n\n        # create index action\n        action = {\n            ""_index"": es_index,\n            ""_type"": es_type,\n            ""_source"": doc\n        }\n        actions.append(action)\n        if len(actions) == 1000:\n            logger.info(\'Bulking {} docs to sever, indexed {}\'\n                        .format(len(actions), num_docs))\n            helpers.bulk(es, actions)\n            del actions[:]\n\n    if len(actions) > 0:\n        helpers.bulk(es, actions)\n        logger.info(\'Bulking {} docs to sever,  total {}\'\n                    .format(len(actions), num_docs))\n\n    sess.close()\n'"
visual_search/lib/setup.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nfrom os.path import join as pjoin\nimport numpy as np\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\n\ndef find_in_path(name, path):\n    ""Find a file in a search path""\n    #adapted fom http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\ndef locate_cuda():\n    """"""Locate the CUDA environment on the system\n\n    Returns a dict with keys \'home\', \'nvcc\', \'include\', and \'lib64\'\n    and values giving the absolute path to each directory.\n\n    Starts by looking for the CUDAHOME env variable. If not found, everything\n    is based on finding \'nvcc\' in the PATH.\n    """"""\n\n    # first check if the CUDAHOME env variable is in use\n    if \'CUDAHOME\' in os.environ:\n        home = os.environ[\'CUDAHOME\']\n        nvcc = pjoin(home, \'bin\', \'nvcc\')\n    else:\n        # otherwise, search the PATH for NVCC\n        default_path = pjoin(os.sep, \'usr\', \'local\', \'cuda\', \'bin\')\n        nvcc = find_in_path(\'nvcc\', os.environ[\'PATH\'] + os.pathsep + default_path)\n        if nvcc is None:\n            raise EnvironmentError(\'The nvcc binary could not be \'\n                \'located in your $PATH. Either add it to your path, or set $CUDAHOME\')\n        home = os.path.dirname(os.path.dirname(nvcc))\n\n    cudaconfig = {\'home\':home, \'nvcc\':nvcc,\n                  \'include\': pjoin(home, \'include\'),\n                  \'lib64\': pjoin(home, \'lib64\')}\n    for k, v in cudaconfig.iteritems():\n        if not os.path.exists(v):\n            raise EnvironmentError(\'The CUDA %s path could not be located in %s\' % (k, v))\n\n    return cudaconfig\nCUDA = locate_cuda()\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to gcc/nvcc works.\n\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        print extra_postargs\n        if os.path.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', CUDA[\'nvcc\'])\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'gcc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\next_modules = [\n    Extension(\n        ""utils.cython_bbox"",\n        [""utils/bbox.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n    ),\n    Extension(\n        ""utils.cython_nms"",\n        [""utils/nms.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n    ),\n    Extension(\n        ""nms.cpu_nms"",\n        [""nms/cpu_nms.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs = [numpy_include]\n    ),\n    Extension(\'nms.gpu_nms\',\n        [\'nms/nms_kernel.cu\', \'nms/gpu_nms.pyx\'],\n        library_dirs=[CUDA[\'lib64\']],\n        libraries=[\'cudart\'],\n        language=\'c++\',\n        runtime_library_dirs=[CUDA[\'lib64\']],\n        # this syntax is specific to this build system\n        # we\'re only going to use certain compiler args with nvcc and not with gcc\n        # the implementation of this trick is in customize_compiler() below\n        extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n                            \'nvcc\': [\'-arch=sm_61\',\n                                     \'--ptxas-options=-v\',\n                                     \'-c\',\n                                     \'--compiler-options\',\n                                     ""\'-fPIC\'""]},\n        include_dirs = [numpy_include, CUDA[\'include\']]\n    )\n]\n\nsetup(\n    name=\'tf_faster_rcnn\',\n    ext_modules=ext_modules,\n    # inject our custom trigger\n    cmdclass={\'build_ext\': custom_build_ext},\n)\n'"
visual_search/server/server.py,2,"b'#!/usr/bin/env python\n\nfrom flask import Flask\nfrom flask import request, jsonify, \\\n    send_from_directory\n\nimport base64\nimport tensorflow as tf\nfrom elasticsearch import Elasticsearch\n\nfrom extractor import Extractor\nfrom utils.im_util import read_img_blob\nfrom es.ImFea_pb2 import ImFea, ImFeaArr,\\\n        ImFeaBinArr, ImFeaBin\n\n\nIMGS_PATH = \'./images/\'\napp = Flask(__name__, static_url_path=\'\')\n\nclass InvalidUsage(Exception):\n    status_code = 400\n\n    def __init__(self, message, status_code=None, payload=None):\n        Exception.__init__(self)\n        self.message = message\n        if status_code is not None:\n            self.status_code = status_code\n        self.payload = payload\n\n    def to_dict(self):\n        rv = dict(self.payload or ())\n        rv[\'message\'] = self.message\n        return rv\n\n@app.errorhandler(InvalidUsage)\ndef handle_invalid_usage(error):\n    response = jsonify(error.to_dict())\n    response.status_code = error.status_code\n    return response\n\n\ndef load_model():\n    """"""Load feature extractor model""""""\n\n    # create feature extractor\n    weight_path = \'./models/vgg16.weights\'\n    model_path = \'./models/vgg16_faster_rcnn_iter_490000.ckpt\'\n\n    tfconfig = tf.ConfigProto(allow_soft_placement=True)\n    tfconfig.gpu_options.allow_growth = True\n    # init session\n    sess = tf.Session(config=tfconfig)\n    extractor = Extractor(model_path, weight_path, sess=sess)\n    return extractor\n\nextractor = load_model()\nes = Elasticsearch(hosts=\'http://elasticsearch:9200\')\n\n@app.route(""/hello"", methods=[\'GET\'])\ndef hello():\n    return ""Hello, world!""\n\n\n@app.route(""/extract_fea"", methods=[\'GET\', \'POST\'])\ndef extract_fea():\n    imgStr = request.values.get(\'img\')\n    if imgStr is None:\n        raise InvalidUsage(\'parameter ""img"" is missing\', status_code=410)\n    try:\n        img = read_img_blob(imgStr)\n    except:\n        raise InvalidUsage(\'Invalid ""img"" param, must be a base64 string\',\n                           status_code=410)\n    fea = extractor.extract_imfea(img)\n    is_binary = request.values.get(\'is_binary\')\n    if is_binary and is_binary == \'true\':\n        fea = extractor.binarize_fea(fea)\n        fea_obj = ImFeaBin()\n    else:\n        fea_obj = ImFea()\n    fea_obj.f.extend(fea)\n    base64str = base64.b64encode(fea_obj.SerializeToString())\n\n    out = {}\n    out[\'fea\'] = base64str\n    return jsonify(out)\n\n\n@app.route(""/get_tags"", methods=[\'GET\', \'POST\'])\ndef get_tags():\n    """"""get tags corresponding to a image""""""\n    if not \'img\' in request.files:\n        raise InvalidUsage(\'parameter ""img"" is missing\', status_code=410)\n    try:\n        f = request.files.get(\'img\')\n        img_str = f.read()\n        img = read_img_blob(img_str)\n    except:\n        raise InvalidUsage(\'Invalid ""img"" param, must be a blob string\',\n                           status_code=410)\n    tags = extractor.get_tags(img)\n    out = {}\n    out[\'tags\'] = tags\n    return jsonify(out)\n\n\n\nQUERY = """"""\n{\n""_source"": [""im_src"", ""cl"", ""coords""],\n""query"": {\n  ""function_score"" : {\n    ""query"" : {\n      ""match_all"" : {\n        ""boost"" : 1.0\n      }\n    },\n    ""functions"" : [\n      {\n        ""filter"" : {\n          ""match_all"" : {\n            ""boost"" : 1.0\n          }\n        },\n        ""script_score"" : {\n          ""script"" : {\n            ""inline"" : ""hamming_score"",\n            ""lang"" : ""native"",\n            ""params"" : {\n              ""f"" : ""bin_sigs"",\n              ""fea"" : [##fea##],\n              ""verbose"" : true\n            }\n          }\n        }\n      }\n    ],\n    ""score_mode"" : ""sum"",\n    ""boost_mode"" : ""replace"",\n    ""max_boost"" : 3.4028235E38,\n    ""boost"" : 1.0\n  }\n}\n}\n""""""\n@app.route(""/search"", methods=[\'GET\', \'POST\'])\ndef search():\n    """"""get tags corresponding to a image""""""\n    if not \'img\' in request.files:\n        raise InvalidUsage(\'parameter ""img"" is missing\', status_code=410)\n    try:\n        f = request.files.get(\'img\')\n        img_str = f.read()\n        img = read_img_blob(img_str)\n    except:\n        raise InvalidUsage(\'Invalid ""img"" param, must be a blob string\',\n                           status_code=410)\n    fea = extractor.extract_imfea(img)\n    fea = extractor.binarize_fea(fea)\n    fea_str = \',\'.join([str(int(t)) for  t in fea])\n    query = QUERY.replace(\'##fea##\', fea_str)\n    print(query)\n    result = es.search(index=\'im_data\', doc_type=\'obj\', body=query)\n    rs = []\n    if \'hits\' in result and \\\n        \'hits\' in result[\'hits\']:\n        #distinct\n        all_imgs = set([])\n        hits = result[\'hits\'][\'hits\']\n        for hit in hits:\n            o = hit[\'_source\']\n            o[\'score\'] = hit[\'_score\']\n            #update im_src\n            im_src = \'/img/{}\'.format(o[\'im_src\'])\n            if not im_src in all_imgs:\n                o[\'im_src\'] = im_src\n                all_imgs.add(im_src)\n                rs.append(o)\n        print all_imgs\n\n    out = {}\n    out[\'hits\'] = rs\n    return jsonify(out)\n\n\n@app.route(\'/static/<path:path>\')\ndef send_static_files(path):\n    ""static files""\n    return send_from_directory(\'static_data\', path)\n\n\n@app.route(\'/img/<path:path>\')\ndef send_image(path):\n    ""static files""\n    return send_from_directory(IMGS_PATH, path)\n\n\nif __name__ == \'__main__\':\n    app.run(host=\'0.0.0.0\', port=5000, debug=True)\n'"
visual_search/lib/datasets/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n'
visual_search/lib/datasets/coco.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nfrom datasets.imdb import imdb\nimport datasets.ds_utils as ds_utils\nfrom model.config import cfg\nimport os.path as osp\nimport sys\nimport os\nimport numpy as np\nimport scipy.sparse\nimport scipy.io as sio\nimport cPickle\nimport json\nimport uuid\n# COCO API\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nfrom pycocotools import mask as COCOmask\n\ndef _filter_crowd_proposals(roidb, crowd_thresh):\n    """"""\n    Finds proposals that are inside crowd regions and marks them with\n    overlap = -1 (for all gt rois), which means they will be excluded from\n    training.\n    """"""\n    for ix, entry in enumerate(roidb):\n        overlaps = entry[\'gt_overlaps\'].toarray()\n        crowd_inds = np.where(overlaps.max(axis=1) == -1)[0]\n        non_gt_inds = np.where(entry[\'gt_classes\'] == 0)[0]\n        if len(crowd_inds) == 0 or len(non_gt_inds) == 0:\n            continue\n        iscrowd = [int(True) for _ in xrange(len(crowd_inds))]\n        crowd_boxes = ds_utils.xyxy_to_xywh(entry[\'boxes\'][crowd_inds, :])\n        non_gt_boxes = ds_utils.xyxy_to_xywh(entry[\'boxes\'][non_gt_inds, :])\n        ious = COCOmask.iou(non_gt_boxes, crowd_boxes, iscrowd)\n        bad_inds = np.where(ious.max(axis=1) > crowd_thresh)[0]\n        overlaps[non_gt_inds[bad_inds], :] = -1\n        roidb[ix][\'gt_overlaps\'] = scipy.sparse.csr_matrix(overlaps)\n    return roidb\n\nclass coco(imdb):\n    def __init__(self, image_set, year):\n        imdb.__init__(self, \'coco_\' + year + \'_\' + image_set)\n        # COCO specific config options\n        self.config = {\'top_k\' : 2000,\n                       \'use_salt\' : True,\n                       \'cleanup\' : True,\n                       \'crowd_thresh\' : 0.7,\n                       \'min_size\' : 2}\n        # name, paths\n        self._year = year\n        self._image_set = image_set\n        self._data_path = osp.join(cfg.DATA_DIR, \'coco\')\n        # load COCO API, classes, class <-> id mappings\n        self._COCO = COCO(self._get_ann_file())\n        cats = self._COCO.loadCats(self._COCO.getCatIds())\n        self._classes = tuple([\'__background__\'] + [c[\'name\'] for c in cats])\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._class_to_coco_cat_id = dict(zip([c[\'name\'] for c in cats],\n                                              self._COCO.getCatIds()))\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        self.set_proposal_method(\'gt\')\n        self.competition_mode(False)\n\n        # Some image sets are ""views"" (i.e. subsets) into others.\n        # For example, minival2014 is a random 5000 image subset of val2014.\n        # This mapping tells us where the view\'s images and proposals come from.\n        self._view_map = {\n            \'minival2014\' : \'val2014\',          # 5k val2014 subset\n            \'valminusminival2014\' : \'val2014\',  # val2014 \\setminus minival2014\n            \'test-dev2015\' : \'test2015\', \n        }\n        coco_name = image_set + year  # e.g., ""val2014""\n        self._data_name = (self._view_map[coco_name]\n                           if self._view_map.has_key(coco_name)\n                           else coco_name)\n        # Dataset splits that have ground-truth annotations (test splits\n        # do not have gt annotations)\n        self._gt_splits = (\'train\', \'val\', \'minival\')\n\n    def _get_ann_file(self):\n        prefix = \'instances\' if self._image_set.find(\'test\') == -1 \\\n                             else \'image_info\'\n        return osp.join(self._data_path, \'annotations\',\n                        prefix + \'_\' + self._image_set + self._year + \'.json\')\n\n    def _load_image_set_index(self):\n        """"""\n        Load image ids.\n        """"""\n        image_ids = self._COCO.getImgIds()\n        return image_ids\n\n    def _get_widths(self):\n        anns = self._COCO.loadImgs(self._image_index)\n        widths = [ann[\'width\'] for ann in anns]\n        return widths\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self._image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        # Example image path for index=119993:\n        #   images/train2014/COCO_train2014_000000119993.jpg\n        file_name = (\'COCO_\' + self._data_name + \'_\' +\n                     str(index).zfill(12) + \'.jpg\')\n        image_path = osp.join(self._data_path, \'images\',\n                              self._data_name, file_name)\n        assert osp.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def selective_search_roidb(self):\n        return self._roidb_from_proposals(\'selective_search\')\n\n    def edge_boxes_roidb(self):\n        return self._roidb_from_proposals(\'edge_boxes_AR\')\n\n    def mcg_roidb(self):\n        return self._roidb_from_proposals(\'MCG\')\n\n    def _roidb_from_proposals(self, method):\n        """"""\n        Creates a roidb from pre-computed proposals of a particular methods.\n        """"""\n        top_k = self.config[\'top_k\']\n        cache_file = osp.join(self.cache_path, self.name +\n                              \'_{:s}_top{:d}\'.format(method, top_k) +\n                              \'_roidb.pkl\')\n\n        if osp.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{:s} {:s} roidb loaded from {:s}\'.format(self.name, method,\n                                                            cache_file)\n            return roidb\n\n        if self._image_set in self._gt_splits:\n            gt_roidb = self.gt_roidb()\n            method_roidb = self._load_proposals(method, gt_roidb)\n            roidb = imdb.merge_roidbs(gt_roidb, method_roidb)\n            # Make sure we don\'t use proposals that are contained in crowds\n            roidb = _filter_crowd_proposals(roidb, self.config[\'crowd_thresh\'])\n        else:\n            roidb = self._load_proposals(method, None)\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote {:s} roidb to {:s}\'.format(method, cache_file)\n        return roidb\n\n    def _load_proposals(self, method, gt_roidb):\n        """"""\n        Load pre-computed proposals in the format provided by Jan Hosang:\n        http://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-\n          computing/research/object-recognition-and-scene-understanding/how-\n          good-are-detection-proposals-really/\n        For MCG, use boxes from http://www.eecs.berkeley.edu/Research/Projects/\n          CS/vision/grouping/mcg/ and convert the file layout using\n        lib/datasets/tools/mcg_munge.py.\n        """"""\n        box_list = []\n        top_k = self.config[\'top_k\']\n        valid_methods = [\n            \'MCG\',\n            \'selective_search\',\n            \'edge_boxes_AR\',\n            \'edge_boxes_70\']\n        assert method in valid_methods\n\n        print \'Loading {} boxes\'.format(method)\n        for i, index in enumerate(self._image_index):\n            if i % 1000 == 0:\n                print \'{:d} / {:d}\'.format(i + 1, len(self._image_index))\n\n            box_file = osp.join(\n                cfg.DATA_DIR, \'coco_proposals\', method, \'mat\',\n                self._get_box_file(index))\n\n            raw_data = sio.loadmat(box_file)[\'boxes\']\n            boxes = np.maximum(raw_data - 1, 0).astype(np.uint16)\n            if method == \'MCG\':\n                # Boxes from the MCG website are in (y1, x1, y2, x2) order\n                boxes = boxes[:, (1, 0, 3, 2)]\n            # Remove duplicate boxes and very small boxes and then take top k\n            keep = ds_utils.unique_boxes(boxes)\n            boxes = boxes[keep, :]\n            keep = ds_utils.filter_small_boxes(boxes, self.config[\'min_size\'])\n            boxes = boxes[keep, :]\n            boxes = boxes[:top_k, :]\n            box_list.append(boxes)\n            # Sanity check\n            im_ann = self._COCO.loadImgs(index)[0]\n            width = im_ann[\'width\']\n            height = im_ann[\'height\']\n            ds_utils.validate_boxes(boxes, width=width, height=height)\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = osp.join(self.cache_path, self.name + \'_gt_roidb.pkl\')\n        if osp.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} gt roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = [self._load_coco_annotation(index)\n                    for index in self._image_index]\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote gt roidb to {}\'.format(cache_file)\n        return gt_roidb\n\n    def _load_coco_annotation(self, index):\n        """"""\n        Loads COCO bounding-box instance annotations. Crowd instances are\n        handled by marking their overlaps (with all categories) to -1. This\n        overlap value means that crowd ""instances"" are excluded from training.\n        """"""\n        im_ann = self._COCO.loadImgs(index)[0]\n        width = im_ann[\'width\']\n        height = im_ann[\'height\']\n\n        annIds = self._COCO.getAnnIds(imgIds=index, iscrowd=None)\n        objs = self._COCO.loadAnns(annIds)\n        # Sanitize bboxes -- some are invalid\n        valid_objs = []\n        for obj in objs:\n            x1 = np.max((0, obj[\'bbox\'][0]))\n            y1 = np.max((0, obj[\'bbox\'][1]))\n            x2 = np.min((width - 1, x1 + np.max((0, obj[\'bbox\'][2] - 1))))\n            y2 = np.min((height - 1, y1 + np.max((0, obj[\'bbox\'][3] - 1))))\n            if obj[\'area\'] > 0 and x2 >= x1 and y2 >= y1:\n                obj[\'clean_bbox\'] = [x1, y1, x2, y2]\n                valid_objs.append(obj)\n        objs = valid_objs\n        num_objs = len(objs)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        seg_areas = np.zeros((num_objs), dtype=np.float32)\n\n        # Lookup table to map from COCO category ids to our internal class\n        # indices\n        coco_cat_id_to_class_ind = dict([(self._class_to_coco_cat_id[cls],\n                                          self._class_to_ind[cls])\n                                         for cls in self._classes[1:]])\n\n        for ix, obj in enumerate(objs):\n            cls = coco_cat_id_to_class_ind[obj[\'category_id\']]\n            boxes[ix, :] = obj[\'clean_bbox\']\n            gt_classes[ix] = cls\n            seg_areas[ix] = obj[\'area\']\n            if obj[\'iscrowd\']:\n                # Set overlap to -1 for all classes for crowd objects\n                # so they will be excluded during training\n                overlaps[ix, :] = -1.0\n            else:\n                overlaps[ix, cls] = 1.0\n\n        ds_utils.validate_boxes(boxes, width=width, height=height)\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n        return {\'width\': width,\n                \'height\': height,\n                \'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_overlaps\' : overlaps,\n                \'flipped\' : False,\n                \'seg_areas\' : seg_areas}\n\n    def _get_widths(self):\n        return [r[\'width\'] for r in self.roidb]\n\n    def append_flipped_images(self):\n        num_images = self.num_images\n        widths = self._get_widths()\n        for i in xrange(num_images):\n            boxes = self.roidb[i][\'boxes\'].copy()\n            oldx1 = boxes[:, 0].copy()\n            oldx2 = boxes[:, 2].copy()\n            boxes[:, 0] = widths[i] - oldx2 - 1\n            boxes[:, 2] = widths[i] - oldx1 - 1\n            assert (boxes[:, 2] >= boxes[:, 0]).all()\n            entry = {\'width\': widths[i],\n                   \'height\': self.roidb[i][\'height\'],\n                   \'boxes\' : boxes,\n                   \'gt_classes\' : self.roidb[i][\'gt_classes\'],\n                   \'gt_overlaps\' : self.roidb[i][\'gt_overlaps\'],\n                   \'flipped\' : True,\n                   \'seg_areas\' : self.roidb[i][\'seg_areas\']}\n\n            self.roidb.append(entry)\n        self._image_index = self._image_index * 2\n\n    def _get_box_file(self, index):\n        # first 14 chars / first 22 chars / all chars + .mat\n        # COCO_val2014_0/COCO_val2014_000000447/COCO_val2014_000000447991.mat\n        file_name = (\'COCO_\' + self._data_name +\n                     \'_\' + str(index).zfill(12) + \'.mat\')\n        return osp.join(file_name[:14], file_name[:22], file_name)\n\n    def _print_detection_eval_metrics(self, coco_eval):\n        IoU_lo_thresh = 0.5\n        IoU_hi_thresh = 0.95\n        def _get_thr_ind(coco_eval, thr):\n            ind = np.where((coco_eval.params.iouThrs > thr - 1e-5) &\n                           (coco_eval.params.iouThrs < thr + 1e-5))[0][0]\n            iou_thr = coco_eval.params.iouThrs[ind]\n            assert np.isclose(iou_thr, thr)\n            return ind\n\n        ind_lo = _get_thr_ind(coco_eval, IoU_lo_thresh)\n        ind_hi = _get_thr_ind(coco_eval, IoU_hi_thresh)\n        # precision has dims (iou, recall, cls, area range, max dets)\n        # area range index 0: all area ranges\n        # max dets index 2: 100 per image\n        precision = \\\n            coco_eval.eval[\'precision\'][ind_lo:(ind_hi + 1), :, :, 0, 2]\n        ap_default = np.mean(precision[precision > -1])\n        print (\'~~~~ Mean and per-category AP @ IoU=[{:.2f},{:.2f}] \'\n               \'~~~~\').format(IoU_lo_thresh, IoU_hi_thresh)\n        print \'{:.1f}\'.format(100 * ap_default)\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == \'__background__\':\n                continue\n            # minus 1 because of __background__\n            precision = coco_eval.eval[\'precision\'][ind_lo:(ind_hi + 1), :, cls_ind - 1, 0, 2]\n            ap = np.mean(precision[precision > -1])\n            print \'{:.1f}\'.format(100 * ap)\n\n        print \'~~~~ Summary metrics ~~~~\'\n        coco_eval.summarize()\n\n    def _do_detection_eval(self, res_file, output_dir):\n        ann_type = \'bbox\'\n        coco_dt = self._COCO.loadRes(res_file)\n        coco_eval = COCOeval(self._COCO, coco_dt)\n        coco_eval.params.useSegm = (ann_type == \'segm\')\n        coco_eval.evaluate()\n        coco_eval.accumulate()\n        self._print_detection_eval_metrics(coco_eval)\n        eval_file = osp.join(output_dir, \'detection_results.pkl\')\n        with open(eval_file, \'wb\') as fid:\n            cPickle.dump(coco_eval, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'Wrote COCO eval results to: {}\'.format(eval_file)\n\n    def _coco_results_one_category(self, boxes, cat_id):\n        results = []\n        for im_ind, index in enumerate(self.image_index):\n            dets = boxes[im_ind].astype(np.float)\n            if dets == []:\n                continue\n            scores = dets[:, -1]\n            xs = dets[:, 0]\n            ys = dets[:, 1]\n            ws = dets[:, 2] - xs + 1\n            hs = dets[:, 3] - ys + 1\n            results.extend(\n              [{\'image_id\' : index,\n                \'category_id\' : cat_id,\n                \'bbox\' : [xs[k], ys[k], ws[k], hs[k]],\n                \'score\' : scores[k]} for k in xrange(dets.shape[0])])\n        return results\n\n    def _write_coco_results_file(self, all_boxes, res_file):\n        # [{""image_id"": 42,\n        #   ""category_id"": 18,\n        #   ""bbox"": [258.15,41.29,348.26,243.78],\n        #   ""score"": 0.236}, ...]\n        results = []\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == \'__background__\':\n                continue\n            print \'Collecting {} results ({:d}/{:d})\'.format(cls, cls_ind,\n                                                          self.num_classes - 1)\n            coco_cat_id = self._class_to_coco_cat_id[cls]\n            results.extend(self._coco_results_one_category(all_boxes[cls_ind],\n                                                           coco_cat_id))\n        print \'Writing results json to {}\'.format(res_file)\n        with open(res_file, \'w\') as fid:\n            json.dump(results, fid)\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        res_file = osp.join(output_dir, (\'detections_\' +\n                                         self._image_set +\n                                         self._year +\n                                         \'_results\'))\n        if self.config[\'use_salt\']:\n            res_file += \'_{}\'.format(str(uuid.uuid4()))\n        res_file += \'.json\'\n        self._write_coco_results_file(all_boxes, res_file)\n        # Only do evaluation on non-test sets\n        if self._image_set.find(\'test\') == -1:\n            self._do_detection_eval(res_file, output_dir)\n        # Optionally cleanup results json file\n        if self.config[\'cleanup\']:\n            os.remove(res_file)\n\n    def competition_mode(self, on):\n        if on:\n            self.config[\'use_salt\'] = False\n            self.config[\'cleanup\'] = False\n        else:\n            self.config[\'use_salt\'] = True\n            self.config[\'cleanup\'] = True\n'"
visual_search/lib/datasets/ds_utils.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\n\ndef unique_boxes(boxes, scale=1.0):\n    """"""Return indices of unique boxes.""""""\n    v = np.array([1, 1e3, 1e6, 1e9])\n    hashes = np.round(boxes * scale).dot(v)\n    _, index = np.unique(hashes, return_index=True)\n    return np.sort(index)\n\ndef xywh_to_xyxy(boxes):\n    """"""Convert [x y w h] box format to [x1 y1 x2 y2] format.""""""\n    return np.hstack((boxes[:, 0:2], boxes[:, 0:2] + boxes[:, 2:4] - 1))\n\ndef xyxy_to_xywh(boxes):\n    """"""Convert [x1 y1 x2 y2] box format to [x y w h] format.""""""\n    return np.hstack((boxes[:, 0:2], boxes[:, 2:4] - boxes[:, 0:2] + 1))\n\ndef validate_boxes(boxes, width=0, height=0):\n    """"""Check that a set of boxes are valid.""""""\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    assert (x1 >= 0).all()\n    assert (y1 >= 0).all()\n    assert (x2 >= x1).all()\n    assert (y2 >= y1).all()\n    assert (x2 < width).all()\n    assert (y2 < height).all()\n\ndef filter_small_boxes(boxes, min_size):\n    w = boxes[:, 2] - boxes[:, 0]\n    h = boxes[:, 3] - boxes[:, 1]\n    keep = np.where((w >= min_size) & (h > min_size))[0]\n    return keep\n'"
visual_search/lib/datasets/factory.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Factory method for easily getting imdbs by name.""""""\n\n__sets = {}\n\nfrom datasets.pascal_voc import pascal_voc\nfrom datasets.coco import coco\n\nimport numpy as np\n\n# Set up voc_<year>_<split> using selective search ""fast"" mode\nfor year in [\'2007\', \'2012\']:\n    for split in [\'train\', \'val\', \'trainval\', \'test\']:\n        name = \'voc_{}_{}\'.format(year, split)\n        __sets[name] = (lambda split=split, year=year: pascal_voc(split, year))\n\n# Set up coco_2014_<split>\nfor year in [\'2014\']:\n    for split in [\'train\', \'val\', \'minival\', \'valminusminival\', \'trainval\']:\n        name = \'coco_{}_{}\'.format(year, split)\n        __sets[name] = (lambda split=split, year=year: coco(split, year))\n\n# Set up coco_2015_<split>\nfor year in [\'2015\']:\n    for split in [\'test\', \'test-dev\']:\n        name = \'coco_{}_{}\'.format(year, split)\n        __sets[name] = (lambda split=split, year=year: coco(split, year))\n\ndef get_imdb(name):\n    """"""Get an imdb (image database) by name.""""""\n    if not __sets.has_key(name):\n        raise KeyError(\'Unknown dataset: {}\'.format(name))\n    return __sets[name]()\n\ndef list_imdbs():\n    """"""List all registered imdbs.""""""\n    return __sets.keys()\n'"
visual_search/lib/datasets/imdb.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nimport os.path as osp\nimport PIL\nfrom utils.cython_bbox import bbox_overlaps\nimport numpy as np\nimport scipy.sparse\nfrom model.config import cfg\n\nclass imdb(object):\n    """"""Image database.""""""\n\n    def __init__(self, name, classes=None):\n        self._name = name\n        self._num_classes = 0\n        if not classes:\n            self._classes = []\n        else:\n            self._classes = classes\n        self._image_index = []\n        self._obj_proposer = \'selective_search\'\n        self._roidb = None\n        self._roidb_handler = self.default_roidb\n        # Use this dict for storing dataset specific config options\n        self.config = {}\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def num_classes(self):\n        return len(self._classes)\n\n    @property\n    def classes(self):\n        return self._classes\n\n    @property\n    def image_index(self):\n        return self._image_index\n\n    @property\n    def roidb_handler(self):\n        return self._roidb_handler\n\n    @roidb_handler.setter\n    def roidb_handler(self, val):\n        self._roidb_handler = val\n\n    def set_proposal_method(self, method):\n        method = eval(\'self.\' + method + \'_roidb\')\n        self.roidb_handler = method\n\n    @property\n    def roidb(self):\n        # A roidb is a list of dictionaries, each with the following keys:\n        #   boxes\n        #   gt_overlaps\n        #   gt_classes\n        #   flipped\n        if self._roidb is not None:\n            return self._roidb\n        self._roidb = self.roidb_handler()\n        return self._roidb\n\n    @property\n    def cache_path(self):\n        cache_path = osp.abspath(osp.join(cfg.DATA_DIR, \'cache\'))\n        if not os.path.exists(cache_path):\n            os.makedirs(cache_path)\n        return cache_path\n\n    @property\n    def num_images(self):\n      return len(self.image_index)\n\n    def image_path_at(self, i):\n        raise NotImplementedError\n\n    def default_roidb(self):\n        raise NotImplementedError\n\n    def evaluate_detections(self, all_boxes, output_dir=None):\n        """"""\n        all_boxes is a list of length number-of-classes.\n        Each list element is a list of length number-of-images.\n        Each of those list elements is either an empty list []\n        or a numpy array of detection.\n\n        all_boxes[class][image] = [] or np.array of shape #dets x 5\n        """"""\n        raise NotImplementedError\n\n    def _get_widths(self):\n      return [PIL.Image.open(self.image_path_at(i)).size[0]\n              for i in xrange(self.num_images)]\n\n    def append_flipped_images(self):\n        num_images = self.num_images\n        widths = self._get_widths()\n        for i in xrange(num_images):\n            boxes = self.roidb[i][\'boxes\'].copy()\n            oldx1 = boxes[:, 0].copy()\n            oldx2 = boxes[:, 2].copy()\n            boxes[:, 0] = widths[i] - oldx2 - 1\n            boxes[:, 2] = widths[i] - oldx1 - 1\n            assert (boxes[:, 2] >= boxes[:, 0]).all()\n            entry = {\'boxes\' : boxes,\n                     \'gt_overlaps\' : self.roidb[i][\'gt_overlaps\'],\n                     \'gt_classes\' : self.roidb[i][\'gt_classes\'],\n                     \'flipped\' : True}\n            self.roidb.append(entry)\n        self._image_index = self._image_index * 2\n\n    def evaluate_recall(self, candidate_boxes=None, thresholds=None,\n                        area=\'all\', limit=None):\n        """"""Evaluate detection proposal recall metrics.\n\n        Returns:\n            results: dictionary of results with keys\n                \'ar\': average recall\n                \'recalls\': vector recalls at each IoU overlap threshold\n                \'thresholds\': vector of IoU overlap thresholds\n                \'gt_overlaps\': vector of all ground-truth overlaps\n        """"""\n        # Record max overlap value for each gt box\n        # Return vector of overlap values\n        areas = { \'all\': 0, \'small\': 1, \'medium\': 2, \'large\': 3,\n                  \'96-128\': 4, \'128-256\': 5, \'256-512\': 6, \'512-inf\': 7}\n        area_ranges = [ [0**2, 1e5**2],    # all\n                        [0**2, 32**2],     # small\n                        [32**2, 96**2],    # medium\n                        [96**2, 1e5**2],   # large\n                        [96**2, 128**2],   # 96-128\n                        [128**2, 256**2],  # 128-256\n                        [256**2, 512**2],  # 256-512\n                        [512**2, 1e5**2],  # 512-inf\n                      ]\n        assert areas.has_key(area), \'unknown area range: {}\'.format(area)\n        area_range = area_ranges[areas[area]]\n        gt_overlaps = np.zeros(0)\n        num_pos = 0\n        for i in xrange(self.num_images):\n            # Checking for max_overlaps == 1 avoids including crowd annotations\n            # (...pretty hacking :/)\n            max_gt_overlaps = self.roidb[i][\'gt_overlaps\'].toarray().max(axis=1)\n            gt_inds = np.where((self.roidb[i][\'gt_classes\'] > 0) &\n                               (max_gt_overlaps == 1))[0]\n            gt_boxes = self.roidb[i][\'boxes\'][gt_inds, :]\n            gt_areas = self.roidb[i][\'seg_areas\'][gt_inds]\n            valid_gt_inds = np.where((gt_areas >= area_range[0]) &\n                                     (gt_areas <= area_range[1]))[0]\n            gt_boxes = gt_boxes[valid_gt_inds, :]\n            num_pos += len(valid_gt_inds)\n\n            if candidate_boxes is None:\n                # If candidate_boxes is not supplied, the default is to use the\n                # non-ground-truth boxes from this roidb\n                non_gt_inds = np.where(self.roidb[i][\'gt_classes\'] == 0)[0]\n                boxes = self.roidb[i][\'boxes\'][non_gt_inds, :]\n            else:\n                boxes = candidate_boxes[i]\n            if boxes.shape[0] == 0:\n                continue\n            if limit is not None and boxes.shape[0] > limit:\n                boxes = boxes[:limit, :]\n\n            overlaps = bbox_overlaps(boxes.astype(np.float),\n                                     gt_boxes.astype(np.float))\n\n            _gt_overlaps = np.zeros((gt_boxes.shape[0]))\n            for j in xrange(gt_boxes.shape[0]):\n                # find which proposal box maximally covers each gt box\n                argmax_overlaps = overlaps.argmax(axis=0)\n                # and get the iou amount of coverage for each gt box\n                max_overlaps = overlaps.max(axis=0)\n                # find which gt box is \'best\' covered (i.e. \'best\' = most iou)\n                gt_ind = max_overlaps.argmax()\n                gt_ovr = max_overlaps.max()\n                assert(gt_ovr >= 0)\n                # find the proposal box that covers the best covered gt box\n                box_ind = argmax_overlaps[gt_ind]\n                # record the iou coverage of this gt box\n                _gt_overlaps[j] = overlaps[box_ind, gt_ind]\n                assert(_gt_overlaps[j] == gt_ovr)\n                # mark the proposal box and the gt box as used\n                overlaps[box_ind, :] = -1\n                overlaps[:, gt_ind] = -1\n            # append recorded iou coverage level\n            gt_overlaps = np.hstack((gt_overlaps, _gt_overlaps))\n\n        gt_overlaps = np.sort(gt_overlaps)\n        if thresholds is None:\n            step = 0.05\n            thresholds = np.arange(0.5, 0.95 + 1e-5, step)\n        recalls = np.zeros_like(thresholds)\n        # compute recall for each iou threshold\n        for i, t in enumerate(thresholds):\n            recalls[i] = (gt_overlaps >= t).sum() / float(num_pos)\n        # ar = 2 * np.trapz(recalls, thresholds)\n        ar = recalls.mean()\n        return {\'ar\': ar, \'recalls\': recalls, \'thresholds\': thresholds,\n                \'gt_overlaps\': gt_overlaps}\n\n    def create_roidb_from_box_list(self, box_list, gt_roidb):\n        assert len(box_list) == self.num_images, \\\n                \'Number of boxes must match number of ground-truth images\'\n        roidb = []\n        for i in xrange(self.num_images):\n            boxes = box_list[i]\n            num_boxes = boxes.shape[0]\n            overlaps = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n\n            if gt_roidb is not None and gt_roidb[i][\'boxes\'].size > 0:\n                gt_boxes = gt_roidb[i][\'boxes\']\n                gt_classes = gt_roidb[i][\'gt_classes\']\n                gt_overlaps = bbox_overlaps(boxes.astype(np.float),\n                                            gt_boxes.astype(np.float))\n                argmaxes = gt_overlaps.argmax(axis=1)\n                maxes = gt_overlaps.max(axis=1)\n                I = np.where(maxes > 0)[0]\n                overlaps[I, gt_classes[argmaxes[I]]] = maxes[I]\n\n            overlaps = scipy.sparse.csr_matrix(overlaps)\n            roidb.append({\n                \'boxes\' : boxes,\n                \'gt_classes\' : np.zeros((num_boxes,), dtype=np.int32),\n                \'gt_overlaps\' : overlaps,\n                \'flipped\' : False,\n                \'seg_areas\' : np.zeros((num_boxes,), dtype=np.float32),\n            })\n        return roidb\n\n    @staticmethod\n    def merge_roidbs(a, b):\n        assert len(a) == len(b)\n        for i in xrange(len(a)):\n            a[i][\'boxes\'] = np.vstack((a[i][\'boxes\'], b[i][\'boxes\']))\n            a[i][\'gt_classes\'] = np.hstack((a[i][\'gt_classes\'],\n                                            b[i][\'gt_classes\']))\n            a[i][\'gt_overlaps\'] = scipy.sparse.vstack([a[i][\'gt_overlaps\'],\n                                                       b[i][\'gt_overlaps\']])\n            a[i][\'seg_areas\'] = np.hstack((a[i][\'seg_areas\'],\n                                           b[i][\'seg_areas\']))\n        return a\n\n    def competition_mode(self, on):\n        """"""Turn competition mode on or off.""""""\n        pass\n'"
visual_search/lib/datasets/pascal_voc.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nfrom datasets.imdb import imdb\nimport datasets.ds_utils as ds_utils\nimport xml.etree.ElementTree as ET\nimport numpy as np\nimport scipy.sparse\nimport scipy.io as sio\nimport utils.cython_bbox\nimport cPickle\nimport subprocess\nimport uuid\nfrom voc_eval import voc_eval\nfrom model.config import cfg\n\nclass pascal_voc(imdb):\n    def __init__(self, image_set, year, devkit_path=None):\n        imdb.__init__(self, \'voc_\' + year + \'_\' + image_set)\n        self._year = year\n        self._image_set = image_set\n        self._devkit_path = self._get_default_path() if devkit_path is None \\\n                            else devkit_path\n        self._data_path = os.path.join(self._devkit_path, \'VOC\' + self._year)\n        self._classes = (\'__background__\', # always index 0\n                         \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n                         \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n                         \'cow\', \'diningtable\', \'dog\', \'horse\',\n                         \'motorbike\', \'person\', \'pottedplant\',\n                         \'sheep\', \'sofa\', \'train\', \'tvmonitor\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.jpg\'\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        self._roidb_handler = self.selective_search_roidb\n        self._salt = str(uuid.uuid4())\n        self._comp_id = \'comp4\'\n\n        # PASCAL specific config options\n        self.config = {\'cleanup\'     : True,\n                       \'use_salt\'    : True,\n                       \'use_diff\'    : False,\n                       \'matlab_eval\' : False,\n                       \'rpn_file\'    : None,\n                       \'min_size\'    : 2}\n\n        assert os.path.exists(self._devkit_path), \\\n                \'VOCdevkit path does not exist: {}\'.format(self._devkit_path)\n        assert os.path.exists(self._data_path), \\\n                \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self._image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        image_path = os.path.join(self._data_path, \'JPEGImages\',\n                                  index + self._image_ext)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        # Example path to image set file:\n        # self._devkit_path + /VOCdevkit2007/VOC2007/ImageSets/Main/val.txt\n        image_set_file = os.path.join(self._data_path, \'ImageSets\', \'Main\',\n                                      self._image_set + \'.txt\')\n        assert os.path.exists(image_set_file), \\\n                \'Path does not exist: {}\'.format(image_set_file)\n        with open(image_set_file) as f:\n            image_index = [x.strip() for x in f.readlines()]\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where PASCAL VOC is expected to be installed.\n        """"""\n        return os.path.join(cfg.DATA_DIR, \'VOCdevkit\' + self._year)\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path, self.name + \'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} gt roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = [self._load_pascal_annotation(index)\n                    for index in self.image_index]\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote gt roidb to {}\'.format(cache_file)\n\n        return gt_roidb\n\n    def selective_search_roidb(self):\n        """"""\n        Return the database of selective search regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_selective_search_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} ss roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if int(self._year) == 2007 or self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n            ss_roidb = self._load_selective_search_roidb(gt_roidb)\n            roidb = imdb.merge_roidbs(gt_roidb, ss_roidb)\n        else:\n            roidb = self._load_selective_search_roidb(None)\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote ss roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def rpn_roidb(self):\n        if int(self._year) == 2007 or self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n            rpn_roidb = self._load_rpn_roidb(gt_roidb)\n            roidb = imdb.merge_roidbs(gt_roidb, rpn_roidb)\n        else:\n            roidb = self._load_rpn_roidb(None)\n\n        return roidb\n\n    def _load_rpn_roidb(self, gt_roidb):\n        filename = self.config[\'rpn_file\']\n        print \'loading {}\'.format(filename)\n        assert os.path.exists(filename), \\\n               \'rpn data not found at: {}\'.format(filename)\n        with open(filename, \'rb\') as f:\n            box_list = cPickle.load(f)\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def _load_selective_search_roidb(self, gt_roidb):\n        filename = os.path.abspath(os.path.join(cfg.DATA_DIR,\n                                                \'selective_search_data\',\n                                                self.name + \'.mat\'))\n        assert os.path.exists(filename), \\\n               \'Selective search data not found at: {}\'.format(filename)\n        raw_data = sio.loadmat(filename)[\'boxes\'].ravel()\n\n        box_list = []\n        for i in xrange(raw_data.shape[0]):\n            boxes = raw_data[i][:, (1, 0, 3, 2)] - 1\n            keep = ds_utils.unique_boxes(boxes)\n            boxes = boxes[keep, :]\n            keep = ds_utils.filter_small_boxes(boxes, self.config[\'min_size\'])\n            boxes = boxes[keep, :]\n            box_list.append(boxes)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def _load_pascal_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from XML file in the PASCAL VOC\n        format.\n        """"""\n        filename = os.path.join(self._data_path, \'Annotations\', index + \'.xml\')\n        tree = ET.parse(filename)\n        objs = tree.findall(\'object\')\n        if not self.config[\'use_diff\']:\n            # Exclude the samples labeled as difficult\n            non_diff_objs = [\n                obj for obj in objs if int(obj.find(\'difficult\').text) == 0]\n            # if len(non_diff_objs) != len(objs):\n            #     print \'Removed {} difficult objects\'.format(\n            #         len(objs) - len(non_diff_objs))\n            objs = non_diff_objs\n        num_objs = len(objs)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        # ""Seg"" area for pascal is just the box area\n        seg_areas = np.zeros((num_objs), dtype=np.float32)\n\n        # Load object bounding boxes into a data frame.\n        for ix, obj in enumerate(objs):\n            bbox = obj.find(\'bndbox\')\n            # Make pixel indexes 0-based\n            x1 = float(bbox.find(\'xmin\').text) - 1\n            y1 = float(bbox.find(\'ymin\').text) - 1\n            x2 = float(bbox.find(\'xmax\').text) - 1\n            y2 = float(bbox.find(\'ymax\').text) - 1\n            cls = self._class_to_ind[obj.find(\'name\').text.lower().strip()]\n            boxes[ix, :] = [x1, y1, x2, y2]\n            gt_classes[ix] = cls\n            overlaps[ix, cls] = 1.0\n            seg_areas[ix] = (x2 - x1 + 1) * (y2 - y1 + 1)\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_overlaps\' : overlaps,\n                \'flipped\' : False,\n                \'seg_areas\' : seg_areas}\n\n    def _get_comp_id(self):\n        comp_id = (self._comp_id + \'_\' + self._salt if self.config[\'use_salt\']\n            else self._comp_id)\n        return comp_id\n\n    def _get_voc_results_file_template(self):\n        # VOCdevkit/results/VOC2007/Main/<comp_id>_det_test_aeroplane.txt\n        filename = self._get_comp_id() + \'_det_\' + self._image_set + \'_{:s}.txt\'\n        path = os.path.join(\n            self._devkit_path,\n            \'results\',\n            \'VOC\' + self._year,\n            \'Main\',\n            filename)\n        return path\n\n    def _write_voc_results_file(self, all_boxes):\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == \'__background__\':\n                continue\n            print \'Writing {} VOC results file\'.format(cls)\n            filename = self._get_voc_results_file_template().format(cls)\n            with open(filename, \'wt\') as f:\n                for im_ind, index in enumerate(self.image_index):\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    # the VOCdevkit expects 1-based indices\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                                format(index, dets[k, -1],\n                                       dets[k, 0] + 1, dets[k, 1] + 1,\n                                       dets[k, 2] + 1, dets[k, 3] + 1))\n\n    def _do_python_eval(self, output_dir = \'output\'):\n        annopath = os.path.join(\n            self._devkit_path,\n            \'VOC\' + self._year,\n            \'Annotations\',\n            \'{:s}.xml\')\n        imagesetfile = os.path.join(\n            self._devkit_path,\n            \'VOC\' + self._year,\n            \'ImageSets\',\n            \'Main\',\n            self._image_set + \'.txt\')\n        cachedir = os.path.join(self._devkit_path, \'annotations_cache\')\n        aps = []\n        # The PASCAL VOC metric changed in 2010\n        use_07_metric = True if int(self._year) < 2010 else False\n        print \'VOC07 metric? \' + (\'Yes\' if use_07_metric else \'No\')\n        if not os.path.isdir(output_dir):\n            os.mkdir(output_dir)\n        for i, cls in enumerate(self._classes):\n            if cls == \'__background__\':\n                continue\n            filename = self._get_voc_results_file_template().format(cls)\n            rec, prec, ap = voc_eval(\n                filename, annopath, imagesetfile, cls, cachedir, ovthresh=0.5,\n                use_07_metric=use_07_metric)\n            aps += [ap]\n            print(\'AP for {} = {:.4f}\'.format(cls, ap))\n            with open(os.path.join(output_dir, cls + \'_pr.pkl\'), \'w\') as f:\n                cPickle.dump({\'rec\': rec, \'prec\': prec, \'ap\': ap}, f)\n        print(\'Mean AP = {:.4f}\'.format(np.mean(aps)))\n        print(\'~~~~~~~~\')\n        print(\'Results:\')\n        for ap in aps:\n            print(\'{:.3f}\'.format(ap))\n        print(\'{:.3f}\'.format(np.mean(aps)))\n        print(\'~~~~~~~~\')\n        print(\'\')\n        print(\'--------------------------------------------------------------\')\n        print(\'Results computed with the **unofficial** Python eval code.\')\n        print(\'Results should be very close to the official MATLAB eval code.\')\n        print(\'Recompute with `./tools/reval.py --matlab ...` for your paper.\')\n        print(\'-- Thanks, The Management\')\n        print(\'--------------------------------------------------------------\')\n\n    def _do_matlab_eval(self, output_dir=\'output\'):\n        print \'-----------------------------------------------------\'\n        print \'Computing results with the official MATLAB eval code.\'\n        print \'-----------------------------------------------------\'\n        path = os.path.join(cfg.ROOT_DIR, \'lib\', \'datasets\',\n                            \'VOCdevkit-matlab-wrapper\')\n        cmd = \'cd {} && \'.format(path)\n        cmd += \'{:s} -nodisplay -nodesktop \'.format(cfg.MATLAB)\n        cmd += \'-r ""dbstop if error; \'\n        cmd += \'voc_eval(\\\'{:s}\\\',\\\'{:s}\\\',\\\'{:s}\\\',\\\'{:s}\\\'); quit;""\' \\\n               .format(self._devkit_path, self._get_comp_id(),\n                       self._image_set, output_dir)\n        print(\'Running:\\n{}\'.format(cmd))\n        status = subprocess.call(cmd, shell=True)\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        self._write_voc_results_file(all_boxes)\n        self._do_python_eval(output_dir)\n        if self.config[\'matlab_eval\']:\n            self._do_matlab_eval(output_dir)\n        if self.config[\'cleanup\']:\n            for cls in self._classes:\n                if cls == \'__background__\':\n                    continue\n                filename = self._get_voc_results_file_template().format(cls)\n                os.remove(filename)\n\n    def competition_mode(self, on):\n        if on:\n            self.config[\'use_salt\'] = False\n            self.config[\'cleanup\'] = False\n        else:\n            self.config[\'use_salt\'] = True\n            self.config[\'cleanup\'] = True\n\nif __name__ == \'__main__\':\n    from datasets.pascal_voc import pascal_voc\n    d = pascal_voc(\'trainval\', \'2007\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
visual_search/lib/datasets/voc_eval.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Bharath Hariharan\n# --------------------------------------------------------\n\nimport xml.etree.ElementTree as ET\nimport os\nimport cPickle\nimport numpy as np\n\ndef parse_rec(filename):\n    """""" Parse a PASCAL VOC xml file """"""\n    tree = ET.parse(filename)\n    objects = []\n    for obj in tree.findall(\'object\'):\n        obj_struct = {}\n        obj_struct[\'name\'] = obj.find(\'name\').text\n        obj_struct[\'pose\'] = obj.find(\'pose\').text\n        obj_struct[\'truncated\'] = int(obj.find(\'truncated\').text)\n        obj_struct[\'difficult\'] = int(obj.find(\'difficult\').text)\n        bbox = obj.find(\'bndbox\')\n        obj_struct[\'bbox\'] = [int(bbox.find(\'xmin\').text),\n                              int(bbox.find(\'ymin\').text),\n                              int(bbox.find(\'xmax\').text),\n                              int(bbox.find(\'ymax\').text)]\n        objects.append(obj_struct)\n\n    return objects\n\ndef voc_ap(rec, prec, use_07_metric=False):\n    """""" ap = voc_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:False).\n    """"""\n    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\ndef voc_eval(detpath,\n             annopath,\n             imagesetfile,\n             classname,\n             cachedir,\n             ovthresh=0.5,\n             use_07_metric=False):\n    """"""rec, prec, ap = voc_eval(detpath,\n                                annopath,\n                                imagesetfile,\n                                classname,\n                                [ovthresh],\n                                [use_07_metric])\n\n    Top level function that does the PASCAL VOC evaluation.\n\n    detpath: Path to detections\n        detpath.format(classname) should produce the detection results file.\n    annopath: Path to annotations\n        annopath.format(imagename) should be the xml annotations file.\n    imagesetfile: Text file containing the list of images, one image per line.\n    classname: Category name (duh)\n    cachedir: Directory for caching the annotations\n    [ovthresh]: Overlap threshold (default = 0.5)\n    [use_07_metric]: Whether to use VOC07\'s 11 point AP computation\n        (default False)\n    """"""\n    # assumes detections are in detpath.format(classname)\n    # assumes annotations are in annopath.format(imagename)\n    # assumes imagesetfile is a text file with each line an image name\n    # cachedir caches the annotations in a pickle file\n\n    # first load gt\n    if not os.path.isdir(cachedir):\n        os.mkdir(cachedir)\n    cachefile = os.path.join(cachedir, \'annots.pkl\')\n    # read list of images\n    with open(imagesetfile, \'r\') as f:\n        lines = f.readlines()\n    imagenames = [x.strip() for x in lines]\n\n    if not os.path.isfile(cachefile):\n        # load annots\n        recs = {}\n        for i, imagename in enumerate(imagenames):\n            recs[imagename] = parse_rec(annopath.format(imagename))\n            if i % 100 == 0:\n                print \'Reading annotation for {:d}/{:d}\'.format(\n                    i + 1, len(imagenames))\n        # save\n        print \'Saving cached annotations to {:s}\'.format(cachefile)\n        with open(cachefile, \'w\') as f:\n            cPickle.dump(recs, f)\n    else:\n        # load\n        with open(cachefile, \'r\') as f:\n            recs = cPickle.load(f)\n\n    # extract gt objects for this class\n    class_recs = {}\n    npos = 0\n    for imagename in imagenames:\n        R = [obj for obj in recs[imagename] if obj[\'name\'] == classname]\n        bbox = np.array([x[\'bbox\'] for x in R])\n        difficult = np.array([x[\'difficult\'] for x in R]).astype(np.bool)\n        det = [False] * len(R)\n        npos = npos + sum(~difficult)\n        class_recs[imagename] = {\'bbox\': bbox,\n                                 \'difficult\': difficult,\n                                 \'det\': det}\n\n    # read dets\n    detfile = detpath.format(classname)\n    with open(detfile, \'r\') as f:\n        lines = f.readlines()\n\n    splitlines = [x.strip().split(\' \') for x in lines]\n    image_ids = [x[0] for x in splitlines]\n    confidence = np.array([float(x[1]) for x in splitlines])\n    BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n\n    nd = len(image_ids)\n    tp = np.zeros(nd)\n    fp = np.zeros(nd)\n\n    if BB.shape[0] > 0:\n        # sort by confidence\n        sorted_ind = np.argsort(-confidence)\n        sorted_scores = np.sort(-confidence)\n        BB = BB[sorted_ind, :]\n        image_ids = [image_ids[x] for x in sorted_ind]\n\n        # go down dets and mark TPs and FPs\n        for d in range(nd):\n            R = class_recs[image_ids[d]]\n            bb = BB[d, :].astype(float)\n            ovmax = -np.inf\n            BBGT = R[\'bbox\'].astype(float)\n\n            if BBGT.size > 0:\n                # compute overlaps\n                # intersection\n                ixmin = np.maximum(BBGT[:, 0], bb[0])\n                iymin = np.maximum(BBGT[:, 1], bb[1])\n                ixmax = np.minimum(BBGT[:, 2], bb[2])\n                iymax = np.minimum(BBGT[:, 3], bb[3])\n                iw = np.maximum(ixmax - ixmin + 1., 0.)\n                ih = np.maximum(iymax - iymin + 1., 0.)\n                inters = iw * ih\n\n                # union\n                uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n                       (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n                       (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n\n                overlaps = inters / uni\n                ovmax = np.max(overlaps)\n                jmax = np.argmax(overlaps)\n\n            if ovmax > ovthresh:\n                if not R[\'difficult\'][jmax]:\n                    if not R[\'det\'][jmax]:\n                        tp[d] = 1.\n                        R[\'det\'][jmax] = 1\n                    else:\n                        fp[d] = 1.\n            else:\n                fp[d] = 1.\n\n    # compute precision recall\n    fp = np.cumsum(fp)\n    tp = np.cumsum(tp)\n    rec = tp / float(npos)\n    # avoid divide by zero in case the first detection matches a difficult\n    # ground truth\n    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n    ap = voc_ap(rec, prec, use_07_metric)\n\n    return rec, prec, ap\n'"
visual_search/lib/es/ImFea_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: protobuf/ImFea.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'protobuf/ImFea.proto\',\n  package=\'es.fea\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n\\x14protobuf/ImFea.proto\\x12\\x06\\x65s.fea\\""&\\n\\x08ImFeaArr\\x12\\x1a\\n\\x03\\x61rr\\x18\\x01 \\x03(\\x0b\\x32\\r.es.fea.ImFea\\""\\x12\\n\\x05ImFea\\x12\\t\\n\\x01\\x66\\x18\\x01 \\x03(\\x01\\"",\\n\\x0bImFeaBinArr\\x12\\x1d\\n\\x03\\x61rr\\x18\\x01 \\x03(\\x0b\\x32\\x10.es.fea.ImFeaBin\\""\\x15\\n\\x08ImFeaBin\\x12\\t\\n\\x01\\x66\\x18\\x01 \\x03(\\x04\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_IMFEAARR = _descriptor.Descriptor(\n  name=\'ImFeaArr\',\n  full_name=\'es.fea.ImFeaArr\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'arr\', full_name=\'es.fea.ImFeaArr.arr\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=32,\n  serialized_end=70,\n)\n\n\n_IMFEA = _descriptor.Descriptor(\n  name=\'ImFea\',\n  full_name=\'es.fea.ImFea\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'f\', full_name=\'es.fea.ImFea.f\', index=0,\n      number=1, type=1, cpp_type=5, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=72,\n  serialized_end=90,\n)\n\n\n_IMFEABINARR = _descriptor.Descriptor(\n  name=\'ImFeaBinArr\',\n  full_name=\'es.fea.ImFeaBinArr\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'arr\', full_name=\'es.fea.ImFeaBinArr.arr\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=92,\n  serialized_end=136,\n)\n\n\n_IMFEABIN = _descriptor.Descriptor(\n  name=\'ImFeaBin\',\n  full_name=\'es.fea.ImFeaBin\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'f\', full_name=\'es.fea.ImFeaBin.f\', index=0,\n      number=1, type=4, cpp_type=4, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=138,\n  serialized_end=159,\n)\n\n_IMFEAARR.fields_by_name[\'arr\'].message_type = _IMFEA\n_IMFEABINARR.fields_by_name[\'arr\'].message_type = _IMFEABIN\nDESCRIPTOR.message_types_by_name[\'ImFeaArr\'] = _IMFEAARR\nDESCRIPTOR.message_types_by_name[\'ImFea\'] = _IMFEA\nDESCRIPTOR.message_types_by_name[\'ImFeaBinArr\'] = _IMFEABINARR\nDESCRIPTOR.message_types_by_name[\'ImFeaBin\'] = _IMFEABIN\n\nImFeaArr = _reflection.GeneratedProtocolMessageType(\'ImFeaArr\', (_message.Message,), dict(\n  DESCRIPTOR = _IMFEAARR,\n  __module__ = \'protobuf.ImFea_pb2\'\n  # @@protoc_insertion_point(class_scope:es.fea.ImFeaArr)\n  ))\n_sym_db.RegisterMessage(ImFeaArr)\n\nImFea = _reflection.GeneratedProtocolMessageType(\'ImFea\', (_message.Message,), dict(\n  DESCRIPTOR = _IMFEA,\n  __module__ = \'protobuf.ImFea_pb2\'\n  # @@protoc_insertion_point(class_scope:es.fea.ImFea)\n  ))\n_sym_db.RegisterMessage(ImFea)\n\nImFeaBinArr = _reflection.GeneratedProtocolMessageType(\'ImFeaBinArr\', (_message.Message,), dict(\n  DESCRIPTOR = _IMFEABINARR,\n  __module__ = \'protobuf.ImFea_pb2\'\n  # @@protoc_insertion_point(class_scope:es.fea.ImFeaBinArr)\n  ))\n_sym_db.RegisterMessage(ImFeaBinArr)\n\nImFeaBin = _reflection.GeneratedProtocolMessageType(\'ImFeaBin\', (_message.Message,), dict(\n  DESCRIPTOR = _IMFEABIN,\n  __module__ = \'protobuf.ImFea_pb2\'\n  # @@protoc_insertion_point(class_scope:es.fea.ImFeaBin)\n  ))\n_sym_db.RegisterMessage(ImFeaBin)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
visual_search/lib/es/__init__.py,0,b'\nfrom . import *\n'
visual_search/lib/layer_utils/__init__.py,0,b''
visual_search/lib/layer_utils/anchor_target_layer.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Xinlei Chen\n# --------------------------------------------------------\n\nimport os\nfrom model.config import cfg\nimport numpy as np\nimport numpy.random as npr\nfrom utils.cython_bbox import bbox_overlaps\nfrom model.bbox_transform import bbox_transform\n\ndef anchor_target_layer(rpn_cls_score, gt_boxes, im_info, _feat_stride, all_anchors, anchor_scales):\n    """"""Same as the anchor target layer in original Fast/er RCNN """"""\n    scales = np.array(anchor_scales)\n    num_anchors = scales.shape[0] * 3\n    A = num_anchors\n    total_anchors = all_anchors.shape[0]\n    K = total_anchors / num_anchors\n    im_info = im_info[0]\n\n    # allow boxes to sit over the edge by a small amount\n    _allowed_border = 0\n\n    # map of shape (..., H, W)\n    height, width = rpn_cls_score.shape[1:3]\n\n    # only keep anchors inside the image\n    inds_inside = np.where(\n        (all_anchors[:, 0] >= -_allowed_border) &\n        (all_anchors[:, 1] >= -_allowed_border) &\n        (all_anchors[:, 2] < im_info[1] + _allowed_border) &  # width\n        (all_anchors[:, 3] < im_info[0] + _allowed_border)    # height\n    )[0]\n\n    # keep only inside anchors\n    anchors = all_anchors[inds_inside, :]\n\n    # label: 1 is positive, 0 is negative, -1 is dont care\n    labels = np.empty((len(inds_inside), ), dtype=np.float32)\n    labels.fill(-1)\n\n    # overlaps between the anchors and the gt boxes\n    # overlaps (ex, gt)\n    overlaps = bbox_overlaps(\n        np.ascontiguousarray(anchors, dtype=np.float),\n        np.ascontiguousarray(gt_boxes, dtype=np.float))\n    argmax_overlaps = overlaps.argmax(axis=1)\n    max_overlaps = overlaps[np.arange(len(inds_inside)), argmax_overlaps]\n    gt_argmax_overlaps = overlaps.argmax(axis=0)\n    gt_max_overlaps = overlaps[gt_argmax_overlaps,\n                               np.arange(overlaps.shape[1])]\n    gt_argmax_overlaps = np.where(overlaps == gt_max_overlaps)[0]\n\n    if not cfg.TRAIN.RPN_CLOBBER_POSITIVES:\n        # assign bg labels first so that positive labels can clobber them\n        # first set the negatives\n        labels[max_overlaps < cfg.TRAIN.RPN_NEGATIVE_OVERLAP] = 0\n\n    # fg label: for each gt, anchor with highest overlap\n    labels[gt_argmax_overlaps] = 1\n\n    # fg label: above threshold IOU\n    labels[max_overlaps >= cfg.TRAIN.RPN_POSITIVE_OVERLAP] = 1\n\n    if cfg.TRAIN.RPN_CLOBBER_POSITIVES:\n        # assign bg labels last so that negative labels can clobber positives\n        labels[max_overlaps < cfg.TRAIN.RPN_NEGATIVE_OVERLAP] = 0\n\n    # subsample positive labels if we have too many\n    num_fg = int(cfg.TRAIN.RPN_FG_FRACTION * cfg.TRAIN.RPN_BATCHSIZE)\n    fg_inds = np.where(labels == 1)[0]\n    if len(fg_inds) > num_fg:\n        disable_inds = npr.choice(\n            fg_inds, size=(len(fg_inds) - num_fg), replace=False)\n        labels[disable_inds] = -1\n\n    # subsample negative labels if we have too many\n    num_bg = cfg.TRAIN.RPN_BATCHSIZE - np.sum(labels == 1)\n    bg_inds = np.where(labels == 0)[0]\n    if len(bg_inds) > num_bg:\n        disable_inds = npr.choice(\n            bg_inds, size=(len(bg_inds) - num_bg), replace=False)\n        labels[disable_inds] = -1\n\n    bbox_targets = np.zeros((len(inds_inside), 4), dtype=np.float32)\n    bbox_targets = _compute_targets(anchors, gt_boxes[argmax_overlaps, :])\n\n    bbox_inside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n    # only the positive ones have regression targets\n    bbox_inside_weights[labels == 1, :] = np.array(cfg.TRAIN.RPN_BBOX_INSIDE_WEIGHTS)\n\n    bbox_outside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n    if cfg.TRAIN.RPN_POSITIVE_WEIGHT < 0:\n        # uniform weighting of examples (given non-uniform sampling)\n        num_examples = np.sum(labels >= 0)\n        positive_weights = np.ones((1, 4)) * 1.0 / num_examples\n        negative_weights = np.ones((1, 4)) * 1.0 / num_examples\n    else:\n        assert ((cfg.TRAIN.RPN_POSITIVE_WEIGHT > 0) &\n                (cfg.TRAIN.RPN_POSITIVE_WEIGHT < 1))\n        positive_weights = (cfg.TRAIN.RPN_POSITIVE_WEIGHT /\n                            np.sum(labels == 1))\n        negative_weights = ((1.0 - cfg.TRAIN.RPN_POSITIVE_WEIGHT) /\n                            np.sum(labels == 0))\n    bbox_outside_weights[labels == 1, :] = positive_weights\n    bbox_outside_weights[labels == 0, :] = negative_weights\n\n    # map up to original set of anchors\n    labels = _unmap(labels, total_anchors, inds_inside, fill=-1)\n    bbox_targets = _unmap(bbox_targets, total_anchors, inds_inside, fill=0)\n    bbox_inside_weights = _unmap(bbox_inside_weights, total_anchors, inds_inside, fill=0)\n    bbox_outside_weights = _unmap(bbox_outside_weights, total_anchors, inds_inside, fill=0)\n\n    # labels\n    labels = labels.reshape((1, height, width, A)).transpose(0, 3, 1, 2)\n    labels = labels.reshape((1, 1, A * height, width))\n    rpn_labels = labels\n\n    # bbox_targets\n    bbox_targets = bbox_targets \\\n        .reshape((1, height, width, A * 4))\n\n    rpn_bbox_targets = bbox_targets\n    # bbox_inside_weights\n    bbox_inside_weights = bbox_inside_weights \\\n        .reshape((1, height, width, A * 4))\n\n    rpn_bbox_inside_weights = bbox_inside_weights\n\n    # bbox_outside_weights\n    bbox_outside_weights = bbox_outside_weights \\\n        .reshape((1, height, width, A * 4))\n\n    rpn_bbox_outside_weights = bbox_outside_weights\n    return rpn_labels, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights\n\n\ndef _unmap(data, count, inds, fill=0):\n    """""" Unmap a subset of item (data) back to the original set of items (of\n    size count) """"""\n    if len(data.shape) == 1:\n        ret = np.empty((count, ), dtype=np.float32)\n        ret.fill(fill)\n        ret[inds] = data\n    else:\n        ret = np.empty((count, ) + data.shape[1:], dtype=np.float32)\n        ret.fill(fill)\n        ret[inds, :] = data\n    return ret\n\n\ndef _compute_targets(ex_rois, gt_rois):\n    """"""Compute bounding-box regression targets for an image.""""""\n\n    assert ex_rois.shape[0] == gt_rois.shape[0]\n    assert ex_rois.shape[1] == 4\n    assert gt_rois.shape[1] == 5\n\n    return bbox_transform(ex_rois, gt_rois[:, :4]).astype(np.float32, copy=False)\n'"
visual_search/lib/layer_utils/generate_anchors.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\n\nimport numpy as np\n\n# Verify that we compute the same anchors as Shaoqing\'s matlab implementation:\n#\n#    >> load output/rpn_cachedir/faster_rcnn_VOC2007_ZF_stage1_rpn/anchors.mat\n#    >> anchors\n#\n#    anchors =\n#\n#       -83   -39   100    56\n#      -175   -87   192   104\n#      -359  -183   376   200\n#       -55   -55    72    72\n#      -119  -119   136   136\n#      -247  -247   264   264\n#       -35   -79    52    96\n#       -79  -167    96   184\n#      -167  -343   184   360\n\n#array([[ -83.,  -39.,  100.,   56.],\n#       [-175.,  -87.,  192.,  104.],\n#       [-359., -183.,  376.,  200.],\n#       [ -55.,  -55.,   72.,   72.],\n#       [-119., -119.,  136.,  136.],\n#       [-247., -247.,  264.,  264.],\n#       [ -35.,  -79.,   52.,   96.],\n#       [ -79., -167.,   96.,  184.],\n#       [-167., -343.,  184.,  360.]])\n\ndef generate_anchors(base_size=16, ratios=[0.5, 1, 2],\n                     scales=2**np.arange(3, 6)):\n    """"""\n    Generate anchor (reference) windows by enumerating aspect ratios X\n    scales wrt a reference (0, 0, 15, 15) window.\n    """"""\n\n    base_anchor = np.array([1, 1, base_size, base_size]) - 1\n    ratio_anchors = _ratio_enum(base_anchor, ratios)\n    anchors = np.vstack([_scale_enum(ratio_anchors[i, :], scales)\n                         for i in xrange(ratio_anchors.shape[0])])\n    return anchors\n\ndef _whctrs(anchor):\n    """"""\n    Return width, height, x center, and y center for an anchor (window).\n    """"""\n\n    w = anchor[2] - anchor[0] + 1\n    h = anchor[3] - anchor[1] + 1\n    x_ctr = anchor[0] + 0.5 * (w - 1)\n    y_ctr = anchor[1] + 0.5 * (h - 1)\n    return w, h, x_ctr, y_ctr\n\ndef _mkanchors(ws, hs, x_ctr, y_ctr):\n    """"""\n    Given a vector of widths (ws) and heights (hs) around a center\n    (x_ctr, y_ctr), output a set of anchors (windows).\n    """"""\n\n    ws = ws[:, np.newaxis]\n    hs = hs[:, np.newaxis]\n    anchors = np.hstack((x_ctr - 0.5 * (ws - 1),\n                         y_ctr - 0.5 * (hs - 1),\n                         x_ctr + 0.5 * (ws - 1),\n                         y_ctr + 0.5 * (hs - 1)))\n    return anchors\n\ndef _ratio_enum(anchor, ratios):\n    """"""\n    Enumerate a set of anchors for each aspect ratio wrt an anchor.\n    """"""\n\n    w, h, x_ctr, y_ctr = _whctrs(anchor)\n    size = w * h\n    size_ratios = size / ratios\n    ws = np.round(np.sqrt(size_ratios))\n    hs = np.round(ws * ratios)\n    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n    return anchors\n\ndef _scale_enum(anchor, scales):\n    """"""\n    Enumerate a set of anchors for each scale wrt an anchor.\n    """"""\n\n    w, h, x_ctr, y_ctr = _whctrs(anchor)\n    ws = w * scales\n    hs = h * scales\n    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n    return anchors\n\nif __name__ == \'__main__\':\n    import time\n    t = time.time()\n    a = generate_anchors()\n    print time.time() - t\n    print a\n    from IPython import embed; embed()\n'"
visual_search/lib/layer_utils/proposal_layer.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Xinlei Chen\n# --------------------------------------------------------\n\nimport numpy as np\nfrom model.config import cfg\nfrom model.bbox_transform import bbox_transform_inv, clip_boxes\nfrom model.nms_wrapper import nms\n\ndef proposal_layer(rpn_cls_prob, rpn_bbox_pred, im_info, cfg_key, _feat_stride, anchors, anchor_scales):\n  """"""A simplified version compared to fast/er RCNN\n     For details please see the technical report\n  """"""\n  pre_nms_topN  = cfg[cfg_key].RPN_PRE_NMS_TOP_N\n  post_nms_topN = cfg[cfg_key].RPN_POST_NMS_TOP_N\n  nms_thresh  = cfg[cfg_key].RPN_NMS_THRESH\n\n  scales = np.array(anchor_scales)\n  num_anchors = scales.shape[0] * 3\n  im_info = im_info[0]\n\n  # Get the scores and bounding boxes\n  scores = rpn_cls_prob[:, :, :, num_anchors:]\n  rpn_bbox_pred = rpn_bbox_pred.reshape((-1, 4))\n  scores = scores.reshape((-1, 1))\n  proposals = bbox_transform_inv(anchors, rpn_bbox_pred)\n  proposals = clip_boxes(proposals, im_info[:2])\n\n  # Pick the top region proposals\n  order = scores.ravel().argsort()[::-1]\n  if pre_nms_topN > 0:\n    order = order[:pre_nms_topN]\n  proposals = proposals[order, :]\n  scores = scores[order]\n\n  # Non-maximal suppression\n  keep = nms(np.hstack((proposals, scores)), nms_thresh)\n\n  # Pick th top region proposals after NMS\n  if post_nms_topN > 0:\n    keep = keep[:post_nms_topN]\n  proposals = proposals[keep, :]\n  scores = scores[keep]\n  \n  # Only support single image as input\n  batch_inds = np.zeros((proposals.shape[0], 1), dtype=np.float32)\n  blob = np.hstack((batch_inds, proposals.astype(np.float32, copy=False)))\n\n  return blob, scores\n\n\n'"
visual_search/lib/layer_utils/proposal_target_layer.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick, Sean Bell and Xinlei Chen\n# --------------------------------------------------------\n\nimport numpy as np\nimport numpy.random as npr\nfrom model.config import cfg\nfrom model.bbox_transform import bbox_transform\nfrom utils.cython_bbox import bbox_overlaps\n\ndef proposal_target_layer(rpn_rois, rpn_scores, gt_boxes, _num_classes):\n    """"""\n    Assign object detection proposals to ground-truth targets. Produces proposal\n    classification labels and bounding-box regression targets.\n    """"""\n\n    # Proposal ROIs (0, x1, y1, x2, y2) coming from RPN\n    # (i.e., rpn.proposal_layer.ProposalLayer), or any other source\n    all_rois = rpn_rois\n    all_scores = rpn_scores\n\n    # Include ground-truth boxes in the set of candidate rois\n    if cfg.TRAIN.USE_GT:\n        zeros = np.zeros((gt_boxes.shape[0], 1), dtype=gt_boxes.dtype)\n        all_rois = np.vstack(\n            (all_rois, np.hstack((zeros, gt_boxes[:, :-1])))\n        )\n        # not sure if it a wise appending, but anyway i am not using it\n        all_scores = np.vstack((all_scores, zeros))\n\n    num_images = 1\n    rois_per_image = cfg.TRAIN.BATCH_SIZE / num_images\n    fg_rois_per_image = np.round(cfg.TRAIN.FG_FRACTION * rois_per_image)\n\n    # Sample rois with classification labels and bounding box regression\n    # targets\n    labels, rois, roi_scores, bbox_targets, bbox_inside_weights = _sample_rois(\n        all_rois, all_scores, gt_boxes, fg_rois_per_image,\n        rois_per_image, _num_classes)\n\n    rois = rois.reshape(-1,5)\n    roi_scores = roi_scores.reshape(-1)\n    labels = labels.reshape(-1,1)\n    bbox_targets = bbox_targets.reshape(-1,_num_classes*4)\n    bbox_inside_weights = bbox_inside_weights.reshape(-1,_num_classes*4)\n    bbox_outside_weights = np.array(bbox_inside_weights > 0).astype(np.float32)\n\n    return rois, roi_scores, labels, bbox_targets, bbox_inside_weights, bbox_outside_weights\n\ndef _get_bbox_regression_labels(bbox_target_data, num_classes):\n    """"""Bounding-box regression targets (bbox_target_data) are stored in a\n    compact form N x (class, tx, ty, tw, th)\n\n    This function expands those targets into the 4-of-4*K representation used\n    by the network (i.e. only one class has non-zero targets).\n\n    Returns:\n        bbox_target (ndarray): N x 4K blob of regression targets\n        bbox_inside_weights (ndarray): N x 4K blob of loss weights\n    """"""\n\n    clss = bbox_target_data[:, 0]\n    bbox_targets = np.zeros((clss.size, 4 * num_classes), dtype=np.float32)\n    bbox_inside_weights = np.zeros(bbox_targets.shape, dtype=np.float32)\n    inds = np.where(clss > 0)[0]\n    for ind in inds:\n        cls = clss[ind]\n        start = int(4 * cls)\n        end = start + 4\n        bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]\n        bbox_inside_weights[ind, start:end] = cfg.TRAIN.BBOX_INSIDE_WEIGHTS\n    return bbox_targets, bbox_inside_weights\n\ndef _compute_targets(ex_rois, gt_rois, labels):\n    """"""Compute bounding-box regression targets for an image.""""""\n\n    assert ex_rois.shape[0] == gt_rois.shape[0]\n    assert ex_rois.shape[1] == 4\n    assert gt_rois.shape[1] == 4\n\n    targets = bbox_transform(ex_rois, gt_rois)\n    if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n        # Optionally normalize targets by a precomputed mean and stdev\n        targets = ((targets - np.array(cfg.TRAIN.BBOX_NORMALIZE_MEANS))\n                / np.array(cfg.TRAIN.BBOX_NORMALIZE_STDS))\n    return np.hstack(\n            (labels[:, np.newaxis], targets)).astype(np.float32, copy=False)\n\ndef _sample_rois(all_rois, all_scores, gt_boxes, fg_rois_per_image, rois_per_image, num_classes):\n    """"""Generate a random sample of RoIs comprising foreground and background\n    examples.\n    """"""\n    # overlaps: (rois x gt_boxes)\n    overlaps = bbox_overlaps(\n        np.ascontiguousarray(all_rois[:, 1:5], dtype=np.float),\n        np.ascontiguousarray(gt_boxes[:, :4], dtype=np.float))\n    gt_assignment = overlaps.argmax(axis=1)\n    max_overlaps = overlaps.max(axis=1)\n    labels = gt_boxes[gt_assignment, 4]\n\n    # Select foreground RoIs as those with >= FG_THRESH overlap\n    fg_inds = np.where(max_overlaps >= cfg.TRAIN.FG_THRESH)[0]\n    # Guard against the case when an image has fewer than fg_rois_per_image\n    # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n    bg_inds = np.where((max_overlaps < cfg.TRAIN.BG_THRESH_HI) &\n                       (max_overlaps >= cfg.TRAIN.BG_THRESH_LO))[0]\n    \n    # Small modification to the original version where we ensure a fixed number of regions are sampled\n    if fg_inds.size > 0 and bg_inds.size > 0:\n        fg_rois_per_image = min(fg_rois_per_image, fg_inds.size)\n        fg_inds = npr.choice(fg_inds, size=int(fg_rois_per_image), replace=False)\n        bg_rois_per_image = rois_per_image - fg_rois_per_image\n        to_replace = bg_inds.size < bg_rois_per_image\n        bg_inds = npr.choice(bg_inds, size=int(bg_rois_per_image), replace=to_replace)\n    elif fg_inds.size > 0:\n        to_replace = fg_inds.size < rois_per_image\n        fg_inds = npr.choice(fg_inds, size=int(rois_per_image), replace=to_replace)\n        fg_rois_per_image = rois_per_image\n    elif bg_inds.size > 0:\n        to_replace = bg_inds.size < rois_per_image\n        bg_inds = npr.choice(bg_inds, size=int(rois_per_image), replace=to_replace)\n        fg_rois_per_image = 0\n    else:\n        import pdb\n        pdb.set_trace()\n\n    # The indices that we\'re selecting (both fg and bg)\n    keep_inds = np.append(fg_inds, bg_inds)\n    # Select sampled values from various arrays:\n    labels = labels[keep_inds]\n    # Clamp labels for the background RoIs to 0\n    labels[int(fg_rois_per_image):] = 0\n    rois = all_rois[keep_inds]\n    roi_scores = all_scores[keep_inds]\n\n    bbox_target_data = _compute_targets(\n        rois[:, 1:5], gt_boxes[gt_assignment[keep_inds], :4], labels)\n\n    bbox_targets, bbox_inside_weights = \\\n        _get_bbox_regression_labels(bbox_target_data, num_classes)\n\n    return labels, rois, roi_scores, bbox_targets, bbox_inside_weights\n'"
visual_search/lib/layer_utils/proposal_top_layer.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Xinlei Chen\n# --------------------------------------------------------\n\nimport numpy as np\nfrom model.config import cfg\nfrom model.bbox_transform import bbox_transform_inv, clip_boxes\nimport numpy.random as npr\n\ndef proposal_top_layer(rpn_cls_prob, rpn_bbox_pred, im_info, _feat_stride, anchors, anchor_scales):\n  """"""A layer that just selects the top region proposals\n     without using non-maximal suppression,\n     For details please see the technical report\n  """"""\n  rpn_top_n = cfg.TEST.RPN_TOP_N\n  scales = np.array(anchor_scales)\n  num_anchors = scales.shape[0] * 3\n  im_info = im_info[0]\n\n  scores = rpn_cls_prob[:, :, :, num_anchors:]\n\n  rpn_bbox_pred = rpn_bbox_pred.reshape((-1, 4))\n  scores = scores.reshape((-1, 1))\n\n  length = scores.shape[0]\n  if length < rpn_top_n:\n    # Random selection, maybe unnecessary and loses good proposals\n    # But this case rarely happen\n    top_inds = npr.choice(length, size=rpn_top_n, replace=True)\n  else:\n    top_inds = scores.argsort(0)[::-1]\n    top_inds = top_inds[:rpn_top_n]\n    top_inds = top_inds.reshape(rpn_top_n,)\n\n  # Do the selection here\n  anchors = anchors[top_inds,:]\n  rpn_bbox_pred = rpn_bbox_pred[top_inds,:]\n  scores = scores[top_inds]\n\n  # Convert anchors into proposals via bbox transformations\n  proposals = bbox_transform_inv(anchors, rpn_bbox_pred)\n\n  # Clip predicted boxes to image\n  proposals = clip_boxes(proposals, im_info[:2])\n\n  # Output rois blob\n  # Our RPN implementation only supports a single input image, so all\n  # batch inds are 0\n  batch_inds = np.zeros((proposals.shape[0], 1), dtype=np.float32)\n  blob = np.hstack((batch_inds, proposals.astype(np.float32, copy=False)))\n  return blob, scores\n\n'"
visual_search/lib/layer_utils/snippets.py,0,"b'# --------------------------------------------------------\n# Tensorflow Faster R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Xinlei Chen\n# --------------------------------------------------------\n\nimport numpy as np\nimport numpy.random as npr\nfrom model.config import cfg\nfrom layer_utils.generate_anchors import generate_anchors\nfrom model.bbox_transform import bbox_transform_inv, clip_boxes\nfrom utils.cython_bbox import bbox_overlaps\n\ndef generate_anchors_pre(height, width, feat_stride, anchor_scales):\n  """""" A wrapper function to generate anchors given different scales\n    Also return the number of anchors in variable \'length\'\n  """"""\n  anchors = generate_anchors(scales=np.array(anchor_scales))\n  A = anchors.shape[0]\n  shift_x = np.arange(0, width) * feat_stride\n  shift_y = np.arange(0, height) * feat_stride\n  shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n  shifts = np.vstack((shift_x.ravel(), shift_y.ravel(), shift_x.ravel(), shift_y.ravel())).transpose()\n  K = shifts.shape[0]\n  # width changes faster, so here it is H, W, C\n  anchors = anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2))\n  anchors = anchors.reshape((K * A, 4)).astype(np.float32, copy=False)\n  length = np.int32(anchors.shape[0])\n\n  return anchors, length'"
visual_search/lib/model/__init__.py,0,b'from . import config\n'
visual_search/lib/model/bbox_transform.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\n\ndef bbox_transform(ex_rois, gt_rois):\n    ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0\n    ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0\n    ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\n    ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n\n    gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + 1.0\n    gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + 1.0\n    gt_ctr_x = gt_rois[:, 0] + 0.5 * gt_widths\n    gt_ctr_y = gt_rois[:, 1] + 0.5 * gt_heights\n\n    targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n    targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n    targets_dw = np.log(gt_widths / ex_widths)\n    targets_dh = np.log(gt_heights / ex_heights)\n\n    targets = np.vstack(\n        (targets_dx, targets_dy, targets_dw, targets_dh)).transpose()\n    return targets\n\ndef bbox_transform_inv(boxes, deltas):\n    if boxes.shape[0] == 0:\n        return np.zeros((0, deltas.shape[1]), dtype=deltas.dtype)\n\n    boxes = boxes.astype(deltas.dtype, copy=False)\n\n    widths = boxes[:, 2] - boxes[:, 0] + 1.0\n    heights = boxes[:, 3] - boxes[:, 1] + 1.0\n    ctr_x = boxes[:, 0] + 0.5 * widths\n    ctr_y = boxes[:, 1] + 0.5 * heights\n\n    dx = deltas[:, 0::4]\n    dy = deltas[:, 1::4]\n    dw = deltas[:, 2::4]\n    dh = deltas[:, 3::4]\n\n    pred_ctr_x = dx * widths[:, np.newaxis] + ctr_x[:, np.newaxis]\n    pred_ctr_y = dy * heights[:, np.newaxis] + ctr_y[:, np.newaxis]\n    pred_w = np.exp(dw) * widths[:, np.newaxis]\n    pred_h = np.exp(dh) * heights[:, np.newaxis]\n\n    pred_boxes = np.zeros(deltas.shape, dtype=deltas.dtype)\n    # x1\n    pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n    # y1\n    pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n    # x2\n    pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n    # y2\n    pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n    return pred_boxes\n\ndef clip_boxes(boxes, im_shape):\n    """"""\n    Clip boxes to image boundaries.\n    """"""\n\n    # x1 >= 0\n    boxes[:, 0::4] = np.maximum(np.minimum(boxes[:, 0::4], im_shape[1] - 1), 0)\n    # y1 >= 0\n    boxes[:, 1::4] = np.maximum(np.minimum(boxes[:, 1::4], im_shape[0] - 1), 0)\n    # x2 < im_shape[1]\n    boxes[:, 2::4] = np.maximum(np.minimum(boxes[:, 2::4], im_shape[1] - 1), 0)\n    # y2 < im_shape[0]\n    boxes[:, 3::4] = np.maximum(np.minimum(boxes[:, 3::4], im_shape[0] - 1), 0)\n    return boxes\n'"
visual_search/lib/model/config.py,0,"b'import os\nimport os.path as osp\nimport numpy as np\n# `pip install easydict` if you don\'t have it\nfrom easydict import EasyDict as edict\n\n__C = edict()\n# Consumers can get config by:\n#   from fast_rcnn_config import cfg\ncfg = __C\n\n#\n# Training options\n#\n__C.TRAIN = edict()\n\n# Initial learning rate\n__C.TRAIN.LEARNING_RATE = 0.001\n\n# Momentum\n__C.TRAIN.MOMENTUM = 0.9 \n\n# Weight decay, for regularization\n__C.TRAIN.WEIGHT_DECAY = 0.0005\n\n# Factor for reducing the learning rate\n__C.TRAIN.GAMMA = 0.1\n\n# Step size for reducing the learning rate, currently only support one step\n__C.TRAIN.STEPSIZE = 30000\n\n# Iteration intervals for showing the loss during training, on command line interface\n__C.TRAIN.DISPLAY = 10\n\n# Whether to double the learning rate for bias\n__C.TRAIN.DOUBLE_BIAS = True\n\n# Whether to initialize the weights with truncated normal distribution \n__C.TRAIN.TRUNCATED = False\n\n# Whether to have weight decay on bias as well\n__C.TRAIN.BIAS_DECAY = False\n\n# Whether to add ground truth boxes to the pool when sampling regions\n__C.TRAIN.USE_GT = False\n\n# Whether to use aspect-ratio grouping of training images, introduced merely for saving\n# GPU memory\n__C.TRAIN.ASPECT_GROUPING = False\n\n# The number of snapshots kept, older ones are deleted to save space\n__C.TRAIN.SNAPSHOT_KEPT = 3\n\n# The time interval for saving tensorflow summaries\n__C.TRAIN.SUMMARY_INTERVAL = 180\n\n# Scales to use during training (can list multiple scales)\n# Each scale is the pixel size of an image\'s shortest side\n__C.TRAIN.SCALES = (600,)\n\n# Max pixel size of the longest side of a scaled input image\n__C.TRAIN.MAX_SIZE = 1000\n\n# Images to use per minibatch\n__C.TRAIN.IMS_PER_BATCH = 2\n\n# Minibatch size (number of regions of interest [ROIs])\n__C.TRAIN.BATCH_SIZE = 128\n\n# Fraction of minibatch that is labeled foreground (i.e. class > 0)\n__C.TRAIN.FG_FRACTION = 0.25\n\n# Overlap threshold for a ROI to be considered foreground (if >= FG_THRESH)\n__C.TRAIN.FG_THRESH = 0.5\n\n# Overlap threshold for a ROI to be considered background (class = 0 if\n# overlap in [LO, HI))\n__C.TRAIN.BG_THRESH_HI = 0.5\n__C.TRAIN.BG_THRESH_LO = 0.1\n\n# Use horizontally-flipped images during training?\n__C.TRAIN.USE_FLIPPED = True\n\n# Train bounding-box regressors\n__C.TRAIN.BBOX_REG = True\n\n# Overlap required between a ROI and ground-truth box in order for that ROI to\n# be used as a bounding-box regression training example\n__C.TRAIN.BBOX_THRESH = 0.5\n\n# Iterations between snapshots\n__C.TRAIN.SNAPSHOT_ITERS = 5000\n\n# solver.prototxt specifies the snapshot path prefix, this adds an optional\n# infix to yield the path: <prefix>[_<infix>]_iters_XYZ.caffemodel\n__C.TRAIN.SNAPSHOT_PREFIX = \'vgg16_faster_rcnn\'\n# __C.TRAIN.SNAPSHOT_INFIX = \'\'\n\n# Use a prefetch thread in roi_data_layer.layer\n# So far I haven\'t found this useful; likely more engineering work is required\n# __C.TRAIN.USE_PREFETCH = False\n\n# Normalize the targets (subtract empirical mean, divide by empirical stddev)\n__C.TRAIN.BBOX_NORMALIZE_TARGETS = True\n# Deprecated (inside weights)\n__C.TRAIN.BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n# Normalize the targets using ""precomputed"" (or made up) means and stdevs\n# (BBOX_NORMALIZE_TARGETS must also be True)\n__C.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED = True\n__C.TRAIN.BBOX_NORMALIZE_MEANS = (0.0, 0.0, 0.0, 0.0)\n__C.TRAIN.BBOX_NORMALIZE_STDS = (0.1, 0.1, 0.2, 0.2)\n\n# Train using these proposals\n__C.TRAIN.PROPOSAL_METHOD = \'selective_search\'\n\n# Make minibatches from images that have similar aspect ratios (i.e. both\n# tall and thin or both short and wide) in order to avoid wasting computation\n# on zero-padding.\n\n# Use RPN to detect objects\n__C.TRAIN.HAS_RPN = False\n# IOU >= thresh: positive example\n__C.TRAIN.RPN_POSITIVE_OVERLAP = 0.7\n# IOU < thresh: negative example\n__C.TRAIN.RPN_NEGATIVE_OVERLAP = 0.3\n# If an anchor statisfied by positive and negative conditions set to negative\n__C.TRAIN.RPN_CLOBBER_POSITIVES = False\n# Max number of foreground examples\n__C.TRAIN.RPN_FG_FRACTION = 0.5\n# Total number of examples\n__C.TRAIN.RPN_BATCHSIZE = 256\n# NMS threshold used on RPN proposals\n__C.TRAIN.RPN_NMS_THRESH = 0.7\n# Number of top scoring boxes to keep before apply NMS to RPN proposals\n__C.TRAIN.RPN_PRE_NMS_TOP_N = 12000\n# Number of top scoring boxes to keep after applying NMS to RPN proposals\n__C.TRAIN.RPN_POST_NMS_TOP_N = 2000\n# Proposal height and width both need to be greater than RPN_MIN_SIZE (at orig image scale)\n# __C.TRAIN.RPN_MIN_SIZE = 16\n# Deprecated (outside weights)\n__C.TRAIN.RPN_BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n# Give the positive RPN examples weight of p * 1 / {num positives}\n# and give negatives a weight of (1 - p)\n# Set to -1.0 to use uniform example weighting\n__C.TRAIN.RPN_POSITIVE_WEIGHT = -1.0\n\n#\n# Testing options\n#\n__C.TEST = edict()\n\n# Scales to use during testing (can list multiple scales)\n# Each scale is the pixel size of an image\'s shortest side\n__C.TEST.SCALES = (600,)\n\n# Max pixel size of the longest side of a scaled input image\n__C.TEST.MAX_SIZE = 1000\n\n# Overlap threshold used for non-maximum suppression (suppress boxes with\n# IoU >= this threshold)\n__C.TEST.NMS = 0.3\n\n# Experimental: treat the (K+1) units in the cls_score layer as linear\n# predictors (trained, eg, with one-vs-rest SVMs).\n__C.TEST.SVM = False\n\n# Test using bounding-box regressors\n__C.TEST.BBOX_REG = True\n\n# Propose boxes\n__C.TEST.HAS_RPN = False\n\n# Test using these proposals\n__C.TEST.PROPOSAL_METHOD = \'selective_search\'\n\n## NMS threshold used on RPN proposals\n__C.TEST.RPN_NMS_THRESH = 0.7\n## Number of top scoring boxes to keep before apply NMS to RPN proposals\n__C.TEST.RPN_PRE_NMS_TOP_N = 6000\n\n## Number of top scoring boxes to keep after applying NMS to RPN proposals\n__C.TEST.RPN_POST_NMS_TOP_N = 300\n\n# Proposal height and width both need to be greater than RPN_MIN_SIZE (at orig image scale)\n# __C.TEST.RPN_MIN_SIZE = 16\n\n# Testing mode, default to be \'nms\', \'top\' is slower but better\n# See report for details\n__C.TEST.MODE = \'nms\'\n\n# Only useful when TEST.MODE is \'top\', specifies the number of top proposals to select\n__C.TEST.RPN_TOP_N = 5000\n\n#\n# MISC\n#\n\n# The mapping from image coordinates to feature map coordinates might cause\n# some boxes that are distinct in image space to become identical in feature\n# coordinates. If DEDUP_BOXES > 0, then DEDUP_BOXES is used as the scale factor\n# for identifying duplicate boxes.\n# 1/16 is correct for {Alex,Caffe}Net, VGG_CNN_M_1024, and VGG16\n__C.DEDUP_BOXES = 1./16.\n\n# Pixel mean values (BGR order) as a (1, 1, 3) array\n# We use the same pixel mean for all networks even though it\'s not exactly what\n# they were trained with\n__C.PIXEL_MEANS = np.array([[[102.9801, 115.9465, 122.7717]]])\n\n# For reproducibility\n__C.RNG_SEED = 3\n\n# A small number that\'s used many times\n__C.EPS = 1e-14\n\n# Root directory of project\n__C.ROOT_DIR = osp.abspath(osp.join(osp.dirname(__file__), \'..\', \'..\'))\n\n# Data directory\n__C.DATA_DIR = osp.abspath(osp.join(__C.ROOT_DIR, \'data\'))\n\n# Name (or path to) the matlab executable\n__C.MATLAB = \'matlab\'\n\n# Place outputs under an experiments directory\n__C.EXP_DIR = \'default\'\n\n# Use GPU implementation of non-maximum suppression\n__C.USE_GPU_NMS = True\n\n# Default GPU device id\n__C.GPU_ID = 0\n\n# Default pooling mode, only \'crop\' is available\n__C.POOLING_MODE = \'crop\'\n\n\ndef get_output_dir(imdb, weights_filename):\n    """"""Return the directory where experimental artifacts are placed.\n    If the directory does not exist, it is created.\n\n    A canonical path is built using the name from an imdb and a network\n    (if not None).\n    """"""\n    outdir = osp.abspath(osp.join(__C.ROOT_DIR, \'output\', __C.EXP_DIR, imdb.name))\n    if weights_filename is None:\n        weights_filename = \'default\'\n    outdir = osp.join(outdir, weights_filename)\n    if not os.path.exists(outdir):\n        os.makedirs(outdir)\n    return outdir\n\ndef get_output_tb_dir(imdb, weights_filename):\n    """"""Return the directory where tensorflow summaries are placed.\n    If the directory does not exist, it is created.\n\n    A canonical path is built using the name from an imdb and a network\n    (if not None).\n    """"""\n    outdir = osp.abspath(osp.join(__C.ROOT_DIR, \'tensorboard\', __C.EXP_DIR, imdb.name))\n    if weights_filename is None:\n        weights_filename = \'default\'\n    outdir = osp.join(outdir, weights_filename)\n    if not os.path.exists(outdir):\n        os.makedirs(outdir)\n    return outdir\n\ndef _merge_a_into_b(a, b):\n    """"""Merge config dictionary a into config dictionary b, clobbering the\n    options in b whenever they are also specified in a.\n    """"""\n    if type(a) is not edict:\n        return\n\n    for k, v in a.iteritems():\n        # a must specify keys that are in b\n        if not b.has_key(k):\n            raise KeyError(\'{} is not a valid config key\'.format(k))\n\n        # the types must match, too\n        old_type = type(b[k])\n        if old_type is not type(v):\n            if isinstance(b[k], np.ndarray):\n                v = np.array(v, dtype=b[k].dtype)\n            else:\n                raise ValueError((\'Type mismatch ({} vs. {}) \'\n                                \'for config key: {}\').format(type(b[k]),\n                                                            type(v), k))\n\n        # recursively merge dicts\n        if type(v) is edict:\n            try:\n                _merge_a_into_b(a[k], b[k])\n            except:\n                print(\'Error under config key: {}\'.format(k))\n                raise\n        else:\n            b[k] = v\n\ndef cfg_from_file(filename):\n    """"""Load a config file and merge it into the default options.""""""\n    import yaml\n    with open(filename, \'r\') as f:\n        yaml_cfg = edict(yaml.load(f))\n\n    _merge_a_into_b(yaml_cfg, __C)\n\ndef cfg_from_list(cfg_list):\n    """"""Set config keys via list (e.g., from command line).""""""\n    from ast import literal_eval\n    assert len(cfg_list) % 2 == 0\n    for k, v in zip(cfg_list[0::2], cfg_list[1::2]):\n        key_list = k.split(\'.\')\n        d = __C\n        for subkey in key_list[:-1]:\n            assert d.has_key(subkey)\n            d = d[subkey]\n        subkey = key_list[-1]\n        assert d.has_key(subkey)\n        try:\n            value = literal_eval(v)\n        except:\n            # handle the case when v is a string literal\n            value = v\n        assert type(value) == type(d[subkey]), \\\n            \'type {} does not match original type {}\'.format(\n            type(value), type(d[subkey]))\n        d[subkey] = value\n\n'"
visual_search/lib/model/nms_wrapper.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nfrom model.config import cfg\nfrom nms.gpu_nms import gpu_nms\nfrom nms.cpu_nms import cpu_nms\n\ndef nms(dets, thresh, force_cpu=False):\n    """"""Dispatch to either CPU or GPU NMS implementations.""""""\n\n    if dets.shape[0] == 0:\n        return []\n    if cfg.USE_GPU_NMS and not force_cpu:\n        return gpu_nms(dets, thresh, device_id=cfg.GPU_ID)\n    else:\n        return cpu_nms(dets, thresh)\n'"
visual_search/lib/model/test.py,0,"b'# --------------------------------------------------------\n# Tensorflow Faster R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Xinlei Chen\n# --------------------------------------------------------\n\nimport cv2\nimport numpy as np\nimport cPickle\nimport os\nimport math\n\nfrom utils.timer import Timer\nfrom utils.cython_nms import nms, nms_new\nfrom utils.boxes_grid import get_boxes_grid\nfrom utils.blob import im_list_to_blob\n\nfrom model.config import cfg, get_output_dir\nfrom model.bbox_transform import clip_boxes, bbox_transform_inv\n\ndef _get_image_blob(im):\n  """"""Converts an image into a network input.\n  Arguments:\n    im (ndarray): a color image in BGR order\n  Returns:\n    blob (ndarray): a data blob holding an image pyramid\n    im_scale_factors (list): list of image scales (relative to im) used\n      in the image pyramid\n  """"""\n  im_orig = im.astype(np.float32, copy=True)\n  im_orig -= cfg.PIXEL_MEANS\n\n  im_shape = im_orig.shape\n  im_size_min = np.min(im_shape[0:2])\n  im_size_max = np.max(im_shape[0:2])\n\n  processed_ims = []\n  im_scale_factors = []\n\n  for target_size in cfg.TEST.SCALES:\n    im_scale = float(target_size) / float(im_size_min)\n    # Prevent the biggest axis from being more than MAX_SIZE\n    if np.round(im_scale * im_size_max) > cfg.TEST.MAX_SIZE:\n      im_scale = float(cfg.TEST.MAX_SIZE) / float(im_size_max)\n    im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale,\n            interpolation=cv2.INTER_LINEAR)\n    im_scale_factors.append(im_scale)\n    processed_ims.append(im)\n\n  # Create a blob to hold the input images\n  blob = im_list_to_blob(processed_ims)\n\n  return blob, np.array(im_scale_factors)\n\ndef _get_blobs(im):\n  """"""Convert an image and RoIs within that image into network inputs.""""""\n  blobs = {}\n  blobs[\'data\'], im_scale_factors = _get_image_blob(im)\n\n  return blobs, im_scale_factors\n\ndef _clip_boxes(boxes, im_shape):\n  """"""Clip boxes to image boundaries.""""""\n  # x1 >= 0\n  boxes[:, 0::4] = np.maximum(boxes[:, 0::4], 0)\n  # y1 >= 0\n  boxes[:, 1::4] = np.maximum(boxes[:, 1::4], 0)\n  # x2 < im_shape[1]\n  boxes[:, 2::4] = np.minimum(boxes[:, 2::4], im_shape[1] - 1)\n  # y2 < im_shape[0]\n  boxes[:, 3::4] = np.minimum(boxes[:, 3::4], im_shape[0] - 1)\n  return boxes\n\ndef _rescale_boxes(boxes, inds, scales):\n  """"""Rescale boxes according to image rescaling.""""""\n  for i in xrange(boxes.shape[0]):\n    boxes[i,:] = boxes[i,:] / scales[int(inds[i])]\n\n  return boxes\n\ndef im_detect(sess, net, im):\n  blobs, im_scales = _get_blobs(im)\n  assert len(im_scales) == 1, ""Only single-image batch implemented""\n\n  im_blob = blobs[\'data\']\n  # seems to have height, width, and image scales\n  # still not sure about the scale, maybe full image it is 1.\n  blobs[\'im_info\'] = np.array([[im_blob.shape[1], im_blob.shape[2], im_scales[0]]], dtype=np.float32)\n\n  _, scores, bbox_pred, rois, feats = net.test_image(sess, blobs[\'data\'], blobs[\'im_info\'])\n\n  boxes = rois[:, 1:5] / im_scales[0]\n\n  if cfg.TEST.BBOX_REG:\n    # Apply bounding-box regression deltas\n    box_deltas = bbox_pred\n    pred_boxes = bbox_transform_inv(boxes, box_deltas)\n    pred_boxes = _clip_boxes(pred_boxes, im.shape)\n  else:\n    # Simply repeat the boxes, once for each class\n    pred_boxes = np.tile(boxes, (1, scores.shape[1]))\n\n  return scores, pred_boxes, feats\n\ndef apply_nms(all_boxes, thresh):\n  """"""Apply non-maximum suppression to all predicted boxes output by the\n  test_net method.\n  """"""\n  num_classes = len(all_boxes)\n  num_images = len(all_boxes[0])\n  nms_boxes = [[[] for _ in xrange(num_images)] for _ in xrange(num_classes)]\n  for cls_ind in xrange(num_classes):\n    for im_ind in xrange(num_images):\n      dets = all_boxes[cls_ind][im_ind]\n      if dets == []:\n        continue\n\n      x1 = dets[:, 0]\n      y1 = dets[:, 1]\n      x2 = dets[:, 2]\n      y2 = dets[:, 3]\n      scores = dets[:, 4]\n      inds = np.where((x2 > x1) & (y2 > y1) & (scores > cfg.TEST.DET_THRESHOLD))[0]\n      dets = dets[inds,:]\n      if dets == []:\n        continue\n\n      keep = nms(dets, thresh)\n      if len(keep) == 0:\n        continue\n      nms_boxes[cls_ind][im_ind] = dets[keep, :].copy()\n  return nms_boxes\n\ndef test_net(sess, net, imdb, weights_filename, max_per_image=100, thresh=0.05):\n  np.random.seed(cfg.RNG_SEED)\n  """"""Test a Fast R-CNN network on an image database.""""""\n  num_images = len(imdb.image_index)\n  # all detections are collected into:\n  #  all_boxes[cls][image] = N x 5 array of detections in\n  #  (x1, y1, x2, y2, score)\n  all_boxes = [[[] for _ in xrange(num_images)]\n         for _ in xrange(imdb.num_classes)]\n\n  output_dir = get_output_dir(imdb, weights_filename)\n  # timers\n  _t = {\'im_detect\' : Timer(), \'misc\' : Timer()}\n\n  for i in xrange(num_images):\n    p = \'data/coco/images/val2014/COCO_val2014_000000532481.jpg\'\n    # im = cv2.imread(imdb.image_path_at(i))\n    im = cv2.imread(p)\n\n    _t[\'im_detect\'].tic()\n    scores, boxes, feats = im_detect(sess, net, im)\n    _t[\'im_detect\'].toc()\n\n    _t[\'misc\'].tic()\n\n    # skip j = 0, because it\'s the background class\n    for j in xrange(1, imdb.num_classes):\n      inds = np.where(scores[:, j] > thresh)[0]\n      cls_scores = scores[inds, j]\n      cls_boxes = boxes[inds, j*4:(j+1)*4]\n      cls_dets = np.hstack((cls_boxes, cls_scores[:, np.newaxis])).astype(np.float32, copy=False)\n      keep = nms(cls_dets, cfg.TEST.NMS)\n      cls_dets = cls_dets[keep, :]\n      all_boxes[j][i] = cls_dets\n\n    # Limit to max_per_image detections *over all classes*\n    if max_per_image > 0:\n      image_scores = np.hstack([all_boxes[j][i][:, -1]\n                    for j in xrange(1, imdb.num_classes)])\n      if len(image_scores) > max_per_image:\n        image_thresh = np.sort(image_scores)[-max_per_image]\n        for j in xrange(1, imdb.num_classes):\n          keep = np.where(all_boxes[j][i][:, -1] >= image_thresh)[0]\n          all_boxes[j][i] = all_boxes[j][i][keep, :]\n    _t[\'misc\'].toc()\n\n    print \'im_detect: {:d}/{:d} {:.3f}s {:.3f}s\' \\\n        .format(i + 1, num_images, _t[\'im_detect\'].average_time,\n            _t[\'misc\'].average_time)\n\n\n  det_file = os.path.join(output_dir, \'detections.pkl\')\n  with open(det_file, \'wb\') as f:\n    cPickle.dump(all_boxes, f, cPickle.HIGHEST_PROTOCOL)\n\n  print \'Evaluating detections\'\n  imdb.evaluate_detections(all_boxes, output_dir)\n\n\nclasses= (\'__background__\', u\'person\', u\'bicycle\', u\'car\', u\'motorcycle\', u\'airplane\', u\'bus\', u\'train\', u\'truck\', u\'boat\', u\'traffic light\', u\'fire hydrant\', u\'stop sign\', u\'parking meter\', u\'bench\', u\'bird\', u\'cat\', u\'dog\', u\'horse\', u\'sheep\', u\'cow\', u\'elephant\', u\'bear\', u\'zebra\', u\'giraffe\', u\'backpack\', u\'umbrella\', u\'handbag\', u\'tie\', u\'suitcase\', u\'frisbee\', u\'skis\', u\'snowboard\', u\'sports ball\', u\'kite\', u\'baseball bat\', u\'baseball glove\', u\'skateboard\', u\'surfboard\', u\'tennis racket\', u\'bottle\', u\'wine glass\', u\'cup\', u\'fork\', u\'knife\', u\'spoon\', u\'bowl\', u\'banana\', u\'apple\', u\'sandwich\', u\'orange\', u\'broccoli\', u\'carrot\', u\'hot dog\', u\'pizza\', u\'donut\', u\'cake\', u\'chair\', u\'couch\', u\'potted plant\', u\'bed\', u\'dining table\', u\'toilet\', u\'tv\', u\'laptop\', u\'mouse\', u\'remote\', u\'keyboard\', u\'cell phone\', u\'microwave\', u\'oven\', u\'toaster\', u\'sink\', u\'refrigerator\', u\'book\', u\'clock\', u\'vase\', u\'scissors\', u\'teddy bear\', u\'hair drier\', u\'toothbrush\')\n\ndef get_boxes(sess, net, img, all_feats, all_boxes):\n  boxes = {}\n  for cls_ind, cls in enumerate(classes):\n    if cls == \'__background__\':\n      continue\n    dets = all_boxes[cls_ind]\n    feats = all_feats[cls_ind]\n    if dets == []:\n      continue\n    for k in xrange(dets.shape[0]):\n      det = dets[k]\n      box = {}\n      box[\'lt\'] = [int(det[0]), int(det[1])]\n      box[\'rb\'] = [int(det[2]), int(det[3])]\n      box[\'f\'] = feats[k]\n      box[\'cl\'] = cls\n      box[\'score\'] = det[4]\n      box_width = box[\'rb\'][0] - box[\'lt\'][0]\n      box_height = box[\'rb\'][1] - box[\'lt\'][1]\n      min_size = min(box_width, box_height)\n      if min_size > 10:\n        if not cls in boxes:\n           boxes[cls] = []\n        class_boxes = boxes[cls]\n        # boxes.append(box)\n        class_boxes.append(box)\n  return boxes\n\n\n\ndef extract_regions_and_feats(sess, net, im, max_per_image=10, max_per_class=3, thresh=0.1):\n  """"""Extract regions and features with respect to each region""""""\n  # all detections are collected into:\n  #  all_boxes[cls][image] = N x 5 array of detections in\n  #  (x1, y1, x2, y2, score)\n # timecrs\n  _t = {\'im_detect\' : Timer(), \'misc\' : Timer()}\n  if type(im) == str:\n      im = cv2.imread(im)\n\n  _t[\'im_detect\'].tic()\n  scores, boxes, feats = im_detect(sess, net, im)\n  _t[\'im_detect\'].toc()\n\n  _t[\'misc\'].tic()\n\n  all_boxes = [[] for _ in xrange(81)]\n  all_feats = [[] for _ in xrange(81)]\n  # skip j = 0, because it\'s the background class\n  for j in xrange(1, 81):\n    image_thresh = np.sort(scores[:,j])[-max_per_image]\n    th = thresh if thresh > image_thresh else image_thresh\n    inds = np.where(scores[:, j] > th)[0]\n    cls_scores = scores[inds, j]\n    cls_boxes = boxes[inds, j*4:(j+1)*4]\n    cls_dets = np.hstack((cls_boxes, cls_scores[:, np.newaxis])).astype(np.float32, copy=False)\n    keep = nms(cls_dets, cfg.TEST.NMS)\n    cls_dets = cls_dets[keep, :]\n    feats_part = feats[keep, :]\n    all_boxes[j] = cls_dets\n    all_feats[j] = feats_part\n\n  # Limit to max_per_image detections *over all classes*\n  if max_per_image > 0:\n    image_scores = np.hstack([all_boxes[j][:, -1]\n        for j in xrange(1, 81)])\n    if len(image_scores) > max_per_image:\n        image_thresh = np.sort(image_scores)[-max_per_image]\n        for j in xrange(1, 81):\n            keep = np.where(all_boxes[j][:, -1] >= image_thresh)[0]\n            all_boxes[j] = all_boxes[j][keep, :]\n            all_feats[j] = all_feats[j][keep, :]\n  _t[\'misc\'].toc()\n\n  # print \'im_detect in {:.3f}s {:.3f}s\' \\\n          # .format(_t[\'im_detect\'].average_time,\n                  # _t[\'misc\'].average_time)\n\n  boxes = get_boxes(sess, net, im, all_feats, all_boxes)\n  return boxes\n\ndef extract_imfea(sess, net, img):\n  # im = cv2.imread(path)\n  # im_orig = im.astype(np.float32, copy=True)\n  # resized image first\n  resized_im = cv2.resize(img, (224, 224))\n  resized_im -= cfg.PIXEL_MEANS\n  fea = net.extract_fc7(sess, [resized_im])\n  return np.squeeze(fea)\n\n'"
visual_search/lib/model/train_val.py,16,"b'# --------------------------------------------------------\n# Tensorflow Faster R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Xinlei Chen\n# --------------------------------------------------------\n\nfrom model.config import cfg\nimport roi_data_layer.roidb as rdl_roidb\nfrom roi_data_layer.layer import RoIDataLayer\nfrom utils.timer import Timer\nimport cPickle\nimport numpy as np\nimport os\nimport sys\nimport glob\nimport time\n\nimport tensorflow as tf\n\nclass SolverWrapper(object):\n  """"""\n    A wrapper class for the training process\n  """"""\n  def __init__(self, sess, network, imdb, roidb, valroidb, output_dir, tbdir, pretrained_model=None):\n    self.net = network\n    self.imdb = imdb\n    self.roidb = roidb\n    self.valroidb = valroidb\n    self.output_dir = output_dir\n    self.tbdir = tbdir\n    # Simply put \'_val\' at the end to save the summaries from the validation set\n    self.tbvaldir = tbdir + \'_val\'\n    if not os.path.exists(self.tbvaldir):\n      os.makedirs(self.tbvaldir)\n    self.pretrained_model = pretrained_model\n\n  def snapshot(self, sess, iter):\n    net = self.net\n\n    if not os.path.exists(self.output_dir):\n      os.makedirs(self.output_dir)\n\n    # Store the model snapshot\n    filename = cfg.TRAIN.SNAPSHOT_PREFIX + \'_iter_{:d}\'.format(iter) + \'.ckpt\'\n    filename = os.path.join(self.output_dir, filename)\n    self.saver.save(sess, filename)\n    print \'Wrote snapshot to: {:s}\'.format(filename)\n\n    # Also store some meta information, random state, etc.\n    nfilename = cfg.TRAIN.SNAPSHOT_PREFIX + \'_iter_{:d}\'.format(iter) + \'.pkl\'\n    nfilename = os.path.join(self.output_dir, nfilename)\n    # current state of numpy random\n    st0 = np.random.get_state()\n    # current position in the database\n    cur = self.data_layer._cur\n    # current shuffled indeces of the database\n    perm = self.data_layer._perm\n    # current position in the validation database\n    cur_val = self.data_layer_val._cur\n    # current shuffled indeces of the validation database\n    perm_val = self.data_layer_val._perm\n\n    # Dump the meta info\n    with open(nfilename, \'wb\') as fid:\n      cPickle.dump(st0, fid, cPickle.HIGHEST_PROTOCOL)\n      cPickle.dump(cur, fid, cPickle.HIGHEST_PROTOCOL)\n      cPickle.dump(perm, fid, cPickle.HIGHEST_PROTOCOL)\n      cPickle.dump(cur_val, fid, cPickle.HIGHEST_PROTOCOL)\n      cPickle.dump(perm_val, fid, cPickle.HIGHEST_PROTOCOL)\n      cPickle.dump(iter, fid, cPickle.HIGHEST_PROTOCOL)\n\n    return filename, nfilename\n\n  def train_model(self, sess, max_iters):\n    # Build data layers for both training and validation set\n    self.data_layer = RoIDataLayer(self.roidb, self.imdb.num_classes)\n    self.data_layer_val = RoIDataLayer(self.valroidb, self.imdb.num_classes, random=True)\n\n    # Determine different scales for anchors, see paper\n    if self.imdb.name.startswith(\'voc\'):\n      anchors = [8, 16, 32]\n    else:\n      anchors = [4, 8, 16, 32]\n\n    with sess.graph.as_default():\n      # Set the random seed for tensorflow\n      tf.set_random_seed(cfg.RNG_SEED)\n      # Build the main computation graph\n      layers = self.net.create_architecture(sess, ""TRAIN"", self.imdb.num_classes,\n                                            caffe_weight_path=self.pretrained_model, \n                                            tag=\'default\', anchor_scales=anchors)\n      # Define the loss\n      loss = layers[\'total_loss\']\n\n      # Set learning rate and momentum\n      lr = tf.Variable(cfg.TRAIN.LEARNING_RATE, trainable=False)\n      momentum = cfg.TRAIN.MOMENTUM\n      self.optimizer = tf.train.MomentumOptimizer(lr, momentum)\n      # Compute the gradients wrt the loss\n      gvs = self.optimizer.compute_gradients(loss)\n      # Double the gradient of the bias if set\n      if cfg.TRAIN.DOUBLE_BIAS:\n        final_gvs = []\n        with tf.variable_scope(\'Gradient_Mult\') as scope:\n          for grad, var in gvs:\n            scale = 1.\n            if cfg.TRAIN.DOUBLE_BIAS and \'/bias:\' in var.name:\n              scale *= 2.\n            if not np.allclose(scale, 1.0):\n              grad = tf.mul(grad, scale)\n            final_gvs.append((grad, var))\n        train_op = self.optimizer.apply_gradients(final_gvs)\n      else:\n        train_op = self.optimizer.apply_gradients(gvs)\n\n      # We will handle the snapshots ourselves\n      self.saver = tf.train.Saver(max_to_keep=100000)\n      # Write the train and validation information to tensorboard\n      self.writer = tf.summary.FileWriter(self.tbdir, sess.graph)\n      self.valwriter = tf.summary.FileWriter(self.tbvaldir)\n\n    # Find previous snapshots if there is any to restore from\n    sfiles = os.path.join(self.output_dir, cfg.TRAIN.SNAPSHOT_PREFIX + \'_iter_*.ckpt.meta\')\n    sfiles = glob.glob(sfiles)\n    sfiles.sort(key=os.path.getmtime)\n    # Get the snapshot name in TensorFlow\n    sfiles = [ss.replace(\'.meta\', \'\') for ss in sfiles]\n\n    nfiles = os.path.join(self.output_dir, cfg.TRAIN.SNAPSHOT_PREFIX + \'_iter_*.pkl\')\n    nfiles = glob.glob(nfiles)\n    nfiles.sort(key=os.path.getmtime)\n\n    lsf = len(sfiles)\n    assert len(nfiles) == lsf\n\n    np_paths = nfiles\n    ss_paths = sfiles\n\n    if lsf == 0:\n      # Fresh train directly from VGG weights\n      print (\'Loading initial model weights from {:s}\').format(self.pretrained_model)\n      variables = tf.global_variables()\n      # Only initialize the variables that were not initialized when the graph was built\n      for vbs in self.net._initialized:\n        variables.remove(vbs)\n      sess.run(tf.variables_initializer(variables, name=\'init\'))\n      print \'Loaded.\'\n      sess.run(tf.assign(lr, cfg.TRAIN.LEARNING_RATE))\n      last_snapshot_iter = 0\n    else:\n      # Get the most recent snapshot and restore\n      ss_paths = [ss_paths[-1]]\n      np_paths = [np_paths[-1]]\n\n      print (\'Restorining model snapshots from {:s}\').format(sfiles[-1])\n      self.saver.restore(sess, str(sfiles[-1]))\n      print \'Restored.\'\n      # Needs to restore the other hyperparameters/states for training, (TODO xinlei) I have\n      # tried my best to find the random states so that it can be recovered exactly\n      # However the Tensorflow state is currently not available\n      with open(str(nfiles[-1]), \'rb\') as fid:\n        st0 = cPickle.load(fid)\n        cur = cPickle.load(fid)\n        perm = cPickle.load(fid)\n        cur_val = cPickle.load(fid)\n        perm_val = cPickle.load(fid)\n        last_snapshot_iter = cPickle.load(fid)\n\n        np.random.set_state(st0)\n        self.data_layer._cur = cur\n        self.data_layer._perm = perm\n        self.data_layer_val._cur = cur_val\n        self.data_layer_val._perm = perm_val\n        \n        # Set the learning rate, only reduce once\n        if last_snapshot_iter >= cfg.TRAIN.STEPSIZE:\n          sess.run(tf.assign(lr, cfg.TRAIN.LEARNING_RATE * cfg.TRAIN.GAMMA))\n        else:\n          sess.run(tf.assign(lr, cfg.TRAIN.LEARNING_RATE))\n\n    timer = Timer()\n    iter = last_snapshot_iter+1\n    last_summary_time = time.time()\n    while iter < max_iters+1:\n      # Learning rate\n      if iter == cfg.TRAIN.STEPSIZE:\n        sess.run(tf.assign(lr, cfg.TRAIN.LEARNING_RATE * cfg.TRAIN.GAMMA))\n\n      timer.tic()\n      # Get training data, one batch at a time\n      blobs = self.data_layer.forward()\n\n      now = time.time()\n      if now - last_summary_time > cfg.TRAIN.SUMMARY_INTERVAL:\n        # Compute the graph with summary\n        rpn_loss_cls, rpn_loss_box, loss_cls, loss_box, total_loss, summary = \\\n                        self.net.train_step_with_summary(sess, blobs, train_op)\n        self.writer.add_summary(summary, float(iter))\n        # Also check the summary on the validation set\n        blobs_val = self.data_layer_val.forward()\n        summary_val = self.net.get_summary(sess, blobs_val)\n        self.valwriter.add_summary(summary_val, float(iter))\n        last_summary_time = now\n      else:\n        # Compute the graph without summary\n        rpn_loss_cls, rpn_loss_box, loss_cls, loss_box, total_loss = \\\n                        self.net.train_step(sess, blobs, train_op)\n      timer.toc()\n\n      # Display training information\n      if iter % (cfg.TRAIN.DISPLAY) == 0:\n        print \'iter: %d / %d, total loss: %.6f\\n >>> rpn_loss_cls: %.6f\\n >>> rpn_loss_box: %.6f\\n >>> loss_cls: %.6f\\n >>> loss_box: %.6f\\n >>> lr: %f\'%\\\n              (iter, max_iters, total_loss, rpn_loss_cls, rpn_loss_box, loss_cls, loss_box, lr.eval())\n        print \'speed: {:.3f}s / iter\'.format(timer.average_time)\n\n      if iter % cfg.TRAIN.SNAPSHOT_ITERS == 0:\n        last_snapshot_iter = iter\n        snapshot_path, np_path = self.snapshot(sess, iter)\n        np_paths.append(np_path)\n        ss_paths.append(snapshot_path)\n\n        # Remove the old snapshots if there are too many\n        if len(np_paths) > cfg.TRAIN.SNAPSHOT_KEPT:\n          to_remove = len(np_paths) - cfg.TRAIN.SNAPSHOT_KEPT\n          for c in xrange(to_remove):\n            nfile = np_paths[0]\n            os.remove(str(nfile))\n            np_paths.remove(nfile)\n\n        if len(ss_paths) > cfg.TRAIN.SNAPSHOT_KEPT:\n          to_remove = len(ss_paths) - cfg.TRAIN.SNAPSHOT_KEPT\n          for c in xrange(to_remove):\n            sfile = ss_paths[0]\n            # To make the code compatible to earlier versions of Tensorflow,\n            # where the naming tradition for checkpoints are different\n            if os.path.exists(str(sfile)):\n              os.remove(str(sfile))\n            else:\n              os.remove(str(sfile + \'.data-00000-of-00001\'))\n              os.remove(str(sfile + \'.index\'))\n            sfile_meta = sfile + \'.meta\'\n            os.remove(str(sfile_meta))\n            ss_paths.remove(sfile)\n\n      iter += 1\n\n    if last_snapshot_iter != iter - 1:\n      self.snapshot(sess, iter - 1)\n\n    self.writer.close()\n    self.valwriter.close()\n\ndef get_training_roidb(imdb):\n  """"""Returns a roidb (Region of Interest database) for use in training.""""""\n  if cfg.TRAIN.USE_FLIPPED:\n    print \'Appending horizontally-flipped training examples...\'\n    imdb.append_flipped_images()\n    print \'done\'\n\n  print \'Preparing training data...\'\n  rdl_roidb.prepare_roidb(imdb)\n  print \'done\'\n\n  return imdb.roidb\n\ndef filter_roidb(roidb):\n  """"""Remove roidb entries that have no usable RoIs.""""""\n\n  def is_valid(entry):\n    # Valid images have:\n    #   (1) At least one foreground RoI OR\n    #   (2) At least one background RoI\n    overlaps = entry[\'max_overlaps\']\n    # find boxes with sufficient overlap\n    fg_inds = np.where(overlaps >= cfg.TRAIN.FG_THRESH)[0]\n    # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n    bg_inds = np.where((overlaps < cfg.TRAIN.BG_THRESH_HI) &\n                       (overlaps >= cfg.TRAIN.BG_THRESH_LO))[0]\n    # image is only valid if such boxes exist\n    valid = len(fg_inds) > 0 or len(bg_inds) > 0\n    return valid\n\n  num = len(roidb)\n  filtered_roidb = [entry for entry in roidb if is_valid(entry)]\n  num_after = len(filtered_roidb)\n  print \'Filtered {} roidb entries: {} -> {}\'.format(num - num_after,\n                                                     num, num_after)\n  return filtered_roidb\n\ndef train_net(network, imdb, roidb, valroidb, output_dir, tb_dir,\n              pretrained_model=None, \n              max_iters=40000):\n  """"""Train a Fast R-CNN network.""""""\n  roidb = filter_roidb(roidb)\n  valroidb = filter_roidb(valroidb)\n\n  tfconfig = tf.ConfigProto(allow_soft_placement=True)\n  tfconfig.gpu_options.allow_growth=True\n\n  with tf.Session(config=tfconfig) as sess:\n    sw = SolverWrapper(sess, network, imdb, roidb, valroidb, output_dir, tb_dir,\n                      pretrained_model=pretrained_model)\n    print \'Solving...\'\n    sw.train_model(sess, max_iters)\n    print \'done solving\'\n'"
visual_search/lib/nets/__init__.py,0,b''
visual_search/lib/nets/vgg16.py,106,"b'# --------------------------------------------------------\n# Tensorflow Faster R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Xinlei Chen\n# --------------------------------------------------------\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.contrib.slim import losses\nfrom tensorflow.contrib.slim import arg_scope\n\nimport numpy as np\nimport cPickle as pickle\n\nfrom layer_utils.snippets import generate_anchors_pre\nfrom layer_utils.proposal_layer import proposal_layer\nfrom layer_utils.proposal_top_layer import proposal_top_layer\nfrom layer_utils.anchor_target_layer import anchor_target_layer\nfrom layer_utils.proposal_target_layer import proposal_target_layer\n\nfrom model.config import cfg\n\nclass vgg16(object):\n  def __init__(self, batch_size=1):\n    self._feat_stride = [16,]\n    self._feat_compress = [1./16.,]\n    self._batch_size = batch_size\n    self._predictions = {}\n    self._losses={}\n    self._anchor_targets={}\n    self._proposal_targets={}\n    self._layers = {}\n    self._act_summaries = []\n    self._score_summaries = {}\n    self._train_summaries = []\n    self._event_summaries = {}\n    self._initialized = []\n\n  def _add_act_summary(self, tensor):\n    tf.summary.histogram(\'ACT/\' + tensor.op.name + \'/activations\', tensor)\n    tf.summary.scalar(\'ACT/\' + tensor.op.name + \'/zero_fraction\',\n                    tf.nn.zero_fraction(tensor))\n\n  def _add_score_summary(self, key, tensor):\n    tf.summary.histogram(\'SCORE/\' + tensor.op.name + \'/\' + key + \'/scores\', tensor)\n\n  def _add_train_summary(self, var):\n    tf.summary.histogram(\'TRAIN/\' + var.op.name, var)\n\n  def _caffe_weights(self, layer_name):\n    layer=self._caffe_layers[layer_name]\n    return layer[\'weights\']\n\n  def _caffe_bias(self, layer_name):\n    layer=self._caffe_layers[layer_name]\n    return layer[\'bias\']\n\n  def _caffe2tf_filter(self, name):\n    f=self._caffe_weights(name)\n    return f.transpose((2, 3, 1, 0))\n\n  # Session is used to assign initial values, so that the big constant is not stored in the graph\n  def _get_conv_filter(self, sess, name, trainable):\n    w=self._caffe2tf_filter(name)\n    phw=tf.placeholder(tf.float32, shape=w.shape)\n    conv=tf.get_variable(""weight"", initializer=phw, dtype=tf.float32, trainable=trainable)\n    sess.run(conv.initializer, feed_dict={phw: w})\n    self._initialized.append(conv)\n\n    return conv\n\n  def _get_bias(self, sess, name, trainable):\n    b=self._caffe_bias(name)\n    if name == ""bbox_pred"":\n      stds = np.tile(np.array(cfg.TRAIN.BBOX_NORMALIZE_STDS), (self._num_classes))\n      means = np.tile(np.array(cfg.TRAIN.BBOX_NORMALIZE_MEANS), (self._num_classes))\n      b -= means\n      b /= stds\n    phb=tf.placeholder(tf.float32, shape=b.shape)\n    if cfg.TRAIN.BIAS_DECAY:\n      bias = tf.get_variable(""bias"", initializer=phb, dtype=tf.float32, trainable=trainable)\n    else:\n      bias = tf.get_variable(""bias"", initializer=phb, regularizer=tf.no_regularizer, dtype=tf.float32, trainable=trainable)\n    sess.run(bias.initializer, feed_dict={phb: b})\n    self._initialized.append(bias)\n\n    return bias\n\n  def _get_fc_weight(self, sess, name, trainable):\n    cw = self._caffe_weights(name)\n    if name == ""fc6"":\n      assert cw.shape == (4096, 25088)\n      cw = cw.reshape((4096, 512, 7, 7)) \n      cw = cw.transpose((2, 3, 1, 0))\n      cw = cw.reshape(25088, 4096)\n    elif name == ""bbox_pred"":\n      cw = cw.transpose((1, 0))\n      stds = np.tile(np.array(cfg.TRAIN.BBOX_NORMALIZE_STDS), (self._num_classes))\n      cw /= stds\n    else:\n      cw = cw.transpose((1, 0))\n    phcw=tf.placeholder(tf.float32, shape=cw.shape)\n    weight = tf.get_variable(""weight"", initializer=phcw, dtype=tf.float32, trainable=trainable)\n    sess.run(weight.initializer, feed_dict={phcw: cw})\n    self._initialized.append(weight)\n    \n    return weight\n\n  def _conv_layer(self, sess, bottom, name, trainable=True, padding=\'SAME\', relu=True):\n    with tf.variable_scope(name) as scope:\n      filt=self._get_conv_filter(sess, name, trainable=trainable)\n      conv_biases=self._get_bias(sess, name, trainable=trainable)\n\n      conv=tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding=padding)\n      bias=tf.nn.bias_add(conv, conv_biases)\n\n      if relu:\n        bias=tf.nn.relu(bias)\n      return bias\n\n  def _fc_layer(self, sess, bottom, weight, bias, name, relu=True):\n    shape = bottom.get_shape().as_list()\n    dim = 1\n    for d in shape[1:]:\n      dim *= d\n    x = tf.reshape(bottom, [-1, dim])\n\n\n    fc=tf.nn.bias_add(tf.matmul(x, weight), bias)\n\n    if relu:\n      fc=tf.nn.relu(fc)\n    return fc\n\n  def _conv_layer_shape(self, bottom, size, channels, name, initializer=None, trainable=True, padding=\'SAME\', relu=True):\n    bottom_shape = bottom.get_shape().as_list()\n    size.extend([bottom_shape[3],channels])\n    with tf.variable_scope(name) as scope:\n      filt=tf.get_variable(\'weight\', size, initializer=initializer, trainable=trainable)\n      if cfg.TRAIN.BIAS_DECAY:\n        conv_biases=tf.get_variable(\'bias\', [channels], initializer=tf.constant_initializer(0.0), trainable=trainable)\n      else:\n        conv_biases=tf.get_variable(\'bias\', [channels], initializer=tf.constant_initializer(0.0), regularizer=tf.no_regularizer, trainable=trainable)\n\n      conv=tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding=padding)\n      bias=tf.nn.bias_add(conv, conv_biases)\n\n      if relu:\n        bias=tf.nn.relu(bias)\n      return bias\n\n  def _fc_layer_shape(self, bottom, channels, name, initializer=None, trainable=True, relu=True):\n    with tf.variable_scope(name) as scope:\n      shape = bottom.get_shape().as_list()\n      dim = 1\n      for d in shape[1:]:\n        dim *= d\n      x = tf.reshape(bottom, [-1, dim])\n\n      weight = tf.get_variable(\'weight\', [dim, channels], initializer=initializer, trainable=trainable)\n      if cfg.TRAIN.BIAS_DECAY:\n        bias = tf.get_variable(\'bias\', [channels], initializer=tf.constant_initializer(0.0), trainable=trainable)\n      else:\n        bias = tf.get_variable(\'bias\', [channels], initializer=tf.constant_initializer(0.0), regularizer=tf.no_regularizer, trainable=trainable)\n\n      fc=tf.nn.bias_add(tf.matmul(x, weight), bias)\n\n      if relu:\n        fc=tf.nn.relu(fc)\n      return fc\n\n  def _reshape_layer(self, bottom, num_dim, name):\n    input_shape = tf.shape(bottom)\n    with tf.variable_scope(name) as scope:\n      # change the channel to the caffe format\n      to_caffe = tf.transpose(bottom, [0,3,1,2])\n      # then force it to have channel 2\n      reshaped = tf.reshape(to_caffe, tf.concat(0, [[self._batch_size], [num_dim, -1], [input_shape[2]]]))\n      # then swap the channel back\n      to_tf = tf.transpose(reshaped, [0,2,3,1])\n      return to_tf\n\n  def _softmax_layer(self, bottom, name):\n    if name == \'rpn_cls_prob_reshape\':\n      input_shape = tf.shape(bottom)\n      bottom_reshaped = tf.reshape(bottom, [-1, input_shape[-1]])\n      reshaped_score = tf.nn.softmax(bottom_reshaped, name=name)\n      return tf.reshape(reshaped_score, input_shape)\n    return tf.nn.softmax(bottom, name=name)\n\n  def _proposal_top_layer(self, rpn_cls_prob, rpn_bbox_pred, name):\n    with tf.variable_scope(name) as scope:\n      rois, rpn_scores = tf.py_func(proposal_top_layer,\n                                [rpn_cls_prob, rpn_bbox_pred, self._im_info, \n                                self._feat_stride, self._anchors, self._anchor_scales], [tf.float32, tf.float32])\n      rois.set_shape([cfg.TEST.RPN_TOP_N, 5])\n      rpn_scores.set_shape([cfg.TEST.RPN_TOP_N, 1])\n\n    return rois, rpn_scores\n\n  def _proposal_layer(self, rpn_cls_prob, rpn_bbox_pred, name):\n    with tf.variable_scope(name) as scope:\n      rois, rpn_scores = tf.py_func(proposal_layer,\n                                [rpn_cls_prob, rpn_bbox_pred, self._im_info, self._mode, \n                                self._feat_stride, self._anchors, self._anchor_scales], [tf.float32, tf.float32])\n      rois.set_shape([None, 5])\n      rpn_scores.set_shape([None, 1])\n\n    return rois, rpn_scores\n\n  # Only use it if you have roi_pooling op written in tf.image\n  def _roi_pool_layer(self, bootom, rois, name):\n    with tf.variable_scope(name) as scope:\n      return tf.image.roi_pooling(bootom, rois,\n                                    pooled_height=7,\n                                    pooled_width=7,\n                                    spatial_scale=1./16)[0]\n\n  def _crop_pool_layer(self, bottom, rois, name):\n    with tf.variable_scope(name) as scope:\n      batch_ids = tf.squeeze(tf.slice(rois, [0, 0], [-1, 1], name=""batch_id""), [1])\n      height = tf.ceil(self._im_info[0, 0] / 16. - 1.) * 16.\n      width = tf.ceil(self._im_info[0, 1] / 16. - 1.) * 16.\n      x1 = tf.slice(rois, [0, 1], [-1, 1], name=""x1"") / width\n      y1 = tf.slice(rois, [0, 2], [-1, 1], name=""y1"") / height\n      x2 = tf.slice(rois, [0, 3], [-1, 1], name=""x2"") / width\n      y2 = tf.slice(rois, [0, 4], [-1, 1], name=""y2"") / height\n      bboxes = tf.concat(1, [y1, x1, y2, x2])\n      crops = tf.image.crop_and_resize(bottom, bboxes, tf.to_int32(batch_ids), [14, 14], name=""crops"")\n\n    return slim.max_pool2d(crops, [2, 2], padding=\'SAME\')\n\n  def _dropout_layer(self, bottom, name, ratio=0.5):\n    return tf.nn.dropout(bottom, ratio, name=name)\n\n  def _anchor_target_layer(self, rpn_cls_score, name):\n    with tf.variable_scope(name) as scope:\n      rpn_labels, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights = tf.py_func(anchor_target_layer, \n                          [rpn_cls_score, self._gt_boxes, self._im_info, self._feat_stride, self._anchors, self._anchor_scales], \n                          [tf.float32, tf.float32, tf.float32, tf.float32])\n\n      rpn_labels.set_shape([1, 1, None, None])\n      rpn_bbox_targets.set_shape([1, None, None, self._num_scales*12])\n      rpn_bbox_inside_weights.set_shape([1, None, None, self._num_scales*12])\n      rpn_bbox_outside_weights.set_shape([1, None, None, self._num_scales*12])\n\n      rpn_labels = tf.to_int32(rpn_labels, name=""to_int32"")\n      self._anchor_targets[\'rpn_labels\'] = rpn_labels\n      self._anchor_targets[\'rpn_bbox_targets\'] = rpn_bbox_targets\n      self._anchor_targets[\'rpn_bbox_inside_weights\'] = rpn_bbox_inside_weights\n      self._anchor_targets[\'rpn_bbox_outside_weights\'] = rpn_bbox_outside_weights\n\n      self._score_summaries.update(self._anchor_targets)\n\n    return rpn_labels\n\n  def _proposal_target_layer(self, rois, roi_scores, name):\n    with tf.variable_scope(name) as scope:\n      rois, roi_scores, labels, bbox_targets, bbox_inside_weights, bbox_outside_weights = tf.py_func(proposal_target_layer, \n                                        [rois, roi_scores, self._gt_boxes, self._num_classes], \n                                        [tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32])\n\n      rois.set_shape([cfg.TRAIN.BATCH_SIZE, 5])\n      roi_scores.set_shape([cfg.TRAIN.BATCH_SIZE])\n      labels.set_shape([cfg.TRAIN.BATCH_SIZE, 1])\n      bbox_targets.set_shape([cfg.TRAIN.BATCH_SIZE, self._num_classes*4])\n      bbox_inside_weights.set_shape([cfg.TRAIN.BATCH_SIZE, self._num_classes*4])\n      bbox_outside_weights.set_shape([cfg.TRAIN.BATCH_SIZE, self._num_classes*4])\n\n      self._proposal_targets[\'rois\'] = rois\n      self._proposal_targets[\'labels\'] = tf.to_int32(labels, name=""to_int32"")\n      self._proposal_targets[\'bbox_targets\'] = bbox_targets\n      self._proposal_targets[\'bbox_inside_weights\'] = bbox_inside_weights\n      self._proposal_targets[\'bbox_outside_weights\'] = bbox_outside_weights\n\n      self._score_summaries.update(self._proposal_targets)\n\n      return rois, roi_scores\n\n  def _anchor_component(self):\n    with tf.variable_scope(\'ANCHOR_\' + self._tag) as scope:\n      height = tf.to_int32(tf.ceil(self._im_info[0, 0] / 16.))\n      width = tf.to_int32(tf.ceil(self._im_info[0, 1] / 16.))\n      anchors, anchor_length = tf.py_func(generate_anchors_pre,\n                          [height, width, \n                          self._feat_stride, self._anchor_scales], \n                          [tf.float32, tf.int32], name=""generate_anchors"")\n      anchors.set_shape([None, 4])\n      anchor_length.set_shape([])\n      self._anchors = anchors\n      self._anchor_length = anchor_length\n\n  def _vgg16_from_imagenet(self, sess, train=True):\n    with tf.variable_scope(\'vgg16_\' + self._tag, regularizer=tf.contrib.layers.l2_regularizer(cfg.TRAIN.WEIGHT_DECAY)) as scope:\n      # select initializers\n      if cfg.TRAIN.TRUNCATED:\n        initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.01)\n        initializer_bbox=tf.truncated_normal_initializer(mean=0.0, stddev=0.001)\n      else:\n        initializer=tf.random_normal_initializer(mean=0.0, stddev=0.01)\n        initializer_bbox=tf.random_normal_initializer(mean=0.0, stddev=0.001)\n      # first layer\n      net = self._conv_layer(sess, self._image, ""conv1_1"", False)\n      # self._act_summaries.append(net)\n      net = self._conv_layer(sess, net, ""conv1_2"", False)\n      # self._act_summaries.append(net)\n      net = slim.max_pool2d(net, [2, 2], padding=\'SAME\', scope=\'pool1\')\n      # second layer\n      net = self._conv_layer(sess, net, ""conv2_1"", False)\n      # self._act_summaries.append(net)\n      net = self._conv_layer(sess, net, ""conv2_2"", False)\n      # self._act_summaries.append(net)\n      net = slim.max_pool2d(net, [2, 2], padding=\'SAME\', scope=\'pool2\')\n      # third layer\n      net = self._conv_layer(sess, net, ""conv3_1"", train)\n      # self._act_summaries.append(net)\n      net = self._conv_layer(sess, net, ""conv3_2"", train)\n      # self._act_summaries.append(net)\n      net = self._conv_layer(sess, net, ""conv3_3"", train)\n      # self._act_summaries.append(net)\n      net = slim.max_pool2d(net, [2, 2], padding=\'SAME\', scope=\'pool3\')\n      # fourth layer\n      net = self._conv_layer(sess, net, ""conv4_1"", train)\n      # self._act_summaries.append(net)\n      net = self._conv_layer(sess, net, ""conv4_2"", train)\n      # self._act_summaries.append(net)\n      net = self._conv_layer(sess, net, ""conv4_3"", train)\n      # self._act_summaries.append(net)\n      net = slim.max_pool2d(net, [2, 2], padding=\'SAME\', scope=\'pool4\')\n      # fifth layer\n      net = self._conv_layer(sess, net, ""conv5_1"", train)\n      # self._act_summaries.append(net)\n      net = self._conv_layer(sess, net, ""conv5_2"", train)\n      # self._act_summaries.append(net)\n      net = self._conv_layer(sess, net, ""conv5_3"", train)\n      self._act_summaries.append(net)\n\n      self._layers[\'conv5_3\'] = net\n      # build the anchors for the image\n      self._anchor_component()\n\n      # rpn\n      rpn = self._conv_layer_shape(net, [3,3], 512, ""rpn_conv/3x3"", initializer, train)\n      self._act_summaries.append(rpn)\n      rpn_cls_score = self._conv_layer_shape(rpn, [1,1], self._num_scales * 6, ""rpn_cls_score"", initializer, train, \'VALID\', False)\n      # change it so that the score has 2 as its channel size\n      rpn_cls_score_reshape = self._reshape_layer(rpn_cls_score, 2, ""rpn_cls_score_reshape"")\n      rpn_cls_prob_reshape = self._softmax_layer(rpn_cls_score_reshape, ""rpn_cls_prob_reshape"")\n      rpn_cls_prob = self._reshape_layer(rpn_cls_prob_reshape, self._num_scales * 6, ""rpn_cls_prob"")\n      rpn_bbox_pred = self._conv_layer_shape(rpn, [1,1], self._num_scales * 12, ""rpn_bbox_pred"", initializer, train, \'VALID\', False)\n\n      if train:\n        rois, roi_scores = self._proposal_layer(rpn_cls_prob, rpn_bbox_pred, ""rois"")\n        rpn_labels = self._anchor_target_layer(rpn_cls_score, ""anchor"")\n        # Try to have a determinestic order for the computing graph, for reproducibility\n        with tf.control_dependencies([rpn_labels]):\n          rois, _ = self._proposal_target_layer(rois, roi_scores, ""rpn_rois"")\n      else:\n        if cfg.TEST.MODE == \'nms\':\n          rois, _ = self._proposal_layer(rpn_cls_prob, rpn_bbox_pred, ""rois"")\n        elif cfg.TEST.MODE == \'top\':\n          rois, _ = self._proposal_top_layer(rpn_cls_prob, rpn_bbox_pred, ""rois"")\n        else:\n          raise NotImplementedError\n\n      # rcnn\n      if cfg.POOLING_MODE == \'crop\':\n        pool5 = self._crop_pool_layer(net, rois,  ""pool5"")\n        resized_img = tf.image.resize_images(net, [14, 14])\n        pool5_img = slim.max_pool2d(resized_img, [2, 2], padding=\'SAME\')\n      else:\n        raise NotImplementedError\n\n\n\n\n      with tf.variable_scope(""fc6"") as scope:\n        weight = self._get_fc_weight(sess, ""fc6"", trainable=train)\n        bias = self._get_bias(sess, ""fc6"", trainable=train)\n        fc6 = self._fc_layer(sess, pool5, weight, bias, ""fc6"")\n        fc6_img = self._fc_layer(sess, pool5_img, weight, bias, ""fc6"")\n      self._act_summaries.append(fc6)\n      if train:\n        fc6 = self._dropout_layer(fc6, ""dropout6"")\n\n      with tf.variable_scope(""fc7"") as scope:\n        weight = self._get_fc_weight(sess, ""fc7"", trainable=train)\n        bias = self._get_bias(sess, ""fc7"", trainable=train)\n        fc7 = self._fc_layer(sess, fc6, weight, bias, ""fc7"")\n        fc7_img = self._fc_layer(sess, fc6_img, weight, bias, ""fc7"")\n\n      self._act_summaries.append(fc7)\n      if train:\n        fc7 = self._dropout_layer(fc7, ""dropout7"")\n\n      self._layers[\'fc7\'] = fc7_img\n      cls_score = self._fc_layer_shape(fc7, self._num_classes, ""cls_score"", initializer, train, False)\n      cls_prob = self._softmax_layer(cls_score, ""cls_prob"")\n      bbox_pred = self._fc_layer_shape(fc7, self._num_classes * 4, ""bbox_pred"", initializer_bbox, train, False)\n\n      self._predictions[""rpn_cls_score""] = rpn_cls_score\n      self._predictions[""rpn_cls_score_reshape""] = rpn_cls_score_reshape\n      self._predictions[""rpn_cls_prob""] = rpn_cls_prob\n      self._predictions[""rpn_bbox_pred""] = rpn_bbox_pred\n      self._predictions[""cls_score""] = cls_score\n      self._predictions[""cls_prob""] = cls_prob\n      self._predictions[""bbox_pred""] = bbox_pred\n      self._predictions[""rois""] = rois\n      self._predictions[\'fc7\'] = fc7\n\n      self._score_summaries.update(self._predictions)\n\n      return rois, cls_prob, bbox_pred\n\n  def _smooth_l1_loss(self, bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights, sigma=1.0, dim=[1]):\n    sigma_2 = sigma**2\n    box_diff = bbox_pred - bbox_targets\n    in_box_diff = bbox_inside_weights * box_diff\n    abs_in_box_diff = tf.abs(in_box_diff)\n    smoothL1_sign = tf.stop_gradient(tf.to_float(tf.less(abs_in_box_diff, 1./sigma_2)))\n    in_loss_box = tf.pow(in_box_diff, 2) * (sigma_2 / 2.0) * smoothL1_sign \\\n                  + (abs_in_box_diff - (0.5 / sigma_2)) * (1.0 - smoothL1_sign)\n    out_loss_box = bbox_outside_weights * in_loss_box\n    loss_box = tf.reduce_mean(tf.reduce_sum(\n                        out_loss_box, \n                        reduction_indices=dim\n                )) \n    return loss_box\n\n  def _add_losses(self, sigma_rpn=3.0):\n    with tf.variable_scope(\'vgg16-loss_\' + self._tag) as scope:\n      # RPN, class loss\n      rpn_cls_score = tf.reshape(self._predictions[\'rpn_cls_score_reshape\'], [-1,2])\n      rpn_label = tf.reshape(self._anchor_targets[\'rpn_labels\'], [-1])\n      rpn_select = tf.where(tf.not_equal(rpn_label,-1))\n      rpn_cls_score = tf.reshape(tf.gather(rpn_cls_score, rpn_select),[-1,2])\n      rpn_label = tf.reshape(tf.gather(rpn_label, rpn_select),[-1])\n      rpn_cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(rpn_cls_score, rpn_label))\n\n      # RPN, bbox loss\n      rpn_bbox_pred = self._predictions[\'rpn_bbox_pred\']\n      rpn_bbox_targets = self._anchor_targets[\'rpn_bbox_targets\']\n      rpn_bbox_inside_weights = self._anchor_targets[\'rpn_bbox_inside_weights\']\n      rpn_bbox_outside_weights = self._anchor_targets[\'rpn_bbox_outside_weights\']\n\n      rpn_loss_box = self._smooth_l1_loss(rpn_bbox_pred, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights, sigma=sigma_rpn, dim=[1,2,3])\n\n      # RCNN, class loss\n      cls_score = self._predictions[""cls_score""]\n      label = tf.reshape(self._proposal_targets[""labels""],[-1])\n      cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(cls_score, label))\n\n      # RCNN, bbox loss\n      bbox_pred = self._predictions[\'bbox_pred\']\n      bbox_targets = self._proposal_targets[\'bbox_targets\']\n      bbox_inside_weights = self._proposal_targets[\'bbox_inside_weights\']\n      bbox_outside_weights = self._proposal_targets[\'bbox_outside_weights\']\n\n      loss_box = self._smooth_l1_loss(bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights)\n\n      self._losses[\'cross_entropy\'] = cross_entropy\n      self._losses[\'loss_box\'] = loss_box\n      self._losses[\'rpn_cross_entropy\'] = rpn_cross_entropy\n      self._losses[\'rpn_loss_box\'] = rpn_loss_box\n\n      loss = cross_entropy + loss_box + rpn_cross_entropy + rpn_loss_box\n      self._losses[\'total_loss\'] = loss\n\n      self._event_summaries.update(self._losses)\n\n    return loss\n\n  def create_architecture(self, sess, mode, num_classes, \n                          caffe_weight_path=None, \n                          tag=None, anchor_scales=[8, 16, 32]):\n    self._image = tf.placeholder(tf.float32, shape=[self._batch_size, None, None, 3])\n    self._im_info = tf.placeholder(tf.float32, shape=[self._batch_size, 3])\n    self._gt_boxes = tf.placeholder(tf.float32, shape=[None, 5])\n    self._caffe_weight_path=caffe_weight_path\n    self._tag=tag\n    \n    self._num_classes = num_classes\n    self._mode = mode\n    self._anchor_scales = anchor_scales\n    self._num_scales = len(anchor_scales)\n\n    training = mode == \'TRAIN\'\n    testing = mode == \'TEST\'\n\n    assert tag != None\n    print \'Loading caffe weights...\'\n    with open(self._caffe_weight_path, \'r\') as f:\n      self._caffe_layers = pickle.load(f)\n    print \'Done!\'\n\n    rois, cls_prob, bbox_pred = self._vgg16_from_imagenet(sess, training)\n\n    layers_to_output = {\'rois\': rois}\n    layers_to_output.update(self._predictions)\n\n    for var in tf.trainable_variables():\n      self._train_summaries.append(var)\n\n    if mode == \'TEST\':\n      stds = np.tile(np.array(cfg.TRAIN.BBOX_NORMALIZE_STDS), (self._num_classes))\n      means = np.tile(np.array(cfg.TRAIN.BBOX_NORMALIZE_MEANS), (self._num_classes))\n      self._predictions[""bbox_pred""] *= stds\n      self._predictions[""bbox_pred""] += means\n    else:\n      self._add_losses()\n      layers_to_output.update(self._losses)\n\n    val_summaries = []\n    with tf.device(""/cpu:0""):\n      for key, var in self._event_summaries.items():\n        val_summaries.append(tf.summary.scalar(key, var))\n      for key, var in self._score_summaries.items():\n        self._add_score_summary(key, var)\n      for var in self._act_summaries:\n        self._add_act_summary(var)\n      for var in self._train_summaries:\n        self._add_train_summary(var)\n\n    self._summary_op = tf.summary.merge_all()\n    if not testing:\n      self._summary_op_val = tf.summary.merge(val_summaries)\n\n    return layers_to_output\n\n  # only useful during testing mode\n  def extract_conv5(self, sess, image):\n    feed_dict={self._image: image}\n    feat = sess.run(self._layers[""conv5_3""], feed_dict=feed_dict)\n    return feat\n\n  def extract_fc7(self, sess, image):\n    feed_dict={self._image: image}\n    feat = sess.run(self._layers[\'fc7\'], feed_dict=feed_dict)\n    return feat\n\n\n  # only useful during testing mode\n  def test_image(self, sess, image, im_info):\n    feed_dict={self._image: image,\n              self._im_info: im_info}\n    cls_score, cls_prob, bbox_pred, rois, feats = sess.run([self._predictions[""cls_score""], \n                                                    self._predictions[\'cls_prob\'], \n                                                    self._predictions[\'bbox_pred\'], \n                                                    self._predictions[\'rois\'], \n                                                    self._predictions[\'fc7\']], \n                                                    feed_dict=feed_dict)\n    return cls_score, cls_prob, bbox_pred, rois, feats\n\n  def get_summary(self, sess, blobs):\n    feed_dict={self._image: blobs[\'data\'], self._im_info: blobs[\'im_info\'], \\\n             self._gt_boxes: blobs[\'gt_boxes\']}\n    summary = sess.run(self._summary_op_val, feed_dict=feed_dict)\n\n    return summary\n\n  def train_step(self, sess, blobs, train_op):\n    feed_dict={self._image: blobs[\'data\'], self._im_info: blobs[\'im_info\'], \\\n             self._gt_boxes: blobs[\'gt_boxes\']}\n    rpn_loss_cls, rpn_loss_box, loss_cls, loss_box, loss, _ = sess.run([self._losses[""rpn_cross_entropy""], \n                                                    self._losses[\'rpn_loss_box\'], \n                                                    self._losses[\'cross_entropy\'], \n                                                    self._losses[\'loss_box\'],\n                                                    self._losses[\'total_loss\'],\n                                                    train_op], \n                                                    feed_dict=feed_dict)\n    return rpn_loss_cls, rpn_loss_box, loss_cls, loss_box, loss\n\n  def train_step_with_summary(self, sess, blobs, train_op):\n    feed_dict={self._image: blobs[\'data\'], self._im_info: blobs[\'im_info\'], \\\n             self._gt_boxes: blobs[\'gt_boxes\']}\n    rpn_loss_cls, rpn_loss_box, loss_cls, loss_box, loss, summary, _ = sess.run([self._losses[""rpn_cross_entropy""], \n                                                    self._losses[\'rpn_loss_box\'], \n                                                    self._losses[\'cross_entropy\'], \n                                                    self._losses[\'loss_box\'],\n                                                    self._losses[\'total_loss\'],\n                                                    self._summary_op,\n                                                    train_op], \n                                                    feed_dict=feed_dict)\n    return rpn_loss_cls, rpn_loss_box, loss_cls, loss_box, loss, summary\n\n  def train_step_no_return(self, sess, blobs, train_op):\n    feed_dict={self._image: blobs[\'data\'], self._im_info: blobs[\'im_info\'], \\\n             self._gt_boxes: blobs[\'gt_boxes\']}\n    sess.run([train_op], feed_dict=feed_dict)\n\n'"
visual_search/lib/nms/__init__.py,0,b''
visual_search/lib/nms/py_cpu_nms.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\n\ndef py_cpu_nms(dets, thresh):\n    """"""Pure Python NMS baseline.""""""\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n'"
visual_search/lib/roi_data_layer/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n'
visual_search/lib/roi_data_layer/layer.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Xinlei Chen\n# --------------------------------------------------------\n\n""""""The data layer used during training to train a Fast R-CNN network.\n\nRoIDataLayer implements a Caffe Python layer.\n""""""\n\nfrom model.config import cfg\nfrom roi_data_layer.minibatch import get_minibatch\nimport numpy as np\nimport time\n\nclass RoIDataLayer(object):\n  """"""Fast R-CNN data layer used for training.""""""\n\n  def __init__(self, roidb, num_classes, random=False):\n    """"""Set the roidb to be used by this layer during training.""""""\n    self._roidb = roidb\n    self._num_classes = num_classes\n    # Also set a random flag\n    self._random = random\n    self._shuffle_roidb_inds()\n\n  def _shuffle_roidb_inds(self):\n    """"""Randomly permute the training roidb.""""""\n    # If the random flag is set, \n    # then the database is shuffled according to system time\n    # Useful for the validation set\n    if self._random:\n      st0 = np.random.get_state()\n      millis = int(round(time.time() * 1000)) % 4294967295\n      np.random.seed(millis)\n    \n    if cfg.TRAIN.ASPECT_GROUPING:\n      widths = np.array([r[\'width\'] for r in self._roidb])\n      heights = np.array([r[\'height\'] for r in self._roidb])\n      horz = (widths >= heights)\n      vert = np.logical_not(horz)\n      horz_inds = np.where(horz)[0]\n      vert_inds = np.where(vert)[0]\n      inds = np.hstack((\n          np.random.permutation(horz_inds),\n          np.random.permutation(vert_inds)))\n      inds = np.reshape(inds, (-1, 2))\n      row_perm = np.random.permutation(np.arange(inds.shape[0]))\n      inds = np.reshape(inds[row_perm, :], (-1,))\n      self._perm = inds\n    else:\n      self._perm = np.random.permutation(np.arange(len(self._roidb)))\n    # Restore the random state\n    if self._random:\n      np.random.set_state(st0)\n      \n    self._cur = 0\n\n  def _get_next_minibatch_inds(self):\n    """"""Return the roidb indices for the next minibatch.""""""\n    \n    if self._cur + cfg.TRAIN.IMS_PER_BATCH >= len(self._roidb):\n      self._shuffle_roidb_inds()\n\n    db_inds = self._perm[self._cur:self._cur + cfg.TRAIN.IMS_PER_BATCH]\n    self._cur += cfg.TRAIN.IMS_PER_BATCH\n\n    return db_inds\n\n  def _get_next_minibatch(self):\n    """"""Return the blobs to be used for the next minibatch.\n\n    If cfg.TRAIN.USE_PREFETCH is True, then blobs will be computed in a\n    separate process and made available through self._blob_queue.\n    """"""\n    db_inds = self._get_next_minibatch_inds()\n    minibatch_db = [self._roidb[i] for i in db_inds]\n    return get_minibatch(minibatch_db, self._num_classes)\n      \n  def forward(self):\n    """"""Get blobs and copy them into this layer\'s top blob vector.""""""\n    blobs = self._get_next_minibatch()\n    return blobs\n'"
visual_search/lib/roi_data_layer/minibatch.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Xinlei Chen\n# --------------------------------------------------------\n\n""""""Compute minibatch blobs for training a Fast R-CNN network.""""""\n\nimport numpy as np\nimport numpy.random as npr\nimport cv2\nfrom model.config import cfg\nfrom utils.blob import prep_im_for_blob, im_list_to_blob\nimport os\nimport cPickle\n\ndef get_minibatch(roidb, num_classes):\n  """"""Given a roidb, construct a minibatch sampled from it.""""""\n  num_images = len(roidb)\n  # Sample random scales to use for each image in this batch\n  random_scale_inds = npr.randint(0, high=len(cfg.TRAIN.SCALES),\n                  size=num_images)\n  assert(cfg.TRAIN.BATCH_SIZE % num_images == 0), \\\n    \'num_images ({}) must divide BATCH_SIZE ({})\'. \\\n    format(num_images, cfg.TRAIN.BATCH_SIZE)\n\n  # Get the input image blob, formatted for caffe\n  im_blob, im_scales = _get_image_blob(roidb, random_scale_inds)\n\n  blobs = {\'data\': im_blob}\n\n  assert len(im_scales) == 1, ""Single batch only""\n  assert len(roidb) == 1, ""Single batch only""\n  \n  # gt boxes: (x1, y1, x2, y2, cls)\n  gt_inds = np.where(roidb[0][\'gt_classes\'] != 0)[0]\n  gt_boxes = np.empty((len(gt_inds), 5), dtype=np.float32)\n  gt_boxes[:, 0:4] = roidb[0][\'boxes\'][gt_inds, :] * im_scales[0]\n  gt_boxes[:, 4] = roidb[0][\'gt_classes\'][gt_inds]\n  blobs[\'gt_boxes\'] = gt_boxes\n  blobs[\'im_info\'] = np.array(\n    [[im_blob.shape[1], im_blob.shape[2], im_scales[0]]],\n    dtype=np.float32)\n\n  return blobs\n\ndef _get_image_blob(roidb, scale_inds):\n  """"""Builds an input blob from the images in the roidb at the specified\n  scales.\n  """"""\n  num_images = len(roidb)\n  processed_ims = []\n  im_scales = []\n  for i in xrange(num_images):\n    im = cv2.imread(roidb[i][\'image\'])\n    if roidb[i][\'flipped\']:\n      im = im[:, ::-1, :]\n    target_size = cfg.TRAIN.SCALES[scale_inds[i]]\n    im, im_scale = prep_im_for_blob(im, cfg.PIXEL_MEANS, target_size,\n                    cfg.TRAIN.MAX_SIZE)\n    im_scales.append(im_scale)\n    processed_ims.append(im)\n\n  # Create a blob to hold the input images\n  blob = im_list_to_blob(processed_ims)\n\n  return blob, im_scales\n'"
visual_search/lib/roi_data_layer/roidb.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Transform a roidb into a trainable roidb by adding a bunch of metadata.""""""\n\nimport numpy as np\nfrom model.config import cfg\nfrom model.bbox_transform import bbox_transform\nfrom utils.cython_bbox import bbox_overlaps\nimport PIL\n\ndef prepare_roidb(imdb):\n  """"""Enrich the imdb\'s roidb by adding some derived quantities that\n  are useful for training. This function precomputes the maximum\n  overlap, taken over ground-truth boxes, between each ROI and\n  each ground-truth box. The class with maximum overlap is also\n  recorded.\n  """"""\n  roidb = imdb.roidb\n  if not (imdb.name.startswith(\'vg\') or imdb.name.startswith(\'avg\') or imdb.name.startswith(\'coco\')):\n    sizes = [PIL.Image.open(imdb.image_path_at(i)).size\n         for i in xrange(imdb.num_images)]\n  for i in xrange(len(imdb.image_index)):\n    roidb[i][\'image\'] = imdb.image_path_at(i)\n    if not (imdb.name.startswith(\'vg\') or imdb.name.startswith(\'avg\') or imdb.name.startswith(\'coco\')):\n      roidb[i][\'width\'] = sizes[i][0]\n      roidb[i][\'height\'] = sizes[i][1]\n    # need gt_overlaps as a dense array for argmax\n    gt_overlaps = roidb[i][\'gt_overlaps\'].toarray()\n    # max overlap with gt over classes (columns)\n    max_overlaps = gt_overlaps.max(axis=1)\n    # gt class that had the max overlap\n    max_classes = gt_overlaps.argmax(axis=1)\n    roidb[i][\'max_classes\'] = max_classes\n    roidb[i][\'max_overlaps\'] = max_overlaps\n    # sanity checks\n    # max overlap of 0 => class should be zero (background)\n    zero_inds = np.where(max_overlaps == 0)[0]\n    assert all(max_classes[zero_inds] == 0)\n    # max overlap > 0 => class should not be zero (must be a fg class)\n    nonzero_inds = np.where(max_overlaps > 0)[0]\n    assert all(max_classes[nonzero_inds] != 0)\n'"
visual_search/lib/utils/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n'
visual_search/lib/utils/blob.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Blob helper functions.""""""\n\nimport numpy as np\nimport cv2\n\ndef im_list_to_blob(ims):\n    """"""Convert a list of images into a network input.\n\n    Assumes images are already prepared (means subtracted, BGR order, ...).\n    """"""\n    max_shape = np.array([im.shape for im in ims]).max(axis=0)\n    num_images = len(ims)\n    blob = np.zeros((num_images, max_shape[0], max_shape[1], 3),\n                    dtype=np.float32)\n    for i in xrange(num_images):\n        im = ims[i]\n        blob[i, 0:im.shape[0], 0:im.shape[1], :] = im\n\n    return blob\n\ndef prep_im_for_blob(im, pixel_means, target_size, max_size):\n    """"""Mean subtract and scale an image for use in a blob.""""""\n    im = im.astype(np.float32, copy=False)\n    im -= pixel_means\n    im_shape = im.shape\n    im_size_min = np.min(im_shape[0:2])\n    im_size_max = np.max(im_shape[0:2])\n    im_scale = float(target_size) / float(im_size_min)\n    # Prevent the biggest axis from being more than MAX_SIZE\n    if np.round(im_scale * im_size_max) > max_size:\n        im_scale = float(max_size) / float(im_size_max)\n    im = cv2.resize(im, None, None, fx=im_scale, fy=im_scale,\n                    interpolation=cv2.INTER_LINEAR)\n\n    return im, im_scale\n'"
visual_search/lib/utils/boxes_grid.py,0,"b'# --------------------------------------------------------\n# Subcategory CNN\n# Copyright (c) 2015 CVGL Stanford\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Yu Xiang\n# --------------------------------------------------------\n\nimport numpy as np\nimport math\nfrom model.config import cfg\n\ndef get_boxes_grid(image_height, image_width):\n    """"""\n    Return the boxes on image grid.\n    """"""\n\n    # height and width of the heatmap\n    if cfg.NET_NAME == \'CaffeNet\':\n        height = np.floor((image_height * max(cfg.TRAIN.SCALES) - 1) / 4.0 + 1)\n        height = np.floor((height - 1) / 2.0 + 1 + 0.5)\n        height = np.floor((height - 1) / 2.0 + 1 + 0.5)\n\n        width = np.floor((image_width * max(cfg.TRAIN.SCALES) - 1) / 4.0 + 1)\n        width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n        width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n    elif cfg.NET_NAME == \'VGGnet\':\n        height = np.floor(image_height * max(cfg.TRAIN.SCALES) / 2.0 + 0.5)\n        height = np.floor(height / 2.0 + 0.5)\n        height = np.floor(height / 2.0 + 0.5)\n        height = np.floor(height / 2.0 + 0.5)\n\n        width = np.floor(image_width * max(cfg.TRAIN.SCALES) / 2.0 + 0.5)\n        width = np.floor(width / 2.0 + 0.5)\n        width = np.floor(width / 2.0 + 0.5)\n        width = np.floor(width / 2.0 + 0.5)\n    else:\n        assert (1), \'The network architecture is not supported in utils.get_boxes_grid!\'\n\n    # compute the grid box centers\n    h = np.arange(height)\n    w = np.arange(width)\n    y, x = np.meshgrid(h, w, indexing=\'ij\') \n    centers = np.dstack((x, y))\n    centers = np.reshape(centers, (-1, 2))\n    num = centers.shape[0]\n\n    # compute width and height of grid box\n    area = cfg.TRAIN.KERNEL_SIZE * cfg.TRAIN.KERNEL_SIZE\n    aspect = cfg.TRAIN.ASPECTS  # height / width\n    num_aspect = len(aspect)\n    widths = np.zeros((1, num_aspect), dtype=np.float32)\n    heights = np.zeros((1, num_aspect), dtype=np.float32)\n    for i in xrange(num_aspect):\n        widths[0,i] = math.sqrt(area / aspect[i])\n        heights[0,i] = widths[0,i] * aspect[i]\n\n    # construct grid boxes\n    centers = np.repeat(centers, num_aspect, axis=0)\n    widths = np.tile(widths, num).transpose()\n    heights = np.tile(heights, num).transpose()\n\n    x1 = np.reshape(centers[:,0], (-1, 1)) - widths * 0.5\n    x2 = np.reshape(centers[:,0], (-1, 1)) + widths * 0.5\n    y1 = np.reshape(centers[:,1], (-1, 1)) - heights * 0.5\n    y2 = np.reshape(centers[:,1], (-1, 1)) + heights * 0.5\n    \n    boxes_grid = np.hstack((x1, y1, x2, y2)) / cfg.TRAIN.SPATIAL_SCALE\n\n    return boxes_grid, centers[:,0], centers[:,1]\n'"
visual_search/lib/utils/im_util.py,0,"b'import cv2\nimport re\nimport numpy as np\n\ndef read_img_base64(base64str):\n    """"""Read image from base64 string\n    """"""\n    #check if contains javascript\n    image_data = re.sub(\'^data:image/.+;base64,\', \'\', base64str)\\\n        .decode(\'base64\')\n\n    nparr = np.fromstring(image_data, np.uint8)\n    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\\\n        .astype(np.float32)\n    return img\n\ndef read_img_blob(im_str):\n    """"""Read image from base64 string\n    """"""\n    #check if contains javascript\n    # arr = np.asarray(bytearray(im_str), dtype=np.uint8)\n    arr = np.fromstring(im_str, np.uint8)\n    # img = cv2.imdecode(arr,-1) # \'load it as it is\'\n    img = cv2.imdecode(arr, cv2.IMREAD_COLOR)\\\n        .astype(np.float32)\n    return img\n'"
visual_search/lib/utils/nms.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\n\ndef nms(dets, thresh):\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n'"
visual_search/lib/utils/timer.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport time\n\nclass Timer(object):\n    """"""A simple timer.""""""\n    def __init__(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n\n    def tic(self):\n        # using time.time instead of time.clock because time time.clock\n        # does not normalize for multithreading\n        self.start_time = time.time()\n\n    def toc(self, average=True):\n        self.diff = time.time() - self.start_time\n        self.total_time += self.diff\n        self.calls += 1\n        self.average_time = self.total_time / self.calls\n        if average:\n            return self.average_time\n        else:\n            return self.diff\n'"
visual_search/lib/datasets/tools/mcg_munge.py,0,"b'import os\nimport sys\n\n""""""Hacky tool to convert file system layout of MCG boxes downloaded from\nhttp://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/\nso that it\'s consistent with those computed by Jan Hosang (see:\nhttp://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-\n  computing/research/object-recognition-and-scene-understanding/how-\n  good-are-detection-proposals-really/)\n\nNB: Boxes from the MCG website are in (y1, x1, y2, x2) order.\nBoxes from Hosang et al. are in (x1, y1, x2, y2) order.\n""""""\n\ndef munge(src_dir):\n    # stored as: ./MCG-COCO-val2014-boxes/COCO_val2014_000000193401.mat\n    # want:      ./MCG/mat/COCO_val2014_0/COCO_val2014_000000141/COCO_val2014_000000141334.mat\n\n    files = os.listdir(src_dir)\n    for fn in files:\n        base, ext = os.path.splitext(fn)\n        # first 14 chars / first 22 chars / all chars + .mat\n        # COCO_val2014_0/COCO_val2014_000000447/COCO_val2014_000000447991.mat\n        first = base[:14]\n        second = base[:22]\n        dst_dir = os.path.join(\'MCG\', \'mat\', first, second)\n        if not os.path.exists(dst_dir):\n            os.makedirs(dst_dir)\n        src = os.path.join(src_dir, fn)\n        dst = os.path.join(dst_dir, fn)\n        print \'MV: {} -> {}\'.format(src, dst)\n        os.rename(src, dst)\n\nif __name__ == \'__main__\':\n    # src_dir should look something like:\n    #  src_dir = \'MCG-COCO-val2014-boxes\'\n    src_dir = sys.argv[1]\n    munge(src_dir)\n'"
