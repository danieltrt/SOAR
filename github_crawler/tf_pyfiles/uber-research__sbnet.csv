file_path,api_count,code
sbnet_tensorflow/benchmark/__init__.py,0,b''
sbnet_tensorflow/benchmark/benchmark_configs.py,0,"b'""""""\n\n   Sparse Blocks Network\n   Copyright (c) 2017, Uber Technologies, Inc.\n\n   Licensed under the Apache License, Version 2.0 (the ""License"");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an ""AS IS"" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n""""""\n\n#\n# Benchmark common configs.\n#\nINPUT_SIZE_DICT = {\n    \'resnet-50\': [\n        [400, 700, 256, 64],    # YAPF_NO_FORMAT\n        [200, 350, 512, 128],\n        [100, 175, 1024, 256],\n        [50, 88, 2048, 512],\n    ],\n    \'resnet-v2\': [\n        [400, 700, 96, 24],    # YAPF_NO_FORMAT\n        [200, 350, 192, 48],\n        [100, 175, 256, 64],\n        [50, 88, 384, 96],\n    ],\n    \'resnet-v3\': [\n        [400, 700, 128, 32],    # YAPF_NO_FORMAT\n        [200, 350, 256, 64],\n        [100, 175, 384, 96],\n        [50, 88, 512, 128],\n    ]\n}\nSPARSITY_LIST = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.99]\nSPARSITY_LIST = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.99]\n'"
sbnet_tensorflow/benchmark/benchmark_topleft.py,2,"b'""""""\n\n   Sparse Blocks Network\n   Copyright (c) 2017, Uber Technologies, Inc.\n\n   Licensed under the Apache License, Version 2.0 (the ""License"");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an ""AS IS"" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n""""""\n\n#\n# Benchmark performance for a synthetic top left corner solid mask.\n#\n# Usage:\n# python benchmark_topleft.py --test [conv | res] --arch [resnet-50 | resnet-v2]\n#\n# Flags:\n# --test: Which benchmark, a convolutional layer or a residual block.\n# --arch: Which architecture, original ResNet-50  (high channel) or modified ResNet-v2 (low channel).\n#\nfrom __future__ import division, print_function\n\nimport os\nimport tensorflow as tf\nimport time\n\nfrom argparse import ArgumentParser\nfrom collections import namedtuple\n\nfrom benchmark_configs import INPUT_SIZE_DICT, SPARSITY_LIST\nfrom benchmark_utils import append_result, create_result, get_out_filename\nfrom sparse_conv_perf import run_one, TestConfig, generate_top_left_mask\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nBATCH_SIZE = 1\nTOL = 0.05\nAVGPOOL = True    # Use average pooling.\nBLOCK_SIZE_LIST_LIST = [\n    range(11, 33, 2),\n    range(7, 25, 2),\n    range(5, 21, 2),\n    range(5, 21, 2),\n]\n\nperf_result_fields = [\n    \'H\', \'W\', \'C\', \'K\', \'BH\', \'BW\', \'sparsity\', \'block_sparsity\', \'dense_time\', \'sparse_time\',\n    \'speedup\'\n]\n\nPerfResult = namedtuple(\'PerfResult\', perf_result_fields)\n\n\ndef main():\n    # Output performance metrics to a CSV file.\n    res_block = args.test == \'res\'\n    out_file = get_out_filename(prefix=\'ours_topleft_{}\'.format(args.test))\n    print(\'Writing output to {}\'.format(out_file))\n    create_result(out_file, perf_result_fields)\n\n    for xsize, bsize_list in zip(INPUT_SIZE_LIST, BLOCK_SIZE_LIST_LIST):\n        # Benchmark dense convolution.\n        if args.test == \'conv\':\n            xsize_ = [BATCH_SIZE] + [xsize[0], xsize[1], xsize[3]]\n            ksize_ = [3, 3, xsize[3], xsize[3]]\n        elif args.test == \'res\':\n            xsize_ = [BATCH_SIZE] + xsize[:-1]\n            ksize_ = [3, 3, xsize[2], xsize[3]]\n        test_config = TestConfig(\n            xsize=xsize_,\n            ksize=ksize_,\n            strides=[1, 1, 1, 1],\n            padding=\'SAME\',\n            bsize=None,\n            is_sparse=False,\n            tol=None,\n            avgpool=None)\n        with tf.Graph().as_default(), tf.Session() as sess:\n            test_result = run_one(sess, None, test_config, res_block=res_block)\n        dense_time = test_result.avg_time\n\n        # Benchmark sparse convolution.\n        for sparsity in SPARSITY_LIST:\n            best_speedup = 0.0\n            best_time = 0.0\n            best_bsize = None\n            mask = generate_top_left_mask(test_config.xsize, sparsity)\n            for bsize in bsize_list:\n                # For this synthetic mask, I found that rectangular block size does not help.\n                bsize_h = bsize\n                bsize_w = bsize\n                # Whether the block size is larger than input size.\n                if bsize_h > xsize[0] or bsize_w > xsize[1]:\n                    continue\n                test_config = TestConfig(\n                    xsize=xsize_,\n                    ksize=ksize_,\n                    strides=[1, 1, 1, 1],\n                    padding=\'SAME\',\n                    bsize=[1, bsize_h, bsize_w, 1],\n                    is_sparse=True,\n                    tol=TOL,\n                    avgpool=AVGPOOL)\n                with tf.Graph().as_default(), tf.Session() as sess:\n                    test_result = run_one(sess, mask, test_config, res_block=res_block)\n                speedup = dense_time / test_result.avg_time\n                if speedup > best_speedup:\n                    best_bsize = (bsize_h, bsize_w)\n                    best_speedup = speedup\n                    best_time = test_result.avg_time\n                    best_block_sparsity = test_result.block_sparsity\n            result = PerfResult(\n                H=xsize[0],\n                W=xsize[1],\n                C=xsize[2],\n                K=xsize[3],\n                BH=best_bsize[0],\n                BW=best_bsize[1],\n                sparsity=sparsity,\n                block_sparsity=best_block_sparsity,\n                dense_time=dense_time,\n                sparse_time=best_time,\n                speedup=best_speedup)\n            append_result(out_file, result)\n\n\nif __name__ == \'__main__\':\n    parser = ArgumentParser(\n        description=\'Sparse block\\\'s convolution and resnet blocks benchmark top left mask\')\n    parser.add_argument(\'--test\', type=str, default=\'conv\', choices=set((\'conv\', \'res\')))\n    parser.add_argument(\n        \'--arch\', type=str, default=\'resnet-v2\', choices=set((\'resnet-50\', \'resnet-v2\')))\n    args = parser.parse_args()\n    print(""Benchmarking with --test=%s --arch=%s"" % (args.test, args.arch))\n    INPUT_SIZE_LIST = INPUT_SIZE_DICT[args.arch]\n    main()\n'"
sbnet_tensorflow/benchmark/benchmark_utils.py,0,"b'""""""\n\n   Sparse Blocks Network\n   Copyright (c) 2017, Uber Technologies, Inc.\n\n   Licensed under the Apache License, Version 2.0 (the ""License"");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an ""AS IS"" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n""""""\n\n#\n# Sparse convolution benchmark utility.\n#\nimport os\nimport time\nuname = os.environ[\'USER\']\n\n\ndef prefix_path(x):\n    """"""Gets home folder path.""""""\n    return os.path.join(\'/home\', uname, x)\n\n\ndef append_result(out_file, r):\n    """"""Appends results to the output file.""""""\n    print(r)\n    rdict = r._asdict()\n    keys = rdict.keys()\n    with open(out_file, \'a\') as f:\n        f.write(\',\'.join([str(rdict[ff]) for ff in keys]) + \'\\n\')\n\n\ndef create_result(out_file, fields):\n    """"""Creates output file and CSV header.""""""\n    with open(out_file, \'w\') as f:\n        f.write(\',\'.join(fields) + \'\\n\')\n\n\ndef get_out_filename(prefix=\'sparse_conv_perf_out\'):\n    """"""Generates output filename.""""""\n    out_file = prefix_path(\'{}_{:d}.csv\'.format(prefix, int(time.time())))\n    return out_file\n'"
sbnet_tensorflow/benchmark/cu_prof.py,4,"b'""""""\n\n   Sparse Blocks Network\n   Copyright (c) 2017, Uber Technologies, Inc.\n\n   Licensed under the Apache License, Version 2.0 (the ""License"");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an ""AS IS"" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n""""""\n\nfrom .benchmark_utils import prefix_path\nimport ctypes\n\n_cudart = ctypes.CDLL(\'libcudart.so\')\n\n\ndef cu_prof_start():\n    # As shown at http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__PROFILER.html,\n    # the return value will unconditionally be 0. This check is just in case it changes in\n    # the future.\n    ret = _cudart.cudaProfilerStart()\n    if ret != 0:\n        raise Exception(\'cudaProfilerStart() returned %d\' % ret)\n\n\ndef cu_prof_stop():\n    ret = _cudart.cudaProfilerStop()\n    if ret != 0:\n        raise Exception(\'cudaProfilerStop() returned %d\' % ret)\n\n\ndef cu_prof_stop_func(func, do_trace=False):\n    """"""Profile a single function with both CUDA and TF trace.""""""\n    import tensorflow as tf\n    from tensorflow.python.client import timeline\n    if do_trace:\n        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n    else:\n        run_options = tf.RunOptions()\n    run_metadata = tf.RunMetadata()\n    with tf.Graph().as_default(), tf.Session() as sess:\n        func(sess, run_options, run_metadata)\n    # Create the Timeline object, and write it to a json\n    if do_trace:\n        tl = timeline.Timeline(run_metadata.step_stats)\n        ctf = tl.generate_chrome_trace_format()\n        with open(prefix_path(\'timeline.json\'), \'w\') as f:\n            f.write(ctf)\n        print(\'Done writing timeline.\')\n    ret = _cudart.cudaProfilerStop()\n    if ret != 0:\n        raise Exception(\'cudaProfilerStop() returned %d\' % ret)\n'"
sbnet_tensorflow/benchmark/logger.py,0,"b'""""""\n\n   Sparse Blocks Network\n   Copyright (c) 2017, Uber Technologies, Inc.\n\n   Licensed under the Apache License, Version 2.0 (the ""License"");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an ""AS IS"" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n""""""\n\n#\n# Logging utility.\n#\n\nimport datetime\nimport logging\n\n\nclass _MyFormatter(logging.Formatter):\n    width = 24\n    datefmt = \'%Y-%m-%d %H:%M:%S.%f\'\n\n    def format(self, record):\n        cpath = \'%s:%s:%s\' % (record.module, record.funcName, record.lineno)\n        if len(cpath) > self.width:\n            cpath = ""..."" + cpath[-self.width + 3:]\n        record.message = record.getMessage()\n        s = ""{} {}: {}: {}"".format(record.levelname,\n                                   datetime.datetime.now().isoformat(chr(32)), cpath,\n                                   record.getMessage())\n        if record.exc_info:\n            # Cache the traceback text to avoid converting it multiple times\n            # (it\'s constant anyway)\n            if not record.exc_text:\n                record.exc_text = self.formatException(record.exc_info)\n        if record.exc_text:\n            if s[-1:] != ""\\n"":\n                s = s + ""\\n""\n            s = s + record.exc_text\n        return s\n\n\nlogger = None\n\n\ndef get():\n    global logger\n    if logger is None:\n        logging.addLevelName(logging.WARNING,\n                             ""\\033[0;33m%s:\\033[1;0m"" % logging.getLevelName(logging.WARNING))\n        logging.addLevelName(logging.ERROR,\n                             ""\\033[0;31m%s:\\033[1;0m"" % logging.getLevelName(logging.ERROR))\n        logging.addLevelName(logging.INFO,\n                             ""\\033[0;32m%s:\\033[1;0m"" % logging.getLevelName(logging.INFO))\n        logging.addLevelName(logging.DEBUG,\n                             ""\\033[0;39m%s:\\033[1;0m"" % logging.getLevelName(logging.DEBUG))\n        logger = logging.getLogger(__name__)\n        logger.propagate = False\n        ch = logging.StreamHandler()\n        formatter = _MyFormatter()\n        ch.setFormatter(formatter)\n        logger.addHandler(ch)\n    return logger\n\n\ndef main():\n    log = get()\n    log.setLevel(logging.DEBUG)\n    log.debug(""Hey"")\n    log.info(""Hey"")\n    log.warning(""Hey"")\n    log.error(""Hey"")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
sbnet_tensorflow/benchmark/reduce_mask_tests.py,4,"b'""""""\n\n   Sparse Blocks Network\n   Copyright (c) 2017, Uber Technologies, Inc.\n\n   Licensed under the Apache License, Version 2.0 (the ""License"");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an ""AS IS"" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n""""""\n\nfrom __future__ import division, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom sparse_conv_lib import convert_mask_to_indices, convert_mask_to_indices_custom\nfrom sparse_conv_lib import calc_block_params\n\ndef to_tuples(l):\n    return [tuple(x) for x in l]\n\nclass ReduceMaskTests(tf.test.TestCase):\n    def _test_reduce_mask(self, mask, bsize, ksize, strides, padding):\n        with tf.Session():\n            mask = tf.constant(mask)\n            indices = convert_mask_to_indices(mask, bsize, ksize, strides, padding, 0.0)\n            indices_val = indices.eval()\n\n            x_shape = [1] + [int(ss) for ss in mask.get_shape()[1:]] + [1]\n            block_params = calc_block_params(x_shape, bsize, ksize, strides, padding)\n            indices_custom = convert_mask_to_indices_custom(mask, block_params, 0.0)\n\n            activeBlockIndicesResult = indices_custom.active_block_indices.eval()\n            binCountsResult = indices_custom.bin_counts.eval()\n            clippedResults = activeBlockIndicesResult[:binCountsResult[0], :]\n            clippedResults = to_tuples(clippedResults.tolist())\n            refResults = to_tuples(indices_val.tolist())\n            np.testing.assert_equal(set(clippedResults), set(refResults))\n\n    def test_basic(self):\n        bsize = [1, 3, 3, 1]\n        ksize = [3, 3, 1, 1]\n        strides = [1, 1, 1, 1]\n        padding = \'SAME\'\n        mask = np.array(\n            [[\n                [0, 0, 0, 0, 0],    # YAPF_NO_FORMAT\n                [0, 0, 1, 0, 0],\n                [1, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0]\n            ]],\n            dtype=np.float32)\n        self._test_reduce_mask(mask, bsize, ksize, strides, padding)\n\n    def test_larger(self):\n        bsize = [1, 5, 5, 1]\n        ksize = [2, 2, 1, 1]\n        strides = [1, 1, 1, 1]\n        padding = \'VALID\'\n        mask = np.array(\n            [[\n                [0, 0, 0, 0, 0, 1, 1, 1],    # YAPF_NO_FORMAT\n                [0, 0, 0, 0, 0, 1, 1, 1],\n                [1, 0, 0, 0, 0, 1, 1, 1],\n                [0, 0, 0, 0, 0, 1, 1, 1],\n                [0, 0, 0, 0, 0, 1, 1, 1],\n                [0, 0, 0, 0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 0, 0, 0]\n            ]],\n            dtype=np.float32)\n        self._test_reduce_mask(mask, bsize, ksize, strides, padding)\n\n\nif __name__ == \'__main__\':\n    tf.test.main()\n'"
sbnet_tensorflow/benchmark/sparse_conv_lib.py,105,"b'""""""\n\n   Sparse Blocks Network\n   Copyright (c) 2017, Uber Technologies, Inc.\n\n   Licensed under the Apache License, Version 2.0 (the ""License"");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an ""AS IS"" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n""""""\n\n#\n# Sparse convolution operators.\n#\n# Usage:\n# ```\n# import numpy as np\n# import tensorflow as tf\n#\n# from sparse_conv_lib import convert_mask_to_block_indices, sparse_conv2d\n#\n# # Binary mask to define sparsity.\n# mask = tf.constant(\n#     np.array(\n#         [[\n#             [0, 0, 0, 0, 0],    # YAPF_NO_FORMAT\n#             [0, 0, 1, 0, 0],\n#             [1, 0, 0, 0, 0],\n#             [0, 0, 0, 0, 0],\n#             [0, 0, 0, 0, 0]\n#         ]],\n#         dtype=np.float32))\n# # Convert binary mask to block representation.\n# ind_blk = convert_mask_to_block_indices(mask, [1, 3, 3, 1], [1, 1, 1, 1], [3, 3, 1, 1],\n#                                         [1, 1, 1, 1], \'SAME\', .1)\n#\n# # Sparse convolution.\n# x = tf.constant(np.ones([1, 5, 5, 1], dtype=np.float32))\n# w = tf.constant(np.ones([3, 3, 1, 1], dtype=np.float32))\n# y = sparse_conv2d(x, w, ind_blk, [1, 1, 1, 1], \'SAME\')\n#\n# with tf.Session():\n#     print(np.squeeze(y.eval()))\n#\n# >> Output\n# >> [[ 0.  6.  6.  6.  0.]\n#     [ 6.  9.  9.  9.  0.]\n#     [ 6.  9.  9.  9.  0.]\n#     [ 6.  9.  0.  0.  0.]\n#     [ 0.  0.  0.  0.  0.]]\n# ```\nfrom __future__ import division, print_function\n\nimport os\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.framework import ops\nfrom collections import namedtuple\n\nimport logger\nfrom tf_conv_dims import calc_padding_4d, calc_out_size_4d, calc_out_size_4d_np\n\nlog = logger.get()\n\nsbnet_module = tf.load_op_library(\'../sbnet_ops/libsbnet.so\')\n\nBlockParams = namedtuple(\'BlockParams\', [\'bsize\', \'bsize_out\', \'boffset\', \'bcount\', \'bstrides\'])\n\n\n# Gradients registration.\n@ops.RegisterGradient(""SparseGather"")\ndef _sparse_gather_grad(op, grad):\n    # x is shaped like full tensor [NHWC]\n    # grad is shaped as gathered blocks [Nblocks*BH*BW*C]\n    x = op.inputs[0]\n    binCounts = op.inputs[1]\n    activeBlockIndices = op.inputs[2]\n    bsize = op.inputs[3]\n    bstride = op.inputs[4]\n    boffset = op.inputs[5]\n    transpose = op.get_attr(""transpose"")\n\n    # if scatter is overlapping then gradient should still work\n    # because we are overwriting the same values\n    # compute dOutput/dx\n    result = sbnet_module.sparse_scatter(\n        grad,\n        binCounts,\n        activeBlockIndices,\n        tf.zeros_like(x),    # output base tensor to add on top of\n        dynamic_bsize=bsize,\n        dynamic_bstride=bstride,\n        dynamic_boffset=boffset,\n        add=True,\n        transpose=transpose,\n        atomic=True)\n\n    return [result, None, None, None, None, None]    # no gradients wrt indices or block params\n\n\n@ops.RegisterGradient(""SparseScatter"")\ndef _sparse_scatter_grad(op, grad):\n    # x is shaped like blocked tensor of gathered blocks [Nblocks*BH*BW*C]\n    # grad is shaped as output tensor [NHWC]\n    blocksX = op.inputs[0]\n    binCounts = op.inputs[1]\n    activeBlockIndices = op.inputs[2]\n    ybase = op.inputs[3]\n    bsize = op.inputs[4]\n    bstride = op.inputs[5]\n    boffset = op.inputs[6]\n    doAdd = op.get_attr(""add"")\n\n    dout_dx = sbnet_module.sparse_gather(\n        grad,\n        binCounts,\n        activeBlockIndices,\n        dynamic_bsize=bsize,\n        dynamic_bstride=bstride,\n        dynamic_boffset=boffset)\n\n    # return a list of gradients of output with respect to each input\n    if not doAdd:\n        # scatter blocks of zeroes over a base tensor of ones to compute a stamp-out gradient mask for dy_dybase\n        stamp_out_blocks = sbnet_module.sparse_scatter(\n            tf.zeros_like(blocksX),\n            binCounts,\n            activeBlockIndices,\n            tf.ones_like(grad),\n            dynamic_bsize=bsize,\n            dynamic_bstride=bstride,\n            dynamic_boffset=boffset,\n            add=False)\n        dy_dybase = grad * stamp_out_blocks\n        return [dout_dx, None, None, dy_dybase, None, None, None]\n    else:\n        # d(x+ybase)/dybase = 1, so just pass back grad as dout_dybase\n        return [dout_dx, None, None, grad, None, None, None]\n\n\ndef _pad_input(x, ksize, strides, padding, bsize=None, bstrides=None):\n    """"""Pads the input tensor.\n    Optional to pass in block strides. The right hand side padding will be increased if the last\n    block does not fit in (no effect on the convolution results.\n\n    :param x:        [Tensor]   [N, H, W, C]. input tensor, dtype float32.\n    :param ksize:    [list]     List of 4 int. Sparse convolution kernel size.\n    :param strides:  [list]     List of 4 int. Sparse convolution stride size.\n    :param padding:  [string]   `VALID` or `SAME`, padding method for sparse convolution.\n    :param bsize     [list]     List of 4 int. Block size. Optional.\n    :param bstrides: [list]     List of 4 int. Block strides. Optional.\n\n    :return          [Tensor]   [N, H+Ph, W+Pw, C]. Padded input tensor.\n    """"""\n    x_shape = tf.shape(x)\n    if padding == \'SAME\':\n        pad_h0, pad_h1, pad_w0, pad_w1 = calc_padding_4d(x_shape, ksize, strides, padding)\n\n        if bstrides is not None:\n            # Here we do not use the standard padding on the right hand side.\n            # If the convolution results is larger than expected, the scatter function will not use\n            # out-of-boundary points.\n            assert bsize is not None, \'Must pass in bsize and bstrides together.\'\n            h = x_shape[1] + pad_h0 + pad_h1\n            w = x_shape[2] + pad_w0 + pad_w1\n            pad_h1 += tf.mod(-h + bsize[1], bstrides[1])\n            pad_w1 += tf.mod(-w + bsize[2], bstrides[2])\n        return tf.pad(x, [[0, 0], [pad_h0, pad_h1], [pad_w0, pad_w1], [0, 0]])\n    else:\n        if bstrides is not None:\n            assert bsize is not None, \'Must pass in bsize and bstrides together.\'\n            h = x_shape[1]\n            w = x_shape[2]\n            pad_h1 = tf.mod(-h + bsize[1], bstrides[1])\n            pad_w1 = tf.mod(-w + bsize[2], bstrides[2])\n            return tf.cond(\n                tf.logical_or(tf.greater(pad_h1, 0), tf.greater(pad_w1, 0)),\n                lambda: tf.pad(x, [[0, 0], [0, pad_h1], [0, pad_w1], [0, 0]]), lambda: x)\n        else:\n            return x\n\n\ndef _get_offset_array_tf(shape):\n    """"""\n    Computes the offset array used to upsample indices with TensorFlow.\n\n    :param shape:   [list]     Window shape.\n    """"""\n    center = [(ss - 1) // 2 for ss in shape]\n    axes = [tf.range(-cc, ss - cc, dtype=tf.int32) for cc, ss in zip(center, shape)]\n    # Broadcast and match dimension.\n    if len(shape) > 1:\n        for jj in range(len(shape)):\n            for ii in range(len(shape) + 1):\n                if ii != jj:\n                    axes[jj] = tf.expand_dims(axes[jj], ii)\n        for jj in range(len(shape)):\n            shape_ = [ss for ss in shape] + [1]\n            shape_[jj] = 1\n            axes[jj] = tf.tile(axes[jj], shape_)\n        offset = tf.concat(axes, len(shape))\n    return offset\n\n\ndef _get_offset_array(shape):\n    """"""\n    Computes the offset array used to upsample indices with NumPy (static).\n\n    :param shape:   [list]     Window shape.\n    """"""\n    center = [int(ss - 1) // 2 for ss in shape]\n    axes = [np.arange(-cc, int(ss) - cc).astype(np.int32) for cc, ss in zip(center, shape)]\n    if len(shape) > 1:\n        for jj in range(len(shape)):\n            for ii in range(len(shape) + 1):\n                if ii != jj:\n                    axes[jj] = np.expand_dims(axes[jj], ii)\n        for jj in range(len(shape)):\n            shape_ = [int(ss) for ss in shape] + [1]\n            shape_[jj] = 1\n            axes[jj] = np.tile(axes[jj], shape_)\n        offset = np.concatenate(axes, len(shape))\n        return tf.constant(offset)\n    else:\n        return tf.constant(axes[0])\n\n\ndef _calc_block_strides(bsize, ksize, strides):\n    """"""Calculates strides for blocks.\n\n    :param bsize:     [list]        List of 4 int. Size of blocks, or downsample ratio.\n    :param ksize:     [list]        List of 4 int. Sparse convolution kernel size.\n    :param strides:   [list]        List of 4 int. Sparse convolution strides.\n\n    :return           [list]        List of 4 int. Block strides.\n    """"""\n    return [1, bsize[1] - ksize[0] + strides[1], bsize[2] - ksize[1] + strides[2], 1]\n\n\ndef upsample_indices(indices, ksize, strides):\n    """"""\n    Upsamples the indices to have all indices in a rectangle.\n\n    :param indices:   [Tensor]      [M, 3]. Center locations (N, H, W) of the M rectangles.\n                                    Dtype int32.\n    :param ksize:     [list]        Size of the rectangle, or downsample ratio.\n    :param strides:   [list]        Strides of the pooling operation.\n\n    :return           [Tensor]      [M, h, w, 3]. Locations of all pixels in the rectangles.\n                                    Dtype int32.\n    """"""\n    assert len(indices.get_shape()) == 2, \'Expect indices rank = 2\'\n    assert ksize[0] == ksize[3] == 1, \'Expect first and last dimensions of ksize = 1\'\n    assert strides[0] == strides[3] == 1, \'Expect first and last dimensions of strides = 1, {}\'.format(\n        strides)\n    h_scale = strides[1]\n    w_scale = strides[2]\n    scale = tf.stack([1, h_scale, w_scale])\n    indices *= scale\n    # Since we always use VALID to perform pooling, shift is needed here.\n    shift = tf.stack([0, (ksize[1] - 1) // 2, (ksize[2] - 1) // 2])\n    indices += shift\n    indices_ = tf.expand_dims(tf.expand_dims(indices, 1), 2)\n    # indices_ = tf.tile(indices_, [1, ksize[1], ksize[2], 1])\n    offset = _get_offset_array(ksize[0:3])\n    indices_ += offset\n    return indices_\n\n\ndef convert_mask_to_indices(mask, bsize, ksize, strides, padding, tol):\n    """"""\n    Converts a binary mask to sparse indices.\n\n    :param mask:     [Tensor]   [N, H, W]. 1 indicates non-sparse locations. Dtype float32.\n    :param bsize:    [list]     List of 4 int. Size of blocks, or downsample ratio.\n    :param ksize:    [list]     List of 4 int. Sparse convolution kernel size.\n    :param strides:  [list]     List of 4 int. Sparse convolution stride size.\n                                Currently only supports when,\n                                1) (bsize[1] - ksize[0]) % strides[1] == 0 and,\n                                2) (bsize[2] - ksize[1]) % strides[2] == 0\n    :param padding:  [string]   `VALID` or `SAME`, padding method for sparse convolution.\n    :param tol:      [float]    Lower bound of occupancy for creating a rectangle.\n\n    :return          [Tensor]   [M, 3]. Center locations (N, H, W) of M rectangles. Dtype int32.\n    """"""\n    ERR_MSG_RANK = \'Expect mask rank = 3\'\n    ERR_MSG_DIV = \'Expect `stride` divides `bsize` - `ksize`. stride {}, bsize {}, ksize {}.\'\n    ERR_MSG_DIM = \'Expect first and last dimensions of strides = 1. Dim {}.\'\n\n    assert len(mask.get_shape()) == 3, ERR_MSG_RANK\n    assert type(bsize) in [list, tuple], \'`bsize` needs to be a list or tuple.\'\n    assert type(ksize) in [list, tuple], \'`ksize` needs to be a list or tuple.\'\n    assert type(strides) in [list, tuple], \'`strides` needs to be a list or tuple.\'\n    assert (bsize[1] - ksize[0]) % strides[1] == 0, ERR_MSG_DIV.format(\n        strides[1], bsize[1], ksize[0])\n    assert (bsize[2] - ksize[1]) % strides[2] == 0, ERR_MSG_DIV.format(\n        strides[2], bsize[2], ksize[1])\n    assert strides[0] == strides[3] == 1, ERR_MSG_DIM.format(strides)\n\n    bstrides = _calc_block_strides(bsize, ksize, strides)\n\n    # Pad mask.\n    mask_ = tf.expand_dims(mask, 3)\n    mask_ = _pad_input(mask_, ksize, strides, padding, bsize=bsize, bstrides=bstrides)\n    mask_ = tf.nn.max_pool(mask_, bsize, bstrides, \'VALID\')    # Blocks are always valid conv.\n    mask_ = tf.squeeze(mask_, [3])\n    indices = tf.where(tf.greater(mask_, tol))\n    indices = tf.cast(indices, tf.int32)\n    return indices\n\n\ndef convert_mask_to_block_indices(mask, bsize, ksize, strides, padding, tol):\n    """"""\n    Converts a binary mask to block sparse indices.\n\n    :param mask:     [Tensor]   [N, H, W]. 1 indicates non-sparse locations. Dtype float32.\n    :param bsize:    [list]     List of 4 int. Size of blocks, or downsample ratio.\n    :param ksize:    [list]     List of 4 int. Sparse convolution kernel size.\n    :param strides:  [list]     List of 4 int. Sparse convolution stride size.\n                                Currently only supports when,\n                                1) (bsize[1] - ksize[0]) % strides[1] == 0 and,\n                                2) (bsize[2] - ksize[1]) % strides[2] == 0\n    :param padding:  [string]   `VALID` or `SAME`, padding method for sparse convolution.\n    :param tol:      [float]    Lower bound of occupancy for creating a rectangle.\n\n    :return          [Tensor]   [M, h, w, 3]. Pixel locations of M rectangles. Dtype int32.\n    """"""\n    indices = convert_mask_to_indices(mask, bsize, ksize, strides, padding, tol)\n    bstrides = _calc_block_strides(bsize, ksize, strides)\n    blk_indices = upsample_indices(indices, bsize, bstrides)\n    return blk_indices\n\n\ndef calc_block_params(in_size, bsize, ksize, strides, padding):\n    """"""\n    Calculates block parameters for a single convolution layer.\n\n    :param in_size:  [list]     List of 4 int, or a Tensor of size 4. Size of the convolution input.\n    :param bsize:    [list]     List of 4 int. Size of blocks, or downsample ratio.\n    :param ksize:    [list]     List of 4 int. Sparse convolution kernel size.\n    :param strides:  [list]     List of 4 int. Sparse convolution stride size.\n                                Currently only supports when,\n                                1) (bsize[1] - ksize[0]) % strides[1] == 0 and,\n                                2) (bsize[2] - ksize[1]) % strides[2] == 0\n    :param padding:  [string]   `VALID` or `SAME`, padding method for sparse convolution.\n\n    :return          [tuple]\n        bsize:\n        bsize_out:\n        boffset:\n        bcount:\n        bstrides:\n    """"""\n    static = not (type(in_size) == tf.Tensor)\n\n    assert ((bsize[1] - ksize[0]) % strides[1] == 0)\n    assert ((bsize[2] - ksize[1]) % strides[2] == 0)\n\n    bstrides = _calc_block_strides(bsize, ksize, strides)\n    pad_h0, pad_h1, pad_w0, pad_w1 = calc_padding_4d(in_size, ksize, strides, padding)\n    h = in_size[1]\n    w = in_size[2]\n    # Make padding divides blocks.\n    pad_h1 += (-h + bsize[1]) % bstrides[1]\n    pad_w1 += (-w + bsize[2]) % bstrides[2]\n    boffset = [-pad_h0, -pad_w0]\n    x_pad_shape = [\n        in_size[0], in_size[1] + pad_h0 + pad_h1, in_size[2] + pad_w0 + pad_w1, in_size[3]\n    ]\n    if static:\n        out_shape = calc_out_size_4d_np(x_pad_shape, [bsize[1], bsize[2], 1, 1], bstrides, \'VALID\')\n    else:\n        out_shape = calc_out_size_4d(x_pad_shape, [bsize[1], bsize[2], 1, 1], bstrides, \'VALID\')\n    bcount = [out_shape[1], out_shape[2]]\n    bsize_out = calc_out_size_4d_np(bsize, ksize, strides, \'VALID\')\n    bsize = bsize[1:3]\n    bstrides = bstrides[1:3]\n    bsize_out = bsize_out[1:3]\n    if static:\n        assert (pad_h0 == -boffset[0])\n        assert (pad_w0 == -boffset[1])\n        for i, siz in zip([0, 1], [h, w]):\n            # make sure last block is inside\n            err_msg = \'Making sure last block is inside boffset {} bstrides {} bcount {} size {}\'.format(\n                boffset[i], bstrides[i], bcount[i], siz)\n            assert (boffset[i] + bstrides[i] * (bcount[i] - 1) < siz), err_msg\n    return BlockParams(\n        bsize=bsize, bsize_out=bsize_out, boffset=boffset, bcount=bcount, bstrides=bstrides)\n\n\ndef calc_block_params_res_block(in_size, bsize, ksize_list, strides, padding):\n    """"""\n    Calculates block parameters for a residual block.\n\n    :param in_size:  [list]     List of 4 int. Size of the residual block input.\n    :param bsize:    [list]     List of 4 int. Size of blocks, or downsample ratio, for each\n                                convolution layer in the residual block.\n    :param ksize:    [list]     List of list of 4 int. Sparse convolution kernel size.\n    :param strides:  [list]     List of 4 int. Sparse convolution stride size, for the first\n                                convolution in the residual block.\n                                Currently only supports when,\n                                1) (bsize[1] - ksize[0]) % strides[1] == 0 and,\n                                2) (bsize[2] - ksize[1]) % strides[2] == 0\n    :param padding:  [string]   `VALID` or `SAME`, padding method for sparse convolution.\n\n    :return\n    """"""\n    # Use the receptive field as the kernel size.\n    ksize_h = 1 + sum([kk[0] - 1 for kk in ksize_list])\n    ksize_w = 1 + sum([kk[1] - 1 for kk in ksize_list])\n    ksize_real = [ksize_h, ksize_w, 1, 1]\n    return calc_block_params(in_size, bsize, ksize_real, strides, padding)\n\n\ndef convert_mask_to_indices_custom(mask, block_params, tol, avgpool=False):\n    """"""\n    Converts a binary mask to sparse index format for custom CUDA kernel and TF ops.\n\n    :param mask:         [Tensor]   [N, H, W]. 1 indicates non-sparse locations. Dtype float32.\n    :param block_params  [tuple]    Contains bsize, boffset, bcount, bstrides.\n    :param tol:          [float]    Lower bound of occupancy for creating a rectangle.\n\n    :return          [tuple]\n        bin_counts:           [Tensor]. Number of active locations for each bin.\n        active_block_indices: [Tensor]. [M]. Center locations of M rectangles. Dtype int64.\n    """"""\n\n    def to_tensor(a, dtype):\n        if type(a) == tf.Tensor:\n            if a.dtype != dtype:\n                return tf.cast(a, dtype)\n            else:\n                return a\n        elif type(a) == list:\n            if type(a[0]) == tf.Tensor:\n                return tf.stack(a, 0)\n            else:\n                return tf.constant(a, dtype)\n        else:\n            print(type(a))\n            return tf.constant(a, dtype)\n\n    return sbnet_module.reduce_mask(\n        mask,\n        block_params.bcount,\n        dynamic_bsize=to_tensor(block_params.bsize, tf.int32),\n        dynamic_bstride=to_tensor(block_params.bstrides, tf.int32),\n        dynamic_boffset=to_tensor(block_params.boffset, tf.int32),\n        avgpool=avgpool,\n        tol=tol)\n\n\ndef sparse_conv2d(x, w, blk_indices, strides, padding):\n    """"""\n    Performs 2D convolution on a sparse feature map, given indices.\n    Naive python implementation of sparse convolution using gather and scatter.\n\n    :param x:           [Tensor]  [N, H, W, C]. Input activation tensor, dtype float32.\n    :param w:           [Tensor]  [I, J, C, K]. Convolution kernel, dtype float32.\n    :param blk_indices: [Tensor]  [M, h, w, 3]. Block indices of rectangles.\n    :param strides:     [list]    List of 4 int, convolution strides.\n    :param padding:     [string]  `VALID` or `SAME`, padding method for sparse convolution.\n\n    :return             [Tensor]  [N, H\', W\', C]. Convolution results.\n    """"""\n    blk_shape = tf.shape(blk_indices)\n    blk_indices_ = tf.reshape(blk_indices, [-1, 3])\n    ksize = tf.shape(w)\n\n    # Calculate the block strides.\n    bstrides = _calc_block_strides(blk_shape, ksize, strides)\n\n    # Calculate the output size.\n    x_shape = tf.shape(x)\n    out_shape = calc_out_size_4d(x_shape, ksize, strides, padding)\n\n    # Pad input.\n    x_ = _pad_input(\n        x, ksize, strides, padding, bsize=[1, blk_shape[1], blk_shape[2], 1], bstrides=bstrides)\n\n    # Convolution when number of indices is larger than zero.\n    def _conv_nonzero():\n        # Gather patches.\n        p = tf.gather_nd(x_, blk_indices_)\n\n        # Reshape patches.\n        p = tf.reshape(p, [blk_shape[0], blk_shape[1], blk_shape[2], -1])\n\n        # Convolution on patches.\n        q = tf.nn.conv2d(p, w, strides, \'VALID\', use_cudnn_on_gpu=True)\n\n        # Paste convolution results.\n        q_shape = tf.shape(q)\n\n        def _strides_gt_one():\n            # Calculate output indices when strides > 1.\n            blk_indices_crop = tf.strided_slice(blk_indices, [0, 0, 0, 0], [\n                blk_shape[0], q_shape[1] * strides[1], q_shape[2] * strides[2], 3\n            ], strides)\n            blk_indices_crop = blk_indices_crop // tf.stack([1, strides[1], strides[2]])\n            return blk_indices_crop\n\n        def _strides_one():\n            # Calculate otuput indices when strides = 1.\n            return blk_indices[:, :q_shape[1], :q_shape[2], :]\n\n        strides_gt_one = tf.logical_or(tf.greater(strides[1], 1), tf.greater(strides[2], 1))\n        blk_indices_crop = tf.cond(strides_gt_one, _strides_gt_one, _strides_one)\n        y = tf.scatter_nd(blk_indices_crop, q, out_shape)\n        return y\n\n    return tf.cond(\n        tf.equal(tf.size(blk_indices_), 0), lambda: tf.zeros(out_shape, dtype=x.dtype),\n        _conv_nonzero)\n\n\n# returns an int64 start timer handle that should be passed to cuda_timer_end_op\ndef cuda_timer_start_op():\n    return sbnet_module.cuda_timer_start()\n\n\n# returns a float\ndef cuda_timer_end_op(start_timer):\n    return sbnet_module.cuda_timer_end(start_timer)\n\n\ndef sparse_conv2d_custom(x,\n                         w,\n                         indices,\n                         block_params,\n                         strides,\n                         use_var=False,\n                         transpose=False,\n                         atomic=False):\n    assert strides[1] == strides[2] == 1, \'Only accept strides=1\'\n    # TODO: make the gather op also accepting a Tensor for bsize, ksize, etc.\n    ksize = [int(ss) for ss in w.get_shape()]\n    p = sbnet_module.sparse_gather(\n        x,\n        indices.bin_counts,\n        indices.active_block_indices,\n        dynamic_bsize=block_params.bsize,\n        dynamic_bstride=block_params.bstrides,\n        dynamic_boffset=block_params.boffset,\n        transpose=transpose)\n\n    # Convolution on patches.\n    if transpose:\n        q = tf.nn.conv2d(p, w, strides, \'VALID\', data_format=\'NCHW\', use_cudnn_on_gpu=True)\n    else:\n        q = tf.nn.conv2d(p, w, strides, \'VALID\', use_cudnn_on_gpu=True)\n\n    # Allocate output tensor.\n    if use_var:\n        y = sbnet_module.sparse_scatter_var(\n            q,\n            indices.bin_counts,\n            indices.active_block_indices,\n            x,\n            dynamic_bsize=tf.constant(block_params.bsize_out, dtype=tf.int32),\n            dynamic_bstride=tf.constant(block_params.bstrides, dtype=tf.int32),\n            dynamic_boffset=tf.constant([0, 0], dtype=tf.int32),\n            add=False,\n            transpose=transpose,\n            atomic=atomic)\n    else:\n        y = sbnet_module.sparse_scatter(\n            q,\n            indices.bin_counts,\n            indices.active_block_indices,\n            x,\n            dynamic_bsize=tf.constant(block_params.bsize_out, dtype=tf.int32),\n            dynamic_bstride=tf.constant(block_params.bsize_out, dtype=tf.int32),\n            dynamic_boffset=tf.constant([0, 0], dtype=tf.int32),\n            add=False,\n            transpose=transpose,\n            atomic=atomic)\n    return y\n\n\ndef _batch_norm(name, x, is_training, data_format=\'NHWC\'):\n    """"""\n    Applies batch normalization.\n\n    :param name:       [string]    Name of the variable scope.\n    :param x:          [Tensor]    Tensor to apply BN on.\n    :param is_training [bool]   Whether in training mode.\n\n    :return:           [Tensor]    Normalized activation.\n    """"""\n    bn = tf.contrib.layers.batch_norm(\n        x, fused=True, scale=True, data_format=data_format, is_training=is_training, scope=name)\n    return bn\n    # log.warning(\'Not using BN to test performance at inference time\')\n    # return x\n\n\ndef _relu(name, x):\n    """"""\n    Applies ReLU function.\n\n    :param name: [string]     Name of the op.\n    :param x:    [Tensor]     Input to the function.\n\n    :return:     [Tensor]     Output of the function.\n    """"""\n    return tf.nn.relu(x, name=name)\n    # log.warning(\'Not using ReLU to test performance at inference time\')\n    # return x\n\n\ndef _stride_arr(n, data_format=\'NHWC\'):\n    """"""Makes strides array for downsampling convolution.""""""\n    if data_format == \'NHWC\':\n        return [1, n, n, 1]\n    elif data_format == \'NCHW\':\n        return [1, 1, n, n]\n    else:\n        raise ValueError(\'Unknown data format: {}\'.format(data_format))\n\n\ndef _conv(name,\n          x,\n          ksize,\n          strides,\n          padding,\n          data_format=\'NHWC\',\n          weight_decay=None,\n          dtype=tf.float32,\n          weights_on_cpu=False):\n    """"""\n    Convolution layer.\n\n    :param name           [string]     Name of the op.\n    :param x:             [Tensor]     Input to the downsample.\n    :param ksize          [list]       4-D kernel shape.\n    :param strides:       [list]       4-D strides array.\n    :param padding:       [string]     Convolution padding strategy.\n    :param data_format:   [string]     \'NHWC\' or \'NCHW\'.\n\n    :return:              [Tensor]     Convolution output.\n    """"""\n    with tf.variable_scope(name):\n        in_filters = ksize[2]\n        out_filters = ksize[3]\n        n = ksize[0] * ksize[1] * out_filters\n        init = tf.truncated_normal_initializer(\n            mean=0.0, stddev=np.sqrt(2.0 / n), seed=0, dtype=dtype)\n\n        def _reg(x):\n            if weight_decay is not None:\n                return tf.multiply(tf.nn.l2_loss(x), weight_decay)\n            else:\n                return None\n\n        if weight_decay is not None:\n            reg = _reg\n        else:\n            reg = None\n\n        kernel = tf.get_variable(\n            \'w\', ksize, initializer=init, regularizer=reg, dtype=dtype, trainable=True)\n\n        return tf.nn.conv2d(\n            x, kernel, strides, padding, data_format=data_format, use_cudnn_on_gpu=True)\n\n\ndef _bottleneck_residual(x,\n                         ksize_list,\n                         strides,\n                         padding,\n                         is_training,\n                         data_format=\'NHWC\',\n                         no_activation=False):\n    with tf.variable_scope(\'sub1\'):\n        if not no_activation:\n            x = _batch_norm(\'bn1\', x, is_training, data_format)\n            x = _relu(\'relu1\', x)\n\n        STRIDES_ERR_MSG = \'Strides height and width are not the same.\'\n        if data_format == \'NHWC\':\n            assert strides[1] == strides[2], STRIDES_ERR_MSG\n        elif data_format == \'NCHW\':\n            assert strides[2] == strides[3], STRIDES_ERR_MSG\n        x = _conv(\n            \'conv1\',\n            x,\n            ksize_list[0],\n            _stride_arr(strides[2], data_format),\n            padding,\n            data_format=data_format)\n\n    with tf.variable_scope(\'sub2\'):\n        x = _batch_norm(\'bn2\', x, is_training, data_format)\n        x = _relu(\'relu2\', x)\n        x = _conv(\n            \'conv2\',\n            x,\n            ksize_list[1],\n            _stride_arr(1, data_format),\n            padding,\n            data_format=data_format)\n\n    with tf.variable_scope(\'sub3\'):\n        x = _batch_norm(\'bn3\', x, is_training, data_format)\n        x = _relu(\'relu3\', x)\n        x = _conv(\n            \'conv3\',\n            x,\n            ksize_list[2],\n            _stride_arr(1, data_format),\n            padding,\n            data_format=data_format)\n    return x\n\n\ndef res_block_bottleneck(x,\n                         ksize_list,\n                         strides,\n                         is_training,\n                         data_format=\'NHWC\',\n                         w_project=None,\n                         no_activation=False):\n    """"""\n    Computes y = x + F(x), where F(x) is the residual block function. At downsample layers, applies\n    a downsample function on x as well.\n    """"""\n    if w_project is not None:\n        x_ = tf.conv2d(x, w_project, strides, padding=\'SAME\', data_format=data_format)\n    else:\n        x_ = x\n    return x_ + _bottleneck_residual(\n        x,\n        ksize_list,\n        strides,\n        \'SAME\',\n        is_training,\n        data_format=data_format,\n        no_activation=no_activation)\n\n\ndef sparse_res_block_bottleneck(x,\n                                ksize_list,\n                                indices,\n                                block_params,\n                                strides,\n                                is_training,\n                                data_format=\'NHWC\',\n                                w_project=None,\n                                no_activation=False,\n                                use_var=False):\n    """"""\n    Computes y = x + F(x), where F(x) is the residual block function. At downsample layers, applies\n    a downsample function on x as well.\n\n    :param x:                 [Tensor]  [N, H, W, C]. Input activation tensor, dtype float32.\n    :param ksize_list:        [list]    List of list of 4 int. Kernel size for each convolution\n                                        layer in the residual block.\n    :param indices:           [tuple]   Non-sparse locations returned by reduce_mask.\n    :param block_params:      [tuple]   BlockParam namedtuple.\n    :param\n\n    :return\n    """"""\n    transpose = True if data_format == \'NCHW\' else False\n    p = sbnet_module.sparse_gather(\n        x,\n        indices.bin_counts,\n        indices.active_block_indices,\n        dynamic_bsize=block_params.bsize,\n        dynamic_bstride=block_params.bstrides,\n        dynamic_boffset=block_params.boffset,\n        transpose=transpose)\n\n    if w_project is not None:\n        x = tf.conv2d(x, w_project, strides, padding=\'SAME\')\n\n    # Set shape for BN in the residual function.\n    if transpose:\n        p.set_shape([None, x.get_shape()[3], block_params.bsize[0], block_params.bsize[1]])\n    else:\n        p.set_shape([None, block_params.bsize[0], block_params.bsize[1], x.get_shape()[3]])\n\n    q = _bottleneck_residual(\n        p,\n        ksize_list,\n        strides,\n        \'VALID\',\n        is_training,\n        data_format=data_format,\n        no_activation=no_activation)\n\n    if use_var:\n        y = sbnet_module.sparse_scatter_var(\n            q,\n            indices.bin_counts,\n            indices.active_block_indices,\n            x,\n            dynamic_bsize=tf.constant(block_params.bsize_out, dtype=tf.int32),\n            dynamic_bstride=tf.constant(block_params.bsize_out, dtype=tf.int32),\n            dynamic_boffset=tf.constant([0, 0], dtype=tf.int32),\n            add=True,\n            transpose=transpose)\n    else:\n        y = sbnet_module.sparse_scatter(\n            q,\n            indices.bin_counts,\n            indices.active_block_indices,\n            x,\n            dynamic_bsize=tf.constant(block_params.bsize_out, dtype=tf.int32),\n            dynamic_bstride=tf.constant(block_params.bsize_out, dtype=tf.int32),\n            dynamic_boffset=tf.constant([0, 0], dtype=tf.int32),\n            add=True,\n            transpose=transpose)\n    return y\n\n\ndef sparse_conv2d_matmul(x, w, blk_indices, strides, padding):\n    """"""\n    Performs 2D convolution using matrix multiplication on a sparse feature map.\n    Naive python implementation of sparse convolution using gather and scatter.\n\n    :param x:           [Tensor]  [N, H, W, C]. Input activation tensor, dtype float32.\n    :param w:           [Tensor]  [I, J, C, K]. Convolution kernel, dtype float32.\n    :param blk_indices: [Tensor]  [M, h, w, 3]. Block indices of rectangles.\n    :param strides:     [list]    List of 4 int, convolution strides.\n    :param padding:     [string]  `VALID` or `SAME`, padding method for sparse convolution.\n\n    :return             [Tensor]  [N, H\', W\', C]. Convolution results.\n    """"""\n    blk_indices_ = tf.reshape(blk_indices, [-1, 3])\n    blk_shape = tf.shape(blk_indices)\n    ksize = tf.shape(w)\n\n    # Calculate the block strides.\n    bstrides = _calc_block_strides(blk_shape, ksize, strides)\n\n    # Calculate the output size.\n    x_shape = tf.shape(x)\n    out_shape = calc_out_size_4d(x_shape, ksize, strides, padding)\n\n    # Pad input.\n    x_ = _pad_input(\n        x, ksize, strides, padding, bsize=[1, blk_shape[1], blk_shape[2], 1], bstrides=bstrides)\n\n    # In matrix multiplication mode, the block patch should be the same as the kernel size.\n    assert_shape = tf.assert_equal(\n        tf.stack([blk_shape[1], blk_shape[2]]),\n        tf.stack([ksize[0], ksize[1]]),\n        message=\'Expect blk_indices.shape[1] == w.shape[0] and blk_indices.shape[2] == w.shape[1].\')\n\n    # Currently we do not support strides > 1 in this matrix multiplication mode. Could be supported\n    # in the future.\n    assert_strides = tf.assert_equal(\n        tf.cast(tf.stack([strides[1], strides[2]]), tf.int64),\n        tf.constant([1, 1], dtype=tf.int64),\n        message=\'Strides > 1 not supported.\')\n\n    # Convolution when number of indices is larger than zero.\n    def _conv_nonzero():\n        # Gather patches.\n        p = tf.gather_nd(x_, blk_indices_)\n        p_ = tf.reshape(p, [-1, ksize[0] * ksize[1] * ksize[2]])\n\n        # Convolution on patches.\n        w_ = tf.reshape(w, [ksize[0] * ksize[1] * ksize[2], -1])\n        q = tf.matmul(p_, w_)\n\n        # Center locations.\n        blk_indices_crop = blk_indices[:, 0, 0, :]\n\n        #  Project back to an image.\n        y = tf.scatter_nd(blk_indices_crop, q, out_shape)\n        return y\n\n    with tf.control_dependencies([assert_shape, assert_strides]):\n        return tf.cond(\n            tf.equal(tf.size(blk_indices_), 0), lambda: tf.zeros(out_shape, dtype=x.dtype),\n            _conv_nonzero)\n\n\ndef mask_conv2d(x, w, mask, strides, padding):\n    """"""Masked 2D convolution. Used to check 2D sparse convolution.\n\n    :param x:         [Tensor]    Convolution feature map, 4D, dtype float32.\n    :param w:         [Tensor]    Convolution kernel, 4D, dtype float32.\n    :param mask:      [Tensor]    Binary mask, 3D or 4D, [N, H, W] or [N, H, W, 1], dtype float32.\n    :param strides:   [list]      List of 4 int. Convolution strides.\n    :param padding:   [string]    Convolution padding method, `VALID` or `SAME`.\n    """"""\n    assert len(mask.get_shape()) in [3, 4], \'Mask shape must be 3D or 4D.\'\n    if len(mask.get_shape()) == 3:\n        mask_ = tf.expand_dims(mask, 3)\n    elif len(mask.get_shape()) == 4:\n        mask_ = mask\n        assert mask.get_shape()[-1] == 1, \'4D mask last dimension must be 1.\'\n    ksize = [int(ss) for ss in w.get_shape()]\n    psize = [1, ksize[0], ksize[1], 1]\n    mask_ = tf.nn.max_pool(mask_, psize, strides, padding)\n    return tf.nn.conv2d(x, w, strides, padding) * mask_\n'"
sbnet_tensorflow/benchmark/sparse_conv_perf.py,32,"b'""""""\n\n   Sparse Blocks Network\n   Copyright (c) 2017, Uber Technologies, Inc.\n\n   Licensed under the Apache License, Version 2.0 (the ""License"");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an ""AS IS"" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n""""""\n\n#\n# Sparse convolution performance profile utilities.\n#\nfrom __future__ import print_function\n\nimport ctypes\nimport cv2\nimport itertools\nimport numpy as np\nimport os\nimport six\nimport tensorflow as tf\nimport time\n\nfrom collections import namedtuple\nfrom sparse_conv_lib import sparse_conv2d, sparse_conv2d_custom\nfrom sparse_conv_lib import convert_mask_to_block_indices, convert_mask_to_indices_custom\nfrom sparse_conv_lib import calc_block_params, calc_block_params_res_block\nfrom sparse_conv_lib import res_block_bottleneck, sparse_res_block_bottleneck\nfrom sparse_conv_lib import cuda_timer_start_op, cuda_timer_end_op\nfrom tensorflow.python.framework.errors_impl import ResourceExhaustedError, InternalError\n\nN_REPEAT = 5\nN_WARMUP = 3\nN_RUN = 3\n\n\ndef generate_top_left_mask(xsize, sparsity):\n    """"""\n    Generates a square top-left mask with a target sparsity value.\n\n    :param xsize:       [list]      List of 4 int.\n    :param sparsity:    [float]     Target sparsity value.\n\n    :return:            [Tensor]    A tensor with shape to be `xsize` and contains a square of 1\'s\n                                    and the rest being 0\'s.\n    """"""\n    density = 1.0 - sparsity\n    edge_ratio = np.sqrt(density)\n    height = int(np.ceil(edge_ratio * xsize[1]))\n    width = int(np.ceil(edge_ratio * xsize[2]))\n    x = np.zeros(xsize[:-1], dtype=np.float32)\n    x[:, :height, :width] = 1.0\n    return x\n\n\nTestConfig = namedtuple(\n    \'TestConfig\', [\'xsize\', \'ksize\', \'bsize\', \'strides\', \'padding\', \'is_sparse\', \'tol\', \'avgpool\'])\n\nTestResult = namedtuple(\'TestResult\', [\'avg_time\', \'block_sparsity\'])\n\nReduceMask = namedtuple(\'ReduceMask\', [\'active_block_indices\', \'bin_counts\'])\n\nTestGraph = namedtuple(\'TestGraph\', [\'x_init\', \'mask_init\', \'bin_init\', \'ind_init\', \'y\', \'dt\'])\n\n\ndef _sparse_res_block_with_mask(x, ksize_list, block_params, strides, ind_init, bin_init):\n    """"""Sparse conv 2d with mask.""""""\n    ind_obj = ReduceMask(active_block_indices=ind_init, bin_counts=bin_init)\n    y_ = sparse_res_block_bottleneck(\n        x, ksize_list, ind_obj, block_params, strides, True, use_var=True, data_format=\'NCHW\')\n    return y_\n\n\ndef _sparse_conv2d_custom_with_mask(x, w, block_params, strides, ind_init, bin_init):\n    """"""Sparse conv 2d with mask.""""""\n    ind_obj = ReduceMask(active_block_indices=ind_init, bin_counts=bin_init)\n    y_ = sparse_conv2d_custom(x, w, ind_obj, block_params, strides, use_var=True, transpose=True)\n    return y_\n\n\ndef _sparse_conv2d_with_mask(x, w, strides, padding, mask, bsize, tol):\n    """"""Sparse conv 2d with mask.""""""\n    ksize = [int(ss) for ss in w.get_shape()]\n    ind_blk = convert_mask_to_block_indices(mask, bsize, ksize, strides, padding, tol, avgpool=True)\n    y = sparse_conv2d(x, w, ind_blk, strides, padding)\n    return y\n\n\n# MASK_FOLDER = \'/mnt/yyz_data_0/users/byang10/pva_det/output/delete_me/odtac_4d_val_vis/mask/\'\n# MASK = cv2.imread(MASK_FOLDER + \'odtac_4d_val_vis_1000-0000_label.png\')\n\n\ndef run_block_sparsity(sess, mask, config):\n    block_params = calc_block_params(config.xsize, config.bsize, config.ksize, config.strides,\n                                     config.padding)\n    ind = convert_mask_to_indices_custom(mask, block_params, config.tol, config.avgpool)\n    ind_val, bin_val = sess.run([ind.active_block_indices, ind.bin_counts])\n    block_density = bin_val[0] / float(ind_val.shape[0])\n    return 1 - block_density\n\n\ndef _build_res_block(mask, config, x_init, ind_init, bin_init, n_repeat=N_REPEAT):\n    """"""Buildds a computation graph for a single residual block.""""""\n    ksize_list = [[1, 1, config.ksize[2], config.ksize[3]]]\n    ksize_list += [[3, 3, config.ksize[3], config.ksize[3]]]\n    ksize_list += [[1, 1, config.ksize[3], config.ksize[2]]]\n    xs = []\n    ys = []\n    if config.is_sparse:\n        # pre-create xs to exclude from timing, need independent variables to disable TF identical subgraph folding\n        for i in six.moves.xrange(n_repeat):\n            with tf.variable_scope(\'sparse_{}\'.format(i)):\n                xs.append( tf.Variable(x_init) )\n        with tf.control_dependencies(xs):\n            dt0 = cuda_timer_start_op()\n        with tf.control_dependencies([mask, dt0]):\n            block_params = calc_block_params_res_block(config.xsize, config.bsize, ksize_list,\n                                                       config.strides, config.padding)\n            ind = convert_mask_to_indices_custom(mask, block_params, config.tol, config.avgpool)\n        for i in six.moves.xrange(n_repeat):\n            with tf.control_dependencies(ys + [dt0]):\n                with tf.variable_scope(\'sparse_{}\'.format(i)):\n                    y_ = _sparse_res_block_with_mask(xs[i], ksize_list, block_params, config.strides,\n                                                     ind_init, bin_init)\n                ys.append(y_)\n    else:\n        ind = None\n        # pre-create xs to exclude from timing, need independent variables to disable TF identical subgraph folding\n        for i in six.moves.xrange(n_repeat):\n            with tf.variable_scope(\'dense_{}\'.format(i)):\n                xs.append(tf.Variable(tf.transpose(x_init, [0, 3, 1, 2])))    # NCHW\n        with tf.control_dependencies(xs):\n            dt0 = cuda_timer_start_op()\n        for i in six.moves.xrange(n_repeat):\n            with tf.control_dependencies(ys + [dt0]):\n                with tf.variable_scope(\'dense_{}\'.format(i)):\n                    y_ = res_block_bottleneck(\n                        xs[i],\n                        ksize_list,\n                        config.strides,\n                        True,\n                        data_format=\'NCHW\',\n                        w_project=None,\n                        no_activation=False)\n                ys.append(y_)\n    with tf.control_dependencies(ys+[dt0]):\n        dt = cuda_timer_end_op(dt0)\n        with tf.control_dependencies(ys+[dt]):\n            y = tf.concat(ys, 0)\n    return y, ind, dt\n\n\ndef _build_conv(mask, config, x_init, ind_init, bin_init, n_repeat=N_REPEAT):\n    """"""Builds a computation graph for a single convolution.""""""\n    wnp = np.random.uniform(-1, 1, config.ksize)    # filter is RSCK\n    w = tf.constant(wnp, dtype=tf.float32)\n    # AP: Tensorflow doesn\'t support KCRS from my investigation\n    #wt = tf.constant(np.transpose(wnp, [3, 2, 0, 1]), dtype=tf.float32) # transpose to KCRS\n    xs = []\n    ys = []\n    if config.is_sparse:\n        for i in six.moves.xrange(n_repeat):\n            xs.append( tf.Variable(x_init) )\n        with tf.control_dependencies(xs):\n            dt0 = cuda_timer_start_op()\n        with tf.control_dependencies([mask, dt0]):\n            block_params = calc_block_params(config.xsize, config.bsize, config.ksize,\n                                             config.strides, config.padding)\n            ind = convert_mask_to_indices_custom(mask, block_params, config.tol, config.avgpool)\n        for i in six.moves.xrange(n_repeat):\n            with tf.control_dependencies(ys + [dt0]):\n                y_ = _sparse_conv2d_custom_with_mask(xs[i], w, block_params, config.strides, ind_init,\n                                                     bin_init)\n                ys.append(y_)\n    else:\n        ind = None\n        for i in six.moves.xrange(n_repeat):\n            xs.append(tf.Variable(tf.transpose(x_init, [0, 3, 1, 2])))    # NCHW\n        with tf.control_dependencies(xs):\n            dt0 = cuda_timer_start_op()\n        for i in six.moves.xrange(n_repeat):\n            with tf.control_dependencies(ys + [dt0]):\n                y_ = tf.nn.conv2d(xs[i], w, config.strides, config.padding, data_format=\'NCHW\')\n                ys.append(y_)\n    with tf.control_dependencies(ys+[dt0]):\n        dt = cuda_timer_end_op(dt0)\n        with tf.control_dependencies(ys+[dt]):\n            y = tf.concat(ys, 0)\n    return y, ind, dt\n\n\ndef run_one(sess,\n            mask,\n            config,\n            res_block=False,\n            options=None,\n            run_metadata=None,\n            n_warmup=N_WARMUP,\n            n_run=N_RUN,\n            n_repeat=N_REPEAT):\n    """"""Runs a single setting timing.\n\n    :param sess:         [object]      TensorFlow Session object.\n    :param config:       [object]      TestConfig object.\n    :param res_block:    [bool]        Whether do single convolution or residual block.\n    :param options:      [object]      Session run options.\n    :param run_metadata  [object]      RunMetadata object.\n    :param n_warmup      [int]         Number of warm-up runs.\n    :param n_run         [int]         Number of runs for timing.\n\n    :return:             [object]      TestResult object.\n    """"""\n    # Placeholder is needed when x\'s size is larger than 2GB.\n    x_init = tf.placeholder(tf.float32, config.xsize)\n    ind_init = tf.placeholder(tf.int16)\n    bin_init = tf.placeholder(tf.int32)\n    if config.is_sparse:\n        mask = tf.constant(mask)\n\n    # Build computation graph.\n    if not res_block:\n        y, ind, dt = _build_conv(mask, config, x_init, ind_init, bin_init, n_repeat=n_repeat)\n    else:\n        y, ind, dt = _build_res_block(mask, config, x_init, ind_init, bin_init, n_repeat=n_repeat)\n\n    # Initialize inputs.\n    sess.run(\n        tf.global_variables_initializer(),\n        feed_dict={x_init: np.random.uniform(-1, 1, config.xsize).astype(np.float32)})\n\n    # Sparse indices.\n    if ind is not None:\n        ind_val, bin_val = sess.run([ind.active_block_indices, ind.bin_counts])\n        block_density = bin_val[0] / float(ind_val.shape[0])\n        feed_dict = {ind_init: ind_val, bin_init: bin_val}\n    else:\n        block_density = 1.0\n        feed_dict = None\n\n    # Warm up.\n    for ii in six.moves.xrange(n_warmup):\n        sess.run(y, feed_dict=feed_dict)\n\n    # Actual timing.\n    all_dt = []\n    for trial in six.moves.xrange(n_run):\n        _, dtval = sess.run(\n            [y, dt], options=options, run_metadata=run_metadata, feed_dict=feed_dict)\n        all_dt.append(dtval)\n    avg_time = np.array(all_dt).mean() / n_repeat\n\n    return TestResult(avg_time=avg_time, block_sparsity=1 - block_density)\n'"
sbnet_tensorflow/benchmark/sparse_conv_tests.py,32,"b'""""""\n\n   Sparse Blocks Network\n   Copyright (c) 2017, Uber Technologies, Inc.\n\n   Licensed under the Apache License, Version 2.0 (the ""License"");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an ""AS IS"" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n""""""\n\n#\n# Unit tests for sparse_conv.py\n#\nfrom __future__ import division, print_function, unicode_literals\n\nimport itertools\nimport numpy as np\nimport tensorflow as tf\n\nfrom sparse_conv_lib import _get_offset_array\nfrom sparse_conv_lib import calc_block_params\nfrom sparse_conv_lib import convert_mask_to_block_indices\nfrom sparse_conv_lib import convert_mask_to_indices_custom\nfrom sparse_conv_lib import mask_conv2d\nfrom sparse_conv_lib import sparse_conv2d\nfrom sparse_conv_lib import sparse_conv2d_custom\nfrom sparse_conv_lib import sparse_conv2d_matmul\nfrom sparse_conv_lib import upsample_indices\n\n\nclass UpsampleIndicesTests(tf.test.TestCase):\n    def test_offset_array(self):\n        offset_exp = np.array(\n            [\n                [\n                    [-1, -1],    # YAPF_NO_FORMAT\n                    [-1, 0],\n                    [-1, 1]\n                ],\n                [\n                    [0, -1],    # YAPF_NO_FORMAT\n                    [0, 0],\n                    [0, 1]\n                ],\n                [\n                    [1, -1],    # YAPF_NO_FORMAT\n                    [1, 0],\n                    [1, 1]\n                ]\n            ],\n            dtype=np.int32)\n        offset = _get_offset_array([3, 3])\n        with self.test_session():\n            offset_act = offset.eval()\n            np.testing.assert_array_equal(offset_act.shape, [3, 3, 2])\n            np.testing.assert_array_equal(offset_act, offset_exp)\n\n    def test_upsample_indices(self):\n        ind = np.array(\n            [\n                [0, 1, 2],    # YAPF_NO_FORMAT\n                [0, 2, 0]\n            ],\n            dtype=np.int32)\n        ind = tf.constant(ind)\n        ind_up_exp = np.array(\n            [\n                [0, 1, 2],    # YAPF_NO_FORMAT\n                [0, 1, 3],\n                [0, 1, 4],\n                [0, 2, 2],\n                [0, 2, 3],\n                [0, 2, 4],\n                [0, 3, 2],\n                [0, 3, 3],\n                [0, 3, 4],\n                [0, 2, 0],\n                [0, 2, 1],\n                [0, 2, 2],\n                [0, 3, 0],\n                [0, 3, 1],\n                [0, 3, 2],\n                [0, 4, 0],\n                [0, 4, 1],\n                [0, 4, 2],\n            ],\n            dtype=np.int32)\n        ind_up = upsample_indices(ind, [1, 3, 3, 1], [1, 1, 1, 1])\n        with self.test_session():\n            ind_up_act = ind_up.eval()\n            np.testing.assert_array_equal(ind_up_act.shape, [2, 3, 3, 3])\n            np.testing.assert_array_equal(ind_up_act.reshape([-1, 3]), ind_up_exp)\n\n\nclass SparseConv2DTests(tf.test.TestCase):\n    def _test_sparse_conv2d(self, ind_blk, padding, y_exp):\n        ind_blk = tf.reshape(ind_blk, [2, 3, 3, 3])\n        x = tf.constant(np.ones([1, 5, 5, 1], dtype=np.float32))\n        w = tf.constant(np.ones([3, 3, 1, 1], dtype=np.float32))\n        y = sparse_conv2d(x, w, ind_blk, [1, 1, 1, 1], padding)\n        with self.test_session():\n            y_act = y.eval()\n            np.testing.assert_array_equal(y_act.reshape(y_exp.shape), y_exp)\n\n    def test_sparse_conv2d_valid(self):\n        ind_blk = tf.constant(\n            np.array(\n                [\n                    [0, 1, 2],    # YAPF_NO_FORMAT\n                    [0, 1, 3],\n                    [0, 1, 4],\n                    [0, 2, 2],\n                    [0, 2, 3],\n                    [0, 2, 4],\n                    [0, 3, 2],\n                    [0, 3, 3],\n                    [0, 3, 4],\n                    [0, 2, 0],\n                    [0, 2, 1],\n                    [0, 2, 2],\n                    [0, 3, 0],\n                    [0, 3, 1],\n                    [0, 3, 2],\n                    [0, 4, 0],\n                    [0, 4, 1],\n                    [0, 4, 2],\n                ],\n                dtype=np.int32))\n        ind_blk = tf.reshape(ind_blk, [2, 3, 3, 3])\n        y_exp = np.array(\n            [[\n                [[0], [0], [0]],    # YPAF_NO_FORAMT\n                [[0], [0], [9]],\n                [[9], [0], [0]]\n            ]],\n            dtype=np.float32)\n        padding = \'VALID\'\n        self._test_sparse_conv2d(ind_blk, padding, y_exp)\n\n    def test_sparse_conv2d_same(self):\n        ind_blk = tf.constant(\n            np.array(\n                [\n                    [0, 1, 2],    # YAPF_NO_FORMAT\n                    [0, 1, 3],\n                    [0, 1, 4],\n                    [0, 2, 2],\n                    [0, 2, 3],\n                    [0, 2, 4],\n                    [0, 3, 2],\n                    [0, 3, 3],\n                    [0, 3, 4],\n                    [0, 2, 0],\n                    [0, 2, 1],\n                    [0, 2, 2],\n                    [0, 3, 0],\n                    [0, 3, 1],\n                    [0, 3, 2],\n                    [0, 4, 0],\n                    [0, 4, 1],\n                    [0, 4, 2],\n                ],\n                dtype=np.int32))\n        ind_blk = tf.reshape(ind_blk, [2, 3, 3, 3])\n        y_exp = np.array(\n            [[\n                [[0], [0], [0], [0], [0]],    # YAPF_NO_FORAMT\n                [[0], [0], [9], [0], [0]],\n                [[6], [0], [0], [0], [0]],\n                [[0], [0], [0], [0], [0]],\n                [[0], [0], [0], [0], [0]]\n            ]],\n            dtype=np.float32)\n        padding = \'SAME\'\n        self._test_sparse_conv2d(ind_blk, padding, y_exp)\n\n    def _test_sparse_conv2d_with_mask(self, mask, bsize, ksize, strides, padding, y_exp):\n        mask_ = tf.constant(mask)\n        ind_blk = convert_mask_to_block_indices(mask_, bsize, ksize, strides, padding, 0.)\n        x = tf.constant(np.ones([1, mask.shape[1], mask.shape[2], 1], dtype=np.float32))\n        w = tf.constant(np.ones(ksize, dtype=np.float32))\n        y = sparse_conv2d(x, w, ind_blk, strides, padding)\n        with self.test_session():\n            y_act = y.eval()\n            self.assertEqual(y_act.size, y_exp.size)\n            np.testing.assert_array_equal(y_act.reshape(y_exp.shape), y_exp)\n\n    def test_sparse_conv2d_with_mask_valid(self):\n        bsize = [1, 3, 3, 1]\n        ksize = [3, 3, 1, 1]\n        strides = [1, 1, 1, 1]\n        padding = \'VALID\'\n        mask = np.array(\n            [[\n                [0, 0, 0, 0, 0],    # YAPF_NO_FORMAT\n                [0, 0, 1, 0, 0],\n                [1, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0]\n            ]],\n            dtype=np.float32)\n        y_exp = np.array(\n            [[\n                [[9], [9], [9]],    # YAPF_NO_FORMAT\n                [[9], [9], [9]],\n                [[9], [0], [0]],\n            ]],\n            dtype=np.float32)\n        self._test_sparse_conv2d_with_mask(mask, bsize, ksize, strides, padding, y_exp)\n\n    def test_sparse_conv2d_with_mask_same(self):\n        bsize = [1, 3, 3, 1]\n        ksize = [3, 3, 1, 1]\n        strides = [1, 1, 1, 1]\n        padding = \'SAME\'\n        mask = np.array(\n            [[\n                [0, 0, 0, 0, 0],    # YAPF_NO_FORMAT\n                [0, 0, 1, 0, 0],\n                [1, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0]\n            ]],\n            dtype=np.float32)\n        y_exp = np.array(\n            [[\n                [[0], [6], [6], [6], [0]],    # YAPF_NO_FORMAT\n                [[6], [9], [9], [9], [0]],\n                [[6], [9], [9], [9], [0]],\n                [[6], [9], [0], [0], [0]],\n                [[0], [0], [0], [0], [0]]\n            ]],\n            dtype=np.float32)\n        self._test_sparse_conv2d_with_mask(mask, bsize, ksize, strides, padding, y_exp)\n\n    def test_sparse_conv2d_with_mask_same_even_block(self):\n        bsize = [1, 4, 4, 1]\n        ksize = [3, 3, 1, 1]\n        strides = [1, 1, 1, 1]\n        padding = \'SAME\'\n        mask = np.array(\n            [[\n                [0, 0, 0, 0, 0],    # YAPF_NO_FORMAT\n                [0, 0, 1, 0, 0],\n                [1, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0]\n            ]],\n            dtype=np.float32)\n        y_exp = np.array(\n            [[\n                [[4], [6], [6], [6], [0]],    # YAPF_NO_FORMAT\n                [[6], [9], [9], [9], [0]],\n                [[6], [9], [9], [9], [0]],\n                [[6], [9], [9], [9], [0]],\n                [[0], [0], [0], [0], [0]]\n            ]],\n            dtype=np.float32)\n        self._test_sparse_conv2d_with_mask(mask, bsize, ksize, strides, padding, y_exp)\n\n    def test_sparse_conv2d_with_mask_same_even_block_strides(self):\n        bsize = [1, 5, 5, 1]\n        ksize = [3, 3, 1, 1]\n        strides = [1, 2, 2, 1]\n        padding = \'SAME\'\n        mask = np.array(\n            [[\n                [0, 0, 0, 0, 0],    # YAPF_NO_FORMAT\n                [0, 0, 1, 0, 0],\n                [1, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0]\n            ]],\n            dtype=np.float32)\n        y_exp = np.array(\n            [[\n                [[4], [6], [0]],    # YAPF_NO_FORMAT\n                [[6], [9], [0]],\n                [[0], [0], [0]]\n            ]],\n            dtype=np.float32)\n        self._test_sparse_conv2d_with_mask(mask, bsize, ksize, strides, padding, y_exp)\n\n    def test_sparse_conv2d_with_large_block_strides(self):\n        bsize = [1, 5, 5, 1]\n        ksize = [2, 2, 1, 1]\n        strides = [1, 1, 1, 1]\n        padding = \'VALID\'\n        mask = np.array(\n            [[\n                [0, 0, 0, 0, 0, 1, 1, 1],    # YAPF_NO_FORMAT\n                [0, 0, 0, 0, 0, 1, 1, 1],\n                [1, 0, 0, 0, 0, 1, 1, 1],\n                [0, 0, 0, 0, 0, 1, 1, 1],\n                [0, 0, 0, 0, 0, 1, 1, 1],\n                [0, 0, 0, 0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 0, 0, 0]\n            ]],\n            dtype=np.float32)\n        y_exp = np.array(\n            [[\n                [[4], [4], [4], [4], [4], [4], [4]],    # YAPF_NO_FORMAT\n                [[4], [4], [4], [4], [4], [4], [4]],\n                [[4], [4], [4], [4], [4], [4], [4]],\n                [[4], [4], [4], [4], [4], [4], [4]],\n                [[0], [0], [0], [0], [4], [4], [4]],\n                [[0], [0], [0], [0], [4], [4], [4]],\n                [[0], [0], [0], [0], [4], [4], [4]]\n            ]],\n            dtype=np.float32)\n        self._test_sparse_conv2d_with_mask(mask, bsize, ksize, strides, padding, y_exp)\n\n    def _test_sparse_conv2d_correctness(self,\n                                        xsize,\n                                        bsize,\n                                        ksize,\n                                        strides,\n                                        padding,\n                                        dtype=tf.float32):\n        np.random.RandomState(0)\n        x = tf.constant(np.random.uniform(-1, 1, xsize), dtype=dtype)\n        w = tf.constant(np.random.uniform(-1, 1, ksize), dtype=dtype)\n        mask = np.random.uniform(-5, 1, xsize[:-1])\n        mask = tf.constant((mask > 0).astype(np.float32), dtype=dtype)\n        mask_out = tf.nn.max_pool(\n            tf.expand_dims(mask, 3), [1, ksize[0], ksize[1], 1], strides, padding)\n        y_exp = mask_conv2d(x, w, mask, strides, padding)\n        ind_blk = convert_mask_to_block_indices(mask, bsize, ksize, strides, padding, 0.)\n        y_act = sparse_conv2d(x, w, ind_blk, strides, padding)\n        with self.test_session():\n            y_exp_val = y_exp.eval()\n            y_act_val = y_act.eval()\n            mask_val = mask_out.eval()\n            mask_val = np.tile(mask_val, [1, 1, 1, ksize[-1]])\n            y_exp_val = y_exp_val * mask_val\n            y_act_val = y_act_val * mask_val\n            y_exp_val = y_exp_val[mask_val > 0.]\n            y_act_val = y_act_val[mask_val > 0.]\n            np.testing.assert_allclose(y_act_val, y_exp_val, rtol=1e-2, atol=0.1)\n\n    def test_sparse_conv2d_correctness(self):\n        xsize = [10, 28, 28, 10]\n        padding = \'SAME\'\n        test_func = self._test_sparse_conv2d_correctness\n        for kk in [1, 3, 5]:\n            for padding in [\'SAME\', \'VALID\']:\n                for ss in [1, 2]:\n                    _ksize = [kk, kk, xsize[-1], xsize[-1]]\n                    _bsize = [1, kk + 2, kk + 2, 1]\n                    _strides = [1, ss, ss, 1]\n                    test_func(xsize, _bsize, _ksize, _strides, padding)\n\n    def _test_sparse_conv2d_matmul_correctness(self, xsize, ksize, padding, dtype=tf.float32):\n        np.random.RandomState(0)\n        strides = [1, 1, 1, 1]    # Only [1, 1, 1, 1] is supported currently.\n        # Use block size to be the same with kernel size makes it the same with matrix multiplication.\n        bsize = [1, ksize[0], ksize[1], 1]\n        x = tf.constant(np.random.uniform(-1, 1, xsize), dtype=dtype)\n        w = tf.constant(np.random.uniform(-1, 1, ksize), dtype=dtype)\n        mask = np.random.uniform(-5, 1, xsize[:-1])\n        mask = tf.constant((mask > 0).astype(np.float32), dtype=dtype)\n        mask_out = tf.nn.max_pool(\n            tf.expand_dims(mask, 3), [1, ksize[0], ksize[1], 1], strides, padding)\n        y_exp = mask_conv2d(x, w, mask, strides, padding)\n        ind_blk = convert_mask_to_block_indices(mask, bsize, ksize, strides, padding, 0.)\n        y_act = sparse_conv2d_matmul(x, w, ind_blk, strides, padding)\n        with self.test_session():\n            y_exp_val = y_exp.eval()\n            y_act_val = y_act.eval()\n            mask_val = mask_out.eval()\n            mask_val = np.tile(mask_val, [1, 1, 1, ksize[-1]])\n            y_exp_val = y_exp_val * mask_val\n            y_act_val = y_act_val * mask_val\n            y_exp_val = y_exp_val[mask_val > 0.]\n            y_act_val = y_act_val[mask_val > 0.]\n            np.testing.assert_allclose(y_act_val, y_exp_val, rtol=1e-2, atol=0.1)\n\n    def test_sparse_conv2d_matmul_correctness(self):\n        xsize = [10, 28, 28, 10]\n        padding = \'SAME\'\n        test_func = self._test_sparse_conv2d_matmul_correctness\n        for kk in [1, 3, 5]:\n            for padding in [\'SAME\', \'VALID\']:\n                _ksize = [kk, kk, xsize[-1], xsize[-1]]\n                test_func(xsize, _ksize, padding)\n\n\nclass SparseConv2DCustomTests(tf.test.TestCase):\n    def _test_sparse_conv2d_custom_with_mask(self,\n                                             mask,\n                                             bsize,\n                                             ksize,\n                                             strides,\n                                             padding,\n                                             y_exp,\n                                             use_var=True,\n                                             transpose=False):\n        # Currently we don\'t care about VALID convolution.\n        assert padding == \'SAME\', \'We do not support VALID conv at the moment.\'\n        mask_ = tf.constant(mask)\n        blk_params = calc_block_params(\n            list(mask.shape) + [ksize[2]], bsize, ksize, strides, padding)\n        ind = convert_mask_to_indices_custom(mask_, blk_params, 0.)\n        xval = np.ones([1, mask.shape[1], mask.shape[2], 1], dtype=np.float32)\n        x = tf.constant(xval)\n        if use_var:\n            x = tf.Variable(x)\n        w = tf.constant(np.ones(ksize, dtype=np.float32))\n        y = sparse_conv2d_custom(\n            x, w, ind, blk_params, strides, use_var=use_var, transpose=transpose)\n        # Manually paste the input tensor in the expected output.\n        y_exp = (\n            y_exp == 0).astype(np.float32) * xval[:, :y_exp.shape[1], :y_exp.shape[2], :] + y_exp\n        with self.test_session() as sess:\n            if use_var:\n                sess.run(tf.variables_initializer([x]))\n            y_act = y.eval()\n            # print(\'===============\')\n            # print(\'Actual\')\n            # print(y_act.reshape([y_act.shape[1], y_act.shape[2]]))\n            # print(\'Expected\')\n            # print(y_exp.reshape([y_exp.shape[1], y_exp.shape[2]]))\n            # print(y_exp.shape)\n            self.assertEqual(y_act.size, y_exp.size)\n            np.testing.assert_array_equal(y_act.reshape(y_exp.shape), y_exp)\n\n    def test_sparse_conv2d_with_mask_same(self):\n        bsize = [1, 3, 3, 1]\n        ksize = [3, 3, 1, 1]\n        strides = [1, 1, 1, 1]\n        padding = \'SAME\'\n        mask = np.array(\n            [[\n                [0, 0, 0, 0, 0],    # YAPF_NO_FORMAT\n                [0, 0, 1, 0, 0],\n                [1, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0]\n            ]],\n            dtype=np.float32)\n        y_exp = np.array(\n            [[\n                [[0], [6], [6], [6], [0]],    # YAPF_NO_FORMAT\n                [[6], [9], [9], [9], [0]],\n                [[6], [9], [9], [9], [0]],\n                [[6], [9], [0], [0], [0]],\n                [[0], [0], [0], [0], [0]]\n            ]],\n            dtype=np.float32)\n        self._test_sparse_conv2d_custom_with_mask(mask, bsize, ksize, strides, padding, y_exp)\n\n\nif __name__ == \'__main__\':\n    tf.test.main()\n'"
sbnet_tensorflow/benchmark/sparse_gather_tests.py,13,"b'""""""\n\n   Sparse Blocks Network\n   Copyright (c) 2017, Uber Technologies, Inc.\n\n   Licensed under the Apache License, Version 2.0 (the ""License"");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an ""AS IS"" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n""""""\n\n\nfrom __future__ import division, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom sparse_conv_lib import convert_mask_to_block_indices, convert_mask_to_indices_custom\nfrom sparse_conv_lib import calc_block_params\nfrom sparse_conv_lib import _calc_block_strides, _pad_input\nfrom sparse_conv_lib import sbnet_module\n\n\ndef gather_tf(x, mask, bsize, ksize, strides, padding):\n    blk_indices = convert_mask_to_block_indices(mask, bsize, ksize, strides, padding, 0.0)\n    blk_shape = tf.shape(blk_indices)\n    blk_indices_ = tf.reshape(blk_indices, [-1, 3])\n\n    # Calculate the block strides.\n    bstrides = _calc_block_strides(blk_shape, ksize, strides)\n\n    # Pad input.\n    x_ = _pad_input(\n        x, ksize, strides, padding, bsize=[1, blk_shape[1], blk_shape[2], 1], bstrides=bstrides)\n\n    # Gather patches.\n    p = tf.gather_nd(x_, blk_indices_)\n\n    # Reshape patches.\n    p = tf.reshape(p, [blk_shape[0], blk_shape[1], blk_shape[2], -1])\n    return p, blk_indices\n\n\ndef gather_custom(x, mask, bsize, ksize, strides, padding):\n    x_shape = [int(ss) for ss in x.get_shape()]\n    block_params = calc_block_params(x_shape, bsize, ksize, strides, padding)\n    indices = convert_mask_to_indices_custom(mask, block_params, 0.0)\n    p = sbnet_module.sparse_gather(\n        x,\n        indices.bin_counts,\n        indices.active_block_indices,\n        dynamic_bsize=tf.constant(block_params.bsize, tf.int32),\n        dynamic_bstride=tf.constant(block_params.bstrides, tf.int32),\n        dynamic_boffset=tf.constant(block_params.boffset, tf.int32))\n    return p, indices\n\n\nclass SparseGatherTests(tf.test.TestCase):\n    def _test_sparse_gather(self, mask, x, w, bsize, ksize, strides, padding):\n        with tf.Session() as sess:\n            mask = tf.constant(mask)\n            x = tf.constant(x)\n            w = tf.constant(w)\n            a_tf, _ = gather_tf(x, mask, bsize, ksize, strides, padding)\n            a_custom, ind = gather_custom(x, mask, bsize, ksize, strides, padding)\n\n            a1, a2, active, num = sess.run(\n                [a_tf, a_custom, ind.active_block_indices, ind.bin_counts])\n            l1 = tuple([tuple(x) for x in a1.reshape(-1, 3).tolist()])\n            l2 = tuple([tuple(x) for x in a2.reshape(-1, 3).tolist()])\n            np.testing.assert_array_equal(set(l1), set(l2))\n\n    def test_basic(self):\n        bsize = [1, 3, 3, 1]\n        ksize = [3, 3, 1, 1]\n        strides = [1, 1, 1, 1]\n        padding = \'SAME\'\n        mask = np.array(\n            [[\n                [0, 0, 0, 0, 0],    # YAPF_NO_FORMAT\n                [0, 0, 1, 0, 0],\n                [1, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0]\n            ]],\n            dtype=np.float32)\n        # x = np.ones([1, mask.shape[1], mask.shape[2], 1], dtype=np.float32)\n        x = np.arange(mask.shape[1] * mask.shape[2]).reshape([1, mask.shape[1], mask.shape[2],\n                                                              1]).astype(np.float32)\n        w = np.ones(ksize, dtype=np.float32)\n        self._test_sparse_gather(mask, x, w, bsize, ksize, strides, padding)\n\n    def test_large(self):\n        bsize = [1, 5, 5, 1]\n        ksize = [2, 2, 1, 1]\n        strides = [1, 1, 1, 1]\n        padding = \'VALID\'\n        mask = np.array(\n            [[\n                [0, 0, 0, 0, 0, 1, 1, 1],    # YAPF_NO_FORMAT\n                [0, 0, 0, 0, 0, 1, 1, 1],\n                [1, 0, 0, 0, 0, 1, 1, 1],\n                [0, 0, 0, 0, 0, 1, 1, 1],\n                [0, 0, 0, 0, 0, 1, 1, 1],\n                [0, 0, 0, 0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 0, 0, 0]\n            ]],\n            dtype=np.float32)\n        # x = np.ones([1, mask.shape[1], mask.shape[2], 1], dtype=np.float32)\n        x = np.arange(mask.shape[1] * mask.shape[2]).reshape([1, mask.shape[1], mask.shape[2],\n                                                              1]).astype(np.float32)\n        w = np.ones(ksize, dtype=np.float32)\n        self._test_sparse_gather(mask, x, w, bsize, ksize, strides, padding)\n\n\nif __name__ == \'__main__\':\n    tf.test.main()\n'"
sbnet_tensorflow/benchmark/sparse_res_block_tests.py,24,"b'""""""\n\n   Sparse Blocks Network\n   Copyright (c) 2017, Uber Technologies, Inc.\n\n   Licensed under the Apache License, Version 2.0 (the ""License"");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an ""AS IS"" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n""""""\n\n#\n# Unit tests for sparse_conv.py\n#\nfrom __future__ import division, print_function, unicode_literals\n\nimport itertools\nimport numpy as np\nimport os\nimport tensorflow as tf\n\nfrom collections import namedtuple\n\nfrom sparse_conv_lib import _get_offset_array\nfrom sparse_conv_lib import calc_block_params\nfrom sparse_conv_lib import convert_mask_to_block_indices\nfrom sparse_conv_lib import convert_mask_to_indices_custom\nfrom sparse_conv_lib import mask_conv2d\nfrom sparse_conv_lib import sparse_conv2d\nfrom sparse_conv_lib import sparse_conv2d_custom\nfrom sparse_conv_lib import sparse_conv2d_matmul\nfrom sparse_conv_lib import upsample_indices\nfrom sparse_conv_lib import calc_block_params_res_block\nfrom sparse_conv_lib import sparse_res_block_bottleneck\nfrom sparse_conv_lib import res_block_bottleneck\nfrom tensorflow.python.ops.gradient_checker import compute_gradient\nfrom tensorflow.python.ops import gradient_checker\n\n# os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\n\ndef cosine_angle(v1, v2):\n    # eps = np.finfo(np.float32).eps\n    v1_norm = np.sqrt(np.dot(v1, v1))\n    v2_norm = np.sqrt(np.dot(v2, v2))\n    if v1_norm == 0.0 and v2_norm == 0.0:\n        return 1.0, 1.0, 1.0\n    if v1_norm == 0.0:\n        v1_norm = 1.0\n    if v2_norm == 0.0:\n        v2_norm = 1.0\n    cosine = np.dot(v1, v2) / v1_norm / v2_norm\n    cosine = min(max(cosine, -1.0), 1.0)\n    return cosine, v1_norm, v2_norm\n\n\ndef get_degree(radian):\n    return radian * 180 / np.pi\n\n\ndef compute_gradient_angle(x,\n                           x_shape,\n                           y,\n                           y_shape,\n                           x_init_value=None,\n                           delta=1e-3,\n                           init_targets=None,\n                           extra_feed_dict=None):\n    grad = compute_gradient(\n        x, x_shape, y, y_shape, x_init_value, delta, init_targets, extra_feed_dict=extra_feed_dict)\n    if isinstance(grad, tuple):\n        grad = [grad]\n    error = 0\n    for j_t, j_n in grad:\n        if j_t.size or j_n.size:    # Handle zero size tensors correctly\n            #error = np.maximum(error, np.fabs(j_t - j_n).max())\n            # print(j_t.shape, j_n.shape)\n            # xxx\'\n            # print(\'grad1\', j_t.ravel())\n            # print(\'grad2\', j_n.ravel())\n            cosine, norm1, norm2 = cosine_angle(j_t.ravel(), j_n.ravel())\n            # print(\'Jt\', j_t.ravel()[:10], \'norm\', norm1)\n            # print(\'Jn\', j_n.ravel()[:10], \'norm\', norm2)\n            error = get_degree(np.arccos(cosine))\n    return error\n\n\ndef compute_gradient_abs_error(x,\n                               x_shape,\n                               y,\n                               y_shape,\n                               x_init_value=None,\n                               delta=1e-3,\n                               init_targets=None,\n                               extra_feed_dict=None):\n    grad = compute_gradient(\n        x, x_shape, y, y_shape, x_init_value, delta, init_targets, extra_feed_dict=extra_feed_dict)\n    if isinstance(grad, tuple):\n        grad = [grad]\n    error = 0\n    atol = 1e-5\n    for j_t, j_n in grad:\n        if j_t.size or j_n.size:    # Handle zero size tensors correctly\n            idx = np.logical_and(j_t - j_n > atol, np.fabs(j_t) > atol)\n            # error = np.maximum(error, np.fabs((j_t[idx] - j_n[idx]) / j_t[idx]).max())\n            error = np.maximum(error, np.fabs(j_t - j_n).max())\n    return error\n\n\nclass ResBlockGradientTests(tf.test.TestCase):\n    def _test_resblock_gradients(self, xval, maskval, bsize, strides, padding, data_format=\'NHWC\'):\n        with tf.Graph().as_default() as g:\n            x = tf.constant(xval)\n            mask = tf.constant(maskval)\n            ch_in = xval.shape[3]\n            ch_out = xval.shape[3] // 4\n            ksize_list = [[1, 1, ch_in, ch_out], [3, 3, ch_out, ch_out], [1, 1, ch_out, ch_in]]\n            y = res_block_bottleneck(\n                x,\n                ksize_list,\n                strides,\n                is_training=True,\n                data_format=data_format,\n                w_project=None,\n                no_activation=False)\n            trainable_vars = tf.trainable_variables()\n            print(\'\')\n            print(\'-\' * 55)\n            print(\'Dense Residual\')\n            print(\'{:30s} {:>10s} {:>10s}\'.format(\'name\', \'grad angle\', \'abs err\'))\n            with self.test_session() as sess:\n                sess.run(tf.global_variables_initializer())\n                yval = y.eval()\n                err = compute_gradient_angle(x, xval.shape, y, yval.shape, x_init_value=xval)\n                err2 = compute_gradient_abs_error(x, xval.shape, y, yval.shape, x_init_value=xval)\n                print(\'{:30s} {:>10.3f} {:>10.3f}\'.format(\'x\', err, err2))\n\n                for name in [\n                        \'sub3/conv3/Conv2D:0\', \'sub3/relu3:0\', \'sub3/bn3/FusedBatchNorm:0\',\n                        \'sub2/conv2/Conv2D:0\', \'sub2/relu2:0\', \'sub2/bn2/FusedBatchNorm:0\',\n                        \'sub1/conv1/Conv2D:0\', \'sub1/relu1:0\', \'sub1/bn1/FusedBatchNorm:0\'\n                ]:\n                    act = g.get_tensor_by_name(name)\n                    actval = act.eval()\n                    err = compute_gradient_angle(\n                        act, actval.shape, y, yval.shape, x_init_value=actval)\n                    err2 = compute_gradient_abs_error(\n                        act, actval.shape, y, yval.shape, x_init_value=actval)\n                    print(\'{:30s} {:>10.3f} {:>10.3f}\'.format(name, err, err2))\n\n                # self.assertTrue(err < 0.001)\n                for vv in trainable_vars:\n                    vvval = vv.eval()\n                    err = compute_gradient_angle(vv, vvval.shape, y, yval.shape, x_init_value=vvval)\n                    err2 = compute_gradient_abs_error(\n                        vv, vvval.shape, y, yval.shape, x_init_value=vvval)\n                    print(\'{:30s} {:>10.3f} {:>10.3f}\'.format(vv.name, err, err2))\n                    # self.assertTrue(err < 0.001)\n\n    def test_resblock_gradients(self):\n        bsize = [1, 5, 5, 1]\n        ksize = [3, 3, 4, 4]\n        strides = [1, 1, 1, 1]\n        padding = \'SAME\'\n        rnd = np.random.RandomState(0)\n        mask = rnd.uniform(-1, 1, [3, 9, 9]).astype(np.float32)\n        mask = (mask > 0.5).astype(np.float32)\n        xval = rnd.uniform(-1, 1, [mask.shape[0], mask.shape[1], mask.shape[2],\n                                   ksize[2]]).astype(np.float32)\n        self._test_resblock_gradients(xval, mask, bsize, strides, padding, data_format=\'NHWC\')\n\n\nclass SparseResBlockGradientTests(tf.test.TestCase):\n    def _test_sparse_resblock_gradients(self,\n                                        xval,\n                                        maskval,\n                                        bsize,\n                                        strides,\n                                        padding,\n                                        data_format=\'NHWC\',\n                                        dynamic_size=False):\n        with tf.Graph().as_default() as g:\n            x = tf.constant(xval)\n            mask = tf.constant(maskval)\n            ch_in = xval.shape[3]\n            ch_out = xval.shape[3] // 4\n            ksize_list = [[1, 1, ch_in, ch_out], [3, 3, ch_out, ch_out], [1, 1, ch_out, ch_in]]\n            if dynamic_size:\n                blk_params = calc_block_params_res_block(\n                    tf.shape(xval), bsize, ksize_list, strides, padding)\n            else:\n                blk_params = calc_block_params_res_block(\n                    xval.shape, bsize, ksize_list, strides, padding)\n            ind = convert_mask_to_indices_custom(mask, blk_params, 0.)\n            ReduceMask = namedtuple(\'ReduceMask\', [\'active_block_indices\', \'bin_counts\'])\n            ind.active_block_indices.set_shape([27, 3])\n            ind.bin_counts.set_shape([1])\n            ind_var = tf.Variable(ind.active_block_indices, trainable=False)\n            bin_var = tf.Variable(ind.bin_counts, trainable=False)\n            ind_fixed = ReduceMask(active_block_indices=ind_var, bin_counts=bin_var)\n\n            y = sparse_res_block_bottleneck(\n                x,\n                ksize_list,\n                ind_fixed,\n                blk_params,\n                strides,\n                is_training=True,\n                data_format=data_format,\n                w_project=None,\n                no_activation=False,\n                use_var=False)\n            trainable_vars = tf.trainable_variables()\n            print(\'\')\n            print(\'-\' * 55)\n            print(\'Sparse Residual\')\n            print(\'{:30s} {:>10s} {:>10s}\'.format(\'name\', \'grad angle\', \'abs err\'))\n            with tf.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                yval = y.eval()\n                err = compute_gradient_angle(x, xval.shape, y, yval.shape, x_init_value=xval)\n                err2 = compute_gradient_abs_error(x, xval.shape, y, yval.shape, x_init_value=xval)\n                print(\'{:30s} {:>10.3f} {:>10.3f}\'.format(\'x\', err, err2))\n\n                #\'sub3/bn3/batchnorm/add_1:0\',\n                for name in [\n                        \'SparseScatter:0\', \'SparseGather:0\', \'sub3/bn3/FusedBatchNorm:0\',\n                        \'sub3/conv3/Conv2D:0\', \'sub3/relu3:0\', \'sub2/conv2/Conv2D:0\',\n                        \'sub2/relu2:0\', \'sub2/bn2/FusedBatchNorm:0\', \'sub1/conv1/Conv2D:0\',\n                        \'sub1/relu1:0\', \'sub1/bn1/FusedBatchNorm:0\'\n                ]:\n                    act = g.get_tensor_by_name(name)\n                    actval = act.eval()\n                    err = compute_gradient_angle(\n                        act, actval.shape, y, yval.shape, x_init_value=actval)\n                    err2 = compute_gradient_abs_error(\n                        act, actval.shape, y, yval.shape, x_init_value=actval)\n                    print(\'{:30s} {:>10.3f} {:>10.3f}\'.format(name, err, err2))\n\n                for vv in trainable_vars:\n                    vvval = vv.eval()\n                    err = compute_gradient_angle(vv, vvval.shape, y, yval.shape, x_init_value=vvval)\n                    err2 = compute_gradient_abs_error(\n                        vv, vvval.shape, y, yval.shape, x_init_value=vvval)\n                    print(\'{:30s} {:>10.3f} {:>10.3f}\'.format(vv.name, err, err2))\n\n    def test_sparse_resblock_gradients(self):\n        bsize = [1, 5, 5, 1]\n        ksize = [3, 3, 4, 4]\n        strides = [1, 1, 1, 1]\n        padding = \'SAME\'\n        rnd = np.random.RandomState(0)\n        mask = rnd.uniform(0, 1, [3, 9, 9]).astype(np.float32)\n        mask = (mask > 0.5).astype(np.float32)\n        xval = rnd.uniform(-0.1, 0.1, [mask.shape[0], mask.shape[1], mask.shape[2], ksize[2]\n                                       ]).astype(np.float32) + 1.0\n        for dynamic_size in [True, False]:\n            self._test_sparse_resblock_gradients(\n                xval, mask, bsize, strides, padding, data_format=\'NHWC\', dynamic_size=dynamic_size)\n\n\nclass SparseConv2DGradientTests(tf.test.TestCase):\n    def _test_sparse_conv2d_gradient(self, mask, bsize, ksize, strides, padding, transpose=False):\n        # Currently we don\'t care about VALID convolution.\n        assert padding == \'SAME\', \'We do not support VALID conv at the moment.\'\n        use_var = False\n        mask_ = tf.constant(mask)\n        blk_params = calc_block_params(\n            list(mask.shape) + [ksize[2]], bsize, ksize, strides, padding)\n        ind = convert_mask_to_indices_custom(mask_, blk_params, 0.)\n        ReduceMask = namedtuple(\'ReduceMask\', [\'active_block_indices\', \'bin_counts\'])\n        ind.active_block_indices.set_shape([27, 3])\n        ind.bin_counts.set_shape([1])\n        ind_var = tf.Variable(ind.active_block_indices, trainable=False)\n        bin_var = tf.Variable(ind.bin_counts, trainable=False)\n        ind_fixed = ReduceMask(active_block_indices=ind_var, bin_counts=bin_var)\n        rnd = np.random.RandomState(0)\n        batch_size = 1\n        xval = rnd.uniform(-0.1, 0.1, [mask.shape[0], mask.shape[1], mask.shape[2],\n                                       ksize[2]]).astype(np.float32)\n        x = tf.constant(xval)\n        wval = rnd.uniform(-1, 1, ksize).astype(np.float32)\n        w = tf.constant(wval)\n        y = sparse_conv2d_custom(\n            x, w, ind_fixed, blk_params, strides, use_var=use_var, transpose=transpose)\n        print(\'\')\n        print(\'-\' * 55)\n        print(\'Sparse Conv Layer\')\n        print(\'{:30s} {:>10s} {:>10s}\'.format(\'name\', \'grad angle\', \'abs err\'))\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            yval = y.eval()\n            err = compute_gradient_angle(x, xval.shape, y, yval.shape, x_init_value=xval)\n            err2 = compute_gradient_abs_error(x, xval.shape, y, yval.shape, x_init_value=xval)\n            print(\'{:30s} {:>10.3f} {:>10.3f}\'.format(\'x\', err, err2))\n\n            err = compute_gradient_angle(w, wval.shape, y, yval.shape, x_init_value=wval)\n            err = compute_gradient_abs_error(w, wval.shape, y, yval.shape, x_init_value=wval)\n            print(\'{:30s} {:>10.3f} {:>10.3f}\'.format(\'w\', err, err2))\n\n    def test_sparse_conv2d_gradient(self):\n        bsize = [1, 5, 5, 1]\n        ksize = [3, 3, 4, 4]\n        strides = [1, 1, 1, 1]\n        padding = \'SAME\'\n        rnd = np.random.RandomState(0)\n        mask = rnd.uniform(0, 1, [3, 9, 9]).astype(np.float32)\n        mask = (mask > 0.5).astype(np.float32)\n        self._test_sparse_conv2d_gradient(mask, bsize, ksize, strides, padding, transpose=False)\n\n\nif __name__ == \'__main__\':\n    tf.test.main()\n'"
sbnet_tensorflow/benchmark/sparse_scatter_tests.py,17,"b'""""""\n\n   Sparse Blocks Network\n   Copyright (c) 2017, Uber Technologies, Inc.\n\n   Licensed under the Apache License, Version 2.0 (the ""License"");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an ""AS IS"" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n""""""\n\n\nfrom __future__ import division, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom sparse_conv_lib import sbnet_module\nfrom sparse_conv_lib import calc_block_params, convert_mask_to_indices_custom\nfrom tf_conv_dims import calc_out_size_4d_np\nfrom sparse_gather_tests import gather_tf\n\n\ndef scatter_tf(q, blk_indices, out_shape):\n    q_shape = tf.shape(q)\n    blk_indices_crop = blk_indices[:, :q_shape[1], :q_shape[2], :]\n    y = tf.scatter_nd(blk_indices_crop, q, out_shape)\n    return y\n\n\ndef scatter_custom(q, indices, out_shape, bsize_out, boffset, bstride):\n    y = tf.zeros(out_shape, dtype=q.dtype)\n    y = sbnet_module.sparse_scatter(\n        q,\n        indices.bin_counts,\n        indices.active_block_indices,\n        y,\n        dynamic_bsize=tf.constant(bsize_out, tf.int32),\n        dynamic_bstride=tf.constant(bstride, tf.int32),\n        dynamic_boffset=tf.constant(boffset, tf.int32),\n        add=False)\n    return y\n\n\nclass SparseScatterTests(tf.test.TestCase):\n    def _test_sparse_scatter(self, mask, x, w, out_shape, bsize, ksize, strides, padding):\n        with tf.Session() as sess:\n            x = tf.constant(x)\n            w = tf.constant(w)\n            p, blk_indices = gather_tf(x, mask, bsize, ksize, strides, padding)\n            block_params = calc_block_params([int(ss) for ss in x.get_shape()], bsize, ksize,\n                                             strides, padding)\n            ind_custom = convert_mask_to_indices_custom(mask, block_params, 0.0)\n            p_custom = sbnet_module.sparse_gather(\n                x,\n                ind_custom.bin_counts,\n                ind_custom.active_block_indices,\n                dynamic_bsize=tf.constant(block_params.bsize, tf.int32),\n                dynamic_bstride=tf.constant(block_params.bstrides, tf.int32),\n                dynamic_boffset=tf.constant(block_params.boffset, tf.int32))\n            p_shape = [\n                int(x.get_shape()[0]), block_params.bsize[0], block_params.bsize[1],\n                int(x.get_shape()[3])\n            ]\n            q = tf.nn.conv2d(p, w, strides, \'VALID\')\n            q_custom = tf.nn.conv2d(p_custom, w, strides, \'VALID\')\n            y_tf = scatter_tf(q, blk_indices, out_shape)\n            q_shape = calc_out_size_4d_np(p_shape, ksize, strides, \'VALID\')\n            bsize_out = [q_shape[1], q_shape[2]]\n            boffset = [0, 0]\n            y_custom = scatter_custom(q_custom, ind_custom, out_shape, bsize_out, boffset,\n                                      block_params.bstrides)\n            p1, p2, q_val, y1, y2, active, num = sess.run([\n                p, p_custom, q, y_tf, y_custom, ind_custom.active_block_indices,\n                ind_custom.bin_counts\n            ])\n\n            # Make sure p\'s are the same.\n            l1 = tuple([tuple(x) for x in p1.reshape(-1, 3).tolist()])\n            l2 = tuple([tuple(x) for x in p2.reshape(-1, 3).tolist()])\n            np.testing.assert_array_equal(set(l1), set(l2))\n\n            # Check y\'s are the same.\n            np.testing.assert_array_equal(y1, y2)\n\n    def test_basic(self):\n        bsize = [1, 3, 3, 1]\n        ksize = [3, 3, 1, 1]\n        strides = [1, 1, 1, 1]\n        padding = \'SAME\'\n        mask = np.array(\n            [[\n                [0, 0, 0, 0, 0],    # YAPF_NO_FORMAT\n                [0, 0, 1, 0, 0],\n                [1, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0]\n            ]],\n            dtype=np.float32)\n        # x = np.ones([1, mask.shape[1], mask.shape[2], 1], dtype=np.float32)\n        x = np.arange(mask.shape[1] * mask.shape[2]).reshape([1, mask.shape[1], mask.shape[2],\n                                                              1]).astype(np.float32)\n        mask = tf.constant(mask)\n        w = np.ones(ksize, dtype=np.float32)\n        out_shape = [1, 5, 5, 1]\n        self._test_sparse_scatter(mask, x, w, out_shape, bsize, ksize, strides, padding)\n\n\nif __name__ == \'__main__\':\n    tf.test.main()\n'"
sbnet_tensorflow/benchmark/tf_conv_dims.py,19,"b'""""""\n\n   Sparse Blocks Network\n   Copyright (c) 2017, Uber Technologies, Inc.\n\n   Licensed under the Apache License, Version 2.0 (the ""License"");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an ""AS IS"" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n""""""\n\n#\n# Utility functions for computing convolution sizes.\n#\n# Implmented according to the doc:\n# https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/convolution\n#\n# For the \'SAME\' padding, the output height and width are computed as:\n# ```\n# out_height = ceil(float(in_height) / float(strides[1]))\n# out_width  = ceil(float(in_width) / float(strides[2]))\n# ```\n# and the padding on the top and left are computed as:\n# ```\n# pad_along_height = ((out_height - 1) * strides[1] + filter_height - in_height)\n# pad_along_width = ((out_width - 1) * strides[2] + filter_width - in_width)\n# pad_top = pad_along_height / 2\n# pad_left = pad_along_width / 2\n# ```\n#\n# For the \'VALID\' padding, the output height and width are computed as:\n# ```\n# out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\n# out_width  = ceil(float(in_width - filter_width + 1) / float(strides[2]))\n# ```\nfrom __future__ import division, print_function, unicode_literals\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef _check_strides(strides):\n    """"""\n    Validates strides parameters.\n\n    :param strides:  [list]    List of 4 int or a Tensor of 4 elements. Convolution stride size.\n\n    :returns:        [list]    List of 4 int or a Tensor of 4 elements, if inputs are valid.\n    """"""\n    if type(strides) == list or type(strides) == tuple:\n        assert len(strides) == 4, \'Expect `strides` a list/tuple of length 4.\'\n        assert strides[0] == strides[3] == 1, \'Expect first and last dimension of `strides` = 1.\'\n    elif type(strides) == tf.Tensor:\n        assert len(strides.get_shape()) == 1, \'Expect `strides` a rank 1 Tensor.\'\n        assert int(strides.get_shape()[0]) == 4, \'Expect `strides` to have 4 elements.\'\n        assert_strides = tf.assert_equal(\n            tf.stack([strides[0], strides[3]]),\n            tf.constant([1, 1], dtype=strides.dtype),\n            message=\'Expect first and last dimension of `strides` = 1.\')\n        with tf.control_dependencies([assert_strides]):\n            strides = tf.cast(strides, tf.int32)\n    else:\n        assert False, \'`strides` has unknown type: {}\'.format(type(strides))\n    return strides\n\n\ndef _check_ksize(ksize):\n    """"""\n    Validates ksize parameters.\n\n    :param ksize:    [list]    List of 4 int or a Tensor of 4 elements. Convolution kernel size.\n\n    :returns:        [list]    List of 4 int or a Tensor of 4 elements, if inputs are valid.\n    """"""\n    if type(ksize) == list or type(ksize) == tuple:\n        assert len(ksize) == 4, \'Expect `ksize` a list/tuple of length 4.\'\n    elif type(ksize) == tf.Tensor:\n        assert len(ksize.get_shape()) == 1, \'Expect `ksize` a rank 1 Tensor.\'\n        assert int(ksize.get_shape()[0]) == 4, \'Expect `ksize` to have 4 elements.\'\n        ksize = tf.cast(ksize, tf.int32)\n    else:\n        assert False, \'`ksize` has unknown type: {}\'.format(type(ksize))\n    return ksize\n\n\ndef calc_padding_4d(in_shape, ksize, strides, padding):\n    """"""\n    Calculates padding width on four dimensions: top, bottom, left, and right.\n\n    :param x:        [Tensor]  Input tensor.\n    :param ksize     [list]    List of 4 int or a Tensor of 4 elements. Convolution kernel size.\n    :param strides   [list]    List of 4 int or a Tensor of 4 elements. Convolution stride size.\n    :param padding   [list]    Padding method, `VALID` or `SAME`.\n\n    :return          [tuple]   Tuple of 4 int. Padding length on top, bottom, left, and right.\n    """"""\n    ksize = _check_ksize(ksize)\n    strides = _check_strides(strides)\n    if padding == \'VALID\':\n        return 0, 0, 0, 0\n    elif padding == \'SAME\':\n        if type(in_shape[1]) == int:\n            out_size_h = calc_out_size_1d_np(in_shape[1], ksize[0], strides[1], padding)\n            out_size_w = calc_out_size_1d_np(in_shape[2], ksize[1], strides[2], padding)\n        elif type(in_shape[1]) == tf.Tensor:\n            out_size_h = calc_out_size_1d(in_shape[1], ksize[0], strides[1], padding)\n            out_size_w = calc_out_size_1d(in_shape[2], ksize[1], strides[2], padding)\n        else:\n            raise ValueError(\'Unknown type \\""{}\\""\'.format(type(in_shape[1])))\n        pad_h = calc_padding_1d(in_shape[1], out_size_h, ksize[0], strides[1], padding)\n        pad_w = calc_padding_1d(in_shape[2], out_size_w, ksize[1], strides[2], padding)\n        if type(pad_h) == int:\n            pad_h0, pad_h1 = _div_padding_np(pad_h)\n            pad_w0, pad_w1 = _div_padding_np(pad_w)\n        elif type(pad_h) == tf.Tensor:\n            pad_h0, pad_h1 = _div_padding(pad_h)\n            pad_w0, pad_w1 = _div_padding(pad_w)\n        else:\n            raise ValueError(\'Unknown type \\""{}\\""\'.format(type(pad_h)))\n        return pad_h0, pad_h1, pad_w0, pad_w1\n    else:\n        raise ValueError(\'Unknown padding method \\""{}\\""\'.format(padding))\n\n\ndef calc_padding_1d(in_size, out_size, ksize, stride, padding):\n    """"""\n    Calculates padding width on one dimension.\n\n    :param in_size:  [Tensor]  Scalar. Input size.\n    :param out_size: [Tensor]  Scalar. Output size.\n    :param ksize:    [Tensor]  Scalar or int. Kernel size.\n    :param strides:  [Tensor]  Scalar or int. Stride size.\n    :param padding:  [string]  Padding method, `SAME` or `VALID`.\n\n    :returns:        [Tensor]  Scalar. Padding size.\n    """"""\n    if padding == \'VALID\':\n        return 0\n    elif padding == \'SAME\':\n        _pad = (out_size - 1) * stride + ksize - in_size\n        if type(_pad) == int:\n            return max(_pad, 0)\n        elif type(_pad) == tf.Tensor:\n            return tf.maximum(_pad, 0)\n        else:\n            raise ValueError(\'Unknown type \\""{}\\""\'.format(type(_pad)))\n    else:\n        raise ValueError(\'Unknown padding method \\""{}\\""\'.format(padding))\n\n\ndef _div_padding(pad_size):\n    """"""\n    Divides padding to two sides so that the features are centered.\n\n    :param pad_size: [Tensor]  Scalar. Padding size.\n\n    :return          [Tensor]  Scalar. First padding size.\n    :return          [Tensor]  Scalar. Second padding size.\n    """"""\n    return tf.cast(tf.floor(tf.to_float(pad_size) / 2.0), tf.int32), tf.cast(\n        tf.ceil(tf.to_float(pad_size) / 2.0), tf.int32)\n\n\ndef _div_padding_np(pad_size):\n    """"""\n    Divides padding to two sides so that the features are centered.\n\n    :param pad_size: [np.ndarray]  Scalar. Padding size.\n\n    :return          [int]  Scalar. First padding size.\n    :return          [int]  Scalar. Second padding size.\n    """"""\n    return int(np.floor(float(pad_size) / 2.0)), int(np.ceil(float(pad_size) / 2.0))\n\n\ndef calc_out_size_4d(in_shape, ksize, strides, padding):\n    """"""Calculates output shape (rank 4) of a 2D convolution operation.\n\n    :param in_shape: [list]    Input tensor shape.\n    :param ksize:    [list]    Kernel shape.\n    :param strides:  [list]    Strides list.\n    :param padding:  [string]  Padding method, `SAME` or `VALID`.\n\n    :return          [list]    Output tensor shape.\n    """"""\n    strides = _check_strides(strides)\n    ksize = _check_ksize(ksize)\n    return tf.stack([\n        in_shape[0],\n        calc_out_size_1d(in_shape[1], ksize[0], strides[1], padding),\n        calc_out_size_1d(in_shape[2], ksize[1], strides[2], padding), ksize[3]\n    ])\n\n\ndef calc_out_size_1d(in_size, ksize, stride, padding):\n    """"""\n    Calculates output size on one dimension.\n\n    :param in_size:  [int]     Input size.\n    :param ksize:    [int]     Kernel size.\n    :param stride:   [int]     Stride size.\n    :param pad:      [string]  Padding method, `SAME` or `VALID`.\n\n    :return          [int]     Output size.\n    """"""\n\n    if padding == \'VALID\':\n        return tf.cast(tf.ceil(tf.to_float(in_size - ksize + 1) / tf.to_float(stride)), tf.int32)\n    elif padding == \'SAME\':\n        return tf.cast(tf.ceil(tf.to_float(in_size) / tf.to_float(stride)), tf.int32)\n    else:\n        raise ValueError(\'Unknown padding method \\""{}\\""\'.format(padding))\n\n\ndef calc_out_size_1d_maxpool(in_size, ksize, stride, padding):\n    """"""\n    Calculates output size on one dimension.\n\n    :param in_size:  [int]     Input size.\n    :param ksize:    [int]     Kernel size.\n    :param stride:   [int]     Stride size.\n    :param pad:      [string]  Padding method, `SAME` or `VALID`.\n\n    :return          [int]     Output size.\n    """"""\n\n    if padding == \'VALID\':\n        return tf.cast(tf.ceil(tf.to_float(in_size - ksize + 1) / tf.to_float(stride)), tf.int32)\n    elif padding == \'SAME\':\n        return tf.cast(tf.ceil(tf.to_float(in_size) / tf.to_float(stride)), tf.int32)\n    else:\n        raise ValueError(\'Unknown padding method \\""{}\\""\'.format(padding))\n\n\ndef calc_out_size_4d_np(in_shape, ksize, strides, padding):\n    """"""Calculates output shape (rank 4) of a 2D convolution operation.\n\n    :param in_shape: [list]    Input tensor shape.\n    :param ksize:    [list]    Kernel shape.\n    :param strides:  [list]    Strides list.\n    :param padding:  [string]  Padding method, `SAME` or `VALID`.\n\n    :return          [list]    Output tensor shape.\n    """"""\n    strides = _check_strides(strides)\n    ksize = _check_ksize(ksize)\n    return [\n        in_shape[0],\n        calc_out_size_1d_np(in_shape[1], ksize[0], strides[1], padding),\n        calc_out_size_1d_np(in_shape[2], ksize[1], strides[2], padding), ksize[3]\n    ]\n\n\ndef calc_out_size_1d_np(in_size, ksize, stride, padding):\n    """"""\n    Calculates output size on one dimension.\n\n    :param in_size:  [int]     Input size.\n    :param ksize:    [int]     Kernel size.\n    :param stride:   [int]     Stride size.\n    :param pad:      [string]  Padding method, `SAME` or `VALID`.\n\n    :return          [int]     Output size.\n    """"""\n\n    if padding == \'VALID\':\n        return int(np.ceil(float(in_size - ksize + 1) / float(stride)))\n    elif padding == \'SAME\':\n        return int(np.ceil(float(in_size) / float(stride)))\n    else:\n        raise ValueError(\'Unknown padding method \\""{}\\""\'.format(padding))\n'"
sbnet_tensorflow/benchmark/tf_conv_dims_tests.py,29,"b'""""""\n\n   Sparse Blocks Network\n   Copyright (c) 2017, Uber Technologies, Inc.\n\n   Licensed under the Apache License, Version 2.0 (the ""License"");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an ""AS IS"" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n""""""\n\n#\n# Unit tests for tf_conv_dims.py\n#\nfrom __future__ import division, print_function, unicode_literals\n\nimport numpy as np\nimport tensorflow as tf\n\n\nclass CalcOutSizeTests(tf.test.TestCase):\n    def _test_calc_out_size(self, in_size, ksize, stride, padding):\n        from tf_conv_dims import calc_out_size_1d\n        x = tf.ones([1, in_size, in_size, 1], dtype=tf.float32)\n        w = tf.ones([ksize, ksize, 1, 1], dtype=tf.float32)\n        y = tf.nn.conv2d(x, w, [1, stride, stride, 1], padding)\n        with self.test_session():\n            out_size_exp = tf.shape(y)[1].eval()\n            out_size_act = calc_out_size_1d(in_size, ksize, stride, padding).eval()\n        self.assertEqual(out_size_act, out_size_exp)\n\n    def test_calc_out_size(self):\n        for insize in [6, 7, 10, 11]:\n            for ksize in [1, 2, 3, 4, 7]:\n                for stride in [1, 2, 3]:\n                    for padding in [\'SAME\', \'VALID\']:\n                        if ksize <= insize:\n                            self._test_calc_out_size(insize, ksize, stride, padding)\n\n\nclass CalcOutSizeDeconvTests(tf.test.TestCase):\n    def _test_calc_in_size(self, out_size, ksize, stride, padding):\n        from tf_conv_dims import calc_out_size_1d_np\n        w = tf.ones([ksize, ksize, 1, 1], dtype=tf.float32)\n        in_size = calc_out_size_1d_np(out_size, ksize, 1 / float(stride), padding)\n        x = tf.ones([1, in_size, in_size, 1], dtype=tf.float32)\n        y = tf.nn.conv2d(x, w, [1, stride, stride, 1], padding)\n        with self.test_session():\n            out_size_exp = tf.shape(y)[1].eval()\n        self.assertEqual(out_size, out_size_exp)\n\n    def test_calc_out_size(self):\n        for insize in [6, 7, 10, 11]:\n            for ksize in [1, 2, 3, 4, 7]:\n                for stride in [1, 2, 3]:\n                    # Fractional stride methods only works for SAME.\n                    # Instead of calc out_size, maybe possible to have another function to calc\n                    # in_size\n                    for padding in [\'SAME\']:\n                        if ksize <= insize:\n                            self._test_calc_in_size(insize, ksize, stride, padding)\n\n\nclass CalcPaddingTests(tf.test.TestCase):\n    def test_calc_padding(self):\n        from tf_conv_dims import calc_padding_4d\n        x = tf.zeros([1, 5, 6, 1])\n        p_exp = np.array([0, 1, 1, 1], dtype=np.int32)\n        p = calc_padding_4d(tf.shape(x), [2, 3, 1, 1], [1, 1, 1, 1], \'SAME\')\n        p = tf.stack(p)\n        with self.test_session():\n            p_act = p.eval()\n            np.testing.assert_array_equal(p_act, p_exp)\n\n    def test_calc_padding_err_ksize_list(self):\n        from tf_conv_dims import calc_padding_4d\n        x = tf.zeros([1, 5, 6, 1])\n        err_raised = False\n        try:\n            calc_padding_4d(tf.shape(x), [2, 3, 1, 1, 1], [2, 1, 1, 1], \'SAME\')\n        except AssertionError as e:\n            self.assertEqual(e.args[0], \'Expect `ksize` a list/tuple of length 4.\')\n            err_raised = True\n        self.assertTrue(err_raised)\n\n    def test_calc_padding_err_strides_list(self):\n        from tf_conv_dims import calc_padding_4d\n        x = tf.zeros([1, 5, 6, 1])\n        err_raised = False\n        try:\n            calc_padding_4d(tf.shape(x), [2, 3, 1, 1], [2, 1, 1, 1], \'SAME\')\n        except AssertionError as e:\n            self.assertEqual(e.args[0], \'Expect first and last dimension of `strides` = 1.\')\n            err_raised = True\n        self.assertTrue(err_raised)\n\n    def test_calc_padding_err_strides_tensor(self):\n        from tf_conv_dims import calc_padding_4d\n        x = tf.zeros([1, 5, 6, 1])\n        err_raised = False\n        p = calc_padding_4d(tf.shape(x), [2, 3, 1, 1], tf.constant(np.array([2, 1, 1, 1])), \'SAME\')\n        p = tf.stack(p)\n        with self.test_session():\n            try:\n                p.eval()\n            except tf.errors.InvalidArgumentError as e:\n                self.assertTrue(\n                    e.message.startswith(\n                        \'assertion failed: [Expect first and last dimension of `strides` = 1.]\'))\n                err_raised = True\n\n        self.assertTrue(err_raised)\n\n    def test_calc_padding_valid(self):\n        from tf_conv_dims import calc_padding_4d\n        x = tf.zeros([1, 5, 5, 1])\n        p_exp = np.array([0, 0, 0, 0], dtype=np.int32)\n        p = calc_padding_4d(tf.shape(x), [2, 3, 1, 1], [1, 1, 1, 1], \'VALID\')\n        p = tf.stack(p)\n        with self.test_session():\n            p_act = p.eval()\n            np.testing.assert_array_equal(p_act, p_exp)\n\n    def test_calc_padding_stride(self):\n        from tf_conv_dims import calc_padding_4d\n        x = tf.zeros([1, 5, 6, 1])\n        p_exp = np.array([0, 1, 0, 1], dtype=np.int32)\n        p = calc_padding_4d(tf.shape(x), [2, 3, 1, 1], [1, 2, 2, 1], \'SAME\')\n        p = tf.stack(p)\n        with self.test_session():\n            p_act = p.eval()\n            np.testing.assert_array_equal(p_act, p_exp)\n\n\nif __name__ == \'__main__\':\n    tf.test.main()\n'"
sbnet_tensorflow/benchmark_submanifold/benchmark_topleft.py,0,"b'""""""\n\n   Sparse Blocks Network\n   Copyright (c) 2017, Uber Technologies, Inc.\n\n   Licensed under the Apache License, Version 2.0 (the ""License"");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an ""AS IS"" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n""""""\n\n\n#\n# Benchmark submanifold top left mask performance.\n#\n# Usage:\n# python benchmark_topleft.py --test [conv | res] --arch [resnet-50 | resnet-v2]\n#\n# Flags:\n# --test: Which benchmark, a convolutional layer or a residual block.\n# --arch: Which architecture, original ResNet-50  (high channel) or modified ResNet-v2 (low channel).\n#\nfrom __future__ import division, print_function\n\nimport torch\ntorch.backends.cudnn.enabled = True\ntorch.backends.cudnn.benchmark = True\n\nfrom argparse import ArgumentParser\nfrom collections import namedtuple\n\nfrom perf import run_dense, run_sparse, generate_top_left_mask\n\n# Import from our benchmark folder.\nfrom benchmark_configs import INPUT_SIZE_DICT, SPARSITY_LIST\nfrom benchmark_utils import append_result, create_result, get_out_filename\n\nN_RUN_CONV = 15\nN_RUN_RES = 15\n\nperf_result_fields = [\'H\', \'W\', \'C\', \'K\', \'sparsity\', \'dense_time\', \'sparse_time\', \'speedup\']\nPerfResult = namedtuple(\'PerfResult\', perf_result_fields)\n\n\ndef main():\n    out_file = get_out_filename(prefix=\'submanifold_topleft_{}\'.format(args.test))\n    print(\'Writing output to {}\'.format(out_file))\n    create_result(out_file, perf_result_fields)\n    for sz in INPUT_SIZE_LIST:\n        for sparsity in SPARSITY_LIST:\n            if args.test == \'conv\':\n                x = generate_top_left_mask([1, sz[3], sz[0], sz[1]], sparsity)\n            elif args.test == \'res\':\n                x = generate_top_left_mask([1, sz[2], sz[0], sz[1]], sparsity)\n            img_tensor = torch.FloatTensor(x)\n            stream = torch.cuda.current_stream()\n            nchw = img_tensor.size()\n            if args.test == ""conv"":\n                n_run = N_RUN_CONV\n                res_block = False\n            else:\n                n_run = N_RUN_RES\n                res_block = True\n            dense_ms = run_dense(\n                img_tensor, sz[3], res_block=res_block, n_warmup=n_run, n_run=n_run)\n            sparse_ms = run_sparse(\n                img_tensor, sz[3], res_block=res_block, n_warmup=n_run, n_run=n_run)\n            result = PerfResult(\n                H=sz[0],\n                W=sz[1],\n                C=sz[2],\n                K=sz[3],\n                sparsity=sparsity,\n                dense_time=dense_ms,\n                sparse_time=sparse_ms,\n                speedup=dense_ms / sparse_ms)\n            append_result(out_file, result)\n\n\nif __name__ == \'__main__\':\n    parser = ArgumentParser(\n        description=\'Submanifold convolution and resnet blocks benchmarking script\')\n    parser.add_argument(\'--test\', type=str, default=\'conv\', choices=set((\'conv\', \'res\')))\n    parser.add_argument(\n        \'--arch\', type=str, default=\'resnet-v2\', choices=set((\'resnet-50\', \'resnet-v2\')))\n    args = parser.parse_args()\n    print(\'Benchmarking with --test=%s --arch=%s\' % (args.test, args.arch))\n    INPUT_SIZE_LIST = INPUT_SIZE_DICT[args.arch]\n    main()\n'"
sbnet_tensorflow/benchmark_submanifold/perf.py,0,"b'""""""\n\n   Sparse Blocks Network\n   Copyright (c) 2017, Uber Technologies, Inc.\n\n   Licensed under the Apache License, Version 2.0 (the ""License"");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an ""AS IS"" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n""""""\n\n\nfrom __future__ import division, print_function\n\nimport torch\ntorch.backends.cudnn.enabled = True\ntorch.backends.cudnn.benchmark = True\n\nimport torchnet\nimport torch.nn as nn\nimport numpy as np\nimport sparseconvnet.legacy as scn\nimport time\nfrom PIL import Image\nfrom torchvision import models, transforms\nfrom torch.autograd import Variable\nimport io\nimport requests\nimport os\nimport six\nimport sys\n\nsys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), ""../benchmark/""))\nfrom cu_prof import cu_prof_start, cu_prof_stop\n\n\ndef submanifold_single_conv(inchan, outchan):\n    dtype = \'torch.cuda.FloatTensor\'\n    model = scn.Sequential()\n    model.add(scn.ValidConvolution(2, inchan, outchan, 3, False))\n    model.type(dtype)\n    model.cuda()\n    return model\n\n\ndef regular_single_conv(inout_chan, bottleneck_chan):\n    dtype = \'torch.cuda.FloatTensor\'\n    model = nn.Sequential(nn.Conv2d(inout_chan, bottleneck_chan, 3, padding=1, bias=False))\n    model.type(dtype)\n    model.cuda()\n    return model\n\n\ndef submanifold_resnet_block(inout_chan, bottleneck_chan):\n    # relu is fused in this implementation\n    dtype = \'torch.cuda.FloatTensor\'\n\n    bn1 = scn.BatchNormReLU(inout_chan)\n    bn2 = scn.BatchNormReLU(bottleneck_chan)\n    bn3 = scn.BatchNormReLU(bottleneck_chan)\n    bn1.train = False\n    bn2.train = False\n    bn3.train = False\n    model = scn.Sequential() \\\n        .add(bn1) \\\n        .add(scn.ValidConvolution(2, inout_chan, bottleneck_chan, 3, False)) \\\n        .add(bn2) \\\n        .add(scn.ValidConvolution(2, bottleneck_chan, bottleneck_chan, 3, False)) \\\n        .add(bn3) \\\n        .add(scn.ValidConvolution(2, bottleneck_chan, inout_chan, 3, False))\n    model.type(dtype)\n    model.cuda()\n\n    return model\n\n\ndef regular_resnet_block(inout_chan, bottleneck_chan):\n    dtype = \'torch.cuda.FloatTensor\'\n    model = nn.Sequential( \\\n        nn.BatchNorm2d(inout_chan), \\\n        nn.ReLU(), \\\n        nn.Conv2d(inout_chan, bottleneck_chan, 3, padding=1, bias=False), \\\n        nn.BatchNorm2d(bottleneck_chan), \\\n        nn.ReLU(), \\\n        nn.Conv2d(bottleneck_chan, bottleneck_chan, 3, padding=1, bias=False), \\\n        nn.BatchNorm2d(bottleneck_chan), \\\n        nn.ReLU(), \\\n        nn.Conv2d(bottleneck_chan, inout_chan, 3, padding=1, bias=False))\n    model.type(dtype)\n    model.cuda()\n    return model\n\n\ndef generate_top_left_mask(xsize, sparsity):\n    # xsize is NCHW\n    density = 1.0 - sparsity\n    edge_ratio = np.sqrt(density)\n    height = int(np.ceil(edge_ratio * xsize[2]))\n    width = int(np.ceil(edge_ratio * xsize[3]))\n    x = np.zeros(xsize, dtype=np.float32)\n    x[:, :, :height, :width] = 1.0\n    return x\n\n\ndef run_dense(img_tensor, out_chan, res_block=False, n_warmup=15, n_run=15):\n    stream = torch.cuda.current_stream()\n    img_tensor_cu = img_tensor.cuda()\n    input_size = img_tensor_cu.shape\n    in_chan = int(input_size[1])\n    if not res_block:\n        denseModel = regular_single_conv(in_chan, out_chan)\n    else:\n        denseModel = regular_resnet_block(in_chan, out_chan)\n\n    # warmup\n    var = Variable(img_tensor_cu)\n    for i in six.moves.xrange(n_warmup):\n        out = denseModel.forward(var)\n\n    cu_prof_start()\n    starte = torch.cuda.Event(enable_timing=True)\n    ende = torch.cuda.Event(enable_timing=True)\n    stream.record_event(starte)\n    for i in six.moves.xrange(n_run):\n        out = denseModel.forward(var)\n    stream.record_event(ende)\n    ende.synchronize()\n    evt_dt = starte.elapsed_time(ende)\n    cu_prof_stop()\n    outDense = out\n    dense_ms = evt_dt / n_run\n    return dense_ms\n\n\ndef run_sparse(img_tensor, out_chan, res_block=False, n_warmup=15, n_run=15):\n    stream = torch.cuda.current_stream()\n    input_size = img_tensor.shape\n    batch = input_size[0]\n    in_chan = input_size[1]\n    h = input_size[2]\n    w = input_size[3]\n    if not res_block:\n        sparseModel = submanifold_single_conv(in_chan, out_chan)\n    else:\n        sparseModel = submanifold_resnet_block(in_chan, out_chan)\n    sparse_batch = scn.InputBatch(2, torch.LongTensor([h, w]))\n    sparse_batch.addSample()\n    count = 0\n    for y, x in np.ndindex((h, w)):\n        val = img_tensor[0, :, y, x]\n        if val[0] > 0.5:\n            location = torch.LongTensor([y, x])\n            featureVector = torch.FloatTensor(val)\n            sparse_batch.setLocation(location, featureVector, 0)\n            count += 1\n\n    # warmup\n    inp = sparse_batch.cuda()\n    for i in range(n_warmup):\n        out = sparseModel.forward(inp)\n\n    cu_prof_start()\n\n    # Use CUDA events for accurate timing.\n    starte = torch.cuda.Event(enable_timing=True)\n    ende = torch.cuda.Event(enable_timing=True)\n    stream.record_event(starte)\n    for i in range(n_run):\n        out = sparseModel.forward(inp)\n    stream.record_event(ende)\n    ende.synchronize()\n    evt_dt = starte.elapsed_time(ende)\n    cu_prof_stop()\n    sparse_ms = evt_dt / n_run\n    return sparse_ms\n'"
sbnet_tensorflow/sbnet_ops/sample/sample.py,5,"b'""""""\n\n   Sparse Blocks Network\n   Copyright (c) 2017, Uber Technologies, Inc.\n\n   Licensed under the Apache License, Version 2.0 (the ""License"");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an ""AS IS"" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n""""""\n\n#\n# A minimal sample implementing a single sparse convolution layer with synthetic data using SBNet primitives.\n#\n\nimport numpy as np\nimport tensorflow as tf\n\nsbnet_module = tf.load_op_library(\'../libsbnet.so\')\n\ndef divup(a, b):\n    return (a+b-1) // b\n\n# Specify input tensor dimensions and block-sparsity parameters\nbatch = 4\nhw = 256\nchannels = 64\nblockSize = [16, 16]\nblockStride = [14, 14]\nblockOffset = [0, 0]\nblockCount = [divup(hw, blockStride[0]), divup(hw, blockStride[1])]\n\n# build kwargs to simplify op calls\ninBlockParams = { ""bsize"": blockSize, ""boffset"": blockOffset, ""bstride"": blockStride }\noutBlockParams = { ""bsize"": [blockSize[0]-2, blockSize[1]-2], ""boffset"": blockOffset, ""bstride"": blockStride }\n\n# create a random mask representing attention/a priori sparsity\n# threshold the mask to a specified percentile sparsity\nmask = np.random.randn(batch, blockCount[0], blockCount[1], channels).astype(np.float32)\nthreshold = np.percentile(mask, 90)\nsparseMask = np.greater(mask, threshold).astype(np.float32)\n\n# upsample the mask to full resolution\nupsampledMask = sparseMask.repeat(blockStride[0], axis=1).repeat(blockStride[1], axis=2)\n\n# create a random input tensor\nx = tf.constant( np.random.randn(batch, hw, hw, channels).astype(np.float32) )\n\n# create a random weight tensor\nw = tf.constant( np.random.randn(3, 3, channels, channels).astype(np.float32) )\n\n# reduce the mask to indices by using a fused pooling+indexing operation\nindices = sbnet_module.reduce_mask(mask, blockCount, tol=0.5, **inBlockParams)\n\n# stack active overlapping tiles to batch dimension\nblockStack = sbnet_module.sparse_gather(\n    x, indices.bin_counts, indices.active_block_indices, transpose=True, **inBlockParams)\n\n# perform dense convolution on a sparse stack of tiles\nconvBlocks = tf.nn.conv2d(\n    blockStack, w, strides=[1, 1, 1, 1], padding=\'VALID\', data_format=\'NCHW\')\n\n# write/scatter the tiles back on top of original tensor\n# note that the output tensor is reduced by 1 on each side due to \'VALID\' convolution\nvalidX = x[:, 1:hw-1, 1:hw-1, :]\ny = sbnet_module.sparse_scatter(\n    convBlocks, indices.bin_counts, indices.active_block_indices,\n    validX, transpose=True, add=False, atomic=False, **outBlockParams)\n\nsess = tf.Session()\ny_output, = sess.run([y])\n\n\n'"
sbnet_tensorflow/sbnet_ops/tests/test_sparse_blocks.py,35,"b'""""""\n\n   Sparse Blocks Network\n   Copyright (c) 2017, Uber Technologies, Inc.\n\n   Licensed under the Apache License, Version 2.0 (the ""License"");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an ""AS IS"" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n""""""\n\n\nimport os\nimport numpy as np\nimport unittest\n\nfrom math import floor, ceil\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import gradients_impl\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\' \nsbnet_module = tf.load_op_library(\'../libsbnet.so\')\n\n\n# Python implementation of gradients\n@ops.RegisterGradient(""SparseGather"")\ndef _sparse_gather_grad(op, grad):\n    # x is shaped like full tensor [NHWC]\n    # grad is shaped as gathered blocks [Nblocks*BH*BW*C]\n    x = op.inputs[0]\n    binCounts = op.inputs[1]\n    activeBlockIndices = op.inputs[2]\n    bsize = op.inputs[3]\n    bstride = op.inputs[4]\n    boffset = op.inputs[5]\n    transpose = op.get_attr(""transpose"")\n\n    # if scatter is overlapping then gradient should still work\n    # because we are overwriting the same values\n    # compute dOutput/dx\n    result = sbnet_module.sparse_scatter(\n        grad,\n        binCounts,\n        activeBlockIndices,\n        tf.zeros_like(x),    # output base tensor to add on top of\n        dynamic_bsize=bsize,\n        dynamic_bstride=bstride,\n        dynamic_boffset=boffset,\n        add=True,\n        transpose=transpose,\n        atomic=True)\n\n    return [result, None, None, None, None, None]    # no gradients wrt indices or block params\n\n\n@ops.RegisterGradient(""SparseScatter"")\ndef _sparse_scatter_grad(op, grad):\n    # x is shaped like blocked tensor of gathered blocks [Nblocks*BH*BW*C]\n    # grad is shaped as output tensor [NHWC]\n    blocksX = op.inputs[0]\n    binCounts = op.inputs[1]\n    activeBlockIndices = op.inputs[2]\n    ybase = op.inputs[3]\n    bsize = op.inputs[4]\n    bstride = op.inputs[5]\n    boffset = op.inputs[6]\n    doAdd = op.get_attr(""add"")\n\n    dout_dx = sbnet_module.sparse_gather(\n        grad, binCounts, activeBlockIndices, dynamic_bsize=bsize, dynamic_bstride=bstride, dynamic_boffset=boffset)\n\n    # return a list of gradients of output with respect to each input\n    if not doAdd:\n        # scatter blocks of zeroes over a base tensor of ones to compute a stamp-out gradient mask for dy_dybase\n        stamp_out_blocks = sbnet_module.sparse_scatter(\n            tf.zeros_like(blocksX),\n            binCounts,\n            activeBlockIndices,\n            tf.ones_like(grad),\n            dynamic_bsize=bsize,\n            dynamic_bstride=bstride,\n            dynamic_boffset=boffset,\n            add=False)\n        dy_dybase = grad * stamp_out_blocks\n        return [dout_dx, None, None, dy_dybase, None, None, None]\n    else:\n        # d(x+ybase)/dybase = 1, so just pass back grad as dout_dybase\n        return [dout_dx, None, None, grad, None, None, None]\n\n\ndef calcBlockCount1d(WW, SS, VV, BOFFSW):\n    assert (WW >= SS)\n    assert (SS >= 1)\n    assert (VV >= 1)\n    assert (BOFFSW < WW)\n    #k = 0\n    #while BOFFSW+k*VV+SS <= WW:\n    #while k*VV <= WW-BOFFSW-SS:\n    #    k += 1\n    #return k\n    pixelsOfLastBlock = 1  # set to SS to fully fit the last block\n    return 1 + (WW - BOFFSW - pixelsOfLastBlock) // VV\n\n\ndef calcBlockCounts(HH, WW, RR, SS, UU, VV, BOFFSH, BOFFSW):\n    # HH, WW = image sizes\n    # RR, SS = block sizes\n    # UU, VV = block strides\n    # BOFFSH,W = block offsets (where block 0,0 starts)\n    # Computes block counts to cover HH fully with padding if necessary, given offset, block size and block stride\n    return calcBlockCount1d(HH, RR, UU, BOFFSH), calcBlockCount1d(WW, SS, VV, BOFFSW)\n\n\ndef generateInputs(inputSize, RS, UV, BOFFS, N):\n    np.random.seed(0)\n    NN, HH, WW, CC = N, inputSize[0], inputSize[1], inputSize[2]\n    RR, SS = RS[0], RS[1]    # block sizes\n    UU, VV = UV[0], UV[1]    # block strides\n    BCH, BCW = calcBlockCounts(HH, WW, RR, SS, UU, VV, BOFFS[0], BOFFS[1])\n    mask = np.random.randn(NN, HH, WW, 1).astype(np.float32)    # mask is NHW1\n    x = np.random.randn(NN, HH, WW, CC).astype(np.float32)\n    numBins = 1    # number of bins for block counter\n    return NN, HH, WW, CC, RR, SS, UU, VV, BCH, BCW, numBins, mask, x\n\n\nconfig = tf.ConfigProto(log_device_placement=False, allow_soft_placement=False)\nconfig.graph_options.optimizer_options.opt_level = -1\n\n\nclass TestSparseConvolutions(unittest.TestCase):\n    def runTestForOneSize(self,\n            layerSize,\n            RS,\n            UV,\n            BOFFS=[-1, -2],\n            checkGrads = False,\n            sparsity = 70,\n            do_add = False,\n            N = 1,\n            test_var = False,\n            avgpool = False,\n            use_atomics = False):\n\n        tf.logging.set_verbosity(tf.logging.ERROR)\n        print(""============== Testing batch, layerSize = "", N, layerSize, ""========="")\n        results = [[], []]\n        NN, HH, WW, CC, RR, SS, UU, VV, BCH, BCW, numBins, mask, x \\\n            = generateInputs(layerSize, RS, UV, BOFFS, N)\n\n        # x doesn\'t need to be transposed, it\'s expected to be in NHWC\n        offset = 10.0\n        for transpose in [False, True]:\n            for devStr in [""/gpu:0"", ""/cpu:0""]:\n                isGpu = (devStr == ""/gpu:0"")\n                tf.reset_default_graph()\n                with tf.Session(config=config) as sess, tf.device(devStr):\n                    if test_var:\n                        x1000 = tf.Variable(x * 0.0 + offset, tf.float32)    # was: convert_to_tensor\n                        sess.run(tf.global_variables_initializer())    # initialize x1000\n                    #print(mask)\n                    tol = np.percentile(x, sparsity)\n                    if sparsity == 100:\n                       tol = 1e32 # make sure at 100% sparsity we get zero blocks\n                    a = tf.constant(mask, dtype=tf.float32)\n                    print(""-------------- BCNT="", BCH, BCW)\n                    print(""-------------- BSZ="", RR, SS)\n                    print(""-------------- BSTR="", UU, VV)\n                    print(""-------------- BOFFS="", BOFFS[0], BOFFS[1])\n                    if 1:\n                        b = sbnet_module.reduce_mask(\n                            mask=a,\n                            dynamic_bcount=tf.constant([BCH, BCW], dtype=tf.int32),\n                            dynamic_bsize=tf.constant([int(RR), int(SS)], dtype=tf.int32),\n                            dynamic_bstride=tf.constant([int(UU), int(VV)], dtype=tf.int32),\n                            dynamic_boffset=tf.constant([int(BOFFS[0]), int(BOFFS[1])], dtype=tf.int32),\n                            tol=tol,\n                            avgpool=avgpool)\n\n                        # decouple indeterminstic portion into a separate subgraph for grad checker consistency\n                        py_bin_counts, py_active_block_indices = sess.run(\n                            [b.bin_counts, b.active_block_indices])\n                        #print "">>>>>>>>>>>>>>>>>>>> devStr="", devStr\n                        #print ""PY_BINC="", py_bin_counts\n                        #print ""PY_ABI="", py_active_block_indices\n                        #print ""++++ i="", i\n\n                    tf_bin_counts = tf.constant(py_bin_counts)\n                    tf_active_block_indices = tf.constant(py_active_block_indices)\n                    #bin_counts = tfx_print(b.bin_counts, ""bin_counts"")\n\n                    tf_x = tf.convert_to_tensor(x, tf.float32)\n                    if isGpu:\n                        with tf.control_dependencies([tf_x]):\n                            dt0 = sbnet_module.cuda_timer_start()\n                        with tf.control_dependencies([dt0]):\n                            tf_x = tf.identity(tf_x)\n                    blockStack = sbnet_module.sparse_gather(\n                        tf_x,\n                        tf_bin_counts,\n                        tf_active_block_indices,\n                        dynamic_bsize=tf.constant([RR, SS], dtype=tf.int32),\n                        dynamic_bstride=tf.constant([UU, VV], dtype=tf.int32),\n                        dynamic_boffset=tf.constant([BOFFS[0], BOFFS[1]], dtype=tf.int32),\n                        transpose=transpose)\n                    if test_var:\n                        y1000 = sbnet_module.sparse_scatter_var(\n                            blockStack,\n                            tf_bin_counts,\n                            tf_active_block_indices,\n                            x1000, # base variable to copy to output and overwrite on top of\n                            dynamic_bsize=tf.constant([RR, SS], dtype=tf.int32),\n                            dynamic_bstride=tf.constant([UU, VV], dtype=tf.int32),\n                            dynamic_boffset=tf.constant([BOFFS[0], BOFFS[1]], dtype=tf.int32),\n                            add=do_add,\n                            atomic=use_atomics,\n                            transpose=transpose)\n                    else:\n                        x1000 = tf.convert_to_tensor(x * 0.0 + offset, tf.float32)\n                        y1000 = sbnet_module.sparse_scatter(\n                            blockStack,\n                            tf_bin_counts,\n                            tf_active_block_indices,\n                            x1000,    # base tensor to copy to output and overwrite on top of\n                            dynamic_bsize=tf.constant([RR, SS], dtype=tf.int32),\n                            dynamic_bstride=tf.constant([UU, VV], dtype=tf.int32),\n                            dynamic_boffset=tf.constant([BOFFS[0], BOFFS[1]], dtype=tf.int32),\n                            add=do_add,\n                            atomic=use_atomics,\n                            transpose=transpose)\n                    if isGpu:\n                        with tf.control_dependencies([y1000]):\n                            dt = sbnet_module.cuda_timer_end(dt0)\n                        with tf.control_dependencies([dt]):\n                            y1000 = tf.identity(y1000)\n\n                    if isGpu:\n                        result = sess.run([b, blockStack, y1000, dt])\n                        print(""CUDA time="", result[3])\n                    else:\n                        dt = tf.constant(-1.0, dtype=tf.float32)\n                        result = sess.run([b, blockStack, y1000, dt])\n\n                    result[0] = lambda: 0\n                    result[0].bin_counts = py_bin_counts\n                    result[0].active_block_indices = py_active_block_indices\n                    result.append(x)\n                    result.append(mask)\n                    #print(""BLOCKS="", result[1])\n                    #print(""BLKIDS="", result[0])\n\n                    tidx = 1 if transpose else 0\n                    results[tidx].append(result)\n\n                    if checkGrads and not test_var:\n                        blockStackResult = result[1]\n                        err = gradient_checker.compute_gradient_error(\n                            tf_x, x.shape, blockStack, blockStackResult.shape, x_init_value=x)\n                        print(""Device, grad error="", devStr, err)\n                        self.assertTrue(err < 0.001)\n                        #grads = gradients_impl.gradients([blockStack], [tf_x])\n                        #resultGrads = sess.run([grads])\n                        #print(resultGrads)\n\n                        # if forward pass scatters are overlapping, some values will be overwritten indeterministically\n                        # so gradient currently doesn\'t make sense without atomicAdd in forward scatter\n                        if UU >= RR and VV >= SS:\n                            y1000Result = result[2]\n                            err = gradient_checker.compute_gradient_error(\n                                [tf_x, x1000], [x.shape, x.shape],\n                                y1000,\n                                y1000Result.shape,\n                                x_init_value=[x, x * 0.0 + offset])\n                            self.assertTrue(err < 0.001)\n                            print(""Device, grad error="", devStr, err)\n\n        # tidx = 0 : untransposed results\n        # tidx = 1 : transposed results\n        for tidx in range(2):\n            icpu = 1\n            igpu = 0\n            rt = results[tidx]\n            # check the input matched\n            self.assertTrue(np.array_equal(rt[icpu][4], rt[igpu][4]))\n            # check the mask matched\n            self.assertTrue(np.array_equal(rt[icpu][5], rt[igpu][5]))\n\n            reducedCpu = rt[icpu][0]\n            reducedGpu = rt[igpu][0]\n            cpuIndices = reducedCpu.active_block_indices[0:reducedCpu.bin_counts[0]]\n            gpuIndices = reducedGpu.active_block_indices[0:reducedGpu.bin_counts[0]]\n            self.assertTrue(reducedCpu.bin_counts[0] == reducedGpu.bin_counts[0]\n                    )    # make sure the count of indices matches\n            if sparsity == 100:\n                self.assertTrue(\n                    reducedCpu.bin_counts[0] == 1 and\n                    np.array_equal(reducedCpu.active_block_indices[0], np.array([0, 0, 0])))\n            bin_count = reducedCpu.bin_counts[0]\n            set0 = set([tuple(x) for x in cpuIndices.tolist()])\n            set1 = set([tuple(x) for x in gpuIndices.tolist()])\n            toSort = reducedGpu.active_block_indices[0:bin_count].tolist()\n            toSort = np.array([(x[0]*BCH*BCW + x[1]*BCW + x[2]) for x in toSort])\n            sorted = toSort.argsort()\n            self.assertTrue(set0 == set1)    # make sure the sets of indices match\n            self.assertTrue(np.array_equal(cpuIndices, gpuIndices[sorted]))    # make sure sorted indices match\n\n            gatheredCpu = rt[icpu][1]\n            gatheredGpu = rt[igpu][1][sorted]\n            if tidx == 0:\n                gatheredCpuUT = gatheredCpu\n                gatheredGpuUT = gatheredGpu\n            else:\n                gatheredCpuT = gatheredCpu\n                gatheredGpuT = gatheredGpu\n                # transposed is NCHW, convert to NHWC via [0, 2, 3, 1]\n                self.assertTrue(np.array_equal(gatheredCpuUT, np.transpose(gatheredCpuT, [0, 2, 3, 1])))\n                self.assertTrue(np.array_equal(gatheredCpuUT, np.transpose(gatheredGpuT, [0, 2, 3, 1])))\n\n            #print(""GCPU="", gatheredCpu)\n            #print(""GGPU="", gatheredGpu)\n\n            gatherEq = np.array_equal(gatheredCpu, gatheredGpu)\n            self.assertTrue(gatherEq)\n\n            # check that scattered results match\n            scatteredCpu = rt[icpu][2]\n            scatteredGpu = rt[igpu][2]\n            self.assertTrue(np.array_equal(scatteredCpu, scatteredGpu))\n            #errIndex = np.unravel_index(np.absolute(gatheredCpu - gatheredGpu).argmax(), gatheredCpu.shape)\n            #print(errIndex)\n\n        self.assertTrue(np.array_equal(gatheredCpuT, np.transpose(gatheredCpuUT, [0, 3, 1, 2])))\n        self.assertTrue(np.array_equal(gatheredGpuT, np.transpose(gatheredGpuUT, [0, 3, 1, 2])))\n        print(""========================= Test PASSED =========================="")\n\n    def testSimple(self):\n        self.runTestForOneSize((2, 3, 2), RS=(2, 3), UV=(3, 4))\n        self.runTestForOneSize((5, 5, 1), RS=(3, 3), UV=(1, 1))\n        self.runTestForOneSize((5, 5, 1), RS=(3, 3), UV=(1, 1))\n\n    def testZeroBlockCount(self):\n        # check 0-block-count from reduceMask\n        self.runTestForOneSize((2, 2, 1), RS=(1, 1), UV=(1, 1),\n                          BOFFS=(0, 0), checkGrads=True, sparsity=100)\n\n    def testBasicGradients(self):\n        # check basic grads\n        self.runTestForOneSize((2, 2, 1), RS=(1, 1), UV=(1, 1),\n                          BOFFS=(0, 0), checkGrads=True, sparsity=0)\n        # multibatch\n        self.runTestForOneSize((2, 2, 1), RS=(1, 1), UV=(1, 1),\n                          BOFFS=(0, 0), checkGrads=True, sparsity=0, N=3)\n        # slightly bigger tensor\n        self.runTestForOneSize((6, 6, 1), RS=(1, 1), UV=(1, 1), checkGrads=True)\n\n    def testOverlappingGradients(self):\n        # check overlapping grads - exercise atomic reduce path\n        self.runTestForOneSize((3, 3, 1), RS=(2, 2), UV=(1, 1),\n                          BOFFS=(0, 0), checkGrads=True, sparsity=0)\n        self.runTestForOneSize((13, 12, 1), RS=(2, 3), UV=(1, 2),\n                          BOFFS=(0, 0), checkGrads=True, sparsity=0.1)\n\n    def testSparsities(self):\n        for sparsity in [10, 20, 30, 40, 50, 70, 90, 99]:\n            self.runTestForOneSize((200, 300, 32), RS=(3, 4), UV=(2, 3),\n                              sparsity=sparsity, avgpool=True, use_atomics=True)\n\n    def testAvgPoolingWithHoles(self):\n        self.runTestForOneSize((2, 3, 2), RS=(2, 3), UV=(3, 4), avgpool=True)\n        self.runTestForOneSize((2, 3, 2), RS=(2, 3), UV=(3, 4), avgpool=False)\n\n    def testChannelSweep(self):\n        for cc in [15, 16, 24, 32, 48, 64]:\n            self.runTestForOneSize((380, 480, cc), RS=(18, 18), UV=(18, 18))\n        self.runTestForOneSize((200, 300, 32), RS=(16, 16), UV=(16, 16))\n\n    def testSimplePermutations(self):\n        for nn in [1, 7]:\n            for tv in [True, False]:\n                for avgpool in [True, False]:\n                    for sp in [0, 10, 50, 99]:\n                        args = {\n                            ""RS"": (1, 1),\n                            ""UV"": (1, 1),\n                            ""BOFFS"": [0, 0],\n                            ""checkGrads"": False,\n                            ""sparsity"": sp,\n                            ""N"": nn,\n                            ""test_var"": tv,\n                            ""avgpool"": avgpool\n                        }\n                        self.runTestForOneSize((1, 1, 1), **args)\n                        self.runTestForOneSize((1, 1, 2), **args)\n                        self.runTestForOneSize((1, 2, 3), **args)\n                        self.runTestForOneSize((2, 1, 3), **args)\n                        self.runTestForOneSize((1, 1, 1), **args)\n                        self.runTestForOneSize((3, 2, 1), **args)\n                        self.runTestForOneSize((1, 1, 1), **args)\n                        self.runTestForOneSize((1, 2, 1), **args)\n                        self.runTestForOneSize((3, 2, 1), **args)\n                        self.runTestForOneSize((3, 2, 7), **args)\n                        self.runTestForOneSize((3, 2, 7), **args)\n                        self.runTestForOneSize((1, 1, 1024 * 49), **args)\n\n    def testUnevenKernelSizes(self):\n        self.runTestForOneSize((8, 10, 7), RS=(3, 4), UV=(3, 4))\n\n    def testGapsSingleChannel(self):\n        self.runTestForOneSize((8, 10, 1), RS=(3, 4), UV=(4, 5))\n\n    def testGapsMultiChannel(self):\n        self.runTestForOneSize((8, 10, 7), RS=(3, 4), UV=(4, 5))\n\n    def testUnevenBoffs(self):\n        self.runTestForOneSize((5, 5, 1), RS=(3, 2), UV=(3, 2), BOFFS=[-1, -2], checkGrads=True)\n\n    def testUnevenNonOverlapping(self):\n        self.runTestForOneSize((5, 3, 1), RS=(2, 3), UV=(2, 3), BOFFS=[0, 0])\n        self.runTestForOneSize((5, 3, 1), RS=(2, 3), UV=(2, 3), BOFFS=[-1, -2])\n\n    def testSimpleEvenOverlapping(self):\n        # test even overlapping block sizes and strides\n        self.runTestForOneSize((3, 3, 1), RS=(2, 2), UV=(1, 1), BOFFS=[0, 0])\n\n    def testSimpleStidesWithHoles(self):\n        # test block sizes/strides with holes\n        self.runTestForOneSize((3, 3, 1), RS=(1, 1), UV=(2, 2))\n\n    def testSimpleStridesWithHolesMultichannel(self):\n        # test uneven block sizes/strides with holes + larger channel count\n        self.runTestForOneSize((8, 10, 7), RS=(3, 4), UV=(4, 5))\n\n    def benchmarkLayerSizes(self):\n        layerSizes = ((300, 480, 64), (600, 960, 64), (300, 480, 256), (300, 480, 64), (300, 480, 256))\n        for ls in layerSizes:\n            self.runTestForOneSize(ls, RS=(18, 18), UV=(18, 18))\n\n    def testLargeBlockSizes(self):\n        # test very large block sizes\n        self.runTestForOneSize((2048, 2048, 1), RS=(1500, 490), UV=(300, 390), use_atomics=True)\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
