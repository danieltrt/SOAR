file_path,api_count,code
setup.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport io\nimport re\n\nimport setuptools\nfrom setuptools import setup\n\nPACKAGE_NAME = \'petastorm\'\n\nwith open(\'README.rst\') as f:\n    long_description = f.read()\n\nwith io.open(\'petastorm/__init__.py\', \'rt\', encoding=\'utf8\') as f:\n    version = re.search(r\'__version__ = \\\'(.*?)\\\'\', f.read()).group(1)\n    if version is None:\n        raise ImportError(\'Could not find __version__ in petastorm/__init__.py\')\n\nREQUIRED_PACKAGES = [\n    \'dill>=0.2.1\',\n    \'diskcache>=3.0.0\',\n    \'future>=0.10.2\',\n    \'futures>=2.0; python_version == ""2.7""\',  # TODO(yevgeni): check if 2.0 is the minimum\n    \'numpy>=1.13.3\',\n    \'packaging>=15.0\',\n    \'pandas>=0.19.0\',\n    \'psutil>=4.0.0\',\n    \'pyspark>=2.1.0\',\n    \'pyzmq>=14.0.0\',\n    \'pyarrow>=0.12.0\',\n    \'six>=1.5.0\',\n]\n\nEXTRA_REQUIRE = {\n    # `docs` versions are to facilitate local generation of documentation.\n    # Sphinx 1.3 would be desirable, but is incompatible with current ATG setup.\n    # Thus the pinning of both sphinx and alabaster versions.\n    \'docs\': [\n        \'sphinx>=1.2.2\',\n        \'alabaster>=0.7.11\'\n    ],\n    \'opencv\': [\'opencv-python>=3.2.0.6\'],\n    \'tf\': [\'tensorflow>=1.14.0\'],\n    \'tf_gpu\': [\'tensorflow-gpu>=1.14.0\'],\n    \'test\': [\n        \'Pillow>=3.0\',\n        \'codecov>=2.0.15\',\n        \'mock>=2.0.0\',\n        \'opencv-python>=3.2.0.6\',\n        \'flake8\',\n        \'pylint>=1.9\',\n        \'pytest>=3.0.0\',\n        \'pytest-cov>=2.5.1\',\n        \'pytest-forked>=0.2\',\n        \'pytest-logger>=0.4.0\',\n        \'pytest-timeout>=1.3.3\',\n        \'requests>=2.22.0\',\n        \'s3fs>=0.0.1\',\n        \'gcsfs>=0.2.0\',\n    ],\n    \'torch\': [\'torchvision>=0.2.1\'],\n}\n\npackages = setuptools.find_packages()\n\nsetup(\n    name=PACKAGE_NAME,\n    version=version,\n    install_requires=REQUIRED_PACKAGES,\n    packages=packages,\n    description=\'Petastorm is a library enabling the use of Parquet storage from Tensorflow, Pytorch, and\'\n                \' other Python-based ML training frameworks.\',\n    long_description=long_description,\n    long_description_content_type=""text/x-rst"",\n    license=\'Apache License, Version 2.0\',\n    extras_require=EXTRA_REQUIRE,\n    entry_points={\n        \'console_scripts\': [\n            \'petastorm-copy-dataset.py=petastorm.tools.copy_dataset:main\',\n            \'petastorm-generate-metadata.py=petastorm.etl.petastorm_generate_metadata:main\',\n            \'petastorm-throughput.py=petastorm.benchmark.cli:main\',\n        ],\n    },\n    url=\'https://github.com/uber/petastorm\',\n    author=\'Uber Technologies, Inc.\',\n    classifiers=[\n        \'Environment :: Console\',\n        \'Environment :: Web Environment\',\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Science/Research\',\n        \'License :: OSI Approved :: Apache Software License\',\n        \'Programming Language :: Python :: 2.7\',\n        \'Programming Language :: Python :: 3.4\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n    ],\n)\n'"
examples/__init__.py,0,b''
petastorm/__init__.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport sys\nimport warnings\n\nfrom petastorm.errors import NoDataAvailableError  # noqa: F401\nfrom petastorm.reader import make_reader, make_batch_reader  # noqa: F401\nfrom petastorm.transform import TransformSpec  # noqa: F401\n\n__version__ = \'0.9.2\'\n\nif sys.version_info.major < 3:\n    warnings.warn(\'Petastorm on Python 2 is deprecated and will remove Python 2 support in next release.\')\n'"
petastorm/arrow_reader_worker.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom __future__ import division\n\nimport hashlib\nimport operator\n\nimport numpy as np\nimport pandas as pd\nimport pyarrow as pa\nfrom pyarrow import parquet as pq\nfrom pyarrow.parquet import ParquetFile\n\nfrom petastorm.cache import NullCache\nfrom petastorm.compat import compat_piece_read, compat_table_columns_gen, compat_column_data\nfrom petastorm.workers_pool import EmptyResultError\nfrom petastorm.workers_pool.worker_base import WorkerBase\n\n\nclass ArrowReaderWorkerResultsQueueReader(object):\n    def __init__(self):\n        pass\n\n    @property\n    def batched_output(self):\n        return True\n\n    def read_next(self, workers_pool, schema, ngram):\n        try:\n            assert not ngram, \'ArrowReader does not support ngrams for now\'\n\n            result_table = workers_pool.get_results()\n\n            # Convert arrow table columns into numpy. Strings are handled differently since to_pandas() returns\n            # numpy array of dtype=object.\n            result_dict = dict()\n            for column_name, column in compat_table_columns_gen(result_table):\n                # Assume we get only one chunk since reader worker reads one rowgroup at a time\n\n                # `to_pandas` works slower when called on the entire `data` rather directly on a chunk.\n                if compat_column_data(result_table.column(0)).num_chunks == 1:\n                    column_as_pandas = column.data.chunks[0].to_pandas()\n                else:\n                    column_as_pandas = column.data.to_pandas()\n\n                # pyarrow < 0.15.0 would always return a numpy array. Starting 0.15 we get pandas series, hence we\n                # convert it into numpy array\n                if isinstance(column_as_pandas, pd.Series):\n                    column_as_numpy = column_as_pandas.values\n                else:\n                    column_as_numpy = column_as_pandas\n\n                if pa.types.is_string(column.type):\n                    result_dict[column_name] = column_as_numpy.astype(np.unicode_)\n                elif pa.types.is_list(column.type):\n                    # Assuming all lists are of the same length, hence we can collate them into a matrix\n                    list_of_lists = column_as_numpy\n                    try:\n                        col_data = np.vstack(list_of_lists.tolist())\n                        shape = schema.fields[column_name].shape\n                        if len(shape) > 1:\n                            col_data = col_data.reshape((len(list_of_lists),) + shape)\n                        result_dict[column_name] = col_data\n\n                    except ValueError:\n                        raise RuntimeError(\'Length of all values in column \\\'{}\\\' are expected to be the same length. \'\n                                           \'Got the following set of lengths: \\\'{}\\\'\'\n                                           .format(column_name,\n                                                   \', \'.join(str(value.shape[0]) for value in list_of_lists)))\n                else:\n                    result_dict[column_name] = column_as_numpy\n\n            return schema.make_namedtuple(**result_dict)\n\n        except EmptyResultError:\n            raise StopIteration\n\n\nclass ArrowReaderWorker(WorkerBase):\n    def __init__(self, worker_id, publish_func, args):\n        super(ArrowReaderWorker, self).__init__(worker_id, publish_func, args)\n\n        self._filesystem = args[0]\n        self._dataset_path_or_paths = args[1]\n        self._schema = args[2]\n        self._ngram = args[3]\n        self._split_pieces = args[4]\n        self._local_cache = args[5]\n        self._transform_spec = args[6]\n        self._transformed_schema = args[7]\n\n        if self._ngram:\n            raise NotImplementedError(\'ngrams are not supported by ArrowReaderWorker\')\n\n        # We create datasets lazily in the first invocation of \'def process\'. This speeds up startup time since\n        # all Worker constructors are serialized\n        self._dataset = None\n\n    @staticmethod\n    def new_results_queue_reader():\n        return ArrowReaderWorkerResultsQueueReader()\n\n    # pylint: disable=arguments-differ\n    def process(self, piece_index, worker_predicate, shuffle_row_drop_partition):\n        """"""Main worker function. Loads and returns all rows matching the predicate from a rowgroup\n\n        Looks up the requested piece (a single row-group in a parquet file). If a predicate is specified,\n        columns needed by the predicate are loaded first. If no rows in the rowgroup matches the predicate criteria\n        the rest of the columns are not loaded.\n\n        :param piece_index:\n        :param shuffle_row_drop_partition: A tuple 2 of the current row drop partition and the total number\n            of partitions.\n        :return:\n        """"""\n\n        if not self._dataset:\n            self._dataset = pq.ParquetDataset(\n                self._dataset_path_or_paths,\n                filesystem=self._filesystem,\n                validate_schema=False)\n\n        if self._dataset.partitions is None:\n            # When read from parquet file list, the `dataset.partitions` will be None.\n            # But other petastorm code require at least an empty `ParquetPartitions` object.\n            self._dataset.partitions = pq.ParquetPartitions()\n\n        piece = self._split_pieces[piece_index]\n\n        # Create pyarrow file system\n        parquet_file = ParquetFile(self._dataset.fs.open(piece.path))\n\n        if not isinstance(self._local_cache, NullCache):\n            if worker_predicate:\n                raise RuntimeError(\'Local cache is not supported together with predicates, \'\n                                   \'unless the dataset is partitioned by the column the predicate operates on.\')\n            if shuffle_row_drop_partition[1] != 1:\n                raise RuntimeError(\'Local cache is not supported together with shuffle_row_drop_partitions > 1\')\n\n        if worker_predicate:\n            all_cols = self._load_rows_with_predicate(parquet_file, piece, worker_predicate, shuffle_row_drop_partition)\n        else:\n            # Using hash of the dataset path with the relative path in order to:\n            #  1. Make sure if a common cache serves multiple processes (e.g. redis), we don\'t have conflicts\n            #  2. Dataset path is hashed, to make sure we don\'t create too long keys, which maybe incompatible with\n            #     some cache implementations\n            #  3. Still leave relative path and the piece_index in plain text to make it easier to debug\n            if isinstance(self._dataset_path_or_paths, list):\n                path_str = \',\'.join(self._dataset_path_or_paths)\n            else:\n                path_str = self._dataset_path_or_paths\n            cache_key = \'{}:{}:{}\'.format(hashlib.md5(path_str.encode(\'utf-8\')).hexdigest(),\n                                          piece.path, piece_index)\n            all_cols = self._local_cache.get(cache_key,\n                                             lambda: self._load_rows(parquet_file, piece, shuffle_row_drop_partition))\n\n        if all_cols:\n            self.publish_func(all_cols)\n\n    @staticmethod\n    def _check_shape_and_ravel(x, field):\n        if not isinstance(x, np.ndarray):\n            raise ValueError(\'field {name} must be numpy array type.\'.format(name=field.name))\n        if x.shape != field.shape:\n            raise ValueError(\'field {name} must be the shape {shape}\'\n                             .format(name=field.name, shape=field.shape))\n        if not x.flags.c_contiguous:\n            raise ValueError(\'field {name} error: only support row major multi-dimensional array.\'\n                             .format(name=field.name))\n        return x.ravel()\n\n    def _load_rows(self, pq_file, piece, shuffle_row_drop_range):\n        """"""Loads all rows from a piece""""""\n\n        column_names_in_schema = set(field.name for field in self._schema.fields.values())\n\n        result = self._read_with_shuffle_row_drop(piece, pq_file, column_names_in_schema, shuffle_row_drop_range)\n\n        if self._transform_spec:\n            result_as_pandas = result.to_pandas()\n            # A user may omit `func` value if they intend just to delete some fields using the TransformSpec\n            if self._transform_spec.func:\n                transformed_result = self._transform_spec.func(result_as_pandas)\n            else:\n                transformed_result = result_as_pandas\n\n            # If transform function left a field that is listed in transform_spec\'s remove_fields, we remove it\n            # ourselves. Allows for the following transform-spec objects to be created:\n            # TransformSpec(removed_fields=[\'some field\'])\n            for field_to_remove in set(transformed_result.columns) & set(self._transform_spec.removed_fields):\n                del transformed_result[field_to_remove]\n\n            transformed_result_column_set = set(transformed_result.columns)\n            transformed_schema_column_set = set([f.name for f in self._transformed_schema.fields.values()])\n\n            if transformed_result_column_set != transformed_schema_column_set:\n                raise ValueError(\'Transformed result columns ({rc}) do not match required schema columns({sc})\'\n                                 .format(rc=\',\'.join(transformed_result_column_set),\n                                         sc=\',\'.join(transformed_schema_column_set)))\n\n            # For fields return multidimensional array, we need to ravel them\n            # because pyarrow do not support multidimensional array.\n            # later we will reshape it back.\n            for field in self._transformed_schema.fields.values():\n                if len(field.shape) > 1:\n                    transformed_result[field.name] = transformed_result[field.name] \\\n                        .map(lambda x, f=field: self._check_shape_and_ravel(x, f))\n\n            result = pa.Table.from_pandas(transformed_result, preserve_index=False)\n\n        return result\n\n    def _load_rows_with_predicate(self, pq_file, piece, worker_predicate, shuffle_row_drop_partition):\n        """"""Loads all rows that match a predicate from a piece""""""\n\n        # 1. Read all columns needed by predicate\n        # 2. Apply the predicate. If nothing matches, exit early\n        # 3. Read the remaining columns\n\n        # Split all column names into ones that are needed by predicateand the rest.\n        predicate_column_names = set(worker_predicate.get_fields())\n\n        if not predicate_column_names:\n            raise ValueError(\'At least one field name must be returned by predicate\\\'s get_field() method\')\n\n        all_schema_names = set(field.name for field in self._schema.fields.values())\n\n        invalid_column_names = predicate_column_names - all_schema_names\n        if invalid_column_names:\n            raise ValueError(\'At least some column names requested by the predicate ({}) \'\n                             \'are not valid schema names: ({})\'.format(\', \'.join(invalid_column_names),\n                                                                       \', \'.join(all_schema_names)))\n\n        # Split into \'columns for predicate evaluation\' and \'other columns\'. We load \'other columns\' only if at\n        # least one row in the rowgroup matched the predicate\n        other_column_names = all_schema_names - predicate_column_names\n\n        # Read columns needed for the predicate\n        predicates_table = self._read_with_shuffle_row_drop(piece, pq_file, predicate_column_names,\n                                                            shuffle_row_drop_partition)\n\n        predicates_data_frame = predicates_table.to_pandas()\n\n        match_predicate_mask = worker_predicate.do_include(predicates_data_frame)\n        erase_mask = match_predicate_mask.map(operator.not_)\n\n        # Don\'t have anything left after filtering? Exit early.\n        if erase_mask.all():\n            return []\n\n        predicates_data_frame[erase_mask] = None\n\n        if other_column_names:\n            # Read remaining columns\n            other_table = self._read_with_shuffle_row_drop(piece, pq_file, other_column_names,\n                                                           shuffle_row_drop_partition)\n            other_data_frame = other_table.to_pandas()\n            other_data_frame[erase_mask] = None\n\n            # Partition-by columns will appear in both other and predicate data frames. Deduplicate.\n            columns_from_predicates = predicates_data_frame.columns.difference(other_data_frame.columns)\n            result_data_frame = pd.merge(predicates_data_frame[columns_from_predicates], other_data_frame,\n                                         copy=False, left_index=True, right_index=True)\n        else:\n            result_data_frame = predicates_data_frame\n\n        result = result_data_frame[match_predicate_mask]\n\n        if self._transform_spec:\n            result = self._transform_spec.func(result)\n\n        return pa.Table.from_pandas(result, preserve_index=False)\n\n    def _read_with_shuffle_row_drop(self, piece, pq_file, column_names, shuffle_row_drop_partition):\n        partition_names = self._dataset.partitions.partition_names\n\n        # pyarrow would fail if we request a column names that the dataset is partitioned by\n        table = compat_piece_read(piece, lambda _: pq_file, columns=column_names - partition_names,\n                                  partitions=self._dataset.partitions)\n\n        # Drop columns we did not explicitly request. This may happen when a table is partitioned. Besides columns\n        # requested, pyarrow will also return partition values. Having these unexpected fields will break some\n        # downstream code.\n        loaded_column_names = set(column[0] for column in compat_table_columns_gen(table))\n        unasked_for_columns = loaded_column_names - column_names\n        if unasked_for_columns:\n            table = table.drop(unasked_for_columns)\n\n        num_rows = len(table)\n        num_partitions = shuffle_row_drop_partition[1]\n        this_partition = shuffle_row_drop_partition[0]\n\n        if num_partitions > 1:\n            data_frame_pandas = table.to_pandas()\n            partition_indexes = np.floor(np.arange(num_rows) / (float(num_rows) / min(num_rows, num_partitions)))\n\n            table = pa.Table.from_pandas(data_frame_pandas.loc[partition_indexes == this_partition],\n                                         preserve_index=False)\n\n        return table\n'"
petastorm/cache.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport abc\n\nimport six\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass CacheBase(object):\n    @abc.abstractmethod\n    def get(self, key, fill_cache_func):\n        """"""Gets an entry from the cache implementation.\n\n        If there is a cache miss, ``fill_cache_func()`` will be evaluated to get the value.\n\n        :param key: A key identifying cache entry\n        :param fill_cache_func: This function will be evaluated (``fill_cache_func()``) to populate cache, if no\n            value is present in the cache.\n        :return: A value from cache\n        """"""\n\n\nclass NullCache(CacheBase):\n    """"""A pass-through cache implementation: value generating function will be called each.""""""\n\n    def get(self, key, fill_cache_func):\n        return fill_cache_func()\n'"
petastorm/codecs.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""\nA set of dataframe-column-codecs complements the limited data type variety of spark-/pyarrow-supported datatypes,\nenabling storage of numpy multidimensional arrays, as well as compressed images, into spark dataframes, and\ntransitively to parquet files.\n\nNOTE: Due to the way unischema is stored alongside dataset (with pickling), changing any of these codecs class names\nand fields can result in reader breakages.\n""""""\nfrom abc import abstractmethod\nfrom io import BytesIO\n\ntry:\n    import cv2\n\n    OPENCV_AVAILABLE = True\nexcept ImportError:\n    OPENCV_AVAILABLE = False\n\nimport numpy as np\n\n\nclass DataframeColumnCodec(object):\n    """"""The abstract base class of codecs.""""""\n\n    @abstractmethod\n    def encode(self, unischema_field, value):\n        raise RuntimeError(\'Abstract method was called\')\n\n    @abstractmethod\n    def decode(self, unischema_field, value):\n        raise RuntimeError(\'Abstract method was called\')\n\n    @abstractmethod\n    def spark_dtype(self):\n        """"""Spark datatype to be used for underlying storage""""""\n        raise RuntimeError(\'Abstract method was called\')\n\n\nclass CompressedImageCodec(DataframeColumnCodec):\n    def __init__(self, image_codec=\'png\', quality=80):\n        """"""CompressedImageCodec would compress/encompress images.\n\n        :param image_codec: any format string supported by opencv. e.g. ``png``, ``jpeg``\n        :param quality: used when using ``jpeg`` lossy compression\n        """"""\n        assert OPENCV_AVAILABLE, ""{} requires opencv-python to be installed"".format(type(self).__name__)\n\n        self._image_codec = \'.\' + image_codec\n        self._quality = quality\n\n    @property\n    def image_codec(self):\n        """"""Returns image_codec type use by the codec: png or jpeg.""""""\n        return self._image_codec[1:]\n\n    def encode(self, unischema_field, value):\n        """"""Encodes the image using OpenCV.""""""\n        if unischema_field.numpy_dtype != value.dtype:\n            raise ValueError(""Unexpected type of {} feature, expected {}, got {}"".format(\n                unischema_field.name, unischema_field.numpy_dtype, value.dtype\n            ))\n\n        if not _is_compliant_shape(value.shape, unischema_field.shape):\n            raise ValueError(""Unexpected dimensions of {} feature, expected {}, got {}"".format(\n                unischema_field.name, unischema_field.shape, value.shape\n            ))\n\n        if len(value.shape) == 2:\n            # Greyscale image\n            image_bgr_or_gray = value\n        elif len(value.shape) == 3 and value.shape[2] == 3:\n            # Convert RGB to BGR\n            image_bgr_or_gray = value[:, :, (2, 1, 0)]\n        else:\n            raise ValueError(\'Unexpected image dimensions. Supported dimensions are (H, W) or (H, W, 3). \'\n                             \'Got {}\'.format(value.shape))\n\n        _, contents = cv2.imencode(self._image_codec,\n                                   image_bgr_or_gray,\n                                   [int(cv2.IMWRITE_JPEG_QUALITY), self._quality])\n        return bytearray(contents)\n\n    def decode(self, unischema_field, value):\n        """"""Decodes the image using OpenCV.""""""\n\n        # cv returns a BGR or grayscale image. Convert to RGB (unless a grayscale image).\n        image_bgr_or_gray = cv2.imdecode(np.frombuffer(value, dtype=np.uint8), cv2.IMREAD_UNCHANGED)\n        if len(image_bgr_or_gray.shape) == 2:\n            # Greyscale image\n            return image_bgr_or_gray\n        elif len(image_bgr_or_gray.shape) == 3 and image_bgr_or_gray.shape[2] == 3:\n            # Convert BGR to RGB (opencv assumes BGR)\n            image_rgb = image_bgr_or_gray[:, :, (2, 1, 0)]\n            return image_rgb\n        else:\n            raise ValueError(\'Unexpected image dimensions. Supported dimensions are (H, W) or (H, W, 3). \'\n                             \'Got {}\'.format(image_bgr_or_gray.shape))\n\n    def spark_dtype(self):\n        # Lazy loading pyspark to avoid creating pyspark dependency on data reading code path\n        # (currently works only with make_batch_reader). We should move all pyspark related code into a separate module\n        import pyspark.sql.types as sql_types\n\n        return sql_types.BinaryType()\n\n\nclass NdarrayCodec(DataframeColumnCodec):\n    """"""Encodes numpy ndarray into, or decodes an ndarray from, a spark dataframe field.""""""\n\n    def encode(self, unischema_field, value):\n        expected_dtype = unischema_field.numpy_dtype\n        if isinstance(value, np.ndarray):\n            if expected_dtype != value.dtype.type:\n                raise ValueError(\'Unexpected type of {} feature. \'\n                                 \'Expected {}. Got {}\'.format(unischema_field.name, expected_dtype, value.dtype))\n\n            expected_shape = unischema_field.shape\n            if not _is_compliant_shape(value.shape, expected_shape):\n                raise ValueError(\'Unexpected dimensions of {} feature. \'\n                                 \'Expected {}. Got {}\'.format(unischema_field.name, expected_shape, value.shape))\n        else:\n            raise ValueError(\'Unexpected type of {} feature. \'\n                             \'Expected ndarray of {}. Got {}\'.format(unischema_field.name, expected_dtype, type(value)))\n\n        memfile = BytesIO()\n        np.save(memfile, value)\n        return bytearray(memfile.getvalue())\n\n    def decode(self, unischema_field, value):\n        memfile = BytesIO(value)\n        return np.load(memfile)\n\n    def spark_dtype(self):\n        # Lazy loading pyspark to avoid creating pyspark dependency on data reading code path\n        # (currently works only with make_batch_reader). We should move all pyspark related code into a separate module\n        import pyspark.sql.types as sql_types\n\n        return sql_types.BinaryType()\n\n\nclass CompressedNdarrayCodec(DataframeColumnCodec):\n    """"""Encodes numpy ndarray with compression into a spark dataframe field""""""\n\n    def encode(self, unischema_field, value):\n        expected_dtype = unischema_field.numpy_dtype\n        if isinstance(value, np.ndarray):\n            if expected_dtype != value.dtype.type:\n                raise ValueError(\'Unexpected type of {} feature. \'\n                                 \'Expected {}. Got {}\'.format(unischema_field.name, expected_dtype, value.dtype))\n\n            expected_shape = unischema_field.shape\n            if not _is_compliant_shape(value.shape, expected_shape):\n                raise ValueError(\'Unexpected dimensions of {} feature. \'\n                                 \'Expected {}. Got {}\'.format(unischema_field.name, expected_shape, value.shape))\n        else:\n            raise ValueError(\'Unexpected type of {} feature. \'\n                             \'Expected ndarray of {}. Got {}\'.format(unischema_field.name, expected_dtype, type(value)))\n\n        memfile = BytesIO()\n        np.savez_compressed(memfile, arr=value)\n        return bytearray(memfile.getvalue())\n\n    def decode(self, unischema_field, value):\n        memfile = BytesIO(value)\n        return np.load(memfile)[\'arr\']\n\n    def spark_dtype(self):\n        # Lazy loading pyspark to avoid creating pyspark dependency on data reading code path\n        # (currently works only with make_batch_reader). We should move all pyspark related code into a separate module\n        import pyspark.sql.types as sql_types\n\n        return sql_types.BinaryType()\n\n\nclass ScalarCodec(DataframeColumnCodec):\n    """"""Encodes a scalar into a spark dataframe field.""""""\n\n    def __init__(self, spark_type):\n        """"""Constructs a codec.\n\n        :param spark_type: an instance of a Type object from :mod:`pyspark.sql.types`\n        """"""\n        self._spark_type = spark_type\n\n    def encode(self, unischema_field, value):\n        # Lazy loading pyspark to avoid creating pyspark dependency on data reading code path\n        # (currently works only with make_batch_reader). We should move all pyspark related code into a separate module\n        import pyspark.sql.types as sql_types\n\n        # We treat ndarrays with shape=() as scalars\n        unsized_numpy_array = isinstance(value, np.ndarray) and value.shape == ()\n        # Validate the input to be a scalar (or an unsized numpy array)\n        if not unsized_numpy_array and hasattr(value, \'__len__\') and (not isinstance(value, str)):\n            raise TypeError(\'Expected a scalar as a value for field \\\'{}\\\'. \'\n                            \'Got a non-numpy type\\\'{}\\\'\'.format(unischema_field.name, type(value)))\n\n        if unischema_field.shape:\n            raise ValueError(\'The shape field of unischema_field \\\'%s\\\' must be an empty tuple (i.e. \\\'()\\\' \'\n                             \'to indicate a scalar. However, the actual shape is %s\',\n                             unischema_field.name, unischema_field.shape)\n        if isinstance(self._spark_type, (sql_types.ByteType, sql_types.ShortType, sql_types.IntegerType,\n                                         sql_types.LongType)):\n            return int(value)\n        if isinstance(self._spark_type, (sql_types.FloatType, sql_types.DoubleType)):\n            return float(value)\n        if isinstance(self._spark_type, sql_types.BooleanType):\n            return bool(value)\n        if isinstance(self._spark_type, sql_types.StringType):\n            if not isinstance(value, str):\n                raise ValueError(\n                    \'Expected a string value for field {}. Got type {}\'.format(unischema_field.name, type(value)))\n            return str(value)\n\n        return value\n\n    def decode(self, unischema_field, value):\n        # We are using pyarrow.serialize that does not support Decimal field types.\n        # Tensorflow does not support Decimal types neither. We convert all decimals to\n        # strings hence prevent Decimals from getting into anywhere in the reader. We may\n        # choose to resurrect Decimals support in the future.\n        return unischema_field.numpy_dtype(value)\n\n    def spark_dtype(self):\n        return self._spark_type\n\n\ndef _is_compliant_shape(a, b):\n    """"""Compares the shapes of two arguments.\n\n    If size of a dimensions is None, this dimension size is ignored.\n\n    Example:\n\n    >>> assert _is_compliant_shape((1, 2, 3), (1, 2, 3))\n    >>> assert _is_compliant_shape((1, 2, 3), (1, None, 3))\n    >>> assert not _is_compliant_shape((1, 2, 3), (1, 10, 3))\n    >>> assert not _is_compliant_shape((1, 2), (1,))\n\n    :return: True, if the shapes are compliant\n    """"""\n    if len(a) != len(b):\n        return False\n    for i in range(len(a)):\n        if a[i] and b[i]:\n            if a[i] != b[i]:\n                return False\n    return True\n'"
petastorm/compat.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""0.15.0 cancelled previously deprecated functions. We still want to support 0.11 as it is being used by some users.\nThis file implements compatibility interfaces. Once we drop support of 0.11, we can get rid of this file.""""""\n\nimport pyarrow as pa\nfrom packaging import version\nfrom pyarrow import parquet as pq\n\n_PYARROW_BEFORE_013 = version.parse(pa.__version__) < version.parse(\'0.13.0\')\n_PYARROW_BEFORE_014 = version.parse(pa.__version__) < version.parse(\'0.14.0\')\n_PYARROW_BEFORE_015 = version.parse(pa.__version__) < version.parse(\'0.15.0\')\n\n\ndef compat_get_metadata(piece, open_func):\n    if _PYARROW_BEFORE_013:\n        arrow_metadata = piece.get_metadata(open_func)\n    else:\n        arrow_metadata = piece.get_metadata()\n    return arrow_metadata\n\n\ndef compat_piece_read(piece, open_file_func, **kwargs):\n    if _PYARROW_BEFORE_013:\n        table = piece.read(open_file_func=open_file_func, **kwargs)\n    else:\n        table = piece.read(**kwargs)\n    return table\n\n\ndef compat_table_columns_gen(table):\n    if _PYARROW_BEFORE_014:\n        for column in table.columns:\n            name = column.name\n            yield name, column\n    else:\n        for name in table.column_names:\n            column = table.column(name)\n            yield name, column\n\n\ndef compat_column_data(column):\n    if _PYARROW_BEFORE_015:\n        return column.data\n    else:\n        return column\n\n\ndef compat_make_parquet_piece(path, open_file_func, **kwargs):\n    if _PYARROW_BEFORE_013:\n        return pq.ParquetDatasetPiece(path, **kwargs)\n    else:\n        return pq.ParquetDatasetPiece(path, open_file_func=open_file_func,  # pylint: disable=unexpected-keyword-arg\n                                      **kwargs)\n\n\ndef compat_with_metadata(schema, metadata):\n    if _PYARROW_BEFORE_015:\n        return schema.add_metadata(metadata)\n    else:\n        return schema.with_metadata(metadata)\n\n\ndef compat_schema_field(schema, name):\n    if _PYARROW_BEFORE_013:\n        return schema.field_by_name(name)\n    else:\n        return schema.field(name)\n'"
petastorm/errors.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nclass NoDataAvailableError(Exception):\n    """"""Raised when a sharding was enabled shard_count > 1 from a reader but non-empty shards can not be produced""""""\n'"
petastorm/fs_utils.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\n\nimport pyarrow\nimport six\nfrom six.moves.urllib.parse import urlparse\n\nfrom petastorm.gcsfs_helpers.gcsfs_wrapper import GCSFSWrapper\nfrom petastorm.hdfs.namenode import HdfsNamenodeResolver, HdfsConnector\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_dataset_path(parsed_url):\n    """"""\n    The dataset path is different than the one in `_parsed_dataset_url` for some filesystems.\n    For example s3fs expects the bucket name to be included in the path and doesn\'t support\n    paths that start with a `/`\n    """"""\n    if parsed_url.scheme.lower() in [\'s3\', \'gs\', \'gcs\']:\n        # s3/gs/gcs filesystem expects paths of the form `bucket/path`\n        return parsed_url.netloc + parsed_url.path\n\n    return parsed_url.path\n\n\nclass FilesystemResolver(object):\n    """"""Resolves a dataset URL, makes a connection via pyarrow, and provides a filesystem object.""""""\n\n    def __init__(self, dataset_url, hadoop_configuration=None, connector=HdfsConnector,\n                 hdfs_driver=\'libhdfs3\', user=None):\n        """"""\n        Given a dataset URL and an optional hadoop configuration, parse and interpret the URL to\n        instantiate a pyarrow filesystem.\n\n        Interpretation of the URL ``scheme://hostname:port/path`` occurs in the following order:\n\n        1. If no ``scheme``, no longer supported, so raise an exception!\n        2. If ``scheme`` is ``file``, use local filesystem path.\n        3. If ``scheme`` is ``hdfs``:\n           a. Try the ``hostname`` as a namespace and attempt to connect to a name node.\n              1. If that doesn\'t work, try connecting directly to namenode ``hostname:port``.\n           b. If no host, connect to the default name node.\n        5. If ``scheme`` is ``s3``, use s3fs. The user must manually install s3fs before using s3\n        6. If ``scheme`` is ``gs``or ``gcs``, use gcsfs. The user must manually install gcsfs before using GCS\n        7. Fail otherwise.\n\n        :param dataset_url: The hdfs URL or absolute path to the dataset\n        :param hadoop_configuration: an optional hadoop configuration\n        :param connector: the HDFS connector object to use (ONLY override for testing purposes)\n        :param hdfs_driver: A string denoting the hdfs driver to use (if using a dataset on hdfs). Current choices are\n        libhdfs (java through JNI) or libhdfs3 (C++)\n        :param user: String denoting username when connecting to HDFS. None implies login user.\n        """"""\n        # Cache both the original URL and the resolved, urlparsed dataset_url\n        self._dataset_url = dataset_url\n        self._parsed_dataset_url = None\n        # Cache the instantiated filesystem object\n        self._filesystem = None\n\n        if isinstance(self._dataset_url, six.string_types):\n            self._parsed_dataset_url = urlparse(self._dataset_url)\n        else:\n            self._parsed_dataset_url = self._dataset_url\n\n        if not self._parsed_dataset_url.scheme:\n            # Case 1\n            raise ValueError(\'ERROR! A scheme-less dataset url ({}) is no longer supported. \'\n                             \'Please prepend ""file://"" for local filesystem.\'.format(self._parsed_dataset_url.scheme))\n\n        elif self._parsed_dataset_url.scheme == \'file\':\n            # Case 2: definitely local\n            self._filesystem = pyarrow.localfs\n            self._filesystem_factory = lambda: pyarrow.localfs\n\n        elif self._parsed_dataset_url.scheme == \'hdfs\':\n\n            if hdfs_driver == \'libhdfs3\':\n                # libhdfs3 does not do any namenode resolution itself so we do it manually. This is not necessary\n                # if using libhdfs\n\n                # Obtain singleton and force hadoop config evaluation\n                namenode_resolver = HdfsNamenodeResolver(hadoop_configuration)\n\n                # Since we can\'t tell for sure, first treat the URL as though it references a name service\n                if self._parsed_dataset_url.netloc:\n                    # Case 3a: Use the portion of netloc before any port, which doesn\'t get lowercased\n                    nameservice = self._parsed_dataset_url.netloc.split(\':\')[0]\n                    namenodes = namenode_resolver.resolve_hdfs_name_service(nameservice)\n                    if namenodes:\n                        self._filesystem = connector.connect_to_either_namenode(namenodes, user=user)\n                        self._filesystem_factory = lambda: connector.connect_to_either_namenode(namenodes, user=user)\n                    if self._filesystem is None:\n                        # Case 3a1: That didn\'t work; try the URL as a namenode host\n                        self._filesystem = connector.hdfs_connect_namenode(self._parsed_dataset_url, user=user)\n                        self._filesystem_factory = \\\n                            lambda url=self._dataset_url, user=user: \\\n                            connector.hdfs_connect_namenode(urlparse(url), user=user)\n                else:\n                    # Case 3b: No netloc, so let\'s try to connect to default namenode\n                    # HdfsNamenodeResolver will raise exception if it fails to connect.\n                    nameservice, namenodes = namenode_resolver.resolve_default_hdfs_service()\n                    filesystem = connector.connect_to_either_namenode(namenodes, user=user)\n                    self._filesystem_factory = lambda: connector.connect_to_either_namenode(namenodes, user=user)\n                    if filesystem is not None:\n                        # Properly replace the parsed dataset URL once default namenode is confirmed\n                        self._parsed_dataset_url = urlparse(\n                            \'hdfs://{}{}\'.format(nameservice, self._parsed_dataset_url.path))\n                        self._filesystem = filesystem\n            else:\n                self._filesystem = connector.hdfs_connect_namenode(self._parsed_dataset_url, hdfs_driver, user=user)\n                self._filesystem_factory = \\\n                    lambda url=self._dataset_url, user=user: \\\n                    connector.hdfs_connect_namenode(urlparse(url), hdfs_driver, user=user)\n\n        elif self._parsed_dataset_url.scheme == \'s3\':\n            # Case 5\n            # S3 support requires s3fs to be installed\n            try:\n                import s3fs\n            except ImportError:\n                raise ValueError(\'Must have s3fs installed in order to use datasets on s3. \'\n                                 \'Please install s3fs and try again.\')\n\n            if not self._parsed_dataset_url.netloc:\n                raise ValueError(\'URLs must be of the form s3://bucket/path\')\n\n            fs = s3fs.S3FileSystem()\n            self._filesystem = pyarrow.filesystem.S3FSWrapper(fs)\n            self._filesystem_factory = lambda: pyarrow.filesystem.S3FSWrapper(s3fs.S3FileSystem())\n\n        elif self._parsed_dataset_url.scheme in [\'gs\', \'gcs\']:\n            # Case 6\n            # GCS support requires gcsfs to be installed\n            try:\n                import gcsfs\n            except ImportError:\n                raise ValueError(\'Must have gcsfs installed in order to use datasets on GCS. \'\n                                 \'Please install gcsfs and try again.\')\n\n            if not self._parsed_dataset_url.netloc:\n                raise ValueError(\'URLs must be of the form gs://bucket/path or gcs://bucket/path\')\n\n            fs = gcsfs.GCSFileSystem()\n            self._filesystem = GCSFSWrapper(fs)\n            self._filesystem_factory = lambda: GCSFSWrapper(gcsfs.GCSFileSystem())\n\n        else:\n            # Case 7\n            raise ValueError(\'Unsupported scheme in dataset url {}. \'\n                             \'Currently, only ""file"" and ""hdfs"" are supported.\'.format(self._parsed_dataset_url.scheme))\n\n    def parsed_dataset_url(self):\n        """"""\n        :return: The urlparse\'d dataset_url\n        """"""\n        return self._parsed_dataset_url\n\n    def get_dataset_path(self):\n        """"""\n        The dataset path is different than the one in `_parsed_dataset_url` for some filesystems.\n        For example s3fs expects the bucket name to be included in the path and doesn\'t support\n        paths that start with a `/`\n        """"""\n        return get_dataset_path(self._parsed_dataset_url)\n\n    def filesystem(self):\n        """"""\n        :return: The pyarrow filesystem object\n        """"""\n        return self._filesystem\n\n    def filesystem_factory(self):\n        """"""\n        :return: A serializable function that can be used to recreate the filesystem\n        object; useful for providing access to the filesystem object on distributed\n        Spark executors.\n        """"""\n        return self._filesystem_factory\n\n    def __getstate__(self):\n        raise RuntimeError(\'Pickling FilesystemResolver is not supported as it may contain some \'\n                           \'a file-system instance objects that do not support pickling but do not have \'\n                           \'anti-pickling protection\')\n\n\ndef get_filesystem_and_path_or_paths(url_or_urls, hdfs_driver=\'libhdfs3\'):\n    """"""\n    Given a url or url list, return a tuple ``(filesystem, path_or_paths)``\n    ``filesystem`` is created from the given url(s), and ``path_or_paths`` is a path or path list\n    extracted from the given url(s)\n    if url list given, the urls must have the same scheme and netloc.\n    """"""\n    if isinstance(url_or_urls, list):\n        url_list = url_or_urls\n    else:\n        url_list = [url_or_urls]\n\n    parsed_url_list = [urlparse(url) for url in url_list]\n\n    first_scheme = parsed_url_list[0].scheme\n    first_netloc = parsed_url_list[0].netloc\n\n    for parsed_url in parsed_url_list:\n        if parsed_url.scheme != first_scheme or parsed_url.netloc != first_netloc:\n            raise ValueError(\'The dataset url list must contain url with the same scheme and netloc.\')\n\n    fs = FilesystemResolver(url_list[0], hdfs_driver=hdfs_driver).filesystem()\n    path_list = [get_dataset_path(parsed_url) for parsed_url in parsed_url_list]\n\n    if isinstance(url_or_urls, list):\n        path_or_paths = path_list\n    else:\n        path_or_paths = path_list[0]\n\n    return fs, path_or_paths\n\n\ndef normalize_dir_url(dataset_url):\n    if dataset_url is None or not isinstance(dataset_url, six.string_types):\n        raise ValueError(\'directory url must be a string\')\n\n    dataset_url = dataset_url[:-1] if dataset_url[-1] == \'/\' else dataset_url\n    logger.debug(\'directory url: %s\', dataset_url)\n    return dataset_url\n'"
petastorm/generator.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nfrom numpy.random import rand\n\nLIST_SIZE = 13\n\n\ndef generate_datapoint(schema):\n    """"""Generates random data point following a schema.\n\n    :param schema: an instance of Unischema specifying the columns of the dataset (with name, dtype, shape and codec)\n\n    :returns: a randomly generated datapoint with the fields and format specified by schema\n    """"""\n\n    # Init dict representing datapoint\n    d = {}\n\n    for key, f in schema.fields.items():\n        dtype = f.numpy_dtype\n        shape = tuple(d if d is not None else LIST_SIZE for d in f.shape)\n\n        # Extract range information from data type\n        min_val, max_val = 0, 1\n\n        if issubclass(dtype, np.integer):\n            min_val, max_val = np.iinfo(dtype).min, np.iinfo(dtype).max\n\n        spread = max_val - min_val\n\n        value = rand(*shape) * spread + min_val\n        d[key] = np.array(value, dtype=dtype)\n\n    return d\n'"
petastorm/local_disk_arrow_table_cache.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pyarrow as pa\n\nfrom petastorm.local_disk_cache import LocalDiskCache\n\n\nclass LocalDiskArrowTableCache(LocalDiskCache):\n    """"""A disk cache implementation """"""\n    def __init__(self, *args, **kwargs):\n        super(LocalDiskArrowTableCache, self).__init__(*args, **kwargs)\n        # Workaround for https://issues.apache.org/jira/browse/ARROW-5260\n        # unless we try to serialize something before deserialize_components is called, we would crash with a sigsegv\n        pa.serialize(0)\n\n    def get(self, key, fill_cache_func):\n        value = self._cache.get(key, default=None)\n        if value is None:\n            value = fill_cache_func()\n            table_pandas = value.to_pandas()\n            serialized_df = pa.serialize(table_pandas)\n            components = serialized_df.to_components()\n            self._cache.set(key, components)\n        else:\n            original_df = pa.deserialize_components(value)\n            value = pa.Table.from_pandas(original_df, preserve_index=False)\n\n        return value\n'"
petastorm/local_disk_cache.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom __future__ import division\n\nimport shutil\nfrom diskcache import FanoutCache\n\nfrom petastorm.cache import CacheBase\n\n\nclass LocalDiskCache(CacheBase):\n    def __init__(self, path, size_limit_bytes, expected_row_size_bytes, shards=6, cleanup=False, **settings):\n        """"""LocalDiskCache is an adapter to a diskcache implementation.\n\n        LocalDiskCache can be used by a petastorm Reader class to temporarily keep parts of the dataset on a local\n        file system storage.\n\n        :param path: Path where the dataset cache is being stored.\n        :param size_limit_bytes: Maximal size of the disk-space to be used by cache. The size of the cache may actually\n                                 grow somewhat above the size_limit_bytes, so the limit is not very strict.\n        :param expected_row_size_bytes: Approximate size of a single row. This argument is used to perform a sanity\n                                 check on the capacity of individual shards.\n        :param shards: Cache can be sharded. Larger number of shards improve writing parallelism.\n        :param cleanup: If set to True, cache directory would be removed when cleanup() method is called.\n        :param settings: these parameters passed-through to the diskcache.Cache class.\n                         For details, see: http://www.grantjenks.com/docs/diskcache/tutorial.html#settings\n        """"""\n        if size_limit_bytes / shards < 5 * expected_row_size_bytes:\n            raise ValueError(\'Condition \\\'size_limit_bytes / shards < 5 * expected_row_size_bytes\\\' needs to hold, \'\n                             \'otherwise, newly added cached values might end up being immediately evicted.\')\n\n        default_settings = {\n            \'size_limit\': size_limit_bytes,\n            \'eviction_policy\': \'least-recently-stored\',\n        }\n        default_settings.update(settings)\n\n        self._cleanup = cleanup\n        self._path = path\n        self._cache = FanoutCache(path, shards, **default_settings)\n\n    def get(self, key, fill_cache_func):\n        value = self._cache.get(key, default=None)\n        if value is None:\n            value = fill_cache_func()\n            self._cache.set(key, value)\n\n        return value\n\n    def cleanup(self):\n        if self._cleanup:\n            shutil.rmtree(self._path)\n'"
petastorm/namedtuple_gt_255_fields.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import print_function\n\nimport sys as _sys\nfrom keyword import iskeyword as _iskeyword\n\n_class_template = """"""\\\nfrom builtins import property as _property, tuple as _tuple\nfrom operator import itemgetter as _itemgetter\nfrom collections import OrderedDict\nfrom petastorm.namedtuple_gt_255_fields import _restore_namedtuple_gt_255_fields\nclass {typename}(tuple):\n    \'{typename}({arg_list})\'\n    __slots__ = ()\n    _fields = {field_names!r}\n    def __new__(_cls, *args, **kwargs):\n        \'Create new instance of {typename}({arg_list})\'\n        values = []\n        missings = []\n        for index, field in enumerate(_cls._fields):\n            if index < len(args):\n                values.append(args[index])\n                if field in kwargs:\n                    raise TypeError(\'__new__() got multiple values for argument %r\' % field)\n            elif field in kwargs:\n                values.append(kwargs[field])\n            else:\n                values.append(None)\n                missings.append(field)\n        count_missings = len(missings)\n        if count_missings > 0:\n            last_missing = \' and %s\' % missings[-1] if count_missings > 1 else \'\'\n            missings = missings[:-1] if count_missings > 1 else missings\n            raise TypeError(\'__new__() missing %d required positional argument%s: %s%s\' %\n                            (count_missings, \'s\' if count_missings > 1 else \'\',\n                            \', \'.join(missings), last_missing))\n        return _tuple.__new__(_cls, values)\n    @classmethod\n    def _make(cls, iterable, new=tuple.__new__, len=len):\n        \'Make a new {typename} object from a sequence or iterable\'\n        result = new(cls, iterable)\n        if len(result) != {num_fields:d}:\n            raise TypeError(\'Expected {num_fields:d} arguments, got %d\' % len(result))\n        return result\n    def _replace(_self, **kwds):\n        \'Return a new {typename} object replacing specified fields with new values\'\n        result = _self._make(map(kwds.pop, {field_names!r}, _self))\n        if kwds:\n            raise ValueError(\'Got unexpected field names: %r\' % list(kwds))\n        return result\n    def __repr__(self):\n        \'Return a nicely formatted representation string\'\n        return self.__class__.__name__ + \'({repr_fmt})\' % self\n    def _asdict(self):\n        \'Return a new OrderedDict which maps field names to their values.\'\n        return OrderedDict(zip(self._fields, self))\n    def __getnewargs__(self):\n        \'Return self as a plain tuple. Used by copy and pickle.\'\n        return tuple(self)\n    def __reduce__(self):\n        \'Makes the namedtuple pickable.\'\n        cls = self.__class__\n        return (_restore_namedtuple_gt_255_fields, (cls.__name__, cls._fields, tuple(self)))\n{field_defs}\n""""""\n\n_repr_template = \'{name}=%r\'\n\n_field_template = \'\'\'\\\n    {name} = _property(_itemgetter({index:d}), doc=\'Alias for field number {index:d}\')\n\'\'\'\n\n\ndef _restore_namedtuple_gt_255_fields(typename, fields, values):\n    """"""Creates an namedtuple_gt_255_fields objects along based its __reduce__ description.\n\n    The __reduce__ protocol to support pickling requires:\n    - a callable method that returns the pickled object on restore\n    - a tuple of its arguments.\n\n    This function is called by the __reduce__ method of a generated namedtuple_gt_255_fields\n    to recreate the inital object.\n    """"""\n    return namedtuple_gt_255_fields(typename, fields)(*values)\n\n\ndef namedtuple_gt_255_fields(typename, field_names, verbose=False, rename=False, module=None):\n    """"""Returns a new subclass of tuple with named fields.\n    >>> Point = namedtuple2(\'Point\', [\'x\', \'y\'])\n    >>> Point.__doc__                   # docstring for the new class\n    \'Point(x, y)\'\n    >>> p = Point(11, y=22)             # instantiate with positional args or keywords\n    >>> p[0] + p[1]                     # indexable like a plain tuple\n    33\n    >>> x, y = p                        # unpack like a regular tuple\n    >>> x, y\n    (11, 22)\n    >>> p.x + p.y                       # fields also accessible by name\n    33\n    >>> d = p._asdict()                 # convert to a dictionary\n    >>> d[\'x\']\n    11\n    >>> Point(**d)                      # convert from a dictionary\n    Point(x=11, y=22)\n    >>> p._replace(x=100)               # _replace() is like str.replace() but targets named fields\n    Point(x=100, y=22)\n    """"""\n\n    # Validate the field names.  At the user\'s option, either generate an error\n    # message or automatically replace the field name with a valid name.\n    if isinstance(field_names, str):\n        field_names = field_names.replace(\',\', \' \').split()\n    field_names = list(map(str, field_names))\n    typename = str(typename)\n    if rename:\n        seen = set()\n        for index, name in enumerate(field_names):\n            if (not name.isidentifier()\n                    or _iskeyword(name)\n                    or name.startswith(\'_\')\n                    or name in seen):\n                field_names[index] = \'_%d\' % index\n            seen.add(name)\n    for name in [typename] + field_names:\n        if type(name) is not str:\n            raise TypeError(\'Type names and field names must be strings\')\n        if not name.isidentifier():\n            raise ValueError(\'Type names and field names must be valid \'\n                             \'identifiers: %r\' % name)\n        if _iskeyword(name):\n            raise ValueError(\'Type names and field names cannot be a \'\n                             \'keyword: %r\' % name)\n    seen = set()\n    for name in field_names:\n        if name.startswith(\'_\') and not rename:\n            raise ValueError(\'Field names cannot start with an underscore: \'\n                             \'%r\' % name)\n        if name in seen:\n            raise ValueError(\'Encountered duplicate field name: %r\' % name)\n        seen.add(name)\n\n    # Fill-in the class template\n    class_definition = _class_template.format(\n        typename=typename,\n        field_names=tuple(list(field_names)),\n        num_fields=len(field_names),\n        arg_list=repr(list(field_names)).replace(""\'"", """")[1:-1],\n        repr_fmt=\', \'.join(_repr_template.format(name=name)\n                           for name in field_names),\n        field_defs=\'\\n\'.join(_field_template.format(index=index, name=name)\n                             for index, name in enumerate(field_names))\n    )\n\n    # Execute the template string in a temporary namespace and support\n    # tracing utilities by setting a value for frame.f_globals[\'__name__\']\n    namespace = dict(__name__=\'namedtuple_gt_255_fields_%s\' % typename)\n    # pylint: disable=W0122\n    exec(class_definition, namespace)\n    result = namespace[typename]\n    result._source = class_definition\n    if verbose:\n        print(result._source)\n\n    # For pickling to work, the __module__ variable needs to be set to the frame\n    # where the named tuple is created.  Bypass this step in environments where\n    # sys._getframe is not defined (Jython for example) or sys._getframe is not\n    # defined for arguments greater than 0 (IronPython), or where the user has\n    # specified a particular module.\n    if module is None:\n        try:\n            module = _sys._getframe(1).f_globals.get(\'__name__\', \'__main__\')\n        except (AttributeError, ValueError):\n            pass\n    if module is not None:\n        result.__module__ = module\n\n    return result\n'"
petastorm/ngram.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport numbers\n\nfrom petastorm.unischema import UnischemaField, match_unischema_fields\nfrom six import string_types\n\n\nclass NGram(object):\n    """"""\n    Defines an NGram, having certain fields as set by fields, where consecutive items in an NGram are no further apart\n    than the argument delta_threshold (inclusive). The argument timestamp_field indicate which field refers to the\n    timestamp in the data.\n\n    The argument fields is a dictionary, where the keys are integers, and the value is an array of the Unischema fields\n    to include at that timestep.\n\n    The delta_threshold and timestamp_field defines how far away each item in the NGram will be as described in the\n    rules above.\n\n    The following are examples of what NGram will return based on the parameters:\n\n    A. Case 1:\n\n    >>> fields = {\n    >>>  -1: [TestSchema.id, TestSchema.id2, TestSchema.image_png,\n    >>>       TestSchema.matrix],\n    >>>   0: [TestSchema.id, TestSchema.id2,\n    >>>       TestSchema.sensor_name],\n    >>> }\n    >>> delta_threshold = 5\n    >>> timestamp_field = \'id\'\n\n      - The data being:\n\n    >>> A {\'id\': 0,  ....}\n    >>> B {\'id\': 10, ....}\n    >>> C {\'id\': 20, ....}\n    >>> D {\'id\': 30, ....}\n\n      - The result will be empty, since delta is 10 (more than allowed delta_threshold of 5)\n\n    B. Case 2:\n\n    >>> fields = {\n    >>>     -1: [TestSchema.id, TestSchema.id2, TestSchema.image_png,\n    >>>          TestSchema.matrix],\n    >>>      0: [TestSchema.id, TestSchema.id2,\n    >>>          TestSchema.sensor_name],\n    >>> }\n    >>> delta_threshold = 4\n    >>> timestamp_field = \'id\'\n\n      - The data being:\n\n    >>> A {\'id\': 0, .....}\n    >>> B {\'id\': 3, .....}\n    >>> C {\'id\': 8, .....}\n    >>> D {\'id\': 10, .....}\n    >>> E {\'id\': 11, .....}\n    >>> G {\'id\': 20, .....}\n    >>> H {\'id\': 30, .....}\n\n      - The result will be:\n\n    >>> {-1: A, 0: B},\n    >>> {-1: C, 0: D},\n    >>> {-1: D, 0: E}\n\n      - Notice how:\n        - ``(B, C)`` was skipped since B ``id`` is 3 and C ``id`` is 8 (difference of 5 ``>= delta_threshold`` of 4)\n        - ``(E, G), (G, H)`` were also skipped due to the same reason\n\n    One caveat to note: All NGrams within the same parquet row group are guaranteed to be returned, but not across\n    different parquet row groups. i.e. if row group 1 has ``[0, 5]``, row group 2 has ``[6, 10]`` then this will result\n    in ``(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (6, 7), (7, 8), (8, 9), (9, 10)``.\n\n    Notice how the ``(5, 6)`` was skipped because it is across two different row groups.\n    In order to potentially produce more NGrams, the row group size should be increased (at minimum it needs to be\n    at least as large as the NGram length).\n\n    Note: Passing a field argument like ``{-1: [TestSchema.id], 1: [TestSchema.id]}`` are allowed, the 0 field will\n    just be empty.\n    Passing a field argument like ``{1: [TestSchema.id], 0: [TestSchema.id]}`` is the same as passing a field\n    argument like ``{0: [TestSchema.id], 1: [TestSchema.id]}``\n\n    The return type will be a dictionary where the keys are the same as the keys passed to fields and the value of each\n    key will be the item.\n    """"""\n\n    def __init__(self, fields, delta_threshold, timestamp_field, timestamp_overlap=True):\n        """"""\n        Constructor to initialize ngram with fields, delta_threshold and timestamp_field.\n\n        :param fields: A dictionary, with consecutive integers as keys and each value is an array of Unischema fields.\n        :param delta_threshold: The maximum threshold of delta between timestamp_field.\n        :param timestamp_field: The field that represents the timestamp.\n        :param timestamp_overlap: Whether timestamps in sequences are allowed to overlap (defaults to True),\n            e.g., If the data consists of consecutive timestamps ``[{\'id\': 0}, {\'id\': 1}, ..., {\'id\': 5}]``\n            and you are asking for NGram of length 3 with timestamp_overlap set to True you will receive\n            NGrams of ``[{\'id\': 0}, {\'id\': 1}, {\'id\': 2}]`` and ``[{\'id\': 1}, {\'id\': 2}, {\'id\': 3}]`` (in addition\n            to others); however, note that ``{\'id\': 1}``, and ``{\'id\': 2}`` appear twice. With timestamp_overlap set\n            to False, this would not occur and instead return ``[{\'id\': 0}, {\'id\': 1}, {\'id\': 2}]`` and\n            ``[{\'id\': 3}, {\'id\': 4}, {\'id\': 5}]``. There is no overlap of timestamps between NGrams (and each\n            timestamp record should only occur once in the returned data)\n        """"""\n        self._fields = fields\n        self._delta_threshold = delta_threshold\n\n        self._timestamp_field = timestamp_field\n\n        self.timestamp_overlap = timestamp_overlap\n\n        self._validate_ngram(fields, delta_threshold, timestamp_field, timestamp_overlap)\n\n    @property\n    def length(self):\n        """"""\n        Returns the ngram length requested.\n        :return: the ngram length.\n        """"""\n        return max(self._fields.keys()) - min(self._fields.keys()) + 1\n\n    @property\n    def fields(self):\n        """"""\n        Returns the ngram fields.\n        :return: The ngram fields.\n        """"""\n        return self._fields\n\n    @property\n    def delta_threshold(self):\n        """"""\n        The maximum difference between one entry and the following one in timestamp_field.\n        :return: The delta threshold.\n        """"""\n        return self._delta_threshold\n\n    def _validate_ngram(self, fields, delta_threshold, timestamp_field, timestamp_overlap):\n        """"""\n        Validates the fields, delta_threshold and timestamp_field are set and of the correct types.\n        :param fields: The ngram fields.\n        :param delta_threshold: The delta threshold.\n        :param timestamp_field: The timestamp field.\n        :param timestamp_overlap: Whether timestamps in sequences are allowed to overlap\n        """"""\n        if fields is None or not isinstance(fields, dict):\n            raise ValueError(\'Fields must be set and must be a dictionary.\')\n\n        for key in fields:\n            if not isinstance(fields[key], list):\n                raise ValueError(\'Each field value must be a list of unischema fields/regular expression(s)\')\n            for field in fields[key]:\n                if not (isinstance(field, UnischemaField) or isinstance(field, string_types)):\n                    raise ValueError(\'All field values must be of type UnischemaField/regular expression\')\n\n        if delta_threshold is None or not isinstance(delta_threshold, numbers.Number):\n            raise ValueError(\'delta_threshold must be a number.\')\n\n        if timestamp_field is None or not (isinstance(timestamp_field, UnischemaField) or\n                                           isinstance(timestamp_field, string_types)):\n            raise ValueError(\'timestamp_field must be set and must be of type UnischemaField or regular expression\')\n\n        if timestamp_overlap is None or not isinstance(timestamp_overlap, bool):\n            raise ValueError(\'timestamp_overlap must be set and must be of type bool\')\n\n    def _ngram_pass_threshold(self, ngram):\n        """"""\n        Returns true if each item in a ngram passes the threshold, otherwise False.\n        Specifically that means that timestamp of an item - timestamp of previous item <= delta_threshold\n\n        It is assumed here that items read are all sorted by timestamp field.\n\n        :param ngram: An array of items\n        :return: True if each item in a ngram passes threshold, otherwise False.\n        """"""\n        # Verify that each element and it\'s previous element do not exceed the delta_threshold\n        for previous, current in zip(ngram[:-1], ngram[1:]):\n            if current[self._timestamp_field.name] - previous[self._timestamp_field.name] > self.delta_threshold:\n                return False\n        return True\n\n    def resolve_regex_field_names(self, schema):\n        """"""Resolve string(s) (regular expression(s)) that were passed into \'fields\' and \'timestamp_field\' parameters\n        """"""\n        self._fields = {k: self.convert_fields(schema, self._fields[k]) for k in self._fields.keys()}\n        converted_ts_field = self.convert_fields(schema, [self._timestamp_field])\n        if len(converted_ts_field) > 1:\n            raise ValueError(""timestamp_field was matched to more than one unischema field"")\n\n        self._timestamp_field = converted_ts_field[0]\n\n    def get_field_names_at_timestep(self, timestep):\n        """"""\n        Return the field names at a certain timestep.\n        :param timestep: The timestep to return the field names at.\n        :return: A list of all the field names at that timestep.\n        """"""\n        if timestep not in self._fields:\n            return []\n        return [field.name for field in self._fields[timestep]]\n\n    def get_schema_at_timestep(self, schema, timestep):\n        """"""\n        Returns the schema of the data at a certain timestep.\n        :param schema: The schema of the data, which schema at a certain timestep is a subset of.\n        :param timestep: The timestep to get the schema at.\n        :return: The schema of the data at a certain timestep.\n        """"""\n        return schema.create_schema_view([schema.fields.get(field) for field in schema.fields if\n                                          field in self.get_field_names_at_timestep(timestep)])\n\n    def form_ngram(self, data, schema):\n        """"""\n        Return all the ngrams as dictated by fields, delta_threshold and timestamp_field.\n        :param data: The data items, which is a list of Unischema items.\n        :return: A dictionary, with keys ``[0, length - 1]``. The value of each key is the corresponding item in the\n        ngram at that position.\n        """"""\n\n        base_key = min(self._fields.keys())\n        result = []\n        prev_ngram_end_timestamp = None\n\n        for index in range(len(data) - self.length + 1):\n            # Potential ngram: [index, index + self.length[\n            potential_ngram = data[index:index + self.length]\n\n            is_sorted = all(potential_ngram[i][self._timestamp_field.name] <=\n                            potential_ngram[i + 1][self._timestamp_field.name]\n                            for i in range(len(potential_ngram) - 1))\n            if not is_sorted:\n                raise NotImplementedError(\'NGram assumes that the data is sorted by {0} field which is not the case\'\n                                          .format(self._timestamp_field.name))\n\n            if not self.timestamp_overlap and prev_ngram_end_timestamp is not None:\n                # If we dont want timestamps of NGrams to overlap, check that the start timestamp of the next NGram\n                # is not less than the end timestamp of the previous NGram\n                next_ngram_start_timestamp = potential_ngram[0][self._timestamp_field.name]\n                if next_ngram_start_timestamp <= prev_ngram_end_timestamp:\n                    continue\n\n            # If all elements in potential_ngram passes the ngram threshold\n            # (i.e. current element timestamp - previous element timestamp <= delta_threshold)\n            # then add the potential ngram in the results, otherwise skip it\n            if len(potential_ngram) == self.length and self._ngram_pass_threshold(potential_ngram):\n                new_item = {(base_key + key): value for (key, value) in enumerate(potential_ngram)}\n                for key in new_item:\n                    # Get the data for that current timestep and create a namedtuple\n                    current_item = new_item[key]\n                    new_item[key] = {k: current_item[k]\n                                     for k in current_item if k in self.get_field_names_at_timestep(key)}\n                result.append(new_item)\n\n                if not self.timestamp_overlap:\n                    prev_ngram_end_timestamp = potential_ngram[-1][self._timestamp_field.name]\n\n        return result\n\n    def make_namedtuple(self, schema, ngram_as_dicts):\n        """"""Converts a ngram structure where mapped values are dictionaries to a mapped structure where mapped values\n        are namedtuples.\n\n        Example:\n\n        >>> { -1 : {\'f1\': 10, \'f2\': 20},\n        >>>    0 : {\'f1\': 30},\n        >>> }\n\n        is converted to\n\n        >>> { -1 : namedtuple(f1=10, f2=20),\n        >>>    0 : namedtuple(f1=30),\n        >>> }\n\n        :param schema: schema used for conversion\n        :param ngram_as_dicts: ngram in the timestamp-to-dict mapping\n        :return: ngram in the timestamp-to-namedtuple mapping\n        """"""\n        ngram_as_tuples = {}\n        for timestamp in ngram_as_dicts.keys():\n            data_as_dict = ngram_as_dicts[timestamp]\n            current_schema = self.get_schema_at_timestep(schema=schema, timestep=timestamp)\n            ngram_as_tuples[timestamp] = current_schema.make_namedtuple(**data_as_dict)\n        return ngram_as_tuples\n\n    def get_field_names_at_all_timesteps(self):\n        """"""Returns a list of fields that are present at least in the one of the time offsets""""""\n        return list({field for fields in self._fields.values() for field in fields})\n\n    def convert_fields(self, unischema, field_list):\n        """"""Convert all the fields in field_list into Unischema fields.\n        field_list can contain unischema fields and strings (regular expressions)\n\n        :param unischema: Unischema object\n        :param field_list: A list of unischema fields or strings (regular expressions)\n        :return: list of unischema fields\n        """"""\n        # Split fields parameter to regex pattern strings and UnischemaField objects\n        regex_patterns = [f for f in field_list if isinstance(f, string_types)]\n        # We can not check type against UnischemaField because the artifact introduced by\n        # pickling, since depickled UnischemaField are of type collections.UnischemaField\n        # while withing depickling they are of petastorm.unischema.UnischemaField\n        # Since UnischemaField is a tuple, we check against it since it is invariant to\n        # pickling\n        unischema_field_objects = [f for f in field_list if isinstance(f, tuple)]\n\n        if len(unischema_field_objects) + len(regex_patterns) != len(field_list):\n            raise ValueError(\'""Elements of fields""/""timestamp field"" must be either a string (regular expressions) or\'\n                             \' an instance of UnischemaField class.\')\n\n        converted_fields = unischema_field_objects + match_unischema_fields(unischema, regex_patterns)\n\n        return converted_fields\n\n    def __eq__(self, other):\n        if set(self.fields.keys()) != set(other.fields.keys()):\n            return False\n\n        for key in self.fields.keys():\n            if set(self.fields[key]) != set(other.fields[key]):\n                return False\n\n        return True\n\n    def __ne__(self, other):\n        return not self == other\n'"
petastorm/predicates.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""\nPredicates for petastorm\n""""""\nimport abc\nimport collections\nimport hashlib\nimport numpy as np\nimport six\nimport sys\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass PredicateBase(object):\n    """""" Base class for row predicates """"""\n\n    @abc.abstractmethod\n    def get_fields(self):\n        pass\n\n    @abc.abstractmethod\n    def do_include(self, values):\n        pass\n\n\ndef _string_to_bucket(string, bucket_num):\n    hash_str = hashlib.md5(string.encode(\'utf-8\')).hexdigest()\n    return int(hash_str, 16) % bucket_num\n\n\nclass in_set(PredicateBase):\n    """""" Test if predicate_field value is in inclusion_values set """"""\n\n    def __init__(self, inclusion_values, predicate_field):\n        self._inclusion_values = set(inclusion_values)\n        self._predicate_field = predicate_field\n\n    def get_fields(self):\n        return {self._predicate_field}\n\n    def do_include(self, values):\n        return values[self._predicate_field] in self._inclusion_values\n\n\nclass in_intersection(PredicateBase):\n    """""" Test if predicate_field list contain at least one value from inclusion_values set """"""\n\n    def __init__(self, inclusion_values, _predicate_field):\n        self._inclusion_values = list(inclusion_values)\n        self._predicate_field = _predicate_field\n\n    def get_fields(self):\n        return {self._predicate_field}\n\n    def do_include(self, values):\n        if not isinstance(values[self._predicate_field], collections.Iterable):\n            raise ValueError(\'Predicate field should have iterable type\')\n        return any(np.in1d(values[self._predicate_field], self._inclusion_values))\n\n\nclass in_lambda(PredicateBase):\n    """""" Wrap up custom function to be used as a predicate\n        example: in_lambda([\'labels_object_roles\'], lambda labels_object_roles : len(labels_object_roles) > 3)\n    """"""\n\n    def __init__(self, predicate_fields, predicate_func, state_arg=None):\n        """"""\n        :param predicate_fields: list of fields to be used in predicate\n        :param predicate_func: predicate function\n               example: lambda labels_object_roles : len(labels_object_roles) > 3\n        :param state_arg: additional object to keep function state. it will be passed to\n               predicate_func after fields arguments ONLY if it is not None\n        """"""\n        if not isinstance(predicate_fields, list):\n            raise ValueError(\'Predicate fields should be a list\')\n        self._predicate_fields = predicate_fields\n        self._predicate_func = predicate_func\n        self._state_arg = state_arg\n\n    def get_fields(self):\n        return set(self._predicate_fields)\n\n    def do_include(self, values):\n        args = [values[field] for field in self._predicate_fields]\n        if self._state_arg is not None:\n            args.append(self._state_arg)\n        return self._predicate_func(*args)\n\n\nclass in_negate(PredicateBase):\n    """""" A predicate used to negate another predicate. """"""\n\n    def __init__(self, predicate):\n        if not isinstance(predicate, PredicateBase):\n            raise ValueError(\'Predicate is nor derived from PredicateBase\')\n\n        self._predicate = predicate\n\n    def get_fields(self):\n        return self._predicate.get_fields()\n\n    def do_include(self, values):\n        return not self._predicate.do_include(values)\n\n\nclass in_reduce(PredicateBase):\n    """""" A predicate used to aggregate other predicates using any reduce logical operation.""""""\n\n    def __init__(self, predicate_list, reduce_func):\n        """""" predicate_list: list of predicates\n            reduce_func: function to aggregate result of all predicates in the list\n            e.g. all() will implements logical \'And\', any() implements logical \'Or\'\n        """"""\n        check_list = [isinstance(p, PredicateBase) for p in predicate_list]\n        if not all(check_list):\n            raise ValueError(\'Predicate is nor derived from PredicateBase\')\n        self._predicate_list = predicate_list\n        self._reduce_func = reduce_func\n\n    def get_fields(self):\n        fields = set()\n        for p in self._predicate_list:\n            fields |= p.get_fields()\n        return fields\n\n    def do_include(self, values):\n        include_list = [p.do_include(values) for p in self._predicate_list]\n        return self._reduce_func(include_list)\n\n\nclass in_pseudorandom_split(PredicateBase):\n    """""" Split dataset according to a split list based on volume_guid.\n        The split is pseudorandom (can not supply the seed yet), i.e. the split outcome is always the same.\n        Split is performed by hashing volume_guid uniformly to 0:1 range and returning part of full dataset\n        which was hashed in given sub-range\n\n        Example:\n            \'split_list = [0.5, 0.2, 0.3]\' - dataset will be split on three subsets in proportion\n            subset 1: 0.5 of log data\n            subset 2: 0.2 of log data\n            subset 3: 0.3 of log data\n            Note, split is not exact, so avoid small fraction (e.g. 0.001) to avoid empty sets\n    """"""\n\n    def __init__(self, fraction_list, subset_index, predicate_field):\n        """""" split_list: a list of log fractions (real numbers in range [0:1])\n            subset_index: define which subset will be used by the Reader\n        """"""\n        if subset_index >= len(fraction_list):\n            raise ValueError(\'subset_index is out of range\')\n        self._predicate_field = predicate_field\n        # build CDF\n        subsets_high_borders = [sum(fraction_list[:i + 1]) for i in range(len(fraction_list))]\n        if subset_index:\n            fraction_low = subsets_high_borders[subset_index - 1]\n        else:\n            fraction_low = 0\n        fraction_high = subsets_high_borders[subset_index]\n        self._bucket_low = fraction_low * (sys.maxsize - 1)\n        self._bucket_high = fraction_high * (sys.maxsize - 1)\n\n    def get_fields(self):\n        return {self._predicate_field}\n\n    def do_include(self, values):\n        if self._predicate_field not in values.keys():\n            raise ValueError(\'Tested values does not have split key: %s\' % self._predicate_field)\n        bucket_idx = _string_to_bucket(str(values[self._predicate_field]), sys.maxsize)\n        return self._bucket_low <= bucket_idx < self._bucket_high\n'"
petastorm/py_dict_reader_worker.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom __future__ import division\n\nimport hashlib\nimport threading\n\nimport numpy as np\nfrom pyarrow import parquet as pq\nfrom pyarrow.parquet import ParquetFile\n\nfrom petastorm import utils\nfrom petastorm.cache import NullCache\nfrom petastorm.compat import compat_piece_read\nfrom petastorm.workers_pool import EmptyResultError\nfrom petastorm.workers_pool.worker_base import WorkerBase\n\n\ndef _merge_two_dicts(a, b):\n    """"""Merges two dictionaries together. If the same key is present in both input dictionaries, the value from \'b\'\n    dominates.""""""\n    result = a.copy()\n    result.update(b)\n    return result\n\n\ndef _apply_transform_spec(all_rows, transform_spec):\n    """"""Applies transform_spec.func to all rows in all_rows (if func is specified). Removes all fields as specified by\n    transform_spec.removed_fields, unless the fields were already deleted by transform_spec.func.\n\n    All modifications are performed in-place.\n    """"""\n    if transform_spec.func:\n        all_rows = [transform_spec.func(row) for row in all_rows]\n\n    for field_to_remove in transform_spec.removed_fields:\n        for row in all_rows:\n            if field_to_remove in row:\n                del row[field_to_remove]\n\n    return all_rows\n\n\ndef _select_cols(a_dict, keys):\n    """"""Filters out entries in a dictionary that have a key which is not part of \'keys\' argument. `a_dict` is not\n    modified and a new dictionary is returned.""""""\n    if keys == list(a_dict.keys()):\n        return a_dict\n    else:\n        return {field_name: a_dict[field_name] for field_name in keys}\n\n\nclass PyDictReaderWorkerResultsQueueReader(object):\n    def __init__(self):\n        self._result_buffer_lock = threading.Lock()\n        self._result_buffer = []\n\n    @property\n    def batched_output(self):\n        return False\n\n    def read_next(self, workers_pool, schema, ngram):\n        try:\n            # We are receiving decoded rows from the worker in chunks. We store the list internally\n            # and return a single item upon each consequent call to __next__\n            with self._result_buffer_lock:\n                if not self._result_buffer:\n                    # Reverse order, so we can pop from the end of the list in O(1) while maintaining\n                    # order the items are returned from the worker\n                    list_of_rows = list(reversed(workers_pool.get_results()))\n\n                    if ngram:\n                        for ngram_row in list_of_rows:\n                            for timestamp in ngram_row.keys():\n                                row = ngram_row[timestamp]\n                                schema_at_timestamp = ngram.get_schema_at_timestep(schema, timestamp)\n\n                                ngram_row[timestamp] = schema_at_timestamp.make_namedtuple(**row)\n                        self._result_buffer = list_of_rows\n                    else:\n                        self._result_buffer = [schema.make_namedtuple(**row) for row in list_of_rows]\n\n                return self._result_buffer.pop()\n\n        except EmptyResultError:\n            raise StopIteration\n\n\nclass PyDictReaderWorker(WorkerBase):\n    def __init__(self, worker_id, publish_func, args):\n        super(PyDictReaderWorker, self).__init__(worker_id, publish_func, args)\n\n        self._filesystem = args[0]\n        self._dataset_path = args[1]\n        self._schema = args[2]\n        self._ngram = args[3]\n        self._split_pieces = args[4]\n        self._local_cache = args[5]\n        self._transform_spec = args[6]\n\n        # We create datasets lazily in the first invocation of \'def process\'. This speeds up startup time since\n        # all Worker constructors are serialized\n        self._dataset = None\n\n    @staticmethod\n    def new_results_queue_reader():\n        return PyDictReaderWorkerResultsQueueReader()\n\n    # pylint: disable=arguments-differ\n    def process(self, piece_index, worker_predicate, shuffle_row_drop_partition):\n        """"""Main worker function. Loads and returns all rows matching the predicate from a rowgroup\n\n        Looks up the requested piece (a single row-group in a parquet file). If a predicate is specified,\n        columns needed by the predicate are loaded first. If no rows in the rowgroup matches the predicate criteria\n        the rest of the columns are not loaded.\n\n        :param piece_index:\n        :param shuffle_row_drop_partition: A tuple 2 of the current row drop partition and the total number\n            of partitions.\n        :return:\n        """"""\n\n        if not self._dataset:\n            self._dataset = pq.ParquetDataset(\n                self._dataset_path,\n                filesystem=self._filesystem,\n                validate_schema=False)\n\n        piece = self._split_pieces[piece_index]\n\n        # Create pyarrow file system\n        parquet_file = ParquetFile(self._dataset.fs.open(piece.path))\n\n        if not isinstance(self._local_cache, NullCache):\n            if worker_predicate:\n                raise RuntimeError(\'Local cache is not supported together with predicates, \'\n                                   \'unless the dataset is partitioned by the column the predicate operates on.\')\n            if shuffle_row_drop_partition[1] != 1:\n                raise RuntimeError(\'Local cache is not supported together with shuffle_row_drop_partitions > 1\')\n\n        if worker_predicate:\n            all_cols = self._load_rows_with_predicate(parquet_file, piece, worker_predicate, shuffle_row_drop_partition)\n        else:\n            # Using hash of the dataset path with the relative path in order to:\n            #  1. Make sure if a common cache serves multiple processes (e.g. redis), we don\'t have conflicts\n            #  2. Dataset path is hashed, to make sure we don\'t create too long keys, which maybe incompatible with\n            #     some cache implementations\n            #  3. Still leave relative path and the piece_index in plain text to make it easier to debug\n            cache_key = \'{}:{}:{}\'.format(hashlib.md5(self._dataset_path.encode(\'utf-8\')).hexdigest(),\n                                          piece.path, piece_index)\n            all_cols = self._local_cache.get(cache_key,\n                                             lambda: self._load_rows(parquet_file, piece, shuffle_row_drop_partition))\n\n        if self._ngram:\n            all_cols = self._ngram.form_ngram(data=all_cols, schema=self._schema)\n\n        if all_cols:\n            self.publish_func(all_cols)\n\n    def _load_rows(self, pq_file, piece, shuffle_row_drop_range):\n        """"""Loads all rows from a piece""""""\n\n        # pyarrow would fail if we request a column names that the dataset is partitioned by, so we strip them from\n        # the `columns` argument.\n        partitions = self._dataset.partitions\n        column_names = set(field.name for field in self._schema.fields.values()) - partitions.partition_names\n\n        all_rows = self._read_with_shuffle_row_drop(piece, pq_file, column_names, shuffle_row_drop_range)\n\n        all_rows = [utils.decode_row(row, self._schema) for row in all_rows]\n\n        if self._transform_spec:\n            all_rows = _apply_transform_spec(all_rows, self._transform_spec)\n\n        return all_rows\n\n    def _load_rows_with_predicate(self, pq_file, piece, worker_predicate, shuffle_row_drop_partition):\n        """"""Loads all rows that match a predicate from a piece""""""\n\n        # 1. Read all columns needed by predicate and decode\n        # 2. Apply the predicate. If nothing matches, exit early\n        # 3. Read the remaining columns and decode\n        # 4. Combine with columns already decoded for the predicate.\n\n        # Split all column names into ones that are needed by predicateand the rest.\n        predicate_column_names = set(worker_predicate.get_fields())\n\n        if not predicate_column_names:\n            raise ValueError(\'At least one field name must be returned by predicate\\\'s get_field() method\')\n\n        all_schema_names = set(field.name for field in self._schema.fields.values())\n\n        invalid_column_names = predicate_column_names - all_schema_names\n        if invalid_column_names:\n            raise ValueError(\'At least some column names requested by the predicate ({}) \'\n                             \'are not valid schema names: ({})\'.format(\', \'.join(invalid_column_names),\n                                                                       \', \'.join(all_schema_names)))\n\n        other_column_names = all_schema_names - predicate_column_names - self._dataset.partitions.partition_names\n\n        # Read columns needed for the predicate\n        predicate_rows = self._read_with_shuffle_row_drop(piece, pq_file, predicate_column_names,\n                                                          shuffle_row_drop_partition)\n\n        # Decode values\n        decoded_predicate_rows = [\n            utils.decode_row(_select_cols(row, predicate_column_names), self._schema)\n            for row in predicate_rows]\n\n        # Use the predicate to filter\n        match_predicate_mask = [worker_predicate.do_include(row) for row in decoded_predicate_rows]\n\n        # Don\'t have anything left after filtering? Exit early.\n        if not any(match_predicate_mask):\n            return []\n\n        # Remove rows that were filtered out by the predicate\n        filtered_decoded_predicate_rows = [row for i, row in enumerate(decoded_predicate_rows) if\n                                           match_predicate_mask[i]]\n\n        if other_column_names:\n            # Read remaining columns\n            other_rows = self._read_with_shuffle_row_drop(piece, pq_file, other_column_names,\n                                                          shuffle_row_drop_partition)\n\n            # Remove rows that were filtered out by the predicate\n            filtered_other_rows = [row for i, row in enumerate(other_rows) if match_predicate_mask[i]]\n\n            # Decode remaining columns\n            decoded_other_rows = [utils.decode_row(row, self._schema) for row in filtered_other_rows]\n\n            # Merge predicate needed columns with the remaining\n            all_cols = [_merge_two_dicts(a, b) for a, b in zip(decoded_other_rows, filtered_decoded_predicate_rows)]\n            result = all_cols\n        else:\n            result = filtered_decoded_predicate_rows\n\n        if self._transform_spec:\n            result = _apply_transform_spec(result, self._transform_spec)\n\n        return result\n\n    def _read_with_shuffle_row_drop(self, piece, pq_file, column_names, shuffle_row_drop_partition):\n        # If integer_object_nulls is set to False, nullable integer fields are return as floats\n        # with nulls translated to nans\n        data_frame = compat_piece_read(piece, lambda _: pq_file, columns=column_names,\n                                       partitions=self._dataset.partitions).to_pandas(integer_object_nulls=True)\n\n        num_rows = len(data_frame)\n        num_partitions = shuffle_row_drop_partition[1]\n        this_partition = shuffle_row_drop_partition[0]\n\n        partition_indexes = np.floor(np.arange(num_rows) / (float(num_rows) / min(num_rows, num_partitions)))\n\n        if self._ngram:\n            # If we have an ngram we need to take elements from the next partition to build the sequence\n            next_partition_indexes = np.where(partition_indexes >= this_partition + 1)\n            if next_partition_indexes[0].size:\n                next_partition_to_add = next_partition_indexes[0][0:self._ngram.length - 1]\n                partition_indexes[next_partition_to_add] = this_partition\n\n        selected_dataframe = data_frame.loc[partition_indexes == this_partition]\n        return selected_dataframe.to_dict(\'records\')\n'"
petastorm/pytorch.py,2,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport collections\nimport decimal\n# Must import pyarrow before torch. See: https://github.com/uber/petastorm/blob/master/docs/troubleshoot.rst\nimport re\nimport logging\nimport numpy as np\nfrom six import PY2\nfrom torch.utils.data.dataloader import default_collate\nimport torch\nfrom packaging import version\n\nfrom petastorm.reader_impl.shuffling_buffer import RandomShufflingBuffer, NoopShufflingBuffer\nfrom petastorm.reader_impl.pytorch_shuffling_buffer import BatchedRandomShufflingBuffer, BatchedNoopShufflingBuffer\n\n_TORCH_BEFORE_1_1 = version.parse(torch.__version__) < version.parse(\'1.1.0\')\n\nif PY2:\n    _string_classes = basestring  # noqa: F821\nelse:\n    _string_classes = (str, bytes)\n\nlogger = logging.getLogger(__name__)\n\n\ndef _sanitize_pytorch_types(row_as_dict):\n    """"""Promotes values types in a dictionary to the types supported by pytorch. Raises an error if type is clear error\n    if the type can not be promoted.\n\n    The parameter is modified in-place.\n\n    int8, uint16 are promoted to int32; uint32 -> int64;\n    numpy string_, unicode_, object arrays are not supported.\n\n    :param dict[str,obj] row_as_dict: a dictionary of key-value pairs. The values types are promoted to\n        pytorch compatible.\n    :return: None\n    """"""\n    for name, value in row_as_dict.items():\n        # PyTorch supported types are: double, float, float16, int64, int32, and uint8\n        if isinstance(value, np.ndarray):\n            if value.dtype == np.int8 and _TORCH_BEFORE_1_1:\n                row_as_dict[name] = value.astype(np.int16)\n            elif value.dtype == np.uint16:\n                row_as_dict[name] = value.astype(np.int32)\n            elif value.dtype == np.uint32:\n                row_as_dict[name] = value.astype(np.int64)\n            elif value.dtype == np.bool_:\n                row_as_dict[name] = value.astype(np.uint8)\n            elif re.search(\'[SaUO]\', value.dtype.str):\n                raise TypeError(\'Pytorch does not support arrays of string or object classes. \'\n                                \'Found in field {}.\'.format(name))\n        elif isinstance(value, np.bool_):\n            row_as_dict[name] = np.uint8(value)\n        elif value is None:\n            raise TypeError(\'Pytorch does not support nullable fields. Found None in {}\'.format(name))\n\n\ndef decimal_friendly_collate(batch):\n    """"""A wrapper on top of ``default_collate`` function that allows decimal.Decimal types to be collated.\n\n    We use ``decimal.Decimal`` types in petastorm dataset to represent timestamps. PyTorch\'s ``default_collate``\n    implementation does not support collating ``decimal.Decimal`` types. ``decimal_friendly_collate`` collates\n    ``decimal.Decimal`` separately and then combines with the rest of the fields collated by a standard\n    ``default_collate``.\n\n    :param batch: A list of dictionaries to collate\n    :return: A dictionary of lists/pytorch.Tensor types\n    """"""\n\n    if isinstance(batch[0], decimal.Decimal):\n        return batch\n    elif isinstance(batch[0], collections.Mapping):\n        return {key: decimal_friendly_collate([d[key] for d in batch]) for key in batch[0]}\n    elif isinstance(batch[0], _string_classes):\n        return batch\n    elif isinstance(batch[0], collections.Sequence):\n        transposed = zip(*batch)\n        return [decimal_friendly_collate(samples) for samples in transposed]\n    else:\n        return default_collate(batch)\n\n\n_PARALLEL_ITER_ERROR = ""You must finish a full pass of Petastorm DataLoader before making another pass from the \\\nbeginning.If you do need to terminate early and restart from beginning, please re-create the reader and the data \\\nloader.""\n\n\nclass LoaderBase(object):\n\n    def __init__(self):\n        self._in_iter = None\n        self._error = None\n\n    def __iter__(self):\n        if self._error is not None:\n            raise RuntimeError(\'Cannot start a new iteration because last time iteration failed with error {err}.\'\n                               .format(err=repr(self._error)))\n        if self._in_iter is not None and self._in_iter == True:  # noqa: E712\n            raise RuntimeError(_PARALLEL_ITER_ERROR)\n        if self._in_iter is not None:\n            self.reader.reset()\n            logger.warning(\'Start a new pass of Petastorm DataLoader, reset underlying Petastorm reader to position 0.\')\n        self._in_iter = True\n\n        try:\n            for batch in self._iter_impl():\n                yield batch\n        except Exception as e:\n            self._error = e\n            logger.error(\'Iteration on Petastorm DataLoader raise error: %s\', repr(e))\n            raise\n        finally:\n            self._in_iter = False\n\n\nclass DataLoader(LoaderBase):\n    """"""\n    A data loader adaptor for ``torch.utils.data.DataLoader``.\n\n    This class iterates and returns items from the Reader in batches.\n\n    This loader can be used as an iterator and will terminate when the reader used in the construction of the class\n    runs out of samples.\n    """"""\n\n    def __init__(self, reader, batch_size=1, collate_fn=decimal_friendly_collate,\n                 shuffling_queue_capacity=0):\n        """"""\n        Initializes a data loader object, with a default collate.\n\n        Number of epochs is defined by the configuration of the reader argument.\n\n        An optional shuffling queue is created if shuffling_queue_capacity is greater than 0. No samples will be\n        returned to a user by the ``DataLoader`` until the queue is full. After that, batches of `batch_size`\n        will be created by uniformly sampling the shuffling queue. Once no more samples are available from the data\n        reader, the shuffling queue is allowed to be consumed till no further samples are available.\n\n        Note that the last returned batch could have less then ``batch_size`` samples.\n\n        NOTE: if you are using ``make_batch_reader``, this shuffling queue will be randomizing the order of the\n        entire batches and not changing the order of elements within a batch. This is likely not what you intend to do.\n\n        :param reader: petastorm Reader instance\n        :param batch_size: the number of items to return per batch; factored into the len() of this reader\n        :param collate_fn: an optional callable to merge a list of samples to form a mini-batch.\n        :param shuffling_queue_capacity: Queue capacity is passed to the underlying :class:`tf.RandomShuffleQueue`\n          instance. If set to 0, no suffling will be done.\n        """"""\n        super(DataLoader, self).__init__()\n        self.reader = reader\n        self.batch_size = batch_size\n        self.collate_fn = collate_fn\n\n        # _batch_acc accumulates samples for a single batch.\n        self._batch_acc = []\n        self.shuffling_queue_capacity = shuffling_queue_capacity\n        self._in_iter = None\n\n    def _iter_impl(self):\n        """"""\n        The Data Loader iterator stops the for-loop when reader runs out of samples.\n        """"""\n        # As we iterate over incoming samples, we are going to store them in `self._batch_acc`, until we have a batch of\n        # the requested batch_size ready.\n\n        keys = None\n        if self.shuffling_queue_capacity > 0:\n            # We can not know what is the reasonable number to use for the extra capacity, so we set a huge number\n            # and give up on the unbound growth protection mechanism.\n            min_after_dequeue = self.shuffling_queue_capacity - 1\n            self._shuffling_buffer = RandomShufflingBuffer(self.shuffling_queue_capacity,\n                                                           min_after_retrieve=min_after_dequeue,\n                                                           extra_capacity=100000000)\n        else:\n            self._shuffling_buffer = NoopShufflingBuffer()\n\n        for row in self.reader:\n            # Default collate does not work nicely on namedtuples and treat them as lists\n            # Using dict will result in the yielded structures being dicts as well\n            row_as_dict = row._asdict()\n\n            keys = row_as_dict.keys()\n\n            # Promote some types that are incompatible with pytorch to be pytorch friendly.\n            _sanitize_pytorch_types(row_as_dict)\n\n            # Add rows to shuffling buffer\n            if not self.reader.is_batched_reader:\n                self._shuffling_buffer.add_many([row_as_dict])\n            else:\n                # Transposition:\n                #   row_as_dict:        {\'a\': [1,2,3], \'b\':[4,5,6]}\n                #   row_group_as_tuple: [(1, 4), (2, 5), (3, 6)]\n                # The order within a tuple is defined by key order in \'keys\'\n                row_group_as_tuple = list(zip(*(row_as_dict[k] for k in keys)))\n\n                # Adding data as \'row-by-row\' into a shuffling buffer. This is a pretty\n                # slow implementation though. Probably can comeup with a faster way to shuffle,\n                # perhaps at the expense of a larger memory consumption...\n                self._shuffling_buffer.add_many(row_group_as_tuple)\n\n            # _yield_batches will emit as much batches as are allowed by the shuffling_buffer (RandomShufflingBuffer\n            # will avoid underflowing below a certain number of samples to guarantee some samples decorrelation)\n            for batch in self._yield_batches(keys):\n                yield batch\n\n        # Once reader can not read new rows, we might still have a bunch of rows waiting in the shuffling buffer.\n        # Telling shuffling buffer that we are finished allows to deplete the buffer completely, regardless its\n        # min_after_dequeue setting.\n        self._shuffling_buffer.finish()\n\n        for batch in self._yield_batches(keys):\n            yield batch\n\n        # Yield the last and partial batch\n        if self._batch_acc:\n            yield self.collate_fn(self._batch_acc)\n\n    def _yield_batches(self, keys):\n        while self._shuffling_buffer.can_retrieve():\n            post_shuffled_row = self._shuffling_buffer.retrieve()\n            if not isinstance(post_shuffled_row, dict):\n                # This is for the case of batched reads. Here we restore back the\n                # dictionary format of records\n                post_shuffled_row = dict(zip(keys, post_shuffled_row))\n\n            self._batch_acc.append(post_shuffled_row)\n\n            # Batch is ready? Collate and emmit\n            if len(self._batch_acc) == self.batch_size:\n                yield self.collate_fn(self._batch_acc)\n                self._batch_acc = []\n\n    # Functions needed to treat data loader as a context manager\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.reader.stop()\n        self.reader.join()\n\n\nclass BatchedDataLoader(LoaderBase):\n    """"""\n    Same as DataLoader except it uses torch-based shuffling buffers which enable batched buffering\n    (significantly faster for small data).\n    """"""\n\n    def __init__(self, reader, batch_size=1, transform_fn=None,\n                 shuffling_queue_capacity=0):\n        """"""\n        Initializes a data loader object.\n\n        Number of epochs is defined by the configuration of the reader argument.\n\n        An optional shuffling queue is created if shuffling_queue_capacity is greater than 0. No samples will be\n        returned to a user by the ``BatchedDataLoader`` until the queue is full. After that, batches of `batch_size`\n        will be created by uniformly sampling the shuffling queue. Once no more samples are available from the data\n        reader, the shuffling queue is allowed to be consumed till no further samples are available.\n\n        Note that the last returned batch could have less then ``batch_size`` samples.\n\n        NOTE: if you are using ``make_batch_reader``, this shuffling queue will be randomizing the order of the\n        entire batches and not changing the order of elements within a batch. This is likely not what you intend to do.\n\n        This class does not support special types that are not supported in PyTorch (decimal/string).\n\n        :param reader: petastorm Reader instance\n        :param batch_size: the number of items to return per batch; factored into the len() of this reader\n        :param transform_fn: an optional callable to convert batches from the reader to PyTorch tensors\n        :param shuffling_queue_capacity: Queue capacity is passed to the underlying :class:`tf.RandomShuffleQueue`\n          instance. If set to 0, no suffling will be done.\n        """"""\n        super(BatchedDataLoader, self).__init__()\n        self.reader = reader\n        self.batch_size = batch_size\n        self.transform_fn = transform_fn or torch.as_tensor\n\n        # _batch_acc accumulates samples for a single batch.\n        self._batch_acc = []\n        self.shuffling_queue_capacity = shuffling_queue_capacity\n        self._in_iter = None\n\n    def _iter_impl(self):\n        """"""\n        The Data Loader iterator stops the for-loop when reader runs out of samples.\n        """"""\n        # As we iterate over incoming samples, we are going to store them in `self._batch_acc`, until we have a batch of\n        # the requested batch_size ready.\n\n        keys = None\n        if self.shuffling_queue_capacity > 0:\n            # We can not know what is the reasonable number to use for the extra capacity, so we set a huge number\n            # and give up on the unbound growth protection mechanism.\n            # To keep the same behavior as DataLoader, we need to increase the shuffling_queue_capacity\n            min_after_dequeue = self.shuffling_queue_capacity - 1\n            shuffling_queue_capacity = min_after_dequeue + self.batch_size\n            self._shuffling_buffer = BatchedRandomShufflingBuffer(\n                shuffling_queue_capacity,\n                min_after_retrieve=min_after_dequeue,\n                extra_capacity=100000000,\n                batch_size=self.batch_size\n            )\n        else:\n            self._shuffling_buffer = BatchedNoopShufflingBuffer(batch_size=self.batch_size)\n\n        for row in self.reader:\n            # Default collate does not work nicely on namedtuples and treat them as lists\n            # Using dict will result in the yielded structures being dicts as well\n            row_as_dict = row._asdict()\n\n            keys = row_as_dict.keys()\n\n            # Promote some types that are incompatible with pytorch to be pytorch friendly.\n            _sanitize_pytorch_types(row_as_dict)\n\n            # Add rows to shuffling buffer\n            for k, v in row_as_dict.items():\n                if not self.reader.is_batched_reader:\n                    row_as_dict[k] = self.transform_fn([v])\n                else:\n                    row_as_dict[k] = self.transform_fn(v)\n            self._shuffling_buffer.add_many(row_as_dict.values())\n\n            # _yield_batches will emit as much batches as are allowed by the shuffling_buffer (RandomShufflingBuffer\n            # will avoid underflowing below a certain number of samples to guarantee some samples decorrelation)\n            for batch in self._yield_batches(keys):\n                yield batch\n\n        # Once reader can not read new rows, we might still have a bunch of rows waiting in the shuffling buffer.\n        # Telling shuffling buffer that we are finished allows to deplete the buffer completely, regardless its\n        # min_after_dequeue setting.\n        self._shuffling_buffer.finish()\n\n        for batch in self._yield_batches(keys):\n            yield batch\n\n    def _yield_batches(self, keys):\n        while self._shuffling_buffer.can_retrieve():\n            batch = self._shuffling_buffer.retrieve()\n            if not isinstance(batch, dict):\n                # This is for the case of batched reads. Here we restore back the\n                # dictionary format of records\n                batch = dict(zip(keys, batch))\n            yield batch\n\n    # Functions needed to treat data loader as a context manager\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.reader.stop()\n        self.reader.join()\n'"
petastorm/reader.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport collections\nimport logging\nimport warnings\n\nimport six\nfrom pyarrow import parquet as pq\n\nfrom petastorm.arrow_reader_worker import ArrowReaderWorker\nfrom petastorm.cache import NullCache\nfrom petastorm.errors import NoDataAvailableError\nfrom petastorm.etl import dataset_metadata, rowgroup_indexing\nfrom petastorm.etl.dataset_metadata import PetastormMetadataError, infer_or_load_unischema\nfrom petastorm.fs_utils import get_filesystem_and_path_or_paths, normalize_dir_url\nfrom petastorm.local_disk_arrow_table_cache import LocalDiskArrowTableCache\nfrom petastorm.local_disk_cache import LocalDiskCache\nfrom petastorm.ngram import NGram\nfrom petastorm.predicates import PredicateBase\nfrom petastorm.py_dict_reader_worker import PyDictReaderWorker\nfrom petastorm.reader_impl.arrow_table_serializer import ArrowTableSerializer\nfrom petastorm.reader_impl.pickle_serializer import PickleSerializer\nfrom petastorm.reader_impl.pyarrow_serializer import PyArrowSerializer\nfrom petastorm.selectors import RowGroupSelectorBase\nfrom petastorm.transform import transform_schema\nfrom petastorm.workers_pool.dummy_pool import DummyPool\nfrom petastorm.workers_pool.process_pool import ProcessPool\nfrom petastorm.workers_pool.thread_pool import ThreadPool\nfrom petastorm.workers_pool.ventilator import ConcurrentVentilator\n\nlogger = logging.getLogger(__name__)\n\n# Ventilator guarantees that no more than workers + _VENTILATE_EXTRA_ROWGROUPS are processed at a moment by a\n# worker pool. This guarantees that we don\'t run out of memory if data consumer is slower than the Reader.\n_VENTILATE_EXTRA_ROWGROUPS = 2\n\n\ndef normalize_dataset_url_or_urls(dataset_url_or_urls):\n    if isinstance(dataset_url_or_urls, list):\n        if not dataset_url_or_urls:\n            raise ValueError(\'dataset url list must be non-empty.\')\n        return [normalize_dir_url(url) for url in dataset_url_or_urls]\n    else:\n        return normalize_dir_url(dataset_url_or_urls)\n\n\ndef make_reader(dataset_url,\n                schema_fields=None,\n                reader_pool_type=\'thread\', workers_count=10, pyarrow_serialize=False, results_queue_size=50,\n                shuffle_row_groups=True, shuffle_row_drop_partitions=1,\n                predicate=None,\n                rowgroup_selector=None,\n                num_epochs=1,\n                cur_shard=None, shard_count=None,\n                cache_type=\'null\', cache_location=None, cache_size_limit=None,\n                cache_row_size_estimate=None, cache_extra_settings=None,\n                hdfs_driver=\'libhdfs3\',\n                transform_spec=None):\n    """"""\n    Creates an instance of Reader for reading Petastorm datasets. A Petastorm dataset is a dataset generated using\n    :func:`~petastorm.etl.dataset_metadata.materialize_dataset` context manager as explained\n    `here <https://petastorm.readthedocs.io/en/latest/readme_include.html#generating-a-dataset>`_.\n\n    See :func:`~petastorm.make_batch_reader` to read from a Parquet store that was not generated using\n    :func:`~petastorm.etl.dataset_metadata.materialize_dataset`.\n\n    :param dataset_url: an filepath or a url to a parquet directory,\n        e.g. ``\'hdfs://some_hdfs_cluster/user/yevgeni/parquet8\'``, or ``\'file:///tmp/mydataset\'``,\n        or ``\'s3://bucket/mydataset\'``, or ``\'gs://bucket/mydataset\'``.\n    :param schema_fields: Can be: a list of unischema fields and/or regex pattern strings; ``None`` to read all fields;\n            an NGram object, then it will return an NGram of the specified fields.\n    :param reader_pool_type: A string denoting the reader pool type. Should be one of [\'thread\', \'process\', \'dummy\']\n        denoting a thread pool, process pool, or running everything in the master thread. Defaults to \'thread\'\n    :param workers_count: An int for the number of workers to use in the reader pool. This only is used for the\n        thread or process pool. Defaults to 10\n    :param pyarrow_serialize: Whether to use pyarrow for serialization. Currently only applicable to process pool.\n        Defaults to False.\n    :param results_queue_size: Size of the results queue to store prefetched row-groups. Currently only applicable to\n        thread reader pool type.\n    :param shuffle_row_groups: Whether to shuffle row groups (the order in which full row groups are read)\n    :param shuffle_row_drop_partitions: This is is a positive integer which determines how many partitions to\n        break up a row group into for increased shuffling in exchange for worse performance (extra reads).\n        For example if you specify 2 each row group read will drop half of the rows within every row group and\n        read the remaining rows in separate reads. It is recommended to keep this number below the regular row\n        group size in order to not waste reads which drop all rows.\n    :param predicate: instance of :class:`.PredicateBase` object to filter rows to be returned by reader. The predicate\n        will be passed a single row and must return a boolean value indicating whether to include it in the results.\n    :param rowgroup_selector: instance of row group selector object to select row groups to be read\n    :param num_epochs: An epoch is a single pass over all rows in the dataset. Setting ``num_epochs`` to\n        ``None`` will result in an infinite number of epochs.\n    :param cur_shard: An int denoting the current shard number. Each node reading a shard should\n        pass in a unique shard number in the range [0, shard_count). shard_count must be supplied as well.\n        Defaults to None\n    :param shard_count: An int denoting the number of shards to break this dataset into. Defaults to None\n    :param cache_type: A string denoting the cache type, if desired. Options are [None, \'null\', \'local-disk\'] to\n        either have a null/noop cache or a cache implemented using diskcache. Caching is useful when communication\n        to the main data store is either slow or expensive and the local machine has large enough storage\n        to store entire dataset (or a partition of a dataset if shard_count is used). By default will be a null cache.\n    :param cache_location: A string denoting the location or path of the cache.\n    :param cache_size_limit: An int specifying the size limit of the cache in bytes\n    :param cache_row_size_estimate: An int specifying the estimated size of a row in the dataset\n    :param cache_extra_settings: A dictionary of extra settings to pass to the cache implementation,\n    :param hdfs_driver: A string denoting the hdfs driver to use (if using a dataset on hdfs). Current choices are\n        libhdfs (java through JNI) or libhdfs3 (C++)\n    :param transform_spec: An instance of :class:`~petastorm.transform.TransformSpec` object defining how a record\n        is transformed after it is loaded and decoded. The transformation occurs on a worker thread/process (depends\n        on the ``reader_pool_type`` value).\n    :return: A :class:`Reader` object\n    """"""\n    dataset_url = normalize_dir_url(dataset_url)\n\n    filesystem, dataset_path = get_filesystem_and_path_or_paths(dataset_url, hdfs_driver)\n\n    if cache_type is None or cache_type == \'null\':\n        cache = NullCache()\n    elif cache_type == \'local-disk\':\n        cache = LocalDiskCache(cache_location, cache_size_limit, cache_row_size_estimate, **cache_extra_settings or {})\n    else:\n        raise ValueError(\'Unknown cache_type: {}\'.format(cache_type))\n\n    try:\n        dataset_metadata.get_schema_from_dataset_url(dataset_url, hdfs_driver=hdfs_driver)\n    except PetastormMetadataError:\n        raise RuntimeError(\'Currently make_reader supports reading only Petastorm datasets. \'\n                           \'To read from a non-Petastorm Parquet store use make_batch_reader\')\n\n    if reader_pool_type == \'thread\':\n        reader_pool = ThreadPool(workers_count, results_queue_size)\n    elif reader_pool_type == \'process\':\n        if pyarrow_serialize:\n            serializer = PyArrowSerializer()\n        else:\n            serializer = PickleSerializer()\n        reader_pool = ProcessPool(workers_count, serializer)\n    elif reader_pool_type == \'dummy\':\n        reader_pool = DummyPool()\n    else:\n        raise ValueError(\'Unknown reader_pool_type: {}\'.format(reader_pool_type))\n\n    kwargs = {\n        \'schema_fields\': schema_fields,\n        \'reader_pool\': reader_pool,\n        \'shuffle_row_groups\': shuffle_row_groups,\n        \'shuffle_row_drop_partitions\': shuffle_row_drop_partitions,\n        \'predicate\': predicate,\n        \'rowgroup_selector\': rowgroup_selector,\n        \'num_epochs\': num_epochs,\n        \'cur_shard\': cur_shard,\n        \'shard_count\': shard_count,\n        \'cache\': cache,\n        \'transform_spec\': transform_spec,\n    }\n\n    try:\n        return Reader(filesystem, dataset_path,\n                      worker_class=PyDictReaderWorker,\n                      is_batched_reader=False,\n                      **kwargs)\n    except PetastormMetadataError as e:\n        logger.error(\'Unexpected exception: %s\', str(e))\n        raise RuntimeError(\'make_reader has failed. If you were trying to open a Parquet store that was not \'\n                           \'created using Petastorm materialize_dataset and it contains only scalar columns, \'\n                           \'you may use make_batch_reader to read it.\\n\'\n                           \'Inner exception: %s\', str(e))\n\n\ndef make_batch_reader(dataset_url_or_urls,\n                      schema_fields=None,\n                      reader_pool_type=\'thread\', workers_count=10,\n                      shuffle_row_groups=True, shuffle_row_drop_partitions=1,\n                      predicate=None,\n                      rowgroup_selector=None,\n                      num_epochs=1,\n                      cur_shard=None, shard_count=None,\n                      cache_type=\'null\', cache_location=None, cache_size_limit=None,\n                      cache_row_size_estimate=None, cache_extra_settings=None,\n                      hdfs_driver=\'libhdfs3\',\n                      transform_spec=None):\n    """"""\n    Creates an instance of Reader for reading batches out of a non-Petastorm Parquet store.\n\n    Currently, only stores having native scalar parquet data types are supported.\n    Use :func:`~petastorm.make_reader` to read Petastorm Parquet stores generated with\n    :func:`~petastorm.etl.dataset_metadata.materialize_dataset`.\n\n    NOTE: only scalar columns or array type (of primitive type element) columns are currently supported.\n\n    NOTE: If without `schema_fields` specified, the reader schema will be inferred from parquet dataset. then the\n    reader schema fields order will preserve parqeut dataset fields order (partition column come first), but if\n    setting `transform_spec` and specified `TransformSpec.selected_fields`, then the reader schema fields order\n    will be the order of \'selected_fields\'.\n\n    :param dataset_url_or_urls: a url to a parquet directory or a url list (with the same scheme) to parquet files.\n        e.g. ``\'hdfs://some_hdfs_cluster/user/yevgeni/parquet8\'``, or ``\'file:///tmp/mydataset\'``,\n        or ``\'s3://bucket/mydataset\'``, or ``\'gs://bucket/mydataset\'``,\n        or ``[file:///tmp/mydataset/00000.parquet, file:///tmp/mydataset/00001.parquet]``.\n    :param schema_fields: A list of regex pattern strings. Only columns matching at least one of the\n        patterns in the list will be loaded.\n    :param reader_pool_type: A string denoting the reader pool type. Should be one of [\'thread\', \'process\', \'dummy\']\n        denoting a thread pool, process pool, or running everything in the master thread. Defaults to \'thread\'\n    :param workers_count: An int for the number of workers to use in the reader pool. This only is used for the\n        thread or process pool. Defaults to 10\n    :param shuffle_row_groups: Whether to shuffle row groups (the order in which full row groups are read)\n    :param shuffle_row_drop_partitions: This is is a positive integer which determines how many partitions to\n        break up a row group into for increased shuffling in exchange for worse performance (extra reads).\n        For example if you specify 2 each row group read will drop half of the rows within every row group and\n        read the remaining rows in separate reads. It is recommended to keep this number below the regular row\n        group size in order to not waste reads which drop all rows.\n    :param predicate: instance of :class:`.PredicateBase` object to filter rows to be returned by reader. The predicate\n        will be passed a pandas DataFrame object and must return a pandas Series with boolean values of matching\n        dimensions.\n    :param rowgroup_selector: instance of row group selector object to select row groups to be read\n    :param num_epochs: An epoch is a single pass over all rows in the dataset. Setting ``num_epochs`` to\n        ``None`` will result in an infinite number of epochs.\n    :param cur_shard: An int denoting the current shard number. Each node reading a shard should\n        pass in a unique shard number in the range [0, shard_count). shard_count must be supplied as well.\n        Defaults to None\n    :param shard_count: An int denoting the number of shards to break this dataset into. Defaults to None\n    :param cache_type: A string denoting the cache type, if desired. Options are [None, \'null\', \'local-disk\'] to\n        either have a null/noop cache or a cache implemented using diskcache. Caching is useful when communication\n        to the main data store is either slow or expensive and the local machine has large enough storage\n        to store entire dataset (or a partition of a dataset if shard_count is used). By default will be a null cache.\n    :param cache_location: A string denoting the location or path of the cache.\n    :param cache_size_limit: An int specifying the size limit of the cache in bytes\n    :param cache_row_size_estimate: An int specifying the estimated size of a row in the dataset\n    :param cache_extra_settings: A dictionary of extra settings to pass to the cache implementation,\n    :param hdfs_driver: A string denoting the hdfs driver to use (if using a dataset on hdfs). Current choices are\n        libhdfs (java through JNI) or libhdfs3 (C++)\n    :param transform_spec: An instance of :class:`~petastorm.transform.TransformSpec` object defining how a record\n        is transformed after it is loaded and decoded. The transformation occurs on a worker thread/process (depends\n        on the ``reader_pool_type`` value).\n    :return: A :class:`Reader` object\n    """"""\n    dataset_url_or_urls = normalize_dataset_url_or_urls(dataset_url_or_urls)\n\n    filesystem, dataset_path_or_paths = get_filesystem_and_path_or_paths(dataset_url_or_urls, hdfs_driver)\n\n    try:\n        dataset_metadata.get_schema_from_dataset_url(dataset_url_or_urls, hdfs_driver=hdfs_driver)\n        warnings.warn(\'Please use make_reader (instead of \\\'make_batch_dataset\\\' function to read this dataset. \'\n                      \'You may get unexpected results. \'\n                      \'Currently make_batch_reader supports reading only Parquet stores that contain \'\n                      \'standard Parquet data types and do not require petastorm decoding.\')\n    except PetastormMetadataError:\n        pass\n\n    if cache_type is None or cache_type == \'null\':\n        cache = NullCache()\n    elif cache_type == \'local-disk\':\n        cache = LocalDiskArrowTableCache(cache_location, cache_size_limit, cache_row_size_estimate,\n                                         **cache_extra_settings or {})\n    else:\n        raise ValueError(\'Unknown cache_type: {}\'.format(cache_type))\n\n    if reader_pool_type == \'thread\':\n        reader_pool = ThreadPool(workers_count)\n    elif reader_pool_type == \'process\':\n        serializer = ArrowTableSerializer()\n        reader_pool = ProcessPool(workers_count, serializer)\n    elif reader_pool_type == \'dummy\':\n        reader_pool = DummyPool()\n    else:\n        raise ValueError(\'Unknown reader_pool_type: {}\'.format(reader_pool_type))\n\n    return Reader(filesystem, dataset_path_or_paths,\n                  schema_fields=schema_fields,\n                  worker_class=ArrowReaderWorker,\n                  reader_pool=reader_pool,\n                  shuffle_row_groups=shuffle_row_groups,\n                  shuffle_row_drop_partitions=shuffle_row_drop_partitions,\n                  predicate=predicate,\n                  rowgroup_selector=rowgroup_selector,\n                  num_epochs=num_epochs,\n                  cur_shard=cur_shard,\n                  shard_count=shard_count,\n                  cache=cache,\n                  transform_spec=transform_spec,\n                  is_batched_reader=True)\n\n\nclass Reader(object):\n    """"""Reads a dataset from a Petastorm dataset.\n\n    :ivar last_row_consumed: True if the last row was already returned by the Reader.\n    """"""\n\n    def __init__(self, pyarrow_filesystem, dataset_path, schema_fields=None,\n                 shuffle_row_groups=True, shuffle_row_drop_partitions=1,\n                 predicate=None, rowgroup_selector=None, reader_pool=None, num_epochs=1,\n                 cur_shard=None, shard_count=None, cache=None, worker_class=None,\n                 transform_spec=None, is_batched_reader=False):\n        """"""Initializes a reader object.\n\n        :param pyarrow_filesystem: An instance of ``pyarrow.FileSystem`` that will be used. If not specified,\n            then a default one will be selected based on the url (only for ``hdfs://`` or ``file://``; for\n            ``s3://`` and ``gs://`` support, use ``make_reader``). The default hdfs driver is ``libhdfs3``.\n            If you want to to use ``libhdfs``, use\n            ``pyarrow_filesystem=pyarrow.hdfs.connect(\'hdfs:///some/path\', driver=\'libhdfs\')``.\n        :param dataset_path: filepath to a parquet directory or parquet file path list on the specified filesystem.\n            e.g. ``\'/user/yevgeni/parquet8\'``, or ``\'/tmp/mydataset\'``,\n            or ``[/tmp/mydataset/00000.parquet, /tmp/mydataset/00001.parquet]``\n        :param schema_fields: Either list of unischema fields to subset, or ``None`` to read all fields.\n            OR an NGram object, then it will return an NGram of the specified properties.\n        :param shuffle_row_groups: Whether to shuffle row groups (the order in which full row groups are read)\n        :param shuffle_row_drop_partitions: This is is a positive integer which determines how many partitions to\n            break up a row group into for increased shuffling in exchange for worse performance (extra reads).\n            For example if you specify 2 each row group read will drop half of the rows within every row group and\n            read the remaining rows in separate reads. It is recommended to keep this number below the regular row\n            group size in order to not waste reads which drop all rows.\n        :param predicate: instance of predicate object to filter rows to be returned by reader.\n        :param rowgroup_selector: instance of row group selector object to select row groups to be read\n        :param reader_pool: parallelization pool. ``ThreadPool(10)`` (10 threads) is used by default.\n            This pool is a custom implementation used to parallelize reading data from the dataset.\n            Any object from workers_pool package can be used\n            (e.g. :class:`petastorm.workers_pool.process_pool.ProcessPool`).\n        :param num_epochs: An epoch is a single pass over all rows in the dataset. Setting ``num_epochs`` to\n            ``None`` will result in an infinite number of epochs.\n        :param cur_shard: An int denoting the current shard number used. Each reader instance should\n            pass in a unique shard number in the range ``[0, shard_count)``.\n            ``shard_count`` must be supplied as well. Defaults to None\n        :param shard_count: An int denoting the number of shard partitions there are. Defaults to None\n        :param cache: An object conforming to :class:`.CacheBase` interface. Before loading row groups from a parquet\n            file the Reader will attempt to load these values from cache. Caching is useful when communication\n            to the main data store is either slow or expensive and the local machine has large enough storage\n            to store entire dataset (or a partition of a dataset if shards are used).\n            By default, use the :class:`.NullCache` implementation.\n\n        :param worker_class: This is the class that will be instantiated on a different thread/process. It\'s\n            responsibility is to load and filter the data.\n        """"""\n\n        # 1. Open the parquet storage (dataset)\n        # 2. Get a list of all groups\n        # 3. Filter rowgroups\n        #    a. predicates\n        #    b. row-group selector (our indexing mechanism)\n        #    c. partition: used to get a subset of data for distributed training\n        # 4. Create a rowgroup ventilator object\n        # 5. Start workers pool\n        if not (isinstance(schema_fields, collections.Iterable) or isinstance(schema_fields, NGram)\n                or schema_fields is None):\n            raise ValueError(\'Fields must be either None, an iterable collection of Unischema fields \'\n                             \'or an NGram object.\')\n\n        self.is_batched_reader = is_batched_reader\n        # 1. Resolve dataset path (hdfs://, file://) and open the parquet storage (dataset)\n        self.dataset = pq.ParquetDataset(dataset_path, filesystem=pyarrow_filesystem,\n                                         validate_schema=False, metadata_nthreads=10)\n\n        if self.dataset.partitions is None:\n            # When read from parquet file list, the `dataset.partitions` will be None.\n            # But other petastorm code require at least an empty `ParquetPartitions` object.\n            self.dataset.partitions = pq.ParquetPartitions()\n\n        stored_schema = infer_or_load_unischema(self.dataset)\n\n        if isinstance(schema_fields, NGram):\n            self.ngram = schema_fields\n            self.ngram.resolve_regex_field_names(stored_schema)\n        else:\n            self.ngram = None\n\n        # By default, use original method of working with list of dictionaries and not arrow tables\n        worker_class = worker_class or PyDictReaderWorker\n        self._results_queue_reader = worker_class.new_results_queue_reader()\n\n        if self.ngram and not self.ngram.timestamp_overlap and shuffle_row_drop_partitions > 1:\n            raise NotImplementedError(\'Using timestamp_overlap=False is not implemented with\'\n                                      \' shuffle_options.shuffle_row_drop_partitions > 1\')\n\n        cache = cache or NullCache()\n\n        self._workers_pool = reader_pool or ThreadPool(10)\n\n        # Make a schema view (a view is a Unischema containing only a subset of fields\n        # Will raise an exception if invalid schema fields are in schema_fields\n        if self.ngram:\n            fields = self.ngram.get_field_names_at_all_timesteps()\n        else:\n            fields = schema_fields if isinstance(schema_fields, collections.Iterable) else None\n\n        storage_schema = stored_schema.create_schema_view(fields) if fields else stored_schema\n        if transform_spec:\n            self.schema = transform_schema(storage_schema, transform_spec)\n        else:\n            self.schema = storage_schema\n\n        # 2. Get a list of all row groups\n        row_groups = dataset_metadata.load_row_groups(self.dataset)\n\n        # 3. Filter rowgroups\n        filtered_row_group_indexes, worker_predicate = self._filter_row_groups(self.dataset, row_groups, predicate,\n                                                                               rowgroup_selector, cur_shard,\n                                                                               shard_count)\n        # 4. Create a rowgroup ventilator object\n        normalized_shuffle_row_drop_partitions = \\\n            self._normalize_shuffle_options(shuffle_row_drop_partitions, self.dataset)\n        self.ventilator = self._create_ventilator(filtered_row_group_indexes, shuffle_row_groups,\n                                                  normalized_shuffle_row_drop_partitions, num_epochs, worker_predicate,\n                                                  self._workers_pool.workers_count + _VENTILATE_EXTRA_ROWGROUPS)\n\n        # 5. Start workers pool\n        self._workers_pool.start(worker_class, (pyarrow_filesystem, dataset_path, storage_schema, self.ngram,\n                                                row_groups, cache, transform_spec, self.schema),\n                                 ventilator=self.ventilator)\n        logger.debug(\'Workers pool started\')\n\n        self.last_row_consumed = False\n        self.stopped = False\n\n    def reset(self):\n        """"""Resets ``Reader`` state and allows to fetch more samples once the ``Reader`` finished reading all epochs,\n        as specified by the ``num_epochs`` parameter.\n\n        Once all samples were read from a reader, an attempt to fetch new sample (e.g. ``next(reader)`` would raise\n        ``StopIterationError``. You can reset the reader to the original state and restart reading samples\n        calling ``reset()``.\n\n        We do not support calling ``reset()`` until all samples were consumed. ``NotImplementedError``\n        will be raised if a user attempt to do so.\n\n        Calling reset after ``stop()`` was called has no effect.\n\n        :return: None\n        """"""\n        # TODO(yevgeni): could there be a race here?\n        if not self.last_row_consumed:\n            # Don\'t allow reseting in the middle of epoch iterations since it is not very well defined how\n            # to treat samples that are already \'in-flight\': do we need to stop emitting results immediately and\n            # drop these in-flight samples? Or just ignore it? What would happen if we have two concurrent ventilators\n            # that are emitting load requests at the same time?\n            raise NotImplementedError(\'Currently do not support resetting a reader while in the middle of iteration. \'\n                                      \'You can call reset only after all samples were consumed.\')\n        self.last_row_consumed = False\n        self.ventilator.reset()\n\n    @property\n    def batched_output(self):\n        return self._results_queue_reader.batched_output\n\n    def _filter_row_groups(self, dataset, row_groups, predicate, rowgroup_selector, cur_shard,\n                           shard_count):\n        """"""Calculates which rowgroups will be read during.\n\n        The following filters are applied:\n        - predicates;\n        - row-group selector (our indexing mechanism);\n        - training partition\n\n        :param dataset: ParquetDataset instance\n        :param row_groups: a list of row groups (a list of ParquetDatasetPiece objects)\n        :param predicate: instance of predicate object to filter rows to be returned by reader.\n        :param rowgroup_selector: instance of row group selector object to select row groups to be read\n        :param cur_shard: An int denoting the current shard number used. Each node should\n                       pass in a unique partition number in the range [0, shard_count).\n        :param shard_count An int denoting the number of reader shards\n        :return: (filtered_row_group_indexes, worker_predicate): filtered_row_group_indexes an integer index into\n        row_groups array. worker_predicate contains only predicates that could not be resolved on the partitioned fields\n        and need to be evaluated by workers.\n        """"""\n\n        filtered_row_group_indexes, worker_predicate = \\\n            self._apply_predicate_to_row_groups(dataset, row_groups, predicate)\n\n        if rowgroup_selector:\n            filtered_row_group_indexes = self._apply_row_group_selector(dataset, rowgroup_selector,\n                                                                        filtered_row_group_indexes)\n\n        if cur_shard is not None or shard_count is not None:\n            filtered_row_group_indexes = self._partition_row_groups(dataset, row_groups, shard_count,\n                                                                    cur_shard,\n                                                                    filtered_row_group_indexes)\n\n        if not filtered_row_group_indexes:\n            warnings.warn(\'No matching data is available for loading after rowgroup \'\n                          \'selector were applied and the data was sharded.\')\n\n        return filtered_row_group_indexes, worker_predicate\n\n    def _partition_row_groups(self, dataset, row_groups, shard_count, cur_shard,\n                              filtered_row_group_indexes):\n        """"""Filters the list of row group indexes based on the requested training partitions. Returns\n        a modified list of rowgroup indexes.""""""\n\n        if not shard_count \\\n                or not isinstance(cur_shard, int) \\\n                or not isinstance(shard_count, int):\n            raise ValueError(\'partition and num_partitions must be ints and both specified to use partitioning\')\n\n        if shard_count is not None and len(row_groups) < shard_count:\n            raise NoDataAvailableError(\'Number of row-groups in the dataset must be greater or equal to the number of \'\n                                       \'requested shards. Otherwise, some of the shards will end up being empty.\')\n\n        # We hash on the relative path of each parquet file to guarantee consistency between different reader\n        # constructions even after moving the dataset\n        filtered_row_group_indexes = [index for index in filtered_row_group_indexes if index % shard_count == cur_shard]\n        return filtered_row_group_indexes\n\n    def _apply_row_group_selector(self, dataset, rowgroup_selector, filtered_row_group_indexes):\n        """"""Filters the list of row group indexes using rowgroup selector object. Returns a modified list of rowgroup\n        indexes.""""""\n\n        if not isinstance(rowgroup_selector, RowGroupSelectorBase):\n            raise ValueError(\'rowgroup_selector parameter is expected to be derived from RowGroupSelectorBase\')\n\n        # Load indexes from metadata\n        available_row_group_indexes = rowgroup_indexing.get_row_group_indexes(dataset)\n\n        required_indexes = rowgroup_selector.get_index_names()\n        if not set(required_indexes).issubset(set(available_row_group_indexes.keys())):\n            raise ValueError(\'Some of required indexes {} are not available in {}\'.format(\n                required_indexes, list(available_row_group_indexes.keys())))\n\n        selected_indexes = rowgroup_selector.select_row_groups(available_row_group_indexes)\n\n        # include only selected_indexes but in filtered_row_group_indexes order\n        filtered_row_group_indexes = [idx for idx in filtered_row_group_indexes if idx in selected_indexes]\n        return filtered_row_group_indexes\n\n    def _apply_predicate_to_row_groups(self, dataset, row_groups, predicate):\n        """"""Filters the list of row group indexes using rowgroup selector object. Returns a modified list of rowgroup\n        indexes and a list of worker_predicate: predicates that could not be applied at this level\n        (parquet partitioning).""""""\n\n        if predicate:\n            if not isinstance(predicate, PredicateBase):\n                raise ValueError(\'predicate parameter is expected to be derived from PredicateBase\')\n            predicate_fields = predicate.get_fields()\n\n            if set(predicate_fields) == dataset.partitions.partition_names:\n                assert len(dataset.partitions.partition_names) == 1, \\\n                    \'Datasets with only a single partition level supported at the moment\'\n\n                filtered_row_group_indexes = []\n                for piece_index, piece in enumerate(row_groups):\n                    partition_name, partition_index = piece.partition_keys[0]\n                    partition_value = dataset.partitions[0].keys[partition_index]\n\n                    # Convert partition value to correct type per the schema\n                    partition_value = self.schema.fields[partition_name].numpy_dtype(partition_value)\n                    if predicate.do_include({partition_name: partition_value}):\n                        filtered_row_group_indexes.append(piece_index)\n                worker_predicate = None\n            else:\n                filtered_row_group_indexes = list(range(len(row_groups)))\n                worker_predicate = predicate\n\n        else:\n            filtered_row_group_indexes = list(range(len(row_groups)))\n            worker_predicate = None\n        return filtered_row_group_indexes, worker_predicate\n\n    @staticmethod\n    def _normalize_shuffle_options(shuffle_row_drop_partitions, dataset):\n        """"""Checks that shuffle_options doesnt ask for more patitions than rows in a row group.\n        This prevents sending partitions to workers which will result in not reading anything.""""""\n        if shuffle_row_drop_partitions > 1 and dataset.metadata and dataset.metadata.num_row_groups:\n            max_rows_in_row_group = 1\n            for i in six.moves.xrange(dataset.metadata.num_row_groups):\n                max_rows_in_row_group = max(max_rows_in_row_group, dataset.metadata.row_group(i).num_rows)\n\n            return min(shuffle_row_drop_partitions, max_rows_in_row_group)\n        return shuffle_row_drop_partitions\n\n    def _create_ventilator(self, row_group_indexes, shuffle_row_groups, shuffle_row_drop_partitions,\n                           num_epochs, worker_predicate, max_ventilation_queue_size):\n        items_to_ventilate = []\n        for piece_index in row_group_indexes:\n            for shuffle_row_drop_partition in range(shuffle_row_drop_partitions):\n                items_to_ventilate.append(\n                    {\'piece_index\': piece_index,\n                     \'worker_predicate\': worker_predicate,\n                     \'shuffle_row_drop_partition\': (shuffle_row_drop_partition,\n                                                    shuffle_row_drop_partitions)})\n\n        return ConcurrentVentilator(self._workers_pool.ventilate,\n                                    items_to_ventilate,\n                                    iterations=num_epochs,\n                                    max_ventilation_queue_size=max_ventilation_queue_size,\n                                    randomize_item_order=shuffle_row_groups)\n\n    def stop(self):\n        """"""Stops all worker threads/processes.""""""\n        self._workers_pool.stop()\n        self.stopped = True\n\n    def join(self):\n        """"""Joins all worker threads/processes. Will block until all worker workers have been fully terminated.""""""\n        self._workers_pool.join()\n\n    @property\n    def diagnostics(self):\n        return self._workers_pool.diagnostics\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.stopped:\n            raise RuntimeError(\'Trying to read a sample after a reader created by \'\n                               \'make_reader/make_batch_reader has stopped. This may happen if the \'\n                               \'make_reader/make_batch_reader context manager has exited but you try to \'\n                               \'fetch a sample from it anyway\')\n        try:\n            return self._results_queue_reader.read_next(self._workers_pool, self.schema, self.ngram)\n        except StopIteration:\n            self.last_row_consumed = True\n            raise\n\n    def next(self):\n        return self.__next__()\n\n    # Functions needed to treat reader as a context manager\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.stop()\n        self.join()\n'"
petastorm/selectors.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport abc\nimport six\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass RowGroupSelectorBase(object):\n    """""" Base class for row group selectors.""""""\n\n    @abc.abstractmethod\n    def get_index_names(self):\n        """""" Return list of indexes required for given selector.""""""\n\n    @abc.abstractmethod\n    def select_row_groups(self, index_dict):\n        """""" Return set of row groups which are selected.""""""\n\n\nclass SingleIndexSelector(RowGroupSelectorBase):\n    """"""\n    Generic selector for single field indexer.\n    Select all row groups containing any of given values.\n    """"""\n\n    def __init__(self, index_name, values_list):\n        self._index_name = index_name\n        self._values_to_select = values_list\n\n    def get_index_names(self):\n        return [self._index_name]\n\n    def select_row_groups(self, index_dict):\n        indexer = index_dict[self._index_name]\n        row_groups = set()\n        for value in self._values_to_select:\n            row_groups |= indexer.get_row_group_indexes(value)\n        return row_groups\n\n\nclass IntersectIndexSelector(RowGroupSelectorBase):\n    """"""\n    Multiple single-field indexers selector.\n    Select row groups containing any of the values in all given selectors.\n    """"""\n\n    def __init__(self, single_index_selectors):\n        """"""\n        :param single_index_selectors: List of SingleIndexSelector\n        """"""\n        self._single_index_selectors = single_index_selectors\n\n    def get_index_names(self):\n        index_names = []\n        for single_index_selector in self._single_index_selectors:\n            index_names.append(single_index_selector.get_index_names()[0])\n        return index_names\n\n    def select_row_groups(self, index_dict):\n        row_groups = self._single_index_selectors[0].select_row_groups(index_dict)\n        for single_index_selector in self._single_index_selectors:\n            row_groups &= single_index_selector.select_row_groups(index_dict)\n        return row_groups\n\n\nclass UnionIndexSelector(RowGroupSelectorBase):\n    """"""\n    Multiple single-field indexers selector.\n    Select row groups containing any of the values in at least one selector.\n    """"""\n\n    def __init__(self, single_index_selectors):\n        """"""\n        :param single_index_selectors: List of SingleIndexSelector\n        """"""\n        self._single_index_selectors = single_index_selectors\n\n    def get_index_names(self):\n        index_names = []\n        for single_index_selector in self._single_index_selectors:\n            index_names.append(single_index_selector.get_index_names()[0])\n        return index_names\n\n    def select_row_groups(self, index_dict):\n        row_groups = set()\n        for single_index_selector in self._single_index_selectors:\n            row_groups |= single_index_selector.select_row_groups(index_dict)\n        return row_groups\n'"
petastorm/spark_utils.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A set of Spark specific helper functions for the petastorm dataset""""""\nfrom six.moves.urllib.parse import urlparse\n\nfrom petastorm import utils\nfrom petastorm.etl.dataset_metadata import get_schema_from_dataset_url\nfrom petastorm.fs_utils import FilesystemResolver\n\n\ndef dataset_as_rdd(dataset_url, spark_session, schema_fields=None, hdfs_driver=\'libhdfs3\'):\n    """"""\n    Retrieve a spark rdd for a given petastorm dataset\n\n    :param dataset_url: A string for the dataset url (e.g. hdfs:///path/to/dataset)\n    :param spark_session: A spark session\n    :param schema_fields: list of unischema fields to subset, or None to read all fields.\n    :param hdfs_driver: A string denoting the hdfs driver to use (if using a dataset on hdfs). Current choices are\n        libhdfs (java through JNI) or libhdfs3 (C++)\n    :return: A rdd of dictionary records from the dataset\n    """"""\n    schema = get_schema_from_dataset_url(dataset_url, hdfs_driver=hdfs_driver)\n\n    dataset_url_parsed = urlparse(dataset_url)\n\n    resolver = FilesystemResolver(dataset_url_parsed, spark_session.sparkContext._jsc.hadoopConfiguration(),\n                                  hdfs_driver=hdfs_driver)\n\n    dataset_df = spark_session.read.parquet(resolver.get_dataset_path())\n    if schema_fields is not None:\n        # If wanting a subset of fields, create the schema view and run a select on those fields\n        schema = schema.create_schema_view(schema_fields)\n        field_names = [field.name for field in schema_fields]\n        dataset_df = dataset_df.select(*field_names)\n\n    dataset_rows = dataset_df.rdd \\\n        .map(lambda row: utils.decode_row(row.asDict(), schema)) \\\n        .map(lambda record: schema.make_namedtuple(**record))\n\n    return dataset_rows\n'"
petastorm/tf_utils.py,37,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A set of Tensorflow specific helper functions for the unischema""""""\nimport datetime\nimport sys\nimport warnings\nfrom calendar import timegm\nfrom collections import OrderedDict, namedtuple\nfrom decimal import Decimal\n\nimport numpy as np\nimport tensorflow.compat.v1 as tf  # pylint: disable=import-error\n\n# Mapping of identical datatypes in numpy-ish and tensorflow-ish\n_NUMPY_TO_TF_DTYPES_MAPPING = {\n    np.bool: tf.bool,\n    np.int8: tf.int8,\n    np.int16: tf.int16,\n    np.int32: tf.int32,\n    np.int64: tf.int64,\n    np.uint8: tf.uint8,\n    np.uint16: tf.int32,\n    np.uint32: tf.int64,\n    np.float32: tf.float32,\n    np.float64: tf.float64,\n    np.string_: tf.string,\n    np.unicode_: tf.string,\n    np.str_: tf.string,\n    np.bool_: tf.bool,\n    Decimal: tf.string,\n    np.datetime64: tf.int64,\n}\n\n# Name of an op in the TF graph used for the random shuffling queue. This name can be used by diagnostics code that\n# wishes to read-out shuffling queue size\nRANDOM_SHUFFLING_QUEUE_SIZE = \'random_shuffling_queue_size\'\n\n\ndef date_to_nsec_from_epoch(dt):\n    return timegm(dt.timetuple()) * 1000000000\n\n\n_date_to_nsec_from_epoch_vectorized = np.vectorize(date_to_nsec_from_epoch)\n\n\ndef _sanitize_field_tf_types(sample):\n    """"""Takes a named tuple and casts/promotes types unknown to TF to the types that are known.\n\n    Three casts that are currently implemented\n      - Decimal to string\n      - uint16 to int32\n      - np.datetime64 to int64, as nanoseconds since unix epoch\n\n    :param sample: named tuple or a dictionary\n    :return: same type as the input with values casted to types supported by Tensorflow\n    """"""\n    next_sample_dict = sample._asdict()\n\n    for k, v in next_sample_dict.items():\n        if v is None:\n            raise RuntimeError(\'Encountered ""{}""=None. Tensorflow does not support None values as a tensor.\'\n                               \'Consider filtering out these rows using a predicate.\'.format(k))\n        # Assuming conversion to the same numpy type is trivial and dirty cheap\n        if isinstance(v, Decimal):\n            # Normalizing decimals only to get rid of the trailing zeros (makes testing easier, assuming has\n            # no other effect)\n            next_sample_dict[k] = str(v.normalize())\n        elif isinstance(v, np.ndarray) and np.issubdtype(v.dtype, np.datetime64):\n            # Convert to nanoseconds from POSIX epoch\n            next_sample_dict[k] = (v - np.datetime64(\'1970-01-01T00:00:00.0\')) \\\n                .astype(\'timedelta64[ns]\').astype(np.int64)\n        elif isinstance(v, np.ndarray) and v.dtype == np.uint16:\n            next_sample_dict[k] = v.astype(np.int32)\n        elif isinstance(v, np.ndarray) and v.dtype == np.uint32:\n            next_sample_dict[k] = v.astype(np.int64)\n        elif isinstance(v, np.ndarray) and v.dtype.type in (np.bytes_, np.unicode_):\n            if v.size != 0:\n                next_sample_dict[k] = v.tolist()\n        elif isinstance(v, np.ndarray) and v.dtype.kind == \'O\' and isinstance(v[0], datetime.date):\n            # Pyarrow 0.12.1 started returning python datetime.date when parquet column is a DateType() column.\n            # Convert values in such column into nsec from epoch int64.\n            next_sample_dict[k] = _date_to_nsec_from_epoch_vectorized(v)\n\n    # Construct object of the same type as the input\n    return sample.__class__(**next_sample_dict)\n\n\ndef _schema_to_tf_dtypes(schema):\n    """"""Returns schema as a list of tensorflow dtypes.\n    :param schema: The schema.\n    :return: List of tensorflow dtypes.\n    """"""\n    return [_numpy_to_tf_dtypes(f.numpy_dtype) for f in schema.fields.values()]\n\n\ndef _schema_to_tf_dtypes_ngram(schema, ngram):\n    """"""Returns schema as a list of tensorflow dtypes for a ngram.\n    :param schema: The schema.\n    :param ngram: The ngram.\n    :return: tensorflow dtypes for a ngram.\n    """"""\n    result = []\n    # Iterate over each timestep\n    for key in sorted(ngram.fields.keys()):\n        # Get schema at that timestep\n        new_schema = ngram.get_schema_at_timestep(schema=schema, timestep=key)\n        for field in new_schema.fields.values():\n            result.append(_numpy_to_tf_dtypes(field.numpy_dtype))\n    return result\n\n\ndef _numpy_to_tf_dtypes(numpy_dtype):\n    """"""Returns a tensorflow dtype object corresponding to numpy\'s dtype.\n\n    A :class:`ValueError` is raised if there is no known mapping between the types\n\n    :param numpy_dtype: numpy dtype object\n    :return: tensorflow dtype object\n    """"""\n    if numpy_dtype in _NUMPY_TO_TF_DTYPES_MAPPING:\n        if numpy_dtype == np.unicode_ and sys.version_info >= (3, 0):\n            warnings.warn(""Tensorflow will convert all unicode strings back to bytes type. ""\n                          ""You may need to decode values."", UnicodeWarning)\n        return _NUMPY_TO_TF_DTYPES_MAPPING[numpy_dtype]\n    else:\n        raise ValueError(\'Unknown mapping of numpy {} to tensorflow dtype\'.format(numpy_dtype))\n\n\ndef _flatten(data):\n    """"""Flattens the data, where it takes a dictionary of timesteps, each value is a dictionary and converts it to\n    one flat dictionary having a key that is the key of the inner dictionary + \'_\' + timestep.\n\n    For example, ``data`` would be ``{1: {\'a\': \'avalue\', \'b\': \'bvalue\'}, 2: {\'c\': \'cvalue\', \'d\': \'dvalue\'}}`` and the\n    output of :func:`._flatten` would be ``{\'a_1\': \'avalue\', \'b_1\': \'bvalue\', \'c_2\': \'cvalue\', \'d_2\': \'dvalue\'}``.\n\n    :param data: The data to flatten.\n    :return: The flattened dictionary.\n    """"""\n    flattened = OrderedDict()\n    for index, key in enumerate(sorted(data.keys())):\n        data_dict = data[key]._asdict()\n        for subkey in data_dict:\n            encoded_key = subkey + \'_\' + str(index)\n            flattened[encoded_key] = data_dict[subkey]\n\n    FlattenedTuple = namedtuple(\'flattened\', list(flattened.keys()))\n    return FlattenedTuple(**flattened)\n\n\ndef make_namedtuple_tf_ngram(unischema, ngram, *args, **kargs):\n    """"""Creates a dictionary of timestep keys and namedtuple values from args and kargs.\n\n    :param ngram: The ngram definition.\n    :param args: args.\n    :param kargs: kargs.\n    :return: A dictionary of timestep keys and namedtuple values.\n    """"""\n\n    ngram_result = {}\n    previous_args_end = 0\n    for timestep in range(min(ngram.fields.keys()), max(ngram.fields.keys()) + 1):\n        # For each timestep iteration, mark the args and kargs for that timestep and create\n        # a namedtuple from them.\n        current_field_names = ngram.get_field_names_at_timestep(timestep)\n        new_schema = ngram.get_schema_at_timestep(schema=unischema, timestep=timestep)\n        new_args_end = previous_args_end + len(current_field_names)\n        args_timestep = args[previous_args_end:new_args_end]\n        previous_args_end = new_args_end\n        kargs_timestep = (kargs[str(timestep)] if str(timestep) in kargs else {})\n        ngram_result[timestep] = new_schema._get_namedtuple()(*args_timestep, **kargs_timestep)\n    return ngram_result\n\n\ndef _set_shape(schema, fields_as_dict, batched_output=None):\n    # Assign static shape for all tensors\n    # Workaround of an issue described here:\n    # https://stackoverflow.com/questions/49161316/trailing-x00-characters-in-tensor-when-numpy-string-array-is-returned-from-tf\n    for k in fields_as_dict.keys():\n        unischema_field = schema.fields[k]\n\n        if fields_as_dict[k].get_shape().dims is None:\n            if batched_output:\n                shape = (None,) + unischema_field.shape\n            else:\n                shape = unischema_field.shape\n            # Set static shape\n            fields_as_dict[k].set_shape(shape)\n\n\ndef _shuffling_queue(shuffling_queue_capacity, min_after_dequeue, dtypes, fields_as_list):\n    """"""Creates a shuffling queue with enqueue/dequeue pair. Always a single writing thread.""""""\n\n    # Named tuples loose the \'named\' part when going via queue\n    shuffling_queue = tf.RandomShuffleQueue(shuffling_queue_capacity, min_after_dequeue, dtypes)\n\n    # The following call to .size has a side effect of creating a new node in the TF graph. We are interested\n    # in the side effect so we can read the queue size somewhere else, addressing the node by a \'well-known-name\'\n    shuffling_queue.size(name=RANDOM_SHUFFLING_QUEUE_SIZE)\n\n    # We need the queue only for shuffling, so we use only a single enqueuing thread (actually would be happy\n    # not to introduce any threads. Not sure if there is such a mechanism in TF)\n    queue_runner = tf.train.QueueRunner(shuffling_queue, 1 * [shuffling_queue.enqueue(fields_as_list)])\n\n    tf.train.add_queue_runner(queue_runner)\n\n    # Passed through the queue. We got an ordered list. The order matches the order of fields in unischema\n    fields_as_list = shuffling_queue.dequeue()\n    return fields_as_list\n\n\ndef _tf_tensors_nonngram(reader, shuffling_queue_capacity, min_after_dequeue):\n    """"""A tensorflow data adapter for non ngrams. Return value is a named tuple with tensorflow tensors supplying\n    the data directly into a Tensoflow graph. See `tf_tensor` documentation for input/output arguments meaning.""""""\n\n    # TODO(yevgeni): implement a mechanism for signaling that we have no more data\n    def dequeue_sample_impl(x):\n        next_sample = next(reader)\n        # Decimal is not supported by TF. int8,16,32,64 scalars are all returned as python native int type\n        # (casted to 64 bit by tensorflow). sanitize_field_tf_types will explicitly convert all values\n        # to explicit numpy types making it compatible with return values expected by Tensorflow\n        return _sanitize_field_tf_types(next_sample)\n\n    # fields_as_list is a list with tensors matching the order of the values in the schema. named-tuple semantics is\n    # not preserved across tf.py_func call boundary.\n    fields_as_list = tf.py_func(dequeue_sample_impl, [tf.constant(1)], _schema_to_tf_dtypes(reader.schema))\n\n    if shuffling_queue_capacity > 0:\n        # Pass py_func output via shuffling queue if requested.\n        fields_as_list = _shuffling_queue(shuffling_queue_capacity, min_after_dequeue,\n                                          _schema_to_tf_dtypes(reader.schema), fields_as_list)\n\n    # Going via `make_namedtuple_tf` is a little wasteful, since we are converting directly to dict. However, this\n    # spares the need to implement a function similar to make_namedtuple_tf that returns dict instead of a named tuple\n    fields_as_dict = reader.schema.make_namedtuple_tf(*fields_as_list)._asdict()\n\n    # Force all static shapes to be set in the returned value based on the unischema\n    _set_shape(reader.schema, fields_as_dict, reader.batched_output)\n\n    # Make a row tensor into a nice named tuple\n    return reader.schema.make_namedtuple_tf(**fields_as_dict)\n\n\ndef _tf_tensors_ngram(reader, shuffling_queue_capacity, min_after_dequeue):\n    """"""A tensorflow data adapter for ngrams. Return value is a named tuple with tensorflow tensors supplying\n    the data directly into a Tensoflow graph. See `tf_tensor` documentation for input/output arguments meaning.""""""\n\n    fields_as_list = tf.py_func(lambda _: _sanitize_and_flatten(next(reader)), [tf.constant(1)],\n                                _schema_to_tf_dtypes_ngram(reader.schema, reader.ngram))\n\n    if shuffling_queue_capacity > 0:\n        # Pass py_func output via shuffling queue if requested.\n        fields_as_list = _shuffling_queue(shuffling_queue_capacity, min_after_dequeue,\n                                          _schema_to_tf_dtypes_ngram(reader.schema, reader.ngram), fields_as_list)\n\n    return _unflatten_and_set_shape(reader.schema, reader.ngram, fields_as_list)\n\n\ndef tf_tensors(reader, shuffling_queue_capacity=0, min_after_dequeue=0):\n    """"""Bridges between python-only interface of the Reader (next(Reader)) and tensorflow world.\n\n    This function returns a named tuple of tensors from the dataset, e.g.,\n\n    >>> row_tensors\n    >>> Out[2]: TestSchema_view(field_1=<tf.Tensor \'PyFunc:0\' shape=() dtype=string>,\n    >>>         field_2=<tf.Tensor \'StringSplit:1\' shape=(?,) dtype=string>,\n    >>>         field_3=<tf.Tensor \'PyFunc:2\' shape=() dtype=int64>, ...)\n\n    If the reader was created with ``ngram=NGram(...)`` parameter, then a dictionary of named tuples is returned\n    (indexed by time):\n\n    >>> row_tensors\n    >>> Out[6]:\n    >>> {0: TestSchema_view(field_1=<tf.Tensor \'PyFunc_4:0\' shape=() dtype=string>, field_2=...),\n    >>>  1: TestSchema_view(field_1=<tf.Tensor \'PyFunc_4:11\' shape=() dtype=string>, field_2=...),\n    >>>  2: TestSchema_view(field_1=<tf.Tensor \'PyFunc_4:22\' shape=() dtype=string>, field_2=...)}\n\n    An optional shuffling queue is created if shuffling_queue_capacity is greater than 0.\n\n    Note that if reading a unischema field that is unicode (``np.unicode_`` or ``np.str_``) tensorflow will\n    represent it as a tf.string which will be an array of bytes. If using python3 you may need to decode\n    it to convert it back to a python str type.\n\n    :param reader: An instance of petastorm.Reader object used as the data source\n    :param shuffling_queue_capacity: Queue capacity is passed to the underlying :class:`tf.RandomShuffleQueue`\n        instance. If set to 0, no suffling will be done.\n    :param min_after_dequeue: If ``shuffling_queue_capacity > 0``, this value is passed to the underlying\n        :class:`tf.RandomShuffleQueue`.\n    :return: If no ngram reading is used, the function will return a named tuple with tensors that are populated\n        from the underlying dataset. If ngram reading is enabled, a dictionary of named tuples of tensors is returned.\n        The dictionary is indexed by time.\n    """"""\n\n    # NGram enabled and disabled code is quite different. It appears to be cleaner to simply go in orthogonal\n    # execution paths.\n\n    if reader.batched_output:\n        if shuffling_queue_capacity > 0:\n            raise ValueError(\'shuffling_queue_capacity can not be used with a reader that produces \'\n                             \'batched_output, since each batch is a parquet read rowgroup. Extra \'\n                             \'shuffling of the batches does not further decrease correlation.\')\n\n    if reader.ngram:\n        result = _tf_tensors_ngram(reader, shuffling_queue_capacity, min_after_dequeue)\n    else:\n        result = _tf_tensors_nonngram(reader, shuffling_queue_capacity, min_after_dequeue)\n\n    return result\n\n\ndef _set_shape_to_named_tuple(schema, fields, batched_output):\n    """"""Assign static shape for all tensors""""""\n    fields_as_dict = fields._asdict()\n    _set_shape(schema, fields_as_dict, batched_output)\n    return schema.make_namedtuple_tf(**fields_as_dict)\n\n\ndef make_petastorm_dataset(reader):\n    """"""Creates a `tensorflow.data.Dataset <https://www.tensorflow.org/api_docs/python/tf/data/Dataset>`_ object from\n    a Petastorm :class:`~petastorm.reader.Reader`.\n\n    The returned object can be used as any ``tf.data.Dataset`` with some limitations described below.\n\n    * ``repeat``: An error will be raised if you call ``repeat`` on the returned dataset. Please use ``num_epochs``\n      argument of the :meth:`~petastorm.reader.Reader` constructor.\n    * ``shard``: Consider using ``training_partition`` and ``num_training_partitions`` arguments of the\n      :class:`~petastorm.reader.Reader` constructor as it will not load any unused shards.\n    * ``filter``: Consider using :class:`~petastorm.reader.Reader` ``predicate`` constructor argument.\n      It will make use of columnar nature of the underlying Apache Parquet store to load only the columns that the\n      predicate operates on prior to loading and decoding other columns. :class:`~petastorm.reader.Reader`\'s predicate\n      feature will also make use of Parquet partitioning (if the dataset is partitioned).\n\n    The elements produced by the returned dataset object are namedtuples based on the\n    :class:`~petastorm.unischema.Unischema`.\n\n    >>> import tensorflow.compat.v1 as tf  # pylint: disable=import-error\n    >>> from petastorm.reader import Reader\n    >>> from petastorm.tf_utils import make_petastorm_dataset\n    >>>\n    >>> with Reader(\'file:///some/path\') as reader:\n    >>>     dataset = make_petastorm_dataset(reader)\n    >>>     next_sample = dataset.make_one_shot_iterator().get_next()\n    >>>     with tf.Session() as sess:\n    >>>         x = sess.run(next_sample)\n\n\n    NGrams are not yet supported by this function.\n\n    :param reader: An instance of :class:`~petastorm.reader.Reader` object that would serve as a data source.\n    :return: A ``tf.data.Dataset`` instance.\n    """"""\n\n    if not reader.ngram:\n\n        def dequeue_sample_impl():\n            if reader.last_row_consumed:\n                # This means that Dataset is trying to create a new instance of the generator. Can not do that\n                # (nor want to do that) since this is an expensive operation. num_epochs is a more efficient way\n                # to do this.\n                raise RuntimeError(\'Multiple iterations over make_petastorm_dataset are not supported. \'\n                                   \'Multiple iterations can be triggered by calling \\\'repeat\\\' method of Datset class.\'\n                                   \'Use Reader\\\'s num_epochs contructor arguments to set number of iterations.\')\n            for row in reader:\n                yield _sanitize_field_tf_types(row)\n\n        flat_dataset = tf.data.Dataset.from_generator(dequeue_sample_impl, tuple(_schema_to_tf_dtypes(reader.schema)))\n\n        # Don\'t write this function as a inline lambda like `dataset.map(lambda row: _set_shape_to_named_tuple(...))`,\n        # It can avoid this error: https://github.com/tensorflow/tensorflow/issues/30149\n        def set_shape(row):\n            return _set_shape_to_named_tuple(reader.schema, row, reader.batched_output)\n\n        schema_tuple = reader.schema._get_namedtuple()\n        named_tuple_dataset = flat_dataset \\\n            .map(schema_tuple) \\\n            .map(set_shape)\n        return named_tuple_dataset\n    else:\n        # flat_dataset is a tf.data.Dataset with a tuple containined ngram field stored in one flat tuple produced by\n        # _flatten() function.\n        flat_dataset = tf.data.Dataset.from_generator(lambda: _ngrams_generator(reader),\n                                                      tuple(_schema_to_tf_dtypes_ngram(reader.schema, reader.ngram)))\n\n        # Unflatten the tuple into a dictionary\n        named_tuple_dataset = flat_dataset.map(\n            lambda *nargs: _unflatten_and_set_shape(reader.schema, reader.ngram, nargs))\n\n        return named_tuple_dataset\n\n\ndef _unflatten_and_set_shape(schema, ngram, fields_as_list):\n    """"""Takes a flat list of fields produced by _flatten function and unflatten it back into an ngrams dictionary""""""\n    fields_as_namedtuple = make_namedtuple_tf_ngram(schema, ngram, *fields_as_list)\n\n    # We change the key to str format here in order to be able to use ** later to expand the dictionary as kargs.\n    fields_as_dict = {str(timestep): fields_as_namedtuple[timestep]._asdict() for timestep in fields_as_namedtuple}\n\n    for timestep in fields_as_dict:\n        _set_shape(schema, fields_as_dict[timestep])\n\n    return make_namedtuple_tf_ngram(schema, ngram, **fields_as_dict)\n\n\ndef _ngrams_generator(reader):\n    """"""A generator producing flattened and sanitized ngrams""""""\n    if reader.last_row_consumed:\n        # This means that Dataset is trying to create a new instance of the generator. Can not do that\n        # (nor want to do that) since this is an expensive operation. num_epochs is a more efficient way\n        # to do this.\n        raise RuntimeError(\'Multiple iterations over make_petastorm_dataset are not supported. \'\n                           \'Multiple iterations can be triggered by calling \\\'repeat\\\' method of Datset class.\'\n                           \'Use Reader\\\'s num_epochs contructor arguments to set number of iterations.\')\n\n    for next_sample in reader:\n        yield _sanitize_and_flatten(next_sample)\n\n\ndef _sanitize_and_flatten(ngram):\n    """"""Fetches next sample from the reader, sanitizes tf types and flattens ngram to a list of values""""""\n    sanitized_ngram = {k: _sanitize_field_tf_types(v) for k, v in ngram.items()}\n\n    return _flatten(sanitized_ngram)\n'"
petastorm/transform.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport warnings\n\nfrom petastorm.unischema import UnischemaField, Unischema\n\n\ndef edit_field(name, numpy_dtype, shape, nullable=False):\n    """"""\n    A helper method to create the 4-tuples (name, numpy_dtype, shape, is_nullable)\n    used in the `edit_fields` of `TransformSpec`.\n    """"""\n    return name, numpy_dtype, shape, nullable\n\n\nclass TransformSpec(object):\n    def __init__(self, func=None, edit_fields=None, removed_fields=None, selected_fields=None):\n        """"""TransformSpec defines a user transformation that is applied to a loaded row on a worker thread/process.\n\n        The object defines the function (callable) that perform the transform as well as the\n        schema transform: pre-transform-schema to post-transform-schema.\n\n        ``func`` argument is a callable which takes a row as its parameter and returns a modified row.\n        ``edit_fields`` and ``removed_fields`` define mutating operations performed on the original schema that\n        produce a post-transform schema. ``func`` return value must comply to this post-transform schema.\n\n        :param func: Optional. A callable. The function is called on the worker thread. It takes a dictionary that\n          complies to the input schema and must return a dictionary that complies to a post-transform schema. User may\n          In case the user wants to only remove certain fields, the user may omit this argument and specify only\n          `remove_fields` argument.\n        :param edit_fields: Optional. A list of 4-tuples with the following fields:\n          ``(name, numpy_dtype, shape, is_nullable)``.\n        :param removed_fields: Optional[list]. A list of field names that will be removed from the original schema.\n        :param selected_fields: Optional[list]. A list of field names specify the fields to be selected.\n          If selected_fields specified, The reader schema will preserve the field order in selected_fields.\n\n        Note: For param `removed_fields` and `selected_fields`, user can only specify one of them.\n        """"""\n        self.func = func\n        self.edit_fields = edit_fields or []\n\n        if removed_fields is not None and selected_fields is not None:\n            raise ValueError(\'User can only specify one of removed_fields and selected_fields in TransformSpec.\')\n\n        self.removed_fields = removed_fields or []\n        self.selected_fields = selected_fields\n\n\ndef transform_schema(schema, transform_spec):\n    """"""Creates a post-transform given a pre-transform schema and a transform_spec with mutation instructions.\n\n    :param schema: A pre-transform schema\n    :param transform_spec: a TransformSpec object with mutation instructions.\n    :return: A post-transform schema\n    """"""\n    removed_fields = set(transform_spec.removed_fields)\n    unknown_field_names = removed_fields - set(schema.fields.keys())\n    if unknown_field_names:\n        warnings.warn(\'remove_fields specified some field names that are not part of the schema. \'\n                      \'These field names will be ignored ""{}"". \'.format(\', \'.join(unknown_field_names)))\n\n    exclude_fields = {f[0] for f in transform_spec.edit_fields} | removed_fields\n    fields = [v for k, v in schema.fields.items() if k not in exclude_fields]\n\n    for field_to_edit in transform_spec.edit_fields:\n        edited_unischema_field = UnischemaField(name=field_to_edit[0], numpy_dtype=field_to_edit[1],\n                                                shape=field_to_edit[2], codec=None, nullable=field_to_edit[3])\n        fields.append(edited_unischema_field)\n\n    if transform_spec.selected_fields is not None:\n        unknown_field_names = set(transform_spec.selected_fields) - set(f.name for f in fields)\n        if unknown_field_names:\n            warnings.warn(\'selected_fields specified some field names that are not part of the schema. \'\n                          \'These field names will be ignored ""{}"". \'.format(\', \'.join(unknown_field_names)))\n        fields = [f for f in fields if f.name in transform_spec.selected_fields]\n        fields = sorted(fields, key=lambda f: transform_spec.selected_fields.index(f.name))\n\n    return Unischema(schema._name + \'_transformed\', fields)\n'"
petastorm/unischema.py,1,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A ``unischema`` is a data structure definition which can be rendered as native schema/data-types objects\nin several different python libraries. Currently supported are pyspark, tensorflow, and numpy.\n""""""\nimport copy\nimport re\nimport sys\nimport warnings\nfrom collections import namedtuple, OrderedDict\nfrom decimal import Decimal\n\nimport numpy as np\nimport pyarrow as pa\nimport six\nfrom pyarrow.lib import ListType\nfrom pyarrow.lib import StructType as pyStructType\nfrom six import string_types\n\nfrom petastorm.compat import compat_get_metadata, compat_schema_field\n\n# _UNISCHEMA_FIELD_ORDER available values are \'preserve_input_order\' or \'alphabetical\'\n# Current default behavior is \'preserve_input_order\', the legacy behavior is \'alphabetical\', which is deprecated and\n# will be removed in future versions.\n_UNISCHEMA_FIELD_ORDER = \'preserve_input_order\'\n\n\ndef _fields_as_tuple(field):\n    """"""Common representation of UnischemaField for equality and hash operators.\n    Defined outside class because the method won\'t be accessible otherwise.\n\n    Today codec instance also responsible for defining spark dataframe type. This knowledge should move\n    to a different class in order to support backends other than Apache Parquet. For now we ignore the codec\n    in comparison. From the checks does not seem that it should negatively effect the rest of the code.\n    """"""\n    return (field.name, field.numpy_dtype, field.shape, field.nullable)\n\n\nclass UnischemaField(namedtuple(\'UnischemaField\', [\'name\', \'numpy_dtype\', \'shape\', \'codec\', \'nullable\'])):\n    """"""A type used to describe a single field in the schema:\n\n    - name: name of the field.\n    - numpy_dtype: a numpy ``dtype`` reference\n    - shape: shape of the multidimensional array. None value is used to define a dimension with variable number of\n             elements. E.g. ``(None, 3)`` defines a point cloud with three coordinates but unknown number of points.\n    - codec: An instance of a codec object used to encode/decode data during serialization\n             (e.g. ``CompressedImageCodec(\'png\')``)\n    - nullable: Boolean indicating whether field can be None\n\n    A field is considered immutable, so we override both equality and hash operators for consistency\n    and efficiency.\n    """"""\n\n    def __eq__(self, other):\n        """"""Comparing field objects via default namedtuple __repr__ representation doesn\'t work due to\n        codec object ID changing when unpickled.\n\n        Instead, compare all field attributes, except for codec type.\n\n        Future: Give codec a mime identifier.\n        """"""\n        return _fields_as_tuple(self) == _fields_as_tuple(other)\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __hash__(self):\n        return hash(_fields_as_tuple(self))\n\n\n# Defines default arguments for UnischemaField namedtuple:\n# Makes the signature equivalent to UnischemaField(name, numpy_dtype, shape, codec=None, nullable=False)\nUnischemaField.__new__.__defaults__ = (None, False)\n\n\nclass _NamedtupleCache(object):\n    """"""_NamedtupleCache makes sure the same instance of a namedtuple is returned for a given schema and a set of\n     fields. This makes comparison between types possible. For example, `tf.data.Dataset.concatenate` implementation\n     compares types to make sure two datasets can be concatenated.""""""\n    _store = dict()\n\n    @staticmethod\n    def get(parent_schema_name, field_names):\n        """"""Creates a nametuple with field_names as values. Returns an existing instance if was already created.\n\n        :param parent_schema_name: Schema name becomes is part of the cache key\n        :param field_names: defines names of the fields in the namedtuple created/returned. Also part of the cache key.\n        :return: A namedtuple with field names defined by `field_names`\n        """"""\n        # Cache key is a combination of schema name and all field names\n        if _UNISCHEMA_FIELD_ORDER.lower() == \'alphabetical\':\n            field_names = list(sorted(field_names))\n        else:\n            field_names = list(field_names)\n        key = \' \'.join([parent_schema_name] + field_names)\n        if key not in _NamedtupleCache._store:\n            _NamedtupleCache._store[key] = _new_gt_255_compatible_namedtuple(\n                \'{}_view\'.format(parent_schema_name), field_names)\n        return _NamedtupleCache._store[key]\n\n\ndef _new_gt_255_compatible_namedtuple(*args, **kwargs):\n    # Between Python 3 - 3.6.8 namedtuple can not have more than 255 fields. We use\n    # our custom version of namedtuple in these cases\n    if six.PY3 and sys.version_info[1] < 7:\n        # Have to hide the codeblock in namedtuple_gt_255_fields.py from Python 2 interpreter\n        # as it would trigger ""unqualified exec is not allowed in function"" SyntaxError\n        from petastorm.namedtuple_gt_255_fields import namedtuple_gt_255_fields\n        namedtuple_cls = namedtuple_gt_255_fields\n    else:  # Python 2 or Python 3.7 and later.\n        namedtuple_cls = namedtuple\n\n    return namedtuple_cls(*args, **kwargs)\n\n\ndef _numpy_to_spark_mapping():\n    """"""Returns a mapping from numpy to pyspark.sql type. Caches the mapping dictionary inorder to avoid instantiation\n    of multiple objects in each call.""""""\n\n    # Refer to the attribute of the function we use to cache the map using a name in the variable instead of a \'dot\'\n    # notation to avoid copy/paste/typo mistakes\n    cache_attr_name = \'cached_numpy_to_pyspark_types_map\'\n    if not hasattr(_numpy_to_spark_mapping, cache_attr_name):\n        import pyspark.sql.types as T\n\n        setattr(_numpy_to_spark_mapping, cache_attr_name,\n                {\n                    np.int8: T.ByteType(),\n                    np.uint8: T.ShortType(),\n                    np.int16: T.ShortType(),\n                    np.uint16: T.IntegerType(),\n                    np.int32: T.IntegerType(),\n                    np.int64: T.LongType(),\n                    np.float32: T.FloatType(),\n                    np.float64: T.DoubleType(),\n                    np.string_: T.StringType(),\n                    np.str_: T.StringType(),\n                    np.unicode_: T.StringType(),\n                    np.bool_: T.BooleanType(),\n                })\n\n    return getattr(_numpy_to_spark_mapping, cache_attr_name)\n\n\n# TODO: Changing fields in this class or the UnischemaField will break reading due to the schema being pickled next to\n# the dataset on disk\ndef _field_spark_dtype(field):\n    if field.codec is None:\n        if field.shape == ():\n            spark_type = _numpy_to_spark_mapping().get(field.numpy_dtype, None)\n            if not spark_type:\n                raise ValueError(\'Was not able to map type {} to a spark type.\'.format(str(field.numpy_dtype)))\n        else:\n            raise ValueError(\'An instance of non-scalar UnischemaField \\\'{}\\\' has codec set to None. \'\n                             \'Don\\\'t know how to guess a Spark type for it\'.format(field.name))\n    else:\n        spark_type = field.codec.spark_dtype()\n\n    return spark_type\n\n\nclass Unischema(object):\n    """"""Describes a schema of a data structure which can be rendered as native schema/data-types objects\n    in several different python libraries. Currently supported are pyspark, tensorflow, and numpy.\n    """"""\n\n    def __init__(self, name, fields):\n        """"""Creates an instance of a Unischema object.\n\n        :param name: name of the schema\n        :param fields: a list of ``UnischemaField`` instances describing the fields. The element order in the list\n            represent the schema field order.\n        """"""\n        self._name = name\n        if _UNISCHEMA_FIELD_ORDER.lower() == \'alphabetical\':\n            fields = sorted(fields, key=lambda t: t.name)\n\n        self._fields = OrderedDict([(f.name, f) for f in fields])\n        # Generates attributes named by the field names as an access syntax sugar.\n        for f in fields:\n            if not hasattr(self, f.name):\n                setattr(self, f.name, f)\n            else:\n                warnings.warn((\'Can not create dynamic property {} because it conflicts with an existing property of \'\n                               \'Unischema\').format(f.name))\n\n    def create_schema_view(self, fields):\n        """"""Creates a new instance of the schema using a subset of fields.\n\n        Fields can be either UnischemaField objects or regular expression patterns.\n\n        If one of the fields does not exist in this schema, an error is raised.\n\n        The example returns a schema, with field_1 and any other field matching ``other.*$`` pattern.\n\n        >>> SomeSchema.create_schema_view(\n        >>>     [SomeSchema.field_1,\n        >>>      \'other.*$\'])\n\n        :param fields: A list of UnischemaField objects and/or regular expressions\n        :return: a new view of the original schema containing only the supplied fields\n        """"""\n\n        # Split fields parameter to regex pattern strings and UnischemaField objects\n        regex_patterns = [field for field in fields if isinstance(field, string_types)]\n        # We can not check type against UnischemaField because the artifact introduced by\n        # pickling, since depickled UnischemaField are of type collections.UnischemaField\n        # while withing depickling they are of petastorm.unischema.UnischemaField\n        # Since UnischemaField is a tuple, we check against it since it is invariant to\n        # pickling\n        unischema_field_objects = [field for field in fields if isinstance(field, tuple)]\n        if len(unischema_field_objects) + len(regex_patterns) != len(fields):\n            raise ValueError(\'Elements of ""fields"" must be either a string (regular expressions) or \'\n                             \'an instance of UnischemaField class.\')\n\n        # For fields that are specified as instances of Unischema: make sure that this schema contains fields\n        # with these names.\n        exact_field_names = [field.name for field in unischema_field_objects]\n        unknown_field_names = set(exact_field_names) - set(self.fields.keys())\n        if unknown_field_names:\n            raise ValueError(\'field {} does not belong to the schema {}\'.format(unknown_field_names, self))\n\n        # Do not use instances of Unischema fields passed as an argument as it could contain codec/shape\n        # info that is different from the one stored in this schema object\n        exact_fields = [self._fields[name] for name in exact_field_names]\n        view_fields = exact_fields + match_unischema_fields(self, regex_patterns)\n\n        return Unischema(\'{}_view\'.format(self._name), view_fields)\n\n    def _get_namedtuple(self):\n        return _NamedtupleCache.get(self._name, self._fields.keys())\n\n    def __str__(self):\n        """"""Represent this as the following form:\n\n        >>> Unischema(name, [\n        >>>   UnischemaField(name, numpy_dtype, shape, codec, field_nullable),\n        >>>   ...\n        >>> ])\n        """"""\n        fields_str = \'\'\n        for field in self._fields.values():\n            fields_str += \'  {}(\\\'{}\\\', {}, {}, {}, {}),\\n\'.format(type(field).__name__, field.name,\n                                                                   field.numpy_dtype.__name__,\n                                                                   field.shape, field.codec, field.nullable)\n        return \'{}({}, [\\n{}])\'.format(type(self).__name__, self._name, fields_str)\n\n    @property\n    def fields(self):\n        return self._fields\n\n    def as_spark_schema(self):\n        """"""Returns an object derived from the unischema as spark schema.\n\n        Example:\n\n        >>> spark.createDataFrame(dataset_rows,\n        >>>                       SomeSchema.as_spark_schema())\n        """"""\n        # Lazy loading pyspark to avoid creating pyspark dependency on data reading code path\n        # (currently works only with make_batch_reader)\n        import pyspark.sql.types as sql_types\n\n        schema_entries = []\n        for field in self._fields.values():\n            spark_type = _field_spark_dtype(field)\n            schema_entries.append(sql_types.StructField(field.name, spark_type, field.nullable))\n\n        return sql_types.StructType(schema_entries)\n\n    def make_namedtuple(self, **kargs):\n        """"""Returns schema as a namedtuple type intialized with arguments passed to this method.\n\n        Example:\n\n        >>> some_schema.make_namedtuple(field1=10, field2=\'abc\')\n        """"""\n        # TODO(yevgeni): verify types\n        typed_dict = dict()\n        for key in kargs.keys():\n            if kargs[key] is not None:\n                typed_dict[key] = kargs[key]\n            else:\n                typed_dict[key] = None\n        return self._get_namedtuple()(**typed_dict)\n\n    def make_namedtuple_tf(self, *args, **kargs):\n        return self._get_namedtuple()(*args, **kargs)\n\n    @classmethod\n    def from_arrow_schema(cls, parquet_dataset, omit_unsupported_fields=False):\n        """"""\n        Convert an apache arrow schema into a unischema object. This is useful for datasets of only scalars\n        which need no special encoding/decoding. If there is an unsupported type in the arrow schema, it will\n        throw an exception.\n        When the warn_only parameter is turned to True, unsupported column types prints only warnings.\n\n        We do not set codec field in the generated fields since all parquet fields are out-of-the-box supported\n        by pyarrow and we do not need perform any custom decoding.\n\n        :param arrow_schema: :class:`pyarrow.lib.Schema`\n        :param omit_unsupported_fields: :class:`Boolean`\n        :return: A :class:`Unischema` object.\n        """"""\n        meta = compat_get_metadata(parquet_dataset.pieces[0], parquet_dataset.fs.open)\n        arrow_schema = meta.schema.to_arrow_schema()\n        unischema_fields = []\n\n        for partition in parquet_dataset.partitions:\n            if (pa.types.is_binary(partition.dictionary.type) and six.PY2) or \\\n                    (pa.types.is_string(partition.dictionary.type) and six.PY3):\n                numpy_dtype = np.str_\n            elif pa.types.is_int64(partition.dictionary.type):\n                numpy_dtype = np.int64\n            else:\n                raise RuntimeError((\'Expected partition type to be one of currently supported types: string or int64. \'\n                                    \'Got {}\').format(partition.dictionary.type))\n\n            unischema_fields.append(UnischemaField(partition.name, numpy_dtype, (), None, False))\n\n        for column_name in arrow_schema.names:\n            arrow_field = compat_schema_field(arrow_schema, column_name)\n            field_type = arrow_field.type\n            field_shape = ()\n            if isinstance(field_type, ListType):\n                if isinstance(field_type.value_type, ListType) or isinstance(field_type.value_type, pyStructType):\n                    warnings.warn(\'[ARROW-1644] Ignoring unsupported structure %r for field %r\'\n                                  % (field_type, column_name))\n                    continue\n                field_shape = (None,)\n            try:\n                np_type = _numpy_and_codec_from_arrow_type(field_type)\n            except ValueError:\n                if omit_unsupported_fields:\n                    warnings.warn(\'Column %r has an unsupported field %r. Ignoring...\'\n                                  % (column_name, field_type))\n                    continue\n                else:\n                    raise\n            unischema_fields.append(UnischemaField(column_name, np_type, field_shape, None, arrow_field.nullable))\n        return Unischema(\'inferred_schema\', unischema_fields)\n\n\ndef dict_to_spark_row(unischema, row_dict):\n    """"""Converts a single row into a spark Row object.\n\n    Verifies that the data confirms with unischema definition types and encodes the data using the codec specified\n    by the unischema.\n\n    The parameters are keywords to allow use of functools.partial.\n\n    :param unischema: an instance of Unischema object\n    :param row_dict: a dictionary where the keys match name of fields in the unischema.\n    :return: a single pyspark.Row object\n    """"""\n\n    # Lazy loading pyspark to avoid creating pyspark dependency on data reading code path\n    # (currently works only with make_batch_reader)\n    import pyspark\n\n    assert isinstance(unischema, Unischema)\n    # Add null fields. Be careful not to mutate the input dictionary - that would be an unexpected side effect\n    copy_row_dict = copy.copy(row_dict)\n    insert_explicit_nulls(unischema, copy_row_dict)\n\n    if set(copy_row_dict.keys()) != set(unischema.fields.keys()):\n        raise ValueError(\'Dictionary fields \\n{}\\n do not match schema fields \\n{}\'.format(\n            \'\\n\'.join(sorted(copy_row_dict.keys())), \'\\n\'.join(unischema.fields.keys())))\n\n    encoded_dict = {}\n    for field_name, value in copy_row_dict.items():\n        schema_field = unischema.fields[field_name]\n        if value is None:\n            if not schema_field.nullable:\n                raise ValueError(\'Field {} is not ""nullable"", but got passes a None value\')\n        if schema_field.codec:\n            encoded_dict[field_name] = schema_field.codec.encode(schema_field, value) if value is not None else None\n        else:\n            if isinstance(value, (np.generic,)):\n                encoded_dict[field_name] = value.tolist()\n            else:\n                encoded_dict[field_name] = value\n\n    field_list = list(unischema.fields.keys())\n    # generate a value list which match the schema column order.\n    value_list = [encoded_dict[name] for name in field_list]\n    # create a row by value list\n    row = pyspark.Row(*value_list)\n    # set row fields\n    row.__fields__ = field_list\n    return row\n\n\ndef insert_explicit_nulls(unischema, row_dict):\n    """"""If input dictionary has missing fields that are nullable, this function will add the missing keys with\n    None value.\n\n    If the fields that are missing are not nullable, a ``ValueError`` is raised.\n\n    :param unischema: An instance of a unischema\n    :param row_dict: dictionary that would be checked for missing nullable fields. The dictionary is modified inplace.\n    :return: None\n    """"""\n    for field_name, value in unischema.fields.items():\n        if field_name not in row_dict:\n            if value.nullable:\n                row_dict[field_name] = None\n            else:\n                raise ValueError(\'Field {} is not found in the row_dict, but is not nullable.\'.format(field_name))\n\n\ndef _fullmatch(regex, string, flags=0):\n    """"""Emulate python-3.4 re.fullmatch().""""""\n    if six.PY2:\n        m = re.match(regex, string, flags=flags)\n        if m and (m.span() == (0, len(string))):\n            return m\n    else:\n        return re.fullmatch(regex, string, flags)\n\n\ndef match_unischema_fields(schema, field_regex):\n    """"""Returns a list of :class:`~petastorm.unischema.UnischemaField` objects that match a regular expression.\n\n    :param schema: An instance of a :class:`~petastorm.unischema.Unischema` object.\n    :param field_regex: A list of regular expression patterns. A field is matched if the regular expression matches\n      the entire field name.\n    :return: A list of :class:`~petastorm.unischema.UnischemaField` instances matching at least one of the regular\n      expression patterns given by ``field_regex``.\n    """"""\n    if field_regex:\n        unischema_fields = set()\n        legacy_unischema_fields = set()\n        for pattern in field_regex:\n            unischema_fields |= {field for field_name, field in schema.fields.items() if\n                                 _fullmatch(pattern, field_name)}\n            legacy_unischema_fields |= {field for field_name, field in schema.fields.items()\n                                        if re.match(pattern, field_name)}\n        if unischema_fields != legacy_unischema_fields:\n            field_names = {f.name for f in unischema_fields}\n            legacy_field_names = {f.name for f in legacy_unischema_fields}\n            # Sorting list of diff_names so it\'s easier to unit-test the message\n            diff_names = sorted(list((field_names | legacy_field_names) - (field_names & legacy_field_names)))\n            warnings.warn(\'schema_fields behavior has changed. Now, regular expression pattern must match\'\n                          \' the entire field name. The change in the behavior affects \'\n                          \'the following fields: {}\'.format(\', \'.join(diff_names)))\n        return list(unischema_fields)\n    else:\n        return []\n\n\ndef _numpy_and_codec_from_arrow_type(field_type):\n    from pyarrow import types\n\n    if types.is_int8(field_type):\n        np_type = np.int8\n    elif types.is_int16(field_type):\n        np_type = np.int16\n    elif types.is_int32(field_type):\n        np_type = np.int32\n    elif types.is_int64(field_type):\n        np_type = np.int64\n    elif types.is_string(field_type):\n        np_type = np.unicode_\n    elif types.is_boolean(field_type):\n        np_type = np.bool_\n    elif types.is_float32(field_type):\n        np_type = np.float32\n    elif types.is_float64(field_type):\n        np_type = np.float64\n    elif types.is_decimal(field_type):\n        np_type = Decimal\n    elif types.is_binary(field_type):\n        np_type = np.string_\n    elif types.is_fixed_size_binary(field_type):\n        np_type = np.string_\n    elif types.is_date(field_type):\n        np_type = np.datetime64\n    elif types.is_timestamp(field_type):\n        np_type = np.datetime64\n    elif types.is_list(field_type):\n        np_type = _numpy_and_codec_from_arrow_type(field_type.value_type)\n    else:\n        raise ValueError(\'Cannot auto-create unischema due to unsupported column type {}\'.format(field_type))\n    return np_type\n'"
petastorm/utils.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport os\nfrom decimal import Decimal\nfrom multiprocessing import Pool\n\nimport numpy as np\nimport pyarrow\nfrom future.utils import raise_with_traceback\nfrom pyarrow.filesystem import LocalFileSystem\n\nfrom petastorm.compat import compat_get_metadata, compat_with_metadata\n\nlogger = logging.getLogger(__name__)\n\n\ndef run_in_subprocess(func, *args, **kwargs):\n    """"""\n    Run some code in a separate process and return the result. Once the code is done, terminate the process.\n    This prevents a memory leak in the other process from affecting the current process.\n\n    Gotcha: func must be a functioned defined at the top level of the module.\n    :param kwargs: dict\n    :param args: list\n    :param func:\n    :return:\n    """"""\n    pool = Pool(1)\n    result = pool.apply(func, args=args, kwds=kwargs)\n\n    # Probably not strictly necessary since terminate is called on GC, but it\'s not guaranteed when the pool will get\n    # GC\'d.\n    pool.terminate()\n    return result\n\n\nclass DecodeFieldError(RuntimeError):\n    pass\n\n\ndef decode_row(row, schema):\n    """"""\n    Decode dataset row according to coding spec from unischema object\n\n    If a codec is set, we use codec.decode() to produce decoded value.\n\n    For scalar fields, the codec maybe set to `None`. In that case:\n     - If the numpy_dtype is a numpy scalar or a Decimal, cast the \'encoded\' value before returning.\n     - In any other case, return the value \'as is\'.\n\n    :param row: dictionary with encodded values\n    :param schema: unischema object\n    :return:\n    """"""\n    decoded_row = dict()\n    for field_name_unicode, _ in row.items():\n        field_name = str(field_name_unicode)\n        if field_name in schema.fields:\n            try:\n                if row[field_name] is not None:\n                    field = schema.fields[field_name]\n                    codec = schema.fields[field_name].codec\n                    if codec:\n                        decoded_row[field_name] = codec.decode(field, row[field_name])\n                    elif field.numpy_dtype and issubclass(field.numpy_dtype, (np.generic, Decimal)):\n                        decoded_row[field_name] = field.numpy_dtype(row[field_name])\n                    else:\n                        decoded_row[field_name] = row[field_name]\n                else:\n                    decoded_row[field_name] = None\n            except Exception:  # pylint: disable=broad-except\n                raise_with_traceback(DecodeFieldError(\'Decoding field ""{}"" failed\'.format(field_name)))\n\n    return decoded_row\n\n\ndef add_to_dataset_metadata(dataset, key, value):\n    """"""\n    Adds a key and value to the parquet metadata file of a parquet dataset.\n    :param dataset: (ParquetDataset) parquet dataset\n    :param key:     (str) key of metadata entry\n    :param value:   (str) value of metadata\n    """"""\n    if not isinstance(dataset.paths, str):\n        raise ValueError(\'Expected dataset.paths to be a single path, not a list of paths\')\n\n    metadata_file_path = dataset.paths.rstrip(\'/\') + \'/_metadata\'\n    common_metadata_file_path = dataset.paths.rstrip(\'/\') + \'/_common_metadata\'\n    common_metadata_file_crc_path = dataset.paths.rstrip(\'/\') + \'/._common_metadata.crc\'\n\n    # If the metadata file already exists, add to it.\n    # Otherwise fetch the schema from one of the existing parquet files in the dataset\n    if dataset.fs.exists(common_metadata_file_path):\n        with dataset.fs.open(common_metadata_file_path) as f:\n            arrow_metadata = pyarrow.parquet.read_metadata(f)\n    elif dataset.fs.exists(metadata_file_path):\n        # If just the metadata file exists and not the common metadata file, copy the contents of\n        # the metadata file to the common_metadata file for backwards compatibility\n        with dataset.fs.open(metadata_file_path) as f:\n            arrow_metadata = pyarrow.parquet.read_metadata(f)\n    else:\n        arrow_metadata = compat_get_metadata(dataset.pieces[0], dataset.fs.open)\n\n    base_schema = arrow_metadata.schema.to_arrow_schema()\n\n    # base_schema.metadata may be None, e.g.\n    metadata_dict = base_schema.metadata or dict()\n    metadata_dict[key] = value\n    schema = compat_with_metadata(base_schema, metadata_dict)\n\n    with dataset.fs.open(common_metadata_file_path, \'wb\') as metadata_file:\n        pyarrow.parquet.write_metadata(schema, metadata_file)\n\n    # We have just modified _common_metadata file, but the filesystem implementation used by pyarrow does not\n    # update the .crc value. We better delete the .crc to make sure there is no mismatch between _common_metadata\n    # content and the checksum.\n    if isinstance(dataset.fs, LocalFileSystem) and dataset.fs.exists(common_metadata_file_crc_path):\n        try:\n            dataset.fs.rm(common_metadata_file_crc_path)\n        except NotImplementedError:\n            os.remove(common_metadata_file_crc_path)\n'"
petastorm/weighted_sampling_reader.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import division\n\nimport numpy as np\n\n\nclass WeightedSamplingReader(object):\n    """"""Allows to combine outputs of two or more Reader objects, sampling them with a configurable probability.\n    Complies to the same interfaces as :class:`~petastorm.reader.Reader`, hence\n    :class:`~petastorm.weighted_sampling_reader.WeightedSamplingReader` can be used anywhere the\n    :class:`~petastorm.reader.Reader` can be used.""""""\n\n    def __init__(self, readers, probabilities):\n        """"""Creates an instance WeightedSamplingReader.\n\n        The constructor gets a list of readers and probabilities as its parameters. The lists must be the same length.\n        :class:`~petastorm.weighted_sampling_reader.WeightedSamplingReader` implements an iterator interface. Each time\n        a new element is requested, one of the readers is selected, weighted by the matching probability. An element\n        produced by the selected reader is returned.\n\n        The iterator raises StopIteration exception once one of the embedded readers has no more data left.\n\n        The following example shows how a :class:`~petastorm.weighted_sampling_reader.WeightedSamplingReader` can be\n        instantiated with two readers which are sampled with 10% and 90% probabilities respectively.\n\n        >>> from petastorm.weighted_sampling_reader import WeightedSamplingReader\n        >>> from petastorm.reader import Reader\n        >>>\n        >>> with WeightedSamplingReader([Reader(\'file:///dataset1\'), Reader(\'file:///dataset1\')], [0.1, 0.9]) as reader:\n        >>>     new_sample = next(reader)\n\n\n        :param readers: A list of readers. The length of the list must be the same as the length of the\n         ``probabilities`` list.\n        :param probabilities: A list of probabilities. The length of the list must be the same as the length\n          of ``readers`` argument. If the sum of all probability values is not 1.0, it will be automatically\n          normalized.\n\n        """"""\n        if len(readers) <= 1:\n            raise ValueError(\'Two or more readers must be specified. Got {}.\'.format(len(readers)))\n\n        if len(readers) != len(probabilities):\n            raise ValueError(\'readers and probabilities are expected to be lists of the same length\')\n\n        self._readers = readers\n\n        # Normalize probabilities\n        self._cum_prob = np.cumsum(np.asarray(probabilities, dtype=np.float) / np.sum(probabilities))\n\n        for other_idx in range(1, len(readers)):\n            if readers[0].batched_output != readers[other_idx].batched_output:\n                raise ValueError(\'All readers passed to WeightedSamplingReader should have the same value of \'\n                                 \'""batched_output"" attribute\')\n\n            if set(readers[0].schema.fields.keys()) != set(readers[other_idx].schema.fields.keys()):\n                raise ValueError(\'All readers passed to WeightedSamplingReader should have the same schema\')\n\n            # If either of ngram attribute is not None, or the ngrams are different, then we can not mix\n            both_have_ngram = (readers[0].ngram is not None) and (readers[other_idx].ngram is not None)\n            ngram_differ = both_have_ngram and readers[0].ngram != readers[other_idx].ngram\n            only_one_have_ngram = (readers[0].ngram is None) != (readers[other_idx].ngram is None)\n            if only_one_have_ngram or ngram_differ:\n                raise ValueError(\'All readers passed to WeightedSamplingReader should have the same ngram spec\')\n\n        self.batched_output = readers[0].batched_output\n        self.ngram = readers[0].ngram\n        self.schema = readers[0].schema\n\n    def __len__(self):\n        return sum(len(reader) for reader in self._readers)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        r = np.random.random()\n        reader_index = np.where(r < self._cum_prob)[0][0]\n        return next(self._readers[reader_index])\n\n    def next(self):\n        return self.__next__()\n\n    @property\n    def last_row_consumed(self):\n        return any(map(lambda r: r.last_row_consumed, self._readers))\n\n    # Functions needed to treat reader as a context manager\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        for reader in self._readers:\n            reader.stop()\n\n        for reader in self._readers:\n            reader.join()\n'"
docs/autodoc/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# petastorm documentation build configuration file, created by\n# sphinx-quickstart on Wed Aug 29 16:05:14 2018.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport datetime\nimport os\nimport sys\n\nimport alabaster\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nsys.path.insert(0, os.path.abspath(os.path.join(\'..\', \'..\')))\n\nfrom petastorm.__init__ import __version__  # noqa\n\n# append the __init__ to class definitions\nautoclass_content = \'both\'\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\nneeds_sphinx = \'1.2\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'alabaster\',\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.viewcode\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix of source filenames.\nsource_suffix = \'.rst\'\n\n# The encoding of source files.\n# source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'petastorm\'\nauthors = u\'Uber Technologies, Inc.\'\ncopyright = u""2017-{}, {}"".format(datetime.datetime.now().year, authors)\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = __version__\n# The full version, including alpha/beta/rc tags.\nrelease = __version__\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n# language = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n# today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n# today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = [\'_build\']\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n# default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n# add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n# add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n# show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n# modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n# keep_warnings = False\n\n# By default, get only the non-private members\nautodoc_default_flags = [\n    \'members\',\n    \'undoc-members\',\n    #  \'private-members\',\n    #  \'special-members\',\n]\nautodoc_member_order = \'bysource\'\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = \'alabaster\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# For alabaster: https://alabaster.readthedocs.io/en/latest/customization.html\n#\nhtml_theme_options = {\n    \'github_user\': \'uber\',\n    \'github_repo\': \'petastorm\',\n    \'github_button\': True,\n    \'github_type\': \'fork\',\n    \'github_count\': \'true\',\n    \'travis_button\': \'uber/petastorm\',\n    \'fixed_sidebar\': True,\n    \'sidebar_collapse\': True,\n}\n\n# Add any paths that contain custom themes here, relative to this directory.\nhtml_theme_path = [alabaster.get_path()]\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# ""<project> v<release> documentation"".\n# html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\nhtml_short_title = \'petastorm\'\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\nhtml_logo = \'../images/logo-75.png\'\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n# html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n# html_extra_path = []\n\n# If not \'\', a \'Last updated on:\' timestamp is inserted at every page bottom,\n# using the given strftime format.\n# html_last_updated_fmt = \'%b %d, %Y\'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n# html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\nhtml_sidebars = {\n    \'**\': [\n        \'about.html\',\n        \'navigation.html\',\n        \'relations.html\',\n        \'searchbox.html\',\n    ]\n}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n# html_additional_pages = {}\n\n# If false, no module index is generated.\n# html_domain_indices = True\n\n# If false, no index is generated.\n# html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n# html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n# html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n# html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n# html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n# html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n# html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'petastormdoc\'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    # \'preamble\': \'\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (\'index\', \'petastorm.tex\', u\'petastorm Documentation\',\n     u\'Uber Technologies, Inc.\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n# latex_logo = \'../images/logo-120.png\'\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n# latex_use_parts = False\n\n# If true, show page references after internal links.\n# latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n# latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n# latex_appendices = []\n\n# If false, no module index is generated.\n# latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (\'index\', \'petastorm\', u\'petastorm Documentation\',\n     [u\'Uber Technologies, Inc.\'], 1)\n]\n\n# If true, show URL addresses after external links.\n# man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\'index\', \'petastorm\', u\'petastorm Documentation\',\n     u\'Uber Technologies, Inc.\', \'petastorm\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n# Documents to append as an appendix to all manuals.\n# texinfo_appendices = []\n\n# If false, no module index is generated.\n# texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n# texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n# texinfo_no_detailmenu = False\n\nhighlight_language = \'python\'\n'"
examples/hello_world/__init__.py,0,b''
examples/imagenet/__init__.py,0,b''
examples/imagenet/generate_petastorm_imagenet.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""\nThis utility converts a directory with files from the Imagenet dataset (http://image-net.org/) into a\npetastorm dataset (Parquet format).\n\nThe script can run locally (use \'--master=local[*]\' command line argument), or submitted to a spark cluster.\n\nSchema defined in examples.imagenet.schema.ImagenetSchema will be used. The schema\n\nNOTE: Imagenet dataset needs to be requested and downloaded separately by the user.\n""""""\nfrom __future__ import division\n\nimport argparse\nimport glob\nimport json\nimport os\n\nimport cv2\nfrom pyspark.sql import SparkSession\nfrom six.moves.urllib.request import urlopen  # pylint: disable=import-error\n\nfrom examples.imagenet.schema import ImagenetSchema\nfrom petastorm.etl.dataset_metadata import materialize_dataset\nfrom petastorm.unischema import dict_to_spark_row\n\n\ndef _arg_parser():\n    parser = argparse.ArgumentParser(description=__doc__, add_help=False,\n                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(\'-i\', \'--input-path\', type=str, required=True,\n                        help=\'Path to the imagenet directory. If you are running this script on a Spark cluster, \'\n                             \'you should have this file be mounted and accessible to executors.\')\n    parser.add_argument(\'-o\', \'--output-url\', type=str, required=True,\n                        help=\'hdfs://... or file:/// url where the parquet dataset will be written to.\')\n    parser.add_argument(\'-m\', \'--master\', type=str, required=False, default=None,\n                        help=\'Spark master. Use --master=local[*] to run locally.\')\n\n    return parser\n\n\ndef download_nouns_mapping():\n    """"""Downloads a mapping between noun id (``nXXXXXXXX`` form) and the noun string representation.\n\n    :return: A dictionary: ``{noun_id : text}``\n    """"""\n    NOUN_MAP_URL = \'https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\'\n    request = urlopen(NOUN_MAP_URL)\n    class_map_json = request.read()\n\n    # raw_dict has the form of {id: [noun_id, text]}. We flatten it into {noun_id: text}\n    raw_dict = json.loads(class_map_json.decode(""utf-8""))\n    nouns_map = {k: v for k, v in raw_dict.values()}\n    return nouns_map\n\n\ndef imagenet_directory_to_petastorm_dataset(imagenet_path, output_url, spark_master=None, parquet_files_count=100,\n                                            noun_id_to_text=None):\n    """"""Converts a directory with imagenet data into a petastorm dataset.\n\n    Expected directory format is:\n\n    >>> nXXXXXXXX/\n    >>>    *.JPEG\n\n    >>> nZZZZZZZZ/\n    >>>    *.JPEG\n\n    :param imagenet_path: a path to the directory containing ``n*/`` subdirectories. If you are running this script on\n      a Spark cluster, you should have this file be mounted and accessible to executors.\n    :param output_url: the location where your dataset will be written to. Should be a url: either\n      ``file://...`` or ``hdfs://...``\n    :param spark_master: A master parameter used by spark session builder. Use default value (``None``) to use system\n      environment configured spark cluster. Use ``local[*]`` to run on a local box.\n    :param noun_id_to_text: A dictionary: ``{noun_id : text}``. If ``None``, this function will download the dictionary\n      from the Internet.\n    :return: ``None``\n    """"""\n    session_builder = SparkSession \\\n        .builder \\\n        .appName(\'Imagenet Dataset Creation\') \\\n        .config(\'spark.executor.memory\', \'10g\') \\\n        .config(\'spark.driver.memory\', \'10g\')  # Increase the memory if running locally with high number of executors\n    if spark_master:\n        session_builder.master(spark_master)\n\n    spark = session_builder.getOrCreate()\n    sc = spark.sparkContext\n\n    # Get a list of noun_ids\n    noun_ids = os.listdir(imagenet_path)\n    if not all(noun_id.startswith(\'n\') for noun_id in noun_ids):\n        raise RuntimeError(\'Directory {} expected to contain only subdirectories with name \'\n                           \'starting with ""n"".\'.format(imagenet_path))\n\n    if not noun_id_to_text:\n        noun_id_to_text = download_nouns_mapping()\n\n    ROWGROUP_SIZE_MB = 256\n    with materialize_dataset(spark, output_url, ImagenetSchema, ROWGROUP_SIZE_MB):\n        # list of [(nXXXX, \'noun-text\'), ...]\n        noun_id_text_list = map(lambda noun_id: (noun_id, noun_id_to_text[noun_id]), noun_ids)\n\n        # rdd of [(nXXXX, \'noun-text\', path), ...]\n        noun_id_text_image_path_rdd = sc.parallelize(noun_id_text_list, min(len(noun_ids) / 10 + 1, 10000)) \\\n            .flatMap(lambda word_id_label: [word_id_label + (image_path,) for image_path in\n                                            glob.glob(os.path.join(imagenet_path, word_id_label[0], \'*.JPEG\'))])\n\n        # rdd of [(nXXXX, \'noun-text\', image), ...]\n        noun_id_text_image_rdd = noun_id_text_image_path_rdd \\\n            .map(lambda id_word_image_path:\n                 {ImagenetSchema.noun_id.name: id_word_image_path[0],\n                  ImagenetSchema.text.name: id_word_image_path[1],\n                  ImagenetSchema.image.name: cv2.imread(id_word_image_path[2])})\n\n        # Convert to pyspark.sql.Row\n        sql_rows_rdd = noun_id_text_image_rdd.map(lambda r: dict_to_spark_row(ImagenetSchema, r))\n\n        # Write out the result\n        spark.createDataFrame(sql_rows_rdd, ImagenetSchema.as_spark_schema()) \\\n            .coalesce(parquet_files_count) \\\n            .write \\\n            .mode(\'overwrite\') \\\n            .option(\'compression\', \'none\') \\\n            .parquet(output_url)\n\n\nif __name__ == \'__main__\':\n    args = _arg_parser().parse_args()\n    imagenet_directory_to_petastorm_dataset(args.input_path, args.output_url)\n'"
examples/imagenet/schema.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nfrom pyspark.sql.types import StringType\n\nfrom petastorm.codecs import ScalarCodec, CompressedImageCodec\nfrom petastorm.unischema import Unischema, UnischemaField\n\nImagenetSchema = Unischema(\'ImagenetSchema\', [\n    UnischemaField(\'noun_id\', np.string_, (), ScalarCodec(StringType()), False),\n    UnischemaField(\'text\', np.string_, (), ScalarCodec(StringType()), False),\n    UnischemaField(\'image\', np.uint8, (None, None, 3), CompressedImageCodec(\'png\'), False),\n])\n'"
examples/mnist/__init__.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nDEFAULT_MNIST_DATA_PATH = \'/tmp/mnist\'\n'"
examples/mnist/generate_petastorm_mnist.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""\nThis utility converts the MNIST standard dataset (http://yann.lecun.com/exdb/mnist/) into\na petastorm dataset (Parquet format).  The resulting dataset can then be used by main.py\nto demonstrate petastorm usage with pytorch.\n\nThe script can run locally (use \'--master=local[*]\' command line argument), or submitted to a spark cluster.\n\nSchema defined in examples.mnist.schema.MnistSchema will be used.\n\nNOTE: MNIST train and test data will be downloaded automatically.\n""""""\n\nimport argparse\nimport numpy as np\nimport os\nimport shutil\nimport tempfile\n\nfrom pyspark.sql import SparkSession\n\nfrom examples.mnist import DEFAULT_MNIST_DATA_PATH\nfrom examples.mnist.schema import MnistSchema\nfrom petastorm.etl.dataset_metadata import materialize_dataset\nfrom petastorm.unischema import dict_to_spark_row\n\n\ndef _arg_parser():\n    parser = argparse.ArgumentParser(description=__doc__, add_help=True,\n                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'-d\', \'--download-dir\', type=str, required=False, default=None,\n                        help=\'Directory to where the MNIST data will be downloaded; \'\n                             \'default to a tempdir that gets wiped after generation.\')\n    parser.add_argument(\'-o\', \'--output-url\', type=str, required=False,\n                        default=\'file://{}\'.format(DEFAULT_MNIST_DATA_PATH),\n                        help=\'hdfs://... or file:/// url where the parquet dataset will be written to.\')\n    parser.add_argument(\'-m\', \'--master\', type=str, required=False, default=\'local[*]\',\n                        help=\'Spark master; default is local[*] to run locally.\')\n    return parser\n\n\ndef download_mnist_data(download_dir, train=True):\n    """"""\n    Downloads the dataset files and returns the torch Dataset object, which\n    represents the data as an array of (img, label) pairs.\n\n    Each image is a PIL.Image of black-and-white 28x28 pixels.\n    Each label is a long integer representing the digit 0..9.\n    """"""\n    # This is the only function requiring torch in this module.\n\n    # Must import pyarrow before torch. See: https://github.com/uber/petastorm/blob/master/docs/troubleshoot.rst\n    import pyarrow  # noqa: F401 pylint: disable=W0611,W0612\n    from torchvision import datasets\n\n    return datasets.MNIST(\'{}/{}\'.format(download_dir, \'data\'), train=train, download=True)\n\n\ndef mnist_data_to_petastorm_dataset(download_dir, output_url, spark_master=None, parquet_files_count=1,\n                                    mnist_data=None):\n    """"""Converts a directory with MNIST data into a petastorm dataset.\n\n    Data files are as specified in http://yann.lecun.com/exdb/mnist/:\n        * train-images-idx3-ubyte.gz:  training set images (9912422 bytes)\n        * train-labels-idx1-ubyte.gz:  training set labels (28881 bytes)\n        * t10k-images-idx3-ubyte.gz:   test set images (1648877 bytes)\n        * t10k-labels-idx1-ubyte.gz:   test set labels (4542 bytes)\n\n    The images and labels and stored in the IDX file format for vectors and multidimensional matrices of\n    various numerical types, as defined in the same URL.\n\n    :param download_dir: the path to where the MNIST data will be downloaded.\n    :param output_url: the location where your dataset will be written to. Should be a url: either\n      file://... or hdfs://...\n    :param spark_master: A master parameter used by spark session builder. Use default value (None) to use system\n      environment configured spark cluster. Use \'local[*]\' to run on a local box.\n    :param mnist_data: A dictionary of MNIST data, with name of dataset as key, and the dataset object as value;\n      if None is suplied, download it.\n    :return: None\n    """"""\n    session_builder = SparkSession \\\n        .builder \\\n        .appName(\'MNIST Dataset Creation\')\n    if spark_master:\n        session_builder.master(spark_master)\n\n    spark = session_builder.getOrCreate()\n\n    # Get training and test data\n    if mnist_data is None:\n        mnist_data = {\n            \'train\': download_mnist_data(download_dir, train=True),\n            \'test\': download_mnist_data(download_dir, train=False)\n        }\n\n    # The MNIST data is small enough to do everything here in Python\n    for dset, data in mnist_data.items():\n        dset_output_url = \'{}/{}\'.format(output_url, dset)\n        # Using row_group_size_mb=1 to avoid having just a single rowgroup in this example. In a real store, the value\n        # should be similar to an HDFS block size.\n        with materialize_dataset(spark, dset_output_url, MnistSchema, row_group_size_mb=1):\n            # List of [(idx, image, digit), ...]\n            # where image is shaped as a 28x28 numpy matrix\n            idx_image_digit_list = map(lambda idx_image_digit: {\n                MnistSchema.idx.name: idx_image_digit[0],\n                MnistSchema.digit.name: idx_image_digit[1][1],\n                MnistSchema.image.name: np.array(list(idx_image_digit[1][0].getdata()), dtype=np.uint8).reshape(28, 28)\n            }, enumerate(data))\n\n            # Convert to pyspark.sql.Row\n            sql_rows = map(lambda r: dict_to_spark_row(MnistSchema, r), idx_image_digit_list)\n\n            # Write out the result\n            spark.createDataFrame(sql_rows, MnistSchema.as_spark_schema()) \\\n                .coalesce(parquet_files_count) \\\n                .write \\\n                .option(\'compression\', \'none\') \\\n                .parquet(dset_output_url)\n\n\nif __name__ == \'__main__\':\n    args = _arg_parser().parse_args()\n    if args.download_dir is None:\n        # Make a temp dir that we\'ll clean up afterward\n        download_dir = tempfile.mkdtemp()\n    else:\n        download_dir = args.download_dir\n    mnist_data_to_petastorm_dataset(download_dir, args.output_url)\n    if args.download_dir is None:\n        if os.path.exists(download_dir):\n            shutil.rmtree(download_dir)\n'"
examples/mnist/pytorch_example.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n###\n# Adapted to petastorm dataset using original contents from\n# https://github.com/pytorch/examples/mnist/main.py .\n###\nfrom __future__ import division, print_function\n\nimport argparse\n\n# Must import pyarrow before torch. See: https://github.com/uber/petastorm/blob/master/docs/troubleshoot.rst\nimport pyarrow  # noqa: F401 pylint: disable=W0611\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import transforms\n\nfrom examples.mnist import DEFAULT_MNIST_DATA_PATH\nfrom petastorm import make_reader, TransformSpec\nfrom petastorm.pytorch import DataLoader\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    # pylint: disable=arguments-differ\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\ndef train(model, device, train_loader, log_interval, optimizer, epoch):\n    model.train()\n    for batch_idx, row in enumerate(train_loader):\n        data, target = row[\'image\'].to(device), row[\'digit\'].to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % log_interval == 0:\n            print(\'Train Epoch: {} [{}]\\tLoss: {:.6f}\'.format(\n                epoch, batch_idx * len(data), loss.item()))\n\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    count = 0\n    with torch.no_grad():\n        for row in test_loader:\n            data, target = row[\'image\'].to(device), row[\'digit\'].to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction=\'sum\').item()  # sum up batch loss\n            pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n            count += data.shape[0]\n\n    test_loss /= count\n    print(\'\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\'.format(\n        test_loss, correct, count, 100. * correct / count))\n\n\ndef _transform_row(mnist_row):\n    # For this example, the images are stored as simpler ndarray (28,28), but the\n    # training network expects 3-dim images, hence the additional lambda transform.\n    transform = transforms.Compose([\n        transforms.Lambda(lambda nd: nd.reshape(28, 28, 1)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n    # In addition, the petastorm pytorch DataLoader does not distinguish the notion of\n    # data or target transform, but that actually gives the user more flexibility\n    # to make the desired partial transform, as shown here.\n    result_row = {\n        \'image\': transform(mnist_row[\'image\']),\n        \'digit\': mnist_row[\'digit\']\n    }\n\n    return result_row\n\n\ndef main():\n    # Training settings\n    parser = argparse.ArgumentParser(description=\'Petastorm MNIST Example\')\n    default_dataset_url = \'file://{}\'.format(DEFAULT_MNIST_DATA_PATH)\n    parser.add_argument(\'--dataset-url\', type=str,\n                        default=default_dataset_url, metavar=\'S\',\n                        help=\'hdfs:// or file:/// URL to the MNIST petastorm dataset \'\n                             \'(default: %s)\' % default_dataset_url)\n    parser.add_argument(\'--batch-size\', type=int, default=64, metavar=\'N\',\n                        help=\'input batch size for training (default: 64)\')\n    parser.add_argument(\'--test-batch-size\', type=int, default=1000, metavar=\'N\',\n                        help=\'input batch size for testing (default: 1000)\')\n    parser.add_argument(\'--epochs\', type=int, default=10, metavar=\'N\',\n                        help=\'number of epochs to train (default: 10)\')\n    parser.add_argument(\'--all-epochs\', action=\'store_true\', default=False,\n                        help=\'train all epochs before testing accuracy/loss\')\n    parser.add_argument(\'--lr\', type=float, default=0.01, metavar=\'LR\',\n                        help=\'learning rate (default: 0.01)\')\n    parser.add_argument(\'--momentum\', type=float, default=0.5, metavar=\'M\',\n                        help=\'SGD momentum (default: 0.5)\')\n    parser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                        help=\'disables CUDA training\')\n    parser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                        help=\'random seed (default: 1)\')\n    parser.add_argument(\'--log-interval\', type=int, default=10, metavar=\'N\',\n                        help=\'how many batches to wait before logging training status\')\n    args = parser.parse_args()\n    use_cuda = not args.no_cuda and torch.cuda.is_available()\n\n    torch.manual_seed(args.seed)\n\n    device = torch.device(\'cuda\' if use_cuda else \'cpu\')\n\n    model = Net().to(device)\n    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n\n    # Configure loop and Reader epoch for illustrative purposes.\n    # Typical training usage would use the `all_epochs` approach.\n    #\n    if args.all_epochs:\n        # Run training across all the epochs before testing for accuracy\n        loop_epochs = 1\n        reader_epochs = args.epochs\n    else:\n        # Test training accuracy after each epoch\n        loop_epochs = args.epochs\n        reader_epochs = 1\n\n    transform = TransformSpec(_transform_row, removed_fields=[\'idx\'])\n\n    # Instantiate each petastorm Reader with a single thread, shuffle enabled, and appropriate epoch setting\n    for epoch in range(1, loop_epochs + 1):\n        with DataLoader(make_reader(\'{}/train\'.format(args.dataset_url), num_epochs=reader_epochs,\n                                    transform_spec=transform),\n                        batch_size=args.batch_size) as train_loader:\n            train(model, device, train_loader, args.log_interval, optimizer, epoch)\n        with DataLoader(make_reader(\'{}/test\'.format(args.dataset_url), num_epochs=reader_epochs,\n                                    transform_spec=transform),\n                        batch_size=args.test_batch_size) as test_loader:\n            test(model, device, test_loader)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/mnist/schema.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nfrom pyspark.sql.types import IntegerType\n\nfrom petastorm.codecs import ScalarCodec, NdarrayCodec\nfrom petastorm.unischema import Unischema, UnischemaField\n\nMnistSchema = Unischema(\'MnistSchema\', [\n    UnischemaField(\'idx\', np.int_, (), ScalarCodec(IntegerType()), False),\n    UnischemaField(\'digit\', np.int_, (), ScalarCodec(IntegerType()), False),\n    UnischemaField(\'image\', np.uint8, (28, 28), NdarrayCodec(), False),\n])\n'"
examples/mnist/tf_example.py,18,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n###\n# Adapted to petastorm dataset using original contents from\n# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py\n###\n\nfrom __future__ import division, print_function\n\nimport argparse\nimport os\n\nimport tensorflow.compat.v1 as tf  # pylint: disable=import-error\n\nfrom examples.mnist import DEFAULT_MNIST_DATA_PATH\nfrom petastorm import make_reader\nfrom petastorm.tf_utils import tf_tensors\n\n\ndef train_and_test(dataset_url, training_iterations, batch_size, evaluation_interval):\n    """"""\n    Train a model for training iterations with a batch size batch_size, printing accuracy every log_interval.\n    :param dataset_url: The MNIST dataset url.\n    :param training_iterations: The training iterations to train for.\n    :param batch_size: The batch size for training.\n    :param evaluation_interval: The interval used to print the accuracy.\n    :return:\n    """"""\n    with make_reader(os.path.join(dataset_url, \'train\'), num_epochs=None) as train_reader:\n        with make_reader(os.path.join(dataset_url, \'test\'), num_epochs=None) as test_reader:\n            train_readout = tf_tensors(train_reader)\n            train_image = tf.cast(tf.reshape(train_readout.image, [784]), tf.float32)\n            train_label = train_readout.digit\n            batch_image, batch_label = tf.train.batch(\n                [train_image, train_label], batch_size=batch_size\n            )\n\n            W = tf.Variable(tf.zeros([784, 10]))\n            b = tf.Variable(tf.zeros([10]))\n            y = tf.matmul(batch_image, W) + b\n\n            # The raw formulation of cross-entropy,\n            #\n            #   tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.nn.softmax(y)),\n            #                                 reduction_indices=[1]))\n            #\n            # can be numerically unstable.\n            #\n            # So here we use tf.losses.sparse_softmax_cross_entropy on the raw\n            # outputs of \'y\', and then average across the batch.\n            cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=batch_label, logits=y)\n            train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n\n            correct_prediction = tf.equal(tf.argmax(y, 1), batch_label)\n            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n            test_readout = tf_tensors(test_reader)\n            test_image = tf.cast(tf.reshape(test_readout.image, [784]), tf.float32)\n            test_label = test_readout.digit\n            test_batch_image, test_batch_label = tf.train.batch(\n                [test_image, test_label], batch_size=batch_size\n            )\n\n            # Train\n            print(\'Training model for {0} training iterations with batch size {1} and evaluation interval {2}\'.format(\n                training_iterations, batch_size, evaluation_interval\n            ))\n            with tf.Session() as sess:\n                sess.run([\n                    tf.local_variables_initializer(),\n                    tf.global_variables_initializer(),\n                ])\n                coord = tf.train.Coordinator()\n                threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n                try:\n                    for i in range(training_iterations):\n                        if coord.should_stop():\n                            break\n\n                        sess.run(train_step)\n\n                        if (i % evaluation_interval) == 0 or i == (training_iterations - 1):\n                            feed_batch_image, feed_batch_label = sess.run([test_batch_image, test_batch_label])\n                            print(\'After {0} training iterations, the accuracy of the model is: {1:.2f}\'.format(\n                                i,\n                                sess.run(accuracy, feed_dict={\n                                    batch_image: feed_batch_image, batch_label: feed_batch_label\n                                })))\n                finally:\n                    coord.request_stop()\n                    coord.join(threads)\n\n\ndef main():\n    # Training settings\n    parser = argparse.ArgumentParser(description=\'Petastorm Tensorflow MNIST Example\')\n    default_dataset_url = \'file://{}\'.format(DEFAULT_MNIST_DATA_PATH)\n    parser.add_argument(\'--dataset-url\', type=str,\n                        default=default_dataset_url, metavar=\'S\',\n                        help=\'hdfs:// or file:/// URL to the MNIST petastorm dataset\'\n                             \'(default: %s)\' % default_dataset_url)\n    parser.add_argument(\'--training-iterations\', type=int, default=100, metavar=\'N\',\n                        help=\'number of training iterations to train (default: 100)\')\n    parser.add_argument(\'--batch-size\', type=int, default=100, metavar=\'N\',\n                        help=\'input batch size for training (default: 100)\')\n    parser.add_argument(\'--evaluation-interval\', type=int, default=10, metavar=\'N\',\n                        help=\'how many batches to wait before evaluating the model accuracy (default: 10)\')\n    args = parser.parse_args()\n\n    train_and_test(\n        dataset_url=args.dataset_url,\n        training_iterations=args.training_iterations,\n        batch_size=args.batch_size,\n        evaluation_interval=args.evaluation_interval,\n    )\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/spark_dataset_converter/__init__.py,0,b''
examples/spark_dataset_converter/pytorch_converter_example.py,0,"b'#  Copyright (c) 2020 Databricks, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n###\n# Adapted to spark_dataset_converter using original contents from\n# https://github.com/pytorch/examples/blob/master/mnist/main.py\n# This example runs with PySpark > 3.0.0\n###\nfrom __future__ import division\n\nimport logging\nimport tempfile\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom pyspark.sql import SparkSession\nfrom torch.autograd import Variable\n\nfrom examples.spark_dataset_converter.utils import download_mnist_libsvm\nfrom petastorm.spark import SparkDatasetConverter, make_spark_converter\n\ntry:\n    from pyspark.sql.functions import col\nexcept ImportError:\n    raise ImportError(""This script runs with PySpark>=3.0.0"")\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout2d(0.25)\n        self.dropout2 = nn.Dropout2d(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):  # pylint: disable=arguments-differ\n        x = x.view((-1, 1, 28, 28))\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\n\ndef train(data_loader, steps=100, lr=0.0005, momentum=0.5):\n    model = Net()\n    model.train()\n    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n    loss_hist = []\n    for batch_idx, batch in enumerate(data_loader):\n        if batch_idx > steps:\n            break\n        data, target = Variable(batch[\'features\']), Variable(batch[\'label\'])\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            logging.info(\'[%d/%d]\\tLoss: %.6f\', batch_idx, steps, loss.data.item())\n            loss_hist.append(loss.data.item())\n    return model\n\n\ndef test(model, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    test_len = 0\n    with torch.no_grad():\n        for batch in test_loader:\n            data, target = batch[\'features\'], batch[\'label\']\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction=\'sum\').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n            test_len += data.shape[0]\n\n    test_loss /= test_len\n    accuracy = correct / test_len\n\n    logging.info(\'Test set: Average loss: %.4f, Accuracy: %d/%d (%.0f%%)\',\n                 test_loss, correct, test_len, 100. * accuracy)\n    return accuracy\n\n\ndef run(data_dir):\n    # Get SparkSession\n    spark = SparkSession.builder \\\n        .master(""local[2]"") \\\n        .appName(""petastorm.spark pytorch_example"") \\\n        .getOrCreate()\n\n    # Load and preprocess data using Spark\n    df = spark.read.format(""libsvm"") \\\n        .option(""numFeatures"", ""784"") \\\n        .load(data_dir) \\\n        .select(col(""features""), col(""label"").cast(""long"").alias(""label""))\n\n    # Randomly split data into train and test dataset\n    df_train, df_test = df.randomSplit([0.9, 0.1], seed=12345)\n\n    # Set a cache directory for intermediate data.\n    # The path should be accessible by both Spark workers and driver.\n    spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, ""file:///tmp/petastorm/cache/torch-example"")\n\n    converter_train = make_spark_converter(df_train)\n    converter_test = make_spark_converter(df_test)\n\n    def train_and_evaluate(_=None):\n        with converter_train.make_torch_dataloader() as loader:\n            model = train(loader)\n\n        with converter_test.make_torch_dataloader(num_epochs=1) as loader:\n            accuracy = test(model, loader)\n        return accuracy\n\n    # Train and evaluate the model on the local machine\n    accuracy = train_and_evaluate()\n    logging.info(""Train and evaluate the model on the local machine."")\n    logging.info(""Accuracy: %.6f"", accuracy)\n\n    # Train and evaluate the model on a spark worker\n    accuracy = spark.sparkContext.parallelize(range(1)).map(train_and_evaluate).collect()[0]\n    logging.info(""Train and evaluate the model remotely on a spark worker, ""\n                 ""which can be used for distributed hyperparameter tuning."")\n    logging.info(""Accuracy: %.6f"", accuracy)\n\n    # Cleanup\n    converter_train.delete()\n    converter_test.delete()\n    spark.stop()\n\n\ndef main():\n    mnist_dir = tempfile.mkdtemp(\'_mnist_data\')\n    download_mnist_libsvm(mnist_dir)\n    run(data_dir=mnist_dir)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/spark_dataset_converter/tensorflow_converter_example.py,2,"b'#  Copyright (c) 2020 Databricks, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n###\n# Adapted to spark_dataset_converter using original contents from\n# https://github.com/tensorflow/docs/blob/master/site/en/tutorials/quickstart/beginner.ipynb\n# This example runs with PySpark > 3.0.0\n###\nimport logging\nimport tempfile\n\nfrom pyspark.sql import SparkSession\n\nfrom examples.spark_dataset_converter.utils import download_mnist_libsvm\nfrom petastorm.spark import SparkDatasetConverter, make_spark_converter\n\ntry:\n    from pyspark.sql.functions import col\nexcept ImportError:\n    raise ImportError(""This script runs with PySpark>=3.0.0"")\n\n\ndef get_compiled_model(lr=0.001):\n    from tensorflow import keras\n\n    model = keras.models.Sequential([\n        keras.layers.Flatten(input_shape=(28, 28)),\n        keras.layers.Dense(128, activation=\'relu\'),\n        keras.layers.Dropout(0.2),\n        keras.layers.Dense(10),\n    ])\n\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n                  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                  metrics=[\'accuracy\'])\n\n    return model\n\n\ndef train(dataset, steps=1000, lr=0.001):\n    model = get_compiled_model(lr=lr)\n    model.fit(dataset, steps_per_epoch=steps)\n    return model\n\n\ndef run(data_dir):\n    # Get SparkSession\n    spark = SparkSession.builder \\\n        .master(""local[2]"") \\\n        .appName(""petastorm.spark tensorflow_example"") \\\n        .getOrCreate()\n\n    # Load and preprocess data using Spark\n    df = spark.read.format(""libsvm"") \\\n        .option(""numFeatures"", ""784"") \\\n        .load(data_dir) \\\n        .select(col(""features""), col(""label"").cast(""long"").alias(""label""))\n\n    # Randomly split data into train and test dataset\n    df_train, df_test = df.randomSplit([0.9, 0.1], seed=12345)\n\n    # Set a cache directory for intermediate data.\n    # The path should be accessible by both Spark workers and driver.\n    spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, ""file:///tmp/petastorm/cache/tf-example"")\n\n    converter_train = make_spark_converter(df_train)\n    converter_test = make_spark_converter(df_test)\n\n    def train_and_evaluate(_=None):\n        import tensorflow.compat.v1 as tf  # pylint: disable=import-error\n\n        with converter_train.make_tf_dataset() as dataset:\n            dataset = dataset.map(lambda x: (tf.reshape(x.features, [-1, 28, 28]), x.label))\n            model = train(dataset)\n\n        with converter_test.make_tf_dataset(num_epochs=1) as dataset:\n            dataset = dataset.map(lambda x: (tf.reshape(x.features, [-1, 28, 28]), x.label))\n            hist = model.evaluate(dataset)\n\n        return hist[1]\n\n    # Train and evaluate the model on the local machine\n    accuracy = train_and_evaluate()\n    logging.info(""Train and evaluate the model on the local machine."")\n    logging.info(""Accuracy: %.6f"", accuracy)\n\n    # Train and evaluate the model on a spark worker\n    accuracy = spark.sparkContext.parallelize(range(1)).map(train_and_evaluate).collect()[0]\n    logging.info(""Train and evaluate the model remotely on a spark worker, ""\n                 ""which can be used for distributed hyperparameter tuning."")\n    logging.info(""Accuracy: %.6f"", accuracy)\n\n    # Cleanup\n    converter_train.delete()\n    converter_test.delete()\n    spark.stop()\n\n\ndef main():\n    mnist_dir = tempfile.mkdtemp(\'_mnist_data\')\n    download_mnist_libsvm(mnist_dir)\n    run(data_dir=mnist_dir)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/spark_dataset_converter/utils.py,0,"b'import os\n\nimport requests\n\n\ndef download_mnist_libsvm(mnist_data_dir):\n    mnist_data_path = os.path.join(mnist_data_dir, ""mnist.bz2"")\n    data_url = ""https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/mnist.bz2""\n    r = requests.get(data_url)\n    with open(mnist_data_path, ""wb"") as f:\n        f.write(r.content)\n'"
petastorm/benchmark/__init__.py,0,b''
petastorm/benchmark/cli.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""This command line utility instantiates an instance of a Reader and measures its throughput. """"""\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport logging\nimport sys\n\nfrom petastorm.benchmark.throughput import reader_throughput, \\\n    WorkerPoolType, ReadMethod\n\nlogger = logging.getLogger(__name__)\n\n\ndef _parse_args(args):\n    # If min-after-dequeue value is not explicitly set from the command line, it will be calculated from the total\n    # shuffling queue size multiplied by this ratio\n    DEFAULT_MIN_AFTER_DEQUEUE_TO_QUEUE_SIZE_RATIO = 0.8\n\n    parser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.RawTextHelpFormatter)\n\n    parser.add_argument(\'dataset_path\', type=str, help=\'Path to a petastorm dataset\')\n    parser.add_argument(\'--field-regex\', type=str, nargs=\'+\',\n                        help=\'A list of regular expressions. Only fields that match one of the regex patterns will \'\n                             \'be used during the benchmark.\')\n\n    parser.add_argument(\'-w\', \'--workers-count\', type=int, default=3,\n                        help=\'Number of workers used by the reader\')\n    parser.add_argument(\'-p\', \'--pool-type\', type=WorkerPoolType, default=WorkerPoolType.THREAD,\n                        choices=list(WorkerPoolType),\n                        help=\'Type of a worker pool used by the reader\')\n\n    parser.add_argument(\'-m\', \'--warmup-cycles\', type=int, default=200,\n                        help=\'Number of warmup read cycles. Warmup read cycles run before measurement cycles and \'\n                             \'the throughput during these cycles is not accounted for in the reported results.\')\n    parser.add_argument(\'-n\', \'--measure-cycles\', type=int, default=1000,\n                        help=\'Number cycles used for benchmark measurements. Measurements cycles are run after \'\n                             \'warmup cycles.\')\n\n    parser.add_argument(\'--profile-threads\', dest=\'profile_threads\', action=\'store_true\',\n                        help=\'Enables profiling threads. Will print result when thread pool is shut down.\')\n\n    parser.add_argument(\'-d\', \'--read-method\', type=ReadMethod, choices=list(ReadMethod),\n                        default=ReadMethod.PYTHON,\n                        help=\'Which read mode to use: \\\'python\\\': using python implementation. \'\n                             \'\\\'tf\\\': constructing a small TF graph streaming data from pure python implementation.\')\n\n    parser.add_argument(\'-q\', \'--shuffling-queue-size\', type=int, default=500, required=False,\n                        help=\'Size of the shuffling queue used to decorrelate row-group chunks. \')\n\n    parser.add_argument(\'--min-after-dequeue\', type=int, default=None, required=False,\n                        help=\'Minimum number of elements in a shuffling queue before entries can be read from it. \'\n                             \'Default value is set to {}%% of the --shuffling-queue-size \'\n                             \'parameter\'.format(100 * DEFAULT_MIN_AFTER_DEQUEUE_TO_QUEUE_SIZE_RATIO))\n\n    parser.add_argument(\'--pyarrow-serialize\', action=\'store_true\', required=False,\n                        help=\'When specified, faster pyarrow.serialize library is used. However, it does not support \'\n                             \'all data types and implicitly converts some datatypes (e.g. int64->int32) which may\'\n                             \'trigger errors when reading the data from Tensorflow.\')\n\n    parser.add_argument(\'-vv\', action=\'store_true\', default=False, help=\'Sets logging level to DEBUG.\')\n    parser.add_argument(\'-v\', action=\'store_true\', default=False, help=\'Sets logging level to INFO.\')\n\n    args = parser.parse_args(args)\n\n    if not args.min_after_dequeue:\n        args.min_after_dequeue = DEFAULT_MIN_AFTER_DEQUEUE_TO_QUEUE_SIZE_RATIO * args.shuffling_queue_size\n\n    return args\n\n\ndef _main(args):\n    logging.basicConfig()\n    args = _parse_args(args)\n\n    if args.v:\n        logging.getLogger().setLevel(logging.INFO)\n    if args.vv:\n        logging.getLogger().setLevel(logging.DEBUG)\n\n    results = reader_throughput(args.dataset_path, args.field_regex, warmup_cycles_count=args.warmup_cycles,\n                                measure_cycles_count=args.measure_cycles, pool_type=args.pool_type,\n                                loaders_count=args.workers_count, profile_threads=args.profile_threads,\n                                read_method=args.read_method, shuffling_queue_size=args.shuffling_queue_size,\n                                min_after_dequeue=args.min_after_dequeue, pyarrow_serialize=args.pyarrow_serialize)\n\n    logger.info(\'Done\')\n    print(\'Average sample read rate: {:1.2f} samples/sec; RAM {:1.2f} MB (rss); \'\n          \'CPU {:1.2f}%\'.format(results.samples_per_second, results.memory_info.rss / 2 ** 20, results.cpu))\n\n\ndef main():\n    _main(sys.argv[1:])\n\n\nif __name__ == \'__main__\':\n    _main(sys.argv[1:])\n'"
petastorm/benchmark/dummy_reader.py,0,"b'#  Copyright (c) 2017-2020 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom __future__ import print_function\nfrom __future__ import division\nimport time\nimport numpy as np\nfrom petastorm.pytorch import BatchedDataLoader, DataLoader\nfrom collections import namedtuple\nimport torch\nfrom functools import partial\nimport sys\n\n\nclass DummyReader(object):\n    def __init__(self, batch=1000, dim=64):\n        self.batch = batch\n        self.dim = dim\n\n    @property\n    def is_batched_reader(self):\n        return True\n\n    def stop(self):\n        pass\n\n    def join(self):\n        pass\n\n    def __iter__(self):\n        nt = namedtuple(""row"", [""test""])\n        batch = nt(np.random.rand(self.batch, self.dim).astype(np.float32))\n        while True:\n            yield batch\n\n\ndef main(device=\'cpu\', batch=1000, dim=64):\n    print(""Testing DataLoader on cpu"")\n    reader = DummyReader(int(batch), int(dim))\n\n    for batch_size in [10, 100, 1000]:\n        iterations = 100\n        loader = DataLoader(reader, shuffling_queue_capacity=batch_size * 10, batch_size=batch_size)\n        it = iter(loader)\n\n        # Warmup\n        for _ in range(iterations):\n            next(it)\n        print(""Done warming up"")\n\n        tstart = time.time()\n        for _ in range(iterations):\n            next(it)\n        print(""Samples per second for batch {}: {:.4g}"".format(\n            batch_size, (iterations * batch_size) / (time.time() - tstart)))\n\n    print(""Testing BatchedDataLoader on"", device)\n    for batch_size in [10, 100, 1000, 100000]:\n        iterations = 100\n        loader = BatchedDataLoader(reader, shuffling_queue_capacity=batch_size * 10, batch_size=batch_size,\n                                   transform_fn=partial(torch.as_tensor, device=device))\n        it = iter(loader)\n\n        # Warmup\n        for _ in range(iterations):\n            next(it)\n        print(""Done warming up"")\n\n        tstart = time.time()\n        for _ in range(iterations):\n            next(it)\n        print(""Samples per second for batch {}: {:.4g}"".format(\n            batch_size, (iterations * batch_size) / (time.time() - tstart)))\n\n\nif __name__ == ""__main__"":\n    main(*sys.argv[1:])\n'"
petastorm/benchmark/throughput.py,4,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom __future__ import division\n\nimport copy\nimport logging\nimport time\nfrom collections import namedtuple\nfrom concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\nfrom enum import Enum\n\nimport psutil\nimport six\nimport tensorflow.compat.v1 as tf  # pylint: disable=import-error\n\nfrom petastorm import make_reader\nfrom petastorm.etl.dataset_metadata import get_schema_from_dataset_url\nfrom petastorm.reader_impl.pickle_serializer import PickleSerializer\nfrom petastorm.reader_impl.pyarrow_serializer import PyArrowSerializer\nfrom petastorm.tf_utils import tf_tensors\nfrom petastorm.unischema import match_unischema_fields\nfrom petastorm.workers_pool.dummy_pool import DummyPool\nfrom petastorm.workers_pool.process_pool import ProcessPool\nfrom petastorm.workers_pool.thread_pool import ThreadPool\n\nlogger = logging.getLogger(__name__)\n\nBenchmarkResult = namedtuple(\'BenchmarkResult\', [\'time_mean\', \'samples_per_second\', \'memory_info\', \'cpu\'])\n\n\nclass WorkerPoolType(Enum):\n    """"""Defines a type of parallelism used in the benchmark: multithreading, multiprocessing or none (single-thread)""""""\n    THREAD = \'thread\'\n    """"""A thread pool is used by the benchmark""""""\n\n    PROCESS = \'process\'\n    """"""A process pool is used by the benchmark""""""\n\n    NONE = \'dummy\'\n    """"""IO and loading will be done on a single thread. No parallelism.""""""\n\n    def __str__(self):\n        return self.value\n\n\nclass ReadMethod(Enum):\n    """"""Defines whether a Tensorflow or plain Python reading method would be used during the benchmark""""""\n    TF = \'tf\'\n    """"""Tensorflow reading method will be used during the benchmark (``tf_tensor`` method)""""""\n\n    PYTHON = \'python\'\n    """"""Pure python reading method will be used during the benchmark (``next(reader)``)""""""\n\n    def __str__(self):\n        return self.value\n\n\ndef _time_warmup_and_work(reader, warmup_cycles_count, measure_cycles_count, do_work_func=None):\n    if not do_work_func:\n        do_work_func = lambda: next(reader)  # noqa\n\n    _time_multiple_iterations(warmup_cycles_count, do_work_func, lambda: reader.diagnostics)\n\n    logger.info(\'Done warmup\')\n\n    this_process = psutil.Process()\n    this_process.cpu_percent()\n\n    duration = _time_multiple_iterations(measure_cycles_count, do_work_func, lambda: reader.diagnostics)\n\n    cpu_percent = this_process.cpu_percent()\n\n    time_mean = duration / measure_cycles_count\n    result = BenchmarkResult(time_mean=time_mean,\n                             samples_per_second=1.0 / time_mean,\n                             memory_info=this_process.memory_full_info(),\n                             cpu=cpu_percent)\n    logger.info(\'Done measuring: %s\', str(result))\n\n    return result\n\n\ndef _time_warmup_and_work_tf(reader, warmup_cycles_count, measure_cycles_count, shuffling_queue_size,\n                             min_after_dequeue):\n    with tf.Session() as sess:\n        sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n\n        readout_tensors = tf_tensors(reader, shuffling_queue_size, min_after_dequeue)\n\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(coord=coord, start=True, sess=sess)\n\n        result = _time_warmup_and_work(reader, warmup_cycles_count, measure_cycles_count,\n                                       lambda: sess.run(readout_tensors))\n\n        coord.request_stop()\n        coord.join(threads)\n\n    return result\n\n\ndef reader_throughput(dataset_url, field_regex=None, warmup_cycles_count=300, measure_cycles_count=1000,\n                      pool_type=WorkerPoolType.THREAD, loaders_count=3, profile_threads=False,\n                      read_method=ReadMethod.PYTHON, shuffling_queue_size=500, min_after_dequeue=400,\n                      reader_extra_args=None, pyarrow_serialize=False, spawn_new_process=True):\n    """"""Constructs a Reader instance and uses it to performs throughput measurements.\n\n    The function will spawn a new process if ``spawn_separate_process`` is set. This is needed to make memory footprint\n    measurements accurate.\n\n    :param dataset_url: A url of the dataset to be used for measurements.\n    :param field_regex:  A list of regular expressions. Only fields that match one of the regex patterns will be used\n      during the benchmark.\n    :param warmup_cycles_count: Number of warmup cycles. During warmup cycles no measurements are being recorded.\n    :param measure_cycles_count: Number of measurements cycles. Only time elapsed during measurements cycles are used\n      in throughput calculations.\n    :param pool_type: :class:`WorkerPoolType` enum value.\n    :param loaders_count: Number of threads (same thread is used for IO and decoding).\n    :param profile_threads:  Enables profiling threads. Will print result when thread pool is shut down.\n    :param read_method:  An enum :class:`ReadMethod` that defines whether a :class:`petastorm.reader.Reader` will be\n      used.\n    :param shuffling_queue_size: Maximum number of elements in the shuffling queue.\n    :param min_after_dequeue: Minimum number of elements in a shuffling queue before entries can be read from it.\n    :param reader_extra_args: Extra arguments that would be passed to Reader constructor.\n    :param pyarrow_serialize: When True, pyarrow.serialize library will be used for serializing decoded payloads.\n    :param spawn_new_process: This function will respawn itself in a new process if the argument is True. Spawning\n      a new process is needed to get an accurate memory footprint.\n\n    :return: An instance of ``BenchmarkResult`` namedtuple with the results of the benchmark. The namedtuple has\n      the following fields: `time_mean`, `samples_per_second`, `memory_info` and `cpu`\n    """"""\n    if not reader_extra_args:\n        reader_extra_args = dict()\n\n    if spawn_new_process:\n        args = copy.deepcopy(locals())\n        args[\'spawn_new_process\'] = False\n        executor = ProcessPoolExecutor(1)\n        future = executor.submit(reader_throughput, **args)\n        return future.result()\n\n    logger.info(\'Arguments: %s\', locals())\n\n    if \'schema_fields\' not in reader_extra_args:\n        unischema_fields = match_unischema_fields(get_schema_from_dataset_url(dataset_url), field_regex)\n        reader_extra_args[\'schema_fields\'] = unischema_fields\n\n    logger.info(\'Fields used in the benchmark: %s\', str(reader_extra_args[\'schema_fields\']))\n\n    with make_reader(dataset_url,\n                     num_epochs=None,\n                     reader_pool_type=str(pool_type), workers_count=loaders_count, pyarrow_serialize=pyarrow_serialize,\n                     **reader_extra_args) as reader:\n\n        if read_method == ReadMethod.PYTHON:\n            result = _time_warmup_and_work(reader, warmup_cycles_count, measure_cycles_count)\n        elif read_method == ReadMethod.TF:\n            result = _time_warmup_and_work_tf(reader, warmup_cycles_count, measure_cycles_count,\n                                              shuffling_queue_size, min_after_dequeue)\n        else:\n            raise RuntimeError(\'Unexpected reader_type value: %s\', str(read_method))\n\n    return result\n\n\ndef _create_concurrent_executor(pool_type, decoders_count):\n    if pool_type == WorkerPoolType.PROCESS:\n        decoder_pool_executor = ProcessPoolExecutor(decoders_count)\n    elif pool_type == WorkerPoolType.THREAD:\n        decoder_pool_executor = ThreadPoolExecutor(decoders_count)\n    else:\n        raise ValueError(\'Unexpected pool type value: %s\', pool_type)\n    return decoder_pool_executor\n\n\ndef _create_worker_pool(pool_type, workers_count, profiling_enabled, pyarrow_serialize):\n    """"""Different worker pool implementation (in process none or thread-pool, out of process pool)""""""\n    if pool_type == WorkerPoolType.THREAD:\n        worker_pool = ThreadPool(workers_count, profiling_enabled=profiling_enabled)\n    elif pool_type == WorkerPoolType.PROCESS:\n        worker_pool = ProcessPool(workers_count,\n                                  serializer=PyArrowSerializer() if pyarrow_serialize else PickleSerializer())\n    elif pool_type == WorkerPoolType.NONE:\n        worker_pool = DummyPool()\n    else:\n        raise ValueError(\'Supported pool types are thread, process or dummy. Got {}.\'.format(pool_type))\n    return worker_pool\n\n\ndef _time_multiple_iterations(iterations, work_func, diags_info_func=None, report_period=1.0):\n    start_time = time.time()\n    last_reported_time = start_time\n    last_reported_count = 0\n\n    for current_cycle in six.moves.xrange(iterations):\n        work_func()\n        now = time.time()\n        eps = 1e-9\n        if now - last_reported_time > report_period:\n            message = \'{:2.2f} (mean: {:2.2f}) iterations/sec.\' \\\n                .format(float(current_cycle - last_reported_count) / (eps + now - last_reported_time),\n                        float(current_cycle) / (eps + now - start_time))\n            last_reported_count = current_cycle\n            last_reported_time = now\n            if diags_info_func:\n                message += \' diags:{}\'.format(str(diags_info_func()))\n            logging.debug(message)\n\n    return time.time() - start_time\n'"
petastorm/etl/__init__.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport abc\n\nimport six\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass RowGroupIndexerBase(object):\n    """""" Base class for row group indexers.""""""\n\n    @abc.abstractmethod\n    def __add__(self, other):\n        pass\n\n    @abc.abstractproperty\n    def index_name(self):\n        """""" Return unique index name.""""""\n        return None\n\n    @abc.abstractproperty\n    def column_names(self):\n        """""" Return list of column(s) reuired to build index.""""""\n        return None\n\n    @abc.abstractproperty\n    def indexed_values(self):\n        """""" Return list of values in index""""""\n        return None\n\n    @abc.abstractmethod\n    def get_row_group_indexes(self, value_key):\n        """""" Return row groups for given value in index.""""""\n        return None\n\n    @abc.abstractmethod\n    def build_index(self, decoded_rows, piece_index):\n        """""" index values in given rows.""""""\n'"
petastorm/etl/dataset_metadata.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport logging\nimport os\nfrom concurrent import futures\nfrom contextlib import contextmanager\nfrom operator import attrgetter\n\nfrom pyarrow import parquet as pq\nfrom six.moves import cPickle as pickle\nfrom six.moves.urllib.parse import urlparse\n\nfrom petastorm import utils\nfrom petastorm.compat import compat_get_metadata, compat_make_parquet_piece\nfrom petastorm.etl.legacy import depickle_legacy_package_name_compatible\nfrom petastorm.fs_utils import FilesystemResolver, get_filesystem_and_path_or_paths\nfrom petastorm.unischema import Unischema\nfrom packaging import version\n\nlogger = logging.getLogger(__name__)\n\nROW_GROUPS_PER_FILE_KEY = b\'dataset-toolkit.num_row_groups_per_file.v1\'\nUNISCHEMA_KEY = b\'dataset-toolkit.unischema.v1\'\n\n\nclass PetastormMetadataError(Exception):\n    """"""\n    Error to specify when the petastorm metadata does not exist, does not contain the necessary information,\n    or is corrupt/invalid.\n    """"""\n\n\nclass PetastormMetadataGenerationError(Exception):\n    """"""\n    Error to specify when petastorm could not generate metadata properly.\n    This error is usually accompanied with a message to try to regenerate dataset metadata.\n    """"""\n\n\n@contextmanager\ndef materialize_dataset(spark, dataset_url, schema, row_group_size_mb=None, use_summary_metadata=False,\n                        filesystem_factory=None):\n    """"""\n    A Context Manager which handles all the initialization and finalization necessary\n    to generate metadata for a petastorm dataset. This should be used around your\n    spark logic to materialize a dataset (specifically the writing of parquet output).\n\n    Note: Any rowgroup indexing should happen outside the materialize_dataset block\n\n    Example:\n\n    >>> spark = SparkSession.builder...\n    >>> ds_url = \'hdfs:///path/to/my/dataset\'\n    >>> with materialize_dataset(spark, ds_url, MyUnischema, 64):\n    >>>   spark.sparkContext.parallelize(range(0, 10)).\n    >>>     ...\n    >>>     .write.parquet(ds_url)\n    >>> indexer = [SingleFieldIndexer(...)]\n    >>> build_rowgroup_index(ds_url, spark.sparkContext, indexer)\n\n    A user may provide their own recipe for creation of pyarrow filesystem object in ``filesystem_factory``\n    argument (otherwise, petastorm will create a default one based on the url).\n\n    The following example shows how a custom pyarrow HDFS filesystem, instantiated using ``libhdfs`` driver can be used\n    during Petastorm dataset generation:\n\n    >>> resolver=FilesystemResolver(dataset_url, spark.sparkContext._jsc.hadoopConfiguration(),\n    >>>                             hdfs_driver=\'libhdfs\')\n    >>> with materialize_dataset(..., filesystem_factory=resolver.filesystem_factory()):\n    >>>     ...\n\n\n    :param spark: The spark session you are using\n    :param dataset_url: The dataset url to output your dataset to (e.g. ``hdfs:///path/to/dataset``)\n    :param schema: The :class:`petastorm.unischema.Unischema` definition of your dataset\n    :param row_group_size_mb: The parquet row group size to use for your dataset\n    :param use_summary_metadata: Whether to use the parquet summary metadata for row group indexing or a custom\n      indexing method. The custom indexing method is more scalable for very large datasets.\n    :param filesystem_factory: A filesystem factory function to be used when saving Petastorm specific metadata to the\n      Parquet store.\n    """"""\n    spark_config = {}\n    _init_spark(spark, spark_config, row_group_size_mb, use_summary_metadata)\n    yield\n    # After job completes, add the unischema metadata and check for the metadata summary file\n    if filesystem_factory is None:\n        resolver = FilesystemResolver(dataset_url, spark.sparkContext._jsc.hadoopConfiguration(),\n                                      user=spark.sparkContext.sparkUser())\n        filesystem_factory = resolver.filesystem_factory()\n        dataset_path = resolver.get_dataset_path()\n    else:\n        dataset_path = urlparse(dataset_url).path\n    filesystem = filesystem_factory()\n\n    dataset = pq.ParquetDataset(\n        dataset_path,\n        filesystem=filesystem,\n        validate_schema=False)\n\n    _generate_unischema_metadata(dataset, schema)\n    if not use_summary_metadata:\n        _generate_num_row_groups_per_file(dataset, spark.sparkContext, filesystem_factory)\n\n    # Reload the dataset to take into account the new metadata\n    dataset = pq.ParquetDataset(\n        dataset_path,\n        filesystem=filesystem,\n        validate_schema=False)\n    try:\n        # Try to load the row groups, if it fails that means the metadata was not generated properly\n        load_row_groups(dataset)\n    except PetastormMetadataError:\n        raise PetastormMetadataGenerationError(\n            \'Could not find summary metadata file. The dataset will exist but you will need\'\n            \' to execute petastorm-generate-metadata.py before you can read your dataset \'\n            \' in order to generate the necessary metadata.\'\n            \' Try increasing spark driver memory next time and making sure you are\'\n            \' using parquet-mr >= 1.8.3\')\n\n    _cleanup_spark(spark, spark_config, row_group_size_mb)\n\n\ndef _init_spark(spark, current_spark_config, row_group_size_mb=None, use_summary_metadata=False):\n    """"""\n    Initializes spark and hdfs config with necessary options for petastorm datasets\n    before running the spark job.\n    """"""\n\n    # It\'s important to keep pyspark import local because when placed at the top level it somehow messes up with\n    # namedtuple serialization code and we end up getting UnischemaFields objects depickled without overriden __eq__\n    # and __hash__ methods.\n    import pyspark\n    _PYSPARK_BEFORE_24 = version.parse(pyspark.__version__) < version.parse(\'2.4\')\n\n    hadoop_config = spark.sparkContext._jsc.hadoopConfiguration()\n\n    # Store current values so we can restore them later\n    current_spark_config[\'parquet.summary.metadata.level\'] = \\\n        hadoop_config.get(\'parquet.summary.metadata.level\')\n    current_spark_config[\'parquet.enable.summary-metadata\'] = \\\n        hadoop_config.get(\'parquet.enable.summary-metadata\')\n    current_spark_config[\'parquet.summary.metadata.propagate-errors\'] = \\\n        hadoop_config.get(\'parquet.summary.metadata.propagate-errors\')\n    current_spark_config[\'parquet.block.size.row.check.min\'] = \\\n        hadoop_config.get(\'parquet.block.size.row.check.min\')\n    current_spark_config[\'parquet.row-group.size.row.check.min\'] = \\\n        hadoop_config.get(\'parquet.row-group.size.row.check.min\')\n    current_spark_config[\'parquet.block.size\'] = \\\n        hadoop_config.get(\'parquet.block.size\')\n\n    if _PYSPARK_BEFORE_24:\n        hadoop_config.setBoolean(""parquet.enable.summary-metadata"", use_summary_metadata)\n    else:\n        hadoop_config.set(\'parquet.summary.metadata.level\', ""ALL"" if use_summary_metadata else ""NONE"")\n\n    # Our atg fork includes https://github.com/apache/parquet-mr/pull/502 which creates this\n    # option. This forces a job to fail if the summary metadata files cannot be created\n    # instead of just having them fail to be created silently\n    hadoop_config.setBoolean(\'parquet.summary.metadata.propagate-errors\', True)\n    # In our atg fork this config is called parquet.block.size.row.check.min however in newer\n    # parquet versions it will be renamed to parquet.row-group.size.row.check.min\n    # We use both for backwards compatibility\n    hadoop_config.setInt(\'parquet.block.size.row.check.min\', 3)\n    hadoop_config.setInt(\'parquet.row-group.size.row.check.min\', 3)\n    if row_group_size_mb:\n        hadoop_config.setInt(\'parquet.block.size\', row_group_size_mb * 1024 * 1024)\n\n\ndef _cleanup_spark(spark, current_spark_config, row_group_size_mb=None):\n    """"""\n    Cleans up config changes performed in _init_spark\n    """"""\n    hadoop_config = spark.sparkContext._jsc.hadoopConfiguration()\n\n    for key, val in current_spark_config.items():\n        if val is not None:\n            hadoop_config.set(key, val)\n        else:\n            hadoop_config.unset(key)\n\n\ndef _generate_unischema_metadata(dataset, schema):\n    """"""\n    Generates the serialized unischema and adds it to the dataset parquet metadata to be used upon reading.\n    :param dataset: (ParquetDataset) Dataset to attach schema\n    :param schema:  (Unischema) Schema to attach to dataset\n    :return: None\n    """"""\n    # TODO(robbieg): Simply pickling unischema will break if the UnischemaField class is changed,\n    #  or the codec classes are changed. We likely need something more robust.\n    assert schema\n    serialized_schema = pickle.dumps(schema)\n    utils.add_to_dataset_metadata(dataset, UNISCHEMA_KEY, serialized_schema)\n\n\ndef _generate_num_row_groups_per_file(dataset, spark_context, filesystem_factory):\n    """"""\n    Generates the metadata file containing the number of row groups in each file\n    for the parquet dataset located at the dataset_url. It does this in spark by\n    opening all parquet files in the dataset on the executors and collecting the\n    number of row groups in each file back on the driver.\n    :param dataset: :class:`pyarrow.parquet.ParquetDataset`\n    :param spark_context: spark context to use for retrieving the number of row groups\n    in each parquet file in parallel\n    :return: None, upon successful completion the metadata file will exist.\n    """"""\n    if not isinstance(dataset.paths, str):\n        raise ValueError(\'Expected dataset.paths to be a single path, not a list of paths\')\n\n    # Get the common prefix of all the base path in order to retrieve a relative path\n    paths = [piece.path for piece in dataset.pieces]\n\n    # Needed pieces from the dataset must be extracted for spark because the dataset object is not serializable\n    base_path = dataset.paths\n\n    def get_row_group_info(path):\n        fs = filesystem_factory()\n        relative_path = os.path.relpath(path, base_path)\n        pq_file = fs.open(path)\n        num_row_groups = pq.read_metadata(pq_file).num_row_groups\n        pq_file.close()\n        return relative_path, num_row_groups\n\n    row_groups = spark_context.parallelize(paths, len(paths)) \\\n        .map(get_row_group_info) \\\n        .collect()\n    num_row_groups_str = json.dumps(dict(row_groups))\n    # Add the dict for the number of row groups in each file to the parquet file metadata footer\n    utils.add_to_dataset_metadata(dataset, ROW_GROUPS_PER_FILE_KEY, num_row_groups_str)\n\n\ndef load_row_groups(dataset):\n    """"""\n    Load dataset row group pieces from metadata\n    :param dataset: parquet dataset object.\n    :param allow_read_footers: whether to allow reading parquet footers if there is no better way\n            to load row group information\n    :return: splitted pieces, one piece per row group\n    """"""\n    # We try to get row group information from metadata file\n    metadata = dataset.metadata\n    common_metadata = dataset.common_metadata\n    if not metadata and not common_metadata:\n        # If we are inferring the schema we allow reading the footers to get the row group information\n        return _split_row_groups_from_footers(dataset)\n\n    if metadata and metadata.num_row_groups > 0:\n        # If the metadata file exists and has row group information we use it to split the dataset pieces\n        return _split_row_groups(dataset)\n\n    # If we don\'t have row groups in the common metadata we look for the old way of loading it\n    dataset_metadata_dict = common_metadata.metadata\n    if ROW_GROUPS_PER_FILE_KEY not in dataset_metadata_dict:\n        raise PetastormMetadataError(\n            \'Could not find row group metadata in _common_metadata file.\'\n            \' Use materialize_dataset(..) in petastorm.etl.dataset_metadata.py to generate\'\n            \' this file in your ETL code.\'\n            \' You can generate it on an existing dataset using petastorm-generate-metadata.py\')\n    metadata_dict_key = ROW_GROUPS_PER_FILE_KEY\n    row_groups_per_file = json.loads(dataset_metadata_dict[metadata_dict_key].decode())\n\n    rowgroups = []\n    # Force order of pieces. The order is not deterministic since it depends on multithreaded directory\n    # listing implementation inside pyarrow. We stabilize order here, this way we get reproducable order\n    # when pieces shuffling is off. This also enables implementing piece shuffling given a seed\n    sorted_pieces = sorted(dataset.pieces, key=attrgetter(\'path\'))\n    for piece in sorted_pieces:\n        # If we are not using absolute paths, we need to convert the path to a relative path for\n        # looking up the number of row groups.\n        row_groups_key = os.path.relpath(piece.path, dataset.paths)\n        for row_group in range(row_groups_per_file[row_groups_key]):\n            rowgroups.append(compat_make_parquet_piece(piece.path, dataset.fs.open, row_group=row_group,\n                                                       partition_keys=piece.partition_keys))\n    return rowgroups\n\n\n# This code has been copied (with small adjustments) from https://github.com/apache/arrow/pull/2223\n# Once that is merged and released this code can be deleted since we can use the open source\n# implementation.\ndef _split_row_groups(dataset):\n    if not dataset.metadata or dataset.metadata.num_row_groups == 0:\n        raise NotImplementedError(""split_row_groups is only implemented ""\n                                  ""if dataset has parquet summary files ""\n                                  ""with row group information"")\n\n    # We make a dictionary of how many row groups are in each file in\n    # order to split them. The Parquet Metadata file stores paths as the\n    # relative path from the dataset base dir.\n    row_groups_per_file = dict()\n    for i in range(dataset.metadata.num_row_groups):\n        row_group = dataset.metadata.row_group(i)\n        path = row_group.column(0).file_path\n        row_groups_per_file[path] = row_groups_per_file.get(path, 0) + 1\n\n    base_path = os.path.normpath(os.path.dirname(dataset.metadata_path))\n    split_pieces = []\n    for piece in dataset.pieces:\n        # Since the pieces are absolute path, we get the\n        # relative path to the dataset base dir to fetch the\n        # number of row groups in the file\n        relative_path = os.path.relpath(piece.path, base_path)\n\n        # If the path is not in the metadata file, that means there are\n        # no row groups in that file and that file should be skipped\n        if relative_path not in row_groups_per_file:\n            continue\n\n        for row_group in range(row_groups_per_file[relative_path]):\n            split_piece = compat_make_parquet_piece(piece.path, dataset.fs.open, row_group=row_group,\n                                                    partition_keys=piece.partition_keys)\n            split_pieces.append(split_piece)\n\n    return split_pieces\n\n\ndef _split_piece(piece, fs_open):\n    metadata = compat_get_metadata(piece, fs_open)\n    return [compat_make_parquet_piece(piece.path, fs_open,\n                                      row_group=row_group,\n                                      partition_keys=piece.partition_keys)\n            for row_group in range(metadata.num_row_groups)]\n\n\ndef _split_row_groups_from_footers(dataset):\n    """"""Split the row groups by reading the footers of the parquet pieces""""""\n\n    logger.info(\'Recovering rowgroup information for the entire dataset. This can take a long time for datasets with \'\n                \'large number of files. If this dataset was generated by Petastorm \'\n                \'(i.e. by using ""with materialize_dataset(...)"") and you still see this message, \'\n                \'this indicates that the materialization did not finish successfully.\')\n\n    thread_pool = futures.ThreadPoolExecutor()\n\n    futures_list = [thread_pool.submit(_split_piece, piece, dataset.fs.open) for piece in dataset.pieces]\n    result = [item for f in futures_list for item in f.result()]\n    thread_pool.shutdown()\n    return result\n\n\ndef get_schema(dataset):\n    """"""Retrieves schema object stored as part of dataset methadata.\n\n    :param dataset: an instance of :class:`pyarrow.parquet.ParquetDataset object`\n    :return: A :class:`petastorm.unischema.Unischema` object\n    """"""\n    if not dataset.common_metadata:\n        raise PetastormMetadataError(\n            \'Could not find _common_metadata file. Use materialize_dataset(..) in\'\n            \' petastorm.etl.dataset_metadata.py to generate this file in your ETL code.\'\n            \' You can generate it on an existing dataset using petastorm-generate-metadata.py\')\n\n    dataset_metadata_dict = dataset.common_metadata.metadata\n\n    # Read schema\n    if UNISCHEMA_KEY not in dataset_metadata_dict:\n        raise PetastormMetadataError(\n            \'Could not find the unischema in the dataset common metadata file.\'\n            \' Please provide or generate dataset with the unischema attached.\'\n            \' Common Metadata file might not be generated properly.\'\n            \' Make sure to use materialize_dataset(..) in petastorm.etl.dataset_metadata to\'\n            \' properly generate this file in your ETL code.\'\n            \' You can generate it on an existing dataset using petastorm-generate-metadata.py\')\n    ser_schema = dataset_metadata_dict[UNISCHEMA_KEY]\n    # Since we have moved the unischema class around few times, unpickling old schemas will not work. In this case we\n    # override the old import path to get backwards compatibility\n\n    schema = depickle_legacy_package_name_compatible(ser_schema)\n\n    return schema\n\n\ndef get_schema_from_dataset_url(dataset_url_or_urls, hdfs_driver=\'libhdfs3\'):\n    """"""Returns a :class:`petastorm.unischema.Unischema` object loaded from a dataset specified by a url.\n\n    :param dataset_url_or_urls: a url to a parquet directory or a url list (with the same scheme) to parquet files.\n    :param hdfs_driver: A string denoting the hdfs driver to use (if using a dataset on hdfs). Current choices are\n        libhdfs (java through JNI) or libhdfs3 (C++)\n    :return: A :class:`petastorm.unischema.Unischema` object\n    """"""\n    fs, path_or_paths = get_filesystem_and_path_or_paths(dataset_url_or_urls, hdfs_driver)\n\n    dataset = pq.ParquetDataset(path_or_paths, filesystem=fs, validate_schema=False, metadata_nthreads=10)\n\n    # Get a unischema stored in the dataset metadata.\n    stored_schema = get_schema(dataset)\n\n    return stored_schema\n\n\ndef infer_or_load_unischema(dataset):\n    """"""Try to recover Unischema object stored by ``materialize_dataset`` function. If it can be loaded, infer\n    Unischema from native Parquet schema""""""\n    try:\n        return get_schema(dataset)\n    except PetastormMetadataError:\n        logger.info(\'Failed loading Unischema from metadata in %s. Assuming the dataset was not created with \'\n                    \'Petastorm. Will try to construct from native Parquet schema.\')\n        return Unischema.from_arrow_schema(dataset)\n'"
petastorm/etl/legacy.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\n\nfrom six.moves import cPickle as pickle\n\nlogger = logging.getLogger(__name__)\n\n\ndef depickle_legacy_package_name_compatible(pickled_string):\n    """"""Backward compatible way of depickling old pickled strings.\n\n    Previously petastorm package was named differently. In order to be able to load older datasets, we modify\n    module names in the pickled stream with the new ones.\n\n    :param pickled_string: A pickled string to be passed to pickle.loads\n    :return:\n    """"""\n    LEGACY_PACKAGE_NAMES = [\'av.experimental.deepdrive.dataset_toolkit\', \'av.ml.dataset_toolkit\']\n    LEGACY_MODULES = [\'codecs\', \'unischema\', \'sequence\']\n\n    for legacy_package_name in LEGACY_PACKAGE_NAMES:\n        for legacy_module in LEGACY_MODULES:\n            # Substitute module names directly in the pickled stream. Encode as \'ascii\' to make sure no non-ascii\n            # character made its way into package/module name\n            legacy_package_entry = \'\\n(c{}.{}\\n\'.format(legacy_package_name, legacy_module).encode(\'ascii\')\n            new_module_name = \'\\n(cpetastorm.{}\\n\'.format(legacy_module).encode(\'ascii\')\n            modified_pickled_string = pickled_string.replace(legacy_package_entry, new_module_name)\n            if modified_pickled_string != pickled_string:\n                logger.warning(\'Depickling ""%s.%s"" which has moved to ""petastorm.%s"". \'\n                               \'Regenerate metadata.\', legacy_package_name, legacy_module, legacy_module)\n\n            pickled_string = modified_pickled_string\n\n    return pickle.loads(pickled_string)\n'"
petastorm/etl/metadata_util.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n"""""" Script display parquet dataset metadata """"""\nfrom __future__ import print_function\n\nimport argparse\nfrom pyarrow import parquet as pq\n\nfrom petastorm.etl import dataset_metadata, rowgroup_indexing\nfrom petastorm.fs_utils import FilesystemResolver\n\nif __name__ == ""__main__"":\n\n    parser = argparse.ArgumentParser(prog=\'metadata utility\',\n                                     description=\'Show and manipulate parquet dataset metadata\',\n                                     formatter_class=argparse.RawDescriptionHelpFormatter)\n    parser.add_argument(\'--dataset-url\', type=str,\n                        help=\'the url to the dataset base directory\', required=True)\n    parser.add_argument(\'--schema\', action=\'store_true\',\n                        help=\'Display schema from metadata\')\n    parser.add_argument(\'--index\', action=\'store_true\',\n                        help=\'Display list of row group indexes\')\n    parser.add_argument(\'--print-values\', action=\'store_true\',\n                        help=\'Print index values (dataset piece indexes)\')\n    parser.add_argument(\'--skip-index\', nargs=\'+\', type=str,\n                        help=\'Donot display indexed values for given fields\')\n    parser.add_argument(\'--hdfs-driver\', type=str, default=\'libhdfs3\',\n                        help=\'A string denoting the hdfs driver to use (if using a dataset on hdfs). \'\n                             \'Current choices are libhdfs (java through JNI) or libhdfs3 (C++)\')\n\n    args = parser.parse_args()\n\n    if args.dataset_url and args.dataset_url[-1] == \'/\':\n        args.dataset_url = args.dataset_url[:-1]\n\n    # Create pyarrow file system\n    resolver = FilesystemResolver(args.dataset_url, hdfs_driver=args.hdfs_driver)\n    dataset = pq.ParquetDataset(resolver.get_dataset_path(), filesystem=resolver.filesystem(),\n                                validate_schema=False)\n\n    print_all = not args.schema and not args.index\n    if args.schema or print_all:\n        print(\'*** Schema from dataset metadata ***\')\n        print((dataset_metadata.get_schema(dataset)))\n\n    if args.index or print_all:\n        index_dict = rowgroup_indexing.get_row_group_indexes(dataset)\n        print(\'*** Row group indexes from dataset metadata ***\')\n        for index_name in index_dict:\n            print((\'Index: {}\'.format(index_name)))\n            if args.skip_index is None or index_name not in args.skip_index:\n                for field_value in index_dict[index_name].indexed_values:\n                    print(\'  -- {}({})\'.format(field_value,\n                                               len(index_dict[index_name].get_row_group_indexes(field_value))))\n                    if args.print_values:\n                        print(index_dict[index_name].get_row_group_indexes(field_value))\n            else:\n                print(\'  (skipped)\')\n'"
petastorm/etl/petastorm_generate_metadata.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Script to add petastorm metadata to an existing parquet dataset""""""\n\nimport argparse\nimport sys\nfrom pydoc import locate\n\nfrom pyarrow import parquet as pq\nfrom pyspark.sql import SparkSession\n\nfrom petastorm.etl.dataset_metadata import materialize_dataset, get_schema, ROW_GROUPS_PER_FILE_KEY\nfrom petastorm.etl.rowgroup_indexing import ROWGROUPS_INDEX_KEY\nfrom petastorm.fs_utils import FilesystemResolver\nfrom petastorm.unischema import Unischema\nfrom petastorm.utils import add_to_dataset_metadata\n\nexample_text = \'\'\'Example (some replacement required):\n\nLocally:\npetastorm-generate-metadata.py \\\\\n    --dataset_url hdfs:///path/to/my/hello_world_dataset \\\\\n    --unischema_class examples.hello_world.generate_hello_world_dataset.HelloWorldSchema \\\\\n    --master local[*]\n\nOn Spark:\nspark-submit \\\\\n    --master spark://ip:port \\\\\n    $(which petastorm-generate-metadata.py) \\\\\n    --dataset_url hdfs:///path/to/my/hello_world_dataset \\\\\n    --unischema_class examples.hello_world.generate_hello_world_dataset.HelloWorldSchema\n\'\'\'\n\n\ndef generate_petastorm_metadata(spark, dataset_url, unischema_class=None, use_summary_metadata=False,\n                                hdfs_driver=\'libhdfs3\'):\n    """"""\n    Generates metadata necessary to read a petastorm dataset to an existing dataset.\n\n    :param spark: spark session\n    :param dataset_url: url of existing dataset\n    :param unischema_class: (optional) fully qualified dataset unischema class. If not specified will attempt\n        to find one already in the dataset. (e.g.\n        :class:`examples.hello_world.generate_hello_world_dataset.HelloWorldSchema`)\n    :param hdfs_driver: A string denoting the hdfs driver to use (if using a dataset on hdfs). Current choices are\n        libhdfs (java through JNI) or libhdfs3 (C++)\n    :param user: String denoting username when connecting to HDFS\n    """"""\n    sc = spark.sparkContext\n\n    resolver = FilesystemResolver(dataset_url, sc._jsc.hadoopConfiguration(), hdfs_driver=hdfs_driver,\n                                  user=spark.sparkContext.sparkUser())\n    fs = resolver.filesystem()\n    dataset = pq.ParquetDataset(\n        resolver.get_dataset_path(),\n        filesystem=fs,\n        validate_schema=False)\n\n    if unischema_class:\n        schema = locate(unischema_class)\n        if not isinstance(schema, Unischema):\n            raise ValueError(\'The specified class %s is not an instance of a petastorm.Unischema object.\',\n                             unischema_class)\n    else:\n\n        try:\n            schema = get_schema(dataset)\n        except ValueError:\n            raise ValueError(\'Unischema class could not be located in existing dataset,\'\n                             \' please specify it\')\n\n    # In order to be backwards compatible, we retrieve the common metadata from the dataset before\n    # overwriting the metadata to keep row group indexes and the old row group per file index\n    arrow_metadata = dataset.common_metadata or None\n\n    with materialize_dataset(spark, dataset_url, schema, use_summary_metadata=use_summary_metadata,\n                             filesystem_factory=resolver.filesystem_factory()):\n        if use_summary_metadata:\n            # Inside the materialize dataset context we just need to write the metadata file as the schema will\n            # be written by the context manager.\n            # We use the java ParquetOutputCommitter to write the metadata file for the existing dataset\n            # which will read all the footers of the dataset in parallel and merge them.\n            hadoop_config = sc._jsc.hadoopConfiguration()\n            Path = sc._gateway.jvm.org.apache.hadoop.fs.Path\n            parquet_output_committer = sc._gateway.jvm.org.apache.parquet.hadoop.ParquetOutputCommitter\n            parquet_output_committer.writeMetaDataFile(hadoop_config, Path(dataset_url))\n\n    spark.stop()\n\n    if use_summary_metadata and arrow_metadata:\n        # When calling writeMetaDataFile it will overwrite the _common_metadata file which could have schema information\n        # or row group indexers. Therefore we want to retain this information and will add it to the new\n        # _common_metadata file. If we were using the old legacy metadata method this file wont be deleted\n        base_schema = arrow_metadata.schema.to_arrow_schema()\n        metadata_dict = base_schema.metadata\n        if ROW_GROUPS_PER_FILE_KEY in metadata_dict:\n            add_to_dataset_metadata(dataset, ROW_GROUPS_PER_FILE_KEY, metadata_dict[ROW_GROUPS_PER_FILE_KEY])\n        if ROWGROUPS_INDEX_KEY in metadata_dict:\n            add_to_dataset_metadata(dataset, ROWGROUPS_INDEX_KEY, metadata_dict[ROWGROUPS_INDEX_KEY])\n\n\ndef _main(args):\n    parser = argparse.ArgumentParser(prog=\'petastorm_generate_metadata\',\n                                     description=\'Add necessary petastorm metadata to an existing dataset\',\n                                     epilog=example_text,\n                                     formatter_class=argparse.RawDescriptionHelpFormatter)\n    parser.add_argument(\'--dataset_url\',\n                        help=\'the url to the dataset base directory\', required=True)\n    parser.add_argument(\'--unischema_class\',\n                        help=\'the fully qualified class of the dataset unischema. If not specified will attempt\'\n                             \' to reuse schema already in dataset. \'\n                             \'(e.g. examples.hello_world.generate_hello_world_dataset.HelloWorldSchema)\',\n                        required=False)\n    parser.add_argument(\'--master\', type=str,\n                        help=\'Spark master. Default if not specified. To run on a local machine, specify \'\n                             \'""local[W]"" (where W is the number of local spark workers, e.g. local[10])\')\n    parser.add_argument(\'--spark-driver-memory\', type=str, help=\'The amount of memory the driver process will have\',\n                        default=\'4g\')\n    parser.add_argument(\'--use-summary-metadata\', action=\'store_true\',\n                        help=\'Whether to use the parquet summary metadata format.\'\n                             \' Not scalable for large amounts of columns and/or row groups.\')\n    parser.add_argument(\'--hdfs-driver\', type=str, default=\'libhdfs3\',\n                        help=\'A string denoting the hdfs driver to use (if using a dataset on hdfs). \'\n                             \'Current choices are libhdfs (java through JNI) or libhdfs3 (C++)\')\n    args = parser.parse_args(args)\n\n    # Open Spark Session\n    spark_session = SparkSession \\\n        .builder \\\n        .appName(""Petastorm Generate Metadata"") \\\n        .config(\'spark.driver.memory\', args.spark_driver_memory)\n    if args.master:\n        spark_session.master(args.master)\n\n    spark = spark_session.getOrCreate()\n\n    generate_petastorm_metadata(spark, args.dataset_url, args.unischema_class, args.use_summary_metadata,\n                                hdfs_driver=args.hdfs_driver)\n\n    # Shut down the spark sessions and context\n    spark.stop()\n\n\ndef main():\n    _main(sys.argv[1:])\n\n\nif __name__ == \'__main__\':\n    _main(sys.argv[1:])\n'"
petastorm/etl/rowgroup_indexers.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nfrom collections import defaultdict\n\nfrom petastorm.etl import RowGroupIndexerBase\n\n\nclass SingleFieldIndexer(RowGroupIndexerBase):\n    """"""\n    Class to index single field in parquet dataset.\n\n    This indexer only indexes numpty strings, numpty integers, or numpy arrays of strings.\n    """"""\n\n    def __init__(self, index_name, index_field):\n        self._index_name = index_name\n        self._column_name = index_field\n        self._index_data = defaultdict(set)\n\n    def __add__(self, other):\n        if not isinstance(other, SingleFieldIndexer):\n            raise TypeError(""Make sure Spark map function return the same indexer type"")\n        if self._column_name != other._column_name:\n            raise ValueError(""Make sure indexers in Spark map function index the same fields"")\n\n        for value_key in other._index_data:\n            self._index_data[value_key].update(other._index_data[value_key])\n\n        return self\n\n    @property\n    def index_name(self):\n        return self._index_name\n\n    @property\n    def column_names(self):\n        return [self._column_name]\n\n    @property\n    def indexed_values(self):\n        return list(self._index_data.keys())\n\n    def get_row_group_indexes(self, value_key):\n        return self._index_data[value_key]\n\n    def build_index(self, decoded_rows, piece_index):\n        field_column = [row[self._column_name] for row in decoded_rows]\n        if not field_column:\n            raise ValueError(""Cannot build index for empty rows, column \'{}\'""\n                             .format(self._column_name))\n\n        for field_val in field_column:\n            if field_val is not None:\n                # check type of field, if it is array index each array value,\n                # otherwise index field value directly\n                if isinstance(field_val, np.ndarray):\n                    for val in field_val:\n                        self._index_data[val].add(piece_index)\n                else:\n                    self._index_data[field_val].add(piece_index)\n\n        return self._index_data\n\n\nclass FieldNotNullIndexer(RowGroupIndexerBase):\n    """"""\n    Class to index \'Not Null\' condition forsingle field in parquet dataset\n    """"""\n\n    def __init__(self, index_name, index_field):\n        self._index_name = index_name\n        self._column_name = index_field\n        self._index_data = set()\n\n    def __add__(self, other):\n        if not isinstance(other, FieldNotNullIndexer):\n            raise TypeError(""Make sure Spark map function return the same indexer type"")\n        if self._column_name != other._column_name:\n            raise ValueError(""Make sure indexers in Spark map function index the same fields"")\n\n        self._index_data.update(other._index_data)\n\n        return self\n\n    @property\n    def index_name(self):\n        return self._index_name\n\n    @property\n    def column_names(self):\n        return [self._column_name]\n\n    @property\n    def indexed_values(self):\n        return [\'Field is Not Null\']\n\n    def get_row_group_indexes(self, value_key=None):\n        return self._index_data\n\n    def build_index(self, decoded_rows, piece_index):\n        field_column = [row[self._column_name] for row in decoded_rows]\n        if not field_column:\n            raise ValueError(""Cannot build index for empty rows, column \'{}\'""\n                             .format(self._column_name))\n\n        for field_val in field_column:\n            if field_val is not None:\n                self._index_data.add(piece_index)\n                break\n\n        return self._index_data\n'"
petastorm/etl/rowgroup_indexing.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport time\nfrom collections import namedtuple\n\nfrom pyarrow import parquet as pq\nfrom six.moves import cPickle as pickle\nfrom six.moves import range\n\nfrom petastorm import utils\nfrom petastorm.compat import compat_piece_read, compat_make_parquet_piece\nfrom petastorm.etl import dataset_metadata\nfrom petastorm.etl.legacy import depickle_legacy_package_name_compatible\nfrom petastorm.fs_utils import FilesystemResolver\n\nlogger = logging.getLogger(__name__)\n\nPARALLEL_SLICE_NUM = 2000\n\nROWGROUPS_INDEX_KEY = b\'dataset-toolkit.rowgroups_index.v1\'\n\nPieceInfo = namedtuple(\'PieceInfo\', [\'piece_index\', \'path\', \'row_group\', \'partition_keys\'])\n\n\ndef build_rowgroup_index(dataset_url, spark_context, indexers, hdfs_driver=\'libhdfs3\'):\n    """"""\n    Build index for given list of fields to use for fast rowgroup selection\n    :param dataset_url: (str) the url for the dataset (or a path if you would like to use the default hdfs config)\n    :param spark_context: (SparkContext)\n    :param indexers: list of objects to build row groups indexes. Should support RowGroupIndexerBase interface\n    :param hdfs_driver: A string denoting the hdfs driver to use (if using a dataset on hdfs). Current choices are\n    libhdfs (java through JNI) or libhdfs3 (C++)\n    :return: None, upon successful completion the rowgroup predicates will be saved to _metadata file\n    """"""\n\n    if dataset_url and dataset_url[-1] == \'/\':\n        dataset_url = dataset_url[:-1]\n\n    # Create pyarrow file system\n    resolver = FilesystemResolver(dataset_url, spark_context._jsc.hadoopConfiguration(),\n                                  hdfs_driver=hdfs_driver, user=spark_context.sparkUser())\n    dataset = pq.ParquetDataset(resolver.get_dataset_path(), filesystem=resolver.filesystem(),\n                                validate_schema=False)\n\n    split_pieces = dataset_metadata.load_row_groups(dataset)\n    schema = dataset_metadata.get_schema(dataset)\n\n    # We need direct reference on partitions object\n    partitions = dataset.partitions\n    pieces_num = len(split_pieces)\n    piece_info_list = []\n    for piece_index in range(pieces_num):\n        #  indexes relies on the ordering of the split dataset pieces.\n        # This relies on how the dataset pieces are split and sorted which although should not change,\n        # still might and we should make sure not to forget that could break this.\n        piece = split_pieces[piece_index]\n        piece_info_list.append(PieceInfo(piece_index, piece.path, piece.row_group, piece.partition_keys))\n\n    start_time = time.time()\n    piece_info_rdd = spark_context.parallelize(piece_info_list, min(len(piece_info_list), PARALLEL_SLICE_NUM))\n    indexer_rdd = piece_info_rdd.map(lambda piece_info: _index_columns(piece_info, dataset_url, partitions,\n                                                                       indexers, schema, hdfs_driver=hdfs_driver))\n    indexer_list = indexer_rdd.reduce(_combine_indexers)\n\n    indexer_dict = {indexer.index_name: indexer for indexer in indexer_list}\n    serialized_indexers = pickle.dumps(indexer_dict, pickle.HIGHEST_PROTOCOL)\n    utils.add_to_dataset_metadata(dataset, ROWGROUPS_INDEX_KEY, serialized_indexers)\n    logger.info(""Elapsed time of index creation: %f s"", (time.time() - start_time))\n\n\ndef _index_columns(piece_info, dataset_url, partitions, indexers, schema, hdfs_driver=\'libhdfs3\'):\n    """"""\n    Function build indexes for  dataset piece described in piece_info\n    :param piece_info: description of dataset piece\n    :param dataset_url: dataset location\n    :param partitions: dataset partitions\n    :param indexers: list of indexer objects\n    :param schema: dataset schema\n    :param hdfs_driver: A string denoting the hdfs driver to use (if using a dataset on hdfs). Current choices are\n        libhdfs (java through JNI) or libhdfs3 (C++)\n    :return: list of indexers containing index data\n    """"""\n    # Resolver in executor context will get hadoop config from environment\n    resolver = FilesystemResolver(dataset_url, hdfs_driver=hdfs_driver)\n    fs = resolver.filesystem()\n\n    # Create pyarrow piece\n    piece = compat_make_parquet_piece(piece_info.path, fs.open, row_group=piece_info.row_group,\n                                      partition_keys=piece_info.partition_keys)\n\n    # Collect column names needed for indexing\n    column_names = set()\n    for indexer in indexers:\n        column_names.update(indexer.column_names)\n\n    # Read columns needed for indexing\n    column_rows = compat_piece_read(piece, fs.open, columns=list(column_names),\n                                    partitions=partitions).to_pandas().to_dict(\'records\')\n\n    # Decode columns values\n    decoded_rows = [utils.decode_row(row, schema) for row in column_rows]\n    if not decoded_rows:\n        raise ValueError(\'Cannot build index with empty decoded_rows, columns: {}, partitions: {}\'\n                         .format(column_names, partitions))\n\n    # Index columns values\n    for indexer in indexers:\n        indexer.build_index(decoded_rows, piece_info.piece_index)\n\n    # Indexer objects contain index data, it will be consolidated on reduce phace\n    return indexers\n\n\ndef _combine_indexers(indexers1, indexers2):\n    """""" Conbine index data from two indexers\n    :param indexers1: list of indexers to combine index data\n    :param indexers2: second list of indexers to combine index data\n    :return: first list of indexers containing index data from both indexers in pair""""""\n    if len(indexers1) != len(indexers2):\n        raise ValueError(\'Cannot reduce results with different dimensions\')\n\n    return [indexer_pair[0] + indexer_pair[1] for indexer_pair in zip(indexers1, indexers2)]\n\n\ndef get_row_group_indexes(dataset):\n    """"""\n    Extract and return row group indexes from dataset\n    :param dataset: dataset object\n    :return: dataset indexes as dictionary\n    """"""\n    if not dataset.common_metadata:\n        raise ValueError(\'Could not find _metadata file. add_dataset_metadata(..) in\'\n                         \' petastorm.etl.dataset_metadata.py should be used to\'\n                         \' generate this file in your ETL code.\'\n                         \' You can generate it on an existing dataset using rowgroup_indexing_run.py\')\n\n    dataset_metadata_dict = dataset.common_metadata.metadata\n\n    # Load rowgroups_index\n    if ROWGROUPS_INDEX_KEY not in dataset_metadata_dict:\n        raise ValueError(\'Row groups index is not available in the dataset metadata file. \'\n                         \'You can generate it on an existing dataset using rowgroup_indexing_run.py\')\n\n    serialized_indexes = dataset_metadata_dict[ROWGROUPS_INDEX_KEY]\n\n    index_dict = depickle_legacy_package_name_compatible(serialized_indexes)\n    return index_dict\n'"
petastorm/gcsfs_helpers/__init__.py,0,b''
petastorm/gcsfs_helpers/gcsfs_wrapper.py,0,"b'import posixpath\n\nfrom pyarrow.filesystem import FileSystem, DaskFileSystem\nfrom pyarrow.util import implements, _stringify_path\n\n\nclass GCSFSWrapper(DaskFileSystem):\n\n    @implements(FileSystem.isdir)\n    def isdir(self, path):\n        from gcsfs.core import norm_path\n        path = norm_path(_stringify_path(path))\n        try:\n            contents = self.fs.ls(path)\n            if len(contents) == 1 and contents[0] == path:\n                return False\n            else:\n                return True\n        except OSError:\n            return False\n\n    @implements(FileSystem.isfile)\n    def isfile(self, path):\n        from gcsfs.core import norm_path\n        path = norm_path(_stringify_path(path))\n        try:\n            contents = self.fs.ls(path)\n            return len(contents) == 1 and contents[0] == path\n        except OSError:\n            return False\n\n    def walk(self, path):\n        """"""\n        Directory tree generator, like os.walk\n\n        Generator version of what is in gcsfs, which yields a flattened list of\n        files\n        """"""\n        from gcsfs.core import norm_path\n        path = norm_path(_stringify_path(path))\n        directories = set()\n        files = set()\n\n        for key in self.fs.ls(path, detail=True):\n            # each info name must be at least [path]/part , but here\n            # we check also for names like [path]/part/\n            path = key[\'name\']\n            if key[\'storageClass\'] == \'DIRECTORY\':\n                if path.endswith(\'/\'):\n                    directories.add(path[:-1])\n                else:\n                    directories.add(path)\n            elif key[\'storageClass\'] == \'BUCKET\':\n                pass\n            else:\n                files.add(path)\n\n        files = sorted([posixpath.split(f)[1] for f in files\n                        if f not in directories])\n        directories = sorted([posixpath.split(x)[1]\n                              for x in directories])\n\n        yield path, directories, files\n\n        for directory in directories:\n            for tup in self.walk(directory):\n                yield tup\n'"
petastorm/hdfs/__init__.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""\nCommon HDFS functional modules.\n""""""\n'"
petastorm/hdfs/namenode.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport functools\nimport inspect\nimport logging\nimport os\nfrom distutils.version import LooseVersion\nfrom xml.etree import ElementTree as ET\n\nimport pyarrow\nimport six\nfrom pyarrow.hdfs import HadoopFileSystem\nfrom pyarrow.lib import ArrowIOError\nfrom six.moves.urllib.parse import urlparse\n\nlogger = logging.getLogger(__name__)\n\n\nclass HdfsNamenodeResolver(object):\n    """"""This class embodies functionality to resolve HDFS namenodes: per default or a nameservice.""""""\n\n    def __init__(self, hadoop_configuration=None):\n        """"""\n        Sets the given HadoopConfiguration object for the resolver; or check for and pull hadoop\n        configuration from an environment variable, in below preferred order to check.\n\n        :param hadoop_configuration: an optional ``HadoopConfiguration``\n        """"""\n        self._hadoop_env = None\n        self._hadoop_path = None\n        if hadoop_configuration is None:\n            # Pull from environment variable, in this preferred order\n            for env in [""HADOOP_HOME"", ""HADOOP_PREFIX"", ""HADOOP_INSTALL""]:\n                # Use the first available\n                if env in os.environ:\n                    self._hadoop_env = env\n                    self._hadoop_path = os.environ[env]\n                    hadoop_configuration = {}\n                    self._load_site_xml_into_dict(\n                        \'{}/etc/hadoop/hdfs-site.xml\'.format(self._hadoop_path),\n                        hadoop_configuration)\n                    self._load_site_xml_into_dict(\n                        \'{}/etc/hadoop/core-site.xml\'.format(self._hadoop_path),\n                        hadoop_configuration)\n                    break\n            if hadoop_configuration is None:\n                # ensures at least an empty dict so no further checks required in member functions\n                logger.warning(\'Unable to populate a sensible HadoopConfiguration for namenode resolution!\\n\'\n                               \'Path of last environment var (%s) tried [%s]. Please set up your Hadoop and \\n\'\n                               \'define environment variable HADOOP_HOME to point to your Hadoop installation path.\',\n                               self._hadoop_env, self._hadoop_path)\n                hadoop_configuration = {}\n        self._hadoop_configuration = hadoop_configuration\n\n    def _load_site_xml_into_dict(self, xml_path, in_dict):\n        assert in_dict is not None, \'A valid dictionary must be supplied to process site XML\'\n        try:\n            for prop in ET.parse(xml_path).getroot().iter(\'property\'):\n                in_dict[prop.find(\'name\').text] = prop.find(\'value\').text\n        except ET.ParseError as ex:\n            logger.error(\n                \'Unable to obtain a root node for the supplied XML in %s: %s\', xml_path, ex)\n\n    def _build_error_string(self, msg):\n        if self._hadoop_path is not None:\n            return msg + \'\\nHadoop path {} in environment variable {}!\\n\' \\\n                         \'Please check your hadoop configuration!\' \\\n                .format(self._hadoop_path, self._hadoop_env)\n        else:\n            return msg + \' the supplied Spark HadoopConfiguration\'\n\n    def resolve_hdfs_name_service(self, namespace):\n        """"""\n        Given the namespace of a name service, resolves the configured list of name nodes, and\n        returns them as a list of URL strings.\n\n        :param namespace: the HDFS name service to resolve\n        :return: a list of URL strings of the name nodes for the given name service; or None of not\n            properly configured.\n        """"""\n        list_of_namenodes = None\n        namenodes = self._hadoop_configuration.get(\'dfs.ha.namenodes.\' + namespace)\n        if namenodes:\n            # populate namenode_urls list for the given namespace\n            list_of_namenodes = []\n            for nn in namenodes.split(\',\'):\n                prop_key = \'dfs.namenode.rpc-address.{}.{}\'.format(namespace, nn)\n                namenode_url = self._hadoop_configuration.get(prop_key)\n                if namenode_url:\n                    list_of_namenodes.append(namenode_url)\n                else:\n                    raise RuntimeError(self._build_error_string(\'Failed to get property ""{}"" from\'\n                                                                .format(prop_key)))\n        # Don\'t raise and exception otherwise, because the supplied name could just be a hostname.\n        # We don\'t have an easy way to tell at this point.\n        return list_of_namenodes\n\n    def resolve_default_hdfs_service(self):\n        """"""\n        Resolves the default namenode using the given, or environment-derived, hadoop configuration,\n        by parsing the configuration for ``fs.defaultFS``.\n\n        :return: a tuple of structure ``(nameservice, list of namenodes)``\n        """"""\n        default_fs = self._hadoop_configuration.get(\'fs.defaultFS\')\n        if default_fs:\n            nameservice = urlparse(default_fs).netloc\n            list_of_namenodes = self.resolve_hdfs_name_service(nameservice)\n            if list_of_namenodes is None:\n                raise IOError(self._build_error_string(\'Unable to get namenodes for \'\n                                                       \'default service ""{}"" from\'\n                                                       .format(default_fs)))\n            return [nameservice, list_of_namenodes]\n        else:\n            raise RuntimeError(\n                self._build_error_string(\'Failed to get property ""fs.defaultFS"" from\'))\n\n\nclass HdfsConnectError(IOError):\n    pass\n\n\nclass MaxFailoversExceeded(RuntimeError):\n    def __init__(self, failed_exceptions, max_failover_attempts, func_name):\n        self.failed_exceptions = failed_exceptions\n        self.max_failover_attempts = max_failover_attempts\n        self.__name__ = func_name\n        message = \'Failover attempts exceeded maximum ({}) for action ""{}"". \' \\\n                  \'Exceptions:\\n{}\'.format(self.max_failover_attempts, self.__name__,\n                                           self.failed_exceptions)\n        super(MaxFailoversExceeded, self).__init__(message)\n\n\nclass namenode_failover(object):\n    """"""\n    This decorator class ensures seamless namenode failover and retry, when an HDFS call fails\n    due to StandbyException, up to a maximum retry.\n    """"""\n    # Allow for 2 failovers to a different namenode (i.e., if 2 NNs, try back to the original)\n    MAX_FAILOVER_ATTEMPTS = 2\n\n    def __init__(self, func):\n        # limit wrapper attributes updated to just name and doc string\n        functools.update_wrapper(self, func, (\'__name__\', \'__doc__\'))\n        # cache the function name, only because we don\'t need the function object in __call__\n        self._func_name = func.__name__\n\n    def __get__(self, obj, obj_type):\n        """""" Support usage of decorator on instance methods. """"""\n        # This avoids needing to cache the `obj` as member variable\n        return functools.partial(self.__call__, obj)\n\n    def __call__(self, obj, *args, **kwargs):\n        """"""\n        Attempts the function call, catching exception, re-connecting, and retrying, up to a\n        pre-configured maximum number of attempts.\n\n        :param obj: calling class instance, the HDFS client object\n        :param args: positional arguments to func\n        :param kwargs: arbitrary keyword arguments to func\n        :return: return of ``func`` call; if max retries exceeded, raise a RuntimeError; or raise\n                any unexpected exception\n        """"""\n        failures = []\n        while len(failures) <= self.MAX_FAILOVER_ATTEMPTS:\n            try:\n                # Invoke the filesystem function on the connected HDFS object\n                return getattr(obj._hdfs, self._func_name)(*args, **kwargs)\n            except ArrowIOError as e:\n                # An HDFS IP error occurred, retry HDFS connect to failover\n                obj._do_connect()\n                failures.append(e)\n        # Failover attempts exceeded at this point!\n        raise MaxFailoversExceeded(failures, self.MAX_FAILOVER_ATTEMPTS, self._func_name)\n\n\ndef failover_all_class_methods(decorator):\n    """"""\n    This decorator function wraps an entire class to decorate each member method, incl. inherited.\n\n    Adapted from https://stackoverflow.com/a/6307868\n    """"""\n\n    # Convenience function to ensure `decorate` gets wrapper function attributes: name, docs, etc.\n    @functools.wraps(decorator)\n    def decorate(cls):\n        all_methods = inspect.getmembers(cls, inspect.isbuiltin) \\\n            + inspect.getmembers(cls, inspect.ismethod) \\\n            + inspect.getmembers(cls, inspect.isroutine)\n        for name, method in all_methods:\n            if not name.startswith(\'_\'):\n                # It\'s safer to exclude all protected/private method from decoration\n                setattr(cls, name, decorator(method))\n        return cls\n\n    return decorate\n\n\n@failover_all_class_methods(namenode_failover)\nclass HAHdfsClient(HadoopFileSystem):\n    def __init__(self, connector_cls, list_of_namenodes, user=None):\n        """"""\n        Attempt HDFS connection operation, storing the hdfs object for intercepted calls.\n\n        :param connector_cls: HdfsConnector class, so connector logic resides in one place, and\n            also facilitates testing.\n        :param list_of_namenodes: List of name nodes to failover, cached to enable un-/pickling\n        :param user: String denoting username when connecting to HDFS. None implies login user.\n        :return: None\n        """"""\n        # Use protected attribute to prevent mistaken decorator application\n        self._connector_cls = connector_cls\n        self._list_of_namenodes = list_of_namenodes\n        self._user = user\n        # Ensure that a retry will attempt a different name node in the list\n        self._index_of_nn = -1\n        self._do_connect()\n\n    def __reduce__(self):\n        """""" Returns object state for pickling. """"""\n        return self.__class__, (self._connector_cls, self._list_of_namenodes, self._user)\n\n    def _do_connect(self):\n        """""" Makes a new connection attempt, caching the new namenode index and HDFS connection. """"""\n        self._index_of_nn, self._hdfs = \\\n            self._connector_cls._try_next_namenode(self._index_of_nn, self._list_of_namenodes, user=self._user)\n\n\nclass HdfsConnector(object):\n    """""" HDFS connector class where failover logic is implemented.  Facilitates testing. """"""\n    # Refactored constant\n    MAX_NAMENODES = 2\n\n    @classmethod\n    def hdfs_connect_namenode(cls, url, driver=\'libhdfs3\', user=None):\n        """"""\n        Performs HDFS connect in one place, facilitating easy change of driver and test mocking.\n\n        :param url: An parsed URL object to the HDFS end point\n        :param driver: An optional driver identifier\n        :param user: String denoting username when connecting to HDFS. None implies login user.\n        :return: Pyarrow HDFS connection object.\n        """"""\n\n        # According to pyarrow.hdfs.connect:\n        #    host : NameNode. Set to ""default"" for fs.defaultFS from core-site.xml\n        # So we pass \'default\' as a host name if the url does not specify one (i.e. hdfs:///...)\n        if LooseVersion(pyarrow.__version__) < LooseVersion(\'0.12.0\'):\n            hostname = url.hostname or \'default\'\n            driver = driver\n        else:\n            hostname = six.text_type(url.hostname or \'default\')\n            driver = six.text_type(driver)\n\n        kwargs = dict(user=user)\n        if LooseVersion(pyarrow.__version__) < LooseVersion(\'0.17.0\'):\n            # Support for libhdfs3 was removed in v0.17.0, we include it here for backwards\n            # compatibility\n            kwargs[\'driver\'] = driver\n        return pyarrow.hdfs.connect(hostname, url.port or 8020, **kwargs)\n\n    @classmethod\n    def connect_to_either_namenode(cls, list_of_namenodes, user=None):\n        """"""\n        Returns a wrapper HadoopFileSystem ""high-availability client"" object that enables\n        name node failover.\n\n        Raises a HdfsConnectError if no successful connection can be established.\n\n        :param list_of_namenodes: a required list of name node URLs to connect to.\n        :param user: String denoting username when connecting to HDFS. None implies login user.\n        :return: the wrapped HDFS connection object\n        """"""\n        assert list_of_namenodes is not None and len(list_of_namenodes) <= cls.MAX_NAMENODES, \\\n            ""Must supply a list of namenodes, but HDFS only supports up to {} namenode URLs"" \\\n            .format(cls.MAX_NAMENODES)\n        return HAHdfsClient(cls, list_of_namenodes, user=user)\n\n    @classmethod\n    def _try_next_namenode(cls, index_of_nn, list_of_namenodes, user=None):\n        """"""\n        Instead of returning an inline function, this protected class method implements the\n        failover logic: circling between namenodes using the supplied index as the last\n        index into the name nodes list.\n\n        :param list_of_namenodes: a required list of name node URLs to connect to.\n        :param user: String denoting username when connecting to HDFS. None implies login user.\n        :return: a tuple of (new index into list, actual pyarrow HDFS connection object), or raise\n                a HdfsConnectError if no successful connection can be established.\n        """"""\n        nn_len = len(list_of_namenodes)\n        if nn_len > 0:\n            for i in range(1, cls.MAX_NAMENODES + 1):\n                # Use a modulo mechanism to hit the ""next"" name node, as opposed to always\n                # starting from the first entry in the list\n                idx = (index_of_nn + i) % nn_len\n                host = list_of_namenodes[idx]\n                try:\n                    return idx, \\\n                        cls.hdfs_connect_namenode(urlparse(\'hdfs://\' + str(host or \'default\')), user=user)\n                except ArrowIOError as e:\n                    # This is an expected error if the namenode we are trying to connect to is\n                    # not the active one\n                    logger.debug(\'Attempted to connect to namenode %s but failed: %e\', host, str(e))\n        # It is a problem if we cannot connect to either of the namenodes when tried back-to-back,\n        # so better raise an error.\n        raise HdfsConnectError(""Unable to connect to HDFS cluster!"")\n'"
petastorm/pyarrow_helpers/__init__.py,0,b''
petastorm/pyarrow_helpers/batching_table_queue.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom collections import deque\n\nimport pyarrow as pa\n\n\nclass BatchingTableQueue(object):\n    def __init__(self, batch_size):\n        """"""The class is a FIFO queue. Arrow tables are added to the queue. When read, rows are regrouped into Arrow\n        tables of a fixed size specified during construction of the object. The order of the rows in the output tables\n        is the same as the order of the rows in the input tables.\n\n        :param batch_size: number of rows in tables that will be returned by the ``get`` method.\n        """"""\n        self._batch_size = batch_size\n        self._buffer = deque()\n        self._head_idx = 0\n        self._cumulative_len = 0\n\n    def put(self, table):\n        """"""Adds a table to the queue. All tables added during lifetime of an instance must have the same schema.\n\n        :param table: An instance of a pyarrow table.\n        :return: None\n        """"""\n\n        # We store a list of arrow batches. When retrieving, we consume parts or entire batches, until batch_size of\n        # rows are acquired.\n        record_batches = table.to_batches()\n        for record_batch in record_batches:\n            self._buffer.append(record_batch)\n            self._cumulative_len += record_batch.num_rows\n\n    def empty(self):\n        """"""Checks if more tables can be returned by get. If the number of rows in the internal buffer is less then\n        ``batch_size``, empty would return False.\n        """"""\n        return self._head_idx + self._batch_size > self._cumulative_len\n\n    def get(self):\n        """"""Return a table with ``batch_size`` number of rows.\n\n        :return: An instance of an Arrow table with exactly ``batch_size`` rows.\n        """"""\n\n        assert not self.empty()\n\n        # head_idx points to the next row in the buffer[0] batch to be consumed.\n        # Accumulate selices/full batches until result_rows reaches desired batch_size.\n\n        # Pop left of the deque once exhausted all rows there.\n        result = []\n        result_rows = 0\n        while result_rows < self._batch_size and self._cumulative_len > 0:\n            head = self._buffer[0]\n            piece = head[self._head_idx:self._head_idx + self._batch_size - result_rows]\n            self._head_idx += piece.num_rows\n            result_rows += piece.num_rows\n            result.append(piece)\n\n            if head.num_rows == self._head_idx:\n                self._head_idx = 0\n                self._buffer.popleft()\n                self._cumulative_len -= head.num_rows\n\n        return pa.Table.from_batches(result)\n'"
petastorm/reader_impl/__init__.py,0,b''
petastorm/reader_impl/arrow_table_serializer.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pyarrow as pa\n\n\nclass ArrowTableSerializer(object):\n    """"""This implementation of serializer is used to facilitate faster serialization of pyarrow tables.\n    Even a better solution would be to use shared memory (e.g. using plasma).""""""\n\n    def serialize(self, rows):\n        # Need to be able to serialize `None`. A bit hacky, but we use an empty buffer to encode \'None\'.\n        sink = pa.BufferOutputStream()\n        writer = pa.RecordBatchStreamWriter(sink, rows.schema)\n        writer.write_table(rows)\n        writer.close()\n        return sink.getvalue()\n\n    def deserialize(self, serialized_rows):\n        reader = pa.open_stream(serialized_rows)\n        table = reader.read_all()\n        return table\n'"
petastorm/reader_impl/pickle_serializer.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport pickle\n\n\nclass PickleSerializer(object):\n\n    def serialize(self, rows):\n        return pickle.dumps(rows)\n\n    def deserialize(self, serialized_rows):\n        return pickle.loads(serialized_rows)\n'"
petastorm/reader_impl/pyarrow_serializer.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom decimal import Decimal\n\nimport pyarrow\nfrom pyarrow import register_default_serialization_handlers\n\n\nclass PyArrowSerializer(object):\n\n    def serialize(self, rows):\n        return pyarrow.serialize(rows, self._get_serialization_context()).to_buffer()\n\n    def deserialize(self, serialized_rows):\n        return pyarrow.deserialize(serialized_rows, self._get_serialization_context())\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        # The context is not picklable, so we have to delete it from the state when saving\n        # we initialize and create it lazily in _get_serialization_context\n        if \'_context\' in state:\n            del state[\'_context\']\n        return state\n\n    def _get_serialization_context(self):\n        # Create _context lazily.\n        if not hasattr(self, \'_context\'):\n            self._context = pyarrow.SerializationContext()\n            register_default_serialization_handlers(self._context)\n            self._context.register_type(Decimal, \'decimal.Decimal\', pickle=True)\n\n        return self._context\n'"
petastorm/reader_impl/pytorch_shuffling_buffer.py,0,"b'#  Copyright (c) 2017-2020 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport abc\nfrom collections import deque\n\nimport six\nimport torch\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass BatchedShufflingBufferBase(object):\n    """"""Shuffling implements a shuffling algorithm. Items can be added to the shuffling buffer and removed in a\n    different order as defined by the concrete shuffling algorithm. A shuffling buffer is intended to be used from\n    a single thread, hence, not thread safe.\n    Functionality is similar to ShufflingBufferBase except operations are batched and based on PyTorch.""""""\n\n    def __init__(self, batch_size=1):\n        self._keys = None\n        self.batch_size = batch_size\n\n    def add_many(self, items):\n        items = [torch.as_tensor(v) for v in items]\n\n        return self._add_many(items)\n\n    @abc.abstractmethod\n    def _add_many(self, items):\n        """"""Adds multiple items to the buffer.\n\n        :param items: items to be added to the shuffling buffer.\n        :return: None\n        """"""\n\n    @abc.abstractmethod\n    def retrieve(self):\n        """"""Selects an batch of items from the buffer and returns the batch to the caller.\n        The items are removed from the buffer.\n\n        :return: The selected batch.\n        """"""\n\n    @abc.abstractmethod\n    def can_add(self):\n        """"""Checks the state of the buffer and returns whether a new item can be added to the buffer at the time.\n\n        :return: A boolean indicating whether an item can be added to the buffer at the time.\n        """"""\n\n    @abc.abstractmethod\n    def can_retrieve(self):\n        """"""Checks the state of the buffer and returns whether a batch can be removed from the buffer..\n\n        :return: A boolean indicating whether an batch can be returned from the buffer at the time.\n        """"""\n\n    @abc.abstractproperty\n    def size(self):\n        """"""Returns the number of elements currently present in the buffer.\n\n        :return: number of elements currently present in the buffer\n        """"""\n\n    @abc.abstractmethod\n    def finish(self):\n        """"""Call this method when no more :func:`add_many` calls will be made.\n\n        This allows a user to deplete the buffer. Typically during last epoch. Otherwise, we would always have leftovers\n        in the buffer at the end of the lifecycle.\n\n        :return: number of elements currently present in the buffer\n        """"""\n\n\nclass BatchedNoopShufflingBuffer(BatchedShufflingBufferBase):\n    """"""A \'no-operation\' (noop) implementation of a shuffling buffer. Useful in cases where no shuffling is desired, such\n    as test scenarios or iterating over a dataset in a predeterministic order.\n    """"""\n\n    def __init__(self, batch_size=1):\n        super(BatchedNoopShufflingBuffer, self).__init__(batch_size=batch_size)\n        self._batches = []\n        self._num_samples = 0\n        self.store = deque()\n        self._size = 0\n\n    def _make_batch(self):\n        # TODO: Add test for the zip\n        batch = [torch.cat(b, 0) for b in zip(*self._batches)]\n        if self._num_samples > self.batch_size:\n            leftover = [b[self.batch_size:] for b in batch]\n            batch = [b[:self.batch_size] for b in batch]\n            self._batches = [leftover]\n        else:\n            self._batches = []\n        self._num_samples -= min(self._num_samples, self.batch_size)\n        self.store.append(batch)\n\n    def _add_many(self, items):\n        self._num_samples += len(items[0])\n        self._size += len(items[0])\n        self._batches.append(items)\n        while self._num_samples >= self.batch_size:\n            self._make_batch()\n\n    def retrieve(self):\n        batch = self.store.popleft()\n        self._size -= len(batch[0])\n        return batch\n\n    def can_retrieve(self):\n        return len(self.store) > 0\n\n    def can_add(self):\n        return True\n\n    @property\n    def size(self):\n        return self._size\n\n    def finish(self):\n        if self._batches:\n            self._make_batch()\n\n\nclass BatchedRandomShufflingBuffer(BatchedShufflingBufferBase):\n    """"""\n    A random shuffling buffer implementation. Items can be added to the buffer and retrieved in a random order.\n    """"""\n\n    def __init__(self, shuffling_buffer_capacity, min_after_retrieve, extra_capacity=1000, batch_size=1):\n        """"""Initializes a new BatchedRandomShufflingBuffer instance.\n\n        Items may be retrieved from the buffer once ``min_after_retrieve`` items were added to the queue\n        (indicated by ``can_retrieve``).\n\n        Items may be added to the buffer as long as the number of items in the buffer (not including the items\n        passed to :func:`add_many`) does not exceed ``shuffling_queue_capacity``.\n\n        The amount of items in the buffer may actually become more than ``shuffling_buffer_capacity`` since\n        :func:`add_many` is passed a list of items. The *hard limit* on the number of items in the buffer is\n        ``shuffling_buffer_capacity + extra_capacity``.\n\n        Explanation:\n        This batch loader performs some non-conventional operations:\n\n        Let\'s say we enqueued several samples:\n\n        [1, 2, 3, 4, 5, 6, 7]\n\n        Now during a retrieve() we sample the order these samples will be retrieved:\n\n        [2, 4, 5, 1, 3, 0, 6]\n\n        Once an order has been sampled, we slice the order into batches of ``batch_size`` samples.\n        And index 1 batch at a time:\n\n        [1, 2, X, 4, X, 6, 7] -> [3, 5] (batch 1)\n        [1, X, X, 4, X, X, 7] -> [6 ,2] (batch 2)\n\n        We could compress the buffer after every retrieve(), but that would require custom ops.\n\n        When we call add_many we first rearrange the remaining elements:\n\n        [1, 4, 7]\n\n        Then append new elements:\n        [1, 4, 7, 8, 9, 10]\n\n        After add_many we have to resample a permutation for the buffer.\n\n        :param shuffling_buffer_capacity: Items may be added to the buffer as long as the amount of items in the\n          buffer does not exceed the value of ``shuffling_queue_capacity`` (not including the items\n          passed to :func:`add_many`).\n        :param min_after_retrieve: Minimal amount of items in the buffer that allows retrieval. This is needed to\n          guarantee good random shuffling of elements. Once :func:`finish` is called, items can be retrieved even if\n          the condition does not hold.\n        :param extra_capacity: The amount of items in the buffer may grow above ``shuffling_buffer_capacity``\n          (due to a call to :func:`add_many` with a list of items), but must remain under ``extra_capacity``. Should be\n          set to the upper bound of the number of items that can be added in a single call to :func:`add_many` (can be a\n          loose bound).\n        :param batch_size: The number of items to be retrieved for each self.retrieve() call.\n        This also affects the can_add and can can_retrieve accordingly.\n        """"""\n        super(BatchedRandomShufflingBuffer, self).__init__(batch_size=batch_size)\n        self._extra_capacity = extra_capacity\n        # Preallocate the shuffling buffer.\n        self._items = None\n        self._shuffling_queue_capacity = shuffling_buffer_capacity\n        self._min_after_dequeue = min_after_retrieve\n        self._size = 0\n        self._done_adding = False\n\n        self._random_indices = None\n        self.next_sample_head = 0\n\n    def _add_many(self, items):\n        if self._done_adding:\n            raise RuntimeError(\'Can not call add_many after done_adding() was called.\')\n\n        if not self.can_add():\n            raise RuntimeError(\'Can not enqueue. Check the return value of ""can_enqueue()"" to check if more \'\n                               \'items can be added.\')\n\n        expected_size = self._size + len(items[0])\n        maximal_capacity = self._shuffling_queue_capacity + self._extra_capacity\n        if expected_size > maximal_capacity:\n            raise RuntimeError(\'Attempt to enqueue more elements than the capacity allows. \'\n                               \'Current size: {}, new size {}, maximum allowed: {}\'.format(self._size, expected_size,\n                                                                                           maximal_capacity))\n\n        new_capacity = self._shuffling_queue_capacity\n        while new_capacity < expected_size:\n            # Will double capacity until it is large enough to fit new batch\n            new_capacity *= 2\n\n        if self._items is None:\n            # Create Buffer:\n            self._items = []\n            for v in items:\n                self._items.append(torch.empty((new_capacity,) + v.shape[1:], dtype=v.dtype, device=v.device))\n\n        if self.next_sample_head > 0:\n            # Before we can append a new batch, we compress the remaining samples\n            for k, v in enumerate(self._items):\n                # We need to clone the right-side to avoid racing conditions\n                self._items[k][:self.size] = self._items[k][self._random_indices[self.next_sample_head:]].clone()\n        self._random_indices = None\n        self.next_sample_head = 0\n\n        if new_capacity > self._items[0].shape[0]:\n            for k, v in enumerate(self._items):\n                self._items[k] = torch.empty((new_capacity,) + v.shape[1:], dtype=v.dtype, device=v.device)\n                self._items[k][:self._size] = v[:self._size]\n\n        # Copy new items over\n        for k, v in enumerate(items):\n            self._items[k][self._size:expected_size] = v\n        self._size = expected_size\n\n    def retrieve(self):\n        if not self._done_adding and not self.can_retrieve():\n            raise RuntimeError(\'Can not dequeue. Check the return value of ""can_dequeue()"" to check if any \'\n                               \'items are available.\')\n        batch_size = min(self.batch_size, self._size)\n\n        if self._random_indices is None:\n            # We randomize the order of all samples ahead of time and then slice it into chunks with ```batch_size```\n            self.next_sample_head = 0\n            self._random_indices = torch.randperm(int(self._size), device=self._items[0].device)\n        idx = self._random_indices[self.next_sample_head:self.next_sample_head + batch_size]\n        self.next_sample_head += batch_size\n        sample = [v[idx] for v in self._items]\n        self._size -= batch_size\n        return sample\n\n    def can_add(self):\n        return self._size < self._shuffling_queue_capacity and not self._done_adding\n\n    def can_retrieve(self):\n        return self._size >= self._min_after_dequeue + self.batch_size - 1 or (self._done_adding and self._size > 0)\n\n    @property\n    def size(self):\n        return self._size\n\n    def finish(self):\n        self._done_adding = True\n'"
petastorm/reader_impl/shuffling_buffer.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport abc\nfrom collections import deque\n\nimport numpy as np\nimport six\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass ShufflingBufferBase(object):\n    """"""Shuffling implements a shuffling algorithm. Items can be added to the shuffling buffer and removed in a\n    different order as defined by the concrete shuffling algorithm. A shuffling buffer is intended to be used from\n    a single thread, hence, not thread safe.""""""\n\n    @abc.abstractmethod\n    def add_many(self, items):\n        """"""Adds multiple items to the buffer.\n\n        :param items: items to be added to the shuffling buffer.\n        :return: None\n        """"""\n\n    @abc.abstractmethod\n    def retrieve(self):\n        """"""Selects an item from the buffer and returns the item to the caller. The item is removed from the buffer.\n\n        :return: The selected item.\n        """"""\n\n    @abc.abstractmethod\n    def can_add(self):\n        """"""Checks the state of the buffer and returns whether a new item can be added to the buffer at the time.\n\n        :return: A boolean indicating whether an item can be added to the buffer at the time.\n        """"""\n\n    @abc.abstractmethod\n    def can_retrieve(self):\n        """"""Checks the state of the buffer and returns whether an item can be removed from the buffer..\n\n        :return: A boolean indicating whether an item can be returned from the buffer at the time.\n        """"""\n\n    @abc.abstractproperty\n    def size(self):\n        """"""Returns the number of elements currently present in the buffer.\n\n        :return: number of elements currently present in the buffer\n        """"""\n\n    @abc.abstractmethod\n    def finish(self):\n        """"""Call this method when no more :func:`add_many` calls will be made.\n\n        This allows a user to deplete the buffer. Typically during last epoch. Otherwise, we would always have leftovers\n        in the buffer at the end of the lifecycle.\n\n        :return: number of elements currently present in the buffer\n        """"""\n\n\nclass NoopShufflingBuffer(ShufflingBufferBase):\n    """"""A \'no-operation\' (noop) implementation of a shuffling buffer. Useful in cases where no shuffling is desired, such\n    as test scenarios or iterating over a dataset in a predeterministic order.\n    """"""\n\n    def __init__(self):\n        self.store = deque()\n\n    def add_many(self, items):\n        self.store.extend(items)\n\n    def retrieve(self):\n        return self.store.popleft()\n\n    def can_retrieve(self):\n        return len(self.store) > 0\n\n    def can_add(self):\n        return True\n\n    @property\n    def size(self):\n        return len(self.store)\n\n    def finish(self):\n        pass\n\n\nclass RandomShufflingBuffer(ShufflingBufferBase):\n    """"""\n    A random shuffling buffer implementation. Items can be added to the buffer and retrieved in a random order.\n    """"""\n\n    def __init__(self, shuffling_buffer_capacity, min_after_retrieve, extra_capacity=1000):\n        """"""Initializes a new ShufflingBuffer instance.\n\n        Items may be retrieved from the buffer once ``min_after_retrieve`` items were added to the queue\n        (indicated by ``can_retrieve``).\n\n        Items may be added to the buffer as long as the number of items in the buffer (not including the items\n        passed to :func:`add_many`) does not exceed ``shuffling_queue_capacity``.\n\n        The amount of items in the buffer may actually become more than ``shuffling_buffer_capacity`` since\n        :func:`add_many` is passed a list of items. The *hard limit* on the number of items in the buffer is\n        ``shuffling_buffer_capacity + extra_capacity``.\n\n        :param shuffling_buffer_capacity: Items may be added to the buffer as long as the amount of items in the\n          buffer does not exceed the value of ``shuffling_queue_capacity`` (not including the items\n          passed to :func:`add_many`).\n        :param min_after_retrieve: Minimal amount of items in the buffer that allows retrieval. This is needed to\n          guarantee good random shuffling of elements. Once :func:`finish` is called, items can be retrieved even if\n          the condition does not hold.\n        :param extra_capacity: The amount of items in the buffer may grow above ``shuffling_buffer_capacity``\n          (due to a call to :func:`add_many` with a list of items), but must remain under ``extra_capacity``. Should be\n          set to the upper bound of the number of items that can be added in a single call to :func:`add_many` (can be a\n          loose bound).\n        """"""\n        self._extra_capacity = extra_capacity\n        # Preallocate the shuffling buffer.\n        self._items = [None] * (shuffling_buffer_capacity + self._extra_capacity)\n        self._shuffling_queue_capacity = shuffling_buffer_capacity\n        self._min_after_dequeue = min_after_retrieve\n        self._size = 0\n        self._done_adding = False\n\n    def add_many(self, items):\n        if self._done_adding:\n            raise RuntimeError(\'Can not call add_many after done_adding() was called.\')\n\n        if not self.can_add():\n            raise RuntimeError(\'Can not enqueue. Check the return value of ""can_enqueue()"" to check if more \'\n                               \'items can be added.\')\n\n        # We leave self._extra_capacity slack to make sure we don\'t reallocate self._items array\n        expected_size = self._size + len(items)\n        maximal_capacity = self._shuffling_queue_capacity + self._extra_capacity\n        if expected_size > maximal_capacity:\n            raise RuntimeError(\'Attempt to enqueue more elements than the capacity allows. \'\n                               \'Current size: {}, new size {}, maximum allowed: {}\'.format(self._size, expected_size,\n                                                                                           maximal_capacity))\n        self._items[self._size:self._size + len(items)] = items\n        self._size = expected_size\n\n    def retrieve(self):\n        if not self._done_adding and not self.can_retrieve():\n            raise RuntimeError(\'Can not dequeue. Check the return value of ""can_dequeue()"" to check if any \'\n                               \'items are available.\')\n        random_index = np.random.randint(0, self._size)\n        return_value = self._items[random_index]\n        self._items[random_index] = self._items[self._size - 1]\n        self._items[self._size - 1] = None\n        self._size -= 1\n        return return_value\n\n    def can_add(self):\n        return self._size < self._shuffling_queue_capacity and not self._done_adding\n\n    def can_retrieve(self):\n        return self._size >= self._min_after_dequeue or (self._done_adding and self._size > 0)\n\n    @property\n    def size(self):\n        return self._size\n\n    def finish(self):\n        self._done_adding = True\n'"
petastorm/spark/__init__.py,0,"b'#  Copyright (c) 2020 Databricks, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .spark_dataset_converter import make_spark_converter  # noqa: F401\nfrom .spark_dataset_converter import SparkDatasetConverter  # noqa: F401\n'"
petastorm/spark/spark_dataset_converter.py,4,"b'#  Copyright (c) 2020 Databricks, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport atexit\nimport datetime\nimport logging\nimport os\nimport shutil\nimport threading\nimport time\nimport uuid\nfrom distutils.version import LooseVersion\nfrom multiprocessing.pool import ThreadPool\n\nimport pyspark\nfrom pyarrow import LocalFileSystem\nfrom pyspark.sql.session import SparkSession\nfrom pyspark.sql.types import ArrayType, DoubleType, FloatType\nfrom six.moves.urllib.parse import urlparse\n\nfrom petastorm import make_batch_reader\nfrom petastorm.fs_utils import (FilesystemResolver,\n                                get_filesystem_and_path_or_paths, normalize_dir_url)\n\nif LooseVersion(pyspark.__version__) < LooseVersion(\'3.0\'):\n    def vector_to_array(_1, _2=\'float32\'):\n        raise RuntimeError(""Vector columns are only supported in pyspark>=3.0"")\nelse:\n    from pyspark.ml.functions import vector_to_array  # pylint: disable=import-error\n\nDEFAULT_ROW_GROUP_SIZE_BYTES = 32 * 1024 * 1024\n\nlogger = logging.getLogger(__name__)\n\n\ndef _get_spark_session():\n    """"""Get or create spark session. Note: This function can only be invoked from driver side.""""""\n    if pyspark.TaskContext.get() is not None:\n        # This is a safety check.\n        raise RuntimeError(\'_get_spark_session should not be invoked from executor side.\')\n    return SparkSession.builder.getOrCreate()\n\n\n_parent_cache_dir_url = None\n\n\ndef _get_parent_cache_dir_url():\n    """"""Get parent cache dir url from `petastorm.spark.converter.parentCacheDirUrl`\n    We can only set the url config once.\n    """"""\n    global _parent_cache_dir_url  # pylint: disable=global-statement\n\n    conf_url = _get_spark_session().conf \\\n        .get(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, None)\n\n    if conf_url is None:\n        raise ValueError(\n            ""Please set the spark config {}."".format(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF))\n\n    conf_url = normalize_dir_url(conf_url)\n    _check_parent_cache_dir_url(conf_url)\n    _parent_cache_dir_url = conf_url\n    logger.info(\n        \'Read %s %s\', SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, _parent_cache_dir_url)\n\n    return _parent_cache_dir_url\n\n\ndef _default_delete_dir_handler(dataset_url):\n    resolver = FilesystemResolver(dataset_url)\n    fs = resolver.filesystem()\n    parsed = urlparse(dataset_url)\n    if isinstance(fs, LocalFileSystem):\n        # pyarrow has a bug: LocalFileSystem.delete() is not implemented.\n        # https://issues.apache.org/jira/browse/ARROW-7953\n        # We can remove this branch once ARROW-7953 is fixed.\n        local_path = parsed.path\n        if os.path.exists(local_path):\n            shutil.rmtree(local_path, ignore_errors=False)\n    else:\n        if fs.exists(parsed.path):\n            fs.delete(parsed.path, recursive=True)\n\n\n_delete_dir_handler = _default_delete_dir_handler\n\n\ndef register_delete_dir_handler(handler):\n    """"""Register a handler for delete a directory url.\n\n    :param handler: A deleting function which take a argument of directory url.\n                    If ``None``, use the default handler, note the default handler\n                    will use libhdfs3 driver.\n\n    """"""\n    global _delete_dir_handler  # pylint: disable=global-statement\n    if handler is None:\n        _delete_dir_handler = _default_delete_dir_handler\n    else:\n        _delete_dir_handler = handler\n\n\ndef _delete_cache_data_atexit(dataset_url):\n    try:\n        _delete_dir_handler(dataset_url)\n    except Exception as e:  # pylint: disable=broad-except\n        logger.warning(\'Delete cache data %s failed due to %s\', dataset_url, repr(e))\n\n\ndef _get_horovod_rank_and_size():\n    """"""Get rank and size from environment, return (rank, size), if failed, return (``None``, ``None``)""""""\n    rank_env = [\'HOROVOD_RANK\', \'OMPI_COMM_WORLD_RANK\', \'PMI_RANK\']\n    size_env = [\'HOROVOD_SIZE\', \'OMPI_COMM_WORLD_SIZE\', \'PMI_SIZE\']\n\n    for rank_var, size_var in zip(rank_env, size_env):\n        rank = os.environ.get(rank_var)\n        size = os.environ.get(size_var)\n        if rank is not None and size is not None:\n            return int(rank), int(size)\n        elif rank is not None or size is not None:\n            return None, None\n\n    return None, None\n\n\ndef _check_rank_and_size_consistent_with_horovod(petastorm_reader_kwargs):\n    """"""Check whether the ``cur_shard`` and ``shard_count`` args are consistent with horovod environment variables.\n\n    If not consistent with horovod environment variables, log warning message and return ``False``.\n    If there\'re no related horovod environment variable set, return ``True``.\n    """"""\n    hvd_rank, hvd_size = _get_horovod_rank_and_size()\n    cur_shard = petastorm_reader_kwargs.get(\'cur_shard\')\n    shard_count = petastorm_reader_kwargs.get(\'shard_count\')\n\n    if hvd_rank is not None and hvd_size is not None:\n        if cur_shard != hvd_rank or shard_count != hvd_size:\n            logger.warning(\n                \'The petastorm reader arguments cur_shard(%d) and \'\n                \'shard_count(%d) is not consistent with horovod \'\n                \'environments hvd_rank(%d) and hvd_size(%d), If you want \'\n                \'each horovod worker train on one corresponding shard data, \'\n                \'you should set argument `cur_shard` to be `hvd.rank()` \'\n                \'and argument `shard_count` to be `hvd.size()`.\',\n                cur_shard, shard_count, hvd_rank, hvd_size)\n            return False\n    return True\n\n\nclass SparkDatasetConverter(object):\n    """"""A `SparkDatasetConverter` object holds one materialized spark dataframe and\n    can be used to make one or more tensorflow datasets or torch dataloaders.\n    The `SparkDatasetConverter` object is picklable and can be used in remote\n    processes.\n    See `make_spark_converter`\n    """"""\n\n    PARENT_CACHE_DIR_URL_CONF = \'petastorm.spark.converter.parentCacheDirUrl\'\n\n    def __init__(self, cache_dir_url, file_urls, dataset_size):\n        """"""\n        :param cache_dir_url: A string denoting the path to store the cache files.\n        :param file_urls: a list of parquet file url list of this dataset.\n        :param dataset_size: An int denoting the number of rows in the dataframe.\n        """"""\n        self.cache_dir_url = cache_dir_url\n        self.file_urls = file_urls\n        self.dataset_size = dataset_size\n\n    def __len__(self):\n        """"""\n        :return: dataset size\n        """"""\n        return self.dataset_size\n\n    @staticmethod\n    def _check_and_set_overriden_petastorm_args(petastorm_reader_kwargs, num_epochs, workers_count):\n        # override some arguments default values of petastorm reader\n        petastorm_reader_kwargs[\'num_epochs\'] = num_epochs\n        if workers_count is None:\n            # TODO: generate a best tuned value for default worker count value\n            workers_count = 4\n        petastorm_reader_kwargs[\'workers_count\'] = workers_count\n        _check_rank_and_size_consistent_with_horovod(petastorm_reader_kwargs)\n\n    def make_tf_dataset(\n            self,\n            batch_size=None,\n            prefetch=None,\n            num_epochs=None,\n            workers_count=None,\n            **petastorm_reader_kwargs\n    ):\n        """"""Make a tensorflow dataset.\n\n        This method will do the following two steps:\n          1) Open a petastorm reader on the materialized dataset dir.\n          2) Create a tensorflow dataset based on the reader created in (1)\n\n        :param batch_size: The number of items to return per batch. Default ``None``.\n            If ``None``, current implementation will set batch size to be 32, in future,\n            ``None`` value will denotes auto tuned best value for batch size.\n        :param prefetch: Prefetch size for tensorflow dataset. If ``None`` will use\n            tensorflow autotune size. Note only available on tensorflow>=1.14\n        :param num_epochs: An epoch is a single pass over all rows in the dataset.\n            Setting ``num_epochs`` to ``None`` will result in an infinite number\n            of epochs.\n        :param workers_count: An int for the number of workers to use in the\n            reader pool. This only is used for the thread or process pool.\n            ``None`` denotes auto tune best value (current implementation when auto tune,\n            it will always use 4 workers, but it may be improved in future)\n            Default value ``None``.\n        :param petastorm_reader_kwargs: arguments for `petastorm.make_batch_reader()`,\n            exclude these arguments: ``dataset_url``, ``num_epochs``, ``workers_count``.\n\n        :return: a context manager for a `tf.data.Dataset` object.\n                 when exit the returned context manager, the reader\n                 will be closed.\n        """"""\n        self._check_and_set_overriden_petastorm_args(\n            petastorm_reader_kwargs, num_epochs=num_epochs, workers_count=workers_count)\n        return TFDatasetContextManager(\n            self.file_urls,\n            batch_size=batch_size,\n            prefetch=prefetch,\n            petastorm_reader_kwargs=petastorm_reader_kwargs)\n\n    def make_torch_dataloader(self,\n                              batch_size=32,\n                              num_epochs=None,\n                              workers_count=None,\n                              **petastorm_reader_kwargs):\n        """"""Make a PyTorch DataLoader.\n\n        This method will do the following two steps:\n          1) Open a petastorm reader on the materialized dataset dir.\n          2) Create a PyTorch DataLoader based on the reader created in (1)\n\n        :param batch_size: The number of items to return per batch. Default ``None``.\n            If ``None``, current implementation will set batch size to be 32, in future,\n            ``None`` value will denotes auto tuned best value for batch size.\n        :param num_epochs: An epoch is a single pass over all rows in the\n            dataset. Setting ``num_epochs`` to ``None`` will result in an\n            infinite number of epochs.\n        :param workers_count: An int for the number of workers to use in the\n            reader pool. This only is used for the thread or process pool.\n            Defaults value ``None``, which means using the default value from\n            `petastorm.make_batch_reader()`. We can autotune it in the future.\n        :param petastorm_reader_kwargs: arguments for `petastorm.make_batch_reader()`,\n            exclude these arguments: ``dataset_url``, ``num_epochs``, ``workers_count``.\n\n        :return: a context manager for a `torch.utils.data.DataLoader` object.\n                 when exit the returned context manager, the reader\n                 will be closed.\n        """"""\n        self._check_and_set_overriden_petastorm_args(\n            petastorm_reader_kwargs, num_epochs=num_epochs, workers_count=workers_count)\n        return TorchDatasetContextManager(\n            self.file_urls,\n            batch_size=batch_size,\n            petastorm_reader_kwargs=petastorm_reader_kwargs)\n\n    def delete(self):\n        """"""Delete cache files at self.cache_dir_url.""""""\n        _remove_cache_metadata_and_data(self.cache_dir_url)\n\n\nclass TFDatasetContextManager(object):\n    """"""A context manager that manages the creation and termination of a\n    :class:`petastorm.Reader`.\n    """"""\n\n    def __init__(\n            self,\n            parquet_file_url_list,\n            batch_size,\n            prefetch,\n            petastorm_reader_kwargs\n    ):\n        """"""\n        :param parquet_file_url_list: A string specifying the parquet file URL list.\n        :param batch_size: batch size for tensorflow dataset.\n        :param prefetch: the prefectch size for tensorflow dataset.\n        :param petastorm_reader_kwargs: other arguments for petastorm reader\n        """"""\n        self.parquet_file_url_list = parquet_file_url_list\n        self.batch_size = batch_size\n        self.prefetch = prefetch\n        self.petastorm_reader_kwargs = petastorm_reader_kwargs\n\n    def __enter__(self):\n        # import locally to avoid importing tensorflow globally.\n        from petastorm.tf_utils import make_petastorm_dataset\n        import tensorflow.compat.v1 as tf  # pylint: disable=import-error\n\n        _wait_file_available(self.parquet_file_url_list)\n        self.reader = make_batch_reader(self.parquet_file_url_list, **self.petastorm_reader_kwargs)\n\n        # unroll dataset\n        dataset = make_petastorm_dataset(self.reader).flat_map(\n            tf.data.Dataset.from_tensor_slices)\n\n        # TODO: auto tune best batch size in default case.\n        batch_size = self.batch_size or 32\n        dataset = dataset.batch(batch_size=batch_size)\n\n        prefetch = self.prefetch\n\n        if prefetch is None:\n            if LooseVersion(tf.__version__) >= LooseVersion(\'1.14\'):\n                # We can make prefetch optimization\n                prefetch = tf.data.experimental.AUTOTUNE\n            else:\n                prefetch = 1\n\n        dataset = dataset.prefetch(prefetch)\n\n        return dataset\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        self.reader.stop()\n        self.reader.join()\n\n\nclass TorchDatasetContextManager(object):\n    """"""A context manager that manages the creation and termination of a\n    :class:`petastorm.Reader`.\n    """"""\n\n    def __init__(self, parquet_file_url_list, batch_size, petastorm_reader_kwargs):\n        """"""\n        :param parquet_file_url_list: A string specifying the parquet file URL list.\n        :param batch_size: The number of items to return per batch. Default ``None``.\n            If ``None``, current implementation will set batch size to be 32, in future,\n            ``None`` value will denotes auto tuned best value for batch size.\n        :param petastorm_reader_kwargs: other arguments for petastorm reader\n\n        See `SparkDatasetConverter.make_torch_dataloader()`  for the definitions\n        of the other parameters.\n        """"""\n        self.parquet_file_url_list = parquet_file_url_list\n        self.batch_size = batch_size\n        self.petastorm_reader_kwargs = petastorm_reader_kwargs\n\n    def __enter__(self):\n        from petastorm.pytorch import DataLoader\n\n        _wait_file_available(self.parquet_file_url_list)\n        self.reader = make_batch_reader(self.parquet_file_url_list,\n                                        **self.petastorm_reader_kwargs)\n        self.loader = DataLoader(reader=self.reader, batch_size=self.batch_size)\n        return self.loader\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        self.reader.stop()\n        self.reader.join()\n\n\ndef _get_df_plan(df):\n    return df._jdf.queryExecution().analyzed()\n\n\nclass CachedDataFrameMeta(object):\n\n    def __init__(self, df, parent_cache_dir_url, row_group_size, compression_codec, dtype):\n        self.row_group_size = row_group_size\n        self.compression_codec = compression_codec\n        # Note: the metadata will hold dataframe plan, but it won\'t\n        # hold the dataframe object (dataframe plan will not reference\n        # dataframe object),\n        # This means the dataframe can be released by spark gc.\n        self.df_plan = _get_df_plan(df)\n        self.cache_dir_url = None\n        self.dtype = dtype\n        self.parent_cache_dir_url = parent_cache_dir_url\n\n    @classmethod\n    def create_cached_dataframe_meta(cls, df, parent_cache_dir_url, row_group_size,\n                                     compression_codec, dtype):\n        meta = cls(df, parent_cache_dir_url, row_group_size, compression_codec, dtype)\n        meta.cache_dir_url = _materialize_df(\n            df,\n            parent_cache_dir_url=parent_cache_dir_url,\n            parquet_row_group_size_bytes=row_group_size,\n            compression_codec=compression_codec,\n            dtype=dtype)\n        return meta\n\n\n_cache_df_meta_list = []\n_cache_df_meta_list_lock = threading.Lock()\n\n\ndef _is_spark_local_mode():\n    return _get_spark_session().conf.get(\'spark.master\').strip().lower().startswith(\'local\')\n\n\ndef _check_url(dir_url):\n    """"""Check dir url, will check scheme, raise error if empty scheme""""""\n    parsed = urlparse(dir_url)\n    if not parsed.scheme:\n        raise ValueError(\n            \'ERROR! A scheme-less directory url ({}) is no longer supported. \'\n            \'Please prepend ""file://"" for local filesystem.\'.format(dir_url))\n\n\ndef _check_parent_cache_dir_url(dir_url):\n    """"""Check dir url whether is suitable to be used as parent cache directory.""""""\n    _check_url(dir_url)\n    fs, dir_path = get_filesystem_and_path_or_paths(dir_url)\n    if \'DATABRICKS_RUNTIME_VERSION\' in os.environ and not _is_spark_local_mode():\n        if isinstance(fs, LocalFileSystem):\n            # User need to use dbfs fuse URL.\n            if not dir_path.startswith(\'/dbfs/\'):\n                logger.warning(\n                    ""Usually, when running on databricks spark cluster, you should specify a dbfs fuse path ""\n                    ""for %s, like: \'file:/dbfs/path/to/cache_dir\', otherwise, you should mount NFS to this ""\n                    ""directory \'%s\' on all nodes of the cluster, e.g. using EFS."",\n                    SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, dir_url)\n\n\ndef _make_sub_dir_url(dir_url, name):\n    parsed = urlparse(dir_url)\n    new_path = parsed.path + \'/\' + name\n    return parsed._replace(path=new_path).geturl()\n\n\ndef _cache_df_or_retrieve_cache_data_url(df, parent_cache_dir_url,\n                                         parquet_row_group_size_bytes,\n                                         compression_codec,\n                                         dtype):\n    """"""Check whether the df is cached.\n\n    If so, return the existing cache file path.\n    If not, cache the df into the cache_dir in parquet format and return the\n    cache file path.\n    Use atexit to delete the cache before the python interpreter exits.\n    :param df: A :class:`DataFrame` object.\n    :param parquet_row_group_size_bytes: An int denoting the number of bytes\n        in a parquet row group.\n    :param compression_codec: Specify compression codec.\n    :param dtype: ``None``, \'float32\' or \'float64\', specifying the precision of the floating-point\n        elements in the output dataset. Integer types will remain unchanged. If ``None``, all types\n        will remain unchanged. Default \'float32\'.\n    :return: A string denoting the path of the saved parquet file.\n    """"""\n    # TODO\n    #  Improve the cache list by hash table (Note we need use hash(df_plan +\n    #  row_group_size)\n    with _cache_df_meta_list_lock:\n        df_plan = _get_df_plan(df)\n        for meta in _cache_df_meta_list:\n            if meta.row_group_size == parquet_row_group_size_bytes and \\\n                    meta.compression_codec == compression_codec and \\\n                    meta.df_plan.sameResult(df_plan) and \\\n                    meta.dtype == dtype and \\\n                    meta.parent_cache_dir_url == parent_cache_dir_url:\n                return meta.cache_dir_url\n        # do not find cached dataframe, start materializing.\n        cached_df_meta = CachedDataFrameMeta.create_cached_dataframe_meta(\n            df, parent_cache_dir_url, parquet_row_group_size_bytes,\n            compression_codec, dtype)\n        _cache_df_meta_list.append(cached_df_meta)\n        return cached_df_meta.cache_dir_url\n\n\ndef _remove_cache_metadata_and_data(cache_dir_url):\n    with _cache_df_meta_list_lock:\n        for i in range(len(_cache_df_meta_list)):\n            if _cache_df_meta_list[i].cache_dir_url == cache_dir_url:\n                _cache_df_meta_list.pop(i)\n                break\n    _delete_dir_handler(cache_dir_url)\n\n\ndef _convert_precision(df, dtype):\n    if dtype is None:\n        return df\n\n    if dtype != ""float32"" and dtype != ""float64"":\n        raise ValueError(""dtype {} is not supported. \\\n            Use \'float32\' or float64"".format(dtype))\n\n    source_type, target_type = (DoubleType, FloatType) \\\n        if dtype == ""float32"" else (FloatType, DoubleType)\n\n    logger.warning(""Converting floating-point columns to %s"", dtype)\n\n    for field in df.schema:\n        col_name = field.name\n        if isinstance(field.dataType, source_type):\n            df = df.withColumn(col_name, df[col_name].cast(target_type()))\n        elif isinstance(field.dataType, ArrayType) and \\\n                isinstance(field.dataType.elementType, source_type):\n            df = df.withColumn(col_name, df[col_name].cast(ArrayType(target_type())))\n    return df\n\n\ndef _convert_vector(df, dtype):\n    from pyspark.ml.linalg import VectorUDT\n    from pyspark.mllib.linalg import VectorUDT as OldVectorUDT\n\n    for field in df.schema:\n        col_name = field.name\n        if isinstance(field.dataType, VectorUDT) or \\\n                isinstance(field.dataType, OldVectorUDT):\n            df = df.withColumn(col_name,\n                               vector_to_array(df[col_name], dtype))\n    return df\n\n\ndef _gen_cache_dir_name():\n    """"""Generate a random directory name for storing dataset.\n    The directory name format is:\n      {datetime}-{spark_application_id}-{uuid4}\n    This will help user to find the related spark application for a directory.\n    So that if atexit deletion failed, user can manually delete them.\n    """"""\n    uuid_str = str(uuid.uuid4())\n    time_str = datetime.datetime.now().strftime(\'%Y%m%d%H%M%S\')\n    appid = _get_spark_session().sparkContext.applicationId\n    return \'{time}-appid-{appid}-{uuid}\'.format(time=time_str, appid=appid, uuid=uuid_str)\n\n\ndef _materialize_df(df, parent_cache_dir_url, parquet_row_group_size_bytes,\n                    compression_codec, dtype):\n    dir_name = _gen_cache_dir_name()\n    save_to_dir_url = _make_sub_dir_url(parent_cache_dir_url, dir_name)\n    df = _convert_vector(df, dtype)\n    df = _convert_precision(df, dtype)\n\n    df.write \\\n        .option(""compression"", compression_codec) \\\n        .option(""parquet.block.size"", parquet_row_group_size_bytes) \\\n        .parquet(save_to_dir_url)\n\n    logger.info(\'Materialize dataframe to url %s successfully.\', save_to_dir_url)\n\n    atexit.register(_delete_cache_data_atexit, save_to_dir_url)\n\n    return save_to_dir_url\n\n\n_FILE_AVAILABILITY_WAIT_TIMEOUT_SECS = 30\n\n\ndef _wait_file_available(url_list):\n    """"""Waiting about _FILE_AVAILABILITY_WAIT_TIMEOUT_SECS seconds (default 30 seconds) to make sure\n    all files are available for reading. This is useful in some filesystems, such as S3 which only\n    providing eventually consistency.\n    """"""\n    fs, path_list = get_filesystem_and_path_or_paths(url_list)\n    logger.debug(\'Waiting some seconds until all parquet-store files appear at urls %s\', \',\'.join(url_list))\n\n    def wait_for_file(path):\n        end_time = time.time() + _FILE_AVAILABILITY_WAIT_TIMEOUT_SECS\n        while time.time() < end_time:\n            if fs.exists(path):\n                return True\n            time.sleep(0.1)\n        return False\n\n    pool = ThreadPool(64)\n    try:\n        results = pool.map(wait_for_file, path_list)\n        failed_list = [url for url, result in zip(url_list, results) if not result]\n        if failed_list:\n            raise RuntimeError(\'Timeout while waiting for all parquet-store files to appear at urls {failed_list},\'\n                               \'Please check whether these files were saved successfully when materializing dataframe.\'\n                               .format(failed_list=\',\'.join(failed_list)))\n    finally:\n        pool.close()\n        pool.join()\n\n\ndef _check_dataset_file_median_size(url_list):\n    fs, path_list = get_filesystem_and_path_or_paths(url_list)\n    RECOMMENDED_FILE_SIZE_BYTES = 50 * 1024 * 1024\n\n    # TODO: also check file size for other file system.\n    if isinstance(fs, LocalFileSystem):\n        pool = ThreadPool(64)\n        try:\n            file_size_list = pool.map(os.path.getsize, path_list)\n            if len(file_size_list) > 1:\n                mid_index = len(file_size_list) // 2\n                median_size = sorted(file_size_list)[mid_index]  # take the larger one if tie\n                if median_size < RECOMMENDED_FILE_SIZE_BYTES:\n                    logger.warning(\'The median size %d B (< 50 MB) of the parquet files is too small. \'\n                                   \'Total size: %d B. Increase the median file size by calling df.repartition(n) or \'\n                                   \'df.coalesce(n), which might help improve the performance. Parquet files: %s, ...\',\n                                   median_size, sum(file_size_list), url_list[0])\n        finally:\n            pool.close()\n            pool.join()\n\n\ndef make_spark_converter(\n        df,\n        parquet_row_group_size_bytes=DEFAULT_ROW_GROUP_SIZE_BYTES,\n        compression_codec=None,\n        dtype=\'float32\'\n):\n    """"""Convert a spark dataframe into a :class:`SparkDatasetConverter` object.\n    It will materialize a spark dataframe to the directory specified by\n    spark conf \'petastorm.spark.converter.parentCacheDirUrl\'.\n    The dataframe will be materialized in parquet format, and we can specify\n    `parquet_row_group_size_bytes` and `compression_codec` for the parquet\n    format. See params documentation for details.\n\n    The returned `SparkDatasetConverter` object will hold the materialized\n    dataframe, and can be used to make one or more tensorflow datasets or\n    torch dataloaders.\n\n    We can explicitly delete the materialized dataframe data, see\n    `SparkDatasetConverter.delete`, and when the spark application exit,\n    it will try best effort to delete the materialized dataframe data.\n\n    :param df: The :class:`DataFrame` object to be converted.\n    :param parquet_row_group_size_bytes: An int denoting the number of bytes\n        in a parquet row group when materializing the dataframe.\n    :param compression_codec: Specify compression codec.\n        It can be one of \'uncompressed\', \'bzip2\', \'gzip\', \'lz4\', \'snappy\', \'deflate\'.\n        Default ``None``. If ``None``, it will leave the data uncompressed.\n    :param dtype: ``None``, \'float32\' or \'float64\', specifying the precision of the floating-point\n        elements in the output dataset. Integer types will remain unchanged. If ``None``, all types\n        will remain unchanged. Default \'float32\'.\n\n    :return: a :class:`SparkDatasetConverter` object that holds the\n        materialized dataframe and can be used to make one or more tensorflow\n        datasets or torch dataloaders.\n    """"""\n\n    parent_cache_dir_url = _get_parent_cache_dir_url()\n\n    # TODO: Improve default behavior to be automatically choosing the best way.\n    compression_codec = compression_codec or ""uncompressed""\n\n    if compression_codec.lower() not in \\\n            [\'uncompressed\', \'bzip2\', \'gzip\', \'lz4\', \'snappy\', \'deflate\']:\n        raise RuntimeError(\n            ""compression_codec should be None or one of the following values: ""\n            ""\'uncompressed\', \'bzip2\', \'gzip\', \'lz4\', \'snappy\', \'deflate\'"")\n\n    dataset_cache_dir_url = _cache_df_or_retrieve_cache_data_url(\n        df, parent_cache_dir_url, parquet_row_group_size_bytes, compression_codec, dtype)\n\n    # TODO: improve this by read parquet file metadata to get count\n    #  Currently spark can make sure to only read the minimal column\n    #  so count will usually be fast.\n    spark = _get_spark_session()\n    spark_df = spark.read.parquet(dataset_cache_dir_url)\n\n    dataset_size = spark_df.count()\n    parquet_file_url_list = list(spark_df._jdf.inputFiles())\n    _check_dataset_file_median_size(parquet_file_url_list)\n\n    return SparkDatasetConverter(dataset_cache_dir_url, parquet_file_url_list, dataset_size)\n'"
petastorm/test_util/__init__.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
petastorm/test_util/reader_mock.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom decimal import Decimal\nimport numpy as np\n\n\nclass ReaderMock(object):\n    """"""Reads a unischema based mock dataset.""""""\n\n    def __init__(self, schema, schema_data_generator, ngram=None):\n        """"""Initializes a reader object.\n\n        :param schema: unischema instance\n        :param schema_data_generator: A function that takes names of fields in unischema and returns the actual\n                values that complies with the schema.\n        """"""\n        self.schema = schema\n        self.schema_data_generator = schema_data_generator\n        if ngram is not None:\n            raise ValueError(\'Sequence argument not supported for ReaderMock\')\n        self.ngram = ngram\n        self.batched_output = False\n\n    def fetch(self):\n        """"""\n        Generates the mock dataset based on the schema.\n\n        :return: named tuple data according to schema.\n        """"""\n        fields_as_dict = self.schema_data_generator(self.schema)\n        return self.schema.make_namedtuple(**fields_as_dict)\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        return self.__next__()\n\n    def __next__(self):\n        return self.fetch()\n\n    # Functions needed to treat reader as a context manager\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n    def stop(self):\n        pass\n\n    def join(self):\n        pass\n\n\ndef schema_data_generator_example(schema):\n    """"""\n    Generates dummy data for a given schema.\n\n    :param schema: unischema instance\n    :return: A dictionary of schema dummy values.\n    """"""\n    fields_as_dict = {}\n    for field in schema.fields.values():\n        if field.numpy_dtype is Decimal:\n            fields_as_dict[field.name] = Decimal(\'0.0\')\n        else:\n            field_shape = tuple([10 if dim is None else dim for dim in field.shape])\n            fields_as_dict[field.name] = np.zeros(field_shape, dtype=field.numpy_dtype)\n    return fields_as_dict\n'"
petastorm/test_util/shuffling_analysis.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom __future__ import division\n\nfrom functools import partial\n\nimport numpy as np\nfrom pyspark.sql.types import LongType\n\nfrom petastorm import make_reader\nfrom petastorm.codecs import ScalarCodec\nfrom petastorm.etl.dataset_metadata import materialize_dataset\nfrom petastorm.unischema import Unischema, UnischemaField, dict_to_spark_row\n\n_ShuffleAnalysisSchema = Unischema(\'_ShuffleAnalysisSchema\',\n                                   [UnischemaField(\'id\', np.int64, (), ScalarCodec(LongType()), False)])\n\n\ndef generate_shuffle_analysis_dataset(spark, output_dataset_url, num_rows=1000, row_group_size=100):\n    """"""\n    Generates a small dataset useful for doing analysis on shuffling algorithms\n\n    :param spark: spark session\n    :param output_dataset_url: location to write dataset\n    :param num_rows: how many rows should the dataset include\n    :param row_group_size: how many rows in each row group (there is a minimum of 5)\n    :return:\n    """"""\n    spark_context = spark.sparkContext\n    with materialize_dataset(spark, output_dataset_url, _ShuffleAnalysisSchema):\n        rows_rdd = spark_context.parallelize(range(num_rows), numSlices=50) \\\n            .map(lambda i: {\'id\': i}) \\\n            .map(partial(dict_to_spark_row, _ShuffleAnalysisSchema))\n        spark.createDataFrame(rows_rdd, _ShuffleAnalysisSchema.as_spark_schema()) \\\n            .sort(\'id\') \\\n            .coalesce(max(1, int(num_rows / row_group_size))) \\\n            .write.option(\'compression\', \'none\') \\\n            .parquet(output_dataset_url)\n\n\ndef compute_correlation_distribution(dataset_url,\n                                     id_column,\n                                     shuffle_row_drop_partitions,\n                                     num_corr_samples=100):\n    """"""\n    Compute the correlation distribution of a given shuffle_options on an existing dataset.\n    Use this to compare 2 different shuffling options compare.\n    It is encouraged to use a dataset generated by generate_shuffle_analysis_dataset for this analysis.\n\n    :param dataset_url: Dataset url to compute correlation distribution of\n    :param id_column: Column where an integer or string id can be found\n    :param shuffle_row_drop_partitions: shuffle_row_drop_partitions to test correlation against\n    :param num_corr_samples: How many samples of the correlation to take to compute distribution\n    :return: (mean, standard deviation) of computed distribution\n    """"""\n\n    # Read the dataset without any shuffling in order (need to use a dummy pool for this).\n    with make_reader(dataset_url,\n                     shuffle_row_groups=False,\n                     reader_pool_type=\'dummy\') as reader:\n        unshuffled = [row[id_column] for row in reader]\n\n    correlations = []\n    for _ in range(num_corr_samples):\n        with make_reader(dataset_url,\n                         shuffle_row_groups=True,\n                         shuffle_row_drop_partitions=shuffle_row_drop_partitions) as reader:\n            shuffled = [row[id_column] for row in reader]\n            correlations.append(abs(np.corrcoef(unshuffled, shuffled)[0, 1]))\n\n    mean = np.mean(correlations)\n    std_dev = np.std(correlations)\n\n    return mean, std_dev\n'"
petastorm/tests/__init__.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
petastorm/tests/bootstrap_test_schema_data.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom __future__ import print_function\n\nimport getopt\nimport sys\n\nfrom petastorm.tests.test_common import create_test_dataset\n\n\n# Number of rows in a fake dataset\nROWS_COUNT = 10\n\n\ndef usage_exit(msg=None):\n    if msg:\n        print(msg)\n    print(""""""\\\nUsage: {} [options]\n\nOptions:\n  -h, --help           Show this message\n  --output-dir <dir>   Path of directory where to write test data\n"""""".format(sys.argv[0]))\n    sys.exit(1)\n\n\ndef make_test_metadata(path):\n    """"""\n    Use test_common to make a dataset for the TestSchema.\n\n    :param path: path to store the test dataset\n    :return: resulting dataset as a dictionary\n    """"""\n    assert path, \'Please supply a nonempty path to store test dataset.\'\n    return create_test_dataset(\'file://{}\'.format(path), range(ROWS_COUNT))\n\n\nif __name__ == \'__main__\':\n    try:\n        options, args = getopt.getopt(sys.argv[1:], \'ho:\', [\'--help\', \'output-dir=\'])\n        path = None\n        for opt, value in options:\n            if opt in (\'-h\', \'--help\'):\n                usage_exit()\n            if opt in (\'-o\', \'--output-dir\'):\n                if value:\n                    path = value\n        if path is None or not path == 0:\n            usage_exit(\'Please supply an output directory.\')\n        else:\n            make_test_metadata(path)\n    except getopt.GetoptError as msg:\n        usage_exit(msg)\n'"
petastorm/tests/conftest.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\nimport os\nimport pickle\nimport tempfile\nfrom base64 import b64encode, b64decode\nfrom collections import namedtuple\n\nimport pytest\nimport six\n\nfrom petastorm.spark import spark_dataset_converter, SparkDatasetConverter\nfrom petastorm.tests.test_common import create_test_dataset, create_test_scalar_dataset, \\\n    create_many_columns_non_petastorm_dataset\nfrom pyspark.sql import SparkSession\n\nSyntheticDataset = namedtuple(\'synthetic_dataset\', [\'url\', \'data\', \'path\'])\n\n# Number of rows in a fake dataset\n_ROWS_COUNT = 100\n\n_CACHE_FAKE_DATASET_OPTION_SHORT = \'-Y\'\n_CACHE_FAKE_DATASET_OPTION = \'--cache-synthetic-dataset\'\n\nlogger = logging.getLogger(__name__)\n\n\ndef pytest_logger_config(logger_config):\n    logger_config.add_loggers(\n        [\n            \'petastorm.workers_pool.process_pool\',\n            \'petastorm.workers_pool.thread_pool\',\n            \'petastorm.workers_pool.dummy_pool\',\n            \'petastorm.workers_pool.ventilator\',\n            \'petastorm.reader\',\n        ], stdout_level=\'debug\')\n\n\ndef pytest_addoption(parser):\n    parser.addoption(\n        _CACHE_FAKE_DATASET_OPTION_SHORT, _CACHE_FAKE_DATASET_OPTION, action=""store_true"", default=False,\n        help=\'Use a cached version of synthetic dataset if available. This helps speedup local tests reruns as \'\n             \'we don\\\'t have to rerun spark. CAUTION: you won\\\'t be exercising dataset generating parts of petastorm \'\n             \'hence tests results maybe inaccurate\'\n    )\n\n\ndef maybe_cached_dataset(config, name, generating_func):\n    """"""Returns cached dataset instance if caching of datasets is enabled and a valid dataset is available.\n\n    We speedup test startup time by caching previously generated synthetic dataset.\n    This is useful while developing for tests reruns, but can be dangerous since we can\n    get stale results when petastorm code participating in dataset generation is used.\n\n    :param config: request.config object.\n    :param name: name of the cached dataset. Used as a cache key.\n    :param generating_func: This function will be called (`generating_func()`) if dataset cache is disabled or\n      no valid dataset is found in cache.\n    :return:\n    """"""\n    if config.getoption(_CACHE_FAKE_DATASET_OPTION):\n        cache_key = \'{}_{}\'.format(name, \'PY2\' if six.PY2 else \'PY3\')\n        serialized = config.cache.get(cache_key, None)\n        dataset = pickle.loads(b64decode(serialized)) if serialized else None\n        if not dataset or not os.path.exists(dataset.path):\n            dataset = generating_func()\n            config.cache.set(cache_key, b64encode(pickle.dumps(dataset)).decode(\'ascii\'))\n        else:\n            logger.warning(\'CAUTION: %s HAS BEEN USED. USING %s CACHED TEST DATASET! MAYBE STALE!\',\n                           _CACHE_FAKE_DATASET_OPTION, name)\n    else:\n        dataset = generating_func()\n\n    return dataset\n\n\n@pytest.fixture(scope=""session"")\ndef synthetic_dataset(request, tmpdir_factory):\n    def _synthetic_dataset_no_cache():\n        path = tmpdir_factory.mktemp(""data"").strpath\n        url = \'file://\' + path\n        data = create_test_dataset(url, range(_ROWS_COUNT))\n        dataset = SyntheticDataset(url=url, path=path, data=data)\n        return dataset\n\n    return maybe_cached_dataset(request.config, \'synthetic_dataset\', _synthetic_dataset_no_cache)\n\n\n@pytest.fixture(scope=""session"")\ndef scalar_dataset(request, tmpdir_factory):\n    def _pure_parquet_dataset_no_cache():\n        path = tmpdir_factory.mktemp(""data"").strpath\n        url = \'file://\' + path\n        data = create_test_scalar_dataset(url, 100)\n        dataset = SyntheticDataset(url=url, path=path, data=data)\n        return dataset\n\n    return maybe_cached_dataset(request.config, \'scalar\', _pure_parquet_dataset_no_cache)\n\n\n@pytest.fixture(scope=""session"")\ndef many_columns_non_petastorm_dataset(request, tmpdir_factory):\n    """"""This dataset has 1000 columns. All of the same int32 type.""""""\n\n    def _dataset_no_cache():\n        path = tmpdir_factory.mktemp(""data"").strpath\n        url = \'file://\' + path\n        data = create_many_columns_non_petastorm_dataset(url, 10)\n        dataset = SyntheticDataset(url=url, path=path, data=data)\n        return dataset\n\n    return maybe_cached_dataset(request.config, \'many_column_non_petastorm\', _dataset_no_cache)\n\n\nclass SparkTestContext(object):\n    def __init__(self):\n        self.spark = SparkSession.builder \\\n            .master(""local[2]"") \\\n            .appName(""petastorm.spark tests"") \\\n            .getOrCreate()\n        self.tempdir = tempfile.mkdtemp(\'_spark_converter_test\')\n        self.temp_url = \'file://\' + self.tempdir.replace(os.sep, \'/\')\n        self.spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, self.temp_url)\n        spark_dataset_converter._FILE_AVAILABILITY_WAIT_TIMEOUT_SECS = 2\n\n    def tear_down(self):\n        # restore default file availability wait timeout\n        spark_dataset_converter._FILE_AVAILABILITY_WAIT_TIMEOUT_SECS = 30\n        self.spark.stop()\n\n\n@pytest.fixture(scope=\'module\')\ndef spark_test_ctx():\n    ctx = SparkTestContext()\n    try:\n        yield ctx\n    finally:\n        ctx.tear_down()\n'"
petastorm/tests/generate_dataset_for_legacy_tests.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\n\nimport petastorm\nfrom petastorm.tests.test_common import create_test_dataset\n\n\ndef generate_dataset_for_legacy_test():\n    """"""Generates a test dataset and stores it into petastorm/tests/data/legacy/x.x.x folder. The version number\n    is acquired automatically from petastorm.__version__""""""\n    dataset_name = petastorm.__version__\n    path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \'data\', \'legacy\', dataset_name)\n    url = \'file://\' + path\n\n    create_test_dataset(url, range(100))\n\n\nif __name__ == \'__main__\':\n    generate_dataset_for_legacy_test()\n'"
petastorm/tests/tempdir.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport contextlib\nimport os\nimport shutil\nimport tempfile\n\n\n@contextlib.contextmanager\ndef temporary_directory(*args, **kwargs):\n    """"""Create and return the path to a temporary directory\n\n    Return a path to a newly created temporary directory with visibility\n    in the file system. The root directory must exist; parents of\n    the temporary directory will neither be created nor destroyed. The\n    created directory (and any of its contents) will be automatically deleted\n    when the context is exited.\n\n        with tempfile.temporary_directory() as f:\n            ...\n\n    :param dir: The directory to root the temporary directory under\n    :yields: The path to the temporary directory\n    """"""\n    path = None\n    try:\n        path = tempfile.mkdtemp(*args, **kwargs)\n        yield path\n    finally:\n        if path and os.path.exists(path):\n            shutil.rmtree(path)\n'"
petastorm/tests/test_arrow_table_serializer.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport pandas as pd\nimport pyarrow as pa\nfrom pandas.util.testing import assert_frame_equal\n\nfrom petastorm.reader_impl.arrow_table_serializer import ArrowTableSerializer\n\n\ndef test_random_table():\n    """"""Serialize/deserialize some small table""""""\n    expected_dataframe = pd.DataFrame(np.random.randn(50, 4), columns=list(\'ABCD\'))\n    expected_table = pa.Table.from_pandas(expected_dataframe)\n\n    serializer = ArrowTableSerializer()\n    actual_table = serializer.deserialize(serializer.serialize(expected_table))\n    assert_frame_equal(actual_table.to_pandas(), expected_dataframe)\n\n\ndef test_empty_table():\n    """"""See that we can transmit empty tables""""""\n    expected_dataframe = pd.DataFrame(np.empty(shape=(0, 4), dtype=np.int8), columns=list(\'ABCD\'))\n    expected_table = pa.Table.from_pandas(expected_dataframe)\n\n    serializer = ArrowTableSerializer()\n    stream = serializer.serialize(expected_table)\n    actual_table = serializer.deserialize(stream)\n    assert_frame_equal(actual_table.to_pandas(), expected_dataframe)\n'"
petastorm/tests/test_benchmark.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom time import sleep\n\nimport six\n\ntry:\n    from unittest import mock\nexcept ImportError:\n    from mock import mock\n\nfrom petastorm.benchmark.throughput import reader_throughput, _time_warmup_and_work, WorkerPoolType, ReadMethod\n\n\ndef test_pure_python_process_pool_run(synthetic_dataset):\n    reader_throughput(synthetic_dataset.url, [\'id\'], warmup_cycles_count=5, measure_cycles_count=5,\n                      pool_type=WorkerPoolType.PROCESS, loaders_count=1, read_method=ReadMethod.PYTHON,\n                      spawn_new_process=False)\n\n\ndef test_pure_python_process_pool_run_with_pyarrow_serialize(synthetic_dataset):\n    reader_throughput(synthetic_dataset.url, [\'id\'], warmup_cycles_count=5, measure_cycles_count=5,\n                      pool_type=WorkerPoolType.PROCESS, loaders_count=1, read_method=ReadMethod.PYTHON,\n                      spawn_new_process=False, pyarrow_serialize=True)\n\n\ndef test_tf_thread_pool_run(synthetic_dataset):\n    reader_throughput(synthetic_dataset.url, [\'id\', \'id2\'], warmup_cycles_count=5, measure_cycles_count=5,\n                      pool_type=WorkerPoolType.THREAD, loaders_count=1, read_method=ReadMethod.TF)\n\n\ndef test_pure_python_thread_pool_run(synthetic_dataset):\n    # Use a regex to match field name (\'i.\' instead of \'id\')\n    reader_throughput(synthetic_dataset.url, [\'i.\'], warmup_cycles_count=5, measure_cycles_count=5,\n                      pool_type=WorkerPoolType.THREAD, loaders_count=1, read_method=ReadMethod.PYTHON)\n\n\ndef test_pure_python_dummy_pool_run(synthetic_dataset):\n    # Use a regex to match field name (\'i.\' instead of \'id\')\n    reader_throughput(synthetic_dataset.url, [\'i.\'], warmup_cycles_count=5, measure_cycles_count=5,\n                      pool_type=WorkerPoolType.NONE, loaders_count=1, read_method=ReadMethod.PYTHON)\n\n\ndef test_all_fields(synthetic_dataset):\n    reader_throughput(synthetic_dataset.url, None, warmup_cycles_count=5, measure_cycles_count=5,\n                      pool_type=WorkerPoolType.THREAD, loaders_count=1, read_method=ReadMethod.PYTHON)\n\n\ndef test_run_benchmark_cycle_length_of_warmup_and_measure_cycles():\n    measurable = mock.Mock()\n    reader_mock = mock.Mock()\n    _time_warmup_and_work(reader_mock, 2, 3, measurable.next_item)\n    assert 5 == measurable.next_item.call_count\n\n    measurable = mock.Mock()\n    _time_warmup_and_work(reader_mock, 6, 7, measurable.next_item)\n    assert 13 == measurable.next_item.call_count\n\n\ndef test_time_measure():\n    T = 1.2\n    measurable = mock.Mock()\n    reader = mock.Mock()\n    reader.diagnostics.side_effect = {\'some_diags\': 1}\n    wait_times = [0.0, T, T]\n\n    def mock_next_item():\n        a = 1\n        for _ in six.moves.xrange(10000):\n            a += 1\n        sleep(wait_times.pop(0))\n        return 0\n\n    measurable.next_item.side_effect = mock_next_item\n    result = _time_warmup_and_work(reader, 1, 2, measurable.next_item)\n    assert result.time_mean >= T / 2.0\n    assert result.samples_per_second < 1.0 / T\n    assert result.memory_info\n'"
petastorm/tests/test_cache.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nfrom petastorm.cache import NullCache\n\n\nclass TestNullCache(unittest.TestCase):\n\n    def test_null_cache(self):\n        """"""Testing trivial NullCache: should trigger value generating function on each run""""""\n        cache = NullCache()\n        self.assertEqual(42, cache.get(\'some_key\', lambda: 42))\n\n\nif __name__ == \'__main__\':\n    # Delegate to the test framework.\n    unittest.main()\n'"
petastorm/tests/test_codec_compressed_image.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport io\n\nimport numpy as np\nimport pytest\nfrom PIL import Image\n\nfrom petastorm.codecs import CompressedImageCodec\nfrom petastorm.unischema import UnischemaField\n\n\ndef test_png():\n    """"""Simple noop encode/decode using png codec. Verify that supports uint16 png codec and monochrome and\n    color images.""""""\n    for size in [(300, 200), (300, 200, 3)]:\n        for dtype in [np.uint8, np.uint16]:\n            expected_image = np.random.randint(0, np.iinfo(dtype).max, size=size, dtype=dtype)\n            codec = CompressedImageCodec(\'png\')\n            field = UnischemaField(name=\'field_image\', numpy_dtype=dtype, shape=size, codec=codec,\n                                   nullable=False)\n\n            actual_image = codec.decode(field, codec.encode(field, expected_image))\n            np.testing.assert_array_equal(expected_image, actual_image)\n            assert expected_image.dtype == actual_image.dtype\n\n\ndef test_jpeg():\n    """"""Test lossy image codec""""""\n    for size in [(300, 200), (300, 200, 3)]:\n        expected_image = np.random.randint(0, 255, size=size, dtype=np.uint8)\n        codec = CompressedImageCodec(\'jpeg\', quality=100)\n        assert codec.image_codec == \'jpeg\'\n        field = UnischemaField(name=\'field_image\', numpy_dtype=np.uint8, shape=size, codec=codec, nullable=False)\n\n        actual_image = codec.decode(field, codec.encode(field, expected_image))\n        # Check a non exact match between the images. Verifying reasonable mean absolute error (up to 10)\n        mean_abs_error = np.mean(np.abs(expected_image.astype(np.float) - actual_image.astype(np.float)))\n        # The threshold is relatively high as compressing random images with jpeg results in a significant\n        # quality loss\n        assert mean_abs_error < 50\n        assert np.any(expected_image != actual_image, axis=None)\n\n\ndef test_jpeg_quality():\n    """"""Compare mean abs error between different encoding quality settings. Higher quality value should result\n    in a smaller error""""""\n    size = (300, 200, 3)\n    expected_image = np.random.randint(0, 255, size=size, dtype=np.uint8)\n\n    errors = dict()\n    for quality in [10, 99]:\n        codec = CompressedImageCodec(\'jpeg\', quality=quality)\n        field = UnischemaField(name=\'field_image\', numpy_dtype=np.uint8, shape=size, codec=codec, nullable=False)\n        actual_image = codec.decode(field, codec.encode(field, expected_image))\n        errors[quality] = np.mean(np.abs(expected_image.astype(np.float) - actual_image.astype(np.float)))\n\n    assert errors[10] > errors[99]\n\n\ndef test_bad_shape():\n    codec = CompressedImageCodec(\'png\')\n    field = UnischemaField(name=\'field_image\', numpy_dtype=np.uint8, shape=(10, 20), codec=codec, nullable=False)\n    with pytest.raises(ValueError, match=\'Unexpected dimensions\'):\n        codec.encode(field, np.zeros((100, 200), dtype=np.uint8))\n\n\ndef test_bad_dtype():\n    codec = CompressedImageCodec(\'png\')\n    assert codec.image_codec == \'png\'\n    field = UnischemaField(name=\'field_image\', numpy_dtype=np.uint8, shape=(10, 20), codec=codec, nullable=False)\n    with pytest.raises(ValueError, match=\'Unexpected type\'):\n        codec.encode(field, np.zeros((100, 200), dtype=np.uint16))\n\n\ndef test_cross_coding():\n    """"""Encode using PIL and decode using opencv. Previously had an error with channel ordering. This test\n    covers this issue for the future """"""\n    for size in [(300, 200), (300, 200, 3)]:\n        dtype = np.uint8\n        expected_image = np.random.randint(0, np.iinfo(dtype).max, size=size, dtype=np.uint8)\n        codec = CompressedImageCodec(\'png\')\n        field = UnischemaField(name=\'field_image\', numpy_dtype=dtype, shape=size, codec=codec,\n                               nullable=False)\n\n        encoded = Image.fromarray(expected_image)\n        encoded_bytes = io.BytesIO()\n        encoded.save(encoded_bytes, format=\'PNG\')\n\n        actual_image = codec.decode(field, encoded_bytes.getvalue())\n        np.testing.assert_array_equal(expected_image, actual_image)\n        assert expected_image.dtype == actual_image.dtype\n\n\ndef test_invalid_image_size():\n    """"""Codec can encode only (H, W) and (H, W, 3) images""""""\n    codec = CompressedImageCodec(\'png\')\n\n    field = UnischemaField(name=\'field_image\', numpy_dtype=np.uint8, shape=(10, 10, 3), codec=codec,\n                           nullable=False)\n\n    with pytest.raises(ValueError):\n        codec.encode(field, np.zeros((10,), dtype=np.uint8))\n\n    with pytest.raises(ValueError):\n        codec.encode(field, np.zeros((10, 10, 2), dtype=np.uint8))\n\n    with pytest.raises(ValueError):\n        codec.encode(field, np.zeros((10, 10, 10, 10), dtype=np.uint8))\n'"
petastorm/tests/test_codec_ndarray.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport pytest\n\nfrom petastorm.codecs import NdarrayCodec, CompressedNdarrayCodec\nfrom petastorm.unischema import UnischemaField\n\nNUMERIC_DTYPES = [np.uint8, np.int8, np.uint16, np.int16, np.uint32, np.int32, np.uint64, np.int64, np.float32,\n                  np.float64]\n\n\n@pytest.mark.parametrize(\'codec_factory\', [\n    NdarrayCodec,\n    CompressedNdarrayCodec,\n])\ndef test_ndarray_codec(codec_factory):\n    SHAPE = (10, 20, 3)\n    for dtype in NUMERIC_DTYPES:\n        expected = np.random.rand(*SHAPE).astype(dtype=dtype)\n        codec = codec_factory()\n        field = UnischemaField(name=\'test_name\', numpy_dtype=dtype, shape=SHAPE, codec=codec, nullable=False)\n        actual = codec.decode(field, codec.encode(field, expected))\n        np.testing.assert_equal(actual, expected)\n        assert expected.dtype == actual.dtype\n'"
petastorm/tests/test_codec_scalar.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom decimal import Decimal\n\nimport numpy as np\nimport pytest\nfrom pyspark.sql.types import StringType, ByteType, ShortType, IntegerType, LongType, DecimalType, BooleanType, \\\n    FloatType\n\nfrom petastorm.codecs import ScalarCodec\nfrom petastorm.unischema import UnischemaField\n\n\ndef test_byte_string():\n    codec = ScalarCodec(StringType())\n    field = UnischemaField(name=\'field_string\', numpy_dtype=np.string_, shape=(), codec=codec, nullable=False)\n\n    assert codec.decode(field, codec.encode(field, \'abc\')) == b\'abc\'\n    assert codec.decode(field, codec.encode(field, \'\')) == b\'\'\n\n\ndef test_unicode():\n    codec = ScalarCodec(StringType())\n    field = UnischemaField(name=\'field_string\', numpy_dtype=np.unicode_, shape=(), codec=codec, nullable=False)\n\n    assert codec.decode(field, codec.encode(field, \'abc\')) == \'abc\'\n    assert codec.decode(field, codec.encode(field, \'\')) == \'\'\n\n\n@pytest.mark.parametrize(\'spark_numpy_types\', [\n    (ByteType, np.uint8),\n    (ByteType, np.int8),\n    (ShortType, np.int16),\n    (IntegerType, np.int32),\n    (LongType, np.int64),\n])\ndef test_numeric_types(spark_numpy_types):\n    spark_type, numpy_type = spark_numpy_types\n\n    codec = ScalarCodec(spark_type())\n    field = UnischemaField(name=\'field_int\', numpy_dtype=numpy_type, shape=(), codec=codec, nullable=False)\n\n    min_val, max_val = np.iinfo(numpy_type).min, np.iinfo(numpy_type).max\n\n    assert codec.decode(field, codec.encode(field, numpy_type(min_val))) == min_val\n    assert codec.decode(field, codec.encode(field, numpy_type(max_val))) == max_val\n\n\ndef test_scalar_codec_decimal():\n    codec = ScalarCodec(DecimalType(4, 3))\n    field = UnischemaField(name=\'field_decimal\', numpy_dtype=Decimal, shape=(), codec=codec, nullable=False)\n\n    value = Decimal(\'123.4567\')\n    assert codec.decode(field, codec.encode(field, value)) == value\n\n\ndef test_bad_encoded_data_shape():\n    codec = ScalarCodec(IntegerType())\n    field = UnischemaField(name=\'field_int\', numpy_dtype=np.int32, shape=(), codec=codec, nullable=False)\n    with pytest.raises(TypeError):\n        codec.decode(field, codec.encode(field, np.asarray([10, 10])))\n\n\ndef test_bad_unischema_field_shape():\n    codec = ScalarCodec(IntegerType())\n    field = UnischemaField(name=\'field_int\', numpy_dtype=np.int32, shape=(1,), codec=codec, nullable=False)\n    with pytest.raises(ValueError, match=\'must be an empty tuple\'):\n        codec.encode(field, np.int32(1))\n\n\ndef test_encode_scalar_bool():\n    codec = ScalarCodec(BooleanType())\n    field = UnischemaField(name=\'field_bool\', numpy_dtype=np.bool, shape=(), codec=codec, nullable=False)\n\n    encoded = codec.encode(field, np.bool_(True))\n    assert isinstance(codec.encode(field, encoded), bool)\n    assert encoded\n\n    encoded = codec.encode(field, np.bool_(False))\n    assert not encoded\n\n\ndef test_encode_scalar_int():\n    codec = ScalarCodec(IntegerType())\n    field = UnischemaField(name=\'field_int\', numpy_dtype=np.int32, shape=(), codec=codec, nullable=False)\n    encoded = codec.encode(field, np.int32(42))\n    assert isinstance(encoded, int)\n    assert 42 == encoded\n\n\ndef test_encode_scalar_float():\n    codec = ScalarCodec(FloatType())\n    expected = np.random.random(()).astype(np.float64)\n    field = UnischemaField(name=\'field_float\', numpy_dtype=np.float32, shape=(), codec=codec, nullable=False)\n    encoded = codec.encode(field, expected)\n    assert isinstance(encoded, float)\n    assert expected == encoded\n\n\ndef test_encode_scalar_string():\n    codec = ScalarCodec(StringType())\n    expected = \'surprise\'\n    field = UnischemaField(name=\'field_string\', numpy_dtype=np.unicode_, shape=(), codec=codec, nullable=False)\n    encoded = codec.encode(field, expected)\n    assert isinstance(encoded, str)\n    assert expected == encoded\n\n\n@pytest.mark.parametrize(""non_scalar_value"", [[1.2], np.asarray([3.4]), [5, 6]])\ndef test_encode_non_scalar_type_is_passed(non_scalar_value):\n    codec = ScalarCodec(FloatType())\n    field = UnischemaField(name=\'field_float\', numpy_dtype=np.float32, shape=(), codec=codec, nullable=False)\n    with pytest.raises(TypeError, match=\'Expected a scalar\'):\n        codec.encode(field, non_scalar_value)\n'"
petastorm/tests/test_common.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import division\n\nimport random\nfrom collections import OrderedDict\nfrom decimal import Decimal\nfrom functools import partial\n\nimport numpy as np\nimport pyarrow as pa\nimport pytz\nfrom pyspark import Row\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StringType, DecimalType, DoubleType, StructField, \\\n    IntegerType, StructType, DateType, TimestampType, ShortType, ArrayType\n\nfrom petastorm.codecs import CompressedImageCodec, NdarrayCodec, \\\n    ScalarCodec\nfrom petastorm.etl.dataset_metadata import materialize_dataset\nfrom petastorm.etl.rowgroup_indexers import SingleFieldIndexer\nfrom petastorm.etl.rowgroup_indexing import build_rowgroup_index\nfrom petastorm.unischema import Unischema, UnischemaField, dict_to_spark_row\n\n_DEFAULT_IMAGE_SIZE = (32, 16, 3)\n\nTestSchema = Unischema(\'TestSchema\', [\n    UnischemaField(\'partition_key\', np.unicode_, ()),\n    UnischemaField(\'id\', np.int64, ()),\n    UnischemaField(\'id2\', np.int32, (), ScalarCodec(ShortType()), False),  # Explicit scalar codec in some scalar fields\n    UnischemaField(\'id_float\', np.float64, ()),\n    UnischemaField(\'id_odd\', np.bool_, ()),\n    UnischemaField(\'python_primitive_uint8\', np.uint8, ()),\n    UnischemaField(\'image_png\', np.uint8, _DEFAULT_IMAGE_SIZE, CompressedImageCodec(\'png\'), False),\n    UnischemaField(\'matrix\', np.float32, _DEFAULT_IMAGE_SIZE, NdarrayCodec(), False),\n    UnischemaField(\'decimal\', Decimal, (), ScalarCodec(DecimalType(10, 9)), False),\n    UnischemaField(\'matrix_uint16\', np.uint16, _DEFAULT_IMAGE_SIZE, NdarrayCodec(), False),\n    UnischemaField(\'matrix_uint32\', np.uint32, _DEFAULT_IMAGE_SIZE, NdarrayCodec(), False),\n    UnischemaField(\'matrix_string\', np.string_, (None, None,), NdarrayCodec(), False),\n    UnischemaField(\'empty_matrix_string\', np.string_, (None,), NdarrayCodec(), False),\n    UnischemaField(\'matrix_nullable\', np.uint16, _DEFAULT_IMAGE_SIZE, NdarrayCodec(), True),\n    UnischemaField(\'sensor_name\', np.unicode_, (1,), NdarrayCodec(), False),\n    UnischemaField(\'string_array_nullable\', np.unicode_, (None,), NdarrayCodec(), True),\n    UnischemaField(\'integer_nullable\', np.int32, (), nullable=True),\n])\n\n\ndef _random_binary_string_gen(max_length):\n    """"""Returns a single random string up to max_length specified length that may include \\x00 character anywhere in the\n    string""""""\n    size = random.randint(0, max_length)\n    return \'\'.join(random.choice((\'\\x00\', \'A\', \'B\')) for _ in range(size))\n\n\ndef _random_binary_string_matrix(rows, cols, max_length):\n    """"""Returns a list of lists of random strings""""""\n    return [[_random_binary_string_gen(max_length) for _ in range(cols)] for _ in range(rows)]\n\n\ndef _randomize_row(id_num):\n    """"""Returns a row with random values""""""\n    row_dict = {\n        TestSchema.id.name: np.int64(id_num),\n        TestSchema.id2.name: np.int32(id_num % 2),\n        TestSchema.id_float.name: np.float64(id_num),\n        TestSchema.id_odd.name: np.bool_(id_num % 2),\n        TestSchema.partition_key.name: np.unicode_(\'p_{}\'.format(int(id_num / 10))),\n        TestSchema.python_primitive_uint8.name: np.random.randint(0, 255, dtype=np.uint8),\n        TestSchema.image_png.name: np.random.randint(0, 255, _DEFAULT_IMAGE_SIZE).astype(np.uint8),\n        TestSchema.matrix.name: np.random.random(size=_DEFAULT_IMAGE_SIZE).astype(np.float32),\n        TestSchema.decimal.name: Decimal(np.random.randint(0, 255) / Decimal(100)),\n        TestSchema.matrix_uint16.name: np.random.randint(0, 2 ** 16 - 1, _DEFAULT_IMAGE_SIZE).astype(np.uint16),\n        TestSchema.matrix_uint32.name: np.random.randint(0, 2 ** 32 - 1, _DEFAULT_IMAGE_SIZE).astype(np.uint32),\n        TestSchema.matrix_string.name: np.asarray(_random_binary_string_matrix(2, 3, 10)).astype(np.bytes_),\n        TestSchema.empty_matrix_string.name: np.asarray([], dtype=np.string_),\n        TestSchema.matrix_nullable.name: None,\n        TestSchema.sensor_name.name: np.asarray([\'test_sensor\'], dtype=np.unicode_),\n        TestSchema.string_array_nullable.name:\n            None if id_num % 5 == 0 else np.asarray([], dtype=np.unicode_)\n            if id_num % 4 == 0 else np.asarray([str(i + id_num) for i in range(2)], dtype=np.unicode_),\n        TestSchema.integer_nullable.name: None if id_num % 2 else np.int32(id_num),\n    }\n    return row_dict\n\n\ndef create_test_dataset(tmp_url, rows, num_files=2, spark=None, use_summary_metadata=False):\n    """"""\n    Creates a test dataset under tmp_dir, with rows and num_files that has TestSchema.\n    :param tmp_url: The URL of the temp directory to store the test dataset in.\n    :param rows: The number of rows for the dataset.\n    :param num_files: The number of files to partition the data between.\n    :param spark: An optional spark session to use\n    :param use_summary_metadata: If True, _metadata file will be created.\n    :return: A list of the dataset dictionary.\n    """"""\n\n    shutdown = False\n    if not spark:\n        spark_session = SparkSession \\\n            .builder \\\n            .appName(\'petastorm_end_to_end_test\') \\\n            .master(\'local[*]\')\n\n        spark = spark_session.getOrCreate()\n        shutdown = True\n    spark_context = spark.sparkContext\n\n    with materialize_dataset(spark, tmp_url, TestSchema, use_summary_metadata=use_summary_metadata):\n        id_rdd = spark_context.parallelize(rows, numSlices=40)\n\n        # Make up some random data and store it for referencing in the tests\n        random_dicts_rdd = id_rdd.map(_randomize_row).cache()\n        dataset_dicts = random_dicts_rdd.collect()\n\n        def _partition_key_to_str(row):\n            row[\'partition_key\'] = str(row[\'partition_key\'])\n            return row\n\n        random_dicts_rdd = random_dicts_rdd.map(_partition_key_to_str)\n\n        random_rows_rdd = random_dicts_rdd.map(partial(dict_to_spark_row, TestSchema))\n\n        # Create a spark dataframe with the random rows\n        dataframe = spark. \\\n            createDataFrame(random_rows_rdd, TestSchema.as_spark_schema()).sort(\'id\')\n\n        # Save a parquet\n        dataframe. \\\n            coalesce(num_files). \\\n            write.option(\'compression\', \'none\'). \\\n            partitionBy(\'partition_key\'). \\\n            mode(\'overwrite\'). \\\n            parquet(tmp_url)\n\n    # Create list of objects to build row group indexes\n    indexers = [\n        SingleFieldIndexer(TestSchema.id.name, TestSchema.id.name),\n        SingleFieldIndexer(TestSchema.sensor_name.name, TestSchema.sensor_name.name),\n        SingleFieldIndexer(TestSchema.string_array_nullable.name, TestSchema.string_array_nullable.name),\n        SingleFieldIndexer(TestSchema.partition_key.name, TestSchema.partition_key.name),\n    ]\n    build_rowgroup_index(tmp_url, spark_context, indexers)\n\n    if shutdown:\n        spark.stop()\n\n    return dataset_dicts\n\n\ndef create_test_scalar_dataset(output_url, num_rows, num_files=4, spark=None, partition_by=None):\n    """"""Creates a dataset in tmp_url location. The dataset emulates non-petastorm dataset, i.e. contains only native\n    parquet types.\n\n    These are the fields with mock data:\n      \'id\', \'int_fixed_size_list\', \'datetime\', \'timestamp\', \'string\', \'string2\', \'float64\'\n\n    :param output_url: Url specifying the location the parquet store is written to\n    :param num_rows: Number of rows in the generated dataset\n    :param num_files: Number of parquet files that will be written into the parquet store\n    :param spark: An instance of spark session object. If `None` (default), a new spark session is created.\n    :param partition_by: A list of fields to partition the parquet store by.\n    :return: A list of records with a copy of the data written to the dataset.\n    """"""\n\n    is_list_of_scalar_broken = pa.__version__ == \'0.15.0\'\n\n    partition_by = partition_by or []\n    shutdown = False\n    if not spark:\n        spark_session = SparkSession \\\n            .builder \\\n            .appName(\'petastorm_end_to_end_test\') \\\n            .master(\'local[*]\')\n\n        spark = spark_session.getOrCreate()\n        shutdown = True\n        hadoop_config = spark.sparkContext._jsc.hadoopConfiguration()\n        hadoop_config.setInt(\'parquet.block.size\', 100)\n\n    def expected_row(i):\n        result = {\'id\': np.int32(i),\n                  \'id_div_700\': np.int32(i // 700),\n                  \'datetime\': np.datetime64(\'2019-01-02\'),\n                  \'timestamp\': np.datetime64(\'2005-02-25T03:30\'),\n                  \'string\': np.unicode_(\'hello_{}\'.format(i)),\n                  \'string2\': np.unicode_(\'world_{}\'.format(i)),\n                  \'float64\': np.float64(i) * .66}\n        if not is_list_of_scalar_broken:\n            result[\'int_fixed_size_list\'] = np.arange(1 + i, 10 + i).astype(np.int32)\n        result = OrderedDict(sorted(result.items(), key=lambda item: item[0]))\n        return result\n\n    expected_data = [expected_row(i) for i in range(num_rows)]\n\n    expected_data_as_scalars = [{k: v.item() if isinstance(v, np.generic) else v for k, v in row.items()} for row\n                                in expected_data]\n\n    # np.datetime64 is converted to a timezone unaware datetime instances. Working explicitly in UTC so we don\'t need\n    # to think about local timezone in the tests\n    for row in expected_data_as_scalars:\n        row[\'timestamp\'] = row[\'timestamp\'].replace(tzinfo=pytz.UTC)\n        if not is_list_of_scalar_broken:\n            row[\'int_fixed_size_list\'] = row[\'int_fixed_size_list\'].tolist()\n\n    rows = [Row(**row) for row in expected_data_as_scalars]\n\n    maybe_int_fixed_size_list_field = [StructField(\'int_fixed_size_list\', ArrayType(IntegerType(), False), False)] \\\n        if not is_list_of_scalar_broken else []\n\n    # WARNING: surprisingly, schema fields and row fields are matched only by order and not name.\n    # We must maintain alphabetical order of the struct fields for the code to work!!!\n    schema = StructType(\n        [\n            StructField(\'datetime\', DateType(), False),\n            StructField(\'float64\', DoubleType(), False),\n            StructField(\'id\', IntegerType(), False),\n            StructField(\'id_div_700\', IntegerType(), False),\n        ] + maybe_int_fixed_size_list_field +\n        [\n            StructField(\'string\', StringType(), False),\n            StructField(\'string2\', StringType(), False),\n            StructField(\'timestamp\', TimestampType(), False),\n        ])\n\n    dataframe = spark.createDataFrame(rows, schema)\n    dataframe. \\\n        coalesce(num_files). \\\n        write.option(\'compression\', \'none\'). \\\n        mode(\'overwrite\'). \\\n        partitionBy(*partition_by). \\\n        parquet(output_url)\n\n    if shutdown:\n        spark.stop()\n\n    return expected_data\n\n\ndef create_many_columns_non_petastorm_dataset(output_url, num_rows, num_columns=1000, num_files=4, spark=None):\n    """"""Creates a dataset with the following properties (used in tests)\n\n    1. Has 1000 columns\n    2. Each column is an int32 integer\n    3. Parquet store consists of 4 files (controlled by ``num_files`` argument)\n\n    :param output_url: The dataset is written to this url (e.g. ``file:///tmp/some_directory``)\n    :param num_rows: Number of rows in the generated dataset\n    :param num_columns: Number of columns (1000 is the default)\n    :param num_files: Number of parquet files that will be created in the store\n    :param spark: An instance of SparkSession object. A new instance will be created if non specified\n    :return:\n    """"""\n    shutdown = False\n    if not spark:\n        spark_session = SparkSession \\\n            .builder \\\n            .appName(\'petastorm_end_to_end_test\') \\\n            .master(\'local[*]\')\n\n        spark = spark_session.getOrCreate()\n        shutdown = True\n\n    column_names = [\'col_{}\'.format(col_id) for col_id in range(num_columns)]\n\n    def generate_row(i):\n        return {\'col_{}\'.format(col_id): i * 10000 for col_id, col_name in enumerate(column_names)}\n\n    expected_data = [generate_row(row_number) for row_number in range(num_rows)]\n\n    rows = [Row(**row) for row in expected_data]\n\n    # WARNING: surprisingly, schema fields and row fields are matched only by order and not name.\n    schema = StructType([StructField(column_name, IntegerType(), False) for column_name in column_names])\n\n    dataframe = spark.createDataFrame(rows, schema)\n    dataframe. \\\n        coalesce(num_files). \\\n        write.option(\'compression\', \'none\'). \\\n        mode(\'overwrite\'). \\\n        parquet(output_url)\n\n    if shutdown:\n        spark.stop()\n\n    return expected_data\n'"
petastorm/tests/test_copy_dataset.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport glob\nimport os\n\nimport numpy as np\nimport pytest\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.utils import AnalysisException\n\nfrom petastorm.reader import make_reader\nfrom petastorm.tools.copy_dataset import _main, copy_dataset\n\n\n@pytest.fixture()\ndef spark_session():\n    return SparkSession.builder.appName(\'petastorm-copy\').getOrCreate()\n\n\ndef test_copy_and_overwrite_cli(tmpdir, synthetic_dataset):\n    target_url = \'file:///\' + os.path.join(tmpdir.strpath, \'copied_data\')\n    _main([synthetic_dataset.url, target_url])\n\n    with make_reader(target_url, num_epochs=1) as reader:\n        for row in reader:\n            actual = row._asdict()\n            expected = next(d for d in synthetic_dataset.data if d[\'id\'] == actual[\'id\'])\n            np.testing.assert_equal(actual, expected)\n\n    with pytest.raises(AnalysisException, match=\'already exists\'):\n        _main([synthetic_dataset.url, target_url])\n\n    _main([synthetic_dataset.url, target_url, \'--overwrite\'])\n\n\ndef test_copy_some_fields_with_repartition_cli(tmpdir, synthetic_dataset):\n    target_path = os.path.join(tmpdir.strpath, \'copied_data\')\n    target_url = \'file://\' + target_path\n    _main([synthetic_dataset.url, target_url, \'--field-regex\', r\'\\bid\\b\', \'--partition-count\', \'1\'])\n\n    # Check reparititioning\n    assert 1 == len(glob.glob(os.path.join(target_path, \'part-*\')))\n\n    # Check we the regex filter worked\n    with make_reader(target_url, num_epochs=1) as reader:\n        assert list(reader.schema.fields.keys()) == [\'id\']\n\n\ndef test_copy_not_null_rows_cli(tmpdir, synthetic_dataset):\n    target_url = \'file://\' + os.path.join(tmpdir.strpath, \'copied_data\')\n\n    _main([synthetic_dataset.url, target_url, \'--not-null-fields\', \'string_array_nullable\'])\n    with make_reader(target_url, num_epochs=1) as reader:\n        not_null_data = list(reader)\n    assert len(not_null_data) < len(synthetic_dataset.data)\n\n\ndef test_bad_regex(synthetic_dataset):\n    with pytest.raises(ValueError, match=\'do not match any fields\'):\n        copy_dataset(None, synthetic_dataset.url, \'\', [\'bogus_name\'], [], False, 1, 196)\n'"
petastorm/tests/test_dataset_metadata.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport pytest\nimport pyarrow\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import IntegerType\n\nfrom petastorm.codecs import ScalarCodec\nfrom petastorm.etl.dataset_metadata import get_schema_from_dataset_url, materialize_dataset\nfrom petastorm.tests.test_common import TestSchema\nfrom petastorm.unischema import Unischema, UnischemaField, dict_to_spark_row\n\n\ndef test_get_schema_from_dataset_url(synthetic_dataset):\n    schema = get_schema_from_dataset_url(synthetic_dataset.url)\n    assert TestSchema.fields == schema.fields\n\n\ndef test_get_schema_from_dataset_url_bogus_url():\n    with pytest.raises(IOError):\n        get_schema_from_dataset_url(\'file:///non-existing-path\')\n\n    with pytest.raises(ValueError):\n        get_schema_from_dataset_url(\'/invalid_url\')\n\n\ndef test_serialize_filesystem_factory(tmpdir):\n    SimpleSchema = Unischema(\'SimpleSchema\', [\n        UnischemaField(\'id\', np.int32, (), ScalarCodec(IntegerType()), False),\n        UnischemaField(\'foo\', np.int32, (), ScalarCodec(IntegerType()), False),\n    ])\n\n    class BogusFS(pyarrow.LocalFileSystem):\n        def __getstate__(self):\n            raise RuntimeError(""can not serialize"")\n\n    rows_count = 10\n    output_url = ""file://{0}/fs_factory_test"".format(tmpdir)\n    rowgroup_size_mb = 256\n    spark = SparkSession.builder.config(\'spark.driver.memory\', \'2g\').master(\'local[2]\').getOrCreate()\n    sc = spark.sparkContext\n    with materialize_dataset(spark, output_url, SimpleSchema, rowgroup_size_mb, filesystem_factory=BogusFS):\n        rows_rdd = sc.parallelize(range(rows_count))\\\n            .map(lambda x: {\'id\': x, \'foo\': x})\\\n            .map(lambda x: dict_to_spark_row(SimpleSchema, x))\n\n        spark.createDataFrame(rows_rdd, SimpleSchema.as_spark_schema()) \\\n            .write \\\n            .parquet(output_url)\n'"
petastorm/tests/test_decode_row.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom decimal import Decimal\n\nimport numpy as np\nimport pytest\nfrom pyspark.sql.types import DoubleType\n\nfrom petastorm.codecs import NdarrayCodec, ScalarCodec\nfrom petastorm.unischema import UnischemaField, Unischema\nfrom petastorm.utils import decode_row, DecodeFieldError\n\nMatrixField = UnischemaField(\'matrix\', np.float64, (10, 10), NdarrayCodec(), False)\nMatrixSchema = Unischema(\'TestSchema\', [MatrixField])\n\n\ndef test_nominal_case():\n    """"""Nominal flow: can decode field successfully""""""\n    expected = np.random.rand(10, 10)\n    row = {\'matrix\': NdarrayCodec().encode(MatrixField, expected)}\n\n    actual = decode_row(row, MatrixSchema)[\'matrix\']\n\n    np.testing.assert_equal(actual, expected)\n\n\ndef test_can_not_decode():\n    """"""Make sure field name is part of the error message""""""\n    row = {\'matrix\': \'bogus\'}\n\n    with pytest.raises(DecodeFieldError, match=\'matrix\'):\n        decode_row(row, MatrixSchema)\n\n\ndef test_decode_numpy_scalar_when_codec_is_none():\n    """"""Decoding a row that has a field with the codec set to None. The type should be deduced automatically\n    from UnischemaField\'s numpy_dtype attribute""""""\n\n    MatrixSchema = Unischema(\'TestSchema\', [UnischemaField(\'scalar\', np.float64, ())])\n    row = {\'scalar\': 42.0}\n    decoded_value = decode_row(row, MatrixSchema)[\'scalar\']\n    assert decoded_value == 42\n    assert isinstance(decoded_value, np.float64)\n\n\ndef test_decode_decimal_scalar_when_codec_is_none():\n    """"""Decoding a row that has a field with the codec set to None. The type should be deduced automatically\n    from UnischemaField\'s numpy_dtype attribute if the type is either a numpy scalar or a Decimal""""""\n\n    MatrixSchema = Unischema(\'TestSchema\', [UnischemaField(\'scalar\', Decimal, ())])\n\n    row = {\'scalar\': \'123.45\'}\n    decoded_value = decode_row(row, MatrixSchema)[\'scalar\']\n    assert decoded_value == Decimal(\'123.45\')\n    assert isinstance(decoded_value, Decimal)\n\n    row = {\'scalar\': Decimal(\'123.45\')}\n    decoded_value = decode_row(row, MatrixSchema)[\'scalar\']\n    assert decoded_value == Decimal(\'123.45\')\n    assert isinstance(decoded_value, Decimal)\n\n\ndef test_decode_numpy_scalar_with_explicit_scalar_codec():\n    """"""Decoding a row that has a field with the codec set explicitly""""""\n\n    MatrixSchema = Unischema(\'TestSchema\', [UnischemaField(\'scalar\', np.float64, (), ScalarCodec(DoubleType()), False)])\n    row = {\'scalar\': 42.0}\n    decoded_value = decode_row(row, MatrixSchema)[\'scalar\']\n    assert decoded_value == 42\n    assert isinstance(decoded_value, np.float64)\n\n\ndef test_decode_numpy_scalar_with_unknown_dtype():\n    """"""If numpy_dtype is None, then the value is not decoded, just passed through.""""""\n\n    MatrixSchema = Unischema(\'TestSchema\', [UnischemaField(\'scalar\', None, ())])\n    row = {\'scalar\': [4, 2]}\n    decoded_value = decode_row(row, MatrixSchema)[\'scalar\']\n    assert decoded_value == [4, 2]\n'"
petastorm/tests/test_disk_cache.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport pyarrow as pa\n\nfrom petastorm.local_disk_arrow_table_cache import LocalDiskArrowTableCache\nfrom petastorm.local_disk_cache import LocalDiskCache\n\nMB = 2 ** 20\nKB = 2 ** 10\n\n\ndef _recursive_folder_size(folder):\n    folder_size = 0\n    for (path, _, files) in os.walk(folder):\n        for flename in files:\n            filename = os.path.join(path, flename)\n            folder_size += os.path.getsize(filename)\n    return folder_size\n\n\ndef test_simple_scalar_cache(tmpdir):\n    """"""Testing trivial NullCache: should trigger value generating function on each run""""""\n    cache = LocalDiskCache(tmpdir.strpath, 1 * MB, 4)\n    assert 42 == cache.get(\'some_key\', lambda: 42)\n    assert 42 == cache.get(\'some_key\', lambda: 43)\n\n\ndef test_size_limit_constraint(tmpdir):\n    """"""Testing trivial NullCache: should trigger value generating function on each run""""""\n    # We will write total of 5MB to the cache (50KB items x 100)\n    RECORD_SIZE_BYTES = 50 * KB\n    RECORDS_COUNT = 100\n\n    a_record = np.random.randint(0, 255, (RECORD_SIZE_BYTES,), np.uint8)\n    cache = LocalDiskCache(tmpdir.strpath, 1 * MB, RECORD_SIZE_BYTES, shards=1)\n\n    for i in range(RECORDS_COUNT):\n        cache.get(\'some_key_{}\'.format(i), lambda: a_record)\n\n    # Check that we are more or less within the size limit\n    assert _recursive_folder_size(tmpdir.strpath) < 3 * MB\n\n\ndef _should_never_be_called():\n    assert False, \'Should not be called\'\n\n\ndef test_arrow_table_caching(tmpdir):\n    cache = LocalDiskArrowTableCache(tmpdir.strpath, 10 * MB, 4)\n\n    df = pd.DataFrame(np.random.randn(50, 4), columns=list(\'ABCD\'))\n    dummy_table = pa.Table.from_pandas(df)\n\n    table_from_cache = cache.get(\'my_key\', lambda: dummy_table)\n    assert table_from_cache == dummy_table\n\n    cache.get(\'my_key\', _should_never_be_called)\n'"
petastorm/tests/test_end_to_end.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport operator\nimport os\nfrom concurrent.futures import ThreadPoolExecutor\nfrom shutil import rmtree, copytree\nfrom six.moves.urllib.parse import urlparse\n\nimport numpy as np\nimport pyarrow.hdfs\nimport pytest\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import LongType, ShortType, StringType\n\ntry:\n    from mock import mock\nexcept ImportError:\n    from unittest import mock\n\nfrom petastorm import make_reader, make_batch_reader, TransformSpec\nfrom petastorm.codecs import ScalarCodec, CompressedImageCodec\nfrom petastorm.errors import NoDataAvailableError\nfrom petastorm.etl.dataset_metadata import materialize_dataset\nfrom petastorm.predicates import in_lambda\nfrom petastorm.selectors import SingleIndexSelector, IntersectIndexSelector, UnionIndexSelector\nfrom petastorm.tests.test_common import create_test_dataset, TestSchema\nfrom petastorm.tests.test_end_to_end_predicates_impl import \\\n    PartitionKeyInSetPredicate, EqualPredicate, VectorizedEqualPredicate\nfrom petastorm.unischema import UnischemaField, Unischema\n\n# pylint: disable=unnecessary-lambda\nMINIMAL_READER_FLAVOR_FACTORIES = [\n    lambda url, **kwargs: make_reader(url, reader_pool_type=\'dummy\', **kwargs),\n]\n\n# pylint: disable=unnecessary-lambda\nALL_READER_FLAVOR_FACTORIES = MINIMAL_READER_FLAVOR_FACTORIES + [\n    lambda url, **kwargs: make_reader(url, reader_pool_type=\'thread\', **kwargs),\n    lambda url, **kwargs: make_reader(url, reader_pool_type=\'process\', workers_count=2, **kwargs),\n    lambda url, **kwargs: make_reader(url, reader_pool_type=\'process\', workers_count=2, **kwargs),\n]\n\nSCALAR_FIELDS = [f for f in TestSchema.fields.values() if isinstance(f.codec, ScalarCodec)]\n\nSCALAR_ONLY_READER_FACTORIES = [\n    lambda url, **kwargs: make_batch_reader(url, reader_pool_type=\'dummy\', **kwargs),\n    lambda url, **kwargs: make_batch_reader(url, reader_pool_type=\'process\', workers_count=2, **kwargs),\n]\n\n\ndef _check_simple_reader(reader, expected_data, expected_rows_count=None, check_types=True, limit_checked_rows=None):\n    # Read a bunch of entries from the dataset and compare the data to reference\n    def _type(v):\n        if isinstance(v, np.ndarray):\n            if v.dtype.str.startswith(\'|S\'):\n                return \'|S\'\n            else:\n                return v.dtype\n        else:\n            return type(v)\n\n    expected_rows_count = expected_rows_count or len(expected_data)\n    count = 0\n\n    for i, row in enumerate(reader):\n        if limit_checked_rows and i >= limit_checked_rows:\n            break\n\n        actual = row._asdict()\n        expected = next(d for d in expected_data if d[\'id\'] == actual[\'id\'])\n        np.testing.assert_equal(actual, expected)\n        actual_types = {k: _type(v) for k, v in actual.items()}\n        expected_types = {k: _type(v) for k, v in expected.items()}\n        assert not check_types or actual_types == expected_types\n        count += 1\n\n    if limit_checked_rows:\n        assert count == min(expected_rows_count, limit_checked_rows)\n    else:\n        assert count == expected_rows_count\n\n\ndef _readout_all_ids(reader, limit=None):\n    ids = []\n    for i, row in enumerate(reader):\n        if limit is not None and i >= limit:\n            break\n        ids.append(row.id)\n\n    # Flatten ids if reader returns batches (make_batch_reader)\n    if isinstance(ids[0], np.ndarray):\n        ids = [i for arr in ids for i in arr]\n\n    return ids\n\n\n@pytest.mark.parametrize(\'reader_factory\', ALL_READER_FLAVOR_FACTORIES)\ndef test_simple_read(synthetic_dataset, reader_factory):\n    """"""Just a bunch of read and compares of all values to the expected values using the different reader pools""""""\n    with reader_factory(synthetic_dataset.url) as reader:\n        _check_simple_reader(reader, synthetic_dataset.data)\n\n\n@pytest.mark.parametrize(\'reader_factory\', [\n    lambda url, **kwargs: make_reader(url, reader_pool_type=\'dummy\', **kwargs)\n])\ndef test_transform_function(synthetic_dataset, reader_factory):\n    """"""""""""\n\n    def double_matrix(sample):\n        sample[\'matrix\'] *= 2\n        return sample\n\n    with reader_factory(synthetic_dataset.url, schema_fields=[TestSchema.id, TestSchema.matrix],\n                        transform_spec=TransformSpec(double_matrix)) as reader:\n        actual = next(reader)\n        original_sample = next(d for d in synthetic_dataset.data if d[\'id\'] == actual.id)\n        expected_matrix = original_sample[\'matrix\'] * 2\n        np.testing.assert_equal(expected_matrix, actual.matrix)\n\n\n@pytest.mark.parametrize(\'reader_factory\', [\n    lambda url, **kwargs: make_reader(url, reader_pool_type=\'dummy\', **kwargs)\n])\ndef test_transform_function_returns_a_new_dict(synthetic_dataset, reader_factory):\n    """"""""""""\n\n    def double_matrix(sample):\n        return {\'id\': -1}\n\n    with reader_factory(synthetic_dataset.url, schema_fields=[TestSchema.id],\n                        transform_spec=TransformSpec(double_matrix)) as reader:\n        all_samples = list(reader)\n        actual_ids = list(map(lambda x: x.id, all_samples))\n\n        np.testing.assert_equal(actual_ids, [-1] * len(synthetic_dataset.data))\n\n\n@pytest.mark.parametrize(\'reader_factory\', [\n    lambda url, **kwargs: make_reader(url, reader_pool_type=\'dummy\', **kwargs)\n])\ndef test_transform_remove_field(synthetic_dataset, reader_factory):\n    """"""Make sure we apply transform only after we apply the predicate""""""\n\n    with reader_factory(synthetic_dataset.url, schema_fields=[TestSchema.id, TestSchema.id2],\n                        transform_spec=TransformSpec(removed_fields=[\'id2\'])) as reader:\n        row = next(reader)\n        assert \'id2\' not in row._fields\n        assert \'id\' in row._fields\n\n\n@pytest.mark.parametrize(\'reader_factory\', [\n    lambda url, **kwargs: make_reader(url, reader_pool_type=\'dummy\', **kwargs)\n])\ndef test_transform_function_with_predicate(synthetic_dataset, reader_factory):\n    """"""Make sure we apply transform only after we apply the predicate""""""\n\n    with reader_factory(synthetic_dataset.url, schema_fields=[TestSchema.id, TestSchema.id2],\n                        predicate=in_lambda([\'id2\'], lambda id2: id2 == 1),\n                        transform_spec=TransformSpec(removed_fields=[\'id2\'])) as reader:\n        rows = list(reader)\n        assert \'id2\' not in rows[0]._fields\n        actual_ids = np.asarray(list(row.id for row in rows))\n        assert actual_ids.size > 0\n        # In the test data id2 = id % 2, which means we expect only odd ids to remain after\n        # we apply lambda id2: id2 == 1 predicate.\n        assert np.all(actual_ids % 2 == 1)\n\n\n@pytest.mark.parametrize(\'reader_factory\', [\n    lambda url, **kwargs: make_reader(url, reader_pool_type=\'dummy\', **kwargs)\n])\ndef test_transform_function_returns_a_new_dict_with_predicate(synthetic_dataset, reader_factory):\n    def transform(sample):\n        return {\'id\': sample[\'id\'], \'id2\': -1}\n\n    with reader_factory(synthetic_dataset.url, schema_fields=[TestSchema.id, TestSchema.id2],\n                        predicate=in_lambda([\'id2\'], lambda id2: id2 == 1),\n                        transform_spec=TransformSpec(func=transform)) as reader:\n        rows = list(reader)\n        actual_ids = np.asarray(list(row.id for row in rows))\n        assert actual_ids.size > 0\n        # In the test data id2 = id % 2, which means we expect only odd ids to remain after\n        # we apply lambda id2: id2 == 1 predicate.\n        assert np.all(actual_ids % 2 == 1)\n\n        transformed_ids = np.asarray(list(row.id2 for row in rows))\n        assert np.all(transformed_ids == -1)\n\n\n@pytest.mark.parametrize(\'reader_factory\', [\n    lambda url, **kwargs: make_reader(url, reader_pool_type=\'dummy\', **kwargs)\n])\ndef test_transform_function_new_field(synthetic_dataset, reader_factory):\n    """"""""""""\n\n    def double_matrix(sample):\n        sample[\'double_matrix\'] = sample[\'matrix\'] * 2\n        del sample[\'matrix\']\n        return sample\n\n    with reader_factory(synthetic_dataset.url, schema_fields=[TestSchema.id, TestSchema.matrix],\n                        transform_spec=TransformSpec(double_matrix,\n                                                     [(\'double_matrix\', np.float32, (32, 16, 3), False)],\n                                                     [\'matrix\'])) as reader:\n        actual = next(reader)\n        original_sample = next(d for d in synthetic_dataset.data if d[\'id\'] == actual.id)\n        expected_matrix = original_sample[\'matrix\'] * 2\n        np.testing.assert_equal(expected_matrix, actual.double_matrix)\n\n\n@mock.patch(\'petastorm.unischema._UNISCHEMA_FIELD_ORDER\', \'alphabetical\')\ndef test_transform_function_batched(scalar_dataset):\n    def double_float64(sample):\n        sample[\'float64\'] *= 2\n        return sample\n\n    with make_batch_reader(scalar_dataset.url, transform_spec=TransformSpec(double_float64)) as reader:\n        actual = next(reader)\n        for actual_id, actual_float64 in zip(actual.id, actual.float64):\n            original_sample = next(d for d in scalar_dataset.data if d[\'id\'] == actual_id)\n            expected_matrix = original_sample[\'float64\'] * 2\n            np.testing.assert_equal(expected_matrix, actual_float64)\n\n\ndef test_transform_function_batched_deleting_column(scalar_dataset):\n    def double_float64(sample):\n        del sample[\'float64\']\n        return sample\n\n    with make_batch_reader(scalar_dataset.url,\n                           transform_spec=TransformSpec(double_float64, removed_fields=[\'float64\'])) as reader:\n        actual = next(reader)\n        assert \'float64\' not in actual._fields\n\n\ndef test_transform_function_batched_auto_deleting_column(scalar_dataset):\n    with make_batch_reader(scalar_dataset.url,\n                           transform_spec=TransformSpec(removed_fields=[\'float64\'])) as reader:\n        actual = next(reader)\n        assert \'float64\' not in actual._fields\n\n\ndef test_transform_function_with_predicate_batched(scalar_dataset):\n    def double_float64(sample):\n        assert all(sample[\'id\'] % 2 == 0)\n        sample[\'float64\'] *= 2\n        return sample\n\n    with make_batch_reader(scalar_dataset.url, transform_spec=TransformSpec(double_float64),\n                           predicate=in_lambda([\'id\'], lambda id: id % 2 == 0)) as reader:\n        actual = next(reader)\n        for actual_id, actual_float64 in zip(actual.id, actual.float64):\n            assert actual_id % 2 == 0\n            original_sample = next(d for d in scalar_dataset.data if d[\'id\'] == actual_id)\n            expected_matrix = original_sample[\'float64\'] * 2\n            np.testing.assert_equal(expected_matrix, actual_float64)\n\n\ndef test_simple_read_with_pyarrow_serialize(synthetic_dataset):\n    """"""Same as test_simple_read, but don\'t check type correctness as pyarrow_serialize messes up integer types""""""\n    with make_reader(synthetic_dataset.url, reader_pool_type=\'process\', workers_count=1,\n                     pyarrow_serialize=True) as reader:\n        _check_simple_reader(reader, synthetic_dataset.data, check_types=False)\n\n\n@pytest.mark.parametrize(\'reader_factory\', ALL_READER_FLAVOR_FACTORIES + SCALAR_ONLY_READER_FACTORIES)\n@pytest.mark.forked\ndef test_simple_read_with_disk_cache(synthetic_dataset, reader_factory, tmpdir):\n    """"""Try using the Reader with LocalDiskCache using different flavors of pools""""""\n    CACHE_SIZE = 10 * 2 ** 30  # 20GB\n    ROW_SIZE_BYTES = 100  # not really important for this test\n    with reader_factory(synthetic_dataset.url, num_epochs=2,\n                        cache_type=\'local-disk\', cache_location=tmpdir.strpath,\n                        cache_size_limit=CACHE_SIZE, cache_row_size_estimate=ROW_SIZE_BYTES) as reader:\n        ids = _readout_all_ids(reader)\n        assert 200 == len(ids)  # We read 2 epochs\n        assert set(ids) == set(range(100))\n\n\n@pytest.mark.parametrize(\'reader_factory\', MINIMAL_READER_FLAVOR_FACTORIES + SCALAR_ONLY_READER_FACTORIES)\ndef test_simple_read_with_added_slashes(synthetic_dataset, reader_factory):\n    """"""Tests that using relative paths for the dataset metadata works as expected""""""\n    with reader_factory(synthetic_dataset.url + \'///\') as reader:\n        next(reader)\n\n\n@pytest.mark.parametrize(\'reader_factory\', MINIMAL_READER_FLAVOR_FACTORIES + SCALAR_ONLY_READER_FACTORIES)\ndef test_simple_read_moved_dataset(synthetic_dataset, tmpdir, reader_factory):\n    """"""Tests that a dataset may be opened after being moved to a new location""""""\n    a_moved_path = tmpdir.join(\'moved\').strpath\n    copytree(synthetic_dataset.path, a_moved_path)\n\n    with reader_factory(\'file://{}\'.format(a_moved_path)) as reader:\n        next(reader)\n\n    rmtree(a_moved_path)\n\n\n@pytest.mark.parametrize(\'reader_factory\', MINIMAL_READER_FLAVOR_FACTORIES)\ndef test_reading_subset_of_columns(synthetic_dataset, reader_factory):\n    """"""Just a bunch of read and compares of all values to the expected values""""""\n    with reader_factory(synthetic_dataset.url, schema_fields=[TestSchema.id2, TestSchema.id]) as reader:\n        # Read a bunch of entries from the dataset and compare the data to reference\n        for row in reader:\n            actual = dict(row._asdict())\n            expected = next(d for d in synthetic_dataset.data if d[\'id\'] == actual[\'id\'])\n            np.testing.assert_equal(expected[\'id2\'], actual[\'id2\'])\n\n\n@pytest.mark.parametrize(\'reader_factory\', MINIMAL_READER_FLAVOR_FACTORIES)\ndef test_reading_subset_of_columns_using_regex(synthetic_dataset, reader_factory):\n    """"""Just a bunch of read and compares of all values to the expected values""""""\n    with reader_factory(synthetic_dataset.url, schema_fields=[\'id$\', \'id_.*$\', \'partition_key$\']) as reader:\n        # Read a bunch of entries from the dataset and compare the data to reference\n        for row in reader:\n            actual = dict(row._asdict())\n            assert set(actual.keys()) == {\'id_float\', \'id_odd\', \'id\', \'partition_key\'}\n            expected = next(d for d in synthetic_dataset.data if d[\'id\'] == actual[\'id\'])\n            np.testing.assert_equal(expected[\'id_float\'], actual[\'id_float\'])\n\n\n@pytest.mark.parametrize(\'reader_factory\', [\n    lambda url, **kwargs: make_reader(url, reader_pool_type=\'dummy\', **kwargs),\n    lambda url, **kwargs: make_batch_reader(url, reader_pool_type=\'dummy\', **kwargs)])\ndef test_shuffle(synthetic_dataset, reader_factory):\n    rows_count = len(synthetic_dataset.data)\n\n    # Read ids twice without shuffle: assert we have the same array and all expected ids are in the array\n    with reader_factory(synthetic_dataset.url, shuffle_row_groups=False) as reader_1:\n        first_readout = _readout_all_ids(reader_1)\n    with reader_factory(synthetic_dataset.url, shuffle_row_groups=False) as reader_2:\n        second_readout = _readout_all_ids(reader_2)\n\n    np.testing.assert_array_equal(range(rows_count), sorted(first_readout))\n    np.testing.assert_array_equal(first_readout, second_readout)\n\n    # Now read with shuffling\n    with reader_factory(synthetic_dataset.url, shuffle_row_groups=True) as shuffled_reader:\n        shuffled_readout = _readout_all_ids(shuffled_reader)\n    assert np.any(np.not_equal(first_readout, shuffled_readout))\n\n\n@pytest.mark.parametrize(\'reader_factory\', [\n    lambda url, **kwargs: make_reader(url, reader_pool_type=\'dummy\', **kwargs),\n    lambda url, **kwargs: make_batch_reader(url, reader_pool_type=\'dummy\', **kwargs)])\ndef test_shuffle_drop_ratio(synthetic_dataset, reader_factory):\n    # Read ids twice without shuffle: assert we have the same array and all expected ids are in the array\n    with reader_factory(synthetic_dataset.url, shuffle_row_groups=False, shuffle_row_drop_partitions=1) as reader:\n        first_readout = _readout_all_ids(reader)\n    np.testing.assert_array_equal([r[\'id\'] for r in synthetic_dataset.data], sorted(first_readout))\n\n    # Test that the ids are increasingly not consecutive numbers as we increase the shuffle dropout\n    prev_jumps_not_1 = 0\n    for shuffle_dropout in [2, 5, 8]:\n        with reader_factory(synthetic_dataset.url, shuffle_row_groups=True,\n                            shuffle_row_drop_partitions=shuffle_dropout) as reader:\n            readout = _readout_all_ids(reader)\n\n        assert len(first_readout) == len(readout)\n        jumps_not_1 = np.sum(np.diff(readout) != 1)\n        assert jumps_not_1 > prev_jumps_not_1\n        prev_jumps_not_1 = jumps_not_1\n\n\n@pytest.mark.parametrize(\'reader_factory\', ALL_READER_FLAVOR_FACTORIES)\ndef test_predicate_on_partition(synthetic_dataset, reader_factory):\n    for expected_partition_keys in [{\'p_0\', \'p_2\'}, {\'p_0\'}, {\'p_1\', \'p_2\'}]:\n        with reader_factory(synthetic_dataset.url,\n                            predicate=PartitionKeyInSetPredicate(expected_partition_keys)) as reader:\n            partition_keys = set(row.partition_key for row in reader)\n            assert partition_keys == expected_partition_keys\n\n\n@pytest.mark.parametrize(\'reader_factory\', MINIMAL_READER_FLAVOR_FACTORIES)\ndef test_predicate_on_partition_filters_out_everything(synthetic_dataset, reader_factory):\n    with pytest.warns(UserWarning, match=\'No matching data is available for loading\'):\n        # This predicate should filter out all rowgroups. We should raise an error in this case.\n        make_reader(synthetic_dataset.url, reader_pool_type=\'dummy\',\n                    predicate=PartitionKeyInSetPredicate({\'non existing value\'}))\n\n\n@pytest.mark.parametrize(\'reader_factory\', MINIMAL_READER_FLAVOR_FACTORIES)\ndef test_too_many_shards(synthetic_dataset, reader_factory):\n    with pytest.raises(NoDataAvailableError, match=\'Number of row-groups in the dataset\'):\n        # If number of shards is greater than number of rowgroups, users might be surprised if a reader\n        # does not produce any error, hence we raise an explicit exception\n        make_reader(synthetic_dataset.url, reader_pool_type=\'dummy\', cur_shard=0, shard_count=10000000)\n\n\n@pytest.mark.parametrize(\'reader_factory\', SCALAR_ONLY_READER_FACTORIES)\ndef test_predicate_on_partition_batched(synthetic_dataset, reader_factory):\n    for expected_partition_keys in [{\'p_0\', \'p_2\'}, {\'p_0\'}, {\'p_1\', \'p_2\'}]:\n        # TODO(yevgeni): scalar only reader takes \'vectorized\' predicate that processes entire columns. Not\n        # yet implemented for the case of a prediction on partition, hence we use a non-vectorized\n        # PartitionKeyInSetPredicate here\n        with reader_factory(synthetic_dataset.url,\n                            predicate=PartitionKeyInSetPredicate(expected_partition_keys)) as reader:\n            partition_keys = set()\n            for row in reader:\n                partition_keys |= set(row.partition_key)\n            assert partition_keys == expected_partition_keys\n\n\n@pytest.mark.parametrize(\'reader_factory\', ALL_READER_FLAVOR_FACTORIES)\ndef test_predicate_on_multiple_fields(synthetic_dataset, reader_factory):\n    expected_values = {\'id\': 11, \'id2\': 1}\n    with reader_factory(synthetic_dataset.url, shuffle_row_groups=False,\n                        predicate=EqualPredicate(expected_values)) as reader:\n        actual = next(reader)\n        assert actual.id == expected_values[\'id\']\n        assert actual.id2 == expected_values[\'id2\']\n\n\n@pytest.mark.parametrize(\'reader_factory\', SCALAR_ONLY_READER_FACTORIES)\ndef test_predicate_on_multiple_fields_batched(synthetic_dataset, reader_factory):\n    expected_values = {\'id\': 11, \'id2\': 1}\n    with reader_factory(synthetic_dataset.url, shuffle_row_groups=False,\n                        predicate=VectorizedEqualPredicate(expected_values)) as reader:\n        actual = next(reader)\n        assert actual.id.shape == (1,)\n        assert actual.id[0] == expected_values[\'id\']\n        assert actual.id2[0] == expected_values[\'id2\']\n\n\n@pytest.mark.parametrize(\'reader_factory\', MINIMAL_READER_FLAVOR_FACTORIES + SCALAR_ONLY_READER_FACTORIES)\ndef test_predicate_with_invalid_fields(synthetic_dataset, reader_factory):\n    """"""Try passing an invalid field name from a predicate to the reader. An error should be raised.""""""\n    TEST_CASES = [\n        {\'invalid_field_name\': 1},\n        dict(),\n        {\'invalid_field_name\': 1, \'id\': 11},\n        {\'invalid_field_name\': 1, \'invalid_field_name_2\': 11}]\n\n    for predicate_spec in TEST_CASES:\n        with reader_factory(synthetic_dataset.url, shuffle_row_groups=False,\n                            predicate=EqualPredicate(predicate_spec)) as reader:\n            with pytest.raises(ValueError):\n                next(reader)\n\n\n@pytest.mark.parametrize(\'reader_factory\', MINIMAL_READER_FLAVOR_FACTORIES + SCALAR_ONLY_READER_FACTORIES)\ndef test_partition_multi_node(synthetic_dataset, reader_factory):\n    """"""Tests that the reader only returns half of the expected data consistently""""""\n    with reader_factory(synthetic_dataset.url, cur_shard=0, shard_count=5) as reader:\n        with reader_factory(synthetic_dataset.url, cur_shard=0, shard_count=5) as reader_2:\n            results_1 = set(_readout_all_ids(reader))\n            results_2 = set(_readout_all_ids(reader_2))\n\n            assert results_1, \'Non empty shard expected\'\n\n            np.testing.assert_equal(results_1, results_2)\n\n            assert len(results_1) < len(synthetic_dataset.data)\n\n            # Test that separate partitions also have no overlap by checking ids)\n            for partition in range(1, 5):\n                with reader_factory(synthetic_dataset.url, cur_shard=partition,\n                                    shard_count=5) as reader_other:\n                    ids_in_other_partition = set(_readout_all_ids(reader_other))\n\n                    assert not ids_in_other_partition.intersection(results_1)\n\n\n@pytest.mark.parametrize(\'reader_factory\', MINIMAL_READER_FLAVOR_FACTORIES + SCALAR_ONLY_READER_FACTORIES)\ndef test_partition_value_error(synthetic_dataset, reader_factory):\n    """"""Tests that the reader raises value errors when appropriate""""""\n\n    # shard_count has to be greater than 0\n    with pytest.raises(ValueError):\n        reader_factory(synthetic_dataset.url, shard_count=0)\n\n    # missing cur_shard value\n    with pytest.raises(ValueError):\n        reader_factory(synthetic_dataset.url, shard_count=5)\n\n    # cur_shard is a string\n    with pytest.raises(ValueError):\n        reader_factory(synthetic_dataset.url, cur_shard=\'0\',\n                       shard_count=5)\n\n    # shard_count is a string\n    with pytest.raises(ValueError):\n        reader_factory(synthetic_dataset.url, cur_shard=0,\n                       shard_count=\'5\')\n\n\n@pytest.mark.parametrize(\'reader_factory\', [\n    lambda url, **kwargs: make_reader(url, reader_pool_type=\'dummy\', **kwargs),\n    lambda url, **kwargs: make_batch_reader(url, reader_pool_type=\'dummy\', **kwargs),\n])\ndef test_stable_pieces_order(synthetic_dataset, reader_factory):\n    """"""Tests that the reader raises value errors when appropriate""""""\n\n    RERUN_THE_TEST_COUNT = 4\n    baseline_run = None\n    for _ in range(RERUN_THE_TEST_COUNT):\n        # TODO(yevgeni): factor out. Reading all ids appears multiple times in this test.\n        with reader_factory(synthetic_dataset.url, shuffle_row_groups=False) as reader:\n            this_run = _readout_all_ids(reader)\n\n        if baseline_run:\n            assert this_run == baseline_run\n\n        baseline_run = this_run\n\n\n@pytest.mark.parametrize(\'reader_factory\', MINIMAL_READER_FLAVOR_FACTORIES)\ndef test_invalid_schema_field(synthetic_dataset, reader_factory):\n    # Let\'s assume we are selecting columns using a schema which is different from the one\n    # stored in the dataset. Would expect to get a reasonable error message\n    BogusSchema = Unischema(\'BogusSchema\', [\n        UnischemaField(\'partition_key\', np.string_, (), ScalarCodec(StringType()), False),\n        UnischemaField(\'id\', np.int64, (), ScalarCodec(LongType()), False),\n        UnischemaField(\'bogus_key\', np.int32, (), ScalarCodec(ShortType()), False)])\n\n    expected_values = {\'bogus_key\': 11, \'id\': 1}\n    with pytest.raises(ValueError, match=\'bogus_key\'):\n        reader_factory(synthetic_dataset.url, schema_fields=BogusSchema.fields.values(),\n                       shuffle_row_groups=False,\n                       predicate=EqualPredicate(expected_values))\n\n\n@pytest.mark.parametrize(\'reader_factory\', MINIMAL_READER_FLAVOR_FACTORIES)\ndef test_use_persisted_codec_and_not_provided_by_user(synthetic_dataset, reader_factory):\n    """"""In order to start using new codec for some field while maintain the ability to read old datasets that were\n    written using an old codec, we need to make sure we are using stored UnischemaField.codec object (that contains\n    an old codec/shape).""""""\n    new_unischema_instance = UnischemaField(\'matrix_uint16\', np.uint16, (2, 3, 4), CompressedImageCodec(\'png\'), False)\n\n    with reader_factory(synthetic_dataset.url, schema_fields=[new_unischema_instance]) as reader:\n        row = next(reader)\n    assert row.matrix_uint16.shape == (32, 16, 3)\n\n\n@pytest.mark.parametrize(\'reader_factory\', MINIMAL_READER_FLAVOR_FACTORIES)\ndef test_single_column_predicate(synthetic_dataset, reader_factory):\n    """"""Test quering a single column with a predicate on the same column """"""\n    with reader_factory(synthetic_dataset.url, schema_fields=[TestSchema.id], predicate=EqualPredicate({\'id\': 1})) \\\n            as reader:\n        all_rows = list(reader)\n        assert 1 == len(all_rows)\n        assert 1 == all_rows[0].id\n\n\n@pytest.mark.parametrize(\'reader_factory\', MINIMAL_READER_FLAVOR_FACTORIES)\ndef test_two_column_predicate(synthetic_dataset, reader_factory):\n    """"""Test quering a single column with a predicate on the same column """"""\n    with reader_factory(synthetic_dataset.url, schema_fields=[TestSchema.id2, TestSchema.partition_key],\n                        predicate=EqualPredicate({\'id2\': 1, \'partition_key\': \'p_2\'})) as reader:\n        all_rows = list(reader)\n        all_id2 = np.array(list(map(operator.attrgetter(\'id2\'), all_rows)))\n        all_partition_key = np.array(list(map(operator.attrgetter(\'partition_key\'), all_rows)))\n        assert (all_id2 == 1).all()\n        assert (all_partition_key == \'p_2\').all()\n\n\n@pytest.mark.parametrize(\'reader_factory\',\n                         [lambda url, **kwargs: make_reader(url, reader_pool_type=\'dummy\', **kwargs),\n                          lambda url, **kwargs: make_reader(url, reader_pool_type=\'thread\', **kwargs),\n                          lambda url, **kwargs: make_reader(url, reader_pool_type=\'process\', **kwargs)])\ndef test_multiple_epochs(synthetic_dataset, reader_factory):\n    """"""Tests that multiple epochs works as expected""""""\n    num_epochs = 5\n    with reader_factory(synthetic_dataset.url, num_epochs=num_epochs) as reader:\n        # Read all expected entries from the dataset and compare the data to reference\n        single_epoch_id_set = [d[\'id\'] for d in synthetic_dataset.data]\n        actual_ids_in_all_epochs = _readout_all_ids(reader)\n        np.testing.assert_equal(sorted(actual_ids_in_all_epochs), sorted(num_epochs * single_epoch_id_set))\n\n        # Reset reader should reset ventilator. Should produce another `num_epochs` results\n        reader.reset()\n        actual_ids_in_all_epochs = _readout_all_ids(reader)\n        np.testing.assert_equal(sorted(actual_ids_in_all_epochs), sorted(num_epochs * single_epoch_id_set))\n\n\n@pytest.mark.parametrize(\'reader_factory\',\n                         [lambda url, **kwargs: make_reader(url, reader_pool_type=\'dummy\', **kwargs),\n                          lambda url, **kwargs: make_reader(url, reader_pool_type=\'thread\', **kwargs),\n                          lambda url, **kwargs: make_reader(url, reader_pool_type=\'process\', **kwargs)])\ndef test_fail_if_resetting_in_the_middle_of_epoch(synthetic_dataset, reader_factory):\n    """"""Tests that multiple epochs works as expected""""""\n    num_epochs = 5\n    with reader_factory(synthetic_dataset.url, num_epochs=num_epochs) as reader:\n        # Read all expected entries from the dataset and compare the data to reference\n        actual_ids = _readout_all_ids(reader, limit=20)\n        assert len(actual_ids) == 20\n\n        with pytest.raises(NotImplementedError):\n            reader.reset()\n\n\n# TODO(yevgeni) this test is broken for reader_v2\n@pytest.mark.parametrize(\'reader_factory\', [MINIMAL_READER_FLAVOR_FACTORIES[0]] + SCALAR_ONLY_READER_FACTORIES)\ndef test_unlimited_epochs(synthetic_dataset, reader_factory):\n    """"""Tests that unlimited epochs works as expected""""""\n    with reader_factory(synthetic_dataset.url, num_epochs=None) as reader:\n        read_limit = len(synthetic_dataset.data) * 3 + 2\n        actual_ids = _readout_all_ids(reader, read_limit)\n        expected_ids = [d[\'id\'] for d in synthetic_dataset.data]\n        assert len(actual_ids) > len(expected_ids)\n        assert set(actual_ids) == set(expected_ids)\n\n\n@pytest.mark.parametrize(\'reader_factory\', MINIMAL_READER_FLAVOR_FACTORIES + SCALAR_ONLY_READER_FACTORIES)\ndef test_num_epochs_value_error(synthetic_dataset, reader_factory):\n    """"""Tests that the reader raises value errors when appropriate""""""\n\n    # Testing only Reader v1, as the v2 uses an epoch generator. The error would raise only when the generator is\n    # evaluated. Parameter validation for Reader v2 is covered by test_epoch_generator.py\n\n    with pytest.raises(ValueError):\n        reader_factory(synthetic_dataset.url, num_epochs=-10)\n\n    with pytest.raises(ValueError):\n        reader_factory(synthetic_dataset.url, num_epochs=\'abc\')\n\n\n@pytest.mark.parametrize(\'reader_factory\', MINIMAL_READER_FLAVOR_FACTORIES)\ndef test_rowgroup_selector_integer_field(synthetic_dataset, reader_factory):\n    """""" Select row groups to read based on dataset index for integer field""""""\n    with reader_factory(synthetic_dataset.url, rowgroup_selector=SingleIndexSelector(TestSchema.id.name, [2, 18])) \\\n            as reader:\n        status = [False, False]\n        count = 0\n        for row in reader:\n            if row.id == 2:\n                status[0] = True\n            if row.id == 18:\n                status[1] = True\n            count += 1\n        # both id values in reader result\n        assert all(status)\n        # read only 2 row groups, 100 rows per row group\n        assert 20 == count\n\n\n@pytest.mark.parametrize(\'reader_factory\', MINIMAL_READER_FLAVOR_FACTORIES)\ndef test_rowgroup_selector_string_field(synthetic_dataset, reader_factory):\n    """""" Select row groups to read based on dataset index for string field""""""\n    with reader_factory(synthetic_dataset.url,\n                        rowgroup_selector=SingleIndexSelector(TestSchema.sensor_name.name, [\'test_sensor\'])) as reader:\n        count = sum(1 for _ in reader)\n\n        # Since we use artificial dataset all sensors have the same name,\n        # so all row groups should be selected and all 1000 generated rows should be returned\n        assert 100 == count\n\n\n@pytest.mark.parametrize(\'reader_factory\', MINIMAL_READER_FLAVOR_FACTORIES)\ndef test_rowgroup_selector_multiple_fields_intersection(synthetic_dataset, reader_factory):\n    intersect_index_selector = IntersectIndexSelector(\n        [SingleIndexSelector(TestSchema.sensor_name.name, [\'test_sensor\']),\n         SingleIndexSelector(TestSchema.id.name, [2, 18])]\n    )\n    with reader_factory(synthetic_dataset.url,\n                        rowgroup_selector=intersect_index_selector) as reader:\n        count = 0\n        status = [False, False, False]\n        for row in reader:\n            if row.id == 2:\n                status[0] = True\n            if row.id == 18:\n                status[1] = True\n            if row.sensor_name == \'test_sensor\':\n                status[2] = True\n            count += 1\n        assert all(status)\n        assert 20 == count\n\n\n@pytest.mark.parametrize(\'reader_factory\', MINIMAL_READER_FLAVOR_FACTORIES)\ndef test_rowgroup_selector_multiple_fields_union(synthetic_dataset, reader_factory):\n    union_index_selector = UnionIndexSelector(\n        [SingleIndexSelector(TestSchema.sensor_name.name, [\'test_sensor\']),\n         SingleIndexSelector(TestSchema.id.name, [2, 18])]\n    )\n    with reader_factory(synthetic_dataset.url,\n                        rowgroup_selector=union_index_selector) as reader:\n        count = 0\n        status = [False, False, False]\n        for row in reader:\n            if row.id == 2:\n                status[0] = True\n            if row.id == 18:\n                status[1] = True\n            if row.sensor_name == \'test_sensor\':\n                status[2] = True\n            count += 1\n        assert all(status)\n        assert 100 == count\n\n\n@pytest.mark.parametrize(\'reader_factory\', MINIMAL_READER_FLAVOR_FACTORIES)\ndef test_rowgroup_selector_nullable_array_field(synthetic_dataset, reader_factory):\n    """""" Select row groups to read based on dataset index for array field""""""\n    with reader_factory(synthetic_dataset.url,\n                        rowgroup_selector=SingleIndexSelector(TestSchema.string_array_nullable.name,\n                                                              [\'100\'])) as reader:\n        count = sum(1 for _ in reader)\n        # This field contain id string, generated like this\n        #   None if id % 5 == 0 else np.asarray([], dtype=np.string_) if id % 4 == 0 else\n        #   np.asarray([str(i+id) for i in xrange(2)], dtype=np.string_)\n        # hence \'100\' could be present in row id 99 as 99+1 and row id 100 as 100+0\n        # but row 100 will be skipped by \' None if id % 5 == 0\' condition, so only one row group should be selected\n        assert 10 == count\n\n\n@pytest.mark.parametrize(\'reader_factory\', MINIMAL_READER_FLAVOR_FACTORIES)\ndef test_rowgroup_selector_partition_key(synthetic_dataset, reader_factory):\n    """""" Select row groups to read based on dataset index for array field""""""\n    with reader_factory(synthetic_dataset.url,\n                        rowgroup_selector=SingleIndexSelector(TestSchema.partition_key.name,\n                                                              [\'p_1\'])) as reader:\n        count = sum(1 for _ in reader)\n        assert 10 == count\n\n\n@pytest.mark.parametrize(\'reader_factory\', MINIMAL_READER_FLAVOR_FACTORIES)\ndef test_rowgroup_selector_wrong_index_name(synthetic_dataset, reader_factory):\n    """""" Attempt to select row groups to based on wrong dataset index,\n        Reader should raise exception\n    """"""\n    with pytest.raises(ValueError):\n        reader_factory(synthetic_dataset.url, rowgroup_selector=SingleIndexSelector(\'WrongIndexName\', [\'some_value\']))\n\n\ndef test_materialize_dataset_hadoop_config(tmpdir_factory):\n    """"""Test that using materialize_dataset does not alter the hadoop_config""""""\n\n    path = tmpdir_factory.mktemp(\'data\').strpath\n    tmp_url = ""file://"" + path\n    # This test does not properly check if parquet.enable.summary-metadata is restored properly with pyspark < 2.4\n    spark = SparkSession.builder.getOrCreate()\n    hadoop_config = spark.sparkContext._jsc.hadoopConfiguration()\n\n    parquet_metadata_level = ""COMMON_ONLY""\n    parquet_row_group_check = 100\n\n    # Set the parquet summary files and row group size check min\n    hadoop_config.set(\'parquet.summary.metadata.level\', parquet_metadata_level)\n    hadoop_config.setInt(\'parquet.row-group.size.row.check.min\', parquet_row_group_check)\n    assert hadoop_config.get(\'parquet.summary.metadata.level\') == str(parquet_metadata_level)\n    assert hadoop_config.get(\'parquet.row-group.size.row.check.min\') == str(parquet_row_group_check)\n\n    create_test_dataset(tmp_url, range(10), spark=spark)\n\n    assert not os.path.exists(os.path.join(path, ""_metadata""))\n\n    # Check that they are back to the original values after writing the dataset\n    hadoop_config = spark.sparkContext._jsc.hadoopConfiguration()\n    assert hadoop_config.get(\'parquet.summary.metadata.level\') == str(parquet_metadata_level)\n    assert hadoop_config.get(\'parquet.row-group.size.row.check.min\') == str(parquet_row_group_check)\n    # Other options should return to being unset\n    assert hadoop_config.get(\'parquet.block.size\') is None\n    assert hadoop_config.get(\'parquet.block.size.row.check.min\') is None\n    spark.stop()\n\n\ndef test_materialize_with_summary_metadata(tmpdir_factory):\n    """"""Verify _summary_metadata appears, when requested""""""\n    path = tmpdir_factory.mktemp(\'data\').strpath\n    tmp_url = ""file://"" + path\n\n    spark = SparkSession.builder.getOrCreate()\n    create_test_dataset(tmp_url, range(10), spark=spark, use_summary_metadata=True)\n\n    assert os.path.exists(os.path.join(path, ""_metadata""))\n    spark.stop()\n\n\ndef test_pass_in_pyarrow_filesystem_to_materialize_dataset(synthetic_dataset, tmpdir):\n    a_moved_path = tmpdir.join(\'moved\').strpath\n    copytree(synthetic_dataset.path, a_moved_path)\n\n    local_fs = pyarrow.LocalFileSystem\n    os.remove(a_moved_path + \'/_common_metadata\')\n\n    spark = SparkSession.builder.getOrCreate()\n\n    with materialize_dataset(spark, a_moved_path, TestSchema, filesystem_factory=local_fs):\n        pass\n\n    with make_reader(\'file://{}\'.format(a_moved_path), reader_pool_type=\'dummy\') as reader:\n        _check_simple_reader(reader, synthetic_dataset.data)\n\n    spark.stop()\n    rmtree(a_moved_path)\n\n\n@pytest.mark.parametrize(\'reader_factory\', MINIMAL_READER_FLAVOR_FACTORIES + SCALAR_ONLY_READER_FACTORIES)\ndef test_dataset_path_is_a_unicode(synthetic_dataset, reader_factory):\n    """"""Just a bunch of read and compares of all values to the expected values using the different reader pools""""""\n    # Making sure unicode_in_p23 is a unicode both in python 2 and 3\n    unicode_in_p23 = synthetic_dataset.url.encode().decode(\'utf-8\')\n    with reader_factory(unicode_in_p23) as reader:\n        next(reader)\n\n\ndef test_make_reader_fails_loading_non_petastrom_dataset(scalar_dataset):\n    with pytest.raises(RuntimeError, match=\'use make_batch_reader\'):\n        make_reader(scalar_dataset.url)\n\n\ndef test_multithreaded_reads(synthetic_dataset):\n    with make_reader(synthetic_dataset.url, workers_count=5, num_epochs=1) as reader:\n        with ThreadPoolExecutor(max_workers=10) as executor:\n            def read_one_row():\n                return next(reader)\n\n            futures = [executor.submit(read_one_row) for _ in range(100)]\n            results = [f.result() for f in futures]\n            assert len(results) == len(synthetic_dataset.data)\n            assert set(r.id for r in results) == set(d[\'id\'] for d in synthetic_dataset.data)\n\n\ndef test_should_fail_if_reading_out_of_context_manager(synthetic_dataset):\n    with make_reader(synthetic_dataset.url, workers_count=1) as reader:\n        next(reader)\n\n    with pytest.raises(RuntimeError, match=\'Trying to read a sample.*\'):\n        next(reader)\n\n\ndef test_should_fail_if_reading_after_stop(synthetic_dataset):\n    reader = make_reader(synthetic_dataset.url, workers_count=1)\n    next(reader)\n    reader.stop()\n\n    with pytest.raises(RuntimeError, match=\'Trying to read a sample.*\'):\n        next(reader)\n\n\ndef _get_local_fs_url_list(dir_url):\n    url_list = []\n    dir_path = urlparse(dir_url).path\n    for file_name in os.listdir(dir_path):\n        url_list.append(\'file://{dir_path}/{file_name}\'.format(dir_path=dir_path, file_name=file_name))\n    return url_list\n\n\ndef test_make_batch_reader_with_url_list(scalar_dataset):\n    url_list = _get_local_fs_url_list(scalar_dataset.url)\n    url_list = list(filter(lambda x: x.endswith(\'.parquet\'), url_list))\n\n    with make_batch_reader(url_list, workers_count=1) as reader:\n        row_count = 0\n        for batch in reader:\n            row_count += len(batch.id)\n\n        assert row_count == 100\n'"
petastorm/tests/test_end_to_end_predicates_impl.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""In order for the predicates to be accessible from a process_pool and the test_end_to_end.py is ran directly as\n__main__, these predicates have to be implemented in a separate module""""""\nfrom petastorm.predicates import PredicateBase\n\n\nclass PartitionKeyInSetPredicate(PredicateBase):\n    def __init__(self, inclusion_values):\n        self._inclusion_values = inclusion_values\n\n    def get_fields(self):\n        return {\'partition_key\'}\n\n    def do_include(self, values):\n        return values[\'partition_key\'] in self._inclusion_values\n\n\nclass EqualPredicate(PredicateBase):\n    def __init__(self, values):\n        self._values = values\n\n    def get_fields(self):\n        return list(self._values.keys())\n\n    def do_include(self, values):\n        return self._values == values\n\n\nclass VectorizedEqualPredicate(PredicateBase):\n    def __init__(self, values):\n        self._values = values\n\n    def get_fields(self):\n        return list(self._values.keys())\n\n    def do_include(self, values):\n        result = [True] * len(values)\n        for field_name in self._values.keys():\n            result &= values[field_name] == self._values[field_name]\n        return result\n'"
petastorm/tests/test_fs_utils.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport unittest\n\nimport dill\nimport mock\nfrom pyarrow.filesystem import LocalFileSystem, S3FSWrapper\nfrom pyarrow.lib import ArrowIOError\nfrom six.moves.urllib.parse import urlparse\n\nfrom petastorm.fs_utils import FilesystemResolver, get_filesystem_and_path_or_paths\nfrom petastorm.gcsfs_helpers.gcsfs_wrapper import GCSFSWrapper\nfrom petastorm.hdfs.tests.test_hdfs_namenode import HC, MockHadoopConfiguration, \\\n    MockHdfs, MockHdfsConnector\n\nABS_PATH = \'/abs/path\'\n\n\nclass FilesystemResolverTest(unittest.TestCase):\n    """"""\n    Checks the full filesystem resolution functionality, exercising each URL interpretation case.\n    """"""\n\n    @classmethod\n    def setUpClass(cls):\n        cls.mock = MockHdfsConnector()\n        cls.mock_name = ""mock-manager""\n\n    def setUp(self):\n        """"""Initializes a mock hadoop config and populate with basic properties.""""""\n        # Reset counters in mock connector\n        self.mock.reset()\n        self._hadoop_configuration = MockHadoopConfiguration()\n        self._hadoop_configuration.set(\'fs.defaultFS\', HC.FS_WARP_TURTLE)\n        self._hadoop_configuration.set(\'dfs.ha.namenodes.{}\'.format(HC.WARP_TURTLE), \'nn2,nn1\')\n        self._hadoop_configuration.set(\'dfs.namenode.rpc-address.{}.nn1\'.format(HC.WARP_TURTLE), HC.WARP_TURTLE_NN1)\n        self._hadoop_configuration.set(\'dfs.namenode.rpc-address.{}.nn2\'.format(HC.WARP_TURTLE), HC.WARP_TURTLE_NN2)\n\n    def test_error_url_cases(self):\n        """"""Various error cases that result in exception raised.""""""\n        # Case 1: Schemeless path asserts\n        with self.assertRaises(ValueError):\n            FilesystemResolver(ABS_PATH, {})\n\n        # Case 4b: HDFS default path case with NO defaultFS\n        with self.assertRaises(RuntimeError):\n            FilesystemResolver(\'hdfs:///some/path\', {})\n\n        # Case 4b: Using `default` as host, while apparently a pyarrow convention, is NOT valid\n        with self.assertRaises(ArrowIOError):\n            FilesystemResolver(\'hdfs://default\', {})\n\n        # Case 5: other schemes result in ValueError; urlparse to cover an else branch!\n        with self.assertRaises(ValueError):\n            FilesystemResolver(urlparse(\'http://foo/bar\'), {})\n        with self.assertRaises(ValueError):\n            FilesystemResolver(urlparse(\'ftp://foo/bar\'), {})\n        with self.assertRaises(ValueError):\n            FilesystemResolver(urlparse(\'ssh://foo/bar\'), {})\n\n        # s3 paths must have the bucket as the netloc\n        with self.assertRaises(ValueError):\n            FilesystemResolver(urlparse(\'s3:///foo/bar\'), {})\n\n        # GCS paths must have the bucket as the netloc\n        with self.assertRaises(ValueError):\n            FilesystemResolver(urlparse(\'gcs:///foo/bar\'), {})\n\n    def test_file_url(self):\n        """""" Case 2: File path, agnostic to content of hadoop configuration.""""""\n        suj = FilesystemResolver(\'file://{}\'.format(ABS_PATH), self._hadoop_configuration, connector=self.mock)\n        self.assertTrue(isinstance(suj.filesystem(), LocalFileSystem))\n        self.assertEqual(\'\', suj.parsed_dataset_url().netloc)\n        self.assertEqual(ABS_PATH, suj.get_dataset_path())\n\n        # Make sure we did not capture FilesystemResolver in a closure by mistake\n        dill.dumps(suj.filesystem_factory())\n\n    def test_hdfs_url_with_nameservice(self):\n        """""" Case 3a: HDFS nameservice.""""""\n        suj = FilesystemResolver(HC.WARP_TURTLE_PATH, self._hadoop_configuration, connector=self.mock,\n                                 user=self.mock_name)\n        self.assertEqual(MockHdfs, type(suj.filesystem()._hdfs))\n        self.assertEqual(self.mock_name, suj.filesystem()._user)\n        self.assertEqual(HC.WARP_TURTLE, suj.parsed_dataset_url().netloc)\n        self.assertEqual(1, self.mock.connect_attempted(HC.WARP_TURTLE_NN2))\n        self.assertEqual(0, self.mock.connect_attempted(HC.WARP_TURTLE_NN1))\n        self.assertEqual(0, self.mock.connect_attempted(HC.DEFAULT_NN))\n\n        # Make sure we did not capture FilesystemResolver in a closure by mistake\n        dill.dumps(suj.filesystem_factory())\n\n    def test_hdfs_url_no_nameservice(self):\n        """""" Case 3b: HDFS with no nameservice should connect to default namenode.""""""\n        suj = FilesystemResolver(\'hdfs:///some/path\', self._hadoop_configuration, connector=self.mock,\n                                 user=self.mock_name)\n        self.assertEqual(MockHdfs, type(suj.filesystem()._hdfs))\n        self.assertEqual(self.mock_name, suj.filesystem()._user)\n        self.assertEqual(HC.WARP_TURTLE, suj.parsed_dataset_url().netloc)\n        # ensure path is preserved in parsed URL\n        self.assertEqual(\'/some/path\', suj.get_dataset_path())\n        self.assertEqual(1, self.mock.connect_attempted(HC.WARP_TURTLE_NN2))\n        self.assertEqual(0, self.mock.connect_attempted(HC.WARP_TURTLE_NN1))\n        self.assertEqual(0, self.mock.connect_attempted(HC.DEFAULT_NN))\n\n        # Make sure we did not capture FilesystemResolver in a closure by mistake\n        dill.dumps(suj.filesystem_factory())\n\n    def test_hdfs_url_direct_namenode(self):\n        """""" Case 4: direct namenode.""""""\n        suj = FilesystemResolver(\'hdfs://{}/path\'.format(HC.WARP_TURTLE_NN1),\n                                 self._hadoop_configuration,\n                                 connector=self.mock,\n                                 user=self.mock_name)\n        self.assertEqual(MockHdfs, type(suj.filesystem()))\n        self.assertEqual(self.mock_name, suj.filesystem()._user)\n        self.assertEqual(HC.WARP_TURTLE_NN1, suj.parsed_dataset_url().netloc)\n        self.assertEqual(0, self.mock.connect_attempted(HC.WARP_TURTLE_NN2))\n        self.assertEqual(1, self.mock.connect_attempted(HC.WARP_TURTLE_NN1))\n        self.assertEqual(0, self.mock.connect_attempted(HC.DEFAULT_NN))\n\n        # Make sure we did not capture FilesystemResolver in a closure by mistake\n        dill.dumps(suj.filesystem_factory())\n\n    def test_hdfs_url_direct_namenode_driver_libhdfs(self):\n        suj = FilesystemResolver(\'hdfs://{}/path\'.format(HC.WARP_TURTLE_NN1),\n                                 self._hadoop_configuration,\n                                 connector=self.mock, hdfs_driver=\'libhdfs\', user=self.mock_name)\n        self.assertEqual(MockHdfs, type(suj.filesystem()))\n        self.assertEqual(self.mock_name, suj.filesystem()._user)\n        # Make sure we did not capture FilesystemResolver in a closure by mistake\n        dill.dumps(suj.filesystem_factory())\n\n    def test_hdfs_url_direct_namenode_retries(self):\n        """""" Case 4: direct namenode fails first two times thru, but 2nd retry succeeds.""""""\n        self.mock.set_fail_n_next_connect(2)\n        with self.assertRaises(ArrowIOError):\n            suj = FilesystemResolver(\'hdfs://{}/path\'.format(HC.WARP_TURTLE_NN2),\n                                     self._hadoop_configuration,\n                                     connector=self.mock, user=self.mock_name)\n        self.assertEqual(1, self.mock.connect_attempted(HC.WARP_TURTLE_NN2))\n        self.assertEqual(0, self.mock.connect_attempted(HC.WARP_TURTLE_NN1))\n        self.assertEqual(0, self.mock.connect_attempted(HC.DEFAULT_NN))\n        with self.assertRaises(ArrowIOError):\n            suj = FilesystemResolver(\'hdfs://{}/path\'.format(HC.WARP_TURTLE_NN2),\n                                     self._hadoop_configuration,\n                                     connector=self.mock)\n        self.assertEqual(2, self.mock.connect_attempted(HC.WARP_TURTLE_NN2))\n        self.assertEqual(0, self.mock.connect_attempted(HC.WARP_TURTLE_NN1))\n        self.assertEqual(0, self.mock.connect_attempted(HC.DEFAULT_NN))\n        # this one should connect ""successfully""\n        suj = FilesystemResolver(\'hdfs://{}/path\'.format(HC.WARP_TURTLE_NN2),\n                                 self._hadoop_configuration,\n                                 connector=self.mock, user=self.mock_name)\n        self.assertEqual(MockHdfs, type(suj.filesystem()))\n        self.assertEqual(self.mock_name, suj.filesystem()._user)\n        self.assertEqual(HC.WARP_TURTLE_NN2, suj.parsed_dataset_url().netloc)\n        self.assertEqual(3, self.mock.connect_attempted(HC.WARP_TURTLE_NN2))\n        self.assertEqual(0, self.mock.connect_attempted(HC.WARP_TURTLE_NN1))\n        self.assertEqual(0, self.mock.connect_attempted(HC.DEFAULT_NN))\n\n    def test_s3_without_s3fs(self):\n        with mock.patch.dict(\'sys.modules\', s3fs=None):\n            # `import s3fs` will fail in this context\n            with self.assertRaises(ValueError):\n                FilesystemResolver(urlparse(\'s3://foo/bar\'), {})\n\n    def test_s3_url(self):\n        suj = FilesystemResolver(\'s3://bucket{}\'.format(ABS_PATH), self._hadoop_configuration, connector=self.mock)\n        self.assertTrue(isinstance(suj.filesystem(), S3FSWrapper))\n        self.assertEqual(\'bucket\', suj.parsed_dataset_url().netloc)\n        self.assertEqual(\'bucket\' + ABS_PATH, suj.get_dataset_path())\n\n        # Make sure we did not capture FilesystemResolver in a closure by mistake\n        dill.dumps(suj.filesystem_factory())\n\n    def test_gcs_without_gcsfs(self):\n        with mock.patch.dict(\'sys.modules\', gcsfs=None):\n            # `import gcsfs` will fail in this context\n            with self.assertRaises(ValueError):\n                FilesystemResolver(urlparse(\'gcs://foo/bar\'), {})\n\n    def test_gcs_url(self):\n        suj = FilesystemResolver(\'gcs://bucket{}\'.format(ABS_PATH), self._hadoop_configuration, connector=self.mock)\n        self.assertTrue(isinstance(suj.filesystem(), GCSFSWrapper))\n        self.assertEqual(\'bucket\', suj.parsed_dataset_url().netloc)\n        self.assertEqual(\'bucket\' + ABS_PATH, suj.get_dataset_path())\n\n        # Make sure we did not capture FilesystemResolver in a closure by mistake\n        dill.dumps(suj.filesystem_factory())\n\n    def test_get_filesystem_and_path_or_paths(self):\n        fs1, path1 = get_filesystem_and_path_or_paths(\'file:///some/path\')\n        assert isinstance(fs1, LocalFileSystem) and path1 == \'/some/path\'\n\n        fs2, paths2 = get_filesystem_and_path_or_paths(\n            [\'file:///some/path/01.parquet\', \'file:///some/path/02.parquet\'])\n        assert isinstance(fs2, LocalFileSystem) and \\\n            paths2 == [\'/some/path/01.parquet\', \'/some/path/02.parquet\']\n\n        with self.assertRaises(ValueError):\n            get_filesystem_and_path_or_paths(\n                [\'file:///some/path/01.parquet\', \'hdfs:///some/path/02.parquet\'])\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
petastorm/tests/test_generate_metadata.py,0,"b'import os\nfrom collections import namedtuple\nfrom shutil import copytree\n\nimport pyarrow.parquet as pq\nimport pytest\n\nfrom petastorm import make_reader\nfrom petastorm.etl import petastorm_generate_metadata\nfrom petastorm.selectors import SingleIndexSelector\nfrom petastorm.tests.test_common import create_test_dataset, TestSchema\n\nROWS_COUNT = 1000\n\nSyntheticDataset = namedtuple(\'synthetic_dataset\', [\'url\', \'data\', \'path\'])\n\n\n@pytest.fixture(scope=""session"")\ndef synthetic_dataset(tmpdir_factory):\n    path = tmpdir_factory.mktemp(\'data\').strpath\n    url = \'file://\' + path\n    data = create_test_dataset(url, range(ROWS_COUNT))\n    return SyntheticDataset(url=url, path=path, data=data)\n\n\ndef _check_reader(path, rowgroup_selector=None):\n    # Just check that you can open and read from a reader successfully\n    with make_reader(\'file://{}\'.format(path), reader_pool_type=\'dummy\', rowgroup_selector=rowgroup_selector) as reader:\n        [next(reader) for _ in range(10)]\n\n\ndef test_regenerate_metadata(synthetic_dataset, tmpdir):\n    a_moved_path = tmpdir.join(\'moved\').strpath\n    copytree(synthetic_dataset.path, a_moved_path)\n\n    # Make sure we can read dataset before\n    _check_reader(a_moved_path)\n\n    # Delete both metadata files\n    dataset = pq.ParquetDataset(a_moved_path)\n    os.remove(dataset.common_metadata_path)\n\n    # make_reader should not be able to read a dataset without Petastorm metadat.\n    with pytest.raises(RuntimeError, match=\'make_reader supports reading only Petastorm datasets\'):\n        _check_reader(a_moved_path)\n\n    # Regenerate all metadata including unischema information\n    petastorm_generate_metadata._main([\n        \'--dataset_url\', \'file://{}\'.format(a_moved_path),\n        \'--unischema_class\', \'petastorm.tests.test_common.TestSchema\',\n    ])\n\n    # Reader should now work again (row group selector will not since we removed all metadata)\n    _check_reader(a_moved_path)\n\n\ndef test_regenerate_using_row_group_summary_metadata(synthetic_dataset, tmpdir):\n    a_moved_path = tmpdir.join(\'moved\').strpath\n    copytree(synthetic_dataset.path, a_moved_path)\n\n    # Make sure we can read dataset before\n    _check_reader(a_moved_path)\n\n    # Regenerate the metadata (taking the schema information from the common_metadata which exists)\n    petastorm_generate_metadata._main([\'--dataset_url\', \'file://{}\'.format(a_moved_path), \'--use-summary-metadata\'])\n\n    dataset = pq.ParquetDataset(a_moved_path)\n    # Metadata path should not exist still (should be only _common_metadata)\n    assert dataset.metadata\n\n    # Reader should now work again with rowgroup selector since it was in original metadata\n    _check_reader(a_moved_path, SingleIndexSelector(TestSchema.id.name, [2, 18]))\n'"
petastorm/tests/test_metadata_read.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport unittest\nfrom shutil import move, rmtree\nfrom tempfile import mkdtemp\n\nfrom petastorm import make_reader\nfrom petastorm.tests.test_common import create_test_dataset\n\n# Tiny count of rows in a fake dataset\nROWS_COUNT = 10\n\n\nclass MetadataUnischemaReadTest(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        """"""Initializes dataset once per test. All tests in this class will use the same fake dataset.""""""\n        # Write a fake dataset to this location\n        cls._dataset_dir = mkdtemp(\'test_metadata_read\')\n        cls._dataset_url = \'file://{}\'.format(cls._dataset_dir)\n        cls._dataset_dicts = create_test_dataset(cls._dataset_url, range(ROWS_COUNT))\n\n    @classmethod\n    def tearDownClass(cls):\n        """""" Remove everything created in setUpClass. """"""\n        rmtree(cls._dataset_dir)\n\n    def vanish_metadata(self, filename=\'_common_metadata\'):\n        """""" Move the already generated _metadata to a different name, leveraging tempdir uniqueness. """"""\n        move(\'{}/{}\'.format(self._dataset_dir, filename), \'{}{}\'.format(self._dataset_dir, filename + \'_gone\'))\n\n    def restore_metadata(self, filename=\'_common_metadata\'):\n        """""" Restore _metadata file for other tests. """"""\n        move(\'{}{}\'.format(self._dataset_dir, filename + \'_gone\'), \'{}/{}\'.format(self._dataset_dir, filename))\n\n    def test_no_common_metadata_crc(self):\n        """"""We add our own entries to the _common_metadata file, unfortunatelly, the .crc file is not updated by\n        current pyarrow implementation, so we delete the .crc to make sure there is no mismatch with the content of\n        _common_metadata file""""""\n        self.assertFalse(os.path.exists(os.path.join(MetadataUnischemaReadTest._dataset_dir, \'._common_metadata.crc\')))\n\n    def test_no_metadata(self):\n        self.vanish_metadata()\n        with self.assertRaises(RuntimeError) as e:\n            make_reader(self._dataset_url, reader_pool_type=\'dummy\')\n        self.assertTrue(\'make_reader supports reading only Petastorm datasets\' in str(e.exception))\n        self.restore_metadata()\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
petastorm/tests/test_ngram.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\n\nfrom petastorm.ngram import NGram\nfrom petastorm.unischema import UnischemaField, Unischema\n\nTestSchema = Unischema(\'TestSchema\', [\n    UnischemaField(\'string\', np.unicode_, (), None, False),\n    UnischemaField(\'int\', np.int32, (), None, False),\n    UnischemaField(\'double\', np.float64, (), None, False),\n])\n\n\ndef test_eq():\n    ngram1 = NGram({-1: [TestSchema.string], 0: [TestSchema.int]}, delta_threshold=1, timestamp_field=TestSchema.int)\n    ngram2 = NGram({-1: [TestSchema.string], 0: [TestSchema.int]}, delta_threshold=1, timestamp_field=TestSchema.int)\n\n    assert ngram1 == ngram1\n    assert ngram1 == ngram2\n\n    assert not ngram1 != ngram1\n    assert not ngram1 != ngram2\n\n    ngram3 = NGram({0: [TestSchema.string], 2: [TestSchema.int]}, delta_threshold=1, timestamp_field=TestSchema.int)\n    assert ngram1 != ngram3\n    assert not ngram1 == ngram3\n\n    ngram4 = NGram({-1: [TestSchema.int], 0: [TestSchema.int]}, delta_threshold=1, timestamp_field=TestSchema.int)\n    assert ngram1 != ngram4\n    assert not ngram1 == ngram4\n'"
petastorm/tests/test_ngram_end_to_end.py,7,"b'# pylint: disable=bad-continuation\n# Disabling lint bad-continuation due to lint issues between python 2.7 and python 3.6\n\n#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom decimal import Decimal\n\nimport numpy as np\nimport pytest\nimport tensorflow.compat.v1 as tf  # pylint: disable=import-error\n\ntry:\n    from mock import mock\nexcept ImportError:\n    from unittest import mock\n\nfrom petastorm import make_reader\nfrom petastorm.ngram import NGram\nfrom petastorm.tests.conftest import SyntheticDataset, maybe_cached_dataset\nfrom petastorm.tests.test_common import create_test_dataset, TestSchema\nfrom petastorm.tests.test_tf_utils import create_tf_graph\nfrom petastorm.tf_utils import tf_tensors\n\n# Tests in this module will run once for each entry in the READER_FACTORIES\n# pylint: disable=unnecessary-lambda\nREADER_FACTORIES = [\n    lambda url, **kwargs: make_reader(url, reader_pool_type=\'dummy\', **kwargs),\n    lambda url, **kwargs: make_reader(url, reader_pool_type=\'process\', workers_count=1, **kwargs),\n]\n\n\n@pytest.fixture(scope=""session"")\ndef dataset_num_files_1(request, tmpdir_factory):\n    def _dataset_generator():\n        path = tmpdir_factory.mktemp(""data"").strpath\n        url = \'file://\' + path\n        data = create_test_dataset(url, range(99), num_files=1)\n        return SyntheticDataset(url=url, path=path, data=data)\n\n    return maybe_cached_dataset(request.config, \'dataset_num_files_1\', _dataset_generator)\n\n\n@pytest.fixture(scope=""session"")\ndef dataset_0_3_8_10_11_20_23(request, tmpdir_factory):\n    def _dataset_generator():\n        path = tmpdir_factory.mktemp(""data"").strpath\n        url = \'file://\' + path\n        ids = [0, 3, 8, 10, 11, 20, 23]\n        data = create_test_dataset(url, ids, num_files=1)\n        return SyntheticDataset(url=url, path=path, data=data)\n\n    return maybe_cached_dataset(request.config, \'dataset_0_3_8_10_11_20_23\', _dataset_generator)\n\n\n@pytest.fixture(scope=""session"")\ndef dataset_range_0_99_5(request, tmpdir_factory):\n    def _dataset_generator():\n        path = tmpdir_factory.mktemp(""data"").strpath\n        url = \'file://\' + path\n        ids = range(0, 99, 5)\n        data = create_test_dataset(url, ids)\n        return SyntheticDataset(url=url, path=path, data=data)\n\n    return maybe_cached_dataset(request.config, \'dataset_range_0_99_5\', _dataset_generator)\n\n\ndef _assert_equal_ngram(actual_ngram, expected_ngram):\n    np.testing.assert_equal(sorted(actual_ngram.keys()), sorted(expected_ngram.keys()))\n    for timestep in actual_ngram:\n        actual_dict = actual_ngram[timestep]._asdict()\n        expected_dict = expected_ngram[timestep]._asdict()\n        np.testing.assert_equal(sorted(list(actual_dict.keys())), sorted(list(expected_dict.keys())))\n        for field_name in actual_dict:\n            actual_field = actual_dict[field_name]\n            expected_field = expected_dict[field_name]\n\n            if isinstance(expected_field, Decimal) or isinstance(expected_field, str):\n                # Tensorflow returns all strings as bytes in python3. So we will need to decode it\n                actual_field = actual_field.decode()\n            elif isinstance(expected_field, np.ndarray) and expected_field.dtype.type == np.unicode_:\n                actual_field = np.array([item.decode() for item in actual_field])\n\n            if isinstance(expected_field, Decimal):\n                np.testing.assert_equal(expected_field, Decimal(actual_field),\n                                        \'{0} field is different\'.format(field_name))\n            else:\n                np.testing.assert_equal(expected_field, actual_field, \'{0} field is different\'.format(field_name))\n\n\ndef _get_named_tuple_from_ngram(ngram, dataset_dicts, starting_index):\n    expected_ngram = {}\n    for index, key in enumerate(range(min(ngram.fields.keys()), max(ngram.fields.keys()) + 1)):\n        if key in ngram.fields:\n            current_field_names = [field.name for field in ngram.fields[key]]\n        else:\n            current_field_names = []\n        new_schema = TestSchema.create_schema_view([\n            TestSchema.fields.get(field) for field in TestSchema.fields if field in current_field_names])\n        current_dict = dataset_dicts[starting_index + index]\n        new_dict = {k: current_dict[k] for k in current_dict if k in current_field_names}\n        expected_ngram[key] = new_schema.make_namedtuple(**new_dict)\n    return expected_ngram\n\n\n@create_tf_graph\ndef _test_continuous_ngram_tf(ngram_fields, dataset_num_files_1, reader_factory):\n    """"""Tests continuous ngram in tf of a certain length. Continuous here refers to\n    that this reader will always return consecutive ngrams due to shuffle being false\n    and partition being 1.\n    """"""\n\n    ngram = NGram(fields=ngram_fields, delta_threshold=10, timestamp_field=TestSchema.id)\n    with reader_factory(dataset_num_files_1.url,\n                        schema_fields=ngram,\n                        shuffle_row_groups=False) as reader:\n\n        readout_examples = tf_tensors(reader)\n\n        # Make sure we have static shape info for all fields\n        for timestep in readout_examples:\n            for field in readout_examples[timestep]:\n                assert field.get_shape().dims is not None\n\n        # Read a bunch of entries from the dataset and compare the data to reference\n        expected_id = 0\n        with tf.Session() as sess:\n            for _ in range(5):\n                actual = sess.run(readout_examples)\n                expected_ngram = _get_named_tuple_from_ngram(ngram, dataset_num_files_1.data, expected_id)\n                _assert_equal_ngram(actual, expected_ngram)\n                expected_id = expected_id + 1\n\n\ndef _test_continuous_ngram(ngram_fields, dataset_num_files_1, reader_factory):\n    """"""Test continuous ngram of a certain length. Continuous here refers to\n    that this reader will always return consecutive ngrams due to shuffle being false\n    and partition being 1.""""""\n\n    ngram = NGram(fields=ngram_fields, delta_threshold=10, timestamp_field=TestSchema.id)\n    with reader_factory(dataset_num_files_1.url, schema_fields=ngram, shuffle_row_groups=False) as reader:\n        expected_id = 0\n\n        for _ in range(ngram.length):\n            actual = next(reader)\n            expected_ngram = _get_named_tuple_from_ngram(ngram, dataset_num_files_1.data, expected_id)\n            np.testing.assert_equal(actual, expected_ngram)\n            expected_id = expected_id + 1\n\n\n@create_tf_graph\ndef _test_noncontinuous_ngram_tf(ngram_fields, synthetic_dataset, reader_factory):\n    """"""Test non continuous ngram in tf of a certain length. Non continuous here refers\n    to that the reader will not necessarily return consecutive ngrams because partition is more\n    than one and false is true.""""""\n\n    dataset_dicts = synthetic_dataset.data\n    ngram = NGram(fields=ngram_fields, delta_threshold=10, timestamp_field=TestSchema.id)\n    reader = reader_factory(synthetic_dataset.url, schema_fields=ngram)\n\n    readout_examples = tf_tensors(reader)\n\n    # Make sure we have static shape info for all fields\n    for timestep in readout_examples:\n        for field in readout_examples[timestep]:\n            assert field.get_shape().dims is not None\n\n    # Read a bunch of entries from the dataset and compare the data to reference\n    with tf.Session() as sess:\n        for _ in range(5):\n            actual = sess.run(readout_examples)\n            expected_ngram = _get_named_tuple_from_ngram(ngram, dataset_dicts, actual[min(actual.keys())].id)\n            _assert_equal_ngram(actual, expected_ngram)\n\n    reader.stop()\n    reader.join()\n\n\ndef _test_noncontinuous_ngram(ngram_fields, synthetic_dataset, reader_factory):\n    """"""Test noncontinuous ngram of a certain length. Non continuous here refers\n    to that the reader will not necessarily return consecutive ngrams because partition is more\n    than one and false is true.""""""\n\n    dataset_dicts = synthetic_dataset.data\n    ngram = NGram(fields=ngram_fields, delta_threshold=10, timestamp_field=TestSchema.id)\n    with reader_factory(synthetic_dataset.url,\n                        schema_fields=ngram,\n                        shuffle_row_groups=True,\n                        shuffle_row_drop_partitions=5) as reader:\n        for _ in range(10):\n            actual = next(reader)\n            expected_ngram = _get_named_tuple_from_ngram(ngram, dataset_dicts, actual[min(actual.keys())].id)\n            np.testing.assert_equal(actual, expected_ngram)\n\n\n@pytest.mark.forked\n@pytest.mark.parametrize(\'reader_factory\', READER_FACTORIES)\ndef test_ngram_basic_tf(dataset_num_files_1, reader_factory):\n    """"""Tests basic ngram with no delta threshold with no shuffle and in the same partition.""""""\n    fields = {\n        -1: [TestSchema.id, TestSchema.id2, TestSchema.image_png, TestSchema.matrix],\n        0: [TestSchema.id, TestSchema.id2, TestSchema.sensor_name],\n    }\n    _test_continuous_ngram_tf(fields, dataset_num_files_1, reader_factory)\n\n\n@pytest.mark.parametrize(\'reader_factory\', READER_FACTORIES)\n@mock.patch(\'petastorm.unischema._UNISCHEMA_FIELD_ORDER\', \'alphabetical\')\ndef test_ngram_basic(dataset_num_files_1, reader_factory):\n    """"""Tests basic ngram with no delta threshold with no shuffle and in the same partition.""""""\n    fields = {\n        -1: [TestSchema.id, TestSchema.id2, TestSchema.image_png, TestSchema.matrix],\n        0: [TestSchema.id, TestSchema.id2, TestSchema.sensor_name],\n    }\n    _test_continuous_ngram(fields, dataset_num_files_1, reader_factory)\n\n\n@pytest.mark.forked\n@pytest.mark.parametrize(\'reader_factory\', READER_FACTORIES)\ndef test_ngram_basic_longer_tf(dataset_num_files_1, reader_factory):\n    """"""Tests basic ngram with no delta threshold with no shuffle and in the same partition.""""""\n    fields = {\n        -2: [TestSchema.id, TestSchema.id2, TestSchema.matrix],\n        -1: [TestSchema.id, TestSchema.id2, TestSchema.image_png],\n        0: [TestSchema.id, TestSchema.id2, TestSchema.decimal],\n        1: [TestSchema.id, TestSchema.id2, TestSchema.sensor_name],\n        2: [TestSchema.id, TestSchema.id2]\n    }\n    _test_continuous_ngram_tf(fields, dataset_num_files_1, reader_factory)\n\n\n@pytest.mark.parametrize(\'reader_factory\', READER_FACTORIES)\n@mock.patch(\'petastorm.unischema._UNISCHEMA_FIELD_ORDER\', \'alphabetical\')\ndef test_ngram_basic_longer(dataset_num_files_1, reader_factory):\n    """"""Tests basic ngram with no delta threshold with no shuffle and in the same partition.""""""\n    fields = {\n        -2: [TestSchema.id, TestSchema.id2, TestSchema.matrix],\n        -1: [TestSchema.id, TestSchema.id2, TestSchema.image_png],\n        0: [TestSchema.id, TestSchema.id2, TestSchema.decimal],\n        1: [TestSchema.id, TestSchema.id2, TestSchema.sensor_name],\n        2: [TestSchema.id, TestSchema.id2]\n    }\n    _test_continuous_ngram(fields, dataset_num_files_1, reader_factory)\n\n\n@pytest.mark.forked\n@pytest.mark.parametrize(\'reader_factory\', READER_FACTORIES)\ndef test_ngram_basic_shuffle_multi_partition_tf(synthetic_dataset, reader_factory):\n    """"""Tests basic ngram with no delta threshold with shuffle and in many partitions.""""""\n    fields = {\n        -1: [TestSchema.id, TestSchema.id2, TestSchema.image_png, TestSchema.matrix],\n        0: [TestSchema.id, TestSchema.id2, TestSchema.sensor_name],\n    }\n    _test_noncontinuous_ngram_tf(fields, synthetic_dataset, reader_factory)\n\n\n@pytest.mark.parametrize(\'reader_factory\', READER_FACTORIES)\n@mock.patch(\'petastorm.unischema._UNISCHEMA_FIELD_ORDER\', \'alphabetical\')\ndef test_ngram_basic_shuffle_multi_partition(synthetic_dataset, reader_factory):\n    """"""Tests basic ngram with no delta threshold with shuffle and in many partitions.""""""\n    fields = {\n        0: [TestSchema.id, TestSchema.id2, TestSchema.image_png, TestSchema.matrix],\n        1: [TestSchema.id, TestSchema.id2, TestSchema.sensor_name],\n    }\n    _test_noncontinuous_ngram(fields, synthetic_dataset, reader_factory)\n\n\n@pytest.mark.forked\n@pytest.mark.parametrize(\'reader_factory\', READER_FACTORIES)\ndef test_ngram_basic_longer_shuffle_multi_partition_tf(synthetic_dataset, reader_factory):\n    """"""Tests basic ngram with no delta threshold with shuffle and in many partitions.""""""\n    fields = {\n        -2: [TestSchema.id, TestSchema.id2, TestSchema.matrix],\n        -1: [TestSchema.id, TestSchema.id2, TestSchema.image_png],\n        0: [TestSchema.id, TestSchema.id2, TestSchema.decimal],\n        1: [TestSchema.id, TestSchema.id2, TestSchema.sensor_name],\n        2: [TestSchema.id, TestSchema.id2]\n    }\n    _test_noncontinuous_ngram_tf(fields, synthetic_dataset, reader_factory)\n\n\n@pytest.mark.parametrize(\'reader_factory\', READER_FACTORIES)\n@mock.patch(\'petastorm.unischema._UNISCHEMA_FIELD_ORDER\', \'alphabetical\')\ndef test_ngram_basic_longer_shuffle_multi_partition(synthetic_dataset, reader_factory):\n    """"""Tests basic ngram with no delta threshold with shuffle and in many partitions.""""""\n    fields = {\n        -5: [TestSchema.id, TestSchema.id2, TestSchema.matrix],\n        -4: [TestSchema.id, TestSchema.id2, TestSchema.image_png],\n        -3: [TestSchema.id, TestSchema.id2, TestSchema.decimal],\n        -2: [TestSchema.id, TestSchema.id2, TestSchema.sensor_name],\n        -1: [TestSchema.id, TestSchema.id2]\n    }\n    _test_noncontinuous_ngram(fields, synthetic_dataset, reader_factory)\n\n\n@pytest.mark.parametrize(\'reader_factory\', READER_FACTORIES)\n@mock.patch(\'petastorm.unischema._UNISCHEMA_FIELD_ORDER\', \'alphabetical\')\ndef test_ngram_basic_longer_no_overlap(synthetic_dataset, reader_factory):\n    """"""Tests basic ngram with no delta threshold with no overlaps of timestamps.""""""\n    fields = {\n        -5: [TestSchema.id, TestSchema.id2, TestSchema.matrix],\n        -4: [TestSchema.id, TestSchema.id2, TestSchema.image_png],\n        -3: [TestSchema.id, TestSchema.id2, TestSchema.decimal],\n        -2: [TestSchema.id, TestSchema.id2, TestSchema.sensor_name],\n        -1: [TestSchema.id, TestSchema.id2]\n    }\n\n    dataset_dicts = synthetic_dataset.data\n    ngram = NGram(fields=fields, delta_threshold=10, timestamp_field=TestSchema.id, timestamp_overlap=False)\n    with reader_factory(synthetic_dataset.url, schema_fields=ngram, shuffle_row_groups=False) as reader:\n        timestamps_seen = set()\n        for actual in reader:\n            expected_ngram = _get_named_tuple_from_ngram(ngram, dataset_dicts, actual[min(actual.keys())].id)\n            np.testing.assert_equal(actual, expected_ngram)\n            for step in actual.values():\n                timestamp = step.id\n                assert timestamp not in timestamps_seen\n                timestamps_seen.add(timestamp)\n\n\n@pytest.mark.forked\n@pytest.mark.parametrize(\'reader_factory\', READER_FACTORIES)\n@create_tf_graph\ndef test_ngram_delta_threshold_tf(dataset_0_3_8_10_11_20_23, reader_factory):\n    """"""Test to verify that delta threshold work as expected in one partition in the same ngram\n    and between consecutive ngrams. delta threshold here refers that each ngram must not be\n    more than delta threshold apart for the field specified by timestamp_field.""""""\n\n    fields = {\n        0: [TestSchema.id, TestSchema.id2, TestSchema.image_png, TestSchema.matrix],\n        1: [TestSchema.id, TestSchema.id2, TestSchema.sensor_name],\n    }\n    ngram = NGram(fields=fields, delta_threshold=4, timestamp_field=TestSchema.id)\n    with reader_factory(\n            dataset_0_3_8_10_11_20_23.url,\n            schema_fields=ngram,\n            shuffle_row_groups=False) as reader:\n\n        # Ngrams expected: (0, 3), (8, 10), (10, 11)\n\n        with tf.Session() as sess:\n            readout = tf_tensors(reader)\n            for timestep in readout:\n                for field in readout[timestep]:\n                    assert field.get_shape().dims is not None\n            first_item = sess.run(readout)\n            expected_item = _get_named_tuple_from_ngram(ngram, dataset_0_3_8_10_11_20_23.data, 0)\n            _assert_equal_ngram(first_item, expected_item)\n\n            readout = tf_tensors(reader)\n            for timestep in readout:\n                for field in readout[timestep]:\n                    assert field.get_shape().dims is not None\n            second_item = sess.run(readout)\n            expected_item = _get_named_tuple_from_ngram(ngram, dataset_0_3_8_10_11_20_23.data, 3)\n            _assert_equal_ngram(second_item, expected_item)\n\n            readout = tf_tensors(reader)\n            for timestep in readout:\n                for field in readout[timestep]:\n                    assert field.get_shape().dims is not None\n            third_item = sess.run(readout)\n            expected_item = _get_named_tuple_from_ngram(ngram, dataset_0_3_8_10_11_20_23.data, 5)\n            _assert_equal_ngram(third_item, expected_item)\n\n            with pytest.raises(tf.errors.OutOfRangeError):\n                sess.run(tf_tensors(reader))\n\n\n@pytest.mark.parametrize(\'reader_factory\', READER_FACTORIES)\n@mock.patch(\'petastorm.unischema._UNISCHEMA_FIELD_ORDER\', \'alphabetical\')\ndef test_ngram_delta_threshold(dataset_0_3_8_10_11_20_23, reader_factory):\n    """"""Test to verify that delta threshold work as expected in one partition in the same ngram\n    and between consecutive ngrams. delta threshold here refers that each ngram must not be\n    more than delta threshold apart for the field specified by timestamp_field.""""""\n\n    fields = {\n        0: [TestSchema.id, TestSchema.id2, TestSchema.image_png, TestSchema.matrix],\n        1: [TestSchema.id, TestSchema.id2, TestSchema.sensor_name],\n    }\n    ngram = NGram(fields=fields, delta_threshold=4, timestamp_field=TestSchema.id)\n    with reader_factory(dataset_0_3_8_10_11_20_23.url, schema_fields=ngram,\n                        shuffle_row_groups=False) as reader:\n        # NGrams expected: (0, 3), (8, 10), (10, 11)\n\n        first_item = next(reader)\n        expected_ngram = _get_named_tuple_from_ngram(ngram, dataset_0_3_8_10_11_20_23.data, 0)\n        np.testing.assert_equal(first_item, expected_ngram)\n\n        second_item = next(reader)\n        expected_ngram = _get_named_tuple_from_ngram(ngram, dataset_0_3_8_10_11_20_23.data, 3)\n        np.testing.assert_equal(second_item, expected_ngram)\n\n        third_item = next(reader)\n        expected_ngram = _get_named_tuple_from_ngram(ngram, dataset_0_3_8_10_11_20_23.data, 5)\n        np.testing.assert_equal(third_item, expected_ngram)\n\n        with pytest.raises(StopIteration):\n            next(reader)\n\n\n@pytest.mark.forked\n@pytest.mark.parametrize(\'reader_factory\', READER_FACTORIES)\n@create_tf_graph\ndef test_ngram_delta_small_threshold_tf(reader_factory, dataset_range_0_99_5):\n    """"""Test to verify that a small threshold work in ngrams.""""""\n\n    fields = {\n        0: [TestSchema.id, TestSchema.id2, TestSchema.image_png, TestSchema.matrix],\n        1: [TestSchema.id, TestSchema.id2, TestSchema.sensor_name],\n    }\n    ngram = NGram(fields=fields, delta_threshold=1, timestamp_field=TestSchema.id)\n    with reader_factory(dataset_range_0_99_5.url, schema_fields=ngram) as reader:\n        with tf.Session() as sess:\n            with pytest.raises(tf.errors.OutOfRangeError):\n                sess.run(tf_tensors(reader))\n\n\n@pytest.mark.parametrize(\'reader_factory\', READER_FACTORIES)\ndef test_ngram_delta_small_threshold(reader_factory, dataset_range_0_99_5):\n    """"""Test to verify that a small threshold work in ngrams.""""""\n\n    fields = {\n        0: [TestSchema.id, TestSchema.id2, TestSchema.image_png, TestSchema.matrix],\n        1: [TestSchema.id, TestSchema.id2, TestSchema.sensor_name],\n    }\n    ngram = NGram(fields=fields, delta_threshold=1, timestamp_field=TestSchema.id)\n    with reader_factory(dataset_range_0_99_5.url, schema_fields=ngram) as reader:\n        with pytest.raises(StopIteration):\n            next(reader)\n\n\ndef test_ngram_validation():\n    """"""Test to verify that ngram validation work as expected.""""""\n\n    fields = {\n        0: [TestSchema.id, TestSchema.id2, TestSchema.image_png, TestSchema.matrix],\n        1: [TestSchema.id, TestSchema.id2, TestSchema.sensor_name],\n    }\n\n    with pytest.raises(ValueError):\n        # delta threshold must be an int\n        NGram(fields=fields, delta_threshold=\'abc\', timestamp_field=TestSchema.id)\n\n    with pytest.raises(ValueError):\n        # timestamp_field must be a field\n        NGram(fields=fields, delta_threshold=5, timestamp_field=5)\n\n    with pytest.raises(ValueError):\n        # Fields must be a dict\n        NGram(fields=[], delta_threshold=5, timestamp_field=TestSchema.id)\n\n    with pytest.raises(ValueError):\n        # Each value in fields must be an array\n        NGram(fields={0: \'test\'}, delta_threshold=5, timestamp_field=TestSchema.id)\n\n    with pytest.raises(ValueError):\n        # timestamp_overlap must be bool\n        NGram(fields=fields, delta_threshold=0.5, timestamp_field=TestSchema.id, timestamp_overlap=2)\n\n    # Check some positive cases\n    NGram(fields=fields, delta_threshold=0.5, timestamp_field=TestSchema.id)\n    NGram(fields=fields, delta_threshold=Decimal(\'0.5\'), timestamp_field=TestSchema.id)\n\n\n@pytest.mark.forked\n@pytest.mark.parametrize(\'reader_factory\', READER_FACTORIES)\n@create_tf_graph\ndef test_ngram_length_1_tf(synthetic_dataset, reader_factory):\n    """"""Test to verify that ngram generalize to support length 1""""""\n    dataset_dicts = synthetic_dataset.data\n    fields = {0: [TestSchema.id, TestSchema.id2]}\n    ngram = NGram(fields=fields, delta_threshold=0.012, timestamp_field=TestSchema.id)\n    reader = reader_factory(synthetic_dataset.url, schema_fields=ngram,\n                            shuffle_row_groups=True, shuffle_row_drop_partitions=5)\n    with tf.Session() as sess:\n        for _ in range(10):\n            actual = sess.run(tf_tensors(reader))\n            expected_ngram = _get_named_tuple_from_ngram(ngram, dataset_dicts, actual[min(actual.keys())].id)\n            _assert_equal_ngram(actual, expected_ngram)\n\n    reader.stop()\n    reader.join()\n\n\n@pytest.mark.parametrize(\'reader_factory\', READER_FACTORIES)\ndef test_ngram_length_1(synthetic_dataset, reader_factory):\n    """"""Test to verify that ngram generalize to support length 1""""""\n    dataset_dicts = synthetic_dataset.data\n    fields = {0: [TestSchema.id, TestSchema.id2]}\n    ngram = NGram(fields=fields, delta_threshold=0.012, timestamp_field=TestSchema.id)\n    with reader_factory(synthetic_dataset.url, schema_fields=ngram,\n                        shuffle_row_groups=True, shuffle_row_drop_partitions=3) as reader:\n        for _ in range(10):\n            actual = next(reader)\n            expected_ngram = _get_named_tuple_from_ngram(ngram, dataset_dicts, actual[min(actual.keys())].id)\n            _assert_equal_ngram(actual, expected_ngram)\n\n\n@pytest.mark.forked\n@pytest.mark.parametrize(\'reader_factory\', READER_FACTORIES)\ndef test_non_consecutive_ngram(dataset_num_files_1, reader_factory):\n    """"""Test to verify that non consecutive keys for fields argument in ngrams work.""""""\n    fields = {\n        -1: [TestSchema.id, TestSchema.id2, TestSchema.image_png, TestSchema.matrix],\n        1: [TestSchema.id, TestSchema.id2, TestSchema.sensor_name],\n    }\n    _test_continuous_ngram_tf(fields, dataset_num_files_1, reader_factory)\n\n\n@pytest.mark.forked\n@pytest.mark.parametrize(\'reader_factory\', READER_FACTORIES)\ndef test_shuffled_fields(dataset_num_files_1, reader_factory):\n    """"""Test to verify not sorted keys for fields argument in ngrams work.""""""\n    fields = {\n        2: [TestSchema.id, TestSchema.id2, TestSchema.image_png, TestSchema.matrix],\n        -1: [TestSchema.id, TestSchema.id2, TestSchema.sensor_name],\n    }\n    _test_continuous_ngram_tf(fields, dataset_num_files_1, reader_factory)\n\n\n@pytest.mark.parametrize(\'reader_factory\', READER_FACTORIES)\ndef test_ngram_shuffle_drop_ratio(synthetic_dataset, reader_factory):\n    """"""Test to verify the shuffle drop ratio work as expected.""""""\n    fields = {\n        -2: [TestSchema.id, TestSchema.id2, TestSchema.matrix],\n        -1: [TestSchema.id, TestSchema.id2, TestSchema.image_png],\n        0: [TestSchema.id, TestSchema.id2, TestSchema.decimal],\n        1: [TestSchema.id, TestSchema.id2, TestSchema.sensor_name],\n        2: [TestSchema.id, TestSchema.id2]\n    }\n    ngram = NGram(fields=fields, delta_threshold=10, timestamp_field=TestSchema.id)\n    with reader_factory(synthetic_dataset.url,\n                        schema_fields=ngram,\n                        shuffle_row_groups=False) as reader:\n        unshuffled = [row[0].id for row in reader]\n    with reader_factory(synthetic_dataset.url,\n                        schema_fields=ngram,\n                        shuffle_row_groups=True,\n                        shuffle_row_drop_partitions=6) as reader:\n        shuffled = [row[0].id for row in reader]\n    assert len(unshuffled) == len(shuffled)\n    assert unshuffled != shuffled\n\n\ndef _test_continuous_ngram_returns(ngram_fields, ts_field, dataset_num_files_1, reader_factory):\n    """"""Test continuous ngram of a certain length. Continuous here refers to\n    that this reader will always return consecutive ngrams due to shuffle being false\n    and partition being 1. Returns the ngram object""""""\n\n    ngram = NGram(fields=ngram_fields, delta_threshold=10, timestamp_field=ts_field)\n    with reader_factory(dataset_num_files_1.url, schema_fields=ngram, shuffle_row_groups=False) as reader:\n        expected_id = 0\n\n        for _ in range(ngram.length):\n            actual = next(reader)\n            expected_ngram = _get_named_tuple_from_ngram(ngram, dataset_num_files_1.data, expected_id)\n            np.testing.assert_equal(actual, expected_ngram)\n            expected_id = expected_id + 1\n\n    return ngram\n\n\n@pytest.mark.parametrize(\'reader_factory\', READER_FACTORIES)\n@mock.patch(\'petastorm.unischema._UNISCHEMA_FIELD_ORDER\', \'alphabetical\')\ndef test_ngram_with_regex_fields(dataset_num_files_1, reader_factory):\n    """"""Tests to verify fields and timestamp field can be regular expressions and work with a reader\n    """"""\n    fields = {\n        -1: [""^id.*$"", ""sensor_name"", TestSchema.partition_key],\n        0: [""^id.*$"", ""sensor_name"", TestSchema.partition_key],\n        1: [""^id.*$"", ""sensor_name"", TestSchema.partition_key]\n    }\n\n    ts_field = \'^id$\'\n\n    expected_fields = [TestSchema.id, TestSchema.id2, TestSchema.id_float, TestSchema.id_odd,\n                       TestSchema.sensor_name, TestSchema.partition_key]\n\n    ngram = _test_continuous_ngram_returns(fields, ts_field, dataset_num_files_1, reader_factory)\n\n    # fields should get resolved after call to a reader\n    ngram_fields = ngram.fields\n\n    # Can\'t do direct set equality between expected fields and ngram.fields b/c of issue\n    # with `Collections.UnischemaField` (see unischema.py for more information). __hash__\n    # and __eq__ is implemented correctly for UnischemaField. However, a collections.UnischemaField\n    # object will not use the __hash__ definied in `petastorm.unischema.py`\n    for k in ngram_fields.keys():\n        assert len(expected_fields) == len(ngram_fields[k])\n\n        for curr_field in expected_fields:\n            assert curr_field in ngram_fields[k]\n\n    assert TestSchema.id == ngram._timestamp_field\n\n\n@pytest.mark.parametrize(\'reader_factory\', READER_FACTORIES)\ndef test_ngram_regex_field_resolve(dataset_num_files_1, reader_factory):\n    """"""Tests ngram.resolve_regex_field_names function\n    """"""\n    fields = {\n        -1: [""^id.*"", ""sensor_name"", TestSchema.partition_key],\n        0: [""^id.*"", ""sensor_name"", TestSchema.partition_key],\n        1: [""^id.*"", ""sensor_name"", TestSchema.partition_key]\n    }\n\n    ts_field = \'^id$\'\n\n    ngram = NGram(fields=fields, delta_threshold=10, timestamp_field=ts_field)\n\n    expected_fields = {TestSchema.id, TestSchema.id2, TestSchema.id_float, TestSchema.id_odd,\n                       TestSchema.sensor_name, TestSchema.partition_key}\n\n    ngram.resolve_regex_field_names(TestSchema)\n\n    ngram_fields = ngram.fields\n\n    # Can\'t do direct set equality between expected fields and ngram.fields b/c of issue\n    # with `Collections.UnischemaField` (see unischema.py for more information). __hash__\n    # and __eq__ is implemented correctly for UnischemaField. However, a collections.UnischemaField\n    # object will not use the __hash__ definied in `petastorm.unischema.py`\n    for k in ngram_fields.keys():\n        assert len(expected_fields) == len(ngram_fields[k])\n\n        for curr_field in expected_fields:\n            assert curr_field in ngram_fields[k]\n\n    assert TestSchema.id == ngram._timestamp_field\n'"
petastorm/tests/test_parquet_reader.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport itertools\n\nimport numpy as np\nimport pandas as pd\nimport pytest\nfrom pyarrow import parquet as pq\n\nfrom petastorm import make_batch_reader\nfrom petastorm.arrow_reader_worker import ArrowReaderWorker\n# pylint: disable=unnecessary-lambda\nfrom petastorm.compat import compat_get_metadata\nfrom petastorm.tests.test_common import create_test_scalar_dataset\nfrom petastorm.transform import TransformSpec\nfrom petastorm.unischema import UnischemaField\n\n_D = [lambda url, **kwargs: make_batch_reader(url, reader_pool_type=\'dummy\', **kwargs)]\n\n# pylint: disable=unnecessary-lambda\n_TP = [\n    lambda url, **kwargs: make_batch_reader(url, reader_pool_type=\'thread\', **kwargs),\n    lambda url, **kwargs: make_batch_reader(url, reader_pool_type=\'process\', **kwargs),\n]\n\n\ndef _check_simple_reader(reader, expected_data):\n    # Read a bunch of entries from the dataset and compare the data to reference\n    expected_field_names = expected_data[0].keys()\n    count = 0\n    for row in reader:\n        actual = row._asdict()\n\n        # Compare value of each entry in the batch\n        for i, id_value in enumerate(actual[\'id\']):\n            expected = next(d for d in expected_data if d[\'id\'] == id_value)\n            for field in expected_field_names:\n                expected_value = expected[field]\n                actual_value = actual[field][i, ...]\n                np.testing.assert_equal(actual_value, expected_value)\n\n        count += len(actual[\'id\'])\n\n    assert count == len(expected_data)\n\n\ndef _get_bad_field_name(field_list):\n    """""" Grab first name from list of valid fields, append random characters to it to get an invalid\n    field name. """"""\n    bad_field = field_list[0]\n    while bad_field in field_list:\n        bad_field += ""VR46""\n    return bad_field\n\n\n@pytest.mark.parametrize(\'reader_factory\', _D + _TP)\ndef test_simple_read(scalar_dataset, reader_factory):\n    """"""Just a bunch of read and compares of all values to the expected values using the different reader pools""""""\n    with reader_factory(scalar_dataset.url) as reader:\n        _check_simple_reader(reader, scalar_dataset.data)\n\n\n@pytest.mark.parametrize(\'reader_factory\', _D)\ndef test_specify_columns_to_read(scalar_dataset, reader_factory):\n    """"""Just a bunch of read and compares of all values to the expected values using the different reader pools""""""\n    with reader_factory(scalar_dataset.url, schema_fields=[\'id\', \'float.*$\']) as reader:\n        sample = next(reader)\n        assert set(sample._asdict().keys()) == {\'id\', \'float64\'}\n        assert sample.float64.size > 0\n\n\n@pytest.mark.parametrize(\'reader_factory\', _D)\ndef test_many_columns_non_petastorm_dataset(many_columns_non_petastorm_dataset, reader_factory):\n    """"""Check if we can read a dataset with huge number of columns (1000 in this case)""""""\n    with reader_factory(many_columns_non_petastorm_dataset.url) as reader:\n        sample = next(reader)\n        assert set(sample._fields) == set(many_columns_non_petastorm_dataset.data[0].keys())\n\n\n# TODO(yevgeni): missing tests: https://github.com/uber/petastorm/issues/257\n\n@pytest.mark.parametrize(\'reader_factory\', _D)\n@pytest.mark.parametrize(\'partition_by\', [[\'string\'], [\'id\'], [\'string\', \'id\']])\ndef test_string_partition(reader_factory, tmpdir, partition_by):\n    """"""Try datasets partitioned by a string, integer and string+integer fields""""""\n    url = \'file://\' + tmpdir.strpath\n\n    data = create_test_scalar_dataset(url, 10, partition_by=partition_by)\n    with reader_factory(url) as reader:\n        row_ids_batched = [row.id for row in reader]\n    actual_row_ids = list(itertools.chain(*row_ids_batched))\n    assert len(data) == len(actual_row_ids)\n\n\n@pytest.mark.parametrize(\'reader_factory\', _D)\ndef test_partitioned_field_is_not_queried(reader_factory, tmpdir):\n    """"""Try datasets partitioned by a string, integer and string+integer fields""""""\n    url = \'file://\' + tmpdir.strpath\n\n    data = create_test_scalar_dataset(url, 10, partition_by=[\'id\'])\n    with reader_factory(url, schema_fields=[\'string\']) as reader:\n        all_rows = list(reader)\n    assert len(data) == len(all_rows)\n    assert all_rows[0]._fields == (\'string\',)\n\n\n@pytest.mark.parametrize(\'reader_factory\', _D)\ndef test_asymetric_parquet_pieces(reader_factory, tmpdir):\n    """"""Check that datasets with parquet files that all rows in datasets that have different number of rowgroups can\n    be fully read """"""\n    url = \'file://\' + tmpdir.strpath\n\n    ROWS_COUNT = 1000\n    # id_div_700 forces asymetric split between partitions and hopefully get us files with different number of row\n    # groups\n    create_test_scalar_dataset(url, ROWS_COUNT, partition_by=[\'id_div_700\'])\n\n    # We verify we have pieces with different number of row-groups\n    dataset = pq.ParquetDataset(tmpdir.strpath)\n    row_group_counts = set(compat_get_metadata(piece, dataset.fs.open).num_row_groups for piece in dataset.pieces)\n    assert len(row_group_counts) > 1\n\n    # Make sure we are not missing any rows.\n    with reader_factory(url, schema_fields=[\'id\']) as reader:\n        row_ids_batched = [row.id for row in reader]\n        actual_row_ids = list(itertools.chain(*row_ids_batched))\n\n    assert ROWS_COUNT == len(actual_row_ids)\n\n\n@pytest.mark.parametrize(\'reader_factory\', _D)\ndef test_invalid_column_name(scalar_dataset, reader_factory):\n    """"""Request a column that doesn\'t exist. When request only invalid fields,\n    DummyPool returns an EmptyResultError, which then causes a StopIteration in\n    ArrowReaderWorkerResultsQueueReader.""""""\n    all_fields = list(scalar_dataset.data[0].keys())\n    bad_field = _get_bad_field_name(all_fields)\n    requested_fields = [bad_field]\n\n    with reader_factory(scalar_dataset.url, schema_fields=requested_fields) as reader:\n        with pytest.raises(StopIteration):\n            sample = next(reader)._asdict()\n            assert not sample\n\n\n@pytest.mark.parametrize(\'reader_factory\', _D)\ndef test_invalid_and_valid_column_names(scalar_dataset, reader_factory):\n    """"""Request one column that doesn\'t exist and one that does. Confirm that only get one field back and\n    that get exception when try to read from invalid field.""""""\n    all_fields = list(scalar_dataset.data[0].keys())\n    bad_field = _get_bad_field_name(all_fields)\n    requested_fields = [bad_field, all_fields[1]]\n\n    with reader_factory(scalar_dataset.url, schema_fields=requested_fields) as reader:\n        sample = next(reader)._asdict()\n        assert len(sample) == 1\n        with pytest.raises(KeyError):\n            assert sample[bad_field] == """"\n\n\n@pytest.mark.parametrize(\'reader_factory\', _D)\ndef test_transform_spec_support_return_tensor(scalar_dataset, reader_factory):\n\n    field1 = UnischemaField(name=\'abc\', shape=(2, 3), numpy_dtype=np.float32)\n\n    with pytest.raises(ValueError, match=\'field abc must be numpy array type\'):\n        ArrowReaderWorker._check_shape_and_ravel(\'xyz\', field1)\n\n    with pytest.raises(ValueError, match=\'field abc must be the shape\'):\n        ArrowReaderWorker._check_shape_and_ravel(np.zeros((2, 5)), field1)\n\n    with pytest.raises(ValueError, match=\'field abc error: only support row major multi-dimensional array\'):\n        ArrowReaderWorker._check_shape_and_ravel(np.zeros((2, 3), order=\'F\'), field1)\n\n    assert (6,) == ArrowReaderWorker._check_shape_and_ravel(np.zeros((2, 3)), field1).shape\n\n    def preproc_fn1(x):\n        return pd.DataFrame({\n            \'tensor_col_1\': x[\'id\'].map(lambda _: np.random.rand(2, 3)),\n            \'tensor_col_2\': x[\'id\'].map(lambda _: np.random.rand(3, 4, 5)),\n        })\n    edit_fields = [\n        (\'tensor_col_1\', np.float32, (2, 3), False),\n        (\'tensor_col_2\', np.float32, (3, 4, 5), False),\n    ]\n\n    # This spec will remove all input columns and return one new column \'tensor_col_1\' with shape (2, 3)\n    spec1 = TransformSpec(\n        preproc_fn1,\n        edit_fields=edit_fields,\n        removed_fields=list(scalar_dataset.data[0].keys())\n    )\n\n    with reader_factory(scalar_dataset.url, transform_spec=spec1) as reader:\n        sample = next(reader)._asdict()\n        assert len(sample) == 2\n        assert (2, 3) == sample[\'tensor_col_1\'].shape[1:] and \\\n            (3, 4, 5) == sample[\'tensor_col_2\'].shape[1:]\n'"
petastorm/tests/test_pickle_serializer.py,0,"b'# -*- coding: utf-8 -*-\n\n#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom decimal import Decimal\n\nimport numpy as np\n\nfrom petastorm.reader_impl.pickle_serializer import PickleSerializer\n\n\ndef _foo():\n    pass\n\n\ndef test_nominal():\n    s = PickleSerializer()\n    expected = [{\'a\': np.asarray([1, 2], dtype=np.uint64), \'b\': Decimal(1.2), \'c\': _foo}]\n    actual = s.deserialize(s.serialize(expected))\n    np.testing.assert_array_equal(actual[0][\'a\'], expected[0][\'a\'])\n'"
petastorm/tests/test_predicates.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\n\nimport numpy as np\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import IntegerType\n\nfrom petastorm import make_reader\nfrom petastorm.codecs import ScalarCodec\nfrom petastorm.etl.dataset_metadata import materialize_dataset\nfrom petastorm.predicates import in_set, in_intersection, \\\n    in_negate, in_reduce, in_pseudorandom_split, in_lambda\nfrom petastorm.tests.test_common import TestSchema\nfrom petastorm.unischema import dict_to_spark_row, Unischema, UnischemaField\n\n\n@pytest.fixture(scope=""session"")\ndef all_values(request, tmpdir_factory):\n    all_values = set()\n    for i in range(10000):\n        all_values.add(\'guid_\' + str(i))\n    return all_values\n\n\ndef test_inclusion(all_values):\n    for values in [{\'guid_2\', \'guid_1\'}, {\'guid_5\', \'guid_XXX\'}, {\'guid_2\'}]:\n        test_predicate = in_set(values, \'volume_guid\')\n        included_values = set()\n        for val in all_values:\n            if test_predicate.do_include({\'volume_guid\': val}):\n                included_values.add(val)\n        assert included_values == all_values.intersection(values)\n\n\ndef test_list_inclusion(all_values):\n    for values in [{\'guid_2\', \'guid_1\'}, {\'guid_5\', \'guid_XXX\'}, {\'guid_XX\'}]:\n        test_predicate = in_intersection(values, \'volume_guid\')\n        included = test_predicate.do_include({\'volume_guid\': list(all_values)})\n        assert included != all_values.intersection(values)\n\n\ndef test_custom_function(all_values):\n    for value in [\'guid_2\', \'guid_1\', \'guid_5\', \'guid_XXX\', \'guid_XX\']:\n        test_predicate = in_lambda([\'volume_guids\'], lambda volume_guids, val=value: val in volume_guids)\n        included = test_predicate.do_include({\'volume_guids\': all_values})\n        assert included == (value in all_values)\n\n\ndef test_custom_function_with_state(all_values):\n    counter = [0]\n\n    def pred_func(volume_guids, cntr):\n        cntr[0] += 1\n        return volume_guids in all_values\n\n    test_predicate = in_lambda([\'volume_guids\'], pred_func, counter)\n    for value in [\'guid_2\', \'guid_1\', \'guid_5\', \'guid_XXX\', \'guid_XX\']:\n        included = test_predicate.do_include({\'volume_guids\': value})\n        assert included == (value in all_values)\n    assert counter[0] == 5\n\n\ndef test_negation(all_values):\n    for values in [{\'guid_2\', \'guid_1\'}, {\'guid_5\', \'guid_9\'}, {\'guid_2\'}]:\n        test_predicate = in_negate(in_set(values, \'volume_guid\'))\n        included_values = set()\n        for val in all_values:\n            if test_predicate.do_include({\'volume_guid\': val}):\n                included_values.add(val)\n        assert included_values == all_values.difference(values)\n\n\ndef test_and_argegarion(all_values):\n    for values1 in [{\'guid_0\', \'guid_1\'}, {\'guid_3\', \'guid_6\', \'guid_20\'}, {\'guid_2\'}]:\n        for values2 in [{\'guid_2\', \'guid_1\'}, {\'guid_5\', \'guid_9\'}, {\'guid_2\'}]:\n            test_predicate = in_reduce(\n                [in_set(values1, \'volume_guid\'), in_set(values2, \'volume_guid\')], all)\n            included_values = set()\n            for val in all_values:\n                if test_predicate.do_include({\'volume_guid\': val}):\n                    included_values.add(val)\n            assert included_values == values1.intersection(values2)\n\n\ndef test_or_argegarion(all_values):\n    for values1 in [{\'guid_0\', \'guid_1\'}, {\'guid_3\', \'guid_6\', \'guid_20\'}, {\'guid_2\'}]:\n        for values2 in [{\'guid_2\', \'guid_1\'}, {\'guid_5\', \'guid_9\'}, {\'guid_2\'}]:\n            test_predicate = in_reduce(\n                [in_set(values1, \'volume_guid\'), in_set(values2, \'volume_guid\')], any)\n            included_values = set()\n            for val in all_values:\n                if test_predicate.do_include({\'volume_guid\': val}):\n                    included_values.add(val)\n            assert included_values == values1.union(values2)\n\n\ndef test_pseudorandom_split_on_string_field(all_values):\n    split_list = [0.3, 0.4, 0.1, 0.0, 0.2]\n    values_num = len(all_values)\n    for idx in range(len(split_list)):\n        test_predicate = in_pseudorandom_split(split_list, idx, \'string_partition_field\')\n        included_values = set()\n        for val in all_values:\n            if test_predicate.do_include({\'string_partition_field\': val}):\n                included_values.add(val)\n        expected_num = values_num * split_list[idx]\n        assert pytest.approx(len(included_values), expected_num * 0.1) == expected_num\n\n\ndef test_pseudorandom_split_on_integer_field():\n    split_list = [0.3, 0.4, 0.1, 0.0, 0.2]\n    int_values = list(range(1000))\n    values_num = len(int_values)\n    for idx, _ in enumerate(split_list):\n        test_predicate = in_pseudorandom_split(split_list, idx, \'int_partitioning_field\')\n        included_values = set()\n        for val in int_values:\n            if test_predicate.do_include({\'int_partitioning_field\': val}):\n                included_values.add(val)\n        expected_num = values_num * split_list[idx]\n        assert pytest.approx(len(included_values), expected_num * 0.1) == expected_num\n\n\ndef test_predicate_on_single_column(synthetic_dataset):\n    reader = make_reader(synthetic_dataset.url,\n                         schema_fields=[TestSchema.id2],\n                         predicate=in_lambda([\'id2\'], lambda id2: True),\n                         reader_pool_type=\'dummy\')\n    counter = 0\n    for row in reader:\n        counter += 1\n        actual = dict(row._asdict())\n        assert actual[\'id2\'] < 2\n    assert counter == len(synthetic_dataset.data)\n\n\ndef test_predicate_on_partitioned_dataset(tmpdir):\n    """"""\n    Generates a partitioned dataset and ensures that readers evaluate the type of the partition\n    column according to the type given in the Unischema.\n    """"""\n    TestSchema = Unischema(\'TestSchema\', [\n        UnischemaField(\'id\', np.int32, (), ScalarCodec(IntegerType()), False),\n        UnischemaField(\'test_field\', np.int32, (), ScalarCodec(IntegerType()), False),\n    ])\n\n    def test_row_generator(x):\n        """"""Returns a single entry in the generated dataset.""""""\n        return {\'id\': x,\n                \'test_field\': x*x}\n\n    rowgroup_size_mb = 256\n    dataset_url = ""file://{0}/partitioned_test_dataset"".format(tmpdir)\n\n    spark = SparkSession.builder.config(\'spark.driver.memory\', \'2g\').master(\'local[2]\').getOrCreate()\n    sc = spark.sparkContext\n\n    rows_count = 10\n    with materialize_dataset(spark, dataset_url, TestSchema, rowgroup_size_mb):\n\n        rows_rdd = sc.parallelize(range(rows_count))\\\n            .map(test_row_generator)\\\n            .map(lambda x: dict_to_spark_row(TestSchema, x))\n\n        spark.createDataFrame(rows_rdd, TestSchema.as_spark_schema()) \\\n            .write \\\n            .partitionBy(\'id\') \\\n            .parquet(dataset_url)\n\n    with make_reader(dataset_url, predicate=in_lambda([\'id\'], lambda x: x == 3)) as reader:\n        assert next(reader).id == 3\n    with make_reader(dataset_url, predicate=in_lambda([\'id\'], lambda x: x == \'3\')) as reader:\n        with pytest.raises(StopIteration):\n            # Predicate should have selected none, so a StopIteration should be raised.\n            next(reader)\n'"
petastorm/tests/test_pyarrow_serializer.py,0,"b'# -*- coding: utf-8 -*-\n\n#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport pickle\nfrom decimal import Decimal\n\nimport numpy as np\n\nfrom petastorm.reader_impl.pyarrow_serializer import PyArrowSerializer\n\n\ndef test_nominal():\n    s = PyArrowSerializer()\n    expected = [{\'a\': np.asarray([1, 2], dtype=np.uint64)}]\n    actual = s.deserialize(s.serialize(expected))\n    np.testing.assert_array_equal(actual[0][\'a\'], expected[0][\'a\'])\n\n\ndef test_serializer_is_pickable():\n    """"""Pickle/depickle the serializer to make sure it can be passed\n    as a parameter cross process boundaries when using futures""""""\n    s = PyArrowSerializer()\n    deserialized_s = pickle.loads(pickle.dumps(s))\n\n    expected = [{\'a\': np.asarray([1, 2], dtype=np.uint64)}]\n    actual = deserialized_s.deserialize(deserialized_s.serialize(expected))\n    np.testing.assert_array_equal(actual[0][\'a\'], expected[0][\'a\'])\n\n\ndef test_decimal():\n    s = PyArrowSerializer()\n    expected = [{\'a\': Decimal(\'1.2\')}]\n    actual = s.deserialize(s.serialize(expected))\n    np.testing.assert_array_equal(actual[0][\'a\'], expected[0][\'a\'])\n\n    expected = [{\'a\': [Decimal(\'1.2\')]}]\n    actual = s.deserialize(s.serialize(expected))\n    np.testing.assert_array_equal(actual[0][\'a\'], expected[0][\'a\'])\n\n\ndef test_all_matrix_types():\n    s = PyArrowSerializer()\n    # We would be using serializer with arrays of dictionaries or arrays of dictionaries of dictionaries (ngram)\n    serialized_values = [\n        (np.int8, -127),\n        (np.uint8, 255),\n        (np.int16, -2 ** 15),\n        (np.uint16, 2 ** 16 - 1),\n        (np.int32, -2 ** 31),\n        (np.uint32, 2 ** 32 - 1),\n        (np.float16, 1.2),\n        (np.float32, 1.2),\n        (np.float64, 1.2),\n        (np.string_, \'abc\'),\n        (np.unicode_, u\'\xd7\x90\xd7\x91\xd7\x92\'),\n        (np.int64, -2 ** 63),\n        (np.uint64, 2 ** 64 - 1),\n    ]\n\n    for type_factory, value in serialized_values:\n        desired = [{\'value\': np.asarray(4 * [value], dtype=type_factory)}]\n        actual = s.deserialize(s.serialize(desired))\n        assert actual[0][\'value\'].dtype == desired[0][\'value\'].dtype\n        np.testing.assert_array_equal(actual[0][\'value\'], desired[0][\'value\'])\n'"
petastorm/tests/test_pytorch_dataloader.py,0,"b'from decimal import Decimal\nfrom packaging import version\n\nimport numpy as np\nimport pyarrow as pa\nimport pytest\n# Must import pyarrow before torch. See: https://github.com/uber/petastorm/blob/master/docs/troubleshoot.rst\nimport torch\n\nfrom petastorm import make_reader, TransformSpec, make_batch_reader\nfrom petastorm.pytorch import _sanitize_pytorch_types, DataLoader, BatchedDataLoader, decimal_friendly_collate\nfrom petastorm.tests.test_common import TestSchema\n\nALL_DATA_LOADERS = [DataLoader, BatchedDataLoader]\n\nBATCHABLE_FIELDS = set(TestSchema.fields.values()) - \\\n    {TestSchema.matrix_nullable, TestSchema.string_array_nullable,\n     TestSchema.matrix_string, TestSchema.empty_matrix_string, TestSchema.integer_nullable}\n\nTORCH_BATCHABLE_FIELDS = BATCHABLE_FIELDS - \\\n    {TestSchema.decimal, TestSchema.partition_key, }\n\n# pylint: disable=unnecessary-lambda\nMINIMAL_READER_FLAVOR_FACTORIES = [\n    lambda url, **kwargs: make_reader(url, reader_pool_type=\'dummy\', **kwargs),\n]\n\n# pylint: disable=unnecessary-lambda\nALL_READER_FLAVOR_FACTORIES = MINIMAL_READER_FLAVOR_FACTORIES + [\n    lambda url, **kwargs: make_reader(url, reader_pool_type=\'thread\', **kwargs),\n    lambda url, **kwargs: make_reader(url, reader_pool_type=\'process\', pyarrow_serialize=False, **kwargs),\n    lambda url, **kwargs: make_reader(url, reader_pool_type=\'process\', workers_count=1, pyarrow_serialize=True,\n                                      **kwargs),\n]\n\n\ndef _check_simple_reader(loader, expected_data, expected_fields):\n    # Read a bunch of entries from the dataset and compare the data to reference\n    def _type(v):\n        return v.dtype if isinstance(v, np.ndarray) else type(v)\n\n    def _unbatch(x):\n        if isinstance(x, torch.Tensor):\n            x_numpy = x.numpy()\n            assert x_numpy.shape[0] == 1\n            return x_numpy.squeeze(0)\n        elif isinstance(x, list):\n            return x[0]\n        else:\n            raise RuntimeError(\'Unexpected type while unbatching.\')\n\n    expected_field_names = [f.name for f in expected_fields]\n    for actual in loader:\n        actual_numpy = {k: _unbatch(v) for k, v in actual.items() if k in expected_field_names}\n        expected_all_fields = next(d for d in expected_data if d[\'id\'] == actual_numpy[\'id\'])\n        expected = {k: v for k, v in expected_all_fields.items() if k in expected_field_names}\n        np.testing.assert_equal(actual_numpy, expected)\n        actual_types = [_type(v) for v in actual_numpy.values()]\n        expected_types = [_type(v) for v in actual_numpy.values()]\n        assert actual_types == expected_types\n\n\ndef _sensor_name_to_int(row):\n    result_row = dict(**row)\n    result_row[\'sensor_name\'] = 0\n    return result_row\n\n\n@pytest.mark.parametrize(\'reader_factory\', ALL_READER_FLAVOR_FACTORIES)\ndef test_simple_read(synthetic_dataset, reader_factory):\n    with DataLoader(reader_factory(synthetic_dataset.url, schema_fields=BATCHABLE_FIELDS,\n                                   transform_spec=TransformSpec(_sensor_name_to_int))) as loader:\n        _check_simple_reader(loader, synthetic_dataset.data, BATCHABLE_FIELDS - {TestSchema.sensor_name})\n\n\n@pytest.mark.parametrize(\'reader_factory\', ALL_READER_FLAVOR_FACTORIES)\ndef test_simple_read_batched(synthetic_dataset, reader_factory):\n    with BatchedDataLoader(reader_factory(synthetic_dataset.url, schema_fields=TORCH_BATCHABLE_FIELDS,\n                                          transform_spec=TransformSpec(_sensor_name_to_int))) as loader:\n        _check_simple_reader(loader, synthetic_dataset.data, TORCH_BATCHABLE_FIELDS - {TestSchema.sensor_name})\n\n\ndef test_sanitize_pytorch_types_int8():\n    _TORCH_BEFORE_1_1 = version.parse(torch.__version__) < version.parse(\'1.1.0\')\n\n    dict_to_sanitize = {\'a\': np.asarray([-1, 1], dtype=np.int8)}\n    _sanitize_pytorch_types(dict_to_sanitize)\n\n    np.testing.assert_array_equal(dict_to_sanitize[\'a\'], [-1, 1])\n    if _TORCH_BEFORE_1_1:\n        assert dict_to_sanitize[\'a\'].dtype == np.int16\n    else:\n        assert dict_to_sanitize[\'a\'].dtype == np.int8\n\n\ndef test_decimal_friendly_collate_empty_input():\n    assert decimal_friendly_collate([dict()]) == dict()\n\n\n@pytest.mark.parametrize(\'numpy_dtype\',\n                         [np.int8, np.uint8, np.int16, np.uint16, np.int32, np.uint32, np.int64])\ndef test_torch_tensorable_types(numpy_dtype):\n    """"""Make sure that we \'sanitize\' only integer types that can not be made into torch tensors natively""""""\n    value = np.zeros((2, 2), dtype=numpy_dtype)\n    dict_to_sanitize = {\'value\': value}\n    _sanitize_pytorch_types(dict_to_sanitize)\n\n    torchable = False\n    try:\n        torch.Tensor(value)\n        torchable = True\n    except TypeError:\n        pass\n\n    tensor = torch.as_tensor(dict_to_sanitize[\'value\'])\n\n    tensor_and_back = tensor.numpy()\n\n    if tensor_and_back.dtype != value.dtype:\n        assert tensor_and_back.dtype.itemsize > value.dtype.itemsize\n        assert not torchable, \'_sanitize_pytorch_types modified value of type {}, but it was possible to create a \' \\\n                              \'Tensor directly from a value with that type\'.format(numpy_dtype)\n\n\ndef test_decimal_friendly_collate_input_has_decimals_in_dictionary():\n    desired = {\n        \'decimal\': [Decimal(\'1.0\'), Decimal(\'1.1\')],\n        \'int\': [1, 2]\n    }\n    input_batch = [\n        {\'decimal\': Decimal(\'1.0\'), \'int\': 1},\n        {\'decimal\': Decimal(\'1.1\'), \'int\': 2},\n    ]\n    actual = decimal_friendly_collate(input_batch)\n\n    assert len(actual) == 2\n    assert desired[\'decimal\'] == actual[\'decimal\']\n    np.testing.assert_equal(desired[\'int\'], actual[\'int\'].numpy())\n\n\ndef test_decimal_friendly_collate_input_has_decimals_in_tuple():\n    input_batch = ([Decimal(\'1.0\'), 1], [Decimal(\'1.1\'), 2])\n    desired = [(Decimal(\'1.0\'), Decimal(\'1.1\')), (1, 2)]\n    actual = decimal_friendly_collate(input_batch)\n\n    assert len(actual) == 2\n    assert desired[0] == actual[0]\n    np.testing.assert_equal(desired[1], actual[1].numpy())\n\n\n@pytest.mark.parametrize(\'reader_factory\', MINIMAL_READER_FLAVOR_FACTORIES)\n@pytest.mark.parametrize(""data_loader_type"", ALL_DATA_LOADERS)\ndef test_no_shuffling(synthetic_dataset, reader_factory, data_loader_type):\n    with data_loader_type(reader_factory(synthetic_dataset.url, schema_fields=[\'^id$\'], workers_count=1,\n                                         shuffle_row_groups=False)) as loader:\n        ids = [row[\'id\'][0].numpy() for row in loader]\n        # expected_ids would be [0, 1, 2, ...]\n        expected_ids = [row[\'id\'] for row in synthetic_dataset.data]\n        np.testing.assert_array_equal(expected_ids, ids)\n\n\n@pytest.mark.parametrize(\'reader_factory\', MINIMAL_READER_FLAVOR_FACTORIES)\n@pytest.mark.parametrize(""data_loader_type"", ALL_DATA_LOADERS)\ndef test_with_shuffling_buffer(synthetic_dataset, reader_factory, data_loader_type):\n    with data_loader_type(reader_factory(synthetic_dataset.url, schema_fields=[\'^id$\'], workers_count=1,\n                                         shuffle_row_groups=False),\n                          shuffling_queue_capacity=51) as loader:\n        ids = [row[\'id\'][0].numpy() for row in loader]\n\n        assert len(ids) == len(synthetic_dataset.data), \'All samples should be returned after reshuffling\'\n\n        # diff(ids) would return all-\'1\' for the seqeunce (note that we used shuffle_row_groups=False)\n        # We assume we get less then 10% of consequent elements for the sake of the test (this probability is very\n        # close to zero)\n        assert sum(np.diff(ids) == 1) < len(synthetic_dataset.data) / 10.0\n\n\n@pytest.mark.parametrize(\'shuffling_queue_capacity\', [0, 3, 11, 1000])\n@pytest.mark.parametrize(""data_loader_type"", ALL_DATA_LOADERS)\ndef test_with_batch_reader(scalar_dataset, shuffling_queue_capacity, data_loader_type):\n    """"""See if we are getting correct batch sizes when using DataLoader with make_batch_reader""""""\n    pytorch_compatible_fields = [k for k, v in scalar_dataset.data[0].items()\n                                 if not isinstance(v, (np.datetime64, np.unicode_))]\n    with data_loader_type(make_batch_reader(scalar_dataset.url, schema_fields=pytorch_compatible_fields),\n                          batch_size=3, shuffling_queue_capacity=shuffling_queue_capacity) as loader:\n        batches = list(loader)\n        assert len(scalar_dataset.data) == sum(batch[\'id\'].shape[0] for batch in batches)\n\n        # list types are broken in pyarrow 0.15.0. Don\'t test list-of-int field\n        if pa.__version__ != \'0.15.0\':\n            assert len(scalar_dataset.data) == sum(batch[\'int_fixed_size_list\'].shape[0] for batch in batches)\n            assert batches[0][\'int_fixed_size_list\'].shape[1] == len(scalar_dataset.data[0][\'int_fixed_size_list\'])\n\n\n@pytest.mark.parametrize(\'reader_factory\', ALL_READER_FLAVOR_FACTORIES)\n@pytest.mark.parametrize(""data_loader_type"", ALL_DATA_LOADERS)\ndef test_call_iter_on_dataloader_multiple_times(synthetic_dataset, reader_factory, data_loader_type):\n    with data_loader_type(reader_factory(synthetic_dataset.url, schema_fields=[TestSchema.id]),\n                          batch_size=2) as loader:\n        pass1_set = set()\n        for batch in iter(loader):\n            pass1_set |= set(batch[\'id\'].numpy())\n        pass2_set = set()\n        for batch in iter(loader):\n            pass2_set |= set(batch[\'id\'].numpy())\n        assert pass1_set == pass2_set\n        match_str = \'You must finish a full pass of Petastorm DataLoader before making another pass from the beginning\'\n        with pytest.raises(RuntimeError, match=match_str):\n            iter3 = iter(loader)\n            next(iter3)\n            iter4 = iter(loader)\n            next(iter4)\n'"
petastorm/tests/test_pytorch_utils.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom __future__ import division\n\nimport numpy as np\n\nfrom petastorm import make_reader, TransformSpec\nfrom petastorm.pytorch import DataLoader\nfrom petastorm.tests.test_common import TestSchema\n\nALL_FIELDS = set(TestSchema.fields.values())\nNULLABLE_FIELDS = {f for f in TestSchema.fields.values() if f.nullable}\nSTRING_TENSOR_FIELDS = {f for f in TestSchema.fields.values()\n                        if len(f.shape) > 0 and f.numpy_dtype in (np.string_, np.unicode_)}\n\nPYTORCH_COMPATIBLE_FIELDS = ALL_FIELDS - STRING_TENSOR_FIELDS - NULLABLE_FIELDS\n\n\ndef _noop_collate(alist):\n    return alist\n\n\ndef _str_to_int(sample):\n    for k, v in sample.items():\n        if v is not None and isinstance(v, np.ndarray) and v.dtype.type in (np.string_, np.unicode_):\n            sample[k] = np.zeros_like(v, dtype=np.int8)\n    return sample\n\n\ndef test_basic_pytorch_dataloader(synthetic_dataset):\n    with DataLoader(make_reader(synthetic_dataset.url, schema_fields=PYTORCH_COMPATIBLE_FIELDS,\n                                reader_pool_type=\'dummy\'), collate_fn=_noop_collate) as loader:\n        for item in loader:\n            assert len(item) == 1\n\n\ndef test_pytorch_dataloader_with_transform_function(synthetic_dataset):\n    with DataLoader(make_reader(synthetic_dataset.url, schema_fields=ALL_FIELDS - NULLABLE_FIELDS,\n                                reader_pool_type=\'dummy\',\n                                transform_spec=TransformSpec(_str_to_int)), collate_fn=_noop_collate) as loader:\n        for item in loader:\n            assert len(item) == 1\n\n\ndef test_pytorch_dataloader_batched(synthetic_dataset):\n    batch_size = 10\n    loader = DataLoader(\n        make_reader(synthetic_dataset.url, schema_fields=PYTORCH_COMPATIBLE_FIELDS, reader_pool_type=\'dummy\'),\n        batch_size=batch_size, collate_fn=_noop_collate)\n    for item in loader:\n        assert len(item) == batch_size\n\n\ndef test_pytorch_dataloader_context(synthetic_dataset):\n    reader = make_reader(synthetic_dataset.url, schema_fields=PYTORCH_COMPATIBLE_FIELDS, reader_pool_type=\'dummy\')\n    with DataLoader(reader, collate_fn=_noop_collate) as loader:\n        for item in loader:\n            assert len(item) == 1\n'"
petastorm/tests/test_reader.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom time import sleep\n\nimport pyarrow.parquet as pq\nimport pytest\n\nfrom petastorm import make_reader\nfrom petastorm.reader import Reader\n\n# pylint: disable=unnecessary-lambda\nREADER_FACTORIES = [\n    make_reader,\n]\n\n\n@pytest.mark.parametrize(\'reader_factory\', READER_FACTORIES)\ndef test_dataset_url_must_be_string(reader_factory):\n    with pytest.raises(ValueError):\n        reader_factory(None)\n\n    with pytest.raises(ValueError):\n        reader_factory(123)\n\n    with pytest.raises(ValueError):\n        reader_factory([])\n\n\ndef test_diagnostics_reader_v1(synthetic_dataset):\n    with make_reader(synthetic_dataset.url) as reader:\n        next(reader)\n        diags = reader.diagnostics\n        # Hard to make a meaningful assert on the content of the diags without potentially introducing a race\n        assert \'output_queue_size\' in diags\n\n\n@pytest.mark.skip(\'We no longer know how many rows in each row group\')\ndef test_normalize_shuffle_partitions(synthetic_dataset):\n    dataset = pq.ParquetDataset(synthetic_dataset.path)\n    row_drop_partitions = Reader._normalize_shuffle_options(2, dataset)\n    assert row_drop_partitions == 2\n\n    row_drop_partitions = Reader._normalize_shuffle_options(1000, dataset)\n    assert row_drop_partitions == 10\n\n\ndef test_bound_size_of_output_queue_size_reader(synthetic_dataset):\n    """"""This test is timing sensitive so it might become flaky""""""\n    TIME_TO_GET_TO_STATIONARY_STATE = 0.5\n\n    with make_reader(synthetic_dataset.url, reader_pool_type=\'process\', workers_count=1) as reader:\n        assert 0 == reader.diagnostics[\'items_produced\']\n        next(reader)\n        # Verify that we did not consume all rowgroups (should be 10) and ventilator throttles number of ventilated\n        # items\n        sleep(TIME_TO_GET_TO_STATIONARY_STATE)\n        assert reader.diagnostics[\'items_consumed\'] < 5\n        assert reader.diagnostics[\'items_inprocess\'] < 5\n\n\n@pytest.mark.parametrize(\'reader_factory\', READER_FACTORIES)\ndef test_invalid_cache_type(synthetic_dataset, reader_factory):\n    with pytest.raises(ValueError, match=\'Unknown cache_type\'):\n        reader_factory(synthetic_dataset.url, cache_type=\'bogus_cache_type\')\n\n\n@pytest.mark.parametrize(\'reader_factory\', READER_FACTORIES)\ndef test_invalid_reader_pool_type(synthetic_dataset, reader_factory):\n    with pytest.raises(ValueError, match=\'Unknown reader_pool_type\'):\n        reader_factory(synthetic_dataset.url, reader_pool_type=\'bogus_pool_type\')\n'"
petastorm/tests/test_reader_mock.py,1,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nfrom decimal import Decimal\n\nimport pytest\nimport tensorflow.compat.v1 as tf  # pylint: disable=import-error\n\nfrom petastorm.test_util.reader_mock import ReaderMock, schema_data_generator_example\nfrom petastorm.tests.test_end_to_end import TestSchema\nfrom petastorm.tf_utils import tf_tensors, _numpy_to_tf_dtypes\nfrom petastorm.tests.test_tf_utils import create_tf_graph\n\n\nclass ReaderMockTest(unittest.TestCase):\n\n    def setUp(self):\n        self.reader = ReaderMock(TestSchema, schema_data_generator_example)\n\n    def test_simple_read(self):\n        """"""Just a bunch of read and compares of all values to the expected values for their types and\n        shapes.""""""\n        # Read a bunch of entries from the dataset and compare the data to reference\n        for _ in range(10):\n            actual = dict(next(self.reader)._asdict())\n            for schema_field in TestSchema.fields.values():\n                if schema_field.numpy_dtype == Decimal:\n                    self.assertTrue(isinstance(actual[schema_field.name], Decimal))\n                else:\n                    self.assertTrue(actual[schema_field.name].dtype.type is schema_field.numpy_dtype)\n                    self.assertEqual(len(actual[schema_field.name].shape), len(schema_field.shape))\n\n        self.reader.stop()\n        self.reader.join()\n\n    @pytest.mark.forked\n    @create_tf_graph\n    def test_simple_read_tf(self):\n        """"""Just a bunch of read and compares of all values to the expected values for their types\n        and shapes""""""\n        reader_tensors = tf_tensors(self.reader)._asdict()\n\n        for schema_field in TestSchema.fields.values():\n            self.assertEqual(reader_tensors[schema_field.name].dtype,\n                             _numpy_to_tf_dtypes(schema_field.numpy_dtype))\n            self.assertEqual(len(reader_tensors[schema_field.name].shape), len(schema_field.shape))\n\n        # Read a bunch of entries from the dataset and compare the data to reference\n        with tf.Session() as sess:\n            for _ in range(10):\n                sess.run(reader_tensors)\n\n        self.reader.stop()\n        self.reader.join()\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
petastorm/tests/test_reading_legacy_datasets.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\n\nimport pytest\n\nfrom petastorm import make_reader\n\n\ndef dataset_urls():\n    """"""Returns a list of legacy datasets available for testing""""""\n    legacy_data_directory = os.path.join(os.path.dirname(__file__), \'data\', \'legacy\')\n    versions = os.listdir(legacy_data_directory)\n    urls = [\'file://\' + os.path.join(legacy_data_directory, v) for v in versions]\n    return urls\n\n\n@pytest.mark.parametrize(\'legacy_dataset_url\', dataset_urls())\ndef test_reading_legacy_dataset(legacy_dataset_url):\n    """"""The test runs for a single legacy dataset. Opens the dataset using `make_reader` and reads all records from it""""""\n    with make_reader(legacy_dataset_url, workers_count=1) as reader:\n        all_data = list(reader)\n\n        # Some basic check on the data\n        assert len(all_data) == 100\n        assert len(all_data[0]._fields) > 5\n        assert all_data[0].matrix.shape == (32, 16, 3)\n'"
petastorm/tests/test_run_in_subprocess.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nfrom functools import partial\n\nfrom petastorm.unischema import dict_to_spark_row, Unischema\nfrom petastorm.utils import run_in_subprocess\n\n\ndef builtin_func():\n    return list(range(10))\n\n\ndef multiply(a, b):\n    return a * b\n\n\nclass RunInSubprocessTest(unittest.TestCase):\n\n    def test_run_in_subprocess(self):\n        # Serialization of a built in function\n        self.assertEquals(run_in_subprocess(builtin_func), builtin_func())\n\n        # Arg passing\n        self.assertEquals(run_in_subprocess(multiply, 2, 3), 6)\n\n    def test_partial_application(self):\n        unischema = Unischema(\'foo\', [])\n        func = partial(dict_to_spark_row, unischema)\n        func({})\n\n        # Must pass as positional arg in the right order\n        func = partial(dict_to_spark_row, {})\n        with self.assertRaises(AssertionError):\n            func(Unischema)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
petastorm/tests/test_shuffling_buffer.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport pytest\nimport six\n\nfrom petastorm.reader_impl.shuffling_buffer import NoopShufflingBuffer, RandomShufflingBuffer\nfrom petastorm.reader_impl.pytorch_shuffling_buffer import BatchedNoopShufflingBuffer, BatchedRandomShufflingBuffer\n\nNOOP_SHUFFLING_BUFFERS = [NoopShufflingBuffer, BatchedNoopShufflingBuffer]\nRANDOM_SHUFFLING_BUFFERS = [RandomShufflingBuffer, BatchedRandomShufflingBuffer]\n\n\n@pytest.mark.parametrize(\'buffer_type\', NOOP_SHUFFLING_BUFFERS)\ndef test_noop_shuffling_buffer(buffer_type):\n    """"""Noop should not do any shuffling. Add/retrieve some items while checking correcness of can_retrieve""""""\n    q = buffer_type()\n\n    # Empty buffer. Can add, can not retrieve with zero size\n    assert q.size == 0\n    assert q.can_add()\n    assert not q.can_retrieve()\n\n    # Try adding some items. Check queue size and can_retrieve indicator\n    _add_many(q, [1])\n    assert q.can_add()\n    assert q.size == 1\n\n    _add_many(q, [2, 3])\n    assert q.size == 3\n\n    assert 1 == _retrieve(q)\n    assert q.can_retrieve()\n\n    assert 2 == _retrieve(q)\n    assert 3 == _retrieve(q)\n    assert not q.can_retrieve()\n\n    # No effect is expected in noop implementation\n    q.finish()\n\n\ndef _add_many(q, lst):\n    if isinstance(q, (NoopShufflingBuffer, RandomShufflingBuffer)):\n        q.add_many(lst)\n    else:\n        q.add_many([lst])\n\n\ndef _retrieve(q):\n    if isinstance(q, (NoopShufflingBuffer, RandomShufflingBuffer)):\n        return q.retrieve()\n    else:\n        return q.retrieve()[0][0].item()\n\n\n@pytest.mark.parametrize(\'buffer_type\', RANDOM_SHUFFLING_BUFFERS)\ndef test_random_shuffling_buffer_can_add_retrieve_flags(buffer_type):\n    """"""Check can_add/can_retrieve flags at all possible states""""""\n    q = buffer_type(5, 3)\n\n    # Empty buffer. Can start adding, nothing to retrieve yet\n    assert q.size == 0\n    assert q.can_add()\n    assert not q.can_retrieve()\n\n    # Under min_after_retrieve elements, so can not retrieve just yet\n    _add_many(q, [1, 2])\n    assert q.can_add()\n    assert not q.can_retrieve()\n    assert q.size == 2\n\n    # Got to min_after_retrieve elements, can start retrieving\n    _add_many(q, [3])\n    assert q.can_retrieve()\n    assert q.size == 3\n\n    # But when we retrieve we are again under min_after_retrieve, so can not retrieve again\n    _retrieve(q)\n    assert not q.can_retrieve()\n    assert q.size == 2\n\n    # Getting back to the retrievable state with enough items in the buffer\n    _add_many(q, [4, 5])\n    assert q.can_add()\n    assert q.can_retrieve()\n    assert q.size == 4\n\n    # Can overrun the capacity (as long as below extra_capacity), but can not add if we are above\n    # shuffling_buffer_capacity\n    _add_many(q, [6, 7, 8, 9])\n    assert not q.can_add()\n    with pytest.raises(RuntimeError):\n        _add_many(q, [1])\n    assert q.can_retrieve()\n    assert q.size == 8\n\n    # Getting one out. Still have more than shuffling_buffer_capacity\n    _retrieve(q)\n    assert not q.can_add()\n    assert q.can_retrieve()\n    assert q.size == 7\n\n    # Retrieve enough to get back to addable state\n    [_retrieve(q) for _ in range(4)]\n    assert q.can_add()\n    assert q.can_retrieve()\n    assert q.size == 3\n\n    # Retrieve the last element so we go under min_after_retrieve and can not retrieve any more\n    _retrieve(q)\n    assert q.can_add()\n    assert not q.can_retrieve()\n    with pytest.raises(RuntimeError):\n        _retrieve(q)\n\n    assert q.size == 2\n\n    # finish() will allow us to deplete the buffer completely\n    q.finish()\n    assert not q.can_add()\n    assert q.can_retrieve()\n    assert q.size == 2\n\n    _retrieve(q)\n    assert not q.can_add()\n    assert q.can_retrieve()\n    assert q.size == 1\n\n    _retrieve(q)\n    assert not q.can_add()\n    assert not q.can_retrieve()\n    assert q.size == 0\n\n\ndef _retrieve_many(q):\n    if isinstance(q, (NoopShufflingBuffer, RandomShufflingBuffer)):\n        return [q.retrieve()]\n    else:\n        return [a.item() for a in q.retrieve()[0]]\n\n\ndef _feed_a_sequence_through_the_queue(shuffling_buffer, input_sequence):\n    assert shuffling_buffer.size == 0\n    assert not shuffling_buffer.can_retrieve()\n\n    retrieve_sequence = []\n    fro = 0\n    while True:\n        if shuffling_buffer.can_add():\n            to = min(fro + 3, len(input_sequence))\n            next_input_chunk = input_sequence[fro:to]\n            fro = to\n\n            _add_many(shuffling_buffer, next_input_chunk)\n\n            if to >= len(input_sequence):\n                shuffling_buffer.finish()\n                break\n\n        for _ in range(2):\n            if shuffling_buffer.can_retrieve():\n                retrieve_sequence.extend(_retrieve_many(shuffling_buffer))\n\n    while shuffling_buffer.can_retrieve():\n        retrieve_sequence.extend(_retrieve_many(shuffling_buffer))\n\n    return retrieve_sequence\n\n\n@pytest.mark.parametrize(\'buffer_type\', RANDOM_SHUFFLING_BUFFERS)\ndef test_random_shuffling_buffer_stream_through(buffer_type):\n    """"""Feed a 0:99 sequence through a (Batched)RandomShufflingBuffer. Check that the order has changed.""""""\n    input_sequence = range(100)\n    a = _feed_a_sequence_through_the_queue(buffer_type(10, 3), input_sequence)\n    b = _feed_a_sequence_through_the_queue(buffer_type(10, 3), input_sequence)\n    assert len(a) == len(input_sequence)\n    assert set(a) == set(b)\n    assert a != b\n\n\n@pytest.mark.parametrize(\'buffer_type\', NOOP_SHUFFLING_BUFFERS)\ndef test_noop_shuffling_buffer_stream_through(buffer_type):\n    """"""Feed a 0:99 sequence through a (Batched)NoopShufflingBuffer. Check that the order has not changed.""""""\n    expected = list(range(100))\n    actual = _feed_a_sequence_through_the_queue(buffer_type(), expected)\n    assert expected == actual\n\n\n@pytest.mark.parametrize(\'batch_size\', [2, 3, 6, 10])\ndef test_batched_random_shuffling_buffer_stream_through(batch_size):\n    """"""Feed a 0:99 sequence through a BatchedRandomShufflingBuffer. Check that the order has changed.""""""\n    input_sequence = range(100)\n    a = _feed_a_sequence_through_the_queue(BatchedRandomShufflingBuffer(10, 3, batch_size), input_sequence)\n    b = _feed_a_sequence_through_the_queue(BatchedRandomShufflingBuffer(10, 3, batch_size), input_sequence)\n    assert len(a) == len(input_sequence)\n    assert set(a) == set(b)\n    assert a != b\n\n\n@pytest.mark.parametrize(\'batch_size\', [2, 3, 6, 10])\ndef test_batched_buffer_stream_through(batch_size):\n    """"""Feed a 0:99 sequence through a BatchedNoopShufflingBuffer. Check that the order has not changed.""""""\n    expected = list(range(100))\n    actual = _feed_a_sequence_through_the_queue(BatchedNoopShufflingBuffer(batch_size), expected)\n    assert expected == actual\n\n\n@pytest.mark.parametrize(\'buffer_type\', RANDOM_SHUFFLING_BUFFERS)\ndef test_longer_random_sequence_of_queue_ops(buffer_type):\n    """"""A long random sequence of added and retrieved values""""""\n    q = buffer_type(100, 80)\n\n    for _ in six.moves.xrange(10000):\n        if q.can_add():\n            _add_many(q, np.random.random((np.random.randint(1, 10),)))\n        assert q.size < 100 + 10\n        for _ in range(np.random.randint(1, 10)):\n            if not q.can_retrieve():\n                break\n            # Make sure never get to less than `min_after_retrieve` elements\n            assert 80 <= q.size\n            _retrieve(q)\n'"
petastorm/tests/test_spark_dataset_converter.py,9,"b'#  Copyright (c) 2020 Databricks, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport subprocess\nimport sys\nimport tempfile\nimport threading\nimport time\nfrom distutils.version import LooseVersion\n\nimport numpy as np\nimport pyspark\nimport pytest\nimport py4j\nimport tensorflow.compat.v1 as tf  # pylint: disable=import-error\nfrom pyspark.sql.functions import pandas_udf\nfrom pyspark.sql.types import (ArrayType, BinaryType, BooleanType, ByteType,\n                               DoubleType, FloatType, IntegerType, LongType,\n                               ShortType, StringType, StructField, StructType)\nfrom six.moves.urllib.parse import urlparse\n\nfrom petastorm import make_batch_reader\nfrom petastorm.fs_utils import FilesystemResolver\nfrom petastorm.spark import (SparkDatasetConverter, make_spark_converter,\n                             spark_dataset_converter)\nfrom petastorm.spark.spark_dataset_converter import (\n    _check_dataset_file_median_size, _check_parent_cache_dir_url,\n    _check_rank_and_size_consistent_with_horovod, _check_url,\n    _get_horovod_rank_and_size, _get_spark_session, _make_sub_dir_url,\n    register_delete_dir_handler, _wait_file_available)\n\ntry:\n    from mock import mock\nexcept ImportError:\n    from unittest import mock\n\nfrom petastorm.tests.test_tf_utils import create_tf_graph\n\n\n@create_tf_graph\ndef test_primitive(spark_test_ctx):\n    schema = StructType([\n        StructField(""bool_col"", BooleanType(), False),\n        StructField(""float_col"", FloatType(), False),\n        StructField(""double_col"", DoubleType(), False),\n        StructField(""short_col"", ShortType(), False),\n        StructField(""int_col"", IntegerType(), False),\n        StructField(""long_col"", LongType(), False),\n        StructField(""str_col"", StringType(), False),\n        StructField(""bin_col"", BinaryType(), False),\n        StructField(""byte_col"", ByteType(), False),\n    ])\n    df = spark_test_ctx.spark.createDataFrame(\n        [(True, 0.12, 432.1, 5, 5, 0, ""hello"",\n          bytearray(b""spark\\x01\\x02""), -128),\n         (False, 123.45, 0.987, 9, 908, 765, ""petastorm"",\n          bytearray(b""\\x0012345""), 127)],\n        schema=schema).coalesce(1)\n    # If we use numPartition > 1, the order of the loaded dataset would\n    # be non-deterministic.\n    expected_df = df.collect()\n\n    converter = make_spark_converter(df)\n    with converter.make_tf_dataset() as dataset:\n        iterator = dataset.make_one_shot_iterator()\n        tensor = iterator.get_next()\n        with tf.Session() as sess:\n            ts = sess.run(tensor)\n            # TODO: we will improve the test once the batch_size argument\n            #  added.\n            # Now we only have one batch.\n        for i in range(converter.dataset_size):\n            for col in df.schema.names:\n                actual_ele = getattr(ts, col)[i]\n                expected_ele = expected_df[i][col]\n                if col == ""str_col"":\n                    actual_ele = actual_ele.decode()\n                if col == ""bin_col"":\n                    actual_ele = bytearray(actual_ele)\n                if col == ""float_col"" or col == ""double_col"":\n                    # Note that the default dtype is float32\n                    assert pytest.approx(expected_ele, rel=1e-6) == actual_ele\n                else:\n                    assert expected_ele == actual_ele\n\n        assert len(expected_df) == len(converter)\n\n    assert np.bool_ == ts.bool_col.dtype.type\n    assert np.float32 == ts.float_col.dtype.type\n    # Default dtype float32\n    assert np.float32 == ts.double_col.dtype.type\n    assert np.int16 == ts.short_col.dtype.type\n    assert np.int32 == ts.int_col.dtype.type\n    assert np.int64 == ts.long_col.dtype.type\n    assert np.object_ == ts.str_col.dtype.type\n    assert np.object_ == ts.bin_col.dtype.type\n\n\n@create_tf_graph\ndef test_array_field(spark_test_ctx):\n    @pandas_udf(\'array<float>\')\n    def gen_array(v):\n        return v.map(lambda x: np.random.rand(10))\n    df1 = spark_test_ctx.spark.range(10).withColumn(\'v\', gen_array(\'id\')).repartition(2)\n    cv1 = make_spark_converter(df1)\n    # we can auto infer one-dim array shape\n    with cv1.make_tf_dataset(batch_size=4, num_epochs=1) as dataset:\n        tf_iter = dataset.make_one_shot_iterator()\n        next_op = tf_iter.get_next()\n        with tf.Session() as sess:\n            batch1 = sess.run(next_op)\n        assert batch1.v.shape == (4, 10)\n\n\ndef test_delete(spark_test_ctx):\n    df = spark_test_ctx.spark.createDataFrame([(1, 2), (4, 5)], [""col1"", ""col2""])\n    # TODO add test for hdfs url\n    converter = make_spark_converter(df)\n    local_path = urlparse(converter.cache_dir_url).path\n    assert os.path.exists(local_path)\n    converter.delete()\n    assert not os.path.exists(local_path)\n\n\ndef test_atexit(spark_test_ctx):\n    lines = """"""\n    from petastorm.spark import SparkDatasetConverter, make_spark_converter\n    from pyspark.sql import SparkSession\n    import os\n    spark = SparkSession.builder.getOrCreate()\n    spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, \'{temp_url}\')\n    df = spark.createDataFrame([(1, 2),(4, 5)], [""col1"", ""col2""])\n    converter = make_spark_converter(df)\n    f = open(os.path.join(\'{tempdir}\', \'test_atexit.out\'), ""w"")\n    f.write(converter.cache_dir_url)\n    f.close()\n    """""".format(tempdir=spark_test_ctx.tempdir, temp_url=spark_test_ctx.temp_url)\n    code_str = ""; "".join(\n        line.strip() for line in lines.strip().splitlines())\n    ret_code = subprocess.call([sys.executable, ""-c"", code_str])\n    assert 0 == ret_code\n    with open(os.path.join(spark_test_ctx.tempdir, \'test_atexit.out\')) as f:\n        cache_dir_url = f.read()\n\n    fs = FilesystemResolver(cache_dir_url).filesystem()\n    assert not fs.exists(urlparse(cache_dir_url).path)\n\n\ndef test_set_delete_handler(spark_test_ctx):\n    def test_delete_handler(dir_url):\n        raise RuntimeError(\'Not implemented delete handler.\')\n\n    register_delete_dir_handler(test_delete_handler)\n\n    with pytest.raises(RuntimeError, match=\'Not implemented delete handler\'):\n        spark_dataset_converter._delete_dir_handler(spark_test_ctx.temp_url)\n\n    # Restore default delete handler (other test will use it)\n    register_delete_dir_handler(None)\n\n\ndef _get_compression_type(data_url):\n    files = os.listdir(urlparse(data_url).path)\n    pq_files = list(filter(lambda x: x.endswith(\'.parquet\'), files))\n    filename_splits = pq_files[0].split(\'.\')\n    if len(filename_splits) == 2:\n        return ""uncompressed""\n    else:\n        return filename_splits[1]\n\n\ndef test_compression(spark_test_ctx):\n    df1 = spark_test_ctx.spark.range(10)\n\n    converter1 = make_spark_converter(df1)\n    assert ""uncompressed"" == \\\n           _get_compression_type(converter1.cache_dir_url).lower()\n\n    converter2 = make_spark_converter(df1, compression_codec=""snappy"")\n    assert ""snappy"" == \\\n           _get_compression_type(converter2.cache_dir_url).lower()\n\n\ndef test_df_caching(spark_test_ctx):\n    df1 = spark_test_ctx.spark.range(10)\n    df2 = spark_test_ctx.spark.range(10)\n    df3 = spark_test_ctx.spark.range(20)\n\n    # Test caching for the dataframes with the same logical plan\n    converter1 = make_spark_converter(df1)\n    converter2 = make_spark_converter(df2)\n    assert converter1.cache_dir_url == converter2.cache_dir_url\n\n    # Test no caching for different dataframes\n    converter3 = make_spark_converter(df3)\n    assert converter1.cache_dir_url != converter3.cache_dir_url\n\n    # Test no caching for the same dataframe with different row group size\n    converter11 = make_spark_converter(\n        df1, parquet_row_group_size_bytes=8 * 1024 * 1024)\n    converter21 = make_spark_converter(\n        df1, parquet_row_group_size_bytes=16 * 1024 * 1024)\n    assert converter11.cache_dir_url != converter21.cache_dir_url\n\n    # Test no caching for the same dataframe with different compression_codec\n    converter12 = make_spark_converter(df1, compression_codec=None)\n    converter22 = make_spark_converter(df1, compression_codec=""snappy"")\n    assert converter12.cache_dir_url != converter22.cache_dir_url\n\n    ori_temp_url = spark_test_ctx.spark.conf.get(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF)\n    tempdir = tempfile.mkdtemp(\'_spark_converter_test1\')\n    new_temp_url = \'file://\' + tempdir.replace(os.sep, \'/\')\n    try:\n        # Test no caching for the same dataframe with different parent cache dirs\n        spark_test_ctx.spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF,\n                                      new_temp_url)\n        assert ori_temp_url != new_temp_url\n        converter13 = make_spark_converter(df1)\n        assert converter1.cache_dir_url != converter13.cache_dir_url\n\n        # Test caching for the same dataframe with different parent cache dirs\n        # that could be normalized to the same parent cache dir\n        new_temp_url_2 = new_temp_url + os.sep\n        spark_test_ctx.spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF,\n                                      new_temp_url_2)\n        assert new_temp_url != new_temp_url_2\n        converter14 = make_spark_converter(df1)\n        assert converter13.cache_dir_url == converter14.cache_dir_url\n    finally:\n        spark_test_ctx.spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF,\n                                      ori_temp_url)\n\n\ndef test_df_delete_caching_meta(spark_test_ctx):\n    from petastorm.spark.spark_dataset_converter import _cache_df_meta_list\n    df1 = spark_test_ctx.spark.range(10)\n    df2 = spark_test_ctx.spark.range(20)\n    converter1 = make_spark_converter(df1)\n    converter2 = make_spark_converter(df2)\n    converter1.delete()\n    cached_list = set(map(lambda x: x.cache_dir_url, _cache_df_meta_list))\n    assert converter1.cache_dir_url not in cached_list\n    assert converter2.cache_dir_url in cached_list\n    # test recreate converter1 after delete should work.\n    make_spark_converter(df1)\n\n\ndef test_check_url():\n    with pytest.raises(ValueError, match=\'scheme-less\'):\n        _check_url(\'/a/b/c\')\n\n\ndef test_make_sub_dir_url():\n    assert _make_sub_dir_url(\'file:///a/b\', \'c\') == \'file:///a/b/c\'\n    assert _make_sub_dir_url(\'hdfs:/a/b\', \'c\') == \'hdfs:/a/b/c\'\n    assert _make_sub_dir_url(\'hdfs://nn1:9000/a/b\', \'c\') == \'hdfs://nn1:9000/a/b/c\'\n\n\ndef test_pickling_remotely(spark_test_ctx):\n    df1 = spark_test_ctx.spark.range(100, 101)\n    converter1 = make_spark_converter(df1)\n\n    @create_tf_graph\n    def map_fn(_):\n        with converter1.make_tf_dataset() as dataset:\n            iterator = dataset.make_one_shot_iterator()\n            tensor = iterator.get_next()\n            with tf.Session() as sess:\n                ts = sess.run(tensor)\n        return getattr(ts, \'id\')[0]\n\n    result = spark_test_ctx.spark.sparkContext.parallelize(range(1), 1).map(map_fn).collect()[0]\n    assert result == 100\n\n\n@create_tf_graph\ndef test_tf_dataset_batch_size(spark_test_ctx):\n    df1 = spark_test_ctx.spark.range(100)\n\n    batch_size = 30\n    converter1 = make_spark_converter(df1)\n\n    with converter1.make_tf_dataset(batch_size=batch_size) as dataset:\n        iterator = dataset.make_one_shot_iterator()\n        tensor = iterator.get_next()\n        with tf.Session() as sess:\n            ts = sess.run(tensor)\n    assert len(ts.id) == batch_size\n\n\n@mock.patch(\'petastorm.spark.spark_dataset_converter.make_batch_reader\')\ndef test_tf_dataset_petastorm_args(mock_make_batch_reader, spark_test_ctx):\n    df1 = spark_test_ctx.spark.range(100).repartition(4)\n    conv1 = make_spark_converter(df1)\n\n    mock_make_batch_reader.return_value = make_batch_reader(conv1.cache_dir_url)\n\n    with conv1.make_tf_dataset(reader_pool_type=\'dummy\', cur_shard=1, shard_count=4):\n        pass\n    peta_args = mock_make_batch_reader.call_args.kwargs\n    assert peta_args[\'reader_pool_type\'] == \'dummy\' and \\\n        peta_args[\'cur_shard\'] == 1 and \\\n        peta_args[\'shard_count\'] == 4 and \\\n        peta_args[\'num_epochs\'] is None and \\\n        peta_args[\'workers_count\'] == 4\n\n    with conv1.make_tf_dataset(num_epochs=1, workers_count=2):\n        pass\n    peta_args = mock_make_batch_reader.call_args.kwargs\n    assert peta_args[\'num_epochs\'] == 1 and peta_args[\'workers_count\'] == 2\n\n\ndef test_horovod_rank_compatibility(spark_test_ctx):\n    with mock.patch.dict(os.environ, {\'HOROVOD_RANK\': \'1\', \'HOROVOD_SIZE\': \'3\'}, clear=True):\n        assert (1, 3) == _get_horovod_rank_and_size()\n        assert _check_rank_and_size_consistent_with_horovod(\n            petastorm_reader_kwargs={""cur_shard"": 1, ""shard_count"": 3})\n        assert not _check_rank_and_size_consistent_with_horovod(\n            petastorm_reader_kwargs={""cur_shard"": 1, ""shard_count"": 2})\n        assert not _check_rank_and_size_consistent_with_horovod(\n            petastorm_reader_kwargs={""cur_shard"": 0, ""shard_count"": 3})\n\n    with mock.patch.dict(os.environ, {\'OMPI_COMM_WORLD_RANK\': \'1\', \'OMPI_COMM_WORLD_SIZE\': \'3\'}, clear=True):\n        assert (1, 3) == _get_horovod_rank_and_size()\n    with mock.patch.dict(os.environ, {\'PMI_RANK\': \'1\', \'PMI_SIZE\': \'3\'}, clear=True):\n        assert (1, 3) == _get_horovod_rank_and_size()\n    with mock.patch.dict(os.environ, {}, clear=True):\n        assert (None, None) == _get_horovod_rank_and_size()\n        assert _check_rank_and_size_consistent_with_horovod(\n            petastorm_reader_kwargs={""cur_shard"": 1, ""shard_count"": 3})\n\n\n@create_tf_graph\ndef test_dtype(spark_test_ctx):\n    df = spark_test_ctx.spark.range(10)\n    df = df.withColumn(""float_col"", df.id.cast(FloatType())) \\\n        .withColumn(""double_col"", df.id.cast(DoubleType()))\n\n    converter1 = make_spark_converter(df)\n    with converter1.make_tf_dataset() as dataset:\n        iterator = dataset.make_one_shot_iterator()\n        tensor = iterator.get_next()\n        with tf.Session() as sess:\n            ts = sess.run(tensor)\n    assert np.float32 == ts.double_col.dtype.type\n\n    converter2 = make_spark_converter(df, dtype=\'float64\')\n    with converter2.make_tf_dataset() as dataset:\n        iterator = dataset.make_one_shot_iterator()\n        tensor = iterator.get_next()\n        with tf.Session() as sess:\n            ts = sess.run(tensor)\n    assert np.float64 == ts.float_col.dtype.type\n\n    converter3 = make_spark_converter(df, dtype=None)\n    with converter3.make_tf_dataset() as dataset:\n        iterator = dataset.make_one_shot_iterator()\n        tensor = iterator.get_next()\n        with tf.Session() as sess:\n            ts = sess.run(tensor)\n    assert np.float32 == ts.float_col.dtype.type\n    assert np.float64 == ts.double_col.dtype.type\n\n    with pytest.raises(ValueError, match=""dtype float16 is not supported. \\\n            Use \'float32\' or float64""):\n        make_spark_converter(df, dtype=""float16"")\n\n\n@create_tf_graph\ndef test_array(spark_test_ctx):\n    df = spark_test_ctx.spark.createDataFrame(\n        [([1., 2., 3.],),\n         ([4., 5., 6.],)],\n        StructType([\n            StructField(name=\'c1\', dataType=ArrayType(DoubleType()))\n        ])\n    )\n    converter1 = make_spark_converter(df)\n    with converter1.make_tf_dataset() as dataset:\n        iterator = dataset.make_one_shot_iterator()\n        tensor = iterator.get_next()\n        with tf.Session() as sess:\n            ts = sess.run(tensor)\n    assert np.float32 == ts.c1.dtype.type\n\n\n@pytest.mark.skipif(\n    LooseVersion(pyspark.__version__) < LooseVersion(""3.0""),\n    reason=""Vector columns are not supported for pyspark {} < 3.0.0""\n    .format(pyspark.__version__))\n@create_tf_graph\ndef test_vector_to_array(spark_test_ctx):\n    from pyspark.ml.linalg import Vectors\n    from pyspark.mllib.linalg import Vectors as OldVectors\n    df = spark_test_ctx.spark.createDataFrame([\n        (Vectors.dense(1.0, 2.0, 3.0), OldVectors.dense(10.0, 20.0, 30.0)),\n        (Vectors.dense(5.0, 6.0, 7.0), OldVectors.dense(50.0, 60.0, 70.0))\n    ], [""vec"", ""oldVec""])\n    converter1 = make_spark_converter(df)\n    with converter1.make_tf_dataset(num_epochs=1) as dataset:\n        iterator = dataset.make_one_shot_iterator()\n        tensor = iterator.get_next()\n        with tf.Session() as sess:\n            ts = sess.run(tensor)\n    assert np.float32 == ts.vec.dtype.type\n    assert np.float32 == ts.oldVec.dtype.type\n    vec_col = ts.vec[ts.vec[:, 0].argsort()]\n    old_vec_col = ts.oldVec[ts.oldVec[:, 0].argsort()]\n    assert (2, 3) == ts.vec.shape\n    assert (2, 3) == ts.oldVec.shape\n    assert ([1., 2., 3.] == vec_col[0]).all() and \\\n           ([5., 6., 7.] == vec_col[1]).all()\n    assert ([10., 20., 30.] == old_vec_col[0]).all() and \\\n           ([50., 60., 70] == old_vec_col[1]).all()\n\n\ndef test_torch_primitive(spark_test_ctx):\n    import torch\n\n    schema = StructType([\n        StructField(""bool_col"", BooleanType(), False),\n        StructField(""float_col"", FloatType(), False),\n        StructField(""double_col"", DoubleType(), False),\n        StructField(""short_col"", ShortType(), False),\n        StructField(""int_col"", IntegerType(), False),\n        StructField(""long_col"", LongType(), False),\n        StructField(""byte_col"", ByteType(), False),\n    ])\n    df = spark_test_ctx.spark.createDataFrame(\n        [(True, 0.12, 432.1, 5, 5, 0, -128),\n         (False, 123.45, 0.987, 9, 908, 765, 127)],\n        schema=schema).coalesce(1)\n    # If we use numPartition > 1, the order of the loaded dataset would\n    # be non-deterministic.\n    expected_df = df.collect()\n\n    converter = make_spark_converter(df)\n    batch = None\n    with converter.make_torch_dataloader(num_epochs=1) as dataloader:\n        for i, batch in enumerate(dataloader):\n            # default batch_size = 1\n            for col in df.schema.names:\n                actual_ele = batch[col][0]\n                expected_ele = expected_df[i][col]\n                if col == ""float_col"" or col == ""double_col"":\n                    # Note that the default dtype is float32\n                    assert pytest.approx(expected_ele, rel=1e-6) == actual_ele\n                else:\n                    assert expected_ele == actual_ele\n\n        assert len(expected_df) == len(converter)\n    assert torch.uint8 == batch[""bool_col""].dtype\n    assert torch.int8 == batch[""byte_col""].dtype\n    assert torch.float32 == batch[""double_col""].dtype\n    assert torch.float32 == batch[""float_col""].dtype\n    assert torch.int32 == batch[""int_col""].dtype\n    assert torch.int64 == batch[""long_col""].dtype\n    assert torch.int16 == batch[""short_col""].dtype\n\n\ndef test_torch_pickling_remotely(spark_test_ctx):\n    df1 = spark_test_ctx.spark.range(100, 101)\n    converter1 = make_spark_converter(df1)\n\n    def map_fn(_):\n        with converter1.make_torch_dataloader(num_epochs=1) as dataloader:\n            for batch in dataloader:\n                ret = batch[""id""][0]\n        return ret\n\n    result = spark_test_ctx.spark.sparkContext.parallelize(range(1), 1) \\\n        .map(map_fn).collect()[0]\n    assert result == 100\n\n\ndef test_torch_batch_size(spark_test_ctx):\n    df = spark_test_ctx.spark.range(8)\n    conv = make_spark_converter(df)\n    batch_size = 2\n    with conv.make_torch_dataloader(batch_size=batch_size,\n                                    num_epochs=1) as dataloader:\n        for batch in dataloader:\n            assert batch_size == batch[\'id\'].shape[0]\n\n\ndef test_torch_transform_spec(spark_test_ctx):\n    df = spark_test_ctx.spark.range(8)\n    conv = make_spark_converter(df)\n\n    from torchvision import transforms\n    from petastorm import TransformSpec\n\n    def _transform_row(df_row):\n        scale_tranform = transforms.Compose([\n            transforms.Lambda(lambda x: x * 0.1),\n        ])\n        return scale_tranform(df_row)\n\n    transform = TransformSpec(_transform_row)\n    with conv.make_torch_dataloader(transform_spec=transform,\n                                    num_epochs=1) as dataloader:\n        for batch in dataloader:\n            assert min(batch[\'id\']) >= 0 and max(batch[\'id\']) < 1\n\n\ndef test_torch_unexpected_param(spark_test_ctx):\n    df = spark_test_ctx.spark.range(8)\n    conv = make_spark_converter(df)\n\n    with pytest.raises(TypeError, match=""unexpected keyword argument \'xyz\'""):\n        with conv.make_torch_dataloader(xyz=1) as _:\n            pass\n\n\n@mock.patch(\'petastorm.spark.spark_dataset_converter.make_batch_reader\')\ndef test_torch_dataloader_advanced_params(mock_torch_make_batch_reader, spark_test_ctx):\n    SHARD_COUNT = 3\n    df = spark_test_ctx.spark.range(100).repartition(SHARD_COUNT)\n    conv = make_spark_converter(df)\n\n    mock_torch_make_batch_reader.return_value = \\\n        make_batch_reader(conv.cache_dir_url)\n\n    with conv.make_torch_dataloader(reader_pool_type=\'dummy\', cur_shard=1,\n                                    shard_count=SHARD_COUNT) as _:\n        pass\n    peta_args = mock_torch_make_batch_reader.call_args.kwargs\n    assert peta_args[\'reader_pool_type\'] == \'dummy\' and \\\n        peta_args[\'cur_shard\'] == 1 and \\\n        peta_args[\'shard_count\'] == SHARD_COUNT and \\\n        peta_args[\'num_epochs\'] is None and \\\n        peta_args[\'workers_count\'] == 4\n\n    # Test default value overridden arguments.\n    with conv.make_torch_dataloader(num_epochs=1, workers_count=2) as _:\n        pass\n    peta_args = mock_torch_make_batch_reader.call_args.kwargs\n    assert peta_args[\'num_epochs\'] == 1 and peta_args[\'workers_count\'] == 2\n\n\ndef test_wait_file_available(spark_test_ctx):\n    pq_dir = os.path.join(spark_test_ctx.tempdir, \'test_ev\')\n    os.makedirs(pq_dir)\n    file1_path = os.path.join(pq_dir, \'file1\')\n    file2_path = os.path.join(pq_dir, \'file2\')\n    url1 = \'file://\' + file1_path.replace(os.sep, \'/\')\n    url2 = \'file://\' + file2_path.replace(os.sep, \'/\')\n\n    url_list = [url1, url2]\n\n    def create_file(p):\n        with open(p, \'w\'):\n            pass\n\n    # 1. test all files exists.\n    create_file(file1_path)\n    create_file(file2_path)\n    _wait_file_available(url_list)\n\n    # 2. test one file does not exists. Raise error.\n    os.remove(file2_path)\n    with pytest.raises(RuntimeError,\n                       match=\'Timeout while waiting for all parquet-store files to appear at urls\'):\n        _wait_file_available(url_list)\n\n    # 3. test one file accessible after 1 second.\n    def delay_create_file2():\n        time.sleep(1)\n        create_file(file2_path)\n\n    threading.Thread(target=delay_create_file2()).start()\n\n    _wait_file_available(url_list)\n\n\ndef test_check_dataset_file_median_size(spark_test_ctx, caplog):\n    file_size_map = {\n        \'/a/b/01.parquet\': 30,\n        \'/a/b/02.parquet\': 40,\n        \'/a/b/03.parquet\': 50,\n        \'/a/b/04.parquet\': 60,\n        \'/a/b/05.parquet\': 999000,\n    }\n    with mock.patch(\'os.path.getsize\') as mock_path_get_size:\n        mock_path_get_size.side_effect = lambda p: file_size_map[p]\n        url_list = [\'file://\' + path for path in file_size_map.keys()]\n        caplog.clear()\n        _check_dataset_file_median_size(url_list)\n        assert \'The median size\' in "" "".join(caplog.messages)\n\n        for k in file_size_map:\n            file_size_map[k] *= (1024 * 1024)\n        caplog.clear()\n        _check_dataset_file_median_size(url_list)\n        assert \'The median size\' not in "" "".join(caplog.messages)\n\n        file_size_map = {\'/a/b/01.parquet\': 29}\n        url_list = [\'file:///a/b/01.parquet\']\n        caplog.clear()\n        _check_dataset_file_median_size(url_list)\n        assert \'The median size\' not in "" "".join(caplog.messages)\n\n\n@mock.patch.dict(os.environ, {\'DATABRICKS_RUNTIME_VERSION\': \'7.0\'}, clear=True)\ndef test_check_parent_cache_dir_url(spark_test_ctx, caplog):\n    def log_warning_occur():\n        return \'you should specify a dbfs fuse path\' in \'\\n\'.join([r.message for r in caplog.records])\n    with mock.patch(\'petastorm.spark.spark_dataset_converter._is_spark_local_mode\') as mock_is_local:\n        mock_is_local.return_value = False\n        caplog.clear()\n        _check_parent_cache_dir_url(\'file:/dbfs/a/b\')\n        assert not log_warning_occur()\n        caplog.clear()\n        _check_parent_cache_dir_url(\'file:/a/b\')\n        assert log_warning_occur()\n        mock_is_local.return_value = True\n        caplog.clear()\n        _check_parent_cache_dir_url(\'file:/dbfs/a/b\')\n        assert not log_warning_occur()\n        caplog.clear()\n        _check_parent_cache_dir_url(\'file:/a/b\')\n        assert not log_warning_occur()\n\n\ndef test_get_spark_session_safe_check(spark_test_ctx):\n    def map_fn(_):\n        _get_spark_session()\n        return 0\n\n    with pytest.raises(py4j.protocol.Py4JJavaError):\n        spark_test_ctx.spark.sparkContext.parallelize(range(1), 1).map(map_fn).collect()\n'"
petastorm/tests/test_spark_session_cli.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport argparse\n\nimport pytest\nfrom pyspark.sql import SparkSession\n\nfrom petastorm.tools.spark_session_cli import add_configure_spark_arguments, configure_spark\n\n\n@pytest.fixture(scope=\'session\')\ndef configured_arg_parser():\n    parser = argparse.ArgumentParser()\n    add_configure_spark_arguments(parser)\n    return parser\n\n\ndef test_default_values(configured_arg_parser):\n    args = configured_arg_parser.parse_args([])\n    assert args.master is None\n    assert not args.spark_session_config\n\n\ndef test_some_values(configured_arg_parser):\n    args = configured_arg_parser.parse_args([\'--master\', \'local\', \'--spark-session-config\', \'a=1\', \'b=2\'])\n    assert args.master == \'local\'\n    assert args.spark_session_config == [\'a=1\', \'b=2\']\n\n\ndef test_session_config(configured_arg_parser):\n    args = configured_arg_parser.parse_args([\'--master\', \'local[1]\', \'--spark-session-config\', \'a=1\', \'b=2\'])\n    spark = configure_spark(SparkSession.builder, args).getOrCreate()\n    assert spark.conf.get(\'a\') == \'1\'\n    assert spark.conf.get(\'b\') == \'2\'\n    assert spark.conf.get(\'spark.master\') == \'local[1]\'\n\n\ndef test_unconfigured_argparser():\n    args = argparse.ArgumentParser().parse_args([])\n    with pytest.raises(RuntimeError, match=\'add_configure_spark_arguments\'):\n        configure_spark(SparkSession.builder, args)\n\n\ndef test_invalid_key_value_setting(configured_arg_parser):\n    args = configured_arg_parser.parse_args([\'--spark-session-config\', \'WRONG FORMAT\', \'b=2\', \'--master\', \'local[1]\'])\n    with pytest.raises(ValueError, match=\'key=value\'):\n        configure_spark(SparkSession.builder, args)\n'"
petastorm/tests/test_spark_utils.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport glob\nimport os\nimport unittest\nfrom shutil import rmtree\nfrom tempfile import mkdtemp\n\nimport numpy as np\nfrom pyspark.sql import SparkSession\n\nfrom petastorm.spark_utils import dataset_as_rdd\nfrom petastorm.tests.test_common import create_test_dataset, TestSchema\n\n\nclass TestSparkUtils(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        """"""Initializes dataset once per test. All tests in this class will use the same fake dataset.""""""\n        # Write a fake dataset to this location\n        cls._dataset_dir = mkdtemp(\'end_to_end_petastorm\')\n        cls._dataset_url = \'file://{}\'.format(cls._dataset_dir)\n        ROWS_COUNT = 1000\n        cls._dataset_dicts = create_test_dataset(cls._dataset_url, range(ROWS_COUNT))\n\n        # Remove crc files due to https://issues.apache.org/jira/browse/HADOOP-7199\n        for crc_file in glob.glob(cls._dataset_dir + \'/.*.crc\'):\n            os.remove(crc_file)\n\n    @classmethod\n    def tearDownClass(cls):\n        # Remove everything created with ""get_temp_dir""\n        rmtree(cls._dataset_dir)\n\n    def _get_spark_session(self):\n        return SparkSession \\\n            .builder \\\n            .appName(\'petastorm_spark_utils_test\') \\\n            .master(\'local[*]\')\\\n            .getOrCreate()\n\n    def test_simple_read_rdd(self):\n        """"""Read dataset into spark rdd. Collects and makes sure they all return as expected""""""\n        spark = self._get_spark_session()\n        rows = dataset_as_rdd(self._dataset_url, spark).collect()\n\n        for row in rows:\n            actual = dict(row._asdict())\n            expected = next(d for d in self._dataset_dicts if d[\'id\'] == actual[\'id\'])\n            np.testing.assert_equal(expected, actual)\n\n        spark.stop()\n\n    def test_reading_subset_of_columns(self):\n        """"""Read subset of dataset fields into spark rdd. Collects and makes sure they all return as expected""""""\n        spark = self._get_spark_session()\n        rows = dataset_as_rdd(self._dataset_url, spark, schema_fields=[TestSchema.id2, TestSchema.id]).collect()\n\n        for row in rows:\n            actual = dict(row._asdict())\n            expected = next(d for d in self._dataset_dicts if d[\'id\'] == actual[\'id\'])\n            np.testing.assert_equal(expected[\'id2\'], actual[\'id2\'])\n\n        spark.stop()\n\n\nif __name__ == \'__main__\':\n    # Delegate to the test framework.\n    unittest.main()\n'"
petastorm/tests/test_tf_autograph.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\n\nfrom petastorm.spark import make_spark_converter\nfrom petastorm.tests.test_tf_utils import _IS_TF_VERSION_1\n\n\n@pytest.mark.skipif(_IS_TF_VERSION_1, reason=""Only test autograph transform on tensorflow>=2"")\ndef test_tf_autograph(spark_test_ctx, caplog):\n    caplog.clear()\n    df1 = spark_test_ctx.spark.range(100)\n    converter1 = make_spark_converter(df1)\n    results = []\n    with converter1.make_tf_dataset(num_epochs=1) as dataset:\n        for batch in dataset:\n            results.append(batch)\n    assert ""AutoGraph could not transform"" not in "" "".join(caplog.messages)\n'"
petastorm/tests/test_tf_dataset.py,12,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport functools\nimport operator\nfrom copy import copy\n\nimport numpy as np\nimport pytest\nimport tensorflow.compat.v1 as tf  # pylint: disable=import-error\n\nfrom petastorm import make_reader, make_batch_reader\nfrom petastorm.ngram import NGram\nfrom petastorm.predicates import in_lambda\nfrom petastorm.tests.test_common import TestSchema\nfrom petastorm.tests.test_tf_utils import create_tf_graph\nfrom petastorm.tf_utils import make_petastorm_dataset\n\n_EXCLUDE_FIELDS = set(TestSchema.fields.values()) \\\n                  - {TestSchema.matrix_nullable, TestSchema.string_array_nullable, TestSchema.decimal,\n                     TestSchema.integer_nullable}\n\nMINIMAL_READER_FLAVOR_FACTORIES = [\n    lambda url, **kwargs: make_reader(url, **_merge_params({\'reader_pool_type\': \'dummy\',\n                                                            \'schema_fields\': _EXCLUDE_FIELDS}, kwargs)),\n]\n\nALL_READER_FLAVOR_FACTORIES = MINIMAL_READER_FLAVOR_FACTORIES + [\n    lambda url, **kwargs: make_reader(url, **_merge_params({\'reader_pool_type\': \'thread\', \'workers_count\': 1,\n                                                            \'schema_fields\': _EXCLUDE_FIELDS}, kwargs)),\n    lambda url, **kwargs: make_reader(url, **_merge_params({\'reader_pool_type\': \'process\', \'workers_count\': 1,\n                                                            \'schema_fields\': _EXCLUDE_FIELDS}, kwargs)),\n]\n\n\ndef _merge_params(base, overwrite):\n    """"""Merges two dictionaries when values from ``overwrite`` takes precedence over values of ``base`` dictionary.\n\n    Both input parameters are not modified.\n\n    :param base: A dictionary\n    :param overwrite: A dictionary. If a value with the same key exists in ``base``, it is overwritten by the value from\n      this dictionary.\n    :return: A combined dictionary\n    """"""\n    # Create a shallow copy of base\n    combined = copy(base)\n    combined.update(overwrite)\n    return combined\n\n\n@pytest.mark.forked\n@pytest.mark.parametrize(\'reader_factory\', ALL_READER_FLAVOR_FACTORIES)\n@create_tf_graph\ndef test_with_one_shot_iterator(synthetic_dataset, reader_factory):\n    """"""Just a bunch of read and compares of all values to the expected values""""""\n    with reader_factory(synthetic_dataset.url) as reader:\n        dataset = make_petastorm_dataset(reader)\n        iterator = dataset.make_one_shot_iterator()\n\n        # Make sure we have static shape info for all fields\n        for shape in dataset.output_shapes:\n            # TODO(yevgeni): check that the shapes are actually correct, not just not None\n            assert shape.dims is not None\n\n        # Read a bunch of entries from the dataset and compare the data to reference\n        with tf.Session() as sess:\n            iterator = iterator.get_next()\n            for _, _ in enumerate(synthetic_dataset.data):\n                actual = sess.run(iterator)._asdict()\n                expected = next(d for d in synthetic_dataset.data if d[\'id\'] == actual[\'id\'])\n                for key in actual.keys():\n                    if isinstance(expected[key], str):\n                        # Tensorflow returns all strings as bytes in python3. So we will need to decode it\n                        actual_value = actual[key].decode()\n                    elif isinstance(expected[key], np.ndarray) and expected[key].dtype.type == np.unicode_:\n                        actual_value = np.array([item.decode() for item in actual[key]])\n                    else:\n                        actual_value = actual[key]\n\n                    np.testing.assert_equal(actual_value, expected[key])\n\n            # Exhausted one full epoch. Fetching next value should trigger OutOfRangeError\n            with pytest.raises(tf.errors.OutOfRangeError):\n                sess.run(iterator)\n\n\n@pytest.mark.forked\n@pytest.mark.parametrize(\'reader_factory\', ALL_READER_FLAVOR_FACTORIES)\n@create_tf_graph\ndef test_with_dataset_repeat(synthetic_dataset, reader_factory):\n    """"""``tf.data.Dataset``\'s ``repeat`` should not be used on ``make_petastorm_dataset`` due to high costs of\n    ``Reader initialization``. A user should use ``Reader`` built-in epochs support. Check that we raise an\n    error to alert of misuse.""""""\n    with reader_factory(synthetic_dataset.url) as reader:\n        dataset = make_petastorm_dataset(reader)\n\n        dataset = dataset.repeat(2)\n\n        iterator = dataset.make_one_shot_iterator()\n\n        # Read a bunch of entries from the dataset and compare the data to reference\n        with tf.Session() as sess:\n            iterator = iterator.get_next()\n\n            for _, _ in enumerate(synthetic_dataset.data):\n                sess.run(iterator)\n\n            with pytest.raises(tf.errors.UnknownError, match=r\'.*Multiple iterations.*\'):\n                sess.run(iterator)\n\n\n@pytest.mark.forked\n@pytest.mark.parametrize(\'reader_factory\', ALL_READER_FLAVOR_FACTORIES)\n@create_tf_graph\ndef test_some_processing_functions(synthetic_dataset, reader_factory):\n    """"""Try several ``tf.data.Dataset`` dataset operations on make_petastorm_dataset""""""\n\n    # reader1 will have a single row with id=1, reader2: a single row with id=2\n\n    # Using functools.partial(_eq, 1)) which is equivalent to lambda x: x==1 because standard python pickle\n    # can not pickle this lambda\n    with reader_factory(synthetic_dataset.url,\n                        predicate=in_lambda([\'id\'], functools.partial(operator.eq, 1))) as reader1:\n        with reader_factory(synthetic_dataset.url,\n                            predicate=in_lambda([\'id\'], functools.partial(operator.eq, 2))) as reader2:\n            dataset = make_petastorm_dataset(reader1) \\\n                .prefetch(10) \\\n                .concatenate(make_petastorm_dataset(reader2)) \\\n                .map(lambda x: x.id) \\\n                .batch(2)\n\n            next_sample = dataset.make_one_shot_iterator().get_next()\n\n            with tf.Session() as sess:\n                # \'actual\' is expected to be content of id column of a concatenated dataset\n                actual = sess.run(next_sample)\n                np.testing.assert_array_equal(actual, [1, 2])\n\n\n@pytest.mark.parametrize(\'reader_factory\', MINIMAL_READER_FLAVOR_FACTORIES)\n@create_tf_graph\ndef test_dataset_with_ngrams(synthetic_dataset, reader_factory):\n    ngram = NGram({-1: [TestSchema.id, TestSchema.image_png], 2: [TestSchema.id]}, 100, TestSchema.id)\n    with reader_factory(synthetic_dataset.url, schema_fields=ngram, num_epochs=1) as reader:\n        dataset = make_petastorm_dataset(reader)\n        next_sample = dataset.make_one_shot_iterator().get_next()\n        with tf.Session() as sess:\n            while True:\n                try:\n                    actual = sess.run(next_sample)\n                    assert actual[-1].id + 3 == actual[2].id\n                    assert np.all(actual[-1].image_png.shape > (10, 10, 3))\n                except tf.errors.OutOfRangeError:\n                    break\n\n\n@pytest.mark.forked\n@create_tf_graph\ndef test_non_petastorm_with_many_colums_with_one_shot_iterator(many_columns_non_petastorm_dataset):\n    """"""Just a bunch of read and compares of all values to the expected values""""""\n    with make_batch_reader(many_columns_non_petastorm_dataset.url, workers_count=1) as reader:\n        dataset = make_petastorm_dataset(reader)\n        iterator = dataset.make_one_shot_iterator()\n\n        # Make sure we have static shape info for all fields\n        for shape in dataset.output_shapes:\n            # TODO(yevgeni): check that the shapes are actually correct, not just not None\n            assert shape.dims is not None\n\n        # Read a bunch of entries from the dataset and compare the data to reference\n        with tf.Session() as sess:\n            get_next = iterator.get_next()\n            sample = sess.run(get_next)._asdict()\n            assert set(sample.keys()) == set(many_columns_non_petastorm_dataset.data[0].keys())\n\n\n@pytest.mark.forked\n@create_tf_graph\ndef test_non_petastorm_with_many_colums_epoch_count(many_columns_non_petastorm_dataset):\n    """"""Just a bunch of read and compares of all values to the expected values""""""\n    expected_num_epochs = 4\n    with make_batch_reader(many_columns_non_petastorm_dataset.url, workers_count=1,\n                           num_epochs=expected_num_epochs) as reader:\n        dataset = make_petastorm_dataset(reader)\n        iterator = dataset.make_one_shot_iterator()\n\n        # Read a bunch of entries from the dataset and compare the data to reference\n        with tf.Session() as sess:\n            get_next = iterator.get_next()\n            rows_count = 0\n            while True:\n                try:\n                    sample = sess.run(get_next)._asdict()\n                    rows_count += sample[\'col_0\'].shape[0]\n                except tf.errors.OutOfRangeError:\n                    break\n\n            assert expected_num_epochs * len(many_columns_non_petastorm_dataset.data) == rows_count\n'"
petastorm/tests/test_tf_utils.py,12,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom __future__ import division\n\nimport datetime\nfrom calendar import timegm\nfrom collections import namedtuple\nfrom contextlib import contextmanager\nfrom decimal import Decimal\nfrom functools import wraps\nimport inspect\n\nimport numpy as np\nfrom packaging import version\nimport pytest\nimport tensorflow.compat.v1 as tf  # pylint: disable=import-error\n\ntry:\n    from mock import mock\nexcept ImportError:\n    from unittest import mock\n\nfrom petastorm import make_reader, make_batch_reader, TransformSpec\nfrom petastorm.ngram import NGram\nfrom petastorm.tests.test_common import TestSchema\nfrom petastorm.tf_utils import _sanitize_field_tf_types, _numpy_to_tf_dtypes, \\\n    _schema_to_tf_dtypes, tf_tensors, _ngrams_generator, _unflatten_and_set_shape\nfrom petastorm.unischema import Unischema, UnischemaField\n\nNON_NULLABLE_FIELDS = set(TestSchema.fields.values()) - \\\n                      {TestSchema.matrix_nullable, TestSchema.string_array_nullable, TestSchema.integer_nullable}\n\n\n_IS_TF_VERSION_1 = version.parse(tf.__version__) < version.parse(\'2\')\n_IS_TF_VERSION_2 = version.parse(tf.__version__) >= version.parse(\'2\')\n\n\ndef create_tf_graph(func):\n    def run_func_with_tf_graph(*args, **kwargs):\n        with tf.Graph().as_default():\n            return func(*args, **kwargs)\n    func_args = \',\'.join(inspect.getargspec(func).args)  # pylint: disable=deprecated-method\n    # add a lambda wrap in order to keep the function argument list unchanged.\n    # otherwise some other pytest decorator like @pytest.mark.parametrize may not work correctly in python2\n    # pylint: disable=eval-used\n    warpped_fn = eval(""lambda {func_args}: f({func_args})"".format(func_args=func_args), {\'f\': run_func_with_tf_graph})\n    return wraps(func)(warpped_fn)\n\n\n@contextmanager\ndef _tf_session():\n    with tf.Session() as sess:\n        sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(coord=coord, start=True)\n\n        yield sess\n\n        coord.request_stop()\n        coord.join(threads)\n\n\ndef test_sanitize_field_tf_types():\n    expected_datetime_array = [datetime.date(1970, 1, 1), datetime.date(2015, 9, 29)]\n    expected_datetime_ns_from_epoch = [timegm(dt.timetuple()) * 1000000000 for dt in expected_datetime_array]\n    assert expected_datetime_ns_from_epoch[0] == 0\n\n    sample_input_dict = {\n        \'int32\': np.asarray([-2 ** 31, 0, 100, 2 ** 31 - 1], dtype=np.int32),\n        \'uint16\': np.asarray([0, 2, 2 ** 16 - 1], dtype=np.uint16),\n        \'uint32\': np.asarray([0, 2, 2 ** 32 - 1], dtype=np.uint32),\n        \'Decimal\': Decimal(1234) / Decimal(10),\n        \'array_of_datetime_date\': np.asarray(expected_datetime_array),\n        \'array_of_np_datetime_64\': np.asarray(expected_datetime_array).astype(np.datetime64),\n    }\n\n    TestNamedTuple = namedtuple(\'TestNamedTuple\', sample_input_dict.keys())\n    sample_input_tuple = TestNamedTuple(**sample_input_dict)\n    sanitized_tuple = _sanitize_field_tf_types(sample_input_tuple)\n\n    np.testing.assert_equal(sanitized_tuple.int32.dtype, np.int32)\n    np.testing.assert_equal(sanitized_tuple.uint16.dtype, np.int32)\n    np.testing.assert_equal(sanitized_tuple.uint32.dtype, np.int64)\n    assert isinstance(sanitized_tuple.Decimal, str)\n\n    np.testing.assert_equal(sanitized_tuple.int32, sample_input_dict[\'int32\'])\n    np.testing.assert_equal(sanitized_tuple.uint16, sample_input_dict[\'uint16\'])\n    np.testing.assert_equal(sanitized_tuple.uint32, sample_input_dict[\'uint32\'])\n    np.testing.assert_equal(str(sanitized_tuple.Decimal), str(sample_input_dict[\'Decimal\'].normalize()))\n\n    np.testing.assert_equal(sanitized_tuple.array_of_datetime_date, expected_datetime_ns_from_epoch)\n    np.testing.assert_equal(sanitized_tuple.array_of_np_datetime_64, expected_datetime_ns_from_epoch)\n\n\ndef test_decimal_conversion():\n    assert _numpy_to_tf_dtypes(Decimal) == tf.string\n\n\ndef test_uint16_promotion_to_int32():\n    assert _numpy_to_tf_dtypes(np.uint16) == tf.int32\n\n\ndef test_unknown_type():\n    with pytest.raises(ValueError):\n        _numpy_to_tf_dtypes(np.uint64)\n\n\n@mock.patch(\'petastorm.unischema._UNISCHEMA_FIELD_ORDER\', \'alphabetical\')\ndef test_schema_to_dtype_list():\n    TestSchema = Unischema(\'TestSchema\', [\n        UnischemaField(\'int32\', np.int32, (), None, False),\n        UnischemaField(\'uint8\', np.uint8, (), None, False),\n        UnischemaField(\'uint16\', np.uint16, (), None, False),\n        UnischemaField(\'Decimal\', Decimal, (), None, False),\n    ])\n\n    actual_tf_dtype_list = _schema_to_tf_dtypes(TestSchema)\n    # Note that the order of the fields is defined by alphabetical order of keys and always sorted by Unischema\n    # to avoid ambiguity\n    #  [Decimal,   int32,    uint16,   uint8] <- alphabetical order\n    #  [tf.string, tf.int32, tf.int32, tf.uint8]\n    np.testing.assert_equal(actual_tf_dtype_list, [tf.string, tf.int32, tf.int32, tf.uint8])\n\n\n@create_tf_graph\ndef _read_from_tf_tensors(synthetic_dataset, count, shuffling_queue_capacity, min_after_dequeue, ngram):\n    """"""Used by several test cases. Reads a \'count\' rows using reader.\n\n    The reader is configured without row-group shuffling and guarantees deterministic order of rows up to the\n    results queue TF shuffling which is controlled by \'shuffling_queue_capacity\', \'min_after_dequeue\' arguments.\n\n    The function returns a tuple with: (actual data read from the dataset, a TF tensor returned by the reader)\n    """"""\n\n    schema_fields = (NON_NULLABLE_FIELDS if ngram is None else ngram)\n\n    with make_reader(schema_fields=schema_fields, dataset_url=synthetic_dataset.url, reader_pool_type=\'dummy\',\n                     shuffle_row_groups=False) as reader:\n        row_tensors = tf_tensors(reader, shuffling_queue_capacity=shuffling_queue_capacity,\n                                 min_after_dequeue=min_after_dequeue)\n        with _tf_session() as sess:\n            rows_data = [sess.run(row_tensors) for _ in range(count)]\n\n    return rows_data, row_tensors\n\n\ndef _assert_all_tensors_have_shape(row_tensors):\n    """"""Asserts that all elements in row_tensors list/tuple have static shape.""""""\n    for column in row_tensors:\n        assert column.get_shape().dims is not None\n\n\ndef _assert_fields_eq(actual, desired):\n    if isinstance(desired, Decimal) or isinstance(actual, bytes):\n        # Tensorflow returns all strings as bytes in python3. So we will need to decode it\n        actual = actual.decode()\n    elif isinstance(desired, np.ndarray) and desired.dtype.type == np.unicode_:\n        actual = np.array([item.decode() for item in actual])\n\n    if isinstance(desired, Decimal):\n        np.testing.assert_equal(Decimal(actual), desired)\n    elif issubclass(desired.dtype.type, np.datetime64):\n        # tf_utils will convert timestamps to ns from epoch int64 value.\n        assert desired.astype(\'<M8[ns]\').astype(np.int64) == actual\n    else:\n        np.testing.assert_equal(actual, desired)\n\n\ndef _assert_expected_rows_data(expected_data, rows_data):\n    """"""Asserts all elements of rows_data list of rows match reference data used to create the dataset""""""\n    for row_tuple in rows_data:\n\n        # It is easier to work with dict as we will be indexing column names using strings\n        row = row_tuple._asdict()\n\n        # Find corresponding row in the reference data\n        expected = next(d for d in expected_data if d[\'id\'] == row[\'id\'])\n\n        # Check equivalence of all values between a checked row and a row from reference data\n        for column_name, actual in row.items():\n            expected_val = expected[column_name]\n            _assert_fields_eq(actual, expected_val)\n\n\n@pytest.mark.forked\n@create_tf_graph\ndef test_simple_read_tensorflow(synthetic_dataset):\n    """"""Read couple of rows. Make sure all tensors have static shape sizes assigned and the data matches reference\n    data""""""\n    with make_reader(schema_fields=NON_NULLABLE_FIELDS, dataset_url=synthetic_dataset.url) as reader:\n        row_tensors = tf_tensors(reader)\n        with _tf_session() as sess:\n            rows_data = [sess.run(row_tensors) for _ in range(30)]\n\n    # Make sure we have static shape info for all fields\n    _assert_all_tensors_have_shape(row_tensors)\n    _assert_expected_rows_data(synthetic_dataset.data, rows_data)\n\n\n@pytest.mark.forked\ndef test_shuffling_queue(synthetic_dataset):\n    """"""Read data without tensorflow shuffling queue and with it. Check the the order is deterministic within\n    unshuffled read and is random with shuffled read""""""\n    unshuffled_1, _ = _read_from_tf_tensors(synthetic_dataset, 30, shuffling_queue_capacity=0, min_after_dequeue=0,\n                                            ngram=None)\n    unshuffled_2, _ = _read_from_tf_tensors(synthetic_dataset, 30, shuffling_queue_capacity=0, min_after_dequeue=0,\n                                            ngram=None)\n\n    shuffled_1, shuffled_1_row_tensors = \\\n        _read_from_tf_tensors(synthetic_dataset, 30, shuffling_queue_capacity=10, min_after_dequeue=9, ngram=None)\n    shuffled_2, _ = \\\n        _read_from_tf_tensors(synthetic_dataset, 30, shuffling_queue_capacity=10, min_after_dequeue=9, ngram=None)\n\n    # Make sure we have static shapes and the data matches reference data (important since a different code path\n    # is executed within tf_tensors when shuffling is specified\n    _assert_all_tensors_have_shape(shuffled_1_row_tensors)\n    _assert_expected_rows_data(synthetic_dataset.data, shuffled_1)\n\n    assert [f.id for f in unshuffled_1] == [f.id for f in unshuffled_2]\n    assert [f.id for f in unshuffled_1] != [f.id for f in shuffled_2]\n    assert [f.id for f in shuffled_1] != [f.id for f in shuffled_2]\n\n\n@pytest.mark.forked\ndef test_simple_ngram_read_tensorflow(synthetic_dataset):\n    """"""Read a single ngram. Make sure all shapes are set and the data read matches reference data""""""\n    fields = {\n        0: [TestSchema.id],\n        1: [TestSchema.id],\n        2: [TestSchema.id]\n    }\n\n    # Expecting delta between ids to be 1. Setting 1.5 as upper bound\n    ngram = NGram(fields=fields, delta_threshold=1.5, timestamp_field=TestSchema.id)\n\n    ngrams, row_tensors_seq = \\\n        _read_from_tf_tensors(synthetic_dataset, 30, shuffling_queue_capacity=0, min_after_dequeue=0, ngram=ngram)\n\n    for row_tensors in row_tensors_seq.values():\n        _assert_all_tensors_have_shape(row_tensors)\n\n    for one_ngram_dict in ngrams:\n        _assert_expected_rows_data(synthetic_dataset.data, one_ngram_dict.values())\n\n\n@pytest.mark.forked\ndef test_shuffling_queue_with_ngrams(synthetic_dataset):\n    """"""Read data without tensorflow shuffling queue and with it (no rowgroup shuffling). Read ngrams\n    Check the the order is deterministic within unshuffled read and is random with shuffled read""""""\n    fields = {\n        0: [TestSchema.id],\n        1: [TestSchema.id],\n        2: [TestSchema.id]\n    }\n\n    # Expecting delta between ids to be 1. Setting 1.5 as upper bound\n    ngram = NGram(fields=fields, delta_threshold=1.5, timestamp_field=TestSchema.id)\n    unshuffled_1, _ = _read_from_tf_tensors(synthetic_dataset, 30, shuffling_queue_capacity=0, min_after_dequeue=0,\n                                            ngram=ngram)\n    unshuffled_2, _ = _read_from_tf_tensors(synthetic_dataset, 30, shuffling_queue_capacity=0, min_after_dequeue=0,\n                                            ngram=ngram)\n\n    shuffled_1, shuffled_1_ngram = \\\n        _read_from_tf_tensors(synthetic_dataset, 20, shuffling_queue_capacity=30, min_after_dequeue=29, ngram=ngram)\n    shuffled_2, _ = \\\n        _read_from_tf_tensors(synthetic_dataset, 20, shuffling_queue_capacity=30, min_after_dequeue=29, ngram=ngram)\n\n    # shuffled_1_ngram is a dictionary of named tuple indexed by time:\n    # {0: (tensor, tensor, tensor, ...),\n    #  1: (tensor, tensor, tensor, ...),\n    #  ...}\n    for row_tensor in shuffled_1_ngram.values():\n        _assert_all_tensors_have_shape(row_tensor)\n\n    # shuffled_1 is a list of dictionaries of named tuples indexed by time:\n    # [{0: (tensor, tensor, tensor, ...),\n    #  1: (tensor, tensor, tensor, ...),\n    #  ...}\n    # {0: (tensor, tensor, tensor, ...),\n    #  1: (tensor, tensor, tensor, ...),\n    #  ...},...\n    # ]\n    for one_ngram_dict in shuffled_1:\n        _assert_expected_rows_data(synthetic_dataset.data, one_ngram_dict.values())\n\n    def flatten(list_of_ngrams):\n        return [row for seq in list_of_ngrams for row in seq.values()]\n\n    assert [f.id for f in flatten(unshuffled_1)] == [f.id for f in flatten(unshuffled_2)]\n\n    assert [f.id for f in flatten(unshuffled_1)] != [f.id for f in flatten(shuffled_2)]\n    assert [f.id for f in flatten(shuffled_1)] != [f.id for f in flatten(shuffled_2)]\n\n\n@pytest.mark.forked\n@create_tf_graph\ndef test_simple_read_tensorflow_with_parquet_dataset(scalar_dataset):\n    """"""Read couple of rows. Make sure all tensors have static shape sizes assigned and the data matches reference\n    data""""""\n    with make_batch_reader(dataset_url_or_urls=scalar_dataset.url) as reader:\n        row_tensors = tf_tensors(reader)\n        row_tensors_dict = row_tensors._asdict()\n        # Make sure we have static shape info for all fields\n        for column_name in row_tensors_dict:\n            column = row_tensors_dict[column_name]\n            column_shape = column.get_shape().as_list()\n            if column_name == \'int_fixed_size_list\':\n                assert column_shape == [None, None]\n            else:\n                assert column_shape == [None]\n\n        with _tf_session() as sess:\n            for _ in range(2):\n                batch = sess.run(row_tensors)._asdict()\n                for i, id_value in enumerate(batch[\'id\']):\n                    expected_row = next(d for d in scalar_dataset.data if d[\'id\'] == id_value)\n                    for field_name in expected_row.keys():\n                        _assert_fields_eq(batch[field_name][i], expected_row[field_name])\n\n\n@pytest.mark.forked\n@create_tf_graph\ndef test_simple_read_tensorflow_with_non_petastorm_many_columns_dataset(many_columns_non_petastorm_dataset):\n    """"""Read couple of rows. Make sure all tensors have static shape sizes assigned and the data matches reference\n    data""""""\n    with make_batch_reader(dataset_url_or_urls=many_columns_non_petastorm_dataset.url) as reader:\n        row_tensors = tf_tensors(reader)\n        # Make sure we have static shape info for all fields\n        for column in row_tensors:\n            assert column.get_shape().as_list() == [None]\n\n        with _tf_session() as sess:\n            batch = sess.run(row_tensors)._asdict()\n            assert set(batch.keys()) == set(many_columns_non_petastorm_dataset.data[0].keys())\n\n\n@create_tf_graph\ndef test_shuffling_queue_with_make_batch_reader(scalar_dataset):\n    with make_batch_reader(dataset_url_or_urls=scalar_dataset.url) as reader:\n        with pytest.raises(ValueError):\n            tf_tensors(reader, 100, 90)\n\n\n@mock.patch(\'petastorm.unischema._UNISCHEMA_FIELD_ORDER\', \'alphabetical\')\n@create_tf_graph\ndef test_transform_function_new_field(synthetic_dataset):\n    def double_matrix(sample):\n        sample[\'double_matrix\'] = sample[\'matrix\'] * 2\n        del sample[\'matrix\']\n        return sample\n\n    with make_reader(synthetic_dataset.url, reader_pool_type=\'dummy\', schema_fields=[TestSchema.id, TestSchema.matrix],\n                     transform_spec=TransformSpec(double_matrix,\n                                                  [(\'double_matrix\', np.float32, (32, 16, 3), False)],\n                                                  [\'matrix\'])) as reader:\n        row_tensors = tf_tensors(reader)\n        with _tf_session() as sess:\n            actual = sess.run(row_tensors)\n\n        original_sample = next(d for d in synthetic_dataset.data if d[\'id\'] == actual.id)\n        expected_matrix = original_sample[\'matrix\'] * 2\n        np.testing.assert_equal(expected_matrix, actual.double_matrix)\n\n\n@mock.patch(\'petastorm.unischema._UNISCHEMA_FIELD_ORDER\', \'alphabetical\')\n@create_tf_graph\ndef test_transform_function_new_field_batched(scalar_dataset):\n    def double_float64(sample):\n        sample[\'new_float64\'] = sample[\'float64\'] * 2\n        del sample[\'float64\']\n        return sample\n\n    with make_batch_reader(scalar_dataset.url, reader_pool_type=\'dummy\',\n                           transform_spec=TransformSpec(double_float64,\n                                                        [(\'new_float64\', np.float64, (), False)],\n                                                        [\'float64\'])) as reader:\n        row_tensors = tf_tensors(reader)\n        with _tf_session() as sess:\n            actual = sess.run(row_tensors)\n\n        for actual_id, actual_float64 in zip(actual.id, actual.new_float64):\n            original_sample = next(d for d in scalar_dataset.data if d[\'id\'] == actual_id)\n            expected = original_sample[\'float64\'] * 2\n            np.testing.assert_equal(expected, actual_float64)\n\n\n@create_tf_graph\ndef test_ngram_generator(synthetic_dataset):\n    """"""Testing private _ngrams_generator and _unflatten_and_set_shape functions.""""""\n\n    # 1. Use _ngrams_generator to read out ngrams as flattened tuples\n    # 2. Convert flattened tuples back to ngrams of tensors\n    # 3. Evaluate tensors and make sure the\n    fields = {\n        -1: [TestSchema.id],\n        2: [TestSchema.id, TestSchema.image_png]\n    }\n    ngram = NGram(fields=fields, delta_threshold=1.5, timestamp_field=TestSchema.id)\n\n    with make_reader(schema_fields=ngram, dataset_url=synthetic_dataset.url, reader_pool_type=\'dummy\',\n                     shuffle_row_groups=False) as reader:\n        flat_row_numpy = next(iter(_ngrams_generator(reader)))\n\n        flat_row_tf = tuple([tf.constant(x) for x in flat_row_numpy])\n        ngram_tf = _unflatten_and_set_shape(reader.schema, reader.ngram, flat_row_tf)\n\n        with _tf_session() as sess:\n            ngram_numpy = sess.run(ngram_tf)\n\n        assert ngram_numpy[-1].id == flat_row_numpy.id_0\n        assert ngram_numpy[2].id == flat_row_numpy.id_3\n        np.testing.assert_equal(ngram_numpy[2].image_png, flat_row_numpy.image_png_3)\n'"
petastorm/tests/test_transform.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport pytest\n\nfrom petastorm.transform import edit_field, transform_schema, TransformSpec\nfrom petastorm.unischema import Unischema, UnischemaField\n\nTestSchema = Unischema(\'TestSchema\', [\n    UnischemaField(\'string\', np.unicode_, (), None, False),\n    UnischemaField(\'int\', np.int32, (), None, False),\n    UnischemaField(\'double\', np.float64, (), None, False),\n])\n\n\ndef test_noop_transform():\n    transformed_schema = transform_schema(TestSchema, TransformSpec(lambda x: x, edit_fields=None, removed_fields=None))\n    assert set(transformed_schema.fields) == set(TestSchema.fields)\n\n\ndef test_remove_field_transform():\n    one_removed = transform_schema(TestSchema, TransformSpec(lambda x: x, edit_fields=None,\n                                                             removed_fields=[\'int\']))\n    assert set(one_removed.fields.keys()) == {\'string\', \'double\'}\n\n    two_removed = transform_schema(TestSchema, TransformSpec(lambda x: x, edit_fields=None,\n                                                             removed_fields=[\'int\', \'double\']))\n    assert set(two_removed.fields.keys()) == {\'string\'}\n\n\ndef test_select_field_transform():\n    test_list = [\n        [\'string\', \'double\', \'int\'],\n        [\'int\', \'string\', \'double\'],\n        [\'string\', \'int\'],\n        [\'int\']\n    ]\n    for selected_fields in test_list:\n        transformed = transform_schema(TestSchema, TransformSpec(selected_fields=selected_fields))\n        assert list(transformed.fields.keys()) == selected_fields\n\n\ndef test_add_field_transform():\n    one_added = transform_schema(TestSchema,\n                                 TransformSpec(lambda x: x,\n                                               edit_fields=[UnischemaField(\'double2\', np.float64, (), None, False)]))\n    assert set(one_added.fields.keys()) == {\'string\', \'double\', \'double2\', \'int\'}\n\n\ndef test_change_field_transform():\n    one_added = transform_schema(TestSchema,\n                                 TransformSpec(lambda x: x,\n                                               edit_fields=[UnischemaField(\'double\', np.float16, (), None, False)]))\n    assert one_added.fields[\'double\'].numpy_dtype == np.float16\n\n\ndef test_unknown_fields_in_remove_field_transform():\n    with pytest.warns(UserWarning, match=\'not part of the schema.*unknown_1\'):\n        one_removed = transform_schema(TestSchema, TransformSpec(lambda x: x, edit_fields=None,\n                                                                 removed_fields=[\'int\', \'unknown_1\', \'unknown_2\']))\n    assert set(one_removed.fields.keys()) == {\'string\', \'double\'}\n\n\ndef test_create_edit_field():\n    e1 = edit_field(name=\'ab\', numpy_dtype=np.float64, shape=(2, 3), nullable=True)\n    assert e1 == (\'ab\', np.float64, (2, 3), True)\n'"
petastorm/tests/test_unischema.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import division\n\nfrom decimal import Decimal\n\nimport numpy as np\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport pytest\nfrom pyspark import Row\nfrom pyspark.sql.types import StringType, IntegerType, DecimalType, ShortType, LongType\n\nfrom petastorm.codecs import ScalarCodec, NdarrayCodec\nfrom petastorm.unischema import Unischema, UnischemaField, dict_to_spark_row, \\\n    insert_explicit_nulls, match_unischema_fields, _new_gt_255_compatible_namedtuple, _fullmatch\n\ntry:\n    from unittest import mock\nexcept ImportError:\n    from mock import mock\n\n\ndef _mock_parquet_dataset(partitions, arrow_schema):\n    """"""Creates a pyarrow.ParquetDataset mock capable of returning:\n\n        parquet_dataset.pieces[0].get_metadata(parquet_dataset.fs.open).schema.to_arrow_schema() == schema\n        parquet_dataset.partitions = partitions\n\n    :param partitions: expected to be a list of pa.parquet.PartitionSet\n    :param arrow_schema: an instance of pa.arrow_schema to be assumed by the mock parquet dataset object.\n    :return:\n    """"""\n    piece_mock = mock.Mock()\n    piece_mock.get_metadata().schema.to_arrow_schema.return_value = arrow_schema\n\n    dataset_mock = mock.Mock()\n    type(dataset_mock).pieces = mock.PropertyMock(return_value=[piece_mock])\n    type(dataset_mock).partitions = partitions\n\n    return dataset_mock\n\n\ndef test_fields():\n    """"""Try using \'fields\' getter""""""\n    TestSchema = Unischema(\'TestSchema\', [\n        UnischemaField(\'int_field\', np.int8, (), ScalarCodec(IntegerType()), False),\n        UnischemaField(\'string_field\', np.string_, (), ScalarCodec(StringType()), False),\n    ])\n\n    assert len(TestSchema.fields) == 2\n    assert TestSchema.fields[\'int_field\'].name == \'int_field\'\n    assert TestSchema.fields[\'string_field\'].name == \'string_field\'\n\n\ndef test_as_spark_schema():\n    """"""Try using \'as_spark_schema\' function""""""\n    TestSchema = Unischema(\'TestSchema\', [\n        UnischemaField(\'int_field\', np.int8, (), ScalarCodec(IntegerType()), False),\n        UnischemaField(\'string_field\', np.string_, (), ScalarCodec(StringType()), False),\n        UnischemaField(\'string_field_implicit\', np.string_, ()),\n    ])\n\n    spark_schema = TestSchema.as_spark_schema()\n    assert spark_schema.fields[0].name == \'int_field\'\n\n    assert spark_schema.fields[1].name == \'string_field\'\n    assert spark_schema.fields[1].dataType == StringType()\n\n    assert spark_schema.fields[2].name == \'string_field_implicit\'\n    assert spark_schema.fields[2].dataType == StringType()\n\n    assert TestSchema.fields[\'int_field\'].name == \'int_field\'\n    assert TestSchema.fields[\'string_field\'].name == \'string_field\'\n\n\ndef test_as_spark_schema_unspecified_codec_type_for_non_scalars_raises():\n    """"""Do not currently support choosing spark type automatically for non-scalar types.""""""\n    TestSchema = Unischema(\'TestSchema\', [\n        UnischemaField(\'int_vector_unspecified_codec\', np.int8, (1,)),\n    ])\n\n    with pytest.raises(ValueError, match=\'has codec set to None\'):\n        TestSchema.as_spark_schema()\n\n\ndef test_as_spark_schema_unspecified_codec_type_unknown_scalar_type_raises():\n    """"""We have a limited list of scalar types we can automatically map from numpy (+Decimal) types to spark types.\n    Make sure that a ValueError is raised if an unknown type is used.""""""\n    TestSchema = Unischema(\'TestSchema\', [\n        UnischemaField(\'int_vector_unspecified_codec\', object, ()),\n    ])\n\n    with pytest.raises(ValueError, match=\'Was not able to map type\'):\n        TestSchema.as_spark_schema()\n\n\ndef test_dict_to_spark_row_field_validation_scalar_types():\n    """"""Test various validations done on data types when converting a dictionary to a spark row""""""\n    TestSchema = Unischema(\'TestSchema\', [\n        UnischemaField(\'string_field\', np.string_, (), ScalarCodec(StringType()), False),\n    ])\n\n    assert isinstance(dict_to_spark_row(TestSchema, {\'string_field\': \'abc\'}), Row)\n\n    # Not a nullable field\n    with pytest.raises(ValueError):\n        isinstance(dict_to_spark_row(TestSchema, {\'string_field\': None}), Row)\n\n    # Wrong field type\n    with pytest.raises(TypeError):\n        isinstance(dict_to_spark_row(TestSchema, {\'string_field\': []}), Row)\n\n\ndef test_dict_to_spark_row_field_validation_scalar_nullable():\n    """"""Test various validations done on data types when converting a dictionary to a spark row""""""\n    TestSchema = Unischema(\'TestSchema\', [\n        UnischemaField(\'string_field\', np.string_, (), ScalarCodec(StringType()), True),\n        UnischemaField(\'nullable_implicitly_set\', np.string_, (), ScalarCodec(StringType()), True),\n    ])\n\n    assert isinstance(dict_to_spark_row(TestSchema, {\'string_field\': None}), Row)\n\n\ndef test_dict_to_spark_row_field_validation_ndarrays():\n    """"""Test various validations done on data types when converting a dictionary to a spark row""""""\n    TestSchema = Unischema(\'TestSchema\', [\n        UnischemaField(\'tensor3d\', np.float32, (10, 20, 30), NdarrayCodec(), False),\n    ])\n\n    assert isinstance(dict_to_spark_row(TestSchema, {\'tensor3d\': np.zeros((10, 20, 30), dtype=np.float32)}), Row)\n\n    # Null value into not nullable field\n    with pytest.raises(ValueError):\n        isinstance(dict_to_spark_row(TestSchema, {\'string_field\': None}), Row)\n\n    # Wrong dimensions\n    with pytest.raises(ValueError):\n        isinstance(dict_to_spark_row(TestSchema, {\'string_field\': np.zeros((1, 2, 3), dtype=np.float32)}), Row)\n\n\ndef test_dict_to_spark_row_order():\n    TestSchema = Unischema(\'TestSchema\', [\n        UnischemaField(\'float_col\', np.float64, ()),\n        UnischemaField(\'int_col\', np.int64, ()),\n    ])\n    row_dict = {\n        TestSchema.int_col.name: 3,\n        TestSchema.float_col.name: 2.0,\n    }\n    spark_row = dict_to_spark_row(TestSchema, row_dict)\n    schema_field_names = list(TestSchema.fields)\n    assert spark_row[0] == row_dict[schema_field_names[0]]\n    assert spark_row[1] == row_dict[schema_field_names[1]]\n\n\ndef test_make_named_tuple():\n    TestSchema = Unischema(\'TestSchema\', [\n        UnischemaField(\'string_scalar\', np.string_, (), ScalarCodec(StringType()), True),\n        UnischemaField(\'int32_scalar\', np.int32, (), ScalarCodec(ShortType()), False),\n        UnischemaField(\'uint8_scalar\', np.uint8, (), ScalarCodec(ShortType()), False),\n        UnischemaField(\'int32_matrix\', np.float32, (10, 20, 3), NdarrayCodec(), True),\n        UnischemaField(\'decimal_scalar\', Decimal, (10, 20, 3), ScalarCodec(DecimalType(10, 9)), False),\n    ])\n\n    TestSchema.make_namedtuple(string_scalar=\'abc\', int32_scalar=10, uint8_scalar=20,\n                               int32_matrix=np.int32((10, 20, 3)), decimal_scalar=Decimal(123) / Decimal(10))\n\n    TestSchema.make_namedtuple(string_scalar=None, int32_scalar=10, uint8_scalar=20,\n                               int32_matrix=None, decimal_scalar=Decimal(123) / Decimal(10))\n\n\ndef test_insert_explicit_nulls():\n    TestSchema = Unischema(\'TestSchema\', [\n        UnischemaField(\'nullable\', np.int32, (), ScalarCodec(StringType()), True),\n        UnischemaField(\'not_nullable\', np.int32, (), ScalarCodec(ShortType()), False),\n    ])\n\n    # Insert_explicit_nulls to leave the dictionary as is.\n    row_dict = {\'nullable\': 0, \'not_nullable\': 1}\n    insert_explicit_nulls(TestSchema, row_dict)\n    assert len(row_dict) == 2\n    assert row_dict[\'nullable\'] == 0\n    assert row_dict[\'not_nullable\'] == 1\n\n    # Insert_explicit_nulls to leave the dictionary as is.\n    row_dict = {\'nullable\': None, \'not_nullable\': 1}\n    insert_explicit_nulls(TestSchema, row_dict)\n    assert len(row_dict) == 2\n    assert row_dict[\'nullable\'] is None\n    assert row_dict[\'not_nullable\'] == 1\n\n    # We are missing a nullable field here. insert_explicit_nulls should add a None entry.\n    row_dict = {\'not_nullable\': 1}\n    insert_explicit_nulls(TestSchema, row_dict)\n    assert len(row_dict) == 2\n    assert row_dict[\'nullable\'] is None\n    assert row_dict[\'not_nullable\'] == 1\n\n    # We are missing a not_nullable field here. Should raise an ValueError.\n    row_dict = {\'nullable\': 0}\n    with pytest.raises(ValueError):\n        insert_explicit_nulls(TestSchema, row_dict)\n\n\ndef test_create_schema_view_fails_validate():\n    """""" Exercises code paths unischema.create_schema_view ValueError, and unischema.__str__.""""""\n    TestSchema = Unischema(\'TestSchema\', [\n        UnischemaField(\'int_field\', np.int8, (), ScalarCodec(IntegerType()), False),\n        UnischemaField(\'string_field\', np.string_, (), ScalarCodec(StringType()), False),\n    ])\n    with pytest.raises(ValueError, match=\'does not belong to the schema\'):\n        TestSchema.create_schema_view([UnischemaField(\'id\', np.int64, (), ScalarCodec(LongType()), False)])\n\n\ndef test_create_schema_view_using_invalid_type():\n    """""" Exercises code paths unischema.create_schema_view ValueError, and unischema.__str__.""""""\n    TestSchema = Unischema(\'TestSchema\', [\n        UnischemaField(\'int_field\', np.int8, (), ScalarCodec(IntegerType()), False),\n        UnischemaField(\'string_field\', np.string_, (), ScalarCodec(StringType()), False),\n    ])\n    with pytest.raises(ValueError, match=\'must be either a string\'):\n        TestSchema.create_schema_view([42])\n\n\ndef test_create_schema_view_using_unischema_fields():\n    TestSchema = Unischema(\'TestSchema\', [\n        UnischemaField(\'int_field\', np.int8, (), ScalarCodec(IntegerType()), False),\n        UnischemaField(\'string_field\', np.string_, (), ScalarCodec(StringType()), False),\n    ])\n    view = TestSchema.create_schema_view([TestSchema.int_field])\n    assert set(view.fields.keys()) == {\'int_field\'}\n\n\ndef test_create_schema_view_using_regex():\n    TestSchema = Unischema(\'TestSchema\', [\n        UnischemaField(\'int_field\', np.int8, (), ScalarCodec(IntegerType()), False),\n        UnischemaField(\'string_field\', np.string_, (), ScalarCodec(StringType()), False),\n    ])\n    view = TestSchema.create_schema_view([\'int.*$\'])\n    assert set(view.fields.keys()) == {\'int_field\'}\n\n    view = TestSchema.create_schema_view([u\'int.*$\'])\n    assert set(view.fields.keys()) == {\'int_field\'}\n\n\ndef test_create_schema_view_using_regex_and_unischema_fields():\n    TestSchema = Unischema(\'TestSchema\', [\n        UnischemaField(\'int_field\', np.int8, (), ScalarCodec(IntegerType()), False),\n        UnischemaField(\'string_field\', np.string_, (), ScalarCodec(StringType()), False),\n        UnischemaField(\'other_string_field\', np.string_, (), ScalarCodec(StringType()), False),\n    ])\n    view = TestSchema.create_schema_view([\'int.*$\', TestSchema.string_field])\n    assert set(view.fields.keys()) == {\'int_field\', \'string_field\'}\n\n\ndef test_create_schema_view_using_regex_and_unischema_fields_with_duplicates():\n    TestSchema = Unischema(\'TestSchema\', [\n        UnischemaField(\'int_field\', np.int8, (), ScalarCodec(IntegerType()), False),\n        UnischemaField(\'string_field\', np.string_, (), ScalarCodec(StringType()), False),\n        UnischemaField(\'other_string_field\', np.string_, (), ScalarCodec(StringType()), False),\n    ])\n    view = TestSchema.create_schema_view([\'int.*$\', TestSchema.int_field])\n    assert set(view.fields.keys()) == {\'int_field\'}\n\n\ndef test_create_schema_view_no_field_matches_regex():\n    TestSchema = Unischema(\'TestSchema\', [\n        UnischemaField(\'int_field\', np.int8, (), ScalarCodec(IntegerType()), False),\n        UnischemaField(\'string_field\', np.string_, (), ScalarCodec(StringType()), False),\n    ])\n    view = TestSchema.create_schema_view([\'bogus\'])\n    assert not view.fields\n\n\ndef test_name_property():\n    TestSchema = Unischema(\'TestSchema\', [\n        UnischemaField(\'nullable\', np.int32, (), ScalarCodec(StringType()), True),\n    ])\n\n    assert \'TestSchema\' == TestSchema._name\n\n\ndef test_field_name_conflict_with_unischema_attribute():\n    # fields is an existing attribute of Unischema\n    with pytest.warns(UserWarning, match=\'Can not create dynamic property\'):\n        Unischema(\'TestSchema\', [UnischemaField(\'fields\', np.int32, (), ScalarCodec(StringType()), True)])\n\n\ndef test_match_unischema_fields():\n    TestSchema = Unischema(\'TestSchema\', [\n        UnischemaField(\'int32\', np.int32, (), None, False),\n        UnischemaField(\'uint8\', np.uint8, (), None, False),\n        UnischemaField(\'uint16\', np.uint16, (), None, False),\n    ])\n\n    assert match_unischema_fields(TestSchema, [\'.*nt.*6\']) == [TestSchema.uint16]\n    assert match_unischema_fields(TestSchema, [\'nomatch\']) == []\n    assert set(match_unischema_fields(TestSchema, [\'.*\'])) == set(TestSchema.fields.values())\n    assert set(match_unischema_fields(TestSchema, [\'int32\', \'uint8\'])) == {TestSchema.int32, TestSchema.uint8}\n\n\ndef test_match_unischema_fields_legacy_warning():\n    TestSchema = Unischema(\'TestSchema\', [\n        UnischemaField(\'int32\', np.int32, (), None, False),\n        UnischemaField(\'uint8\', np.uint8, (), None, False),\n        UnischemaField(\'uint16\', np.uint16, (), None, False),\n    ])\n\n    # Check that no warnings are shown if the legacy and the new way of filtering produce the same results.\n    with pytest.warns(None) as unexpected_warnings:\n        match_unischema_fields(TestSchema, [\'uint8\'])\n    assert not unexpected_warnings\n\n    # uint8 and uint16 would have been matched using the old method, but not the new one\n    with pytest.warns(UserWarning, match=r\'schema_fields behavior has changed.*uint16, uint8\'):\n        assert match_unischema_fields(TestSchema, [\'uint\']) == []\n\n    # Now, all fields will be matched, but in different order (legacy vs current). Make sure we don\'t issue a warning.\n    with pytest.warns(None) as unexpected_warnings:\n        match_unischema_fields(TestSchema, [\'int\', \'uint8\', \'uint16\', \'int32\'])\n    assert not unexpected_warnings\n\n\ndef test_arrow_schema_convertion():\n    fields = [\n        pa.field(\'string\', pa.string()),\n        pa.field(\'int8\', pa.int8()),\n        pa.field(\'int16\', pa.int16()),\n        pa.field(\'int32\', pa.int32()),\n        pa.field(\'int64\', pa.int64()),\n        pa.field(\'float\', pa.float32()),\n        pa.field(\'double\', pa.float64()),\n        pa.field(\'bool\', pa.bool_(), False),\n        pa.field(\'fixed_size_binary\', pa.binary(10)),\n        pa.field(\'variable_size_binary\', pa.binary()),\n        pa.field(\'decimal\', pa.decimal128(3, 4)),\n        pa.field(\'timestamp_s\', pa.timestamp(\'s\')),\n        pa.field(\'timestamp_ns\', pa.timestamp(\'ns\')),\n        pa.field(\'date_32\', pa.date32()),\n        pa.field(\'date_64\', pa.date64())\n    ]\n    arrow_schema = pa.schema(fields)\n\n    mock_dataset = _mock_parquet_dataset([], arrow_schema)\n\n    unischema = Unischema.from_arrow_schema(mock_dataset)\n    for name in arrow_schema.names:\n        assert getattr(unischema, name).name == name\n        assert getattr(unischema, name).codec is None\n\n        if name == \'bool\':\n            assert not getattr(unischema, name).nullable\n        else:\n            assert getattr(unischema, name).nullable\n\n    # Test schema preserve fields order\n    field_name_list = [f.name for f in fields]\n    assert list(unischema.fields.keys()) == field_name_list\n\n\ndef test_arrow_schema_convertion_with_string_partitions():\n    arrow_schema = pa.schema([\n        pa.field(\'int8\', pa.int8()),\n    ])\n\n    mock_dataset = _mock_parquet_dataset([pq.PartitionSet(\'part_name\', [\'a\', \'b\'])], arrow_schema)\n\n    unischema = Unischema.from_arrow_schema(mock_dataset)\n    assert unischema.part_name.numpy_dtype == np.str_\n\n\ndef test_arrow_schema_convertion_with_int_partitions():\n    arrow_schema = pa.schema([\n        pa.field(\'int8\', pa.int8()),\n    ])\n\n    mock_dataset = _mock_parquet_dataset([pq.PartitionSet(\'part_name\', [\'0\', \'1\', \'2\'])], arrow_schema)\n\n    unischema = Unischema.from_arrow_schema(mock_dataset)\n    assert unischema.part_name.numpy_dtype == np.int64\n\n\ndef test_arrow_schema_convertion_fail():\n    arrow_schema = pa.schema([\n        pa.field(\'list_of_int\', pa.float16()),\n    ])\n\n    mock_dataset = _mock_parquet_dataset([], arrow_schema)\n\n    with pytest.raises(ValueError, match=\'Cannot auto-create unischema due to unsupported column type\'):\n        Unischema.from_arrow_schema(mock_dataset)\n\n\ndef test_arrow_schema_arrow_1644_list_of_struct():\n    arrow_schema = pa.schema([\n        pa.field(\'id\', pa.string()),\n        pa.field(\'list_of_struct\', pa.list_(pa.struct([pa.field(\'a\', pa.string()), pa.field(\'b\', pa.int32())])))\n    ])\n\n    mock_dataset = _mock_parquet_dataset([], arrow_schema)\n\n    unischema = Unischema.from_arrow_schema(mock_dataset)\n    assert getattr(unischema, \'id\').name == \'id\'\n    assert not hasattr(unischema, \'list_of_struct\')\n\n\ndef test_arrow_schema_arrow_1644_list_of_list():\n    arrow_schema = pa.schema([\n        pa.field(\'id\', pa.string()),\n        pa.field(\'list_of_list\',\n                 pa.list_(pa.list_(pa.struct([pa.field(\'a\', pa.string()), pa.field(\'b\', pa.int32())]))))\n    ])\n\n    mock_dataset = _mock_parquet_dataset([], arrow_schema)\n\n    unischema = Unischema.from_arrow_schema(mock_dataset)\n    assert getattr(unischema, \'id\').name == \'id\'\n    assert not hasattr(unischema, \'list_of_list\')\n\n\ndef test_arrow_schema_convertion_ignore():\n    arrow_schema = pa.schema([\n        pa.field(\'list_of_int\', pa.float16()),\n        pa.field(\'struct\', pa.struct([pa.field(\'a\', pa.string()), pa.field(\'b\', pa.int32())])),\n    ])\n\n    mock_dataset = _mock_parquet_dataset([], arrow_schema)\n\n    unischema = Unischema.from_arrow_schema(mock_dataset, omit_unsupported_fields=True)\n    assert not hasattr(unischema, \'list_of_int\')\n\n\n@pytest.fixture()\ndef equality_fields():\n    class Fixture(object):\n        string1 = UnischemaField(\'random\', np.string_, (), ScalarCodec(StringType()), False)\n        string2 = UnischemaField(\'random\', np.string_, (), ScalarCodec(StringType()), False)\n        string_implicit = UnischemaField(\'random\', np.string_, ())\n        string_nullable = UnischemaField(\'random\', np.string_, (), nullable=True)\n        other_string = UnischemaField(\'Random\', np.string_, (), ScalarCodec(StringType()), False)\n        int1 = UnischemaField(\'id\', np.int32, (), ScalarCodec(ShortType()), False)\n        int2 = UnischemaField(\'id\', np.int32, (), ScalarCodec(ShortType()), False)\n        other_int = UnischemaField(\'ID\', np.int32, (), ScalarCodec(ShortType()), False)\n\n    return Fixture()\n\n\ndef test_equality(equality_fields):\n    # Use assertTrue instead of assertEqual/assertNotEqual so we don\'t depend on which operator (__eq__ or __ne__)\n    # actual implementation of assert uses\n    assert equality_fields.string1 == equality_fields.string2\n    assert equality_fields.string1 == equality_fields.string_implicit\n    assert equality_fields.int1 == equality_fields.int2\n    assert equality_fields.string1 != equality_fields.other_string\n    assert equality_fields.other_string != equality_fields.string_implicit\n    assert equality_fields.int1 != equality_fields.other_int\n    assert equality_fields.string_nullable != equality_fields.string_implicit\n\n\ndef test_hash(equality_fields):\n    assert hash(equality_fields.string1) == hash(equality_fields.string2)\n    assert hash(equality_fields.int1) == hash(equality_fields.int2)\n    assert hash(equality_fields.string1) != hash(equality_fields.other_string)\n    assert hash(equality_fields.int1) != hash(equality_fields.other_int)\n\n\ndef test_new_gt_255_compatible_namedtuple():\n    fields_count = 1000\n    field_names = [\'f{}\'.format(i) for i in range(fields_count)]\n    values = list(range(1000))\n    huge_tuple = _new_gt_255_compatible_namedtuple(\'HUGE_TUPLE\', field_names)\n    huge_tuple_instance = huge_tuple(**dict(zip(field_names, values)))\n    assert len(huge_tuple_instance) == fields_count\n    assert huge_tuple_instance.f764 == 764\n\n\ndef test_fullmatch():\n    assert _fullmatch(\'abc\', \'abc\')\n    assert _fullmatch(\'^abc\', \'abc\')\n    assert _fullmatch(\'abc$\', \'abc\')\n    assert _fullmatch(\'a.c\', \'abc\')\n    assert _fullmatch(\'.*abcdef\', \'abcdef\')\n    assert _fullmatch(\'abc.*\', \'abcdef\')\n    assert _fullmatch(\'.*c.*\', \'abcdef\')\n    assert _fullmatch(\'\', \'\')\n    assert not _fullmatch(\'abc\', \'xyz\')\n    assert not _fullmatch(\'abc\', \'abcx\')\n    assert not _fullmatch(\'abc\', \'xabc\')\n'"
petastorm/tests/test_weighted_sampling_reader.py,4,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import division\n\nimport numpy as np\nimport pytest\nimport six\nimport tensorflow.compat.v1 as tf  # pylint: disable=import-error\n\nfrom petastorm import make_reader\nfrom petastorm.ngram import NGram\nfrom petastorm.predicates import in_lambda\nfrom petastorm.test_util.reader_mock import ReaderMock\nfrom petastorm.tf_utils import tf_tensors, make_petastorm_dataset\nfrom petastorm.unischema import Unischema, UnischemaField\nfrom petastorm.weighted_sampling_reader import WeightedSamplingReader\nfrom petastorm.tests.test_tf_utils import create_tf_graph\n\nTestSchema = Unischema(\'TestSchema\', [\n    UnischemaField(\'f1\', np.int32, (), None, False),\n])\n\nreader0 = ReaderMock(TestSchema, lambda _: {\'f1\': 0})\nreader1 = ReaderMock(TestSchema, lambda _: {\'f1\': 1})\nreader2 = ReaderMock(TestSchema, lambda _: {\'f1\': 2})\n\n\ndef _count_mixed(readers, probabilities, num_of_reads):\n    result = len(probabilities) * [0]\n\n    with WeightedSamplingReader(readers, probabilities) as mixer:\n        for _ in six.moves.xrange(num_of_reads):\n            reader_index = next(mixer).f1\n            result[reader_index] += 1\n\n    return result\n\n\ndef test_select_only_one_of_readers():\n    num_of_reads = 1000\n    np.testing.assert_array_equal(_count_mixed([reader0, reader1], [1.0, 0.0], num_of_reads), [num_of_reads, 0])\n    np.testing.assert_array_equal(_count_mixed([reader0, reader1], [0.0, 1.0], num_of_reads), [0, num_of_reads])\n\n    np.testing.assert_array_equal(\n        _count_mixed([reader0, reader1, reader2], [0.0, 1.0, 0.0], num_of_reads), [0, num_of_reads, 0])\n    np.testing.assert_array_equal(\n        _count_mixed([reader0, reader1, reader2], [0.0, 0.0, 1.0], num_of_reads), [0, 0, num_of_reads])\n\n\ndef test_not_normalized_probabilities():\n    num_of_reads = 1000\n    mix_10_90 = _count_mixed([reader0, reader1], [10, 90], num_of_reads)\n    np.testing.assert_allclose(mix_10_90, [num_of_reads * 0.1, num_of_reads * 0.9], atol=num_of_reads / 10)\n\n\ndef test_mixing():\n    num_of_reads = 1000\n    mix_10_90 = _count_mixed([reader0, reader1], [0.1, 0.9], num_of_reads)\n\n    np.testing.assert_allclose(mix_10_90, [num_of_reads * 0.1, num_of_reads * 0.9], atol=num_of_reads / 10)\n\n    mix_10_50_40 = _count_mixed([reader0, reader1, reader2], [0.1, 0.5, 0.4], num_of_reads)\n    np.testing.assert_allclose(mix_10_50_40, [num_of_reads * 0.1, num_of_reads * 0.5, num_of_reads * 0.4],\n                               atol=num_of_reads / 10)\n\n\ndef test_real_reader(synthetic_dataset):\n    readers = [make_reader(synthetic_dataset.url, predicate=in_lambda([\'id\'], lambda id: id % 2 == 0), num_epochs=None,\n                           reader_pool_type=\'dummy\'),\n               make_reader(synthetic_dataset.url, predicate=in_lambda([\'id\'], lambda id: id % 2 == 1), num_epochs=None,\n                           reader_pool_type=\'dummy\')]\n    results = [0, 0]\n    num_of_reads = 300\n    with WeightedSamplingReader(readers, [0.5, 0.5]) as mixer:\n        # Piggyback on this test to verify container interface of the WeightedSamplingReader\n        for i, sample in enumerate(mixer):\n            next_id = sample.id % 2\n            results[next_id] += 1\n            if i >= num_of_reads:\n                break\n\n    np.testing.assert_allclose(results, [num_of_reads * 0.5, num_of_reads * 0.5], atol=num_of_reads / 10)\n\n\ndef test_bad_arguments():\n    with pytest.raises(ValueError):\n        WeightedSamplingReader([reader1], [0.1, 0.9])\n\n\n@create_tf_graph\ndef test_with_tf_tensors(synthetic_dataset):\n    fields_to_read = [\'id.*\', \'image_png\']\n    readers = [make_reader(synthetic_dataset.url, schema_fields=fields_to_read, workers_count=1),\n               make_reader(synthetic_dataset.url, schema_fields=fields_to_read, workers_count=1)]\n\n    with WeightedSamplingReader(readers, [0.5, 0.5]) as mixer:\n        mixed_tensors = tf_tensors(mixer)\n\n        with tf.Session() as sess:\n            sess.run(mixed_tensors)\n\n\ndef test_schema_mismatch(synthetic_dataset):\n    readers = [make_reader(synthetic_dataset.url, schema_fields=[\'id\'], workers_count=1),\n               make_reader(synthetic_dataset.url, schema_fields=[\'image_png\'], workers_count=1)]\n\n    with pytest.raises(ValueError, match=\'.*should have the same schema.*\'):\n        WeightedSamplingReader(readers, [0.5, 0.5])\n\n\n@create_tf_graph\ndef test_ngram_mix(synthetic_dataset):\n    ngram1_fields = {\n        -1: [\'id\', ],\n        0: [\'id\', \'image_png\'],\n    }\n\n    ts_field = \'^id$\'\n\n    ngram1 = NGram(fields=ngram1_fields, delta_threshold=10, timestamp_field=ts_field)\n    ngram2 = NGram(fields=ngram1_fields, delta_threshold=10, timestamp_field=ts_field)\n\n    readers = [make_reader(synthetic_dataset.url, schema_fields=ngram1, workers_count=1),\n               make_reader(synthetic_dataset.url, schema_fields=ngram2, workers_count=1)]\n\n    with WeightedSamplingReader(readers, [0.5, 0.5]) as mixer:\n        mixed_tensors = tf_tensors(mixer)\n\n        with tf.Session() as sess:\n            for _ in range(10):\n                actual = sess.run(mixed_tensors)\n                assert set(actual.keys()) == {-1, 0}\n\n\ndef test_ngram_mismsatch(synthetic_dataset):\n    ngram1_fields = {\n        -1: [\'id\', ],\n        0: [\'id\', \'image_png\'],\n    }\n\n    ngram2_fields = {\n        -1: [\'id\', \'image_png\'],\n        0: [\'id\', ],\n    }\n\n    ts_field = \'^id$\'\n\n    ngram1 = NGram(fields=ngram1_fields, delta_threshold=10, timestamp_field=ts_field)\n    ngram2 = NGram(fields=ngram2_fields, delta_threshold=10, timestamp_field=ts_field)\n\n    readers = [make_reader(synthetic_dataset.url, schema_fields=ngram1, workers_count=1),\n               make_reader(synthetic_dataset.url, schema_fields=ngram2, workers_count=1)]\n\n    with pytest.raises(ValueError, match=\'.*ngram.*\'):\n        WeightedSamplingReader(readers, [0.5, 0.5])\n\n\n@create_tf_graph\ndef test_with_tf_data_api(synthetic_dataset):\n    """"""Verify that WeightedSamplingReader is compatible with make_petastorm_dataset""""""\n\n    np.random.seed(42)\n\n    fields_to_read = [\'id.*\', \'image_png\']\n\n    # Use cur_shard=0, shard_count=2 to get only half samples from the second reader.\n    readers = [make_reader(synthetic_dataset.url, schema_fields=fields_to_read, workers_count=1),\n               make_reader(synthetic_dataset.url, schema_fields=fields_to_read, workers_count=1,\n                           cur_shard=0, shard_count=2)]\n\n    with WeightedSamplingReader(readers, [0.5, 0.5]) as mixer:\n        dataset = make_petastorm_dataset(mixer)\n        iterator = dataset.make_one_shot_iterator()\n        tensor = iterator.get_next()\n        rows_count = 0\n        with tf.Session() as sess:\n            while True:\n                try:\n                    sess.run(tensor)\n                    rows_count += 1\n                except tf.errors.OutOfRangeError:\n                    break\n\n        # We expect iterations to finish once the second read has exhausted its samples. For each sample in the\n        # second reaader we read approximately 1 sample from the first.\n        expected_rows_approx = len(synthetic_dataset.data)\n        np.testing.assert_allclose(rows_count, expected_rows_approx, atol=20)\n'"
petastorm/tools/__init__.py,0,b''
petastorm/tools/copy_dataset.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n""""""This command line utility creates a copy of a Petastorm dataset while optionally:\n - selects a set of columns\n - filters out rows with null values in specified fields.""""""\n\nimport argparse\nimport logging\nimport operator\nimport sys\nfrom functools import reduce  # pylint: disable=W0622\n\nfrom pyspark.sql import SparkSession\n\nfrom petastorm.unischema import match_unischema_fields\nfrom petastorm.etl.dataset_metadata import materialize_dataset, get_schema_from_dataset_url\nfrom petastorm.tools.spark_session_cli import add_configure_spark_arguments, configure_spark\nfrom petastorm.fs_utils import FilesystemResolver\n\n\ndef copy_dataset(spark, source_url, target_url, field_regex, not_null_fields, overwrite_output, partitions_count,\n                 row_group_size_mb, hdfs_driver=\'libhdfs3\'):\n    """"""\n    Creates a copy of a dataset. A new dataset will optionally contain a subset of columns. Rows that have NULL\n    values in fields defined by ``not_null_fields`` argument are filtered out.\n\n\n    :param spark: An instance of ``SparkSession`` object\n    :param source_url: A url of the dataset to be copied.\n    :param target_url: A url specifying location of the target dataset.\n    :param field_regex: A list of regex patterns. Only columns that match one of these patterns are copied to the new\n      dataset.\n    :param not_null_fields: A list of fields that must have non-NULL valus in the target dataset.\n    :param overwrite_output: If ``False`` and there is an existing path defined by ``target_url``, the operation will\n      fail.\n    :param partitions_count: If not ``None``, the dataset is repartitioned before write. Number of files in the target\n      Parquet store is defined by this parameter.\n    :param row_group_size_mb: The size of the rowgroup in the target dataset. Specified in megabytes.\n    :param hdfs_driver: A string denoting the hdfs driver to use (if using a dataset on hdfs). Current choices are\n        libhdfs (java through JNI) or libhdfs3 (C++)\n    :param user: String denoting username when connecting to HDFS. None implies login user.\n    :return: None\n    """"""\n    schema = get_schema_from_dataset_url(source_url, hdfs_driver=hdfs_driver)\n\n    fields = match_unischema_fields(schema, field_regex)\n\n    if field_regex and not fields:\n        field_names = list(schema.fields.keys())\n        raise ValueError(\'Regular expressions (%s) do not match any fields (%s)\', str(field_regex), str(field_names))\n\n    if fields:\n        subschema = schema.create_schema_view(fields)\n    else:\n        subschema = schema\n\n    resolver = FilesystemResolver(target_url, spark.sparkContext._jsc.hadoopConfiguration(),\n                                  hdfs_driver=hdfs_driver, user=spark.sparkContext.sparkUser())\n    with materialize_dataset(spark, target_url, subschema, row_group_size_mb,\n                             filesystem_factory=resolver.filesystem_factory()):\n        data_frame = spark.read \\\n            .parquet(source_url)\n\n        if fields:\n            data_frame = data_frame.select(*[f.name for f in fields])\n\n        if not_null_fields:\n            not_null_condition = reduce(operator.__and__, (data_frame[f].isNotNull() for f in not_null_fields))\n            data_frame = data_frame.filter(not_null_condition)\n\n        if partitions_count:\n            data_frame = data_frame.repartition(partitions_count)\n\n        data_frame.write \\\n            .mode(\'overwrite\' if overwrite_output else \'error\') \\\n            .option(\'compression\', \'none\') \\\n            .parquet(target_url)\n\n\ndef args_parser():\n    parser = argparse.ArgumentParser(description=__doc__)\n\n    parser.add_argument(\'source_url\',\n                        help=\'A url of a source petastorm dataset\',\n                        type=str)\n\n    parser.add_argument(\'target_url\',\n                        help=\'A url of a target petastorm datset\',\n                        type=str)\n\n    parser.add_argument(\'--overwrite-output\', action=\'store_true\',\n                        help=\'If the flag is set to false, the script will fail \'\n                             \'in case when the output directory already exists\')\n\n    parser.add_argument(\'--field-regex\', type=str, nargs=\'+\',\n                        help=\'A list of regular expressions. Only fields that match one of the regex patterns will \'\n                             \'be copied.\')\n\n    parser.add_argument(\'--not-null-fields\', type=str, nargs=\'+\',\n                        help=\'All names in this list must be not null in the source dataset in order to be copied to \'\n                             \'the target dataset.\')\n\n    parser.add_argument(\'--partition-count\', type=int, required=False,\n                        help=\'Specifies number of partitions in the output dataset\')\n\n    parser.add_argument(\'--row-group-size-mb\', type=int, required=False,\n                        help=\'Specifies the row group size in the created dataset\')\n    parser.add_argument(\'--hdfs-driver\', type=str, default=\'libhdfs3\',\n                        help=\'A string denoting the hdfs driver to use (if using a dataset on hdfs). \'\n                             \'Current choices are libhdfs (java through JNI) or libhdfs3 (C++)\')\n\n    add_configure_spark_arguments(parser)\n\n    return parser\n\n\ndef _main(sys_argv):\n    logging.basicConfig()\n\n    args = args_parser().parse_args(sys_argv)\n\n    # We set spark.sql.files.maxPartitionBytes to a large value since we typically have small number of rows per\n    # rowgroup. Reading a parquet store with default settings would result in excessively large number of partitions\n    # and inefficient processing\n    spark = configure_spark(SparkSession.builder.appName(\'petastorm-copy\'), args) \\\n        .config(\'spark.sql.files.maxPartitionBytes\', \'1010612736\') \\\n        .getOrCreate()\n\n    copy_dataset(spark, args.source_url, args.target_url, args.field_regex, args.not_null_fields, args.overwrite_output,\n                 args.partition_count, args.row_group_size_mb, hdfs_driver=args.hdfs_driver)\n\n    spark.stop()\n\n\ndef main():\n    _main(sys.argv[1:])\n\n\nif __name__ == \'__main__\':\n    _main(sys.argv[1:])\n'"
petastorm/tools/spark_session_cli.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""This module contains a set of utils that enables uniform interface for all command line tools that end up creating\nspark session objects""""""\n\n\ndef configure_spark(spark_session_builder, args):\n    """"""Applies configuration to a ``SparkSession.Builder`` object.\n\n    Call :func:`add_configure_spark_arguments` to add command line arguments to the argparser object.\n    This function returns the ``SparkSession.Builder`` to allow chaining additional calls to the ``Builder``.\n\n    >>> from pyspark.sql import SparkSession\n    >>>\n    >>> arg_parser = argparse.ArgumentParser()\n    >>> add_configure_spark_arguments(arg_parser)\n    >>> # ... more argparse arguments\n\n    >>> args = arg_parser.parse_args()\n    >>> spark = configure_spark(SparkSession.builder.appName(\'petastorm-copy\'), args).getOrCreate()\n\n    :param spark_session_builder: An instance of the ``pyspark.sql.session.SparkSession.Builder`` object.\n    :param args: A value returned by ``argparser.ArgumentParser.parse_args()`` call.\n    :return: ``SparkSession.Builder`` object.\n    """"""\n    if \'spark_session_config\' not in args or \'master\' not in args:\n        raise RuntimeError(\'--spark-session-config and/or --master were not found in parsed arguments. \'\n                           \'Call add_configure_spark_arguments() to add them.\')\n\n    spark_session_config = _cli_spark_session_config_to_dict(args.spark_session_config)\n\n    for key, value in spark_session_config.items():\n        spark_session_builder.config(key, value)\n\n    if args.master:\n        spark_session_builder.master(args.master)\n\n    return spark_session_builder\n\n\ndef add_configure_spark_arguments(argparser):\n    """"""Adds a set of arguments that are needed for spark session configuration.\n\n    >>> from pyspark.sql import SparkSession\n    >>>\n    >>> arg_parser = argparse.ArgumentParser()\n    >>> add_configure_spark_arguments(arg_parser)\n    >>> # ... more argparse arguments\n\n    >>> args = arg_parser.parse_args()\n    >>> spark = configure_spark(SparkSession.builder.appName(\'petastorm-copy\'), args).getOrCreate()\n\n    :param argparser: An instance of ``argparse.ArgumentParser`` object\n    :return: None\n    """"""\n    argparser.add_argument(\'--master\', type=str,\n                           help=\'Spark master. Default if not specified. To run on a local machine, specify \'\n                                \'""local[W]"" (where W is the number of local spark workers, e.g. local[10])\')\n\n    argparser.add_argument(\'--spark-session-config\', type=str, nargs=\'+\',\n                           help=\'A list of ""="" separated key-value pairs used to configure SparkSession object. \'\n                                \'For example: --spark-session-config spark.executor.cores=2 spark.executor.memory=10g\')\n\n\ndef _cli_spark_session_config_to_dict(spark_session_config):\n    config_dict = dict()\n\n    if not spark_session_config:\n        return config_dict\n\n    for config_pair in spark_session_config:\n        key_value_split = config_pair.split(\'=\')\n        if len(key_value_split) != 2:\n            raise ValueError(\'Elements of spark_session_config list are expected to be in key=value format. Got: %s\',\n                             config_pair)\n        config_dict[key_value_split[0]] = key_value_split[1]\n\n    return config_dict\n'"
petastorm/workers_pool/__init__.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nclass EmptyResultError(RuntimeError):\n    """"""Exception used to signal that there are no new elements in the queue and no new elements are expected, unless\n    ventilate is called again""""""\n\n\nclass TimeoutWaitingForResultError(RuntimeError):\n    """"""Indicates that timeout has elapsed while waiting for a result""""""\n\n\nclass VentilatedItemProcessedMessage(object):\n    """"""Object to signal that a worker has completed processing an item from the ventilation queue""""""\n'"
petastorm/workers_pool/dummy_pool.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom time import sleep\n\nfrom petastorm.workers_pool import EmptyResultError\n\n\nclass DummyPool(object):\n    """"""This class has pool interface but performs all work in calls to get_results. It is sometimes convenient\n    to substitute a real pool with this dummy implementation.\n\n    Found this class useful when profiling worker code. When on a separate thread, the worker code was not observable\n    (out of the box) by the profiler""""""\n\n    # Have workers argument just to make compatible with other pool implementations\n    def __init__(self, workers=None):\n        # We just accumulate all ventilated items in the list\n        self._ventilator_queue = []\n\n        # get_results will populate this list\n        self._results_queue = []\n        self._worker = None\n        self._ventilator = None\n        self.workers_count = 1\n\n    def start(self, worker_class, worker_args=None, ventilator=None):\n        # Instantiate a single worker with all the args\n        self._worker = worker_class(0, self._results_queue.append, worker_args)\n\n        if ventilator:\n            self._ventilator = ventilator\n            self._ventilator.start()\n\n    def ventilate(self, *args, **kargs):\n        """"""Send a work item to a worker process.""""""\n        self._ventilator_queue.append((args, kargs))\n\n    def get_results(self):\n        """"""Returns results\n\n        The processing is done on the get_results caller thread if the results queue is empty\n\n        :return: arguments passed to publish_func(...) by a worker\n        """"""\n\n        if self._results_queue:\n            # We have already calculated result. Just return it\n            return self._results_queue.pop(0)\n        else:\n            # If we don\'t have any tasks waiting for processing, then indicate empty queue\n            while self._ventilator_queue or (self._ventilator and not self._ventilator.completed()):\n\n                # To prevent a race condition of the ventilator working but not yet placing an item\n                # on the ventilator queue. We block until something is on the ventilator queue.\n                while not self._ventilator_queue:\n                    sleep(.1)\n\n                # If we do have some tasks, then process a task from the head of a queue\n                args, kargs = self._ventilator_queue.pop(0)\n                self._worker.process(*args, **kargs)\n\n                if self._ventilator:\n                    self._ventilator.processed_item()\n\n                if self._results_queue:\n                    return self._results_queue.pop(0)\n\n            raise EmptyResultError()\n\n    def stop(self):\n        if self._ventilator:\n            self._ventilator.stop()\n\n    def join(self):\n        pass\n\n    @property\n    def diagnostics(self):\n        return dict()\n'"
petastorm/workers_pool/exec_in_new_process.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport os\nimport subprocess\nimport sys\nfrom tempfile import mkstemp\n\nimport dill\n\nlogger = logging.getLogger(__name__)\n\n\ndef exec_in_new_process(func, *args, **kargs):\n    """"""Launches a function in a separate process. Takes variable number of arguments which are passed to the function.\n    The process IS NOT FORKED by \'exec\'ed.\n\n    :param func: Function to be executed in a separate process.\n    :param args: position arguments passed to the func\n    :param kargs: named arguments passed to the func\n    :return:\n    """"""\n\n    # Store function handle and arguments into a pickle\n    new_process_runnable_handle, new_process_runnable_file = mkstemp(suffix=\'runnable\')\n    with os.fdopen(new_process_runnable_handle, \'wb\') as f:\n        dill.dump((func, args, kargs), f)\n\n    bootstrap_package_name = \'{}.{}\'.format(__package__, os.path.splitext(os.path.basename(__file__))[0])\n    # Popen this script (__main__) below will be an entry point\n    process = subprocess.Popen(args=[sys.executable,\n                                     \'-m\',\n                                     bootstrap_package_name,\n                                     new_process_runnable_file],\n                               executable=sys.executable)\n    return process\n\n\nif __name__ == \'__main__\':\n    # An entry point to the newely executed process.\n    # Will unpickle function handle and arguments and call the function.\n    try:\n        logging.basicConfig()\n        if len(sys.argv) != 2:\n            raise RuntimeError(\'Expected a single command line argument\')\n        new_process_runnable_file = sys.argv[1]\n\n        with open(new_process_runnable_file, \'rb\') as f:\n            func, args, kargs = dill.load(f)\n\n        # Don\'t need the pickle file with the runable. Cleanup.\n        os.remove(new_process_runnable_file)\n\n        func(*args, **kargs)\n    except Exception as e:\n        logger.error(\'Unhandled exception in the function launched by exec_in_new_process: %s\', str(e))\n        raise\n'"
petastorm/workers_pool/process_pool.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""This pool is different from standard Python pool implementations by the fact that the workers are spawned\nwithout using fork. Some issues with using jvm based HDFS driver were observed when the process was forked\n(could not access HDFS from the forked worker if the driver was already used in the parent process)""""""\nimport logging\nimport pickle\nimport sys\nimport os\nfrom time import sleep, time\nfrom traceback import format_exc\n\nfrom threading import Thread\nfrom psutil import process_iter\n\nimport zmq\nfrom zmq import ZMQBaseError\n\nfrom petastorm.reader_impl.pickle_serializer import PickleSerializer\nfrom petastorm.workers_pool import EmptyResultError, VentilatedItemProcessedMessage\nfrom petastorm.workers_pool.exec_in_new_process import exec_in_new_process\n\n# When _CONTROL_FINISHED is passed via control socket to a worker, the worker will terminate\n_CONTROL_FINISHED = ""FINISHED""\n# This is the amount of seconds we will wait to all processes to be created. We throw an error if can not start them\n# on time\n_WORKERS_STARTED_TIMEOUT_S = 20\n_SOCKET_LINGER_MS = 1000\n_KEEP_TRYING_WHILE_ZMQ_AGAIN_IS_RAIZED_TIMEOUT_S = 20\n\n# Amount of time we will wait on a the queue to get the next result. If no results received until then, we will\n# recheck if no more items are expected to be ventilated\n_VERIFY_END_OF_VENTILATION_PERIOD = 0.1\n\n_WORKER_STARTED_INDICATOR = \'worker started indicator\'\n\nlogger = logging.getLogger(__name__)\n\n\n#\n# ----------------                                    ------------------\n# |              |  --- _ventilator_send  (push) -->  |                |\n# | main process |  --- _control_sender   (pub)  -->  | worker process |\n# |              |  <-- _results_receiver (pull)  --  |                |\n# ----------------                                    ------------------\n#\n# 1. When ProcessPool start is called, it creates _ventilator_send, _control_sender and _result_receiver\n#    sockets.\n# 2. After initialization is done, worker process sends _WORKER_STARTED_INDICATOR\n# 3. Once ProcessPool receives _WORKER_STARTED_INDICATOR from all workers, the ProcessPool\n#    is ready to start ventilating.\n#\n# 4. Each ventilated message is picked up by one of the workers.\n# 5. Worker process would send 0..n responses for each ventilated message. Each response\n#    is a tuple of (data payload, control). Data payload is serialized using\n#    _serializer instance. Control is always pickled.\n# 6. After the last response to a single ventilated item is transmitted, an instance of VentilatedItemProcessedMessage\n#    is transmitted as a control. This control message is needed to count how many ventilated\n#    items are being processed at each time.\n#\n# 7. Workers are terminated by broadcasting _CONTROL_FINISHED message.\n#\n\n\ndef _keep_retrying_while_zmq_again(timeout, func, allowed_failures=3):\n    """"""Will keep executing func() as long as zmq.Again is being thrown.\n\n    Usage example:\n\n    >>> _keep_retrying_while_zmq_again(\n    >>>   _KEEP_TRYING_WHILE_ZMQ_AGAIN_IS_RAIZED_TIMEOUT_S,\n    >>>   lambda: self._ventilator_send.send_pyobj(\n    >>>      (args, kargs),\n    >>>      flags=zmq.constants.NOBLOCK))\n\n    :param timeout: A :class:`RuntimeError` is raised if could not execute ``func()`` without getting a\n        :class:`zmq.Again` within this timeout. The timeout is defined in seconds.\n    :param func: The function will be executed (as ``func()``)\n    :return: None\n    """"""\n    now = time()\n    failures = 0\n    while time() < now + timeout:\n        try:\n            return func()\n        except zmq.Again:\n            logger.debug(\'zmq.Again exception caught. Will try again\')\n            sleep(0.1)\n            continue\n        except ZMQBaseError as e:\n            # There are race conditions while setting up the zmq socket so you can get unexpected errors\n            # for the first bit of time. We therefore allow for a few unknown failures while the sockets\n            # are warming up. Before propogating them as a true problem.\n            sleep(0.1)\n            failures += 1\n            logger.debug(\'Unexpected ZMQ error \\\'%s\\\' received. Failures %d/%d\', str(e), failures, allowed_failures)\n            if failures > allowed_failures:\n                raise\n    raise RuntimeError(\'Timeout ({} [sec]) has elapsed while keep getting \\\'zmq.Again\\\'\'.format(timeout))\n\n\nclass ProcessPool(object):\n    def __init__(self, workers_count, serializer=None, zmq_copy_buffers=True):\n        """"""Initializes a ProcessPool.\n\n        This pool is different from standard Python pool implementations by the fact that the workers are spawned\n        without using fork. Some issues with using jvm based HDFS driver were observed when the process was forked\n        (could not access HDFS from the forked worker if the driver was already used in the parent process).\n\n        :param workers_count: Number of processes to be spawned\n        :param serializer: An object that would be used for data payload serialization when sending data from a worker\n          process to the main process. ``PickleSerializer`` is used by default. May use\n          :class:`petastorm.reader_impl.PyarrowSerializer` or\n          :class:`petastorm.reader_impl.ArrowTableSerializer` (should be used together with\n          :class:`petastorm.reader.ArrowReader`)\n        :param zmq_copy_buffers: When set to False, we will use a zero-memory-copy feature of recv_multipart.\n          A downside of using this zero memory copy feature is that it does not play nice with Python GC and cases\n          were observed when it resulted in wild memory footprint swings. Having the buffers copied is typically a\n          safer alternative.\n        """"""\n        self._workers = []\n        self._ventilator_send = None\n        self._control_sender = None\n        self.workers_count = workers_count\n        self._results_receiver_poller = None\n        self._results_receiver = None\n\n        self._ventilated_items = 0\n        self._ventilated_items_processed = 0\n        self._ventilator = None\n        self._serializer = serializer or PickleSerializer()\n        self._zmq_copy_buffers = zmq_copy_buffers\n\n    def _create_local_socket_on_random_port(self, context, socket_type):\n        """"""Creates a zmq socket on a random port.\n\n        :param context: zmq context\n        :param socket_type: zmq socket type\n        :return: A tuple: ``(zmq_socket, endpoint_address)``\n        """"""\n        LOCALHOST = \'tcp://127.0.0.1\'\n        socket = context.socket(socket_type)\n\n        # There are race conditions where the socket can close when messages are still trying to be sent by zmq.\n        # This can end up causing zmq to block indefinitely when sending objects or shutting down. Having the socket\n        # linger on close helps prevent this.\n        socket.linger = _SOCKET_LINGER_MS\n\n        port = socket.bind_to_random_port(LOCALHOST)\n        return socket, \'{}:{}\'.format(LOCALHOST, port)\n\n    def start(self, worker_class, worker_setup_args=None, ventilator=None):\n        """"""Starts worker processes.\n\n        Will block until all processes to subscribe to the worker queue (the messages are distributed by zmq on write\n        so if only one, out of many, workers is up at the time of \'ventilation\', the initial load won\'t be balanced\n        between workers. If can not start the workers in timely fashion, will raise an exception.\n\n        :param worker_class: A class of the worker class. The class will be instantiated in the worker process. The\n            class must implement :class:`.WorkerBase` protocol.\n        :param worker_setup_args: Argument that will be passed to \'args\' property of the instantiated\n            :class:`.WorkerBase`.\n        :param ventilator: Optional ventilator to handle ventilating items to the process pool. Process pool needs\n            to know about the ventilator to know if it has completed ventilating items.\n        :return: ``None``\n        """"""\n        # Initialize a zeromq context\n        self._context = zmq.Context()\n\n        # Ventilator socket used to send out tasks to workers\n        self._ventilator_send, worker_receiver_socket = self._create_local_socket_on_random_port(self._context,\n                                                                                                 zmq.PUSH)\n\n        # Control socket is used to signal termination of the pool\n        self._control_sender, control_socket = self._create_local_socket_on_random_port(self._context, zmq.PUB)\n        self._results_receiver, results_sender_socket = self._create_local_socket_on_random_port(self._context,\n                                                                                                 zmq.PULL)\n\n        # We need poller to be able to read results from workers in a non-blocking manner\n        self._results_receiver_poller = zmq.Poller()\n        self._results_receiver_poller.register(self._results_receiver, zmq.POLLIN)\n\n        # Start a bunch of processes\n        self._workers = [\n            exec_in_new_process(_worker_bootstrap, worker_class, worker_id, control_socket, worker_receiver_socket,\n                                results_sender_socket, os.getpid(), self._serializer, worker_setup_args)\n            for worker_id in range(self.workers_count)]\n\n        # Block until we have get a _WORKER_STARTED_INDICATOR from all our workers\n        self._wait_for_workers_to_start()\n\n        if ventilator:\n            self._ventilator = ventilator\n            self._ventilator.start()\n\n    def _wait_for_workers_to_start(self):\n        """"""Waits for all workers to start.""""""\n        for _ in range(self.workers_count):\n            started_indicator = _keep_retrying_while_zmq_again(\n                _KEEP_TRYING_WHILE_ZMQ_AGAIN_IS_RAIZED_TIMEOUT_S,\n                lambda: self._results_receiver.recv_pyobj(flags=zmq.constants.NOBLOCK))\n            assert _WORKER_STARTED_INDICATOR == started_indicator\n\n    def ventilate(self, *args, **kargs):\n        """"""Sends a work item to a worker process. Will result in worker.process(...) call with arbitrary arguments.""""""\n        self._ventilated_items += 1\n        logger.debug(\'ventilate called. total ventilated items count %d\', self._ventilated_items)\n        # There is a race condition when sending objects to zmq that if all workers have been killed, sending objects\n        # can block indefinitely. By using NOBLOCK, an exception is thrown stating that all resources have been\n        # exhausted which the user can decide how to handle instead of just having the process hang.\n        _keep_retrying_while_zmq_again(_KEEP_TRYING_WHILE_ZMQ_AGAIN_IS_RAIZED_TIMEOUT_S,\n                                       lambda: self._ventilator_send.send_pyobj((args, kargs),\n                                                                                flags=zmq.constants.NOBLOCK))\n\n    def get_results(self):\n        """"""Returns results from worker pool\n\n        :param timeout: If None, will block forever, otherwise will raise :class:`.TimeoutWaitingForResultError`\n            exception if no data received within the timeout (in seconds)\n        :return: arguments passed to ``publish_func(...)`` by a worker. If no more results are anticipated,\n            :class:`.EmptyResultError` is raised.\n        """"""\n\n        while True:\n            # If there is no more work to do, raise an EmptyResultError\n            logger.debug(\'ventilated_items=%d ventilated_items_processed=%d ventilator.completed=%s\',\n                         self._ventilated_items, self._ventilated_items_processed,\n                         str(self._ventilator.completed()) if self._ventilator else \'N/A\')\n            if self._ventilated_items == self._ventilated_items_processed:\n                # We also need to check if we are using a ventilator and if it is completed\n                if not self._ventilator or self._ventilator.completed():\n                    logger.debug(\'ventilator reported it has completed. Reporting end of results\')\n                    raise EmptyResultError()\n\n            logger.debug(\'get_results polling on the next result\')\n            socks = self._results_receiver_poller.poll(_VERIFY_END_OF_VENTILATION_PERIOD * 1e3)\n            if not socks:\n                continue\n            # Result message is a tuple containing data payload and possible exception (or None).\n            # By specifying pyarrow_serialize=True, we may choose to use pyarrow serializer which is faster, but\n            # does not support all data types correctly.\n            fast_serialized, pickle_serialized = self._results_receiver.recv_multipart(copy=self._zmq_copy_buffers)\n            pickle_serialized = pickle.loads(pickle_serialized)\n\n            if pickle_serialized:\n                logger.debug(\'get_results a pickled message %s\', type(pickle_serialized))\n                if isinstance(pickle_serialized, VentilatedItemProcessedMessage):\n                    self._ventilated_items_processed += 1\n                    if self._ventilator:\n                        self._ventilator.processed_item()\n                elif isinstance(pickle_serialized, Exception):\n                    self.stop()\n                    self.join()\n                    raise pickle_serialized\n            else:\n                logger.debug(\'get_results received new results\')\n                if self._zmq_copy_buffers:\n                    deserialized_result = self._serializer.deserialize(fast_serialized)\n                else:\n                    deserialized_result = self._serializer.deserialize(fast_serialized.buffer)\n                return deserialized_result\n\n    def stop(self):\n        """"""Stops all workers (non-blocking)""""""\n        logger.debug(\'stopping\')\n        if self._ventilator:\n            self._ventilator.stop()\n        try:\n            self._control_sender.send_string(_CONTROL_FINISHED)\n        except ZMQBaseError as e:\n            logger.warning(\'Stopping worker processes failed with \\\'%s\\\'. Does not necessary indicates an error.\'\n                           \'This can happen if worker processes were terminated due to an error raised in that \'\n                           \'process. See the log for additional messages from the failed worker.\', str(e))\n\n    def join(self):\n        """"""Blocks until all workers are terminated.""""""\n\n        logger.debug(\'joining\')\n\n        # Slow joiner problem with zeromq means that not all workers are guaranteed to have gotten\n        # the stop event. Therefore we will keep sending it until all workers are stopped to prevent\n        # a deadlock.\n        while any([w.poll() is None for w in self._workers]):\n            self.stop()\n            sleep(.1)\n\n        for w in self._workers:\n            w.wait()\n        self._ventilator_send.close()\n        self._control_sender.close()\n        self._results_receiver.close()\n        self._context.destroy()\n\n    @property\n    def diagnostics(self):\n        # items_produced is updated only when VentilatedItemProcessedMessage is received. This will happen only on the\n        # next call to get_results, so it\'s value may lag.\n        return {\n            \'items_consumed\': self._ventilated_items,\n            \'items_produced\': self._ventilated_items_processed,\n            \'items_inprocess\': self._ventilated_items - self._ventilated_items_processed,\n        }\n\n\ndef _serialize_result_and_send(socket, serializer, data):\n    # Result message is a tuple containing data payload and possible exception (or None).\n    # By specifying pyarrow_serialize=True, we may choose to use pyarrow serializer which is faster, but\n    # does not support all data types correctly.\n    socket.send_multipart([serializer.serialize(data), pickle.dumps(None)])\n\n\ndef _monitor_thread_function(main_process_pid):\n    while True:\n        logger.debug(\'Monitor thread monitoring pid: %d\', main_process_pid)\n        main_process_alive = any([process.pid for process in process_iter() if process.pid == main_process_pid])\n        if not main_process_alive:\n            logger.debug(\'Main process with pid %d is dead. Killing worker\', main_process_pid)\n            os._exit(0)\n        sleep(1)\n\n\ndef _worker_bootstrap(worker_class, worker_id, control_socket, worker_receiver_socket, results_sender_socket,\n                      main_process_pid, serializer, worker_args):\n    """"""This is the root of the spawned worker processes.\n\n    :param worker_class: A class with worker implementation.\n    :param worker_id: An integer. Unique for each worker.\n    :param control_socket: zmq socket used to control the worker (currently supports only :class:`zmq.FINISHED` signal)\n    :param worker_receiver_socket: A zmq socket used to deliver tasks to the worker\n    :param results_sender_socket: A zmq socket used to deliver the work products to the consumer\n    :param serializer: A serializer object (with serialize/deserialize methods) or None.\n    :param worker_args: Application specific parameter passed to worker constructor\n    :return: ``None``\n    """"""\n    logger.debug(\'Starting _worker_bootstrap\')\n    context = zmq.Context()\n\n    logger.debug(\'Connecting sockets\')\n    # Set up a channel to receive work from the ventilator\n    work_receiver = context.socket(zmq.PULL)\n    work_receiver.linger = 0\n    work_receiver.connect(worker_receiver_socket)\n\n    # Set up a channel to send result of work to the results reporter\n    results_sender = context.socket(zmq.PUSH)\n    results_sender.linger = 0\n    results_sender.connect(results_sender_socket)\n\n    # Set up a channel to receive control messages over\n    control_receiver = context.socket(zmq.SUB)\n    control_receiver.linger = 0\n    control_receiver.connect(control_socket)\n    _setsockopt(control_receiver, zmq.SUBSCRIBE, b"""")\n\n    logger.debug(\'Setting up poller\')\n    # Set up a poller to multiplex the work receiver and control receiver channels\n    poller = zmq.Poller()\n    poller.register(work_receiver, zmq.POLLIN)\n    poller.register(control_receiver, zmq.POLLIN)\n\n    results_sender.send_pyobj(_WORKER_STARTED_INDICATOR)\n\n    # Use this \'none_marker\' as the first argument to send_multipart.\n    none_marker = bytes()\n\n    logger.debug(\'Instantiating a worker\')\n    # Instantiate a worker\n    worker = worker_class(worker_id, lambda data: _serialize_result_and_send(results_sender, serializer, data),\n                          worker_args)\n\n    logger.debug(\'Starting monitor loop\')\n    thread = Thread(target=_monitor_thread_function, args=(main_process_pid,))\n    thread.daemon = True\n    thread.start()\n\n    # Loop and accept messages from both channels, acting accordingly\n    logger.debug(\'Entering worker loop\')\n    while True:\n        logger.debug(\'Polling new message\')\n        socks = dict(poller.poll())\n\n        # If the message came from work_receiver channel\n        if socks.get(work_receiver) == zmq.POLLIN:\n            try:\n                args, kargs = work_receiver.recv_pyobj()\n                logger.debug(\'Starting worker.process\')\n                worker.process(*args, **kargs)\n                logger.debug(\'Finished worker.process\')\n                results_sender.send_multipart([none_marker, pickle.dumps(VentilatedItemProcessedMessage())])\n                logger.debug(\'Sending result\')\n            except Exception as e:  # pylint: disable=broad-except\n                stderr_message = \'Worker %d terminated: unexpected exception:\\n\' % worker_id\n                stderr_message += format_exc()\n                logger.debug(\'worker.process failed with exception %s\', stderr_message)\n                sys.stderr.write(stderr_message)\n                results_sender.send_multipart([none_marker, pickle.dumps(e)])\n                return\n\n        # If the message came over the control channel, shut down the worker.\n        if socks.get(control_receiver) == zmq.POLLIN:\n            control_message = control_receiver.recv_string()\n            logger.debug(\'Received control message %s\', control_message)\n            if control_message == _CONTROL_FINISHED:\n                worker.shutdown()\n                break\n\n\ndef _setsockopt(sock, option, value):\n    """"""\n    This wraps setting socket options since python2 vs python3 handles strings differently\n    and pyzmq requires a different call. See http://pyzmq.readthedocs.io/en/latest/unicode.html\n    """"""\n    try:\n        sock.setsockopt(option, value)\n    except TypeError:\n        sock.setsockopt_string(option, value)\n'"
petastorm/workers_pool/thread_pool.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport cProfile\nimport pstats\nimport random\nimport sys\nfrom threading import Thread, Event\nfrom traceback import format_exc\n\nfrom six.moves import queue\n\nfrom petastorm.workers_pool import EmptyResultError, VentilatedItemProcessedMessage\n\n# Defines how frequently will we check the stop event while waiting on a blocking queue\nIO_TIMEOUT_INTERVAL_S = 0.001\n# Amount of time we will wait on a the queue to get the next result. If no results received until then, we will\n# recheck if no more items are expected to be ventilated\n_VERIFY_END_OF_VENTILATION_PERIOD = 0.1\n\n\nclass WorkerTerminationRequested(Exception):\n    """"""This exception will be raised if a thread is being stopped while waiting to write to the results queue.""""""\n\n\nclass WorkerThread(Thread):\n    """"""Thread class with a stop() method. The thread itself has to check\n    regularly for the stopped() condition.""""""\n\n    def __init__(self, worker_impl, stop_event, ventilator_queue, results_queue, profiling_enabled=False):\n        super(WorkerThread, self).__init__()\n        self._stop_event = stop_event\n        self._worker_impl = worker_impl\n        self._ventilator_queue = ventilator_queue\n        self._results_queue = results_queue\n        self._profiling_enabled = profiling_enabled\n        if profiling_enabled:\n            self.prof = cProfile.Profile()\n\n    def run(self):\n        if self._profiling_enabled:\n            self.prof.enable()\n        # Loop and accept messages from both channels, acting accordingly\n        while True:\n            # Check for stop event first to prevent erroneous reuse\n            if self._stop_event.is_set():\n                break\n            # If the message came from work_receiver channel\n            try:\n                (args, kargs) = self._ventilator_queue.get(block=True, timeout=IO_TIMEOUT_INTERVAL_S)\n                self._worker_impl.process(*args, **kargs)\n                self._worker_impl.publish_func(VentilatedItemProcessedMessage())\n            except queue.Empty:\n                pass\n            except WorkerTerminationRequested:\n                pass\n            except Exception as e:  # pylint: disable=broad-except\n                stderr_message = \'Worker %d terminated: unexpected exception:\\n\' % self._worker_impl.worker_id\n                stderr_message += format_exc()\n                sys.stderr.write(stderr_message)\n                self._results_queue.put(e)\n                break\n        if self._profiling_enabled:\n            self.prof.disable()\n\n\nclass ThreadPool(object):\n    def __init__(self, workers_count, results_queue_size=50, profiling_enabled=False):\n        """"""Initializes a thread pool.\n\n        TODO: consider using a standard thread pool\n        (e.g. http://elliothallmark.com/2016/12/23/requests-with-concurrent-futures-in-python-2-7/ as an implementation)\n\n        Originally implemented our own pool to match the interface of ProcessPool (could not find a process pool\n        implementation that would not use fork)\n\n        :param workers_count: Number of threads\n        :param profile: Whether to run a profiler on the threads\n        """"""\n        self._seed = random.randint(0, 100000)\n        self._workers = []\n        self._ventilator_queue = None\n        self.workers_count = workers_count\n        self._results_queue_size = results_queue_size\n        # Worker threads will watch this event and gracefully shutdown when the event is set\n        self._stop_event = Event()\n        self._profiling_enabled = profiling_enabled\n\n        self._ventilated_items = 0\n        self._ventilated_items_processed = 0\n        self._ventilator = None\n\n    def start(self, worker_class, worker_args=None, ventilator=None):\n        """"""Starts worker threads.\n\n        :param worker_class: A class of the worker class. The class will be instantiated in the worker process. The\n          class must implement :class:`.WorkerBase` protocol\n        :param worker_setup_args: Argument that will be passed to ``args`` property of the instantiated\n          :class:`.WorkerBase`\n        :return: ``None``\n        """"""\n        # Verify stop_event and raise exception if it\'s already set!\n        if self._stop_event.is_set():\n            raise RuntimeError(\'ThreadPool({}) cannot be reused! stop_event set? {}\'\n                               .format(len(self._workers), self._stop_event.is_set()))\n\n        # Set up a channel to send work\n        self._ventilator_queue = queue.Queue()\n        self._results_queue = queue.Queue(self._results_queue_size)\n        self._workers = []\n        for worker_id in range(self.workers_count):\n            worker_impl = worker_class(worker_id, self._stop_aware_put, worker_args)\n            new_thread = WorkerThread(worker_impl, self._stop_event, self._ventilator_queue,\n                                      self._results_queue, self._profiling_enabled)\n            # Make the thread daemonic. Since it only reads it\'s ok to abort while running - no resource corruption\n            # will occur.\n            new_thread.daemon = True\n            self._workers.append(new_thread)\n\n        # Spin up all worker threads\n        for w in self._workers:\n            w.start()\n\n        if ventilator:\n            self._ventilator = ventilator\n            self._ventilator.start()\n\n    def ventilate(self, *args, **kargs):\n        """"""Sends a work item to a worker process. Will result in ``worker.process(...)`` call with arbitrary arguments.\n        """"""\n        self._ventilated_items += 1\n        self._ventilator_queue.put((args, kargs))\n\n    def get_results(self):\n        """"""Returns results from worker pool or re-raise worker\'s exception if any happen in worker thread.\n\n        :param timeout: If None, will block forever, otherwise will raise :class:`.TimeoutWaitingForResultError`\n            exception if no data received within the timeout (in seconds)\n\n        :return: arguments passed to ``publish_func(...)`` by a worker. If no more results are anticipated,\n                 :class:`.EmptyResultError`.\n        """"""\n\n        while True:\n            # If there is no more work to do, raise an EmptyResultError\n            if self._results_queue.empty() and self._ventilated_items == self._ventilated_items_processed:\n                # We also need to check if we are using a ventilator and if it is completed\n                if not self._ventilator or self._ventilator.completed():\n                    raise EmptyResultError()\n\n            try:\n                result = self._results_queue.get(timeout=_VERIFY_END_OF_VENTILATION_PERIOD)\n                if isinstance(result, VentilatedItemProcessedMessage):\n                    self._ventilated_items_processed += 1\n                    if self._ventilator:\n                        self._ventilator.processed_item()\n                    continue\n                elif isinstance(result, Exception):\n                    self.stop()\n                    self.join()\n                    raise result\n                else:\n                    return result\n            except queue.Empty:\n                continue\n\n    def stop(self):\n        """"""Stops all workers (non-blocking).""""""\n        if self._ventilator:\n            self._ventilator.stop()\n        self._stop_event.set()\n\n    def join(self):\n        """"""Block until all workers are terminated.""""""\n        for w in self._workers:\n            if w.isAlive():\n                w.join()\n\n        if self._profiling_enabled:\n            # If we have profiling set, collect stats and print them\n            stats = None\n            for w in self._workers:\n                if stats:\n                    stats.add(w.prof)\n                else:\n                    stats = pstats.Stats(w.prof)\n            stats.sort_stats(\'cumulative\').print_stats()\n\n    def _stop_aware_put(self, data):\n        """"""This method is called to write the results to the results queue. We use ``put`` in a non-blocking way so we\n        can gracefully terminate the worker thread without being stuck on :func:`Queue.put`.\n\n        The method raises :class:`.WorkerTerminationRequested` exception that should be passed through all the way up to\n        :func:`WorkerThread.run` which will gracefully terminate main worker loop.""""""\n        while True:\n            try:\n                self._results_queue.put(data, block=True, timeout=IO_TIMEOUT_INTERVAL_S)\n                return\n            except queue.Full:\n                pass\n\n            if self._stop_event.is_set():\n                raise WorkerTerminationRequested()\n\n    def results_qsize(self):\n        return self._results_queue.qsize()\n\n    @property\n    def diagnostics(self):\n        return {\'output_queue_size\': self.results_qsize()}\n'"
petastorm/workers_pool/ventilator.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport random\nimport threading\nfrom abc import ABCMeta, abstractmethod\nfrom time import sleep\n\nimport six\n\n_VENTILATION_INTERVAL = 0.01\n\n\n@six.add_metaclass(ABCMeta)\nclass Ventilator(object):\n    """"""Manages items to be ventilated to a worker pool.""""""\n\n    def __init__(self, ventilate_fn):\n        self._ventilate_fn = ventilate_fn\n\n    @abstractmethod\n    def start(self):\n        """"""Starts the ventilator, beginning to ventilate to the worker pool after this call.\n        Therefore the worker pool must be ready to receive ventilated items.""""""\n        return\n\n    @abstractmethod\n    def processed_item(self):\n        """"""A callback for the worker pool to tell the ventilator that it has processed an item from the ventilation\n        queue. This allows the ventilator to know how many items are currently on the ventilation queue.\n        This function should not have a return value.""""""\n\n    @abstractmethod\n    def completed(self):\n        """"""Returns whether the ventilator has completed ventilating all items it expects to ever ventilate.""""""\n        return\n\n    @abstractmethod\n    def stop(self):\n        """"""Tells the ventilator to stop ventilating.""""""\n        return\n\n\nclass ConcurrentVentilator(Ventilator):\n    """"""\n    A ConcurrentVentilator handles ventilation of a pre-determined list of items to a worker pool and performs\n    the ventilation concurrently in a separate thread. It will keep track of how many items are currently in the\n    ventilation queue and prevent it from monotonically increasing in order to prevent boundless memory requirements.\n    It allows for multiple (or infinite) iterations of ventilating the items, optionally randomizing the order of\n    items being ventilated at the start of each iteration.\n    """"""\n\n    def __init__(self,\n                 ventilate_fn,\n                 items_to_ventilate,\n                 iterations=1,\n                 randomize_item_order=False,\n                 max_ventilation_queue_size=None,\n                 ventilation_interval=_VENTILATION_INTERVAL):\n        """"""\n        Constructor for a concurrent ventilator.\n\n        :param ventilate_fn: The function to be called when ventilating. Usually the worker pool ventilate function.\n        :param items_to_ventilate: (``list[dict]``) The list of items to ventilate. Each item is a ``dict`` denoting\n                the ``**kwargs`` eventually passed to a worker process function\n        :param iterations: (int) How many iterations through items_to_ventilate should be done and ventilated to the\n                worker pool. For example if set to 2 each item in items_to_ventilate will be ventilated 2 times. If\n                ``None`` is passed, the ventilator will continue ventilating forever.\n        :param randomize_item_order: (``bool``) Whether to randomize the item order in items_to_ventilate. This will be\n                done on every individual iteration.\n        :param max_ventilation_queue_size: (``int``) The maximum number of items to be stored in the ventilation queue.\n                The higher this number, the higher potential memory requirements. By default it will use the size\n                of items_to_ventilate since that can definitely be held in memory.\n        :param ventilation_interval: (``float`` in seconds) How much time passes between checks on whether something\n                can be ventilated (when the ventilation queue is considered full).\n        """"""\n        super(ConcurrentVentilator, self).__init__(ventilate_fn)\n\n        if iterations is not None and (not isinstance(iterations, int) or iterations < 1):\n            raise ValueError(\'iterations must be positive integer or None\')\n\n        if not isinstance(items_to_ventilate, list) or any(not isinstance(item, dict) for item in items_to_ventilate):\n            raise ValueError(\'items_to_ventilate must be a list of dicts\')\n\n        self._items_to_ventilate = items_to_ventilate\n        self._iterations_remaining = iterations\n        self._randomize_item_order = randomize_item_order\n\n        self._iterations = iterations\n\n        # For the default max ventilation queue size we will use the size of the items to ventilate\n        self._max_ventilation_queue_size = max_ventilation_queue_size or len(items_to_ventilate)\n        self._ventilation_interval = ventilation_interval\n\n        self._current_item_to_ventilate = 0\n        self._ventilation_thread = None\n        self._ventilated_items_count = 0\n        self._processed_items_count = 0\n        self._stop_requested = False\n\n    def start(self):\n        # Start the ventilation thread\n        self._ventilation_thread = threading.Thread(target=self._ventilate, args=())\n        self._ventilation_thread.daemon = True\n        self._ventilation_thread.start()\n\n    def processed_item(self):\n        self._processed_items_count += 1\n\n    def completed(self):\n        assert self._iterations_remaining is None or self._iterations_remaining >= 0\n        return self._stop_requested or self._iterations_remaining == 0 or not self._items_to_ventilate\n\n    def reset(self):\n        """"""Will restart the ventilation from the beginning. Currently, we may do this only if the ventilator has\n        finished ventilating all its items (i.e. ventilator.completed()==True)\n        """"""\n        if not self.completed():\n            # Might be hard to solve all race conditions, unless no more ventilation is going on.\n            raise NotImplementedError(\'Reseting ventilator while ventilating is not supported.\')\n\n        self._iterations_remaining = self._iterations\n        self.start()\n\n    def _ventilate(self):\n        while True:\n            # Stop condition is when no iterations are remaining or there are no items to ventilate\n            if self.completed():\n                break\n\n            # If we are ventilating the first item, we check if we would like to randomize the item order\n            if self._current_item_to_ventilate == 0 and self._randomize_item_order:\n                random.shuffle(self._items_to_ventilate)\n\n            # Block until queue has room, but use continue to allow for checking if stop has been called\n            if self._ventilated_items_count - self._processed_items_count >= self._max_ventilation_queue_size:\n                sleep(self._ventilation_interval)\n                continue\n\n            item_to_ventilate = self._items_to_ventilate[self._current_item_to_ventilate]\n            self._ventilate_fn(**item_to_ventilate)\n            self._current_item_to_ventilate += 1\n            self._ventilated_items_count += 1\n\n            if self._current_item_to_ventilate >= len(self._items_to_ventilate):\n                self._current_item_to_ventilate = 0\n                # If iterations was set to None, that means we will iterate until stop is called\n                if self._iterations_remaining is not None:\n                    self._iterations_remaining -= 1\n\n    def stop(self):\n        self._stop_requested = True\n        if self._ventilation_thread:\n            self._ventilation_thread.join()\n            self._ventilation_thread = None\n'"
petastorm/workers_pool/worker_base.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import abstractmethod\n\n\nclass WorkerBase(object):\n    def __init__(self, worker_id, publish_func, args):\n        """"""Initializes a worker.\n\n        :param worker_id: An integer uniquely identifying a worker instance\n        :param publish_func: Function handler to be used to publish data\n        :param args: application specific args\n        """"""\n        self.worker_id = worker_id\n        self.publish_func = publish_func\n        self.args = args\n\n    @abstractmethod\n    def process(self, *args, **kargs):\n        pass\n\n    def shutdown(self):\n        pass\n'"
examples/hello_world/external_dataset/__init__.py,0,b''
examples/hello_world/external_dataset/generate_external_dataset.py,0,"b'#  Copyright (c) 2018-2019 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""\nThis is part of a minimal example of how to use petastorm to read a dataset not created\nwith petastorm. Generates a sample dataset from random data.\n""""""\n\nimport random\n\nfrom pyspark.sql import SparkSession, Row\n\n\ndef row_generator(x):\n    """"""Returns a single entry in the generated dataset. Return a bunch of random values as an example.""""""\n    return Row(id=x, value1=random.randint(-255, 255), value2=random.randint(-255, 255))\n\n\ndef generate_external_dataset(output_url=\'file:///tmp/external_dataset\'):\n    """"""Creates an example dataset at output_url in Parquet format""""""\n    spark = SparkSession.builder \\\n        .master(\'local[2]\') \\\n        .getOrCreate()\n    sc = spark.sparkContext\n\n    rows_count = 10\n    rows_rdd = sc.parallelize(range(rows_count)) \\\n        .map(row_generator)\n\n    spark.createDataFrame(rows_rdd). \\\n        write. \\\n        mode(\'overwrite\'). \\\n        parquet(output_url)\n\n\nif __name__ == \'__main__\':\n    generate_external_dataset()\n'"
examples/hello_world/external_dataset/python_hello_world.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Minimal example of how to read samples from a dataset generated by `generate_non_petastorm_dataset.py`\nusing plain Python""""""\n\nfrom __future__ import print_function\n\nfrom petastorm import make_batch_reader\n\n\ndef python_hello_world(dataset_url=\'file:///tmp/external_dataset\'):\n    # Reading data from the non-Petastorm Parquet via pure Python\n    with make_batch_reader(dataset_url, schema_fields=[""id"", ""value1"", ""value2""]) as reader:\n        for schema_view in reader:\n            # make_batch_reader() returns batches of rows instead of individual rows\n            print(""Batched read:\\nid: {0} value1: {1} value2: {2}"".format(\n                schema_view.id, schema_view.value1, schema_view.value2))\n\n\nif __name__ == \'__main__\':\n    python_hello_world()\n'"
examples/hello_world/external_dataset/pytorch_hello_world.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Minimal example of how to read samples from a dataset generated by `generate_external_dataset.py`\nusing pytorch, using make_batch_reader() instead of make_reader()""""""\n\nfrom __future__ import print_function\n\nfrom petastorm import make_batch_reader\nfrom petastorm.pytorch import DataLoader\n\n\ndef pytorch_hello_world(dataset_url=\'file:///tmp/external_dataset\'):\n    with DataLoader(make_batch_reader(dataset_url)) as train_loader:\n        sample = next(iter(train_loader))\n        # Because we are using make_batch_reader(), each read returns a batch of rows instead of a single row\n        print(""id batch: {0}"".format(sample[\'id\']))\n\n\nif __name__ == \'__main__\':\n    pytorch_hello_world()\n'"
examples/hello_world/external_dataset/tensorflow_hello_world.py,3,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Minimal example of how to read samples from a dataset generated by `generate_external_dataset.py`\nusing tensorflow, using make_batch_reader() instead of make_reader()""""""\n\nfrom __future__ import print_function\n\nimport tensorflow.compat.v1 as tf  # pylint: disable=import-error\n\nfrom petastorm import make_batch_reader\nfrom petastorm.tf_utils import tf_tensors, make_petastorm_dataset\n\n\ndef tensorflow_hello_world(dataset_url=\'file:///tmp/external_dataset\'):\n    # Example: tf_tensors will return tensors with dataset data\n    with make_batch_reader(dataset_url) as reader:\n        tensor = tf_tensors(reader)\n        with tf.Session() as sess:\n            # Because we are using make_batch_reader(), each read returns a batch of rows instead of a single row\n            batched_sample = sess.run(tensor)\n            print(""id batch: {0}"".format(batched_sample.id))\n\n    # Example: use tf.data.Dataset API\n    with make_batch_reader(dataset_url) as reader:\n        dataset = make_petastorm_dataset(reader)\n        iterator = dataset.make_one_shot_iterator()\n        tensor = iterator.get_next()\n        with tf.Session() as sess:\n            batched_sample = sess.run(tensor)\n            print(""id batch: {0}"".format(batched_sample.id))\n\n\nif __name__ == \'__main__\':\n    tensorflow_hello_world()\n'"
examples/hello_world/petastorm_dataset/__init__.py,0,b''
examples/hello_world/petastorm_dataset/generate_petastorm_dataset.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""\nThis is a minimal example of how to generate a petastorm dataset. Generates a\nsample dataset with some random data.\n""""""\n\nimport numpy as np\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import IntegerType\n\nfrom petastorm.codecs import ScalarCodec, CompressedImageCodec, NdarrayCodec\nfrom petastorm.etl.dataset_metadata import materialize_dataset\nfrom petastorm.unischema import dict_to_spark_row, Unischema, UnischemaField\n\n# The schema defines how the dataset schema looks like\nHelloWorldSchema = Unischema(\'HelloWorldSchema\', [\n    UnischemaField(\'id\', np.int32, (), ScalarCodec(IntegerType()), False),\n    UnischemaField(\'image1\', np.uint8, (128, 256, 3), CompressedImageCodec(\'png\'), False),\n    UnischemaField(\'array_4d\', np.uint8, (None, 128, 30, None), NdarrayCodec(), False),\n])\n\n\ndef row_generator(x):\n    """"""Returns a single entry in the generated dataset. Return a bunch of random values as an example.""""""\n    return {\'id\': x,\n            \'image1\': np.random.randint(0, 255, dtype=np.uint8, size=(128, 256, 3)),\n            \'array_4d\': np.random.randint(0, 255, dtype=np.uint8, size=(4, 128, 30, 3))}\n\n\ndef generate_petastorm_dataset(output_url=\'file:///tmp/hello_world_dataset\'):\n    rowgroup_size_mb = 256\n\n    spark = SparkSession.builder.config(\'spark.driver.memory\', \'2g\').master(\'local[2]\').getOrCreate()\n    sc = spark.sparkContext\n\n    # Wrap dataset materialization portion. Will take care of setting up spark environment variables as\n    # well as save petastorm specific metadata\n    rows_count = 10\n    with materialize_dataset(spark, output_url, HelloWorldSchema, rowgroup_size_mb):\n\n        rows_rdd = sc.parallelize(range(rows_count))\\\n            .map(row_generator)\\\n            .map(lambda x: dict_to_spark_row(HelloWorldSchema, x))\n\n        spark.createDataFrame(rows_rdd, HelloWorldSchema.as_spark_schema()) \\\n            .coalesce(10) \\\n            .write \\\n            .mode(\'overwrite\') \\\n            .parquet(output_url)\n\n\nif __name__ == \'__main__\':\n    generate_petastorm_dataset()\n'"
examples/hello_world/petastorm_dataset/pyspark_hello_world.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Minimal example of how to read samples from a dataset generated by `generate_hello_world_dataset.py`\nusing pyspark""""""\n\nfrom __future__ import print_function\n\nfrom pyspark.sql import SparkSession\n\nfrom examples.hello_world.petastorm_dataset.generate_petastorm_dataset import HelloWorldSchema\nfrom petastorm.spark_utils import dataset_as_rdd\n\n\ndef pyspark_hello_world(dataset_url=\'file:///tmp/hello_world_dataset\'):\n    spark = SparkSession \\\n        .builder \\\n        .master(\'local[1]\') \\\n        .getOrCreate()\n\n    # dataset_as_rdd creates an rdd of named tuples.\n    rdd = dataset_as_rdd(dataset_url, spark, [HelloWorldSchema.id, HelloWorldSchema.image1])\n    print(\'An id in the dataset: \', rdd.first().id)\n\n    # Create a dataframe object from a parquet file\n    dataframe = spark.read.parquet(dataset_url)\n\n    # Show a schema\n    dataframe.printSchema()\n\n    # Count all\n    dataframe.count()\n\n    # Show just some columns\n    dataframe.select(\'id\').show()\n\n    # This is how you can use a standard SQL to query a dataset. Note that the data is not decoded in this case.\n    number_of_rows = spark.sql(\n        \'SELECT count(id) \'\n        \'from parquet.`{}` \'.format(dataset_url)).collect()\n    print(\'Number of rows in the dataset: {}\'.format(number_of_rows[0][0]))\n\n\nif __name__ == \'__main__\':\n    pyspark_hello_world()\n'"
examples/hello_world/petastorm_dataset/python_hello_world.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Minimal example of how to read samples from a dataset generated by `generate_hello_world_dataset.py`\nusing plain Python""""""\n\nfrom __future__ import print_function\n\nfrom petastorm import make_reader\n\n\ndef python_hello_world(dataset_url=\'file:///tmp/hello_world_dataset\'):\n    with make_reader(dataset_url) as reader:\n        # Pure python\n        for sample in reader:\n            print(sample.id)\n            # plt.imshow(sample.image1)\n\n\nif __name__ == \'__main__\':\n    python_hello_world()\n'"
examples/hello_world/petastorm_dataset/pytorch_hello_world.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Minimal example of how to read samples from a dataset generated by `generate_hello_world_dataset.py`\nusing pytorch.""""""\n\nfrom __future__ import print_function\n\nfrom petastorm import make_reader\nfrom petastorm.pytorch import DataLoader\n\n\ndef pytorch_hello_world(dataset_url=\'file:///tmp/hello_world_dataset\'):\n    with DataLoader(make_reader(dataset_url)) as train_loader:\n        sample = next(iter(train_loader))\n        print(sample[\'id\'])\n\n\nif __name__ == \'__main__\':\n    pytorch_hello_world()\n'"
examples/hello_world/petastorm_dataset/tensorflow_hello_world.py,3,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Minimal example of how to read samples from a dataset generated by `generate_hello_world_dataset.py`\nusing tensorflow.""""""\n\nfrom __future__ import print_function\n\nimport tensorflow.compat.v1 as tf  # pylint: disable=import-error\n\nfrom petastorm import make_reader\nfrom petastorm.tf_utils import tf_tensors, make_petastorm_dataset\n\n\ndef tensorflow_hello_world(dataset_url=\'file:///tmp/hello_world_dataset\'):\n    # Example: tf_tensors will return tensors with dataset data\n    with make_reader(dataset_url) as reader:\n        tensor = tf_tensors(reader)\n        with tf.Session() as sess:\n            sample = sess.run(tensor)\n            print(sample.id)\n\n    # Example: use tf.data.Dataset API\n    with make_reader(dataset_url) as reader:\n        dataset = make_petastorm_dataset(reader)\n        iterator = dataset.make_one_shot_iterator()\n        tensor = iterator.get_next()\n        with tf.Session() as sess:\n            sample = sess.run(tensor)\n            print(sample.id)\n\n\nif __name__ == \'__main__\':\n    tensorflow_hello_world()\n'"
examples/imagenet/tests/test_generate_imagenet_dataset.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport shutil\nimport tempfile\nimport unittest\n\nimport cv2\nimport numpy as np\n\nfrom examples.imagenet.generate_petastorm_imagenet import download_nouns_mapping, \\\n    imagenet_directory_to_petastorm_dataset\n\n# Set test image sizes and number of mock nouns/variants\nMOCK_IMAGE_SIZE = (64, 32, 3)\nMOCK_NOUNS_COUNT = 5\nMOCK_VARIANTS_COUNT = 3\n\n\ndef _mock_imagenet_dir(temp_dir):\n    """"""Creates a mock directory with 5 noun-id directores and 3 variants of the noun images. Random images are used.""""""\n    noun_id_to_text = dict()\n    for i in range(MOCK_NOUNS_COUNT):\n        # Make noun-id directory (e.g. n00000001 format)\n        noun_id = \'n0000000{}\'.format(i)\n        noun_id_to_text[noun_id] = \'text for {}\'.format(noun_id)\n        noun_dir = os.path.join(temp_dir, noun_id)\n        os.mkdir(noun_dir)\n\n        # Create 3 noun image variants (e.g n00000001_0001.JPEG)\n        for variant_id in range(MOCK_VARIANTS_COUNT):\n            jpeg_path = os.path.join(noun_dir, \'{}_000{}.JPEG\'.format(noun_id, variant_id))\n            dummy_image = np.random.randint(0, 255, size=MOCK_IMAGE_SIZE, dtype=np.uint8)\n            cv2.imwrite(jpeg_path, dummy_image)\n    return noun_id_to_text\n\n\nclass TestGenerate(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.mock_imagenet_dir = tempfile.mkdtemp()\n        cls.mock_output_dir = tempfile.mkdtemp()\n        cls.noun_id_to_text = _mock_imagenet_dir(cls.mock_imagenet_dir)\n\n    @classmethod\n    def tearDownClass(cls):\n        if os.path.exists(cls.mock_imagenet_dir):\n            shutil.rmtree(cls.mock_imagenet_dir)\n        if os.path.exists(cls.mock_output_dir):\n            shutil.rmtree(cls.mock_output_dir)\n\n    @unittest.skip(\'\')\n    def test_get_labels(self):\n        a = download_nouns_mapping()\n        self.assertEqual(1000, len(a))\n        self.assertEqual(a[\'n03887697\'], \'paper_towel\')\n\n    def test_generate(self):\n        # Use parquet_files_count to speed up the test\n        imagenet_directory_to_petastorm_dataset(TestGenerate.mock_imagenet_dir,\n                                                \'file://\' + TestGenerate.mock_output_dir,\n                                                spark_master=\'local[3]\', parquet_files_count=3,\n                                                noun_id_to_text=TestGenerate.noun_id_to_text)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
examples/mnist/tests/__init__.py,0,b''
examples/mnist/tests/conftest.py,0,"b'import numpy as np\nimport pytest\n\nMOCK_IMAGE_SIZE = (28, 28)\nMOCK_IMAGE_3DIM_SIZE = (28, 28, 1)\nSMALL_MOCK_IMAGE_COUNT = {\n    \'train\': 30,\n    \'test\': 5\n}\nLARGE_MOCK_IMAGE_COUNT = {\n    \'train\': 600,\n    \'test\': 100\n}\n\n\nclass MockDataObj(object):\n    """""" Wraps a mock image array and provide a needed getdata() interface function. """"""\n\n    def __init__(self, a):\n        self.a = a\n\n    def getdata(self):\n        return self.a\n\n\ndef _mock_mnist_data(mock_spec):\n    """"""\n    Creates a mock data dictionary with train and test sets, each containing 5 mock pairs:\n\n        ``(random images, random digit)``.\n    """"""\n    bogus_data = {\n        \'train\': [],\n        \'test\': []\n    }\n\n    for dset, data in bogus_data.items():\n        for _ in range(mock_spec[dset]):\n            pair = (MockDataObj(np.random.randint(0, 255, size=MOCK_IMAGE_SIZE, dtype=np.uint8)),\n                    np.random.randint(0, 9))\n            data.append(pair)\n\n    return bogus_data\n\n\n@pytest.fixture(scope=""session"")\ndef small_mock_mnist_data():\n    return _mock_mnist_data(SMALL_MOCK_IMAGE_COUNT)\n\n\n@pytest.fixture(scope=""session"")\ndef large_mock_mnist_data():\n    return _mock_mnist_data(LARGE_MOCK_IMAGE_COUNT)\n'"
examples/mnist/tests/test_pytorch_mnist.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport os\n\nimport pyarrow  # noqa: F401 pylint: disable=W0611\nimport torch\n\nimport pytest\n\nimport examples.mnist.pytorch_example as pytorch_example\nfrom examples.mnist.generate_petastorm_mnist import mnist_data_to_petastorm_dataset, download_mnist_data\nfrom examples.mnist.tests.conftest import SMALL_MOCK_IMAGE_COUNT\nfrom petastorm import make_reader, TransformSpec\n\nlogging.basicConfig(level=logging.INFO)\n\n\n# Set test image sizes and number of mock nouns/variants\n\n@pytest.fixture(scope=""session"")\ndef generate_mnist_dataset(small_mock_mnist_data, tmpdir_factory):\n    # Using parquet_files_count to speed up the test\n    path = tmpdir_factory.mktemp(\'data\').strpath\n    dataset_url = \'file://{}\'.format(path)\n    mnist_data_to_petastorm_dataset(path, dataset_url, mnist_data=small_mock_mnist_data,\n                                    spark_master=\'local[1]\', parquet_files_count=1)\n    return path\n\n\ndef test_full_pytorch_example(large_mock_mnist_data, tmpdir):\n    # First, generate mock dataset\n    dataset_url = \'file://{}\'.format(tmpdir)\n    mnist_data_to_petastorm_dataset(tmpdir, dataset_url, mnist_data=large_mock_mnist_data,\n                                    spark_master=\'local[1]\', parquet_files_count=1)\n\n    # Next, run a round of training using the pytorce adapting data loader\n    from petastorm.pytorch import DataLoader\n\n    torch.manual_seed(1)\n    device = torch.device(\'cpu\')\n    model = pytorch_example.Net().to(device)\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n\n    transform = TransformSpec(pytorch_example._transform_row, removed_fields=[\'idx\'])\n\n    with DataLoader(make_reader(\'{}/train\'.format(dataset_url), reader_pool_type=\'dummy\', num_epochs=1,\n                                transform_spec=transform), batch_size=32) as train_loader:\n        pytorch_example.train(model, device, train_loader, 10, optimizer, 1)\n    with DataLoader(make_reader(\'{}/test\'.format(dataset_url), reader_pool_type=\'dummy\', num_epochs=1,\n                                transform_spec=transform), batch_size=100) as test_loader:\n        pytorch_example.test(model, device, test_loader)\n\n\ndef test_mnist_download(tmpdir):\n    """""" Demonstrates that MNIST download works, using only the \'test\' data. Assumes data does not change often. """"""\n    o = download_mnist_data(tmpdir, train=False)\n    assert 10000 == len(o)\n    assert o[0][1] == 7\n    assert o[len(o) - 1][1] == 6\n\n\ndef test_generate_mnist_dataset(generate_mnist_dataset):\n    train_path = os.path.join(generate_mnist_dataset, \'train\')\n    assert os.path.exists(train_path)\n    assert os.path.exists(os.path.join(train_path, \'_common_metadata\'))\n\n    test_path = os.path.join(generate_mnist_dataset, \'test\')\n    assert os.path.exists(test_path)\n    assert os.path.exists(os.path.join(test_path, \'_common_metadata\'))\n\n\ndef test_read_mnist_dataset(generate_mnist_dataset):\n    # Verify both datasets via a reader\n    for dset in SMALL_MOCK_IMAGE_COUNT.keys():\n        with make_reader(\'file://{}/{}\'.format(generate_mnist_dataset, dset),\n                         reader_pool_type=\'dummy\', num_epochs=1) as reader:\n            assert sum(1 for _ in reader) == SMALL_MOCK_IMAGE_COUNT[dset]\n'"
examples/mnist/tests/test_tf_mnist.py,0,"b""from examples.mnist import tf_example as tf_example\nfrom examples.mnist.generate_petastorm_mnist import mnist_data_to_petastorm_dataset\nfrom petastorm.tests.test_tf_utils import create_tf_graph\n\n\n@create_tf_graph\ndef test_full_tf_example(large_mock_mnist_data, tmpdir):\n    # First, generate mock dataset\n    dataset_url = 'file://{}'.format(tmpdir)\n    mnist_data_to_petastorm_dataset(tmpdir, dataset_url, mnist_data=large_mock_mnist_data,\n                                    spark_master='local[1]', parquet_files_count=1)\n\n    # Tensorflow train and test\n    tf_example.train_and_test(\n        dataset_url=dataset_url,\n        training_iterations=10,\n        batch_size=10,\n        evaluation_interval=10,\n    )\n"""
examples/spark_dataset_converter/tests/test_converter_example.py,0,"b'import tempfile\nfrom distutils.version import LooseVersion\n\nimport pyspark\nimport pytest\n\nfrom examples.spark_dataset_converter.utils import download_mnist_libsvm\n\n\n@pytest.fixture(scope=\'module\')\ndef mnist_dir():\n    tmp_dir = tempfile.mkdtemp(\'_converter_example_test\')\n    download_mnist_libsvm(tmp_dir)\n    return tmp_dir\n\n\n@pytest.mark.skipif(\n    LooseVersion(pyspark.__version__) < LooseVersion(""3.0""),\n    reason=""Vector columns are not supported for pyspark {} < 3.0.0""\n    .format(pyspark.__version__))\ndef test_converter_pytorch_example(mnist_dir):\n    from examples.spark_dataset_converter.pytorch_converter_example import run\n    run(mnist_dir)\n\n\n@pytest.mark.skipif(\n    LooseVersion(pyspark.__version__) < LooseVersion(""3.0""),\n    reason=""Vector columns are not supported for pyspark {} < 3.0.0""\n    .format(pyspark.__version__))\ndef test_converter_tf_example(mnist_dir):\n    from examples.spark_dataset_converter.tensorflow_converter_example import run\n    run(mnist_dir)\n'"
petastorm/hdfs/tests/__init__.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
petastorm/hdfs/tests/test_hdfs_namenode.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport pickle\nimport textwrap\nimport unittest\n\nimport pytest\nfrom pyarrow.lib import ArrowIOError\n\ntry:\n    from unittest import mock\nexcept ImportError:\n    from mock import mock\n\nfrom petastorm.hdfs.namenode import HdfsNamenodeResolver, HdfsConnector, \\\n    HdfsConnectError, MaxFailoversExceeded, HAHdfsClient, namenode_failover\n\n\nclass HC:\n    """"""Hadoop constants for testing convenience""""""\n    WARP_TURTLE = \'WARP-TURTLE\'\n    FS_WARP_TURTLE = \'hdfs://{}\'.format(WARP_TURTLE)\n    DEFAULT_NN = \'default:8020\'\n    WARP_TURTLE_NN1 = \'some.domain.name.net:8020\'\n    WARP_TURTLE_NN2 = \'some.other.domain.name.net:8020\'\n    WARP_TURTLE_PATH = \'{}/x/y/z\'.format(FS_WARP_TURTLE)\n    HADOOP_CONFIG_PATH = \'/etc/hadoop\'\n\n\nclass MockHadoopConfiguration(object):\n    def __init__(self):\n        self._dict = {}\n\n    def get(self, key):\n        val = None\n        if key in self._dict:\n            val = self._dict[key]\n        # print(\'MockHadoopConfiguration: ""{}"" == ""{}""\'.format(key, val))\n        return val\n\n    def set(self, key, val):\n        self._dict[key] = val\n\n\nclass HdfsNamenodeResolverTest(unittest.TestCase):\n    def setUp(self):\n        """"""Initializes a mock hadoop config and a namenode resolver instance, for convenience.""""""\n        self._hadoop_configuration = MockHadoopConfiguration()\n        self.suj = HdfsNamenodeResolver(self._hadoop_configuration)\n\n    def test_default_hdfs_service_errors(self):\n        """"""Check error cases with connecting to default namenode""""""\n        # No default yields RuntimeError\n        with self.assertRaises(RuntimeError):\n            self.suj.resolve_default_hdfs_service()\n        # Bad default FS yields IOError\n        self._hadoop_configuration.set(\'fs.defaultFS\', \'invalidFS\')\n        with self.assertRaises(IOError):\n            self.suj.resolve_default_hdfs_service()\n        # Random FS host yields IOError\n        self._hadoop_configuration.set(\'fs.defaultFS\', \'hdfs://random\')\n        with self.assertRaises(IOError):\n            self.suj.resolve_default_hdfs_service()\n        # Valid FS host with no namenode defined yields IOError\n        self._hadoop_configuration.set(\'fs.defaultFS\', HC.FS_WARP_TURTLE)\n        with self.assertRaises(IOError):\n            self.suj.resolve_default_hdfs_service()\n\n    def test_default_hdfs_service_typical(self):\n        """"""Check typical cases resolving default namenode""""""\n        # One nn\n        self._hadoop_configuration.set(\'fs.defaultFS\', HC.FS_WARP_TURTLE)\n        self._hadoop_configuration.set(\'dfs.ha.namenodes.{}\'.format(HC.WARP_TURTLE), \'nn1\')\n        self._hadoop_configuration.set(\n            \'dfs.namenode.rpc-address.{}.nn1\'.format(HC.WARP_TURTLE), HC.WARP_TURTLE_NN1)\n        nameservice, namenodes = self.suj.resolve_default_hdfs_service()\n        self.assertEqual(HC.WARP_TURTLE, nameservice)\n        self.assertEqual(HC.WARP_TURTLE_NN1, namenodes[0])\n\n        # Second of two nns, when the first is undefined\n        self._hadoop_configuration.set(\'dfs.ha.namenodes.{}\'.format(HC.WARP_TURTLE), \'nn2,nn1\')\n        with self.assertRaises(RuntimeError):\n            self.suj.resolve_default_hdfs_service()\n\n        # Two valid and defined nns\n        self._hadoop_configuration.set(\n            \'dfs.namenode.rpc-address.{}.nn2\'.format(HC.WARP_TURTLE), HC.WARP_TURTLE_NN2)\n        nameservice, namenodes = self.suj.resolve_default_hdfs_service()\n        self.assertEqual(HC.WARP_TURTLE, nameservice)\n        self.assertEqual(HC.WARP_TURTLE_NN2, namenodes[0])\n        self.assertEqual(HC.WARP_TURTLE_NN1, namenodes[1])\n\n    def test_resolve_hdfs_name_service(self):\n        """"""Check edge cases with resolving a nameservice""""""\n        # Most cases already covered by test_default_hdfs_service_ok above...\n        # Empty config or no namespace yields None\n        self.assertIsNone(HdfsNamenodeResolver({}).resolve_hdfs_name_service(\'\'))\n        self.assertIsNone(self.suj.resolve_hdfs_name_service(\'\'))\n\n        # Test a single undefined namenode case, as well as an unconventional multi-NN case;\n        # both result in an exception raised\n        self._hadoop_configuration.set(\'fs.defaultFS\', HC.FS_WARP_TURTLE)\n        self._hadoop_configuration.set(\'dfs.ha.namenodes.{}\'.format(HC.WARP_TURTLE), \'nn1\')\n        with self.assertRaises(RuntimeError):\n            self.suj.resolve_hdfs_name_service(HC.WARP_TURTLE)\n\n        # Test multiple undefined NNs, which will also throw HdfsConnectError\n        nns = \'nn1,nn2,nn3,nn4,nn5,nn6,nn7,nn8\'\n        self._hadoop_configuration.set(\'dfs.ha.namenodes.{}\'.format(HC.WARP_TURTLE), nns)\n        with self.assertRaises(RuntimeError):\n            self.suj.resolve_hdfs_name_service(HC.WARP_TURTLE)\n\n\n@pytest.fixture()\ndef mock_hadoop_home_directory(tmpdir):\n    """"""Create hadoop site files once""""""\n    tmpdir_path = tmpdir.strpath\n    os.makedirs(\'{}{}\'.format(tmpdir_path, HC.HADOOP_CONFIG_PATH))\n    with open(\'{}{}/core-site.xml\'.format(tmpdir_path, HC.HADOOP_CONFIG_PATH), \'wt\') as f:\n        f.write(textwrap.dedent(""""""\\\n            <?xml version=""1.0""?>\n            <?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?>\n            <configuration>\n              <property>\n                <name>fs.defaultFS</name>\n                <value>hdfs://{0}</value>\n              </property>\n            </configuration>\n            """""".format(HC.WARP_TURTLE)))\n    with open(\'{}{}/hdfs-site.xml\'.format(tmpdir_path, HC.HADOOP_CONFIG_PATH), \'wt\') as f:\n        f.write(textwrap.dedent(""""""\\\n            <?xml version=""1.0""?>\n            <?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?>\n            <configuration>\n              <property>\n                <name>dfs.ha.namenodes.{0}</name>\n                <value>nn2,nn1</value>\n              </property>\n              <property>\n                <name>dfs.namenode.rpc-address.{0}.nn1</name>\n                <value>{1}</value>\n              </property>\n              <property>\n                <name>dfs.namenode.rpc-address.{0}.nn2</name>\n                <value>{2}</value>\n              </property>\n              <property>\n                <name>dfs.ha.namenodes.foobar</name>\n                <value>nn</value>\n              </property>\n            </configuration>\n            """""".format(HC.WARP_TURTLE, HC.WARP_TURTLE_NN1, HC.WARP_TURTLE_NN2)))\n    return tmpdir_path\n\n\ndef _test_default_hdfs_service(mock_hadoop_home_directory, env_var):\n    # Trigger env var evaluation\n    suj = HdfsNamenodeResolver()\n    assert env_var == suj._hadoop_env\n    assert mock_hadoop_home_directory == suj._hadoop_path\n    # List of namenodes returned nominally\n    nameservice, namenodes = suj.resolve_default_hdfs_service()\n    assert HC.WARP_TURTLE == nameservice\n    assert HC.WARP_TURTLE_NN2 == namenodes[0]\n    assert HC.WARP_TURTLE_NN1 == namenodes[1]\n    # Exception raised for badly defined nameservice (XML issue)\n    with pytest.raises(RuntimeError):\n        suj.resolve_hdfs_name_service(\'foobar\')\n    # None for nonexistent nameservice (intentional design)\n    assert suj.resolve_hdfs_name_service(\'nonexistent\') is None\n\n\ndef test_env_hadoop_home_prefix_install(mock_hadoop_home_directory):\n    # The second+third env vars won\'t cause an error\n    with mock.patch.dict(os.environ, {\'HADOOP_PREFIX\': \'{}/no/where/here\'.format(mock_hadoop_home_directory),\n                                      \'HADOOP_INSTALL\': \'{}/no/where/here\'.format(mock_hadoop_home_directory),\n                                      \'HADOOP_HOME\': mock_hadoop_home_directory}, clear=True):\n        _test_default_hdfs_service(mock_hadoop_home_directory, \'HADOOP_HOME\')\n\n\ndef test_env_hadoop_prefix_only(mock_hadoop_home_directory):\n    with mock.patch.dict(os.environ, {\'HADOOP_PREFIX\': mock_hadoop_home_directory}, clear=True):\n        _test_default_hdfs_service(mock_hadoop_home_directory, \'HADOOP_PREFIX\')\n\n\ndef test_env_hadoop_install_only(mock_hadoop_home_directory):\n    with mock.patch.dict(os.environ, {\'HADOOP_INSTALL\': mock_hadoop_home_directory}, clear=True):\n        _test_default_hdfs_service(mock_hadoop_home_directory, \'HADOOP_INSTALL\')\n\n\ndef test_env_bad_hadoop_home_with_hadoop_install(mock_hadoop_home_directory):\n    with mock.patch.dict(os.environ, {\'HADOOP_HOME\': \'{}/no/where/here\'.format(mock_hadoop_home_directory),\n                                      \'HADOOP_INSTALL\': mock_hadoop_home_directory}, clear=True):\n        with pytest.raises(IOError):\n            # Trigger env var evaluation\n            HdfsNamenodeResolver()\n\n\ndef test_unmatched_env_var(mock_hadoop_home_directory):\n    with mock.patch.dict(os.environ, {\'HADOOP_HOME_X\': mock_hadoop_home_directory}, clear=True):\n        suj = HdfsNamenodeResolver()\n        # No successful connection\n        with pytest.raises(RuntimeError):\n            suj.resolve_default_hdfs_service()\n\n\ndef test_bad_hadoop_path(mock_hadoop_home_directory):\n    with mock.patch.dict(os.environ, {\'HADOOP_HOME\': \'{}/no/where/here\'.format(mock_hadoop_home_directory)},\n                         clear=True):\n        with pytest.raises(IOError):\n            HdfsNamenodeResolver()\n\n\ndef test_missing_or_empty_core_site(mock_hadoop_home_directory):\n    with mock.patch.dict(os.environ, {\'HADOOP_HOME\': mock_hadoop_home_directory}):\n        # Make core-site ""disappear"" and make sure we raise an error\n        cur_path = \'{}{}/core-site.xml\'.format(mock_hadoop_home_directory, HC.HADOOP_CONFIG_PATH)\n        new_path = \'{}{}/core-site.xml.bak\'.format(mock_hadoop_home_directory, HC.HADOOP_CONFIG_PATH)\n        os.rename(cur_path, new_path)\n        with pytest.raises(IOError):\n            HdfsNamenodeResolver()\n        # Make an empty file\n        with open(cur_path, \'wt\') as f:\n            f.write(\'\')\n        # Re-trigger env var evaluation\n        suj = HdfsNamenodeResolver()\n        with pytest.raises(RuntimeError):\n            suj.resolve_default_hdfs_service()\n        # restore file for other tests to work\n        os.rename(new_path, cur_path)\n\n\nclass HdfsMockError(Exception):\n    pass\n\n\nclass MockHdfs(object):\n    """"""\n    Any operation in the mock class raises an exception for the first N failovers, and then returns\n    True after those N calls.\n    """"""\n\n    def __init__(self, n_failovers=0, user=None):\n        self._n_failovers = n_failovers\n        self._user = user\n\n    def __getattribute__(self, attr):\n        """"""\n        The Mock HDFS simply calls check_failover, regardless of the filesystem operator invoked.\n        """"""\n\n        def op(*args, **kwargs):\n            """""" Mock operator """"""\n            return self._check_failovers()\n\n        # Of course, exclude any protected/private method calls\n        if not attr.startswith(\'_\'):\n            return op\n        return object.__getattribute__(self, attr)\n\n    def _check_failovers(self):\n        if self._n_failovers == -1:\n            # Special case to exercise the unhandled exception path\n            raise HdfsMockError(\'Some random HDFS exception!\')\n\n        if self._n_failovers > 0:\n            self._n_failovers -= 1\n            raise ArrowIOError(\'org.apache.hadoop.ipc.RemoteException\'\n                               \'(org.apache.hadoop.ipc.StandbyException): \'\n                               \'Operation category READ is not supported in state standby. \'\n                               \'Visit https://s.apache.org/sbnn-error\\n\'\n                               \'{} namenode failover(s) remaining!\'.format(self._n_failovers))\n        return True\n\n    def __reduce__(self):\n        raise AssertionError(\'A connection object can not be pickled. If we try to pickle it, it means \'\n                             \'it leaks somehow with a closure that holds it and we need to make sure it \'\n                             \'does not happen.\')\n\n\nclass MockHdfsConnector(HdfsConnector):\n    # static member for static hdfs_connect_namenode to access\n    _n_failovers = 0\n    _fail_n_next_connect = 0\n    _connect_attempted = {}\n\n    @classmethod\n    def reset(cls):\n        cls._n_failovers = 0\n        cls._fail_n_next_connect = 0\n        cls._connect_attempted = {}\n\n    @classmethod\n    def set_n_failovers(cls, failovers):\n        cls._n_failovers = failovers\n\n    @classmethod\n    def set_fail_n_next_connect(cls, fails):\n        cls._fail_n_next_connect = fails\n\n    @classmethod\n    def connect_attempted(cls, host):\n        if host in cls._connect_attempted:\n            return cls._connect_attempted[host]\n        else:\n            return 0\n\n    @classmethod\n    def hdfs_connect_namenode(cls, url, driver=\'libhdfs3\', user=None):\n        netloc = \'{}:{}\'.format(url.hostname or \'default\', url.port or 8020)\n        if netloc not in cls._connect_attempted:\n            cls._connect_attempted[netloc] = 0\n        cls._connect_attempted[netloc] += 1\n        # We just want to check connection attempt, but also raise an error if\n        # \'default\' or fail counter\n        if cls._fail_n_next_connect != 0 or netloc == HC.DEFAULT_NN:\n            if cls._fail_n_next_connect != 0:\n                cls._fail_n_next_connect -= 1\n            raise ArrowIOError(\'ERROR! Mock pyarrow hdfs connect to {} using driver {}, \'\n                               \'fail counter: {}\'\n                               .format(netloc, driver, cls._fail_n_next_connect))\n        # Return a mock HDFS object with optional failovers, so that this connector mock can\n        # be shared for the HAHdfsClient failover tests below.\n        hdfs = MockHdfs(cls._n_failovers, user=user)\n        if cls._n_failovers > 0:\n            cls._n_failovers -= 1\n        return hdfs\n\n\nclass HdfsConnectorTest(unittest.TestCase):\n    """"""Check correctness of connecting to a list of namenodes. """"""\n\n    @classmethod\n    def setUpClass(cls):\n        """"""Initializes a mock HDFS namenode connector to track connection attempts.""""""\n        cls.NAMENODES = [HC.WARP_TURTLE_NN1, HC.WARP_TURTLE_NN2]\n        cls.suj = MockHdfsConnector()\n\n    def setUp(self):\n        self.suj.reset()\n\n    def test_connect_to_either_namenode_ok(self):\n        """""" Test connecting OK to first of name node URLs. """"""\n        self.assertIsNotNone(self.suj.connect_to_either_namenode(self.NAMENODES))\n        self.assertEqual(0, self.suj.connect_attempted(HC.DEFAULT_NN))\n        self.assertEqual(1, self.suj.connect_attempted(HC.WARP_TURTLE_NN1))\n        self.assertEqual(0, self.suj.connect_attempted(HC.WARP_TURTLE_NN2))\n\n    def test_connect_to_either_with_user(self):\n        mock_name = ""mock-manager""\n        mocked_hdfs = self.suj.connect_to_either_namenode(self.NAMENODES, user=mock_name)\n        self.assertEqual(mocked_hdfs._user, mock_name)\n\n    def test_connect_to_either_namenode_ok_one_failed(self):\n        """""" With one failver, test that both namenode URLS are attempted, with 2nd connected. """"""\n        self.suj.set_fail_n_next_connect(1)\n        self.assertIsNotNone(self.suj.connect_to_either_namenode(self.NAMENODES))\n        self.assertEqual(0, self.suj.connect_attempted(HC.DEFAULT_NN))\n        self.assertEqual(1, self.suj.connect_attempted(HC.WARP_TURTLE_NN1))\n        self.assertEqual(1, self.suj.connect_attempted(HC.WARP_TURTLE_NN2))\n\n    def test_connect_to_either_namenode_exception_two_failed(self):\n        """""" With 2 failvers, test no connection, and no exception is raised. """"""\n        self.suj.set_fail_n_next_connect(2)\n        with self.assertRaises(HdfsConnectError):\n            self.suj.connect_to_either_namenode(self.NAMENODES)\n        self.assertEqual(0, self.suj.connect_attempted(HC.DEFAULT_NN))\n        self.assertEqual(1, self.suj.connect_attempted(HC.WARP_TURTLE_NN1))\n        self.assertEqual(1, self.suj.connect_attempted(HC.WARP_TURTLE_NN2))\n\n    def test_connect_to_either_namenode_exception_four_failed(self):\n        """""" With 4 failvers, test that exception is raised. """"""\n        self.suj.set_fail_n_next_connect(4)\n        with self.assertRaises(HdfsConnectError):\n            self.suj.connect_to_either_namenode(self.NAMENODES)\n        with self.assertRaises(HdfsConnectError):\n            self.suj.connect_to_either_namenode(self.NAMENODES)\n        self.assertEqual(0, self.suj.connect_attempted(HC.DEFAULT_NN))\n        self.assertEqual(2, self.suj.connect_attempted(HC.WARP_TURTLE_NN1))\n        self.assertEqual(2, self.suj.connect_attempted(HC.WARP_TURTLE_NN2))\n\n\nclass HAHdfsClientTest(unittest.TestCase):\n    """"""\n    The HDFS testing functions are enumerated explicitly below for simplicity and clarity, but it\n    should impose but a minute maintenance overhead, since MockHdfs class requires no enumeration.\n    """"""\n\n    @classmethod\n    def setUpClass(cls):\n        """"""Initializes namenodes list and mock HDFS namenode connector.""""""\n        cls.NAMENODES = [HC.WARP_TURTLE_NN1, HC.WARP_TURTLE_NN2]\n\n    def setUp(self):\n        """"""Reset mock HDFS failover count.""""""\n        MockHdfsConnector.reset()\n\n    def test_unhandled_exception(self):\n        """"""Exercise the unhandled exception execution path.""""""\n        MockHdfsConnector.set_n_failovers(-1)\n        with self.assertRaises(HdfsMockError) as e:\n            getattr(HAHdfsClient(MockHdfsConnector, [HC.WARP_TURTLE_NN1]), \'ls\')(\'random\')\n        self.assertTrue(\'random HDFS exception\' in str(e.exception))\n\n    def test_invalid_namenode_list(self):\n        """"""Make sure robust to invalid namenode list.""""""\n        MockHdfsConnector.set_n_failovers(-1)\n        with self.assertRaises(HdfsConnectError) as e:\n            getattr(HAHdfsClient(MockHdfsConnector, []), \'ls\')(\'random\')\n        self.assertTrue(\'Unable to connect\' in str(e.exception))\n        with self.assertRaises(HdfsConnectError) as e:\n            getattr(HAHdfsClient(MockHdfsConnector, [None]), \'ls\')(\'random\')\n        self.assertTrue(\'Unable to connect\' in str(e.exception))\n\n    def test_client_pickles_correctly(self):\n        """"""\n        Does HAHdfsClient pickle properly?\n\n        Check that all attributes are equal, with the exception of the HDFS object, which is fine\n        as long as the types are the same.\n        """"""\n        mock_name = ""mock-manager""\n        client = HAHdfsClient(MockHdfsConnector, self.NAMENODES, user=mock_name)\n        client_unpickled = pickle.loads(pickle.dumps(client))\n        self.assertEqual(client._connector_cls, client_unpickled._connector_cls)\n        self.assertEqual(client._list_of_namenodes, client_unpickled._list_of_namenodes)\n        self.assertEqual(client._index_of_nn, client_unpickled._index_of_nn)\n        self.assertEqual(client._user, client_unpickled._user)\n        self.assertEqual(type(client._hdfs), type(client_unpickled._hdfs))\n\n    def _try_failover_combos(self, func, *args, **kwargs):\n        """"""Common tests for each of the known HDFS operators, with varying failover counts.""""""\n        MockHdfsConnector.set_n_failovers(1)\n        suj = HAHdfsClient(MockHdfsConnector, self.NAMENODES)\n        self.assertTrue(getattr(suj, func)(*args, **kwargs))\n\n        MockHdfsConnector.set_n_failovers(namenode_failover.MAX_FAILOVER_ATTEMPTS)\n        suj = HAHdfsClient(MockHdfsConnector, self.NAMENODES)\n        self.assertTrue(getattr(suj, func)(*args, **kwargs))\n\n        MockHdfsConnector.set_n_failovers(namenode_failover.MAX_FAILOVER_ATTEMPTS + 1)\n        suj = HAHdfsClient(MockHdfsConnector, self.NAMENODES)\n        with self.assertRaises(MaxFailoversExceeded) as e:\n            getattr(suj, func)(*args, **kwargs)\n        self.assertEqual(len(e.exception.failed_exceptions),\n                         namenode_failover.MAX_FAILOVER_ATTEMPTS + 1)\n        self.assertEqual(e.exception.max_failover_attempts, namenode_failover.MAX_FAILOVER_ATTEMPTS)\n        self.assertEqual(e.exception.__name__, func)\n        self.assertTrue(\'Failover attempts exceeded\' in str(e.exception))\n\n    def test_cat(self):\n        self._try_failover_combos(\'cat\', \'random\')\n\n    def test_chmod(self):\n        self._try_failover_combos(\'chmod\', \'random\', 0)\n\n    def test_chown(self):\n        self._try_failover_combos(\'chown\', \'random\', \'user\')\n\n    def test_delete(self):\n        self._try_failover_combos(\'delete\', \'random\', recursive=True)\n\n    def test_df(self):\n        self._try_failover_combos(\'df\')\n\n    def test_disk_usage(self):\n        self._try_failover_combos(\'disk_usage\', \'random\')\n\n    def test_download(self):\n        self._try_failover_combos(\'download\', \'random\', None)\n\n    def test_exists(self):\n        self._try_failover_combos(\'exists\', \'random\')\n\n    def test_get_capacity(self):\n        self._try_failover_combos(\'get_capacity\')\n\n    def test_get_space_used(self):\n        self._try_failover_combos(\'get_space_used\')\n\n    def test_info(self):\n        self._try_failover_combos(\'info\', \'random\')\n\n    def test_ls(self):\n        self._try_failover_combos(\'ls\', \'random\', detail=True)\n\n    def test_mkdir(self):\n        self._try_failover_combos(\'mkdir\', \'random\', create_parents=False)\n\n    def test_open(self):\n        self._try_failover_combos(\'open\', \'random\', \'rb\')\n\n    def test_rename(self):\n        self._try_failover_combos(\'rename\', \'random\', \'new_random\')\n\n    def test_rm(self):\n        self._try_failover_combos(\'rm\', \'random\', recursive=True)\n\n    def test_upload(self):\n        self._try_failover_combos(\'upload\', \'random\', None)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
petastorm/pyarrow_helpers/tests/test_batch_buffer.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport pyarrow as pa\n\nfrom petastorm.compat import compat_column_data\nfrom petastorm.pyarrow_helpers.batching_table_queue import BatchingTableQueue\n\n\ndef _new_record_batch(values):\n    sequence = pa.array(values)\n    return pa.RecordBatch.from_arrays([sequence, sequence], [\'a\', \'b\'])\n\n\ndef test_single_table_of_10_rows_added_and_2_batches_of_4_read():\n    """"""Add a single table composed of a single batch with 0..9 rows into batcher. Then read two batches of 4\n    and verify that no more batches can be read""""""\n\n    # Table with two columns. Each column with 0..9 sequence\n    one_batch_of_10_records = [_new_record_batch(range(0, 10))]\n    table_0_10 = pa.Table.from_batches(one_batch_of_10_records)\n\n    batcher = BatchingTableQueue(4)\n    assert batcher.empty()\n\n    # Load 10 rows into batcher\n    batcher.put(table_0_10)\n\n    # Get first batch of 4\n    assert not batcher.empty()\n    next_batch = batcher.get()\n\n    assert 4 == next_batch.num_rows\n    np.testing.assert_equal(compat_column_data(next_batch.column(0)).to_pylist(), list(range(0, 4)))\n    np.testing.assert_equal(compat_column_data(next_batch.column(1)).to_pylist(), list(range(0, 4)))\n\n    # Get second batch of 4\n    assert not batcher.empty()\n    next_batch = batcher.get()\n\n    assert 4 == next_batch.num_rows\n    np.testing.assert_equal(compat_column_data(next_batch.column(0)).to_pylist(), list(range(4, 8)))\n    np.testing.assert_equal(compat_column_data(next_batch.column(1)).to_pylist(), list(range(4, 8)))\n\n    # No more batches available\n    assert batcher.empty()\n\n\ndef test_two_tables_of_10_added_reading_5_batches_of_4():\n    """"""Add two tables to batcher and read a batch that covers parts of both tables""""""\n    table_0_9 = pa.Table.from_batches([_new_record_batch(range(0, 10))])\n    table_10_19 = pa.Table.from_batches([_new_record_batch(range(10, 20))])\n\n    batcher = BatchingTableQueue(4)\n    assert batcher.empty()\n\n    batcher.put(table_0_9)\n    batcher.put(table_10_19)\n\n    for i in range(5):\n        assert not batcher.empty()\n        next_batch = batcher.get()\n\n        assert (i != 4) == (not batcher.empty())\n\n        assert 4 == next_batch.num_rows\n        expected_values = list(range(i * 4, i * 4 + 4))\n        np.testing.assert_equal(compat_column_data(next_batch.column(0)).to_pylist(), expected_values)\n        np.testing.assert_equal(compat_column_data(next_batch.column(1)).to_pylist(), expected_values)\n\n\ndef test_read_batches_larger_than_a_table_added():\n    """"""Add a single table composed of 10 one row ten_batches_each_with_one_record. Then read-out two batches of 4\n    and verify that no more batches can be read""""""\n    ten_batches_each_with_one_record = [_new_record_batch([i]) for i in range(10)]\n    table_0_10 = pa.Table.from_batches(ten_batches_each_with_one_record)\n\n    batcher = BatchingTableQueue(4)\n    batcher.put(table_0_10)\n\n    assert not batcher.empty()\n    next_batch = batcher.get()\n\n    assert 4 == next_batch.num_rows\n    np.testing.assert_equal(compat_column_data(next_batch.column(0)).to_pylist(), list(range(0, 4)))\n    np.testing.assert_equal(compat_column_data(next_batch.column(1)).to_pylist(), list(range(0, 4)))\n\n    assert not batcher.empty()\n    next_batch = batcher.get()\n\n    assert 4 == next_batch.num_rows\n    np.testing.assert_equal(compat_column_data(next_batch.column(0)).to_pylist(), list(range(4, 8)))\n    np.testing.assert_equal(compat_column_data(next_batch.column(1)).to_pylist(), list(range(4, 8)))\n\n    assert batcher.empty()\n\n\ndef test_batch_size_of_one():\n    """"""Try if BatchingTableQueue can be used to retrieve row-by-row data (batch size of 1)""""""\n    batches = [_new_record_batch([i]) for i in range(3)]\n    table = pa.Table.from_batches(batches)\n\n    batcher = BatchingTableQueue(1)\n    batcher.put(table)\n\n    for _ in range(3):\n        assert not batcher.empty()\n        next_batch = batcher.get()\n        assert 1 == next_batch.num_rows\n\n    assert batcher.empty()\n\n\ndef test_random_table_size_and_random_batch_sizes():\n    """"""Add a random number of rows, then read a random number of batches. Repeat multiple times.""""""\n    batch_size = 5\n    input_table_size = 50\n    read_iter_count = 1000\n\n    batcher = BatchingTableQueue(batch_size)\n    write_seq = 0\n    read_seq = 0\n\n    for _ in range(read_iter_count):\n        next_batch_size = np.random.randint(0, input_table_size)\n        new_batch = _new_record_batch(list(range(write_seq, write_seq + next_batch_size)))\n        write_seq += next_batch_size\n\n        batcher.put(pa.Table.from_batches([new_batch]))\n\n        next_read = np.random.randint(1, input_table_size // batch_size)\n        for _ in range(next_read):\n            if not batcher.empty():\n                read_batch = batcher.get()\n                for value in compat_column_data(read_batch.columns[0]):\n                    assert value == read_seq\n                    read_seq += 1\n\n    assert read_seq > 0\n'"
petastorm/workers_pool/tests/__init__.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
petastorm/workers_pool/tests/stub_workers.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom time import sleep\n\nfrom petastorm.workers_pool.worker_base import WorkerBase\n\n\nclass CoeffMultiplierWorker(WorkerBase):\n    def process(self, *args, **kargs):\n        # If value is a list, generate multiple outputs\n        value = kargs[\'value\']\n        if isinstance(value, list):\n            for v in value:\n                self.publish_func(v * self.args[\'coeff\'])\n        else:\n            self.publish_func(value * self.args[\'coeff\'])\n\n\nclass IdentityWorker(WorkerBase):\n    def process(self, *args, **kargs):\n        self.publish_func(kargs[\'item\'])\n\n\nclass WorkerIdGeneratingWorker(WorkerBase):\n    def process(self, *args, **kargs):\n        self.publish_func(self.worker_id)\n\n\nclass WorkerMultiIdGeneratingWorker(WorkerBase):\n    def process(self, *args, **kargs):\n        for _ in range(2):\n            self.publish_func(self.worker_id)\n\n\nclass SleepyDoingNothingWorker(WorkerIdGeneratingWorker):\n    def __init__(self, worker_id, publish_func, args):\n        """"""\n        :param args: The worker will sleep for this time (seconds) until returned\n        """"""\n        super(SleepyDoingNothingWorker, self).__init__(worker_id, publish_func, args)\n        self._sleep = args\n\n    def process(self, *args, **kargs):\n        sleep(self._sleep)\n\n\nclass SleepyWorkerIdGeneratingWorker(WorkerIdGeneratingWorker):\n    def process(self, *args, **kargs):\n        sleep(1)\n        super(SleepyWorkerIdGeneratingWorker, self).process()\n\n\nclass ExceptionGeneratingWorker_5(WorkerBase):\n    def process(self, *args, **kargs):\n        raise ValueError(""worker %d raise test exception - IT SHOULD BE EXCEPTION!"" % self.worker_id)\n\n\nclass PreprogrammedReturnValueWorker(WorkerBase):\n    def __init__(self, worker_id, publish_func, args):\n        """"""\n        :param args: Array of arrays. Defines which values to return at consequent process calls. For example,\n        args = [[], [1], [12, 13]] will result in process not generating any results in the first call, generating \'1\'\n        in the second and \'12\', \'13\' at the third invocation of \'process\'\n        """"""\n        super(PreprogrammedReturnValueWorker, self).__init__(worker_id, publish_func, args)\n        self._program = args\n        self._current_step = 0\n\n    def process(self, *args, **kargs):\n        for value in self._program[self._current_step]:\n            self.publish_func(value)\n        self._current_step += 1\n'"
petastorm/workers_pool/tests/test_ventilator.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport time\nimport unittest\n\nfrom petastorm.workers_pool import EmptyResultError\nfrom petastorm.workers_pool.dummy_pool import DummyPool\nfrom petastorm.workers_pool.process_pool import ProcessPool\nfrom petastorm.workers_pool.tests.stub_workers import IdentityWorker\nfrom petastorm.workers_pool.thread_pool import ThreadPool\nfrom petastorm.workers_pool.ventilator import ConcurrentVentilator\n\n\nclass TestWorkersPool(unittest.TestCase):\n\n    def _test_simple_ventilation(self, pool_class_factory):\n        pool = pool_class_factory()\n\n        items_to_ventilate = [{\'item\': i} for i in range(50)]\n        ventilator = ConcurrentVentilator(ventilate_fn=pool.ventilate, items_to_ventilate=items_to_ventilate)\n        pool.start(IdentityWorker, ventilator=ventilator)\n\n        all_results = [pool.get_results() for _ in items_to_ventilate]\n        self.assertEqual([i[\'item\'] for i in items_to_ventilate], sorted(all_results))\n\n        pool.stop()\n        pool.join()\n\n    def test_ventilator_processes(self):\n        self._test_simple_ventilation(lambda: ProcessPool(10))\n\n    def test_ventilator_threads(self):\n        self._test_simple_ventilation(lambda: ThreadPool(10))\n\n    def test_ventilator_dummy(self):\n        self._test_simple_ventilation(DummyPool)\n\n    def test_max_ventilation_size(self):\n        """"""Tests that we dont surpass a max ventilation size in each pool type\n        (since it relies on accurate ventilation size reporting)""""""\n        max_ventilation_size = 10\n\n        for pool in [DummyPool(), ProcessPool(10), ThreadPool(10)]:\n            ventilator = ConcurrentVentilator(ventilate_fn=pool.ventilate,\n                                              items_to_ventilate=[{\'item\': i} for i in range(100)],\n                                              max_ventilation_queue_size=max_ventilation_size)\n            pool.start(IdentityWorker, ventilator=ventilator)\n\n            # Give time for the thread to fill the ventilation queue\n            while ventilator._ventilated_items_count - ventilator._processed_items_count < max_ventilation_size:\n                time.sleep(.1)\n\n            # After stopping the ventilator queue, we should only get 10 results\n            ventilator.stop()\n            for _ in range(max_ventilation_size):\n                pool.get_results()\n\n            with self.assertRaises(EmptyResultError):\n                pool.get_results()\n\n            pool.stop()\n            pool.join()\n\n    def test_reset_in_the_middle_of_ventilation(self):\n        """"""Can not reset ventilator in the middle of ventilation""""""\n        for pool in [DummyPool(), ThreadPool(10)]:\n            ventilator = ConcurrentVentilator(ventilate_fn=pool.ventilate,\n                                              items_to_ventilate=[{\'item\': i} for i in range(100)],\n                                              iterations=None)\n            pool.start(IdentityWorker, ventilator=ventilator)\n\n            # Resetting is supported only when the ventilator has finished\n            with self.assertRaises(NotImplementedError):\n                ventilator.reset()\n\n            pool.stop()\n            pool.join()\n\n    def test_reset_ventilator(self):\n        """"""Resetting ventilator after all items were ventilated will make it re-ventilate the same items""""""\n        items_count = 100\n        for pool in [DummyPool(), ThreadPool(10)]:\n            ventilator = ConcurrentVentilator(ventilate_fn=pool.ventilate,\n                                              items_to_ventilate=[{\'item\': i} for i in range(items_count)],\n                                              iterations=1)\n            pool.start(IdentityWorker, ventilator=ventilator)\n\n            # Readout all ventilated items\n            for _ in range(items_count):\n                pool.get_results()\n\n            # Should fail reading the next, as all items were read by now\n            with self.assertRaises(EmptyResultError):\n                pool.get_results()\n\n            # Resetting, hence will be read out the items all over again\n            ventilator.reset()\n\n            for _ in range(items_count):\n                pool.get_results()\n\n            with self.assertRaises(EmptyResultError):\n                pool.get_results()\n\n            pool.stop()\n            pool.join()\n\n    def test_empty_ventilation(self):\n        pool = DummyPool()\n        ventilator = ConcurrentVentilator(pool.ventilate, [])\n        pool.start(IdentityWorker, ventilator=ventilator)\n        with self.assertRaises(EmptyResultError):\n            pool.get_results()\n\n        pool.stop()\n        pool.join()\n\n    def test_multiple_iterations(self):\n        size = 10\n        iterations = 5\n\n        pool = DummyPool()\n        ventilator = ConcurrentVentilator(pool.ventilate, [{\'item\': i} for i in range(size)], iterations=iterations)\n        pool.start(IdentityWorker, ventilator=ventilator)\n\n        results = [pool.get_results() for _ in range(size * iterations)]\n        self.assertEqual(sorted(results), sorted(list(range(size)) * iterations))\n        with self.assertRaises(EmptyResultError):\n            pool.get_results()\n\n        ventilator.stop()\n        pool.stop()\n        pool.join()\n\n    def test_ventilator_stop(self):\n        size = 100\n        max_ventilation_queue_size = 10\n\n        pool = DummyPool()\n\n        ventilator = ConcurrentVentilator(ventilate_fn=pool.ventilate,\n                                          items_to_ventilate=[{\'item\': i} for i in range(size)],\n                                          max_ventilation_queue_size=max_ventilation_queue_size)\n        pool.start(IdentityWorker, ventilator=ventilator)\n\n        [pool.get_results() for _ in range(max_ventilation_queue_size)]\n\n        # Stop the ventilator queue after some time, so there should only be 10 items left on it\n        while ventilator._ventilated_items_count - ventilator._processed_items_count < max_ventilation_queue_size:\n            time.sleep(.1)\n\n        ventilator.stop()\n\n        [pool.get_results() for _ in range(max_ventilation_queue_size)]\n        with self.assertRaises(EmptyResultError):\n            pool.get_results()\n\n        pool.stop()\n        pool.join()\n\n    def test_randomize_item_order(self):\n        size = 100\n        pool = DummyPool()\n        items_to_ventilate_1 = [{\'item\': i} for i in range(size)]\n        ventilator = ConcurrentVentilator(pool.ventilate, items_to_ventilate=items_to_ventilate_1)\n        pool.start(IdentityWorker, ventilator=ventilator)\n        first_results = [pool.get_results() for _ in range(size)]\n        pool.stop()\n        pool.join()\n\n        pool = DummyPool()\n        items_to_ventilate_2 = [{\'item\': i} for i in range(size)]\n        ventilator = ConcurrentVentilator(pool.ventilate,\n                                          items_to_ventilate=items_to_ventilate_2,\n                                          randomize_item_order=True)\n        pool.start(IdentityWorker, ventilator=ventilator)\n        second_results = [pool.get_results() for _ in range(size)]\n        pool.stop()\n        pool.join()\n\n        # Because we\'re using the dummy pool, without randomizing item order the results\n        # should be exactly the list of ventilation items\n        self.assertEqual(first_results, [item[\'item\'] for item in items_to_ventilate_1])\n        self.assertNotEqual(first_results, second_results)\n\n        pool.stop()\n        pool.join()\n\n\nif __name__ == \'__main__\':\n    # Delegate to the test framework.\n    unittest.main()\n'"
petastorm/workers_pool/tests/test_workers_pool.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport time\nimport unittest\n\nimport numpy as np\nfrom multiprocessing import Process, Manager\nfrom psutil import process_iter\n\n\nfrom petastorm.workers_pool import EmptyResultError\nfrom petastorm.workers_pool.dummy_pool import DummyPool\nfrom petastorm.workers_pool.process_pool import ProcessPool\nfrom petastorm.workers_pool.tests.stub_workers import CoeffMultiplierWorker, \\\n    WorkerIdGeneratingWorker, WorkerMultiIdGeneratingWorker, SleepyWorkerIdGeneratingWorker, \\\n    ExceptionGeneratingWorker_5, PreprogrammedReturnValueWorker\nfrom petastorm.workers_pool.thread_pool import ThreadPool\n\n\nclass TestWorkersPool(unittest.TestCase):\n\n    def _passing_args_impl(self, pool_class_factory):\n        """"""Pass a coefficient to the workers and make it multiply the input with this coefficient""""""\n        DELTA = 12\n        ITERATIONS = 100\n        pool = pool_class_factory()\n\n        pool.start(CoeffMultiplierWorker, {\'coeff\': DELTA})\n        for i in range(ITERATIONS):\n            pool.ventilate(message=\'Vent data {}\'.format(i), value=i)\n\n        all_results = [pool.get_results() for _ in range(ITERATIONS)]\n        self.assertEqual({DELTA}, set(np.diff(sorted(all_results))))\n\n        pool.stop()\n        pool.join()\n\n    def test_passing_args_processes(self):\n        for zmq_copy_buffers in [False, True]:\n            self._passing_args_impl(lambda do_copy=zmq_copy_buffers: ProcessPool(10, zmq_copy_buffers=do_copy))\n\n    def test_passing_args_threads(self):\n        self._passing_args_impl(lambda: ThreadPool(10))\n\n    def test_passing_args_dummy(self):\n        self._passing_args_impl(DummyPool)\n\n    def test_all_workers_are_active_processes(self):\n        """"""Check that the work is distributed among all workers""""""\n        WORKERS_COUNT = 10\n\n        # Testing only ProcessPool since only it has the mechanism that waits for all workers to come online before\n        # start finishes\n        pool = ProcessPool(WORKERS_COUNT)\n\n        pool.start(WorkerIdGeneratingWorker)\n        for _ in range(100):\n            pool.ventilate()\n\n        active_worker_ids = [pool.get_results() for _ in range(100)]\n        self.assertEqual(set(range(WORKERS_COUNT)), set(active_worker_ids))\n\n        pool.stop()\n        pool.join()\n\n    def block_on_get_results_impl(self, pool_class):\n        """"""Check that the get_results blocking timeout works""""""\n\n        # COULD BECOME A FLAKY TEST SINCE RELIES ON TIME\n        WORKERS_COUNT = 10\n        pool = pool_class(WORKERS_COUNT)\n\n        pool.start(SleepyWorkerIdGeneratingWorker)\n        tic = time.time()\n\n        pool.ventilate()\n        pool.get_results()\n\n        toc = time.time()\n        # Leave a huge slack so we don\'t get a flaky test\n        self.assertTrue(np.isclose(1.0, toc - tic, atol=0.5))\n\n        pool.stop()\n        pool.join()\n\n    def raise_empty_result_error_on_get_results_impl(self, pool_class):\n        """"""Check that the get_results returns None when there is no work left to do""""""\n\n        # COULD BECOME A FLAKY TEST SINCE RELIES ON TIME\n        WORKERS_COUNT = 10\n        pool = pool_class(WORKERS_COUNT)\n\n        pool.start(WorkerMultiIdGeneratingWorker)\n\n        with self.assertRaises(EmptyResultError):\n            pool.get_results()\n\n        pool.ventilate()\n        self.assertIsNotNone(pool.get_results())\n        self.assertIsNotNone(pool.get_results())\n\n        with self.assertRaises(EmptyResultError):\n            pool.get_results()\n\n        pool.stop()\n        pool.join()\n\n    def test_block_on_get_results_processes(self):\n        self.block_on_get_results_impl(ProcessPool)\n\n    def test_block_on_get_results_threads(self):\n        self.block_on_get_results_impl(ThreadPool)\n\n    def test_block_on_get_results_dummy(self):\n        self.block_on_get_results_impl(DummyPool)\n\n    def test_return_none_on_get_results_process(self):\n        self.raise_empty_result_error_on_get_results_impl(ProcessPool)\n\n    def test_return_none_on_get_results_threads(self):\n        self.raise_empty_result_error_on_get_results_impl(ThreadPool)\n\n    def test_return_none_on_get_results_dummy(self):\n        self.raise_empty_result_error_on_get_results_impl(DummyPool)\n\n    def test_stop_when_result_queue_is_full(self):\n        """"""Makes sure we don\'t block indefinitely on ventilator queue""""""\n        SLEEP_DELTA = 0.01\n        TIMEOUT = 20\n        QUEUE_SIZE = 2\n\n        pool = ThreadPool(10, results_queue_size=QUEUE_SIZE)\n        pool.start(WorkerIdGeneratingWorker)\n\n        for _ in range(100):\n            pool.ventilate()\n\n        cumulative_wait = 0\n        while pool.results_qsize() != QUEUE_SIZE:\n            time.sleep(SLEEP_DELTA)\n            cumulative_wait += SLEEP_DELTA\n            # Make sure we wait no longer than the timeout. Otherwise, something is very wrong\n            self.assertLess(cumulative_wait, TIMEOUT, msg=\'Timeout while waiting for the results queue to fill\')\n\n        # No need to read from the queue. We are testing ability to exit when workers might be blocked on the\n        # results queue\n\n        pool.stop()\n        pool.join()\n\n    def test_dummy_pool_should_process_tasks_in_fifo_order(self):\n        """"""Check that the dummy pool processes in fifo order""""""\n        pool = DummyPool()\n        pool.start(CoeffMultiplierWorker, {\'coeff\': 1})\n\n        # Important to try a case where a worker generates multiple results. That\'s why we have some irregular\n        # ventilate/get_results pattern in this test\n        actual_output = []\n\n        pool.ventilate(message=\'dummy message\', value=[0, 1])\n        pool.ventilate(message=\'dummy message\', value=2)\n\n        actual_output.append(pool.get_results())\n\n        pool.ventilate(message=\'dummy message\', value=[3, 4])\n\n        actual_output.append(pool.get_results())\n        actual_output.append(pool.get_results())\n        actual_output.append(pool.get_results())\n        actual_output.append(pool.get_results())\n\n        self.assertEqual(actual_output, [0, 1, 2, 3, 4])\n\n    def _test_exception_in_worker_impl(self, pool, num_to_ventilate):\n        """""" Test exception handler in worker. Pool should be terminated """"""\n        # exception should be propagated to calling thread\n        pool.start(ExceptionGeneratingWorker_5)\n        for i in range(num_to_ventilate):\n            pool.ventilate(""Datanum_%d"" % i)\n        with self.assertRaises(ValueError):\n            pool.get_results()\n\n    def test_exception_in_worker_thread(self):\n        """""" Test exception handler in thread pool """"""\n        QUEUE_SIZE = 100\n        pool = ThreadPool(10, results_queue_size=QUEUE_SIZE)\n        self._test_exception_in_worker_impl(pool, QUEUE_SIZE)\n        pool.stop()\n        pool.join()\n\n    def test_exception_in_worker_process(self):\n        """""" Test exception handler in process pool """"""\n\n        # NOTE: The process pool has a problem that if the workers are throwing exceptions, their\n        # zmq sockets will be closed and there is some race condition that can cause the ventilate\n        # to raise an exception. Only ventilating a single time guarantees that it will be properly\n        # sent to a worker before it has exited due to an exception\n        pool = ProcessPool(2)\n        self._test_exception_in_worker_impl(pool, 1)\n        pool.stop()\n        pool.join()\n\n    def test_exception_in_all_worker_process(self):\n        """""" Tests that when all worker processes have exited, zmq will properly throw an exception\n         when trying to ventilate instead of blocking indefinitely""""""\n        pool = ProcessPool(5)\n        pool.start(ExceptionGeneratingWorker_5)\n        with self.assertRaises(RuntimeError):\n            for _ in range(10000):\n                pool.ventilate(""Datanum"")\n                time.sleep(.1)\n\n    def test_workers_die_when_main_process_dies(self):\n        """""" Tests that when the main processes dies, the process workers will kill themselves """"""\n        manager = Manager()\n        return_list = manager.list()\n\n        def run_process_pool(return_list):\n            pool = ProcessPool(1)\n            pool.start(WorkerIdGeneratingWorker)\n            return_list.append(pool._workers[0].pid)\n            # We dont call pool.stop() and hence leave workers alive\n\n        process = Process(target=run_process_pool, args=(return_list,))\n        process.start()\n        process.join()\n        # The worker has now started\n\n        worker_pid = return_list[0]\n\n        for _ in range(20):\n            worker_is_alive = any([p.pid for p in process_iter() if p.pid == worker_pid])\n            if not worker_is_alive:\n                break\n            time.sleep(0.1)\n        self.assertFalse(worker_is_alive)\n\n    def test_exception_reusing_thread_pool(self):\n        WORKERS_COUNT = 10\n        pool = ThreadPool(WORKERS_COUNT)\n        pool.start(WorkerIdGeneratingWorker)\n        with self.assertRaises(EmptyResultError):\n            pool.get_results()\n        pool.ventilate()\n        self.assertIsNotNone(pool.get_results())\n        with self.assertRaises(EmptyResultError):\n            pool.get_results()\n        pool.stop()\n        pool.join()\n        with self.assertRaises(RuntimeError) as e:\n            pool.start(WorkerIdGeneratingWorker)\n        self.assertTrue(\'ThreadPool({}) cannot be reused! stop_event set? {}\'\n                        .format(WORKERS_COUNT, True) in str(e.exception))\n\n    def test_worker_produces_no_results(self):\n        """"""Check edge case, when workers consistently does not produce results""""""\n        # 10000 is an interesting case as in the original implementation it caused stack overflow\n        for ventilate_count in [10, 10000]:\n            for pool in [DummyPool(), ThreadPool(2)]:\n                pool.start(PreprogrammedReturnValueWorker, ventilate_count * [[]])\n                for _ in range(ventilate_count):\n                    pool.ventilate(\'not_important\')\n\n                with self.assertRaises(EmptyResultError):\n                    pool.get_results()\n\n                pool.stop()\n                pool.join()\n\n    def test_worker_produces_some_results(self):\n        """"""Check edge case, when workers consistently does not produce results""""""\n        # 10000 is an interesting case as in the original implementation it caused stack overflow\n        VENTILATE_COUNT = 4\n        for pool in [DummyPool(), ThreadPool(1)]:\n            pool.start(PreprogrammedReturnValueWorker, [[], [], [42], []])\n            for _ in range(VENTILATE_COUNT):\n                pool.ventilate(\'not_important\')\n\n            self.assertEqual(42, pool.get_results())\n            with self.assertRaises(EmptyResultError):\n                pool.get_results()\n\n            pool.stop()\n            pool.join()\n\n\nif __name__ == \'__main__\':\n    # Delegate to the test framework.\n    unittest.main()\n'"
examples/hello_world/external_dataset/tests/test_generate_external_dataset.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport unittest\n\nimport pytest\n\nfrom examples.hello_world.external_dataset.generate_external_dataset import generate_external_dataset\nfrom examples.hello_world.external_dataset.python_hello_world import python_hello_world\nfrom examples.hello_world.external_dataset.pytorch_hello_world import pytorch_hello_world\nfrom examples.hello_world.external_dataset.tensorflow_hello_world import tensorflow_hello_world\nfrom petastorm import make_batch_reader\nfrom petastorm.tests.conftest import SyntheticDataset\nfrom petastorm.tests.test_tf_utils import create_tf_graph\n\n\n@pytest.fixture(scope=""session"")\ndef external_dataset(tmpdir_factory):\n    path = tmpdir_factory.mktemp(""data"").strpath\n    url = \'file://\' + path\n\n    generate_external_dataset(url)\n\n    dataset = SyntheticDataset(url=url, path=path, data=None)\n\n    # Generate a dataset\n    assert os.path.exists(os.path.join(path, \'_SUCCESS\'))\n\n    return dataset\n\n\ndef test_generate(external_dataset):\n    # Read from it using a plain reader\n    with make_batch_reader(external_dataset.url) as reader:\n        all_samples = list(reader)\n    assert all_samples\n\n\n@unittest.skip(\'Some conflict between pytorch and parquet shared libraries results in occasional \'\n               \'segfaults in this case.\')\ndef test_pytorch_hello_world_external_dataset_example(external_dataset):\n    pytorch_hello_world(external_dataset.url)\n\n\ndef test_python_hello_world_external_dataset_example(external_dataset):\n    python_hello_world(external_dataset.url)\n\n\n@create_tf_graph\ndef test_tensorflow_hello_world_external_dataset_example(external_dataset):\n    tensorflow_hello_world(external_dataset.url)\n'"
examples/hello_world/petastorm_dataset/tests/test_generate_petastorm_dataset.py,0,"b'#  Copyright (c) 2017-2018 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport unittest\n\nimport pytest\n\nfrom examples.hello_world.petastorm_dataset.generate_petastorm_dataset import generate_petastorm_dataset\nfrom examples.hello_world.petastorm_dataset.pyspark_hello_world import pyspark_hello_world\nfrom examples.hello_world.petastorm_dataset.pytorch_hello_world import pytorch_hello_world\nfrom examples.hello_world.petastorm_dataset.tensorflow_hello_world import tensorflow_hello_world\nfrom petastorm import make_reader\nfrom petastorm.tests.conftest import SyntheticDataset\nfrom petastorm.tests.test_tf_utils import create_tf_graph\n\n\n@pytest.fixture(scope=""session"")\ndef petastorm_dataset(tmpdir_factory):\n    path = tmpdir_factory.mktemp(""data"").strpath\n    url = \'file://\' + path\n\n    generate_petastorm_dataset(url)\n\n    dataset = SyntheticDataset(url=url, path=path, data=None)\n\n    # Generate a dataset\n    assert os.path.exists(os.path.join(path, \'_SUCCESS\'))\n\n    return dataset\n\n\ndef test_generate(petastorm_dataset):\n    # Read from it using a plain reader\n    with make_reader(petastorm_dataset.url) as reader:\n        all_samples = list(reader)\n    assert all_samples\n\n\n@unittest.skip(\'Some conflict between pytorch and parquet shared libraries results in occasional \'\n               \'segfaults in this case.\')\ndef test_pytorch_hello_world_petastorm_dataset_example(petastorm_dataset):\n    pytorch_hello_world(petastorm_dataset.url)\n\n\ndef test_pyspark_hello_world_petastorm_dataset_example(petastorm_dataset):\n    pyspark_hello_world(petastorm_dataset.url)\n\n\ndef test_python_hello_world_petastorm_dataset_example(petastorm_dataset):\n    pyspark_hello_world(petastorm_dataset.url)\n\n\n@create_tf_graph\ndef test_tensorflow_hello_world_petastorm_dataset_example(petastorm_dataset):\n    tensorflow_hello_world(petastorm_dataset.url)\n'"
