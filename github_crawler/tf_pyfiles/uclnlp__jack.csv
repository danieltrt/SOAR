file_path,api_count,code
setup.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom setuptools import find_packages\nfrom setuptools import setup\nfrom setuptools.command.develop import develop as _develop\nfrom setuptools.command.install import install as _install\n\n\ndef spacy_download_en():\n    import spacy\n    try:\n        spacy.load(\'en\')\n    except:\n        import subprocess\n        args = [\'python3 -m spacy download en\']\n        subprocess.call(args, shell=True)\n\n\nclass Install(_install):\n    def run(self):\n        _install.do_egg_install(self)\n        spacy_download_en()\n        _install.run(self)\n\n\nclass Develop(_develop):\n    def run(self):\n        spacy_download_en()\n        _develop.run(self)\n\n\nwith open(\'requirements.txt\', \'r\') as f:\n    install_requires = [l for l in f.readlines() if not l.startswith(\'http://\')]\n\nextras_require = {\n    \'tf\': [\'tensorflow==1.8.0\'],\n    \'tf_gpu\': [\'tensorflow-gpu==1.8.0\'],\n    \'torch\': [\'torch\']\n}\n\nwith open(""README.md"", ""r+"", encoding=""utf-8"") as f:\n    long_description = f.read()\n\nsetup(name=\'uclmr-jack\',\n      version=\'0.2.1\',\n      description=\'Jack the Reader is a Python framework for Machine Reading\',\n      long_description=long_description,\n      long_description_content_type=""text/markdown"",\n      author=\'UCL Machine Reading\',\n      author_email=\'s.riedel@cs.ucl.ac.uk\',\n      url=\'https://github.com/uclmr/jack\',\n      test_suite=\'tests\',\n      license=\'MIT\',\n      packages=find_packages(),\n      cmdclass={\n          \'install\': Install,\n          \'develop\': Develop\n      },\n      install_requires=install_requires,\n      extras_require=extras_require,\n      setup_requires=install_requires,\n      tests_require=install_requires,\n      classifiers=[\n          \'Development Status :: 4 - Beta\',\n          \'Intended Audience :: Developers\',\n          \'Intended Audience :: Education\',\n          \'Intended Audience :: Science/Research\',\n          \'License :: OSI Approved :: MIT License\',\n          \'Programming Language :: Python :: 3\',\n          \'Programming Language :: Python :: 3.6\',\n          \'Topic :: Software Development :: Libraries\',\n          \'Topic :: Software Development :: Libraries :: Python Modules\',\n          \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n          \'Operating System :: OS Independent\'\n      ],\n      keywords=\'tensorflow machine learning natural language processing question answering\')\n'"
bin/create-squad-predictions.py,7,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport json\nimport logging\nimport os\nimport sys\n\nimport tensorflow as tf\n\nfrom jack.io.load import loaders\nfrom jack.readers.implementations import reader_from_file\n\nlogger = logging.getLogger(os.path.basename(sys.argv[0]))\nlogging.basicConfig(level=logging.INFO)\n\ntf.app.flags.DEFINE_string(\'dataset\', None, \'dataset file\')\ntf.app.flags.DEFINE_string(\'loader\', \'squad\', \'either squad or jack\')\ntf.app.flags.DEFINE_string(\'load_dir\', None, \'directory to saved model\')\ntf.app.flags.DEFINE_string(\'out\', ""results.json"", \'Result file path.\')\ntf.app.flags.DEFINE_integer(\'batch_size\', 64, \'batch size\')\ntf.app.flags.DEFINE_string(\'overwrite\', \'{}\', \'json string that can overwrite configuration.\')\n\nFLAGS = tf.app.flags.FLAGS\n\nlogger.info(""Creating and loading reader from {}..."".format(FLAGS.load_dir))\nconfig = {""max_support_length"": None}\nconfig.update(json.loads(FLAGS.overwrite))\nreader = reader_from_file(FLAGS.load_dir, **config)\n\ndataset = loaders[FLAGS.loader](FLAGS.file)\n\nlogger.info(""Start!"")\nanswers = reader.process_dataset(dataset, FLAGS.batch_size, silent=False)\nresults = {dataset[i][0].id: a.text for i, a in enumerate(answers)}\nwith open(FLAGS.out, ""w"") as out_file:\n    json.dump(results, out_file)\n\nlogger.info(""Done!"")\n'"
bin/jack-eval.py,7,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport json\nimport logging\nimport os\nimport sys\n\nimport tensorflow as tf\n\nfrom jack.eval import evaluate_reader, pretty_print_results\nfrom jack.io.load import loaders\nfrom jack.readers import reader_from_file\n\nlogger = logging.getLogger(os.path.basename(sys.argv[0]))\nlogging.basicConfig(level=logging.INFO)\n\ntf.app.flags.DEFINE_string(\'dataset\', None, \'dataset file\')\ntf.app.flags.DEFINE_string(\'loader\', \'jack\', \'name of loader\')\ntf.app.flags.DEFINE_string(\'load_dir\', None, \'directory to saved model\')\ntf.app.flags.DEFINE_integer(\'batch_size\', 64, \'batch size\')\ntf.app.flags.DEFINE_integer(\'max_examples\', None, \'maximum number of examples to evaluate\')\ntf.app.flags.DEFINE_string(\'overwrite\', \'{}\', \'json string that overwrites configuration.\')\n\nFLAGS = tf.app.flags.FLAGS\n\nlogger.info(""Creating and loading reader from {}..."".format(FLAGS.load_dir))\n\nkwargs = json.loads(FLAGS.overwrite)\n\nreader = reader_from_file(FLAGS.load_dir, **kwargs)\ndataset = loaders[FLAGS.loader](FLAGS.dataset)\nif FLAGS.max_examples:\n    dataset = dataset[:FLAGS.max_examples]\n\nlogger.info(""Start!"")\nresult_dict = evaluate_reader(reader, dataset, FLAGS.batch_size)\n\n\nlogger.info(""############### RESULTS ##############"")\npretty_print_results(result_dict)\n'"
bin/jack-train.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport logging\nimport os\nimport os.path as path\nimport shutil\nimport sys\nimport tempfile\nimport uuid\n\nfrom sacred import Experiment\nfrom sacred.arg_parser import parse_args\n\nfrom jack import readers\nfrom jack import train as jtrain\nfrom jack.core.shared_resources import SharedResources\nfrom jack.io.embeddings.embeddings import load_embeddings\nfrom jack.io.load import loaders\nfrom jack.util.vocab import Vocab\n\n# register knowledge integration models\n\nlogger = logging.getLogger(os.path.basename(sys.argv[0]))\n\nparsed_args = dict(x.split(""="") for x in parse_args(sys.argv)[""UPDATE""])\nif ""config"" in parsed_args:\n    path = parsed_args[""config""]\nelse:\n    path = ""./conf/jack.yaml""\n\n\ndef fetch_parents(current_path):\n    tmp_ex = Experiment(\'jack\')\n    if not isinstance(current_path, list):\n        current_path = [current_path]\n    all_paths = list(current_path)\n    for p in current_path:\n        tmp_ex.add_config(p)\n        if ""parent_config"" in tmp_ex.configurations[-1]._conf:\n            all_paths = fetch_parents(tmp_ex.configurations[-1]._conf[""parent_config""]) + all_paths\n    return all_paths\n\n\nconfigs = fetch_parents(path)\nlogger.info(""Loading {}"".format(configs))\nex = Experiment(\'jack\')\nfor c_path in configs:\n    ex.add_config(c_path)\n\nlogging.basicConfig(level=logging.INFO)\n\n\n@ex.automain\ndef run(loader,\n        debug,\n        debug_examples,\n        embedding_file,\n        embedding_format,\n        repr_dim_task_embedding,\n        reader,\n        train,\n        num_train_examples,\n        dev,\n        num_dev_examples,\n        test,\n        vocab_from_embeddings,\n        **kwargs):\n    logger.info(""TRAINING"")\n\n    # build JTReader\n    parsed_config = ex.current_run.config\n    ex.run(\'print_config\', config_updates=parsed_config)\n\n    if \'JACK_TEMP\' not in os.environ:\n        jack_temp = os.path.join(tempfile.gettempdir(), \'jack\', str(uuid.uuid4()))\n        os.environ[\'JACK_TEMP\'] = jack_temp\n        logger.info(""JACK_TEMP not set, setting it to %s. Might be used for caching."" % jack_temp)\n    else:\n        jack_temp = os.environ[\'JACK_TEMP\']\n    if not os.path.exists(jack_temp):\n        os.makedirs(jack_temp)\n\n    if debug:\n        train_data = loaders[loader](train, debug_examples)\n\n        logger.info(\'loaded {} samples as debug train/dev/test dataset \'.format(debug_examples))\n\n        dev_data = train_data\n        test_data = train_data\n\n        if embedding_file is not None and embedding_format is not None:\n            emb_file = \'glove.6B.50d.txt\'\n            embeddings = load_embeddings(path.join(\'data\', \'GloVe\', emb_file), \'glove\')\n            logger.info(\'loaded pre-trained embeddings ({})\'.format(emb_file))\n        else:\n            embeddings = None\n    else:\n        train_data = loaders[loader](train, num_train_examples)\n        dev_data = loaders[loader](dev, num_dev_examples)\n        test_data = loaders[loader](test) if test else None\n\n        logger.info(\'loaded train/dev/test data\')\n        if embedding_file is not None and embedding_format is not None:\n            embeddings = load_embeddings(embedding_file, embedding_format)\n            logger.info(\'loaded pre-trained embeddings ({})\'.format(embedding_file))\n        else:\n            embeddings = None\n            if vocab_from_embeddings:\n                raise ValueError(""If you want to create vocab from embeddings, embeddings have to be provided"")\n\n    vocab = Vocab(vocab=embeddings.vocabulary if vocab_from_embeddings and embeddings is not None else None)\n\n    if repr_dim_task_embedding < 1 and embeddings is None:\n        raise ValueError(""Either provide pre-trained embeddings or set repr_dim_task_embedding > 0."")\n\n\n    # name defaults to name of the model\n    if \'name\' not in parsed_config or parsed_config[\'name\'] is None:\n        parsed_config[\'name\'] = reader\n\n    shared_resources = SharedResources(vocab, parsed_config, embeddings)\n    jtreader = readers.readers[reader](shared_resources)\n\n    try:\n        jtrain(jtreader, train_data, test_data, dev_data, parsed_config, debug=debug)\n    finally:  # clean up temporary dir\n        if os.path.exists(jack_temp):\n            shutil.rmtree(jack_temp)\n'"
bin/mmap-cli.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport os\nimport sys\n\nfrom jack.io.embeddings import load_embeddings\nfrom jack.io.embeddings.memory_map import save_as_memory_map_dir\n\nimport logging\nlogger = logging.getLogger(os.path.basename(sys.argv[0]))\n\n\ndef main():\n    import argparse\n    parser = argparse.ArgumentParser(description=\'Convert embeddings to memory map directory\')\n    parser.add_argument(""input_file"", help=""The input embedding file."")\n    parser.add_argument(""output_dir"",\n                        help=""The name of the directory to store the memory map in. Will be created if it doesn\'t ""\n                             ""exist."")\n    parser.add_argument(""-f"", ""--input_format"", help=""Format of input embeddings."", default=""glove"",\n                        choices=[""glove"", ""word2vec"", ""memory_map_dir""])\n    args = parser.parse_args()\n    input_name = args.input_file\n    output_dir = args.output_dir\n    embeddings = load_embeddings(input_name, typ=args.input_format)\n    logging.info(""Loaded embeddings from {}"".format(input_name))\n    save_as_memory_map_dir(output_dir, embeddings)\n    logging.info(""Stored embeddings to {}"".format(output_dir))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
bin/squad_evaluate-v1.1.py,0,"b'"""""" Official evaluation script for v1.1 of the SQuAD dataset. """"""\nfrom __future__ import print_function\n\nimport argparse\nimport json\n\nfrom jack.eval.extractive_qa_eval import *\n\n\ndef evaluate(dataset, predictions):\n    f1 = exact_match = total = 0\n    for article in dataset:\n        for paragraph in article[\'paragraphs\']:\n            for qa in paragraph[\'qas\']:\n                total += 1\n                if qa[\'id\'] not in predictions:\n                    message = \'Unanswered question \' + qa[\'id\'] + \\\n                              \' will receive score 0.\'\n                    print(message, file=sys.stderr)\n                    continue\n                ground_truths = list(map(lambda x: x[\'text\'], qa[\'answers\']))\n                prediction = predictions[qa[\'id\']]\n                exact_match += metric_max_over_ground_truths(\n                    exact_match_score, prediction, ground_truths)\n                f1 += metric_max_over_ground_truths(\n                    f1_score, prediction, ground_truths)\n\n    exact_match = 100.0 * exact_match / total\n    f1 = 100.0 * f1 / total\n\n    return {\'exact_match\': exact_match, \'f1\': f1}\n\n\nif __name__ == \'__main__\':\n    expected_version = \'1.1\'\n    parser = argparse.ArgumentParser(\n        description=\'Evaluation for SQuAD \' + expected_version)\n    parser.add_argument(\'dataset_file\', help=\'Dataset file\')\n    parser.add_argument(\'prediction_file\', help=\'Prediction File\')\n    args = parser.parse_args()\n    with open(args.dataset_file) as dataset_file:\n        dataset_json = json.load(dataset_file)\n        if (dataset_json[\'version\'] != expected_version):\n            print(\'Evaluation expects v-\' + expected_version +\n                  \', but got dataset with v-\' + dataset_json[\'version\'],\n                  file=sys.stderr)\n        dataset = dataset_json[\'data\']\n    with open(args.prediction_file) as prediction_file:\n        predictions = json.load(prediction_file)\n    print(json.dumps(evaluate(dataset, predictions)))\n'"
jack/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom jack.train_reader import train\n\n__all__ = [\n    'train'\n]\n"""
jack/train_reader.py,11,"b'# -*- coding: utf-8 -*-\n\nimport logging\nimport math\nimport os\nimport random\nimport shutil\n\nimport tensorflow as tf\n\nfrom jack import readers\nfrom jack.core.tensorflow import TFReader\nfrom jack.eval import evaluate_reader, pretty_print_results\nfrom jack.util.hooks import LossHook, ExamplesPerSecHook, ETAHook\n\nlogger = logging.getLogger(__name__)\n\n\ndef train(reader, train_data, test_data, dev_data, configuration: dict, debug=False):\n    if isinstance(reader, TFReader):\n        train_tensorflow(reader, train_data, test_data, dev_data, configuration, debug)\n    else:\n        train_pytorch(reader, train_data, test_data, dev_data, configuration, debug)\n\n\ndef train_tensorflow(reader, train_data, test_data, dev_data, configuration: dict, debug=False):\n    import tensorflow as tf\n    seed = configuration.get(\'seed\', 0)\n\n    # make everything deterministic\n    random.seed(seed)\n    tf.set_random_seed(seed)\n\n    clip_value = configuration.get(\'clip_value\')\n    batch_size = configuration.get(\'batch_size\')\n    dev_batch_size = configuration.get(\'dev_batch_size\') or batch_size\n    epochs = configuration.get(\'epochs\')\n    l2 = configuration.get(\'l2\')\n    optimizer = configuration.get(\'optimizer\')\n    learning_rate = configuration.get(\'learning_rate\')\n    min_learning_rate = configuration.get(\'min_learning_rate\')\n    learning_rate_decay = configuration.get(\'learning_rate_decay\')\n    log_interval = configuration.get(\'log_interval\')\n    validation_interval = configuration.get(\'validation_interval\')\n    tensorboard_folder = configuration.get(\'tensorboard_folder\')\n    reader_type = configuration.get(\'reader\')\n    save_dir = configuration.get(\'save_dir\')\n    write_metrics_to = configuration.get(\'write_metrics_to\')\n\n    if clip_value != 0.0:\n        clip_value = - abs(clip_value), abs(clip_value)\n\n    learning_rate = tf.get_variable(""learning_rate"", initializer=learning_rate, dtype=tf.float32, trainable=False)\n    lr_decay_op = learning_rate.assign(tf.maximum(learning_rate_decay * learning_rate, min_learning_rate))\n\n    name_to_optimizer = {\n        \'gd\': tf.train.GradientDescentOptimizer,\n        \'adam\': tf.train.AdamOptimizer,\n        \'adagrad\': tf.train.AdagradOptimizer,\n        \'adadelta\': tf.train.AdadeltaOptimizer,\n        \'rmsprop\': tf.train.RMSPropOptimizer\n    }\n\n    if optimizer not in name_to_optimizer:\n        raise ValueError(\'Unknown optimizer: {}\'.format(optimizer))\n\n    tf_optimizer_class = name_to_optimizer[optimizer]\n    tf_optimizer = tf_optimizer_class(learning_rate=learning_rate)\n\n    sw = None\n    if tensorboard_folder is not None:\n        if os.path.exists(tensorboard_folder):\n            shutil.rmtree(tensorboard_folder)\n        sw = tf.summary.FileWriter(tensorboard_folder)\n\n    # Hooks\n    iter_interval = 1 if debug else log_interval\n    hooks = [LossHook(reader, iter_interval, summary_writer=sw),\n             ETAHook(reader, iter_interval, int(math.ceil(len(train_data) / batch_size)), epochs),\n             ExamplesPerSecHook(reader, batch_size, iter_interval, sw)]\n\n    preferred_metric, best_metric = readers.eval_hooks[reader_type].preferred_metric_and_initial_score()\n\n    def side_effect(metrics, prev_metric):\n        """"""Returns: a state (in this case a metric) that is used as input for the next call""""""\n        if prev_metric is None:  # store whole reader only at beginning of training\n            reader.store(save_dir)\n        m = metrics[preferred_metric]\n        if prev_metric is not None and m < prev_metric:\n            reader.session.run(lr_decay_op)\n            logger.info(""Decayed learning rate to: %.5f"" % reader.session.run(learning_rate))\n        elif m > best_metric[0] and save_dir is not None:\n            best_metric[0] = m\n            reader.model_module.store(os.path.join(save_dir, ""model_module""))\n            logger.info(""Saving reader to: %s"" % save_dir)\n        return m\n\n    # this is the standard hook for the reader\n    hooks.append(readers.eval_hooks[reader_type](\n        reader, dev_data, dev_batch_size, summary_writer=sw, side_effect=side_effect,\n        iter_interval=validation_interval,\n        epoch_interval=(1 if validation_interval is None else None),\n        write_metrics_to=write_metrics_to))\n\n    # Train\n    reader.train(tf_optimizer, train_data, batch_size, max_epochs=epochs, hooks=hooks,\n                 l2=l2, clip=clip_value, clip_op=tf.clip_by_value, summary_writer=sw)\n\n    # Test final reader\n    if dev_data is not None and save_dir is not None:\n        reader.load(save_dir)\n        result_dict = evaluate_reader(reader, dev_data, batch_size)\n\n        logger.info(""############### Results on the Dev Set##############"")\n        pretty_print_results(result_dict)\n\n    if test_data is not None and save_dir is not None:\n        reader.load(save_dir)\n        result_dict = evaluate_reader(reader, test_data, batch_size)\n\n        logger.info(""############### Results on the Test Set##############"")\n        pretty_print_results(result_dict)\n\n\ndef train_pytorch(reader, train_data, test_data, dev_data, configuration: dict, debug=False):\n    import torch\n    seed = configuration.get(\'seed\')\n\n    # make everything deterministic\n    random.seed(seed)\n    torch.manual_seed(seed)\n\n    clip_value = configuration.get(\'clip_value\')\n    batch_size = configuration.get(\'batch_size\')\n    epochs = configuration.get(\'epochs\')\n    l2 = configuration.get(\'l2\')\n    optimizer = configuration.get(\'optimizer\')\n    learning_rate = configuration.get(\'learning_rate\')\n    learning_rate_decay = configuration.get(\'learning_rate_decay\')\n    log_interval = configuration.get(\'log_interval\')\n    validation_interval = configuration.get(\'validation_interval\')\n    tensorboard_folder = configuration.get(\'tensorboard_folder\')\n    model = configuration.get(\'reader\')\n    save_dir = configuration.get(\'save_dir\')\n    write_metrics_to = configuration.get(\'write_metrics_to\')\n\n    # need setup here already :(\n    reader.setup_from_data(train_data, is_training=True)\n\n    if clip_value != 0.0:\n        clip_value = - abs(clip_value), abs(clip_value)\n\n    name_to_optimizer = {\n        \'gd\': torch.optim.SGD,\n        \'adam\': torch.optim.Adam,\n        \'adagrad\': torch.optim.Adagrad,\n        \'adadelta\': torch.optim.Adadelta\n    }\n\n    if optimizer not in name_to_optimizer:\n        raise ValueError(\'Unknown optimizer: {}\'.format(optimizer))\n\n    torch_optimizer_class = name_to_optimizer[optimizer]\n    params = list(reader.model_module.prediction_module.parameters())\n    params.extend(reader.model_module.loss_module.parameters())\n\n    torch_optimizer = torch_optimizer_class(params, lr=learning_rate)\n\n    sw = None\n    if tensorboard_folder is not None:\n        if os.path.exists(tensorboard_folder):\n            shutil.rmtree(tensorboard_folder)\n        sw = tf.summary.FileWriter(tensorboard_folder)\n\n    # Hooks\n    iter_interval = 1 if debug else log_interval\n    hooks = [LossHook(reader, iter_interval, summary_writer=sw),\n             ExamplesPerSecHook(reader, batch_size, iter_interval, sw)]\n\n    preferred_metric, best_metric = readers.eval_hooks[model].preferred_metric_and_initial_score()\n\n    def side_effect(metrics, prev_metric):\n        """"""Returns: a state (in this case a metric) that is used as input for the next call""""""\n        m = metrics[preferred_metric]\n        if prev_metric is not None and m < prev_metric:\n            for param_group in torch_optimizer.param_groups:\n                param_group[\'lr\'] *= learning_rate_decay\n                logger.info(""Decayed learning rate to: %.5f"" % param_group[\'lr\'])\n        elif m > best_metric[0] and save_dir is not None:\n            best_metric[0] = m\n            if prev_metric is None:  # store whole model only at beginning of training\n                reader.store(save_dir)\n            else:\n                reader.model_module.store(os.path.join(save_dir, ""model_module""))\n            logger.info(""Saving model to: %s"" % save_dir)\n        return m\n\n    # this is the standard hook for the model\n    hooks.append(readers.eval_hooks[model](\n        reader, dev_data, batch_size, summary_writer=sw, side_effect=side_effect,\n        iter_interval=validation_interval,\n        epoch_interval=(1 if validation_interval is None else None),\n        write_metrics_to=write_metrics_to))\n\n    # Train\n    reader.train(torch_optimizer, train_data, batch_size, max_epochs=epochs, hooks=hooks,\n                 l2=l2, clip=clip_value)\n\n    # Test final model\n    if dev_data is not None and save_dir is not None:\n        reader.load(save_dir)\n        result_dict = evaluate_reader(reader, dev_data, batch_size)\n\n        logger.info(""############### Results on the Dev Set##############"")\n        pretty_print_results(result_dict)\n\n    if test_data is not None and save_dir is not None:\n        reader.load(save_dir)\n        result_dict = evaluate_reader(reader, test_data, batch_size)\n\n        logger.info(""############### Results on the Test Set##############"")\n        pretty_print_results(result_dict)\n'"
notebooks/prettyprint.py,0,"b'class QAPrettyPrint:\n    def __init__(self, support, span):\n        self.support = support\n        self.span = span\n\n    def _repr_html_(self):\n        start, end = self.span\n        pre_highlight = self.support[:start]\n        highlight = self.support[start:end]\n        post_highlight = self.support[end:]\n        \n        def _highlight(text):\n            return \'<span style=""background-color: #ff00ff; color: white"">\' + text + \'</span>\'\n        \n        text = pre_highlight + _highlight(highlight) + post_highlight\n        return text.replace(\'\\n\', \'<br>\')\n\ndef print_nli(premise, hypothesis, label):\n\tprint(\'{}\\t--({})-->\\t{}\'.format(premise, label, hypothesis))\n'"
tests/conftest.py,0,"b'import pytest\n\n\ndef pytest_collection_modifyitems(items):\n    for item in items:\n        if ""sentihood"" in item.nodeid:\n            item.add_marker(pytest.mark.sentihood)\n        elif ""SNLI"" in item.nodeid:\n            item.add_marker(pytest.mark.SNLI)\n\n        if ""overfit"" in item.nodeid:\n            item.add_marker(pytest.mark.overfit)\n        elif ""smalldata"" in item.nodeid:\n            item.add_marker(pytest.mark.smalldata)\n        elif ""readme"" in item.nodeid:\n            item.add_marker(pytest.mark.readme)\n'"
tests/test_readme.py,2,"b'# -*- coding: utf-8 -*-\n\nimport subprocess\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom jack import readers\nfrom jack.core.data_structures import QASetting\n\n\ndef test_readme_fastqa():\n    args = [\'python3\', \'./bin/jack-train.py\', \'with\', \'config=tests/test_conf/fastqa_test.yaml\']\n    p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    out, err = p.communicate()\n\n    tf.reset_default_graph()\n\n    fastqa_reader = readers.fastqa_reader()\n    fastqa_reader.load_and_setup(""tests/test_results/fastqa_reader_test"")\n\n    support = """"""""Architecturally, the school has a Catholic character.\n    Atop the Main Building\'s gold dome is a golden statue of the Virgin Mary.\n    Immediately in front of the Main Building and facing it, is a copper statue of\n    Christ with arms upraised with the legend \\""Venite Ad Me Omnes\\"". Next to the\n    Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto,\n    a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes,\n    France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858.\n    At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome),\n    is a simple, modern stone statue of Mary.""""""\n\n    answers = fastqa_reader([QASetting(\n        question=""To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?"",\n        support=[support]\n    )])\n\n    assert answers[0][0].text is not None\n\n\ndef test_readme_dam():\n    args = [\'python3\', \'./bin/jack-train.py\', \'with\', \'config=tests/test_conf/dam_test.yaml\']\n    p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    out, err = p.communicate()\n\n    tf.reset_default_graph()\n\n    dam_reader = readers.dam_snli_reader()\n    dam_reader.load_and_setup(""tests/test_results/dam_reader_test"")\n\n    atomic_candidates = [\'entailment\', \'neutral\', \'contradiction\']\n    answers = dam_reader([QASetting(\n        question=""The boy plays with the ball."",\n        support=[""The boy plays with the ball.""],\n        candidates=atomic_candidates\n    )])\n\n    assert answers[0] is not None\n    assert isinstance(answers[0][0].score, np.float32)\n    assert answers[0][0].text in atomic_candidates\n'"
api_docs/APIDocGeneration/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# jack documentation build configuration file, created by\n# sphinx-quickstart on Mon Jan  9 17:30:20 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(\'..\'))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\'sphinx.ext.autodoc\',\n    \'sphinx.ext.doctest\',\n    \'sphinxcontrib.napoleon\',\n    \'sphinxtogithub\']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'jack\'\ncopyright = u\'2017, UCL Machine Reading Group\'\nauthor = u\'UCL Machine Reading Group\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = u\'\'\n# The full version, including alpha/beta/rc tags.\nrelease = u\'\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'alabaster\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'jtrdoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'jack.tex\', u\'jack Documentation\',\n     u\'UCL Machine Reading Group\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'jack\', u\'jack Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'jack\', u\'jack Documentation\',\n     author, \'jack\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n\n'"
data/QAngaroo/qangaroo2squad.py,0,"b'import json\nimport sys\n\n\ndef load_json(path):\n    with open(path, \'r\') as f:\n        return json.load(f)\n\n\ndef convert2SQUAD_format(hoppy_data, write_file_name):\n    """"""\n    Converts QAngaroo data (hoppy_data) into SQuAD format.\n    The SQuAD-formatted data is written to disk at write_file_name.\n    Note: All given support documents per example are concatenated\n        into one super-document. All text is lowercased.\n    """"""\n    # adapt the JSON tree structure used in SQUAD.\n    squad_formatted_content = dict()\n    squad_formatted_content[\'version\'] = \'hoppy_squad_format\'\n    data = []\n\n    # loop over dataset\n    for datum in hoppy_data:\n\n        # Format is deeply nested JSON -- prepare data structures\n        data_ELEMENT = dict()\n        data_ELEMENT[\'title\'] = \'dummyTitle\'\n        paragraphs = []\n        paragraphs_ELEMENT = dict()\n        qas = []\n        qas_ELEMENT = dict()\n        qas_ELEMENT_ANSWERS = []\n        ANSWERS_ELEMENT = dict()\n\n\n        ### content start\n        qas_ELEMENT[\'id\'] = datum[\'id\']\n        qas_ELEMENT[\'question\'] = datum[\'query\']\n\n        # concatenate all support documents into one superdocument\n        superdocument = "" <new_doc> "".join(datum[\'supports\']).lower()\n\n        # where is the answer in the superdocument?\n        answer_position = superdocument.find(datum[\'answer\'].lower())\n        if answer_position == -1:\n            continue\n\n        ANSWERS_ELEMENT[\'answer_start\'] = answer_position\n        ANSWERS_ELEMENT[\'text\'] = datum[\'answer\'].lower()\n        ### content end\n\n\n        # recursively fill in content into the nested SQuAD data format\n        paragraphs_ELEMENT[\'context\'] = superdocument\n        qas_ELEMENT_ANSWERS.append(ANSWERS_ELEMENT)\n\n        qas_ELEMENT[\'answers\'] = qas_ELEMENT_ANSWERS\n        qas.append(qas_ELEMENT)\n\n        paragraphs_ELEMENT[\'qas\'] = qas\n        paragraphs.append(paragraphs_ELEMENT)\n\n        data_ELEMENT[\'paragraphs\'] = paragraphs\n        data.append(data_ELEMENT)\n\n    squad_formatted_content[\'data\'] = data\n\n    with open(write_file_name, \'w\') as f:\n        json.dump(squad_formatted_content, f, indent=1)\n\n    print(\'Done writing SQuAD-formatted data to: \',write_file_name)\n\n\n\n\ndef main():\n    input_path = sys.argv[1]\n    output_path = sys.argv[2]\n    convert2SQUAD_format(load_json(input_path), output_path)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
data/emoji2vec/visualize.py,6,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\nfrom tensorflow.contrib.tensorboard.plugins import projector\nimport os\n\nimport numpy as np\n\ndir = ""./jack/data/emoji2vec/""\nemojis = []\nvecs = []\nwith open(dir + ""metadata.tsv"", ""w"") as f_out:\n    # f_out.write(""emoji\\n"")\n    with open(dir + ""emoji2vec.txt"", ""r"") as f_in:\n        for ix, line in enumerate(f_in.readlines()[1:]):\n            splits = line.strip().split("" "")\n            emoji = splits[0]\n            vec = [float(x) for x in splits[1:]]\n            assert len(vec) == 300\n            # print(emoji, vec)\n            emojis.append(emoji)\n            vecs.append(vec)\n            f_out.write(emoji+""\\n"")\n        f_in.close()\n    f_out.close()\n\nemoji2vec = tf.constant(np.array(vecs))\ntf_emoji2vec = tf.get_variable(""emoji2vec"", [len(vecs), 300], tf.float64)\n\n# save embeddings to file\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf_emoji2vec.assign(emoji2vec))\n\n    saver = tf.train.Saver()\n    saver.save(sess, os.path.join(dir, ""model.ckpt""), 0)\n\n    # Use the same LOG_DIR where you stored your checkpoint.\n    summary_writer = tf.summary.FileWriter(dir)\n\n    # Format: tensorflow/contrib/tensorboard/plugins/projector/projector_config.proto\n    config = projector.ProjectorConfig()\n\n    # You can add multiple embeddings. Here we add only one.\n    embedding = config.embeddings.add()\n    embedding.tensor_name = tf_emoji2vec.name\n    # Link this tensor to its metadata file (e.g. labels).\n    embedding.metadata_path = os.path.join(dir, \'metadata.tsv\')\n\n    # Saves a configuration file that TensorBoard will read during startup.\n    projector.visualize_embeddings(summary_writer, config)\n'"
data/triviaqa/config.py,0,"b'import os\nfrom os.path import join\n\n""""""\nGlobal config options\n""""""\n\nTRIVIA_QA = os.environ.get(\'TRIVIAQA_HOME\', None)\nTRIVIA_QA_UNFILTERED = os.environ.get(\'TRIVIAQA_UNFILTERED_HOME\', None)\n\nCORPUS_DIR = join(os.environ.get(\'TRIVIAQA_HOME\', \'\'), ""preprocessed"")\n\nVEC_DIR = \'\'\n'"
data/triviaqa/convert2jack.py,0,"b'import os\nimport pickle\nimport sys\nfrom concurrent.futures import ProcessPoolExecutor\n\nfrom jack.util.preprocessing import sort_by_tfidf\n\n\ndef extract_support(triviaqa_question, docs, corpus, max_num_support, max_tokens):\n    answers = []\n    supports = []\n    paragraph_tokens = []\n    separator = \'$|$\'\n    for doc in docs:\n        doc_tokens = corpus.get_document(doc.doc_id)\n        doc_tokens_flat = [t for p in doc_tokens for s in p for t in s]\n        doc_paragraph_tokens = [[t for s in p for t in s] for p in doc_tokens]\n\n        # merge many small paragraphs\n        if max_tokens > 0:\n            new_paragraph_tokens = [[]]\n            for s in doc_paragraph_tokens:\n                if len(new_paragraph_tokens[-1]) + len(s) >= max_tokens and len(new_paragraph_tokens[-1]) > 0:\n                    # start new paragraph\n                    if len(s) >= max_tokens:\n                        while s:\n                            new_paragraph_tokens.append(s[:max_tokens])\n                            s = s[max_tokens:]\n                    else:\n                        new_paragraph_tokens.append(s)\n                else:\n                    # merge with recent paragraph\n                    if len(new_paragraph_tokens[-1]) > 0:\n                        new_paragraph_tokens[-1].append(separator)\n                    new_paragraph_tokens[-1].extend(s)\n        else:\n            new_paragraph_tokens = doc_paragraph_tokens\n        paragraph_tokens.extend(new_paragraph_tokens)\n\n        p_idx_flat = [i for i, p in enumerate(new_paragraph_tokens) for t in p if t != separator]\n        assert len(doc_tokens_flat) == len(p_idx_flat)\n\n        doc_idx_offset = len(supports)\n        supports.extend("" "".join(s) for s in new_paragraph_tokens)\n        support_offsets = [0]\n        for s in supports[doc_idx_offset:]:\n            support_offsets.append(support_offsets[-1] + len(s) + 1)\n        if doc.answer_spans is not None:\n            for flat_s, flat_e in doc.answer_spans:\n                p_idx = p_idx_flat[flat_s]\n                s = flat_s - sum(1 for p2 in new_paragraph_tokens[:p_idx] for t in p2 if t != separator)\n                p = new_paragraph_tokens[p_idx]\n                k = 0\n                char_s = 0\n                while s > k:\n                    if p[k] == separator:\n                        s += 1\n                    char_s += len(p[k]) + 1\n                    k += 1\n                char_e = char_s + sum(len(t) + 1 for t in doc_tokens_flat[flat_s:flat_e + 1]) - 1\n                answers.append({\n                    ""text"": "" "".join(doc_tokens_flat[flat_s:flat_e + 1]),\n                    ""span"": [char_s, char_e],\n                    ""doc_idx"": p_idx + doc_idx_offset\n                })\n        del doc_tokens, p_idx_flat, doc_tokens_flat\n\n    if max_num_support > 0 and len(supports) > max_num_support:\n        sorted_supports = sort_by_tfidf("" "".join(triviaqa_question.question), [\' \'.join(p) for p in paragraph_tokens])\n        sorted_supports = [i for i, _ in sorted_supports]\n        sorted_supports_rev = {v: k for k, v in enumerate(sorted_supports)}\n        if answers:\n            min_answer_rev = min(sorted_supports_rev[a[\'doc_idx\']] for a in answers)\n            if min_answer_rev >= max_num_support:\n                min_answer = sorted_supports[min_answer_rev]\n                # force at least one answer by swapping best paragraph with answer to be the n-th paragraph\n                old_nth_best = sorted_supports[max_num_support - 1]\n                sorted_supports[min_answer_rev] = sorted_supports[max_num_support - 1]\n                sorted_supports[max_num_support - 1] = min_answer\n                sorted_supports_rev[old_nth_best] = min_answer_rev\n                sorted_supports_rev[min_answer] = max_num_support - 1\n\n        sorted_supports_rev = {v: k for k, v in enumerate(sorted_supports)}\n        supports = [supports[i] for i in sorted_supports[:max_num_support]]\n        is_an_answer = len(answers) > 0\n        answers = [a for a in answers if sorted_supports_rev[a[\'doc_idx\']] < max_num_support]\n        for a in answers:\n            a[\'doc_idx\'] = sorted_supports_rev[a[\'doc_idx\']]\n        assert not is_an_answer or len(answers) > 0\n\n    return supports, answers\n\n\ndef convert_triviaqa(triviaqa_question, corpus, max_num_support, max_tokens, is_web):\n    question = "" "".join(triviaqa_question.question)\n    if is_web:\n        for doc in triviaqa_question.web_docs:\n            supports, answers = extract_support(triviaqa_question, [doc], corpus, max_num_support, max_tokens)\n            filename = corpus.file_id_map[doc.doc_id]\n            question_id = triviaqa_question.question_id + \'--\' + filename[4:] + "".txt""\n            yield {""questions"": [{""answers"": answers,\n                                  ""question"": {""text"": question, ""id"": question_id}}],\n                   ""support"": supports}\n        for doc in triviaqa_question.entity_docs:\n            supports, answers = extract_support(triviaqa_question, [doc], corpus, max_num_support, max_tokens)\n            question_id = triviaqa_question.question_id + \'--\' + doc.title.replace(\' \', \'_\') + "".txt""\n            yield {""questions"": [{""answers"": answers,\n                                  ""question"": {""text"": question, ""id"": question_id}}],\n                   ""support"": supports}\n    else:\n        question_id = triviaqa_question.question_id\n        supports, answers = extract_support(triviaqa_question, triviaqa_question.entity_docs,\n                                            corpus, max_num_support, max_tokens)\n        yield {""questions"": [{""answers"": answers,\n                              ""question"": {""text"": question, ""id"": question_id}}],\n               ""support"": supports}\n\n\ndef process(x, verbose=False):\n    dataset, filemap, max_num_support, max_tokens, is_web = x\n    instances = []\n    corpus = TriviaQaEvidenceCorpusTxt(filemap)\n    for i, q in enumerate(dataset):\n        if verbose and i % 1000 == 0:\n            print(""%d/%d done"" % (i, len(dataset)))\n        instances.extend(x for x in convert_triviaqa(q, corpus, max_num_support, max_tokens, is_web))\n    return instances\n\n\ndef convert_dataset(path, filemap, name, num_processes, max_num_support, max_tokens, is_web=True):\n    with open(path, \'rb\') as f:\n        dataset = pickle.load(f)\n\n    if num_processes == 1:\n        instances = process((dataset, filemap, max_num_support, max_tokens, is_web), True)\n    else:\n        chunk_size = 1000\n        executor = ProcessPoolExecutor(num_processes)\n        instances = []\n        i = 0\n        for processed in executor.map(\n                process, [(dataset[i * chunk_size:(i + 1) * chunk_size], filemap, max_num_support, max_tokens, is_web)\n                          for i in range(len(dataset) // chunk_size + 1)]):\n            instances.extend(processed)\n            i += chunk_size\n            print(""%d/%d done"" % (min(len(dataset), i), len(dataset)))\n\n    return {""meta"": {""source"": name}, \'instances\': instances}\n\n\nif __name__ == \'__main__\':\n    from docqa.triviaqa.evidence_corpus import TriviaQaEvidenceCorpusTxt\n    import json\n\n    dataset = sys.argv[1]\n\n    if len(sys.argv) > 2:\n        num_processes = int(sys.argv[2])\n    else:\n        num_processes = 1\n\n    if len(sys.argv) > 3:\n        max_paragraphs = int(sys.argv[3])\n    else:\n        max_paragraphs = -1\n\n    if len(sys.argv) > 4:\n        max_tokens = int(sys.argv[4])\n    else:\n        max_tokens = -1\n\n    triviaqa_prepro = os.environ[\'TRIVIAQA_HOME\'] + \'/preprocessed\'\n\n    is_web = dataset.startswith(\'web\')\n    dataset, split = dataset.split(\'-\')\n\n    ds = os.path.join(triviaqa_prepro, \'triviaqa/\', dataset)\n    with open(ds + ""/file_map.json"") as f:\n        filemap = json.load(f)\n\n    fn = \'%s-%s.json\' % (dataset, split)\n    print(""Converting %s..."" % fn)\n    new_ds = convert_dataset(os.path.join(ds, split + \'.pkl\'), filemap, fn, num_processes,\n                             max_paragraphs, max_tokens, is_web)\n    with open(\'data/triviaqa/%s\' % fn, \'w\') as f:\n        json.dump(new_ds, f)\n'"
jack/core/__init__.py,0,b'from jack.core.input_module import *\nfrom jack.core.model_module import *\nfrom jack.core.output_module import *\nfrom jack.core.reader import *\nfrom jack.core.tensorport import *\nfrom jack.core.shared_resources import *\n'
jack/core/data_structures.py,0,"b'# -*- coding: utf-8 -*-\n\n""""""\nHere we define light data structures to store the input to jack readers, and their output.\n""""""\n\nfrom typing import Tuple, Sequence\n\n\nclass Answer:\n    """"""\n    Representation of an answer to a question.\n    """"""\n\n    def __init__(self, text: str, span: Tuple[int, int] = None, doc_idx: int = 0, score: float = 1.0):\n        """"""\n        Create a new answer.\n        Args:\n            text: The text string of the answer.\n            span: For extractive QA, a span in the support documents. The triple `(start, end)`\n                represents a span in support document with index `doc_index` in the ordered sequence of\n            doc_idx: index of document where answer was found\n            support documents. The span starts at `start` and ends at `end` (exclusive).\n            score: the score a model associates with this answer.\n        """"""\n        assert span is None or len(span) == 2, ""span should be (char_start, char_end) tuple""\n\n        self.score = score\n        self.span = span\n        self.doc_idx = doc_idx\n        self.text = text\n\n\nclass QASetting:\n    """"""\n    Representation of a single question answering problem. It primarily consists of a question,\n    a list of support documents, and optionally, some set of candidate answers.\n    """"""\n\n    def __init__(self,\n                 question: str,\n                 support: Sequence[str] = (),\n                 id: str = None,\n                 candidates: Sequence[str] = None,\n                 seq_candidates: Sequence[str] = None,\n                 candidate_spans: Sequence[Tuple[int, int, int]] = None):\n        """"""\n        Create a new QASetting.\n        Args:\n            question: the question text.\n            support: a sequence of support documents the answerer has access to when answering the question.\n            id: an identifier for this question setting.\n            candidates: a list of candidate answer strings.\n            candidate_spans: for extractive QA, a sequence of candidate spans in the support documents.\n            A span `(doc_index,start,end)` corresponds to a span in support document with index `doc_index`,\n            with start position `start` and end position `end`.\n        """"""\n        self.id = id\n        self.candidate_spans = candidate_spans\n        self.candidates = candidates\n        self.support = support\n        self.question = question\n\n\ndef _jack_to_qasetting(instance, value, global_candidates):\n    support = [value(s) for s in instance[""support""]] if ""support"" in instance else None\n    idd = value(instance, \'id\')\n    for question_instance in instance[""questions""]:\n        question = value(question_instance[\'question\'])\n        idd = value(question_instance, \'id\') or idd\n        idd = value(question_instance[\'question\'], \'id\') or idd\n        if global_candidates is None:\n            candidates = [value(c) for c in question_instance[\'candidates\']] if ""candidates"" in question_instance else None\n        else:\n            candidates = global_candidates\n        answers = [Answer(value(c), value(c, \'span\'), value(c, \'doc_idx\', 0)) for c in\n                   question_instance[\'answers\']] if ""answers"" in question_instance else None\n        yield QASetting(question, support, candidates=candidates, id=idd), answers\n\n\ndef jack_to_qasetting(jtr_data, max_count=None):\n    """"""\n    Converts a python dictionary in Jack format to a QASetting.\n    Args:\n        jtr_data: dictionary extracted from jack json file.\n        max_count: maximal number of instances to load.\n\n    Returns:\n        list of QASetting\n    """"""\n\n    def value(c, key=""text"", default=None):\n        return c.get(key, default) if isinstance(c, dict) else c if key == \'text\' else default\n\n    global_candidates = [value(c) for c in jtr_data[\'globals\'][\'candidates\']] if \'globals\' in jtr_data else None\n\n    ans = [(inp, answer) for i in jtr_data[""instances""]\n           for inp, answer in _jack_to_qasetting(i, value, global_candidates)][:max_count]\n    return ans\n'"
jack/core/input_module.py,0,"b'# -*- coding: utf-8 -*-\n\nimport logging\nimport os\nimport random\nimport tempfile\nfrom abc import abstractmethod\nfrom typing import Iterable, Tuple, List, Mapping, TypeVar, Generic, Optional\n\nimport diskcache as dc\nimport numpy as np\n\nfrom jack.core.data_structures import QASetting, Answer\nfrom jack.core.shared_resources import SharedResources\nfrom jack.core.tensorport import TensorPort\nfrom jack.util.batch import shuffle_and_batch, GeneratorWithRestart\n\nlogger = logging.getLogger(__name__)\n\n\nclass InputModule:\n    """"""An input module processes inputs and turns them into tensors to be processed by the model module.\n\n    Note that all setting up should be done in the setup method, NOT in the constructor. Only use the constructor to\n    hand over external variables/states, like `SharedResources`.\n    """"""\n\n    def setup(self):\n        """"""Optionally, sets up the module (if needs setup after loading shared resources for instance).\n\n        Assumes shared resources are fully setup. Usually called after loading and after `setup_from_data` as well.""""""\n        pass\n\n    def setup_from_data(self, data: Iterable[Tuple[QASetting, List[Answer]]]):\n        """"""Optionally, sets up the module based on input data.\n\n        This usually involves setting up vocabularies and other resources. This\n        should and is only called before training, not before loading a saved model.\n\n        Args:\n            data: a set of pairs of input and answer.\n        """"""\n        pass\n\n    @property\n    @abstractmethod\n    def output_ports(self) -> List[TensorPort]:\n        """"""\n        Defines what types of tensors the output module produces in each batch.\n        Returns: a list of tensor ports that correspond to the tensor ports in the mapping\n        produced by `__call__`. The `batch_generator` method will return bindings for these\n        ports and the ones in `training_ports`.\n        """"""\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def training_ports(self) -> List[TensorPort]:\n        """"""\n        Defines what types of tensor are provided in addition to `output_ports` during training\n        in the `batch_generator` function. Typically these will be ports that describe\n        the target solution at training time.\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def __call__(self, qa_settings: List[QASetting]) -> Mapping[TensorPort, np.ndarray]:\n        """"""\n        Converts a list of inputs into a single batch of tensors, consistent with the `output_ports` of this\n        module.\n        Args:\n            qa_settings: a list of instances (question, support, optional candidates)\n\n        Returns:\n            A mapping from ports to tensors.\n\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def batch_generator(self, dataset: Iterable[Tuple[QASetting, List[Answer]]], batch_size: int,\n                        is_eval: bool) -> Iterable[Mapping[TensorPort, np.ndarray]]:\n        """"""\n        Given a training set of input-answer pairs, this method produces an iterable/generator\n        that when iterated over returns a sequence of batches. These batches map ports to tensors\n        just as `__call__` does, but provides additional bindings for the `training_ports` ports in\n        case `is_eval` is `False`.\n\n        Args:\n            dataset: a set of pairs of input and answer.\n            is_eval: is this dataset generated for evaluation only (not training).\n\n        Returns: An iterable/generator that, on each pass through the data, produces a list of batches.\n        """"""\n        raise NotImplementedError\n\n    def store(self, path):\n        """"""Store the state of this module. Default is that there is no state, so nothing to store.""""""\n        pass\n\n    def load(self, path):\n        """"""Load the state of this module. Default is that there is no state, so nothing to load.""""""\n        pass\n\n\nAnnotationType = TypeVar(\'AnnotationType\')\n\n\nclass OnlineInputModule(InputModule, Generic[AnnotationType]):\n    """"""InputModule that preprocesses datasets on the fly.\n\n    It provides implementations for `create_batch()` and `__call__()` and\n    introduces two abstract methods:\n    - `preprocess()`: Converts a list of instances to annotations.\n    - `create_batch()`: Converts a list of annotations to a batch.\n\n    Both of these methods are parameterized by `AnnotationType`. In the simplest\n    case, this could be a `dict`, but you could also define a separate class\n    for your annotation, in order to get stronger typing.\n    """"""\n\n    def __init__(self, shared_resources: SharedResources, seed=None):\n        self.shared_resources = shared_resources\n        self._rng = random.Random(seed or random.randint(0, 9999))\n\n    @abstractmethod\n    def preprocess(self, questions: List[QASetting], answers: Optional[List[List[Answer]]] = None,\n                   is_eval: bool = False) -> List[AnnotationType]:\n        """"""Preprocesses a list of samples, returning a list of annotations.\n\n        Batches of these annotation objects are then passed to the the `create_batch` method.\n\n        Args:\n            questions: The list of instances to preprocess\n            answers: (Optional) answers associated with the instances\n            is_eval: Whether this preprocessing is done for evaluation data\n\n        Returns:\n            List of annotations of the instances.\n        """"""\n\n        raise NotImplementedError\n\n    @abstractmethod\n    def create_batch(self, annotations: List[AnnotationType],\n                     is_eval: bool, with_answers: bool) -> Mapping[TensorPort, np.ndarray]:\n        """"""Creates a batch from a list of preprocessed questions.\n\n        These are given by a list of annotations as returned by `preprocess_instance`.\n        Args:\n            annotations: a list of annotations to be included in the batch\n            is_eval: whether the method is called for evaluation data\n            with_answers: whether answers are included in the annotations\n\n        Returns:\n            A mapping from ports to numpy arrays.\n        """"""\n\n        raise NotImplementedError\n\n    def _batch_questions(self, questions: List[Tuple[QASetting, List[Answer]]], batch_size, is_eval: bool):\n        """"""Optionally shuffles and batches annotations.\n\n        By default, all annotations are shuffled (if self.shuffle(is_eval) and\n        then batched. Override this method if you want to customize the\n        batching, e.g., to do stratified sampling, sampling with replacement,\n        etc.\n\n        Args:\n            - annotations: List of annotations to shuffle & batch.\n            - is_eval: Whether batches are generated for evaluation.\n\n        Returns: Batch iterator\n        """"""\n        rng = self._rng if self._shuffle(is_eval) else None\n        return shuffle_and_batch(questions, batch_size, rng)\n\n    def _shuffle(self, is_eval: bool) -> bool:\n        """"""Whether to shuffle the dataset in batch_annotations(). Default is noe is_eval.""""""\n        return not is_eval\n\n    def __call__(self, qa_settings: List[QASetting]) -> Mapping[TensorPort, np.ndarray]:\n        """"""Preprocesses all qa_settings, returns a single batch with all instances.""""""\n\n        annotations = self.preprocess(qa_settings, answers=None, is_eval=True)\n        return self.create_batch(annotations, is_eval=True, with_answers=False)\n\n    def batch_generator(self, dataset: List[Tuple[QASetting, List[Answer]]], batch_size: int, is_eval: bool) \\\n            -> Iterable[Mapping[TensorPort, np.ndarray]]:\n        """"""Preprocesses all instances, batches & shuffles them and generates batches in dicts.""""""\n        logger.info(""OnlineInputModule pre-processes data on-the-fly in first epoch and caches results for subsequent ""\n                    ""epochs! That means, first epoch might be slower."")\n        # only cache training data on file\n        use_cache = not is_eval and self.shared_resources.config.get(\'file_cache\', False)\n        if use_cache:\n            cache_dir = os.path.join(os.environ.get(\'JACK_TEMP\', tempfile.gettempdir()), \'cache\')\n            db = dc.Cache(cache_dir)\n            db.reset(\'cull_limit\', 0)\n            logger.info(""Caching temporary preprocessed data in %s. You can change cache dir using the""\n                        "" JACK_TEMP environment variable which defaults to /tmp/jack."" % cache_dir)\n        else:\n            db = dict()\n        preprocessed = set()\n        def make_generator():\n            running_idx = 0\n            for i, batch in enumerate(self._batch_questions(dataset, batch_size, is_eval)):\n                questions, answers = zip(*batch)\n                if any(q.id not in preprocessed for q in questions):\n                    annots = self.preprocess(questions, answers)\n                    if questions[0].id is None:  # make sure there is an id, if not we set it here\n                        for q in questions:\n                            if q.id is None:\n                                q.id = running_idx\n                                running_idx += 1\n                    for q, a in zip(questions, annots):\n                        preprocessed.add(q.id)\n                        db[q.id] = a\n                else:\n                    annots = [db[q.id] for q in questions]\n\n                yield self.create_batch(annots, is_eval, True)\n\n        return GeneratorWithRestart(make_generator)\n'"
jack/core/model_module.py,0,"b'# -*- coding: utf-8 -*-\nimport logging\nfrom abc import abstractmethod\nfrom typing import Mapping, List, Sequence\n\nimport numpy as np\n\nfrom jack.core.tensorport import TensorPort\n\nlogger = logging.getLogger(__name__)\n\n\nclass ModelModule:\n    """"""A model module defines the actual reader model by processing input tensors and producing output tensors.\n\n    A model module encapsulates two computations (possibly overlapping): one which computes all\n    predictions (to be processed by the output module) and another representing the loss(es) and potenially other\n    training related outputs. It defines the expected input and output tensor shapes and types via its respective input\n    and output pairs.\n    """"""\n\n    @abstractmethod\n    def __call__(self, batch: Mapping[TensorPort, np.ndarray],\n                 goal_ports: List[TensorPort] = None) -> Mapping[TensorPort, np.ndarray]:\n        """"""Runs a batch and returns values/outputs for specified goal ports.\n        Args:\n            batch: mapping from ports to values\n            goal_ports: optional output ports, defaults to output_ports of this module will be returned\n\n        Returns:\n            A mapping from goal ports to tensors.\n\n        """"""\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def output_ports(self) -> Sequence[TensorPort]:\n        """"""Returns: Definition of the output ports of this module (predictions made by this model).""""""\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def input_ports(self) -> Sequence[TensorPort]:\n        """"""Returns: Definition of the input ports.""""""\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def training_input_ports(self) -> Sequence[TensorPort]:\n        """"""Returns: Definition of the input ports necessary to create the training output ports, i.e., they do not have\n        to be provided during eval and they can include output ports of this module.""""""\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def training_output_ports(self) -> Sequence[TensorPort]:\n        """"""Returns: Definition of the output ports provided during training for this module (usually just the loss).""""""\n        raise NotImplementedError\n\n    @abstractmethod\n    def setup(self, is_training=True, reuse=False):\n        """"""Sets up the module.""""""\n        raise NotImplementedError\n\n    @abstractmethod\n    def store(self, path):\n        """"""Store the state of this module.""""""\n        raise NotImplementedError\n\n    @abstractmethod\n    def load(self, path):\n        """"""Load the state of this module.""""""\n        raise NotImplementedError\n'"
jack/core/output_module.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom abc import abstractmethod\nfrom typing import Sequence, Mapping\n\nimport numpy as np\n\nfrom jack.core.data_structures import QASetting, Answer\nfrom jack.core.tensorport import TensorPort\n\n\nclass OutputModule:\n    """"""\n    An output module takes the output (numpy) tensors of the model module and turns them into\n    jack data structures.\n    """"""\n\n    @property\n    @abstractmethod\n    def input_ports(self) -> Sequence[TensorPort]:\n        """"""Returns: correspond to a subset of output ports of model module.""""""\n        raise NotImplementedError\n\n    @abstractmethod\n    def __call__(self, questions: Sequence[QASetting], tensors: Mapping[TensorPort, np.array]) \\\n            -> Sequence[Answer]:\n        """"""\n        Process the tensors corresponding to the defined `input_ports` for a batch to produce a list of answers.\n        The module has access to the original inputs.\n        Args:\n            questions:\n            prediction:\n\n        Returns:\n\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def setup(self):\n        pass\n\n    def store(self, path):\n        """"""Store the state of this module. Default is that there is no state, so nothing to store.""""""\n        pass\n\n    def load(self, path):\n        """"""Load the state of this module. Default is that there is no state, so nothing to load.""""""\n        pass\n'"
jack/core/reader.py,0,"b'# -*- coding: utf-8 -*-\n\n""""""\nHere we define jack readers. Jack readers consist of 3 layers, one that transform\njack data structures into tensors, one that processes predicts the outputs and losses\nusing a TensorFlow model into other tensors, and one that converts these tensors back to jack data structures.\n""""""\n\nimport logging\nimport math\nimport os\nimport shutil\nfrom typing import Iterable, List\n\nimport progressbar\n\nfrom jack.core.data_structures import *\nfrom jack.core.input_module import InputModule\nfrom jack.core.model_module import ModelModule\nfrom jack.core.output_module import OutputModule\nfrom jack.core.shared_resources import SharedResources\n\nlogger = logging.getLogger(__name__)\n\n\nclass JTReader:\n    """"""\n    A tensorflow reader reads inputs consisting of questions, supports and possibly candidates, and produces answers.\n    It consists of three layers: input to tensor (input_module), tensor to tensor (model_module), and tensor to answer\n    (output_model). These layers are called in-turn on a given input (list).\n    """"""\n\n    def __init__(self,\n                 shared_resources: SharedResources,\n                 input_module: InputModule,\n                 model_module: ModelModule,\n                 output_module: OutputModule):\n        self._shared_resources = shared_resources\n        self._output_module = output_module\n        self._model_module = model_module\n        self._input_module = input_module\n        self._is_setup = False\n\n        assert all(port in self.input_module.output_ports for port in self.model_module.input_ports), \\\n            ""Input Module outputs must include model module inputs""\n\n        assert all(port in self.input_module.training_ports or port in self.model_module.output_ports or\n                   port in self.input_module.output_ports for port in self.model_module.training_input_ports), \\\n            ""Input Module (training) outputs and model module outputs must include model module training inputs""\n\n        assert all(port in self.model_module.output_ports or port in self.input_module.output_ports\n                   for port in self.output_module.input_ports), \\\n            ""Module model output must match output module inputs""\n\n    @property\n    def input_module(self) -> InputModule:\n        """"""Returns: input module""""""\n        return self._input_module\n\n    @property\n    def model_module(self) -> ModelModule:\n        """"""Returns: model module""""""\n        return self._model_module\n\n    @property\n    def output_module(self) -> OutputModule:\n        """"""Returns: output module""""""\n        return self._output_module\n\n    @property\n    def shared_resources(self) -> SharedResources:\n        """"""Returns: SharedResources object""""""\n        return self._shared_resources\n\n    def __call__(self, inputs: Sequence[QASetting]) -> Sequence[Answer]:\n        """"""\n        Answers a list of question settings\n        Args:\n            inputs: a list of inputs.\n\n        Returns:\n            predicted outputs/answers to a given (labeled) dataset\n        """"""\n        batch = self.input_module(inputs)\n        output_module_input = self.model_module(batch, self.output_module.input_ports)\n        answers = self.output_module(inputs, {p: output_module_input[p] for p in self.output_module.input_ports})\n        return answers\n\n    def process_dataset(self, dataset: Sequence[Tuple[QASetting, Answer]], batch_size: int, silent=True):\n        """"""\n        Similar to the call method, only that it works on a labeled dataset and applies batching. However, assumes\n        that batches in input_module.batch_generator are processed in order and do not get shuffled during with\n        flag is_eval set to true.\n\n        Args:\n            dataset:\n            batch_size: note this information is needed here, but does not set the batch_size the model is using.\n            This has to happen during setup/configuration.\n            silent: if true, no output\n\n        Returns:\n            predicted outputs/answers to a given (labeled) dataset\n        """"""\n        batches = self.input_module.batch_generator(dataset, batch_size, is_eval=True)\n        answers = list()\n        enumerator = enumerate(batches)\n        if not silent:\n            logger.info(""Start answering..."")\n            bar = progressbar.ProgressBar(\n                max_value=math.ceil(len(dataset) / batch_size),\n                widgets=[\' [\', progressbar.Timer(), \'] \', progressbar.Bar(), \' (\', progressbar.ETA(), \') \'])\n            enumerator = bar(enumerator)\n        for j, batch in enumerator:\n            output_module_input = self.model_module(batch, self.output_module.input_ports)\n            questions = [q for q, a in dataset[j * batch_size:(j + 1) * batch_size]]\n            answers.extend(a[0] for a in self.output_module(\n                questions, {p: output_module_input[p] for p in self.output_module.input_ports}))\n\n        return answers\n\n    def train(self, optimizer, training_set: Iterable[Tuple[QASetting, List[Answer]]], batch_size: int,\n              max_epochs=10, hooks=tuple(), **kwargs):\n        """"""\n        This method trains the reader (and changes its state).\n\n        Args:\n            optimizer: TF optimizer\n            training_set: the training instances.\n            max_epochs: maximum number of epochs\n            hooks: TrainingHook implementations that are called after epochs and batches\n            kwargs: additional reader specific options\n        """"""\n        raise NotImplementedError\n\n    def setup_from_data(self, data: Iterable[Tuple[QASetting, List[Answer]]], is_training=False):\n        """"""\n        Sets up modules given a training dataset if necessary.\n\n        Args:\n            data: training dataset\n            is_training: indicates whether it\'s the training phase or not\n        """"""\n        self.input_module.setup_from_data(data)\n        self.input_module.setup()\n        self.model_module.setup(is_training)\n        self.output_module.setup()\n        self._is_setup = True\n\n    def load_and_setup(self, path, is_training=False):\n        """"""\n        Sets up already stored reader from model directory.\n\n        Args:\n            path: training dataset\n            is_training: indicates whether it\'s the training phase or not\n        """"""\n        self.shared_resources.load(os.path.join(path, ""shared_resources""))\n        self.load_and_setup_modules(path, is_training)\n\n    def load_and_setup_modules(self, path, is_training=False):\n        """"""\n        Sets up already stored reader from model directory.\n\n        Args:\n            path: training dataset\n            is_training: indicates whether it\'s the training phase or not\n        """"""\n        self.input_module.setup()\n        self.input_module.load(os.path.join(path, ""input_module""))\n        self.model_module.setup(is_training)\n        self.model_module.load(os.path.join(path, ""model_module""))\n        self.output_module.setup()\n        self.output_module.load(os.path.join(path, ""output_module""))\n        self._is_setup = True\n\n    def load(self, path):\n        """"""\n        (Re)loads module states on a setup reader (but not shared resources).\n        If reader is not setup yet use setup from file instead.\n\n        Args:\n            path: model directory\n        """"""\n        self.input_module.load(os.path.join(path, ""input_module""))\n        self.model_module.load(os.path.join(path, ""model_module""))\n        self.output_module.load(os.path.join(path, ""output_module""))\n\n    def store(self, path):\n        """"""\n        Store module states and shared resources.\n\n        Args:\n            path: model directory\n        """"""\n        if os.path.exists(path):\n            shutil.rmtree(path)\n        os.makedirs(path)\n        self.shared_resources.store(os.path.join(path, ""shared_resources""))\n        self.input_module.store(os.path.join(path, ""input_module""))\n        self.model_module.store(os.path.join(path, ""model_module""))\n        self.output_module.store(os.path.join(path, ""output_module""))\n'"
jack/core/shared_resources.py,0,"b'""""""Shared resources are used to store reader all stateful information about a reader and share it between modules.\n\nExamples are include the vocabulary, hyper-parameters or name of a reader that are mostly stored in a configuration\ndict. Shared resources are also used later to setup an already saved reader.\n""""""\n\nimport os\nimport pickle\n\nimport yaml\n\nfrom jack.io.embeddings import Embeddings\nfrom jack.util.vocab import Vocab\n\n\nclass SharedResources:\n    """"""Shared resources between modules.\n\n    A class to provide and store generally shared resources, such as vocabularies,\n    across the reader sub-modules.\n    """"""\n\n    def __init__(self, vocab: Vocab = None, config: dict = None, embeddings: Embeddings = None):\n        """"""\n        Several shared resources are initialised here, even if no arguments\n        are passed when calling __init__.\n        The instantiated objects will be filled by the InputModule.\n        - self.config holds hyperparameter values and general configuration\n            parameters.\n        - self.vocab serves as default Vocabulary object.\n        - self.answer_vocab is by default the same as self.vocab. However,\n            this attribute can be changed by the InputModule, e.g. by setting\n            sepvocab=True when calling the setup_from_data() of the InputModule.\n        """"""\n        self.config = config or dict()\n        self.vocab = vocab\n        self.embeddings = embeddings\n\n    def store(self, path):\n        """"""\n        Saves all attributes of this object.\n\n        Args:\n            path: path to save shared resources\n        """"""\n        if not os.path.exists(path):\n            os.mkdir(path)\n        vocabs = [(k, v) for k, v in self.__dict__.items() if isinstance(v, Vocab)]\n        with open(os.path.join(path, \'remainder\'), \'wb\') as f:\n            remaining = {k: v for k, v in self.__dict__.items()\n                         if not isinstance(v, Vocab) and not k == \'config\' and not k == \'embeddings\'}\n            pickle.dump(remaining, f, pickle.HIGHEST_PROTOCOL)\n        for k, v in vocabs:\n            v.store(os.path.join(path, k))\n        with open(os.path.join(path, \'config.yaml\'), \'w\') as f:\n            yaml.dump(self.config, f)\n        if self.embeddings is not None:\n            self.embeddings.store(os.path.join(path, \'embeddings\'))\n\n    def load(self, path):\n        """"""\n        Loads this (potentially empty) resource from path (all object attributes).\n        Args:\n            path: path to shared resources\n        """"""\n        remainder_path = os.path.join(path, \'remainder\')\n        if os.path.exists(remainder_path):\n            with open(remainder_path, \'rb\') as f:\n                self.__dict__.update(pickle.load(f))\n        for f in os.listdir(path):\n            if f == \'config.yaml\':\n                with open(os.path.join(path, f), \'r\') as f:\n                    self.config = yaml.load(f)\n            elif f == \'embeddings\':\n                self.embeddings = Embeddings.from_dir(os.path.join(path, f))\n            else:\n                v = Vocab()\n                v.load(os.path.join(path, f))\n                self.__dict__[f] = v\n'"
jack/core/tensorflow.py,33,"b'# -*- coding: utf-8 -*-\n\nimport logging\nimport os\nimport sys\nfrom abc import abstractmethod\nfrom functools import reduce\nfrom typing import Iterable, Tuple, List, Mapping, Sequence\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom jack.core import JTReader, QASetting, Answer, Ports, ModelModule, SharedResources, TensorPort\nfrom jack.core.reader import logger\n\n\nclass TFModelModule(ModelModule):\n    """"""This class represents an  abstract ModelModule for tensroflow models which requires the implementation of\n    a small set of methods that produce the TF graphs to create predictions and the training outputs,\n    and define the ports.\n    """"""\n\n    def __init__(self, shared_resources: SharedResources, session=None):\n        self.shared_resources = shared_resources\n        if session is None:\n            session_config = tf.ConfigProto(allow_soft_placement=True)\n            session_config.gpu_options.allow_growth = True\n            session = tf.Session(config=session_config)\n        self.tf_session = session\n        # will be set in setup\n        self._tensors = None\n        self._placeholders = None\n\n    def __call__(self, batch: Mapping[TensorPort, np.ndarray],\n                 goal_ports: List[TensorPort] = None) -> Mapping[TensorPort, np.ndarray]:\n        """"""Runs a batch and returns values/outputs for specified goal ports.\n        Args:\n            batch: mapping from ports to values\n            goal_ports: optional output ports, defaults to output_ports of this module will be returned\n\n        Returns:\n            A mapping from goal ports to tensors.\n        """"""\n        goal_ports = goal_ports or self.output_ports\n        feed_dict = self.convert_to_feed_dict(batch)\n        goal_tensors = {p: self.tensors[p] for p in goal_ports\n                        if p in self.output_ports or p in self.training_output_ports}\n        outputs = self.tf_session.run(goal_tensors, feed_dict)\n\n        for p in goal_ports:\n            if p not in outputs and p in batch:\n                outputs[p] = batch[p]\n\n        return outputs\n\n    @abstractmethod\n    def create_output(self, shared_resources: SharedResources,\n                      input_tensors: Mapping[TensorPort, tf.Tensor]) -> \\\n            Mapping[TensorPort, tf.Tensor]:\n        """"""\n        This function needs to be implemented in order to define how the module produces\n        output from input tensors corresponding to `input_ports`.\n\n        Args:\n            shared_resources: contains resources shared by modules, such as hyper-parameters or vocabularies.\n            *input_tensors: a list of input tensors.\n\n        Returns:\n            mapping from defined output ports to their tensors.\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def create_training_output(self, shared_resources: SharedResources,\n                               training_input_tensors: Mapping[TensorPort, tf.Tensor]) \\\n            -> Mapping[TensorPort, tf.Tensor]:\n        """"""\n        This function needs to be implemented in order to define how the module produces tensors only used\n        during training given tensors corresponding to the ones defined by `training_input_ports`, which might include\n        tensors corresponding to ports defined by `output_ports`. This sub-graph should only be created during training.\n\n        Args:\n            shared_resources: contains resources shared by modules, such as hyper-parameters or vocabularies.\n            training_input_tensors: a mapping from training input tensorports to tensors.\n\n        Returns:\n            mapping from defined training output ports to their tensors.\n        """"""\n        raise NotImplementedError\n\n    def setup(self, is_training=True, reuse=False):\n        """"""Sets up the module.\n\n        This usually involves creating the actual tensorflow graph. It is expected to be called after the input module\n        is set up and shared resources, such as the vocab, config, etc., are prepared already at this point.\n        """"""\n        old_train_variables = tf.trainable_variables()\n        old_variables = tf.global_variables()\n\n        with tf.variable_scope(self.shared_resources.config.get(""name"", ""jtreader""),\n                               initializer=tf.contrib.layers.xavier_initializer(), reuse=reuse):\n            self._tensors = {p: p.create_tf_placeholder() for p in self.input_ports}\n            output_tensors = self.create_output(\n                self.shared_resources, {port: self._tensors[port] for port in self.input_ports})\n\n            self._placeholders = dict(self._tensors)\n            self._tensors.update(output_tensors)\n\n            if is_training:\n                self._placeholders.update((p, p.create_tf_placeholder()) for p in self.training_input_ports\n                                          if p not in self._placeholders and p not in self._tensors)\n                self._tensors.update(self._placeholders)\n                input_target_tensors = {p: self._tensors.get(p, None) for p in self.training_input_ports}\n                training_output_tensors = self.create_training_output(\n                    self.shared_resources, {port: input_target_tensors[port] for port in self.training_input_ports})\n                self._tensors.update(training_output_tensors)\n        self._training_variables = [v for v in tf.trainable_variables() if v not in old_train_variables]\n        self._saver = tf.train.Saver(self._training_variables, max_to_keep=1)\n        self._variables = [v for v in tf.global_variables() if v not in old_variables]\n        self.tf_session.run([v.initializer for v in self.variables])\n\n        for var in tf.global_variables():\n            is_trainable = var in tf.trainable_variables()\n            logger.debug(\'Variable: {} (Trainable: {})\'.format(var, is_trainable))\n\n        # Sometimes we want to initialize (partially) with a pre-trained model\n        load_dir = self.shared_resources.config.get(\'load_dir\')\n        if is_training and load_dir is not None:\n            if not load_dir.endswith(\'model_module\'):\n                # path to a reader was provided\n                load_dir = os.path.join(load_dir, \'model_module\')\n            # get all variables in the checkpoint file\n            from tensorflow.python import pywrap_tensorflow\n            reader = pywrap_tensorflow.NewCheckpointReader(load_dir)\n            init_vars = []\n            for n in reader.get_variable_to_shape_map().keys():\n                found = False\n                for v in self.variables:\n                    if v.op.name == n:\n                        found = True\n                        init_vars.append(v)\n                        break\n                if not found:\n                    logger.warn(""Could not find variable %s in computation graph to restore from pretrained model."" % n)\n\n            saver = tf.train.Saver(init_vars)\n            saver.restore(self.tf_session, load_dir)\n\n    @property\n    def placeholders(self) -> Mapping[TensorPort, tf.Tensor]:\n        if hasattr(self, ""_placeholders""):\n            return self._placeholders\n        else:\n            logger.warn(""Asking for placeholders without having setup this module. Returning None."")\n            return None\n\n    @property\n    def tensors(self) -> Mapping[TensorPort, tf.Tensor]:\n        if hasattr(self, ""_tensors""):\n            return self._tensors\n        else:\n            logger.warn(""Asking for tensors without having setup this module. Returning None."")\n            return None\n\n    def store(self, path):\n        self._saver.save(self.tf_session, path)\n\n    def load(self, path):\n        self._saver.restore(self.tf_session, path)\n\n    @property\n    def train_variables(self) -> Sequence[tf.Tensor]:\n        return self._training_variables\n\n    @property\n    def variables(self) -> Sequence[tf.Tensor]:\n        return self._variables\n\n    def convert_to_feed_dict(self, mapping: Mapping[TensorPort, np.ndarray]) -> Mapping[tf.Tensor, np.ndarray]:\n        result = {ph: mapping[port] for port, ph in self.placeholders.items() if port in mapping}\n        return result\n\n\nclass TFReader(JTReader):\n    """"""Tensorflow implementation of JTReader.\n\n    A tensorflow reader reads inputs consisting of questions, supports and possibly candidates, and produces answers.\n    It consists of three layers: input to tensor (input_module), tensor to tensor (model_module), and tensor to answer\n    (output_model). These layers are called in-turn on a given input (list).\n    """"""\n\n    @property\n    def model_module(self) -> TFModelModule:\n        return super().model_module\n\n    @property\n    def session(self) -> tf.Session:\n        """"""Returns: input module""""""\n        return self.model_module.tf_session\n\n    def train(self, optimizer,\n              training_set: Iterable[Tuple[QASetting, List[Answer]]],\n              batch_size: int, max_epochs=10, hooks=tuple(),\n              l2=0.0, clip=None, clip_op=tf.clip_by_value, summary_writer=None, **kwargs):\n        """"""\n        This method trains the reader (and changes its state).\n\n        Args:\n            optimizer: TF optimizer\n            training_set: the training instances.\n            batch_size: size of training batches\n            max_epochs: maximum number of epochs\n            hooks: TrainingHook implementations that are called after epochs and batches\n            l2: whether to use l2 regularization\n            clip: whether to apply gradient clipping and at which value\n            clip_op: operation to perform for clipping\n            summary_writer: summary writer\n        """"""\n        batches, loss, min_op, summaries = self._setup_training(\n            batch_size, clip, optimizer, training_set, summary_writer, l2, clip_op, **kwargs)\n\n        self._train_loop(min_op, loss, batches, hooks, max_epochs, summaries, summary_writer, **kwargs)\n\n    def _setup_training(self, batch_size, clip, optimizer, training_set, summary_writer, l2, clip_op, **kwargs):\n        global_step = tf.train.get_global_step()\n        if global_step is None:\n            global_step = tf.train.create_global_step()\n        if not self._is_setup:\n            # First setup shared resources, e.g., vocabulary. This depends on the input module.\n            logger.info(""Setting up model..."")\n            self.setup_from_data(training_set, is_training=True)\n        logger.info(""Preparing training data..."")\n        batches = self.input_module.batch_generator(training_set, batch_size, is_eval=False)\n        logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n        loss = self.model_module.tensors[Ports.loss]\n        summaries = None\n        if summary_writer is not None:\n            summaries = tf.summary.merge_all()\n        if l2:\n            loss += tf.add_n([tf.nn.l2_loss(v) for v in self.model_module.train_variables]) * l2\n        if clip:\n            gradients = optimizer.compute_gradients(loss)\n            if clip_op == tf.clip_by_value:\n                gradients = [(tf.clip_by_value(grad, clip[0], clip[1]), var)\n                             for grad, var in gradients if grad is not None]\n            elif clip_op == tf.clip_by_norm:\n                gradients = [(tf.clip_by_norm(grad, clip), var)\n                             for grad, var in gradients if grad is not None]\n            min_op = optimizer.apply_gradients(gradients, global_step)\n        else:\n            min_op = optimizer.minimize(loss, global_step)\n\n        variable_size = lambda v: reduce(lambda x, y: x * y, v.get_shape().as_list()) if v.get_shape() else 1\n        num_params = sum(variable_size(v) for v in self.model_module.train_variables)\n        logger.info(""Number of parameters: %d"" % num_params)\n\n        # initialize non model variables like learning rate, optimizer vars ...\n        self.session.run([v.initializer for v in tf.global_variables() if v not in self.model_module.variables])\n        return batches, loss, min_op, summaries\n\n    def _train_loop(self, optimization_op, loss_op, batches, hooks, max_epochs, summaries, summary_writer, **kwargs):\n        logger.info(""Start training..."")\n        for i in range(1, max_epochs + 1):\n            for j, batch in enumerate(batches):\n                feed_dict = self.model_module.convert_to_feed_dict(batch)\n\n                if summaries is not None:\n                    step, sums, current_loss, _ = self.session.run(\n                        [tf.train.get_global_step(), summaries, loss_op, optimization_op], feed_dict=feed_dict)\n                    summary_writer.add_summary(sums, step)\n                else:\n                    current_loss, _ = self.session.run([loss_op, optimization_op], feed_dict=feed_dict)\n\n                for hook in hooks:\n                    hook.at_iteration_end(i, current_loss, set_name=\'train\')\n\n            # calling post-epoch hooks\n            for hook in hooks:\n                hook.at_epoch_end(i)\n'"
jack/core/tensorport.py,4,"b'""""""Tensor ports are used to define \'module signatures\'\n\nModules are loosly coupled with each other via the use of tensor ports. They simply define what kind of tensors are\nproduced at the input and/or output of each module, thus defining a kind of signature. This allows for maximum\nflexibility when (re-)using modules in different combinations.\n""""""\nimport logging\nfrom typing import Mapping, Sequence, Any\n\nimport numpy as np\nimport tensorflow as tf\n\ntry:\n    import torch\nexcept ImportError as e:\n    pass\n\nlogger = logging.getLogger(__name__)\n\n_allowed_torch = {np.dtype(np.int8), np.dtype(np.int16), np.dtype(np.int32), np.dtype(np.int64),\n                  np.dtype(np.float16), np.dtype(np.float32), np.dtype(np.float64)}\n\n\nclass TensorPort:\n    """"""A TensorPort defines an input or output tensor for a modules.\n\n    A port defines at least a shape, name, and its data type.\n    """"""\n\n    def __init__(self, dtype, shape, name, doc_string=None, shape_string=None):\n        """"""Create a new TensorPort.\n\n        Args:\n            dtype: the (numpy) data type of the port.\n            shape: the shape of the tensor.\n            name: the name of this port (should be a valid TF name)\n            doc_string: a documentation string associated with this port\n            shape_string: a string of the form [size_1,size_2,size_3] where size_i is a text describing the\n                size of the tensor\'s dimension i (such as ""number of batches"").\n        """"""\n        self.dtype = np.dtype(dtype)\n        self.shape = shape\n        self.name = name\n        self.__doc__ = doc_string\n        self.shape_string = shape_string\n\n    def create_tf_placeholder(self):\n        """"""Convenience method that produces a placeholder of the type and shape defined by the port.\n\n        Returns: a placeholder of same type, shape and name.\n        """"""\n        return tf.placeholder(tf.as_dtype(self.dtype), self.shape, self.name)\n\n    def create_torch_variable(self, value, gpu=False):\n        """"""Convenience method that produces a tensor given the value of the defined type.\n\n        Returns: a torch tensor of same type.\n        """"""\n        if isinstance(value, torch.autograd.Variable):\n            if gpu:\n                value = value.cuda()\n            return value\n        if not torch.is_tensor(value):\n            if not isinstance(value, np.ndarray):\n                value = np.array(value, dtype=self.dtype)\n            else:\n                value = value.astype(self.dtype)\n            if value.size == 0:\n                return value\n\n            if self.dtype in _allowed_torch:\n                value = torch.autograd.Variable(torch.from_numpy(value))\n        else:\n            value = torch.autograd.Variable(value)\n        if gpu and isinstance(value, torch.autograd.Variable):\n            value = value.cuda()\n        return value\n\n    @staticmethod\n    def torch_to_numpy(value):\n        """"""Convenience method that produces a tensor given the value of the defined type.\n\n        Returns: a torch tensor of same type.\n        """"""\n        if isinstance(value, torch.autograd.Variable):\n            value = value.data\n        if torch.is_tensor(value):\n            return value.cpu().numpy()\n        elif isinstance(value, np.ndarray):\n            return value\n        else:\n            return np.ndarray(value)\n\n    def get_description(self):\n        """"""Returns a multi-line description string of the TensorPort.""""""\n\n        return ""Tensorport \'%s\'"" % self.name + ""\\n"" + \\\n               ""  dtype: "" + str(self.dtype) + ""\\n"" + \\\n               ""  shape: "" + str(self.shape) + ""\\n"" + \\\n               ""  doc_string: "" + str(self.__doc__) + ""\\n"" + \\\n               ""  shape_string: "" + str(self.shape_string)\n\n    def __gt__(self, port):\n        return self.name > port.name\n\n    def __repr__(self):\n        return ""<TensorPort (%s)>"" % self.name\n\n    @staticmethod\n    def to_mapping(ports: Sequence[\'TensorPort\'], tensors: Sequence[tf.Tensor]):\n        """"""\n        Create a dictionary of ports to tensors based on ordered port and tensor sequences.\n        Args:\n            ports: list of ports\n            tensors: list of tensors (same length as ports)\n\n        Returns: mapping from the i-th port to the i-th tensor in the lists.\n\n        """"""\n        return dict(zip(ports, tensors))\n\n\nclass TensorPortWithDefault(TensorPort):\n    """"""\n    TensorPort that also defines a default value.\n    """"""\n\n    def __init__(self, default_value, shape, name, doc_string=None, shape_string=None):\n        """"""Default value must be a numpy array.""""""\n        self.default_value = default_value\n        super().__init__(default_value.dtype, shape, name, doc_string=doc_string, shape_string=shape_string)\n\n    def create_tf_placeholder(self):\n        """"""Creates a TF placeholder_with_default.\n\n        Convenience method that produces a constant of the type, value and shape defined by the port.\n        Returns: a constant tensor of same type, shape and name. It can nevertheless be fed with external values\n        as if it was a placeholder.\n        """"""\n        ph = tf.placeholder_with_default(self.default_value, self.shape, self.name)\n        if ph.dtype != tf.as_dtype(self.dtype):\n            logger.warning(\n                ""Placeholder {} with default of type {} created for TensorPort with type {}!"".format(self.name,\n                                                                                                     ph.dtype,\n                                                                                                     self.dtype))\n        return ph\n\n    def create_torch_variable(self, value, gpu=False):\n        if value is None:\n            value = self.default_value\n        return super(TensorPortWithDefault, self).create_torch_variable(value, gpu)\n\n\nclass Ports:\n    """"""Defines sopme common ports for reusability and as examples. Readers can of course define their own.\n\n    This class groups input ports. Different modules can refer to these ports\n    to define their input or output, respectively.\n    """"""\n\n    loss = TensorPort(np.float32, [None], ""loss"",\n                      ""Represents loss on each instance in the batch"",\n                      ""[batch_size]"")\n    keep_prob = TensorPortWithDefault(np.array(1.0, np.float32), [], ""keep_prob"",\n                                      ""scalar representing keep probability when using dropout"",\n                                      ""[]"")\n    is_eval = TensorPortWithDefault(np.array(True), [], ""is_eval"",\n                                    ""boolean that determines whether input is eval or training."",\n                                    ""[]"")\n\n    class Input:\n        question = TensorPort(np.int32, [None, None], ""question"",\n                              ""Represents questions using symbol vectors"",\n                              ""[batch_size, max_num_question_tokens]"")\n\n        support = TensorPort(np.int32, [None, None], ""support"",\n                             ""Represents instances with single support documents"",\n                             ""[batch_size, max_num_tokens]"")\n\n        multiple_support = TensorPort(np.int32, [None, None, None], ""multiple_support"",\n                                      (""Represents instances with multiple support documents"",\n                                       "" or single instances with extra dimension set to 1""),\n                                      ""[batch_size, max_num_support, max_num_tokens]"")\n\n        atomic_candidates = TensorPort(np.int32, [None, None], ""atomic_candidates"",\n                                       (""Represents candidate choices using single symbols. "",\n                                        ""This could be a list of entities from global entities "",\n                                        ""for example atomic_candidates = [e1, e7, e83] from "",\n                                        ""global_entities = [e1, e2, e3, ..., eN-1, eN""),\n                                       ""[batch_size, num_candidates]"")\n\n        sample_id = TensorPort(np.int32, [None], ""sample_id"",\n                               ""Maps this sample to the index in the input data"",\n                               ""[batch_size]"")\n\n        muliple_support_length = TensorPort(np.int32, [None, None], ""muliple_support_length"",\n                                            ""Represents length of supports in each support in batch"",\n                                            ""[batch_size, num_supports]"")\n\n        support_length = TensorPort(np.int32, [None], ""support_length"",\n                                    ""Represents length of supports in batch"",\n                                    ""[batch_size]"")\n\n        question_length = TensorPort(np.int32, [None], ""question_length"",\n                                     ""Represents length of questions in batch"",\n                                     ""[batch_size]"")\n\n        emb_support = TensorPort(np.float32, [None, None, None], ""emb_support"",\n                                      ""Represents the embedded support"",\n                                      ""[S, max_num_tokens, N]"")\n\n        emb_question = TensorPort(np.float32, [None, None, None], ""emb_question"",\n                                       ""Represents the embedded question"",\n                                       ""[Q, max_num_question_tokens, N]"")\n\n        # character based information\n        word_chars = TensorPort(np.int32, [None, None], ""word_chars"",\n                                ""Represents questions using symbol vectors"",\n                                ""[U, max_num_chars]"")\n        word_char_length = TensorPort(np.int32, [None], ""word_char_length"",\n                                      ""Represents questions using symbol vectors"",\n                                      ""[U]"")\n        question_batch_words = TensorPort(np.int32, [None, None], ""question_batch_words"",\n                                          ""Represents question using in-batch vocabulary."",\n                                          ""[batch_size, max_num_question_tokens]"")\n        support_batch_words = TensorPort(np.int32, [None, None], ""support_batch_words"",\n                                         ""Represents support using in-batch vocabulary"",\n                                         ""[batch_size, max_num_support_tokens]"")\n\n        # Number of questions in batch is Q, number of supports is S, number of answers is A, number of candidates is C.\n        # Typical input ports such as support, candidates, answers are defined together with individual mapping ports.\n        # This allows for more flexibility when numbers can vary between questions.\n\n        support2question = TensorPort(np.int32, [None], ""support2question"",\n                                      ""Represents mapping to question idx per support"",\n                                      ""[S]"")\n        candidate2question = TensorPort(np.int32, [None], ""candidate2question"",\n                                        ""Represents mapping to question idx per candidate"",\n                                        ""[C]"")\n        answer2support = TensorPortWithDefault(np.array([0], np.int32), [None], ""answer2support"",\n                                               ""Represents mapping to support idx per answer"", ""[A]"")\n        atomic_candidates1D = TensorPort(np.int32, [None], ""candidates1D"",\n                                         ""Represents candidate choices using single symbols"",\n                                         ""[C]"")\n\n        seq_candidates = TensorPort(np.int32, [None, None], ""seq_candidates"",\n                                    ""Represents candidate choices using single symbols"",\n                                    ""[C, max_num_tokens]"")\n\n        # MISC intermediate ports that might come in handy\n        # -embeddings\n        embedded_seq_candidates = TensorPort(np.float32, [None, None, None], ""embedded_seq_candidates_flat"",\n                                             ""Represents the embedded sequential candidates"",\n                                             ""[C, max_num_tokens, N]"")\n\n        embedded_candidates = TensorPort(np.float32, [None, None], ""embedded_candidates_flat"",\n                                         ""Represents the embedded candidates"",\n                                         ""[C, N]"")\n\n    class Prediction:\n        logits = TensorPort(np.float32, [None, None], ""logits"",\n                            ""Represents output scores for each candidate"",\n                            ""[C, num_candidates]"")\n\n        candidate_index = TensorPort(np.float32, [None], ""candidate_idx"",\n                                     ""Represents answer as a single index"",\n                                     ""[C]"")\n\n        candidate_scores = TensorPort(np.float32, [None], ""candidate_scores_flat"",\n                                      ""Represents output scores for each candidate"",\n                                      ""[C]"")\n\n        candidate_idx = TensorPort(np.float32, [None], ""candidate_predictions_flat"",\n                                   ""Represents groundtruth candidate labels, usually 1 or 0"",\n                                   ""[C]"")\n\n        # extractive QA\n        start_scores = TensorPort(np.float32, [None, None], ""start_scores"",\n                                  ""Represents start scores for each support sequence"",\n                                  ""[S, max_num_tokens]"")\n\n        end_scores = TensorPort(np.float32, [None, None], ""end_scores"",\n                                ""Represents end scores for each support sequence"",\n                                ""[S, max_num_tokens]"")\n\n        answer_span = TensorPort(np.int32, [None, 3], ""answer_span"",\n                                 ""Represents answer as a (doc_idx, start, end) span"", ""[A, 3]"")\n\n        # generative QA\n        generative_symbol_scores = TensorPort(np.int32, [None, None, None], ""symbol_scores"",\n                                              ""Represents symbol scores for each possible ""\n                                              ""sequential answer given during training"",\n                                              ""[A, max_num_tokens, vocab_len]"")\n\n        generative_symbols = TensorPort(np.int32, [None, None], ""symbol_prediction"",\n                                        ""Represents symbol sequence for each possible ""\n                                        ""answer target_indexpredicted by the model"",\n                                        ""[A, max_num_tokens]"")\n\n    class Target:\n        candidate_1hot = TensorPort(np.float32, [None, None], ""candidate_targets"",\n                                    ""Represents target (0/1) values for each candidate"",\n                                    ""[batch_size, num_candidates]"")\n\n        target_index = TensorPort(np.int32, [None], ""target_index"",\n                                  (""Represents symbol id of target candidate. "",\n                                   ""This can either be an index into a full list of candidates,"",\n                                   "" which is fixed, or an index into a partial list of "",\n                                   ""candidates, for example a list of potential entities "",\n                                   ""from a list of many candidates""),\n                                  ""[batch_size]"")\n\n        answer_span = TensorPort(np.int32, [None, 2], ""answer_span_target"",\n                                 ""Represents answer as a (start, end) span"", ""[A, 2]"")\n\n        seq_answer = TensorPort(np.int32, [None, None], ""answer_seq_target"",\n                                ""Represents answer as a sequence of symbols"",\n                                ""[A, max_num_tokens]"")\n\n        symbols = TensorPort(np.int32, [None, None], ""symbol_targets"",\n                             ""Represents symbols for each possible target answer sequence"",\n                             ""[A, max_num_tokens]"")\n\n\nclass TensorPortTensors:\n    """"""\n    This class wraps around mappings from tensor ports to tensors and makes the tensors available by\n    by `x.foo` instead of `x[\'foo\']` calls.\n    """"""\n\n    def __init__(self, mapping: Mapping[TensorPort, Any]):\n        """"""\n        Create a wrapping based on the passed in mapping/dictionary.\n        Args:\n            mapping: Mapping from ports to tensors.\n        """"""\n        self.name_to_tensor = ({key.name: value for key, value in mapping.items()})\n\n    def __getattr__(self, item):\n        """"""\n        Returns the tensor belonging to the port with the given name.\n        Args:\n            item: the tensor port name.\n\n        Returns: the tensor associated with the tensor port of the given name.\n        """"""\n        return self.name_to_tensor[item]\n'"
jack/core/torch.py,0,"b'import logging\nimport sys\nfrom abc import abstractmethod\nfrom typing import Mapping, List, Iterable, Tuple\n\nimport numpy as np\nimport torch\nfrom torch import nn\n\nfrom jack.core import ModelModule, SharedResources, TensorPort\nfrom jack.core import reader\nfrom jack.core.data_structures import Answer\nfrom jack.core.data_structures import QASetting\nfrom jack.core.tensorport import Ports\n\nlogger = reader.logger\n\n\nclass PyTorchModelModule(ModelModule):\n    """"""This class represents an  abstract ModelModule for PyTorch models.\n\n    It requires the implementation of 2 nn.modules that create predictions and the training outputs for the defined\n    the ports.\n    """"""\n\n    def __init__(self, shared_resources: SharedResources):\n        self.shared_resources = shared_resources\n        # will be set in setup later\n        self._prediction_module = None\n        self._loss_module = None\n\n    def __call__(self, batch: Mapping[TensorPort, np.ndarray],\n                 goal_ports: List[TensorPort] = None) -> Mapping[TensorPort, np.ndarray]:\n        """"""Runs a batch and returns values/outputs for specified goal ports.\n        Args:\n            batch: mapping from ports to values\n            goal_ports: optional output ports, defaults to output_ports of this module will be returned\n\n        Returns:\n            A mapping from goal ports to tensors.\n        """"""\n        goal_ports = goal_ports or self.output_ports\n        inputs = [p.create_torch_variable(batch.get(p), gpu=torch.cuda.device_count() > 0) for p in self.input_ports]\n        outputs = self.prediction_module.forward(*inputs)\n        ret = {p: p.torch_to_numpy(t) for p, t in zip(self.output_ports, outputs) if p in goal_ports}\n        for p in goal_ports:\n            if p not in ret and p in batch:\n                ret[p] = batch[p]\n        return ret\n\n    @abstractmethod\n    def create_prediction_module(self, shared_resources: SharedResources) -> nn.Module:\n        """"""Creates and returns a PyTorch nn.Module for computing predictions.\n\n        It takes inputs as defined by `input_ports` and produces  outputs as defined by `output_ports`""""""\n        raise NotImplementedError\n\n    @abstractmethod\n    def create_loss_module(self, shared_resources: SharedResources) -> nn.Module:\n        """"""Creates and returns a PyTorch nn.Module for computing output necessary for training, such as a loss.\n\n        It takes inputs as defined by `training_input_ports` and produces outputs as defined by\n        `training_output_ports`.""""""\n        raise NotImplementedError\n\n    @property\n    def prediction_module(self) -> nn.Module:\n        return self._prediction_module\n\n    @property\n    def loss_module(self) -> nn.Module:\n        return self._loss_module\n\n    def setup(self, is_training=True):\n        """"""Sets up the module.\n\n        This usually involves creating the actual tensorflow graph. It is expected to be called after the input module\n        is set up and shared resources, such as the vocab, config, etc., are prepared already at this point.\n        """"""\n        self._prediction_module = self.create_prediction_module(self.shared_resources)\n        self._loss_module = self.create_loss_module(self.shared_resources)\n        if torch.cuda.device_count() > 0:\n            self._prediction_module.cuda()\n            self._loss_module.cuda()\n\n    def store(self, path):\n        with open(path, \'wb\') as f:\n            torch.save({\'prediction_module\': self.prediction_module.state_dict(),\n                        \'loss_module\': self.loss_module.state_dict()}, f)\n\n    def load(self, path):\n        with open(path, \'rb\') as f:\n            d = torch.load(f)\n        self.prediction_module.load_state_dict(d[\'prediction_module\'])\n        self.loss_module.load_state_dict(d[\'loss_module\'])\n\n\nclass PyTorchReader(reader.JTReader):\n    """"""Tensorflow implementation of JTReader.\n\n    A tensorflow reader reads inputs consisting of questions, supports and possibly candidates, and produces answers.\n    It consists of three layers: input to tensor (input_module), tensor to tensor (model_module), and tensor to answer\n    (output_model). These layers are called in-turn on a given input (list).\n    """"""\n\n    @property\n    def model_module(self) -> PyTorchModelModule:\n        return super().model_module\n\n    def train(self, optimizer,\n              training_set: Iterable[Tuple[QASetting, List[Answer]]],\n              batch_size: int, max_epochs=10, hooks=tuple(), **kwargs):\n        """"""This method trains the reader (and changes its state).\n\n        Args:\n            optimizer: optimizer\n            training_set: the training instances.\n            batch_size: size of training batches\n            max_epochs: maximum number of epochs\n            hooks: TrainingHook implementations that are called after epochs and batches\n        """"""\n        logger.info(""Setting up data and model..."")\n        if not self._is_setup:\n            # First setup shared resources, e.g., vocabulary. This depends on the input module.\n            self.setup_from_data(training_set, is_training=True)\n        batches = self.input_module.batch_generator(training_set, batch_size, is_eval=False)\n        logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n        loss_idx = self.model_module.training_output_ports.index(Ports.loss)\n\n        logger.info(""Start training..."")\n        p_module = self.model_module.prediction_module\n        l_module = self.model_module.loss_module\n        for i in range(1, max_epochs + 1):\n            for j, batch in enumerate(batches):\n                for p, v in batch.items():\n                    if isinstance(p, TensorPort):\n                        batch[p] = p.create_torch_variable(v, gpu=torch.cuda.device_count() > 0)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n                pred_outputs = p_module.forward(\n                    *(batch[p] for p in self.model_module.input_ports))\n                batch.update(zip(self.model_module.output_ports, pred_outputs))\n                train_outputs = l_module.forward(\n                    *(batch[p] for p in self.model_module.training_input_ports))\n                current_loss = train_outputs[loss_idx]\n                current_loss.backward()\n                optimizer.step()\n\n                for hook in hooks:\n                    hook.at_iteration_end(i, current_loss.data[0], set_name=\'train\')\n\n            # calling post-epoch hooks\n            for hook in hooks:\n                hook.at_epoch_end(i)\n'"
jack/eval/__init__.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom jack.eval import extractive_qa, link_prediction, classification\nfrom jack.eval.base import evaluators, evaluate_reader, pretty_print_results\n'"
jack/eval/base.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom jack.eval import extractive_qa, link_prediction, classification\n\nevaluators = {\n    \'extractive_qa\': extractive_qa.evaluate,\n    \'link_prediction\': link_prediction.evaluate,\n    \'classification\': None\n}\n\n\ndef evaluate_reader(reader, dataset, batch_size):\n    from jack.readers.implementations import extractive_qa_readers, classification_readers, link_prediction_readers\n    reader_name = reader.shared_resources.config.get(\'reader\')\n    if reader_name in extractive_qa_readers:\n        return extractive_qa.evaluate(reader, dataset, batch_size)\n    elif reader_name in link_prediction_readers:\n        return link_prediction.evaluate(reader, dataset, batch_size)\n    elif reader_name in classification_readers:\n        return classification.evaluate(reader, dataset, batch_size)\n\n\ndef pretty_print_results(d, prefix=\'\'):\n    for k, v in sorted(d.items(), key=lambda x: x[0]):\n        if isinstance(v, dict):\n            print(prefix + k + "":"")\n            pretty_print_results(v, prefix + \'\\t\')\n        elif \'\\n\' in str(v):\n            print(prefix + k + "":"")\n            print(str(v).replace(\'\\n\', \'\\n\' + prefix + \'\\t\'))\n        else:\n            print(prefix + k + "":"", str(v))\n'"
jack/eval/classification.py,0,"b""from collections import defaultdict\n\n\ndef evaluate(reader, dataset, batch_size):\n    answers = reader.process_dataset(dataset, batch_size, silent=False)\n\n    confusion_matrix = defaultdict(lambda: defaultdict(int))\n\n    for (q, a), pa in zip(dataset, answers):\n        confusion_matrix[a[0].text][pa.text] += 1\n\n    classes = sorted(confusion_matrix.keys())\n    max_class = max(6, len(max(classes, key=len)))\n\n    precision = dict()\n    recall = dict()\n    f1 = dict()\n\n    confusion_matrix_string = ['\\n', ' ' * max_class]\n    for c in classes:\n        confusion_matrix_string.append('\\t')\n        confusion_matrix_string.append(c)\n        confusion_matrix_string.append(' ' * (max_class - len(c)))\n    confusion_matrix_string.append('\\n')\n    for c1 in classes:\n        confusion_matrix_string.append(c1)\n        confusion_matrix_string.append(' ' * (max_class - len(c1)))\n        for c2 in classes:\n            confusion_matrix_string.append('\\t')\n            ct = str(confusion_matrix[c1][c2])\n            confusion_matrix_string.append(ct)\n            confusion_matrix_string.append(' ' * (max_class - len(ct)))\n        confusion_matrix_string.append('\\n')\n        precision[c1] = confusion_matrix[c1][c1] / max(1.0, sum(p[c1] for p in confusion_matrix.values()))\n        recall[c1] = confusion_matrix[c1][c1] / max(1.0, sum(confusion_matrix[c1].values()))\n        f1[c1] = 2 * precision[c1] * recall[c1] / max(1.0, precision[c1] + recall[c1])\n\n    accuracy = sum(confusion_matrix[c][c] for c in classes) / max(\n        1.0, sum(sum(vs.values()) for vs in confusion_matrix.values()))\n\n    return {\n        'Accuracy': accuracy,\n        'Precision': precision,\n        'Recall': recall,\n        'F1': f1,\n        'Confusion Matrix': ''.join(confusion_matrix_string)\n    }\n"""
jack/eval/extractive_qa.py,0,"b'import logging\nimport re\nimport string\nfrom collections import Counter\n\nlogger = logging.getLogger(__name__)\n\n\ndef evaluate(reader, dataset, batch_size):\n    answers = reader.process_dataset(dataset, batch_size, silent=False)\n\n    f1 = exact_match = 0\n    for pa, (q, ass) in zip(answers, dataset):\n        ground_truth = [a.text for a in ass]\n        f1 += metric_max_over_ground_truths(f1_score, pa.text, ground_truth)\n        exact_match += metric_max_over_ground_truths(exact_match_score, pa.text, ground_truth)\n\n    f1 /= len(answers)\n    exact_match /= len(answers)\n\n    return {\'F1\': f1, \'Exact\': exact_match}\n\n\ndef normalize_answer(s):\n    """"""Lower text and remove punctuation, articles and extra whitespace.""""""\n\n    def remove_articles(text):\n        return re.sub(r\'\\b(a|an|the)\\b\', \' \', text)\n\n    def white_space_fix(text):\n        return \' \'.join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \'\'.join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\n\ndef f1_score(prediction, ground_truth):\n    prediction_tokens = normalize_answer(prediction).split()\n    ground_truth_tokens = normalize_answer(ground_truth).split()\n    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(prediction_tokens)\n    recall = 1.0 * num_same / len(ground_truth_tokens)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\n\n\ndef exact_match_score(prediction, ground_truth):\n    return normalize_answer(prediction) == normalize_answer(ground_truth)\n\n\ndef metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n    scores_for_ground_truths = [0.0]\n    for ground_truth in ground_truths:\n        score = metric_fn(prediction, ground_truth)\n        scores_for_ground_truths.append(score)\n    return max(scores_for_ground_truths)\n'"
jack/eval/link_prediction.py,0,"b'# -*- coding: utf-8 -*-\n\nimport logging\n\nimport numpy as np\nimport progressbar\n\nfrom jack.core import QASetting\nfrom jack.io.load import loaders\n\nlogger = logging.getLogger(__name__)\n\n\ndef evaluate(reader, dataset, batch_size):\n    triples = {tuple(q.question.split()) for q, a in dataset if a[0].text == ""True""}\n    entity_set = reader.shared_resources.entity_to_index\n\n    conf = reader.shared_resources.config\n    all_triples = set()\n    if conf.get(\'train\'):\n        all_triples.update(tuple(qa.question.split()) for qa, a in loaders[conf[\'loader\']](conf[\'train\'])\n                           if a[0].text == ""True"")\n    if conf.get(\'dev\'):\n        all_triples.update(tuple(qa.question.split()) for qa, a in loaders[conf[\'loader\']](conf[\'dev\'])\n                           if a[0].text == ""True"")\n    if conf.get(\'test\'):\n        all_triples.update(tuple(qa.question.split()) for qa, a in loaders[conf[\'loader\']](conf[\'test\'])\n                           if a[0].text == ""True"")\n\n    def scoring_function(triples):\n        scores = []\n        for i in range(0, len(triples), batch_size):\n            batch_qas = [\n                QASetting("" "".join(triples[k]))\n                for k in range(i, min(len(triples), (i + batch_size)))]\n            if batch_qas:\n                for a in reader(batch_qas):\n                    scores.append(a[0].score)\n        return scores\n\n    ranks, filtered_ranks = compute_ranks(scoring_function, triples, entity_set, all_triples)\n    results = dict()\n    results[\'Unfiltered Results\'] = ranking_summary(ranks)\n    results[\'Filtered Results\'] = ranking_summary(filtered_ranks)\n    return results\n\n\ndef compute_ranks(scoring_function, triples, entity_set, true_triples=None):\n    subject_ranks, object_ranks = [], []\n    subject_ranks_filtered, object_ranks_filtered = [], []\n\n    bar = progressbar.ProgressBar(\n        max_value=len(triples),\n        widgets=[\' [\', progressbar.Timer(), \'] \', progressbar.Bar(), \' (\', progressbar.ETA(), \') \'])\n    for s, p, o in bar(triples):\n        subject_triples = [(s, p, o)] + [(x, p, o) for x in entity_set if x != s]\n        object_triples = [(s, p, o)] + [(s, p, x) for x in entity_set if x != o]\n\n        subject_triple_scores = np.array(scoring_function(subject_triples))\n        object_triple_scores = np.array(scoring_function(object_triples))\n\n        subject_rank = 1 + np.argsort(np.argsort(- subject_triple_scores))[0]\n        object_rank = 1 + np.argsort(np.argsort(- object_triple_scores))[0]\n\n        subject_ranks.append(subject_rank)\n        object_ranks.append(object_rank)\n\n        if true_triples is None:\n            true_triples = []\n\n        for idx, triple in enumerate(subject_triples):\n            if triple != (s, p, o) and triple in true_triples:\n                subject_triple_scores[idx] = - np.inf\n\n        for idx, triple in enumerate(object_triples):\n            if triple != (s, p, o) and triple in true_triples:\n                object_triple_scores[idx] = - np.inf\n\n        subject_rank_filtered = 1 + np.argsort(np.argsort(- subject_triple_scores))[0]\n        object_rank_filtered = 1 + np.argsort(np.argsort(- object_triple_scores))[0]\n\n        subject_ranks_filtered.append(subject_rank_filtered)\n        object_ranks_filtered.append(object_rank_filtered)\n\n    return (subject_ranks, object_ranks), (subject_ranks_filtered, object_ranks_filtered)\n\n\ndef ranking_summary(res, n=10, tag=None):\n    dres = {\'subject\': dict(), \'object\': dict(), \'all\': dict()}\n\n    dres[\'subject\'][\'mean_rank\'] = np.mean(res[0])\n    dres[\'subject\'][\'median_rank\'] = np.median(res[0])\n    dres[\'subject\'][\'hits@1\'] = np.mean(np.asarray(res[0]) <= 1)\n    dres[\'subject\'][\'hits@\' + str(n)] = np.mean(np.asarray(res[0]) <= n)\n    dres[\'object\'][\'mean_rank\'] = np.mean(res[1])\n    dres[\'object\'][\'median_rank\'] = np.median(res[1])\n    dres[\'object\'][\'hits@1\'] = np.mean(np.asarray(res[1]) <= 1)\n    dres[\'object\'][\'hits@\' + str(n)] = np.mean(np.asarray(res[1]) <= n)\n\n    resg = res[0] + res[1]\n\n    dres[\'all\'][\'mean_rank\'] = np.mean(resg)\n    dres[\'all\'][\'median_rank\'] = np.median(resg)\n    dres[\'all\'][\'hits@1\'] = np.mean(np.asarray(resg) <= 1)\n    dres[\'all\'][\'hits@\' + str(n)] = np.mean(np.asarray(resg) <= n)\n\n    dres[\'subject\'][\'mrr\'] = np.mean(1. / np.asarray(res[0]))\n    dres[\'object\'][\'mrr\'] = np.mean(1. / np.asarray(res[1]))\n    dres[\'all\'][\'mrr\'] = np.mean(1. / np.asarray(resg))\n\n    return dres\n'"
jack/io/CBT2jtr.py,0,"b'""""""\n\nHill, Felix, et al.\n""The Goldilocks Principle: Reading Children\'s Books with Explicit Memory Representations.""\narXiv preprint arXiv:1511.02301 (2015).\n\n\nOriginal paper: https://arxiv.org/abs/1511.02301\nData: https://research.fb.com/projects/babi/\nJTR download script: data/CBT/download.sh\n\n""""""\n\nimport json\nimport argparse\n\n\ndef __load_cbt_file(path=None, part=\'train\', mode=\'NE\'):\n    """"""\n    NOT USED RIGHT NOW\n\n    Path should be given and function will load raw data.\n    If it is no given however, there\'s three parts:\n    \'train\', \'valid\' and \'test\' as well as 5 modes.\n    The modes are:\n            - \'CN\' (predicting common nouns)\n            - \'NE\' (predicting named entities)\n            - \'P\'  (predicting prepositions)\n            - \'V\'  (predicting verbs.)\n            - \'all\'(all of the above four categories)\n    When calling this function both the dataset part and the mode have to\n    be specified.\n    \'all\' is not suitable for QA format, it seems to be just raw text.\n\n    Args:\n        path:\n        part:\n        mode:\n\n    Returns:\n\n    """"""\n    if path is None:\n        if mode == \'all\':\n            path += \'cbt_\' + part + \'.txt\'\n        else:\n            path += \'cbtest_\' + mode + \'_\' + part\n            if part == \'valid\':\n                path += \'_2000ex.txt\'\n            elif part == \'test\':\n                path += \'_2500ex.txt\'\n    with open(path, \'r\') as f:\n        data = f.read()\n    return data\n\n\ndef __split_cbt(raw_data, first_n=None):\n    """""" splits raw cbt data into parts corresponding to each instance """"""\n    story_instances = []\n    instance = []\n    for l in raw_data.split(\'\\n\')[:-1]:\n        if l == \'\':  # reset instance every time an empty line is encountered\n            story_instances.append(instance)\n            instance = []\n            continue\n        instance.append(l)\n    if first_n:\n        return story_instances[:first_n]\n    return story_instances\n\n\ndef __parse_cbt_example(instance):\n    support = question = answer = candidates_string = []\n    for line in instance:\n        line_number, line_content = line.split("" "", 1)\n        if int(line_number) < 21:    # line contains sentence\n            support.append(line_content)\n        else:\n            question, answer, candidates_string = line_content.split(\'\\t\', 2)\n    candidates_list = candidates_string.strip(\'\\t\').split(\'|\')\n    qdict = {\n        \'question\': question,\n        \'candidates\': [{\'text\': cand} for cand in candidates_list],\n        \'answers\': [{\'text\': answer}]\n    }\n    questions = [qdict]\n    qset_dict = {\n        \'suport\': [{\'text\': supp} for supp in support],\n        \'questions\': questions\n    }\n\n    return qset_dict\n\n\ndef create_jtr_snippet(path, n_instances=5):\n    """"""\n    Creates a jack format snippet.\n\n    Args:\n        path: path to the file\n        n_instances: number of instances\n\n    Returns: jack json\n\n    """"""\n    return convert_cbt(path, n_instances)\n\n\ndef convert_cbt(path, n_instances=None):\n    """"""\n    Convert the Children\'s Book Test file into jack format.\n\n    Args:\n        path: the file which should be converted\n        n_instances: how many instances to filter out\n    Returns: dictionary in jack format\n\n    """"""\n    # raw_data = __load_cbt_file(path)\n    with open(path, \'r\') as f:\n        raw_data = f.read()\n\n    instances = __split_cbt(raw_data, n_instances)\n\n    corpus = []\n    for inst in instances:\n        corpus.append(__parse_cbt_example(inst))\n\n    return {\n        \'meta\': \'Children\\\'s Book Test\',\n        \'globals\': {\'candidates\': []},\n        \'instances\': corpus\n    }\n\n\ndef main():\n    """"""\n        Main call function\n\n    Usage:\n        from other code:\n            call convert_cbt(filename)\n        from command line:\n            call with --help for help\n\n    Returns: nothing\n    """"""\n\n    parser = argparse.ArgumentParser(description=\'The Children\xe2\x80\x99s Book Test (CBT) dataset to jack format converter.\')\n    parser.add_argument(\'infile\',\n                        help=""path to the file to be converted (e.g. data/CBT/CBTest/data/cbtest_CN_train.txt)"")\n    parser.add_argument(\'outfile\',\n                        help=""path to the jack format -generated output file (e.g. data/CBT/train.jack.json)"")\n    parser.add_argument(\'-s\', \'--snippet\', action=""store_true"",\n                        help=""Export a snippet (first 5 instances) instead of the full file"")\n    args = parser.parse_args()\n\n    if args.snippet:\n        corpus = convert_cbt(args.infile, n_instances=5)\n    else:\n        corpus = convert_cbt(args.infile)\n    with open(args.outfile, \'w\') as outfile:\n        json.dump(corpus, outfile, indent=2)\n\nif __name__ == ""__main__"":\n    main()\n'"
jack/io/FB15K2jtr.py,0,"b'""""""\n\njtr converter for the fb15k dataset.\n\nBordes, Antoine, et al.\n""Translating embeddings for modeling multi-relational data.""\nAdvances in neural information processing systems. 2013.\n\nOriginal paper:\n        https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data\nData:   https://everest.hds.utc.fr/lib/exe/fetch.php?media=en:fb15k.tgz\nWeb:    https://everest.hds.utc.fr/doku.php?id=en:transe\nJTR download script: data/FB15k/download.sh\n\nMetadata:\n\nTraining data:\n    483142 triples (subject, relation, object)\n    14951 different entities\n    1345 different relation types\n\n""""""\n\nimport argparse\nimport gc\nimport json\nfrom collections import defaultdict\n\n\ndef load_fb15k_triples(path):\n    """"""\n    Loads the raw data from file provided.\n\n    Args:\n        path: path to the file\n\n    Returns: triples\n    """"""\n    with open(path, \'r\') as f:\n        triples = [line.strip(\'\\n\').split(\'\\t\') for line in f.readlines()]\n    return triples\n\n\ndef extract_unique_entities_and_relations(triples):\n    """"""\n    Identifies unique entities and relation types in collection of triples.\n\n    Args:\n        triples: List of string triples.\n\n    Returns:\n        unique_entities: List of strings\n        unique_relations: List of strings\n    """"""\n    s_entities = set([triple[0] for triple in triples])\n    o_entities = set([triple[2] for triple in triples])\n    r_types = set([triple[1] for triple in triples])\n\n    unique_relations = sorted(list(r_types))\n    unique_entities = sorted(list(s_entities | o_entities))  # union of sets\n\n    return unique_entities, unique_relations\n\n\ndef get_facts_per_entity(triples):\n    """"""\n    Obtain dictionary with all train fact ids that contain an entity.\n\n    Args:\n        triples: List of fact triples\n\n    Returns:\n        Dictionary entity --> fact IDs it participates in\n    """"""\n    d = defaultdict(set)\n    for i_triple, triple in enumerate(triples):\n        d[triple[0]].add(i_triple)\n        d[triple[2]].add(i_triple)\n    return d\n\n\ndef get_facts_per_relation(triples):\n    """"""\n    Obtain dictionary with all train fact ids that contain a relation type.\n\n    Args:\n        triples: List of fact triples\n\n    Returns:\n        Dictionary relation type --> fact IDs it participates in\n    """"""\n    d = defaultdict(set)\n    for i_triple, triple in enumerate(triples):\n        d[triple[1]].add(i_triple)\n    return d\n\n\ndef get_fact_neighbourhoods(triples, facts_per_entity, facts_per_relation,\n                            include_relations=False):\n    """"""\n    Extracts neighbouring facts for a collection of triples. neighbouring\n    facts of fact f are such facts that share at least an entity with f.\n    If relations are included, facts which share a relation are also considered\n    neighbours.\n\n    Args:\n        triples: list of facts triples\n        facts_per_entity: dictionary; The facts an entity appears in\n        facts_per_relation: dictionary; The facts a relation appears in\n        include_relations: boolean. whether facts sharing the relation should\n            be considered neighbours as well.\n\n    Returns:\n        fact_neighbourhoods: dictionary mapping fact ID to set of fact IDs.\n    """"""\n    fact_neighbourhoods = defaultdict(set)\n    for i_triple, triple in enumerate(triples):\n        # get triple ids which share subject, object or rel. with current triple\n        subject_neighbours = facts_per_entity[triple[0]]\n        object_neighbours = facts_per_entity[triple[2]]\n        relation_neighbours = set()\n        if include_relations:\n            relation_neighbours = facts_per_relation[triple[1]]\n\n        fact_neighbourhoods[i_triple].update(subject_neighbours)\n        fact_neighbourhoods[i_triple].update(object_neighbours)\n        fact_neighbourhoods[i_triple].update(relation_neighbours)\n\n    return fact_neighbourhoods\n\n\ndef convert_fb15k(triples, neighbourhoods):\n    """"""\n    Converts into jack format.\n    Args:\n        triples: fact triples that should be converted.\n        neighbourhoods: dictionary of supporting facts per triple\n\n    Returns:\n        jack formatted fb15k data.\n    """"""\n    instances = []\n    for i, triple in enumerate(triples):\n        if not i % 1000:\n            # print(i)\n            gc.collect()\n\n        # obtain supporting facts for this triple\n        neighbour_ids = neighbourhoods.get(i)\n        qset_dict = {}\n        if neighbour_ids:\n            neighbour_triples = [triples[ID] for ID in neighbour_ids]\n            qset_dict[\'support\'] = ["" "".join(t) for t in neighbour_triples]\n\n        qset_dict[\'questions\'] = [{\n            ""question"": "" "".join(triple),\n            ""answers"": [""True""]\n        }]\n        instances.append(qset_dict)\n\n    return {\n        \'meta\': \'FB15K-237 dataset.\',\n        \'instances\': instances\n    }\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'FB15K to jack format converter.\')\n    #\n    parser.add_argument(\'infile\',\n                        help=""dataset path you\'re interested in, train/dev/test.""\n                             ""(e.g. data/FB15k-237/Release/train.txt)"")\n    parser.add_argument(\'outfile\',\n                        help=""path to the jack format -generated output file (e.g. data/FB15K-237/FB15k_train.jack.json)"")\n    # parser.add_argument(\'dataset\', choices=[\'cnn\', \'dailymail\'],\n    #                     help=""which dataset to access: cnn or dailymail"")\n    parser.add_argument(\'--support\', default=\'\',\n                        help=""use training set path here (e.g. data/FB15k-237/Release/train.txt).""\n                             ""Default is not to create supporting facts."")\n    args = parser.parse_args()\n\n    print(""Loading data..."")\n    # load data from files into fact triples\n    triples = load_fb15k_triples(args.infile)\n\n    # get neighbouring facts for each fact in triples\n    if args.support:\n        print(""Creating fact neighbourhoods as support..."")\n        if args.infile == args.support:\n            reference_triples = triples\n        else:\n            reference_triples = load_fb15k_triples(args.support)\n        facts_per_entity = get_facts_per_entity(reference_triples)\n        facts_per_relation = get_facts_per_relation(reference_triples)\n        neighbourhoods = get_fact_neighbourhoods(triples, facts_per_entity, facts_per_relation)\n    else:\n        neighbourhoods = dict()\n\n    # dump the entity and relation ids for understanding the jack contents.\n    print(""Convert to json..."")\n    corpus = convert_fb15k(triples, neighbourhoods)\n    with open(args.outfile, \'w\') as outfile:\n        json.dump(corpus, outfile, indent=2)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
jack/io/MCTest2jtr.py,0,"b'""""""\n\nRichardson, Matthew, Christopher JC Burges, and Erin Renshaw.\n""MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text.""\nEMNLP. Vol. 3. 2013.\n\nOriginal paper: http://research.microsoft.com/en-us/um/redmond/projects/mctest/MCTest_EMNLP2013.pdf\nData:   research.microsoft.com/en-us/um/redmond/projects/mctest/\nJTR download script: data/MCTest/download.sh\n\n""""""\n\nimport json\nimport argparse\n\nlabels = [""A"", ""B"", ""C"", ""D""]\n\n\ndef __clean_mctest_text(text):\n    return text.replace(\'\\\\newline\', \'  \')\n\n\ndef create_jtr_snippet(tsv_file, ans_file, n_instances=5):\n    return convert_mctest(tsv_file, ans_file, n_instances)\n\n\ndef convert_mctest(tsv_file, ans_file, first_n=None):\n    with open(tsv_file) as tsv_data:\n        tsv_lines = tsv_data.readlines()\n    with open(ans_file) as ans_data:\n        ans_lines = ans_data.readlines()\n        instances = []\n    for tsv, ans in zip(tsv_lines, ans_lines):\n        instances.append(__parse_mctest_instance(tsv, ans))\n    if first_n:\n        instances = instances[:first_n]\n    return {\n        ""meta"": ""MCTest"",\n        ""instances"": instances\n    }\n\n\n\ndef __parse_mctest_instance(tsv_chunk, ans_chunk):\n    tsv_tabs = tsv_chunk.strip().split(\'\\t\')\n    ans_tabs = ans_chunk.strip().split(\'\\t\')\n    # id = tsv_tabs[0]\n    # ann = tsv_tabs[1]\n    passage = tsv_tabs[2]\n    # the dictionary for populating a set of passage/questions/answers\n    qset_dict = {\n        \'support\': [{\'text\': __clean_mctest_text(passage)}],\n        \'questions\': __parse_mctest_questions(tsv_tabs[3:], ans_tabs)\n    }\n    return qset_dict\n\n\ndef __parse_mctest_questions(question_list, ans_tabs):\n    # print(ans_tabs)\n    questions = []\n    for i in range(0, len(question_list), 5):\n        # qdict = {}\n        # parse answers\n        candidates = []\n        correct_answer = ans_tabs[int(i / 5)]\n        for j in range(1, 5):\n            label = labels[j-1]\n            answer = {\n                \'label\': label,\n                \'text\': question_list[i + j]\n            }\n            candidates.append(answer)\n        correct_index = labels.index(correct_answer)\n        answer = {\n            \'index\': correct_index,\n            \'text\': question_list[i + correct_index + 1]\n        }\n        # parse question\n        qcols = question_list[i].split(\':\')\n        qdict = {\n            \'question\': qcols[1],\n            \'candidates\': candidates,\n            \'answers\': [answer]\n        }\n        questions.append(qdict)\n    return questions\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Machine Comprehension Test (MCTest) dataset to jack format converter.\')\n    parser.add_argument(\'in_tsv\',\n                        help=""path to the MCTest tsv file (e.g. data/MCTest/MCTest/mc160.train.tsv)"")\n    parser.add_argument(\'in_ans\',\n                        help=""path to the MCTest ans file (e.g. data/MCTest/MCTest/mc160.train.ans)"")\n    parser.add_argument(\'outfile\',\n                        help=""path to the jack format -generated output file (e.g. data/MCTest/train.160.jack.json)"")\n    parser.add_argument(\'-s\', \'--snippet\', action=""store_true"",\n                        help=""Export a snippet (first 5 instances) instead of the full file"")\n    args = parser.parse_args()\n\n\n    if args.snippet:\n        corpus = convert_mctest(args.in_tsv, args.in_ans, 5)\n    else:\n        corpus = convert_mctest(args.in_tsv, args.in_ans)\n\n    with open(args.outfile, \'w\') as outfile:\n        json.dump(corpus, outfile, indent=2)\n\nif __name__ == ""__main__"":\n    main()\n'"
jack/io/NYT2jtr.py,0,"b'""""""\nThis files allows creating a jtr train and test datafiles for the NYT corpus\nbased on jtr/data/NYT/naacl2013.txt\n* train file: one jtr per train instance ; candidates = all entity pairs\n* test file: one jtr per test relation, with all correct answers in the valid answers list\n\nusage:\npython3 NYT2jtr path/to/naacl2013.txt mode >  naacl2013_mode.jtr.json\nin which mode = train or test\n""""""\n\nimport json\n\n\ndef load_naacl2013(path, mode):\n    assert mode in [\'train\', \'test\', \'Train\', \'Test\'], \'provide mode argument: train or test\'\n    facts = []\n    # load facts of type mode\n    with open(path) as fID:\n        for line in fID:\n            if len(line.strip()) > 0:\n                r, e1, e2, typ, truth = line.strip().split(\'\\t\')\n                rel = r\n                truth = {\'1.0\': True, \'0.0\': False}[truth]\n                tup = \'(%s|||%s)\' % (e1, e2)\n                if typ.lower() == mode.lower():\n                    facts.append(((rel, tup), truth, typ))\n\n    # global candidates: all entity tuples appearing in the training data\n    # (for model F, test data should have no tuples not occurring in training data)\n    tup_candidates_train = sorted(list(set([f[0][1] for f in facts if f[2].lower() == \'train\'])))\n    tup_candidates_test = sorted(list(set([f[0][1] for f in facts if f[2].lower() == \'test\'])))\n\n    if mode.lower() == \'train\':\n        return create_train_jtr(facts, tup_candidates_train)\n    elif mode.lower() == \'test\':\n        return create_test_jtr(facts, tup_candidates_test)\n\n\ndef create_train_jtr(trainfacts, tuples):\n    """"""\n    TODO comment\n    one jack per positive train fact\n\n    Args:\n        trainfacts:\n        tuples:\n\n    Returns:\n\n    """"""\n\n    instances = [{\n                     \'support\': [],\n                     \'questions\': [{\n                         \'question\': fact[0][0],\n                         \'candidates\': \'#/globals/candidates\',\n                         \'answers\': [{\n                             \'text\': fact[0][1]\n                         }]\n                     }]\n                     # only add true facts for training (data should be consistent with that)\n                  } for fact in trainfacts if fact[1]]\n\n    # @Johannes: originally we had \'instances\' as the jack format;\n    # now: added metadata field \'meta\' and \'globals\' field with the overall candidates.\n    return {\n        \'meta\': \'MFmodel-train\',\n        \'globals\': {\n            \'candidates\': [{\'text\': tup} for tup in tuples]},\n        \'instances\': instances\n    }\n\n\ndef create_test_jtr(testfacts, tuples):\n    """"""\n    TODO comment\n    one jack with all correct answers per relation in the test data\n\n    Args:\n        testfacts:\n        tuples:\n\n    Returns:\n\n    """"""\n    # map test relations to all true/false entity tuples\n    relmap = {}\n    for fact in testfacts:\n        rel, tup = fact[0]\n        truth = fact[1]\n        if not truth:\n            continue\n        if rel not in relmap:\n            relmap[rel] = {\n                \'candidates\': set(),\n                \'answers\': set()\n            }\n\n        # relmap[rel][\'candidates\'].add(tup)\n        relmap[rel][\'answers\'].add(tup)\n\n    # test instances:\n    instances = [{\n                     \'support\': [],\n                     \'questions\': [{\n                         \'question\': rel,\n                         \'candidates\': [{\'text\': c} for c in relmap[rel][\'candidates\']],\n                         \'answers\': [{\'text\': a} for a in relmap[rel][\'answers\']]\n                     }]\n                  } for rel in relmap]\n    return {\n        \'meta\': \'MFmodel-test\',\n        \'globals\': {\n            \'candidates\': [{\'text\': tup} for tup in tuples]},\n        \'instances\': instances\n          }\n\n\ndef main():\n    import sys\n    if len(sys.argv) == 4:\n        data = load_naacl2013(sys.argv[1], sys.argv[2])\n        with open(sys.argv[3], \'w\') as outfile:\n            json.dumps(data, outfile, indent=2)\n\n    else:\n        print(""""""Usage:\n    python3 NYT2jtr.py path/to/naacl2013 {mode} /save/to/naacl2013_mode.jack.json\n        where {mode} = {train, test}"""""")\n\nif __name__ == ""__main__"":\n    main()\n'"
jack/io/SNLI2jtr.py,0,"b'""""""\nThis files allows creating a jtr datafile for the SNLI corpus,\nwhereby each instance receives support under the form of \'related\' instances\n""""""\n\n\nimport json\n\n__candidate_labels = [\'entailment\', \'neutral\', \'contradiction\']\n__candidates = [{\'text\': cl} for cl in __candidate_labels]\n\n\ndef convert_snli(snli_file_jsonl):\n    """""" io SNLI files into jack format.\n    Data source: http://nlp.stanford.edu/projects/snli/snli_1.0.zip\n    Files to be converted: snli_1.0_dev.jsonl, snli_1.0_train.jsonl, snli_1.0_test.jsonl\n    (the *.txt files contain the same data in a different format)\n\n    Format:\n        - support = the premise = \'sentence1\' in original SNLI data (as id, use \'captionID\', the id of sentence1)\n        - question = the hypothesis = \'sentence2\' in original SNLI data\n    Notes:\n        - instances with gold labels \'-\' are removed from the corpus\n    """"""\n    with open(snli_file_jsonl, \'r\') as f:\n        data = [__convert_snli_instance(json.loads(line.strip())) for line in f.readlines()]\n\n        return {\'meta\': \'SNLI\',\n                \'globals\': {\'candidates\': __candidates},\n                \'instances\': [d for d in data if d]  # filter out invalid ones\n                }\n\n\ndef __convert_snli_instance(instance):\n    queb = {}\n    if instance[\'gold_label\'] in __candidate_labels:\n        queb[\'id\'] = instance[\'pairID\']\n        queb[\'support\'] = [\n            {\'id\': instance.get(\'captionID\'), \'text\': instance[\'sentence1\']}]\n        queb[\'questions\'] = [\n            {\'question\': instance[\'sentence2\'],\n             \'answers\': [{\'text\': instance[\'gold_label\']}]}]\n    return queb\n\n\ndef main():\n    import sys\n    if len(sys.argv) == 2:\n        corpus = convert_snli(sys.argv[1])\n    else:\n        for corpus_name in [""dev"", ""train"", ""test""]:\n            corpus = convert_snli(""./data/SNLI/snli_1.0/snli_1.0_%s.jsonl"" % corpus_name)\n            with open(""./data/SNLI/snli_1.0/snli_1.0_%s_jtr_v1.json"" % corpus_name, \'w\') as outfile:\n                print(""Create file snli_1.0_%s_jtr.json"" % corpus_name)\n                json.dump(corpus, outfile, indent=2)\n\n        # create train set test data\n        corpus = convert_snli(""./data/SNLI/snli_1.0/snli_1.0_train.jsonl"")\n        corpus[\'instances\'] = corpus[\'instances\'][:2000]\n        with open(""./tests/test_data/SNLI/2000_samples_train_jtr_v1.json"", \'w\') as outfile:\n            json.dump(corpus, outfile, indent=2)\n\n        corpus[\'instances\'] = corpus[\'instances\'][:100]\n        with open(""./tests/test_data/SNLI/overfit.json"", \'w\') as outfile:\n            json.dump(corpus, outfile, indent=2)\n\n        # create snippets and overfit test data\n        corpus[\'instances\'] = corpus[\'instances\'][:10]\n        with open(""./data/SNLI/snli_1.0/snli_1.0_debug_jtr_v1.json"", \'w\') as outfile:\n            json.dump(corpus, outfile, indent=2)\n        with open(""./data/SNLI/snippet.jtr_v1.json"", \'w\') as outfile:\n            json.dump(corpus, outfile, indent=2)\n\n        # create dev set test data\n        corpus = convert_snli(""./data/SNLI/snli_1.0/snli_1.0_dev.jsonl"")\n        corpus[\'instances\'] = corpus[\'instances\'][:1000]\n        with open(""./tests/test_data/SNLI/1000_samples_dev_jtr_v1.json"", \'w\') as outfile:\n            json.dump(corpus, outfile, indent=2)\n\n        # create dev set test data\n        corpus = convert_snli(""./data/SNLI/snli_1.0/snli_1.0_test.jsonl"")\n        corpus[\'instances\'] = corpus[\'instances\'][:2000]\n        with open(""./tests/test_data/SNLI/2000_samples_test_jtr_v1.json"", \'w\') as outfile:\n            json.dump(corpus, outfile, indent=2)\n\nif __name__ == ""__main__"":\n    main()\n'"
jack/io/SNLI2jtr_concat.py,0,"b'""""""\nThis files allows creating a jtr datafile for the SNLI corpus,\nwhereby\n- questions consist of premise + delim. + hypothesis\n- TODO: support facts can optinally be loaded from Wordnet / SNLItraining / PPDB\n""""""\n\n\nimport json\n\n__candidate_labels = [\'entailment\',\'neutral\',\'contradiction\']\n__candidates = [{\'text\':cl} for cl in __candidate_labels]\n\nCONJ = \'|||||\'\n\n\ndef convert_snli(snli_file_jsonl, support=False):\n    """""" io SNLI files into jack format.\n    Data source: http://nlp.stanford.edu/projects/snli/snli_1.0.zip\n    Files to be converted: snli_1.0_dev.jsonl, snli_1.0_train.jsonl, snli_1.0_test.jsonl\n    (the *.txt files contain the same data in a different format)\n\n    Format:\n        - question = the premise + CONJ + the hypothesis  = \'sentence1\' + CONJ + \'sentence2\'\n        - support instance = part1 + CONJ + part2 + CONJ + label\n    Notes:\n        - instances with gold labels \'-\' are removed from the corpus\n\n    Args:\n        snli_file_json: input file\n        support: False (no support), TODO: \'WordNet\', \'SNLItrain\', \'PPDB\'\n    """"""\n    assert \'snli_1.0\' in snli_file_jsonl and snli_file_jsonl.endswith(\'.jsonl\'), ""input should be the snli_1.0_X.jsonl files (X=test/train/dev)""\n\n    with open(snli_file_jsonl,\'r\') as f:\n        data = [__convert_snli_instance(json.loads(line.strip())) for line in f.readlines()]\n        instances = __add_support([d for d in data if d], support)\n\n        return {\'meta\': \'SNLI\',\n                \'globals\': {\'candidates\': __candidates},\n                \'instances\': instances  # filter out invalid ones\n                }\n\n\ndef __convert_snli_instance(instance):\n    try:\n        if not instance[\'gold_label\'] in __candidate_labels:\n            raise IOError(\'invalid gold label\')\n        queb = {}\n        queb[\'id\'] = instance[\'pairID\']\n        queb[\'support\'] = []\n        queb[\'questions\'] = [\n            {\'question\': instance[\'sentence1\'] + CONJ + instance[\'sentence2\'],\n             \'answers\': [\n                 {\'text\': __candidate_labels[__candidate_labels.index(instance[\'gold_label\'])]}]}]\n\n        return queb\n\n    except IOError:\n        return None\n\n\ndef __add_support(instances, support):\n    """"""\n    Args:\n        instances: list of jack instances (with or without support)\n        support: False (no support), TODO: \'WordNet\', \'SNLItrain\', \'PPDB\'\n    """"""\n\n    if support in [\'WordNet\', \'SNLItrain\', \'PPDB\']:\n        # TODO: add support\n        pass\n\n    return instances\n\n\ndef main():\n    import sys\n    if len(sys.argv) == 2:\n        corpus = convert_snli(sys.argv[1])\n        print(json.dumps(corpus, indent=2))\n    else:\n        for corpus_name in [""dev"",""train"",""test""]:\n            corpus = convert_snli(""./jack/data/SNLI/snli_1.0/snli_1.0_%s.jsonl"" % corpus_name, support=False)\n            with open(""./jack/data/SNLI/snli_1.0/snli_1.0_%s_jtr_v2.json"" % corpus_name, \'w\') as outfile:\n                json.dump(corpus, outfile, indent=2)\n\n        # create snippet\n        corpus = convert_snli(""./jack/data/SNLI/snli_1.0/snli_1.0_train.jsonl"", support=False)\n        corpus[\'instances\'] = corpus[\'instances\'][:10]\n        with open(""./jack/data/SNLI/snli_1.0/snli_1.0_debug_jtr_v2.json"", \'w\') as outfile:\n            json.dump(corpus, outfile, indent=2)\n        with open(""./jack/data/SNLI/snippet_jtrformat_v2.json"", \'w\') as outfile:\n            json.dump(corpus, outfile, indent=2)\n\nif __name__ == ""__main__"":\n    main()\n'"
jack/io/SQuAD2jtr.py,0,"b'""""""\n\nRajpurkar, Pranav, et al.\n""Squad: 100,000+ questions for machine comprehension of text.""\narXiv preprint arXiv:1606.05250 (2016).\n\nOriginal paper: https://arxiv.org/abs/1606.05250\nData: https://rajpurkar.github.io/SQuAD-explorer/\nJTR download script: data/SQuAD/download.sh\n\n""""""\n\nimport argparse\nimport json\n\n\ndef create_snippet(file_path):\n    """"""\n    Creates a snippet of the original SQuAD data.\n\n    Args:\n        file_path: path to the original file\n\n    Returns: string containing file contents\n    """"""\n    with open(file_path) as data_file:\n        data = json.load(data_file)[\'data\']\n        out = {\n            \'data\': [{\n                \'title\': data[0][\'title\'],\n                \'paragraphs\': [{\n                    \'context\': data[0][\'paragraphs\'][0][\'context\'],\n                    \'qas\': data[0][\'paragraphs\'][0][\'qas\'][0:3]\n                }]\n            }]\n        }\n        return json.dumps(out, indent=2)\n\n\ndef create_jtr_snippet(jtr_dict, num_instances=1):\n    """"""\n    Creates a jack format snippet from SQuAD data.\n\n    Args:\n        jtr_dict: jack dictionary\n        num_instances: number of (first) instances\n\n    Returns: dictionary in jack format\n    """"""\n    out = dict()\n    out[\'meta\'] = jtr_dict[\'meta\']\n    out[\'instances\'] = jtr_dict[\'instances\'][0:num_instances]\n    return out\n\n\ndef convert_squad(file_path):\n    """"""\n    Converts SQuAD dataset to jack format.\n\n    Args:\n        file_path: path to the SQuAD json file (train-v1.1.json and dev-v1.1.json in data/SQuAD/)\n\n    Returns: dictionary in jack format\n    """"""\n    # meta info\n    if \'/\' in file_path:\n        filename = file_path[file_path.rfind(\'/\')+1:]   # Maybe support a system-specific delimiter\n    else:\n        filename = file_path\n    # data\n    question_sets = []\n    with open(file_path) as data_file:\n        data = json.load(data_file)[\'data\']\n        for article in data:\n            for paragraph in article[\'paragraphs\']:\n                qa_set = {\n                    \'support\': [__parse_support(paragraph)],\n                    \'questions\': [__parse_question(qa_dict) for qa_dict in paragraph[\'qas\']]\n                }\n                question_sets.append(qa_set)\n    corpus_dict = {\n        \'meta\': {\n            \'source\': filename\n        },\n        \'instances\': question_sets\n    }\n    return corpus_dict\n\n\ndef __parse_support(para_dict):\n    return {\n            \'text\': para_dict[\'context\']\n    }\n\n\ndef __parse_question(qa_dict):\n    answers = [__parse_answer(answer_dict) for answer_dict in qa_dict[\'answers\']]\n    return {\n        \'question\': {\n            \'text\': qa_dict[\'question\'],\n            \'id\': qa_dict[\'id\']\n        },\n        \'answers\': answers\n    }\n\n\ndef __parse_answer(answer_dict):\n    answer_text = answer_dict[\'text\']\n    answer_start = answer_dict[\'answer_start\']\n    answer_end = answer_start + len(answer_text)\n    return {\n        \'text\': answer_text,\n        \'span\': (answer_start, answer_end),\n        \'doc_idx\': 0,  # in SQuAD there is always only a single document\n    }\n\n\ndef main():\n    """"""\n    Main call function\n\n    Usage:\n        from other code:\n            call convert_squad(filename)\n        from command line:\n            call with --help for help\n\n    Returns: nothing\n    """"""\n    parser = argparse.ArgumentParser(description=\'SQuAD dataset to jack format converter.\')\n    parser.add_argument(\'infile\',\n                        help=""path to the input file, original SQuAD file (e.g. data/SQuAD/train-v1.1.json)"")\n    parser.add_argument(\'outfile\',\n                        help=""path to the jack format -generated output file (e.g. data/SQuAD/train.jack.json)"")\n    parser.add_argument(\'-s\', \'--snippet\', action=""store_true"",\n                        help=""Export a snippet (first 5 instances) instead of the full file"")\n\n    args = parser.parse_args()\n    corpus = convert_squad(args.infile)\n    if args.snippet:\n        corpus = create_jtr_snippet(corpus, num_instances=5)\n\n    with open(args.outfile, \'w\') as outfile:\n        json.dump(corpus, outfile, indent=2)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
jack/io/WN182jtr.py,0,"b'""""""\n\njtr converter for the wn18 dataset.\n\nBordes, Antoine, et al.\n""Translating embeddings for modeling multi-relational data.""\nAdvances in neural information processing systems. 2013.\n\nOriginal paper:\n        https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data\nData:   https://everest.hds.utc.fr/lib/exe/fetch.php?media=en:wordnet-mlj12.tar.gz\nWeb:    https://everest.hds.utc.fr/doku.php?id=en:transe\nJTR download script: data/WN18/download.sh\n\nMetadata:\n\nTraining data:\n    151442 triples (subject, relation, object)\n    40943 different entities\n    18 different relation types\n\n""""""\n\nimport argparse\nimport json\nfrom collections import defaultdict\n\n\ndef load_wn18_triples(path):\n    """"""\n    Loads the raw data from file provided.\n\n    Args:\n        path: path to the file\n\n    Returns: triples\n    """"""\n    with open(path, \'r\') as f:\n        triples = [line.strip(\'\\n\').split(\'\\t\') for line in f.readlines()]\n    return triples\n\n\ndef extract_unique_entities_and_relations(triples):\n    """"""\n    Identifies unique entities and relation types in collection of triples.\n\n    Args:\n        triples: List of string triples.\n\n    Returns:\n        unique_entities: List of strings\n        unique_relations: List of strings\n    """"""\n    s_entities = set([triple[0] for triple in triples])\n    o_entities = set([triple[2] for triple in triples])\n    r_types = set([triple[1] for triple in triples])\n\n    unique_relations = sorted(list(r_types))\n    unique_entities = sorted(list(s_entities | o_entities))  # union of sets\n\n    return unique_entities, unique_relations\n\n\ndef get_facts_per_entity(triples):\n    """"""\n    Obtain dictionary with all train fact ids that contain an entity.\n\n    Args:\n        triples: List of fact triples\n\n    Returns:\n        Dictionary entity --> fact IDs it participates in\n    """"""\n    d = defaultdict(set)\n    for i_triple, triple in enumerate(triples):\n        d[triple[0]].add(i_triple)\n        d[triple[2]].add(i_triple)\n    return d\n\n\ndef get_facts_per_relation(triples):\n    """"""\n    Obtain dictionary with all train fact ids that contain a relation type.\n\n    Args:\n        triples: List of fact triples\n\n    Returns:\n        Dictionary relation type --> fact IDs it participates in\n    """"""\n    d = defaultdict(set)\n    for i_triple, triple in enumerate(triples):\n        d[triple[1]].add(i_triple)\n    return d\n\n\ndef get_fact_neighbourhoods(triples, facts_per_entity, facts_per_relation,\n                            include_relations=False):\n    """"""\n    Extracts neighbouring facts for a collection of triples. neighbouring\n    facts of fact f are such facts that share at least an entity with f.\n    If relations are included, facts which share a relation are also considered\n    neighbours.\n\n    Args:\n        triples: list of facts triples\n        facts_per_entity: dictionary; The facts an entity appears in\n        facts_per_relation: dictionary; The facts a relation appears in\n        include_relations: boolean. whether facts sharing the relation should\n            be considered neighbours as well.\n\n    Returns:\n        fact_neighbourhoods: dictionary mapping fact ID to set of fact IDs.\n    """"""\n    fact_neighbourhoods = defaultdict(set)\n    for i_triple, triple in enumerate(triples):\n        # get triple ids which share subject, object or rel. with current triple\n        subject_neighbours = facts_per_entity[triple[0]]\n        object_neighbours = facts_per_entity[triple[2]]\n        relation_neighbours = set()\n        if include_relations:\n            relation_neighbours = facts_per_relation[triple[1]]\n\n        fact_neighbourhoods[i_triple].update(subject_neighbours)\n        fact_neighbourhoods[i_triple].update(object_neighbours)\n        fact_neighbourhoods[i_triple].update(relation_neighbours)\n\n    return fact_neighbourhoods\n\n\ndef convert_wn18(triples, neighbourhoods):\n    """"""\n    Converts into jack format.\n    Args:\n        triples: fact triples that should be converted.\n        neighbourhoods: dictionary of supporting facts per triple\n\n    Returns:\n        jack formatted wn18 data.\n    """"""\n    # figure out cases with multiple possible true answers\n    instances = []\n    for i, triple in enumerate(triples):\n        # correct answers for this (s,r,.) case\n        qset_dict = {}\n\n        # obtain supporting facts for this triple\n        neighbour_ids = neighbourhoods.get(i)\n        if neighbour_ids:\n            neighbour_triples = [triples[ID] for ID in neighbour_ids]\n            qset_dict[\'support\'] = ["" "".join(t) for t in neighbour_triples]\n\n        qset_dict[\'questions\'] = [{\n            ""question"": "" "".join([str(triple[0]), str(triple[1]), str(triple[2])]),  # subject relation object\n            ""answers"": [""True""]\n        }]\n        instances.append(qset_dict)\n\n    return {\n        \'meta\': \'WN18 with entity neighbours as supporting facts.\',\n        \'instances\': instances\n    }\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'WN18 dataset to jack format converter.\')\n    #\n    parser.add_argument(\'infile\',\n                        help=""dataset path you\'re interested in, train/dev/test.""\n                             ""(e.g. data/FB15k-237/Release/train.txt)"")\n    parser.add_argument(\'outfile\',\n                        help=""path to the jack format -generated output file (e.g. data/FB15K-237/FB15k_train.jack.json)"")\n    # parser.add_argument(\'dataset\', choices=[\'cnn\', \'dailymail\'],\n    #                     help=""which dataset to access: cnn or dailymail"")\n    parser.add_argument(\'--support\', default=\'\',\n                        help=""use training set path here (e.g. data/FB15k-237/Release/train.txt).""\n                             ""Default is not to create supporting facts."")\n    args = parser.parse_args()\n\n    print(""Loading data..."")\n    # load data from files into fact triples\n    triples = load_wn18_triples(args.infile)\n\n    # get neighbouring facts for each fact in triples\n    if args.support:\n        print(""Creating fact neighbourhoods as support..."")\n        if args.infile == args.support:\n            reference_triples = triples\n        else:\n            reference_triples = load_wn18_triples(args.support)\n        facts_per_entity = get_facts_per_entity(reference_triples)\n        facts_per_relation = get_facts_per_relation(reference_triples)\n        neighbourhoods = get_fact_neighbourhoods(triples, facts_per_entity, facts_per_relation)\n    else:\n        neighbourhoods = dict()\n\n    # dump the entity and relation ids for understanding the jack contents.\n    print(""Convert to json..."")\n    corpus = convert_wn18(triples, neighbourhoods)\n    with open(args.outfile, \'w\') as outfile:\n        json.dump(corpus, outfile, indent=2)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
jack/io/__init__.py,0,b''
jack/io/bAbI2JTR.py,0,"b'""""""\n\nWeston et al. 2015\n""Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks""\narXiv preprint https://arxiv.org/abs/1502.05698.\n\nData: https://research.fb.com/projects/babi/\nJTR download script: data/bAbI/download.sh\n\n""""""\n\nimport json\nimport argparse\n\n# adapted from https://github.com/YerevaNN/Dynamic-memory-networks-in-Theano/\ndef load_babi_task(filename):\n    """"""\n    loads the babi data from its original format.\n    """"""\n    tasks = []\n    task = None\n    for i, line in enumerate(open(filename, \'r\')):\n        ID = int(line[0:line.find(\' \')])\n\n        # new data example\n        if ID == 1:\n            current_story = []\n            counter = 0\n            id_map = {}\n\n        line = line.strip()\n        line = line.replace(\'.\', \' . \')\n        line = line[line.find(\' \')+1:]\n\n        if \'\\t\' not in line:\n            # if line is not a question\n            current_story.append(line)\n            id_map[ID] = counter\n            counter += 1\n        else:\n            # if the line is a question\n            idx = line.find(\'?\')\n            tmp = line[idx+1:].split(\'\\t\')\n            question = line[:idx]\n            answer = tmp[1].strip()\n            # copy by value.\n            this_task = {""Story"": [x for x in current_story], ""Question"": question, ""Answer"": answer}\n            tasks.append(this_task)\n\n    return tasks\n\n\n\ndef single_babi_example_in_JTR_format(instance):\n    candidates_list = []\n    qdict = {\n        \'question\': instance[\'Question\'],\n        \'candidates\': [{\'text\': cand} for cand in candidates_list],\n        \'answers\': [{\'text\': instance[\'Answer\']}]\n    }\n    questions = [qdict]\n    qset_dict = {\n        \'support\': [{\'text\': supp} for supp in instance[""Story""]],\n        \'questions\': questions\n    }\n\n    return qset_dict\n\n\n\ndef convert_babi(path, n_instances=None):\n    """"""\n    Convert the babi data into jack format.\n\n    Args:\n        path: the file which should be converted\n        n_instances: how many instances to filter out\n    Returns: dictionary in jack format\n\n    """"""\n    # load data, select only first few if required.\n    babi_data = load_babi_task(path)\n    if n_instances != None:\n        babi_data = babi_data[:n_instances]\n\n\n    corpus = []\n    for instance in babi_data:\n        corpus.append(single_babi_example_in_JTR_format(instance))\n\n    return {\n        \'meta\': \'bAbI\',\n        \'globals\': {\'candidates\': []},\n        \'instances\': corpus\n    }\n\n\ndef main():\n    """"""\n        Main call function\n\n    Usage:\n        from command line:\n            call with --help for help\n\n    Returns: nothing\n    """"""\n\n    parser = argparse.ArgumentParser(description=\'The bAbI dataset to jack format converter.\')\n    parser.add_argument(\'infile\',\n                        help=""path to the file to be converted (e.g. data/bAbI/tasks_1-20_v1-2/en/qa2_two-supporting-facts_train.txt)"")\n    parser.add_argument(\'outfile\',\n                        help=""path to the jack format -generated output file (e.g. data/bAbI/jtr_format/train.jack.json)"")\n    parser.add_argument(\'-s\', \'--snippet\', action=""store_true"",\n                        help=""Export a snippet (first 5 instances) instead of the full file"")\n    args = parser.parse_args()\n\n    if args.snippet:\n        jtr_corpus = convert_babi(args.infile, n_instances=4)\n    else:\n        jtr_corpus = convert_babi(args.infile)\n    with open(args.outfile, \'w\') as outfile:\n        json.dump(jtr_corpus, outfile, indent=2)\n\nif __name__ == ""__main__"":\n    main()\n'"
jack/io/load.py,0,"b'""""""Implementation of loaders for common datasets.""""""\n\nimport json\n\nfrom jack.core.data_structures import *\nfrom jack.io.SNLI2jtr import convert_snli\nfrom jack.io.SQuAD2jtr import convert_squad\n\nloaders = dict()\n\n\ndef _register(name):\n    def _decorator(f):\n        loaders[name] = f\n        return f\n\n    return _decorator\n\n\n@_register(\'jack\')\ndef load_jack(path, max_count=None):\n    """"""\n    This function loads a jack json file from a specific location.\n    Args:\n        path: the location to load from.\n        max_count: how many instances to load at most\n\n    Returns:\n        A list of input-answer pairs.\n\n    """"""\n    # We load json directly instead\n    with open(path) as f:\n        jtr_data = json.load(f)\n\n    return jack_to_qasetting(jtr_data, max_count)\n\n\n@_register(\'squad\')\ndef load_squad(path, max_count=None):\n    """"""\n    This function loads a squad json file from a specific location.\n    Args:\n        path: the location to load from.\n        max_count: how many instances to load at most\n\n    Returns:\n        A list of input-answer pairs.\n    """"""\n    # We load to jtr dict and convert to qa settings for now\n    jtr_data = convert_squad(path)\n    return jack_to_qasetting(jtr_data, max_count)\n\n\n@_register(\'snli\')\ndef load_snli(path, max_count=None):\n    """"""\n    This function loads a jack json file with labelled answers from a specific location.\n    Args:\n        path: the location to load from.\n        max_count: how many instances to load at most\n\n    Returns:\n        A list of input-answer pairs.\n    """"""\n    # We load to jtr dict and convert to qa settings for now\n    jtr_data = convert_snli(path)\n    return jack_to_qasetting(jtr_data, max_count)\n'"
jack/io/ls2jtr.py,0,"b'""""""\nThis script converts data from the SemEval-2007 Task 10 on English Lexical\nSubstitution to the jtr format.\n""""""\n\nimport json\nimport xmltodict\nimport re\nimport os\n\n\ndef load_substitituons(path):\n    subs = {}\n    with open(path, ""r"") as f:\n        for line in f.readlines()[1:]:\n            splits = line.split("" :: "")\n            id = splits[0].split("" "")[1]\n            sub = [x[:-2] for x in splits[1].split("";"")][:-1]\n            print(id, sub)\n            subs[id] = sub\n    return subs\n\n\nif __name__ == ""__main__"":\n    CLOZE_STYLE = False\n\n    for corpus_name in [""trial""]:\n        file_path = ""./jack/data/LS/%s/lexsub_%s_cleaned.xml"" % (\n        corpus_name, corpus_name)\n        subs_path = ""./jack/data/LS/%s/gold.%s"" % (corpus_name, corpus_name)\n        subs = load_substitituons(subs_path)\n\n        with open(file_path) as fd:\n            file_text = fd.read().replace(""&"", ""&#038;"")\n            corpus = xmltodict.parse(file_text)[""corpus""]\n\n            jtr = {""meta"": ""SemEval-2007 Task 10: Lexical Substitution""}\n\n            instances = []\n\n            for lexelt in corpus[""lexelt""]:\n                for instance in lexelt[""instance""]:\n                    # fixme: not sure what is happening here\n                    if str(instance) != ""@id"" and str(instance) != ""context"":\n                        context = instance[""context""]\n                        id = instance[""@id""]\n                        word = re.search(\'_START_\\w+_END_\', context).group(0)[\n                               7:-5]\n                        context_masked = re.sub(\'_START_\\w+_END_\', \'XXXXX\',\n                                                context)\n                        context_recovered = re.sub(\'_START_\\w+_END_\', word,\n                                                   context)\n                        context_tokenized = context.split("" "")\n                        word_position = [i for i, word in enumerate(context_tokenized) if word.startswith(\'_START_\')][0]\n\n                        # print(""%s\\t%s\\t%s"" % (id, word, context_masked))\n\n                        if CLOZE_STYLE:\n                            queb = {\'id\': id, \'support\': [], \'questions\': [\n                                {\'question\': context_masked,\n                                 \'answers\': [\n                                     {\'text\': word}\n                                 ]}\n                            ]}\n                        else:\n                            queb = {\'id\': id,\n                                    \'support\': [{\'text\': context_recovered}],\n                                    \'questions\': [\n                                        {\'question\': str(word_position),\n                                         \'answers\': [\n                                             {\'text\': sub} for sub in subs[id]\n                                         ]}\n                                    ]}\n\n                        instances.append(queb)\n\n            jtr[""instances""] = instances\n\n            with open(""./jack/data/LS/%s/lexsub_%s_cleaned.jsonl"" % \\\n                              (corpus_name, corpus_name), \'w\') as outfile:\n                json.dump(jtr, outfile, indent=2)\n\n            # create snippet\n            jtr[\'instances\'] = jtr[\'instances\'][:10]\n\n\n            def save_debug(directory_path, file_name):\n                if not os.path.exists(directory_path):\n                    os.makedirs(directory_path)\n\n                with open(directory_path + ""/"" + file_name, \'w\') as outfile:\n                    json.dump(jtr, outfile, indent=2)\n\n\n            save_debug(""./data/LS/debug"", ""lexsub_debug_cleaned.jsonl"")\n            save_debug(""./data/LS"", ""snippet.jack.json"")\n'"
jack/io/merge_JTR_data_files.py,0,"b'""""""\nThis files merges two data files, both in JTR format, into a single JTR data file.\nIt assumes that the structure of instances is identical for both input files\nand only concatenates the two instances lists.\nIt also assumes that the global variables are identical in both input files.\n""""""\n\nimport json\nimport sys\n\n\ndef main():\n\n    if len(sys.argv) != 4:\n        print(\'Wrong arguments for merging two data files in Jack format into one. Usage:\')\n        print(\'\\tpython3 merge_JTR_data_files.py input1.json input2.json output.json\')\n    else:\n        # load input 1\n        with open(sys.argv[1], \'r\') as inputfile1:\n            content1 = json.load(inputfile1)\n\n        # load input 2\n        with open(sys.argv[2], \'r\') as inputfile2:\n            content2 = json.load(inputfile2)\n\n        # define new \'meta\' field\n        meta_ = ""Merged Content of {} and {}"".format(content1[\'meta\'], content2[\'meta\'])\n\n        # define new \'globals\' field. Note: so far assuming same globals in both input files.\n        assert (content1[\'globals\']) == content2[\'globals\']\n        globals_ = content1[\'globals\']\n\n        # concatenating instances of both input files\n        instances_ = content1[\'instances\'] + content2[\'instances\']\n\n        # defining the dictionary for dumping into json\n        merged_content = {\'meta\': meta_, \'globals\': globals_, \'instances\': instances_}\n\n        # sanity check: nothing unexpected got lost or added\n        assert len(content1[\'instances\']) + len(content2[\'instances\']) == len(merged_content[\'instances\'])\n\n        # summary print\n        print(\'Merged file {} with {} into {}\'.format(sys.argv[1],sys.argv[2],sys.argv[3]))\n        print(\'Number of instances: input1: {} input2: {} output: {}\'\\\n            .format(len(content1[\'instances\']), len(content2[\'instances\']), len(merged_content[\'instances\'])))\n\n        # dump merged content into JTR output file.\n        with open(sys.argv[3], \'w\') as outputfile:\n            json.dump(merged_content, outputfile)\n\n\n\nif __name__ == ""__main__"":\n    main()\n'"
jack/io/multiNLI2jtr.py,0,"b'""""""\nThis files allows creating a jtr datafile for the SNLI corpus,\nwhereby each instance receives support under the form of \'related\' instances\n""""""\n\n\nimport json\n\n__candidate_labels = [\'entailment\', \'neutral\', \'contradiction\']\n__candidates = [{\'text\': cl} for cl in __candidate_labels]\n\n\ndef convert_snli(multisnli_file):\n    """""" io SNLI files into jack format.\n    Data source: http://nlp.stanford.edu/projects/snli/snli_1.0.zip\n    Files to be converted: snli_1.0_dev.jsonl, snli_1.0_train.jsonl, snli_1.0_test.jsonl\n    (the *.txt files contain the same data in a different format)\n\n    Format:\n        - support = the premise = \'sentence1\' in original SNLI data (as id, use \'captionID\', the id of sentence1)\n        - question = the hypothesis = \'sentence2\' in original SNLI data\n    Notes:\n        - instances with gold labels \'-\' are removed from the corpus\n    """"""\n    assert \'multinli_0.9\' in multisnli_file and multisnli_file.endswith(\'.txt\'),\\\n        ""input should be the multinli_0.9_X.txt files (X=test/train/dev)""\n\n    with open(multisnli_file, \'r\') as f:\n        data = [__convert_snli_instance(line.strip().strip(""\\n"").split(""\\t"")) for line in f.readlines()]\n\n        return {\'meta\': \'MultiSNLI\',\n                \'globals\': {\'candidates\': __candidates},\n                \'instances\': [d for d in data if d]  # filter out invalid ones\n                }\n\n\ndef __convert_snli_instance(lspl):\n    if len(lspl) != 15:\n        return None\n    try:\n        gold_label, _, _, _, _, sentence1, sentence2, promptID, pairID, genre, _, _, _, _, _ = lspl\n        if gold_label == ""gold_label"":\n            return None\n\n        if not gold_label in __candidate_labels:\n            raise IOError(\'invalid gold label\')\n        queb = {}\n        queb[\'id\'] = pairID\n        queb[\'genre\'] = genre,\n        queb[\'support\'] = [\n            {\'id\': promptID, \'text\': sentence1}]\n        queb[\'questions\'] = [\n            {\'question\': sentence2,\n             \'answers\': [\n                 # {\'index\': __candidate_labels.index(instance[\'gold_label\'])},\n                 {\'text\': __candidate_labels[__candidate_labels.index(gold_label)]}]}]\n\n        return queb\n\n    except IOError:\n        return None\n\n\ndef main():\n    import sys\n    if len(sys.argv) == 2:\n        corpus = convert_snli(sys.argv[1])\n    else:\n        for corpus_name in [""dev_matched"", ""train""]:\n            corpus = convert_snli(""../../data/MultiNLI/multinli_0.9_%s.txt"" % corpus_name)\n            with open(""../../data/MultiNLI/multinli_0.9_%s_jtr.json"" % corpus_name, \'w\') as outfile:\n                print(""Create file snli_0.9_%s_jtr.txt"" % corpus_name)\n                json.dump(corpus, outfile, indent=2)\n\n        # create train set test data\n        corpus = convert_snli(""../../data/MultiNLI/multinli_0.9_train.txt"")\n        corpus[\'instances\'] = corpus[\'instances\'][:2000]\n        with open(""../../tests/test_data/MultiNLI/2000_samples_train_jtr.json"", \'w\') as outfile:\n            json.dump(corpus, outfile, indent=2)\n\n        corpus[\'instances\'] = corpus[\'instances\'][:100]\n        with open(""../../tests/test_data/MultiNLI/overfit.json"", \'w\') as outfile:\n            json.dump(corpus, outfile, indent=2)\n\n        # create snippets and overfit test data\n        corpus[\'instances\'] = corpus[\'instances\'][:10]\n        with open(""../../data/MultiNLI/multinli_0.9_debug_jtr.json"", \'w\') as outfile:\n            json.dump(corpus, outfile, indent=2)\n        with open(""../../data/MultiNLI/snippet.jack.json"", \'w\') as outfile:\n            json.dump(corpus, outfile, indent=2)\n\n        # create dev set test data\n        corpus = convert_snli(""../../data/MultiNLI/multinli_0.9_dev_matched.txt"")\n        corpus[\'instances\'] = corpus[\'instances\'][:1000]\n        with open(""../../tests/test_data/MultiNLI/1000_samples_dev_jtr.json"", \'w\') as outfile:\n            json.dump(corpus, outfile, indent=2)\n\n        # create dev set test data\n        corpus = convert_snli(""../../data/MultiNLI/multinli_0.9_train.txt"")\n        corpus[\'instances\'] = corpus[\'instances\'][:2000]\n        with open(""../../tests/test_data/MultiNLI/2000_samples_train_jtr.json"", \'w\') as outfile:\n            json.dump(corpus, outfile, indent=2)\n\n\nif __name__ == ""__main__"":\n    main()'"
jack/io/newsqa2jtr.py,0,"b'import argparse\nimport csv\nimport json\nfrom collections import Counter\n\n\ndef convert_newsqa(file_path):\n    """"""\n    Converts NewsQA dataset to jack format.\n\n    Args:\n        file_path: path to the NewsQA CSV file (data/NewsQA/)\n\n    Returns: dictionary in jack format\n    """"""\n    # meta info\n    if \'/\' in file_path:\n        filename = file_path[file_path.rfind(\'/\') + 1:]  # Maybe support a system-specific delimiter\n    else:\n        filename = file_path\n\n    # data\n    question_sets = []\n    with open(file_path) as data_file:\n        reader = csv.reader(data_file)\n        reader.__next__()\n        for row in reader:\n            [story_id, question, answer_char_ranges, is_answer_absent, is_question_bad, validated_answers,\n             story_text] = row\n\n            if validated_answers:\n                answers = json.loads(validated_answers)\n                spans = [k for k, v in answers.items() if "":"" in k]\n            else:\n                answers = Counter()\n                for rs in answer_char_ranges.split(""|""):\n                    for r in set(rs.split("","")):\n                        if "":"" in r:\n                            answers[r] += 1\n                spans = [k for k, v in answers.items() if "":"" in k and v >= 2]\n\n            if spans:\n                qa_set = {\n                    ""support"": [story_text],\n                    ""questions"": [{\n                        \'question\': {\n                            \'text\': question,\n                            \'id\': story_id + ""_"" + question.replace("" "", ""_"")\n                        },\n                        \'answers\': [{""span"": [int(span.split("":"")[0]), int(span.split("":"")[1])],\n                                     ""text"": story_text[int(span.split("":"")[0]):int(span.split("":"")[1])]\n                                     } for span in spans]\n                    }]\n                }\n                question_sets.append(qa_set)\n\n    corpus_dict = {\n        \'meta\': {\n            \'source\': filename\n        },\n        \'instances\': question_sets\n    }\n\n    return corpus_dict\n\n\ndef main():\n    """"""\n    Main call function\n\n    Usage:\n        from other code:\n            call convert_squad(filename)\n        from command line:\n            call with --help for help\n\n    Returns: nothing\n    """"""\n    parser = argparse.ArgumentParser(description=\'NewsQA dataset to jack format converter.\')\n    parser.add_argument(\'infile\',\n                        help=""path to the input file, original NewsQA file (e.g. data/NewsQA/newsqa/maluuba/newsqa/split_data/train.csv)"")\n    parser.add_argument(\'outfile\',\n                        help=""path to the jack format -generated output file (e.g. data/NewsQA/train.jack.json)"")\n\n    args = parser.parse_args()\n    corpus = convert_newsqa(args.infile)\n\n    with open(args.outfile, \'w\') as outfile:\n        json.dump(corpus, outfile, indent=2)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
jack/io/newsqa2squad.py,0,"b'import csv\nimport json\nimport sys\nfrom collections import Counter\n\ninput_fn = sys.argv[1]\noutput_fn = sys.argv[2]\n\ndataset = []\nsquad_style_dataset = {""data"": dataset, ""version"": ""1""}\n\nwith open(input_fn, ""r"") as f:\n    reader = csv.reader(f)\n    reader.__next__()\n    for row in reader:\n        [story_id, question, answer_char_ranges, is_answer_absent, is_question_bad, validated_answers, story_text] = row\n\n        spans = None\n        if validated_answers:\n            answers = json.loads(validated_answers)\n            spans = [k for k, v in answers.items() if "":"" in k]\n        else:\n            answers = Counter()\n            for rs in answer_char_ranges.split(""|""):\n                for r in set(rs.split("","")):\n                    if "":"" in r:\n                        answers[r] += 1\n            spans = [k for k, v in answers.items() if "":"" in k and v >= 2]\n\n        if spans:\n            example = {""title"": story_id, ""paragraphs"": [\n                {\n                    ""context"": story_text,\n                    ""qas"": [{\n                        ""question"": question,\n                        ""id"": story_id + ""_"" + question.replace("" "", ""_""),\n                        ""answers"": [{\n                                        ""answer_start"": int(span.split("":"")[0]),\n                                        ""text"": story_text[int(span.split("":"")[0]):int(span.split("":"")[1])]\n                                    } for span in spans]\n                    }]\n                }\n            ]}\n            dataset.append(example)\n            # else:\n            #    print(""No span found for %s"" % story_id)\n\nwith open(output_fn, ""w"") as f:\n    json.dump(squad_style_dataset, f)\n'"
jack/io/rc-data2jtr.py,0,"b'""""""\n\nHermann, Karl Moritz, et al.\n""Teaching machines to read and comprehend.""\nAdvances in Neural Information Processing Systems. 2015.\n\nOriginal paper: https://arxiv.org/abs/1506.03340\nData:   https://github.com/deepmind/rc-data\n        http://cs.nyu.edu/~kcho/DMQA/       (direct download)\nJTR download script: no download script, check README.md\n\nMetadata:\n\nNumber of questions:\n\n        CNN     DailyMail\n\ntrain   380298  879450\ndev     3924    64835\ntest    3198    53182\n\n""""""\n\nimport json\nimport os\nimport argparse\n\n\ndef create_jtr_snippet(directory, dataset, split, resolve_entities=False, num_instances=5):\n    """"""\n    Creates a jack format snippet from rc-data data.\n\n    Args:\n        directory: root directory of rc-data\n        dataset: which dataset, \'cnn\' of \'dailymail\'\n        split: \'train\', \'dev\' or \'test\'\n        resolve_entities: whether to de-anonymise entities. Default: False\n        num_instances: number of (first) instances\n\n    Returns:\n    """"""\n    return convert_rcdata(directory, dataset, split, resolve_entities, num_instances)\n\n\ndef convert_rcdata(directory, dataset, split, resolve_entities=False, first_n=None):\n    """"""\n    Convert subset of rc-data (definet by a combination of dataset and mode) to jtk format\n    Args:\n        directory: root directory of rc-data\n        dataset: which dataset, \'cnn\' of \'dailymail\'\n        split: \'train\', \'dev\' or \'test\'\n        resolve_entities: whether to de-anonymise entities. Default: False\n        first_n: export a snippet containing the first n instances of the dataset\n    Returns:\n        jack json\n\n    """"""\n    split_mapping = {\'train\': \'training\', \'test\': \'test\', \'dev\': \'validation\'}\n    assert split in split_mapping.keys()\n    assert dataset in {\'cnn\', \'dailymail\'}\n\n    if directory[-1] != \'/\':\n        directory += ""/""\n    directory += ""{0}/questions/{1}/"".format(dataset, split_mapping[split])\n    filenames = [file for file in os.listdir(directory) if file.endswith(\'question\')]\n\n    data = {}\n    i = 0\n    for fname in filenames:\n        with open(directory + fname, \'r\') as f:\n            url = f.readline().strip()\n            f.readline()\n            text = f.readline().strip()\n            f.readline()\n            cloze_q = f.readline().strip()\n            f.readline()\n            answer = f.readline().strip()\n            f.readline()\n            if resolve_entities:\n                entity_dict = {}\n                for line in f:\n                    split = line.rstrip().split(sep=\':\')\n                    if len(split) != 2:\n                        print(\'Bad split\')\n                    entity_dict[split[0]] = split[1]\n                for key, value in entity_dict.items():\n                    text = text.replace(key, value)\n                    cloze_q = cloze_q.replace(key, value)\n                    answer = answer.replace(key, value)\n\n            if url in data:\n                data[url][\'rest\'].append((fname, cloze_q, answer))\n            else:\n                data[url] = {\n                    \'text\': text,\n                    \'rest\': [(fname, cloze_q, answer)]\n                }\n        i += 1\n        if first_n and i == first_n:\n            break\n\n    instances = []\n    counter = 0\n    for k, v in data.items():\n        url = k\n        text = v[\'text\']\n        questions = []\n        for elem in v[\'rest\']:\n            fname, cloze_q, answer = elem\n            questions.append({\n                \'question\': cloze_q,\n                \'answers\': [{\n                    \'text\': answer\n                }],\n                \'id\': fname\n            })\n            counter += 1\n\n        instance = {\n            # \'id\': fname,\n            \'support\': {\n                ""id"": url,\n                ""text"": text\n            },\n            \'questions\': questions\n        }\n        instances.append(instance)\n\n    print(\' ...loaded {0} questions.\'.format(counter))\n    return {\n        ""meta"": ""{0}_{1}"".format(dataset, split),\n        ""instances"": instances\n    }\n\n\ndef main():\n    """"""\n    Main call function\n\n    Usage:\n        from other code:\n            call convert_rcdata(filename)\n        from command line:\n            call with --help for help\n\n    Returns: nothing\n    """"""\n    parser = argparse.ArgumentParser(description=\'rc-data datasets to jack format converter.\')\n    parser.add_argument(\'indir\',\n                        help=""path to the rc-data root directory (e.g. data/rc-data/)"")\n    parser.add_argument(\'outfile\',\n                        help=""path to the jack format -generated output file (e.g. data/rc-data/cnn_train.jack.json)"")\n    parser.add_argument(\'dataset\', choices=[\'cnn\', \'dailymail\'],\n                        help=""which dataset to access: cnn or dailymail"")\n    parser.add_argument(\'split\', choices=[\'train\', \'dev\', \'test\'],\n                        help=""which split of the dataset to io: train, dev or test"")\n    parser.add_argument(\'-s\', \'--snippet\', action=""store_true"",\n                        help=""Export a snippet (first 5 instances) instead of the full file"")\n    args = parser.parse_args()\n\n    if args.snippet:\n        corpus = create_jtr_snippet(args.indir, args.dataset, args.split, num_instances=5)\n    else:\n        corpus = convert_rcdata(args.indir, args.dataset, args.split)\n\n    with open(args.outfile, \'w\') as outfile:\n        json.dump(corpus, outfile, indent=2)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
jack/io/read_semeval2017Task10.py,0,"b'import os\n\n\ndef readAnn(textfolder=""../data/SemEval2017Task10/""):\n    \'\'\'\n    Read .ann files and look up corresponding spans in .txt files\n    \n    Args:\n        textfolder:\n    \'\'\'\n\n    flist = os.listdir(textfolder)\n    for f in flist:\n        if not f.endswith("".ann""):\n            continue\n\n        f_anno = open(os.path.join(textfolder, f), ""rU"")\n        f_text = open(os.path.join(textfolder, f.replace("".ann"", "".txt"")), ""rU"")\n\n        # there\'s only one line, as each .ann file is one text paragraph\n        for l in f_text:\n            text = l\n\n        #@TODO: collect all keyphrase and relation annotations, create pairs of all keyphrase that appear in same sentence for USchema style RE\n\n        for l in f_anno:\n            anno_inst = l.strip().split(""\\t"")\n            if len(anno_inst) == 3:\n                keytype, start, end = anno_inst[1].split("" "")\n                if not keytype.endswith(""-of""):\n\n                    # look up span in text and print error message if it doesn\'t match the .ann span text\n                    keyphr_text_lookup = text[int(start):int(end)]\n                    keyphr_ann = anno_inst[2]\n                    if keyphr_text_lookup != keyphr_ann:\n                        print(""Spans don\'t match for anno "" + l.strip() + "" in file "" + f)\n\n                #if keytype.endswith(""-of""):\n\n\nif __name__ == \'__main__\':\n    readAnn()'"
jack/io/scienceQA2jtr.py,0,"b'import json\nimport io\nimport random\n\ndef convert_scienceCloze_to_jtr(scienceQAFile):\n\n    instances = []\n\n    f = io.open(scienceQAFile, ""r"", encoding=""utf-8"")\n\n    for l in f:\n        l = l.strip().lower().split(""\\t"")  # do the lower case preprocessing here\n        try:\n            quest, answs, cands, context, contextID = l\n        except ValueError:\n            print(l)\n            continue\n\n        context = context[2:-2].split(\'\\\', \\\'\')\n\n        support = []\n        for i, c in enumerate(context):\n            support.append({""id"": contextID + ""_"" + str(i), ""text"": c})\n        candidates = cands[2:-2].split(\'\\\', \\\'\')\n\n        qdict = {\n            \'question\': quest,\n            \'candidates\': [\n                {\n                    \'text\': cand\n                } for cand in candidates\n                ],\n            \'answers\': [{\'text\': answs}]\n        }\n        qset_dict = {\n            \'support\': support,\n            \'questions\': [qdict]\n        }\n\n        instances.append(qset_dict)\n\n\n    instances.append(qset_dict)\n    random.shuffle(instances)\n\n    corpus_dict = {\n        \'meta\': ""scienceQA.json"",\n        \'instances\': instances\n    }\n\n    f.close()\n\n    return corpus_dict\n\n\n\nif __name__ == ""__main__"":\n    corpus = convert_scienceCloze_to_jtr(""../data/scienceQA/clozeSummaryLocal_test.txt"")\n    with open(""../data/scienceQA/scienceQA_clozeSummaryLocal_test.json"", \'w\') as outfile:\n        json.dump(corpus, outfile, indent=2, ensure_ascii=False)'"
jack/io/sentihood2jtr.py,0,"b'import json\nfrom collections import defaultdict\nimport sys\nimport os\n\n\ndef main():\n    # = parse_cbt_example(instances[0])\n    if len(sys.argv) == 2:\n        with open(sys.argv[1], \'r\') as f:\n            sentihood_data = json.load(f)\n\n        convert_to_jtr(sentihood_data)\n    elif len(sys.argv) ==1:\n        data_path = \'../data/sentihood/\'\n        filenames = [\'sentihood-train.json\', \'sentihood-dev.json\',\n        \'sentihood-test.json\']\n        for i, f in enumerate(filenames):\n            raw_data = json.load(open(os.path.join(data_path, f)))\n            instances = convert_to_jtr(raw_data)\n\n            if i == 0: # training data -> write overfit set\n                json.dump(wrap_into_jtr_global(instances[:100]),\n                        open(\'../../tests/test_data/sentihood/overfit.json\',\'w\'),\n                        indent=2)\n\n            # write data sets for smalldata tests\n            json.dump(wrap_into_jtr_global(instances[:1000]),\n                    open(os.path.join(\'../../tests/test_data/sentihood/\',f),\'w\'),\n                    indent=2)\n\ndef wrap_into_jtr_global(instances):\n    reading_dataset = {\n        \'globals\': {\n            \'candidates\': [\n                {\'text\': \'Negative\'},\n                {\'text\': \'Positive\'},\n                {\'text\': \'Neutral\'}\n            ]\n        },\n        \'instances\': instances\n    }\n    return reading_dataset\n\n\n\ndef convert_to_jtr(sentihood_data, exhaustive=True):\n    instances = []\n    # collect all aspects\n    aspects = set()\n    for instance in sentihood_data:\n        if \'opinions\' in instance.keys():\n            for opinion in instance[\'opinions\']:\n                aspects.add(opinion[\'aspect\'])\n    for instance in sentihood_data:\n        text = instance[\'text\']\n        answers = defaultdict(lambda: \'Neutral\')\n        if \'opinions\' in instance.keys():\n            for opinion in instance[\'opinions\']:\n                aspect = opinion[\'aspect\']\n                answers[aspect] = opinion[\'sentiment\']\n\n        for aspect in aspects if exhaustive else answers.keys():\n            reading_instance = {\n                \'support\': [{\'text\': text}],\n                \'questions\': [{\'question\': aspect, \'answers\': [{\'text\': answers[aspect]}]}]\n            }\n            instances.append(reading_instance)\n\n    return instances\n\n\nif __name__ == ""__main__"":\n    main()\n'"
jack/io/simpleQuestions2jtr.py,0,"b'import json\nimport io\n\n\ndef create_snippet(file_path, first_n=5):\n    with open(file_path, \'r\') as f:\n        return [next(f) for _ in range(first_n)]\n\n\ndef create_jtr_snippet(file_path):\n    return convert_simplequestions(file_path, first_n=5)\n\n\ndef convert_simplequestions(file_path, first_n=None):\n    instances = []\n    f = io.open(file_path, ""r"")\n    i = 0\n    for l in f:\n        i += 1\n        if first_n and i > first_n:\n            break\n        subj, rel, obj, qu = l.strip().split(""\\t"")\n\n        support = ["" "".join([subj, rel])]\n        qdict = {\n            \'question\': qu,\n            \'answers\': [obj]\n        }\n        qset_dict = {\n            \'support\': [{\'text\': supp} for supp in support],\n            \'questions\': [qdict]\n        }\n        instances.append(qset_dict)\n\n    corpus_dict = {\n        \'meta\': ""simpleQuestions.json"",\n        \'instances\': instances\n    }\n\n    f.close()\n\n    return corpus_dict\n\n\ndef main():\n    # some tests:\n    # raw_data = load_cbt_file(path=None, part=\'valid\', mode=\'NE\')\n    # instances = split_cbt(raw_data)\n    # = parse_cbt_example(instances[0])\n\n    import sys\n    if len(sys.argv) == 3:\n        # corpus = create_jtr_snippet(sys.argv[1])\n        # out = create_snippet(sys.argv[1])\n        # with open(sys.argv[2], \'w\') as outfile:\n        #     outfile.writelines(out)\n        corpus = convert_simplequestions(sys.argv[1])\n        with open(sys.argv[2], \'w\') as outfile:\n            json.dump(corpus, outfile, indent=2)\n    else:\n        print(""Usage: python3 simpleQuestions2jtr.py path/to/simpleQuestions save/to/simpleQuestions.jack.json"")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
jack/io/validate.py,0,"b""#!/usr/bin/env python3\n\nimport json\nimport jsonschema\nfrom sys import argv\n\ndef main(arg1, arg2):\n    with open(arg1) as f:\n        data = json.load(f)\n\n    with open(arg2) as f:\n        schema = json.load(f)\n\n    try:\n        jsonschema.validate(data, schema)\n        return 'JSON successfully validated.'\n    except jsonschema.ValidationError as e:\n        return e.message\n    except jsonschema.SchemaError as e:\n        return e\n\n\nif __name__ == '__main__':\n    response = main(argv[1], argv[2])\n    print(response)\n"""
jack/readers/__init__.py,0,b'from jack.readers.implementations import *\n'
jack/readers/implementations.py,0,"b'import os\nfrom typing import Union\n\nfrom jack.core.shared_resources import SharedResources\nfrom jack.core.tensorflow import TFReader\nfrom jack.util.hooks import XQAEvalHook, ClassificationEvalHook\n\nreaders = {}\neval_hooks = {}\n\nextractive_qa_readers = {}\nclassification_readers = {}\nnli_readers = {}\nlink_prediction_readers = {}\n\n\ndef __reader(f):\n    readers.setdefault(f.__name__, f)\n    return f\n\n\ndef extractive_qa_reader(f):\n    __reader(f)\n    extractive_qa_readers.setdefault(f.__name__, f)\n    eval_hooks.setdefault(f.__name__, XQAEvalHook)\n    return f\n\n\ndef classification_reader(f):\n    __reader(f)\n    classification_readers.setdefault(f.__name__, f)\n    eval_hooks.setdefault(f.__name__, ClassificationEvalHook)\n    return f\n\n\ndef nli_reader(f):\n    nli_readers.setdefault(f.__name__, f)\n    return classification_reader(f)\n\n\ndef link_prediction_reader(f):\n    from jack.util.hooks import LogProbEvalHook\n    __reader(f)\n    link_prediction_readers.setdefault(f.__name__, f)\n    eval_hooks.setdefault(f.__name__, LogProbEvalHook)\n    return f\n\n\ndef reader_from_file(load_dir: str, **kwargs):\n    """"""Loads a reader from a file.\n\n    Args:\n        load_dir: directory containing the reader being loaded.\n        kwargs: can be used to overwrite the reader configuration if necessary.\n\n    Returns:\n        the reader.\n    """"""\n    shared_resources = create_shared_resources()\n    shared_resources.load(os.path.join(load_dir, ""shared_resources""))\n    if kwargs:\n        shared_resources.config.update(kwargs)\n    reader = readers[shared_resources.config[""reader""]](shared_resources)\n    reader.load_and_setup_modules(load_dir)\n    return reader\n\n\ndef create_shared_resources(resources_or_config: Union[dict, SharedResources] = None) -> SharedResources:\n    """"""\n    Produces a SharedResources object based on the input.\n    Args:\n        resources_or_config: either nothing, a configuration dictionary, or shared resources\n\n    Returns: a SharedResources object.\n    """"""\n    if resources_or_config is None:\n        return SharedResources()\n    elif isinstance(resources_or_config, SharedResources):\n        return resources_or_config\n    else:\n        return SharedResources(config=resources_or_config)\n\n\n# Question Answering\n\n\ndef _tf_extractive_qa_reader(model_module_constructor, resources_or_conf: Union[dict, SharedResources]):\n    from jack.readers.extractive_qa.shared import XQAInputModule, XQAOutputModule\n    shared_resources = create_shared_resources(resources_or_conf)\n    input_module = XQAInputModule(shared_resources)\n    model_module = model_module_constructor(shared_resources)\n    output_module = XQAOutputModule()\n    return TFReader(shared_resources, input_module, model_module, output_module)\n\n\n@extractive_qa_reader\ndef fastqa_reader(resources_or_conf: Union[dict, SharedResources] = None):\n    """"""Creates a FastQA reader instance (extractive qa model).""""""\n    from jack.readers.extractive_qa.tensorflow.fastqa import FastQAModule\n    return _tf_extractive_qa_reader(FastQAModule, resources_or_conf)\n\n\n@extractive_qa_reader\ndef modular_qa_reader(resources_or_conf: Union[dict, SharedResources] = None):\n    """"""Creates a FastQA reader instance (extractive qa model).""""""\n    from jack.readers.extractive_qa.tensorflow.modular_qa_model import ModularQAModel\n    return _tf_extractive_qa_reader(ModularQAModel, resources_or_conf)\n\n\n@extractive_qa_reader\ndef fastqa_reader_torch(resources_or_conf: Union[dict, SharedResources] = None):\n    """""" Creates a FastQA reader instance (extractive qa model). """"""\n    from jack.readers.extractive_qa.torch.fastqa import FastQAPyTorchModelModule\n    from jack.readers.extractive_qa.shared import XQAInputModule, XQAOutputModule\n    from jack.core.torch import PyTorchReader\n    shared_resources = create_shared_resources(resources_or_conf)\n    input_module = XQAInputModule(shared_resources)\n    model_module = FastQAPyTorchModelModule(shared_resources)\n    output_module = XQAOutputModule()\n    return PyTorchReader(shared_resources, input_module, model_module, output_module)\n\n\n# Natural Language Inference\n\n\ndef _tf_nli_reader(model_module_constructor, resources_or_conf: Union[dict, SharedResources] = None):\n    from jack.readers.classification.shared import ClassificationSingleSupportInputModule\n    from jack.readers.classification.shared import SimpleClassificationOutputModule\n    shared_resources = create_shared_resources(resources_or_conf)\n    input_module = ClassificationSingleSupportInputModule(shared_resources)\n    model_module = model_module_constructor(shared_resources)\n    output_module = SimpleClassificationOutputModule(shared_resources)\n    return TFReader(shared_resources, input_module, model_module, output_module)\n\n\n@nli_reader\ndef dam_snli_reader(resources_or_conf: Union[dict, SharedResources] = None):\n    """"""Creates a NLI reader instance.\n    This particular reader uses a Decomposable Attention Model, as described in [1].\n    [1] Ankur P. Parikh et al. - A Decomposable Attention Model for Natural Language Inference. EMNLP 2016\n    """"""\n    from jack.readers.natural_language_inference.decomposable_attention import DecomposableAttentionModel\n    return _tf_nli_reader(DecomposableAttentionModel, resources_or_conf)\n\n\n@nli_reader\ndef cbilstm_nli_reader(resources_or_conf: Union[dict, SharedResources] = None):\n    """"""Creates a NLI reader based on conditional BiLSTMs.""""""\n    from jack.readers.natural_language_inference.conditional_bilstm import ConditionalBiLSTMClassificationModel\n    return _tf_nli_reader(ConditionalBiLSTMClassificationModel, resources_or_conf)\n\n\n@nli_reader\ndef modular_nli_reader(resources_or_conf: Union[dict, SharedResources] = None):\n    """"""Creates a Modular NLI reader instance. Model defined in config.""""""\n    from jack.readers.natural_language_inference.modular_nli_model import ModularNLIModel\n    return _tf_nli_reader(ModularNLIModel, resources_or_conf)\n\n\n# Link Prediction/Knowledge Base Population Models\n\n\n@link_prediction_reader\ndef distmult_reader(resources_or_conf: Union[dict, SharedResources] = None):\n    """"""Creates a knowledge_base_population DistMult model.""""""\n    from jack.readers.link_prediction.models import KnowledgeGraphEmbeddingInputModule, \\\n        KnowledgeGraphEmbeddingModelModule, \\\n        KnowledgeGraphEmbeddingOutputModule\n    shared_resources = create_shared_resources(resources_or_conf)\n    input_module = KnowledgeGraphEmbeddingInputModule(shared_resources)\n    model_module = KnowledgeGraphEmbeddingModelModule(shared_resources, model_name=\'DistMult\')\n    output_module = KnowledgeGraphEmbeddingOutputModule()\n    return TFReader(shared_resources, input_module, model_module, output_module)\n\n\n@link_prediction_reader\ndef complex_reader(resources_or_conf: Union[dict, SharedResources] = None):\n    """""" Creates a knowledge_base_population Complex model.""""""\n    from jack.readers.link_prediction.models import KnowledgeGraphEmbeddingInputModule, \\\n        KnowledgeGraphEmbeddingModelModule, \\\n        KnowledgeGraphEmbeddingOutputModule\n    shared_resources = create_shared_resources(resources_or_conf)\n    input_module = KnowledgeGraphEmbeddingInputModule(shared_resources)\n    model_module = KnowledgeGraphEmbeddingModelModule(shared_resources, model_name=\'ComplEx\')\n    output_module = KnowledgeGraphEmbeddingOutputModule()\n    return TFReader(shared_resources, input_module, model_module, output_module)\n\n\n@link_prediction_reader\ndef transe_reader(resources_or_conf: Union[dict, SharedResources] = None):\n    """""" Creates a knowledge_base_population TransE model.""""""\n    from jack.readers.link_prediction.models import KnowledgeGraphEmbeddingInputModule, \\\n        KnowledgeGraphEmbeddingModelModule, \\\n        KnowledgeGraphEmbeddingOutputModule\n    shared_resources = create_shared_resources(resources_or_conf)\n\n    input_module = KnowledgeGraphEmbeddingInputModule(shared_resources)\n    model_module = KnowledgeGraphEmbeddingModelModule(shared_resources, model_name=\'TransE\')\n    output_module = KnowledgeGraphEmbeddingOutputModule()\n    return TFReader(shared_resources, input_module, model_module, output_module)\n'"
jack/util/__init__.py,0,b''
jack/util/batch.py,0,"b'# -*- coding: utf-8 -*-\n\nimport random\n\nfrom itertools import islice\nfrom typing import TypeVar, List, Iterator, Optional\n\nimport numpy as np\n\nfrom jack.util.map import numpify\nfrom jack.util.random import DefaultRandomState\n\nrs = DefaultRandomState(1337)\n\n\nclass GeneratorWithRestart(object):\n    def __init__(self, iterator):\n        self.iterator = iterator\n\n    def __iter__(self):\n        return self.iterator()\n\n\ndef get_buckets(data, order, structure):\n    """"""\n    Generates mapping between data instances and bucket-ID\'s.\n\n    `data`: dict of nested sequences in which each top-level sequence has the same length,\n        and all inner sequences have the __len__ attribute.\n    `order`: (None or) tuple with data keys used for bucketing\n        For example:\n        ```list(data.keys()) = [""sentences1"", ""lengths1"", ""sentences2"", ""lengths2"", ""targets""]```\n        and we want bucketing according to the lengths of inner sequences in ""sentences1"" and ""sentences2"":\n        `order = (""sentences1"", ""sentences2"")` performs bucketing on ""sentences1"", and within each bucket,\n        again creates buckets according to ""sentences2""\n        (automatic bucketing will result in different ""sentences2"" bucket boundaries\n        within each bucket according to ""sentences1"").\n        `order = (""sentences2"", ""sentences1"")`: vice versa, with ""sentences2"" for highest-level buckets\n    `structure`: (None or) sequence with same length as `order`, each element is an integer or a list of integers\n        For each position:\n            - integer: denotes number of buckets, to be determined automatically\n            - list: determines bucket boundaries. E.g.: [10, 20, 30] will result in 4 buckets\n              (1) lengths 0-10, (2) lengths 11-20, (3) lengths 21-30, (4) lengths > 30\n        For example:\n        `order` = (""sentences1"", ""sentences2"") and `structure` = (3, [10]) generates 6 buckets:\n        within each of 3 partitions based on ""sentences1"",\n        there is a bucket with instances of ""sentences2"" with length 10 or less,\n        and one for lengths > 10.\n\n    Returns:\n        buckets2ids, ids2buckets\n        dicts that map instance-id (index along 1st dimension of values in data) to bucket-id,\n        and vice versa.\n    """"""\n    assert isinstance(data, dict)\n\n    n_tot = len(list(data.values())[0])\n    if order is None or structure is None:\n        # all in 1 bucket, with id \'(0)\'\n        buckets2ids = {\'(0)\': list(range(n_tot))}\n        ids2buckets = dict(zip(list(range(n_tot)), [\'(0)\'] * n_tot))\n        return buckets2ids, ids2buckets\n\n    def _chunk(it, size):\n        """"""returns iterator of chunks (tuples) from it (input iterator), with given size (last one may be shorter)""""""\n        it = iter(it)\n        return iter(lambda: tuple(islice(it, size)), ())\n\n    def _partition(_buckets2ids, _order, _structure):\n        """"""update _buckets2ids according to _order and _structure""""""\n        # update all current buckets according to first item in _order and _structure\n        buckets2ids_new = {}\n        for bid, ids in sorted(_buckets2ids.items(), key=lambda x: x[0]):\n            lengths = [len(data[_order[0]][id]) for id in ids]\n            sorted_ids_lengths = sorted(zip(ids, lengths), key=lambda x: x[1])\n            if isinstance(_structure[0], int):  # automatic bucketing\n                size = len(lengths) // _structure[0] if len(lengths) % _structure[0] == 0 \\\n                    else 1 + (len(lengths) // _structure[0])\n                buckets = list(_chunk([tup[0] for tup in sorted_ids_lengths], size))\n            else:  # structure_is sequence of ints\n                struct = list(sorted(_structure[0])) + [np.inf]\n                bin_max, struct = struct[0], struct[1:]\n                buckets = [[]]\n                for id, l in sorted_ids_lengths:\n                    if l > bin_max:  # never happens when bin_max = np.inf\n                        bin_max, struct = struct[0], struct[1:]\n                        buckets.append([])\n                    buckets[-1].append(id)\n            buckets2ids_new.update({tuple(list(bid) + [i]): list(bucket) for i, bucket in enumerate(buckets)})\n        # call again if _order and _structure have more than 1 item\n        if len(_order) > 1:\n            buckets2ids_new = _partition(buckets2ids_new, _order[1:], _structure[1:])\n\n        buckets2ids_new = {bid: bucket for bid, bucket in buckets2ids_new.items() if len(bucket) > 0}\n        return buckets2ids_new\n\n    buckets2ids = _partition({(): list(range(n_tot))}, order, structure)\n    buckets2ids = {str(bid): buckets2ids[bid] for bid in buckets2ids}  # make bucket-ids strings (for random.choice)\n\n    ids2buckets = {}\n    for bid, bucket in buckets2ids.items():\n        ids2buckets.update({id: bid for id in bucket})\n    return buckets2ids, ids2buckets\n\n\ndef get_batches(data, batch_size=32, pad=0, bucket_order=None, bucket_structure=None, exact_epoch=False):\n    """"""\n    Creates generator that batches `data`.\n    To avoid biases, it is advised to keep `bucket_order=None` and `bucket_structure=None` if computationally possible.\n    (which will sample batches from all instances)\n\n    Args:\n        `data`: dict with (multi-dimensional) numpy arrays or (nested) lists;\n            first inner dimension (`num_instances`) should be the same over all data values.\n        `batch_size`: the desired batch size\n        `pad`: padding symbol in case data contains lists of lists of different sizes\n        `bucket_order`: argument `order` in get_buckets (list with keys); `None` if no bucketing\n        `bucket_structure`: argument `structure` in get_buckets; `None` if no bucketing\n        `exact_epoch`: if set to `True`, final batch per bucket may be smaller, but each instance will be seen exactly\n            once during training. Default: `False`, to be certain during training\n            that each instance per batch gets same weight in the total loss\n            (but not all instances are observed per epoch if bucket sizes are no multiple of `batch_size`).\n\n    Returns:\n        a generator that generates a dict with same keys as `data`, and\n        as values data batches consisting of `[batch_size x num_instances]` 2D numpy tensors\n        (1st dimension is at most `batch_size` but may be smaller to cover all instances exactly once per epoch,\n        if `exact_epoch=True`)\n     """"""\n    assert isinstance(data, dict)\n\n    data0 = list(data.values())[0]\n    if not isinstance(data0, np.ndarray):\n        data_np = numpify(data, pad)  # still need original data for length-based bucketing\n    else:\n        data_np = data\n\n    def get_bucket_probs(_buckets2instances):\n        N = float(np.sum([len(ids) for ids in _buckets2instances.values()]))\n        return {bid: len(ids) / N if N > 0. else 0. for bid, ids in _buckets2instances.items()}\n\n    def shuffle_buckets(_buckets2instances):\n        for bid in sorted(_buckets2instances.keys()):  # sorted: to keep deterministic\n            rs.shuffle(_buckets2instances[bid])\n\n    buckets2instances, _ = get_buckets(data, bucket_order, bucket_structure)\n    n_buckets = len(buckets2instances)\n\n    exact_epoch = True if len(data0) < n_buckets * batch_size else exact_epoch\n\n    # if average instances/bucket smaller than batch_size: set exact_epoch = True\n    # to avoid empty batches during debugging on small data samples\n\n    def bucket_generator():\n        buckets2instances, _ = get_buckets(data, bucket_order, bucket_structure)\n        shuffle_buckets(buckets2instances)\n        all_seen = False\n        while not all_seen:\n            bids, probs = zip(*sorted(get_bucket_probs(buckets2instances).items(), key=lambda x: x[0]))\n            # sorted keys: to keep deterministic\n            if np.sum(probs) == 0.:\n                all_seen = True\n            else:\n                bid = rs.choice(bids, replace=False, p=probs)  # sample bucket according to remaining size\n                batch_indices = buckets2instances[bid][:batch_size]\n                buckets2instances[bid] = buckets2instances[bid][batch_size:]\n                # if required by exact_epoch: also include last batch in bucket if too small\n                if len(batch_indices) == batch_size or exact_epoch:\n                    yield {k: data_np[k][batch_indices] for k in data_np}\n\n    return GeneratorWithRestart(bucket_generator)\n\n\nT = TypeVar(\'T\')\n\n\ndef shuffle_and_batch(items: List[T], batch_size: int,\n                      rng: Optional[random.Random] = None) \\\n        -> Iterator[List[T]]:\n    """"""Optionally shuffles and batches items in a list.\n\n    Args:\n        - items: List of items to shuffle & batch.\n        - batch_size: size of batches.\n        - rng: random number generator if items should be shuffles, else None.\n\n    Returns: Batch iterator\n    """"""\n\n    todo = list(range(len(items)))\n    if rng is not None:\n        rng.shuffle(todo)\n    while todo:\n        indices = todo[:batch_size]\n        todo = todo[batch_size:]\n        items_batch = [items[i] for i in indices]\n        yield items_batch\n'"
jack/util/hooks.py,2,"b'# -*- coding: utf-8 -*-\n\nfrom abc import *\nfrom collections import defaultdict\nfrom datetime import datetime\nfrom time import strftime, localtime\nfrom time import time\nfrom typing import List, Tuple, Mapping\n\nimport numpy as np\nimport progressbar\nimport tensorflow as tf\n\nfrom jack.core.data_structures import QASetting, Answer\nfrom jack.core.reader import JTReader\nfrom jack.core.tensorflow import TFReader\nfrom jack.core.tensorport import TensorPort, Ports\nfrom jack.eval.extractive_qa import *\n\nlogger = logging.getLogger(__name__)\n\n""""""\nTODO -- hooks should also have prefixes so that one can use the same hook with different parameters\n""""""\n\n\nclass TrainingHook(metaclass=ABCMeta):\n    """"""Serves as Hook interface.""""""\n\n    @abstractmethod\n    def reader(self) -> JTReader:\n        """""" Returns: JTReader instance""""""\n        raise NotImplementedError\n\n    @abstractmethod\n    def at_epoch_end(self, epoch: int, **kwargs):\n        raise NotImplementedError\n\n    @abstractmethod\n    def at_iteration_end(self, epoch: int, loss: float, set_name=\'train\', **kwargs):\n        raise NotImplementedError\n\n\nclass TFTrainingHook(TrainingHook):\n    """"""Serves as Hook interface.""""""\n\n    @abstractmethod\n    def reader(self) -> TFReader:\n        """""" Returns: JTReader instance""""""\n        raise NotImplementedError\n\n\nclass TraceHook(TFTrainingHook):\n    """"""Abstract hook class, which implements an update function the summary.""""""\n\n    def __init__(self, reader, summary_writer=None):\n        self._summary_writer = summary_writer\n        self._reader = reader\n        self.scores = {}\n\n    @property\n    def reader(self) -> TFReader:\n        return self._reader\n\n    def update_summary(self, current_step, title, value):\n        """"""Adds summary (title, value) to summary writer object.\n\n        Args:\n            current_step (int): Current step in the training procedure.\n            value (float): Scalar value for the message.\n        """"""\n        if self._summary_writer is not None:\n            summary = tf.Summary(value=[\n                tf.Summary.Value(tag=title, simple_value=value),\n            ])\n            self._summary_writer.add_summary(summary, current_step)\n\n    def add_to_history(self, score_dict, iter_value, epoch, set_name=\'train\'):\n        for metric in score_dict:\n            if metric not in self.scores: self.scores[metric] = {}\n            if set_name not in self.scores[metric]: self.scores[metric][set_name] = [[], [], []]\n            self.scores[metric][set_name][0].append(score_dict[metric])\n            self.scores[metric][set_name][1].append(iter_value)\n            self.scores[metric][set_name][2].append(epoch)\n\n\nclass LossHook(TraceHook):\n    """"""A hook at prints the current loss and adds it to the summary.""""""\n\n    def __init__(self, reader, iter_interval=None, summary_writer=None):\n        super(LossHook, self).__init__(reader, summary_writer)\n        self._iter_interval = iter_interval\n        self._acc_loss = {\'train\': 0.0}\n        self._iter = {\'train\': 0}\n        self._epoch_loss = {\'train\': 0.0}\n        self._iter_epoch = {\'train\': 0}\n\n    def at_iteration_end(self, epoch, loss, set_name=\'train\', **kwargs):\n        """"""Prints the loss, epoch, and #calls; adds it to the summary. Loss should be batch normalized.""""""\n        if self._iter_interval is None: return loss\n        if set_name not in self._acc_loss:\n            self._acc_loss[set_name] = 0.0\n            self._iter[set_name] = 0\n            self._epoch_loss[set_name] = 0.0\n            self._iter_epoch[set_name] = 0\n\n        self._iter_epoch[set_name] += 1\n        self._epoch_loss[set_name] += 1\n        self._iter[set_name] += 1\n        self._acc_loss[set_name] += loss\n\n        if not self._iter[set_name] == 0 and self._iter[set_name] % self._iter_interval == 0:\n            loss = self._acc_loss[set_name] / self._iter_interval\n            super().add_to_history({\'loss\': loss, },\n                                   self._iter[set_name], epoch, set_name)\n            logger.info(""Epoch {0}\\tIter {1}\\t{3} loss {2}"".format(epoch,\n                                                                   self._iter[set_name], loss, set_name))\n            self.update_summary(self._iter[set_name], ""{0} loss"".format(set_name), loss)\n            self._acc_loss[set_name] = 0\n\n        ret = (0.0 if self._iter[set_name] == 0 else self._acc_loss[set_name] / self._iter[set_name])\n\n        return ret\n\n    def at_epoch_end(self, epoch, set_name=\'train\', **kwargs):\n        if self._iter_interval is None:\n            loss = self._acc_loss[set_name] / self._iter_interval[set_name]\n            logger.info(""Epoch {}\\tIter {}\\t{3} Loss {}"".format(epoch,\n                                                                self._iter, loss, set_name))\n            self.update_summary(self._iter[set_name], ""Loss"", loss)\n            self._epoch_loss[set_name] = 0\n            self._iter_epoch[set_name] = 0\n\n        ret = (0.0 if self._iter_epoch[set_name] == 0 else self._epoch_loss[set_name] / self._iter_epoch[set_name])\n\n        return ret\n\n\nclass ExamplesPerSecHook(TraceHook):\n    """"""Prints the examples per sec and adds it to the summary writer.""""""\n\n    def __init__(self, reader, batch_size, iter_interval=None, summary_writer=None):\n        super(ExamplesPerSecHook, self).__init__(reader, summary_writer)\n        self._iter_interval = iter_interval\n        self._iter = 0\n        self.num_examples = iter_interval * batch_size\n        self.reset = True\n\n    def __tag__(self):\n        return ""Speed""\n\n    def at_epoch_end(self, epoch, **kwargs):\n        # to eliminate drop in measured speed due to post-epoch hooks:\n        # do not execute; reset for use during epochs only\n        self.reset = True\n\n    def at_iteration_end(self, epoch, loss, **kwargs):\n        """"""Prints the examples per sec and adds it to the summary writer.""""""\n        self._iter += 1\n        if self.reset:\n            self.t0 = time()\n            self.reset = False\n        elif self._iter % self._iter_interval == 0:\n            diff = time() - self.t0\n            speed = ""%.2f"" % (self.num_examples / diff)\n            logger.info(""Epoch {}\\tIter {}\\tExamples/s {}"".format(str(epoch), str(self._iter), str(speed)))\n            self.update_summary(self._iter, self.__tag__(), float(speed))\n            self.t0 = time()\n\n\nclass ETAHook(TraceHook):\n    """"""Estimates ETA to next checkpoint, epoch end and training end.""""""\n\n    def __init__(self, reader, iter_interval, iter_per_epoch, max_epochs, iter_per_checkpoint=None,\n                 summary_writer=None):\n        super(ETAHook, self).__init__(reader, summary_writer)\n        self.iter_interval = iter_interval\n        self.iter_per_epoch = iter_per_epoch\n        self.iter_per_checkpoint = iter_per_checkpoint\n        self.iter = 0\n        self.epoch = 1\n        self.max_epochs = max_epochs\n        self.max_iters = max_epochs * iter_per_epoch\n        self.start = time()\n        self.start_checkpoint = time()\n        self.start_epoch = time()\n        self.reestimate = True\n\n    def __tag__(self):\n        return ""ETA""\n\n    def at_epoch_end(self, epoch, **kwargs):\n        # to eliminate drop in measured speed due to post-epoch hooks:\n        # do not execute; reset for use during epochs only\n        self.start_epoch = time()\n\n    def at_iteration_end(self, epoch, loss, **kwargs):\n        """"""Estimates ETA from max_iter vs current_iter.""""""\n        self.iter += 1\n\n        def format_eta(seconds):\n            if seconds == float(""inf""):\n                return ""never""\n            else:\n                seconds, _ = divmod(seconds, 1)\n                minutes, seconds = divmod(seconds, 60)\n                hours, minutes = divmod(minutes, 60)\n                seconds = str(int(seconds))\n                minutes = str(int(minutes))\n                hours = str(int(hours))\n\n                if len(hours) < 2:\n                    hours = ""0"" + hours\n                if len(minutes) < 2:\n                    minutes = ""0"" + minutes\n                if len(seconds) < 2:\n                    seconds = ""0"" + seconds\n\n                return ""{}:{}:{}"".format(hours, minutes, seconds)\n\n        if not self.iter == 0 and self.iter % self.iter_interval == 0:\n            current_time = time()\n\n            def get_eta(progress, start_time, name):\n                elapsed = current_time - start_time\n                eta = elapsed / progress * (1.0 - progress)\n                eta_date = strftime(""%y-%m-%d %H:%M:%S"", localtime(current_time + eta))\n                self.update_summary(self.iter, self.__tag__() + ""_"" + name, float(eta))\n\n                return format_eta(eta), eta_date\n\n            log = ""Epoch %d\\tIter %d"" % (epoch, self.iter)\n            total_progress = float(self.iter) / self.max_iters\n            eta, eta_data = get_eta(total_progress, self.start, ""total"")\n            log += ""\\tETA: %s, %s (%.2f%%)"" % (eta, eta_data, total_progress * 100)\n            epoch_progress = float((self.iter - 1) % self.iter_per_epoch + 1) / self.iter_per_epoch\n            eta, _ = get_eta(epoch_progress, self.start_epoch, ""epoch"")\n            log += ""\\tETA(epoch): %s (%.2f%%)"" % (eta, epoch_progress * 100)\n            if self.iter_per_checkpoint is not None:\n                checkpoint_progress = float((self.iter - 1) % self.iter_per_checkpoint + 1) / self.iter_per_checkpoint\n                eta, _ = get_eta(checkpoint_progress, self.start_checkpoint, ""checkpoint"")\n                log += ""\\tETA(checkpoint): %s (%.2f%%)"" % (eta, checkpoint_progress * 100)\n\n            logger.info(log)\n\n        if self.iter_per_checkpoint is not None and self.iter % self.iter_per_checkpoint == 0:\n            self.start_checkpoint = time()\n\n\nclass EvalHook(TraceHook):\n    def __init__(self, reader: JTReader, dataset, batch_size: int, ports: List[TensorPort],\n                 iter_interval=None, epoch_interval=1, metrics=None, summary_writer=None,\n                 write_metrics_to=None, info="""", side_effect=None):\n        super(EvalHook, self).__init__(reader, summary_writer)\n        self._total = len(dataset)\n        self._dataset = dataset\n        self._batches = None\n        self._ports = ports\n        self._epoch_interval = epoch_interval\n        self._iter_interval = iter_interval\n        self._batch_size = batch_size\n        # self.done_for_epoch = False\n        self._iter = 0\n        self._info = info or self.__class__.__name__\n        self._write_metrics_to = write_metrics_to\n        self._metrics = metrics or self.possible_metrics\n        self._side_effect = side_effect\n        self._side_effect_state = None\n\n    @abstractmethod\n    def possible_metrics(self) -> List[str]:\n        """"""Returns: list of metric keys this evaluation hook produces. """"""\n        raise NotImplementedError\n\n    @staticmethod\n    @abstractmethod\n    def preferred_metric_and_initial_score():\n        """"""Returns: Tuple of preferred metric to optimize and its initial value (usually the lowest possible one)""""""\n        raise NotImplementedError\n\n    @abstractmethod\n    def apply_metrics(self, inputs: List[Tuple[QASetting, List[Answer]]], tensors: Mapping[TensorPort, np.ndarray]) \\\n            -> Mapping[str, float]:\n        """"""Returns: dict from metric name to float""""""\n        raise NotImplementedError\n\n    def combine_metrics(self, accumulated_metrics: Mapping[str, List[float]]) -> Mapping[str, float]:\n        """"""Returns:\n               dict from metric name to float. Per default batch metrics are simply averaged by\n               total number of examples""""""\n        return {k: sum(vs) / self._total for k, vs in accumulated_metrics.items()}\n\n    def __call__(self, epoch):\n        if self._batches is None:\n            logger.info(""Preparing evaluation data..."")\n            self._batches = self.reader.input_module.batch_generator(self._dataset, self._batch_size, is_eval=True)\n\n        logger.info(""Started evaluation %s"" % self._info)\n        metrics = defaultdict(lambda: list())\n        bar = progressbar.ProgressBar(\n            max_value=len(self._dataset) // self._batch_size + 1,\n            widgets=[\' [\', progressbar.Timer(), \'] \', progressbar.Bar(), \' (\', progressbar.ETA(), \') \'])\n        for i, batch in bar(enumerate(self._batches)):\n            inputs = self._dataset[i * self._batch_size:(i + 1) * self._batch_size]\n            predictions = self.reader.model_module(batch, self._ports)\n            m = self.apply_metrics(inputs, predictions)\n            for k in self._metrics:\n                metrics[k].append(m[k])\n\n        metrics = self.combine_metrics(metrics)\n        super().add_to_history(metrics, self._iter, epoch)\n\n        printmetrics = sorted(metrics.keys())\n        res = ""Epoch %d\\tIter %d\\ttotal %d"" % (epoch, self._iter, self._total)\n        for m in printmetrics:\n            res += \'\\t%s: %.3f\' % (m, metrics[m])\n            self.update_summary(self._iter, self._info + \'_\' + m, metrics[m])\n            if self._write_metrics_to is not None:\n                with open(self._write_metrics_to, \'a\') as f:\n                    f.write(""{0} {1} {2:.5}\\n"".format(datetime.now(), self._info + \'_\' + m,\n                                                      np.round(metrics[m], 5)))\n        res += \'\\t\' + self._info\n        logger.info(res)\n\n        if self._side_effect is not None:\n            self._side_effect_state = self._side_effect(metrics, self._side_effect_state)\n\n    def at_epoch_end(self, epoch: int, **kwargs):\n        if self._epoch_interval is not None and epoch % self._epoch_interval == 0:\n            self.__call__(epoch)\n\n    def at_iteration_end(self, epoch: int, loss: float, **kwargs):\n        self._iter += 1\n        if self._iter_interval is not None and self._iter % self._iter_interval == 0:\n            self.__call__(epoch)\n\n\nclass XQAEvalHook(EvalHook):\n    """"""This evaluation hook computes the following metrics: exact and per-answer f1 on token basis.""""""\n\n    def __init__(self, reader: JTReader, dataset: List[Tuple[QASetting, List[Answer]]], batch_size: int,\n                 iter_interval=None, epoch_interval=1, metrics=None, summary_writer=None,\n                 write_metrics_to=None, info="""", side_effect=None,\n                 predicted_answer_span_port=Ports.Prediction.answer_span,\n                 target_answer_span_port=Ports.Target.answer_span,\n                 answer2support_port=Ports.Input.answer2support,\n                 support2question_port=Ports.Input.support2question, **kwargs):\n        self._predicted_answer_span_port = predicted_answer_span_port\n        self._target_answer_span_port = target_answer_span_port\n        self._answer2support_port = answer2support_port\n        self._support2question_port = support2question_port\n        ports = reader.output_module.input_ports\n        super().__init__(reader, dataset, batch_size, ports, iter_interval, epoch_interval, metrics, summary_writer,\n                         write_metrics_to, info, side_effect)\n\n    @property\n    def possible_metrics(self) -> List[str]:\n        return [""exact"", ""f1""]\n\n    @staticmethod\n    def preferred_metric_and_initial_score():\n        return \'f1\', [0.0]\n\n    def apply_metrics(self, inputs: List[Tuple[QASetting, List[Answer]]], tensors: Mapping[TensorPort, np.ndarray]) \\\n            -> Mapping[str, float]:\n        qs = [q for q, a in inputs]\n        p_answers = self.reader.output_module(qs, {p: tensors[p] for p in self.reader.output_module.input_ports})\n\n        f1 = exact_match = 0\n        for pa, (q, ass) in zip(p_answers, inputs):\n            ground_truth = [a.text for a in ass]\n            f1 += metric_max_over_ground_truths(f1_score, pa[0].text, ground_truth)\n            exact_match += metric_max_over_ground_truths(exact_match_score, pa[0].text, ground_truth)\n\n        return {""f1"": f1, ""exact"": exact_match}\n\n\nclass ClassificationEvalHook(EvalHook):\n    def __init__(self, reader: JTReader, dataset: List[Tuple[QASetting, List[Answer]]], batch_size: int,\n                 iter_interval=None, epoch_interval=1, metrics=None, summary_writer=None,\n                 write_metrics_to=None, info="""", side_effect=None,\n                 predicted_index_port=Ports.Prediction.candidate_index,\n                 target_index_port=Ports.Target.target_index, **kwargs):\n        self._predicted_index_port = predicted_index_port\n        self._target_index_port = target_index_port\n        ports = [self._predicted_index_port, self._target_index_port]\n\n        super().__init__(reader, dataset, batch_size, ports, iter_interval, epoch_interval, metrics, summary_writer,\n                         write_metrics_to, info, side_effect)\n\n    @property\n    def possible_metrics(self) -> List[str]:\n        return [""Accuracy""]\n\n    @staticmethod\n    def preferred_metric_and_initial_score():\n        return \'Accuracy\', [0.0]\n\n    def apply_metrics(self, inputs: List[Tuple[QASetting, List[Answer]]], tensors: Mapping[TensorPort, np.ndarray]) \\\n            -> Mapping[str, float]:\n        labels = tensors[self._target_index_port]\n        predictions = tensors[self._predicted_index_port]\n\n        labels_np = np.array(labels)\n        acc_exact = np.sum(np.equal(labels_np, predictions))\n\n        return {""Accuracy"": acc_exact}\n\n\nclass LogProbEvalHook(EvalHook):\n    """"""Assumes that the loss represents the negative log  probability of the model.""""""\n\n    def __init__(self, reader: JTReader, dataset: List[Tuple[QASetting, List[Answer]]], batch_size: int,\n                 iter_interval=None, epoch_interval=1, metrics=None, summary_writer=None,\n                 write_metrics_to=None, info="""", side_effect=None, **kwargs):\n        ports = [Ports.loss]\n        self.epoch = 0\n        super().__init__(reader, dataset, batch_size, ports, iter_interval, epoch_interval, metrics, summary_writer,\n                         write_metrics_to, info, side_effect)\n\n    @property\n    def possible_metrics(self) -> List[str]:\n        return [""log_p""]\n\n    @staticmethod\n    def preferred_metric_and_initial_score():\n        return \'log_p\', [float(\'-inf\')]\n\n    def apply_metrics(self, inputs: List[Tuple[QASetting, List[Answer]]], tensors: Mapping[TensorPort, np.ndarray]) \\\n            -> Mapping[str, float]:\n        loss = tensors[Ports.loss]\n        return {""log_p"": -loss * len(inputs)}\n\n    def at_epoch_end(self, epoch: int, **kwargs):\n        self.epoch += 1\n        if self._epoch_interval is not None and epoch % self._epoch_interval == 0:\n            self.__call__(epoch)\n'"
jack/util/map.py,0,"b'# -*- coding: utf-8 -*-\nimport logging\n\nimport numpy as np\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_list_shape(xs):\n    if isinstance(xs, int):\n        shape = []\n    else:\n        shape = [len(xs)]\n        for i, x in enumerate(xs):\n            if isinstance(x, list) or isinstance(x, tuple):\n                if len(shape) == 1:\n                    shape.append(0)\n                shape[1] = max(len(x), shape[1])\n                for j, y in enumerate(x):\n                    if isinstance(y, list):\n                        if len(shape) == 2:\n                            shape.append(0)\n                        shape[2] = max(len(y), shape[2])\n    return shape\n\n\ndef numpify(xs, pad=0, keys=None, dtypes=None):\n    """"""Converts a dict or list of Python data into a dict of numpy arrays.""""""\n    is_dict = isinstance(xs, dict)\n    xs_np = {} if is_dict else [0] * len(xs)\n    xs_iter = xs.items() if is_dict else enumerate(xs)\n\n    for i, (key, x) in enumerate(xs_iter):\n        try:\n            if (keys is None or key in keys) and not isinstance(x, np.ndarray):\n                shape = get_list_shape(x)\n                dtype = dtypes[i] if dtypes is not None else np.int64\n                x_np = np.full(shape, pad, dtype)\n\n                nb_dims = len(shape)\n\n                if nb_dims == 0:\n                    x_np = x\n                else:\n                    def f(tensor, values):\n                        t_shp = tensor.shape\n                        if len(t_shp) > 1:\n                            for _i, _values in enumerate(values):\n                                f(tensor[_i], _values)\n                        else:\n                            tensor[0:len(values)] = [v for v in values]\n\n                    f(x_np, x)\n\n                xs_np[key] = x_np\n            else:\n                xs_np[key] = x\n        except Exception as e:\n            logger.error(\'Error numpifying value \' + str(x) + \' of key \' + str(key))\n            raise e\n    return xs_np\n'"
jack/util/preprocessing.py,0,"b'# -*- coding: utf-8 -*-\n\nimport re\nfrom typing import Mapping, List, Any, Union, Tuple, Optional\n\nimport numpy as np\nimport spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import pairwise_distances\n\nfrom jack.util.vocab import Vocab\n\n\ndef fill_vocab(qa_settings, vocab=None, lowercase=False, lemmatize=False, spacy_nlp=False):\n    vocab = vocab or Vocab(unk=None)\n    assert not vocab._frozen, \'Filling frozen vocabs does not make a lot fo sense...\'\n    for qa_setting in qa_settings:\n        nlp_preprocess(qa_setting.question, vocab, lowercase, lemmatize, use_spacy=spacy_nlp)\n        for s in qa_setting.support:\n            nlp_preprocess(s, vocab, lowercase, lemmatize, use_spacy=spacy_nlp)\n    return vocab\n\n\n__pattern = re.compile(\'\\w+|[^\\w\\s]\')\n\n\ndef tokenize(text, pattern=__pattern):\n    return __pattern.findall(text)\n\n\ndef token_to_char_offsets(text, tokenized_text):\n    offsets = []\n    offset = 0\n    for t in tokenized_text:\n        offset = text.index(t, offset)\n        offsets.append(offset)\n        offset += len(t)\n    return offsets\n\n\ndef nlp_preprocess_all(qa_settings,\n                       vocab: Vocab,\n                       lowercase: bool = False,\n                       lemmatize: bool = False,\n                       with_lemmas: bool = False,\n                       with_tokens_offsets: bool = False,\n                       use_spacy: bool = False):\n    assert not vocab._frozen, \'Filling frozen vocabs does not make a lot fo sense...\'\n    processed_questions = []\n    processed_support = []\n    for qa_setting in qa_settings:\n        processed_questions.append(\n            nlp_preprocess(qa_setting.question, vocab, lowercase, lemmatize, use_spacy=use_spacy))\n        processed_support.append([])\n        for s in qa_settings.support:\n            processed_support[-1].append(nlp_preprocess(s, vocab, lowercase, lemmatize, use_spacy=use_spacy))\n    return processed_questions, processed_support\n\n\n__spacy_nlp = None\n\n\ndef spacy_nlp(parser=False, entity=False, matcher=False):\n    import spacy\n    global __spacy_nlp\n    if __spacy_nlp is None:\n        __spacy_nlp = spacy.load(""en"", parser=parser, entity=entity, matcher=matcher)\n    return __spacy_nlp\n\n\ndef nlp_preprocess(text: str,\n                   vocab: Vocab,\n                   lowercase: bool = False,\n                   lemmatize: bool = False,\n                   with_lemmas: bool = False,\n                   with_tokens_offsets: bool = False,\n                   use_spacy: bool = False) \\\n        -> Tuple[List[str], List[int], int, Optional[List[str]],\n                 Optional[List[int]]]:\n    """"""Preprocesses a question and support:\n    The steps include tokenization, lower-casing. It also includes the computation of token-to-character offsets for\n    the support. Lemmatization is supported in 2 ways. If lemmatize is True then the returned tokens are lemmatized\n    and the ids correspond to the lemma ids in the vocab. If with_lemmas and not lemmatize then an additional list\n    of the lemmatized token in string form is returned.\n\n    Returns:\n        tokens, ids, length, lemmas or None, token_offsets or None\n    """"""\n    assert not with_lemmas or use_spacy, ""enable spacy when using lemmas""\n    assert not lemmatize or use_spacy, ""enable spacy when using lemmas""\n\n    if use_spacy:\n        import spacy\n        nlp = spacy_nlp()\n        thistokenize = lambda t: nlp(t)\n    else:\n        thistokenize = tokenize\n    if lowercase:\n        text = text.lower()\n    tokens = thistokenize(text)\n\n    token_offsets = None\n    lemmas = None\n    if use_spacy:\n        if with_lemmas:\n            lemmas = [t.lemma_ for t in tokens]\n        if with_tokens_offsets:\n            token_offsets = [t.idx for t in tokens]\n        tokens = [t.lemma for t in tokens] if lemmatize else [t.orth_ for t in tokens]\n    else:\n        # char to token offsets\n        if with_tokens_offsets:\n            token_offsets = token_to_char_offsets(text, tokens)\n\n    length = len(tokens)\n    ids = vocab(tokens)\n\n    return tokens, ids, length, lemmas, token_offsets\n\n\ndef transpose_dict_of_lists(dict_of_lists: Mapping[str, list], keys: List[str]) \\\n        -> List[Mapping[str, Any]]:\n    """"""Takes a dict of lists, and turns it into a list of dicts.""""""\n\n    return [{key: dict_of_lists[key][i] for key in keys}\n            for i in range(len(dict_of_lists[keys[0]]))]\n\n\ndef char_vocab_from_vocab(vocab):\n    char_vocab = dict()\n    char_vocab[""PAD""] = 0\n    for i in range(len(vocab)):\n        w = vocab.get_sym(i)\n        if w is not None:\n            for c in w:\n                if c not in char_vocab:\n                    char_vocab[c] = len(char_vocab)\n    return char_vocab\n\n\ndef stack_and_pad(values: List[Union[np.ndarray, int, float]], pad=0) -> np.ndarray:\n    """"""Pads a list of numpy arrays so that they have equal dimensions, then stacks them.""""""\n    if isinstance(values[0], int) or isinstance(values[0], float):\n        return np.array(values)\n\n    dims = len(values[0].shape)\n    max_shape = [max(sizes) for sizes in zip(*[v.shape for v in values])]\n\n    padded_values = []\n\n    for value in values:\n        pad_width = [(0, max_shape[i] - value.shape[i])\n                     for i in range(dims)]\n        padded_value = np.lib.pad(value, pad_width, mode=\'constant\',\n                                  constant_values=pad)\n        padded_values.append(padded_value)\n\n    return np.stack(padded_values)\n\n\ndef unique_words_with_chars(tokens, char_vocab, char_limit=20):\n    vocab = dict()\n    rev_vocab = list()\n    unique_words = list()\n    unique_word_lengths = list()\n    token2unique = list()\n\n    for j in range(len(tokens)):\n        t2u = list()\n        for w in tokens[j]:\n            if w not in vocab:\n                unique_word_lengths.append(min(char_limit, len(w)))\n                unique_words.append([char_vocab.get(c, 0) for c in w[:char_limit]])\n                vocab[w] = len(vocab)\n                rev_vocab.append(w)\n            t2u.append(vocab[w])\n        token2unique.append(t2u)\n\n    return unique_words, unique_word_lengths, token2unique, vocab, rev_vocab\n\n\ndef sort_by_tfidf(reference, candidates):\n    tfidf = TfidfVectorizer(strip_accents=""unicode"", stop_words=spacy.en.STOP_WORDS, decode_error=\'replace\')\n    try:\n        para_features = tfidf.fit_transform(candidates)\n        q_features = tfidf.transform([reference])\n    except ValueError:\n        return [(i, 0.0) for i in range(len(candidates))]\n\n    dists = pairwise_distances(q_features, para_features, ""cosine"").ravel()\n    sorted_ix = np.lexsort((candidates, dists))  # in case of ties, use the earlier paragraph\n\n    return [(i, 1.0 - dists[i]) for i in sorted_ix]\n'"
jack/util/random.py,0,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\n\ndef singleton(cls):\n    instances = {}\n\n    def getinstance(*args, **kwargs):\n        if cls not in instances:\n            instances[cls] = cls(*args, **kwargs)\n        return instances[cls]\n    return getinstance\n\n\n@singleton\nclass DefaultRandomState(np.random.RandomState):\n    def __init__(self, seed=None):\n        super().__init__(seed)\n'"
jack/util/vocab.py,0,"b'# -*- coding: utf-8 -*-\n\nimport operator\nimport pickle\nimport sys\nfrom collections import OrderedDict\n\n\nclass Vocab:\n    """"""\n    Vocab objects for use in jack pipelines.\n    """"""\n    DEFAULT_UNK = ""<UNK>""\n\n    def __init__(self, unk=DEFAULT_UNK, vocab: dict = None):\n        """"""\n        Creates Vocab object.\n\n        Args:\n            `unk`: symbol for unknown term (default: ""<UNK>"").\n              If set to `None`, and `None` is not included as symbol while unfrozen,\n              it will return `None` upon calling `get_id(None)` when frozen.\n            `vocab`: init from vocab dict (sym -> id)\n        """"""\n        self._unk = unk\n        if vocab is not None:\n            self._sym2id = dict(vocab)\n            self._id2sym = {v: k for k, v in vocab.items()}\n            if unk is not None and unk not in self._sym2id:\n                self._sym2id[unk] = len(self._sym2id)\n                self._id2sym[len(self._id2sym)] = unk\n            self._sym2freqs = {w: 0 for w in self._sym2id}\n            self._frozen = True\n        else:\n            self._sym2id = {}\n            # with pos and neg indices\n            self._id2sym = {}\n            self._sym2freqs = OrderedDict()\n            if unk is not None:\n                self._sym2id[unk] = 0\n                # with pos and neg indices\n                self._id2sym[0] = unk\n                self._sym2freqs[unk] = 0\n            self._frozen = False\n\n    def freeze(self):\n        """"""Freeze current Vocab object (set `self._frozen` to True).""""""\n        # if any pretrained have been encountered\n        self._frozen = True\n\n    def unfreeze(self):\n        """"""Unfreeze current Vocab object (set `self.frozen` to False).""""""\n        self._frozen = False\n\n    def get_id(self, sym):\n        """"""Get id of symbol. Counts frequency if not frozen.""""""\n        if not self._frozen:\n            if sym not in self._sym2id:\n                self._sym2id[sym] = len(self._sym2id)\n                self._id2sym[len(self._id2sym)] = sym\n                self._sym2freqs[sym] = 1\n            else:\n                self._sym2freqs[sym] += 1\n        return self._sym2id.get(sym, self._sym2id.get(self._unk))\n\n    def get_sym(self, id):\n        """"""returns symbol for a given id (consistent with the `self.frozen` state), and None if not found.""""""\n        return self._id2sym.get(id)\n\n    def __call__(self, *args, **kwargs):\n        """"""\n        calls the `get_id` function for the provided symbol(s), which adds symbols to the Vocab if needed and allowed,\n        and returns their id(s).\n\n        Args:\n            *args: a single symbol, a list of symbols, or multiple symbols\n        """"""\n        symbols = args\n        if len(args) == 1:\n            if isinstance(args[0], list):\n                symbols = args[0]\n            else:\n                return self.get_id(args[0])\n        return [self.get_id(sym) for sym in symbols]\n\n    def __len__(self):\n        """"""returns number of unique symbols (including the unknown symbol)""""""\n        return len(self._id2sym)\n\n    def __contains__(self, sym):\n        """"""checks if `sym` already in the Vocab object""""""\n        return sym in self._sym2id\n\n    @property\n    def frozen(self):\n        return self._frozen\n\n    @property\n    def unk(self):\n        return self._unk\n\n    def prune(self, min_freq=5, max_size=sys.maxsize):\n        """"""returns new Vocab object, pruned based on minimum symbol frequency""""""\n        pruned_vocab = Vocab(unk=self._unk)\n        cnt = 0\n        for sym, freq in sorted(self._sym2freqs.items(), key=operator.itemgetter(1), reverse=True):\n            # for sym in self.sym2freqs:\n            # freq = self.sym2freqs[sym]\n            cnt += 1\n            if freq >= min_freq and cnt < max_size:\n                pruned_vocab(sym)\n                pruned_vocab._sym2freqs[sym] = freq\n        if self._frozen:\n            # if original Vocab was frozen, freeze new one\n            pruned_vocab.freeze()\n\n        return pruned_vocab\n\n    def store(self, path: str):\n        with open(path, ""wb"") as f:\n            pickle.dump(self.__dict__, f)\n\n    def load(self, path: str):\n        with open(path, ""rb"") as f:\n            data = pickle.load(f)\n        # backwards compability\n        new_data = {}\n        for k in data:\n            if k not in self.__dict__:\n                new_k = \'_\' + k\n                if new_k in self.__dict__:\n                    new_data[new_k] = data[k]\n            else:\n                new_data[k] = data[k]\n        self.__dict__ = new_data\n'"
projects/knowledge_integration/__init__.py,0,b'import projects.knowledge_integration.readers\n'
projects/knowledge_integration/knowledge_store.py,0,"b'import os\nimport pickle\nimport shelve\n\nimport spacy\n\n\nclass KnowledgeStore(object):\n    def __init__(self, path, writeback=False):\n        self._path = path\n        self._sws = spacy.en.STOP_WORDS\n        self._assertion_db = dict()\n        self._object2assertions = dict()\n        self._subject2assertions = dict()\n        self._num_assertions = 0\n        self._writeback = writeback\n\n        self._assertion_cache = dict()\n\n        if os.path.exists(os.path.join(path, \'object2assertions\')):\n            for fn in os.listdir(os.path.join(path, \'object2assertions\')):\n                with open(os.path.join(path, \'object2assertions\', fn), \'rb\') as f:\n                    self._object2assertions[fn] = pickle.load(f)\n            for fn in os.listdir(os.path.join(path, \'subject2assertions\')):\n                with open(os.path.join(path, \'subject2assertions\', fn), \'rb\') as f:\n                    self._subject2assertions[fn] = pickle.load(f)\n            for fn in os.listdir(os.path.join(path, \'assertions\')):\n                self._assertion_db[fn] = shelve.open(\n                    os.path.join(path, \'assertions\', fn), flag=\'c\' if writeback else \'r\', writeback=writeback)\n                self._assertion_cache[fn] = dict()\n                self._num_assertions += len(self._assertion_db[fn])\n        else:\n            os.makedirs(os.path.join(path, \'object2assertions\'))\n            os.makedirs(os.path.join(path, \'subject2assertions\'))\n            os.makedirs(os.path.join(path, \'assertions\'))\n\n    def get_connecting_assertion_keys(self, source_tokens, target_tokens, resources):\n        """"""Returns: mapping from assertion keys to IDF scores.""""""\n\n        def key_iterator(tokens):\n            for i in range(len(tokens)):\n                for j in range(i + 1, min(i + 6, len(tokens) + 1)):\n                    if tokens[j - 1] not in self._sws and tokens[j - 1].isalnum():\n                        yield tokens[i:j], i, j\n\n        source_obj_assertions = dict()\n        source_subj_assertions = dict()\n        keys = set()\n        for ks, start, end in key_iterator(source_tokens):\n            k = \' \'.join(ks)\n            if k in keys:\n                continue\n            keys.add(k)\n            for source in resources:\n                k_assertions = self._object2assertions[source].get(k)\n                if k_assertions is not None:\n                    idf = 1.0 / len(k_assertions)\n                    for a in k_assertions:\n                        source_obj_assertions[a] = (\n                            max(source_obj_assertions.get(a, (0.0, None))[0], idf), ks, start, end)\n                k_assertions = self._subject2assertions[source].get(k)\n                if k_assertions is not None:\n                    idf = 1.0 / len(k_assertions)\n                    for a in k_assertions:\n                        source_subj_assertions[a] = (\n                            max(source_subj_assertions.get(a, (0.0, None))[0], idf), ks, start, end)\n\n        assertions = dict()\n        assertion_args = dict()\n        keys = set()\n        for ks, start, end in key_iterator(target_tokens):\n            k = \' \'.join(ks)\n            if k in keys:\n                continue\n            keys.add(k)\n            for source in resources:\n                # subject from target, object from source\n                k_assertions_subj = self._subject2assertions[source].get(k)\n                if k_assertions_subj is not None:\n                    idf2 = 1.0 / len(k_assertions_subj)\n                    for a in k_assertions_subj:\n                        idf, ks2, start2, end2 = source_obj_assertions.get(a, (None, None, None, None))\n                        if idf is None or all(k in ks2 for k in ks) or all(k in ks for k in ks2):\n                            continue\n                        assertions[a] = max(assertions.get(a, 0.0), idf * idf2)\n                        assertion_args[a] = [start2, end2], [start, end]\n                # subject from source, object from target\n                k_assertions = self._object2assertions[source].get(k)\n                if k_assertions is not None:\n                    idf2 = 1.0 / len(k_assertions)\n                    for a in k_assertions:\n                        idf, ks2, start2, end2 = source_subj_assertions.get(a, (None, None, None, None))\n                        if idf is None or all(k in ks2 for k in ks) or all(k in ks for k in ks2):\n                            continue\n                        assertions[a] = max(assertions.get(a, 0.0), idf * idf2)\n                        assertion_args[a] = [start2, end2], [start, end]\n        return assertions, assertion_args\n\n    def get_assertion_keys(self, tokens, resources):\n        """"""Returns: mapping from assertion keys to IDF scores.""""""\n\n        def key_iterator(tokens):\n            for i in range(len(tokens)):\n                for j in range(i + 1, min(i + 6, len(tokens) + 1)):\n                    if tokens[j - 1] not in self._sws and tokens[j - 1].isalnum():\n                        yield tokens[i:j], i, j\n\n        assertions = dict()\n        assertion_args = dict()\n        keys = set()\n        for ks, start, end in key_iterator(tokens):\n            k = \' \'.join(ks)\n            if k in keys:\n                continue\n            keys.add(k)\n            for source in resources:\n                k_assertions = self._object2assertions[source].get(k)\n                if k_assertions is not None:\n                    idf = 1.0 / len(k_assertions)\n                    for a in k_assertions:\n                        assertions[a] = max(assertions.get(a, 0.0), idf)\n                        assertion_args[a] = [start, end]\n                k_assertions = self._subject2assertions[source].get(k)\n                if k_assertions is not None:\n                    idf = 1.0 / len(k_assertions)\n                    for a in k_assertions:\n                        assertions[a] = max(assertions.get(a, 0.0), idf)\n                        assertion_args[a] = [start, end]\n\n        return assertions, assertion_args\n\n    def assertion_keys_for_subject(self, subj, resource=\'default\'):\n        return self._subject2assertions[resource].get(subj, set())\n\n    def assertion_keys_for_object(self, subj, resource=\'default\'):\n        return self._object2assertions[resource].get(subj, set())\n\n    def get_assertion(self, assertion_key, cache=False):\n        resource = assertion_key[:assertion_key.index(\'$\')]\n        ret = self._assertion_cache[resource].get(assertion_key)\n        if ret is None:\n            ret = self._assertion_db[resource].get(assertion_key)\n            if cache:\n                self._assertion_cache[resource][assertion_key] = ret\n        return ret\n\n    def add_assertion(self, assertion, subjects, objects, resource=\'default\', key=None):\n        assert \'$\' not in resource\n        key = resource + \'$\' + (key or str(self._num_assertions))\n        if resource not in self._object2assertions:\n            self._object2assertions[resource] = dict()\n            self._subject2assertions[resource] = dict()\n            self._assertion_db[resource] = shelve.open(\n                os.path.join(self._path, \'assertions\', resource), flag=\'c\' if self._writeback else \'r\',\n                writeback=self._writeback)\n            self._assertion_cache[resource] = dict()\n        o2a = self._object2assertions[resource]\n        s2a = self._subject2assertions[resource]\n        for o in objects:\n            if o not in o2a:\n                o2a[o] = set()\n            o2a[o].add(key)\n        for s in subjects:\n            if s not in s2a:\n                s2a[s] = set()\n            s2a[s].add(key)\n        self._assertion_db[resource][key] = assertion\n        self._num_assertions += 1\n\n    def save(self):\n        for key in self._object2assertions:\n            with open(os.path.join(self._path, \'object2assertions\', key), \'wb\') as f:\n                pickle.dump(self._object2assertions[key], f)\n            with open(os.path.join(self._path, \'subject2assertions\', key), \'wb\') as f:\n                pickle.dump(self._subject2assertions[key], f)\n            self._assertion_db[key].sync()\n'"
projects/knowledge_integration/nli.py,13,"b'import random\nfrom typing import Mapping, List, Optional, Iterable, Tuple\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom jack.core.data_structures import QASetting, Answer\nfrom jack.core.input_module import OnlineInputModule\nfrom jack.core.shared_resources import SharedResources\nfrom jack.core.tensorflow import TFModelModule\nfrom jack.core.tensorport import Ports, TensorPort, TensorPortTensors\nfrom jack.readers.classification.util import create_answer_vocab\nfrom jack.util import preprocessing\nfrom jack.util.map import numpify\nfrom jack.util.tf import misc\nfrom jack.util.tf.rnn import fused_birnn\nfrom projects.knowledge_integration.knowledge_store import KnowledgeStore\nfrom projects.knowledge_integration.shared import AssertionMRPorts\nfrom projects.knowledge_integration.tfutil import embedding_refinement\n\n\nclass MultipleChoiceAssertionInputModule(OnlineInputModule[Mapping[str, any]]):\n    def __init__(self, shared_resources):\n        self.shared_resources = shared_resources\n        self.__nlp = preprocessing.spacy_nlp()\n        self._rng = random.Random(123)\n\n    @property\n    def training_ports(self):\n        return [Ports.Target.target_index]\n\n    @property\n    def output_ports(self):\n        return [AssertionMRPorts.question_length, AssertionMRPorts.support_length,\n                # char\n                AssertionMRPorts.word_chars, AssertionMRPorts.word_char_length,\n                AssertionMRPorts.question, AssertionMRPorts.support,\n                # optional, only during training\n                AssertionMRPorts.is_eval,\n                # for assertionss\n                AssertionMRPorts.word_embeddings,\n                AssertionMRPorts.assertion_lengths,\n                AssertionMRPorts.assertion2question,\n                AssertionMRPorts.assertions,\n                AssertionMRPorts.question_arg_span,\n                AssertionMRPorts.assertion2question_arg_span,\n                AssertionMRPorts.support_arg_span,\n                AssertionMRPorts.assertion2support_arg_span,\n                AssertionMRPorts.word2lemma]\n\n    def preprocess(self, questions: List[QASetting], answers: Optional[List[List[Answer]]] = None,\n                   is_eval: bool = False) -> List[Mapping[str, any]]:\n        preprocessed = list()\n        for i, qa in enumerate(questions):\n            tokens, _, length, lemmas, _ = preprocessing.nlp_preprocess(\n                qa.question, self.shared_resources.vocab, lowercase=True, with_lemmas=True, use_spacy=True)\n            s_tokens, _, s_length, s_lemmas, _ = preprocessing.nlp_preprocess(\n                qa.support[0], self.shared_resources.vocab, lowercase=True, with_lemmas=True, use_spacy=True)\n\n            preprocessed.append({\n                \'support_tokens\': s_tokens,\n                \'support_lemmas\': s_lemmas,\n                \'support_lengths\': s_length,\n                \'question_tokens\': tokens,\n                \'question_lemmas\': lemmas,\n                \'question_lengths\': length,\n                \'ids\': i,\n            })\n            if answers is not None:\n                preprocessed[-1][""answers""] = self.shared_resources.answer_vocab(answers[i][0].text)\n\n        return preprocessed\n\n    def create_batch(self, annotations: List[Mapping[str, any]], is_eval: bool, with_answers: bool):\n        support_lengths = list()\n        question_lengths = list()\n\n        ass_lengths = []\n        ass2question = []\n        ass2unique = []\n        lemma2idx = dict()\n        answer_labels = []\n        question_arg_span = []\n        support_arg_span = []\n        assertions2question_arg_span = []\n        assertions2support_arg_span = []\n\n        question_arg_span_idx = dict()\n        support_arg_span_idx = dict()\n\n        word_chars, word_lengths, tokens, vocab, rev_vocab = \\\n            preprocessing.unique_words_with_chars(\n                [a[""question_tokens""] for a in annotations] + [a[""support_tokens""] for a in annotations],\n                self.char_vocab)\n        question, support = tokens[:len(annotations)], tokens[len(annotations):]\n\n        word2lemma = [None] * len(rev_vocab)\n\n        # we have to create batches here and cannot precompute them because of the batch-specific wiq feature\n        for i, annot in enumerate(annotations):\n            support_lengths.append(annot[\'support_lengths\'])\n            question_lengths.append(annot[\'question_lengths\'])\n\n            if ""answers"" in annot:\n                answer_labels.append(annot[""answers""])\n\n            # collect uniq lemmas:\n            for k, l in enumerate(annot[\'question_lemmas\']):\n                if l not in lemma2idx:\n                    lemma2idx[l] = len(lemma2idx)\n                word2lemma[question[i][k]] = lemma2idx[l]\n            for k, l in enumerate(annot[\'support_lemmas\']):\n                if l not in lemma2idx:\n                    lemma2idx[l] = len(lemma2idx)\n                word2lemma[support[i][k]] = lemma2idx[l]\n\n            assertions, assertion_args = self._knowledge_store.get_connecting_assertion_keys(\n                annot[\'question_lemmas\'], annot[\'support_lemmas\'], self._sources)\n\n            sorted_assertionss = sorted(assertions.items(), key=lambda x: -x[1])\n            added_assertionss = set()\n            for key, _ in sorted_assertionss:\n                if len(added_assertionss) == self._limit:\n                    break\n                a = self.__nlp(self._knowledge_store.get_assertion(key))\n                a_lemma = "" "".join(t.lemma_ for t in a)\n                if a_lemma in added_assertionss:\n                    continue\n                else:\n                    added_assertionss.add(a_lemma)\n                ass2question.append(i)\n                ass_lengths.append(len(a))\n                q_arg_span = assertion_args[key][0]\n                q_arg_span = (i, q_arg_span[0], q_arg_span[1])\n                s_arg_span = assertion_args[key][1]\n                s_arg_span = (i, s_arg_span[0], s_arg_span[1])\n                if q_arg_span not in question_arg_span_idx:\n                    question_arg_span_idx[q_arg_span] = len(question_arg_span)\n                    question_arg_span.append(assertion_args[key][0])\n                if s_arg_span not in support_arg_span_idx:\n                    support_arg_span_idx[s_arg_span] = len(support_arg_span)\n                    support_arg_span.append(assertion_args[key][1])\n                assertions2question_arg_span.append(question_arg_span_idx[q_arg_span])\n                assertions2support_arg_span.append(support_arg_span_idx[s_arg_span])\n\n                u_ass = []\n                for t in a:\n                    w = t.orth_\n                    if w not in vocab:\n                        vocab[w] = len(vocab)\n                        word_lengths.append(min(len(w), 20))\n                        word_chars.append([self.char_vocab.get(c, 0) for c in w[:20]])\n                        rev_vocab.append(w)\n                        if t.lemma_ not in lemma2idx:\n                            lemma2idx[t.lemma_] = len(lemma2idx)\n                        word2lemma.append(lemma2idx[t.lemma_])\n                    u_ass.append(vocab[w])\n                ass2unique.append(u_ass)\n\n        word_embeddings = np.zeros([len(rev_vocab), self.emb_matrix.shape[1]])\n        for i, w in enumerate(rev_vocab):\n            word_embeddings[i] = self._get_emb(self.shared_resources.vocab(w))\n\n        if not ass2unique:\n            ass2unique.append([])\n            question_arg_span = support_arg_span = np.zeros([0, 2], dtype=np.int32)\n\n        output = {\n            AssertionMRPorts.word_chars: word_chars,\n            AssertionMRPorts.word_char_length: word_lengths,\n            AssertionMRPorts.question: question,\n            AssertionMRPorts.support: support,\n            AssertionMRPorts.support_length: support_lengths,\n            AssertionMRPorts.question_length: question_lengths,\n            AssertionMRPorts.is_eval: is_eval,\n            AssertionMRPorts.word_embeddings: word_embeddings,\n            AssertionMRPorts.assertion_lengths: ass_lengths,\n            AssertionMRPorts.assertion2question: ass2question,\n            AssertionMRPorts.assertions: ass2unique,\n            AssertionMRPorts.word2lemma: word2lemma,\n            AssertionMRPorts.question_arg_span: question_arg_span,\n            AssertionMRPorts.support_arg_span: support_arg_span,\n            AssertionMRPorts.assertion2question_arg_span: assertions2question_arg_span,\n            AssertionMRPorts.assertion2support_arg_span: assertions2support_arg_span,\n            \'__vocab\': vocab,\n            \'__rev_vocab\': rev_vocab,\n            \'__lemma_vocab\': lemma2idx,\n        }\n        if ""answers"" in annotations[0]:\n            output[Ports.Target.target_index] = [a[""answers""] for a in annotations]\n\n        return numpify(output, keys=self.output_ports + self.training_ports)\n\n    def setup_from_data(self, data: Iterable[Tuple[QASetting, List[Answer]]]):\n        if not self.shared_resources.vocab.frozen:\n            self.shared_resources.vocab = preprocessing.fill_vocab(\n                (q for q, _ in data), self.shared_resources.vocab, lowercase=True)\n            self.shared_resources.vocab.freeze()\n        if not hasattr(self.shared_resources, \'answer_vocab\') or not self.shared_resources.answer_vocab.frozen:\n            self.shared_resources.answer_vocab = create_answer_vocab(answers=(a for _, ass in data for a in ass))\n            self.shared_resources.answer_vocab.freeze()\n        self.shared_resources.config[\'answer_size\'] = self.shared_resources.config.get(\n            \'answer_size\', len(self.shared_resources.answer_vocab))\n        self.shared_resources.char_vocab = {chr(i): i for i in range(256)}\n\n    def setup(self):\n        self._knowledge_store = KnowledgeStore(self.shared_resources.config[""assertion_dir""])\n        self._sources = self.shared_resources.config[""assertion_sources""]\n        self._limit = self.shared_resources.config.get(""assertion_limit"", 10)\n        self.vocab = self.shared_resources.vocab\n        self.config = self.shared_resources.config\n        self.batch_size = self.config.get(""batch_size"", 1)\n        self.dropout = self.config.get(""dropout"", 0.0)\n        self._rng = random.Random(self.config.get(""seed"", 123))\n        self.emb_matrix = self.vocab.emb.lookup\n        self.default_vec = np.zeros([self.vocab.emb_length])\n        self.char_vocab = self.shared_resources.char_vocab\n\n    def _get_emb(self, idx):\n        if idx < self.emb_matrix.shape[0]:\n            return self.emb_matrix[idx]\n        else:\n            return self.default_vec\n\n\nclass ClassificationAssertionMixin:\n    @property\n    def input_ports(self) -> List[TensorPort]:\n        return [AssertionMRPorts.question_length, AssertionMRPorts.support_length,\n                # char embedding inputs\n                AssertionMRPorts.word_chars, AssertionMRPorts.word_char_length,\n                AssertionMRPorts.question, AssertionMRPorts.support,\n                # optional input, provided only during training\n                AssertionMRPorts.is_eval,\n                # assertions related ports\n                AssertionMRPorts.word_embeddings, AssertionMRPorts.assertion_lengths,\n                AssertionMRPorts.assertion2question, AssertionMRPorts.assertions,\n                AssertionMRPorts.word2lemma]\n\n    @property\n    def output_ports(self) -> List[TensorPort]:\n        return [Ports.Prediction.logits, Ports.Prediction.candidate_index]\n\n    @property\n    def training_input_ports(self) -> List[TensorPort]:\n        return [Ports.Prediction.logits, Ports.Target.target_index]\n\n    @property\n    def training_output_ports(self) -> List[TensorPort]:\n        return [Ports.loss]\n\n\nclass NLIAssertionModel(ClassificationAssertionMixin, TFModelModule):\n    def create_output(self, shared_resources, input_tensors):\n        tensors = TensorPortTensors(input_tensors)\n\n        question_length = tensors.question_length\n        support_length = tensors.support_length\n        word_chars = tensors.word_chars\n        word_char_length = tensors.word_char_length\n        question = tensors.question\n        support = tensors.support\n        is_eval = tensors.is_eval\n        word_embeddings = tensors.word_embeddings\n        assertion_length = tensors.assertion_lengths\n        assertion2question = tensors.assertion2question\n        assertions = tensors.assertions\n        word2lemma = tensors.word2lemma\n\n        # Some helpers\n        input_size = shared_resources.config[""repr_dim_input""]\n        size = shared_resources.config[""repr_dim""]\n        num_classes = shared_resources.config[""answer_size""]\n        with_char_embeddings = shared_resources.config.get(""with_char_embeddings"", False)\n        reading_encoder_config = shared_resources.config[\'reading_module\']\n\n        word_embeddings.set_shape([None, input_size])\n\n        reading_sequence = [support, question, assertions]\n        reading_sequence_lengths = [support_length, question_length, assertion_length]\n        reading_sequence_2_batch = [None, None, assertion2question]\n\n        new_word_embeddings, reading_sequence_offset, _ = embedding_refinement(\n            size, word_embeddings, reading_encoder_config,\n            reading_sequence, reading_sequence_2_batch, reading_sequence_lengths,\n            word2lemma, word_chars, word_char_length, is_eval,\n            keep_prob=1.0 - shared_resources.config.get(\'dropout\', 0.0),\n            with_char_embeddings=with_char_embeddings, num_chars=len(shared_resources.char_vocab))\n\n        emb_question = tf.nn.embedding_lookup(new_word_embeddings, reading_sequence_offset[1],\n                                              name=\'embedded_question\')\n        emb_support = tf.nn.embedding_lookup(new_word_embeddings, reading_sequence_offset[0],\n                                             name=\'embedded_support\')\n\n        logits = nli_model(size, num_classes, emb_question, question_length, emb_support, support_length)\n\n        return {\n            Ports.Prediction.logits: logits,\n            Ports.Prediction.candidate_index: tf.argmax(logits, 1)\n        }\n\n    def create_training_output(self, shared_resources: SharedResources, input_tensors):\n        tensors = TensorPortTensors(input_tensors)\n        return {\n            Ports.loss: tf.losses.sparse_softmax_cross_entropy(logits=tensors.logits, labels=tensors.target_index),\n        }\n\n\ndef nli_model(size, num_classes, emb_question, question_length, emb_support, support_length):\n    fused_rnn = tf.contrib.rnn.LSTMBlockFusedCell(size)\n    # [batch, 2*output_dim] -> [batch, num_classes]\n    _, q_states = fused_birnn(fused_rnn, emb_question, sequence_length=question_length,\n                              dtype=tf.float32, time_major=False, scope=""question_rnn"")\n\n    outputs, _ = fused_birnn(fused_rnn, emb_support, sequence_length=support_length,\n                             dtype=tf.float32, initial_state=q_states, time_major=False, scope=""support_rnn"")\n\n    # [batch, T, 2 * dim] -> [batch, dim]\n    outputs = tf.concat([outputs[0], outputs[1]], axis=2)\n    hidden = tf.layers.dense(outputs, size, tf.nn.relu, name=""hidden"") * tf.expand_dims(\n        misc.mask_for_lengths(support_length, max_length=tf.shape(outputs)[1], mask_right=False, value=1.0), 2)\n    hidden = tf.reduce_max(hidden, axis=1)\n    # [batch, dim] -> [batch, num_classes]\n    outputs = tf.layers.dense(hidden, num_classes, name=""classification"")\n    return outputs\n'"
projects/knowledge_integration/readers.py,0,"b'""""""Reader definitions that use back""""""\n\nfrom jack.core.tensorflow import TFReader\nfrom jack.readers.implementations import nli_reader, create_shared_resources, extractive_qa_reader\n\n\n@extractive_qa_reader\ndef modular_assertion_qa_reader(resources_or_conf=None):\n    from projects.knowledge_integration.qa.shared import XQAAssertionInputModule\n    from jack.readers.extractive_qa.shared import XQAOutputModule\n    from projects.knowledge_integration.qa.shared import ModularAssertionQAModel\n    shared_resources = create_shared_resources(resources_or_conf)\n\n    input_module = XQAAssertionInputModule(shared_resources)\n    model_module = ModularAssertionQAModel(shared_resources)\n    output_module = XQAOutputModule()\n    return TFReader(shared_resources, input_module, model_module, output_module)\n\n\n@extractive_qa_reader\ndef modular_assertion_definition_qa_reader(resources_or_conf=None):\n    from projects.knowledge_integration.qa.definition_model import XQAAssertionDefinitionInputModule\n    from projects.knowledge_integration.qa.definition_model import ModularAssertionDefinitionQAModel\n    from jack.readers.extractive_qa.shared import XQAOutputModule\n    shared_resources = create_shared_resources(resources_or_conf)\n\n    input_module = XQAAssertionDefinitionInputModule(shared_resources)\n    model_module = ModularAssertionDefinitionQAModel(shared_resources)\n    output_module = XQAOutputModule()\n    reader = TFReader(shared_resources, input_module, model_module, output_module)\n    input_module.set_reader(reader)\n    return TFReader(shared_resources, input_module, model_module, output_module)\n\n\n@nli_reader\ndef cbilstm_nli_assertion_reader(resources_or_conf=None):\n    from projects.knowledge_integration.nli import NLIAssertionModel\n    from projects.knowledge_integration.nli import MultipleChoiceAssertionInputModule\n    from jack.readers.classification.shared import SimpleClassificationOutputModule\n    shared_resources = create_shared_resources(resources_or_conf)\n    input_module = MultipleChoiceAssertionInputModule(shared_resources)\n    model_module = NLIAssertionModel(shared_resources)\n    output_module = SimpleClassificationOutputModule(shared_resources)\n    return TFReader(shared_resources, input_module, model_module, output_module)\n'"
projects/knowledge_integration/shared.py,0,"b'import numpy as np\n\nfrom jack.core import TensorPort, Ports\n\n\nclass AssertionMRPorts:\n    # When feeding embeddings directly\n    question_length = Ports.Input.question_length\n    support_length = Ports.Input.support_length\n\n    # but also ids, for char-based embeddings\n    question = Ports.Input.question\n    support = Ports.Input.support\n\n    word_char_length = TensorPort(np.int32, [None], ""word_char_length"", ""words length"", ""[U]"")\n\n    token_char_offsets = TensorPort(np.int32, [None, None], ""token_char_offsets"",\n                                    ""Character offsets of tokens in support."", ""[S, support_length]"")\n\n    keep_prob = Ports.keep_prob\n    is_eval = Ports.is_eval\n\n    word_embeddings = TensorPort(np.float32, [None, None], ""word_embeddings"",\n                                 ""Embeddings only for words occuring in batch."", ""[None, N]"")\n\n    assertion_lengths = TensorPort(np.int32, [None], ""assertion_lengths"", ""Length of assertion."", ""[R]"")\n\n    assertions = TensorPort(np.int32, [None, None], ""assertions"",\n                            ""Represents batch dependent assertion word ids."",\n                            ""[R, L]"")\n    assertion2question = TensorPort(np.int32, [None], ""assertion2question"", ""Question idx per assertion"", ""[R]"")\n\n    word2lemma = TensorPort(np.int32, [None], ""word2lemma"", ""Lemma idx per word"", ""[U]"")\n\n    word_chars = TensorPort(np.int32, [None, None], ""word_chars"", ""Represents words as sequence of chars"",\n                            ""[U, max_num_chars]"")\n\n    question_arg_span = TensorPort(np.int32, [None, 2], ""question_arg_span"",\n                                   ""span of an argument in the question"", ""[Q, 2]"")\n\n    support_arg_span = TensorPort(np.int32, [None, 2], ""support_arg_span"",\n                                  ""span of an argument in the suppoort"", ""[S, 2]"")\n\n    assertion2question_arg_span = TensorPort(np.int32, [None], ""assertion2question_arg_span"",\n                                             ""assertion to question span mapping"", ""[A]"")\n    assertion2support_arg_span = TensorPort(np.int32, [None], ""assertion2support_arg_span"",\n                                            ""assertion to support span mapping"", ""[A]"")\n'"
projects/knowledge_integration/tfutil.py,31,"b'import tensorflow as tf\n\nfrom jack.util.tf import modular_encoder, misc\nfrom jack.util.tf.embedding import conv_char_embedding\n\n\ndef embedding_refinement(size, word_embeddings, sequence_module, reading_sequence, reading_sequence_2_batch,\n                         reading_sequence_lengths, word2lemma, unique_word_chars=None,\n                         unique_word_char_length=None, is_eval=False, sequence_indices=None, num_sequences=4,\n                         only_refine=False, keep_prob=1.0, batch_size=None, with_char_embeddings=False, num_chars=0):\n    if batch_size is None:\n        batch_size = tf.reduce_max(tf.stack([tf.shape(s)[0] if s2b is None else tf.reduce_max(s2b) + 1\n                                             for s, s2b in zip(reading_sequence, reading_sequence_2_batch)]))\n\n    sequence_indices = sequence_indices if sequence_indices is not None else list(range(len(reading_sequence)))\n\n    if not only_refine:\n        word_embeddings = tf.layers.dense(word_embeddings, size, activation=tf.nn.relu, name=""embeddings_projection"")\n        if with_char_embeddings:\n            word_embeddings = word_with_char_embed(\n                size, word_embeddings, unique_word_chars, unique_word_char_length, num_chars)\n        if keep_prob < 1.0:\n            word_embeddings = tf.cond(is_eval,\n                                      lambda: word_embeddings,\n                                      lambda: tf.nn.dropout(word_embeddings, keep_prob, [1, size]))\n        # tile word_embeddings by batch size (individual batches update embeddings individually)\n        ctxt_word_embeddings = tf.tile(word_embeddings, tf.stack([batch_size, 1]))\n        # HACK so that backprop works with indexed slices that come through here which are not handled by tile\n        ctxt_word_embeddings *= 1.0\n    else:\n        ctxt_word_embeddings = word_embeddings\n\n    num_words = tf.shape(word2lemma)[0]\n\n    # divide uniq words for each question by offsets\n    offsets = tf.expand_dims(tf.range(0, num_words * batch_size, num_words), 1)\n\n    # each token is assigned a word idx + offset for distinguishing words between batch instances\n    reading_sequence_offset = [\n        s + offsets if s2b is None else s + tf.gather(offsets, s2b)\n        for s, s2b in zip(reading_sequence, reading_sequence_2_batch)]\n\n    word2lemma_off = tf.tile(tf.reshape(word2lemma, [1, -1]), [batch_size, 1]) + offsets\n    word2lemma_off = tf.reshape(word2lemma_off, [-1])\n\n    with tf.variable_scope(""refinement"") as vs:\n        for i, seq, length in zip(sequence_indices, reading_sequence_offset, reading_sequence_lengths):\n            if i > 0:\n                vs.reuse_variables()\n            num_seq = tf.shape(length)[0]\n\n            def non_zero_batchsize_op():\n                max_length = tf.shape(seq)[1]\n                encoded = tf.nn.embedding_lookup(ctxt_word_embeddings, seq)\n                one_hot = [0.0] * num_sequences\n                one_hot[i] = 1.0\n                mode_feature = tf.constant([[one_hot]], tf.float32)\n                mode_feature = tf.tile(mode_feature, tf.stack([num_seq, max_length, 1]))\n                encoded = tf.concat([encoded, mode_feature], 2)\n                encoded = modular_encoder.modular_encoder(\n                    sequence_module, {\'text\': encoded}, {\'text\': length}, {\'text\': None}, size,\n                    1.0 - keep_prob, is_eval)[0][\'text\']\n\n                mask = misc.mask_for_lengths(length, max_length, mask_right=False, value=1.0)\n                encoded = encoded * tf.expand_dims(mask, 2)\n\n                seq_lemmas = tf.gather(word2lemma_off, tf.reshape(seq, [-1]))\n                new_lemma_embeddings = tf.unsorted_segment_max(\n                    tf.reshape(encoded, [-1, size]), seq_lemmas, tf.reduce_max(word2lemma_off) + 1)\n                new_lemma_embeddings = tf.nn.relu(new_lemma_embeddings)\n\n                return tf.gather(new_lemma_embeddings, word2lemma_off)\n\n            new_word_embeddings = tf.cond(num_seq > 0, non_zero_batchsize_op,\n                                          lambda: tf.zeros_like(ctxt_word_embeddings))\n            # update old word embeddings with new ones via gated addition\n            gate = tf.layers.dense(tf.concat([ctxt_word_embeddings, new_word_embeddings], 1), size, tf.nn.sigmoid,\n                                   bias_initializer=tf.constant_initializer(1.0), name=""embeddings_gating"")\n            ctxt_word_embeddings = ctxt_word_embeddings * gate + (1.0 - gate) * new_word_embeddings\n\n    return ctxt_word_embeddings, reading_sequence_offset, offsets\n\n\ndef word_with_char_embed(size, word_embeddings, unique_word_chars, unique_word_char_length, num_chars):\n    # compute combined embeddings\n    char_word_embeddings = conv_char_embedding(\n        num_chars, size, unique_word_chars, unique_word_char_length)\n    char_word_embeddings = tf.nn.relu(char_word_embeddings)\n    gate = tf.layers.dense(tf.concat([word_embeddings, char_word_embeddings], 1), size, tf.nn.sigmoid,\n                           bias_initializer=tf.constant_initializer(1.0), name=""embeddings_gating"")\n    word_embeddings = word_embeddings * gate + (1.0 - gate) * char_word_embeddings\n\n    return word_embeddings\n'"
tests/jack/test_core.py,0,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom jack.core import SharedResources\nfrom jack.io.embeddings import load_embeddings\nfrom jack.util.vocab import Vocab\n\n\ndef test_shared_resources_store():\n    embeddings_file = ""data/GloVe/glove.the.50d.txt""\n    embeddings = load_embeddings(embeddings_file, \'glove\')\n    config = {\n        ""embedding_file"": embeddings_file,\n        ""embedding_format"": ""glove""\n    }\n    some_vocab = Vocab(vocab=embeddings.vocabulary)\n    some_vocab(\'foo\')\n    shared_resources = SharedResources(some_vocab, config, embeddings)\n\n    import tempfile\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        path = tmp_dir + ""_resources""\n        shared_resources.store(path)\n\n        new_shared_resources = SharedResources()\n        new_shared_resources.load(path)\n\n        type_a, type_b = type(new_shared_resources.vocab), type(shared_resources.vocab)\n        assert type_a == type_b\n\n        for k in new_shared_resources.vocab.__dict__:\n            assert new_shared_resources.vocab.__dict__[k] == shared_resources.vocab.__dict__[k]\n        assert new_shared_resources.config == shared_resources.config\n        assert new_shared_resources.embeddings.lookup.shape == embeddings.lookup.shape\n        assert np.array_equal(new_shared_resources.embeddings.get(b""the""), embeddings.get(b""the""))\n'"
tests/jack/test_embeddings.py,0,"b'from jack.io.embeddings import load_embeddings\nimport numpy as np\n\n\ndef test_memory_map_dir():\n    import tempfile\n    from jack.io.embeddings.memory_map import save_as_memory_map_dir, load_memory_map_dir\n    embeddings_file = ""data/GloVe/glove.the.50d.txt""\n    embeddings = load_embeddings(embeddings_file, \'glove\')\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        mem_map_dir = tmp_dir + ""/glove.the.50d.memmap""\n        save_as_memory_map_dir(mem_map_dir, embeddings)\n        loaded_embeddings = load_memory_map_dir(mem_map_dir)\n        assert loaded_embeddings.shape == embeddings.shape\n        assert len(loaded_embeddings.vocabulary) == 1\n        assert loaded_embeddings.vocabulary[""the""] == 0\n        assert ""foo"" not in loaded_embeddings.vocabulary\n        assert np.isclose(loaded_embeddings.get(""the""), embeddings.get(""the""), 1.e-5).all()\n'"
tests/test_results/rename_recursively.py,0,"b""import sys\nimport os\n\npath = sys.argv[1]\nexecute = False\ntry:\n    execute = sys.argv[2] == '1'\nexcept:\n    pass\n\nroot_dir = os.getcwd()\n\nfiles = []\ndirs = []\nfor root, directories, filenames in os.walk(path):\n    for filename in filenames:\n            dirs.append(os.path.join(root_dir, root))\n            files.append(os.path.join(root_dir, root, filename))\n\n\nfor f in files:\n    if 'expected_results.txt' in f:\n        if execute:\n            os.remove(f)\n\nfor f, d in zip(files, dirs):\n    if not 'expected_results.txt' in f:\n        if execute:\n            os.rename(f,os.path.join(d,'expected_results.txt'))\n        else:\n            print('{0} --> {1}'.format(f,\n                os.path.join(d,'expected_results.txt')))\n"""
jack/io/embeddings/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom jack.io.embeddings.embeddings import Embeddings, load_embeddings\nfrom jack.io.embeddings.glove import load_glove\n\n__all__ = [\n    'Embeddings',\n    'load_embeddings'\n    'load_word2vec',\n    'get_word2vec_vocabulary',\n    'load_glove',\n]\n"""
jack/io/embeddings/embeddings.py,0,"b'# -*- coding: utf-8 -*-\nimport os\nimport pickle\nimport zipfile\n\nimport yaml\n\nfrom jack.io.embeddings.fasttext import load_fasttext\nfrom jack.io.embeddings.glove import load_glove\nfrom jack.io.embeddings.word_to_vec import load_word2vec\n\n\nclass Embeddings:\n    """"""Wraps Vocabulary and embedding matrix to do lookups""""""\n\n    def __init__(self, vocabulary: dict, lookup, filename: str = None, emb_format: str = None):\n        """"""\n        Args:\n            vocabulary:\n            lookup:\n            filename:\n        """"""\n        self.filename = filename\n        self.vocabulary = vocabulary\n        self.lookup = lookup\n        self.emb_format = emb_format\n\n    def get(self, word, default=None):\n        _id = None\n        if self.vocabulary is not None:\n            _id = self.vocabulary.get(word, None)\n        # Handling OOV words - Note: lookup[None] would return entire lookup table\n        return self.lookup[_id] if _id is not None else default\n\n    def __call__(self, word):\n        return self.get(word)\n\n    @property\n    def shape(self):\n        return self.lookup.shape\n\n    def store(self, path):\n        if not os.path.exists(path):\n            os.mkdir(path)\n        if self.filename is None:\n            self.filename = os.path.join(path, ""emb.pkl"")\n            self.emb_format = \'pkl\'\n            with open(self.filename, ""wb"") as f:\n                pickle.dump(self, f)\n        conf_file = os.path.join(path, ""config.yaml"")\n        with open(conf_file, ""w"") as f:\n            yaml.dump({""embedding_file"": self.filename, ""emb_format"": self.emb_format}, f)\n\n    @staticmethod\n    def from_config(conf_file):\n        with open(conf_file, ""r"") as f:\n            config = yaml.load(f)\n        if config[""embedding_file""] is not None:\n            return load_embeddings(config[""embedding_file""], typ=config.get(""emb_format"", None))\n\n    @staticmethod\n    def from_dir(dir):\n        with open(os.path.join(dir, ""config.yaml""), ""r"") as f:\n            config = yaml.load(f)\n        if config[""embedding_file""] is not None:\n            return load_embeddings(config[""embedding_file""], typ=config.get(""emb_format"", None))\n\n\ndef load_embeddings(file, typ=\'glove\', **options):\n    """"""\n    Loads either GloVe or word2vec embeddings and wraps it into Embeddings\n\n    Args:\n        file: string, path to a file like ""GoogleNews-vectors-negative300.bin.gz"" or ""glove.42B.300d.zip""\n        typ: string, either ""word2vec"", ""glove"", ""fasttext"", ""mem_map"" or ""pkl""\n        options: dict, other options.\n    Returns:\n        Embeddings object, wrapper class around Vocabulary embedding matrix.\n    """"""\n    type_set = {""word2vec"", ""glove"", ""fasttext"", ""memory_map_dir"", ""pkl""}\n    assert typ.lower() in type_set, ""so far only {} foreseen"".format(\', \'.join(type_set))\n\n    if typ.lower() == ""word2vec"":\n        return Embeddings(*load_word2vec(file, **options))\n\n    elif typ.lower() == ""glove"":\n        if file.endswith(\'.txt\'):\n            with open(file, \'rb\') as f:\n                return Embeddings(*load_glove(f), filename=file, emb_format=typ)\n        elif file.endswith(\'.zip\'):\n            with zipfile.ZipFile(file) as zf:\n                txtfile = file.split(\'/\')[-1][:-4] + \'.txt\'\n                with zf.open(txtfile, \'r\') as f:\n                    return Embeddings(*load_glove(f), filename=file, emb_format=typ)\n        else:\n            raise NotImplementedError\n\n    elif typ.lower() == ""fasttext"":\n        with open(file, \'rb\') as f:\n            return Embeddings(*load_fasttext(f), filename=file, emb_format=typ)\n\n    elif typ.lower() == ""memory_map_dir"":\n        from jack.io.embeddings.memory_map import load_memory_map_dir\n        return load_memory_map_dir(file)\n\n    elif typ.lower() == ""pkl"":\n        with open(file, \'rb\') as f:\n            return pickle.load(f)\n\n    else:\n        raise ValueError(""Unknown type: {}"".format(type))\n'"
jack/io/embeddings/fasttext.py,0,"b'# -*- coding: utf-8 -*-\n\nimport logging\n\nimport numpy as np\n\nlogger = logging.getLogger(__name__)\n\n\ndef load_fasttext(stream, vocab=None):\n    """"""Loads fastText file and merges it if optional vocabulary\n    Args:\n        stream (iterable): An opened filestream to the fastText file.\n        vocab (dict=None): Word2idx dict of existing vocabulary.\n    Returns:\n        return_vocab (Vocabulary), lookup (matrix); Vocabulary contains the\n                     word2idx and the matrix contains the embedded words.\n    """"""\n    logger.info(\'Loading fastText vectors ..\')\n\n    word2idx = {}\n    vec_n, vec_size = map(int, stream.readline().split())\n    lookup = np.empty([vocab.get_size() if vocab is not None else vec_n, vec_size], dtype=np.float)\n    n = 0\n    for line in stream:\n        word, vec = line.rstrip().split(maxsplit=1)\n        if vocab is None or word in vocab and word not in word2idx:\n            word = word.decode(\'utf-8\')\n            idx = len(word2idx)\n            word2idx[word] = idx\n            # if idx > np.size(lookup, axis=0) - 1:\n            #    lookup.resize([lookup.shape[0] + 500000, lookup.shape[1]])\n            lookup[idx] = np.fromstring(vec, sep=\' \')\n        n += 1\n    # lookup.resize([len(word2idx), dim])\n    logger.info(\'Loading fastText vectors completed.\')\n    return word2idx, lookup\n'"
jack/io/embeddings/glove.py,0,"b'# -*- coding: utf-8 -*-\n\nimport logging\n\nimport numpy as np\n\nlogger = logging.getLogger(__name__)\n\n\ndef load_glove(stream, vocab=None):\n    """"""Loads GloVe file and merges it if optional vocabulary\n    Args:\n        stream (iterable): An opened filestream to the GloVe file.\n        vocab (dict=None): Word2idx dict of existing vocabulary.\n    Returns:\n        return_vocab (Vocabulary), lookup (matrix); Vocabulary contains the\n                     word2idx and the matrix contains the embedded words.\n    """"""\n    logger.info(\'Loading GloVe vectors ..\')\n\n    word2idx = {}\n    first_line = stream.readline()\n    dim = len(first_line.split()) - 1\n    lookup = np.empty([500000, dim], dtype=np.float)\n    lookup[0] = np.fromstring(first_line.split(maxsplit=1)[1], sep=\' \')\n    word2idx[first_line.split(maxsplit=1)[0].decode(\'utf-8\')] = 0\n    n = 1\n    for line in stream:\n        word, vec = line.rstrip().split(maxsplit=1)\n        if vocab is None or word in vocab and word not in word2idx:\n            word = word.decode(\'utf-8\')\n            idx = len(word2idx)\n            word2idx[word] = idx\n            if idx > np.size(lookup, axis=0) - 1:\n                lookup.resize([lookup.shape[0] + 500000, lookup.shape[1]])\n            lookup[idx] = np.fromstring(vec, sep=\' \')\n        n += 1\n    lookup.resize([len(word2idx), dim])\n    logger.info(\'Loading GloVe vectors completed.\')\n    return word2idx, lookup\n'"
jack/io/embeddings/memory_map.py,0,"b'# -*- coding: utf-8 -*-\n\nimport json\nimport os\n\nimport numpy as np\n\nfrom jack.io.embeddings import Embeddings\n\n\ndef load_memory_map_dir(directory: str) -> Embeddings:\n    """"""\n    Loads embeddings from a memory map directory to allow lazy loading (and reduce the memory usage).\n    Args:\n        directory: a file prefix. This function loads two files in the directory: a meta json file with shape information\n        and the vocabulary, and the actual memory map file.\n\n    Returns:\n        Embeddings object with a lookup matrix that is backed by a memory map.\n\n    """"""\n    meta_file = os.path.join(directory, ""meta.json"")\n    mem_map_file = os.path.join(directory, ""memory_map"")\n    with open(meta_file, ""r"") as f:\n        meta = json.load(f)\n    shape = tuple(meta[\'shape\'])\n    vocab = meta[\'vocab\']\n    mem_map = np.memmap(mem_map_file, dtype=\'float32\', mode=\'r+\', shape=shape)\n    result = Embeddings(vocab, mem_map, filename=directory, emb_format=""memory_map_dir"")\n    return result\n\n\ndef save_as_memory_map_dir(directory: str, emb: Embeddings):\n    """"""\n    Saves the given embeddings as memory map file and corresponding meta data in a directory.\n    Args:\n        directory: the directory to store the memory map file in (called `memory_map`) and the meta file (called\n        `meta.json` that stores the shape of the memory map and the actual vocabulary.\n        emb: the embeddings to store.\n    """"""\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    meta_file = os.path.join(directory, ""meta.json"")\n    mem_map_file = os.path.join(directory, ""memory_map"")\n    with open(meta_file, ""w"") as f:\n        json.dump({\n            ""vocab"": emb.vocabulary,\n            ""shape"": emb.shape\n        }, f)\n    mem_map = np.memmap(mem_map_file, dtype=\'float32\', mode=\'w+\', shape=emb.shape)\n    mem_map[:] = emb.lookup[:]\n    mem_map.flush()\n    del mem_map\n'"
jack/io/embeddings/word_to_vec.py,0,"b'# -*- coding: utf-8 -*-\n\nimport gzip\nimport numpy as np\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\ndef load_word2vec(filename, vocab=None, normalise=True):\n    """"""Loads a word2vec file and merges existing vocabulary.\n\n    Args:\n        filename (string): Path to the word2vec file.\n        vocab (Vocabulary=None): Existing vocabulary to be merged.\n        normalise (bool=True): If the word embeddings should be unit\n                  normalized or not.\n    Returns:\n        return_vocab (dict), lookup (matrix): The dict is a word2idx dict and\n        the lookup matrix is the matrix of embedding vectors.\n    """"""\n    logger.info(""Loading word2vec vectors .."")\n    with gzip.open(filename, \'rb\') as f:\n        vec_n, vec_size = map(int, f.readline().split())\n        byte_size = vec_size * 4\n        lookup = np.empty([vocab.get_size() if vocab is not None else vec_n, vec_size], dtype=np.float32)\n        word2idx = {}\n        idx = 0\n        for n in range(vec_n):\n            word = b\'\'\n            while True:\n                c = f.read(1)\n                if c == b\' \':\n                    break\n                else:\n                    word += c\n\n            word = word.decode(\'utf-8\')\n            vector = np.fromstring(f.read(byte_size), dtype=np.float32)\n            if vocab is None or vocab.contains_word(word):\n                word2idx[word] = idx\n                lookup[idx] = _normalise(vector) if normalise else vector\n                idx += 1\n\n    lookup.resize([idx, vec_size])\n    logger.info(\'Loading word2vec vectors completed.\')\n    return word2idx, lookup\n\n\ndef _normalise(x):\n    """"""Unit normalize x with L2 norm.""""""\n    return (1.0 / np.linalg.norm(x, ord=2)) * x\n\n\ndef get_word2vec_vocabulary(fname):\n    """"""Loads word2vec file and returns the vocabulary as dict word2idx.""""""\n    voc, _ = load_word2vec(fname)\n    return voc\n\n\nif __name__ == ""__main__"":\n    pickle_tokens = False\n    vocab, _ = load_word2vec(\'../../data/word2vec/GoogleNews-vectors-negative300.bin.gz\')\n\n    # pickle token set\n    if pickle_tokens:\n        import pickle\n        w2v_words = set(vocab.get_all_words())\n        pickle.dump(w2v_words, open(\'./data/w2v_tokens.pickle\', \'wb\'))\n'"
jack/readers/classification/__init__.py,0,b''
jack/readers/classification/shared.py,9,"b'# -*- coding: utf-8 -*-\n\nfrom typing import NamedTuple\n\nfrom jack.core import *\nfrom jack.core.data_structures import *\nfrom jack.core.tensorflow import TFModelModule\nfrom jack.readers.classification import util\nfrom jack.util import preprocessing\nfrom jack.util.map import numpify\n\nlogger = logging.getLogger(__name__)\n\n\nclass AbstractSingleSupportClassificationModel(TFModelModule):\n    def __init__(self, shared_resources):\n        self.shared_resources = shared_resources\n        self.vocab = self.shared_resources.vocab\n        self.config = self.shared_resources.config\n\n        super(AbstractSingleSupportClassificationModel, self).__init__(shared_resources)\n\n    @property\n    def input_ports(self) -> List[TensorPort]:\n        if self.shared_resources.embeddings is not None:\n            return [Ports.Input.emb_support, Ports.Input.emb_question,\n                    Ports.Input.support, Ports.Input.question,\n                    Ports.Input.support_length, Ports.Input.question_length,\n                    # character information\n                    Ports.Input.word_chars, Ports.Input.word_char_length,\n                    Ports.Input.question_batch_words, Ports.Input.support_batch_words,\n                    Ports.is_eval]\n        else:\n            return [Ports.Input.support, Ports.Input.question,\n                    Ports.Input.support_length, Ports.Input.question_length,\n                    # character information\n                    Ports.Input.word_chars, Ports.Input.word_char_length,\n                    Ports.Input.question_batch_words, Ports.Input.support_batch_words, Ports.is_eval]\n\n    @property\n    def output_ports(self) -> List[TensorPort]:\n        return [Ports.Prediction.logits, Ports.Prediction.candidate_index]\n\n    @property\n    def training_input_ports(self) -> List[TensorPort]:\n        return [Ports.Prediction.logits, Ports.Target.target_index]\n\n    @property\n    def training_output_ports(self) -> List[TensorPort]:\n        return [Ports.loss]\n\n    def create_output(self, shared_resources: SharedResources, input_tensors) -> Mapping[TensorPort, tf.Tensor]:\n        tensors = TensorPortTensors(input_tensors)\n        embedded_question = []\n        embedded_support = []\n\n        input_size = shared_resources.config.get(\'repr_dim_task_embedding\', 0)\n\n        if input_size > 0:\n            vocab_size = len(shared_resources.vocab)\n            e = tf.get_variable(\'embeddings\', [vocab_size, input_size],\n                                tf.float32, initializer=tf.random_normal_initializer(0.0, 0.1), trainable=True)\n            embedded_question.append(tf.nn.embedding_lookup(e, input_tensors[Ports.Input.question]))\n            embedded_support.append(tf.nn.embedding_lookup(e, input_tensors[Ports.Input.support]))\n\n        if shared_resources.embeddings is not None:\n            embedded_support.append(input_tensors[Ports.Input.emb_support])\n            embedded_question.append(input_tensors[Ports.Input.emb_question])\n            input_size += shared_resources.embeddings.shape[-1]\n\n        if len(embedded_question) > 1:\n            embedded_question = tf.concat(embedded_question, 2)\n            embedded_support = tf.concat(embedded_support, 2)\n        else:\n            embedded_question = embedded_question[0]\n            embedded_support = embedded_support[0]\n\n        embedded_question.set_shape([None, None, input_size])\n        embedded_support.set_shape([None, None, input_size])\n\n        logits = self.forward_pass(shared_resources, embedded_question, embedded_support,\n                                   len(self.shared_resources.answer_vocab), tensors)\n\n        predictions = tf.argmax(logits, 1, name=\'prediction\')\n\n        return {\n            Ports.Prediction.logits: logits,\n            Ports.Prediction.candidate_index: predictions\n        }\n\n    @abstractmethod\n    def forward_pass(self, shared_resources, embedded_question, embedded_support, num_classes, tensors):\n        """"""Takes embedded support and question and produces logits""""""\n        raise NotImplementedError\n\n    def create_training_output(self, shared_resources: SharedResources, input_tensors):\n        tensors = TensorPortTensors(input_tensors)\n        return {\n            Ports.loss: tf.losses.sparse_softmax_cross_entropy(logits=tensors.logits, labels=tensors.target_index)\n        }\n\n\nMCAnnotation = NamedTuple(\'MCAnnotation\', [\n    (\'question_tokens\', List[str]),\n    (\'question_ids\', List[int]),\n    (\'question_length\', int),\n    (\'support_tokens\', List[str]),\n    (\'support_ids\', List[int]),\n    (\'support_length\', int),\n    (\'answer\', Optional[int]),\n    (\'id\', Optional[int]),\n])\n\n\nclass ClassificationSingleSupportInputModule(OnlineInputModule[MCAnnotation]):\n    def setup(self):\n        self.vocab = self.shared_resources.vocab\n        self.config = self.shared_resources.config\n        self.embeddings = self.shared_resources.embeddings\n        if self.embeddings is not None:\n            self.__default_vec = np.zeros([self.embeddings.shape[-1]])\n\n    def setup_from_data(self, data: Iterable[Tuple[QASetting, List[Answer]]]):\n        vocab = self.shared_resources.vocab\n        if not vocab.frozen:\n            preprocessing.fill_vocab(\n                (q for q, _ in data), vocab, lowercase=self.shared_resources.config.get(\'lowercase\', True))\n            vocab.freeze()\n        if not hasattr(self.shared_resources, \'answer_vocab\') or not self.shared_resources.answer_vocab.frozen:\n            self.shared_resources.answer_vocab = util.create_answer_vocab(\n                qa_settings=(q for q, _ in data), answers=(a for _, ass in data for a in ass))\n            self.shared_resources.answer_vocab.freeze()\n        self.shared_resources.char_vocab = preprocessing.char_vocab_from_vocab(self.shared_resources.vocab)\n\n    @property\n    def training_ports(self) -> List[TensorPort]:\n        return [Ports.Target.target_index]\n\n    @property\n    def output_ports(self) -> List[TensorPort]:\n        """"""Defines the outputs of the InputModule""""""\n        if self.shared_resources.embeddings is not None:\n            return [Ports.Input.emb_support, Ports.Input.emb_question,\n                    Ports.Input.support, Ports.Input.question,\n                    Ports.Input.support_length, Ports.Input.question_length,\n                    Ports.Input.sample_id,\n                    # character information\n                    Ports.Input.word_chars, Ports.Input.word_char_length,\n                    Ports.Input.question_batch_words, Ports.Input.support_batch_words,\n                    Ports.is_eval]\n        else:\n            return [Ports.Input.support, Ports.Input.question,\n                    Ports.Input.support_length, Ports.Input.question_length,\n                    Ports.Input.sample_id,\n                    # character information\n                    Ports.Input.word_chars, Ports.Input.word_char_length,\n                    Ports.Input.question_batch_words, Ports.Input.support_batch_words,\n                    Ports.is_eval]\n\n    def preprocess(self, questions: List[QASetting],\n                   answers: Optional[List[List[Answer]]] = None,\n                   is_eval: bool = False) -> List[MCAnnotation]:\n        if answers is None:\n            answers = [None] * len(questions)\n        preprocessed = []\n        if len(questions) > 1000:\n            bar = progressbar.ProgressBar(\n                max_value=len(questions),\n                widgets=[\' [\', progressbar.Timer(), \'] \', progressbar.Bar(), \' (\', progressbar.ETA(), \') \'])\n            for i, (q, a) in bar(enumerate(zip(questions, answers))):\n                preprocessed.append(self.preprocess_instance(i, q, a))\n        else:\n            for i, (q, a) in enumerate(zip(questions, answers)):\n                preprocessed.append(self.preprocess_instance(i, q, a))\n\n        return preprocessed\n\n    def preprocess_instance(self, idd: int, question: QASetting,\n                            answers: Optional[List[Answer]] = None) -> MCAnnotation:\n        has_answers = answers is not None\n\n        q_tokenized, q_ids, q_length, _, _ = preprocessing.nlp_preprocess(\n            question.question, self.shared_resources.vocab,\n            lowercase=self.shared_resources.config.get(\'lowercase\', True))\n        s_tokenized, s_ids, s_length, _, _ = preprocessing.nlp_preprocess(\n            question.support[0], self.shared_resources.vocab,\n            lowercase=self.shared_resources.config.get(\'lowercase\', True))\n\n        return MCAnnotation(\n            question_tokens=q_tokenized,\n            question_ids=q_ids,\n            question_length=q_length,\n            support_tokens=s_tokenized,\n            support_ids=s_ids,\n            support_length=s_length,\n            answer=self.shared_resources.answer_vocab(answers[0].text) if has_answers else 0,\n            id=idd\n        )\n\n    def create_batch(self, annotations: List[MCAnnotation],\n                     is_eval: bool, with_answers: bool) -> Mapping[TensorPort, np.ndarray]:\n        # also add character information\n        word_chars, word_lengths, tokens, vocab, rev_vocab = \\\n            preprocessing.unique_words_with_chars(\n                [a.question_tokens for a in annotations] + [a.support_tokens for a in annotations],\n                self.shared_resources.char_vocab)\n        question_words, support_words = tokens[:len(annotations)], tokens[len(annotations):]\n\n        q_lengths = [a.question_length for a in annotations]\n        s_lengths = [a.support_length for a in annotations]\n        xy_dict = {\n            Ports.Input.question_length: q_lengths,\n            Ports.Input.support_length: s_lengths,\n            Ports.Input.sample_id: [a.id for a in annotations],\n            Ports.Input.word_chars: word_chars,\n            Ports.Input.word_char_length: word_lengths,\n            Ports.Input.question_batch_words: question_words,\n            Ports.Input.support_batch_words: support_words,\n            Ports.is_eval: is_eval,\n            Ports.Input.support: [a.support_ids for a in annotations],\n            Ports.Input.question: [a.question_ids for a in annotations]\n        }\n\n        if self.embeddings is not None:\n            emb_support = np.zeros([len(annotations), max(s_lengths), self.embeddings.shape[-1]])\n            emb_question = np.zeros([len(annotations), max(q_lengths), self.embeddings.shape[-1]])\n            for i, a in enumerate(annotations):\n                for j, t in enumerate(a.support_tokens):\n                    emb_support[i, j] = self.embeddings.get(t, self.__default_vec)\n                for j, t in enumerate(a.question_tokens):\n                    emb_question[i, j] = self.embeddings.get(t, self.__default_vec)\n\n            xy_dict[Ports.Input.emb_support] = emb_support\n            xy_dict[Ports.Input.emb_question] = emb_question\n\n        if with_answers:\n            xy_dict[Ports.Target.target_index] = [a.answer for a in annotations]\n        return numpify(xy_dict)\n\n\ndef _np_softmax(x):\n    """"""Compute softmax values for each sets of scores in x.""""""\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\n\nclass SimpleClassificationOutputModule(OutputModule):\n    def __init__(self, shared_resources=None):\n        self._shared_resources = shared_resources\n\n    def setup(self):\n        pass\n\n    @property\n    def input_ports(self) -> List[TensorPort]:\n        return [Ports.Prediction.logits]\n\n    def __call__(self, questions: List[QASetting], tensors: Mapping[TensorPort, np.ndarray]) -> List[Answer]:\n        # len(inputs) == batch size\n        # logits: [batch_size, max_num_candidates]\n        logits = tensors[Ports.Prediction.logits]\n        winning_indices = np.argmax(logits, axis=1)\n        result = []\n        for index_in_batch, question in enumerate(questions):\n            winning_index = winning_indices[index_in_batch]\n            score = _np_softmax(logits[index_in_batch])[winning_index]\n            if self._shared_resources is not None and hasattr(self._shared_resources, \'answer_vocab\'):\n                ans = Answer(self._shared_resources.answer_vocab.get_sym(winning_index), score=score)\n            else:\n                ans = Answer(question.candidates[winning_index], score=score)\n            result.append([ans])\n        return result\n'"
jack/readers/classification/util.py,0,"b'""""""Shared utilities for multiple choice.""""""\nfrom typing import Iterable\n\nfrom jack.core.data_structures import QASetting, Answer\nfrom jack.util.vocab import Vocab\n\n\ndef create_answer_vocab(qa_settings: Iterable[QASetting] = None, answers: Iterable[Answer] = None):\n    vocab = Vocab(unk=None)\n    if qa_settings is not None:\n        for qa in qa_settings:\n            if qa.candidates:\n                for c in qa.candidates:\n                    vocab(c)\n    if answers is not None:\n        for a in answers:\n            vocab(a.text)\n    return vocab\n\n\ndef candidate_one_hot(candidates, answer_str):\n    return [1.0 if candidates[answer_str] == cand else 0.0 for cand in candidates]\n'"
jack/readers/extractive_qa/__init__.py,0,b'\n\n'
jack/readers/extractive_qa/shared.py,0,"b'""""""\nThis file contains reusable modules for extractive QA models and ports\n""""""\nfrom typing import NamedTuple\n\nfrom jack.core import *\nfrom jack.readers.extractive_qa.util import prepare_data\nfrom jack.util import preprocessing\nfrom jack.util.map import numpify\nfrom jack.util.preprocessing import sort_by_tfidf\n\nlogger = logging.getLogger(__name__)\n\n\nclass XQAPorts:\n    # When feeding embeddings directly\n    emb_question = Ports.Input.emb_question\n    question_length = Ports.Input.question_length\n    emb_support = Ports.Input.emb_support\n    support_length = Ports.Input.support_length\n    support2question = Ports.Input.support2question\n\n    # but also ids, for char-based embeddings\n    word_chars = Ports.Input.word_chars\n    word_char_length = Ports.Input.word_char_length\n    question_batch_words = Ports.Input.question_batch_words\n    support_batch_words = Ports.Input.support_batch_words\n\n    is_eval = Ports.is_eval\n\n    # This feature is model specific and thus, not part of the conventional Ports\n    word_in_question = TensorPort(np.float32, [None, None], ""word_in_question"",\n                                  ""Represents a 1/0 feature for all context tokens denoting""\n                                  "" whether it is part of the question or not"",\n                                  ""[Q, support_length]"")\n\n    correct_start = TensorPortWithDefault(np.array([0], np.int32), [None], ""correct_start"",\n                                          ""Represents the correct start of the span which is given to the""\n                                          ""model during training for use to predicting end."",\n                                          ""[A]"")\n\n    # output ports\n    start_scores = Ports.Prediction.start_scores\n    end_scores = Ports.Prediction.end_scores\n    answer_span = Ports.Prediction.answer_span\n    token_offsets = TensorPort(np.int32, [None, None], ""token_offsets"",\n                               ""Character index of tokens in support."",\n                               ""[S, support_length]"")\n    selected_support = TensorPort(np.int32, [None], ""selected_support"",\n                                  ""Selected support based on TF IDF with question"", ""[num_support]"")\n\n    # ports used during training\n    answer2support_training = Ports.Input.answer2support\n    answer_span_target = Ports.Target.answer_span\n\n\nXQAAnnotation = NamedTuple(\'XQAAnnotation\', [\n    (\'question_tokens\', List[str]),\n    (\'question_ids\', List[int]),\n    (\'question_length\', int),\n    (\'support_tokens\', List[List[str]]),\n    (\'support_ids\', List[List[int]]),\n    (\'support_length\', List[int]),\n    (\'word_in_question\', List[List[float]]),\n    (\'token_offsets\', List[List[int]]),\n    (\'answer_spans\', Optional[List[List[Tuple[int, int]]]]),\n    (\'selected_supports\', Optional[List[int]]),\n])\n\n\nclass XQAInputModule(OnlineInputModule[XQAAnnotation]):\n    _output_ports = [XQAPorts.emb_question, XQAPorts.question_length,\n                     XQAPorts.emb_support, XQAPorts.support_length,\n                     XQAPorts.support2question,\n                     # char\n                     XQAPorts.word_chars, XQAPorts.word_char_length,\n                     XQAPorts.question_batch_words, XQAPorts.support_batch_words,\n                     # features\n                     XQAPorts.word_in_question,\n                     # optional, only during training\n                     XQAPorts.correct_start, XQAPorts.answer2support_training,\n                     XQAPorts.is_eval,\n                     # for output module\n                     XQAPorts.token_offsets, XQAPorts.selected_support]\n    _training_ports = [XQAPorts.answer_span_target, XQAPorts.answer2support_training]\n\n    def setup_from_data(self, data: Iterable[Tuple[QASetting, List[Answer]]]):\n        # create character vocab + word lengths + char ids per word\n        if not self.shared_resources.vocab.frozen:\n            preprocessing.fill_vocab((q for q, _ in data), self.shared_resources.vocab,\n                                     self.shared_resources.config.get(""lowercase"", False))\n            self.shared_resources.vocab.freeze()\n        self.shared_resources.char_vocab = preprocessing.char_vocab_from_vocab(self.shared_resources.vocab)\n\n    def setup(self):\n        self._rng = random.Random(1)\n        self.vocab = self.shared_resources.vocab\n        self.config = self.shared_resources.config\n        self.embeddings = self.shared_resources.embeddings\n        self.__default_vec = np.zeros([self.embeddings.shape[-1]])\n        self.char_vocab = self.shared_resources.char_vocab\n\n    @property\n    def output_ports(self) -> List[TensorPort]:\n        return self._output_ports\n\n    @property\n    def training_ports(self) -> List[TensorPort]:\n        return self._training_ports\n\n    def preprocess(self, questions: List[QASetting],\n                   answers: Optional[List[List[Answer]]] = None,\n                   is_eval: bool = False) -> List[XQAAnnotation]:\n\n        if answers is None:\n            answers = [None] * len(questions)\n        preprocessed = []\n        if len(questions) > 1000:\n            bar = progressbar.ProgressBar(\n                max_value=len(questions),\n                widgets=[\' [\', progressbar.Timer(), \'] \', progressbar.Bar(), \' (\', progressbar.ETA(), \') \'])\n            for q, a in bar(zip(questions, answers)):\n                preprocessed.append(self.preprocess_instance(q, a))\n        else:\n            for q, a in zip(questions, answers):\n                preprocessed.append(self.preprocess_instance(q, a))\n\n        return preprocessed\n\n    def preprocess_instance(self, question: QASetting, answers: Optional[List[Answer]] = None) -> XQAAnnotation:\n        has_answers = answers is not None\n\n        q_tokenized, q_ids, _, q_length, s_tokenized, s_ids, _, s_length, \\\n        word_in_question, token_offsets, answer_spans = prepare_data(\n            question, answers, self.vocab, self.config.get(""lowercase"", False),\n            with_answers=has_answers, max_support_length=self.config.get(""max_support_length"", None))\n\n        max_num_support = self.config.get(""max_num_support"", len(question.support))  # take all per default\n\n        # take max supports by TF-IDF (we subsample to max_num_support in create batch)\n        # following https://arxiv.org/pdf/1710.10723.pdf\n        if len(question.support) > 1:\n            scores = sort_by_tfidf(\' \'.join(q_tokenized), [\' \'.join(s) for s in s_tokenized])\n            selected_supports = [s_idx for s_idx, _ in scores[:max_num_support]]\n            s_tokenized = [s_tokenized[s_idx] for s_idx in selected_supports]\n            s_ids = [s_ids[s_idx] for s_idx in selected_supports]\n            s_length = [s_length[s_idx] for s_idx in selected_supports]\n            word_in_question = [word_in_question[s_idx] for s_idx in selected_supports]\n            token_offsets = [token_offsets[s_idx] for s_idx in selected_supports]\n            answer_spans = [answer_spans[s_idx] for s_idx in selected_supports]\n        else:\n            selected_supports = list(range(len(question.support)))\n\n        return XQAAnnotation(\n            question_tokens=q_tokenized,\n            question_ids=q_ids,\n            question_length=q_length,\n            support_tokens=s_tokenized,\n            support_ids=s_ids,\n            support_length=s_length,\n            word_in_question=word_in_question,\n            token_offsets=token_offsets,\n            answer_spans=answer_spans if has_answers else None,\n            selected_supports=selected_supports,\n        )\n\n    def create_batch(self, annotations: List[XQAAnnotation], is_eval: bool, with_answers: bool) \\\n            -> Mapping[TensorPort, np.ndarray]:\n\n        q_tokenized = [a.question_tokens for a in annotations]\n        question_lengths = [a.question_length for a in annotations]\n\n        max_training_support = self.config.get(\'max_training_support\', 2)\n        s_tokenized = []\n        support_lengths = []\n        wiq = []\n        offsets = []\n        support2question = []\n        support_ids = []\n        # aligns with support2question, used in output module to get correct index to original set of supports\n        selected_support = []\n        all_spans = []\n        for i, a in enumerate(annotations):\n            all_spans.append([])\n            if len(a.support_tokens) > max_training_support > 0 and not is_eval:\n                # sample only 2 paragraphs and take first with double probability (the best) to speed\n                # things up. Following https://arxiv.org/pdf/1710.10723.pdf\n                is_done = False\n                any_answer = any(a.answer_spans)\n                # sample until there is at least one possible answer (if any)\n                while not is_done:\n                    selected = self._rng.sample(range(0, len(a.support_tokens) + 1), max_training_support + 1)\n                    if 0 in selected and 1 in selected:\n                        selected = [s - 1 for s in selected if s > 0]\n                    else:\n                        selected = [max(0, s - 1) for s in selected[:max_training_support]]\n                    is_done = not any_answer or any(a.answer_spans[s] for s in selected)\n            else:\n                selected = set(range(len(a.support_tokens)))\n            for s in selected:\n                s_tokenized.append(a.support_tokens[s])\n                support_lengths.append(a.support_length[s])\n                wiq.append(a.word_in_question[s])\n                offsets.append(a.token_offsets[s])\n                selected_support.append(a.selected_supports[s])\n                support_ids.append(a.support_ids[s])\n                support2question.append(i)\n                if with_answers:\n                    all_spans[-1].append(a.answer_spans[s])\n\n        word_chars, word_lengths, batch_word_ids, batch_vocab, batch_rev_vocab = \\\n            preprocessing.unique_words_with_chars(q_tokenized + s_tokenized, self.char_vocab)\n\n        emb_support = np.zeros([len(support_lengths), max(support_lengths), self.embeddings.shape[-1]])\n        emb_question = np.zeros([len(question_lengths), max(question_lengths), self.embeddings.shape[-1]])\n\n        for i, a in enumerate(annotations):\n            for j, t in enumerate(a.question_tokens):\n                emb_question[i, j] = self.embeddings.get(t, self.__default_vec)\n        for k, s_ids in enumerate(s_tokenized):\n            for j, t in enumerate(s_ids):\n                emb_support[k, j] = self.embeddings.get(t, self.__default_vec)\n\n        output = {\n            XQAPorts.word_chars: word_chars,\n            XQAPorts.word_char_length: word_lengths,\n            XQAPorts.question_batch_words: batch_word_ids[:len(q_tokenized)],\n            XQAPorts.support_batch_words: batch_word_ids[len(q_tokenized):],\n            XQAPorts.emb_support: emb_support,\n            XQAPorts.support_length: support_lengths,\n            XQAPorts.emb_question: emb_question,\n            XQAPorts.question_length: question_lengths,\n            XQAPorts.word_in_question: wiq,\n            XQAPorts.support2question: support2question,\n            XQAPorts.is_eval: is_eval,\n            XQAPorts.token_offsets: offsets,\n            XQAPorts.selected_support: selected_support,\n            \'__vocab\': batch_vocab,\n            \'__rev_vocab\': batch_rev_vocab,\n        }\n\n        if with_answers:\n            spans = [s for a in all_spans for spans_per_support in a for s in spans_per_support]\n            span2support = []\n            support_idx = 0\n            for a in all_spans:\n                for spans_per_support in a:\n                    span2support.extend([support_idx] * len(spans_per_support))\n                    support_idx += 1\n            output.update({\n                XQAPorts.answer_span_target: [span for span in spans] if spans else np.zeros([0, 2], np.int32),\n                XQAPorts.correct_start: [] if is_eval else [span[0] for span in spans],\n                XQAPorts.answer2support_training: span2support,\n            })\n\n        # we can only numpify in here, because bucketing is not possible prior\n        batch = numpify(output, keys=[XQAPorts.word_chars,\n                                      XQAPorts.question_batch_words, XQAPorts.support_batch_words,\n                                      XQAPorts.word_in_question, XQAPorts.token_offsets])\n        return batch\n\n\ndef _np_softmax(x):\n    """"""Compute softmax values for each sets of scores in x.""""""\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\n\ndef get_answer_and_span(question, doc_idx, start, end, token_offsets, selected_support):\n    doc_idx = selected_support[doc_idx]\n    char_start = token_offsets[start]\n    char_end = token_offsets[end + 1] if end < len(token_offsets) - 1 else len(question.support[doc_idx])\n    answer = question.support[doc_idx][char_start: char_end]\n    answer = answer.rstrip()\n    char_end = char_start + len(answer)\n    return answer, doc_idx, (char_start, char_end)\n\n\nclass XQAOutputModule(OutputModule):\n    @property\n    def input_ports(self) -> List[TensorPort]:\n        return [XQAPorts.answer_span, XQAPorts.token_offsets,\n                XQAPorts.selected_support, XQAPorts.support2question,\n                XQAPorts.start_scores, XQAPorts.end_scores]\n\n    def __call__(self, questions, tensors: Mapping[TensorPort, np.ndarray]) -> Sequence[Sequence[Answer]]:\n        """"""Produces top-k answers for each question.""""""\n        tensors = TensorPortTensors(tensors)\n        topk = tensors.answer_span.shape[0] // len(questions)\n        all_answers = []\n        for n, q in enumerate(questions):\n            answers = []\n            doc_idx_map = [i for i, q_id in enumerate(tensors.support2question) if q_id == n]\n            for j in range(topk):\n                i = n * topk + j\n                doc_idx, start, end = tensors.answer_span[i]\n                score = (_np_softmax(tensors.start_scores[doc_idx_map[doc_idx]])[start] *\n                         _np_softmax(tensors.end_scores[doc_idx_map[doc_idx]])[end])\n                answer, doc_idx, span = get_answer_and_span(\n                    q, doc_idx, start, end, tensors.token_offsets[doc_idx_map[doc_idx]],\n                    [i for q_id, i in zip(tensors.support2question, tensors.selected_support) if q_id == n])\n                answers.append(Answer(answer, span=span, doc_idx=doc_idx, score=score))\n            all_answers.append(answers)\n\n        return all_answers\n\n'"
jack/readers/extractive_qa/util.py,0,"b'import random\nimport re\nfrom typing import List, Optional, Tuple\n\nfrom jack.core.data_structures import QASetting, Answer\nfrom jack.util import preprocessing\nfrom jack.util.vocab import Vocab\n\n__pattern = re.compile(\'\\w+|[^\\w\\s]\')\n\n\ndef tokenize(text):\n    return __pattern.findall(text)\n\n\ndef token_to_char_offsets(text, tokenized_text):\n    offsets = []\n    offset = 0\n    for t in tokenized_text:\n        offset = text.index(t, offset)\n        offsets.append(offset)\n        offset += len(t)\n    return offsets\n\n\ndef prepare_data(qa_setting: QASetting,\n                 answers: Optional[List[Answer]],\n                 vocab: Vocab,\n                 lowercase: bool = False,\n                 with_answers: bool = False,\n                 wiq_contentword: bool = False,\n                 spacy_nlp: bool = False,\n                 max_support_length: int = None,\n                 lemmatize=False,\n                 with_lemmas=False) \\\n        -> Tuple[List[str], List[int], Optional[List[int]], int,\n                 List[List[str]], List[List[int]], Optional[List[List[int]]], List[int],\n                 List[List[float]], List[List[int]], List[List[Tuple[int, int]]]]:\n    """"""Preprocesses a question and (optionally) answers:\n    The steps include tokenization, lower-casing, translation to IDs,\n    computing the word-in-question feature, computing token offsets,\n    truncating supports, and computing answer spans.\n    """"""\n    supports = qa_setting.support\n    question = qa_setting.question\n\n    question_tokens, question_ids, question_length, question_lemmas, _ = preprocessing.nlp_preprocess(\n        question, vocab, lowercase=lowercase, use_spacy=spacy_nlp,\n        lemmatize=lemmatize, with_lemmas=with_lemmas, with_tokens_offsets=False)\n    question_tokens_set = set(t.lower() for t in question_tokens)\n\n    preprocessed_supports = [\n        preprocessing.nlp_preprocess(\n            support, vocab, lowercase=lowercase, use_spacy=spacy_nlp,\n            lemmatize=lemmatize, with_lemmas=with_lemmas, with_tokens_offsets=True)\n        for support in supports]\n\n    all_support_tokens = [s[0] for s in preprocessed_supports]\n    all_support_ids = [s[1] for s in preprocessed_supports]\n    all_support_length = [s[2] for s in preprocessed_supports]\n    all_support_lemmas = [s[3] for s in preprocessed_supports]\n    all_token_offsets = [s[4] for s in preprocessed_supports]\n\n    rng = random.Random(12345)\n\n    all_word_in_question = []\n    if with_lemmas:\n        assert all_support_lemmas is not None\n        for support_lemmas in all_support_lemmas:\n            all_word_in_question.append([])\n            if with_lemmas:\n                for lemma in support_lemmas:\n                    all_word_in_question[-1].append(float(\n                        lemma in question_lemmas and (not wiq_contentword or (lemma.isalnum() and not lemma.is_stop))))\n    else:\n        for support_tokens in all_support_tokens:\n            all_word_in_question.append([])\n            for token in support_tokens:\n                all_word_in_question[-1].append(\n                    float(token.lower() in question_tokens_set and (not wiq_contentword or token.isalnum())))\n\n    all_answer_spans = []\n    for doc_idx, support_tokens in enumerate(all_support_tokens):\n        min_answer = len(support_tokens)\n        max_answer = 0\n        token_offsets = all_token_offsets[doc_idx]\n\n        answer_spans = []\n        if with_answers:\n            assert isinstance(answers, list)\n            for a in answers:\n                if a.doc_idx != doc_idx:\n                    continue\n\n                start = 0\n                while start < len(token_offsets) and token_offsets[start] < a.span[0]:\n                    start += 1\n\n                if start == len(token_offsets):\n                    continue\n\n                end = start\n                while end + 1 < len(token_offsets) and token_offsets[end + 1] < a.span[1]:\n                    end += 1\n\n                if (start, end) not in answer_spans:\n                    answer_spans.append((start, end))\n                    min_answer = min(min_answer, start)\n                    max_answer = max(max_answer, end)\n\n        # cut support whenever there is a maximum allowed length and recompute answer spans\n        support_length = all_support_length[doc_idx]\n        if max_support_length is not None and support_length > max_support_length > 0:\n            if max_answer < max_support_length:\n                # Find new start and end in the flattened support\n                new_start = 0\n                new_end = max_support_length\n            else:\n                offset = rng.randint(1, 11)\n                new_end = max_answer\n                new_start = max(0, min(min_answer, new_end + 2 * offset - max_support_length))\n                while new_end - new_start > max_support_length - 2 * offset:\n                    answer_spans = [(s, e) for s, e in answer_spans if e < new_end]\n                    new_end = max(answer_spans, key=lambda span: span[1])[1]\n                    new_start = max(0, min(min_answer, new_end + 2 * offset - max_support_length))\n                new_end = min(new_end + offset, support_length)\n                new_start = max(new_start - offset, 0)\n\n            # Crop support according to new start and end pointers\n            all_support_tokens[doc_idx] = support_tokens[new_start:new_end]\n            all_support_ids[doc_idx] = all_support_ids[doc_idx][new_start:new_end]\n            if with_lemmas:\n                all_support_lemmas[doc_idx] = all_support_lemmas[doc_idx][new_start:new_end]\n            answer_spans = [(s - new_start, e - new_start) for s, e in answer_spans]\n            all_word_in_question[doc_idx] = all_word_in_question[doc_idx][new_start:new_end]\n            all_support_length[doc_idx] = new_end - new_start\n            all_token_offsets[doc_idx] = token_offsets[new_start:new_end]\n        all_answer_spans.append(answer_spans)\n\n    return question_tokens, question_ids, question_lemmas, question_length, \\\n           all_support_tokens, all_support_ids, all_support_lemmas, all_support_length, \\\n           all_word_in_question, all_token_offsets, all_answer_spans\n'"
jack/readers/link_prediction/__init__.py,0,b''
jack/readers/link_prediction/models.py,13,"b'# -*- coding: utf-8 -*-\n\nfrom jack.core import *\nfrom jack.core.data_structures import *\nfrom jack.core.tensorflow import TFModelModule\nfrom jack.readers.link_prediction import scores\nfrom jack.util.map import numpify\n\n\nclass KnowledgeGraphEmbeddingInputModule(OnlineInputModule[List[List[int]]]):\n    def __init__(self, shared_resources):\n        self._kbp_rng = np.random.RandomState(0)\n        super(KnowledgeGraphEmbeddingInputModule, self).__init__(shared_resources)\n\n    def setup_from_data(self, data: Iterable[Tuple[QASetting, List[Answer]]]):\n        triples = [tuple(x[0].question.split()) for x in data]\n\n        entity_set = {s for [s, _, _] in triples} | {o for [_, _, o] in triples}\n        predicate_set = {p for [_, p, _] in triples}\n\n        entity_to_index = {entity: index for index, entity in enumerate(entity_set, start=1)}\n        predicate_to_index = {predicate: index for index, predicate in enumerate(predicate_set, start=1)}\n\n        self.shared_resources.entity_to_index = entity_to_index\n        self.shared_resources.predicate_to_index = predicate_to_index\n\n        self.shared_resources.nb_entities = max(self.shared_resources.entity_to_index.values()) + 1\n        self.shared_resources.nb_predicates = max(self.shared_resources.predicate_to_index.values()) + 1\n\n\n    def preprocess(self, questions: List[QASetting], answers: Optional[List[List[Answer]]] = None,\n                   is_eval: bool = False) -> List[List[int]]:\n        """"""Converts questions to triples.""""""\n        triples = []\n        for qa_setting in questions:\n            s, p, o = qa_setting.question.split()\n            s_idx = self.shared_resources.entity_to_index.get(s, 0)\n            o_idx = self.shared_resources.entity_to_index.get(o, 0)\n            p_idx = self.shared_resources.predicate_to_index.get(p, 0)\n            triples.append([s_idx, p_idx, o_idx])\n\n        return triples\n\n    def create_batch(self, triples: List[List[int]],\n                     is_eval: bool, with_answers: bool) -> Mapping[TensorPort, np.ndarray]:\n        _triples = list(triples)\n\n        if with_answers:\n            target = [1] * len(_triples)\n\n        nb_entities = self.shared_resources.nb_entities\n        nb_predicates = self.shared_resources.nb_predicates\n\n        if with_answers:\n            for i in range(len(_triples)):\n                s, p, o = triples[i]\n\n                for _ in range(self.shared_resources.config.get(\'num_negative\', 1)):\n\n                    random_subject_index = self._kbp_rng.randint(0, nb_entities)\n                    random_object_index = self._kbp_rng.randint(0, nb_predicates)\n\n                    _triples.append([random_subject_index, p, o])\n                    _triples.append([s, p, random_object_index])\n\n                    target.append(0)\n                    target.append(0)\n\n        xy_dict = {Ports.Input.question: _triples}\n\n        if with_answers:\n            xy_dict[Ports.Target.target_index] = target\n\n        return numpify(xy_dict)\n\n    @property\n    def output_ports(self) -> List[TensorPort]:\n        return [Ports.Input.question]\n\n    @property\n    def training_ports(self) -> List[TensorPort]:\n        return [Ports.Target.target_index]\n\n\nclass KnowledgeGraphEmbeddingModelModule(TFModelModule):\n    def __init__(self, *args, model_name=\'DistMult\', **kwargs):\n        super().__init__(*args, **kwargs)\n        self.model_name = model_name\n\n    @property\n    def input_ports(self) -> List[TensorPort]:\n        return [Ports.Input.question]\n\n    @property\n    def output_ports(self) -> List[TensorPort]:\n        return [Ports.Prediction.logits]\n\n    @property\n    def training_input_ports(self) -> List[TensorPort]:\n        return [Ports.Target.target_index, Ports.Prediction.logits]\n\n    @property\n    def training_output_ports(self) -> List[TensorPort]:\n        return [Ports.loss]\n\n    def create_training_output(self, shared_resources: SharedResources, input_tensors) \\\n            -> Mapping[TensorPort, tf.Tensor]:\n        tensors = TensorPortTensors(input_tensors)\n        losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=tensors.logits,\n                                                         labels=tf.to_float(tensors.target_index))\n        loss = tf.reduce_mean(losses, axis=0)\n        return {Ports.loss: loss}\n\n    def create_output(self, shared_resources: SharedResources, input_tensors) -> Mapping[TensorPort, tf.Tensor]:\n        tensors = TensorPortTensors(input_tensors)\n        with tf.variable_scope(\'knowledge_graph_embedding\'):\n            embedding_size = shared_resources.config[\'repr_dim\']\n\n            nb_entities = max(shared_resources.entity_to_index.values()) + 1\n            nb_predicates = max(shared_resources.predicate_to_index.values()) + 1\n\n            entity_embeddings = tf.get_variable(\'entity_embeddings\',\n                                                shape=[nb_entities, embedding_size],\n                                                initializer=tf.contrib.layers.xavier_initializer(),\n                                                dtype=\'float32\')\n\n            predicate_embeddings = tf.get_variable(\'predicate_embeddings\',\n                                                   shape=[nb_predicates, embedding_size],\n                                                   initializer=tf.contrib.layers.xavier_initializer(),\n                                                   dtype=\'float32\')\n\n            subject_idx = tensors.question[:, 0]\n            predicate_idx = tensors.question[:, 1]\n            object_idx = tensors.question[:, 2]\n\n            subject_emb = tf.nn.embedding_lookup(entity_embeddings, subject_idx, max_norm=1.0)\n            predicate_emb = tf.nn.embedding_lookup(predicate_embeddings, predicate_idx)\n            object_emb = tf.nn.embedding_lookup(entity_embeddings, object_idx, max_norm=1.0)\n\n            assert self.model_name is not None\n\n            model_class = scores.get_function(self.model_name)\n            model = model_class(\n                subject_embeddings=subject_emb,\n                predicate_embeddings=predicate_emb,\n                object_embeddings=object_emb)\n\n            logits = model()\n\n        return {\n            Ports.Prediction.logits: logits\n        }\n\n\nclass KnowledgeGraphEmbeddingOutputModule(OutputModule):\n    def setup(self):\n        pass\n\n    @property\n    def input_ports(self) -> List[TensorPort]:\n        return [Ports.Prediction.logits]\n\n    def __call__(self, questions: Sequence[QASetting], tensors: Mapping[TensorPort, np.array]) \\\n            -> Sequence[Sequence[Answer]]:\n        # len(inputs) == batch size\n        # logits: [batch_size, max_num_candidates]\n        logits = tensors[Ports.Prediction.logits]\n        results = []\n        for index_in_batch, question in enumerate(questions):\n            score = logits[index_in_batch]\n            results.append([Answer(question.question, score=score)])\n        return results\n'"
jack/readers/link_prediction/scores.py,9,"b'# -*- coding: utf-8 -*-\n\nimport abc\nimport sys\n\nimport tensorflow as tf\n\nfrom jack.readers.link_prediction.similarities import negative_l1_distance\n\n\nclass BaseModel(metaclass=abc.ABCMeta):\n    def __init__(self, subject_embeddings=None, predicate_embeddings=None, object_embeddings=None, *args, **kwargs):\n        """"""\n        Abstract class inherited by all models.\n\n        Args:\n            subject_embeddings: (batch_size, entity_embedding_size) Tensor.\n            predicate_embeddings: (batch_size, predicate_embedding_size) Tensor.\n            object_embeddings: (batch_size, entity_embedding_size) Tensor.\n        """"""\n        self.subject_embeddings = subject_embeddings\n        self.predicate_embeddings = predicate_embeddings\n        self.object_embeddings = object_embeddings\n\n    @abc.abstractmethod\n    def __call__(self):\n        raise NotImplementedError\n\n    @property\n    def parameters(self):\n        return []\n\n\nclass TranslatingModel(BaseModel):\n    def __init__(self, similarity_function=negative_l1_distance, *args, **kwargs):\n        """"""\n        Implementation of the Translating Embeddings model [1].\n        [1] Bordes, A. et al. - Translating Embeddings for Modeling Multi-relational Data - NIPS 2013\n\n        Args:\n            similarity_function: Similarity function.\n        """"""\n        super().__init__(*args, **kwargs)\n        self.similarity_function = similarity_function\n\n    def __call__(self):\n        """"""\n        :return: (batch_size) Tensor containing the scores associated by the models to the walks.\n        """"""\n        translated_subject_embedding = self.subject_embeddings + self.predicate_embeddings\n        return self.similarity_function(translated_subject_embedding, self.object_embeddings)\n\n\nclass BilinearDiagonalModel(BaseModel):\n    def __init__(self, *args, **kwargs):\n        """"""\n        Implementation of the Bilinear-Diagonal model [1]\n        [1] Yang, B. et al. - Embedding Entities and Relations for Learning and Inference in Knowledge Bases - ICLR 2015\n\n        Args:\n            similarity_function: Similarity function.\n        """"""\n        super().__init__(*args, **kwargs)\n\n    def __call__(self):\n        """"""\n        :return: (batch_size) Tensor containing the scores associated by the models to the walks.\n        """"""\n        scaled_subject_embedding = self.subject_embeddings * self.predicate_embeddings\n        return tf.reduce_sum(scaled_subject_embedding * self.object_embeddings, axis=1)\n\n\nclass BilinearModel(BaseModel):\n    def __init__(self, *args, **kwargs):\n        """"""\n        Implementation of the Bilinear model [1]\n        [1] Nickel, M. et al. - A Three-Way Model for Collective Learning on Multi-Relational Data - ICML 2011\n\n        ArgS:\n            similarity_function: Similarity function.\n        """"""\n        super().__init__(*args, **kwargs)\n\n    def __call__(self):\n        """"""\n        :return: (batch_size) Tensor containing the scores associated by the models to the walks.\n        """"""\n        es, emb_size = tf.expand_dims(self.subject_embeddings, 1), tf.shape(self.subject_embeddings)[1]\n        W = tf.reshape(self.predicate_embeddings, (-1, emb_size, emb_size))\n        sW = tf.matmul(es, W)[:, 0, :]\n        return tf.reduce_sum(sW * self.object_embeddings, axis=1)\n\n\nclass ComplexModel(BaseModel):\n    def __init__(self, *args, **kwargs):\n        """"""\n        Implementation of the ComplEx model [1]\n        [1] Trouillon, T. et al. - Complex Embeddings for Simple Link Prediction - ICML 2016\n\n        Args:\n            embedding size: Embedding size.\n        """"""\n        super().__init__(*args, **kwargs)\n\n    def __call__(self):\n        """"""\n        :return: (batch_size) Tensor containing the scores associated by the models to the walks.\n        """"""\n        es_re, es_im = tf.split(value=self.subject_embeddings, num_or_size_splits=2, axis=1)\n        eo_re, eo_im = tf.split(value=self.object_embeddings, num_or_size_splits=2, axis=1)\n        ew_re, ew_im = tf.split(value=self.predicate_embeddings, num_or_size_splits=2, axis=1)\n\n        def dot3(arg1, rel, arg2):\n            return tf.reduce_sum(arg1 * rel * arg2, axis=1)\n\n        score = dot3(es_re, ew_re, eo_re) + dot3(es_re, ew_im, eo_im) + dot3(es_im, ew_re, eo_im) - dot3(es_im, ew_im, eo_re)\n        return score\n\n# Aliases\nTransE = TranslatingEmbeddings = TranslatingModel\nDistMult = BilinearDiagonal = BilinearDiagonalModel\nRESCAL = Bilinear = BilinearModel\nComplEx = ComplexE = ComplexModel\n\n\ndef get_function(function_name):\n    this_module = sys.modules[__name__]\n    if not hasattr(this_module, function_name):\n        raise ValueError(\'Unknown model: {}\'.format(function_name))\n    return getattr(this_module, function_name)\n'"
jack/readers/link_prediction/similarities.py,4,"b'# -*- coding: utf-8 -*-\n\nimport sys\n\nimport tensorflow as tf\n\n\ndef negative_l1_distance(x1, x2, axis=1):\n    """"""\n    Negative L1 Distance.\n\n    .. math:: L = - \\\\sum_i \\\\abs(x1_i - x2_i)\n\n    Args:\n        x1: First term.\n        x2: Second term.\n        axis: Reduction Indices.\n\n    Returns:\n        Similarity Value.\n    """"""\n    distance = tf.reduce_sum(tf.abs(x1 - x2), axis=axis)\n    return - distance\n\n\ndef negative_l2_distance(x1, x2, axis=1):\n    """"""\n    Negative L2 Distance.\n\n    .. math:: L = - \\\\sqrt{\\\\sum_i (x1_i - x2_i)^2}\n\n    Args:\n        x1: First term.\n        x2: Second term.\n        axis: Reduction Indices.\n\n    Returns:\n        Similarity Value.\n    """"""\n\n    distance = tf.sqrt(tf.reduce_sum(tf.square(x1 - x2), axis=axis))\n    return - distance\n\n\ndef negative_square_l2_distance(x1, x2, axis=1):\n    """"""\n    Negative Square L2 Distance.\n\n    .. math:: L = - \\\\sum_i (x1_i - x2_i)^2\n\n    Args:\n        x1: First term.\n        x2: Second term.\n        axis: Reduction Indices.\n\n    Returns:\n        Similarity Value.\n    """"""\n    distance = tf.reduce_sum(tf.square(x1 - x2), axis=axis)\n    return - distance\n\n\ndef dot_product(x1, x2, axis=1):\n    """"""\n    Dot Product.\n\n    .. math:: L = \\\\sum_i x1_i x2_i\n\n    Args:\n        x1: First term.\n        x2: Second term.\n        axis: Reduction Indices.\n\n    Returns:\n        Similarity Value.\n    """"""\n\n    similarity = tf.reduce_sum(x1 * x2, axis=axis)\n    return similarity\n\n\n# Aliases\nl1 = L1 = negative_l1_distance\nl2 = L2 = negative_l2_distance\nl2_sqr = L2_SQR = negative_square_l2_distance\ndot = DOT = dot_product\n\n\ndef get_function(function_name):\n    this_module = sys.modules[__name__]\n    if not hasattr(this_module, function_name):\n        raise ValueError(\'Unknown similarity function: {}\'.format(function_name))\n    return getattr(this_module, function_name)\n'"
jack/readers/natural_language_inference/__init__.py,0,b''
jack/readers/natural_language_inference/conditional_bilstm.py,15,"b'import tensorflow as tf\n\nfrom jack.readers.classification.shared import AbstractSingleSupportClassificationModel\nfrom jack.util.tf.rnn import fused_birnn\n\n\nclass ConditionalBiLSTMClassificationModel(AbstractSingleSupportClassificationModel):\n    def forward_pass(self, shared_resources, embedded_question, embedded_support, num_classes, tensors):\n        # question - hypothesis; support - premise\n        repr_dim = shared_resources.config[\'repr_dim\']\n        dropout = shared_resources.config.get(""dropout"", 0.0)\n\n        with tf.variable_scope(\'embedding_projection\') as vs:\n            embedded_question = tf.layers.dense(embedded_question, repr_dim, tf.tanh, name=\'projection\')\n            vs.reuse_variables()\n            embedded_support = tf.layers.dense(embedded_support, repr_dim, tf.tanh, name=\'projection\')\n            # keep dropout mask constant over time\n            dropout_shape = [tf.shape(embedded_question)[0], 1, tf.shape(embedded_question)[2]]\n            embedded_question = tf.nn.dropout(embedded_question, 1.0 - dropout, dropout_shape)\n            embedded_support = tf.nn.dropout(embedded_support, 1.0 - dropout, dropout_shape)\n\n        fused_rnn = tf.contrib.rnn.LSTMBlockFusedCell(repr_dim)\n        # [batch, 2*output_dim] -> [batch, num_classes]\n        _, q_states = fused_birnn(fused_rnn, embedded_question, sequence_length=tensors.question_length,\n                                  dtype=tf.float32, time_major=False, scope=""question_rnn"")\n\n        outputs, _ = fused_birnn(fused_rnn, embedded_support, sequence_length=tensors.support_length,\n                                 dtype=tf.float32, initial_state=q_states, time_major=False, scope=""support_rnn"")\n\n        # [batch, T, 2 * dim] -> [batch, dim]\n        outputs = tf.concat([outputs[0], outputs[1]], axis=2)\n        hidden = tf.layers.dense(outputs, repr_dim, tf.nn.relu, name=""hidden"") * tf.expand_dims(\n            tf.sequence_mask(tensors.support_length, maxlen=tf.shape(outputs)[1], dtype=tf.float32), 2)\n        hidden = tf.reduce_max(hidden, axis=1)\n        # [batch, dim] -> [batch, num_classes]\n        outputs = tf.layers.dense(hidden, num_classes, name=""classification"")\n        return outputs\n'"
jack/readers/natural_language_inference/decomposable_attention.py,59,"b'# -*- coding: utf-8 -*-\n\nimport logging\nfrom abc import abstractmethod\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom jack.readers.classification.shared import AbstractSingleSupportClassificationModel\nfrom jack.util.tf.attention import attention_softmax3d\nfrom jack.util.tf.masking import mask_3d\n\nlogger = logging.getLogger(__name__)\n\n\nclass DecomposableAttentionModel(AbstractSingleSupportClassificationModel):\n\n    def forward_pass(self, shared_resources, embedded_question, embedded_support, num_classes, tensors,\n                     has_bos_token=True):\n        # final states_fw_bw dimensions:\n        # [[[batch, output dim], [batch, output_dim]]\n\n        if has_bos_token:\n            # batch_size = embedded_question.get_shape()[0]\n            embedding_size = embedded_question.get_shape().as_list()[2]\n\n            bos_token_emb = tf.get_variable(\'bos_token_embedding\',\n                                            shape=(1, 1, embedding_size),\n                                            initializer=tf.ones_initializer())\n\n            batch_size = tf.shape(embedded_question)[0]\n\n            t_bos_token_emb = tf.tile(\n                input=bos_token_emb,\n                multiples=[batch_size, 1, 1])\n\n            embedded_question = tf.concat(values=[t_bos_token_emb, embedded_question], axis=1)\n            embedded_support = tf.concat(values=[t_bos_token_emb, embedded_support], axis=1)\n\n            tensors.question_length += 1\n            tensors.support_length += 1\n\n        if shared_resources.config.get(\'normalize_embeddings\', False):\n            embedded_question = tf.nn.l2_normalize(embedded_question, 2)\n            embedded_support = tf.nn.l2_normalize(embedded_support, 2)\n\n        dropout_rate = shared_resources.config.get(\'dropout\', 0)\n        dropout_keep_prob = tf.cond(tf.logical_not(tensors.is_eval), lambda: 1.0 - dropout_rate, lambda: 1.0)\n\n        model_kwargs = {\n            \'sequence1\': embedded_question,\n            \'sequence1_length\': tensors.question_length,\n            \'sequence2\': embedded_support,\n            \'sequence2_length\': tensors.support_length,\n            \'representation_size\': shared_resources.config[\'repr_dim\'],\n            \'dropout_keep_prob\': dropout_keep_prob,\n            \'use_masking\': True,\n        }\n\n        model = FeedForwardDAM(**model_kwargs)\n        logits = model()\n        return logits\n\n\nclass BaseDecomposableAttentionModel:\n    @abstractmethod\n    def _transform_input(self, sequence, reuse=False):\n        raise NotImplementedError\n\n    @abstractmethod\n    def _transform_attend(self, sequence, reuse=False):\n        raise NotImplementedError\n\n    @abstractmethod\n    def _transform_compare(self, sequence, reuse=False):\n        raise NotImplementedError\n\n    @abstractmethod\n    def _transform_aggregate(self, v1_v2, reuse=False):\n        raise NotImplementedError\n\n    def __init__(self, sequence1, sequence1_length, sequence2, sequence2_length,\n                 nb_classes=3, reuse=False, use_masking=True, init_std_dev=0.01, *args, **kwargs):\n        self.init_std_dev = init_std_dev\n        self.nb_classes = nb_classes\n\n        self.sequence1 = sequence1\n        self.sequence1_length = sequence1_length\n\n        self.sequence2 = sequence2\n        self.sequence2_length = sequence2_length\n\n        self.reuse = reuse\n\n        embedding1_size = self.sequence1.get_shape()[-1].value\n        embedding2_size = self.sequence2.get_shape()[-1].value\n\n        assert embedding1_size == embedding2_size\n\n        # [batch_size, time_steps, embedding_size] -> [batch_size, time_steps, representation_size]\n        self.transformed_sequence1 = self._transform_input(self.sequence1, reuse=self.reuse)\n\n        # [batch_size, time_steps, embedding_size] -> [batch_size, time_steps, representation_size]\n        self.transformed_sequence2 = self._transform_input(self.sequence2, reuse=True)\n\n        self.transformed_sequence1_length = self.sequence1_length\n        self.transformed_sequence2_length = self.sequence2_length\n\n        logger.info(\'Building the Attend graph ..\')\n\n        self.raw_attentions = None\n        self.attention_sentence1 = self.attention_sentence2 = None\n\n        # tensors with shape (batch_size, time_steps, num_units)\n        self.alpha, self.beta = self.attend(self.transformed_sequence1, self.transformed_sequence2,\n                                            sequence1_lengths=self.transformed_sequence1_length,\n                                            sequence2_lengths=self.transformed_sequence2_length,\n                                            use_masking=use_masking, reuse=self.reuse)\n\n        logger.info(\'Building the Compare graph ..\')\n\n        # tensor with shape (batch_size, time_steps, num_units)\n        self.v1 = self.compare(self.transformed_sequence1, self.beta, reuse=self.reuse)\n\n        # tensor with shape (batch_size, time_steps, num_units)\n        self.v2 = self.compare(self.transformed_sequence2, self.alpha, reuse=True)\n\n        logger.info(\'Building the Aggregate graph ..\')\n        self.logits = self.aggregate(self.v1, self.v2, self.nb_classes,\n                                     v1_lengths=self.transformed_sequence1_length,\n                                     v2_lengths=self.transformed_sequence2_length,\n                                     use_masking=use_masking, reuse=self.reuse)\n\n    def __call__(self):\n            return self.logits\n\n    def attend(self, sequence1, sequence2,\n               sequence1_lengths=None, sequence2_lengths=None, use_masking=True, reuse=False):\n        """"""\n        Attend phase.\n\n        Args:\n            sequence1: tensor with shape (batch_size, time_steps, num_units)\n            sequence2: tensor with shape (batch_size, time_steps, num_units)\n            sequence1_lengths: time_steps in sequence1\n            sequence2_lengths: time_steps in sequence2\n            use_masking: use masking\n            reuse: reuse variables\n\n        Returns:\n            two tensors with shape (batch_size, time_steps, num_units)\n        """"""\n        with tf.variable_scope(\'attend\') as _:\n            # tensor with shape (batch_size, time_steps, num_units)\n            transformed_sequence1 = self._transform_attend(sequence1, reuse)\n\n            # tensor with shape (batch_size, time_steps, num_units)\n            transformed_sequence2 = self._transform_attend(sequence2, True)\n\n            # tensor with shape (batch_size, time_steps, time_steps)\n            self.raw_attentions = tf.matmul(transformed_sequence1, tf.transpose(transformed_sequence2, [0, 2, 1]))\n\n            masked_raw_attentions = self.raw_attentions\n            if use_masking:\n                masked_raw_attentions = mask_3d(sequences=masked_raw_attentions,\n                                                sequence_lengths=sequence2_lengths,\n                                                mask_value=- np.inf, dimension=2)\n            self.attention_sentence1 = attention_softmax3d(masked_raw_attentions)\n\n            # tensor with shape (batch_size, time_steps, time_steps)\n            attention_transposed = tf.transpose(self.raw_attentions, [0, 2, 1])\n            masked_attention_transposed = attention_transposed\n            if use_masking:\n                masked_attention_transposed = mask_3d(sequences=masked_attention_transposed,\n                                                      sequence_lengths=sequence1_lengths,\n                                                      mask_value=- np.inf, dimension=2)\n            self.attention_sentence2 = attention_softmax3d(masked_attention_transposed)\n\n            # tensors with shape (batch_size, time_steps, num_units)\n            alpha = tf.matmul(self.attention_sentence2, sequence1, name=\'alpha\')\n            beta = tf.matmul(self.attention_sentence1, sequence2, name=\'beta\')\n            return alpha, beta\n\n    def compare(self, sentence, soft_alignment, reuse=False):\n        """"""\n        Compare phase.\n\n        Args:\n            sentence: tensor with shape (batch_size, time_steps, num_units)\n            soft_alignment: tensor with shape (batch_size, time_steps, num_units)\n            reuse: reuse variables\n\n        Returns:\n            tensor with shape (batch_size, time_steps, num_units)\n        """"""\n        # tensor with shape (batch, time_steps, num_units)\n        sentence_and_alignment = tf.concat(axis=2, values=[sentence, soft_alignment])\n        transformed_sentence_and_alignment = self._transform_compare(sentence_and_alignment, reuse=reuse)\n        return transformed_sentence_and_alignment\n\n    def aggregate(self, v1, v2, num_classes,\n                  v1_lengths=None, v2_lengths=None, use_masking=True, reuse=False):\n        """"""\n        Aggregate phase.\n\n        Args:\n            v1: tensor with shape (batch_size, time_steps, num_units)\n            v2: tensor with shape (batch_size, time_steps, num_units)\n            num_classes: number of output units\n            v1_lengths: time_steps in v1\n            v2_lengths: time_steps in v2\n            use_masking: use masking\n            reuse: reuse variables\n        """"""\n        with tf.variable_scope(\'aggregate\', reuse=reuse) as _:\n            if use_masking:\n                v1 = mask_3d(sequences=v1, sequence_lengths=v1_lengths, mask_value=0, dimension=1)\n                v2 = mask_3d(sequences=v2, sequence_lengths=v2_lengths, mask_value=0, dimension=1)\n\n            v1_sum, v2_sum = tf.reduce_sum(v1, [1]), tf.reduce_sum(v2, [1])\n\n            v1_v2 = tf.concat(axis=1, values=[v1_sum, v2_sum])\n            transformed_v1_v2 = self._transform_aggregate(v1_v2, reuse=reuse)\n\n            logits = tf.contrib.layers.fully_connected(inputs=transformed_v1_v2,\n                                                       num_outputs=num_classes,\n                                                       weights_initializer=tf.random_normal_initializer(0.0, 0.01),\n                                                       biases_initializer=tf.zeros_initializer(),\n                                                       activation_fn=None)\n        return logits\n\n\nclass FeedForwardDAM(BaseDecomposableAttentionModel):\n    def __init__(self, representation_size=200, dropout_keep_prob=1.0, *args, **kwargs):\n        self.representation_size = representation_size\n        self.dropout_keep_prob = dropout_keep_prob\n        super().__init__(*args, **kwargs)\n\n    def _transform_input(self, sequence, reuse=False):\n        with tf.variable_scope(\'transform_embeddings\', reuse=reuse) as _:\n            projection = tf.contrib.layers.fully_connected(inputs=sequence, num_outputs=self.representation_size,\n                                                           weights_initializer=tf.random_normal_initializer(0.0,\n                                                                                                            self.init_std_dev),\n                                                           biases_initializer=None, activation_fn=None)\n        return projection\n\n    def _transform_attend(self, sequence, reuse=False):\n        with tf.variable_scope(\'transform_attend\', reuse=reuse) as _:\n            projection = tf.nn.dropout(sequence, keep_prob=self.dropout_keep_prob)\n            projection = tf.contrib.layers.fully_connected(inputs=projection, num_outputs=self.representation_size,\n                                                           weights_initializer=tf.random_normal_initializer(0.0,\n                                                                                                            self.init_std_dev),\n                                                           biases_initializer=tf.zeros_initializer(),\n                                                           activation_fn=tf.nn.relu)\n            projection = tf.nn.dropout(projection, keep_prob=self.dropout_keep_prob)\n            projection = tf.contrib.layers.fully_connected(inputs=projection, num_outputs=self.representation_size,\n                                                           weights_initializer=tf.random_normal_initializer(0.0,\n                                                                                                            self.init_std_dev),\n                                                           biases_initializer=tf.zeros_initializer(),\n                                                           activation_fn=tf.nn.relu)\n        return projection\n\n    def _transform_compare(self, sequence, reuse=False):\n        with tf.variable_scope(\'transform_compare\', reuse=reuse) as _:\n            projection = tf.nn.dropout(sequence, keep_prob=self.dropout_keep_prob)\n            projection = tf.contrib.layers.fully_connected(inputs=projection, num_outputs=self.representation_size,\n                                                           weights_initializer=tf.random_normal_initializer(0.0,\n                                                                                                            self.init_std_dev),\n                                                           biases_initializer=tf.zeros_initializer(),\n                                                           activation_fn=tf.nn.relu)\n            projection = tf.nn.dropout(projection, keep_prob=self.dropout_keep_prob)\n            projection = tf.contrib.layers.fully_connected(inputs=projection, num_outputs=self.representation_size,\n                                                           weights_initializer=tf.random_normal_initializer(0.0,\n                                                                                                            self.init_std_dev),\n                                                           biases_initializer=tf.zeros_initializer(),\n                                                           activation_fn=tf.nn.relu)\n        return projection\n\n    def _transform_aggregate(self, v1_v2, reuse=False):\n        with tf.variable_scope(\'transform_aggregate\', reuse=reuse) as _:\n            projection = tf.nn.dropout(v1_v2, keep_prob=self.dropout_keep_prob)\n            projection = tf.contrib.layers.fully_connected(inputs=projection, num_outputs=self.representation_size,\n                                                           weights_initializer=tf.random_normal_initializer(0.0,\n                                                                                                            self.init_std_dev),\n                                                           biases_initializer=tf.zeros_initializer(),\n                                                           activation_fn=tf.nn.relu)\n            projection = tf.nn.dropout(projection, keep_prob=self.dropout_keep_prob)\n            projection = tf.contrib.layers.fully_connected(inputs=projection, num_outputs=self.representation_size,\n                                                           weights_initializer=tf.random_normal_initializer(0.0,\n                                                                                                            self.init_std_dev),\n                                                           biases_initializer=tf.zeros_initializer(),\n                                                           activation_fn=tf.nn.relu)\n        return projection\n'"
jack/readers/natural_language_inference/modular_nli_model.py,4,"b'import tensorflow as tf\n\nfrom jack.readers.classification.shared import AbstractSingleSupportClassificationModel\nfrom jack.readers.natural_language_inference import prediction_layer\nfrom jack.util.tf.embedding import conv_char_embedding\nfrom jack.util.tf.modular_encoder import modular_encoder\n\n\nclass ModularNLIModel(AbstractSingleSupportClassificationModel):\n    def forward_pass(self, shared_resources, embedded_question, embedded_support, num_classes, tensors):\n        model = shared_resources.config[\'model\']\n        repr_dim = shared_resources.config[\'repr_dim\']\n        dropout = shared_resources.config.get(""dropout"")\n\n        if shared_resources.config.get(\'with_char_embeddings\'):\n            [char_emb_question, char_emb_support] = conv_char_embedding(\n                len(shared_resources.char_vocab), shared_resources.config[\'repr_dim\'], tensors.word_chars,\n                tensors.word_char_length, [tensors.question_batch_words, tensors.support_batch_words])\n            inputs = {\'hypothesis\': embedded_question, \'premise\': embedded_support,\n                      \'char_hypothesis\': char_emb_question, \'char_premise\': char_emb_support}\n            inputs_length = {\'hypothesis\': tensors.question_length, \'premise\': tensors.support_length,\n                             \'char_hypothesis\': tensors.question_length, \'char_premise\': tensors.support_length}\n        else:\n            inputs = {\'hypothesis\': embedded_question, \'premise\': embedded_support}\n            inputs_length = {\'hypothesis\': tensors.question_length, \'premise\': tensors.support_length}\n\n        if dropout:\n            for k in inputs:\n                inputs[k] = tf.cond(tensors.is_eval, lambda: inputs[k], lambda: tf.nn.dropout(inputs[k], 1.0 - dropout))\n\n        inputs_mapping = {}\n\n        encoder_config = model[\'encoder_layer\']\n        encoded, _, _ = modular_encoder(\n            encoder_config, inputs, inputs_length, inputs_mapping, repr_dim, dropout, tensors.is_eval)\n\n        with tf.variable_scope(\'prediction_layer\'):\n            prediction_layer_config = model[\'prediction_layer\']\n            encoded_question = encoded[prediction_layer_config.get(\'hypothesis\', \'hypothesis\')]\n            encoded_support = encoded[prediction_layer_config.get(\'premise\', \'premise\')]\n\n            if \'repr_dim\' not in prediction_layer_config:\n                prediction_layer_config[\'repr_dim\'] = repr_dim\n            prediction_layer_config[\'dropout\'] = dropout if prediction_layer_config.get(\'dropout\', False) else 0.0\n            logits = prediction_layer.prediction_layer(\n                encoded_question, tensors.question_length, encoded_support,\n                tensors.support_length, num_classes, tensors.is_eval, **prediction_layer_config)\n\n        return logits\n'"
jack/readers/natural_language_inference/prediction_layer.py,37,"b'import tensorflow as tf\n\n\ndef prediction_layer(hypothesis, hypothesis_length, premise, premise_length, num_classes, is_eval,\n                     repr_dim=100, activation=None, module=\'max_avg_mlp\', dropout=0.0, **kwargs):\n    if module == \'max_avg_mlp\':\n        return max_avg_pool_prediction_layer(\n            repr_dim, num_classes, hypothesis, hypothesis_length, premise, premise_length, is_eval, activation, dropout)\n    elif module == \'max_mlp\':\n        return max_pool_prediction_layer(\n            repr_dim, num_classes, hypothesis, hypothesis_length, premise, premise_length, is_eval, activation, dropout)\n    elif module == \'max_mlp_hypothesis\':\n        return max_pool_prediction_layer_hypothesis(\n            repr_dim, num_classes, hypothesis, hypothesis_length, is_eval, activation, dropout)\n    elif module == \'max_interaction_mlp\':\n        return max_pool_interaction_prediction_layer(\n            repr_dim, num_classes, hypothesis, hypothesis_length, premise, premise_length, is_eval, activation, dropout)\n    else:\n        raise ValueError(""Unknown answer layer type: %s"" % module)\n\n\ndef _mask(hypothesis, hypothesis_length, premise, premise_length):\n    p_mask = tf.sequence_mask(premise_length, tf.shape(premise)[1], dtype=tf.float32)\n    h_mask = tf.sequence_mask(hypothesis_length, tf.shape(hypothesis)[1], dtype=tf.float32)\n\n    premise *= tf.expand_dims(p_mask, 2)\n    hypothesis *= tf.expand_dims(h_mask, 2)\n    return hypothesis, premise\n\n\ndef max_avg_pool_prediction_layer(size, num_classes, hypothesis, hypothesis_length, premise, premise_length, is_eval,\n                                  activation=tf.tanh, dropout=0.0):\n    hypothesis, premise = _mask(hypothesis, hypothesis_length, premise, premise_length)\n\n    p_max = tf.reduce_max(premise, axis=1)\n    p_avg = tf.reduce_sum(premise, axis=1) / tf.expand_dims(tf.to_float(premise_length), 1)\n\n    h_max = tf.reduce_max(hypothesis, axis=1)\n    h_avg = tf.reduce_sum(hypothesis, axis=1) / tf.expand_dims(tf.to_float(hypothesis_length), 1)\n\n    inputs = tf.concat([p_max, p_avg, h_max, h_avg], 1)\n    if dropout:\n        inputs = tf.cond(is_eval, lambda: inputs, lambda: tf.nn.dropout(inputs, 1.0 - dropout))\n    hidden = tf.layers.dense(inputs, size, activation)\n    if dropout:\n        hidden = tf.cond(is_eval, lambda: hidden, lambda: tf.nn.dropout(hidden, 1.0 - dropout))\n    logits = tf.layers.dense(hidden, num_classes)\n\n    return logits\n\n\ndef max_pool_prediction_layer(size, num_classes, hypothesis, hypothesis_length, premise, premise_length, is_eval,\n                              activation=tf.tanh, dropout=0.0):\n    hypothesis, premise = _mask(hypothesis, hypothesis_length, premise, premise_length)\n\n    p_max = tf.reduce_max(premise, axis=1)\n\n    h_max = tf.reduce_max(hypothesis, axis=1)\n\n    inputs = tf.concat([p_max, h_max], 1)\n    if dropout:\n        inputs = tf.cond(is_eval, lambda: inputs, lambda: tf.nn.dropout(inputs, 1.0 - dropout))\n    hidden = tf.layers.dense(inputs, size, activation)\n    if dropout:\n        hidden = tf.cond(is_eval, lambda: hidden, lambda: tf.nn.dropout(hidden, 1.0 - dropout))\n    logits = tf.layers.dense(hidden, num_classes)\n\n    return logits\n\n\ndef max_pool_interaction_prediction_layer(size, num_classes, hypothesis, hypothesis_length, premise, premise_length,\n                                          is_eval, activation=tf.tanh, dropout=0.0):\n    hypothesis, premise = _mask(hypothesis, hypothesis_length, premise, premise_length)\n\n    p_max = tf.reduce_max(premise, axis=1)\n    h_max = tf.reduce_max(hypothesis, axis=1)\n\n    inputs = tf.concat([p_max, h_max, p_max - h_max, p_max * h_max], 1)\n    if dropout:\n        inputs = tf.cond(is_eval, lambda: inputs, lambda: tf.nn.dropout(inputs, 1.0 - dropout))\n    hidden = tf.layers.dense(inputs, size, activation)\n    if dropout:\n        hidden = tf.cond(is_eval, lambda: hidden, lambda: tf.nn.dropout(hidden, 1.0 - dropout))\n    logits = tf.layers.dense(hidden, num_classes)\n\n    return logits\n\n\ndef max_pool_prediction_layer_hypothesis(size, num_classes, hypothesis, hypothesis_length, is_eval,\n                                         activation=tf.tanh, dropout=0.0):\n    h_mask = tf.sequence_mask(hypothesis_length, tf.shape(hypothesis)[1], dtype=tf.float32)\n    hypothesis *= tf.expand_dims(h_mask, 2)\n\n    h_max = tf.reduce_max(hypothesis, axis=1)\n\n    hidden = tf.layers.dense(h_max, size, activation)\n    if dropout:\n        hidden = tf.cond(is_eval, lambda: hidden, lambda: tf.nn.dropout(hidden, 1.0 - dropout))\n    logits = tf.layers.dense(hidden, num_classes)\n\n    return logits\n'"
jack/util/tf/__init__.py,0,"b'""""""The tf package should contain all tf functionality of jtr for maximal reuse""""""\n'"
jack/util/tf/activations.py,8,"b""# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\n\ndef parametric_relu(x, name=None):\n    alphas = tf.get_variable('{}/alpha'.format(name) if name else 'alpha',\n                             x.get_shape()[-1],\n                             initializer=tf.constant_initializer(0.0),\n                             dtype=tf.float32)\n    return tf.nn.relu(x) + alphas * (x - abs(x)) * 0.5\n\n\ndef selu(x, name=None):\n    with tf.name_scope('{}/elu'.format(name) if name else 'elu') as _:\n        alpha = 1.6732632423543772848170429916717\n        scale = 1.0507009873554804934193349852946\n        return scale*tf.where(x >= 0.0, x, alpha*tf.nn.elu(x))\n\n\n# Aliases\nprelu = parametric_relu\n\n\ndef activation_from_string(activation_str):\n    if activation_str is None:\n        return tf.identity\n    return getattr(tf.nn, activation_str)\n"""
jack/util/tf/attention.py,48,"b'# -*- coding: utf-8 -*-\nimport math\n\nimport tensorflow as tf\n\nfrom jack.util.tf import segment, misc\n\n\ndef attention_softmax(attn_scores, length=None):\n    if length is not None:\n        attn_scores += misc.mask_for_lengths(length, tf.shape(attn_scores)[2])\n    return tf.nn.softmax(attn_scores)\n\n\ndef apply_attention(attn_scores, states, length, is_self=False, with_sentinel=True, reuse=False, seq2_to_seq1=None):\n    attn_scores += tf.expand_dims(misc.mask_for_lengths(length, tf.shape(attn_scores)[2]), 1)\n    softmax = tf.nn.softmax if seq2_to_seq1 is None else lambda x: segment.segment_softmax(x, seq2_to_seq1)\n    if is_self:\n        # exclude attending to state itself\n        attn_scores += tf.expand_dims(tf.diag(tf.fill([tf.shape(attn_scores)[1]], -1e6)), 0)\n    if with_sentinel:\n        with tf.variable_scope(\'sentinel\', reuse=reuse):\n            s = tf.get_variable(\'score\', [1, 1, 1], tf.float32, tf.zeros_initializer())\n        s = tf.tile(s, [tf.shape(attn_scores)[0], tf.shape(attn_scores)[1], 1])\n        attn_probs = softmax(tf.concat([s, attn_scores], 2))\n        attn_probs = attn_probs[:, :, 1:]\n    else:\n        attn_probs = softmax(attn_scores)\n    attn_states = tf.einsum(\'abd,adc->abc\', attn_probs, states)\n    if seq2_to_seq1 is not None:\n        attn_states = tf.unsorted_segment_sum(attn_states, seq2_to_seq1, tf.reduce_max(seq2_to_seq1) + 1)\n    return attn_scores, attn_probs, attn_states\n\n\ndef dot_attention(seq1, seq2, len2, scaled=True, with_sentinel=True, seq2_to_seq1=None, **kwargs):\n    query, key, value = _get_query_key_value(seq1, seq2, **kwargs)\n    if seq2_to_seq1 is not None:\n        query = tf.gather(query, seq2_to_seq1)\n    attn_scores = tf.einsum(\'abc,adc->abd\', query, key)\n    if scaled:\n        attn_scores /= tf.sqrt(float(query.get_shape()[-1].value))\n    return apply_attention(attn_scores, value, len2, seq1 is seq2, with_sentinel, seq2_to_seq1=seq2_to_seq1)\n\n\ndef bilinear_attention(seq1, seq2, len2, scaled=True, with_sentinel=True, seq2_to_seq1=None, **kwargs):\n    query, key, value = _get_query_key_value(seq1, seq2, **kwargs)\n    if seq2_to_seq1 is not None:\n        query = tf.gather(query, seq2_to_seq1)\n    attn_scores = tf.einsum(\'abc,adc->abd\', tf.layers.dense(query, key.get_shape()[-1].value, use_bias=False), key)\n    attn_scores += tf.layers.dense(query, 1, use_bias=False)\n    attn_scores += tf.transpose(tf.layers.dense(key, 1, use_bias=False), [0, 2, 1])\n    if scaled:\n        attn_scores /= math.sqrt(float(query.get_shape()[-1].value))\n    return apply_attention(attn_scores, value, len2, seq1 is seq2, with_sentinel, seq2_to_seq1=seq2_to_seq1)\n\n\ndef diagonal_bilinear_attention(seq1, seq2, len2, scaled=True, with_sentinel=True, seq2_to_seq1=None, **kwargs):\n    query, key, value = _get_query_key_value(seq1, seq2, **kwargs)\n    if seq2_to_seq1 is not None:\n        query = tf.gather(query, seq2_to_seq1)\n    v = tf.get_variable(\'attn_weight\', [1, 1, query.get_shape()[-1].value], tf.float32,\n                        initializer=tf.ones_initializer())\n    attn_scores = tf.einsum(\'abc,adc->abd\', v * query, key)\n    attn_scores += tf.layers.dense(query, 1, use_bias=False)\n    attn_scores += tf.transpose(tf.layers.dense(key, 1, use_bias=False), [0, 2, 1])\n    if scaled:\n        attn_scores /= math.sqrt(float(query.get_shape()[-1].value))\n    return apply_attention(attn_scores, value, len2, seq1 is seq2, with_sentinel, seq2_to_seq1=seq2_to_seq1)\n\n\ndef mlp_attention(hidden_dim, seq1, seq2, len2, activation=tf.nn.relu, with_sentinel=True, seq2_to_seq1=None, **kwargs):\n    query, key, value = _get_query_key_value(seq1, seq2, **kwargs)\n    if seq2_to_seq1 is not None:\n        query = tf.gather(query, seq2_to_seq1)\n    hidden1 = tf.layers.dense(query, hidden_dim)\n    hidden2 = tf.layers.dense(key, hidden_dim)\n    hidden = tf.tile(tf.expand_dims(hidden1, 2), [1, 1, tf.shape(key)[1], -1]) + tf.expand_dims(hidden2, 1)\n    attn_scores = tf.layers.dense(activation(hidden), 1, use_bias=with_sentinel)\n    return apply_attention(tf.squeeze(attn_scores, 3), value, len2, seq1 is seq2, with_sentinel,\n                           seq2_to_seq1=seq2_to_seq1)\n\n\ndef _get_query_key_value(seq1, seq2, key_value_attn=False, repr_dim=None, **kwargs):\n    repr_dim = repr_dim or seq2.get_shape()[-1].value\n    if key_value_attn:\n        with tf.variable_scope(\'key_value_projection\') as vs:\n            key = tf.layers.dense(seq2, repr_dim, name=\'key\')\n            value = tf.layers.dense(seq2, repr_dim, name=\'value\')\n            query = tf.layers.dense(seq1, repr_dim, name=\'query\')\n        return query, key, value\n    else:\n        return seq1, seq2, seq2\n\n\ndef coattention(seq1, len1, seq2, len2, scaled=True, with_sentinel=True, attn_fn=dot_attention, seq2_to_seq1=None):\n    attn_scores, attn_probs1, attn_states1 = attn_fn(seq1, seq2, len2, scaled, with_sentinel, seq2_to_seq1)\n    _, _, attn_states2 = apply_attention(\n        tf.transpose(attn_scores, [0, 2, 1]), seq1, len1, seq1 is seq2, with_sentinel, reuse=True)\n    co_attn_state = tf.einsum(\'abd,adc->abc\', attn_probs1, attn_states2)\n\n    return attn_scores, attn_probs1, attn_states1, attn_states2, co_attn_state\n\n\ndef attention_softmax3d(values):\n    """"""\n    Performs a softmax over the attention values.\n    Args:\n        values: tensor with shape (batch_size, time_steps, time_steps)\n    Returns:\n        tensor with shape (batch_size, time_steps, time_steps)\n    """"""\n    original_shape = tf.shape(values)\n    # tensor with shape (batch_size * time_steps, time_steps)\n    reshaped_values = tf.reshape(tensor=values, shape=[-1, original_shape[2]])\n    # tensor with shape (batch_size * time_steps, time_steps)\n    softmax_reshaped_values = tf.nn.softmax(reshaped_values)\n    # tensor with shape (batch_size, time_steps, time_steps)\n    return tf.reshape(softmax_reshaped_values, original_shape)\n\n\ndef distance_biases(time_steps, window_size=10, reuse=False):\n    """"""\n    Return a 2-d tensor with the values of the distance biases to be applied\n    on the intra-attention matrix of size sentence_size\n\n    Args:\n        time_steps: tensor scalar\n        window_size: window size\n        reuse: reuse variables\n    Returns:\n        2-d tensor (time_steps, time_steps)\n    """"""\n    with tf.variable_scope(\'distance-bias\', reuse=reuse):\n        # this is d_{i-j}\n        distance_bias = tf.get_variable(\'dist_bias\', [window_size], initializer=tf.zeros_initializer())\n        r = tf.range(0, time_steps)\n        r_matrix = tf.tile(tf.reshape(r, [1, -1]), tf.stack([time_steps, 1]))\n        raw_idxs = r_matrix - tf.reshape(r, [-1, 1])\n        clipped_idxs = tf.clip_by_value(raw_idxs, 0, window_size - 1)\n        values = tf.nn.embedding_lookup(distance_bias, clipped_idxs)\n    return values\n'"
jack/util/tf/dropout.py,4,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\n\ndef fixed_dropout(xs, keep_prob, noise_shape, seed=None):\n    """"""\n    Apply dropout with same mask over all inputs\n    Args:\n        xs: list of tensors\n        keep_prob:\n        noise_shape:\n        seed:\n\n    Returns:\n        list of dropped inputs\n    """"""\n    with tf.name_scope(""dropout"", values=xs):\n        noise_shape = noise_shape\n        # uniform [keep_prob, 1.0 + keep_prob)\n        random_tensor = keep_prob\n        random_tensor += tf.random_uniform(noise_shape, seed=seed, dtype=xs[0].dtype)\n        # 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)\n        binary_tensor = tf.floor(random_tensor)\n        outputs = []\n        for x in xs:\n            ret = tf.div(x, keep_prob) * binary_tensor\n            ret.set_shape(x.get_shape())\n            outputs.append(ret)\n        return outputs\n'"
jack/util/tf/embedding.py,30,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nfrom jack.util.tf import misc\n\n\ndef conv_char_embedding(num_chars, repr_dim, word_chars, word_lengths, word_sequences=None,\n                        conv_width=5, emb_initializer=tf.random_normal_initializer(0.0, 0.1), scope=None):\n    """"""Build simple convolutional character based embeddings for words with a fixed filter and size.\n\n    After the convolution max-pooling over characters is employed for each filter. If word sequences are given,\n    these will be embedded with the newly created embeddings.\n    """"""\n    # ""fixed PADDING on character level""\n    pad = tf.zeros(tf.stack([tf.shape(word_lengths)[0], conv_width // 2]), tf.int32)\n    word_chars = tf.concat([pad, word_chars, pad], 1)\n\n    with tf.variable_scope(scope or ""char_embeddings""):\n        char_embedding_matrix = \\\n            tf.get_variable(""char_embedding_matrix"", shape=(num_chars, repr_dim),\n                            initializer=emb_initializer, trainable=True)\n\n        max_word_length = tf.reduce_max(word_lengths)\n        embedded_chars = tf.nn.embedding_lookup(char_embedding_matrix, tf.cast(word_chars, tf.int32))\n\n        with tf.variable_scope(""conv""):\n            # create filter like this to get fan-in and fan-out right for initializers depending on those\n            filter = tf.get_variable(""filter"", [conv_width * repr_dim, repr_dim])\n            filter_reshaped = tf.reshape(filter, [conv_width, repr_dim, repr_dim])\n            # [B, T, S + pad_right]\n            conv_out = tf.nn.conv1d(embedded_chars, filter_reshaped, 1, ""VALID"")\n            conv_mask = tf.expand_dims(misc.mask_for_lengths(word_lengths, max_length=max_word_length), 2)\n            conv_out = conv_out + conv_mask\n\n        embedded_words = tf.reduce_max(conv_out, 1)\n\n    if word_sequences is None:\n        return embedded_words\n\n    if not isinstance(word_sequences, list):\n        word_sequences = [word_sequences]\n    all_embedded = []\n    for word_idxs in word_sequences:\n        all_embedded.append(tf.nn.embedding_lookup(embedded_words, word_idxs))\n\n    return all_embedded\n\n\ndef conv_char_embedding_multi_filter(\n        num_chars, filter_sizes, embedding_size, word_chars, word_lengths, word_sequences=None,\n        emb_initializer=tf.random_normal_initializer(0.0, 0.1), projection_size=None, scope=None):\n    """"""Build convolutional character based embeddings for words with multiple filters.\n\n    Filter sizes is a list and each the position of each size in the list entry refers to its corresponding conv width.\n    It can also be 0 (i.e., no filter of that conv width). E.g., sizes [4, 0, 7, 8] will create 4 conv filters of width\n    1, no filter of width 2, 7 of width 3 and 8 of width 4. After the convolution max-pooling over characters is\n    employed for each filter.\n\n    embedding_size refers to the size of the character embeddings and projection size, if given, to the final size of\n    the embedded characters after a final projection. If it is None, no projection will be applied and the resulting\n    size is the sum of all filters.\n\n    If word sequences are given, these will be embedded with the newly created embeddings.\n    """"""\n    with tf.variable_scope(scope or ""char_embeddings""):\n        char_embedding_matrix = \\\n            tf.get_variable(""char_embedding_matrix"", shape=(num_chars, embedding_size),\n                            initializer=emb_initializer, trainable=True)\n\n        pad = tf.zeros(tf.stack([tf.shape(word_lengths)[0], len(filter_sizes) // 2]), tf.int32)\n        word_chars = tf.concat([pad, word_chars, pad], 1)\n\n        max_word_length = tf.reduce_max(word_lengths)\n        embedded_chars = tf.nn.embedding_lookup(char_embedding_matrix, tf.cast(word_chars, tf.int32))\n        conv_mask = tf.expand_dims(misc.mask_for_lengths(word_lengths, max_length=max_word_length), 2)\n\n        embedded_words = []\n        for i, size in enumerate(filter_sizes):\n            if size == 0:\n                continue\n            conv_width = i + 1\n            with tf.variable_scope(""conv_%d"" % conv_width):\n                # create filter like this to get fan-in and fan-out right for initializers depending on those\n                filter = tf.get_variable(""filter"", [conv_width * embedding_size, size])\n                filter_reshaped = tf.reshape(filter, [conv_width, embedding_size, size])\n                cut = len(filter_sizes) // 2 - conv_width // 2\n                embedded_chars_conv = embedded_chars[:, cut:-cut, :] if cut else embedded_chars\n                conv_out = tf.nn.conv1d(embedded_chars_conv, filter_reshaped, 1, ""VALID"")\n                conv_out += conv_mask\n                embedded_words.append(tf.reduce_max(conv_out, 1))\n\n        embedded_words = tf.concat(embedded_words, 1)\n        if projection_size is not None:\n            embedded_words = tf.layers.dense(embedded_words, projection_size)\n\n    if word_sequences is None:\n        return embedded_words\n\n    if not isinstance(word_sequences, list):\n        word_sequences = [word_sequences]\n    all_embedded = []\n    for word_idxs in word_sequences:\n        embedded_words = tf.nn.embedding_lookup(embedded_words, word_idxs)\n        all_embedded.append(embedded_words)\n\n    return all_embedded\n'"
jack/util/tf/highway.py,6,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\n\ndef highway_layer(inputs, activation, name=None):\n    with tf.variable_scope(name or ""highway_layer""):\n        d = inputs.get_shape()[-1].value\n        trans_gate = tf.contrib.layers.fully_connected(inputs, 2 * d, activation_fn=None, weights_initializer=None,\n                                                       scope=\'trans_gate\')\n        trans, gate = tf.split(trans_gate, 2, len(inputs.get_shape()) - 1)\n        trans, gate = activation(trans), tf.sigmoid(gate)\n        out = gate * trans + (1 - gate) * inputs\n        return out\n\n\ndef highway_network(inputs, num_layers, activation=tf.tanh, name=None, reuse=False):\n    with tf.variable_scope(name or ""highway_network"", reuse=reuse):\n        prev = inputs\n        cur = None\n        for layer_idx in range(num_layers):\n            cur = highway_layer(prev, activation, name=""layer_{}"".format(layer_idx))\n            prev = cur\n    return cur\n'"
jack/util/tf/interaction_layer.py,16,"b'import tensorflow as tf\n\nfrom jack.util.tf import attention, sequence_encoder\nfrom jack.util.tf.misc import mask_for_lengths\n\n\ndef interaction_layer(seq1, seq1_length, seq2, seq2_length, seq1_to_seq2, seq2_to_seq1,\n                      module=\'attention_matching\', name=\'interaction_layer\', reuse=False, concat=True,\n                      repr_dim=None, **kwargs):\n    with tf.variable_scope(name, reuse=reuse):\n        if seq1_to_seq2 is not None:\n            seq2 = tf.gather(seq2, seq1_to_seq2)\n            seq2_length = tf.gather(seq2_length, seq1_to_seq2)\n        if module == \'attention_matching\':\n            out = attention_matching_layer(seq1, seq1_length, seq2, seq2_length, seq2_to_seq1, **kwargs)\n        elif module == \'bidaf\':\n            out = bidaf_layer(seq1, seq1_length, seq2, seq2_length, **kwargs)\n        elif module == \'coattention\':\n            if kwargs.get(\'encoder\') and \'repr_dim\' not in kwargs[\'encoder\']:\n                kwargs[\'encoder\'][\'repr_dim\'] = repr_dim\n            out = coattention_layer(\n                seq1, seq1_length, seq2, seq2_length, **kwargs)\n        else:\n            raise ValueError(""Unknown interaction type: %s"" % module)\n\n    if concat:\n        out = tf.concat([seq1, out], 2)\n    return out\n\n\ndef bidaf_layer(seq1, seq1_length, seq2, seq2_length, seq2_to_seq1=None, **kwargs):\n    """"""Encodes seq1 conditioned on seq2, e.g., using word-by-word attention.""""""\n    attn_scores, attn_probs, seq2_weighted = attention.diagonal_bilinear_attention(\n        seq1, seq2, seq2_length, False, seq2_to_seq1=seq2_to_seq1)\n\n    attn_scores += tf.expand_dims(mask_for_lengths(seq1_length, tf.shape(attn_scores)[1]), 2)\n\n    max_seq1 = tf.reduce_max(attn_scores, 2)\n    seq1_attention = tf.nn.softmax(max_seq1, 1)\n    seq1_weighted = tf.einsum(\'ij,ijk->ik\', seq1_attention, seq1)\n    seq1_weighted = tf.expand_dims(seq1_weighted, 1)\n    seq1_weighted = tf.tile(seq1_weighted, [1, tf.shape(seq1)[1], 1])\n\n    return tf.concat([seq2_weighted, seq1 * seq2_weighted, seq1 * seq1_weighted], 2)\n\n\ndef attention_matching_layer(seq1, seq1_length, seq2, seq2_length, seq2_to_seq1=None,\n                             attn_type=\'diagonal_bilinear\', key_value_attn=False,\n                             scaled=True, with_sentinel=False, repr_dim=None, **kwargs):\n    """"""Encodes seq1 conditioned on seq2, e.g., using word-by-word attention.""""""\n    if attn_type == \'bilinear\':\n        _, _, attn_states = attention.bilinear_attention(seq1, seq2, seq2_length, scaled, with_sentinel,\n                                                         seq2_to_seq1=seq2_to_seq1)\n    elif attn_type == \'dot\':\n        _, _, attn_states = attention.dot_attention(seq1, seq2, seq2_length, scaled, with_sentinel,\n                                                    seq2_to_seq1=seq2_to_seq1)\n    elif attn_type == \'diagonal_bilinear\':\n        _, _, attn_states = attention.diagonal_bilinear_attention(seq1, seq2, seq2_length, scaled,\n                                                                  with_sentinel, seq2_to_seq1=seq2_to_seq1)\n    elif attn_type == \'mlp\':\n        _, _, attn_states = attention.mlp_attention(seq1.get_shape()[-1].value, tf.nn.relu, seq1, seq2,\n                                                    seq2_length, with_sentinel, seq2_to_seq1=seq2_to_seq1)\n    else:\n        raise ValueError(""Unknown attention type: %s"" % attn_type)\n\n    return attn_states\n\n\ndef coattention_layer(seq1, seq1_length, seq2, seq2_length,\n                      attn_type=\'diagonal_bilinear\', scaled=True, with_sentinel=False, seq2_to_seq1=None,\n                      num_layers=1, encoder=None, **kwargs):\n    """"""Encodes seq1 conditioned on seq2, e.g., using word-by-word attention.""""""\n    if attn_type == \'bilinear\':\n        attn_fun = attention.bilinear_attention\n    elif attn_type == \'dot\':\n        attn_fun = attention.dot_attention\n    elif attn_type == \'diagonal_bilinear\':\n        attn_fun = attention.diagonal_bilinear_attention\n    else:\n        raise ValueError(""Unknown attention type: %s"" % attn_type)\n\n    _, _, attn_states1, attn_states2, co_attn_state = attention.coattention(\n        seq1, seq1_length, seq2, seq2_length, scaled, with_sentinel, attn_fun)\n\n    if num_layers < 2:\n        out = tf.concat([attn_states1, co_attn_state], 2)\n    else:\n        seq1, attn_states1, attn_states2, co_attn_state = [], [attn_states1], [attn_states2], [co_attn_state]\n        for i in range(1, num_layers):\n            with tf.variable_scope(str(i)):\n                enc_1 = sequence_encoder.encoder(\n                    attn_states1[-1], seq1_length, name=\'encoder1\', **encoder)\n                enc_2 = sequence_encoder.encoder(\n                    attn_states2[-1], seq2_length, name=\'encoder2\', **encoder)\n                seq1.append(enc_1)\n                _, _, new_attn_states1, new_attn_states2, new_co_attn_state = attention.coattention(\n                    enc_1, seq1_length, enc_2, seq2_length, scaled, with_sentinel, attn_fun, seq2_to_seq1=seq2_to_seq1)\n                attn_states1.append(new_attn_states1)\n                attn_states2.append(new_attn_states2)\n                co_attn_state.append(new_co_attn_state)\n        out = tf.concat(seq1 + attn_states1 + co_attn_state, 2)\n\n    return out\n'"
jack/util/tf/masking.py,8,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\n\ndef mask_3d(sequences, sequence_lengths, mask_value, dimension=2):\n    """"""\n    Given a batch of matrices, each with shape m x n, mask the values in each\n    row after the positions indicated in sentence_sizes.\n    This function is supposed to mask the last columns in the raw attention\n    matrix (e_{i, j}) in cases where the sentence2 is smaller than the\n    maximum.\n    \n    Args:\n        sequences: tensor with shape (batch_size, m, n)\n        sequence_lengths: tensor with shape (batch_size) containing the sentence sizes that\n           should be limited\n        mask_value: scalar value to assign to items after sentence size\n        dimension: over which dimension to mask values\n    Returns:\n        A tensor with the same shape as `values`\n    """"""\n    if dimension == 1:\n        sequences = tf.transpose(sequences, [0, 2, 1])\n    time_steps1, time_steps2 = tf.shape(sequences)[1], tf.shape(sequences)[2]\n    ones = tf.ones_like(sequences, dtype=tf.int32)\n    pad_values = mask_value * tf.cast(ones, tf.float32)\n    mask = tf.sequence_mask(sequence_lengths, time_steps2)\n    # mask is (batch_size, sentence2_size). we have to tile it for 3d\n    mask3d = tf.tile(tf.expand_dims(mask, 1), (1, time_steps1, 1))\n    masked = tf.where(mask3d, sequences, pad_values)\n    return tf.transpose(masked, [0, 2, 1]) if dimension == 1 else masked\n'"
jack/util/tf/misc.py,1,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\n\ndef mask_for_lengths(lengths, max_length=None, mask_right=True, value=-1000.0):\n    """"""\n    Creates a [batch_size x max_length] mask.\n\n    Args:\n        lengths: int32 1-dim tensor of batch_size lengths\n        max_length: int32 0-dim tensor or python int\n        mask_right: if True, everything before ""lengths"" becomes zero and the\n            rest ""value"", else vice versa\n        value: value for the mask\n\n    Returns:\n        [batch_size x max_length] mask of zeros and ""value""s\n    """"""\n    mask = tf.sequence_mask(lengths, max_length, dtype=tf.float32)\n    if mask_right:\n        mask = 1.0 - mask\n    mask *= value\n    return mask\n'"
jack/util/tf/modular_encoder.py,9,"b""import copy\nimport logging\n\nimport tensorflow as tf\n\nfrom jack.util.tf.interaction_layer import interaction_layer\nfrom jack.util.tf.sequence_encoder import encoder\n\nlogger = logging.getLogger(__name__)\n\n\ndef _flatten(l):\n    if isinstance(l, list):\n        return [module for sl in l for module in _flatten(sl)]\n    else:\n        return [l]\n\n\ndef _unique_module_name(module, layer_depth):\n    inp = module.get('input', '')\n    inp_str = inp if isinstance(inp, str) else '_'.join(inp)\n    name = '_'.join([str(layer_depth), inp_str, module['module']])\n    return name\n\n\ndef modular_encoder(encoder_config, inputs, inputs_length, inputs_mapping, default_repr_dim, dropout, is_eval):\n    outputs = dict(inputs)\n    outputs_length = dict(inputs_length)\n    outputs_mapping = dict(inputs_mapping)\n    seen_layer = set()\n\n    def encode_module(module):\n        module_type = module['module']\n\n        reuse = module['name'] in seen_layer\n        seen_layer.add(module['name'])\n\n        if module_type == 'repeat':\n            reuse = module.get('reuse')\n            for k in range(module['num']):\n                prefix = module['name'] + '/' if reuse else '%s_%d/' % (module['name'], k)\n                for j, inner_module in enumerate(module['encoder']):\n                    # copy this configuration\n                    inner_module = copy.deepcopy(inner_module)\n                    if 'name' not in inner_module:\n                        inner_module['name'] = _unique_module_name(inner_module, j)\n                    inner_module['name'] = prefix + inner_module['name']\n                    encode_module(inner_module)\n            return\n\n        try:\n            key = module['input']\n            out_key = module.get('output', key)\n            if module['module'] in ['concat', 'add', 'mul', 'weighted_add', 'sub']:\n                outputs_length[out_key] = outputs_length[key[0]]\n                outputs_mapping[out_key] = outputs_mapping.get(key[0])\n                if module['module'] == 'concat':\n                    outputs[out_key] = tf.concat([outputs[k] for k in key], 2, name=module['name'])\n                    return\n                if module['module'] == 'add':\n                    outputs[out_key] = tf.add_n([outputs[k] for k in key], name=module['name'])\n                    return\n                if module['module'] == 'sub':\n                    outputs[out_key] = tf.subtract(outputs[key[0]], outputs[key[1]], name=module['name'])\n                    return\n                if module['module'] == 'mul':\n                    o = outputs[key[0]]\n                    for k in key[1:-1]:\n                        o *= outputs[k]\n                    outputs[out_key] = tf.multiply(o, outputs[key[-1]], name=module['name'])\n                    return\n                if module['module'] == 'weighted_add':\n                    bias = module.get('bias', 0.0)\n                    g = tf.layers.dense(tf.concat([outputs[k] for k in key], 2), outputs[key[0]].get_shape()[-1].value,\n                                        tf.sigmoid, bias_initializer=tf.constant_initializer(bias))\n                    outputs[out_key] = tf.identity(g * outputs[key[0]] + (1.0 - g) * outputs[key[0]],\n                                                   name=module['name'])\n                    return\n            if 'repr_dim' not in module:\n                module['repr_dim'] = default_repr_dim\n            if 'dependent' in module:\n                dep_key = module['dependent']\n                outputs[out_key] = interaction_layer(\n                    outputs[key], outputs_length[key],\n                    outputs[dep_key], outputs_length[dep_key],\n                    outputs_mapping.get(key), outputs_mapping.get(dep_key), reuse=reuse, **module)\n            else:\n                if module.get('dropout') is True:\n                    # set dropout to default dropout\n                    module['dropout'] = dropout\n                outputs[out_key] = encoder(outputs[key], outputs_length[key], reuse=reuse, is_eval=is_eval, **module)\n            outputs_length[out_key] = outputs_length[key]\n            outputs_mapping[out_key] = outputs_mapping.get(key)\n        except Exception as e:\n            logger.error('Creating module %s failed.', module['name'])\n            raise e\n\n    encoder_config = _flatten(encoder_config)\n    # don't change original config but copy here\n    encoder_config = copy.deepcopy(encoder_config)\n\n    for i, module in enumerate(encoder_config):\n        if 'name' not in module:\n            module['name'] = _unique_module_name(module, i)\n        encode_module(module)\n\n    return outputs, outputs_length, outputs_mapping\n"""
jack/util/tf/pairwise_losses.py,10,"b'# -*- coding: utf-8 -*-\n\nimport sys\n\nimport tensorflow as tf\n\n\ndef hinge_loss(positive_scores, negative_scores, margin=1.0):\n    """"""\n    Pairwise hinge loss [1]:\n        loss(p, n) = \\sum_i [\\gamma - p_i + n_i]_+\n\n    [1] http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf\n\n    Args:\n        positive_scores: (N,) Tensor containing scores of positive examples.\n        negative_scores: (N,) Tensor containing scores of negative examples.\n        margin: Margin.\n    Returns:\n        Loss value.\n    """"""\n    hinge_losses = tf.nn.relu(margin - positive_scores + negative_scores)\n    loss = tf.reduce_sum(hinge_losses)\n    return loss\n\n\ndef logistic_loss(positive_scores, negative_scores):\n    """"""\n    Pairwise logistic loss [1]:\n        loss(p, n) = \\sum_i log(1 + e^(1 - p_i + n_i))\n\n    [1] http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf\n\n    Args:\n        positive_scores: (N,) Tensor containing scores of positive examples.\n        negative_scores: (N,) Tensor containing scores of negative examples.\n    Returns:\n        Loss value.\n    """"""\n    logistic_losses = tf.log(1 + tf.exp(1 - positive_scores + negative_scores))\n    loss = tf.reduce_sum(logistic_losses)\n    return loss\n\n\ndef mce_loss(positive_scores, negative_scores):\n    """"""\n    Minimum Classification Error (MCE) loss [1]:\n        loss(p, n) = \\sum_i \\sigma(- p_i + n_i)\n\n    [1] http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf\n\n    Args:\n        positive_scores: (N,) Tensor containing scores of positive examples.\n        negative_scores: (N,) Tensor containing scores of negative examples.\n    Returns:\n        Loss value.\n    """"""\n    mce_losses = tf.sigmoid(- positive_scores + negative_scores)\n    loss = tf.reduce_sum(mce_losses)\n    return loss\n\n\ndef square_square_loss(positive_scores, negative_scores, margin=1.0):\n    """"""\n    Square-Square loss [1]:\n        loss(p, n) = \\sum_i - p_i^2 + [\\gamma + n_i]^2_+\n\n    [1] http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf\n\n    Args:\n        positive_scores: (N,) Tensor containing scores of positive examples.\n        negative_scores: (N,) Tensor containing scores of negative examples.\n    :   margin: Margin.\n    Returns:\n        Loss value.\n    """"""\n    square_square_losses = - positive_scores + tf.nn.relu(margin + negative_scores) ** 2\n    loss = tf.reduce_sum(square_square_losses)\n    return loss\n\n\ndef square_exponential_loss(positive_scores, negative_scores, gamma=1.0):\n    """"""\n    Square-Exponential loss [1]:\n        loss(p, n) = \\sum_i - p_i^2 + \\gamma e^(n_i)\n\n    [1] http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf\n\n    Args:\n        positive_scores: (N,) Tensor containing scores of positive examples.\n        negative_scores: (N,) Tensor containing scores of negative examples.\n        gamma: Gamma hyper-parameter.\n    Returns:\n        Loss value.\n    """"""\n    square_exponential_losses = - positive_scores + gamma * tf.exp(negative_scores)\n    loss = tf.reduce_sum(square_exponential_losses)\n    return loss\n\n\ndef get_function(function_name):\n    this_module = sys.modules[__name__]\n    if not hasattr(this_module, function_name):\n        raise ValueError(\'Unknown objective function: {}\'.format(function_name))\n    return getattr(this_module, function_name)\n\n'"
jack/util/tf/rnn.py,40,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef birnn_with_projection(size, fused_rnn, inputs, length, share_rnn=False, projection_scope=None):\n    projection_initializer = tf.constant_initializer(np.concatenate([np.eye(size), np.eye(size)]))\n    with tf.variable_scope(""RNN"", reuse=share_rnn):\n        encoded = fused_birnn(fused_rnn, inputs, sequence_length=length, dtype=tf.float32, time_major=False)[0]\n        encoded = tf.concat(encoded, 2)\n\n    projected = tf.layers.dense(encoded, size, kernel_initializer=projection_initializer, name=projection_scope)\n    return projected\n\n\ndef fused_rnn_backward(fused_rnn, inputs, sequence_length, initial_state=None, dtype=None, scope=None, time_major=True):\n    if not time_major:\n        inputs = tf.transpose(inputs, [1, 0, 2])\n    # assumes that time dim is 0 and batch is 1\n    rev_inputs = tf.reverse_sequence(inputs, sequence_length, 0, 1)\n    rev_outputs, last_state = fused_rnn(rev_inputs, sequence_length=sequence_length, initial_state=initial_state,\n                                        dtype=dtype, scope=scope)\n    outputs = tf.reverse_sequence(rev_outputs, sequence_length, 0, 1)\n    if not time_major:\n        outputs = tf.transpose(outputs, [1, 0, 2])\n    return outputs, last_state\n\n\ndef fused_birnn(fused_rnn, inputs, sequence_length, initial_state=(None, None), dtype=None, scope=None,\n                time_major=False, backward_device=None):\n    with tf.variable_scope(scope or ""BiRNN""):\n        sequence_length = tf.cast(sequence_length, tf.int32)\n        if not time_major:\n            inputs = tf.transpose(inputs, [1, 0, 2])\n        outputs_fw, state_fw = fused_rnn(inputs, sequence_length=sequence_length, initial_state=initial_state[0],\n                                         dtype=dtype, scope=""FW"")\n\n        if backward_device is not None:\n            with tf.device(backward_device):\n                outputs_bw, state_bw = fused_rnn_backward(fused_rnn, inputs, sequence_length, initial_state[1], dtype,\n                                                          scope=""BW"")\n        else:\n            outputs_bw, state_bw = fused_rnn_backward(fused_rnn, inputs, sequence_length, initial_state[1], dtype,\n                                                      scope=""BW"")\n\n        if not time_major:\n            outputs_fw = tf.transpose(outputs_fw, [1, 0, 2])\n            outputs_bw = tf.transpose(outputs_bw, [1, 0, 2])\n    return (outputs_fw, outputs_bw), (state_fw, state_bw)\n\n\ndef pair_of_bidirectional_LSTMs(seq1, seq1_lengths, seq2, seq2_lengths,\n                                output_size, scope=None, drop_keep_prob=1.0,\n                                conditional_encoding=True):\n    """"""Duo of bi-LSTMs over seq1 and seq2 with (optional)conditional encoding.\n\n    Args:\n        seq1 (tensor = time x batch x input): The inputs into the first biLSTM\n        seq1_lengths (tensor = batch): The lengths of the sequences.\n        seq2 (tensor = time x batch x input): The inputs into the second biLSTM\n        seq1_lengths (tensor = batch): The lengths of the sequences.\n        output_size (int): Size of the LSTMs state.\n        scope (string): The TensorFlow scope for the reader.\n        drop_keep_drop (float=1.0): The keep propability for dropout.\n\n    Returns:\n        Outputs (tensor): The outputs from the second bi-LSTM.\n        States (tensor): The cell states from the second bi-LSTM.\n    """"""\n    with tf.variable_scope(scope or ""paired_LSTM_seq1"") as varscope1:\n        # seq1_states: (c_fw, h_fw), (c_bw, h_bw)\n        _, seq1_final_states = dynamic_bidirectional_lstm(\n                        seq1, seq1_lengths, output_size, scope=varscope1,\n                        drop_keep_prob=drop_keep_prob)\n\n\n    with tf.variable_scope(scope or ""paired_LSTM_seq2"") as varscope2:\n        varscope1.reuse_variables()\n        # each [batch_size x max_seq_length x output_size]\n        all_states_fw_bw, final_states_fw_bw = dynamic_bidirectional_lstm(\n                                            seq2, seq2_lengths, output_size,\n                                            seq1_final_states, scope=varscope2,\n                                            drop_keep_prob=drop_keep_prob)\n\n    return all_states_fw_bw, final_states_fw_bw\n\n\ndef dynamic_bidirectional_lstm(inputs, lengths, output_size,\n                               initial_state=(None, None), scope=None,\n                               drop_keep_prob=1.0):\n    """"""Dynamic bi-LSTM reader, with optional initial state.\n\n    Args:\n        inputs (tensor): The inputs into the bi-LSTM\n        lengths (tensor): The lengths of the sequences\n        output_size (int): Size of the LSTM state of the reader.\n        context (tensor=None, tensor=None): Tuple of initial\n                                            (forward, backward) states\n                                            for the LSTM\n        scope (string): The TensorFlow scope for the reader.\n        drop_keep_drop (float=1.0): The keep probability for dropout.\n\n    Returns:\n        all_states (tensor): All forward and backward states\n        final_states (tensor): The final forward and backward states\n    """"""\n    with tf.variable_scope(scope or ""reader""):\n        cell_fw = tf.contrib.rnn.LSTMCell(\n            output_size,\n            state_is_tuple=True,\n            initializer=tf.contrib.layers.xavier_initializer()\n        )\n        cell_bw = tf.contrib.rnn.LSTMCell(\n            output_size,\n            state_is_tuple=True,\n            initializer=tf.contrib.layers.xavier_initializer()\n        )\n\n        if drop_keep_prob != 1.0:\n            cell_fw = tf.contrib.rnn.DropoutWrapper(\n                                    cell=cell_fw,\n                                    output_keep_prob=drop_keep_prob,\n                                    input_keep_prob=drop_keep_prob, seed=1233)\n            cell_bw = tf.contrib.rnn.DropoutWrapper(\n                                    cell=cell_bw,\n                                    output_keep_prob=drop_keep_prob,\n                                    input_keep_prob=drop_keep_prob, seed=1233)\n\n        all_states_fw_bw, final_states_fw_bw = tf.nn.bidirectional_dynamic_rnn(\n            cell_fw,\n            cell_bw,\n            inputs,\n            sequence_length=lengths,\n            initial_state_fw=initial_state[0],\n            initial_state_bw=initial_state[1],\n            dtype=tf.float32\n        )\n\n        return all_states_fw_bw, final_states_fw_bw\n\n\nclass SRUFusedRNN(tf.contrib.rnn.FusedRNNCell):\n    """"""Simple Recurrent Unit, very fast.  https://openreview.net/pdf?id=rJBiunlAW""""""\n\n    def __init__(self, num_units, f_bias=1.0, r_bias=0.0, with_residual=True):\n        self._num_units = num_units\n        cell = _SRUUpdateCell(num_units, with_residual)\n        self._rnn = tf.contrib.rnn.FusedRNNCellAdaptor(cell, use_dynamic_rnn=True)\n        self._constant_bias = [0.0] * self._num_units + [f_bias] * self._num_units\n        if with_residual:\n            self._constant_bias += [r_bias] * self._num_units\n\n        self._constant_bias = np.array(self._constant_bias, np.float32)\n        self._with_residual = with_residual\n\n    def __call__(self, inputs, initial_state=None, dtype=tf.float32, sequence_length=None, scope=None):\n        num_gates = 3 if self._with_residual else 2\n        transformed = tf.layers.dense(inputs, num_gates * self._num_units,\n                                      bias_initializer=tf.constant_initializer(self._constant_bias))\n\n        gates = tf.split(transformed, num_gates, axis=2)\n        forget_gate = tf.sigmoid(gates[1])\n        transformed_inputs = (1.0 - forget_gate) * gates[0]\n        if self._with_residual:\n            residual_gate = tf.sigmoid(gates[2])\n            inputs *= (1.0 - residual_gate)\n            new_inputs = tf.concat([inputs, transformed_inputs, forget_gate, residual_gate], axis=2)\n        else:\n            new_inputs = tf.concat([transformed_inputs, forget_gate], axis=2)\n\n        return self._rnn(new_inputs, initial_state, dtype, sequence_length, scope)\n\n\nclass _SRUUpdateCell(tf.contrib.rnn.RNNCell):\n    """"""Simple Recurrent Unit, very fast.  https://openreview.net/pdf?id=rJBiunlAW""""""\n\n    def __init__(self, num_units, with_residual, activation=None, reuse=None):\n        super(_SRUUpdateCell, self).__init__(_reuse=reuse)\n        self._num_units = num_units\n        self._with_residual = with_residual\n        self._activation = activation or tf.tanh\n\n    @property\n    def state_size(self):\n        return self._num_units\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    def call(self, inputs, state):\n        """"""Simple recurrent unit (SRU).""""""\n        if self._with_residual:\n            base_inputs, transformed_inputs, forget_gate, residual_gate = tf.split(inputs, 4, axis=1)\n            new_state = forget_gate * state + transformed_inputs\n            new_h = residual_gate * self._activation(new_state) + base_inputs\n        else:\n            transformed_inputs, forget_gate = tf.split(inputs, 2, axis=1)\n            new_state = new_h = forget_gate * state + transformed_inputs\n        return new_h, new_state\n'"
jack/util/tf/segment.py,51,"b'import tensorflow as tf\n\n\ndef segment_softmax(scores, segment_ids):\n    """"""Given scores and a partition, converts scores to probs by performing\n    softmax over all rows within a partition.""""""\n\n    # Subtract max\n    num_segments = tf.reduce_max(segment_ids) + 1\n    if len(scores.get_shape()) == 2:\n        max_per_partition = tf.unsorted_segment_max(tf.reduce_max(scores, axis=1), segment_ids, num_segments)\n        scores -= tf.expand_dims(tf.gather(max_per_partition, segment_ids), axis=1)\n    else:\n        max_per_partition = tf.unsorted_segment_max(scores, segment_ids, num_segments)\n        scores -= tf.gather(max_per_partition, segment_ids)\n\n    # Compute probs\n    scores_exp = tf.exp(scores)\n    if len(scores.get_shape()) == 2:\n        scores_exp_sum_per_partition = tf.unsorted_segment_sum(tf.reduce_sum(scores_exp, axis=1), segment_ids,\n                                                               num_segments)\n        probs = scores_exp / tf.expand_dims(tf.gather(scores_exp_sum_per_partition, segment_ids), axis=1)\n    else:\n        scores_exp_sum_per_partition = tf.unsorted_segment_sum(scores_exp, segment_ids, num_segments)\n        probs = scores_exp / tf.gather(scores_exp_sum_per_partition, segment_ids)\n\n    return probs\n\n\ndef segment_argmax(input, segment_ids):\n    """"""Computes row and col indices Tensors of the segment max in the 2D input.""""""\n\n    with tf.name_scope(""segment_argmax""):\n        num_partitions = tf.reduce_max(segment_ids) + 1\n        is_max = segment_is_max(input, segment_ids)\n\n        # The current is_max could still contain multiple True entries per\n        # partition. As long as they are in the same row, that is not a problem.\n        # However, we do need to remove duplicate Trues in the same partition\n        # in multiple rows.\n        # For that, we\'ll multiply is_max with the row indices + 1 and perform\n        # segment_is_max() again.\n\n        rows = tf.shape(input)[0]\n        cols = tf.shape(input)[1]\n        row_indices = tf.tile(tf.expand_dims(tf.range(rows), 1), [1, cols])\n        is_max = segment_is_max(tf.cast(is_max, tf.int32) * (row_indices + 1), segment_ids)\n\n        # Get selected rows and columns\n        row_selected = tf.reduce_any(is_max, axis=1)\n        row_indices = tf.squeeze(tf.where(row_selected))\n        rows_selected = tf.reduce_sum(tf.cast(row_selected, tf.int64))\n\n        # Assert rows_selected is correct & ensure row_indices is always 1D\n        with tf.control_dependencies([tf.assert_equal(rows_selected, num_partitions)]):\n            row_indices = tf.reshape(row_indices, [-1])\n\n        selected_rows_is_max = tf.gather(is_max, row_indices)\n        col_indices = tf.argmax(tf.cast(selected_rows_is_max, tf.int64), axis=1)\n\n        # Pack indices\n        return row_indices, col_indices\n\n\ndef segment_is_max(inputs, segment_ids):\n    num_segments = tf.reduce_max(segment_ids) + 1\n    if len(inputs.get_shape()) > 1:\n        inputs_max = tf.reduce_max(inputs, reduction_indices=list(range(1, len(inputs.get_shape()))))\n    else:\n        inputs_max = inputs\n    max_per_partition = tf.unsorted_segment_max(inputs_max, segment_ids, num_segments)\n    return tf.equal(inputs, tf.gather(max_per_partition, segment_ids))\n\n\ndef segment_sample_select(probs, segment_ids):\n    num_segments = tf.reduce_max(segment_ids) + 1\n    sampled = tf.random_uniform([num_segments])\n\n    def scan_fn(acc, x):\n        p, i = x[0], x[1]\n        prev_v = tf.gather(acc[0], i)\n        new_probs = acc[0] + tf.one_hot(i, num_segments, p)\n        select = tf.logical_and(tf.less(prev_v, 0.0), tf.greater_equal(prev_v + p, 0.0))\n        return new_probs, select\n\n    _, selection = tf.scan(scan_fn, (probs, segment_ids), initializer=(-sampled, False))\n\n    return selection\n\n\ndef segment_top_k(input, segment_ids, k):\n    """"""Computes top k elements for segments in 2D input.\n\n    segment_idx needs to be sorted.\n\n    Returns:\n        [num_segments, k]- tensors for rows, columns, scores of best k results in each segment\n    """"""\n\n    with tf.name_scope(""segment_top_k""):\n        all_top_k_scores, all_top_k_indices = tf.nn.top_k(input, k)\n        rows = tf.tile(tf.expand_dims(tf.range(tf.shape(input)[0], dtype=tf.int32), 1), [1, k])\n\n        result_rows = tf.zeros([k], tf.int32)\n        result_columns = tf.zeros([k], tf.int32)\n        result_scores = tf.zeros([k], tf.float32)\n\n        def replace(old, new):\n            return tf.concat([old[:-1], tf.expand_dims(new, 0)], 0)\n\n        def scan_fn(acc, x):\n            result_row, result_column, result_score, last_index = acc\n\n            row_indices = x[0]\n            segment_idx = x[1]\n            top_k_scores = x[2]\n            col_indices = x[3]\n\n            def merge():\n                new_result_row = tf.concat([result_row, row_indices], 0)\n                new_result_column = tf.concat([result_column, col_indices], 0)\n                new_result_score = tf.concat([result_score, top_k_scores], 0)\n                new_result_score, new_top_k_indices = tf.nn.top_k(new_result_score, k)\n                new_result_row = tf.gather(new_result_row, new_top_k_indices[:k])\n                new_result_column = tf.gather(new_result_column, new_top_k_indices[:k])\n\n                return new_result_row, new_result_column, new_result_score, segment_idx\n\n            return tf.cond(segment_idx > last_index,\n                           lambda: (row_indices, col_indices, top_k_scores, segment_idx),\n                           merge)\n\n        last_index = -1\n        result_rows, result_columns, result_scores, _ = tf.scan(\n            scan_fn, (rows, segment_ids, all_top_k_scores, all_top_k_indices),\n            initializer=(result_rows, result_columns, result_scores, last_index))\n\n        to_gather = tf.squeeze(tf.where((segment_ids[1:] - segment_ids[:-1]) > 0))\n        to_gather = tf.concat([to_gather, tf.shape(segment_ids, out_type=tf.int64) - 1], 0)\n\n        return tf.gather(result_rows, to_gather), tf.gather(result_columns, to_gather), tf.gather(result_scores,\n                                                                                                  to_gather)\n'"
jack/util/tf/sequence_encoder.py,40,"b'import logging\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom jack.util.tf import attention, rnn\nfrom jack.util.tf.activations import activation_from_string\nfrom jack.util.tf.highway import highway_network\n\nlogger = logging.getLogger(__name__)\n\n\ndef encoder(sequence, seq_length, repr_dim=100, module=\'lstm\', num_layers=1, reuse=None, residual=False,\n            activation=None, layer_norm=False, name=\'encoder\', dropout=None, is_eval=True, **kwargs):\n    if num_layers == 1:\n        if layer_norm:\n            with tf.variable_scope(\'layernorm\', reuse=False) as vs:\n                vs._reuse = False  # HACK\n                num_layernorms = sum(1 for v in vs.global_variables() if \'layernorm\' in v.name)\n                sequence = tf.contrib.layers.layer_norm(sequence, scope=str(num_layernorms))\n\n        with tf.variable_scope(name, reuse=reuse):\n            if module == \'lstm\':\n                out = bi_lstm(repr_dim, sequence, seq_length, **kwargs)\n                if activation:\n                    out = activation_from_string(activation)(out)\n            elif module == \'sru\':\n                with_residual = sequence.get_shape()[2].value == repr_dim\n                out = bi_sru(repr_dim, sequence, seq_length, with_residual, **kwargs)\n                if activation:\n                    out = activation_from_string(activation)(out)\n            elif module == \'rnn\':\n                out = bi_rnn(repr_dim, tf.nn.rnn_cell.BasicRNNCell(repr_dim, activation_from_string(activation)),\n                             sequence, seq_length, **kwargs)\n            elif module == \'gru\':\n                out = bi_rnn(repr_dim, tf.contrib.rnn.GRUBlockCell(repr_dim), sequence,\n                             seq_length, **kwargs)\n                if activation:\n                    out = activation_from_string(activation)(out)\n            elif module == \'gldr\':\n                out = gated_linear_dilated_residual_network(\n                    repr_dim, sequence, **kwargs)\n            elif module == \'conv\':\n                out = convnet(repr_dim, sequence, 1, activation=activation_from_string(activation), **kwargs)\n            elif module == \'conv_glu\':\n                out = gated_linear_convnet(repr_dim, sequence, 1, **kwargs)\n            elif module == \'conv_separable\':\n                out = depthwise_separable_convolution(\n                    repr_dim, sequence, activation=activation_from_string(activation), **kwargs)\n            elif module == \'dense\':\n                out = tf.layers.dense(sequence, repr_dim)\n                if activation:\n                    out = activation_from_string(activation)(out)\n            elif module == \'highway\':\n                out = highway_network(sequence, num_layers, activation_from_string(activation))\n            elif module == \'self_attn\':\n                outs = []\n                num_attn = kwargs.get(\'num_attn_heads\', 1)\n                for i in range(num_attn):\n                    with tf.variable_scope(str(i)):\n                        attn = self_attention(sequence, seq_length, repr_dim=repr_dim, **kwargs)\n                        outs.append(attn)\n                out = tf.concat(outs, 2) if num_attn > 1 else outs[0]\n            elif module == \'positional_encoding\':\n                out = positional_encoding(sequence, seq_length)\n            else:\n                raise ValueError(""Unknown encoder type: %s"" % module)\n\n            if residual:\n                if out.get_shape()[-1].value != sequence.get_shape()[-1].value:\n                    logging.error(\n                        \'Residual connection only possible if input to sequence encoder %s of type %s has same \'\n                        \'dimension (%d) as output (%d).\' % (name, module, sequence.get_shape()[-1].value,\n                                                            out.get_shape()[-1].value))\n                    raise RuntimeError()\n                out += sequence\n\n            if dropout is not None:\n                out = tf.cond(\n                    tf.logical_and(tf.greater(dropout, 0.0), tf.logical_not(is_eval)),\n                    lambda: tf.nn.dropout(out, 1.0 - dropout, noise_shape=[tf.shape(out)[0], 1, tf.shape(out)[-1]]),\n                    lambda: out)\n    else:\n        out = encoder(sequence, seq_length, repr_dim, module, num_layers - 1, reuse, residual,\n                      activation, layer_norm, name, dropout=dropout, is_eval=is_eval, **kwargs)\n\n        out = encoder(out, seq_length, repr_dim, module, 1, reuse, residual, activation, layer_norm,\n                      name + str(num_layers - 1), dropout=dropout, is_eval=is_eval, **kwargs)\n\n    return out\n\n\n# RNN Encoders\ndef _bi_rnn(size, fused_rnn, sequence, seq_length, with_projection=False):\n    output = rnn.fused_birnn(fused_rnn, sequence, seq_length, dtype=tf.float32, scope=\'rnn\')[0]\n    output = tf.concat(output, 2)\n    if with_projection:\n        projection_initializer = tf.constant_initializer(np.concatenate([np.eye(size), np.eye(size)]))\n        output = tf.layers.dense(output, size, kernel_initializer=projection_initializer, name=\'projection\')\n    return output\n\n\ndef bi_lstm(size, sequence, seq_length, with_projection=False, **kwargs):\n    fused_rnn = tf.contrib.rnn.LSTMBlockFusedCell(size)\n    return _bi_rnn(size, fused_rnn, sequence, seq_length, with_projection)\n\n\ndef bi_rnn(size, rnn_cell, sequence, seq_length, with_projection=False, **kwargs):\n    fused_rnn = tf.contrib.rnn.FusedRNNCellAdaptor(rnn_cell, use_dynamic_rnn=True)\n    return _bi_rnn(size, fused_rnn, sequence, seq_length, with_projection)\n\n\ndef bi_sru(size, sequence, seq_length, with_residual=True, name=\'bi_sru\', reuse=None, with_projection=False, **kwargs):\n    """"""Simple Recurrent Unit, very fast.  https://openreview.net/pdf?id=rJBiunlAW.""""""\n    fused_rnn = rnn.SRUFusedRNN(size, with_residual)\n    return _bi_rnn(size, fused_rnn, sequence, seq_length, with_projection)\n\n\n# Convolution Encoders\n\n\ndef convnet(repr_dim, inputs, num_layers, conv_width=3, activation=tf.nn.relu, **kwargs):\n    # dim reduction\n    output = inputs\n    for i in range(num_layers):\n        output = _convolutional_block(output, repr_dim, conv_width=conv_width, name=""conv_%d"" % i)\n    return output\n\n\ndef _convolutional_block(inputs, out_channels, conv_width=3, name=\'conv\', activation=tf.nn.relu, **kwargs):\n    channels = inputs.get_shape()[2].value\n    # [filter_height, filter_conv_width, in_channels, out_channels]\n    f = tf.get_variable(name + \'_filter\', [conv_width, channels, out_channels])\n    output = tf.nn.conv1d(inputs, f, 1, padding=\'SAME\', name=name)\n    return activation(output)\n\n\ndef depthwise_separable_convolution(repr_dim, inputs, conv_width, activation=tf.nn.relu, bias=True, **kwargs):\n    inputs = tf.expand_dims(inputs, 1)\n    shapes = inputs.shape.as_list()\n\n    depthwise_filter = tf.get_variable(""depthwise_filter"",\n                                       (1, conv_width, shapes[-1], 1),\n                                       dtype=tf.float32)\n    pointwise_filter = tf.get_variable(""pointwise_filter"", (1, 1, shapes[-1], repr_dim),\n                                       dtype=tf.float32)\n    outputs = tf.nn.separable_conv2d(inputs,\n                                     depthwise_filter,\n                                     pointwise_filter,\n                                     strides=(1, 1, 1, 1),\n                                     padding=""SAME"")\n    outputs = tf.squeeze(outputs, 1)\n    if bias:\n        b = tf.get_variable(""bias"", outputs.shape[-1], initializer=tf.zeros_initializer())\n        outputs += b\n    outputs = activation(outputs)\n    return outputs\n\n\n# following implementation of fast encoding in https://openreview.net/pdf?id=HJRV1ZZAW\ndef _residual_dilated_convolution_block(inputs, dilation=1, conv_width=3, name=""dilated_conv""):\n    # [filter_height, filter_conv_width, in_channels, out_channels]\n    output = inputs\n    channels = inputs.get_shape()[2].value\n    for i in range(2):\n        # [filter_height, filter_conv_width, in_channels, out_channels]\n        output = _convolutional_glu_block(output, channels, dilation, conv_width, name=name + \'_\' + str(i))\n    return output + inputs\n\n\ndef _convolutional_glu_block(inputs, out_channels, dilation=1, conv_width=3, name=\'conv_glu\', **kwargs):\n    channels = inputs.get_shape()[2].value\n    # [filter_height, filter_conv_width, in_channels, out_channels]\n    f = tf.get_variable(name + \'_filter\', [1, conv_width, channels, out_channels * 2])\n    output = tf.nn.atrous_conv2d(tf.expand_dims(inputs, 1), f, dilation, \'SAME\', name=name)\n    output = tf.squeeze(output, 1)\n    output, gate = tf.split(output, 2, 2)\n    output *= tf.sigmoid(gate)\n    return output\n\n\ndef gated_linear_dilated_residual_network(out_size, inputs, dilations, conv_width=3, name=\'gldr_network\', reuse=None,\n                                          **kwargs):\n    """"""Follows https://openreview.net/pdf?id=HJRV1ZZAW.\n\n    Args:\n        out_size: size of output\n        inputs: input sequence tensor [batch_size, length, size]\n        dilations: list, representing (half of the) network depth; each residual dilation block comprises 2 convolutions\n\n    Returns:\n        [batch_size, length, out_size] tensor\n    """"""\n    # dim reduction\n    output = _convolutional_glu_block(inputs, out_size, name=\'conv_dim_reduction\')\n    for i, d in enumerate(dilations):\n        output = _residual_dilated_convolution_block(output, d, conv_width, name=\'dilated_conv_%d\' % i)\n    return output\n\n\ndef gated_linear_convnet(out_size, inputs, num_layers, conv_width=3, **kwargs):\n    """"""Follows https://openreview.net/pdf?id=HJRV1ZZAW.\n\n    Args:\n        out_size: size of output\n        inputs: input sequence tensor [batch_size, length, size]\n        num_layers: number of conv layers with conv_width\n\n    Returns:\n        [batch_size, length, out_size] tensor\n    """"""\n    # dim reduction\n    output = inputs\n    for i in range(num_layers):\n        output = _convolutional_glu_block(output, out_size, conv_width=conv_width, name=""conv_%d"" % i)\n    return output\n\n\ndef positional_encoding(inputs, lengths, **kwargs):\n    repr_dim = inputs.get_shape()[-1].value\n    pos = tf.reshape(tf.range(0.0, tf.to_float(tf.reduce_max(lengths)), dtype=tf.float32), [-1, 1])\n    i = np.arange(0, repr_dim, 2, np.float32)\n    denom = np.reshape(np.power(10000.0, i / repr_dim), [1, -1])\n    enc = tf.expand_dims(tf.concat([tf.sin(pos / denom), tf.cos(pos / denom)], 1), 0)\n    return inputs + tf.tile(enc, [tf.shape(inputs)[0], 1, 1])\n\n\n# Self attention layers\ndef self_attention(inputs, lengths, attn_type=\'bilinear\', scaled=True, activation=None, with_sentinel=False, **kwargs):\n    if attn_type == \'bilinear\':\n        attn_states = attention.bilinear_attention(\n            inputs, inputs, lengths, scaled, with_sentinel, **kwargs)[2]\n    elif attn_type == \'dot\':\n        attn_states = attention.dot_attention(\n            inputs, inputs, lengths, scaled, with_sentinel, **kwargs)[2]\n    elif attn_type == \'diagonal_bilinear\':\n        attn_states = \\\n            attention.diagonal_bilinear_attention(\n                inputs, inputs, lengths, scaled, with_sentinel, **kwargs)[2]\n    elif attn_type == \'mlp\':\n        attn_states = \\\n            attention.mlp_attention(\n                kwargs[\'repr_dim\'], activation, inputs, inputs, lengths, with_sentinel, **kwargs)[2]\n    else:\n        raise ValueError(""Unknown attention type: %s"" % attn_type)\n\n    return attn_states\n'"
jack/util/tf/simple.py,2,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\n\ndef fully_connected_projection(inputs, output_size):\n    """"""Projects inputs onto target dimension. Returns logits, loss, and argmax.\n\n    Creates fully connected projection layer. Then applies cross entropy\n    softmax to get the loss. Calculate predictions via argmax.\n    Args:\n        inputs (tensor): Input into the projection layer.\n        output_size (int): Size of the targets (used in projection layer).\n    """"""\n    init = tf.contrib.layers.xavier_initializer(uniform=True) #uniform=False for truncated normal\n    logits = tf.contrib.layers.fully_connected(inputs, output_size, weights_initializer=init, activation_fn=None)\n    return logits\n'"
jack/util/tf/xqa.py,14,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nfrom jack.util.tf.segment import segment_softmax\n\n\ndef xqa_crossentropy_loss(start_scores, end_scores, answer_span, answer2support, support2question, use_sum=True):\n    """"""Very common XQA loss function.""""""\n    num_questions = tf.reduce_max(support2question) + 1\n\n    start, end = answer_span[:, 0], answer_span[:, 1]\n\n    start_probs = segment_softmax(start_scores, support2question)\n    start_probs = tf.gather_nd(start_probs, tf.stack([answer2support, start], 1))\n\n    # only start probs are normalized on multi-paragraph, end probs conditioned on start only on per support level\n    num_answers = tf.shape(answer_span)[0]\n    is_aligned = tf.equal(tf.shape(end_scores)[0], num_answers)\n    end_probs = tf.cond(\n        is_aligned,\n        lambda: tf.gather_nd(tf.nn.softmax(end_scores), tf.stack([tf.range(num_answers, dtype=tf.int32), end], 1)),\n        lambda: tf.gather_nd(segment_softmax(end_scores, support2question), tf.stack([answer2support, end], 1))\n    )\n\n    answer2question = tf.gather(support2question, answer2support)\n    # compute losses individually\n    if use_sum:\n        span_probs = tf.unsorted_segment_sum(\n            start_probs, answer2question, num_questions) * tf.unsorted_segment_sum(\n            end_probs, answer2question, num_questions)\n    else:\n        span_probs = tf.unsorted_segment_max(\n            start_probs, answer2question, num_questions) * tf.unsorted_segment_max(\n            end_probs, answer2question, num_questions)\n\n    return -tf.reduce_mean(tf.log(tf.maximum(1e-6, span_probs + 1e-6)))\n'"
jack/util/torch/__init__.py,0,b''
jack/util/torch/embedding.py,0,"b'# -*- coding: utf-8 -*-\n\nimport math\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional\n\nfrom jack.util.torch import misc\n\n\nclass ConvCharEmbeddingModule(nn.Module):\n    def __init__(self, num_chars, size, conv_width=5):\n        super(ConvCharEmbeddingModule, self).__init__()\n        self._size = size\n        self._conv_width = conv_width\n        self._embeddings = torch.nn.Embedding(num_chars, size)\n        self._embeddings.weight.data.mul_(0.1)\n        self._conv = torch.nn.Conv1d(size, size, conv_width, padding=math.floor(conv_width / 2))\n\n    def forward(self, unique_word_chars, unique_word_lengths, sequences_as_uniqs=None):\n        long_tensor = torch.cuda.LongTensor if torch.cuda.device_count() > 0 else torch.LongTensor\n        embedded_chars = self._embeddings(unique_word_chars.type(long_tensor))\n        # [N, S, L]\n        conv_out = self._conv(embedded_chars.transpose(1, 2))\n        # [N, L]\n        conv_mask = misc.mask_for_lengths(unique_word_lengths)\n        conv_out = conv_out + conv_mask.unsqueeze(1)\n        embedded_words = conv_out.max(2)[0]\n\n        if sequences_as_uniqs is None:\n            return embedded_words\n        else:\n            if not isinstance(sequences_as_uniqs, list):\n                sequences_as_uniqs = [sequences_as_uniqs]\n\n            all_embedded = []\n            for word_idxs in sequences_as_uniqs:\n                all_embedded.append(functional.embedding(\n                    word_idxs.type(long_tensor), embedded_words))\n            return all_embedded\n'"
jack/util/torch/highway.py,0,"b'""""""Credits: https://github.com/kefirski/pytorch_Highway""""""\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Highway(nn.Module):\n    def __init__(self, size, num_layers, f=F.tanh):\n        super(Highway, self).__init__()\n\n        self.num_layers = num_layers\n\n        self.nonlinear = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n\n        self.linear = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n\n        self.gate = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n\n        self.f = f\n\n    def forward(self, x):\n        """"""\n            :param x: tensor with shape of [batch_size, size]\n            :return: tensor with shape of [batch_size, size]\n            applies \xcf\x83(x) \xe2\xa8\x80 (f(G(x))) + (1 - \xcf\x83(x)) \xe2\xa8\x80 (Q(x)) transformation | G and Q is affine transformation,\n            f is non-linear transformation, \xcf\x83(x) is affine transformation with sigmoid non-linearition\n            and \xe2\xa8\x80 is element-wise multiplication\n            """"""\n\n        for layer in range(self.num_layers):\n            gate = F.sigmoid(self.gate[layer](x))\n\n            nonlinear = self.f(self.nonlinear[layer](x))\n            linear = self.linear[layer](x)\n\n            x = gate * nonlinear + (1 - gate) * linear\n\n        return x\n'"
jack/util/torch/misc.py,0,"b""import torch\nfrom torch.autograd import Function\n\n\ndef mask_for_lengths(length, max_length=None, mask_right=True, value=-1e6):\n    max_length = max_length or length.max().data[0]\n    mask = torch.cuda.IntTensor() if length.is_cuda else torch.IntTensor()\n    mask = torch.arange(0, max_length, 1, out=mask)\n    mask = torch.autograd.Variable(mask).type_as(length)\n    mask /= length.unsqueeze(1)\n    mask = mask.clamp(0, 1)\n    mask = mask.float()\n    if not mask_right:\n        mask = 1.0 - mask\n    mask *= value\n    return mask\n\n\ndef segment_max(inputs, segment_ids, num_segments=None, default=0.0):\n    # highly optimized to decrease the amount of actual invocation of pytorch calls\n    # assumes that most segments have 1 or 0 elements\n    segment_ids, indices = torch.sort(segment_ids)\n    inputs = torch.index_select(inputs, 0, indices)\n    output = SegmentMax.apply(inputs, segment_ids, num_segments, default)\n    return output\n\n\nclass SegmentMax(Function):\n    @staticmethod\n    def forward(ctx, inputs, segment_ids, num_segments=None, default=0.0):\n        # segments must be sorted by ids\n        # highly optimized code, do not change if you don't know what you are doing.\n        num_segments = num_segments or segment_ids.max() + 1\n\n        zero_t = inputs[0].clone().view(1, -1)\n        zero_t.fill_(default)\n\n        if torch.is_tensor(segment_ids):\n            segment_ids = segment_ids.cpu()\n\n        lengths = []\n        segm_lengths = []\n        num_lengths = [0] * 10\n        num_zeros = [0]\n\n        lengths_extend = lengths.extend\n        num_lengths_extend = num_lengths.extend\n        segm_lengths_append = segm_lengths.append\n        segm_lengths_extend = segm_lengths.extend\n\n        _lengths = [[l] * l for l in range(len(num_lengths))]\n        _zeros = [0] * num_segments\n\n        def add_length(l):\n            if l >= len(num_lengths):\n                diff = l + 5 - len(num_lengths)\n                num_lengths_extend([0] * diff)\n                _lengths.extend([new_l] * new_l for new_l in range(len(_lengths), l + 5))\n            num_lengths[l] += l\n            segm_lengths_append(l)\n            lengths_extend(_lengths[l])\n\n        def add_zeros(n):\n            segm_lengths_extend(_zeros[:n])\n            num_zeros[0] += n\n\n        offset = 0\n        prev_s = segment_ids[0]\n        if prev_s:\n            add_zeros(prev_s)\n        for i, s in enumerate(segment_ids):\n            if prev_s != s:\n                n_z = s - prev_s - 1\n                if n_z > 0:\n                    add_zeros(n_z)\n                add_length(i - offset)\n                offset = i\n            prev_s = s\n        add_length(segment_ids.shape[0] - offset)\n        add_zeros(num_segments - prev_s - 1)\n\n        lengths = torch.cuda.LongTensor(lengths) if inputs.is_cuda else torch.LongTensor(lengths)\n        segm_lengths = torch.cuda.LongTensor(segm_lengths) if inputs.is_cuda else torch.LongTensor(segm_lengths)\n        _, lengths_sorted = torch.sort(lengths, 0)\n        _, segm_lengths_sorted = torch.sort(segm_lengths, 0)\n\n        inputs_sorted = torch.index_select(inputs, 0, lengths_sorted)\n\n        offset = [0]\n        ctx.maxes = dict()\n\n        def compute_segment(l, n):\n            segment = inputs_sorted.narrow(0, offset[0], n)\n            if l > 1:\n                segment = segment.view(n // l, l, -1)\n                ctx.maxes[l] = MyMax()\n                segment = MyMax.forward(ctx.maxes[l], segment, dim=1)[0]\n            offset[0] += n\n            return segment\n\n        segments = [compute_segment(l, n) for l, n in enumerate(num_lengths) if n > 0]\n        segments = [zero_t.expand(num_zeros[0], zero_t.shape[1])] + segments\n\n        segments = torch.cat(segments, 0)\n        _, rev_segm_sorted = torch.sort(segm_lengths_sorted)\n\n        ctx.rev_segm_sorted = rev_segm_sorted\n        ctx.num_lengths = num_lengths\n        ctx.lengths_sorted = lengths_sorted\n        ctx.num_zeros = num_zeros[0]\n\n        output = torch.index_select(segments, 0, rev_segm_sorted)\n\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_outputs):\n        size = grad_outputs.size(1)\n        segm_sorted = torch.sort(ctx.rev_segm_sorted)[1]\n        grad_outputs = torch.index_select(grad_outputs, 0, segm_sorted)\n\n        offset = [ctx.num_zeros]\n\n        def backward_segment(l, n):\n            segment_grad = grad_outputs.narrow(0, offset[0], n // l)\n            if l > 1:\n                segment_grad = MyMax.backward(ctx.maxes[l], segment_grad)[0].view(n, size)\n            offset[0] += n // l\n            return segment_grad\n\n        segment_grads = [backward_segment(l, n) for l, n in enumerate(ctx.num_lengths) if n > 0]\n        grads = torch.cat(segment_grads, 0)\n        rev_length_sorted = torch.sort(ctx.lengths_sorted)[1]\n        grads = torch.index_select(grads, 0, rev_length_sorted)\n\n        return grads, None, None, None\n\n\nclass MyMax:\n    @classmethod\n    def forward(cls, ctx, input, dim, keepdim=None):\n        ctx.dim = dim\n        ctx.keepdim = False if keepdim is None else keepdim\n        ctx.input_size = input.size()\n\n        output, indices = input.max(dim=dim, keepdim=ctx.keepdim)\n        ctx.indices = indices\n\n        return output, indices\n\n    @classmethod\n    def backward(cls, ctx, grad_output, grad_indices=None):\n        grad_input = torch.autograd.Variable(grad_output.data.new(*ctx.input_size).zero_())\n        dim = ctx.dim\n        indices = ctx.indices\n        if not ctx.keepdim:\n            grad_output = grad_output.unsqueeze(dim)\n            indices = indices.unsqueeze(dim)\n\n        grad_input.scatter_(dim, indices, grad_output)\n\n        return grad_input, None, None\n"""
jack/util/torch/rnn.py,0,"b'import torch\nfrom torch import nn\n\n\nclass BiLSTM(nn.Module):\n    def __init__(self, input_size, size, start_state_given=False):\n        super(BiLSTM, self).__init__()\n        self._size = size\n        self._bilstm = nn.LSTM(input_size, size, 1, bidirectional=True, batch_first=True)\n        self._bilstm.bias_ih_l0.data[size:2 * size].fill_(1.0)\n        self._bilstm.bias_ih_l0_reverse.data[size:2 * size].fill_(1.0)\n        self._start_state_given = start_state_given\n        if not start_state_given:\n            self._lstm_start_hidden = nn.Parameter(torch.zeros(2, size))\n            self._lstm_start_state = nn.Parameter(torch.zeros(2, size))\n\n    def forward(self, inputs, lengths=None, start_state=None):\n        if not self._start_state_given:\n            batch_size = inputs.size(0)\n            start_hidden = self._lstm_start_hidden.unsqueeze(1).expand(2, batch_size, self._size).contiguous()\n            start_state = self._lstm_start_state.unsqueeze(1).expand(2, batch_size, self._size).contiguous()\n            start_state = (start_hidden, start_state)\n\n        if lengths is not None:\n            new_lengths, indices = torch.sort(lengths, dim=0, descending=True)\n            inputs = torch.index_select(inputs, 0, indices)\n            if self._start_state_given:\n                start_state = (torch.index_select(start_state[0], 1, indices),\n                               torch.index_select(start_state[1], 1, indices))\n            new_lengths = [l.data[0] for l in new_lengths]\n            inputs = nn.utils.rnn.pack_padded_sequence(inputs, new_lengths, batch_first=True)\n\n        output, (h_n, c_n) = self._bilstm(inputs, start_state)\n\n        if lengths is not None:\n            output = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)[0]\n            _, back_indices = torch.sort(indices, dim=0)\n            output = torch.index_select(output, 0, back_indices)\n            h_n = torch.index_select(h_n, 1, back_indices)\n            c_n = torch.index_select(c_n, 1, back_indices)\n\n        return output, (h_n, c_n)\n'"
jack/util/torch/segment.py,0,"b""import torch\nfrom torch.autograd import Function\n\n\ndef mask_for_lengths(length, max_length=None, mask_right=True, value=-1e6):\n    max_length = max_length or length.max().data[0]\n    mask = torch.cuda.IntTensor() if length.is_cuda else torch.IntTensor()\n    mask = torch.arange(0, max_length, 1, out=mask)\n    mask = torch.autograd.Variable(mask).type_as(length)\n    mask /= length.unsqueeze(1)\n    mask = mask.clamp(0, 1)\n    mask = mask.float()\n    if not mask_right:\n        mask = 1.0 - mask\n    mask *= value\n    return mask\n\n\ndef segment_max(inputs, segment_ids, num_segments=None, default=0.0):\n    # highly optimized to decrease the amount of actual invocation of pytorch calls\n    # assumes that most segments have 1 or 0 elements\n    segment_ids, indices = torch.sort(segment_ids)\n    inputs = torch.index_select(inputs, 0, indices)\n    output = SegmentMax.apply(inputs, segment_ids, num_segments, default)\n    return output\n\n\nclass SegmentMax(Function):\n    @staticmethod\n    def forward(ctx, inputs, segment_ids, num_segments=None, default=0.0):\n        # segments must be sorted by ids\n        # highly optimized code, do not change if you don't know what you are doing.\n        num_segments = num_segments or segment_ids.max() + 1\n\n        zero_t = inputs[0].clone().view(1, -1)\n        zero_t.fill_(default)\n\n        if torch.is_tensor(segment_ids):\n            segment_ids = segment_ids.cpu()\n\n        lengths = []\n        segm_lengths = []\n        num_lengths = [0] * 10\n        num_zeros = [0]\n\n        lengths_extend = lengths.extend\n        num_lengths_extend = num_lengths.extend\n        segm_lengths_append = segm_lengths.append\n        segm_lengths_extend = segm_lengths.extend\n\n        _lengths = [[l] * l for l in range(len(num_lengths))]\n        _zeros = [0] * num_segments\n\n        def add_length(l):\n            if l >= len(num_lengths):\n                diff = l + 5 - len(num_lengths)\n                num_lengths_extend([0] * diff)\n                _lengths.extend([new_l] * new_l for new_l in range(len(_lengths), l + 5))\n            num_lengths[l] += l\n            segm_lengths_append(l)\n            lengths_extend(_lengths[l])\n\n        def add_zeros(n):\n            segm_lengths_extend(_zeros[:n])\n            num_zeros[0] += n\n\n        offset = 0\n        prev_s = segment_ids[0]\n        if prev_s:\n            add_zeros(prev_s)\n        for i, s in enumerate(segment_ids):\n            if prev_s != s:\n                n_z = s - prev_s - 1\n                if n_z > 0:\n                    add_zeros(n_z)\n                add_length(i - offset)\n                offset = i\n            prev_s = s\n        add_length(segment_ids.shape[0] - offset)\n        add_zeros(num_segments - prev_s - 1)\n\n        lengths = torch.cuda.LongTensor(lengths) if inputs.is_cuda else torch.LongTensor(lengths)\n        segm_lengths = torch.cuda.LongTensor(segm_lengths) if inputs.is_cuda else torch.LongTensor(segm_lengths)\n        _, lengths_sorted = torch.sort(lengths, 0)\n        _, segm_lengths_sorted = torch.sort(segm_lengths, 0)\n\n        inputs_sorted = torch.index_select(inputs, 0, lengths_sorted)\n\n        offset = [0]\n        ctx.maxes = dict()\n\n        def compute_segment(l, n):\n            segment = inputs_sorted.narrow(0, offset[0], n)\n            if l > 1:\n                segment = segment.view(n // l, l, -1)\n                ctx.maxes[l] = _MyMax()\n                segment = _MyMax.forward(ctx.maxes[l], segment, dim=1)[0]\n            offset[0] += n\n            return segment\n\n        segments = [compute_segment(l, n) for l, n in enumerate(num_lengths) if n > 0]\n        segments = [zero_t.expand(num_zeros[0], zero_t.shape[1])] + segments\n\n        segments = torch.cat(segments, 0)\n        _, rev_segm_sorted = torch.sort(segm_lengths_sorted)\n\n        ctx.rev_segm_sorted = rev_segm_sorted\n        ctx.num_lengths = num_lengths\n        ctx.lengths_sorted = lengths_sorted\n        ctx.num_zeros = num_zeros[0]\n\n        output = torch.index_select(segments, 0, rev_segm_sorted)\n\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_outputs):\n        size = grad_outputs.size(1)\n        segm_sorted = torch.sort(ctx.rev_segm_sorted)[1]\n        grad_outputs = torch.index_select(grad_outputs, 0, segm_sorted)\n\n        offset = [ctx.num_zeros]\n\n        def backward_segment(l, n):\n            segment_grad = grad_outputs.narrow(0, offset[0], n // l)\n            if l > 1:\n                segment_grad = _MyMax.backward(ctx.maxes[l], segment_grad)[0].view(n, size)\n            offset[0] += n // l\n            return segment_grad\n\n        segment_grads = [backward_segment(l, n) for l, n in enumerate(ctx.num_lengths) if n > 0]\n        grads = torch.cat(segment_grads, 0)\n        rev_length_sorted = torch.sort(ctx.lengths_sorted)[1]\n        grads = torch.index_select(grads, 0, rev_length_sorted)\n\n        return grads, None, None, None\n\n\nclass _MyMax:\n    @classmethod\n    def forward(cls, ctx, input, dim, keepdim=None):\n        ctx.dim = dim\n        ctx.keepdim = False if keepdim is None else keepdim\n        ctx.input_size = input.size()\n\n        output, indices = input.max(dim=dim, keepdim=ctx.keepdim)\n        ctx.indices = indices\n\n        return output, indices\n\n    @classmethod\n    def backward(cls, ctx, grad_output, grad_indices=None):\n        grad_input = torch.autograd.Variable(grad_output.data.new(*ctx.input_size).zero_())\n        dim = ctx.dim\n        indices = ctx.indices\n        if not ctx.keepdim:\n            grad_output = grad_output.unsqueeze(dim)\n            indices = indices.unsqueeze(dim)\n\n        grad_input.scatter_(dim, indices, grad_output)\n\n        return grad_input, None, None\n"""
jack/util/torch/xqa.py,0,"b'# -*- coding: utf-8 -*-\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\nclass XQAMinCrossentropyLossModule(nn.Module):\n    def forward(self, start_scores, end_scores, answer_span, answer_to_question):\n        """"""very common XQA loss function.""""""\n        long_tensor = torch.cuda.LongTensor if torch.cuda.device_count() > 0 else torch.LongTensor\n        answer_span = answer_span.type(long_tensor)\n        start, end = answer_span[:, 0], answer_span[:, 1]\n\n        batch_size1 = start.data.shape[0]\n        batch_size2 = start_scores.data.shape[0]\n        is_aligned = batch_size1 == batch_size2\n\n        start_scores = start_scores if is_aligned else torch.index_select(start_scores, dim=0, index=answer_to_question)\n        end_scores = end_scores if is_aligned else torch.index_select(end_scores, dim=0, index=answer_to_question)\n\n        partitioned_loss = []\n        for i, j in enumerate(answer_to_question):\n            j = j.data[0]\n            while j >= len(partitioned_loss):\n                partitioned_loss.append([])\n            loss = -torch.index_select(F.log_softmax(start_scores[i], dim=0), dim=0, index=start[i])\n            loss -= torch.index_select(F.log_softmax(end_scores[i], dim=0), dim=0, index=end[i])\n            partitioned_loss[j].append(loss)\n\n        for j, l in enumerate(partitioned_loss):\n            partitioned_loss[j] = torch.stack(l).min()\n\n        loss = torch.stack(partitioned_loss).mean()\n        return loss\n'"
projects/knowledge_integration/qa/__init__.py,0,b''
projects/knowledge_integration/qa/definition_model.py,13,"b'import random\n\nimport numpy as np\nimport spacy\nimport tensorflow as tf\n\nfrom jack.core import TensorPortWithDefault, TensorPortTensors, TensorPort\nfrom jack.readers.extractive_qa.shared import XQAPorts\nfrom jack.readers.extractive_qa.tensorflow.abstract_model import AbstractXQAModelModule\nfrom jack.readers.extractive_qa.tensorflow.answer_layer import answer_layer\nfrom jack.util.map import numpify\nfrom jack.util.preprocessing import sort_by_tfidf\nfrom jack.util.tf.modular_encoder import modular_encoder\nfrom projects.knowledge_integration.qa.shared import XQAAssertionInputModule\nfrom projects.knowledge_integration.shared import AssertionMRPorts\nfrom projects.knowledge_integration.tfutil import word_with_char_embed, embedding_refinement\n\n\nclass DefinitionPorts:\n    definition_lengths = TensorPortWithDefault(np.zeros([0], np.int32), [None],\n                                               ""definition_lengths"", ""Length of definition."", ""[R]"")\n    definitions = TensorPortWithDefault(np.zeros([0, 0], np.int32), [None, None], ""definitions"",\n                                        ""Represents batch dependent definition word ids."", ""[R, L]"")\n    definition2question = TensorPortWithDefault(np.zeros([0], np.int32), [None], ""definition2question"",\n                                                ""Question idx per definition"", ""[R]"")\n\n\nclass XQAAssertionDefinitionInputModule(XQAAssertionInputModule):\n\n    def setup(self):\n        super().setup()\n        self.use_definitions = True\n        self._rng = random.Random(1)\n\n    def set_reader(self, reader):\n        self.reader = reader\n\n    @property\n    def output_ports(self):\n        return super(XQAAssertionDefinitionInputModule, self).output_ports + [\n            DefinitionPorts.definitions, DefinitionPorts.definition_lengths, DefinitionPorts.definition2question]\n\n    def create_batch(self, annotations, is_eval, with_answers):\n        frac = self.config.get(\'training_fraction_with_definition\', 1.0)\n        if not self.use_definitions or (frac < 1.0 and not is_eval and self._rng.random() > frac):\n            return super(XQAAssertionDefinitionInputModule, self).create_batch(annotations, is_eval, with_answers)\n        batch = super(XQAAssertionDefinitionInputModule, self).create_batch(annotations, True, with_answers)\n\n        lemma_vocab = batch[\'__lemma_vocab\']\n        vocab = batch[\'__vocab\']\n        rev_vocab = batch[\'__rev_vocab\']\n        word_chars = batch[AssertionMRPorts.word_chars].tolist()\n        word_lengths = batch[AssertionMRPorts.word_char_length].tolist()\n        word2lemma = batch[AssertionMRPorts.word2lemma].tolist()\n        support = batch[AssertionMRPorts.support]\n\n        rev_lemma_vocab = {v: k for k, v in lemma_vocab.items()}\n        topk = self.config[\'topk\']\n        self.reader.model_module.set_topk(topk)\n        spans = self.reader.model_module(batch, [XQAPorts.answer_span])[XQAPorts.answer_span]\n\n        definitions = []\n        definition_lengths = []\n        definition2question = []\n\n        seen_answer_lemmas = None\n        for i, s in enumerate(spans):\n            j = i // topk\n            if i % topk == 0:\n                seen_answer_lemmas = set()\n            doc_idx_map = [i for i, q_id in enumerate(batch[XQAPorts.support2question]) if q_id == j]\n            doc_idx, start, end = s[0], s[1], s[2]\n            answer_token_ids = support[doc_idx_map[doc_idx], start:end + 1]\n            answer_lemmas = [rev_lemma_vocab[word2lemma[idd]] for idd in answer_token_ids]\n            answer_lemma = \' \'.join(answer_lemmas)\n            if answer_lemma in seen_answer_lemmas:\n                continue\n            seen_answer_lemmas.add(answer_lemma)\n            ks = self._knowledge_store.assertion_keys_for_subject(answer_lemma, resource=\'wikipedia_firstsent\')\n            if not ks:\n                # remove leading or trailing stop words or non alnum words\n                while answer_lemmas and (answer_lemmas[0] in spacy.en.STOP_WORDS or not answer_lemmas[0].isalnum()):\n                    answer_lemmas = answer_lemmas[1:]\n                while answer_lemmas and (answer_lemmas[-1] in spacy.en.STOP_WORDS or not answer_lemmas[-1].isalnum()):\n                    answer_lemmas = answer_lemmas[:-1]\n                answer_lemma = \' \'.join(answer_lemmas)\n                if answer_lemma in seen_answer_lemmas:\n                    continue\n                seen_answer_lemmas.add(answer_lemma)\n                ks = self._knowledge_store.assertion_keys_for_subject(answer_lemma, resource=\'wikipedia_firstsent\')\n\n            defns = [self._nlp(self._knowledge_store.get_assertion(key)) for key in ks]\n            if len(defns) > 3:\n                indices_scores = sort_by_tfidf(\n                    \' \'.join(annotations[j].question_lemmas + annotations[j].support_lemmas[doc_idx]),\n                    [\' \'.join(t.lemma_ for t in d) for d in defns])\n                # only select the top 3 definition with best match to the support and question\n                defns = [defns[i] for i, _ in indices_scores[:3]]\n\n            for defn in defns:\n                definition_lengths.append(len(defn))\n                definition2question.append(j)\n                defn_ids = []\n                for t in defn:\n                    w = t.orth_\n                    if w not in vocab:\n                        vocab[w] = len(vocab)\n                        word_lengths.append(min(len(w), 20))\n                        word_chars.append([self.char_vocab.get(c, 0) for c in w[:20]])\n                        rev_vocab.append(w)\n                        if t.lemma_ not in lemma_vocab:\n                            lemma_vocab[t.lemma_] = len(lemma_vocab)\n                        word2lemma.append(lemma_vocab[t.lemma_])\n                    defn_ids.append(vocab[w])\n                definitions.append(defn_ids)\n\n        batch[DefinitionPorts.definitions] = definitions\n        batch[DefinitionPorts.definition_lengths] = definition_lengths\n        batch[DefinitionPorts.definition2question] = definition2question\n        batch[AssertionMRPorts.word_chars] = word_chars\n        batch[AssertionMRPorts.word_char_length] = word_lengths\n        batch[AssertionMRPorts.word2lemma] = word2lemma\n        batch[AssertionMRPorts.is_eval] = is_eval\n\n        word_embeddings = np.zeros([len(rev_vocab), self.emb_matrix.shape[1]])\n        for i, w in enumerate(rev_vocab):\n            word_embeddings[i] = self._get_emb(self.vocab(w))\n\n        batch[AssertionMRPorts.word_embeddings] = word_embeddings\n\n        return numpify(batch, keys=[\n            DefinitionPorts.definitions, DefinitionPorts.definition_lengths, DefinitionPorts.definition2question,\n            AssertionMRPorts.word_chars, AssertionMRPorts.word_char_length, AssertionMRPorts.word2lemma])\n\n\nclass ModularAssertionDefinitionQAModel(AbstractXQAModelModule):\n    _input_ports = [AssertionMRPorts.question_length, AssertionMRPorts.support_length,\n                    # char\n                    AssertionMRPorts.word_chars, AssertionMRPorts.word_char_length,\n                    AssertionMRPorts.question, AssertionMRPorts.support,\n                    # optional, only during training\n                    AssertionMRPorts.is_eval,\n                    # for assertions\n                    AssertionMRPorts.word_embeddings,\n                    AssertionMRPorts.assertion_lengths,\n                    AssertionMRPorts.assertion2question,\n                    AssertionMRPorts.assertions,\n                    AssertionMRPorts.word2lemma,\n                    XQAPorts.word_in_question,\n                    XQAPorts.support2question,\n                    XQAPorts.correct_start,\n                    XQAPorts.answer2support_training,\n                    DefinitionPorts.definitions,\n                    DefinitionPorts.definition_lengths,\n                    DefinitionPorts.definition2question]\n\n    @property\n    def input_ports(self):\n        return self._input_ports\n\n    def set_topk(self, k):\n        self._topk_assign(k)\n\n    def create_output(self, shared_resources, input_tensors):\n        tensors = TensorPortTensors(input_tensors)\n\n        question_length = tensors.question_length\n        support_length = tensors.support_length\n        support2question = tensors.support2question\n        word_chars = tensors.word_chars\n        word_char_length = tensors.word_char_length\n        question = tensors.question\n        support = tensors.support\n        is_eval = tensors.is_eval\n        word_embeddings = tensors.word_embeddings\n        assertion_lengths = tensors.assertion_lengths\n        assertion2question = tensors.assertion2question\n        assertions = tensors.assertions\n        definition_lengths = tensors.definition_lengths\n        definition2question = tensors.definition2question\n        definitions = tensors.definitions\n        word2lemma = tensors.word2lemma\n\n        model = shared_resources.config[\'model\']\n        repr_dim = shared_resources.config[\'repr_dim\']\n        input_size = shared_resources.config[""repr_dim_input""]\n        dropout = shared_resources.config.get(""dropout"", 0.0)\n        size = shared_resources.config[""repr_dim""]\n        with_char_embeddings = shared_resources.config.get(""with_char_embeddings"", False)\n\n        word_embeddings.set_shape([None, input_size])\n\n        if shared_resources.config.get(\'no_reading\', False):\n            new_word_embeddings = tf.layers.dense(word_embeddings, size, activation=tf.nn.relu,\n                                                  name=""embeddings_projection"")\n            if with_char_embeddings:\n                new_word_embeddings = word_with_char_embed(\n                    size, new_word_embeddings, tensors.word_chars, tensors.word_char_length,\n                    len(shared_resources.char_vocab))\n            keep_prob = 1.0 - dropout\n            if keep_prob < 1.0:\n                new_word_embeddings = tf.cond(is_eval,\n                                              lambda: new_word_embeddings,\n                                              lambda: tf.nn.dropout(new_word_embeddings, keep_prob, [1, size]))\n            reading_sequence_offset = [support, question, assertions]\n        else:\n            if shared_resources.config.get(""assertion_limit"", 0) > 0:\n                reading_sequence = [support, question, assertions, definitions]\n                reading_sequence_lengths = [support_length, question_length, assertion_lengths, definition_lengths]\n                reading_sequence_to_batch = [support2question, None, assertion2question, definition2question]\n            else:\n                reading_sequence = [support, question, definitions]\n                reading_sequence_lengths = [support_length, question_length, definition_lengths]\n                reading_sequence_to_batch = [support2question, None, definition2question]\n\n            reading_encoder_config = shared_resources.config[\'reading_module\']\n            new_word_embeddings, reading_sequence_offset, _ = embedding_refinement(\n                size, word_embeddings, reading_encoder_config,\n                reading_sequence, reading_sequence_to_batch, reading_sequence_lengths,\n                word2lemma, word_chars, word_char_length, is_eval,\n                keep_prob=1.0 - shared_resources.config.get(\'dropout\', 0.0),\n                with_char_embeddings=with_char_embeddings, num_chars=len(shared_resources.char_vocab))\n\n        emb_question = tf.nn.embedding_lookup(new_word_embeddings, reading_sequence_offset[1],\n                                              name=\'embedded_question\')\n        emb_support = tf.nn.embedding_lookup(new_word_embeddings, reading_sequence_offset[0],\n                                             name=\'embedded_support\')\n\n        inputs = {\'question\': emb_question, \'support\': emb_support,\n                  \'word_in_question\': tf.expand_dims(tensors.word_in_question, 2)}\n        inputs_length = {\'question\': question_length, \'support\': support_length,\n                         \'word_in_question\': support_length}\n        inputs_mapping = {\'question\': None, \'support\': support2question}\n\n        encoder_config = model[\'encoder_layer\']\n\n        encoded, lengths, mappings = modular_encoder(\n            encoder_config, inputs, inputs_length, inputs_mapping, repr_dim, dropout, tensors.is_eval)\n\n        with tf.variable_scope(\'answer_layer\'):\n            answer_layer_config = model[\'answer_layer\']\n            encoded_question = encoded[answer_layer_config.get(\'question\', \'question\')]\n            encoded_support = encoded[answer_layer_config.get(\'support\', \'support\')]\n\n            if \'repr_dim\' not in answer_layer_config:\n                answer_layer_config[\'repr_dim\'] = repr_dim\n            if \'max_span_size\' not in answer_layer_config:\n                answer_layer_config[\'max_span_size\'] = shared_resources.config.get(\'max_span_size\', 16)\n            topk = tf.get_variable(\n                \'topk\', initializer=shared_resources.config.get(\'topk\', 1), dtype=tf.int32, trainable=False)\n            topk_p = tf.placeholder(tf.int32, [], \'topk_setter\')\n            topk_assign = topk.assign(topk_p)\n            self._topk_assign = lambda k: self.tf_session.run(topk_assign, {topk_p: k})\n\n            start_scores, end_scores, doc_idx, predicted_start_pointer, predicted_end_pointer = \\\n                answer_layer(encoded_question, lengths[answer_layer_config.get(\'question\', \'question\')],\n                             encoded_support, lengths[answer_layer_config.get(\'support\', \'support\')],\n                             mappings[answer_layer_config.get(\'support\', \'support\')],\n                             tensors.answer2support, tensors.is_eval,\n                             tensors.correct_start, topk=topk, **answer_layer_config)\n\n        span = tf.stack([doc_idx, predicted_start_pointer, predicted_end_pointer], 1)\n\n        span = tf.stack([doc_idx, predicted_start_pointer, predicted_end_pointer], 1)\n\n        return TensorPort.to_mapping(self.output_ports, (start_scores, end_scores, span))\n'"
projects/knowledge_integration/qa/shared.py,13,"b'import random\nfrom typing import *\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom jack.core import TensorPortTensors, TensorPort\nfrom jack.readers.extractive_qa.shared import XQAInputModule, XQAPorts\nfrom jack.readers.extractive_qa.tensorflow.abstract_model import AbstractXQAModelModule\nfrom jack.readers.extractive_qa.tensorflow.answer_layer import answer_layer\nfrom jack.readers.extractive_qa.util import prepare_data\nfrom jack.util import preprocessing\nfrom jack.util.map import numpify\nfrom jack.util.preprocessing import sort_by_tfidf\nfrom jack.util.tf.modular_encoder import modular_encoder\nfrom projects.knowledge_integration.knowledge_store import KnowledgeStore\nfrom projects.knowledge_integration.shared import AssertionMRPorts\nfrom projects.knowledge_integration.tfutil import embedding_refinement, word_with_char_embed\n\nXQAAssertionAnnotation = NamedTuple(\'XQAAssertionAnnotation\', [\n    (\'question_tokens\', List[str]),\n    (\'question_lemmas\', List[str]),\n    (\'question_ids\', List[int]),\n    (\'question_length\', int),\n    (\'support_tokens\', List[str]),\n    (\'support_lemmas\', List[str]),\n    (\'support_ids\', List[int]),\n    (\'support_length\', int),\n    (\'word_in_question\', List[float]),\n    (\'token_offsets\', List[int]),\n    (\'answer_spans\', Optional[List[Tuple[int, int]]]),\n    (\'selected_supports\', List[int]),\n])\n\n\nclass XQAAssertionInputModule(XQAInputModule):\n    _output_ports = [AssertionMRPorts.question_length, AssertionMRPorts.support_length,\n                     # char\n                     AssertionMRPorts.word_chars, AssertionMRPorts.word_char_length,\n                     AssertionMRPorts.question, AssertionMRPorts.support,\n                     # optional, only during training\n                     AssertionMRPorts.is_eval,\n                     # for assertions\n                     AssertionMRPorts.word_embeddings,\n                     AssertionMRPorts.assertion_lengths,\n                     AssertionMRPorts.assertion2question,\n                     AssertionMRPorts.assertions,\n                     AssertionMRPorts.question_arg_span,\n                     AssertionMRPorts.assertion2question_arg_span,\n                     AssertionMRPorts.support_arg_span,\n                     AssertionMRPorts.assertion2support_arg_span,\n                     AssertionMRPorts.word2lemma,\n                     XQAPorts.word_in_question,\n                     XQAPorts.support2question,\n                     # optional, only during training\n                     XQAPorts.answer2support_training, XQAPorts.correct_start,\n                     # for output module\n                     XQAPorts.token_offsets, XQAPorts.selected_support]\n\n    def __init__(self, shared_resources):\n        super(XQAAssertionInputModule, self).__init__(shared_resources)\n        self._nlp = preprocessing.spacy_nlp()\n        self._rng = random.Random(123)\n\n    def setup(self):\n        self._knowledge_store = KnowledgeStore(self.shared_resources.config[""assertion_dir""])\n        self._sources = self.shared_resources.config[""assertion_sources""]\n        self._limit = self.shared_resources.config.get(""assertion_limit"", 10)\n        self.vocab = self.shared_resources.vocab\n        self.config = self.shared_resources.config\n        self.batch_size = self.config.get(""batch_size"", 1)\n        self.dropout = self.config.get(""dropout"", 0.0)\n        self._rng = random.Random(self.config.get(""seed"", 123))\n        self.emb_matrix = self.vocab.emb.lookup\n        self.default_vec = np.zeros([self.vocab.emb_length])\n        self.char_vocab = self.shared_resources.char_vocab\n\n    @property\n    def output_ports(self):\n        return self._output_ports\n\n    def preprocess_instance(self, question, answers=None):\n        has_answers = answers is not None\n\n        q_tokenized, q_ids, q_lemmas, q_length, s_tokenized, s_ids, s_lemmas, s_length, \\\n        word_in_question, token_offsets, answer_spans = prepare_data(\n            question, answers, self.vocab, self.config.get(""lowercase"", False),\n            with_answers=has_answers, max_support_length=self.config.get(""max_support_length"", None),\n            spacy_nlp=True, with_lemmas=True)\n\n        max_num_support = self.config.get(""max_num_support"", len(question.support))  # take all per default\n\n        # take max supports by TF-IDF (we subsample to max_num_support in create batch)\n        # following https://arxiv.org/pdf/1710.10723.pdf\n        if len(question.support) > 1:\n            scores = sort_by_tfidf(\' \'.join(q_tokenized), [\' \'.join(s) for s in s_tokenized])\n            selected_supports = [s_idx for s_idx, _ in scores[:max_num_support]]\n            s_tokenized = [s_tokenized[s_idx] for s_idx in selected_supports]\n            s_lemmas = [s_lemmas[s_idx] for s_idx in selected_supports]\n            s_ids = [s_ids[s_idx] for s_idx in selected_supports]\n            s_length = [s_length[s_idx] for s_idx in selected_supports]\n            word_in_question = [word_in_question[s_idx] for s_idx in selected_supports]\n            token_offsets = [token_offsets[s_idx] for s_idx in selected_supports]\n            answer_spans = [answer_spans[s_idx] for s_idx in selected_supports]\n        else:\n            selected_supports = list(range(len(question.support)))\n\n        return XQAAssertionAnnotation(\n            question_tokens=q_tokenized,\n            question_lemmas=q_lemmas,\n            question_ids=q_ids,\n            question_length=q_length,\n            support_tokens=s_tokenized,\n            support_lemmas=s_lemmas,\n            support_ids=s_ids,\n            support_length=s_length,\n            word_in_question=word_in_question,\n            token_offsets=token_offsets,\n            answer_spans=answer_spans if has_answers else None,\n            selected_supports=selected_supports,\n        )\n\n    def create_batch(self, annotations, is_eval: bool, with_answers: bool):\n        q_tokenized = [a.question_tokens for a in annotations]\n        question_lengths = [a.question_length for a in annotations]\n\n        max_training_support = self.config.get(\'max_training_support\', 2)\n        s_tokenized = []\n        s_lemmas = []\n        support_lengths = []\n        wiq = []\n        offsets = []\n        support2question = []\n        # aligns with support2question, used in output module to get correct index to original set of supports\n        selected_support = []\n        all_spans = []\n        for i, a in enumerate(annotations):\n            s_lemmas.append([])\n            all_spans.append([])\n            if len(a.support_tokens) > max_training_support > 0 and not is_eval:\n                # sample only 2 paragraphs and take first with double probability (the best) to speed\n                # things up. Following https://arxiv.org/pdf/1710.10723.pdf\n                is_done = False\n                any_answer = any(a.answer_spans)\n                # sample until there is at least one possible answer (if any)\n                while not is_done:\n                    selected = self._rng.sample(range(0, len(a.support_tokens) + 1), max_training_support + 1)\n                    if 0 in selected and 1 in selected:\n                        selected = [s - 1 for s in selected if s > 0]\n                    else:\n                        selected = [max(0, s - 1) for s in selected[:max_training_support]]\n                    is_done = not any_answer or any(a.answer_spans[s] for s in selected)\n                selected = set(max(0, s - 1) for s in selected)\n            else:\n                selected = set(range(len(a.support_tokens)))\n            for s in selected:\n                s_tokenized.append(a.support_tokens[s])\n                s_lemmas[-1].append(a.support_lemmas[s])\n                support_lengths.append(a.support_length[s])\n                wiq.append(a.word_in_question[s])\n                offsets.append(a.token_offsets[s])\n                selected_support.append(a.selected_supports[s])\n                support2question.append(i)\n                if with_answers:\n                    all_spans[-1].append(a.answer_spans[s])\n\n        word_chars, word_lengths, word_ids, vocab, rev_vocab = \\\n            preprocessing.unique_words_with_chars(q_tokenized + s_tokenized, self.char_vocab)\n\n        question = word_ids[:len(q_tokenized)]\n        support = word_ids[len(q_tokenized):]\n\n        ass_lengths = []\n        ass2question = []\n        ass2unique = []\n        lemma2idx = dict()\n        question_arg_span = []\n        support_arg_span = []\n        assertion2question_arg_span = []\n        assertion2support_arg_span = []\n        question_arg_span_idx = dict()\n        support_arg_span_idx = dict()\n\n        word2lemma = [None] * len(rev_vocab)\n\n        heuristic = self.config.get(\'heuristic\', \'pair\')\n        s_offset = 0\n        for i, annot in enumerate(annotations):\n            # collect uniq lemmas:\n            for k, l in enumerate(annot.question_lemmas):\n                if l not in lemma2idx:\n                    lemma2idx[l] = len(lemma2idx)\n                word2lemma[question[i][k]] = lemma2idx[l]\n            for k, ls in enumerate(s_lemmas[i]):\n                for k2, l in enumerate(ls):\n                    if l not in lemma2idx:\n                        lemma2idx[l] = len(lemma2idx)\n                    word2lemma[support[s_offset + k][k2]] = lemma2idx[l]\n\n            if self._limit == 0:\n                s_offset += len(s_lemmas[i])\n                continue\n\n            if heuristic == \'pair\':\n                assertions, assertion_args = self._knowledge_store.get_connecting_assertion_keys(\n                    annot.question_lemmas, [l for ls in s_lemmas[i] for l in ls], self._sources)\n            elif heuristic == \'tfidf\':\n                assertions, assertion_args = self._knowledge_store.get_assertion_keys(\n                    [l for ls in s_lemmas[i] for l in ls], self._sources)\n                assertions = list(assertions.keys())\n                assertion_strings = [self._knowledge_store.get_assertion(key) for key in assertions]\n                scores = sort_by_tfidf(\' \'.join(annot.question_tokens), assertion_strings)\n                assertions = {assertions[i]: s for i, s in scores}\n\n            sorted_assertions = sorted(assertions.items(), key=lambda x: -x[1])\n            added_assertions = set()\n            for key, _ in sorted_assertions:\n                if len(added_assertions) == self._limit:\n                    break\n                a = self._nlp(self._knowledge_store.get_assertion(key, cache=True))\n                a_lemma = "" "".join(t.lemma_ for t in a)\n                if a_lemma in added_assertions:\n                    continue\n                else:\n                    added_assertions.add(a_lemma)\n                ass2question.append(i)\n                ass_lengths.append(len(a))\n                if heuristic == \'pair\':\n                    q_arg_span = assertion_args[key][0]\n                    q_arg_span = (i, q_arg_span[0], q_arg_span[1])\n                    s_arg_start, s_arg_end = assertion_args[key][1]\n                    doc_idx = 0\n                    for ls in s_lemmas[i]:\n                        if s_arg_start < len(ls):\n                            break\n                        else:\n                            doc_idx += 1\n                            s_arg_start -= len(ls)\n                            s_arg_end -= len(ls)\n                    s_arg_span = (s_offset + doc_idx, s_arg_start, s_arg_end)\n                    if q_arg_span not in question_arg_span_idx:\n                        question_arg_span_idx[q_arg_span] = len(question_arg_span)\n                        question_arg_span.append(assertion_args[key][0])\n                    if s_arg_span not in support_arg_span_idx:\n                        support_arg_span_idx[s_arg_span] = len(support_arg_span)\n                        support_arg_span.append(assertion_args[key][1])\n                    assertion2question_arg_span.append(question_arg_span_idx[q_arg_span])\n                    assertion2support_arg_span.append(support_arg_span_idx[s_arg_span])\n\n                u_ass = []\n                for t in a:\n                    w = t.orth_\n                    if w not in vocab:\n                        vocab[w] = len(vocab)\n                        word_lengths.append(min(len(w), 20))\n                        word_chars.append([self.char_vocab.get(c, 0) for c in w[:20]])\n                        rev_vocab.append(w)\n                        if t.lemma_ not in lemma2idx:\n                            lemma2idx[t.lemma_] = len(lemma2idx)\n                        word2lemma.append(lemma2idx[t.lemma_])\n                    u_ass.append(vocab[w])\n                ass2unique.append(u_ass)\n\n            s_offset += len(s_lemmas[i])\n\n        word_embeddings = np.zeros([len(rev_vocab), self.emb_matrix.shape[1]])\n        for i, w in enumerate(rev_vocab):\n            word_embeddings[i] = self._get_emb(self.vocab(w))\n\n        if not ass2unique:\n            ass2unique.append([])\n            question_arg_span = support_arg_span = np.zeros([0, 2], dtype=np.int32)\n\n        output = {\n            AssertionMRPorts.word_chars: word_chars,\n            AssertionMRPorts.word_char_length: word_lengths,\n            AssertionMRPorts.question: question,\n            AssertionMRPorts.support: support,\n            AssertionMRPorts.support_length: support_lengths,\n            AssertionMRPorts.question_length: question_lengths,\n            AssertionMRPorts.is_eval: is_eval,\n            AssertionMRPorts.word_embeddings: word_embeddings,\n            AssertionMRPorts.assertion_lengths: ass_lengths,\n            AssertionMRPorts.assertion2question: ass2question,\n            AssertionMRPorts.assertions: ass2unique,\n            AssertionMRPorts.word2lemma: word2lemma,\n            AssertionMRPorts.question_arg_span: question_arg_span,\n            AssertionMRPorts.support_arg_span: support_arg_span,\n            AssertionMRPorts.assertion2question_arg_span: assertion2question_arg_span,\n            AssertionMRPorts.assertion2support_arg_span: assertion2support_arg_span,\n            XQAPorts.word_in_question: wiq,\n            XQAPorts.support2question: support2question,\n            XQAPorts.token_offsets: offsets,\n            XQAPorts.selected_support: selected_support,\n            \'__vocab\': vocab,\n            \'__rev_vocab\': rev_vocab,\n            \'__lemma_vocab\': lemma2idx,\n        }\n\n        if with_answers:\n            spans = [s for a in all_spans for spans_per_support in a for s in spans_per_support]\n            span2support = []\n            support_idx = 0\n            for a in all_spans:\n                for spans_per_support in a:\n                    span2support.extend([support_idx] * len(spans_per_support))\n                    support_idx += 1\n            output.update({\n                XQAPorts.answer_span_target: [span for span in spans] if spans else np.zeros([0, 2], np.int32),\n                XQAPorts.correct_start: [] if is_eval else [span[0] for span in spans],\n                XQAPorts.answer2support_training: span2support,\n            })\n\n        # we can only numpify in here, because bucketing is not possible prior\n        batch = numpify(output, keys=self.output_ports + self.training_ports)\n        return batch\n\n\nclass ModularAssertionQAModel(AbstractXQAModelModule):\n    _input_ports = [AssertionMRPorts.question_length, AssertionMRPorts.support_length,\n                    # char\n                    AssertionMRPorts.word_chars, AssertionMRPorts.word_char_length,\n                    AssertionMRPorts.question, AssertionMRPorts.support,\n                    # optional, only during training\n                    AssertionMRPorts.is_eval,\n                    # for assertions\n                    AssertionMRPorts.word_embeddings,\n                    AssertionMRPorts.assertion_lengths,\n                    AssertionMRPorts.assertion2question,\n                    AssertionMRPorts.assertions,\n                    AssertionMRPorts.word2lemma,\n                    XQAPorts.word_in_question,\n                    XQAPorts.support2question,\n                    XQAPorts.correct_start,\n                    XQAPorts.answer2support_training]\n\n    @property\n    def input_ports(self) -> Sequence[TensorPort]:\n        return self._input_ports\n\n    def set_topk(self, k):\n        self._topk_assign(k)\n\n    def create_output(self, shared_resources, input_tensors):\n        tensors = TensorPortTensors(input_tensors)\n\n        question_length = tensors.question_length\n        support_length = tensors.support_length\n        support2question = tensors.support2question\n        word_chars = tensors.word_chars\n        word_char_length = tensors.word_char_length\n        question = tensors.question\n        support = tensors.support\n        is_eval = tensors.is_eval\n        word_embeddings = tensors.word_embeddings\n        assertion_lengths = tensors.assertion_lengths\n        assertion2question = tensors.assertion2question\n        assertions = tensors.assertions\n        word2lemma = tensors.word2lemma\n\n        model = shared_resources.config[\'model\']\n        repr_dim = shared_resources.config[\'repr_dim\']\n        input_size = shared_resources.config[""repr_dim_input""]\n        dropout = shared_resources.config.get(""dropout"", 0.0)\n        size = shared_resources.config[""repr_dim""]\n        with_char_embeddings = shared_resources.config.get(""with_char_embeddings"", False)\n\n        word_embeddings.set_shape([None, input_size])\n\n        if shared_resources.config.get(\'no_reading\', False):\n            new_word_embeddings = tf.layers.dense(word_embeddings, size, activation=tf.nn.relu,\n                                                  name=""embeddings_projection"")\n            if with_char_embeddings:\n                new_word_embeddings = word_with_char_embed(\n                    size, new_word_embeddings, tensors.word_chars, tensors.word_char_length,\n                    len(shared_resources.char_vocab))\n            keep_prob = 1.0 - dropout\n            if keep_prob < 1.0:\n                new_word_embeddings = tf.cond(is_eval,\n                                              lambda: new_word_embeddings,\n                                              lambda: tf.nn.dropout(new_word_embeddings, keep_prob, [1, size]))\n            reading_sequence_offset = [support, question, assertions]\n        else:\n            if shared_resources.config.get(""assertion_limit"", 0) > 0:\n                reading_sequence = [support, question, assertions]\n                reading_sequence_lengths = [support_length, question_length, assertion_lengths]\n                reading_sequence_to_batch = [support2question, None, assertion2question]\n            else:\n                reading_sequence = [support, question]\n                reading_sequence_lengths = [support_length, question_length]\n                reading_sequence_to_batch = [support2question, None]\n\n            reading_encoder_config = shared_resources.config[\'reading_module\']\n            new_word_embeddings, reading_sequence_offset, _ = embedding_refinement(\n                size, word_embeddings, reading_encoder_config,\n                reading_sequence, reading_sequence_to_batch, reading_sequence_lengths,\n                word2lemma, word_chars, word_char_length, is_eval,\n                keep_prob=1.0 - shared_resources.config.get(\'dropout\', 0.0),\n                with_char_embeddings=with_char_embeddings, num_chars=len(shared_resources.char_vocab))\n\n        emb_question = tf.nn.embedding_lookup(new_word_embeddings, reading_sequence_offset[1],\n                                              name=\'embedded_question\')\n        emb_support = tf.nn.embedding_lookup(new_word_embeddings, reading_sequence_offset[0],\n                                             name=\'embedded_support\')\n\n        inputs = {\'question\': emb_question, \'support\': emb_support,\n                  \'word_in_question\': tf.expand_dims(tensors.word_in_question, 2),\n                  \'question_ones\': tf.expand_dims(tf.ones(tf.shape(emb_question)[:2], tf.float32), 2)}\n        inputs_length = {\'question\': question_length, \'support\': support_length,\n                         \'word_in_question\': support_length}\n        inputs_mapping = {\'question\': None, \'support\': support2question}\n\n        encoder_config = model[\'encoder_layer\']\n\n        encoded, lengths, mappings = modular_encoder(\n            encoder_config, inputs, inputs_length, inputs_mapping, repr_dim, dropout, tensors.is_eval)\n\n        with tf.variable_scope(\'answer_layer\'):\n            answer_layer_config = model[\'answer_layer\']\n            encoded_question = encoded[answer_layer_config.get(\'question\', \'question\')]\n            encoded_support = encoded[answer_layer_config.get(\'support\', \'support\')]\n\n            if \'repr_dim\' not in answer_layer_config:\n                answer_layer_config[\'repr_dim\'] = repr_dim\n            if \'max_span_size\' not in answer_layer_config:\n                answer_layer_config[\'max_span_size\'] = shared_resources.config.get(\'max_span_size\', 16)\n            topk = tf.get_variable(\n                \'topk\', initializer=shared_resources.config.get(\'topk\', 1), dtype=tf.int32, trainable=False)\n            topk_p = tf.placeholder(tf.int32, [], \'topk_setter\')\n            topk_assign = topk.assign(topk_p)\n            self._topk_assign = lambda k: self.tf_session.run(topk_assign, {topk_p: k})\n\n            start_scores, end_scores, doc_idx, predicted_start_pointer, predicted_end_pointer = \\\n                answer_layer(encoded_question, lengths[answer_layer_config.get(\'question\', \'question\')],\n                             encoded_support, lengths[answer_layer_config.get(\'support\', \'support\')],\n                             mappings[answer_layer_config.get(\'support\', \'support\')],\n                             tensors.answer2support, tensors.is_eval,\n                             tensors.correct_start, topk=topk, **answer_layer_config)\n\n        span = tf.stack([doc_idx, predicted_start_pointer, predicted_end_pointer], 1)\n\n        return TensorPort.to_mapping(self.output_ports, (start_scores, end_scores, span))\n'"
projects/knowledge_integration/scripts/__init__.py,0,b''
projects/knowledge_integration/scripts/extract_conceptnet.py,0,"b'""""""Extract ConceptNet assertions and add them to an assertion store.""""""\n\nimport gzip\nimport re\n\nimport spacy\n\nfrom projects.knowledge_integration.knowledge_store import KnowledgeStore\n\n\ndef uncamel(name):\n    s1 = re.sub(\'(.)([A-Z][a-z]+)\', r\'\\1 \\2\', name)\n    return re.sub(\'([a-z0-9])([A-Z])\', r\'\\1 \\2\', s1).lower()\n\n\ndef only_string(concept_str):\n    if concept_str.startswith(""/r/""):\n        return uncamel(concept_str[3:])\n    else:\n        res = concept_str[6:]\n        if ""/"" in res:\n            res = res[:res.index(""/"")]\n        return res.replace(""_"", "" "")\n\n\ndef normalize_and_sois(s):\n    i = s.find(""[["")\n    offset = 0\n    sois = list()\n    while i >= 0:\n        start = i - offset\n        offset += 2\n        sois.append((start, s.find(""]]"", i) - offset))\n        offset += 2\n        i = s.find(""[["", i + 1)\n    s_norm = s.replace(""[["", """").replace(""]]"", """").lower()\n    return s_norm, sois\n\n\n__reg = r\'^[^\\t]+/en/([^\\t])+/en/\'  # only get connections between english concepts\ndef is_valid(l):\n    return re.match(__reg, l) is not None and ""/d/verbosity"" not in l and (\n        ""/d/conceptnet/4/en"" not in l or l.count(""/s/contributor/omcs/"") > 1)\n\n\ndef lemmatized(tokens, start, end):\n    return "" "".join(t.lemma_ for t in tokens if t.idx < end and t.idx >= start)\n\n\ndef extract_assertions(conceptnet_path, knowledge_store):\n    # nlp = spacy.load(\'en\', disable=[\'parser\', \'ner\', \'textcat\'])\n    nlp = spacy.load(\'en\', parser=False, entity=False, matcher=False)\n    rel2sf = dict()\n    counter = 0\n    with gzip.GzipFile(conceptnet_path) as f:\n        for l in f:\n            l = l.decode(\'utf-8\')\n            if not is_valid(l):\n                continue\n            if counter % 100000 == 0:\n                logger.info(\'%d assertions added\' % counter)\n            try:\n                split = l.strip().split(""\\t"")\n                [rel, subj, obj] = split[1:4]\n                rel, subj, obj = only_string(rel), only_string(subj), only_string(obj)\n                if subj == obj:\n                    continue\n\n                surface_form = rel2sf.get(rel)\n                if surface_form is None:\n                    j = json.loads(split[4])\n                    surface_form = j.get(""surfaceText"", ""[[%s]] %s [[%s]]"" % (subj, rel, obj))\n                    surface_form = surface_form.replace(""[[%s]]"" % subj, ""[[subj]]"").replace(""[[%s]]"" % obj, ""[[obj]]"")\n                    rel2sf[rel] = surface_form\n\n                subj_start = surface_form.find(""[[subj]]"")\n                obj_start = surface_form.find(""[[obj]]"")\n                if subj_start < 0 or obj_start < 0:\n                    continue\n                if subj_start < obj_start:\n                    surface_form = surface_form.replace(""[[subj]]"", subj)\n                    obj_start = surface_form.index(""[[obj]]"")\n                    surface_form = surface_form.replace(""[[obj]]"", obj)\n                else:\n                    surface_form = surface_form.replace(""[[obj]]"", obj)\n                    subj_start = surface_form.index(""[[subj]]"")\n                    surface_form = surface_form.replace(""[[subj]]"", subj)\n\n                tokens = nlp(surface_form)\n                subj = lemmatized(tokens, subj_start, subj_start + len(subj))\n                obj = lemmatized(tokens, obj_start, obj_start + len(obj))\n\n                knowledge_store.add_assertion(surface_form, [subj], [obj], resource=\'conceptnet\')\n                counter += 1\n            except Exception as e:\n                logger.error(\'Error processing line: \' + l)\n                raise e\n\n\nif __name__ == \'__main__\':\n    import json\n    import logging\n    import os\n    import sys\n\n    logger = logging.getLogger(os.path.basename(sys.argv[0]))\n    logging.basicConfig(level=logging.INFO)\n\n    store = KnowledgeStore(sys.argv[2], True)\n    extract_assertions(sys.argv[1], store)\n    store.save()\n'"
projects/knowledge_integration/scripts/extract_side_information_for_dataset.py,7,"b'""""""Extraction script for side information used by reader on dataset.""""""\n\nimport json\nimport math\n\nimport progressbar\nimport tensorflow as tf\n\nfrom jack import readers\nfrom jack.io.load import loaders\nfrom projects.knowledge_integration.qa.definition_model import DefinitionPorts\nfrom projects.knowledge_integration.shared import AssertionMRPorts\n\ntf.app.flags.DEFINE_string(\'dataset\', None, \'dataset file.\')\ntf.app.flags.DEFINE_string(\'output\', None, \'output json.\')\ntf.app.flags.DEFINE_string(\'loader\', \'squad\', \'loader type.\')\ntf.app.flags.DEFINE_string(\'knowledge_store\', None, \'assertion store.\')\ntf.app.flags.DEFINE_string(\'load_dir\', None, \'path to reader.\')\ntf.app.flags.DEFINE_integer(\'batch_size\', 64, \'batch size.\')\n\nFLAGS = tf.app.flags.FLAGS\ndataset = loaders[FLAGS.loader](FLAGS.dataset)\n\nif FLAGS.knowledge_store:\n    reader = readers.reader_from_file(FLAGS.reader, assertion_dir=FLAGS.knowledge_store)\nelse:\n    reader = readers.reader_from_file(FLAGS.reader)\n\ninput_module = reader.input_module\n\nnum_batches = int(math.ceil(len(dataset) / FLAGS.batch_size))\n\nid2sideinformation = {}\nbar = progressbar.ProgressBar(\n    max_value=num_batches,\n    widgets=[\' [\', progressbar.Timer(), \'] \', progressbar.Bar(), \' (\', progressbar.ETA(), \') \'])\n\nfor idx in bar(range(0, len(dataset), FLAGS.batch_size)):\n    instances = [q for q, _ in dataset[idx:idx + FLAGS.batch_size]]\n    processed = input_module(instances)\n    assertions = processed[AssertionMRPorts.assertions]\n    assertion_lengths = processed[AssertionMRPorts.assertion_lengths]\n    assertion2question = processed[AssertionMRPorts.assertion2question]\n    rev_vocab = processed[\'__rev_vocab\']\n    a_strings = [[] for _ in instances]\n\n    for i in range(assertions.shape[0]):\n        b_idx = assertion2question[i]\n        try:\n            a_strings[b_idx].append(\' \'.join(rev_vocab[idx] for idx in assertions[i, :assertion_lengths[i]]))\n        except Exception:\n            pass\n    for instance, a_s in zip(instances, a_strings):\n        id2sideinformation[instance.id] = {\'conceptnet\': a_s}\n    if DefinitionPorts.definitions in processed:\n        definitions = processed[DefinitionPorts.definitions]\n        definition_lengths = processed[DefinitionPorts.definition_lengths]\n        definition2question = processed[DefinitionPorts.definition2question]\n        d_strings = [[] for _ in instances]\n        for i in range(definitions.shape[0]):\n            b_idx = definition2question[i]\n            d_strings[b_idx].append(\' \'.join(rev_vocab[idx] for idx in definitions[i, :definition_lengths[i]]))\n        for instance, d_s in zip(instances, d_strings):\n            id2sideinformation[instance.id][\'wikipedia\'] = d_s\n\nwith open(FLAGS.output, \'w\') as f:\n    json.dump(id2sideinformation, f, sort_keys=True, indent=2, separators=(\',\', \': \'))\n'"
projects/knowledge_integration/scripts/extract_wikipedia_short_abstract.py,8,"b'""""""Extract Wikipedia abstracts and add them to an assertion store.""""""\n\nimport re\nfrom bz2 import BZ2File\nfrom collections import defaultdict\n\nimport spacy\n\nfrom projects.knowledge_integration.knowledge_store import KnowledgeStore\n\n\ndef uncamel(name):\n    return re.sub(\'([a-z0-9])([A-Z])\', r\'\\1 \\2\', name)\n\n\ndef write_assertions(abstracts, labels, store):\n    # nlp = spacy.load(\'en\', disable=[\'parser\', \'ner\', \'textcat\'])\n    nlp = spacy.load(\'en\', parser=False, entity=False)\n\n    lemma_labels = dict()\n    counter = 0\n\n    for article, assertion in abstracts.items():\n        if counter % 100000 == 0:\n            logger.info(\'%d assertions added\' % counter)\n            if counter > 0:\n                store._assertion_db[\'wikipedia_firstsent\'].sync()\n        subjects = set()\n        ll = labels.get(article)\n        if ll is None or not ll:\n            ll = [uncamel(article).replace(\'_\', \' \')]\n        for l in ll:\n            if l not in lemma_labels:\n                lemma_labels[l] = \' \'.join(t.lemma_ for t in nlp(l))\n            subjects.add(lemma_labels[l])\n        store.add_assertion(assertion, subjects, [], \'wikipedia_firstsent\')\n        counter += 1\n\n\nif __name__ == \'__main__\':\n    import logging\n    import os\n    import sys\n    import tensorflow as tf\n\n    logger = logging.getLogger(os.path.basename(sys.argv[0]))\n    logging.basicConfig(level=logging.INFO)\n\n    tf.app.flags.DEFINE_string(\'knowledge_store_path\', None, \'directory to assertion store\')\n    tf.app.flags.DEFINE_string(\'short_abstracts\', None, \'path to dbpedia short abstracts\')\n    tf.app.flags.DEFINE_string(\'anchor_texts\', None, \'path to dbpedia anchor texts\')\n    tf.app.flags.DEFINE_string(\'transitive_redirects\', None, \'path to dbpedia transitive redirects\')\n    tf.app.flags.DEFINE_integer(\'max_articles_per_anchor\', 3, \'maximum number of articles per anchor\')\n    tf.app.flags.DEFINE_integer(\'min_num_anchor_per_article\', 10,\n                                \'minimum number of mentions per anchor for an article\')\n    tf.app.flags.DEFINE_integer(\'max_abstract_tokens\', 50, \'maximum number of tokens to take from abstract\')\n\n    FLAGS = tf.app.flags.FLAGS\n\n\n    def myopen(fn):\n        return BZ2File(fn) if fn.endswith(\'.bz2\') else open(fn)\n\n\n    prefix = ""http://dbpedia.org/resource/""\n    prefix_len = len(prefix)\n    logger.info(\'Loading DBpedia abstracts...\')\n\n\n    def process_abstract(l):\n        l = l.decode(\'utf-8\')\n        if l.startswith(\'#\'):\n            return None, None\n        split = l.split(\'> \')\n        article = split[0][prefix_len + 1:]\n        if article.startswith(\'List_of\') or \'(disambiguation)\' in article or \'__\' in article:\n            return None, None\n        abstract = split[2]\n        abstract = abstract[1:abstract.find(\'""@en\')]\n        abstract = \' \'.join(abstract.split()[:FLAGS.max_abstract_tokens])\n        return article, abstract\n\n\n    abstracts = dict()\n    with myopen(FLAGS.short_abstracts) as f:\n        ct = 0\n        for article, abstract in map(process_abstract, f):\n            if article is not None:\n                abstracts[article] = abstract\n            ct += 1\n            if ct % 1000000 == 0:\n                logger.info(\'%d lines processed...\' % ct)\n\n\n    def process_line(l):\n        l = l.decode(\'utf-8\')\n        if l.startswith(\'#\'):\n            return None, None\n        split = l.split(\'> \')\n        subj = split[0][prefix_len + 1:]\n        if subj.startswith(\'List_of\') or \'(disambiguation)\' in subj or \'__\' in subj:\n            return None, None\n        obj = split[2]\n        if obj.startswith(\'""\'):\n            obj = obj[1:obj.find(\'""@en\')]\n        else:\n            obj = obj[prefix_len + 1:]\n        return subj, obj\n\n\n    logger.info(\'Loading DBpedia redirects...\')\n    transitive_redirects = dict()\n    with myopen(FLAGS.transitive_redirects) as f:\n        ct = 0\n        for subj, obj in map(process_line, f):\n            if subj is not None and not (obj.startswith(\'List_of\') or \'(disambiguation)\' in obj or \'__\' in obj):\n                transitive_redirects[subj] = obj\n            ct += 1\n            if ct % 1000000 == 0:\n                logger.info(\'%d lines processed...\' % ct)\n\n    logger.info(\'Loading DBpedia anchor texts for articles...\')\n    anchor2articles = defaultdict(lambda: defaultdict(int))\n    with myopen(FLAGS.anchor_texts) as f:\n        ct = 0\n        for article, anchor_text in map(process_line, f):\n            if article is not None:\n                anchor2articles[anchor_text.lower()][transitive_redirects.get(article, article)] += 1\n            ct += 1\n            if ct % 10000000 == 0:\n                logger.info(\'%d lines processed...\' % ct)\n\n    logger.info(\'Selecting Top-%d articles for anchors that were at least %d times linked...\' %\n                (FLAGS.max_articles_per_anchor, FLAGS.min_num_anchor_per_article))\n    labels = defaultdict(set)\n    for anchor, articles in anchor2articles.items():\n        articles = sorted([(a, ct) for a, ct in articles.items() if ct > FLAGS.min_num_anchor_per_article],\n                          key=lambda x: -x[1])[:FLAGS.max_articles_per_anchor]\n        for a, _ in articles:\n            labels[a].add(anchor)\n    del anchor2articles\n\n    logger.info(\'Extending DBpedia labels with redirects...\')\n    for k, v in transitive_redirects.items():\n        labels[v].add(uncamel(k).replace(\'_\', \' \').lower())\n    del transitive_redirects\n\n    logger.info(\'Writing shortened wikipedia abstracts for %d entities...\' % len(abstracts))\n    store = KnowledgeStore(FLAGS.knowledge_store_path, writeback=True)\n    write_assertions(abstracts, labels, store)\n    del abstracts\n    del labels\n    store.save()\n'"
tests/jack/debug/test_debug.py,0,b'# -*- coding: utf-8 -*-\n\n'
tests/jack/eval/test_kbp_eval.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom jack.eval.link_prediction import compute_ranks\n\ntriple_to_score_map = {\n    ('a', 'p', 'a'): 1,\n    ('a', 'p', 'b'): 2,\n    ('a', 'p', 'c'): 3,\n    ('a', 'p', 'd'): 4\n}\n\ntriples = sorted(triple for triple, _ in triple_to_score_map.items())\nentity_set = {s for (s, _, _) in triples} | {o for (_, _, o) in triples}\n\n\ndef scoring_function(triples):\n    return [triple_to_score_map.get(triple, 0) for triple in triples]\n\n\ndef test_kbp_eval():\n    ranks, f_ranks = compute_ranks(scoring_function=scoring_function, triples=triples, entity_set=entity_set)\n\n    ranks_l, ranks_r = ranks\n    f_ranks_l, f_ranks_r = f_ranks\n\n    assert ranks_l == [1, 1, 1, 1]\n    assert ranks_r == [4, 3, 2, 1]\n\n    assert f_ranks_l == ranks_l\n    assert f_ranks_r == ranks_r\n"""
tests/jack/load/test_loaders.py,0,"b'# -*- coding: utf-8 -*-\n\nimport subprocess\n\nimport pytest\n\nfrom jack.io import SNLI2jtr\n\n\ndef pytest_collection_modifyitems(items):\n    for item in items:\n        if ""interface"" in item.nodeid:\n            item.add_marker(pytest.mark.interface)\n        elif ""event"" in item.nodeid:\n            item.add_marker(pytest.mark.event)\n\n\ndef get_pipeline_script_cmdcall_snli_converter():\n    """"""\n    Creates a bash cmd to io the SNLI data into our format\n    :return:\n    """"""\n    # io snli files into jack format\n    cmd = ""python3 jack/io/SNLI2jtr.py""\n    return cmd\n\n\ndef check_file_adheres_to_schema(data_file_name):\n    """"""\n    Checks if a given data file adheres to the schema\n    """"""\n    schema_file_name = ""jack/load/dataset_schema.json""\n    # validate schema adherence\n    cmd = ""python3 jack/format/validate.py "" + data_file_name + "" "" + schema_file_name\n\n    # Execute command and wait for results\n    try:\n        pass\n        #subprocess.check_output(cmd, stderr=subprocess.STDOUT, shell=True)\n        #response = subprocess.Popen(cmd, stdout=subprocess.PIPE).stdout.read()\n        #assert response == ""JSON successfully validated.""\n    except subprocess.CalledProcessError as e:\n        assert False, str(e.output)\n\n\ndef loaders_test(dataset_name):\n    """"""\n    Tests a dataset converter, checking whether the converted data adheres\n    to the Jack-the-Reader (.jack) format.\n    Args:\n        dataset_name (string): Dataset name as defined in\n                DATASET_TO_CMD_CALL_STRING.\n    Returns: None\n    """"""\n    # Setup command - get the command line call for a given dataset\n    DATASET_TO_CMD_CALL_STRING = dict()\n    DATASET_TO_CMD_CALL_STRING[\'SNLI\'] = get_pipeline_script_cmdcall_snli_converter()\n    cmd = DATASET_TO_CMD_CALL_STRING[dataset_name]\n\n    # Execute conversion command and wait for results\n\n    try:\n        subprocess.check_output(cmd, stderr=subprocess.STDOUT, shell=True)\n    except subprocess.CalledProcessError as e:\n        assert False, str(e.output)\n\n    data_file_name = ""jack/tests/test_data/SNLI/2000_samples_train_jtr_v1.json""\n    check_file_adheres_to_schema(data_file_name)\n\n\n#-------------------------------\n#       DATA LOADER TESTS\n#-------------------------------\n@pytest.mark.data_loaders\ndef test_test():\n    assert 2 == 2\n\n\n#@pytest.mark.data_loaders\n#def test_snli_converter():\n#    loaders_test(dataset_name=\'SNLI\')\n\n@pytest.mark.data_loaders\ndef test_snli_converter():\n    path = \'tests/test_data/SNLI/1000_samples_snli_1.0_train.jsonl\'\n    res = SNLI2jtr.convert_snli(snli_file_jsonl=path)\n    assert len(res) == 3\n    assert set(res.keys()) == {\'meta\', \'instances\', \'globals\'}\n    assert res[\'meta\'] == \'SNLI\'\n    assert res[\'globals\'] == {\'candidates\': [{\'text\': \'entailment\'}, {\'text\': \'neutral\'}, {\'text\': \'contradiction\'}]}\n    assert isinstance(res[\'instances\'], list)\n    assert res[\'instances\'][0] == {\'questions\': [{\'question\': \'A person is training his horse for a competition.\', \'answers\': [{\'text\': \'neutral\'}]}], \'id\': \'3416050480.jpg#4r1n\', \'support\': [{\'text\': \'A person on a horse jumps over a broken down airplane.\', \'id\': \'3416050480.jpg#4\'}]}\n    assert res[\'instances\'][1] == {\'questions\': [{\'question\': \'A person is at a diner, ordering an omelette.\', \'answers\': [{\'text\': \'contradiction\'}]}], \'support\': [{\'id\': \'3416050480.jpg#4\', \'text\': \'A person on a horse jumps over a broken down airplane.\'}], \'id\': \'3416050480.jpg#4r1c\'}\n    assert res[\'instances\'][2] == {\'support\': [{\'text\': \'A person on a horse jumps over a broken down airplane.\', \'id\': \'3416050480.jpg#4\'}], \'questions\': [{\'answers\': [{\'text\': \'entailment\'}], \'question\': \'A person is outdoors, on a horse.\'}], \'id\': \'3416050480.jpg#4r1e\'}\n\n\n@pytest.mark.data_loaders\ndef test_snli_schema():\n    data_file_name = ""jack/tests/test_data/SNLI/2000_samples_train_jtr_v1.json""\n    check_file_adheres_to_schema(data_file_name)'"
tests/jack/preprocess/test_batch.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom jack.util import batch\n\n\ndef test_get_buckets():\n    data = {\n        'data0': [i * [i] for i in range(1, 10)],\n        'data1': [i * [i] for i in range(3, 12)]\n    }\n\n    buckets2ids, ids2buckets = batch.get_buckets(data=data,\n                                                 order=('data0', 'data1'),\n                                                 structure=(2, 2))\n\n    assert buckets2ids == {\n        '(1, 0)': [5, 6],\n        '(1, 1)': [7, 8],\n        '(0, 0)': [0, 1, 2],\n        '(0, 1)': [3, 4]\n    }\n    assert ids2buckets == {\n        0: '(0, 0)',\n        1: '(0, 0)',\n        2: '(0, 0)',\n        3: '(0, 1)',\n        4: '(0, 1)',\n        5: '(1, 0)',\n        6: '(1, 0)',\n        7: '(1, 1)',\n        8: '(1, 1)'\n    }\n\n\ndef test_get_batches():\n    data = {\n        'data0': [[i] * 2 for i in range(10)],\n        'data1': [[i] * 3 for i in range(10)]\n    }\n\n    batch_generator = batch.get_batches(data, batch_size=3, exact_epoch=True)\n    batches = list(batch_generator)\n\n    assert batches[0]['data0'].shape == batches[1]['data0'].shape == batches[2]['data0'].shape == (3, 2)\n    assert batches[0]['data1'].shape == batches[1]['data1'].shape == batches[2]['data1'].shape == (3, 3)\n\n    assert batches[3]['data0'].shape == (1, 2)\n    assert batches[3]['data1'].shape == (1, 3)\n\n    assert len(batches) == 4\n\n    batch_generator = batch.get_batches(data, batch_size=3, exact_epoch=False)\n    batches = list(batch_generator)\n\n    assert len(batches) == 3\n"""
tests/jack/preprocess/test_map.py,0,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom jack.util import map\nfrom jack.util import preprocessing\n\ntext = \'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et \' \\\n       \'dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex \' \\\n       \'ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat \' \\\n       \'nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit \' \\\n       \'anim id est laborum.\'\n\ntokenized_text = [\'Lorem\', \'ipsum\', \'dolor\', \'sit\', \'amet\', \',\', \'consectetur\', \'adipiscing\', \'elit\', \',\', \'sed\',\n                  \'do\', \'eiusmod\', \'tempor\', \'incididunt\', \'ut\', \'labore\', \'et\', \'dolore\', \'magna\', \'aliqua\', \'.\',\n                  \'Ut\', \'enim\', \'ad\', \'minim\', \'veniam\', \',\', \'quis\', \'nostrud\', \'exercitation\', \'ullamco\',\n                  \'laboris\', \'nisi\', \'ut\', \'aliquip\', \'ex\', \'ea\', \'commodo\', \'consequat\', \'.\', \'Duis\', \'aute\',\n                  \'irure\', \'dolor\', \'in\', \'reprehenderit\', \'in\', \'voluptate\', \'velit\', \'esse\', \'cillum\', \'dolore\',\n                  \'eu\', \'fugiat\', \'nulla\', \'pariatur\', \'.\', \'Excepteur\', \'sint\', \'occaecat\', \'cupidatat\', \'non\',\n                  \'proident\', \',\', \'sunt\', \'in\', \'culpa\', \'qui\', \'officia\', \'deserunt\', \'mollit\', \'anim\', \'id\',\n                  \'est\', \'laborum\', \'.\']\n\n\ndef test_tokenize():\n    assert preprocessing.tokenize(text) == tokenized_text\n    question_text = ""where is the cat?""\n    desired_tokenised_question = [""where"",""is"",""the"",""cat"",""?""]\n    assert preprocessing.tokenize(question_text) == desired_tokenised_question\n\n\ndef test_get_list_shape():\n    data = [[1, 2, 3], [4, 5]]\n    assert map.get_list_shape(data) == [2, 3]\n\n    data = [[[1, 2, 3]], [[4, 5], [6, 7]]]\n    assert map.get_list_shape(data) == [2, 2, 3]\n\n\ndef test_numpify():\n    def _fillna(xs):\n        data = np.array(xs)\n        lens = np.array([len(i) for i in data])\n        mask = np.arange(lens.max()) < lens[:, None]\n        out = np.zeros(mask.shape, dtype=data.dtype)\n        out[mask] = np.concatenate(data)\n        return out\n\n    data = [[1, 2, 3], [4, 5], [6, 7, 8]]\n    data_np = map.numpify(data)\n\n    for a, b in zip([np.array(x) for x in data], data_np):\n        assert (a == b).all()\n\n    data = {0: [[1, 2, 3]], 1: [[4, 5], [6, 7, 8]], 2: [[6, 7, 8]]}\n    data_np = map.numpify(data)\n\n    for ak, bk in zip(data.keys(), data_np.keys()):\n        a, b = data[ak], data_np[bk]\n        assert (_fillna(a) == b).all()\n'"
tests/jack/preprocess/test_vocab_prune.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom pprint import pprint\n\nfrom jack.core import QASetting\nfrom jack.util import preprocessing\n\n\ndef test_vocab():\n    train_data = [\n        QASetting(question='A person is training his horse for a competition.',\n                  support=['A person on a horse jumps over a broken down airplane.'],\n                  candidates=['entailment', 'neutral', 'contradiction'])\n    ]\n\n    print('build vocab based on train data')\n    train_vocab = preprocessing.fill_vocab(train_data)\n    train_vocab.freeze()\n    pprint(train_vocab._sym2freqs)\n    pprint(train_vocab._sym2id)\n\n    MIN_VOCAB_FREQ, MAX_VOCAB_CNT = 2, 10\n    train_vocab = train_vocab.prune(MIN_VOCAB_FREQ, MAX_VOCAB_CNT)\n\n    pprint(train_vocab._sym2freqs)\n    pprint(train_vocab._sym2id)\n\n    print('encode train data')\n    train_data = preprocessing.nlp_preprocess(train_data[0].question, train_vocab)[0]\n    print(train_data)\n"""
tests/jack/readers/test_fastqa.py,1,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport tensorflow as tf\n\nimport jack.readers as readers\nfrom jack.core import SharedResources\nfrom jack.io.embeddings.embeddings import Embeddings\nfrom jack.io.load import load_jack\nfrom jack.readers.extractive_qa.util import tokenize\nfrom jack.util.vocab import Vocab\n\n\ndef test_fastqa():\n    tf.reset_default_graph()\n\n    data = load_jack(\'tests/test_data/squad/snippet_jtr.json\')\n    questions = []\n    # fast qa must be initialized with existing embeddings, so we create some\n    vocab = dict()\n    for question, _ in data:\n        questions.append(question)\n        for t in tokenize(question.question):\n            if t not in vocab:\n                vocab[t] = len(vocab)\n    embeddings = Embeddings(vocab, np.random.random([len(vocab), 10]))\n\n    # we need a vocabulary (with embeddings for our fastqa_reader, but this is not always necessary)\n    vocab = Vocab(vocab=vocab)\n\n    # ... and a config\n    config = {""batch_size"": 1, ""repr_dim"": 10, ""with_char_embeddings"": True}\n\n    # create/setup reader\n    shared_resources = SharedResources(vocab, config, embeddings)\n    fastqa_reader = readers.fastqa_reader(shared_resources)\n    fastqa_reader.setup_from_data(data)\n\n    answers = fastqa_reader(questions)\n\n    assert answers, ""FastQA reader should produce answers""\n'"
tests/jack/readers/test_fastqa_loop.py,3,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nfrom jack.core import SharedResources\nfrom jack.core.tensorflow import TFReader\nfrom jack.core.tensorport import Ports\nfrom jack.io.embeddings.embeddings import load_embeddings\nfrom jack.io.load import load_jack\nfrom jack.readers.extractive_qa.shared import XQAInputModule, XQAOutputModule\nfrom jack.readers.extractive_qa.tensorflow.fastqa import FastQAModule\nfrom jack.util.vocab import Vocab\n\n\ndef test_fastqa():\n    tf.reset_default_graph()\n\n    data = load_jack(\'tests/test_data/squad/snippet_jtr.json\')\n\n    # fast qa must be initialized with existing embeddings, so we create some\n    embeddings = load_embeddings(\'./tests/test_data/glove.840B.300d_top256.txt\', \'glove\')\n\n    # we need a vocabulary (with embeddings for our fastqa_reader, but this is not always necessary)\n    vocab = Vocab(vocab=embeddings.vocabulary)\n\n    # ... and a config\n    config = {\n        ""batch_size"": 1,\n        ""repr_dim"": 10,\n        ""with_char_embeddings"": True\n    }\n\n    # create/setup reader\n    shared_resources = SharedResources(vocab, config, embeddings)\n\n    input_module = XQAInputModule(shared_resources)\n    model_module = FastQAModule(shared_resources)\n    output_module = XQAOutputModule()\n\n    reader = TFReader(shared_resources, input_module, model_module, output_module)\n    reader.setup_from_data(data, is_training=True)\n\n    loss = reader.model_module.tensors[Ports.loss]\n    optimizer = tf.train.AdagradOptimizer(learning_rate=0.01)\n    min_op = optimizer.minimize(loss)\n\n    session = model_module.tf_session\n    session.run(tf.global_variables_initializer())\n\n    for epoch in range(0, 10):\n        for batch in reader.input_module.batch_generator(data, 1, False):\n            feed_dict = reader.model_module.convert_to_feed_dict(batch)\n            loss_value, _ = session.run((loss, min_op), feed_dict=feed_dict)\n            print(loss_value)\n'"
tests/jack/readers/test_kbp.py,1,"b""# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nimport jack.readers as readers\nfrom jack.io.load import loaders\n\n\ndef test_kbp():\n    data = loaders['jack']('tests/test_data/WN18/wn18-snippet.jack.json')\n    questions = [question for question, _ in data]\n\n    for model_name in ['transe', 'distmult', 'complex']:\n\n        with tf.variable_scope(model_name):\n            config = {\n                'batch_size': 1,\n                'repr_dim': 10\n            }\n\n            reader = readers.readers['{}_reader'.format(model_name)](config)\n            reader.setup_from_data(data)\n\n            answers = reader(questions)\n\n            assert len(answers) == 5000\n\n            assert answers, 'KBP reader should produce answers'\n"""
tests/jack/readers/test_models.py,0,"b'# -*- coding: utf-8 -*-\n\nimport os\nimport subprocess\nimport time\nfrom os.path import join, exists\n\nimport numpy as np\nimport pytest\n\nOVERFIT_PATH = \'./tests/test_results/overfit_test/\'\nSMALLDATA_PATH = \'./tests/test_results/smalldata_test/\'\n\n# if you add a model here, you need the data in the format of:\n# test_data/dataset-name/train.json\n# test_data/dataset-name/dev.json\n# test_data/dataset-name/test.json\n# test_data/dataset-name/overfit.json\n\nmodels2dataset = {}\nmodels2dataset[\'esim\'] = \'SNLI\'\nmodels2dataset[\'dam\'] = \'SNLI\'\nmodels2dataset[\'fastqa\'] = \'squad\'\n# models2dataset[\'jackqa\'] = \'squad\'\n\nmodels2config = {}\nmodels2config[\'esim\'] = \'./conf/nli/esim.yaml\'\nmodels2config[\'dam\'] = \'./conf/nli/dam.yaml\'\nmodels2config[\'fastqa\'] = \'./conf/qa/fastqa.yaml\'\nmodels2config[\'jackqa\'] = \'./conf/qa/jackqa.yaml\'\n\noverfit_epochs = {\'SNLI\': 15, \'squad\': 15}\nsmall_data_epochs = {\'SNLI\': 5, \'squad\': 10}\n\nmodelspecifics = {\n    \'fastqa\': lambda is_small_data: (\' embedding_file=tests/test_data/glove.500.50d.txt\' +\n                                     \' embedding_format=glove\' +\n                                     \' vocab_from_embeddings=True\'),\n    \'jackqa\': lambda is_small_data: (\' embedding_file=tests/test_data/glove.500.50d.txt\' +\n                                     \' embedding_format=glove\' +\n                                     \' vocab_from_embeddings=True\'),\n    \'dam\': lambda is_small_data: (\' embedding_file=tests/test_data/glove.500.50d.txt\' +\n                                  \' embedding_format=glove\' +\n                                  \' vocab_from_embeddings=True\'),\n    \'esim\': lambda is_small_data: (\' embedding_file=tests/test_data/glove.500.50d.txt\' +\n                                   \' embedding_format=glove\' +\n                                   \' vocab_from_embeddings=True\'),\n}\n\nids = []\ntestdata = []\n\n\ndef generate_test_data():\n    \'\'\'Creates all permutations of models and datasets as tests.\'\'\'\n    for reader, dataset in models2dataset.items():\n        for use_small_data in [False, True]:\n            epochs = small_data_epochs[dataset] if use_small_data else overfit_epochs[dataset]\n            testdata.append([reader, epochs, use_small_data, dataset])\n\n\ndef get_string_for_test(reader, epochs, use_small_data, dataset):\n    \'\'\'Creates a name for each test, so the output of PyTest is readable\'\'\'\n    return (\'reader={0}, \'\n            \'epochs={1}, run_type={2}, \'\n            \'dataset={3}\').format(reader,\n                                  epochs,\n                                  (\'smalldata\' if use_small_data else \'overfit\'), dataset)\n\n\ndef generate_names():\n    \'\'\'Generates all names for all test cases\'\'\'\n    for args in testdata:\n        ids.append(get_string_for_test(*args))\n\n\ngenerate_test_data()\ngenerate_names()\n\n\n@pytest.mark.parametrize(""reader, epochs, use_small_data, dataset"", testdata, ids=ids)\ndef test_model(reader, epochs, use_small_data, dataset):\n    \'\'\'Tests a model via training_pipeline.py by comparing with expected_result.txt\n    Args:\n        reader (string): The reader\n        epochs (int=5): Some models need more time to overfit data; increase\n               this the case of overfitting problems.\n        use_small_data (bool=False): Switches between \'overfit\' and \'smalldata\'\n                       mode.\n        dataset (string): Dataset name as defined in\n                DATASET_TO_CMD_CALL_STRING. This value is also used as the name\n                to the test_result folder.\n    Returns: None\n    \'\'\'\n    # Setup paths and filenames for the expected_results file\n    test_result_path = join(SMALLDATA_PATH if use_small_data else OVERFIT_PATH, dataset, reader)\n    metric_filepath = join(test_result_path, datetime_test_result_filename())\n\n    # create dir if it does not exists\n    if not exists(test_result_path):\n        os.makedirs(test_result_path)\n\n    # Stich together test data paths\n    if not use_small_data:\n        train_file = \'tests/test_data/{0}/overfit.json\'.format(dataset)\n        dev_file = train_file\n        test_file = None\n    else:\n        train_file = \'tests/test_data/{0}/train.json\'.format(dataset)\n        dev_file = \'tests/test_data/{0}/dev.json\'.format(dataset)\n        test_file = None\n\n    # Setup the process call command\n    cmd = \'CUDA_VISIBLE_DEVICES=-1 \'  # we only test on the CPU\n    cmd += \'python3 ./bin/jack-train.py with\'\n    cmd += \' config={0}\'.format(models2config[reader])\n    cmd += \' train={0} dev={1} test={2}\'.format(train_file, dev_file, test_file, )\n    cmd += \' write_metrics_to={0}\'.format(metric_filepath)\n    cmd += \' epochs={0}\'.format(epochs)\n    cmd += \' learning_rate_decay=1.0\'\n    cmd += \' learning_rate=0.01\'\n    cmd += \' repr_dim=32\'\n    cmd += \' save_dir=None\'\n    cmd += \' validation_interval=-1\'\n    if reader in modelspecifics:\n        # this is a function which takes use_small_data as argument\n        cmd += modelspecifics[reader](use_small_data)\n    print(\'command: \' + cmd)\n    # Execute command and wait for results\n    t0 = time.time()\n    try:\n        subprocess.check_output(cmd, stderr=subprocess.STDOUT, shell=True)\n    except subprocess.CalledProcessError as e:\n        for line in e.output.split(b\'\\n\'):\n            print(line)\n        assert False, str(e.output)\n\n    # Load and parse the results and the expected rults for testing\n    new_results, runtime = load_and_parse_test_results(metric_filepath)\n    expected_results, expected_runtime = load_and_parse_test_results(join(test_result_path,\n                                                                          \'expected_results.txt\'))\n\n    # Match expected results with current results; the order is important to\n    # assert np.testing.assert_array_almost_equal(results[:,1], atol=0.01)\n\n    for new, base in zip(new_results, expected_results):\n        assert new[0] == base[0], ""Different order of metrics!""\n        assert np.allclose([new[1]], [base[1]], atol=0.1), \\\n            ""Metric value different from expected results!""\n\n\ndef load_and_parse_test_results(filepath):\n    \'\'\'This method loads and parses a metric file writen by EvalHook.\'\'\'\n    name_value_metric_pair = []\n    runtime = 0\n    with open(filepath) as f:\n        data = f.readlines()\n        for i, line in enumerate(data):\n            _date, _time, metric_name, metric_value = line.strip().split(\' \')\n            name_value_metric_pair.append([metric_name,\n                                           np.float32(metric_value)])\n    return name_value_metric_pair, runtime\n\n\ndef datetime_test_result_filename():\n    \'\'\'Generates a string of the format testresult_CURRENT_DATE-TIME\'\'\'\n    timestr = time.strftime(""%Y%m%d-%H%M%S"")\n    return \'testresult_\' + timestr\n'"
tests/jack/readers/test_readers.py,3,"b'# -*- coding: utf-8 -*-\n\n""""""Smoke test: train all readers for one iteration & run inference.""""""\n\nfrom functools import partial\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom jack import readers\nfrom jack.core.data_structures import QASetting, Answer\nfrom jack.core.shared_resources import SharedResources\nfrom jack.core.tensorflow import TFReader\nfrom jack.io.embeddings import Embeddings\nfrom jack.readers.extractive_qa.util import tokenize\nfrom jack.util.vocab import Vocab\n\n\ndef teardown_function(_):\n    tf.reset_default_graph()\n\n\ndef build_vocab(questions):\n    """"""Since some readers require an initialized vocabulary, initialize it here.""""""\n\n    vocab = dict()\n    for question in questions:\n        for t in tokenize(question.question):\n            if t not in vocab:\n                vocab[t] = len(vocab)\n    embeddings = Embeddings(vocab, np.random.random([len(vocab), 10]))\n\n    vocab = Vocab(vocab=embeddings.vocabulary)\n    return vocab, embeddings\n\n\ndef smoke_test(reader_name):\n    """"""Instantiate the reader, train for one epoch, and run inference.""""""\n\n    data_set = [\n        (QASetting(\n            question=""Which is it?"",\n            support=[""While b seems plausible, answer a is correct.""],\n            id=""1"",\n            candidates=[""a"", ""b"", ""c""]),\n         [Answer(""a"", (6, 6))])\n    ]\n    questions = [q for q, _ in data_set]\n    v, e = build_vocab(questions)\n    shared_resources = SharedResources(v, {""repr_dim"": 10, ""dropout"": 0.5}, e)\n    tf.reset_default_graph()\n    reader = readers.readers[reader_name](shared_resources)\n    if isinstance(reader, TFReader):\n        reader.train(tf.train.AdamOptimizer(), data_set, batch_size=1, max_epochs=1)\n    else:\n        import torch\n        reader.setup_from_data(data_set, is_training=True)\n        params = list(reader.model_module.prediction_module.parameters())\n        params.extend(reader.model_module.loss_module.parameters())\n        optimizer = torch.optim.Adam(params, lr=0.01)\n        reader.train(optimizer, data_set, batch_size=1, max_epochs=1)\n\n    answers = reader(questions)\n\n    assert answers, ""{} should produce answers"".format(reader_name)\n\n\nBLACKLIST = [\'fastqa_reader_torch\', \'modular_qa_reader\', \'modular_nli_reader\']\nREADERS = [r for r in readers.readers.keys()\n           if r not in BLACKLIST]\n\n# Dynamically generate one test for each reader\ncurrent_module = __import__(__name__)\n\nfor reader_name in READERS:\n    setattr(current_module, ""test_{}"".format(reader_name), partial(smoke_test, reader_name))\n'"
tests/jack/readers/test_serialization.py,1,"b'# -*- coding: utf-8 -*-\n\nimport tempfile\n\nimport tensorflow as tf\n\nfrom jack.io.embeddings import load_embeddings\nfrom jack.io.load import loaders\nfrom jack.readers.implementations import *\nfrom jack.util.vocab import Vocab\n\n\ndef test_serialization():\n    all_readers = [\n        fastqa_reader,\n        modular_qa_reader,\n        # fastqa_reader_torch,\n        dam_snli_reader,\n        cbilstm_nli_reader,\n        modular_nli_reader,\n        distmult_reader,\n        complex_reader,\n        transe_reader,\n    ]\n\n    for reader in all_readers:\n        vocab, config = {}, {}\n\n        data = None\n        if reader in {distmult_reader, complex_reader, transe_reader}:\n            data = loaders[\'jack\'](\'tests/test_data/WN18/wn18-snippet.jack.json\')\n            config[\'repr_dim\'] = 50\n        elif reader in {cbilstm_nli_reader, dam_snli_reader}:\n            data = loaders[\'snli\'](\'tests/test_data/SNLI/1000_samples_snli_1.0_train.jsonl\')\n\n            embeddings = load_embeddings(""data/GloVe/glove.the.50d.txt"", \'glove\')\n            vocab = Vocab(vocab=embeddings.vocabulary)\n            config[\'repr_dim\'] = 50\n        elif reader in {fastqa_reader}:\n            data = loaders[\'squad\'](\'data/SQuAD/snippet.json\')\n\n            embeddings = load_embeddings(""data/GloVe/glove.the.50d.txt"", \'glove\')\n            vocab = Vocab(vocab=embeddings.vocabulary)\n            config[\'repr_dim\'] = 50\n\n        if data is not None:\n            tf.reset_default_graph()\n\n            shared_resources = SharedResources(vocab, config, embeddings)\n            reader_instance = reader(shared_resources)\n            reader_instance.setup_from_data(data)\n\n            temp_dir_path = tempfile.mkdtemp()\n            reader_instance.store(temp_dir_path)\n\n            reader_instance.load(temp_dir_path)\n\n            assert reader_instance is not None\n'"
jack/readers/extractive_qa/tensorflow/__init__.py,0,b''
jack/readers/extractive_qa/tensorflow/abstract_model.py,1,"b""from typing import Sequence\n\nfrom jack.core import Ports, TensorPort, TensorPortTensors\nfrom jack.core.tensorflow import TFModelModule\nfrom jack.readers.extractive_qa.shared import XQAPorts\nfrom jack.util.tf.xqa import xqa_crossentropy_loss\n\n\nclass AbstractXQAModelModule(TFModelModule):\n    _input_ports = [XQAPorts.emb_question, XQAPorts.question_length,\n                    XQAPorts.emb_support, XQAPorts.support_length, XQAPorts.support2question,\n                    # char embedding inputs\n                    XQAPorts.word_chars, XQAPorts.word_char_length,\n                    XQAPorts.question_batch_words, XQAPorts.support_batch_words,\n                    # feature input\n                    XQAPorts.word_in_question,\n                    # optional input, provided only during training\n                    XQAPorts.correct_start, XQAPorts.answer2support_training,\n                    XQAPorts.is_eval]\n\n    _output_ports = [XQAPorts.start_scores, XQAPorts.end_scores,\n                     XQAPorts.answer_span]\n    _training_input_ports = [XQAPorts.start_scores, XQAPorts.end_scores,\n                             XQAPorts.answer_span_target, XQAPorts.answer2support_training, XQAPorts.support2question]\n    _training_output_ports = [Ports.loss]\n\n    @property\n    def output_ports(self) -> Sequence[TensorPort]:\n        return self._output_ports\n\n    @property\n    def input_ports(self) -> Sequence[TensorPort]:\n        return self._input_ports\n\n    @property\n    def training_input_ports(self) -> Sequence[TensorPort]:\n        return self._training_input_ports\n\n    @property\n    def training_output_ports(self) -> Sequence[TensorPort]:\n        return self._training_output_ports\n\n    def create_training_output(self, shared_resources, input_tensors):\n        tensors = TensorPortTensors(input_tensors)\n        return {\n            Ports.loss: xqa_crossentropy_loss(tensors.start_scores, tensors.end_scores,\n                                              tensors.answer_span_target, tensors.answer2support,\n                                              tensors.support2question,\n                                              use_sum=shared_resources.config.get('loss', 'sum') == 'sum')\n        }\n"""
jack/readers/extractive_qa/tensorflow/answer_layer.py,110,"b'import tensorflow as tf\n\nfrom jack.util.tf import attention, sequence_encoder, misc\nfrom jack.util.tf.segment import segment_top_k, segment_softmax\n\n\ndef answer_layer(encoded_question, question_length, encoded_support, support_length,\n                 support2question, answer2support, is_eval, correct_start=None, topk=1, max_span_size=10000,\n                 encoder=None, module=\'bilinear\', repr_dim=100, **kwargs):\n    if module == \'bilinear\':\n        return bilinear_answer_layer(\n            repr_dim, encoded_question, question_length, encoded_support, support_length,\n            support2question, answer2support, is_eval, topk, max_span_size)\n    elif module == \'mlp\':\n        return mlp_answer_layer(repr_dim, encoded_question, question_length, encoded_support, support_length,\n                                support2question, answer2support, is_eval, topk, max_span_size)\n    elif module == \'conditional\':\n        return conditional_answer_layer(\n            repr_dim, encoded_question, question_length, encoded_support, support_length,\n            correct_start, support2question, answer2support, is_eval, topk, max_span_size)\n    elif module == \'conditional_bilinear\':\n        return conditional_answer_layer(\n            repr_dim, encoded_question, question_length, encoded_support, support_length,\n            correct_start, support2question, answer2support, is_eval, topk, max_span_size, bilinear=True)\n    elif module == \'san\':\n        return san_answer_layer(\n            repr_dim, encoded_question, question_length, encoded_support, support_length,\n            support2question, answer2support, is_eval, topk, max_span_size, **kwargs)\n    elif module == \'bidaf\':\n        if \'repr_dim\' not in encoder:\n            encoder[\'repr_dim\'] = repr_dim\n        encoded_support_end = sequence_encoder.encoder(\n            encoded_support, support_length, name=\'encoded_support_end\', is_eval=is_eval, **encoder)\n        encoded_support_end = tf.concat([encoded_support, encoded_support_end], 2)\n        return bidaf_answer_layer(encoded_support, encoded_support_end, support_length,\n                                  support2question, answer2support, is_eval, topk=1, max_span_size=10000)\n    else:\n        raise ValueError(""Unknown answer layer type: %s"" % module)\n\n\ndef compute_question_state(encoded_question, question_length):\n    attention_scores = tf.layers.dense(encoded_question, 1, name=""question_attention"")\n    q_mask = misc.mask_for_lengths(question_length)\n    attention_scores = attention_scores + tf.expand_dims(q_mask, 2)\n    question_attention_weights = tf.nn.softmax(attention_scores, 1, name=""question_attention_weights"")\n    question_state = tf.reduce_sum(question_attention_weights * encoded_question, 1)\n    return question_state\n\n\ndef bilinear_answer_layer(size, encoded_question, question_length, encoded_support, support_length,\n                          support2question, answer2support, is_eval, topk=1, max_span_size=10000):\n    """"""Answer layer for multiple paragraph QA.""""""\n    # computing single time attention over question\n    size = encoded_support.get_shape()[-1].value\n    question_state = compute_question_state(encoded_question, question_length)\n\n    # compute logits\n    hidden = tf.gather(tf.layers.dense(question_state, 2 * size, name=""hidden""), support2question)\n    hidden_start, hidden_end = tf.split(hidden, 2, 1)\n\n    support_mask = misc.mask_for_lengths(support_length)\n\n    start_scores = tf.einsum(\'ik,ijk->ij\', hidden_start, encoded_support)\n    start_scores = start_scores + support_mask\n\n    end_scores = tf.einsum(\'ik,ijk->ij\', hidden_end, encoded_support)\n    end_scores = end_scores + support_mask\n\n    return compute_spans(start_scores, end_scores, answer2support, is_eval, support2question, topk, max_span_size)\n\n\ndef mlp_answer_layer(size, encoded_question, question_length, encoded_support, support_length,\n                     support2question, answer2support, is_eval, topk=1, max_span_size=10000):\n    """"""Answer layer for multiple paragraph QA.""""""\n    # computing single time attention over question\n    question_state = compute_question_state(encoded_question, question_length)\n\n    # compute logits\n    static_input = tf.concat([tf.gather(tf.expand_dims(question_state, 1), support2question) * encoded_support,\n                              encoded_support], 2)\n\n    hidden = tf.gather(tf.layers.dense(question_state, 2 * size, name=""hidden_1""), support2question)\n    hidden = tf.layers.dense(\n        static_input, 2 * size, use_bias=False, name=""hidden_2"") + tf.expand_dims(hidden, 1)\n\n    hidden_start, hidden_end = tf.split(tf.nn.relu(hidden), 2, 2)\n\n    support_mask = misc.mask_for_lengths(support_length)\n\n    start_scores = tf.layers.dense(hidden_start, 1, use_bias=False, name=""start_scores"")\n    start_scores = tf.squeeze(start_scores, [2])\n    start_scores = start_scores + support_mask\n\n    end_scores = tf.layers.dense(hidden_end, 1, use_bias=False, name=""end_scores"")\n    end_scores = tf.squeeze(end_scores, [2])\n    end_scores = end_scores + support_mask\n\n    return compute_spans(start_scores, end_scores, answer2support, is_eval, support2question, topk, max_span_size)\n\n\ndef compute_spans(start_scores, end_scores, answer2support, is_eval, support2question,\n                  topk=1, max_span_size=10000, correct_start=None):\n    max_support_length = tf.shape(start_scores)[1]\n    _, _, num_doc_per_question = tf.unique_with_counts(support2question)\n    offsets = tf.cumsum(num_doc_per_question, exclusive=True)\n    doc_idx_for_support = tf.range(tf.shape(support2question)[0]) - tf.gather(offsets, support2question)\n\n    def train():\n        gathered_end_scores = tf.gather(end_scores, answer2support)\n        gathered_start_scores = tf.gather(start_scores, answer2support)\n\n        if correct_start is not None:\n            # assuming we know the correct start we only consider ends after that\n            left_mask = misc.mask_for_lengths(tf.cast(correct_start, tf.int32), max_support_length, mask_right=False)\n            gathered_end_scores = gathered_end_scores + left_mask\n\n        predicted_start_pointer = tf.argmax(gathered_start_scores, axis=1, output_type=tf.int32)\n        predicted_end_pointer = tf.argmax(gathered_end_scores, axis=1, output_type=tf.int32)\n\n        return (start_scores, end_scores,\n                tf.gather(doc_idx_for_support, answer2support), predicted_start_pointer, predicted_end_pointer)\n\n    def eval():\n        # we collect spans for top k starts and top k ends and select the top k from those top 2k\n        doc_idx1, start_pointer1, end_pointer1, span_score1 = _get_top_k(\n            start_scores, end_scores, topk, max_span_size, support2question)\n        doc_idx2, end_pointer2, start_pointer2, span_score2 = _get_top_k(\n            end_scores, start_scores, topk, -max_span_size, support2question)\n\n        doc_idx = tf.concat([doc_idx1, doc_idx2], 1)\n        start_pointer = tf.concat([start_pointer1, start_pointer2], 1)\n        end_pointer = tf.concat([end_pointer1, end_pointer2], 1)\n        span_score = tf.concat([span_score1, span_score2], 1)\n\n        _, idx = tf.nn.top_k(span_score, topk)\n\n        r = tf.range(tf.shape(span_score)[0], dtype=tf.int32)\n        r = tf.reshape(tf.tile(tf.expand_dims(r, 1), [1, topk]), [-1, 1])\n\n        idx = tf.concat([r, tf.reshape(idx, [-1, 1])], 1)\n        doc_idx = tf.gather_nd(doc_idx, idx)\n        start_pointer = tf.gather_nd(start_pointer, idx)\n        end_pointer = tf.gather_nd(end_pointer, idx)\n\n        return start_scores, end_scores, tf.gather(doc_idx_for_support, doc_idx), start_pointer, end_pointer\n\n    return tf.cond(is_eval, eval, train)\n\n\ndef _get_top_k(scores1, scores2, k, max_span_size, support2question):\n    max_support_length = tf.shape(scores1)[1]\n    doc_idx, pointer1, topk_scores1 = segment_top_k(scores1, support2question, k)\n\n    # [num_questions * topk]\n    doc_idx_flat = tf.reshape(doc_idx, [-1])\n    pointer_flat1 = tf.reshape(pointer1, [-1])\n\n    # [num_questions * topk, support_length]\n    scores_gathered2 = tf.gather(scores2, doc_idx_flat)\n    if max_span_size < 0:\n        pointer_flat1, max_span_size = pointer_flat1 + max_span_size + 1, -max_span_size\n    left_mask = misc.mask_for_lengths(tf.cast(pointer_flat1, tf.int32),\n                                      max_support_length, mask_right=False)\n    right_mask = misc.mask_for_lengths(tf.cast(pointer_flat1 + max_span_size, tf.int32),\n                                       max_support_length)\n    scores_gathered2 = scores_gathered2 + left_mask + right_mask\n\n    pointer2 = tf.argmax(scores_gathered2, axis=1, output_type=tf.int32)\n\n    topk_score2 = tf.gather_nd(scores2, tf.stack([doc_idx_flat, pointer2], 1))\n\n    return doc_idx, pointer1, tf.reshape(pointer2, [-1, k]), topk_scores1 + tf.reshape(topk_score2, [-1, k])\n\n\ndef conditional_answer_layer(size, encoded_question, question_length, encoded_support, support_length,\n                             correct_start, support2question, answer2support, is_eval, topk=1, max_span_size=10000,\n                             bilinear=False):\n    question_state = compute_question_state(encoded_question, question_length)\n    question_state = tf.gather(question_state, support2question)\n\n    # Prediction\n    # start\n    if bilinear:\n        hidden_start = tf.layers.dense(question_state, size, name=""hidden_start"")\n        start_scores = tf.einsum(\'ik,ijk->ij\', hidden_start, encoded_support)\n    else:\n        static_input = tf.concat([tf.expand_dims(question_state, 1) * encoded_support, encoded_support], 2)\n        hidden_start = tf.layers.dense(question_state, size, name=""hidden_start_1"")\n        hidden_start = tf.layers.dense(\n            static_input, size, use_bias=False, name=""hidden_start_2"") + tf.expand_dims(hidden_start, 1)\n        start_scores = tf.layers.dense(tf.nn.relu(hidden_start), 1, use_bias=False, name=""start_scores"")\n        start_scores = tf.squeeze(start_scores, [2])\n\n    support_mask = misc.mask_for_lengths(support_length)\n    start_scores = start_scores + support_mask\n\n    max_support_length = tf.shape(start_scores)[1]\n    _, _, num_doc_per_question = tf.unique_with_counts(support2question)\n    offsets = tf.cumsum(num_doc_per_question, exclusive=True)\n    doc_idx_for_support = tf.range(tf.shape(support2question)[0]) - tf.gather(offsets, support2question)\n\n    doc_idx, start_pointer = tf.cond(\n        is_eval,\n        lambda: segment_top_k(start_scores, support2question, topk)[:2],\n        lambda: (tf.expand_dims(answer2support, 1), tf.expand_dims(correct_start, 1)))\n\n    doc_idx_flat = tf.reshape(doc_idx, [-1])\n    start_pointer = tf.reshape(start_pointer, [-1])\n\n    start_state = tf.gather_nd(encoded_support, tf.stack([doc_idx_flat, start_pointer], 1))\n    start_state.set_shape([None, size])\n\n    encoded_support_gathered = tf.gather(encoded_support, doc_idx_flat)\n    question_state = tf.gather(question_state, doc_idx_flat)\n    if bilinear:\n        hidden_end = tf.layers.dense(tf.concat([question_state, start_state], 1), size, name=""hidden_end"")\n        end_scores = tf.einsum(\'ik,ijk->ij\', hidden_end, encoded_support_gathered)\n    else:\n        end_input = tf.concat([tf.expand_dims(start_state, 1) * encoded_support_gathered,\n                               tf.gather(static_input, doc_idx_flat)], 2)\n\n        hidden_end = tf.layers.dense(tf.concat([question_state, start_state], 1), size,\n                                     name=""hidden_end_1"")\n        hidden_end = tf.layers.dense(\n            end_input, size, use_bias=False, name=""hidden_end_2"") + tf.expand_dims(hidden_end, 1)\n\n        end_scores = tf.layers.dense(tf.nn.relu(hidden_end), 1, use_bias=False, name=""end_scores"")\n        end_scores = tf.squeeze(end_scores, [2])\n\n    end_scores = end_scores + tf.gather(support_mask, doc_idx_flat)\n\n    def train():\n        predicted_end_pointer = tf.argmax(end_scores, axis=1, output_type=tf.int32)\n        return start_scores, end_scores, doc_idx, start_pointer, predicted_end_pointer\n\n    def eval():\n        # [num_questions * topk, support_length]\n        left_mask = misc.mask_for_lengths(tf.cast(start_pointer, tf.int32),\n                                          max_support_length, mask_right=False)\n        right_mask = misc.mask_for_lengths(tf.cast(start_pointer + max_span_size, tf.int32),\n                                           max_support_length)\n        masked_end_scores = end_scores + left_mask + right_mask\n        predicted_ends = tf.argmax(masked_end_scores, axis=1, output_type=tf.int32)\n\n        return (start_scores, masked_end_scores,\n                tf.gather(doc_idx_for_support, doc_idx_flat), start_pointer, predicted_ends)\n\n    return tf.cond(is_eval, eval, train)\n\n\ndef bidaf_answer_layer(encoded_support_start, encoded_support_end, support_length,\n                       support2question, answer2support, is_eval, topk=1, max_span_size=10000):\n    # BiLSTM(M) = M^2 = encoded_support_end\n    start_scores = tf.squeeze(tf.layers.dense(encoded_support_start, 1, use_bias=False), 2)\n    end_scores = tf.squeeze(tf.layers.dense(encoded_support_end, 1, use_bias=False), 2)\n    # mask out-of-bounds slots by adding -1000\n    support_mask = misc.mask_for_lengths(support_length)\n    start_scores = start_scores + support_mask\n    end_scores = end_scores + support_mask\n    return compute_spans(start_scores, end_scores, answer2support, is_eval,\n                         support2question, topk=topk, max_span_size=max_span_size)\n\n\ndef san_answer_layer(size, encoded_question, question_length, encoded_support, support_length,\n                     support2question, answer2support, is_eval, topk=1, max_span_size=10000,\n                     num_steps=5, dropout=0.4, **kwargs):\n    question_state = compute_question_state(encoded_question, question_length)\n    question_state = tf.layers.dense(question_state, encoded_support.get_shape()[-1].value, tf.tanh)\n    question_state = tf.gather(question_state, support2question)\n\n    cell = tf.contrib.rnn.GRUBlockCell(size)\n\n    all_start_scores = []\n    all_end_scores = []\n\n    support_mask = misc.mask_for_lengths(support_length)\n\n    for i in range(num_steps):\n        with tf.variable_scope(\'SAN\', reuse=i > 0):\n            question_state = tf.expand_dims(question_state, 1)\n            support_attn = attention.bilinear_attention(\n                question_state, encoded_support, support_length, False, False)[2]\n            question_state = tf.squeeze(question_state, 1)\n            support_attn = tf.squeeze(support_attn, 1)\n            question_state = cell(support_attn, question_state)[0]\n\n            hidden_start = tf.layers.dense(question_state, size, name=""hidden_start"")\n\n            start_scores = tf.einsum(\'ik,ijk->ij\', hidden_start, encoded_support)\n            start_scores = start_scores + support_mask\n\n            start_probs = segment_softmax(start_scores, support2question)\n            start_states = tf.einsum(\'ij,ijk->ik\', start_probs, encoded_support)\n            start_states = tf.unsorted_segment_sum(start_states, support2question, tf.shape(question_length)[0])\n            start_states = tf.gather(start_states, support2question)\n\n            hidden_end = tf.layers.dense(tf.concat([question_state, start_states], 1), size, name=""hidden_end"")\n\n            end_scores = tf.einsum(\'ik,ijk->ij\', hidden_end, encoded_support)\n            end_scores = end_scores + support_mask\n            all_start_scores.append(start_scores)\n            all_end_scores.append(end_scores)\n\n    all_start_scores = tf.stack(all_start_scores)\n    all_end_scores = tf.stack(all_end_scores)\n    dropout_mask = tf.nn.dropout(tf.ones([num_steps, 1, 1]), 1.0 - dropout)\n    all_start_scores = tf.cond(is_eval, lambda: all_start_scores * dropout_mask, lambda: all_start_scores)\n    all_end_scores = tf.cond(is_eval, lambda: all_end_scores * dropout_mask, lambda: all_end_scores)\n\n    start_scores = tf.reduce_mean(all_start_scores, axis=0)\n    end_scores = tf.reduce_mean(all_end_scores, axis=0)\n\n    return compute_spans(start_scores, end_scores, answer2support, is_eval, support2question, topk=topk,\n                         max_span_size=max_span_size)\n'"
jack/readers/extractive_qa/tensorflow/fastqa.py,31,"b'""""""\nThis file contains FastQA specific modules and ports\n""""""\n\nfrom jack.core import *\nfrom jack.readers.extractive_qa.tensorflow.abstract_model import AbstractXQAModelModule\nfrom jack.readers.extractive_qa.tensorflow.answer_layer import conditional_answer_layer, bilinear_answer_layer\nfrom jack.util.tf import misc\nfrom jack.util.tf.embedding import conv_char_embedding\nfrom jack.util.tf.highway import highway_network\nfrom jack.util.tf.sequence_encoder import encoder\n\n\nclass FastQAModule(AbstractXQAModelModule):\n    def set_topk(self, k):\n        self._topk_assign(k)\n\n    def create_output(self, shared_resources, input_tensors):\n        tensors = TensorPortTensors(input_tensors)\n        with tf.variable_scope(""fast_qa"", initializer=tf.contrib.layers.xavier_initializer()):\n            # Some helpers\n            batch_size = tf.shape(tensors.question_length)[0]\n            max_question_length = tf.reduce_max(tensors.question_length)\n            support_mask = misc.mask_for_lengths(tensors.support_length)\n\n            input_size = shared_resources.embeddings.shape[-1]\n            size = shared_resources.config[""repr_dim""]\n            with_char_embeddings = shared_resources.config.get(""with_char_embeddings"", False)\n\n            # set shapes for inputs\n            tensors.emb_question.set_shape([None, None, input_size])\n            tensors.emb_support.set_shape([None, None, input_size])\n\n            emb_question = tensors.emb_question\n            emb_support = tensors.emb_support\n            if with_char_embeddings:\n                # compute combined embeddings\n                [char_emb_question, char_emb_support] = conv_char_embedding(\n                    len(shared_resources.char_vocab), size, tensors.word_chars, tensors.word_char_length,\n                    [tensors.question_batch_words, tensors.support_batch_words])\n\n                emb_question = tf.concat([emb_question, char_emb_question], 2)\n                emb_support = tf.concat([emb_support, char_emb_support], 2)\n                input_size += size\n\n                # set shapes for inputs\n                emb_question.set_shape([None, None, input_size])\n                emb_support.set_shape([None, None, input_size])\n\n            # compute encoder features\n            question_features = tf.ones(tf.stack([batch_size, max_question_length, 2]))\n\n            v_wiqw = tf.get_variable(""v_wiq_w"", [1, 1, input_size],\n                                     initializer=tf.constant_initializer(1.0))\n\n            wiq_w = tf.matmul(tf.gather(emb_question * v_wiqw, tensors.support2question), emb_support, adjoint_b=True)\n            wiq_w = wiq_w + tf.expand_dims(support_mask, 1)\n\n            question_binary_mask = tf.gather(tf.sequence_mask(tensors.question_length, dtype=tf.float32),\n                                             tensors.support2question)\n            wiq_w = tf.reduce_sum(tf.nn.softmax(wiq_w) * tf.expand_dims(question_binary_mask, 2), [1])\n\n            # [B, L , 2]\n            support_features = tf.stack([tensors.word_in_question, wiq_w], 2)\n\n            # highway layer to allow for interaction between concatenated embeddings\n            if with_char_embeddings:\n                with tf.variable_scope(""char_embeddings"") as vs:\n                    emb_question = tf.layers.dense(emb_question, size, name=""embeddings_projection"")\n                    emb_question = highway_network(emb_question, 1)\n                    vs.reuse_variables()\n                    emb_support = tf.layers.dense(emb_support, size, name=""embeddings_projection"")\n                    emb_support = highway_network(emb_support, 1)\n\n            keep_prob = 1.0 - shared_resources.config.get(""dropout"", 0.0)\n            emb_question, emb_support = tf.cond(\n                tensors.is_eval,\n                lambda: (emb_question, emb_support),\n                lambda: (tf.nn.dropout(emb_question, keep_prob, noise_shape=[1, 1, emb_question.get_shape()[-1].value]),\n                         tf.nn.dropout(emb_support, keep_prob, noise_shape=[1, 1, emb_question.get_shape()[-1].value]))\n            )\n\n            # extend embeddings with features\n            emb_question_ext = tf.concat([emb_question, question_features], 2)\n            emb_support_ext = tf.concat([emb_support, support_features], 2)\n\n            # encode question and support\n            encoder_type = shared_resources.config.get(\'encoder\', \'lstm\').lower()\n            if encoder_type in [\'lstm\', \'sru\', \'gru\']:\n                size = size + 2 if encoder_type == \'sru\' else size  # to allow for use of residual in SRU\n                encoded_question = encoder(emb_question_ext, tensors.question_length, size, module=encoder_type)\n                encoded_support = encoder(emb_support_ext, tensors.support_length, size, module=encoder_type,\n                                          reuse=True)\n                projection_initializer = tf.constant_initializer(np.concatenate([np.eye(size), np.eye(size)]))\n                encoded_question = tf.layers.dense(encoded_question, size, tf.tanh, use_bias=False,\n                                                   kernel_initializer=projection_initializer,\n                                                   name=\'projection_q\')\n                encoded_support = tf.layers.dense(encoded_support, size, tf.tanh, use_bias=False,\n                                                  kernel_initializer=projection_initializer, name=\'projection_s\')\n            else:\n                raise ValueError(""Only rnn (\'lstm\', \'sru\', \'gru\') encoder allowed for FastQA!"")\n\n            answer_layer = shared_resources.config.get(\'answer_layer\', \'conditional\').lower()\n\n            topk = tf.get_variable(\n                \'topk\', initializer=shared_resources.config.get(\'topk\', 1), dtype=tf.int32, trainable=False)\n            topk_p = tf.placeholder(tf.int32, [], \'beam_size_setter\')\n            topk_assign = topk.assign(topk_p)\n            self._topk_assign = lambda k: self.tf_session.run(topk_assign, {topk_p: k})\n\n            if answer_layer == \'conditional\':\n                start_scores, end_scores, doc_idx, predicted_start_pointer, predicted_end_pointer = \\\n                    conditional_answer_layer(size, encoded_question, tensors.question_length, encoded_support,\n                                             tensors.support_length,\n                                             tensors.correct_start, tensors.support2question, tensors.answer2support,\n                                             tensors.is_eval,\n                                             topk=topk,\n                                             max_span_size=shared_resources.config.get(""max_span_size"", 10000))\n            elif answer_layer == \'conditional_bilinear\':\n                start_scores, end_scores, doc_idx, predicted_start_pointer, predicted_end_pointer = \\\n                    conditional_answer_layer(size, encoded_question, tensors.question_length, encoded_support,\n                                             tensors.support_length,\n                                             tensors.correct_start, tensors.support2question, tensors.answer2support,\n                                             tensors.is_eval,\n                                             topk=topk,\n                                             max_span_size=shared_resources.config.get(""max_span_size"", 10000),\n                                             bilinear=True)\n            elif answer_layer == \'bilinear\':\n                start_scores, end_scores, doc_idx, predicted_start_pointer, predicted_end_pointer = \\\n                    bilinear_answer_layer(size, encoded_question, tensors.question_length, encoded_support,\n                                          tensors.support_length,\n                                          tensors.support2question, tensors.answer2support, tensors.is_eval,\n                                          topk=topk,\n                                          max_span_size=shared_resources.config.get(""max_span_size"", 10000))\n            else:\n                raise ValueError\n\n            span = tf.stack([doc_idx, predicted_start_pointer, predicted_end_pointer], 1)\n\n            return TensorPort.to_mapping(self.output_ports, (start_scores, end_scores, span))\n'"
jack/readers/extractive_qa/tensorflow/modular_qa_model.py,8,"b'import tensorflow as tf\n\nfrom jack.core import TensorPortTensors, TensorPort\nfrom jack.readers.extractive_qa.tensorflow.abstract_model import AbstractXQAModelModule\nfrom jack.readers.extractive_qa.tensorflow.answer_layer import answer_layer\nfrom jack.util.tf.embedding import conv_char_embedding\nfrom jack.util.tf.modular_encoder import modular_encoder\n\n\nclass ModularQAModel(AbstractXQAModelModule):\n    def set_topk(self, k):\n        self._topk_assign(k)\n\n    def create_output(self, shared_resources, input_tensors):\n        tensors = TensorPortTensors(input_tensors)\n\n        [char_emb_question, char_emb_support] = conv_char_embedding(\n            len(shared_resources.char_vocab), shared_resources.config[\'repr_dim\'], tensors.word_chars,\n            tensors.word_char_length, [tensors.question_batch_words, tensors.support_batch_words])\n\n        model = shared_resources.config[\'model\']\n        repr_dim = shared_resources.config[\'repr_dim\']\n        input_size = shared_resources.embeddings.shape[-1]\n        dropout = shared_resources.config.get(""dropout"")\n        tensors.emb_question.set_shape([None, None, input_size])\n        tensors.emb_support.set_shape([None, None, input_size])\n\n        inputs = {\'question\': tensors.emb_question, \'support\': tensors.emb_support,\n                  \'char_question\': char_emb_question, \'char_support\': char_emb_support,\n                  \'word_in_question\': tf.expand_dims(tensors.word_in_question, 2)}\n        inputs_length = {\'question\': tensors.question_length, \'support\': tensors.support_length,\n                         \'char_question\': tensors.question_length, \'char_support\': tensors.support_length,\n                         \'word_in_question\': tensors.support_length}\n        inputs_mapping = {\'question\': None, \'support\': tensors.support2question,\n                          \'char_support\': tensors.support2question}\n\n        encoder_config = model[\'encoder_layer\']\n\n        encoded, lengths, mappings = modular_encoder(\n            encoder_config, inputs, inputs_length, inputs_mapping, repr_dim, dropout, tensors.is_eval)\n\n        with tf.variable_scope(\'answer_layer\'):\n            answer_layer_config = model[\'answer_layer\']\n            encoded_question = encoded[answer_layer_config.get(\'question\', \'question\')]\n            encoded_support = encoded[answer_layer_config.get(\'support\', \'support\')]\n\n            if \'repr_dim\' not in answer_layer_config:\n                answer_layer_config[\'repr_dim\'] = repr_dim\n            if \'max_span_size\' not in answer_layer_config:\n                answer_layer_config[\'max_span_size\'] = shared_resources.config.get(\'max_span_size\', 16)\n            topk = tf.get_variable(\n                \'topk\', initializer=shared_resources.config.get(\'topk\', 1), dtype=tf.int32, trainable=False)\n            topk_p = tf.placeholder(tf.int32, [], \'beam_size_setter\')\n            topk_assign = topk.assign(topk_p)\n            self._topk_assign = lambda k: self.tf_session.run(topk_assign, {topk_p: k})\n\n            start_scores, end_scores, doc_idx, predicted_start_pointer, predicted_end_pointer = \\\n                answer_layer(encoded_question, lengths[answer_layer_config.get(\'question\', \'question\')],\n                             encoded_support, lengths[answer_layer_config.get(\'support\', \'support\')],\n                             mappings[answer_layer_config.get(\'support\', \'support\')],\n                             tensors.answer2support, tensors.is_eval,\n                             tensors.correct_start, topk=topk, **answer_layer_config)\n\n        span = tf.stack([doc_idx, predicted_start_pointer, predicted_end_pointer], 1)\n\n        return TensorPort.to_mapping(self.output_ports, (start_scores, end_scores, span))\n'"
jack/readers/extractive_qa/torch/__init__.py,0,b''
jack/readers/extractive_qa/torch/fastqa.py,0,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom jack.core import SharedResources\nfrom jack.core.tensorport import Ports\nfrom jack.core.torch import PyTorchModelModule\nfrom jack.readers.extractive_qa.shared import XQAPorts\nfrom jack.util.torch import Highway\nfrom jack.util.torch import embedding, misc, xqa\nfrom jack.util.torch.rnn import BiLSTM\n\n\nclass FastQAPyTorchModelModule(PyTorchModelModule):\n    """"""PyTorch implementation of FastQA.""""""\n\n    # TODO: does not support multiparagraph yet\n    _input_ports = [XQAPorts.emb_question, XQAPorts.question_length,\n                    XQAPorts.emb_support, XQAPorts.support_length,\n                    # char embedding inputs\n                    XQAPorts.word_chars, XQAPorts.word_char_length,\n                    XQAPorts.question_batch_words, XQAPorts.support_batch_words,\n                    # feature input\n                    XQAPorts.word_in_question,\n                    # optional input, provided only during training\n                    XQAPorts.correct_start, XQAPorts.answer2support_training,\n                    XQAPorts.is_eval]\n\n    _output_ports = [XQAPorts.start_scores, XQAPorts.end_scores,\n                     XQAPorts.answer_span]\n    _training_input_ports = [XQAPorts.start_scores, XQAPorts.end_scores,\n                             XQAPorts.answer_span_target, XQAPorts.answer2support_training]\n    _training_output_ports = [Ports.loss]\n\n    @property\n    def output_ports(self):\n        return self._output_ports\n\n    @property\n    def input_ports(self):\n        return self._input_ports\n\n    @property\n    def training_input_ports(self):\n        return self._training_input_ports\n\n    @property\n    def training_output_ports(self):\n        return self._training_output_ports\n\n    def create_loss_module(self, shared_resources: SharedResources):\n        return xqa.XQAMinCrossentropyLossModule()\n\n    def create_prediction_module(self, shared_resources: SharedResources):\n        return FastQAPyTorchModule(shared_resources)\n\n\nclass FastQAPyTorchModule(nn.Module):\n    def __init__(self, shared_resources: SharedResources):\n        super(FastQAPyTorchModule, self).__init__()\n        self._shared_resources = shared_resources\n        input_size = shared_resources.embeddings.shape[-1]\n        size = shared_resources.config[""repr_dim""]\n        self._size = size\n        self._with_char_embeddings = self._shared_resources.config.get(""with_char_embeddings"", False)\n\n        # modules & parameters\n        if self._with_char_embeddings:\n            self._conv_char_embedding = embedding.ConvCharEmbeddingModule(\n                len(shared_resources.char_vocab), size)\n            self._embedding_projection = nn.Linear(size + input_size, size)\n            self._embedding_highway = Highway(size, 1)\n            self._v_wiq_w = nn.Parameter(torch.ones(1, 1, input_size + size))\n            input_size = size\n        else:\n            self._v_wiq_w = nn.Parameter(torch.ones(1, 1, input_size))\n\n        self._bilstm = BiLSTM(input_size + 2, size)\n        self._answer_layer = FastQAAnswerModule(shared_resources)\n\n        # [size, 2 * size]\n        self._question_projection = nn.Parameter(torch.cat([torch.eye(size), torch.eye(size)], dim=1))\n        self._support_projection = nn.Parameter(torch.cat([torch.eye(size), torch.eye(size)], dim=1))\n\n    def forward(self, emb_question, question_length,\n                emb_support, support_length,\n                unique_word_chars, unique_word_char_length,\n                question_words2unique, support_words2unique,\n                word_in_question, correct_start, answer2support,\n                is_eval):\n        """"""fast_qa model\n\n        Args:\n            emb_question: [Q, L_q, N]\n            question_length: [Q]\n            emb_support: [Q, L_s, N]\n            support_length: [Q]\n            unique_word_chars\n            unique_word_char_length\n            question_words2unique\n            support_words2unique\n            word_in_question: [Q, L_s]\n            correct_start: [A], only during training, i.e., is_eval=False\n            answer2question: [A], only during training, i.e., is_eval=False\n            is_eval: []\n\n        Returns:\n            start_scores [B, L_s, N], end_scores [B, L_s, N], span_prediction [B, 2]\n        """"""\n        # Some helpers\n        float_tensor = torch.cuda.FloatTensor if emb_question.is_cuda else torch.FloatTensor\n        long_tensor = torch.cuda.LongTensor if emb_question.is_cuda else torch.LongTensor\n        batch_size = question_length.data.shape[0]\n        max_question_length = question_length.max().data[0]\n        support_mask = misc.mask_for_lengths(support_length)\n        question_binary_mask = misc.mask_for_lengths(question_length, mask_right=False, value=1.0)\n\n        if self._with_char_embeddings:\n            # compute combined embeddings\n            [char_emb_question, char_emb_support] = self._conv_char_embedding(\n                unique_word_chars, unique_word_char_length, [question_words2unique, support_words2unique])\n\n            emb_question = torch.cat([emb_question, char_emb_question], 2)\n            emb_support = torch.cat([emb_support, char_emb_support], 2)\n\n        # compute encoder features\n        question_features = torch.autograd.Variable(torch.ones(batch_size, max_question_length, 2, out=float_tensor()))\n        question_features = question_features.type_as(emb_question)\n\n        v_wiqw = self._v_wiq_w\n        # [B, L_q, L_s]\n        wiq_w = torch.matmul(emb_question * v_wiqw, emb_support.transpose(1, 2))\n        # [B, L_q, L_s]\n        wiq_w = wiq_w + support_mask.unsqueeze(1)\n        wiq_w = F.softmax(\n            wiq_w.view(batch_size * max_question_length, -1), dim=1).view(batch_size, max_question_length, -1)\n        # [B, L_s]\n        wiq_w = torch.matmul(question_binary_mask.unsqueeze(1), wiq_w).squeeze(1)\n\n        # [B, L , 2]\n        support_features = torch.stack([word_in_question, wiq_w], dim=2)\n\n        if self._with_char_embeddings:\n            # highway layer to allow for interaction between concatenated embeddings\n            emb_question = self._embedding_projection(emb_question)\n            emb_support = self._embedding_projection(emb_support)\n            emb_question = self._embedding_highway(emb_question)\n            emb_support = self._embedding_highway(emb_support)\n\n        # dropout\n        dropout = self._shared_resources.config.get(""dropout"", 0.0)\n        emb_question = F.dropout(emb_question, dropout, training=not is_eval)\n        emb_support = F.dropout(emb_support, dropout, training=not is_eval)\n\n        # extend embeddings with features\n        emb_question_ext = torch.cat([emb_question, question_features], 2)\n        emb_support_ext = torch.cat([emb_support, support_features], 2)\n\n        # encode question and support\n        # [B, L, 2 * size]\n        encoded_question = self._bilstm(emb_question_ext)[0]\n        encoded_support = self._bilstm(emb_support_ext)[0]\n\n        # [B, L, size]\n        encoded_support = F.tanh(F.linear(encoded_support, self._support_projection))\n        encoded_question = F.tanh(F.linear(encoded_question, self._question_projection))\n\n        start_scores, end_scores, predicted_start_pointer, predicted_end_pointer = \\\n            self._answer_layer(encoded_question, question_length, encoded_support, support_length,\n                               correct_start, answer2support, is_eval)\n\n        # no multi paragraph support yet\n        doc_idx = torch.autograd.Variable(torch.zeros(predicted_start_pointer.data.shape[0], out=long_tensor()))\n        span = torch.stack([doc_idx, predicted_start_pointer, predicted_end_pointer], 1)\n\n        return start_scores, end_scores, span\n\n\nclass FastQAAnswerModule(nn.Module):\n    def __init__(self, shared_resources: SharedResources):\n        super(FastQAAnswerModule, self).__init__()\n        self._size = shared_resources.config[""repr_dim""]\n\n        # modules\n        self._linear_question_attention = nn.Linear(self._size, 1, bias=False)\n\n        self._linear_q_start_q = nn.Linear(self._size, self._size)\n        self._linear_q_start = nn.Linear(2 * self._size, self._size, bias=False)\n        self._linear_start_scores = nn.Linear(self._size, 1, bias=False)\n\n        self._linear_q_end_q = nn.Linear(self._size, self._size)\n        self._linear_q_end = nn.Linear(3 * self._size, self._size, bias=False)\n        self._linear_end_scores = nn.Linear(self._size, 1, bias=False)\n\n    def forward(self, encoded_question, question_length, encoded_support, support_length,\n                correct_start, answer2question, is_eval):\n        # casting\n        long_tensor = torch.cuda.LongTensor if encoded_question.is_cuda else torch.LongTensor\n        answer2question = answer2question.type(long_tensor)\n\n        # computing single time attention over question\n        attention_scores = self._linear_question_attention(encoded_question)\n        q_mask = misc.mask_for_lengths(question_length)\n        attention_scores = attention_scores.squeeze(2) + q_mask\n        question_attention_weights = F.softmax(attention_scores, dim=1)\n        question_state = torch.matmul(question_attention_weights.unsqueeze(1),\n                                      encoded_question).squeeze(1)\n\n        # Prediction\n        # start\n        start_input = torch.cat([question_state.unsqueeze(1) * encoded_support, encoded_support], 2)\n\n        q_start_state = self._linear_q_start(start_input) + self._linear_q_start_q(question_state).unsqueeze(1)\n        start_scores = self._linear_start_scores(F.relu(q_start_state)).squeeze(2)\n\n        support_mask = misc.mask_for_lengths(support_length)\n        start_scores = start_scores + support_mask\n        _, predicted_start_pointer = start_scores.max(1)\n\n        def align(t):\n            return torch.index_select(t, 0, answer2question)\n\n        if is_eval:\n            start_pointer = predicted_start_pointer\n        else:\n            # use correct start during training, because p(end|start) should be optimized\n            start_pointer = correct_start.type(long_tensor)\n            predicted_start_pointer = align(predicted_start_pointer)\n            start_scores = align(start_scores)\n            start_input = align(start_input)\n            encoded_support = align(encoded_support)\n            question_state = align(question_state)\n            support_mask = align(support_mask)\n\n        # end\n        u_s = []\n        for b, p in enumerate(start_pointer):\n            u_s.append(encoded_support[b, p.data[0]])\n        u_s = torch.stack(u_s)\n\n        end_input = torch.cat([encoded_support * u_s.unsqueeze(1), start_input], 2)\n\n        q_end_state = self._linear_q_end(end_input) + self._linear_q_end_q(question_state).unsqueeze(1)\n        end_scores = self._linear_end_scores(F.relu(q_end_state)).squeeze(2)\n\n        end_scores = end_scores + support_mask\n\n        max_support = support_length.max().data[0]\n\n        if is_eval:\n            end_scores += misc.mask_for_lengths(start_pointer, max_support, mask_right=False)\n\n        _, predicted_end_pointer = end_scores.max(1)\n\n        return start_scores, end_scores, predicted_start_pointer, predicted_end_pointer\n'"
tests/jack/readers/extractive_qa/test_util.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom jack.core import QASetting, Answer\nfrom jack.readers.extractive_qa.util import prepare_data\nfrom jack.util.vocab import Vocab\n\nqa_setting = QASetting(question=""What is the answer?"",\n                       support=[""It is not A."", ""It is B.""])\nanswers = [Answer(text=""B"", span=(6, 7), doc_idx=1)]\n\n\ndef test_prepare_data():\n\n    result = prepare_data(qa_setting, answers, Vocab(),\n                          with_answers=True)\n\n    question_tokens, question_ids, question_lemmas, question_length, \\\n    support_tokens, support_ids, support_lemmas, support_length, \\\n    word_in_question, token_offsets, answer_spans = result\n\n    assert question_tokens == [\'What\', \'is\', \'the\', \'answer\', \'?\']\n    assert question_ids == [1, 2, 3, 4, 5]\n    assert question_lemmas is None\n    assert question_length == 5\n\n    assert support_tokens == [[\'It\', \'is\', \'not\', \'A\', \'.\', ], [\'It\', \'is\', \'B\', \'.\']]\n    assert support_ids == [[6, 2, 7, 8, 9], [6, 2, 10, 9]]\n    assert support_lemmas == [None, None]\n    assert support_length == [5, 4]\n    assert word_in_question == [[0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]]\n    assert token_offsets == [[0, 3, 6, 10, 11], [0, 3, 6, 7]]\n    assert answer_spans == [[], [(2, 2)]]\n'"
tests/jack/readers/multiple_choice/test_simple_mcqa.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom jack.readers.classification.shared import *\n\nfrom jack.util.vocab import Vocab\n\n\ndef test_single_support_fixed_class_inputs():\n    import logging\n    logging.basicConfig(level=logging.INFO)\n    data_set = [\n        (QASetting(""Where is the cat?"", [""the cat is on the mat.""]), [Answer(""mat"")])\n    ]\n    shared_resources = SharedResources(Vocab(), {})\n    input_module = ClassificationSingleSupportInputModule(shared_resources)\n    input_module.setup_from_data(data_set)\n    input_module.setup()\n\n    assert len(input_module.shared_resources.answer_vocab) == 1\n    assert len(input_module.shared_resources.vocab) == 9\n\n    tensor_data_set = list(input_module.batch_generator(data_set, batch_size=3, is_eval=False))\n\n    expected_support = [""the"", ""cat"", ""is"", ""on"", ""the"", ""mat"", "".""]\n    expected_support_ids = [[shared_resources.vocab.get_id(sym) for sym in expected_support]]\n    first_instance = tensor_data_set[0]\n    actual_support_ids = first_instance[Ports.Input.support]\n    assert np.array_equal(actual_support_ids, expected_support_ids)\n    assert first_instance[Ports.Input.support_length][0] == len(expected_support)\n\n    actual_answer_ids = first_instance[Ports.Target.target_index]\n    expected_answer = [input_module.shared_resources.answer_vocab.get_id(""mat"")]\n    assert np.array_equal(actual_answer_ids, expected_answer)\n\n    actual_question_ids = first_instance[Ports.Input.question]\n    expected_question = [""where"", ""is"", ""the"", ""cat"", ""?""]\n    expected_question_ids = [[shared_resources.vocab.get_id(sym) for sym in expected_question]]\n    assert np.array_equal(actual_question_ids, expected_question_ids)\n    assert first_instance[Ports.Input.question_length][0] == len(expected_question)\n'"
