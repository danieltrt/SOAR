file_path,api_count,code
lib/__init__.py,0,"b'#!/usr/bin/env python3\n\n\n#import os\n#import sys\n\n#root = os.path.dirname(__file__)\n#sys.path.append(root + ""/subword-nmt"")\n\n#from apply_bpe import BPE\n'"
neuralmonkey/__init__.py,0,"b'""""""The neuralmonkey package is the root package of this project.""""""\r\n'"
neuralmonkey/checking.py,2,"b'""""""API checking module.\n\nThis module serves as a library of API checks used as assertions during\nconstructing the computational graph.\n""""""\n\nfrom typing import List, Optional\nimport tensorflow as tf\n\n\nclass CheckingException(Exception):\n    pass\n\n\ndef assert_shape(tensor: tf.Tensor,\n                 expected_shape: List[Optional[int]]) -> None:\n    """"""Check shape of a tensor.\n\n    Args:\n        tensor: Tensor to be chcecked.\n        expected_shape: Expected shape where `None` means the same as in TF and\n            `-1` means not checking the dimension.\n    """"""\n\n    shape_list = tensor.get_shape().as_list()\n\n    if len(shape_list) != len(expected_shape):\n        raise CheckingException(\n            ""Tensor \'{}\' with shape {} should have {} dimensions."".format(\n                tensor.name, shape_list, len(expected_shape)))\n\n    mismatching_dims = []\n    for i, (real, expected) in enumerate(zip(shape_list, expected_shape)):\n        if expected not in (real, -1):\n            mismatching_dims.append(i)\n\n    if mismatching_dims:\n        expected_str = "", "".join(\n            ""?"" if x == -1 else str(x) for x in expected_shape)\n        raise CheckingException(\n            (""Shape mismatch of {} in dimensions: {}. ""\n             ""Shape was {}, but should be [{}]"").format(\n                 tensor.name,\n                 "", "".join(str(d) for d in mismatching_dims),\n                 shape_list, expected_str))\n\n\ndef assert_same_shape(tensor_a: tf.Tensor, tensor_b: tf.Tensor) -> None:\n    """"""Check if two tensors have the same shape.""""""\n\n    shape_a = tensor_a.get_shape().as_list()\n    shape_b = tensor_b.get_shape().as_list()\n\n    if len(shape_a) != len(shape_b):\n        raise CheckingException(\n            (""Tensor \'{}\' has {} dimensions and tensor \'{}\' has {} ""\n             ""dimension, but should have the same shape."").format(\n                 tensor_a.name, len(shape_a), tensor_b.name, len(shape_b)))\n\n    mismatching_dims = []\n    for i, (size_a, size_b) in enumerate(zip(shape_a, shape_b)):\n        if size_a != size_b:\n            mismatching_dims.append(i)\n\n    if mismatching_dims:\n        raise CheckingException(\n            (""Shape mismatch of \'{}\' and \'{}\' in dimensions: {}. ""\n             ""Shapes were {} and {}"").format(\n                 tensor_a.name, tensor_b.name,\n                 "", "".join(str(d) for d in mismatching_dims),\n                 shape_a, shape_b))\n'"
neuralmonkey/checkpython.py,0,"b'import sys\n\nif sys.version_info[0] < 3 or sys.version_info[1] < 6:\n    print(""Error:"", file=sys.stderr)\n    print(""Neural Monkey must use Python >= 3.6"", file=sys.stderr)\n    print(""Your Python is"", sys.version, sys.executable, file=sys.stderr)\n    sys.exit(1)\n'"
neuralmonkey/dataset.py,0,"b'""""""Implementation of the dataset class.""""""\n# pylint: disable=too-many-lines\n# After deleting the legacy function load_dataset_from_files, this file becomes\n# short again.\nimport glob\nimport os\nimport random\nimport re\n\nfrom collections import deque\nfrom itertools import islice\nfrom typing import (\n    Any, TypeVar, Iterator, Callable, Optional, Dict, Union, List, Tuple, cast)\n\nfrom typeguard import check_argument_types\nfrom neuralmonkey.config.parsing import get_first_match\nfrom neuralmonkey.logging import debug, log, warn\nfrom neuralmonkey.readers.plain_text_reader import UtfPlainTextReader\nfrom neuralmonkey.util.match_type import match_type\nfrom neuralmonkey.writers.auto import AutoWriter\nfrom neuralmonkey.writers.plain_text_writer import Writer\n\n# pylint: disable=invalid-name\nDataType = TypeVar(""DataType"")\nDataSeries = Iterator[DataType]\nDataExample = Dict[str, DataType]\n\n# Reader: function that gets list of files and yields data\nReader = Callable[[List[str]], Any]\n\nDatasetPreprocess = Callable[[""Dataset""], DataSeries]\nDatasetPostprocess = Callable[\n    [""Dataset"", Dict[str, DataSeries]], DataSeries]\n\nFileDef = Union[str, List[str]]  # one or many files\nReaderDef = Union[FileDef, Tuple[FileDef, Reader]]  # files and optional reader\n\nSeriesConfig = Dict[str, Union[ReaderDef, DatasetPreprocess]]\n\n# SourceSpec: either a ReaderDef, series and preprocessor, or a dataset-level\n# preprocessor\nSourceSpec = Union[ReaderDef, Tuple[Callable, str], DatasetPreprocess]\n\n# OutputSpec: Tuple of series name, path, and optionally a writer\nOutputSpec = Union[Tuple[str, str], Tuple[str, str, Writer]]\n# pylint: enable=invalid-name\n\nPREPROCESSED_SERIES = re.compile(""pre_([^_]*)$"")\nSERIES_SOURCE = re.compile(""s_([^_]*)$"")\nSERIES_OUTPUT = re.compile(""s_(.*)_out"")\n\n\n# pylint: disable=too-few-public-methods\n# After migrating to py3.7, make this dataclass or namedtuple with defaults\nclass BatchingScheme:\n\n    def __init__(self,\n                 batch_size: int = None,\n                 drop_remainder: bool = False,\n                 bucket_boundaries: List[int] = None,\n                 bucket_batch_sizes: List[int] = None,\n                 ignore_series: List[str] = None) -> None:\n        """"""Construct the baching scheme.\n\n        Attributes:\n            batch_size: Number of examples in one mini-batch.\n            drop_remainder: Whether to throw out the last batch in the epoch\n                if it is not complete.\n            bucket_boundaries: Upper length boundaries of buckets.\n            bucket_batch_sizes:  Batch size per bucket. Lenght should be\n                `len(bucket_boundaries) + 1`\n            ignore_series: Series to ignore during bucketing.\n        """"""\n        check_argument_types()\n\n        self.batch_size = batch_size\n        self.drop_remainder = drop_remainder\n        self.bucket_boundaries = bucket_boundaries\n        self.bucket_batch_sizes = bucket_batch_sizes\n\n        self.ignore_series = []  # type: List[str]\n        if ignore_series is not None:\n            self.ignore_series = ignore_series\n\n        if (self.batch_size is None) == (self.bucket_boundaries is None):\n            raise ValueError(""You must specify either batch_size or ""\n                             ""bucket_boundaries, not both"")\n\n        if self.bucket_boundaries is not None:\n            if self.bucket_batch_sizes is None:\n                raise ValueError(""You must specify bucket_batch_sizes"")\n            if len(self.bucket_batch_sizes) != len(self.bucket_boundaries) + 1:\n                raise ValueError(\n                    ""There should be N+1 batch sizes for N bucket boundaries"")\n# pylint: enable=too-few-public-methods\n\n\n# The protected functions below are designed to convert the ambiguous spec\n# structures to a normalized form.\n\ndef _normalize_readerdef(reader_def: ReaderDef) -> Tuple[List[str], Reader]:\n    if isinstance(reader_def, tuple):\n        reader = reader_def[1]\n        files = _normalize_filedef(reader_def[0])\n    else:\n        reader = UtfPlainTextReader\n        files = _normalize_filedef(reader_def)\n\n    return files, reader\n\n\ndef _normalize_outputspec(output_spec: OutputSpec) -> Tuple[str, str, Writer]:\n    if len(output_spec) == 2:\n        return output_spec[0], output_spec[1], AutoWriter\n    return cast(Tuple[str, str, Writer], output_spec)\n\n\ndef _normalize_filedef(file_def: FileDef) -> List[str]:\n    if isinstance(file_def, str):\n        return _expand_patterns_flat([file_def])\n    return _expand_patterns_flat(file_def)\n\n\ndef _expand_patterns_flat(patterns: List[str]) -> List[str]:\n    paths = []\n    for pattern in patterns:\n        matched_files = sorted(glob.glob(pattern))\n        if not matched_files:\n            raise FileNotFoundError(\n                ""Pattern did not match any files: {}"".format(pattern))\n        paths.extend(matched_files)\n\n    return paths\n\n\ndef _get_series_paths_and_readers(\n        series_config: SeriesConfig) -> Dict[str, Tuple[List[str], Reader]]:\n    """"""Get paths to files that contain data from the dataset kwargs.\n\n    Input file for a serie named \'xxx\' is specified by parameter \'s_xxx\'. The\n    dataset series is defined by a string with a path / list of strings with\n    paths, or a tuple whose first member is a path or a list of paths and the\n    second memeber is a reader function.\n\n    The paths can contain wildcards, which will be expanded using\n    :py:func:`glob.glob` in sorted order.\n\n    Arguments:\n        series_config: A dictionary containing the dataset keyword argument\n            specs.\n\n    Returns:\n        A dictionary which maps serie names to the paths of their input files\n        and readers..\n    """"""\n    keys = [k for k in list(series_config.keys()) if SERIES_SOURCE.match(k)]\n    names = [get_first_match(SERIES_SOURCE, k) for k in keys]\n\n    series_sources = {}\n    for name, key in zip(names, keys):\n        value = cast(ReaderDef, series_config[key])\n\n        if isinstance(value, tuple):\n            patterns, reader = value  # type: ignore\n        else:\n            patterns = value\n            reader = UtfPlainTextReader\n\n        if isinstance(patterns, str):\n            patterns = [patterns]\n\n        paths = _expand_patterns_flat(patterns)\n        debug(""Series \'{}\' has the following files: {}"".format(name, paths))\n\n        series_sources[name] = (paths, reader)\n\n    return series_sources\n\n\ndef _get_series_outputs(series_config: SeriesConfig) -> List[OutputSpec]:\n    """"""Get paths to series outputs from the dataset keyword argument specs.\n\n    Output file for a series named \'xxx\' is specified by parameter \'s_xxx_out\'\n\n    Arguments:\n        series_config: A dictionary containing the dataset keyword argument\n           specs.\n\n    Returns:\n        A dictionary which maps serie names to the paths for their output\n        files.\n    """"""\n    outputs = {}\n    for key, value in series_config.items():\n        matcher = SERIES_OUTPUT.match(key)\n        if matcher:\n            name = matcher.group(1)\n            if not isinstance(value, str):\n                raise ValueError(\n                    ""Output path for \'{}\' series must be a string, was {}."".\n                    format(name, type(value)))\n            outputs[name] = cast(str, value)\n    return [(key, val, AutoWriter) for key, val in outputs.items()]\n\n\n# pylint: disable=too-many-locals,too-many-branches\ndef load(name: str,\n         series: List[str],\n         data: List[SourceSpec],\n         batching: BatchingScheme = None,\n         outputs: List[OutputSpec] = None,\n         buffer_size: int = None,\n         shuffled: bool = False) -> ""Dataset"":\n    """"""Create a dataset using specification from the configuration.\n\n    The dataset provides iterators over data series. The dataset has a buffer,\n    which pre-fetches a given number of the data series lazily. In case the\n    dataset is not lazy (buffer size is `None`), the iterators are built on top\n    of in-memory arrays. Otherwise, the iterators operate on the data sources\n    directly.\n\n    Arguments:\n        name: The name of the dataset.\n        series: A list of names of data series the dataset contains.\n        data: The specification of the data sources for each series.\n        outputs: A list of output specifications.\n        buffer_size: The size of the buffer. If set, the dataset will be loaded\n            lazily into the buffer (useful for large datasets). The buffer size\n            specifies the number of sequences to pre-load. This is useful for\n            pseudo-shuffling of large data on-the-fly. Ideally, this should be\n            (much) larger than the batch size. Note that the buffer gets\n            refilled each time its size is less than half the `buffer_size`.\n            When refilling, the buffer gets refilled to the specified size.\n    """"""\n    check_argument_types()\n\n    if batching is None:\n        from neuralmonkey.experiment import Experiment\n        log(""Using default batching scheme for dataset {}."".format(name))\n        # pylint: disable=no-member\n        batch_size = Experiment.get_current().config.args.batch_size\n        # pylint: enable=no-member\n        if batch_size is None:\n            raise ValueError(""Argument main.batch_size is not specified, ""\n                             ""cannot use default batching scheme."")\n        batching = BatchingScheme(batch_size=batch_size)\n\n    if not series:\n        raise ValueError(""No dataset series specified."")\n\n    if not [s for s in data if match_type(s, ReaderDef)]:  # type: ignore\n        raise ValueError(""At least one data series should be from a file"")\n\n    if len(series) != len(data):\n        raise ValueError(\n            ""The \'series\' and \'data\' lists should have the same number""\n            "" of elements: {} vs {}."".format(len(series), len(data)))\n\n    if len(series) != len(set(series)):\n        raise ValueError(""There are duplicate series."")\n\n    if outputs is not None:\n        output_sources = [o[0] for o in outputs]\n        if len(output_sources) != len(set(output_sources)):\n            raise ValueError(""Multiple outputs for a single series"")\n\n    log(""Initializing dataset {}."".format(name))\n\n    iterators = {}  # type: Dict[str, Callable[[], DataSeries]]\n\n    prep_sl = {}  # type: Dict[str, Tuple[Callable, str]]\n    prep_dl = {}  # type: Dict[str, DatasetPreprocess]\n\n    def _make_iterator(reader, files):\n        def itergen():\n            return reader(files)\n        return itergen\n\n    def _make_sl_iterator(src, prep):\n        def itergen():\n            return (prep(item) for item in iterators[src]())\n        return itergen\n\n    def _make_dl_iterator(func):\n        def itergen():\n            return func(iterators)\n        return itergen\n\n    # First, prepare iterators for series using file readers\n    for s_name, source_spec in zip(series, data):\n        if match_type(source_spec, ReaderDef):  # type: ignore\n            files, reader = _normalize_readerdef(cast(ReaderDef, source_spec))\n            for path in files:\n                if not os.path.isfile(path):\n                    raise FileNotFoundError(\n                        ""File not found. Series: {}, Path: {}""\n                        .format(s_name, path))\n\n            iterators[s_name] = _make_iterator(reader, files)\n\n        elif match_type(source_spec, Tuple[Callable, str]):\n            prep_sl[s_name] = cast(Tuple[Callable, str], source_spec)\n\n        else:\n            assert match_type(source_spec, DatasetPreprocess)  # type: ignore\n            prep_dl[s_name] = cast(DatasetPreprocess, source_spec)\n\n    # Second, prepare series-level preprocessors.\n    # Note that series-level preprocessors cannot be stacked on the dataset\n    # specification level.\n    for s_name, (preprocessor, source) in prep_sl.items():\n        if source not in iterators:\n            raise ValueError(\n                ""Source series for series-level preprocessor nonexistent: ""\n                ""Preprocessed series \'{}\', source series \'{}\'"")\n        iterators[s_name] = _make_sl_iterator(source, preprocessor)\n\n    # Finally, dataset-level preprocessors.\n    for s_name, func in prep_dl.items():\n        iterators[s_name] = _make_dl_iterator(func)\n\n    output_dict = None\n    if outputs is not None:\n        output_dict = {s_name: (path, writer)\n                       for s_name, path, writer\n                       in [_normalize_outputspec(out) for out in outputs]}\n\n    if buffer_size is not None:\n        return Dataset(name, iterators, batching, output_dict,\n                       (buffer_size // 2, buffer_size), shuffled)\n\n    return Dataset(name, iterators, batching, output_dict, None, shuffled)\n# pylint: enable=too-many-locals,too-many-branches\n\n\nclass Dataset:\n    """"""Buffered and batched dataset.\n\n    This class serves as collection of data series for particular encoders and\n    decoders in the model.\n\n    Dataset has a number of data series, which are sequences of data (of any\n    type) that should have the same length. The sequences are loaded in a\n    buffer and can be loaded lazily.\n\n    Using the `batches` method, dataset yields batches, through which the data\n    are accessed by the model.\n    """"""\n\n    def __init__(self,\n                 name: str,\n                 iterators: Dict[str, Callable[[], Iterator]],\n                 batching: BatchingScheme,\n                 outputs: Dict[str, Tuple[str, Writer]] = None,\n                 buffer_size: Tuple[int, int] = None,\n                 shuffled: bool = False) -> None:\n        """"""Construct a new instance of the dataset class.\n\n        Do not call this method from the configuration directly. Instead, use\n        the `from_files` function of this module.\n\n        The dataset iterators are provided through factory functions, which\n        return the opened iterators when called with no arguments.\n\n        Arguments:\n            name: The name for the dataset.\n            iterators: A series-iterator generator mapping.\n            lazy: If False, load the data from iterators to a list and store\n                the list in memory.\n            buffer_size: Use this tuple as a minimum and maximum buffer size\n                for pre-loading data. This should be (a few times) larger than\n                the batch size used for mini-batching. When the buffer size\n                gets under the lower threshold, it is refilled with the new\n                data and optionally reshuffled. If the buffer size is `None`,\n                all data is loaded into memory.\n            shuffled: Whether to shuffle the buffer during batching.\n        """"""\n        self.name = name\n        self.iterators = iterators\n        self.batching = batching\n        self.outputs = outputs\n\n        if buffer_size is not None:\n            self.lazy = True\n            self.buffer_min_size, self.buffer_size = buffer_size\n        else:\n            self.lazy = False\n\n        self.shuffled = shuffled\n        self.length = None\n\n        if not self.lazy:\n            # Load the data from iterators to memory and point new iterators\n            # to these structures. (This prevents multiple loads from disk.)\n            data = {s_name: list(it())\n                    for s_name, it in self.iterators.items()}\n\n            # Check whether all loaded series have the same length\n            length_dict = {\n                s_name: len(s_data) for s_name, s_data in data.items()}\n            if len(set(length_dict.values())) > 1:\n                raise ValueError(""Lengths of data series do not match: {}""\n                                 .format(str(length_dict)))\n\n            self.length = next(iter(length_dict.values()))\n            self.iterators = {\n                s_name: lambda n=s_name: iter(data[n])  # type: ignore\n                for s_name in self.iterators}\n\n    def __len__(self) -> int:\n        """"""Get the length of the dataset.\n\n        Returns:\n            The length of the dataset.\n\n        Raises:\n            `NotImplementedError` when the dataset is lazy.\n        """"""\n        if self.lazy:\n            raise NotImplementedError(""Querying the len of a lazy dataset."")\n        assert self.length is not None\n        return self.length\n\n    def __contains__(self, name: str) -> bool:\n        """"""Check if the dataset contains a series of a given name.\n\n        Arguments:\n            name: Series name\n\n        Returns:\n            True if the dataset contains the series, False otherwise.\n        """"""\n        return name in self.iterators\n\n    @property\n    def series(self) -> List[str]:\n        return list(sorted(self.iterators.keys()))\n\n    def get_series(self, name: str) -> Iterator:\n        """"""Get the data series with a given name.\n\n        Arguments:\n            name: The name of the series to fetch.\n\n        Returns:\n            A freshly initialized iterator over the data series.\n\n        Raises:\n            KeyError if the series does not exists.\n        """"""\n        return self.iterators[name]()\n\n    def maybe_get_series(self, name: str) -> Optional[Iterator]:\n        """"""Get the data series with a given name, if it exists.\n\n        Arguments:\n            name: The name of the series to fetch.\n\n        Returns:\n            The data series or None if it does not exist.\n        """"""\n        if name in self.iterators:\n            return self.get_series(name)\n        return None\n\n    # pylint: disable=too-many-locals,too-many-branches,too-many-statements\n    def batches(self) -> Iterator[""Dataset""]:\n        """"""Split the dataset into batches.\n\n        Returns:\n            Generator yielding the batches.\n        """"""\n        if self.batching.batch_size is not None:\n            max_bs = self.batching.batch_size\n        else:\n            assert self.batching.bucket_batch_sizes is not None\n            max_bs = max(self.batching.bucket_batch_sizes)\n\n        if self.lazy and self.buffer_min_size < max_bs:\n            warn(""Minimum buffer size ({}) lower than batch size ({}). ""\n                 ""It is recommended to use large buffer size.""\n                 .format(self.buffer_min_size, max_bs))\n\n        # Initialize iterators\n        iterators = {s: it() for s, it in self.iterators.items()}\n\n        # Create iterator over instances\n        zipped_iterator = (\n            dict(zip(iterators, row)) for row in zip(*iterators.values()))\n\n        # Fill the buffer with initial values, shuffle optionally\n        if self.lazy:\n            # pylint: disable=stop-iteration-return\n            # This is pylint issue https://github.com/PyCQA/pylint/issues/2158\n            lbuf = list(next(zipped_iterator) for _ in range(self.buffer_size))\n            # pylint: enable=stop-iteration-return\n        else:\n            lbuf = list(zipped_iterator)\n        if self.shuffled:\n            random.shuffle(lbuf)\n        buf = deque(lbuf)\n\n        def _make_datagen(rows, key):\n            def itergen():\n                return (row[key] for row in rows)\n            return itergen\n\n        # Iterate over the rest of the data until buffer is empty\n        batch_index = 0\n        buckets = [[]]  # type: List[List[DataExample]]\n\n        if self.batching.bucket_boundaries is not None:\n            buckets += [[] for _ in self.batching.bucket_boundaries]\n\n        while buf:\n            row = buf.popleft()\n\n            if self.batching.bucket_boundaries is None:\n                bucket_id = 0\n            else:\n                # TODO: use only specific series to determine the bucket number\n                length = max(len(row[key]) for key in row)\n\n                bucket_id = -1\n                for b_id, limit in enumerate(self.batching.bucket_boundaries):\n                    fits_in = length <= limit\n                    tighter_fit = (\n                        bucket_id == -1\n                        or limit < self.batching.bucket_boundaries[\n                            bucket_id])\n\n                    if fits_in and tighter_fit:\n                        bucket_id = b_id\n\n            buckets[bucket_id].append(row)\n\n            if self.batching.bucket_batch_sizes is None:\n                assert self.batching.batch_size is not None\n                is_full = len(buckets[bucket_id]) >= self.batching.batch_size\n            else:\n                is_full = (len(buckets[bucket_id])\n                           >= self.batching.bucket_batch_sizes[bucket_id])\n\n            if is_full:\n                # Create the batch\n                name = ""{}.batch.{}"".format(self.name, batch_index)\n                data = {key: _make_datagen(buckets[bucket_id], key)\n                        for key in buckets[bucket_id][0]}\n\n                yield Dataset(\n                    name=name, iterators=data, batching=self.batching)\n                batch_index += 1\n                buckets[bucket_id] = []\n\n            # If lazy, refill buffer & shuffle if needed\n            # Otherwise, all of the data is already loaded in the buffer.\n            if self.lazy and len(buf) < self.buffer_min_size:\n                # In case buffer_size is lower than batch_size\n                to_add = self.buffer_size - len(buf)\n\n                for _, item in zip(range(to_add), zipped_iterator):\n                    buf.append(item)\n\n                if self.shuffled:\n                    lbuf = list(buf)\n                    random.shuffle(lbuf)\n                    buf = deque(lbuf)\n\n        if not self.batching.drop_remainder:\n            for bucket in buckets:\n                if bucket:\n                    name = ""{}.batch.{}"".format(self.name, batch_index)\n                    data = {key: _make_datagen(bucket, key)\n                            for key in bucket[0]}\n\n                    yield Dataset(\n                        name=name, iterators=data, batching=self.batching)\n                    batch_index += 1\n    # pylint: enable=too-many-locals,too-many-branches\n\n    def subset(self, start: int, length: int) -> ""Dataset"":\n        """"""Create a subset of the dataset.\n\n        The sub-dataset will inherit the laziness and buffer size and shuffling\n        from the parent dataset.\n\n        Arguments:\n            start: Index of the first data instance in the dataset.\n            length: Number of instances to include in the subset.\n\n        Returns:\n            A subset `Dataset` object.\n        """"""\n        name = ""{}.{}.{}"".format(self.name, start, length)\n\n        outputs = None\n        if self.outputs is not None:\n            outputs = {key: (""{}.{:010}"".format(path, start), writer)\n                       for key, (path, writer) in self.outputs.items()}\n\n        slices = {s_id: lambda s=s_id: islice(self.get_series(s),\n                                              start, start + length)\n                  for s_id in self.iterators}\n\n        # Here, the type: ignore is because of the tied argument to the lambda\n        # function above, which made it Callable[[Any], ...] instead of just\n        # Callable[[], ...].\n        return Dataset(  # type: ignore\n            name=name,\n            iterators=slices,\n            batching=self.batching,\n            outputs=outputs,\n            buffer_size=((self.buffer_min_size, self.buffer_size)\n                         if self.lazy else None),\n            shuffled=self.shuffled)\n'"
neuralmonkey/decorators.py,1,"b'from functools import wraps\n\nimport tensorflow as tf\n\nfrom neuralmonkey.model.parameterized import Parameterized\nfrom neuralmonkey.tf_utils import tf_print\n\n\ndef tensor(func):\n    @wraps(func)\n    def decorate(self, *args, **kwargs):\n        attribute_name = ""_{}_cached_placeholder"".format(func.__name__)\n        if not hasattr(self, attribute_name):\n            if isinstance(self, Parameterized):\n                # jump out of the caller\'s scope and into the ModelPart\'s scope\n                with self.use_scope():\n                    value = func(self, *args, **kwargs)\n                    if isinstance(value, tf.Tensor):\n                        value = tf_print(\n                            value, ""<{}.{}>"".format(self.name, func.__name__),\n                            ""tensorval"")\n            else:\n                value = func(self, *args, **kwargs)\n            setattr(self, attribute_name, value)\n\n        return getattr(self, attribute_name)\n    return property(decorate)\n'"
neuralmonkey/experiment.py,8,"b'""""""Provides a high-level API for training and using a model.""""""\n# pylint: disable=too-many-lines\n\nfrom argparse import Namespace\nimport os\nimport random\nimport shutil\nimport subprocess\nfrom typing import Any, Callable, Dict, Iterable, List, Optional, Tuple\nfrom typing import Set  # pylint: disable=unused-import\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.tensorboard.plugins import projector\n\nfrom neuralmonkey.checking import CheckingException\nfrom neuralmonkey.dataset import Dataset\nfrom neuralmonkey.logging import Logging, log, debug, warn\nfrom neuralmonkey.config.configuration import Configuration\nfrom neuralmonkey.config.normalize import normalize_configuration\nfrom neuralmonkey.learning_utils import (training_loop, evaluation,\n                                         run_on_dataset,\n                                         print_final_evaluation)\nfrom neuralmonkey.runners.base_runner import ExecutionResult\nfrom neuralmonkey.runners.dataset_runner import DatasetRunner\n\n\n_TRAIN_ARGS = [\n    ""val_dataset"", ""trainer"", ""name"", ""train_dataset"", ""epochs"",\n    ""test_datasets"", ""initial_variables"", ""validation_period"",\n    ""val_preview_input_series"", ""val_preview_output_series"",\n    ""val_preview_num_examples"", ""logging_period"", ""visualize_embeddings"",\n    ""overwrite_output_dir""\n]\n\n\n_EXPERIMENT_FILES = [""experiment.log"", ""experiment.ini"", ""original.ini"",\n                     ""git_commit"", ""git_diff"", ""variables.data.best""]\n\n\nclass Experiment:\n    # pylint: disable=no-member\n\n    _current_experiment = None\n\n    def __init__(self,\n                 config_path: str,\n                 train_mode: bool = False,\n                 overwrite_output_dir: bool = False,\n                 config_changes: List[str] = None) -> None:\n        """"""Initialize a Neural Monkey experiment.\n\n        Arguments:\n            config_path: The path to the experiment configuration file.\n            train_mode: Indicates whether the model should be prepared for\n                training.\n            overwrite_output_dir: Indicates whether an existing experiment\n                should be reused. If `True`, this overrides the setting in\n                the configuration file.\n            config_changes: A list of modifications that will be made to the\n                loaded configuration file before parsing.\n        """"""\n        self.train_mode = train_mode\n        self._config_path = config_path\n\n        self.graph = tf.Graph()\n        self._initializers = {}  # type: Dict[str, Callable]\n        self._initialized_variables = set()  # type: Set[str]\n        self.cont_index = -1\n        self._model_built = False\n        self._vars_loaded = False\n        self._model = None  # type: Optional[Namespace]\n\n        self.config = create_config(train_mode)\n        self.config.load_file(config_path, config_changes)\n        args = self.config.args\n\n        if self.train_mode:\n            # We may need to create the experiment directory.\n            if (os.path.isdir(args.output)\n                    and os.path.exists(\n                        os.path.join(args.output, ""experiment.ini""))):\n                if args.overwrite_output_dir or overwrite_output_dir:\n                    # we do not want to delete the directory contents\n                    log(""Directory with experiment.ini \'{}\' exists, ""\n                        ""overwriting enabled, proceeding."".format(args.output))\n                else:\n                    raise RuntimeError(\n                        ""Directory with experiment.ini \'{}\' exists, ""\n                        ""overwriting disabled."".format(args.output))\n\n            if not os.path.isdir(args.output):\n                os.mkdir(args.output)\n\n        # Find how many times the experiment has been continued.\n        while any(os.path.exists(self.get_path(f, self.cont_index + 1))\n                  for f in _EXPERIMENT_FILES):\n            self.cont_index += 1\n\n    @property\n    def model(self) -> Namespace:\n        """"""Get configuration namespace of the experiment.\n\n        The `Experiment` stores the configuration recipe in `self.config`.\n        When the configuration is built (meaning the classes referenced from\n        the config file are instantiated), it is saved in the `model` property\n        of the experiment.\n\n        Returns:\n            The built namespace config object.\n\n        Raises:\n            `RuntimeError` when the configuration model has not been built.\n        """"""\n        if self._model is None:\n            raise RuntimeError(""Experiment argument model not initialized"")\n\n        return self._model\n\n    def _bless_graph_executors(self) -> None:\n        """"""Pre-compute the tensors referenced by the graph executors.\n\n        Due to the lazy nature of the computational graph related components,\n        nothing is actually added to the graph until it is ""blessed"" (\n        referenced, and therefore, executed).\n\n        ""Blessing"" is usually implemented in the form of a log or a debug call\n        with the blessed tensor as parameter. Referencing a `Tensor` causes the\n        whole computational graph that is needed to evaluate the tensor to be\n        built.\n\n        This function ""blesses"" all tensors that could be potentially used\n        using the `fetches` property of the provided runner objects.\n\n        If the experiment runs in the training mode, this function also\n        blesses the tensors fetched by the trainer(s).\n        """"""\n        log(""Building TF Graph"")\n        if hasattr(self.model, ""trainer""):\n            if isinstance(self.model.trainer, List):\n                trainers = self.model.trainer\n            else:\n                trainers = [self.model.trainer]\n\n            for trainer in trainers:\n                debug(""Trainer fetches: {}"".format(trainer.fetches), ""bless"")\n\n        for runner in self.model.runners:\n            debug(""Runner fetches: {}"".format(runner.fetches), ""bless"")\n        log(""TF Graph built"")\n\n    def register_inputs(self) -> None:\n        feedables = set.union(*[ex.feedables for ex in self.model.runners])\n        if self.train_mode:\n            feedables |= set.union(\n                *[ex.feedables for ex in self.model.trainers])\n\n        # collect input shapes and types\n        input_types = {}  # type: Dict[str, tf.DType]\n        input_shapes = {}  # type: Dict[str, tf.TensorShape]\n\n        for feedable in feedables:\n            input_types.update(feedable.input_types)\n            input_shapes.update(feedable.input_shapes)\n\n        dataset = {}  # type: Dict[str, tf.Tensor]\n        for s_id, dtype in input_types.items():\n            shape = input_shapes[s_id]\n            dataset[s_id] = tf.placeholder(dtype, shape, s_id)\n\n        for feedable in feedables:\n            feedable.register_input(dataset)\n\n        self.model.dataset_runner.register_input(dataset)\n\n    def build_model(self) -> None:\n        """"""Build the configuration and the computational graph.\n\n        This function is invoked by all of the main entrypoints of the\n        `Experiment` class (`train`, `evaluate`, `run`). It manages the\n        building of the TensorFlow graph.\n\n        The bulding procedure is executed as follows:\n        1. Random seeds are set.\n        2. Configuration is built (instantiated) and normalized.\n        3. TODO(tf-data) tf.data.Dataset instance is created and registered\n            in the model parts. (This is not implemented yet!)\n        4. Graph executors are ""blessed"". This causes the rest of the TF Graph\n            to be built.\n        5. Sessions are initialized using the TF Manager object.\n\n        Raises:\n            `RuntimeError` when the model is already built.\n        """"""\n        if self._model_built:\n            raise RuntimeError(""build_model() called twice"")\n\n        random.seed(self.config.args.random_seed)\n        np.random.seed(self.config.args.random_seed)\n\n        with self.graph.as_default():\n            tf.set_random_seed(self.config.args.random_seed)\n\n            # Enable the created model parts to find this experiment.\n            type(self)._current_experiment = self  # type: ignore\n\n            self.config.build_model(warn_unused=self.train_mode)\n            normalize_configuration(self.config.model, self.train_mode)\n\n            self._model = self.config.model\n            self._model_built = True\n\n            # prepare dataset runner\n            self.model.dataset_runner = DatasetRunner()\n\n            # build dataset\n            self.register_inputs()\n\n            self._bless_graph_executors()\n            self.model.tf_manager.initialize_sessions()\n\n            type(self)._current_experiment = None\n\n            if self.train_mode and self.model.visualize_embeddings is not None:\n                self.visualize_embeddings()\n\n        self._check_unused_initializers()\n\n    def train(self) -> None:\n        """"""Train model specified by this experiment.\n\n        This function is one of the main functions (entrypoints) called on\n        the experiment. It builds the model (if needed) and runs the training\n        procedure.\n\n        Raises:\n            `RuntimeError` when the experiment is not intended for training.\n        """"""\n        if not self.train_mode:\n            raise RuntimeError(""train() was called, but the experiment was ""\n                               ""created with train_mode=False"")\n        if not self._model_built:\n            self.build_model()\n\n        self.cont_index += 1\n\n        # Initialize the experiment directory.\n        self.config.save_file(self.get_path(""experiment.ini""))\n        shutil.copyfile(self._config_path, self.get_path(""original.ini""))\n        save_git_info(self.get_path(""git_commit""), self.get_path(""git_diff""))\n        Logging.set_log_file(self.get_path(""experiment.log""))\n\n        Logging.print_header(self.model.name, self.model.output)\n\n        with self.graph.as_default():\n            self.model.tf_manager.init_saving(self.get_path(""variables.data""))\n\n            training_loop(cfg=self.model)\n\n            final_variables = self.get_path(""variables.data.final"")\n            log(""Saving final variables in {}"".format(final_variables))\n            self.model.tf_manager.save(final_variables)\n\n            if self.model.test_datasets:\n                if self.model.tf_manager.best_score_index is not None:\n                    self.model.tf_manager.restore_best_vars()\n\n                for test_id, dataset in enumerate(self.model.test_datasets):\n                    self.evaluate(dataset, write_out=True,\n                                  name=""test_{}"".format(test_id))\n\n            log(""Finished."")\n            self._vars_loaded = True\n\n    def load_variables(self, variable_files: List[str] = None) -> None:\n        """"""Load variables of the built model from file(s).\n\n        When variable files are not provided, Neural Monkey will try to infer\n        the name of a default checkpoint file using the following key:\n        1. Look for the averaged checkpoints named `variables.data.avg` or\n           `variables.data.avg-0`.\n        2. Look for file `variables.data.best` file which usually contains the\n           best scoring checkpoint from the run.\n        3. Look for the final checkpoint saved in `variables.data.final`.\n\n        Arguments:\n            variable_files: A list of variable files to load. The length of\n                this list should match the number of sessions.\n        """"""\n        if not self._model_built:\n            self.build_model()\n\n        if variable_files is None:\n            if os.path.exists(self.get_path(""variables.data.avg-0.index"")):\n                variable_files = [self.get_path(""variables.data.avg-0"")]\n            elif os.path.exists(self.get_path(""variables.data.avg.index"")):\n                variable_files = [self.get_path(""variables.data.avg"")]\n            elif os.path.exists(self.get_path(""variables.data.best"")):\n                best_var_file = self.get_path(""variables.data.best"")\n                with open(best_var_file, ""r"") as f_best:\n                    var_path = f_best.read().rstrip()\n                variable_files = [os.path.join(self.config.args.output,\n                                               var_path)]\n            elif os.path.exists(self.get_path(""variables.data.final.index"")):\n                variable_files = [self.get_path(""variables.data.final"")]\n            else:\n                raise RuntimeError(""Cannot infer default variables file"")\n\n            log(""Default variable file \'{}\' will be used for loading ""\n                ""variables."".format(variable_files[0]))\n\n        for vfile in variable_files:\n            if not os.path.exists(""{}.index"".format(vfile)):\n                raise RuntimeError(\n                    ""Index file for var prefix {} does not exist""\n                    .format(vfile))\n\n        self.model.tf_manager.restore(variable_files)\n        self._vars_loaded = True\n\n    def run_model(self,\n                  dataset: Dataset,\n                  write_out: bool = False,\n                  log_progress: int = 0) -> Tuple[\n                      List[ExecutionResult], Dict[str, List], Dict[str, List]]:\n        """"""Run the model on a given dataset.\n\n        Args:\n            dataset: The dataset on which the model will be executed.\n            write_out: Flag whether the outputs should be printed to a file\n                defined in the dataset object.\n            log_progress: log progress every X seconds\n\n        Returns:\n            A list of `ExecutionResult`s and a dictionary of the output series.\n        """"""\n        if not self._model_built:\n            self.build_model()\n        if not self._vars_loaded:\n            self.load_variables()\n\n        with self.graph.as_default():\n            return run_on_dataset(\n                self.model.tf_manager,\n                self.model.runners,\n                self.model.dataset_runner,\n                dataset,\n                self.model.postprocess,\n                write_out=write_out,\n                log_progress=log_progress)\n\n    def evaluate(self,\n                 dataset: Dataset,\n                 write_out: bool = False,\n                 log_progress: int = 0,\n                 name: str = None) -> Dict[str, Any]:\n        """"""Run the model on a given dataset and evaluate the outputs.\n\n        Args:\n            dataset: The dataset on which the model will be executed.\n            write_out: Flag whether the outputs should be printed to a file\n                defined in the dataset object.\n            log_progress: log progress every X seconds\n            name: The name of the evaluated dataset\n\n        Returns:\n            Dictionary of evaluation names and their values which includes the\n            metrics applied on respective series loss and loss values from the\n            run.\n        """"""\n        execution_results, output_data, f_dataset = self.run_model(\n            dataset, write_out, log_progress)\n\n        evaluators = [(e[0], e[0], e[1]) if len(e) == 2 else e\n                      for e in self.model.evaluation]\n        with self.graph.as_default():\n            eval_result = evaluation(\n                evaluators, f_dataset, execution_results, output_data)\n        if eval_result:\n            print_final_evaluation(eval_result, name)\n\n        return eval_result\n\n    def get_path(self, filename: str, cont_index: int = None) -> str:\n        """"""Return the path to the most recent version of the given file.""""""\n        if cont_index is None:\n            cont_index = self.cont_index\n        cont_suffix = "".cont-{}"".format(cont_index) if cont_index > 0 else """"\n\n        if filename.startswith(""variables.data""):\n            new_filename = ""variables.data"" + cont_suffix + filename[14:]\n        else:\n            new_filename = filename + cont_suffix\n\n        return os.path.join(self.config.args.output, new_filename)\n\n    def update_initializers(\n            self, initializers: Iterable[Tuple[str, Callable]]) -> None:\n        """"""Update the dictionary mapping variable names to initializers.""""""\n        self._initializers.update(initializers)\n\n    def get_initializer(self, var_name: str,\n                        default: Callable = None) -> Optional[Callable]:\n        """"""Return the initializer associated with the given variable name.\n\n        Calling the method marks the given initializer as used.\n        """"""\n        initializer = self._initializers.get(var_name, default)\n        if initializer is not default:\n            debug(""Using {} for variable {}"".format(initializer, var_name))\n        self._initialized_variables.add(var_name)\n        return initializer\n\n    def _check_unused_initializers(self) -> None:\n        unused_initializers = [name for name in self._initializers\n                               if name not in self._initialized_variables]\n        if unused_initializers:\n            raise CheckingException(\n                ""Initializers were specified for the following non-existent ""\n                ""variables: "" + "", "".join(unused_initializers))\n\n    def visualize_embeddings(self) -> None:\n        """"""Insert visualization of embeddings in TensorBoard.\n\n        Visualize the embeddings of `EmbeddedFactorSequence` objects specified\n        in the `main.visualize_embeddings` config attribute.\n        """"""\n        tb_projector = projector.ProjectorConfig()\n\n        for sequence in self.model.visualize_embeddings:\n            for i, (vocabulary, emb_matrix) in enumerate(\n                    zip(sequence.vocabularies, sequence.embedding_matrices)):\n\n                # TODO when vocabularies will have name parameter, change it\n                path = self.get_path(""seq.{}-{}.tsv"".format(sequence.name, i))\n                vocabulary.save_wordlist(path)\n\n                embedding = tb_projector.embeddings.add()\n                # pylint: disable=unsubscriptable-object\n                embedding.tensor_name = emb_matrix.name\n                embedding.metadata_path = path\n                # pylint: enable=unsubscriptable-object\n\n        summary_writer = tf.summary.FileWriter(self.model.output)\n        projector.visualize_embeddings(summary_writer, tb_projector)\n\n    @classmethod\n    def get_current(cls) -> ""Experiment"":\n        """"""Return the experiment that is currently being built.""""""\n        return cls._current_experiment or _DUMMY_EXPERIMENT\n\n\ndef create_config(train_mode: bool = True) -> Configuration:\n    config = Configuration()\n    config.add_argument(""tf_manager"", required=False, default=None)\n    config.add_argument(""batch_size"", required=False, default=None,\n                        cond=lambda x: x is None or x > 0)\n    config.add_argument(""output"")\n    config.add_argument(""postprocess"", required=False, default=None)\n    config.add_argument(""runners"")\n    config.add_argument(""random_seed"", required=False, default=2574600)\n\n    if train_mode:\n        config.add_argument(""epochs"", cond=lambda x: x >= 0)\n        config.add_argument(""trainer"")\n        config.add_argument(""train_dataset"")\n        config.add_argument(""val_dataset"", required=False, default=[])\n        config.add_argument(""evaluation"")\n        config.add_argument(""test_datasets"", required=False, default=[])\n        config.add_argument(""logging_period"", required=False, default=20)\n        config.add_argument(""validation_period"", required=False, default=500)\n        config.add_argument(""visualize_embeddings"", required=False,\n                            default=None)\n        config.add_argument(""val_preview_input_series"",\n                            required=False, default=None)\n        config.add_argument(""val_preview_output_series"",\n                            required=False, default=None)\n        config.add_argument(""val_preview_num_examples"",\n                            required=False, default=15)\n        config.add_argument(""train_start_offset"", required=False, default=0)\n        config.add_argument(""name"", required=False,\n                            default=""Neural Monkey Experiment"")\n        config.add_argument(""initial_variables"", required=False, default=None)\n        config.add_argument(""overwrite_output_dir"", required=False,\n                            default=False)\n    else:\n        config.add_argument(""evaluation"", required=False, default=None)\n        for argument in _TRAIN_ARGS:\n            config.ignore_argument(argument)\n\n    return config\n\n\nclass _DummyExperiment(Experiment):\n    """"""A dummy Experiment.\n\n    An instance of this class takes care of initializers when no other\n    experiment is the current experiment. This is needed when someone creates\n    a model part outside an experiment (e.g. in a unit test).\n    """"""\n\n    def __init__(self):\n        # pylint: disable=super-init-not-called\n        self._initializers = {}  # type: Dict[str, Callable]\n        self._initialized_variables = set()  # type: Set[str]\n        self._warned = False\n\n    def update_initializers(\n            self, initializers: Iterable[Tuple[str, Callable]]) -> None:\n        self._warn()\n        super().update_initializers(initializers)\n\n    def get_initializer(self, var_name: str,\n                        default: Callable = None) -> Optional[Callable]:\n        """"""Return the initializer associated with the given variable name.""""""\n        self._warn()\n        return super().get_initializer(var_name, default)\n\n    def _warn(self) -> None:\n        if not self._warned:\n            log(""Warning: Creating a model part outside of an experiment."",\n                color=""red"")\n            self._warned = True\n\n\n_DUMMY_EXPERIMENT = _DummyExperiment()\n\n\ndef save_git_info(git_commit_file: str, git_diff_file: str,\n                  branch: str = ""HEAD"", repo_dir: str = None) -> None:\n    if shutil.which(""git"") is not None:\n        if repo_dir is None:\n            # This points inside the neuralmonkey/ dir inside the repo, but\n            # it does not matter for git.\n            repo_dir = os.path.dirname(os.path.realpath(__file__))\n\n        with open(git_commit_file, ""wb"") as file:\n            subprocess.run([""git"", ""log"", ""-1"", ""--format=%H"", branch],\n                           cwd=repo_dir, stdout=file)\n\n        with open(git_diff_file, ""wb"") as file:\n            subprocess.run(\n                [""git"", ""--no-pager"", ""diff"", ""--color=always"", branch],\n                cwd=repo_dir, stdout=file\n            )\n    else:\n        warn(""No git executable found. Not storing git commit and diffs"")\n'"
neuralmonkey/functions.py,13,"b'""""""Collection of various functions and function wrappers.""""""\n\nfrom typing import Optional\n\nimport math\nimport tensorflow as tf\n\n\ndef inverse_sigmoid_decay(param, rate, min_value: float = 0.,\n                          max_value: float = 1.,\n                          name: Optional[str] = None,\n                          dtype=tf.float32) -> tf.Tensor:\n    """"""Compute an inverse sigmoid decay: k/(k+exp(x/k)).\n\n    The result will be scaled to the range (min_value, max_value).\n\n    Arguments:\n        param: The parameter x from the formula.\n        rate: Non-negative k from the formula.\n    """"""\n\n    with tf.name_scope(name, ""InverseSigmoidDecay"",\n                       [rate, param, min_value, max_value]) as s_name:\n        result = rate / (rate + tf.exp(param / rate))\n        result = result * (max_value - min_value) + min_value\n        result = tf.cast(result, dtype, name=s_name)\n\n    return result\n\n\ndef piecewise_function(param, values, changepoints, name=None,\n                       dtype=tf.float32):\n    """"""Compute a piecewise function.\n\n    Arguments:\n        param: The function parameter.\n        values: List of function values (numbers or tensors).\n        changepoints: Sorted list of points where the function changes from\n            one value to the next. Must be one item shorter than `values`.\n    """"""\n\n    if len(changepoints) != len(values) - 1:\n        raise ValueError(""changepoints has length {}, expected {} (values ""\n                         ""has length {})"".format(len(changepoints),\n                                                 len(values) - 1,\n                                                 len(values)))\n\n    with tf.name_scope(name, ""PiecewiseFunction"",\n                       [param, values, changepoints]) as s_name:\n        values = [tf.convert_to_tensor(y, dtype=dtype) for y in values]\n        # this is a trick to make each lambda return a different y:\n        lambdas = [lambda y=y: y for y in values]\n        predicates = [tf.less(param, x) for x in changepoints]\n        return tf.case(list(zip(predicates, lambdas[:-1])), lambdas[-1],\n                       name=s_name)\n\n\ndef noam_decay(learning_rate: float,\n               model_dimension: int,\n               warmup_steps: int) -> tf.Tensor:\n    """"""Return decay function as defined in Vaswani et al., 2017, Equation 3.\n\n    https://arxiv.org/abs/1706.03762\n\n    lrate = (d_model)^-0.5 * min(step_num^-0.5, step_num * warmup_steps^-1.5)\n\n    Arguments:\n        model_dimension: Size of the hidden states of decoder and encoder\n        warmup_steps: Number of warm-up steps\n    """"""\n    step = tf.to_float(tf.train.get_or_create_global_step())\n\n    inv_sq_dim = 1 / math.sqrt(model_dimension)\n    inv_sq3_warmup_steps = math.pow(warmup_steps, -1.5)\n\n    inv_sq_step = 1 / tf.sqrt(step)\n    warmup = step * inv_sq3_warmup_steps\n\n    return learning_rate * inv_sq_dim * tf.minimum(inv_sq_step, warmup)\n'"
neuralmonkey/learning_utils.py,6,"b'# pylint: disable=too-many-lines\n# TODO de-clutter this file!\n\nfrom argparse import Namespace\nimport time\n# pylint: disable=unused-import\nfrom typing import (Any, Callable, Dict, List, Tuple, Optional, Union,\n                    Iterable, Iterator, Set)\n# pylint: enable=unused-import\n\nimport numpy as np\nimport tensorflow as tf\nfrom termcolor import colored\n\nfrom neuralmonkey.logging import log, log_print, warn\nfrom neuralmonkey.dataset import Dataset\nfrom neuralmonkey.tf_manager import TensorFlowManager\nfrom neuralmonkey.runners.base_runner import (\n    BaseRunner, ExecutionResult, GraphExecutor, OutputSeries)\nfrom neuralmonkey.runners.dataset_runner import DatasetRunner\nfrom neuralmonkey.trainers.generic_trainer import GenericTrainer\nfrom neuralmonkey.trainers.multitask_trainer import MultitaskTrainer\nfrom neuralmonkey.trainers.delayed_update_trainer import DelayedUpdateTrainer\nfrom neuralmonkey.training_profiler import TrainingProfiler\n\n# pylint: disable=invalid-name\nEvaluation = Dict[str, float]\nSeriesName = str\nEvalConfiguration = List[Union[Tuple[SeriesName, Any],\n                               Tuple[SeriesName, SeriesName, Any]]]\nPostprocess = Optional[List[Tuple[SeriesName, Callable]]]\nTrainer = Union[GenericTrainer, MultitaskTrainer, DelayedUpdateTrainer]\n# pylint: enable=invalid-name\n\n\n# pylint: disable=too-many-nested-blocks,too-many-locals\n# pylint: disable=too-many-branches,too-many-statements,too-many-arguments\ndef training_loop(cfg: Namespace) -> None:\n    """"""Execute the training loop for given graph and data.\n\n    Arguments:\n        cfg: Experiment configuration namespace.\n    """"""\n    _check_series_collisions(cfg.runners, cfg.postprocess)\n    _log_model_variables(cfg.trainers)\n    _initialize_model(cfg.tf_manager, cfg.initial_variables,\n                      cfg.runners + cfg.trainers)\n\n    log(""Initializing TensorBoard summary writer."")\n    tb_writer = tf.summary.FileWriter(cfg.output,\n                                      cfg.tf_manager.sessions[0].graph)\n    log(""TensorBoard writer initialized."")\n\n    feedables = set.union(*[ex.feedables for ex in cfg.runners + cfg.trainers])\n\n    log(""Starting training"")\n    profiler = TrainingProfiler()\n    profiler.training_start()\n\n    step = 0\n    seen_instances = 0\n    last_seen_instances = 0\n    interrupt = None\n\n    try:\n        for epoch_n in range(1, cfg.epochs + 1):\n            train_batches = cfg.train_dataset.batches()\n\n            if epoch_n == 1 and cfg.train_start_offset:\n                if cfg.train_dataset.shuffled and not cfg.train_dataset.lazy:\n                    warn(""Not skipping training instances with shuffled ""\n                         ""non-lazy dataset"")\n                else:\n                    _skip_lines(cfg.train_start_offset, train_batches)\n\n            log_print("""")\n            log(""Epoch {} begins"".format(epoch_n), color=""red"")\n            profiler.epoch_start()\n\n            for batch_n, batch in enumerate(train_batches):\n                step += 1\n                seen_instances += len(batch)\n\n                if cfg.log_timer(step, profiler.last_log_time):\n                    trainer_result = cfg.tf_manager.execute(\n                        batch, feedables, cfg.trainers, train=True,\n                        summaries=True)\n                    train_results, train_outputs, f_batch = run_on_dataset(\n                        cfg.tf_manager, cfg.runners, cfg.dataset_runner, batch,\n                        cfg.postprocess, write_out=False)\n                    # ensure train outputs are iterable more than once\n                    train_outputs = {\n                        k: list(v) for k, v in train_outputs.items()}\n\n                    train_evaluation = evaluation(\n                        cfg.evaluation, f_batch, train_results, train_outputs)\n\n                    _log_continuous_evaluation(\n                        tb_writer, cfg.main_metric, train_evaluation,\n                        seen_instances, epoch_n, cfg.epochs, trainer_result,\n                        train=True)\n\n                    profiler.log_done()\n\n                else:\n                    cfg.tf_manager.execute(\n                        batch, feedables, cfg.trainers, train=True,\n                        summaries=False)\n\n                if cfg.val_timer(step, profiler.last_val_time):\n\n                    log_print("""")\n                    profiler.validation_start()\n\n                    val_examples = 0\n                    for val_id, valset in enumerate(cfg.val_datasets):\n                        val_examples += len(valset)\n\n                        val_results, val_outputs, f_valset = run_on_dataset(\n                            cfg.tf_manager, cfg.runners, cfg.dataset_runner,\n                            valset, cfg.postprocess, write_out=False)\n                        # ensure val outputs are iterable more than once\n                        val_outputs = {k: list(v)\n                                       for k, v in val_outputs.items()}\n                        val_evaluation = evaluation(\n                            cfg.evaluation, f_valset, val_results, val_outputs)\n\n                        valheader = (""Validation (epoch {}, batch number {}):""\n                                     .format(epoch_n, batch_n))\n                        log(valheader, color=""blue"")\n                        _print_examples(\n                            f_valset, val_outputs,\n                            cfg.val_preview_input_series,\n                            cfg.val_preview_output_series,\n                            cfg.val_preview_num_examples)\n                        log_print("""")\n                        log(valheader, color=""blue"")\n\n                        # The last validation set is selected to be the main\n                        if val_id == len(cfg.val_datasets) - 1:\n                            this_score = val_evaluation[cfg.main_metric]\n                            cfg.tf_manager.validation_hook(this_score, epoch_n,\n                                                           batch_n)\n\n                            if this_score == cfg.tf_manager.best_score:\n                                best_score_str = colored(\n                                    ""{:.4g}"".format(cfg.tf_manager.best_score),\n                                    attrs=[""bold""])\n\n                                # store also graph parts\n                                rnrs = cfg.runners + cfg.trainers\n                                # TODO: refactor trainers/runners so that they\n                                # have the same API predecessor\n                                parameterizeds = set.union(\n                                    *[rnr.parameterizeds\n                                      for rnr in rnrs])\n                                for coder in parameterizeds:\n                                    for session in cfg.tf_manager.sessions:\n                                        coder.save(session)\n                            else:\n                                best_score_str = ""{:.4g}"".format(\n                                    cfg.tf_manager.best_score)\n\n                            log(""best {} on validation: {} (in epoch {}, ""\n                                ""after batch number {})""\n                                .format(cfg.main_metric, best_score_str,\n                                        cfg.tf_manager.best_score_epoch,\n                                        cfg.tf_manager.best_score_batch),\n                                color=""blue"")\n\n                        v_name = ""val_{}"".format(val_id) if len(\n                            cfg.val_datasets) > 1 else None\n                        _log_continuous_evaluation(\n                            tb_writer, cfg.main_metric, val_evaluation,\n                            seen_instances, epoch_n, cfg.epochs, val_results,\n                            train=False, dataset_name=v_name)\n\n                    profiler.validation_done()\n                    profiler.log_after_validation(\n                        val_examples, seen_instances - last_seen_instances)\n                    last_seen_instances = seen_instances\n\n                    log_print("""")\n\n    except KeyboardInterrupt as ex:\n        interrupt = ex\n\n    log(""Training finished. Maximum {} on validation data: {:.4g}, epoch {}""\n        .format(cfg.main_metric, cfg.tf_manager.best_score,\n                cfg.tf_manager.best_score_epoch))\n\n    if interrupt is not None:\n        raise interrupt  # pylint: disable=raising-bad-type\n\n\ndef _log_model_variables(trainers: List[Trainer]) -> None:\n\n    var_list = list(set().union(*[t.var_list for t in trainers])) \\\n               # type: List[tf.Variable]\n\n    trainable_vars = tf.trainable_variables()\n    if not var_list:\n        var_list = trainable_vars\n\n    assert var_list is not None\n    fixed_vars = [var for var in trainable_vars if var not in var_list]\n\n    total_params = 0\n\n    logstr = ""The model has {} trainable variables{}:\\n\\n"".format(\n        len(trainable_vars),\n        "" ({} {})"".format(len(fixed_vars), colored(""fixed"", on_color=""on_red""))\n        if fixed_vars else """")\n\n    logstr += colored(\n        ""{: ^80}{: ^20}{: ^10}\\n"".format(""Variable name"", ""Shape"", ""Size""),\n        color=""yellow"", attrs=[""bold""])\n\n    for var in trainable_vars:\n\n        shape = var.get_shape().as_list()\n        params_in_var = int(np.prod(shape))\n        total_params += params_in_var\n\n        name = var.name\n        if var not in var_list:\n            name = colored(name, on_color=""on_red"")\n        # Pad and compensate for control characters:\n        name = name.ljust(80 + (len(name) - len(var.name)))\n        log_entry = ""{}{: <20}{: >10}"".format(name, str(shape), params_in_var)\n        logstr += ""\\n{}"".format(log_entry)\n\n    logstr += ""\\n""\n\n    log(logstr)\n    log(""Total number of all parameters: {}"".format(total_params))\n\n\ndef _initialize_model(tf_manager: TensorFlowManager,\n                      initial_variables: Optional[List[str]],\n                      executables: List[GraphExecutor]):\n\n    if initial_variables is None:\n        # Assume we don\'t look at coder checkpoints when global\n        # initial variables are supplied\n        tf_manager.initialize_model_parts(executables)\n    else:\n        try:\n            tf_manager.restore(initial_variables)\n        except tf.errors.NotFoundError:\n            warn(""Some variables were not found in checkpoint.)"")\n\n\ndef _check_series_collisions(runners: List[BaseRunner],\n                             postprocess: Postprocess) -> None:\n    """"""Check if output series names do not collide.""""""\n    runners_outputs = set()  # type: Set[str]\n    for runner in runners:\n        series = runner.output_series\n        if series in runners_outputs:\n            raise Exception((""Output series \'{}\' is multiple times among the ""\n                             ""runners\' outputs."").format(series))\n        runners_outputs.add(series)\n    if postprocess is not None:\n        for series, _ in postprocess:\n            if series in runners_outputs:\n                raise Exception((""Postprocess output series \'{}\' ""\n                                 ""already exists."").format(series))\n            runners_outputs.add(series)\n\n\ndef run_on_dataset(tf_manager: TensorFlowManager,\n                   runners: List[BaseRunner],\n                   dataset_runner: DatasetRunner,\n                   dataset: Dataset,\n                   postprocess: Postprocess,\n                   write_out: bool = False,\n                   log_progress: int = 0) -> Tuple[\n                       List[ExecutionResult],\n                       Dict[str, List],\n                       Dict[str, List]]:\n    """"""Apply the model on a dataset and optionally write outputs to files.\n\n    This function processes the dataset in batches and optionally prints out\n    the execution progress.\n\n    Args:\n        tf_manager: TensorFlow manager with initialized sessions.\n        runners: A function that runs the code\n        dataset_runner: A runner object that fetches the data inputs\n        dataset: The dataset on which the model will be executed.\n        evaluators: List of evaluators that are used for the model\n            evaluation if the target data are provided.\n        postprocess: Dataset-level postprocessors\n        write_out: Flag whether the outputs should be printed to a file defined\n            in the dataset object.\n        log_progress: log progress every X seconds\n\n        extra_fetches: Extra tensors to evaluate for each batch.\n\n    Returns:\n        Tuple of resulting sentences/numpy arrays, and evaluation results if\n        they are available which are dictionary function -> value.\n\n    """"""\n    # If the dataset contains the target series, compute also losses.\n    contains_targets = all(runner.decoder_data_id in dataset\n                           for runner in runners\n                           if runner.decoder_data_id is not None)\n\n    last_log_time = time.process_time()\n    batch_results = [[] for _ in runners]  # type: List[List[ExecutionResult]]\n    batch_results.append([])  # For dataset runner\n\n    feedables = set.union(*[runner.feedables for runner in runners])\n    feedables |= dataset_runner.feedables\n\n    fetched_input = {s: [] for s in dataset.series}  # type: Dict[str, List]\n\n    processed_examples = 0\n    for batch in dataset.batches():\n        if 0 < log_progress < time.process_time() - last_log_time:\n            log(""Processed {} examples."".format(processed_examples))\n            last_log_time = time.process_time()\n\n        executors = []  # type: List[GraphExecutor]\n        executors.extend(runners)\n        executors.append(dataset_runner)\n\n        execution_results = tf_manager.execute(\n            batch, feedables, executors, compute_losses=contains_targets)\n\n        processed_examples += len(batch)\n\n        for script_list, ex_result in zip(batch_results, execution_results):\n            script_list.append(ex_result)\n\n        for s_id in batch.series:\n            fetched_input[s_id].extend(batch.get_series(s_id))\n\n    # Transpose runner interim results.\n    all_results = [join_execution_results(res) for res in batch_results[:-1]]\n\n    # TODO uncomment this when dataset runner starts outputting the dataset\n    # input_transposed = join_execution_results(batch_results[-1]).outputs\n    # fetched_input = {\n    #     k: [dic[k] for dic in input_transposed] for k in input_transposed[0]}\n\n    fetched_input_lengths = {s: len(fetched_input[s]) for s in dataset.series}\n\n    if len(set(fetched_input_lengths.values())) != 1:\n        warn(""Fetched input dataset series are not of the same length: {}""\n             .format(str(fetched_input_lengths)))\n\n    dataset_len = fetched_input_lengths[dataset.series[0]]\n\n    # Convert execution results to dictionary.\n    result_data = {}  # type: Dict[str, Union[List, np.ndarray]]\n    for s_id, data in (\n            pair for res in all_results for pair in res.outputs.items()):\n        if s_id in result_data:\n            raise ValueError(""Overwriting output series forbidden."")\n        result_data[s_id] = data\n\n    # Run dataset-level postprocessing.\n    if postprocess is not None:\n        for series_name, postprocessor in postprocess:\n            postprocessed = postprocessor(fetched_input, result_data)\n            if not hasattr(postprocessed, ""__len__""):\n                postprocessed = list(postprocessed)\n\n            result_data[series_name] = postprocessed\n\n    # Check output series lengths.\n    for series_id, data in result_data.items():\n        if len(data) != dataset_len:\n            warn(""Output \'{}\' for dataset \'{}\' has length {}, but input ""\n                 ""dataset size is {}"".format(series_id, dataset.name,\n                                             len(data), dataset_len))\n\n    if write_out and dataset.outputs is not None:\n        for series_id, data in result_data.items():\n            if series_id in dataset.outputs:\n                path, writer = dataset.outputs[series_id]\n                writer(path, data)\n            else:\n                log(""There is no file for output series \'{}\' in dataset: \'{}\'""\n                    .format(series_id, dataset.name), color=""red"")\n    elif write_out:\n        log(""Dataset does not have any outputs, nothing to write out."",\n            color=""red"")\n\n    return all_results, result_data, fetched_input\n\n\ndef join_execution_results(\n        execution_results: List[ExecutionResult]) -> ExecutionResult:\n    """"""Aggregate batch of execution results from a single runner.""""""\n\n    losses_sum = {loss: 0. for loss in execution_results[0].losses}\n\n    def join(output_series: List[OutputSeries]) -> OutputSeries:\n        """"""Join a list of batches of results into a flat list of outputs.""""""\n        joined = []  # type: List[Any]\n\n        for item in output_series:\n            joined.extend(item)\n\n        # If the list is a list of np.arrays, concatenate the list along first\n        # dimension (batch). Otherwise, return the list.\n        if joined and isinstance(joined[0], np.ndarray):\n            return np.array(joined)\n\n        return joined\n\n    outputs = {}  # type: Dict[str, Any]\n    for key in execution_results[0].outputs.keys():\n        outputs[key] = join([res.outputs[key] for res in execution_results])\n\n    for result in execution_results:\n        for l_id, loss in result.losses.items():\n            losses_sum[l_id] += loss * result.size\n\n    total_size = sum(res.size for res in execution_results)\n    losses = {l_id: loss / total_size for l_id, loss in losses_sum.items()}\n\n    all_summaries = [\n        summ for res in execution_results if res.summaries is not None\n        for summ in res.summaries]\n\n    return ExecutionResult(outputs, losses, total_size, all_summaries)\n\n\ndef evaluation(evaluators, batch, execution_results, result_data):\n    """"""Evaluate the model outputs.\n\n    Args:\n        evaluators: List of tuples of series and evaluation functions.\n        batch: Batch of data against which the evaluation is done.\n        execution_results: Execution results that include the loss values.\n        result_data: Dictionary from series names to list of outputs.\n\n    Returns:\n        Dictionary of evaluation names and their values which includes the\n        metrics applied on respective series loss and loss values from the run.\n    """"""\n    eval_result = {}\n\n    # losses\n    for result in execution_results:\n        if any(l in eval_result for l in result.losses):\n            # TODO(tf-data) this will go away with further exec_res refactor\n            raise ValueError(""Duplicate loss result keys found."")\n\n        eval_result.update(result.losses)\n\n    # evaluation metrics\n    for hypothesis_id, reference_id, function in evaluators:\n        if reference_id not in batch or hypothesis_id not in result_data:\n            continue\n\n        desired_output = batch[reference_id]\n        model_output = result_data[hypothesis_id]\n        eval_result[""{}/{}"".format(hypothesis_id, function.name)] = function(\n            model_output, desired_output)\n\n    return eval_result\n\n\ndef _log_continuous_evaluation(tb_writer: tf.summary.FileWriter,\n                               main_metric: str,\n                               eval_result: Evaluation,\n                               seen_instances: int,\n                               epoch: int,\n                               max_epochs: int,\n                               execution_results: List[ExecutionResult],\n                               train: bool = False,\n                               dataset_name: str = None) -> None:\n    """"""Log the evaluation results and the TensorBoard summaries.""""""\n\n    color, prefix = (""yellow"", ""train"") if train else (""blue"", ""val"")\n\n    if dataset_name is not None:\n        prefix += ""_"" + dataset_name\n\n    eval_string = _format_evaluation_line(eval_result, main_metric)\n    eval_string = ""Epoch {}/{}  Instances {}  {}"".format(epoch, max_epochs,\n                                                         seen_instances,\n                                                         eval_string)\n    log(eval_string, color=color)\n\n    if tb_writer:\n        for result in execution_results:\n            for summaries in result.summaries:\n                tb_writer.add_summary(summaries, seen_instances)\n\n        external_str = \\\n            tf.Summary(value=[tf.Summary.Value(tag=prefix + ""_"" + name,\n                                               simple_value=value)\n                              for name, value in eval_result.items()])\n        tb_writer.add_summary(external_str, seen_instances)\n\n\ndef _format_evaluation_line(evaluation_res: Evaluation,\n                            main_metric: str) -> str:\n    """"""Format the evaluation metric for stdout with last one bold.""""""\n    eval_string = ""    "".join(""{}: {:.4g}"".format(name, value)\n                              for name, value in evaluation_res.items()\n                              if name != main_metric)\n\n    eval_string += colored(\n        ""    {}: {:.4g}"".format(main_metric,\n                                evaluation_res[main_metric]),\n        attrs=[""bold""])\n\n    return eval_string\n\n\ndef print_final_evaluation(eval_result: Evaluation, name: str = None) -> None:\n    """"""Print final evaluation from a test dataset.""""""\n    line_len = 22\n\n    if name is not None:\n        log(""Model evaluated on \'{}\'"".format(name))\n\n    for eval_name, value in eval_result.items():\n        space = """".join(["" "" for _ in range(line_len - len(eval_name))])\n        log(""... {}:{} {:.4g}"".format(eval_name, space, value))\n\n    log_print("""")\n\n\ndef _data_item_to_str(item: Any) -> str:\n    if isinstance(item, list):\n        return "" "".join([_data_item_to_str(i) for i in item])\n\n    if isinstance(item, dict):\n        return ""{\\n      "" + ""\\n      "".join(\n            [""{}: {}"".format(_data_item_to_str(key), _data_item_to_str(val))\n             for key, val in item.items()]) + ""\\n    }""\n\n    if isinstance(item, np.ndarray) and len(item.shape) > 1:\n        return ""[numpy tensor, shape {}]"".format(item.shape)\n\n    return str(item)\n\n\ndef _print_examples(dataset: Dict[str, List[Any]],\n                    outputs: Dict[str, List[Any]],\n                    val_preview_input_series: Optional[List[str]] = None,\n                    val_preview_output_series: Optional[List[str]] = None,\n                    num_examples=15) -> None:\n    """"""Print examples of the model output.\n\n    Arguments:\n        dataset: The dataset from which to take examples\n        outputs: A mapping from the output series ID to the list of its\n            contents\n        val_preview_input_series: An optional list of input series to include\n            in the preview. An input series is a data series that is present in\n            the dataset. It can be either a target series (one that is also\n            present in the outputs, i.e. reference), or a source series (one\n            that is not among the outputs). In the validation preview, source\n            input series and preprocessed target series are yellow and target\n            (reference) series are red. If None, all series are written.\n        val_preview_output_series: An optional list of output series to include\n            in the preview. An output series is a data series that is present\n            among the outputs. In the preview, magenta is used as the font\n            color for output series\n    """"""\n    log_print(colored(""Examples:"", attrs=[""bold""]))\n\n    source_series_names = [s for s in dataset if s not in outputs]\n    target_series_names = [s for s in dataset if s in outputs]\n    output_series_names = list(outputs.keys())\n\n    assert outputs\n\n    if val_preview_input_series is not None:\n        target_series_names = [s for s in target_series_names\n                               if s in val_preview_input_series]\n        source_series_names = [s for s in source_series_names\n                               if s in val_preview_input_series]\n\n    if val_preview_output_series is not None:\n        output_series_names = [s for s in output_series_names\n                               if s in val_preview_output_series]\n\n    # for further indexing we need to make sure, all relevant\n    # dataset series are lists\n    target_series = {series_id: list(dataset[series_id])\n                     for series_id in target_series_names}\n    source_series = {series_id: list(dataset[series_id])\n                     for series_id in source_series_names}\n\n    dataset_length = len(next(iter(dataset.values())))\n    num_examples = min(dataset_length, num_examples)\n\n    for i in range(num_examples):\n        log_print(colored(""  [{}]"".format(i + 1), color=""magenta"",\n                          attrs=[""bold""]))\n\n        def print_line(prefix, color, content):\n            colored_prefix = colored(prefix, color=color)\n            formatted = _data_item_to_str(content)\n            log_print(""  {}: {}"".format(colored_prefix, formatted))\n\n        # Input source series = yellow\n        for series_id, data in sorted(source_series.items(),\n                                      key=lambda x: x[0]):\n            print_line(series_id, ""yellow"", data[i])\n\n        # Output series = magenta\n        for series_id in sorted(output_series_names):\n            data = list(outputs[series_id])\n            model_output = data[i]\n            print_line(series_id, ""magenta"", model_output)\n\n        # Input target series (a.k.a. references) = red\n        for series_id in sorted(target_series_names):\n            data = outputs[series_id]\n            desired_output = target_series[series_id][i]\n            print_line(series_id + "" (ref)"", ""red"", desired_output)\n\n        log_print("""")\n\n\ndef _skip_lines(start_offset: int,\n                batches: Iterator[Dataset]) -> None:\n    """"""Skip training instances from the beginning.\n\n    Arguments:\n        start_offset: How many training instances to skip (minimum)\n        batches: Iterator over batches to skip\n    """"""\n    log(""Skipping first {} instances in the dataset"".format(start_offset))\n\n    skipped_instances = 0\n    while skipped_instances < start_offset:\n        try:\n            skipped_instances += len(next(batches))  # type: ignore\n        except StopIteration:\n            raise ValueError(""Trying to skip more instances than ""\n                             ""the size of the dataset"")\n\n    if skipped_instances > 0:\n        log(""Skipped {} instances"".format(skipped_instances))\n'"
neuralmonkey/logging.py,0,"b'import time\nimport sys\nimport os\n\n# pylint: disable=unused-import\nfrom typing import Any, List\n# pylint: enable=unused-import\n\nfrom termcolor import colored\n\n\nclass Logging:\n\n    log_file = None  # type: Any\n\n    # \'all\' and \'none\' are special symbols,\n    # others are filtered according the labels\n    debug_enabled_for = [\n        os.environ.get(""NEURALMONKEY_DEBUG_ENABLE"", ""none"")]  # type: List[str]\n    debug_disabled_for = [\n        os.environ.get(""NEURALMONKEY_DEBUG_DISABLE"", """")]  # type: List[str]\n    strict_mode = os.environ.get(""NEURALMONKEY_STRICT"", """")  # type: str\n\n    @staticmethod\n    def _get_time() -> str:\n        return time.strftime(""%Y-%m-%d %H:%M:%S"")\n\n    @staticmethod\n    def set_log_file(path: str) -> None:\n        """"""Set up the file where the logging will be done.""""""\n        if Logging.log_file is not None and not Logging.log_file.closed:\n            Logging.log_file.close()\n        Logging.log_file = open(path, ""w"", encoding=""utf-8"", buffering=1)\n\n    @staticmethod\n    def log_print(text: str) -> None:\n        """"""Print a string both to console and a log file is it is defined.""""""\n        if Logging.log_file is not None:\n            if not isinstance(text, str):\n                text = str(text)\n            Logging.log_file.write(text + ""\\n"")\n            Logging.log_file.flush()\n\n        print(text, file=sys.stderr)\n\n    @staticmethod\n    def log(message: str, color: str = ""yellow"") -> None:\n        """"""Log a message with a colored timestamp.""""""\n        log_print(""{}: {}"".format(colored(\n            Logging._get_time(), color), message))\n\n    @staticmethod\n    def notice(message: str) -> None:\n        """"""Log a notice with a colored timestamp.""""""\n        log_print(""{}: {}"".format(colored(\n            Logging._get_time(), ""red""), message))\n\n    @staticmethod\n    def warn(message: str) -> None:\n        """"""Log a warning.""""""\n        log_print(colored(""{}: Warning! {}"".format(\n            Logging._get_time(), message), color=""red""))\n        if Logging.strict_mode:\n            raise Exception(\n                ""Encountered a warning in strict mode: "" + message)\n\n    @staticmethod\n    def print_header(title: str, path: str) -> None:\n        """"""Print the title of the experiment and a set of arguments it uses.""""""\n        log_print(colored("""".join(""="" for _ in range(80)), ""green""))\n        log_print(colored(title.upper(), ""green""))\n        log_print(colored("""".join(""="" for _ in range(80)), ""green""))\n        log_print(""Launched at {}"".format(Logging._get_time()))\n        log_print(""Experiment directory: {}"".format(path))\n\n        log_print("""")\n\n    @staticmethod\n    def debug(message: str, label: str = None):\n        if not debug_enabled(label):\n            return\n\n        if label:\n            prefix = ""{}: DEBUG ({}): "".format(Logging._get_time(), label)\n        else:\n            prefix = ""{}: DEBUG: "".format(Logging._get_time())\n\n        log_print(""{}{}"".format(colored(prefix, color=""cyan""), message))\n\n    @staticmethod\n    def debug_enabled(label: str = None):\n        if ""none"" in Logging.debug_enabled_for:\n            return False\n\n        if label is None:\n            return True\n\n        if (label in Logging.debug_disabled_for\n                or (""all"" not in Logging.debug_enabled_for\n                    and label not in Logging.debug_enabled_for)):\n            return False\n\n        return True\n\n\n# pylint: disable=invalid-name\n# we want these helper functions to have this exact name\nlog = Logging.log\nlog_print = Logging.log_print\ndebug = Logging.debug\nwarn = Logging.warn\nnotice = Logging.notice\ndebug_enabled = Logging.debug_enabled\n'"
neuralmonkey/run.py,0,"b'# pylint: disable=unused-import, wrong-import-order\nimport neuralmonkey.checkpython\n# pylint: enable=unused-import, wrong-import-order\n\nimport argparse\nimport json\nimport os\n\nfrom neuralmonkey.config.configuration import Configuration\nfrom neuralmonkey.experiment import Experiment\nfrom neuralmonkey.logging import log\n\n\ndef load_runtime_config(config_path: str) -> argparse.Namespace:\n    """"""Load a runtime configuration file.""""""\n    cfg = Configuration()\n    cfg.add_argument(""test_datasets"")\n    cfg.add_argument(""variables"", cond=lambda x: isinstance(x, list))\n\n    cfg.load_file(config_path)\n    cfg.build_model()\n    return cfg.model\n\n\ndef main() -> None:\n    # pylint: disable=no-member\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(""config"", metavar=""INI-FILE"",\n                        help=""the configuration file of the experiment"")\n    parser.add_argument(""datasets"", metavar=""INI-TEST-DATASETS"",\n                        help=""the configuration of the test datasets"")\n    parser.add_argument(""-s"", ""--set"", type=str, metavar=""SETTING"",\n                        action=""append"", dest=""config_changes"", default=[],\n                        help=""override an option in the configuration; the ""\n                        ""syntax is [section.]option=value"")\n    parser.add_argument(""-v"", ""--var"", type=str, metavar=""VAR"", default=[],\n                        action=""append"", dest=""config_vars"",\n                        help=""set a variable in the configuration; the syntax ""\n                        ""is var=value (shorthand for -s vars.var=value)"")\n    parser.add_argument(""--json"", type=str, help=""write the evaluation ""\n                        ""results to this file in JSON format"")\n    parser.add_argument(""-g"", ""--grid"", dest=""grid"", action=""store_true"",\n                        help=""look at the SGE variables for slicing the data"")\n    args = parser.parse_args()\n\n    datasets_model = load_runtime_config(args.datasets)\n\n    args.config_changes.extend(""vars.{}"".format(s) for s in args.config_vars)\n    exp = Experiment(config_path=args.config,\n                     config_changes=args.config_changes)\n\n    exp.build_model()\n    exp.load_variables(datasets_model.variables)\n\n    if args.grid and len(datasets_model.test_datasets) > 1:\n        raise ValueError(""Only one test dataset supported when using --grid"")\n\n    results = []\n    for dataset in datasets_model.test_datasets:\n        if args.grid:\n            if (""SGE_TASK_FIRST"" not in os.environ\n                    or ""SGE_TASK_LAST"" not in os.environ\n                    or ""SGE_TASK_STEPSIZE"" not in os.environ\n                    or ""SGE_TASK_ID"" not in os.environ):\n                raise EnvironmentError(\n                    ""Some SGE environment variables are missing"")\n\n            length = int(os.environ[""SGE_TASK_STEPSIZE""])\n            start = int(os.environ[""SGE_TASK_ID""]) - 1\n            end = int(os.environ[""SGE_TASK_LAST""]) - 1\n\n            if start + length > end:\n                length = end - start + 1\n\n            log(""Running grid task {} starting at {} with step {}""\n                .format(start // length, start, length))\n\n            dataset = dataset.subset(start, length)\n\n        if exp.config.args.evaluation is None:\n            exp.run_model(dataset, write_out=True)\n        else:\n            eval_result = exp.evaluate(dataset, write_out=True)\n            results.append(eval_result)\n\n    if args.json:\n        with open(args.json, ""w"") as f_out:\n            json.dump(results, f_out)\n            f_out.write(""\\n"")\n\n    for session in exp.config.model.tf_manager.sessions:\n        session.close()\n'"
neuralmonkey/tf_manager.py,7,"b'""""""TensorFlow Manager.\n\nTensorFlow manager is a helper object in Neural Monkey which manages TensorFlow\nsessions, execution of the computation graph, and saving and restoring of model\nvariables.\n\n""""""\n# pylint: disable=unused-import\nfrom typing import Any, List, Union, Optional, Set, Sequence\n# pylint: enable=unused-import\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n# pylint: disable=no-name-in-module\nfrom tensorflow.python import debug as tf_debug\n# pylint: enable=no-name-in-module\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.logging import log\nfrom neuralmonkey.dataset import Dataset\nfrom neuralmonkey.model.feedable import Feedable\nfrom neuralmonkey.runners.base_runner import (\n    FeedDict, ExecutionResult, GraphExecutor)\n\n\n# pylint: disable=too-many-instance-attributes\nclass TensorFlowManager:\n    """"""Inteface between computational graph, data and TF sessions.\n\n    Attributes:\n        sessions: List of active Tensorflow sessions.\n    """"""\n\n    # pylint: disable=too-many-arguments\n    def __init__(self,\n                 num_sessions: int,\n                 num_threads: int,\n                 save_n_best: int = 1,\n                 minimize_metric: bool = False,\n                 gpu_allow_growth: bool = True,\n                 per_process_gpu_memory_fraction: float = 1.0,\n                 enable_tf_debug: bool = False) -> None:\n        """"""Initialize a TensorflowManager.\n\n        At this moment the graph must already exist. This method initializes\n        required number of TensorFlow sessions and initializes them with\n        provided variable files if they are provided.\n\n        Args:\n            num_sessions: Number of sessions to be initialized.\n            num_threads: Number of threads sessions will run in.\n            save_n_best: How many best models to keep\n            minimize_metric: Whether the best model is the one with the lowest\n                or the highest score\n            gpu_allow_growth: TF to allocate incrementally, not all at once.\n            per_process_gpu_memory_fraction: Limit TF memory use.\n        """"""\n        check_argument_types()\n\n        self.session_cfg = tf.ConfigProto()\n        self.session_cfg.inter_op_parallelism_threads = num_threads\n        self.session_cfg.intra_op_parallelism_threads = num_threads\n        self.session_cfg.allow_soft_placement = True  # needed for more GPUs\n        # pylint: disable=no-member\n        self.session_cfg.gpu_options.allow_growth = gpu_allow_growth\n        self.session_cfg.gpu_options.per_process_gpu_memory_fraction = \\\n            per_process_gpu_memory_fraction\n        # pylint: enable=no-member\n\n        if save_n_best < 1:\n            raise Exception(""save_n_best parameter must be greater than zero"")\n        self.saver_max_to_keep = save_n_best\n        self.minimize_metric = minimize_metric\n        self.num_sessions = num_sessions\n\n        self.sessions = [tf.Session(config=self.session_cfg)\n                         for _ in range(self.num_sessions)]\n\n        if enable_tf_debug:\n            self.sessions = [tf_debug.LocalCLIDebugWrapperSession(sess)\n                             for sess in self.sessions]\n\n        self.saver = None\n\n        self.best_score_index = None  # type: Optional[int]\n        self.best_score_epoch = 0\n        self.best_score_batch = 0\n\n        init_score = np.inf if self.minimize_metric else -np.inf\n        self.saved_scores = [init_score for _ in range(self.saver_max_to_keep)]\n        self.best_score = init_score\n\n        self.variables_files = []  # type: List[str]\n        self._best_vars_file = None  # type: Optional[str]\n    # pylint: enable=too-many-arguments\n\n    @property\n    def best_vars_file(self) -> str:\n        if self._best_vars_file is None:\n            raise RuntimeError(""Saving not initialized yet."")\n\n        return self._best_vars_file\n\n    def _is_better(self, score1: float, score2: float) -> bool:\n        if self.minimize_metric:\n            return score1 < score2\n\n        return score1 > score2\n\n    def _argworst(self, scores: List[float]) -> int:\n        if self.minimize_metric:\n            return np.argmax(scores)\n\n        return np.argmin(scores)\n\n    def _update_best_vars(self, var_index: int) -> None:\n        best_vars_prefix = os.path.basename(self.variables_files[var_index])\n\n        with open(self.best_vars_file, ""w"") as var_file:\n            var_file.write(best_vars_prefix)\n\n    def init_saving(self, vars_prefix: str) -> None:\n        if self.saver_max_to_keep == 1:\n            self.variables_files = [vars_prefix]\n        else:\n            self.variables_files = [""{}.{}"".format(vars_prefix, i)\n                                    for i in range(self.saver_max_to_keep)]\n\n        self._best_vars_file = ""{}.best"".format(vars_prefix)\n\n    def validation_hook(self, score: float, epoch: int, batch: int) -> None:\n        if self._is_better(score, self.best_score):\n            self.best_score = score\n            self.best_score_epoch = epoch\n            self.best_score_batch = batch\n\n        worst_index = self._argworst(self.saved_scores)\n        worst_score = self.saved_scores[worst_index]\n\n        if self._is_better(score, worst_score):\n            # we need to save this score instead the worst score\n            worst_var_file = self.variables_files[worst_index]\n            self.save(worst_var_file)\n            self.saved_scores[worst_index] = score\n            log(""Variable file saved in {}"".format(worst_var_file))\n\n            # update symlink and best score index\n            if self.best_score == score:\n                self._update_best_vars(worst_index)\n                self.best_score_index = worst_index\n\n            log(""Best scores saved so far: {}"".format(\n                self.saved_scores))\n\n    # pylint: disable=too-many-locals\n    def _run_executables(self,\n                         feed_dict: FeedDict,\n                         executables: List[GraphExecutor.Executable]) -> None:\n        all_fetches = {}\n\n        # We might want to feed different values to each session\n        # E.g. when executing only step at a time during ensembling\n        feed_dicts = [{} for _ in range(len(self.sessions))] \\\n            # type: List[FeedDict]\n\n        for executable in (ex for ex in executables if ex.result is None):\n            fetches, add_feed_dicts = executable.next_to_execute()\n            all_fetches[executable] = fetches\n\n            if add_feed_dicts:\n                for fdict, add_fd in zip(feed_dicts, add_feed_dicts):\n                    fdict.update(add_fd)\n\n        for fdict in feed_dicts:\n            fdict.update(feed_dict)\n\n        session_results = [sess.run(all_fetches, feed_dict=fd)\n                           for sess, fd in zip(self.sessions, feed_dicts)]\n\n        for executable in executables:\n            if executable.result is None:\n                executable.collect_results(\n                    [res[executable] for res in session_results])\n\n    # pylint: disable=too-many-locals\n    def execute(self,\n                batch: Dataset,\n                feedables: Set[Feedable],\n                runners: Sequence[GraphExecutor],\n                train: bool = False,\n                compute_losses: bool = True,\n                summaries: bool = True) -> List[ExecutionResult]:\n        """"""Execute runners on a batch of data.\n\n        First, extract executables from the provided runners, telling the\n        runners whether to compute also losses and summaries. Second, until\n        all executables are satisfied (have the `result` attribute set),\n        run the executables on the batch.\n\n        Arguments:\n            batch: A batch of data.\n            execution_scripts: List of runners to execute.\n            train: Training mode flag (this value is fed to the `train_mode`\n                 placeholders in model parts).\n            compute_losses: Flag to runners whether run loss operations.\n            summaries: Flag to runners whether to run summary operations.\n\n        Returns:\n            A list of `ExecutionResult` tuples, one for each executable\n            (runner).\n        """"""\n        default_feed_dict = _feed_dicts(batch, feedables, train=train)\n\n        executables = [runner.get_executable(compute_losses=compute_losses,\n                                             summaries=summaries,\n                                             num_sessions=len(self.sessions))\n                       for runner in runners]\n\n        # TODO refactor runner results to properties\n        while not all(getattr(ex, ""result"") is not None for ex in executables):\n            self._run_executables(default_feed_dict, executables)\n\n        return [getattr(ex, ""result"") for ex in executables]\n\n    def save(self, variable_files: Union[str, List[str]]) -> None:\n        if self.saver is None:\n            raise RuntimeError(""Saver uninitialized"")\n\n        if isinstance(variable_files, str) and len(self.sessions) == 1:\n            self.saver.save(self.sessions[0], variable_files)\n            return\n\n        if isinstance(variable_files, str):\n            variable_files = [""{}.{}"".format(\n                variable_files, i) for i in range(len(self.sessions))]\n\n        if len(variable_files) != len(self.sessions):\n            raise Exception(\n                ""Provided {} files for saving {} sessions."".format(\n                    len(variable_files), len(self.sessions)))\n\n        for sess, file_name in zip(self.sessions, variable_files):\n            self.saver.save(sess, file_name)\n\n    def restore(self, variable_files: Union[str, List[str]]) -> None:\n        if self.saver is None:\n            raise RuntimeError(""Saver uninitialized"")\n\n        if isinstance(variable_files, str):\n            variable_files = [variable_files]\n        if len(variable_files) != len(self.sessions):\n            raise Exception(\n                ""Provided {} files for restoring {} sessions."".format(\n                    len(variable_files), len(self.sessions)))\n\n        for sess, file_name in zip(self.sessions, variable_files):\n            log(""Loading variables from {}"".format(file_name))\n            self.saver.restore(sess, file_name)\n            log(""Variables loaded from {}"".format(file_name))\n\n    def restore_best_vars(self) -> None:\n        assert self.best_score_index is not None\n        self.restore(self.variables_files[self.best_score_index])\n\n    def initialize_sessions(self) -> None:\n        log(""Initializing variables"")\n        init_op = tf.global_variables_initializer()\n        init_tables = tf.tables_initializer()\n        for sess in self.sessions:\n            sess.run([init_op, init_tables])\n\n        log(""Initializing tf.train.Saver"")\n        self.saver = tf.train.Saver(max_to_keep=None,\n                                    var_list=[g for g in tf.global_variables()\n                                              if ""reward_"" not in g.name])\n\n    def initialize_model_parts(self, runners: Sequence[GraphExecutor]) -> None:\n        """"""Initialize model parts variables from their checkpoints.""""""\n        if any(not hasattr(r, ""parameterizeds"") for r in runners):\n            raise TypeError(\n                ""Args to initialize_model_parts must be trainers or runners"")\n\n        parameterizeds = set.union(*[rnr.parameterizeds for rnr in runners])\n        for coder in parameterizeds:\n            for session in self.sessions:\n                coder.load(session)\n\n\ndef _feed_dicts(dataset: Dataset, coders: Set[Feedable], train: bool = False):\n    """"""Feed the coders with data from dataset.\n\n    This function ensures all encoder and decoder objects feed their the data\n    they need from the dataset.\n    """"""\n    res = {}\n\n    for coder in coders:\n        res.update(coder.feed_dict(dataset, train=train))\n\n    return res\n\n\ndef get_default_tf_manager() -> TensorFlowManager:\n    return TensorFlowManager(num_sessions=1, num_threads=4)\n'"
neuralmonkey/tf_utils.py,41,"b'""""""A set of helper functions for TensorFlow.""""""\nfrom typing import Callable, Iterable, List, Optional, Tuple, Union\nimport numpy as np\nimport tensorflow as tf\n\nfrom neuralmonkey.logging import debug, debug_enabled\n\n# pylint: disable=invalid-name\nShapeSpec = List[int]\n# pylint: enable=invalid-name\n\n\ndef _get_current_experiment():\n    # This is needed to avoid circular imports.\n    from neuralmonkey.experiment import Experiment\n    return Experiment.get_current()\n\n\ndef update_initializers(initializers: Iterable[Tuple[str, Callable]]) -> None:\n    _get_current_experiment().update_initializers(initializers)\n\n\ndef get_initializer(var_name: str,\n                    default: Callable = None) -> Optional[Callable]:\n    """"""Return the initializer associated with the given variable name.\n\n    The name of the current variable scope is prepended to the variable name.\n\n    This should only be called during model building.\n    """"""\n    full_name = tf.get_variable_scope().name + ""/"" + var_name\n    return _get_current_experiment().get_initializer(full_name, default)\n\n\ndef get_variable(name: str,\n                 shape: ShapeSpec = None,\n                 dtype: tf.DType = None,\n                 initializer: Callable = None,\n                 **kwargs) -> tf.Variable:\n    """"""Get an existing variable with these parameters or create a new one.\n\n    This is a wrapper around `tf.get_variable`. The `initializer` parameter is\n    treated as a default which can be overriden by a call to\n    `update_initializers`.\n\n    This should only be called during model building.\n    """"""\n    return tf.get_variable(\n        name=name, shape=shape, dtype=dtype,\n        initializer=get_initializer(name, initializer),\n        **kwargs)\n\n\ndef get_shape_list(x: tf.Tensor) -> List[Union[int, tf.Tensor]]:\n    """"""Return list of dims, statically where possible.\n\n    Compute the static shape of a tensor. Where the dimension is not static\n    (e.g. batch or time dimension), symbolic Tensor is returned.\n\n    Based on tensor2tensor.\n\n    Arguments:\n        x: The ``Tensor`` to process.\n\n    Returns:\n        A list of integers and Tensors.\n    """"""\n    x = tf.convert_to_tensor(x)\n\n    # If unknown rank, return dynamic shape\n    if x.get_shape().dims is None:\n        return tf.shape(x)\n\n    static = x.get_shape().as_list()\n    shape = tf.shape(x)\n\n    ret = []\n    for i, dim in enumerate(static):\n        if dim is None:\n            dim = shape[i]\n        ret.append(dim)\n    return ret\n\n\ndef get_state_shape_invariants(state: tf.Tensor) -> tf.TensorShape:\n    """"""Return the shape invariant of a tensor.\n\n    This function computes the loosened shape invariant of a state tensor.\n    Only invariant dimension is the state size dimension, which is the last.\n\n    Based on tensor2tensor.\n\n    Arguments:\n        state: The state tensor.\n\n    Returns:\n        A ``TensorShape`` object with all but the last dimensions set to\n        ``None``.\n    """"""\n    shape = state.shape.as_list()\n    for i in range(0, len(shape) - 1):\n        shape[i] = None\n    return tf.TensorShape(shape)\n\n\ndef gather_flat(x: tf.Tensor,\n                indices: tf.Tensor,\n                batch_size: Union[int, tf.Tensor] = 1,\n                beam_size: Union[int, tf.Tensor] = 1) -> tf.Tensor:\n    """"""Gather values from the flattened (shape=[batch * beam, ...]) input.\n\n    This function expects a flattened tensor with first dimension of size\n    *batch x beam* elements. Using the given batch and beam size, it reshapes\n    the input tensor to a tensor of shape ``(batch, beam, ...)`` and gather\n    the values from it using the index tensor.\n\n    Arguments:\n        x: A flattened ``Tensor`` from which to gather values.\n        indices: Index tensor.\n        batch_size: The size of the batch.\n        beam_size: The size of the beam.\n\n    Returns:\n        The ``Tensor`` of gathered values.\n    """"""\n    if x.shape.ndims == 0:\n        return x\n\n    shape = [batch_size, beam_size] + get_shape_list(x)[1:]\n    gathered = tf.gather_nd(tf.reshape(x, shape), indices)\n    return tf.reshape(gathered, [-1] + shape[2:])\n\n\ndef partial_transpose(x: tf.Tensor, indices: List[int]) -> tf.Tensor:\n    """"""Do a transpose on a subset of tensor dimensions.\n\n    Compute a permutation of first k dimensions of a tensor.\n\n    Arguments:\n        x: The ``Tensor`` to transpose.\n        indices: The permutation of the first k dimensions of ``x``.\n\n    Returns:\n        The transposed tensor.\n    """"""\n    dims = x.shape.ndims\n    orig_indices = list(range(dims))\n\n    return tf.transpose(x, indices + orig_indices[len(indices):])\n\n\ndef tf_print(tensor: tf.Tensor,\n             message: str = None,\n             debug_label: str = None) -> tf.Tensor:\n    """"""Print the value of a tensor to the debug log.\n\n    Better than tf.Print, logs to console only when the ""tensorval"" debug\n    subject is turned on.\n\n    Idea found at: https://stackoverflow.com/a/39649614\n\n    Args:\n        tensor: The tensor whose value to print\n\n    Returns:\n        As tf.Print, this function returns a tensor identical to the input\n        tensor, with the printing side-effect added.\n    """"""\n    def print_tensor(x: np.ndarray) -> tf.Tensor:\n        if message is not None:\n            debug(\n                ""{}, shape: {}:\\n{}"".format(message, x.shape, x), debug_label)\n        else:\n            debug(""Shape: {}\\n{}"".format(x.shape, x), debug_label)\n        return x\n\n    # To save time, check if debug will print something\n    if not debug_enabled(debug_label):\n        return tensor\n\n    log_op = tf.py_func(print_tensor, [tensor], [tensor.dtype])[0]\n\n    with tf.control_dependencies([log_op]):\n        res = tf.identity(tensor)\n\n    return res\n\n\ndef layer_norm(x: tf.Tensor, epsilon: float = 1e-6) -> tf.Tensor:\n    """"""Layer normalize the tensor x, averaging over the last dimension.\n\n    Implementation based on tensor2tensor.\n\n    Arguments:\n        x: The ``Tensor`` to normalize.\n        epsilon: The smoothing parameter of the normalization.\n\n    Returns:\n        The normalized tensor.\n    """"""\n    with tf.variable_scope(""LayerNorm""):\n        gamma = get_variable(\n            name=""gamma"",\n            shape=[x.get_shape()[-1]],\n            dtype=tf.float32,\n            initializer=tf.ones_initializer())\n        beta = get_variable(\n            name=""beta"",\n            shape=[x.get_shape()[-1]],\n            dtype=tf.float32,\n            initializer=tf.zeros_initializer())\n\n        mean = tf.reduce_mean(x, axis=[-1], keepdims=True)\n        variance = tf.reduce_mean(\n            tf.square(x - mean),\n            axis=[-1],\n            keepdims=True)\n        norm_x = (x - mean) * tf.rsqrt(variance + epsilon)\n        return norm_x * gamma + beta\n\n\ndef append_tensor(tensor: tf.Tensor,\n                  appendval: tf.Tensor,\n                  axis: int = 0) -> tf.Tensor:\n    """"""Append an ``N``-D Tensor to an ``(N+1)``-D Tensor.\n\n    Arguments:\n        tensor: The original Tensor\n        appendval: The Tensor to add\n        axis: Which axis should we use\n\n    Returns:\n        An ``(N+1)``-D Tensor with ``appendval`` on the last position.\n    """"""\n    return tf.concat([tensor, tf.expand_dims(appendval, axis)], axis)\n'"
neuralmonkey/train.py,0,"b'""""""Training script for sequence to sequence learning.""""""\n\n# pylint: disable=unused-import, wrong-import-order\nimport neuralmonkey.checkpython\n# pylint: enable=unused-import, wrong-import-order\n\nimport argparse\nimport os\nimport shlex\nfrom shutil import copyfile\nimport sys\nimport traceback\n\nfrom neuralmonkey.logging import log, debug\nfrom neuralmonkey.experiment import Experiment\n\n\n# pylint: disable=too-many-statements, too-many-locals, too-many-branches\ndef _main() -> None:\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(""config"", metavar=""INI-FILE"",\n                        help=""the configuration file for the experiment"")\n    parser.add_argument(""-s"", ""--set"", type=str, metavar=""SETTING"",\n                        action=""append"", dest=""config_changes"", default=[],\n                        help=""override an option in the configuration; the ""\n                        ""syntax is [section.]option=value"")\n    parser.add_argument(""-v"", ""--var"", type=str, metavar=""VAR"", default=[],\n                        action=""append"", dest=""config_vars"",\n                        help=""set a variable in the configuration; the syntax ""\n                        ""is var=value (shorthand for -s vars.var=value)"")\n    parser.add_argument(""-i"", ""--init"", dest=""init_only"", action=""store_true"",\n                        help=""initialize the experiment directory and exit ""\n                        ""without building the model"")\n    parser.add_argument(""-f"", ""--overwrite"", action=""store_true"",\n                        help=""force overwriting the output directory; can be ""\n                        ""used to start an experiment created with --init"")\n    args = parser.parse_args()\n\n    args.config_changes.extend(""vars.{}"".format(s) for s in args.config_vars)\n\n    exp = Experiment(config_path=args.config,\n                     config_changes=args.config_changes,\n                     train_mode=True,\n                     overwrite_output_dir=args.overwrite)\n\n    with open(exp.get_path(""args"", exp.cont_index + 1), ""w"") as file:\n        print("" "".join(shlex.quote(a) for a in sys.argv), file=file)\n\n    if args.init_only:\n        if exp.cont_index >= 0:\n            log(""The experiment directory already exists."", color=""red"")\n            exit(1)\n\n        exp.config.save_file(exp.get_path(""experiment.ini"", 0))\n        copyfile(args.config, exp.get_path(""original.ini"", 0))\n\n        log(""Experiment directory initialized."")\n\n        cmd = [os.path.basename(sys.argv[0]), ""-f"",\n               exp.get_path(""experiment.ini"", 0)]\n        log(""To start experiment, run: {}"".format("" "".join(shlex.quote(a)\n                                                           for a in cmd)))\n        exit(0)\n\n    try:\n        exp.train()\n    except KeyboardInterrupt:  # pylint: disable=try-except-raise\n        raise\n    except Exception:  # pylint: disable=broad-except\n        log(traceback.format_exc(), color=""red"")\n        exit(1)\n\n\ndef main() -> None:\n    try:\n        _main()\n    except KeyboardInterrupt:\n        log(""Training interrupted by user."")\n        debug(traceback.format_exc())\n        exit(1)\n'"
neuralmonkey/training_profiler.py,0,"b'# pylint: disable=unused-import\nfrom typing import List, Optional\n# pylint: enable=unused-import\nimport time\n\nfrom neuralmonkey.logging import log, notice\n\n\nclass TrainingProfiler:\n    """"""Training profiler class.\n\n    This class is used for measuring inter-validation and validation times\n    during training. It stores the training profile in the `inter_val_times`\n    and `validation_times` lists. These can be accessed by the toolkit to\n    provide the user with insight about the training and validation time ratio.\n\n    Additionally, this class provides getters for last logging and validation\n    times, which can be used for deciding whether to log training progress\n    or validate the model.\n    """"""\n\n    def __init__(self) -> None:\n        self._start_time = None  # type: Optional[float]\n        self._epoch_starts = []  # type: List[float]\n\n        self._last_val_time = None  # type: Optional[float]\n        self._last_log_time = None  # type: Optional[float]\n        self._current_validation_start = None  # type: Optional[float]\n\n        self.inter_val_times = []  # type: List[float]\n        self.validation_times = []  # type: List[float]\n\n        self.time = time.process_time\n\n    @property\n    def start_time(self) -> float:\n        if self._start_time is None:\n            raise RuntimeError(""Training did not start yet"")\n        return self._start_time\n\n    @property\n    def last_log_time(self) -> float:\n        if self._last_log_time is None:\n            return self.start_time\n        return self._last_log_time\n\n    @property\n    def last_val_time(self) -> float:\n        if self._last_val_time is None:\n            return self.start_time\n        return self._last_val_time\n\n    def training_start(self) -> None:\n        self._start_time = self.time()\n\n    def epoch_start(self) -> None:\n        self._epoch_starts.append(self.time())\n\n    def log_done(self) -> None:\n        self._last_log_time = self.time()\n\n    def validation_start(self) -> None:\n        assert self._current_validation_start is None\n        self._current_validation_start = self.time()\n        self.inter_val_times.append(\n            self._current_validation_start - self.last_val_time)\n\n    def validation_done(self) -> None:\n        assert self._current_validation_start is not None\n        self._last_val_time = self.time()\n\n        self.validation_times.append(\n            self.last_val_time - self._current_validation_start)\n\n        self._current_validation_start = None\n\n    def log_after_validation(\n            self, val_examples: int, train_examples: int) -> None:\n\n        train_duration = self.inter_val_times[-1]\n        val_duration = self.validation_times[-1]\n\n        train_speed = train_examples / train_duration\n        val_speed = val_examples / val_duration\n\n        log(""Validation time: {:.2f}s ({:.1f} instances/sec), ""\n            ""inter-validation: {:.2f}s, ({:.1f} instances/sec)""\n            .format(val_duration, val_speed, train_duration, train_speed),\n            color=""blue"")\n\n        if self.inter_val_times[-1] < 2 * self.validation_times[-1]:\n            notice(""Validation period setting is inefficient."")\n'"
neuralmonkey/vocabulary.py,6,"b'""""""Vocabulary class module.\n\nThis module implements the Vocabulary class and the helper functions that\ncan be used to obtain a Vocabulary instance.\n""""""\n\nimport collections\nimport json\nimport os\n\nfrom typing import List, Set, Union\n\nimport numpy as np\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.logging import log, warn, notice\n\nPAD_TOKEN = ""<pad>""\nSTART_TOKEN = ""<s>""\nEND_TOKEN = ""</s>""\nUNK_TOKEN = ""<unk>""\n\nSPECIAL_TOKENS = [PAD_TOKEN, START_TOKEN, END_TOKEN, UNK_TOKEN]\n\nPAD_TOKEN_INDEX = 0\nSTART_TOKEN_INDEX = 1\nEND_TOKEN_INDEX = 2\nUNK_TOKEN_INDEX = 3\n\n\ndef from_wordlist(path: str,\n                  encoding: str = ""utf-8"",\n                  contains_header: bool = True,\n                  contains_frequencies: bool = True) -> ""Vocabulary"":\n    """"""Load a vocabulary from a wordlist.\n\n    The file can contain either list of words with no header.\n    Or it can contain words and their counts separated\n    by tab and a header on the first line.\n\n    Arguments:\n        path: The path to the wordlist file\n        encoding: The encoding of the wordlist file (defaults to UTF-8)\n        contains_header: if the file have a header on first line\n        contains_frequencies: if the file contains a second column\n\n    Returns:\n        The new Vocabulary instance.\n    """"""\n    check_argument_types()\n    vocabulary = []  # type: List[str]\n\n    with open(path, encoding=encoding) as wordlist:\n        line_number = 1\n        if contains_header:\n            # skip the header\n            line_number += 1\n            next(wordlist)\n\n        for line in wordlist:\n            line = line.strip()\n            # check if line is empty\n            if not line:\n                warn(""Vocabulary file {}:{}: line empty""\n                     .format(path, line_number))\n                line_number += 1\n                continue\n\n            if contains_frequencies:\n                info = line.split(""\\t"")\n                if len(info) != 2:\n                    raise ValueError(\n                        ""Vocabulary file {}:{}: line does not have two columns""\n                        .format(path, line_number))\n                word = info[0]\n            else:\n                if ""\\t"" in line:\n                    warn(""Vocabulary file {}:{}: line contains a tabulator""\n                         .format(path, line_number))\n                word = line\n\n            if line_number <= len(SPECIAL_TOKENS) + int(contains_header):\n                should_be = SPECIAL_TOKENS[\n                    line_number - 1 - int(contains_header)]\n                if word != should_be:\n                    notice(""Expected special token {} but encountered a ""\n                           ""different word: {}"".format(should_be, word))\n                    vocabulary.append(word)\n                line_number += 1\n                continue\n\n            vocabulary.append(word)\n            line_number += 1\n\n    log(""Vocabulary from wordlist loaded, containing {} words""\n        .format(len(vocabulary)))\n    log_sample(vocabulary)\n    return Vocabulary(vocabulary)\n\n\ndef from_t2t_vocabulary(path: str,\n                        encoding: str = ""utf-8"") -> ""Vocabulary"":\n    """"""Load a vocabulary generated during tensor2tensor training.\n\n    Arguments:\n        path: The path to the vocabulary file.\n        encoding: The encoding of the vocabulary file (defaults to UTF-8).\n\n    Returns:\n        The new Vocabulary instantce.\n    """"""\n    check_argument_types()\n    vocabulary = []  # type: List[str]\n\n    with open(path, encoding=encoding) as wordlist:\n        for line in wordlist:\n            line = line.strip()\n\n            # T2T vocab tends to wrap words in single quotes\n            if ((line.startswith(""\'"") and line.endswith(""\'""))\n                    or (line.startswith(\'""\') and line.endswith(\'""\'))):\n                line = line[1:-1]\n\n            if line in [""<pad>"", ""<EOS>""]:\n                continue\n\n            vocabulary.append(line)\n\n    log(""Vocabulary form wordlist loaded, containing {} words""\n        .format(len(vocabulary)))\n    log_sample(vocabulary)\n\n    return Vocabulary(vocabulary)\n\n\ndef from_nematus_json(path: str, max_size: int = None,\n                      pad_to_max_size: bool = False) -> ""Vocabulary"":\n    """"""Load vocabulary from Nematus JSON format.\n\n    The JSON format is a flat dictionary that maps words to their index in the\n    vocabulary.\n\n    Args:\n        path: Path to the file.\n        max_size: Maximum vocabulary size including \'unk\' and \'eos\' symbols,\n            but not including <pad> and <s> symbol.\n        pad_to_max_size: If specified, the vocabulary is padded with dummy\n            symbols up to the specified maximum size.\n    """"""\n    check_argument_types()\n    with open(path, ""r"", encoding=""utf-8"") as f_json:\n        contents = json.load(f_json)\n\n    vocabulary = []  # type: List[str]\n    for word in sorted(contents.keys(), key=lambda x: contents[x]):\n        if contents[word] < 2:\n            continue\n        vocabulary.append(word)\n        if max_size is not None and len(vocabulary) == max_size:\n            break\n\n    if max_size is None:\n        max_size = len(vocabulary) - 2  # the ""2"" is ugly HACK\n\n    if pad_to_max_size and max_size is not None:\n        current_length = len(vocabulary)\n        for i in range(max_size - current_length + 2):  # the ""2"" is ugly HACK\n            word = ""<pad_{}>"".format(i)\n            vocabulary.append(word)\n\n    return Vocabulary(vocabulary)\n\n\nclass Vocabulary(collections.Sized):\n\n    def __init__(self, words: List[str], num_oov_buckets: int = 0) -> None:\n        """"""Create a new instance of a vocabulary.\n\n        Arguments:\n            words: The mapping of indices to words.\n        """"""\n\n        self._vocabulary = SPECIAL_TOKENS + words\n        self._alphabet = {c for word in words for c in word}\n\n        self._index_to_string = (\n            tf.contrib.lookup.index_to_string_table_from_tensor(\n                mapping=self._vocabulary,\n                default_value=UNK_TOKEN))\n\n        self._string_to_index = tf.contrib.lookup.index_table_from_tensor(\n            mapping=self._vocabulary,\n            num_oov_buckets=num_oov_buckets,\n            default_value=UNK_TOKEN_INDEX)\n\n    def __len__(self) -> int:\n        """"""Get the size of the vocabulary.\n\n        Returns:\n            The number of distinct words in the vocabulary.\n        """"""\n        return len(self._vocabulary)\n\n    def __contains__(self, word: str) -> bool:\n        """"""Check if a word is in the vocabulary.\n\n        Arguments:\n            word: The word to look up.\n\n        Returns:\n            True if the word was added to the vocabulary, False otherwise.\n        """"""\n        return word in self._vocabulary\n\n    @property\n    def alphabet(self) -> Set[str]:\n        return self._alphabet\n\n    @property\n    def index_to_word(self) -> List[str]:\n        return self._vocabulary\n\n    def strings_to_indices(self,\n                           # add_start_symbol: bool = False,\n                           # add_end_symbol: bool = False\n                           sentences: tf.Tensor) -> tf.Tensor:\n        """"""Generate the tensor representation for the provided sentences.\n\n        Arguments:\n            sentences: List of sentences as lists of tokens.\n            add_start_symbol: If True, the `<s>` token will be added to the\n                beginning of each sentence vector. Enabling this option extends\n                the maximum length by one.\n            add_end_symbol: If True, the `</s>` token will be added to the end\n                of each sentence vector, provided that the sentence is shorter\n                than `max_len`. If not, the end token is not added. Unlike\n                `add_start_symbol`, enabling this option **does not alter**\n                the maximum length.\n\n        Returns:\n            Tensor of indices of the words.\n        """"""\n        return self._string_to_index.lookup(sentences)\n\n    def indices_to_strings(self, vectors: tf.Tensor) -> tf.Tensor:\n        """"""Convert tensors of indexes of vocabulary items to lists of words.\n\n        Arguments:\n            vectors: An int Tensor with indices to the vocabulary.\n\n        Returns:\n            A string Tensor with the corresponding words.\n        """"""\n        return self._index_to_string.lookup(vectors)\n\n    def vectors_to_sentences(\n            self,\n            vectors: Union[List[np.ndarray], np.ndarray]) -> List[List[str]]:\n        """"""Convert vectors of indexes of vocabulary items to lists of words.\n\n        Arguments:\n            vectors: TIME-MAJOR List of vectors of vocabulary indices.\n\n        Returns:\n            List of lists of words.\n        """"""\n        if isinstance(vectors, list):\n            if not vectors:\n                raise ValueError(\n                    ""Cannot infer batch size because decoder returned an ""\n                    ""empty output."")\n\n            batch_size = vectors[0].shape[0]\n        elif isinstance(vectors, np.ndarray):\n            batch_size = vectors.shape[1]\n        else:\n            raise TypeError(\n                ""Unexpected type of decoder output: {}"".format(type(vectors)))\n\n        sentences = [[] for _ in range(batch_size)]  # type: List[List[str]]\n\n        for vec in vectors:\n            for sentence, word_i in zip(sentences, vec):\n                if not sentence or sentence[-1] != END_TOKEN:\n                    sentence.append(self.index_to_word[word_i])\n\n        return [s[:-1] if s and s[-1] == END_TOKEN else s for s in sentences]\n\n    def save_wordlist(self, path: str, overwrite: bool = False,\n                      encoding: str = ""utf-8"") -> None:\n        """"""Save the vocabulary as a wordlist.\n\n        The file is ordered by the ids of words.\n        This function is used mainly for embedding visualization.\n\n        Arguments:\n            path: The path to save the file to.\n            overwrite: Flag whether to overwrite existing file.\n                Defaults to False.\n\n        Raises:\n            FileExistsError if the file exists and overwrite flag is\n            disabled.\n        """"""\n        if os.path.exists(path) and not overwrite:\n            raise FileExistsError(""Cannot save vocabulary: File exists and ""\n                                  ""overwrite is disabled. {}"".format(path))\n\n        with open(path, ""w"", encoding=encoding) as output_file:\n            log(""Storing vocabulary without frequencies."")\n\n            for word in self._vocabulary:\n                output_file.write(""{}\\n"".format(word))\n\n\ndef log_sample(vocabulary: List[str], size: int = 5) -> None:\n    """"""Log a sample of the vocabulary.\n\n    Arguments:\n        size: How many sample words to log.\n    """"""\n    if size > len(vocabulary):\n        log(""Vocabulary: {}"".format(vocabulary))\n    else:\n        sample_ids = np.random.permutation(np.arange(len(vocabulary)))[:size]\n        log(""Sample of the vocabulary: {}"".format(\n            [vocabulary[i] for i in sample_ids]))\n\n\ndef pad_batch(sentences: List[List[str]],\n              max_length: int = None,\n              add_start_symbol: bool = False,\n              add_end_symbol: bool = False) -> List[List[str]]:\n\n    max_len = max(len(s) for s in sentences)\n    if add_end_symbol:\n        max_len += 1\n\n    if max_length is not None:\n        max_len = min(max_length, max_len)\n\n    padded_sentences = []\n    for sent in sentences:\n        if add_end_symbol:\n            padded = (sent + [END_TOKEN] + [PAD_TOKEN] * max_len)[:max_len]\n        else:\n            padded = (sent + [PAD_TOKEN] * max_len)[:max_len]\n\n        if add_start_symbol:\n            padded.insert(0, START_TOKEN)\n        padded_sentences.append(padded)\n\n    return padded_sentences\n\n\ndef sentence_mask(sentences: tf.Tensor) -> tf.Tensor:\n    return tf.to_float(tf.not_equal(sentences, PAD_TOKEN_INDEX))\n'"
scripts/avg_checkpoints.py,10,"b'#!/usr/bin/env python3\n""""""Compute the average of each variable in a list of checkpoint files.\n\nGiven a list of model checkpoints, it generates a new checkpoint with\nparameters which are the arithmetic average of them.\n\nBased on a script from Tensor2Tensor:\nhttps://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/avg_checkpoints.py\n""""""\n\nimport argparse\nimport os\nimport re\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom neuralmonkey.logging import log as _log\n\nIGNORED_PATTERNS = [""global_step""]\n\n\ndef log(message: str, color: str = ""blue"") -> None:\n    _log(message, color)\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(""checkpoints"", type=str, nargs=""+"",\n                        help=""Space-separated list of checkpoints to average."")\n    parser.add_argument(""output_path"", type=str,\n                        help=""Path to output the averaged checkpoint to."")\n    args = parser.parse_args()\n\n    non_existing_chckpoints = []\n    for ckpt in args.checkpoints:\n        if not os.path.exists(""{}.index"".format(ckpt)):\n            non_existing_chckpoints.append(ckpt)\n    if non_existing_chckpoints:\n        raise ValueError(\n            ""Provided checkpoints do not exist: {}"".format(\n                "", "".join(non_existing_chckpoints)))\n\n    # Read variables from all checkpoints and average them.\n    log(""Getting list of variables"")\n    var_list = tf.contrib.framework.list_variables(args.checkpoints[0])\n    var_values, var_dtypes = {}, {}\n    for (name, shape) in var_list:\n        if not any(re.match(pat, name) for pat in IGNORED_PATTERNS):\n            var_values[name] = np.zeros(shape)\n    for checkpoint in args.checkpoints:\n        log(""Reading from checkpoint {}"".format(checkpoint))\n        reader = tf.contrib.framework.load_checkpoint(checkpoint)\n        for name in var_values:\n            tensor = reader.get_tensor(name)\n            var_dtypes[name] = tensor.dtype\n            var_values[name] += tensor\n    for name in var_values:    # Average.\n        var_values[name] /= len(args.checkpoints)\n\n    tf_vars = [\n        tf.get_variable(v, shape=var_values[v].shape, dtype=var_dtypes[v])\n        for v in var_values\n    ]\n    placeholders = [tf.placeholder(v.dtype, shape=v.shape) for v in tf_vars]\n    assign_ops = [tf.assign(v, p) for (v, p) in zip(tf_vars, placeholders)]\n    global_step = tf.Variable(\n            0, name=""global_step"", trainable=False, dtype=tf.int64)\n    saver = tf.train.Saver()\n\n    # Build a model only with variables, set them to the average values.\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for p, assign_op, (name, value) in zip(placeholders, assign_ops,\n                                               var_values.items()):\n            sess.run(assign_op, {p: value})\n        saver.save(sess, args.output_path, global_step=global_step)\n\n    log(""Averaged checkpoints saved in {}"".format(args.output_path))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/build_config.py,1,"b'#!/usr/bin/env python3\n""""""Loads and builds a given config file in memory.\n\nCan be used for checking that a model can be loaded successfully, or for\ngenerating a vocabulary from a dataset, without the need to run the model.\n""""""\n\nimport argparse\nimport collections\nfrom typing import Any, Dict\n\nimport neuralmonkey\nfrom neuralmonkey.config.parsing import parse_file\nfrom neuralmonkey.config.builder import build_config, ObjectRef, ClassSymbol\n\n\ndef _patch_config_builder():\n    imports = set()\n    statements = []\n\n    def get_class_name(symbol: ClassSymbol):\n        name = symbol.clazz\n        if name.startswith(""tf.""):\n            return name\n        full_name = ""neuralmonkey."" + name\n        module, _, _ = full_name.rpartition(""."")\n        imports.add(""import "" + module)\n        return full_name\n\n    def build_object(value: str,\n                     all_dicts: Dict[str, Any],\n                     existing_objects: Dict[str, Any],\n                     depth: int) -> Any:\n        if depth > 20:\n            raise AssertionError(\n                ""Config recursion should not be deeper that 20."")\n\n        if (isinstance(value, collections.Iterable) and\n            not isinstance(value, str)):\n            objects = [build_object(\n                val, all_dicts, existing_objects, depth + 1) for val in value]\n            if isinstance(value, tuple):\n                if len(objects) == 1:\n                    objects[0] += "",""  # Singleton tuple needs a comma.\n                return ""("" + "", "".join(objects) + "")""\n            else:\n                return ""["" + "", "".join(objects) + ""]""\n\n        if isinstance(value, ObjectRef):\n            if value.name not in existing_objects:\n                clazz = all_dicts[value.name][""class""]\n                args = [\n                    ""\\n    {}={}"".format(key, build_object(\n                        val, all_dicts, existing_objects, depth + 1))\n                    for key, val in all_dicts[value.name].items()\n                    if key != ""class""\n                ]\n                statements.append(\n                    ""{} = {}({})"".format(\n                        value.name, get_class_name(clazz), "","".join(args)))\n\n                existing_objects[value.name] = True\n            return value.expression\n\n        if isinstance(value, ClassSymbol):\n            return get_class_name(value)\n\n        return repr(value)\n\n    neuralmonkey.config.builder.build_object = build_object\n\n    return imports, statements\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(""config"", metavar=""INI-FILE"",\n                        help=""a configuration file"")\n    parser.add_argument(""--code"", ""-c"", action=""store_true"",\n                        help=""instead of building the config, generate ""\n                        ""equivalent Python code and write it to stdout"")\n    args = parser.parse_args()\n\n    with open(args.config, ""r"", encoding=""utf-8"") as f:\n        _, config_dict = parse_file(f)\n\n    if args.code:\n        imports, statements = _patch_config_builder()\n\n    config, _ = build_config(config_dict, ignore_names=set())\n\n    if args.code:\n        print(""import argparse\\nimport tensorflow as tf"")\n        print(*sorted(imports), sep=""\\n"", end=""\\n\\n"")\n        print(*statements, sep=""\\n"", end=""\\n\\n"")\n        print(""model = argparse.Namespace({})"".format(\n            "","".join(""\\n    {}={}"".format(key, config[key]) for key in config)))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/caffe_image_features.py,0,"b'import argparse\nimport sys\nimport os\nos.environ[\'GLOG_minloglevel\'] = \'4\'\nsys.path.append(""caffe/python"")\nimport caffe\nimport numpy as np\nimport skimage\n\ndef crop_image(x, target_height=227, target_width=227):\n    image = skimage.img_as_float(skimage.io.imread(x)).astype(np.float32)\n\n    if len(image.shape) == 2:\n        image = np.tile(image[:,:,None], 3)\n    elif len(image.shape) == 4:\n        image = image[:,:,:,0]\n\n    height, width, rgb = image.shape\n    if width == height:\n        resized_image = skimage.transform.resize(image, (target_height,target_width))\n\n    elif height < width:\n        resized_image = skimage.transform.resize(image, (int(width * float(target_height)/height), target_width))\n        cropping_length = int((resized_image.shape[1] - target_height) / 2)\n        resized_image = resized_image[:,cropping_length:resized_image.shape[1] - cropping_length]\n\n    else:\n        resized_image = skimage.transform.resize(image, (target_height, int(height * float(target_width) / width)))\n        cropping_length = int((resized_image.shape[0] - target_width) / 2)\n        resized_image = resized_image[cropping_length:resized_image.shape[0] - cropping_length,:]\n\n    return skimage.transform.resize(resized_image, (target_height, target_width))\n\nclass CNN:\n\n    def __init__(self, deploy, model, mean, batch_size=10, width=227, height=227):\n\n        self.deploy = deploy\n        self.model = model\n        self.mean = mean\n\n        self.batch_size = batch_size\n        self.net, self.transformer = self.get_net()\n        self.net.blobs[\'data\'].reshape(self.batch_size, 3, height, width)\n\n        self.width = width\n        self.height = height\n\n    def get_net(self):\n        #caffe.set_mode_cpu()\n        net = caffe.Net(self.deploy, self.model, caffe.TEST)\n\n        transformer = caffe.io.Transformer({\'data\':net.blobs[\'data\'].data.shape})\n        transformer.set_transpose(\'data\', (2,0,1))\n        transformer.set_mean(\'data\', np.load(self.mean).mean(1).mean(1))\n        transformer.set_raw_scale(\'data\', 255)\n        transformer.set_channel_swap(\'data\', (2,1,0))\n\n        return net, transformer\n\n    def get_features(self, image_list, layers=\'fc7\', layer_sizes=[4096]):\n        iter_until = len(image_list) + self.batch_size\n        all_feats = np.zeros([len(image_list)] + layer_sizes, dtype=np.float32)\n\n        for start, end in zip(list(range(0, iter_until, self.batch_size)), \\\n                              list(range(self.batch_size, iter_until, self.batch_size))):\n\n            image_batch_file = image_list[start:end]\n            image_batch = np.array([crop_image(x, target_width=self.width, target_height=self.height) for x in image_batch_file])\n\n            caffe_in = np.zeros(np.array(image_batch.shape)[[0,3,1,2]], dtype=np.float32)\n\n            for idx, in_ in enumerate(image_batch):\n                caffe_in[idx] = self.transformer.preprocess(\'data\', in_)\n\n            out = self.net.forward_all(blobs=[layers], **{\'data\':caffe_in})\n            feats = out[layers]\n\n            all_feats[start:end] = feats\n\n        return all_feats\n\ndef shape(string):\n    return [int(s) for s in string.split(""x"")]\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=""Image feature extraction"")\n    parser.add_argument(""--model-prototxt"", type=str, required=True)\n    parser.add_argument(""--model-parameters"", type=str, required=True)\n    parser.add_argument(""--img-mean"", type=str, required=True)\n    parser.add_argument(""--feature-layer"", type=str, required=True)\n    parser.add_argument(""--image-directory"", type=str, required=True)\n    parser.add_argument(""--image-list"", type=argparse.FileType(\'r\'), required=True)\n    parser.add_argument(""--output-file"", type=argparse.FileType(\'wb\'), required=True)\n    parser.add_argument(""--img-shape"", type=shape, required=True)\n    parser.add_argument(""--output-shape"", type=shape, required=True)\n    args = parser.parse_args()\n\n    cnn = CNN(deploy=args.model_prototxt, model=args.model_parameters, mean=args.img_mean,\n              batch_size=10, width=args.img_shape[0], height=args.img_shape[1])\n    path_list = [os.path.join(args.image_directory, f.rstrip()) for f in args.image_list]\n    features_shape = [args.output_shape[2]] + args.output_shape[:2]\n    features = cnn.get_features(path_list, layers=args.feature_layer, layer_sizes=features_shape)\n\n    np.save(args.output_file, features.transpose((0, 2, 3, 1)))\n'"
scripts/column_selector.py,0,"b'#!/usr/bin/env python3.5\n""""""\nSelect lines from N files according to an index file. This work nicely in\ncombination with the max_column_finder.py script.\n\nThese two scripts are particularly useful for hypotheses rescoring.\n""""""\nimport argparse\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(""--selector"", metavar=""SELECTOR"",\n                        type=argparse.FileType(""r""),\n                        help=""file with column indices"")\n    parser.add_argument(""input_files"", nargs=""+"",\n                        metavar=""INPUT_FILES"", type=argparse.FileType(""r""),\n                        help=""the files to traverse"")\n    args = parser.parse_args()\n\n    for lines in zip(*([args.selector] + args.input_files)):\n        index = int(lines[0])\n        print(lines[index + 1].strip())\n\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/decompound_truecased.py,0,"b'#!/usr/bin/env python3\n\nimport sys\nimport codecs\nimport javabridge\n\nfrom tokenize_data import get_decompounder\n\ndef main():\n    sys.stdin = codecs.getreader(\'utf-8\')(sys.stdin)\n    sys.stdout = codecs.getwriter(\'utf-8\')(sys.stdout)\n    sys.stderr = codecs.getwriter(\'utf-8\')(sys.stderr)\n\n    try:\n        decompounder = get_decompounder()\n        for line in sys.stdin:\n            tokens = []\n            for token in line.rstrip().split("" ""):\n                if not token:\n                    continue\n                if token[0].isupper():\n                    decompounded = decompounder.splitWord(token)\n                    if decompounded.size() >= 2:\n                        parts = [decompounded.get(j)\n                                 for j in range(decompounded.size())]\n                        parts_with_hyphens = [\'-\' if not p else p\n                                              for p in parts]\n                        tokens.append("">><<"".join(parts_with_hyphens))\n                        del decompounded\n                    else:\n                        tokens.append(token)\n                else:\n                    tokens.append(token)\n            print("" "".join(tokens))\n#    except:\n#        javabridge.kill_vm()\n#        exit(1)\n    finally:\n        javabridge.kill_vm()\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/estimate_scheduled_sampling.py,0,"b'#!/usr/bin/env python3\n\n""""""\n\nA script for estimating the parameters of scheduled sampling. Based on the\nthreshold value and number of step you want to achieve the value, it computes\nthe coefficient of the inverse sigmoid decay function.\n\n""""""\n\nimport argparse\nimport numpy as np\nfrom scipy.special import lambertw\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=""Estimates parameter for scheduled sampling."")\n    parser.add_argument(""--value"", type=float, required=True,\n                        help=""The value the threshold should achieve."")\n    parser.add_argument(""--step"", type=int, required=True,\n                        help=""Step when you want to achieve the value."")\n    args = parser.parse_args()\n\n    x = args.step\n    c = args.value\n\n    coeff = c * np.exp(lambertw((1 - c) / c * x)) / (1 - c)\n\n    print(coeff.real)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/export_embeddings_to_w2v.py,2,"b'#!/usr/bin/env python3\n\n""""""Export embeddings to Word2Vec format.\n\nThis script loads a checkpoint (only variables without the model) and extract\nembeddings from a given model part. Note that for model imported from Nematus,\nthe length of the vocabulary JSON and embeddings might differ. In this case,\nturn off the check whether the embeddings and vocabulary have the same length.\n""""""\n\nimport argparse\nimport os\nimport sys\n\nimport tensorflow as tf\n\nfrom neuralmonkey.logging import log as _log\nfrom neuralmonkey.vocabulary import (\n    from_wordlist, from_nematus_json, from_t2t_vocabulary)\n\n\nPOSSIBLE_EMBEDDINGS_NAMES = [\n    ""word_embeddings"", ""embedding_matrix_0"",\n    ""input_projection/word_embeddings""]\n\n\ndef log(message: str, color: str = ""blue"") -> None:\n    _log(message, color)\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\n        ""model_checkpoint"", metavar=""MODEL-CHECKPOINT"",\n        help=""Path to the model checkpoint."")\n    parser.add_argument(\n        ""model_part_name"", metavar=""MODEL-PART"",\n        help=""Name of model part with embeddings."")\n    parser.add_argument(\n        ""vocabulary"", metavar=""VOCABULARY"", help=""Vocabulary file."")\n    parser.add_argument(\n        ""--output-file"", metavar=""OUTPUT"", default=sys.stdout,\n        type=argparse.FileType(\'w\'), required=False,\n        help=""Output file in Word2Vec format, STDOUT by default."")\n    parser.add_argument(\n        ""--vocabulary-format"", type=str,\n        choices=[""tsv"", ""word_list"", ""nematus_json"", ""t2t_vocabulary""],\n        default=""tsv"",\n        help=""Vocabulary format (see functions in the vocabulary module)."")\n    parser.add_argument(\n        ""--validate-length"", type=bool, default=True,\n        help=(""Check if the vocabulary and the embeddings have the ""\n              ""same length.""))\n    args = parser.parse_args()\n\n    if args.vocabulary_format == ""word_list"":\n        vocabulary = from_wordlist(\n            args.vocabulary, contains_header=False, contains_frequencies=False)\n    elif args.vocabulary_format == ""tsv"":\n        vocabulary = from_wordlist(\n            args.vocabulary, contains_header=True, contains_frequencies=True)\n    elif args.vocabulary_format == ""nematus_json"":\n        vocabulary = from_nematus_json(args.vocabulary)\n    elif args.vocabulary_format == ""t2t_vocabulary"":\n        vocabulary = from_t2t_vocabulary(args.vocabulary)\n    else:\n        raise ValueError(""Unknown type of vocabulary file: {}"".format(\n            args.vocabulary_format))\n\n    if not os.path.exists(""{}.index"".format(args.model_checkpoint)):\n        log(""Checkpoint \'{}\' does not exist."".format(\n            args.model_checkpoint), color=""red"")\n        exit(1)\n\n    embeddings_name = None\n    for model_part in [args.model_part_name,\n                       ""{}_input"".format(args.model_part_name)]:\n        log(""Getting list of variables in \'{}\'."".format(model_part))\n        var_list = [\n            name for name, shape in\n            tf.contrib.framework.list_variables(args.model_checkpoint)\n            if name.startswith(""{}/"".format(model_part))]\n\n        for name in POSSIBLE_EMBEDDINGS_NAMES:\n            candidate_name = ""{}/{}"".format(model_part, name)\n            if candidate_name in var_list:\n                embeddings_name = candidate_name\n                break\n\n    if embeddings_name is None:\n        log(""No embeddings found in the model part."", color=""red"")\n        exit(1)\n\n    reader = tf.contrib.framework.load_checkpoint(args.model_checkpoint)\n    embeddings = reader.get_tensor(embeddings_name)\n\n    word_count, dimension = embeddings.shape\n\n    if word_count != len(vocabulary):\n        if args.validate_length:\n            log((""Vocabulary has length of {}, but there are {} ""\n                 ""embeddings."").format(len(vocabulary), word_count),\n                color=""red"")\n            exit(1)\n        else:\n            word_count = min(word_count, len(vocabulary))\n\n    print(""{}\\t{}"".format(word_count, dimension), file=args.output_file)\n    for word, vector in zip(vocabulary.index_to_word, embeddings):\n        formatted_vector = ""\\t"".join([""{:.8f}"".format(x) for x in vector])\n        print(""{}\\t{}"".format(word, formatted_vector), file=args.output_file)\n    args.output_file.close()\n\n    log(""Done"")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/extract_model_part_from_ckpt.py,8,"b'#!/usr/bin/env python3\n\n""""""Extract variables of one model part into a single checkpoint file.\n\nCan be used to load the model part in a different setup.""""""\n\nimport argparse\nimport os\nimport tensorflow as tf\n\nfrom neuralmonkey.logging import log as _log\n\n\ndef log(message: str, color: str = ""blue"") -> None:\n    _log(message, color)\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(""orig_checkpoint"", metavar=""EXPERIMENT-CHECKPOINT"",\n                        help=""path to the original checkpoint"")\n    parser.add_argument(""model_part_name"", metavar=""MODEL-PART"",\n                        help=""name of the extracted model part"")\n    parser.add_argument(""output_path"", metavar=""OUTPUT-CHECKPOINT"",\n                        help=""output checkopint file"")\n    args = parser.parse_args()\n\n    if not os.path.exists(""{}.index"".format(args.orig_checkpoint)):\n        log(""Checkpoint \'{}\' does not exist."".format(\n            args.orig_checkpoint), color=""red"")\n        exit(1)\n\n    log(""Getting list of variables."")\n    var_list = [\n        name for name, shape in\n        tf.contrib.framework.list_variables(args.orig_checkpoint)\n        if name.startswith(""{}/"".format(args.model_part_name))]\n\n    if not var_list:\n        log(""No variables for model part \'{}\' in checkpoint \'{}\'."".format(\n            args.model_part_name, args.orig_checkpoint), color=""red"")\n        exit(1)\n\n    log(""Reading variables from the checkpoint: {}"".format(\n        "", "".join(var_list)))\n\n    var_values, var_dtypes = {}, {}\n    reader = tf.contrib.framework.load_checkpoint(args.orig_checkpoint)\n    for name in var_list:\n        tensor = reader.get_tensor(name)\n        var_dtypes[name] = tensor.dtype\n        var_values[name] = tensor\n\n    tf_vars = [\n        tf.get_variable(v, shape=var_values[v].shape, dtype=var_dtypes[v])\n        for v in var_values]\n    placeholders = [tf.placeholder(v.dtype, shape=v.shape) for v in tf_vars]\n    assign_ops = [tf.assign(v, p) for (v, p) in zip(tf_vars, placeholders)]\n    saver = tf.train.Saver()\n\n    # Build a model only with variables, set them to the average values.\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for p, assign_op, (name, value) in zip(placeholders, assign_ops,\n                                               var_values.items()):\n            sess.run(assign_op, {p: value})\n        saver.save(sess, os.path.abspath(args.output_path))\n\n    log(""Extracted model part saved to {}"".format(args.output_path))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/imagenet_features.py,2,"b'#!/usr/bin/env python3\n""""""Extract imagenet features from given images.\n\nThe script reads a list of pahts to images (specified by path prefix and list\nof relative paths), process the images using an imagenet network and extract a\ngiven convolutional map from the image. The maps are saved as numpy tensors in\nfiles with a different prefix and the same relative path from this prefix\nending with .npz.\n""""""\n\nimport argparse\nimport os\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom neuralmonkey.dataset import Dataset, BatchingScheme\nfrom neuralmonkey.encoders.imagenet_encoder import ImageNet\nfrom neuralmonkey.logging import log\nfrom neuralmonkey.readers.image_reader import single_image_for_imagenet\n\n\nSUPPORTED_NETWORKS = [\n    ""vgg_16"", ""vgg_19"", ""resnet_v2_50"", ""resnet_v2_101"", ""resnet_v2_152""]\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(""--net"", type=str, choices=SUPPORTED_NETWORKS,\n                        help=""Type of imagenet network."")\n    parser.add_argument(""--input-prefix"", type=str, default="""",\n                        help=""Prefix of the image path."")\n    parser.add_argument(""--output-prefix"", type=str, default="""",\n                        help=""Prefix of the path to the output numpy files."")\n    parser.add_argument(""--slim-models"", type=str, required=True,\n                        help=""Path to SLIM models in cloned tensorflow/models ""\n                        ""repository"")\n    parser.add_argument(""--model-checkpoint"", type=str, required=True,\n                        help=""Path to the ImageNet model checkpoint."")\n    parser.add_argument(""--conv-map"", type=str, required=False, default=None,\n                        help=""Name of the convolutional map that is."")\n    parser.add_argument(""--vector"", type=str, required=False, default=None,\n                        help=""Name of the feed-forward layer."")\n    parser.add_argument(""--images"", type=str,\n                        help=""File with paths to images or stdin by default."")\n    parser.add_argument(""--batch-size"", type=int, default=128)\n    args = parser.parse_args()\n\n    if args.conv_map is None == args.vector is None:\n        raise ValueError(\n            ""You must provide either convolutional map or feed-forward layer."")\n\n    if not os.path.exists(args.input_prefix):\n        raise ValueError(""Directory {} does not exist."".format(\n            args.input_prefix))\n    if not os.path.exists(args.output_prefix):\n        raise ValueError(""Directory {} does not exist."".format(\n            args.output_prefix))\n\n    if args.net.startswith(""vgg_""):\n        img_size = 224\n        vgg_normalization = True\n        zero_one_normalization = False\n    elif args.net.startswith(""resnet_v2""):\n        img_size = 229\n        vgg_normalization = False\n        zero_one_normalization = True\n    else:\n        raise ValueError(""Unspported network: {}."".format(args._net))\n\n    log(""Creating graph for the ImageNet network."")\n    imagenet = ImageNet(\n        name=""imagenet"", data_id=""images"", network_type=args.net,\n        slim_models_path=args.slim_models, load_checkpoint=args.model_checkpoint,\n        spatial_layer=args.conv_map, encoded_layer=args.vector)\n\n    log(""Creating TensorFlow session."")\n    session = tf.Session()\n    session.run(tf.global_variables_initializer())\n    log(""Loading ImageNet model variables."")\n    imagenet.load(session)\n\n    if args.images is None:\n        log(""No input file provided, reading paths from stdin."")\n        source = sys.stdin\n    else:\n        source = open(args.images)\n\n    images = []\n    image_paths = []\n\n    def process_images():\n        dataset = Dataset(""dataset"", {""images"": np.array(images)},\n                          BatchingScheme(batch_size=1), {})\n        feed_dict = imagenet.feed_dict(dataset)\n\n        fetch = imagenet.encoded if args.vector else imagenet.spatial_states\n        feature_maps = session.run(fetch, feed_dict=feed_dict)\n\n        for features, rel_path in zip(feature_maps, image_paths):\n            npz_path = os.path.join(args.output_prefix, rel_path + "".npz"")\n            os.makedirs(os.path.dirname(npz_path), exist_ok=True)\n            np.savez(npz_path, features)\n            print(npz_path)\n\n\n    for img in source:\n        img_path = os.path.join(args.input_prefix, img.rstrip())\n        images.append(single_image_for_imagenet(\n            img_path, img_size, img_size, vgg_normalization,\n            zero_one_normalization))\n        image_paths.append(img.rstrip())\n\n        if len(images) >= args.batch_size:\n            process_images()\n            images = []\n            image_paths = []\n    process_images()\n\n    if args.images is not None:\n        source.close()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/import_nematus.py,7,"b'#!/usr/bin/env python3\n""""""Imports nematus model file and convert it into a neural monkey experiment\ngiven a neural monkey configuration file.\n""""""\nfrom typing import Dict, Tuple, List\nimport argparse\nimport json\nimport os\nimport numpy as np\nimport tensorflow as tf\n\nfrom neuralmonkey.config.parsing import parse_file\nfrom neuralmonkey.config.builder import build_config\nfrom neuralmonkey.attention.feed_forward import Attention\nfrom neuralmonkey.encoders.recurrent import RecurrentEncoder\nfrom neuralmonkey.decoders.decoder import Decoder\nfrom neuralmonkey.decoders.encoder_projection import nematus_projection\nfrom neuralmonkey.decoders.output_projection import nematus_output\nfrom neuralmonkey.model.sequence import EmbeddedSequence\nfrom neuralmonkey.vocabulary import from_nematus_json\nfrom neuralmonkey.logging import log as _log\n\n\ndef log(message: str, color: str = ""blue"") -> None:\n    _log(message, color)\n\n\ndef check_shape(var1_tf: tf.Variable, var2_np: np.ndarray):\n    if var1_tf.get_shape().as_list() != list(var2_np.shape):\n        log(""Shapes do not match! Exception will follow."", color=""red"")\n\n\n# Here come a few functions that fiddle with the Nematus parameters in order to\n# fit them to Neural Monkey parameter shapes.\ndef emb_fix_dim1(variables: List[np.ndarray]) -> np.ndarray:\n    return emb_fix(variables, dim=1)\n\n\ndef emb_fix(variables: List[np.ndarray], dim: int = 0) -> np.ndarray:\n    """"""Process nematus tensors with vocabulary dimension.\n\n    Nematus uses only two special symbols, eos and UNK. For embeddings of start\n    and pad tokens, we use zero vectors, inserted to the correct position in\n    the parameter matrix.\n\n    Arguments:\n        variables: the list of variables. Must be of length 1.\n        dim: The vocabulary dimension.\n    """"""\n    if len(variables) != 1:\n        raise ValueError(""VocabFix only works with single vars. {} given.""\n                         .format(len(variables)))\n\n    if dim != 0 and dim != 1:\n        raise ValueError(""dim can only be 0 or 1. is: {}"".format(dim))\n\n    variable = variables[0]\n    shape = variable.shape\n\n    # insert start token (hack from nematus - last from vocab - does it work? NO)\n    # to_insert = np.squeeze(variable[-1] if dim == 0 else variable[:, -1])\n    to_insert = np.zeros(shape[1 - dim]) if len(shape) > 1 else 0.\n    variable = np.insert(variable, 0, to_insert, axis=dim)\n\n    # insert padding token\n    to_insert = np.zeros(shape[1 - dim]) if len(shape) > 1 else 0.\n    variable = np.insert(variable, 0, to_insert, axis=dim)\n\n    return variable\n\n\ndef sum_vars(variables: List[np.ndarray]) -> np.ndarray:\n    return sum(variables)\n\n\ndef concat_vars(variables: List[np.ndarray]) -> np.ndarray:\n    return np.concatenate(variables)\n\n\ndef squeeze(variables: List[np.ndarray]) -> np.ndarray:\n    if len(variables) != 1:\n        raise ValueError(""Squeeze only works with single vars. {} given.""\n                         .format(len(variables)))\n    return np.squeeze(variables[0])\n\n# pylint: disable=line-too-long\n# No point in line wrapping\nVARIABLE_MAP = {\n    ""encoder_input/embedding_matrix_0"": ([""Wemb""], emb_fix),\n    ""decoder/word_embeddings"": ([""Wemb_dec""], emb_fix),\n    ""decoder/state_to_word_W"": ([""ff_logit_W""], emb_fix_dim1),\n    ""decoder/state_to_word_b"": ([""ff_logit_b""], emb_fix),\n    ""encoder/bidirectional_rnn/fw/nematus_gru_cell/gates/state_proj/kernel"": ([""encoder_U""], None),\n    ""encoder/bidirectional_rnn/fw/nematus_gru_cell/gates/input_proj/kernel"": ([""encoder_W""], None),\n    ""encoder/bidirectional_rnn/fw/nematus_gru_cell/gates/input_proj/bias"": ([""encoder_b""], None),\n    ""encoder/bidirectional_rnn/fw/nematus_gru_cell/candidate/state_proj/kernel"": ([""encoder_Ux""], None),\n    ""encoder/bidirectional_rnn/fw/nematus_gru_cell/candidate/input_proj/kernel"": ([""encoder_Wx""], None),\n    ""encoder/bidirectional_rnn/fw/nematus_gru_cell/candidate/input_proj/bias"": ([""encoder_bx""], None),\n    ""encoder/bidirectional_rnn/bw/nematus_gru_cell/gates/state_proj/kernel"": ([""encoder_r_U""], None),\n    ""encoder/bidirectional_rnn/bw/nematus_gru_cell/gates/input_proj/kernel"": ([""encoder_r_W""], None),\n    ""encoder/bidirectional_rnn/bw/nematus_gru_cell/gates/input_proj/bias"": ([""encoder_r_b""], None),\n    ""encoder/bidirectional_rnn/bw/nematus_gru_cell/candidate/state_proj/kernel"": ([""encoder_r_Ux""], None),\n    ""encoder/bidirectional_rnn/bw/nematus_gru_cell/candidate/input_proj/kernel"": ([""encoder_r_Wx""], None),\n    ""encoder/bidirectional_rnn/bw/nematus_gru_cell/candidate/input_proj/bias"": ([""encoder_r_bx""], None),\n    ""decoder/initial_state/encoders_projection/kernel"": ([""ff_state_W""], None),\n    ""decoder/initial_state/encoders_projection/bias"": ([""ff_state_b""], None),\n    ""decoder/attention_decoder/nematus_gru_cell/gates/state_proj/kernel"": ([""decoder_U""], None),\n    ""decoder/attention_decoder/nematus_gru_cell/gates/input_proj/kernel"": ([""decoder_W""], None),\n    ""decoder/attention_decoder/nematus_gru_cell/gates/input_proj/bias"": ([""decoder_b""], None),\n    ""decoder/attention_decoder/nematus_gru_cell/candidate/state_proj/kernel"": ([""decoder_Ux""], None),\n    ""decoder/attention_decoder/nematus_gru_cell/candidate/input_proj/kernel"": ([""decoder_Wx""], None),\n    ""decoder/attention_decoder/nematus_gru_cell/candidate/input_proj/bias"": ([""decoder_bx""], None),\n    ""attention/attn_key_projection"": ([""decoder_Wc_att""], None),\n    ""attention/attn_projection_bias"": ([""decoder_b_att""], None),\n    ""attention/Attention/attn_query_projection"": ([""decoder_W_comb_att""], None),\n    ""attention/attn_similarity_v"": ([""decoder_U_att""], squeeze),\n    ""attention/attn_bias"": ([""decoder_c_tt""], squeeze),\n    ""decoder/attention_decoder/cond_gru_2_cell/gates/state_proj/kernel"": ([""decoder_U_nl""], None),\n    ""decoder/attention_decoder/cond_gru_2_cell/gates/input_proj/kernel"": ([""decoder_Wc""], None),\n    ""decoder/attention_decoder/cond_gru_2_cell/gates/state_proj/bias"": ([""decoder_b_nl""], None),\n    ""decoder/attention_decoder/cond_gru_2_cell/candidate/state_proj/kernel"": ([""decoder_Ux_nl""], None),\n    ""decoder/attention_decoder/cond_gru_2_cell/candidate/input_proj/kernel"": ([""decoder_Wcx""], None),\n    ""decoder/attention_decoder/cond_gru_2_cell/candidate/state_proj/bias"": ([""decoder_bx_nl""], None),\n    ""decoder/attention_decoder/rnn_state/kernel"": ([""ff_logit_lstm_W""], None),\n    ""decoder/attention_decoder/rnn_state/bias"": ([""ff_logit_lstm_b""], None),\n    ""decoder/attention_decoder/prev_out/kernel"": ([""ff_logit_prev_W""], None),\n    ""decoder/attention_decoder/prev_out/bias"": ([""ff_logit_prev_b""], None),\n    ""decoder/attention_decoder/context/kernel"": ([""ff_logit_ctx_W""], None),\n    ""decoder/attention_decoder/context/bias"": ([""ff_logit_ctx_b""], None)\n}\n# pylint: enable=line-too-long\n\nENCODER_NAME = ""encoder""\nDECODER_NAME = ""decoder""\nATTENTION_NAME = ""attention""\n\n\ndef load_nematus_json(path: str) -> Dict:\n    with open(path, ""r"", encoding=""utf-8"") as f_json:\n        contents = json.load(f_json)\n\n    prefix = os.path.realpath(os.path.dirname(path))\n    config = {\n        ""encoder_type"": contents[""encoder""],\n        ""decoder_type"": contents[""decoder""],\n        ""n_words_src"": contents[""n_words_src""],\n        ""n_words_tgt"": contents[""n_words""],\n        ""variables_file"": contents[""saveto""],\n        ""rnn_size"": contents[""dim""],\n        ""embedding_size"": contents[""dim_word""],\n        ""src_vocabulary"": os.path.join(\n            prefix, contents[""dictionaries""][0]),\n        ""tgt_vocabulary"": os.path.join(\n            prefix, contents[""dictionaries""][1]),\n        ""max_length"": contents[""maxlen""]\n    }\n\n    if config[""encoder_type""] != ""gru"":\n        raise ValueError(""Unsupported encoder type: {}""\n                         .format(config[""encoder_type""]))\n\n    if config[""decoder_type""] != ""gru_cond"":\n        raise ValueError(""Unsupported decoder type: {}""\n                         .format(config[""decoder_type""]))\n\n    if not os.path.isfile(config[""src_vocabulary""]):\n        raise FileNotFoundError(""Vocabulary file not found: {}""\n                                .format(config[""src_vocabulary""]))\n\n    if not os.path.isfile(config[""tgt_vocabulary""]):\n        raise FileNotFoundError(""Vocabulary file not found: {}""\n                                .format(config[""tgt_vocabulary""]))\n\n    return config\n\n\nVOCABULARY_TEMPLATE = """"""\\\n[vocabulary_{}]\nclass=vocabulary.from_nematus_json\npath=""{}""\nmax_size={}\npad_to_max_size=True\n""""""\n\nENCODER_TEMPLATE = """"""\\\n[encoder]\nclass=encoders.RecurrentEncoder\nname=""{}""\ninput_sequence=<input_sequence>\nrnn_size={}\nrnn_cell=""NematusGRU""\ndropout_keep_prob=1.0\n\n[input_sequence]\nclass=model.sequence.EmbeddedSequence\nname=""{}""\nvocabulary=<vocabulary_src>\ndata_id=""source""\nembedding_size={}\nmax_length={}\nadd_end_symbol=True\n""""""\n\n\ndef build_encoder(config: Dict) -> Tuple[RecurrentEncoder, str]:\n    vocabulary = from_nematus_json(\n        config[""src_vocabulary""], max_size=config[""n_words_src""],\n        pad_to_max_size=True)\n\n    vocabulary_ini = VOCABULARY_TEMPLATE.format(\n        ""src"", config[""src_vocabulary""], config[""n_words_src""])\n\n    inp_seq_name = ""{}_input"".format(ENCODER_NAME)\n    inp_seq = EmbeddedSequence(\n        name=inp_seq_name,\n        vocabulary=vocabulary,\n        data_id=""source"",\n        embedding_size=config[""embedding_size""])\n\n    encoder = RecurrentEncoder(\n        name=ENCODER_NAME,\n        input_sequence=inp_seq,\n        rnn_size=config[""rnn_size""],\n        rnn_cell=""NematusGRU"")\n\n    encoder_ini = ENCODER_TEMPLATE.format(\n        ENCODER_NAME, config[""rnn_size""],\n        inp_seq_name, config[""embedding_size""], config[""max_length""])\n\n    return encoder, ""\\n"".join([vocabulary_ini, encoder_ini])\n\n\nATTENTION_TEMPLATE = """"""\\\n[attention]\nclass=attention.Attention\nname=""{}""\nencoder=<encoder>\ndropout_keep_prob=1.0\n""""""\n\n\ndef build_attention(config: Dict,\n                    encoder: RecurrentEncoder) -> Tuple[Attention, str]:\n    attention = Attention(\n        name=ATTENTION_NAME,\n        encoder=encoder)\n\n    attention_ini = ATTENTION_TEMPLATE.format(ATTENTION_NAME)\n\n    return attention, attention_ini\n\n\nDECODER_TEMPLATE = """"""\\\n[decoder]\nclass=decoders.Decoder\nname=""{}""\nvocabulary=<vocabulary_tgt>\ndata_id=""target""\nembedding_size={}\nrnn_size={}\nmax_output_len={}\nencoders=[<encoder>]\nencoder_projection=<nematus_mean>\nattentions=[<attention>]\nattention_on_input=False\nconditional_gru=True\noutput_projection=<nematus_nonlinear>\nrnn_cell=""NematusGRU""\ndropout_keep_prob=1.0\n\n[nematus_nonlinear]\nclass=decoders.output_projection.nematus_output\noutput_size={}\ndropout_keep_prob=1.0\n\n[nematus_mean]\nclass=decoders.encoder_projection.nematus_projection\ndropout_keep_prob=1.0\n""""""\n\n\ndef build_decoder(config: Dict,\n                  attention: Attention,\n                  encoder: RecurrentEncoder) -> Tuple[Decoder, str]:\n    vocabulary = from_nematus_json(\n        config[""tgt_vocabulary""],\n        max_size=config[""n_words_tgt""],\n        pad_to_max_size=True)\n\n    vocabulary_ini = VOCABULARY_TEMPLATE.format(\n        ""tgt"", config[""tgt_vocabulary""], config[""n_words_tgt""])\n\n    decoder = Decoder(\n        name=DECODER_NAME,\n        vocabulary=vocabulary,\n        data_id=""target"",\n        max_output_len=config[""max_length""],\n        embedding_size=config[""embedding_size""],\n        rnn_size=config[""rnn_size""],\n        encoders=[encoder],\n        attentions=[attention],\n        attention_on_input=False,\n        conditional_gru=True,\n        encoder_projection=nematus_projection(dropout_keep_prob=1.0),\n        output_projection=nematus_output(config[""embedding_size""]),\n        rnn_cell=""NematusGRU"")\n\n    decoder_ini = DECODER_TEMPLATE.format(\n        DECODER_NAME, config[""embedding_size""], config[""rnn_size""],\n        config[""max_length""], config[""embedding_size""])\n\n    return decoder, ""\\n"".join([vocabulary_ini, decoder_ini])\n\n\ndef build_model(config: Dict) -> Tuple[\n        RecurrentEncoder, Attention, Decoder, str]:\n    encoder, encoder_cfg = build_encoder(config)\n    attention, attention_cfg = build_attention(config, encoder)\n    decoder, decoder_cfg = build_decoder(config, attention, encoder)\n\n    ini = ""\\n"".join([encoder_cfg, attention_cfg, decoder_cfg])\n\n    return ini\n\n\ndef load_nematus_file(path: str) -> Dict[str, np.ndarray]:\n    contents = np.load(path)\n    cnt_dict = dict(contents)\n    contents.close()\n    return cnt_dict\n\n\ndef assign_vars(variables: Dict[str, np.ndarray]) -> List[tf.Tensor]:\n    """"""For each variable in the map, assign the value from the dict""""""\n\n    trainable_vars = tf.trainable_variables()\n    assign_ops = []\n\n    for var in trainable_vars:\n        map_key = var.op.name\n\n        if map_key not in VARIABLE_MAP:\n            raise ValueError(""Map key {} not in variable map"".format(map_key))\n\n        nem_var_list, fun = VARIABLE_MAP[map_key]\n\n        for nem_var in nem_var_list:\n            if nem_var not in variables:\n                raise ValueError(""Alleged nematus var {} not found in loaded ""\n                                 ""nematus vars."".format(nem_var))\n\n        if fun is None:\n            if len(nem_var_list) != 1:\n                raise ValueError(\n                    ""Var list for map key {} must have length 1. ""\n                    ""Length {} found instead.""\n                    .format(map_key, len(nem_var_list)))\n\n            to_assign = variables[nem_var_list[0]]\n        else:\n            to_assign = fun([variables[v] for v in nem_var_list])\n\n        check_shape(var, to_assign)\n        assign_ops.append(tf.assign(var, to_assign))\n\n    return assign_ops\n\n\nINI_HEADER = """"""\\\n; This is an automatically generated configuration file\n; for running imported nematus model\n; For further training, set the configuration as appropriate\n\n[main]\nname=""nematus imported translation""\ntf_manager=<tf_manager>\noutput=""{}""\nrunners=[<runner>]\npostprocess=None\nevaluation=[(""target"", evaluators.bleu.BLEU)]\nrunners_batch_size=1\nrandom_seed=1234\n\n; TODO Set these additional attributes for further training\n; batch_size=80\n; epochs=10\n; train_dataset=<train_data>\n; val_dataset=<val_data>\n; trainer=<trainer>\n; logging_period=20\n; validation_period=60\n\n; [train_data]\n; class=dataset.load_dataset_from_files\n; s_source=""PATH/TO/DATA"" ; TODO do not forget to fill this out!\n; s_target=""PATH/TO/DATA"" ; TODO do not forget to fill this out!\n; lazy=True\n\n; [val_data]\n; class=dataset.load_dataset_from_files\n; s_source=""PATH/TO/DATA"" ; TODO do not forget to fill this out!\n; s_target=""PATH/TO/DATA"" ; TODO do not forget to fill this out!\n\n; [trainer]\n; class=trainers.cross_entropy_trainer.CrossEntropyTrainer\n; decoders=[<decoder>]\n; l2_weight=1.0e-8\n; clip_norm=1.0\n\n[tf_manager]\nclass=tf_manager.TensorFlowManager\nnum_threads=4\nnum_sessions=1\n\n[runner]\nclass=runners.runner.GreedyRunner\ndecoder=<decoder>\noutput_series=""target""\n""""""\n\n\ndef write_config(experiment_dir: str, ini: str) -> None:\n    experiment_file = os.path.join(experiment_dir, ""experiment.ini"")\n    with open(experiment_file, ""w"", encoding=""utf-8"") as f_out:\n        f_out.write(INI_HEADER.format(experiment_dir))\n        f_out.write(ini)\n\n\ndef prepare_output_dir(output_dir: str) -> bool:\n    if os.path.isdir(output_dir):\n        log(""Directory {} already exists. Choose a nonexistent one."".\n            format(output_dir))\n        exit(1)\n\n    os.mkdir(output_dir)\n\n\ndef main() -> None:\n    log(""Script started."")\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(""nematus_json"", metavar=""NEMATUS-JSON"",\n                        help=""nematus json file"")\n    parser.add_argument(""nematus_variables"", metavar=""NEMATUS-FILE"",\n                        help=""nematus variable file"")\n    parser.add_argument(""output_dir"", metavar=""OUTPUT-DIR"",\n                        help=""output directory"")\n    args = parser.parse_args()\n\n    log(""Loading nematus variables from {}."".format(args.nematus_variables))\n    nematus_vars = load_nematus_file(args.nematus_variables)\n\n    log(""Loading nematus JSON config from {}."".format(args.nematus_json))\n    nematus_json_cfg = load_nematus_json(args.nematus_json)\n\n    log(""Bulding model."")\n    ini = build_model(nematus_json_cfg)\n\n    log(""Defining assign Ops."")\n    assign_ops = assign_vars(nematus_vars)\n\n    log(""Preparing output directory {}"".format(args.output_dir))\n    prepare_output_dir(args.output_dir)\n\n    log(""Writing configuration file to {}/experiment.ini.""\n        .format(args.output_dir))\n    write_config(args.output_dir, ini)\n\n    log(""Creating TF session."")\n    s = tf.Session()\n\n    log(""Running session to assign to Neural Monkey variables."")\n    s.run(assign_ops)\n\n    log(""Initializing saver."")\n    saver = tf.train.Saver()\n\n    variables_file = os.path.join(args.output_dir, ""variables.data"")\n    log(""Saving variables to {}"".format(variables_file))\n    saver.save(s, variables_file)\n\n    log(""Finished."")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/import_transformer.1.2.9.py,16,"b'#!/usr/bin/env python3\n""""""Import transformer model checkpoint file and convert it into a neural monkey experiment\ngiven a neural monkey configuration file.\n\nTested with version 1.2.9\n""""""\nfrom typing import Dict, Tuple, List\nimport argparse\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport json\n\nfrom neuralmonkey.logging import log as _log\nfrom neuralmonkey.vocabulary import from_t2t_vocabulary, Vocabulary\nfrom neuralmonkey.encoders.transformer import TransformerEncoder\nfrom neuralmonkey.decoders.transformer import TransformerDecoder\nfrom neuralmonkey.model.sequence import EmbeddedSequence\n\nENCODER_NAME = ""encoder""\nDECODER_NAME = ""decoder""\n\nVOCABULARY_TEMPLATE = """"""\\\n[vocabulary]\nclass=vocabulary.from_t2t_vocabulary\npath=""{}""\n""""""\n\nENCODER_TEMPLATE = """"""\\\n[input_sequence]\nclass=model.sequence.EmbeddedSequence\nname=""{}""\nvocabulary=<vocabulary>\ndata_id=""source_wp""\nembedding_size={}\nscale_embeddings_by_depth={}\nmax_length={}\nadd_end_symbol=True\n\n[encoder]\nclass=encoders.transformer.TransformerEncoder\nname=""{}""\ninput_sequence=<input_sequence>\nff_hidden_size={}\ndepth={}\nn_heads={}\ndropout_keep_prob=1.0\nattention_dropout_keep_prob=1.0\n; See Problem registry specification to set correct value\ntarget_space_id=21\nuse_att_transform_bias=True\n""""""\n\nDECODER_TEMPLATE = """"""\\\n[decoder]\nclass=decoders.transformer.TransformerDecoder\nname=""{}""\nvocabulary=<vocabulary>\ndata_id=""target""\nencoder=<encoder>\nff_hidden_size={}\nn_heads_self={}\nn_heads_enc={}\ndepth={}\nembedding_size={}\nembeddings_source=<input_sequence>\nmax_output_len=50\ndropout_keep_prob=1.0\nattention_dropout_keep_prob=1.0\nuse_att_transform_bias=True\n""""""\n\nINI_HEADER = """"""\\\n; This is an automatically generated configuration file\n; for running imported nematus model\n; For further training, set the configuration as appropriate\n\n[main]\nname=""t2t-transformer imported translation""\ntf_manager=<tf_manager>\noutput=""{}""\nrunners=[<runner>]\nevaluation=[(""target"", evaluators.bleu.BLEU)]\nrunners_batch_size=1\n\n; TODO Set these additional attributes for further training\n; batch_size=80\n; epochs=10\n; train_dataset=<train_data>\n; val_dataset=<val_data>\n; trainer=<trainer>\n; logging_period=20\n; validation_period=60\n; random_seed=1234\n\n[wp_preprocess]\nclass=processors.wordpiece.WordpiecePreprocessor\nvocabulary=<vocabulary>\n\n; [train_data]\n; class=dataset.load_dataset_from_files\n; s_source=(""PATH/TO/DATA"", readers.plain_text_reader.T2TReader); TODO do not forget to fill this out!\n; s_target=(""PATH/TO/DATA"", readers.plain_text_reader.T2TReader); TODO do not forget to fill this out!\n; preprocessors=[(""source"", ""source_wp"", <wp_preprocess>), (""target"", ""target_wp"", <wp_postprocess>)]\n; lazy=True\n\n; [val_data]\n; class=dataset.load_dataset_from_files\n; s_source=(""PATH/TO/DATA"", readers.plain_text_reader.T2TReader); TODO do not forget to fill this out!\n; s_target=(""PATH/TO/DATA"", readers.plain_text_reader.T2TReader); TODO do not forget to fill this out!\n; preprocessors=[(""source"", ""source_wp"", <wp_preprocess>), (""target"", ""target_wp"", <wp_postprocess>)]\n\n; [trainer]\n; class=trainers.cross_entropy_trainer.CrossEntropyTrainer\n; decoders=[<decoder>]\n; l2_weight=1.0e-8\n; clip_norm=1.0\n\n[tf_manager]\nclass=tf_manager.TensorFlowManager\nnum_threads=4\nnum_sessions=1\n\n[runner]\nclass=runners.runner.GreedyRunner\ndecoder=<decoder>\npostprocess=processors.wordpiece.WordpiecePostprocessor\noutput_series=""target""\n\n""""""\n\n\n# pylint: disable=line-too-long\n# No point in line wrapping\ndef create_variable_map(hparams: Dict, np_vars) -> Dict:\n\n    # Always present\n    var_map = {\n        ""encoder_input/embedding_matrix_0"": (get_shared_emb_vars(np_vars), emb_fix),\n        ""encoder/target_modality_embedding_matrix"": ([""body/target_space_embedding/kernel""], None),\n        ""encoder/LayerNorm/beta"": ([""body/encoder/layer_prepostprocess/layer_norm/layer_norm_bias""], None),\n        ""encoder/LayerNorm/gamma"": ([""body/encoder/layer_prepostprocess/layer_norm/layer_norm_scale""], None),\n        ""decoder/LayerNorm/beta"": ([""body/decoder/layer_prepostprocess/layer_norm/layer_norm_bias""], None),\n        ""decoder/LayerNorm/gamma"": ([""body/decoder/layer_prepostprocess/layer_norm/layer_norm_scale""], None)\n    }\n\n    for i in range(hparams[""depth""]):\n        # Encoder\n        var_map.update({\n            ""encoder/layer_{}/self_attention/query_proj/kernel"".format(i): ([""body/encoder/layer_{}/self_attention/multihead_attention/qkv_transform_single/kernel"".format(i)], create_transform_matrix_getter(3, 0)),\n            ""encoder/layer_{}/self_attention/query_proj/bias"".format(i): ([""body/encoder/layer_{}/self_attention/multihead_attention/qkv_transform_single/bias"".format(i)], create_transform_matrix_getter(3, 0)),\n            ""encoder/layer_{}/self_attention/keys_proj/kernel"".format(i): ([""body/encoder/layer_{}/self_attention/multihead_attention/qkv_transform_single/kernel"".format(i)], create_transform_matrix_getter(3, 1)),\n            ""encoder/layer_{}/self_attention/keys_proj/bias"".format(i): ([""body/encoder/layer_{}/self_attention/multihead_attention/qkv_transform_single/bias"".format(i)], create_transform_matrix_getter(3, 1)),\n            ""encoder/layer_{}/self_attention/vals_proj/kernel"".format(i): ([""body/encoder/layer_{}/self_attention/multihead_attention/qkv_transform_single/kernel"".format(i)], create_transform_matrix_getter(3, 2)),\n            ""encoder/layer_{}/self_attention/vals_proj/bias"".format(i): ([""body/encoder/layer_{}/self_attention/multihead_attention/qkv_transform_single/bias"".format(i)], create_transform_matrix_getter(3, 2)),\n            ""encoder/layer_{}/self_attention/output_proj/kernel"".format(i): ([""body/encoder/layer_{}/self_attention/multihead_attention/output_transform_single/kernel"".format(i)], reshape4d2d),\n            ""encoder/layer_{}/self_attention/output_proj/bias"".format(i): ([""body/encoder/layer_{}/self_attention/multihead_attention/output_transform_single/bias"".format(i)], None),\n            ""encoder/layer_{}/self_attention/LayerNorm/beta"".format(i): ([""body/encoder/layer_{}/self_attention/layer_prepostprocess/layer_norm/layer_norm_bias"".format(i)], None),\n            ""encoder/layer_{}/self_attention/LayerNorm/gamma"".format(i): ([""body/encoder/layer_{}/self_attention/layer_prepostprocess/layer_norm/layer_norm_scale"".format(i)], None),\n            ""encoder/layer_{}/feedforward/hidden_state/kernel"".format(i): ([""body/encoder/layer_{}/ffn/conv_hidden_relu/conv1_single/kernel"".format(i)], reshape4d2d),\n            ""encoder/layer_{}/feedforward/hidden_state/bias"".format(i): ([""body/encoder/layer_{}/ffn/conv_hidden_relu/conv1_single/bias"".format(i)], None),\n            ""encoder/layer_{}/feedforward/output/kernel"".format(i): ([""body/encoder/layer_{}/ffn/conv_hidden_relu/conv2_single/kernel"".format(i)], reshape4d2d),\n            ""encoder/layer_{}/feedforward/output/bias"".format(i): ([""body/encoder/layer_{}/ffn/conv_hidden_relu/conv2_single/bias"".format(i)], None),\n            ""encoder/layer_{}/feedforward/LayerNorm/beta"".format(i): ([""body/encoder/layer_{}/ffn/layer_prepostprocess/layer_norm/layer_norm_bias"".format(i)], None),\n            ""encoder/layer_{}/feedforward/LayerNorm/gamma"".format(i): ([""body/encoder/layer_{}/ffn/layer_prepostprocess/layer_norm/layer_norm_scale"".format(i)], None)})\n\n        # Decoder\n        var_map.update({\n            ""decoder/layer_{}/encdec_attention/query_proj/kernel"".format(i): ([""body/decoder/layer_{}/encdec_attention/multihead_attention/q_transform_single/kernel"".format(i)], reshape4d2d),\n            ""decoder/layer_{}/encdec_attention/query_proj/bias"".format(i): ([""body/decoder/layer_{}/encdec_attention/multihead_attention/q_transform_single/bias"".format(i)], None),\n            ""decoder/layer_{}/encdec_attention/keys_proj/kernel"".format(i): ([""body/decoder/layer_{}/encdec_attention/multihead_attention/kv_transform_single/kernel"".format(i)], create_transform_matrix_getter(2, 0)),\n            ""decoder/layer_{}/encdec_attention/keys_proj/bias"".format(i): ([""body/decoder/layer_{}/encdec_attention/multihead_attention/kv_transform_single/bias"".format(i)], create_transform_matrix_getter(2, 0)),\n            ""decoder/layer_{}/encdec_attention/vals_proj/kernel"".format(i): ([""body/decoder/layer_{}/encdec_attention/multihead_attention/kv_transform_single/kernel"".format(i)], create_transform_matrix_getter(2, 1)),\n            ""decoder/layer_{}/encdec_attention/vals_proj/bias"".format(i): ([""body/decoder/layer_{}/encdec_attention/multihead_attention/kv_transform_single/bias"".format(i)], create_transform_matrix_getter(2, 1)),\n            ""decoder/layer_{}/encdec_attention/output_proj/kernel"".format(i): ([""body/decoder/layer_{}/encdec_attention/multihead_attention/output_transform_single/kernel"".format(i)], reshape4d2d),\n            ""decoder/layer_{}/encdec_attention/output_proj/bias"".format(i): ([""body/decoder/layer_{}/encdec_attention/multihead_attention/output_transform_single/bias"".format(i)], None),\n            ""decoder/layer_{}/encdec_attention/LayerNorm/beta"".format(i): ([""body/decoder/layer_{}/encdec_attention/layer_prepostprocess/layer_norm/layer_norm_bias"".format(i)], None),\n            ""decoder/layer_{}/encdec_attention/LayerNorm/gamma"".format(i): ([""body/decoder/layer_{}/encdec_attention/layer_prepostprocess/layer_norm/layer_norm_scale"".format(i)], None),\n            ""decoder/layer_{}/self_attention/query_proj/kernel"".format(i): ([""body/decoder/layer_{}/self_attention/multihead_attention/qkv_transform_single/kernel"".format(i)], create_transform_matrix_getter(3, 0)),\n            ""decoder/layer_{}/self_attention/query_proj/bias"".format(i): ([""body/decoder/layer_{}/self_attention/multihead_attention/qkv_transform_single/bias"".format(i)], create_transform_matrix_getter(3, 0)),\n            ""decoder/layer_{}/self_attention/keys_proj/kernel"".format(i): ([""body/decoder/layer_{}/self_attention/multihead_attention/qkv_transform_single/kernel"".format(i)], create_transform_matrix_getter(3, 1)),\n            ""decoder/layer_{}/self_attention/keys_proj/bias"".format(i): ([""body/decoder/layer_{}/self_attention/multihead_attention/qkv_transform_single/bias"".format(i)], create_transform_matrix_getter(3, 1)),\n            ""decoder/layer_{}/self_attention/vals_proj/kernel"".format(i): ([""body/decoder/layer_{}/self_attention/multihead_attention/qkv_transform_single/kernel"".format(i)], create_transform_matrix_getter(3, 2)),\n            ""decoder/layer_{}/self_attention/vals_proj/bias"".format(i): ([""body/decoder/layer_{}/self_attention/multihead_attention/qkv_transform_single/bias"".format(i)], create_transform_matrix_getter(3, 2)),\n            ""decoder/layer_{}/self_attention/output_proj/kernel"".format(i): ([""body/decoder/layer_{}/self_attention/multihead_attention/output_transform_single/kernel"".format(i)], reshape4d2d),\n            ""decoder/layer_{}/self_attention/output_proj/bias"".format(i): ([""body/decoder/layer_{}/self_attention/multihead_attention/output_transform_single/bias"".format(i)], None),\n            ""decoder/layer_{}/self_attention/LayerNorm/beta"".format(i): ([""body/decoder/layer_{}/self_attention/layer_prepostprocess/layer_norm/layer_norm_bias"".format(i)], None),\n            ""decoder/layer_{}/self_attention/LayerNorm/gamma"".format(i): ([""body/decoder/layer_{}/self_attention/layer_prepostprocess/layer_norm/layer_norm_scale"".format(i)], None),\n            ""decoder/layer_{}/feedforward/hidden_state/kernel"".format(i): ([""body/decoder/layer_{}/ffn/conv_hidden_relu/conv1_single/kernel"".format(i)], reshape4d2d),\n            ""decoder/layer_{}/feedforward/hidden_state/bias"".format(i): ([""body/decoder/layer_{}/ffn/conv_hidden_relu/conv1_single/bias"".format(i)], None),\n            ""decoder/layer_{}/feedforward/output/kernel"".format(i): ([""body/decoder/layer_{}/ffn/conv_hidden_relu/conv2_single/kernel"".format(i)], reshape4d2d),\n            ""decoder/layer_{}/feedforward/output/bias"".format(i): ([""body/decoder/layer_{}/ffn/conv_hidden_relu/conv2_single/bias"".format(i)], None),\n            ""decoder/layer_{}/feedforward/LayerNorm/beta"".format(i): ([""body/decoder/layer_{}/ffn/layer_prepostprocess/layer_norm/layer_norm_bias"".format(i)], None),\n            ""decoder/layer_{}/feedforward/LayerNorm/gamma"".format(i): ([""body/decoder/layer_{}/ffn/layer_prepostprocess/layer_norm/layer_norm_scale"".format(i)], None)})\n\n    return var_map\n# pylint: enable=line-too-long\n\n\ndef log(message: str, color: str = ""blue"") -> None:\n    _log(message, color)\n\ndef check_shape(var1_tf: tf.Variable, var2_np: np.ndarray):\n    if var1_tf.get_shape().as_list() != list(var2_np.shape):\n        log(""Shapes do not match! Exception will follow."", color=""red"")\n\ndef get_shared_emb_vars(np_vars: Dict) -> List[str]:\n    modality_vars = [var for var in np_vars if ""symbol_modality"" in var]\n    modality_prefix = modality_vars[0].split(""/"")[:-1]\n    sorted_vars = []\n    for i in range(len(modality_vars)):\n        modality_arr = modality_prefix + [""weights_{}"".format(i)]\n        sorted_vars.append(""/"".join(modality_arr))\n    return sorted_vars\n    \n\ndef emb_fix(variables: List[tf.Tensor]) -> tf.Tensor:\n    """"""Concat sharded embedding matrix and include NMonkey special symbols.\n\n    We need to include embeddings for \n    """"""\n    concat = np.concatenate(variables, axis=0)\n    concat_split = np.split(concat, [1, 2], axis=0)\n\n    emb_shape = concat.shape[-1]\n    return np.concatenate([concat_split[0],\n                           np.zeros([1, emb_shape]),\n                           concat_split[1],\n                           np.zeros([1, emb_shape]),\n                           concat_split[2]], axis=0)\n\ndef reshape4d2d(variables: List[tf.Tensor]) -> tf.Tensor:\n    return tf.squeeze(variables[0], [0, 1])\n\ndef create_transform_matrix_getter(num_of_splits: int, matrix_pos: int) -> tf.Tensor:\n\n    def get_transform_matrix(variables: List[tf.Tensor]):\n        matrix = variables[0]\n        if len(matrix.shape) > 2:\n            matrix = tf.squeeze(matrix, [0, 1])\n        matrix_split = tf.split(matrix, num_of_splits, axis=-1)\n\n        return matrix_split[matrix_pos]\n\n    return get_transform_matrix\n\n\ndef build_encoder(hparams: Dict,\n                  vocab_path: str) -> Tuple[TransformerEncoder, str]:\n    vocabulary = from_t2t_vocabulary(vocab_path)\n    vocabulary_ini = VOCABULARY_TEMPLATE.format(vocab_path)\n\n    inp_seq_name = ""{}_input"".format(ENCODER_NAME)\n    inp_seq = EmbeddedSequence(\n        name=inp_seq_name,\n        vocabulary=vocabulary,\n        data_id=""source_wp"",\n        embedding_size=hparams[""embedding_size""],\n        scale_embeddings_by_depth=hparams[\n            ""multiply_embedding_mode""] == ""sqrt_depth"",\n        add_end_symbol=True)\n\n    encoder = TransformerEncoder(\n        name=ENCODER_NAME,\n        input_sequence=inp_seq,\n        ff_hidden_size=hparams[""ff_hidden_size""],\n        depth=hparams[""depth""],\n        n_heads=hparams[""n_heads""],\n        target_space_id=21,\n        use_att_transform_bias=True)\n\n    encoder_ini = ENCODER_TEMPLATE.format(\n        inp_seq_name, hparams[""embedding_size""],\n        hparams[""multiply_embedding_mode""] == ""sqrt_depth"",\n        hparams[""max_length""],\n        ENCODER_NAME, hparams[""ff_hidden_size""], hparams[""depth""],\n        hparams[""n_heads""])\n\n    return encoder, vocabulary, ""\\n"".join([vocabulary_ini, encoder_ini])\n\n\ndef build_decoder(hparams: Dict,\n                  encoder: TransformerEncoder,\n                  vocab: Vocabulary) -> Tuple[\n        TransformerDecoder, str]:\n    decoder = TransformerDecoder(\n        name=DECODER_NAME,\n        encoder=encoder,\n        vocabulary=vocab,\n        data_id=""target_wp"",\n        ff_hidden_size=hparams[""ff_hidden_size""],\n        n_heads_self=hparams[""n_heads""],\n        n_heads_enc=hparams[""n_heads""],\n        depth=hparams[""depth""],\n        embeddings_source=encoder.input_sequence,\n        embedding_size=hparams[""embedding_size""],\n        max_output_len=50,\n        use_att_transform_bias=True)\n\n    decoder_ini = DECODER_TEMPLATE.format(\n        DECODER_NAME, hparams[""ff_hidden_size""], hparams[""n_heads""],\n        hparams[""n_heads""], hparams[""depth""], hparams[""embedding_size""])\n\n    return decoder, ""\\n"".join([decoder_ini])\n\n\ndef build_model(hparams: Dict,\n                vocab_path: str) -> Tuple[\n        TransformerEncoder, TransformerDecoder, str]:\n    encoder, vocab, encoder_cfg = build_encoder(hparams, vocab_path)\n    decocer, decoder_cfg = build_decoder(hparams, encoder, vocab)\n\n    ini = ""\\n"".join([encoder_cfg, decoder_cfg])\n\n    return ini\n\n\ndef load_hparams(path: str) -> Dict:\n    """"""Open a JSON file containing model transformer model hyperparameters.""""""\n\n    with open(path, ""r"", encoding=""utf-8"") as f_json:\n        contents = json.load(f_json)\n\n    hparams = {\n        ""n_heads"": contents[""num_heads""],\n        ""ff_hidden_size"": contents[""filter_size""],\n        ""embedding_size"": contents[""hidden_size""],\n        ""max_length"": contents[""max_length""],\n        ""label_smoothing"": contents[""label_smoothing""],\n        ""depth"": contents[""num_hidden_layers""],\n        ""multiply_embedding_mode"": contents[""multiply_embedding_mode""]\n    }\n\n    # TODO: check, whether the hparams that we do not set in NeuralMonkey\n    # are set to correct values\n\n    return hparams\n\ndef get_assign_ops(hparams: Dict, np_vars: Dict) -> List[tf.Tensor]:\n    """"""Create assign operations from transformer model to neuralmonkey.\n\n    The assign operations are used to load variables from the transformer\n    model to their correct parts in the graph created by Neural Monkey.\n    """"""\n\n    trainable_vars = tf.trainable_variables()\n    assign_ops = []\n\n    var_map = create_variable_map(hparams, np_vars)\n    for var in trainable_vars:\n        map_key = var.op.name\n\n        if map_key not in var_map:\n            raise ValueError(""Map key {} not in variable map"".format(map_key))\n\n        t2t_var_list, fun = var_map[map_key]\n\n        for t2t_var in t2t_var_list:\n            if t2t_var not in np_vars:\n                raise ValueError(""Alleged transformer var {} not found ""\n                                 ""in loaded transformer vars. For neuralmonkey""\n                                 "" var {}."".format(t2t_var, map_key))\n\n        if fun is None:\n            if len(t2t_var_list) != 1:\n                raise ValueError(\n                    ""Var list for map key {} must have length 1. ""\n                    ""Length {} found instead.""\n                    .format(map_key, len(t2t_var_list)))\n            to_assign = np_vars[t2t_var_list[0]]\n        else:\n            to_assign = fun([np_vars[v] for v in t2t_var_list])\n\n        check_shape(var, to_assign)\n        assign_ops.append(tf.assign(var, to_assign))\n\n    return assign_ops\n\n\ndef write_config(experiment_dir: str, ini: str) -> None:\n    experiment_file = os.path.join(experiment_dir, ""experiment.ini"")\n    with open(experiment_file, ""w"", encoding=""utf-8"") as f_out:\n        f_out.write(INI_HEADER.format(experiment_dir))\n        f_out.write(ini)\n\n\ndef prepare_output_dir(output_dir: str) -> bool:\n    if os.path.isdir(output_dir):\n        log(""Directory {} already exists. Choose a nonexistent one."".\n            format(output_dir))\n        exit(1)\n\n    os.mkdir(output_dir)\n\n\ndef main() -> None:\n    log(""Script started."")\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(""--t2t-checkpoint"", metavar=""T2T-CHECKPOINT"",\n                        help=""t2t checkpoint file"")\n    parser.add_argument(""--t2t-hparams"", metavar=""T2T-HPARAMS"",\n                        help=""t2t hparams json file"")\n    parser.add_argument(""--vocabulary"", metavar=""VOCABULARY"",\n                        help=""vocabulary file"")\n    parser.add_argument(""--output-dir"", metavar=""OUTPUT-DIR"",\n                        help=""output directory"")\n    args = parser.parse_args()\n\n    ckpt = args.t2t_checkpoint\n\n    log(""Loading transformer hparams JSON from {}."".format(args.t2t_hparams))\n    hparams = load_hparams(args.t2t_hparams)\n\n    log(""Bulding model."")\n    ini = build_model(hparams, args.vocabulary)\n\n    log(""Read from checkpoint {}."".format(ckpt))\n    t2t_var_list = tf.contrib.framework.list_variables(ckpt)\n    t2t_reader = tf.contrib.framework.load_checkpoint(ckpt)\n    t2t_var_values = {}\n    for (name, shape) in t2t_var_list:\n        if name.startswith(""training""):\n            continue\n        t2t_var_values[name] = t2t_reader.get_tensor(name)\n\n    log(""Defining assign_ops."")\n    assign_ops = get_assign_ops(hparams, t2t_var_values)\n\n    log(""Preparing output directory {}."".format(args.output_dir))\n    prepare_output_dir(args.output_dir)\n\n    log(""Writing configuration file to {}/experiment.ini.""\n        .format(args.output_dir))\n    write_config(args.output_dir, ini)\n\n    log(""Creating TF session."")\n    s =  tf.Session()\n\n    log(""Running session to assign to Neural Monkey variables."")\n    s.run(assign_ops)\n\n    log(""Initializing saver."")\n    saver = tf.train.Saver()\n\n    variables_file = os.path.join(args.output_dir, ""variables.data"")\n    log(""Saving variables to {}."".format(variables_file))\n    saver.save(s, variables_file)\n\n    log(""Finished."")\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/import_transformer.1.3.2.py,10,"b'#!/usr/bin/env python3\n""""""Import transformer model checkpoint file and convert it into a neural monkey experiment\ngiven a neural monkey configuration file.\n\nTested with version 1.3.2\n""""""\nfrom typing import Dict, Tuple, List\nimport argparse\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport json\n\nfrom neuralmonkey.logging import log as _log\nfrom neuralmonkey.vocabulary import from_t2t_vocabulary, Vocabulary\nfrom neuralmonkey.encoders.transformer import TransformerEncoder\nfrom neuralmonkey.decoders.transformer import TransformerDecoder\nfrom neuralmonkey.model.sequence import EmbeddedSequence\n\nENCODER_NAME = ""encoder""\nDECODER_NAME = ""decoder""\n\nVOCABULARY_TEMPLATE = """"""\\\n[vocabulary]\nclass=vocabulary.from_t2t_vocabulary\npath=""{}""\n""""""\n\nENCODER_TEMPLATE = """"""\\\n[input_sequence]\nclass=model.sequence.EmbeddedSequence\nname=""{}""\nvocabulary=<vocabulary>\ndata_id=""source_wp""\nembedding_size={}\nscale_embeddings_by_depth={}\nmax_length={}\nadd_end_symbol=True\n\n[encoder]\nclass=encoders.transformer.TransformerEncoder\nname=""{}""\ninput_sequence=<input_sequence>\nff_hidden_size={}\ndepth={}\nn_heads={}\ndropout_keep_prob=1.0\nattention_dropout_keep_prob=1.0\n; See Problem registry specification to set correct value\ntarget_space_id=21\n""""""\n\nDECODER_TEMPLATE = """"""\\\n[decoder]\nclass=decoders.transformer.TransformerDecoder\nname=""{}""\nvocabulary=<vocabulary>\ndata_id=""target""\nencoder=<encoder>\nff_hidden_size={}\nn_heads_self={}\nn_heads_enc={}\ndepth={}\nembedding_size={}\nembeddings_source=<input_sequence>\nmax_output_len=50\ndropout_keep_prob=1.0\nattention_dropout_keep_prob=1.0\n""""""\n\nINI_HEADER = """"""\\\n; This is an automatically generated configuration file\n; for running imported nematus model\n; For further training, set the configuration as appropriate\n\n[main]\nname=""t2t-transformer imported translation""\ntf_manager=<tf_manager>\noutput=""{}""\nrunners=[<runner>]\nevaluation=[(""target"", evaluators.bleu.BLEU)]\nrunners_batch_size=1\n\n; TODO Set these additional attributes for further training\n; batch_size=80\n; epochs=10\n; train_dataset=<train_data>\n; val_dataset=<val_data>\n; trainer=<trainer>\n; logging_period=20\n; validation_period=60\n; random_seed=1234\n\n[wp_preprocess]\nclass=processors.wordpiece.WordpiecePreprocessor\nvocabulary=<vocabulary>\n\n; [train_data]\n; class=dataset.load_dataset_from_files\n; s_source=(""PATH/TO/DATA"", readers.plain_text_reader.T2TReader); TODO do not forget to fill this out!\n; s_target=(""PATH/TO/DATA"", readers.plain_text_reader.T2TReader); TODO do not forget to fill this out!\n; preprocessors=[(""source"", ""source_wp"", <wp_preprocess>), (""target"", ""target_wp"", <wp_postprocess>)]\n; lazy=True\n\n; [val_data]\n; class=dataset.load_dataset_from_files\n; s_source=(""PATH/TO/DATA"", readers.plain_text_reader.T2TReader); TODO do not forget to fill this out!\n; s_target=(""PATH/TO/DATA"", readers.plain_text_reader.T2TReader); TODO do not forget to fill this out!\n; preprocessors=[(""source"", ""source_wp"", <wp_preprocess>), (""target"", ""target_wp"", <wp_postprocess>)]\n\n; [trainer]\n; class=trainers.cross_entropy_trainer.CrossEntropyTrainer\n; decoders=[<decoder>]\n; l2_weight=1.0e-8\n; clip_norm=1.0\n\n[tf_manager]\nclass=tf_manager.TensorFlowManager\nnum_threads=4\nnum_sessions=1\n\n[runner]\nclass=runners.runner.GreedyRunner\ndecoder=<decoder>\npostprocess=processors.wordpiece.WordpiecePostprocessor\noutput_series=""target""\n\n""""""\n\n\n# pylint: disable=line-too-long\n# No point in line wrapping\ndef create_variable_map(hparams: Dict, np_vars) -> Dict:\n\n    # Always present\n    var_map = {\n        ""encoder_input/embedding_matrix_0"": (get_shared_emb_vars(np_vars), emb_fix),\n        ""encoder/target_modality_embedding_matrix"": ([""transformer/body/target_space_embedding/kernel""], None),\n        ""encoder/LayerNorm/beta"": ([""transformer/body/encoder/layer_prepostprocess/layer_norm/layer_norm_bias""], None),\n        ""encoder/LayerNorm/gamma"": ([""transformer/body/encoder/layer_prepostprocess/layer_norm/layer_norm_scale""], None),\n        ""decoder/LayerNorm/beta"": ([""transformer/body/decoder/layer_prepostprocess/layer_norm/layer_norm_bias""], None),\n        ""decoder/LayerNorm/gamma"": ([""transformer/body/decoder/layer_prepostprocess/layer_norm/layer_norm_scale""], None)\n    }\n\n    for i in range(hparams[""depth""]):\n        # Encoder\n        var_map.update({\n            ""encoder/layer_{}/self_attention/query_proj/kernel"".format(i): ([""transformer/body/encoder/layer_{}/self_attention/multihead_attention/q/kernel"".format(i)], None),\n            ""encoder/layer_{}/self_attention/keys_proj/kernel"".format(i): ([""transformer/body/encoder/layer_{}/self_attention/multihead_attention/k/kernel"".format(i)], None),\n            ""encoder/layer_{}/self_attention/vals_proj/kernel"".format(i): ([""transformer/body/encoder/layer_{}/self_attention/multihead_attention/v/kernel"".format(i)], None),\n            ""encoder/layer_{}/self_attention/output_proj/kernel"".format(i): ([""transformer/body/encoder/layer_{}/self_attention/multihead_attention/output_transform/kernel"".format(i)], None),\n            ""encoder/layer_{}/self_attention/LayerNorm/beta"".format(i): ([""transformer/body/encoder/layer_{}/self_attention/layer_prepostprocess/layer_norm/layer_norm_bias"".format(i)], None),\n            ""encoder/layer_{}/self_attention/LayerNorm/gamma"".format(i): ([""transformer/body/encoder/layer_{}/self_attention/layer_prepostprocess/layer_norm/layer_norm_scale"".format(i)], None),\n            ""encoder/layer_{}/feedforward/hidden_state/kernel"".format(i): ([""transformer/body/encoder/layer_{}/ffn/conv1/kernel"".format(i)], None),\n            ""encoder/layer_{}/feedforward/hidden_state/bias"".format(i): ([""transformer/body/encoder/layer_{}/ffn/conv1/bias"".format(i)], None),\n            ""encoder/layer_{}/feedforward/output/kernel"".format(i): ([""transformer/body/encoder/layer_{}/ffn/conv2/kernel"".format(i)], None),\n            ""encoder/layer_{}/feedforward/output/bias"".format(i): ([""transformer/body/encoder/layer_{}/ffn/conv2/bias"".format(i)], None),\n            ""encoder/layer_{}/feedforward/LayerNorm/beta"".format(i): ([""transformer/body/encoder/layer_{}/ffn/layer_prepostprocess/layer_norm/layer_norm_bias"".format(i)], None),\n            ""encoder/layer_{}/feedforward/LayerNorm/gamma"".format(i): ([""transformer/body/encoder/layer_{}/ffn/layer_prepostprocess/layer_norm/layer_norm_scale"".format(i)], None)})\n\n        # Decoder\n        var_map.update({\n            ""decoder/layer_{}/encdec_attention/query_proj/kernel"".format(i): ([""transformer/body/decoder/layer_{}/encdec_attention/multihead_attention/q/kernel"".format(i)], None),\n            ""decoder/layer_{}/encdec_attention/keys_proj/kernel"".format(i): ([""transformer/body/decoder/layer_{}/encdec_attention/multihead_attention/k/kernel"".format(i)], None),\n            ""decoder/layer_{}/encdec_attention/vals_proj/kernel"".format(i): ([""transformer/body/decoder/layer_{}/encdec_attention/multihead_attention/v/kernel"".format(i)], None),\n            ""decoder/layer_{}/encdec_attention/output_proj/kernel"".format(i): ([""transformer/body/decoder/layer_{}/encdec_attention/multihead_attention/output_transform/kernel"".format(i)], None),\n            ""decoder/layer_{}/encdec_attention/LayerNorm/beta"".format(i): ([""transformer/body/decoder/layer_{}/encdec_attention/layer_prepostprocess/layer_norm/layer_norm_bias"".format(i)], None),\n            ""decoder/layer_{}/encdec_attention/LayerNorm/gamma"".format(i): ([""transformer/body/decoder/layer_{}/encdec_attention/layer_prepostprocess/layer_norm/layer_norm_scale"".format(i)], None),\n            ""decoder/layer_{}/self_attention/query_proj/kernel"".format(i): ([""transformer/body/decoder/layer_{}/self_attention/multihead_attention/q/kernel"".format(i)], None),\n            ""decoder/layer_{}/self_attention/keys_proj/kernel"".format(i): ([""transformer/body/decoder/layer_{}/self_attention/multihead_attention/k/kernel"".format(i)], None),\n            ""decoder/layer_{}/self_attention/vals_proj/kernel"".format(i): ([""transformer/body/decoder/layer_{}/self_attention/multihead_attention/v/kernel"".format(i)], None),\n            ""decoder/layer_{}/self_attention/output_proj/kernel"".format(i): ([""transformer/body/decoder/layer_{}/self_attention/multihead_attention/output_transform/kernel"".format(i)], None),\n            ""decoder/layer_{}/self_attention/LayerNorm/beta"".format(i): ([""transformer/body/decoder/layer_{}/self_attention/layer_prepostprocess/layer_norm/layer_norm_bias"".format(i)], None),\n            ""decoder/layer_{}/self_attention/LayerNorm/gamma"".format(i): ([""transformer/body/decoder/layer_{}/self_attention/layer_prepostprocess/layer_norm/layer_norm_scale"".format(i)], None),\n            ""decoder/layer_{}/feedforward/hidden_state/kernel"".format(i): ([""transformer/body/decoder/layer_{}/ffn/conv1/kernel"".format(i)], None),\n            ""decoder/layer_{}/feedforward/hidden_state/bias"".format(i): ([""transformer/body/decoder/layer_{}/ffn/conv1/bias"".format(i)], None),\n            ""decoder/layer_{}/feedforward/output/kernel"".format(i): ([""transformer/body/decoder/layer_{}/ffn/conv2/kernel"".format(i)], None),\n            ""decoder/layer_{}/feedforward/output/bias"".format(i): ([""transformer/body/decoder/layer_{}/ffn/conv2/bias"".format(i)], None),\n            ""decoder/layer_{}/feedforward/LayerNorm/beta"".format(i): ([""transformer/body/decoder/layer_{}/ffn/layer_prepostprocess/layer_norm/layer_norm_bias"".format(i)], None),\n            ""decoder/layer_{}/feedforward/LayerNorm/gamma"".format(i): ([""transformer/body/decoder/layer_{}/ffn/layer_prepostprocess/layer_norm/layer_norm_scale"".format(i)], None)})\n\n    return var_map\n# pylint: enable=line-too-long\n\n\ndef log(message: str, color: str = ""blue"") -> None:\n    _log(message, color)\n\ndef check_shape(var1_tf: tf.Variable, var2_np: np.ndarray):\n    if var1_tf.get_shape().as_list() != list(var2_np.shape):\n        log(""Shapes do not match! Exception will follow."", color=""red"")\n\ndef get_shared_emb_vars(np_vars: Dict) -> List[str]:\n    modality_vars = [var for var in np_vars if ""symbol_modality"" in var]\n    modality_prefix = modality_vars[0].split(""/"")[:-1]\n    sorted_vars = []\n    for i in range(len(modality_vars)):\n        modality_arr = modality_prefix + [""weights_{}"".format(i)]\n        sorted_vars.append(""/"".join(modality_arr))\n    return sorted_vars\n    \n\ndef emb_fix(variables: List[tf.Tensor]) -> tf.Tensor:\n    """"""Concat sharded embedding matrix and include NMonkey special symbols.\n\n    We need to include embeddings for special symbols that are exclusive\n    to Neural Monkey (<s>, <unk>).\n    """"""\n    concat = np.concatenate(variables, axis=0)\n    concat_split = np.split(concat, [1, 2], axis=0)\n\n    emb_shape = concat.shape[-1]\n    return np.concatenate([concat_split[0],\n                           np.zeros([1, emb_shape]),\n                           concat_split[1],\n                           np.zeros([1, emb_shape]),\n                           concat_split[2]], axis=0)\n\n\ndef build_encoder(hparams: Dict,\n                  vocab_path: str) -> Tuple[TransformerEncoder, str]:\n    vocabulary = from_t2t_vocabulary(vocab_path)\n    vocabulary_ini = VOCABULARY_TEMPLATE.format(vocab_path)\n\n    inp_seq_name = ""{}_input"".format(ENCODER_NAME)\n    inp_seq = EmbeddedSequence(\n        name=inp_seq_name,\n        vocabulary=vocabulary,\n        data_id=""source_wp"",\n        embedding_size=hparams[""embedding_size""],\n        scale_embeddings_by_depth=hparams[\n            ""multiply_embedding_mode""] == ""sqrt_depth"",\n        add_end_symbol=True)\n\n    encoder = TransformerEncoder(\n        name=ENCODER_NAME,\n        input_sequence=inp_seq,\n        ff_hidden_size=hparams[""ff_hidden_size""],\n        depth=hparams[""depth""],\n        n_heads=hparams[""n_heads""],\n        target_space_id=21)\n\n    encoder_ini = ENCODER_TEMPLATE.format(\n        inp_seq_name, hparams[""embedding_size""],\n        hparams[""multiply_embedding_mode""] == ""sqrt_depth"",\n        hparams[""max_length""],\n        ENCODER_NAME, hparams[""ff_hidden_size""], hparams[""depth""],\n        hparams[""n_heads""])\n\n    return encoder, vocabulary, ""\\n"".join([vocabulary_ini, encoder_ini])\n\n\ndef build_decoder(hparams: Dict,\n                  encoder: TransformerEncoder,\n                  vocab: Vocabulary) -> Tuple[\n        TransformerDecoder, str]:\n    decoder = TransformerDecoder(\n        name=DECODER_NAME,\n        encoder=encoder,\n        vocabulary=vocab,\n        data_id=""target_wp"",\n        ff_hidden_size=hparams[""ff_hidden_size""],\n        n_heads_self=hparams[""n_heads""],\n        n_heads_enc=hparams[""n_heads""],\n        depth=hparams[""depth""],\n        embeddings_source=encoder.input_sequence,\n        embedding_size=hparams[""embedding_size""],\n        max_output_len=50)\n\n    decoder_ini = DECODER_TEMPLATE.format(\n        DECODER_NAME, hparams[""ff_hidden_size""], hparams[""n_heads""],\n        hparams[""n_heads""], hparams[""depth""], hparams[""embedding_size""])\n\n    return decoder, ""\\n"".join([decoder_ini])\n\n\ndef build_model(hparams: Dict,\n                vocab_path: str) -> Tuple[\n        TransformerEncoder, TransformerDecoder, str]:\n    encoder, vocab, encoder_cfg = build_encoder(hparams, vocab_path)\n    decocer, decoder_cfg = build_decoder(hparams, encoder, vocab)\n\n    ini = ""\\n"".join([encoder_cfg, decoder_cfg])\n\n    return ini\n\n\ndef load_hparams(path: str) -> Dict:\n    """"""Open a JSON file containing model transformer model hyperparameters.""""""\n\n    with open(path, ""r"", encoding=""utf-8"") as f_json:\n        contents = json.load(f_json)\n\n    # TODO: check, whether this is all we need\n    hparams = {\n        ""n_heads"": contents[""num_heads""],\n        ""ff_hidden_size"": contents[""filter_size""],\n        ""embedding_size"": contents[""hidden_size""],\n        ""max_length"": contents[""max_length""],\n        ""label_smoothing"": contents[""label_smoothing""],\n        ""depth"": contents[""num_hidden_layers""],\n        ""multiply_embedding_mode"": contents[""multiply_embedding_mode""]\n    }\n\n    return hparams\n\ndef get_assign_ops(hparams: Dict, np_vars: Dict) -> List[tf.Tensor]:\n    """"""Create assign operations from transformer model to neuralmonkey.\n\n    The assign operations are used to load variables from the transformer\n    model to their correct parts in the graph created by Neural Monkey.\n    """"""\n\n    trainable_vars = tf.trainable_variables()\n    assign_ops = []\n\n    var_map = create_variable_map(hparams, np_vars)\n    for var in trainable_vars:\n        map_key = var.op.name\n\n        if map_key not in var_map:\n            raise ValueError(""Map key {} not in variable map"".format(map_key))\n\n        t2t_var_list, fun = var_map[map_key]\n\n        for t2t_var in t2t_var_list:\n            if t2t_var not in np_vars:\n                raise ValueError(""Alleged transformer var {} not found ""\n                                 ""in loaded transformer vars. For neuralmonkey""\n                                 "" var {}."".format(t2t_var, map_key))\n\n        if fun is None:\n            if len(t2t_var_list) != 1:\n                raise ValueError(\n                    ""Var list for map key {} must have length 1. ""\n                    ""Length {} found instead.""\n                    .format(map_key, len(t2t_var_list)))\n            to_assign = np_vars[t2t_var_list[0]]\n        else:\n            to_assign = fun([np_vars[v] for v in t2t_var_list])\n\n        check_shape(var, to_assign)\n        assign_ops.append(tf.assign(var, to_assign))\n\n    return assign_ops\n\n\ndef write_config(experiment_dir: str, ini: str) -> None:\n    experiment_file = os.path.join(experiment_dir, ""experiment.ini"")\n    with open(experiment_file, ""w"", encoding=""utf-8"") as f_out:\n        f_out.write(INI_HEADER.format(experiment_dir))\n        f_out.write(ini)\n\n\ndef prepare_output_dir(output_dir: str) -> bool:\n    if os.path.isdir(output_dir):\n        log(""Directory {} already exists. Choose a nonexistent one."".\n            format(output_dir))\n        exit(1)\n\n    os.mkdir(output_dir)\n\n\ndef main() -> None:\n    log(""Script started."")\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(""--t2t-checkpoint"", metavar=""T2T-CHECKPOINT"",\n                        help=""t2t checkpoint file"")\n    parser.add_argument(""--t2t-hparams"", metavar=""T2T-HPARAMS"",\n                        help=""t2t hparams json file"")\n    parser.add_argument(""--vocabulary"", metavar=""VOCABULARY"",\n                        help=""vocabulary file"")\n    parser.add_argument(""--output-dir"", metavar=""OUTPUT-DIR"",\n                        help=""output directory"")\n    args = parser.parse_args()\n\n    ckpt = args.t2t_checkpoint\n\n    log(""Loading transformer hparams JSON from {}."".format(args.t2t_hparams))\n    hparams = load_hparams(args.t2t_hparams)\n\n    log(""Bulding model."")\n    ini = build_model(hparams, args.vocabulary)\n\n    log(""Read from checkpoint {}."".format(ckpt))\n    t2t_var_list = tf.contrib.framework.list_variables(ckpt)\n    t2t_reader = tf.contrib.framework.load_checkpoint(ckpt)\n    t2t_var_values = {}\n    for (name, shape) in t2t_var_list:\n        if name.startswith(""training""):\n            continue\n        t2t_var_values[name] = t2t_reader.get_tensor(name)\n\n    log(""Defining assign_ops."")\n    assign_ops = get_assign_ops(hparams, t2t_var_values)\n\n    log(""Preparing output directory {}."".format(args.output_dir))\n    prepare_output_dir(args.output_dir)\n\n    log(""Writing configuration file to {}/experiment.ini.""\n        .format(args.output_dir))\n    write_config(args.output_dir, ini)\n\n    log(""Creating TF session."")\n    s =  tf.Session()\n\n    log(""Running session to assign to Neural Monkey variables."")\n    s.run(assign_ops)\n\n    log(""Initializing saver."")\n    saver = tf.train.Saver()\n\n    variables_file = os.path.join(args.output_dir, ""variables.data"")\n    log(""Saving variables to {}."".format(variables_file))\n    saver.save(s, variables_file)\n\n    log(""Finished."")\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/max_column_finder.py,0,"b'#!/usr/bin/env python3.5\n""""""\nFor each line in N input files with scores, outputs the index of the file\nwhich has maximum value on that line.\nOutput can be processed with the column_selector.py script.\n\nThese two scripts are particularly useful for hypotheses rescoring.\n""""""\nimport argparse\nimport numpy as np\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(""score_files"", nargs=""+"",\n                        metavar=""SCORE_FILES"", type=argparse.FileType(""r""),\n                        help=""the files to traverse"")\n    args = parser.parse_args()\n\n    for lines in zip(*args.score_files):\n        numbers = np.array(lines, dtype=float)\n        best_index = np.argmax(numbers)\n        # best_score = np.amax(numbers)\n\n        print(best_index)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/postedit_prepare_data.py,0,"b'#!/usr/bin/env python3\n\n""""""\nThis is a script preparing data for the post-editing task. It encodes the\nsentences to be post-edited as  a sequence of delete, keep and insert\noperations from the target sentence.\n\nExample:\n    source:   Good afternoon , John ! !\n    target:   Good evening , John !\n\n    result:   <keep> <delete> evening <keep> <keep> <delete>\n\nThe inverse to this script is \'postedit_reconstruct_data.py\'.\n""""""\n\n# tests: lint\n\nimport argparse\nimport re\nimport numpy as np\nfrom neuralmonkey.processors.german import GermanPreprocessor\n\n\ndef load_tokenized(text_file, preprocess=None):\n    if not preprocess:\n        preprocess = lambda x: x\n    return [preprocess(re.split(r""[ ]"", l.rstrip())) for l in text_file]\n\n\ndef convert_to_edits(source, target):\n    keep = \'<keep>\'\n    delete = \'<delete>\'\n\n    lev = np.zeros([len(source) + 1, len(target) + 1])\n    edits = [[[] for _ in range(len(target) + 1)]\n             for _ in range(len(source) + 1)]\n\n    for i in range(len(source) + 1):\n        lev[i, 0] = i\n        edits[i][0] = [delete for _ in range(i)]\n\n    for j in range(len(target) + 1):\n        lev[0, j] = j\n        edits[0][j] = target[:j]\n\n    for j in range(1, len(target) + 1):\n        for i in range(1, len(source) + 1):\n\n            if source[i - 1] == target[j - 1]:\n                keep_cost = lev[i - 1, j - 1]\n            else:\n                keep_cost = np.inf\n\n            delete_cost = lev[i - 1, j] + 1\n            insert_cost = lev[i, j - 1] + 1\n\n            lev[i, j] = min(keep_cost, delete_cost, insert_cost)\n\n            if lev[i, j] == keep_cost:\n                edits[i][j] = edits[i - 1][j - 1] + [keep]\n\n            elif lev[i, j] == delete_cost:\n                edits[i][j] = edits[i - 1][j] + [delete]\n\n            else:\n                edits[i][j] = edits[i][j - 1] + [target[j - 1]]\n\n    return edits[-1][-1]\n\n\ndef main():\n    # print convert_to_edits([""hello"", ""john""], [""hi"", ""john"", ""how""])\n\n    parser = argparse.ArgumentParser(\n        description=""Convert postediting target data to sequence of edits"")\n    parser.add_argument(""--translated-sentences"",\n                        type=argparse.FileType(\'r\'), required=True)\n    parser.add_argument(""--target-sentences"",\n                        type=argparse.FileType(\'r\'), required=True)\n    parser.add_argument(""--target-german"", type=bool, default=False)\n\n    args = parser.parse_args()\n\n    preprocess = None\n    if args.target_german:\n        preprocess = GermanPreprocessor()\n\n    trans_sentences = load_tokenized(\n        args.translated_sentences, preprocess=preprocess)\n    tgt_sentences = load_tokenized(\n        args.target_sentences, preprocess=preprocess)\n\n    for trans, tgt in zip(trans_sentences, tgt_sentences):\n        edits = convert_to_edits(trans, tgt)\n        print("" "".join(edits))\n\nif __name__ == \'__main__\':\n    main()\n'"
scripts/postedit_reconstruct_data.py,0,"b'#!/usr/bin/env python3\n""""""\nThis a script that takes the result of automatic postediting encoded as a\nsequence of <keep>, <delete> and insert operations and applies them on the\noriginal text being post-edited.\n\nThe inverse script to this one is \'postedit_prepare_data.py\'.\n""""""\nimport argparse\nfrom neuralmonkey.processors.german import GermanPreprocessor\nfrom neuralmonkey.processors.german import GermanPostprocessor\nfrom postedit_prepare_data import load_tokenized\n\n# TODO make reconstruct a postprocessor\n\n\ndef reconstruct(source, edits):\n    index = 0\n    target = []\n\n    for edit in edits:\n        if edit == \'<keep>\':\n            if index < len(source):\n                target.append(source[index])\n            index += 1\n\n        elif edit == \'<delete>\':\n            index += 1\n\n        else:\n            target.append(edit)\n\n    # we may have created a shorter sequence of edit ops due to the\n    # decoder limitations -> now copy the rest of source\n    if index < len(source):\n        target.extend(source[index:])\n\n    return target\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=""Convert postediting target data to sequence of edits"")\n    parser.add_argument(""--edits"", type=argparse.FileType(\'r\'), required=True)\n    parser.add_argument(""--translated-sentences"",\n                        type=argparse.FileType(\'r\'), required=True)\n    parser.add_argument(""--target-german"", type=bool, default=False)\n\n    args = parser.parse_args()\n\n    postprocess = lambda x: x\n    preprocess = None  # type: GermanPreprocessor\n    if args.target_german:\n        # pylint: disable=redefined-variable-type\n        postprocess = GermanPostprocessor()\n        preprocess = GermanPreprocessor()\n\n    trans_sentences = load_tokenized(\n        args.translated_sentences, preprocess=preprocess)\n    edit_sequences = load_tokenized(args.edits, preprocess=None)\n\n    for trans, edits in zip(trans_sentences, edit_sequences):\n        target = reconstruct(trans, edits)\n        # TODO refactor this (change postprocessor api)\n        print("" "".join(postprocess([target])[0]))\n\n\nif __name__ == \'__main__\':\n    # edits = [\'<keep>\', \'ahoj\', \'<delete>\', \'proc?\']\n    # source = [\'Karle\', \'co\', \'kdy\']\n    # print reconstruct(source, edits)\n\n    main()\n'"
scripts/prepare_str_images.py,0,"b'#!/usr/bin/env python3\n\nimport argparse\nimport os\nimport gzip\nimport pickle as pickle\nimport numpy as np\n\nfrom neuralmonkey.logging import log\nfrom neuralmonkey.image_utils import STRPreprocessor\n\ndef main():\n    parser = argparse.ArgumentParser(description=""Prepares the STR data."")\n    parser.add_argument(""--list"", type=argparse.FileType(\'r\'),\n                        help=""File with images."", required=True)\n    parser.add_argument(""--img-root"", type=str, required=True,\n                        help=""Directory with images."")\n    parser.add_argument(""--height"", type=int, default=32)\n    parser.add_argument(""--max-width"", type=int, default=320)\n    parser.add_argument(""--output-file"", type=str, required=True)\n    parser.add_argument(""--output-log"", type=argparse.FileType(\'w\'), required=True)\n    args = parser.parse_args()\n\n    preprocessor = STRPreprocessor(args.height, args.max_width)\n\n    f_out = gzip.open(args.output_file, mode=\'wb\')\n    processed = 0\n    for i, line in enumerate(args.list):\n        img_path = os.path.join(args.img_root, line.rstrip())\n\n        try:\n            img = preprocessor(img_path)\n            pickle.dump(img, f_out)\n\n            args.output_log.write(""{}\\n"".format(img_path))\n            processed += 1\n            if i % 1000 == 999:\n                log(""Processed {} images"".format(i + 1))\n        except Exception as exc:\n            log(""Skipped {} (no. {}), expeption {}"".format(img_path, i, exc), color=\'red\')\n\n    log(""Done, saved {} images to {}"".format(processed, args.output_file))\n\n    f_out.close()\n\n    log(""Padded {} times, on averaged {:.0f} pixels"".\\\n            format(len(preprocessor.paddings),\n                   np.mean(preprocessor.paddings) if preprocessor.paddings else 0.0))\n    log(""Shrinked {} times, on averaged {:.0f} pixels"".\\\n            format(len(preprocessor.shrinkages),\n                   np.mean(preprocessor.shrinkages) if preprocessor.shrinkages else 0.0))\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/preprocess_bert.py,0,"b'#!/usr/bin/env python3\n""""""Creates training data for the BERT network training\n(noisified + masked gold predictions) using the input corpus.\n\nThe masked Gold predictions use Neural Monkey\'s PAD_TOKEN to indicate\ntokens that should not be classified during training.\n\nWe only leave `coverage` percent of symbols for classification. These\nsymbols are left unchanged on input with a probability of `1 - mask_prob`.\nIf they are being changed, they are replaced by the `mask_token` with a\nprobability of `1 - replace_prob` and by a random vocabulary token otherwise.\n""""""\n\nimport argparse\nimport os\n\nimport numpy as np\n\nfrom neuralmonkey.logging import log as _log\nfrom neuralmonkey.vocabulary import (\n    Vocabulary, PAD_TOKEN, UNK_TOKEN, from_wordlist)\n\n\ndef log(message: str, color: str = ""blue"") -> None:\n    _log(message, color)\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(""--input_file"", type=str, default=""/dev/stdin"")\n    parser.add_argument(""--vocabulary"", type=str, required=True)\n    parser.add_argument(""--output_prefix"", type=str, default=None)\n    parser.add_argument(""--mask_token"", type=str, default=UNK_TOKEN,\n                        help=""token used to mask the tokens"")\n    parser.add_argument(""--coverage"", type=float, default=0.15,\n                        help=(""percentage of tokens that should be left ""\n                              ""for classification during training""))\n    parser.add_argument(""--mask_prob"", type=float, default=0.8,\n                        help=(""probability of the classified token being ""\n                             ""replaced by a different token on input""))\n    parser.add_argument(""--replace_prob"", type=float, default=0.1,\n                        help=(""probability of the classified token being ""\n                              ""replaced by a random token instead of ""\n                              ""mask_token""))\n    parser.add_argument(""--vocab_contains_header"", type=bool, default=True)\n    parser.add_argument(""--vocab_contains_frequencies"",\n                        type=bool, default=True)\n    args = parser.parse_args()\n\n    assert (args.coverage <= 1 and args.coverage >= 0)\n    assert (args.mask_prob <= 1 and args.mask_prob >= 0)\n    assert (args.replace_prob <= 1 and args.replace_prob >= 0)\n\n    log(""Loading vocabulary."")\n    vocabulary = from_wordlist(\n        args.vocabulary,\n        contains_header=args.vocab_contains_header,\n        contains_frequencies=args.vocab_contains_frequencies)\n\n    mask_prob = args.mask_prob\n    replace_prob = args.replace_prob\n    keep_prob = 1 - mask_prob - replace_prob\n    sample_probs = (keep_prob, mask_prob, replace_prob)\n\n    output_prefix = args.output_prefix\n    if output_prefix is None:\n        output_prefix = args.input_file\n    out_f_noise = ""{}.noisy"".format(output_prefix)\n    out_f_mask = ""{}.mask"".format(output_prefix)\n\n    out_noise_h = open(out_f_noise, ""w"", encoding=""utf-8"")\n    out_mask_h = open(out_f_mask, ""w"", encoding=""utf-8"")\n    log(""Processing data."")\n    with open(args.input_file, ""r"", encoding=""utf-8"") as input_h:\n        # TODO: performance optimizations\n        for line in input_h:\n            line = line.strip().split("" "")\n            num_samples = int(args.coverage * len(line))\n            sampled_indices = np.random.choice(len(line), num_samples, False)\n\n            output_noisy = list(line)\n            output_masked = [PAD_TOKEN] * len(line)\n            for i in sampled_indices:\n                random_token = np.random.choice(vocabulary.index_to_word[4:])\n                new_token = np.random.choice(\n                    [line[i], args.mask_token, random_token], p=sample_probs)\n                output_noisy[i] = new_token\n                output_masked[i] = line[i]\n            out_noise_h.write(str("" "".join(output_noisy)) + ""\\n"")\n            out_mask_h.write(str("" "".join(output_masked)) + ""\\n"")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/speech_features.py,0,"b'#!/usr/bin/env python3\n""""""\nThis script precomputes speech features for a given set of recordings and\nsaves them as a list of NumPy arrays.\n\nusage example:\n  %(prog)s src.train src.train.npy -t mfcc -o delta_order 2\n""""""\n\nimport argparse\nimport os\nfrom typing import Union\n\nimport numpy as np\n\nfrom neuralmonkey.processors.speech import SpeechFeaturesPreprocessor\nfrom neuralmonkey.readers.audio_reader import audio_reader\n\n\ndef try_parse_number(str_value: str) -> Union[str, float, int]:\n    value = str_value\n    try:\n        value = float(str_value)\n        value = int(str_value)\n    except ValueError:\n        pass\n    \n    return value\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=__doc__,\n        formatter_class=argparse.RawDescriptionHelpFormatter)\n    parser.add_argument(\'input\', help=\'a file with a list of audio files\')\n    parser.add_argument(\'output\', help=\'the .npy output file\')\n    parser.add_argument(\'-f\', \'--format\',\n                        default=\'wav\',\n                        help=\'the audio format (default: %(default)s)\')\n    parser.add_argument(\'-p\', \'--prefix\',\n                        help=\'the prefix for the audio file paths (default: \'\n                        \'the input file location)\')\n    parser.add_argument(\'-t\', \'--type\',\n                        default=\'mfcc\',\n                        help=\'the feature type (default: %(default)s)\')\n    parser.add_argument(\'-o\', \'--option\',\n                        nargs=2, action=\'append\', default=[],\n                        metavar=(\'OPTION\', \'VALUE\'),\n                        help=\'other arguments for SpeechFeaturesPreprocessor\')\n\n    args = parser.parse_args()\n\n\n    prefix = args.prefix\n    if prefix is None:\n        prefix = os.path.dirname(os.path.abspath(args.input))\n\n    feats_kwargs = {k: try_parse_number(v) for k, v in args.option}\n\n    read = audio_reader(prefix=prefix, audio_format=args.format)\n    process = SpeechFeaturesPreprocessor(\n        feature_type=args.type, **feats_kwargs)\n\n    output = [process(audio) for audio in read([args.input])]\n    \n    np.save(args.output, output)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
scripts/tf_save_images.py,1,"b'#!/usr/bin/env python3\n""""""Extract a given image summary from an event file.\n\nExample usage:\n\n    %(prog)s myevents.out example/image --prefix example_\n\nThis will extract the contents of the ""example/image"" summary from the event\nfile ""myevents.out"" and write them into the files\n\nexample_000000023940.png\nexample_000000047900.png\n...\n\nwhere the numbers correspond to the steps associated with the images.\n\nThe filenames can also be based on the order in which the images appear in the\nevent file, e.g. ""--suffix \'{i:06d}.png\'"" will produce the files\n\nexample_000000.png\nexample_000001.png\n...\n\n""""""\n\nimport argparse\nimport tensorflow as tf\n\ndef main():\n    parser = argparse.ArgumentParser(description=__doc__,\n        formatter_class=argparse.RawDescriptionHelpFormatter)\n    parser.add_argument(\'event_file\', metavar=\'event-file\', help=\'the event file\')\n    parser.add_argument(\'tag\', help=\'the image summary tag\')\n    parser.add_argument(\'--prefix\', default=\'image_\',\n                        help=\'the image filename prefix (default: %(default)s)\')\n    parser.add_argument(\'--suffix\', default=\'{step:012d}.png\',\n                        help=\'the image filename suffix formatting string \'\n                        \'(default: %(default)s)\')\n    args = parser.parse_args()\n\n    i = 0\n    for e in tf.train.summary_iterator(args.event_file):\n        if e.HasField(\'summary\'):\n            for v in e.summary.value:\n                if v.HasField(\'image\') and v.tag == args.tag:\n                    fname = (\'{prefix}\' + args.suffix).format(\n                        prefix=args.prefix, i=i, step=e.step)\n\n                    with open(fname, \'wb\') as f:\n                        f.write(v.image.encoded_image_string)\n\n                    i += 1\n\n\nif __name__ == \'__main__\':\n    main()\n'"
scripts/tokenize_data.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport sys, codecs, argparse\nfrom nltk import word_tokenize\nimport javabridge\nimport gc\nimport regex as re\nfrom termcolor import cprint\n\ndef get_decompounder():\n    """"""\n    Restarts the JVM with the decompounder. It is necessary once in a while.\n    """"""\n    javabridge.start_vm(class_path=[""tf/jwordsplitter/target/jwordsplitter-4.2-SNAPSHOT.jar""])\n    java_instance = javabridge.make_instance(""de/danielnaber/jwordsplitter/GermanWordSplitter"", ""(Z)V"", False)\n    decompounder = javabridge.JWrapper(java_instance)\n    return decompounder\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=""Tokenizes and decompounds the text on stdin."")\n    parser.add_argument(""--language"", required=True, help=""Lanuage the text is in."")\n    args = parser.parse_args()\n\n    if args.language not in set([\'english\', \'german\']):\n        raise Exception(""Language must be \'english\' or \'german\', not \'{}\'."".format(args.language))\n\n    sys.stdin = codecs.getreader(\'utf-8\')(sys.stdin)\n    sys.stdout = codecs.getwriter(\'utf-8\')(sys.stdout)\n    sys.stderr = codecs.getwriter(\'utf-8\')(sys.stderr)\n\n    try:\n        if args.language == ""german"":\n            decompounder = get_decompounder()\n            decompounded_count = 0\n\n        for ln, line in enumerate(sys.stdin):\n            line = re.sub(r""[[:space:]]+"", "" "", line.rstrip())\n            line = re.sub(r""^[[:space:]]+"", """", line)\n            line = re.sub(r""\'\'"", ""\\"""", line)\n            line = re.sub(r""``"", ""\\"""", line)\n            line = re.sub(r""-([[:punct:]\\$])"", ""\\g<1>"", line)\n            line = re.sub(r""([[:punct:]\\$])-"", ""\\g<1>"", line)\n            line = re.sub(r""^[[:space:]]*-[[:space:]]"", """", line)\n            line = re.sub(r""([[:alpha:]0-9\xc3\x9f])-([ [:punct:]])"", ""\\g<1>\\g<2>"", line, re.UNICODE)\n            line = re.sub(r""([ [:punct:]])-([[:alpha:]0-9\xc3\x9f])"", ""\\g<1>\\g<2>"", line, re.UNICODE)\n            line = re.sub(r"" - "", "" \xe2\x80\x93 "", line)\n            line = re.sub(r""\xe2\x80\x93 -"", ""\xe2\x80\x93"", line)\n\n            def normalize_quotes(token):\n                token = re.sub(r""-$"", \'\', token)\n                token = re.sub(r""``"", \'\\u201c\', token)\n                token = re.sub(r""\'\'"", \'\\u201d\', token)\n                return token\n\n            tokenized = [normalize_quotes(t) for t in word_tokenize(line, language=args.language)]\n\n            if args.language == ""german"":\n                for i, token in enumerate(tokenized):\n                    decompounded_count += 1\n                    decompounded = decompounder.splitWord(token)\n                    if decompounded.size() >= 2:\n                        tokenized[i] = \\\n                            ""#"".join([decompounded.get(j) for j in range(decompounded.size())])\n                    del decompounded\n\n                    if token.endswith(""s"") and not tokenized[i].endswith(""s""):\n                        tokenized[i] += ""s""\n\n                    # we need to manually garbage collect because of Java Heap Space\n                    if decompounded_count % 150 == 0:\n                        gc.collect()\n\n            tokenized_string = \' \'.join(tokenized)\n\n            # Now put special character for spaces introduced by the tokenizer\n            original_i = 0\n            tokenized_chars_result = []\n            for tokenized_i, char in enumerate(tokenized_string):\n                #print u""pair \'{}\' ({}) and \'{}\' ({})"".format(char, ord(char), line[original_i], ord(line[original_i]))\n                if char == line[original_i] or (char == "" "" and ord(line[original_i]) == 160):\n                    tokenized_chars_result.append(char)\n                    original_i += 1\n                    #print u""same characters {}"".format(char)\n                elif line[original_i] == \'""\' and (char == \'\\u201c\' or char == \'\\u201d\'):\n                    original_i += 1\n                    #print u""quotation mark {}"".format(char)\n                elif char == "" "":\n                    tokenized_chars_result.append(""@"")\n                    #print ""space added by tokenizer""\n                elif char == ""#"":\n                    if line[original_i] == ""-"":\n                        tokenized_chars_result.append(""-"")\n                        original_i += 1\n                    else:\n                        if args.language == \'german\' and \\\n                                (line[original_i] == ""s"" or line[original_i] == ""S"") \\\n                                and line[original_i + 1] == tokenized_string[tokenized_i + 1]:\n                            original_i += 1\n                            tokenized_chars_result.append(""$"")\n                            #print ""decompounded with inserted s""\n                        if args.language == \'german\' and line[original_i] == ""s"" and line[original_i + 1] == ""-"":\n                            original_i += 2\n                            tokenized_chars_result.append(""-"")\n                        else:\n                            #print ""decompompounded""\n                            tokenized_chars_result.append(""#"")\n                else:\n                    #print """"\n                    #print ""Error on line {}"".format(ln)\n                    #cprint(u""tokenized on index {}: \\""{}\\"", original on index {}: \\""{}\\""""\\\n                    #        .format(tokenized_i, char, original_i, line[original_i]), \'yellow\')\n                    #cprint(line, \'red\')\n                    #cprint(tokenized_string, \'red\')\n                    #javabridge.kill_vm()\n                    #exit()\n                    tokenized_chars_result = list(""<ERROR>"")\n                    break\n\n            print("""".join(tokenized_chars_result))\n    except:\n        javabridge.kill_vm()\n        exit(1)\n    finally:\n        javabridge.kill_vm()\n'"
docs/source/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Neural Monkey documentation build configuration file, created by\n# sphinx-quickstart on Wed Aug 31 14:49:25 2016.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys\nimport os\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nsys.path.insert(0, os.path.abspath(\'../..\'))\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.pngmath\',\n    \'sphinx.ext.intersphinx\'\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The encoding of source files.\n#source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'Neural Monkey\'\ncopyright = u\'2016, Jind\xc5\x99ich Libovick\xc3\xbd, Jind\xc5\x99ich Helcl, Tom\xc3\xa1\xc5\xa1 Musil\'\nauthor = u\'Jind\xc5\x99ich Libovick\xc3\xbd, Jind\xc5\x99ich Helcl, Tom\xc3\xa1\xc5\xa1 Musil\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = u\'0.1\'\n# The full version, including alpha/beta/rc tags.\nrelease = u\'0.1\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = []\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n#keep_warnings = False\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nimport sphinx_rtd_theme\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# ""<project> v<release> documentation"".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\nhtml_logo = \'img/gorilla-logo-half.png\'\n\n# The name of an image file (relative to this directory) to use as a favicon of\n# the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\nhtml_favicon = \'img/gorilla.ico\'\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#html_extra_path = []\n\n# If not \'\', a \'Last updated on:\' timestamp is inserted at every page bottom,\n# using the given strftime format.\n#html_last_updated_fmt = \'%b %d, %Y\'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n#html_file_suffix = None\n\n# Language to be used for generating the HTML full-text search index.\n# Sphinx supports the following languages:\n#   \'da\', \'de\', \'en\', \'es\', \'fi\', \'fr\', \'hu\', \'it\', \'ja\'\n#   \'nl\', \'no\', \'pt\', \'ro\', \'ru\', \'sv\', \'tr\'\n#html_search_language = \'en\'\n\n# A dictionary with options for the search language support, empty by default.\n# Now only \'ja\' uses this config value\n#html_search_options = {\'type\': \'default\'}\n\n# The name of a javascript file (relative to the configuration directory) that\n# implements a search results scorer. If empty, the default will be used.\n#html_search_scorer = \'scorer.js\'\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'NeuralMonkeydoc\'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n# The paper size (\'letterpaper\' or \'a4paper\').\n#\'papersize\': \'letterpaper\',\n\n# The font size (\'10pt\', \'11pt\' or \'12pt\').\n#\'pointsize\': \'10pt\',\n\n# Additional stuff for the LaTeX preamble.\n#\'preamble\': \'\',\n\n# Latex figure (float) alignment\n#\'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'NeuralMonkey.tex\', u\'Neural Monkey Documentation\',\n     u\'Jind\xc5\x99ich Libovick\xc3\xbd, Jind\xc5\x99ich Helcl, Tom\xc3\xa1\xc5\xa1 Musil\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'neuralmonkey\', u\'Neural Monkey Documentation\',\n     [author], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'NeuralMonkey\', u\'Neural Monkey Documentation\',\n     author, \'NeuralMonkey\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n#texinfo_no_detailmenu = False\n\n\ndef run_apidoc(_):\n\n    cur_dir = os.path.abspath(os.path.dirname(__file__))\n    print(cur_dir)\n    module = os.path.abspath(os.path.join(cur_dir, "".."", "".."", ""neuralmonkey""))\n    print(module)\n\n    from sphinx.apidoc import main\n    main([\'--separate\', \'-o\', cur_dir, module, \'--force\'])\n\n\ndef skip(app, what, name, obj, skip, options):\n    if name == \'__init__\':\n        return False\n    return skip\n\n\ndef setup(app):\n    app.connect(\'autodoc-skip-member\', skip)\n    app.connect(\'builder-inited\', run_apidoc)\n'"
lib/subword_nmt/__init__.py,0,b''
lib/subword_nmt/apply_bpe.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Author: Rico Sennrich\n\n""""""Use operations learned with learn_bpe.py to encode a new text.\nThe text will not be smaller, but use only a fixed vocabulary, with rare words\nencoded as variable-length sequences of subword units.\n\nReference:\nRico Sennrich, Barry Haddow and Alexandra Birch (2016). Neural Machine Translation of Rare Words with Subword Units.\nProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016). Berlin, Germany.\n""""""\n\nfrom __future__ import unicode_literals, division\n\nimport sys\nimport codecs\nimport argparse\nfrom collections import defaultdict\n\n# hack for python2/3 compatibility\n# from io import open\n# argparse.open = open\n\n# # python 2/3 compatibility\n# if sys.version_info < (3, 0):\n#   sys.stderr = codecs.getwriter(\'UTF-8\')(sys.stderr)\n#   sys.stdout = codecs.getwriter(\'UTF-8\')(sys.stdout)\n#   sys.stdin = codecs.getreader(\'UTF-8\')(sys.stdin)\n\nclass BPE:\n\n    def __init__(self, codes, separator=\'@@\'):\n        self.bpe_codes = [tuple(item.split()) for item in codes]\n        # some hacking to deal with duplicates (only consider first instance)\n        self.bpe_codes = dict([(code,i) for (i,code) in reversed(list(enumerate(self.bpe_codes)))])\n\n        self.separator = separator\n\n    def segment(self, sentence):\n        """"""segment single sentence (whitespace-tokenized string) with BPE encoding""""""\n\n        output = []\n        for word in sentence.split():\n            new_word = encode(word, self.bpe_codes)\n\n            for item in new_word[:-1]:\n                output.append(item + self.separator)\n            output.append(new_word[-1])\n\n        return \' \'.join(output)\n\ndef create_parser():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        description=""learn BPE-based word segmentation"")\n\n    parser.add_argument(\n        \'--input\', \'-i\', type=argparse.FileType(\'r\'), default=sys.stdin,\n        metavar=\'PATH\',\n        help=""Input file (default: standard input)."")\n    parser.add_argument(\n        \'--codes\', \'-c\', type=argparse.FileType(\'r\'), metavar=\'PATH\',\n        required=True,\n        help=""File with BPE codes (created by learn_bpe.py)."")\n    parser.add_argument(\n        \'--output\', \'-o\', type=argparse.FileType(\'w\'), default=sys.stdout,\n        metavar=\'PATH\',\n        help=""Output file (default: standard output)"")\n    parser.add_argument(\n        \'--separator\', \'-s\', type=str, default=\'@@\', metavar=\'STR\',\n        help=""Separator between non-final subword units (default: \'%(default)s\'))"")\n\n    return parser\n\ndef get_pairs(word):\n    """"""Return set of symbol pairs in a word.\n\n    word is represented as tuple of symbols (symbols being variable-length strings)\n    """"""\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\ndef encode(orig, bpe_codes, cache={}):\n    """"""Encode word based on list of BPE merge operations, which are applied consecutively\n    """"""\n\n    if orig in cache:\n        return cache[orig]\n\n    word = tuple(orig) + (\'</w>\',)\n    pairs = get_pairs(word)\n\n    while True:\n        bigram = min(pairs, key = lambda pair: bpe_codes.get(pair, float(\'inf\')))\n        if bigram not in bpe_codes:\n            break\n        first, second = bigram\n        new_word = []\n        i = 0\n        while i < len(word):\n            try:\n                j = word.index(first, i)\n                new_word.extend(word[i:j])\n                i = j\n            except:\n                new_word.extend(word[i:])\n                break\n\n            if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                new_word.append(first+second)\n                i += 2\n            else:\n                new_word.append(word[i])\n                i += 1\n        new_word = tuple(new_word)\n        word = new_word\n        if len(word) == 1:\n            break\n        else:\n            pairs = get_pairs(word)\n\n    # don\'t print end-of-word symbols\n    if word[-1] == \'</w>\':\n        word = word[:-1]\n    elif word[-1].endswith(\'</w>\'):\n        word = word[:-1] + (word[-1].replace(\'</w>\',\'\'),)\n\n    cache[orig] = word\n    return word\n\n\nif __name__ == \'__main__\':\n    parser = create_parser()\n    args = parser.parse_args()\n\n    bpe = BPE(args.codes, args.separator)\n\n    for line in args.input:\n        args.output.write(bpe.segment(line).strip())\n        args.output.write(\'\\n\')\n'"
lib/subword_nmt/bpe_toy.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Author: Rico Sennrich\n\n""""""Use byte pair encoding (BPE) to learn a variable-length encoding of the vocabulary in a text.\nUnlike the original BPE, it does not compress the plain text, but can be used to reduce the vocabulary\nof a text to a configurable number of symbols, with only a small increase in the number of tokens.\nThis is an (inefficient) toy implementation that shows the algorithm. For processing large datasets,\nindexing and incremental updates can be used to speed up the implementation (see learn_bpe.py).\n\nReference:\nRico Sennrich, Barry Haddow and Alexandra Birch (2016). Neural Machine Translation of Rare Words with Subword Units.\nProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016). Berlin, Germany.\n""""""\n\n\nimport re\nimport sys\nimport collections\n\ndef get_stats(vocab):\n  pairs = collections.defaultdict(int)\n  for word, freq in vocab.items():\n    symbols = word.split()\n    for i in range(len(symbols)-1):\n      pairs[symbols[i],symbols[i+1]] += freq\n  return pairs\n\ndef merge_vocab(pair, v_in):\n  v_out = {}\n  bigram_pattern = re.escape(\' \'.join(pair))\n  p = re.compile(r\'(?<!\\S)\' + bigram_pattern + r\'(?!\\S)\')\n  for word in v_in:\n    w_out = p.sub(\'\'.join(pair), word)\n    v_out[w_out] = v_in[word]\n  return v_out\n\nvocab = {\'l o w </w>\' : 5, \'l o w e r </w>\' : 2,\n         \'n e w e s t </w>\' : 6, \'w i d e s t </w>\' : 3}\nnum_merges = 15\nfor i in range(num_merges):\n  pairs = get_stats(vocab)\n  best = max(pairs, key=pairs.get)\n  if pairs[best] < 2:\n     sys.stderr.write(\'no pair has frequency > 1. Stopping\\n\')\n     break\n  vocab = merge_vocab(best, vocab)\n  print(best)\n'"
lib/subword_nmt/chrF.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Author: Rico Sennrich\n\n""""""Compute chrF3 for machine translation evaluation\n\nReference:\nMaja Popovi\xc4\x87 (2015). chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translationn, pages 392\xe2\x80\x93395, Lisbon, Portugal.\n""""""\n\nfrom __future__ import print_function, unicode_literals, division\nimport sys\nimport codecs\nimport io\nimport argparse\nfrom collections import defaultdict\nfrom math import log, exp\n\n# hack for python2/3 compatibility\nfrom io import open\nargparse.open = open\n\n# python 2/3 compatibility\nif sys.version_info < (3, 0):\n  sys.stderr = codecs.getwriter(\'UTF-8\')(sys.stderr)\n  sys.stdout = codecs.getwriter(\'UTF-8\')(sys.stdout)\n  sys.stdin = codecs.getreader(\'UTF-8\')(sys.stdin)\n\n\ndef create_parser():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        description=""learn BPE-based word segmentation"")\n\n    parser.add_argument(\n        \'--ref\', \'-r\', type=argparse.FileType(\'r\'), required=True,\n        metavar=\'PATH\',\n        help=""Reference file"")\n    parser.add_argument(\n        \'--hyp\', type=argparse.FileType(\'r\'), metavar=\'PATH\',\n        default=sys.stdin,\n        help=""Hypothesis file (default: stdin)."")\n    parser.add_argument(\n        \'--beta\', \'-b\', type=float, default=3,\n        metavar=\'FLOAT\',\n        help=""beta parameter (default: \'%(default)s\')"")\n    parser.add_argument(\n        \'--ngram\', \'-n\', type=int, default=6,\n        metavar=\'INT\',\n        help=""ngram order (default: \'%(default)s\')"")\n    parser.add_argument(\n        \'--space\', \'-s\', action=\'store_true\',\n        help=""take spaces into account (default: \'%(default)s\')"")\n    parser.add_argument(\n        \'--precision\', action=\'store_true\',\n        help=""report precision (default: \'%(default)s\')"")\n    parser.add_argument(\n        \'--recall\', action=\'store_true\',\n        help=""report recall (default: \'%(default)s\')"")\n\n    return parser\n\ndef extract_ngrams(words, max_length=4, spaces=False):\n\n    if not spaces:\n        words = \'\'.join(words.split())\n    else:\n        words = words.strip()\n\n    results = defaultdict(lambda: defaultdict(int))\n    for length in range(max_length):\n        for start_pos in range(len(words)):\n            end_pos = start_pos + length + 1\n            if end_pos <= len(words):\n                results[length][tuple(words[start_pos: end_pos])] += 1\n    return results\n\n\ndef get_correct(ngrams_ref, ngrams_test, correct, total):\n\n    for rank in ngrams_test:\n        for chain in ngrams_test[rank]:\n            total[rank] += ngrams_test[rank][chain]\n            if chain in ngrams_ref[rank]:\n                correct[rank] += min(ngrams_test[rank][chain], ngrams_ref[rank][chain])\n\n    return correct, total\n\n\ndef f1(correct, total_hyp, total_ref, max_length, beta=3, smooth=0):\n\n    precision = 0\n    recall = 0\n\n    for i in range(max_length):\n      if total_hyp[i] + smooth and total_ref[i] + smooth:\n        precision += (correct[i] + smooth) / (total_hyp[i] + smooth)\n        recall += (correct[i] + smooth) / (total_ref[i] + smooth)\n\n    precision /= max_length\n    recall /= max_length\n\n    return (1 + beta**2) * (precision*recall) / ((beta**2 * precision) + recall), precision, recall\n\ndef main(args):\n\n    correct = [0]*args.ngram\n    total = [0]*args.ngram\n    total_ref = [0]*args.ngram\n    for line in args.ref:\n      line2 = args.hyp.readline()\n\n      ngrams_ref = extract_ngrams(line, max_length=args.ngram, spaces=args.space)\n      ngrams_test = extract_ngrams(line2, max_length=args.ngram, spaces=args.space)\n\n      get_correct(ngrams_ref, ngrams_test, correct, total)\n\n      for rank in ngrams_ref:\n          for chain in ngrams_ref[rank]:\n              total_ref[rank] += ngrams_ref[rank][chain]\n\n    chrf, precision, recall = f1(correct, total, total_ref, args.ngram, args.beta)\n\n    print(\'chrF3: {0:.4f}\'.format(chrf))\n    if args.precision:\n        print(\'chrPrec: {0:.4f}\'.format(precision))\n    if args.recall:\n        print(\'chrRec: {0:.4f}\'.format(recall))\n\nif __name__ == \'__main__\':\n\n    parser = create_parser()\n    args = parser.parse_args()\n\n    main(args)\n'"
lib/subword_nmt/get_vocab.py,0,"b'#! /usr/bin/env python\n\nimport sys\nfrom collections import Counter\n\nc = Counter()\n\nfor line in sys.stdin:\n    for word in line.split():\n        c[word] += 1\n\nfor key,f in sorted(c.items(), key=lambda x: x[1], reverse=True):\n    print key, f\n'"
lib/subword_nmt/learn_bpe.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Author: Rico Sennrich\n\n""""""Use byte pair encoding (BPE) to learn a variable-length encoding of the vocabulary in a text.\nUnlike the original BPE, it does not compress the plain text, but can be used to reduce the vocabulary\nof a text to a configurable number of symbols, with only a small increase in the number of tokens.\n\nReference:\nRico Sennrich, Barry Haddow and Alexandra Birch (2016). Neural Machine Translation of Rare Words with Subword Units.\nProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016). Berlin, Germany.\n""""""\n\nfrom __future__ import unicode_literals\n\nimport sys\nimport codecs\nimport re\nimport copy\nimport argparse\nfrom collections import defaultdict, Counter\n\n# hack for python2/3 compatibility\nfrom io import open\nargparse.open = open\n\n# python 2/3 compatibility\nif sys.version_info < (3, 0):\n  sys.stderr = codecs.getwriter(\'UTF-8\')(sys.stderr)\n  sys.stdout = codecs.getwriter(\'UTF-8\')(sys.stdout)\n  sys.stdin = codecs.getreader(\'UTF-8\')(sys.stdin)\n\ndef create_parser():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        description=""learn BPE-based word segmentation"")\n\n    parser.add_argument(\n        \'--input\', \'-i\', type=argparse.FileType(\'r\'), default=sys.stdin,\n        metavar=\'PATH\',\n        help=""Input text (default: standard input)."")\n    parser.add_argument(\n        \'--output\', \'-o\', type=argparse.FileType(\'w\'), default=sys.stdout,\n        metavar=\'PATH\',\n        help=""Output file for BPE codes (default: standard output)"")\n    parser.add_argument(\n        \'--symbols\', \'-s\', type=int, default=10000,\n        help=""Create this many new symbols (each representing a character n-gram) (default: %(default)s))"")\n    parser.add_argument(\n        \'--verbose\', \'-v\', action=""store_true"",\n        help=""verbose mode."")\n\n    return parser\n\ndef get_vocabulary(fobj):\n    """"""Read text and return dictionary that encodes vocabulary\n    """"""\n    vocab = Counter()\n    for line in fobj:\n        for word in line.split():\n            vocab[word] += 1\n    return vocab\n\ndef update_pair_statistics(pair, changed, stats, indices):\n    """"""Minimally update the indices and frequency of symbol pairs\n\n    if we merge a pair of symbols, only pairs that overlap with occurrences\n    of this pair are affected, and need to be updated.\n    """"""\n    stats[pair] = 0\n    indices[pair] = defaultdict(int)\n    first, second = pair\n    new_pair = first+second\n    for j, word, old_word, freq in changed:\n\n        # find all instances of pair, and update frequency/indices around it\n        i = 0\n        while True:\n            try:\n                i = old_word.index(first, i)\n            except ValueError:\n                break\n            if i < len(old_word)-1 and old_word[i+1] == second:\n                if i:\n                    prev = old_word[i-1:i+1]\n                    stats[prev] -= freq\n                    indices[prev][j] -= 1\n                if i < len(old_word)-2:\n                    # don\'t double-count consecutive pairs\n                    if old_word[i+2] != first or i >= len(old_word)-3 or old_word[i+3] != second:\n                        nex = old_word[i+1:i+3]\n                        stats[nex] -= freq\n                        indices[nex][j] -= 1\n                i += 2\n            else:\n                i += 1\n\n        i = 0\n        while True:\n            try:\n                i = word.index(new_pair, i)\n            except ValueError:\n                break\n            if i:\n                prev = word[i-1:i+1]\n                stats[prev] += freq\n                indices[prev][j] += 1\n            # don\'t double-count consecutive pairs\n            if i < len(word)-1 and word[i+1] != new_pair:\n                nex = word[i:i+2]\n                stats[nex] += freq\n                indices[nex][j] += 1\n            i += 1\n\n\ndef get_pair_statistics(vocab):\n    """"""Count frequency of all symbol pairs, and create index""""""\n\n    # data structure of pair frequencies\n    stats = defaultdict(int)\n\n    #index from pairs to words\n    indices = defaultdict(lambda: defaultdict(int))\n\n    for i, (word, freq) in enumerate(vocab):\n        prev_char = word[0]\n        for char in word[1:]:\n            stats[prev_char, char] += freq\n            indices[prev_char, char][i] += 1\n            prev_char = char\n\n    return stats, indices\n\n\ndef replace_pair(pair, vocab, indices):\n    """"""Replace all occurrences of a symbol pair (\'A\', \'B\') with a new symbol \'AB\'""""""\n    first, second = pair\n    pair_str = \'\'.join(pair)\n    pair_str = pair_str.replace(\'\\\\\',\'\\\\\\\\\')\n    changes = []\n    pattern = re.compile(r\'(?<!\\S)\' + re.escape(first + \' \' + second) + r\'(?!\\S)\')\n    if sys.version_info < (3, 0):\n        iterator = indices[pair].iteritems()\n    else:\n        iterator = indices[pair].items()\n    for j, freq in iterator:\n        if freq < 1:\n            continue\n        word, freq = vocab[j]\n        new_word = \' \'.join(word)\n        new_word = pattern.sub(pair_str, new_word)\n        new_word = tuple(new_word.split())\n\n        vocab[j] = (new_word, freq)\n        changes.append((j, new_word, word, freq))\n\n    return changes\n\ndef prune_stats(stats, big_stats, threshold):\n    """"""Prune statistics dict for efficiency of max()\n\n    The frequency of a symbol pair never increases, so pruning is generally safe\n    (until we the most frequent pair is less frequent than a pair we previously pruned)\n    big_stats keeps full statistics for when we need to access pruned items\n    """"""\n    for item,freq in list(stats.items()):\n        if freq < threshold:\n            del stats[item]\n            if freq < 0:\n                big_stats[item] += freq\n            else:\n                big_stats[item] = freq\n\nif __name__ == \'__main__\':\n\n    parser = create_parser()\n    args = parser.parse_args()\n\n    vocab = get_vocabulary(args.input)\n    vocab = dict([(tuple(x)+(\'</w>\',) ,y) for (x,y) in vocab.items()])\n    sorted_vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n\n    stats, indices = get_pair_statistics(sorted_vocab)\n    big_stats = copy.deepcopy(stats)\n    # threshold is inspired by Zipfian assumption, but should only affect speed\n    threshold = max(stats.values()) / 10\n    for i in range(args.symbols):\n        if stats:\n            most_frequent = max(stats, key=stats.get)\n\n        # we probably missed the best pair because of pruning; go back to full statistics\n        if not stats or (i and stats[most_frequent] < threshold):\n            prune_stats(stats, big_stats, threshold)\n            stats = copy.deepcopy(big_stats)\n            most_frequent = max(stats, key=stats.get)\n            # threshold is inspired by Zipfian assumption, but should only affect speed\n            threshold = stats[most_frequent] * i/(i+10000.0)\n            prune_stats(stats, big_stats, threshold)\n\n        if stats[most_frequent] < 2:\n            sys.stderr.write(\'no pair has frequency > 1. Stopping\\n\')\n            break\n\n        if args.verbose:\n            sys.stderr.write(\'pair {0}: {1} {2} -> {1}{2} (frequency {3})\\n\'.format(i, most_frequent[0], most_frequent[1], stats[most_frequent]))\n        args.output.write(\'{0} {1}\\n\'.format(*most_frequent))\n        changes = replace_pair(most_frequent, sorted_vocab, indices)\n        update_pair_statistics(most_frequent, changes, stats, indices)\n        stats[most_frequent] = 0\n        if not i % 100:\n            prune_stats(stats, big_stats, threshold)\n'"
lib/subword_nmt/segment-char-ngrams.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Author: Rico Sennrich\n\nfrom __future__ import unicode_literals, division\n\nimport sys\nimport codecs\nimport argparse\n\n# hack for python2/3 compatibility\nfrom io import open\nargparse.open = open\n\n# python 2/3 compatibility\nif sys.version_info < (3, 0):\n  sys.stderr = codecs.getwriter(\'UTF-8\')(sys.stderr)\n  sys.stdout = codecs.getwriter(\'UTF-8\')(sys.stdout)\n  sys.stdin = codecs.getreader(\'UTF-8\')(sys.stdin)\n\ndef create_parser():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        description=""segment rare words into character n-grams"")\n\n    parser.add_argument(\n        \'--input\', \'-i\', type=argparse.FileType(\'r\'), default=sys.stdin,\n        metavar=\'PATH\',\n        help=""Input file (default: standard input)."")\n    parser.add_argument(\n        \'--vocab\', type=argparse.FileType(\'r\'), metavar=\'PATH\',\n        required=True,\n        help=""Vocabulary file."")\n    parser.add_argument(\n        \'--shortlist\', type=int, metavar=\'INT\', default=0,\n        help=""do not segment INT most frequent words in vocabulary (default: \'%(default)s\'))."")\n    parser.add_argument(\n        \'-n\', type=int, metavar=\'INT\', default=2,\n        help=""segment rare words into character n-grams of size INT (default: \'%(default)s\'))."")\n    parser.add_argument(\n        \'--output\', \'-o\', type=argparse.FileType(\'w\'), default=sys.stdout,\n        metavar=\'PATH\',\n        help=""Output file (default: standard output)"")\n    parser.add_argument(\n        \'--separator\', \'-s\', type=str, default=\'@@\', metavar=\'STR\',\n        help=""Separator between non-final subword units (default: \'%(default)s\'))"")\n\n    return parser\n\n\nif __name__ == \'__main__\':\n\n    parser = create_parser()\n    args = parser.parse_args()\n\n    vocab = [line.split()[0] for line in args.vocab if len(line.split()) == 2]\n    vocab = dict((y,x) for (x,y) in enumerate(vocab))\n\n    for line in args.input:\n      for word in line.split():\n        if word not in vocab or vocab[word] > args.shortlist:\n          i = 0\n          while i*args.n < len(word):\n            args.output.write(word[i*args.n:i*args.n+args.n])\n            i += 1\n            if i*args.n < len(word):\n              args.output.write(args.separator)\n            args.output.write(\' \')\n        else:\n          args.output.write(word + \' \')\n      args.output.write(\'\\n\')\n'"
neuralmonkey/attention/__init__.py,0,b'from .feed_forward import Attention\nfrom .coverage import CoverageAttention\nfrom .scaled_dot_product import ScaledDotProdAttention\n'
neuralmonkey/attention/base_attention.py,20,"b'""""""Decoding functions using multiple attentions for RNN decoders.\n\nSee http://arxiv.org/abs/1606.07481\n\nThe attention mechanisms used in Neural Monkey are inherited from the\n``BaseAttention`` class defined in this module.\n\nThe attention function can be viewed as a soft lookup over an associative\nmemory. The *query* vector is used to compute a similarity score of the *keys*\nof the associative memory and the resulting scores are used as weights in a\nweighted sum of the *values* associated with the keys. We call the\n(unnormalized) similarity scores *energies*, we call *attention distribution*\nthe energies after (softmax) normalization, and we call the resulting\nweighted sum of states a *context vector*.\n\nNote that it is possible (and true in most cases) that the attention keys\nare equal to the values. In case of self-attention, even queries are from the\nsame set of vectors.\n\nTo abstract over different flavors of attention mechanism, we conceptualize the\nprocedure as follows: Each attention object has the ``attention`` function\nwhich operates on the query tensor. The attention function receives the query\ntensor (the decoder state) and optionally the previous state of the decoder,\nand computes the context vector. The function also receives a *loop state*,\nwhich is used to store data in an autoregressive loop that generates a\nsequence.\n\nThe attention uses the loop state to store to store attention distributions\nand context vectors in time. This structure is called ``AttentionLoopState``.\nTo be able to initialize the loop state, each attention object that uses this\nfeature defines the ``initial_loop_state`` function with empty tensors.\n\nSince there can be many *modes* in which the decoder that uses the attention\noperates, the attention objects have the ``finalize_loop`` method, which takes\nthe last attention loop state and the name of the mode (a string) and processes\nthis data to be available in the ``histories`` dictionary. The single and most\nused example of two *modes* are the *train* and *runtime* modes of the\nautoregressive decoder.\n""""""\nfrom typing import Dict, Optional, Any, Tuple, Union\n\nimport tensorflow as tf\n\nfrom neuralmonkey.attention.namedtuples import AttentionLoopState\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.model.parameterized import InitializerSpecs\nfrom neuralmonkey.model.stateful import TemporalStateful, SpatialStateful\n\n# pylint: disable=invalid-name\nAttendable = Union[TemporalStateful, SpatialStateful]\n# pylint: enable=invalid-name\n\n\ndef empty_attention_loop_state(\n        batch_size: Union[int, tf.Tensor],\n        length: Union[int, tf.Tensor],\n        dimension: Union[int, tf.Tensor]) -> AttentionLoopState:\n    """"""Create an empty attention loop state.\n\n    The attention loop state is a technical object for storing the attention\n    distributions and the context vectors in time. It is used with the\n    ``tf.while_loop`` dynamic implementation of decoders.\n\n    Arguments:\n        batch_size: The size of the batch.\n        length: The number of encoder states (keys).\n        dimension: The dimension of the context vector\n\n    Returns:\n        This function returns an empty attention loop state which means\n        there are two empty Tensors one for attention distributions in time,\n        and one for the attention context vectors in time.\n    """"""\n    return AttentionLoopState(\n        contexts=tf.zeros(shape=[0, batch_size, dimension], name=""contexts""),\n        weights=tf.zeros(shape=[0, batch_size, length], name=""distributions""))\n\n\ndef get_attention_states(encoder: Attendable) -> tf.Tensor:\n    """"""Return the temporal or spatial states of an encoder.\n\n    Arguments:\n        encoder: The encoder with the states to attend.\n\n    Returns:\n        Either a 3D or a 4D tensor, depending on whether the encoder is\n        temporal (e.g. recurrent encoder) or spatial (e.g. a CNN encoder).\n        The first two dimensions are (batch, time).\n    """"""\n    if isinstance(encoder, TemporalStateful):\n        return encoder.temporal_states\n\n    if isinstance(encoder, SpatialStateful):\n        shape = encoder.spatial_states.get_shape().as_list()\n        return tf.reshape(encoder.spatial_states,\n                          [-1, shape[1] * shape[2], shape[3]])\n\n    raise TypeError(""Unknown encoder type"")\n\n\ndef get_attention_mask(encoder: Attendable) -> Optional[tf.Tensor]:\n    """"""Return the temporal or spatial mask of an encoder.\n\n    Arguments:\n        encoder: The encoder to get the mask from.\n\n    Returns:\n        Either a 2D or a 3D tensor, depending on whether the encoder is\n        temporal (e.g. recurrent encoder) or spatial (e.g. a CNN encoder).\n    """"""\n    if isinstance(encoder, TemporalStateful):\n        if encoder.temporal_mask is None:\n            raise ValueError(""The encoder temporal mask should not be none"")\n        return encoder.temporal_mask\n\n    if isinstance(encoder, SpatialStateful):\n        if encoder.spatial_mask is None:\n            return None\n        shape = encoder.spatial_states.get_shape().as_list()\n        return tf.reshape(encoder.spatial_mask, [-1, shape[1] * shape[2]])\n\n    raise TypeError(""Unknown encoder type"")\n\n\nclass BaseAttention(ModelPart):\n    """"""The abstract class for the attenion mechanism flavors.""""""\n\n    def __init__(self,\n                 name: str,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        """"""Create a new ``BaseAttention`` object.""""""\n        ModelPart.__init__(\n            self, name, reuse, save_checkpoint, load_checkpoint, initializers)\n\n        self.query_state_size = None  # type: tf.Tensor\n        self._histories = {}  # type: Dict[str, tf.Tensor]\n\n    @property\n    def histories(self) -> Dict[str, tf.Tensor]:\n        """"""Return the attention histories dictionary.\n\n        Use this property after it has been populated.\n\n        Returns:\n            The attention histories dictionary.\n        """"""\n        return self._histories\n\n    def attention(self,\n                  query: tf.Tensor,\n                  decoder_prev_state: tf.Tensor,\n                  decoder_input: tf.Tensor,\n                  loop_state: Any) -> Tuple[tf.Tensor, Any]:\n        """"""Get context vector for a given query.""""""\n        raise NotImplementedError(""Abstract method"")\n\n    def initial_loop_state(self) -> Any:\n        """"""Get initial loop state for the attention object.\n\n        Returns:\n            The newly created initial loop state object.\n        """"""\n        raise NotImplementedError(""Abstract method"")\n\n    def finalize_loop(self, key: str, last_loop_state: Any) -> None:\n        """"""Store the attention histories from loop state under a given key.\n\n        Arguments:\n            key: The key to the histories dictionary to store the data in.\n            last_loop_state: The loop state object from the last state of\n                the decoding loop.\n        """"""\n        raise NotImplementedError(""Abstract method"")\n\n    @property\n    def context_vector_size(self) -> int:\n        """"""Return the static size of the context vector.\n\n        Returns:\n            An integer specifying the context vector dimension.\n        """"""\n        raise NotImplementedError(""Abstract property"")\n\n    def visualize_attention(self, key: str, max_outputs: int = 16) -> None:\n        """"""Include the attention histories under a given key into a summary.\n\n        Arguments:\n            key: The key to the attention histories dictionary.\n            max_outputs: Maximum number of images to save.\n        """"""\n        if key not in self.histories:\n            raise KeyError(\n                ""Key {} not among attention histories"".format(key))\n\n        alignments = tf.expand_dims(\n            tf.transpose(self.histories[key], perm=[1, 2, 0]), -1)\n\n        summary_name = ""{}.{}"".format(self.name, key)\n\n        tf.summary.image(\n            summary_name, alignments, collections=[""summary_att_plots""],\n            max_outputs=max_outputs)\n'"
neuralmonkey/attention/combination.py,84,"b'""""""Attention combination strategies.\n\nThis modules implements attention combination strategies for multi-encoder\nscenario when we may want to combine the hidden states of the encoders in\nmore complicated fashion.\n\nCurrently there are two attention combination strategies flat and hierarchical\n(see paper `Attention Combination Strategies for Multi-Source\nSequence-to-Sequence Learning <www.aclweb.org/anthology/P/P17/P17-2031.pdf>`_).\n\nThe combination strategies may use the sentinel mechanism which allows the\ndecoder not to attend to the, and extract information on its own hidden state\n(see paper `Knowing when to Look: Adaptive Attention via a Visual Sentinel for\nImage Captioning  <https://arxiv.org/pdf/1612.01887.pdf>`_).\n""""""\nfrom typing import Any, List, Tuple\n\nfrom typeguard import check_argument_types\nimport tensorflow as tf\n\nfrom neuralmonkey.attention.base_attention import (\n    BaseAttention, AttentionLoopState, empty_attention_loop_state,\n    get_attention_states, get_attention_mask, Attendable)\nfrom neuralmonkey.attention.namedtuples import HierarchicalLoopState\nfrom neuralmonkey.checking import assert_shape\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.logging import debug\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.model.parameterized import InitializerSpecs\nfrom neuralmonkey.tf_utils import get_variable\n\n\nclass MultiAttention(BaseAttention):\n    """"""Base class for attention combination.""""""\n\n    # pylint: disable=unused-argument,too-many-arguments\n    def __init__(self,\n                 name: str,\n                 attention_state_size: int,\n                 share_attn_projections: bool = False,\n                 use_sentinels: bool = False,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        BaseAttention.__init__(self, name, reuse, save_checkpoint,\n                               load_checkpoint, initializers)\n        self.attentions_in_time = []  # type: List[tf.Tensor]\n        self.attention_state_size = attention_state_size\n        self._share_projections = share_attn_projections\n        self._use_sentinels = use_sentinels\n\n        self.att_scope_name = ""attention_{}"".format(name)\n    # pylint: enable=unused-argument,too-many-arguments\n\n    def attention(self,\n                  query: tf.Tensor,\n                  decoder_prev_state: tf.Tensor,\n                  decoder_input: tf.Tensor,\n                  loop_state: Any) -> Tuple[tf.Tensor, Any]:\n        """"""Get context vector for given decoder state.""""""\n        raise NotImplementedError(""Abstract method"")\n\n    @tensor\n    def attn_v(self) -> tf.Tensor:\n        return get_variable(\n            ""attn_v"", [1, 1, self.attention_state_size],\n            initializer=tf.random_normal_initializer(stddev=0.001))\n\n    @property\n    def attn_size(self):\n        return self.attention_state_size\n\n    def _vector_logit(self,\n                      projected_decoder_state: tf.Tensor,\n                      vector_value: tf.Tensor,\n                      scope: str) -> tf.Tensor:\n        """"""Get logit for a single vector, e.g., sentinel vector.""""""\n        assert_shape(projected_decoder_state, [-1, 1, -1])\n        assert_shape(vector_value, [-1, -1])\n\n        with tf.variable_scope(""{}_logit"".format(scope)):\n            vector_bias = get_variable(\n                ""vector_bias"", [],\n                initializer=tf.zeros_initializer())\n\n            proj_vector_for_logit = tf.expand_dims(\n                tf.layers.dense(vector_value, self.attention_state_size,\n                                name=""vector_projection""), 1)\n\n            if self._share_projections:\n                proj_vector_for_ctx = proj_vector_for_logit\n            else:\n                proj_vector_for_ctx = tf.expand_dims(\n                    tf.layers.dense(vector_value, self.attention_state_size,\n                                    name=""vector_ctx_proj""), 1)\n\n            vector_logit = tf.reduce_sum(\n                self.attn_v\n                * tf.tanh(projected_decoder_state + proj_vector_for_logit),\n                [2]) + vector_bias\n            assert_shape(vector_logit, [-1, 1])\n            return proj_vector_for_ctx, vector_logit\n\n\nclass FlatMultiAttention(MultiAttention):\n    """"""Flat attention combination strategy.\n\n    Using this attention combination strategy, hidden states of the encoders\n    are first projected to the same space (different projection for different\n    encoders) and then we compute a joint distribution over all the hidden\n    states. The context vector is then a weighted sum of another / then\n    projection of the encoders hidden states. The sentinel vector can be added\n    as an additional hidden state.\n\n    See equations 8 to 10 in the Attention Combination Strategies paper.\n    """"""\n\n    # pylint: disable=too-many-arguments\n    def __init__(self,\n                 name: str,\n                 encoders: List[Attendable],\n                 attention_state_size: int,\n                 share_attn_projections: bool = False,\n                 use_sentinels: bool = False,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        check_argument_types()\n        MultiAttention.__init__(\n            self,\n            name=name,\n            attention_state_size=attention_state_size,\n            share_attn_projections=share_attn_projections,\n            use_sentinels=use_sentinels,\n            reuse=reuse,\n            save_checkpoint=save_checkpoint,\n            load_checkpoint=load_checkpoint,\n            initializers=initializers)\n        self._encoders = encoders\n    # pylint: enable=too-many-arguments\n\n    @tensor\n    def _encoders_tensors(self) -> List[tf.Tensor]:\n        tensors = [get_attention_states(e) for e in self._encoders]\n        for e_t in tensors:\n            assert_shape(e_t, [-1, -1, -1])\n        return tensors\n\n    @tensor\n    def _encoders_masks(self) -> List[tf.Tensor]:\n        masks = [get_attention_mask(e) for e in self._encoders]\n        for e_m in masks:\n            assert_shape(e_m, [-1, -1])\n\n        if self._use_sentinels:\n            masks.append(tf.ones([tf.shape(masks[0])[0], 1]))\n        return masks\n\n    @tensor\n    def encoder_projections_for_logits(self) -> List[tf.Tensor]:\n        return self.get_encoder_projections(""logits_projections"")\n\n    @tensor\n    def encoder_attn_biases(self) -> List[tf.Variable]:\n        return [get_variable(name=""attn_bias_{}"".format(i), shape=[],\n                             initializer=tf.zeros_initializer())\n                for i in range(len(self._encoders_tensors))]\n\n    @tensor\n    def encoder_projections_for_ctx(self) -> List[tf.Tensor]:\n        if self._share_projections:\n            return self.encoder_projections_for_logits\n        return self.get_encoder_projections(""context_projections"")\n\n    @tensor\n    def masks_concat(self) -> tf.Tensor:\n        return tf.concat(self._encoders_masks, 1)\n\n    def initial_loop_state(self) -> AttentionLoopState:\n\n        # Similarly to the feed_forward attention, we need to build the encoder\n        # projections and masks before the while loop is entered so they are\n        # not created as a part of the loop\n\n        # pylint: disable=not-an-iterable\n        for val in self.encoder_projections_for_logits:\n            debug(val, ""bless"")\n        debug(self.masks_concat, ""bless"")\n\n        length = sum(tf.shape(s)[1] for s in self._encoders_tensors)\n        # pylint: enable=not-an-iterable\n\n        if self._use_sentinels:\n            length += 1\n\n        return empty_attention_loop_state(self.batch_size, length,\n                                          self.context_vector_size)\n\n    def get_encoder_projections(self, scope: str) -> List[tf.Tensor]:\n        encoder_projections = []\n        with tf.variable_scope(scope):\n            for i, encoder_tensor in enumerate(self._encoders_tensors):\n                encoder_state_size = encoder_tensor.get_shape()[2].value\n                encoder_tensor_shape = tf.shape(encoder_tensor)\n\n                proj_matrix = get_variable(\n                    ""proj_matrix_{}"".format(i),\n                    [encoder_state_size, self.attention_state_size],\n                    initializer=tf.random_normal_initializer(stddev=0.001))\n\n                proj_bias = get_variable(\n                    ""proj_bias_{}"".format(i),\n                    shape=[self.attention_state_size],\n                    initializer=tf.zeros_initializer())\n\n                encoder_tensor_2d = tf.reshape(\n                    encoder_tensor, [-1, encoder_state_size])\n\n                projected_2d = tf.matmul(\n                    encoder_tensor_2d, proj_matrix) + proj_bias\n                assert_shape(projected_2d, [-1, self.attention_state_size])\n\n                projection = tf.reshape(\n                    projected_2d, [encoder_tensor_shape[0],\n                                   encoder_tensor_shape[1],\n                                   self.attention_state_size])\n\n                encoder_projections.append(projection)\n            return encoder_projections\n\n    # pylint: disable=unsubscriptable-object\n    @property\n    def context_vector_size(self) -> int:\n        return self.encoder_projections_for_ctx[0].get_shape()[2].value\n    # pylint: enable=unsubscriptable-object\n\n    # pylint: disable=too-many-locals\n    def attention(self,\n                  query: tf.Tensor,\n                  decoder_prev_state: tf.Tensor,\n                  decoder_input: tf.Tensor,\n                  loop_state: AttentionLoopState) -> Tuple[\n                      tf.Tensor, AttentionLoopState]:\n\n        with tf.variable_scope(self.att_scope_name):\n            projected_state = tf.layers.dense(query, self.attention_state_size)\n            projected_state = tf.expand_dims(projected_state, 1)\n\n            assert_shape(projected_state, [-1, 1, self.attention_state_size])\n\n            logits = []\n\n            for proj, bias in zip(self.encoder_projections_for_logits,\n                                  self.encoder_attn_biases):\n\n                logits.append(tf.reduce_sum(\n                    self.attn_v * tf.tanh(projected_state + proj), [2]) + bias)\n\n            if self._use_sentinels:\n                sentinel_value = _sentinel(query,\n                                           decoder_prev_state,\n                                           decoder_input)\n                projected_sentinel, sentinel_logit = self._vector_logit(\n                    projected_state, sentinel_value, scope=""sentinel"")\n                logits.append(sentinel_logit)\n\n            attentions = self._renorm_softmax(tf.concat(logits, 1))\n\n            self.attentions_in_time.append(attentions)\n\n            if self._use_sentinels:\n                tiled_encoder_projections = self._tile_encoders_for_beamsearch(\n                    projected_sentinel)\n\n                projections_concat = tf.concat(\n                    tiled_encoder_projections + [projected_sentinel], 1)\n\n            else:\n                projections_concat = tf.concat(\n                    self.encoder_projections_for_ctx, 1)\n\n            contexts = tf.reduce_sum(\n                tf.expand_dims(attentions, 2) * projections_concat, [1])\n\n            next_contexts = tf.concat(\n                [loop_state.contexts, tf.expand_dims(contexts, 0)], 0)\n            next_weights = tf.concat(\n                [loop_state.weights, tf.expand_dims(attentions, 0)], 0)\n\n            next_loop_state = AttentionLoopState(\n                contexts=next_contexts,\n                weights=next_weights)\n\n            return contexts, next_loop_state\n    # pylint: enable=too-many-locals\n\n    # pylint: disable=not-an-iterable,unsubscriptable-object\n    def _tile_encoders_for_beamsearch(self, projected_sentinel):\n        sentinel_batch_size = tf.shape(projected_sentinel)[0]\n        encoders_batch_size = tf.shape(\n            self.encoder_projections_for_ctx[0])[0]\n\n        modulo = tf.mod(sentinel_batch_size, encoders_batch_size)\n\n        with tf.control_dependencies([tf.assert_equal(modulo, 0)]):\n            beam_size = tf.div(sentinel_batch_size,\n                               encoders_batch_size)\n\n        return [tf.tile(proj, [beam_size, 1, 1])\n                for proj in self.encoder_projections_for_ctx]\n    # pylint: enable=not-an-iterable,unsubscriptable-object\n\n    def _renorm_softmax(self, logits):\n        """"""Renormalized softmax wrt. attention mask.""""""\n        softmax_concat = tf.nn.softmax(logits) * self.masks_concat\n        norm = tf.reduce_sum(softmax_concat, 1, keepdims=True) + 1e-8\n        attentions = softmax_concat / norm\n\n        return attentions\n\n    def finalize_loop(self, key: str,\n                      last_loop_state: AttentionLoopState) -> None:\n        # TODO factorization of the flat distribution across encoders\n        # could take place here.\n        self.histories[key] = last_loop_state.weights\n\n\ndef _sentinel(state, prev_state, input_):\n    """"""Sentinel value given the decoder state.""""""\n    with tf.variable_scope(""sentinel""):\n\n        decoder_state_size = state.get_shape()[-1].value\n        st_with_inp = tf.concat([prev_state, input_], 1)\n\n        gate = tf.nn.sigmoid(tf.layers.dense(st_with_inp, decoder_state_size))\n        sentinel_value = gate * state\n\n        assert_shape(sentinel_value, [-1, decoder_state_size])\n\n        return sentinel_value\n\n\nclass HierarchicalMultiAttention(MultiAttention):\n    """"""Hierarchical attention combination.\n\n    Hierarchical attention combination strategy first computes the context\n    vector for each encoder separately using whatever attention type the\n    encoders have. After that it computes a second attention over the resulting\n    context vectors and optionally the sentinel vector.\n\n    See equations 6 and 7 in the Attention Combination Strategies paper.\n    """"""\n\n    # pylint: disable=too-many-arguments\n    def __init__(self,\n                 name: str,\n                 attentions: List[BaseAttention],\n                 attention_state_size: int,\n                 use_sentinels: bool,\n                 share_attn_projections: bool,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        check_argument_types()\n        MultiAttention.__init__(\n            self,\n            name=name,\n            attention_state_size=attention_state_size,\n            use_sentinels=use_sentinels,\n            share_attn_projections=share_attn_projections,\n            reuse=reuse,\n            save_checkpoint=save_checkpoint,\n            load_checkpoint=load_checkpoint,\n            initializers=initializers)\n\n        self.attentions = attentions\n    # pylint: enable=too-many-arguments\n\n    def initial_loop_state(self) -> HierarchicalLoopState:\n        length = len(self.attentions)\n        if self._use_sentinels:\n            length += 1\n\n        return HierarchicalLoopState(\n            child_loop_states=[a.initial_loop_state()\n                               for a in self.attentions],\n            loop_state=empty_attention_loop_state(\n                self.batch_size, length, self.context_vector_size))\n\n    # pylint: disable=too-many-locals\n    def attention(self,\n                  query: tf.Tensor,\n                  decoder_prev_state: tf.Tensor,\n                  decoder_input: tf.Tensor,\n                  loop_state: HierarchicalLoopState) -> Tuple[\n                      tf.Tensor, HierarchicalLoopState]:\n\n        with tf.variable_scope(self.att_scope_name):\n            projected_state = tf.layers.dense(query, self.attention_state_size)\n            projected_state = tf.expand_dims(projected_state, 1)\n\n            assert_shape(projected_state, [-1, 1, self.attention_state_size])\n            attn_ctx_vectors, child_loop_states = zip(*[\n                a.attention(query, decoder_prev_state, decoder_input, ls)\n                for a, ls in zip(self.attentions,\n                                 loop_state.child_loop_states)])\n\n            proj_ctxs, attn_logits = [list(t) for t in zip(*[\n                self._vector_logit(projected_state,\n                                   ctx_vec, scope=att.name)  # type: ignore\n                for ctx_vec, att in zip(attn_ctx_vectors, self.attentions)])]\n\n            if self._use_sentinels:\n                sentinel_value = _sentinel(query,\n                                           decoder_prev_state,\n                                           decoder_input)\n                proj_sentinel, sentinel_logit = self._vector_logit(\n                    projected_state, sentinel_value, scope=""sentinel"")\n                proj_ctxs.append(proj_sentinel)\n                attn_logits.append(sentinel_logit)\n\n            attention_distr = tf.nn.softmax(tf.concat(attn_logits, 1))\n            self.attentions_in_time.append(attention_distr)\n\n            if self._share_projections:\n                output_cxts = proj_ctxs\n            else:\n                output_cxts = [\n                    tf.expand_dims(\n                        tf.layers.dense(ctx_vec, self.attention_state_size,\n                                        name=""proj_attn_{}"".format(\n                                            att.name)), 1)  # type: ignore\n                    for ctx_vec, att in zip(attn_ctx_vectors, self.attentions)]\n                if self._use_sentinels:\n                    output_cxts.append(tf.expand_dims(\n                        tf.layers.dense(\n                            sentinel_value, self.attention_state_size,\n                            name=""proj_sentinel""), 1))\n\n            projections_concat = tf.concat(output_cxts, 1)\n            context = tf.reduce_sum(\n                tf.expand_dims(attention_distr, 2) * projections_concat, [1])\n\n            prev_loop_state = loop_state.loop_state\n\n            next_contexts = tf.concat(\n                [prev_loop_state.contexts, tf.expand_dims(context, 0)], axis=0)\n            next_weights = tf.concat(\n                [prev_loop_state.weights, tf.expand_dims(attention_distr, 0)],\n                axis=0)\n\n            next_loop_state = AttentionLoopState(\n                contexts=next_contexts,\n                weights=next_weights)\n\n            next_hier_loop_state = HierarchicalLoopState(\n                child_loop_states=list(child_loop_states),\n                loop_state=next_loop_state)\n\n            return context, next_hier_loop_state\n    # pylint: enable=too-many-locals\n\n    def finalize_loop(self, key: str, last_loop_state: Any) -> None:\n        for c_attention, c_loop_state in zip(\n                self.attentions, last_loop_state.child_loop_states):\n            c_attention.finalize_loop(key, c_loop_state)\n\n        self.histories[key] = last_loop_state.loop_state.weights\n\n    @property\n    def context_vector_size(self) -> int:\n        return self.attention_state_size\n'"
neuralmonkey/attention/coverage.py,12,"b'""""""Coverage attention introduced in Tu et al. (2016).\n\nSee arxiv.org/abs/1601.04811\n\nThe CoverageAttention class inherites from the basic feed-forward attention\nintroduced by Bahdanau et al. (2015)\n""""""\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.attention.base_attention import Attendable\nfrom neuralmonkey.attention.feed_forward import Attention\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.model.parameterized import InitializerSpecs\nfrom neuralmonkey.tf_utils import get_variable\n\n\nclass CoverageAttention(Attention):\n    # pylint: disable=too-many-arguments\n    def __init__(self,\n                 name: str,\n                 encoder: Attendable,\n                 dropout_keep_prob: float = 1.0,\n                 state_size: int = None,\n                 max_fertility: int = 5,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        check_argument_types()\n        Attention.__init__(self, name, encoder, dropout_keep_prob, state_size,\n                           reuse, save_checkpoint, load_checkpoint,\n                           initializers)\n        self.max_fertility = max_fertility\n    # pylint: enable=too-many-arguments\n\n    @tensor\n    def coverage_weights(self) -> tf.Variable:\n        return get_variable(""coverage_matrix"", [1, 1, 1, self.state_size])\n\n    @tensor\n    def fertility_weights(self) -> tf.Variable:\n        return get_variable(\n            ""fertility_matrix"", [1, 1, self.context_vector_size])\n\n    @tensor\n    def fertility(self) -> tf.Tensor:\n        return 1e-8 + self.max_fertility * tf.sigmoid(\n            tf.reduce_sum(self.fertility_weights * self.attention_states, [2]))\n\n    def get_energies(self, y: tf.Tensor, weights_in_time: tf.Tensor):\n        weight_sum = tf.cond(\n            tf.greater(weights_in_time.size(), 0),\n            lambda: tf.reduce_sum(weights_in_time, axis=0),\n            lambda: 0.0)\n\n        coverage = weight_sum / self.fertility * self.attention_mask\n        coverage_exp = tf.expand_dims(tf.expand_dims(coverage, -1), -1)\n        logits = tf.reduce_sum(\n            self.similarity_bias_vector * tf.tanh(\n                self.hidden_features + y\n                + self.coverage_weights * coverage_exp),\n            [2, 3])\n\n        return logits\n'"
neuralmonkey/attention/feed_forward.py,41,"b'""""""The feed-forward attention mechanism.\n\nThis is the attention mechanism used in Bahdanau et al. (2015)\n\nSee arxiv.org/abs/1409.0473\n""""""\nfrom typing import Optional, Tuple\n\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.attention.base_attention import (\n    BaseAttention, AttentionLoopState, empty_attention_loop_state,\n    get_attention_states, get_attention_mask, Attendable)\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.logging import debug\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.model.parameterized import InitializerSpecs\nfrom neuralmonkey.nn.utils import dropout\nfrom neuralmonkey.tf_utils import get_variable\n\n\nclass Attention(BaseAttention):\n\n    # pylint: disable=too-many-arguments\n    def __init__(self,\n                 name: str,\n                 encoder: Attendable,\n                 dropout_keep_prob: float = 1.0,\n                 state_size: int = None,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        check_argument_types()\n        BaseAttention.__init__(\n            self, name, reuse, save_checkpoint, load_checkpoint, initializers)\n\n        self.encoder = encoder\n        self.dropout_keep_prob = dropout_keep_prob\n        self._state_size = state_size\n\n        self._variable_scope.set_initializer(\n            tf.random_normal_initializer(stddev=0.001))\n    # pylint: enable=too-many-arguments\n\n    @tensor\n    def attention_states(self) -> tf.Tensor:\n        return dropout(get_attention_states(self.encoder),\n                       self.dropout_keep_prob,\n                       self.train_mode)\n\n    @tensor\n    def attention_mask(self) -> Optional[tf.Tensor]:\n        return get_attention_mask(self.encoder)\n\n    # pylint: disable=no-member\n    # Pylint fault from resolving tensor decoration\n    @property\n    def context_vector_size(self) -> int:\n        return self.attention_states.get_shape()[2].value\n    # pylint: enable=no-member\n\n    @property\n    def state_size(self) -> int:\n        if self._state_size is not None:\n            return self._state_size\n        return self.context_vector_size\n\n    @tensor\n    def query_projection_matrix(self) -> tf.Variable:\n        with tf.variable_scope(""Attention""):\n            return get_variable(\n                name=""attn_query_projection"",\n                shape=[self.query_state_size, self.state_size])\n\n    @tensor\n    def key_projection_matrix(self) -> tf.Variable:\n        return get_variable(\n            name=""attn_key_projection"",\n            # TODO tohle neni spravne\n            shape=[self.context_vector_size, self.state_size])\n\n    @tensor\n    def similarity_bias_vector(self) -> tf.Variable:\n        return get_variable(\n            name=""attn_similarity_v"",\n            shape=[self.state_size])\n\n    @tensor\n    def projection_bias_vector(self) -> tf.Variable:\n        return get_variable(\n            name=""attn_projection_bias"", shape=[self.state_size],\n            initializer=tf.zeros_initializer())\n\n    # pylint: disable=no-self-use\n    # Implicit self use in tensor annotation\n    @tensor\n    def bias_term(self) -> tf.Variable:\n        return get_variable(\n            name=""attn_bias"", shape=[],\n            initializer=tf.zeros_initializer())\n    # pylint: enable=no-self-use\n\n    @tensor\n    def _att_states_reshaped(self) -> tf.Tensor:\n        # To calculate W1 * h_t we use a 1-by-1 convolution, need to\n        # reshape before.\n        return tf.expand_dims(self.attention_states, 2)\n\n    @tensor\n    def hidden_features(self) -> tf.Tensor:\n        # This variable corresponds to Bahdanau\'s U_a in the paper\n        key_proj_reshaped = tf.expand_dims(\n            tf.expand_dims(self.key_projection_matrix, 0), 0)\n\n        return tf.nn.conv2d(\n            self._att_states_reshaped, key_proj_reshaped, [1, 1, 1, 1], ""SAME"")\n\n    def get_energies(self, y, _):\n        return tf.reduce_sum(\n            self.similarity_bias_vector * tf.tanh(self.hidden_features + y),\n            [2, 3]) + self.bias_term\n\n    def attention(self,\n                  query: tf.Tensor,\n                  decoder_prev_state: tf.Tensor,\n                  decoder_input: tf.Tensor,\n                  loop_state: AttentionLoopState) -> Tuple[\n                      tf.Tensor, AttentionLoopState]:\n        self.query_state_size = query.get_shape()[-1].value\n\n        y = tf.matmul(query, self.query_projection_matrix)\n        y = y + self.projection_bias_vector\n        y = tf.reshape(y, [-1, 1, 1, self.state_size])\n\n        energies = self.get_energies(y, loop_state.weights)\n\n        if self.attention_mask is None:\n            weights = tf.nn.softmax(energies)\n        else:\n            weights_all = tf.nn.softmax(energies) * self.attention_mask\n            norm = tf.reduce_sum(weights_all, 1, keepdims=True) + 1e-8\n            weights = weights_all / norm\n\n            # condition = tf.equal(self.attention_mask, 1)\n            # masked_logits = tf.where(\n            #     tf.tile(condition, [tf.shape(energies)[0], 1]),\n            #     energies, -np.inf * tf.ones_like(energies))\n            # weights = tf.nn.softmax(masked_logits)\n\n        # Now calculate the attention-weighted vector d.\n        context = tf.reduce_sum(\n            tf.expand_dims(tf.expand_dims(weights, -1), -1)\n            * self._att_states_reshaped, [1, 2])\n        context = tf.reshape(context, [-1, self.context_vector_size])\n\n        next_contexts = tf.concat(\n            [loop_state.contexts, tf.expand_dims(context, 0)], 0)\n        next_weights = tf.concat(\n            [loop_state.weights, tf.expand_dims(weights, 0)], 0)\n        next_loop_state = AttentionLoopState(\n            contexts=next_contexts,\n            weights=next_weights)\n\n        return context, next_loop_state\n\n    def initial_loop_state(self) -> AttentionLoopState:\n\n        # Here we need to make sure that the hidden_features and attention_mask\n        # are pre-computed. If this is used in combination with a decoder which\n        # has train and runtime while loops, these tensors need to be created\n        # outside of any of those loops in order to be available to both.\n\n        # Note that we are not breaking lazy loading here because this method\n        # is called from a lazy tensor.\n\n        debug(""Pre-computing attention tensors"", ""bless"")\n        debug(""Hidden features: {}"".format(self.hidden_features), ""bless"")\n        debug(""Hidden mask: {}"".format(self.attention_mask), ""bless"")\n\n        return empty_attention_loop_state(\n            self.batch_size,\n            tf.shape(self.attention_states)[1],\n            self.context_vector_size)\n\n    def finalize_loop(self, key: str,\n                      last_loop_state: AttentionLoopState) -> None:\n        self.histories[key] = last_loop_state.weights\n'"
neuralmonkey/attention/namedtuples.py,4,"b'from typing import NamedTuple, List\nimport tensorflow as tf\n\n\nclass AttentionLoopState(NamedTuple(\n        ""AttentionLoopState"",\n        [(""contexts"", tf.Tensor),\n         (""weights"", tf.Tensor)])):\n    """"""Basic loop state of an attention mechanism.\n\n    Attributes:\n        contexts: A tensor of shape ``(query_time, batch, context_dim)`` which\n            stores the context vectors for every decoder time step.\n        weights: A tensor of shape ``(query_time, batch, keys_len)`` which\n            stores the attention distribution over the keys given the query in\n            each decoder time step.\n    """"""\n\n\nclass HierarchicalLoopState(NamedTuple(\n        ""HierarchicalLoopState"",\n        [(""child_loop_states"", List),\n         (""loop_state"", AttentionLoopState)])):\n    """"""Loop state of the hierarchical attention mechanism.\n\n    The input to the hierarchical attetnion is the output of a set of\n    underlying (child) attentions. To record the inner states of the underlying\n    attentions, we use the ``HierarchicalLoopState``, which holds information\n    about both the underlying attentions, and the top-level attention itself.\n\n    Attributes:\n        child_loop_states: A list of attention loop states of the underlying\n            attention mechanisms.\n        loop_state: The attention loop state of the top-level attention.\n    """"""\n\n\nclass MultiHeadLoopState(NamedTuple(\n        ""MultiHeadLoopState"",\n        [(""contexts"", tf.Tensor),\n         (""head_weights"", List[tf.Tensor])])):\n    """"""Loop state of a multi-head attention.\n\n    Attributes:\n        contexts: A tensor of shape ``(query_time, batch, context_dim)`` which\n            stores the context vectors for every decoder time step.\n        head_weights: A tensor of shape ``(query_time, n_heads, batch,\n            keys_len)`` which stores the attention distribution over the keys\n            given the query in each decoder time step **for each attention\n            head**.\n    """"""\n'"
neuralmonkey/attention/scaled_dot_product.py,58,"b'""""""The scaled dot-product attention mechanism defined in Vaswani et al. (2017).\n\nThe attention energies are computed as dot products between the query vector\nand the key vector. The query vector is scaled down by the square root of its\ndimensionality. This attention function has no trainable parameters.\n\nSee arxiv.org/abs/1706.03762\n""""""\nimport math\nfrom typing import Tuple, Callable, Union\n\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.attention.base_attention import (\n    BaseAttention, Attendable, get_attention_states, get_attention_mask)\nfrom neuralmonkey.attention.namedtuples import MultiHeadLoopState\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.model.parameterized import InitializerSpecs\nfrom neuralmonkey.nn.utils import dropout\n\n\ndef split_for_heads(x: tf.Tensor, n_heads: int, head_dim: int) -> tf.Tensor:\n    """"""Split a tensor for multi-head attention.\n\n    Split last dimension of 3D vector of shape ``(batch, time, dim)`` and\n    return a 4D vector with shape ``(batch, n_heads, time, dim/n_heads)``.\n\n    Arguments:\n        x: input Tensor of shape ``(batch, time, dim)``.\n        n_heads: Number of attention heads.\n        head_dim: Dimension of the attention heads.\n\n    Returns:\n        A 4D Tensor of shape ``(batch, n_heads, time, head_dim/n_heads)``\n    """"""\n    x_shape = tf.shape(x)\n    x_4d = tf.reshape(tf.expand_dims(x, 2),\n                      [x_shape[0], x_shape[1], n_heads, head_dim])\n\n    return tf.transpose(x_4d, perm=[0, 2, 1, 3])\n\n\ndef mask_energies(energies_4d: tf.Tensor,\n                  mask: tf.Tensor,\n                  mask_value=-1e9) -> tf.Tensor:\n    """"""Apply mask to the attention energies before passing to softmax.\n\n    Arguments:\n        energies_4d: Energies of shape ``(batch, n_heads, time(q), time(k))``.\n        mask: Float Tensor of zeros and ones of shape ``(batch, time(k))``,\n            specifies valid positions in the energies tensor.\n        mask_value: Value used to mask energies. Default taken value\n            from tensor2tensor.\n\n    Returns:\n        Energies (logits) of valid positions. Same shape as ``energies_4d``.\n\n    NOTE:\n        We do not use ``mask_value=-np.inf`` to avoid potential underflow.\n    """"""\n    mask_4d = tf.expand_dims(tf.expand_dims(mask, 1), 1)\n    energies_all = energies_4d * mask_4d\n\n    # Energies are log probabilities, so setting the invalid energies to\n    # negative infinity (aka -1e9 for compatibility with tensor2tensor) yields\n    # probability of zero to the padded positions.\n    return energies_all + (1.0 - mask_4d) * mask_value\n\n\ndef mask_future(energies: tf.Tensor, mask_value=-1e9) -> tf.Tensor:\n    """"""Mask energies of keys using lower triangular matrix.\n\n    Mask simulates autoregressive decoding, such that it prevents\n    the attention to look at what has not yet been decoded.\n    Mask is not necessary during training when true output values\n    are used instead of the decoded ones.\n\n    Arguments:\n        energies: A tensor to mask.\n        mask_value: Value used to mask energies.\n\n    Returns:\n        Masked energies tensor.\n    """"""\n    triangular_mask = tf.matrix_band_part(tf.ones_like(energies), -1, 0)\n    mask_area = tf.equal(triangular_mask, 1)\n\n    # Note that for compatibility with tensor2tensor, we use -1e9 for negative\n    # infinity.\n    masked_value = tf.fill(tf.shape(energies), mask_value)\n    return tf.where(mask_area, energies, masked_value)\n\n\n# pylint: disable=too-many-locals\n# TODO split this to more functions\ndef attention(\n        queries: tf.Tensor,\n        keys: tf.Tensor,\n        values: tf.Tensor,\n        keys_mask: tf.Tensor,\n        num_heads: int,\n        dropout_callback: Callable[[tf.Tensor], tf.Tensor],\n        masked: bool = False,\n        use_bias: bool = False) -> tf.Tensor:\n    """"""Run multi-head scaled dot-product attention.\n\n    See arxiv.org/abs/1706.03762\n\n    When performing multi-head attention, the queries, keys and values\n    vectors are first split to sets of smaller vectors, one for each attention\n    head. Next, they are transformed using a linear layer and a separate\n    attention (from a corresponding head) is applied on each set of\n    the transformed triple of query, key and value. The resulting contexts\n    from each head are then concatenated and a linear layer is applied\n    on this concatenated output. The following can be summed by following\n    equations::\n\n        MultiHead(Q, K, V) = Concat(head_1, ..., head_h) * W_o\n        head_i = Attention(Q * W_Q_i, K * W_K_i, V * W_V_i)\n\n    The scaled dot-product attention is a simple dot-product between\n    the query and a transposed key vector. The result is then scaled\n    using square root of the vector dimensions and a softmax layer is applied.\n    Finally, the output of the softmax layer is multiplied by the value vector.\n    See the following equation::\n\n        Attention(Q, K, V) = softmax(Q * K^T / \xe2\x88\x9a(d_k)) * V\n\n    Arguments:\n        queries: Input queries of shape ``(batch, time(q), k_channels)``.\n        keys: Input keys of shape ``(batch, time(k), k_channels)``.\n        values: Input values of shape ``(batch, time(k), v_channels)``.\n        keys_mask: A float Tensor for masking sequences in keys.\n        num_heads: Number of attention heads.\n        dropout_callback: Callable function implementing dropout.\n        masked: Boolean indicating whether we want to mask future energies.\n        use_bias: If True, enable bias in the attention head projections\n            (for all queries, keys and values).\n\n    Returns:\n        Contexts of shape ``(batch, time(q), v_channels)`` and\n        weights of shape ``(batch, time(q), time(k))``.\n    """"""\n    if num_heads <= 0:\n        raise ValueError(""Number of heads must be greater than zero."")\n\n    queries_dim = queries.shape.as_list()[-1]\n    keys_shape = keys.shape.as_list()\n    values_shape = values.shape.as_list()\n\n    # Query and keys should match in the last dimension\n    if queries_dim != keys_shape[-1]:\n        raise ValueError(\n            ""Queries and keys do not match in the last dimension.""\n            "" Queries: {}, Keys: {}"".format(queries_dim, keys_shape[-1]))\n\n    if keys_shape[1] != values_shape[1]:\n        raise ValueError(\n            ""Keys and values \'time\' dimension does not match. ""\n            ""Keys: {}, Values: {}"".format(keys_shape[1], values_shape[1]))\n\n    # Last dimension must be divisible by num_heads\n    if queries_dim % num_heads != 0:\n        raise ValueError(\n            ""Last dimension of the query ({}) should be divisible by the ""\n            ""number of heads ({})"".format(queries_dim, num_heads))\n\n    head_dim = int(queries_dim / num_heads)\n\n    # For multi-head attention, queries, keys and values are linearly projected\n    if num_heads > 1:\n        queries = tf.layers.dense(\n            queries, queries_dim, use_bias=use_bias, name=""query_proj"")\n        keys = tf.layers.dense(\n            keys, queries_dim, use_bias=use_bias, name=""keys_proj"")\n        values = tf.layers.dense(\n            values, queries_dim, use_bias=use_bias, name=""vals_proj"")\n\n    # Scale first:\n    queries_scaled = queries / math.sqrt(head_dim)\n\n    # Reshape the k_channels dimension to the number of heads\n    queries = split_for_heads(queries_scaled, num_heads, head_dim)\n    keys = split_for_heads(keys, num_heads, head_dim)\n    values = split_for_heads(values, num_heads, head_dim)\n\n    # For dot-product, we use matrix multiplication\n    # shape: batch, head, time(q), time(k) (k_channels is the matmul axis)\n    energies = tf.matmul(queries, keys, transpose_b=True)\n\n    # To protect the attention from looking ahead of time, we must replace the\n    # energies of future keys with negative infinity\n    if masked:\n        energies = mask_future(energies)\n\n    # To exclude the padded positions (those after the end of sentence),\n    # we mask the attention energies given this mask.\n    if keys_mask is not None:\n        energies = mask_energies(energies, keys_mask)\n    energies = tf.identity(energies, ""energies"")\n\n    # Softmax along the last axis\n    # shape: batch, head, time(q), time(k)\n    weights = tf.nn.softmax(energies)\n\n    # apply dropout to the weights (Attention Dropout)\n    weights = dropout_callback(weights)\n\n    context = tf.matmul(weights, values)\n\n    # transpose and reshape to shape [batch, time(q), v_channels]\n    context_shape = tf.shape(context)\n    context = tf.reshape(\n        tf.transpose(context, perm=[0, 2, 1, 3]),\n        [context_shape[0], context_shape[2], queries_dim])\n\n    if num_heads > 1:\n        # pylint: disable=redefined-variable-type\n        # This seems like a pylint bug\n        context = tf.layers.dense(\n            context, queries_dim, use_bias=use_bias, name=""output_proj"")\n        # pylint: enable=redefined-variable-type\n\n    return context, weights\n# pylint: enable=too-many-locals\n\n\ndef empty_multi_head_loop_state(\n        batch_size: Union[int, tf.Tensor],\n        num_heads: Union[int, tf.Tensor],\n        length: Union[int, tf.Tensor],\n        dimension: Union[int, tf.Tensor]) -> MultiHeadLoopState:\n\n    return MultiHeadLoopState(\n        contexts=tf.zeros(\n            shape=[0, batch_size, dimension],\n            dtype=tf.float32,\n            name=""contexts""),\n        head_weights=[tf.zeros(\n            shape=[0, batch_size, length],\n            dtype=tf.float32,\n            name=""distributions_head{}"".format(i)) for i in range(num_heads)])\n\n\nclass MultiHeadAttention(BaseAttention):\n\n    # pylint: disable=too-many-arguments\n    def __init__(self,\n                 name: str,\n                 n_heads: int,\n                 keys_encoder: Attendable,\n                 values_encoder: Attendable = None,\n                 dropout_keep_prob: float = 1.0,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        check_argument_types()\n        BaseAttention.__init__(self, name, reuse, save_checkpoint,\n                               load_checkpoint, initializers)\n\n        self.n_heads = n_heads\n        self.dropout_keep_prob = dropout_keep_prob\n\n        self.keys_encoder = keys_encoder\n\n        if values_encoder is not None:\n            self.values_encoder = values_encoder\n        else:\n            self.values_encoder = self.keys_encoder\n\n        if self.n_heads <= 0:\n            raise ValueError(""Number of heads must be greater than zero."")\n\n        if self.dropout_keep_prob <= 0.0 or self.dropout_keep_prob > 1.0:\n            raise ValueError(""Dropout keep prob must be inside (0,1]."")\n\n        self._variable_scope.set_initializer(tf.variance_scaling_initializer(\n            mode=""fan_avg"", distribution=""uniform""))\n    # pylint: enable=too-many-arguments\n\n    @tensor\n    def attention_keys(self) -> tf.Tensor:\n        return get_attention_states(self.keys_encoder)\n\n    @tensor\n    def attention_mask(self) -> tf.Tensor:\n        return get_attention_mask(self.keys_encoder)\n\n    @tensor\n    def attention_values(self) -> tf.Tensor:\n        return get_attention_states(self.values_encoder)\n\n    def attention(self,\n                  query: tf.Tensor,\n                  decoder_prev_state: tf.Tensor,\n                  decoder_input: tf.Tensor,\n                  loop_state: MultiHeadLoopState) -> Tuple[tf.Tensor,\n                                                           MultiHeadLoopState]:\n        """"""Run a multi-head attention getting context vector for a given query.\n\n        This method is an API-wrapper for the global function \'attention\'\n        defined in this module. Transforms a query of shape(batch, query_size)\n        to shape(batch, 1, query_size) and applies the attention function.\n        Output context has shape(batch, 1, value_size) and weights\n        have shape(batch, n_heads, 1, time(k)). The output is then processed\n        to produce output vector of contexts and the following attention\n        loop state.\n\n        Arguments:\n            query: Input query for the current decoding step\n                of shape(batch, query_size).\n            decoder_prev_state: Previous state of the decoder.\n            decoder_input: Input to the RNN cell of the decoder.\n            loop_state: Attention loop state.\n\n        Returns:\n            Vector of contexts and the following attention loop state.\n        """"""\n\n        context_3d, weights_4d = attention(\n            queries=tf.expand_dims(query, 1),\n            keys=self.attention_keys,\n            values=self.attention_values,\n            keys_mask=self.attention_mask,\n            num_heads=self.n_heads,\n            dropout_callback=lambda x: dropout(\n                x, self.dropout_keep_prob, self.train_mode))\n\n        # head_weights_3d is HEAD-wise list of (batch, 1, 1, time(keys))\n        head_weights_3d = tf.split(weights_4d, self.n_heads, axis=1)\n\n        context = tf.squeeze(context_3d, axis=1)\n        head_weights = [tf.squeeze(w, axis=[1, 2]) for w in head_weights_3d]\n\n        next_contexts = tf.concat(\n            [loop_state.contexts, tf.expand_dims(context, 0)], axis=0)\n        next_head_weights = [\n            tf.concat([loop_state.head_weights[i],\n                       tf.expand_dims(head_weights[i], 0)], axis=0)\n            for i in range(self.n_heads)]\n\n        next_loop_state = MultiHeadLoopState(\n            contexts=next_contexts,\n            head_weights=next_head_weights)\n\n        return context, next_loop_state\n\n    def initial_loop_state(self) -> MultiHeadLoopState:\n        return empty_multi_head_loop_state(\n            self.batch_size, self.n_heads, tf.shape(self.attention_keys)[1],\n            self.context_vector_size)\n\n    def finalize_loop(self, key: str,\n                      last_loop_state: MultiHeadLoopState) -> None:\n        for i in range(self.n_heads):\n            head_weights = last_loop_state.head_weights[i]\n            self.histories[""{}_head{}"".format(key, i)] = head_weights\n\n    # pylint: disable=no-member\n    @property\n    def context_vector_size(self) -> int:\n        return self.attention_values.get_shape()[-1].value\n    # pylint: enable=no-member\n\n    def visualize_attention(self, key: str, max_outputs: int = 16) -> None:\n        for i in range(self.n_heads):\n            head_key = ""{}_head{}"".format(key, i)\n            if head_key not in self.histories:\n                raise ValueError(\n                    ""Key {} not among attention histories"".format(head_key))\n\n            alignments = tf.expand_dims(\n                tf.transpose(self.histories[head_key], perm=[1, 2, 0]), -1)\n\n            tf.summary.image(""{}_head{}"".format(self.name, i), alignments,\n                             collections=[""summary_att_plots""],\n                             max_outputs=max_outputs)\n\n\nclass ScaledDotProdAttention(MultiHeadAttention):\n\n    # pylint: disable=too-many-arguments\n    def __init__(self,\n                 name: str,\n                 keys_encoder: Attendable,\n                 values_encoder: Attendable = None,\n                 dropout_keep_prob: float = 1.0,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        check_argument_types()\n        MultiHeadAttention.__init__(\n            self, name, 1, keys_encoder, values_encoder, dropout_keep_prob,\n            reuse, save_checkpoint, load_checkpoint, initializers)\n    # pylint: enable=too-many-arguments\n'"
neuralmonkey/attention/stateful_context.py,13,"b'from typing import Optional, Tuple\n\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.attention.base_attention import (\n    BaseAttention, AttentionLoopState, empty_attention_loop_state)\nfrom neuralmonkey.model.stateful import Stateful\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.model.parameterized import InitializerSpecs\n\n\nclass StatefulContext(BaseAttention):\n    """"""Provides a `Stateful` encoder\'s output as context to a decoder.\n\n    This is not really an attention mechanism, but rather a hack which\n    (mis)uses the attention interface to provide a ""static"" context vector to\n    the decoder cell. In other words, the context vector is the same for all\n    positions in the sequence and doesn\'t depend on the query vector.\n\n    To use this, simply pass an instance of this class to the decoder using\n    the `attentions` parameter.\n    """"""\n\n    def __init__(self,\n                 name: str,\n                 encoder: Stateful,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        check_argument_types()\n        BaseAttention.__init__(self, name, reuse, save_checkpoint,\n                               load_checkpoint, initializers)\n\n        self.encoder = encoder\n\n    @tensor\n    def attention_states(self) -> tf.Tensor:\n        return tf.expand_dims(self.encoder.output, 1)\n\n    # pylint: disable=no-self-use\n    @tensor\n    def attention_mask(self) -> Optional[tf.Tensor]:\n        pass\n    # pylint: enable=no-self-use\n\n    # pylint: disable=no-member\n    # Pylint fault from resolving tensor decoration\n    @property\n    def context_vector_size(self) -> int:\n        return self.attention_states.get_shape()[2].value\n    # pylint: enable=no-member\n\n    @property\n    def state_size(self) -> int:\n        return self.context_vector_size\n\n    def attention(self,\n                  query: tf.Tensor,\n                  decoder_prev_state: tf.Tensor,\n                  decoder_input: tf.Tensor,\n                  loop_state: AttentionLoopState) -> Tuple[tf.Tensor,\n                                                           AttentionLoopState]:\n        context = tf.reshape(self.attention_states,\n                             [-1, self.context_vector_size])\n        weights = tf.ones(shape=[self.batch_size, 1])\n\n        next_contexts = tf.concat(\n            [loop_state.contexts, tf.expand_dims(context, 0)], 0)\n        next_weights = tf.concat(\n            [loop_state.weights, tf.expand_dims(weights, 0)], 0)\n        next_loop_state = AttentionLoopState(\n            contexts=next_contexts,\n            weights=next_weights)\n\n        return context, next_loop_state\n\n    def initial_loop_state(self) -> AttentionLoopState:\n        return empty_attention_loop_state(\n            self.batch_size, 1,\n            self.context_vector_size)\n\n    def finalize_loop(self, key: str,\n                      last_loop_state: AttentionLoopState) -> None:\n        pass\n\n    def visualize_attention(self, key: str, max_outputs: int = 16) -> None:\n        pass\n'"
neuralmonkey/attention/transformer_cross_layer.py,39,"b'""""""Input combination strategies for multi-source Transformer decoder.""""""\n# TODO add citation when URL becomes available\n\nfrom typing import Callable, List\nimport tensorflow as tf\n\nfrom neuralmonkey.attention.scaled_dot_product import attention\nfrom neuralmonkey.tf_utils import layer_norm\n\n\n# pylint: disable=too-many-arguments\ndef single(\n        queries: tf.Tensor,\n        states: tf.Tensor,\n        mask: tf.Tensor,\n        n_heads: int,\n        attention_dropout_callback: Callable[[tf.Tensor], tf.Tensor],\n        dropout_callback: Callable[[tf.Tensor], tf.Tensor],\n        normalize: bool = True,\n        use_dropout: bool = True,\n        residual: bool = True,\n        use_att_transform_bias: bool = False):\n    """"""Run attention on a single encoder.\n\n    Arguments:\n        queries: The input for the attention.\n        states: The encoder states (keys & values).\n        mask: The temporal mask of the encoder.\n        n_heads: Number of attention heads to use.\n        attention_dropout_callback: Dropout function to apply in attention.\n        dropout_callback: Dropout function to apply on the attention output.\n        normalize: If True, run layer normalization on the queries.\n        use_dropout: If True, perform dropout on the attention output.\n        residual: If True, sum the context vector with the input queries.\n        use_att_transform_bias: If True, enable bias in the attention head\n            projections (for all queries, keys and values).\n\n    Returns:\n        A Tensor that contains the context vector.\n    """"""\n\n    # Layer normalization\n    normalized_queries = layer_norm(queries) if normalize else queries\n\n    # Attend to the encoder\n    # TODO handle attention histories\n    encoder_context, _ = attention(\n        queries=normalized_queries,\n        keys=states,\n        values=states,\n        keys_mask=mask,\n        num_heads=n_heads,\n        dropout_callback=attention_dropout_callback,\n        use_bias=use_att_transform_bias)\n\n    # Apply dropout\n    if use_dropout:\n        encoder_context = dropout_callback(encoder_context)\n\n    # Add residual connections\n    if residual:\n        encoder_context += queries\n\n    return encoder_context\n# pylint: enable=too-many-arguments\n\n\ndef serial(queries: tf.Tensor,\n           encoder_states: List[tf.Tensor],\n           encoder_masks: List[tf.Tensor],\n           heads: List[int],\n           attention_dropout_callbacks: List[Callable[[tf.Tensor], tf.Tensor]],\n           dropout_callback: Callable[[tf.Tensor], tf.Tensor]) -> tf.Tensor:\n    """"""Run attention with serial input combination.\n\n    The procedure is as follows:\n    1. repeat for every encoder:\n       - lnorm + attend + dropout + add residual\n    2. update queries between layers\n\n    Arguments:\n        queries: The input for the attention.\n        encoder_states: The states of each encoder.\n        encoder_masks: The temporal mask of each encoder.\n        heads: Number of attention heads to use for each encoder.\n        attention_dropout_callbacks: Dropout functions to apply in attention\n            over each encoder.\n        dropout_callback: The dropout function to apply on the outputs of each\n            sub-attention.\n\n    Returns:\n        A Tensor that contains the context vector.\n    """"""\n    context = queries\n    for i, (states, mask, n_heads, attn_drop_cb) in enumerate(zip(\n            encoder_states, encoder_masks, heads,\n            attention_dropout_callbacks)):\n\n        with tf.variable_scope(""enc_{}"".format(i)):\n            context = single(context, states, mask, n_heads,\n                             attention_dropout_callback=attn_drop_cb,\n                             dropout_callback=dropout_callback)\n    return context\n\n\ndef parallel(\n        queries: tf.Tensor,\n        encoder_states: List[tf.Tensor],\n        encoder_masks: List[tf.Tensor],\n        heads: List[int],\n        attention_dropout_callbacks: List[Callable[[tf.Tensor], tf.Tensor]],\n        dropout_callback: Callable[[tf.Tensor], tf.Tensor]) -> tf.Tensor:\n    """"""Run attention with parallel input combination.\n\n    The procedure is as follows:\n    1. normalize queries,\n    2. attend and dropout independently for every encoder,\n    3. sum up the results\n    4. add residual and return\n\n    Arguments:\n        queries: The input for the attention.\n        encoder_states: The states of each encoder.\n        encoder_masks: The temporal mask of each encoder.\n        heads: Number of attention heads to use for each encoder.\n        attention_dropout_callbacks: Dropout functions to apply in attention\n            over each encoder.\n        dropout_callback: The dropout function to apply on the outputs of each\n            sub-attention.\n\n    Returns:\n        A Tensor that contains the context vector.\n    """"""\n    normalized_queries = layer_norm(queries)\n    contexts = []\n\n    for i, (states, mask, n_heads, attn_drop_cb) in enumerate(zip(\n            encoder_states, encoder_masks, heads,\n            attention_dropout_callbacks)):\n\n        with tf.variable_scope(""enc_{}"".format(i)):\n            contexts.append(\n                single(normalized_queries, states, mask, n_heads,\n                       attention_dropout_callback=attn_drop_cb,\n                       dropout_callback=dropout_callback,\n                       normalize=False, residual=False))\n\n    return sum(contexts) + queries\n\n\n# pylint: disable=too-many-locals\ndef hierarchical(\n        queries: tf.Tensor,\n        encoder_states: List[tf.Tensor],\n        encoder_masks: List[tf.Tensor],\n        heads: List[int],\n        heads_hier: int,\n        attention_dropout_callbacks: List[Callable[[tf.Tensor], tf.Tensor]],\n        dropout_callback: Callable[[tf.Tensor], tf.Tensor]) -> tf.Tensor:\n    """"""Run attention with hierarchical input combination.\n\n    The procedure is as follows:\n    1. normalize queries\n    2. attend to every encoder\n    3. attend to the resulting context vectors (reuse normalized queries)\n    4. apply dropout, add residual connection and return\n\n    Arguments:\n        queries: The input for the attention.\n        encoder_states: The states of each encoder.\n        encoder_masks: The temporal mask of each encoder.\n        heads: Number of attention heads to use for each encoder.\n        heads_hier: Number of attention heads to use in the second attention.\n        attention_dropout_callbacks: Dropout functions to apply in attention\n            over each encoder.\n        dropout_callback: The dropout function to apply in the second attention\n            and over the outputs of each sub-attention.\n\n    Returns:\n        A Tensor that contains the context vector.\n    """"""\n    normalized_queries = layer_norm(queries)\n    contexts = []\n\n    batch = tf.shape(queries)[0]\n    time_q = tf.shape(queries)[1]\n    dimension = tf.shape(queries)[2]\n\n    for i, (states, mask, n_heads, attn_drop_cb) in enumerate(zip(\n            encoder_states, encoder_masks, heads,\n            attention_dropout_callbacks)):\n\n        with tf.variable_scope(""enc_{}"".format(i)):\n            contexts.append(\n                single(normalized_queries, states, mask, n_heads,\n                       attention_dropout_callback=attn_drop_cb,\n                       dropout_callback=dropout_callback,\n                       normalize=False, residual=False))\n\n    # context is of shape [batch, time(q), channels(v)],\n    # stack to [batch, time(q), n_encoders, channels(v)]\n    # reshape to [batch x time(q), n_encoders, channels(v)]\n    stacked_contexts = tf.reshape(\n        tf.stack(contexts, axis=2),\n        [batch * time_q, len(encoder_states), dimension])\n\n    # hierarchical mask: ones of shape [batch x time(q), n_encoders]\n    hier_mask = tf.ones([batch * time_q, len(encoder_states)])\n\n    # reshape queries to [batch x time(q), 1, channels(v)]\n    reshaped_queries = tf.reshape(\n        normalized_queries, [batch * time_q, 1, dimension])\n\n    # returned shape [batch x time(q), 1, channels(v)]\n    with tf.variable_scope(""enc_hier""):\n        # NOTE as attention dropout keep probability, we use the\n        # dropout_keep_prob value instead of attention_dropout_keep_prob.\n        encoder_context_stacked_batch = single(\n            reshaped_queries, stacked_contexts, hier_mask, heads_hier,\n            attention_dropout_callback=dropout_callback,\n            dropout_callback=lambda x: x, normalize=False, use_dropout=False,\n            residual=False)\n\n        # reshape back to [batch, time(q), channels(v)]\n        encoder_context = tf.reshape(\n            encoder_context_stacked_batch, [batch, time_q, dimension])\n\n        encoder_context = dropout_callback(encoder_context)\n\n    return encoder_context + queries\n# pylint: enable=too-many-locals\n\n\ndef flat(queries: tf.Tensor,\n         encoder_states: List[tf.Tensor],\n         encoder_masks: List[tf.Tensor],\n         heads: int,\n         attention_dropout_callback: Callable[[tf.Tensor], tf.Tensor],\n         dropout_callback: Callable[[tf.Tensor], tf.Tensor]) -> tf.Tensor:\n    """"""Run attention with flat input combination.\n\n    The procedure is as follows:\n    1. concatenate the states and mask along the time axis\n    2. run attention over the concatenation\n\n    Arguments:\n        queries: The input for the attention.\n        encoder_states: The states of each encoder.\n        encoder_masks: The temporal mask of each encoder.\n        heads: Number of attention heads to use for each encoder.\n        attention_dropout_callbacks: Dropout functions to apply in attention\n            over each encoder.\n        dropout_callback: The dropout function to apply on the output of the\n            attention.\n\n    Returns:\n        A Tensor that contains the context vector.\n    """"""\n    concat_states = tf.concat(encoder_states, 1)\n    concat_mask = tf.concat(encoder_masks, 1)\n\n    return single(queries, concat_states, concat_mask, heads,\n                  attention_dropout_callback, dropout_callback)\n'"
neuralmonkey/config/__init__.py,0,b''
neuralmonkey/config/builder.py,0,"b'""""""Configuration Object Builder.\n\nThis module is responsible for instantiating objects\nspecified by the experiment configuration.\n""""""\n\nimport collections\nimport importlib\nfrom argparse import Namespace\nfrom inspect import signature, isclass, isfunction, Parameter\nfrom typing import Any, Dict, Set, Tuple\n\nfrom neuralmonkey.logging import debug, warn\nfrom neuralmonkey.config.exceptions import (ConfigInvalidValueException,\n                                            ConfigBuildException)\n\n\n# pylint:disable=too-few-public-methods\nclass ClassSymbol:\n    """"""Represents a class (or other callable) in configuration.""""""\n\n    def __init__(self, string: str) -> None:\n        self.clazz = string\n\n    def create(self) -> Any:\n        class_parts = self.clazz.split(""."")\n\n        class_name = class_parts[-1]\n\n        simple_module_path = ""."".join(class_parts[:-1])\n        try:\n            module = importlib.import_module(simple_module_path)\n        except ImportError:\n            try:\n                if class_parts[0] == ""tf"":\n                    # Due to the architecture of TensorFlow, it must be\n                    # imported this way.\n                    tensorflow = importlib.import_module(""tensorflow"")\n                    module = tensorflow\n                    for i in range(1, len(class_parts) - 1):\n                        module = getattr(module, class_parts[i])\n                else:\n                    module_name = ""."".join([""neuralmonkey""] + class_parts[:-1])\n                    module = importlib.import_module(module_name)\n            except ImportError as exc:\n                # if the problem is really importing the module\n                if exc.name == module_name:  # type: ignore\n                    raise Exception(\n                        ""Cannot import module {}."".format(module_name))\n                raise\n\n        try:\n            clazz = getattr(module, class_name)\n        except AttributeError as exc:\n            raise Exception((""Interpretation \'{}\' as type name, class \'{}\' ""\n                             ""does not exist. Did you mean file \'./{}\'? \\n{}"")\n                            .format(self.clazz, class_name, self.clazz, exc))\n        return clazz\n\n\nclass ObjectRef:\n    """"""Represents a named object or its attribute in configuration.""""""\n\n    def __init__(self, expression: str) -> None:\n        self.expression = expression\n        self.name, *self.attr_chain = expression.split(""."")\n        self._obj = None\n\n    def bind(self, value: Any):\n        self._obj = value\n\n    @property\n    def target(self) -> Any:\n        value = self._obj\n        for attr in self.attr_chain:\n            value = getattr(value, attr)\n        return value\n\n\n# pylint: disable=too-many-return-statements\ndef build_object(value: str,\n                 all_dicts: Dict[str, Any],\n                 existing_objects: Dict[str, Any],\n                 depth: int) -> Any:\n    """"""Build an object from config dictionary of its arguments.\n\n    Works recursively.\n\n    Arguments:\n        value: Value that should be resolved (either a literal value or\n               a config section name)\n        all_dicts: Configuration dictionaries used to find configuration\n                   of unconstructed objects.\n        existing_objects: Dictionary of already constructed objects.\n        depth: The current depth of recursion. Used to prevent an infinite\n        recursion.\n    """"""\n    # TODO detect infinite recursion by other means than depth argument\n    # TODO as soon as config is run from an entrypoint, remove the\n    # ignore_names feature\n    if depth > 20:\n        raise AssertionError(""Config recursion should not be deeper that 20."")\n\n    debug(""Building value on depth {}: {}"".format(depth, value), ""configBuild"")\n\n    # if isinstance(value, str) and value in ignore_names:\n    # TODO zapisovani do argumentu\n    #   existing_objects[value] = None\n\n    if isinstance(value, tuple):\n        return tuple(build_object(val, all_dicts, existing_objects, depth + 1)\n                     for val in value)\n    if (isinstance(value, collections.Iterable)\n            and not isinstance(value, str)):\n        return [build_object(val, all_dicts, existing_objects, depth + 1)\n                for val in value]\n\n    if isinstance(value, ObjectRef):\n        if value.name in existing_objects:\n            debug(""Skipping already initialized object: {}"".format(value.name),\n                  ""configBuild"")\n        else:\n            existing_objects[value.name] = instantiate_class(\n                value.name, all_dicts, existing_objects, depth)\n        value.bind(existing_objects[value.name])\n        return value.target\n\n    if isinstance(value, ClassSymbol):\n        return value.create()\n\n    return value\n\n\ndef instantiate_class(name: str,\n                      all_dicts: Dict[str, Any],\n                      existing_objects: Dict[str, Any],\n                      depth: int) -> Any:\n    """"""Instantiate a class from the configuration.\n\n    Arguments: see help(build_object)\n    """"""\n    if name not in all_dicts:\n        debug(str(all_dicts), ""configBuild"")\n        raise ConfigInvalidValueException(name, ""Undefined object"")\n    this_dict = all_dicts[name]\n\n    if ""class"" not in this_dict:\n        raise ConfigInvalidValueException(name, ""Undefined object type"")\n    clazz = this_dict[""class""].create()\n\n    if not isclass(clazz) and not isfunction(clazz):\n        raise ConfigInvalidValueException(\n            name, ""Cannot instantiate object with \'{}\'"".format(clazz))\n\n    # prepare the arguments for the constructor\n    arguments = dict()\n\n    for key, value in this_dict.items():\n        if key == ""class"":\n            continue\n\n        arguments[key] = build_object(value, all_dicts, existing_objects,\n                                      depth + 1)\n\n    # get a signature of the constructing function\n    construct_sig = signature(clazz)\n\n    # if a signature contains a ""name"" attribute which is not in arguments,\n    # replace it with the name of the section\n    if ""name"" in construct_sig.parameters and ""name"" not in arguments:\n        annotation = construct_sig.parameters[""name""].annotation\n\n        if annotation == Parameter.empty:\n            debug(""No type annotation for the \'name\' parameter in ""\n                  ""class/function {}. Default value will not be used.""\n                  .format(this_dict[""class""].clazz), ""configBuild"")\n        elif annotation != str:\n            debug(""Type annotation for the \'name\' parameter in class/function ""\n                  ""{} is not \'str\'. Default value will not be used.""\n                  .format(this_dict[""class""].clazz), ""configBuild"")\n            debug(""Annotation is {}"".format(str(annotation)))\n        else:\n            debug(""Using default \'name\' for object {}""\n                  .format(this_dict[""class""].clazz), ""configBuild"")\n            arguments[""name""] = name\n\n    try:\n        # try to bound the arguments to the signature\n        bounded_params = construct_sig.bind(**arguments)\n    except TypeError as exc:\n        raise ConfigBuildException(clazz, exc)\n\n    debug(""Instantiating class {} with arguments {}"".format(clazz, arguments),\n          ""configBuild"")\n\n    # call the function with the arguments\n    # NOTE: any exception thrown from the body of the constructor is\n    # not worth catching here\n    obj = clazz(*bounded_params.args, **bounded_params.kwargs)\n\n    debug(""Class {} initialized into object {}"".format(clazz, obj),\n          ""configBuild"")\n\n    return obj\n\n\ndef build_config(config_dicts: Dict[str, Any],\n                 ignore_names: Set[str],\n                 warn_unused: bool = False) -> Tuple[Dict[str, Any],\n                                                     Dict[str, Any]]:\n    """"""Build the model from the configuration.\n\n    Arguments:\n        config_dicts: The parsed configuration file\n        ignore_names: A set of names that should be ignored during the loading.\n        warn_unused: Emit a warning if there are unused sections.\n\n    Returns:\n        A tuple containing a dictionary corresponding to the main section and\n        a dictionary mapping section names to objects.\n    """"""\n    if ""main"" not in config_dicts:\n        raise Exception(""Configuration does not contain the main block."")\n\n    existing_objects = collections.OrderedDict()  # type: Dict[str, Any]\n\n    main_config = config_dicts[""main""]\n    existing_objects[""main""] = Namespace(**main_config)\n\n    configuration = collections.OrderedDict()  # type: Dict[str, Any]\n    # TODO ensure tf_manager goes last in a better way\n    for key, value in sorted(main_config.items(),\n                             key=lambda t: t[0] if t[0] != ""tf_manager""\n                             else ""zzz""):\n        if key not in ignore_names:\n            try:\n                configuration[key] = build_object(\n                    value, config_dicts, existing_objects, 0)\n            except Exception as exc:\n                raise ConfigBuildException(key, exc) from None\n\n    if warn_unused:\n        existing_names = set(existing_objects.keys()) | {""main""}\n        unused = config_dicts.keys() - existing_names\n        if unused:\n            warn(""Configuration contains unused sections: ""\n                 + str(unused) + ""."")\n\n    return configuration, existing_objects\n'"
neuralmonkey/config/configuration.py,0,"b'import traceback\nfrom argparse import Namespace\nfrom collections import OrderedDict\nfrom typing import Any, Callable, List, Optional\n\nfrom neuralmonkey.logging import log\nfrom neuralmonkey.config.builder import build_config\nfrom neuralmonkey.config.parsing import parse_file, write_file\n\n\nclass Configuration:\n    """"""Configuration loader.\n\n    Loads the configuration file in an analogical way the python\'s\n    argparse.ArgumentParser works.\n    """"""\n\n    def __init__(self):\n        self.names = []\n        self.defaults = {}\n        self.conditions = {}\n        self.ignored = set()\n        self.raw_config = OrderedDict()\n        self.config_dict = OrderedDict()\n        self.objects = None\n        self.args = None\n        self.model = None\n\n    # pylint: disable=too-many-arguments\n    def add_argument(self,\n                     name: str,\n                     required: bool = False,\n                     default: Any = None,\n                     cond: Callable[[Any], bool] = None) -> None:\n\n        if name in self.names:\n            raise Exception(""Data filed defined multiple times."")\n        self.names.append(name)\n\n        if not required:\n            self.defaults[name] = default\n        if cond is not None:\n            self.conditions[name] = cond\n\n    def ignore_argument(self, name: str) -> None:\n        self.ignored.add(name)\n\n    def make_namespace(self, d_obj) -> Namespace:\n        n_space = Namespace()\n        for name, value in d_obj.items():\n            if name in self.conditions and not self.conditions[name](value):\n                cond_code = self.conditions[name].__code__\n                cond_filename = cond_code.co_filename\n                cond_line_number = cond_code.co_firstlineno\n                raise Exception(\n                    ""Value of field \'{}\' does not satisfy ""\n                    ""condition defined at {}:{}.""\n                    .format(name, cond_filename, cond_line_number))\n\n            setattr(n_space, name, value)\n\n        for name, value in self.defaults.items():\n            if name not in n_space.__dict__:\n                n_space.__dict__[name] = value\n        return n_space\n\n    def load_file(self, path: str,\n                  changes: Optional[List[str]] = None) -> None:\n        log(""Loading INI file: \'{}\'"".format(path), color=""blue"")\n\n        try:\n            with open(path, ""r"", encoding=""utf-8"") as file:\n                raw_config, config_dict = parse_file(file, changes)\n            log(""INI file is parsed."")\n\n            self.raw_config.update(raw_config)\n            self.config_dict.update(config_dict)\n        # pylint: disable=broad-except\n        except Exception as exc:\n            log(""Failed to load INI file: {}"".format(exc), color=""red"")\n            traceback.print_exc()\n            exit(1)\n\n        if ""main"" in self.config_dict:\n            self.args = self.make_namespace(self.config_dict[""main""])\n\n    def build_model(self, warn_unused=False) -> None:\n        log(""Building model based on the config."")\n        self._check_loaded_conf()\n        try:\n            model, self.objects = build_config(self.config_dict, self.ignored,\n                                               warn_unused)\n        # pylint: disable=broad-except\n        except Exception as exc:\n            log(""Failed to build model: {}"".format(exc), color=""red"")\n            traceback.print_exc()\n            exit(1)\n        log(""Model built."")\n        self.model = self.make_namespace(model)\n\n    def _check_loaded_conf(self) -> None:\n        """"""Check whether there are unexpected or missing fields.""""""\n        expected_missing = []\n        for name in self.names:\n            if name not in self.args.__dict__:\n                expected_missing.append(name)\n        if expected_missing:\n            raise Exception(""Missing mandatory fields: {}""\n                            .format("", "".join(expected_missing)))\n        unexpected = []\n        for name in self.config_dict[""main""]:\n            if name not in self.names and name not in self.ignored:\n                unexpected.append(name)\n        if unexpected:\n            raise Exception(""Unexpected fields: {}""\n                            .format("", "".join(unexpected)))\n\n    def save_file(self, path: str) -> None:\n        with open(path, ""w"", encoding=""utf-8"") as file:\n            write_file(self.raw_config, file)\n'"
neuralmonkey/config/exceptions.py,0,"b'""""""Module that contains exceptions handled in config parsing and loading.""""""\n\nimport traceback\nfrom typing import Any\n\n\nclass ParseError(Exception):\n    """"""Parsing exception caused by a syntax error in INI file.""""""\n\n    def __init__(self, message: str, line: int = None) -> None:\n        super().__init__()\n        self.message = message\n        self.line = line\n\n    def set_line(self, line: int) -> None:\n        self.line = line\n\n    def __str__(self) -> str:\n        """"""Convert this exception to string.""""""\n        if self.line is not None:\n            return ""INI error on line {}: {}"".format(self.line, self.message)\n\n        return ""INI parsing error: {}"".format(self.message)\n\n\nclass ConfigInvalidValueException(Exception):\n\n    def __init__(self, value: Any, message: str) -> None:\n        """"""Create an instance of the exception.\n\n        Arguments:\n            value: The invalid value\n            message: String that describes the nature of the error\n        """"""\n        super().__init__()\n        self.value = value\n        self.message = message\n\n    def __str__(self) -> str:\n        """"""Convert this exception to string.""""""\n        return ""Error in configuration of {}: {}"".format(\n            self.value, self.message)\n\n\nclass ConfigBuildException(Exception):\n    """"""Exception caused by error in loading the model.""""""\n\n    def __init__(self, object_name: str,\n                 original_exception: Exception) -> None:\n        """"""Create an instance of the exception.\n\n        Arguments:\n            object_name: The name of the object that has failed to build\n            original_exception: The exception that caused the failure\n        """"""\n        super().__init__()\n        self.object_name = object_name\n        self.original_exception = original_exception\n\n    def __str__(self) -> str:\n        """"""Convert this exception to string.""""""\n\n        trc = """".join(traceback.format_list(traceback.extract_tb(\n            self.original_exception.__traceback__)))\n        return ""Error while loading \'{}\': {}\\nTraceback: {}"".format(\n            self.object_name, self.original_exception, trc)\n'"
neuralmonkey/config/normalize.py,0,"b'""""""Module for configuration normalization.\n\nThe `[main]` configuration section contains arguments that can be filled with\ndifferent types of values, e.g. `trainer` can be either a single trainer\nobject or a list of them. This module provides functions for unifying the\nconfiguration interface.\n""""""\n\nfrom argparse import Namespace\nfrom datetime import timedelta\nimport re\nimport time\nfrom typing import List, Union, Callable\n\nimport numpy as np\n\nfrom neuralmonkey.logging import warn\nfrom neuralmonkey.tf_manager import get_default_tf_manager\nfrom neuralmonkey.trainers.delayed_update_trainer import DelayedUpdateTrainer\n\n\ndef normalize_configuration(cfg: Namespace, train_mode: bool) -> None:\n    """"""Given a configuration namespace, normalize the values it contains.\n\n    Arguments:\n        cfg: The namespace object returned by `Configuration.make_namespace`\n        train_mode: Boolean flag controlling normalization of parameters only\n            used during training.\n    """"""\n    if train_mode:\n        _normalize_train_cfg(cfg)\n\n    if cfg.tf_manager is None:\n        cfg.tf_manager = get_default_tf_manager()\n\n    cfg.evaluation = [(e[0], e[0], e[1]) if len(e) == 2 else e\n                      for e in cfg.evaluation]\n\n    if cfg.evaluation:\n        cfg.main_metric = ""{}/{}"".format(cfg.evaluation[-1][0],\n                                         cfg.evaluation[-1][-1].name)\n    else:\n        cfg.main_metric = ""{}/{}"".format(cfg.runners[-1].decoder_data_id,\n                                         cfg.runners[-1].loss_names[0])\n\n        if not cfg.tf_manager.minimize_metric:\n            raise ValueError(""minimize_metric must be set to True in ""\n                             ""TensorFlowManager when using loss as ""\n                             ""the main metric"")\n\n\ndef _normalize_train_cfg(cfg: Namespace) -> None:\n    """"""Given a configuration namespace, normalize the values it contains.\n\n    This function is only executed when training mode has been invoked.\n\n    Arguments:\n        cfg: The namespace object returned by `Configuration.make_namespace`\n    """"""\n    if not isinstance(cfg.val_dataset, List):\n        cfg.val_datasets = [cfg.val_dataset]\n    else:\n        cfg.val_datasets = cfg.val_dataset\n\n    if not isinstance(cfg.trainer, List):\n        cfg.trainers = [cfg.trainer]\n    else:\n        cfg.trainers = cfg.trainer\n\n    # deal with delayed trainer and logging periods\n    # the correct way if there are more trainers is perhaps to do a\n    # lowest common denominator of their batches_per_update.\n    # But we can also warn because it is a very weird setup.\n\n    delayed_trainers = [t for t in cfg.trainers\n                        if isinstance(t, DelayedUpdateTrainer)]\n\n    denominator = 1\n    if len(cfg.trainers) > 1 and delayed_trainers:\n        warn(""Weird setup: using more trainers and one of them is delayed ""\n             ""update trainer. No-one can vouch for your safety, user!"")\n        warn(""Using the lowest common denominator of all delayed trainers\'""\n             "" batches_per_update parameters for logging period"")\n        warn(""Note that if you are using a multi-task trainer, it is on ""\n             ""your own risk"")\n\n        denominator = np.lcm.reduce([t.batches_per_update\n                                     for t in delayed_trainers])\n    elif delayed_trainers:\n        assert len(cfg.trainers) == 1\n        denominator = cfg.trainers[0].batches_per_update\n\n    cfg.log_timer = _resolve_period(cfg.logging_period, denominator)\n    cfg.val_timer = _resolve_period(cfg.validation_period, denominator)\n\n\ndef _resolve_period(period: Union[str, int],\n                    denominator: int) -> Callable[[int, float], bool]:\n    """"""Convert logging period into a function for logging time checks.\n\n    Logging and validation periods can both be provided either as a number of\n    batches after which to log/validate, or as a time interval between the\n    logs/validation runs.\n\n    This function unifies both representations into a function that decides\n    whether to log/validate based on a given training step and time since the\n    last log/validation.\n\n    Arguments:\n        period: Either a string representing time, or a number representing\n            number of batches.\n        denominator: Only allow logging when the given step (number of batches\n            since the start of the training) is divisible by this value.\n            This is used e.g. when `DelayedUpdateTrainer` is used.\n\n    Returns:\n        A function of the current training step and time since the last logging\n        period that returns a boolean value.\n    """"""\n    def get_batch_logger(period: int) -> Callable[[int, float], bool]:\n        def is_time(step: int, _: float) -> bool:\n            return step != 0 and step % period == 0\n        return is_time\n\n    def get_time_logger(period: float) -> Callable[[int, float], bool]:\n        def is_time(step: int, last_time: float) -> bool:\n            if step % denominator != 0:\n                return False\n            return last_time + period < time.process_time()\n        return is_time\n\n    if isinstance(period, int):\n        if period % denominator != 0:\n            raise ValueError(\n                ""When using delayed update trainer, the logging/validation ""\n                ""periods must be divisible by batches_per_update."")\n\n        return get_batch_logger(period)\n\n    regex = re.compile(\n        r""((?P<days>\\d+?)d)?((?P<hours>\\d+?)h)?((?P<minutes>\\d+?)m)?""\n        r""((?P<seconds>\\d+?)s)?"")\n    parts = regex.match(period)\n\n    if not parts:\n        raise ValueError(\n            ""Validation or logging period have incorrect format. ""\n            ""It should be in format: 3h; 5m; 14s"")\n\n    time_params = {}\n    for (name, param) in parts.groupdict().items():\n        if param:\n            time_params[name] = int(param)\n\n    delta_seconds = timedelta(**time_params).total_seconds()\n    if delta_seconds <= 0:\n        raise ValueError(""Validation or logging period must be bigger than 0"")\n\n    return get_time_logger(delta_seconds)\n'"
neuralmonkey/config/parsing.py,0,"b'""""""Module responsible for INI parsing.""""""\n\nfrom collections import OrderedDict\nimport configparser\nimport os\nimport re\nimport time\nfrom typing import Any, Dict, Callable, Iterable, IO, List, Tuple, Pattern\n\nfrom neuralmonkey.config.builder import ClassSymbol, ObjectRef\nfrom neuralmonkey.config.exceptions import ParseError\nfrom neuralmonkey.logging import log\n\nLINE_NUM = re.compile(r""^(.*) ([0-9]+)$"")\n\nINTEGER = re.compile(r""^-?[0-9]+$"")\nFLOAT = re.compile(r""^-?[0-9]*\\.[0-9]*(e[+-]?[0-9]+)?$|^-?[0-9]+e[+-]?[0-9]+$"")\nLIST = re.compile(r""\\[([^]]*)\\]"")\nTUPLE = re.compile(r""\\(([^)]+)\\)"")\nSTRING = re.compile(r\'^""(.*)""$\')\nVAR_REF = re.compile(r""^\\$([a-zA-Z][a-zA-Z0-9_]*)$"")\nOBJECT_REF = re.compile(\n    r""^<([a-zA-Z][a-zA-Z0-9_]*(\\.[a-zA-Z][a-zA-Z0-9_]*)*)>$"")\nCLASS_NAME = re.compile(\n    r""^_*[a-zA-Z][a-zA-Z0-9_]*(\\._*[a-zA-Z][a-zA-Z0-9_]*)+$"")\n\n\nCONSTANTS = {\n    ""False"": False,\n    ""True"": True,\n    ""None"": None\n}\n\n\ndef get_first_match(pattern: Pattern, string: str) -> str:\n    """"""Return the first matching substring.\n\n    Args:\n        pattern: The pattern to find.\n        string: The string to search.\n\n    Returns:\n        The first occurence of the pattern in the string.\n\n    Raises:\n        ValueError if the string does not match the pattern.\n    """"""\n    match = pattern.match(string)\n    if match is None:\n        raise ValueError(""String \'{}\' does not match the pattern \'{}\'""\n                         .format(string, pattern.pattern))\n    return match.group(1)\n\n\n# this is a function because of the parse_*\n# functions which are not defined yet\ndef _keyval_parser_dict() -> Dict[Any, Callable]:\n    return {\n        INTEGER: lambda x, _: int(x),\n        FLOAT: lambda x, _: float(x),\n        STRING: _parse_string,\n        VAR_REF: lambda x, vars_dict: vars_dict[get_first_match(VAR_REF, x)],\n        CLASS_NAME: _parse_class_name,\n        OBJECT_REF: lambda x, _: ObjectRef(get_first_match(OBJECT_REF, x)),\n        LIST: _parse_list,\n        TUPLE: _parse_tuple\n    }\n\n\nclass VarsDict(OrderedDict, Dict[str, Any]):\n\n    def __missing__(self, key):\n        """"""Try to fetch and parse the variable value from `os.environ`.""""""\n        if key in os.environ:\n            try:\n                value = _parse_value(os.environ[key], self)\n            except ParseError:\n                # If we cannot parse it, use it as a string.\n                value = os.environ[key]\n            log(""Variable {}={!r} taken from the environment.""\n                .format(key, value))\n            return value\n\n        raise ParseError(""Undefined variable: {}"".format(key))\n\n\ndef _split_on_commas(string: str) -> List[str]:\n    """"""Split a bracketed string on commas.\n\n    The commas inside brackets are preserved.\n    """"""\n\n    items = []\n    char_buffer = []  # type: List[str]\n    openings = []  # type: List[str]\n\n    for i, char in enumerate(string):\n        if char == "","" and not openings:\n            if char_buffer:\n                items.append("""".join(char_buffer))\n            char_buffer = []\n            continue\n        elif char == "" "" and not char_buffer:\n            continue\n        elif char in (""("", ""[""):\n            openings.append(char)\n        elif char == "")"":\n            if openings.pop() != ""("":\n                raise ParseError(""Invalid bracket end \')\', col {}."".format(i))\n        elif char == ""]"":\n            if openings.pop() != ""["":\n                raise ParseError(""Invalid bracket end \']\', col {}."".format(i))\n        char_buffer.append(char)\n\n    if char_buffer:\n        items.append("""".join(char_buffer))\n    return items\n\n\ndef _parse_string(string: str, vars_dict: VarsDict) -> str:\n    return get_first_match(STRING, string).format_map(vars_dict)\n\n\ndef _parse_list(string: str, vars_dict: VarsDict) -> List[Any]:\n    """"""Parse the string recursively as a list.""""""\n\n    matched_content = get_first_match(LIST, string)\n    if not matched_content:\n        return []\n\n    items = _split_on_commas(matched_content)\n    values = [_parse_value(val, vars_dict) for val in items]\n\n    return values\n\n\ndef _parse_tuple(string: str, vars_dict: VarsDict) -> Tuple[Any, ...]:\n    """"""Parse the string recursively as a tuple.""""""\n\n    items = _split_on_commas(get_first_match(TUPLE, string))\n    values = [_parse_value(val, vars_dict) for val in items]\n\n    return tuple(values)\n\n\ndef _parse_class_name(string: str, vars_dict: VarsDict) -> ClassSymbol:\n    """"""Parse the string as a module or class name.""""""\n    del vars_dict\n    return ClassSymbol(string)\n\n\ndef _parse_value(string: str, vars_dict: VarsDict) -> Any:\n    """"""Parse the value recursively according to the Nerual Monkey grammar.\n\n    Arguments:\n        string: the string to be parsed\n        vars_dict: a dictionary of variables for substitution\n    """"""\n    string = string.strip()\n\n    if string in CONSTANTS:\n        return CONSTANTS[string]\n\n    for matcher, parser in _keyval_parser_dict().items():\n        if matcher.match(string) is not None:\n            return parser(string, vars_dict)\n\n    raise ParseError(""Cannot parse value: \'{}\'."".format(string))\n\n\ndef _parse_ini(config_file: Iterable[str],\n               filename: str = """") -> Dict[str, Any]:\n    """"""Parse an INI file into a dictionary.""""""\n\n    line_numbers = (line.strip() + "" "" + str(i + 1)\n                    if line.strip() else """"\n                    for i, line in\n                    enumerate(config_file))\n    config = configparser.ConfigParser()\n    config.read_file(line_numbers, source=filename)\n\n    new_config = OrderedDict()  # type: Dict[str, Any]\n    for section in config.sections():\n        new_config[section] = OrderedDict()\n\n        for key in config[section]:\n            match = LINE_NUM.match(config[section][key])\n            assert match is not None\n\n            new_config[section][key] = match.group(2), match.group(1)\n\n    return new_config\n\n\ndef _apply_change(config_dict: Dict[str, Any], setting: str) -> None:\n    if ""="" not in setting:\n        raise ParseError(""Invalid setting \'{}\'"".format(setting))\n    key, value = (s.strip() for s in setting.split(""="", maxsplit=1))\n\n    if ""."" in key:\n        section, option = key.split(""."", maxsplit=1)\n    else:\n        section = ""main""\n        option = key\n\n    if section not in config_dict:\n        log(""Creating new section \'{}\'"".format(section))\n        config_dict[section] = OrderedDict()\n\n    config_dict[section][option] = -1, value  # no line number\n\n\ndef parse_file(config_file: Iterable[str],\n               changes: Iterable[str] = None\n              ) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n    """"""Parse an INI file and creates all values.""""""\n\n    parsed_dicts = OrderedDict()  # type: Dict[str, Any]\n\n    config = _parse_ini(config_file)\n\n    if changes is not None:\n        for change in changes:\n            _apply_change(config, change)\n\n    vars_dict = VarsDict()\n    vars_dict[""TIME""] = time.strftime(""%Y-%m-%d-%H-%M-%S"")\n\n    def parse_section(section: str, output_dict: Dict[str, Any]):\n        for key, (lineno, value_string) in config[section].items():\n            try:\n                value = _parse_value(value_string, vars_dict)\n            except ParseError as exc:\n                exc.set_line(lineno)\n                raise\n\n            output_dict[key] = value\n\n    if ""vars"" in config:\n        parse_section(""vars"", vars_dict)\n\n    for section in config:\n        if section != ""vars"":\n            parsed_dicts[section] = OrderedDict()\n            parse_section(section, parsed_dicts[section])\n\n    # also return the unparsed config dict; need to remove line numbers\n    raw_config = OrderedDict([\n        (name, OrderedDict([(key, val) for key, (_, val) in section.items()]))\n        for name, section in config.items()])\n\n    return raw_config, parsed_dicts\n\n\ndef write_file(config_dict: Dict[str, Any], config_file: IO[str]) -> None:\n    config = configparser.ConfigParser()\n    config.read_dict(config_dict)\n    config.write(config_file, space_around_delimiters=False)\n'"
neuralmonkey/decoders/__init__.py,0,b'from .sequence_regressor import SequenceRegressor\nfrom .beam_search_decoder import BeamSearchDecoder\nfrom .classifier import Classifier\nfrom .ctc_decoder import CTCDecoder\nfrom .decoder import Decoder\nfrom .sequence_labeler import SequenceLabeler\nfrom .word_alignment_decoder import WordAlignmentDecoder\n'
neuralmonkey/decoders/autoregressive.py,99,"b'# pylint: disable=too-many-lines\n""""""Abstract class for autoregressive decoding.\n\nEither for the recurrent decoder, or for the transformer decoder.\n\nThe autoregressive decoder uses the while loop to get the outputs.\nDescendants should only specify the initial state and the while loop body.\n""""""\nfrom typing import NamedTuple, Callable, Optional, Any, List, Dict, Tuple\n\nimport tensorflow as tf\n\nfrom neuralmonkey.dataset import Dataset\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.model.feedable import FeedDict\nfrom neuralmonkey.model.parameterized import InitializerSpecs\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.logging import warn\nfrom neuralmonkey.model.sequence import EmbeddedSequence\nfrom neuralmonkey.nn.utils import dropout\nfrom neuralmonkey.tf_utils import (\n    append_tensor, get_variable, get_state_shape_invariants)\nfrom neuralmonkey.vocabulary import (\n    Vocabulary, pad_batch, sentence_mask, UNK_TOKEN_INDEX, START_TOKEN_INDEX,\n    END_TOKEN_INDEX, PAD_TOKEN_INDEX)\n\n\nclass LoopState(NamedTuple(\n        ""LoopState"",\n        [(""histories"", Any),\n         (""constants"", Any),\n         (""feedables"", Any)])):\n    """"""The loop state object.\n\n    The LoopState is a structure that works with the tf.while_loop function the\n    decoder loop state stores all the information that is not invariant for the\n    decoder run.\n\n    Attributes:\n        histories: A set of tensors that grow in time as the decoder proceeds.\n        constants: A set of independent tensors that do not change during the\n            entire decoder run.\n        feedables: A set of tensors used as the input of a single decoder step.\n    """"""\n\n\nclass DecoderHistories(NamedTuple(\n        ""DecoderHistories"",\n        [(""logits"", tf.Tensor),\n         (""output_states"", tf.Tensor),\n         (""output_symbols"", tf.Tensor),\n         (""output_mask"", tf.Tensor),\n         (""other"", Any)])):\n    """"""The values collected during the run of an autoregressive decoder.\n\n    This should only record decoding history and the decoding should not be\n    dependent on these values.\n\n    Attributes defined here (and in the `other`) substructure should always\n    be time-major (e.g., shape(time, batch, ...)).\n\n    Attributes:\n        logits: A tensor of shape ``(time, batch, vocabulary)`` which contains\n            the unnormalized output scores of words in a vocabulary.\n        output_states: A tensor of shape ``(time, batch, state_size)``. The\n            states of the decoder before the final output (logit) projection.\n        output_symbols: An int tensor of shape ``(time, batch)``. Stores the\n            generated symbols. (Either an argmax-ed value from the logits, or\n            a target token, during training.)\n        output_mask: A float tensor of zeros and ones of shape\n            ``(time, batch)``. Keeps track of valid positions in the decoded\n            data.\n        other: A structure related to a specific AutoregressiveDecoder\n            implementation.\n    """"""\n\n\nclass DecoderConstants(NamedTuple(\n        ""DecoderConstants"",\n        [(""train_inputs"", Optional[tf.Tensor])])):\n    """"""The constants used by an autoregressive decoder.\n\n    Attributes:\n        train_inputs: During training, this is populated by the target token\n            ids.\n    """"""\n\n\nclass DecoderFeedables(NamedTuple(\n        ""DecoderFeedables"",\n        [(""step"", tf.Tensor),\n         (""finished"", tf.Tensor),\n         (""embedded_input"", tf.Tensor),\n         (""other"", Any)])):\n    """"""The input of a single step of an autoregressive decoder.\n\n    The decoder should be able to generate an output symbol only using the\n    information contained in this structure.\n\n    Attributes defined here (and in the `other`) substructure should always\n    be batch-major (e.g., shape(batch, ...)).\n\n    Attributes:\n        step: A scalar int tensor, stores the number of the current time step.\n        finished: A boolean tensor of shape ``(batch)``,  which says whether\n            the decoding of a sentence in the batch is finished or not. (E.g.\n            whether the end token has already been generated.)\n        embedded_input: A ``batch``-sized tensor with embedded inputs to the\n            decoder. During inference, this contains the previously generated\n            tokens. During training, this contains the reference tokens.\n        other: A structure related to a specific AutoregressiveDecoder\n            implementation.\n    """"""\n\n\n# pylint: disable=too-many-public-methods,too-many-instance-attributes\nclass AutoregressiveDecoder(ModelPart):\n\n    # pylint: disable=too-many-arguments,too-many-locals\n    def __init__(self,\n                 name: str,\n                 vocabulary: Vocabulary,\n                 data_id: str,\n                 max_output_len: int,\n                 dropout_keep_prob: float = 1.0,\n                 embedding_size: int = None,\n                 embeddings_source: EmbeddedSequence = None,\n                 tie_embeddings: bool = False,\n                 label_smoothing: float = None,\n                 supress_unk: bool = False,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        """"""Initialize parameters common for all autoregressive decoders.\n\n        Arguments:\n            name: Name of the decoder. Should be unique accross all Neural\n                Monkey objects.\n            vocabulary: Target vocabulary.\n            data_id: Target data series.\n            max_output_len: Maximum length of an output sequence.\n            reuse: Reuse the variables from the model part.\n            dropout_keep_prob: Probability of keeping a value during dropout.\n            embedding_size: Size of embedding vectors for target words.\n            embeddings_source: Embedded sequence to take embeddings from.\n            tie_embeddings: Use decoder.embedding_matrix also in place\n                of the output decoding matrix.\n            label_smoothing: Label smoothing parameter.\n            supress_unk: If true, decoder will not produce symbols for unknown\n                tokens.\n        """"""\n        ModelPart.__init__(self, name, reuse, save_checkpoint, load_checkpoint,\n                           initializers)\n\n        self.vocabulary = vocabulary\n        self.data_id = data_id\n        self.max_output_len = max_output_len\n        self.dropout_keep_prob = dropout_keep_prob\n        self._embedding_size = embedding_size\n        self.embeddings_source = embeddings_source\n        self.label_smoothing = label_smoothing\n        self.tie_embeddings = tie_embeddings\n        self.supress_unk = supress_unk\n\n        self.encoder_states = lambda: []  # type: Callable[[], List[tf.Tensor]]\n        self.encoder_masks = lambda: []  # type: Callable[[], List[tf.Tensor]]\n\n        # Check the values of the parameters (max_output_len, ...)\n        if self.max_output_len <= 0:\n            raise ValueError(\n                ""Maximum sequence length must be a positive integer."")\n\n        if self._embedding_size is not None and self._embedding_size <= 0:\n            raise ValueError(""Embedding size must be a positive integer."")\n\n        if self.dropout_keep_prob < 0.0 or self.dropout_keep_prob > 1.0:\n            raise ValueError(""Dropout keep probability must be a real number ""\n                             ""in the interval [0,1]."")\n    # pylint: enable=too-many-arguments,too-many-locals\n\n    @property\n    def embedding_size(self) -> int:\n        if self.embeddings_source is None:\n            if self._embedding_size is None:\n                raise ValueError(\n                    ""You must specify either embedding size or the embedded ""\n                    ""sequence from which to reuse the embeddings (e.g. set ""\n                    ""\'embedding_size\' or \'embeddings_source\' parameter)"")\n            return self._embedding_size\n\n        if self.embeddings_source is not None:\n            if self._embedding_size is not None:\n                warn(""Overriding the embedding_size parameter with the ""\n                     ""size of the reused embeddings from the encoder."")\n\n        return self.embeddings_source.embedding_matrix.get_shape()[1].value\n\n    @tensor\n    def go_symbols(self) -> tf.Tensor:\n        return tf.fill([self.batch_size],\n                       tf.constant(START_TOKEN_INDEX, dtype=tf.int64))\n\n    @property\n    def input_types(self) -> Dict[str, tf.DType]:\n        return {self.data_id: tf.string}\n\n    @property\n    def input_shapes(self) -> Dict[str, tf.TensorShape]:\n        return {self.data_id: tf.TensorShape([None, None])}\n\n    @tensor\n    def train_tokens(self) -> tf.Tensor:\n        return self.dataset[self.data_id]\n\n    @tensor\n    def train_inputs(self) -> tf.Tensor:\n        return tf.transpose(\n            self.vocabulary.strings_to_indices(self.train_tokens))\n\n    @tensor\n    def train_mask(self) -> tf.Tensor:\n        return sentence_mask(self.train_inputs)\n\n    @tensor\n    def decoding_w(self) -> tf.Variable:\n        if (self.tie_embeddings\n                and self.embedding_size != self.output_dimension):\n            raise ValueError(\n                ""`embedding_size must be equal to the output_projection ""\n                ""size when using the `tie_embeddings` option"")\n\n        with tf.name_scope(""output_projection""):\n            if self.tie_embeddings:\n                return tf.transpose(self.embedding_matrix)\n\n            return get_variable(\n                ""state_to_word_W"",\n                [self.output_dimension, len(self.vocabulary)],\n                initializer=tf.random_uniform_initializer(-0.5, 0.5))\n\n    @tensor\n    def decoding_b(self) -> Optional[tf.Variable]:\n        if self.tie_embeddings:\n            return tf.zeros(len(self.vocabulary))\n\n        with tf.name_scope(""output_projection""):\n            return get_variable(\n                ""state_to_word_b"",\n                [len(self.vocabulary)],\n                initializer=tf.zeros_initializer())\n\n    @tensor\n    def embedding_matrix(self) -> tf.Variable:\n        """"""Variables and operations for embedding of input words.\n\n        If we are reusing word embeddings, this function takes the embedding\n        matrix from the first encoder\n        """"""\n        if self.embeddings_source is not None:\n            return self.embeddings_source.embedding_matrix\n\n        assert self.embedding_size is not None\n\n        return get_variable(\n            name=""word_embeddings"",\n            shape=[len(self.vocabulary), self.embedding_size])\n\n    def embed_input_symbols(self, input_symbols: tf.Tensor) -> tf.Tensor:\n        embedded_input = tf.nn.embedding_lookup(\n            self.embedding_matrix, input_symbols)\n        return dropout(embedded_input, self.dropout_keep_prob, self.train_mode)\n\n    @tensor\n    def train_loop_result(self) -> LoopState:\n        return self.decoding_loop(train_mode=True)\n\n    @tensor\n    def train_logits(self) -> tf.Tensor:\n        train_result = LoopState(*self.train_loop_result)\n        return train_result.histories.logits\n\n    @tensor\n    def train_output_states(self) -> tf.Tensor:\n        train_result = LoopState(*self.train_loop_result)\n        return train_result.histories.output_states\n\n    @tensor\n    def train_logprobs(self) -> tf.Tensor:\n        return tf.nn.log_softmax(self.train_logits)\n\n    @tensor\n    def train_xents(self) -> tf.Tensor:\n        train_targets = tf.transpose(self.train_inputs)\n        softmax_function = None\n        if self.label_smoothing:\n            softmax_function = (\n                lambda labels, logits: tf.losses.softmax_cross_entropy(\n                    tf.one_hot(labels, len(self.vocabulary)),\n                    logits, label_smoothing=self.label_smoothing))\n\n        # Return losses of shape (batch, time). Losses on invalid positions\n        # are zero.\n        return tf.contrib.seq2seq.sequence_loss(\n            tf.transpose(self.train_logits, perm=[1, 0, 2]),\n            train_targets,\n            tf.transpose(self.train_mask),\n            average_across_batch=False,\n            average_across_timesteps=False,\n            softmax_loss_function=softmax_function)\n\n    @tensor\n    def train_loss(self) -> tf.Tensor:\n        # Cross entropy mean over all words in the batch\n        # (could also be done as a mean over sentences)\n        return tf.reduce_sum(self.train_xents) / tf.reduce_sum(self.train_mask)\n\n    @property\n    def cost(self) -> tf.Tensor:\n        return self.train_loss\n\n    @tensor\n    def runtime_loop_result(self) -> LoopState:\n        return self.decoding_loop(train_mode=False)\n\n    @tensor\n    def runtime_logits(self) -> tf.Tensor:\n        runtime_result = LoopState(*self.runtime_loop_result)\n        return runtime_result.histories.logits\n\n    @tensor\n    def runtime_output_states(self) -> tf.Tensor:\n        runtime_result = LoopState(*self.runtime_loop_result)\n        return runtime_result.histories.output_states\n\n    @tensor\n    def runtime_mask(self) -> tf.Tensor:\n        runtime_result = LoopState(*self.runtime_loop_result)\n        return runtime_result.histories.output_mask\n\n    @tensor\n    def decoded(self) -> tf.Tensor:\n        # We disable generating of <pad> tokens at index 0\n        # (self.runtime_logits[:, :, 1:]). This shifts the indices\n        # of the decoded tokens (therefore, we add +1 to the decoded\n        # output indices).\n\n        # self.runtime_logits is of size [batch, sentence_len, vocabulary_size]\n        return tf.argmax(self.runtime_logits[:, :, 1:], -1) + 1\n\n    @tensor\n    def runtime_xents(self) -> tf.Tensor:\n        train_targets = tf.transpose(self.train_inputs)\n        batch_major_logits = tf.transpose(self.runtime_logits, [1, 0, 2])\n        min_time = tf.minimum(tf.shape(train_targets)[1],\n                              tf.shape(batch_major_logits)[1])\n\n        # NOTE if done properly, there should be padding of the shorter\n        # sequence instead of cropping to the length of the shorter one\n\n        return tf.contrib.seq2seq.sequence_loss(\n            logits=batch_major_logits[:, :min_time],\n            targets=train_targets[:, :min_time],\n            weights=tf.transpose(self.train_mask)[:, :min_time],\n            average_across_batch=False,\n            average_across_timesteps=False)\n\n    @tensor\n    def runtime_loss(self) -> tf.Tensor:\n        return (tf.reduce_sum(self.runtime_xents)\n                / tf.reduce_sum(tf.to_float(self.runtime_mask)))\n\n    @tensor\n    def runtime_logprobs(self) -> tf.Tensor:\n        return tf.nn.log_softmax(self.runtime_logits)\n\n    @property\n    def output_dimension(self) -> int:\n        raise NotImplementedError(""Abstract property"")\n\n    def get_initial_feedables(self) -> DecoderFeedables:\n        return DecoderFeedables(\n            step=tf.constant(0, tf.int32),\n            finished=tf.zeros([self.batch_size], dtype=tf.bool),\n            embedded_input=self.embed_input_symbols(self.go_symbols),\n            other=None)\n\n    def get_initial_histories(self) -> DecoderHistories:\n        output_states = tf.zeros(\n            shape=[0, self.batch_size, self.embedding_size],\n            dtype=tf.float32,\n            name=""hist_output_states"")\n\n        output_mask = tf.zeros(\n            shape=[0, self.batch_size],\n            dtype=tf.bool,\n            name=""hist_output_mask"")\n\n        output_symbols = tf.zeros(\n            shape=[0, self.batch_size],\n            dtype=tf.int64,\n            name=""hist_output_symbols"")\n\n        logits = tf.zeros(\n            shape=[0, self.batch_size, len(self.vocabulary)],\n            dtype=tf.float32,\n            name=""hist_logits"")\n\n        return DecoderHistories(\n            logits=logits,\n            output_states=output_states,\n            output_mask=output_mask,\n            output_symbols=output_symbols,\n            other=None)\n\n    def get_initial_constants(self) -> DecoderConstants:\n        return DecoderConstants(train_inputs=self.train_inputs)\n\n    def get_initial_loop_state(self) -> LoopState:\n        return LoopState(\n            feedables=self.get_initial_feedables(),\n            histories=self.get_initial_histories(),\n            constants=self.get_initial_constants())\n\n    def loop_continue_criterion(self, *args) -> tf.Tensor:\n        """"""Decide whether to break out of the while loop.\n\n        Arguments:\n            loop_state: ``LoopState`` instance (see the docs for this module).\n                Represents current decoder loop state.\n        """"""\n        loop_state = LoopState(*args)\n        finished = loop_state.feedables.finished\n        not_all_done = tf.logical_not(tf.reduce_all(finished))\n        before_max_len = tf.less(loop_state.feedables.step,\n                                 self.max_output_len)\n        return tf.logical_and(not_all_done, before_max_len)\n\n    def next_state(self, loop_state: LoopState) -> Tuple[tf.Tensor, Any, Any]:\n        raise NotImplementedError(""Abstract method."")\n\n    def get_body(self, train_mode: bool, sample: bool = False,\n                 temperature: float = 1.) -> Callable:\n        """"""Return the while loop body function.""""""\n\n        def is_finished(finished: tf.Tensor, symbols: tf.Tensor) -> tf.Tensor:\n            has_just_finished = tf.equal(symbols, END_TOKEN_INDEX)\n            return tf.logical_or(finished, has_just_finished)\n\n        def state_to_logits(state: tf.Tensor) -> tf.Tensor:\n            logits = tf.matmul(state, self.decoding_w)\n            logits += self.decoding_b\n\n            if self.supress_unk:\n                unk_mask = tf.one_hot(\n                    UNK_TOKEN_INDEX, depth=len(self.vocabulary), on_value=-1e9)\n                logits += unk_mask\n\n            return logits\n\n        def logits_to_symbols(logits: tf.Tensor,\n                              loop_state: LoopState) -> tf.Tensor:\n            step = loop_state.feedables.step\n            if sample:\n                next_symbols = tf.squeeze(\n                    tf.multinomial(logits, num_samples=1), axis=1)\n            elif train_mode:\n                next_symbols = loop_state.constants.train_inputs[step]\n            else:\n                next_symbols = tf.argmax(logits, axis=1)\n\n            int_unfinished_mask = tf.to_int64(\n                tf.logical_not(loop_state.feedables.finished))\n\n            # Note this works only when PAD_TOKEN_INDEX is 0. Otherwise\n            # this have to be rewritten\n            assert PAD_TOKEN_INDEX == 0\n            next_symbols = next_symbols * int_unfinished_mask\n\n            return next_symbols\n\n        def body(*args) -> LoopState:\n\n            loop_state = LoopState(*args)\n            feedables = loop_state.feedables\n            histories = loop_state.histories\n\n            with tf.variable_scope(self._variable_scope, reuse=tf.AUTO_REUSE):\n                output_state, dec_other, hist_other = self.next_state(\n                    loop_state)\n\n                logits = state_to_logits(output_state)\n                logits /= temperature\n\n                next_symbols = logits_to_symbols(logits, loop_state)\n                finished = is_finished(feedables.finished, next_symbols)\n\n            next_feedables = DecoderFeedables(\n                step=feedables.step + 1,\n                finished=finished,\n                embedded_input=self.embed_input_symbols(next_symbols),\n                other=dec_other)\n\n            next_histories = DecoderHistories(\n                logits=append_tensor(histories.logits, logits),\n                output_states=append_tensor(\n                    histories.output_states, output_state),\n                output_symbols=append_tensor(\n                    histories.output_symbols, next_symbols),\n                output_mask=append_tensor(\n                    histories.output_mask, tf.logical_not(finished)),\n                other=hist_other)\n\n            return LoopState(\n                feedables=next_feedables,\n                histories=next_histories,\n                constants=loop_state.constants)\n\n        return body\n\n    def finalize_loop(self, final_loop_state: LoopState,\n                      train_mode: bool) -> None:\n        """"""Execute post-while loop operations.\n\n        Arguments:\n            final_loop_state: Decoder loop state at the end\n                of the decoding loop.\n            train_mode: Boolean flag, telling whether this is\n                a training run.\n        """"""\n\n    def decoding_loop(self, train_mode: bool, sample: bool = False,\n                      temperature: float = 1) -> LoopState:\n        """"""Run the decoding while loop.\n\n        Calls get_initial_loop_state and constructs tf.while_loop\n        with the continuation criterion returned from loop_continue_criterion,\n        and body function returned from get_body.\n\n        After finishing the tf.while_loop, it calls finalize_loop\n        to further postprocess the final decoder loop state (usually\n        by stacking Tensors containing decoding histories).\n\n        Arguments:\n            train_mode: Boolean flag, telling whether this is\n                a training run.\n            sample: Boolean flag, telling whether we should sample\n                the output symbols from the output distribution instead\n                of using argmax or gold data.\n            temperature: float value specifying the softmax temperature\n        """"""\n        initial_loop_state = self.get_initial_loop_state()\n        with tf.control_dependencies([self.decoding_w, self.decoding_b]):\n            final_loop_state = tf.while_loop(\n                self.loop_continue_criterion,\n                self.get_body(train_mode, sample, temperature),\n                initial_loop_state,\n                shape_invariants=tf.contrib.framework.nest.map_structure(\n                    get_state_shape_invariants, initial_loop_state))\n        self.finalize_loop(final_loop_state, train_mode)\n\n        return final_loop_state\n\n    def feed_dict(self, dataset: Dataset, train: bool = False) -> FeedDict:\n        """"""Populate the feed dictionary for the decoder object.\n\n        Arguments:\n            dataset: The dataset to use for the decoder.\n            train: Boolean flag, telling whether this is a training run.\n        """"""\n        fd = ModelPart.feed_dict(self, dataset, train)\n\n        sentences = dataset.maybe_get_series(self.data_id)\n\n        if sentences is None and train:\n            raise ValueError(""When training, you must feed ""\n                             ""reference sentences"")\n\n        if sentences is not None:\n            fd[self.train_tokens] = pad_batch(\n                list(sentences), self.max_output_len, add_start_symbol=False,\n                add_end_symbol=True)\n\n        return fd\n'"
neuralmonkey/decoders/beam_search_decoder.py,74,"b'""""""Beam search decoder.\n\nThis module implements the beam search algorithm for autoregressive decoders.\n\nAs any autoregressive decoder, this decoder works dynamically, which means\nit uses the ``tf.while_loop`` function conditioned on both maximum output\nlength and list of finished hypotheses.\n\nThe beam search decoder uses four data strcutures during the decoding process.\n``SearchState``, ``SearchResults``, ``BeamSearchLoopState``, and\n``BeamSearchOutput``. The purpose of these is described in their own docstring.\n\nThese structures help the decoder to keep track of the decoding, enabling it\nto be called e.g. during ensembling, when the content of the structures can be\nchanged and then fed back to the model.\n\nThe implementation mimics the API of the ``AutoregressiveDecoder`` class. There\nare functions that prepare and return values that are supplied to the\n``tf.while_loop`` function.\n\n""""""\n# pylint: disable=too-many-lines\n# Maybe move the definitions of the named tuple structures to a separate file?\nfrom typing import Any, Callable, List, NamedTuple\n# pylint: disable=unused-import\nfrom typing import Optional\n# pylint: enable=unused-import\n\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.decoders.autoregressive import (\n    AutoregressiveDecoder, LoopState)\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.tf_utils import (\n    append_tensor, gather_flat, get_state_shape_invariants, partial_transpose,\n    get_shape_list)\nfrom neuralmonkey.vocabulary import (\n    Vocabulary, END_TOKEN_INDEX, PAD_TOKEN_INDEX)\n\n# Constant we use in place of the np.inf\nINF = 1e9\n\n\nclass SearchState(NamedTuple(\n        ""SearchState"",\n        [(""logprob_sum"", tf.Tensor),\n         (""prev_logprobs"", tf.Tensor),\n         (""lengths"", tf.Tensor),\n         (""finished"", tf.Tensor)])):\n    """"""Search state of a beam search decoder.\n\n    This structure keeps track of a current state of the beam search\n    algorithm. The search state contains tensors that represent hypotheses in\n    the beam, namely their log probability, length, and distribution over the\n    vocabulary when decoding the last word, as well as if the hypothesis is\n    finished or not.\n\n    Attributes:\n        logprob_sum: A ``(batch, beam)``-shaped tensor with the sums of token\n            log-probabilities of each hypothesis.\n        prev_logprobs: A ``(batch, beam, vocabulary)``-sized tensor. Stores\n            the log-distribution over the vocabulary from the previous decoding\n            step for each hypothesis.\n        lengths: A ``(batch, beam)``-shaped tensor with the lengths of the\n            hypotheses.\n        finished: A boolean tensor with shape ``(batch, beam)``. Marks finished\n            and unfinished hypotheses.\n    """"""\n\n\nclass SearchResults(NamedTuple(\n        ""SearchResults"",\n        [(""scores"", tf.Tensor),\n         (""token_ids"", tf.Tensor)])):\n    """"""The intermediate results of the beam search decoding.\n\n    A cummulative structure that holds the actual decoded tokens and hypotheses\n    scores (after applying a length penalty term).\n\n    Attributes:\n        scores: A ``(time, batch, beam)``-shaped tensor with the scores for\n            each hypothesis. The score is computed from the ``logprob_sum`` of\n            a hypothesis and accounting for the hypothesis length.\n        token_ids: A ``(time, batch, beam)``-shaped tensor with the vocabulary\n            indices of the tokens in each hypothesis.\n    """"""\n\n\nclass BeamSearchLoopState(NamedTuple(\n        ""BeamSearchLoopState"",\n        [(""search_state"", SearchState),\n         (""search_results"", SearchResults),\n         (""decoder_loop_state"", LoopState)])):\n    """"""The loop state of the beam search decoder.\n\n    A loop state object that is used for transferring data between cycles\n    through the symbolic while loop. It groups together the ``SearchState`` and\n    ``SearchResults`` structures and also keeps track of the underlying decoder\n    loop state.\n\n    Attributes:\n        search_state: A ``SearchState`` object representing the current search\n            state.\n        search_results: The growing ``SearchResults`` object which accummulates\n            the outputs of the decoding process.\n        decoder_loop_state: The current loop state of the underlying\n            autoregressive decoder.\n    """"""\n\n\nclass BeamSearchOutput(NamedTuple(\n        ""BeamSearchOutput"",\n        [(""last_search_step_output"", SearchResults),\n         (""last_dec_loop_state"", NamedTuple),\n         (""last_search_state"", SearchState),\n         (""attention_loop_states"", List[Any])])):\n    """"""The final structure that is returned from the while loop.\n\n    Attributes:\n        last_search_step_output: A populated ``SearchResults`` object.\n        last_dec_loop_state: Final loop state of the underlying decoder.\n        last_search_state: Final loop state of the beam search decoder.\n        attention_loop_states: The final loop states of the attention objects.\n    """"""\n\n\nclass BeamSearchDecoder(ModelPart):\n    """"""In-graph beam search decoder.\n\n    The hypothesis scoring algorithm is taken from\n    https://arxiv.org/pdf/1609.08144.pdf. Length normalization is parameter\n    alpha from equation 14.\n    """"""\n\n    def __init__(self,\n                 name: str,\n                 parent_decoder: AutoregressiveDecoder,\n                 beam_size: int,\n                 max_steps: int,\n                 length_normalization: float) -> None:\n        """"""Construct the beam search decoder graph.\n\n        Arguments:\n            name: The name for the model part.\n            parent_decoder: An autoregressive decoder from which to sample.\n            beam_size: The number of hypotheses in the beam.\n            max_steps: The maximum number of time steps to perform.\n            length_normalization: The alpha parameter from Eq. 14 in the paper.\n        """"""\n        check_argument_types()\n        ModelPart.__init__(self, name)\n\n        self.parent_decoder = parent_decoder\n        self.beam_size = beam_size\n        self.length_normalization = length_normalization\n        self.max_steps_int = max_steps\n\n        # Create a placeholder for maximum number of steps that is necessary\n        # during ensembling, when the decoder is called repetitively with the\n        # max_steps attribute set to one.\n        self.max_steps = tf.placeholder_with_default(self.max_steps_int, [])\n\n        self._initial_loop_state = None  # type: Optional[BeamSearchLoopState]\n\n    @tensor\n    def outputs(self) -> tf.Tensor:\n        # This is an ugly hack for handling the whole graph when expanding to\n        # the beam. We need to access all the inner states of the network in\n        # the graph, replace them with beam-size-times copied originals, create\n        # the beam search graph, and then replace the inner states back.\n\n        enc_states = self.parent_decoder.encoder_states\n        enc_masks = self.parent_decoder.encoder_masks\n\n        setattr(self.parent_decoder, ""encoder_states"",\n                lambda: [self.expand_to_beam(sts) for sts in enc_states()])\n        setattr(self.parent_decoder, ""encoder_masks"",\n                lambda: [self.expand_to_beam(mask) for mask in enc_masks()])\n\n        # Create the beam search symbolic graph.\n        with self.use_scope():\n            self._initial_loop_state = self.get_initial_loop_state()\n            outputs = self.decoding_loop()\n\n        # Reassign the original encoder states and mask back\n        setattr(self.parent_decoder, ""encoder_states"", enc_states)\n        setattr(self.parent_decoder, ""encoder_masks"", enc_masks)\n\n        return outputs\n\n    @property\n    def initial_loop_state(self) -> BeamSearchLoopState:\n        if self._initial_loop_state is None:\n            raise RuntimeError(""Initial loop state was not initialized"")\n        return self._initial_loop_state\n\n    @property\n    def vocabulary(self) -> Vocabulary:\n        return self.parent_decoder.vocabulary\n\n    # Note that the attributes search_state, decoder_state, and search_results\n    # are used only when ensembling, which is done with max_steps set to one\n    # and calling the beam search decoder repetitively.\n    @tensor\n    def search_state(self) -> SearchState:\n        return self.initial_loop_state.search_state\n\n    @tensor\n    def decoder_state(self) -> LoopState:\n        return self.initial_loop_state.decoder_loop_state\n\n    @tensor\n    def search_results(self) -> SearchResults:\n        return self.initial_loop_state.search_results\n\n    def get_initial_loop_state(self) -> BeamSearchLoopState:\n        """"""Construct the initial loop state for the beam search decoder.\n\n        During the construction, the body function of the underlying decoder\n        is called once to retrieve the initial log probabilities of the first\n        token.\n\n        The values are initialized as follows:\n\n        - ``search_state``\n            - ``logprob_sum`` - For each sentence in batch, logprob sum of the\n              first hypothesis in the beam is set to zero while the others are\n              set to negative infinity.\n            - ``prev_logprobs`` - This is the softmax over the logits from the\n              initial decoder step.\n            - ``lengths`` - All zeros.\n            - ``finshed`` - All false.\n\n        - ``search_results``\n            - ``scores`` - A (batch, beam)-sized tensor of zeros.\n            - ``token_ids`` - A (1, batch, beam)-sized tensor filled with\n              indices of decoder-specific initial input symbols (usually start\n              symbol IDs).\n\n        - ``decoder_loop_state`` - The loop state of the underlying\n            autoregressive decoder, as returned from the initial call to the\n            body function.\n\n        Returns:\n            A populated ``BeamSearchLoopState`` structure.\n        """"""\n        # Get the initial loop state of the underlying decoder. Then, expand\n        # the tensors from the loop state to (batch * beam) and inject them\n        # back into the decoder loop state.\n\n        dec_init_ls = self.parent_decoder.get_initial_loop_state()\n\n        feedables = tf.contrib.framework.nest.map_structure(\n            self.expand_to_beam, dec_init_ls.feedables)\n        histories = tf.contrib.framework.nest.map_structure(\n            lambda x: self.expand_to_beam(x, dim=1), dec_init_ls.histories)\n\n        constants = tf.constant(0)\n        if dec_init_ls.constants:\n            constants = tf.contrib.framework.nest.map_structure(\n                self.expand_to_beam, dec_init_ls.constants)\n\n        dec_init_ls = dec_init_ls._replace(\n            feedables=feedables,\n            histories=histories,\n            constants=constants)\n\n        # Call the decoder body function with the expanded loop state to get\n        # the log probabilities of the possible first tokens.\n\n        decoder_body = self.parent_decoder.get_body(False)\n        dec_next_ls = decoder_body(*dec_init_ls)\n\n        # Construct the initial loop state of the beam search decoder. To allow\n        # ensembling, the values are replaced with placeholders with a default\n        # value. Despite this is necessary only for variables that grow in\n        # time, the placeholder replacement is done on the whole structures, as\n        # you can see below.\n\n        logits = dec_next_ls.histories.logits[-1, :, :]\n        search_state = SearchState(\n            logprob_sum=tf.tile(\n                tf.expand_dims([0.0] + [-INF] * (self.beam_size - 1), 0),\n                [self.batch_size, 1],\n                name=""bs_logprob_sum""),\n            prev_logprobs=tf.reshape(\n                tf.nn.log_softmax(logits),\n                [self.batch_size, self.beam_size, len(self.vocabulary)]),\n            lengths=tf.zeros(\n                [self.batch_size, self.beam_size], dtype=tf.int32,\n                name=""bs_lengths""),\n            finished=tf.zeros(\n                [self.batch_size, self.beam_size], dtype=tf.bool))\n\n        # We add the input_symbol to token_ids during search_results\n        # initialization for simpler beam_body implementation\n\n        input_symbols = dec_next_ls.histories.output_symbols[-1, :]\n        search_results = SearchResults(\n            scores=tf.zeros(\n                shape=[self.batch_size, self.beam_size],\n                dtype=tf.float32,\n                name=""beam_scores""),\n            token_ids=tf.reshape(\n                input_symbols,\n                [1, self.batch_size, self.beam_size],\n                name=""beam_tokens""))\n\n        # In structures that contain tensors that grow in time, we replace\n        # tensors with placeholders with loosened shape constraints in the time\n        # dimension.\n\n        dec_next_ls = tf.contrib.framework.nest.map_structure(\n            lambda x: tf.placeholder_with_default(\n                x, get_state_shape_invariants(x)),\n            dec_next_ls)\n\n        search_results = tf.contrib.framework.nest.map_structure(\n            lambda x: tf.placeholder_with_default(\n                x, get_state_shape_invariants(x)),\n            search_results)\n\n        return BeamSearchLoopState(\n            search_state=search_state,\n            search_results=search_results,\n            decoder_loop_state=dec_next_ls)\n\n    def loop_continue_criterion(self, *args) -> tf.Tensor:\n        """"""Decide whether to break out of the while loop.\n\n        The criterion for stopping the loop is that either all hypotheses are\n        finished or a maximum number of steps has been reached. Here the number\n        of steps is the number of steps of the underlying decoder minus one,\n        because this function is evaluated after the decoder step has been\n        called and its step has been incremented. This is caused by the fact\n        that we call the decoder body function at the end of the beam body\n        function. (And that, in turn, is to support ensembling.)\n\n        Arguments:\n            args: A ``BeamSearchLoopState`` instance.\n\n        Returns:\n            A scalar boolean ``Tensor``.\n        """"""\n        loop_state = BeamSearchLoopState(*args)\n\n        beam_step = loop_state.decoder_loop_state.feedables.step - 1\n        finished = loop_state.search_state.finished\n\n        max_step_cond = tf.less(beam_step, self.max_steps)\n        unfinished_cond = tf.logical_not(tf.reduce_all(finished))\n\n        return tf.logical_and(max_step_cond, unfinished_cond)\n\n    def decoding_loop(self) -> BeamSearchOutput:\n        """"""Create the decoding loop.\n\n        This function mimics the behavior of the ``decoding_loop`` method of\n        the ``AutoregressiveDecoder``, except the initial loop state is created\n        outside this method because it is accessed and fed during ensembling.\n\n        TODO: The ``finalize_loop`` method and the handling of attention loop\n        states might be implemented in the future.\n\n        Returns:\n            This method returns a populated ``BeamSearchOutput`` object.\n        """"""\n\n        final_loop_state = tf.while_loop(\n            self.loop_continue_criterion,\n            self.get_body(),\n            self.initial_loop_state,\n            shape_invariants=tf.contrib.framework.nest.map_structure(\n                get_state_shape_invariants, self.initial_loop_state))\n\n        # TODO: return att_loop_states properly\n        return BeamSearchOutput(\n            last_search_step_output=final_loop_state.search_results,\n            last_dec_loop_state=final_loop_state.decoder_loop_state,\n            last_search_state=final_loop_state.search_state,\n            attention_loop_states=[])\n\n    def get_body(self) -> Callable[[Any], BeamSearchLoopState]:\n        """"""Return a body function for ``tf.while_loop``.\n\n        Returns:\n            A function that performs a single decoding step.\n        """"""\n        decoder_body = self.parent_decoder.get_body(train_mode=False)\n\n        # pylint: disable=too-many-locals\n        def body(*args: Any) -> BeamSearchLoopState:\n            """"""Execute a single beam search step.\n\n            An implementation of the beam search algorithm, which works as\n            follows:\n\n            1. Create a valid ``logprobs`` tensor which contains distributions\n               over the output tokens for each hypothesis in the beam. For\n               finished hypotheses, the log probabilities of all tokens except\n               the padding token are set to negative infinity.\n\n            2. Expand the beam by appending every possible token to every\n               existing hypothesis. Update the log probabilitiy sum of each\n               hypothesis and its length (add one for unfinished hypotheses).\n               For each hypothesis, compute the score using the length penalty\n               term.\n\n            3. Select the ``beam_size`` best hypotheses from the score pool.\n               This is implemented by flattening the scores tensor and using\n               the ``tf.nn.top_k`` function.\n\n            4. Reconstruct the beam by gathering elements from the original\n               data structures using the data indices computed in the previous\n               step.\n\n            5. Call the ``body`` function of the underlying decoder.\n\n            6. Populate a new ``BeamSearchLoopState`` object with the selected\n               values and with the newly obtained decoder loop state.\n\n            Note that this function expects the decoder to be called at least\n            once prior the first execution.\n\n            Arguments:\n                args: An instance of the ``BeamSearchLoopState`` structure.\n                    (see the docs for this module)\n\n            Returns:\n                A ``BeamSearchLoopState`` after one step of the decoding.\n\n            """"""\n            loop_state = BeamSearchLoopState(*args)\n            dec_loop_state = loop_state.decoder_loop_state\n            search_state = loop_state.search_state\n            search_results = loop_state.search_results\n\n            # mask the probabilities\n            # shape(logprobs) = [batch, beam, vocabulary]\n            logprobs = search_state.prev_logprobs\n\n            finished_mask = tf.expand_dims(\n                tf.to_float(search_state.finished), 2)\n            unfinished_logprobs = (1. - finished_mask) * logprobs\n\n            finished_row = tf.one_hot(\n                PAD_TOKEN_INDEX,\n                len(self.vocabulary),\n                dtype=tf.float32,\n                on_value=0.,\n                off_value=-INF)\n\n            finished_logprobs = finished_mask * finished_row\n            logprobs = unfinished_logprobs + finished_logprobs\n\n            # update hypothesis scores\n            # shape(hyp_probs) = [batch, beam, vocabulary]\n            hyp_probs = tf.expand_dims(search_state.logprob_sum, 2) + logprobs\n\n            # update hypothesis lengths\n            hyp_lengths = search_state.lengths + 1 - tf.to_int32(\n                search_state.finished)\n\n            # shape(scores) = [batch, beam, vocabulary]\n            scores = hyp_probs / tf.expand_dims(\n                self._length_penalty(hyp_lengths), 2)\n\n            # reshape to [batch, beam * vocabulary] for topk\n            scores_flat = tf.reshape(\n                scores, [-1, self.beam_size * len(self.vocabulary)])\n\n            # shape(both) = [batch, beam]\n            topk_scores, topk_indices = tf.nn.top_k(\n                scores_flat, k=self.beam_size)\n\n            topk_indices.set_shape([None, self.beam_size])\n            topk_scores.set_shape([None, self.beam_size])\n\n            next_word_ids = tf.to_int64(\n                tf.mod(topk_indices, len(self.vocabulary)))\n            next_beam_ids = tf.div(topk_indices, len(self.vocabulary))\n\n            # batch offset for tf.gather_nd\n            batch_offset = tf.tile(\n                tf.expand_dims(tf.range(self.batch_size), 1),\n                [1, self.beam_size])\n            batch_beam_ids = tf.stack([batch_offset, next_beam_ids], axis=2)\n\n            # gather the topk logprob_sums\n            next_beam_lengths = tf.gather_nd(hyp_lengths, batch_beam_ids)\n            next_beam_logprob_sum = tf.gather_nd(\n                tf.reshape(\n                    hyp_probs, [-1, self.beam_size * len(self.vocabulary)]),\n                tf.stack([batch_offset, topk_indices], axis=2))\n\n            # mark finished beams\n            next_finished = tf.gather_nd(search_state.finished, batch_beam_ids)\n            next_just_finished = tf.equal(next_word_ids, END_TOKEN_INDEX)\n            next_finished = tf.logical_or(next_finished, next_just_finished)\n\n            # we need to flatten the feedables for the parent_decoder\n            next_feedables = tf.contrib.framework.nest.map_structure(\n                lambda x: gather_flat(x, batch_beam_ids,\n                                      self.batch_size, self.beam_size),\n                dec_loop_state.feedables)\n\n            next_feedables = next_feedables._replace(\n                embedded_input=self.parent_decoder.embed_input_symbols(\n                    tf.reshape(next_word_ids, [-1])),\n                finished=tf.reshape(next_finished, [-1]))\n\n            # histories have shape [len, batch, ...]\n            def gather_fn(x):\n                if len(x.shape.dims) < 2:\n                    return x\n\n                return partial_transpose(\n                    gather_flat(\n                        partial_transpose(x, [1, 0]),\n                        batch_beam_ids,\n                        self.batch_size,\n                        self.beam_size),\n                    [1, 0])\n\n            next_histories = tf.contrib.framework.nest.map_structure(\n                gather_fn, dec_loop_state.histories)\n\n            dec_loop_state = dec_loop_state._replace(\n                feedables=next_feedables,\n                histories=next_histories)\n\n            # CALL THE DECODER BODY FUNCTION\n            next_loop_state = decoder_body(*dec_loop_state)\n\n            logits = next_loop_state.histories.logits[-1, :, :]\n            next_search_state = SearchState(\n                logprob_sum=next_beam_logprob_sum,\n                prev_logprobs=tf.reshape(\n                    tf.nn.log_softmax(logits),\n                    [self.batch_size, self.beam_size, len(self.vocabulary)]),\n                lengths=next_beam_lengths,\n                finished=next_finished)\n\n            next_token_ids = tf.transpose(search_results.token_ids, [1, 2, 0])\n            next_token_ids = tf.gather_nd(next_token_ids, batch_beam_ids)\n            next_token_ids = tf.transpose(next_token_ids, [2, 0, 1])\n            next_output = SearchResults(\n                scores=topk_scores,\n                token_ids=append_tensor(next_token_ids, next_word_ids))\n\n            return BeamSearchLoopState(\n                search_state=next_search_state,\n                search_results=next_output,\n                decoder_loop_state=next_loop_state)\n        # pylint: enable=too-many-locals\n\n        return body\n\n    def _length_penalty(self, lengths: tf.Tensor) -> tf.Tensor:\n        """"""Apply length penalty (""lp"") term from Eq. 14.\n\n        https://arxiv.org/pdf/1609.08144.pdf\n\n        Arguments:\n            lengths: A ``Tensor`` of lengths of the hypotheses in the beam.\n\n        Returns:\n            A float ``Tensor`` with the length penalties for each hypothesis\n            in the beam.\n        """"""\n        return ((5. + tf.to_float(lengths)) / 6.) ** self.length_normalization\n\n    def expand_to_beam(self, val: tf.Tensor, dim: int = 0) -> tf.Tensor:\n        """"""Copy a tensor along a new beam dimension.\n\n        Arguments:\n            val: The ``Tensor`` to expand.\n            dim: The dimension along which to expand. Usually, the batch axis.\n\n        Returns:\n            The expanded tensor.\n        """"""\n        orig_shape = get_shape_list(val)\n        if val.shape.ndims == 0:\n            return val\n\n        orig_shape[dim] *= self.beam_size\n        tile_shape = [1] * (len(orig_shape) + 1)\n        tile_shape[dim + 1] = self.beam_size\n\n        val = tf.tile(tf.expand_dims(val, 1), tile_shape)\n        val = tf.reshape(val, orig_shape)\n\n        return val\n'"
neuralmonkey/decoders/classifier.py,20,"b'from typing import Callable, Dict, List\n\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.dataset import Dataset\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.model.feedable import FeedDict\nfrom neuralmonkey.model.parameterized import InitializerSpecs\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.model.stateful import Stateful\nfrom neuralmonkey.nn.mlp import MultilayerPerceptron\nfrom neuralmonkey.vocabulary import Vocabulary, pad_batch\n\n\nclass Classifier(ModelPart):\n    """"""A simple MLP classifier over encoders.\n\n    The API pretends it is an RNN decoder which always generates a sequence of\n    length exactly one.\n    """"""\n\n    # pylint: disable=too-many-arguments\n    def __init__(self,\n                 name: str,\n                 encoders: List[Stateful],\n                 vocabulary: Vocabulary,\n                 data_id: str,\n                 layers: List[int],\n                 activation_fn: Callable[[tf.Tensor], tf.Tensor] = tf.nn.relu,\n                 dropout_keep_prob: float = 0.5,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        """"""Construct a new instance of the sequence classifier.\n\n        Args:\n            name: Name of the decoder. Should be unique accross all Neural\n                Monkey objects\n            encoders: Input encoders of the decoder\n            vocabulary: Target vocabulary\n            data_id: Target data series\n            layers: List defining structure of the NN. Ini example:\n                    layers=[100,20,5] ;creates classifier with hidden layers of\n                                       size 100, 20, 5 and one output layer\n                                       depending on the size of vocabulary\n            activation_fn: activation function used on the output of each\n                           hidden layer.\n            dropout_keep_prob: Probability of keeping a value during dropout\n        """"""\n        check_argument_types()\n        ModelPart.__init__(self, name, reuse, save_checkpoint, load_checkpoint,\n                           initializers)\n\n        self.encoders = encoders\n        self.vocabulary = vocabulary\n        self.data_id = data_id\n        self.layers = layers\n        self.activation_fn = activation_fn\n        self.dropout_keep_prob = dropout_keep_prob\n        self.max_output_len = 1\n    # pylint: enable=too-many-arguments\n\n    @property\n    def input_types(self) -> Dict[str, tf.DType]:\n        return {self.data_id: tf.string}\n\n    @property\n    def input_shapes(self) -> Dict[str, tf.TensorShape]:\n        return {self.data_id: tf.TensorShape([None])}\n\n    @tensor\n    def gt_inputs(self) -> tf.Tensor:\n        return self.vocabulary.strings_to_indices(self.targets)\n\n    @tensor\n    def targets(self) -> tf.Tensor:\n        return self.dataset[self.data_id]\n\n    @tensor\n    def _mlp(self) -> MultilayerPerceptron:\n        mlp_input = tf.concat([enc.output for enc in self.encoders], 1)\n        return MultilayerPerceptron(\n            mlp_input, self.layers, self.dropout_keep_prob,\n            len(self.vocabulary), activation_fn=self.activation_fn,\n            train_mode=self.train_mode)\n\n    @property\n    def loss_with_decoded_ins(self) -> tf.Tensor:\n        return self.loss_with_gt_ins\n\n    @property\n    def cost(self) -> tf.Tensor:\n        tf.summary.scalar(\n            ""train_optimization_cost"",\n            self.loss_with_gt_ins, collections=[""summary_train""])\n\n        return self.loss_with_gt_ins\n\n    # pylint: disable=no-member\n    # this is for the _mlp attribute (pylint property bug)\n    @tensor\n    def loss_with_gt_ins(self) -> tf.Tensor:\n        return tf.reduce_mean(\n            tf.nn.sparse_softmax_cross_entropy_with_logits(\n                logits=self._mlp.logits, labels=self.gt_inputs))\n\n    @tensor\n    def decoded_seq(self) -> tf.Tensor:\n        return tf.expand_dims(self._mlp.classification, 0)\n\n    @tensor\n    def decoded_logits(self) -> tf.Tensor:\n        return tf.expand_dims(self._mlp.logits, 0)\n\n    @tensor\n    def runtime_logprobs(self) -> tf.Tensor:\n        return tf.expand_dims(tf.nn.log_softmax(self._mlp.logits), 0)\n    # pylint: enable=no-member\n\n    @property\n    def train_loss(self):\n        return self.loss_with_gt_ins\n\n    @property\n    def runtime_loss(self):\n        return self.loss_with_decoded_ins\n\n    @property\n    def decoded(self):\n        return self.decoded_seq\n\n    def feed_dict(self, dataset: Dataset, train: bool = False) -> FeedDict:\n        fd = ModelPart.feed_dict(self, dataset, train)\n        sentences = dataset.maybe_get_series(self.data_id)\n\n        if sentences is not None:\n            labels = [l[0] for l in pad_batch(list(sentences),\n                                              self.max_output_len)]\n            fd[self.targets] = labels\n\n        return fd\n'"
neuralmonkey/decoders/ctc_decoder.py,32,"b'from typing import Dict\n\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.dataset import Dataset\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.model.feedable import FeedDict\nfrom neuralmonkey.model.parameterized import InitializerSpecs\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.model.stateful import TemporalStateful\nfrom neuralmonkey.tf_utils import get_variable\nfrom neuralmonkey.vocabulary import (Vocabulary, pad_batch, END_TOKEN_INDEX,\n                                     PAD_TOKEN_INDEX)\n\n\nclass CTCDecoder(ModelPart):\n    """"""Connectionist Temporal Classification.\n\n    See `tf.nn.ctc_loss`, `tf.nn.ctc_greedy_decoder` etc.\n    """"""\n\n    # pylint: disable=too-many-arguments\n    def __init__(self,\n                 name: str,\n                 encoder: TemporalStateful,\n                 vocabulary: Vocabulary,\n                 data_id: str,\n                 max_length: int = None,\n                 merge_repeated_targets: bool = False,\n                 merge_repeated_outputs: bool = True,\n                 beam_width: int = 1,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        check_argument_types()\n        ModelPart.__init__(self, name, reuse, save_checkpoint, load_checkpoint,\n                           initializers)\n\n        self.encoder = encoder\n        self.vocabulary = vocabulary\n        self.data_id = data_id\n        self.max_length = max_length\n\n        self.merge_repeated_targets = merge_repeated_targets\n        self.merge_repeated_outputs = merge_repeated_outputs\n        self.beam_width = beam_width\n    # pylint: enable=too-many-arguments\n\n    @property\n    def input_types(self) -> Dict[str, tf.DType]:\n        return {self.data_id: tf.string}\n\n    @property\n    def input_shapes(self) -> Dict[str, tf.TensorShape]:\n        return {self.data_id: tf.TensorShape([None, None])}\n\n    @tensor\n    def target_tokens(self) -> tf.Tensor:\n        return self.dataset[self.data_id]\n\n    @tensor\n    def train_targets(self) -> tf.SparseTensor:\n        params = self.vocabulary.strings_to_indices(self.target_tokens)\n\n        indices = tf.where(tf.not_equal(params, PAD_TOKEN_INDEX))\n        values = tf.gather_nd(params, indices)\n\n        return tf.cast(\n            tf.SparseTensor(\n                indices, values, tf.shape(params, out_type=tf.int64)),\n            tf.int32)\n\n    @tensor\n    def decoded(self) -> tf.Tensor:\n        if self.beam_width == 1:\n            decoded, _ = tf.nn.ctc_greedy_decoder(\n                inputs=self.logits, sequence_length=self.encoder.lengths,\n                merge_repeated=self.merge_repeated_outputs)\n        else:\n            decoded, _ = tf.nn.ctc_beam_search_decoder(\n                inputs=self.logits, sequence_length=self.encoder.lengths,\n                beam_width=self.beam_width,\n                merge_repeated=self.merge_repeated_outputs)\n\n        return tf.sparse_tensor_to_dense(\n            tf.sparse_transpose(decoded[0]),\n            default_value=END_TOKEN_INDEX)\n\n    @property\n    def train_loss(self) -> tf.Tensor:\n        return self.cost\n\n    @property\n    def runtime_loss(self) -> tf.Tensor:\n        return self.cost\n\n    @tensor\n    def cost(self) -> tf.Tensor:\n        loss = tf.nn.ctc_loss(\n            labels=self.train_targets, inputs=self.logits,\n            sequence_length=self.encoder.lengths,\n            preprocess_collapse_repeated=self.merge_repeated_targets,\n            ignore_longer_outputs_than_inputs=True,\n            ctc_merge_repeated=self.merge_repeated_outputs)\n\n        return tf.reduce_sum(loss)\n\n    @tensor\n    def logits(self) -> tf.Tensor:\n        vocabulary_size = len(self.vocabulary)\n\n        encoder_states = self.encoder.temporal_states\n\n        weights = get_variable(\n            name=""state_to_word_W"",\n            shape=[encoder_states.shape[2], vocabulary_size + 1],\n            initializer=tf.random_uniform_initializer(-0.5, 0.5))\n\n        biases = get_variable(\n            name=""state_to_word_b"",\n            shape=[vocabulary_size + 1],\n            initializer=tf.zeros_initializer())\n\n        # To multiply 3-D matrix (encoder hidden states) by a 2-D matrix\n        # (weights), we use 1-by-1 convolution (similar trick can be found in\n        # attention computation)\n\n        encoder_states = tf.expand_dims(encoder_states, 2)\n        weights_4d = tf.expand_dims(tf.expand_dims(weights, 0), 0)\n\n        multiplication = tf.nn.conv2d(\n            encoder_states, weights_4d, [1, 1, 1, 1], ""SAME"")\n        multiplication_3d = tf.squeeze(multiplication, axis=2)\n\n        biases_3d = tf.expand_dims(tf.expand_dims(biases, 0), 0)\n\n        logits = multiplication_3d + biases_3d\n        return tf.transpose(logits, perm=[1, 0, 2])  # time major\n\n    def feed_dict(self, dataset: Dataset, train: bool = False) -> FeedDict:\n        fd = ModelPart.feed_dict(self, dataset, train)\n\n        sentences = dataset.maybe_get_series(self.data_id)\n\n        if sentences is None and train:\n            raise ValueError(""You must feed reference sentences when training"")\n\n        if sentences is not None:\n            fd[self.target_tokens] = pad_batch(list(sentences),\n                                               self.max_length)\n\n        return fd\n'"
neuralmonkey/decoders/decoder.py,26,"b'from typing import Any, List, Tuple, cast, NamedTuple\n\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.decoders.autoregressive import (\n    AutoregressiveDecoder, DecoderFeedables, DecoderHistories, LoopState)\nfrom neuralmonkey.attention.base_attention import BaseAttention\nfrom neuralmonkey.vocabulary import Vocabulary\nfrom neuralmonkey.model.sequence import EmbeddedSequence\nfrom neuralmonkey.model.stateful import Stateful\nfrom neuralmonkey.model.parameterized import InitializerSpecs\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.logging import log\nfrom neuralmonkey.nn.ortho_gru_cell import OrthoGRUCell, NematusGRUCell\nfrom neuralmonkey.nn.utils import dropout\nfrom neuralmonkey.tf_utils import append_tensor\nfrom neuralmonkey.decoders.encoder_projection import (\n    linear_encoder_projection, concat_encoder_projection, empty_initial_state,\n    EncoderProjection)\nfrom neuralmonkey.decoders.output_projection import (\n    OutputProjectionSpec, OutputProjection, nonlinear_output)\nfrom neuralmonkey.decorators import tensor\n\n\nRNN_CELL_TYPES = {\n    ""NematusGRU"": NematusGRUCell,\n    ""GRU"": OrthoGRUCell,\n    ""LSTM"": tf.contrib.rnn.LSTMCell\n}\n\n\nclass RNNFeedables(NamedTuple(\n        ""RNNFeedables"", [\n            (""prev_rnn_state"", tf.Tensor),\n            (""prev_rnn_output"", tf.Tensor),\n            (""prev_contexts"", List[tf.Tensor])])):\n    """"""Additional feedables used only by the RNN-based decoder.\n\n    Attributes:\n        prev_rnn_state: The recurrent state from the previous step. A tensor\n            of shape ``(batch, rnn_size)``\n        prev_rnn_output: The output of the recurrent network from the previous\n            step. A tensor of shape ``(batch, output_size)``\n        prev_contexts: A list of context vectors returned from attention\n            mechanisms. Tensors of shape ``(batch, encoder_state_size)`` for\n            each attended encoder.\n    """"""\n\n\nclass RNNHistories(NamedTuple(\n        ""RNNHistories"", [\n            (""rnn_outputs"", tf.Tensor),\n            (""attention_histories"", List[Tuple])])):\n    """"""The loop state histories specific for RNN-based decoders.\n\n    Attributes:\n        rnn_outputs: History of outputs produced by RNN cell itself (before\n            applying output projections).\n        attention_histories: A list of ``AttentionLoopState`` objects (or\n            similar) populated by values from the attention mechanisms used in\n            the decoder.\n    """"""\n\n\n# pylint: disable=too-many-instance-attributes\nclass Decoder(AutoregressiveDecoder):\n    """"""A class managing parts of the computation graph used during decoding.""""""\n\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-arguments,too-many-branches,too-many-statements\n    def __init__(self,\n                 encoders: List[Stateful],\n                 vocabulary: Vocabulary,\n                 data_id: str,\n                 name: str,\n                 max_output_len: int,\n                 dropout_keep_prob: float = 1.0,\n                 embedding_size: int = None,\n                 embeddings_source: EmbeddedSequence = None,\n                 tie_embeddings: bool = False,\n                 label_smoothing: float = None,\n                 rnn_size: int = None,\n                 output_projection: OutputProjectionSpec = None,\n                 encoder_projection: EncoderProjection = None,\n                 attentions: List[BaseAttention] = None,\n                 attention_on_input: bool = False,\n                 rnn_cell: str = ""GRU"",\n                 conditional_gru: bool = False,\n                 supress_unk: bool = False,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        """"""Create a refactored version of monster decoder.\n\n        Arguments:\n            encoders: Input encoders of the decoder.\n            vocabulary: Target vocabulary.\n            data_id: Target data series.\n            name: Name of the decoder. Should be unique accross all Neural\n                Monkey objects.\n            max_output_len: Maximum length of an output sequence.\n            dropout_keep_prob: Probability of keeping a value during dropout.\n            embedding_size: Size of embedding vectors for target words.\n            embeddings_source: Embedded sequence to take embeddings from.\n            tie_embeddings: Use decoder.embedding_matrix also in place\n                of the output decoding matrix.\n            rnn_size: Size of the decoder hidden state, if None set\n                according to encoders.\n            output_projection: How to generate distribution over vocabulary\n                from decoder_outputs.\n            encoder_projection: How to construct initial state from encoders.\n            attention: The attention object to use. Optional.\n            rnn_cell: RNN Cell used by the decoder (GRU or LSTM).\n            conditional_gru: Flag whether to use the Conditional GRU\n                architecture.\n            attention_on_input: Flag whether attention from previous decoding\n                step should be combined with the input in the next step.\n            supress_unk: If true, decoder will not produce symbols for unknown\n                tokens.\n            reuse: Reuse the model variables from the given model part.\n        """"""\n        check_argument_types()\n        AutoregressiveDecoder.__init__(\n            self,\n            name=name,\n            vocabulary=vocabulary,\n            data_id=data_id,\n            max_output_len=max_output_len,\n            dropout_keep_prob=dropout_keep_prob,\n            embedding_size=embedding_size,\n            embeddings_source=embeddings_source,\n            tie_embeddings=tie_embeddings,\n            label_smoothing=label_smoothing,\n            supress_unk=supress_unk,\n            reuse=reuse,\n            save_checkpoint=save_checkpoint,\n            load_checkpoint=load_checkpoint,\n            initializers=initializers)\n\n        self.encoders = encoders\n        self._output_projection_spec = output_projection\n        self._conditional_gru = conditional_gru\n        self._attention_on_input = attention_on_input\n        self._rnn_cell_str = rnn_cell\n        self._rnn_size = rnn_size\n        self._encoder_projection = encoder_projection\n\n        self.attentions = []  # type: List[BaseAttention]\n        if attentions is not None:\n            self.attentions = attentions\n\n        if not rnn_size and not encoder_projection and not encoders:\n            raise ValueError(\n                ""No RNN size, no encoders and no encoder_projection specified"")\n\n        if self._rnn_cell_str not in RNN_CELL_TYPES:\n            raise ValueError(""RNN cell must be a either \'GRU\', \'LSTM\', or ""\n                             ""\'NematusGRU\'. Not {}"".format(self._rnn_cell_str))\n\n        if self._attention_on_input:\n            self.input_projection = self.input_plus_attention\n        else:\n            self.input_projection = (\n                lambda *args: LoopState(*args).feedables.embedded_input)\n\n        with self.use_scope():\n            with tf.variable_scope(""attention_decoder"") as self.step_scope:\n                pass\n\n        self._variable_scope.set_initializer(\n            tf.random_normal_initializer(stddev=0.001))\n    # pylint: enable=too-many-arguments,too-many-branches,too-many-statements\n\n    @property\n    def encoder_projection(self) -> EncoderProjection:\n        if self._encoder_projection is not None:\n            return self._encoder_projection\n\n        if not self.encoders:\n            log(""No direct encoder input. Using empty initial state"")\n            return empty_initial_state\n\n        if self._rnn_size is None:\n            log(""No rnn_size or encoder_projection: Using concatenation of ""\n                ""encoded states"")\n            return concat_encoder_projection\n\n        log(""Using linear projection of encoders as the initial state"")\n        return linear_encoder_projection(self.dropout_keep_prob)\n\n    @property\n    def rnn_size(self) -> int:\n        if self._rnn_size is not None:\n            return self._rnn_size\n\n        if self._encoder_projection is None:\n            assert self.encoders\n            return sum(e.output.get_shape()[1].value for e in self.encoders)\n\n        raise ValueError(""Cannot infer RNN size."")\n\n    @tensor\n    def output_projection_spec(self) -> Tuple[OutputProjection, int]:\n        if self._output_projection_spec is None:\n            log(""No output projection specified - using tanh projection"")\n            return (nonlinear_output(self.rnn_size, tf.tanh)[0], self.rnn_size)\n\n        if isinstance(self._output_projection_spec, tuple):\n            return self._output_projection_spec\n\n        return cast(OutputProjection,\n                    self._output_projection_spec), self.rnn_size\n\n    # pylint: disable=unsubscriptable-object\n    @property\n    def output_projection(self) -> OutputProjection:\n        return self.output_projection_spec[0]\n\n    @property\n    def output_dimension(self) -> int:\n        return self.output_projection_spec[1]\n    # pylint: enable=unsubscriptable-object\n\n    @tensor\n    def initial_state(self) -> tf.Tensor:\n        """"""Compute initial decoder state.\n\n        The part of the computation graph that computes\n        the initial state of the decoder.\n        """"""\n        with tf.variable_scope(""initial_state""):\n            # pylint: disable=not-callable\n            initial_state = dropout(\n                self.encoder_projection(self.train_mode,\n                                        self.rnn_size,\n                                        self.encoders),\n                self.dropout_keep_prob,\n                self.train_mode)\n            # pylint: enable=not-callable\n\n            init_state_shape = initial_state.get_shape()\n\n            # Broadcast the initial state to the whole batch if needed\n            if len(init_state_shape) == 1:\n                assert init_state_shape[0].value == self.rnn_size\n                tiles = tf.tile(initial_state,\n                                tf.expand_dims(self.batch_size, 0))\n                initial_state = tf.reshape(tiles, [-1, self.rnn_size])\n\n        return initial_state\n\n    def _get_rnn_cell(self) -> tf.contrib.rnn.RNNCell:\n        return RNN_CELL_TYPES[self._rnn_cell_str](self.rnn_size)\n\n    def _get_conditional_gru_cell(self) -> tf.contrib.rnn.GRUCell:\n        if self._rnn_cell_str == ""NematusGRU"":\n            return NematusGRUCell(\n                self.rnn_size, use_state_bias=True, use_input_bias=False)\n\n        return RNN_CELL_TYPES[self._rnn_cell_str](self.rnn_size)\n\n    def input_plus_attention(self, *args) -> tf.Tensor:\n        """"""Merge input and previous attentions.\n\n        Input and previous attentions are merged into a single vector\n        of the size fo embedding.\n        """"""\n        loop_state = LoopState(*args)\n        feedables = loop_state.feedables\n        emb_with_ctx = tf.concat(\n            [feedables.embedded_input] + feedables.prev_contexts, 1)\n\n        return dropout(\n            tf.layers.dense(emb_with_ctx, self.embedding_size),\n            self.dropout_keep_prob, self.train_mode)\n\n    def next_state(self, loop_state: LoopState) -> Tuple[tf.Tensor, Any, Any]:\n        rnn_feedables = loop_state.feedables.other\n        rnn_histories = loop_state.histories.other\n\n        with tf.variable_scope(self.step_scope):\n            rnn_input = self.input_projection(*loop_state)\n\n            cell = self._get_rnn_cell()\n            if self._rnn_cell_str in [""GRU"", ""NematusGRU""]:\n                cell_output, next_state = cell(\n                    rnn_input, rnn_feedables.prev_rnn_output)\n\n                attns = [\n                    a.attention(\n                        cell_output, rnn_feedables.prev_rnn_output,\n                        rnn_input, att_loop_state)\n                    for a, att_loop_state in zip(\n                        self.attentions,\n                        rnn_histories.attention_histories)]\n                if self.attentions:\n                    contexts, att_loop_states = zip(*attns)\n                else:\n                    contexts, att_loop_states = [], []\n\n                if self._conditional_gru:\n                    cell_cond = self._get_conditional_gru_cell()\n                    cond_input = tf.concat(contexts, -1)\n                    cell_output, next_state = cell_cond(\n                        cond_input, next_state, scope=""cond_gru_2_cell"")\n\n            elif self._rnn_cell_str == ""LSTM"":\n                prev_state = tf.contrib.rnn.LSTMStateTuple(\n                    rnn_feedables.prev_rnn_state,\n                    rnn_feedables.prev_rnn_output)\n                cell_output, state = cell(rnn_input, prev_state)\n                next_state = state.c\n                attns = [\n                    a.attention(\n                        cell_output, rnn_feedables.prev_rnn_output,\n                        rnn_input, att_loop_state)\n                    for a, att_loop_state in zip(\n                        self.attentions,\n                        rnn_histories.attention_histories)]\n                if self.attentions:\n                    contexts, att_loop_states = zip(*attns)\n                else:\n                    contexts, att_loop_states = [], []\n            else:\n                raise ValueError(""Unknown RNN cell."")\n\n            # TODO: attention functions should apply dropout on output\n            #       themselves before returning the tensors\n            contexts = [dropout(ctx, self.dropout_keep_prob, self.train_mode)\n                        for ctx in list(contexts)]\n            cell_output = dropout(\n                cell_output, self.dropout_keep_prob, self.train_mode)\n\n            with tf.name_scope(""rnn_output_projection""):\n                if self.embedding_size != self.output_dimension:\n                    raise ValueError(\n                        ""The dimension ({}) of the output projection must be ""\n                        ""same as the dimension of the input embedding ""\n                        ""({})"".format(self.output_dimension,\n                                      self.embedding_size))\n                # pylint: disable=not-callable\n                output = self.output_projection(\n                    cell_output, loop_state.feedables.embedded_input,\n                    list(contexts), self.train_mode)\n                # pylint: enable=not-callable\n\n        new_feedables = RNNFeedables(\n            prev_rnn_state=next_state,\n            prev_rnn_output=cell_output,\n            prev_contexts=list(contexts))\n\n        new_histories = RNNHistories(\n            rnn_outputs=append_tensor(rnn_histories.rnn_outputs, cell_output),\n            attention_histories=list(att_loop_states))\n\n        return (output, new_feedables, new_histories)\n\n    def get_initial_feedables(self) -> DecoderFeedables:\n        feedables = AutoregressiveDecoder.get_initial_feedables(self)\n\n        rnn_feedables = RNNFeedables(\n            prev_contexts=[tf.zeros([self.batch_size, a.context_vector_size])\n                           for a in self.attentions],\n            prev_rnn_state=self.initial_state,\n            prev_rnn_output=self.initial_state)\n\n        return feedables._replace(other=rnn_feedables)\n\n    def get_initial_histories(self) -> DecoderHistories:\n        histories = AutoregressiveDecoder.get_initial_histories(self)\n\n        rnn_histories = RNNHistories(\n            rnn_outputs=tf.zeros(\n                shape=[0, self.batch_size, self.rnn_size],\n                dtype=tf.float32,\n                name=""hist_rnn_output_states""),\n            attention_histories=[a.initial_loop_state()\n                                 for a in self.attentions if a is not None])\n\n        return histories._replace(other=rnn_histories)\n\n    def finalize_loop(self, final_loop_state: LoopState,\n                      train_mode: bool) -> None:\n        for att_state, attn_obj in zip(\n                final_loop_state.histories.other.attention_histories,\n                self.attentions):\n\n            att_history_key = ""{}_{}"".format(\n                self.name, ""train"" if train_mode else ""run"")\n\n            attn_obj.finalize_loop(att_history_key, att_state)\n\n            if not train_mode:\n                attn_obj.visualize_attention(att_history_key)\n'"
neuralmonkey/decoders/encoder_projection.py,17,"b'""""""Encoder Projection Module.\n\nThis module contains different variants of projection of encoders into the\ninitial state of the decoder.\n\nEncoder projections are specified in the configuration file.  Each encoder\nprojection function has a unified type ``EncoderProjection``, which is a\ncallable that takes three arguments:\n\n1. ``train_mode`` -- boolean tensor specifying whether the train mode is on\n2. ``rnn_size`` -- the size of the resulting initial state\n3. ``encoders`` -- a list of ``Stateful`` objects used as the encoders.\n\nTo enable further parameterization of encoder projection functions, one can\nuse higher-order functions.\n""""""\nfrom typing import List, Callable, cast\n\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.model.stateful import Stateful, TemporalStatefulWithOutput\nfrom neuralmonkey.nn.utils import dropout\nfrom neuralmonkey.nn.ortho_gru_cell import orthogonal_initializer\nfrom neuralmonkey.logging import log\nfrom neuralmonkey.tf_utils import get_initializer\n\n\n# pylint: disable=invalid-name\nEncoderProjection = Callable[\n    [tf.Tensor, int, List[Stateful]], tf.Tensor]\n# pylint: enable=invalid-name\n\n\n# pylint: disable=unused-argument\n# The function must conform the API\ndef empty_initial_state(train_mode: tf.Tensor,\n                        rnn_size: int,\n                        encoders: List[Stateful] = None) -> tf.Tensor:\n    """"""Return an empty vector.""""""\n    if rnn_size is None:\n        raise ValueError(\n            ""You must supply rnn_size for this type of encoder projection"")\n    return tf.zeros([rnn_size])\n\n\ndef linear_encoder_projection(dropout_keep_prob: float) -> EncoderProjection:\n    """"""Return a linear encoder projection.\n\n    Return a projection function which applies dropout on concatenated\n    encoder final states and returns a linear projection to a rnn_size-sized\n    tensor.\n\n    Arguments:\n        dropout_keep_prob: The dropout keep probability\n    """"""\n    check_argument_types()\n\n    def func(train_mode: tf.Tensor,\n             rnn_size: int,\n             encoders: List[Stateful]) -> tf.Tensor:\n\n        if rnn_size is None:\n            raise ValueError(\n                ""You must supply rnn_size for this type of encoder projection"")\n\n        en_concat = concat_encoder_projection(train_mode, None, encoders)\n\n        return dropout(\n            tf.layers.dense(en_concat, rnn_size, name=""encoders_projection""),\n            dropout_keep_prob, train_mode)\n\n    return cast(EncoderProjection, func)\n\n\ndef concat_encoder_projection(\n        train_mode: tf.Tensor,\n        rnn_size: int = None,\n        encoders: List[Stateful] = None) -> tf.Tensor:\n    """"""Concatenate the encoded values of the encoders.""""""\n\n    if encoders is None or not encoders:\n        raise ValueError(""There must be at least one encoder for this type ""\n                         ""of encoder projection"")\n\n    output_size = sum(e.output.get_shape()[1].value for e in encoders)\n    if rnn_size is not None and rnn_size != output_size:\n        raise ValueError(""RNN size supplied for concat projection ({}) does ""\n                         ""not match the size of the concatenated vectors ({}).""\n                         .format(rnn_size, output_size))\n\n    log(""The inferred rnn_size of this encoder projection will be {}""\n        .format(output_size))\n\n    encoded_concat = tf.concat([e.output for e in encoders], 1)\n    return encoded_concat\n\n\ndef nematus_projection(dropout_keep_prob: float = 1.0) -> EncoderProjection:\n    """"""Return encoder projection used in Nematus.\n\n    The initial state is a dense projection with tanh activation computed on\n    the averaged states of the encoders. Dropout is applied to the means\n    (before the projection).\n\n    Arguments:\n        dropout_keep_prob: The dropout keep probability.\n    """"""\n    check_argument_types()\n\n    def func(\n            train_mode: tf.Tensor,\n            rnn_size: int,\n            encoders: List[TemporalStatefulWithOutput]) -> tf.Tensor:\n\n        if len(encoders) != 1:\n            raise ValueError(""Exactly one encoder required for this type of ""\n                             ""projection. {} given."".format(len(encoders)))\n        encoder = encoders[0]\n\n        # shape (batch, time)\n        masked_sum = tf.reduce_sum(\n            encoder.temporal_states\n            * tf.expand_dims(encoder.temporal_mask, 2), 1)\n\n        # shape (batch, 1)\n        lengths = tf.reduce_sum(encoder.temporal_mask, 1, keepdims=True)\n\n        means = masked_sum / lengths\n\n        encoder_rnn_size = means.get_shape()[1].value\n\n        kernel_initializer = orthogonal_initializer()\n        if encoder_rnn_size != rnn_size:\n            kernel_initializer = None\n\n        return dropout(\n            tf.layers.dense(\n                means, rnn_size, activation=tf.tanh,\n                kernel_initializer=get_initializer(\n                    ""encoders_projection/kernel"", kernel_initializer),\n                name=""encoders_projection""),\n            dropout_keep_prob, train_mode)\n\n    return cast(EncoderProjection, func)\n'"
neuralmonkey/decoders/output_projection.py,17,"b'""""""Output Projection Module.\n\nThis module contains different variants of projection functions of decoder\noutputs into the logit function inputs.\n\nOutput projections are specified in the configuration file. Each output\nprojection function has a unified type ``OutputProjection``, which is a\ncallable that takes four arguments and returns a tensor:\n\n1. ``prev_state`` -- the hidden state of the decoder.\n2. ``prev_output`` -- embedding of the previously decoded word (or train input)\n3. ``ctx_tensots`` -- a list of context vectors (for each attention object)\n\nTo enable further parameterization of output projection functions, one can\nuse higher-order functions.\n""""""\nfrom typing import Union, Tuple, List, Callable\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.nn.projection import multilayer_projection, maxout\nfrom neuralmonkey.nn.utils import dropout\nfrom neuralmonkey.tf_utils import get_initializer\n\n\n# pylint: disable=invalid-name\nOutputProjection = Callable[\n    [tf.Tensor, tf.Tensor, List[tf.Tensor], tf.Tensor], tf.Tensor]\n\nOutputProjectionSpec = Union[Tuple[OutputProjection, int],\n                             OutputProjection]\n# pylint: enable=invalid-name\n\n\ndef _legacy_linear(output_size: int) -> Tuple[OutputProjection, int]:\n    """"""Apply a legacy linear projection.\n\n    This was the default projection before commit 9a09553.\n\n    For backward compatibility, set the output_size parameter\n    to decoder\'s rnn_size param.\n    """"""\n    check_argument_types()\n\n    # pylint: disable=unused-argument\n    def _projection(prev_state, prev_output, ctx_tensors, train_mode):\n        state_with_ctx = tf.concat([prev_state] + ctx_tensors, 1)\n        return tf.layers.dense(state_with_ctx, output_size,\n                               name=""AttnOutputProjection"")\n    # pylint: enable=unused-argument\n\n    return _projection, output_size\n\n\ndef _legacy_relu(output_size: int) -> Tuple[OutputProjection, int]:\n    """"""Apply a legacy relu projection.\n\n    This was the default projection after commit 9a09553.\n\n    For backward compatibility, set the output_size parameter\n    to decoder\'s rnn_size param.\n    """"""\n    check_argument_types()\n\n    # pylint: disable=unused-argument\n    def _projection(prev_state, prev_output, ctx_tensors, train_mode):\n        state_with_ctx = tf.concat([prev_state] + ctx_tensors, 1)\n        return tf.layers.dense(state_with_ctx, output_size,\n                               activation=tf.nn.relu,\n                               name=""AttnOutputProjection"")\n    # pylint: enable=unused-argument\n\n    return _projection, output_size\n\n\ndef nematus_output(\n        output_size: int,\n        activation_fn: Callable[[tf.Tensor], tf.Tensor] = tf.tanh,\n        dropout_keep_prob: float = 1.0) -> Tuple[OutputProjection, int]:\n    """"""Apply nonlinear one-hidden-layer deep output.\n\n    Implementation consistent with Nematus.\n    Can be used instead of (and is in theory equivalent to) nonlinear_output.\n\n    Projects the RNN state, embedding of the previously outputted word, and\n    concatenation of all context vectors into a shared vector space, sums them\n    up and apply a hyperbolic tangent activation function.\n    """"""\n    check_argument_types()\n\n    def _projection(prev_state, prev_output, ctx_tensors, train_mode):\n        ctx_concat = tf.concat(ctx_tensors, 1)\n\n        logit_rnn = tf.layers.dense(\n            prev_state, output_size,\n            kernel_initializer=get_initializer(""rnn_state/kernel"", None),\n            name=""rnn_state"")\n\n        logit_emb = tf.layers.dense(\n            prev_output, output_size,\n            kernel_initializer=get_initializer(""prev_out/kernel"", None),\n            name=""prev_out"")\n\n        logit_ctx = tf.layers.dense(\n            ctx_concat, output_size,\n            kernel_initializer=get_initializer(""context/kernel"", None),\n            name=""context"")\n\n        return dropout(activation_fn(logit_rnn + logit_emb + logit_ctx),\n                       dropout_keep_prob, train_mode)\n\n    return _projection, output_size\n\n\ndef nonlinear_output(\n        output_size: int,\n        activation_fn: Callable[[tf.Tensor], tf.Tensor] = tf.tanh,\n        dropout_keep_prob: float = 1.0) -> Tuple[OutputProjection, int]:\n    check_argument_types()\n\n    # pylint: disable=unused-argument\n    def _projection(prev_state, prev_output, ctx_tensors, train_mode):\n        state_out_ctx = tf.concat([prev_state, prev_output] + ctx_tensors, 1)\n        return dropout(\n            tf.layers.dense(\n                state_out_ctx, output_size, activation=activation_fn),\n            dropout_keep_prob, train_mode)\n    # pylint: enable=unused-argument\n\n    return _projection, output_size\n\n\ndef maxout_output(\n        maxout_size: int,\n        dropout_keep_prob: float = 1.0) -> Tuple[OutputProjection, int]:\n    """"""Apply maxout.\n\n    Compute RNN output out of the previous state and output, and the\n    context tensors returned from attention mechanisms, as described\n    in the article\n\n    This function corresponds to the equations for computation the\n    t_tilde in the Bahdanau et al. (2015) paper, on page 14,\n    with the maxout projection, before the last linear projection.\n\n    Arguments:\n        maxout_size: The size of the hidden maxout layer in the deep output\n\n    Returns:\n        Returns the maxout projection of the concatenated inputs\n    """"""\n    check_argument_types()\n\n    def _projection(prev_state, prev_output, ctx_tensors, train_mode):\n        state_out_ctx = tf.concat([prev_state, prev_output] + ctx_tensors, 1)\n        return dropout(\n            maxout(state_out_ctx, maxout_size),\n            dropout_keep_prob, train_mode)\n\n    return _projection, maxout_size\n\n\ndef mlp_output(layer_sizes: List[int],\n               activation: Callable[[tf.Tensor], tf.Tensor] = tf.tanh,\n               dropout_keep_prob: float = 1.0) -> Tuple[OutputProjection, int]:\n    """"""Apply a multilayer perceptron.\n\n    Compute RNN deep output using the multilayer perceptron\n    with a specified activation function.\n    (Pascanu et al., 2013 [https://arxiv.org/pdf/1312.6026v5.pdf])\n\n    Arguments:\n        layer_sizes: A list of sizes of the hiddel layers of the MLP\n        dropout_keep_prob: the dropout keep probability\n        activation: The activation function to use in each layer.\n    """"""\n    check_argument_types()\n\n    def _projection(prev_state, prev_output, ctx_tensors, train_mode):\n        mlp_input = tf.concat([prev_state, prev_output] + ctx_tensors, 1)\n\n        return multilayer_projection(mlp_input, layer_sizes,\n                                     activation=activation,\n                                     dropout_keep_prob=dropout_keep_prob,\n                                     train_mode=train_mode,\n                                     scope=""deep_output_mlp"")\n\n    return _projection, layer_sizes[-1]\n'"
neuralmonkey/decoders/sequence_labeler.py,38,"b'from typing import List, Dict, Callable\n\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.dataset import Dataset\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.model.stateful import TemporalStateful\nfrom neuralmonkey.model.feedable import FeedDict\nfrom neuralmonkey.model.parameterized import InitializerSpecs\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.model.sequence import EmbeddedSequence\nfrom neuralmonkey.nn.utils import dropout\nfrom neuralmonkey.vocabulary import Vocabulary, pad_batch, sentence_mask\n\n\nclass SequenceLabeler(ModelPart):\n    """"""Classifier assing a label to each encoder\'s state.""""""\n\n    # pylint: disable=too-many-arguments,too-many-locals\n    def __init__(self,\n                 name: str,\n                 encoders: List[TemporalStateful],\n                 vocabulary: Vocabulary,\n                 data_id: str,\n                 max_output_len: int = None,\n                 hidden_dim: int = None,\n                 activation: Callable = tf.nn.relu,\n                 dropout_keep_prob: float = 1.0,\n                 add_start_symbol: bool = False,\n                 add_end_symbol: bool = False,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        check_argument_types()\n        ModelPart.__init__(self, name, reuse, save_checkpoint, load_checkpoint,\n                           initializers)\n\n        self.encoders = encoders\n        self.vocabulary = vocabulary\n        self.data_id = data_id\n        self.max_output_len = max_output_len\n        self.hidden_dim = hidden_dim\n        self.activation = activation\n        self.dropout_keep_prob = dropout_keep_prob\n        self.add_start_symbol = add_start_symbol\n        self.add_end_symbol = add_end_symbol\n    # pylint: enable=too-many-arguments,too-many-locals\n\n    @property\n    def input_types(self) -> Dict[str, tf.DType]:\n        return {self.data_id: tf.string}\n\n    @property\n    def input_shapes(self) -> Dict[str, tf.TensorShape]:\n        return {self.data_id: tf.TensorShape([None, None])}\n\n    @tensor\n    def input_mask(self) -> tf.Tensor:\n        mask_main = self.encoders[0].temporal_mask\n\n        asserts = [\n            tf.assert_equal(\n                mask_main, enc.temporal_mask,\n                message=(""Encoders \'{}\' and \'{}\' does not have equal temporal ""\n                         ""masks."".format(str(self.encoders[0]), str(enc))))\n            for enc in self.encoders[1:]]\n\n        with tf.control_dependencies(asserts):\n            return mask_main\n\n    @tensor\n    def target_tokens(self) -> tf.Tensor:\n        return self.dataset[self.data_id]\n\n    @tensor\n    def train_targets(self) -> tf.Tensor:\n        return self.vocabulary.strings_to_indices(\n            self.dataset[self.data_id])\n\n    @tensor\n    def train_mask(self) -> tf.Tensor:\n        return sentence_mask(self.train_targets)\n\n    # TODO(tf-dataset) Uncomment this method after tf-dataset is done\n    # @tensor\n    # def output_mask(self) -> tf.Tensor:\n    #     return tf.cond(\n    #         self.train_mode, lambda: self.train_mask,\n    #         lambda: self.input_mask)\n\n    @tensor\n    def concatenated_inputs(self) -> tf.Tensor:\n        # Validate shapes first\n        with tf.control_dependencies([self.input_mask]):\n            return tf.concat(\n                [inp.temporal_states for inp in self.encoders], axis=2)\n\n    @tensor\n    def states(self) -> tf.Tensor:\n        if self.hidden_dim is None:\n            return self.concatenated_inputs\n        states = tf.layers.dense(\n            self.concatenated_inputs, self.hidden_dim, self.activation,\n            name=""hidden_layer"")\n        return dropout(states, self.dropout_keep_prob, self.train_mode)\n\n    @tensor\n    def logits(self) -> tf.Tensor:\n        return tf.layers.dense(\n            self.states, len(self.vocabulary), name=""logits"")\n\n    @tensor\n    def logprobs(self) -> tf.Tensor:\n        return tf.nn.log_softmax(self.logits)\n\n    @tensor\n    def decoded(self) -> tf.Tensor:\n        return tf.argmax(self.logits, 2)\n\n    @tensor\n    def train_xents(self) -> tf.Tensor:\n        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n            labels=self.train_targets, logits=self.logits)\n\n        # loss is now of shape [batch, time]. Need to mask it now by\n        # element-wise multiplication with weights placeholder\n        return loss * self.train_mask\n\n    @tensor\n    def cost(self) -> tf.Tensor:\n        # Cross entropy mean over all words in the batch\n        # (could also be done as a mean over sentences)\n        return (tf.reduce_sum(self.train_xents)\n                / (tf.reduce_sum(self.train_mask) + 1e-9))\n\n    @property\n    def train_loss(self) -> tf.Tensor:\n        return self.cost\n\n    @property\n    def runtime_loss(self) -> tf.Tensor:\n        return self.cost\n\n    def feed_dict(self, dataset: Dataset, train: bool = False) -> FeedDict:\n        fd = ModelPart.feed_dict(self, dataset, train)\n\n        sentences = dataset.maybe_get_series(self.data_id)\n        if sentences is not None:\n            fd[self.target_tokens] = pad_batch(\n                list(sentences), self.max_output_len, self.add_start_symbol,\n                self.add_end_symbol)\n\n        return fd\n\n\nclass EmbeddingsLabeler(SequenceLabeler):\n    """"""SequenceLabeler that uses an embedding matrix for output projection.""""""\n\n    # pylint: disable=too-many-arguments,too-many-locals\n    def __init__(self,\n                 name: str,\n                 encoders: List[TemporalStateful],\n                 embedded_sequence: EmbeddedSequence,\n                 data_id: str,\n                 max_output_len: int = None,\n                 hidden_dim: int = None,\n                 activation: Callable = tf.nn.relu,\n                 train_embeddings: bool = True,\n                 dropout_keep_prob: float = 1.0,\n                 add_start_symbol: bool = False,\n                 add_end_symbol: bool = False,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n\n        check_argument_types()\n        SequenceLabeler.__init__(\n            self, name, encoders, embedded_sequence.vocabulary, data_id,\n            max_output_len, hidden_dim=hidden_dim, activation=activation,\n            dropout_keep_prob=dropout_keep_prob,\n            add_start_symbol=add_start_symbol, add_end_symbol=add_end_symbol,\n            reuse=reuse, save_checkpoint=save_checkpoint,\n            load_checkpoint=load_checkpoint, initializers=initializers)\n\n        self.embedded_sequence = embedded_sequence\n        self.train_embeddings = train_embeddings\n    # pylint: enable=too-many-arguments,too-many-locals\n\n    @tensor\n    def logits(self) -> tf.Tensor:\n        embeddings = self.embedded_sequence.embedding_matrix\n        if not self.train_embeddings:\n            embeddings = tf.stop_gradient(embeddings)\n\n        states = self.states\n        # pylint: disable=no-member\n        states_dim = self.states.get_shape()[-1].value\n        # pylint: enable=no-member\n        embedding_dim = self.embedded_sequence.embedding_sizes[0]\n        # pylint: disable=redefined-variable-type\n        if states_dim != embedding_dim:\n            states = tf.layers.dense(\n                states, embedding_dim, name=""project_for_embeddings"")\n            states = dropout(states, self.dropout_keep_prob, self.train_mode)\n        # pylint: enable=redefined-variable-type\n\n        reshaped_states = tf.reshape(states, [-1, embedding_dim])\n        reshaped_logits = tf.matmul(\n            reshaped_states, embeddings, transpose_b=True, name=""logits"")\n        return tf.reshape(\n            reshaped_logits, [self.batch_size, -1, len(self.vocabulary)])\n'"
neuralmonkey/decoders/sequence_regressor.py,11,"b'from typing import Callable, Dict, List\n\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.nn.projection import multilayer_projection\nfrom neuralmonkey.dataset import Dataset\nfrom neuralmonkey.model.feedable import FeedDict\nfrom neuralmonkey.model.parameterized import InitializerSpecs\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.model.stateful import Stateful\nfrom neuralmonkey.decorators import tensor\n\n\nclass SequenceRegressor(ModelPart):\n    """"""A simple MLP regression over encoders.\n\n    The API pretends it is an RNN decoder which always generates a sequence of\n    length exactly one.\n    """"""\n\n    # pylint: disable=too-many-arguments\n    def __init__(self,\n                 name: str,\n                 encoders: List[Stateful],\n                 data_id: str,\n                 layers: List[int] = None,\n                 activation_fn: Callable[[tf.Tensor], tf.Tensor] = tf.nn.relu,\n                 dropout_keep_prob: float = 1.0,\n                 dimension: int = 1,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        check_argument_types()\n        ModelPart.__init__(self, name, reuse, save_checkpoint, load_checkpoint,\n                           initializers)\n\n        self.encoders = encoders\n        self.data_id = data_id\n        self.max_output_len = 1\n        self.dimension = dimension\n\n        self._layers = layers\n        self._activation_fn = activation_fn\n        self._dropout_keep_prob = dropout_keep_prob\n    # pylint: enable=too-many-arguments\n\n    @property\n    def input_types(self) -> Dict[str, tf.DType]:\n        return {self.data_id: tf.float32}\n\n    @property\n    def input_shapes(self) -> Dict[str, tf.TensorShape]:\n        return {self.data_id: tf.TensorShape([None])}\n\n    @tensor\n    def train_inputs(self) -> tf.Tensor:\n        return self.dataset[self.data_id]\n\n    @tensor\n    def _mlp_input(self):\n        return tf.concat([enc.output for enc in self.encoders], 1)\n\n    @tensor\n    def _mlp_output(self):\n        return multilayer_projection(\n            self._mlp_input, self._layers, self.train_mode,\n            self._activation_fn, self._dropout_keep_prob)\n\n    @tensor\n    def predictions(self):\n        return tf.layers.dense(\n            self._mlp_output, self.dimension, name=""output_projection"")\n\n    @tensor\n    def cost(self):\n        cost = tf.reduce_mean(tf.square(\n            self.predictions - tf.expand_dims(self.train_inputs, 1)))\n\n        tf.summary.scalar(""optimization_cost"", cost,\n                          collections=[""summary_val"", ""summary_train""])\n\n        return cost\n\n    @property\n    def train_loss(self):\n        return self.cost\n\n    @property\n    def runtime_loss(self):\n        return self.cost\n\n    @property\n    def decoded(self):\n        return self.predictions\n\n    def feed_dict(self, dataset: Dataset, train: bool = False) -> FeedDict:\n        fd = ModelPart.feed_dict(self, dataset, train)\n\n        sentences = dataset.maybe_get_series(self.data_id)\n        sentences_list = list(sentences) if sentences is not None else None\n        if sentences_list is not None:\n            fd[self.train_inputs] = list(zip(*sentences_list))[0]\n\n        return fd\n'"
neuralmonkey/decoders/transformer.py,46,"b'""""""Implementation of the decoder of the Transformer model.\n\nDescribed in Vaswani et al. (2017), arxiv.org/abs/1706.03762\n""""""\n# TODO make this code simpler\n# pylint: disable=too-many-lines\nfrom typing import Any, Callable, NamedTuple, List, Union, Tuple\nimport math\n\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.attention.scaled_dot_product import attention\nfrom neuralmonkey.attention.base_attention import (\n    Attendable, get_attention_states, get_attention_mask)\nfrom neuralmonkey.attention.transformer_cross_layer import (\n    serial, parallel, flat, hierarchical)\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.decoders.autoregressive import (\n    AutoregressiveDecoder, LoopState, DecoderFeedables, DecoderHistories)\nfrom neuralmonkey.encoders.transformer import (\n    TransformerLayer, position_signal)\nfrom neuralmonkey.logging import warn\nfrom neuralmonkey.model.sequence import EmbeddedSequence\nfrom neuralmonkey.model.parameterized import InitializerSpecs\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.nn.utils import dropout\nfrom neuralmonkey.vocabulary import (\n    Vocabulary, END_TOKEN_INDEX)\nfrom neuralmonkey.tf_utils import append_tensor, layer_norm\n\n\nSTRATEGIES = [""serial"", ""parallel"", ""flat"", ""hierarchical""]\n\n\nclass TransformerFeedables(NamedTuple(\n        ""TransformerFeedables"",\n        [(""input_sequence"", tf.Tensor),\n         (""input_mask"", tf.Tensor)])):\n    """"""Additional feedables used only by the Transformer-based decoder.\n\n    Follows the shape pattern of having batch_sized first dimension\n    shape(batch_size, ...)\n\n    Attributes:\n        input_sequence: The whole input sequence (embedded) that is fed into\n            the decoder in each decoding step.\n            shape(batch, len, emb)\n        input_mask: Mask for masking finished sequences. The last dimension\n            is required for compatibility with the beam_search_decoder.\n            shape(batch, len, 1)\n    """"""\n\n\nclass TransformerHistories(NamedTuple(\n        ""TransformerHistories"",\n        [(""self_attention_histories"", List[Tuple]),\n         (""encoder_attention_histories"", List[Tuple])])):\n    """"""The loop state histories specific for Transformer-based decoders.\n\n    Attributes:\n        self_attention_histories: A list of ``MultiHeadLoopState`` objects\n            populated by values from the self-attention mechanisms used\n            in the decoder.\n        encoder_attention_histories: A list of ``MultiHeadLoopState`` objects\n            populated by values from the encoder-attention mechanisms used\n            in the decoder.\n    """"""\n\n\n# pylint: disable=too-many-instance-attributes\nclass TransformerDecoder(AutoregressiveDecoder):\n\n    # pylint: disable=too-many-arguments,too-many-locals,too-many-branches\n    def __init__(self,\n                 name: str,\n                 encoders: List[Attendable],\n                 vocabulary: Vocabulary,\n                 data_id: str,\n                 # TODO infer the default for these three from the encoder\n                 ff_hidden_size: int,\n                 n_heads_self: int,\n                 n_heads_enc: Union[List[int], int],\n                 depth: int,\n                 max_output_len: int,\n                 attention_combination_strategy: str = ""serial"",\n                 n_heads_hier: int = None,\n                 dropout_keep_prob: float = 1.0,\n                 embedding_size: int = None,\n                 embeddings_source: EmbeddedSequence = None,\n                 tie_embeddings: bool = True,\n                 label_smoothing: float = None,\n                 self_attention_dropout_keep_prob: float = 1.0,\n                 attention_dropout_keep_prob: Union[float, List[float]] = 1.0,\n                 use_att_transform_bias: bool = False,\n                 supress_unk: bool = False,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        """"""Create a decoder of the Transformer model.\n\n        Described in Vaswani et al. (2017), arxiv.org/abs/1706.03762\n\n        Arguments:\n            encoders: Input encoders for the decoder.\n            vocabulary: Target vocabulary.\n            data_id: Target data series.\n            name: Name of the decoder. Should be unique accross all Neural\n                Monkey objects.\n            max_output_len: Maximum length of an output sequence.\n            dropout_keep_prob: Probability of keeping a value during dropout.\n            embedding_size: Size of embedding vectors for target words.\n            embeddings_source: Embedded sequence to take embeddings from.\n            tie_embeddings: Use decoder.embedding_matrix also in place\n                of the output decoding matrix.\n            ff_hidden_size: Size of the feedforward sublayers.\n            n_heads_self: Number of the self-attention heads.\n            n_heads_enc: Number of the attention heads over each encoder.\n                Either a list which size must be equal to ``encoders``, or a\n                single integer. In the latter case, the number of heads is\n                equal for all encoders.\n            attention_comnbination_strategy: One of ``serial``, ``parallel``,\n                ``flat``, ``hierarchical``. Controls the attention combination\n                strategy for enc-dec attention.\n            n_heads_hier: Number of the attention heads for the second\n                attention in the ``hierarchical`` attention combination.\n            depth: Number of sublayers.\n            label_smoothing: A label smoothing parameter for cross entropy\n                loss computation.\n            attention_dropout_keep_prob: Probability of keeping a value\n                during dropout on the attention output.\n            supress_unk: If true, decoder will not produce symbols for unknown\n                tokens.\n            reuse: Reuse the variables from the given model part.\n        """"""\n        check_argument_types()\n        AutoregressiveDecoder.__init__(\n            self,\n            name=name,\n            vocabulary=vocabulary,\n            data_id=data_id,\n            max_output_len=max_output_len,\n            dropout_keep_prob=dropout_keep_prob,\n            embedding_size=embedding_size,\n            embeddings_source=embeddings_source,\n            tie_embeddings=tie_embeddings,\n            label_smoothing=label_smoothing,\n            supress_unk=supress_unk,\n            reuse=reuse,\n            save_checkpoint=save_checkpoint,\n            load_checkpoint=load_checkpoint)\n\n        self.encoders = encoders\n        self.ff_hidden_size = ff_hidden_size\n        self.n_heads_self = n_heads_self\n\n        if isinstance(n_heads_enc, int):\n            if attention_combination_strategy == ""flat"":\n                self.n_heads_enc = [n_heads_enc]\n            else:\n                self.n_heads_enc = [n_heads_enc for _ in self.encoders]\n        else:\n            self.n_heads_enc = n_heads_enc\n\n        self.depth = depth\n        if isinstance(attention_dropout_keep_prob, float):\n            self.attention_dropout_keep_prob = [\n                attention_dropout_keep_prob for _ in encoders]\n        else:\n            self.attention_dropout_keep_prob = attention_dropout_keep_prob\n        self.self_att_dropout_keep_prob = self_attention_dropout_keep_prob\n        self.use_att_transform_bias = use_att_transform_bias\n        self.attention_combination_strategy = attention_combination_strategy\n        self.n_heads_hier = n_heads_hier\n\n        self.encoder_states = lambda: [get_attention_states(e)\n                                       for e in self.encoders]\n        self.encoder_masks = lambda: [get_attention_mask(e)\n                                      for e in self.encoders]\n\n        if self.attention_combination_strategy not in STRATEGIES:\n            raise ValueError(\n                ""Unknown attention combination strategy \'{}\'. ""\n                ""Allowed: {}."".format(self.attention_combination_strategy,\n                                      "", "".join(STRATEGIES)))\n\n        if (self.attention_combination_strategy == ""hierarchical""\n                and self.n_heads_hier is None):\n            raise ValueError(\n                ""You must provide n_heads_hier when using the hierarchical ""\n                ""attention combination strategy."")\n\n        if (self.attention_combination_strategy != ""hierarchical""\n                and self.n_heads_hier is not None):\n            warn(""Ignoring n_heads_hier parameter -- use the hierarchical ""\n                 ""attention combination strategy instead."")\n\n        if (self.attention_combination_strategy == ""flat""\n                and len(self.n_heads_enc) != 1):\n            raise ValueError(\n                ""For the flat attention combination strategy, only a single ""\n                ""value is permitted in n_heads_enc."")\n\n        self._variable_scope.set_initializer(tf.variance_scaling_initializer(\n            mode=""fan_avg"", distribution=""uniform""))\n    # pylint: enable=too-many-arguments,too-many-locals,too-many-branches\n\n    @property\n    def dimension(self) -> int:\n        enc_states = self.encoder_states()\n\n        if enc_states:\n            first_dim = enc_states[0].get_shape()[2].value  # type: int\n\n            for i, states in enumerate(enc_states):\n                enc_dim = states.get_shape()[2].value\n                if enc_dim != first_dim:\n                    raise ValueError(\n                        ""Dimension of the {}-th encoder ({}) differs from the ""\n                        ""dimension of the first one ({}).""\n                        .format(i, enc_dim, first_dim))\n\n            if self.embedding_size is not None:\n                if self.embedding_size != first_dim:\n                    raise ValueError(""Model dimension and input embedding ""\n                                     ""size do not match"")\n            return first_dim\n\n        if self.embedding_size is None:\n            raise ValueError(""\'embedding_size\' must be specified when ""\n                             ""no encoders are provided"")\n\n        return self.embedding_size\n\n    @property\n    def output_dimension(self) -> int:\n        return self.dimension\n\n    def embed_input_symbol(self, inputs: tf.Tensor) -> tf.Tensor:\n        embedded = tf.nn.embedding_lookup(self.embedding_matrix, inputs)\n\n        if (self.embeddings_source is not None\n                and self.embeddings_source.scale_embeddings_by_depth):\n\n            # Pylint @property-related bug\n            # pylint: disable=no-member\n            embedding_size = self.embedding_matrix.shape.as_list()[-1]\n            # pylint: enable=no-member\n\n            embedded *= math.sqrt(embedding_size)\n\n        length = tf.shape(inputs)[1]\n        return dropout(embedded + position_signal(self.dimension, length),\n                       self.dropout_keep_prob,\n                       self.train_mode)\n\n    @tensor\n    def train_input_symbols(self) -> tf.Tensor:\n        # THE LAST TRAIN INPUT IS NOT USED IN DECODING FUNCTION\n        # (just as a target)\n\n        # shape (batch, 1 + (time - 1))\n        # pylint: disable=unsubscriptable-object\n        return tf.concat(\n            [tf.expand_dims(self.go_symbols, 1),\n             tf.transpose(self.train_inputs[:-1])], 1)\n        # pylint: enable=unsubscriptable-object\n\n    def self_attention_sublayer(\n            self, prev_layer: TransformerLayer) -> tf.Tensor:\n        """"""Create the decoder self-attention sublayer with output mask.""""""\n\n        # Layer normalization\n        normalized_states = layer_norm(prev_layer.temporal_states)\n\n        # Run self-attention\n        # TODO handle attention histories\n        self_context, _ = attention(\n            queries=normalized_states,\n            keys=normalized_states,\n            values=normalized_states,\n            keys_mask=prev_layer.temporal_mask,\n            num_heads=self.n_heads_self,\n            masked=True,\n            dropout_callback=lambda x: dropout(\n                x, self.self_att_dropout_keep_prob, self.train_mode),\n            use_bias=self.use_att_transform_bias)\n\n        # Apply dropout\n        self_context = dropout(\n            self_context, self.dropout_keep_prob, self.train_mode)\n\n        # Add residual connections\n        return self_context + prev_layer.temporal_states\n\n    def encoder_attention_sublayer(self, queries: tf.Tensor) -> tf.Tensor:\n        """"""Create the encoder-decoder attention sublayer.""""""\n        enc_states = self.encoder_states()\n        enc_masks = self.encoder_masks()\n        assert enc_states is not None\n        assert enc_masks is not None\n\n        # Attention dropout callbacks are created in a loop so we need to\n        # use a factory function to prevent late binding.\n        def make_attn_callback(\n                prob: float) -> Callable[[tf.Tensor], tf.Tensor]:\n            def callback(x: tf.Tensor) -> tf.Tensor:\n                return dropout(x, prob, self.train_mode)\n            return callback\n\n        dropout_cb = make_attn_callback(self.dropout_keep_prob)\n        attn_dropout_cbs = [make_attn_callback(prob)\n                            for prob in self.attention_dropout_keep_prob]\n\n        if self.attention_combination_strategy == ""serial"":\n            return serial(queries, enc_states, enc_masks, self.n_heads_enc,\n                          attn_dropout_cbs, dropout_cb)\n\n        if self.attention_combination_strategy == ""parallel"":\n            return parallel(queries, enc_states, enc_masks, self.n_heads_enc,\n                            attn_dropout_cbs, dropout_cb)\n\n        if self.attention_combination_strategy == ""flat"":\n            assert len(set(self.n_heads_enc)) == 1\n            assert len(set(self.attention_dropout_keep_prob)) == 1\n\n            return flat(queries, enc_states, enc_masks, self.n_heads_enc[0],\n                        attn_dropout_cbs[0], dropout_cb)\n\n        if self.attention_combination_strategy == ""hierarchical"":\n            assert self.n_heads_hier is not None\n\n            return hierarchical(\n                queries, enc_states, enc_masks, self.n_heads_enc,\n                self.n_heads_hier, attn_dropout_cbs, dropout_cb)\n\n        raise NotImplementedError(\n            ""Unknown attention combination strategy: {}""\n            .format(self.attention_combination_strategy))\n\n    def feedforward_sublayer(self, layer_input: tf.Tensor) -> tf.Tensor:\n        """"""Create the feed-forward network sublayer.""""""\n\n        # Layer normalization\n        normalized_input = layer_norm(layer_input)\n\n        # Feed-forward network hidden layer + ReLU\n        ff_hidden = tf.layers.dense(\n            normalized_input, self.ff_hidden_size, activation=tf.nn.relu,\n            name=""hidden_state"")\n\n        # Apply dropout on the activations\n        ff_hidden = dropout(ff_hidden, self.dropout_keep_prob, self.train_mode)\n\n        # Feed-forward output projection\n        ff_output = tf.layers.dense(ff_hidden, self.dimension, name=""output"")\n\n        # Apply dropout on the output projection\n        ff_output = dropout(ff_output, self.dropout_keep_prob, self.train_mode)\n\n        # Add residual connections\n        return ff_output + layer_input\n\n    def layer(self, level: int, inputs: tf.Tensor,\n              mask: tf.Tensor) -> TransformerLayer:\n        # Recursive implementation. Outputs of the zeroth layer\n        # are the inputs\n\n        if level == 0:\n            return TransformerLayer(inputs, mask)\n\n        # Compute the outputs of the previous layer\n        prev_layer = self.layer(level - 1, inputs, mask)\n\n        with tf.variable_scope(""layer_{}"".format(level - 1)):\n\n            with tf.variable_scope(""self_attention""):\n                self_context = self.self_attention_sublayer(prev_layer)\n\n            with tf.variable_scope(""encdec_attention""):\n                encoder_context = self.encoder_attention_sublayer(self_context)\n\n            with tf.variable_scope(""feedforward""):\n                output_states = self.feedforward_sublayer(encoder_context)\n\n        # Layer normalization on the decoder output\n        if self.depth == level:\n            output_states = layer_norm(output_states)\n\n        return TransformerLayer(states=output_states, mask=mask)\n\n    @tensor\n    def train_loop_result(self) -> LoopState:\n        # We process all decoding the steps together during training.\n        # However, we still want to pretend that a proper decoding_loop\n        # was called.\n        decoder_ls = AutoregressiveDecoder.get_initial_loop_state(self)\n\n        input_sequence = self.embed_input_symbols(self.train_input_symbols)\n        input_mask = tf.transpose(self.train_mask)\n\n        last_layer = self.layer(\n            self.depth, input_sequence, input_mask)\n\n        tr_feedables = TransformerFeedables(\n            input_sequence=input_sequence,\n            input_mask=tf.expand_dims(input_mask, -1))\n\n        # t_states shape: (batch, time, channels)\n        # dec_w shape: (channels, vocab)\n        last_layer_shape = tf.shape(last_layer.temporal_states)\n        last_layer_states = tf.reshape(\n            last_layer.temporal_states,\n            [-1, last_layer_shape[-1]])\n\n        # shape (batch, time, vocab)\n        logits = tf.reshape(\n            tf.matmul(last_layer_states, self.decoding_w),\n            [last_layer_shape[0], last_layer_shape[1], len(self.vocabulary)])\n        logits += tf.reshape(self.decoding_b, [1, 1, -1])\n\n        # TODO: record histories properly\n        tr_histories = tf.zeros([])\n        # tr_histories = TransformerHistories(\n        #    self_attention_histories=[\n        #        empty_multi_head_loop_state(self.batch_size,\n        #                                    self.n_heads_self)\n        #        for a in range(self.depth)],\n        #    encoder_attention_histories=[\n        #        empty_multi_head_loop_state(self.batch_size,\n        #                                    self.n_heads_enc)\n        #        for a in range(self.depth)])\n\n        feedables = DecoderFeedables(\n            step=last_layer_shape[1],\n            finished=tf.ones([self.batch_size], dtype=tf.bool),\n            embedded_input=self.embed_input_symbols(tf.tile(\n                [END_TOKEN_INDEX], [self.batch_size])),\n            other=tr_feedables)\n\n        histories = DecoderHistories(\n            logits=tf.transpose(logits, perm=[1, 0, 2]),\n            output_states=tf.transpose(\n                last_layer.temporal_states, [1, 0, 2]),\n            output_mask=self.train_mask,\n            output_symbols=self.train_inputs,\n            other=tr_histories)\n\n        return LoopState(\n            feedables=feedables,\n            histories=histories,\n            constants=decoder_ls.constants)\n\n    def get_initial_feedables(self) -> DecoderFeedables:\n        feedables = AutoregressiveDecoder.get_initial_feedables(self)\n\n        tr_feedables = TransformerFeedables(\n            input_sequence=tf.zeros(\n                shape=[self.batch_size, 0, self.dimension],\n                dtype=tf.float32,\n                name=""input_sequence""),\n            input_mask=tf.zeros(\n                shape=[self.batch_size, 0, 1],\n                dtype=tf.float32,\n                name=""input_mask""))\n\n        return feedables._replace(other=tr_feedables)\n\n    def get_initial_histories(self) -> DecoderHistories:\n        histories = AutoregressiveDecoder.get_initial_histories(self)\n\n        # TODO: record histories properly\n        tr_histories = tf.zeros([])\n        # tr_histories = TransformerHistories(\n        #    self_attention_histories=[\n        #        empty_multi_head_loop_state(self.batch_size,\n        #                                    self.n_heads_self)\n        #        for a in range(self.depth)],\n        #    encoder_attention_histories=[\n        #        empty_multi_head_loop_state(self.batch_size,\n        #                                    self.n_heads_enc)\n        #        for a in range(self.depth)])\n\n        return histories._replace(other=tr_histories)\n\n    def next_state(self, loop_state: LoopState) -> Tuple[tf.Tensor, Any, Any]:\n        feedables = loop_state.feedables\n        tr_feedables = feedables.other\n        tr_histories = loop_state.histories.other\n\n        with tf.variable_scope(self._variable_scope, reuse=tf.AUTO_REUSE):\n            # shape (time, batch)\n            input_sequence = append_tensor(\n                tr_feedables.input_sequence, feedables.embedded_input, 1)\n\n            unfinished_mask = tf.to_float(tf.logical_not(feedables.finished))\n            input_mask = append_tensor(\n                tr_feedables.input_mask,\n                tf.expand_dims(unfinished_mask, -1),\n                axis=1)\n\n            last_layer = self.layer(\n                self.depth, input_sequence, tf.squeeze(input_mask, -1))\n\n            # (batch, state_size)\n            output_state = last_layer.temporal_states[:, -1, :]\n\n        new_feedables = TransformerFeedables(\n            input_sequence=input_sequence,\n            input_mask=input_mask)\n\n        # TODO: do something more interesting here\n        new_histories = tr_histories\n\n        return (output_state, new_feedables, new_histories)\n# pylint: enable=too-many-instance-attributes\n'"
neuralmonkey/decoders/word_alignment_decoder.py,19,"b'# TODO untested module\nfrom typing import cast, Dict, Tuple\n\nimport numpy as np\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.dataset import Dataset\nfrom neuralmonkey.encoders.recurrent import RecurrentEncoder\nfrom neuralmonkey.decoders.decoder import Decoder\nfrom neuralmonkey.logging import warn\nfrom neuralmonkey.model.feedable import FeedDict\nfrom neuralmonkey.model.parameterized import InitializerSpecs\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.model.sequence import Sequence\nfrom neuralmonkey.decorators import tensor\n\n\nclass WordAlignmentDecoder(ModelPart):\n    """"""A decoder that computes soft alignment from an attentive encoder.\n\n    Loss is computed as cross-entropy against a reference alignment.\n    """"""\n\n    def __init__(self,\n                 encoder: RecurrentEncoder,\n                 decoder: Decoder,\n                 data_id: str,\n                 name: str,\n                 reuse: ModelPart = None,\n                 initializers: InitializerSpecs = None) -> None:\n        check_argument_types()\n        ModelPart.__init__(self, name, reuse, None, None, initializers)\n\n        self.encoder = encoder\n        self.decoder = decoder\n        self.data_id = data_id\n\n    @property\n    def enc_input(self) -> Sequence:\n        if not isinstance(self.encoder.input_sequence, Sequence):\n            raise TypeError(""Expected Sequence type in encoder.input_sequence"")\n\n        return cast(Sequence, self.encoder.input_sequence)\n\n    @property\n    def input_types(self) -> Dict[str, tf.DType]:\n        return {self.data_id: tf.float32}\n\n    @property\n    def input_shapes(self) -> Dict[str, tf.TensorShape]:\n        return {self.data_id: tf.TensorShape(\n            [None, self.decoder.max_output_len, self.enc_input.max_length])}\n\n    @tensor\n    def ref_alignment(self) -> tf.Tensor:\n        return self.dataset[self.data_id]\n\n    @tensor\n    def alignment_target(self) -> tf.Tensor:\n        # shape will be [max_output_len, batch_size, max_input_len]\n        return tf.transpose(self.ref_alignment, perm=[1, 0, 2])\n\n    @tensor\n    def train_loss(self) -> tf.Tensor:\n        loss = self._make_decoder(runtime_mode=False)\n        tf.summary.scalar(\n            ""alignment_train_xent"", loss, collections=[""summary_train""])\n\n        return loss\n\n    # pylint: disable=unsubscriptable-object\n    # Bug in pylint\n    @tensor\n    def decoded(self) -> tf.Tensor:\n        return self.runtime_outputs[0]\n\n    @tensor\n    def runtime_loss(self) -> tf.Tensor:\n        return self.runtime_outputs[1]\n    # pylint: enable=unsubscriptable-object\n\n    @tensor\n    def runtime_outputs(self) -> Tuple[tf.Tensor, tf.Tensor]:\n        return self._make_decoder(runtime_mode=True)\n\n    def _make_decoder(self, runtime_mode=False):\n        attn_obj = self.decoder.get_attention_object(self.encoder,\n                                                     not runtime_mode)\n        if runtime_mode:\n            alignment_logits = tf.stack(\n                attn_obj.histories[""{}_run"".format(\n                    self.decoder.name)],\n                name=""alignment_logits"")\n            # make batch_size the first dimension\n            alignment = tf.transpose(tf.nn.softmax(alignment_logits),\n                                     perm=[1, 0, 2])\n            loss = tf.constant(0)\n        else:\n            alignment_logits = tf.stack(\n                attn_obj.histories[""{}_train"".format(\n                    self.decoder.name)],\n                name=""alignment_logits"")\n            alignment = None\n\n            xent = tf.nn.softmax_cross_entropy_with_logits(\n                labels=self.alignment_target, logits=alignment_logits)\n            loss = tf.reduce_sum(xent * self.decoder.train_padding)\n\n        return alignment, loss\n\n    @property\n    def cost(self) -> tf.Tensor:\n        return self.train_loss\n\n    def feed_dict(self, dataset: Dataset, train: bool = False) -> FeedDict:\n        fd = ModelPart.feed_dict(self, dataset, train)\n\n        alignment = dataset.maybe_get_series(self.data_id)\n        if alignment is None:\n            if train:\n                warn(""Training alignment not present!"")\n\n            alignment = np.zeros((len(dataset),\n                                  self.decoder.max_output_len,\n                                  self.enc_input.max_length),\n                                 np.float32)\n\n        fd[self.ref_alignment] = alignment\n\n        return fd\n'"
neuralmonkey/encoders/__init__.py,0,b'from .cnn_encoder import CNNEncoder\nfrom .cnn_encoder import CNNTemporalView\nfrom .recurrent import FactoredEncoder\nfrom .recurrent import RecurrentEncoder\nfrom .recurrent import SentenceEncoder\nfrom .sentence_cnn_encoder import SentenceCNNEncoder\nfrom .sequence_cnn_encoder import SequenceCNNEncoder\n'
neuralmonkey/encoders/attentive.py,16,"b'import tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.model.stateful import TemporalStatefulWithOutput\nfrom neuralmonkey.model.parameterized import InitializerSpecs\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.nn.utils import dropout\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.attention.base_attention import (\n    get_attention_states, get_attention_mask, Attendable)\n\n\nclass AttentiveEncoder(ModelPart, TemporalStatefulWithOutput):\n    """"""An encoder with attention over the input and a fixed-dimension output.\n\n    Based on ""A Structured Self-attentive Sentence Embedding"",\n    https://arxiv.org/abs/1703.03130.\n\n    The encoder combines a sequence of vectors into a fixed-size matrix where\n    each row of the matrix is computed using a different attention head. This\n    matrix is exposed as the ``temporal_states`` property (the time dimension\n    corresponds to the different attention heads). The ``output`` property\n    provides a flattened and, optionally, projected representation of this\n    matrix.\n    """"""\n\n    # pylint: disable=too-many-arguments\n    def __init__(self,\n                 name: str,\n                 input_sequence: Attendable,\n                 hidden_size: int,\n                 num_heads: int,\n                 output_size: int = None,\n                 state_proj_size: int = None,\n                 dropout_keep_prob: float = 1.0,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        """"""Initialize an instance of the encoder.""""""\n        check_argument_types()\n        ModelPart.__init__(self, name, reuse, save_checkpoint, load_checkpoint,\n                           initializers)\n\n        self.input_sequence = input_sequence\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.output_size = output_size\n        self.state_proj_size = state_proj_size\n        self.dropout_keep_prob = dropout_keep_prob\n\n        if self.dropout_keep_prob <= 0.0 or self.dropout_keep_prob > 1.0:\n            raise ValueError(""Dropout keep prob must be inside (0,1]."")\n\n    @tensor\n    def _attention_states_dropped(self) -> tf.Tensor:\n        return dropout(get_attention_states(self.input_sequence),\n                       self.dropout_keep_prob, self.train_mode)\n\n    @tensor\n    def attention_weights(self) -> tf.Tensor:\n        mask = get_attention_mask(self.input_sequence)\n        hidden = tf.layers.dense(self._attention_states_dropped,\n                                 units=self.hidden_size,\n                                 activation=tf.tanh, use_bias=False,\n                                 name=""S1"")\n        energies = tf.layers.dense(hidden, units=self.num_heads,\n                                   use_bias=False, name=""S2"")\n        # shape: [batch_size, max_time, num_heads]\n        weights = tf.nn.softmax(energies, axis=1)\n        if mask is not None:\n            weights *= tf.expand_dims(mask, -1)\n            weights /= tf.reduce_sum(weights, axis=1, keepdims=True) + 1e-8\n\n        return weights\n\n    @tensor\n    def temporal_states(self) -> tf.Tensor:\n        states = self._attention_states_dropped\n        if self.state_proj_size is not None:\n            # pylint: disable=redefined-variable-type\n            # pylint property-related bug\n            states = tf.layers.dense(states, units=self.state_proj_size,\n                                     name=""state_projection"")\n            # pylint: enable=redefined-variable-type\n\n        return tf.matmul(a=self.attention_weights, b=states, transpose_a=True)\n\n    @tensor\n    def temporal_mask(self) -> tf.Tensor:\n        return tf.ones(tf.shape(self.temporal_states)[:2], tf.float32)\n\n    @tensor\n    def output(self) -> tf.Tensor:\n        # pylint: disable=no-member\n        state_size = self.temporal_states.get_shape()[2].value\n        # pylint: enable=no-member\n        output = tf.reshape(self.temporal_states,\n                            [-1, self.num_heads * state_size])\n        if self.output_size is not None:\n            output = tf.layers.dense(output, self.output_size,\n                                     name=""output_projection"")\n\n        return output\n'"
neuralmonkey/encoders/cnn_encoder.py,52,"b'""""""CNN for image processing.""""""\n\nfrom typing import cast, Callable, Dict, List, Tuple, Union\nfrom typeguard import check_argument_types\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom neuralmonkey.dataset import Dataset\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.model.feedable import FeedDict\nfrom neuralmonkey.model.parameterized import InitializerSpecs\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.model.stateful import (SpatialStatefulWithOutput,\n                                         TemporalStatefulWithOutput)\nfrom neuralmonkey.nn.projection import multilayer_projection\n\n\n# Tuples used for configuration of the convolutional layers. See docstring of\n# CNNEncoder initialization for more details.\n# pylint: disable=invalid-name\nConvSpec = Tuple[str, int, int, str, int]\nResNetSpec = Tuple[str, int, int]\nMaxPoolSpec = Tuple[str, int, int, str]\n# pylint: enable=invalid-name\n\n\nclass CNNEncoder(ModelPart, SpatialStatefulWithOutput):\n    """"""An image encoder.\n\n    It projects the input image through a serie of convolutioal operations. The\n    projected image is vertically cut and fed to stacked RNN layers which\n    encode the image into a single vector.\n    """"""\n\n    # pylint: disable=too-many-arguments, too-many-locals\n    def __init__(self,\n                 name: str,\n                 data_id: str,\n                 convolutions: List[Union[ConvSpec, ResNetSpec, MaxPoolSpec]],\n                 image_height: int, image_width: int, pixel_dim: int,\n                 fully_connected: List[int] = None,\n                 batch_normalize: bool = False,\n                 dropout_keep_prob: float = 0.5,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        """"""Initialize a convolutional network for image processing.\n\n        The convolutional network can consist of plain convolutions,\n        max-pooling layers and residual block. In the configuration, they are\n        specified using the following tuples.\n\n            * convolution: (""C"", kernel_size, stride, padding, out_channel);\n            * max / average pooling: (""M""/""A"", kernel_size, stride, padding);\n            * residual block: (""R"", kernel_size, out_channels).\n\n        Padding must be either ""valid"" or ""same"".\n\n        Args:\n            convolutions: Configuration of convolutional layers.\n            data_id: Identifier of the data series in the dataset.\n            image_height: Height of the input image in pixels.\n            image_width: Width of the image.\n            pixel_dim: Number of color channels in the input images.\n            dropout_keep_prob: Probability of keeping neurons active in\n                dropout. Dropout is done between all convolutional layers and\n                fully connected layer.\n        """"""\n        check_argument_types()\n        ModelPart.__init__(\n            self, name, reuse, save_checkpoint, load_checkpoint, initializers)\n\n        self.data_id = data_id\n        self.dropout_keep_prob = dropout_keep_prob\n\n        self.image_height = image_height\n        self.image_width = image_width\n        self.pixel_dim = pixel_dim\n        self.convolutions = convolutions\n        self.fully_connected = fully_connected\n        self.batch_normalize = batch_normalize\n    # pylint: enable=too-many-arguments, too-many-locals\n\n    @property\n    def input_types(self) -> Dict[str, tf.DType]:\n        return {self.data_id: tf.float32}\n\n    @property\n    def input_shapes(self) -> Dict[str, tf.TensorShape]:\n        return {self.data_id: tf.TensorShape(\n            [None, self.image_height, self.image_width, self.pixel_dim])}\n\n    @tensor\n    def image_input(self) -> tf.Tensor:\n        return self.dataset[self.data_id]\n\n    @tensor\n    def image_mask(self) -> tf.Tensor:\n        # the image mask is one everywhere where the image is non-zero, i.e.\n        # zero pixels are masked out\n        return tf.sign(tf.reduce_sum(self.image_input, axis=3, keepdims=True))\n\n    def batch_norm_callback(self, layer_output: tf.Tensor) -> tf.Tensor:\n        if self.batch_normalize:\n            return tf.layers.batch_normalization(\n                layer_output, training=self.train_mode)\n        return layer_output\n\n    @tensor\n    def image_processing_layers(self) -> List[Tuple[tf.Tensor, tf.Tensor]]:\n        """"""Do all convolutions and return the last conditional map.\n\n        No dropout is applied between the convolutional layers. By default, the\n        activation function is ReLU.\n        """"""\n        last_layer = self.image_input\n        last_mask = self.image_mask\n        last_channels = self.pixel_dim\n        image_processing_layers = []  # type: List[Tuple[tf.Tensor, tf.Tensor]]\n\n        with tf.variable_scope(""convolutions""):\n            for i, specification in enumerate(self.convolutions):\n                if specification[0] == ""C"":\n                    (last_layer, last_mask,\n                     last_channels) = plain_convolution(\n                         last_layer, last_mask,\n                         cast(ConvSpec, specification),\n                         self.batch_norm_callback, i)\n                    image_processing_layers.append((last_layer, last_mask))\n                elif specification[0] in [""M"", ""A""]:\n                    last_layer, last_mask = pooling(\n                        last_layer, last_mask,\n                        cast(MaxPoolSpec, specification), i)\n                    image_processing_layers.append((last_layer, last_mask))\n                elif specification[0] == ""R"":\n                    if not self.batch_normalize:\n                        raise ValueError(\n                            ""Using ResNet blocks requires batch normalization ""\n                            ""to be turned on."")\n                    (last_layer, last_mask,\n                     last_channels) = residual_block(\n                         last_layer, last_mask, last_channels,\n                         cast(ResNetSpec, specification),\n                         self.batch_norm_callback, i)\n                    image_processing_layers.append((last_layer, last_mask))\n                else:\n                    raise ValueError(\n                        ""Unknown type of convoutional layer #{}: \'{}\'"".format(\n                            i + 1, specification[0]))\n\n        return image_processing_layers\n\n    @tensor\n    def spatial_states(self):\n        # pylint: disable=unsubscriptable-object\n        return self.image_processing_layers[-1][0]\n        # pylint: enable=unsubscriptable-object\n\n    @tensor\n    def spatial_mask(self) -> tf.Tensor:\n        # pylint: disable=unsubscriptable-object\n        return self.image_processing_layers[-1][1]\n        # pylint: enable=unsubscriptable-object\n\n    @tensor\n    def output(self) -> tf.Tensor:\n        """"""Output vector of the CNN.\n\n        If there are specified some fully connected layers, there are applied\n        on top of the last convolutional map. Dropout is applied between all\n        layers, default activation function is ReLU. There are only projection\n        layers, no softmax is applied.\n\n        If there is fully_connected layer specified, average-pooled last\n        convolutional map is used as a vector output.\n        """"""\n        # pylint: disable=no-member\n        last_height, last_width, last_n_channels = [\n            s.value for s in self.spatial_states.get_shape()[1:]]\n        # pylint: enable=no-member\n\n        if self.fully_connected is None:\n            # we average out by the image size -> shape is number\n            # channels from the last convolution\n            encoded = tf.reduce_mean(self.spatial_states, [1, 2])\n            return encoded\n\n        states_flat = tf.reshape(\n            self.spatial_states,\n            [-1, last_width * last_height * last_n_channels])\n        return multilayer_projection(\n            states_flat, self.fully_connected,\n            activation=tf.nn.relu,\n            dropout_keep_prob=self.dropout_keep_prob,\n            train_mode=self.train_mode)\n\n    def feed_dict(self, dataset: Dataset, train: bool = False) -> FeedDict:\n        fd = ModelPart.feed_dict(self, dataset, train)\n\n        # if it is from the pickled file, it is a list, not a numpy tensor,\n        # so convert it as as a prevention\n        images = np.array(list(dataset.get_series(self.data_id)))\n        fd[self.image_input] = images / 255.0\n        return fd\n\n\ndef plain_convolution(\n        prev_layer: tf.Tensor,\n        prev_mask: tf.Tensor,\n        specification: ConvSpec,\n        batch_norm_callback: Callable[[tf.Tensor], tf.Tensor],\n        layer_num: int) -> Tuple[tf.Tensor, tf.Tensor, int]:\n    try:\n        check_argument_types()\n    except TypeError as err:\n        raise ValueError((\n            ""Specification of a convolutional layer (number {} in config) ""\n            \'needs to have 5 members: ""C"", kernel size, stride, \'\n            ""padding, output channels, was {}"").format(\n                layer_num, specification)) from err\n    kernel_size, stride, pad, out_channels = specification[1:]\n\n    if pad not in [""same"", ""valid""]:\n        raise ValueError(\n            (""Padding must be \'same\' or \'valid\', ""\n             ""was \'{}\' in layer {}."").format(pad, layer_num + 1))\n\n    with tf.variable_scope(""layer_{}_convolution"".format(layer_num)):\n        next_layer = tf.layers.conv2d(\n            prev_layer, out_channels, kernel_size,\n            activation=None, padding=pad)\n\n        next_layer = batch_norm_callback(next_layer)\n        next_layer = tf.nn.relu(next_layer)\n\n        next_mask = tf.layers.max_pooling2d(\n            prev_mask, kernel_size, stride, padding=pad)\n\n    return next_layer, next_mask, out_channels\n\n\ndef residual_block(\n        prev_layer: tf.Tensor,\n        prev_mask: tf.Tensor,\n        prev_channels: int,\n        specification: ResNetSpec,\n        batch_norm_callback: Callable[[tf.Tensor], tf.Tensor],\n        layer_num: int) -> Tuple[tf.Tensor, tf.Tensor, int]:\n    try:\n        check_argument_types()\n    except TypeError as err:\n        raise ValueError((\n            ""Specification of a residual block (number {} in config) ""\n            \'needs to have 3 members: ""R"", kernel size, channels; \'\n            ""was {}"").format(layer_num, specification)) from err\n    kernel_size, out_channels = specification[1:]\n\n    with tf.variable_scope(""layer_{}_resnet_block"".format(layer_num)):\n        if out_channels == prev_channels:\n            before_resnet_block = prev_layer\n        else:\n            with tf.variable_scope(""project_input""):\n                before_resnet_block = tf.layers.conv2d(\n                    prev_layer, out_channels, 1, 1,\n                    ""same"", activation=None)\n                before_resnet_block = batch_norm_callback(before_resnet_block)\n\n        with tf.variable_scope(""conv_a""):\n            after_cnn = batch_norm_callback(prev_layer)\n            after_cnn = tf.nn.relu(after_cnn)\n            after_cnn = tf.layers.conv2d(\n                after_cnn, out_channels, kernel_size,\n                padding=""same"", activation=None)\n\n        with tf.variable_scope(""conv_b""):\n            after_cnn = batch_norm_callback(after_cnn)\n            after_cnn = tf.nn.relu(after_cnn)\n            after_cnn = tf.layers.conv2d(\n                after_cnn, out_channels, kernel_size,\n                padding=""same"", activation=None)\n\n        next_layer = after_cnn + before_resnet_block\n\n    return next_layer, prev_mask, out_channels\n\n\ndef pooling(\n        prev_layer: tf.Tensor,\n        prev_mask: tf.Tensor,\n        specification: MaxPoolSpec,\n        layer_num: int) -> Tuple[tf.Tensor, tf.Tensor]:\n    try:\n        check_argument_types()\n    except TypeError as err:\n        raise ValueError((\n            ""Specification of a max-pooling layer (number {} in config) ""\n            \'needs to have 3 members: ""M"", pool size, stride, padding, \'\n            ""was {}"").format(layer_num, specification)) from err\n    pool_type, pool_size, stride, pad = specification\n\n    if pool_type == ""M"":\n        pool_fn = tf.layers.max_pooling2d\n    elif pool_type == ""A"":\n        pool_fn = tf.layers.average_pooling2d\n    else:\n        raise ValueError(\n            (""Unsupported type of pooling: {}, use \'M\' for max-pooling or ""\n             ""\'A\' for average pooling."").format(pool_type))\n\n    if pad not in [""same"", ""valid""]:\n        raise ValueError(\n            ""Padding must be \'same\' or \'valid\', was \'{}\' in layer {}.""\n            .format(pad, layer_num + 1))\n\n    with tf.variable_scope(""layer_{}_max_pool"".format(layer_num)):\n        next_layer = pool_fn(prev_layer, pool_size, stride)\n        next_mask = tf.layers.max_pooling2d(prev_mask, pool_size, stride)\n    return next_layer, next_mask\n\n\nclass CNNTemporalView(ModelPart, TemporalStatefulWithOutput):\n    """"""Slice the convolutional maps left to right.""""""\n\n    def __init__(self,\n                 name: str,\n                 cnn: CNNEncoder) -> None:\n        check_argument_types()\n        ModelPart.__init__(\n            self, name, save_checkpoint=None, load_checkpoint=None)\n        self._cnn = cnn\n\n    @tensor\n    def output(self) -> tf.Tensor:\n        return self._cnn.output\n\n    @tensor\n    def temporal_states(self):\n        states = tf.transpose(self._cnn.spatial_states, perm=[0, 2, 1, 3])\n        shape = states.get_shape()\n        res = tf.reshape(\n            states, [-1, shape[1].value, shape[2].value * shape[3].value])\n        return res\n\n    @tensor\n    def temporal_mask(self) -> tf.Tensor:\n        mask = tf.squeeze(self._cnn.spatial_mask, 3)\n        summed = tf.reduce_sum(mask, axis=1)\n        return tf.to_float(tf.greater(summed, 0))\n\n    @property\n    def dependencies(self) -> List[str]:\n        return super().dependencies + [""_cnn""]\n'"
neuralmonkey/encoders/facebook_conv.py,17,"b'""""""From the paper Convolutional Sequence to Sequence Learning.\n\nhttp://arxiv.org/abs/1705.03122\n""""""\n\nimport tensorflow as tf\nimport numpy as np\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.logging import log\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.model.parameterized import InitializerSpecs\nfrom neuralmonkey.model.sequence import EmbeddedSequence\nfrom neuralmonkey.model.stateful import TemporalStatefulWithOutput\nfrom neuralmonkey.nn.projection import glu\nfrom neuralmonkey.tf_utils import get_variable\n\n\nclass SentenceEncoder(ModelPart, TemporalStatefulWithOutput):\n\n    # pylint: disable=too-many-arguments\n    def __init__(self,\n                 name: str,\n                 input_sequence: EmbeddedSequence,\n                 conv_features: int,\n                 encoder_layers: int,\n                 kernel_width: int = 5,\n                 dropout_keep_prob: float = 1.0,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        check_argument_types()\n        ModelPart.__init__(self, name, reuse, save_checkpoint, load_checkpoint,\n                           initializers)\n\n        self.input_sequence = input_sequence\n        self.encoder_layers = encoder_layers\n        self.conv_features = conv_features\n        self.kernel_width = kernel_width\n        self.dropout_keep_prob = dropout_keep_prob\n\n        if conv_features <= 0:\n            raise ValueError(""Number of features must be a positive integer."")\n        if encoder_layers <= 0:\n            raise ValueError(\n                ""Number of encoder layers must be a positive integer."")\n\n        if self.input_sequence.max_length is None:\n            raise ValueError(""Input sequence must have a maximum length for ""\n                             ""positional embeddings with this encoder"")\n        self.max_input_length = self.input_sequence.max_length\n\n        log(""Initializing convolutional seq2seq encoder, name {}""\n            .format(self.name))\n    # pylint: enable=too-many-arguments\n\n    @tensor\n    def temporal_states(self) -> tf.Tensor:\n        convolutions = tf.layers.dense(\n            self.ordered_embedded_inputs,\n            self.conv_features,\n            name=""order_and_embed"")\n\n        for layer in range(self.encoder_layers):\n            convolutions = self._residual_conv(\n                convolutions, ""encoder_conv_{}"".format(layer))\n\n        return convolutions + tf.layers.dense(\n            self.ordered_embedded_inputs,\n            self.conv_features,\n            name=""input_to_final_state"")\n\n    @tensor\n    def output(self) -> tf.Tensor:\n        # This state concatenation is not based on any paper, but was\n        # tested empirically\n        return tf.reduce_max(self.temporal_states, axis=1)\n\n    @tensor\n    def temporal_mask(self) -> tf.Tensor:\n        return self.input_sequence.temporal_mask\n\n    @tensor\n    def order_embeddings(self) -> tf.Tensor:\n        # initialization in the same way as in original CS2S implementation\n        with tf.variable_scope(""input_projection""):\n            return get_variable(\n                ""order_embeddings"", [self.max_input_length,\n                                     self.input_sequence.embedding_sizes[0]])\n\n    @tensor\n    def ordered_embedded_inputs(self) -> tf.Tensor:\n        # shape (batch, time, embedding size)\n        ordering_additive = tf.expand_dims(self.order_embeddings, 0)\n        batch_max_len = tf.shape(self.input_sequence.temporal_states)[1]\n        clipped_ordering_embed = ordering_additive[:, :batch_max_len, :]\n\n        return self.input_sequence.temporal_states + clipped_ordering_embed\n\n    def _residual_conv(self, input_signals: tf.Tensor, name: str):\n        with tf.variable_scope(name):\n            # Initialized as described in the paper.\n            # Note: this may be equivalent to tf.glorot_normal_initializer\n            init_deviat = np.sqrt(4 / self.conv_features)\n            convolution_filters = get_variable(\n                ""convolution_filters"",\n                [self.kernel_width, self.conv_features,\n                 2 * self.conv_features],\n                initializer=tf.random_normal_initializer(stddev=init_deviat))\n\n            bias = get_variable(\n                name=""conv_bias"",\n                shape=[2 * self.conv_features],\n                initializer=tf.zeros_initializer())\n\n            conv = (tf.nn.conv1d(input_signals, convolution_filters, 1, ""SAME"")\n                    + bias)\n\n            return glu(conv) + input_signals\n'"
neuralmonkey/encoders/imagenet_encoder.py,19,"b'""""""Pre-trained ImageNet networks.""""""\n\nimport sys\nfrom typing import Any, Callable, Dict, NamedTuple, Optional, Tuple\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.slim as tf_slim\n# pylint: disable=unused-import\n# Workaround of missing slim\'s import\n# see https://github.com/tensorflow/tensorflow/issues/6064\nimport tensorflow.contrib.slim.nets\n# pylint: enable=unused-import\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.dataset import Dataset\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.model.feedable import FeedDict\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.model.parameterized import InitializerSpecs\nfrom neuralmonkey.model.stateful import SpatialStatefulWithOutput\n\n\nclass ImageNetSpec(NamedTuple(\n        ""ImageNetSpec"",\n        [(""scope"", Callable),\n         (""image_size"", Tuple[int, int]),\n         (""apply_net"", Callable)])):\n    """"""Specification of the Imagenet encoder.\n\n    Do not use this object directly, instead, use one of the ``get_*``functions\n    in this module.\n\n    Attributes:\n        scope: The variable scope of the network to use.\n        image_size: A tuple of two integers giving the image width and height\n            in pixels.\n        apply_net: The function that receives an image and applies the network.\n    """"""\n\n\n# pylint: disable=import-error\ndef get_alexnet() -> ImageNetSpec:\n    import nets.alexnet_v2\n    return ImageNetSpec(\n        scope=nets.alexnet.alexnet_v2_arg_scope,\n        image_size=(224, 224),\n        apply_net=lambda image: nets.alexnet.alexnet_v2(\n            image, is_training=False))\n\n\ndef get_vgg_by_type(vgg_type: str) -> Callable[[], ImageNetSpec]:\n    def get_vgg() -> ImageNetSpec:\n        import nets.vgg\n        if vgg_type == ""vgg16"":\n            net_fn = nets.vgg.vgg_16\n        elif vgg_type == ""vgg19"":\n            net_fn = nets.vgg.vgg_19\n        else:\n            raise ValueError(\n                ""Unknown type of VGG net: {}"".format(vgg_type))\n\n        return ImageNetSpec(\n            scope=nets.vgg.vgg_arg_scope,\n            image_size=(224, 224),\n            apply_net=lambda image: net_fn(\n                image, is_training=False, dropout_keep_prob=1.0))\n    return get_vgg\n\n\ndef get_resnet_by_type(resnet_type: str) -> Callable[[], ImageNetSpec]:\n    def get_resnet() -> ImageNetSpec:\n        import nets.resnet_v2\n        if resnet_type == ""resnet_50"":\n            net_fn = nets.resnet_v2.resnet_v2_50\n        elif resnet_type == ""resnet_101"":\n            net_fn = nets.resnet_v2.resnet_v2_101\n        elif resnet_type == ""resnet_152"":\n            net_fn = nets.resnet_v2.resnet_v2_152\n        else:\n            raise ValueError(\n                ""Unknown type of ResNet: {}"".format(resnet_type))\n\n        return ImageNetSpec(\n            scope=nets.resnet_v2.resnet_arg_scope,\n            image_size=(229, 229),\n            apply_net=lambda image: net_fn(\n                image, is_training=False, global_pool=False))\n    return get_resnet\n# pylint: enable=import-error\n\n\nSUPPORTED_NETWORKS = {\n    ""alexnet_v2"": get_alexnet,\n    ""vgg_16"": get_vgg_by_type(""vgg16""),\n    ""vgg_19"": get_vgg_by_type(""vgg19""),\n    ""resnet_v2_50"": get_resnet_by_type(""resnet_50""),\n    ""resnet_v2_101"": get_resnet_by_type(""resnet_101""),\n    ""resnet_v2_152"": get_resnet_by_type(""resnet_152"")\n}\n\n\nclass ImageNet(ModelPart, SpatialStatefulWithOutput):\n    """"""Pre-trained ImageNet network.\n\n    We use the ImageNet networks as they are in the tesnorflow/models\n    repository (https://github.com/tensorflow/models). In order use them, you\n    need to clone the repository and configure the ImageNet object such that it\n    has a full path to ""research/slim"" in the repository.  Visit\n    https://github.com/tensorflow/models/tree/master/research/slim for\n    information about checkpoints of the pre-trained models.\n    """"""\n\n    # pylint: disable=too-many-arguments\n    def __init__(self,\n                 name: str,\n                 data_id: str,\n                 network_type: str,\n                 slim_models_path: str,\n                 load_checkpoint: str = None,\n                 spatial_layer: str = None,\n                 encoded_layer: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        """"""Initialize pre-trained ImageNet network.\n\n        Args:\n            name: Name of the model part (the ImageNet network, will be in its\n                scope, independently on `name`).\n            data_id: Id of series with images (list of 3D numpy arrays)\n            network_type: Identifier of ImageNet network from TFSlim.\n            spatial_layer: String identifier of the convolutional map\n                (model\'s endpoint). Check\n                TFSlim documentation for end point specifications.\n            encoded_layer: String id of the network layer that will be used as\n                input of a decoder. `None` means averaging the convolutional\n                maps.\n            path_to_models: Path to Slim models in tensorflow/models\n                repository.\n            load_checkpoint: Checkpoint file from which the pre-trained network\n                is loaded.\n        """"""\n        check_argument_types()\n\n        ModelPart.__init__(self, name, load_checkpoint=load_checkpoint,\n                           initializers=initializers, save_checkpoint=None)\n        sys.path.insert(0, slim_models_path)\n\n        self.data_id = data_id\n        self.network_type = network_type\n        self.spatial_layer = spatial_layer\n        self.encoded_layer = encoded_layer\n\n        if self.network_type not in SUPPORTED_NETWORKS:\n            raise ValueError(\n                ""Network \'{}\' is not among the supported ones ({})"".format(\n                    self.network_type, "", "".join(SUPPORTED_NETWORKS.keys())))\n\n        self.net_specification = SUPPORTED_NETWORKS[self.network_type]()\n        self.height, self.width = self.net_specification.image_size\n\n    @property\n    def input_types(self) -> Dict[str, tf.DType]:\n        return {self.data_id: tf.float32}\n\n    @property\n    def input_shapes(self) -> Dict[str, tf.TensorShape]:\n        return {\n            self.data_id: tf.TensorShape([None, self.height, self.width, 3])}\n\n    @tensor\n    def input_image(self) -> tf.Tensor:\n        return self.dataset[self.data_id]\n\n    @tensor\n    def end_points(self) -> Any:\n        with tf_slim.arg_scope(self.net_specification.scope()):\n            _, end_points = self.net_specification.apply_net(self.input_image)\n\n        if (self.spatial_layer is not None\n                and self.spatial_layer not in end_points):\n            raise ValueError(\n                ""Network \'{}\' does not contain endpoint \'{}\'"".format(\n                    self.network_type, self.spatial_layer))\n\n        if self.spatial_layer is not None:\n            net_output = end_points[self.spatial_layer]\n            if len(net_output.get_shape()) != 4:\n                raise ValueError(\n                    ""Endpoint \'{}\' for network \'{}\' cannot be a convolutional ""\n                    "" map, its dimensionality is: {}."".format(\n                        self.spatial_layer, self.network_type,\n                        "", "".join(\n                            [str(d.value) for d in net_output.get_shape()])))\n\n        if (self.encoded_layer is not None\n                and self.encoded_layer not in end_points):\n            raise ValueError(\n                ""Network \'{}\' does not contain endpoint \'{}\'."".format(\n                    self.network_type, self.encoded_layer))\n\n        return end_points\n\n    @tensor\n    def spatial_states(self) -> Optional[tf.Tensor]:\n        if self.spatial_layer is None:\n            return None\n\n        # pylint: disable=unsubscriptable-object\n        net_output = self.end_points[self.spatial_layer]\n        # pylint: enable=unsubscriptable-object\n\n        net_output = tf.stop_gradient(net_output)\n        return net_output\n\n    @tensor\n    def spatial_mask(self) -> tf.Tensor:\n        if self.spatial_layer is None:\n            return None\n        mask = tf.ones(tf.shape(self.spatial_states)[:3])\n        # pylint: disable=no-member\n        mask.set_shape(self.spatial_states.get_shape()[:3])\n        # pylint: enable=no-member\n        return mask\n\n    @tensor\n    def output(self) -> tf.Tensor:\n        if self.encoded_layer is None:\n            return tf.reduce_mean(self.spatial_states, [1, 2])\n\n        # pylint: disable=unsubscriptable-object\n        encoded = tf.squeeze(self.end_points[self.encoded_layer], [1, 2])\n        # pylint: enable=unsubscriptable-object\n\n        encoded = tf.stop_gradient(encoded)\n        return encoded\n\n    def _init_saver(self) -> None:\n        if not self._saver:\n            with tf.variable_scope(self.name, reuse=True):\n                local_variables = tf.get_collection(\n                    tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n                slim_variables = tf.get_collection(\n                    tf.GraphKeys.GLOBAL_VARIABLES, scope=self.network_type)\n                self._saver = tf.train.Saver(\n                    var_list=local_variables + slim_variables)\n\n    def feed_dict(self, dataset: Dataset, train: bool = False) -> FeedDict:\n        fd = ModelPart.feed_dict(self, dataset, train)\n\n        images = np.array(dataset.get_series(self.data_id))\n        assert images.shape[1:] == (self.height, self.width, 3)\n        fd[self.input_image] = images\n\n        return fd\n'"
neuralmonkey/encoders/numpy_stateful_filler.py,29,"b'# TODO untested module\nfrom typing import Dict, List\n\nimport numpy as np\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.dataset import Dataset\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.model.feedable import FeedDict\nfrom neuralmonkey.model.parameterized import InitializerSpecs\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.model.stateful import (\n    Stateful, SpatialStatefulWithOutput, TemporalStateful)\n\n\n# pylint: disable=too-few-public-methods\nclass StatefulFiller(ModelPart, Stateful):\n    """"""Placeholder class for stateful input.\n\n    This model part is used to feed 1D tensors to the model. Optionally, it\n    projects the states to given dimension.\n    """"""\n\n    # pylint: disable=too-many-arguments\n    def __init__(self,\n                 name: str,\n                 dimension: int,\n                 data_id: str,\n                 output_shape: int = None,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        """"""Instantiate StatefulFiller.\n\n        Arguments:\n            name: Name of the model part.\n            dimension: Dimensionality of the input.\n            data_id: Series containing the numpy objects.\n            output_shape: Dimension of optional state projection.\n        """"""\n        check_argument_types()\n        ModelPart.__init__(\n            self, name, reuse, save_checkpoint, load_checkpoint, initializers)\n\n        self.data_id = data_id\n        self.dimension = dimension\n        self.output_shape = output_shape\n\n        if self.dimension <= 0:\n            raise ValueError(""Input vector dimension must be positive."")\n        if self.output_shape is not None and self.output_shape <= 0:\n            raise ValueError(""Output vector dimension must be positive."")\n    # pylint: enable=too-many-arguments\n\n    @property\n    def input_types(self) -> Dict[str, tf.DType]:\n        return {self.data_id: tf.float32}\n\n    @property\n    def input_shapes(self) -> Dict[str, tf.TensorShape]:\n        return {self.data_id: tf.TensorShape([None, self.dimension])}\n\n    @tensor\n    def vector(self) -> tf.Tensor:\n        return self.dataset[self.data_id]\n\n    @tensor\n    def output(self) -> tf.Tensor:\n        if self.output_shape is None or self.dimension == self.output_shape:\n            return self.vector\n\n        return tf.layers.dense(self.vector, self.output_shape)\n\n    def feed_dict(self, dataset: Dataset, train: bool = False) -> FeedDict:\n        fd = ModelPart.feed_dict(self, dataset, train)\n        fd[self.vector] = dataset.get_series(self.data_id)\n        return fd\n\n\nclass TemporalFiller(ModelPart, TemporalStateful):\n    """"""Placeholder class for 2D numerical input.\n\n    This model part is used to feed 2D tensors (e.g., audio input).\n    """"""\n\n    # pylint: disable=too-many-arguments\n    def __init__(self,\n                 name: str,\n                 data_id: str,\n                 input_size: int,\n                 max_input_len: int = None,\n                 dropout_keep_prob: float = 1.0,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        check_argument_types()\n        ModelPart.__init__(\n            self, name, reuse, save_checkpoint, load_checkpoint, initializers)\n\n        self.data_id = data_id\n        self.input_size = input_size\n        self.max_input_len = max_input_len\n        self.dropout_keep_prob = dropout_keep_prob\n    # pylint: enable=too-many-arguments\n\n    @property\n    def input_types(self) -> Dict[str, tf.DType]:\n        return {self.data_id: tf.float32}\n\n    @property\n    def input_shapes(self) -> Dict[str, tf.TensorShape]:\n        return {self.data_id: tf.TensorShape([None, None, self.input_size])}\n\n    @tensor\n    def temporal_states(self) -> tf.Tensor:\n        return self.dataset[self.data_id]\n\n    # pylint: disable=no-self-use\n    @tensor\n    def _input_lengths(self) -> tf.Tensor:\n        return tf.placeholder(tf.int32, [None], ""encoder_padding_lengths"")\n    # pylint: enable=no-self-use\n\n    @tensor\n    def temporal_mask(self) -> tf.Tensor:\n        return tf.sequence_mask(self._input_lengths, dtype=tf.float32)\n\n    def feed_dict(self, dataset: Dataset, train: bool = False) -> FeedDict:\n        fd = ModelPart.feed_dict(self, dataset, train)\n\n        series = list(dataset.get_series(self.data_id))\n        lengths = []\n        inputs = []\n\n        max_len = max(x.shape[0] for x in series)\n        if self.max_input_len is not None:\n            max_len = min(self.max_input_len, max_len)\n\n        for x in series:\n            length = min(max_len, x.shape[0])\n            x_padded = np.zeros(shape=(max_len,) + x.shape[1:],\n                                dtype=x.dtype)\n            x_padded[:length] = x[:length]\n\n            lengths.append(length)\n            inputs.append(x_padded)\n\n        fd[self.temporal_states] = inputs\n        fd[self._input_lengths] = lengths\n\n        return fd\n\n\nclass SpatialFiller(ModelPart, SpatialStatefulWithOutput):\n    """"""Placeholder class for 3D numerical input.\n\n    This model part is used to feed 3D tensors (e.g., pre-trained convolutional\n    maps image captioning). Optionally, the states are projected to given size.\n    """"""\n\n    # pylint: disable=too-many-arguments\n    def __init__(self,\n                 name: str,\n                 input_shape: List[int],\n                 data_id: str,\n                 projection_dim: int = None,\n                 ff_hidden_dim: int = None,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        """"""Instantiate SpatialFiller.\n\n        Args:\n            name: Name of the model part.\n            input_shape: Dimensionality of the input.\n            data_id: Name of the data series with numpy objects.\n            projection_dim: Optional, dimension of the states projection.\n        """"""\n        check_argument_types()\n        ModelPart.__init__(\n            self, name, reuse, save_checkpoint, load_checkpoint, initializers)\n\n        self.data_id = data_id\n        self.input_shape = input_shape\n        self.projection_dim = projection_dim\n        self.ff_hidden_dim = ff_hidden_dim\n\n        if self.ff_hidden_dim is not None and self.projection_dim is None:\n            raise ValueError(\n                ""projection_dim must be provided when using ff_hidden_dim"")\n\n        if len(self.input_shape) != 3:\n            raise ValueError(""The input shape should have 3 dimensions."")\n    # pylint: enable=too-many-arguments\n\n    @property\n    def input_types(self) -> Dict[str, tf.DType]:\n        return {self.data_id: tf.float32}\n\n    @property\n    def input_shapes(self) -> Dict[str, tf.TensorShape]:\n        shape = [None] + self.input_shape  # type: ignore\n        return {self.data_id: tf.TensorShape(shape)}\n\n    @tensor\n    def spatial_input(self) -> tf.Tensor:\n        return self.dataset[self.data_id]\n\n    @tensor\n    def output(self) -> tf.Tensor:\n        return tf.reduce_mean(\n            self.spatial_states, axis=[1, 2], name=""average_image"")\n\n    @tensor\n    def spatial_states(self) -> tf.Tensor:\n        if self.ff_hidden_dim:\n            projected = tf.layers.conv2d(\n                self.spatial_input, filters=self.ff_hidden_dim,\n                kernel_size=1, activation=tf.nn.relu)\n        else:\n            projected = self.spatial_input\n\n        if self.projection_dim:\n            return tf.layers.conv2d(\n                projected, filters=self.projection_dim,\n                kernel_size=1, activation=None)\n\n        # pylint: disable=comparison-with-callable\n        assert projected == self.spatial_input\n        # pylint: enable=comparison-with-callable\n\n        return self.spatial_input\n\n    @tensor\n    def spatial_mask(self) -> tf.Tensor:\n        return tf.ones(tf.shape(self.spatial_states)[:3])\n\n    def feed_dict(self, dataset: Dataset, train: bool = False) -> FeedDict:\n        fd = ModelPart.feed_dict(self, dataset, train)\n        fd[self.spatial_input] = list(dataset.get_series(self.data_id))\n        return fd\n'"
neuralmonkey/encoders/pooling.py,10,"b'import tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.model.stateful import Stateful, TemporalStateful\nfrom neuralmonkey.model.parameterized import InitializerSpecs\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.decorators import tensor\n\n\n# pylint: disable=abstract-method\n# Pylint bug: https://github.com/PyCQA/pylint/issues/179\nclass SequencePooling(ModelPart, Stateful):\n    """"""An abstract pooling layer over a sequence.""""""\n\n    def __init__(self,\n                 name: str,\n                 input_sequence: TemporalStateful,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        """"""Initialize an instance of the pooling layer.""""""\n        check_argument_types()\n        ModelPart.__init__(self, name, reuse, save_checkpoint, load_checkpoint,\n                           initializers)\n        self.input_sequence = input_sequence\n\n    @tensor\n    def _masked_input(self) -> tf.Tensor:\n        return self.input_sequence.temporal_states * self._input_mask\n\n    @tensor\n    def _input_mask(self) -> tf.Tensor:\n        return tf.expand_dims(self.input_sequence.temporal_mask, -1)\n# pylint: enable=abstract-method\n\n\nclass SequenceMaxPooling(SequencePooling):\n    """"""A max pooling layer over a sequence.\n\n    Takes the maximum of a sequence over time to produce a single state.\n    """"""\n\n    @tensor\n    def output(self) -> tf.Tensor:\n        # Pad the sequence with a large negative value, but make sure it has\n        # non-zero length.\n        length = tf.reduce_sum(self._input_mask)\n        with tf.control_dependencies([tf.assert_greater(length, 0.5)]):\n            padded_input = self._masked_input + 1e-15 * (1 - self._input_mask)\n        return tf.reduce_max(padded_input, axis=1)\n\n\nclass SequenceAveragePooling(SequencePooling):\n    """"""An average pooling layer over a sequence.\n\n    Averages a sequence over time to produce a single state.\n    """"""\n\n    @tensor\n    def output(self) -> tf.Tensor:\n        return (tf.reduce_sum(self._masked_input, axis=1)\n                / (tf.reduce_sum(self._input_mask, axis=1) + 1e-8))\n'"
neuralmonkey/encoders/recurrent.py,23,"b'from typing import Tuple, List, Union, Callable, NamedTuple\n\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.model.stateful import (\n    TemporalStatefulWithOutput, TemporalStateful)\nfrom neuralmonkey.model.parameterized import InitializerSpecs\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.nn.ortho_gru_cell import OrthoGRUCell, NematusGRUCell\nfrom neuralmonkey.nn.utils import dropout\nfrom neuralmonkey.vocabulary import Vocabulary\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.model.sequence import (\n    EmbeddedSequence, EmbeddedFactorSequence)\nfrom neuralmonkey.tf_utils import layer_norm\n\nRNN_CELL_TYPES = {\n    ""NematusGRU"": NematusGRUCell,\n    ""GRU"": OrthoGRUCell,\n    ""LSTM"": tf.nn.rnn_cell.LSTMCell\n}\n\nRNN_DIRECTIONS = [""forward"", ""backward"", ""bidirectional""]\n\n# pylint: disable=invalid-name\nRNNSpecTuple = Union[Tuple[int], Tuple[int, str], Tuple[int, str, str]]\nRNNCellTuple = Tuple[tf.nn.rnn_cell.RNNCell, tf.nn.rnn_cell.RNNCell]\n# pylint: enable=invalid-name\n\n\nclass RNNSpec(NamedTuple(\n        ""RNNSpec"",\n        [(""size"", int),\n         (""direction"", str),\n         (""cell_type"", str)])):\n    """"""Recurrent neural network specifications.\n\n    Attributes:\n        size: The state size.\n        direction: The RNN processing direction. One of ``forward``,\n            ``backward``, and ``bidirectional``.\n        cell_type: The recurrent cell type to use. Refer to\n            ``encoders.recurrent.RNN_CELL_TYPES`` for possible values.\n    """"""\n\n\ndef _make_rnn_spec(size: int,\n                   direction: str = ""bidirectional"",\n                   cell_type: str = ""GRU"") -> RNNSpec:\n    if size <= 0:\n        raise ValueError(\n            ""RNN size must be a positive integer. {} given."".format(size))\n\n    if direction not in RNN_DIRECTIONS:\n        raise ValueError(""RNN direction must be one of {}. {} given.""\n                         .format(str(RNN_DIRECTIONS), direction))\n\n    if cell_type not in RNN_CELL_TYPES:\n        raise ValueError(""RNN cell type must be one of {}. {} given.""\n                         .format(str(RNN_CELL_TYPES), cell_type))\n\n    return RNNSpec(size, direction, cell_type)\n\n\ndef _make_rnn_cell(spec: RNNSpec) -> Callable[[], tf.nn.rnn_cell.RNNCell]:\n    """"""Return the graph template for creating RNN cells.""""""\n    return RNN_CELL_TYPES[spec.cell_type](spec.size)\n\n\ndef rnn_layer(rnn_input: tf.Tensor,\n              lengths: tf.Tensor,\n              rnn_spec: RNNSpec) -> Tuple[tf.Tensor, tf.Tensor]:\n    """"""Construct a RNN layer given its inputs and specs.\n\n    Arguments:\n        rnn_inputs: The input sequence to the RNN.\n        lengths: Lengths of input sequences.\n        rnn_spec: A valid RNNSpec tuple specifying the network architecture.\n        add_residual: Add residual connections to the layer output.\n    """"""\n    if rnn_spec.direction == ""bidirectional"":\n        fw_cell = _make_rnn_cell(rnn_spec)\n        bw_cell = _make_rnn_cell(rnn_spec)\n\n        outputs_tup, states_tup = tf.nn.bidirectional_dynamic_rnn(\n            fw_cell, bw_cell, rnn_input, sequence_length=lengths,\n            dtype=tf.float32)\n\n        outputs = tf.concat(outputs_tup, 2)\n\n        if rnn_spec.cell_type == ""LSTM"":\n            states_tup = (state.h for state in states_tup)\n\n        final_state = tf.concat(list(states_tup), 1)\n    else:\n        if rnn_spec.direction == ""backward"":\n            rnn_input = tf.reverse_sequence(rnn_input, lengths, seq_axis=1)\n\n        cell = _make_rnn_cell(rnn_spec)\n        outputs, final_state = tf.nn.dynamic_rnn(\n            cell, rnn_input, sequence_length=lengths, dtype=tf.float32)\n\n        if rnn_spec.direction == ""backward"":\n            outputs = tf.reverse_sequence(outputs, lengths, seq_axis=1)\n\n        if rnn_spec.cell_type == ""LSTM"":\n            final_state = final_state.h\n\n    return outputs, final_state\n\n\nclass RecurrentEncoder(ModelPart, TemporalStatefulWithOutput):\n\n    # pylint: disable=too-many-arguments\n    def __init__(self,\n                 name: str,\n                 input_sequence: TemporalStateful,\n                 rnn_layers: List[RNNSpecTuple],\n                 add_residual: bool = False,\n                 add_layer_norm: bool = False,\n                 include_final_layer_norm: bool = True,\n                 dropout_keep_prob: float = 1.0,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        """"""Create a new instance of a recurrent encoder.\n\n        Arguments:\n            name: ModelPart name.\n            input_seqeunce: The input sequence for the encoder.\n            rnn_size: The dimension of the RNN hidden state vector.\n            rnn_cell: One of ""GRU"", ""NematusGRU"", ""LSTM"". Which kind of memory\n                cell to use.\n            rnn_direction: One of ""forward"", ""backward"", ""bidirectional"". In\n                what order to process the input sequence. Note that choosing\n                ""bidirectional"" will double the resulting vector dimension as\n                well as the number of encoder parameters.\n            add_residual: Add residual connections to the RNN layer output.\n            add_layer_norm: Add layer normalization after each RNN layer.\n            include_final_layer_norm: Normalize also output of the network.\n            dropout_keep_prob: 1 - dropout probability.\n            save_checkpoint: ModelPart save checkpoint file.\n            load_checkpoint: ModelPart load checkpoint file.\n        """"""\n        check_argument_types()\n        ModelPart.__init__(self, name, reuse, save_checkpoint, load_checkpoint,\n                           initializers)\n        TemporalStatefulWithOutput.__init__(self)\n\n        self.input_sequence = input_sequence\n        self.dropout_keep_prob = dropout_keep_prob\n        self.rnn_specs = [_make_rnn_spec(*r) for r in rnn_layers]\n        self.add_residual = add_residual\n        self.add_layer_norm = add_layer_norm\n        self.include_final_layer_norm = include_final_layer_norm\n\n        if self.dropout_keep_prob <= 0.0 or self.dropout_keep_prob > 1.0:\n            raise ValueError(""Dropout keep prob must be inside (0,1]."")\n\n        layer_sizes = [\n            2 * layer.size if layer.direction == ""bidirectional""\n            else layer.size for layer in self.rnn_specs]\n        if add_residual and len(set(layer_sizes)) > 1:\n            raise ValueError(\n                ""When using residual connectiong, all layers must have ""\n                ""the same size, but are {}."".format(layer_sizes))\n\n        self._variable_scope.set_initializer(\n            tf.random_normal_initializer(stddev=0.001))\n    # pylint: enable=too-many-arguments\n\n    @tensor\n    def rnn_input(self) -> tf.Tensor:\n        return dropout(self.input_sequence.temporal_states,\n                       self.dropout_keep_prob, self.train_mode)\n\n    @tensor\n    def rnn(self) -> Tuple[tf.Tensor, tf.Tensor]:\n        layer_input = self.rnn_input  # type: tf.Tensor\n        # pylint: disable=unsubscriptable-object\n        layer_final = self.rnn_input[:, -1]\n        # pylint: enable=unsubscriptable-object\n\n        for i, rnn_spec in enumerate(self.rnn_specs):\n            with tf.variable_scope(""rnn_{}_{}"".format(i, rnn_spec.direction),\n                                   reuse=tf.AUTO_REUSE):\n\n                if self.add_layer_norm:\n                    layer_input = layer_norm(layer_input)\n\n                layer_output, layer_final_output = rnn_layer(\n                    layer_input, self.input_sequence.lengths, rnn_spec)\n\n                layer_output = dropout(\n                    layer_output, self.dropout_keep_prob, self.train_mode)\n                layer_final_output = dropout(\n                    layer_final_output, self.dropout_keep_prob,\n                    self.train_mode)\n\n                in_dim = layer_input.get_shape()[-1]\n                out_dim = layer_output.get_shape()[-1]\n\n                if self.add_residual and in_dim == out_dim:\n                    layer_input += layer_output\n                    layer_final += layer_final_output\n                else:\n                    # pylint: disable=redefined-variable-type\n                    layer_input = layer_output\n                    layer_final = layer_final_output\n                    # pylint: enable=redefined-variable-type\n\n        assert layer_final is not None\n        if self.include_final_layer_norm:\n            return layer_norm(layer_input), layer_norm(layer_final)\n        return layer_input, layer_final\n\n    @tensor\n    def temporal_states(self) -> tf.Tensor:\n        # pylint: disable=unsubscriptable-object\n        return self.rnn[0]\n        # pylint: enable=unsubscriptable-object\n\n    @tensor\n    def temporal_mask(self) -> tf.Tensor:\n        return self.input_sequence.temporal_mask\n\n    @tensor\n    def output(self) -> tf.Tensor:\n        # pylint: disable=unsubscriptable-object\n        return self.rnn[1]\n        # pylint: enable=unsubscriptable-object\n\n\nclass SentenceEncoder(RecurrentEncoder):\n    # pylint: disable=too-many-arguments,too-many-locals\n    def __init__(self,\n                 name: str,\n                 vocabulary: Vocabulary,\n                 data_id: str,\n                 embedding_size: int,\n                 rnn_size: int,\n                 rnn_cell: str = ""GRU"",\n                 rnn_direction: str = ""bidirectional"",\n                 add_residual: bool = False,\n                 add_layer_norm: bool = False,\n                 max_input_len: int = None,\n                 dropout_keep_prob: float = 1.0,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None,\n                 embedding_initializer: Callable = None) -> None:\n        """"""Create a new instance of the sentence encoder.\n\n        Arguments:\n            name: ModelPart name.\n            vocabulary: The input vocabulary.\n            data_id: The input sequence data ID.\n            embedding_size: The dimension of the embedding vectors in the input\n                sequence.\n            max_input_len: Maximum length of the input sequence (disregard\n                tokens after this position).\n            rnn_size: The dimension of the RNN hidden state vector.\n            rnn_cell: One of ""GRU"", ""NematusGRU"", ""LSTM"". Which kind of memory\n                cell to use.\n            rnn_direction: One of ""forward"", ""backward"", ""bidirectional"". In\n                what order to process the input sequence. Note that choosing\n                ""bidirectional"" will double the resulting vector dimension as\n                well as the number of encoder parameters.\n            add_residual: Add residual connections to the RNN layer output.\n            add_layer_norm: Add layer normalization after each RNN layer.\n            dropout_keep_prob: 1 - dropout probability.\n            save_checkpoint: ModelPart save checkpoint file.\n            load_checkpoint: ModelPart load checkpoint file.\n        """"""\n        check_argument_types()\n        s_ckp = ""input_{}"".format(save_checkpoint) if save_checkpoint else None\n        l_ckp = ""input_{}"".format(load_checkpoint) if load_checkpoint else None\n        input_initializers = []\n        if embedding_initializer is not None:\n            input_initializers.append(\n                (""embedding_matrix_0"", embedding_initializer))\n\n        # TODO! Representation runner needs this. It is not simple to do it in\n        # recurrent encoder since there may be more source data series. The\n        # best way could be to enter the data_id parameter manually to the\n        # representation runner\n        self.data_id = data_id\n\n        input_sequence = EmbeddedSequence(\n            name=""{}_input"".format(name),\n            vocabulary=vocabulary,\n            data_id=data_id,\n            embedding_size=embedding_size,\n            max_length=max_input_len,\n            save_checkpoint=s_ckp,\n            load_checkpoint=l_ckp,\n            initializers=input_initializers)\n\n        RecurrentEncoder.__init__(\n            self,\n            name=name,\n            input_sequence=input_sequence,\n            rnn_layers=[(rnn_size, rnn_direction, rnn_cell)],\n            add_residual=add_residual,\n            add_layer_norm=add_layer_norm,\n            dropout_keep_prob=dropout_keep_prob,\n            reuse=reuse,\n            save_checkpoint=save_checkpoint,\n            load_checkpoint=load_checkpoint,\n            initializers=initializers)\n    # pylint: enable=too-many-arguments,too-many-locals\n\n\nclass FactoredEncoder(RecurrentEncoder):\n    # pylint: disable=too-many-arguments,too-many-locals\n    def __init__(self,\n                 name: str,\n                 vocabularies: List[Vocabulary],\n                 data_ids: List[str],\n                 embedding_sizes: List[int],\n                 rnn_size: int,\n                 rnn_cell: str = ""GRU"",\n                 rnn_direction: str = ""bidirectional"",\n                 add_residual: bool = False,\n                 add_layer_norm: bool = False,\n                 max_input_len: int = None,\n                 dropout_keep_prob: float = 1.0,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None,\n                 input_initializers: InitializerSpecs = None) -> None:\n        """"""Create a new instance of the factored encoder.\n\n        Arguments:\n            name: ModelPart name.\n            vocabularies: The vocabularies for each factor.\n            data_ids: The input sequence data ID for each factor.\n            embedding_sizes: The dimension of the embedding vectors in the\n                input sequence for each factor.\n            max_input_len: Maximum length of the input sequence (disregard\n                tokens after this position).\n            rnn_size: The dimension of the RNN hidden state vector.\n            rnn_cell: One of ""GRU"", ""NematusGRU"", ""LSTM"". Which kind of memory\n                cell to use.\n            rnn_direction: One of ""forward"", ""backward"", ""bidirectional"". In\n                what order to process the input sequence. Note that choosing\n                ""bidirectional"" will double the resulting vector dimension as\n                well as the number of encoder parameters.\n            add_residual: Add residual connections to the RNN layer output.\n            add_layer_norm: Add layer normalization after each RNN layer.\n            dropout_keep_prob: 1 - dropout probability.\n            save_checkpoint: ModelPart save checkpoint file.\n            load_checkpoint: ModelPart load checkpoint file.\n        """"""\n        check_argument_types()\n        s_ckp = ""input_{}"".format(save_checkpoint) if save_checkpoint else None\n        l_ckp = ""input_{}"".format(load_checkpoint) if load_checkpoint else None\n\n        input_sequence = EmbeddedFactorSequence(\n            name=""{}_input"".format(name),\n            vocabularies=vocabularies,\n            data_ids=data_ids,\n            embedding_sizes=embedding_sizes,\n            max_length=max_input_len,\n            save_checkpoint=s_ckp,\n            load_checkpoint=l_ckp,\n            initializers=input_initializers)\n\n        RecurrentEncoder.__init__(\n            self,\n            name=name,\n            input_sequence=input_sequence,\n            rnn_layers=[(rnn_size, rnn_direction, rnn_cell)],\n            add_residual=add_residual,\n            add_layer_norm=add_layer_norm,\n            dropout_keep_prob=dropout_keep_prob,\n            reuse=reuse,\n            save_checkpoint=save_checkpoint,\n            load_checkpoint=load_checkpoint,\n            initializers=initializers)\n    # pylint: enable=too-many-arguments,too-many-locals\n'"
neuralmonkey/encoders/sentence_cnn_encoder.py,28,"b'""""""Encoder for sentences withou explicit segmentation.""""""\n\nfrom typing import Tuple, List\n\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.encoders.recurrent import RNNCellTuple\nfrom neuralmonkey.model.parameterized import InitializerSpecs\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.model.sequence import Sequence\nfrom neuralmonkey.model.stateful import TemporalStatefulWithOutput\nfrom neuralmonkey.nn.noisy_gru_cell import NoisyGRUCell\nfrom neuralmonkey.nn.ortho_gru_cell import OrthoGRUCell\nfrom neuralmonkey.nn.utils import dropout\nfrom neuralmonkey.nn.highway import highway\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.tf_utils import get_variable\n\n\n# pylint: disable=too-many-instance-attributes\nclass SentenceCNNEncoder(ModelPart, TemporalStatefulWithOutput):\n    """"""Recurrent over Convolutional Encoder.\n\n    Encoder processing a sentence using a CNN\n    then running a bidirectional RNN on the result.\n\n    Based on: Jason Lee, Kyunghyun Cho, Thomas Hofmann: Fully\n    Character-Level Neural Machine Translation without Explicit\n    Segmentation.\n\n    See https://arxiv.org/pdf/1610.03017.pdf\n    """"""\n\n    # pylint: disable=too-many-arguments,too-many-locals\n    # pylint: disable=too-many-statements\n    def __init__(self,\n                 name: str,\n                 input_sequence: Sequence,\n                 segment_size: int,\n                 highway_depth: int,\n                 rnn_size: int,\n                 filters: List[Tuple[int, int]],\n                 dropout_keep_prob: float = 1.0,\n                 use_noisy_activations: bool = False,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        """"""Create a new instance of the sentence encoder.\n\n        Arguments:\n            name: An unique identifier for this encoder\n            segment_size: The size of the segments over which we apply\n                max-pooling.\n            highway_depth: Depth of the highway layer.\n            rnn_size: The size of the encoder\'s hidden state. Note\n                that the actual encoder output state size will be\n                twice as long because it is the result of\n                concatenation of forward and backward hidden states.\n            filters: Specification of CNN filters. It is a list of tuples\n                specifying the filter size and number of channels.\n\n        Keyword arguments:\n            dropout_keep_prob: The dropout keep probability\n                (default 1.0)\n        """"""\n        ModelPart.__init__(self, name, reuse, save_checkpoint, load_checkpoint,\n                           initializers)\n        check_argument_types()\n\n        self.input_sequence = input_sequence\n        self.segment_size = segment_size\n        self.highway_depth = highway_depth\n        self.rnn_size = rnn_size\n        self.filters = filters\n        self.dropout_keep_prob = dropout_keep_prob\n        self.use_noisy_activations = use_noisy_activations\n\n        if dropout_keep_prob <= 0. or dropout_keep_prob > 1.:\n            raise ValueError(\n                (""Dropout keep probability must be ""\n                 ""in (0; 1], was {}"").format(dropout_keep_prob))\n\n        if rnn_size <= 0:\n            raise ValueError(""RNN size must be a positive integer."")\n\n        if highway_depth <= 0:\n            raise ValueError(""Highway depth must be a positive integer."")\n\n        if segment_size <= 0:\n            raise ValueError(""Segment size be a positive integer."")\n\n        if not filters:\n            raise ValueError(""You must specify convolutional filters."")\n\n        for filter_size, num_filters in self.filters:\n            if filter_size <= 0:\n                raise ValueError(""Filter size must be a positive integer."")\n            if num_filters <= 0:\n                raise ValueError(""Number of filters must be a positive int."")\n\n    @tensor\n    def cnn_encoded(self) -> tf.Tensor:\n        """"""1D convolution with max-pool that processing characters.""""""\n        dropped_inputs = dropout(self.input_sequence.temporal_states,\n                                 self.dropout_keep_prob, self.train_mode)\n\n        pooled_outputs = []\n        for filter_size, num_filters in self.filters:\n            with tf.variable_scope(""conv-maxpool-%s"" % filter_size):\n                filter_shape = [filter_size, self.input_sequence.dimension,\n                                num_filters]\n                w_filter = get_variable(\n                    ""conv_W"", filter_shape,\n                    initializer=tf.variance_scaling_initializer(\n                        mode=""fan_avg"", distribution=""uniform""))\n                b_filter = get_variable(\n                    ""conv_bias"", [num_filters],\n                    initializer=tf.zeros_initializer())\n                conv = tf.nn.conv1d(\n                    dropped_inputs,\n                    w_filter,\n                    stride=1,\n                    padding=""SAME"",\n                    name=""conv"")\n\n                # Apply nonlinearity\n                conv_relu = tf.nn.relu(tf.nn.bias_add(conv, b_filter))\n\n                # Max-pooling over the output segments\n                expanded_conv_relu = tf.expand_dims(conv_relu, -1)\n                pooled = tf.nn.max_pool(\n                    expanded_conv_relu,\n                    ksize=[1, self.segment_size, 1, 1],\n                    strides=[1, self.segment_size, 1, 1],\n                    padding=""SAME"",\n                    name=""maxpool"")\n                pooled_outputs.append(pooled)\n\n        # Combine all the pooled features\n        concat = tf.concat(pooled_outputs, axis=2)\n        return tf.squeeze(concat, [3])\n\n    @tensor\n    def highway_layer(self) -> tf.Tensor:\n        """"""Highway net projection following the CNN.""""""\n        # pylint: disable=no-member\n        cnn_out_size = self.cnn_encoded.get_shape().as_list()[-1]\n        # pylint: enable=no-member\n        highway_layer = tf.reshape(self.cnn_encoded, [-1, cnn_out_size])\n        for i in range(self.highway_depth):\n            highway_layer = highway(\n                highway_layer,\n                scope=(""highway_layer_%s"" % i))\n        return tf.reshape(\n            highway_layer,\n            [self.batch_size, -1, cnn_out_size])\n\n    @tensor\n    def bidirectional_rnn(self) -> Tuple[Tuple[tf.Tensor, tf.Tensor],\n                                         Tuple[tf.Tensor, tf.Tensor]]:\n        # BiRNN Network\n        fw_cell, bw_cell = self.rnn_cells()  # type: RNNCellTuple\n        seq_lens = tf.ceil(tf.divide(\n            self.input_sequence.lengths,\n            self.segment_size))\n        seq_lens = tf.cast(seq_lens, tf.int32)\n        return tf.nn.bidirectional_dynamic_rnn(\n            fw_cell, bw_cell, self.highway_layer,\n            sequence_length=seq_lens,\n            dtype=tf.float32)\n\n    @tensor\n    def temporal_states(self) -> tf.Tensor:\n        # pylint: disable=unsubscriptable-object\n        return tf.concat(self.bidirectional_rnn[0], 2)\n        # pylint: enable=unsubscriptable-object\n\n    @tensor\n    def output(self) -> tf.Tensor:\n        # pylint: disable=unsubscriptable-object\n        return tf.concat(self.bidirectional_rnn[1], 1)\n        # pylint: enable=unsubscriptable-object\n\n    @tensor\n    def temporal_mask(self) -> tf.Tensor:\n        expanded = tf.expand_dims(\n            tf.expand_dims(self.input_sequence.temporal_mask, -1),\n            -1)\n        pooled = tf.nn.max_pool(\n            expanded,\n            ksize=[1, self.segment_size, 1, 1],\n            strides=[1, self.segment_size, 1, 1],\n            padding=""SAME"")\n        return tf.squeeze(pooled, [2, 3])\n\n    def rnn_cells(self) -> RNNCellTuple:\n        """"""Return the graph template to for creating RNN memory cells.""""""\n\n        if self.use_noisy_activations:\n            return(NoisyGRUCell(self.rnn_size, self.train_mode),\n                   NoisyGRUCell(self.rnn_size, self.train_mode))\n\n        return (OrthoGRUCell(self.rnn_size),\n                OrthoGRUCell(self.rnn_size))\n'"
neuralmonkey/encoders/sequence_cnn_encoder.py,19,"b'""""""Encoder for sentence classification with 1D convolutions and max-pooling.""""""\n\nfrom typing import Dict, List, Tuple\n\nfrom typeguard import check_argument_types\nimport tensorflow as tf\n\nfrom neuralmonkey.dataset import Dataset\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.model.feedable import FeedDict\nfrom neuralmonkey.model.parameterized import InitializerSpecs\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.model.stateful import Stateful\nfrom neuralmonkey.nn.utils import dropout\nfrom neuralmonkey.vocabulary import Vocabulary, pad_batch, sentence_mask\nfrom neuralmonkey.tf_utils import get_variable\n\n\nclass SequenceCNNEncoder(ModelPart, Stateful):\n    """"""Encoder processing a sequence using a CNN.""""""\n\n    # pylint: disable=too-many-arguments\n    def __init__(self,\n                 name: str,\n                 vocabulary: Vocabulary,\n                 data_id: str,\n                 embedding_size: int,\n                 filters: List[Tuple[int, int]],\n                 max_input_len: int = None,\n                 dropout_keep_prob: float = 1.0,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        """"""Create a new instance of the CNN sequence encoder.\n\n        Based on: Yoon Kim: Convolutional Neural Networks for Sentence\n        Classification (http://emnlp2014.org/papers/pdf/EMNLP2014181.pdf)\n\n        Arguments:\n            vocabulary: Input vocabulary\n            data_id: Identifier of the data series fed to this encoder\n            name: An unique identifier for this encoder\n            max_input_len: Maximum length of an encoded sequence\n            embedding_size: The size of the embedding vector assigned\n                to each word\n            filters: Specification of CNN filters. It is a list of tuples\n                specifying the filter size and number of channels.\n            dropout_keep_prob: The dropout keep probability\n                (default 1.0)\n        """"""\n        check_argument_types()\n        ModelPart.__init__(self, name, reuse, save_checkpoint, load_checkpoint,\n                           initializers)\n\n        self.vocabulary = vocabulary\n        self.data_id = data_id\n        self.max_input_len = max_input_len\n        self.embedding_size = embedding_size\n        self.dropout_keep_prob = dropout_keep_prob\n        self.filters = filters\n\n    @property\n    def input_types(self) -> Dict[str, tf.DType]:\n        return {self.data_id: tf.string}\n\n    @property\n    def input_shapes(self) -> Dict[str, tf.TensorShape]:\n        return {self.data_id: tf.TensorShape([None, None])}\n\n    @tensor\n    def inputs(self) -> tf.Tensor:\n        return self.vocabulary.strings_to_indices(self.input_tokens)\n\n    @tensor\n    def input_tokens(self) -> tf.Tensor:\n        return self.dataset[self.data_id]\n\n    @tensor\n    def input_mask(self) -> tf.Tensor:\n        return sentence_mask(self.inputs)\n\n    @tensor\n    def embedded_inputs(self) -> tf.Tensor:\n        with tf.variable_scope(""input_projection""):\n            embedding_matrix = get_variable(\n                ""word_embeddings"",\n                [len(self.vocabulary), self.embedding_size],\n                initializer=tf.variance_scaling_initializer(\n                    mode=""fan_avg"", distribution=""uniform""))\n            return dropout(\n                tf.nn.embedding_lookup(embedding_matrix, self.inputs),\n                self.dropout_keep_prob,\n                self.train_mode)\n\n    @tensor\n    def output(self) -> tf.Tensor:\n        pooled_outputs = []\n        for filter_size, num_filters in self.filters:\n            with tf.variable_scope(""conv-maxpool-%s"" % filter_size):\n                # Convolution Layer\n                filter_shape = [filter_size, self.embedding_size, num_filters]\n                w_filter = get_variable(\n                    ""conv_W"", filter_shape,\n                    initializer=tf.variance_scaling_initializer(\n                        mode=""fan_avg"", distribution=""uniform""))\n                b_filter = get_variable(\n                    ""conv_bias"", [num_filters],\n                    initializer=tf.zeros_initializer())\n                conv = tf.nn.conv1d(\n                    self.embedded_inputs,\n                    w_filter,\n                    stride=1,\n                    padding=""VALID"",\n                    name=""conv"")\n\n                # Apply nonlinearity\n                conv_relu = tf.nn.relu(tf.nn.bias_add(conv, b_filter))\n\n                # Max-pooling over the outputs\n                pooled = tf.reduce_max(conv_relu, 1)\n                pooled_outputs.append(pooled)\n\n        # Combine all the pooled features\n        return tf.concat(pooled_outputs, axis=1)\n\n    def feed_dict(self, dataset: Dataset, train: bool = False) -> FeedDict:\n        """"""Populate the feed dictionary with the encoder inputs.\n\n        Arguments:\n            dataset: The dataset to use\n            train: Boolean flag telling whether it is training time\n        """"""\n        fd = ModelPart.feed_dict(self, dataset, train)\n        sentences = dataset.get_series(self.data_id)\n        fd[self.input_tokens] = pad_batch(list(sentences), self.max_input_len)\n        return fd\n'"
neuralmonkey/encoders/transformer.py,34,"b'""""""Implementation of the encoder of the Transformer model.\n\nDescribed in Vaswani et al. (2017), arxiv.org/abs/1706.03762\n""""""\nfrom typing import List\n\nimport math\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.attention.base_attention import (\n    Attendable, get_attention_states, get_attention_mask)\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.attention.scaled_dot_product import attention\nfrom neuralmonkey.model.parameterized import InitializerSpecs\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.model.stateful import (TemporalStateful,\n                                         TemporalStatefulWithOutput)\nfrom neuralmonkey.nn.utils import dropout\nfrom neuralmonkey.tf_utils import get_variable, layer_norm\n\n\ndef position_signal(dimension: int, length: tf.Tensor) -> tf.Tensor:\n    # Code simplified and copied from github.com/tensorflow/tensor2tensor\n\n    # TODO write this down on a piece of paper and understand the code and\n    # compare it to the paper\n    positions = tf.to_float(tf.range(length))\n\n    num_timescales = dimension // 2\n\n    # see: github.com/tensorflow/tensor2tensor/blob/v1.5.5/tensor2tensor/\n    #      layers/common_attention.py#L425\n    log_timescale_increment = math.log(1.0e4) / (num_timescales - 1)\n    inv_timescales = tf.exp(tf.range(num_timescales, dtype=tf.float32)\n                            * -log_timescale_increment)\n\n    scaled_time = tf.expand_dims(positions, 1) * tf.expand_dims(\n        inv_timescales, 0)\n\n    signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n    signal = tf.pad(signal, [[0, 0], [0, tf.mod(dimension, 2)]])\n    signal = tf.reshape(signal, [1, length, dimension])\n\n    return signal\n\n\nclass TransformerLayer(TemporalStateful):\n    def __init__(self, states: tf.Tensor, mask: tf.Tensor) -> None:\n        self._states = states\n        self._mask = mask\n\n    @property\n    def temporal_states(self) -> tf.Tensor:\n        return self._states\n\n    @property\n    def temporal_mask(self) -> tf.Tensor:\n        return self._mask\n\n\n# pylint: disable=too-many-instance-attributes\nclass TransformerEncoder(ModelPart, TemporalStatefulWithOutput):\n\n    # pylint: disable=too-many-arguments,too-many-locals\n    def __init__(self,\n                 name: str,\n                 input_sequence: TemporalStateful,\n                 ff_hidden_size: int,\n                 depth: int,\n                 n_heads: int,\n                 dropout_keep_prob: float = 1.0,\n                 attention_dropout_keep_prob: float = 1.0,\n                 target_space_id: int = None,\n                 use_att_transform_bias: bool = False,\n                 use_positional_encoding: bool = True,\n                 input_for_cross_attention: Attendable = None,\n                 n_cross_att_heads: int = None,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        """"""Create an encoder of the Transformer model.\n\n        Described in Vaswani et al. (2017), arxiv.org/abs/1706.03762\n\n        Arguments:\n            input_sequence: Embedded input sequence.\n            name: Name of the decoder. Should be unique accross all Neural\n                Monkey objects.\n            reuse: Reuse the model variables.\n            dropout_keep_prob: Probability of keeping a value during dropout.\n            target_space_id: Specifies the modality of the target space.\n            use_att_transform_bias: Add bias when transforming qkv vectors\n                for attention.\n            use_positional_encoding: If True, position encoding signal is added\n                to the input.\n\n        Keyword arguments:\n            ff_hidden_size: Size of the feedforward sublayers.\n            n_heads: Number of the self-attention heads.\n            depth: Number of sublayers.\n            attention_dropout_keep_prob: Probability of keeping a value\n                during dropout on the attention output.\n            input_for_cross_attention: An attendable model part that is\n                attended using cross-attention on every layer of the decoder,\n                analogically to how encoder is attended in the decoder.\n            n_cross_att_heads: Number of heads used in the cross-attention.\n\n        """"""\n        check_argument_types()\n        ModelPart.__init__(self, name, reuse, save_checkpoint, load_checkpoint,\n                           initializers)\n\n        self.input_sequence = input_sequence\n        self.ff_hidden_size = ff_hidden_size\n        self.depth = depth\n        self.n_heads = n_heads\n        self.dropout_keep_prob = dropout_keep_prob\n        self.attention_dropout_keep_prob = attention_dropout_keep_prob\n        self.target_space_id = target_space_id\n        self.use_att_transform_bias = use_att_transform_bias\n        self.use_positional_encoding = use_positional_encoding\n        self.input_for_cross_attention = input_for_cross_attention\n        self.n_cross_att_heads = n_cross_att_heads\n\n        if self.depth <= 0:\n            raise ValueError(""Depth must be a positive integer."")\n\n        if self.ff_hidden_size <= 0:\n            raise ValueError(""Feed forward hidden size must be a ""\n                             ""positive integer."")\n\n        if self.dropout_keep_prob <= 0.0 or self.dropout_keep_prob > 1.0:\n            raise ValueError(""Dropout keep prob must be inside (0,1]."")\n\n        if (self.attention_dropout_keep_prob <= 0.0\n                or self.attention_dropout_keep_prob > 1.0):\n            raise ValueError(""Dropout keep prob for attn must be in (0,1]."")\n\n        if self.target_space_id is not None and (self.target_space_id >= 32\n                                                 or self.target_space_id < 0):\n            raise ValueError(\n                ""If provided, the target space ID should be between 0 and 31. ""\n                ""Was: {}"".format(self.target_space_id))\n\n        if (input_for_cross_attention is None) != (n_cross_att_heads is None):\n            raise ValueError(\n                ""Either both input_for_cross_attention and n_cross_att_heads ""\n                ""must be provided or none of them."")\n\n        self._variable_scope.set_initializer(tf.variance_scaling_initializer(\n            mode=""fan_avg"", distribution=""uniform""))\n    # pylint: enable=too-many-arguments,too-many-locals\n\n    @property\n    def model_dimension(self) -> int:\n        dim = self.input_sequence.dimension\n\n        if self.input_for_cross_attention is not None:\n            cross_att_dim = get_attention_states(\n                self.input_for_cross_attention).get_shape()[-1].value\n            if cross_att_dim != dim:\n                raise ValueError(\n                    ""The input for cross-attention must be of the same ""\n                    ""dimension as the model, was {}."".format(cross_att_dim))\n\n        return dim\n\n    @tensor\n    def output(self) -> tf.Tensor:\n        return tf.reduce_sum(self.temporal_states, axis=1)\n\n    @tensor\n    def modality_matrix(self) -> tf.Tensor:\n        """"""Create an embedding matrix for varyining target modalities.\n\n        Used to embed different target space modalities in the tensor2tensor\n        models (e.g. during the zero-shot translation).\n        """"""\n        emb_size = self.input_sequence.temporal_states.shape.as_list()[-1]\n        return get_variable(\n            name=""target_modality_embedding_matrix"",\n            shape=[32, emb_size],\n            dtype=tf.float32,\n            initializer=tf.variance_scaling_initializer(\n                mode=""fan_avg"", distribution=""uniform""))\n\n    @tensor\n    def target_modality_embedding(self) -> tf.Tensor:\n        """"""Gather correct embedding of the target space modality.\n\n        See TransformerEncoder.modality_matrix for more information.\n        """"""\n        return tf.gather(self.modality_matrix,\n                         tf.constant(self.target_space_id))\n\n    @tensor\n    def encoder_inputs(self) -> tf.Tensor:\n        inputs = self.input_sequence.temporal_states\n\n        if self.target_space_id is not None:\n            inputs += tf.reshape(self.target_modality_embedding, [1, 1, -1])\n\n        length = tf.shape(inputs)[1]\n\n        if self.use_positional_encoding:\n            inputs += position_signal(self.model_dimension, length)\n\n        return dropout(inputs, self.dropout_keep_prob, self.train_mode)\n\n    def self_attention_sublayer(\n            self, prev_layer: TransformerLayer) -> tf.Tensor:\n        """"""Create the encoder self-attention sublayer.""""""\n\n        # Layer normalization\n        normalized_states = layer_norm(prev_layer.temporal_states)\n\n        # Run self-attention\n        self_context, _ = attention(\n            queries=normalized_states,\n            keys=normalized_states,\n            values=normalized_states,\n            keys_mask=prev_layer.temporal_mask,\n            num_heads=self.n_heads,\n            dropout_callback=lambda x: dropout(\n                x, self.attention_dropout_keep_prob, self.train_mode),\n            use_bias=self.use_att_transform_bias)\n\n        # Apply dropout\n        self_context = dropout(\n            self_context, self.dropout_keep_prob, self.train_mode)\n\n        # Add residual connections\n        return self_context + prev_layer.temporal_states\n\n    def cross_attention_sublayer(self, queries: tf.Tensor) -> tf.Tensor:\n        assert self.cross_attention_sublayer is not None\n        assert self.n_cross_att_heads is not None\n        assert self.input_for_cross_attention is not None\n\n        encoder_att_states = get_attention_states(\n            self.input_for_cross_attention)\n        encoder_att_mask = get_attention_mask(self.input_for_cross_attention)\n\n        # Layer normalization\n        normalized_queries = layer_norm(queries)\n\n        encoder_context, _ = attention(\n            queries=normalized_queries,\n            keys=encoder_att_states,\n            values=encoder_att_states,\n            keys_mask=encoder_att_mask,\n            num_heads=self.n_cross_att_heads,\n            dropout_callback=lambda x: dropout(\n                x, self.attention_dropout_keep_prob, self.train_mode),\n            use_bias=self.use_att_transform_bias)\n\n        # Apply dropout\n        encoder_context = dropout(\n            encoder_context, self.dropout_keep_prob, self.train_mode)\n\n        # Add residual connections\n        return encoder_context + queries\n\n    def feedforward_sublayer(self, layer_input: tf.Tensor) -> tf.Tensor:\n        """"""Create the feed-forward network sublayer.""""""\n\n        # Layer normalization\n        normalized_input = layer_norm(layer_input)\n\n        # Feed-forward network hidden layer + ReLU\n        ff_hidden = tf.layers.dense(\n            normalized_input, self.ff_hidden_size, activation=tf.nn.relu,\n            name=""hidden_state"")\n\n        # Apply dropout on hidden layer activations\n        ff_hidden = dropout(ff_hidden, self.dropout_keep_prob, self.train_mode)\n\n        # Feed-forward output projection\n        ff_output = tf.layers.dense(\n            ff_hidden, self.model_dimension, name=""output"")\n\n        # Apply dropout on feed-forward output projection\n        ff_output = dropout(ff_output, self.dropout_keep_prob, self.train_mode)\n\n        # Add residual connections\n        return ff_output + layer_input\n\n    def layer(self, level: int) -> TransformerLayer:\n        # Recursive implementation. Outputs of the zeroth layer\n        # are normalized inputs.\n        if level == 0:\n            return TransformerLayer(self.encoder_inputs, self.temporal_mask)\n\n        # Compute the outputs of the previous layer\n        prev_layer = self.layer(level - 1)\n\n        with tf.variable_scope(""layer_{}"".format(level - 1)):\n            with tf.variable_scope(""self_attention""):\n                self_context = self.self_attention_sublayer(prev_layer)\n\n            if self.input_for_cross_attention is not None:\n                with tf.variable_scope(""cross_attention""):\n                    self_context = self.cross_attention_sublayer(self_context)\n\n            with tf.variable_scope(""feedforward""):\n                output_states = self.feedforward_sublayer(self_context)\n\n        # Layer normalization on the encoder outputs\n        if self.depth == level:\n            output_states = layer_norm(output_states)\n\n        return TransformerLayer(states=output_states, mask=self.temporal_mask)\n\n    @tensor\n    def temporal_states(self) -> tf.Tensor:\n        return self.layer(self.depth).temporal_states\n\n    @tensor\n    def temporal_mask(self) -> tf.Tensor:\n        return self.input_sequence.temporal_mask\n\n    @property\n    def dependencies(self) -> List[str]:\n        deps = super().dependencies\n\n        if self.input_for_cross_attention is not None:\n            return deps + [""input_for_cross_attention""]\n        return deps\n'"
neuralmonkey/evaluators/__init__.py,0,"b'from .accuracy import Accuracy, AccuracySeqLevel, AccuracyEvaluator\nfrom .average import AverageEvaluator\nfrom .beer import BeerWrapper\nfrom .bleu import BLEU1, BLEU4, BLEU, BLEUEvaluator\nfrom .chrf import ChrF3, ChrFEvaluator\nfrom .edit_distance import EditDistance, EditDistanceEvaluator\nfrom .f1_bio import BIOF1Score, F1Evaluator\nfrom .gleu import GLEUEvaluator\nfrom .mse import MSE, MeanSquaredErrorEvaluator\nfrom .multeval import MultEvalWrapper\nfrom .sacrebleu import SacreBLEUEvaluator, SacreBLEU\nfrom .ter import TER, TEREvaluator\nfrom .wer import WER, WEREvaluator\nfrom .rouge import ROUGE_1, ROUGE_2, ROUGE_L, RougeEvaluator\nfrom .perplexity import PerplexityEvaluator\n'"
neuralmonkey/evaluators/accuracy.py,0,"b'from typing import Any\nfrom neuralmonkey.evaluators.evaluator import Evaluator, SequenceEvaluator\n\n\n# pylint: disable=too-few-public-methods\n# These classes are technically just a syntactic sugar.\nclass AccuracyEvaluator(SequenceEvaluator[Any]):\n    """"""Accuracy Evaluator.\n\n    This class uses the default `SequenceEvaluator` implementation, i.e. works\n    on sequences of equal lengths (but can be used to others as well) and\n    use `==` as the token scorer.\n    """"""\n\n\nclass AccuracySeqLevelEvaluator(Evaluator[Any]):\n    """"""Sequence-level accuracy evaluator.\n\n    This class uses the default evaluator implementation. It gives 1.0 to equal\n    sequences and 0.0 to others, averaging the scores over the batch.\n    """"""\n\n\n# pylint: disable=invalid-name\nAccuracy = AccuracyEvaluator()\nAccuracySeqLevel = AccuracySeqLevelEvaluator()\n# pylint: enable=invalid-name\n'"
neuralmonkey/evaluators/average.py,0,"b'# pylint: disable=too-few-public-methods, no-self-use, unused-argument\n# This evaluator here is just an ugly hack to work with perplexity runner\nfrom neuralmonkey.evaluators.evaluator import Evaluator\n\n\nclass AverageEvaluator(Evaluator[float]):\n    """"""Just average the numeric output of a runner.""""""\n\n    def score_instance(self, hypothesis: float, reference: float) -> float:\n        return hypothesis\n'"
neuralmonkey/evaluators/beer.py,0,"b'import tempfile\nimport subprocess\nfrom typing import List\n\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.logging import log\nfrom neuralmonkey.evaluators.evaluator import Evaluator\n\n\nclass BeerWrapper(Evaluator[List[str]]):\n    """"""Wrapper for BEER scorer.\n\n    Paper: http://aclweb.org/anthology/D14-1025\n    Code: https://github.com/stanojevic/beer\n    """"""\n\n    def __init__(self,\n                 wrapper: str,\n                 name: str = ""BEER"",\n                 encoding: str = ""utf-8"") -> None:\n        """"""Initialize the BEER wrapper.\n\n        Args:\n            name: Name of the evaluator.\n            wrapper: Path to the BEER\'s executable.\n            encoding: Data encoding.\n        """"""\n        check_argument_types()\n        super().__init__(name)\n        self.wrapper = wrapper\n        self.encoding = encoding\n\n    def serialize_to_bytes(self, sentences: List[List[str]]) -> bytes:\n        joined = ["" "".join(r) for r in sentences]\n        string = ""\\n"".join(joined) + ""\\n""\n        return string.encode(self.encoding)\n\n    def score_batch(self,\n                    hypotheses: List[List[str]],\n                    references: List[List[str]]) -> float:\n\n        ref_bytes = self.serialize_to_bytes(references)\n        hyp_bytes = self.serialize_to_bytes(hypotheses)\n\n        with tempfile.NamedTemporaryFile() as reffile, \\\n                tempfile.NamedTemporaryFile() as hypfile:\n\n            reffile.write(ref_bytes)\n            reffile.flush()\n\n            hypfile.write(hyp_bytes)\n            hypfile.flush()\n\n            args = [self.wrapper, ""-r"", reffile.name, ""-s"", hypfile.name]\n\n            output_proc = subprocess.run(args,\n                                         stderr=subprocess.PIPE,\n                                         stdout=subprocess.PIPE)\n\n            proc_stdout = output_proc.stdout.decode(""utf-8"")  # type: ignore\n            lines = proc_stdout.splitlines()\n\n            if not lines:\n                return 0.0\n\n            try:\n                beer_score = float(lines[0].split()[-1])\n                return beer_score\n            except IndexError:\n                log(""Error: Malformed output from BEER wrapper:"", color=""red"")\n                log(proc_stdout, color=""red"")\n                log(""======="", color=""red"")\n                return 0.0\n            except ValueError:\n                log(""Value error - beer \'{}\' is not a number."".format(\n                    lines[0]), color=""red"")\n                return 0.0\n'"
neuralmonkey/evaluators/bleu.py,0,"b'from collections import Counter\nfrom typing import List, Tuple\nimport numpy as np\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.evaluators.evaluator import Evaluator\n\n\nclass BLEUEvaluator(Evaluator[List[str]]):\n\n    def __init__(self, n: int = 4,\n                 deduplicate: bool = False,\n                 name: str = None,\n                 multiple_references_separator: str = None) -> None:\n        """"""Instantiate BLEU evaluator.\n\n        Args:\n            n: Longest n-grams considered.\n            deduplicate: Flag whether repated tokes should be treated as one.\n            name: Name displayed in the logs and TensorBoard.\n            multiple_references_separator: Token that separates multiple\n                reference sentences. If ``None``, it assumes the reference is\n                one sentence only.\n        """"""\n        check_argument_types()\n\n        if name is None:\n            name = ""BLEU-{}"".format(n)\n            if deduplicate:\n                name += ""-dedup""\n        super().__init__(name)\n\n        self.n = n\n        self.deduplicate = deduplicate\n        self.multiple_references_separator = multiple_references_separator\n\n    def score_batch(self,\n                    hypotheses: List[List[str]],\n                    references: List[List[str]]) -> float:\n\n        if self.multiple_references_separator is None:\n            listed_references = [[s] for s in references]\n        else:\n            listed_references = []\n            for sentences in references:\n                split_sentences = []\n                curr_reference = []  # type: List[str]\n                for tok in sentences:\n                    if tok == self.multiple_references_separator:\n                        split_sentences.append(curr_reference)\n                        curr_reference = []\n                    else:\n                        curr_reference.append(tok)\n                split_sentences.append(curr_reference)\n                listed_references.append(split_sentences)\n\n        if self.deduplicate:\n            hypotheses = BLEUEvaluator.deduplicate_sentences(hypotheses)\n\n        return 100 * BLEUEvaluator.bleu(hypotheses, listed_references, self.n)\n\n    @staticmethod\n    def ngram_counts(sentence: List[str], n: int,\n                     lowercase: bool, delimiter: str = "" "") -> Counter:\n        """"""Get n-grams from a sentence.\n\n        Arguments:\n            sentence: Sentence as a list of words\n            n: n-gram order\n            lowercase: Convert ngrams to lowercase\n            delimiter: delimiter to use to create counter entries\n        """"""\n\n        counts = Counter()  # type: Counter\n\n        # pylint: disable=too-many-locals\n        for begin in range(len(sentence) - n + 1):\n            ngram = delimiter.join(sentence[begin:begin + n])\n            if lowercase:\n                ngram = ngram.lower()\n\n            counts[ngram] += 1\n\n        return counts\n\n    @staticmethod\n    def merge_max_counters(counters: List[Counter]) -> Counter:\n        """"""Merge counters using maximum values.""""""\n        merged = Counter()  # type: Counter\n\n        for counter in counters:\n            for key in counter:\n                merged[key] = max(merged[key], counter[key])\n\n        return merged\n\n    @staticmethod\n    def modified_ngram_precision(hypotheses: List[List[str]],\n                                 references_list: List[List[List[str]]],\n                                 n: int,\n                                 case_sensitive: bool) -> Tuple[float, int]:\n        """"""Compute the modified n-gram precision on a list of sentences.\n\n        Arguments:\n            hypotheses: List of output sentences as lists of words\n            references_list: List of lists of reference sentences (as lists of\n                words)\n            n: n-gram order\n            case_sensitive: Whether to perform case-sensitive computation\n        """"""\n        corpus_true_positives = 0\n        corpus_generated_length = 0\n\n        for hypothesis, references in zip(hypotheses, references_list):\n            reference_counts = BLEUEvaluator.merge_max_counters([\n                BLEUEvaluator.ngram_counts(ref, n, not case_sensitive)\n                for ref in references])\n\n            hypothesis_counts = BLEUEvaluator.ngram_counts(\n                hypothesis, n, not case_sensitive)\n\n            true_positives = 0\n            for ngram in hypothesis_counts:\n                true_positives += reference_counts[ngram]\n\n            corpus_true_positives += true_positives\n            corpus_generated_length += sum(hypothesis_counts.values())\n\n        if corpus_generated_length == 0:\n            return 1, 0\n\n        return (corpus_true_positives / corpus_generated_length,\n                corpus_generated_length)\n\n    @staticmethod\n    def effective_reference_length(\n            hypotheses: List[List[str]],\n            references_list: List[List[List[str]]]) -> int:\n        """"""Compute the effective reference corpus length.\n\n        The effective reference corpus length is based on best match length.\n\n        Arguments:\n            hypotheses: List of output sentences as lists of words\n            references_list: List of lists of references (as lists of words)\n        """"""\n\n        eff_ref_length = 0\n\n        for hypothesis, references in zip(hypotheses, references_list):\n            hypothesis_length = len(hypothesis)\n\n            best_diff = np.inf\n            best_match_length = 0\n\n            for reference in references:\n                diff = np.abs(len(reference) - hypothesis_length)\n\n                if diff < best_diff:\n                    best_diff = diff\n                    best_match_length = len(reference)\n\n            eff_ref_length += best_match_length\n\n        return eff_ref_length\n\n    # pylint: disable=unused-argument\n    # to mainain same API with the function above\n    @staticmethod\n    def minimum_reference_length(hypotheses: List[List[str]],\n                                 references_list: List[List[str]]) -> int:\n        """"""Compute the minimum reference corpus length.\n\n        The minimum reference corpus length is based\n        on the shortest reference sentence length.\n\n        Arguments:\n            hypotheses: List of output sentences as lists of words\n            references_list: List of lists of references (as lists of words)\n        """"""\n\n        eff_ref_length = 0\n\n        for references in references_list:\n            shortest_length = np.inf\n\n            for reference in references:\n                if len(reference) < shortest_length:\n                    shortest_length = len(reference)\n\n            eff_ref_length += shortest_length\n\n        return eff_ref_length\n\n    @staticmethod\n    def bleu(hypotheses: List[List[str]], references: List[List[List[str]]],\n             ngrams: int = 4, case_sensitive: bool = True):\n        """"""Compute BLEU on a corpus with multiple references.\n\n        The n-grams are uniformly weighted.\n\n        Default is to use smoothing as in reference implementation on:\n        https://github.com/ufal/qtleap/blob/master/cuni_train/bin/mteval-v13a.pl#L831-L873\n\n        Arguments:\n            hypotheses: List of hypotheses\n            references: LIst of references. There can be more than one\n                reference.\n            ngrams: Maximum order of n-grams. Default 4.\n            case_sensitive: Perform case-sensitive computation. Default True.\n        """"""\n        log_bleu = 0\n        weight = 1 / ngrams\n\n        smooth = 1.0\n\n        for order in range(1, ngrams + 1):\n            prec, gen_len = BLEUEvaluator.modified_ngram_precision(\n                hypotheses, references, order, case_sensitive)\n\n            if prec == 0:\n                smooth *= 2\n                prec = 1 / (smooth * gen_len)\n\n            log_bleu += weight * np.log(prec)\n\n        # pylint: disable=invalid-name\n        # the symbols \'r\', \'c\', and \'bp\' are taken from the formula in\n        # Papineni et al., it makes sense to follow the notation\n        r = BLEUEvaluator.effective_reference_length(hypotheses, references)\n        c = sum([len(hyp) for hyp in hypotheses])\n\n        bp = min(1 - r / c, 0) if c != 0 else -np.inf\n        log_bleu += bp\n\n        return np.exp(log_bleu)\n\n    @staticmethod\n    def deduplicate_sentences(sentences: List[List[str]]) -> List[List[str]]:\n        deduplicated_sentences = []\n\n        for sentence in sentences:\n            last_w = None\n            dedup_snt = []\n\n            for word in sentence:\n                if word != last_w:\n                    dedup_snt.append(word)\n                    last_w = word\n\n            deduplicated_sentences.append(dedup_snt)\n\n        return deduplicated_sentences\n\n\n# pylint: disable=invalid-name\nBLEU1 = BLEUEvaluator(n=1)\nBLEU4 = BLEUEvaluator(n=4)\nBLEU = BLEUEvaluator()\n'"
neuralmonkey/evaluators/bleu_ref.py,0,"b'import tempfile\nfrom typing import List\nimport subprocess\nfrom neuralmonkey.logging import log\nfrom neuralmonkey.evaluators.evaluator import Evaluator\n\n# pylint: disable=too-few-public-methods\n# to be further refactored\n\n\nclass BLEUReferenceImplWrapper(Evaluator[List[str]]):\n    """"""Wrapper for TectoMT\'s wrapper for reference NIST and BLEU scorer.""""""\n\n    def __init__(self, wrapper, name=""BLEU"", encoding=""utf-8""):\n        log(""Reference BLEU wrapper is deprecated"", color=""red"")\n        self.wrapper = wrapper\n        self.encoding = encoding\n        self.name = name\n\n    def serialize_to_bytes(self, sentences: List[List[str]]) -> bytes:\n        joined = ["" "".join(r) for r in sentences]\n        string = ""\\n"".join(joined) + ""\\n""\n        return string.encode(self.encoding)\n\n    def score_batch(self,\n                    hypotheses: List[List[str]],\n                    references: List[List[str]]) -> float:\n\n        ref_bytes = self.serialize_to_bytes(references)\n        hyp_bytes = self.serialize_to_bytes(hypotheses)\n\n        reffile = tempfile.NamedTemporaryFile()\n        reffile.write(ref_bytes)\n        reffile.flush()\n\n        output_proc = subprocess.run([""perl"", self.wrapper, reffile.name],\n                                     input=hyp_bytes,\n                                     stderr=subprocess.PIPE,\n                                     stdout=subprocess.PIPE)\n\n        proc_stdout = output_proc.stdout.decode(""utf-8"")  # type: ignore\n        lines = proc_stdout.splitlines()\n\n        try:\n            bleu_score = float(lines[0])\n            return bleu_score\n        except IndexError:\n            log(""Error: Malformed output from BLEU wrapper:"", color=""red"")\n            log(proc_stdout, color=""red"")\n            log(""======="", color=""red"")\n            return 0.0\n        except ValueError:\n            log(""Value error - bleu \'{}\' is not a number."".format(lines[0]),\n                color=""red"")\n            return 0.0\n'"
neuralmonkey/evaluators/chrf.py,0,"b'from typing import List, Dict\nfrom typeguard import check_argument_types\nimport numpy as np\nfrom neuralmonkey.evaluators.evaluator import Evaluator\n\n# pylint: disable=invalid-name\nNGramDicts = List[Dict[str, int]]\n# pylint: enable=invalid-name\n\n\nclass ChrFEvaluator(Evaluator[List[str]]):\n    """"""Compute ChrF score.\n\n    See http://www.statmt.org/wmt15/pdf/WMT49.pdf\n    """"""\n\n    def __init__(self,\n                 n: int = 6,\n                 beta: float = 1.0,\n                 ignored_symbols: List[str] = None,\n                 name: str = None) -> None:\n        check_argument_types()\n\n        if name is None:\n            name = ""ChrF-{}"".format(beta)\n        super().__init__(name)\n\n        self.n = n\n        self.beta_2 = beta**2\n\n        self.ignored = []  # type: List[str]\n        if ignored_symbols is not None:\n            self.ignored = ignored_symbols\n\n    def score_instance(self,\n                       hypothesis: List[str],\n                       reference: List[str]) -> float:\n        hyp_joined = "" "".join(hypothesis)\n        hyp_chars = [x for x in list(hyp_joined) if x not in self.ignored]\n        hyp_ngrams = _get_ngrams(hyp_chars, self.n)\n\n        ref_joined = "" "".join(reference)\n        ref_chars = [x for x in list(ref_joined) if x not in self.ignored]\n        ref_ngrams = _get_ngrams(ref_chars, self.n)\n\n        if not hyp_chars or not ref_chars:\n            if """".join(hyp_chars) == """".join(ref_chars):\n                return 1.0\n            return 0.0\n\n        precision = self.chr_p(hyp_ngrams, ref_ngrams)\n        recall = self.chr_r(hyp_ngrams, ref_ngrams)\n\n        if precision == 0.0 and recall == 0.0:\n            return 0.0\n\n        return ((1 + self.beta_2) * (precision * recall)\n                / ((self.beta_2 * precision) + recall))\n\n    def chr_r(self, hyp_ngrams: NGramDicts, ref_ngrams: NGramDicts) -> float:\n        count_all = np.zeros(self.n)\n        count_matched = np.zeros(self.n)\n        for m in range(1, self.n + 1):\n            for ngr in ref_ngrams[m - 1]:\n                ref_count = ref_ngrams[m - 1][ngr]\n                count_all[m - 1] += ref_count\n                if ngr in hyp_ngrams[m - 1]:\n                    count_matched[m - 1] += min(\n                        ref_count, hyp_ngrams[m - 1][ngr])\n        return np.mean(np.divide(\n            count_matched, count_all, out=np.ones_like(count_all),\n            where=(count_all != 0)))\n\n    def chr_p(self, hyp_ngrams: NGramDicts, ref_ngrams: NGramDicts) -> float:\n        count_all = np.zeros(self.n)\n        count_matched = np.zeros(self.n)\n        for m in range(1, self.n + 1):\n            for ngr in hyp_ngrams[m - 1]:\n                hyp_count = hyp_ngrams[m - 1][ngr]\n                count_all[m - 1] += hyp_count\n                if ngr in ref_ngrams[m - 1]:\n                    count_matched[m - 1] += min(\n                        hyp_count, ref_ngrams[m - 1][ngr])\n        return np.mean(np.divide(\n            count_matched, count_all, out=np.ones_like(count_all),\n            where=(count_all != 0)))\n\n\ndef _get_ngrams(tokens: List[str], n: int) -> NGramDicts:\n    ngr_dicts = []\n    for m in range(1, n + 1):\n        ngr_dict = {}  # type: Dict[str, int]\n        for i in range(m, len(tokens) + 1):\n            ngr = """".join(tokens[i - m:i])\n            ngr_dict[ngr] = ngr_dict.setdefault(ngr, 0) + 1\n        ngr_dicts.append(ngr_dict)\n    return ngr_dicts\n\n\n# pylint: disable=invalid-name\nChrF3 = ChrFEvaluator(beta=3)\n'"
neuralmonkey/evaluators/edit_distance.py,0,"b'from typing import List\nfrom difflib import SequenceMatcher\n\nfrom neuralmonkey.evaluators.evaluator import Evaluator\n\n\nclass EditDistanceEvaluator(Evaluator[List[str]]):\n\n    # pylint: disable=no-self-use\n    def score_instance(self,\n                       hypothesis: List[str],\n                       reference: List[str]) -> float:\n        hyp_joined = "" "".join(hypothesis)\n        ref_joined = "" "".join(reference)\n\n        matcher = SequenceMatcher(None, hyp_joined, ref_joined)\n        return matcher.ratio()\n    # pylint: enable=no-self-use\n\n    def score_batch(self,\n                    hypotheses: List[List[str]],\n                    references: List[List[str]]) -> float:\n        score = super().score_batch(hypotheses, references)\n        return 1 - score\n\n    @staticmethod\n    def compare_scores(score1: float, score2: float) -> int:\n        return super().compare_scores(score2, score1)\n\n\n# pylint: disable=invalid-name\nEditDistance = EditDistanceEvaluator(""Edit distance"")\n'"
neuralmonkey/evaluators/evaluator.py,0,"b'from typing import Generic, TypeVar, List, Sequence\nfrom functools import wraps\nimport numpy as np\nfrom typeguard import check_argument_types\n\n# pylint: disable=invalid-name\nEvalType = TypeVar(""EvalType"")\nSeqEvalType = TypeVar(""SeqEvalType"", bound=Sequence)\n# pylint: enable=invalid-name\n\n\ndef check_lengths(scorer):\n    @wraps(scorer)\n    def decorate(self, hypotheses, references):\n        if len(hypotheses) != len(references):\n            raise ValueError(""Hypothesis and reference lists do not have the ""\n                             ""same length: {} vs {}."".format(len(hypotheses),\n                                                             len(references)))\n        if not hypotheses:\n            raise ValueError(""No hyp/ref pair to evaluate."")\n\n        return scorer(self, hypotheses, references)\n    return decorate\n\n\nclass Evaluator(Generic[EvalType]):\n    """"""Base class for evaluators in Neural Monkey.\n\n    Each evaluator has a `__call__` method which returns a score for a batch\n    of model predictions given a the references. This class provides default\n    implementations of `score_batch` and `score_instance` functions.\n    """"""\n\n    def __init__(self, name: str = None) -> None:\n        check_argument_types()\n        if name is None:\n            name = type(self).__name__\n            if name.endswith(""Evaluator""):\n                name = name[:-9]\n\n        self._name = name\n\n    @property\n    def name(self) -> str:\n        return self._name\n\n    # pylint: disable=no-self-use\n    # This function is meant to be overriden.\n    def score_instance(self,\n                       hypothesis: EvalType,\n                       reference: EvalType) -> float:\n        """"""Score a single hyp/ref pair.\n\n        The default implementation of this method returns 1.0 when the\n        hypothesis and the reference are equal and 0.0 otherwise.\n\n        Arguments:\n            hypothesis: The model prediction.\n            reference: The golden output.\n\n        Returns:\n            A float.\n        """"""\n        if hypothesis == reference:\n            return 1.0\n        return 0.0\n    # pylint: enable=no-self-use\n\n    @check_lengths\n    def score_batch(self,\n                    hypotheses: List[EvalType],\n                    references: List[EvalType]) -> float:\n        """"""Score a batch of hyp/ref pairs.\n\n        The default implementation of this method calls `score_instance` for\n        each instance in the batch and returns the average score.\n\n        Arguments:\n            `hypotheses`: List of model predictions.\n            `references`: List of golden outputs.\n\n        Returns:\n            A float.\n        """"""\n        return np.mean([self.score_instance(hyp, ref)\n                        for hyp, ref in zip(hypotheses, references)])\n\n    def __call__(self,\n                 hypotheses: List[EvalType],\n                 references: List[EvalType]) -> float:\n        """"""Call the evaluator on a batch of data.\n\n        By default, this function calls the `score_batch` method and returns\n        the score.\n\n        Arguments:\n            `hypotheses`: List of model predictions.\n            `references`: List of golden outputs.\n\n        Returns:\n            A float.\n        """"""\n        return self.score_batch(hypotheses, references)\n\n    @staticmethod\n    def compare_scores(score1: float, score2: float) -> int:\n        """"""Compare scores using this evaluator.\n\n        The default implementation regards the bigger score as better.\n\n        Arguments:\n            score1: The first score.\n            score2: The second score.\n\n        Returns\n            An int. When `score1` is better, returns 1. When `score2` is\n            better, returns -1. When the scores are equal, returns 0.\n        """"""\n        return (score1 > score2) - (score1 < score2)\n\n\nclass SequenceEvaluator(Evaluator[Sequence[EvalType]]):\n    """"""Base class for token-level evaluators that work with sequences.""""""\n\n    def __init__(self,\n                 name: str = None,\n                 mask_symbol: EvalType = None) -> None:\n        """"""Initialize class.\n\n        Argumets:\n            mask_symbol: Indicates which tokens to ignore during evaluation\n                based on the reference.\n        """"""\n        super().__init__(name)\n        self.mask_symbol = mask_symbol\n\n    # pylint: disable=no-self-use\n    # This method is supposed to be overriden.\n    def score_token(self,\n                    hyp_token: EvalType,\n                    ref_token: EvalType) -> float:\n        """"""Score a single hyp/ref pair of tokens.\n\n        The default implementation returns 1.0 if the tokens are equal, 0.0\n        otherwise.\n\n        Arguments:\n            hyp_token: A prediction token.\n            ref_token: A golden token.\n\n        Returns:\n            A score for the token hyp/ref pair.\n        """"""\n        return float(hyp_token == ref_token)\n    # pylint: enable=no-self-use\n\n    @check_lengths\n    def score_batch(self,\n                    hypotheses: List[Sequence[EvalType]],\n                    references: List[Sequence[EvalType]]) -> float:\n        """"""Score batch of sequences.\n\n        The default implementation assumes equal sequence lengths and operates\n        on the token level (i.e. token-level scores from the whole batch are\n        averaged (in contrast to averaging each sequence first)).\n\n        Arguments:\n            `hypotheses`: List of model predictions.\n            `references`: List of golden outputs.\n\n        Returns:\n            A float.\n        """"""\n        token_scores = [self.score_token(h, r)\n                        for hyp, ref in zip(hypotheses, references)\n                        for h, r in zip(hyp, ref)]\n        references_flat = [r for ref in references for r in ref]\n        if self.mask_symbol:\n            token_scores = [score for score, ref in zip(token_scores,\n                                                        references_flat)\n                            if ref != self.mask_symbol]\n\n        # All hypotheses empty - return zero score (needs to be here because of\n        # the flattening)\n        if not token_scores:\n            return 0.0\n\n        return np.mean(token_scores)\n'"
neuralmonkey/evaluators/f1_bio.py,0,"b'from typing import List, Set\nfrom neuralmonkey.evaluators.evaluator import Evaluator\n\n\nclass F1Evaluator(Evaluator[List[str]]):\n    """"""F1 evaluator for BIO tagging, e.g. NP chunking.\n\n    The entities are annotated as beginning of the entity (B), continuation of\n    the entity (I), the rest is outside the entity (O).\n    """"""\n\n    def score_instance(self,\n                       hypothesis: List[str],\n                       reference: List[str]) -> float:\n        set_dec = self.chunk2set(hypothesis)\n        set_ref = self.chunk2set(reference)\n\n        true_positives = len(set_dec & set_ref)\n        if true_positives == 0:\n            return 0.0\n        precision = true_positives / len(set_dec)\n        recall = true_positives / len(set_ref)\n        return 2 * precision * recall / (precision + recall)\n\n    @staticmethod\n    def chunk2set(seq: List[str]) -> Set[str]:\n        output = set()\n        sid = """"\n        inside_chunk = False\n        for i, s in enumerate(seq):\n            if not inside_chunk:\n                if s == ""B"":\n                    sid = str(i) + ""-""\n                    inside_chunk = True\n            elif s != ""I"":\n                sid += str(i - 1)\n                output.add(sid)\n                if s == ""B"":\n                    sid = str(i) + ""-""\n                    inside_chunk = True\n                else:\n                    inside_chunk = False\n        if inside_chunk:\n            sid += str(len(seq) - 1)\n            output.add(sid)\n        return output\n\n\n# pylint: disable=invalid-name\nBIOF1Score = F1Evaluator(""F1 measure"")\n# pylint: enable=invalid-name\n'"
neuralmonkey/evaluators/gleu.py,0,"b'from typing import List, Tuple\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.evaluators.bleu import BLEUEvaluator\nfrom neuralmonkey.evaluators.evaluator import Evaluator, check_lengths\n\n\nclass GLEUEvaluator(Evaluator[List[str]]):\n    """"""Sentence-level evaluation metric correlating with BLEU on corpus-level.\n\n    From ""Google\'s Neural Machine Translation System: Bridging the Gap\n    between Human and Machine Translation"" by Wu et al.\n    (https://arxiv.org/pdf/1609.08144v2.pdf)\n\n    GLEU is the minimum of recall and precision of all n-grams up to n in\n    references and hypotheses.\n\n    Ngram counts are based on the bleu methods.\n    """"""\n\n    def __init__(self,\n                 n: int = 4,\n                 deduplicate: bool = False,\n                 name: str = None) -> None:\n        check_argument_types()\n        if name is None:\n            name = ""GLEU-{}"".format(n)\n            if deduplicate:\n                name += ""-dedup""\n        super().__init__(name)\n\n        self.n = n\n        self.deduplicate = deduplicate\n\n    @check_lengths\n    def score_batch(self,\n                    hypotheses: List[List[str]],\n                    references: List[List[str]]) -> float:\n        listed_references = [[s] for s in references]\n        if self.deduplicate:\n            hypotheses = BLEUEvaluator.deduplicate_sentences(hypotheses)\n\n        return GLEUEvaluator.gleu(hypotheses, listed_references, self.n)\n\n    # pylint: disable=too-many-locals\n    @staticmethod\n    def total_precision_recall(\n            hypotheses: List[List[str]],\n            references_list: List[List[List[str]]],\n            ngrams: int,\n            case_sensitive: bool) -> Tuple[float, float]:\n        """"""Compute a modified n-gram precision and recall on a sentence list.\n\n        Arguments:\n            hypotheses: List of output sentences as lists of words\n            references_list: List of lists of reference sentences (as lists of\n                words)\n            ngrams: n-gram order\n            case_sensitive: Whether to perform case-sensitive computation\n        """"""\n        corpus_true_positives = 0\n        corpus_generated_length = 0\n        corpus_target_length = 0\n\n        for n in range(1, ngrams + 1):\n            for hypothesis, references in zip(hypotheses, references_list):\n                reference_counters = []\n\n                for reference in references:\n                    counter = BLEUEvaluator.ngram_counts(reference, n,\n                                                         not case_sensitive)\n                    reference_counters.append(counter)\n\n                reference_counts = BLEUEvaluator.merge_max_counters(\n                    reference_counters)\n                corpus_target_length += sum(reference_counts.values())\n\n                hypothesis_counts = BLEUEvaluator.ngram_counts(\n                    hypothesis, n, not case_sensitive)\n                true_positives = 0\n                for ngram in hypothesis_counts:\n                    true_positives += reference_counts[ngram]\n\n                corpus_true_positives += true_positives\n                corpus_generated_length += sum(hypothesis_counts.values())\n\n            if corpus_generated_length == 0:\n                return 0, 0\n\n        return (corpus_true_positives / corpus_generated_length,\n                corpus_true_positives / corpus_target_length)\n\n    @staticmethod\n    def gleu(hypotheses: List[List[str]],\n             references: List[List[List[str]]],\n             ngrams: int = 4,\n             case_sensitive: bool = True) -> float:\n        """"""Compute GLEU on a corpus with multiple references (no smoothing).\n\n        Arguments:\n            hypotheses: List of hypotheses\n            references: LIst of references. There can be more than one\n                reference.\n            ngrams: Maximum order of n-grams. Default 4.\n            case_sensitive: Perform case-sensitive computation. Default True.\n        """"""\n        prec, recall = GLEUEvaluator.total_precision_recall(\n            hypotheses, references, ngrams, case_sensitive)\n\n        return min(recall, prec)\n'"
neuralmonkey/evaluators/mse.py,0,"b'from typing import List\nimport numpy as np\n\nfrom neuralmonkey.evaluators.evaluator import Evaluator, SequenceEvaluator\n\n\nclass MeanSquaredErrorEvaluator(SequenceEvaluator[float]):\n    """"""Mean squared error evaluator.\n\n    Assumes equal vector length across the batch (see\n    `SequenceEvaluator.score_batch`)\n    """"""\n\n    # pylint: disable=no-self-use\n    def score_token(self, hyp_elem: float, ref_elem: float) -> float:\n        return (hyp_elem - ref_elem) ** 2\n    # pylint: enable=no-self-use\n\n    @staticmethod\n    def compare_scores(score1: float, score2: float) -> int:\n        return super().compare_scores(score2, score1)\n\n\nclass PairwiseMeanSquaredErrorEvaluator(Evaluator[List[float]]):\n    """"""Pairwise mean squared error evaluator.\n\n    For vectors of different dimension across the batch.\n    """"""\n\n    # pylint: disable=no-self-use\n    def score_instance(self,\n                       hypothesis: List[float],\n                       reference: List[float]) -> float:\n        """"""Compute mean square error between two vectors.""""""\n        return np.mean([(hyp - ref) ** 2\n                        for hyp, ref in zip(hypothesis, reference)])\n    # pylint: enable=no-self-use\n\n    @staticmethod\n    def compare_scores(score1: float, score2: float) -> int:\n        return super().compare_scores(score2, score1)\n\n\n# pylint: disable=invalid-name\nMSE = MeanSquaredErrorEvaluator()\nPairwiseMSE = PairwiseMeanSquaredErrorEvaluator()\n# pylint: enable=invalid-name\n'"
neuralmonkey/evaluators/multeval.py,0,"b'import tempfile\nimport subprocess\nfrom typing import List\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.logging import warn\nfrom neuralmonkey.evaluators.evaluator import Evaluator\n\n\n# pylint: disable=too-few-public-methods\nclass MultEvalWrapper(Evaluator[List[str]]):\n    """"""Wrapper for mult-eval\'s reference BLEU and METEOR scorer.""""""\n\n    def __init__(self,\n                 wrapper: str,\n                 name: str = ""MultEval"",\n                 encoding: str = ""utf-8"",\n                 metric: str = ""bleu"",\n                 language: str = ""en"") -> None:\n        """"""Initialize the wrapper.\n\n        Arguments:\n            wrapper: Path to multeval.sh script\n            name: Name of the evaluator\n            encoding: Encoding of input files\n            language: Language of hypotheses and references\n            metric: Evaluation metric ""bleu"", ""ter"", ""meteor""\n        """"""\n        check_argument_types()\n        super().__init__(""{}_{}_{}"".format(name, metric, language))\n\n        self.wrapper = wrapper\n        self.encoding = encoding\n        self.language = language\n        self.metric = metric\n\n        if self.metric not in [""bleu"", ""ter"", ""meteor""]:\n            warn(""{} metric is not valid. Using bleu instead."".\n                 format(self.metric))\n            self.metric = ""bleu""\n\n    def score_batch(self,\n                    hypotheses: List[List[str]],\n                    references: List[List[str]]) -> float:\n\n        ref_bytes = self.serialize_to_bytes(references)\n        hyp_bytes = self.serialize_to_bytes(hypotheses)\n\n        with tempfile.NamedTemporaryFile() as reffile, \\\n                tempfile.NamedTemporaryFile() as hypfile:\n\n            reffile.write(ref_bytes)\n            reffile.flush()\n\n            hypfile.write(hyp_bytes)\n            hypfile.flush()\n\n            args = [self.wrapper, ""eval"", ""--refs"", reffile.name,\n                    ""--hyps-baseline"", hypfile.name, ""--metrics"", self.metric]\n            if self.metric == ""meteor"":\n                args.extend([""--meteor.language"", self.language])\n                # problem: if meteor run for the first time,\n                # paraphrase tables are downloaded\n\n            output_proc = subprocess.run(\n                args, stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n\n            proc_stdout = output_proc.stdout.decode(""utf-8"")  # type: ignore\n            lines = proc_stdout.splitlines()\n\n            if not lines:\n                return 0.0\n            try:\n                filtered = float(lines[1].split()[1])\n                eval_score = filtered / 100.\n                return eval_score\n            except IndexError:\n                warn(""Error: Malformed output from MultEval wrapper:"")\n                warn(proc_stdout)\n                warn(""======="")\n                return 0.0\n            except ValueError:\n                warn(""Value error - \'{}\' is not a number."".format(lines[0]))\n                return 0.0\n\n    def serialize_to_bytes(self, sentences: List[List[str]]) -> bytes:\n        joined = ["" "".join(r) for r in sentences]\n        string = ""\\n"".join(joined) + ""\\n""\n        return string.encode(self.encoding)\n'"
neuralmonkey/evaluators/perplexity.py,0,"b'# pylint: disable=too-few-public-methods, no-self-use, unused-argument\nfrom typing import List\n\nfrom neuralmonkey.evaluators.evaluator import Evaluator\n\n\nclass PerplexityEvaluator(Evaluator[float]):\n    """"""Just 2 ** average the numeric output of a runner.\n\n    Masked position get xent of 0. The sum of crosentropies is divided by the\n    number of non-zero numbers.\n    """"""\n\n    def score_batch(self,\n                    hypotheses: List[List[float]],\n                    references: List[List[float]]) -> float:\n\n        sum_of_all = sum(\n            sum(xent for xent in hyp_xent) for hyp_xent in hypotheses)\n        count_of_all = sum(\n            sum(float(xent != 0.0) for xent in hyp_xent)\n            for hyp_xent in hypotheses)\n\n        if count_of_all == 0:\n            return float(""nan"")\n        return 2 ** (sum_of_all / count_of_all)\n'"
neuralmonkey/evaluators/rouge.py,0,"b'from typing import List\nimport rouge\nfrom typeguard import check_argument_types\nfrom neuralmonkey.evaluators.evaluator import Evaluator, check_lengths\n\n\n# pylint: disable=too-few-public-methods\nclass RougeEvaluator(Evaluator[List[str]]):\n    """"""Compute ROUGE score using third-party library.""""""\n\n    def __init__(\n            self, rouge_type: str,\n            name: str = ""ROUGE"") -> None:\n        check_argument_types()\n        super().__init__(name)\n\n        if rouge_type.lower() not in [""1"", ""2"", ""l""]:\n            raise ValueError(\n                (""Invalid type of rouge metric \'{}\', ""\n                 ""must be \'1\', \'2\' or \'L\'"").format(rouge_type))\n\n        self.rouge_type = rouge_type.lower()\n        self.rouge = rouge.Rouge()\n\n    @check_lengths\n    def score_batch(self,\n                    hypotheses: List[List[str]],\n                    references: List[List[str]]) -> float:\n        hypotheses_str = ["" "".join(l) for l in hypotheses]\n        references_str = ["" "".join(l) for l in references]\n\n        rouge_res = self.rouge.get_scores(\n            hypotheses_str, references_str, avg=True)\n\n        rouge_value = rouge_res[""rouge-{}"".format(self.rouge_type)][""f""]\n\n        return rouge_value\n\n\n# pylint: disable=invalid-name\nROUGE_1 = RougeEvaluator(""1"", ""ROUGE-1"")\nROUGE_2 = RougeEvaluator(""2"", ""ROUGE-2"")\nROUGE_L = RougeEvaluator(""l"", ""ROUGE-L"")\n# pylint: enable=invalid-name\n'"
neuralmonkey/evaluators/sacrebleu.py,0,"b'from typing import List\nfrom typeguard import check_argument_types\nfrom sacrebleu import corpus_bleu, TOKENIZERS\nfrom neuralmonkey.evaluators.evaluator import Evaluator, check_lengths\n\nSMOOTH_VARIANTS = [""exp"", ""floor"", ""none""]\n\n\n# pylint: disable=too-few-public-methods\n# TODO: Sentence-level BLEU, chrf and more could be added here.\nclass SacreBLEUEvaluator(Evaluator[List[str]]):\n    """"""SacreBLEU evaluator wrapper.""""""\n\n    def __init__(self,\n                 name: str,\n                 smooth_method: str = ""exp"",\n                 smooth_value: float = 0.0,\n                 force: bool = False,\n                 lowercase: bool = False,\n                 tokenize: str = ""none"",\n                 use_effective_order: bool = False) -> None:\n        check_argument_types()\n        super().__init__(name)\n\n        if tokenize not in TOKENIZERS:\n            raise ValueError(\n                ""Unknown tokenizer \'{}\'. You must use one of sacrebleu\'s ""\n                ""tokenizers: {}"".format(tokenize, str(TOKENIZERS)))\n\n        if smooth_method not in SMOOTH_VARIANTS:\n            raise ValueError(\n                ""Unknown smoothing \'{}\'. You must use one of sacrebleu\'s ""\n                ""smoothing methods: {}"".format(smooth_method,\n                                               str(SMOOTH_VARIANTS)))\n\n        self.smooth_method = smooth_method\n        self.smooth_value = smooth_value\n        self.force = force\n        self.lowercase = lowercase\n        self.tokenize = tokenize\n        self.use_effective_order = use_effective_order\n\n    @check_lengths\n    def score_batch(self,\n                    hypotheses: List[List[str]],\n                    references: List[List[str]]) -> float:\n\n        hyp_joined = ["" "".join(hyp) for hyp in hypotheses]\n        ref_joined = ["" "".join(ref) for ref in references]\n\n        bleu = corpus_bleu(hyp_joined, [ref_joined],\n                           smooth_method=self.smooth_method,\n                           smooth_value=self.smooth_value,\n                           force=self.force,\n                           lowercase=self.lowercase,\n                           tokenize=self.tokenize,\n                           use_effective_order=self.use_effective_order)\n\n        return bleu.score\n\n\n# pylint: disable=invalid-name\nSacreBLEU = SacreBLEUEvaluator(""BLEU"")\n'"
neuralmonkey/evaluators/ter.py,0,"b'from typing import List\nimport pyter\nfrom neuralmonkey.evaluators.evaluator import Evaluator\n\n\n# pylint: disable=too-few-public-methods\nclass TEREvaluator(Evaluator[List[str]]):\n    """"""Compute TER using the pyter library.""""""\n\n    # pylint: disable=no-self-use\n    def score_instance(self,\n                       hypothesis: List[str],\n                       reference: List[str]) -> float:\n        if reference and hypothesis:\n            return pyter.ter(hypothesis, reference)\n        if not reference and not hypothesis:\n            return 0.0\n        return 1.0\n    # pylint: enable=no-self-use\n\n    @staticmethod\n    def compare_scores(score1: float, score2: float) -> int:\n        return super().compare_scores(score2, score1)\n\n\nTER = TEREvaluator(""TER"")\n'"
neuralmonkey/evaluators/wer.py,0,"b'from typing import List\nimport pyter\nfrom neuralmonkey.evaluators.evaluator import Evaluator, check_lengths\n\n\nclass WEREvaluator(Evaluator[List[str]]):\n    """"""Compute WER (word error rate, used in speech recognition).""""""\n\n    # pylint: disable=no-self-use\n    def score_instance(self,\n                       hypothesis: List[str],\n                       reference: List[str]) -> float:\n        if reference and hypothesis:\n            return pyter.edit_distance(hypothesis, reference)\n        if not reference and not hypothesis:\n            return 0.0\n        return len(reference)\n    # pylint: enable=no-self-use\n\n    @check_lengths\n    def score_batch(self,\n                    hypotheses: List[List[str]],\n                    references: List[List[str]]) -> float:\n        total_length = 0\n        total_score = 0.0\n        for hyp, ref in zip(hypotheses, references):\n            total_score += self.score_instance(hyp, ref)\n            total_length += len(ref)\n        return total_score / total_length\n\n    @staticmethod\n    def compare_scores(score1: float, score2: float) -> int:\n        return super().compare_scores(score2, score1)\n\n\nWER = WEREvaluator(""WER"")\n'"
neuralmonkey/logbook/logbook.py,0,"b'# pylint: disable=unused-import, wrong-import-order\nimport neuralmonkey.checkpython\n# pylint: enable=unused-import, wrong-import-order\n\nimport argparse\nimport os\nimport html\nimport json\nfrom flask import Flask, Response\nfrom pygments import highlight\nfrom pygments.lexers.configs import IniLexer\nfrom pygments.formatters import HtmlFormatter\nimport ansiconv\n\n\nAPP = Flask(__name__)\nAPP.config.from_object(__name__)\nAPP.config[\'logdir\'] = None\n\n\ndef root_dir():  # pragma: no cover\n    return os.path.abspath(os.path.dirname(__file__))\n\n\ndef get_file(filename):  # pragma: no cover\n    src = os.path.join(root_dir(), filename)\n    return open(src).read()\n\n\n@APP.route(\'/\', methods=[\'GET\'])\ndef index():\n    content = get_file(\'index.html\')\n    return Response(content, mimetype=""text/html"")\n\n\n@APP.route(\'/experiments\', methods=[\'GET\'])\ndef list_experiments():\n    logdir = APP.config[\'logdir\']\n    experiment_list = [dr for dr in os.listdir(logdir)\n                       if os.path.isdir(os.path.join(logdir, dr))\n                       and os.path.isfile(os.path.join(\n                           logdir, dr, \'experiment.ini\'))]\n\n    if os.path.isfile(os.path.join(logdir, \'experiment.ini\')):\n        experiment_list.append(""."")\n\n    json_response = json.dumps({\'experiments\': experiment_list})\n\n    response = Response(json_response,\n                        content_type=\'application/json; charset=utf-8\')\n    response.headers.add(\'content-length\', len(json_response))\n    response.status_code = 200\n    return response\n\n\n@APP.route(\'/experiments/<path:path>\', methods=[\'GET\'])\ndef get_experiment(path):\n    logdir = APP.config[\'logdir\']\n    complete_path = os.path.join(logdir, path)\n    if os.path.isfile(complete_path):\n        file_content = get_file(complete_path)\n        if path.endswith("".log""):\n            result = ansiconv.to_html(html.escape(file_content))\n        elif path.endswith("".ini""):\n            lexer = IniLexer()\n            formatter = HtmlFormatter(linenos=True)\n            result = highlight(file_content, lexer, formatter)\n        else:\n            result = ""Unknown file type: \'{}\'."".format(complete_path)\n    else:\n        result = ""File \'{}\' does not exist."".format(complete_path)\n    return Response(result, mimetype=\'text/html\', status=200)\n\n\n@APP.route(""/ansiconv.css"")\ndef get_ansiconv_css():\n    return ansiconv.base_css()\n\n\n@APP.route(\'/\', defaults={\'path\': \'\'})\n@APP.route(\'/<path:path>\')\ndef get_resource(path):  # pragma: no cover\n    mimetypes = {\n        "".css"": ""text/css"",\n        "".html"": ""text/html"",\n        "".js"": ""application/javascript"",\n    }\n    try:\n        complete_path = os.path.join(root_dir(), path)\n        ext = os.path.splitext(path)[1]\n        mimetype = mimetypes.get(ext, ""text/html"")\n        content = get_file(complete_path)\n        return Response(content, mimetype=mimetype)\n    except IOError:\n        return Response(""\'{}\' not found."".format(path), status=404)\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=""Runs the Experiment LogBook server"")\n    parser.add_argument(""--port"", type=int, default=5050)\n    parser.add_argument(""--host"", type=str, default=""127.0.0.1"")\n    parser.add_argument(""--logdir"", type=str, required=True)\n    args = parser.parse_args()\n\n    logdir = os.path.abspath(args.logdir)\n\n    if not os.path.isdir(logdir):\n        print(""The log directory \'{}\' does not exist."")\n        exit(1)\n\n    APP.config[\'logdir\'] = logdir\n\n    APP.run(port=args.port, host=args.host)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
neuralmonkey/model/__init__.py,0,b''
neuralmonkey/model/feedable.py,7,"b'from abc import ABCMeta\n\nfrom typing import Any, Dict, List\n# pylint: disable=unused-import\nfrom typing import Optional\n# pylint: enable=unused-import\n\nimport tensorflow as tf\nfrom neuralmonkey.dataset import Dataset\n\n# pylint: disable=invalid-name\nFeedDict = Dict[tf.Tensor, Any]\n# pylint: enable=invalid-name\n\n\nclass Feedable(metaclass=ABCMeta):\n    """"""Base class for feedable model parts.\n\n    In TensorFlow, data is provided to the model using placeholders. Neural\n    Monkey abstraction objects, such as encoders or decoders, can be members of\n    this class in order to be able to receive data inputs from the framework.\n\n    All feedable objects have a `feed_dict` method, which gets the current\n    dataset and returns a `FeedDict` dictionary which assigns values to\n    symbolic placeholders.\n\n    Additionally, each Feedable object has two placeholders which are fed\n    automatically in this super class - `batch_size` and `train_mode`.\n    """"""\n\n    def __init__(self) -> None:\n        self.train_mode = tf.placeholder(tf.bool, [], ""train_mode"")\n        self.batch_size = tf.placeholder(tf.int32, [], ""batch_size"")\n        self._dataset = None  # type: Optional[Dict[str, tf.Tensor]]\n\n    def feed_dict(self, dataset: Dataset, train: bool = True) -> FeedDict:\n        """"""Return a feed dictionary for the given feedable object.\n\n        Arguments:\n            dataset: A dataset instance from which to get the data.\n            train: Boolean indicating whether the model runs in training mode.\n\n        Returns:\n            A `FeedDict` dictionary object.\n        """"""\n        fd = {}  # type: FeedDict\n        fd[self.train_mode] = train\n        fd[self.batch_size] = len(dataset)\n        return fd\n\n    @property\n    def input_types(self) -> Dict[str, tf.DType]:\n        return {}\n\n    @property\n    def input_shapes(self) -> Dict[str, List[int]]:\n        return {}\n\n    @property\n    def dataset(self) -> Dict[str, tf.Tensor]:\n        if self._dataset is None:\n            raise RuntimeError(""Getting dataset before registering it."")\n        return self._dataset\n\n    def register_input(self, dataset: Dict[str, tf.Tensor]) -> None:\n        self._dataset = dataset\n'"
neuralmonkey/model/gradient_blocking.py,8,"b'""""""Module that blocks gradient propagation to model parts.""""""\nfrom typing import List\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.model.stateful import (\n    Stateful, TemporalStateful, SpatialStateful)\n\n\nclass StatefulView(Stateful):\n    """"""Provides a gradient-blocking view of a `Stateful` object.""""""\n\n    def __init__(self, blocked_object: Stateful) -> None:\n        check_argument_types()\n        self._blocked_object = blocked_object\n\n    @tensor\n    def output(self) -> tf.Tensor:\n        return tf.stop_gradient(self._blocked_object.output)\n\n    @property\n    def dependencies(self) -> List[str]:\n        return super().dependencies + [""_blocked_object""]\n\n\nclass TemporalStatefulView(TemporalStateful):\n    """"""Provides a gradient-blocking view of a `TemporalStateful` object.""""""\n\n    def __init__(self, blocked_object: TemporalStateful) -> None:\n        check_argument_types()\n        self._blocked_object = blocked_object\n\n    @tensor\n    def temporal_states(self) -> tf.Tensor:\n        return tf.stop_gradient(self._blocked_object.temporal_states)\n\n    @property\n    def temporal_mask(self) -> tf.Tensor:\n        return self._blocked_object.temporal_mask\n\n    @property\n    def dependencies(self) -> List[str]:\n        return super().dependencies + [""_blocked_object""]\n\n\nclass SpatialStatefulView(SpatialStateful):\n    """"""Provides a gradient-blocking view of a `SpatialStateful` object.""""""\n\n    def __init__(self, blocked_object: SpatialStateful) -> None:\n        check_argument_types()\n        self._blocked_object = blocked_object\n\n    @tensor\n    def spatial_states(self) -> tf.Tensor:\n        return tf.stop_gradient(self._blocked_object.spatial_states)\n\n    @property\n    def spatial_mask(self) -> tf.Tensor:\n        return self._blocked_object.spatial_mask\n\n    @property\n    def dependencies(self) -> List[str]:\n        return super().dependencies + [""_blocked_object""]\n'"
neuralmonkey/model/gradient_reversal.py,8,"b'""""""Module for reverting grandients when passing a model part.""""""\n\nfrom typing import List\nimport tensorflow as tf\n# pylint: disable=no-name-in-module\nfrom tensorflow.python.framework import ops\n# pylint: disable=no-name-in-module\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.model.stateful import (\n    Stateful, TemporalStateful, SpatialStateful)\n\n\ndef _reverse_gradient(x: tf.Tensor) -> tf.Tensor:\n    """"""Flips the sign of the incoming gradient during training.""""""\n\n    grad_name = ""gradient_reversal_{}"".format(x.name)\n\n    # pylint: disable=unused-variable,invalid-name,unused-argument\n    @ops.RegisterGradient(grad_name)\n    def _flip_gradients(op, grad):\n        return [tf.negative(grad)]\n    # pylint: enable=unused-variable,invalid-name,unused-argument\n\n    from neuralmonkey.experiment import Experiment\n    graph = Experiment.get_current().graph\n    with graph.gradient_override_map({""Identity"": grad_name}):\n        y = tf.identity(x)\n\n    return y\n\n\nclass StatefulView(Stateful):\n    """"""Provides an adversarial view of a `Stateful` object.""""""\n\n    def __init__(self, reversed_object: Stateful) -> None:\n        check_argument_types()\n        self._reversed_object = reversed_object\n\n    @tensor\n    def output(self) -> tf.Tensor:\n        return _reverse_gradient(self._reversed_object.output)\n\n    @property\n    def dependencies(self) -> List[str]:\n        return super().dependencies + [""_reversed_object""]\n\n\nclass TemporalStatefulView(TemporalStateful):\n    """"""Provides an adversarial view of a `TemporalStateful` object.""""""\n\n    def __init__(self, reversed_object: TemporalStateful) -> None:\n        check_argument_types()\n        self._reversed_object = reversed_object\n\n    @tensor\n    def temporal_states(self) -> tf.Tensor:\n        return _reverse_gradient(self._reversed_object.temporal_states)\n\n    @property\n    def temporal_mask(self) -> tf.Tensor:\n        return self._reversed_object.temporal_mask\n\n    @property\n    def dependencies(self) -> List[str]:\n        return super().dependencies + [""_reversed_object""]\n\n\nclass SpatialStatefulView(SpatialStateful):\n    """"""Provides an adversarial view of a `SpatialStateful` object.""""""\n\n    def __init__(self, reversed_object: SpatialStateful) -> None:\n        check_argument_types()\n        self._reversed_object = reversed_object\n\n    @tensor\n    def spatial_states(self) -> tf.Tensor:\n        return _reverse_gradient(self._reversed_object.spatial_states)\n\n    @property\n    def spatial_mask(self) -> tf.Tensor:\n        return self._reversed_object.spatial_mask\n\n    @property\n    def dependencies(self) -> List[str]:\n        return super().dependencies + [""_reversed_object""]\n'"
neuralmonkey/model/model_part.py,0,"b'""""""Basic functionality of all model parts.""""""\nfrom abc import ABCMeta\nfrom typing import MutableSet, Set, List, Tuple, Iterable\n\nfrom neuralmonkey.model.parameterized import Parameterized, InitializerSpecs\nfrom neuralmonkey.model.feedable import Feedable\n\n\nclass GenericModelPart(metaclass=ABCMeta):\n    """"""Base class for Neural Monkey model parts.\n\n    Neural Monkey dynamically decides which model parts are in use when using a\n    specific trainer or a runner. Each trainer/runner holds a reference to a\n    top-level model part, which is then responsible for collecting references\n    to all `Parameterized` and `Feedable` objects that contribute to the\n    computation of its Tensors. This behavior is implemented using the\n    `get_dependencies` method, which is called recursively on all instances of\n    `GenericModelPart` class that are references from within a model part.\n\n    Apart from the `get_dependencies` method, this class also provides the\n    `dependencies` property which store the names of the Python class\n    attributes that are regarded as potential dependents of the\n    `GenericModelPart` object. These dependents are automatically checked for\n    type and when they are instances of the `GenericModelPart` class, results\n    of their `get_dependencies` are united and returned as dependencies of the\n    current object.\n    """"""\n\n    @property\n    def dependencies(self) -> List[str]:\n        """"""Return a list of attribute names regarded as dependents.""""""\n        return [""encoder"", ""parent_decoder"", ""input_sequence"", ""attentions"",\n                ""encoders""]\n\n    def __get_deps(\n            self,\n            attr: str,\n            feedables: MutableSet[Feedable],\n            parameterizeds: MutableSet[Parameterized]) -> None:\n\n        attr_val = getattr(self, attr, None)\n\n        if attr_val is None:\n            return\n\n        deps = []  # type: List[GenericModelPart]\n        if isinstance(attr_val, GenericModelPart):\n            deps = [attr_val]\n        elif isinstance(attr_val, Iterable):\n            deps = [a for a in attr_val if isinstance(a, GenericModelPart)]\n\n        for dep in deps:\n            feeds, params = dep.get_dependencies()\n            feedables |= feeds\n            parameterizeds |= params\n\n    def get_dependencies(self) -> Tuple[Set[Feedable], Set[Parameterized]]:\n        """"""Collect all dependents of this object recursively.\n\n        The dependents are collected using the `dependencies` property. Each\n        stores a potential dependent object. If the object exsits and is an\n        instance of `GenericModelPart`, dependents are collected recursively by\n        calling its `get_dependencies` method.\n\n        If the object itself is instance of `Feedable` or `Parameterized`\n        class, it is added among the respective sets returned.\n\n        Returns:\n            A `Tuple` of `Set`s of `Feedable` and `Parameterized` objects.\n\n        """"""\n        feedables = set()  # type: Set[Feedable]\n        parameterizeds = set()  # type: Set[Parameterized]\n\n        if isinstance(self, Feedable):\n            feedables |= {self}\n        if isinstance(self, Parameterized):\n            parameterizeds |= {self}\n\n        for attr in self.dependencies:\n            self.__get_deps(attr, feedables, parameterizeds)\n\n        return feedables, parameterizeds\n\n\nclass ModelPart(Parameterized, GenericModelPart, Feedable):\n    """"""Base class of all parametric feedable model parts.\n\n    Serves as a syntactic sugar for labeling `Feedable`, `Parameterized`, and\n    `GenericModelPart` objects.\n    """"""\n\n    def __init__(self,\n                 name: str,\n                 reuse: ""ModelPart"" = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        Parameterized.__init__(self, name, reuse, save_checkpoint,\n                               load_checkpoint, initializers)\n        GenericModelPart.__init__(self)\n        with self.use_scope():\n            Feedable.__init__(self)\n'"
neuralmonkey/model/parameterized.py,11,"b'from abc import ABCMeta\nfrom contextlib import contextmanager\nfrom typing import List, Tuple, Callable, Iterator\n\nimport tensorflow as tf\n\nfrom neuralmonkey.tf_utils import update_initializers\nfrom neuralmonkey.logging import log, warn\n\n# pylint: enable=invalid-name\nInitializerSpecs = List[Tuple[str, Callable]]\n# pylint: disable=invalid-name\n\n\nclass Parameterized(metaclass=ABCMeta):\n    """"""Base class for parameterized model parts.\n\n    This class is an abstraction for all model parts which use TensorFlow\n    variables. Shared properties and characteristics of all these objects\n    are the capability of loading and saving the variables, re-using variables\n    from a different `Parameterized` object, and managing variable scopes,\n    including overriding the default initializer settings for the variables.\n    """"""\n\n    def __init__(self,\n                 name: str,\n                 reuse: ""Parameterized"" = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        """"""Construct a new parameterized object.\n\n        Arguments:\n            name: The name for the model part. Will be used in the variable\n                and name scopes.\n            reuse: Optional parameterized part with which to share parameters.\n            save_checkpoint: Optional path to a checkpoint file which will\n                store the parameters of this object.\n            load_checkpoint: Optional path to a checkpoint file from which to\n                load initial variables for this object.\n            initializers: An `InitializerSpecs` instance with specification\n                of the initializers.\n        """"""\n        self._name = name\n        self._save_checkpoint = save_checkpoint\n        self._load_checkpoint = load_checkpoint\n\n        self._saver = None  # type: tf.train.Saver\n        self._reuse = reuse is not None\n\n        if reuse is not None:\n            # pylint: disable=unidiomatic-typecheck\n            # Here we need an exact match of types\n            if type(self) != type(reuse):\n                warn(""Warning: sharing parameters between model parts of ""\n                     ""different types."")\n            # pylint: enable=unidiomatic-typecheck\n\n            if initializers is not None:\n                raise ValueError(""Cannot use initializers in model part \'{}\' ""\n                                 ""that reuses variables from \'{}\'.""\n                                 .format(name, reuse.name))\n\n            # pylint: disable=protected-access\n            self._variable_scope = reuse._variable_scope  # type: ignore\n            # pylint: enable=protected-access\n        else:\n            with tf.variable_scope(name) as scope:\n                self._variable_scope = scope\n                if initializers is not None:\n                    update_initializers((scope.name + ""/"" + name, initializer)\n                                        for name, initializer in initializers)\n\n    @property\n    def name(self) -> str:\n        """"""Get the name of the parameterized object and its variable scope.""""""\n        return self._name\n\n    def __str__(self) -> str:\n        """"""Return the name of the object.""""""\n        return self.name\n\n    @contextmanager\n    def use_scope(self) -> Iterator[None]:\n        """"""Return the object variable scope context manager.\n\n        Return the context manager that (re)opens variable and name scopes of\n        the parameterized object..\n        """"""\n        # If we are already reusing, reuse regardless of self._reuse.\n        reuse = self._variable_scope.reuse or self._reuse\n        if not reuse:\n            reuse = tf.AUTO_REUSE\n\n        with tf.variable_scope(self._variable_scope, reuse=reuse):\n            # tf.variable_scope always creates a NEW name scope for ops, but\n            # we want to use the original one:\n            with tf.name_scope(self._variable_scope.original_name_scope):\n                yield\n\n    def _init_saver(self) -> None:\n        if not self._saver:\n            parts_variables = tf.get_collection(\n                tf.GraphKeys.GLOBAL_VARIABLES, scope=self._variable_scope.name)\n\n            with self.use_scope():\n                self._saver = tf.train.Saver(var_list=parts_variables)\n\n    def save(self, session: tf.Session) -> None:\n        """"""Save model part to a checkpoint file.""""""\n        if self._save_checkpoint:\n            self._init_saver()\n            self._saver.save(session, self._save_checkpoint)\n\n            log(""Variables of \'{}\' saved to \'{}\'"".format(\n                self.name, self._save_checkpoint))\n\n    def load(self, session: tf.Session) -> None:\n        """"""Load model part from a checkpoint file.""""""\n        if self._load_checkpoint:\n            self._init_saver()\n            self._saver.restore(session, self._load_checkpoint)\n\n            log(""Variables of \'{}\' loaded from \'{}\'"".format(\n                self.name, self._load_checkpoint))\n'"
neuralmonkey/model/sequence.py,15,"b'""""""Module which impements the sequence class and a few of its subclasses.""""""\n\nfrom typing import List, Dict\n\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.dataset import Dataset\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.model.feedable import FeedDict\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.model.parameterized import InitializerSpecs\nfrom neuralmonkey.model.stateful import TemporalStateful\nfrom neuralmonkey.tf_utils import get_variable\nfrom neuralmonkey.vocabulary import Vocabulary, pad_batch, sentence_mask\n\n\n# pylint: disable=abstract-method\nclass Sequence(ModelPart, TemporalStateful):\n    """"""Base class for a data sequence.\n\n    This abstract class represents a batch of sequences of Tensors of possibly\n    different lengths.\n\n    Sequence is essentialy a temporal stateful object whose states and mask\n    are fed, or computed from fed values. It is also a ModelPart, and\n    therefore, it can store variables such as embedding matrices.\n    """"""\n\n    def __init__(self,\n                 name: str,\n                 max_length: int = None,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        """"""Construct a new `Sequence` object.\n\n        Arguments:\n            name: The name for the `ModelPart` object\n            max_length: Maximum length of sequences in the object (not checked)\n            save_checkpoint: The save_checkpoint parameter for `ModelPart`\n            load_checkpoint: The load_checkpoint parameter for `ModelPart`\n        """"""\n        ModelPart.__init__(self, name, reuse, save_checkpoint, load_checkpoint,\n                           initializers)\n\n        self.max_length = max_length\n        if self.max_length is not None and self.max_length <= 0:\n            raise ValueError(""Max sequence length must be a positive integer."")\n# pylint: enable=abstract-method\n\n\nclass EmbeddedFactorSequence(Sequence):\n    """"""A sequence that stores one or more embedded inputs (factors).""""""\n\n    # pylint: disable=too-many-arguments,too-many-locals\n    def __init__(self,\n                 name: str,\n                 vocabularies: List[Vocabulary],\n                 data_ids: List[str],\n                 embedding_sizes: List[int],\n                 max_length: int = None,\n                 add_start_symbol: bool = False,\n                 add_end_symbol: bool = False,\n                 scale_embeddings_by_depth: bool = False,\n                 trainable: bool = True,\n                 embeddings_source: ""EmbeddedFactorSequence"" = None,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        """"""Construct a new instance of `EmbeddedFactorSequence`.\n\n        Takes three lists of vocabularies, data series IDs, and embedding\n        sizes and construct a `Sequence` object. The supplied lists must be\n        equal in length and the indices to these lists must correspond\n        to each other\n\n        Arguments:\n            name: The name for the `ModelPart` object\n            vocabularies: A list of `Vocabulary` objects used for each factor\n            data_ids: A list of strings identifying the data series used for\n                each factor\n            embedding_sizes: A list of integers specifying the size of the\n                embedding vector for each factor\n            max_length: The maximum length of the sequences\n            add_start_symbol: Includes <s> in the sequence\n            add_end_symbol: Includes </s> in the sequence\n            scale_embeddings_by_depth: Set to True for T2T import compatibility\n            embeddings_source: EmbeddedSequence from which the embeedings will\n                be reused.\n            save_checkpoint: The save_checkpoint parameter for `ModelPart`\n            load_checkpoint: The load_checkpoint parameter for `ModelPart`\n        """"""\n        check_argument_types()\n        Sequence.__init__(\n            self, name, max_length, reuse, save_checkpoint, load_checkpoint,\n            initializers)\n\n        self.vocabularies = vocabularies\n        self.vocabulary_sizes = [len(vocab) for vocab in self.vocabularies]\n        self.data_ids = data_ids\n        self.embedding_sizes = embedding_sizes\n        self.add_start_symbol = add_start_symbol\n        self.add_end_symbol = add_end_symbol\n        self.scale_embeddings_by_depth = scale_embeddings_by_depth\n        self.embeddings_source = embeddings_source\n        self.trainable = trainable\n\n        if not (len(self.data_ids)\n                == len(self.vocabularies)\n                == len(self.embedding_sizes)):\n            raise ValueError(""data_ids, vocabularies, and embedding_sizes ""\n                             ""lists need to have the same length"")\n\n        if any([esize <= 0 for esize in self.embedding_sizes]):\n            raise ValueError(""Embedding size must be a positive integer."")\n\n        if embeddings_source is not None:\n            if not all(v1 == v2 for v1, v2 in zip(\n                    self.vocabularies, embeddings_source.vocabularies)):\n                raise ValueError(\n                    ""When reusing embeedings, vocabularies must be the same."")\n            if not all(s1 == s2 for s1, s2 in zip(\n                    self.embedding_sizes, embeddings_source.embedding_sizes)):\n                raise ValueError(\n                    ""When reusing embeedings, embeddings sizes must be equal."")\n\n        self._variable_scope.set_initializer(\n            tf.random_normal_initializer(stddev=0.001))\n    # pylint: enable=too-many-arguments,too-many-locals\n\n    @property\n    def input_types(self) -> Dict[str, tf.DType]:\n        return {d_id: tf.string for d_id in self.data_ids}\n\n    @property\n    def input_shapes(self) -> Dict[str, tf.TensorShape]:\n        return {d_id: tf.TensorShape([None, None]) for d_id in self.data_ids}\n\n    @tensor\n    def input_factor_indices(self) -> List[tf.Tensor]:\n        return [vocab.strings_to_indices(factor) for\n                vocab, factor in zip(self.vocabularies, self.input_factors)]\n\n    @tensor\n    def input_factors(self) -> List[tf.Tensor]:\n        return [self.dataset[s_id] for s_id in self.data_ids]\n\n    @tensor\n    def embedding_matrices(self) -> List[tf.Tensor]:\n        """"""Return a list of embedding matrices for each factor.""""""\n\n        # Note: Embedding matrices are numbered rather than named by the data\n        # id so the data_id string does not need to be the same across\n        # experiments\n\n        if self.embeddings_source is not None:\n            return self.embeddings_source.embedding_matrices\n\n        return [\n            get_variable(\n                name=""embedding_matrix_{}"".format(i),\n                shape=[vocab_size, emb_size],\n                trainable=self.trainable)\n            for i, (data_id, vocab_size, emb_size) in enumerate(zip(\n                self.data_ids, self.vocabulary_sizes, self.embedding_sizes))]\n\n    @tensor\n    def temporal_states(self) -> tf.Tensor:\n        """"""Return the embedded factors.\n\n        A 3D Tensor of shape (batch, time, dimension),\n        where dimension is the sum of the embedding sizes supplied to the\n        constructor.\n        """"""\n        embedded_factors = []\n        for (factor, embedding_matrix) in zip(\n                self.input_factor_indices, self.embedding_matrices):\n            emb_factor = tf.nn.embedding_lookup(embedding_matrix, factor)\n\n            # github.com/tensorflow/tensor2tensor/blob/v1.5.6/tensor2tensor/\n            #            layers/modalities.py#L104\n            if self.scale_embeddings_by_depth:\n                emb_size = embedding_matrix.shape.as_list()[-1]\n                emb_factor *= emb_size**0.5\n\n            # We explicitly set paddings to zero-value vectors\n            # TODO: remove unnecessary masking in the subesquent modules\n            emb_factor = emb_factor * tf.expand_dims(self.temporal_mask, -1)\n            embedded_factors.append(emb_factor)\n\n        return tf.concat(embedded_factors, 2)\n\n    # pylint: disable=unsubscriptable-object\n    @tensor\n    def temporal_mask(self) -> tf.Tensor:\n        return sentence_mask(self.input_factor_indices[0])\n    # pylint: enable=unsubscriptable-object\n\n    def feed_dict(self, dataset: Dataset, train: bool = False) -> FeedDict:\n        """"""Feed the placholders with the data.\n\n        Arguments:\n            dataset: The dataset.\n            train: A flag whether the train mode is enabled.\n\n        Returns:\n            The constructed feed dictionary that contains the factor data and\n            the mask.\n        """"""\n        fd = ModelPart.feed_dict(self, dataset, train)\n\n        # for checking the lengths of individual factors\n        for factor_plc, name in zip(self.input_factors, self.data_ids):\n            sentences = dataset.get_series(name)\n            fd[factor_plc] = pad_batch(\n                list(sentences), self.max_length, self.add_start_symbol,\n                self.add_end_symbol)\n\n        return fd\n\n\nclass EmbeddedSequence(EmbeddedFactorSequence):\n    """"""A sequence of embedded inputs (for a single factor).""""""\n\n    # pylint: disable=too-many-arguments,too-many-locals\n    def __init__(self,\n                 name: str,\n                 vocabulary: Vocabulary,\n                 data_id: str,\n                 embedding_size: int,\n                 max_length: int = None,\n                 add_start_symbol: bool = False,\n                 add_end_symbol: bool = False,\n                 scale_embeddings_by_depth: bool = False,\n                 trainable: bool = True,\n                 embeddings_source: ""EmbeddedSequence"" = None,\n                 reuse: ModelPart = None,\n                 save_checkpoint: str = None,\n                 load_checkpoint: str = None,\n                 initializers: InitializerSpecs = None) -> None:\n        """"""Construct a new instance of `EmbeddedSequence`.\n\n        Arguments:\n            name: The name for the `ModelPart` object\n            vocabulary: A `Vocabulary` object used for the sequence data\n            data_id: A string that identifies the data series used for\n                the sequence data\n            embedding_sizes: An integer that specifies the size of the\n                embedding vector for the sequence data\n            max_length: The maximum length of the sequences\n            add_start_symbol: Includes <s> in the sequence\n            add_end_symbol: Includes </s> in the sequence\n            scale_embeddings_by_depth: Set to True for T2T import compatibility\n            embeddings_source: `EmbeddedSequence` from which the embeedings\n                will be reused.\n            save_checkpoint: The save_checkpoint parameter for `ModelPart`\n            load_checkpoint: The load_checkpoint parameter for `ModelPart`\n        """"""\n        EmbeddedFactorSequence.__init__(\n            self,\n            name=name,\n            vocabularies=[vocabulary],\n            data_ids=[data_id],\n            embedding_sizes=[embedding_size],\n            max_length=max_length,\n            add_start_symbol=add_start_symbol,\n            add_end_symbol=add_end_symbol,\n            scale_embeddings_by_depth=scale_embeddings_by_depth,\n            trainable=trainable,\n            embeddings_source=embeddings_source,\n            reuse=reuse,\n            save_checkpoint=save_checkpoint,\n            load_checkpoint=load_checkpoint,\n            initializers=initializers)\n    # pylint: enable=too-many-arguments,too-many-locals\n\n    # pylint: disable=unsubscriptable-object\n    @property\n    def inputs(self) -> tf.Tensor:\n        """"""Return a 2D placeholder for the sequence inputs.""""""\n        return self.input_factor_indices[0]\n\n    @property\n    def embedding_matrix(self) -> tf.Tensor:\n        """"""Return the embedding matrix for the sequence.""""""\n        return self.embedding_matrices[0]\n    # pylint: enable=unsubscriptable-object\n\n    @property\n    def vocabulary(self) -> Vocabulary:\n        """"""Return the input vocabulary.""""""\n        return self.vocabularies[0]\n\n    @property\n    def data_id(self) -> str:\n        """"""Return the input data series indentifier.""""""\n        return self.data_ids[0]\n'"
neuralmonkey/model/sequence_split.py,10,"b'""""""Split temporal states such that the sequence is n-times longer.""""""\nfrom typing import Callable, List\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.dataset import Dataset\nfrom neuralmonkey.model.feedable import FeedDict\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.model.stateful import TemporalStateful\n\n\nActivation = Callable[[tf.Tensor], tf.Tensor]\n\n\nclass SequenceSplitter(TemporalStateful, ModelPart):\n    def __init__(\n            self,\n            name: str,\n            parent: TemporalStateful,\n            factor: int,\n            projection_size: int = None,\n            projection_activation: Activation = None) -> None:\n        """"""Initialize SentenceSplitter.\n\n        Args:\n            parent: TemporalStateful whose states will be split.\n            factor: Factor by which the states will be split - the  resulting\n                sequence will be longer by this factor.\n            projection_size: If not None, specifies dimensionality of a\n                projection before state splitting.\n            projection_activation: Non-linearity function for the optional\n                projection.\n        """"""\n        check_argument_types()\n\n        ModelPart.__init__(\n            self, name=name, save_checkpoint=None, load_checkpoint=None,\n            initializers=None)\n        self.parent = parent\n        self.factor = factor\n        self.projection_size = projection_size\n        self.activation = projection_activation\n\n        if projection_size is not None and projection_size % factor != 0:\n            raise ValueError((\n                ""Dimension of projection ({}) must be ""\n                ""dividable by the given factor ({})."").format(\n                    projection_size, factor))\n\n    @tensor\n    def temporal_states(self) -> tf.Tensor:\n        states = self.parent.temporal_states\n        if self.projection_size:\n            states = tf.layers.dense(\n                states, self.projection_size, activation=self.activation)\n\n        return split_by_factor(states, self.batch_size, self.factor)\n\n    @tensor\n    def temporal_mask(self) -> tf.Tensor:\n        double_mask = tf.stack(\n            self.factor * [tf.expand_dims(self.parent.temporal_mask, 2)],\n            axis=2)\n        return tf.squeeze(\n            split_by_factor(double_mask, self.batch_size, self.factor), axis=2)\n\n    def feed_dict(self, dataset: Dataset, train: bool = True) -> FeedDict:\n        return ModelPart.feed_dict(self, dataset, train)\n\n    @property\n    def dependencies(self) -> List[str]:\n        return super().dependencies + [""parent""]\n\n\ndef split_by_factor(\n        tensor_3d: tf.Tensor, batch_size: tf.Tensor, factor: int) -> tf.Tensor:\n    max_time = tf.shape(tensor_3d)[1]\n    state_dim = tensor_3d.get_shape()[2].value\n\n    if state_dim % factor != 0:\n        raise ValueError((\n            ""Dimension of the tensor ({}) must be dividable by the given ""\n            ""factor ({})."").format(state_dim, factor))\n\n    return tf.reshape(\n        tensor_3d, [batch_size, max_time * factor, state_dim // factor])\n'"
neuralmonkey/model/stateful.py,7,"b'""""""Module that provides classes that encapsulate model parts with states.\n\nThere are three classes: `Stateful`, `TemporalStateful`, and `SpatialStateful`.\n\nModel parts that do not keep states in time but have a single tensor on the\noutput should be instances of `Stateful`. Model parts that keep their hidden\nstates in a time-oriented list (e.g. recurrent encoder) should be instances\nof `TemporalStateful`. Model parts that keep the states in a 2D matrix (e.g.\nimage encoders) should be instances of `SpatialStateful`.\n\nThere are also classes that inherit from both stateful and temporal or spatial\nstateful (e.g. `TemporalStatefulWithOutput`) that can be used for model parts\nthat satisfy more requirements (e.g. recurrent encoder).\n""""""\nfrom abc import abstractproperty\nimport tensorflow as tf\nfrom neuralmonkey.model.model_part import GenericModelPart\n\n\n# pylint: disable=too-few-public-methods\n# pydocstyle: disable=\nclass Stateful(GenericModelPart):\n    @abstractproperty\n    def output(self) -> tf.Tensor:\n        """"""Return the object output.\n\n        A 2D `Tensor` of shape (batch, state_size) which contains the\n        resulting state of the object.\n        """"""\n        raise NotImplementedError(""Abstract property"")\n# pylint: enable=too-few-public-methods\n\n\nclass TemporalStateful(GenericModelPart):\n    @abstractproperty\n    def temporal_states(self) -> tf.Tensor:\n        """"""Return object states in time.\n\n        A 3D `Tensor` of shape (batch, time, state_size) which contains the\n        states of the object in time (e.g. hidden states of a recurrent\n        encoder.\n        """"""\n        raise NotImplementedError(""Abstract property"")\n\n    @abstractproperty\n    def temporal_mask(self) -> tf.Tensor:\n        """"""Return mask for the temporal_states.\n\n        A 2D `Tensor` of shape (batch, time) of type float32 which masks the\n        temporal states so each sequence can have a different length. It should\n        only contain ones or zeros.\n        """"""\n        raise NotImplementedError(""Abstract property"")\n\n    @property\n    def lengths(self) -> tf.Tensor:\n        """"""Return the sequence lengths.\n\n        A 1D `Tensor` of type `int32` that stores the lengths of the\n        state sequences in the batch.\n        """"""\n        return tf.to_int32(tf.reduce_sum(self.temporal_mask, 1))\n\n    @property\n    def dimension(self) -> int:\n        """"""Return the dimension of the states.""""""\n        return self.temporal_states.get_shape()[-1].value\n\n\nclass SpatialStateful(GenericModelPart):\n    @property\n    def spatial_states(self) -> tf.Tensor:\n        """"""Return object states in space.\n\n        A 4D `Tensor` of shape (batch, width, height, state_size) which\n        contains the states of the object in space (e.g. final layer of a\n        convolution network processing an image.\n        """"""\n        raise NotImplementedError(""Abstract property"")\n\n    @abstractproperty\n    def spatial_mask(self) -> tf.Tensor:\n        """"""Return mask for the spatial_states.\n\n        A 3D `Tensor` of shape (batch, width, height) of type float32\n        which masks the spatial states that they can be of different shapes.\n        The mask should only contain ones or zeros.\n        """"""\n        raise NotImplementedError(""Abstract property"")\n\n    @property\n    def dimension(self) -> int:\n        """"""Return the dimension of the states.""""""\n        return self.spatial_states.get_shape()[-1].value\n\n\n# pylint: disable=abstract-method\nclass TemporalStatefulWithOutput(Stateful, TemporalStateful):\n    pass\n\n\nclass SpatialStatefulWithOutput(Stateful, SpatialStateful):\n    pass\n'"
neuralmonkey/nn/__init__.py,0,b''
neuralmonkey/nn/highway.py,13,"b'""""""Module implementing the highway networks.""""""\nimport tensorflow as tf\nfrom neuralmonkey.tf_utils import get_variable\n\n\ndef highway(inputs, activation=tf.nn.relu, scope=""HighwayNetwork""):\n    """"""Create a single highway layer.\n\n    y = H(x, Wh) * T(x, Wt) + x * C(x, Wc)\n\n    where:\n\n    C(x, Wc) = 1 - T(x, Wt)\n\n    Arguments:\n        inputs: A tensor or list of tensors. It should be 2D tensors with\n                equal length in the first dimension (batch size)\n        activation: Activation function of the linear part of the formula\n                H(x, Wh).\n        scope: The name of the scope used for the variables.\n\n    Returns:\n        A tensor of shape tf.shape(inputs)\n    """"""\n    with tf.variable_scope(scope):\n        if isinstance(inputs, list):\n            # if there is a list of tensor on the input, concatenate along\n            # the last dimension and project.\n            inputs = tf.concat(inputs, axis=-1)\n\n        vec_size = inputs.get_shape().as_list()[-1]\n\n        # pylint: disable=invalid-name\n        W_shape = [vec_size, vec_size]\n        b_shape = [vec_size]\n\n        W_H = get_variable(""weight_H"", shape=W_shape)\n        b_H = get_variable(""bias_H"", shape=b_shape,\n                           initializer=tf.constant_initializer(-1.0))\n\n        W_T = get_variable(""weight_T"", shape=W_shape)\n        b_T = get_variable(""bias_T"", shape=b_shape,\n                           initializer=tf.constant_initializer(-1.0))\n\n        T = tf.sigmoid(\n            tf.add(tf.matmul(inputs, W_T), b_T),\n            name=""transform_gate"")\n        H = activation(\n            tf.add(tf.matmul(inputs, W_H), b_H),\n            name=""activation"")\n        C = tf.subtract(1.0, T, name=""carry_gate"")\n\n        y = tf.add(\n            tf.multiply(H, T),\n            tf.multiply(inputs, C),\n            ""y"")\n        return y\n'"
neuralmonkey/nn/mlp.py,9,"b'from typing import List, Callable\nimport tensorflow as tf\n\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.nn.projection import multilayer_projection\n\n\nclass MultilayerPerceptron:\n    """"""General implementation of the multilayer perceptron.""""""\n\n    # pylint: disable=too-many-arguments\n    def __init__(self,\n                 mlp_input: tf.Tensor,\n                 layer_configuration: List[int],\n                 dropout_keep_prob: float,\n                 output_size: int,\n                 train_mode: tf.Tensor,\n                 activation_fn: Callable[[tf.Tensor], tf.Tensor] = tf.nn.relu,\n                 name: str = ""multilayer_perceptron"") -> None:\n\n        with tf.variable_scope(name):\n            last_layer = multilayer_projection(\n                mlp_input, layer_configuration, activation=activation_fn,\n                dropout_keep_prob=dropout_keep_prob, train_mode=train_mode,\n                scope=""deep_output_mlp"")\n\n            self.logits = tf.layers.dense(\n                last_layer, output_size, name=""classification_layer"")\n\n    @tensor\n    def softmax(self):\n        with tf.variable_scope(""classification_layer""):\n            return tf.nn.softmax(self.logits, name=""decision_softmax"")\n\n    @tensor\n    def classification(self):\n        with tf.variable_scope(""classification_layer""):\n            return tf.argmax(self.logits, 1)\n'"
neuralmonkey/nn/noisy_gru_cell.py,16,"b'import math\nfrom typing import Tuple\n\nimport tensorflow as tf\n\n\nclass NoisyGRUCell(tf.contrib.rnn.RNNCell):\n    """"""Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).\n\n    GRU with noisy activation functions (http://arxiv.org/abs/1603.00391).\n    The theano code is availble at https://github.com/caglar/noisy_units.\n\n    It is based on the TensorFlow implementatin of GRU just the activation\n    function are changed for the noisy ones.\n    """"""\n\n    def __init__(self, num_units: int, training) -> None:\n        self._num_units = num_units\n        self.training = training\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    @property\n    def state_size(self):\n        return self._num_units\n\n    def __call__(self, inputs, state,\n                 scope=None) -> Tuple[tf.Tensor, tf.Tensor]:\n        """"""Gated recurrent unit (GRU) with nunits cells.""""""\n        with tf.variable_scope(scope or type(self).__name__):  # ""GRUCell""\n            with tf.variable_scope(""Gates""):  # Reset gate and update gate.\n                # We start with bias of 1.0 to not reset and not update.\n                r, u = tf.split(\n                    tf.layers.dense([inputs, state], 2 * self._num_units),\n                    2, 1)\n                r, u = noisy_sigmoid(\n                    r, self.training), noisy_sigmoid(u, self.training)\n        with tf.variable_scope(""Candidate""):\n            c = noisy_tanh(\n                tf.layers.dense([inputs, r * state], self._num_units),\n                self.training)\n            new_h = u * state + (1 - u) * c\n        return new_h, new_h\n\n\ndef noisy_activation(x, generic, linearized, training,\n                     alpha: float = 1.1, c: float = 0.5):\n    """"""Apply the noisy activation.\n\n    Implements the noisy activation with Half-Normal Noise\n    for Hard-Saturation functions.\n\n    See http://arxiv.org/abs/1603.00391, Algorithm 1.\n\n    Args:\n\n        x: Tensor which is an input to the activation function\n\n        generic: The generic formulation of the activation function. (denoted\n            as h in the paper)\n\n        linearized: Linearization of the activation based on the first-order\n            Tailor expansion around zero. (denoted as u in the paper)\n\n        training: A boolean tensor telling whether we are in the training stage\n            (and the noise is sampled) or in runtime when the expactation is\n            used instead.\n\n        alpha: Mixing hyper-parameter. The leakage rate from the linearized\n            function to the nonlinear one.\n\n        c: Standard deviation of the sampled noise.\n\n    """"""\n\n    # pylint: disable=invalid-unary-operand-type\n    # to enable \'minus tf.Tensor\'\n\n    delta = generic(x) - linearized(x)\n    d = -tf.sign(x) * tf.sign(1 - alpha)\n    p = tf.get_variable(""p"", shape=[1], initializer=tf.ones_initializer())\n    scale = c * (tf.sigmoid(p * delta) - 0.5) ** 2\n    noise = tf.where(training, tf.abs(\n        tf.random_normal([])), math.sqrt(2 / math.pi))\n    activation = alpha * generic(x) + (1 - alpha) * \\\n        linearized(x) + d * scale * noise\n    return activation\n\n\n# lin_sigmoid, hard_tanh, hard_sigmoid are equations (1), (3) and (4) in\n# the Noisy Activation Functions paper\n\ndef noisy_sigmoid(x, training):\n    def lin_sigmoid(x):\n        return 0.25 * x + 0.5\n\n    def hard_sigmoid(x):\n        return tf.minimum(tf.maximum(lin_sigmoid(x), 0.), 1.)\n    return noisy_activation(x, hard_sigmoid, lin_sigmoid, training)\n\n\ndef noisy_tanh(x, training):\n    def hard_tanh(x):\n        return tf.minimum(tf.maximum(x, -1.), 1.)\n    return noisy_activation(x, hard_tanh, lambda y: y, training)\n'"
neuralmonkey/nn/ortho_gru_cell.py,17,"b'import tensorflow as tf\n\n\ndef orthogonal_initializer():\n    """"""Return an orthogonal initializer.\n\n    Random orthogonal matrix is byproduct of singular value decomposition\n    applied on a matrix initialized with normal distribution.\n\n    The initializer works with 2D square matrices and matrices that can be\n    splitted along axis 1 to several 2D matrices. In the latter case, each\n    submatrix is initialized independently and the resulting orthogonal\n    matrices are concatenated along axis 1.\n\n    Note this is a higher order function in order to mimic the tensorflow\n    initializer API.\n    """"""\n\n    # pylint: disable=unused-argument\n    def func(shape, dtype, partition_info=None):\n        if len(shape) != 2:\n            raise ValueError(\n                ""Orthogonal initializer only works with 2D matrices."")\n\n        if shape[1] % shape[0] != 0:\n            raise ValueError(""Shape {} is not compatible with orthogonal ""\n                             ""initializer."".format(str(shape)))\n\n        mult = int(shape[1] / shape[0])\n        dim = shape[0]\n\n        orthogonals = []\n        for _ in range(mult):\n            matrix = tf.random_normal([dim, dim], dtype=dtype)\n            orthogonals.append(tf.svd(matrix)[1])\n\n        return tf.concat(orthogonals, 1)\n    # pylint: enable=unused-argument\n\n    return func\n\n\n# pylint: disable=too-few-public-methods\nclass OrthoGRUCell(tf.contrib.rnn.GRUCell):\n    """"""Classic GRU cell but initialized using random orthogonal matrices.""""""\n\n    def __init__(self, num_units, activation=None, reuse=None):\n        tf.contrib.rnn.GRUCell.__init__(\n            self, num_units, activation, reuse,\n            kernel_initializer=tf.orthogonal_initializer())\n\n    def __call__(self, inputs, state, scope=""OrthoGRUCell""):\n        return tf.contrib.rnn.GRUCell.__call__(self, inputs, state, scope)\n\n\n# Note that tensorflow does not like when the type annotations are present.\nclass NematusGRUCell(tf.contrib.rnn.GRUCell):\n    """"""Nematus implementation of gated recurrent unit cell.\n\n    The main difference is the order in which the gating functions and linear\n    projections are applied to the hidden state.\n\n    The math is equivalent, in practice there are differences due to float\n    precision errors.\n    """"""\n\n    def __init__(self, rnn_size, use_state_bias=False, use_input_bias=True):\n        self.use_state_bias = use_state_bias\n        self.use_input_bias = use_input_bias\n\n        tf.contrib.rnn.GRUCell.__init__(self, rnn_size)\n\n    def call(self, inputs, state):\n        """"""Gated recurrent unit (GRU) with nunits cells.""""""\n        with tf.variable_scope(""gates""):\n            input_to_gates = tf.layers.dense(\n                inputs, 2 * self._num_units, name=""input_proj"",\n                use_bias=self.use_input_bias)\n\n            # Nematus does the orthogonal initialization probably differently\n            state_to_gates = tf.layers.dense(\n                state, 2 * self._num_units,\n                use_bias=self.use_state_bias,\n                kernel_initializer=orthogonal_initializer(),\n                name=""state_proj"")\n\n            gates_input = state_to_gates + input_to_gates\n            reset, update = tf.split(\n                tf.sigmoid(gates_input), num_or_size_splits=2, axis=1)\n\n        with tf.variable_scope(""candidate""):\n            input_to_candidate = tf.layers.dense(\n                inputs, self._num_units, use_bias=self.use_input_bias,\n                name=""input_proj"")\n\n            state_to_candidate = tf.layers.dense(\n                state, self._num_units, use_bias=self.use_state_bias,\n                kernel_initializer=orthogonal_initializer(),\n                name=""state_proj"")\n\n            candidate = self._activation(\n                state_to_candidate * reset + input_to_candidate)\n\n        new_state = update * state + (1 - update) * candidate\n        return new_state, new_state\n'"
neuralmonkey/nn/pervasive_dropout_wrapper.py,1,"b'import tensorflow as tf\n\nfrom neuralmonkey.checking import assert_shape\n\n\nclass PervasiveDropoutWrapper(tf.contrib.rnn.RNNCell):\n\n    def __init__(self, cell, mask, scale) -> None:\n        self._cell = cell\n        self._mask = mask\n        assert_shape(mask, [None, cell.sate_size])\n        self._scale = scale\n\n    @property\n    def state_size(self) -> int:\n        return self._cell.state_size\n\n    @property\n    def output_size(self) -> int:\n        return self._cell.output_size\n\n    def __call__(self, inputs, state, scope=None):\n        output, new_state = self._cell(inputs, state)\n\n        # self._mask is of shape [batch_size, state_size]\n        # new_state is of shape [batch_size, state_size] (hopefully)\n        new_state_dropped = new_state * self._scale * self._mask\n        assert_shape(new_state_dropped, [None, self._cell.sate_size])\n        return output, new_state_dropped\n'"
neuralmonkey/nn/projection.py,16,"b'""""""Module which implements various types of projections.""""""\nfrom typing import List, Callable\nimport tensorflow as tf\nfrom neuralmonkey.nn.utils import dropout\n\n\ndef maxout(inputs: tf.Tensor,\n           size: int,\n           scope: str = ""MaxoutProjection"") -> tf.Tensor:\n    """"""Apply a maxout operation.\n\n    Implementation of Maxout layer (Goodfellow et al., 2013).\n\n    http://arxiv.org/pdf/1302.4389.pdf\n\n    z = Wx + b\n    y_i = max(z_{2i-1}, z_{2i})\n\n    Arguments:\n        inputs: A tensor or list of tensors. It should be 2D tensors with\n            equal length in the first dimension (batch size)\n        size: The size of dimension 1 of the output tensor.\n        scope: The name of the scope used for the variables\n\n    Returns:\n        A tensor of shape batch x size\n    """"""\n    with tf.variable_scope(scope):\n        projected = tf.layers.dense(inputs, size * 2, name=scope)\n        maxout_input = tf.reshape(projected, [-1, 1, 2, size])\n        maxpooled = tf.nn.max_pool(\n            maxout_input, [1, 1, 2, 1], [1, 1, 2, 1], ""SAME"")\n\n        reshaped = tf.reshape(maxpooled, [-1, size])\n        return reshaped\n\n\ndef multilayer_projection(\n        input_: tf.Tensor,\n        layer_sizes: List[int],\n        train_mode: tf.Tensor,\n        activation: Callable[[tf.Tensor], tf.Tensor] = tf.nn.relu,\n        dropout_keep_prob: float = 1.0,\n        scope: str = ""mlp"") -> tf.Tensor:\n    mlp_input = input_\n\n    with tf.variable_scope(scope):\n        for i, size in enumerate(layer_sizes):\n            mlp_input = tf.layers.dense(\n                mlp_input,\n                size,\n                activation=activation,\n                name=""mlp_layer_{}"".format(i))\n\n            mlp_input = dropout(mlp_input, dropout_keep_prob, train_mode)\n\n    return mlp_input\n\n\ndef glu(input_: tf.Tensor,\n        gating_fn: Callable[[tf.Tensor], tf.Tensor] = tf.sigmoid) -> tf.Tensor:\n    """"""Apply a Gated Linear Unit.\n\n    Gated Linear Unit - Dauphin et al. (2016).\n\n    http://arxiv.org/abs/1612.08083\n    """"""\n    dimensions = input_.get_shape().as_list()\n\n    if dimensions[-1] % 2 != 0:\n        raise ValueError(""Input size should be an even number"")\n\n    lin, nonlin = tf.split(input_, 2, axis=len(dimensions) - 1)\n\n    return lin * gating_fn(nonlin)\n'"
neuralmonkey/nn/utils.py,5,"b'""""""Module which provides utility functions used across the package.""""""\n\nimport tensorflow as tf\n\n\ndef dropout(variable: tf.Tensor,\n            keep_prob: float,\n            train_mode: tf.Tensor) -> tf.Tensor:\n    """"""Perform dropout on a variable, depending on mode.\n\n    Arguments:\n        variable: The variable to be dropped out\n        keep_prob: The probability of keeping a value in the variable\n        train_mode: A bool Tensor specifying whether to dropout or not\n    """"""\n    # Maintain clean graph - no dropout op when there is none applied\n    with tf.name_scope(""dropout""):\n        if keep_prob == 1.0:\n            return variable\n\n        dropped_value = tf.nn.dropout(variable, keep_prob)\n        return tf.where(train_mode, dropped_value, variable)\n'"
neuralmonkey/processors/__init__.py,0,b''
neuralmonkey/processors/alignment.py,0,"b'import re\nfrom typing import List\n\nimport numpy as np\n\n# pylint: disable=too-few-public-methods\n\nID_SEP = re.compile(r""[-:]"")\n\n\nclass WordAlignmentPreprocessor:\n    """"""A preprocessor for word alignments in a text format.\n\n    One of the following formats is expected:\n\n        s1-t1 s2-t2 ...\n\n        s1:1/w1 s2:t2/w2 ...\n\n    where each `s` and `t` is the index of a word in the source and target\n    sentence, respectively, and `w` is the corresponding weight. If the weight\n    is not given, it is assumend to be 1. The separators `-` and `:` are\n    interchangeable.\n\n    The output of the preprocessor is an alignment matrix of the fixed shape\n    (target_len, source_len) for each sentence.\n    """"""\n\n    def __init__(self, source_len, target_len, dtype=np.float32,\n                 normalize=True, zero_based=True):\n        self._source_len = source_len\n        self._target_len = target_len\n        self._dtype = dtype\n        self._normalize = normalize\n        self._zero_based = zero_based\n\n    def __call__(self, sentence: List[str]):\n        result = np.zeros((self._target_len, self._source_len), self._dtype)\n\n        for ali in sentence:\n            ids, _, str_weight = ali.partition(""/"")\n            i, j = [int(id_str) for id_str in ID_SEP.split(ids)]\n            weight = float(str_weight) if str_weight else 1.\n\n            if not self._zero_based:\n                i -= 1\n                j -= 1\n\n            if i < self._source_len and j < self._target_len:\n                result[j][i] = weight\n\n        if self._normalize:\n            with np.errstate(divide=""ignore"", invalid=""ignore""):\n                result /= result.sum(axis=1, keepdims=True)\n                result[np.isnan(result)] = 0\n\n        return result\n'"
neuralmonkey/processors/bpe.py,0,"b'import re\nfrom typing import List\n\nfrom neuralmonkey.logging import log\nfrom lib.subword_nmt.apply_bpe import BPE, encode\n\n# pylint: disable=too-few-public-methods\n\n\nclass BPEPreprocessor:\n    """"""Wrapper class for Byte-Pair Encoding.\n\n    Paper: https://arxiv.org/abs/1508.07909\n    Code: https://github.com/rsennrich/subword-nmt\n    """"""\n\n    def __init__(self,\n                 merge_file: str,\n                 separator: str = ""@@"",\n                 encoding: str = ""utf-8"") -> None:\n        log(""Initializing BPE preprocessor"")\n\n        with open(merge_file, ""r"", encoding=encoding) as f_data:\n            self.bpe = BPE(f_data, separator)\n\n    def __call__(self, sentence: List[str]) -> List[str]:\n        """"""Adapted code from BPE.segment.""""""\n\n        output = []\n        for word in sentence:\n\n            # Hack. TODO: inspect why there are empty sentences\n            if not word:\n                output.append(word)\n                continue\n\n            new_word = encode(word, self.bpe.bpe_codes)\n\n            for item in new_word[:-1]:\n                output.append(item + self.bpe.separator)\n            output.append(new_word[-1])\n\n        return output\n\n\nclass BPEPostprocessor:\n\n    def __init__(self, separator: str = ""@@"") -> None:\n        esc = re.escape(separator)\n        self.pattern = re.compile(esc + r"" "")\n\n    def __call__(self, decoded_sentences: List[List[str]]) -> List[List[str]]:\n        return [self.decode(s) for s in decoded_sentences]\n\n    def decode(self, sentence: List[str]) -> List[str]:\n        joined = "" "".join(sentence)\n        decoded = self.pattern.sub("""", joined)\n        splitted = decoded.split("" "")\n\n        return splitted\n'"
neuralmonkey/processors/editops.py,0,"b'from typing import Any, Callable, Dict, Iterable, Iterator, List\n\nimport numpy as np\n\n\n# pylint: disable=too-few-public-methods\nclass Preprocess:\n    """"""Preprocessor transorming two series into series of edit operations.""""""\n\n    def __init__(self, source_id: str, target_id: str) -> None:\n        self._source_id = source_id\n        self._target_id = target_id\n\n    def __call__(\n            self,\n            iterators: Dict[str, Callable[[], Iterator[List[str]]]]\n    ) -> Iterator[List[str]]:\n        source_series = iterators[self._source_id]()\n        target_series = iterators[self._target_id]()\n\n        for src_seq, tgt_seq in zip(source_series, target_series):\n            yield convert_to_edits(src_seq, tgt_seq)\n\n\nclass Postprocess:\n    """"""Proprocessor applying edit operations on a series.""""""\n\n    def __init__(\n            self,\n            source_id: str,\n            edits_id: str) -> None:\n\n        self._source_id = source_id\n        self._edits_id = edits_id\n\n    def __call__(self,\n                 dataset: Dict[str, Iterable[Any]],\n                 generated: Dict[str, Iterable[Any]]) -> List[List[str]]:\n\n        if self._source_id not in dataset:\n            raise ValueError(""Source series not present in the input dataset"")\n\n        if self._edits_id not in generated:\n            raise ValueError(""Edits series not present in the output dataset"")\n\n        source_series = dataset[self._source_id]\n        edits_series = generated[self._edits_id]\n\n        reconstructed = []\n        for src_seq, edit_seq in zip(source_series, edits_series):\n            reconstructed.append(reconstruct(src_seq, edit_seq))\n\n        return reconstructed\n# pylint: enable=too-few-public-methods\n\n\nKEEP = ""<keep>""\nDELETE = ""<delete>""\n\n\ndef convert_to_edits(source: List[str], target: List[str]) -> List[str]:\n    lev = np.zeros([len(source) + 1, len(target) + 1])\n    edits = [[[] for _ in range(len(target) + 1)]\n             for _ in range(len(source) + 1)]  # type: List[List[List[str]]]\n\n    for i in range(len(source) + 1):\n        lev[i, 0] = i\n        edits[i][0] = [DELETE for _ in range(i)]\n\n    for j in range(len(target) + 1):\n        lev[0, j] = j\n        edits[0][j] = target[:j]\n\n    for j in range(1, len(target) + 1):\n        for i in range(1, len(source) + 1):\n\n            if source[i - 1] == target[j - 1]:\n                keep_cost = lev[i - 1, j - 1]\n            else:\n                keep_cost = np.inf\n\n            delete_cost = lev[i - 1, j] + 1\n            insert_cost = lev[i, j - 1] + 1\n\n            lev[i, j] = min(keep_cost, delete_cost, insert_cost)\n\n            if lev[i, j] == keep_cost:\n                edits[i][j] = edits[i - 1][j - 1] + [KEEP]\n\n            elif lev[i, j] == delete_cost:\n                edits[i][j] = edits[i - 1][j] + [DELETE]\n\n            else:\n                edits[i][j] = edits[i][j - 1] + [target[j - 1]]\n\n    return edits[-1][-1]\n\n\ndef reconstruct(source: List[str], edits: List[str]) -> List[str]:\n    index = 0\n    target = []\n\n    for edit in edits:\n        if edit == KEEP:\n            if index < len(source):\n                target.append(source[index])\n            index += 1\n\n        elif edit == DELETE:\n            index += 1\n\n        else:\n            target.append(edit)\n\n    # we may have created a shorter sequence of edit ops due to the\n    # decoder limitations -> now copy the rest of source\n    if index < len(source):\n        target.extend(source[index:])\n\n    return target\n'"
neuralmonkey/processors/german.py,0,"b'import re\n\n# pylint: disable=unused-import\nfrom typing import List, Dict\n# pylint: enable=unused-import\n\n\nCONTRACTIONS = [""am"", ""ans"", ""beim"", ""im"", ""ins"", ""vom"", ""zum"", ""zur""]\nCONTRACTIONS_SET = set(CONTRACTIONS)\nUNCONTRACTED_FORMS = [[""an"", ""dem""], [""an"", ""das""], [""bei"", ""dem""],\n                      [""in"", ""dem""], [""in"", ""das""], [""von"", ""dem""],\n                      [""zu"", ""dem""], [""zu"", ""der""]]\nUNCONTRACT = {c: un for c, un in zip(CONTRACTIONS, UNCONTRACTED_FORMS)}\n\nCONTRACT = {}  # type: Dict[str, Dict[str, str]]\nfor cont, (prep, article) in zip(CONTRACTIONS, UNCONTRACTED_FORMS):\n    if article not in CONTRACT:\n        CONTRACT[article] = {}\n    CONTRACT[article][prep] = cont\n\n\nEIN_TYPE_PRONOUNS = \\\n    re.compile(""^(ein|[mdsk]ein|ihr|unser|euer|Ihr)(e|es|er|em|en)$"")\nDER_TYPE_PRONOUNS = re.compile(""^(dies|welch|jed|all)(e|es|er|em|en)$"")\n\n# pylint: disable=too-few-public-methods\n\n\nclass GermanPreprocessor:\n\n    def __init__(self, compounding=True, contracting=True, pronouns=True):\n        self.compounding = compounding\n        self.contracting = contracting\n        self.pronouns = pronouns\n\n    def __call__(self, sentence):\n        result = []\n\n        for word in sentence:\n            if self.pronouns:\n                ein_match = EIN_TYPE_PRONOUNS.match(word)\n                der_match = DER_TYPE_PRONOUNS.match(word)\n\n            if self.contracting and word in CONTRACTIONS_SET:\n                result.extend(UNCONTRACT[word])\n            elif self.pronouns and ein_match:\n                result.append(ein_match.group(1))\n                result.append(""<<"" + ein_match.group(2))\n            elif self.pronouns and der_match:\n                result.append(der_match.group(1))\n                result.append(""<<"" + der_match.group(2))\n            elif self.compounding and word.find("">><<"") > -1:\n                compound_parts = word.split("">><<"")\n                result.append(compound_parts[0])\n                for wrd in compound_parts[1:]:\n                    result.append("">><<"")\n                    result.append(wrd.capitalize())\n            else:\n                result.append(word)\n\n        return result\n\n\nclass GermanPostprocessor:\n\n    def __init__(self, compounding=True, contracting=True, pronouns=True):\n        self.compounding = compounding\n        self.contracting = contracting\n        self.pronouns = pronouns\n\n    def __call__(self, decoded_sentences: List[List[str]]) -> List[List[str]]:\n        return [self.decode(s) for s in decoded_sentences]\n\n    def decode(self, sentence):\n        result = []\n\n        compound = False\n        for word in sentence:\n            if self.contracting and word in CONTRACT \\\n                    and result and result[-1] in CONTRACT[word]:\n                result[-1] = CONTRACT[word][result[-1]]\n            elif self.pronouns and word.startswith(""<<""):\n                if result:\n                    result[-1] += word[2:]\n            elif self.compounding and result and word == "">><<"":\n                compound = True\n            elif self.compounding and compound:\n                result[-1] += word.lower()\n                compound = False\n            else:\n                result.append(word)\n\n        return result\n'"
neuralmonkey/processors/helpers.py,0,"b'from typing import Any, Callable, Generator, List\nfrom random import randint\n\n\ndef preprocess_char_based(sentence: List[str]) -> List[str]:\n    return list("" "".join(sentence))\n\n\ndef preprocess_add_noise(sentence: List[str]) -> List[str]:\n    sent = sentence[:]\n    length = len(sentence)\n    if length > 1:\n        for _ in range(length // 2):\n            swap = randint(0, length - 2)\n            sent[swap] = sent[swap + 1]\n            sent[swap + 1] = sent[swap]\n    return sent\n\n\n# TODO refactor post-processors to work on sentence level\ndef postprocess_char_based(sentences: List[List[str]]) -> List[List[str]]:\n    result = []\n\n    for sentence in sentences:\n        joined = """".join(sentence)\n        tokenized = joined.split("" "")\n        result.append(tokenized)\n\n    return result\n\n\ndef untruecase(\n        sentences: List[List[str]]) -> Generator[List[str], None, None]:\n    for sentence in sentences:\n        if sentence:\n            yield [sentence[0].capitalize()] + sentence[1:]\n        else:\n            yield []\n\n\ndef pipeline(processors: List[Callable]) -> Callable:\n    """"""Concatenate processors.""""""\n\n    def process(data: Any) -> Any:\n        for processor in processors:\n            data = processor(data)\n        return data\n\n    return process\n'"
neuralmonkey/processors/speech.py,0,"b'from typing import Callable\n\nimport numpy as np\nfrom python_speech_features import mfcc, fbank, logfbank, ssc, delta\n\nfrom neuralmonkey.readers.audio_reader import Audio\n\n\n# pylint: disable=invalid-name\ndef SpeechFeaturesPreprocessor(feature_type: str = ""mfcc"",\n                               delta_order: int = 0,\n                               delta_window: int = 2,\n                               **kwargs) -> Callable:\n    """"""Calculate speech features.\n\n    First, the given type of features (e.g. MFCC) is computed using a window\n    of length `winlen` and step `winstep`; for additional keyword arguments\n    (specific to each feature type), see\n    http://python-speech-features.readthedocs.io/. Then, delta features up to\n    `delta_order` are added.\n\n    By default, 13 MFCCs per frame are computed. To add delta and delta-delta\n    features (resulting in 39 coefficients per frame), set `delta_order=2`.\n\n    Arguments:\n        feature_type: mfcc, fbank, logfbank or ssc (default is mfcc)\n        delta_order: maximum order of the delta features (default is 0)\n        delta_window: window size for delta features (default is 2)\n        **kwargs: keyword arguments for the appropriate function from\n            python_speech_features\n\n    Returns:\n        A numpy array of shape [num_frames, num_features].\n    """"""\n\n    if feature_type not in FEATURE_TYPES:\n        raise ValueError(\n            ""Unknown speech feature type \'{}\'"".format(feature_type))\n\n    def preprocess(audio: Audio) -> np.ndarray:\n        features = [FEATURE_TYPES[feature_type](\n            audio.data, samplerate=audio.rate, **kwargs)]\n\n        for _ in range(delta_order):\n            features.append(delta(features[-1], delta_window))\n\n        return np.concatenate(features, axis=1)\n\n    return preprocess\n\n\ndef _fbank(*args, **kwargs) -> np.ndarray:\n    feat, _ = fbank(*args, **kwargs)\n    return feat\n\n\nFEATURE_TYPES = {""mfcc"": mfcc,\n                 ""fbank"": _fbank,\n                 ""logfbank"": logfbank,\n                 ""ssc"": ssc}\n'"
neuralmonkey/processors/wordpiece.py,0,"b'""""""Loose reimplementation of the t2t tokenizer.\n\nOriginal code:\nhttps://github.com/tensorflow/tensor2tensor/blob/v1.5.5/tensor2tensor/data_generators/tokenizer.py\n\nProvides a WordpiecePreprocessor, a higher order function which takes a\nvocabulary object and returns a preprocessor, and a WordpiecePostprocessor.\n\nNote that the latter is not a higher order function and can be used directly\nwithout making a new section in the configuration.\n""""""\nfrom typing import List, Callable, Set\nimport re\n\nfrom typeguard import check_argument_types\nfrom neuralmonkey.vocabulary import Vocabulary\n\n\nUNESCAPE_REGEX = re.compile(r""\\\\u|\\\\\\\\|\\\\([0-9]+);"")\n\n\ndef escape_token(token: str, alphabet: Set[str]) -> str:\n    """"""Escapes the token in the t2t fashion.\n\n    Underscores are regarded as an end of a token, so they must be escaped.\n    Additionally, they/we escape also the OOA (out-of-alphabet) characters\n    using their unicode code.\n    """"""\n\n    esc_token = token.replace(""\\\\"", ""\\\\\\\\"")  # replace 1 backslash with 2\n    esc_token = esc_token.replace(""_"", ""\\\\u"")  # replace underscore with ""\\u""\n\n    # replace OOA symbol `s` with \\1234; where 1234 is `ord(s)`\n    characters = [c if c in alphabet and c != ""\\n"" else ""\\\\{};"".format(ord(c))\n                  for c in token]  # not sure about the ""\\n""-part\n\n    return """".join(characters) + ""_""\n\n\ndef unescape_token(escaped_token: str) -> str:\n    """"""Inverse function for escape_token.""""""\n\n    # Ends with underscore -> remove it\n    token = escaped_token\n    token = token[:-1] if token.endswith(""_"") else token\n\n    def match(m):\n        if m.group(1) is None:\n            return ""_"" if m.group(0) == ""\\\\u"" else ""\\\\""\n\n        try:\n            return chr(int(m.group(1)))\n        except (ValueError, OverflowError):\n            return u""\\u3013""  # Unicode for undefined character.\n\n    # The substitution works because of the left-to-right nature of matching\n    return UNESCAPE_REGEX.sub(match, token)\n\n\ndef wordpiece_encode(sentence: List[str], vocabulary: Vocabulary) -> List[str]:\n    """"""Convert tokens to subtokens using a vocabulary of subtokens.\n\n    A greedy implementation, as in t2t referenced above.\n\n    We search for the longest subtoken available in the vocabulary from left to\n    right.\n    """"""\n    tokens = []\n    for token in sentence:\n        esc_token = escape_token(token, vocabulary.alphabet)\n\n        subtokens = []\n        current_subtoken_start = 0\n        token_len = len(esc_token)\n\n        while current_subtoken_start < len(esc_token):\n\n            # TODO: they optimize this by ranging from\n            # min(token_len, max_subtoken_len + start)\n            # this can be achieved by saving the len of longest word in vocab\n            for end in range(token_len, current_subtoken_start, -1):\n                subtoken = esc_token[current_subtoken_start:end]\n\n                if subtoken in vocabulary:\n                    subtokens.append(subtoken)\n                    current_subtoken_start = end\n                    break\n            else:  # executed if the loop is not exited by the break statement\n                raise AssertionError(\n                    ""No token substring found in the vocab ({}).""\n                    .format(esc_token[current_subtoken_start:]))\n\n        # TODO: they also optimize this by caching the segmentation of the\n        # escaped tokens.\n        tokens.extend(subtokens)\n    return tokens\n\n\ndef wordpiece_decode(sentence: List[str]) -> List[str]:\n    """"""Postprocess the wordpieces into a sentence.\n\n    First, retokenize the sentence - join and split around underscores.\n    Second, unescape tokens throwing away any empty tokens encountered.\n    """"""\n    retokenized = """".join(sentence).split(""_"")\n    unescaped = [unescape_token(tok) for tok in retokenized if tok]\n    return [tok for tok in unescaped if tok]\n\n\ndef wordpiece_decode_batch(sentences: List[List[str]]) -> List[List[str]]:\n    return [wordpiece_decode(s) for s in sentences]\n\n\ndef get_wordpiece_preprocessor(\n        vocabulary: Vocabulary) -> Callable[[List[str]], List[str]]:\n    check_argument_types()\n    return lambda s: wordpiece_encode(s, vocabulary)\n\n\n# pylint: disable=invalid-name\n# Syntactic sugar for configuration\nWordpiecePreprocessor = get_wordpiece_preprocessor\nWordpiecePostprocessor = wordpiece_decode_batch\n'"
neuralmonkey/readers/__init__.py,0,b''
neuralmonkey/readers/audio_reader.py,0,"b'from typing import Callable, Iterable, List, NamedTuple\n\nimport io\nimport os\nimport subprocess\nimport sys\n\nimport numpy as np\nfrom scipy.io import wavfile\n\n\nclass Audio(NamedTuple(""Audio"", [(""rate"", int), (""data"", np.ndarray)])):\n    """"""A raw audio object with its rate as metadata.\n\n    Attribute:\n        rate: The sample rate of the audio.\n        data: The raw audio data.\n    """"""\n\n\ndef audio_reader(prefix: str = """",\n                 audio_format: str = ""wav"") -> Callable:\n    """"""Get a reader of audio files loading them from a list of pahts.\n\n    Args:\n        prefix: Prefix of the paths to the audio files.\n\n    Returns:\n        The reader function that takes a list of audio file paths (relative to\n        provided prefix) and returns a list of numpy arrays.\n    """"""\n\n    if audio_format == ""wav"":\n        load_file = _load_wav\n    elif audio_format == ""sph"":\n        load_file = _load_sph\n    else:\n        raise ValueError(\n            ""Unsupported audio format: {}"".format(audio_format))\n\n    def load(list_files: List[str]) -> Iterable[Audio]:\n        for list_file in list_files:\n            with open(list_file) as f_list:\n                for audio_file in f_list:\n                    path = os.path.join(prefix, audio_file.rstrip())\n                    yield load_file(path)\n\n    return load\n\n\ndef _load_wav(path: str) -> Audio:\n    """"""Read a WAV file.""""""\n    return Audio(*wavfile.read(path))\n\n\ndef _load_sph(path: str) -> Audio:\n    """"""Read a NIST Sphere audio file using the sph2pipe utility.""""""\n    process = subprocess.Popen([""sph2pipe"", ""-f"", ""wav"", path],\n                               stdout=subprocess.PIPE,\n                               stderr=sys.stderr)\n    data = io.BytesIO(process.stdout.read())\n\n    error_code = process.wait()\n    if error_code != 0:\n        raise RuntimeError(""sph2pipe exited with error code {} when ""\n                           ""processing {}"".format(error_code, path))\n\n    return Audio(*wavfile.read(data))\n'"
neuralmonkey/readers/image_reader.py,0,"b'from typing import Callable, Iterable, List\nimport os\n\nimport numpy as np\nfrom typeguard import check_argument_types\nfrom PIL import Image, ImageFile\n\nfrom neuralmonkey.logging import warn\n\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n\ndef image_reader(pad_w: int,\n                 pad_h: int,\n                 channels: int = 3,\n                 prefix: str = """",\n                 rescale_w: bool = False,\n                 rescale_h: bool = False,\n                 keep_aspect_ratio: bool = False,\n                 mode: str = ""RGB"") -> Callable:\n    """"""Get a reader of images loading them from a list of pahts.\n\n    Args:\n        pad_w: Width to which the images will be padded/cropped/resized.\n        pad_h: Height to which the images will be padded/cropped/resized.\n        channels: Number of channels in each image (default 3 for RGB)\n        prefix: Prefix of the paths that are listed in a image files.\n        rescale_w: If true, image is rescaled to have given width. It is\n            cropped/padded otherwise.\n        rescale_h: If true, image is rescaled to have given height. It is\n            cropped/padded otherwise.\n        keep_aspect_ratio: Flag whether the aspect ration should be kept during\n            rescaling. Can only be used if both width and height are rescaled.\n        mode: Scipy image loading mode, see scipy documentation for more\n            details.\n\n    Returns:\n        The reader function that takes a list of image paths (relative to\n        provided prefix) and returns a list of images as numpy arrays of shape\n        pad_h x pad_w x number of channels.\n    """"""\n    check_argument_types()\n    if not rescale_w and not rescale_h and keep_aspect_ratio:\n        raise ValueError(\n            ""It does not make sense to keep the aspect ratio while not ""\n            ""rescaling the image."")\n    if rescale_w != rescale_h and not keep_aspect_ratio:\n        raise ValueError(\n            ""While rescaling only one side, aspect ratio must be kept, ""\n            ""was set to false."")\n\n    def load(list_files: List[str]) -> Iterable[np.ndarray]:\n        for list_file in list_files:\n            with open(list_file) as f_list:\n                for i, image_file in enumerate(f_list):\n                    path = os.path.join(prefix, image_file.rstrip())\n\n                    if not os.path.exists(path):\n                        raise Exception(\n                            (""Image file \'{}\' no.""\n                             ""{}  does not exist."").format(path, i + 1))\n\n                    try:\n                        image = Image.open(path).convert(mode)\n                    except IOError:\n                        warn(""Skipping image from file \'{}\' no. \'{}\'."".format(\n                            path, i + 1))\n                        image = Image.new(mode, (pad_w, pad_h))\n\n                    image = _rescale_or_crop(image, pad_w, pad_h,\n                                             rescale_w, rescale_h,\n                                             keep_aspect_ratio)\n                    image_np = np.array(image)\n\n                    if len(image_np.shape) == 2:\n                        img_channels = 1\n                        image_np = np.expand_dims(image_np, 2)\n                    elif len(image_np.shape) == 3:\n                        img_channels = image_np.shape[2]\n                    else:\n                        raise ValueError(\n                            (""Image should have either 2 (black and white) ""\n                             ""or three dimensions (color channels), has {} ""\n                             ""dimension."").format(len(image_np.shape)))\n\n                    if channels != img_channels:\n                        raise ValueError(\n                            ""Image does not have the pre-declared number of ""\n                            ""channels {}, but {}."".format(\n                                channels, img_channels))\n\n                    yield _pad(image_np, pad_w, pad_h, channels)\n\n    return load\n\n\n# Mean pixel values from preprocessing of the VGG network\nVGG_RGB_MEANS = [[[123.68, 116.779, 103.939]]]\n\n\ndef imagenet_reader(prefix: str,\n                    target_width: int = 227,\n                    target_height: int = 227,\n                    vgg_normalization: bool = False,\n                    zero_one_normalization: bool = False) -> Callable:\n    """"""Load and prepare image the same way as Caffe scripts.\n\n    The image preprocessing first rescales the image such that smaller edge has\n    the target length. Then the middle rectangle is cropped from the resized\n    image, such that the cropped image has the target size.\n\n    Args:\n        prefix: Prefix of the paths that are listed in a image files.\n        target_width: Width of the image fed into an ImageNet network.\n        target_height: Height of the image fed into an ImageNet network.\n        vgg_normalization: If true, a mean pixel value will subtracted\n            from all pixels. This is used for VGG nets.\n        zero_one_normalization: If true, all pixel values are divided by 255\n            such that they are in [0, 1] range. This is used for ResNet.\n\n    Yield:\n        An numpy array with the resized and cropped image for every image file\n        in the list.\n    """"""\n    check_argument_types()\n\n    def load(list_files: List[str]) -> Iterable[np.ndarray]:\n        for list_file in list_files:\n            with open(list_file) as f_list:\n                for i, image_file in enumerate(f_list):\n                    path = os.path.join(prefix, image_file.rstrip())\n\n                    if not os.path.exists(path):\n                        raise Exception(\n                            ""Image file \'{}\' no. {} does not exist.""\n                            .format(path, i + 1))\n\n                    res = single_image_for_imagenet(\n                        path, target_height, target_width,\n                        vgg_normalization, zero_one_normalization)\n\n                    yield res\n    return load\n\n\ndef single_image_for_imagenet(\n        path: str, target_height: int, target_width: int,\n        vgg_normalization: bool, zero_one_normalization: bool) -> np.ndarray:\n    image = Image.open(path).convert(""RGB"")\n\n    width, height = image.size\n    if width == height:\n        _rescale_or_crop(image, target_width, target_height,\n                         True, True, False)\n    elif height < width:\n        _rescale_or_crop(\n            image,\n            int(width * float(target_height) / height),\n            target_height, True, True, False)\n    else:\n        _rescale_or_crop(\n            image, target_width,\n            int(height * float(target_width) / width),\n            True, True, False)\n    cropped_image = _crop(image, target_width, target_height)\n\n    res = _pad(np.array(cropped_image),\n               target_width, target_height, 3)\n    assert res.shape == (target_width, target_height, 3)\n\n    if vgg_normalization:\n        res -= VGG_RGB_MEANS\n    if zero_one_normalization:\n        res /= 255.\n\n    return res\n\n\ndef _rescale_or_crop(image: Image.Image, pad_w: int, pad_h: int,\n                     rescale_w: bool, rescale_h: bool,\n                     keep_aspect_ratio: bool) -> Image.Image:\n    """"""Rescale and/or crop the image based on the rescale configuration.""""""\n    orig_w, orig_h = image.size\n    if orig_w == pad_w and orig_h == pad_h:\n        return image\n\n    if rescale_w and rescale_h and not keep_aspect_ratio:\n        image = image.resize((pad_w, pad_h), Image.BILINEAR)\n    elif rescale_w and rescale_h and keep_aspect_ratio:\n        ratio = min(pad_h / orig_h, pad_w / orig_w)\n        image = image.resize((int(orig_w * ratio), int(orig_h * ratio)))\n    elif rescale_w and not rescale_h:\n        orig_w, orig_h = image.size\n        if orig_w != pad_w:\n            ratio = pad_w / orig_w\n            image = image.resize((pad_w, int(orig_h * ratio)))\n    elif rescale_h and not rescale_w:\n        orig_w, orig_h = image.size\n        if orig_h != pad_h:\n            ratio = pad_h / orig_h\n            image = image.resize((int(orig_w * ratio), pad_h))\n    return _crop(image, pad_w, pad_h)\n\n\ndef _crop(image: Image.Image, pad_w: int, pad_h: int) -> Image.Image:\n    orig_w, orig_h = image.size\n    w_shift = max(orig_w - pad_w, 0) // 2\n    h_shift = max(orig_h - pad_h, 0) // 2\n\n    even_w = max(orig_w - pad_w, 0) % 2\n    even_h = max(orig_h - pad_h, 0) % 2\n\n    return image.crop(\n        (w_shift, h_shift, orig_w - w_shift - even_w,\n         orig_h - h_shift - even_h))\n\n\ndef _pad(image: np.ndarray, pad_w: int, pad_h: int,\n         channels: int) -> np.ndarray:\n    img_h, img_w = image.shape[:2]\n\n    image_padded = np.zeros((pad_h, pad_w, channels))\n    image_padded[:img_h, :img_w, :] = image\n\n    return image_padded\n'"
neuralmonkey/readers/numpy_reader.py,0,"b'from typing import List, Callable, Iterable\nimport os\n\nfrom typeguard import check_argument_types\nimport numpy as np\n\n\ndef single_tensor(files: List[str]) -> np.ndarray:\n    """"""Load a single tensor from a numpy file.""""""\n    check_argument_types()\n    if len(files) == 1:\n        return np.load(files[0])\n\n    return np.concatenate([np.load(f) for f in files], axis=0)\n\n\ndef from_file_list(prefix: str,\n                   shape: List[int],\n                   suffix: str = """",\n                   default_tensor_name: str = ""arr_0"") -> Callable:\n    """"""Load a list of numpy arrays from a list of .npz numpy files.\n\n    Args:\n        prefix: A common prefix for the files in the list.\n        shape: The shape of the numpy arrays stored in the referenced files.\n        suffix: An optional suffix that will be appended to each path\n        default_tensor_name: Key of the tensors to load from the npz files.\n\n    Returns:\n        A generator function that yields the loaded arryas.\n    """"""\n    check_argument_types()\n\n    def load(files: List[str]) -> Iterable[np.ndarray]:\n        for list_file in files:\n            with open(list_file, encoding=""utf-8"") as f_list:\n                for line in f_list:\n                    path = os.path.join(prefix, line.rstrip()) + suffix\n                    with np.load(path) as npz:\n                        arr = npz[default_tensor_name]\n                        arr_shape = list(arr.shape)\n                        if arr_shape != shape:\n                            raise ValueError(\n                                ""Shapes do not match: expected {}, found {}""\n                                .format(shape, arr_shape))\n                        yield arr\n    return load\n'"
neuralmonkey/readers/plain_text_reader.py,0,"b'from typing import List, Iterable, Callable\nimport gzip\nimport csv\nimport io\nimport sys\nimport unicodedata\n\nfrom neuralmonkey.logging import warn\n\n\n# pylint: disable=invalid-name\nPlainTextFileReader = Callable[[List[str]], Iterable[List[str]]]\n# pylint: enable=invalid-name\n\ncsv.field_size_limit(sys.maxsize)\n\nALNUM_CHARSET = set(\n    chr(i) for i in range(sys.maxunicode)\n    if (unicodedata.category(chr(i)).startswith(""L"")\n        or unicodedata.category(chr(i)).startswith(""N"")))\n\n\ndef string_reader(\n        encoding: str = ""utf-8"") -> Callable[[List[str]], Iterable[str]]:\n    def reader(files: List[str]) -> Iterable[str]:\n        for path in files:\n            if path.endswith("".gz""):\n                with gzip.open(path, ""r"") as f_data:\n                    for line in f_data:\n                        yield str(line, ""utf-8"")\n            else:\n                with open(path, encoding=encoding) as f_data:\n                    for line in f_data:\n                        yield line\n\n    return reader\n\n\ndef tokenized_text_reader(encoding: str = ""utf-8"") -> PlainTextFileReader:\n    """"""Get reader for space-separated tokenized text.""""""\n    def reader(files: List[str]) -> Iterable[List[str]]:\n        lines = string_reader(encoding)\n        for line in lines(files):\n            yield line.strip().split()\n\n    return reader\n\n\ndef t2t_tokenized_text_reader(encoding: str = ""utf-8"") -> PlainTextFileReader:\n    """"""Get a tokenizing reader for plain text.\n\n    Tokenization is inspired by the tensor2tensor tokenizer:\n    https://github.com/tensorflow/tensor2tensor/blob/v1.5.5/tensor2tensor/data_generators/text_encoder.py\n\n    The text is split to groups of consecutive alphanumeric or non-alphanumeric\n    tokens, dropping single spaces inside the text. Basically the goal here is\n    to preserve the whitespace around weird characters and whitespace on weird\n    positions (beginning and end of the text).\n    """"""\n    def reader(files: List[str]) -> Iterable[List[str]]:\n        lines = string_reader(encoding)\n        for line in lines(files):\n            if not line:\n                yield []\n            line = line.strip()\n\n            tokens = []\n            is_alnum = [ch in ALNUM_CHARSET for ch in line]\n            current_token_start = 0\n\n            for pos in range(1, len(line)):\n                # Boundary of alnum and non-alnum character groups\n                if is_alnum[pos] != is_alnum[pos - 1]:\n                    token = line[current_token_start:pos]\n\n                    # Drop single space if it\'s not on the beginning\n                    if token != "" "" or current_token_start == 0:\n                        tokens.append(token)\n\n                    current_token_start = pos\n\n            # Add a final token (even if it\'s a single space)\n            final_token = line[current_token_start:]\n            tokens.append(final_token)\n\n            yield tokens\n\n    return reader\n\n\ndef column_separated_reader(\n        column: int, delimiter: str = ""\\t"", quotechar: str = None,\n        encoding: str = ""utf-8"") -> PlainTextFileReader:\n    """"""Get reader for delimiter-separated tokenized text.\n\n    Args:\n        column: number of column to be returned. It starts with 1 for the first\n    """"""\n    def reader(files: List[str]) -> Iterable[List[str]]:\n        column_count = None\n        text_reader = string_reader(encoding)\n        for line in text_reader(files):\n            io_line = io.StringIO(line.strip())\n            if quotechar is not None:\n                parsed_csv = list(csv.reader(io_line, delimiter=delimiter,\n                                             quotechar=quotechar,\n                                             skipinitialspace=True))\n            else:\n                parsed_csv = list(csv.reader(io_line, delimiter=delimiter,\n                                             quoting=csv.QUOTE_NONE,\n                                             skipinitialspace=True))\n            columns = len(parsed_csv[0])\n            if column_count is None:\n                column_count = columns\n            elif column_count != columns:\n                warn(""A mismatch in number of columns. Expected {} got {}""\n                     .format(column_count, columns))\n            if columns < column:\n                warn(""There is a missing column number {} in the dataset.""\n                     .format(column))\n                yield []\n            else:\n                yield parsed_csv[0][column - 1].split()\n\n    return reader\n\n\ndef csv_reader(column: int):\n    return column_separated_reader(column, delimiter="","", quotechar=\'""\')\n\n\ndef tsv_reader(column: int):\n    return column_separated_reader(column, delimiter=""\\t"", quotechar=None)\n\n\n# pylint: disable=invalid-name\nUtfPlainTextReader = tokenized_text_reader()\nT2TReader = t2t_tokenized_text_reader()\n# pylint: enable=invalid-name\n'"
neuralmonkey/readers/string_vector_reader.py,0,"b'from typing import List, Iterable, Type\nimport gzip\nimport numpy as np\n\n\ndef get_string_vector_reader(dtype: Type = np.float32, columns: int = None):\n    """"""Get a reader for vectors encoded as whitespace-separated numbers.""""""\n    def process_line(line: str, lineno: int, path: str) -> np.ndarray:\n        numbers = line.strip().split()\n        if columns is not None and len(numbers) != columns:\n            raise ValueError(""Wrong number of columns ({}) on line {}, file {}""\n                             .format(len(numbers), lineno, path))\n\n        return np.array(numbers, dtype=dtype)\n\n    def reader(files: List[str]) -> Iterable[List[np.ndarray]]:\n        for path in files:\n            current_line = 0\n\n            if path.endswith("".gz""):\n                with gzip.open(path, ""r"") as f_data:\n                    for line in f_data:\n                        current_line += 1\n                        if line.strip():\n                            yield process_line(str(line), current_line, path)\n            else:\n                with open(path) as f_data:\n                    for line in f_data:\n                        current_line += 1\n                        if line.strip():\n                            yield process_line(line, current_line, path)\n\n    return reader\n\n\n# pylint: disable=invalid-name\nFloatVectorReader = get_string_vector_reader(np.float32)\nIntVectorReader = get_string_vector_reader(np.int32)\n# pylint: enable=invalid-name\n'"
neuralmonkey/runners/__init__.py,0,b'from .beamsearch_runner import BeamSearchRunner\nfrom .beamsearch_runner import beam_search_runner_range\nfrom .label_runner import LabelRunner\nfrom .logits_runner import LogitsRunner\nfrom .plain_runner import PlainRunner\nfrom .regression_runner import RegressionRunner\nfrom .runner import GreedyRunner\nfrom .word_alignment_runner import WordAlignmentRunner\nfrom .xent_runner import XentRunner\n'
neuralmonkey/runners/base_runner.py,6,"b'from abc import abstractmethod, abstractproperty\nfrom typing import (Dict, Tuple, List, NamedTuple, Union, Set, TypeVar,\n                    Generic, Optional)\nimport numpy as np\nimport tensorflow as tf\n\nfrom neuralmonkey.model.model_part import GenericModelPart\nfrom neuralmonkey.model.feedable import Feedable\nfrom neuralmonkey.model.parameterized import Parameterized\n\n# pylint: disable=invalid-name\nFeedDict = Dict[tf.Tensor, Union[int, float, np.ndarray]]\nNextExecute = Tuple[Union[Dict, List], List[FeedDict]]\nMP = TypeVar(""MP"", bound=GenericModelPart)\nExecutor = TypeVar(""Executor"", bound=""GraphExecutor"")\nRunner = TypeVar(""Runner"", bound=""BaseRunner"")\nOutputSeries = Union[List, np.ndarray]\n# pylint: enable=invalid-name\n\n\nclass ExecutionResult(NamedTuple(\n        ""ExecutionResult"",\n        [(""outputs"", Dict[str, OutputSeries]),\n         (""losses"", Dict[str, float]),\n         (""size"", int),\n         (""summaries"", List[tf.Summary])])):\n    """"""A data structure that represents the result of a graph execution.\n\n    The goal of each graph executor is to populate this structure using its\n    ``set_result`` function.\n\n    Attributes:\n        outputs: A dictionary mapping an output series to the batch of\n            outputs of the graph executor.\n        losses: A (possibly empty) list of loss values computed during the run.\n        size: The length of the output batch.\n        summaries: A list of TensorFlow summary objects fetched by the graph\n            executor\n    """"""\n\n\nclass GraphExecutor(GenericModelPart):\n    """"""The abstract parent class of all graph executors.\n\n    In Neural Monkey, a graph executor is an object that retrieves tensors\n    from the computational graph. The two major groups of graph executors are\n    trainers and runners.\n\n    Each graph executor is an instance of `GenericModelPart` class, which means\n    it has parameterized and feedable dependencies which reference the model\n    part objects needed to be created in order to compute the tensors of\n    interest (called ""fetches"").\n\n    Every graph executor has a method called `get_executable`, which returns\n    an `GraphExecutor.Executable` instance, which specifies what tensors to\n    execute and collects results from the session execution.\n    """"""\n\n    class Executable(Generic[Executor]):\n        """"""Abstract base class for executables.\n\n        Executables are objects associated with the graph executors. Each\n        executable has two main functions: `next_to_execute` and\n        `collect_results`. These functions are called in a loop, until\n        the executable\'s result has been set.\n\n        To make use of Mypy\'s type checking, the executables are generic and\n        are parameterized by the type of their graph executor. Since Python\n        does not know the concept of nested classes, each executable receives\n        the instance of the graph executor through its constructor.\n\n        When subclassing `GraphExecutor`, it is also necessary to subclass\n        the `Executable` class and name it `Executable`, so it overrides the\n        definition of this class. Following this guideline, the default\n        implementation of the `get_executable` function on the graph executor\n        will work without the need of overriding it.\n        """"""\n\n        def __init__(self,\n                     executor: Executor,\n                     compute_losses: bool,\n                     summaries: bool,\n                     num_sessions: int) -> None:\n            self._executor = executor\n            self.compute_losses = compute_losses\n            self.summaries = summaries\n            self.num_sessions = num_sessions\n\n            self._result = None  # type: Optional[ExecutionResult]\n\n        def set_result(self,\n                       outputs: Dict[str, OutputSeries],\n                       losses: Dict[str, float],\n                       size: int,\n                       summaries: List[tf.Summary]) -> None:\n            self._result = ExecutionResult(outputs, losses, size, summaries)\n\n        @property\n        def result(self) -> Optional[ExecutionResult]:\n            return self._result\n\n        @property\n        def executor(self) -> Executor:\n            return self._executor\n\n        def next_to_execute(self) -> NextExecute:\n            """"""Get the tensors and additional feed dicts for execution.""""""\n            return self.executor.fetches, []\n\n        @abstractmethod\n        def collect_results(self, results: List[Dict]) -> None:\n            return None\n\n    def __init__(self,\n                 dependencies: Set[GenericModelPart]) -> None:\n        self._dependencies = dependencies\n        self._feedables, self._parameterizeds = self.get_dependencies()\n\n    def get_executable(self,\n                       compute_losses: bool,\n                       summaries: bool,\n                       num_sessions: int) -> ""GraphExecutor.Executable"":\n        # Since the executable is always subclassed, we can instantiate it\n        return self.Executable(  # type: ignore\n            self, compute_losses, summaries, num_sessions)\n\n    @abstractproperty\n    def fetches(self) -> Dict[str, tf.Tensor]:\n        raise NotImplementedError()\n\n    @property\n    def dependencies(self) -> List[str]:\n        return [""_dependencies""]\n\n    @property\n    def feedables(self) -> Set[Feedable]:\n        return self._feedables\n\n    @property\n    def parameterizeds(self) -> Set[Parameterized]:\n        return self._parameterizeds\n\n\nclass BaseRunner(GraphExecutor, Generic[MP]):\n    """"""Base class for runners.\n\n    Runners are graph executors that retrieve tensors from the model without\n    changing the model parameters. Each runner has a top-level model part it\n    relates to.\n    """"""\n\n    # pylint: disable=too-few-public-methods\n    # Pylint issue here: https://github.com/PyCQA/pylint/issues/2607\n    class Executable(GraphExecutor.Executable[Runner]):\n\n        def next_to_execute(self) -> NextExecute:\n            fetches = self.executor.fetches\n\n            if not self.compute_losses:\n                for loss in self.executor.loss_names:\n                    fetches[loss] = tf.zeros([])\n\n            return fetches, []\n\n        def set_runner_result(self, outputs: OutputSeries,\n                              losses: List[float], size: int = None,\n                              summaries: List[tf.Summary] = None) -> None:\n            if summaries is None:\n                summaries = []\n\n            if size is None:\n                size = len(outputs)\n\n            loss_names = [""{}/{}"".format(self.executor.output_series, loss)\n                          for loss in self.executor.loss_names]\n\n            self.set_result({self.executor.output_series: outputs},\n                            dict(zip(loss_names, losses)), size, summaries)\n    # pylint: enable=too-few-public-methods\n\n    def __init__(self,\n                 output_series: str,\n                 decoder: MP) -> None:\n        GraphExecutor.__init__(self, {decoder})\n        self.output_series = output_series\n        # TODO(tf-data) rename decoder to something more general\n        self.decoder = decoder\n\n    @property\n    def decoder_data_id(self) -> Optional[str]:\n        return getattr(self.decoder, ""data_id"", None)\n\n    @property\n    def loss_names(self) -> List[str]:\n        raise NotImplementedError()\n'"
neuralmonkey/runners/beamsearch_runner.py,1,"b'from typing import Callable, List, Dict\n\nfrom scipy import special\nimport numpy as np\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.decoders.beam_search_decoder import BeamSearchDecoder\nfrom neuralmonkey.runners.base_runner import BaseRunner, NextExecute\n# pylint: disable=unused-import\nfrom neuralmonkey.runners.base_runner import FeedDict\n# pylint: enable=unused-import\nfrom neuralmonkey.vocabulary import END_TOKEN_INDEX\n\n\nclass BeamSearchRunner(BaseRunner[BeamSearchDecoder]):\n    """"""A runner which takes the output from a beam search decoder.\n\n    The runner and the beam search decoder support ensembling.\n    """"""\n\n    class Executable(BaseRunner.Executable[""BeamSearchRunner""]):\n        def __init__(self,\n                     executor: ""BeamSearchRunner"",\n                     compute_losses: bool,\n                     summaries: bool,\n                     num_sessions: int) -> None:\n            super().__init__(executor, compute_losses, summaries, num_sessions)\n\n            self.rank = executor.rank\n            self.decoder = executor.decoder\n            self.postprocess = executor.postprocess\n\n            self._next_feed = [{} for _ in range(self.num_sessions)] \\\n                # type: List[FeedDict]\n\n            # During ensembling, we set the decoder max_steps to zero because\n            # the loop is run manually in the runner.\n            if self.num_sessions > 1:\n                for fd in self._next_feed:\n                    fd.update({self.decoder.max_steps: 0})\n\n        def next_to_execute(self) -> NextExecute:\n            return {""bs_outputs"": self.decoder.outputs}, self._next_feed\n\n        def collect_results(self, results: List[Dict]) -> None:\n            # Recompute logits\n            # Only necessary when ensembling models\n            prev_logprobs = [res[""bs_outputs""].last_search_state.prev_logprobs\n                             for res in results]\n\n            # Arithmetic mean\n            ens_logprobs = (special.logsumexp(prev_logprobs, 0)\n                            - np.log(self.num_sessions))\n\n            if self._is_finished(results):\n                self.prepare_results(\n                    results[0][""bs_outputs""].last_search_step_output)\n                return\n\n            # Prepare the next feed_dict (required for ensembles)\n            self._next_feed = []\n            for result in results:\n                bout = result[""bs_outputs""]\n\n                search_state = bout.last_search_state._replace(\n                    prev_logprobs=ens_logprobs)\n\n                dec_ls = bout.last_dec_loop_state\n                feedables = dec_ls.feedables._replace(step=1)\n                dec_ls = dec_ls._replace(feedables=feedables)\n\n                fd = {\n                    self.decoder.max_steps: 1,\n                    self.decoder.search_state: search_state,\n                    self.decoder.search_results: bout.last_search_step_output,\n                    self.decoder.decoder_state: dec_ls}\n\n                self._next_feed.append(fd)\n\n            return\n\n        def prepare_results(self, output):\n            bs_scores = [s[self.rank - 1] for s in output.scores]\n\n            tok_ids = np.transpose(output.token_ids, [1, 2, 0])\n            decoded_tokens = [toks[self.rank - 1][1:] for toks in tok_ids]\n\n            for i, sent in enumerate(decoded_tokens):\n                decoded = []\n                for tok_id in sent:\n                    if tok_id == END_TOKEN_INDEX:\n                        break\n                    decoded.append(\n                        self.decoder.vocabulary.index_to_word[tok_id])\n                    decoded_tokens[i] = decoded\n\n            if self.postprocess is not None:\n                decoded_tokens = self.postprocess(decoded_tokens)\n\n            # TODO: provide better summaries in case (issue #599)\n            # we want to use the runner during training.\n            self.set_runner_result(\n                outputs=decoded_tokens,\n                losses=[np.mean(bs_scores) * len(bs_scores)])\n\n        def _is_finished(self, results):\n            finished = [\n                all(res[""bs_outputs""].last_dec_loop_state.feedables.finished)\n                for res in results]\n            if all(finished):\n                return True\n            bs_out = results[0][""bs_outputs""]\n            step = len(bs_out.last_search_step_output.token_ids) - 1\n            if step >= self.decoder.max_steps_int:\n                return True\n            return False\n\n    def __init__(self,\n                 output_series: str,\n                 decoder: BeamSearchDecoder,\n                 rank: int = 1,\n                 postprocess: Callable[[List[str]], List[str]] = None) -> None:\n        """"""Initialize the beam search runner.\n\n        Arguments:\n            output_series: Name of the series produced by the runner.\n            decoder: The beam search decoder to use.\n            rank: The hypothesis from the beam to select. Setting rank to 1\n                selects the best hypothesis.\n            postprocess: The postprocessor to apply to the output data.\n        """"""\n        check_argument_types()\n        super().__init__(output_series, decoder)\n\n        if rank < 1 or rank > decoder.beam_size:\n            raise ValueError(\n                (""Rank of output hypothesis must be between 1 and the beam ""\n                 ""size ({}), was {}."").format(decoder.beam_size, rank))\n\n        self.rank = rank\n        self.postprocess = postprocess\n\n    @tensor\n    def fetches(self) -> Dict[str, tf.Tensor]:\n        return {""bs_outputs"": self.decoder.outputs}\n\n    @property\n    def loss_names(self) -> List[str]:\n        return [""beam_search_score""]\n\n\n# TODO: allow the beam search runner to accept multiple ranks because using\n# beam_search_runner_range is too slow.\ndef beam_search_runner_range(\n        output_series: str,\n        decoder: BeamSearchDecoder,\n        max_rank: int = None,\n        postprocess: Callable[[List[str]], List[str]] = None) -> List[\n            BeamSearchRunner]:\n    """"""Return beam search runners for a range of ranks from 1 to max_rank.\n\n    This means there is max_rank output series where the n-th series contains\n    the n-th best hypothesis from the beam search.\n\n    Args:\n        output_series: Prefix of output series.\n        decoder: Beam search decoder shared by all runners.\n        max_rank: Maximum rank of the hypotheses.\n        postprocess: Series-level postprocess applied on output.\n\n    Returns:\n        List of beam search runners getting hypotheses with rank from 1 to\n        max_rank.\n    """"""\n    check_argument_types()\n\n    if max_rank is None:\n        max_rank = decoder.beam_size\n\n    if max_rank > decoder.beam_size:\n        raise ValueError(\n            (""The maximum rank ({}) cannot be ""\n             ""bigger than beam size {}."").format(\n                 max_rank, decoder.beam_size))\n\n    return [BeamSearchRunner(""{}.rank{:03d}"".format(output_series, r),\n                             decoder, r, postprocess)\n            for r in range(1, max_rank + 1)]\n'"
neuralmonkey/runners/ctc_debug_runner.py,1,"b'from typing import Dict, List\n\nimport numpy as np\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.runners.base_runner import BaseRunner\nfrom neuralmonkey.decoders.ctc_decoder import CTCDecoder\nfrom neuralmonkey.decorators import tensor\n\n\nclass CTCDebugRunner(BaseRunner[CTCDecoder]):\n    """"""A runner that print out raw CTC output including the blank symbols.""""""\n\n    # pylint: disable=too-few-public-methods\n    # Pylint issue here: https://github.com/PyCQA/pylint/issues/2607\n    class Executable(BaseRunner.Executable[""CTCDebugRunner""]):\n\n        def collect_results(self, results: List[Dict]) -> None:\n\n            vocabulary = self.executor.decoder.vocabulary\n            if len(results) != 1:\n                raise RuntimeError(""CTCDebugRunners do not support ensembles."")\n\n            logits = results[0][""logits""]\n            argmaxes = np.argmax(logits, axis=2).T\n\n            decoded_batch = []\n            for indices in argmaxes:\n                decoded_instance = []\n                for index in indices:\n                    if index == len(vocabulary):\n                        symbol = ""<BLANK>""\n                    else:\n                        symbol = vocabulary.index_to_word[index]\n                    decoded_instance.append(symbol)\n                decoded_batch.append(decoded_instance)\n\n            self.set_runner_result(outputs=decoded_batch, losses=[])\n    # pylint: enable=too-few-public-methods\n\n    def __init__(self,\n                 output_series: str,\n                 decoder: CTCDecoder) -> None:\n        check_argument_types()\n        super().__init__(output_series, decoder)\n\n    @tensor\n    def fetches(self) -> Dict[str, tf.Tensor]:\n        return {""logits"": self.decoder.logits}\n\n    @property\n    def loss_names(self) -> List[str]:\n        return []\n'"
neuralmonkey/runners/dataset_runner.py,1,"b'from typing import List, Dict\nimport tensorflow as tf\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.model.feedable import Feedable\nfrom neuralmonkey.runners.base_runner import GraphExecutor\n\n\nclass DatasetRunner(GraphExecutor, Feedable):\n\n    # pylint: disable=too-few-public-methods\n    # Pylint issue here: https://github.com/PyCQA/pylint/issues/2607\n    class Executable(GraphExecutor.Executable[""DatasetRunner""]):\n\n        def collect_results(self, results: List[Dict]) -> None:\n            res = results[0]\n            size = res[""batch""]\n            self.set_result(res, {}, size, [])\n    # pylint: enable=too-few-public-methods\n\n    def __init__(self) -> None:\n        GraphExecutor.__init__(self, set())\n        Feedable.__init__(self)\n\n    @tensor\n    def fetches(self) -> Dict[str, tf.Tensor]:\n        assert self.dataset is not None\n        # TODO(tf-data) this will change to fetch real data\n        return {""batch"": self.batch_size}\n'"
neuralmonkey/runners/label_runner.py,1,"b'from typing import List, Dict, Callable\n\nimport numpy as np\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.vocabulary import END_TOKEN_INDEX\nfrom neuralmonkey.runners.base_runner import BaseRunner\nfrom neuralmonkey.decoders.sequence_labeler import SequenceLabeler\n\n# pylint: disable=invalid-name\nPostprocessor = Callable[[List[List[str]]], List[List[str]]]\n# pylint: enable=invalid-name\n\n\nclass LabelRunner(BaseRunner[SequenceLabeler]):\n\n    # pylint: disable=too-few-public-methods\n    # Pylint issue here: https://github.com/PyCQA/pylint/issues/2607\n    class Executable(BaseRunner.Executable[""LabelRunner""]):\n\n        def collect_results(self, results: List[Dict]) -> None:\n            loss = results[0].get(""loss"", 0.)\n            summed_logprobs = results[0][""label_logprobs""]\n            input_mask = results[0][""input_mask""]\n\n            for sess_result in results[1:]:\n                loss += sess_result.get(""loss"", 0.)\n                summed_logprobs = np.logaddexp(summed_logprobs,\n                                               sess_result[""label_logprobs""])\n                assert input_mask == sess_result[""input_mask""]\n\n            argmaxes = np.argmax(summed_logprobs, axis=2)\n\n            # We change all masked tokens to END_TOKEN\n            argmaxes -= END_TOKEN_INDEX\n            argmaxes *= input_mask.astype(int)\n            argmaxes += END_TOKEN_INDEX\n\n            # transpose argmaxes because vectors_to_sentences is time-major\n            vocabulary = self.executor.decoder.vocabulary\n            decoded_labels = vocabulary.vectors_to_sentences(argmaxes.T)\n\n            if self.executor.postprocess is not None:\n                decoded_labels = self.executor.postprocess(decoded_labels)\n\n            self.set_runner_result(outputs=decoded_labels, losses=[loss])\n    # pylint: enable=too-few-public-methods\n\n    def __init__(self,\n                 output_series: str,\n                 decoder: SequenceLabeler,\n                 postprocess: Postprocessor = None) -> None:\n        check_argument_types()\n        super().__init__(output_series, decoder)\n        self.postprocess = postprocess\n\n    @tensor\n    def fetches(self) -> Dict[str, tf.Tensor]:\n        return {\n            ""label_logprobs"": self.decoder.logprobs,\n            ""input_mask"": self.decoder.input_mask,\n            ""loss"": self.decoder.cost}\n\n    @property\n    def loss_names(self) -> List[str]:\n        return [""loss""]\n'"
neuralmonkey/runners/logits_runner.py,1,"b'""""""A runner outputing logits or normalized distriution from a decoder.""""""\n\nfrom typing import Dict, List, Optional\nfrom typeguard import check_argument_types\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom neuralmonkey.decoders.classifier import Classifier\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.runners.base_runner import BaseRunner\n\n\n# pylint: disable=too-few-public-methods\nclass LogitsRunner(BaseRunner[Classifier]):\n    """"""A runner which takes the output from decoder.decoded_logits.\n\n    The logits / normalized probabilities are outputted as tab-separates string\n    values. If the decoder produces a list of logits (as the recurrent\n    decoder), the tab separated arrays are separated with commas.\n    Alternatively, we may be interested in a single distribution dimension.\n    """"""\n\n    class Executable(BaseRunner.Executable[""LogitsRunner""]):\n\n        def collect_results(self, results: List[Dict]) -> None:\n            if len(results) != 1:\n                raise ValueError(""LogitsRunner needs exactly 1 execution ""\n                                 ""result, got {}"".format(len(results)))\n\n            train_loss = results[0][""train_loss""]\n            runtime_loss = results[0][""runtime_loss""]\n\n            # logits_list in shape (time, batch, vocab)\n            logits_list = results[0][""logits""]\n\n            # outputs are lists of strings (batch, time)\n            outputs = [[] for _ in logits_list[0]]  # type: List[List[str]]\n\n            for time_step in logits_list:\n                for logits, output_list in zip(time_step, outputs):\n\n                    if self.executor.normalize:\n                        logits = np.exp(logits) / np.sum(np.exp(logits),\n                                                         axis=0)\n                    if self.executor.pick_index:\n                        instance_logits = str(logits[self.executor.pick_index])\n                    else:\n                        instance_logits = "","".join(str(l) for l in logits)\n\n                    output_list.append(instance_logits)\n\n            str_outputs = [[""\\t"".join(l)] for l in outputs]\n\n            self.set_runner_result(outputs=str_outputs,\n                                   losses=[train_loss, runtime_loss])\n\n    def __init__(self,\n                 output_series: str,\n                 decoder: Classifier,\n                 normalize: bool = True,\n                 pick_index: int = None,\n                 pick_value: str = None) -> None:\n        """"""Initialize the logits runner.\n\n        Args:\n            output_series: Name of the series produced by the runner.\n            decoder: A decoder having logits.\n            normalize: Flag whether the logits should be normalized with\n                softmax.\n            pick_index: If not None, it specifies the index of the logit or the\n                probability that should be on output.\n            pick_value: If not None, it specifies a value from the decoder\'s\n                vocabulary whose logit or probability should be on output.\n        """"""\n        check_argument_types()\n        super().__init__(output_series, decoder)\n\n        if pick_index is not None and pick_value is not None:\n            raise ValueError(""Either a pick index or a vocabulary value can ""\n                             ""be specified, not both at the same time."")\n\n        self.pick_index = None  # type: Optional[int]\n\n        self.normalize = normalize\n        if pick_value is not None:\n            if pick_value in self.decoder.vocabulary:\n                self.pick_index = self.decoder.vocabulary.index_to_word.index(\n                    pick_value)\n            else:\n                raise ValueError(\n                    ""Value \'{}\' is not in vocabulary of decoder \'{}\'"".format(\n                        pick_value, decoder.name))\n        else:\n            self.pick_index = pick_index\n\n    @tensor\n    def fetches(self) -> Dict[str, tf.Tensor]:\n        return {""logits"": self.decoder.decoded_logits,\n                ""train_loss"": self.decoder.train_loss,\n                ""runtime_loss"": self.decoder.runtime_loss}\n\n    @property\n    def loss_names(self) -> List[str]:\n        return [""train_loss"", ""runtime_loss""]\n'"
neuralmonkey/runners/plain_runner.py,1,"b'from typing import Dict, List, Union, Callable\n\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.decoders.autoregressive import AutoregressiveDecoder\nfrom neuralmonkey.decoders.ctc_decoder import CTCDecoder\nfrom neuralmonkey.decoders.classifier import Classifier\nfrom neuralmonkey.decoders.sequence_labeler import SequenceLabeler\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.runners.base_runner import BaseRunner\n\n# pylint: disable=invalid-name\nSupportedDecoder = Union[\n    AutoregressiveDecoder, CTCDecoder, Classifier, SequenceLabeler]\nPostprocessor = Callable[[List[List[str]]], List[List[str]]]\n# pylint: enable=invalid-name\n\n\nclass PlainRunner(BaseRunner[SupportedDecoder]):\n    """"""A runner which takes the output from decoder.decoded.""""""\n\n    # pylint: disable=too-few-public-methods\n    # Pylint issue here: https://github.com/PyCQA/pylint/issues/2607\n    class Executable(BaseRunner.Executable[""PlainRunner""]):\n\n        def collect_results(self, results: List[Dict]) -> None:\n            if len(results) != 1:\n                raise ValueError(""PlainRunner needs exactly 1 execution ""\n                                 ""result, got {}"".format(len(results)))\n\n            vocabulary = self.executor.decoder.vocabulary\n\n            train_loss = results[0][""train_loss""]\n            runtime_loss = results[0][""runtime_loss""]\n            decoded = results[0][""decoded""]\n\n            decoded_tokens = vocabulary.vectors_to_sentences(decoded)\n\n            if self.executor.postprocess is not None:\n                decoded_tokens = self.executor.postprocess(decoded_tokens)\n\n            self.set_runner_result(outputs=decoded_tokens,\n                                   losses=[train_loss, runtime_loss])\n    # pylint: enable=too-few-public-methods\n\n    def __init__(self,\n                 output_series: str,\n                 decoder: SupportedDecoder,\n                 postprocess: Postprocessor = None) -> None:\n        check_argument_types()\n        super().__init__(output_series, decoder)\n        self.postprocess = postprocess\n\n    @tensor\n    def fetches(self) -> Dict[str, tf.Tensor]:\n        return {""decoded"": self.decoder.decoded,\n                ""train_loss"": self.decoder.train_loss,\n                ""runtime_loss"": self.decoder.runtime_loss}\n\n    @property\n    def loss_names(self) -> List[str]:\n        return [""train_loss"", ""runtime_loss""]\n'"
neuralmonkey/runners/regression_runner.py,1,"b'from typing import Dict, List, Callable\n\nimport numpy as np\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.decoders.sequence_regressor import SequenceRegressor\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.runners.base_runner import BaseRunner\n\n# pylint: disable=invalid-name\nPostprocessor = Callable[[List[float]], List[float]]\n# pylint: enable=invalid-name\n\n\nclass RegressionRunner(BaseRunner[SequenceRegressor]):\n    """"""A runnner that takes the predictions of a sequence regressor.""""""\n\n    # pylint: disable=too-few-public-methods\n    # Pylint issue here: https://github.com/PyCQA/pylint/issues/2607\n    class Executable(BaseRunner.Executable[""RegressionRunner""]):\n\n        def collect_results(self, results: List[Dict]) -> None:\n            predictions_sum = np.zeros_like(results[0][""prediction""])\n            mse_loss = 0.\n\n            for sess_result in results:\n                if ""mse"" in sess_result:\n                    mse_loss += sess_result[""mse""]\n\n                predictions_sum += sess_result[""prediction""]\n\n            predictions = (predictions_sum / len(results)).tolist()\n\n            if self.executor.postprocess is not None:\n                predictions = self.executor.postprocess(predictions)\n\n            self.set_runner_result(outputs=predictions, losses=[mse_loss])\n    # pylint: enable=too-few-public-methods\n\n    def __init__(self,\n                 output_series: str,\n                 decoder: SequenceRegressor,\n                 postprocess: Postprocessor = None) -> None:\n        check_argument_types()\n        super().__init__(output_series, decoder)\n        self.postprocess = postprocess\n\n    @tensor\n    def fetches(self) -> Dict[str, tf.Tensor]:\n        return {""prediction"": self.decoder.predictions,\n                ""mse"": self.decoder.cost}\n\n    @property\n    def loss_names(self) -> List[str]:\n        return [""mse""]\n'"
neuralmonkey/runners/runner.py,5,"b'from typing import Dict, List, Callable, Union\n\nimport numpy as np\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.runners.base_runner import BaseRunner, NextExecute\nfrom neuralmonkey.decoders.autoregressive import AutoregressiveDecoder\nfrom neuralmonkey.decoders.classifier import Classifier\nfrom neuralmonkey.decorators import tensor\n\n# pylint: disable=invalid-name\nSupportedDecoder = Union[AutoregressiveDecoder, Classifier]\nPostprocessor = Callable[[List[List[str]]], List[List[str]]]\n# pylint: enable=invalid-name\n\n\nclass GreedyRunner(BaseRunner[SupportedDecoder]):\n\n    class Executable(BaseRunner.Executable[""GreedyRunner""]):\n\n        def next_to_execute(self) -> NextExecute:\n            """"""Get the tensors and additional feed dicts for execution.""""""\n            fetches = self.executor.fetches\n\n            if not self.summaries:\n                fetches[""image_summaries""] = None\n\n            if not self.compute_losses:\n                fetches[""train_xent""] = tf.zeros([])\n                fetches[""runtime_xent""] = tf.zeros([])\n\n            return fetches, []\n\n        def collect_results(self, results: List[Dict]) -> None:\n            train_loss = 0.\n            runtime_loss = 0.\n            summed_logprobs = [-np.inf for _ in range(\n                results[0][""decoded_logprobs""].shape[0])]\n\n            for sess_result in results:\n                train_loss += sess_result[""train_xent""]\n                runtime_loss += sess_result[""runtime_xent""]\n\n                for i, logprob in enumerate(sess_result[""decoded_logprobs""]):\n                    summed_logprobs[i] = np.logaddexp(\n                        summed_logprobs[i], logprob)\n\n            argmaxes = [np.argmax(l, axis=1) for l in summed_logprobs]\n\n            decoded_tokens = self.executor.vocabulary.vectors_to_sentences(\n                argmaxes)\n\n            if self.executor.postprocess is not None:\n                decoded_tokens = self.executor.postprocess(decoded_tokens)\n\n            summaries = None\n            if ""image_summaries"" in results[0]:\n                summaries = [results[0][""image_summaries""]]\n\n            self.set_runner_result(\n                outputs=decoded_tokens, losses=[train_loss, runtime_loss],\n                summaries=summaries)\n\n    def __init__(self,\n                 output_series: str,\n                 decoder: SupportedDecoder,\n                 postprocess: Postprocessor = None) -> None:\n        check_argument_types()\n        super().__init__(output_series, decoder)\n\n        self.postprocess = postprocess\n        self.vocabulary = self.decoder.vocabulary\n\n    @tensor\n    def fetches(self) -> Dict[str, tf.Tensor]:\n\n        fetches = {""decoded_logprobs"": self.decoder.runtime_logprobs,\n                   ""train_xent"": self.decoder.train_loss,\n                   ""runtime_xent"": self.decoder.runtime_loss}\n\n        att_plot_summaries = tf.get_collection(""summary_att_plots"")\n        if att_plot_summaries:\n            fetches[""image_summaries""] = tf.summary.merge(att_plot_summaries)\n\n        return fetches\n\n    @property\n    def loss_names(self) -> List[str]:\n        return [""train_xent"", ""runtime_xent""]\n'"
neuralmonkey/runners/tensor_runner.py,2,"b'from typing import Dict, List\n\nimport numpy as np\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.logging import warn\nfrom neuralmonkey.model.model_part import GenericModelPart\nfrom neuralmonkey.runners.base_runner import BaseRunner\nfrom neuralmonkey.experiment import Experiment\n\n\nclass TensorRunner(BaseRunner[GenericModelPart]):\n    """"""Runner class for printing tensors from a model.\n\n    Use this runner if you want to retrieve a specific tensor from the model\n    using a given dataset. The runner generates an output data series which\n    will contain the tensors in a dictionary of numpy arrays.\n    """"""\n\n    # pylint: disable=too-few-public-methods\n    # Pylint issue here: https://github.com/PyCQA/pylint/issues/2607\n    class Executable(BaseRunner.Executable[""TensorRunner""]):\n\n        def collect_results(self, results: List[Dict]) -> None:\n            if len(results) > 1 and self.executor.select_session is None:\n                sessions = []\n                for res_dict in results:\n                    sessions.append(self._fetch_values_from_session(res_dict))\n\n                    # one call returns a list of dicts. we need to add another\n                    # list dimension in between, so it\'ll become a 2D list of\n                    # dicts with dimensions (batch, session, tensor_name) the\n                    # ``sessions`` structure is of \'shape\' (session, batch,\n                    # tensor_name) so it should be sufficient to transpose it:\n                    batched = list(zip(*sessions))\n            else:\n                batched = self._fetch_values_from_session(results[0])\n\n            self.set_runner_result(outputs=batched, losses=[])\n\n        def _fetch_values_from_session(self, sess_results: Dict) -> List:\n\n            transposed = {}\n            for name, val in sess_results.items():\n                batch_dim = self.executor.batch_ids[name]\n\n                perm = [batch_dim]\n                for dim in range(len(val.shape)):\n                    if dim != batch_dim:\n                        perm.append(dim)\n\n                transposed_val = np.transpose(val, perm)\n                transposed[name] = transposed_val\n\n            # now we have dict of tensors in batch. we need\n            # to have a batch of dicts with the batch dim removed\n            batched = [dict(zip(transposed, col))\n                       for col in zip(*transposed.values())]\n\n            if self.executor.single_tensor:\n                # extract the only item from each dict\n                batched = [next(iter(d.values())) for d in batched]\n\n            return batched\n    # pylint: enable=too-few-public-methods\n\n    # pylint: disable=too-many-arguments\n    def __init__(self,\n                 output_series: str,\n                 modelparts: List[GenericModelPart],\n                 tensors: List[str],\n                 batch_dims: List[int],\n                 tensors_by_name: List[str],\n                 batch_dims_by_name: List[int],\n                 select_session: int = None,\n                 single_tensor: bool = False) -> None:\n        """"""Construct a new ``TensorRunner`` object.\n\n        Note that at this time, one must specify the toplevel objects so that\n        it is ensured that the graph is built. The reason for this behavior is\n        that the graph is constructed lazily and therefore if the tensors to\n        store are provided by indirect reference (name), the system does not\n        know early enough that it needs to create them.\n\n        Args:\n            output_series: The name of the generated output data series.\n            modelparts: A list of ``GenericModelPart`` objects that hold the\n                tensors that will be retrieved.\n            tensors: A list of names of tensors that should be retrieved.\n            batch_dims_by_ref: A list of integers that correspond to the\n                batch dimension in each wanted tensor.\n            tensors_by_name: A list of tensor names to fetch. If a tensor\n                is not in the graph, a warning is generated and the tensor is\n                ignored.\n            batch_dims_by_name: A list of integers that correspond to the\n                batch dimension in each wanted tensor specified by name.\n            select_session: An optional integer specifying the session to use\n                in case of ensembling. When not used, tensors from all sessions\n                are stored. In case of a single session, this option has no\n                effect.\n            single_tensor: If `True`, it is assumed that only one tensor is to\n                be fetched, and the execution result will consist of this\n                tensor only. If `False`, the result will be a dict mapping\n                tensor names to NumPy arrays.\n        """"""\n        check_argument_types()\n\n        if not modelparts:\n            raise ValueError(""At least one model part is expected"")\n\n        super().__init__(output_series, modelparts[0])\n\n        if len(modelparts) != len(tensors):\n            raise ValueError(""TensorRunner: \'modelparts\' and \'tensors\' lists ""\n                             ""must have the same length"")\n\n        total_tensors = len(tensors_by_name) + len(tensors)\n        if single_tensor and total_tensors > 1:\n            raise ValueError(""single_tensor is True, but {} tensors were given""\n                             .format(total_tensors))\n\n        self._names = tensors_by_name\n        self._modelparts = modelparts\n        self._tensors = tensors\n        self._batch_dims_name = batch_dims_by_name\n        self.batch_dims = batch_dims\n        self.select_session = select_session\n        self.single_tensor = single_tensor\n\n        self.batch_ids = {}  # type: Dict[str, int]\n    # pylint: enable=too-many-arguments\n\n    @tensor\n    def fetches(self) -> Dict[str, tf.Tensor]:\n\n        fetches = {}  # type: Dict[str, tf.Tensor]\n        for name, bid in zip(self._names, self._batch_dims_name):\n            try:\n                fetches[name] = (\n                    Experiment.get_current().graph.get_tensor_by_name(name))\n                self.batch_ids[name] = bid\n            except KeyError:\n                warn((""The tensor of name \'{}\' is not present in the ""\n                      ""graph."").format(name))\n\n        for mpart, tname, bid in zip(self._modelparts, self._tensors,\n                                     self.batch_dims):\n            if not hasattr(mpart, tname):\n                raise ValueError(""Model part {} does not have a tensor called ""\n                                 ""{}."".format(mpart, tname))\n\n            tensorval = getattr(mpart, tname)\n\n            fetches[tensorval.name] = tensorval\n            self.batch_ids[tensorval.name] = bid\n\n        return fetches\n\n    @property\n    def loss_names(self) -> List[str]:\n        return []\n\n\nclass RepresentationRunner(TensorRunner):\n    """"""Runner printing out representation from an encoder.\n\n    Use this runner to get input / other data representation out from one of\n    Neural Monkey encoders.\n    """"""\n\n    def __init__(self,\n                 output_series: str,\n                 encoder: GenericModelPart,\n                 attribute: str = ""output"",\n                 select_session: int = None) -> None:\n        """"""Initialize the representation runner.\n\n        Args:\n            output_series: Name of the output series with vectors.\n            encoder: The encoder to use. This can be any ``GenericModelPart``\n                object.\n            attribute: The name of the encoder attribute that contains the\n                data.\n            used_session: Id of the TensorFlow session used in case of model\n                ensembles.\n        """"""\n        check_argument_types()\n\n        if attribute not in dir(encoder):\n            warn(""The encoder \'{}\' seems not to have the specified ""\n                 ""attribute \'{}\'"".format(encoder, attribute))\n\n        TensorRunner.__init__(\n            self,\n            output_series,\n            modelparts=[encoder],\n            tensors=[attribute],\n            batch_dims=[0],\n            tensors_by_name=[],\n            batch_dims_by_name=[],\n            select_session=select_session,\n            single_tensor=True)\n'"
neuralmonkey/runners/word_alignment_runner.py,2,"b'from typing import Dict, List\n\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.attention.base_attention import BaseAttention\nfrom neuralmonkey.decoders.decoder import Decoder\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.runners.base_runner import BaseRunner\n\n\nclass WordAlignmentRunner(BaseRunner[BaseAttention]):\n\n    # pylint: disable=too-few-public-methods\n    # Pylint issue here: https://github.com/PyCQA/pylint/issues/2607\n    class Executable(BaseRunner.Executable[""WordAlignmentRunner""]):\n\n        def collect_results(self, results: List[Dict]) -> None:\n            self.set_runner_result(outputs=results[0][""alignment""], losses=[])\n    # pylint: enable=too-few-public-methods\n\n    def __init__(self,\n                 output_series: str,\n                 attention: BaseAttention,\n                 decoder: Decoder) -> None:\n        check_argument_types()\n        super().__init__(output_series, attention)\n\n        self._key = ""{}_run"".format(decoder.name)\n\n    @tensor\n    def fetches(self) -> Dict[str, tf.Tensor]:\n        if self._key not in self.decoder.histories:\n            raise KeyError(""Attention has no recorded histories under ""\n                           ""key \'{}\'"".format(self._key))\n\n        att_histories = self.decoder.histories[self._key]\n        alignment = tf.transpose(att_histories, perm=[1, 2, 0])\n\n        return {""alignment"": alignment}\n\n    @property\n    def loss_names(self) -> List[str]:\n        return []\n'"
neuralmonkey/runners/xent_runner.py,1,"b'from typing import Dict, List, Union\n\nfrom typeguard import check_argument_types\nimport tensorflow as tf\nimport numpy as np\n\nfrom neuralmonkey.decoders.autoregressive import AutoregressiveDecoder\nfrom neuralmonkey.decoders.sequence_labeler import SequenceLabeler\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.runners.base_runner import BaseRunner\n\nSupportedDecoders = Union[AutoregressiveDecoder, SequenceLabeler]\n\n\nclass XentRunner(BaseRunner[SupportedDecoders]):\n\n    # pylint: disable=too-few-public-methods\n    # Pylint issue here: https://github.com/PyCQA/pylint/issues/2607\n    class Executable(BaseRunner.Executable[""XentRunner""]):\n\n        def collect_results(self, results: List[Dict]) -> None:\n            xents = np.mean([res[""xents""] for res in results], axis=0)\n            self.set_runner_result(outputs=xents.tolist(),\n                                   losses=[float(np.mean(xents))])\n    # pylint: enable=too-few-public-methods\n\n    def __init__(self,\n                 output_series: str,\n                 decoder: SupportedDecoders) -> None:\n        check_argument_types()\n        super().__init__(output_series, decoder)\n\n    @tensor\n    def fetches(self) -> Dict[str, tf.Tensor]:\n        return {""xents"": self.decoder.train_xents}\n\n    @property\n    def loss_names(self) -> List[str]:\n        return [""xent""]\n'"
neuralmonkey/server/server.py,0,"b'# pylint: disable=unused-import, wrong-import-order\nimport neuralmonkey.checkpython\n# pylint: enable=unused-import, wrong-import-order\n\nimport argparse\nimport os\nimport json\nimport datetime\n\nimport flask\nfrom flask import Flask, request, Response, render_template\nimport numpy as np\n\nfrom neuralmonkey.config.configuration import Configuration\nfrom neuralmonkey.dataset import Dataset, BatchingScheme\nfrom neuralmonkey.experiment import Experiment\n\n\nAPP = Flask(__name__)\nAPP.config.from_object(__name__)\nAPP.config[""experiment""] = None\n\n\ndef root_dir():  # pragma: no cover\n    return os.path.abspath(os.path.dirname(__file__))\n\n\ndef get_file(filename):  # pragma: no cover\n    src = os.path.join(root_dir(), filename)\n    return open(src).read()\n\n\ndef run(data):  # pragma: no cover\n    exp = APP.config[""experiment""]\n    dataset = Dataset(\n        ""request"", data, BatchingScheme(batch_size=1), {},\n        preprocessors=APP.config[""preprocess""])\n\n    _, response_data, _ = exp.run_model(dataset, write_out=False)\n\n    return response_data\n\n\n@APP.route(""/"", methods=[""GET"", ""POST""])\ndef index():\n    if request.method == ""POST"":\n        source_text = request.form[""source""]\n        data = {""source"": [source_text.split()]}\n        translation_response = run(data)\n        translation = "" "".join(translation_response[""target""][0])\n    else:\n        source_text = ""enter tokenized soruce language text here .""\n        translation = """"\n\n    return render_template(\n        ""server.html"", translation=translation, source=source_text)\n\n\n@APP.route(""/run"", methods=[""POST""])\ndef post_request():\n    start_time = datetime.datetime.now()\n    request_data = request.get_json()\n\n    if request_data is None:\n        response_data = {""error"": ""No data were provided.""}\n        code = 400\n    else:\n        try:\n            response_data = run(request_data)\n            code = 200\n        # pylint: disable=broad-except\n        except Exception as exc:\n            response_data = {""error"": str(exc)}\n            code = 400\n\n    # take care of tensors returned by tensor runner\n    for key, value in response_data.items():\n        if isinstance(value[0], dict):\n            new_value = [\n                {k: v.tolist() for k, v in val.items()} for val in value]\n            response_data[key] = new_value\n        if isinstance(value[0], np.ndarray):\n            response_data[key] = [x.tolist() for x in value]\n\n    response_data[""duration""] = (\n        datetime.datetime.now() - start_time).total_seconds()\n    json_response = json.dumps(response_data)\n    response = flask.Response(json_response,\n                              content_type=""application/json; charset=utf-8"")\n    response.headers.add(""content-length"", len(json_response.encode(""utf-8"")))\n    response.status_code = code\n    return response\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(\n        description=""Runs Neural Monkey as a web server."")\n    parser.add_argument(""--port"", type=int, default=5000)\n    parser.add_argument(""--host"", type=str, default=""127.0.0.1"")\n    parser.add_argument(""--configuration"", type=str, required=True)\n    parser.add_argument(""--preprocess"", type=str,\n                        required=False, default=None)\n    args = parser.parse_args()\n\n    print("""")\n\n    if args.preprocess is not None:\n        preprocessing = Configuration()\n        preprocessing.add_argument(""preprocess"")\n        preprocessing.load_file(args.preprocess)\n        preprocessing.build_model()\n        APP.config[""preprocess""] = preprocessing.model.preprocess\n    else:\n        APP.config[""preprocess""] = []\n\n    exp = Experiment(config_path=args.configuration)\n    exp.build_model()\n    APP.config[""experiment""] = exp\n    APP.run(port=args.port, host=args.host)\n'"
neuralmonkey/tests/__init__.py,0,b''
neuralmonkey/tests/test_bleu.py,0,"b'#!/usr/bin/env python3.5\n\n\nimport unittest\n\nfrom neuralmonkey.evaluators.bleu import BLEUEvaluator\n\n\nCORPUS_DECODED = [\n    ""colorful thoughts furiously sleep"",\n    ""little piglet slept all night"",\n    ""working working working working working be be be be be be be"",\n    ""ich bin walrus"",\n    ""walrus for pr\xc3\xa4sident""\n]\n\nCORPUS_REFERENCE = [\n    ""the colorless ideas slept furiously"",\n    ""pooh slept all night"",\n    ""working class hero is something to be"",\n    ""I am the working class walrus"",\n    ""walrus for president""\n]\n\n\nDECODED = [d.split() for d in CORPUS_DECODED]\nREFERENCE = [r.split() for r in CORPUS_REFERENCE]\n\nFUNC = BLEUEvaluator()\n\n\nclass TestBLEU(unittest.TestCase):\n\n    def test_empty_decoded(self):\n        self.assertEqual(FUNC([[] for _ in DECODED], REFERENCE), 0)\n\n    def test_empty_reference(self):\n        score = FUNC(DECODED, [[] for _ in REFERENCE])\n        self.assertIsInstance(score, float)\n\n    def test_identical(self):\n        self.assertEqual(FUNC(REFERENCE, REFERENCE), 100)\n\n    def test_empty_sentence(self):\n        ref_empty = REFERENCE + [[]]\n        out_empty = DECODED + [[""something""]]\n        score = FUNC(out_empty, ref_empty)\n        self.assertAlmostEqual(score, 15, delta=10)\n\n    def test_bleu(self):\n        score = FUNC(DECODED, REFERENCE)\n        self.assertAlmostEqual(score, 15, delta=10)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
neuralmonkey/tests/test_chrf.py,0,"b'#!/usr/bin/env python3.5\n\n\nimport unittest\n\nfrom neuralmonkey.evaluators.chrf import ChrFEvaluator, _get_ngrams\nfrom neuralmonkey.tests.test_bleu import DECODED, REFERENCE\n\n\nTOKENS = [""a"", ""b"", ""a""]\nNGRAMS = [\n    {""a"": 2, ""b"": 1},\n    {""ab"": 1, ""ba"": 1},\n    {""aba"": 1},\n    {}]\n\nFUNC = ChrFEvaluator()\nFUNC_P = FUNC.chr_p\nFUNC_R = FUNC.chr_r\n\n\nclass TestChrF(unittest.TestCase):\n\n    def test_empty_decoded(self):\n        # Recall == 0.0\n        self.assertEqual(FUNC([[] for _ in DECODED], REFERENCE), 0.0)\n\n    def test_empty_reference(self):\n        # Precision == 0.0\n        self.assertEqual(FUNC([[] for _ in REFERENCE], DECODED), 0.0)\n\n    def test_identical(self):\n        self.assertEqual(FUNC(REFERENCE, REFERENCE), 1.0)\n\n    def test_empty_sentence(self):\n        ref_empty = REFERENCE + [[]]\n        out_empty = DECODED + [[""something""]]\n        score = FUNC(out_empty, ref_empty)\n        self.assertAlmostEqual(score, 0.38, delta=10)\n\n    def test_chrf(self):\n        score = FUNC(DECODED, REFERENCE)\n        self.assertAlmostEqual(score, 0.46, delta=10)\n\n    def test_get_ngrams(self):\n        tokens = [""a"", ""b"", ""a""]\n        ngrams_out = _get_ngrams(tokens, 4)\n        self.assertEqual(len(ngrams_out), 4)\n        for i, _ in enumerate(NGRAMS):\n            self.assertDictEqual(ngrams_out[i], NGRAMS[i])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
neuralmonkey/tests/test_config.py,0,"b'#!/usr/bin/env python3.5\n"""""" Tests the config parsing module. """"""\n# pylint: disable=protected-access\n\nimport unittest\nfrom neuralmonkey.config import parsing\n\nSPLITTER_TESTS = [\n    [""empty"", """", []],\n    [""only_commas"", "",,,,,,"", []],\n    [""commas_and_whitespace"", "",    ,   ,,   , , "", []],\n    [""no_commas"", ""without"", [""without""]],\n    [""common"", ""a,b,c"", [""a"", ""b"", ""c""]],\n    [""brackets"", ""(brackets),(brac,kets)"", [""(brackets)"", ""(brac,kets)""]],\n]\n\n\nclass TestParsing(unittest.TestCase):\n\n    def test_splitter_bad_brackets(self):\n        self.assertRaises(Exception, parsing._split_on_commas,\n                          ""(omg,brac],kets"")\n\n    def test_parse_value_int(self):\n        self.assertEqual(parsing._parse_value(""42"", {}), 42)\n        self.assertEqual(parsing._parse_value(""-42"", {}), -42)\n\n    def test_parse_value_float(self):\n        self.assertAlmostEqual(parsing._parse_value(""0.5e-1"", {}), 0.5e-1)\n        self.assertAlmostEqual(parsing._parse_value("".5e-1"", {}), .5e-1)\n        self.assertAlmostEqual(parsing._parse_value(""-.5e-1"", {}), -.5e-1)\n        self.assertAlmostEqual(parsing._parse_value(""5.e-1"", {}), 5.e-1)\n\n    def test_parse_value_string(self):\n        varz = {""pi"": 3.14159, ""greeting"": ""hello""}\n        self.assertEqual(parsing._parse_value(\'""{greeting}""world""\', varz),\n                         \'hello""world\')\n        self.assertEqual(parsing._parse_value(\'""pi = {pi:.0f}""\', varz),\n                         ""pi = 3"")\n\n\ndef test_splitter_gen(a, b):\n    def test_case_fun(self):\n        out = parsing._split_on_commas(a)\n        self.assertEqual(out, b)\n    return test_case_fun\n\n\nif __name__ == ""__main__"":\n    for case in SPLITTER_TESTS:\n        test_name = ""test_{}"".format(case[0])\n        test = test_splitter_gen(case[1], case[2])\n        setattr(TestParsing, test_name, test)\n    unittest.main()\n'"
neuralmonkey/tests/test_dataset.py,0,"b'#!/usr/bin/env python3.5\n\nfrom typing import Iterable, List\nimport os\nimport tempfile\nimport unittest\n\nfrom neuralmonkey.dataset import Dataset, load, BatchingScheme\nfrom neuralmonkey.readers.plain_text_reader import UtfPlainTextReader\n\nDEFAULT_BATCHING_SCHEME = BatchingScheme(batch_size=3)\n\n\nclass TestDataset(unittest.TestCase):\n\n    def test_nonexistent_file(self) -> None:\n        with self.assertRaises(FileNotFoundError):\n            load(name=""name"",\n                 series=[""source""],\n                 data=[([""some_nonexistent_file""], UtfPlainTextReader)],\n                 batching=DEFAULT_BATCHING_SCHEME,\n                 buffer_size=5)\n\n    def test_lazy_dataset(self) -> None:\n        i = 0  # iteration counter\n\n        def reader(files: List[str]) -> Iterable[List[str]]:\n            del files\n            nonlocal i\n            for i in range(10):  # pylint: disable=unused-variable\n                yield [""foo""]\n\n        dataset = load(\n            name=""data"",\n            series=[""source"", ""source_prep""],\n            data=[([""tests/data/train.tc.en""], reader),\n                  (lambda x: x, ""source"")],\n            batching=DEFAULT_BATCHING_SCHEME,\n            buffer_size=5)\n\n        series = dataset.get_series(""source_prep"")\n\n        # Check that the reader is being iterated lazily\n        for j, _ in enumerate(series):\n            self.assertEqual(i, j)\n        self.assertEqual(i, 9)\n\n    def test_glob(self):\n        filenames = sorted([""abc1"", ""abc2"", ""abcxx"", ""xyz""])\n        contents = [""a"", ""b"", ""c"", ""d""]\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            for fname, text in zip(filenames, contents):\n                with open(os.path.join(tmp_dir, fname), ""w"") as file:\n                    print(text, file=file)\n\n            dataset = load(\n                name=""dataset"",\n                series=[""data""],\n                data=[[os.path.join(tmp_dir, ""abc?""),\n                       os.path.join(tmp_dir, ""xyz*"")]],\n                batching=DEFAULT_BATCHING_SCHEME)\n\n            series_iterator = dataset.get_series(""data"")\n            self.assertEqual(list(series_iterator), [[""a""], [""b""], [""d""]])\n\n    def test_batching_eager_noshuffle(self):\n        iterators = {\n            ""a"": lambda: range(5),\n            ""b"": lambda: range(10, 15)\n        }\n\n        dataset = Dataset(\n            ""dataset"", iterators=iterators, batching=DEFAULT_BATCHING_SCHEME,\n            shuffled=False)\n\n        batches = []\n        for epoch in range(2):\n            epoch = []\n            for batch in dataset.batches():\n                epoch.append({s: list(batch.get_series(s)) for s in iterators})\n\n            batches.append(epoch)\n\n        self.assertEqual(\n            batches, [[{""a"": [0, 1, 2], ""b"": [10, 11, 12]},\n                       {""a"": [3, 4], ""b"": [13, 14]}],\n                      [{""a"": [0, 1, 2], ""b"": [10, 11, 12]},\n                       {""a"": [3, 4], ""b"": [13, 14]}]])\n\n    def test_batching_lazy_noshuffle(self):\n        iterators = {\n            ""a"": lambda: range(5),\n            ""b"": lambda: range(10, 15)\n        }\n\n        dataset = Dataset(\n            ""dataset"", iterators=iterators, batching=DEFAULT_BATCHING_SCHEME,\n            shuffled=False, buffer_size=(3, 5))\n\n        batches = []\n        for epoch in range(2):\n            epoch = []\n            for batch in dataset.batches():\n                epoch.append({s: list(batch.get_series(s)) for s in iterators})\n\n            batches.append(epoch)\n\n        self.assertEqual(\n            batches, [[{""a"": [0, 1, 2], ""b"": [10, 11, 12]},\n                       {""a"": [3, 4], ""b"": [13, 14]}],\n                      [{""a"": [0, 1, 2], ""b"": [10, 11, 12]},\n                       {""a"": [3, 4], ""b"": [13, 14]}]])\n\n    def test_batching_eager_shuffle(self):\n        iterators = {\n            ""a"": lambda: range(5),\n            ""b"": lambda: range(5, 10)\n        }\n\n        dataset = Dataset(""dataset"", iterators=iterators,\n                          batching=DEFAULT_BATCHING_SCHEME, shuffled=True)\n\n        batches = []\n        for epoch in range(2):\n            epoch = []\n            for batch in dataset.batches():\n                epoch.append({s: list(batch.get_series(s)) for s in iterators})\n\n            batches.append(epoch)\n\n        epoch_data = []\n        epoch_data.append(\n            [c for batch in batches[0] for b in batch.values() for c in b])\n        epoch_data.append(\n            [c for batch in batches[1] for b in batch.values() for c in b])\n\n        self.assertEqual(set(epoch_data[0]), set(range(10)))\n        self.assertEqual(set(epoch_data[0]), set(epoch_data[1]))\n        self.assertNotEqual(epoch_data[0], epoch_data[1])\n\n    def test_batching_lazy_shuffle(self):\n        iterators = {\n            ""a"": lambda: range(5),\n            ""b"": lambda: range(5, 10)\n        }\n\n        dataset = Dataset(\n            ""dataset"", iterators=iterators, batching=DEFAULT_BATCHING_SCHEME,\n            shuffled=True, buffer_size=(3, 5))\n\n        batches = []\n        for epoch in range(2):\n            epoch = []\n            for batch in dataset.batches():\n                epoch.append({s: list(batch.get_series(s)) for s in iterators})\n\n            batches.append(epoch)\n\n        epoch_data = []\n        epoch_data.append(\n            [c for batch in batches[0] for b in batch.values() for c in b])\n        epoch_data.append(\n            [c for batch in batches[1] for b in batch.values() for c in b])\n\n        self.assertEqual(set(epoch_data[0]), set(range(10)))\n        self.assertEqual(set(epoch_data[0]), set(epoch_data[1]))\n        self.assertNotEqual(epoch_data[0], epoch_data[1])\n\n    def test_bucketing(self):\n\n        # testing dataset is 50 sequences of lengths 1 - 50\n        iterators = {\n            ""sentences"": lambda: ([""word"" for _ in range(l)]\n                                  for l in range(1, 50))\n        }\n\n        # we use batch size 7 and bucket span 10\n        scheme = BatchingScheme(bucket_boundaries=[9, 19, 29, 39, 49],\n                                bucket_batch_sizes=[7, 7, 7, 7, 7, 7])\n\n        dataset = Dataset(""dataset"", iterators=iterators,\n                          batching=scheme, shuffled=False)\n\n        # we process the dataset in two epochs and save what did the batches\n        # look like\n        batches = []\n        for batch in dataset.batches():\n            batches.append(list(batch.get_series(""sentences"")))\n\n        ref_batches = [\n            [[""word"" for _ in range(l)] for l in range(1, 8)],\n            [[""word"" for _ in range(l)] for l in range(10, 17)],\n            [[""word"" for _ in range(l)] for l in range(20, 27)],\n            [[""word"" for _ in range(l)] for l in range(30, 37)],\n            [[""word"" for _ in range(l)] for l in range(40, 47)],\n            [[""word"" for _ in range(l)] for l in range(8, 10)],\n            [[""word"" for _ in range(l)] for l in range(17, 20)],\n            [[""word"" for _ in range(l)] for l in range(27, 30)],\n            [[""word"" for _ in range(l)] for l in range(37, 40)],\n            [[""word"" for _ in range(l)] for l in range(47, 50)]]\n\n        self.assertSequenceEqual(ref_batches, batches)\n\n    def test_bucketing_no_leftovers(self):\n\n        # testing dataset is 50 sequences of lengths 1 - 50\n        iterators = {\n            ""sentences"": lambda: ([""word"" for _ in range(l)]\n                                  for l in range(1, 50))\n        }\n\n        # we use batch size 7 and bucket span 10\n        scheme = BatchingScheme(bucket_boundaries=[9, 19, 29, 39, 49],\n                                bucket_batch_sizes=[7, 7, 7, 7, 7, 7],\n                                drop_remainder=True)\n        dataset = Dataset(""dataset"", iterators=iterators, batching=scheme,\n                          shuffled=False)\n\n        # we process the dataset in two epochs and save what did the batches\n        # look like\n        batches = []\n        for batch in dataset.batches():\n            batches.append(list(batch.get_series(""sentences"")))\n\n        ref_batches = [\n            [[""word"" for _ in range(l)] for l in range(1, 8)],\n            [[""word"" for _ in range(l)] for l in range(10, 17)],\n            [[""word"" for _ in range(l)] for l in range(20, 27)],\n            [[""word"" for _ in range(l)] for l in range(30, 37)],\n            [[""word"" for _ in range(l)] for l in range(40, 47)]]\n\n        self.assertSequenceEqual(ref_batches, batches)\n\n    def test_buckets_similar_size(self):\n        # testing dataset is 3 x 6 sequences of lengths 0 - 5\n        iterators = {\n            ""sentences"": lambda: [[""word"" for _ in range(l)]\n                                  for l in range(6)] * 3\n        }\n\n        # we use batch size 6 and bucket span 2\n        scheme = BatchingScheme(bucket_boundaries=[1, 3, 5],\n                                bucket_batch_sizes=[6, 6, 6, 6])\n        dataset = Dataset(""dataset"", iterators=iterators, batching=scheme,\n                          shuffled=True)\n\n        # we process the dataset in two epochs and save what did the batches\n        # look like\n        batches = []\n        for batch in dataset.batches():\n            batches.append(list(batch.get_series(""sentences"")))\n\n        # this setup should divide the data to 3 batches\n        self.assertEqual(len(batches), 3)\n\n        for batch in batches:\n            # each batch should contain 6 values\n            self.assertEqual(len(batch), 6)\n\n            lengths = set(len(b) for b in batch)\n\n            # the values in the batch should have two lengths\n            self.assertEqual(len(lengths), 2)\n\n            # the lengths should differ by one\n            self.assertEqual(max(lengths) - min(lengths), 1)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
neuralmonkey/tests/test_decoder.py,2,"b'#!/usr/bin/env python3.5\n# -*- coding: utf-8 -*-\n"""""" Unit tests for the decoder. (Tests only initialization so far) """"""\n\nimport unittest\nimport tensorflow as tf\n\nfrom neuralmonkey.decoders.decoder import Decoder\nfrom neuralmonkey.vocabulary import Vocabulary\n\n\nclass TestDecoder(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        tf.reset_default_graph()\n\n    def setUp(self):\n        self.decoder_params = dict(\n            encoders=[],\n            vocabulary=Vocabulary([""a"", ""b"", ""c""]),\n            data_id=""foo"",\n            name=""test-decoder"",\n            max_output_len=5,\n            dropout_keep_prob=1.0,\n            embedding_size=10,\n            rnn_size=10)\n\n    @classmethod\n    def tearDownClass(cls):\n        tf.reset_default_graph()\n\n    def test_init(self):\n        decoder = Decoder(**self.decoder_params)\n        self.assertIsNotNone(decoder)\n\n    def test_max_output_len(self):\n        dparams = self.decoder_params\n        dparams[""max_output_len""] = -10\n        with self.assertRaises(ValueError):\n            Decoder(**dparams)\n\n    def test_dropout(self):\n        dparams = self.decoder_params\n        dparams[""dropout_keep_prob""] = -0.5\n        with self.assertRaises(ValueError):\n            Decoder(**dparams)\n\n        dparams[""dropout_keep_prob""] = 1.5\n        with self.assertRaises(ValueError):\n            Decoder(**dparams)\n\n    def test_embedding_size(self):\n        dparams = self.decoder_params\n        dparams[""embedding_size""] = None\n        with self.assertRaises(ValueError):\n            dec = Decoder(**dparams)\n            print(dec.embedding_size)\n\n        dparams[""embedding_size""] = -10\n        with self.assertRaises(ValueError):\n            Decoder(**dparams)\n\n    def test_cell_type(self):\n        dparams = self.decoder_params\n\n        dparams.update({""rnn_cell"": ""bogus_cell""})\n        with self.assertRaises(ValueError):\n            Decoder(**dparams)\n\n        for cell_type in (""GRU"", ""LSTM"", ""NematusGRU""):\n            print(dparams)\n            dparams[""rnn_cell""] = cell_type\n            dparams[""name""] = ""test-decoder-{}"".format(cell_type)\n            Decoder(**dparams)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
neuralmonkey/tests/test_encoders_init.py,0,"b'#!/usr/bin/env python3.5\n""""""Test init methods of encoders.""""""\n\nimport unittest\nimport copy\n\nfrom typing import Dict, List, Any, Iterable\n\nfrom neuralmonkey.encoders.recurrent import SentenceEncoder\nfrom neuralmonkey.encoders.sentence_cnn_encoder import SentenceCNNEncoder\nfrom neuralmonkey.model.sequence import EmbeddedSequence\nfrom neuralmonkey.vocabulary import Vocabulary\n\n\nVOCABULARY = Vocabulary([""ich"", ""bin"", ""der"", ""walrus""])\nINPUT_SEQUENCE = EmbeddedSequence(""seq"", VOCABULARY, ""marmelade"", 300)\n\nSENTENCE_ENCODER_GOOD = {\n    ""name"": [""encoder""],\n    ""vocabulary"": [VOCABULARY],\n    ""data_id"": [""marmelade""],\n    ""embedding_size"": [20],\n    ""rnn_size"": [30],\n    ""max_input_len"": [None, 15],\n    ""dropout_keep_prob"": [0.5, 1.],\n}\n\nSENTENCE_ENCODER_BAD = {\n    ""nonexistent"": [""ahoj""],\n    ""name"": [None, 1],\n    ""vocabulary"": [0, None, ""ahoj"", dict()],\n    ""data_id"": [0, None, VOCABULARY],\n    ""embedding_size"": [-1, 0, ""ahoj"", 3.14, VOCABULARY, SentenceEncoder, None],\n    ""rnn_size"": [-1, 0, ""ahoj"", 3.14, VOCABULARY, SentenceEncoder, None],\n    ""max_input_len"": [-1, 0, ""ahoj"", 3.14, VOCABULARY, SentenceEncoder],\n    ""dropout_keep_prob"": [0.0, 0, -1.0, 2.0, ""ahoj"", VOCABULARY, None],\n}\n\nTRANSFORMER_ENCODER_GOOD = {\n    ""name"": [""transformer_encoder""],\n    ""input_sequence"": [INPUT_SEQUENCE],\n    ""ff_hidden_size"": [10],\n    ""depth"": [6],\n    ""n_heads"": [3],\n    ""dropout_keep_prob"": [0.5],\n}\n\nTRANSFORMER_ENCODER_BAD = {\n    ""nonexistent"": [""ahoj""],\n    ""name"": [None, 1],\n    ""input_sequence"": [0, None, VOCABULARY],\n    ""ff_hidden_size"": [-1, 0, ""ahoj"", 3.14, VOCABULARY, SentenceEncoder, None],\n    ""depth"": [-1, ""ahoj"", 3.14, SentenceEncoder, None],\n    ""n_heads"": [-1, ""ahoj"", 3.14, SentenceEncoder, None],\n    ""dropout_keep_prob"": [0.0, 0, -1.0, 2.0, ""ahoj"", VOCABULARY, None]\n}\n\nSENTENCE_CNN_ENCODER_GOOD = {\n    ""name"": [""cnn_encoder""],\n    ""input_sequence"": [INPUT_SEQUENCE],\n    ""segment_size"": [10],\n    ""highway_depth"": [11],\n    ""rnn_size"": [30],\n    ""filters"": [[(2, 10)], [(3, 20), (4, 10)]],\n    ""dropout_keep_prob"": [0.5, 1.],\n    ""use_noisy_activations"": [False]\n}\n\nSENTENCE_CNN_ENCODER_BAD = {\n    ""nonexistent"": [""ahoj""],\n    ""name"": [None, 1],\n    ""input_sequence"": [0, None, VOCABULARY],\n    ""segment_size"": [-1, 0, ""ahoj"", 3.14, VOCABULARY, None],\n    ""highway_depth"": [-1, ""ahoj"", 3.14, SentenceEncoder, None],\n    ""rnn_size"": [-1, 0, ""ahoj"", 3.14, VOCABULARY, SentenceEncoder, None],\n    ""filters"": [""ahoj"", [], [(0, 0)], [(1, 2, 3)], [VOCABULARY, None],\n                [(None, None)]],\n    ""dropout_keep_prob"": [0.0, 0, -1.0, 2.0, ""ahoj"", VOCABULARY, None],\n    ""use_noisy_activations"": [None, SentenceEncoder]\n}\n\n\ndef traverse_combinations(\n        params: Dict[str, List[Any]],\n        partial_params: Dict[str, Any]) -> Iterable[Dict[str, Any]]:\n    params = copy.copy(params)\n\n    if params:\n        pivot_key, values = params.popitem()\n\n        for val in values:\n            partial_params[pivot_key] = val\n            yield from traverse_combinations(params, partial_params)\n    else:\n        yield partial_params\n\n\nclass TestEncodersInit(unittest.TestCase):\n\n    def _run_constructors(self, encoder_type, good_params, bad_params):\n        good_index = 0\n        good_options = {par: value[good_index]\n                        for par, value in good_params.items()}\n\n        name_suffix = 0\n\n        for key, bad_values in bad_params.items():\n            for value in bad_values:\n                options = copy.copy(good_options)\n                options[key] = value\n                if key != ""name"":\n                    options[""name""] = ""{}_{}"".format(options[""name""],\n                                                     name_suffix)\n                name_suffix += 1\n\n                try:\n                    with self.assertRaises(Exception):\n                        encoder_type(**options)\n                except Exception:\n                    print(""FAILED \'{}\', configuration: {}"".format(\n                        encoder_type, str(options)))\n                    raise\n\n        for good_param_combo in traverse_combinations(good_params, {}):\n            try:\n                options = copy.copy(good_param_combo)\n                options[""name""] = ""{}_{}"".format(options[""name""], name_suffix)\n                name_suffix += 1\n\n                encoder_type(**options)\n            except Exception:\n                print(""Good param combo FAILED: {}, configuration: {}"".format(\n                    encoder_type, str(options)))\n                raise\n\n    def test_sentence_encoder(self):\n        with self.assertRaises(Exception):\n            # pylint: disable=no-value-for-parameter\n            # on purpose, should fail\n            SentenceEncoder()\n            # pylint: enable=no-value-for-parameter\n\n        self._run_constructors(SentenceEncoder,\n                               SENTENCE_ENCODER_GOOD,\n                               SENTENCE_ENCODER_BAD)\n\n    def test_sentence_cnn_encoder(self):\n        with self.assertRaises(Exception):\n            # pylint: disable=no-value-for-parameter\n            # on purpose, should fail\n            SentenceCNNEncoder()\n            # pylint: enable=no-value-for-parameter\n\n        self._run_constructors(SentenceCNNEncoder,\n                               SENTENCE_CNN_ENCODER_GOOD,\n                               SENTENCE_CNN_ENCODER_BAD)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
neuralmonkey/tests/test_eval_wrappers.py,0,"b'# test evaluation metric wrappers\n\nimport unittest\nimport os.path\n\nfrom neuralmonkey.evaluators.multeval import MultEvalWrapper\nfrom neuralmonkey.evaluators.beer import BeerWrapper\nfrom neuralmonkey.evaluators.gleu import GLEUEvaluator\nfrom neuralmonkey.evaluators.f1_bio import F1Evaluator\nfrom neuralmonkey.evaluators.accuracy import (AccuracyEvaluator,\n                                              AccuracySeqLevelEvaluator)\n\nREF = [""I"", ""like"", ""tulips"", "".""]\nHYP = [""I"", ""hate"", ""flowers"", ""and"", ""stones"", "".""]\n\n# 4 common, |bio| = 7, |bio_ref| = 6\nBIO = ""BBOBOOOBIIOOOOOBIBIIIIB""\nBIO_REF = ""BIOBOOOBIIBIOOOBIBIIIIO""\n\nMULTEVAL = ""scripts/multeval-0.5.1/multeval.sh""\nBEER = ""scripts/beer_2.0/beer""\n\n\nclass TestExternalEvaluators(unittest.TestCase):\n\n    def test_multeval_bleu(self):\n        if os.path.exists(MULTEVAL):\n            multeval = MultEvalWrapper(MULTEVAL, metric=""bleu"")\n            bleu = multeval([HYP], [REF])\n            max_bleu = multeval([REF], [REF], )\n            min_bleu = multeval([[]], [REF])\n            self.assertEqual(max_bleu, 1.0)\n            self.assertEqual(min_bleu, 0.042)  # smoothing\n            self.assertAlmostEqual(bleu, 0.097)\n        else:\n            print(""MultEval not installed, cannot be found here: {}"".\n                  format(MULTEVAL))\n\n    def test_multeval_ter(self):\n        if os.path.exists(MULTEVAL):\n            multeval = MultEvalWrapper(MULTEVAL, metric=""ter"")\n            ter = multeval([HYP], [REF])\n            min_ter = multeval([REF], [REF])\n            max_ter = multeval([[]], [REF])\n            self.assertEqual(min_ter, 0.0)\n            self.assertEqual(max_ter, 1.0)\n            self.assertAlmostEqual(ter, 1.0)\n        else:\n            print(""MultEval not installed, cannot be found here: {}"".\n                  format(MULTEVAL))\n\n    def test_multeval_meteor(self):\n        if os.path.exists(MULTEVAL):\n            multeval = MultEvalWrapper(MULTEVAL, metric=""meteor"")\n            meteor = multeval([HYP], [REF])\n            max_meteor = multeval([REF], [REF])\n            min_meteor = multeval([[]], [REF])\n            self.assertEqual(max_meteor, 1.0)\n            self.assertAlmostEqual(min_meteor, 0.0)\n            self.assertAlmostEqual(meteor, 0.093)\n        else:\n            print(""MultEval not installed, cannot be found here: {}"".\n                  format(MULTEVAL))\n\n    def test_beer(self):\n        if os.path.exists(BEER):\n            beer_evaluator = BeerWrapper(BEER)\n            beer = beer_evaluator([HYP], [REF])\n            max_beer = beer_evaluator([REF], [REF])\n            min_beer = beer_evaluator([[]], [REF])\n            self.assertAlmostEqual(beer, 0.1120231)\n            self.assertAlmostEqual(max_beer, 0.4744488)\n            self.assertEqual(min_beer, 0)\n        else:\n            print(""BEER not installed, cannot be found here: {}"".format(BEER))\n\n    def test_gleu(self):\n        gleu_evaluator = GLEUEvaluator()\n        gleu = gleu_evaluator([HYP], [REF])\n        max_gleu = gleu_evaluator([REF], [REF])\n        min_gleu = gleu_evaluator([[]], [REF])\n        self.assertEqual(min_gleu, 0.0)\n        self.assertEqual(max_gleu, 1.0)\n        self.assertAlmostEqual(gleu, 0.1111111)\n\n    def test_f1(self):\n        f1_evaluator = F1Evaluator()\n        f1val = f1_evaluator([BIO], [BIO_REF])\n        self.assertAlmostEqual(f1val, 8.0 / 13.0)\n\n\nclass TestAccuracyEvaluator(unittest.TestCase):\n\n    def setUp(self):\n        self.hyps = [[""This"", ""is"", ""equal""], [""This"", ""is"", ""not""]]\n        self.refs = [[""This"", ""is"", ""equal""], [""This"", ""isn\'t""]]\n\n    def test_word_level_acc(self):\n        word_acc_evaluator = AccuracyEvaluator()\n        word_acc = word_acc_evaluator(self.hyps, self.refs)\n        max_word_acc = word_acc_evaluator(self.refs, self.refs)\n        min_word_acc = word_acc_evaluator([[], []], self.hyps)\n        self.assertEqual(min_word_acc, 0.0)\n        self.assertEqual(word_acc, 0.8)\n        self.assertEqual(max_word_acc, 1.0)\n\n    def test_seq_level_acc(self):\n        seq_acc_evaluator = AccuracySeqLevelEvaluator()\n        seq_acc = seq_acc_evaluator(self.hyps, self.refs)\n        max_seq_acc = seq_acc_evaluator(self.refs, self.refs)\n        min_seq_acc = seq_acc_evaluator([[], []], self.hyps)\n        self.assertEqual(min_seq_acc, 0.0)\n        self.assertEqual(seq_acc, 0.5)\n        self.assertEqual(max_seq_acc, 1.0)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
neuralmonkey/tests/test_functions.py,3,"b'#!/usr/bin/env python3.5\n""""""Unit tests for functions.py.""""""\n\nimport unittest\nimport tensorflow as tf\n\nfrom neuralmonkey.functions import piecewise_function\n\n\nclass TestPiecewiseFunction(unittest.TestCase):\n\n    def test_piecewise_constant(self):\n        x = tf.placeholder(dtype=tf.int32)\n        y = piecewise_function(x, [-0.5, 1.2, 3, 2], [-1, 2, 1000],\n                               dtype=tf.float32)\n\n        with tf.Session() as sess:\n            self.assertAlmostEqual(sess.run(y, {x: -2}), -0.5)\n            self.assertAlmostEqual(sess.run(y, {x: -1}), 1.2)\n            self.assertAlmostEqual(sess.run(y, {x: 999}), 3)\n            self.assertAlmostEqual(sess.run(y, {x: 1000}), 2)\n            self.assertAlmostEqual(sess.run(y, {x: 1001}), 2)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
neuralmonkey/tests/test_model_part.py,12,"b'#!/usr/bin/env python3.5\n""""""Test ModelPart class.""""""\n\nimport os\nimport tempfile\nimport unittest\n\nimport numpy as np\nfrom numpy.testing import assert_array_equal\nimport tensorflow as tf\n\nfrom neuralmonkey.vocabulary import Vocabulary\nfrom neuralmonkey.encoders.recurrent import SentenceEncoder\nfrom neuralmonkey.model.sequence import EmbeddedSequence\n\n\nclass Test(unittest.TestCase):\n    """"""Test capabilities of model part.""""""\n\n    @classmethod\n    def setUpClass(cls):\n        tf.reset_default_graph()\n        cls.dataset = {\n            ""id"": tf.constant([[""hello"", ""world""], [""test"", ""this""]]),\n            ""data_id"": tf.constant([[""A"", ""B"", ""C""], [""D"", ""E"", ""F""]])}\n\n    def test_reuse(self):\n        vocabulary = Vocabulary([""a"", ""b""])\n\n        seq1 = EmbeddedSequence(\n            name=""seq1"",\n            vocabulary=vocabulary,\n            data_id=""id"",\n            embedding_size=10)\n        seq1.register_input(self.dataset)\n\n        seq2 = EmbeddedSequence(\n            name=""seq2"",\n            vocabulary=vocabulary,\n            embedding_size=10,\n            data_id=""id"")\n        seq2.register_input(self.dataset)\n\n        seq3 = EmbeddedSequence(\n            name=""seq3"",\n            vocabulary=vocabulary,\n            data_id=""id"",\n            embedding_size=10,\n            reuse=seq1)\n        seq3.register_input(self.dataset)\n\n        # blessing\n        self.assertIsNotNone(seq1.embedding_matrix)\n        self.assertIsNotNone(seq2.embedding_matrix)\n        self.assertIsNotNone(seq3.embedding_matrix)\n\n        sess = tf.Session()\n        sess.run(tf.global_variables_initializer())\n\n        params = sess.run((seq1.embedding_matrix, seq2.embedding_matrix,\n                           seq3.embedding_matrix))\n\n        with self.assertRaises(AssertionError):\n            assert_array_equal(params[0], params[1])\n\n        assert_array_equal(params[0], params[2])\n\n    def test_save_and_load(self):\n        """"""Try to save and load encoder.""""""\n        vocabulary = Vocabulary([""a"", ""b""])\n\n        checkpoint_file = tempfile.NamedTemporaryFile(delete=False)\n        checkpoint_file.close()\n\n        encoder = SentenceEncoder(\n            name=""enc"", vocabulary=vocabulary, data_id=""data_id"",\n            embedding_size=10, rnn_size=20, max_input_len=30,\n            save_checkpoint=checkpoint_file.name,\n            load_checkpoint=checkpoint_file.name)\n\n        encoder.input_sequence.register_input(self.dataset)\n\n        # NOTE: This assert needs to be here otherwise the model has\n        # no parameters since the sentence encoder is initialized lazily\n        self.assertIsInstance(encoder.temporal_states, tf.Tensor)\n\n        encoders_variables = tf.get_collection(\n            tf.GraphKeys.GLOBAL_VARIABLES, scope=""enc"")\n\n        sess_1 = tf.Session()\n        sess_1.run(tf.global_variables_initializer())\n        encoder.save(sess_1)\n\n        sess_2 = tf.Session()\n        sess_2.run(tf.global_variables_initializer())\n        encoder.load(sess_2)\n\n        values_in_sess_1 = sess_1.run(encoders_variables)\n        values_in_sess_2 = sess_2.run(encoders_variables)\n\n        self.assertTrue(\n            all(np.all(v1 == v2) for v1, v2 in\n                zip(values_in_sess_1, values_in_sess_2)))\n\n        os.remove(checkpoint_file.name)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
neuralmonkey/tests/test_nn_utils.py,11,"b'#!/usr/bin/env python3.5\n\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\nfrom neuralmonkey.nn.utils import dropout\n\n\nclass TestDropout(unittest.TestCase):\n\n    def test_invalid_keep_prob(self):\n        """"""Tests invalid dropout values""""""\n\n        var = tf.constant(np.arange(5))\n        train_mode = tf.constant(True)\n\n        for kprob in [-1, 2, 0]:\n            with self.assertRaises(ValueError):\n                dropout(var, kprob, train_mode)\n\n    def test_keep_prob(self):\n        """"""Counts dropped items and compare with the expectation""""""\n\n        var = tf.ones([10000])\n        s = tf.Session()\n\n        for kprob in [0.1, 0.7]:\n            dropped_var = dropout(var, kprob, tf.constant(True))\n            dropped_size = tf.reduce_sum(\n                tf.to_int32(tf.equal(dropped_var, 0.0)))\n\n            dsize = s.run(dropped_size)\n\n            expected_dropped_size = 10000 * (1 - kprob)\n\n            self.assertTrue(np.isclose(expected_dropped_size, dsize, atol=500))\n\n    def test_train_false(self):\n        """"""Checks that dropout is not used when not training""""""\n\n        var = tf.ones([10000])\n        s = tf.Session()\n\n        dropped_var = dropout(var, 0.1, tf.constant(False))\n        dropped_size = tf.reduce_sum(dropped_var)\n        dsize = s.run(dropped_size)\n\n        self.assertTrue(dsize == 10000)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
neuralmonkey/tests/test_readers.py,0,"b'#!/usr/bin/env python3.5\n""""""Unit tests for readers""""""\n\nimport unittest\nimport tempfile\nimport numpy as np\n\nfrom neuralmonkey.readers.string_vector_reader import get_string_vector_reader\nfrom neuralmonkey.readers.plain_text_reader import T2TReader\n\nSTRING_INTS = """"""\n1   2 3\n4 5   6\n7 8 9 10\n\n""""""\n\nLIST_INTS = [np.array(row.strip().split(), dtype=np.int32)\n             for row in STRING_INTS.strip().split(""\\n"")]\n\nSTRING_FLOATS = """"""\n1 2       3.5\n      4 -5.0e10     6\n7 8 9.2e-12 10.1123213213214123141234123112312312\n""""""\n\nLIST_FLOATS = [np.array(row.strip().split(), dtype=np.float32)\n               for row in STRING_FLOATS.strip().split(""\\n"")]\n\nSTRING_INTS_FINE = """"""\n1 2 3\n4 5 6\n7 8 9\n""""""\n\nLIST_INTS_FINE = [np.array(row.strip().split(), dtype=np.int32)\n                  for row in STRING_INTS_FINE.strip().split(""\\n"")]\n\n\ndef _make_file(from_var):\n    tmpfile = tempfile.NamedTemporaryFile(mode=""w+"")\n    tmpfile.write(from_var)\n    tmpfile.seek(0)\n    return tmpfile\n\n\nclass TestStringVectorReader(unittest.TestCase):\n\n    def setUp(self):\n        self.tmpfile_floats = _make_file(STRING_FLOATS)\n        self.tmpfile_ints = _make_file(STRING_INTS)\n        self.tmpfile_ints_fine = _make_file(STRING_INTS_FINE)\n\n    def test_reader(self):\n        r = get_string_vector_reader(np.float32)\n        floats = list(r([self.tmpfile_floats.name]))\n        equals = [np.array_equal(f, g) for f, g in zip(floats, LIST_FLOATS)]\n\n        for comp in equals:\n            self.assertTrue(comp)\n\n        r = get_string_vector_reader(np.int32)\n        ints = list(r([self.tmpfile_ints.name, self.tmpfile_ints_fine.name]))\n        equals = [np.array_equal(f, g)\n                  for f, g in zip(ints, LIST_INTS + LIST_INTS_FINE)]\n\n        for comp in equals:\n            self.assertTrue(comp)\n\n    def test_columns(self):\n        for cols in range(2, 4):\n            with self.assertRaisesRegex(ValueError, ""Wrong number of columns""):\n                r = get_string_vector_reader(np.int32, columns=cols)\n                list(r([self.tmpfile_ints.name]))\n\n            with self.assertRaisesRegex(ValueError, ""Wrong number of columns""):\n                r = get_string_vector_reader(np.float32, columns=cols)\n                list(r([self.tmpfile_floats.name]))\n\n            if cols != 3:\n                with self.assertRaisesRegex(ValueError,\n                                            ""Wrong number of columns""):\n                    r = get_string_vector_reader(np.int32, columns=cols)\n                    list(r([self.tmpfile_ints_fine.name]))\n\n        r = get_string_vector_reader(np.int32, columns=3)\n        ints = list(r([self.tmpfile_ints_fine.name]))\n        equals = [np.array_equal(f, g)\n                  for f, g in zip(ints, LIST_INTS_FINE)]\n\n        for comp in equals:\n            self.assertTrue(comp)\n\n    def tearDown(self):\n        self.tmpfile_ints.close()\n        self.tmpfile_floats.close()\n        self.tmpfile_ints_fine.close()\n\n\nclass TestT2TReader(unittest.TestCase):\n\n    def setUp(self):\n        self.reader = T2TReader\n\n    def test_reader(self):\n        text = ""Ich bin  der \xc4\x8derm\xc3\xa1k -=- - !!! alfonso ""\n        gold_tokens = [""Ich"", ""bin"", ""  "", ""der"", ""\xc4\x8derm\xc3\xa1k"", "" -=- - !!! "",\n                       ""alfonso""]\n\n        tmpfile = _make_file(text)\n\n        read = []\n        for line in self.reader([tmpfile.name]):\n            read.append(line)\n\n        tmpfile.close()\n\n        self.assertEqual(len(read), 1)\n        self.assertSequenceEqual(read[0], gold_tokens)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
neuralmonkey/tests/test_ter.py,0,"b'#!/usr/bin/env python3.5\n\nimport unittest\n\nfrom neuralmonkey.evaluators.ter import TER\nfrom neuralmonkey.tests.test_bleu import DECODED, REFERENCE\n\n\nclass TestBLEU(unittest.TestCase):\n\n    def test_empty_decoded(self):\n        self.assertEqual(TER([[] for _ in DECODED], REFERENCE), 1.0)\n\n    def test_empty_reference(self):\n        score = TER(DECODED, [[] for _ in REFERENCE])\n        self.assertIsInstance(score, float)\n\n    def test_identical(self):\n        self.assertEqual(TER(REFERENCE, REFERENCE), 0.0)\n\n    def test_empty_sentence(self):\n        ref_empty = REFERENCE + [[]]\n        out_empty = DECODED + [[""something""]]\n        score = TER(out_empty, ref_empty)\n        self.assertAlmostEqual(score, .84, delta=10)\n\n    def test_ter(self):\n        score = TER(DECODED, REFERENCE)\n        self.assertAlmostEqual(score, .84, delta=10)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
neuralmonkey/tests/test_vocabulary.py,7,"b'#!/usr/bin/env python3.5\n\nimport unittest\nimport tensorflow as tf\nfrom neuralmonkey.vocabulary import Vocabulary, pad_batch\n\n\nclass TestVocabulary(tf.test.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        tf.reset_default_graph()\n\n        cls.corpus = [\n            ""the colorless ideas slept furiously"",\n            ""pooh slept all night"",\n            ""working class hero is something to be"",\n            ""I am the working class walrus"",\n            ""walrus for president""\n        ]\n\n        cls.graph = tf.Graph()\n\n        with cls.graph.as_default():\n            cls.tokenized_corpus = [s.split("" "") for s in cls.corpus]\n            words = [w for sent in cls.tokenized_corpus for w in sent]\n            cls.vocabulary = Vocabulary(list(set(words)))\n\n    @classmethod\n    def tearDownClass(cls):\n        tf.reset_default_graph()\n\n    def test_all_words_in(self):\n        for sentence in self.tokenized_corpus:\n            for word in sentence:\n                self.assertTrue(word in self.vocabulary)\n\n    def test_unknown_word(self):\n        self.assertFalse(""jindrisek"" in self.vocabulary)\n\n    def test_padding(self):\n        padded = pad_batch(self.tokenized_corpus)\n        self.assertTrue(all(len(p) == 7 for p in padded))\n\n    def test_weights(self):\n        pass\n\n    def test_there_and_back_self(self):\n\n        with self.graph.as_default():\n            with self.test_session() as sess:\n                sess.run(tf.tables_initializer())\n\n                padded = tf.constant(\n                    pad_batch(self.tokenized_corpus, max_length=20,\n                              add_start_symbol=False, add_end_symbol=True))\n\n                vectors = tf.transpose(\n                    self.vocabulary.strings_to_indices(padded))\n                f_vectors = sess.run(vectors)\n\n        sentences_again = self.vocabulary.vectors_to_sentences(f_vectors)\n\n        for orig_sentence, reconstructed_sentence in \\\n                zip(self.tokenized_corpus, sentences_again):\n            self.assertSequenceEqual(orig_sentence, reconstructed_sentence)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
neuralmonkey/tests/test_wordpiece.py,0,"b'#!/usr/bin/env python3.5\nimport unittest\n\nfrom neuralmonkey.vocabulary import Vocabulary\nfrom neuralmonkey.processors.wordpiece import (\n    WordpiecePreprocessor, WordpiecePostprocessor)\n\n\nclass TestWordpieces(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        corpus = [\n            ""the colorless ideas slept furiously"",\n            ""pooh slept all night"",\n            ""working class hero is something to be"",\n            ""I am the working class walrus"",\n            ""walrus for president""\n        ]\n\n        tokenized_corpus = [[a + ""_"" for a in s.split()] for s in corpus]\n        vocab_from_corpus = {w for sent in tokenized_corpus for w in sent}\n\n        # Create list of characters required to process the CORPUS with\n        # wordpieces\n        corpus_chars = {x for c in set("""".join(corpus)) for x in [c, c + ""_""]}\n        escape_chars = ""\\\\_u0987654321;""\n        c_caron = ""\\\\269;""\n        a_acute = ""225""\n\n        words = corpus_chars | set(escape_chars) | vocab_from_corpus\n        vocabulary = Vocabulary(list(words) + [c_caron, a_acute])\n\n        cls.preprocessor = WordpiecePreprocessor(vocabulary)\n        cls.postprocessor = WordpiecePostprocessor\n\n    def test_preprocess_ok(self):\n        raw = ""I am the walrus"".split()\n        gold = ""I_ am_ the_ walrus_"".split()\n\n        preprocessed = TestWordpieces.preprocessor(raw)\n        self.assertSequenceEqual(preprocessed, gold)\n\n    def test_preprocess_split(self):\n        raw = ""Ich bin der walrus"".split()\n        gold = ""I c h_ b i n_ d e r_ walrus_"".split()\n\n        preprocessed = TestWordpieces.preprocessor(raw)\n        self.assertSequenceEqual(preprocessed, gold)\n\n    def test_preprocess_unk(self):\n        raw = ""Ich bin der \xc4\x8derm\xc3\xa1k"".split()\n        gold = ""I c h_ b i n_ d e r_ \\\\269; e r m \\\\ 225 ; k_"".split()\n\n        preprocessed = TestWordpieces.preprocessor(raw)\n        self.assertSequenceEqual(preprocessed, gold)\n\n    def test_postprocess_ok(self):\n        output = ""I_ am_ the_ walrus_"".split()\n        gold = [""I am the walrus"".split()]\n\n        postprocessed = TestWordpieces.postprocessor([output])\n        self.assertSequenceEqual(postprocessed, gold)\n\n    def test_postprocess_split(self):\n        output = ""I c h_ b i n_ d e r_ walrus_"".split()\n        gold = [""Ich bin der walrus"".split()]\n\n        postprocessed = TestWordpieces.postprocessor([output])\n        self.assertSequenceEqual(postprocessed, gold)\n\n    def test_postprocess_unk(self):\n        output = ""I c h_ b i n_ d e r_ \\\\269; e r m \\\\ 225 ; k_"".split()\n        gold = [""Ich bin der \xc4\x8derm\xc3\xa1k"".split()]\n\n        postprocessed = TestWordpieces.postprocessor([output])\n        self.assertSequenceEqual(postprocessed, gold)\n\n    # TODO (#669): implement wordpiece generator\n    @unittest.skip(""not implemented yet"")\n    def test_make_wordpieces(self):\n        pass\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
neuralmonkey/trainers/__init__.py,0,b'from .cross_entropy_trainer import CrossEntropyTrainer\nfrom .delayed_update_trainer import DelayedUpdateTrainer\nfrom .multitask_trainer import MultitaskTrainer\n'
neuralmonkey/trainers/cross_entropy_trainer.py,1,"b'from typing import Any, List\n\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.logging import warn\nfrom neuralmonkey.trainers.generic_trainer import GenericTrainer\nfrom neuralmonkey.trainers.objective import (\n    Objective, CostObjective, ObjectiveWeight)\n\n\n# for compatibility reasons\ndef xent_objective(decoder, weight=None) -> Objective:\n    """"""Get XENT objective from decoder with cost.""""""\n    warn(""Using deprecated xent_objective function. Use the CostObjective ""\n         ""class directly."")\n    return CostObjective(decoder, weight)\n\n\n# pylint: disable=too-many-arguments\nclass CrossEntropyTrainer(GenericTrainer):\n\n    def __init__(self,\n                 decoders: List[Any],\n                 decoder_weights: List[ObjectiveWeight] = None,\n                 l1_weight: float = 0.,\n                 l2_weight: float = 0.,\n                 clip_norm: float = None,\n                 optimizer: tf.train.Optimizer = None,\n                 var_scopes: List[str] = None,\n                 var_collection: str = None) -> None:\n        check_argument_types()\n\n        if decoder_weights is None:\n            decoder_weights = [None for _ in decoders]\n\n        if len(decoder_weights) != len(decoders):\n            raise ValueError(\n                ""decoder_weights (length {}) do not match decoders (length {})""\n                .format(len(decoder_weights), len(decoders)))\n\n        objectives = [CostObjective(dec, w)\n                      for dec, w in zip(decoders, decoder_weights)]\n\n        GenericTrainer.__init__(\n            self,\n            objectives=objectives,\n            l1_weight=l1_weight,\n            l2_weight=l2_weight,\n            clip_norm=clip_norm,\n            optimizer=optimizer,\n            var_scopes=var_scopes,\n            var_collection=var_collection)\n'"
neuralmonkey/trainers/delayed_update_trainer.py,39,"b'# pylint: disable=unused-import\nfrom typing import Dict, List, Tuple, Optional\n# pylint: enable=unused-import\n\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.runners.base_runner import GraphExecutor, NextExecute\nfrom neuralmonkey.trainers.generic_trainer import (GenericTrainer, Objective,\n                                                   Gradients)\n\n\nclass DelayedUpdateTrainer(GenericTrainer):\n\n    class Executable(GraphExecutor.Executable[""DelayedUpdateTrainer""]):\n\n        def __init__(self, executor: ""DelayedUpdateTrainer"",\n                     compute_losses: bool, summaries: bool,\n                     num_sessions: int) -> None:\n            assert compute_losses\n            if num_sessions != 1:\n                raise ValueError(\n                    ""Trainer only supports execution in a single session"")\n\n            super().__init__(executor, compute_losses, summaries, num_sessions)\n\n            self.state = 0\n            self.res_sums = []  # type: List[tf.Summary]\n            self.res_losses = None  # type: Optional[List[float]]\n            self.res_batch = None  # type: Optional[int]\n\n        def next_to_execute(self) -> NextExecute:\n\n            if self.state == 0:  # ACCUMULATING\n                fetches = {""accumulators"": self.executor.accumulate_ops,\n                           ""counter"": self.executor.cumulator_counter,\n                           ""batch_size"": self.executor.batch_size,\n                           ""losses"": self.executor.objective_values}\n\n            elif self.state == 1:  # UPDATING\n                fetches = {\n                    ""train_op"": self.executor.train_op,\n                    ""_update_ops"": tf.get_collection(tf.GraphKeys.UPDATE_OPS)}\n\n                if self.summaries:\n                    fetches.update(self.executor.summaries)\n\n            else:  # RESETTING\n                fetches = {""resets"": self.executor.reset_ops}\n\n            return fetches, []\n\n        def collect_results(self, results: List[Dict]) -> None:\n            assert len(results) == 1\n            result = results[0]\n\n            if self.state == 0:  # ACCUMULATING\n                self.res_losses = result[""losses""]\n                self.res_batch = result[""batch_size""]\n\n                # Are we updating?\n                counter = result[""counter""]\n\n                if counter == self.executor.batches_per_update:\n                    self.state = 1\n                    return\n            elif self.state == 1:\n                if self.summaries:\n                    self.res_sums = [result[""scalar_summaries""],\n                                     result[""histogram_summaries""]]\n                self.state = 2\n                return\n\n            assert self.res_losses is not None\n            assert self.res_batch is not None\n\n            objective_names = [obj.name for obj in self.executor.objectives]\n            objective_names += [""L1"", ""L2""]\n            losses = dict(zip(objective_names, self.res_losses))\n\n            self.set_result({}, losses, self.res_batch, self.res_sums)\n\n    # pylint: disable=too-many-arguments\n    def __init__(self,\n                 batches_per_update: int,\n                 objectives: List[Objective],\n                 l1_weight: float = 0.0,\n                 l2_weight: float = 0.0,\n                 clip_norm: float = None,\n                 optimizer: tf.train.Optimizer = None,\n                 var_scopes: List[str] = None,\n                 var_collection: str = None) -> None:\n        check_argument_types()\n        GenericTrainer.__init__(self, objectives, l1_weight, l2_weight,\n                                clip_norm, optimizer, var_scopes,\n                                var_collection)\n\n        self.batches_per_update = batches_per_update\n    # pylint: enable=too-many-arguments\n\n    @tensor\n    def existing_grads_and_vars(self) -> Tuple[\n            List[tf.Tensor], List[tf.Variable]]:\n        orig_grads = super().raw_gradients\n\n        # pylint: disable=not-an-iterable\n        # Pylint does not understand @tensor annotations\n        transposed = tuple(zip(\n            *[(grad, var) for grad, var in orig_grads if grad is not None]))\n        # pylint: enable=not-an-iterable\n\n        return list(transposed[0]), list(transposed[1])\n\n    @tensor\n    def gradient_buffers(self) -> List[tf.Variable]:\n        # pylint: disable=unpacking-non-sequence\n        existing_gradients, _ = self.existing_grads_and_vars\n        # pylint: enable=unpacking-non-sequence\n\n        with tf.variable_scope(""gradient_buffer""):\n            return [tf.Variable(initial_value=tf.zeros_like(grad),\n                                trainable=False)\n                    for grad in existing_gradients]\n\n    @tensor\n    def objective_buffers(self) -> List[tf.Variable]:\n        with tf.variable_scope(""loss_buffers""):\n            return [tf.Variable(0.0, trainable=False) for _ in self.objectives]\n\n    # pylint: disable=no-self-use\n    @tensor\n    def diff_buffer(self) -> tf.Variable:\n        return tf.Variable(0.0, trainable=False)\n\n    @tensor\n    def cumulator_counter(self) -> tf.Variable:\n        return tf.Variable(0, trainable=False, name=""cumulator_counter"")\n    # pylint: enable=no-self-use\n\n    @tensor\n    def accumulate_ops(self) -> List[tf.Operation]:\n        # pylint: disable=unpacking-non-sequence\n        existing_gradients, _ = self.existing_grads_and_vars\n        # pylint: enable=unpacking-non-sequence\n\n        # pylint: disable=not-an-iterable\n        # Pylint does not understand @tensor annotations\n        accumulate_ops = [\n            tf.assign_add(gradbuf, grad)\n            for gradbuf, grad in zip(\n                self.gradient_buffers, existing_gradients)]\n\n        accumulate_ops.extend(\n            tf.assign_add(objbuf, obj.loss)\n            for objbuf, obj in zip(self.objective_buffers, self.objectives))\n        # pylint: enable=not-an-iterable\n\n        accumulate_ops.append(\n            tf.assign_add(self.diff_buffer, self.differentiable_loss_sum))\n        accumulate_ops.append(\n            tf.assign_add(self.cumulator_counter, 1))\n\n        return accumulate_ops\n\n    @tensor\n    def reset_ops(self) -> List[tf.Operation]:\n        # pylint: disable=not-an-iterable\n        # Pylint does not understand @tensor annotations\n        reset_ops = [tf.assign(gradbuf, tf.zeros_like(gradbuf))\n                     for gradbuf in self.gradient_buffers]\n        reset_ops.extend(\n            tf.assign(objbuf, 0.0) for objbuf in self.objective_buffers)\n        # pylint: enable=not-an-iterable\n\n        reset_ops.append(tf.assign(self.diff_buffer, 0.0))\n        reset_ops.append(tf.assign(self.cumulator_counter, 0))\n        return reset_ops\n\n    @tensor\n    def raw_gradients(self) -> Gradients:\n        """"""Return averaged gradients over buffers.""""""\n        # pylint: disable=not-an-iterable\n        # Pylint does not understand @tensor annotations\n        averaged_grads = [grad / tf.to_float(self.cumulator_counter)\n                          for grad in self.gradient_buffers]\n        # pylint: enable=not-an-iterable\n\n        tf.summary.scalar(\n            ""train_opt_cost"",\n            self.diff_buffer / tf.to_float(self.cumulator_counter),\n            collections=[""summary_train""])\n\n        # log all objectives\n        for obj, objbuf in zip(self.objectives, self.objective_buffers):\n            tf.summary.scalar(\n                obj.name, objbuf / tf.to_float(self.cumulator_counter),\n                collections=[""summary_train""])\n\n        # now, zip averaged grads with associated vars to a Gradients struct.\n        # pylint: disable=unpacking-non-sequence\n        _, existing_vars = self.existing_grads_and_vars\n        # pylint: enable=unpacking-non-sequence\n        return list(zip(averaged_grads, existing_vars))\n\n    @tensor\n    def summaries(self) -> Dict[str, tf.Tensor]:\n        # pylint: disable=protected-access\n        if isinstance(self.optimizer._lr, tf.Tensor):\n            tf.summary.scalar(""learning_rate"", self.optimizer._lr,\n                              collections=[""summary_train""])\n        # pylint: enable=protected-access\n\n        # pylint: disable=unpacking-non-sequence\n        l1_norm, l2_norm = self.regularization_losses\n        # pylint: enable=unpacking-non-sequence\n\n        tf.summary.scalar(""train_l1"", l1_norm, collections=[""summary_train""])\n        tf.summary.scalar(""train_l2"", l2_norm, collections=[""summary_train""])\n\n        # pylint: disable=not-an-iterable\n        # Pylint does not understand @tensor annotations\n        for grad, var in self.gradients:\n            if grad is not None:\n                summary_name = ""gr_{}"".format(var.name)\n                tf.summary.histogram(\n                    summary_name, grad, collections=[""summary_gradients""])\n        # pylint: enable=not-an-iterable\n\n        return {\n            ""scalar_summaries"": tf.summary.merge(\n                tf.get_collection(""summary_train"")),\n            ""histogram_summaries"": tf.summary.merge(\n                tf.get_collection(""summary_gradients""))}\n'"
neuralmonkey/trainers/generic_trainer.py,34,"b'from typing import Dict, List, Optional, Tuple, Sequence\nimport re\n\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.logging import warn\nfrom neuralmonkey.model.feedable import Feedable\nfrom neuralmonkey.runners.base_runner import GraphExecutor, NextExecute\nfrom neuralmonkey.trainers.objective import (\n    Objective, Gradients, ObjectiveWeight)\n\nBIAS_REGEX = re.compile(r""[Bb]ias"")\n\n\n# pylint: disable=too-few-public-methods,too-many-locals,too-many-arguments\nclass GenericTrainer(GraphExecutor, Feedable):\n\n    class Executable(GraphExecutor.Executable[""GenericTrainer""]):\n\n        def __init__(self, executor: ""GenericTrainer"", compute_losses: bool,\n                     summaries: bool, num_sessions: int) -> None:\n            assert compute_losses\n            if num_sessions != 1:\n                raise ValueError(\n                    ""Trainer only supports execution in a single session"")\n\n            super().__init__(executor, compute_losses, summaries, num_sessions)\n\n        def next_to_execute(self) -> NextExecute:\n            fetches = self.executor.fetches\n\n            if self.summaries:\n                fetches.update(self.executor.summaries)\n\n            return fetches, []\n\n        def collect_results(self, results: List[Dict]) -> None:\n            assert len(results) == 1\n            result = results[0]\n\n            summaries = []\n            if self.summaries:\n                summaries.extend([result[""scalar_summaries""],\n                                  result[""histogram_summaries""]])\n\n            objective_names = [obj.name for obj in self.executor.objectives]\n            objective_names += [""L1"", ""L2""]\n\n            losses = dict(zip(objective_names, result[""losses""]))\n\n            self.set_result({}, losses, result[""batch_size""], summaries)\n\n    @staticmethod\n    def default_optimizer():\n        return tf.train.AdamOptimizer(learning_rate=1e-4)\n\n    def __init__(self,\n                 objectives: Sequence[Objective],\n                 l1_weight: float = 0.0,\n                 l2_weight: float = 0.0,\n                 clip_norm: float = None,\n                 optimizer: tf.train.Optimizer = None,\n                 var_scopes: List[str] = None,\n                 var_collection: str = None) -> None:\n        check_argument_types()\n        GraphExecutor.__init__(self, {obj.decoder for obj in objectives})\n        Feedable.__init__(self)\n\n        self.objectives = objectives\n        self.l1_weight = l1_weight\n        self.l2_weight = l2_weight\n        self.clip_norm = clip_norm\n        self.var_scopes = var_scopes\n        self.var_collection = var_collection\n        if self.var_collection is None:\n            self.var_collection = tf.GraphKeys.TRAINABLE_VARIABLES\n\n        self.optimizer = (\n            optimizer if optimizer is not None else self.default_optimizer())\n\n    # pylint: disable=no-self-use\n    @tensor\n    def regularization_losses(self) -> Tuple[tf.Tensor, tf.Tensor]:\n        """"""Compute the regularization losses, e.g. L1 and L2.""""""\n        regularizable = [v for v in tf.trainable_variables()\n                         if not BIAS_REGEX.findall(v.name)\n                         and not v.name.startswith(""vgg"")\n                         and not v.name.startswith(""Inception"")\n                         and not v.name.startswith(""resnet"")]\n\n        if not regularizable:\n            warn(""It seems that there are no trainable variables in the model"")\n            return tf.zeros([]), tf.zeros([])\n\n        with tf.name_scope(""regularization""):\n            l1_norm = sum(tf.reduce_sum(abs(v)) for v in regularizable)\n            l2_norm = sum(tf.reduce_sum(v ** 2) for v in regularizable)\n\n        return l1_norm, l2_norm\n    # pylint: enable=no-self-use\n\n    @tensor\n    def objective_values(self) -> List[tf.Tensor]:\n        """"""Compute unweighted losses for fetching.""""""\n        # Note here we need to call the losses first, in case the model is\n        # being built. We need to compute the regularizers after that.\n        losses = [o.loss for o in self.objectives]\n\n        # pylint: disable=unpacking-non-sequence\n        l1_norm, l2_norm = self.regularization_losses\n        # pylint: disable=unpacking-non-sequence\n\n        return losses + [l1_norm, l2_norm]\n\n    @tensor\n    def differentiable_loss_sum(self) -> tf.Tensor:\n        """"""Compute the differentiable loss (including regularization).""""""\n        obj_weights = []  # type: List[Optional[float]]\n        for obj in self.objectives:\n            if obj.gradients is not None:\n                obj_weights.append(None)\n            elif obj.weight is None:\n                obj_weights.append(1.0)\n            else:\n                obj_weights.append(obj.weight)\n\n        obj_weights += [self.l1_weight, self.l2_weight]\n        diff_loss = sum(\n            o * w for o, w in zip(self.objective_values, obj_weights)\n            if w is not None)\n\n        return diff_loss\n\n    @tensor\n    def raw_gradients(self) -> Gradients:\n        """"""Compute the gradients.""""""\n        with tf.name_scope(""gradient_collection""):\n            gradients = self.optimizer.compute_gradients(\n                self.differentiable_loss_sum, self.var_list)\n\n            def scale_grads(gradients: Gradients,\n                            weight: ObjectiveWeight) -> Gradients:\n                result = []  # type: Gradients\n                for grad, var in gradients:\n                    if weight is not None and grad is not None:\n                        result.append((weight * grad, var))\n                    else:\n                        result.append((grad, var))\n                return result\n\n            # objectives that have their gradients explictly computed\n            other_gradients = [\n                scale_grads(o.gradients, o.weight)\n                for o in self.objectives if o.gradients is not None]\n\n            def sum_grads(gradients_list: List[Gradients]) -> Gradients:\n                summed_dict = {}  # type: Dict[tf.Variable, tf.Tensor]\n                for gradients in gradients_list:\n                    for grad, var in gradients:\n                        if grad is not None:\n                            if var not in summed_dict:\n                                summed_dict[var] = grad\n                            else:\n                                summed_dict[var] += grad\n\n                return [(grad, var) for var, grad in summed_dict.items()]\n\n            if other_gradients:\n                gradients = sum_grads([gradients] + other_gradients)\n\n        return gradients\n\n    @tensor\n    def gradients(self) -> Gradients:\n        gradients = self.raw_gradients\n\n        if self.clip_norm:\n            assert self.clip_norm > 0.0\n            # pylint: disable=not-an-iterable\n            # Pylint does not understand @tensor annotations\n            gradients = [\n                (tf.clip_by_norm(grad, self.clip_norm), var)\n                for grad, var in self.raw_gradients if grad is not None]\n            # pylint: disable=not-an-iterable\n\n        return gradients\n\n    @tensor\n    def train_op(self) -> tf.Operation:\n        """"""Construct the training op.""""""\n        with tf.name_scope(""trainer""):\n            step = tf.train.get_or_create_global_step()\n            return self.optimizer.apply_gradients(self.gradients, step)\n\n    @property\n    def var_list(self) -> List[tf.Variable]:\n        if self.var_scopes is None:\n            vlists = [tf.get_collection(self.var_collection)]\n        else:\n            vlists = [tf.get_collection(self.var_collection, scope)\n                      for scope in self.var_scopes]\n\n        # Flatten the list of lists\n        return [var for var_list in vlists for var in var_list]\n\n    @tensor\n    def summaries(self) -> Dict[str, tf.Tensor]:\n\n        # pylint: disable=protected-access\n        if isinstance(self.optimizer._lr, tf.Tensor):\n            tf.summary.scalar(""learning_rate"", self.optimizer._lr,\n                              collections=[""summary_train""])\n        # pylint: enable=protected-access\n\n        # pylint: disable=unpacking-non-sequence\n        l1_norm, l2_norm = self.regularization_losses\n        # pylint: enable=unpacking-non-sequence\n        tf.summary.scalar(""train_l1"", l1_norm, collections=[""summary_train""])\n        tf.summary.scalar(""train_l2"", l2_norm, collections=[""summary_train""])\n\n        for obj in self.objectives:\n            tf.summary.scalar(obj.name, obj.loss,\n                              collections=[""summary_train""])\n\n        tf.summary.scalar(""train_opt_cost"", self.differentiable_loss_sum,\n                          collections=[""summary_train""])\n\n        # pylint: disable=not-an-iterable\n        # Pylint does not understand @tensor annotations\n        for grad, var in self.gradients:\n            if grad is not None:\n                summary_name = ""gr_{}"".format(var.name)\n                tf.summary.histogram(\n                    summary_name, grad, collections=[""summary_gradients""])\n        # pylint: enable=not-an-iterable\n\n        return {\n            ""scalar_summaries"": tf.summary.merge(\n                tf.get_collection(""summary_train"")),\n            ""histogram_summaries"": tf.summary.merge(\n                tf.get_collection(""summary_gradients""))}\n\n    @property\n    def fetches(self) -> Dict[str, tf.Tensor]:\n        return {""train_op"": self.train_op,\n                ""losses"": self.objective_values,\n                ""batch_size"": self.batch_size,\n                ""_update_ops"": tf.get_collection(tf.GraphKeys.UPDATE_OPS)}\n'"
neuralmonkey/trainers/multitask_trainer.py,2,"b'from typing import List, Dict\n\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.runners.base_runner import GraphExecutor\nfrom neuralmonkey.trainers.generic_trainer import GenericTrainer\n\n\n# pylint: disable=too-few-public-methods\nclass MultitaskTrainer(GraphExecutor):\n    """"""Wrapper for scheduling multitask training.\n\n    The wrapper contains a list of trainer objects. They are being\n    called in the order defined by this list thus simulating a task\n    switching schedule.\n    """"""\n\n    def __init__(self,\n                 trainers: List[GenericTrainer]) -> None:\n        check_argument_types()\n        GraphExecutor.__init__(self, set(trainers))\n\n        self.trainers = trainers\n        self.trainer_idx = 0\n\n    @property\n    def var_list(self) -> List[tf.Variable]:\n        return list(set.union(*[set(t.var_list) for t in self.trainers]))\n\n    def get_executable(\n            self, compute_losses: bool = True, summaries: bool = True,\n            num_sessions: int = 1) -> GraphExecutor.Executable:\n\n        focused_trainer = self.trainers[self.trainer_idx]\n        self.trainer_idx = (self.trainer_idx + 1) % len(self.trainers)\n\n        return focused_trainer.get_executable(\n            compute_losses, summaries, num_sessions)\n\n    @tensor\n    def fetches(self) -> Dict[str, tf.Tensor]:\n        fetches = {}\n        for trainer in self.trainers:\n            fetches.update(trainer.fetches)\n        return fetches\n'"
neuralmonkey/trainers/objective.py,7,"b'from abc import abstractproperty\nfrom typing import TypeVar, Union, Tuple, List, Optional, Generic\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.model.model_part import GenericModelPart\n\n# pylint: disable=invalid-name\nObjectiveWeight = Union[tf.Tensor, float, None]\nGradients = List[Tuple[tf.Tensor, tf.Variable]]\nMP = TypeVar(""MP"", bound=GenericModelPart)\n# pylint: enable=invalid-name\n\n\nclass Objective(Generic[MP]):\n    """"""The training objective base class.""""""\n\n    def __init__(self, name: str, decoder: MP) -> None:\n        """"""Construct the objective.\n\n        Arguments:\n            name: The name for the objective. This will be used e.g. in\n                TensorBoard.\n        """"""\n        self._name = name\n        self._decoder = decoder\n\n    @property\n    def decoder(self) -> MP:\n        """"""Get the decoder used by the objective.""""""\n        return self._decoder\n\n    @property\n    def name(self) -> str:\n        """"""Get the name of the objective.""""""\n        return self._name\n\n    @abstractproperty\n    def loss(self) -> tf.Tensor:\n        """"""Return the loss tensor fetched by the trainer.""""""\n        raise NotImplementedError()\n\n    @property\n    def gradients(self) -> Optional[Gradients]:\n        """"""Manually specified gradients - useful for reinforcement learning.""""""\n        return None\n\n    @property\n    def weight(self) -> Optional[tf.Tensor]:\n        """"""Return the weight of this objective.\n\n        The loss will be multiplied by this so the gradients can be controlled\n        in case of multiple objectives.\n\n        Returns:\n            An optional tensor. If None, default weight of 1 is assumed.\n        """"""\n        return None\n\n\nclass CostObjective(Objective[GenericModelPart]):\n    """"""Cost objective class.\n\n    This class represent objectives that are based directly on a `cost`\n    attribute of any compatible model part.\n    """"""\n\n    def __init__(self, decoder: GenericModelPart,\n                 weight: ObjectiveWeight = None) -> None:\n        """"""Construct a new instance of the `CostObjective` class.\n\n        Arguments:\n            decoder: A `GenericModelPart` instance that has a `cost` attribute.\n            weight: The weight of the objective.\n\n        Raises:\n            `TypeError` when the decoder argument does not have the `cost`\n            attribute.\n        """"""\n        check_argument_types()\n        if ""cost"" not in dir(decoder):\n            raise TypeError(""The decoder does not have the \'cost\' attribute"")\n\n        name = ""{} - cost"".format(str(decoder))\n\n        super().__init__(name, decoder)\n        self._weight = weight\n\n    @tensor\n    def loss(self) -> tf.Tensor:\n        return getattr(self.decoder, ""cost"")\n\n    @tensor\n    def weight(self) -> Optional[tf.Tensor]:\n        if self._weight is None:\n            return None\n\n        if isinstance(self._weight, float):\n            return tf.constant(self._weight)\n\n        return self._weight\n'"
neuralmonkey/trainers/rl_trainer.py,24,"b'""""""Training objectives for reinforcement learning.""""""\n\nfrom typing import Callable\n\nimport numpy as np\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.decoders.decoder import Decoder\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.logging import warn\nfrom neuralmonkey.trainers.generic_trainer import Objective\nfrom neuralmonkey.vocabulary import END_TOKEN, PAD_TOKEN\n\n\n# pylint: disable=invalid-name\nRewardFunction = Callable[[np.ndarray, np.ndarray], np.ndarray]\n# pylint: enable=invalid-name\n\n\n# pylint: disable=too-few-public-methods,too-many-locals\nclass ReinforceObjective(Objective[Decoder]):\n\n    # pylint: disable=too-many-arguments\n    def __init__(self,\n                 decoder: Decoder,\n                 reward_function: RewardFunction,\n                 subtract_baseline: bool = False,\n                 normalize: bool = False,\n                 temperature: float = 1.,\n                 ce_smoothing: float = 0.,\n                 alpha: float = 1.,\n                 sample_size: int = 1) -> None:\n        """"""Construct RL objective for training with sentence-level feedback.\n\n        Depending on the options the objective corresponds to:\n        1) sample_size = 1, normalize = False, ce_smoothing = 0.0\n        Bandit objective (Eq. 2) described in \'Bandit Structured Prediction for\n        Neural Sequence-to-Sequence Learning\'\n        (http://www.aclweb.org/anthology/P17-1138)\n        It\'s recommended to set subtract_baseline = True.\n        2) sample_size > 1, normalize = True, ce_smoothing = 0.0\n        Minimum Risk Training as described in \'Minimum Risk Training for Neural\n        Machine Translation\' (http://www.aclweb.org/anthology/P16-1159 Eq. 12).\n        3) sample_size > 1, normalize = False, ce_smoothing = 0.0\n        The Google \'Reinforce\' objective as proposed in \'Google\xe2\x80\x99s NMT System:\n        Bridging the Gap between Human and Machine Translation\'\n        (https://arxiv.org/pdf/1609.08144.pdf) (Eq. 8).\n        4) sample_size > 1, normalize = False, ce_smoothing > 0.0\n        Google\'s \'Mixed\' objective in the above paper (Eq. 9),\n        where ce_smoothing implements alpha.\n\n        Note that \'alpha\' controls the sharpness of the normalized distribution\n        while \'temperature\' controls the sharpness during sampling.\n\n        :param decoder: a recurrent decoder to sample from\n        :param reward_function: any evaluator object\n        :param subtract_baseline: avg reward is subtracted from obtained reward\n        :param normalize: the probabilities of the samples are re-normalized\n        :param sample_size: number of samples to obtain feedback for\n        :param ce_smoothing: add cross-entropy with this coefficient to loss\n        :param alpha: determines the shape of the normalized distribution\n        :param temperature: the softmax temperature for sampling\n        """"""\n        check_argument_types()\n        name = ""{}_rl"".format(decoder.name)\n        super().__init__(name, decoder)\n\n        self.reward_function = reward_function\n        self.subtract_baseline = subtract_baseline\n        self.normalize = normalize\n        self.temperature = temperature\n        self.ce_smoothing = ce_smoothing\n        self.alpha = alpha\n        self.sample_size = sample_size\n    # pylint: enable=too-many-arguments\n\n    @tensor\n    def loss(self) -> tf.Tensor:\n\n        reference = self.decoder.train_inputs\n\n        def _score_with_reward_function(references: np.array,\n                                        hypotheses: np.array) -> np.array:\n            """"""Score (time, batch) arrays with sentence-based reward function.\n\n            Parts of the sentence after generated <pad> or </s> are ignored.\n            BPE-postprocessing is also included.\n\n            :param references: indices of references, shape (time, batch)\n            :param hypotheses: indices of hypotheses, shape (time, batch)\n            :return: an array of batch length with float rewards\n            """"""\n            rewards = []\n            for refs, hyps in zip(references.transpose(),\n                                  hypotheses.transpose()):\n                ref_seq = []\n                hyp_seq = []\n                for r_token in refs:\n                    token = self.decoder.vocabulary.index_to_word[r_token]\n                    if token in (END_TOKEN, PAD_TOKEN):\n                        break\n                    ref_seq.append(token)\n                for h_token in hyps:\n                    token = self.decoder.vocabulary.index_to_word[h_token]\n                    if token in (END_TOKEN, PAD_TOKEN):\n                        break\n                    hyp_seq.append(token)\n                # join BPEs, split on "" "" to prepare list for evaluator\n                refs_tokens = "" "".join(ref_seq).replace(""@@ "", """").split("" "")\n                hyps_tokens = "" "".join(hyp_seq).replace(""@@ "", """").split("" "")\n                reward = float(self.reward_function([hyps_tokens],\n                                                    [refs_tokens]))\n                rewards.append(reward)\n            return np.array(rewards, dtype=np.float32)\n\n        samples_rewards = []\n        samples_logprobs = []\n\n        for _ in range(self.sample_size):\n            # sample from logits\n            # decoded, shape (time, batch)\n            sample_loop_result = self.decoder.decoding_loop(\n                train_mode=False, sample=True, temperature=self.temperature)\n            sample_logits = sample_loop_result.histories.logits\n            sample_decoded = sample_loop_result.histories.output_symbols\n\n            # rewards, shape (batch)\n            # simulate from reference\n            sample_reward = tf.py_func(_score_with_reward_function,\n                                       [reference, sample_decoded],\n                                       tf.float32)\n\n            # pylint: disable=invalid-unary-operand-type\n            word_logprobs = -tf.nn.sparse_softmax_cross_entropy_with_logits(\n                labels=sample_decoded, logits=sample_logits)\n\n            # sum word log prob to sentence log prob\n            # no masking here, since otherwise shorter sentences are preferred\n            sent_logprobs = tf.reduce_sum(word_logprobs, axis=0)\n\n            samples_rewards.append(sample_reward)   # sample_size x batch\n            samples_logprobs.append(sent_logprobs)  # sample_size x batch\n\n        # stack samples, sample_size x batch\n        samples_rewards_stacked = tf.stack(samples_rewards)\n        samples_logprobs_stacked = tf.stack(samples_logprobs)\n\n        if self.subtract_baseline:\n            # if specified, compute the average reward baseline\n            reward_counter = tf.Variable(0.0, trainable=False,\n                                         name=""reward_counter"")\n            reward_sum = tf.Variable(0.0, trainable=False, name=""reward_sum"")\n            # increment the cumulative reward\n            reward_counter = tf.assign_add(\n                reward_counter,\n                tf.to_float(self.decoder.batch_size * self.sample_size))\n            # sum over batch and samples\n            reward_sum = tf.assign_add(reward_sum,\n                                       tf.reduce_sum(samples_rewards_stacked))\n            # compute baseline: avg of previous rewards\n            baseline = tf.div(reward_sum,\n                              tf.maximum(reward_counter, 1.0))\n            samples_rewards_stacked -= baseline\n\n            tf.summary.scalar(\n                ""train_{}/rl_reward_baseline"".format(self.decoder.data_id),\n                tf.reduce_mean(baseline), collections=[""summary_train""])\n\n        if self.normalize:\n            # normalize over sample space\n            samples_logprobs_stacked = tf.nn.softmax(\n                samples_logprobs_stacked * self.alpha, dim=0)\n\n        scored_probs = tf.stop_gradient(\n            tf.negative(samples_rewards_stacked)) * samples_logprobs_stacked\n\n        # sum over samples\n        total_loss = tf.reduce_sum(scored_probs, axis=0)\n\n        # average over batch\n        batch_loss = tf.reduce_mean(total_loss)\n\n        if self.ce_smoothing > 0.0:\n            batch_loss += tf.multiply(self.ce_smoothing, self.decoder.cost)\n\n        tf.summary.scalar(\n            ""train_{}/self_rl_cost"".format(self.decoder.data_id),\n            batch_loss,\n            collections=[""summary_train""])\n\n        return batch_loss\n\n\n# compatibility function\ndef rl_objective(*args, **kwargs) -> ReinforceObjective:\n    warn(""Using deprecated rl_objective function. Use ReinforceObjective class""\n         "" directly."")\n    return ReinforceObjective(*args, **kwargs)\n'"
neuralmonkey/trainers/self_critical_objective.py,21,"b'""""""Training objective for self-critical learning.\n\nSelf-critic learning is a modification of the REINFORCE algorithm that uses the\nreward of the train-time decoder output as a baseline in the update step.\n\nFor more details see: https://arxiv.org/pdf/1612.00563.pdf\n""""""\n\nfrom typing import Callable, Iterable, Tuple, Optional\nfrom itertools import takewhile\nfrom collections import Counter\n\nimport numpy as np\nimport tensorflow as tf\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.decoders.decoder import Decoder\nfrom neuralmonkey.decorators import tensor\nfrom neuralmonkey.trainers.generic_trainer import Objective\nfrom neuralmonkey.vocabulary import END_TOKEN_INDEX\n\n\n# pylint: disable=invalid-name\nRewardFunction = Callable[[np.ndarray, np.ndarray], np.ndarray]\n# pylint: enable=invalid-name\n\n\nclass SelfCriticalObjective(Objective[Decoder]):\n\n    def __init__(self, decoder: Decoder, reward_function: RewardFunction,\n                 weight: float = None) -> None:\n        """"""Self-critical objective.\n\n        Args:\n            decoder: A recurrent decoder.\n            reward_function: A reward function computing score in Python.\n            weight: Mixing weight for a trainer.\n\n        Returns:\n            Objective object to be used in generic trainer.\n        """"""\n        check_argument_types()\n        name = ""{}_self_critical"".format(decoder.name)\n        super().__init__(name, decoder)\n\n        self.reward_function = reward_function\n        self._weight = weight\n\n    @tensor\n    def weight(self) -> Optional[tf.Tensor]:\n        if self._weight is None:\n            return None\n        return tf.constant(self._weight)\n\n    @tensor\n    def loss(self) -> tf.Tensor:\n\n        # decoded, shape (time, batch)\n        train_decoded = tf.argmax(self.decoder.train_logits, axis=2)\n        runtime_decoded = tf.argmax(self.decoder.runtime_logits, axis=2)\n        reference = self.decoder.train_inputs\n\n        # rewards, shape (batch)\n        train_reward = tf.py_func(\n            self.reward_function, [reference, train_decoded], tf.float32)\n        runtime_reward = tf.py_func(\n            self.reward_function, [reference, runtime_decoded], tf.float32)\n\n        tf.summary.scalar(\n            ""train_{}/{}"".format(self.decoder.data_id,\n                                 self.reward_function.__name__),\n            tf.reduce_mean(runtime_reward),\n            collections=[""summary_train""])\n\n        # REINFORCE score: shape (time, batch, vocab)\n        score_by_word = reinforce_score(\n            runtime_reward, train_reward, runtime_decoded,\n            self.decoder.runtime_logits)\n\n        float_mask = tf.to_float(self.decoder.runtime_mask)\n        masked_score_by_word = score_by_word * float_mask\n\n        # sum the matrix (dot product of rows, sum over time, and over batch)\n        # pylint: disable=invalid-unary-operand-type\n        loss = -tf.reduce_sum(masked_score_by_word) / tf.reduce_sum(float_mask)\n        # pylint: enable=invalid-unary-operand-type\n\n        tf.summary.scalar(\n            ""train_{}/self_critical_cost"".format(self.decoder.data_id),\n            loss, collections=[""summary_train""])\n\n        return loss\n\n\ndef reinforce_score(reward: tf.Tensor,\n                    baseline: tf.Tensor,\n                    decoded: tf.Tensor,\n                    logits: tf.Tensor) -> tf.Tensor:\n    """"""Cost function whose derivative is the REINFORCE equation.\n\n    This implements the primitive function to the central equation of the\n    REINFORCE algorithm that estimates the gradients of the loss with respect\n    to decoder logits.\n\n    It uses the fact that the second term of the product (the difference of the\n    word distribution and one hot vector of the decoded word) is a derivative\n    of negative log likelihood of the decoded word. The reward function and the\n    baseline are however treated as a constant, so they influence the derivate\n    only multiplicatively.\n    """"""\n\n    # shape (1, batch, 1)\n    reward_diff = tf.expand_dims(reward - baseline, 0)\n\n    # runtime probabilities, shape (time, batch, vocab)\n    decoded_neg_likelihood = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        labels=decoded, logits=logits)\n\n    # REINFORCE gradient, shape (time, batch, vocab)\n    score = tf.stop_gradient(reward_diff) * decoded_neg_likelihood\n    return score\n\n\ndef sentence_bleu(references: np.ndarray,\n                  hypotheses: np.ndarray) -> np.ndarray:\n    """"""Compute index-based sentence-level BLEU score.\n\n    Computes sentence level BLEU on indices outputed by the decoder, i.e.\n    whatever the decoder uses as a unit is used a token in the BLEU\n    computation, ignoring the tokens may be sub-word units.\n    """"""\n\n    bleu_scores = []\n    for ref, hyp in zip(np.transpose(references),\n                        np.transpose(hypotheses)):\n        matched_counts = []\n        hyp_n_grams_counts = []\n\n        for n in range(1, 5):\n            matched, total, _ = _count_matching_n_grams(ref, hyp, n)\n\n            if n > 1:\n                matched += 1\n                total += 1\n\n            matched_counts.append(matched)\n            hyp_n_grams_counts.append(total)\n\n        if hyp_n_grams_counts[0] == 0:\n            bleu_scores.append(0.)\n        else:\n            precision = (\n                np.prod(matched_counts) / np.prod(hyp_n_grams_counts)) ** .25\n            ref_len = sum(1 for _ in\n                          takewhile(lambda i: i != END_TOKEN_INDEX, ref))\n            brevity_penalty = np.min([\n                1., np.exp(1 - ref_len / hyp_n_grams_counts[0])])\n\n            bleu_scores.append(brevity_penalty * precision)\n\n    assert all(0 <= s <= 1 for s in bleu_scores)\n    return np.array(bleu_scores, dtype=np.float32)\n\n\ndef sentence_gleu(references: np.ndarray,\n                  hypotheses: np.ndarray) -> np.ndarray:\n    """"""Compute index-based GLEU score.\n\n    GLEU score is a sentence-level metric used in Google\'s Neural MT as a\n    reward in reinforcement learning (https://arxiv.org/abs/1609.08144).\n    It is a minimum of precision and recall on 1- to 4-grams.\n\n    It operates over the indices emitted by the decoder which are not\n    necessarily tokens (could be characters or subword units).\n    """"""\n    gleu_scores = []\n\n    for ref, hyp in zip(np.transpose(references),\n                        np.transpose(hypotheses)):\n\n        matched_counts = []\n        hyp_n_grams_counts = []\n        ref_n_grams_counts = []\n\n        for n in range(1, 5):\n            matched, total_hyp, total_ref = _count_matching_n_grams(\n                ref, hyp, n)\n            matched_counts.append(matched)\n            hyp_n_grams_counts.append(total_hyp)\n            ref_n_grams_counts.append(total_ref)\n\n        precision = np.sum(matched_counts) / np.sum(hyp_n_grams_counts)\n        recall = np.sum(matched_counts) / np.sum(ref_n_grams_counts)\n\n        assert 0. <= precision <= 1.0\n        assert 0. <= recall <= 1.0\n\n        gleu_scores.append(min(precision, recall))\n\n    return np.array(gleu_scores, dtype=np.float32)\n\n\ndef _count_matching_n_grams(ref: np.ndarray,\n                            hyp: np.ndarray,\n                            n: int) -> Tuple[int, int, int]:\n    ref_counts = Counter()  # type: Counter\n    total_ref_n_grams = 0\n    for n_gram in _get_n_grams(ref, n):\n        ref_counts[str(n_gram)] += 1\n        total_ref_n_grams += 1\n\n    matched_n_grams = 0\n    total_hyp_n_grams = 0\n    hyp_n_grams = _get_n_grams(hyp, n)\n    for n_gram in hyp_n_grams:\n        n_gram_s = str(n_gram)\n        if ref_counts[n_gram_s] > 0:\n            matched_n_grams += 1\n            ref_counts[n_gram_s] -= 1\n        total_hyp_n_grams += 1\n\n    assert matched_n_grams <= total_hyp_n_grams\n    assert matched_n_grams <= total_ref_n_grams\n\n    return matched_n_grams, total_hyp_n_grams, total_ref_n_grams\n\n\ndef _get_n_grams(indices: np.ndarray, order: int) -> Iterable[np.ndarray]:\n    all_n_grams = [indices[i:i + order]\n                   for i in range(len(indices) - order + 1)]\n    return takewhile(lambda g: g[-1] != END_TOKEN_INDEX, all_n_grams)\n'"
neuralmonkey/trainers/test_multitask_trainer.py,7,"b'#!/usr/bin/env python3.5\n""""""Unit tests for the multitask trainer.""""""\n# pylint: disable=comparison-with-callable,attribute-defined-outside-init\n\nimport unittest\nimport tensorflow as tf\n\nfrom neuralmonkey.model.model_part import ModelPart\nfrom neuralmonkey.logging import log\nfrom neuralmonkey.trainers.generic_trainer import Objective, GenericTrainer\nfrom neuralmonkey.trainers.multitask_trainer import MultitaskTrainer\nfrom neuralmonkey.decorators import tensor\n\n\nclass TestMP(ModelPart):\n\n    # pylint: disable=no-self-use\n    @tensor\n    def var(self) -> tf.Variable:\n        return tf.get_variable(name=""var"", shape=[], dtype=tf.float32)\n    # pylint: enable=no-self-use\n\n    @tensor\n    def loss(self) -> tf.Tensor:\n        return 10 - self.var\n\n\n# pylint: disable=too-few-public-methods\nclass DummyObjective(Objective[TestMP]):\n    @tensor\n    def loss(self) -> tf.Tensor:\n        return self.decoder.loss\n# pylint: enable=too-few-public-methods\n\n\nclass TestMultitaskTrainer(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        tf.reset_default_graph()\n\n        cls.dataset = {\n            ""id"": tf.constant([[""hello"", ""world""], [""test"", ""this""]]),\n            ""data_id"": tf.constant([[""A"", ""B"", ""C""], [""D"", ""E"", ""F""]])}\n\n    def setUp(self):\n        self.mpart = TestMP(""dummy_model_part"")\n        self.mpart_2 = TestMP(""dummy_model_part_2"")\n\n        objective = DummyObjective(name=""dummy"", decoder=self.mpart)\n        objective_2 = DummyObjective(name=""dummy_2"", decoder=self.mpart_2)\n\n        self.trainer1 = GenericTrainer([objective])\n        self.trainer2 = GenericTrainer([objective_2], clip_norm=1.0)\n\n    def test_mt_trainer(self):\n        # TODO(tf-data) multitask trainer is likely broken by the changes\n\n        trainer = MultitaskTrainer(\n            [self.trainer1, self.trainer2, self.trainer1])\n\n        feedables = {self.mpart, self.mpart_2, self.trainer1, self.trainer2}\n        for feedable in feedables:\n            feedable.register_input(self.dataset)\n\n        log(""Blessing trainer fetches: {}"".format(trainer.fetches))\n\n        self.assertSetEqual(trainer.feedables, feedables)\n        self.assertSetEqual(trainer.parameterizeds, {self.mpart, self.mpart_2})\n\n        self.assertSetEqual(\n            set(trainer.var_list), {self.mpart.var, self.mpart_2.var})\n\n        self.assertTrue(trainer.trainer_idx == 0)\n\n        executable = trainer.get_executable()\n        # mparts = trainer.feedables\n        fetches, feeds = executable.next_to_execute()\n        # self.assertSetEqual(mparts, {self.mpart})\n        self.assertFalse(feeds)\n\n        self.assertTrue(trainer.trainer_idx == 1)\n        self.assertTrue(fetches[""losses""][0] == self.mpart.loss)\n\n        executable = trainer.get_executable()\n        fetches, feeds = executable.next_to_execute()\n        # self.assertSetEqual(mparts, {self.mpart_2})\n        self.assertFalse(feeds)\n\n        self.assertTrue(trainer.trainer_idx == 2)\n        self.assertTrue(fetches[""losses""][0] == self.mpart_2.loss)\n\n        executable = trainer.get_executable()\n        fetches, feeds = executable.next_to_execute()\n        # self.assertSetEqual(mparts, {self.mpart})\n        self.assertFalse(feeds)\n\n        self.assertTrue(trainer.trainer_idx == 0)\n        self.assertTrue(fetches[""losses""][0] == self.mpart.loss)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
neuralmonkey/util/__init__.py,0,b''
neuralmonkey/util/match_type.py,0,"b'from typing import Any, Type\nfrom typeguard import check_type\n\n\ndef match_type(value: Any, type_: Type) -> bool:\n    try:\n        check_type(""value"", value, type_, None)  # type: ignore\n    except TypeError:\n        return False\n    return True\n'"
neuralmonkey/util/word2vec.py,0,"b'""""""Word2vec plug-in module.\n\nThis module provides functionality needed to work with word2vec files.\n""""""\n\nfrom typing import Callable, List\n\nimport numpy as np\nfrom typeguard import check_argument_types\n\nfrom neuralmonkey.vocabulary import Vocabulary, SPECIAL_TOKENS\n\n\nclass Word2Vec:\n\n    def __init__(\n            self,\n            path: str,\n            encoding: str = ""utf-8"") -> None:\n        """"""Load the word2vec file.\n\n        Args:\n            path: Path to word embeddings stored in the word2vec format.\n            encoding: File encoding.\n        """"""\n        check_argument_types()\n\n        # Create the vocabulary object, load the words and vectors from the\n        # file\n\n        words = []  # List[str]\n        embedding_vectors = []  # type: List[np.ndarray]\n\n        with open(path, encoding=encoding) as f_data:\n\n            header = next(f_data)\n            emb_size = int(header.split()[1])\n\n            # Add zero embeddings for padding, start, end, and unk token\n            for _ in SPECIAL_TOKENS:\n                embedding_vectors.append(np.zeros(emb_size))\n\n            for line in f_data:\n                fields = line.split()\n                word = fields[0]\n                vector = np.fromiter((float(x) for x in fields[1:]),\n                                     dtype=np.float)\n\n                assert vector.shape[0] == emb_size\n\n                # Embedding of unknown token should be at index 3 to match the\n                # vocabulary implementation\n                if word in SPECIAL_TOKENS:\n                    embedding_vectors[SPECIAL_TOKENS.index(word)] = vector\n                else:\n                    words.append(word)\n                    embedding_vectors.append(vector)\n\n        self.vocab = Vocabulary(words)\n\n        assert emb_size is not None\n\n        self.embedding_matrix = np.stack(embedding_vectors)\n\n    @property\n    def vocabulary(self) -> Vocabulary:\n        """"""Get a vocabulary object generated from this word2vec instance.""""""\n        return self.vocab\n\n    @property\n    def embeddings(self) -> np.ndarray:\n        """"""Get the embedding matrix.""""""\n        return self.embedding_matrix\n\n\ndef get_word2vec_initializer(w2v: Word2Vec) -> Callable:\n    """"""Create a word2vec initializer.\n\n    A higher-order function that can be called from configuration.\n    """"""\n    check_argument_types()\n\n    def init(shape: List[int], **kwargs) -> np.ndarray:\n        if shape != list(w2v.embeddings.shape):\n            raise ValueError(\n                ""Shapes of model and word2vec embeddings do not match. ""\n                ""Word2Vec shape: {}, Should have been: {}""\n                .format(w2v.embeddings.shape, shape))\n        return w2v.embeddings\n\n    return init\n\n\ndef word2vec_vocabulary(w2v: Word2Vec) -> Vocabulary:\n    """"""Return the vocabulary from a word2vec object.\n\n    This is a helper method used from configuration.\n    """"""\n    check_argument_types()\n    return w2v.vocabulary\n'"
neuralmonkey/writers/__init__.py,0,b''
neuralmonkey/writers/auto.py,0,"b'from typing import Any, List, Dict, Union\nimport collections\nimport numpy as np\n\nfrom neuralmonkey.util.match_type import match_type\nfrom neuralmonkey.writers.plain_text_writer import (\n    Writer, tokenized_text_writer, text_writer)\nfrom neuralmonkey.writers.numpy_writer import (\n    numpy_array_writer, numpy_dict_writer)\n\n\ndef _check_savable_dict(data: Any) -> bool:\n    """"""Check if the data is of savable type.\n\n    Arguments:\n        data: Variable that holds some results.\n\n    Returns:\n        Boolean that says whether the saving of this type is implemented.\n    """"""\n    if not (data and data[0]):\n        return False\n\n    supported_type = Union[\n        List[Dict[str, np.ndarray]],\n        List[List[Dict[str, np.ndarray]]]]\n\n    return match_type(data, supported_type)  # type: ignore\n\n\ndef auto_writer(encoding: str = ""utf-8"") -> Writer:\n\n    text_tok_writer = tokenized_text_writer(encoding)\n    text_plain_writer = text_writer(encoding)\n\n    def writer(path: str, data: Any) -> None:\n        if isinstance(data, np.ndarray):\n            numpy_array_writer(path, data)\n        elif _check_savable_dict(data):\n            numpy_dict_writer(path, data)\n        elif isinstance(next(iter(data)), collections.Iterable):\n            text_tok_writer(path, data)\n        else:\n            text_plain_writer(path, data)\n\n    return writer\n\n\n# pylint: disable=invalid-name\nAutoWriter = auto_writer()\n'"
neuralmonkey/writers/numpy_writer.py,0,"b'from typing import Iterator, Dict\nimport numpy as np\nfrom neuralmonkey.logging import log\n\n\ndef numpy_array_writer(path: str, data: np.ndarray) -> None:\n    np.save(path, data)\n    log(""Result saved as numpy array to \'{}\'"".format(path))\n\n\ndef numpy_dict_writer(\n        path: str, data: Iterator[Dict[str, np.ndarray]]) -> None:\n    unbatched = dict(\n        zip(next(iter(data)), zip(*[d.values() for d in data])))\n\n    np.savez(path, **unbatched)\n    log(""Result saved as numpy data to \'{}.npz\'"".format(path))\n'"
neuralmonkey/writers/plain_text_writer.py,0,"b'from typing import Iterator, List, Any, Callable\n\nfrom neuralmonkey.logging import log\nfrom neuralmonkey.readers.plain_text_reader import ALNUM_CHARSET\n\n# pylint: disable=invalid-name\n# Writer: function that gets file and the data\nWriter = Callable[[str, Any], None]\n# pylint: enable=invalid-name\n\n\ndef t2t_detokenize(data: Iterator[List[str]]) -> Iterator[str]:\n    """"""Detokenize text tokenized by t2t_tokenized_text_reader.\n\n    Method is inspired by tensor2tensor tokenizer.decode method:\n    https://github.com/tensorflow/tensor2tensor/blob/v1.5.5/tensor2tensor/data_generators/tokenizer.py\n    """"""\n    for sentence in data:\n        is_alnum = [t[0] in ALNUM_CHARSET for t in sentence]\n        ret = []\n        for i, token in enumerate(sentence):\n            if i > 0 and is_alnum[i - 1] and is_alnum[i]:\n                ret.append("" "")\n            ret.append(token)\n        yield """".join(ret)\n\n\ndef text_writer(encoding: str = ""utf-8"") -> Writer:\n\n    def writer(path: str, data: Iterator) -> None:\n        with open(path, ""w"", encoding=encoding) as f_out:\n            for sentence in data:\n                f_out.write(str(sentence) + ""\\n"")\n        log(""Result saved as plain text in \'{}\'"".format(path))\n\n    return writer\n\n\ndef tokenized_text_writer(encoding: str = ""utf-8"") -> Writer:\n    """"""Get a writer that is reversed to the tokenized_text_reader.""""""\n    def writer(path: str, data: Iterator[List[str]]) -> None:\n        wrt = text_writer(encoding)\n        wrt(path, ("" "".join(s) for s in data))\n\n    return writer\n\n\ndef t2t_tokenized_text_writer(encoding: str = ""utf-8"") -> Writer:\n    """"""Get a writer that is reversed to the t2t_tokenized_text_reader.""""""\n    def writer(path: str, data: Iterator[List[str]]) -> None:\n        wrt = text_writer(encoding)\n        wrt(path, t2t_detokenize(data))\n\n    return writer\n\n\n# pylint: disable=invalid-name\nUtfPlainTextWriter = tokenized_text_writer()\nT2TWriter = t2t_tokenized_text_writer()\n# pylint: enable=invalid-name\n'"
