file_path,api_count,code
src/cameras.py,0,"b'\n""""""Utilities to deal with the cameras of human3.6m""""""\n\nfrom __future__ import division\n\nimport h5py\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport data_utils\nimport viz\n\ndef project_point_radial( P, R, T, f, c, k, p ):\n  """"""\n  Project points from 3d to 2d using camera parameters\n  including radial and tangential distortion\n\n  Args\n    P: Nx3 points in world coordinates\n    R: 3x3 Camera rotation matrix\n    T: 3x1 Camera translation parameters\n    f: (scalar) Camera focal length\n    c: 2x1 Camera center\n    k: 3x1 Camera radial distortion coefficients\n    p: 2x1 Camera tangential distortion coefficients\n  Returns\n    Proj: Nx2 points in pixel space\n    D: 1xN depth of each point in camera space\n    radial: 1xN radial distortion per point\n    tan: 1xN tangential distortion per point\n    r2: 1xN squared radius of the projected points before distortion\n  """"""\n\n  # P is a matrix of 3-dimensional points\n  assert len(P.shape) == 2\n  assert P.shape[1] == 3\n\n  N = P.shape[0]\n  X = R.dot( P.T - T ) # rotate and translate\n  XX = X[:2,:] / X[2,:]\n  r2 = XX[0,:]**2 + XX[1,:]**2\n\n  radial = 1 + np.einsum( \'ij,ij->j\', np.tile(k,(1, N)), np.array([r2, r2**2, r2**3]) );\n  tan = p[0]*XX[1,:] + p[1]*XX[0,:]\n\n  XXX = XX * np.tile(radial+tan,(2,1)) + np.outer(np.array([p[1], p[0]]).reshape(-1), r2 )\n\n  Proj = (f * XXX) + c\n  Proj = Proj.T\n\n  D = X[2,]\n\n  return Proj, D, radial, tan, r2\n\ndef world_to_camera_frame(P, R, T):\n  """"""\n  Convert points from world to camera coordinates\n\n  Args\n    P: Nx3 3d points in world coordinates\n    R: 3x3 Camera rotation matrix\n    T: 3x1 Camera translation parameters\n  Returns\n    X_cam: Nx3 3d points in camera coordinates\n  """"""\n\n  assert len(P.shape) == 2\n  assert P.shape[1] == 3\n\n  X_cam = R.dot( P.T - T ) # rotate and translate\n\n  return X_cam.T\n\ndef camera_to_world_frame(P, R, T):\n  """"""Inverse of world_to_camera_frame\n\n  Args\n    P: Nx3 points in camera coordinates\n    R: 3x3 Camera rotation matrix\n    T: 3x1 Camera translation parameters\n  Returns\n    X_cam: Nx3 points in world coordinates\n  """"""\n\n  assert len(P.shape) == 2\n  assert P.shape[1] == 3\n\n  X_cam = R.T.dot( P.T ) + T # rotate and translate\n\n  return X_cam.T\n\ndef load_camera_params( hf, path ):\n  """"""Load h36m camera parameters\n\n  Args\n    hf: hdf5 open file with h36m cameras data\n    path: path or key inside hf to the camera we are interested in\n  Returns\n    R: 3x3 Camera rotation matrix\n    T: 3x1 Camera translation parameters\n    f: (scalar) Camera focal length\n    c: 2x1 Camera center\n    k: 3x1 Camera radial distortion coefficients\n    p: 2x1 Camera tangential distortion coefficients\n    name: String with camera id\n  """"""\n\n  R = hf[ path.format(\'R\') ][:]\n  R = R.T\n\n  T = hf[ path.format(\'T\') ][:]\n  f = hf[ path.format(\'f\') ][:]\n  c = hf[ path.format(\'c\') ][:]\n  k = hf[ path.format(\'k\') ][:]\n  p = hf[ path.format(\'p\') ][:]\n\n  name = hf[ path.format(\'Name\') ][:]\n  name = """".join( [chr(item) for item in name] )\n\n  return R, T, f, c, k, p, name\n\ndef load_cameras( bpath=\'cameras.h5\', subjects=[1,5,6,7,8,9,11] ):\n  """"""Loads the cameras of h36m\n\n  Args\n    bpath: path to hdf5 file with h36m camera data\n    subjects: List of ints representing the subject IDs for which cameras are requested\n  Returns\n    rcams: dictionary of 4 tuples per subject ID containing its camera parameters for the 4 h36m cams\n  """"""\n  rcams = {}\n\n  with h5py.File(bpath,\'r\') as hf:\n    for s in subjects:\n      for c in range(4): # There are 4 cameras in human3.6m\n        rcams[(s, c+1)] = load_camera_params(hf, \'subject%d/camera%d/{0}\' % (s,c+1) )\n\n  return rcams\n'"
src/data_utils.py,0,"b'\n""""""Utility functions for dealing with human3.6m data.""""""\n\nfrom __future__ import division\n\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport cameras\nimport viz\nimport h5py\nimport glob\nimport copy\n\n# Human3.6m IDs for training and testing\nTRAIN_SUBJECTS = [1,5,6,7,8]\nTEST_SUBJECTS  = [9,11]\n\n# Joints in H3.6M -- data has 32 joints, but only 17 that move; these are the indices.\nH36M_NAMES = [\'\']*32\nH36M_NAMES[0]  = \'Hip\'\nH36M_NAMES[1]  = \'RHip\'\nH36M_NAMES[2]  = \'RKnee\'\nH36M_NAMES[3]  = \'RFoot\'\nH36M_NAMES[6]  = \'LHip\'\nH36M_NAMES[7]  = \'LKnee\'\nH36M_NAMES[8]  = \'LFoot\'\nH36M_NAMES[12] = \'Spine\'\nH36M_NAMES[13] = \'Thorax\'\nH36M_NAMES[14] = \'Neck/Nose\'\nH36M_NAMES[15] = \'Head\'\nH36M_NAMES[17] = \'LShoulder\'\nH36M_NAMES[18] = \'LElbow\'\nH36M_NAMES[19] = \'LWrist\'\nH36M_NAMES[25] = \'RShoulder\'\nH36M_NAMES[26] = \'RElbow\'\nH36M_NAMES[27] = \'RWrist\'\n\n# Stacked Hourglass produces 16 joints. These are the names.\nSH_NAMES = [\'\']*16\nSH_NAMES[0]  = \'RFoot\'\nSH_NAMES[1]  = \'RKnee\'\nSH_NAMES[2]  = \'RHip\'\nSH_NAMES[3]  = \'LHip\'\nSH_NAMES[4]  = \'LKnee\'\nSH_NAMES[5]  = \'LFoot\'\nSH_NAMES[6]  = \'Hip\'\nSH_NAMES[7]  = \'Spine\'\nSH_NAMES[8]  = \'Thorax\'\nSH_NAMES[9]  = \'Head\'\nSH_NAMES[10] = \'RWrist\'\nSH_NAMES[11] = \'RElbow\'\nSH_NAMES[12] = \'RShoulder\'\nSH_NAMES[13] = \'LShoulder\'\nSH_NAMES[14] = \'LElbow\'\nSH_NAMES[15] = \'LWrist\'\n\ndef load_data( bpath, subjects, actions, dim=3 ):\n  """"""\n  Loads 2d ground truth from disk, and puts it in an easy-to-acess dictionary\n\n  Args\n    bpath: String. Path where to load the data from\n    subjects: List of integers. Subjects whose data will be loaded\n    actions: List of strings. The actions to load\n    dim: Integer={2,3}. Load 2 or 3-dimensional data\n  Returns:\n    data: Dictionary with keys k=(subject, action, seqname)\n      values v=(nx(32*2) matrix of 2d ground truth)\n      There will be 2 entries per subject/action if loading 3d data\n      There will be 8 entries per subject/action if loading 2d data\n  """"""\n\n  if not dim in [2,3]:\n    raise(ValueError, \'dim must be 2 or 3\')\n\n  data = {}\n\n  for subj in subjects:\n    for action in actions:\n\n      print(\'Reading subject {0}, action {1}\'.format(subj, action))\n\n      dpath = os.path.join( bpath, \'S{0}\'.format(subj), \'MyPoses/{0}D_positions\'.format(dim), \'{0}*.h5\'.format(action) )\n      print( dpath )\n\n      fnames = glob.glob( dpath )\n\n      loaded_seqs = 0\n      for fname in fnames:\n        seqname = os.path.basename( fname )\n\n        # This rule makes sure SittingDown is not loaded when Sitting is requested\n        if action == ""Sitting"" and seqname.startswith( ""SittingDown"" ):\n          continue\n\n        # This rule makes sure that WalkDog and WalkTogeter are not loaded when\n        # Walking is requested.\n        if seqname.startswith( action ):\n          print( fname )\n          loaded_seqs = loaded_seqs + 1\n\n          with h5py.File( fname, \'r\' ) as h5f:\n            poses = h5f[\'{0}D_positions\'.format(dim)][:]\n\n          poses = poses.T\n          data[ (subj, action, seqname) ] = poses\n\n      if dim == 2:\n        assert loaded_seqs == 8, ""Expecting 8 sequences, found {0} instead"".format( loaded_seqs )\n      else:\n        assert loaded_seqs == 2, ""Expecting 2 sequences, found {0} instead"".format( loaded_seqs )\n\n  return data\n\n\ndef load_stacked_hourglass(data_dir, subjects, actions):\n  """"""\n  Load 2d detections from disk, and put it in an easy-to-acess dictionary.\n\n  Args\n    data_dir: string. Directory where to load the data from,\n    subjects: list of integers. Subjects whose data will be loaded.\n    actions: list of strings. The actions to load.\n  Returns\n    data: dictionary with keys k=(subject, action, seqname)\n          values v=(nx(32*2) matrix of 2d stacked hourglass detections)\n          There will be 2 entries per subject/action if loading 3d data\n          There will be 8 entries per subject/action if loading 2d data\n  """"""\n  # Permutation that goes from SH detections to H36M ordering.\n  SH_TO_GT_PERM = np.array([SH_NAMES.index( h ) for h in H36M_NAMES if h != \'\' and h in SH_NAMES])\n  assert np.all( SH_TO_GT_PERM == np.array([6,2,1,0,3,4,5,7,8,9,13,14,15,12,11,10]) )\n\n  data = {}\n\n  for subj in subjects:\n    for action in actions:\n\n      print(\'Reading subject {0}, action {1}\'.format(subj, action))\n\n      dpath = os.path.join( data_dir, \'S{0}\'.format(subj), \'StackedHourglass/{0}*.h5\'.format(action) )\n      print( dpath )\n\n      fnames = glob.glob( dpath )\n\n      loaded_seqs = 0\n      for fname in fnames:\n        seqname = os.path.basename( fname )\n        seqname = seqname.replace(\'_\',\' \')\n\n        # This rule makes sure SittingDown is not loaded when Sitting is requested\n        if action == ""Sitting"" and seqname.startswith( ""SittingDown"" ):\n          continue\n\n        # This rule makes sure that WalkDog and WalkTogeter are not loaded when\n        # Walking is requested.\n        if seqname.startswith( action ):\n          print( fname )\n          loaded_seqs = loaded_seqs + 1\n\n          # Load the poses from the .h5 file\n          with h5py.File( fname, \'r\' ) as h5f:\n            poses = h5f[\'poses\'][:]\n\n            # Permute the loaded data to make it compatible with H36M\n            poses = poses[:,SH_TO_GT_PERM,:]\n\n            # Reshape into n x (32*2) matrix\n            poses = np.reshape(poses,[poses.shape[0], -1])\n            poses_final = np.zeros([poses.shape[0], len(H36M_NAMES)*2])\n\n            dim_to_use_x    = np.where(np.array([x != \'\' and x != \'Neck/Nose\' for x in H36M_NAMES]))[0] * 2\n            dim_to_use_y    = dim_to_use_x+1\n\n            dim_to_use = np.zeros(len(SH_NAMES)*2,dtype=np.int32)\n            dim_to_use[0::2] = dim_to_use_x\n            dim_to_use[1::2] = dim_to_use_y\n            poses_final[:,dim_to_use] = poses\n            seqname = seqname+\'-sh\'\n            data[ (subj, action, seqname) ] = poses_final\n\n      # Make sure we loaded 8 sequences\n      if (subj == 11 and action == \'Directions\'): # <-- this video is damaged\n        assert loaded_seqs == 7, ""Expecting 7 sequences, found {0} instead. S:{1} {2}"".format(loaded_seqs, subj, action )\n      else:\n        assert loaded_seqs == 8, ""Expecting 8 sequences, found {0} instead. S:{1} {2}"".format(loaded_seqs, subj, action )\n\n  return data\n\n\ndef normalization_stats(complete_data, dim, predict_14=False ):\n  """"""\n  Computes normalization statistics: mean and stdev, dimensions used and ignored\n\n  Args\n    complete_data: nxd np array with poses\n    dim. integer={2,3} dimensionality of the data\n    predict_14. boolean. Whether to use only 14 joints\n  Returns\n    data_mean: np vector with the mean of the data\n    data_std: np vector with the standard deviation of the data\n    dimensions_to_ignore: list of dimensions not used in the model\n    dimensions_to_use: list of dimensions used in the model\n  """"""\n  if not dim in [2,3]:\n    raise(ValueError, \'dim must be 2 or 3\')\n\n  data_mean = np.mean(complete_data, axis=0)\n  data_std  =  np.std(complete_data, axis=0)\n\n  # Encodes which 17 (or 14) 2d-3d pairs we are predicting\n  dimensions_to_ignore = []\n  if dim == 2:\n    dimensions_to_use    = np.where(np.array([x != \'\' and x != \'Neck/Nose\' for x in H36M_NAMES]))[0]\n    dimensions_to_use    = np.sort( np.hstack( (dimensions_to_use*2, dimensions_to_use*2+1)))\n    dimensions_to_ignore = np.delete( np.arange(len(H36M_NAMES)*2), dimensions_to_use )\n  else: # dim == 3\n    dimensions_to_use = np.where(np.array([x != \'\' for x in H36M_NAMES]))[0]\n    dimensions_to_use = np.delete( dimensions_to_use, [0,7,9] if predict_14 else 0 )\n\n    dimensions_to_use = np.sort( np.hstack( (dimensions_to_use*3,\n                                             dimensions_to_use*3+1,\n                                             dimensions_to_use*3+2)))\n    dimensions_to_ignore = np.delete( np.arange(len(H36M_NAMES)*3), dimensions_to_use )\n\n  return data_mean, data_std, dimensions_to_ignore, dimensions_to_use\n\n\ndef transform_world_to_camera(poses_set, cams, ncams=4 ):\n    """"""\n    Project 3d poses from world coordinate to camera coordinate system\n    Args\n      poses_set: dictionary with 3d poses\n      cams: dictionary with cameras\n      ncams: number of cameras per subject\n    Return:\n      t3d_camera: dictionary with 3d poses in camera coordinate\n    """"""\n    t3d_camera = {}\n    for t3dk in sorted( poses_set.keys() ):\n\n      subj, action, seqname = t3dk\n      t3d_world = poses_set[ t3dk ]\n\n      for c in range( ncams ):\n        R, T, f, c, k, p, name = cams[ (subj, c+1) ]\n        camera_coord = cameras.world_to_camera_frame( np.reshape(t3d_world, [-1, 3]), R, T)\n        camera_coord = np.reshape( camera_coord, [-1, len(H36M_NAMES)*3] )\n\n        sname = seqname[:-3]+"".""+name+"".h5"" # e.g.: Waiting 1.58860488.h5\n        t3d_camera[ (subj, action, sname) ] = camera_coord\n\n    return t3d_camera\n\n\ndef normalize_data(data, data_mean, data_std, dim_to_use ):\n  """"""\n  Normalizes a dictionary of poses\n\n  Args\n    data: dictionary where values are\n    data_mean: np vector with the mean of the data\n    data_std: np vector with the standard deviation of the data\n    dim_to_use: list of dimensions to keep in the data\n  Returns\n    data_out: dictionary with same keys as data, but values have been normalized\n  """"""\n  data_out = {}\n\n  for key in data.keys():\n    data[ key ] = data[ key ][ :, dim_to_use ]\n    mu = data_mean[dim_to_use]\n    stddev = data_std[dim_to_use]\n    data_out[ key ] = np.divide( (data[key] - mu), stddev )\n\n  return data_out\n\n\ndef unNormalizeData(normalized_data, data_mean, data_std, dimensions_to_ignore):\n  """"""\n  Un-normalizes a matrix whose mean has been substracted and that has been divided by\n  standard deviation. Some dimensions might also be missing\n\n  Args\n    normalized_data: nxd matrix to unnormalize\n    data_mean: np vector with the mean of the data\n    data_std: np vector with the standard deviation of the data\n    dimensions_to_ignore: list of dimensions that were removed from the original data\n  Returns\n    orig_data: the input normalized_data, but unnormalized\n  """"""\n  T = normalized_data.shape[0] # Batch size\n  D = data_mean.shape[0] # Dimensionality\n\n  orig_data = np.zeros((T, D), dtype=np.float32)\n  dimensions_to_use = np.array([dim for dim in range(D)\n                                if dim not in dimensions_to_ignore])\n\n  orig_data[:, dimensions_to_use] = normalized_data\n\n  # Multiply times stdev and add the mean\n  stdMat = data_std.reshape((1, D))\n  stdMat = np.repeat(stdMat, T, axis=0)\n  meanMat = data_mean.reshape((1, D))\n  meanMat = np.repeat(meanMat, T, axis=0)\n  orig_data = np.multiply(orig_data, stdMat) + meanMat\n  return orig_data\n\n\ndef define_actions( action ):\n  """"""\n  Given an action string, returns a list of corresponding actions.\n\n  Args\n    action: String. either ""all"" or one of the h36m actions\n  Returns\n    actions: List of strings. Actions to use.\n  Raises\n    ValueError: if the action is not a valid action in Human 3.6M\n  """"""\n  actions = [""Directions"",""Discussion"",""Eating"",""Greeting"",\n           ""Phoning"",""Photo"",""Posing"",""Purchases"",\n           ""Sitting"",""SittingDown"",""Smoking"",""Waiting"",\n           ""WalkDog"",""Walking"",""WalkTogether""]\n\n  if action == ""All"" or action == ""all"":\n    return actions\n\n  if not action in actions:\n    raise( ValueError, ""Unrecognized action: %s"" % action )\n\n  return [action]\n\n\ndef project_to_cameras( poses_set, cams, ncams=4 ):\n  """"""\n  Project 3d poses using camera parameters\n\n  Args\n    poses_set: dictionary with 3d poses\n    cams: dictionary with camera parameters\n    ncams: number of cameras per subject\n  Returns\n    t2d: dictionary with 2d poses\n  """"""\n  t2d = {}\n\n  for t3dk in sorted( poses_set.keys() ):\n    subj, a, seqname = t3dk\n    t3d = poses_set[ t3dk ]\n\n    for cam in range( ncams ):\n      R, T, f, c, k, p, name = cams[ (subj, cam+1) ]\n      pts2d, _, _, _, _ = cameras.project_point_radial( np.reshape(t3d, [-1, 3]), R, T, f, c, k, p )\n\n      pts2d = np.reshape( pts2d, [-1, len(H36M_NAMES)*2] )\n      sname = seqname[:-3]+"".""+name+"".h5"" # e.g.: Waiting 1.58860488.h5\n      t2d[ (subj, a, sname) ] = pts2d\n\n  return t2d\n\n\ndef read_2d_predictions( actions, data_dir ):\n  """"""\n  Loads 2d data from precomputed Stacked Hourglass detections\n\n  Args\n    actions: list of strings. Actions to load\n    data_dir: string. Directory where the data can be loaded from\n  Returns\n    train_set: dictionary with loaded 2d stacked hourglass detections for training\n    test_set: dictionary with loaded 2d stacked hourglass detections for testing\n    data_mean: vector with the mean of the 2d training data\n    data_std: vector with the standard deviation of the 2d training data\n    dim_to_ignore: list with the dimensions to not predict\n    dim_to_use: list with the dimensions to predict\n  """"""\n\n  train_set = load_stacked_hourglass( data_dir, TRAIN_SUBJECTS, actions)\n  test_set  = load_stacked_hourglass( data_dir, TEST_SUBJECTS,  actions)\n\n  complete_train = copy.deepcopy( np.vstack( train_set.values() ))\n  data_mean, data_std,  dim_to_ignore, dim_to_use = normalization_stats( complete_train, dim=2 )\n\n  train_set = normalize_data( train_set, data_mean, data_std, dim_to_use )\n  test_set  = normalize_data( test_set,  data_mean, data_std, dim_to_use )\n\n  return train_set, test_set, data_mean, data_std, dim_to_ignore, dim_to_use\n\n\ndef create_2d_data( actions, data_dir, rcams ):\n  """"""\n  Creates 2d poses by projecting 3d poses with the corresponding camera\n  parameters. Also normalizes the 2d poses\n\n  Args\n    actions: list of strings. Actions to load\n    data_dir: string. Directory where the data can be loaded from\n    rcams: dictionary with camera parameters\n  Returns\n    train_set: dictionary with projected 2d poses for training\n    test_set: dictionary with projected 2d poses for testing\n    data_mean: vector with the mean of the 2d training data\n    data_std: vector with the standard deviation of the 2d training data\n    dim_to_ignore: list with the dimensions to not predict\n    dim_to_use: list with the dimensions to predict\n  """"""\n\n  # Load 3d data\n  train_set = load_data( data_dir, TRAIN_SUBJECTS, actions, dim=3 )\n  test_set  = load_data( data_dir, TEST_SUBJECTS,  actions, dim=3 )\n\n  train_set = project_to_cameras( train_set, rcams )\n  test_set  = project_to_cameras( test_set, rcams )\n\n  # Compute normalization statistics.\n  complete_train = copy.deepcopy( np.vstack( train_set.values() ))\n  data_mean, data_std, dim_to_ignore, dim_to_use = normalization_stats( complete_train, dim=2 )\n\n  # Divide every dimension independently\n  train_set = normalize_data( train_set, data_mean, data_std, dim_to_use )\n  test_set  = normalize_data( test_set,  data_mean, data_std, dim_to_use )\n\n  return train_set, test_set, data_mean, data_std, dim_to_ignore, dim_to_use\n\n\ndef read_3d_data( actions, data_dir, camera_frame, rcams, predict_14=False ):\n  """"""\n  Loads 3d poses, zero-centres and normalizes them\n\n  Args\n    actions: list of strings. Actions to load\n    data_dir: string. Directory where the data can be loaded from\n    camera_frame: boolean. Whether to convert the data to camera coordinates\n    rcams: dictionary with camera parameters\n    predict_14: boolean. Whether to predict only 14 joints\n  Returns\n    train_set: dictionary with loaded 3d poses for training\n    test_set: dictionary with loaded 3d poses for testing\n    data_mean: vector with the mean of the 3d training data\n    data_std: vector with the standard deviation of the 3d training data\n    dim_to_ignore: list with the dimensions to not predict\n    dim_to_use: list with the dimensions to predict\n    train_root_positions: dictionary with the 3d positions of the root in train\n    test_root_positions: dictionary with the 3d positions of the root in test\n  """"""\n  # Load 3d data\n  train_set = load_data( data_dir, TRAIN_SUBJECTS, actions, dim=3 )\n  test_set  = load_data( data_dir, TEST_SUBJECTS,  actions, dim=3 )\n\n  if camera_frame:\n    train_set = transform_world_to_camera( train_set, rcams )\n    test_set  = transform_world_to_camera( test_set, rcams )\n\n  # Apply 3d post-processing (centering around root)\n  train_set, train_root_positions = postprocess_3d( train_set )\n  test_set,  test_root_positions  = postprocess_3d( test_set )\n\n  # Compute normalization statistics\n  complete_train = copy.deepcopy( np.vstack( train_set.values() ))\n  data_mean, data_std, dim_to_ignore, dim_to_use = normalization_stats( complete_train, dim=3, predict_14=predict_14 )\n\n  # Divide every dimension independently\n  train_set = normalize_data( train_set, data_mean, data_std, dim_to_use )\n  test_set  = normalize_data( test_set,  data_mean, data_std, dim_to_use )\n\n  return train_set, test_set, data_mean, data_std, dim_to_ignore, dim_to_use, train_root_positions, test_root_positions\n\n\ndef postprocess_3d( poses_set ):\n  """"""\n  Center 3d points around root\n\n  Args\n    poses_set: dictionary with 3d data\n  Returns\n    poses_set: dictionary with 3d data centred around root (center hip) joint\n    root_positions: dictionary with the original 3d position of each pose\n  """"""\n  root_positions = {}\n  for k in poses_set.keys():\n    # Keep track of the global position\n    root_positions[k] = copy.deepcopy(poses_set[k][:,:3])\n\n    # Remove the root from the 3d position\n    poses = poses_set[k]\n    poses = poses - np.tile( poses[:,:3], [1, len(H36M_NAMES)] )\n    poses_set[k] = poses\n\n  return poses_set, root_positions\n'"
src/linear_model.py,46,"b'\n""""""Simple model to regress 3d human poses from 2d joint locations""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.python.ops import variable_scope as vs\n\nimport os\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\nimport data_utils\nimport cameras as cam\n\ndef kaiming(shape, dtype, partition_info=None):\n  """"""Kaiming initialization as described in https://arxiv.org/pdf/1502.01852.pdf\n\n  Args\n    shape: dimensions of the tf array to initialize\n    dtype: data type of the array\n    partition_info: (Optional) info about how the variable is partitioned.\n      See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/init_ops.py#L26\n      Needed to be used as an initializer.\n  Returns\n    Tensorflow array with initial weights\n  """"""\n  return(tf.truncated_normal(shape, dtype=dtype)*tf.sqrt(2/float(shape[0])))\n\nclass LinearModel(object):\n  """""" A simple Linear+RELU model """"""\n\n  def __init__(self,\n               linear_size,\n               num_layers,\n               residual,\n               batch_norm,\n               max_norm,\n               batch_size,\n               learning_rate,\n               summaries_dir,\n               predict_14=False,\n               dtype=tf.float32):\n    """"""Creates the linear + relu model\n\n    Args\n      linear_size: integer. number of units in each layer of the model\n      num_layers: integer. number of bilinear blocks in the model\n      residual: boolean. Whether to add residual connections\n      batch_norm: boolean. Whether to use batch normalization\n      max_norm: boolean. Whether to clip weights to a norm of 1\n      batch_size: integer. The size of the batches used during training\n      learning_rate: float. Learning rate to start with\n      summaries_dir: String. Directory where to log progress\n      predict_14: boolean. Whether to predict 14 instead of 17 joints\n      dtype: the data type to use to store internal variables\n    """"""\n\n    # There are in total 17 joints in H3.6M and 16 in MPII (and therefore in stacked\n    # hourglass detections). We settled with 16 joints in 2d just to make models\n    # compatible (e.g. you can train on ground truth 2d and test on SH detections).\n    # This does not seem to have an effect on prediction performance.\n    self.HUMAN_2D_SIZE = 16 * 2\n\n    # In 3d all the predictions are zero-centered around the root (hip) joint, so\n    # we actually predict only 16 joints. The error is still computed over 17 joints,\n    # because if one uses, e.g. Procrustes alignment, there is still error in the\n    # hip to account for!\n    # There is also an option to predict only 14 joints, which makes our results\n    # directly comparable to those in https://arxiv.org/pdf/1611.09010.pdf\n    self.HUMAN_3D_SIZE = 14 * 3 if predict_14 else 16 * 3\n\n    self.input_size  = self.HUMAN_2D_SIZE\n    self.output_size = self.HUMAN_3D_SIZE\n\n    self.isTraining = tf.placeholder(tf.bool,name=""isTrainingflag"")\n    self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n\n    # Summary writers for train and test runs\n    self.train_writer = tf.summary.FileWriter( os.path.join(summaries_dir, \'train\' ))\n    self.test_writer  = tf.summary.FileWriter( os.path.join(summaries_dir, \'test\' ))\n\n    self.linear_size   = linear_size\n    self.batch_size    = batch_size\n    self.learning_rate = tf.Variable( float(learning_rate), trainable=False, dtype=dtype, name=""learning_rate"")\n    self.global_step   = tf.Variable(0, trainable=False, name=""global_step"")\n    decay_steps = 100000  # empirical\n    decay_rate = 0.96     # empirical\n    self.learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, decay_steps, decay_rate)\n\n    # === Transform the inputs ===\n    with vs.variable_scope(""inputs""):\n\n      # in=2d poses, out=3d poses\n      enc_in  = tf.placeholder(dtype, shape=[None, self.input_size], name=""enc_in"")\n      dec_out = tf.placeholder(dtype, shape=[None, self.output_size], name=""dec_out"")\n\n      self.encoder_inputs  = enc_in\n      self.decoder_outputs = dec_out\n\n    # === Create the linear + relu combos ===\n    with vs.variable_scope( ""linear_model"" ):\n\n      # === First layer, brings dimensionality up to linear_size ===\n      w1 = tf.get_variable( name=""w1"", initializer=kaiming, shape=[self.HUMAN_2D_SIZE, linear_size], dtype=dtype )\n      b1 = tf.get_variable( name=""b1"", initializer=kaiming, shape=[linear_size], dtype=dtype )\n      w1 = tf.clip_by_norm(w1,1) if max_norm else w1\n      y3 = tf.matmul( enc_in, w1 ) + b1\n\n      if batch_norm:\n        y3 = tf.layers.batch_normalization(y3,training=self.isTraining, name=""batch_normalization"")\n      y3 = tf.nn.relu( y3 )\n      y3 = tf.nn.dropout( y3, self.dropout_keep_prob )\n\n      # === Create multiple bi-linear layers ===\n      for idx in range( num_layers ):\n        y3 = self.two_linear( y3, linear_size, residual, self.dropout_keep_prob, max_norm, batch_norm, dtype, idx )\n\n      # === Last linear layer has HUMAN_3D_SIZE in output ===\n      w4 = tf.get_variable( name=""w4"", initializer=kaiming, shape=[linear_size, self.HUMAN_3D_SIZE], dtype=dtype )\n      b4 = tf.get_variable( name=""b4"", initializer=kaiming, shape=[self.HUMAN_3D_SIZE], dtype=dtype )\n      w4 = tf.clip_by_norm(w4,1) if max_norm else w4\n      y = tf.matmul(y3, w4) + b4\n      # === End linear model ===\n\n    # Store the outputs here\n    self.outputs = y\n    self.loss = tf.reduce_mean(tf.square(y - dec_out))\n    self.loss_summary = tf.summary.scalar(\'loss/loss\', self.loss)\n\n    # To keep track of the loss in mm\n    self.err_mm = tf.placeholder( tf.float32, name=""error_mm"" )\n    self.err_mm_summary = tf.summary.scalar( ""loss/error_mm"", self.err_mm )\n\n    # Gradients and update operation for training the model.\n    opt = tf.train.AdamOptimizer( self.learning_rate )\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\n    with tf.control_dependencies(update_ops):\n\n      # Update all the trainable parameters\n      gradients = opt.compute_gradients(self.loss)\n      self.gradients = [[] if i==None else i for i in gradients]\n      self.updates = opt.apply_gradients(gradients, global_step=self.global_step)\n\n    # Keep track of the learning rate\n    self.learning_rate_summary = tf.summary.scalar(\'learning_rate/learning_rate\', self.learning_rate)\n\n    # To save the model\n    self.saver = tf.train.Saver( tf.global_variables(), max_to_keep=10 )\n\n\n  def two_linear( self, xin, linear_size, residual, dropout_keep_prob, max_norm, batch_norm, dtype, idx ):\n    """"""\n    Make a bi-linear block with optional residual connection\n\n    Args\n      xin: the batch that enters the block\n      linear_size: integer. The size of the linear units\n      residual: boolean. Whether to add a residual connection\n      dropout_keep_prob: float [0,1]. Probability of dropping something out\n      max_norm: boolean. Whether to clip weights to 1-norm\n      batch_norm: boolean. Whether to do batch normalization\n      dtype: type of the weigths. Usually tf.float32\n      idx: integer. Number of layer (for naming/scoping)\n    Returns\n      y: the batch after it leaves the block\n    """"""\n\n    with vs.variable_scope( ""two_linear_""+str(idx) ) as scope:\n\n      input_size = int(xin.get_shape()[1])\n\n      # Linear 1\n      w2 = tf.get_variable( name=""w2_""+str(idx), initializer=kaiming, shape=[input_size, linear_size], dtype=dtype)\n      b2 = tf.get_variable( name=""b2_""+str(idx), initializer=kaiming, shape=[linear_size], dtype=dtype)\n      w2 = tf.clip_by_norm(w2,1) if max_norm else w2\n      y = tf.matmul(xin, w2) + b2\n      if  batch_norm:\n        y = tf.layers.batch_normalization(y,training=self.isTraining,name=""batch_normalization1""+str(idx))\n\n      y = tf.nn.relu( y )\n      y = tf.nn.dropout( y, dropout_keep_prob )\n\n      # Linear 2\n      w3 = tf.get_variable( name=""w3_""+str(idx), initializer=kaiming, shape=[linear_size, linear_size], dtype=dtype)\n      b3 = tf.get_variable( name=""b3_""+str(idx), initializer=kaiming, shape=[linear_size], dtype=dtype)\n      w3 = tf.clip_by_norm(w3,1) if max_norm else w3\n      y = tf.matmul(y, w3) + b3\n\n      if  batch_norm:\n        y = tf.layers.batch_normalization(y,training=self.isTraining,name=""batch_normalization2""+str(idx))\n\n      y = tf.nn.relu( y )\n      y = tf.nn.dropout( y, dropout_keep_prob )\n\n      # Residual every 2 blocks\n      y = (xin + y) if residual else y\n\n    return y\n\n  def step(self, session, encoder_inputs, decoder_outputs, dropout_keep_prob, isTraining=True):\n    """"""Run a step of the model feeding the given inputs.\n\n    Args\n      session: tensorflow session to use\n      encoder_inputs: list of numpy vectors to feed as encoder inputs\n      decoder_outputs: list of numpy vectors that are the expected decoder outputs\n      dropout_keep_prob: (0,1] dropout keep probability\n      isTraining: whether to do the backward step or only forward\n\n    Returns\n      if isTraining is True, a 4-tuple\n        loss: the computed loss of this batch\n        loss_summary: tf summary of this batch loss, to log on tensorboard\n        learning_rate_summary: tf summary of learnign rate to log on tensorboard\n        outputs: predicted 3d poses\n      if isTraining is False, a 3-tuple\n        (loss, loss_summary, outputs) same as above\n    """"""\n\n    input_feed = {self.encoder_inputs: encoder_inputs,\n                  self.decoder_outputs: decoder_outputs,\n                  self.isTraining: isTraining,\n                  self.dropout_keep_prob: dropout_keep_prob}\n\n    # Output feed: depends on whether we do a backward step or not.\n    if isTraining:\n      output_feed = [self.updates,       # Update Op that does SGD\n                     self.loss,\n                     self.loss_summary,\n                     self.learning_rate_summary,\n                     self.outputs]\n\n      outputs = session.run( output_feed, input_feed )\n      return outputs[1], outputs[2], outputs[3], outputs[4]\n\n    else:\n      output_feed = [self.loss, # Loss for this batch.\n                     self.loss_summary,\n                     self.outputs]\n\n      outputs = session.run(output_feed, input_feed)\n      return outputs[0], outputs[1], outputs[2]  # No gradient norm\n\n  def get_all_batches( self, data_x, data_y, camera_frame, training=True ):\n    """"""\n    Obtain a list of all the batches, randomly permutted\n    Args\n      data_x: dictionary with 2d inputs\n      data_y: dictionary with 3d expected outputs\n      camera_frame: whether the 3d data is in camera coordinates\n      training: True if this is a training batch. False otherwise.\n\n    Returns\n      encoder_inputs: list of 2d batches\n      decoder_outputs: list of 3d batches\n    """"""\n\n    # Figure out how many frames we have\n    n = 0\n    for key2d in data_x.keys():\n      n2d, _ = data_x[ key2d ].shape\n      n = n + n2d\n\n    encoder_inputs  = np.zeros((n, self.input_size), dtype=float)\n    decoder_outputs = np.zeros((n, self.output_size), dtype=float)\n\n    # Put all the data into big arrays\n    idx = 0\n    for key2d in data_x.keys():\n      (subj, b, fname) = key2d\n      # keys should be the same if 3d is in camera coordinates\n      key3d = key2d if (camera_frame) else (subj, b, \'{0}.h5\'.format(fname.split(\'.\')[0]))\n      key3d = (subj, b, fname[:-3]) if fname.endswith(\'-sh\') and camera_frame else key3d\n\n      n2d, _ = data_x[ key2d ].shape\n      encoder_inputs[idx:idx+n2d, :]  = data_x[ key2d ]\n      decoder_outputs[idx:idx+n2d, :] = data_y[ key3d ]\n      idx = idx + n2d\n\n\n    if training:\n      # Randomly permute everything\n      idx = np.random.permutation( n )\n      encoder_inputs  = encoder_inputs[idx, :]\n      decoder_outputs = decoder_outputs[idx, :]\n\n    # Make the number of examples a multiple of the batch size\n    n_extra  = n % self.batch_size\n    if n_extra > 0:  # Otherwise examples are already a multiple of batch size\n      encoder_inputs  = encoder_inputs[:-n_extra, :]\n      decoder_outputs = decoder_outputs[:-n_extra, :]\n\n    n_batches = n // self.batch_size\n    encoder_inputs  = np.split( encoder_inputs, n_batches )\n    decoder_outputs = np.split( decoder_outputs, n_batches )\n\n    return encoder_inputs, decoder_outputs\n'"
src/predict_3dpose.py,29,"b'\n""""""Predicting 3d poses from 2d joints""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport os\nimport random\nimport sys\nimport time\nimport h5py\nimport copy\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\nimport procrustes\n\nimport viz\nimport cameras\nimport data_utils\nimport linear_model\n\ntf.app.flags.DEFINE_float(""learning_rate"", 1e-3, ""Learning rate"")\ntf.app.flags.DEFINE_float(""dropout"", 1, ""Dropout keep probability. 1 means no dropout"")\ntf.app.flags.DEFINE_integer(""batch_size"", 64, ""Batch size to use during training"")\ntf.app.flags.DEFINE_integer(""epochs"", 200, ""How many epochs we should train for"")\ntf.app.flags.DEFINE_boolean(""camera_frame"", False, ""Convert 3d poses to camera coordinates"")\ntf.app.flags.DEFINE_boolean(""max_norm"", False, ""Apply maxnorm constraint to the weights"")\ntf.app.flags.DEFINE_boolean(""batch_norm"", False, ""Use batch_normalization"")\n\n# Data loading\ntf.app.flags.DEFINE_boolean(""predict_14"", False, ""predict 14 joints"")\ntf.app.flags.DEFINE_boolean(""use_sh"", False, ""Use 2d pose predictions from StackedHourglass"")\ntf.app.flags.DEFINE_string(""action"",""All"", ""The action to train on. \'All\' means all the actions"")\n\n# Architecture\ntf.app.flags.DEFINE_integer(""linear_size"", 1024, ""Size of each model layer."")\ntf.app.flags.DEFINE_integer(""num_layers"", 2, ""Number of layers in the model."")\ntf.app.flags.DEFINE_boolean(""residual"", False, ""Whether to add a residual connection every 2 layers"")\n\n# Evaluation\ntf.app.flags.DEFINE_boolean(""procrustes"", False, ""Apply procrustes analysis at test time"")\ntf.app.flags.DEFINE_boolean(""evaluateActionWise"",False, ""The dataset to use either h36m or heva"")\n\n# Directories\ntf.app.flags.DEFINE_string(""cameras_path"",""data/h36m/cameras.h5"",""Directory to load camera parameters"")\ntf.app.flags.DEFINE_string(""data_dir"",   ""data/h36m/"", ""Data directory"")\ntf.app.flags.DEFINE_string(""train_dir"", ""experiments"", ""Training directory."")\n\n# Train or load\ntf.app.flags.DEFINE_boolean(""sample"", False, ""Set to True for sampling."")\ntf.app.flags.DEFINE_boolean(""use_cpu"", False, ""Whether to use the CPU"")\ntf.app.flags.DEFINE_integer(""load"", 0, ""Try to load a previous checkpoint."")\n\n# Misc\ntf.app.flags.DEFINE_boolean(""use_fp16"", False, ""Train using fp16 instead of fp32."")\n\nFLAGS = tf.app.flags.FLAGS\n\ntrain_dir = os.path.join( FLAGS.train_dir,\n  FLAGS.action,\n  \'dropout_{0}\'.format(FLAGS.dropout),\n  \'epochs_{0}\'.format(FLAGS.epochs) if FLAGS.epochs > 0 else \'\',\n  \'lr_{0}\'.format(FLAGS.learning_rate),\n  \'residual\' if FLAGS.residual else \'not_residual\',\n  \'depth_{0}\'.format(FLAGS.num_layers),\n  \'linear_size{0}\'.format(FLAGS.linear_size),\n  \'batch_size_{0}\'.format(FLAGS.batch_size),\n  \'procrustes\' if FLAGS.procrustes else \'no_procrustes\',\n  \'maxnorm\' if FLAGS.max_norm else \'no_maxnorm\',\n  \'batch_normalization\' if FLAGS.batch_norm else \'no_batch_normalization\',\n  \'use_stacked_hourglass\' if FLAGS.use_sh else \'not_stacked_hourglass\',\n  \'predict_14\' if FLAGS.predict_14 else \'predict_17\')\n\nprint( train_dir )\nsummaries_dir = os.path.join( train_dir, ""log"" ) # Directory for TB summaries\n\n# To avoid race conditions: https://github.com/tensorflow/tensorflow/issues/7448\nos.system(\'mkdir -p {}\'.format(summaries_dir))\n\ndef create_model( session, actions, batch_size ):\n  """"""\n  Create model and initialize it or load its parameters in a session\n\n  Args\n    session: tensorflow session\n    actions: list of string. Actions to train/test on\n    batch_size: integer. Number of examples in each batch\n  Returns\n    model: The created (or loaded) model\n  Raises\n    ValueError if asked to load a model, but the checkpoint specified by\n    FLAGS.load cannot be found.\n  """"""\n\n  model = linear_model.LinearModel(\n      FLAGS.linear_size,\n      FLAGS.num_layers,\n      FLAGS.residual,\n      FLAGS.batch_norm,\n      FLAGS.max_norm,\n      batch_size,\n      FLAGS.learning_rate,\n      summaries_dir,\n      FLAGS.predict_14,\n      dtype=tf.float16 if FLAGS.use_fp16 else tf.float32)\n\n  if FLAGS.load <= 0:\n    # Create a new model from scratch\n    print(""Creating model with fresh parameters."")\n    session.run( tf.global_variables_initializer() )\n    return model\n\n  # Load a previously saved model\n  ckpt = tf.train.get_checkpoint_state( train_dir, latest_filename=""checkpoint"")\n  print( ""train_dir"", train_dir )\n\n  if ckpt and ckpt.model_checkpoint_path:\n    # Check if the specific checkpoint exists\n    if FLAGS.load > 0:\n      if os.path.isfile(os.path.join(train_dir,""checkpoint-{0}.index"".format(FLAGS.load))):\n        ckpt_name = os.path.join( os.path.join(train_dir,""checkpoint-{0}"".format(FLAGS.load)) )\n      else:\n        raise ValueError(""Asked to load checkpoint {0}, but it does not seem to exist"".format(FLAGS.load))\n    else:\n      ckpt_name = os.path.basename( ckpt.model_checkpoint_path )\n\n    print(""Loading model {0}"".format( ckpt_name ))\n    model.saver.restore( session, ckpt.model_checkpoint_path )\n    return model\n  else:\n    print(""Could not find checkpoint. Aborting."")\n    raise( ValueError, ""Checkpoint {0} does not seem to exist"".format( ckpt.model_checkpoint_path ) )\n\n  return model\n\ndef train():\n  """"""Train a linear model for 3d pose estimation""""""\n\n  actions = data_utils.define_actions( FLAGS.action )\n\n  number_of_actions = len( actions )\n\n  # Load camera parameters\n  SUBJECT_IDS = [1,5,6,7,8,9,11]\n  rcams = cameras.load_cameras(FLAGS.cameras_path, SUBJECT_IDS)\n\n  # Load 3d data and load (or create) 2d projections\n  train_set_3d, test_set_3d, data_mean_3d, data_std_3d, dim_to_ignore_3d, dim_to_use_3d, train_root_positions, test_root_positions = data_utils.read_3d_data(\n    actions, FLAGS.data_dir, FLAGS.camera_frame, rcams, FLAGS.predict_14 )\n\n  # Read stacked hourglass 2D predictions if use_sh, otherwise use groundtruth 2D projections\n  if FLAGS.use_sh:\n    train_set_2d, test_set_2d, data_mean_2d, data_std_2d, dim_to_ignore_2d, dim_to_use_2d = data_utils.read_2d_predictions(actions, FLAGS.data_dir)\n  else:\n    train_set_2d, test_set_2d, data_mean_2d, data_std_2d, dim_to_ignore_2d, dim_to_use_2d = data_utils.create_2d_data( actions, FLAGS.data_dir, rcams )\n  print( ""done reading and normalizing data."" )\n\n  # Avoid using the GPU if requested\n  device_count = {""GPU"": 0} if FLAGS.use_cpu else {""GPU"": 1}\n  with tf.Session(config=tf.ConfigProto(\n    device_count=device_count,\n    allow_soft_placement=True )) as sess:\n\n    # === Create the model ===\n    print(""Creating %d bi-layers of %d units."" % (FLAGS.num_layers, FLAGS.linear_size))\n    model = create_model( sess, actions, FLAGS.batch_size )\n    model.train_writer.add_graph( sess.graph )\n    print(""Model created"")\n\n    #=== This is the training loop ===\n    step_time, loss, val_loss = 0.0, 0.0, 0.0\n    current_step = 0 if FLAGS.load <= 0 else FLAGS.load + 1\n    previous_losses = []\n\n    step_time, loss = 0, 0\n    current_epoch = 0\n    log_every_n_batches = 100\n\n    for _ in xrange( FLAGS.epochs ):\n      current_epoch = current_epoch + 1\n\n      # === Load training batches for one epoch ===\n      encoder_inputs, decoder_outputs = model.get_all_batches( train_set_2d, train_set_3d, FLAGS.camera_frame, training=True )\n      nbatches = len( encoder_inputs )\n      print(""There are {0} train batches"".format( nbatches ))\n      start_time, loss = time.time(), 0.\n\n      # === Loop through all the training batches ===\n      for i in range( nbatches ):\n\n        if (i+1) % log_every_n_batches == 0:\n          # Print progress every log_every_n_batches batches\n          print(""Working on epoch {0}, batch {1} / {2}... "".format( current_epoch, i+1, nbatches), end="""" )\n\n        enc_in, dec_out = encoder_inputs[i], decoder_outputs[i]\n        step_loss, loss_summary, lr_summary, _ =  model.step( sess, enc_in, dec_out, FLAGS.dropout, isTraining=True )\n\n        if (i+1) % log_every_n_batches == 0:\n          # Log and print progress every log_every_n_batches batches\n          model.train_writer.add_summary( loss_summary, current_step )\n          model.train_writer.add_summary( lr_summary, current_step )\n          step_time = (time.time() - start_time)\n          start_time = time.time()\n          print(""done in {0:.2f} ms"".format( 1000*step_time / log_every_n_batches ) )\n\n        loss += step_loss\n        current_step += 1\n        # === end looping through training batches ===\n\n      loss = loss / nbatches\n      print(""=============================\\n""\n            ""Global step:         %d\\n""\n            ""Learning rate:       %.2e\\n""\n            ""Train loss avg:      %.4f\\n""\n            ""============================="" % (model.global_step.eval(),\n            model.learning_rate.eval(), loss) )\n      # === End training for an epoch ===\n\n      # === Testing after this epoch ===\n      isTraining = False\n\n      if FLAGS.evaluateActionWise:\n\n        print(""{0:=^12} {1:=^6}"".format(""Action"", ""mm"")) # line of 30 equal signs\n\n        cum_err = 0\n        for action in actions:\n\n          print(""{0:<12} "".format(action), end="""")\n          # Get 2d and 3d testing data for this action\n          action_test_set_2d = get_action_subset( test_set_2d, action )\n          action_test_set_3d = get_action_subset( test_set_3d, action )\n          encoder_inputs, decoder_outputs = model.get_all_batches( action_test_set_2d, action_test_set_3d, FLAGS.camera_frame, training=False)\n\n          act_err, _, step_time, loss = evaluate_batches( sess, model,\n            data_mean_3d, data_std_3d, dim_to_use_3d, dim_to_ignore_3d,\n            data_mean_2d, data_std_2d, dim_to_use_2d, dim_to_ignore_2d,\n            current_step, encoder_inputs, decoder_outputs )\n          cum_err = cum_err + act_err\n\n          print(""{0:>6.2f}"".format(act_err))\n\n        summaries = sess.run( model.err_mm_summary, {model.err_mm: float(cum_err/float(len(actions)))} )\n        model.test_writer.add_summary( summaries, current_step )\n        print(""{0:<12} {1:>6.2f}"".format(""Average"", cum_err/float(len(actions) )))\n        print(""{0:=^19}"".format(\'\'))\n\n      else:\n\n        n_joints = 17 if not(FLAGS.predict_14) else 14\n        encoder_inputs, decoder_outputs = model.get_all_batches( test_set_2d, test_set_3d, FLAGS.camera_frame, training=False)\n\n        total_err, joint_err, step_time, loss = evaluate_batches( sess, model,\n          data_mean_3d, data_std_3d, dim_to_use_3d, dim_to_ignore_3d,\n          data_mean_2d, data_std_2d, dim_to_use_2d, dim_to_ignore_2d,\n          current_step, encoder_inputs, decoder_outputs, current_epoch )\n\n        print(""=============================\\n""\n              ""Step-time (ms):      %.4f\\n""\n              ""Val loss avg:        %.4f\\n""\n              ""Val error avg (mm):  %.2f\\n""\n              ""============================="" % ( 1000*step_time, loss, total_err ))\n\n        for i in range(n_joints):\n          # 6 spaces, right-aligned, 5 decimal places\n          print(""Error in joint {0:02d} (mm): {1:>5.2f}"".format(i+1, joint_err[i]))\n        print(""============================="")\n\n        # Log the error to tensorboard\n        summaries = sess.run( model.err_mm_summary, {model.err_mm: total_err} )\n        model.test_writer.add_summary( summaries, current_step )\n\n      # Save the model\n      print( ""Saving the model... "", end="""" )\n      start_time = time.time()\n      model.saver.save(sess, os.path.join(train_dir, \'checkpoint\'), global_step=current_step )\n      print( ""done in {0:.2f} ms"".format(1000*(time.time() - start_time)) )\n\n      # Reset global time and loss\n      step_time, loss = 0, 0\n\n      sys.stdout.flush()\n\n\ndef get_action_subset( poses_set, action ):\n  """"""\n  Given a preloaded dictionary of poses, load the subset of a particular action\n\n  Args\n    poses_set: dictionary with keys k=(subject, action, seqname),\n      values v=(nxd matrix of poses)\n    action: string. The action that we want to filter out\n  Returns\n    poses_subset: dictionary with same structure as poses_set, but only with the\n      specified action.\n  """"""\n  return {k:v for k, v in poses_set.items() if k[1] == action}\n\n\ndef evaluate_batches( sess, model,\n  data_mean_3d, data_std_3d, dim_to_use_3d, dim_to_ignore_3d,\n  data_mean_2d, data_std_2d, dim_to_use_2d, dim_to_ignore_2d,\n  current_step, encoder_inputs, decoder_outputs, current_epoch=0 ):\n  """"""\n  Generic method that evaluates performance of a list of batches.\n  May be used to evaluate all actions or a single action.\n\n  Args\n    sess\n    model\n    data_mean_3d\n    data_std_3d\n    dim_to_use_3d\n    dim_to_ignore_3d\n    data_mean_2d\n    data_std_2d\n    dim_to_use_2d\n    dim_to_ignore_2d\n    current_step\n    encoder_inputs\n    decoder_outputs\n    current_epoch\n  Returns\n\n    total_err\n    joint_err\n    step_time\n    loss\n  """"""\n\n  n_joints = 17 if not(FLAGS.predict_14) else 14\n  nbatches = len( encoder_inputs )\n\n  # Loop through test examples\n  all_dists, start_time, loss = [], time.time(), 0.\n  log_every_n_batches = 100\n  for i in range(nbatches):\n\n    if current_epoch > 0 and (i+1) % log_every_n_batches == 0:\n      print(""Working on test epoch {0}, batch {1} / {2}"".format( current_epoch, i+1, nbatches) )\n\n    enc_in, dec_out = encoder_inputs[i], decoder_outputs[i]\n    dp = 1.0 # dropout keep probability is always 1 at test time\n    step_loss, loss_summary, poses3d = model.step( sess, enc_in, dec_out, dp, isTraining=False )\n    loss += step_loss\n\n    # denormalize\n    enc_in  = data_utils.unNormalizeData( enc_in,  data_mean_2d, data_std_2d, dim_to_ignore_2d )\n    dec_out = data_utils.unNormalizeData( dec_out, data_mean_3d, data_std_3d, dim_to_ignore_3d )\n    poses3d = data_utils.unNormalizeData( poses3d, data_mean_3d, data_std_3d, dim_to_ignore_3d )\n\n    # Keep only the relevant dimensions\n    dtu3d = np.hstack( (np.arange(3), dim_to_use_3d) ) if not(FLAGS.predict_14) else  dim_to_use_3d\n\n    dec_out = dec_out[:, dtu3d]\n    poses3d = poses3d[:, dtu3d]\n\n    assert dec_out.shape[0] == FLAGS.batch_size\n    assert poses3d.shape[0] == FLAGS.batch_size\n\n    if FLAGS.procrustes:\n      # Apply per-frame procrustes alignment if asked to do so\n      for j in range(FLAGS.batch_size):\n        gt  = np.reshape(dec_out[j,:],[-1,3])\n        out = np.reshape(poses3d[j,:],[-1,3])\n        _, Z, T, b, c = procrustes.compute_similarity_transform(gt,out,compute_optimal_scale=True)\n        out = (b*out.dot(T))+c\n\n        poses3d[j,:] = np.reshape(out,[-1,17*3] ) if not(FLAGS.predict_14) else np.reshape(out,[-1,14*3] )\n\n    # Compute Euclidean distance error per joint\n    sqerr = (poses3d - dec_out)**2 # Squared error between prediction and expected output\n    dists = np.zeros( (sqerr.shape[0], n_joints) ) # Array with L2 error per joint in mm\n    dist_idx = 0\n    for k in np.arange(0, n_joints*3, 3):\n      # Sum across X,Y, and Z dimenstions to obtain L2 distance\n      dists[:,dist_idx] = np.sqrt( np.sum( sqerr[:, k:k+3], axis=1 ))\n      dist_idx = dist_idx + 1\n\n    all_dists.append(dists)\n    assert sqerr.shape[0] == FLAGS.batch_size\n\n  step_time = (time.time() - start_time) / nbatches\n  loss      = loss / nbatches\n\n  all_dists = np.vstack( all_dists )\n\n  # Error per joint and total for all passed batches\n  joint_err = np.mean( all_dists, axis=0 )\n  total_err = np.mean( all_dists )\n\n  return total_err, joint_err, step_time, loss\n\n\ndef sample():\n  """"""Get samples from a model and visualize them""""""\n\n  actions = data_utils.define_actions( FLAGS.action )\n\n  # Load camera parameters\n  SUBJECT_IDS = [1,5,6,7,8,9,11]\n  rcams = cameras.load_cameras(FLAGS.cameras_path, SUBJECT_IDS)\n\n  # Load 3d data and load (or create) 2d projections\n  train_set_3d, test_set_3d, data_mean_3d, data_std_3d, dim_to_ignore_3d, dim_to_use_3d, train_root_positions, test_root_positions = data_utils.read_3d_data(\n    actions, FLAGS.data_dir, FLAGS.camera_frame, rcams, FLAGS.predict_14 )\n\n  if FLAGS.use_sh:\n    train_set_2d, test_set_2d, data_mean_2d, data_std_2d, dim_to_ignore_2d, dim_to_use_2d = data_utils.read_2d_predictions(actions, FLAGS.data_dir)\n  else:\n    train_set_2d, test_set_2d, data_mean_2d, data_std_2d, dim_to_ignore_2d, dim_to_use_2d = data_utils.create_2d_data( actions, FLAGS.data_dir, rcams )\n  print( ""done reading and normalizing data."" )\n\n  device_count = {""GPU"": 0} if FLAGS.use_cpu else {""GPU"": 1}\n  with tf.Session(config=tf.ConfigProto( device_count = device_count )) as sess:\n    # === Create the model ===\n    print(""Creating %d layers of %d units."" % (FLAGS.num_layers, FLAGS.linear_size))\n    batch_size = 128\n    model = create_model(sess, actions, batch_size)\n    print(""Model loaded"")\n\n    for key2d in test_set_2d.keys():\n\n      (subj, b, fname) = key2d\n      print( ""Subject: {}, action: {}, fname: {}"".format(subj, b, fname) )\n\n      # keys should be the same if 3d is in camera coordinates\n      key3d = key2d if FLAGS.camera_frame else (subj, b, \'{0}.h5\'.format(fname.split(\'.\')[0]))\n      key3d = (subj, b, fname[:-3]) if (fname.endswith(\'-sh\')) and FLAGS.camera_frame else key3d\n\n      enc_in  = test_set_2d[ key2d ]\n      n2d, _ = enc_in.shape\n      dec_out = test_set_3d[ key3d ]\n      n3d, _ = dec_out.shape\n      assert n2d == n3d\n\n      # Split into about-same-size batches\n      enc_in   = np.array_split( enc_in,  n2d // batch_size )\n      dec_out  = np.array_split( dec_out, n3d // batch_size )\n      all_poses_3d = []\n\n      for bidx in range( len(enc_in) ):\n\n        # Dropout probability 0 (keep probability 1) for sampling\n        dp = 1.0\n        _, _, poses3d = model.step(sess, enc_in[bidx], dec_out[bidx], dp, isTraining=False)\n\n        # denormalize\n        enc_in[bidx]  = data_utils.unNormalizeData(  enc_in[bidx], data_mean_2d, data_std_2d, dim_to_ignore_2d )\n        dec_out[bidx] = data_utils.unNormalizeData( dec_out[bidx], data_mean_3d, data_std_3d, dim_to_ignore_3d )\n        poses3d = data_utils.unNormalizeData( poses3d, data_mean_3d, data_std_3d, dim_to_ignore_3d )\n        all_poses_3d.append( poses3d )\n\n      # Put all the poses together\n      enc_in, dec_out, poses3d = map( np.vstack, [enc_in, dec_out, all_poses_3d] )\n\n      # Convert back to world coordinates\n      if FLAGS.camera_frame:\n        N_CAMERAS = 4\n        N_JOINTS_H36M = 32\n\n        # Add global position back\n        dec_out = dec_out + np.tile( test_root_positions[ key3d ], [1,N_JOINTS_H36M] )\n\n        # Load the appropriate camera\n        subj, _, sname = key3d\n\n        cname = sname.split(\'.\')[1] # <-- camera name\n        scams = {(subj,c+1): rcams[(subj,c+1)] for c in range(N_CAMERAS)} # cams of this subject\n        scam_idx = [scams[(subj,c+1)][-1] for c in range(N_CAMERAS)].index( cname ) # index of camera used\n        the_cam  = scams[(subj, scam_idx+1)] # <-- the camera used\n        R, T, f, c, k, p, name = the_cam\n        assert name == cname\n\n        def cam2world_centered(data_3d_camframe):\n          data_3d_worldframe = cameras.camera_to_world_frame(data_3d_camframe.reshape((-1, 3)), R, T)\n          data_3d_worldframe = data_3d_worldframe.reshape((-1, N_JOINTS_H36M*3))\n          # subtract root translation\n          return data_3d_worldframe - np.tile( data_3d_worldframe[:,:3], (1,N_JOINTS_H36M) )\n\n        # Apply inverse rotation and translation\n        dec_out = cam2world_centered(dec_out)\n        poses3d = cam2world_centered(poses3d)\n\n  # Grab a random batch to visualize\n  enc_in, dec_out, poses3d = map( np.vstack, [enc_in, dec_out, poses3d] )\n  idx = np.random.permutation( enc_in.shape[0] )\n  enc_in, dec_out, poses3d = enc_in[idx, :], dec_out[idx, :], poses3d[idx, :]\n\n  # Visualize random samples\n  import matplotlib.gridspec as gridspec\n\n  # 1080p\t= 1,920 x 1,080\n  fig = plt.figure( figsize=(19.2, 10.8) )\n\n  gs1 = gridspec.GridSpec(5, 9) # 5 rows, 9 columns\n  gs1.update(wspace=-0.00, hspace=0.05) # set the spacing between axes.\n  plt.axis(\'off\')\n\n  subplot_idx, exidx = 1, 1\n  nsamples = 15\n  for i in np.arange( nsamples ):\n\n    # Plot 2d pose\n    ax1 = plt.subplot(gs1[subplot_idx-1])\n    p2d = enc_in[exidx,:]\n    viz.show2Dpose( p2d, ax1 )\n    ax1.invert_yaxis()\n\n    # Plot 3d gt\n    ax2 = plt.subplot(gs1[subplot_idx], projection=\'3d\')\n    p3d = dec_out[exidx,:]\n    viz.show3Dpose( p3d, ax2 )\n\n    # Plot 3d predictions\n    ax3 = plt.subplot(gs1[subplot_idx+1], projection=\'3d\')\n    p3d = poses3d[exidx,:]\n    viz.show3Dpose( p3d, ax3, lcolor=""#9b59b6"", rcolor=""#2ecc71"" )\n\n    exidx = exidx + 1\n    subplot_idx = subplot_idx + 3\n\n  plt.show()\n\ndef main(_):\n  if FLAGS.sample:\n    sample()\n  else:\n    train()\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
src/procrustes.py,0,"b'\ndef compute_similarity_transform(X, Y, compute_optimal_scale=False):\n  """"""\n  A port of MATLAB\'s `procrustes` function to Numpy.\n  Adapted from http://stackoverflow.com/a/18927641/1884420\n\n  Args\n    X: array NxM of targets, with N number of points and M point dimensionality\n    Y: array NxM of inputs\n    compute_optimal_scale: whether we compute optimal scale or force it to be 1\n\n  Returns:\n    d: squared error after transformation\n    Z: transformed Y\n    T: computed rotation\n    b: scaling\n    c: translation\n  """"""\n  import numpy as np\n\n  muX = X.mean(0)\n  muY = Y.mean(0)\n\n  X0 = X - muX\n  Y0 = Y - muY\n\n  ssX = (X0**2.).sum()\n  ssY = (Y0**2.).sum()\n\n  # centred Frobenius norm\n  normX = np.sqrt(ssX)\n  normY = np.sqrt(ssY)\n\n  # scale to equal (unit) norm\n  X0 = X0 / normX\n  Y0 = Y0 / normY\n\n  # optimum rotation matrix of Y\n  A = np.dot(X0.T, Y0)\n  U,s,Vt = np.linalg.svd(A,full_matrices=False)\n  V = Vt.T\n  T = np.dot(V, U.T)\n\n  # Make sure we have a rotation\n  detT = np.linalg.det(T)\n  V[:,-1] *= np.sign( detT )\n  s[-1]   *= np.sign( detT )\n  T = np.dot(V, U.T)\n\n  traceTA = s.sum()\n\n  if compute_optimal_scale:  # Compute optimum scaling of Y.\n    b = traceTA * normX / normY\n    d = 1 - traceTA**2\n    Z = normX*traceTA*np.dot(Y0, T) + muX\n  else:  # If no scaling allowed\n    b = 1\n    d = 1 + ssY/ssX - 2 * traceTA * normY / normX\n    Z = normY*np.dot(Y0, T) + muX\n\n  c = muX - b*np.dot(muY, T)\n\n  return d, Z, T, b, c\n'"
src/viz.py,0,"b'\n""""""Functions to visualize human poses""""""\n\nimport matplotlib.pyplot as plt\nimport data_utils\nimport numpy as np\nimport h5py\nimport os\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef show3Dpose(channels, ax, lcolor=""#3498db"", rcolor=""#e74c3c"", add_labels=False): # blue, orange\n  """"""\n  Visualize a 3d skeleton\n\n  Args\n    channels: 96x1 vector. The pose to plot.\n    ax: matplotlib 3d axis to draw on\n    lcolor: color for left part of the body\n    rcolor: color for right part of the body\n    add_labels: whether to add coordinate labels\n  Returns\n    Nothing. Draws on ax.\n  """"""\n\n  assert channels.size == len(data_utils.H36M_NAMES)*3, ""channels should have 96 entries, it has %d instead"" % channels.size\n  vals = np.reshape( channels, (len(data_utils.H36M_NAMES), -1) )\n\n  I   = np.array([1,2,3,1,7,8,1, 13,14,15,14,18,19,14,26,27])-1 # start points\n  J   = np.array([2,3,4,7,8,9,13,14,15,16,18,19,20,26,27,28])-1 # end points\n  LR  = np.array([1,1,1,0,0,0,0, 0, 0, 0, 0, 0, 0, 1, 1, 1], dtype=bool)\n\n  # Make connection matrix\n  for i in np.arange( len(I) ):\n    x, y, z = [np.array( [vals[I[i], j], vals[J[i], j]] ) for j in range(3)]\n    ax.plot(x, y, z, lw=2, c=lcolor if LR[i] else rcolor)\n\n  RADIUS = 750 # space around the subject\n  xroot, yroot, zroot = vals[0,0], vals[0,1], vals[0,2]\n  ax.set_xlim3d([-RADIUS+xroot, RADIUS+xroot])\n  ax.set_zlim3d([-RADIUS+zroot, RADIUS+zroot])\n  ax.set_ylim3d([-RADIUS+yroot, RADIUS+yroot])\n\n  if add_labels:\n    ax.set_xlabel(""x"")\n    ax.set_ylabel(""y"")\n    ax.set_zlabel(""z"")\n\n  # Get rid of the ticks and tick labels\n  ax.set_xticks([])\n  ax.set_yticks([])\n  ax.set_zticks([])\n\n  ax.get_xaxis().set_ticklabels([])\n  ax.get_yaxis().set_ticklabels([])\n  ax.set_zticklabels([])\n  ax.set_aspect(\'equal\')\n\n  # Get rid of the panes (actually, make them white)\n  white = (1.0, 1.0, 1.0, 0.0)\n  ax.w_xaxis.set_pane_color(white)\n  ax.w_yaxis.set_pane_color(white)\n  # Keep z pane\n\n  # Get rid of the lines in 3d\n  ax.w_xaxis.line.set_color(white)\n  ax.w_yaxis.line.set_color(white)\n  ax.w_zaxis.line.set_color(white)\n\ndef show2Dpose(channels, ax, lcolor=""#3498db"", rcolor=""#e74c3c"", add_labels=False):\n  """"""\n  Visualize a 2d skeleton\n\n  Args\n    channels: 64x1 vector. The pose to plot.\n    ax: matplotlib axis to draw on\n    lcolor: color for left part of the body\n    rcolor: color for right part of the body\n    add_labels: whether to add coordinate labels\n  Returns\n    Nothing. Draws on ax.\n  """"""\n\n  assert channels.size == len(data_utils.H36M_NAMES)*2, ""channels should have 64 entries, it has %d instead"" % channels.size\n  vals = np.reshape( channels, (len(data_utils.H36M_NAMES), -1) )\n\n  I  = np.array([1,2,3,1,7,8,1, 13,14,14,18,19,14,26,27])-1 # start points\n  J  = np.array([2,3,4,7,8,9,13,14,16,18,19,20,26,27,28])-1 # end points\n  LR = np.array([1,1,1,0,0,0,0, 0, 0, 0, 0, 0, 1, 1, 1], dtype=bool)\n\n  # Make connection matrix\n  for i in np.arange( len(I) ):\n    x, y = [np.array( [vals[I[i], j], vals[J[i], j]] ) for j in range(2)]\n    ax.plot(x, y, lw=2, c=lcolor if LR[i] else rcolor)\n\n  # Get rid of the ticks\n  ax.set_xticks([])\n  ax.set_yticks([])\n\n  # Get rid of tick labels\n  ax.get_xaxis().set_ticklabels([])\n  ax.get_yaxis().set_ticklabels([])\n\n  RADIUS = 350 # space around the subject\n  xroot, yroot = vals[0,0], vals[0,1]\n  ax.set_xlim([-RADIUS+xroot, RADIUS+xroot])\n  ax.set_ylim([-RADIUS+yroot, RADIUS+yroot])\n  if add_labels:\n    ax.set_xlabel(""x"")\n    ax.set_ylabel(""z"")\n\n  ax.set_aspect(\'equal\')\n'"
