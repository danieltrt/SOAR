file_path,api_count,code
tf-rex/agent.py,2,"b'from dqn import DQN\nimport numpy as np\nimport numpy.random as rnd\nimport random\nimport tensorflow as tf\n\nclass Memory:\n\n    def __init__(self, size):\n        self.size = size\n        self.mem = np.ndarray((size,5), dtype=object)\n        self.iter = 0\n        self.current_size = 0\n\n    def remember(self, state1, action, reward, state2, crashed):\n        self.mem[self.iter,:] = state1, action, reward, state2, crashed\n        self.iter = (self.iter + 1) % self.size\n        self.current_size = min(self.current_size + 1, self.size)\n\n    def sample(self, n):\n        n = min(self.current_size, n)\n        random_idx = random.sample(list(range(self.current_size)), n)\n        sample = self.mem[random_idx]\n        return (np.stack(sample[:,i], axis=0) for i in range(5))\n\n\nclass DDQNAgent:\n\n    def __init__(self, session, num_actions, width, height, path, writer=None):\n        self.path_checkpoints = path\n        self.session = session\n        self.num_actions = num_actions\n        self.memory_size = 10000\n        self.explore_prob = 1.\n        self.explore_min = 0.01\n        self.explore_decay = 0.997\n        self.batch_size = 32\n        self.discount = .95\n        self.memory = Memory(self.memory_size)\n        self.main_dqn = DQN(session, height, width, num_actions, ""main"", writer)\n        self.target_dqn = DQN(session, height, width, num_actions, ""target"", None)\n        self.session.run(tf.global_variables_initializer())\n\n        self.update_target_network()\n        self.saver = tf.train.Saver()\n\n    def act(self, state):\n        """"""\n        :return: an action and a boolean.\n        The returned boolean: - False: action generated by the DQN\n                              - True: random action (exploration)\n        """"""\n        if self.explore_prob > 0 and rnd.rand() <= self.explore_prob:\n            # explore\n            return rnd.randint(self.num_actions), True\n\n        return self.main_dqn.get_action(state), False\n\n    def remember(self, state, action, reward, state_next, crashed):\n        self.memory.remember(state, action, reward, state_next, crashed)\n\n    def replay(self, cnt):\n        if self.memory.current_size < self.batch_size:\n            return\n\n        print(""...Training..."")\n        states, actions, rewards, states_next, crashes = self.memory.sample(self.batch_size)\n        target = rewards\n        # add Q value of next state to not terminal states (i.e. not crashed)\n        target[~crashes] += self.discount * self.target_dqn.get_action_and_q(states_next[~crashes])[1]\n        self.main_dqn.train(states, actions, target, cnt)\n\n    def explore_less(self):\n        self.explore_prob = max(self.explore_min, self.explore_prob * self.explore_decay)\n\n    def update_target_network(self):\n        self.target_dqn.tranfer_variables_from(self.main_dqn)\n\n    def save(self, cnt):\n        save_path = self.saver.save(self.session, self.path_checkpoints + ""rex.ckpt"", global_step=cnt)\n        print(""Model saved in file: %s"" % save_path)\n\n    def load(self, checkpoint_name):\n        self.saver.restore(self.session, checkpoint_name)\n        print(""Model restored:"", checkpoint_name)\n'"
tf-rex/dqn.py,29,"b'import numpy as np\nimport tensorflow as tf\nimport random\nfrom functools import reduce\n\n\ndef max_pool_2x2(x, kernel_shape, name):\n    ksize = [1, *kernel_shape, 1]\n    strides = [1, *kernel_shape, 1]\n    return tf.nn.max_pool(x, ksize, strides, padding=\'SAME\', name=name)\n\n\ndef conv2d(x, output_dim, kernel_shape, stride, name):\n    stride = [1, stride[0], stride[1], 1]\n\n    with tf.variable_scope(name):\n        w = tf.Variable(tf.truncated_normal(kernel_shape, mean=0, stddev=.1), dtype=tf.float32, name=""w"")\n        conv = tf.nn.conv2d(x, w, stride, ""VALID"")\n        b = tf.Variable(tf.constant(0.1, shape=[output_dim]), name=""b"")\n        out = tf.nn.bias_add(conv, b)\n        out = tf.nn.relu(out)\n\n    return out, w, b\n\n\ndef linear(x, output_size, name, activation_fn=tf.nn.relu):\n    shape = x.get_shape().as_list()\n\n    with tf.variable_scope(name):\n        w = tf.Variable(tf.random_normal([shape[1], output_size], stddev=.02), dtype=tf.float32, name=\'w\')\n        b = tf.Variable(tf.zeros([output_size]), name=\'b\')\n        out = tf.nn.bias_add(tf.matmul(x, w), b)\n\n        if activation_fn != None:\n            out =  activation_fn(out)\n\n    return out, w, b\n\n\nclass DQN:\n\n    def __init__(self, session, height, width, num_actions, name, writer=None):\n        self.num_actions = num_actions\n        self.height = height\n        self.width = width\n        self.name = name\n        self.vars = []\n        self.session = session\n\n        self.summary_ops = []\n        self._create_network()\n        self.writer = writer\n\n\n    def get_action_and_q(self, states):\n        """"""\n        returns array:\n            array[0]: actions: is a array of length len(state) with the action with the highest score\n            array[1]: q value: is a array of length len(state) with the Q-value belonging to the action\n        """"""\n        states = states.reshape(-1, 4, self.height, self.width)\n        return self.session.run([self.a, self.Q], {self.state: states})\n\n    def get_action(self, states):\n        """"""\n        returns action(s),\n            - if states contains only a single state then we return the optimal action as an integer,\n            - if states contains an array of states then we return the optimal action for each state of the array\n        """"""\n        states = states.reshape(-1, 4, self.height, self.width)\n        num_states = states.shape[0]\n        actions = self.session.run(self.a, {self.state: states})\n        return actions[0] if num_states == 1 else actions\n\n    def train(self, states, actions, targets, cnt):\n        states = states.reshape(-1, 4, self.height, self.width)\n        feed_dict = {self.state: states, self.actions: actions, self.Q_target: targets}\n        summary,_ = self.session.run([tf.summary.merge(self.summary_ops), self.minimize], feed_dict)\n        if self.writer: self.writer.add_summary(summary, global_step=cnt)\n\n    def tranfer_variables_from(self, other):\n        """"""\n            Builds the operations required to transfer the values of the variables\n            from other to self\n        """"""\n        ops = []\n        for var_self, var_other in zip(self.vars, other.vars):\n            ops.append(var_self.assign(var_other.value()))\n\n        self.session.run(ops)\n\n\n    def _create_network(self):\n\n        with tf.variable_scope(self.name):\n            # batchsize x memory x height x width\n            self.state =  tf.placeholder(shape=[None, 4, self.height, self.width],dtype=tf.float32)\n            # batchsize x height x width x memory\n            self.state_perm = tf.transpose(self.state, perm=[0, 2, 3, 1])\n            self.summary_ops.append(tf.summary.image(""states"", self.state[:, 0, :, :][..., tf.newaxis], max_outputs=10))\n\n            conv1, w1, b1 = conv2d(self.state_perm, 32, [8, 8, 4, 32], [4, 4], ""conv1"")\n            max_pool = max_pool_2x2(conv1, [2, 2], ""maxpool"")\n            conv2, w2, b2 = conv2d(max_pool, 64, [4, 4, 32, 64], [2, 2], ""conv2"")\n            conv3, w3, b3 = conv2d(conv2, 64, [3, 3, 64, 64], [1, 1], ""conv3"")\n            self.vars += [w1, b1, w2, b2, w3, b3]\n\n            shape = conv3.get_shape().as_list()\n            conv3_flat = tf.reshape(conv3, [-1, reduce(lambda x, y: x * y, shape[1:])])\n\n            # Dueling\n            value_hid, w4, b4 = linear(conv3_flat, 512, ""value_hid"")\n            adv_hid, w5, b5 = linear(conv3_flat, 512, ""adv_hid"")\n\n            value, w6, b6 = linear(value_hid, 1, ""value"", activation_fn=None)\n            advantage, w7, b7 = linear(adv_hid, self.num_actions, ""advantage"", activation_fn=None)\n            self.vars += [w4, b4, w5, b5, w6, b6, w7, b7]\n\n            # Average Dueling\n            self.Qs = value + (advantage - tf.reduce_mean(advantage, axis=1, keepdims=True))\n\n            # action with highest Q values\n            self.a = tf.argmax(self.Qs, 1)\n            # Q value belonging to selected action\n            self.Q = tf.reduce_max(self.Qs, 1)\n            tf.summary.scalar(""Q"", self.Q)\n\n            # For training\n            self.Q_target = tf.placeholder(shape=[None], dtype=tf.float32)\n            self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n            actions_onehot = tf.one_hot(self.actions, self.num_actions, on_value=1., off_value=0., axis=1, dtype=tf.float32)\n\n            Q_tmp = tf.reduce_sum(tf.multiply(self.Qs, actions_onehot), axis=1)\n            loss = tf.reduce_mean(tf.square(self.Q_target - Q_tmp))\n            self.summary_ops.append(tf.summary.scalar(""loss"", loss))\n            optimizer = tf.train.AdamOptimizer()\n            self.minimize = optimizer.minimize(loss)\n'"
tf-rex/environment.py,0,"b'from PIL import Image\nfrom io import BytesIO\nfrom websocket_server import WebsocketServer\nimport base64, json, re, time, threading\nimport multiprocessing\nimport numpy as np\n\n\nclass Action:\n    UP = 0\n    DOWN = 1\n    FORWARD = 2\n\n\nclass Environment:\n    """"""\n    Environment class is responsible for passing the actions to the game.\n    It is also responsible for retrieving the game status and the reward.\n    """"""\n    actions = {Action.UP:\'UP\', Action.FORWARD:\'FORTH\', Action.DOWN:\'DOWN\'}\n\n    def __init__(self, host, port, debug=False):\n        self.debug = debug\n        self.queue = multiprocessing.Queue()\n        self.game_client = None\n        self.server = WebsocketServer(port, host=host)\n        self.server.set_fn_new_client(self.new_client)\n        self.server.set_fn_message_received(self.new_message)\n        print(""\\nGame can be connected (press F5 in Browser)"")\n        thread = threading.Thread(target = self.server.run_forever)\n        thread.daemon = True\n        thread.start()\n\n    def new_client(self, client, server):\n        if self.debug: print(""GameAgent: Game just connected"")\n        self.game_client = client\n        self.server.send_message(self.game_client, ""Connection to Game Agent Established"");\n\n    def new_message(self, client, server, message):\n        if self.debug: print(""GameAgent: Incoming data from game"")\n        data = json.loads(message)\n        image, crashed = data[\'world\'], data[\'crashed\']\n\n        # remove data-info at the beginning of the image\n        image = re.sub(\'data:image/png;base64,\', \'\',image)\n        # convert image from base64 decoding to np array\n        image = np.array(Image.open(BytesIO(base64.b64decode(image))))\n\n        # cast to bool\n        crashed = True if crashed in [\'True\', \'true\'] else False\n\n        self.queue.put((image, crashed))\n\n    def start_game(self):\n        """"""\n        Starts the game and lets the TRex run for half a second and then returns the initial state.\n\n        :return: the initial state of the game (np.array, reward, crashed).\n        """"""\n        # game can not be started as long as the browser is not ready\n        while self.game_client is None:\n            time.sleep(1)\n\n        self.server.send_message(self.game_client, ""START"");\n        time.sleep(4)\n        return self.get_state(Action.FORWARD)\n\n    def refresh_game(self):\n        time.sleep(0.5)\n        print(""...refreshing game..."")\n        self.server.send_message(self.game_client, ""REFRESH"");\n        time.sleep(1)\n\n    def do_action(self, action):\n        """"""\n        Performs action and returns the updated status\n\n        :param action:  Must come from the class Action.\n                        The only allowed actions are Action.UP, Action.Down and Action.FORWARD.\n        :return: return the image of the game after performing the action, the reward (after the action) and\n                        whether the TRex crashed or not.\n        """"""\n        if action != Action.FORWARD:\n            # noting needs to be send when the action is going forward\n            self.server.send_message(self.game_client, self.actions[action]);\n\n        time.sleep(.05)\n        return self.get_state(action)\n\n    def get_state(self, action):\n        self.server.send_message(self.game_client, ""STATE"");\n\n        image, crashed = self.queue.get()\n\n        if crashed:\n            reward = -100.\n        else:\n            if action == Action.UP:\n                reward = -5.\n            elif action == Action.DOWN:\n                reward = -3.\n            else:\n                reward = 1.\n\n        return image, reward, crashed\n'"
tf-rex/main.py,15,"b'from agent import DDQNAgent\nfrom environment import Environment\nfrom preprocessor import Preprocessor\nfrom functools import partial\nimport numpy as np\nimport numpy.random as rnd\nimport tensorflow as tf\nimport os\n\n## Constants\nwidth = 80\nheight = 80\nlen_epoch = int(1E8)\nnum_actions = len(Environment.actions)\n\n## Application flags\ntf.app.flags.DEFINE_string(""logdir"", ""./logs/"", ""Path to store the model and tensorboard logs or restore the model"")\ntf.app.flags.DEFINE_string(""checkpoint_nr"", None, ""Checkpoint number of the model to restore"")\ntf.app.flags.DEFINE_integer(""checkpoint_hz"", 200, ""Creating a checkpoint every x epochs"")\ntf.app.flags.DEFINE_integer(""refresh_hz"", 100, ""Reloading the browser every x epochs"")\ntf.app.flags.DEFINE_integer(""update_target_network_hz"", 20, ""Replace DQN by its next generation every x epoches"")\ntf.app.flags.DEFINE_boolean(""training"", True, ""Train a new model"")\ntf.app.flags.DEFINE_boolean(""visualize"", True, ""Visualize"")\nFLAGS = tf.app.flags.FLAGS\n\n\ndef check_path_validity():\n    """"""returns -1 if an unvalid path was given.""""""\n\n    if FLAGS.training and os.path.exists(FLAGS.logdir):\n        print(""PATH FOR STORING RESULTS ALREADY EXISTS - Results would be overwritten."")\n        return -1\n\n    elif not FLAGS.training and not os.path.exists(FLAGS.logdir):\n        print(""PATH DOES NOT EXISTS. TRAINED MODEL NOT FOUND."")\n        return -1\n\n    return 0\n\n\ndef setup_summary():\n    with tf.variable_scope(""statistics""):\n        summary_scalars = [""exploration"", ""ep_steps"", ""ep_reward""]\n        summary_placeholders, summary_ops = {}, {}\n        for tag in summary_scalars:\n            summary_placeholders[tag] = tf.placeholder(\'float32\', None)\n            summary_ops[tag]  = tf.summary.scalar(tag, summary_placeholders[tag])\n    return summary_ops, summary_placeholders\n\n\ndef summarize(session, writer, summary_ops, summary_placeholders, cnt, values):\n    ops = [summary_ops[tag] for tag in list(values.keys())]\n    feed_dict = {summary_placeholders[tag]: values[tag] for tag in list(values.keys())}\n    summary_lists = session.run(ops, feed_dict)\n    for summary in summary_lists:\n        writer.add_summary(summary, cnt)\n\n\ndef play(agent, env, preprocessor):\n    # load pretrained model,\n    # will fail if the given path doesn\'t hold a valid model\n    name = FLAGS.logdir + ""rex.ckpt""\n    if FLAGS.checkpoint_nr is not None:\n        name = name + ""-"" + str(FLAGS.checkpoint_nr)\n\n    agent.load(name)\n    agent.explore_prob = 0.0\n\n    while True:\n        frame, _, crashed = env.start_game()\n        frame = preprocessor.process(frame)\n        state = preprocessor.get_initial_state(frame)\n\n        while not crashed:\n            action, _  = agent.act(state)\n            next_frame, reward, crashed = env.do_action(action)\n            print(""action: {}"".format(env.actions[action]))\n            next_frame = preprocessor.process(next_frame)\n            next_state = preprocessor.get_updated_state(next_frame)\n\n            state = next_state\n\n        print(""Crash"")\n\ndef train(agent, env, preprocessor, summarize_function):\n    agent.update_target_network()\n\n    epoch = 0\n    while True:\n        epoch += 1\n        print(""\\nEpoch: "", epoch)\n\n        frame, _ , crashed = env.start_game()\n        frame = preprocessor.process(frame)\n        state = preprocessor.get_initial_state(frame)\n        ep_steps, ep_reward = 0, 0\n\n        while not crashed:\n\n            action, explored = agent.act(state)\n            next_frame, reward, crashed = env.do_action(action)\n            # A \'*\' is appended to the action if it was randomly chosen (i.e. not produced by the network)\n            action_str = Environment.actions[action] + ["""", ""*""][explored]\n            print(""action: {}\\t crashed: {}"".format(action_str, crashed))\n            next_frame = preprocessor.process(next_frame)\n            next_state = preprocessor.get_updated_state(next_frame)\n            agent.remember(state, action, reward, next_state, crashed)\n\n            ep_steps += 1\n            ep_reward += reward\n\n            state = next_state\n\n        agent.replay(epoch)\n        agent.explore_less()\n\n        if epoch % FLAGS.update_target_network_hz == 0:\n            agent.update_target_network()\n\n        stats = {""exploration"": agent.explore_prob, ""ep_steps"": ep_steps, ""ep_reward"": ep_reward}\n        summarize_function(epoch, stats)\n\n        if epoch % FLAGS.checkpoint_hz == 0:\n            agent.save(epoch)\n\n        if epoch % FLAGS.refresh_hz == 0:\n            env.refresh_game()\n\n\ndef main(_):\n\n    if check_path_validity() == -1:\n        exit(1)\n\n    FLAGS.logdir = FLAGS.logdir if FLAGS.logdir.endswith(\'/\') else FLAGS.logdir + \'/\'\n    # Make a new directory to store checkpoints and tensorboard summaries,\n    # this is only necessary if were are going to train a new model.\n    if FLAGS.training:\n        os.makedirs(FLAGS.logdir)\n\n    # Setup tensorflow and tensorboard writers\n    tf.reset_default_graph()\n    session = tf.Session()\n    writer = tf.summary.FileWriter(FLAGS.logdir, session.graph) if FLAGS.visualize else None\n    summary_ops, summary_placeholders = setup_summary()\n\n    # Initialize key objects: environment, agent and preprocessor\n    env = Environment(""127.0.0.1"", 9090)\n    agent = DDQNAgent(session, num_actions, width, height, FLAGS.logdir, writer)\n    preprocessor = Preprocessor(width, height)\n\n    if FLAGS.training:\n        summarize_func = partial(summarize, session, writer, summary_ops, summary_placeholders)\n        train(agent, env, preprocessor, summarize_func)\n    else:\n        play(agent, env, preprocessor)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
tf-rex/preprocessor.py,0,"b'from scipy.misc import imresize\nimport numpy as np\n\n\nclass Preprocessor:\n\n    def __init__(self, width, height):\n        self.width = width\n        self.height = height\n\n    def process(self, frame):\n        roi_height, roi_width = frame.shape[0], int(frame.shape[1] * .68)\n        processed = np.zeros((roi_height, roi_width))\n\n        roi = frame[:, :roi_width, 0]\n        all_obstacles_idx = roi > 50\n        processed[all_obstacles_idx] = 1\n        unharmful_obstacles_idx = roi > 200\n        processed[unharmful_obstacles_idx] = 0\n\n        processed = imresize(processed, (self.height, self.width, 1))\n        processed = processed / 255.0\n        return processed\n\n    def get_initial_state(self, first_frame):\n        self.state = np.array([first_frame, first_frame, first_frame, first_frame])\n        return self.state\n\n    def get_updated_state(self, next_frame):\n        self.state =  np.array([*self.state[-3:], next_frame])\n        return self.state\n'"
tf-rex/websocket_server.py,0,"b'# Author: Johan Hanssen Seferidis\n# License: MIT\n\nimport re, sys\nimport struct\nfrom base64 import b64encode\nfrom hashlib import sha1\nimport logging\n\nif sys.version_info[0] < 3 :\n\tfrom socketserver import ThreadingMixIn, TCPServer, StreamRequestHandler\nelse:\n\tfrom socketserver import ThreadingMixIn, TCPServer, StreamRequestHandler\n\n\n\n\n\'\'\'\n+-+-+-+-+-------+-+-------------+-------------------------------+\n 0                   1                   2                   3\n 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n+-+-+-+-+-------+-+-------------+-------------------------------+\n|F|R|R|R| opcode|M| Payload len |    Extended payload length    |\n|I|S|S|S|  (4)  |A|     (7)     |             (16/64)           |\n|N|V|V|V|       |S|             |   (if payload len==126/127)   |\n| |1|2|3|       |K|             |                               |\n+-+-+-+-+-------+-+-------------+ - - - - - - - - - - - - - - - +\n|     Extended payload length continued, if payload len == 127  |\n+ - - - - - - - - - - - - - - - +-------------------------------+\n|                     Payload Data continued ...                |\n+---------------------------------------------------------------+\n\'\'\'\n\nFIN    = 0x80\nOPCODE = 0x0f\nMASKED = 0x80\nPAYLOAD_LEN = 0x7f\nPAYLOAD_LEN_EXT16 = 0x7e\nPAYLOAD_LEN_EXT64 = 0x7f\n\nOPCODE_TEXT = 0x01\nCLOSE_CONN  = 0x8\n\n\n# ------------------------------ Logging -------------------------------\nlogger = logging.getLogger(__name__)\n\n# -------------------------------- API ---------------------------------\n\nclass API():\n\tdef run_forever(self):\n\t\ttry:\n\t\t\tlogger.info(""Listening on port %d for clients.."" % self.port)\n\t\t\tself.serve_forever()\n\t\texcept KeyboardInterrupt:\n\t\t\tself.server_close()\n\t\t\tlogger.info(""Server terminated."")\n\t\texcept Exception as e:\n\t\t\tlogger.error(""ERROR: WebSocketsServer: "" + str(e), exc_info=True)\n\t\t\texit(1)\n\tdef new_client(self, client, server):\n\t\tpass\n\tdef client_left(self, client, server):\n\t\tpass\n\tdef message_received(self, client, server, message):\n\t\tpass\n\tdef set_fn_new_client(self, fn):\n\t\tself.new_client=fn\n\tdef set_fn_client_left(self, fn):\n\t\tself.client_left=fn\n\tdef set_fn_message_received(self, fn):\n\t\tself.message_received=fn\n\tdef send_message(self, client, msg):\n\t\tself._unicast_(client, msg)\n\tdef send_message_to_all(self, msg):\n\t\tself._multicast_(msg)\n\n\n\n# ------------------------- Implementation -----------------------------\n\nclass WebsocketServer(ThreadingMixIn, TCPServer, API):\n\n\tallow_reuse_address = True\n\tdaemon_threads = True # comment to keep threads alive until finished\n\n\t\'\'\'\n\tclients is a list of dict:\n\t    {\n\t     \'id\'      : id,\n\t     \'handler\' : handler,\n\t     \'address\' : (addr, port)\n\t    }\n\t\'\'\'\n\tclients=[]\n\tid_counter=0\n\n\tdef __init__(self, port, host=\'127.0.0.1\'):\n\t\tself.port=port\n\t\tTCPServer.__init__(self, (host, port), WebSocketHandler)\n\n\tdef _message_received_(self, handler, msg):\n\t\tself.message_received(self.handler_to_client(handler), self, msg)\n\n\tdef _new_client_(self, handler):\n\t\tself.id_counter += 1\n\t\tclient={\n\t\t\t\'id\'      : self.id_counter,\n\t\t\t\'handler\' : handler,\n\t\t\t\'address\' : handler.client_address\n\t\t}\n\t\tself.clients.append(client)\n\t\tself.new_client(client, self)\n\n\tdef _client_left_(self, handler):\n\t\tclient=self.handler_to_client(handler)\n\t\tself.client_left(client, self)\n\t\tif client in self.clients:\n\t\t\tself.clients.remove(client)\n\n\tdef _unicast_(self, to_client, msg):\n\t\tto_client[\'handler\'].send_message(msg)\n\n\tdef _multicast_(self, msg):\n\t\tfor client in self.clients:\n\t\t\tself._unicast_(client, msg)\n\n\tdef handler_to_client(self, handler):\n\t\tfor client in self.clients:\n\t\t\tif client[\'handler\'] == handler:\n\t\t\t\treturn client\n\n\n\nclass WebSocketHandler(StreamRequestHandler):\n\n\tdef __init__(self, socket, addr, server):\n\t\tself.server=server\n\t\tStreamRequestHandler.__init__(self, socket, addr, server)\n\n\tdef setup(self):\n\t\tStreamRequestHandler.setup(self)\n\t\tself.keep_alive = True\n\t\tself.handshake_done = False\n\t\tself.valid_client = False\n\n\tdef handle(self):\n\t\twhile self.keep_alive:\n\t\t\tif not self.handshake_done:\n\t\t\t\tself.handshake()\n\t\t\telif self.valid_client:\n\t\t\t\tself.read_next_message()\n\n\tdef read_bytes(self, num):\n\t\t# python3 gives ordinal of byte directly\n\t\tbytes = self.rfile.read(num)\n\t\tif sys.version_info[0] < 3:\n\t\t\treturn list(map(ord, bytes))\n\t\telse:\n\t\t\treturn bytes\n\n\tdef read_next_message(self):\n\t\ttry:\n\t\t\tb1, b2 = self.read_bytes(2)\n\t\texcept ValueError as e:\n\t\t\tb1, b2 = 0, 0\n\n\t\tfin    = b1 & FIN\n\t\topcode = b1 & OPCODE\n\t\tmasked = b2 & MASKED\n\t\tpayload_length = b2 & PAYLOAD_LEN\n\n\t\tif not b1:\n\t\t\tlogger.info(""Client closed connection."")\n\t\t\tself.keep_alive = 0\n\t\t\treturn\n\t\tif opcode == CLOSE_CONN:\n\t\t\tlogger.info(""Client asked to close connection."")\n\t\t\tself.keep_alive = 0\n\t\t\treturn\n\t\tif not masked:\n\t\t\tlogger.info(""Client must always be masked."")\n\t\t\tself.keep_alive = 0\n\t\t\treturn\n\n\t\tif payload_length == 126:\n\t\t\tpayload_length = struct.unpack("">H"", self.rfile.read(2))[0]\n\t\telif payload_length == 127:\n\t\t\tpayload_length = struct.unpack("">Q"", self.rfile.read(8))[0]\n\n\t\tmasks = self.read_bytes(4)\n\t\tdecoded = """"\n\t\tfor char in self.read_bytes(payload_length):\n\t\t\tchar ^= masks[len(decoded) % 4]\n\t\t\tdecoded += chr(char)\n\t\tself.server._message_received_(self, decoded)\n\n\tdef send_message(self, message):\n\t\tself.send_text(message)\n\n\tdef send_text(self, message):\n\t\t\'\'\'\n\t\tNOTES\n\t\tFragmented(=continuation) messages are not being used since their usage\n\t\tis needed in very limited cases - when we don\'t know the payload length.\n\t\t\'\'\'\n\n\t\t# Validate message\n\t\tif isinstance(message, bytes):\n\t\t\tmessage = try_decode_UTF8(message) # this is slower but assures we have UTF-8\n\t\t\tif not message:\n\t\t\t\tlogger.warning(""Can\\\'t send message, message is not valid UTF-8"")\n\t\t\t\treturn False\n\t\telif isinstance(message, str) or isinstance(message, str):\n\t\t\tpass\n\t\telse:\n\t\t\tlogger.warning(\'Can\\\'t send message, message has to be a string or bytes. Given type is %s\' % type(message))\n\t\t\treturn False\n\n\t\theader  = bytearray()\n\t\tpayload = encode_to_UTF8(message)\n\t\tpayload_length = len(payload)\n\n\t\t# Normal payload\n\t\tif payload_length <= 125:\n\t\t\theader.append(FIN | OPCODE_TEXT)\n\t\t\theader.append(payload_length)\n\n\t\t# Extended payload\n\t\telif payload_length >= 126 and payload_length <= 65535:\n\t\t\theader.append(FIN | OPCODE_TEXT)\n\t\t\theader.append(PAYLOAD_LEN_EXT16)\n\t\t\theader.extend(struct.pack("">H"", payload_length))\n\n\t\t# Huge extended payload\n\t\telif payload_length < 18446744073709551616:\n\t\t\theader.append(FIN | OPCODE_TEXT)\n\t\t\theader.append(PAYLOAD_LEN_EXT64)\n\t\t\theader.extend(struct.pack("">Q"", payload_length))\n\n\t\telse:\n\t\t\traise Exception(""Message is too big. Consider breaking it into chunks."")\n\t\t\treturn\n\n\t\tself.request.send(header + payload)\n\n\tdef handshake(self):\n\t\tmessage = self.request.recv(1024).decode().strip()\n\t\tupgrade = re.search(\'\\nupgrade[\\s]*:[\\s]*websocket\', message.lower())\n\t\tif not upgrade:\n\t\t\tself.keep_alive = False\n\t\t\treturn\n\t\tkey = re.search(\'\\n[sS]ec-[wW]eb[sS]ocket-[kK]ey[\\s]*:[\\s]*(.*)\\r\\n\', message)\n\t\tif key:\n\t\t\tkey = key.group(1)\n\t\telse:\n\t\t\tlogger.warning(""Client tried to connect but was missing a key"")\n\t\t\tself.keep_alive = False\n\t\t\treturn\n\t\tresponse = self.make_handshake_response(key)\n\t\tself.handshake_done = self.request.send(response.encode())\n\t\tself.valid_client = True\n\t\tself.server._new_client_(self)\n\n\tdef make_handshake_response(self, key):\n\t\treturn \\\n\t\t  \'HTTP/1.1 101 Switching Protocols\\r\\n\'\\\n\t\t  \'Upgrade: websocket\\r\\n\'              \\\n\t\t  \'Connection: Upgrade\\r\\n\'             \\\n\t\t  \'Sec-WebSocket-Accept: %s\\r\\n\'        \\\n\t\t  \'\\r\\n\' % self.calculate_response_key(key)\n\n\tdef calculate_response_key(self, key):\n\t\tGUID = \'258EAFA5-E914-47DA-95CA-C5AB0DC85B11\'\n\t\thash = sha1(key.encode() + GUID.encode())\n\t\tresponse_key = b64encode(hash.digest()).strip()\n\t\treturn response_key.decode(\'ASCII\')\n\n\tdef finish(self):\n\t\tself.server._client_left_(self)\n\n\n\ndef encode_to_UTF8(data):\n\ttry:\n\t\treturn data.encode(\'UTF-8\')\n\texcept UnicodeEncodeError as e:\n\t\tlogger.error(""Could not encode data to UTF-8 -- %s"" % e)\n\t\treturn False\n\texcept Exception as e:\n\t\traise(e)\n\t\treturn False\n\n\n\ndef try_decode_UTF8(data):\n\ttry:\n\t\treturn data.decode(\'utf-8\')\n\texcept UnicodeDecodeError:\n\t\treturn False\n\texcept Exception as e:\n\t\traise(e)\n\n\n\n# This is only for testing purposes\nclass DummyWebsocketHandler(WebSocketHandler):\n    def __init__(self, *_):\n        pass\n'"
