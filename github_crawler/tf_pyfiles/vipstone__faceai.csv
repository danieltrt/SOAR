file_path,api_count,code
faceai/chineseText.py,0,"b'#coding=utf-8\n#\xe4\xb8\xad\xe6\x96\x87\xe4\xb9\xb1\xe7\xa0\x81\xe5\xa4\x84\xe7\x90\x86\n\nimport cv2\nimport numpy\nfrom PIL import Image, ImageDraw, ImageFont\n\n# img = cv2.imread(""img/xingye-1.png"")\n\n\ndef cv2ImgAddText(img, text, left, top, textColor=(0, 255, 0), textSize=20):\n    if (isinstance(img, numpy.ndarray)):  #\xe5\x88\xa4\xe6\x96\xad\xe6\x98\xaf\xe5\x90\xa6OpenCV\xe5\x9b\xbe\xe7\x89\x87\xe7\xb1\xbb\xe5\x9e\x8b\n        img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    draw = ImageDraw.Draw(img)\n    fontText = ImageFont.truetype(\n        ""font/simsun.ttc"", textSize, encoding=""utf-8"")\n    draw.text((left, top), text, textColor, font=fontText)\n    return cv2.cvtColor(numpy.asarray(img), cv2.COLOR_RGB2BGR)\n\n\n# img = cv2ImgAddText(img, ""\xe5\xa4\xa7\xe5\xae\xb6\xe5\xa5\xbd\xef\xbc\x8c\xe6\x88\x91\xe6\x98\xaf\xe6\x98\x9f\xe7\x88\xb7"", 140, 60, (255, 255, 0), 20)\n\n# cv2.imshow(""Image"", img)\n# cv2.waitKey(0)\n# cv2.destroyAllWindows()'"
faceai/colorize.py,0,"b'#coding=utf-8\n#\xe5\x9b\xbe\xe7\x89\x87\xe7\x9d\x80\xe8\x89\xb2\nimport keras\n# import tensorflow as tf\nfrom skimage.io import imread, imsave\nfrom skimage.color import rgb2gray, gray2rgb, rgb2lab, lab2rgb\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose\nfrom keras.preprocessing.image import img_to_array, load_img\nimport numpy as np\nfrom keras.preprocessing.image import ImageDataGenerator\nimport os\nimport cv2\n\n\ndef get_train_data(img_file):\n    image = img_to_array(load_img(img_file))\n    image_shape = image.shape\n    image = np.array(image, dtype=float)\n    x = rgb2lab(1.0 / 255 * image)[:, :, 0]\n    y = rgb2lab(1.0 / 255 * image)[:, :, 1:]\n    y /= 128\n    x = x.reshape(1, image_shape[0], image_shape[1], 1)\n    y = y.reshape(1, image_shape[0], image_shape[1], 2)\n    return x, y, image_shape\n\n\ndef build_model():\n    model = Sequential()\n    model.add(InputLayer(input_shape=(None, None, 1)))\n    model.add(Conv2D(8, (3, 3), activation=\'relu\', padding=\'same\', strides=2))\n    model.add(Conv2D(8, (3, 3), activation=\'relu\', padding=\'same\'))\n    model.add(Conv2D(16, (3, 3), activation=\'relu\', padding=\'same\'))\n    model.add(Conv2D(16, (3, 3), activation=\'relu\', padding=\'same\', strides=2))\n    model.add(Conv2D(32, (3, 3), activation=\'relu\', padding=\'same\'))\n    model.add(Conv2D(32, (3, 3), activation=\'relu\', padding=\'same\', strides=2))\n    model.add(UpSampling2D((2, 2)))\n    model.add(Conv2D(32, (3, 3), activation=\'relu\', padding=\'same\'))\n    model.add(UpSampling2D((2, 2)))\n    model.add(Conv2D(16, (3, 3), activation=\'relu\', padding=\'same\'))\n    model.add(UpSampling2D((2, 2)))\n    model.add(Conv2D(2, (3, 3), activation=\'tanh\', padding=\'same\'))\n    # model.compile(optimizer=\'rmsprop\', loss=\'mse\')\n    model.compile(optimizer=\'adam\', loss=\'mse\')\n    return model\n\n\n#\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\ndef train():\n    x, y, img_shape = get_train_data(\'./img/colorize/colorize-original.png\')\n\n    # x2, y2, img_shape2 = get_train_data(\n    #     \'./img/colorize/colorize2-original.png\')\n\n    model = build_model()\n    num_epochs = 1000  #\xe8\xae\xad\xe7\xbb\x83\xe6\xac\xa1\xe6\x95\xb0\n    batch_size = 1\n\n    model.fit(x, y, batch_size=batch_size, epochs=num_epochs)\n    # model.fit(x2, y2, batch_size=batch_size, epochs=num_epochs)\n    model.save(\'./data/simple_colorize.h5\')\n\n\n#\xe7\x9d\x80\xe8\x89\xb2\ndef colorize():\n    path = \'./img/colorize/colorize2.png\'\n    # cv2.imwrite(\'./img/colorize3.png\', cv2.imread(path, 0))\n    x, y, image_shape = get_train_data(path)\n    model = build_model()\n    model.load_weights(\'./data/simple_colorize.h5\')\n    output = model.predict(x)\n    output *= 128\n    tmp = np.zeros((200, 200, 3))\n    tmp[:, :, 0] = x[0][:, :, 0]\n    tmp[:, :, 1:] = output[0]\n    colorizePath = path.replace("".png"", ""-res.png"")\n    imsave(colorizePath, lab2rgb(tmp))\n    cv2.imshow(""I"", cv2.imread(path))\n    cv2.imshow(""II"", cv2.imread(colorizePath))\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n    # imsave(""test_image_gray.png"", rgb2gray(lab2rgb(tmp)))\n\n\nif __name__ == \'__main__\':\n    # train()\n    colorize()'"
faceai/compose.py,0,"b'#coding=utf-8\n#\xe5\xa4\xb4\xe5\x83\x8f\xe7\x89\xb9\xe6\x95\x88\xe5\x90\x88\xe6\x88\x90\nimport cv2\n\n# OpenCV\xe4\xba\xba\xe8\x84\xb8\xe8\xaf\x86\xe5\x88\xab\xe5\x88\x86\xe7\xb1\xbb\xe5\x99\xa8\nclassifier = cv2.CascadeClassifier(\n    ""C:\\Python36\\Lib\\site-packages\\opencv-master\\data\\haarcascades\\haarcascade_frontalface_default.xml""\n)\n\nimg = cv2.imread(""img/ag-3.png"")  # \xe8\xaf\xbb\xe5\x8f\x96\xe5\x9b\xbe\xe7\x89\x87\nimgCompose = cv2.imread(""img/compose/maozi-1.png"")\n\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # \xe8\xbd\xac\xe6\x8d\xa2\xe7\x81\xb0\xe8\x89\xb2\ncolor = (0, 255, 0)  # \xe5\xae\x9a\xe4\xb9\x89\xe7\xbb\x98\xe5\x88\xb6\xe9\xa2\x9c\xe8\x89\xb2\n# \xe8\xb0\x83\xe7\x94\xa8\xe8\xaf\x86\xe5\x88\xab\xe4\xba\xba\xe8\x84\xb8\nfaceRects = classifier.detectMultiScale(\n    gray, scaleFactor=1.2, minNeighbors=3, minSize=(32, 32))\nif len(faceRects):  # \xe5\xa4\xa7\xe4\xba\x8e0\xe5\x88\x99\xe6\xa3\x80\xe6\xb5\x8b\xe5\x88\xb0\xe4\xba\xba\xe8\x84\xb8\n    for faceRect in faceRects:  \n        x, y, w, h = faceRect\n        sp = imgCompose.shape\n        imgComposeSizeH = int(sp[0]/sp[1]*w)\n        if imgComposeSizeH>(y-20):\n            imgComposeSizeH=(y-20)\n        imgComposeSize = cv2.resize(imgCompose,(w, imgComposeSizeH), interpolation=cv2.INTER_NEAREST)\n        top = (y-imgComposeSizeH-20)\n        if top<=0:\n            top=0\n        rows, cols, channels = imgComposeSize.shape\n        roi = img[top:top+rows,x:x+cols]\n\n        # Now create a mask of logo and create its inverse mask also\n        img2gray = cv2.cvtColor(imgComposeSize, cv2.COLOR_RGB2GRAY)\n        ret, mask = cv2.threshold(img2gray, 10, 255, cv2.THRESH_BINARY) \n        mask_inv = cv2.bitwise_not(mask)\n\n        # Now black-out the area of logo in ROI\n        img1_bg = cv2.bitwise_and(roi, roi, mask=mask_inv)\n\n        # Take only region of logo from logo image.\n        img2_fg = cv2.bitwise_and(imgComposeSize, imgComposeSize, mask=mask)\n\n        # Put logo in ROI and modify the main image\n        dst = cv2.add(img1_bg, img2_fg)\n        img[top:top+rows, x:x+cols] = dst\n\ncv2.imshow(""image"", img) \ncv2.waitKey(0)\ncv2.destroyAllWindows()\n\n\n\n\n'"
faceai/detectionDlib.py,0,"b'#coding=utf-8\n#\xe5\x9b\xbe\xe7\x89\x87\xe6\xa3\x80\xe6\xb5\x8b - Dlib\xe7\x89\x88\xe6\x9c\xac\nimport cv2\nimport dlib\n\npath = ""img/ag.png""\nimg = cv2.imread(path)\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n#\xe4\xba\xba\xe8\x84\xb8\xe5\x88\x86\xe7\xb1\xbb\xe5\x99\xa8\ndetector = dlib.get_frontal_face_detector()\n# \xe8\x8e\xb7\xe5\x8f\x96\xe4\xba\xba\xe8\x84\xb8\xe6\xa3\x80\xe6\xb5\x8b\xe5\x99\xa8\npredictor = dlib.shape_predictor(\n    ""C:\\\\Python36\\\\Lib\\\\site-packages\\\\dlib-data\\\\shape_predictor_68_face_landmarks.dat""\n)\n\ndets = detector(gray, 1)\nfor face in dets:\n    # \xe5\x9c\xa8\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\xad\xe6\xa0\x87\xe6\xb3\xa8\xe4\xba\xba\xe8\x84\xb8\xef\xbc\x8c\xe5\xb9\xb6\xe6\x98\xbe\xe7\xa4\xba\n    # left = face.left()\n    # top = face.top()\n    # right = face.right()\n    # bottom = face.bottom()\n    # cv2.rectangle(img, (left, top), (right, bottom), (0, 255, 0), 2)\n    # cv2.imshow(""image"", img)\n\n    shape = predictor(img, face)  # \xe5\xaf\xbb\xe6\x89\xbe\xe4\xba\xba\xe8\x84\xb8\xe7\x9a\x8468\xe4\xb8\xaa\xe6\xa0\x87\xe5\xae\x9a\xe7\x82\xb9\n    # \xe9\x81\x8d\xe5\x8e\x86\xe6\x89\x80\xe6\x9c\x89\xe7\x82\xb9\xef\xbc\x8c\xe6\x89\x93\xe5\x8d\xb0\xe5\x87\xba\xe5\x85\xb6\xe5\x9d\x90\xe6\xa0\x87\xef\xbc\x8c\xe5\xb9\xb6\xe5\x9c\x88\xe5\x87\xba\xe6\x9d\xa5\n    for pt in shape.parts():\n        pt_pos = (pt.x, pt.y)\n        cv2.circle(img, pt_pos, 1, (0, 255, 0), 2)\n    cv2.imshow(""image"", img)\n\ncv2.waitKey(0)\ncv2.destroyAllWindows()'"
faceai/detectionOpencv.py,0,"b'#coding=utf-8\n#\xe5\x9b\xbe\xe7\x89\x87\xe6\xa3\x80\xe6\xb5\x8b - OpenCV\xe7\x89\x88\xe6\x9c\xac\nimport cv2\nimport datetime\nimport time\n\nfilepath = ""img/xingye-1.png""\n# OpenCV\xe4\xba\xba\xe8\x84\xb8\xe8\xaf\x86\xe5\x88\xab\xe5\x88\x86\xe7\xb1\xbb\xe5\x99\xa8\nclassifier = cv2.CascadeClassifier(\n    ""C:\\Python36\\Lib\\site-packages\\opencv-master\\data\\haarcascades\\haarcascade_frontalface_default.xml""\n)\n\n# \xe7\xa8\x8b\xe5\xba\x8f\xe5\xbc\x80\xe5\xa7\x8b\xe6\x97\xb6\xe9\x97\xb4\nstartTime = datetime.datetime.now()\n\nimg = cv2.imread(filepath)  # \xe8\xaf\xbb\xe5\x8f\x96\xe5\x9b\xbe\xe7\x89\x87\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # \xe8\xbd\xac\xe6\x8d\xa2\xe7\x81\xb0\xe8\x89\xb2\ncolor = (0, 255, 0)  # \xe5\xae\x9a\xe4\xb9\x89\xe7\xbb\x98\xe5\x88\xb6\xe9\xa2\x9c\xe8\x89\xb2\n# \xe8\xb0\x83\xe7\x94\xa8\xe8\xaf\x86\xe5\x88\xab\xe4\xba\xba\xe8\x84\xb8\nfaceRects = classifier.detectMultiScale(\n    gray, scaleFactor=1.2, minNeighbors=3, minSize=(32, 32))\nif len(faceRects):  # \xe5\xa4\xa7\xe4\xba\x8e0\xe5\x88\x99\xe6\xa3\x80\xe6\xb5\x8b\xe5\x88\xb0\xe4\xba\xba\xe8\x84\xb8\n    for faceRect in faceRects:  # \xe5\x8d\x95\xe7\x8b\xac\xe6\xa1\x86\xe5\x87\xba\xe6\xaf\x8f\xe4\xb8\x80\xe5\xbc\xa0\xe4\xba\xba\xe8\x84\xb8\n        x, y, w, h = faceRect\n        # \xe6\xa1\x86\xe5\x87\xba\xe4\xba\xba\xe8\x84\xb8\n        cv2.rectangle(img, (x, y), (x + h, y + w), color, 2)\n        # \xe5\xb7\xa6\xe7\x9c\xbc\n        cv2.circle(img, (x + w // 4, y + h // 4 + 30), min(w // 8, h // 8),\n                   color)\n        #\xe5\x8f\xb3\xe7\x9c\xbc\n        cv2.circle(img, (x + 3 * w // 4, y + h // 4 + 30), min(w // 8, h // 8),\n                   color)\n        #\xe5\x98\xb4\xe5\xb7\xb4\n        cv2.rectangle(img, (x + 3 * w // 8, y + 3 * h // 4),\n                      (x + 5 * w // 8, y + 7 * h // 8), color)\n\n# \xe7\xa8\x8b\xe5\xba\x8f\xe7\xbb\x93\xe6\x9d\x9f\xe6\x97\xb6\xe9\x97\xb4\nendTime = datetime.datetime.now()\nprint((endTime - startTime))\ncv2.imshow(""image"", img)  # \xe6\x98\xbe\xe7\xa4\xba\xe5\x9b\xbe\xe5\x83\x8f\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n'"
faceai/emotion.py,0,"b'#coding=utf-8\n#\xe8\xa1\xa8\xe6\x83\x85\xe8\xaf\x86\xe5\x88\xab\n\nimport cv2\nfrom keras.models import load_model\nimport numpy as np\nimport chineseText\nimport datetime\n\nstartTime = datetime.datetime.now()\nemotion_classifier = load_model(\n    \'classifier/emotion_models/simple_CNN.530-0.65.hdf5\')\nendTime = datetime.datetime.now()\nprint(endTime - startTime)\n\nemotion_labels = {\n    0: \'\xe7\x94\x9f\xe6\xb0\x94\',\n    1: \'\xe5\x8e\x8c\xe6\x81\xb6\',\n    2: \'\xe6\x81\x90\xe6\x83\xa7\',\n    3: \'\xe5\xbc\x80\xe5\xbf\x83\',\n    4: \'\xe9\x9a\xbe\xe8\xbf\x87\',\n    5: \'\xe6\x83\x8a\xe5\x96\x9c\',\n    6: \'\xe5\xb9\xb3\xe9\x9d\x99\'\n}\n\nimg = cv2.imread(""img/emotion/emotion.png"")\nface_classifier = cv2.CascadeClassifier(\n    ""C:\\Python36\\Lib\\site-packages\\opencv-master\\data\\haarcascades\\haarcascade_frontalface_default.xml""\n)\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nfaces = face_classifier.detectMultiScale(\n    gray, scaleFactor=1.2, minNeighbors=3, minSize=(40, 40))\ncolor = (255, 0, 0)\n\nfor (x, y, w, h) in faces:\n    gray_face = gray[(y):(y + h), (x):(x + w)]\n    gray_face = cv2.resize(gray_face, (48, 48))\n    gray_face = gray_face / 255.0\n    gray_face = np.expand_dims(gray_face, 0)\n    gray_face = np.expand_dims(gray_face, -1)\n    emotion_label_arg = np.argmax(emotion_classifier.predict(gray_face))\n    emotion = emotion_labels[emotion_label_arg]\n    cv2.rectangle(img, (x + 10, y + 10), (x + h - 10, y + w - 10),\n                  (255, 255, 255), 2)\n    img = chineseText.cv2ImgAddText(img, emotion, x + h * 0.3, y, color, 20)\n\ncv2.imshow(""Image"", img)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n'"
faceai/eye.py,0,"b'#coding=utf-8\n# 38x-37x 44x-43x\n# 40x-39x 46x-45x\n\nimport cv2\nimport dlib\nimport numpy as np\nimport time\n\n# img = cv2.imread(path)\n\n# gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# detector = dlib.get_frontal_face_detector()\n# predictor = dlib.shape_predictor(\n#     ""C:\\\\Python36\\\\Lib\\\\site-packages\\\\dlib-data\\\\shape_predictor_68_face_landmarks.dat""\n# )\n\n# dets = detector(gray, 1)\n# for face in dets:\n#     shape = predictor(img, face)\n\n#     leftDiffer1 = shape.parts()[37].x - shape.parts()[36].x\n#     leftDiffer2 = shape.parts()[39].x - shape.parts()[38].x\n\n#     print(""leftDiffer1:{} leftDiffer2:{} "".format(leftDiffer1, leftDiffer2))\n\ncounter = 1\n\n\n#\xe8\x8e\xb7\xe5\x8f\x96\xe7\x9c\xbc\xe7\x90\x83\xe4\xb8\xad\xe5\xbf\x83\ndef houghCircles(path, counter):\n    img = cv2.imread(path, 0)\n    # img = cv2.medianBlur(img, 5)\n\n    x = cv2.Sobel(img, -1, 1, 0, ksize=3)\n    y = cv2.Sobel(img, -1, 0, 1, ksize=3)\n    absx = cv2.convertScaleAbs(x)\n    absy = cv2.convertScaleAbs(y)\n    img = cv2.addWeighted(absx, 0.5, absy, 0.5, 0)\n\n    # ycrcb = cv2.cvtColor(img, cv2.COLOR_BGR2YCR_CB)\n    # channels = cv2.split(ycrcb)\n    # cv2.equalizeHist(channels[0], channels[0])  #\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe3\x80\x81\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe7\x9f\xa9\xe9\x98\xb5\n    # cv2.merge(channels, ycrcb)  #\xe5\x90\x88\xe5\xb9\xb6\xe7\xbb\x93\xe6\x9e\x9c\xe9\x80\x9a\xe9\x81\x93\n    # cv2.cvtColor(ycrcb, cv2.COLOR_YCR_CB2BGR, img)\n\n    # img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    cimg = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n\n    # cv2.imshow(""img2"", img)\n    # cv2.imshow(""grayimg"", grayimg)\n\n    circles = cv2.HoughCircles(\n        img,\n        cv2.HOUGH_GRADIENT,\n        1,\n        50,\n        param1=50,\n        param2=10,\n        minRadius=2,\n        maxRadius=0)\n\n    circles = np.uint16(np.around(circles))\n    for i in circles[0, :]:\n        # draw the outer circle\n        # cv2.circle(cimg, (i[0], i[1]), i[2], (0, 255, 0), 1)\n        # draw the center of the circle\n        cv2.circle(cimg, (i[0], i[1]), 2, (0, 0, 255), 2)\n    # cv2.imshow(""img"" + str(counter), cimg)\n    return (i[0] + 3, i[1] + 3)\n\n\n#\xe5\xbd\xa9\xe8\x89\xb2\xe7\x9b\xb4\xe6\x96\xb9\xe5\x9b\xbe\xe5\x9d\x87\xe8\xa1\xa1\xe5\x8c\x96\ndef hist(img):\n    ycrcb = cv2.cvtColor(img, cv2.COLOR_BGR2YCR_CB)\n    channels = cv2.split(ycrcb)\n    cv2.equalizeHist(channels[0], channels[0])  #\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe3\x80\x81\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe7\x9f\xa9\xe9\x98\xb5\n    cv2.merge(channels, ycrcb)  #\xe5\x90\x88\xe5\xb9\xb6\xe7\xbb\x93\xe6\x9e\x9c\xe9\x80\x9a\xe9\x81\x93\n    cv2.cvtColor(ycrcb, cv2.COLOR_YCR_CB2BGR, img)\n    return img\n\n\nclassifier = cv2.CascadeClassifier(\n    ""C:\\Python36\\Lib\\site-packages\\opencv-master\\data\\haarcascades\\haarcascade_eye.xml""  #haarcascade_eye_tree_eyeglasses\n)\n\n\ndef discern(img, counter):\n    grayImg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    color = (0, 255, 0)\n    faceRects = classifier.detectMultiScale(\n        grayImg, scaleFactor=1.2, minNeighbors=3, minSize=(58, 58))\n    if len(faceRects):\n        for faceRect in faceRects:\n            x, y, w, h = faceRect\n            rightEyeImg = img[(y):(y + h), (x):(x + w)]\n            # cv2.rectangle(img, (x, y), (x + h, y + w), color, 2)\n            rightEyeImg = cv2.GaussianBlur(rightEyeImg, (5, 5), 1)\n            # rightEyeImg = hist(rightEyeImg)\n            cv2.imwrite(""img/temp.png"", rightEyeImg)\n            # cv2.imwrite(""img/temp.png"", rightEyeImg)\n            circleCenter = houghCircles(""img/temp.png"", counter)  #(x,y)\n            cv2.circle(img, (x + circleCenter[0], y + circleCenter[1]), 2,\n                       (128, 0, 0), 2)\n            counter += 1\n        cv2.imshow(""image"", img)\n\n\n# path = ""img/ag-3.png""\n# img = cv2.imread(path)\n# gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n# faceRects = classifier.detectMultiScale(\n#     gray, scaleFactor=1.2, minNeighbors=3, minSize=(32, 32))\n# if len(faceRects):\n#     for faceRect in faceRects:\n#         x, y, w, h = faceRect\n#         # cv2.rectangle(img, (x, y), (x + h, y + w), (255, 0, 0), 2)\n#         rightEyeImg = img[(y):(y + h), (x):(x + w)]\n#         cv2.imwrite(""img/temp.png"", rightEyeImg)\n#         houghCircles(""img/temp.png"", counter)\n#         counter += 1\n#         # cv2.imshow(""img"", houghCircles(""img/temp.png""))\n\npath = ""img/ag.png""\nimg = cv2.imread(path)\ndiscern(img, counter)\n\n# cap = cv2.VideoCapture(0)\n# while (1):\n#     ret, frame = cap.read()\n\n#     # cv2.imshow(\'frame\', gray)\n#     discern(frame, counter)\n#     if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n#         break\n\n#\n\n# cv2.imwrite(\'img/eye-2.png\', rightEyeImg)\n\n# eyeImg = img[(y):(y + h), (x):(x + w)]\n# eyeImg = cv2.medianBlur(eyeImg, 5)\n# cimg = cv2.cvtColor(eyeImg, cv2.COLOR_GRAY2BGR)\n\n# circles = cv2.HoughCircles(\n#     eyeImg,\n#     cv2.HOUGH_GRADIENT,\n#     1,\n#     20,\n#     param1=50,\n#     param2=30,\n#     minRadius=0,\n#     maxRadius=0)\n\n# circles = np.uint16(np.around(circles))\n# for i in circles[0, :]:\n#     # draw the outer circle\n#     cv2.circle(cimg, (i[0], i[1]), i[2], (0, 255, 0), 2)\n#     # draw the center of the circle\n#     cv2.circle(cimg, (i[0], i[1]), 2, (0, 0, 255), 3)\n\n# cv2.imshow(\'detected circles\', cimg)\n\n# cv2.imshow(""image"", img)  # \xe6\x98\xbe\xe7\xa4\xba\xe5\x9b\xbe\xe5\x83\x8f\n\n# cv2.waitKey(0)\n# cv2.destroyAllWindows()\n\n# time.sleep(1)\n\n# img = cv2.imread(""img/eye-2.png"")\n# ycrcb = cv2.cvtColor(img, cv2.COLOR_BGR2YCR_CB)\n# channels = cv2.split(ycrcb)\n# cv2.equalizeHist(channels[0], channels[0])  #\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe3\x80\x81\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe7\x9f\xa9\xe9\x98\xb5\n# cv2.merge(channels, ycrcb)  #\xe5\x90\x88\xe5\xb9\xb6\xe7\xbb\x93\xe6\x9e\x9c\xe9\x80\x9a\xe9\x81\x93\n# cv2.cvtColor(ycrcb, cv2.COLOR_YCR_CB2BGR, img)\n# # cv2.imshow(""old image"", cv2.imread(""img/hist.png""))\n# # cv2.imshow(""image"", img)\n\n# cv2.imwrite(""img/eye-3.png"", img)\n\n# time.sleep(1)\n\n# cv2.imshow(\'detected circles\', cimg)\n\ncv2.waitKey(0)\ncv2.destroyAllWindows()'"
faceai/eye2.py,0,"b'import cv2\nimport dlib\nimport numpy as np\nimport time\n\nclassifier = cv2.CascadeClassifier(\n    ""C:\\Python36\\Lib\\site-packages\\opencv-master\\data\\haarcascades\\haarcascade_eye_tree_eyeglasses.xml""  # haarcascade_eye\n)\n\n\n#\xe5\xbd\xa9\xe8\x89\xb2\xe7\x9b\xb4\xe6\x96\xb9\xe5\x9b\xbe\xe5\x9d\x87\xe8\xa1\xa1\xe5\x8c\x96\ndef hist(img):\n    ycrcb = cv2.cvtColor(img, cv2.COLOR_BGR2YCR_CB)\n    channels = cv2.split(ycrcb)\n    cv2.equalizeHist(channels[0], channels[0])  #\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe3\x80\x81\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe7\x9f\xa9\xe9\x98\xb5\n    cv2.merge(channels, ycrcb)  #\xe5\x90\x88\xe5\xb9\xb6\xe7\xbb\x93\xe6\x9e\x9c\xe9\x80\x9a\xe9\x81\x93\n    cv2.cvtColor(ycrcb, cv2.COLOR_YCR_CB2BGR, img)\n    return img\n\n\ndef discern(img):\n    grayImg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    faceRects = classifier.detectMultiScale(\n        grayImg, scaleFactor=1.2, minNeighbors=3, minSize=(30, 30))\n    if len(faceRects):\n        for faceRect in faceRects:\n            x, y, w, h = faceRect\n            rightEyeImg = img[(y):(y + h), (x):(x + w)]\n            # cv2.rectangle(img, (x, y), (x + h, y + w), color, 2)\n            # rightEyeImg = hist(rightEyeImg)\n            rightEyeImg = cv2.GaussianBlur(rightEyeImg, (5, 5), 1)\n\n            cv2.imwrite(""img/temp.png"", rightEyeImg)\n            # cv2.imshow(""img"", rightEyeImg)\n    # print(len(faceRects))\n\n\n# discern(cv2.imread(""img/ag-2.png""))\n\nimg = cv2.imread(""img/temp.png"", 0)\n\n# img = cv2.GaussianBlur(img, (3, 3), 0)\n# img = cv2.Canny(img, 50, 150)\nimg = cv2.bilateralFilter(img, 7, 50, 50)\n\n# x = cv2.Sobel(img, -1, 1, 0, ksize=3)\n# y = cv2.Sobel(img, -1, 0, 1, ksize=3)\n# absx = cv2.convertScaleAbs(x)\n# absy = cv2.convertScaleAbs(y)\n# dist = cv2.addWeighted(absx, 0.5, absy, 0.5, 0)\n\n# img = cv2.GaussianBlur(img, (5, 5), 1)\n\n# laplacian = cv2.Laplacian(img, -1, ksize=3)\n\n# laplacian = cv2.GaussianBlur(laplacian, (3, 3), 1)\n# laplacian = cv2.medianBlur(laplacian, 3)\n\n# img = dist\n\n# img = cv2.cvtColor(dist, cv2.COLOR_BGR2GRAY)\n\ncimg = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)  # cv2.imread(""img/temp.png"")  #\n\n# cv2.imshow(""img2"", img)\n# cv2.imshow(""grayimg"", grayimg)\n\ncircles = cv2.HoughCircles(\n    img,\n    cv2.HOUGH_GRADIENT,\n    1,\n    100,\n    param1=50,\n    param2=10,\n    minRadius=2,\n    maxRadius=0)\n\ncircles = np.uint16(np.around(circles))\n\nfor i in circles[0, :]:\n    # draw the outer circle\n    # cv2.circle(cimg, (i[0], i[1]), i[2], (0, 255, 0), 1)\n    # draw the center of the circle\n    cv2.circle(cimg, (i[0], i[1]), 2, (0, 0, 255), 2)\ncv2.imshow(""img"", cimg)\n\ncv2.waitKey(0)\ncv2.destroyAllWindows()'"
faceai/faceRecognition.py,0,"b'#coding=utf-8\n#\xe4\xba\xba\xe8\x84\xb8\xe8\xaf\x86\xe5\x88\xab\xe7\xb1\xbb - \xe4\xbd\xbf\xe7\x94\xa8face_recognition\xe6\xa8\xa1\xe5\x9d\x97\nimport cv2\nimport face_recognition\nimport os\n\npath = ""img/face_recognition""  # \xe6\xa8\xa1\xe5\x9e\x8b\xe6\x95\xb0\xe6\x8d\xae\xe5\x9b\xbe\xe7\x89\x87\xe7\x9b\xae\xe5\xbd\x95\ncap = cv2.VideoCapture(0)\ntotal_image_name = []\ntotal_face_encoding = []\nfor fn in os.listdir(path):  #fn \xe8\xa1\xa8\xe7\xa4\xba\xe7\x9a\x84\xe6\x98\xaf\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8dq\n    print(path + ""/"" + fn)\n    total_face_encoding.append(\n        face_recognition.face_encodings(\n            face_recognition.load_image_file(path + ""/"" + fn))[0])\n    fn = fn[:(len(fn) - 4)]  #\xe6\x88\xaa\xe5\x8f\x96\xe5\x9b\xbe\xe7\x89\x87\xe5\x90\x8d\xef\xbc\x88\xe8\xbf\x99\xe9\x87\x8c\xe5\xba\x94\xe8\xaf\xa5\xe6\x8a\x8aimages\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe5\x90\x8d\xe5\x91\xbd\xe5\x90\x8d\xe4\xb8\xba\xe4\xb8\xba\xe4\xba\xba\xe7\x89\xa9\xe5\x90\x8d\xef\xbc\x89\n    total_image_name.append(fn)  #\xe5\x9b\xbe\xe7\x89\x87\xe5\x90\x8d\xe5\xad\x97\xe5\x88\x97\xe8\xa1\xa8\nwhile (1):\n    ret, frame = cap.read()\n    # \xe5\x8f\x91\xe7\x8e\xb0\xe5\x9c\xa8\xe8\xa7\x86\xe9\xa2\x91\xe5\xb8\xa7\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe8\x84\xb8\xe5\x92\x8cface_enqcodings\n    face_locations = face_recognition.face_locations(frame)\n    face_encodings = face_recognition.face_encodings(frame, face_locations)\n    # \xe5\x9c\xa8\xe8\xbf\x99\xe4\xb8\xaa\xe8\xa7\x86\xe9\xa2\x91\xe5\xb8\xa7\xe4\xb8\xad\xe5\xbe\xaa\xe7\x8e\xaf\xe9\x81\x8d\xe5\x8e\x86\xe6\xaf\x8f\xe4\xb8\xaa\xe4\xba\xba\xe8\x84\xb8\n    for (top, right, bottom, left), face_encoding in zip(\n            face_locations, face_encodings):\n        # \xe7\x9c\x8b\xe7\x9c\x8b\xe9\x9d\xa2\xe9\x83\xa8\xe6\x98\xaf\xe5\x90\xa6\xe4\xb8\x8e\xe5\xb7\xb2\xe7\x9f\xa5\xe4\xba\xba\xe8\x84\xb8\xe7\x9b\xb8\xe5\x8c\xb9\xe9\x85\x8d\xe3\x80\x82\n        for i, v in enumerate(total_face_encoding):\n            match = face_recognition.compare_faces(\n                [v], face_encoding, tolerance=0.5)\n            name = ""Unknown""\n            if match[0]:\n                name = total_image_name[i]\n                break\n        # \xe7\x94\xbb\xe5\x87\xba\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa1\x86\xef\xbc\x8c\xe6\xa1\x86\xe4\xbd\x8f\xe8\x84\xb8\n        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n        # \xe7\x94\xbb\xe5\x87\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb8\xa6\xe5\x90\x8d\xe5\xad\x97\xe7\x9a\x84\xe6\xa0\x87\xe7\xad\xbe\xef\xbc\x8c\xe6\x94\xbe\xe5\x9c\xa8\xe6\xa1\x86\xe4\xb8\x8b\n        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255),\n                      cv2.FILLED)\n        font = cv2.FONT_HERSHEY_DUPLEX\n        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0,\n                    (255, 255, 255), 1)\n    # \xe6\x98\xbe\xe7\xa4\xba\xe7\xbb\x93\xe6\x9e\x9c\xe5\x9b\xbe\xe5\x83\x8f\n    cv2.imshow(\'Video\', frame)\n    if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n'"
faceai/faceRecognitionMakeup.py,0,"b'#coding=utf-8\n#\xe6\x95\xb0\xe5\xad\x97\xe5\x8c\x96\xe5\xa6\x86\xe7\xb1\xbb\nimport face_recognition\nfrom PIL import Image, ImageDraw\n\n#\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x9b\xbe\xe7\x89\x87\xe5\x88\xb0numpy array\nimage = face_recognition.load_image_file(""img/ag.png"")\n\n#\xe6\xa0\x87\xe8\xaf\x86\xe8\x84\xb8\xe9\x83\xa8\xe7\x89\xb9\xe5\xbe\x81\nface_landmarks_list = face_recognition.face_landmarks(image)\n\nfor face_landmarks in face_landmarks_list:\n    pil_image = Image.fromarray(image)\n    d = ImageDraw.Draw(pil_image, \'RGBA\')\n\n    # \xe7\xbb\x98\xe5\x88\xb6\xe7\x9c\x89\xe6\xaf\x9b\n    d.polygon(face_landmarks[\'left_eyebrow\'], fill=(68, 54, 39, 128))\n    d.polygon(face_landmarks[\'right_eyebrow\'], fill=(68, 54, 39, 128))\n    d.line(face_landmarks[\'left_eyebrow\'], fill=(68, 54, 39, 150), width=5)\n    d.line(face_landmarks[\'right_eyebrow\'], fill=(68, 54, 39, 150), width=5)\n\n    # \xe7\xbb\x98\xe5\x88\xb6\xe5\x98\xb4\xe5\x94\x87\n    d.polygon(face_landmarks[\'top_lip\'], fill=(150, 0, 0, 128))\n    d.polygon(face_landmarks[\'bottom_lip\'], fill=(150, 0, 0, 128))\n    d.line(face_landmarks[\'top_lip\'], fill=(150, 0, 0, 64), width=8)\n    d.line(face_landmarks[\'bottom_lip\'], fill=(150, 0, 0, 64), width=8)\n\n    # \xe7\xbb\x98\xe5\x88\xb6\xe7\x9c\xbc\xe7\x9d\x9b\n    d.polygon(face_landmarks[\'left_eye\'], fill=(255, 255, 255, 30))\n    d.polygon(face_landmarks[\'right_eye\'], fill=(255, 255, 255, 30))\n\n    # \xe7\xbb\x98\xe5\x88\xb6\xe7\x9c\xbc\xe7\xba\xbf\n    d.line(\n        face_landmarks[\'left_eye\'] + [face_landmarks[\'left_eye\'][0]],\n        fill=(0, 0, 0, 110),\n        width=6)\n    d.line(\n        face_landmarks[\'right_eye\'] + [face_landmarks[\'right_eye\'][0]],\n        fill=(0, 0, 0, 110),\n        width=6)\n\n    pil_image.show()'"
faceai/faceRecognitionOutline.py,0,"b'#coding=utf-8\n#\xe7\xbb\x98\xe5\x88\xb6\xe9\x9d\xa2\xe9\x83\xa8\xe8\xbd\xae\xe5\xbb\x93\nimport face_recognition\nfrom PIL import Image, ImageDraw\n\n# \xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe6\x96\x87\xe4\xbb\xb6\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x88\xb0numpy \xe6\x95\xb0\xe7\xbb\x84\xe4\xb8\xad\nimage = face_recognition.load_image_file(""img/ag.png"")\n\n#\xe6\x9f\xa5\xe6\x89\xbe\xe5\x9b\xbe\xe5\x83\x8f\xe4\xb8\xad\xe6\x89\x80\xe6\x9c\x89\xe9\x9d\xa2\xe9\x83\xa8\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe9\x9d\xa2\xe9\x83\xa8\xe7\x89\xb9\xe5\xbe\x81\nface_landmarks_list = face_recognition.face_landmarks(image)\n\nfor face_landmarks in face_landmarks_list:\n    facial_features = [\n        \'chin\',  # \xe4\xb8\x8b\xe5\xb7\xb4\n        \'left_eyebrow\',  # \xe5\xb7\xa6\xe7\x9c\x89\xe6\xaf\x9b\n        \'right_eyebrow\',  # \xe5\x8f\xb3\xe7\x9c\x89\xe6\xaf\x9b\n        \'nose_bridge\',  # \xe9\xbc\xbb\xe6\xa8\x91\n        \'nose_tip\',  # \xe9\xbc\xbb\xe5\xb0\x96\n        \'left_eye\',  # \xe5\xb7\xa6\xe7\x9c\xbc\n        \'right_eye\',  # \xe5\x8f\xb3\xe7\x9c\xbc\n        \'top_lip\',  # \xe4\xb8\x8a\xe5\x98\xb4\xe5\x94\x87\n        \'bottom_lip\'  # \xe4\xb8\x8b\xe5\x98\xb4\xe5\x94\x87\n    ]\n    pil_image = Image.fromarray(image)\n    d = ImageDraw.Draw(pil_image)\n    for facial_feature in facial_features:\n        d.line(face_landmarks[facial_feature], fill=(255, 255, 255), width=2)\n    pil_image.show()\n'"
faceai/faceswap.py,0,"b'#coding=utf-8\nimport cv2\nimport numpy\nimport dlib\n\nmodelPath = ""C:\\Python36\\Lib\\site-packages\\dlib-data\\shape_predictor_68_face_landmarks.dat""\nSCALE_FACTOR = 1\nFEATHER_AMOUNT = 11\n\nFACE_POINTS = list(range(17, 68))\nMOUTH_POINTS = list(range(48, 61))\nRIGHT_BROW_POINTS = list(range(17, 22))\nLEFT_BROW_POINTS = list(range(22, 27))\nRIGHT_EYE_POINTS = list(range(36, 42))\nLEFT_EYE_POINTS = list(range(42, 48))\nNOSE_POINTS = list(range(27, 35))\nJAW_POINTS = list(range(0, 17))\n\nALIGN_POINTS = (LEFT_BROW_POINTS + RIGHT_EYE_POINTS + LEFT_EYE_POINTS +\n                RIGHT_BROW_POINTS + NOSE_POINTS + MOUTH_POINTS)\n\nOVERLAY_POINTS = [\n    LEFT_EYE_POINTS + RIGHT_EYE_POINTS + LEFT_BROW_POINTS + RIGHT_BROW_POINTS,\n    NOSE_POINTS + MOUTH_POINTS,\n]\n\nCOLOUR_CORRECT_BLUR_FRAC = 0.6\n\ndetector = dlib.get_frontal_face_detector()\npredictor = dlib.shape_predictor(modelPath)\n\n\nclass TooManyFaces(Exception):\n    pass\n\n\nclass NoFaces(Exception):\n    pass\n\n\ndef get_landmarks(im):\n    rects = detector(im, 1)\n\n    if len(rects) > 1:\n        raise TooManyFaces\n    if len(rects) == 0:\n        raise NoFaces\n\n    return numpy.matrix([[p.x, p.y] for p in predictor(im, rects[0]).parts()])\n\n\ndef annotate_landmarks(im, landmarks):\n    im = im.copy()\n    for idx, point in enumerate(landmarks):\n        pos = (point[0, 0], point[0, 1])\n        cv2.putText(\n            im,\n            str(idx),\n            pos,\n            fontFace=cv2.FONT_HERSHEY_SCRIPT_SIMPLEX,\n            fontScale=0.4,\n            color=(0, 0, 255))\n        cv2.circle(im, pos, 3, color=(0, 255, 255))\n    return im\n\n\ndef draw_convex_hull(im, points, color):\n    points = cv2.convexHull(points)\n    cv2.fillConvexPoly(im, points, color=color)\n\n\ndef get_face_mask(im, landmarks):\n    im = numpy.zeros(im.shape[:2], dtype=numpy.float64)\n\n    for group in OVERLAY_POINTS:\n        draw_convex_hull(im, landmarks[group], color=1)\n\n    im = numpy.array([im, im, im]).transpose((1, 2, 0))\n\n    im = (cv2.GaussianBlur(im, (FEATHER_AMOUNT, FEATHER_AMOUNT), 0) > 0) * 1.0\n    im = cv2.GaussianBlur(im, (FEATHER_AMOUNT, FEATHER_AMOUNT), 0)\n\n    return im\n\n\ndef transformation_from_points(points1, points2):\n    points1 = points1.astype(numpy.float64)\n    points2 = points2.astype(numpy.float64)\n    c1 = numpy.mean(points1, axis=0)\n    c2 = numpy.mean(points2, axis=0)\n    points1 -= c1\n    points2 -= c2\n    s1 = numpy.std(points1)\n    s2 = numpy.std(points2)\n    points1 /= s1\n    points2 /= s2\n    U, S, Vt = numpy.linalg.svd(points1.T * points2)\n    R = (U * Vt).T\n    return numpy.vstack([\n        numpy.hstack(((s2 / s1) * R, c2.T - (s2 / s1) * R * c1.T)),\n        numpy.matrix([0., 0., 1.])\n    ])\n\n\ndef read_im_and_landmarks(fname):\n    im = cv2.imread(fname, cv2.IMREAD_COLOR)\n    im = cv2.resize(im,\n                    (im.shape[1] * SCALE_FACTOR, im.shape[0] * SCALE_FACTOR))\n    s = get_landmarks(im)\n\n    return im, s\n\n\ndef warp_im(im, M, dshape):\n    output_im = numpy.zeros(dshape, dtype=im.dtype)\n    cv2.warpAffine(\n        im,\n        M[:2], (dshape[1], dshape[0]),\n        dst=output_im,\n        borderMode=cv2.BORDER_TRANSPARENT,\n        flags=cv2.WARP_INVERSE_MAP)\n    return output_im\n\n\ndef correct_colours(im1, im2, landmarks1):\n    blur_amount = COLOUR_CORRECT_BLUR_FRAC * numpy.linalg.norm(\n        numpy.mean(landmarks1[LEFT_EYE_POINTS], axis=0) -\n        numpy.mean(landmarks1[RIGHT_EYE_POINTS], axis=0))\n    blur_amount = int(blur_amount)\n    if blur_amount % 2 == 0:\n        blur_amount += 1\n    im1_blur = cv2.GaussianBlur(im1, (blur_amount, blur_amount), 0)\n    im2_blur = cv2.GaussianBlur(im2, (blur_amount, blur_amount), 0)\n\n    im2_blur += (128 * (im2_blur <= 1.0)).astype(im2_blur.dtype)\n\n    return (im2.astype(numpy.float64) * im1_blur.astype(numpy.float64) /\n            im2_blur.astype(numpy.float64))\n\n\nim1, landmarks1 = read_im_and_landmarks(""img/ag-2.png"")\nim2, landmarks2 = read_im_and_landmarks(""img/ag.png"")\n\nM = transformation_from_points(landmarks1[ALIGN_POINTS],\n                               landmarks2[ALIGN_POINTS])\n\nmask = get_face_mask(im2, landmarks2)\nwarped_mask = warp_im(mask, M, im1.shape)\ncombined_mask = numpy.max(\n    [get_face_mask(im1, landmarks1), warped_mask], axis=0)\n\nwarped_im2 = warp_im(im2, M, im1.shape)\nwarped_corrected_im2 = correct_colours(im1, warped_im2, landmarks1)\n\noutput_im = im1 * (1.0 - combined_mask) + warped_corrected_im2 * combined_mask\n\ncv2.imwrite(""img/faceswap.png"", output_im)\n\n# cv2.waitKey(0)\n# cv2.destroyAllWindows()'"
faceai/gender.py,0,"b'#coding=utf-8\n#\xe6\x80\xa7\xe5\x88\xab\xe8\xaf\x86\xe5\x88\xab\n\nimport cv2\nfrom keras.models import load_model\nimport numpy as np\nimport chineseText\n\nimg = cv2.imread(""img/gather.png"")\nface_classifier = cv2.CascadeClassifier(\n    ""C:\\Python36\\Lib\\site-packages\\opencv-master\\data\\haarcascades\\haarcascade_frontalface_default.xml""\n)\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nfaces = face_classifier.detectMultiScale(\n    gray, scaleFactor=1.2, minNeighbors=3, minSize=(140, 140))\n\ngender_classifier = load_model(\n    ""classifier/gender_models/simple_CNN.81-0.96.hdf5"")\ngender_labels = {0: \'\xe5\xa5\xb3\', 1: \'\xe7\x94\xb7\'}\ncolor = (255, 255, 255)\n\nfor (x, y, w, h) in faces:\n    face = img[(y - 60):(y + h + 60), (x - 30):(x + w + 30)]\n    face = cv2.resize(face, (48, 48))\n    face = np.expand_dims(face, 0)\n    face = face / 255.0\n    gender_label_arg = np.argmax(gender_classifier.predict(face))\n    gender = gender_labels[gender_label_arg]\n    cv2.rectangle(img, (x, y), (x + h, y + w), color, 2)\n    img = chineseText.cv2ImgAddText(img, gender, x + h, y, color, 30)\n\ncv2.imshow(""Image"", img)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n'"
faceai/grabCut.py,0,"b""#coding=utf-8\n#\xe6\x8a\xa0\xe5\x9b\xbe\n\nimport numpy as np\nimport cv2\nfrom matplotlib import pyplot as plt\n\nimg = cv2.imread('img/face_recognition/Gates.png')\nmask = np.zeros(img.shape[:2], np.uint8)\nbgdModel = np.zeros((1, 65), np.float64)\nfgdModel = np.zeros((1, 65), np.float64)\nrect = (0, 0, 505, 448)  #\xe5\x88\x92\xe5\xae\x9a\xe5\x8c\xba\xe5\x9f\x9f\ncv2.grabCut(img, mask, rect, bgdModel, fgdModel, 5,\n            cv2.GC_INIT_WITH_RECT)  #\xe5\x87\xbd\xe6\x95\xb0\xe8\xbf\x94\xe5\x9b\x9e\xe5\x80\xbc\xe4\xb8\xbamask,bgdModel,fgdModel\nmask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')  #0\xe5\x92\x8c2\xe5\x81\x9a\xe8\x83\x8c\xe6\x99\xaf\n\nimg = img * mask2[:, :, np.newaxis]  #\xe4\xbd\xbf\xe7\x94\xa8\xe8\x92\x99\xe6\x9d\xbf\xe6\x9d\xa5\xe8\x8e\xb7\xe5\x8f\x96\xe5\x89\x8d\xe6\x99\xaf\xe5\x8c\xba\xe5\x9f\x9f\n\ncv2.imshow('p', img)\n\ncv2.waitKey(0)\ncv2.destroyAllWindows()"""
faceai/tesseractOcr.py,0,"b'#coding=utf-8\n#\xe6\x96\x87\xe5\xad\x97\xe8\xaf\x86\xe5\x88\xab\xe7\xb1\xbb\nfrom PIL import Image\nimport pytesseract\nimport cv2\n\npath = ""img\\\\text-img.png""\n\ntext = pytesseract.image_to_string(Image.open(path), lang=\'chi_sim\')\nprint(text)\n\nimg = cv2.imread(path)\ncv2.imshow(""Image"", img)\n\ncv2.waitKey(0)\ncv2.destroyAllWindows()'"
faceai/test.py,0,b'#coding=utf-8\n#\xe7\xbb\x83\xe4\xb9\xa0\xe7\xb1\xbb\nimport datetime\nimport time\n\n#\xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xa1\xe6\x97\xb6\nstartTime = datetime.datetime.now()\n\ntime.sleep(1)\n\n#\xe7\xbb\x93\xe6\x9d\x9f\xe8\xae\xa1\xe6\x97\xb6\nendTime = datetime.datetime.now()\nprint(endTime - startTime)\n#\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x9a0:00:01.000791'
faceai/versionPut.py,0,b'#coding=utf-8\n#\xe7\x89\x88\xe6\x9c\xac\xe5\x8f\xb7\xe8\xbe\x93\xe5\x87\xba\xe7\xb1\xbb\nimport cv2\nimport dlib\nimport face_recognition\nimport keras\nimport tensorflow\n\nprint(cv2.__version__)  # \xe8\xbe\x93\xe5\x87\xba\xef\xbc\x9a3.4.1\nprint(dlib.__version__)  # \xe8\xbe\x93\xe5\x87\xba\xef\xbc\x9a19.8.1\nprint(face_recognition.__version__)  #\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x9a1.2.2\n\nprint(keras.__version__)  # \xe8\xbe\x93\xe5\x87\xba\xef\xbc\x9a2.1.6\nprint(tensorflow.VERSION)  # \xe8\xbe\x93\xe5\x87\xba\xef\xbc\x9a1.8.0\n'
faceai/videoDlib.py,0,"b'#coding=utf-8\n#\xe8\xa7\x86\xe9\xa2\x91\xe4\xba\xba\xe8\x84\xb8\xe6\xa3\x80\xe6\xb5\x8b\xe7\xb1\xbb - Dlib\xe7\x89\x88\xe6\x9c\xac\nimport cv2\nimport dlib\n\ndetector = dlib.get_frontal_face_detector()  #\xe4\xbd\xbf\xe7\x94\xa8\xe9\xbb\x98\xe8\xae\xa4\xe7\x9a\x84\xe4\xba\xba\xe7\xb1\xbb\xe8\xaf\x86\xe5\x88\xab\xe5\x99\xa8\xe6\xa8\xa1\xe5\x9e\x8b\n\n\ndef discern(img):\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    dets = detector(gray, 1)\n    for face in dets:\n        left = face.left()\n        top = face.top()\n        right = face.right()\n        bottom = face.bottom()\n        cv2.rectangle(img, (left, top), (right, bottom), (0, 255, 0), 2)\n        cv2.imshow(""image"", img)\n\n\ncap = cv2.VideoCapture(0)\nwhile (1):\n    ret, img = cap.read()\n    discern(img)\n    if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()'"
faceai/videoOpencv.py,0,"b'#coding=utf-8\n#\xe8\xa7\x86\xe9\xa2\x91\xe4\xba\xba\xe8\x84\xb8\xe6\xa3\x80\xe6\xb5\x8b\xe7\xb1\xbb - OpenCV\xe7\x89\x88\xe6\x9c\xac\nimport cv2\n\n\n# \xe5\x9b\xbe\xe7\x89\x87\xe8\xaf\x86\xe5\x88\xab\xe6\x96\xb9\xe6\xb3\x95\ndef discern(img):\n    grayImg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    # OpenCV\xe4\xba\xba\xe8\x84\xb8\xe8\xaf\x86\xe5\x88\xab\xe5\x88\x86\xe7\xb1\xbb\xe5\x99\xa8\n    classifier = cv2.CascadeClassifier(\n        ""C:\\Python36\\Lib\\site-packages\\opencv-master\\data\\haarcascades\\haarcascade_frontalface_default.xml""\n    )\n    color = (0, 255, 0)  # \xe5\xae\x9a\xe4\xb9\x89\xe7\xbb\x98\xe5\x88\xb6\xe9\xa2\x9c\xe8\x89\xb2\n    # \xe8\xb0\x83\xe7\x94\xa8\xe8\xaf\x86\xe5\x88\xab\xe4\xba\xba\xe8\x84\xb8\n    faceRects = classifier.detectMultiScale(\n        grayImg, scaleFactor=1.2, minNeighbors=3, minSize=(32, 32))\n    if len(faceRects):  # \xe5\xa4\xa7\xe4\xba\x8e0\xe5\x88\x99\xe6\xa3\x80\xe6\xb5\x8b\xe5\x88\xb0\xe4\xba\xba\xe8\x84\xb8\n        for faceRect in faceRects:  # \xe5\x8d\x95\xe7\x8b\xac\xe6\xa1\x86\xe5\x87\xba\xe6\xaf\x8f\xe4\xb8\x80\xe5\xbc\xa0\xe4\xba\xba\xe8\x84\xb8\n            x, y, w, h = faceRect\n            # \xe6\xa1\x86\xe5\x87\xba\xe4\xba\xba\xe8\x84\xb8\n            cv2.rectangle(img, (x, y), (x + h, y + w), color, 2)\n\n    cv2.imshow(""image"", img)  # \xe6\x98\xbe\xe7\xa4\xba\xe5\x9b\xbe\xe5\x83\x8f\n\n\ncap = cv2.VideoCapture(0)\nwhile (1):\n    ret, frame = cap.read()\n\n    # cv2.imshow(\'frame\', gray)\n    discern(frame)\n    if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n'"
faceai/opencv/hist.py,0,"b'#coding=utf-8\n#\xe7\x9b\xb4\xe6\x96\xb9\xe5\x9b\xbe\n\nimport cv2\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nplt.rcParams[\'font.sans-serif\'] = [\'SimHei\']  #\xe7\x94\xa8\xe6\x9d\xa5\xe6\xad\xa3\xe5\xb8\xb8\xe6\x98\xbe\xe7\xa4\xba\xe4\xb8\xad\xe6\x96\x87\xe6\xa0\x87\xe7\xad\xbe\n\n# ut = np.zeros(256, dtype=img.dtype)  #\xe5\x88\x9b\xe5\xbb\xba\xe7\xa9\xba\xe7\x9a\x84\xe6\x9f\xa5\xe6\x89\xbe\xe8\xa1\xa8\n# hist = cv2.calcHist(\n#     [img],  #\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe7\x9b\xb4\xe6\x96\xb9\xe5\x9b\xbe\n#     [0],  #\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\n#     None,  #\xe6\xb2\xa1\xe6\x9c\x89\xe4\xbd\xbf\xe7\x94\xa8mask\n#     [256],  #it is a 1D histogram\n#     [0.0, 255.0])\n\n# def calcAndDrawHist(image, color):\n#     hist = cv2.calcHist([image], [0], None, [256], [0.0, 255.0])\n#     minVal, maxVal, minLoc, maxLoc = cv2.minMaxLoc(hist)\n#     histImg = np.zeros([256, 256, 3], np.uint8)\n#     hpt = int(0.9 * 256)\n\n#     for h in range(256):\n#         intensity = int(hist[h] * hpt / maxVal)\n#         cv2.line(histImg, (h, 256), (h, 256 - intensity), color)\n\n#     return histImg\n\n# img = cv2.imread(""img/hist.png"")\n# b, g, r = cv2.split(img)\n\n# print(b)\n# print(g)\n# print(r)\n\n# histImgB = calcAndDrawHist(b, [255, 0, 0])\n# histImgG = calcAndDrawHist(g, [0, 255, 0])\n# histImgR = calcAndDrawHist(r, [0, 0, 255])\n\n# cv2.imshow(""histImgB"", histImgB)\n# cv2.imshow(""histImgG"", histImgG)\n# cv2.imshow(""histImgR"", histImgR)\n\n# #\xe7\x81\xb0\xe8\x89\xb2\xe7\x9b\xb4\xe6\x96\xb9\xe5\x9b\xbe\xe5\x9d\x87\xe8\xa1\xa1\xe5\x8c\x96\n# img = cv2.imread(""img/hist.png"", 0)\n# equ = cv2.equalizeHist(img)\n# cv2.imshow(""old image"", img)\n\n# cv2.imshow(""image"", equ)\n\n##\xe5\xbd\xa9\xe8\x89\xb2\xe7\x9b\xb4\xe6\x96\xb9\xe5\x9b\xbe\xe5\x9d\x87\xe8\xa1\xa1\xe5\x8c\x96\n# img = cv2.imread(""img/hist.png"")\n# ycrcb = cv2.cvtColor(img, cv2.COLOR_BGR2YCR_CB)\n# channels = cv2.split(ycrcb)\n# cv2.equalizeHist(channels[0], channels[0])  #\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe3\x80\x81\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe7\x9f\xa9\xe9\x98\xb5\n# cv2.merge(channels, ycrcb)  #\xe5\x90\x88\xe5\xb9\xb6\xe7\xbb\x93\xe6\x9e\x9c\xe9\x80\x9a\xe9\x81\x93\n# cv2.cvtColor(ycrcb, cv2.COLOR_YCR_CB2BGR, img)\n# cv2.imshow(""old image"", cv2.imread(""img/hist.png""))\n# cv2.imshow(""image"", img)\n\n##\xe7\xbb\x98\xe5\x88\xb6\xe7\x9b\xb4\xe6\x96\xb9\xe5\x9b\xbe\n# img = cv2.imread(""img/hist.png"")\n# chans = cv2.split(img)\n# colors = (""b"", ""g"", ""r"")\n# plt.figure()\n# plt.title(""\xe7\x9b\xb4\xe6\x96\xb9\xe5\x9b\xbe\xe5\x88\x86\xe5\xb8\x83"")\n# plt.xlabel(""\xe9\xa2\x9c\xe8\x89\xb2\xe5\x80\xbc"")\n# plt.ylabel(""\xe5\x83\x8f\xe7\xb4\xa0\xe7\x82\xb9"")\n# for (chan, color) in zip(chans, colors):\n#     hist = cv2.calcHist([chan], [0], None, [256], [0, 256])\n#     plt.plot(hist, color=color)\n#     plt.xlim([0, 256])\n# plt.show()\n\n# #\xe6\xb7\xbb\xe5\x8a\xa0\xe5\x99\xaa\xe5\xa3\xb0\n# img = cv2.imread(""img/black.png"")\n\n# for k in range(0, 1000):\n#     xi = int(np.random.uniform(0, img.shape[1]))\n#     xj = int(np.random.uniform(0, img.shape[0]))\n#     if img.ndim == 2:\n#         img[xj, xi] = 255\n#     elif img.ndim == 3:\n#         img[xj, xi, 0] = 255\n#         img[xj, xi, 1] = 255\n#         img[xj, xi, 2] = 255\n# cv2.imwrite(""img/black-noise.png"", img)\n# cv2.imshow(""image"", img)\n\n# #\xe6\xbb\xa4\xe6\xb3\xa2\xe5\x99\xa8\nimg = cv2.imread(""img/black-noise.png"")\n\ndst = cv2.blur(img, (5, 5))  #\xe5\x9d\x87\xe5\x80\xbc\xe6\xbb\xa4\xe6\xb3\xa2\ngaussian = cv2.GaussianBlur(img, (5, 5), 1)  #\xe9\xab\x98\xe6\x96\xaf\xe6\xbb\xa4\xe6\xb3\xa2\nmedian = cv2.medianBlur(img, 5)  #\xe4\xb8\xad\xe5\x80\xbc\xe6\xbb\xa4\xe6\xb3\xa2\ncv2.imshow(""image"", gaussian)\n\n# #Sobel\xe7\xae\x97\xe5\xad\x90 \xe2\x80\x94\xe2\x80\x94 \xe6\x98\xaf\xe4\xb8\x80\xe7\xa7\x8d\xe5\xb8\xa6\xe6\x9c\x89\xe6\x96\xb9\xe5\x90\x91\xe6\x80\xa7\xe7\x9a\x84\xe6\xbb\xa4\xe6\xb3\xa2\xe5\x99\xa8\n# img = cv2.imread(\'img/ag.png\', cv2.IMREAD_COLOR)\n# x = cv2.Sobel(\n#     img, cv2.CV_16S, 1, 0\n# )  #cv2.CV_16S -- Sobel \xe5\x87\xbd\xe6\x95\xb0\xe6\xb1\x82\xe5\xae\x8c\xe5\xaf\xbc\xe6\x95\xb0\xe5\x90\x8e\xe4\xbc\x9a\xe6\x9c\x89\xe8\xb4\x9f\xe5\x80\xbc\xe5\x92\x8c\xe5\xa4\xa7\xe4\xba\x8e255\xe7\x9a\x84\xe5\x80\xbc\xef\xbc\x8c\xe8\x80\x8c\xe5\x8e\x9f\xe5\x9b\xbe\xe5\x83\x8f\xe6\x98\xafuint8\xef\xbc\x888\xe4\xbd\x8d\xe6\x97\xa0\xe7\xac\xa6\xe5\x8f\xb7\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x89\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x9c\xa8\xe5\xbb\xba\xe7\xab\x8b\xe5\x9b\xbe\xe5\x83\x8f\xe6\x97\xb6\xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\x8d\xe5\xa4\x9f\xef\xbc\x8c\xe4\xbc\x9a\xe8\xa2\xab\xe6\x88\xaa\xe6\x96\xad\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa816\xe4\xbd\x8d\xe6\x9c\x89\xe7\xac\xa6\xe5\x8f\xb7\xe6\x95\xb0\xe6\x8d\xae\n# y = cv2.Sobel(img, cv2.CV_16S, 0, 1)\n# absx = cv2.convertScaleAbs(\n#     x)  #convertScaleAbs() --  \xe8\xbd\xac\xe5\x9b\x9euint8\xe5\xbd\xa2\xe5\xbc\x8f\xef\xbc\x8c\xe5\x90\xa6\xe5\x88\x99\xe5\xb0\x86\xe6\x97\xa0\xe6\xb3\x95\xe6\x98\xbe\xe7\xa4\xba\xe5\x9b\xbe\xe5\x83\x8f\xef\xbc\x8c\xe8\x80\x8c\xe5\x8f\xaa\xe6\x98\xaf\xe4\xb8\x80\xe5\x89\xaf\xe7\x81\xb0\xe8\x89\xb2\xe5\x9b\xbe\xe5\x83\x8f\n# absy = cv2.convertScaleAbs(y)\n# dist = cv2.addWeighted(absx, 0.5, absy, 0.5, 0)  #\xe5\x8f\x82\xe6\x95\xb02\xef\xbc\x9a\xe7\xac\xac1\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\xef\xbc\x9b\xe5\x8f\x82\xe6\x95\xb04\xef\xbc\x9a\xe7\xac\xac2\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\n# # cv2.imshow(\'y\', absy)\n# # cv2.imshow(\'x\', absx)\n# cv2.imshow(\'dsit\', dist)\n# cv2.imshow(\'img\', img)\n\ncv2.waitKey(0)\ncv2.destroyAllWindows()'"
faceai/opencv/hsv.py,0,"b""#coding=utf-8\n#HSV\xe8\xbd\xac\xe6\x8d\xa2\xef\xbc\x88\xe9\xa2\x9c\xe8\x89\xb2\xe6\x8f\x90\xe5\x8f\x96\xef\xbc\x89\n\nimport cv2\nimport numpy as np\n\ncap = cv2.VideoCapture(0)\n\nwhile (1):\n    _, frame = cap.read()\n    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n\n    #\xe5\x9c\xa8PS\xe9\x87\x8c\xe7\x94\xa8\xe5\x8f\x96\xe8\x89\xb2\xe5\x99\xa8\xe7\x9a\x84HSV\n    psHSV = [112, 89, 52]\n    diff = 40  #\xe4\xb8\x8a\xe4\xb8\x8b\xe6\xb5\xae\xe5\x8a\xa8\xe5\x80\xbc\n    #\xe5\x9b\xa0\xe4\xb8\xbaPS\xe7\x9a\x84HSV\xef\xbc\x88HSB\xef\xbc\x89\xe5\x8f\x96\xe5\x80\xbc\xe6\x98\xaf\xef\xbc\x9a0~360\xe3\x80\x810~1\xe3\x80\x810~1\xef\xbc\x8c\xe8\x80\x8cOpenCV\xe7\x9a\x84HSV\xe6\x98\xaf\xef\xbc\x9a0~180\xe3\x80\x810~255\xe3\x80\x810~255\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe8\xa6\x81\xe5\xaf\xb9ps\xe7\x9a\x84hsv\xe8\xbf\x9b\xe8\xa1\x8c\xe5\xa4\x84\xe7\x90\x86\xef\xbc\x8cH/2\xe3\x80\x81SV*255\n    lowerHSV = [(psHSV[0] - diff) / 2, (psHSV[1] - diff) * 255 / 100,\n                (psHSV[2] - diff) * 255 / 100]\n    upperHSV = [(psHSV[0] + diff) / 2, (psHSV[1] + diff) * 255 / 100,\n                (psHSV[2] + diff) * 255 / 100]\n\n    mask = cv2.inRange(hsv, np.array(lowerHSV), np.array(upperHSV))\n\n    #\xe4\xbd\xbf\xe7\x94\xa8\xe4\xbd\x8d\xe2\x80\x9c\xe4\xb8\x8e\xe8\xbf\x90\xe7\xae\x97\xe2\x80\x9d\xe6\x8f\x90\xe5\x8f\x96\xe9\xa2\x9c\xe8\x89\xb2\xe9\x83\xa8\xe5\x88\x86\n    res = cv2.bitwise_and(frame, frame, mask=mask)\n    #\xe4\xbd\xbf\xe7\x94\xa8\xe9\xab\x98\xe6\x96\xaf\xe6\xa8\xa1\xe5\xbc\x8f\xe4\xbc\x98\xe5\x8c\x96\xe5\x9b\xbe\xe7\x89\x87\n    res = cv2.GaussianBlur(res, (5, 5), 1)\n\n    cv2.imshow('frame', frame)\n    # cv2.imshow('mask', mask)\n    cv2.imshow('res', res)\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\ncv2.destroyAllWindows()"""
faceai/opencv/imgbase.py,0,"b'#coding=utf-8\n#\xe5\x9b\xbe\xe7\x89\x87\xe5\x9f\xba\xe7\xa1\x80\nimport cv2\nimport numpy as np\n\nimg = cv2.imread(""img/ag.png"")\n\n# shape = img.shape  # \xe5\xbd\xa2\xe7\x8a\xb6 (\xe9\xab\x98,\xe5\xae\xbd,3\xe9\x80\x9a\xe9\x81\x93[\xe5\xbd\xa9\xe8\x89\xb2\xe5\x9b\xbe])\n# size = img.size  # \xe5\x83\x8f\xe7\xb4\xa0\xe6\x80\xbb\xe6\x95\xb0\n# dtype = img.dtype  # uint8 \xe5\x9b\xbe\xe7\x89\x87\xe7\xb1\xbb\xe5\x9e\x8b\n\n# roi = img[200:350, 300:330]  # [y\xe8\xbd\xb4\xe9\x80\x89\xe5\x8f\x96\xe5\x8c\xba\xe5\x9f\x9f\xef\xbc\x8cx\xe8\xbd\xb4\xe9\x80\x89\xe5\x8f\x96\xe5\x8c\xba\xe5\x9f\x9f]\n# img[0:150, 100:130] = roi\n# cv2.imshow(""image"", img)\n\nb, g, r = cv2.split(img)  #\xe5\x88\x86\xe5\x89\xb2\xe9\x80\x9a\xe9\x81\x93\nimg = cv2.merge((b, g, r))  #\xe5\x90\x88\xe5\xb9\xb6\xe9\x80\x9a\xe9\x81\x93\n\nimg = img[:, :, 0]\n\ncv2.imshow(""image"", img)\n\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n'"
faceai/opencv/inpaint.py,0,"b'#coding=utf-8\n#\xe5\x9b\xbe\xe7\x89\x87\xe4\xbf\xae\xe5\xa4\x8d\n\nimport cv2\nimport numpy as np\n\npath = ""img/inpaint.png""\n\nimg = cv2.imread(path)\nhight, width, depth = img.shape[0:3]\n\n#\xe5\x9b\xbe\xe7\x89\x87\xe4\xba\x8c\xe5\x80\xbc\xe5\x8c\x96\xe5\xa4\x84\xe7\x90\x86\xef\xbc\x8c\xe6\x8a\x8a[240, 240, 240]~[255, 255, 255]\xe4\xbb\xa5\xe5\xa4\x96\xe7\x9a\x84\xe9\xa2\x9c\xe8\x89\xb2\xe5\x8f\x98\xe6\x88\x900\nthresh = cv2.inRange(img, np.array([240, 240, 240]), np.array([255, 255, 255]))\n\n#\xe5\x88\x9b\xe5\xbb\xba\xe5\xbd\xa2\xe7\x8a\xb6\xe5\x92\x8c\xe5\xb0\xba\xe5\xaf\xb8\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x84\xe5\x85\x83\xe7\xb4\xa0\nkernel = np.ones((3, 3), np.uint8)\n\n#\xe6\x89\xa9\xe5\xbc\xa0\xe5\xbe\x85\xe4\xbf\xae\xe5\xa4\x8d\xe5\x8c\xba\xe5\x9f\x9f\nhi_mask = cv2.dilate(thresh, kernel, iterations=1)\nspecular = cv2.inpaint(img, hi_mask, 5, flags=cv2.INPAINT_TELEA)\n\ncv2.namedWindow(""Image"", 0)\ncv2.resizeWindow(""Image"", int(width / 2), int(hight / 2))\ncv2.imshow(""Image"", img)\n\ncv2.namedWindow(""newImage"", 0)\ncv2.resizeWindow(""newImage"", int(width / 2), int(hight / 2))\ncv2.imshow(""newImage"", specular)\ncv2.waitKey(0)\ncv2.destroyAllWindows()'"
faceai/opencv/mouse.py,0,"b""#coding=utf-8\n#\xe9\xbc\xa0\xe6\xa0\x87\xe7\xbb\x98\xe5\x9b\xbe\n\nimport cv2\nimport numpy as np\n\n# **************** 1 ****************\n# for i in dir(cv2):\n#     if 'EVENT' in i:\n#         print(i)\n'''\nEVENT_FLAG_ALTKEY  #\xe6\x8c\x89\xe4\xbd\x8falt\xe9\x94\xae\nEVENT_FLAG_CTRLKEY #\xe6\x8c\x89\xe4\xbd\x8fctrl\xe9\x94\xae\nEVENT_FLAG_LBUTTON #\xe6\x8c\x89\xe4\xbd\x8f\xe9\xbc\xa0\xe6\xa0\x87\xe5\xb7\xa6\xe9\x94\xae\nEVENT_FLAG_MBUTTON #\xe6\x8c\x89\xe4\xbd\x8f\xe5\x8f\xb3\xe9\x94\xae\xe7\x82\xb9\xe5\x87\xbb\xe5\xb7\xa6\xe9\x94\xae\nEVENT_FLAG_RBUTTON #\xe6\x8c\x89\xe4\xbd\x8f\xe9\xbc\xa0\xe6\xa0\x87\xe5\x8f\xb3\xe9\x94\xae\nEVENT_FLAG_SHIFTKEY #\xe6\x8c\x89\xe4\xbd\x8fshift\xe9\x94\xae\nEVENT_LBUTTONDBLCLK #\xe5\xb7\xa6\xe9\x94\xae\xe5\x8f\x8c\xe5\x87\xbb\nEVENT_LBUTTONDOWN #\xe5\xb7\xa6\xe9\x94\xae\xe6\x8c\x89\xe4\xb8\x8b\nEVENT_LBUTTONUP #\xe5\xb7\xa6\xe9\x94\xae\xe6\x8a\xac\xe8\xb5\xb7\nEVENT_MBUTTONDBLCLK #\xe6\xbb\x9a\xe8\xbd\xae\xe5\x8f\x8c\xe5\x87\xbb\nEVENT_MBUTTONDOWN #\xe6\xbb\x9a\xe8\xbd\xae\xe6\x8c\x89\xe4\xb8\x8b\nEVENT_MBUTTONUP #\xe6\xbb\x9a\xe8\xbd\xae\xe6\x8a\xac\xe8\xb5\xb7\nEVENT_MOUSEMOVE #\xe9\xbc\xa0\xe6\xa0\x87\xe7\xa7\xbb\xe5\x8a\xa8\nEVENT_MOUSEWHEEL #\xe9\xbc\xa0\xe6\xa0\x87\xe6\xbb\x9a\xe8\xbd\xae\xe6\xbb\x9a\xe5\x8a\xa8\nEVENT_RBUTTONDBLCLK #\xe5\x8f\xb3\xe9\x94\xae\xe5\x8f\x8c\xe5\x87\xbb  \nEVENT_RBUTTONDOWN #\xe5\x8f\xb3\xe9\x94\xae\xe6\x8c\x89\xe4\xb8\x8b\nEVENT_RBUTTONUP #\xe5\x8f\xb3\xe9\x94\xae\xe6\x8a\xac\xe8\xb5\xb7\n'''\n\n# # **************** 2 ****************\n# def draw_circle(event, x, y, flags, param):\n#     if event == cv2.EVENT_MBUTTONDOWN:\n#         cv2.circle(img, (x, y), 20, (255, 0, 0), -1)\n\n# img = np.zeros((512, 512, 3), np.uint8)\n# cv2.namedWindow('image')\n# cv2.setMouseCallback('image', draw_circle)\n\n# while (1):\n#     cv2.imshow('image', img)\n#     if cv2.waitKey(1) & 0xFF == ord('q'):\n#         break\n\n# **************** 3 happy\xe7\x9a\x84\xe8\x87\xaa\xe7\x94\xb1\xe7\xbb\x98\xe5\x9b\xbe ****************\ndrawing = False\n\n\ndef drawDef(event, x, y, flags, param):\n    global drawing\n\n    if event == cv2.EVENT_LBUTTONDOWN:\n        drawing = True\n    if event == cv2.EVENT_LBUTTONUP:\n        drawing = False\n\n    if event == cv2.EVENT_MOUSEMOVE and drawing == True:\n        cv2.circle(img, (x, y), 10, (255, 0, 0), -1)\n\n\nimg = np.zeros((512, 512, 3), np.uint8)\ncv2.namedWindow('image')\ncv2.setMouseCallback('image', drawDef)\n\nwhile (1):\n    cv2.imshow('image', img)\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\ncv2.destroyAllWindows()\n"""
faceai/opencv/trackbar.py,0,"b""#coding=utf-8\n#\xe8\xb0\x83\xe8\x89\xb2\xe6\x9d\xbf\nimport cv2\nimport numpy as np\n\nimg = np.zeros((300, 512, 3), np.uint8)\ncv2.namedWindow('image')\n\n\ndef callback(x):\n    pass\n\n\n#\xe5\x8f\x82\xe6\x95\xb01\xef\xbc\x9a\xe5\x90\x8d\xe7\xa7\xb0\xef\xbc\x9b\xe5\x8f\x82\xe6\x95\xb02\xef\xbc\x9a\xe4\xbd\x9c\xe7\x94\xa8\xe7\xaa\x97\xe5\x8f\xa3\xef\xbc\x8c\xe5\x8f\x82\xe6\x95\xb03\xe3\x80\x814\xef\xbc\x9a\xe6\x9c\x80\xe5\xb0\x8f\xe5\x80\xbc\xe5\x92\x8c\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\xef\xbc\x9b\xe5\x8f\x82\xe6\x95\xb05\xef\xbc\x9a\xe5\x80\xbc\xe6\x9b\xb4\xe6\x94\xb9\xe5\x9b\x9e\xe8\xb0\x83\xe6\x96\xb9\xe6\xb3\x95\ncv2.createTrackbar('R', 'image', 0, 255, callback)\ncv2.createTrackbar('G', 'image', 0, 255, callback)\ncv2.createTrackbar('B', 'image', 0, 255, callback)\n\nwhile (1):\n    cv2.imshow('image', img)\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n    r = cv2.getTrackbarPos('R', 'image')\n    g = cv2.getTrackbarPos('G', 'image')\n    b = cv2.getTrackbarPos('B', 'image')\n\n    img[:] = [b, g, r]\n\ncv2.destroyAllWindows()"""
