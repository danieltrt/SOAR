file_path,api_count,code
SmashLadderClient.py,0,"b'import asyncio\nimport json\nimport requests # pip install requests\nimport websockets # pip install websockets\nimport subprocess, os, signal\nimport time\nfrom enum import IntEnum\n\nclass Characters(IntEnum):\n    bowser = 43\n    captain_falcon = 48\n    donkey_kong = 49\n    dr_mario = 58\n    falco = 59\n    fox = 50\n    ganondorf = 60\n    ice_climbers = 44\n    jigglypuff = 61\n    kirby = 51\n    link = 52\n    luigi = 62\n    mario = 53\n    marth = 63\n    mewtwo = 64\n    mr_game_and_watch = 65\n    ness = 54\n    peach = 45\n    pichu = 66\n    pikachu = 55\n    random = 69\n    roy = 67\n    samus = 56\n    sheik = 46\n    yoshi = 57\n    young_link = 68\n    zelda = 47\n    unknown = 0\n\nclass Stages(IntEnum):\n    yoshis_story = 43\n    fountain_of_dreams = 44\n    battlefield = 45\n    final_destination = 46\n    dream_land = 47\n    pokemon_stadium = 48\n\nclass Actions(IntEnum):\n    player_1_strike_stage = 1\n    player_2_strike_stage = 2\n    player_1_pick_character = 3\n    player_2_pick_character = 4\n    players_blind_pick_characters = 5\n    players_play_game = 6\n    player_1_pick_stage = 7\n    player_2_pick_stage = 8\n    game_over = 9\n    dispute = 10\n    player_1_ban_stage = 11\n    player_2_ban_stage = 12\n    # Nice.\n    play_rps = 13\n\nclass GameResult(IntEnum):\n    lose = 1\n    win = 2\n    cancel = 3\n    finished = 4\n    disputed = 5\n\nclass Feedback(IntEnum):\n    positive = 1\n    neutral = 0\n    negative = -1\n\n\nclass SmashLadderClient:\n    base_url = ""https://www.smashladder.com/""\n    api_url = ""https://www.smashladder.com/api/v1/""\n    socket_url = ""wss://www.smashladder.com/?type=3&version=9.11.4&userlist_visible=false""\n\n    def __init__(self):\n        self.cookies = None\n        self.current_search = None\n        self.last_match = None\n        self.current_match = None\n        self.user_id = None\n        self.phillip_thread = None\n\n    def post(self, url, data=None):\n        response = requests.post(self.api_url + url, data=data, cookies=self.cookies)\n        if not isinstance(response, dict):\n            response = response.json()\n        if ""error"" in response:\n            print(response[""error""])\n            # import ipdb; ipdb.set_trace()\n        return response\n\n    def get(self, url, data=None):\n        return requests.get(self.api_url + url, data=data, cookies=self.cookies)\n\n    def me(self):\n        return self.get(""player/me"").json()\n\n    def on_logged_in(self):\n        return\n\n    def on_connected(self):\n        return\n\n    def on_challenged(self, challenges):\n        return\n\n    def on_game_updated(self, match):\n        return\n\n    def on_game_ended(self, match):\n        return\n    \n    def on_match_chat_recieved(self, message, match_id):\n        return\n\n    def on_search_created(self, match):\n        return\n\n    def on_socket_updated(self):\n        return\n    \n    def post_match(self):\n        pass\n\n    def process_message(self, input):\n        if ""searches"" in input:\n            for id in [key for key in input[""searches""] if key != ""all_entries""]:\n                # Check if the current search has been removed.\n                if (""is_removed"" in input[""searches""][id]) and (input[""searches""][id][""is_removed""] == 1):\n                    if self.current_search == id:\n                        self.current_search = None\n                else:\n                    self.on_search_created(input[""searches""][id])\n\n        if ""open_challenges"" in input:\n            for id in input[""open_challenges""]:\n                if id != ""all_entries"":\n                    self.on_challenged(input[""open_challenges""][id])\n\n        if ""current_matches"" in input:\n            # This dictionary will never contain more than one match (and the all_entries key).\n            # ...Unless the Smash Ladder bugs out. That\'s not our fault.\n            for id in input[""current_matches""]:\n                if id != ""all_entries"":\n                    # Check if the input contains chat, but also make sure it contains only chat.\n                    # If it contains match data, then it\'s the message sent when a client reconnects for the first time.\n                    # Not checking for this would result in previously-sent messages being processed again.\n                    if (""chat"" in input[""current_matches""][id]) and not (""id"" in input[""current_matches""][id]):\n                        chat = input[""current_matches""][id][""chat""][""chat_messages""]\n\n                        # If the type is list, then the message only contains ""<player> is typing..."" data.\n                        if type(chat) is dict:\n                            message = chat[list(chat.keys())[0]]\n                            if str(message[""player""][""id""]) != self.user_id:\n                                # Chat messages don\'t contain any match data, so we have to manually send the ID.\n                                self.on_match_chat_recieved(message[""message""], id)\n\n                    if ""end_phase"" in input[""current_matches""][id]:\n                        if input[""current_matches""][id][""end_phase""] == 0:\n                            self.current_match = id\n\n                            self.on_game_updated(input[""current_matches""][id])\n\n                        else:\n                            if id != self.last_match:\n                                self.on_game_ended(input[""current_matches""][id])\n\n                                self.exit_match(id)\n\n                                self.last_match = id\n                                self.current_match = None\n                                \n                                self.post_match()\n        \n        self.on_socket_updated()\n\n    def log_in(self, username, password):\n        data = {\n            ""username"": username,\n            ""password"": password,\n            ""remember"": ""0"",\n            ""json"": ""1""\n        }\n        response = requests.post(self.base_url + ""log-in"", data=data, cookies=self.cookies)\n        if not response.json()[""success""]:\n            raise ValueError(response.json()[""error""])\n\n        # The reponse, if successful, will have cookies as headers which can be used for authentication.\n        # This includes the user\'s ID (lad_sock_user_id) and authentication hash (lad_sock_hash).\n        self.cookies = response.cookies.get_dict()\n        self.user_id = self.cookies[""lad_sock_user_id""]\n        self.on_logged_in()\n        asyncio.get_event_loop().run_until_complete(self.start_socket())\n        \n    async def start_socket(self):\n        # The websocket requires authentication.\n        header = [(""Cookie"", ""lad_sock_user_id={0}; lad_sock_hash={1}"".format(self.cookies[""lad_sock_user_id""], self.cookies[""lad_sock_hash""]))]\n        async with websockets.connect(SmashLadderClient.socket_url, extra_headers=header) as client:\n            self.on_connected()\n\n            # TODO: figure out what\'s up with this\n            # Process current match and finish any pre-existing match.\n            # data = {""is_in_ladder"": ""1"", ""match_only_mode"": ""1""}\n            # response = self.post(""matchmaking/get_user_going"", data=data)\n            # self.process_message(response)\n\n            while True:\n                message = await client.recv()\n                self.process_message(json.loads(str(message)))\n\n    def send_private_message_to_user(self, user_id, message):\n        data = {\n            ""chat_room_id"": """",\n            ""to_user_id"": user_id,\n            ""message"": message,\n        }\n        self.post(""matchmaking/send_chat"", data=data)\n\n    def challenge_search(self, match):\n        data = {\n            ""challenge_player_id"": match[""player1""][""id""],\n            ""match_id"": match[""id""]\n        }\n        self.post(""matchmaking/challenge_search"", data=data)\n\n    def create_search(self, game_count, title=None):\n        data = {\n            ""team_size"": 1,\n            ""game_id"": 2, # Game ID 2 is Melee.\n            ""match_count"": game_count, # Possible values are 5, 3, and 0 (infinite).\n            ""title"": title,\n            ""ranked"": 0\n        }\n        response = self.post(""matchmaking/start"", data=data)\n        if ""searches"" in response:\n            self.current_search = list(response[""searches""].keys())[0]\n            return True\n        return False\n\n    def cancel_search(self, search_id):\n        data = {\n            ""match_id"": search_id\n        }\n        self.post(""matchmaking/end_matchmaking"", data=data)\n\n    def send_chat(self, match_id, message):\n        data = {\n            ""match_id"": match_id,\n            ""message"": message,\n        }\n        self.post(""chat/send"", data=data)\n        print(""send_chat:"", message)\n\n    def select_stage(self, match_id, stage):\n        data = {\n            ""match_id"": match_id,\n            ""stage_id"": int(stage)\n        }\n        self.post(""match/select_stage"", data=data)\n\n    def select_character(self, match_id, character):\n        data = {\n            ""match_id"": match_id,\n            ""character_id"": int(character)\n        }\n        self.post(""match/select_character"", data=data)\n\n    def report_match(self, match_id, result):\n        # Confusingly, reporting who won is not absolute (in contrast to every other match player reference).\n        data = {\n            ""match_id"": match_id,\n            ""won"": int(result)\n        }\n        self.post(""match/report"", data=data)\n\n    def update_match_feedback(self, match_id, feedback_text, attitude, connection):\n        data = {\n            ""match_id"": match_id,\n            ""feedback"": feedback_text,\n            ""salt_feedback"": attitude, # -1, 0, 1\n            ""connection_feedback"": connection, # -1, 0, 1\n            ""version"": ""2""\n        }\n        self.post(""matchmaking/update_feedback"", data=data)\n\n    def exit_match(self, match_id):\n        data = {\n            ""match_id"": match_id\n        }\n        self.post(""match/exit"", data=data)\n\n    def reply_to_challenge(self, challenge_id, accepted):\n        data = {\n            ""match_id"": challenge_id,\n            ""accept"": (""1"" if accepted else ""0""),\n            ""host_code"": """"\n        }\n        # TODO: replace this with accept/reject\n        self.post(""matchmaking/reply_to_match"", data=data)\n        if accepted:\n            self.current_match = challenge_id\n    \n    def update_main(self, char_id):\n        data = {\n            ""game_id"": 2,\n            ""character_id"": int(char_id),\n            ""add_main"": True,\n        }\n        \n        self.post(""player/edit_mains"", data=data)\n\nclass MatchState:\n\n    def __init__(self):\n        self.chat_sent = False\n        self.host_code = None\n        self.prev_action = None\n\n    def update_action(self, action):\n        if action != self.prev_action:\n            print(action)\n            self.prev_action = action\n\nclass TestSmashLadderClient(SmashLadderClient):\n\n    def __init__(self):\n        super(TestSmashLadderClient, self).__init__()\n\n        self.match_state = None\n\n    def on_logged_in(self):\n        print(""Logged in."")\n\n    def update_builds(self):\n        data = {\n            ""build_preference_id"": 13, # FM 5.9\n            ""active"": True,\n        }\n        \n        self.post(""matchmaking/update_active_build_preferences"", data)\n\n    def leave_matches(self):\n        response = self.get(""match/current_match"").json()\n        if \'match\' in response:\n          self.exit_match(response[\'match\'][\'id\'])\n          print(""Left match"")\n        else:\n          print(""no match to leave"")\n\n    def create_match(self):\n        self.create_search(3, ""FalcoBot"")\n\n    def on_connected(self):\n        print(""Connected."")\n        #me = self.me()\n        self.update_builds()\n        self.update_main(Characters.falco)\n        self.leave_matches()\n        self.create_match()\n        return\n        # TODO: delete rest?\n        # Cancel any already-existing searches.\n        response = self.post(""matchmaking/retrieve_match_searches"").json()\n        # Every response dictionary contains an ""all_entries"" key. We need to filter that out.\n        for id in [key for key in response[""searches""] if key != ""all_entries""]:\n            if str(response[""searches""][id][""player1""][""id""]) == self.user_id:\n                self.cancel_search(id)\n    \n    def on_challenged(self, challenge):\n        print(""Challenged by "", challenge[""player2""][""username""])\n        if self.current_match is None:\n            self.post(""matchmaking/accept"", data={""match_id"": challenge[""id""]})\n        # can\'t do more than one match at a time :(\n        # self.create_match()\n\n    def pick_stage(self, match, ban=True):\n        stages = [\n            Stages.battlefield,\n            Stages.dream_land,\n            Stages.final_destination,\n            Stages.fountain_of_dreams,\n            Stages.pokemon_stadium,\n            Stages.yoshis_story,\n        ]\n        \n        for stage in (reversed(stages) if ban else stages):\n          if str(int(stage)) in match[""game""][""visible_stages""]:\n            self.select_stage(match[""id""], stage)\n            return\n        \n        print(""No stage was available??"")\n        import ipdb; ipdb.set_trace()\n\n    def kill_dolphin(self):\n        if self.phillip_thread is None: return\n        # dolphin needs two sigterms?\n        os.killpg(os.getpgid(self.phillip_thread.pid), signal.SIGTERM)\n        os.killpg(os.getpgid(self.phillip_thread.pid), signal.SIGTERM)\n        self.phillip_thread = None\n\n    def join_host(self, code):\n        if code is None or code == self.match_state.host_code:\n            return\n        self.match_state.host_code = code\n\n        self.kill_dolphin()\n        \n        cmd = [""./netplay.sh"", code, ""6"", ""0""]\n        print(cmd)\n        self.phillip_thread = subprocess.Popen(cmd, start_new_session=True)\n\n    def on_game_updated(self, match):\n        if self.match_state is None:\n            self.match_state = MatchState()\n\n        if not self.match_state.chat_sent:\n            time.sleep(4)\n            self.send_chat(match[""id""], ""hi, you host"")\n            self.match_state.chat_sent = True\n\n        self.join_host(match[""host_code""][""code""])\n\n        # we are 1 when we host\n        player_index = 1\n        other_index = 3 - player_index\n\n        game = match[""game""]\n        current_action = Actions(game[""current_action""])\n        self.match_state.update_action(current_action)\n\n        if (current_action == Actions.player_1_strike_stage) or (current_action == Actions.player_1_ban_stage):\n            self.pick_stage(match, True)\n\n        elif (current_action == Actions.player_1_pick_character) or (current_action == Actions.player_2_pick_character):\n            self.select_character(match[""id""], Characters.falco)\n\n        elif current_action == Actions.players_blind_pick_characters:\n            self.select_character(match[""id""], Characters.falco)\n\n        elif current_action == Actions.players_play_game: # Playing game.\n            # Check for external condition here.\n            \n            # An async await operation is not needed, as SmashLadder reminds the web socket to report the score approximately twice every second.            \n\n            # Make sure we haven\'t reported the match already (again, SmashLadder causes this to happen twice per second).\n            our_report = game[""teams""][str(player_index)][""match_report""]\n            their_report = game[""teams""][str(other_index)][""match_report""]\n            if our_report is None and their_report is not None:\n                result = 3 - their_report\n                print(""Game result: "", GameResult(result))\n                self.report_match(match[""id""], result)\n\n        elif (current_action == Actions.player_1_pick_stage) or (current_action == Actions.player_2_pick_stage):\n            self.pick_stage(match, False)\n\n    def on_game_ended(self, match):\n        time.sleep(1)\n        self.send_chat(match[""id""], ""good games"")\n        self.match_state = None\n        # broken\n        # self.update_match_feedback(match[""id""], """", Feedback.neutral, Feedback.neutral)\n        self.kill_dolphin()\n        print(""Match %s completed."" % match[""id""])\n    \n    def post_match(self):\n        self.create_match()\n    \n    def on_match_chat_recieved(self, message, match_id):\n        print(message)\n        if message.upper() == ""!PING"":\n            self.send_chat(match_id, ""Pong!"")\n        elif message.upper().startswith(""!ECHO""):\n            self.send_chat(match_id, message[6:])\n\n    def on_search_created(self, search):\n        if self.current_match:\n          return\n\n        is_melee = search[""ladder_game""][""id""] == 2\n        if not is_melee:\n          return\n\n        # TODO: Check location.\n        correct_player = search[""player1""][""username""] == ""XPilot""\n        is_not_infinite = search[""match_count""] != 0\n        is_not_ranked = not search[""is_ranked""]\n        #can_use_faster_melee = search[""player1""][""preferred_builds""][""2""][0][""active""] == 1\n\n        if correct_player:\n          self.challenge_search(search)\n          print(""Challenged search created by {0} ({1})."".format(search[""player1""][""username""], search[""player1""][""id""]))\n\n\n# I made a file globals.py on my PYTHONPATH for things like this\nfrom globals import smashladder, dolphin_iso_path\n\nTestSmashLadderClient().log_in(smashladder[\'username\'], smashladder[\'password\'])\n\n'"
launch_lib.py,0,"b'import subprocess\n\ndef add_options(parser):\n  parser.add_argument(\'--dry_run\', action=\'store_true\', help=""don\'t start jobs"")\n  parser.add_argument(\'--init\', action=\'store_true\', help=""initialize model"")\n  #parser.add_argument(\'--trainer\', type=str, help=\'trainer IP address\')\n  parser.add_argument(\'--play\', action=\'store_true\', help=""run only agents, not trainer"")\n  parser.add_argument(\'--local\', action=\'store_true\', help=""run locally"")\n  parser.add_argument(\'--agents\', type=int, help=""number of agents to run"")\n  parser.add_argument(\'--log_agents\', action=\'store_true\', help=\'log agent outputs\')\n  parser.add_argument(\'--profile\', action=\'store_true\', help=\'heap profile trainer\')\n  parser.add_argument(\'--disk\', action=\'store_true\', help=\'run agents and dump experiences to disk\')\n  parser.add_argument(\'-p\', \'--tenenbaum\', action=\'store_true\', help=\'run trainer on higher priority\')\n  parser.add_argument(\'-u\' ,\'--use_everything\', action=\'store_true\', help=\'run agents on lower priority\')\n  parser.add_argument(\'-g\', \'--any_gpu\', action=\'store_true\', help=\'run with any gpu (default is titan-x)\')\n  parser.add_argument(\'-t\', \'--time\', type=str, default=""7-0"", help=\'job runtime in days-hours\')\n  parser.add_argument(\'--cpu\', action=\'store_true\', help=""don\'t run trainer on a gpu"")\n  parser.add_argument(\'--gpu\', type=str, default=\'GEFORCEGTX1080TI\', help=\'gpu type\')\n  parser.add_argument(\'--send\', type=int, default=1, help=\'send params with zmq PUB/SUB\')\n  parser.add_argument(\'--pop_size\', type=int, help=\'max pop size\')\n  parser.add_argument(\'--fast_cpu\', action=""store_true"", help=\'run agents faster on haswell cpus\')\n  parser.add_argument(\'-f\', \'--fixed_enemy\', action=""store_true"", help=""don\'t update enemy live"")\n\n\ndef launch(\n  args, name, command, cpus=2, mem=1, gpu=False, log=True, qos=None,\n  array=None, depends=None, pids=[]):\n\n  #command = ""LD_PRELOAD=$OM_USER/lib/libtcmalloc.so.4 "" + command\n  if gpu:\n    command += "" --gpu""\n  \n  print(command)\n  if args.dry_run:\n    return\n  \n  if args.local:\n    if array is None:\n      array = 1\n    for i in range(array):\n      kwargs = {}\n      for s in [\'out\', \'err\']:\n        kwargs[\'std\' + s] = open(""slurm_logs/%s_%d.%s"" % (name, i, s), \'w\') if log else subprocess.DEVNULL\n      proc = subprocess.Popen(command.split(\' \'), **kwargs)\n      pids.append(proc.pid)\n    return None\n\n  slurmfile = \'slurm_scripts/\' + name + \'.slurm\'\n  with open(slurmfile, \'w\') as f:\n    def opt(s):\n      f.write(""#SBATCH "" + s + ""\\n"")\n    f.write(""#!/bin/bash\\n"")\n    f.write(""#SBATCH --job-name "" + name + ""\\n"")\n    \n    logname = name\n    if array:\n      logname += ""_%a""\n    if log:\n      f.write(""#SBATCH --output slurm_logs/"" + logname + "".out\\n"")\n    else:\n      f.write(""#SBATCH --output /dev/null\\n"")\n    f.write(""#SBATCH --error slurm_logs/"" + logname + "".err\\n"")\n    \n    f.write(""#SBATCH -c %d\\n"" % cpus)\n    f.write(""#SBATCH --mem %dG\\n"" % mem)\n    f.write(""#SBATCH --time %s\\n"" % args.time)\n    #f.write(""#SBATCH --cpu_bind=verbose,cores\\n"")\n    #f.write(""#SBATCH --cpu_bind=threads\\n"")\n    #opt(""--partition=om_all_nodes,om_test_nodes"")\n    if gpu:\n      if args.any_gpu:\n        f.write(""#SBATCH --gres gpu:1\\n"")\n      else:\n        opt(""--gres gpu:%s:1"" % args.gpu)\n      #if not args.any_gpu:  # 31-54 have titan-x, 55-66 have 1080ti\n      #  f.write(""#SBATCH -x node[001-030]\\n"")\n    if qos:\n      f.write(""#SBATCH --qos %s\\n"" % qos)\n    if array:\n      f.write(""#SBATCH --array=1-%d\\n"" % array)\n\n    if depends:\n      opt(""--dependency after:"" + depends)\n\n    if gpu:\n      opt(""-x node069,node067,node060"")\n      f.write(""source ~/.cuda\\n"")\n      f.write(""source activate tf-gpu-src\\n"")\n    else:\n      if args.fast_cpu:\n        f.write(""source activate tf-cpu-src\\n"")\n        opt(""-x node[001-030]"")\n      else:\n        f.write(""source activate tf-cpu-opt\\n"")\n      f.write(""sleep 40s\\n"")\n    f.write(command)\n\n  #command = ""screen -S %s -dm srun --job-name %s --pty singularity exec -B $OM_USER/phillip -B $HOME/phillip/ -H ../home phillip.img gdb -ex r --args %s"" % (name[:10], name, command)\n  output = subprocess.check_output([""sbatch"", slurmfile]).decode()\n  print(output)\n  jobid = output.split()[-1].strip()\n  return jobid\n'"
launch_many.py,0,"b'#!/usr/bin/env python\n\nimport os\nimport sys\nfrom argparse import ArgumentParser\nimport subprocess\nfrom phillip import util\nimport json\nfrom launch_lib import add_options, launch\n\nparser = ArgumentParser()\n\nparser.add_argument(\'path\', type=str, help=""path to enemies file"")\nadd_options(parser)\n\nargs = parser.parse_args()\n\nrun_trainer_b = True\nrun_agents_b = True\n\nif args.disk or args.play:\n  run_trainer_b = False\n\nif args.dry_run:\n  print(""NOT starting jobs:"")\nelse:\n  print(""Starting jobs:"")\n\nif not os.path.exists(""slurm_logs""):\n  os.makedirs(""slurm_logs"")\n\nif not os.path.exists(""slurm_scripts""):\n  os.makedirs(""slurm_scripts"")\n\npids = []\n\nwith open(args.path) as f:\n  agent_paths = json.load(f)\n\nagent_paths = [\'agents/\' + e for e in agent_paths]\n\ndef get_agents(path):\n  params = util.load_params(path)\n  pop_size = params.get(\'pop_size\')\n  if pop_size and args.pop_size:\n    pop_size = min(pop_size, args.pop_size)\n  \n  if pop_size:\n    pop_ids = range(pop_size)\n  else:\n    pop_ids = [-1]\n\n  return [(path, params, pop_id) for pop_id in pop_ids]\n\nagents = []\nfor agent_list in map(get_agents, agent_paths):\n  agents.extend(agent_list)\n\ntrainer_ids = []\n\ndef run_trainer(path, params, pop_id):\n  name = ""trainer_"" + params[\'name\']\n  command = ""python3 -u phillip/train.py --load "" + path\n  command += "" --dump "" + (""lo"" if args.local else ""ib0"")\n  command += "" --send %d"" % args.send\n  \n  if args.init:\n    command += "" --init""\n  \n  if pop_id >= 0:\n    name += ""_%d"" % pop_id\n    command += "" --pop_id %d"" % pop_id\n    if args.pop_size:\n      command += "" --pop_size %d"" % min(args.pop_size, params[\'pop_size\'])\n\n  trainer_id = launch(\n    args, name, command,\n    gpu=not args.cpu,\n    qos=\'tenenbaum\' if args.tenenbaum else None,\n    mem=16,\n    pids=pids,\n  )\n  \n  if trainer_id:\n    trainer_ids.append(trainer_id)\n\ntrainer_depends = None\nif run_trainer_b:\n  for agent_args in agents:\n    run_trainer(*agent_args)\n  if trainer_ids:\n    trainer_depends = "":"".join(trainer_ids)\n\nenemy_commands = []\nfor enemy_path, _, enemy_id in agents:\n  enemy_command = "" --enemy %s"" % enemy_path\n  if enemy_id >= 0:\n    enemy_command += "" --enemy_id %d"" % enemy_id\n  enemy_commands.append(enemy_command)\n\ndef run_agents(path, params, pop_id):\n  actors = args.agents or params.get(\'agents\', 1)\n\n  print(""Using %d actors"" % actors)\n  actors_per_enemy = actors // len(agents)\n\n  common_command = ""python3 -u phillip/run.py --load "" + path\n  if args.disk:\n    common_command += "" --disk 1""\n  else:\n    common_command += "" --dump 1""\n\n  if run_trainer_b:\n    if args.local:\n      common_command += "" --trainer_ip 127.0.0.1""\n\n  if args.local:\n    common_command += "" --dual_core 0""\n  \n  common_command += "" --dolphin --exe dolphin-emu-headless""\n  common_command += "" --zmq 1 --pipe_count 1""\n  common_command += "" --random_swap""\n  # common_command += "" --help""\n  common_command += "" --enemy_dump 1 --enemy_reload 1""\n\n  base_name = ""actor_"" + params[\'name\']\n  if pop_id >= 0:\n    base_name += ""_%d"" % pop_id\n    common_command += "" --pop_id %d"" % pop_id\n\n  for i, enemy_command in enumerate(enemy_commands):\n    name = base_name + ""_%d"" % i\n  \n    full_command = common_command + enemy_command\n\n    launch(\n      args, name, full_command,\n      log=args.log_agents,\n      qos=\'use-everything\' if args.use_everything else None,\n      array=actors_per_enemy,\n      depends=trainer_depends,\n      pids=pids,\n    )\n\nif run_agents_b:\n  for agent_args in agents:\n    run_agents(*agent_args)\n\nif args.local:\n  with open(args.path + \'/pids\', \'w\') as f:\n    for p in pids:\n      f.write(str(p) + \' \')\n'"
launcher.py,0,"b'#!/usr/bin/env python\n\nimport os\nimport sys\nfrom argparse import ArgumentParser\nimport subprocess\nfrom phillip import util\nimport json\nfrom launch_lib import launch, add_options\n\nparser = ArgumentParser()\n\nparser.add_argument(\'path\', type=str, help=""path to experiment"")\nadd_options(parser)\n\nargs = parser.parse_args()\nparams = util.load_params(args.path)\n\nrun_trainer = True\nrun_agents = True\n\nif args.disk or args.play:\n  run_trainer = False\n\nif args.dry_run:\n  print(""NOT starting jobs:"")\nelse:\n  print(""Starting jobs:"")\n\nif not os.path.exists(""slurm_logs""):\n  os.makedirs(""slurm_logs"")\n\nif not os.path.exists(""slurm_scripts""):\n  os.makedirs(""slurm_scripts"")\n\npids = []\n\ndef get_pop_ids(path):\n  agent_params = util.load_params(path)\n  agent_pop_size = args.pop_size or agent_params.get(\'pop_size\')\n  \n  if agent_pop_size:\n    return list(range(agent_pop_size))\n  else:\n    return [-1]\n\npop_ids = get_pop_ids(args.path)\n\ntrainer_depends = None\n\nif run_trainer:\n  common_name = ""trainer_"" + params[\'name\']\n  train_command = ""python3 -u phillip/train.py --load "" + args.path\n  train_command += "" --dump "" + (""lo"" if args.local else ""ib0"")\n  train_command += "" --send %d"" % args.send\n  \n  if args.init:\n    train_command += "" --init""\n  \n  trainer_ids = []\n  \n  for pop_id in pop_ids:\n    train_name = common_name\n    if pop_id >= 0:\n      train_name += ""_%d"" % pop_id\n    \n    command = train_command\n    command += "" --pop_id %d"" % pop_id\n  \n    trainer_id = launch(args, train_name, command,\n      gpu=not args.cpu,\n      qos=\'tenenbaum\' if args.tenenbaum else None,\n      mem=16,\n      cpus=4,\n      pids=pids,\n    )\n    \n    if trainer_id:\n      trainer_ids.append(trainer_id)\n  \n  trainer_depends = \':\'.join(trainer_ids)\n\nclass AgentNamer:\n  def __init__(self, name):\n    self.name = name\n    self.counter = 0\n  \n  def __call__(self):\n    self.counter += 1\n    return ""agent_%d_%s"" % (self.counter, self.name)\n\nif run_agents:\n  enemies = \'easy\'\n  if \'enemies\' in params:\n    enemies = params[\'enemies\']\n\n  if isinstance(enemies, str):\n    with open(\'enemies/\' + enemies) as f:\n      enemies = json.load(f)\n\n  agents = 1\n  if params[\'agents\']:\n    agents = params[\'agents\']\n  if args.agents:\n    agents = args.agents\n\n  print(""Using %d agents"" % agents)\n  agents_per_enemy = agents // len(enemies)\n\n  common_command = ""python3 -u phillip/run.py --load "" + args.path\n  if args.disk:\n    common_command += "" --disk 1""\n  else:\n    common_command += "" --dump 1""\n\n  if run_trainer:\n    if args.local:\n      common_command += "" --trainer_ip 127.0.0.1""\n\n  if args.local:\n    common_command += "" --dual_core 0""\n  \n  common_command += "" --dolphin""\n  common_command += "" --exe dolphin-emu-headless""\n  common_command += "" --zmq 1 --pipe_count 1""\n  common_command += "" --random_swap""\n  # common_command += "" --help""\n  \n  enemy_commands = []\n  \n  for enemy in enemies:\n    if isinstance(enemy, str):\n      command = """"\n      if enemy == ""self"":\n        enemy_path = args.path\n        if not args.fixed_enemy:\n          command += "" --enemy_dump 1 --enemy_reload 1""\n      else:\n        enemy_path = ""agents/%s/"" % enemy\n      command += "" --enemy "" + enemy_path\n      \n      enemy_ids = get_pop_ids(enemy_path)\n      agents_per_enemy2 = agents_per_enemy // len(enemy_ids)\n      for pop_id in enemy_ids:\n        enemy_commands.append((command + "" --enemy_id %d"" % pop_id, agents_per_enemy2))\n    else: # cpu dict\n      command = "" --cpu {level} --p1 {char}"".format(**enemy)\n      enemy_commands.append((command, agents_per_enemy))\n  \n  for pop_id in pop_ids:\n    namer = AgentNamer(params[\'name\'] + ""_%d"" % pop_id)\n    agent_command = common_command\n    agent_command += "" --pop_id %d"" % pop_id\n    \n    for enemy_command, num_agents in enemy_commands:\n      agent_name = namer()\n      full_command = agent_command + enemy_command\n\n      launch(args, agent_name, full_command,\n        log=args.log_agents,\n        qos=\'use-everything\' if args.use_everything else None,\n        array=num_agents,\n        depends=trainer_depends,\n        pids=pids,\n      )\n\nif args.local:\n  with open(args.path + \'/pids\', \'w\') as f:\n    for p in pids:\n      f.write(str(p) + \' \')\n'"
runner.py,0,"b'#!/usr/bin/env python\nimport os\nimport sys\nimport subprocess\nfrom phillip import util\nfrom collections import OrderedDict\nimport argparse\nimport random\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--tag"", action=""store_true"", help=""generate random tag for this experiment"")\nparser.add_argument(""--name"", type=str, help=""experiment name"")\nargs = parser.parse_args()\n\nif args.tag:\n  exp_name = str(random.getrandbits(32)) + ""_""\nelse:\n  exp_name = """"\nparams = OrderedDict()\n\ndef toStr(val):\n  if isinstance(val, list):\n    return ""_"".join(map(str, val))\n  return str(val)\n\ndef add_param(param, value, name=True):\n  global exp_name\n  if name and value:\n    if isinstance(value, bool):\n      exp_name += ""_"" + param\n    else:\n      exp_name += ""_"" + param + ""_"" + toStr(value)\n  params[param] = value\n\nmodel = \'ActorCritic\'\n\nexp_name += model\n\nrecurrent = True\n\n# add_param(\'policy\', model, False)\nadd_param(\'epsilon\', 0.005, False)\n\nnatural = True\nnatural = False\n#natural = ac\n\n#add_param(\'optimizer\', \'Adam\', False)\n#add_param(\'learning_rate\', 1e-4, False),\nadd_param(\'sweeps\', 1, False)\nadd_param(\'batch_size\', 64, False)\nadd_param(\'batch_steps\', 1, False)\nadd_param(""max_buffer"", 64, False)\nadd_param(\'min_collect\', 32, False)\n\nadd_param(\'reward_halflife\', 4, False)\n\n#add_param(\'dynamic\', 0)\n\n# evolution\nevolve = False\n#evolve = True\n\nif evolve:\n  add_param(""evolve"", True, False)\n  add_param(""pop_size"", 4)\n  add_param(""reward_cutoff"", 2e-4, False)\n  add_param(""evo_period"", 4000, False)\n  add_param(""evolve_entropy"", True, False)\n  add_param(""evolve_learning_rate"", True, False)\n  add_param(""reward_decay"", 2e-4, False)\n\n# add_param(\'explore_scale\', 1e-4)\n\ndelay = 2\npredict_steps = 0\n#predict_steps = delay\n\nif predict_steps:\n  add_param(\'predict_steps\', predict_steps)\n  add_param(\'predict\', True, False)\n  add_param(\'model_weight\', .1, False)\n  add_param(\'model_layers\', [256], False)\n  add_param(\'train_model\', True, False)\n  # add_param(\'train_only_last\', True, True)\n\nadd_param(\'core_layers\', [256], False)\nadd_param(\'actor_layers\', [128], False)\nadd_param(\'critic_layers\', [128], False)\n\n#add_param(\'train_policy\', True)\n#add_param(\'train_critic\', False)\n\n#add_param(\'entropy_power\', 0)\nadd_param(\'entropy_scale\', 1e-3, False)\n\nif recurrent:\n  # add_param(\'clip\', 0.05)\n  add_param(\'recurrent\', True)\n  add_param(\'initial\', \'train\', False)\n\nadd_param(\'gae_lambda\', 1., False)\n#add_param(\'retrace\', True)\n\nadd_param(\'unshift_critic\', True, True)\n\n# embed params\n\nadd_param(\'xy_scale\', 0.05, False)\n#add_param(\'speed_scale\n\nadd_param(\'action_space\', 0, False)\nadd_param(\'player_space\', 0, False)\n\n#add_param(\'critic_layers\', [128] * 1)\n#add_param(\'actor_layers\', [128] * 3)\nadd_param(\'nl\', \'elu\', False)\n\nadd_param(\'action_type\', \'custom\', False)\n\nadd_param(\'fix_scopes\', True, False)\n\n# agent settings\n\n#add_param(\'dolphin\', True, False)\n\nadd_param(\'experience_length\', 80, False)\nadd_param(\'reload\', 1, False)\n\n#char = \'falco\'\n#char = \'sheik\'\nchar = \'falcon\'\n#char = \'marth\'\n#char = \'fox\'\n#char = \'peach\'\n#char = \'luigi\'\n#char = \'samus\'\n#char = \'ganon\'\n#char = \'puff\'\n#char = \'bowser\'\n#char = \'dk\'\n\nfrom phillip import data\n#act_every = 2\nact_every = data.short_hop[char]\nadd_param(\'act_every\', act_every, False)\n\nif delay:\n  add_param(\'delay\', delay)\nif not recurrent:\n  add_param(\'memory\', 1)\n  #add_param(\'memory\', 0, False)\n\nstage = \'battlefield\'\n#stage = \'final_destination\'\nadd_param(\'stage\', stage, False)\n\nadd_param(\'char\', char, True)\n\nenemies = None\nenemies = ""cpu""\n#enemies = ""easy""\n#enemies = ""delay0""\n#enemies = ""delay%d"" % delay\n#enemies = [\'self\']\n#enemies = \'hard-self\'\nadd_param(\'enemies\', enemies)\n\nadd_param(\'enemy_reload\', 3600, False)\n\n# total number of agents\nagents = 80\nparams[\'agents\'] = agents\n\nif args.name is not None:\n  exp_name = args.name\n\nadd_param(\'name\', exp_name, False)\npath = ""saves/%s/"" % exp_name\n#add_param(\'path\', path, False)\n\nprint(""Writing to"", path)\nutil.makedirs(path)\n\nimport json\nwith open(path + ""params"", \'w\') as f:\n  json.dump(params, f, indent=2)\n'"
setup.py,0,"b'""""""A setuptools based setup module.\n\nSee:\nhttps://packaging.python.org/en/latest/distributing.html\nhttps://github.com/pypa/sampleproject\n""""""\n\n# Always prefer setuptools over distutils\nfrom setuptools import setup, find_packages\n# To use a consistent encoding\nfrom codecs import open\nfrom os import path\n\nhere = path.abspath(path.dirname(__file__))\n\n# Get the long description from the README file\nwith open(path.join(here, \'Readme.md\'), encoding=\'utf-8\') as f:\n    long_description = f.read()\n\nsetup(\n    name=\'phillip\',\n\n    # Versions should comply with PEP440.  For a discussion on single-sourcing\n    # the version across setup.py and the project code, see\n    # https://packaging.python.org/en/latest/single_source_version.html\n    version=\'0.1.0\',\n\n    description=\'Super Smash Bros. Melee AI\',\n    long_description=long_description,\n\n    # The project\'s main homepage.\n    url=\'https://github.com/vladfi1/phillip\',\n\n    # Author details\n    author=\'Vlad Firoiu\',\n    #author_email=\'pypa-dev@googlegroups.com\',\n\n    # Choose your license\n    license=\'MIT\',\n\n    # See https://pypi.python.org/pypi?%3Aaction=list_classifiers\n    classifiers=[\n        # How mature is this project? Common values are\n        #   3 - Alpha\n        #   4 - Beta\n        #   5 - Production/Stable\n        \'Development Status :: 3 - Alpha\',\n\n        # Indicate who your project is intended for\n        \'Intended Audience :: Developers\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n\n        # Pick your license as you wish (should match ""license"" above)\n        \'License :: OSI Approved :: MIT License\',\n\n        # Specify the Python versions you support here. In particular, ensure\n        # that you indicate whether you support Python 2, Python 3 or both.\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.3\',\n        \'Programming Language :: Python :: 3.4\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n    ],\n\n    # What does your project relate to?\n    keywords=\'ssbm ai ml\',\n\n    # You can just specify the packages manually here if your project is\n    # simple. Or you can use find_packages().\n    #packages=find_packages(exclude=[\'contrib\', \'docs\', \'tests\']),\n    packages=[\'phillip\'],\n\n    # Alternatively, if you want to distribute just a my_module.py, uncomment\n    # this:\n    #   py_modules=[""my_module""],\n\n    # List run-time dependencies here.  These will be installed by pip when\n    # your project is installed. For an analysis of ""install_requires"" vs pip\'s\n    # requirements files see:\n    # https://packaging.python.org/en/latest/requirements.html\n    install_requires=[\'attrs\', \'pyzmq\'],\n\n    # List additional groups of dependencies here (e.g. development\n    # dependencies). You can install these using the following syntax,\n    # for example:\n    # $ pip install -e .[dev,test]\n    #extras_require={\n    #    \'dev\': [\'check-manifest\'],\n    #    \'test\': [\'coverage\'],\n    #},\n\n    # If there are data files included in your packages that need to be\n    # installed, specify them here.  If using Python 2.6 or less, then these\n    # have to be included in MANIFEST.in as well.\n    package_data={\n        #\'sample\': [\'package_data.dat\'],\n        \'phillip\': [\'data/*\'],\n    },\n\n    # Although \'package_data\' is the preferred approach, in some case you may\n    # need to place data files outside of your packages. See:\n    # http://docs.python.org/3.4/distutils/setupscript.html#installing-additional-files # noqa\n    # In this case, \'data_file\' will be installed into \'<sys.prefix>/my_data\'\n    #data_files=[(\'my_data\', [\'data/data_file\'])],\n    \n    #scripts = [\'phillip/run.py\'],\n\n    # To provide executable scripts, use entry points in preference to the\n    # ""scripts"" keyword. Entry points provide cross-platform support and allow\n    # pip to create the appropriate form of executable for the target platform.\n    entry_points={\n        \'console_scripts\': [\n            \'phillip=phillip.run:main\',\n        ],\n    },\n)\n'"
phillip/RL.py,10,"b'""""""\nParent RL class for Actor and Learner. \n""""""\n\nimport os\nimport tensorflow as tf\nimport numpy as np\nfrom enum import Enum\nfrom . import ssbm, tf_lib as tfl, util, embed\nfrom .default import Default, Option\nfrom .rl_common import *\nfrom .ac import ActorCritic\nfrom .critic import Critic\nfrom .model import Model\nfrom .core import Core\n\nclass Mode(Enum):\n  LEARNER = 0\n  ACTOR = 1\n\nclass RL(Default):\n  _options = [\n    Option(\'tfdbg\', action=\'store_true\', help=\'debug tensorflow session\'),\n    Option(\'path\', type=str, help=""path to saved policy""),\n    Option(\'gpu\', action=""store_true"", default=False, help=""run on gpu""),\n    Option(\'action_type\', type=str, default=""diagonal"", choices=ssbm.actionTypes.keys()),\n    Option(\'name\', type=str),\n    Option(\'predict\', type=int, default=0),\n    Option(\'profile\', type=int, default=0, help=\'profile tensorflow graph execution\'),\n    Option(\'save_cpu\', type=int, default=0),\n    Option(\'evolve\', action=""store_true"", help=""are we part of an evolving population""),\n    Option(\'pop_id\', type=int, default=-1, help=""If pop_id >= 0, then we\'re doing"" \\\n      ""population-based training, and pop_id tracks the worker\'s id. Else we\'re not"" \\\n      ""doing PBT and the id is -1.""), \n    Option(\'dynamic\', type=int, default=1, help=\'use dynamic loop unrolling\'),\n    Option(\'action_space_embed\', type=int, default=0, help=\'embed actions\'),\n  ]\n  \n  _members = [\n    (\'config\', RLConfig),\n    (\'embedGame\', embed.GameEmbedding),\n    (\'critic\', Critic),\n    (\'model\', Model),\n    (\'core\', Core),\n    #(\'opt\', Optimizer),\n    (\'policy\', ActorCritic),\n  ]\n  \n  def __init__(self, **kwargs):\n    Default.__init__(self, init_members=False, **kwargs)\n    self.config = RLConfig(**kwargs)\n    \n    if self.name is None: self.name = ""ActorCritic""\n    if self.path is None: self.path = ""saves/%s/"" % self.name\n    # the below is a hack that makes it easier to run phillip from the command\n    # line, if we\'re doing PBT but don\'t want to specify the population ID. \n    if self.evolve and self.pop_id < 0: self.pop_id = 0\n    if self.pop_id >= 0:\n      self.root = self.path\n      self.path = os.path.join(self.path, str(self.pop_id))\n      print(self.path)\n    # where trained agents get saved onto disk. \n    self.snapshot_path = os.path.join(self.path, \'snapshot\')\n    self.actionType = ssbm.actionTypes[self.action_type]\n\n    self.graph = tf.Graph()\n    self.device = \'/gpu:0\' if self.gpu else \'/cpu:0\'\n    print(""Using device "" + self.device)\n    with self.graph.as_default(), tf.device(self.device):\n      if self.action_space_embed:\n        self.embedAction = embed.LookupEmbedding(\'action\', self.actionType.size, self.action_space_embed)\n      else:\n        # takes in action, and returns a one-hot vector corresponding to that action.\n        self.embedAction = embed.OneHotEmbedding(""action"", self.actionType.size)\n\n      # total number of gradient descent steps the learner has taken thus far\n      self.global_step = tf.Variable(0, name=\'global_step\', dtype=tf.int64, trainable=False)\n      self.evo_variables = []\n      \n      self.embedGame = embed.GameEmbedding(**kwargs)\n      state_size = self.embedGame.size\n      combined_size = state_size + self.embedAction.size\n      history_size = (1+self.config.memory) * combined_size\n      print(""History size:"", history_size)\n\n      self.core = Core(history_size, **kwargs)\n      \n    \n  def get_global_step(self):\n    return self.sess.run(self.global_step)\n  \n  def get_reward(self):\n    return self.sess.run(self.reward)\n\n  def mutation(self, rate=1.):\n    mutations = []\n    for op in self.mutators:\n      if np.random.random() < rate / len(self.mutators):\n        mutations.append(op)\n    self.sess.run(mutations)\n\n  # save weights to disk\n  def save(self):\n    util.makedirs(self.path)\n    print(""Saving to"", self.path)\n    self.saver.save(self.sess, self.snapshot_path, write_meta_graph=False)\n\n  # restore weights from disk\n  def restore(self, path=None):\n    if path is None:\n      path = self.snapshot_path\n    print(""Restoring from"", path)\n    tfl.restore(self.sess, self.variables, path)\n    # self.saver.restore(self.sess, path)\n\n  # initializes weights\n  def init(self):\n    self.sess.run(self.initializer)\n\n  # currently enables Learner to serialize itself to send to Actors \n  def blob(self):\n    with self.graph.as_default():\n      values = self.sess.run(self.variables)\n      return {var.name: val for var, val in zip(self.variables, values)}\n\n  # currently enables Actors to unserializing updated weights sent from a Learner\n  def unblob(self, blob):\n    #self.sess.run(self.unblobber, {self.placeholders[k]: v for k, v in blob.items()})\n    self.sess.run(self.unblobber, {v: blob[k] for k, v in self.placeholders.items()})\n  \n  # helper methods for initialization\n  def _init_model(self, **kwargs):\n    print(""Creating model."")\n    self.model = Model(self.embedGame, self.embedAction.size, self.core, self.config, **kwargs)\n\n  def _init_policy(self, **kwargs):\n    effective_delay = self.config.delay\n    if self.predict:\n      effective_delay -= self.model.predict_steps\n    input_size = self.core.output_size + effective_delay * self.embedAction.size\n    \n    self.policy = ActorCritic(input_size, self.embedAction, self.config, **kwargs)\n    self.evo_variables.extend(self.policy.evo_variables)\n\n  def _finalize_setup(self):\n    self.variables = tf.global_variables()\n    self.initializer = tf.global_variables_initializer()\n    \n    self.saver = tf.train.Saver(self.variables)\n    \n    self.placeholders = {v.name : tf.placeholder(v.dtype, v.get_shape()) for v in self.variables}\n    self.unblobber = tf.group(*[tf.assign(v, self.placeholders[v.name]) for v in self.variables])\n    \n    self.graph.finalize()\n\n    tf_config = dict(\n      allow_soft_placement=True,\n      #log_device_placement=True,\n    )\n    \n    if self.save_cpu:\n      tf_config.update(\n        inter_op_parallelism_threads=1,\n        intra_op_parallelism_threads=1,\n      )\n    \n    self.sess = tf.Session(\n      graph=self.graph,\n      config=tf.ConfigProto(**tf_config),\n    )\n'"
phillip/__init__.py,0,b'import os\npath = os.path.dirname(__file__)\n'
phillip/ac.py,13,"b'import tensorflow as tf\nfrom . import tf_lib as tfl, util, opt\nfrom numpy import random\nfrom .default import *\nfrom .mutators import relative\n\nclass ActorCritic(Default):\n  _options = [\n    Option(\'actor_layers\', type=int, nargs=\'+\', default=[128, 128]),\n    Option(\'fix_scopes\', type=bool, default=False),\n\n    Option(\'epsilon\', type=float, default=0.02),\n\n    Option(\'entropy_power\', type=float, default=1),\n    Option(\'entropy_scale\', type=float, default=0.001),\n    Option(\'evolve_entropy\', action=""store_true""),\n\n    Option(\'actor_weight\', type=float, default=1.),\n  ]\n\n  _members = [\n    #(\'optimizer\', opt.Optimizer),\n    (\'nl\', tfl.NL),\n  ]\n  \n  def __init__(self, input_size, embedAction, rlConfig, **kwargs):\n    Default.__init__(self, **kwargs)\n    self.embedAction = embedAction\n    self.action_set = list(range(embedAction.input_size))\n    self.rlConfig = rlConfig\n    self.evo_variables = []\n    \n    name = ""actor""\n    net = tfl.Sequential()\n    with tf.variable_scope(name):\n      prev_size = input_size\n      for i, next_size in enumerate(getattr(self, name + ""_layers"")):\n        with tf.variable_scope(""layer_%d"" % i):\n          net.append(tfl.FCLayer(prev_size, next_size, self.nl))\n        prev_size = next_size\n      \n      if self.fix_scopes:\n        net.append(tfl.FCLayer(prev_size, embedAction.size))\n      \n      if self.evolve_entropy:\n        self.entropy_scale = tf.Variable(self.entropy_scale, trainable=False, name=\'entropy_scale\')\n        self.evo_variables.append((""entropy_scale"", self.entropy_scale, relative(1.25)))\n      \n    if not self.fix_scopes:\n      with tf.variable_scope(\'actor\'):\n        net.append(tfl.FCLayer(prev_size, embedAction.size))\n    \n    self.net = net\n  \n  def epsilon_greedy(self, probs):\n    return (1. - self.epsilon) * probs + self.epsilon / self.embedAction.input_size\n  \n  def get_probs(self, inputs, delayed_actions):\n    """"""Computes probabilites for the actor.\n    \n    B is Batch dim/shape, can have rank > 1\n    C is Core output dim\n    E is action Embed dim\n    D is number of Delay steps\n    A is number of Actions\n    \n    Args:\n      inputs: Tensor of shape [B, C]\n      delayed_actions: list of D tensors with shapes [B, E].\n    Returns:\n      Tensor of shape [B, A] with action probabilities.\n    """"""\n    \n    inputs = tf.concat(axis=-1, values=[inputs] + delayed_actions)\n    net_outputs = self.net(inputs)\n    # FIXME: to_input is the wrong method. Should be embed_to_probs\n    probs = self.embedAction.to_input(net_outputs)\n    return self.epsilon_greedy(probs)\n  \n  def train_probs(self, inputs, delayed_actions, taken_action):\n    actor_probs = self.get_probs(inputs, delayed_actions)\n    log_actor_probs = tf.log(actor_probs)\n\n    entropy = - tfl.batch_dot(actor_probs, log_actor_probs)\n    entropy_avg = tfl.power_mean(self.entropy_power, entropy)\n    tf.summary.scalar(\'entropy_avg\', entropy_avg)\n    tf.summary.scalar(\'entropy_min\', tf.reduce_min(entropy))\n    #tf.summary.histogram(\'entropy\', entropy)\n\n    actions_1hot = tf.one_hot(taken_action, len(self.action_set))\n    taken_probs = tfl.batch_dot(actions_1hot, actor_probs)\n    taken_log_probs = tfl.batch_dot(actions_1hot, log_actor_probs)\n    \n    return taken_probs, taken_log_probs, entropy\n\n  def train(self, taken_log_probs, advantages, entropy):\n    actor_gain = taken_log_probs * tf.stop_gradient(advantages) + self.entropy_scale * entropy\n    return -tf.reduce_mean(actor_gain) * self.actor_weight\n\n  def getVariables(self):\n    return self.net.getVariables()\n  \n  def getPolicy(self, core_output, delayed_actions, **unused):\n    delayed_actions = tf.unstack(delayed_actions, axis=1)\n    return self.get_probs(core_output, delayed_actions)\n\n  def act(self, policy, verbose=False):\n    action = random.choice(self.action_set, p=policy)\n    return action, policy[action]\n\n'"
phillip/actor.py,9,"b'from phillip import RL\nimport tensorflow as tf\nfrom . import ssbm, util, ctype_util as ct, embed\nfrom .core import Core\nfrom .ac import ActorCritic\n\nclass Actor(RL.RL):\n  def __init__(self, **kwargs):\n    super(Actor, self).__init__(**kwargs)\n\n    with self.graph.as_default(), tf.device(self.device): \n      if self.predict: self._init_model(**kwargs)\n      self._init_policy(**kwargs)\n      \n      # build computation graph\n      self.input = ct.inputCType(ssbm.SimpleStateAction, [self.config.memory+1], ""input"")\n      self.input[\'delayed_action\'] = tf.placeholder(tf.int64, [self.config.delay], ""delayed_action"")\n      self.input[\'hidden\'] = util.deepMap(lambda size: tf.placeholder(tf.float32, [size], name=""input/hidden""), self.core.hidden_size)\n\n      batch_input = util.deepMap(lambda t: tf.expand_dims(t, 0), self.input)\n\n      states = self.embedGame(batch_input[\'state\'])\n      prev_actions = self.embedAction(batch_input[\'prev_action\'])\n      combined = tf.concat(axis=-1, values=[states, prev_actions])\n      history = tf.unstack(combined, axis=1)\n      inputs = tf.concat(axis=-1, values=history)\n      core_output, hidden_state = self.core(inputs, batch_input[\'hidden\'])\n      actions = self.embedAction(batch_input[\'delayed_action\'])\n      \n      if self.predict:\n        predict_actions = actions[:, :self.model.predict_steps]\n        delayed_actions = actions[:, self.model.predict_steps:]\n        core_output = self.model.predict(history, core_output, hidden_state, predict_actions, batch_input[\'state\'])\n      else:\n        delayed_actions = actions\n      \n      batch_policy = self.policy.getPolicy(core_output, delayed_actions), hidden_state\n      self.run_policy = util.deepMap(lambda t: tf.squeeze(t, [0]), batch_policy)\n\n      self.check_op = tf.no_op() if self.dynamic else tf.add_check_numerics_ops()\n      \n      self._finalize_setup()\n\n  def act(self, input_dict, verbose=False):\n    feed_dict = dict(util.deepValues(util.deepZip(self.input, input_dict)))\n    policy, hidden = self.sess.run([self.run_policy, self.check_op], feed_dict)[0]\n    return self.policy.act(policy, verbose), hidden\n'"
phillip/agent.py,2,"b'import tensorflow as tf\nfrom . import ssbm, actor, util, tf_lib as tfl, ctype_util as ct\nimport numpy as np\nfrom numpy import random, exp\nfrom .default import *\nfrom .menu_manager import characters\nimport pprint\nimport os\nimport time\nimport uuid\nimport pickle\nfrom . import reward\n\npp = pprint.PrettyPrinter(indent=2)\n\nclass Agent(Default):\n  _options = [\n    Option(\'char\', type=str, choices=characters.keys(), help=""character that this agent plays as""),\n    Option(\'verbose\', action=""store_true"", default=False, help=""print stuff while running""),\n    Option(\'reload\', type=int, default=60, help=""reload model every RELOAD seconds""),\n    Option(\'dump\', type=int, help=""dump experiences over network""),\n    Option(\'receive\', action=""store_true"", help=""receive parameters over network""),\n    Option(\'trainer_id\', type=str, help=""trainer slurm job id""),\n    Option(\'trainer_ip\', type=str, help=""trainer ip address""),\n    Option(\'swap\', type=int, default=0, help=""swap players 1 and 2""),\n    Option(\'disk\', type=int, default=0, help=""dump experiences to disk""),\n    Option(\'real_delay\', type=int, default=0, help=""amount of delay in environment (due to netplay)""),\n    Option(\'tb\', action=""store_true"", help=""log stats to tensorboard""),\n  ]\n  \n  _members = [\n    (\'actor\', actor.Actor)\n  ]\n  \n  def __init__(self, **kwargs):\n    Default.__init__(self, **kwargs)\n\n    self.pid = 0 if self.swap else 1\n    self.frame_counter = 0\n    self.action_chain = None\n    self.action_counter = np.random.randint(0, self.reload+1)  # to desynch actors\n    self.action = 0\n    self.actions = util.CircularQueue(self.actor.config.delay+1, 0)\n    self.probs = util.CircularQueue(self.actor.config.delay+1, 1.)\n    self.history = util.CircularQueue(array=((self.actor.config.memory+1) * ssbm.SimpleStateAction)())\n    \n    self.hidden = util.deepMap(np.zeros, self.actor.core.hidden_size)\n    self.prev_state = ssbm.GameMemory() # for rewards\n    avg_minutes = 30\n    self.avg_reward = util.MovingAverage(1./(self.actor.config.fps * 60 * avg_minutes))\n    \n    self.actor.restore()\n    self.global_step = self.actor.get_global_step()\n\n    self.dump = self.dump or self.trainer_id or self.trainer_ip\n    self.receive = self.dump or self.receive\n    \n    if self.receive:\n      self.update_ip()\n      self.make_sockets()\n\n    # prepare experience buffer\n    if self.dump or self.disk:\n      self.dump_size = self.actor.config.experience_length\n      self.dump_state_actions = (self.dump_size * ssbm.SimpleStateAction)()\n\n      self.dump_frame = 0\n      self.dump_count = 0\n    \n    if self.disk:\n      self.dump_dir = os.path.join(self.actor.path, \'experience\')\n      print(""Dumping to"", self.dump_dir)\n      util.makedirs(self.dump_dir)\n      self.dump_tag = uuid.uuid4().hex\n    \n    if self.tb:\n      self.writer = tf.summary.FileWriterCache.get(self.actor.path)\n\n  def update_ip(self):\n    ip_path = os.path.join(self.actor.path, \'ip\')\n    if os.path.exists(ip_path):\n      with open(ip_path, \'r\') as f:\n        trainer_ip = f.read()\n      print(""Read ip from disk"", self.trainer_ip)\n    elif self.trainer_id:\n      from . import om\n      trainer_ip = om.get_job_ip(self.trainer_id)\n      print(""Got ip from trainer jobid"", self.trainer_ip)\n    elif not self.trainer_ip:\n      import sys\n      sys.exit(""No trainer ip!"")\n    else:\n      trainer_ip = self.trainer_ip\n\n    self.ip_check_time = time.time()\n    if trainer_ip != self.trainer_ip:\n      self.trainer_ip = trainer_ip\n      return True\n    return False\n\n  def make_sockets(self):\n    """"""Updates trainer_ip from disk. Returns True if it has changed.""""""\n    try:\n      import nnpy\n    except ImportError as err:\n      print(""ImportError: {0}"".format(err))\n      sys.exit(""Install nnpy to dump experiences"")\n\n    if self.dump:\n      self.dump_socket = nnpy.Socket(nnpy.AF_SP, nnpy.PUSH)\n      sock_addr = ""tcp://%s:%d"" % (self.trainer_ip, util.port(self.actor.path + ""/experience""))\n      print(""Connecting experience socket to "" + sock_addr)\n      self.dump_socket.connect(sock_addr)\n\n    self.params_socket = nnpy.Socket(nnpy.AF_SP, nnpy.SUB)\n    self.params_socket.setsockopt(nnpy.SUB, nnpy.SUB_SUBSCRIBE, b"""")\n    self.params_socket.setsockopt(nnpy.SOL_SOCKET, nnpy.RCVMAXSIZE, -1)\n    \n    address = ""tcp://%s:%d"" % (self.trainer_ip, util.port(self.actor.path + ""/params""))\n    print(""Connecting params socket to"", address)\n    self.params_socket.connect(address)\n\n  def dump_state(self, state_action):\n    #print(self.frame_counter)\n    # TODO: figure out what\'s wrong with early frames\n    if self.frame_counter < 300:\n      return\n    \n    self.dump_state_actions[self.dump_frame] = state_action\n    \n    if self.dump_frame == 0:\n      self.initial = self.hidden\n\n    self.dump_frame += 1\n\n    if self.dump_frame == self.dump_size:\n      self.dump_count += 1\n      self.dump_frame = 0\n      \n      print(""Dumping"", self.dump_count)\n      \n      prepared = ssbm.prepareStateActions(self.dump_state_actions)\n      prepared[\'initial\'] = self.initial\n      prepared[\'global_step\'] = self.global_step\n      \n      if self.dump:\n        self.dump_socket.send(pickle.dumps(prepared))\n      \n      if self.disk:\n        path = os.path.join(self.dump_dir, self.dump_tag + \'_%d\' % self.dump_count)\n        with open(path, \'wb\') as f:\n          pickle.dump(prepared, f)\n\n  # Given the current state, determine the action you\'ll take and send it to the Smash emulator. \n  # pad is a ""game pad"" object, for interfacing with the emulator\n  def act(self, state, pad):\n    verbose = self.verbose and (self.frame_counter % 600 == 0)\n    self.frame_counter += 1\n    #verbose = False\n    \n    if self.action_chain is not None and not self.action_chain.done():\n      self.action_chain.act(pad, state.players[self.pid], self.char)\n      return\n    \n    r = reward.computeRewards([self.prev_state, state], damage_ratio=0)[0]\n    self.avg_reward.append(r)\n    ct.copy(state, self.prev_state)\n\n    score_per_minute = self.avg_reward.avg * self.actor.config.fps * 60\n    if self.tb and self.frame_counter % 3600:  # once per minute\n      summary = tf.summary.Summary()\n      summary.value.add(tag=\'score_per_minute\', simple_value=score_per_minute)\n      self.writer.add_summary(summary, self.actor.get_global_step())\n\n    if verbose:\n      print(""score_per_minute: %f"" % score_per_minute)\n    \n    current = self.history.peek()\n    current.state = state # copy\n    \n    # extra copying, oh well\n    if self.swap:\n      current.state.players[0] = state.players[1]\n      current.state.players[1] = state.players[0]\n    \n    current.prev_action = self.action\n\n    self.history.increment()\n    history = self.history.as_list()\n    input_dict = ct.vectorizeCTypes(ssbm.SimpleStateAction, history)\n    input_dict[\'hidden\'] = self.hidden\n    input_dict[\'delayed_action\'] = self.actions.as_list()[1:]\n    #print(input_dict[\'delayed_action\'])\n    \n    (action, prob), self.hidden = self.actor.act(input_dict, verbose=verbose)\n\n    #if verbose:\n    #  pp.pprint(ct.toDict(state.players[1]))\n    #  print(action)\n    \n    # the delayed action\n    self.action = self.actions.push(action)\n    current.action = self.action\n    current.prob = self.probs.push(prob)\n    \n    # send a more recent action if the environment itself is delayed (netplay)\n    real_action = self.actions[self.real_delay]\n    self.action_chain = self.actor.actionType.choose(real_action, self.actor.config.act_every)\n    self.action_chain.act(pad, state.players[self.pid], self.char)\n    \n    self.action_counter += 1\n    \n    if self.dump or self.disk:\n      self.dump_state(current)\n    \n    if self.reload:\n      if self.receive:\n        self.receive_params()\n      elif self.action_counter % (self.reload * self.actor.config.fps) == 0:\n        self.actor.restore()\n        self.global_step = self.actor.get_global_step()\n\n    # check if trainer ip has changed each hour\n    if self.receive and time.time() - self.ip_check_time > 60 * 60:\n      if self.update_ip():\n        self.make_sockets()\n\n  # When called, ask the learner if there are new parameters. \n  def receive_params(self):\n    import nnpy\n    num_blobs = 0\n    latest = None\n    \n    # get the latest update from the trainer\n    while True:\n      try:\n        #topic = self.socket.recv_string(zmq.NOBLOCK)\n        blob = self.params_socket.recv(nnpy.DONTWAIT)\n        params = pickle.loads(blob)\n        self.global_step = params[\'global_step:0\']\n        latest = params\n        """"""\n        if global_step > self.global_step:\n          self.global_step = global_step\n          latest = params\n        else:\n          print(""OUT OF ORDER?"")\n        """"""\n        num_blobs += 1\n      except nnpy.NNError as e:\n        if e.error_no == nnpy.EAGAIN:\n          # nothing to receive\n          break\n        # a real error\n        raise e\n    \n    if latest is not None:\n      print(""Unblobbing"", self.global_step)\n      self.actor.unblob(latest)\n'"
phillip/cg.py,13,"b'import tensorflow as tf\nfrom . import tf_lib as tfl\nfrom .default import *\n\ndef mag2(x):\n  return tf.reduce_sum(tf.square(x))\n\nclass ConjugateGradient(Default):\n  _options = [\n    Option(\'cg_iters\', type=int, default=10, help=""Maximum number of conjugate gradient iterations.""),\n    Option(\'residual_tol\', type=float, default=1e-10, help=""Minimum conjugate gradient residual tolerance.""),\n    Option(\'cg_damping\', type=float, default=1e-4, help=""Add a multiple of the identity function during conjugate gradient descent.""),\n  ]\n  \n  def __call__(self, f_Ax, b, debug=False):\n    """"""\n    Conjugate gradient descent. Finds the solution to Ax = b. The matrix A is represented\n    indirectly by a callable that performs multiplication by an arbitrary vector.\n    \n    Arguments:\n      f_Ax: Callable that, given a 1D tensor x, returns Ax.\n      b: The 1D target for Ax.\n      iters: Maximum number of conjugate gradient steps.\n      residual_tol: Stops cg iteration if the residual (squared) magnitude falls below this value.\n      damping: Adds a multiple of the identity matrix to A.\n      debug: Whether to return all the cg loop variables.\n    \n    Returns:\n      The solution to the system Ax = b.\n      If debug is True, then we return the tuple (n, x, p, r, rr).\n    \n    Source: Demmel p 312, adapted from OpenAI\'s numpy implementation.\n    """"""\n    \n    def body(n, x, p, r, rr):\n      z = f_Ax(p) + self.cg_damping * p\n      v = rr / tfl.dot(p, z)\n      x += v*p\n      r -= v*z\n      new_rr = mag2(r)\n      mu = new_rr / rr\n      p = r + mu * p\n\n      rr = new_rr\n      \n      n += 1\n      \n      return (n, x, p, r, rr)\n    \n    def cond(n, x, p, r, rr):\n      return tf.logical_and(tf.less(n, self.cg_iters), tf.less(self.residual_tol, rr))\n    \n    with tf.name_scope(\'cg\'):\n      n = tf.constant(0)\n      x = tf.zeros_like(b)\n      r = b\n      #x = b # this way cg_iters = 0 means we use the original direction\n      #r = b - f_Ax(x)\n      p = r\n      rr = mag2(r)\n      \n      n, x, p, r, rr = tf.while_loop(cond, body, (n, x, p, r, rr), back_prop=False)\n      \n      tf.summary.scalar(\'cg_iters\', n)\n      tf.summary.scalar(\'cg_error\', tf.sqrt(rr / mag2(b)))\n      tf.summary.scalar(\'cg_loss\', -0.5 * tfl.dot(x, b+r))\n      \n      if debug:\n        return n, x, p, r, rr\n      else:\n        return x\n\ndef test_cg():\n  sess = tf.Session()\n\n  with sess.graph.as_default():\n    A = tf.constant([[2, 1], [1, 5]], dtype=tf.float32)\n    b = tf.constant([-4, 7], dtype=tf.float32)\n    \n    f = lambda x: tf.reduce_sum(A * x, 1)\n    \n    cg = ConjugateGradient(cg_damping=0)\n    cg_output = cg(f, b, debug = True)\n  \n  results = sess.run(cg_output)\n  \n  print(*results)\n  \n  import numpy as np\n  expected = np.array([-3, 2])\n  x = results[1]\n  assert(np.linalg.norm(x - expected) < 1e-6)\n\n'"
phillip/core.py,5,"b'import tensorflow as tf\nfrom . import tf_lib as tfl\nfrom .default import *\nimport itertools\n\nclass Core(Default):\n  _options = [\n    Option(\'trunk_layers\', type=int, nargs=\'+\', default=[], help=""Non-recurrent layers.""),\n    Option(\'core_layers\', type=int, nargs=\'+\', default=[], help=""Recurrent layers.""),\n  ]\n  \n  _members = [\n    #(\'optimizer\', opt.Optimizer),\n    (\'nl\', tfl.NL),\n  ]\n  \n  def __init__(self, input_size, scope=\'core\', **kwargs):\n    Default.__init__(self, **kwargs)\n    \n    with tf.variable_scope(scope):\n      prev_size = input_size\n      \n      with tf.variable_scope(""trunk""):\n        self.trunk = tfl.Sequential()\n        for i, next_size in enumerate(self.trunk_layers):\n          with tf.variable_scope(""layer_%d"" % i):\n            self.trunk.append(tfl.FCLayer(prev_size, next_size, self.nl))\n          prev_size = next_size\n        self.variables = self.trunk.getVariables()\n  \n      if self.core_layers:\n        cells = []\n        for i, next_size in enumerate(self.core_layers):\n          with tf.variable_scope(\'layer_%d\' % i):\n            cell = tfl.GRUCell(prev_size, next_size)\n            cells.append(cell)\n            self.variables += cell.getVariables()\n          prev_size = next_size\n        self.core = tf.nn.rnn_cell.MultiRNNCell(cells)\n        self.hidden_size = self.core.state_size\n      else:\n        self.core = None\n        self.hidden_size = []\n      self.output_size = prev_size\n \n  def __call__(self, inputs, state):\n    trunk_outputs = self.trunk(inputs)\n    if self.core:\n      return self.core(trunk_outputs, state)\n    else:\n      return trunk_outputs, []\n\n'"
phillip/cpu.py,0,"b'""""""\nResponsible for interfacing with Dolphin to interface with SSBM, and handles things like:\n* character selection\n* stage selection\n* running Phillip within SSBM\n\nShould probably be renamed from CPU.py\n""""""\n\n\nfrom . import ssbm, state_manager, agent, util, movie\nfrom . import memory_watcher as mw\nfrom .state import *\nfrom .menu_manager import *\nimport os\nfrom .pad import *\nimport time\nfrom . import ctype_util as ct\nfrom numpy import random\nfrom .default import *\nimport functools\n\nclass CPU(Default):\n    _options = [\n      Option(\'tag\', type=int),\n      Option(\'user\', type=str, help=""dolphin user directory""),\n      Option(\'zmq\', type=int, default=0, help=""use zmq for memory watcher""),\n      Option(\'stage\', type=str, default=""final_destination"", choices=movie.stages.keys(), help=""which stage to play on""),\n      Option(\'enemy\', type=str, help=""load enemy agent from file""),\n      Option(\'enemy_reload\', type=int, default=0, help=""enemy reload interval""),\n      Option(\'enemy_id\', type=int, default=-1, help=""enemy population id""),\n      Option(\'cpu\', type=int, help=""enemy cpu level""),\n      Option(\'start\', type=int, default=1, help=""start game in endless time mode""),\n      Option(\'netplay\', type=str),\n      Option(\'frame_limit\', type=int, help=""stop after a given number of frames""),\n      Option(\'debug\', type=int, default=0),\n      Option(\'tcp\', type=int, default=0, help=""use zmq over tcp for memory watcher and pipe input""),\n      Option(\'windows\', action=\'store_true\', help=""set defaults for windows""),\n      Option(\'enemy_dump\', type=int, default=0, help=""also dump frames for the enemy""),\n    ] + [Option(\'p%d\' % i, type=str, choices=characters.keys(), default=""falcon"", help=""character for player %d"" % i) for i in [1, 2]]\n    \n    _members = [\n      (\'agent\', agent.Agent),\n    ]\n    \n    def __init__(self, **kwargs):\n        Default.__init__(self, **kwargs)\n\n        self.toggle = 0\n\n        self.user = os.path.expanduser(self.user)               \n\n        self.state = ssbm.GameMemory()\n        # track players 1 and 2 (pids 0 and 1)\n        self.sm = state_manager.StateManager([0, 1])\n        self.write_locations()\n\n        if self.tag is not None:\n            random.seed(self.tag)\n        \n        pids = [1, 0]\n        if self.agent.swap: pids.reverse()\n        self.pid, enemy_pid = pids\n        \n        self.pids = [self.pid]\n        self.agents = {self.pid: self.agent}\n        self.cpus = {self.pid: None}\n        self.characters = {self.pid: self.agent.char or self.p2}\n\n        if self.enemy:\n            enemy_kwargs = util.load_params(self.enemy, \'agent\')\n            enemy_kwargs.update(\n                reload=self.enemy_reload,\n                swap=not self.agent.swap,\n                dump=self.enemy_dump,\n                pop_id=self.enemy_id,\n                gpu=self.agent.actor.gpu,\n            )\n            enemy = agent.Agent(**enemy_kwargs)\n        \n            self.pids.append(enemy_pid)\n            self.agents[enemy_pid] = enemy\n            self.cpus[enemy_pid] = None\n            self.characters[enemy_pid] = enemy.char or self.p1\n        elif self.cpu:\n            self.pids.append(enemy_pid)\n            self.agents[enemy_pid] = None\n            self.cpus[enemy_pid] = self.cpu\n            self.characters[enemy_pid] = self.p1\n\n        \n        print(\'Creating MemoryWatcher.\')\n        self.tcp = self.tcp or self.windows\n        if self.tcp:\n          self.mw = mw.MemoryWatcherZMQ(port=5555)\n        else:\n          mwType = mw.MemoryWatcherZMQ if self.zmq else mw.MemoryWatcher\n          self.mw = mwType(path=self.user + \'/MemoryWatcher/MemoryWatcher\')\n        \n        pipe_dir = self.user + \'/Pipes/\'\n        print(\'Creating Pads at %s. Open dolphin now.\' % pipe_dir)\n        util.makedirs(self.user + \'/Pipes/\')\n        \n        pads = self.pids\n        if self.netplay:\n          pads = [0]\n        \n        paths = [pipe_dir + \'phillip%d\' % i for i in pads]\n        \n        makePad = functools.partial(Pad, tcp=self.tcp)\n        self.get_pads = util.async_map(makePad, paths)\n\n        self.init_stats()\n\n    def run(self, frames=None, dolphin_process=None):\n        try:\n            self.pads = self.get_pads()\n        except KeyboardInterrupt:\n            print(""Pipes not initialized!"")\n            return\n        \n        print(""Pipes initialized."")\n        \n        pick_chars = []\n        \n        tapA = [\n            (0, movie.pushButton(Button.A)),\n            (0, movie.releaseButton(Button.A)),\n        ]\n        \n        for pid, pad in zip(self.pids, self.pads):\n            actions = []\n            \n            cpu = self.cpus[pid]\n            locator = locateCSSCursor(pid)\n            \n            if cpu:\n                actions.append(MoveTo([0, 20], locator, pad, True))\n                actions.append(movie.Movie(tapA, pad))\n                actions.append(movie.Movie(tapA, pad))\n                actions.append(MoveTo([0, -14], locator, pad, True))\n                actions.append(movie.Movie(tapA, pad))\n                actions.append(MoveTo([cpu * 1.1, 0], locator, pad, True))\n                actions.append(movie.Movie(tapA, pad))\n                #actions.append(Wait(10000))\n            \n            actions.append(MoveTo(characters[self.characters[pid]], locator, pad))\n            actions.append(movie.Movie(tapA, pad))\n            \n            pick_chars.append(Sequential(*actions))\n        \n        pick_chars = Parallel(*pick_chars)\n        \n        enter_settings = Sequential(\n            MoveTo(settings, locateCSSCursor(self.pids[0]), self.pads[0]),\n            movie.Movie(tapA, self.pads[0])\n        )\n        \n        # sets the game mode and picks the stage\n        start_game = movie.Movie(movie.endless_netplay + movie.stages[self.stage], self.pads[0])\n        \n        actions = [pick_chars]\n        \n        if self.start:\n            actions += [enter_settings, start_game]\n        \n        #actions.append(Wait(600))\n        \n        self.navigate_menus = Sequential(*actions)\n        \n        print(\'Starting run loop.\')\n        self.start_time = time.time()\n        \n        try:\n            while self.game_frame != self.frame_limit:\n              self.advance_frame()\n        except KeyboardInterrupt:\n            if dolphin_process is not None:\n                dolphin_process.terminate()\n                #hack to get C-zmq dolphin to shutdown properly\n                #self.update_state()\n                #self.mw.advance()\n            self.print_stats()\n        \n        if dolphin_process is not None:\n            dolphin_process.terminate()\n\n    def init_stats(self):\n        self.game_frame = 0\n        self.total_frames = 1\n        self.skip_frames = 0\n        self.thinking_time = 0\n\n    def print_stats(self):\n        total_time = time.time() - self.start_time\n        frac_skipped = self.skip_frames / self.total_frames\n        frac_thinking = self.thinking_time * 1000 / self.total_frames\n        print(\'Total Time:\', total_time)\n        print(\'Total Frames:\', self.total_frames)\n        print(\'Average FPS:\', self.total_frames / total_time)\n        print(\'Fraction Skipped: {:.6f}\'.format(frac_skipped))\n        print(\'Average Thinking Time (ms): {:.6f}\'.format(frac_thinking))\n\n    def write_locations(self):\n        path = self.user + \'/MemoryWatcher/\'\n        util.makedirs(path)\n        print(\'Writing locations to:\', path)\n        with open(path + \'Locations.txt\', \'w\') as f:\n            f.write(\'\\n\'.join(self.sm.locations()))\n\n    def advance_frame(self):\n        # print(""advance_frame"")\n        last_frame = self.state.frame\n        \n        self.update_state()\n        if self.state.frame > last_frame:\n            skipped_frames = self.state.frame - last_frame - 1\n            if skipped_frames > 0:\n                self.skip_frames += skipped_frames\n                print(""Skipped frames "", skipped_frames)\n            self.total_frames += self.state.frame - last_frame\n            last_frame = self.state.frame\n\n            start = time.time()\n            self.make_action()\n            self.thinking_time += time.time() - start\n\n            if self.agent.verbose and self.state.frame % (15 * 60) == 0:\n                self.print_stats()\n        \n        self.mw.advance()\n\n    def update_state(self):\n        messages = self.mw.get_messages()\n        for message in messages:\n          self.sm.handle(self.state, *message)\n    \n    def spam(self, button, period=120):\n        self.toggle = (self.toggle + 1) % period\n        if self.toggle == 0:\n            self.pads[0].press_button(button)\n        elif self.toggle == 1:\n            self.pads[0].release_button(button)\n    \n    def make_action(self):\n        #menu = Menu(self.state.menu)\n        #print(menu)\n        if self.state.menu == Menu.Game.value:\n            self.game_frame += 1\n            \n            if self.debug and self.game_frame % 60 == 0:\n              print(\'action_frame\', self.state.players[0].action_frame)\n              items = list(util.deepItems(ct.toDict(self.state.players)))\n              print(\'max value\', max(items, key=lambda x: abs(x[1])))\n            \n            if self.game_frame <= 120:\n              return # wait for game to properly load\n            \n            for pid, pad in zip(self.pids, self.pads):\n                agent = self.agents[pid]\n                if agent:\n                    agent.act(self.state, pad)\n\n        elif self.state.menu in [menu.value for menu in [Menu.Characters, Menu.Stages]]:\n            self.game_frame = 0\n            self.navigate_menus.move(self.state)\n            \n            if self.navigate_menus.done():\n                for pid, pad in zip(self.pids, self.pads):\n                    if self.state.menu == Menu.Stages.value:\n                        if self.characters[pid] == \'sheik\':\n                            pad.press_button(Button.A)\n                    else:\n                        pad.send_controller(ssbm.RealControllerState.neutral)\n        \n        elif self.state.menu == Menu.PostGame.value:\n            self.spam(Button.START)\n        else:\n            print(""Weird menu state"", self.state.menu)\n\ndef runCPU(**kwargs):\n  CPU(**kwargs).run()\n\n'"
phillip/critic.py,10,"b'import tensorflow as tf\nfrom . import tf_lib as tfl\nfrom .default import *\nfrom .rl_common import *\n#from .embed import GameEmbedding\n\nclass Critic(Default):\n  _options = [\n    Option(\'critic_layers\', type=int, nargs=\'+\', default=[128, 128]),\n    Option(\'critic_weight\', type=float, default=.5),\n    Option(\'gae_lambda\', type=float, default=1., help=""Generalized Advantage Estimation""),\n    Option(\'fix_scopes\', type=bool, default=False),\n    Option(\'dynamic\', type=int, default=1, help=\'use dynamic loop unrolling\'),\n  ]\n  \n  _members = [\n    (\'rlConfig\', RLConfig),\n    (\'nl\', tfl.NL),\n  ]\n  \n  def __init__(self, input_size, scope=\'critic\', **kwargs):\n    Default.__init__(self, **kwargs)\n    \n    self.net = tfl.Sequential()\n    with tf.variable_scope(scope):\n      prev_size = input_size\n      for i, next_size in enumerate(self.critic_layers):\n        with tf.variable_scope(""layer_%d"" % i):\n          self.net.append(tfl.FCLayer(prev_size, next_size, self.nl))\n        prev_size = next_size\n      \n      if self.fix_scopes:\n        self.net.append(tfl.FCLayer(prev_size, 1))\n    \n    if not self.fix_scopes:\n      with tf.variable_scope(scope):\n        self.net.append(tfl.FCLayer(prev_size, 1))\n    \n    self.variables = self.net.getVariables()\n  \n  def __call__(self, inputs, rewards, prob_ratios, **unused):\n    values = tf.squeeze(self.net(inputs), [-1])\n    trainVs = values[:-1]\n    lastV = values[-1]\n    \n    lambda_ = tf.minimum(1., prob_ratios) * self.gae_lambda\n    targets = tfl.smoothed_returns(trainVs, rewards, self.rlConfig.discount, lambda_, lastV, dynamic=self.dynamic)\n    targets = tf.stop_gradient(targets)\n    advantages = targets - trainVs\n    \n    advantage_avg = tf.reduce_mean(advantages)\n    tfl.stats(advantages, \'advantage\')\n    \n    vLoss = tf.reduce_mean(tf.square(advantages))\n    tf.summary.scalar(\'v_loss\', vLoss)\n    tf.summary.scalar(""v_uev"", vLoss / tfl.sample_variance(targets))\n    \n    return vLoss * self.critic_weight, targets, advantages\n\n'"
phillip/ctype_util.py,6,"b'from ctypes import *\nfrom enum import IntEnum\nfrom itertools import product\nimport numpy as np\nfrom numpy import random\nimport tensorflow as tf\n\ndef copy(src, dst):\n    """"""Copies the contents of src to dst""""""\n    pointer(dst)[0] = src\n\nknownCTypes = set([c_float, c_uint, c_int, c_bool])\n\ndef toString(struct):\n  fields = [field + ""="" + str(getattr(struct, field)) for (field, _) in struct._fields_]\n  return ""%s{%s}"" % (struct.__class__.__name__, "", "".join(fields))\n\ndef toTuple(value, ctype=None):\n  if ctype is None:\n    ctype = type(value)\n  if ctype in knownCTypes:\n    return value\n  if issubclass(ctype, Structure):\n    return tuple(toTuple(getattr(value, f), t) for f, t in ctype._fields_)\n  # an array type\n  return tuple(toTuple(v, ctype._type_) for v in value)\n\ndef toDict(value, ctype=None):\n  if ctype is None:\n    ctype = type(value)\n  if ctype in knownCTypes:\n    return value\n  if issubclass(ctype, Structure):\n    return {f: toDict(getattr(value, f), t) for f, t in ctype._fields_}\n  # an array type\n  return [toDict(v, ctype._type_) for v in value]\n\ndef hashStruct(struct):\n  return hash(toTuple(struct))\n\ndef eqStruct(struct1, struct2):\n  return toTuple(struct1) == toTuple(struct2)\n\ndef toCType(t):\n  if issubclass(t, IntEnum):\n    return c_uint\n  return t\n\n# class decorator\ndef pretty_struct(cls):\n  cls._fields_ = [(name, toCType(t)) for name, t in cls._fields]\n  cls.__repr__ = toString\n  cls.__hash__ = hashStruct\n  cls.__eq__ = eqStruct\n  \n  cls.allValues = classmethod(allValues)\n  cls.randomValue = classmethod(randomValue)\n  \n  return cls\n\ndef allValues(ctype):\n  if issubclass(ctype, IntEnum):\n    return list(ctype)\n  \n  if issubclass(ctype, Structure):\n    names, types = zip(*ctype._fields)\n    values = [allValues(t) for t in types]\n    \n    def make(vals):\n      obj = ctype()\n      for name, val in zip(names, vals):\n        setattr(obj, name, val)\n      return obj\n  \n    return [make(vals) for vals in product(*values)]\n  \n  # TODO: handle bounded ints via _fields\n  # TODO: handle arrays\n  raise TypeError(""Unsupported type %s"" % ctype)\n\ndef randomValue(ctype):\n  if issubclass(ctype, IntEnum):\n    return random.choice(list(ctype))\n  \n  if issubclass(ctype, Structure):\n    obj = ctype()\n    for name, type_ in ctype._fields:\n      setattr(obj, name, randomValue(type_))\n    return obj\n  \n  # TODO: handle arrays\n  raise TypeError(""Unsupported type %s"" % ctype)\n\n# TODO: fill out the rest of this table\nctypes2TF = {\n  c_bool : tf.bool,\n  c_float : tf.float32,\n  c_double : tf.float64,\n  c_uint : tf.int64, # no tf.uint32 :(\n}\n\ndef inputCType(ctype, shape=None, name=""""):\n  if ctype in ctypes2TF:\n    return tf.placeholder(ctypes2TF[ctype], shape, name)\n  elif issubclass(ctype, Structure):\n    return {f : inputCType(t, shape, name + ""/"" + f) for (f, t) in ctype._fields_}\n  else: # assume an array type\n    base_type = ctype._type_\n    return [inputCType(base_type, shape, name + ""/"" + str(i)) for i in range(ctype._length_)]\n\ndef constantCTypes(ctype, values, name=""""):\n  if ctype in ctypes2TF:\n    return tf.constant(values, dtype=ctypes2TF[ctype], name=name)\n  elif issubclass(ctype, Structure):\n    return {f : constantCTypes(t, [getattr(v, f) for v in values], name + ""/"" + f) for (f, t) in ctype._fields_}\n  else: # assume an array type\n    base_type = ctype._type_\n    return [inputCType(base_type, [v[i] for v in values], name + ""/"" + str(i)) for i in range(ctype._length_)]\n\ndef feedCType(ctype, name, value, feed_dict=None):\n  if feed_dict is None:\n    feed_dict = {}\n  if ctype in ctypes2TF:\n    feed_dict[name + \':0\'] = value\n  elif issubclass(ctype, Structure):\n    for f, t in ctype._fields_:\n      feedCType(t, name + \'/\' + f, getattr(value, f), feed_dict)\n  else: # assume an array type\n    base_type = ctype._type_\n    for i in range(ctype._length_):\n      feedCType(base_type, name + \'/\' + str(i), value[i], feed_dict)\n\n  return feed_dict\n\ndef feedCTypes(ctype, name, values, feed_dict=None):\n  if feed_dict is None:\n    feed_dict = {}\n  if ctype in ctypes2TF:\n    feed_dict[name + \':0\'] = values\n  elif issubclass(ctype, Structure):\n    for f, t in ctype._fields_:\n      feedCTypes(t, name + \'/\' + f, [getattr(v, f) for v in values], feed_dict)\n  else: # assume an array type\n    base_type = ctype._type_\n    for i in range(ctype._length_):\n      feedCTypes(base_type, name + \'/\' + str(i), [v[i] for v in values], feed_dict)\n\n  return feed_dict\n\ndef vectorizeCTypes(ctype, values):\n  if ctype in ctypes2TF:\n    return np.array(values)\n  elif issubclass(ctype, Structure):\n    return {f : vectorizeCTypes(t, [getattr(v, f) for v in values]) for (f, t) in ctype._fields_}\n  else: # assume an array type\n    base_type = ctype._type_\n    return [vectorizeCTypes(base_type, [v[i] for v in values]) for i in range(ctype._length_)]\n\n'"
phillip/data.py,0,"b'short_hop = dict(\n  fox = 2,\n  ice_climbers = 2,\n  kirby = 2,\n  samus = 2,\n  sheik = 2,\n  pichu = 2,\n  pikachu = 2,\n  \n  dr_mario = 3,\n  mario = 3,\n  luigi = 3,\n  falcon = 3,\n  ness = 3,\n  young_link = 3,\n  gnw = 3,\n  marth = 3,\n  \n  peach = 4,\n  yoshi = 4,\n  dk = 4,\n  falco = 4,\n  puff = 4,\n  mewtwo = 4,\n  roy = 4,\n  \n  ganon = 5,\n  zelda = 5,\n  link = 5,\n  \n  bowser = 7\n)\n\n'"
phillip/default.py,0,"b'import pickle\n\nclass Default:\n  _options = []\n  \n  _members = []\n  \n  def __init__(self, init_members=True, **kwargs):\n    self._kwargs = kwargs\n    \n    for opt in self._options:\n      value = None\n      if opt.name in kwargs:\n        value = kwargs[opt.name]\n      if value is None:\n        value = opt.default\n      setattr(self, opt.name, value)\n    \n    if init_members:\n      self._init_members(**kwargs)\n  \n  def _init_members(self, **kwargs):\n    for name, cls in self._members:\n      setattr(self, name, cls(**kwargs))\n  \n  def items(self):\n    for opt in self._options:\n      yield opt.name, getattr(self, opt.name)\n    for name, _ in self._members:\n      yield name, getattr(self, name)\n  \n  def label(self):\n    label = self.__class__.__name__\n    for item in self.items():\n      label += ""_%s_%s"" % item\n    return label\n  \n  def __repr__(self):\n    fields = "", "".join(""%s=%s"" % (name, str(value)) for name, value in self.items())\n    return ""%s(%s)"" % (self.__class__.__name__, fields)\n  \n  @classmethod\n  def full_opts(cls):\n    yield from cls._options\n    for _, cls_ in cls._members:\n      yield from cls_.full_opts()\n  \n  def __getstate__(self):\n    return self._kwargs\n  def __setstate__(self, d):\n    self.__init__(**d)\n  \n  def dump(self, f):\n    pickle.dump(self._kwargs, f)\n  \n  @classmethod\n  def load(cls, f, **override):\n    kwargs = pickle.load(f)\n    kwargs.update(**override)\n    return cls(**kwargs)\n  \nclass Option:\n  def __init__(self, name, _skip=False, **kwargs):\n    self.name = name\n    self._skip = _skip\n    self.default = None\n    self.__dict__.update(kwargs)\n    self.kwargs = kwargs.copy()\n    \n    # don\'t pass default on to argparse\n    self.kwargs[\'default\'] = None\n  \n  def update_parser(self, parser):\n    if self._skip:\n      return\n    \n    flag = ""--"" + self.name\n    if flag in parser._option_string_actions:\n      #print(""warning: already have option %s. skipping""%self.name)\n      pass\n    else:\n      parser.add_argument(flag, **self.kwargs)\n\n'"
phillip/dolphin.py,0,"b'pipeConfig = """"""\nButtons/A = `Button A`\nButtons/B = `Button B`\nButtons/X = `Button X`\nButtons/Y = `Button Y`\nButtons/Z = `Button Z`\nMain Stick/Up = `Axis MAIN Y +`\nMain Stick/Down = `Axis MAIN Y -`\nMain Stick/Left = `Axis MAIN X -`\nMain Stick/Right = `Axis MAIN X +`\nTriggers/L = `Button L`\nTriggers/R = `Button R`\nD-Pad/Up = `Button D_UP`\nD-Pad/Down = `Button D_DOWN`\nD-Pad/Left = `Button D_LEFT`\nD-Pad/Right = `Button D_RIGHT`\nButtons/Start = `Button START`\nC-Stick/Up = `Axis C Y +`\nC-Stick/Down = `Axis C Y -`\nC-Stick/Left = `Axis C X -`\nC-Stick/Right = `Axis C X +`\n""""""\n#Triggers/L-Analog = `Axis L -+`\n#Triggers/R-Analog = `Axis R -+`\n\ndef generatePipeConfig(player, count):\n  config = ""[GCPad%d]\\n"" % (player+1)\n  config += ""Device = Pipe/%d/phillip%d\\n"" % (count, player)\n  config += pipeConfig\n  return config\n\ndef generateGCPadNew(pids=[1], pipe_count=True):\n  config = """"\n  count = 0\n  for p in sorted(pids):\n    config += generatePipeConfig(p, count if pipe_count else 0)\n    count += 1\n  return config\n\nimport phillip\ndatapath = phillip.path + \'/data\'\n\nwith open(datapath + \'/Dolphin.ini\', \'r\') as f:\n  dolphin_ini = f.read()\n\ngfx_ini = """"""\n[Settings]\nDumpFramesAsImages = {dump_ppm}\nDumpFramesToPPM = {dump_ppm}\nDumpFramesCounter = False\nCrop = True\nDumpFormat = {dump_format}\nDumpCodec = {dump_codec}\nDumpEncoder = {dump_encoder}\nDumpPath = {dump_path}\n""""""\n\ngale01_ini = """"""\n[Gecko_Enabled]\n$Netplay Community Settings\n""""""\n\nlcancel_ini = """"""\n$Flash White on Successful L-Cancel\n""""""\n#$Flash Red on Unsuccessful L-Cancel\n\n\ngale01_ini_fm = """"""\n[Core]\nCPUThread = True\nGPUDeterminismMode = fake-completion\nPollingMethod = OnSIRead\n[Gecko_Enabled]\n$Faster Melee Netplay Settings\n$Lag Reduction\n$Game Music ON\n""""""\n\nimport os\nfrom phillip import util\nfrom phillip.default import *\n\nclass SetupUser(Default):\n  _options = [\n    Option(\'gfx\', type=str, default=""Null"", help=""graphics backend""),\n    Option(\'dual_core\', type=int, default=1, help=""Use separate gpu and cpu threads.""),\n    Option(\'cpus\', type=int, nargs=\'+\', default=[1], help=""Which players are cpu-controlled.""),\n    Option(\'audio\', type=str, default=""No audio backend"", help=""audio backend""),\n    Option(\'speed\', type=int, default=0, help=\'framerate - 1=normal, 0=unlimited\'),\n    Option(\'dump_frames\', action=""store_true"", default=False, help=""dump frames from dolphin to disk""),\n    Option(\'dump_ppm\', action=""store_true"", help=""dump frames as ppm images""),\n    Option(\'pipe_count\', type=int, default=0, help=""Count pipes alphabetically. Turn on for older dolphins.""),\n    Option(\'netplay\', type=str),\n    Option(\'direct\', action=""store_true"", default=False, help=""netplay direct connect""),\n    Option(\'fullscreen\', action=""store_true"", default=False, help=""run dolphin with fullscreen""),\n    Option(\'iso_path\', type=str, default="""", help=""directory where you keep your isos""),\n    Option(\'human\', action=""store_true"", help=""set p1 to human""),\n    Option(\'fm\', action=""store_true"", help=""set up config for Faster Melee""),\n    Option(\'dump_format\', type=str, default=\'mp4\'),\n    Option(\'dump_codec\', type=str, default=\'h264\'),\n    Option(\'dump_encoder\', type=str, default=\'\'),\n    Option(\'dump_path\', type=str, default=\'\'),\n    Option(\'lcancel_flash\', action=""store_true"", help=""flash on lcancel""),\n  ]\n  \n  def __call__(self, user):\n    configDir = user + \'/Config\'\n    util.makedirs(configDir)\n    \n    if self.dump_ppm:\n      self.dump_frames = True\n\n    with open(configDir + \'/GCPadNew.ini\', \'w\') as f:\n      f.write(generateGCPadNew([0] if self.netplay else self.cpus, self.pipe_count))\n\n    with open(configDir + \'/Dolphin.ini\', \'w\') as f:\n      config_args = dict(\n        user=user,\n        gfx=self.gfx,\n        cpu_thread=bool(self.dual_core),\n        dump_frames=self.dump_frames,\n        audio=self.audio,\n        speed=self.speed,\n        netplay=self.netplay,\n        traversal=\'direct\' if self.direct else \'traversal\',\n        fullscreen=self.fullscreen,\n        iso_path=self.iso_path,\n        port1 = 12 if self.human else 6,\n      )\n      f.write(dolphin_ini.format(**config_args))\n    \n    with open(configDir + \'/GFX.ini\', \'w\') as f:\n      f.write(gfx_ini.format(\n        dump_ppm=self.dump_ppm,\n        dump_path=self.dump_path,\n        dump_codec=self.dump_codec,\n        dump_encoder=self.dump_encoder,\n        dump_format=self.dump_format))\n\n    gameSettings = user + \'/GameSettings\'\n    util.makedirs(gameSettings)\n    with open(gameSettings + \'/GALE01.ini\', \'w\') as f:\n      ini = gale01_ini_fm if self.fm else gale01_ini\n      if self.lcancel_flash:\n        ini += lcancel_ini\n      f.write(ini)\n\n    util.makedirs(user + \'/Dump/Frames\')\n\nimport subprocess\n\nclass DolphinRunner(Default):\n  _options = [\n    Option(\'exe\', type=str, default=\'dolphin-emu-headless\', help=""dolphin executable""),\n    Option(\'user\', type=str, help=""path to dolphin user directory""),\n    Option(\'iso\', type=str, default=""SSBM.iso"", help=""path to SSBM iso""),\n    Option(\'movie\', type=str, help=""path to dolphin movie file to play at startup""),\n    Option(\'setup\', type=int, default=1, help=""setup custom dolphin directory""),\n    Option(\'gui\', action=""store_true"", default=False, help=""run with graphics and sound at normal speed""),\n    Option(\'mute\', action=""store_true"", default=False, help=""mute game audio""),\n    Option(\'windows\', action=\'store_true\', help=""set defaults for windows""),\n    Option(\'netplay\', type=str, help=""join traversal server""),\n  ]\n  \n  _members = [\n    (\'setupUser\', SetupUser)\n  ]\n  \n  def __init__(self, **kwargs):\n    Default.__init__(self, init_members=False, **kwargs)\n    \n    if self.user is None:\n      import tempfile\n      self.user = tempfile.mkdtemp() + \'/\'\n    \n    print(""Dolphin user dir"", self.user)\n    \n    #if self.netplay: # need gui version to netplay\n    #  index = self.exe.rfind(\'dolphin-emu\') + len(\'dolphin-emu\')\n    #  self.exe = self.exe[:index]\n    \n    if self.gui or self.windows:\n      # switch from headless to gui\n      if self.exe.endswith(""-headless""):\n        #self.exe = self.exe[:-9]\n        self.exe = self.exe[:-9] + ""-nogui""\n      \n      # Note: newer dolphins use \'DX11\', but win-mw is an old fork.\n      kwargs.update(\n        speed = 1,\n        gfx = \'D3D\' if self.windows else \'OGL\',\n      )\n      \n      if self.mute:\n        kwargs.update(audio = \'No audio backend\')\n      else:\n        kwargs.update(audio = \'XAudio2\' if self.windows else \'Pulse\')\n      \n    if self.setup:\n      self._init_members(**kwargs)\n      self.setupUser(self.user)\n  \n  def __call__(self):\n    args = [self.exe, ""--user"", self.user]\n    if not self.netplay:\n      args += [""--exec"", self.iso]\n    if self.movie is not None:\n      args += [""--movie"", self.movie]\n    \n    print(args)\n    process = subprocess.Popen(args)\n    \n    if self.netplay:\n      import time\n      time.sleep(2) # let dolphin window spawn\n      \n      import pyautogui\n      #import ipdb; ipdb.set_trace()\n      pyautogui.click(150, 150)\n      #pyautogui.click(50, 50)\n      time.sleep(0.5)\n      pyautogui.hotkey(\'alt\', \'t\') # tools\n\n      time.sleep(0.5)\n      pyautogui.hotkey(\'n\') # netplay\n      \n      time.sleep(1) # allow netplay window time to spawn\n      \n      #return process\n      \n      #pyautogui.hotkey(\'down\') # traversal\n      \n      #for _ in range(3): # move to textbox\n      #  pyautogui.hotkey(\'tab\')\n      \n      #pyautogui.typewrite(self.netplay) # write traversal code\n      \n      #return process\n      \n      time.sleep(0.1)\n      # connect\n      #pyautogui.hotkey(\'tab\')\n      pyautogui.hotkey(\'enter\')\n\n      #import ipdb; ipdb.set_trace()\n    \n    return process\n\ndef main():\n  import argparse\n  \n  parser = argparse.ArgumentParser()\n  \n  for opt in DolphinRunner.full_opts():\n    opt.update_parser(parser)\n  \n  args = parser.parse_args()\n  \n  runner = DolphinRunner(**args.__dict__)\n  runner()\n\n\nif __name__ == ""__main__"":\n  main()\n'"
phillip/embed.py,33,"b'""""""\nConverts SSBM types to Tensorflow types. \n""""""\n\nimport tensorflow as tf\nfrom . import tf_lib as tfl, util, ssbm\nfrom .default import *\nimport math\n\nfloatType = tf.float32\n\ndef nullEmbedding(t):\n  shape = tf.shape(t)\n  shape = tf.concat(axis=0, values=[shape, [0]])\n  return tf.zeros(shape)\n\nnullEmbedding.size = 0\n\nclass FloatEmbedding(object):\n  def __init__(self, name, scale=None, bias=None, lower=-10., upper=10.):\n    self.name = name\n    self.scale = scale\n    self.bias = bias\n    self.lower = lower\n    self.upper = upper\n    self.size = 1\n  \n  def __call__(self, t, **_):\n    if t.dtype is not floatType:\n      t = tf.cast(t, floatType)\n    \n    if self.bias:\n      t += self.bias\n    \n    if self.scale:\n      t *= self.scale\n    \n    if self.lower:\n      t = tf.maximum(t, self.lower)\n    \n    if self.upper:\n      t = tf.minimum(t, self.upper)\n    \n    return tf.expand_dims(t, -1)\n  \n  def init_extract(self):\n    pass\n  \n  def extract(self, t):\n    if self.scale:\n      t /= self.scale\n    \n    if self.bias:\n      t -= self.bias\n    \n    #return t\n    return tf.squeeze(t, [-1])\n  \n  def to_input(self, t):\n    return t\n  \n  def distance(self, predicted, target):\n    if target.dtype is not floatType:\n      target = tf.cast(target, floatType)\n    \n    if self.scale:\n      target *= self.scale\n    \n    if self.bias:\n      target += self.bias\n    \n    predicted = tf.squeeze(predicted, [-1])\n    return tf.squared_difference(predicted, target)\n\nembedFloat = FloatEmbedding(""float"")\n\nclass OneHotEmbedding(object):\n  def __init__(self, name, size):\n    self.name = name\n    self.size = size\n    self.input_size = size\n  \n  def __call__(self, t, residual=False, **_):\n    one_hot = tf.one_hot(t, self.size, 1., 0.)\n    \n    if residual:\n      logits = math.log(self.size * 10) * one_hot\n      return logits\n    else:\n      return one_hot\n  \n  def to_input(self, logits):\n    return tf.nn.softmax(logits)\n  \n  def extract(self, embedded):\n    # TODO: pick a random sample?\n    return tf.argmax(t, -1)\n  \n  def distance(self, embedded, target):\n    logprobs = tf.nn.log_softmax(embedded)\n    target = self(target)\n    return -tfl.batch_dot(logprobs, target)\n\nclass LookupEmbedding(object):\n\n  def __init__(self, name, input_size, output_size):\n    self.name = name\n    with tf.variable_scope(name):\n      self.table = tfl.weight_variable([input_size, output_size], ""table"")\n  \n    self.size = output_size\n    self.input_size = input_size\n  \n  def __call__(self, indices, **kwargs):\n    return tf.nn.embedding_lookup(self.table, indices)\n  \n  def to_input(self, input_):\n    # FIXME: ""to_input"" is the wrong name. model.py uses it to go from logits (""residual"" embedding, used for prediction) to probabilites (""input"" embedding, used for passing through the network). Here we interpret it as going from embedding space (output_size) to probabilities (input_size), which changes the dimensionality and would break model.py.\n    logits = tfl.matmul(input_, tf.transpose(self.table))\n    return tf.nn.softmax(logits)\n\nclass StructEmbedding(object):\n  def __init__(self, name, embedding):\n    self.name = name\n    self.embedding = embedding\n    \n    self.size = 0\n    for _, op in embedding:\n      self.size += op.size\n  \n  def __call__(self, struct, **kwargs):\n    embed = []\n    \n    rank = None\n    for field, op in self.embedding:\n      with tf.name_scope(field):\n        t = op(struct[field], **kwargs)\n        \n        if rank is None:\n          rank = len(t.get_shape())\n        else:\n          assert(rank == len(t.get_shape()))\n        \n        embed.append(t)\n    return tf.concat(axis=rank-1, values=embed)\n  \n  def to_input(self, embedded):\n    rank = len(embedded.get_shape())\n    begin = (rank-1) * [0]\n    size = (rank-1) * [-1]\n    \n    inputs = []\n    offset = 0\n    \n    for _, op in self.embedding:\n      t = tf.slice(embedded, begin + [offset], size + [op.size])\n      inputs.append(op.to_input(t))\n      offset += op.size\n    \n    return tf.concat(axis=rank-1, values=inputs)\n  \n  def extract(self, embedded):\n    rank = len(embedded.get_shape())\n    begin (rank-1) * [0]\n    size = (rank-1) * [-1]\n    \n    struct = {}\n    offset = 0\n    \n    for field, op in self.embedding:\n      t = tf.slice(embedded, begin + [offset], size + [op.size])\n      struct[field] = op.extract(t)\n      offset += op.size\n    \n    return struct\n    \n  def distance(self, embedded, target):\n    rank = len(embedded.get_shape())\n    begin = (rank-1) * [0]\n    size = (rank-1) * [-1]\n    \n    distances = {}\n    offset = 0\n    \n    for field, op in self.embedding:\n      t = tf.slice(embedded, begin + [offset], size + [op.size])\n      distances[field] = op.distance(t, target[field])\n      offset += op.size\n    \n    return distances\n\nclass ArrayEmbedding(object):\n  def __init__(self, name, op, permutation):\n    self.name = name\n    self.op = op\n    self.permutation = permutation\n    self.size = len(permutation) * op.size\n  \n  def __call__(self, array, **kwargs):\n    embed = []\n    rank = None\n    for i in self.permutation:\n      with tf.name_scope(str(i)):\n        t = self.op(array[i], **kwargs)\n        if rank is None:\n          rank = len(t.get_shape())\n        else:\n          assert(rank == len(t.get_shape()))\n        \n        embed.append(t)\n    return tf.concat(axis=rank-1, values=embed)\n  \n  def to_input(self, embedded):\n    rank = len(embedded.get_shape())\n    ts = tf.split(axis=rank-1, num_or_size_splits=len(self.permutation), value=embedded)\n    inputs = list(map(self.op.to_input, ts))\n    return tf.concat(axis=rank-1, values=inputs)\n  \n  def extract(self, embedded):\n    # a bit suspect here, we can\'t recreate the original array,\n    # only the bits that were embedded. oh well\n    array = max(self.permutation) * [None]\n    \n    ts = tf.split(axis=tf.rank(embedded)-1, num_or_size_splits=len(self.permutation), value=embedded)\n    \n    for i, t in zip(self.permutation, ts):\n      array[i] = self.op.extract(t)\n    \n    return array\n\n  def distance(self, embedded, target):\n    distances = []\n  \n    ts = tf.split(axis=tf.rank(embedded)-1, num_or_size_splits=len(self.permutation), value=embedded)\n    \n    for i, t in zip(self.permutation, ts):\n      distances.append(self.op.distance(t, target[i]))\n    \n    return distances\n\nclass FCEmbedding(Default):\n  _options = [\n    Option(\'embed_nl\', type=bool, default=True)\n  ]\n  \n  _members = [\n    (\'nl\', tfl.NL)\n  ]\n\n  def __init__(self, name_, wrapper, size, **kwargs):\n    Default.__init__(self, **kwargs)\n    \n    self.name = name_\n    \n    if not self.embed_nl:\n      self.nl = None\n\n    self.wrapper = wrapper\n    self.fc = tfl.FCLayer(wrapper.size, size, nl=self.nl)\n    self.size = size\n  \n  def init_extract(self):\n    #self.extract = tfl.FCLayer(\n    pass\n\n  def __call__(self, x):\n    wrapped = self.wrapper(x)\n    y = self.fc(wrapped)\n    return y\n\nstickEmbedding = [\n  (\'x\', embedFloat),\n  (\'y\', embedFloat)\n]\n\nembedStick = StructEmbedding(""stick"", stickEmbedding)\n\n# TODO: embed entire controller\ncontrollerEmbedding = [\n  (\'button_A\', embedFloat),\n  (\'button_B\', embedFloat),\n  (\'button_X\', embedFloat),\n  (\'button_Y\', embedFloat),\n  (\'button_L\', embedFloat),\n  (\'button_R\', embedFloat),\n\n  #(\'trigger_L\', embedFloat),\n  #(\'trigger_R\', embedFloat),\n\n  (\'stick_MAIN\', embedStick),\n  (\'stick_C\', embedStick),\n]\n\nembedController = StructEmbedding(""controller"", controllerEmbedding)\n\nmaxAction = 0x017E\nnumActions = 1 + maxAction\n\nmaxCharacter = 32 # should be large enough?\nmaxJumps = 8\n\nclass PlayerEmbedding(StructEmbedding, Default):\n  _options = [\n    Option(\'action_space\', type=int, default=0, help=""embed actions in ACTION_SPACE dimensions (deprecated)""),\n    Option(\'xy_scale\', type=float, default=0.1, help=""scale xy coordinates""),\n    Option(\'shield_scale\', type=float, default=0.01),\n    Option(\'speed_scale\', type=float, default=0.5),\n    Option(\'omit_char\', type=bool, default=False),\n    Option(\'frame_scale\', type=float, default=.1, help=""scale frames""),\n  ]\n  \n  def __init__(self, **kwargs):\n    Default.__init__(self, **kwargs)\n    \n    embedAction = OneHotEmbedding(""action_state"", numActions)\n    if self.action_space:\n      embedAction = FCEmbedding(""action_state_fc"", embedAction, self.action_space, **kwargs)\n    \n    embedXY = FloatEmbedding(""xy"", scale=self.xy_scale)\n    embedSpeed = FloatEmbedding(""speed"", scale=self.speed_scale)\n    embedFrame = FloatEmbedding(""frame"", scale=self.frame_scale)\n\n    playerEmbedding = [\n      (""percent"", FloatEmbedding(""percent"", scale=0.01)),\n      (""facing"", embedFloat),\n      (""x"", embedXY),\n      (""y"", embedXY),\n      (""action_state"", embedAction),\n      # (""action_counter"", embedFloat),\n      (""action_frame"", FloatEmbedding(""action_frame"", scale=0.02)),\n      (""character"", nullEmbedding if self.omit_char else OneHotEmbedding(""character"", maxCharacter)),\n      (""invulnerable"", embedFloat),\n      (""hitlag_frames_left"", embedFrame),\n      (""hitstun_frames_left"", embedFrame),\n      (""jumps_used"", embedFloat),\n      (""charging_smash"", embedFloat),\n      (""shield_size"", FloatEmbedding(""shield_size"", scale=self.shield_scale)),\n      (""in_air"", embedFloat),\n      (\'speed_air_x_self\', embedSpeed),\n      (\'speed_ground_x_self\', embedSpeed),\n      (\'speed_y_self\', embedSpeed),\n      (\'speed_x_attack\', embedSpeed),\n      (\'speed_y_attack\', embedSpeed),\n\n      #(\'controller\', embedController)\n    ]\n    \n    StructEmbedding.__init__(self, ""player"", playerEmbedding)\n\n""""""\nmaxStage = 64 # overestimate\nstageSpace = 32\n\nwith tf.variable_scope(""embed_stage""):\n  stageHelper = tfl.makeAffineLayer(maxStage, stageSpace)\n\ndef embedStage(stage):\n  return stageHelper(one_hot(maxStage)(stage))\n""""""\n\nclass GameEmbedding(StructEmbedding, Default):\n  _options = [\n    Option(\'player_space\', type=int, default=0, help=""embed players into PLAYER_SPACE dimensions (deprecated)""),\n  ]\n  \n  _members = [\n    (\'embedPlayer\', PlayerEmbedding)\n  ]\n  \n  def __init__(self, **kwargs):\n    Default.__init__(self, **kwargs)\n    \n    if self.player_space:\n      self.embedPlayer = FCEmbedding(""player_fc"", self.embedPlayer, self.player_space, **kwargs)\n    \n    players = [0, 1]\n    #if self.swap: players.reverse()\n    \n    gameEmbedding = [\n      (\'players\', ArrayEmbedding(""players"", self.embedPlayer, players)),\n\n      #(\'frame\', c_uint),\n      #(\'stage\', embedStage)\n    ]\n    \n    StructEmbedding.__init__(self, ""game"", gameEmbedding)\n\n""""""\ndef embedEnum(enum):\n  return OneHotEmbedding(len(enum))\n\nsimpleAxisEmbedding = OneHotEmbedding(ssbm.axis_granularity)\n\nsimpleStickEmbedding = [\n  (\'x\', simpleAxisEmbedding),\n  (\'y\', simpleAxisEmbedding)\n]\n\nsimpleControllerEmbedding = [\n  (\'button\', embedEnum(ssbm.SimpleButton)),\n  (\'stick_MAIN\', StructEmbedding(simpleStickEmbedding)),\n]\n\n# NOTE: this is unused for now - we embed the previous action using a simple one-hot\nembedSimpleController = StructEmbedding(simpleControllerEmbedding)\n\naction_size = len(ssbm.simpleControllerStates)\n""""""\n\n'"
phillip/fields.py,0,"b'def getField(obj, field):\n    if isinstance(field, str):\n        return getattr(obj, field)\n    else:\n        return obj[field]\n\ndef setField(obj, field, val, validate=True):\n    if isinstance(field, str):\n        if validate:\n            if not hasattr(obj, field):\n                raise TypeError(""%s object has no field %s"" % (type(obj), field))\n        setattr(obj, field, val)\n    else: # assume an array\n        obj[field] = val\n\ndef getPath(obj, path):\n    for field in path:\n        obj = getField(obj, field)\n    return obj\n\n# doesn\'t work with empty path :(\ndef setPath(obj, path, val):\n    obj = getPath(obj, path[:-1])\n    setField(obj, path[-1], val)\n\n'"
phillip/learner.py,55,"b'import math\nimport tensorflow as tf\nfrom phillip.RL import RL\nfrom . import ssbm, util, ctype_util as ct, embed\nfrom .core import Core\nfrom .ac import ActorCritic\nfrom .critic import Critic\nfrom phillip import tf_lib as tfl\nfrom .mutators import relative\nfrom .default import Option\nfrom phillip import reward\nimport os\n\nclass Learner(RL):\n  \n  _options = RL._options + [\n    Option(\'train_model\', type=int, default=0),\n    Option(\'train_policy\', type=int, default=1),\n    # in theory train_critic should always equal train_policy; keeping them\n    # separate might help for e.g. debugging\n    Option(\'train_critic\', type=int, default=1),\n    Option(\'reward_decay\', type=float, default=1e-3),\n    Option(\'learning_rate\', type=float, default=1e-4),\n    Option(\'adam_epsilon\', type=float, default=1e-8, help=""epsilon for adam optimizer""),\n    Option(\'adam_beta1\', type=float, default=0.9, help=""Adam momentum decay""),\n    Option(\'clip_max_grad\', type=float, default=1.),\n    Option(\'evolve_learning_rate\', action=""store_true"", help=""false by default; if true, then"" \\\n      ""the learning rate is included in PBT among the things that get mutated. ""),\n    Option(\'explore_scale\', type=float, default=0., help=\'use prediction error as additional reward\'),\n    Option(\'evolve_explore_scale\', action=""store_true"", help=\'evolve explore_scale with PBT\'),\n    Option(\'unshift_critic\', action=\'store_true\', help=""don\'t shift critic forward in time""),\n    Option(\'batch_size\', type=int),\n    Option(\'neg_reward_scale\', type=float, default=1., help=""scale down negative rewards for more optimism""),\n    Option(\'unpredict_weight\', type=float, default=0., help=""regress delayed actions (computed with prediction) to the undelayed ones (computed on true states)""),\n\n    Option(\'damage_ratio\', type=float, default=0.01, help=""damage scale vs stocks""),\n    Option(\'distance_scale\', type=float, default=0., help=""distance pseudo-reward""),\n    Option(\'action_state_entropy_scale\', type=float, default=0, help=""reward unusual action states""),\n  ]\n\n  def __init__(self, debug=False, **kwargs):\n    super(Learner, self).__init__(**kwargs)\n\n    with self.graph.as_default(), tf.device(self.device): \n      # initialize predictive model, if either: \n      #  * you want to use the predictive model to ""undo delay""\n      #  * you want a predictive model to help you explore\n      # note: self.predict is perhaps a misnomer. \n      if self.predict or (self.train_model or self.explore_scale):\n        self._init_model(**kwargs)\n\n      if self.train_policy: \n        self._init_policy(**kwargs)\n      \n      # build computation graph\n      losses = {}\n\n      # to train the the policy, you have to train the critic. (self.train_policy and \n      # self.train_critic might both be false, if we\'re only training the predictive\n      # model)\n      if self.train_policy or self.train_critic:\n        print(""Creating critic."")\n        self.critic = Critic(self.core.output_size, **kwargs)\n\n      # experience = trajectory. usually a list of SimpleStateAction\'s. \n      self.experience = ct.inputCType(ssbm.SimpleStateAction, [None, self.config.experience_length], ""experience"")\n      # instantaneous rewards for all but the last state\n      self.experience[\'reward\'] = tf.placeholder(tf.float32, [None, self.config.experience_length-1], name=\'experience/reward\')\n      # manipulating time along the first axis is much more efficient\n      experience = util.deepMap(tf.transpose, self.experience)       \n      # initial state for recurrent networks\n      self.experience[\'initial\'] = tuple(tf.placeholder(tf.float32, [None, size], name=\'experience/initial/%d\' % i) for i, size in enumerate(self.core.hidden_size))\n      experience[\'initial\'] = self.experience[\'initial\']\n\n      # auxiliary reward computation\n      \n      # rewards = experience[\'reward\']\n      # TODO: move these into reward.py?\n      kill_death = reward.compute_rewards(experience[\'state\'], damage_ratio=0., lib=tf)\n      tfl.stats(kill_death * self.config.fps * 60, \'kill_death\')\n      \n      rewards = reward.compute_rewards(experience[\'state\'], damage_ratio=self.damage_ratio, lib=tf)\n      avg_reward, _ = tfl.stats(rewards, \'reward\')\n\n      # distance rewards encourage agents to interact more\n      distances, distance_rewards = reward.pseudo_rewards(experience[\'state\'], reward.distance, 1., lib=tf)\n      tfl.stats(distances, \'distances\')\n      tfl.stats(distance_rewards, \'distance_rewards\')\n      rewards += self.distance_scale * distance_rewards\n\n      # action_state_entropy encourages agent to explore new action states\n      own_action_states = experience[\'state\'][\'players\'][1][\'action_state\']\n      \n      action_state_logits = tf.Variable(tf.zeros([embed.numActions]), name=""action_state_logits"")\n      broadcast_zeros = tf.expand_dims(tf.zeros_like(own_action_states, dtype=tf.float32), -1)  # [T, B, 1]\n      expanded_action_state_logits = action_state_logits + broadcast_zeros  # [T, B, A]\n\n      action_state_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        logits=expanded_action_state_logits, labels=own_action_states)\n\n      # don\'t need to scale because it is independent of the other losses\n      losses[\'global_action_state\'] = tf.reduce_mean(action_state_loss)\n      tf.summary.scalar(\'action_state_loss\', losses[\'global_action_state\'])\n\n      action_state_value = tf.minimum(action_state_loss, math.log(embed.numActions) + 2)\n      action_state_rewards = action_state_value[1:] - action_state_value[:-1]\n      rewards += self.action_state_entropy_scale * action_state_rewards\n      tfl.stats(action_state_rewards, \'action_state_rewards\')\n\n      action_state_entropy = -tf.reduce_sum(\n        tf.nn.softmax(action_state_logits) *\n        tf.nn.log_softmax(action_state_logits))\n      tf.summary.scalar(\'action_state_entropy\', action_state_entropy)\n\n      # main RL training here\n\n      states = self.embedGame(experience[\'state\'])\n      prev_actions = self.embedAction(experience[\'prev_action\'])\n      combined = tf.concat(axis=2, values=[states, prev_actions])\n      actions = self.embedAction(experience[\'action\'])\n\n      memory = self.config.memory\n      delay = self.config.delay\n      length = self.config.experience_length - memory\n      history = [combined[i:i+length] for i in range(memory+1)]\n      inputs = tf.concat(axis=-1, values=history)\n      \n      # this should be handled by the core itself\n      if self.core.core:\n        inputs = self.core.trunk(inputs)\n        def f(prev, current_input):\n          _, prev_state = prev\n          return self.core.core(current_input, prev_state)\n        batch_size = tf.shape(self.experience[\'reward\'])[0]\n        dummy_output = tf.zeros(tf.stack([batch_size, tf.constant(self.core.output_size)]))\n        scan_fn = tf.scan if self.dynamic else tfl.scan\n        core_outputs, hidden_states = scan_fn(f, inputs, (dummy_output, experience[\'initial\']))\n      else:\n        core_outputs, hidden_states = self.core(inputs, experience[\'initial\'])\n\n      actions = actions[memory:]\n      rewards = rewards[memory:]\n      \n      print(""Creating train ops"")\n\n      train_ops = []\n      loss_vars = []\n\n      if self.train_model or self.predict:\n        model_loss, predicted_core_outputs = self.model.train(history, core_outputs, hidden_states, actions, experience[\'state\'])\n      if self.train_model:\n        #train_ops.append(train_model)\n        losses[\'model\'] = model_loss\n        loss_vars.extend(self.model.getVariables())\n      \n      if self.train_policy:\n        if self.predict:\n          predict_steps = self.model.predict_steps\n          actor_inputs = predicted_core_outputs\n        else:\n          predict_steps = 0\n          actor_inputs = core_outputs\n        \n        delay_length = length - delay\n        actor_inputs = actor_inputs[:delay_length]\n\n        # delayed_actions is a D+1-P length list of shape [T-M-D, B] tensors\n        # The valid state indices are [M+P, T+P-D)\n        # Element i corresponds to the i\'th queued up action: 0 is the action about to be taken, D-P was the action chosen on this frame.\n        delayed_actions = []\n        for i in range(predict_steps, delay):\n          delayed_actions.append(actions[i:i+delay_length])\n        taken_actions = experience[\'action\'][memory+delay:]\n        train_probs, train_log_probs, entropy = self.policy.train_probs(actor_inputs, delayed_actions, taken_actions)\n        \n        behavior_probs = experience[\'prob\'][memory+delay:] # these are the actions we can compute probabilities for\n        prob_ratios = tf.minimum(train_probs / behavior_probs, 1.)\n        self.kls = -tf.reduce_mean(tf.log(prob_ratios), 0)\n        tfl.stats(self.kls, \'kl\')\n      else:\n        prob_ratios = tf.ones_like() # todo\n\n      if self.explore_scale:\n        if self.evolve_explore_scale:\n          self.explore_scale = tf.Variable(self.explore_scale, trainable=False, name=\'explore_scale\')\n          self.evo_variables.append((\'explore_scale\', self.explore_scale, relative(1.5)))\n        \n        distances, _ = self.model.distances(history, core_outputs, hidden_states, actions, experience[\'state\'], predict_steps=1)\n        distances = tf.add_n(list(util.deepValues(distances))) # sum over different state components\n        explore_rewards = self.explore_scale * distances[0]\n        explore_rewards = tf.stop_gradient(explore_rewards)\n        tfl.stats(explore_rewards, \'explore_rewards\')\n        rewards += explore_rewards\n\n      # build the critic (which you\'ll also need to train the policy)\n      if self.train_policy or self.train_critic:\n        shifted_core_outputs = core_outputs[:delay_length] if self.unshift_critic else core_outputs[delay:]\n        delayed_rewards = rewards[delay:]\n        delayed_rewards = tf.nn.relu(delayed_rewards) - self.neg_reward_scale * tf.nn.relu(-delayed_rewards)\n        critic_loss, targets, advantages = self.critic(shifted_core_outputs, delayed_rewards, prob_ratios[:-1])\n      \n      if self.train_critic:\n        losses[\'critic\'] = critic_loss\n        loss_vars.extend(self.critic.variables)\n      \n      if self.train_policy:\n        policy_loss = self.policy.train(train_log_probs[:-1], advantages, entropy[:-1])\n        losses[\'policy\'] = policy_loss\n        loss_vars.extend(self.policy.getVariables())\n        \n        if self.unpredict_weight:\n          true_states = core_outputs[predict_steps:]\n          true_probs = self.policy.get_probs(true_states, delayed_actions)\n          true_probs = tf.stop_gradient(true_probs)  # these are supervised targets\n          # some redundancy here with train_probs\n          predicted_probs = self.policy.get_probs(actor_inputs, delayed_actions)\n          unpredict_kl = tfl.batch_dot(tf.log(true_probs) - tf.log(predicted_probs), true_probs)\n          unpredict_kl = tf.reduce_mean(unpredict_kl)\n          tf.summary.scalar(\'unpredict_kl\', unpredict_kl)\n          losses[\'unpredict\'] = self.unpredict_weight * unpredict_kl\n\n      if self.evolve_learning_rate:\n        self.learning_rate = tf.Variable(self.learning_rate, trainable=False, name=\'learning_rate\')\n        self.evo_variables.append((\'learning_rate\', self.learning_rate, relative(1.5)))\n\n      # losses = {k: tf.check_numerics(v, k, ""check_""+k) for k, v in losses.items()}\n      total_loss = tf.add_n(list(losses.values()))\n      with tf.variable_scope(\'train\'):\n        optimizer = tf.train.AdamOptimizer(self.learning_rate, epsilon=self.adam_epsilon, beta1=self.adam_beta1)\n        gvs = optimizer.compute_gradients(total_loss)\n        gvs = [(tf.check_numerics(g, v.name), v) for g, v in gvs]\n        gs, vs = zip(*gvs)\n        \n        norms = tf.stack([tf.norm(g) for g in gs])\n        max_norm = tf.reduce_max(norms)\n        tf.summary.scalar(\'max_grad_norm\', max_norm)\n        capped_gs = [tf.clip_by_norm(g, self.clip_max_grad) for g in gs]\n        train_op = optimizer.apply_gradients(zip(capped_gs, vs))\n        train_ops.append(train_op)\n      \n      print(""Created train op(s)"")\n      \n      misc_ops = []\n      \n      if not self.dynamic:\n        misc_ops.append(tf.add_check_numerics_ops())\n      \n      if self.pop_id >= 0:\n        self.reward = tf.Variable(0., trainable=False, name=\'avg_reward\')\n        tf.summary.scalar(\'avg_reward\', self.reward)\n        new_reward = (1. - self.reward_decay) * self.reward + self.reward_decay * avg_reward\n        misc_ops.append(tf.assign(self.reward, new_reward))\n      \n      self.mutators = []\n      for name, evo_variable, mutator in self.evo_variables:\n        tf.summary.scalar(name, evo_variable, family=\'evolution\')\n        self.mutators.append(tf.assign(evo_variable, mutator(evo_variable)))\n      \n      self.summarize = tf.summary.merge_all()\n      self.num_steps_per_batch = self.batch_size * self.config.experience_length * self.config.act_every\n      misc_ops.append(tf.assign_add(self.global_step, self.num_steps_per_batch))\n      self.misc = tf.group(*misc_ops)\n      self.train_ops = tf.group(*train_ops)\n\n      print(""Creating summary writer at logs/%s."" % self.name)\n      #self.writer = tf.summary.FileWriter(\'logs/\' + self.name)#, self.graph)\n      self.writer = tf.summary.FileWriter(self.path)\n\n      self._finalize_setup()\n\n  def train(self, experiences,\n            batch_steps=1,\n            train=True,\n            log=True,\n            zipped=False,\n            retrieve_kls=False, \n            **kwargs):\n    if not zipped:\n      experiences = util.deepZip(*experiences)\n    \n    input_dict = dict(util.deepValues(util.deepZip(self.experience, experiences)))\n    \n    """"""\n    saved_data = self.sess.run(self.saved_data, input_dict)\n    handles = [t.handle for t in saved_data]\n    \n    saved_dict = dict(zip(self.placeholders, handles))\n    """"""\n    \n    run_dict = dict(\n      global_step = self.global_step,\n      misc = self.misc\n    )\n    \n    if train:\n      run_dict.update(train=self.train_ops)\n    \n    if retrieve_kls:\n      run_dict.update(kls=self.kls)\n    \n    if self.profile:\n      run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n      run_metadata = tf.RunMetadata()\n      print(\'Profiling enabled, disabling logging!\')\n      log = False # logging eats time?\n    else:\n      run_options = None\n      run_metadata = None\n\n    if log:\n      run_dict.update(summary=self.summarize)\n    \n    outputs = []\n    \n    for _ in range(batch_steps):\n      try:\n        results = self.sess.run(run_dict, input_dict,\n            options=run_options, run_metadata=run_metadata)\n      except tf.errors.InvalidArgumentError as e:\n        import pickle\n        with open(os.path.join(self.path, \'error_frame\'), \'wb\') as f:\n          pickle.dump(experiences, f)\n        raise e\n      #print(\'After run: %s\' % resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n      \n      outputs.append(results)\n      global_step = results[\'global_step\']\n      if log:\n        summary_str = results[\'summary\']\n        self.writer.add_summary(summary_str, global_step)\n      if self.profile:\n        # Create the Timeline object, and write it to a json\n        from tensorflow.python.client import timeline\n        tl = timeline.Timeline(run_metadata.step_stats)\n        ctf = tl.generate_chrome_trace_format()\n        path = \'timelines/%s\' % self.name\n        util.makedirs(path)\n        with open(\'%s/%d.json\' % (path, global_step), \'w\') as f:\n          f.write(ctf)\n        #self.writer.add_run_metadata(run_metadata, \'step %d\' % global_step, global_step)\n    \n    return outputs\n'"
phillip/memory_watcher.py,0,"b'import binascii\nfrom . import util\nimport os\nimport sys\nimport socket\n\ndef parseMessage(message):\n  lines = message.splitlines()\n  \n  assert(len(lines) % 2 == 0)\n\n  diffs = util.chunk(lines, 2)\n  \n  for diff in diffs:\n    diff[1] = binascii.unhexlify(diff[1].zfill(8))\n  \n  return diffs\n\nclass MemoryWatcherZMQ:\n  def __init__(self, path=None, port=None, pull=False):\n    try:\n      import zmq\n    except ImportError as err:\n      print(""ImportError: {0}"".format(err))\n      sys.exit(""Need zmq installed."")\n\n    self.pull = pull or port\n    context = zmq.Context()\n    self.socket = context.socket(zmq.PULL if self.pull else zmq.REP)\n    if path:\n      self.socket.bind(""ipc://"" + path)\n    elif port:\n      self.socket.bind(""tcp://127.0.0.1:%d"" % port)\n    else:\n      raise Exception(""Must specify path or port."")\n    \n    self.messages = None\n  \n  def get_messages(self):\n    if self.messages is None:\n      message = self.socket.recv()\n      message = message.decode(\'utf-8\')\n      self.messages = parseMessage(message)\n    \n    return self.messages\n  \n  def advance(self):\n    if not self.pull:\n      self.socket.send(b\'\')\n    self.messages = None\n\nclass MemoryWatcher:\n  """"""Reads and parses game memory changes.\n\n  Pass the location of the socket to the constructor, then either manually\n  call next() on this class to get a single change, or else use it like a\n  normal iterator.\n  """"""\n  def __init__(self, path):\n    """"""Creates the socket if it does not exist, and then opens it.""""""\n    try:\n      os.unlink(path)\n    except OSError:\n      pass\n    self.sock = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)\n    self.sock.settimeout(1)\n    self.sock.bind(path)\n\n  def __del__(self):\n    """"""Closes the socket.""""""\n    self.sock.close()\n  \n  def get_messages(self):\n    try:\n      data = self.sock.recv(1024).decode(\'utf-8\')\n      data = data.strip(\'\\x00\')\n    except socket.timeout:\n      return []\n    \n    return parseMessage(data)\n    \n  def advance(self):\n    pass\n\n'"
phillip/menu_manager.py,0,"b'import math\n\nfrom .pad import *\n\ncharacters = dict(\n  fox = (-23.5, 11.5),\n  falco = (-30, 11),\n  falcon = (18, 18),\n  roy = (18, 5),\n  marth = (11, 5),\n  zelda = (11, 11),\n  sheik = (11, 11),\n  mewtwo = (-2, 5),\n  luigi = (-16, 18),\n  puff = (-10, 5),\n  kirby = (-2, 11),\n  peach = (-2, 18),\n  ganon = (23, 18),\n  samus = (3, 11),\n  bowser = (-9, 19),\n  yoshi = (5, 18),\n  dk = (11, 18),\n)\n\nsettings = (0, 24)\n\nstaged = dict(\n  final_destination = (0, 0),\n)\n\ndef locateCSSCursor(pid):\n  def locate(state):\n    player = state.players[pid]\n    return (player.cursor_x, player.cursor_y)\n  return locate\n\ndef locateSSSCursor(state):\n  return (state.sss_cursor_x, state.sss_cursor_y)\n\nclass MoveTo:\n  def __init__(self, target, locator, pad, relative=False):\n    self.target = target\n    self.locator = locator\n    self.pad = pad\n    self.reached = False\n    self.relative = relative\n    \n  def move(self, state):\n    x, y = self.locator(state)\n    \n    if self.relative:\n      self.target[0] += x\n      self.target[1] += y\n      self.relative = False\n    \n    dx = self.target[0] - x\n    dy = self.target[1] - y\n    mag = math.sqrt(dx * dx + dy * dy)\n    if mag < 0.5:\n      self.pad.tilt_stick(Stick.MAIN, 0.5, 0.5)\n      self.reached = True\n    else:\n      self.pad.tilt_stick(Stick.MAIN, 0.4 * (dx / (mag+2)) + 0.5, 0.4 * (dy / (mag+2)) + 0.5)\n      self.reached = False\n\n  def done(self):\n    return self.reached\n\nclass Wait:\n  def __init__(self, frames):\n    self.frames = frames\n  \n  def done(self):\n    return self.frames == 0\n  \n  def move(self, state):\n    self.frames -= 1\n\nclass Action:\n  def __init__(self, action, pad):\n    self.action = action\n    self.pad = pad\n    self.acted = False\n  \n  def done(self):\n    return self.acted\n  \n  def move(self, state):\n    self.action(self.pad)\n    self.acted = True\n\nclass Sequential:\n  def __init__(self, *actions):\n    self.actions = actions\n    self.index = 0\n  \n  def move(self, state):\n    if not self.done():\n      action = self.actions[self.index]\n      if action.done():\n        self.index += 1\n      else:\n        action.move(state)\n\n  def done(self):\n    return self.index == len(self.actions)\n\nclass Parallel:\n  def __init__(self, *actions):\n    self.actions = actions\n    self.complete = False\n  \n  def move(self, state):\n    self.complete = True\n    for action in self.actions:\n      if not action.done():\n        action.move(state)\n        self.complete = False\n  \n  def done(self):\n    return self.complete\n\n'"
phillip/model.py,17,"b'import itertools\nimport tensorflow as tf\nfrom .default import *\nfrom . import embed, ssbm, tf_lib as tfl, util\nfrom .rl_common import *\n\nclass Model(Default):\n  _options = [\n    Option(""model_layers"", type=int, nargs=\'+\', default=[256]),\n\n    Option(\'model_weight\', type=float, default=1.),\n    Option(\'predict_steps\', type=int, default=1, help=""number of future frames to predict""),\n    Option(\'dynamic\', type=int, default=1, help=\'use dynamic loop unrolling\'),\n  ]\n  \n  _members = [\n    (\'nl\', tfl.NL),\n  ]\n  \n  def __init__(self, embedGame, action_size, core, config, scope=""model"", **kwargs):\n    Default.__init__(self, **kwargs)\n    self.embedGame = embedGame\n    self.rlConfig = config\n    self.action_size = action_size\n    self.core = core\n    \n    self.input_size = core.output_size + action_size\n    self.while_loop = tf.while_loop if self.dynamic else tfl.while_loop\n        \n    with tf.variable_scope(scope):\n      net = tfl.Sequential()\n      \n      prev_size = self.input_size\n      \n      for i, next_size in enumerate(self.model_layers):\n        with tf.variable_scope(""layer_%d"" % i):\n          net.append(tfl.FCLayer(prev_size, next_size, self.nl))\n        prev_size = next_size\n      \n      with tf.variable_scope(""output""):\n        self.delta_layer = tfl.FCLayer(prev_size, self.embedGame.size, bias_init=tfl.constant_init(0.))\n        self.new_layer = tfl.FCLayer(prev_size, self.embedGame.size)\n        self.forget_layer = tfl.FCLayer(prev_size, self.embedGame.size, nl=tf.sigmoid, bias_init=tfl.constant_init(1.))\n      \n      self.net = net\n    \n    layers = [self.net, self.delta_layer, self.new_layer, self.forget_layer]\n    self.variables = list(itertools.chain.from_iterable([l.getVariables() for l in layers]))\n  \n  def getVariables(self):\n    return self.variables\n\n  def apply(self, inputs, last):\n    outputs = self.net(inputs)\n    \n    delta = self.delta_layer(outputs)\n    new = self.new_layer(outputs)\n    forget = self.forget_layer(outputs)\n    \n    return forget * (last + delta) + (1. - forget) * new\n  \n  def distances(self, history, core_outputs, hidden_states, actions, raw_state,\n    predict_steps, **unused):\n    """"""Computes the model\'s loss on a given set of transitions.\n    \n    Args:\n      history: List of game states from past to present, of length M+1. Each element is a tensor of shape [T-M, B, F].\n        The states with valid histories are indexed [M, T).\n      core_outputs: Outputs from the agent Core, of shape [T-M, B, O).\n      hidden_states: Structure of state Tensors of shape [T-M, B, S).\n      actions: Integer tensor of actions taken, of shape [T-M, B], corresponding to times [M, T).\n      raw_state: Unembedded state structure of shape [T, B] tensors. Used for getting the first residual state.\n      predict_steps: Number of steps to predict.\n    \n    Returns:\n      A GameMemory-shaped structure of distances - tensors of shape [P, T-M-P, B] \n    """"""\n    memory = self.rlConfig.memory\n    length = self.rlConfig.experience_length - memory - predict_steps\n\n    cut = (lambda t: t[:-predict_steps]) if predict_steps > 0 else (lambda t: t)\n    history = [cut(h) for h in history]\n    core_outputs = cut(core_outputs)\n    hidden_states = util.deepMap(cut, hidden_states)\n    current_actions = tfl.windowed(actions, predict_steps)\n    \n    states = util.deepMap(lambda t: tfl.windowed(t[memory:], predict_steps), raw_state)\n    begin_states = util.deepMap(lambda t: t[0], states)\n    target_states = util.deepMap(lambda t: t[1:], states)\n    \n    last_states = self.embedGame(begin_states, residual=True)\n    ta_fn = tf.TensorArray if self.dynamic else tfl.TensorArray\n    predicted_ta = ta_fn(last_states.dtype, size=predict_steps, element_shape=last_states.get_shape())\n    \n    def predict_step(i, prev_history, prev_core, prev_hidden, prev_state, predicted_ta):\n      current_action = current_actions[i]\n      inputs = tf.concat(axis=-1, values=[prev_core] + [current_action])\n      \n      predicted_state = self.apply(inputs, prev_state)\n      \n      next_state = self.embedGame.to_input(predicted_state)\n      next_combined = tf.concat(axis=-1, values=[next_state, current_action])\n      next_history = prev_history[1:] + [next_combined]\n      next_input = tf.concat(axis=-1, values=next_history)\n      next_core, next_hidden = self.core(next_input, prev_hidden)\n    \n      predicted_ta = predicted_ta.write(i, predicted_state)\n      return i+1, next_history, next_core, next_hidden, predicted_state, predicted_ta\n    \n    loop_vars = (0, history, core_outputs, hidden_states, last_states, predicted_ta)\n    cond = lambda i, *_: i < predict_steps\n    _, _, final_core_outputs, _, _, predicted_ta = self.while_loop(cond, predict_step, loop_vars)\n    predicted_states = predicted_ta.stack()\n    predicted_states.set_shape([predict_steps, length, None, self.embedGame.size])\n    \n    distances = self.embedGame.distance(predicted_states, target_states)\n    return distances, final_core_outputs\n\n  def train(self, history, core_outputs, hidden_states, actions, raw_state, **unused):\n    distances, final_core_outputs = self.distances(history, core_outputs, hidden_states, actions, raw_state, self.predict_steps)\n    distances = util.deepMap(lambda t: tf.reduce_mean(t, [1, 2]), distances)\n    total_distances = tf.add_n(list(util.deepValues(distances)))\n    for step in range(self.predict_steps):\n      # log all the individual distances\n      for path, tensor in util.deepItems(distances):\n        tag = ""model/%d/"" % step + ""/"".join(map(str, path))\n        tf.summary.scalar(tag, tensor[step])\n    \n      tf.summary.scalar(""model/%d/total"" % step, total_distances[step])\n    \n    total_distance = tf.reduce_mean(total_distances) * self.model_weight\n    return total_distance, final_core_outputs\n  \n  def predict(self, history, core_outputs, hidden_states, actions, raw_state):\n    last_state = util.deepMap(lambda t: t[-1], raw_state)\n    last_state = self.embedGame(last_state, residual=True)\n\n    def predict_step(i, prev_history, prev_core, prev_hidden, prev_state):\n      current_action = actions[:, i]\n      inputs = tf.concat(axis=-1, values=[prev_core, current_action])\n      \n      predicted_state = self.apply(inputs, prev_state)\n      \n      next_state = self.embedGame.to_input(predicted_state)\n      next_combined = tf.concat(axis=-1, values=[next_state, current_action])\n      next_history = prev_history[1:] + [next_combined]\n      next_input = tf.concat(axis=-1, values=next_history)\n      next_core, next_hidden = self.core(next_input, prev_hidden)\n\n      return i+1, next_history, next_core, next_hidden, predicted_state\n\n    loop_vars = (0, history, core_outputs, hidden_states, last_state)\n    cond = lambda i, *_: i < self.predict_steps\n    _, _, predicted_core_outputs, _, _ = self.while_loop(cond, predict_step, loop_vars)\n    return predicted_core_outputs\n\n'"
phillip/movie.py,0,"b'from .pad import *\n\ndef pushButton(button):\n  return lambda pad: pad.press_button(button)\n\ndef releaseButton(button):\n  return lambda pad: pad.release_button(button)\n\ndef tiltStick(stick, x, y):\n  return lambda pad: pad.tilt_stick(stick, x, y)\n\nneutral = tiltStick(Stick.MAIN, 0.5, 0.5)\nleft = tiltStick(Stick.MAIN, 0, 0.5)\ndown = tiltStick(Stick.MAIN, 0.5, 0)\nup = tiltStick(Stick.MAIN, 0.5, 1)\nright = tiltStick(Stick.MAIN, 1, 0.5)\n\nendless_netplay = [\n  # time\n  (0, left),\n  \n  # infinite time\n  (26, down),\n  (19, left),\n  (25, neutral),\n  \n  # exit settings\n  (1, pushButton(Button.START)),\n  (1, releaseButton(Button.START)),\n  \n  # enter stage select\n  (28, pushButton(Button.START)),\n  (1, releaseButton(Button.START)),\n  \n  (10, neutral)\n]\n\nstages = dict(\n  battlefield = [\n    (0, up),\n    (2, neutral),\n    \n    #(60 * 60, neutral),\n    \n    # start game\n    (20, pushButton(Button.START)),\n    (1, releaseButton(Button.START)),\n  ],\n  \n  final_destination = [\n    (0, tiltStick(Stick.MAIN, 1, 0.8)),\n    (5, neutral),\n    \n    #(60 * 60, neutral),\n    \n    # start game\n    (20, pushButton(Button.START)),\n    (1, releaseButton(Button.START)),\n  ]\n)\n\nclass Movie:\n  def __init__(self, actions, pad):\n    self.actions = actions\n    self.frame = 0\n    self.index = 0\n    self.pad = pad\n  \n  def move(self, state):\n    if not self.done():\n      frame, action = self.actions[self.index]\n      if self.frame == frame:\n        action(self.pad)\n        self.index += 1\n        self.frame = 0\n      else:\n       self.frame += 1\n  \n  def done(self):\n    return self.index == len(self.actions)\n'"
phillip/mutators.py,2,"b'import tensorflow as tf\nimport random\n\ndef relative(factor):\n  inverse = 1. / factor\n  def mutate(x):\n    flip = tf.distributions.Bernoulli(.5).sample()\n    return x * tf.where(tf.cast(flip, tf.bool), factor, inverse)\n  return mutate\n\n'"
phillip/natgrad.py,12,"b'import tensorflow as tf\nfrom .default import *\nfrom . import tf_lib as tfl, cg\n\nclass NaturalGradient(Default):\n  _options = [\n    Option(\'target_distance\', type=float, default=None, help=""Target natural gradient distance.""),\n  ]\n  \n  _members = [\n    (\'cg\', cg.ConjugateGradient),\n  ]\n  \n  def __call__(self, params, direction, predictions, metric, **kwargs):\n    """"""\n    Corrects the given direction to account for the natural metric of the model space.\n    \n    Arguments:\n      params: The parameters (tf Variables) that we are optimizing.\n      direction: Initial direction in which we want to adjust the parameters.\n      predictions: Function of the parameters (and data). This is the space in which we want to move.\n      metric: Metric that defines the geometry of the function space.\n    """"""\n    param_shapes = [param.get_shape() for param in params]\n    param_sizes = [shape.num_elements() for shape in param_shapes]\n    param_offsets = []\n    offset = 0\n    for size in param_sizes:\n      low = offset\n      offset += size\n      param_offsets.append((low, offset))\n    \n    def flatten(params_like):\n      return tf.concat(axis=0, values=[tf.reshape(x, [size]) for x, size in zip(params_like, param_sizes)])\n    \n    def unflatten(flat):\n      slices = [flat[low:high] for low, high in param_offsets]\n      return [tf.reshape(x, shape) for x, shape in zip(slices, param_shapes)]\n    \n    if isinstance(predictions, list):\n      predictions_fixed = list(map(tf.stop_gradient, predictions))\n    else: # assume we have a tensor\n      predictions_fixed = tf.stop_gradient(predictions)\n    distance = metric(predictions_fixed, predictions)\n    \n    distance_gradients = tf.gradients(distance, params)\n    distance_gradients = flatten(distance_gradients)\n    \n    def fvp(tangent):\n      with tf.name_scope(\'fvp\'):\n        tangent = tf.stop_gradient(tangent)\n        gvp = tfl.dot(distance_gradients, tangent)\n        fvp = tf.gradients(gvp, params)\n        return flatten(fvp)\n    \n    direction_flat = flatten(direction)\n    \n    if self.cg.cg_iters:\n      direction_natural = self.cg(fvp, direction_flat, **kwargs)\n    else:\n      direction_natural = direction_flat\n    \n    if self.target_distance is not None:\n      projected_distance = .5 * tfl.dot(direction_natural, fvp(direction_natural))\n      \n      step_size = tf.sqrt(self.target_distance / projected_distance)\n      #step_size = tf.minimum(1.0, step_size)\n      tf.summary.scalar(\'ng_step\', step_size)\n      \n      direction_natural *= step_size\n      \n      progress = tfl.dot(direction_natural, direction_flat)\n      tf.summary.scalar(\'ng_progress\', progress)\n    \n    return unflatten(direction_natural)\n\n'"
phillip/om.py,0,"b'import subprocess\nimport os\n\ndef get_job_node(jobid):\n  ""Get the reserved node for a job id.""\n  try:\n    output = subprocess.check_output([\'squeue\', \'--job\', jobid, \'-h\', \'-o\', \'%R\'])\n  except subprocess.CalledProcessError:\n    return None # invalid job id\n  return int(output[4:-1])\n\ndef get_node_ip(node):\n  return \'172.16.24.%d\' % node\n\ndef get_job_ip(jobid):\n  node = get_job_node(jobid)\n  if node is None:\n    return None\n  return get_node_ip(node)\n\ndef get_self_node():\n  return os.environ[\'SLURM_JOB_NODELIST\']\n\ndef get_self_ip():\n  return get_node_ip(get_self_node())'"
phillip/opt.py,10,"b'import tensorflow as tf\n\nfrom .default import *\nfrom . import natgrad\n\nclass Optimizer(Default):\n  _options = [\n    Option(\'learning_rate\', type=float, default=0.001),\n    Option(\'optimizer\', type=str, default=""GradientDescent"", help=""which tf.train optimizer to use""),\n    Option(\'natural\', action=""store_true"", help=""Use natural gradient.""),\n    Option(\'clip\', type=float, help=""clip gradients above a certain value"")\n  ]\n  \n  _members = [\n    (\'natgrad\', natgrad.NaturalGradient)\n  ]\n    \n  def __init__(self, **kwargs):\n    Default.__init__(self, **kwargs)\n    \n    self.optimizer = getattr(tf.train, self.optimizer + \'Optimizer\')(self.learning_rate)\n  \n  def optimize(self, loss, params=None, predictions=None, metric=None):\n    grads_vars = self.optimizer.compute_gradients(loss, var_list=params)\n    \n    grads, params = map(list, zip(*grads_vars))\n    \n    if self.natural:\n      grads = self.natgrad(params, grads, predictions, metric)\n    \n    grads = [tf.check_numerics(g, ""NaN gradient in param %d"" % i) for i, g in enumerate(grads)]\n    \n    flat_params, flat_grads = [tf.abs(tf.concat(axis=0, values=[tf.reshape(t, [-1]) for t in ts])) for ts in (params, grads)]\n    \n    #flat_ratios = flat_grads / flat_params\n    #tf.scalar_summary(\'grad_param_max\', tf.reduce_max(flat_ratios))\n    #tf.scalar_summary(\'grad_param_avg\', tf.reduce_mean(flat_ratios))\n    \n    grad_max = tf.reduce_max(flat_grads)\n    \n    tf.summary.scalar(\'grad_max\', grad_max)\n    tf.summary.scalar(\'grad_avg\', tf.reduce_mean(flat_grads))\n    \n    if self.clip:\n      clip = tf.minimum(self.clip, grad_max) / grad_max\n      grads = [g*clip for g in grads]\n    \n    return self.optimizer.apply_gradients(zip(grads, params))\n'"
phillip/pad.py,0,"b'import enum\nimport os\nfrom threading import Thread\nfrom . import util\n\n@enum.unique\nclass Button(enum.Enum):\n    A = 0\n    B = 1\n    X = 2\n    Y = 3\n    Z = 4\n    START = 5\n    L = 6\n    R = 7\n    D_UP = 8\n    D_DOWN = 9\n    D_LEFT = 10\n    D_RIGHT = 11\n\n@enum.unique\nclass Trigger(enum.Enum):\n    L = 0\n    R = 1\n\n@enum.unique\nclass Stick(enum.Enum):\n    MAIN = 0\n    C = 1\n\nclass Pad:\n    """"""Writes out controller inputs.""""""\n    def __init__(self, path, tcp=False):\n        """"""Opens the fifo. Blocks until the other end is listening.\n        Args:\n          path: Path to pipe file.\n          tcp: Whether to use zmq over tcp or a fifo. If true, the pipe file\n            is simply a text file containing the port number. The port will\n            be a hash of the path.\n        """"""\n        self.tcp = tcp\n        if tcp:\n          import zmq\n          context = zmq.Context()\n          port = util.port(path)\n          \n          with open(path, \'w\') as f:\n            f.write(str(port))\n\n          self.socket = context.socket(zmq.PUSH)\n          address = ""tcp://127.0.0.1:%d"" % port\n          print(""Binding pad %s to address %s"" % (path, address))\n          self.socket.bind(address)\n        else:\n          os.mkfifo(path)\n          self.pipe = open(path, \'w\', buffering=1)\n        \n        self.message = """"\n\n    def __del__(self):\n        """"""Closes the fifo.""""""\n        if not self.tcp:\n            self.pipe.close()\n    \n    def write(self, command, buffering=False):\n        self.message += command + \'\\n\'\n        \n        if not buffering:\n            self.flush()\n    \n    def flush(self):\n        if self.tcp:\n            #print(""sent message"", self.message)\n            self.socket.send_string(self.message)\n        else:\n            self.pipe.write(self.message)\n        self.message = """"\n\n    def press_button(self, button, buffering=False):\n        """"""Press a button.""""""\n        assert button in Button\n        self.write(\'PRESS {}\'.format(button.name), buffering)\n\n    def release_button(self, button, buffering=False):\n        """"""Release a button.""""""\n        assert button in Button\n        self.write(\'RELEASE {}\'.format(button.name), buffering)\n\n    def press_trigger(self, trigger, amount, buffering=False):\n        """"""Press a trigger. Amount is in [0, 1], with 0 as released.""""""\n        assert trigger in Trigger\n        # assert 0 <= amount <= 1\n        self.write(\'SET {} {:.2f}\'.format(trigger.name, amount), buffering)\n\n    def tilt_stick(self, stick, x, y, buffering=False):\n        """"""Tilt a stick. x and y are in [0, 1], with 0.5 as neutral.""""""\n        assert stick in Stick\n        try:\n          assert 0 <= x <= 1 and 0 <= y <= 1\n        except AssertionError:\n          import ipdb; ipdb.set_trace()\n        self.write(\'SET {} {:.2f} {:.2f}\'.format(stick.name, x, y), buffering)\n\n    def send_controller(self, controller):\n        for button in Button:\n            field = \'button_\' + button.name\n            if hasattr(controller, field):\n                if getattr(controller, field):\n                    self.press_button(button, True)\n                else:\n                    self.release_button(button, True)\n\n        # for trigger in Trigger:\n        #     field = \'trigger_\' + trigger.name\n        #     self.press_trigger(trigger, getattr(controller, field))\n\n        for stick in Stick:\n            field = \'stick_\' + stick.name\n            value = getattr(controller, field)\n            self.tilt_stick(stick, value.x, value.y, True)\n        \n        self.flush()\n'"
phillip/reward.py,2,"b'import numpy as np\nfrom . import util\nfrom .default import *\nimport enum\nimport tensorflow as tf\n\ndef isDying(player):\n  # see https://docs.google.com/spreadsheets/d/1JX2w-r2fuvWuNgGb6D3Cs4wHQKLFegZe2jhbBuIhCG8/edit#gid=13\n  return player.action_state <= 0xA\n\n# players tend to be dead for many frames in a row\n# here we prune all but the first frame of the death\ndef processDeaths(deaths):\n  return np.array(util.zipWith(lambda prev, next: float((not prev) and next), deaths, deaths[1:]))\n\ndef processDamages(percents):\n  return np.array(util.zipWith(lambda prev, next: max(next-prev, 0), percents, percents[1:]))\n\n# from player 2\'s perspective\ndef computeRewards(states, enemies=[0], allies=[1], damage_ratio=0.01):\n  pids = enemies + allies\n\n  deaths = {p : processDeaths([isDying(s.players[p]) for s in states]) for p in pids}\n  damages = {p : processDamages([s.players[p].percent for s in states]) for p in pids}\n\n  losses = {p : deaths[p] + damage_ratio * damages[p] for p in pids}\n\n  return sum(losses[p] for p in enemies) - sum(losses[p] for p in allies)\n\n# from StateActions instead of just States\ndef computeRewardsSA(state_actions, **kwargs):\n  states = [sa.state for sa in state_actions]\n  return computeRewards(states, **kwargs)\n\ndef compute_deaths(player, lib=np):\n  dead = player[\'action_state\'] <= 0xA\n  deaths = lib.logical_and(lib.logical_not(dead[:-1]), dead[1:])\n  if lib == tf: deaths = tf.to_float(deaths)\n  return deaths\n\ndef compute_damages(player, lib=np):\n  percents = player[\'percent\']\n  damages = lib.maximum(percents[1:] - percents[:-1], 0)\n  if lib == tf: damages = tf.to_float(damages)\n  return damages\n\ndef compute_rewards(states, enemies=[0], allies=[1], damage_ratio=0.01, lib=np):\n  """"""Computes rewards from a list of state transitions.\n  \n  Args:\n    states: A structure of numpy arrays of length T, as given by ctype_util.vectorizeCTypes.\n    enemies: The list of pids on the enemy team.\n    allies: The list of pids on our team.\n    damage_ratio: How much damage (percent) counts relative to stocks.\n  Returns:\n    A length T numpy array with the rewards on each transition.\n  """"""\n  \n  players = states[\'players\']\n  pids = enemies + allies\n\n  deaths = {p : compute_deaths(players[p], lib) for p in pids}\n  damages = {p : compute_damages(players[p], lib) for p in pids}\n  losses = {p : deaths[p] + damage_ratio * damages[p] for p in pids}\n  \n  return sum(losses[p] for p in enemies) - sum(losses[p] for p in allies)\n\n\ndef distance(state, lib=np):\n  players = state[\'players\']\n  x0 = players[0][\'x\']\n  y0 = players[0][\'y\']\n  x1 = players[1][\'x\']\n  y1 = players[1][\'y\']\n  \n  dx = x1 - x0\n  dy = y1 - y0\n  \n  return -lib.sqrt(lib.square(dx) + lib.square(dy))\n\n\ndef pseudo_rewards(states, potential_fn, gamma, lib=np):\n  potentials = potential_fn(states, lib=lib)\n  return potentials, gamma * potentials[1:] - potentials[:-1]\n'"
phillip/rl_common.py,4,"b'import tensorflow as tf\nfrom .default import *\n\nclass RLConfig(Default):\n  _options = [\n    Option(\'tdN\', type=int, default=5, help=""use n-step TD error""),\n    Option(\'reward_halflife\', type=float, default=2.0, help=""time to discount rewards by half, in seconds""),\n    Option(\'act_every\', type=int, default=3, help=""Take an action every ACT_EVERY frames.""),\n    #Option(\'experience_time\', type=int, default=1, help=""Length of experiences, in seconds.""),\n    Option(\'experience_length\', type=int, default=30, help=""Length of experiences, in frames.""),\n    Option(\'delay\', type=int, default=0, help=""frame delay on actions taken""),\n    Option(\'memory\', type=int, default=0, help=""number of frames to remember""),\n  ]\n  \n  def __init__(self, **kwargs):\n    Default.__init__(self, **kwargs)\n    self.fps = 60 // self.act_every\n    self.discount = 0.5 ** ( 1.0 / (self.fps*self.reward_halflife) )\n    #self.experience_length = self.experience_time * self.fps\n\ndef makeHistory(state, prev_action, memory=0, **unused):\n  combined = tf.concat(axis=2, values=[state, prev_action])\n  #length = tf.shape(combined)[-2] - memory\n  length = combined.get_shape()[-2].value - memory\n  history = [tf.slice(combined, [0, i, 0], [-1, length, -1]) for i in range(memory+1)]\n  return tf.concat(axis=2, values=history)\n\n'"
phillip/run.py,0,"b'#!/usr/bin/env python3\nimport time, os\nimport pprint\nfrom phillip.dolphin import DolphinRunner\nfrom argparse import ArgumentParser\nfrom multiprocessing import Process\nfrom phillip.cpu import CPU\nfrom phillip import util\nimport tempfile\n\npp = pprint.PrettyPrinter(indent=2)\n\ndef run(**kwargs):\n  load = kwargs.get(\'load\')\n  if load:\n    params = util.load_params(load, \'agent\')\n  else:\n    params = {}\n  \n  util.update(params, **kwargs)\n  #pp.pprint(params)\n\n  if params.get(\'gui\'):\n    params[\'dolphin\'] = True\n\n  if params.get(\'user\') is None:\n    params[\'user\'] = tempfile.mkdtemp() + \'/\'\n  \n  if params.get(\'random_swap\'):\n    task_id = os.environ.get(\'SLURM_ARRAY_TASK_ID\')\n    if task_id is not None:\n      params[\'swap\'] = int(task_id) % 2\n    else:\n      import random\n      params[\'swap\'] = random.getrandbits(1)\n\n  print(""Creating cpu."")\n  cpu = CPU(**params)\n\n  params[\'cpus\'] = cpu.pids\n\n  if params.get(\'dolphin\'):\n    dolphinRunner = DolphinRunner(**params)\n    # delay for a bit to let the cpu start up\n    time.sleep(2)\n    print(""Running dolphin."")\n    dolphin = dolphinRunner()\n  else:\n    dolphin = None\n\n  print(""Running cpu."")\n  cpu.run(dolphin_process=dolphin)\n\ndef main():\n  parser = ArgumentParser()\n\n  for opt in CPU.full_opts():\n    opt.update_parser(parser)\n\n  parser.add_argument(""--load"", type=str, help=""path to folder containing snapshot and params"")\n\n  # dolphin options\n  parser.add_argument(""--dolphin"", action=""store_true"", default=None, help=""run dolphin"")\n  \n  parser.add_argument(""--random_swap"", action=""store_true"", help=""randomly swap players"")\n\n  for opt in DolphinRunner.full_opts():\n    opt.update_parser(parser)\n\n  args = parser.parse_args()\n  \n  run(**args.__dict__)\n\nif __name__ == ""__main__"":\n  main()\n\n'"
phillip/ssbm.py,3,"b'""""""\nDefine SSBM types. \n""""""\n\nfrom ctypes import *\nfrom .ctype_util import *\nfrom enum import IntEnum\nimport struct\nimport tempfile\nimport os\n#import h5py\nimport pickle\nfrom . import reward\nimport numpy as np\nimport itertools\nimport attr\nfrom .state import Character\n\n@pretty_struct\nclass Stick(Structure):\n  _fields = [\n    (\'x\', c_float),\n    (\'y\', c_float),\n  ]\n\n  def __init__(self, x=0.5, y=0.5):\n    self.x = x\n    self.y = y\n\n  def reset(self):\n    self.x = 0.5\n    self.y = 0.5\n  \n  @classmethod\n  def polar(cls, theta, r=1.):\n    r /= 2.\n    return cls(x=0.5+r*np.cos(theta), y=0.5+r*np.sin(theta))\n\n@pretty_struct\nclass RealControllerState(Structure):\n  _fields = [\n    (\'button_A\', c_bool),\n    (\'button_B\', c_bool),\n    (\'button_X\', c_bool),\n    (\'button_Y\', c_bool),\n    (\'button_Z\', c_bool),\n    (\'button_L\', c_bool),\n    (\'button_R\', c_bool),\n    (\'button_START\', c_bool),\n\n    (\'trigger_L\', c_float),\n    (\'trigger_R\', c_float),\n\n    (\'stick_MAIN\', Stick),\n    (\'stick_C\', Stick),\n  ]\n\n  def __init__(self):\n    self.reset()\n\n  def reset(self):\n    ""Resets controller to neutral position.""\n    self.button_A = False\n    self.button_B = False\n    self.button_X = False\n    self.button_Y = False\n    self.button_L = False\n    self.button_R = False\n\n    self.analog_L = 0.0\n    self.analog_R = 0.0\n\n    self.stick_MAIN.reset()\n    self.stick_C.reset()\n  \nRealControllerState.neutral = RealControllerState()\n\n@pretty_struct\nclass PlayerMemory(Structure):\n  _fields = [\n    (\'percent\', c_uint),\n    (\'stock\', c_uint),\n    # 1.0 is right, -1.0 is left\n    (\'facing\', c_float),\n    (\'x\', c_float),\n    (\'y\', c_float),\n    (\'z\', c_float),\n    (\'action_state\', c_uint),\n    (\'action_counter\', c_uint),\n    (\'action_frame\', c_float),\n    (\'character\', c_uint),\n    (\'invulnerable\', c_bool),\n    (\'hitlag_frames_left\', c_float),\n    (\'hitstun_frames_left\', c_float),\n    (\'jumps_used\', c_uint),\n    (\'charging_smash\', c_bool),\n    (\'in_air\', c_bool),\n    (\'speed_air_x_self\', c_float),\n    (\'speed_ground_x_self\', c_float),\n    (\'speed_y_self\', c_float),\n    (\'speed_x_attack\', c_float),\n    (\'speed_y_attack\', c_float),\n    (\'shield_size\', c_float),\n\n    (\'cursor_x\', c_float),\n    (\'cursor_y\', c_float),\n\n    # NOTE: the sticks here are [-1, 1],\n    # not [0, 1] like in pad.py\n    (\'controller\', RealControllerState)\n  ]\n\n@pretty_struct\nclass GameMemory(Structure):\n  _fields = [\n    (\'players\', PlayerMemory * 2),\n\n    (\'frame\', c_uint),\n    (\'menu\', c_uint),\n    (\'stage\', c_uint),\n    \n    # stage select screen\n    (\'sss_cursor_x\', c_float),\n    (\'sss_cursor_y\', c_float),\n  ]\n\nclass SimpleButton(IntEnum):\n  NONE = 0\n  A = 1\n  B = 2\n  Z = 3\n  Y = 4\n  L = 5\n\nneutral_stick = (0.5, 0.5)\n\n@attr.s\nclass SimpleController(object):\n  button = attr.ib(default=SimpleButton.NONE)\n  stick = attr.ib(default=neutral_stick)\n  duration = attr.ib(default=None)\n  \n  @classmethod\n  def init(cls, *args, **kwargs):\n    self = cls(*args, **kwargs)\n    self.real_controller = self.realController()\n    return self\n  \n  def realController(self):\n    controller = RealControllerState()\n    if self.button is not SimpleButton.NONE:\n      setattr(controller, ""button_%s"" % self.button.name, True)\n\n    controller.stick_MAIN = self.stick\n    return controller\n  \n  def banned(self, player, char):\n    # toad does weird things. needs investigation\n    if char == \'peach\' and self.button == SimpleButton.B and self.stick == neutral_stick:\n      return True\n    # transforming screws up the memory watcher\n    if char in [\'sheik\', \'zelda\'] and  self.button == SimpleButton.B and self.stick[1] < 0.5:\n      return True\n    # fox dittos can end up in infinite lasers\n    #if char == \'fox\' and self.button == SimpleButton.B and self.stick == neutral_stick:\n    #  return True\n    # fox and falco suck at recovering because they learn to just shine-stall\n    # they also learn to never side-b because it sometimes leads to instant death\n    if char in [\'fox\', \'falco\'] and self.button == SimpleButton.B:\n      if self.stick[0] > 0.5 and player.x > 0: return True  # don\'t side-B off the right\n      if self.stick[0] < 0.5 and player.x < 0: return True  # don\'t side-B off the left\n      if self.stick[1] < 0.5:  # shine stall \n        if abs(player.x) > 100 or player.y < -5: return True\n\n    if char == \'puff\':\n      # side-B spam to avoid dying is lame\n      if (player.jumps_used >= 6 and\n          self.button == SimpleButton.B and self.stick[0] != 0.5 and\n          player.y < -5):\n        return True\n\n    return False\n  \n  def send(self, pad, player, char):\n    if self.banned(player, char):\n      pad.send_controller(RealControllerState.neutral)\n    else:\n      pad.send_controller(self.real_controller)\n\nSimpleController.neutral = SimpleController.init()\n\n\nclass RepeatController(object):\n  duration = None\n\n  def send(self, pad, *args):\n    pass\n\nrepeat_controller = RepeatController()\n\naxis_granularity = 3\naxis_positions = np.linspace(0, 1, axis_granularity)\ndiagonal_sticks = list(itertools.product(axis_positions, repeat=2))\ndiagonal_controllers = [SimpleController.init(*args) for args in itertools.product(SimpleButton, diagonal_sticks)]\n\n\nclass ActionChain(object):\n  """"""\n  A list of actions, each with a duration, and the last duration must be None.\n  \n  TODO: Come up with a better system?\n  """"""\n\n  def __init__(self, action_list, act_every):\n    self.actions = []\n    for action in action_list:\n      if action.duration:\n        self.actions += [action] * action.duration\n      else:\n        self.actions += [action] * (act_every - len(self.actions))\n    assert len(self.actions) == act_every\n    self.index = 0\n\n  def act(self, pad, *args):\n    self.actions[self.index].send(pad, *args)\n    self.index += 1\n  \n  def done(self):\n    return self.index == len(self.actions)\n\n\nclass ActionSet(object):\n  def __init__(self, actions):\n    self.actions = list(map(lambda obj: obj if isinstance(obj, list) else [obj], actions))\n    self.size = len(actions)\n  \n  def choose(self, index, act_every):\n    return ActionChain(self.actions[index], act_every)\n\nold_sticks = [(0.5, 0.5), (0.5, 1), (0.5, 0), (0, 0.5), (1, 0.5)]\nold_controllers = [SimpleController.init(*args) for args in itertools.product(SimpleButton, old_sticks)]\n\ncardinal_sticks = [(0, 0.5), (1, 0.5), (0.5, 0), (0.5, 1), (0.5, 0.5)]\ncardinal_controllers = [SimpleController.init(*args) for args in itertools.product(SimpleButton, cardinal_sticks)]\n\ntilt_sticks = [(0.4, 0.5), (0.6, 0.5), (0.5, 0.4), (0.5, 0.6)]\n\ncustom_controllers = itertools.chain(\n  itertools.product([SimpleButton.A, SimpleButton.B], cardinal_sticks),\n  itertools.product([SimpleButton.A], tilt_sticks),\n  itertools.product([SimpleButton.NONE, SimpleButton.L], diagonal_sticks),\n  itertools.product([SimpleButton.Z, SimpleButton.Y], [neutral_stick]),\n)\ncustom_controllers = [SimpleController.init(*args) for args in custom_controllers]\ncustom_controllers.append(repeat_controller)\n\n# allows fox, sheik, samus, etc to short hop with act_every=3\nshort_hop = SimpleController.init(button=SimpleButton.Y, duration=2)\nshort_hop_chain = [short_hop, SimpleController.neutral]\n\n# this is technically no longer needed because of sh2\njc_chain = [SimpleController.init(button=SimpleButton.Y, duration=1), SimpleController.init(button=SimpleButton.Z)]\n\n# better sh that also allows jc grab and upsmash at act_every 3\nsh2_chain = [\n  SimpleController.init(duration=2),\n  SimpleController.init(button=SimpleButton.Y),\n]\n\nfox_wd_chain_left = [\n  SimpleController.init(button=SimpleButton.Y, duration=3),\n  SimpleController.init(button=SimpleButton.L, stick=Stick.polar(-7/8 * np.pi))\n]\n\nwd_left = SimpleController.init(button=SimpleButton.L, stick=Stick.polar(-7/8 * np.pi))\nwd_right = SimpleController.init(button=SimpleButton.L, stick=Stick.polar(-1/8 * np.pi))\nwd_both = [wd_left, wd_right]\n\nactionTypes = dict(\n  old = ActionSet(old_controllers),\n  cardinal = ActionSet(cardinal_controllers),\n  diagonal = ActionSet(diagonal_controllers),\n  custom = ActionSet(custom_controllers),\n  short_hop_test = ActionSet([SimpleController.neutral] * 10 + [short_hop_chain]),\n  # short_hop = ActionSet(custom_controllers + [short_hop]),\n  custom_sh_jc = ActionSet(custom_controllers + [short_hop_chain, jc_chain]),\n  fox_wd_test = ActionSet([SimpleController.neutral] * 10 + [fox_wd_chain_left]),\n  custom_sh2_wd = ActionSet(custom_controllers + [sh2_chain] + wd_both),\n)\n\n@pretty_struct\nclass SimpleStateAction(Structure):\n  _fields = [\n    (\'state\', GameMemory),\n    (\'prev_action\', c_uint),\n    (\'action\', c_uint),\n    (\'prob\', c_float),\n  ]\n\n\ndef prepareStateActions(state_actions):\n  """"""Prepares an experience for pickling.\n  \n  Args:\n    state_actions: A value of type (SimpleStateAction * T), or [SimpleStateAction].\n  Returns:\n    A structure of numpy arrays of length T.\n  """"""\n\n  vectorized = vectorizeCTypes(SimpleStateAction, state_actions)\n  rewards_ = reward.compute_rewards(vectorized[\'state\'])\n  rewards = reward.computeRewardsSA(state_actions)\n  assert(np.max(np.abs(rewards_ - rewards)) < 1e-5)\n  \n  vectorized[\'reward\'] = rewards\n  return vectorized\n\n# TODO: replace pickle with hdf5\ndef writeStateActions_HDF5(filename, state_actions):\n  with tempfile.NamedTemporaryFile(dir=os.path.dirname(filename), delete=False) as tf:\n    tf.write(intStruct.pack(len(state_actions)))\n    tf.write(state_actions)\n    tempname = tf.name\n  os.rename(tempname, filename)\n\ndef readStateActions_HDF5(filename):\n  with open(filename, \'rb\') as f:\n    size = readInt(f)\n    state_actions = (size * SimpleStateAction)()\n    f.readinto(state_actions)\n\n    if len(f.read()) > 0:\n      raise Exception(filename + "" too long!"")\n\n    return state_actions\n'"
phillip/state.py,0,"b'import enum\n\n@enum.unique\nclass PlayerType(enum.Enum):\n    Human      = 0\n    CPU        = 1\n    Demo       = 2\n    Unselected = 3\n\n@enum.unique\nclass Character(enum.Enum):\n    Doc        = 0\n    Mario      = 1\n    Luigi      = 2\n    Bowser     = 3\n    Peach      = 4\n    Yoshi      = 5\n    DK         = 6\n    Falcon     = 7\n    Ganon      = 8\n    Falco      = 9\n    Fox        = 10\n    Ness       = 11\n    Icies      = 12\n    Kirby      = 13\n    Samus      = 14\n    Zelda      = 15\n    Link       = 16\n    YoungLink  = 17\n    Pichu      = 18\n    Pikachu    = 19\n    Jiggs      = 20\n    Mewtwo     = 21\n    GnW        = 22\n    Marth      = 23\n    Roy        = 24\n    Unselected = 25\n\n@enum.unique\nclass Stage(enum.Enum):\n    PeachCastle = 0\n    Rainbow     = 1\n    Kongo       = 2\n    Japes       = 3\n    GreatBay    = 4\n    Temple      = 5\n    Story       = 6\n    Island      = 7\n    FoD         = 8\n    Greens      = 9\n    Corneria    = 10\n    Venom       = 11\n    Brinstar    = 12\n    Depths      = 13\n    Onett       = 14\n    Fourside    = 15\n    Mute        = 16\n    BigBlue     = 17\n    Pokemon     = 18\n    Floats      = 19\n    Kingdom     = 20\n    Kingdom2    = 21\n    Icicle      = 22\n    FlatZone    = 23\n    Battlefield = 24\n    Final       = 25\n    DreamLand   = 26\n    Island64    = 27\n    Kongo64     = 28\n    Random      = 29\n    Unselected  = 30\n\n@enum.unique\nclass Menu(enum.Enum):\n    Characters = 0\n    Stages     = 1\n    Game       = 2\n    PostGame   = 4\n\n@enum.unique\nclass ActionState(enum.Enum):\n    DeadDown                = 0x0000\n    DeadLeft                = 0x0001\n    DeadRight               = 0x0002\n    DeadUp                  = 0x0003\n    DeadUpStar              = 0x0004\n    DeadUpStarIce           = 0x0005\n    DeadUpFall              = 0x0006\n    DeadUpFallHitCamera     = 0x0007\n    DeadUpFallHitCameraFlat = 0x0008\n    DeadUpFallIce           = 0x0009\n    DeadUpFallHitCameraIce  = 0x000A\n    Sleep                   = 0x000B\n    Rebirth                 = 0x000C\n    RebirthWait             = 0x000D\n    Wait                    = 0x000E\n    WalkSlow                = 0x000F\n    WalkMiddle              = 0x0010\n    WalkFast                = 0x0011\n    Turn                    = 0x0012\n    TurnRun                 = 0x0013\n    Dash                    = 0x0014\n    Run                     = 0x0015\n    RunDirect               = 0x0016\n    RunBrake                = 0x0017\n    KneeBend                = 0x0018\n    JumpF                   = 0x0019\n    JumpB                   = 0x001A\n    JumpAerialF             = 0x001B\n    JumpAerialB             = 0x001C\n    Fall                    = 0x001D\n    FallF                   = 0x001E\n    FallB                   = 0x001F\n    FallAerial              = 0x0020\n    FallAerialF             = 0x0021\n    FallAerialB             = 0x0022\n    FallSpecial             = 0x0023\n    FallSpecialF            = 0x0024\n    FallSpecialB            = 0x0025\n    DamageFall              = 0x0026\n    Squat                   = 0x0027\n    SquatWait               = 0x0028\n    SquatRv                 = 0x0029\n    Landing                 = 0x002A\n    LandingFallSpecial      = 0x002B\n    Attack11                = 0x002C\n    Attack12                = 0x002D\n    Attack13                = 0x002E\n    Attack100Start          = 0x002F\n    Attack100Loop           = 0x0030\n    Attack100End            = 0x0031\n    AttackDash              = 0x0032\n    AttackS3Hi              = 0x0033\n    AttackS3HiS             = 0x0034\n    AttackS3S               = 0x0035\n    AttackS3LwS             = 0x0036\n    AttackS3Lw              = 0x0037\n    AttackHi3               = 0x0038\n    AttackLw3               = 0x0039\n    AttackS4Hi              = 0x003A\n    AttackS4HiS             = 0x003B\n    AttackS4S               = 0x003C\n    AttackS4LwS             = 0x003D\n    AttackS4Lw              = 0x003E\n    AttackHi4               = 0x003F\n    AttackLw4               = 0x0040\n    AttackAirN              = 0x0041\n    AttackAirF              = 0x0042\n    AttackAirB              = 0x0043\n    AttackAirHi             = 0x0044\n    AttackAirLw             = 0x0045\n    LandingAirN             = 0x0046\n    LandingAirF             = 0x0047\n    LandingAirB             = 0x0048\n    LandingAirHi            = 0x0049\n    LandingAirLw            = 0x004A\n    DamageHi1               = 0x004B\n    DamageHi2               = 0x004C\n    DamageHi3               = 0x004D\n    DamageN1                = 0x004E\n    DamageN2                = 0x004F\n    DamageN3                = 0x0050\n    DamageLw1               = 0x0051\n    DamageLw2               = 0x0052\n    DamageLw3               = 0x0053\n    DamageAir1              = 0x0054\n    DamageAir2              = 0x0055\n    DamageAir3              = 0x0056\n    DamageFlyHi             = 0x0057\n    DamageFlyN              = 0x0058\n    DamageFlyLw             = 0x0059\n    DamageFlyTop            = 0x005A\n    DamageFlyRoll           = 0x005B\n    LightGet                = 0x005C\n    HeavyGet                = 0x005D\n    LightThrowF             = 0x005E\n    LightThrowB             = 0x005F\n    LightThrowHi            = 0x0060\n    LightThrowLw            = 0x0061\n    LightThrowDash          = 0x0062\n    LightThrowDrop          = 0x0063\n    LightThrowAirF          = 0x0064\n    LightThrowAirB          = 0x0065\n    LightThrowAirHi         = 0x0066\n    LightThrowAirLw         = 0x0067\n    HeavyThrowF             = 0x0068\n    HeavyThrowB             = 0x0069\n    HeavyThrowHi            = 0x006A\n    HeavyThrowLw            = 0x006B\n    LightThrowF4            = 0x006C\n    LightThrowB4            = 0x006D\n    LightThrowHi4           = 0x006E\n    LightThrowLw4           = 0x006F\n    LightThrowAirF4         = 0x0070\n    LightThrowAirB4         = 0x0071\n    LightThrowAirHi4        = 0x0072\n    LightThrowAirLw4        = 0x0073\n    HeavyThrowF4            = 0x0074\n    HeavyThrowB4            = 0x0075\n    HeavyThrowHi4           = 0x0076\n    HeavyThrowLw4           = 0x0077\n    SwordSwing1             = 0x0078\n    SwordSwing3             = 0x0079\n    SwordSwing4             = 0x007A\n    SwordSwingDash          = 0x007B\n    BatSwing1               = 0x007C\n    BatSwing3               = 0x007D\n    BatSwing4               = 0x007E\n    BatSwingDash            = 0x007F\n    ParasolSwing1           = 0x0080\n    ParasolSwing3           = 0x0081\n    ParasolSwing4           = 0x0082\n    ParasolSwingDash        = 0x0083\n    HarisenSwing1           = 0x0084\n    HarisenSwing3           = 0x0085\n    HarisenSwing4           = 0x0086\n    HarisenSwingDash        = 0x0087\n    StarRodSwing1           = 0x0088\n    StarRodSwing3           = 0x0089\n    StarRodSwing4           = 0x008A\n    StarRodSwingDash        = 0x008B\n    LipStickSwing1          = 0x008C\n    LipStickSwing3          = 0x008D\n    LipStickSwing4          = 0x008E\n    LipStickSwingDash       = 0x008F\n    ItemParasolOpen         = 0x0090\n    ItemParasolFall         = 0x0091\n    ItemParasolFallSpecial  = 0x0092\n    ItemParasolDamageFall   = 0x0093\n    LGunShoot               = 0x0094\n    LGunShootAir            = 0x0095\n    LGunShootEmpty          = 0x0096\n    LGunShootAirEmpty       = 0x0097\n    FireFlowerShoot         = 0x0098\n    FireFlowerShootAir      = 0x0099\n    ItemScrew               = 0x009A\n    ItemScrewAir            = 0x009B\n    DamageScrew             = 0x009C\n    DamageScrewAir          = 0x009D\n    ItemScopeStart          = 0x009E\n    ItemScopeRapid          = 0x009F\n    ItemScopeFire           = 0x00A0\n    ItemScopeEnd            = 0x00A1\n    ItemScopeAirStart       = 0x00A2\n    ItemScopeAirRapid       = 0x00A3\n    ItemScopeAirFire        = 0x00A4\n    ItemScopeAirEnd         = 0x00A5\n    ItemScopeStartEmpty     = 0x00A6\n    ItemScopeRapidEmpty     = 0x00A7\n    ItemScopeFireEmpty      = 0x00A8\n    ItemScopeEndEmpty       = 0x00A9\n    ItemScopeAirStartEmpty  = 0x00AA\n    ItemScopeAirRapidEmpty  = 0x00AB\n    ItemScopeAirFireEmpty   = 0x00AC\n    ItemScopeAirEndEmpty    = 0x00AD\n    LiftWait                = 0x00AE\n    LiftWalk1               = 0x00AF\n    LiftWalk2               = 0x00B0\n    LiftTurn                = 0x00B1\n    GuardOn                 = 0x00B2\n    Guard                   = 0x00B3\n    GuardOff                = 0x00B4\n    GuardSetOff             = 0x00B5\n    GuardReflect            = 0x00B6\n    DownBoundU              = 0x00B7\n    DownWaitU               = 0x00B8\n    DownDamageU             = 0x00B9\n    DownStandU              = 0x00BA\n    DownAttackU             = 0x00BB\n    DownFowardU             = 0x00BC\n    DownBackU               = 0x00BD\n    DownSpotU               = 0x00BE\n    DownBoundD              = 0x00BF\n    DownWaitD               = 0x00C0\n    DownDamageD             = 0x00C1\n    DownStandD              = 0x00C2\n    DownAttackD             = 0x00C3\n    DownFowardD             = 0x00C4\n    DownBackD               = 0x00C5\n    DownSpotD               = 0x00C6\n    Passive                 = 0x00C7\n    PassiveStandF           = 0x00C8\n    PassiveStandB           = 0x00C9\n    PassiveWall             = 0x00CA\n    PassiveWallJump         = 0x00CB\n    PassiveCeil             = 0x00CC\n    ShieldBreakFly          = 0x00CD\n    ShieldBreakFall         = 0x00CE\n    ShieldBreakDownU        = 0x00CF\n    ShieldBreakDownD        = 0x00D0\n    ShieldBreakStandU       = 0x00D1\n    ShieldBreakStandD       = 0x00D2\n    FuraFura                = 0x00D3\n    Catch                   = 0x00D4\n    CatchPull               = 0x00D5\n    CatchDash               = 0x00D6\n    CatchDashPull           = 0x00D7\n    CatchWait               = 0x00D8\n    CatchAttack             = 0x00D9\n    CatchCut                = 0x00DA\n    ThrowF                  = 0x00DB\n    ThrowB                  = 0x00DC\n    ThrowHi                 = 0x00DD\n    ThrowLw                 = 0x00DE\n    CapturePulledHi         = 0x00DF\n    CaptureWaitHi           = 0x00E0\n    CaptureDamageHi         = 0x00E1\n    CapturePulledLw         = 0x00E2\n    CaptureWaitLw           = 0x00E3\n    CaptureDamageLw         = 0x00E4\n    CaptureCut              = 0x00E5\n    CaptureJump             = 0x00E6\n    CaptureNeck             = 0x00E7\n    CaptureFoot             = 0x00E8\n    EscapeF                 = 0x00E9\n    EscapeB                 = 0x00EA\n    Escape                  = 0x00EB\n    EscapeAir               = 0x00EC\n    ReboundStop             = 0x00ED\n    Rebound                 = 0x00EE\n    ThrownF                 = 0x00EF\n    ThrownB                 = 0x00F0\n    ThrownHi                = 0x00F1\n    ThrownLw                = 0x00F2\n    ThrownLwWomen           = 0x00F3\n    Pass                    = 0x00F4\n    Ottotto                 = 0x00F5\n    OttottoWait             = 0x00F6\n    FlyReflectWall          = 0x00F7\n    FlyReflectCeil          = 0x00F8\n    StopWall                = 0x00F9\n    StopCeil                = 0x00FA\n    MissFoot                = 0x00FB\n    CliffCatch              = 0x00FC\n    CliffWait               = 0x00FD\n    CliffClimbSlow          = 0x00FE\n    CliffClimbQuick         = 0x00FF\n    CliffAttackSlow         = 0x0100\n    CliffAttackQuick        = 0x0101\n    CliffEscapeSlow         = 0x0102\n    CliffEscapeQuick        = 0x0103\n    CliffJumpSlow1          = 0x0104\n    CliffJumpSlow2          = 0x0105\n    CliffJumpQuick1         = 0x0106\n    CliffJumpQuick2         = 0x0107\n    AppealR                 = 0x0108\n    AppealL                 = 0x0109\n    ShoulderedWait          = 0x010A\n    ShoulderedWalkSlow      = 0x010B\n    ShoulderedWalkMiddle    = 0x010C\n    ShoulderedWalkFast      = 0x010D\n    ShoulderedTurn          = 0x010E\n    ThrownFF                = 0x010F\n    ThrownFB                = 0x0110\n    ThrownFHi               = 0x0111\n    ThrownFLw               = 0x0112\n    CaptureCaptain          = 0x0113\n    CaptureYoshi            = 0x0114\n    YoshiEgg                = 0x0115\n    CaptureKoopa            = 0x0116\n    CaptureDamageKoopa      = 0x0117\n    CaptureWaitKoopa        = 0x0118\n    ThrownKoopaF            = 0x0119\n    ThrownKoopaB            = 0x011A\n    CaptureKoopaAir         = 0x011B\n    CaptureDamageKoopaAir   = 0x011C\n    CaptureWaitKoopaAir     = 0x011D\n    ThrownKoopaAirF         = 0x011E\n    ThrownKoopaAirB         = 0x011F\n    CaptureKirby            = 0x0120\n    CaptureWaitKirby        = 0x0121\n    ThrownKirbyStar         = 0x0122\n    ThrownCopyStar          = 0x0123\n    ThrownKirby             = 0x0124\n    BarrelWait              = 0x0125\n    Bury                    = 0x0126\n    BuryWait                = 0x0127\n    BuryJump                = 0x0128\n    DamageSong              = 0x0129\n    DamageSongWait          = 0x012A\n    DamageSongRv            = 0x012B\n    DamageBind              = 0x012C\n    CaptureMewtwo           = 0x012D\n    CaptureMewtwoAir        = 0x012E\n    ThrownMewtwo            = 0x012F\n    ThrownMewtwoAir         = 0x0130\n    WarpStarJump            = 0x0131\n    WarpStarFall            = 0x0132\n    HammerWait              = 0x0133\n    HammerWalk              = 0x0134\n    HammerTurn              = 0x0135\n    HammerKneeBend          = 0x0136\n    HammerFall              = 0x0137\n    HammerJump              = 0x0138\n    HammerLanding           = 0x0139\n    KinokoGiantStart        = 0x013A\n    KinokoGiantStartAir     = 0x013B\n    KinokoGiantEnd          = 0x013C\n    KinokoGiantEndAir       = 0x013D\n    KinokoSmallStart        = 0x013E\n    KinokoSmallStartAir     = 0x013F\n    KinokoSmallEnd          = 0x0140\n    KinokoSmallEndAir       = 0x0141\n    Entry                   = 0x0142\n    EntryStart              = 0x0143\n    EntryEnd                = 0x0144\n    DamageIce               = 0x0145\n    DamageIceJump           = 0x0146\n    CaptureMasterhand       = 0x0147\n    CapturedamageMasterhand = 0x0148\n    CapturewaitMasterhand   = 0x0149\n    ThrownMasterhand        = 0x014A\n    CaptureKirbyYoshi       = 0x014B\n    KirbyYoshiEgg           = 0x014C\n    CaptureLeadead          = 0x014D\n    CaptureLikelike         = 0x014E\n    DownReflect             = 0x014F\n    CaptureCrazyhand        = 0x0150\n    CapturedamageCrazyhand  = 0x0151\n    CapturewaitCrazyhand    = 0x0152\n    ThrownCrazyhand         = 0x0153\n    BarrelCannonWait        = 0x0154\n    Wait1                   = 0x0155\n    Wait2                   = 0x0156\n    Wait3                   = 0x0157\n    Wait4                   = 0x0158\n    WaitItem                = 0x0159\n    SquatWait1              = 0x015A\n    SquatWait2              = 0x015B\n    SquatWaitItem           = 0x015C\n    GuardDamage             = 0x015D\n    EscapeN                 = 0x015E\n    AttackS4Hold            = 0x015F\n    HeavyWalk1              = 0x0160\n    HeavyWalk2              = 0x0161\n    ItemHammerWait          = 0x0162\n    ItemHammerMove          = 0x0163\n    ItemBlind               = 0x0164\n    DamageElec              = 0x0165\n    FuraSleepStart          = 0x0166\n    FuraSleepLoop           = 0x0167\n    FuraSleepEnd            = 0x0168\n    WallDamage              = 0x0169\n    CliffWait1              = 0x016A\n    CliffWait2              = 0x016B\n    SlipDown                = 0x016C\n    Slip                    = 0x016D\n    SlipTurn                = 0x016E\n    SlipDash                = 0x016F\n    SlipWait                = 0x0170\n    SlipStand               = 0x0171\n    SlipAttack              = 0x0172\n    SlipEscapeF             = 0x0173\n    SlipEscapeB             = 0x0174\n    AppealS                 = 0x0175\n    Zitabata                = 0x0176\n    CaptureKoopaHit         = 0x0177\n    ThrownKoopaEndF         = 0x0178\n    ThrownKoopaEndB         = 0x0179\n    CaptureKoopaAirHit      = 0x017A\n    ThrownKoopaAirEndF      = 0x017B\n    ThrownKoopaAirEndB      = 0x017C\n    ThrownKirbyDrinkSShot   = 0x017D\n    ThrownKirbySpitSShot    = 0x017E\n    Unselected              = 0x8000\n\nclass State:\n    """"""Databag that is handled by StateManager.""""""\n    pass\n'"
phillip/state_manager.py,0,"b'import struct\nimport attr\nfrom . import ssbm, fields\nimport numpy as np\n\ndef generic_wrapper(value, wrapper, default):\n    if wrapper is not None:\n        try:\n            value = wrapper(value)\n        except ValueError:\n            value = default\n    return value\n\nintStruct = struct.Struct(\'>i\')\n\nbyte_mask = 0xFF\nshort_mask = 0xFFFF\nint_mask = 0xFFFFFFFF\n\n@attr.s\nclass IntHandler(object):\n    shift = attr.ib(default=0)\n    mask = attr.ib(default=int_mask)\n    wrapper = attr.ib(default=None)\n    default = attr.ib(default=0)\n\n    def __call__(self, value):\n        transformed = (intStruct.unpack(value)[0] >> self.shift) & self.mask\n        return generic_wrapper(transformed, self.wrapper, self.default)\n\nintHandler = IntHandler()\nbyteHandler = IntHandler(shift=24, mask=byte_mask)\nshortHandler = IntHandler(shift=16, mask=short_mask)\n\nfloatStruct = struct.Struct(\'>f\')\n\n@attr.s\nclass FloatHandler(object):\n    wrapper = attr.ib(default=None)\n    default = attr.ib(default=0.0)\n\n    def __call__(self, value):\n        as_float = floatStruct.unpack(value)[0]\n        if not np.isfinite(as_float):\n            raise ValueError(\'non-finite value\')\n        return generic_wrapper(as_float, self.wrapper, self.default)\n\nfloatHandler = FloatHandler()\n\n@attr.s\nclass Handler(object):\n    path = attr.ib()\n    handler = attr.ib()\n\n    def __call__(self, obj, value):\n        try:\n            fields.setPath(obj, self.path, self.handler(value))\n        except ValueError as e:\n            print(self.path, e.args)\n\n# TODO: use numbers instead of strings to hash addresses?\ndef add_address(x, y):\n    """"""Returns a string representation of the sum of the two parameters.\n\n    x is a hex string address that can be converted to an int.\n    y is an int.\n    """"""\n    return ""{0:08X}"".format(int(x, 16) + y)\n\n# see https://docs.google.com/spreadsheets/d/1JX2w-r2fuvWuNgGb6D3Cs4wHQKLFegZe2jhbBuIhCG8\n\nglobal_addresses = {}\n\nglobal_addresses[\'80479D60\'] = Handler([\'frame\'], intHandler)\nglobal_addresses[\'80479D30\'] = Handler([\'menu\'], IntHandler(mask=byte_mask))#, Menu, Menu.Characters)\nglobal_addresses[\'804D6CAD\'] = Handler([\'stage\'], shortHandler)#, Stage, Stage.Unselected)\n\n# doesn\'t work yet, crashes dolphin with invalid pointers\nsss_pointer = \'80BDA810 28 \'\n#global_addresses[sss_pointer + \'38\'] = Handler([\'sss_cursor_x\'], floatHandler)\n#global_addresses[sss_pointer + \'3C\'] = Handler([\'sss_cursor_y\'], floatHandler)\n\ndef playerAddresses(player_id, addresses=None):\n    if addresses is None:\n        addresses = {}\n\n    player_path = [\'players\', player_id]\n\n    def playerHandler(field, handler):\n        return Handler(player_path + field.split(\'/\'), handler)\n\n    cursor_pointer = add_address(\'804A0BC0\',  4 * player_id)\n    cursor_x_address = cursor_pointer + \' \' + \'C\'\n    cursor_y_address = cursor_pointer + \' \' + \'10\'\n    addresses[cursor_x_address] = playerHandler(\'cursor_x\', floatHandler)\n    addresses[cursor_y_address] = playerHandler(\'cursor_y\', floatHandler)\n\n    type_address = add_address(\'803F0E08\', 0x24 * player_id)\n    type_handler = playerHandler(\'type\', byteHandler) #, PlayerType, PlayerType.Unselected)\n    character_handler = playerHandler(\'character\', IntHandler(8, byte_mask)) #, Character, Character.Unselected)\n    addresses[type_address] = [character_handler]#, type_handler]\n\n    button_address = add_address(\'0x804C1FAC\', 0x44 * player_id)\n    button_locs = dict(\n        Z = 4,\n        L = 5,\n        R = 6,\n        A = 8,\n        B = 9,\n        X = 10,\n        Y = 11,\n        START = 12\n    ).items()\n    addresses[button_address] = [playerHandler(\'controller/button_%s\' % b, IntHandler(mask=1<<i)) for b, i in button_locs]\n\n    stick_address = 0x804C1FCC\n    for stick in [\'MAIN\', \'C\']:\n        for axis in [\'x\', \'y\']:\n            address = ""{0:08X}"".format(stick_address + 0x44 * player_id)\n            addresses[address] = playerHandler(""controller/stick_%s/%s"" % (stick, axis), floatHandler)\n            stick_address += 4\n\n    static_pointer = 0x80453080 + 0xE90 * player_id\n\n    def add_static_address(offset, name, handler):\n      address = ""{0:08X}"".format(static_pointer + offset)\n      handle = playerHandler(name, handler)\n      if address not in addresses:\n        addresses[address] = [handle]\n      else:\n        addresses[address].append(handle)\n\n    add_static_address(0x60, \'percent\', shortHandler)\n    # add_static_address(0x1890, \'percent\', floatHandler)\n    add_static_address(0x8E, \'stock\', byteHandler)\n\n    # nametag positions\n    add_static_address(0x10, \'x\', floatHandler)\n    add_static_address(0x14, \'y\', floatHandler)\n    add_static_address(0x18, \'z\', floatHandler)\n\n    """""" TODO: figure out why these don\'t work\n    #add_static_address(0x688, \'controller/stick_MAIN/x\', floatHandler)\n    #add_static_address(0x68C, \'controller/stick_MAIN/y\', floatHandler)\n\n    add_static_address(0x698, \'controller/stick_C/x\', floatHandler)\n    add_static_address(0x69C, \'controller/stick_C/y\', floatHandler)\n\n    add_static_address(0x6BC, \'controller/button_Z\', IntHandler(mask=1<<4))\n    add_static_address(0x6BC, \'controller/button_L\', IntHandler(mask=1<<5))\n    add_static_address(0x6BC, \'controller/button_R\', IntHandler(mask=1<<6))\n\n    add_static_address(0x6BC, \'controller/button_A\', IntHandler(mask=1<<8))\n    add_static_address(0x6BC, \'controller/button_B\', IntHandler(mask=1<<9))\n\n    add_static_address(0x6BC, \'controller/button_X\', IntHandler(mask=1<<10))\n    add_static_address(0x6BC, \'controller/button_Y\', IntHandler(mask=1<<11))\n    """"""\n\n    # hitbox positions\n    # add_static_address(0x18B4, \'x\', floatHandler)\n    # add_static_address(0x18B8, \'y\', floatHandler)\n    # add_static_address(0x18BC, \'z\', floatHandler)\n\n    data_pointer = add_address(\'80453130\', 0xE90 * player_id)\n\n    def add_data_address(offset, name, handler):\n      address = data_pointer + \' \' + offset\n      handle = playerHandler(name, handler)\n      if address not in addresses:\n        addresses[address] = [handle]\n      else:\n        addresses[address].append(handle)\n\n    add_data_address(\'70\', \'action_state\', intHandler)\n    add_data_address(\'20CC\', \'action_counter\', shortHandler)\n    add_data_address(\'8F4\', \'action_frame\', floatHandler)\n\n    add_data_address(\'19EC\', \'invulnerable\', intHandler)\n\n    add_data_address(\'19BC\', \'hitlag_frames_left\', floatHandler)\n    add_data_address(\'23A0\', \'hitstun_frames_left\', floatHandler)\n    # TODO: make this an actal int\n    # 2 = charging, 3 = attacking, 0 = otherwise\n    add_data_address(\'2174\', \'charging_smash\', IntHandler(mask=0x2))\n    \n    add_data_address(\'19F8\', \'shield_size\', floatHandler)\n\n    add_data_address(\'19C8\', \'jumps_used\', byteHandler)\n    add_data_address(\'140\', \'in_air\', intHandler)\n\n    add_data_address(\'E0\', \'speed_air_x_self\', floatHandler)\n    add_data_address(\'E4\', \'speed_y_self\', floatHandler)\n    add_data_address(\'EC\', \'speed_x_attack\', floatHandler)\n    add_data_address(\'F0\', \'speed_y_attack\', floatHandler)\n    add_data_address(\'14C\', \'speed_ground_x_self\', floatHandler)\n\n    add_data_address(\'8C\', \'facing\', floatHandler) # 1 is right, -1 is left\n    #add_data_address(\'1E4\', \'speed_fastfall_self\', floatHandler)\n\n    return addresses\n\nclass StateManager(object):\n    def __init__(self, player_ids=range(4)):\n        self.addresses = global_addresses.copy()\n\n        for player_id in player_ids:\n            playerAddresses(player_id, self.addresses)\n\n    def handle(self, obj, address, value):\n        """"""Convert the raw address and value into changes in the State.""""""\n        assert address in self.addresses\n        handlers = self.addresses[address]\n        if isinstance(handlers, list):\n            for handler in handlers:\n                handler(obj, value)\n        else:\n            handlers(obj, value)\n\n    def locations(self):\n        """"""Returns a list of addresses for exporting to Locations.txt.""""""\n        return self.addresses.keys()\n'"
phillip/tf_lib.py,93,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.framework.python.framework import checkpoint_utils\nimport itertools\nfrom phillip.default import *\nfrom phillip import util\n\ndef leaky_relu(x, alpha=0.01):\n  return tf.maximum(alpha * x, x)\n\ndef log_sum_exp(xs):\n  maxes = tf.reduce_max(xs, -1, keep_dims=True)\n  maxes = tf.stop_gradient(maxes)\n  return tf.squeeze(maxes, [-1]) + tf.log(tf.reduce_sum(tf.exp(xs-maxes), -1))\n\ndef leaky_softplus(x, alpha=0.01):\n  ""Really just a special case of log_sum_exp.""\n  ax = alpha * x\n  maxes = tf.stop_gradient(tf.maximum(ax, x))\n  return maxes + tf.log(tf.exp(ax - maxes) + tf.exp(x - maxes))\n\nclass NL(Default):\n  _options = [\n    Option(\'nl\', type=str, choices=[\'leaky_relu\', \'leaky_softplus\', \'elu\', \'relu\', \'tanh\', \'sigmoid\'], default=\'leaky_softplus\'),\n    Option(\'alpha\', type=float, default=0.01),\n  ]\n  \n  def __call__(self, x):\n    if self.nl == \'leaky_relu\':\n      return leaky_relu(x, self.alpha)\n    elif self.nl == \'leaky_softplus\':\n      return leaky_softplus(x, self.alpha)\n    else:\n      return getattr(tf.nn, self.nl)(x)\n\ndef batch_dot(xs, ys):\n  return tf.reduce_sum(tf.multiply(xs, ys), -1)\n\ndef dot(x, y):\n  return tf.reduce_sum(x * y)\n\ndef power(x, p):\n  if p == 1:\n    return x\n  if p == -1:\n    return tf.reciprocal(x)\n  return tf.pow(x, p)\n\ndef geometric_mean(xs):\n  return tf.exp(tf.reduce_mean(tf.log(xs)))\n\ndef power_mean(p, xs):\n  if p == 0:\n    return geometric_mean(xs)\n  return power(tf.reduce_mean(power(xs, p)), 1/p)\n\ndef sym_kl(logp, logq):\n  return 0.5 * batch_dot(tf.exp(logp) - tf.exp(logq), logp - logq)\n\ndef kl(logp, logq):\n  return batch_dot(tf.exp(logp), logp - logq)\n\ndef sample_variance(xs):\n  return tf.reduce_mean(tf.squared_difference(xs, tf.reduce_mean(xs)))\n\ndef stats(xs, name=None, minmax=False):\n  mean = tf.reduce_mean(xs)\n  std = tf.sqrt(tf.reduce_mean(tf.squared_difference(xs, mean)))\n  \n  if name:\n    tf.summary.scalar(name + \'/mean\', mean)\n    tf.summary.scalar(name + \'/std\', std)\n    if minmax:\n      tf.summary.scalar(name + \'/min\', tf.reduce_min(xs))\n      tf.summary.scalar(name + \'/max\', tf.reduce_max(xs))\n\n  return mean, std\n\ndef apply_grads(params, grads):\n  return tf.group(*[tf.assign_add(p, g) for p, g in zip(params, grads)])\n\ndef scale_gradient(t, scale):\n  return (1.-scale) * tf.stop_gradient(t) + scale * t\n\ndef windowed(t, n):\n  """"""Gives a windowed view into a Tensor.\n  \n  Args:\n    t: The input Tensor with shape [T, ...]\n    n: An integer >= 0.\n  Returns:\n    A Tensor with shape [n+1, T-n, ...]\n  """"""\n  return tf.stack([t[i:i-n] for i in range(n)] + [t[n:]])\n\ndef scaled_weight_variable(shape):\n    \'\'\'\n    Generates a TensorFlow Tensor. This Tensor gets initialized with values sampled from the truncated normal\n    distribution. Its purpose will be to store model parameters.\n    :param shape: The dimensions of the desired Tensor\n    :return: The initialized Tensor\n    \'\'\'\n    #input_size = util.product(shape[:-1])\n    w = tf.Variable(tf.truncated_normal(shape, stddev=1.0), name=\'weight\')\n    \n    norms = tf.sqrt(tf.reduce_sum(tf.square(w), list(range(len(shape)-1))))\n    w /= norms\n    \n    scale = tf.Variable(tf.truncated_normal(shape[-1:], stddev=1.0), name=\'scale\')\n    \n    return scale * w\n\ndef weight_init(shape):\n    initial = tf.random_normal(shape, stddev=1.0)\n    \n    norms = tf.sqrt(tf.reduce_sum(tf.square(initial), list(range(len(shape)-1))))\n    initial /= norms\n    \n    return initial\n\ndef weight_variable(shape, name=""weight""):\n    return tf.Variable(weight_init(shape), name=name)\n\ndef bias_init(shape):\n    return tf.truncated_normal(shape, stddev=0.1)\n\ndef bias_variable(shape, name=""bias""):\n    return tf.Variable(bias_init(shape), name=name)\n\ndef constant_init(c):\n    return lambda shape: tf.constant(c, shape=shape)\n\ndef conv2d(x, W):\n    \'\'\'\n    Generates a conv2d TensorFlow Op. This Op flattens the weight matrix (filter) down to 2D, then ""strides"" across the\n    input Tensor x, selecting windows/patches. For each little_patch, the Op performs a right multiply:\n            W . little_patch\n    and stores the result in the output layer of feature maps.\n    :param x: a minibatch of images with dimensions [batch_size, height, width, 3]\n    :param W: a ""filter"" with dimensions [window_height, window_width, input_channels, output_channels]\n    e.g. for the first conv layer:\n          input_channels = 3 (RGB)\n          output_channels = number_of_desired_feature_maps\n    :return: A TensorFlow Op that convolves the input x with the filter W.\n    \'\'\'\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\'SAME\')\n\ndef max_pool_2x2(x):\n    \'\'\'\n    Genarates a max-pool TensorFlow Op. This Op ""strides"" a window across the input x. In each window, the maximum value\n    is selected and chosen to represent that region in the output Tensor. Hence the size/dimensionality of the problem\n    is reduced.\n    :param x: A Tensor with dimensions [batch_size, height, width, 3]\n    :return: A TensorFlow Op that max-pools the input Tensor, x.\n    \'\'\'\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                          strides=[1, 2, 2, 1], padding=\'SAME\')\n\ndef convLayer(x, filter_size=5, filter_depth=64, pool_size=2):\n  x_depth = x.get_shape()[-1].value\n  W = weight_variable([filter_size, filter_size, x_depth, filter_depth])\n  conv = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\'SAME\')\n\n  b = bias_variable([filter_depth])\n  relu = tf.nn.relu(conv + b)\n\n  pool = tf.nn.max_pool(relu,\n                        ksize=[1,pool_size,pool_size,1],\n                        strides=[1,pool_size,pool_size,1],\n                        padding = \'SAME\')\n\n  return pool\n\ndef softmax(x):\n  input_shape = tf.shape(x)\n  input_rank = tf.shape(input_shape)[0]\n  input_size = tf.gather(input_shape, input_rank-1)\n  output_shape = input_shape\n  \n  x = tf.reshape(x, [-1, input_size])\n\n  y = tf.nn.softmax(x)\n  y = tf.reshape(y, output_shape)\n  \n  return y\n\ndef matmul(v, m):\n  v = tf.expand_dims(v, -1)\n  vm = tf.multiply(v, m)\n  return tf.reduce_sum(vm, -2)\n\n# I think this is the more efficient version?\ndef matmul2(x, m, bias=None, nl=None):\n  [input_size, output_size] = m.get_shape().as_list()\n  input_shape_py = x.get_shape().as_list()\n  assert(input_shape_py[-1] == input_size)\n  \n  input_shape_tf = tf.shape(x)\n  batch_rank = len(input_shape_py) - 1\n  batch_shape_tf = input_shape_tf[:batch_rank]\n  output_shape_tf = tf.concat(axis=0, values=[batch_shape_tf, [output_size]])\n  \n  squashed = tf.reshape(x, [-1, input_size])\n  y = tf.matmul(squashed, m)\n  \n  if bias is not None:\n    y += bias\n  \n  if nl is not None:\n    y = nl(y)\n  \n  y = tf.reshape(y, output_shape_tf)\n  \n  # fix shape inference\n  output_shape_py = input_shape_py.copy()\n  output_shape_py[-1] = output_size\n  y.set_shape(output_shape_py)\n  \n  return y\n\ndef cloneVar(var):\n  return tf.Variable(var.initialized_value())\n\nclass FCLayer(Default):\n  _options = [\n    Option(""weight_init"", default=weight_init),\n    Option(""bias_init"", default=bias_init),\n  ]\n  \n  def __init__(self, input_size=None, output_size=None, nl=None, clone=None, **kwargs):\n    if clone:\n      self.input_size = clone.input_size\n      self.output_size = clone.output_size\n      self.nl = clone.nl\n      \n      self.weight = cloneVar(clone.weight)\n      self.bias = cloneVar(clone.bias)\n    else:\n      Default.__init__(self, **kwargs)\n      \n      self.input_size = input_size\n      self.output_size = output_size\n      self.nl = nl\n      \n      self.weight = tf.Variable(self.weight_init([input_size, output_size]), name=""weight"")\n      self.bias = tf.Variable(self.bias_init([output_size]), name=""bias"")\n  \n  def __call__(self, x):\n    return matmul2(x, self.weight, self.bias, self.nl)\n    \n  def clone(self):\n    return FCLayer(clone=self)\n  \n  def assign(self, other):\n    return [\n      self.weight.assign(other.weight),\n      self.bias.assign(other.bias),\n    ]\n  \n  def getVariables(self):\n    return [self.weight, self.bias]\n\nclass Sequential:\n  def __init__(self, *layers):\n    self.layers = list(layers)\n  \n  def append(self, layer):\n    self.layers.append(layer)\n  \n  def __call__(self, x):\n    for f in self.layers:\n      x = f(x)\n    return x\n  \n  def clone(self):\n    layers = [layer.clone() for layer in self.layers]\n    return Sequential(*layers)\n  \n  def assign(self, other):\n    assignments = [l1.assign(l2) for l1, l2 in zip(self.layers, other.layers)]\n    return list(itertools.chain(*assignments))\n\n  def getVariables(self):\n    variables = [layer.getVariables() for layer in self.layers]\n    return list(itertools.chain(*variables))\n\ndef affineLayer(x, output_size, nl=None):\n  W = weight_variable([x.get_shape()[-1].value, output_size])\n  b = bias_variable([output_size])\n\n  fc = matmul2(x, W) + b\n\n  return nl(fc) if nl else fc\n\ndef makeAffineLayer(input_size, output_size, nl=None):\n  W = weight_variable([input_size, output_size])\n  b = bias_variable([output_size])\n\n  def applyLayer(x):\n    return matmul2(x, W, b, nl)\n\n  return applyLayer\n\ndef clamp(x, minimum, maximum):\n  return tf.minimum(tf.maximum(x, minimum), maximum)\n\ndef one_hot(size):\n  """"""\n  A clamped integer to one-hot vector function.\n  """"""\n  return lambda t: tf.one_hot(\n      clamp(tf.cast(t, tf.int64), 0, size - 1),\n      size,\n      1.0,\n      0.0)\n\ndef rank(t):\n  return tf.shape(tf.shape(t))[0]\n\nclass GRUCell(tf.contrib.rnn.RNNCell):\n  def __init__(self, input_size, hidden_size, nl=tf.tanh, name=None):\n    with tf.variable_scope(name or type(self).__name__):\n      with tf.variable_scope(""Gates""):\n        self.Wru = weight_variable([input_size + hidden_size, 2 * hidden_size])\n        self.bru = tf.Variable(tf.constant(1.0, shape=[2 * hidden_size]), name=\'bias\')\n      with tf.variable_scope(""Candidate""):\n        self.Wc = weight_variable([input_size + hidden_size, hidden_size])\n        self.bc = bias_variable([hidden_size])\n    self.nl = nl\n    self._num_units = hidden_size\n\n  @property\n  def state_size(self):\n    return self._num_units\n\n  @property\n  def output_size(self):\n    return self._num_units  \n  \n  def __call__(self, inputs, state):\n    ru = tf.sigmoid(matmul2(tf.concat(axis=-1, values=[inputs, state]), self.Wru) + self.bru)\n    r, u = tf.split(axis=-1, num_or_size_splits=2, value=ru)\n    \n    c = self.nl(matmul2(tf.concat(axis=-1, values=[inputs, r * state]), self.Wc) + self.bc)\n    new_h = u * state + (1 - u) * c\n    \n    return new_h, new_h\n  \n  def getVariables(self):\n    return [self.Wru, self.bru, self.Wc, self.bc]\n\n# auto unpacks and repacks inputs\ndef rnn(cell, inputs, initial_state, time=1):\n  inputs = tf.unstack(inputs, axis=time)\n  outputs = []\n  state = initial_state\n  for i, input_ in enumerate(inputs):\n    output, state = cell(input_, state)\n    outputs.append(output)\n  return tf.stack(outputs, axis=time), state\n\ndef scan(f, inputs, initial_state, axis=0):\n  inputs = util.deepIter(util.deepMap(lambda t: iter(tf.unstack(t, axis=axis)), inputs))\n  outputs = []\n  output = initial_state\n  for input_ in inputs:\n    output = f(output, input_)\n    outputs.append(output)\n  return util.deepZipWith(lambda *ts: tf.stack(ts, axis=axis), *outputs)\n\ndef while_loop(cond, body, initial):\n  while cond(*initial):\n    initial = body(*initial)\n  return initial\n\nclass TensorArray(object):\n  def __init__(self, dtype, size, element_shape):\n    self.elems = [None] * size\n  \n  def write(self, i, t):\n    self.elems[i] = t\n    return self\n  \n  def stack(self):\n    return tf.stack(self.elems)\n\ndef discount(values, gamma, initial=None):\n  values = tf.unstack(values, axis=1)\n  \n  if initial is None:\n    current = tf.zeros_like(values[0])\n  else:\n    current = initial\n  \n  for i in reversed(range(len(values))):\n    current = values[i] + gamma * current\n    values[i] = current\n  \n  return tf.stack(values, axis=1)\n\ndef discount2(values, gamma, initial=None):\n  """"""Compute returns from rewards.\n  \n  Uses tf.while_loop instead of unrolling in python.\n  \n  Arguments:\n    values: Tensor with shape [time, batch]\n    gamma: Discount factor.\n    initial: Value past the end.\n  \n  Returns:\n    A tensor of discounted returns.\n  """"""\n  \n  def body(i, prev, returns):\n    next = values[i] + gamma * prev\n    next.set_shape(prev.get_shape())\n    \n    returns = returns.write(i, next)\n\n    return (i-1, next, returns)\n\n  def cond(i, prev, returns):\n    return i >= 0\n  \n  if initial is None:\n    initial = tf.zeros(tf.shape(values)[1:], values.dtype)\n\n  timesteps = tf.shape(values)[0]\n\n  ta = tf.TensorArray(values.dtype, timesteps)\n  \n  _, _, returns = tf.while_loop(cond, body, (timesteps-1, initial, ta))\n  \n  return returns.stack()\n\ndef testDiscounts():\n  values = tf.constant([[1, 2, 3]])\n  gamma = 2\n  initial = tf.constant([4])\n  \n  correct = [[49, 24, 11]]\n  \n  fs = [discount, discount2]\n  \n  fetches = [f(values, gamma, initial) for f in fs]\n\n  sess = tf.Session()\n  returns = sess.run(fetches)\n  \n  for r in returns:\n    assert((r == correct).all())\n\n  print(""Passed testDiscount()"")\n\ndef smoothed_returns(values, rewards, gamma, lambda_, bootstrap, dynamic=True):\n  def bellman(future, present):\n    v, r, l = present\n    return (1. - l) * v + l * (r + gamma * future)\n  \n  reversed_sequence = [tf.reverse(t, [0]) for t in [values, rewards, lambda_]]\n  scan_fn = tf.scan if dynamic else scan\n  returns = scan_fn(bellman, reversed_sequence, bootstrap)\n  returns = tf.reverse(returns, [0])\n  return returns\n\ndef test_smoothed_returns():\n  values = tf.zeros([3, 1])\n  rewards = tf.constant([[1.], [2.], [3.]])\n  gamma = 2\n  lambda_ = tf.ones([3, 1])\n  initial = tf.constant([4.])\n  \n  correct = [[49], [24], [11]]\n  \n  fs = [smoothed_returns]\n  \n  fetches = [f(values, rewards, gamma, lambda_, initial) for f in fs]\n\n  sess = tf.Session()\n  returns = sess.run(fetches)\n  \n  for r in returns:\n    assert((r == correct).all())\n\n  print(""Passed test_smoothed_returns()"")\n\ndef restore(session, variables, ckpt_path):\n  """"""Does what a saver would do, but handles mismatched shapes.""""""\n  ckpt = checkpoint_utils.load_checkpoint(ckpt_path)\n\n  for var in variables:\n    name = var.name\n    if name.endswith("":0""):\n      name = name[:-2]\n    if not ckpt.has_tensor(name):\n      print(""%s not in checkpoint, initializing"" % name)\n      var.load(session.run(var.initial_value), session)\n      continue\n    value = ckpt.get_tensor(name)\n    pads = [(0, d1 - d2) for d1, d2 in zip(var.get_shape().as_list(), value.shape)]\n    needs_pad = any([p[1] for p in pads])\n    if needs_pad:\n      print(""Variable %s of shape %s padded from %s"" % (var.name, var.get_shape().as_list(), value.shape))\n      value = np.pad(value, pads, ""constant"")\n    var.load(value, session)\n'"
phillip/thompson_dqn.py,14,"b'import tensorflow as tf\nimport tf_lib as tfl\nimport util\nfrom numpy import random, exp, argmax\n\nclass ThompsonDQN:\n  def __init__(self, state_size, action_size, global_step, rlConfig, epsilon=0.04, **kwargs):\n    self.action_size = action_size\n    self.layer_sizes = [state_size, 128, 128]\n    self.layers = []\n\n    for i in range(len(self.layer_sizes)-1):\n      prev_size = self.layer_sizes[i]\n      next_size = self.layer_sizes[i+1]\n\n      with tf.variable_scope(""layer_%d"" % i):\n        self.layers.append(tfl.makeAffineLayer(prev_size, next_size, tfl.leaky_relu))\n\n    with tf.variable_scope(\'mean\'):\n      mean = tfl.makeAffineLayer(self.layer_sizes[-1], action_size)\n\n    with tf.variable_scope(\'log_variance\'):\n      log_variance = tfl.makeAffineLayer(self.layer_sizes[-1], action_size)\n\n    self.layers.append(lambda x: (mean(x), log_variance(x)))\n    \n    self.rlConfig = rlConfig\n    self.epsilon = epsilon\n\n  def getLayers(self, state):\n    outputs = [state]\n    for i, f in enumerate(self.layers):\n      with tf.name_scope(\'q%d\' % i):\n        outputs.append(f(outputs[-1]))\n\n    return outputs\n\n  def getQDists(self, state):\n    return self.getLayers(state)[-1]\n\n  def getQValues(self, state):\n    return self.getQDists(state)[0]\n\n  def getLoss(self, states, actions, rewards, sarsa=False, **kwargs):\n    ""Negative Log-Likelihood""\n    \n    n = self.rlConfig.tdN\n    train_length = [config.experience_length - n]\n    \n    qMeans, qLogVariances = self.getQDists(states)\n    \n    realQs = tfl.batch_dot(actions, qMeans)\n    maxQs = tf.reduce_max(qMeans, 1)\n\n    # smooth between TD(m) for m<=n?\n    targets = tf.slice(realQs if sarsa else maxQs, [n], train_length)\n    for i in reversed(range(n)):\n      targets = tf.slice(rewards, [i], train_length) + self.rlConfig.discount * targets\n    targets = tf.stop_gradient(targets)\n\n    trainQs = tf.slice(realQs, [0], train_length)\n    \n    realLogVariances =  tfl.batch_dot(actions, qLogVariances)\n    trainLogVariances = tf.slice(realLogVariances, [0], train_length)\n    \n    nlls = tf.squared_difference(trainQs, targets) * tf.exp(-trainLogVariances) + trainLogVariances\n    nll = tf.reduce_mean(nlls)\n    return nll, [(""nll"", nll)]\n\n  def getPolicy(self, state, policy=None, temperature=1, **kwargs):\n    qMeans, qLogVariances = self.getQDists(state)\n    qSTDs = tf.exp(qLogVariances / 2) * temperature\n    qDists = tf.concat(axis=2, values=[tf.expand_dims(x, 2) for x in [qMeans, qSTDs]])\n    return qDists\n  \n  def act(self, policy, verbose=False):\n    if util.flip(self.epsilon):\n      return random.randint(0, self.action_size)\n    \n    [qDists] = policy\n    if verbose:\n      print(""qDists"", qDists)\n    \n    samples = [random.normal(mean, std) for mean, std in qDists]\n    return argmax(samples)\n\n'"
phillip/train.py,7,"b'import os, sys\nimport time\nfrom phillip import learner, util, ssbm\nfrom phillip.ac import ActorCritic\nfrom phillip.default import *\nimport numpy as np\nfrom collections import defaultdict\nimport nnpy\nimport resource\nimport gc\nimport tensorflow as tf\n#from memory_profiler import profile\nimport netifaces\nimport random\nfrom random import shuffle\n\n# some helpers for debugging memory leaks\n\ndef count_objects():\n  counts = defaultdict(int)\n  for obj in gc.get_objects():\n    counts[type(obj)] += 1\n  return counts\n\ndef diff_objects(after, before):\n  diff = {k: after[k] - before[k] for k in after}\n  return {k: i for k, i in diff.items() if i}\n\nclass Trainer(Default):\n  _options = [\n    #Option(""debug"", action=""store_true"", help=""set debug breakpoint""),\n    #Option(""-q"", ""--quiet"", action=""store_true"", help=""don\'t print status messages to stdout""),\n    Option(""init"", action=""store_true"", help=""initialize variables""),\n\n    Option(""sweep_limit"", type=int, default=-1),\n    Option(""batch_size"", type=int, default=1, help=""number of trajectories per batch""),\n    Option(""batch_steps"", type=int, default=1, help=""number of gradient steps to take on each batch""),\n    Option(""min_collect"", type=int, default=1, help=""minimum number of experiences to collect between sweeps""),\n    Option(""max_age"", type=int, help=""how old an experience can be before we discard it""),\n    Option(""max_kl"", type=float, help=""how off-policy an experience can be before we discard it""),\n    Option(""max_buffer"", type=int, help=""maximum size of experience buffer""),\n    Option(\'buffer_ratio\', type=float, default=0., help=""fraction of batch taken from buffer""),\n\n    Option(""log_interval"", type=int, default=100),\n    Option(""dump"", type=str, default=""lo"", help=""interface to listen on for experience dumps""),\n    Option(\'send\', type=int, default=1, help=""send the network parameters on an nnpy PUB socket""),\n    Option(""save_interval"", type=float, default=10, help=""length of time between saves to disk, in minutes""),\n\n    Option(""load"", type=str, help=""path to a json file from which to load params""),\n    Option(""pop_size"", type=int, default=0),\n    Option(""evo_period"", type=int, default=1000, help=""evolution period""),\n    Option(""reward_cutoff"", type=float, default=1e-3),\n\n    Option(\'objgraph\', type=int, default=0, help=\'use objgraph to track memory usage\'),\n    Option(\'diff_objects\', action=\'store_true\', help=\'print new objects on each iteration\')\n  ]\n  \n  _members = [\n    (""learner"", learner.Learner),\n  ]\n  \n  def __init__(self, load=None, **kwargs):\n    if load is None:\n      args = {}\n    else:\n      args = util.load_params(load, \'train\')\n\n    util.update(args, **kwargs)\n    util.pp.pprint(args)\n    Default.__init__(self, **args)\n\n    addresses = netifaces.ifaddresses(self.dump)\n    address = addresses[netifaces.AF_INET][0][\'addr\']\n\n    util.makedirs(self.learner.path)\n    with open(os.path.join(self.learner.path, \'ip\'), \'w\') as f:\n      f.write(address)\n\n    self.experience_socket = nnpy.Socket(nnpy.AF_SP, nnpy.PULL)\n    experience_addr = ""tcp://%s:%d"" % (address, util.port(self.learner.path + ""/experience""))\n    self.experience_socket.bind(experience_addr)\n\n    if self.send:\n      self.params_socket = nnpy.Socket(nnpy.AF_SP, nnpy.PUB)\n      params_addr = ""tcp://%s:%d"" % (address, util.port(self.learner.path + ""/params""))\n      print(""Binding params socket to"", params_addr)\n      self.params_socket.bind(params_addr)\n\n    self.sweep_size = self.batch_size\n    print(""Sweep size"", self.sweep_size)\n    \n    if self.init:\n      self.learner.init()\n      self.learner.save()\n    else:\n      self.learner.restore()\n    \n    self.last_save = time.time()\n  \n  def save(self):\n    current_time = time.time()\n    \n    if current_time - self.last_save > 60 * self.save_interval:\n      try:\n        self.learner.save()\n        self.last_save = current_time\n      except tf.errors.InternalError as e:\n        print(e, file=sys.stderr)\n\n  def selection(self):\n    reward = self.learner.get_reward()\n    print(""Evolving. Current reward %f"" % reward)\n    \n    target_id = random.randint(0, self.pop_size-2)\n    if target_id >= self.learner.pop_id:\n      target_id += 1\n    print(""Selection candidate: %d"" % target_id)\n    \n    target_path = os.path.join(self.learner.root, str(target_id))\n    latest_ckpt = tf.train.latest_checkpoint(target_path)\n    reader = tf.train.NewCheckpointReader(latest_ckpt)\n    target_reward = reader.get_tensor(\'avg_reward\')\n    \n    if target_reward - reward < self.reward_cutoff:\n      print(""Target reward %f too low."" % target_reward)\n      return False\n    \n    print(""Selecting %d"" % target_id)\n    self.learner.restore(latest_ckpt)\n    return True\n\n  def train(self):\n    before = count_objects()\n\n    sweeps = 0\n    step = 0\n    global_step = self.learner.get_global_step()\n    \n    times = [\'min_collect\', \'extra_collect\', \'train\', \'save\']\n    averages = {name: util.MovingAverage(.1) for name in times}\n    \n    timer = util.Timer()\n    def split(name):\n      averages[name].append(timer.split())\n\n    num_fresh = max(self.min_collect, int((1 - self.buffer_ratio) * self.batch_size))\n    num_old = self.batch_size - num_fresh\n    print(""num_old"", num_old, ""num_fresh"", num_fresh)\n      \n    def pull_experience(block=True):\n      exp = self.experience_socket.recv(flags=0 if block else nnpy.DONTWAIT)\n      return pickle.loads(exp)\n\n    use_replay = self.max_buffer and self.buffer_ratio\n\n    # fill the buffer\n    if use_replay:\n      print(""Filling replay buffer"")\n      replay_buffer = util.CircularQueue(array=[pull_experience() for _ in range(self.max_buffer)])\n      print(""Filled replay buffer"")\n    \n    while sweeps != self.sweep_limit:\n      sweeps += 1\n      timer.reset()\n      \n      #print(\'Start: %s\' % resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n\n      if self.max_age is not None:\n        age_limit = global_step - self.max_age\n        is_valid = lambda exp: exp[\'global_step\'] >= age_limit\n        # experiences = list(filter(is_valid, experiences))\n      else:\n        is_valid = lambda _: True\n      # dropped = old_len - len(experiences)\n\n      # to_collect = max(self.sweep_size - len(experiences), self.min_collect)\n      to_collect = num_fresh\n      new_experiences = []\n\n      # print(""Collecting experiences"", len(experiences))\n      doa = 0 # dead on arrival\n      while len(new_experiences) < to_collect:\n        #print(""Waiting for experience"")\n        exp = pull_experience()\n        if is_valid(exp):\n          new_experiences.append(exp)\n        else:\n          #print(""dead on arrival"", doa)\n          doa += 1\n\n      split(\'min_collect\')\n      #print(\'min_collected\')\n\n      # pull in all the extra experiences\n      for _ in range(self.sweep_size):\n        try:\n          exp = pull_experience(False)\n          if is_valid(exp):\n            new_experiences.append(exp)\n          else:\n            doa += 1\n        except nnpy.NNError as e:\n          if e.error_no == nnpy.EAGAIN:\n            # nothing to receive\n            break\n          # a real error\n          raise e\n\n      if use_replay:\n        old_experiences = random.sample(replay_buffer.array, num_old)\n        experiences = old_experiences + new_experiences\n      else:\n        experiences = new_experiences\n\n      # prevent OOM if too many come in\n      experiences = experiences[-self.batch_size:]\n\n      ages = np.array([global_step - exp[\'global_step\'] for exp in experiences])\n      print(""Mean age:"", ages.mean() / self.learner.num_steps_per_batch)\n\n      #print(\'After collect: %s\' % resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n      split(\'extra_collect\')\n      \n      try:\n        train_out = self.learner.train(\n            experiences, self.batch_steps,\n            log=(step%self.log_interval==0),\n        )[-1]\n        global_step = train_out[\'global_step\']\n        # print(""global_step"", global_step)\n        step += 1\n      except tf.errors.InvalidArgumentError as e:\n        # always a NaN in histogram summary for entropy - what\'s up with that?\n        new_experiences = []\n        print(e)\n        continue\n\n      # add new experiences to the replay buffer\n      if use_replay:\n        for exp in new_experiences:\n          replay_buffer.push(exp)\n      \n      #print(\'After train: %s\' % resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n      split(\'train\')\n\n      if self.pop_size > 1 and sweeps % self.evo_period == 0:\n        if self.selection():\n          experiences = []\n        self.learner.mutation()\n\n      if self.send:\n        #self.params_socket.send_string("""", zmq.SNDMORE)\n        params = self.learner.blob()\n        global_step = params[\'global_step:0\']\n        blob = pickle.dumps(params)\n        #print(\'After blob: %s\' % resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n        self.params_socket.send(blob)\n        #print(\'After send: %s\' % resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n\n      self.save()\n      \n      #print(\'After save: %s\' % resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n      split(\'save\')\n      \n      if self.diff_objects:\n        after = count_objects()\n        print(diff_objects(after, before))\n        before = after\n\n      f3 = lambda f: \'%.3f\' % f\n      time_avgs = [averages[name].avg for name in times]\n      total_time = sum(time_avgs)\n      time_avgs = [f3(t / total_time) for t in time_avgs]\n      print(sweeps, len(new_experiences), doa, f3(total_time), *time_avgs)\n      #print(\'Memory usage: %s (kb)\' % resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n\n      if self.objgraph:\n        import objgraph\n        #gc.collect()  # don\'t care about stuff that would be garbage collected properly\n        objgraph.show_growth()\n  \n  def fake_train(self):\n    experience = (ssbm.SimpleStateAction * self.learner.config.experience_length)()\n    experience = ssbm.prepareStateActions(experience)\n    experience[\'initial\'] = util.deepMap(np.zeros, self.learner.core.hidden_size)\n    \n    experiences = [experience] * self.batch_size\n    \n    # For more advanced usage, user can control the tracing steps and\n    # dumping steps. User can also run online profiling during training.\n    #\n    # Create options to profile time/memory as well as parameters.\n    builder = tf.profiler.ProfileOptionBuilder\n    opts = builder(builder.time_and_memory()).order_by(\'micros\').build()\n    opts2 = tf.profiler.ProfileOptionBuilder.trainable_variables_parameter()\n\n    # Collect traces of steps 10~20, dump the whole profile (with traces of\n    # step 10~20) at step 20. The dumped profile can be used for further profiling\n    # with command line interface or Web UI.\n    with tf.contrib.tfprof.ProfileContext(\'/tmp/train_dir\',\n                                          trace_steps=range(10, 20),\n                                          dump_steps=[20]) as pctx:\n      # Run online profiling with \'op\' view and \'opts\' options at step 15, 18, 20.\n      pctx.add_auto_profiling(\'op\', opts, [15, 18, 20])\n      # Run online profiling with \'scope\' view and \'opts2\' options at step 20.\n      pctx.add_auto_profiling(\'scope\', opts2, [20])\n      # High level API, such as slim, Estimator, etc.\n      count = 0\n      while count != self.sweep_limit:\n        self.learner.train(experiences, self.batch_steps)\n        count += 1\n\nif __name__ == \'__main__\':\n  from argparse import ArgumentParser\n  parser = ArgumentParser()\n\n  for opt in Trainer.full_opts():\n    opt.update_parser(parser)\n\n  for opt in ActorCritic.full_opts():\n    opt.update_parser(parser)\n      \n  parser.add_argument(\'--fake\', action=\'store_true\', help=\'Train on fake experiences for debugging.\')\n\n  args = parser.parse_args()\n  trainer = Trainer(**args.__dict__)\n  if args.fake:\n    trainer.fake_train()\n  else:\n    trainer.train()\n\n'"
phillip/train_model.py,0,"b'""""""\nAlternative to train.py. Not run by default. \n\n(Alex Zhu 5/28/18: not tested, might not run properly)\n""""""\n\n\nfrom phillip import ssbm, util, learner\nfrom phillip.default import Default, Option\nimport hickle\nimport time\n\nclass ModelTrainer(Default):\n  _options = [\n    Option(\'data\', type=str, help=\'path to experience file\'),\n    Option(\'load\', type=str, help=\'path to params + snapshot\'),\n    Option(\'init\', action=\'store_true\'),\n    Option(\'batch_size\', type=int, default=1),\n    Option(\'valid_batches\', type=int, default=1),\n    Option(\'file_limit\', type=int, default=0, help=""0 means no limit""),\n    Option(\'epochs\', type=int, default=1000000),\n  ]\n  \n  _members = [\n    (\'learner\', learner.Learner),\n  ]\n\n  def __init__(self, load=None, **kwargs):\n    if load is None:\n      args = {}\n    else:\n      args = util.load_params(load, \'train\')\n    \n    util.pp.pprint(args)\n    Default.__init__(self, **args)\n\n    if self.init:\n      self.learner.init()\n      self.learner.save()\n    else:\n      self.learner.restore()\n    \n    print(""Loading experiences from"", self.data)\n    \n    start_time = time.time()\n    self.experiences = hickle.load(self.data)\n    print(""Loaded experiences in %d seconds."" % (time.time() - start_time))\n    if \'initial\' not in self.experiences:\n      self.experiences[\'initial\'] = []\n\n  def train(self):\n    shape = self.experiences[\'action\'].shape\n    data_size = shape[0]\n    \n    batches = []\n    for i in range(0, data_size, self.batch_size):\n      batches.append(util.deepMap(lambda t: t[i:i+self.batch_size], self.experiences))\n  \n    valid_batches = batches[:self.valid_batches]\n    train_batches = batches[self.valid_batches:]\n    \n    for epoch in range(self.epochs):\n      print(""Epoch"", epoch)\n      start_time = time.time()\n      \n      for batch in train_batches:\n        self.learner.train(batch, log=False, zipped=True)\n      \n      print(time.time() - start_time) \n      \n      for batch in valid_batches:\n        self.learner.train(batch, train=False, zipped=True)\n\n      self.learner.save()\n\n      import resource\n      print(\'Memory usage: %s (kb)\' % resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n\ndef main(**kwargs):\n  ModelTrainer(**kwargs).train()\n\nif __name__ == ""__main__"":\n  from argparse import ArgumentParser\n  parser = ArgumentParser()\n\n  for opt in ModelTrainer.full_opts():\n    opt.update_parser(parser)\n\n  args = parser.parse_args()\n  main(**args.__dict__)\n\n'"
phillip/util.py,0,"b'from numpy import random\nimport functools\nimport operator\nfrom threading import Thread\nimport hashlib\nimport os\nimport pprint\nimport time\n\npp = pprint.PrettyPrinter(indent=2)\n\ndef foldl(f, init, l):\n  for x in l:\n    init = f(init, x)\n  return init\n\ndef foldl1(f, l):\n  return foldl(f, l[0], l[1:])\n\ndef foldr(f, init, l):\n  for x in reversed(l):\n    init = f(x, init)\n  return init\n\ndef foldr1(f, l):\n  return foldr(f, l[-1], l[:-1])\n\ndef scanl(f, init, l):\n  r = [init]\n  for x in l:\n    r.append(f(r[-1], x))\n  return r\n\ndef scanl1(f, l):\n  return scanl(f, l[0], l[1:])\n\ndef scanr(f, init, l):\n  r = [init]\n  for x in reversed(l):\n    r.append(f(x, r[-1]))\n  r.reverse()\n  return r\n\ndef scanr1(f, l):\n  return scanr(f, l[-1], l[:-1])\n\ndef zipWith(f, *sequences):\n  return [f(*args) for args in zip(*sequences)]\n\ndef compose(*fs):\n  ""compose(f1, f2, ..., fn)(x) = f1(f2( ... fn(x)))""\n  def composed(x):\n    for f in reversed(fs):\n      x = f(x)\n    return x\n  return composed\n\ndef deepMap(f, obj):\n  if isinstance(obj, dict):\n    return {k : deepMap(f, v) for k, v in obj.items()}\n  if isinstance(obj, (list, tuple)):\n    return type(obj)(deepMap(f, x) for x in obj)\n  return f(obj)\n\ndef deepValues(obj):\n  if isinstance(obj, dict):\n    for v in obj.values():\n      for x in deepValues(v):\n        yield x\n  elif isinstance(obj, list):\n    for v in obj:\n      for x in deepValues(v):\n        yield x\n  else: # note that tuples are values, not lists\n    yield obj\n\ndef deepZip(*objs):\n  if len(objs) == 0:\n    return []\n  \n  first = objs[0]\n  if isinstance(first, dict):\n    return {k : deepZip(*[obj[k] for obj in objs]) for k in first}\n  if isinstance(first, (list, tuple)):\n    return zipWith(deepZip, *objs)\n  return objs\n\ndef deepZipWith(f, *objs):\n  if len(objs) == 0:\n    return []\n  \n  first = objs[0]\n  if isinstance(first, dict):\n    return {k : deepZipWith(f, *[obj[k] for obj in objs]) for k in first}\n  if isinstance(first, (list, tuple)):\n    return type(first)(deepZipWith(f, *vals) for vals in zip(*objs))\n  return f(*objs)\n\ndef deepItems(obj, path=[]):\n  if isinstance(obj, dict):\n    for k, v in obj.items():\n      yield from deepItems(v, path=path+[k])\n  elif isinstance(obj, list):\n    for i, v in enumerate(obj):\n      yield from deepItems(v, path=path+[i])\n  else:\n    yield (path, obj)\n\ndef deepIter(iters):\n  if isinstance(iters, dict):\n    deep_iters = [(k, deepIter(v)) for k, v in iters.items()]\n    while True:\n      try:\n        yield {k: v.next() for k, v in deep_iters}\n      except StopIteration:\n        return\n  elif isinstance(iters, (list, tuple)):\n    yield from zip(*map(deepIter, iters))\n  else:\n    yield from iters\n\ndef flip(p):\n  return random.binomial(1, p)\n\ndef product(xs):\n  return functools.reduce(operator.mul, xs, 1.0)\n\ndef async_map(f, xs):\n  n = len(xs)\n  ys = n * [None]\n  \n  def run(i):\n    ys[i] = f(xs[i])\n\n  threads = n * [None]\n  for i in range(n):\n    threads[i] = Thread(target=run, args=[i])\n    threads[i].start()\n  \n  def wait():\n    for p in threads:\n      p.join()\n    return ys\n  \n  return wait\n\ndef chunk(l, n):\n  return [l[i:i+n] for i in range(0, len(l), n)]\n\nclass MovingAverage:\n  def __init__(self, rate=1e-2, initial=0):\n    self.rate = rate\n    self.avg = initial\n  \n  def append(self, val):\n    self.avg += self.rate * (val - self.avg)\n\nclass Timer:\n  def reset(self):\n    self.time = time.time()\n  \n  def split(self):\n    now = time.time()\n    delta = now - self.time\n    self.time = now\n    return delta\n\nclass CircularQueue:\n  def __init__(self, size=None, init=None, array=None):\n    if array:\n      self.size = len(array)\n      self.array = array\n    else:\n      self.size = size\n      self.array = [init] * size\n    self.index = 0\n  \n  def push(self, obj):\n    self.array[self.index] = obj\n    self.increment()\n    return self.array[self.index]\n  \n  def peek(self):\n    return self.array[self.index]\n  \n  def increment(self):\n    self.index += 1\n    self.index %= self.size\n  \n  def __getitem__(self, index):\n    return self.array[(self.size + self.index + index) % self.size]\n  \n  def __len__(self):\n    return self.size\n  \n  def as_list(self):\n    return self.array[self.index:] + self.array[:self.index]\n\ndef hashString(s):\n  s = s.encode()\n  return hashlib.md5(s).hexdigest()\n\ndef port(s):\n  s = os.path.abspath(s)\n  print(""PORT"", s)\n  return 5536 + int(hashString(s), 16) % 60000\n\ndef makedirs(path):\n  if not os.path.exists(path):\n    os.makedirs(path)\n\ndef update(dikt, **kwargs):\n  for k, v in kwargs.items():\n    if v is not None:\n      dikt[k] = v\n    elif k not in dikt:\n      dikt[k] = None\n\ndef load_params(path, key=None):\n  import json\n  with open(path + \'/params\') as f:\n    params = json.load(f)\n  \n  # support old-style separation of params into train and agent\n  if key and key in params:\n    params.update(params[key])\n  \n  params.update(path=path)\n  return params\n\n'"
scripts/error_frame.py,0,"b""import numpy as np\nimport pickle\n\nerror_frame_path = 'agents/delay12/FalcoBF/0/error_frame'\n\nwith open(error_frame_path, 'rb') as f:\n  error_frame = pickle.load(f)\n\nstate = error_frame['state']\ndel state['frame']\n\ndef check_io(checker):\n  def wrapper(f):\n    def g(*args, **kwargs):\n      output = f(*args, **kwargs)\n      checker(output, *args, **kwargs)\n      return output\n    return g\n  return wrapper\n\ndef check(output, struct):\n  assert isinstance(output[0], list)\n  assert follow(output[0], struct) == output[1]\n\ndef follow(path, struct):\n  for key in path:\n    struct = struct[key]\n  return struct  \n\n@check_io(check)\ndef reduce_max(struct):\n  if isinstance(struct, (tuple, list)):\n    maxes = list(map(reduce_max, struct))\n    best_index = 0\n    best_max = maxes[0]\n    \n    for i in range(1, len(maxes)):\n      if maxes[i][1] > best_max[1]:\n        best_index = i\n        best_max = maxes[i]\n    \n    return [best_index] + best_max[0], best_max[1]\n  elif isinstance(struct, dict):\n    keys, values = zip(*struct.items())\n    path, value = reduce_max(values)\n    return [keys[path[0]]] + path[1:], value\n    \n  elif isinstance(struct, np.ndarray):\n    index = np.argmax(struct)\n    return [index], struct[index]\n  \n  assert TypeError('invalid struct')\n\nerror_path, error_value = reduce_max(state)\nprint(error_path, error_value)\n\n"""
scripts/merge_data.py,0,"b'#!/usr/bin/env python3\n\nimport numpy as np\nimport glob\nfrom phillip import util\nimport pickle\n\nuse_hickle = True\nif use_hickle:\n  import hickle\n\npath = ""experience/FalconFalconBF""\n\ndef load_experience(path):\n  with open(path, \'rb\') as f:\n    return pickle.load(f)\n\ndef prune_experience(experience):\n  state = experience[\'state\']\n  state[\'players\'] = state[\'players\'][:2]\n  return experience\n\nprune_load = util.compose(prune_experience, load_experience)\n\npaths = glob.glob(path+\'/*\')[:100]\nexperiences = []\nfor i, p in enumerate(paths):\n  if i % 10 == 0:\n    print(\'reading %d/%d\' % (i, len(paths)))\n  experiences.append(prune_load(p))\n\ndef to_array(*xs):\n  return np.array(xs)\n\nmerged = util.deepZipWith(to_array, *experiences)\n# hickle can\'t handle empty lists :(\n# but we don\'t want the initial state anyways\ndel merged[\'initial\']\n\nif use_hickle:\n  hickle.dump(merged, \'merged.hickle\')\nelse:\n  with open(\'merged.pickle\', \'wb\') as f:\n    pickle.dump(merged, f)\n\n'"
scripts/test.py,0,b'#!/bin/env python\n\nfrom phillip import tf_lib\n\ntf_lib.testDiscounts()'
server/phillip.py,0,"b'from bottle import Bottle, run, template, request\nimport subprocess\nimport time\n\napp = Bottle()\n\ndef validate(code):\n  return len(code) == 8 and code.isalnum()\n\nstart_times = []\n\ndef cull_times():\n  global start_times\n\n  current = time.time()\n\n  # remove anything more than 20 min old\n  start_times = [t for t in start_times if current - t < 1200]\n\n  return len(start_times)\n\n\nMAX_GAMES = 10\n\ndef play(code, delay=6, real_delay=0):\n  if not validate(code):\n    return request_match_page() + template(\'Invalid code <b>{{code}}</b>\', code=code)\n\n  if cull_times() >= MAX_GAMES:\n    return request_match_page() + template(\'Sorry, too many games running\')\n\n\n  command = \'sbatch -t 20:00 -c 2 --mem 1G -x node[001-030]\' # --qos tenenbaum\'\n  command += \' --output slurm_logs/server/%s.out\' % code\n  command += \' --error slurm_logs/server/%s.err\' % code\n  command += \' netplay.sh %s %s %s\' % (code, delay, real_delay)\n  print(command)\n  try:\n    subprocess.run(command.split())\n  except:\n    return request_match_page() + template(\'Unknown error occurred. Make sure your netplay code <b>{{code}}</b> is valid.\', code=code)\n\n  start_times.append(time.time())\n  return request_match_page() + template(\'Phillip netplay started with code <b>{{code}}</b>!\', code=code)\n\n@app.get(\'/\')\ndef request_match_page():\n  return \'\'\'\n    <form action=""/request_match"" method=""post"">\n      Code: <input name=""code"" type=""text"" /> Your netplay host code. <br/>\n      Reaction Time (x50ms): <input name=""delay"" type=""number"" value=""6""/> Higher => phillip plays worse. <br/>\n      Netplay Lag (x50ms): <input name=""real_delay"" type=""number"" value=""0"" /> If unsure, leave at 0. <br/>\n      <input value=""Request match"" type=""submit"" /> <br/>\n      Please report issues in the <a href=""https://discord.gg/KQ8vhd6"">Discord Support Channel</a> <br/>\n    </form>\n  \'\'\'\n\n@app.post(\'/request_match\')\ndef request_match():\n  args = [\'code\', \'delay\', \'real_delay\']\n  return play(**{k: request.forms.get(k) for k in args})\n    \nrun(app, host=\'0.0.0.0\', port=8484)\n'"
twitchbot/stream.py,0,"b""from globals import twitch_key\n\nstream_args = [\n  'ffmpeg',\n  '-video_size', '1920x1080',\n  '-framerate', '30',\n  '-f', 'x11grab', '-i', ':1',\n  '-f', 'pulse', '-ac', '2', '-i', 'default',\n  '-c:v', 'libx264', '-preset', 'veryfast',\n  '-maxrate', '3000k', '-bufsize', '6000k',\n  '-pix_fmt', 'yuv420p',  '-g', '50',\n  '-c:a', 'mp3', '-b:a', '160k', '-ar', '44100', '-threads', '0', '-strict', '-2',\n  '-f', 'flv',\n  'rtmp://live.twitch.tv/app/' + twitch_key\n  #'test.flv'\n]\n\nimport subprocess\nimport os\n\n#proc = subprocess.Popen(stream_args)\nos.system(' '.join(stream_args))\n\n"""
twitchbot/twitchbot.py,0,"b'# copy/symlink this file to ~/.sopel/modules/\n\n# you will have to set up ~/.sopel/default.cfg to log in automatically as your bot\n# here is my defult.cfg:\n\nsample_cfg = """"""\n[core]\nnick = x_pilot_bot\nhost = irc.chat.twitch.tv\nuse_ssl = false\nport = 6667\nauth_method = server\nauth_password = <bot oauth token>\nowner = x_pilot_bot\nchannels = #x_pilot\nprefix = !\n""""""\n# get the oauth token from https://twitchapps.com/tmi/\n# this should be the token for your bot account, if you made one\n\n# now run sopel from the command line\n\nfrom sopel import module\nimport os, signal, subprocess\n#from phillip import run\n\n# key should be your twitch stream key (https://www.twitch.tv/broadcast/dashboard/streamkey)\n# this is for your regular account, not the bot\n# I made a file globals.py on my PYTHONPATH for things like this\nfrom globals import twitch_key, dolphin_iso_path\n\n# should point to FM 5.9\n# linux install guide: https://github.com/Ptomerty/FasterMelee-installer\ndolphin_path = \'/home/vlad/launch-fm\'\n\nagent_path = \'/home/vlad/Repos/phillip/agents/\'\nagent = \'delay18/FalcoBF\'\n\ncurrent_thread = None\nstream_thread = None\n\nstream = False\n\nstream_args = [\n  \'ffmpeg\',\n  \'-video_size\', \'1366x768\',\n  \'-framerate\', \'30\',\n  \'-f\', \'x11grab\', \'-i\', \':0.0\',\n  \'-c:v\', \'libx264\', \'-preset\', \'veryfast\',\n  \'-maxrate\', \'3000k\', \'-bufsize\', \'6000k\',\n  \'-pix_fmt\', \'yuv420p\',  \'-g\', \'50\',\n  \'-f\', \'flv\',\n  \'rtmp://live.twitch.tv/app/\' + twitch_key\n]\n\n@module.commands(\'echo\', \'repeat\')\ndef echo(bot, trigger):\n    bot.reply(trigger.group(2))\n\n@module.commands(\'helloworld\')\ndef helloworld(bot, trigger):\n    bot.say(\'Hello, world!\')\n\n@module.commands(\'dolphin\')\ndef dolphin(bot, trigger):\n    bot.say(""Faster Melee 4.8.7"")\n\ninstructions = """"""\n1. Install Faster Melee 5.9\n2. Use the smashladder configuration (NTSC 1.02, no memory card, cheats on, netplay community settings Gecko code)\n3. host a netplay lobby (traversal server)\n4. Go to twitch.tv/x_pilot\n\ncommands:\n\n!play <code> - the bot will join your netplay lobby\n!stop - stops the bot so others can play\n!agents - list available agents to play against\n!agent <agent> - set the agent\n\nBot is in London.\n""""""\n\n@module.commands(\'instructions\', \'rules\')\ndef instructions(bot, trigger):\n  bot.say(instructions)\n\n@module.commands(\'agent\')\ndef set_agent(bot, trigger):\n  global agent_path, agent\n  \n  new_agent = trigger.group(2)\n  path = agent_path + new_agent\n  \n  if not os.path.exists(path):\n    bot.say(\'Invalid agent!\')\n    return\n  \n  agent = new_agent\n  bot.say(\'Set agent to %s\' % agent)\n\n@module.commands(\'agents\')\ndef agents(bot, trigger):\n  dirs = []\n  for dirName, subdirList, fileList in os.walk(agent_path):\n    agent_name = dirName[len(agent_path):]\n    if \'params\' in fileList:\n      dirs.append(agent_name)\n  #bot.say(\'Found %d agents.\' % len(dirs))\n  bot.say(\' \'.join(dirs))\n\n@module.thread(False)\n@module.commands(\'play\')\ndef play(bot, trigger):\n  global current_thread, stream_thread\n  \n  if current_thread:\n    bot.say(""already playing, sorry"")\n    return\n  \n  code = trigger.group(2)\n  \n  args = dict(\n    load=agent_path + agent,\n    gui=True,\n    zmq=0,\n    exe=dolphin_path,\n    fm = True,\n    iso_path=dolphin_iso_path,\n    netplay=code,\n    start=0,\n    epsilon=0,\n    # fullscreen=True,\n    # delay=0,\n    # act_every=1,\n    # real_delay=1,\n    reload=0,\n  )\n    \n  #current_thread = Process(target=run.run, kwargs=args)\n  #current_thread.start()\n  #return\n  \n  # cmd = ""python -m ipdb phillip/run.py"".split()\n  cmd = [""phillip""]\n  for k, v in args.items():\n    if type(v) is bool and v:\n      cmd.append(\'--\' + k)\n    else:\n      cmd.append(\'--\' + k)\n      cmd.append(str(v))\n  \n  print(cmd)\n  \n  current_thread = subprocess.Popen(cmd, start_new_session=True)\n  #current_thread = subprocess.Popen([\'echo\', \'fasd\'])\n  if stream:\n    stream_thread = subprocess.Popen(stream_args, start_new_session=True)\n\n@module.thread(False)\n@module.commands(\'stop\')\ndef stop(bot, trigger):\n  #bot.say(\'stop?\')\n  global current_thread, stream_thread\n  \n  #import ipdb; ipdb.set_trace()\n  if current_thread:\n    #current_thread.terminate()\n    os.killpg(os.getpgid(current_thread.pid), signal.SIGTERM)\n    current_thread = None\n    \n    if stream:\n      os.killpg(os.getpgid(stream_thread.pid), signal.SIGTERM)\n      stream_thread = None\n  else:\n    bot.say(""nothing running"")\n\n@module.commands(\'kill\')\ndef kill(bot, trigger):\n  import os\n  os.system(\'killall \' + dolphin_path)\n\n'"
