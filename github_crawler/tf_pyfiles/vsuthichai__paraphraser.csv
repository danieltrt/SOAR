file_path,api_count,code
setup.py,0,"b""from setuptools import setup, find_packages\nfrom codecs import open\nfrom os import path\nfrom setuptools.command.install import install\n\nhere = path.abspath(path.dirname(__file__))\n\nwith open(path.join(here, 'README.md'), encoding='utf-8') as f:\n    long_desc = f.read()\n\nclass DownloadCorpora(install):\n    def run(self):\n        install.run(self)\n        import spacy\n        import nltk\n        nltk.download('wordnet')\n        spacy.cli.download('download', 'en')\n\nclass DownloadParaphraseModel(install):\n    def run(self):\n        install.run(self)\n        from paraphaser.download_models import download_file_from_google_drive\n        download_file_from_google_drive('19QDCd4UMgt3FtlYYwu0qZU3G1F9_XCvk', \n                                        'paraphrase-model.tar.gz')\n\nsetup(\n    name='paraphraser',\n    version='0.1.0',\n    description='Generate sentence paraphrases given an input sentence',\n    long_description=long_desc,\n    url='https://github.com/vsuthichai/paraphraser',\n    author='Victor Suthichai',\n    author_email='victor.suthichai@gmail.com',\n\n    # https://pypi.python.org/pypi?%3Aaction=list_classifiers\n    classifiers=[\n        'Development Status :: 3 - Alpha',\n        'Intended Audience :: Developers',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.4',\n        'Programming Language :: Python :: 3.5',\n        'Programming Language :: Python :: 3.6'\n    ],\n\n    keywords=[\n        'paraphraser'\n    ],\n\n    py_modules=['paraphraser.synonym_model', 'paraphraser.inference', 'paraphraser.download_models'],\n    #packages=find_packages(exclude=['contrib', 'docs', 'tests']),\n    #install_requires=['nltk', 'spacy', 'ipython'],\n    install_requires=[],\n    extras_require={\n\n    },\n    package_data={\n\n    },\n    data_files=[],\n    entry_points={\n    },\n    cmdclass={\n        'download_model': DownloadParaphraseModel\n        #'download_corpora': DownloadCorpora\n    }\n)\n\n"""
paraphraser/__init__.py,0,b'from .synonym_model import synonym_paraphrase\n'
paraphraser/dataset_generator.py,0,"b'import numpy as np\nfrom keras.preprocessing.sequence import pad_sequences\nfrom embeddings import load_sentence_embeddings\nfrom six.moves import xrange\nfrom six import iteritems\nfrom random import shuffle\n\nclass ParaphraseDataset(object):\n    """"""This class is responsible for batching the paraphrase dataset into mini batches\n    for train, dev, and test.  The dataset itself must be partition into files\n    beforehand and must follow this format:\n    \n    ""source sentence\\tsource sentence token ids\\treference sentence\\treference sentence token ids""\n    \n    The intraseparator is a space.  \n    """"""\n\n    def __init__(self, dataset_metadata, batch_size, embeddings, word_to_id, start_id, end_id, unk_id, mask_id):\n        """""" Constructor initialization.\n\n        Args:\n            dataset_metadata: metadata list that follows the format [\n                    {\n                        \'maxlen\': X\n                        \'train\': training filename with sentences of length X,\n                        \'dev\': dev filename with sentences of length X,\n                        \'test\': test filename with sentences of length X,\n                    },\n                ].  Each element is a list that describes the train, dev, and\n                test files for sentences of maximum length X.\n            batch_size: mini batch size\n            embeddings: pretrained embeddings\n            word_to_id: vocabulary index\n            start_id: start of sentence token id\n            end_id: end of sentence token id\n            unk_id: unknown token id\n            mask_id: pad token id applied after the end of sentence.\n        """"""\n                \n        # batch size\n        self.batch_size = batch_size\n\n        # Special tokens\n        self.start_id = start_id\n        self.end_id = end_id\n        self.unk_id = unk_id\n        self.mask_id = mask_id\n        \n        # Word embeddings, vocab\n        self.embeddings = embeddings\n        self.word_to_id = word_to_id\n        self.vocab_size, self.embedding_size = embeddings.shape\n\n        # dataset\n        self.lengths = sorted([ v for d in dataset_metadata for k, v in iteritems(d) if k == \'maxlen\' ])\n        self.dataset_metadata = {}\n        for dm in dataset_metadata:\n            for k, v in iteritems(dm):\n                if k == \'maxlen\':\n                    self.dataset_metadata[v] = dm\n        self.dataset = {}\n\n    def load_dataset_into_memory(self, dataset_type):\n        """"""Load dataset into memory and partition by train, dev, and test.""""""\n\n        if dataset_type not in set([\'train\', \'test\', \'dev\']):\n            raise ValueError(""Invalid dataset type."")\n\n        self.dataset[dataset_type] = {}\n        self.dataset[dataset_type][\'all_source_words\'] = []\n        self.dataset[dataset_type][\'all_source_ids\'] = []\n        self.dataset[dataset_type][\'all_source_len\'] = []\n        self.dataset[dataset_type][\'all_ref_words\'] = []\n        self.dataset[dataset_type][\'all_ref_ids\'] = []\n        self.dataset[dataset_type][\'all_ref_len\'] = []\n\n        batch_source_words = []\n        batch_source_ids = []\n        batch_source_len = []\n        batch_ref_words = []\n        batch_ref_ids = []\n        batch_ref_len = []\n\n        for length in self.lengths:\n            with open(self.dataset_metadata[length][dataset_type], \'r\') as f:\n                for i, line in enumerate(f):\n                    source_words, source_ids, ref_words, ref_ids = line.split(\'\\t\')\n                    batch_source_words.append(source_words.strip().split(\' \'))\n                    batch_source_ids.append(source_ids.strip().split(\' \'))\n                    batch_ref_words.append(ref_words.strip().split(\' \'))\n                    batch_ref_ids.append(ref_ids.strip().split(\' \'))\n\n                    if i % self.batch_size != 0 and i != 0:\n                        continue\n\n                    batch_source_len = [ len(source_ids) for source_ids in batch_source_ids ]\n                    batch_ref_len = [ len(ref_ids) for ref_ids in batch_ref_ids ] \n\n                    self.dataset[dataset_type][\'all_source_ids\'].append(self.pad_batch(batch_source_ids, length))\n                    self.dataset[dataset_type][\'all_source_words\'].append(batch_source_words)\n                    self.dataset[dataset_type][\'all_source_len\'].append(batch_source_len)\n                    self.dataset[dataset_type][\'all_ref_ids\'].append(self.pad_batch(batch_ref_ids, length))\n                    self.dataset[dataset_type][\'all_ref_words\'].append(batch_ref_words)\n                    self.dataset[dataset_type][\'all_ref_len\'].append(batch_ref_len)\n\n                    batch_source_words = []\n                    batch_source_ids = []\n                    batch_source_len = []\n                    batch_ref_words = []\n                    batch_ref_ids = []\n                    batch_ref_len = []\n\n                if len(batch_source_words) > 0:\n                    batch_source_len = [ len(source_ids) for source_ids in batch_source_ids ]\n                    batch_ref_len = [ len(ref_ids) for ref_ids in batch_ref_ids ] \n\n                    self.dataset[dataset_type][\'all_source_ids\'].append(self.pad_batch(batch_source_ids, length))\n                    self.dataset[dataset_type][\'all_source_words\'].append(batch_source_words)\n                    self.dataset[dataset_type][\'all_source_len\'].append(batch_source_len)\n                    self.dataset[dataset_type][\'all_ref_ids\'].append(self.pad_batch(batch_ref_ids, length))\n                    self.dataset[dataset_type][\'all_ref_words\'].append(batch_ref_words)\n                    self.dataset[dataset_type][\'all_ref_len\'].append(batch_ref_len)\n\n                    batch_source_words = []\n                    batch_source_ids = []\n                    batch_source_len = []\n                    batch_ref_words = []\n                    batch_ref_ids = []\n                    batch_ref_len = []\n\n    def generate_batch(self, dataset_type):\n        """"""Return a generator that yields a mini batch of size self.batch_size.\n        \n        Args:\n            dataset_type: \'train\', \'test\', or \'dev\'\n        """"""\n\n        if dataset_type not in set([\'train\', \'test\', \'dev\']):\n            raise ValueError(""Invalid dataset type."")\n\n        if dataset_type not in self.dataset:\n            self.load_dataset_into_memory(dataset_type)\n\n        dataset_size = len(self.dataset[dataset_type][\'all_source_ids\'])\n\n        rs = np.random.get_state()\n        np.random.shuffle(self.dataset[dataset_type][\'all_source_ids\'])\n        np.random.set_state(rs)\n        np.random.shuffle(self.dataset[dataset_type][\'all_source_words\'])\n        np.random.set_state(rs)\n        np.random.shuffle(self.dataset[dataset_type][\'all_source_len\'])\n        np.random.set_state(rs)\n        np.random.shuffle(self.dataset[dataset_type][\'all_ref_ids\'])\n        np.random.set_state(rs)\n        np.random.shuffle(self.dataset[dataset_type][\'all_ref_words\'])\n        np.random.set_state(rs)\n        np.random.shuffle(self.dataset[dataset_type][\'all_ref_len\'])\n        np.random.set_state(rs)\n\n        for i in xrange(dataset_size):\n            yield {\n                \'seq_source_ids\': self.dataset[dataset_type][\'all_source_ids\'][i],\n                \'seq_source_words\': self.dataset[dataset_type][\'all_source_words\'][i],\n                \'seq_source_len\': self.dataset[dataset_type][\'all_source_len\'][i],\n                \'seq_ref_ids\': self.dataset[dataset_type][\'all_ref_ids\'][i],\n                \'seq_ref_words\': self.dataset[dataset_type][\'all_ref_words\'][i],\n                \'seq_ref_len\': self.dataset[dataset_type][\'all_ref_len\'][i]\n            }\n\n    def pad_batch(self, batch_ids, max_len):\n        """""" Pad a mini batch with mask_id.  This is intended to fill in any\n        remaining time steps after the end of sentence tokens.\n\n        Args:\n            batch_ids: The mini batch of token ids of shape (batch_size, time_steps)\n            max_len: The maximum number of time steps.\n\n        Returns:\n            a batch of samples padded with mask_id\n        """"""\n        padded_batch = np.array(pad_sequences(batch_ids, maxlen=max_len, padding=\'post\', value=self.mask_id))\n        return padded_batch\n\n\nif __name__ == \'__main__\':\n    from pprint import pprint as pp\n    from utils import dataset_config\n    dataset = dataset_config()\n    word_to_id, idx_to_word, embeddings, start_id, end_id, unk_id, mask_id = load_sentence_embeddings()\n    pd = ParaphraseDataset(dataset, 10, embeddings, word_to_id, start_id, end_id, unk_id, mask_id)\n    generator = pd.generate_batch(\'train\')\n    for i, d in enumerate(generator):\n        if i == 5:\n            break\n        print(""=== seq source ids ==="")\n        print(d[\'seq_source_ids\'].shape, flush=True)\n        print(d[\'seq_source_ids\'], flush=True)\n        for i in d[\'seq_source_words\']:\n            print(i)\n        print(d[\'seq_source_len\'], flush=True)\n    \n        print(""=== seq ref ids ==="")\n        print(d[\'seq_ref_ids\'].shape, flush=True)\n        print(d[\'seq_ref_ids\'], flush=True)\n        for i in d[\'seq_ref_words\']:\n            print(i)\n        print(d[\'seq_ref_len\'])\n\n'"
paraphraser/download_models.py,0,"b'import sys\nimport os\nimport requests\nimport logging\n\nlogging.basicConfig(format = u\'[LINE:%(lineno)d]# %(levelname)-8s [%(asctime)s]  %(message)s\', level = logging.NOTSET)\n\ndef download_file_from_google_drive(id, destination):\n    URL = ""https://docs.google.com/uc?export=download""\n    logging.info(""Downloading ""+id + "" to ""+destination)\n    logging.info(""Please be patient, it may take a while..."")\n    session = requests.Session()\n\n    response = session.get(URL, params = { \'id\' : id }, stream = True)\n    token = get_confirm_token(response)\n    logging.info(""..."")\n    if token:\n        params = { \'id\' : id, \'confirm\' : token }\n        response = session.get(URL, params = params, stream = True)\n\n    save_response_content(response, destination)    \n    logging.info(""Done with "" + id)\n\ndef get_confirm_token(response):\n    for key, value in response.cookies.items():\n        if key.startswith(\'download_warning\'):\n            return value\n\n    return None\n\ndef save_response_content(response, destination):\n    CHUNK_SIZE = 32768\n\n    with open(destination, ""wb"") as f:\n        for chunk in response.iter_content(CHUNK_SIZE):\n            if chunk: # filter out keep-alive new chunks\n                f.write(chunk)\n\n\'\'\'\n    to download the .t7 NTS models used for text simplification\n    if for some reason, this doanload fails, please use the direct urls:\n    - for NTS:\n        https://drive.google.com/open?id=0B_pjS_ZjPfT9dEtrbV85UXhSelU\n    -for NTS-w2v:\n        https://drive.google.com/open?id=0B_pjS_ZjPfT9ZTRfSFp4Ql92U0E\n\'\'\'\n\nif __name__ == ""__main__"":\n    try:\n        out_dir = sys.argv[1]\n        logging.info(""Saving files to: "" + out_dir)\n    except:\n        out_dir = os.path.dirname(os.path.realpath(__file__))\n        logging.info(""Saving files to: "" + out_dir)\n        \n    #NTS_model = \'0B_pjS_ZjPfT9dEtrbV85UXhSelU\' \n    #NTS_model_output = \'NTS_epoch11_10.19.t7\'\n    #download_file_from_google_drive(NTS_model, os.path.join(out_dir, NTS_model_output))\n\n    #NTS_w2v_model = \'0B_pjS_ZjPfT9ZTRfSFp4Ql92U0E\' \n    #NTS_w2v_model_output = \'NTS-w2v_epoch11_10.20.t7\'\n    #download_file_from_google_drive(NTS_w2v_model, os.path.join(out_dir, NTS_w2v_model_output))\n\n    #model = \'1_JsQ_iMnHwvnyd5vZM-6BMqe9hnVqrPi\'\n    model = \'19QDCd4UMgt3FtlYYwu0qZU3G1F9_XCvk\'\n    download_file_from_google_drive(model, \'paraphrase-model.tar.gz\')\n\n'"
paraphraser/embeddings.py,0,"b'import numpy as np\nimport pickle\nfrom six import iteritems\nfrom pprint import pprint as pp\n#from keras.layers.embeddings import Embedding\n\ndef load_sentence_embeddings():\n    \'\'\'Load John Wieting sentence embeddings\'\'\'\n    with open(""../../para-nmt-50m/data/ngram-word-concat-40.pickle"", \'rb\') as f:\n        # [ numpy.ndarray(95283, 300), numpy.ndarray(74664, 300), (trigram_dict, word_dict)]\n        x = pickle.load(f, encoding=\'latin1\')\n        word_vocab_size, embedding_size = x[1].shape\n\n        trigram_embeddings, word_embeddings, _ = x\n        trigram_to_id, word_to_id = x[2]\n\n        word_to_id[\'<START>\'] = word_vocab_size\n        word_to_id[\'<END>\'] = word_vocab_size + 1\n\n        idx_to_word = { idx: word for word, idx in iteritems(word_to_id) }\n\n        word_embeddings = np.vstack((word_embeddings, np.random.randn(2, embedding_size)))\n\n        return (word_to_id, idx_to_word, word_embeddings, word_to_id[\'<START>\'], \n               word_to_id[\'<END>\'], word_to_id[\'UUUNKKK\'], word_to_id[\'\xe2\x98\x85\'])\n\ndef load_glove_embeddings():\n    with open(""/media/sdb/datasets/glove.6B/glove.6B.300d.pickle"", ""rb"") as f:\n        word_to_id, id_to_word, word_embeddings = pickle.load(f, encoding=\'latin1\')\n        word_vocab_size, embedding_size = word_embeddings.shape\n        word_to_id[\'<START>\'] = word_vocab_size\n        word_to_id[\'<END>\'] = word_vocab_size + 1\n        word_to_id[\'UUUNKKK\'] = word_vocab_size + 2\n        word_to_id[\'\xe2\x98\x85\'] = word_vocab_size + 3\n        id_to_word[word_vocab_size] = \'<START>\'\n        id_to_word[word_vocab_size+1] = \'<END>\'\n        id_to_word[word_vocab_size+2] = \'UUUNKKK\'\n        id_to_word[word_vocab_size+3] = \'\xe2\x98\x85\'\n        word_embeddings = np.vstack((word_embeddings, np.random.randn(4, embedding_size)))\n        return (word_to_id, id_to_word, word_embeddings, word_to_id[\'<START>\'], \n               word_to_id[\'<END>\'], word_to_id[\'UUUNKKK\'], word_to_id[\'\xe2\x98\x85\'])\n        \n\nif __name__ == \'__main__\':\n    word_to_id, idx_to_word, embedding, start_id, end_id, unk_id, mask_id = load_sentence_embeddings()\n    pp(idx_to_word[mask_id])\n    #pp(idx_to_word)\n    #pp(word_to_id)\n    #print(embedding.shape)\n\n'"
paraphraser/inference.py,3,"b'import tensorflow as tf\nfrom embeddings import load_sentence_embeddings\nfrom preprocess_data import preprocess_batch\nfrom six.moves import input\nfrom lstm_model import lstm_model\nimport numpy as np\nfrom pprint import pprint as pp\n\n\nclass Paraphraser(object):\n    \'\'\'Heart of the paraphraser model.  This class loads the checkpoint\n    into the Tensorflow runtime environment and is responsible for inference.\n    Greedy and sampling based approaches are supported\n    \'\'\'\n\n    def __init__(self, checkpoint):\n        """"""Constructor.  Load vocabulary index, start token, end token, unk id,\n        mask_id.  Restore checkpoint.\n\n        Args:\n            checkpoint: A path to the checkpoint\n        """"""\n        self.word_to_id, self.idx_to_word, self.embedding, self.start_id, self.end_id, self.unk_id, self.mask_id = load_sentence_embeddings()\n        self.checkpoint = checkpoint\n        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n        self.sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n        self.model = lstm_model(self.sess, \'infer\', 300, self.embedding, self.start_id, self.end_id, self.mask_id)\n        saver = tf.train.Saver()\n        saver.restore(self.sess, checkpoint)\n\n    def sample_paraphrase(self, sentence, sampling_temp=1.0, how_many=1):\n        """"""Paraphrase by sampling a distribution\n\n        Args:\n            sentence (str): A sentence input that will be paraphrased by \n                sampling from distribution.\n            sampling_temp (int) : A number between 0 an 1\n\n        Returns:\n            str: a candidate paraphrase of the `sentence`\n        """"""\n\n        return self.infer(1, sentence, self.idx_to_word, sampling_temp, how_many)\n\n    def greedy_paraphrase(self, sentence):\n        """"""Paraphrase using greedy sampler\n    \n        Args:\n            sentence : The source sentence to be paraphrased.\n\n        Returns:\n            str : a candidate paraphrase of the `sentence`\n        """"""\n\n        return self.infer(0, sentence, self.idx_to_word, 0., 1)\n\n\n    def infer(self, decoder, source_sent, id_to_vocab, temp, how_many):\n        """""" Perform inferencing.  In other words, generate a paraphrase\n        for the source sentence.\n\n        Args:\n            decoder : 0 for greedy, 1 for sampling\n            source_sent : source sentence to generate a paraphrase for\n            id_to_vocab : dict of vocabulary index to word\n            end_id : the end token\n            temp : the sampling temperature to use when `decoder` is 1\n\n        Returns:\n            str : for the generated paraphrase\n        """"""\n\n        seq_source_words, seq_source_ids = preprocess_batch([ source_sent ] * how_many)\n        #print(seq_source_words)\n        #print(seq_source_ids)\n        seq_source_len = [ len(seq_source) for seq_source in seq_source_ids ]\n        #print(seq_source_len)\n\n        feed_dict = {\n            self.model[\'seq_source_ids\']: seq_source_ids,\n            self.model[\'seq_source_lengths\']: seq_source_len,\n            self.model[\'decoder_technique\']: decoder,\n            self.model[\'sampling_temperature\']: temp\n        }\n\n        feeds = [\n            self.model[\'predictions\']\n            #model[\'final_sequence_lengths\']\n        ]\n\n        predictions = self.sess.run(feeds, feed_dict)[0]\n        #print(predictions)\n        return self.translate(predictions, decoder, id_to_vocab, seq_source_words[0])\n\n    def translate(self, predictions, decoder, id_to_vocab, seq_source_words):\n        """""" Translate the vocabulary ids in `predictions` to actual words\n        that compose the paraphrase.\n\n        Args:\n            predictions : arrays of vocabulary ids\n            decoder : 0 for greedy, 1 for sample, 2 for beam\n            id_to_vocab : dict of vocabulary index to word\n\n        Returns:\n            str : the paraphrase\n        """"""\n        translated_predictions = []\n        #np_end = np.where(translated_predictions == end_id)\n        for sent_pred in predictions:\n            translated = []\n            for pred in sent_pred:\n                word = \'UUNNKK\'\n                if pred == self.end_id:\n                    break\n                if pred == self.unk_id:\n                    # Search for rare word\n                    for seq_source_word in seq_source_words:\n                        if seq_source_word not in self.word_to_id:\n                            word = seq_source_word\n                else:\n                    word = id_to_vocab[pred]\n                translated.append(word)\n            translated_predictions.append(\' \'.join(translated))\n        return translated_predictions\n\ndef main():\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--checkpoint\', type=str, help=\'Checkpoint path\')\n    args = parser.parse_args()\n    paraphraser = Paraphraser(args.checkpoint)\n\n    while 1:\n        source_sentence = input(""Source: "")\n        #p = paraphraser.greedy_paraphrase(source_sentence)\n        #print(p)\n        paraphrases = paraphraser.sample_paraphrase(source_sentence, sampling_temp=0.75, how_many=10)\n        for i, paraphrase in enumerate(paraphrases):\n            print(""Paraph #{}: {}"".format(i, paraphrase))\n\nif __name__ == \'__main__\':\n    main()\n\n'"
paraphraser/inference_frozen_graph.py,7,"b'import tensorflow as tf\nfrom load_sent_embeddings import load_sentence_embeddings\nfrom preprocess_data import preprocess_batch\nfrom six.moves import input\n\nword_to_id, idx_to_word, embedding, start_id, end_id, unk_id  = load_sentence_embeddings()\nmask_id = 5800\n\nwith open(\'/media/sdb/models/paraphraser/frozen_model.pb\', \'rb\') as f:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(f.read())\n\nwith tf.Graph().as_default() as graph:\n    predictions = tf.import_graph_def(\n        graph_def=graph_def,\n        return_elements=[\'predictions:0\'],\n        name=\'\')\n\n    print([op.name for op in graph.get_operations()])\n\n    seq_source_ids = graph.get_tensor_by_name(\'placeholders/source_ids:0\')\n    seq_source_lengths = graph.get_tensor_by_name(\'placeholders/sequence_source_lengths:0\')\n    decoder_technique = graph.get_tensor_by_name(\'placeholders/decoder_technique:0\')\n    sampling_temperature = graph.get_tensor_by_name(\'placeholders/sampling_temperature:0\')\n    keep_prob = graph.get_tensor_by_name(\'placeholders/keep_prob:0\')\n\nmodel = {\n    \'seq_source_ids\': seq_source_ids,\n    \'seq_source_lengths\': seq_source_lengths,\n    \'predictions\': predictions,\n    \'decoder_technique\': decoder_technique,\n    \'sampling_temperature\': sampling_temperature\n}\n\nsess = tf.Session()\n\ndef restore_model(checkpoint):\n    model = lstm_model(sess, \'infer\', 300, embedding, start_id, end_id, mask_id)\n    saver = tf.train.Saver()\n    saver.restore(sess, checkpoint)\n\ndef translate(predictions, decoder, id_to_vocab, end_id):\n    """""" Translate the vocabulary ids in `predictions` to actual words\n    that compose the paraphrase.\n\n    Args:\n        predictions : arrays of vocabulary ids\n        decoder : 0 for greedy, 1 for sample, 2 for beam\n        id_to_vocab : dict of vocabulary index to word\n        end_id : end token index \n\n    Returns:\n        str : the paraphrase\n    """"""\n    if decoder == 2:\n        _, sentence_length, num_samples = predictions.shape\n        for i in xrange(num_samples):\n            sent_pred = []\n            for j in xrange(sentence_length):\n                sent_pred.append(predictions[0][j][i])\n            try:\n                end_index = sent_pred.index(end_id)\n                sent_pred = sent_pred[:end_index]\n            except Exception as e:\n                pass\n            return \' \'.join([ id_to_vocab[pred] for pred in sent_pred ])\n    else:\n        for sent_pred in predictions:\n            if sent_pred[-1] == end_id:\n                sent_pred = sent_pred[0:-1]\n            return \' \'.join([ id_to_vocab[pred] for pred in sent_pred ])\n\n\ndef infer(sess, model, decoder, source_sent, id_to_vocab, end_id, temp):\n    """""" Perform inferencing.  In other words, generate a paraphrase\n    for the source sentence.\n\n    Args:\n        sess : Tensorflow session.\n        model : dict of tensor to value\n        decoder : 0 for greedy, 1 for sampling\n        source_sent : source sentence to generate a paraphrase for\n        id_to_vocab : dict of vocabulary index to word\n        end_id : the end token\n        temp : the sampling temperature to use when `decoder` is 1\n\n    Returns:\n        str : for the generated paraphrase\n    """"""\n\n    seq_source_words, seq_source_ids = preprocess_batch([ source_sent ])\n    seq_source_len = [ len(seq_source) for seq_source in seq_source_ids ]\n\n    feed_dict = {\n        model[\'seq_source_ids\']: seq_source_ids,\n        model[\'seq_source_lengths\']: seq_source_len,\n        model[\'decoder_technique\']: decoder,\n        model[\'sampling_temperature\']: temp\n    }\n\n    feeds = [\n        model[\'predictions\']\n        #model[\'final_sequence_lengths\']\n    ]\n\n    predictions = sess.run(feeds, feed_dict)[0][0]\n    return translate(predictions, decoder, id_to_vocab, end_id)\n\ndef greedy_paraphrase(sentence):\n    """"""Paraphrase using greedy sampler\n    \n    Args:\n        sentence : The source sentence to be paraphrased.\n\n    Returns:\n        str : a candidate paraphrase of the `sentence`\n    """"""\n\n    with tf.Session(graph=graph) as sess:\n        return infer(sess, model, 0, sentence, idx_to_word, end_id, 0.)\n\ndef sampler_paraphrase(sentence, sampling_temp=1.0):\n    """"""Paraphrase by sampling a distribution\n\n    Args:\n        sentence (str): A sentence input that will be paraphrased by \n            sampling from distribution.\n        sampling_temp (int) : A number between 0 an 1\n\n    Returns:\n        str: a candidate paraphrase of the `sentence`\n    """"""\n\n    with tf.Session(graph=graph) as sess:\n        return infer(sess, model, 1, sentence, idx_to_word, end_id, sampling_temp)\n\ndef main():\n    while 1:\n        source_sentence = input(""Source: "")\n        #print(""Paraph: {}"".format(sampler_paraphrase(\'hello world.\')))\n        print(""Paraph: {}"".format(greedy_paraphrase(\'hello world.\')))\n\nif __name__ == \'__main__\':\n    main()\n\n\n'"
paraphraser/inspect_checkpoint.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A simple script for inspect checkpoint files.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\n\nimport numpy as np\n\nfrom tensorflow.python import pywrap_tensorflow\nfrom tensorflow.python.platform import app\nfrom tensorflow.python.platform import flags\n\nFLAGS = None\n\n\ndef print_tensors_in_checkpoint_file(file_name, tensor_name, all_tensors,\n                                     all_tensor_names):\n  """"""Prints tensors in a checkpoint file.\n\n  If no `tensor_name` is provided, prints the tensor names and shapes\n  in the checkpoint file.\n\n  If `tensor_name` is provided, prints the content of the tensor.\n\n  Args:\n    file_name: Name of the checkpoint file.\n    tensor_name: Name of the tensor in the checkpoint file to print.\n    all_tensors: Boolean indicating whether to print all tensors.\n    all_tensor_names: Boolean indicating whether to print all tensor names.\n  """"""\n  try:\n    reader = pywrap_tensorflow.NewCheckpointReader(file_name)\n    if all_tensors or all_tensor_names:\n      var_to_shape_map = reader.get_variable_to_shape_map()\n      for key in sorted(var_to_shape_map):\n        print(""tensor_name: "", key)\n        if all_tensors:\n          print(reader.get_tensor(key))\n    elif not tensor_name:\n      print(reader.debug_string().decode(""utf-8""))\n    else:\n      print(""tensor_name: "", tensor_name)\n      print(reader.get_tensor(tensor_name))\n  except Exception as e:  # pylint: disable=broad-except\n    print(str(e))\n    if ""corrupted compressed block contents"" in str(e):\n      print(""It\'s likely that your checkpoint file has been compressed ""\n            ""with SNAPPY."")\n    if (""Data loss"" in str(e) and\n        (any([e in file_name for e in ["".index"", "".meta"", "".data""]]))):\n      proposed_file = ""."".join(file_name.split(""."")[0:-1])\n      v2_file_error_template = """"""\nIt\'s likely that this is a V2 checkpoint and you need to provide the filename\n*prefix*.  Try removing the \'.\' and extension.  Try:\ninspect checkpoint --file_name = {}""""""\n      print(v2_file_error_template.format(proposed_file))\n\n\ndef parse_numpy_printoption(kv_str):\n  """"""Sets a single numpy printoption from a string of the form \'x=y\'.\n\n  See documentation on numpy.set_printoptions() for details about what values\n  x and y can take. x can be any option listed there other than \'formatter\'.\n\n  Args:\n    kv_str: A string of the form \'x=y\', such as \'threshold=100000\'\n\n  Raises:\n    argparse.ArgumentTypeError: If the string couldn\'t be used to set any\n        nump printoption.\n  """"""\n  k_v_str = kv_str.split(""="", 1)\n  if len(k_v_str) != 2 or not k_v_str[0]:\n    raise argparse.ArgumentTypeError(""\'%s\' is not in the form k=v."" % kv_str)\n  k, v_str = k_v_str\n  printoptions = np.get_printoptions()\n  if k not in printoptions:\n    raise argparse.ArgumentTypeError(""\'%s\' is not a valid printoption."" % k)\n  v_type = type(printoptions[k])\n  if v_type is type(None):\n    raise argparse.ArgumentTypeError(\n        ""Setting \'%s\' from the command line is not supported."" % k)\n  try:\n    v = (v_type(v_str) if v_type is not bool\n         else flags.BooleanParser().parse(v_str))\n  except ValueError as e:\n    raise argparse.ArgumentTypeError(e.message)\n  np.set_printoptions(**{k: v})\n\n\ndef main(unused_argv):\n  if not FLAGS.file_name:\n    print(""Usage: inspect_checkpoint --file_name=checkpoint_file_name ""\n          ""[--tensor_name=tensor_to_print] ""\n          ""[--all_tensors] ""\n          ""[--all_tensor_names] ""\n          ""[--printoptions]"")\n    sys.exit(1)\n  else:\n    print_tensors_in_checkpoint_file(FLAGS.file_name, FLAGS.tensor_name,\n                                     FLAGS.all_tensors, FLAGS.all_tensor_names)\n\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.register(""type"", ""bool"", lambda v: v.lower() == ""true"")\n  parser.add_argument(\n      ""--file_name"", type=str, default="""", help=""Checkpoint filename. ""\n                    ""Note, if using Checkpoint V2 format, file_name is the ""\n                    ""shared prefix between all files in the checkpoint."")\n  parser.add_argument(\n      ""--tensor_name"",\n      type=str,\n      default="""",\n      help=""Name of the tensor to inspect"")\n  parser.add_argument(\n      ""--all_tensors"",\n      nargs=""?"",\n      const=True,\n      type=""bool"",\n      default=False,\n      help=""If True, print the values of all the tensors."")\n  parser.add_argument(\n      ""--all_tensor_names"",\n      nargs=""?"",\n      const=True,\n      type=""bool"",\n      default=False,\n      help=""If True, print the names of all the tensors."")\n  parser.add_argument(\n      ""--printoptions"",\n      nargs=""*"",\n      type=parse_numpy_printoption,\n      help=""Argument for numpy.set_printoptions(), in the form \'k=v\'."")\n  FLAGS, unparsed = parser.parse_known_args()\n  app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
paraphraser/lstm_model.py,79,"b'import tensorflow as tf\nimport datetime as dt\nimport sys\nimport numpy as np\nfrom tensorflow.python.layers import core as layers_core\nfrom sample_embedding_helper import MySampleEmbeddingHelper\n\n#def lstm_model(args, np_embeddings, start_id, end_id, mask_id, mode):\ndef lstm_model(sess, mode, cell_hidden_size, np_embeddings, start_id, end_id, mask_id):\n    vocab_size, hidden_size = np_embeddings.shape\n\n    # Embeddings\n    with tf.variable_scope(\'embeddings\'):\n        encoder_embeddings = tf.get_variable(name=""encoder_embeddings"", shape=np_embeddings.shape, initializer=tf.constant_initializer(np_embeddings), trainable=True)\n        decoder_embeddings = tf.get_variable(name=""decoder_embeddings"", shape=np_embeddings.shape, initializer=tf.constant_initializer(np_embeddings), trainable=True)\n        #embeddings = tf.get_variable(name=""embeddings"", shape=np_embeddings.shape, initializer=tf.constant_initializer(np_embeddings), trainable=True)\n\n    # Define placeholders\n    with tf.variable_scope(\'placeholders\'):\n        lr = tf.placeholder(tf.float32, shape=(), name=""learning_rate"")\n        seq_source_ids = tf.placeholder(tf.int32, shape=(None, None), name=""source_ids"")\n        seq_source_lengths = tf.placeholder(tf.int32, [None], name=""sequence_source_lengths"")\n        keep_prob = tf.placeholder_with_default(1.0, shape=(), name=""keep_prob"")\n        # 0: greedy, 1: sampling, 2: beam\n        sampling_temperature = tf.placeholder_with_default(0.5, shape=(), name=""sampling_temperature"")\n        decoder_technique = tf.placeholder_with_default(1, shape=(), name=""decoder_technique"")\n        #beam_width = tf.placeholder_with_default(5, shape=(), name=""beam_width"")\n        dummy = tf.add(sampling_temperature, 1, name=""dummy"")\n\n        if mode in set([\'train\', \'dev\', \'test\']):\n            seq_reference_ids = tf.placeholder(tf.int32, shape=(None, None), name=""reference_ids"")\n            seq_reference_lengths = tf.placeholder(tf.int32, [None], name=""sequence_reference_lengths"")\n            paddings = tf.constant([[0, 0], [0, 1]])\n            seq_output_ids = tf.pad(seq_reference_ids[:, 1:], paddings, mode=""CONSTANT"", name=""seq_output_ids"", constant_values=mask_id)\n        else:\n            seq_reference_ids = None\n            seq_reference_lengths = None\n            seq_output_ids = None\n\n    #batch_size = tf.cast(tf.shape(seq_source_ids)[0], tf.float32)\n    batch_size = tf.shape(seq_source_ids)[0]\n\n    # Encoder\n    #with tf.variable_scope(\'encoder\'):\n    encoder_embedding = tf.nn.embedding_lookup(encoder_embeddings, seq_source_ids, name=""encoder_embedding"")\n    encoder_fw_cell = tf.contrib.rnn.DropoutWrapper(tf.nn.rnn_cell.BasicLSTMCell(cell_hidden_size), input_keep_prob=keep_prob, output_keep_prob=keep_prob)\n    encoder_bw_cell = tf.contrib.rnn.DropoutWrapper(tf.nn.rnn_cell.BasicLSTMCell(cell_hidden_size), input_keep_prob=keep_prob, output_keep_prob=keep_prob)\n    encoder_outputs, encoder_states = tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_fw_cell, \n                                                                      cell_bw=encoder_bw_cell,\n                                                                      inputs=encoder_embedding, \n                                                                      sequence_length=seq_source_lengths, \n                                                                      dtype=tf.float32)\n    concat_encoder_outputs = tf.concat(encoder_outputs, 2)\n    encoder_fw_state, encoder_bw_state = encoder_states\n    encoder_state_c = tf.concat((encoder_fw_state.c, encoder_bw_state.c), axis=1, name=""encoder_state_c"")\n    encoder_state_h = tf.concat((encoder_fw_state.h, encoder_bw_state.h), axis=1, name=""encoder_state_h"")\n    joined_encoder_state = tf.contrib.rnn.LSTMStateTuple(encoder_state_c, encoder_state_h)\n\n    fc_layer = layers_core.Dense(vocab_size, use_bias=False)\n    attention = tf.contrib.seq2seq.BahdanauAttention(num_units=cell_hidden_size, memory=concat_encoder_outputs)\n    decoder_cell = tf.contrib.rnn.DropoutWrapper(tf.nn.rnn_cell.BasicLSTMCell(cell_hidden_size * 2), input_keep_prob=keep_prob, output_keep_prob=keep_prob)\n    attn_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention, attention_layer_size=cell_hidden_size)\n    zero_state = attn_cell.zero_state(batch_size, tf.float32)\n    decoder_initial_state = zero_state.clone(cell_state=joined_encoder_state)\n\n    \'\'\' Beam search \n    tiled_joined_encoder_state = tf.contrib.seq2seq.tile_batch(joined_encoder_state, multiplier=beam_width)\n    tiled_concat_encoder_outputs = tf.contrib.seq2seq.tile_batch(concat_encoder_outputs, multiplier=beam_width)\n    beam_attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n        num_units=hidden_size,\n        memory=tiled_concat_encoder_outputs)\n    #decoder_cell = tf.contrib.rnn.DropoutWrapper(tf.nn.rnn_cell.BasicLSTMCell(hidden_size * 2), input_keep_prob=1.0, output_keep_prob=1.0)\n    beam_attn_wrapper = tf.contrib.seq2seq.AttentionWrapper(\n        cell=tf.nn.rnn_cell.BasicLSTMCell(hidden_size * 2),\n        attention_mechanism=beam_attention_mechanism, \n        attention_layer_size=hidden_size)\n    \'\'\'\n\n    # Train, dev, test\n    if mode in set([\'train\', \'dev\', \'test\']):\n        # Decoder\n        decoder_embedding = tf.nn.embedding_lookup(decoder_embeddings, seq_reference_ids, name=""decoder_embedding"")\n        helper = tf.contrib.seq2seq.TrainingHelper(decoder_embedding, seq_reference_lengths)\n        decoder = tf.contrib.seq2seq.BasicDecoder(attn_cell, helper, decoder_initial_state, fc_layer)\n        final_outputs, final_state, final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(decoder, swap_memory=True)\n        logits = final_outputs.rnn_output\n        predictions = final_outputs.sample_id\n\n        with tf.variable_scope(\'train_loss\'):\n            max_output_len = tf.shape(logits)[1]\n            seq_output_ids = seq_output_ids[:, :max_output_len]\n            pad = tf.fill((tf.shape(seq_output_ids)[0], max_output_len), -1) #mask_id\n            boolean_mask = tf.not_equal(seq_output_ids, pad)\n            mask = tf.cast(boolean_mask, tf.float32)\n            labels = tf.reshape(seq_output_ids, shape=(-1, 1))\n            crossent = tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(labels, vocab_size), logits=logits)\n            loss = (tf.reduce_sum(crossent * mask) / tf.cast(batch_size, tf.float32))\n\n        with tf.variable_scope(\'summaries\'):\n            tf.summary.scalar(""batch_loss"", loss)\n            summaries = tf.summary.merge_all()\n\n        train_step = tf.train.AdamOptimizer(lr).minimize(loss)\n\n    # Test\n    elif mode == \'infer\':\n        loss = None\n        train_step = None\n        labels = None\n        summaries = None\n        start_tokens = tf.fill([batch_size], start_id)\n\n        # Beach search decoder\n        \'\'\'\n        beam_search_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n            cell=beam_attn_wrapper,\n            embedding=decoder_embeddings,\n            start_tokens=start_tokens,\n            end_token=end_id,\n            initial_state=beam_attn_wrapper.zero_state(batch_size * beam_width, tf.float32).clone(cell_state=tiled_joined_encoder_state),\n            beam_width=beam_width.eval(),\n            output_layer=fc_layer,\n            length_penalty_weight=0.0)\n        \'\'\'\n\n        # Distribution sampling\n        #sample_helper = MySampleEmbeddingHelper(decoder_embeddings, start_tokens, end_id, softmax_temperature=sampling_temperature)\n        sample_helper = tf.contrib.seq2seq.SampleEmbeddingHelper(decoder_embeddings, start_tokens, end_id, softmax_temperature=sampling_temperature)\n        sample_decoder = tf.contrib.seq2seq.BasicDecoder(attn_cell, sample_helper, decoder_initial_state, output_layer=fc_layer)\n\n        # Greedy argmax decoder\n        greedy_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(decoder_embeddings, start_tokens, end_id)\n        # applied per timestep\n        greedy_decoder = tf.contrib.seq2seq.BasicDecoder(attn_cell, greedy_helper, decoder_initial_state, output_layer=fc_layer)\n\n        # Decode!\n        greedy_outputs, greedy_final_state, greedy_fsl = tf.contrib.seq2seq.dynamic_decode(\n            greedy_decoder,\n            #maximum_iterations=maximum_iterations,\n            swap_memory=True)\n        greedy_logits = greedy_outputs.rnn_output\n        greedy_predictions = tf.identity(greedy_outputs.sample_id, name=""greedy_predictions"")\n\n        sample_outputs, sample_final_state, sample_fsl = tf.contrib.seq2seq.dynamic_decode(\n            sample_decoder,\n            swap_memory=True)\n        sample_logits = sample_outputs.rnn_output\n        sample_predictions = tf.identity(sample_outputs.sample_id, name=""sample_predictions"")\n\n        \'\'\'\n        beam_search_outputs, beam_search_final_state, beam_search_fsl = tf.contrib.seq2seq.dynamic_decode(\n            beam_search_decoder,\n            swap_memory=True)\n        beam_search_logits = tf.no_op()\n        beam_search_predictions = tf.identity(beam_search_outputs.predicted_ids, name=""beam_search_predictions"")\n        print(beam_search_predictions)\n        \'\'\'\n        z,y,a = tf.case(\n            pred_fn_pairs={\n                tf.equal(sampling_temperature, tf.constant(0.0)): lambda: (greedy_predictions, greedy_fsl, greedy_logits),\n                tf.equal(sampling_temperature, tf.constant(1.0)): lambda: (sample_predictions, sample_fsl, sample_logits),\n            },\n            default = lambda: (sample_predictions, sample_fsl, sample_logits),\n            exclusive=True\n        )\n\n        predictions, final_sequence_lengths, logits = tf.case(\n            pred_fn_pairs={\n                tf.equal(decoder_technique, tf.constant(0)): lambda: (greedy_predictions, greedy_fsl, greedy_logits),\n                tf.equal(decoder_technique, tf.constant(1)): lambda: (sample_predictions, sample_fsl, sample_logits),\n                #tf.equal(decoder_technique, tf.constant(2)): lambda: (beam_search_predictions, beam_search_fsl)\n            },\n            exclusive=True)\n\n        predictions = tf.identity(predictions, name=\'predictions\')\n        final_sequence_lengths = tf.identity(final_sequence_lengths, name=\'final_sequence_lengths\')\n        logits = tf.identity(logits, name=\'logits\')\n\n    return {\n        \'lr\': lr,\n        \'keep_prob\': keep_prob,\n        \'decoder_technique\': decoder_technique,\n        \'sampling_temperature\': sampling_temperature,\n        #\'beam_width\': beam_width,\n        \'seq_source_ids\': seq_source_ids,\n        \'seq_source_lengths\': seq_source_lengths,\n        \'seq_reference_ids\': seq_reference_ids,\n        \'seq_reference_lengths\': seq_reference_lengths,\n        #\'final_state\': final_state,\n        \'final_sequence_lengths\': final_sequence_lengths,\n        \'embedding_source\': encoder_embedding,\n        \'encoder_states\': encoder_states,\n        \'loss\': loss,\n        \'predictions\': predictions,\n        \'labels\': labels,\n        \'summaries\': summaries,\n        \'train_step\': train_step,\n        \'dummy\': dummy\n    }\n\n'"
paraphraser/nlp_pipeline.py,0,"b""import sys\nimport spacy\nfrom spacy.tokenizer import Tokenizer\nimport datetime as dt\nimport multiprocessing as mp\n\nnlp = spacy.load('en')\ntokenizer = Tokenizer(nlp.vocab)\n\ndef nlp_pipeline(sentence, word_to_id, unk_id):\n    ''' Convert word tokens into their vocab ids '''\n    return [ word_to_id.get(token.lower_, unk_id) for token in nlp_pipeline_0(sentence) ]\n\ndef nlp_pipeline_0(sentence):\n    ''' Execute spacy pipeline, single thread '''\n    return nlp(sentence, disable=['parser', 'tagger', 'ner'])\n\ndef mp_nlp_pipeline(pool, lines):\n    ''' Execute spacy pipeline, multiprocessing style '''\n    return pool.map(nlp_pipeline_0, lines, 1)\n\ndef openmp_nlp_pipeline(lines, n_threads=12):\n    ''' Execute spacy's openmp nlp pipeline '''\n    return [ [ token.lower_ for token in doc ] for doc in nlp.pipe(lines, n_threads=n_threads, disable=['parser', 'tagger', 'ner']) ]\n\ndef single_thread_nlp_pipeline(lines):\n    ''' Another single thread pipeline '''\n    return [ nlp(line) for line in lines ]\n\ndef main():\n    import datetime as dt    \n    from embeddings import load_sentence_embeddings\n    #pool = mp.Pool(10)\n\n    word_to_id, idx_to_word, embedding, start_id, end_id, unk_id = load_sentence_embeddings()\n    print(unk_id)\n\n    with open('/media/sdb/datasets/para-nmt-5m-processed/para-nmt-5m-processed.txt', 'r') as f:\n        lines = []\n        for i, line in enumerate(f):\n            lines.append(line.strip())\n\n            if i % 64 == 0:\n                start = dt.datetime.now()\n                #docs = mp_nlp_pipeline(pool, lines)\n                docs = openmp_nlp_pipeline(lines, word_to_id, unk_id)\n                #docs = single_thread_nlp_pipeline(lines)\n                #doc = nlp_pipeline_0(line)\n                print(docs)\n\n                end = dt.datetime.now()\n                print(end - start, flush=True)\n                lines = []\n            else:\n                continue\n\n\nif __name__ == '__main__':\n    main()\n\n"""
paraphraser/paraphraser.py,0,b'from synonym_model import synonym_paraphrase\nfrom inference import Paraphraser\n'
paraphraser/preprocess_data.py,0,"b'""""""Dataset preprocessing and generation.\n\nThis module\'s purpose is to consume raw paraphrase text and output a dataset\nin an optimal form to later be consumed by ParaphraseDataset class in\ndataset_generator.py.  The raw text are assumed to be valid paraphrases\nand must follow the following format each line:\n\nsource sentence\\treference sentence\n\nThe number of tokens within a sentence are counted so that samples can be \ngrouped into the same file by similar length.  After nlp preprocessing and\ntokenization, the resulting new format per line is:\n\nsource sentence tokens\\tsource sentence token ids\\treference tokens\\treference token ids\n\nThis format is consumed directly into ParaphraseDataset to generate mini\nbatches where each batch contains similar length sentences.\n\n""""""\n\nimport os\nfrom six import iteritems\nfrom nlp_pipeline import openmp_nlp_pipeline\nfrom embeddings import load_sentence_embeddings\n\nword_to_id, idx_to_word, embedding, start_id, end_id, unk_id, mask_id = load_sentence_embeddings()\n\ndef generate_length_index(max_lengths):\n    l = []\n    prev = None\n    for ml in max_lengths:\n        if prev == None:\n            a = (ml+1) * [ml]\n        else:\n            a = (ml - prev) * [ml]\n        prev = ml\n        l.extend(a)\n    return l\n\ndef word_to_token_ids(batch_docs):\n    batch_token_ids = [ [ word_to_id.get(word, unk_id) for word in doc ] for doc in batch_docs ]\n    return batch_token_ids\n\ndef preprocess_batch(batch_sentences):\n    # NLP Pipleine\n    batch_words = openmp_nlp_pipeline(batch_sentences)\n    batch_ids_ = word_to_token_ids(batch_words)\n\n    # Create reference, preprend start id, append end id\n    batch_ids = [ [start_id] + ids + [end_id] for ids in batch_ids_ ]\n    \n    return (batch_words, batch_ids)\n\ndef fsave_data(filename, batch_source_words, batch_source_ids, batch_ref_words, batch_ref_ids):\n    max_lengths = [5, 10, 20, 30, 40, 50]\n\n    for length in max_lengths:\n        try:\n            os.remove(filename + ""."" + str(length))\n        except:\n            pass\n\n    files = { length: open(filename + ""."" + str(length), \'a\') for length in max_lengths }\n    l = generate_length_index(max_lengths)\n\n    z = zip(batch_source_words, batch_source_ids, batch_ref_words, batch_ref_ids)\n\n    for source_words, source_ids, ref_words, ref_ids in z:\n        max_len = max(len(source_ids), len(ref_ids))\n        try:\n            files[l[max_len]].write(""{}\\t{}\\t{}\\t{}\\n"".format(\' \'.join(source_words), \n                                                              \' \'.join([ str(source_id) for source_id in source_ids ]),\n                                                              \' \'.join(ref_words),\n                                                              \' \'.join([ str(ref_id) for ref_id in ref_ids ])))\n        except Exception as e:\n            print(e)\n            print(""Error writing {} {} {} {}"".format(\' \'.join(source_words),\n                                                     \' \'.join([ str(source_id) for source_id in source_ids ]),\n                                                     \' \'.join(ref_words),\n                                                     \' \'.join([ str(ref_id) for ref_id in ref_ids ])))\n            continue\n\n    for length, f in iteritems(files):\n        f.close()\n\ndef preprocess_data(filename):\n    batch_source_sentences = []\n    batch_ref_sentences = []\n\n    with open(filename, \'r\') as f:\n        for i, line in enumerate(f):\n            source, ref = line.split(\'\\t\')\n            batch_source_sentences.append(source.strip())\n            batch_ref_sentences.append(ref.strip())\n\n    batch_source_words, batch_source_ids = preprocess_batch(batch_source_sentences)\n    batch_ref_words, batch_ref_ids = preprocess_batch(batch_ref_sentences)\n\n    fsave_data(filename, batch_source_words, batch_source_ids, batch_ref_words, batch_ref_ids)\n\ndef main():\n    import sys\n    preprocess_data(sys.argv[1])\n\nif __name__ == \'__main__\':\n    main()\n\n'"
paraphraser/sample_embedding_helper.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\n\nimport six\n\nfrom tensorflow.contrib.seq2seq.python.ops import decoder\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import embedding_ops\nfrom tensorflow.python.ops import gen_array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.ops.distributions import bernoulli\nfrom tensorflow.python.ops.distributions import categorical\nfrom tensorflow.python.util import nest\nfrom tensorflow.contrib.seq2seq.python.ops.helper import GreedyEmbeddingHelper\n\n\nclass MySampleEmbeddingHelper(GreedyEmbeddingHelper):\n  """"""A helper for use during inference.\n  Uses sampling (from a distribution) instead of argmax and passes the\n  result through an embedding layer to get the next input.\n  """"""\n\n  def __init__(self, embedding, start_tokens, end_token,\n               softmax_temperature=None, seed=None):\n    """"""Initializer.\n    Args:\n      embedding: A callable that takes a vector tensor of `ids` (argmax ids),\n        or the `params` argument for `embedding_lookup`. The returned tensor\n        will be passed to the decoder input.\n      start_tokens: `int32` vector shaped `[batch_size]`, the start tokens.\n      end_token: `int32` scalar, the token that marks end of decoding.\n      softmax_temperature: (Optional) `float32` scalar, value to divide the\n        logits by before computing the softmax. Larger values (above 1.0) result\n        in more random samples, while smaller values push the sampling\n        distribution towards the argmax. Must be strictly greater than 0.\n        Defaults to 1.0.\n      seed: (Optional) The sampling seed.\n    Raises:\n      ValueError: if `start_tokens` is not a 1D tensor or `end_token` is not a\n        scalar.\n    """"""\n    super(MySampleEmbeddingHelper, self).__init__(\n        embedding, start_tokens, end_token)\n    self._softmax_temperature = softmax_temperature\n    self._seed = seed\n\n  def sample(self, time, outputs, state, name=None):\n    """"""sample for SampleEmbeddingHelper.""""""\n    del time, state  # unused by sample_fn\n    # Outputs are logits, we sample instead of argmax (greedy).\n    if not isinstance(outputs, ops.Tensor):\n      raise TypeError(""Expected outputs to be a single Tensor, got: %s"" %\n                      type(outputs))\n    if self._softmax_temperature is None:\n      logits = outputs\n    else:\n      #logits = outputs / self._softmax_temperature\n      logits = math_ops.divide(outputs, self._softmax_temperature)\n\n    sample_id_sampler = categorical.Categorical(logits=logits)\n    sample_ids = sample_id_sampler.sample(seed=self._seed)\n\n    return sample_ids\n\n'"
paraphraser/synonym_model.py,0,"b""import sys\nimport spacy\nfrom pprint import pprint\nfrom spacy.tokens.token import Token\nfrom nltk.corpus import wordnet as wn\nfrom six.moves import xrange\nimport random\n\nnlp = spacy.load('en')\n\ndef generate_sentence(original_doc, new_tokens):\n    new_sentence = ' '.join(new_tokens).replace('_', ' ')\n    new_doc = nlp(new_sentence)\n    similarity_score = original_doc.similarity(new_doc)\n    return (new_sentence, similarity_score)\n\ndef synonym_model(s):\n    generated_sentences = set([])\n\n    doc = nlp(s)\n    original_tokens = [ token.text for token in doc ]\n\n    index_to_lemmas = {}\n\n    for index, token in enumerate(doc):\n        index_to_lemmas[index] = set([])\n        index_to_lemmas[index].add(token)\n\n        if token.pos_ == 'NOUN' and len(token.text) >= 3:\n            pos = wn.NOUN\n        elif token.pos_ == 'VERB' and len(token.text) >= 3:\n            pos = wn.VERB\n        elif token.pos_ == 'ADV' and len(token.text) >= 3:\n            pos = wn.ADV\n        elif token.pos_ == 'ADJ' and len(token.text) >= 3:\n            pos = wn.ADJ\n        else:\n            continue\n\n        # Synsets\n        for synset in wn.synsets(token.text, pos):\n            for lemma in synset.lemmas():\n                new_tokens = original_tokens.copy()\n                new_tokens[index] = lemma.name()\n                sentence_and_score = generate_sentence(doc, new_tokens)\n                generated_sentences.add(sentence_and_score)\n                index_to_lemmas[index].add(lemma.name())\n\n    count = sum([ len(words) for words in index_to_lemmas.values() ])\n\n    for i in xrange(min(count, 40)):\n        new_tokens = []\n        for index, words in sorted(index_to_lemmas.items(), key=lambda x: x[0]):\n            token = random.sample(index_to_lemmas[index], 1)[0]\n            new_tokens.append(str(token))\n        sentence_and_score = generate_sentence(doc, new_tokens)\n        generated_sentences.add(sentence_and_score)\n\n    #print(generated_sentences)\n    return generated_sentences\n\ndef synonym_paraphrase(s):\n    return synonym_model(s)\n\nif __name__ == '__main__':\n    #x = synonym_model('I am discussing my homework with the teacher.')\n    #x = synonym_model('the rabbit quickly ran down the hole')\n    #x = synonym_model('John tried to fix his computer by hacking away at it.')\n    x = synonym_model('team based multiplayer online first person shooter video game')\n    print(x)\n\n"""
paraphraser/training_pipeline.py,14,"b'import argparse\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport sys\nimport datetime as dt\nfrom six.moves import xrange, input\nfrom lstm_model_beam import lstm_model\nfrom embeddings import load_sentence_embeddings\nfrom dataset_generator import ParaphraseDataset\nfrom nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\nfrom utils import dataset_config, debug_data, summarize_scalar\nimport logging\n\nlogging.basicConfig(format = u\'[%(asctime)s] %(levelname)-8s : %(message)s\', level = logging.INFO)\n\ndef evaluate(sess, model, dataset_generator, mode, id_to_vocab):\n    """"""Evaluate current model on the dev or test set.\n    \n    Args:\n        sess: Tensorflow session\n        model: dictionary containing model\'s tensors of interest for evaluation\n        dataset_generator: dataset batch generator\n        mode: \'dev\' or \'test\'\n        id_to_vocab: voabulary dictionary id -> word\n\n    Returns:\n        loss: the loss after evaluating the dataset\n        bleu_score: BLEU score after evaluation\n    """"""\n\n    batch_generator = dataset_generator.generate_batch(mode)\n    chencherry = SmoothingFunction()\n    batch_losses = []\n    all_seq_ref_words = []\n    all_bleu_pred_words = []\n\n    for batch in batch_generator:\n        seq_source_ids = batch[\'seq_source_ids\']\n        seq_source_words = batch[\'seq_source_words\']\n        seq_source_len = batch[\'seq_source_len\']\n        seq_ref_ids = batch[\'seq_ref_ids\']\n        seq_ref_words = batch[\'seq_ref_words\']\n        seq_ref_len = batch[\'seq_ref_len\']\n\n        feed_dict = {\n            model[\'seq_source_ids\']: seq_source_ids,\n            model[\'seq_source_lengths\']: seq_source_len,\n            model[\'seq_reference_ids\']: seq_ref_ids,\n            model[\'seq_reference_lengths\']: seq_ref_len\n        }\n\n        feeds = [\n            model[\'loss\'],\n            model[\'predictions\'], \n            model[\'final_sequence_lengths\']\n        ]\n\n        try:\n            batch_loss, predictions, fsl = sess.run(feeds, feed_dict)\n        except Exception as e:\n            debug_data(seq_source_ids, seq_ref_ids, seq_source_len, seq_ref_len, id_to_vocab)\n            raise e\n\n        # batch losses\n        batch_losses.append(batch_loss)\n\n        # all ref words\n        seq_ref_words = [ [ref_words] for ref_words in seq_ref_words ]\n        all_seq_ref_words.extend(seq_ref_words)\n\n        # all prediction words to compute bleu on\n        bleu_pred_words = [ [ id_to_vocab[vocab_id] for vocab_id in prediction if vocab_id in id_to_vocab ] for prediction in predictions ]\n        bleu_pred_words = [ pred_words[:pred_words.index(\'<END>\') if \'<END>\' in pred_words else len(pred_words) ] for pred_words in bleu_pred_words ]\n        all_bleu_pred_words.extend(bleu_pred_words)\n\n    bleu_score = corpus_bleu(all_seq_ref_words, all_bleu_pred_words, smoothing_function=chencherry.method1)\n    loss = sum(batch_losses) / len(batch_losses)\n    logging.info(""{} : Evaluating on {} set loss={:.4f} bleu={:.4f}"".format(dt.datetime.now(), mode, loss, bleu_score))\n    return loss, bleu_score\n\ndef infer(sess, args, model, id_to_vocab, end_id):\n    """"""Perform inference on a model.  This is intended to be interactive.\n    A user will run this from the command line to provide an input sentence\n    and receive a paraphrase as output continuously within a loop.\n\n    Args:\n        sess: Tensorflow session\n        args: ArgumentParser object configuration\n        model: a dictionary containing the model tensors\n        id_to_vocab: vocabulary index of id_to_vocab\n        end_id: the end of sentence token\n\n    """"""\n    from preprocess_data import preprocess_batch\n\n    while 1:\n        source_sent = input(""Enter source sentence: "")\n        seq_source_words, seq_source_ids = preprocess_batch([ source_sent ])\n        seq_source_len = [ len(seq_source) for seq_source in seq_source_ids ]\n\n        if args.decoder == \'greedy\':\n            decoder = 0\n        elif args.decoder == \'sample\':\n            decoder = 1\n\n        feed_dict = {\n            model[\'seq_source_ids\']: seq_source_ids,\n            model[\'seq_source_lengths\']: seq_source_len,\n            model[\'decoder_technique\']: decoder,\n            model[\'sampling_temperature\']: args.sampling_temperature,\n        }\n\n        feeds = [\n            model[\'predictions\'], \n            model[\'final_sequence_lengths\']\n        ]\n\n        predictions, final_sequence_lengths = sess.run(feeds, feed_dict)\n\n        for sent_pred in predictions:\n            if sent_pred[-1] == end_id:\n                sent_pred = sent_pred[0:-1]\n            print(""Paraphrase : {}"".format(\' \'.join([ id_to_vocab[pred] for pred in sent_pred ])))\n            \ndef compress_graph(sess, args, model):\n    """"""After training has completed, this function can be called to compress\n    the model.  The computation graph is frozen turning the checkpoint\n    variables into constants.  Finally, optimization is done by stripping\n    away all unnecessary nodes from the graph if they are not used at\n    inference time.\n\n    Args:\n        sess: Tensorflow session\n        args: ArgumentParser config object\n        model: model dictionary containing tensors of interest\n\n    """"""\n    from tensorflow.python.tools import freeze_graph \n    from tensorflow.python.tools import optimize_for_inference_lib\n\n    tf.train.write_graph(sess.graph_def, \'/media/sdb/models/paraphraser\', \'model.pb\', as_text=False)\n\n    freeze_graph.freeze_graph(\n        #input_graph=\'/tmp/model.pbtxt\', \n        input_graph=\'/media/sdb/models/paraphraser/model.pb\',\n        input_saver=\'\',\n        input_binary=True, \n        input_checkpoint=args.checkpoint,\n        output_node_names=\'predictions\',\n        restore_op_name=\'save/restore_all\', \n        filename_tensor_name=\'save/Const:0\',\n        output_graph=\'/media/sdb/models/paraphraser/frozen_model.pb\', \n        clear_devices=True, \n        initializer_nodes=\'\')\n\n    \'\'\'\n    input_graph_def = tf.GraphDef()\n    #with tf.gfile.Open(\'/media/sdb/models/paraphraser/frozen_model.pb\', \'rb\') as f:\n    with tf.gfile.Open(\'/tmp/frozen_model.pb\', \'rb\') as f:\n        data = f.read()\n        input_graph_def.ParseFromString(data)\n        with tf.Graph().as_default() as graph:\n            tf.import_graph_def(input_graph_def)\n            print(dir(graph))\n            print(graph.find_tensor_by_name(\'placeholders/sampling_temperature\'))\n\n    output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n        input_graph_def,\n        [\'placeholders/source_ids\', \'placeholders/sequence_source_lengths\'],\n        [\'predictions\'],\n        tf.float32.as_datatype_enum)\n    \n    f = tf.gfile.FastGFile(\'/tmp/optimized_model.pb\', ""w"")\n    f.write(output_graph_def.SerializeToString())\n    \'\'\'\n\n        \ndef parse_arguments():\n    """"""Argument parser configuration.""""""\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'--log_dir\', type=str, default=""logs"", help=""Log directory to store tensorboard summary and model checkpoints"")\n    parser.add_argument(\'--epochs\', type=int, default=3, help=""Number of epochs to train"")\n    parser.add_argument(\'--lr\', type=float, default=1e-3, help=""Learning rate"")\n    parser.add_argument(\'--batch_size\', type=int, default=64, help=""Mini batch size"")\n    parser.add_argument(\'--max_seq_length\', type=int, default=40, help=""Maximum sequence length.  Sentence lengths beyond this are truncated."")\n    parser.add_argument(\'--hidden_size\', type=int, default=300, help=""Hidden dimension size"")\n    parser.add_argument(\'--keep_prob\', type=float, default=0.8, help=""Keep probability for dropout"")\n    parser.add_argument(\'--decoder\', type=str, choices=[\'greedy\', \'sample\'], help=""Decoder type"")\n    parser.add_argument(\'--sampling_temperature\', type=float, default=0.0, help=""Sampling temperature"")\n    parser.add_argument(\'--mode\', type=str, default=None, choices=[\'train\', \'dev\', \'test\', \'infer\'], help=\'train or dev or test or infer or minimize\')\n    parser.add_argument(\'--checkpoint\', type=str, default=None, help=""Model checkpoint file"")\n    parser.add_argument(\'--minimize_graph\', type=bool, default=False, help=""Save existing checkpoint to minimal graph"")\n\n    return parser.parse_args()\n\ndef main():\n    """"""Entry point for all training, evaluation, and model compression begins here""""""\n    args = parse_arguments()\n    word_to_id, id_to_vocab, embeddings, start_id, end_id, unk_id, mask_id = load_sentence_embeddings()\n    vocab_size, embedding_size = embeddings.shape\n    lr = args.lr\n\n    dataset = dataset_config()\n\n    if args.mode not in set([\'train\', \'dev\', \'test\', \'infer\', \'minimize\']):\n        raise ValueError(""{} is not a valid mode"".format(args.mode))\n\n    with tf.Session() as sess:\n        start = dt.datetime.now()\n        model = lstm_model(sess, args.mode, args.hidden_size, embeddings, start_id, end_id, mask_id)\n\n        # Saver object\n        saver = tf.train.Saver()\n        name_to_var_map = {var.op.name: var for var in tf.global_variables()}\n\n        # Restore checkpoint\n        if args.checkpoint:\n            saver.restore(sess, args.checkpoint)\n\n        # Save minimal graph\n        if args.minimize_graph:\n            compress_graph(sess, args, model)\n            return\n\n        # Load dataset only in train, dev, or test mode\n        if args.mode in set([\'train\', \'dev\', \'test\']):\n            logging.info(""{}: Loading dataset into memory."".format(dt.datetime.now()))\n            dataset_generator = ParaphraseDataset(dataset, args.batch_size, embeddings, word_to_id, start_id, end_id, unk_id, mask_id)\n\n        # Evaluate on dev or test\n        if args.mode == \'dev\' or args.mode == \'test\':\n            evaluate(sess, model, dataset_generator, args.mode, id_to_vocab)\n            return\n\n        # Perform inferencing\n        if args.mode == \'infer\':\n            infer(sess, args, model, id_to_vocab, end_id)\n            return\n\n        ###################################\n        # Training run proceeds from here #\n        ###################################\n\n        # Training summary writer\n        train_logdir = os.path.join(args.log_dir, ""train-"" + start.strftime(""%Y%m%d-%H%M%S""))\n        train_writer = tf.summary.FileWriter(train_logdir)\n\n        # Dev summary writer\n        dev_logdir = os.path.join(args.log_dir, ""dev-"" + start.strftime(""%Y%m%d-%H%M%S""))\n        dev_writer = tf.summary.FileWriter(dev_logdir)\n\n        chencherry = SmoothingFunction()\n        global_step = 0\n        tf.global_variables_initializer().run()\n        sess.run(model[\'dummy\'], {model[\'sampling_temperature\']: 7.5})\n\n        # Training per epoch\n        for epoch in xrange(args.epochs):\n            train_losses = []\n            train_batch_generator = dataset_generator.generate_batch(\'train\')\n            for train_batch in train_batch_generator:\n                seq_source_ids = train_batch[\'seq_source_ids\']\n                seq_source_words = train_batch[\'seq_source_words\']\n                seq_source_len = train_batch[\'seq_source_len\']\n                seq_ref_ids = train_batch[\'seq_ref_ids\']\n                seq_ref_words = train_batch[\'seq_ref_words\']\n                seq_ref_len = train_batch[\'seq_ref_len\']\n\n                feed_dict = {\n                    model[\'lr\']: lr,\n                    model[\'seq_source_ids\']: seq_source_ids,\n                    model[\'seq_source_lengths\']: seq_source_len,\n                    model[\'seq_reference_ids\']: seq_ref_ids,\n                    model[\'seq_reference_lengths\']: seq_ref_len,\n                    model[\'keep_prob\']: args.keep_prob\n                }\n\n                feeds = [\n                    model[\'train_step\'], \n                    model[\'loss\'], \n                    model[\'predictions\'], \n                    model[\'summaries\'],\n                    model[\'final_sequence_lengths\']\n                ]\n\n                try:\n                    _, batch_loss, predictions, summary, fsl = sess.run(feeds, feed_dict)\n                except Exception as e:\n                    debug_data(seq_source_ids, seq_ref_ids, seq_source_len, seq_ref_len, id_to_vocab)\n                    raise e\n\n                train_losses.append(batch_loss)\n\n                # Status update\n                if global_step % 25 == 0:\n                    train_writer.add_summary(summary, global_step)\n                    train_writer.flush()\n                    seq_ref_words = [ [ref_words] for ref_words in seq_ref_words ]\n                    bleu_pred_words = [ [ id_to_vocab[vocab_id] for vocab_id in prediction if vocab_id in id_to_vocab ] for prediction in predictions ]\n                    bleu_pred_words = [ pred_words[:pred_words.index(\'<END>\') if \'<END>\' in pred_words else len(pred_words) ] for pred_words in bleu_pred_words ]\n                    bleu_score = corpus_bleu(seq_ref_words, bleu_pred_words, smoothing_function=chencherry.method1)\n                    summarize_scalar(train_writer, \'bleu_score\', bleu_score, global_step)\n                    train_loss = sum(train_losses) / len(train_losses)\n                    summarize_scalar(train_writer, \'loss\', train_loss, global_step)\n                    logging.info(""step={} epoch={} batch_loss={:.4f} train_loss={:.4f} bleu={:.4f}"".format(global_step, epoch, batch_loss, train_loss, bleu_score))\n\n                # Print predictions for this batch every 1000 steps\n                # Evaluate on dev set\n                if global_step % 1000 == 0 and global_step != 0:\n                    debug_data(seq_source_ids, seq_ref_ids, seq_source_len, seq_ref_len, id_to_vocab)\n                    logging.info(""PREDICTIONS!"")\n                    logging.info(""final_seq_lengths: "" + str(fsl))\n                    logging.info(""len(predictions): "" + str(len(predictions)))\n                    for prediction in predictions:\n                        logging.info(str(len(prediction)) + \' \' + \' \'.join([id_to_vocab[vocab_id] for vocab_id in prediction if vocab_id in id_to_vocab]))\n\n                    dev_loss, bleu_score = evaluate(sess, model, dataset_generator, \'dev\', id_to_vocab)\n                    summarize_scalar(dev_writer, \'bleu_score\', bleu_score, global_step)\n                    summarize_scalar(dev_writer, \'loss\', dev_loss, global_step)\n                    dev_writer.flush()\n\n                # Checkpoint.\n                #if global_step % 50 == 0 and global_step != 0:\n                if global_step % 5000 == 0 and global_step != 0:\n                    saver.save(sess, os.path.join(train_logdir, \'model\'), global_step=global_step)\n\n                global_step += 1\n            # End train batch\n\n            saver.save(sess, os.path.join(train_logdir, \'model\'), global_step=global_step)\n            lr /= 10.\n        # End epoch\n\n        evaluate(sess, model, dataset_generator, \'test\', id_to_vocab)\n    # End sess\n\nif __name__ == \'__main__\':\n    main()\n\n'"
paraphraser/utils.py,1,"b'import logging\nimport tensorflow as tf\n\ndef summarize_scalar(writer, tag, value, step):\n    """"""Prepare data to be written to protobuf event file.  This is later\n    read into tensorboard for visualization.\n\n    Args:\n        writer: summary writer\n        tag: identifier name of the the data in question\n        value: the value the data takes on\n        step: global step during training\n    """"""\n    summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n    writer.add_summary(summary, step)\n\n\ndef debug_data(seq_source_ids, seq_ref_ids, seq_source_len, seq_ref_len, id_to_vocab):\n    """"""Debug dataset batch samples to ensure they take on intended avlues""""""\n    logging.info(""=============================================================="")\n    logging.info(""SOURCE!"")\n    #logging.info(seq_source_ids)\n    for source_ids in seq_source_ids:\n        logging.info(\' \'.join([id_to_vocab[source_id] for source_id in source_ids]))\n    logging.info(seq_source_len)\n    logging.info(""REFERENCE!"")\n    #logging.info(seq_ref_ids)\n    for i in seq_ref_ids:\n        logging.info(\' \'.join([id_to_vocab[label] for label in i if label != -1]))\n    logging.info(seq_ref_len)\n    logging.info(""=============================================================="")\n\ndef dataset_config():\n    """"""Dataset configuration.  Dataset files are grouped by sentences of maximum\n    length for train, dev, and test.  """"""\n\n    dataset = [\n        { \n            \'maxlen\': 5,\n            \'train\': \'/media/sdb/datasets/aggregate_paraphrase_corpus_0/dataset.train.5\',\n            \'dev\': \'/media/sdb/datasets/aggregate_paraphrase_corpus_0/dataset.dev.5\',\n            \'test\': \'/media/sdb/datasets/aggregate_paraphrase_corpus_0/dataset.test.5\' \n        },\n        { \n            \'maxlen\': 10,\n            \'train\': \'/media/sdb/datasets/aggregate_paraphrase_corpus_0/dataset.train.10\',\n            \'dev\': \'/media/sdb/datasets/aggregate_paraphrase_corpus_0/dataset.dev.10\',\n            \'test\': \'/media/sdb/datasets/aggregate_paraphrase_corpus_0/dataset.test.10\' \n        },\n        { \n            \'maxlen\': 20,\n            \'train\': \'/media/sdb/datasets/aggregate_paraphrase_corpus_0/dataset.train.20\',\n            \'dev\': \'/media/sdb/datasets/aggregate_paraphrase_corpus_0/dataset.dev.20\',\n            \'test\': \'/media/sdb/datasets/aggregate_paraphrase_corpus_0/dataset.test.20\' \n        },\n        { \n            \'maxlen\': 30,\n            \'train\': \'/media/sdb/datasets/aggregate_paraphrase_corpus_0/dataset.train.30\',\n            \'dev\': \'/media/sdb/datasets/aggregate_paraphrase_corpus_0/dataset.dev.30\',\n            \'test\': \'/media/sdb/datasets/aggregate_paraphrase_corpus_0/dataset.test.30\' \n        },\n        { \n            \'maxlen\': 40,\n            \'train\': \'/media/sdb/datasets/aggregate_paraphrase_corpus_0/dataset.train.40\',\n            \'dev\': \'/media/sdb/datasets/aggregate_paraphrase_corpus_0/dataset.dev.40\',\n            \'test\': \'/media/sdb/datasets/aggregate_paraphrase_corpus_0/dataset.test.40\' \n        },\n        { \n            \'maxlen\': 50,\n            \'train\': \'/media/sdb/datasets/aggregate_paraphrase_corpus_0/dataset.train.50\',\n            \'dev\': \'/media/sdb/datasets/aggregate_paraphrase_corpus_0/dataset.dev.50\',\n            \'test\': \'/media/sdb/datasets/aggregate_paraphrase_corpus_0/dataset.test.50\' \n        }\n    ]\n\n    return dataset\n\n'"
