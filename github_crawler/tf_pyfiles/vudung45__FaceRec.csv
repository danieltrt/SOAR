file_path,api_count,code
align_custom.py,0,"b'\'\'\'\nImplement  Dlib Face alignment strategy\n\nHowever, this method/approach doesn\'t deform the original image like Dlib does.\n\nThis also categorizes the face in 3 types: Center, Left, Right\n\nAlign face based on facial landmarks\n\'\'\'\nimport math\n\nimport cv2\nimport numpy as np\n\n\nclass AlignCustom(object):\n    def __init__(self):\n        pass\n    \n    def getPos(self, points):\n        if abs(points[0] - points[2]) / abs(points[1] - points[2]) > 2:\n            return ""Right"";\n        elif abs(points[1] - points[2]) / abs(points[0] - points[2]) > 2:\n            return ""Left"";\n        return ""Center""\n\n    def list2colmatrix(self, pts_list):\n        """"""\n            convert list to column matrix\n        Parameters:\n        ----------\n            pts_list:\n                input list\n        Retures:\n        -------\n            colMat:\n\n        """"""\n        assert len(pts_list) > 0\n        colMat = []\n        for i in range(len(pts_list)):\n            colMat.append(pts_list[i][0])\n            colMat.append(pts_list[i][1])\n        colMat = np.matrix(colMat).transpose()\n        return colMat\n\n    def find_tfrom_between_shapes(self, from_shape, to_shape):\n        """"""\n            find transform between shapes\n        Parameters:\n        ----------\n            from_shape:\n            to_shape:\n        Retures:\n        -------\n            tran_m:\n            tran_b:\n        """"""\n        assert from_shape.shape[0] == to_shape.shape[0] and from_shape.shape[0] % 2 == 0\n\n        sigma_from = 0.0\n        sigma_to = 0.0\n        cov = np.matrix([[0.0, 0.0], [0.0, 0.0]])\n\n        # compute the mean and cov\n        from_shape_points = from_shape.reshape(int(from_shape.shape[0] / 2), 2)\n        to_shape_points = to_shape.reshape(int(to_shape.shape[0] / 2), 2)\n        mean_from = from_shape_points.mean(axis=0)\n        mean_to = to_shape_points.mean(axis=0)\n\n        for i in range(from_shape_points.shape[0]):\n            temp_dis = np.linalg.norm(from_shape_points[i] - mean_from)\n            sigma_from += temp_dis * temp_dis\n            temp_dis = np.linalg.norm(to_shape_points[i] - mean_to)\n            sigma_to += temp_dis * temp_dis\n            cov += (to_shape_points[i].transpose() - mean_to.transpose()) * (from_shape_points[i] - mean_from)\n\n        sigma_from = sigma_from / to_shape_points.shape[0]\n        sigma_to = sigma_to / to_shape_points.shape[0]\n        cov = cov / to_shape_points.shape[0]\n\n        # compute the affine matrix\n        s = np.matrix([[1.0, 0.0], [0.0, 1.0]])\n        u, d, vt = np.linalg.svd(cov)\n\n        if np.linalg.det(cov) < 0:\n            if d[1] < d[0]:\n                s[1, 1] = -1\n            else:\n                s[0, 0] = -1\n        r = u * s * vt\n        c = 1.0\n        if sigma_from != 0:\n            c = 1.0 / sigma_from * np.trace(np.diag(d) * s)\n\n        tran_b = mean_to.transpose() - c * r * mean_from.transpose()\n        tran_m = c * r\n\n        return tran_m, tran_b\n\n    def align(self, desired_size, img, landmarks, padding=0.1):\n        """"""\n        Align face in BGR format.\n        :param size: size image\n        :type size: number\n        :param img_face: face image detected\n        :type img_face: array 3D\n        :return aligned_face: align face\n        :rtype aligned_face: array 3D\n        :return pos: position of face\n        :rtype pos: \'Left\', \'Center\', \'Right\'\n        """"""\n        shape = []\n        for k in range(int(len(landmarks) / 2)):\n            shape.append(landmarks[k])\n            shape.append(landmarks[k + 5])\n\n        if padding > 0:\n            padding = padding\n        else:\n            padding = 0\n        # average positions of face points\n        mean_face_shape_x = [0.224152, 0.75610125, 0.490127, 0.254149, 0.726104]\n        mean_face_shape_y = [0.2119465, 0.2119465, 0.628106, 0.780233, 0.780233]\n\n        from_points = []\n        to_points = []\n\n        for i in range(int(len(shape) / 2)):\n            x = (padding + mean_face_shape_x[i]) / (2 * padding + 1) * desired_size\n            y = (padding + mean_face_shape_y[i]) / (2 * padding + 1) * desired_size\n            to_points.append([x, y])\n            from_points.append([shape[2 * i], shape[2 * i + 1]])\n\n        # convert the points to Mat\n        from_mat = self.list2colmatrix(from_points)\n        to_mat = self.list2colmatrix(to_points)\n\n        # compute the similar transfrom\n        tran_m, tran_b = self.find_tfrom_between_shapes(from_mat, to_mat)\n\n        probe_vec = np.matrix([1.0, 0.0]).transpose()\n        probe_vec = tran_m * probe_vec\n\n        scale = np.linalg.norm(probe_vec)\n        angle = 180.0 / math.pi * math.atan2(probe_vec[1, 0], probe_vec[0, 0])\n\n        from_center = [(shape[0] + shape[2]) / 2.0, (shape[1] + shape[3]) / 2.0]\n        to_center = [0, 0]\n        to_center[1] = desired_size * 0.4\n        to_center[0] = desired_size * 0.5\n\n        ex = to_center[0] - from_center[0]\n        ey = to_center[1] - from_center[1]\n\n        rot_mat = cv2.getRotationMatrix2D((from_center[0], from_center[1]), -1 * angle, scale)\n\n        rot_mat[0][2] += ex\n        rot_mat[1][2] += ey\n\n        chips = cv2.warpAffine(img, rot_mat, (desired_size, desired_size))\n        return chips, self.getPos(landmarks)\n'"
face_feature.py,8,"b'\'\'\'\n@Author: David Vu\nRun the pretrained model to extract 128D face features\n\'\'\'\n\nimport tensorflow as tf\nfrom architecture import inception_resnet_v1 as resnet\nfrom tensorflow.python.platform import gfile\nimport numpy as np\nimport os\nclass FaceFeature(object):\n    def __init__(self, face_rec_graph, model_path = \'models/20170512-110547.pb\'):\n        \'\'\'\n\n        :param face_rec_sess: FaceRecSession object\n        :param model_path:\n        \'\'\'\n        print(""Loading model..."")\n        with face_rec_graph.graph.as_default():\n            self.sess = tf.Session()\n            with self.sess.as_default():\n                self.__load_model(model_path)\n                self.x = tf.get_default_graph() \\\n                                            .get_tensor_by_name(""input:0"")\n                self.embeddings = tf.get_default_graph() \\\n                                    .get_tensor_by_name(""embeddings:0"")\n                self.phase_train_placeholder = tf.get_default_graph() \\\n                                                     .get_tensor_by_name(""phase_train:0"")                    \n\n                print(""Model loaded"")\n\n\n    def get_features(self, input_imgs):\n        images = load_data_list(input_imgs,160)\n        feed_dict = {self.x: images, self.phase_train_placeholder: False}\n\n        return self.sess.run(self.embeddings, feed_dict = feed_dict)\n\n\n\n    def __load_model(self, model):\n        # Check if the model is a model directory (containing a metagraph and a checkpoint file)\n        #  or if it is a protobuf file with a frozen graph\n        model_exp = os.path.expanduser(model)\n        if os.path.isfile(model_exp):\n            print(\'Model filename: %s\' % model_exp)\n            with gfile.FastGFile(model_exp, \'rb\') as file_:\n                graph_def = tf.GraphDef()\n                graph_def.ParseFromString(file_.read())\n                tf.import_graph_def(graph_def, name=\'\')\n        else:\n            print(\'Model directory: %s\' % model_exp)\n            meta_file, ckpt_file = get_model_filenames(model_exp)\n            print(\'Metagraph file: %s\' % meta_file)\n            print(\'Checkpoint file: %s\' % ckpt_file)\n            saver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file))\n            saver.restore(tf.get_default_session(), os.path.join(model_exp, ckpt_file))\n\n\ndef get_model_filenames(model_dir):\n    files = os.listdir(model_dir)\n    meta_files = [s for s in files if s.endswith(\'.meta\')]\n    if len(meta_files) == 0:\n        raise ValueError(\'No meta file found in the model directory (%s)\' % model_dir)\n    elif len(meta_files) > 1:\n        raise ValueError(\'There should not be more than one meta file \\\n                                    in the model directory (%s)\' % model_dir)\n    meta_file = meta_files[0]\n    meta_files = [s for s in files if \'.ckpt\' in s]\n    max_step = -1\n    for file_ in files:\n        step_str = re.match(r\'(^model-[\\w\\- ]+.ckpt-(\\d+))\', file_)\n        if step_str is not None and len(step_str.groups()) >= 2:\n            step = int(step_str.groups()[1])\n            if step > max_step:\n                max_step = step\n                ckpt_file = step_str.groups()[0]\n    return meta_file, ckpt_file\n\ndef tensorization(img):\n    \'\'\'\n    Prepare the imgs before input into model\n    :param img: Single face image\n    :return tensor: numpy array in shape(n, 160, 160, 3) ready for input to cnn\n    \'\'\'\n    tensor = img.reshape(-1, Config.Align.IMAGE_SIZE, Config.Align.IMAGE_SIZE, 3)\n    return tensor\n\n#some image preprocess stuff\ndef prewhiten(x):\n    mean = np.mean(x)\n    std = np.std(x)\n    std_adj = np.maximum(std, 1.0 / np.sqrt(x.size))\n    y = np.multiply(np.subtract(x, mean), 1 / std_adj)\n    return y\n\ndef load_data_list(imgList, image_size, do_prewhiten=True):\n    images = np.zeros((len(imgList), image_size, image_size, 3))\n    i = 0\n    for img in imgList:\n        if img is not None:\n            if do_prewhiten:\n                img = prewhiten(img)\n            images[i, :, :, :] = img\n            i += 1\n    return images\n'"
main.py,0,"b'\'\'\'\nMain program\n@Author: David Vu\n\nTo execute simply run:\nmain.py\n\nTo input new user:\nmain.py --mode ""input""\n\n\'\'\'\n\nimport cv2\nfrom align_custom import AlignCustom\nfrom face_feature import FaceFeature\nfrom mtcnn_detect import MTCNNDetect\nfrom tf_graph import FaceRecGraph\nimport argparse\nimport sys\nimport json\nimport time\nimport numpy as np\n\nTIMEOUT = 10 #10 seconds\n\ndef main(args):\n    mode = args.mode\n    if(mode == ""camera""):\n        camera_recog()\n    elif mode == ""input"":\n        create_manual_data();\n    else:\n        raise ValueError(""Unimplemented mode"")\n\'\'\'\nDescription:\nImages from Video Capture -> detect faces\' regions -> crop those faces and align them \n    -> each cropped face is categorized in 3 types: Center, Left, Right \n    -> Extract 128D vectors( face features)\n    -> Search for matching subjects in the dataset based on the types of face positions. \n    -> The preexisitng face 128D vector with the shortest distance to the 128D vector of the face on screen is most likely a match\n    (Distance threshold is 0.6, percentage threshold is 70%)\n    \n\'\'\'\ndef camera_recog():\n    print(""[INFO] camera sensor warming up..."")\n    vs = cv2.VideoCapture(0); #get input from webcam\n    detect_time = time.time()\n    while True:\n        _,frame = vs.read();\n        #u can certainly add a roi here but for the sake of a demo i\'ll just leave it as simple as this\n        rects, landmarks = face_detect.detect_face(frame,80);#min face size is set to 80x80\n        aligns = []\n        positions = []\n\n        for (i, rect) in enumerate(rects):\n            aligned_face, face_pos = aligner.align(160,frame,landmarks[:,i])\n            if len(aligned_face) == 160 and len(aligned_face[0]) == 160:\n                aligns.append(aligned_face)\n                positions.append(face_pos)\n            else: \n                print(""Align face failed"") #log        \n        if(len(aligns) > 0):\n            features_arr = extract_feature.get_features(aligns)\n            recog_data = findPeople(features_arr,positions)\n            for (i,rect) in enumerate(rects):\n                cv2.rectangle(frame,(rect[0],rect[1]),(rect[2],rect[3]),(255,0,0)) #draw bounding box for the face\n                cv2.putText(frame,recog_data[i][0]+"" - ""+str(recog_data[i][1])+""%"",(rect[0],rect[1]),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),1,cv2.LINE_AA)\n\n\n        cv2.imshow(""Frame"",frame)\n        key = cv2.waitKey(1) & 0xFF\n        if key == ord(""q""):\n            break\n\'\'\'\nfacerec_128D.txt Data Structure:\n{\n""Person ID"": {\n    ""Center"": [[128D vector]],\n    ""Left"": [[128D vector]],\n    ""Right"": [[128D Vector]]\n    }\n}\nThis function basically does a simple linear search for \n^the 128D vector with the min distance to the 128D vector of the face on screen\n\'\'\'\ndef findPeople(features_arr, positions, thres = 0.6, percent_thres = 70):\n    \'\'\'\n    :param features_arr: a list of 128d Features of all faces on screen\n    :param positions: a list of face position types of all faces on screen\n    :param thres: distance threshold\n    :return: person name and percentage\n    \'\'\'\n    f = open(\'./facerec_128D.txt\',\'r\')\n    data_set = json.loads(f.read());\n    returnRes = [];\n    for (i,features_128D) in enumerate(features_arr):\n        result = ""Unknown"";\n        smallest = sys.maxsize\n        for person in data_set.keys():\n            person_data = data_set[person][positions[i]];\n            for data in person_data:\n                distance = np.sqrt(np.sum(np.square(data-features_128D)))\n                if(distance < smallest):\n                    smallest = distance;\n                    result = person;\n        percentage =  min(100, 100 * thres / smallest)\n        if percentage <= percent_thres :\n            result = ""Unknown""\n        returnRes.append((result,percentage))\n    return returnRes    \n\n\'\'\'\nDescription:\nUser input his/her name or ID -> Images from Video Capture -> detect the face -> crop the face and align it \n    -> face is then categorized in 3 types: Center, Left, Right \n    -> Extract 128D vectors( face features)\n    -> Append each newly extracted face 128D vector to its corresponding position type (Center, Left, Right)\n    -> Press Q to stop capturing\n    -> Find the center ( the mean) of those 128D vectors in each category. ( np.mean(...) )\n    -> Save\n    \n\'\'\'\ndef create_manual_data():\n    vs = cv2.VideoCapture(0); #get input from webcam\n    print(""Please input new user ID:"")\n    new_name = input(); #ez python input()\n    f = open(\'./facerec_128D.txt\',\'r\');\n    data_set = json.loads(f.read());\n    person_imgs = {""Left"" : [], ""Right"": [], ""Center"": []};\n    person_features = {""Left"" : [], ""Right"": [], ""Center"": []};\n    print(""Please start turning slowly. Press \'q\' to save and add this new user to the dataset"");\n    while True:\n        _, frame = vs.read();\n        rects, landmarks = face_detect.detect_face(frame, 80);  # min face size is set to 80x80\n        for (i, rect) in enumerate(rects):\n            aligned_frame, pos = aligner.align(160,frame,landmarks[:,i]);\n            if len(aligned_frame) == 160 and len(aligned_frame[0]) == 160:\n                person_imgs[pos].append(aligned_frame)\n                cv2.imshow(""Captured face"", aligned_frame)\n        key = cv2.waitKey(1) & 0xFF\n        if key == ord(""q""):\n            break\n\n    for pos in person_imgs: #there r some exceptions here, but I\'ll just leave it as this to keep it simple\n        person_features[pos] = [np.mean(extract_feature.get_features(person_imgs[pos]),axis=0).tolist()]\n    data_set[new_name] = person_features;\n    f = open(\'./facerec_128D.txt\', \'w\');\n    f.write(json.dumps(data_set))\n\n\n\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--mode"", type=str, help=""Run camera recognition"", default=""camera"")\n    args = parser.parse_args(sys.argv[1:]);\n    FRGraph = FaceRecGraph();\n    MTCNNGraph = FaceRecGraph();\n    aligner = AlignCustom();\n    extract_feature = FaceFeature(FRGraph)\n    face_detect = MTCNNDetect(MTCNNGraph, scale_factor=2); #scale_factor, rescales image for faster detection\n    main(args);\n'"
mtcnn_detect.py,24,"b'\'\'\'\nTensorflow implementation of the mtcnn face detection algorithm\n\nCredit: DavidSandBerg for implementing this method on tensorflow\n\'\'\'\nfrom six import string_types, iteritems\nimport numpy as np\nimport tensorflow as tf\nimport cv2\nimport os\n\nclass MTCNNDetect(object):\n    def __init__(self, face_rec_graph, model_path = ""models"", threshold = [0.6, 0.7, 0.7], factor = 0.709, scale_factor = 1):\n        \'\'\'\n        :param face_rec_sess: FaceRecSession\n        :param threshold: detection threshold\n        :param factor: default 0.709 image pyramid -- magic number\n        :param model_path:\n        \'\'\'\n        self.threshold = threshold\n        self.factor = factor\n        self.scale_factor = scale_factor;\n        with face_rec_graph.graph.as_default():\n            print(""Loading MTCNN Face detection model"")\n            self.sess = tf.Session()\n            if not model_path:\n                model_path, _ = os.path.split(os.path.realpath(__file__))\n\n            with tf.variable_scope(\'pnet\'):\n                data = tf.placeholder(tf.float32, (None, None, None, 3), \'input\')\n                pnet = PNet({\'data\': data})\n                pnet.load(os.path.join(model_path, \'det1.npy\'), self.sess)\n            with tf.variable_scope(\'rnet\'):\n                data = tf.placeholder(tf.float32, (None, 24, 24, 3), \'input\')\n                rnet = RNet({\'data\': data})\n                rnet.load(os.path.join(model_path, \'det2.npy\'), self.sess)\n            with tf.variable_scope(\'onet\'):\n                data = tf.placeholder(tf.float32, (None, 48, 48, 3), \'input\')\n                onet = ONet({\'data\': data})\n                onet.load(os.path.join(model_path, \'det3.npy\'), self.sess)\n\n            self.pnet = lambda img: self.sess.run((\'pnet/conv4-2/BiasAdd:0\', \'pnet/prob1:0\'), feed_dict={\'pnet/input:0\': img})\n            self.rnet = lambda img: self.sess.run((\'rnet/conv5-2/conv5-2:0\', \'rnet/prob1:0\'), feed_dict={\'rnet/input:0\': img})\n            self.onet = lambda img: self.sess.run((\'onet/conv6-2/conv6-2:0\', \'onet/conv6-3/conv6-3:0\', \'onet/prob1:0\'),\n                                            feed_dict={\'onet/input:0\': img})\n            print(""MTCNN Model loaded"")\n\n\n\n    def detect_face(self, img, minsize):\n        # im: input image\n        # minsize: minimum of faces\' size\n        if(self.scale_factor > 1):\n            img = cv2.resize(img,(int(len(img[0])/self.scale_factor), int(len(img)/self.scale_factor)))\n        factor_count = 0\n        total_boxes = np.empty((0, 9))\n        points = []\n        h = img.shape[0]\n        w = img.shape[1]\n        minl = np.amin([h, w])\n        m = 12.0 / minsize\n        minl = minl * m\n        # creat scale pyramid\n        scales = []\n        while minl >= 12:\n            scales += [m * np.power(self.factor, factor_count)]\n            minl = minl * self.factor\n            factor_count += 1\n\n        # first stage\n        for j in range(len(scales)):\n            scale = scales[j]\n            hs = int(np.ceil(h * scale))\n            ws = int(np.ceil(w * scale))\n            im_data = imresample(img, (hs, ws))\n            im_data = (im_data - 127.5) * 0.0078125\n            img_x = np.expand_dims(im_data, 0)\n            img_y = np.transpose(img_x, (0, 2, 1, 3))\n            out = self.pnet(img_y)\n            out0 = np.transpose(out[0], (0, 2, 1, 3))\n            out1 = np.transpose(out[1], (0, 2, 1, 3))\n\n            boxes, _ = generateBoundingBox(out1[0, :, :, 1].copy(), out0[0, :, :, :].copy(), scale, self.threshold[0])\n\n            # inter-scale nms\n            pick = nms(boxes.copy(), 0.5, \'Union\')\n            if boxes.size > 0 and pick.size > 0:\n                boxes = boxes[pick, :]\n                total_boxes = np.append(total_boxes, boxes, axis=0)\n\n        numbox = total_boxes.shape[0]\n        if numbox > 0:\n            pick = nms(total_boxes.copy(), 0.7, \'Union\')\n            total_boxes = total_boxes[pick, :]\n            regw = total_boxes[:, 2] - total_boxes[:, 0]\n            regh = total_boxes[:, 3] - total_boxes[:, 1]\n            qq1 = total_boxes[:, 0] + total_boxes[:, 5] * regw\n            qq2 = total_boxes[:, 1] + total_boxes[:, 6] * regh\n            qq3 = total_boxes[:, 2] + total_boxes[:, 7] * regw\n            qq4 = total_boxes[:, 3] + total_boxes[:, 8] * regh\n            total_boxes = np.transpose(np.vstack([qq1, qq2, qq3, qq4, total_boxes[:, 4]]))\n            total_boxes = rerec(total_boxes.copy())\n            total_boxes[:, 0:4] = np.fix(total_boxes[:, 0:4]).astype(np.int32)\n            dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(), w, h)\n\n        numbox = total_boxes.shape[0]\n        if numbox > 0:\n            # second stage\n            tempimg = np.zeros((24, 24, 3, numbox))\n            for k in range(0, numbox):\n                tmp = np.zeros((int(tmph[k]), int(tmpw[k]), 3))\n                tmp[dy[k] - 1:edy[k], dx[k] - 1:edx[k], :] = img[y[k] - 1:ey[k], x[k] - 1:ex[k], :]\n                if tmp.shape[0] > 0 and tmp.shape[1] > 0 or tmp.shape[0] == 0 and tmp.shape[1] == 0:\n                    tempimg[:, :, :, k] = imresample(tmp, (24, 24))\n                else:\n                    return np.empty()\n            tempimg = (tempimg - 127.5) * 0.0078125\n            tempimg1 = np.transpose(tempimg, (3, 1, 0, 2))\n            out = self.rnet(tempimg1)\n            out0 = np.transpose(out[0])\n            out1 = np.transpose(out[1])\n            score = out1[1, :]\n            ipass = np.where(score > self.threshold[1])\n            total_boxes = np.hstack([total_boxes[ipass[0], 0:4].copy(), np.expand_dims(score[ipass].copy(), 1)])\n            mv = out0[:, ipass[0]]\n            if total_boxes.shape[0] > 0:\n                pick = nms(total_boxes, 0.7, \'Union\')\n                total_boxes = total_boxes[pick, :]\n                total_boxes = bbreg(total_boxes.copy(), np.transpose(mv[:, pick]))\n                total_boxes = rerec(total_boxes.copy())\n\n        numbox = total_boxes.shape[0]\n        if numbox > 0:\n            # third stage\n            total_boxes = np.fix(total_boxes).astype(np.int32)\n            dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(), w, h)\n            tempimg = np.zeros((48, 48, 3, numbox))\n            for k in range(0, numbox):\n                tmp = np.zeros((int(tmph[k]), int(tmpw[k]), 3))\n                tmp[dy[k] - 1:edy[k], dx[k] - 1:edx[k], :] = img[y[k] - 1:ey[k], x[k] - 1:ex[k], :]\n                if tmp.shape[0] > 0 and tmp.shape[1] > 0 or tmp.shape[0] == 0 and tmp.shape[1] == 0:\n                    tempimg[:, :, :, k] = imresample(tmp, (48, 48))\n                else:\n                    return np.empty()\n            tempimg = (tempimg - 127.5) * 0.0078125\n            tempimg1 = np.transpose(tempimg, (3, 1, 0, 2))\n            out = self.onet(tempimg1)\n            out0 = np.transpose(out[0])\n            out1 = np.transpose(out[1])\n            out2 = np.transpose(out[2])\n            score = out2[1, :]\n            points = out1\n            ipass = np.where(score > self.threshold[2])\n            points = points[:, ipass[0]]\n            total_boxes = np.hstack([total_boxes[ipass[0], 0:4].copy(), np.expand_dims(score[ipass].copy(), 1)])\n            mv = out0[:, ipass[0]]\n\n            w = total_boxes[:, 2] - total_boxes[:, 0] + 1\n            h = total_boxes[:, 3] - total_boxes[:, 1] + 1\n            points[0:5, :] = np.tile(w, (5, 1)) * points[0:5, :] + np.tile(total_boxes[:, 0], (5, 1)) - 1\n            points[5:10, :] = np.tile(h, (5, 1)) * points[5:10, :] + np.tile(total_boxes[:, 1], (5, 1)) - 1\n            if total_boxes.shape[0] > 0:\n                total_boxes = bbreg(total_boxes.copy(), np.transpose(mv))\n                pick = nms(total_boxes.copy(), 0.7, \'Min\')\n                total_boxes = total_boxes[pick, :]\n                points = points[:, pick]\n        # convert to int before return\n        # multiply conf 100 time to return a int\n        total_boxes[:, 4] = total_boxes[:, 4] * 100\n        total_boxes = np.array((total_boxes), dtype=int)\n        points = np.array((points), dtype=int)\n        return total_boxes * self.scale_factor, points * self.scale_factor\n\n\ndef layer(op):\n    \'\'\'Decorator for composable network layers.\'\'\'\n\n    def layer_decorated(self, *args, **kwargs):\n        # Automatically set a name if not provided.\n        name = kwargs.setdefault(\'name\', self.get_unique_name(op.__name__))\n        # Figure out the layer inputs.\n        if len(self.terminals) == 0:\n            raise RuntimeError(\'No input variables found for layer %s.\' % name)\n        elif len(self.terminals) == 1:\n            layer_input = self.terminals[0]\n        else:\n            layer_input = list(self.terminals)\n        # Perform the operation and get the output.\n        layer_output = op(self, layer_input, *args, **kwargs)\n        # Add to layer LUT.\n        self.layers[name] = layer_output\n        # This output is now the input for the next layer.\n        self.feed(layer_output)\n        # Return self for chained calls.\n        return self\n\n    return layer_decorated\n\nclass Network(object):\n\n    def __init__(self, inputs, trainable=True):\n        # The input nodes for this network\n        self.inputs = inputs\n        # The current list of terminal nodes\n        self.terminals = []\n        # Mapping from layer names to layers\n        self.layers = dict(inputs)\n        # If true, the resulting variables are set as trainable\n        self.trainable = trainable\n\n        self.setup()\n\n    def setup(self):\n        \'\'\'Construct the network. \'\'\'\n        raise NotImplementedError(\'Must be implemented by the subclass.\')\n\n    def load(self, data_path, session, ignore_missing=False):\n        \'\'\'Load network weights.\n        data_path: The path to the numpy-serialized network weights\n        session: The current TensorFlow session\n        ignore_missing: If true, serialized weights for missing layers are ignored.\n        \'\'\'\n        data_dict = np.load(data_path, encoding=\'latin1\').item()  # pylint: disable=no-member\n\n        for op_name in data_dict:\n            with tf.variable_scope(op_name, reuse=True):\n                for param_name, data in iteritems(data_dict[op_name]):\n                    try:\n                        var = tf.get_variable(param_name)\n                        session.run(var.assign(data))\n                    except ValueError:\n                        if not ignore_missing:\n                            raise\n\n    def feed(self, *args):\n        \'\'\'Set the input(s) for the next operation by replacing the terminal nodes.\n        The arguments can be either layer names or the actual layers.\n        \'\'\'\n        assert len(args) != 0\n        self.terminals = []\n        for fed_layer in args:\n            if isinstance(fed_layer, string_types):\n                try:\n                    fed_layer = self.layers[fed_layer]\n                except KeyError:\n                    raise KeyError(\'Unknown layer name fed: %s\' % fed_layer)\n            self.terminals.append(fed_layer)\n        return self\n\n    def get_output(self):\n        \'\'\'Returns the current network output.\'\'\'\n        return self.terminals[-1]\n\n    def get_unique_name(self, prefix):\n        \'\'\'Returns an index-suffixed unique name for the given prefix.\n        This is used for auto-generating layer names based on the type-prefix.\n        \'\'\'\n        ident = sum(t.startswith(prefix) for t, _ in self.layers.items()) + 1\n        return \'%s_%d\' % (prefix, ident)\n\n    def make_var(self, name, shape):\n        \'\'\'Creates a new TensorFlow variable.\'\'\'\n        return tf.get_variable(name, shape, trainable=self.trainable)\n\n    def validate_padding(self, padding):\n        \'\'\'Verifies that the padding is one of the supported ones.\'\'\'\n        assert padding in (\'SAME\', \'VALID\')\n\n    @layer\n    def conv(self,\n             inp,\n             k_h,\n             k_w,\n             c_o,\n             s_h,\n             s_w,\n             name,\n             relu=True,\n             padding=\'SAME\',\n             group=1,\n             biased=True):\n        # Verify that the padding is acceptable\n        self.validate_padding(padding)\n        # Get the number of channels in the input\n        c_i = int(inp.get_shape()[-1])\n        # Verify that the grouping parameter is valid\n        assert c_i % group == 0\n        assert c_o % group == 0\n        # Convolution for a given input and kernel\n        convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n        with tf.variable_scope(name) as scope:\n            kernel = self.make_var(\'weights\', shape=[k_h, k_w, c_i // group, c_o])\n            # This is the common-case. Convolve the input without any further complications.\n            output = convolve(inp, kernel)\n            # Add the biases\n            if biased:\n                biases = self.make_var(\'biases\', [c_o])\n                output = tf.nn.bias_add(output, biases)\n            if relu:\n                # ReLU non-linearity\n                output = tf.nn.relu(output, name=scope.name)\n            return output\n\n    @layer\n    def prelu(self, inp, name):\n        with tf.variable_scope(name):\n            i = int(inp.get_shape()[-1])\n            alpha = self.make_var(\'alpha\', shape=(i,))\n            output = tf.nn.relu(inp) + tf.multiply(alpha, -tf.nn.relu(-inp))\n        return output\n\n    @layer\n    def max_pool(self, inp, k_h, k_w, s_h, s_w, name, padding=\'SAME\'):\n        self.validate_padding(padding)\n        return tf.nn.max_pool(inp,\n                              ksize=[1, k_h, k_w, 1],\n                              strides=[1, s_h, s_w, 1],\n                              padding=padding,\n                              name=name)\n\n    @layer\n    def fc(self, inp, num_out, name, relu=True):\n        with tf.variable_scope(name):\n            input_shape = inp.get_shape()\n            if input_shape.ndims == 4:\n                # The input is spatial. Vectorize it first.\n                dim = 1\n                for d in input_shape[1:].as_list():\n                    dim *= int(d)\n                feed_in = tf.reshape(inp, [-1, dim])\n            else:\n                feed_in, dim = (inp, input_shape[-1].value)\n            weights = self.make_var(\'weights\', shape=[dim, num_out])\n            biases = self.make_var(\'biases\', [num_out])\n            op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b\n            fc = op(feed_in, weights, biases, name=name)\n            return fc\n\n    """"""\n    Multi dimensional softmax,\n    refer to https://github.com/tensorflow/tensorflow/issues/210\n    compute softmax along the dimension of target\n    the native softmax only supports batch_size x dimension\n    """"""\n\n    @layer\n    def softmax(self, target, axis, name=None):\n        max_axis = tf.reduce_max(target, axis, keep_dims=True)\n        target_exp = tf.exp(target - max_axis)\n        normalize = tf.reduce_sum(target_exp, axis, keep_dims=True)\n        softmax = tf.div(target_exp, normalize, name)\n        return softmax\n\nclass PNet(Network):\n    def setup(self):\n        (self.feed(\'data\')  # pylint: disable=no-value-for-parameter, no-member\n         .conv(3, 3, 10, 1, 1, padding=\'VALID\', relu=False, name=\'conv1\')\n         .prelu(name=\'PReLU1\')\n         .max_pool(2, 2, 2, 2, name=\'pool1\')\n         .conv(3, 3, 16, 1, 1, padding=\'VALID\', relu=False, name=\'conv2\')\n         .prelu(name=\'PReLU2\')\n         .conv(3, 3, 32, 1, 1, padding=\'VALID\', relu=False, name=\'conv3\')\n         .prelu(name=\'PReLU3\')\n         .conv(1, 1, 2, 1, 1, relu=False, name=\'conv4-1\')\n         .softmax(3, name=\'prob1\'))\n\n        (self.feed(\'PReLU3\')  # pylint: disable=no-value-for-parameter\n         .conv(1, 1, 4, 1, 1, relu=False, name=\'conv4-2\'))\n\nclass RNet(Network):\n    def setup(self):\n        (self.feed(\'data\')  # pylint: disable=no-value-for-parameter, no-member\n         .conv(3, 3, 28, 1, 1, padding=\'VALID\', relu=False, name=\'conv1\')\n         .prelu(name=\'prelu1\')\n         .max_pool(3, 3, 2, 2, name=\'pool1\')\n         .conv(3, 3, 48, 1, 1, padding=\'VALID\', relu=False, name=\'conv2\')\n         .prelu(name=\'prelu2\')\n         .max_pool(3, 3, 2, 2, padding=\'VALID\', name=\'pool2\')\n         .conv(2, 2, 64, 1, 1, padding=\'VALID\', relu=False, name=\'conv3\')\n         .prelu(name=\'prelu3\')\n         .fc(128, relu=False, name=\'conv4\')\n         .prelu(name=\'prelu4\')\n         .fc(2, relu=False, name=\'conv5-1\')\n         .softmax(1, name=\'prob1\'))\n\n        (self.feed(\'prelu4\')  # pylint: disable=no-value-for-parameter\n         .fc(4, relu=False, name=\'conv5-2\'))\n\nclass ONet(Network):\n    def setup(self):\n        (self.feed(\'data\')  # pylint: disable=no-value-for-parameter, no-member\n         .conv(3, 3, 32, 1, 1, padding=\'VALID\', relu=False, name=\'conv1\')\n         .prelu(name=\'prelu1\')\n         .max_pool(3, 3, 2, 2, name=\'pool1\')\n         .conv(3, 3, 64, 1, 1, padding=\'VALID\', relu=False, name=\'conv2\')\n         .prelu(name=\'prelu2\')\n         .max_pool(3, 3, 2, 2, padding=\'VALID\', name=\'pool2\')\n         .conv(3, 3, 64, 1, 1, padding=\'VALID\', relu=False, name=\'conv3\')\n         .prelu(name=\'prelu3\')\n         .max_pool(2, 2, 2, 2, name=\'pool3\')\n         .conv(2, 2, 128, 1, 1, padding=\'VALID\', relu=False, name=\'conv4\')\n         .prelu(name=\'prelu4\')\n         .fc(256, relu=False, name=\'conv5\')\n         .prelu(name=\'prelu5\')\n         .fc(2, relu=False, name=\'conv6-1\')\n         .softmax(1, name=\'prob1\'))\n\n        (self.feed(\'prelu5\')  # pylint: disable=no-value-for-parameter\n         .fc(4, relu=False, name=\'conv6-2\'))\n\n        (self.feed(\'prelu5\')  # pylint: disable=no-value-for-parameter\n         .fc(10, relu=False, name=\'conv6-3\'))\n\n\n# function [boundingbox] = bbreg(boundingbox,reg)\ndef bbreg(boundingbox, reg):\n    # calibrate bounding boxes\n    if reg.shape[1] == 1:\n        reg = np.reshape(reg, (reg.shape[2], reg.shape[3]))\n\n    w = boundingbox[:, 2] - boundingbox[:, 0] + 1\n    h = boundingbox[:, 3] - boundingbox[:, 1] + 1\n    b1 = boundingbox[:, 0] + reg[:, 0] * w\n    b2 = boundingbox[:, 1] + reg[:, 1] * h\n    b3 = boundingbox[:, 2] + reg[:, 2] * w\n    b4 = boundingbox[:, 3] + reg[:, 3] * h\n    boundingbox[:, 0:4] = np.transpose(np.vstack([b1, b2, b3, b4]))\n    return boundingbox\n\ndef generateBoundingBox(imap, reg, scale, t):\n    # use heatmap to generate bounding boxes\n    stride = 2\n    cellsize = 12\n\n    imap = np.transpose(imap)\n    dx1 = np.transpose(reg[:, :, 0])\n    dy1 = np.transpose(reg[:, :, 1])\n    dx2 = np.transpose(reg[:, :, 2])\n    dy2 = np.transpose(reg[:, :, 3])\n    y, x = np.where(imap >= t)\n    if y.shape[0] == 1:\n        dx1 = np.flipud(dx1)\n        dy1 = np.flipud(dy1)\n        dx2 = np.flipud(dx2)\n        dy2 = np.flipud(dy2)\n    score = imap[(y, x)]\n    reg = np.transpose(np.vstack([dx1[(y, x)], dy1[(y, x)], dx2[(y, x)], dy2[(y, x)]]))\n    if reg.size == 0:\n        reg = np.empty((0, 3))\n    bb = np.transpose(np.vstack([y, x]))\n    q1 = np.fix((stride * bb + 1) / scale)\n    q2 = np.fix((stride * bb + cellsize - 1 + 1) / scale)\n    boundingbox = np.hstack([q1, q2, np.expand_dims(score, 1), reg])\n    return boundingbox, reg\n\n# function pick = nms(boxes,threshold,type)\ndef nms(boxes, threshold, method):\n    if boxes.size == 0:\n        return np.empty((0, 3))\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    s = boxes[:, 4]\n    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n    I = np.argsort(s)\n    pick = np.zeros_like(s, dtype=np.int16)\n    counter = 0\n    while I.size > 0:\n        i = I[-1]\n        pick[counter] = i\n        counter += 1\n        idx = I[0:-1]\n        xx1 = np.maximum(x1[i], x1[idx])\n        yy1 = np.maximum(y1[i], y1[idx])\n        xx2 = np.minimum(x2[i], x2[idx])\n        yy2 = np.minimum(y2[i], y2[idx])\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        if method is \'Min\':\n            o = inter / np.minimum(area[i], area[idx])\n        else:\n            o = inter / (area[i] + area[idx] - inter)\n        I = I[np.where(o <= threshold)]\n    pick = pick[0:counter]\n    return pick\n\n# function [dy edy dx edx y ey x ex tmpw tmph] = pad(total_boxes,w,h)\ndef pad(total_boxes, w, h):\n    # compute the padding coordinates (pad the bounding boxes to square)\n    tmpw = (total_boxes[:, 2] - total_boxes[:, 0] + 1).astype(np.int32)\n    tmph = (total_boxes[:, 3] - total_boxes[:, 1] + 1).astype(np.int32)\n    numbox = total_boxes.shape[0]\n\n    dx = np.ones((numbox), dtype=np.int32)\n    dy = np.ones((numbox), dtype=np.int32)\n    edx = tmpw.copy().astype(np.int32)\n    edy = tmph.copy().astype(np.int32)\n\n    x = total_boxes[:, 0].copy().astype(np.int32)\n    y = total_boxes[:, 1].copy().astype(np.int32)\n    ex = total_boxes[:, 2].copy().astype(np.int32)\n    ey = total_boxes[:, 3].copy().astype(np.int32)\n\n    tmp = np.where(ex > w)\n    edx.flat[tmp] = np.expand_dims(-ex[tmp] + w + tmpw[tmp], 1)\n    ex[tmp] = w\n\n    tmp = np.where(ey > h)\n    edy.flat[tmp] = np.expand_dims(-ey[tmp] + h + tmph[tmp], 1)\n    ey[tmp] = h\n\n    tmp = np.where(x < 1)\n    dx.flat[tmp] = np.expand_dims(2 - x[tmp], 1)\n    x[tmp] = 1\n\n    tmp = np.where(y < 1)\n    dy.flat[tmp] = np.expand_dims(2 - y[tmp], 1)\n    y[tmp] = 1\n\n    return dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph\n\n# function [bboxA] = rerec(bboxA)\ndef rerec(bboxA):\n    # convert bboxA to square\n    h = bboxA[:, 3] - bboxA[:, 1]\n    w = bboxA[:, 2] - bboxA[:, 0]\n    l = np.maximum(w, h)\n    bboxA[:, 0] = bboxA[:, 0] + w * 0.5 - l * 0.5\n    bboxA[:, 1] = bboxA[:, 1] + h * 0.5 - l * 0.5\n    bboxA[:, 2:4] = bboxA[:, 0:2] + np.transpose(np.tile(l, (2, 1)))\n    return bboxA\n\ndef imresample(img, sz):\n    im_data = cv2.resize(img, (sz[1], sz[0]), interpolation=cv2.INTER_AREA)  # @UndefinedVariable\n    return im_data\n'"
tf_graph.py,1,"b""'''\nLoad pretrain models and create a tensorflow session to run them\n\n@Author: David Vu\n'''\nimport tensorflow as tf\n\n\nclass FaceRecGraph(object):\n    def __init__(self):\n        '''\n            There'll be more to come in this class\n        '''\n        self.graph = tf.Graph();\n"""
architecture/inception_resnet_v1.py,31,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Contains the definition of the Inception Resnet V1 architecture.\nAs described in http://arxiv.org/abs/1602.07261.\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\n\n# Inception-Renset-A\ndef block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n    """"""Builds the 35x35 resnet block.""""""\n    with tf.variable_scope(scope, \'Block35\', [net], reuse=reuse):\n        with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 32, 1, scope=\'Conv2d_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n            tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n            tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv2_1 = slim.conv2d(tower_conv2_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n            tower_conv2_2 = slim.conv2d(tower_conv2_1, 32, 3, scope=\'Conv2d_0c_3x3\')\n        mixed = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2], 3)\n        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                         activation_fn=None, scope=\'Conv2d_1x1\')\n        net += scale * up\n        if activation_fn:\n            net = activation_fn(net)\n    return net\n\n\n# Inception-Renset-B\ndef block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n    """"""Builds the 17x17 resnet block.""""""\n    with tf.variable_scope(scope, \'Block17\', [net], reuse=reuse):\n        with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 128, 1, scope=\'Conv2d_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n            tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1_0, 128, [1, 7],\n                                        scope=\'Conv2d_0b_1x7\')\n            tower_conv1_2 = slim.conv2d(tower_conv1_1, 128, [7, 1],\n                                        scope=\'Conv2d_0c_7x1\')\n        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                         activation_fn=None, scope=\'Conv2d_1x1\')\n        net += scale * up\n        if activation_fn:\n            net = activation_fn(net)\n    return net\n\n\n# Inception-Resnet-C\ndef block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n    """"""Builds the 8x8 resnet block.""""""\n    with tf.variable_scope(scope, \'Block8\', [net], reuse=reuse):\n        with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n            tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1_0, 192, [1, 3],\n                                        scope=\'Conv2d_0b_1x3\')\n            tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [3, 1],\n                                        scope=\'Conv2d_0c_3x1\')\n        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                         activation_fn=None, scope=\'Conv2d_1x1\')\n        net += scale * up\n        if activation_fn:\n            net = activation_fn(net)\n    return net\n\n\ndef reduction_a(net, k, l, m, n):\n    with tf.variable_scope(\'Branch_0\'):\n        tower_conv = slim.conv2d(net, n, 3, stride=2, padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n    with tf.variable_scope(\'Branch_1\'):\n        tower_conv1_0 = slim.conv2d(net, k, 1, scope=\'Conv2d_0a_1x1\')\n        tower_conv1_1 = slim.conv2d(tower_conv1_0, l, 3,\n                                    scope=\'Conv2d_0b_3x3\')\n        tower_conv1_2 = slim.conv2d(tower_conv1_1, m, 3,\n                                    stride=2, padding=\'VALID\',\n                                    scope=\'Conv2d_1a_3x3\')\n    with tf.variable_scope(\'Branch_2\'):\n        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n    net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\n    return net\n\n\ndef reduction_b(net):\n    with tf.variable_scope(\'Branch_0\'):\n        tower_conv = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n        tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n                                   padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n    with tf.variable_scope(\'Branch_1\'):\n        tower_conv1 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n        tower_conv1_1 = slim.conv2d(tower_conv1, 256, 3, stride=2,\n                                    padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n    with tf.variable_scope(\'Branch_2\'):\n        tower_conv2 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n        tower_conv2_1 = slim.conv2d(tower_conv2, 256, 3,\n                                    scope=\'Conv2d_0b_3x3\')\n        tower_conv2_2 = slim.conv2d(tower_conv2_1, 256, 3, stride=2,\n                                    padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n    with tf.variable_scope(\'Branch_3\'):\n        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n    net = tf.concat([tower_conv_1, tower_conv1_1,\n                     tower_conv2_2, tower_pool], 3)\n    return net\n\n\ndef inference(images, keep_probability, phase_train=True,\n              bottleneck_layer_size=128, weight_decay=0.0, reuse=None):\n    batch_norm_params = {\n        # Decay for the moving averages.\n        \'decay\': 0.995,\n        # epsilon to prevent 0s in variance.\n        \'epsilon\': 0.001,\n        # force in-place updates of mean and variance estimates\n        \'updates_collections\': None,\n        # Moving averages ends up in the trainable variables collection\n        \'variables_collections\': [tf.GraphKeys.TRAINABLE_VARIABLES],\n    }\n\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                        weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n                        weights_regularizer=slim.l2_regularizer(weight_decay),\n                        normalizer_fn=slim.batch_norm,\n                        normalizer_params=batch_norm_params):\n        return inception_resnet_v1(images, is_training=phase_train,\n                                   dropout_keep_prob=keep_probability, bottleneck_layer_size=bottleneck_layer_size,\n                                   reuse=reuse)\n\n\ndef inception_resnet_v1(inputs, is_training=True,\n                        dropout_keep_prob=0.8,\n                        bottleneck_layer_size=128,\n                        reuse=None,\n                        scope=\'InceptionResnetV1\'):\n    """"""Creates the Inception Resnet V1 model.\n    Args:\n      inputs: a 4-D tensor of size [batch_size, height, width, 3].\n      num_classes: number of predicted classes.\n      is_training: whether is training or not.\n      dropout_keep_prob: float, the fraction to keep before final layer.\n      reuse: whether or not the network and its variables should be reused. To be\n        able to reuse \'scope\' must be given.\n      scope: Optional variable_scope.\n    Returns:\n      logits: the logits outputs of the model.\n      end_points: the set of end_points from the inception model.\n    """"""\n    end_points = {}\n\n    with tf.variable_scope(scope, \'InceptionResnetV1\', [inputs], reuse=reuse):\n        with slim.arg_scope([slim.batch_norm, slim.dropout],\n                            is_training=is_training):\n            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                                stride=1, padding=\'SAME\'):\n                # 149 x 149 x 32\n                net = slim.conv2d(inputs, 32, 3, stride=2, padding=\'VALID\',\n                                  scope=\'Conv2d_1a_3x3\')\n                end_points[\'Conv2d_1a_3x3\'] = net\n                # 147 x 147 x 32\n                net = slim.conv2d(net, 32, 3, padding=\'VALID\',\n                                  scope=\'Conv2d_2a_3x3\')\n                end_points[\'Conv2d_2a_3x3\'] = net\n                # 147 x 147 x 64\n                net = slim.conv2d(net, 64, 3, scope=\'Conv2d_2b_3x3\')\n                end_points[\'Conv2d_2b_3x3\'] = net\n                # 73 x 73 x 64\n                net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                      scope=\'MaxPool_3a_3x3\')\n                end_points[\'MaxPool_3a_3x3\'] = net\n                # 73 x 73 x 80\n                net = slim.conv2d(net, 80, 1, padding=\'VALID\',\n                                  scope=\'Conv2d_3b_1x1\')\n                end_points[\'Conv2d_3b_1x1\'] = net\n                # 71 x 71 x 192\n                net = slim.conv2d(net, 192, 3, padding=\'VALID\',\n                                  scope=\'Conv2d_4a_3x3\')\n                end_points[\'Conv2d_4a_3x3\'] = net\n                # 35 x 35 x 256\n                net = slim.conv2d(net, 256, 3, stride=2, padding=\'VALID\',\n                                  scope=\'Conv2d_4b_3x3\')\n                end_points[\'Conv2d_4b_3x3\'] = net\n\n                # 5 x Inception-resnet-A\n                net = slim.repeat(net, 5, block35, scale=0.17)\n\n                # Reduction-A\n                with tf.variable_scope(\'Mixed_6a\'):\n                    net = reduction_a(net, 192, 192, 256, 384)\n                end_points[\'Mixed_6a\'] = net\n\n                # 10 x Inception-Resnet-B\n                net = slim.repeat(net, 10, block17, scale=0.10)\n\n                # Reduction-B\n                with tf.variable_scope(\'Mixed_7a\'):\n                    net = reduction_b(net)\n                end_points[\'Mixed_7a\'] = net\n\n                # 5 x Inception-Resnet-C\n                net = slim.repeat(net, 5, block8, scale=0.20)\n                net = block8(net, activation_fn=None)\n\n                with tf.variable_scope(\'Logits\'):\n                    end_points[\'PrePool\'] = net\n                    # pylint: disable=no-member\n                    net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n                                          scope=\'AvgPool_1a_8x8\')\n                    net = slim.flatten(net)\n\n                    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                                       scope=\'Dropout\')\n\n                    end_points[\'PreLogitsFlatten\'] = net\n\n                net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None,\n                                           scope=\'Bottleneck\', reuse=False)\n\n    return net, end_points\n'"
