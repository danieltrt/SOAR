file_path,api_count,code
cnn_lstm_otc_ocr.py,58,"b'""""""\n\n""""""\n\nimport tensorflow as tf\nimport utils\n\nFLAGS = utils.FLAGS\nnum_classes = utils.num_classes\n\n\nclass LSTMOCR(object):\n    def __init__(self, mode):\n        self.mode = mode\n        # image\n        self.inputs = tf.placeholder(tf.float32, [None, FLAGS.image_height, FLAGS.image_width, FLAGS.image_channel])\n        # SparseTensor required by ctc_loss op\n        self.labels = tf.sparse_placeholder(tf.int32)\n        # 1d array of size [batch_size]\n        # self.seq_len = tf.placeholder(tf.int32, [None])\n        # l2\n        self._extra_train_ops = []\n\n    def build_graph(self):\n        self._build_model()\n        self._build_train_op()\n\n        self.merged_summay = tf.summary.merge_all()\n\n    def _build_model(self):\n        filters = [1, 64, 128, 128, FLAGS.out_channels]\n        strides = [1, 2]\n\n        feature_h = FLAGS.image_height\n        feature_w = FLAGS.image_width\n\n        count_ = 0\n        min_size = min(FLAGS.image_height, FLAGS.image_width)\n        while min_size > 1:\n            min_size = (min_size + 1) // 2\n            count_ += 1\n        assert (FLAGS.cnn_count <= count_, ""FLAGS.cnn_count should be <= {}!"".format(count_))\n\n        # CNN part\n        with tf.variable_scope(\'cnn\'):\n            x = self.inputs\n            for i in range(FLAGS.cnn_count):\n                with tf.variable_scope(\'unit-%d\' % (i + 1)):\n                    x = self._conv2d(x, \'cnn-%d\' % (i + 1), 3, filters[i], filters[i + 1], strides[0])\n                    x = self._batch_norm(\'bn%d\' % (i + 1), x)\n                    x = self._leaky_relu(x, FLAGS.leakiness)\n                    x = self._max_pool(x, 2, strides[1])\n\n                    # print(\'----x.get_shape().as_list(): {}\'.format(x.get_shape().as_list()))\n                    _, feature_h, feature_w, _ = x.get_shape().as_list()\n            print(\'\\nfeature_h: {}, feature_w: {}\'.format(feature_h, feature_w))\n\n        # LSTM part\n        with tf.variable_scope(\'lstm\'):\n            x = tf.transpose(x, [0, 2, 1, 3])  # [batch_size, feature_w, feature_h, FLAGS.out_channels]\n            # treat `feature_w` as max_timestep in lstm.\n            x = tf.reshape(x, [FLAGS.batch_size, feature_w, feature_h * FLAGS.out_channels])\n            print(\'lstm input shape: {}\'.format(x.get_shape().as_list()))\n            self.seq_len = tf.fill([x.get_shape().as_list()[0]], feature_w)\n            # print(\'self.seq_len.shape: {}\'.format(self.seq_len.shape.as_list()))\n\n            # tf.nn.rnn_cell.RNNCell, tf.nn.rnn_cell.GRUCell\n            cell = tf.nn.rnn_cell.LSTMCell(FLAGS.num_hidden, state_is_tuple=True)\n            if self.mode == \'train\':\n                cell = tf.nn.rnn_cell.DropoutWrapper(cell=cell, output_keep_prob=FLAGS.output_keep_prob)\n\n            cell1 = tf.nn.rnn_cell.LSTMCell(FLAGS.num_hidden, state_is_tuple=True)\n            if self.mode == \'train\':\n                cell1 = tf.nn.rnn_cell.DropoutWrapper(cell=cell1, output_keep_prob=FLAGS.output_keep_prob)\n\n            # Stacking rnn cells\n            stack = tf.nn.rnn_cell.MultiRNNCell([cell, cell1], state_is_tuple=True)\n            initial_state = stack.zero_state(FLAGS.batch_size, dtype=tf.float32)\n\n            # The second output is the last state and we will not use that\n            outputs, _ = tf.nn.dynamic_rnn(\n                cell=stack,\n                inputs=x,\n                sequence_length=self.seq_len,\n                initial_state=initial_state,\n                dtype=tf.float32,\n                time_major=False\n            )  # [batch_size, max_stepsize, FLAGS.num_hidden]\n\n            # Reshaping to apply the same weights over the timesteps\n            outputs = tf.reshape(outputs, [-1, FLAGS.num_hidden])  # [batch_size * max_stepsize, FLAGS.num_hidden]\n\n            W = tf.get_variable(name=\'W_out\',\n                                shape=[FLAGS.num_hidden, num_classes],\n                                dtype=tf.float32,\n                                initializer=tf.glorot_uniform_initializer())  # tf.glorot_normal_initializer\n            b = tf.get_variable(name=\'b_out\',\n                                shape=[num_classes],\n                                dtype=tf.float32,\n                                initializer=tf.constant_initializer())\n\n            self.logits = tf.matmul(outputs, W) + b\n            # Reshaping back to the original shape\n            shape = tf.shape(x)\n            self.logits = tf.reshape(self.logits, [shape[0], -1, num_classes])\n            # Time major\n            self.logits = tf.transpose(self.logits, (1, 0, 2))\n\n    def _build_train_op(self):\n        # self.global_step = tf.Variable(0, trainable=False)\n        self.global_step = tf.train.get_or_create_global_step()\n\n        self.loss = tf.nn.ctc_loss(labels=self.labels,\n                                   inputs=self.logits,\n                                   sequence_length=self.seq_len)\n        self.cost = tf.reduce_mean(self.loss)\n        tf.summary.scalar(\'cost\', self.cost)\n\n        self.lrn_rate = tf.train.exponential_decay(FLAGS.initial_learning_rate,\n                                                   self.global_step,\n                                                   FLAGS.decay_steps,\n                                                   FLAGS.decay_rate,\n                                                   staircase=True)\n        tf.summary.scalar(\'learning_rate\', self.lrn_rate)\n\n        # self.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.lrn_rate,\n        #                                            momentum=FLAGS.momentum).minimize(self.cost,\n        #                                                                              global_step=self.global_step)\n        # self.optimizer = tf.train.MomentumOptimizer(learning_rate=self.lrn_rate,\n        #                                             momentum=FLAGS.momentum,\n        #                                             use_nesterov=True).minimize(self.cost,\n        #                                                                         global_step=self.global_step)\n        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lrn_rate,\n                                                beta1=FLAGS.beta1,\n                                                beta2=FLAGS.beta2).minimize(self.loss,\n                                                                            global_step=self.global_step)\n        train_ops = [self.optimizer] + self._extra_train_ops\n        self.train_op = tf.group(*train_ops)\n\n        # Option 2: tf.nn.ctc_beam_search_decoder\n        # (it\'s slower but you\'ll get better results)\n        self.decoded, self.log_prob = \\\n            tf.nn.ctc_beam_search_decoder(self.logits,\n                                          self.seq_len,\n                                          merge_repeated=False)\n        # self.decoded, self.log_prob = tf.nn.ctc_greedy_decoder(logits, seq_len,merge_repeated=False)\n        self.dense_decoded = tf.sparse_tensor_to_dense(self.decoded[0], default_value=-1)\n\n    def _conv2d(self, x, name, filter_size, in_channels, out_channels, strides):\n        with tf.variable_scope(name):\n            kernel = tf.get_variable(name=\'W\',\n                                     shape=[filter_size, filter_size, in_channels, out_channels],\n                                     dtype=tf.float32,\n                                     initializer=tf.glorot_uniform_initializer())  # tf.glorot_normal_initializer\n\n            b = tf.get_variable(name=\'b\',\n                                shape=[out_channels],\n                                dtype=tf.float32,\n                                initializer=tf.constant_initializer())\n\n            con2d_op = tf.nn.conv2d(x, kernel, [1, strides, strides, 1], padding=\'SAME\')\n\n        return tf.nn.bias_add(con2d_op, b)\n\n    def _batch_norm(self, name, x):\n        """"""Batch normalization.""""""\n        with tf.variable_scope(name):\n            x_bn = \\\n                tf.contrib.layers.batch_norm(\n                    inputs=x,\n                    decay=0.9,\n                    center=True,\n                    scale=True,\n                    epsilon=1e-5,\n                    updates_collections=None,\n                    is_training=self.mode == \'train\',\n                    fused=True,\n                    data_format=\'NHWC\',\n                    zero_debias_moving_mean=True,\n                    scope=\'BatchNorm\'\n                )\n\n        return x_bn\n\n    def _leaky_relu(self, x, leakiness=0.0):\n        return tf.where(tf.less(x, 0.0), leakiness * x, x, name=\'leaky_relu\')\n\n    def _max_pool(self, x, ksize, strides):\n        return tf.nn.max_pool(x,\n                              ksize=[1, ksize, ksize, 1],\n                              strides=[1, strides, strides, 1],\n                              padding=\'SAME\',\n                              name=\'max_pool\')\n'"
helper.py,0,"b'import numpy as np\nimport os\nimport shutil\n\n\ndef split_train_val(X, y, train_size):\n    """"""Split dataset for training and validation.\n\n    Args:\n        X: A 1-D numpy array containing pathes of images.\n        y: A 1-D numpy array containing labels.\n        train_size: Size of training data to split.\n    Returns:\n        1-D numpy array having the same definition with X and y.\n    """"""\n\n    total_size = len(X)\n    # shuffle data\n    shuffle_indices = np.random.permutation(np.arange(total_size))\n    X = X[shuffle_indices]\n    y = y[shuffle_indices]\n\n    # split training data\n    train_indices = np.random.choice(total_size, train_size, replace=False)\n    X_train = X[train_indices]\n    y_train = y[train_indices]\n\n    # split validation data\n    val_indices = [i for i in range(total_size) if i not in train_indices]\n    X_val = X[val_indices]\n    y_val = y[val_indices]\n\n    return X_train, y_train, X_val, y_val\n\n\ndef write_to_file(data, file_to_output):\n    """"""Write X_train/y_train/X_val/y_val/X_infer to file for further\n       processing (e.g. make input queue of tensorflow).\n\n    Args:\n        data: A 1-D numpy array, e.g, X_train/y_train/X_val/y_val/X_infer.\n        file_to_output: A file to store data.\n    """"""\n    # with open(\'X_train.csv\',\'a\') as f_handle:\n    #     np.savetxt(f_handle, X_train, fmt=\'%s\', delimiter="","")\n\n    with open(file_to_output, \'w\') as f:\n        for item in data.tolist():\n            f.write(item + \'\\n\')\n\n\ndef load_labels(file):\n    labels = list(open(file).readlines())\n    labels = [s.strip() for s in labels]\n    labels = [s.split() for s in labels]\n\n    labels_dict = dict(labels)\n\n    labels = np.asarray(labels, dtype=str)\n    labels = labels[:, 0]\n\n    return labels, labels_dict\n\n\ndef load_img_path(images_path):\n    tmp = os.listdir(images_path)\n    tmp.sort(key=lambda x: int(x.split(\'.\')[0]))\n\n    file_names = [images_path + s for s in tmp]\n\n    file_names = np.asarray(file_names)\n\n    return file_names\n\n\ndef load_data(file_to_read):\n    """"""Load X_train/y_train/X_val/y_val/X_infer for further\n       processing (e.g. make input queue of tensorflow).\n\n    Args:\n        file_to_read:\n    Returns:\n        X_train/y_train/X_val/y_val/X_infer.\n    """"""\n\n    data = np.recfromtxt(file_to_read)\n    data = np.asarray(data)\n\n    return data\n\n\ndef cp_file(imgs_list_para, labels_list_para, dst_para):\n    for i in range(imgs_list_para.shape[0]):\n        file_path = imgs_list_para[i]\n\n        filename = os.path.basename(file_path)\n        fn = filename.split(\'.\')[0]\n        ext = filename.split(\'.\')[1]\n\n        dest_filename = dst_para + fn + \'_\' + labels_list_para[i] + \'.\' + ext\n\n        shutil.copyfile(file_path, dest_filename)\n\n\nif __name__ == \'__main__\':\n    labels_path = \'./imgs/labels.txt\'\n    labels, labels_dict = load_labels(labels_path)\n    # print(labels)\n\n    images_path = \'./imgs/image_contest_level_1/\'\n    image_path_list = load_img_path(images_path)\n    # print(image_path_list[:10])\n\n    X_train, y_train, X_val, y_val = split_train_val(image_path_list, labels, 80000)\n    write_to_file(X_train, ""./imgs/X_train.txt"")\n    write_to_file(y_train, ""./imgs/y_train.txt"")\n    write_to_file(X_val, ""./imgs/X_val.txt"")\n    write_to_file(y_val, ""./imgs/y_val.txt"")\n\n    cp_file(X_train, y_train, \'./imgs/train/\')\n    cp_file(X_val, y_val, \'./imgs/val/\')\n'"
main.py,14,"b'""""""\n\n""""""\n\nimport datetime\nimport logging\nimport os\nimport time\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\n\nimport cnn_lstm_otc_ocr\nimport utils\nimport helper\n\nFLAGS = utils.FLAGS\n\nlogger = logging.getLogger(\'Traing for OCR using CNN+LSTM+CTC\')\nlogger.setLevel(logging.INFO)\n\n\ndef train(train_dir=None, val_dir=None, mode=\'train\'):\n    model = cnn_lstm_otc_ocr.LSTMOCR(mode)\n    model.build_graph()\n\n    print(\'loading train data\')\n    train_feeder = utils.DataIterator(data_dir=train_dir)\n    print(\'size: \', train_feeder.size)\n\n    print(\'loading validation data\')\n    val_feeder = utils.DataIterator(data_dir=val_dir)\n    print(\'size: {}\\n\'.format(val_feeder.size))\n\n    num_train_samples = train_feeder.size  # 100000\n    num_batches_per_epoch = int(num_train_samples / FLAGS.batch_size)  # example: 100000/100\n\n    num_val_samples = val_feeder.size\n    num_batches_per_epoch_val = int(num_val_samples / FLAGS.batch_size)  # example: 10000/100\n    shuffle_idx_val = np.random.permutation(num_val_samples)\n\n    config = tf.ConfigProto(allow_soft_placement=True)\n    config.gpu_options.allow_growth = True\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n\n        saver = tf.train.Saver(tf.global_variables(), max_to_keep=100)\n        train_writer = tf.summary.FileWriter(FLAGS.log_dir + \'/train\', sess.graph)\n        if FLAGS.restore:\n            ckpt = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n            if ckpt:\n                # the global_step will restore sa well\n                saver.restore(sess, ckpt)\n                print(\'restore from checkpoint{0}\'.format(ckpt))\n\n        print(\'=============================begin training=============================\')\n        for cur_epoch in range(FLAGS.num_epochs):\n            shuffle_idx = np.random.permutation(num_train_samples)\n            train_cost = 0\n            start_time = time.time()\n            batch_time = time.time()\n\n            # the training part\n            for cur_batch in range(num_batches_per_epoch):\n                if (cur_batch + 1) % 100 == 0:\n                    print(\'batch\', cur_batch, \': time\', time.time() - batch_time)\n                batch_time = time.time()\n                indexs = [shuffle_idx[i % num_train_samples] for i in\n                          range(cur_batch * FLAGS.batch_size, (cur_batch + 1) * FLAGS.batch_size)]\n                batch_inputs, _, batch_labels = \\\n                    train_feeder.input_index_generate_batch(indexs)\n                # batch_inputs,batch_seq_len,batch_labels=utils.gen_batch(FLAGS.batch_size)\n                feed = {model.inputs: batch_inputs,\n                        model.labels: batch_labels}\n\n                # if summary is needed\n                summary_str, batch_cost, step, _ = \\\n                    sess.run([model.merged_summay, model.cost, model.global_step, model.train_op], feed)\n                # calculate the cost\n                train_cost += batch_cost * FLAGS.batch_size\n\n                train_writer.add_summary(summary_str, step)\n\n                # save the checkpoint\n                if step % FLAGS.save_steps == 1:\n                    if not os.path.isdir(FLAGS.checkpoint_dir):\n                        os.mkdir(FLAGS.checkpoint_dir)\n                    logger.info(\'save checkpoint at step {0}\', format(step))\n                    saver.save(sess, os.path.join(FLAGS.checkpoint_dir, \'ocr-model\'), global_step=step)\n\n                # train_err += the_err * FLAGS.batch_size\n                # do validation\n                if step % FLAGS.validation_steps == 0:\n                    acc_batch_total = 0\n                    lastbatch_err = 0\n                    lr = 0\n                    for j in range(num_batches_per_epoch_val):\n                        indexs_val = [shuffle_idx_val[i % num_val_samples] for i in\n                                      range(j * FLAGS.batch_size, (j + 1) * FLAGS.batch_size)]\n                        val_inputs, _, val_labels = \\\n                            val_feeder.input_index_generate_batch(indexs_val)\n                        val_feed = {model.inputs: val_inputs,\n                                    model.labels: val_labels}\n\n                        dense_decoded, lastbatch_err, lr = \\\n                            sess.run([model.dense_decoded, model.cost, model.lrn_rate],\n                                     val_feed)\n\n                        # print the decode result\n                        ori_labels = val_feeder.the_label(indexs_val)\n                        acc = utils.accuracy_calculation(ori_labels, dense_decoded,\n                                                         ignore_value=-1, isPrint=True)\n                        acc_batch_total += acc\n\n                    accuracy = (acc_batch_total * FLAGS.batch_size) / num_val_samples\n\n                    avg_train_cost = train_cost / ((cur_batch + 1) * FLAGS.batch_size)\n\n                    # train_err /= num_train_samples\n                    now = datetime.datetime.now()\n                    log = ""{}/{} {}:{}:{} Epoch {}/{}, "" \\\n                          ""accuracy = {:.3f},avg_train_cost = {:.3f}, "" \\\n                          ""lastbatch_err = {:.3f}, time = {:.3f},lr={:.8f}""\n                    print(log.format(now.month, now.day, now.hour, now.minute, now.second,\n                                     cur_epoch + 1, FLAGS.num_epochs, accuracy, avg_train_cost,\n                                     lastbatch_err, time.time() - start_time, lr))\n\n\ndef infer(img_path, mode=\'infer\'):\n    # imgList = load_img_path(\'/home/yang/Downloads/FILE/ml/imgs/image_contest_level_1_validate/\')\n    imgList = helper.load_img_path(img_path)\n    print(imgList[:5])\n\n    model = cnn_lstm_otc_ocr.LSTMOCR(mode)\n    model.build_graph()\n\n    total_steps = len(imgList) / FLAGS.batch_size\n\n    config = tf.ConfigProto(allow_soft_placement=True)\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n\n        saver = tf.train.Saver(tf.global_variables(), max_to_keep=100)\n        ckpt = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n        if ckpt:\n            saver.restore(sess, ckpt)\n            print(\'restore from ckpt{}\'.format(ckpt))\n        else:\n            print(\'cannot restore\')\n\n        decoded_expression = []\n        for curr_step in range(total_steps):\n\n            imgs_input = []\n            seq_len_input = []\n            for img in imgList[curr_step * FLAGS.batch_size: (curr_step + 1) * FLAGS.batch_size]:\n                im = cv2.imread(img, 0).astype(np.float32) / 255.\n                im = np.reshape(im, [FLAGS.image_height, FLAGS.image_width, FLAGS.image_channel])\n\n                def get_input_lens(seqs):\n                    length = np.array([FLAGS.max_stepsize for _ in seqs], dtype=np.int64)\n\n                    return seqs, length\n\n                inp, seq_len = get_input_lens(np.array([im]))\n                imgs_input.append(im)\n                seq_len_input.append(seq_len)\n\n            imgs_input = np.asarray(imgs_input)\n            seq_len_input = np.asarray(seq_len_input)\n            seq_len_input = np.reshape(seq_len_input, [-1])\n\n            feed = {model.inputs: imgs_input}\n            dense_decoded_code = sess.run(model.dense_decoded, feed)\n\n            for item in dense_decoded_code:\n                expression = \'\'\n\n                for i in item:\n                    if i == -1:\n                        expression += \'\'\n                    else:\n                        expression += utils.decode_maps[i]\n\n                decoded_expression.append(expression)\n\n        with open(\'./result.txt\', \'a\') as f:\n            for code in decoded_expression:\n                f.write(code + \'\\n\')\n\n\ndef main(_):\n    if FLAGS.num_gpus == 0:\n        dev = \'/cpu:0\'\n    elif FLAGS.num_gpus == 1:\n        dev = \'/gpu:0\'\n    else:\n        raise ValueError(\'Only support 0 or 1 gpu.\')\n\n    with tf.device(dev):\n        if FLAGS.mode == \'train\':\n            train(FLAGS.train_dir, FLAGS.val_dir, FLAGS.mode)\n\n        elif FLAGS.mode == \'infer\':\n            infer(FLAGS.infer_dir, FLAGS.mode)\n\n\nif __name__ == \'__main__\':\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run()\n'"
utils.py,27,"b'""""""\n\n""""""\n\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport cv2\n\n# +-* + () + 10 digit + blank + space\nnum_classes = 3 + 2 + 10 + 1 + 1\n\nmaxPrintLen = 100\n\ntf.app.flags.DEFINE_boolean(\'restore\', False, \'whether to restore from the latest checkpoint\')\ntf.app.flags.DEFINE_string(\'checkpoint_dir\', \'./checkpoint/\', \'the checkpoint dir\')\ntf.app.flags.DEFINE_float(\'initial_learning_rate\', 1e-3, \'inital lr\')\n\ntf.app.flags.DEFINE_integer(\'image_height\', 60, \'image height\')\ntf.app.flags.DEFINE_integer(\'image_width\', 180, \'image width\')\ntf.app.flags.DEFINE_integer(\'image_channel\', 1, \'image channels as input\')\n\ntf.app.flags.DEFINE_integer(\'cnn_count\', 4, \'count of cnn module to extract image features.\')\ntf.app.flags.DEFINE_integer(\'out_channels\', 64, \'output channels of last layer in CNN\')\ntf.app.flags.DEFINE_integer(\'num_hidden\', 128, \'number of hidden units in lstm\')\ntf.app.flags.DEFINE_float(\'output_keep_prob\', 0.8, \'output_keep_prob in lstm\')\ntf.app.flags.DEFINE_integer(\'num_epochs\', 10000, \'maximum epochs\')\ntf.app.flags.DEFINE_integer(\'batch_size\', 40, \'the batch_size\')\ntf.app.flags.DEFINE_integer(\'save_steps\', 1000, \'the step to save checkpoint\')\ntf.app.flags.DEFINE_float(\'leakiness\', 0.01, \'leakiness of lrelu\')\ntf.app.flags.DEFINE_integer(\'validation_steps\', 500, \'the step to validation\')\n\ntf.app.flags.DEFINE_float(\'decay_rate\', 0.98, \'the lr decay rate\')\ntf.app.flags.DEFINE_float(\'beta1\', 0.9, \'parameter of adam optimizer beta1\')\ntf.app.flags.DEFINE_float(\'beta2\', 0.999, \'adam parameter beta2\')\n\ntf.app.flags.DEFINE_integer(\'decay_steps\', 10000, \'the lr decay_step for optimizer\')\ntf.app.flags.DEFINE_float(\'momentum\', 0.9, \'the momentum\')\n\ntf.app.flags.DEFINE_string(\'train_dir\', \'./imgs/train/\', \'the train data dir\')\ntf.app.flags.DEFINE_string(\'val_dir\', \'./imgs/val/\', \'the val data dir\')\ntf.app.flags.DEFINE_string(\'infer_dir\', \'./imgs/infer/\', \'the infer data dir\')\ntf.app.flags.DEFINE_string(\'log_dir\', \'./log\', \'the logging dir\')\ntf.app.flags.DEFINE_string(\'mode\', \'train\', \'train, val or infer\')\ntf.app.flags.DEFINE_integer(\'num_gpus\', 0, \'num of gpus\')\n\nFLAGS = tf.app.flags.FLAGS\n\n# num_batches_per_epoch = int(num_train_samples/FLAGS.batch_size)\n\ncharset = \'0123456789+-*()\'\nencode_maps = {}\ndecode_maps = {}\nfor i, char in enumerate(charset, 1):\n    encode_maps[char] = i\n    decode_maps[i] = char\n\nSPACE_INDEX = 0\nSPACE_TOKEN = \'\'\nencode_maps[SPACE_TOKEN] = SPACE_INDEX\ndecode_maps[SPACE_INDEX] = SPACE_TOKEN\n\n\nclass DataIterator:\n    def __init__(self, data_dir):\n        self.image = []\n        self.labels = []\n        for root, sub_folder, file_list in os.walk(data_dir):\n            for file_path in file_list:\n                image_name = os.path.join(root, file_path)\n                im = cv2.imread(image_name, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255.\n                # resize to same height, different width will consume time on padding\n                # im = cv2.resize(im, (image_width, image_height))\n                im = np.reshape(im, [FLAGS.image_height, FLAGS.image_width, FLAGS.image_channel])\n                self.image.append(im)\n\n                # image is named as /.../<folder>/00000_abcd.png\n                code = image_name.split(\'/\')[-1].split(\'_\')[1].split(\'.\')[0]\n                code = [SPACE_INDEX if code == SPACE_TOKEN else encode_maps[c] for c in list(code)]\n                self.labels.append(code)\n\n    @property\n    def size(self):\n        return len(self.labels)\n\n    def the_label(self, indexs):\n        labels = []\n        for i in indexs:\n            labels.append(self.labels[i])\n\n        return labels\n\n    def input_index_generate_batch(self, index=None):\n        if index:\n            image_batch = [self.image[i] for i in index]\n            label_batch = [self.labels[i] for i in index]\n        else:\n            image_batch = self.image\n            label_batch = self.labels\n\n        def get_input_lens(sequences):\n            # 64 is the output channels of the last layer of CNN\n            lengths = np.asarray([FLAGS.out_channels for _ in sequences], dtype=np.int64)\n\n            return sequences, lengths\n\n        batch_inputs, batch_seq_len = get_input_lens(np.array(image_batch))\n        batch_labels = sparse_tuple_from_label(label_batch)\n\n        return batch_inputs, batch_seq_len, batch_labels\n\n\ndef accuracy_calculation(original_seq, decoded_seq, ignore_value=-1, isPrint=False):\n    if len(original_seq) != len(decoded_seq):\n        print(\'original lengths is different from the decoded_seq, please check again\')\n        return 0\n    count = 0\n    for i, origin_label in enumerate(original_seq):\n        decoded_label = [j for j in decoded_seq[i] if j != ignore_value]\n        if isPrint and i < maxPrintLen:\n            # print(\'seq{0:4d}: origin: {1} decoded:{2}\'.format(i, origin_label, decoded_label))\n\n            with open(\'./test.csv\', \'w\') as f:\n                f.write(str(origin_label) + \'\\t\' + str(decoded_label))\n                f.write(\'\\n\')\n\n        if origin_label == decoded_label:\n            count += 1\n\n    return count * 1.0 / len(original_seq)\n\n\ndef sparse_tuple_from_label(sequences, dtype=np.int32):\n    """"""Create a sparse representention of x.\n    Args:\n        sequences: a list of lists of type dtype where each element is a sequence\n    Returns:\n        A tuple with (indices, values, shape)\n    """"""\n    indices = []\n    values = []\n\n    for n, seq in enumerate(sequences):\n        indices.extend(zip([n] * len(seq), range(len(seq))))\n        values.extend(seq)\n\n    indices = np.asarray(indices, dtype=np.int64)\n    values = np.asarray(values, dtype=dtype)\n    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1] + 1], dtype=np.int64)\n\n    return indices, values, shape\n\n\ndef eval_expression(encoded_list):\n    """"""\n    :param encoded_list:\n    :return:\n    """"""\n\n    eval_rs = []\n    for item in encoded_list:\n        try:\n            rs = str(eval(item))\n            eval_rs.append(rs)\n        except:\n            eval_rs.append(item)\n            continue\n\n    with open(\'./result.txt\') as f:\n        for ith in range(len(encoded_list)):\n            f.write(encoded_list[ith] + \' \' + eval_rs[ith] + \'\\n\')\n\n    return eval_rs\n'"
