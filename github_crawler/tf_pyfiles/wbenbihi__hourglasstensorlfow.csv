file_path,api_count,code
config.py,0,"b""import os\r\n\r\n#\r\n# path and dataset parameter\r\n#\r\n\r\nDATA_PATH = 'data'\r\n\r\nPASCAL_PATH = os.path.join(DATA_PATH, 'pascal_voc')\r\n\r\nCACHE_PATH = os.path.join(PASCAL_PATH, 'cache')\r\n\r\nOUTPUT_DIR = os.path.join(PASCAL_PATH, 'output')\r\n\r\nWEIGHTS_DIR = os.path.join(PASCAL_PATH, 'weights')\r\n\r\nWEIGHTS_FILE = None\r\n# WEIGHTS_FILE = os.path.join(DATA_PATH, 'weights', 'YOLO_small.ckpt')\r\n\r\nCLASSES = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\r\n           'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse',\r\n           'motorbike', 'person', 'pottedplant', 'sheep', 'sofa',\r\n           'train', 'tvmonitor']\r\n\r\nFLIPPED = True\r\n\r\n\r\n#\r\n# model parameter\r\n#\r\n\r\nIMAGE_SIZE = 448\r\n\r\nCELL_SIZE = 7\r\n\r\nBOXES_PER_CELL = 2\r\n\r\nALPHA = 0.1\r\n\r\nDISP_CONSOLE = False\r\n\r\nOBJECT_SCALE = 1.0\r\nNOOBJECT_SCALE = 1.0\r\nCLASS_SCALE = 2.0\r\nCOORD_SCALE = 5.0\r\n\r\n\r\n#\r\n# solver parameter\r\n#\r\n\r\nGPU = ''\r\n\r\nLEARNING_RATE = 0.0001\r\n\r\nDECAY_STEPS = 30000\r\n\r\nDECAY_RATE = 0.1\r\n\r\nSTAIRCASE = True\r\n\r\nBATCH_SIZE = 45\r\n\r\nMAX_ITER = 15000\r\n\r\nSUMMARY_ITER = 10\r\n\r\nSAVE_ITER = 1000\r\n\r\n\r\n#\r\n# test parameter\r\n#\r\n\r\nTHRESHOLD = 0.2\r\n\r\nIOU_THRESHOLD = 0.5\r\n"""
datagen.py,0,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nDeep Human Pose Estimation\r\n \r\nProject by Walid Benbihi\r\nMSc Individual Project\r\nImperial College\r\nCreated on Wed Jul 12 15:53:44 2017\r\n \r\n@author: Walid Benbihi\r\n@mail : w.benbihi(at)gmail.com\r\n@github : https://github.com/wbenbihi/hourglasstensorlfow/\r\n \r\nAbstract:\r\n        This python code creates a Stacked Hourglass Model\r\n        (Credits : A.Newell et al.)\r\n        (Paper : https://arxiv.org/abs/1603.06937)\r\n        \r\n        Code translated from \'anewell\' github\r\n        Torch7(LUA) --> TensorFlow(PYTHON)\r\n        (Code : https://github.com/anewell/pose-hg-train)\r\n        \r\n        Modification are made and explained in the report\r\n        Goal : Achieve Real Time detection (Webcam)\r\n        ----- Modifications made to obtain faster results (trade off speed/accuracy)\r\n        \r\n        This work is free of use, please cite the author if you use it!\r\n\r\n""""""\r\nimport numpy as np\r\nimport cv2\r\nimport os\r\nimport matplotlib.pyplot as plt\r\nimport random\r\nimport time\r\nfrom skimage import transform\r\nimport scipy.misc as scm\r\n\r\nclass DataGenerator():\r\n\t"""""" DataGenerator Class : To generate Train, Validatidation and Test sets\r\n\tfor the Deep Human Pose Estimation Model \r\n\tFormalized DATA:\r\n\t\tInputs:\r\n\t\t\tInputs have a shape of (Number of Image) X (Height: 256) X (Width: 256) X (Channels: 3)\r\n\t\tOutputs:\r\n\t\t\tOutputs have a shape of (Number of Image) X (Number of Stacks) X (Heigth: 64) X (Width: 64) X (OutputDimendion: 16)\r\n\tJoints:\r\n\t\tWe use the MPII convention on joints numbering\r\n\t\tList of joints:\r\n\t\t\t00 - Right Ankle\r\n\t\t\t01 - Right Knee\r\n\t\t\t02 - Right Hip\r\n\t\t\t03 - Left Hip\r\n\t\t\t04 - Left Knee\r\n\t\t\t05 - Left Ankle\r\n\t\t\t06 - Pelvis (Not present in other dataset ex : LSP)\r\n\t\t\t07 - Thorax (Not present in other dataset ex : LSP)\r\n\t\t\t08 - Neck\r\n\t\t\t09 - Top Head\r\n\t\t\t10 - Right Wrist\r\n\t\t\t11 - Right Elbow\r\n\t\t\t12 - Right Shoulder\r\n\t\t\t13 - Left Shoulder\r\n\t\t\t14 - Left Elbow\r\n\t\t\t15 - Left Wrist\r\n\t# TODO : Modify selection of joints for Training\r\n\t\r\n\tHow to generate Dataset:\r\n\t\tCreate a TEXT file with the following structure:\r\n\t\t\timage_name.jpg[LETTER] box_xmin box_ymin box_xmax b_ymax joints\r\n\t\t\t[LETTER]:\r\n\t\t\t\tOne image can contain multiple person. To use the same image\r\n\t\t\t\tfinish the image with a CAPITAL letter [A,B,C...] for \r\n\t\t\t\tfirst/second/third... person in the image\r\n \t\t\tjoints : \r\n\t\t\t\tSequence of x_p y_p (p being the p-joint)\r\n\t\t\t\t/!\\ In case of missing values use -1\r\n\t\t\t\t\r\n\tThe Generator will read the TEXT file to create a dictionnary\r\n\tThen 2 options are available for training:\r\n\t\tStore image/heatmap arrays (numpy file stored in a folder: need disk space but faster reading)\r\n\t\tGenerate image/heatmap arrays when needed (Generate arrays while training, increase training time - Need to compute arrays at every iteration) \r\n\t""""""\r\n\tdef __init__(self, joints_name = None, img_dir=None, train_data_file = None, remove_joints = None):\r\n\t\t"""""" Initializer\r\n\t\tArgs:\r\n\t\t\tjoints_name\t\t\t: List of joints condsidered\r\n\t\t\timg_dir\t\t\t\t: Directory containing every images\r\n\t\t\ttrain_data_file\t\t: Text file with training set data\r\n\t\t\tremove_joints\t\t: Joints List to keep (See documentation)\r\n\t\t""""""\r\n\t\tif joints_name == None:\r\n\t\t\tself.joints_list = [\'r_anckle\', \'r_knee\', \'r_hip\', \'l_hip\', \'l_knee\', \'l_anckle\', \'pelvis\', \'thorax\', \'neck\', \'head\', \'r_wrist\', \'r_elbow\', \'r_shoulder\', \'l_shoulder\', \'l_elbow\', \'l_wrist\']\r\n\t\telse:\r\n\t\t\tself.joints_list = joints_name\r\n\t\tself.toReduce = False\r\n\t\tif remove_joints is not None:\r\n\t\t\tself.toReduce = True\r\n\t\t\tself.weightJ = remove_joints\r\n\t\t\r\n\t\tself.letter = [\'A\',\'B\',\'C\',\'D\',\'E\',\'F\',\'G\',\'H\',\'I\',\'J\',\'K\',\'L\',\'M\',\'N\']\r\n\t\tself.img_dir = img_dir\r\n\t\tself.train_data_file = train_data_file\r\n\t\tself.images = os.listdir(img_dir)\r\n\t\r\n\t# --------------------Generator Initialization Methods ---------------------\r\n\t\r\n\t\r\n\tdef _reduce_joints(self, joints):\r\n\t\t"""""" Select Joints of interest from self.weightJ\r\n\t\t""""""\r\n\t\tj = []\r\n\t\tfor i in range(len(self.weightJ)):\r\n\t\t\tif self.weightJ[i] == 1:\r\n\t\t\t\tj.append(joints[2*i])\r\n\t\t\t\tj.append(joints[2*i + 1])\r\n\t\treturn j\r\n\t\r\n\tdef _create_train_table(self):\r\n\t\t"""""" Create Table of samples from TEXT file\r\n\t\t""""""\r\n\t\tself.train_table = []\r\n\t\tself.no_intel = []\r\n\t\tself.data_dict = {}\r\n\t\tinput_file = open(self.train_data_file, \'r\')\r\n\t\tprint(\'READING TRAIN DATA\')\r\n\t\tfor line in input_file:\r\n\t\t\tline = line.strip()\r\n\t\t\tline = line.split(\' \')\r\n\t\t\tname = line[0]\r\n\t\t\tbox = list(map(int,line[1:5]))\r\n\t\t\tjoints = list(map(int,line[5:]))\r\n\t\t\tif self.toReduce:\r\n\t\t\t\tjoints = self._reduce_joints(joints)\r\n\t\t\tif joints == [-1] * len(joints):\r\n\t\t\t\tself.no_intel.append(name)\r\n\t\t\telse:\r\n\t\t\t\tjoints = np.reshape(joints, (-1,2))\r\n\t\t\t\tw = [1] * joints.shape[0]\r\n\t\t\t\tfor i in range(joints.shape[0]):\r\n\t\t\t\t\tif np.array_equal(joints[i], [-1,-1]):\r\n\t\t\t\t\t\tw[i] = 0\r\n\t\t\t\tself.data_dict[name] = {\'box\' : box, \'joints\' : joints, \'weights\' : w}\r\n\t\t\t\tself.train_table.append(name)\r\n\t\tinput_file.close()\r\n\t\r\n\tdef _randomize(self):\r\n\t\t"""""" Randomize the set\r\n\t\t""""""\r\n\t\trandom.shuffle(self.train_table)\r\n\t\r\n\tdef _complete_sample(self, name):\r\n\t\t"""""" Check if a sample has no missing value\r\n\t\tArgs:\r\n\t\t\tname \t: Name of the sample\r\n\t\t""""""\r\n\t\tfor i in range(self.data_dict[name][\'joints\'].shape[0]):\r\n\t\t\tif np.array_equal(self.data_dict[name][\'joints\'][i],[-1,-1]):\r\n\t\t\t\treturn False\r\n\t\treturn True\r\n\t\r\n\tdef _give_batch_name(self, batch_size = 16, set = \'train\'):\r\n\t\t"""""" Returns a List of Samples\r\n\t\tArgs:\r\n\t\t\tbatch_size\t: Number of sample wanted\r\n\t\t\tset\t\t\t\t: Set to use (valid/train)\r\n\t\t""""""\r\n\t\tlist_file = []\r\n\t\tfor i in range(batch_size):\r\n\t\t\tif set == \'train\':\r\n\t\t\t\tlist_file.append(random.choice(self.train_set))\r\n\t\t\telif set == \'valid\':\r\n\t\t\t\tlist_file.append(random.choice(self.valid_set))\r\n\t\t\telse:\r\n\t\t\t\tprint(\'Set must be : train/valid\')\r\n\t\t\t\tbreak\r\n\t\treturn list_file\r\n\t\t\r\n\t\r\n\tdef _create_sets(self, validation_rate = 0.1):\r\n\t\t"""""" Select Elements to feed training and validation set \r\n\t\tArgs:\r\n\t\t\tvalidation_rate\t\t: Percentage of validation data (in ]0,1[, don\'t waste time use 0.1)\r\n\t\t""""""\r\n\t\tsample = len(self.train_table)\r\n\t\tvalid_sample = int(sample * validation_rate)\r\n\t\tself.train_set = self.train_table[:sample - valid_sample]\r\n\t\tself.valid_set = []\r\n\t\tpreset = self.train_table[sample - valid_sample:]\r\n\t\tprint(\'START SET CREATION\')\r\n\t\tfor elem in preset:\r\n\t\t\tif self._complete_sample(elem):\r\n\t\t\t\tself.valid_set.append(elem)\r\n\t\t\telse:\r\n\t\t\t\tself.train_set.append(elem)\r\n\t\tprint(\'SET CREATED\')\r\n\t\tnp.save(\'Dataset-Validation-Set\', self.valid_set)\r\n\t\tnp.save(\'Dataset-Training-Set\', self.train_set)\r\n\t\tprint(\'--Training set :\', len(self.train_set), \' samples.\')\r\n\t\tprint(\'--Validation set :\', len(self.valid_set), \' samples.\')\r\n\t\r\n\tdef generateSet(self, rand = False):\r\n\t\t"""""" Generate the training and validation set\r\n\t\tArgs:\r\n\t\t\trand : (bool) True to shuffle the set\r\n\t\t""""""\r\n\t\tself._create_train_table()\r\n\t\tif rand:\r\n\t\t\tself._randomize()\r\n\t\tself._create_sets()\r\n\t\r\n\t# ---------------------------- Generating Methods --------------------------\t\r\n\t\r\n\t\r\n\tdef _makeGaussian(self, height, width, sigma = 3, center=None):\r\n\t\t"""""" Make a square gaussian kernel.\r\n\t\tsize is the length of a side of the square\r\n\t\tsigma is full-width-half-maximum, which\r\n\t\tcan be thought of as an effective radius.\r\n\t\t""""""\r\n\t\tx = np.arange(0, width, 1, float)\r\n\t\ty = np.arange(0, height, 1, float)[:, np.newaxis]\r\n\t\tif center is None:\r\n\t\t\tx0 =  width // 2\r\n\t\t\ty0 = height // 2\r\n\t\telse:\r\n\t\t\tx0 = center[0]\r\n\t\t\ty0 = center[1]\r\n\t\treturn np.exp(-4*np.log(2) * ((x-x0)**2 + (y-y0)**2) / sigma**2)\r\n\t\r\n\tdef _generate_hm(self, height, width ,joints, maxlenght, weight):\r\n\t\t"""""" Generate a full Heap Map for every joints in an array\r\n\t\tArgs:\r\n\t\t\theight\t\t\t: Wanted Height for the Heat Map\r\n\t\t\twidth\t\t\t: Wanted Width for the Heat Map\r\n\t\t\tjoints\t\t\t: Array of Joints\r\n\t\t\tmaxlenght\t\t: Lenght of the Bounding Box\r\n\t\t""""""\r\n\t\tnum_joints = joints.shape[0]\r\n\t\thm = np.zeros((height, width, num_joints), dtype = np.float32)\r\n\t\tfor i in range(num_joints):\r\n\t\t\tif not(np.array_equal(joints[i], [-1,-1])) and weight[i] == 1:\r\n\t\t\t\ts = int(np.sqrt(maxlenght) * maxlenght * 10 / 4096) + 2\r\n\t\t\t\thm[:,:,i] = self._makeGaussian(height, width, sigma= s, center= (joints[i,0], joints[i,1]))\r\n\t\t\telse:\r\n\t\t\t\thm[:,:,i] = np.zeros((height,width))\r\n\t\treturn hm\r\n\t\t\r\n\tdef _crop_data(self, height, width, box, joints, boxp = 0.05):\r\n\t\t"""""" Automatically returns a padding vector and a bounding box given\r\n\t\tthe size of the image and a list of joints.\r\n\t\tArgs:\r\n\t\t\theight\t\t: Original Height\r\n\t\t\twidth\t\t: Original Width\r\n\t\t\tbox\t\t\t: Bounding Box\r\n\t\t\tjoints\t\t: Array of joints\r\n\t\t\tboxp\t\t: Box percentage (Use 20% to get a good bounding box)\r\n\t\t""""""\r\n\t\tpadding = [[0,0],[0,0],[0,0]]\r\n\t\tj = np.copy(joints)\r\n\t\tif box[0:2] == [-1,-1]:\r\n\t\t\tj[joints == -1] = 1e5\r\n\t\t\tbox[0], box[1] = min(j[:,0]), min(j[:,1])\r\n\t\tcrop_box = [box[0] - int(boxp * (box[2]-box[0])), box[1] - int(boxp * (box[3]-box[1])), box[2] + int(boxp * (box[2]-box[0])), box[3] + int(boxp * (box[3]-box[1]))]\r\n\t\tif crop_box[0] < 0: crop_box[0] = 0\r\n\t\tif crop_box[1] < 0: crop_box[1] = 0\r\n\t\tif crop_box[2] > width -1: crop_box[2] = width -1\r\n\t\tif crop_box[3] > height -1: crop_box[3] = height -1\r\n\t\tnew_h = int(crop_box[3] - crop_box[1])\r\n\t\tnew_w = int(crop_box[2] - crop_box[0])\r\n\t\tcrop_box = [crop_box[0] + new_w //2, crop_box[1] + new_h //2, new_w, new_h]\r\n\t\tif new_h > new_w:\r\n\t\t\tbounds = (crop_box[0] - new_h //2, crop_box[0] + new_h //2)\r\n\t\t\tif bounds[0] < 0:\r\n\t\t\t\tpadding[1][0] = abs(bounds[0])\r\n\t\t\tif bounds[1] > width - 1:\r\n\t\t\t\tpadding[1][1] = abs(width - bounds[1])\r\n\t\telif new_h < new_w:\r\n\t\t\tbounds = (crop_box[1] - new_w //2, crop_box[1] + new_w //2)\r\n\t\t\tif bounds[0] < 0:\r\n\t\t\t\tpadding[0][0] = abs(bounds[0])\r\n\t\t\tif bounds[1] > width - 1:\r\n\t\t\t\tpadding[0][1] = abs(height - bounds[1])\r\n\t\tcrop_box[0] += padding[1][0]\r\n\t\tcrop_box[1] += padding[0][0]\r\n\t\treturn padding, crop_box\r\n\t\r\n\tdef _crop_img(self, img, padding, crop_box):\r\n\t\t"""""" Given a bounding box and padding values return cropped image\r\n\t\tArgs:\r\n\t\t\timg\t\t\t: Source Image\r\n\t\t\tpadding\t: Padding\r\n\t\t\tcrop_box\t: Bounding Box\r\n\t\t""""""\r\n\t\timg = np.pad(img, padding, mode = \'constant\')\r\n\t\tmax_lenght = max(crop_box[2], crop_box[3])\r\n\t\timg = img[crop_box[1] - max_lenght //2:crop_box[1] + max_lenght //2, crop_box[0] - max_lenght // 2:crop_box[0] + max_lenght //2]\r\n\t\treturn img\r\n\t\t\r\n\tdef _crop(self, img, hm, padding, crop_box):\r\n\t\t"""""" Given a bounding box and padding values return cropped image and heatmap\r\n\t\tArgs:\r\n\t\t\timg\t\t\t: Source Image\r\n\t\t\thm\t\t\t: Source Heat Map\r\n\t\t\tpadding\t: Padding\r\n\t\t\tcrop_box\t: Bounding Box\r\n\t\t""""""\r\n\t\timg = np.pad(img, padding, mode = \'constant\')\r\n\t\thm = np.pad(hm, padding, mode = \'constant\')\r\n\t\tmax_lenght = max(crop_box[2], crop_box[3])\r\n\t\timg = img[crop_box[1] - max_lenght //2:crop_box[1] + max_lenght //2, crop_box[0] - max_lenght // 2:crop_box[0] + max_lenght //2]\r\n\t\thm = hm[crop_box[1] - max_lenght //2:crop_box[1] + max_lenght//2, crop_box[0] - max_lenght // 2:crop_box[0] + max_lenght // 2]\r\n\t\treturn img, hm\r\n\t\r\n\tdef _relative_joints(self, box, padding, joints, to_size = 64):\r\n\t\t"""""" Convert Absolute joint coordinates to crop box relative joint coordinates\r\n\t\t(Used to compute Heat Maps)\r\n\t\tArgs:\r\n\t\t\tbox\t\t\t: Bounding Box \r\n\t\t\tpadding\t: Padding Added to the original Image\r\n\t\t\tto_size\t: Heat Map wanted Size\r\n\t\t""""""\r\n\t\tnew_j = np.copy(joints)\r\n\t\tmax_l = max(box[2], box[3])\r\n\t\tnew_j = new_j + [padding[1][0], padding[0][0]]\r\n\t\tnew_j = new_j - [box[0] - max_l //2,box[1] - max_l //2]\r\n\t\tnew_j = new_j * to_size / (max_l + 0.0000001)\r\n\t\treturn new_j.astype(np.int32)\r\n\t\t\r\n\t\t\r\n\tdef _augment(self,img, hm, max_rotation = 30):\r\n\t\t"""""" # TODO : IMPLEMENT DATA AUGMENTATION \r\n\t\t""""""\r\n\t\tif random.choice([0,1]): \r\n\t\t\tr_angle = np.random.randint(-1*max_rotation, max_rotation)\r\n\t\t\timg = \ttransform.rotate(img, r_angle, preserve_range = True)\r\n\t\t\thm = transform.rotate(hm, r_angle)\r\n\t\treturn img, hm\r\n\t\r\n\t# ----------------------- Batch Generator ----------------------------------\r\n\t\r\n\tdef _generator(self, batch_size = 16, stacks = 4, set = \'train\', stored = False, normalize = True, debug = False):\r\n\t\t"""""" Create Generator for Training\r\n\t\tArgs:\r\n\t\t\tbatch_size\t: Number of images per batch\r\n\t\t\tstacks\t\t\t: Number of stacks/module in the network\r\n\t\t\tset\t\t\t\t: Training/Testing/Validation set # TODO: Not implemented yet\r\n\t\t\tstored\t\t\t: Use stored Value # TODO: Not implemented yet\r\n\t\t\tnormalize\t\t: True to return Image Value between 0 and 1\r\n\t\t\t_debug\t\t\t: Boolean to test the computation time (/!\\ Keep False)\r\n\t\t# Done : Optimize Computation time \r\n\t\t\t16 Images --> 1.3 sec (on i7 6700hq)\r\n\t\t"""""" \r\n\t\twhile True:\r\n\t\t\tif debug:\r\n\t\t\t\tt = time.time()\r\n\t\t\ttrain_img = np.zeros((batch_size, 256,256,3), dtype = np.float32)\r\n\t\t\ttrain_gtmap = np.zeros((batch_size, stacks, 64, 64, len(self.joints_list)), np.float32)\r\n\t\t\tfiles = self._give_batch_name(batch_size= batch_size, set = set)\r\n\t\t\tfor i, name in enumerate(files):\r\n\t\t\t\tif name[:-1] in self.images:\r\n\t\t\t\t\ttry :\r\n\t\t\t\t\t\timg = self.open_img(name)\r\n\t\t\t\t\t\tjoints = self.data_dict[name][\'joints\']\r\n\t\t\t\t\t\tbox = self.data_dict[name][\'box\']\r\n\t\t\t\t\t\tweight = self.data_dict[name][\'weights\']\r\n\t\t\t\t\t\tif debug:\r\n\t\t\t\t\t\t\tprint(box)\r\n\t\t\t\t\t\tpadd, cbox = self._crop_data(img.shape[0], img.shape[1], box, joints, boxp = 0.2)\r\n\t\t\t\t\t\tif debug:\r\n\t\t\t\t\t\t\tprint(cbox)\r\n\t\t\t\t\t\t\tprint(\'maxl :\', max(cbox[2], cbox[3]))\r\n\t\t\t\t\t\tnew_j = self._relative_joints(cbox,padd, joints, to_size=64)\r\n\t\t\t\t\t\thm = self._generate_hm(64, 64, new_j, 64, weight)\r\n\t\t\t\t\t\timg = self._crop_img(img, padd, cbox)\r\n\t\t\t\t\t\timg = img.astype(np.uint8)\r\n\t\t\t\t\t\t# On 16 image per batch\r\n\t\t\t\t\t\t# Avg Time -OpenCV : 1.0 s -skimage: 1.25 s -scipy.misc.imresize: 1.05s\r\n\t\t\t\t\t\timg = scm.imresize(img, (256,256))\r\n\t\t\t\t\t\t# Less efficient that OpenCV resize method\r\n\t\t\t\t\t\t#img = transform.resize(img, (256,256), preserve_range = True, mode = \'constant\')\r\n\t\t\t\t\t\t# May Cause trouble, bug in OpenCV imgwrap.cpp:3229\r\n\t\t\t\t\t\t# error: (-215) ssize.area() > 0 in function cv::resize\r\n\t\t\t\t\t\t#img = cv2.resize(img, (256,256), interpolation = cv2.INTER_CUBIC)\r\n\t\t\t\t\t\timg, hm = self._augment(img, hm)\r\n\t\t\t\t\t\thm = np.expand_dims(hm, axis = 0)\r\n\t\t\t\t\t\thm = np.repeat(hm, stacks, axis = 0)\r\n\t\t\t\t\t\tif normalize:\r\n\t\t\t\t\t\t\ttrain_img[i] = img.astype(np.float32) / 255\r\n\t\t\t\t\t\telse :\r\n\t\t\t\t\t\t\ttrain_img[i] = img.astype(np.float32)\r\n\t\t\t\t\t\ttrain_gtmap[i] = hm\r\n\t\t\t\t\texcept :\r\n\t\t\t\t\t\ti = i-1\r\n\t\t\t\telse:\r\n\t\t\t\t\ti = i - 1\r\n\t\t\tif debug:\r\n\t\t\t\tprint(\'Batch : \',time.time() - t, \' sec.\')\r\n\t\t\tyield train_img, train_gtmap\r\n\t\t\t\r\n\tdef _aux_generator(self, batch_size = 16, stacks = 4, normalize = True, sample_set = \'train\'):\r\n\t\t"""""" Auxiliary Generator\r\n\t\tArgs:\r\n\t\t\tSee Args section in self._generator\r\n\t\t""""""\r\n\t\twhile True:\r\n\t\t\ttrain_img = np.zeros((batch_size, 256,256,3), dtype = np.float32)\r\n\t\t\ttrain_gtmap = np.zeros((batch_size, stacks, 64, 64, len(self.joints_list)), np.float32)\r\n\t\t\ttrain_weights = np.zeros((batch_size, len(self.joints_list)), np.float32)\r\n\t\t\ti = 0\r\n\t\t\twhile i < batch_size:\r\n\t\t\t\ttry:\r\n\t\t\t\t\tif sample_set == \'train\':\r\n\t\t\t\t\t\tname = random.choice(self.train_set)\r\n\t\t\t\t\telif sample_set == \'valid\':\r\n\t\t\t\t\t\tname = random.choice(self.valid_set)\r\n\t\t\t\t\tjoints = self.data_dict[name][\'joints\']\r\n\t\t\t\t\tbox = self.data_dict[name][\'box\']\r\n\t\t\t\t\tweight = np.asarray(self.data_dict[name][\'weights\'])\r\n\t\t\t\t\ttrain_weights[i] = weight \r\n\t\t\t\t\timg = self.open_img(name)\r\n\t\t\t\t\tpadd, cbox = self._crop_data(img.shape[0], img.shape[1], box, joints, boxp = 0.2)\r\n\t\t\t\t\tnew_j = self._relative_joints(cbox,padd, joints, to_size=64)\r\n\t\t\t\t\thm = self._generate_hm(64, 64, new_j, 64, weight)\r\n\t\t\t\t\timg = self._crop_img(img, padd, cbox)\r\n\t\t\t\t\timg = img.astype(np.uint8)\r\n\t\t\t\t\timg = scm.imresize(img, (256,256))\r\n\t\t\t\t\timg, hm = self._augment(img, hm)\r\n\t\t\t\t\thm = np.expand_dims(hm, axis = 0)\r\n\t\t\t\t\thm = np.repeat(hm, stacks, axis = 0)\r\n\t\t\t\t\tif normalize:\r\n\t\t\t\t\t\ttrain_img[i] = img.astype(np.float32) / 255\r\n\t\t\t\t\telse :\r\n\t\t\t\t\t\ttrain_img[i] = img.astype(np.float32)\r\n\t\t\t\t\ttrain_gtmap[i] = hm\r\n\t\t\t\t\ti = i + 1\r\n\t\t\t\texcept :\r\n\t\t\t\t\tprint(\'error file: \', name)\r\n\t\t\tyield train_img, train_gtmap, train_weights\r\n\t\t\t\t\t\r\n\tdef generator(self, batchSize = 16, stacks = 4, norm = True, sample = \'train\'):\r\n\t\t"""""" Create a Sample Generator\r\n\t\tArgs:\r\n\t\t\tbatchSize \t: Number of image per batch \r\n\t\t\tstacks \t \t: Stacks in HG model\r\n\t\t\tnorm \t \t \t: (bool) True to normalize the batch\r\n\t\t\tsample \t \t: \'train\'/\'valid\' Default: \'train\'\r\n\t\t""""""\r\n\t\treturn self._aux_generator(batch_size=batchSize, stacks=stacks, normalize=norm, sample_set=sample)\r\n\t\r\n\t# ---------------------------- Image Reader --------------------------------\t\t\t\t\r\n\tdef open_img(self, name, color = \'RGB\'):\r\n\t\t"""""" Open an image \r\n\t\tArgs:\r\n\t\t\tname\t: Name of the sample\r\n\t\t\tcolor\t: Color Mode (RGB/BGR/GRAY)\r\n\t\t""""""\r\n\t\tif name[-1] in self.letter:\r\n\t\t\tname = name[:-1]\r\n\t\timg = cv2.imread(os.path.join(self.img_dir, name))\r\n\t\tif color == \'RGB\':\r\n\t\t\timg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n\t\t\treturn img\r\n\t\telif color == \'BGR\':\r\n\t\t\treturn img\r\n\t\telif color == \'GRAY\':\r\n\t\t\timg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r\n\t\telse:\r\n\t\t\tprint(\'Color mode supported: RGB/BGR. If you need another mode do it yourself :p\')\r\n\t\r\n\tdef plot_img(self, name, plot = \'cv2\'):\r\n\t\t"""""" Plot an image\r\n\t\tArgs:\r\n\t\t\tname\t: Name of the Sample\r\n\t\t\tplot\t: Library to use (cv2: OpenCV, plt: matplotlib)\r\n\t\t""""""\r\n\t\tif plot == \'cv2\':\r\n\t\t\timg = self.open_img(name, color = \'BGR\')\r\n\t\t\tcv2.imshow(\'Image\', img)\r\n\t\telif plot == \'plt\':\r\n\t\t\timg = self.open_img(name, color = \'RGB\')\r\n\t\t\tplt.imshow(img)\r\n\t\t\tplt.show()\r\n\t\r\n\tdef test(self, toWait = 0.2):\r\n\t\t"""""" TESTING METHOD\r\n\t\tYou can run it to see if the preprocessing is well done.\r\n\t\tWait few seconds for loading, then diaporama appears with image and highlighted joints\r\n\t\t/!\\ Use Esc to quit\r\n\t\tArgs:\r\n\t\t\ttoWait : In sec, time between pictures\r\n\t\t""""""\r\n\t\tself._create_train_table()\r\n\t\tself._create_sets()\r\n\t\tfor i in range(len(self.train_set)):\r\n\t\t\timg = self.open_img(self.train_set[i])\r\n\t\t\tw = self.data_dict[self.train_set[i]][\'weights\']\r\n\t\t\tpadd, box = self._crop_data(img.shape[0], img.shape[1], self.data_dict[self.train_set[i]][\'box\'], self.data_dict[self.train_set[i]][\'joints\'], boxp= 0.0)\r\n\t\t\tnew_j = self._relative_joints(box,padd, self.data_dict[self.train_set[i]][\'joints\'], to_size=256)\r\n\t\t\trhm = self._generate_hm(256, 256, new_j,256, w)\r\n\t\t\trimg = self._crop_img(img, padd, box)\r\n\t\t\t# See Error in self._generator\r\n\t\t\t#rimg = cv2.resize(rimg, (256,256))\r\n\t\t\trimg = scm.imresize(rimg, (256,256))\r\n\t\t\t#rhm = np.zeros((256,256,16))\r\n\t\t\t#for i in range(16):\r\n\t\t\t#\trhm[:,:,i] = cv2.resize(rHM[:,:,i], (256,256))\r\n\t\t\tgrimg = cv2.cvtColor(rimg, cv2.COLOR_RGB2GRAY)\r\n\t\t\tcv2.imshow(\'image\', grimg / 255 + np.sum(rhm,axis = 2))\r\n\t\t\t# Wait\r\n\t\t\ttime.sleep(toWait)\r\n\t\t\tif cv2.waitKey(1) == 27:\r\n\t\t\t\tprint(\'Ended\')\r\n\t\t\t\tcv2.destroyAllWindows()\r\n\t\t\t\tbreak\r\n\t\r\n\t\r\n\t\r\n\t# ------------------------------- PCK METHODS-------------------------------\r\n\tdef pck_ready(self, idlh = 3, idrs = 12, testSet = None):\r\n\t\t"""""" Creates a list with all PCK ready samples\r\n\t\t(PCK: Percentage of Correct Keypoints)\r\n\t\t""""""\r\n\t\tid_lhip = idlh\r\n\t\tid_rsho = idrs\r\n\t\tself.total_joints = 0\r\n\t\tself.pck_samples = []\r\n\t\tfor s in self.data_dict.keys():\r\n\t\t\tif testSet == None:\r\n\t\t\t\tif self.data_dict[s][\'weights\'][id_lhip] == 1 and self.data_dict[s][\'weights\'][id_rsho] == 1:\r\n\t\t\t\t\tself.pck_samples.append(s)\r\n\t\t\t\t\twIntel = np.unique(self.data_dict[s][\'weights\'], return_counts = True)\r\n\t\t\t\t\tself.total_joints += dict(zip(wIntel[0], wIntel[1]))[1]\r\n\t\t\telse:\r\n\t\t\t\tif self.data_dict[s][\'weights\'][id_lhip] == 1 and self.data_dict[s][\'weights\'][id_rsho] == 1 and s in testSet:\r\n\t\t\t\t\tself.pck_samples.append(s)\r\n\t\t\t\t\twIntel = np.unique(self.data_dict[s][\'weights\'], return_counts = True)\r\n\t\t\t\t\tself.total_joints += dict(zip(wIntel[0], wIntel[1]))[1]\r\n\t\tprint(\'PCK PREPROCESS DONE: \\n --Samples:\', len(self.pck_samples), \'\\n --Num.Joints\', self.total_joints)\r\n\t\r\n\tdef getSample(self, sample = None):\r\n\t\t"""""" Returns information of a sample\r\n\t\tArgs:\r\n\t\t\tsample : (str) Name of the sample\r\n\t\tReturns:\r\n\t\t\timg: RGB Image\r\n\t\t\tnew_j: Resized Joints \r\n\t\t\tw: Weights of Joints\r\n\t\t\tjoint_full: Raw Joints\r\n\t\t\tmax_l: Maximum Size of Input Image\r\n\t\t""""""\r\n\t\tif sample != None:\r\n\t\t\ttry:\r\n\t\t\t\tjoints = self.data_dict[sample][\'joints\']\r\n\t\t\t\tbox = self.data_dict[sample][\'box\']\r\n\t\t\t\tw = self.data_dict[sample][\'weights\']\r\n\t\t\t\timg = self.open_img(sample)\r\n\t\t\t\tpadd, cbox = self._crop_data(img.shape[0], img.shape[1], box, joints, boxp = 0.2)\r\n\t\t\t\tnew_j = self._relative_joints(cbox,padd, joints, to_size=256)\r\n\t\t\t\tjoint_full = np.copy(joints)\r\n\t\t\t\tmax_l = max(cbox[2], cbox[3])\r\n\t\t\t\tjoint_full = joint_full + [padd[1][0], padd[0][0]]\r\n\t\t\t\tjoint_full = joint_full - [cbox[0] - max_l //2,cbox[1] - max_l //2]\r\n\t\t\t\timg = self._crop_img(img, padd, cbox)\r\n\t\t\t\timg = img.astype(np.uint8)\r\n\t\t\t\timg = scm.imresize(img, (256,256))\r\n\t\t\t\treturn img, new_j, w, joint_full, max_l\r\n\t\t\texcept:\r\n\t\t\t\treturn False\r\n\t\telse:\r\n\t\t\tprint(\'Specify a sample name\')\r\n\t\t\t\t\r\n\t\t\r\n\t\t'"
filters.py,0,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nDeep Human Pose Estimation\r\n \r\nProject by Walid Benbihi\r\nMSc Individual Project\r\nImperial College\r\nCreated on Mon Sep  4 20:54:25 2017\r\n \r\n@author: Walid Benbihi\r\n@mail : w.benbihi(at)gmail.com\r\n@github : https://github.com/wbenbihi/hourglasstensorlfow/\r\n \r\nAbstract:\r\n        This python code creates a Stacked Hourglass Model\r\n        (Credits : A.Newell et al.)\r\n        (Paper : https://arxiv.org/abs/1603.06937)\r\n        \r\n        Code translated from \'anewell\' github\r\n        Torch7(LUA) --> TensorFlow(PYTHON)\r\n        (Code : https://github.com/anewell/pose-hg-train)\r\n        \r\n        Modification are made and explained in the report\r\n        Goal : Achieve Real Time detection (Webcam)\r\n        ----- Modifications made to obtain faster results (trade off speed/accuracy)\r\n        \r\n        This work is free of use, please cite the author if you use it!\r\n\r\n""""""\r\nimport numpy as np\r\nimport cv2\r\nclass VideoFilters():\r\n\t""""""\r\n\tWork In Progress\r\n\t""""""\r\n\t\r\n\tdef __init__(self):\r\n\t\t"""""" Initialize Filters\r\n\t\t""""""\r\n\t\tself._sayan_params()\r\n\t\tself.num_filters = 1\r\n\t\tself.existing_filters = [\'isSayan\']\r\n\t\tself.activated_filters = [0]\r\n\t\tself.filter_func = [\'plotSayan\']\r\n\t\t\r\n\tdef _sayan_params(self):\r\n\t\t""""""\r\n\t\tInitialize Sayan parameters\r\n\t\t""""""\r\n\t\tself.sayan_avg = np.array([40, 53, 167, 59, 53, 122, 29, 136, 39, 128])\r\n\t\tself.sayan_std = np.array([44, 16, 13, 15, 43, 18, 6, 17, 20, 18])\r\n\t\r\n\t\r\n\tdef joint2Vect(self, pt1,pt2):\r\n\t\t"""""" Given 2 Joints (Points), returns the associated Vector\r\n\t\t""""""\r\n\t\tvect = pt1 - pt2\r\n\t\td = np.linalg.norm(vect)\r\n\t\treturn vect/d\r\n\t\r\n\tdef vect2angle(self, u,v):\r\n\t\t"""""" Given 2 vectors, returns the Angle between Vectors\r\n\t\t""""""\r\n\t\treturn abs(np.arccos(np.dot(u,v)))\r\n\r\n\tdef angleAdir(self, joints):\r\n\t\t"""""" Given a list of Joints, returns Vectors and Angles of body\r\n\t\t""""""\r\n\t\tj = joints.reshape((16,2), order = \'F\')\r\n\t\tlinks = [(0,1),(1,2),(2,6),(3,6),(4,3),(5,4),(10,11),(11,12),(12,8),(13,8),(14,13),(15,14)]\r\n\t\tangles_l = [(0,1),(1,2),(2,3),(3,4),(4,5),(6,7),(7,8),(8,9),(9,10),(10,11)]\r\n\t\tvects = []\r\n\t\tangles = []\r\n\t\tfor i in range(len(links)):\r\n\t\t\tvects.append(self.joint2Vect( j[links[i][0]], j[links[i][1]]))\r\n\t\tfor i in range(len(angles_l)):\r\n\t\t\tangles.append(self.vect2angle(vects[angles_l[i][0]], vects[angles_l[i][1]]))\r\n\t\treturn vects, np.degrees(angles)\r\n\r\n\tdef isSayan(self, angles):\r\n\t\t"""""" Given an angle list, returns a boolean to state if the Sayan pose is detected\r\n\t\t""""""\r\n\t\tsay = True\r\n\t\tfor i in range(10):\r\n\t\t\tif not(self.sayan_avg[i] - 1.5*self.sayan_std[i] < angles[i] < self.sayan_avg[i] + 1.5*self.sayan_std[i] ):\r\n\t\t\t\tsay = False\r\n\t\treturn say\r\n\t\r\n\tdef plotSayan(self, img, j):\r\n\t\t""""""\r\n\t\tWORK IN PROGRESS\r\n\t\tFUNCTION MAY CRASH\r\n\t\t""""""\r\n\t\thair = cv2.imread(\'./hair.png\')\r\n\t\tratio = hair.shape[1]/ hair.shape[0]\r\n\t\tmask = cv2.imread(\'./maskhair.png\') /255\r\n\t\tdist_h_n = np.linalg.norm(j[9]-j[8])\r\n\t\th = int(hair.shape[0] *20 / dist_h_n) \r\n\t\tw =  int(h * ratio)\r\n\t\thair = cv2.resize(hair, (w,h))\r\n\t\tmask = cv2.resize(mask, (w,h))\r\n\t\tpadd = [[0,0],[0,0],[0,0]]\r\n\t\tif h / 2 > j[9][0]:\r\n\t\t\tpadd[0][0] = int(h/2 - j[9][0])\r\n\t\tif h / 2 + j[9][0] > img.shape[0]:\r\n\t\t\tpadd[0][1] = int(h/2 + j[9][0] - img.shape[0])\r\n\t\tif w / 2 > j[9][1]:\r\n\t\t\tpadd[1][0] = int(w/2 - j[9][1])\t\r\n\t\tif w / 2 + j[9][1] > img.shape[1]:\r\n\t\t\tpadd[1][1] = int(w/2 + j[9][1] - img.shape[1])\r\n\t\tprint(\'Frame\')\r\n\t\tshape = img[int(j[9][0]) - int(np.ceil(h/2)) + padd[0][0]:int(j[9][0]) + int(np.ceil(h/2)) - padd[0][1] ,int(j[9][1]) - int(np.ceil(w/2)) + padd[1][0]:int(j[9][1]) + int(np.ceil(w/2)) - padd[1][1],:].shape\r\n\t\tprint(shape)\r\n\t\tprint(hair[padd[0][0]:mask.shape[0] -padd[0][1],padd[1][0]:mask.shape[1] -padd[1][1],:].shape)\r\n\t\tprint(mask[padd[0][0]:mask.shape[0] -padd[0][1],padd[1][0]:mask.shape[1] -padd[1][1],:].shape)\r\n\t\tmask = mask[padd[0][0]:mask.shape[0] -padd[0][1],padd[1][0]:mask.shape[1] -padd[1][1],:]\r\n\t\thair = hair[padd[0][0]:mask.shape[0] -padd[0][1],padd[1][0]:mask.shape[1] -padd[1][1],:]\r\n\t\thair = cv2.resize(hair, (shape[1],shape[0]))\r\n\t\tmask = cv2.resize(mask, (shape[1],shape[0]))\r\n\t\tmask[mask != 1] = 0\r\n\t\treco = img[int(j[9][0]) - int(np.ceil(h/2)) + padd[0][0]:int(j[9][0]) + int(np.ceil(h/2)) - padd[0][1] ,int(j[9][1]) - int(np.ceil(w/2)) + padd[1][0]:int(j[9][1]) + int(np.ceil(w/2)) - padd[1][1],:] * mask+ hair\r\n\t\timg[int(j[9][0]) - int(np.ceil(h/2)) + padd[0][0]:int(j[9][0]) + int(np.ceil(h/2)) - padd[0][1] ,int(j[9][1]) - int(np.ceil(w/2)) + padd[1][0]:int(j[9][1]) + int(np.ceil(w/2)) - padd[1][1],:] = reco\r\n\t\timg = img.astype(np.uint8)\r\n\t\treturn img\r\n\t\t\r\n\t\t\r\n\t\t\r\n\t\t\r\n\t\r\n\t\r\n\t'"
hourglass_tiny.py,151,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nDeep Human Pose Estimation\r\n\r\nProject by Walid Benbihi\r\nMSc Individual Project\r\nImperial College\r\nCreated on Mon Jul 10 19:13:56 2017\r\n\r\n@author: Walid Benbihi\r\n@mail : w.benbihi(at)gmail.com\r\n@github : https://github.com/wbenbihi/hourglasstensorlfow/\r\n\r\nAbstract:\r\n\tThis python code creates a Stacked Hourglass Model\r\n\t(Credits : A.Newell et al.)\r\n\t(Paper : https://arxiv.org/abs/1603.06937)\r\n\t\r\n\tCode translated from \'anewell\' github\r\n\tTorch7(LUA) --> TensorFlow(PYTHON)\r\n\t(Code : https://github.com/anewell/pose-hg-train)\r\n\t\r\n\tModification are made and explained in the report\r\n\tGoal : Achieve Real Time detection (Webcam)\r\n\t----- Modifications made to obtain faster results (trade off speed/accuracy)\r\n\t\r\n\tThis work is free of use, please cite the author if you use it!\r\n""""""\r\nimport time\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport sys\r\nimport datetime\r\nimport os\r\n\r\nclass HourglassModel():\r\n\t"""""" HourglassModel class: (to be renamed)\r\n\tGenerate TensorFlow model to train and predict Human Pose from images (soon videos)\r\n\tPlease check README.txt for further information on model management.\r\n\t""""""\r\n\tdef __init__(self, nFeat = 512, nStack = 4, nModules = 1, nLow = 4, outputDim = 16, batch_size = 16, drop_rate = 0.2, lear_rate = 2.5e-4, decay = 0.96, decay_step = 2000, dataset = None, training = True, w_summary = True, logdir_train = None, logdir_test = None,tiny = True, attention = False,modif = True,w_loss = False, name = \'tiny_hourglass\',  joints = [\'r_anckle\', \'r_knee\', \'r_hip\', \'l_hip\', \'l_knee\', \'l_anckle\', \'pelvis\', \'thorax\', \'neck\', \'head\', \'r_wrist\', \'r_elbow\', \'r_shoulder\', \'l_shoulder\', \'l_elbow\', \'l_wrist\']):\r\n\t\t"""""" Initializer\r\n\t\tArgs:\r\n\t\t\tnStack\t\t\t\t: number of stacks (stage/Hourglass modules)\r\n\t\t\tnFeat\t\t\t\t: number of feature channels on conv layers\r\n\t\t\tnLow\t\t\t\t: number of downsampling (pooling) per module\r\n\t\t\toutputDim\t\t\t: number of output Dimension (16 for MPII)\r\n\t\t\tbatch_size\t\t\t: size of training/testing Batch\r\n\t\t\tdro_rate\t\t\t: Rate of neurons disabling for Dropout Layers\r\n\t\t\tlear_rate\t\t\t: Learning Rate starting value\r\n\t\t\tdecay\t\t\t\t: Learning Rate Exponential Decay (decay in ]0,1], 1 for constant learning rate)\r\n\t\t\tdecay_step\t\t\t: Step to apply decay\r\n\t\t\tdataset\t\t\t: Dataset (class DataGenerator)\r\n\t\t\ttraining\t\t\t: (bool) True for training / False for prediction\r\n\t\t\tw_summary\t\t\t: (bool) True/False for summary of weight (to visualize in Tensorboard)\r\n\t\t\ttiny\t\t\t\t: (bool) Activate Tiny Hourglass\r\n\t\t\tattention\t\t\t: (bool) Activate Multi Context Attention Mechanism (MCAM)\r\n\t\t\tmodif\t\t\t\t: (bool) Boolean to test some network modification # DO NOT USE IT ! USED TO TEST THE NETWORK\r\n\t\t\tname\t\t\t\t: name of the model\r\n\t\t""""""\r\n\t\tself.nStack = nStack\r\n\t\tself.nFeat = nFeat\r\n\t\tself.nModules = nModules\r\n\t\tself.outDim = outputDim\r\n\t\tself.batchSize = batch_size\r\n\t\tself.training = training\r\n\t\tself.w_summary = w_summary\r\n\t\tself.tiny = tiny\r\n\t\tself.dropout_rate = drop_rate\r\n\t\tself.learning_rate = lear_rate\r\n\t\tself.decay = decay\r\n\t\tself.name = name\r\n\t\tself.attention = attention\r\n\t\tself.decay_step = decay_step\r\n\t\tself.nLow = nLow\r\n\t\tself.modif = modif\r\n\t\tself.dataset = dataset\r\n\t\tself.cpu = \'/cpu:0\'\r\n\t\tself.gpu = \'/gpu:0\'\r\n\t\tself.logdir_train = logdir_train\r\n\t\tself.logdir_test = logdir_test\r\n\t\tself.joints = joints\r\n\t\tself.w_loss = w_loss\r\n\t\t\r\n\t# ACCESSOR\r\n\t\r\n\tdef get_input(self):\r\n\t\t"""""" Returns Input (Placeholder) Tensor\r\n\t\tImage Input :\r\n\t\t\tShape: (None,256,256,3)\r\n\t\t\tType : tf.float32\r\n\t\tWarning:\r\n\t\t\tBe sure to build the model first\r\n\t\t""""""\r\n\t\treturn self.img\r\n\tdef get_output(self):\r\n\t\t"""""" Returns Output Tensor\r\n\t\tOutput Tensor :\r\n\t\t\tShape: (None, nbStacks, 64, 64, outputDim)\r\n\t\t\tType : tf.float32\r\n\t\tWarning:\r\n\t\t\tBe sure to build the model first\r\n\t\t""""""\r\n\t\treturn self.output\r\n\tdef get_label(self):\r\n\t\t"""""" Returns Label (Placeholder) Tensor\r\n\t\tImage Input :\r\n\t\t\tShape: (None, nbStacks, 64, 64, outputDim)\r\n\t\t\tType : tf.float32\r\n\t\tWarning:\r\n\t\t\tBe sure to build the model first\r\n\t\t""""""\r\n\t\treturn self.gtMaps\r\n\tdef get_loss(self):\r\n\t\t"""""" Returns Loss Tensor\r\n\t\tImage Input :\r\n\t\t\tShape: (1,)\r\n\t\t\tType : tf.float32\r\n\t\tWarning:\r\n\t\t\tBe sure to build the model first\r\n\t\t""""""\r\n\t\treturn self.loss\r\n\tdef get_saver(self):\r\n\t\t"""""" Returns Saver\r\n\t\t/!\\ USE ONLY IF YOU KNOW WHAT YOU ARE DOING\r\n\t\tWarning:\r\n\t\t\tBe sure to build the model first\r\n\t\t""""""\r\n\t\treturn self.saver\r\n\t\r\n\t\r\n\tdef generate_model(self):\r\n\t\t"""""" Create the complete graph\r\n\t\t""""""\r\n\t\tstartTime = time.time()\r\n\t\tprint(\'CREATE MODEL:\')\r\n\t\twith tf.device(self.gpu):\r\n\t\t\twith tf.name_scope(\'inputs\'):\r\n\t\t\t\t# Shape Input Image - batchSize: None, height: 256, width: 256, channel: 3 (RGB)\r\n\t\t\t\tself.img = tf.placeholder(dtype= tf.float32, shape= (None, 256, 256, 3), name = \'input_img\')\r\n\t\t\t\tif self.w_loss:\r\n\t\t\t\t\tself.weights = tf.placeholder(dtype = tf.float32, shape = (None, self.outDim))\r\n\t\t\t\t# Shape Ground Truth Map: batchSize x nStack x 64 x 64 x outDim\r\n\t\t\t\tself.gtMaps = tf.placeholder(dtype = tf.float32, shape = (None, self.nStack, 64, 64, self.outDim))\r\n\t\t\t\t# TODO : Implement weighted loss function\r\n\t\t\t\t# NOT USABLE AT THE MOMENT\r\n\t\t\t\t#weights = tf.placeholder(dtype = tf.float32, shape = (None, self.nStack, 1, 1, self.outDim))\r\n\t\t\tinputTime = time.time()\r\n\t\t\tprint(\'---Inputs : Done (\' + str(int(abs(inputTime-startTime))) + \' sec.)\')\r\n\t\t\tif self.attention:\r\n\t\t\t\tself.output = self._graph_mcam(self.img)\r\n\t\t\telse :\r\n\t\t\t\tself.output = self._graph_hourglass(self.img)\r\n\t\t\tgraphTime = time.time()\r\n\t\t\tprint(\'---Graph : Done (\' + str(int(abs(graphTime-inputTime))) + \' sec.)\')\r\n\t\t\twith tf.name_scope(\'loss\'):\r\n\t\t\t\tif self.w_loss:\r\n\t\t\t\t\tself.loss = tf.reduce_mean(self.weighted_bce_loss(), name=\'reduced_loss\')\r\n\t\t\t\telse:\r\n\t\t\t\t\tself.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.output, labels= self.gtMaps), name= \'cross_entropy_loss\')\r\n\t\t\tlossTime = time.time()\t\r\n\t\t\tprint(\'---Loss : Done (\' + str(int(abs(graphTime-lossTime))) + \' sec.)\')\r\n\t\twith tf.device(self.cpu):\r\n\t\t\twith tf.name_scope(\'accuracy\'):\r\n\t\t\t\tself._accuracy_computation()\r\n\t\t\taccurTime = time.time()\r\n\t\t\tprint(\'---Acc : Done (\' + str(int(abs(accurTime-lossTime))) + \' sec.)\')\r\n\t\t\twith tf.name_scope(\'steps\'):\r\n\t\t\t\tself.train_step = tf.Variable(0, name = \'global_step\', trainable= False)\r\n\t\t\twith tf.name_scope(\'lr\'):\r\n\t\t\t\tself.lr = tf.train.exponential_decay(self.learning_rate, self.train_step, self.decay_step, self.decay, staircase= True, name= \'learning_rate\')\r\n\t\t\tlrTime = time.time()\r\n\t\t\tprint(\'---LR : Done (\' + str(int(abs(accurTime-lrTime))) + \' sec.)\')\r\n\t\twith tf.device(self.gpu):\r\n\t\t\twith tf.name_scope(\'rmsprop\'):\r\n\t\t\t\tself.rmsprop = tf.train.RMSPropOptimizer(learning_rate= self.lr)\r\n\t\t\toptimTime = time.time()\r\n\t\t\tprint(\'---Optim : Done (\' + str(int(abs(optimTime-lrTime))) + \' sec.)\')\r\n\t\t\twith tf.name_scope(\'minimizer\'):\r\n\t\t\t\tself.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n\t\t\t\twith tf.control_dependencies(self.update_ops):\r\n\t\t\t\t\tself.train_rmsprop = self.rmsprop.minimize(self.loss, self.train_step)\r\n\t\t\tminimTime = time.time()\r\n\t\t\tprint(\'---Minimizer : Done (\' + str(int(abs(optimTime-minimTime))) + \' sec.)\')\r\n\t\tself.init = tf.global_variables_initializer()\r\n\t\tinitTime = time.time()\r\n\t\tprint(\'---Init : Done (\' + str(int(abs(initTime-minimTime))) + \' sec.)\')\r\n\t\twith tf.device(self.cpu):\r\n\t\t\twith tf.name_scope(\'training\'):\r\n\t\t\t\ttf.summary.scalar(\'loss\', self.loss, collections = [\'train\'])\r\n\t\t\t\ttf.summary.scalar(\'learning_rate\', self.lr, collections = [\'train\'])\r\n\t\t\twith tf.name_scope(\'summary\'):\r\n\t\t\t\tfor i in range(len(self.joints)):\r\n\t\t\t\t\ttf.summary.scalar(self.joints[i], self.joint_accur[i], collections = [\'train\', \'test\'])\r\n\t\tself.train_op = tf.summary.merge_all(\'train\')\r\n\t\tself.test_op = tf.summary.merge_all(\'test\')\r\n\t\tself.weight_op = tf.summary.merge_all(\'weight\')\r\n\t\tendTime = time.time()\r\n\t\tprint(\'Model created (\' + str(int(abs(endTime-startTime))) + \' sec.)\')\r\n\t\tdel endTime, startTime, initTime, optimTime, minimTime, lrTime, accurTime, lossTime, graphTime, inputTime\r\n\t\t\r\n\t\r\n\tdef restore(self, load = None):\r\n\t\t"""""" Restore a pretrained model\r\n\t\tArgs:\r\n\t\t\tload\t: Model to load (None if training from scratch) (see README for further information)\r\n\t\t""""""\r\n\t\twith tf.name_scope(\'Session\'):\r\n\t\t\twith tf.device(self.gpu):\r\n\t\t\t\tself._init_session()\r\n\t\t\t\tself._define_saver_summary(summary = False)\r\n\t\t\t\tif load is not None:\r\n\t\t\t\t\tprint(\'Loading Trained Model\')\r\n\t\t\t\t\tt = time.time()\r\n\t\t\t\t\tself.saver.restore(self.Session, load)\r\n\t\t\t\t\tprint(\'Model Loaded (\', time.time() - t,\' sec.)\')\r\n\t\t\t\telse:\r\n\t\t\t\t\tprint(\'Please give a Model in args (see README for further information)\')\r\n\t\r\n\tdef _train(self, nEpochs = 10, epochSize = 1000, saveStep = 500, validIter = 10):\r\n\t\t""""""\r\n\t\t""""""\r\n\t\twith tf.name_scope(\'Train\'):\r\n\t\t\tself.generator = self.dataset._aux_generator(self.batchSize, self.nStack, normalize = True, sample_set = \'train\')\r\n\t\t\tself.valid_gen = self.dataset._aux_generator(self.batchSize, self.nStack, normalize = True, sample_set = \'valid\')\r\n\t\t\tstartTime = time.time()\r\n\t\t\tself.resume = {}\r\n\t\t\tself.resume[\'accur\'] = []\r\n\t\t\tself.resume[\'loss\'] = []\r\n\t\t\tself.resume[\'err\'] = []\r\n\t\t\tfor epoch in range(nEpochs):\r\n\t\t\t\tepochstartTime = time.time()\r\n\t\t\t\tavg_cost = 0.\r\n\t\t\t\tcost = 0.\r\n\t\t\t\tprint(\'Epoch :\' + str(epoch) + \'/\' + str(nEpochs) + \'\\n\')\r\n\t\t\t\t# Training Set\r\n\t\t\t\tfor i in range(epochSize):\r\n\t\t\t\t\t# DISPLAY PROGRESS BAR\r\n\t\t\t\t\t# TODO : Customize Progress Bar\r\n\t\t\t\t\tpercent = ((i+1)/epochSize) * 100\r\n\t\t\t\t\tnum = np.int(20*percent/100)\r\n\t\t\t\t\ttToEpoch = int((time.time() - epochstartTime) * (100 - percent)/(percent))\r\n\t\t\t\t\tsys.stdout.write(\'\\r Train: {0}>\'.format(""=""*num) + ""{0}>"".format("" ""*(20-num)) + \'||\' + str(percent)[:4] + \'%\' + \' -cost: \' + str(cost)[:6] + \' -avg_loss: \' + str(avg_cost)[:5] + \' -timeToEnd: \' + str(tToEpoch) + \' sec.\')\r\n\t\t\t\t\tsys.stdout.flush()\r\n\t\t\t\t\timg_train, gt_train, weight_train = next(self.generator)\r\n\t\t\t\t\tif i % saveStep == 0:\r\n\t\t\t\t\t\tif self.w_loss:\r\n\t\t\t\t\t\t\t_, c, summary = self.Session.run([self.train_rmsprop, self.loss, self.train_op], feed_dict = {self.img : img_train, self.gtMaps: gt_train, self.weights: weight_train})\r\n\t\t\t\t\t\telse:\r\n\t\t\t\t\t\t\t_, c, summary = self.Session.run([self.train_rmsprop, self.loss, self.train_op], feed_dict = {self.img : img_train, self.gtMaps: gt_train})\r\n\t\t\t\t\t\t# Save summary (Loss + Accuracy)\r\n\t\t\t\t\t\tself.train_summary.add_summary(summary, epoch*epochSize + i)\r\n\t\t\t\t\t\tself.train_summary.flush()\r\n\t\t\t\t\telse:\r\n\t\t\t\t\t\tif self.w_loss:\r\n\t\t\t\t\t\t\t_, c, = self.Session.run([self.train_rmsprop, self.loss], feed_dict = {self.img : img_train, self.gtMaps: gt_train, self.weights: weight_train})\r\n\t\t\t\t\t\telse:\t\r\n\t\t\t\t\t\t\t_, c, = self.Session.run([self.train_rmsprop, self.loss], feed_dict = {self.img : img_train, self.gtMaps: gt_train})\r\n\t\t\t\t\tcost += c\r\n\t\t\t\t\tavg_cost += c/epochSize\r\n\t\t\t\tepochfinishTime = time.time()\r\n\t\t\t\t#Save Weight (axis = epoch)\r\n\t\t\t\tif self.w_loss:\r\n\t\t\t\t\tweight_summary = self.Session.run(self.weight_op, {self.img : img_train, self.gtMaps: gt_train, self.weights: weight_train})\r\n\t\t\t\telse :\r\n\t\t\t\t\tweight_summary = self.Session.run(self.weight_op, {self.img : img_train, self.gtMaps: gt_train})\r\n\t\t\t\tself.train_summary.add_summary(weight_summary, epoch)\r\n\t\t\t\tself.train_summary.flush()\r\n\t\t\t\t#self.weight_summary.add_summary(weight_summary, epoch)\r\n\t\t\t\t#self.weight_summary.flush()\r\n\t\t\t\tprint(\'Epoch \' + str(epoch) + \'/\' + str(nEpochs) + \' done in \' + str(int(epochfinishTime-epochstartTime)) + \' sec.\' + \' -avg_time/batch: \' + str(((epochfinishTime-epochstartTime)/epochSize))[:4] + \' sec.\')\r\n\t\t\t\twith tf.name_scope(\'save\'):\r\n\t\t\t\t\tself.saver.save(self.Session, os.path.join(os.getcwd(),str(self.name + \'_\' + str(epoch + 1))))\r\n\t\t\t\tself.resume[\'loss\'].append(cost)\r\n\t\t\t\t# Validation Set\r\n\t\t\t\taccuracy_array = np.array([0.0]*len(self.joint_accur))\r\n\t\t\t\tfor i in range(validIter):\r\n\t\t\t\t\timg_valid, gt_valid, w_valid = next(self.generator)\r\n\t\t\t\t\taccuracy_pred = self.Session.run(self.joint_accur, feed_dict = {self.img : img_valid, self.gtMaps: gt_valid})\r\n\t\t\t\t\taccuracy_array += np.array(accuracy_pred, dtype = np.float32) / validIter\r\n\t\t\t\tprint(\'--Avg. Accuracy =\', str((np.sum(accuracy_array) / len(accuracy_array)) * 100)[:6], \'%\' )\r\n\t\t\t\tself.resume[\'accur\'].append(accuracy_pred)\r\n\t\t\t\tself.resume[\'err\'].append(np.sum(accuracy_array) / len(accuracy_array))\r\n\t\t\t\tvalid_summary = self.Session.run(self.test_op, feed_dict={self.img : img_valid, self.gtMaps: gt_valid})\r\n\t\t\t\tself.test_summary.add_summary(valid_summary, epoch)\r\n\t\t\t\tself.test_summary.flush()\r\n\t\t\tprint(\'Training Done\')\r\n\t\t\tprint(\'Resume:\' + \'\\n\' + \'  Epochs: \' + str(nEpochs) + \'\\n\' + \'  n. Images: \' + str(nEpochs * epochSize * self.batchSize) )\r\n\t\t\tprint(\'  Final Loss: \' + str(cost) + \'\\n\' + \'  Relative Loss: \' + str(100*self.resume[\'loss\'][-1]/(self.resume[\'loss\'][0] + 0.1)) + \'%\' )\r\n\t\t\tprint(\'  Relative Improvement: \' + str((self.resume[\'err\'][-1] - self.resume[\'err\'][0]) * 100) +\'%\')\r\n\t\t\tprint(\'  Training Time: \' + str( datetime.timedelta(seconds=time.time() - startTime)))\r\n\t\r\n\tdef record_training(self, record):\r\n\t\t"""""" Record Training Data and Export them in CSV file\r\n\t\tArgs:\r\n\t\t\trecord\t\t: record dictionnary\r\n\t\t""""""\r\n\t\tout_file = open(self.name + \'_train_record.csv\', \'w\')\r\n\t\tfor line in range(len(record[\'accur\'])):\r\n\t\t\tout_string = \'\'\r\n\t\t\tlabels = [record[\'loss\'][line]] + [record[\'err\'][line]] + record[\'accur\'][line]\r\n\t\t\tfor label in labels:\r\n\t\t\t\tout_string += str(label) + \', \'\r\n\t\t\tout_string += \'\\n\'\r\n\t\t\tout_file.write(out_string)\r\n\t\tout_file.close()\r\n\t\tprint(\'Training Record Saved\')\r\n\t\t\t\r\n\tdef training_init(self, nEpochs = 10, epochSize = 1000, saveStep = 500, dataset = None, load = None):\r\n\t\t"""""" Initialize the training\r\n\t\tArgs:\r\n\t\t\tnEpochs\t\t: Number of Epochs to train\r\n\t\t\tepochSize\t\t: Size of one Epoch\r\n\t\t\tsaveStep\t\t: Step to save \'train\' summary (has to be lower than epochSize)\r\n\t\t\tdataset\t\t: Data Generator (see generator.py)\r\n\t\t\tload\t\t\t: Model to load (None if training from scratch) (see README for further information)\r\n\t\t""""""\r\n\t\twith tf.name_scope(\'Session\'):\r\n\t\t\twith tf.device(self.gpu):\r\n\t\t\t\tself._init_weight()\r\n\t\t\t\tself._define_saver_summary()\r\n\t\t\t\tif load is not None:\r\n\t\t\t\t\tself.saver.restore(self.Session, load)\r\n\t\t\t\t\t#try:\r\n\t\t\t\t\t\t#\tself.saver.restore(self.Session, load)\r\n\t\t\t\t\t#except Exception:\r\n\t\t\t\t\t\t#\tprint(\'Loading Failed! (Check README file for further information)\')\r\n\t\t\t\tself._train(nEpochs, epochSize, saveStep, validIter=10)\r\n\t\r\n\tdef weighted_bce_loss(self):\r\n\t\t"""""" Create Weighted Loss Function\r\n\t\tWORK IN PROGRESS\r\n\t\t""""""\r\n\t\tself.bceloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.output, labels= self.gtMaps), name= \'cross_entropy_loss\')\r\n\t\te1 = tf.expand_dims(self.weights,axis = 1, name = \'expdim01\')\r\n\t\te2 = tf.expand_dims(e1,axis = 1, name = \'expdim02\')\r\n\t\te3 = tf.expand_dims(e2,axis = 1, name = \'expdim03\')\r\n\t\treturn tf.multiply(e3,self.bceloss, name = \'lossW\')\r\n\t\r\n\tdef _accuracy_computation(self):\r\n\t\t"""""" Computes accuracy tensor\r\n\t\t""""""\r\n\t\tself.joint_accur = []\r\n\t\tfor i in range(len(self.joints)):\r\n\t\t\tself.joint_accur.append(self._accur(self.output[:, self.nStack - 1, :, :,i], self.gtMaps[:, self.nStack - 1, :, :, i], self.batchSize))\r\n\t\t\r\n\tdef _define_saver_summary(self, summary = True):\r\n\t\t"""""" Create Summary and Saver\r\n\t\tArgs:\r\n\t\t\tlogdir_train\t\t: Path to train summary directory\r\n\t\t\tlogdir_test\t\t: Path to test summary directory\r\n\t\t""""""\r\n\t\tif (self.logdir_train == None) or (self.logdir_test == None):\r\n\t\t\traise ValueError(\'Train/Test directory not assigned\')\r\n\t\telse:\r\n\t\t\twith tf.device(self.cpu):\r\n\t\t\t\tself.saver = tf.train.Saver()\r\n\t\t\tif summary:\r\n\t\t\t\twith tf.device(self.gpu):\r\n\t\t\t\t\tself.train_summary = tf.summary.FileWriter(self.logdir_train, tf.get_default_graph())\r\n\t\t\t\t\tself.test_summary = tf.summary.FileWriter(self.logdir_test)\r\n\t\t\t\t\t#self.weight_summary = tf.summary.FileWriter(self.logdir_train, tf.get_default_graph())\r\n\t\r\n\tdef _init_weight(self):\r\n\t\t"""""" Initialize weights\r\n\t\t""""""\r\n\t\tprint(\'Session initialization\')\r\n\t\tself.Session = tf.Session()\r\n\t\tt_start = time.time()\r\n\t\tself.Session.run(self.init)\r\n\t\tprint(\'Sess initialized in \' + str(int(time.time() - t_start)) + \' sec.\')\r\n\t\r\n\tdef _init_session(self):\r\n\t\t"""""" Initialize Session\r\n\t\t""""""\r\n\t\tprint(\'Session initialization\')\r\n\t\tt_start = time.time()\r\n\t\tself.Session = tf.Session()\r\n\t\tprint(\'Sess initialized in \' + str(int(time.time() - t_start)) + \' sec.\')\r\n\t\t\r\n\tdef _graph_hourglass(self, inputs):\r\n\t\t""""""Create the Network\r\n\t\tArgs:\r\n\t\t\tinputs : TF Tensor (placeholder) of shape (None, 256, 256, 3) #TODO : Create a parameter for customize size\r\n\t\t""""""\r\n\t\twith tf.name_scope(\'model\'):\r\n\t\t\twith tf.name_scope(\'preprocessing\'):\r\n\t\t\t\t# Input Dim : nbImages x 256 x 256 x 3\r\n\t\t\t\tpad1 = tf.pad(inputs, [[0,0],[2,2],[2,2],[0,0]], name=\'pad_1\')\r\n\t\t\t\t# Dim pad1 : nbImages x 260 x 260 x 3\r\n\t\t\t\tconv1 = self._conv_bn_relu(pad1, filters= 64, kernel_size = 6, strides = 2, name = \'conv_256_to_128\')\r\n\t\t\t\t# Dim conv1 : nbImages x 128 x 128 x 64\r\n\t\t\t\tr1 = self._residual(conv1, numOut = 128, name = \'r1\')\r\n\t\t\t\t# Dim pad1 : nbImages x 128 x 128 x 128\r\n\t\t\t\tpool1 = tf.contrib.layers.max_pool2d(r1, [2,2], [2,2], padding=\'VALID\')\r\n\t\t\t\t# Dim pool1 : nbImages x 64 x 64 x 128\r\n\t\t\t\tif self.tiny:\r\n\t\t\t\t\tr3 = self._residual(pool1, numOut=self.nFeat, name=\'r3\')\r\n\t\t\t\telse:\r\n\t\t\t\t\tr2 = self._residual(pool1, numOut= int(self.nFeat/2), name = \'r2\')\r\n\t\t\t\t\tr3 = self._residual(r2, numOut= self.nFeat, name = \'r3\')\r\n\t\t\t# Storage Table\r\n\t\t\thg = [None] * self.nStack\r\n\t\t\tll = [None] * self.nStack\r\n\t\t\tll_ = [None] * self.nStack\r\n\t\t\tdrop = [None] * self.nStack\r\n\t\t\tout = [None] * self.nStack\r\n\t\t\tout_ = [None] * self.nStack\r\n\t\t\tsum_ = [None] * self.nStack\r\n\t\t\tif self.tiny:\r\n\t\t\t\twith tf.name_scope(\'stacks\'):\r\n\t\t\t\t\twith tf.name_scope(\'stage_0\'):\r\n\t\t\t\t\t\thg[0] = self._hourglass(r3, self.nLow, self.nFeat, \'hourglass\')\r\n\t\t\t\t\t\tdrop[0] = tf.layers.dropout(hg[0], rate = self.dropout_rate, training = self.training, name = \'dropout\')\r\n\t\t\t\t\t\tll[0] = self._conv_bn_relu(drop[0], self.nFeat, 1, 1, name = \'ll\')\r\n\t\t\t\t\t\tif self.modif:\r\n\t\t\t\t\t\t\t# TEST OF BATCH RELU\r\n\t\t\t\t\t\t\tout[0] = self._conv_bn_relu(ll[0], self.outDim, 1, 1, \'VALID\', \'out\')\r\n\t\t\t\t\t\telse:\r\n\t\t\t\t\t\t\tout[0] = self._conv(ll[0], self.outDim, 1, 1, \'VALID\', \'out\')\r\n\t\t\t\t\t\tout_[0] = self._conv(out[0], self.nFeat, 1, 1, \'VALID\', \'out_\')\r\n\t\t\t\t\t\tsum_[0] = tf.add_n([out_[0], ll[0], r3], name = \'merge\')\r\n\t\t\t\t\tfor i in range(1, self.nStack - 1):\r\n\t\t\t\t\t\twith tf.name_scope(\'stage_\' + str(i)):\r\n\t\t\t\t\t\t\thg[i] = self._hourglass(sum_[i-1], self.nLow, self.nFeat, \'hourglass\')\r\n\t\t\t\t\t\t\tdrop[i] = tf.layers.dropout(hg[i], rate = self.dropout_rate, training = self.training, name = \'dropout\')\r\n\t\t\t\t\t\t\tll[i] = self._conv_bn_relu(drop[i], self.nFeat, 1, 1, name= \'ll\')\r\n\t\t\t\t\t\t\tif self.modif:\r\n\t\t\t\t\t\t\t\t# TEST OF BATCH RELU\r\n\t\t\t\t\t\t\t\tout[i] = self._conv_bn_relu(ll[i], self.outDim, 1, 1, \'VALID\', \'out\')\r\n\t\t\t\t\t\t\telse:\r\n\t\t\t\t\t\t\t\tout[i] = self._conv(ll[i], self.outDim, 1, 1, \'VALID\', \'out\')\r\n\t\t\t\t\t\t\tout_[i] = self._conv(out[i], self.nFeat, 1, 1, \'VALID\', \'out_\')\r\n\t\t\t\t\t\t\tsum_[i] = tf.add_n([out_[i], ll[i], sum_[i-1]], name= \'merge\')\r\n\t\t\t\t\twith tf.name_scope(\'stage_\' + str(self.nStack - 1)):\r\n\t\t\t\t\t\thg[self.nStack - 1] = self._hourglass(sum_[self.nStack - 2], self.nLow, self.nFeat, \'hourglass\')\r\n\t\t\t\t\t\tdrop[self.nStack-1] = tf.layers.dropout(hg[self.nStack-1], rate = self.dropout_rate, training = self.training, name = \'dropout\')\r\n\t\t\t\t\t\tll[self.nStack - 1] = self._conv_bn_relu(drop[self.nStack-1], self.nFeat,1,1, \'VALID\', \'conv\')\r\n\t\t\t\t\t\tif self.modif:\r\n\t\t\t\t\t\t\tout[self.nStack - 1] = self._conv_bn_relu(ll[self.nStack - 1], self.outDim, 1,1, \'VALID\', \'out\')\r\n\t\t\t\t\t\telse:\r\n\t\t\t\t\t\t\tout[self.nStack - 1] = self._conv(ll[self.nStack - 1], self.outDim, 1,1, \'VALID\', \'out\')\r\n\t\t\t\tif self.modif:\r\n\t\t\t\t\treturn tf.nn.sigmoid(tf.stack(out, axis= 1 , name= \'stack_output\'),name = \'final_output\')\r\n\t\t\t\telse:\r\n\t\t\t\t\treturn tf.stack(out, axis= 1 , name = \'final_output\')\t\r\n\t\t\telse:\r\n\t\t\t\twith tf.name_scope(\'stacks\'):\r\n\t\t\t\t\twith tf.name_scope(\'stage_0\'):\r\n\t\t\t\t\t\thg[0] = self._hourglass(r3, self.nLow, self.nFeat, \'hourglass\')\r\n\t\t\t\t\t\tdrop[0] = tf.layers.dropout(hg[0], rate = self.dropout_rate, training = self.training, name = \'dropout\')\r\n\t\t\t\t\t\tll[0] = self._conv_bn_relu(drop[0], self.nFeat, 1,1, \'VALID\', name = \'conv\')\r\n\t\t\t\t\t\tll_[0] =  self._conv(ll[0], self.nFeat, 1, 1, \'VALID\', \'ll\')\r\n\t\t\t\t\t\tif self.modif:\r\n\t\t\t\t\t\t\t# TEST OF BATCH RELU\r\n\t\t\t\t\t\t\tout[0] = self._conv_bn_relu(ll[0], self.outDim, 1, 1, \'VALID\', \'out\')\r\n\t\t\t\t\t\telse:\r\n\t\t\t\t\t\t\tout[0] = self._conv(ll[0], self.outDim, 1, 1, \'VALID\', \'out\')\r\n\t\t\t\t\t\tout_[0] = self._conv(out[0], self.nFeat, 1, 1, \'VALID\', \'out_\')\r\n\t\t\t\t\t\tsum_[0] = tf.add_n([out_[0], r3, ll_[0]], name=\'merge\')\r\n\t\t\t\t\tfor i in range(1, self.nStack -1):\r\n\t\t\t\t\t\twith tf.name_scope(\'stage_\' + str(i)):\r\n\t\t\t\t\t\t\thg[i] = self._hourglass(sum_[i-1], self.nLow, self.nFeat, \'hourglass\')\r\n\t\t\t\t\t\t\tdrop[i] = tf.layers.dropout(hg[i], rate = self.dropout_rate, training = self.training, name = \'dropout\')\r\n\t\t\t\t\t\t\tll[i] = self._conv_bn_relu(drop[i], self.nFeat, 1, 1, \'VALID\', name= \'conv\')\r\n\t\t\t\t\t\t\tll_[i] = self._conv(ll[i], self.nFeat, 1, 1, \'VALID\', \'ll\')\r\n\t\t\t\t\t\t\tif self.modif:\r\n\t\t\t\t\t\t\t\tout[i] = self._conv_bn_relu(ll[i], self.outDim, 1, 1, \'VALID\', \'out\')\r\n\t\t\t\t\t\t\telse:\r\n\t\t\t\t\t\t\t\tout[i] = self._conv(ll[i], self.outDim, 1, 1, \'VALID\', \'out\')\r\n\t\t\t\t\t\t\tout_[i] = self._conv(out[i], self.nFeat, 1, 1, \'VALID\', \'out_\')\r\n\t\t\t\t\t\t\tsum_[i] = tf.add_n([out_[i], sum_[i-1], ll_[0]], name= \'merge\')\r\n\t\t\t\t\twith tf.name_scope(\'stage_\' + str(self.nStack -1)):\r\n\t\t\t\t\t\thg[self.nStack - 1] = self._hourglass(sum_[self.nStack - 2], self.nLow, self.nFeat, \'hourglass\')\r\n\t\t\t\t\t\tdrop[self.nStack-1] = tf.layers.dropout(hg[self.nStack-1], rate = self.dropout_rate, training = self.training, name = \'dropout\')\r\n\t\t\t\t\t\tll[self.nStack - 1] = self._conv_bn_relu(drop[self.nStack-1], self.nFeat, 1, 1, \'VALID\', \'conv\')\r\n\t\t\t\t\t\tif self.modif:\r\n\t\t\t\t\t\t\tout[self.nStack - 1] = self._conv_bn_relu(ll[self.nStack - 1], self.outDim, 1,1, \'VALID\', \'out\')\r\n\t\t\t\t\t\telse:\r\n\t\t\t\t\t\t\tout[self.nStack - 1] = self._conv(ll[self.nStack - 1], self.outDim, 1,1, \'VALID\', \'out\')\r\n\t\t\t\tif self.modif:\r\n\t\t\t\t\treturn tf.nn.sigmoid(tf.stack(out, axis= 1 , name= \'stack_output\'),name = \'final_output\')\r\n\t\t\t\telse:\r\n\t\t\t\t\treturn tf.stack(out, axis= 1 , name = \'final_output\')\t\t\r\n\t\t\t\t\t\t\r\n\t\t\t\t\r\n\tdef _conv(self, inputs, filters, kernel_size = 1, strides = 1, pad = \'VALID\', name = \'conv\'):\r\n\t\t"""""" Spatial Convolution (CONV2D)\r\n\t\tArgs:\r\n\t\t\tinputs\t\t\t: Input Tensor (Data Type : NHWC)\r\n\t\t\tfilters\t\t: Number of filters (channels)\r\n\t\t\tkernel_size\t: Size of kernel\r\n\t\t\tstrides\t\t: Stride\r\n\t\t\tpad\t\t\t\t: Padding Type (VALID/SAME) # DO NOT USE \'SAME\' NETWORK BUILT FOR VALID\r\n\t\t\tname\t\t\t: Name of the block\r\n\t\tReturns:\r\n\t\t\tconv\t\t\t: Output Tensor (Convolved Input)\r\n\t\t""""""\r\n\t\twith tf.name_scope(name):\r\n\t\t\t# Kernel for convolution, Xavier Initialisation\r\n\t\t\tkernel = tf.Variable(tf.contrib.layers.xavier_initializer(uniform=False)([kernel_size,kernel_size, inputs.get_shape().as_list()[3], filters]), name= \'weights\')\r\n\t\t\tconv = tf.nn.conv2d(inputs, kernel, [1,strides,strides,1], padding=pad, data_format=\'NHWC\')\r\n\t\t\tif self.w_summary:\r\n\t\t\t\twith tf.device(\'/cpu:0\'):\r\n\t\t\t\t\ttf.summary.histogram(\'weights_summary\', kernel, collections = [\'weight\'])\r\n\t\t\treturn conv\r\n\t\t\t\r\n\tdef _conv_bn_relu(self, inputs, filters, kernel_size = 1, strides = 1, pad = \'VALID\', name = \'conv_bn_relu\'):\r\n\t\t"""""" Spatial Convolution (CONV2D) + BatchNormalization + ReLU Activation\r\n\t\tArgs:\r\n\t\t\tinputs\t\t\t: Input Tensor (Data Type : NHWC)\r\n\t\t\tfilters\t\t: Number of filters (channels)\r\n\t\t\tkernel_size\t: Size of kernel\r\n\t\t\tstrides\t\t: Stride\r\n\t\t\tpad\t\t\t\t: Padding Type (VALID/SAME) # DO NOT USE \'SAME\' NETWORK BUILT FOR VALID\r\n\t\t\tname\t\t\t: Name of the block\r\n\t\tReturns:\r\n\t\t\tnorm\t\t\t: Output Tensor\r\n\t\t""""""\r\n\t\twith tf.name_scope(name):\r\n\t\t\tkernel = tf.Variable(tf.contrib.layers.xavier_initializer(uniform=False)([kernel_size,kernel_size, inputs.get_shape().as_list()[3], filters]), name= \'weights\')\r\n\t\t\tconv = tf.nn.conv2d(inputs, kernel, [1,strides,strides,1], padding=\'VALID\', data_format=\'NHWC\')\r\n\t\t\tnorm = tf.contrib.layers.batch_norm(conv, 0.9, epsilon=1e-5, activation_fn = tf.nn.relu, is_training = self.training)\r\n\t\t\tif self.w_summary:\r\n\t\t\t\twith tf.device(\'/cpu:0\'):\r\n\t\t\t\t\ttf.summary.histogram(\'weights_summary\', kernel, collections = [\'weight\'])\r\n\t\t\treturn norm\r\n\t\r\n\tdef _conv_block(self, inputs, numOut, name = \'conv_block\'):\r\n\t\t"""""" Convolutional Block\r\n\t\tArgs:\r\n\t\t\tinputs\t: Input Tensor\r\n\t\t\tnumOut\t: Desired output number of channel\r\n\t\t\tname\t: Name of the block\r\n\t\tReturns:\r\n\t\t\tconv_3\t: Output Tensor\r\n\t\t""""""\r\n\t\tif self.tiny:\r\n\t\t\twith tf.name_scope(name):\r\n\t\t\t\tnorm = tf.contrib.layers.batch_norm(inputs, 0.9, epsilon=1e-5, activation_fn = tf.nn.relu, is_training = self.training)\r\n\t\t\t\tpad = tf.pad(norm, np.array([[0,0],[1,1],[1,1],[0,0]]), name= \'pad\')\r\n\t\t\t\tconv = self._conv(pad, int(numOut), kernel_size=3, strides=1, pad = \'VALID\', name= \'conv\')\r\n\t\t\t\treturn conv\r\n\t\telse:\r\n\t\t\twith tf.name_scope(name):\r\n\t\t\t\twith tf.name_scope(\'norm_1\'):\r\n\t\t\t\t\tnorm_1 = tf.contrib.layers.batch_norm(inputs, 0.9, epsilon=1e-5, activation_fn = tf.nn.relu, is_training = self.training)\r\n\t\t\t\t\tconv_1 = self._conv(norm_1, int(numOut/2), kernel_size=1, strides=1, pad = \'VALID\', name= \'conv\')\r\n\t\t\t\twith tf.name_scope(\'norm_2\'):\r\n\t\t\t\t\tnorm_2 = tf.contrib.layers.batch_norm(conv_1, 0.9, epsilon=1e-5, activation_fn = tf.nn.relu, is_training = self.training)\r\n\t\t\t\t\tpad = tf.pad(norm_2, np.array([[0,0],[1,1],[1,1],[0,0]]), name= \'pad\')\r\n\t\t\t\t\tconv_2 = self._conv(pad, int(numOut/2), kernel_size=3, strides=1, pad = \'VALID\', name= \'conv\')\r\n\t\t\t\twith tf.name_scope(\'norm_3\'):\r\n\t\t\t\t\tnorm_3 = tf.contrib.layers.batch_norm(conv_2, 0.9, epsilon=1e-5, activation_fn = tf.nn.relu, is_training = self.training)\r\n\t\t\t\t\tconv_3 = self._conv(norm_3, int(numOut), kernel_size=1, strides=1, pad = \'VALID\', name= \'conv\')\r\n\t\t\t\treturn conv_3\r\n\t\r\n\tdef _skip_layer(self, inputs, numOut, name = \'skip_layer\'):\r\n\t\t"""""" Skip Layer\r\n\t\tArgs:\r\n\t\t\tinputs\t: Input Tensor\r\n\t\t\tnumOut\t: Desired output number of channel\r\n\t\t\tname\t: Name of the bloc\r\n\t\tReturns:\r\n\t\t\tTensor of shape (None, inputs.height, inputs.width, numOut)\r\n\t\t""""""\r\n\t\twith tf.name_scope(name):\r\n\t\t\tif inputs.get_shape().as_list()[3] == numOut:\r\n\t\t\t\treturn inputs\r\n\t\t\telse:\r\n\t\t\t\tconv = self._conv(inputs, numOut, kernel_size=1, strides = 1, name = \'conv\')\r\n\t\t\t\treturn conv\t\t\t\t\r\n\t\r\n\tdef _residual(self, inputs, numOut, name = \'residual_block\'):\r\n\t\t"""""" Residual Unit\r\n\t\tArgs:\r\n\t\t\tinputs\t: Input Tensor\r\n\t\t\tnumOut\t: Number of Output Features (channels)\r\n\t\t\tname\t: Name of the block\r\n\t\t""""""\r\n\t\twith tf.name_scope(name):\r\n\t\t\tconvb = self._conv_block(inputs, numOut)\r\n\t\t\tskipl = self._skip_layer(inputs, numOut)\r\n\t\t\tif self.modif:\r\n\t\t\t\treturn tf.nn.relu(tf.add_n([convb, skipl], name = \'res_block\'))\r\n\t\t\telse:\r\n\t\t\t\treturn tf.add_n([convb, skipl], name = \'res_block\')\r\n\t\r\n\tdef _hourglass(self, inputs, n, numOut, name = \'hourglass\'):\r\n\t\t"""""" Hourglass Module\r\n\t\tArgs:\r\n\t\t\tinputs\t: Input Tensor\r\n\t\t\tn\t\t: Number of downsampling step\r\n\t\t\tnumOut\t: Number of Output Features (channels)\r\n\t\t\tname\t: Name of the block\r\n\t\t""""""\r\n\t\twith tf.name_scope(name):\r\n\t\t\t# Upper Branch\r\n\t\t\tup_1 = self._residual(inputs, numOut, name = \'up_1\')\r\n\t\t\t# Lower Branch\r\n\t\t\tlow_ = tf.contrib.layers.max_pool2d(inputs, [2,2], [2,2], padding=\'VALID\')\r\n\t\t\tlow_1= self._residual(low_, numOut, name = \'low_1\')\r\n\t\t\t\r\n\t\t\tif n > 0:\r\n\t\t\t\tlow_2 = self._hourglass(low_1, n-1, numOut, name = \'low_2\')\r\n\t\t\telse:\r\n\t\t\t\tlow_2 = self._residual(low_1, numOut, name = \'low_2\')\r\n\t\t\t\t\r\n\t\t\tlow_3 = self._residual(low_2, numOut, name = \'low_3\')\r\n\t\t\tup_2 = tf.image.resize_nearest_neighbor(low_3, tf.shape(low_3)[1:3]*2, name = \'upsampling\')\r\n\t\t\tif self.modif:\r\n\t\t\t\t# Use of RELU\r\n\t\t\t\treturn tf.nn.relu(tf.add_n([up_2,up_1]), name=\'out_hg\')\r\n\t\t\telse:\r\n\t\t\t\treturn tf.add_n([up_2,up_1], name=\'out_hg\')\r\n\t\r\n\tdef _argmax(self, tensor):\r\n\t\t"""""" ArgMax\r\n\t\tArgs:\r\n\t\t\ttensor\t: 2D - Tensor (Height x Width : 64x64 )\r\n\t\tReturns:\r\n\t\t\targ\t\t: Tuple of max position\r\n\t\t""""""\r\n\t\tresh = tf.reshape(tensor, [-1])\r\n\t\targmax = tf.arg_max(resh, 0)\r\n\t\treturn (argmax // tensor.get_shape().as_list()[0], argmax % tensor.get_shape().as_list()[0])\r\n\t\r\n\tdef _compute_err(self, u, v):\r\n\t\t"""""" Given 2 tensors compute the euclidean distance (L2) between maxima locations\r\n\t\tArgs:\r\n\t\t\tu\t\t: 2D - Tensor (Height x Width : 64x64 )\r\n\t\t\tv\t\t: 2D - Tensor (Height x Width : 64x64 )\r\n\t\tReturns:\r\n\t\t\t(float) : Distance (in [0,1])\r\n\t\t""""""\r\n\t\tu_x,u_y = self._argmax(u)\r\n\t\tv_x,v_y = self._argmax(v)\r\n\t\treturn tf.divide(tf.sqrt(tf.square(tf.to_float(u_x - v_x)) + tf.square(tf.to_float(u_y - v_y))), tf.to_float(91))\r\n\t\r\n\tdef _accur(self, pred, gtMap, num_image):\r\n\t\t"""""" Given a Prediction batch (pred) and a Ground Truth batch (gtMaps),\r\n\t\treturns one minus the mean distance.\r\n\t\tArgs:\r\n\t\t\tpred\t\t: Prediction Batch (shape = num_image x 64 x 64)\r\n\t\t\tgtMaps\t\t: Ground Truth Batch (shape = num_image x 64 x 64)\r\n\t\t\tnum_image \t: (int) Number of images in batch\r\n\t\tReturns:\r\n\t\t\t(float)\r\n\t\t""""""\r\n\t\terr = tf.to_float(0)\r\n\t\tfor i in range(num_image):\r\n\t\t\terr = tf.add(err, self._compute_err(pred[i], gtMap[i]))\r\n\t\treturn tf.subtract(tf.to_float(1), err/num_image)\r\n\t\r\n\t# MULTI CONTEXT ATTENTION MECHANISM\r\n\t# WORK IN PROGRESS DO NOT USE THESE METHODS\r\n\t# BASED ON:\r\n\t# Multi-Context Attention for Human Pose Estimation\r\n\t# Authors: Xiao Chu, Wei Yang, Wanli Ouyang, Cheng Ma, Alan L. Yuille, Xiaogang Wang\r\n\t# Paper: https://arxiv.org/abs/1702.07432\r\n\t# GitHub Torch7 Code: https://github.com/bearpaw/pose-attention\r\n\t\t\r\n\tdef _bn_relu(self, inputs):\r\n\t\tnorm = tf.contrib.layers.batch_norm(inputs, 0.9, epsilon=1e-5, activation_fn = tf.nn.relu, is_training = self.training)\r\n\t\treturn norm\r\n\t\r\n\tdef _pool_layer(self, inputs, numOut, name = \'pool_layer\'):\r\n\t\twith tf.name_scope(name):\r\n\t\t\tbnr_1 = self._bn_relu(inputs)\r\n\t\t\tpool = tf.contrib.layers.max_pool2d(bnr_1,[2,2],[2,2],padding=\'VALID\')\r\n\t\t\tpad_1 = tf.pad(pool, np.array([[0,0],[1,1],[1,1],[0,0]]))\r\n\t\t\tconv_1 = self._conv(pad_1, numOut, kernel_size=3, strides=1, name=\'conv\')\r\n\t\t\tbnr_2 = self._bn_relu(conv_1)\r\n\t\t\tpad_2 = tf.pad(bnr_2, np.array([[0,0],[1,1],[1,1],[0,0]]))\r\n\t\t\tconv_2 = self._conv(pad_2, numOut, kernel_size=3, strides=1, name=\'conv\')\r\n\t\t\tupsample = tf.image.resize_nearest_neighbor(conv_2, tf.shape(conv_2)[1:3]*2, name = \'upsampling\')\r\n\t\treturn upsample\r\n\t\r\n\tdef _attention_iter(self, inputs, lrnSize, itersize, name = \'attention_iter\'):\r\n\t\twith tf.name_scope(name):\r\n\t\t\tnumIn = inputs.get_shape().as_list()[3]\r\n\t\t\tpadding = np.floor(lrnSize/2)\r\n\t\t\tpad = tf.pad(inputs, np.array([[0,0],[1,1],[1,1],[0,0]]))\r\n\t\t\tU = self._conv(pad, filters=1, kernel_size=3, strides=1)\r\n\t\t\tpad_2 = tf.pad(U, np.array([[0,0],[padding,padding],[padding,padding],[0,0]]))\r\n\t\t\tsharedK = tf.Variable(tf.contrib.layers.xavier_initializer(uniform=False)([lrnSize,lrnSize, 1, 1]), name= \'shared_weights\')\r\n\t\t\tQ = []\r\n\t\t\tC = []\r\n\t\t\tfor i in range(itersize):\r\n\t\t\t\tif i ==0:\r\n\t\t\t\t\tconv = tf.nn.conv2d(pad_2, sharedK, [1,1,1,1], padding=\'VALID\', data_format=\'NHWC\')\r\n\t\t\t\telse:\r\n\t\t\t\t\tconv = tf.nn.conv2d(Q[i-1], sharedK, [1,1,1,1], padding=\'SAME\', data_format=\'NHWC\')\r\n\t\t\t\tC.append(conv)\r\n\t\t\t\tQ_tmp = tf.nn.sigmoid(tf.add_n([C[i], U]))\r\n\t\t\t\tQ.append(Q_tmp)\r\n\t\t\tstacks = []\r\n\t\t\tfor i in range(numIn):\r\n\t\t\t\tstacks.append(Q[-1]) \r\n\t\t\tpfeat = tf.multiply(inputs,tf.concat(stacks, axis = 3) )\r\n\t\treturn pfeat\r\n\t\r\n\tdef _attention_part_crf(self, inputs, lrnSize, itersize, usepart, name = \'attention_part\'):\r\n\t\twith tf.name_scope(name):\r\n\t\t\tif usepart == 0:\r\n\t\t\t\treturn self._attention_iter(inputs, lrnSize, itersize)\r\n\t\t\telse:\r\n\t\t\t\tpartnum = self.outDim\r\n\t\t\t\tpre = []\r\n\t\t\t\tfor i in range(partnum):\r\n\t\t\t\t\tatt = self._attention_iter(inputs, lrnSize, itersize)\r\n\t\t\t\t\tpad = tf.pad(att, np.array([[0,0],[0,0],[0,0],[0,0]]))\r\n\t\t\t\t\ts = self._conv(pad, filters=1, kernel_size=1, strides=1)\r\n\t\t\t\t\tpre.append(s)\r\n\t\t\t\treturn tf.concat(pre, axis = 3)\r\n\r\n\tdef _residual_pool(self, inputs, numOut, name = \'residual_pool\'):\r\n\t\twith tf.name_scope(name):\r\n\t\t\treturn tf.add_n([self._conv_block(inputs, numOut), self._skip_layer(inputs, numOut), self._pool_layer(inputs, numOut)])\r\n\t\r\n\tdef _rep_residual(self, inputs, numOut, nRep, name = \'rep_residual\'):\r\n\t\twith tf.name_scope(name):\r\n\t\t\tout = [None]*nRep\r\n\t\t\tfor i in range(nRep):\r\n\t\t\t\tif i == 0:\r\n\t\t\t\t\ttmpout = self._residual(inputs,numOut)\r\n\t\t\t\telse:\r\n\t\t\t\t\ttmpout = self._residual_pool(out[i-1],numOut)\r\n\t\t\t\tout[i] = tmpout\r\n\t\t\treturn out[nRep-1]\r\n\t\r\n\tdef _hg_mcam(self, inputs, n, numOut, imSize, nModual, name = \'mcam_hg\'):\r\n\t\twith tf.name_scope(name):\r\n\t\t\t#------------Upper Branch\r\n\t\t\tpool = tf.contrib.layers.max_pool2d(inputs,[2,2],[2,2],padding=\'VALID\')\r\n\t\t\tup = []\r\n\t\t\tlow = [] \r\n\t\t\tfor i in range(nModual):\r\n\t\t\t\tif i == 0:\r\n\t\t\t\t\tif n>1:\r\n\t\t\t\t\t\ttmpup = self._rep_residual(inputs, numOut, n -1)\r\n\t\t\t\t\telse:\r\n\t\t\t\t\t\ttmpup = self._residual(inputs, numOut)\r\n\t\t\t\t\ttmplow = self._residual(pool, numOut)\r\n\t\t\t\telse:\r\n\t\t\t\t\tif n>1:\r\n\t\t\t\t\t\ttmpup = self._rep_residual(up[i-1], numOut, n-1)\r\n\t\t\t\t\telse:\r\n\t\t\t\t\t\ttmpup = self._residual_pool(up[i-1], numOut)\r\n\t\t\t\t\ttmplow = self._residual(low[i-1], numOut)\r\n\t\t\t\tup.append(tmpup)\r\n\t\t\t\tlow.append(tmplow)\r\n\t\t\t\t#up[i] = tmpup\r\n\t\t\t\t#low[i] = tmplow\r\n\t\t\t#----------------Lower Branch\r\n\t\t\tif n>1:\r\n\t\t\t\tlow2 = self._hg_mcam(low[-1], n-1, numOut, int(imSize/2), nModual)\r\n\t\t\telse:\r\n\t\t\t\tlow2 = self._residual(low[-1], numOut)\r\n\t\t\tlow3 = self._residual(low2, numOut)\r\n\t\t\tup_2 = tf.image.resize_nearest_neighbor(low3, tf.shape(low3)[1:3]*2, name = \'upsampling\')\r\n\t\t\treturn tf.add_n([up[-1], up_2], name = \'out_hg\')\r\n\t\r\n\tdef _lin(self, inputs, numOut, name = \'lin\'):\r\n\t\tl = self._conv(inputs, filters = numOut, kernel_size = 1, strides = 1)\r\n\t\treturn self._bn_relu(l)\r\n\t\r\n\tdef _graph_mcam(self, inputs):\r\n\t\twith tf.name_scope(\'preprocessing\'):\r\n\t\t\tpad1 = tf.pad(inputs, np.array([[0,0],[3,3],[3,3],[0,0]]))\r\n\t\t\tcnv1_ = self._conv(pad1, filters = 64, kernel_size = 7, strides = 1)\r\n\t\t\tcnv1 = self._bn_relu(cnv1_)\r\n\t\t\tr1 = self._residual(cnv1, 64)\r\n\t\t\tpool1 = tf.contrib.layers.max_pool2d(r1,[2,2],[2,2],padding=\'VALID\')\r\n\t\t\tr2 = self._residual(pool1, 64)\r\n\t\t\tr3 = self._residual(r2, 128)\r\n\t\t\tpool2 = tf.contrib.layers.max_pool2d(r3,[2,2],[2,2],padding=\'VALID\')\r\n\t\t\tr4 = self._residual(pool2,128)\r\n\t\t\tr5 = self._residual(r4, 128)\r\n\t\t\tr6 = self._residual(r5, 256)\r\n\t\tout = []\r\n\t\tinter = []\r\n\t\tinter.append(r6)\r\n\t\tif self.nLow == 3:\r\n\t\t\tnModual = int(16/self.nStack)\r\n\t\telse:\r\n\t\t\tnModual = int(8/self.nStack)\r\n\t\twith tf.name_scope(\'stacks\'):\r\n\t\t\tfor i in range(self.nStack):\r\n\t\t\t\twith tf.name_scope(\'houglass_\' + str(i+1)):\r\n\t\t\t\t\thg = self._hg_mcam(inter[i], self.nLow, self.nFeat, 64, nModual)\r\n\t\t\t\t\r\n\t\t\t\tif i == self.nStack - 1:\r\n\t\t\t\t\tll1 = self._lin(hg, self.nFeat*2)\r\n\t\t\t\t\tll2 = self._lin(ll1, self.nFeat*2)\r\n\t\t\t\t\tdrop = tf.layers.dropout(ll2, rate=0.1, training = self.training)\r\n\t\t\t\t\tatt =  self._attention_part_crf(drop, 1, 3, 0)\r\n\t\t\t\t\ttmpOut = self._attention_part_crf(att, 1, 3, 1)\r\n\t\t\t\telse:\r\n\t\t\t\t\tll1 = self._lin(hg, self.nFeat)\r\n\t\t\t\t\tll2 = self._lin(ll1, self.nFeat)\r\n\t\t\t\t\tdrop = tf.layers.dropout(ll2, rate=0.1, training = self.training)\r\n\t\t\t\t\tif i > self.nStack // 2:\r\n\t\t\t\t\t\tatt = self._attention_part_crf(drop, 1, 3, 0)\r\n\t\t\t\t\t\ttmpOut = self._attention_part_crf( att, 1, 3, 1)\r\n\t\t\t\t\telse:\r\n\t\t\t\t\t\tatt = self._attention_part_crf(ll2, 1, 3, 0)\r\n\t\t\t\t\t\ttmpOut = self._conv(att, filters = self.outDim, kernel_size = 1, strides = 1)\r\n\t\t\t\tout.append(tmpOut)\r\n\t\t\t\tif i < self.nStack - 1:\r\n\t\t\t\t\toutmap = self._conv(tmpOut, filters = self.nFeat, kernel_size = 1, strides = 1)\r\n\t\t\t\t\tll3 = self._lin(outmap, self.nFeat)\r\n\t\t\t\t\ttmointer = tf.add_n([inter[i], outmap, ll3])\r\n\t\t\t\t\tinter.append(tmointer)\r\n\t\treturn tf.stack(out, axis= 1 , name = \'final_output\')\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t'"
inference.py,0,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nDeep Human Pose Estimation\r\n\r\nProject by Walid Benbihi\r\nMSc Individual Project\r\nImperial College\r\nCreated on Mon Sep  4 18:11:46 2017\r\n\r\n@author: Walid Benbihi\r\n@mail : w.benbihi(at)gmail.com\r\n@github : https://github.com/wbenbihi/hourglasstensorlfow/\r\n\r\nAbstract:\r\n\tThis python code creates a Stacked Hourglass Model\r\n\t(Credits : A.Newell et al.)\r\n\t(Paper : https://arxiv.org/abs/1603.06937)\r\n\t\r\n\tCode translated from \'anewell\' github\r\n\tTorch7(LUA) --> TensorFlow(PYTHON)\r\n\t(Code : https://github.com/anewell/pose-hg-train)\r\n\t\r\n\tModification are made and explained in the report\r\n\tGoal : Achieve Real Time detection (Webcam)\r\n\t----- Modifications made to obtain faster results (trade off speed/accuracy)\r\n\t\r\n\tThis work is free of use, please cite the author if you use it!\r\n\r\n""""""\r\nimport sys\r\nsys.path.append(\'./\')\r\n\r\nfrom hourglass_tiny import HourglassModel\r\nfrom time import time, clock\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport scipy.io\r\nfrom train_launcher import process_config\r\nimport cv2\r\nfrom predictClass import PredictProcessor\r\nfrom yolo_net import YOLONet\r\nfrom datagen import DataGenerator\r\nimport config as cfg\r\nfrom filters import VideoFilters\r\n\r\nclass Inference():\r\n\t"""""" Inference Class\r\n\tUse this file to make your prediction\r\n\tEasy to Use\r\n\tImages used for inference should be RGB images (int values in [0,255])\r\n\tMethods:\r\n\t\twebcamSingle : Single Person Pose Estimation on Webcam Stream\r\n\t\twebcamMultiple : Multiple Person Pose Estimation on Webcam Stream\r\n\t\twebcamPCA : Single Person Pose Estimation with reconstruction error (PCA)\r\n\t\twebcamYOLO : Object Detector\r\n\t\tpredictHM : Returns Heat Map for an input RGB Image\r\n\t\tpredictJoints : Returns joint\'s location (for a 256x256 image)\r\n\t\tpltSkeleton : Plot skeleton on image\r\n\t\trunVideoFilter : SURPRISE !!!\r\n\t""""""\r\n\tdef __init__(self, config_file = \'config.cfg\', model = \'hg_refined_tiny_200\', yoloModel = \'YOLO_small.ckpt\'):\r\n\t\t"""""" Initilize the Predictor\r\n\t\tArgs:\r\n\t\t\tconfig_file \t \t: *.cfg file with model\'s parameters\r\n\t\t\tmodel \t \t \t \t: *.index file\'s name. (weights to load) \r\n\t\t\tyoloModel \t \t: *.ckpt file (YOLO weights to load)\r\n\t\t""""""\r\n\t\tt = time()\r\n\t\tparams = process_config(config_file)\r\n\t\tself.predict = PredictProcessor(params)\r\n\t\tself.predict.color_palette()\r\n\t\tself.predict.LINKS_JOINTS()\r\n\t\tself.predict.model_init()\r\n\t\tself.predict.load_model(load = model)\r\n\t\tself.predict.yolo_init()\r\n\t\tself.predict.restore_yolo(load = yoloModel)\r\n\t\tself.predict._create_prediction_tensor()\r\n\t\tself.filter = VideoFilters()\r\n\t\tprint(\'Done: \', time() - t, \' sec.\')\r\n\t\t\r\n\t# -------------------------- WebCam Inference-------------------------------\r\n\tdef webcamSingle(self, thresh = 0.2, pltJ = True, pltL = True):\r\n\t\t"""""" Run Single Pose Estimation on Webcam Stream\r\n\t\tArgs :\r\n\t\t\tthresh: Joint Threshold\r\n\t\t\tpltJ: (bool) True to plot joints\r\n\t\t\tpltL: (bool) True to plot limbs\r\n\t\t""""""\r\n\t\tself.predict.hpeWebcam(thresh = thresh, plt_j = pltJ, plt_l = pltL, plt_hm = False, debug = False)\r\n\t\r\n\tdef webcamMultiple(self, thresh = 0.2, nms = 0.5, resolution = 800,pltL = True, pltJ = True, pltB = True, isolate = False):\r\n\t\t"""""" Run Multiple Pose Estimation on Webcam Stream\r\n\t\tArgs:\r\n\t\t\tthresh: Joint Threshold\r\n\t\t\tnms : Non Maxima Suppression Threshold\r\n\t\t\tresolution : Stream Resolution\r\n\t\t\tpltJ: (bool) True to plot joints\r\n\t\t\tpltL: (bool) True to plot limbs\r\n\t\t\tpltB: (bool) True to plot bounding boxes\r\n\t\t\tisolate: (bool) True to show isolated skeletons\r\n\t\t""""""\r\n\t\tself.predict.mpe(j_thresh = thresh, nms_thresh = nms, plt_l = pltL, plt_j = pltJ, plt_b = pltB, img_size = resolution, skeleton = isolate)\r\n\t\r\n\tdef webcamPCA(self, n = 5, matrix = \'p4frames.mat\'):\r\n\t\t"""""" Run Single Pose Estimation with Error Reconstruction on Webcam Stream\r\n\t\tArgs:\r\n\t\t\tn : Number of dimension to keep before reconstruction\r\n\t\t\tmatrix : MATLAB eigenvector matrix to load\r\n\t\t""""""\r\n\t\tself.predict.reconstructACPVideo(load = matrix, n = n)\r\n\t\t\r\n\tdef webcamYOLO(self):\r\n\t\t"""""" Run Object Detection on Webcam Stream\r\n\t\t""""""\r\n\t\tcam = cv2.VideoCapture(0)\r\n\t\treturn self.predict.camera_detector( cam, wait=0, mirror = True)\r\n\t\t\r\n\t# ----------------------- Heat Map Prediction ------------------------------\r\n\t\r\n\tdef predictHM(self, img):\r\n\t\t"""""" Return Sigmoid Prediction Heat Map\r\n\t\tArgs:\r\n\t\t\timg : Input Image -shape=(256x256x3) -value= uint8 (in [0, 255])\r\n\t\t""""""\r\n\t\treturn self.predict.pred(self, img / 255, debug = False, sess = None)\r\n\t# ------------------------- Joint Prediction -------------------------------\r\n\t\r\n\tdef predictJoints(self, img, mode = \'cpu\', thresh = 0.2):\r\n\t\t"""""" Return Joint Location\r\n\t\t/!\\ Location with respect to 256x256 image\r\n\t\tArgs:\r\n\t\t\timg : Input Image -shape=(256x256x3) -value= uint8 (in [0, 255])\r\n\t\t\tmode : \'cpu\' / \'gpu\' Select a mode to compute joints\' location\r\n\t\t\tthresh : Joint Threshold\r\n\t\t""""""\r\n\t\tSIZE = False\r\n\t\tif len(img.shape) == 3:\r\n\t\t\tbatch = np.expand_dims(img, axis = 0)\r\n\t\t\tSIZE = True\r\n\t\telif len(img.shape) == 4:\r\n\t\t\tbatch = np.copy(img)\r\n\t\t\tSIZE = True\r\n\t\tif SIZE:\r\n\t\t\tif mode == \'cpu\':\r\n\t\t\t\treturn self.predict.joints_pred_numpy(batch / 255, coord = \'img\', thresh = thresh, sess = None)\r\n\t\t\telif mode == \'gpu\':\r\n\t\t\t\treturn self.predict.joints_pred(batch / 255, coord = \'img\', debug = False, sess = None)\r\n\t\t\telse :\r\n\t\t\t\tprint(""Error : Mode should be \'cpu\'/\'gpu\'"")\r\n\t\telse:\r\n\t\t\tprint(\'Error : Input is not a RGB image nor a batch of RGB images\')\r\n\t\r\n\t# ----------------------------- Plot Skeleton ------------------------------\r\n\t\r\n\tdef pltSkeleton(self, img, thresh, pltJ, pltL):\r\n\t\t"""""" Return an image with plotted joints and limbs\r\n\t\tArgs:\r\n\t\t\timg : Input Image -shape=(256x256x3) -value= uint8 (in [0, 255]) \r\n\t\t\tthresh: Joint Threshold\r\n\t\t\tpltJ: (bool) True to plot joints\r\n\t\t\tpltL: (bool) True to plot limbs\r\n\t\t""""""\r\n\t\treturn self.predict.pltSkeleton(img, thresh = thresh, pltJ = pltJ, pltL = pltL, tocopy = True, norm = True)\r\n\t\r\n\t# -------------------------- Video Processing ------------------------------\r\n\t\r\n\tdef processVideo(self, source = None, outfile = None, thresh = 0.2, nms = 0.5 , codec = \'DIVX\', pltJ = True, pltL = True, pltB = True, show = False):\r\n\t\t"""""" Run Multiple Pose Estimation on Video Footage\r\n\t\tArgs:\r\n\t\t\tsource : Input Footage\r\n\t\t\toutfile : File to Save\r\n\t\t\tthesh : Joints Threshold\r\n\t\t\tnms : Non Maxima Suppression Threshold\r\n\t\t\tcodec : Codec to use\r\n\t\t\tpltJ: (bool) True to plot joints\r\n\t\t\tpltL: (bool) True to plot limbs\r\n\t\t\tpltB: (bool) True to plot bounding boxes\r\n\t\t\tshow: (bool) Show footage during processing\r\n\t\t""""""\r\n\t\treturn self.predict.videoDetection(src = source, outName = outfile, codec = codec, j_thresh = thresh, nms_thresh = nms, show = show, plt_j = pltJ, plt_l = pltL, plt_b = pltB)\r\n\t\r\n\t# -------------------------- Process Stream --------------------------------\r\n\t\r\n\tdef centerStream(self, img):\r\n\t\timg = cv2.flip(img, 1)\r\n\t\timg[:, self.predict.cam_res[1]//2 - self.predict.cam_res[0]//2:self.predict.cam_res[1]//2 + self.predict.cam_res[0]//2]\r\n\t\timg_hg = cv2.resize(img, (256,256))\r\n\t\timg_res = cv2.resize(img, (800,800))\r\n\t\timg_hg = cv2.cvtColor(img_hg, cv2.COLOR_BGR2RGB)\r\n\t\treturn img_res, img_hg\r\n\t\r\n\tdef plotLimbs(self, img_res, j):\r\n\t\t"""""" \r\n\t\t""""""\t\r\n\t\tfor i in range(len(self.predict.links)):\r\n\t\t\tl = self.predict.links[i][\'link\']\r\n\t\t\tgood_link = True\r\n\t\t\tfor p in l:\r\n\t\t\t\tif np.array_equal(j[p], [-1,-1]):\r\n\t\t\t\t\tgood_link = False\r\n\t\t\tif good_link:\r\n\t\t\t\tpos = self.predict.givePixel(l, j)\r\n\t\t\t\tcv2.line(img_res, tuple(pos[0])[::-1], tuple(pos[1])[::-1], self.predict.links[i][\'color\'][::-1], thickness = 5)\r\n\t\r\n\t# -----------------------------  Filters -----------------------------------\r\n\t\r\n\tdef runVideoFilter(self, debug = False):\r\n\t\t"""""" WORK IN PROGRESS\r\n\t\tMystery Function\r\n\t\t""""""\r\n\t\tthresh = 0.2\r\n\t\tcam = cv2.VideoCapture(self.predict.src)\r\n\t\tself.filter.activated_filters = [0]*self.filter.num_filters\r\n\t\twhile True:\r\n\t\t\tt = time()\r\n\t\t\tret_val, img = cam.read()\r\n\t\t\timg_res, img_hg = self.centerStream(img)\r\n\t\t\thg = self.predict.pred(img_hg / 255)\r\n\t\t\tj = np.ones(shape = (self.predict.params[\'num_joints\'],2)) * -1\r\n\t\t\tfor i in range(len(j)):\r\n\t\t\t\tidx = np.unravel_index( hg[0,:,:,i].argmax(), (64,64))\r\n\t\t\t\tif hg[0, idx[0], idx[1], i] > thresh:\r\n\t\t\t\t\tj[i] = np.asarray(idx) * 800 / 64\r\n\t\t\t\t\tif debug:\r\n\t\t\t\t\t\tcv2.circle(img_res, center = tuple(j[i].astype(np.int))[::-1], radius= 5, color= self.predict.color[i][::-1], thickness= -1)\r\n\t\t\tif debug:\r\n\t\t\t\tprint(j[9])\r\n\t\t\t\tself.plotLimbs(img_res, j)\r\n\t\t\tX = j.reshape((32,1),order = \'F\')\r\n\t\t\t_, angles = self.filter.angleAdir(X)\r\n\t\t\tfor f in range(len(self.filter.existing_filters)):\r\n\t\t\t\tif np.sum(self.filter.activated_filters) > 0:\r\n\t\t\t\t\tbreak\r\n\t\t\t\tself.filter.activated_filters[f] = int(eval(\'self.filter.\'+self.filter.existing_filters[f])(angles))\r\n\t\t\tfilter_to_activate = np.argmax(self.filter.activated_filters)\r\n\t\t\tif self.filter.activated_filters[0] > 0:\r\n\t\t\t\timg_res = eval(\'self.filter.\'+self.filter.filter_func[filter_to_activate])(img_res, j)\r\n\t\t\tfps = 1/(time()-t)\r\n\t\t\tcv2.putText(img_res, str(self.filter.activated_filters[0]) +\'- FPS: \' + str(fps)[:4], (60, 40), 2, 2, (0,0,0), thickness = 2)\r\n\t\t\tcv2.imshow(\'stream\', img_res)\r\n\t\t\tif cv2.waitKey(1) == 27:\r\n\t\t\t\tprint(\'Stream Ended\')\r\n\t\t\t\tcv2.destroyAllWindows()\r\n\t\t\t\tbreak\r\n\t\tcv2.destroyAllWindows()\r\n\t\tcam.release()\r\n\t\t\t\r\n\t\t\t\t\r\n\t\t\r\n\t\t\r\n\t\t\r\n\t\t'"
predictClass.py,16,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nDeep Human Pose Estimation\r\n\r\nProject by Walid Benbihi\r\nMSc Individual Project\r\nImperial College\r\nCreated on Mon Jul 17 15:50:43 2017\r\n\r\n@author: Walid Benbihi\r\n@mail : w.benbihi(at)gmail.com\r\n@github : https://github.com/wbenbihi/hourglasstensorlfow/\r\n\r\nAbstract:\r\n\tThis python code creates a Stacked Hourglass Model\r\n\t(Credits : A.Newell et al.)\r\n\t(Paper : https://arxiv.org/abs/1603.06937)\r\n\t\r\n\tCode translated from \'anewell\' github\r\n\tTorch7(LUA) --> TensorFlow(PYTHON)\r\n\t(Code : https://github.com/anewell/pose-hg-train)\r\n\t\r\n\tModification are made and explained in the report\r\n\tGoal : Achieve Real Time detection (Webcam)\r\n\t----- Modifications made to obtain faster results (trade off speed/accuracy)\r\n\t\r\n\tThis work is free of use, please cite the author if you use it!\r\n\r\n""""""\r\n\r\nimport sys\r\nsys.path.append(\'./\')\r\n\r\nfrom hourglass_tiny import HourglassModel\r\nfrom time import time, clock, sleep\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport scipy.io\r\nfrom train_launcher import process_config\r\nimport cv2\r\n#from yolo_tiny_net import YoloTinyNet\r\nfrom yolo_net import YOLONet\r\nfrom datagen import DataGenerator\r\nimport config as cfg\r\nimport threading\r\n\r\nclass PredictProcessor():\r\n\t""""""\r\n\tPredictProcessor class: Give the tools to open and use a trained model for\r\n\tprediction.\r\n\tDependency: OpenCV or PIL (OpenCV prefered)\r\n\t\r\n\tComments:\r\n\t\tEvery CAPITAL LETTER methods are to be modified with regard to your needs and dataset\r\n\t""""""\r\n\t#-------------------------INITIALIZATION METHODS---------------------------\r\n\tdef __init__(self, config_dict):\r\n\t\t"""""" Initializer\r\n\t\tArgs:\r\n\t\t\tconfig_dict\t: config_dict\r\n\t\t""""""\r\n\t\tself.params = config_dict\r\n\t\tself.HG = HourglassModel(nFeat= self.params[\'nfeats\'], nStack = self.params[\'nstacks\'], \r\n\t\t\t\t\t\tnModules = self.params[\'nmodules\'], nLow = self.params[\'nlow\'], outputDim = self.params[\'num_joints\'], \r\n\t\t\t\t\t\tbatch_size = self.params[\'batch_size\'], drop_rate = self.params[\'dropout_rate\'], lear_rate = self.params[\'learning_rate\'],\r\n\t\t\t\t\t\tdecay = self.params[\'learning_rate_decay\'], decay_step = self.params[\'decay_step\'], dataset = None, training = False,\r\n\t\t\t\t\t\tw_summary = True, logdir_test = self.params[\'log_dir_test\'],\r\n\t\t\t\t\t\tlogdir_train = self.params[\'log_dir_test\'], tiny = self.params[\'tiny\'], \r\n\t\t\t\t\t\tmodif = False, name = self.params[\'name\'], attention = self.params[\'mcam\'], w_loss=self.params[\'weighted_loss\'] , joints= self.params[\'joint_list\'])\r\n\t\tself.graph = tf.Graph()\r\n\t\tself.src = 0\r\n\t\tself.cam_res = (480,640)\r\n\t\t\r\n\tdef color_palette(self):\r\n\t\t"""""" Creates a color palette dictionnary\r\n\t\tDrawing Purposes\r\n\t\tYou don\'t need to modify this function.\r\n\t\tIn case you need other colors, add BGR color code to the color list\r\n\t\tand make sure to give it a name in the color_name list\r\n\t\t/!\\ Make sure those 2 lists have the same size\r\n\t\t""""""\r\n\t\t#BGR COLOR CODE\r\n\t\tself.color = [(241,242,224), (196,203,128), (136,150,0), (64,77,0), \r\n\t\t\t\t(201,230,200), (132,199,129), (71,160,67), (32,94,27),\r\n\t\t\t\t(130,224,255), (7,193,255), (0,160,255), (0,111,255),\r\n\t\t\t\t(220,216,207), (174,164,144), (139,125,96), (100,90,69),\r\n\t\t\t\t(252,229,179), (247,195,79), (229,155,3), (155,87,1),\r\n\t\t\t\t(231,190,225), (200,104,186), (176,39,156), (162,31,123),\r\n\t\t\t\t(210,205,255), (115,115,229), (80,83,239), (40,40,198)]\r\n\t\t# Color Names\r\n\t\tself.color_name = [\'teal01\', \'teal02\', \'teal03\', \'teal04\',\r\n\t\t\t\t\'green01\', \'green02\', \'green03\', \'green04\',\r\n\t\t\t\t\'amber01\', \'amber02\', \'amber03\', \'amber04\',\r\n\t\t\t\t\'bluegrey01\', \'bluegrey02\', \'bluegrey03\', \'bluegrey04\',\r\n\t\t\t\t\'lightblue01\', \'lightblue02\', \'lightblue03\', \'lightblue04\',\r\n\t\t\t\t\'purple01\', \'purple02\', \'purple03\', \'purple04\',\r\n\t\t\t\t\'red01\', \'red02\', \'red03\', \'red04\']\r\n\t\tself.classes_name =  [""aeroplane"", ""bicycle"", ""bird"",\r\n\t\t\t\t""boat"", ""bottle"", ""bus"", ""car"", ""cat"", ""chair"",\r\n\t\t\t\t""cow"", ""diningtable"", ""dog"", ""horse"", ""motorbike"",\r\n\t\t\t\t""person"", ""pottedplant"", ""sheep"",\r\n\t\t\t\t""sofa"", ""train"",""tvmonitor""]\r\n\t\t# Person ID = 14\r\n\t\tself.color_class = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\r\n\t\tself.palette = {}\r\n\t\tfor i, name in enumerate(self.color_name):\r\n\t\t\tself.palette[name] = self.color[i]\r\n\t\t\t\r\n\tdef LINKS_JOINTS(self):\r\n\t\t"""""" Defines links to be joined for visualization\r\n\t\tDrawing Purposes\r\n\t\tYou may need to modify this function\r\n\t\t""""""\r\n\t\tself.links = {}\r\n\t\t# Edit Links with your needed skeleton\r\n\t\tLINKS = [(0,1),(1,2),(2,6),(6,3),(3,4),(4,5),(6,8),(8,13),(13,14),(14,15),(8,12),(12,11),(11,10)]\r\n\t\tself.LINKS_ACP = [(0,1),(1,2),(3,4),(4,5),(7,8),(8,9),(10,11),(11,12)]\r\n\t\tcolor_id = [1,2,3,3,2,1,5,27,26,25,27,26,25]\r\n\t\tself.color_id_acp = [8,9,9,8,19,20,20,19]\r\n\t\t# 13 joints version\r\n\t\t#LINKS = [(0,1),(1,2),(2,3),(3,4),(4,5),(6,7),(6,11),(11,12),(12,13),(6,11),(10,9),(9,8)]\r\n\t\t#color_id = [1,2,3,2,1,0,27,26,25,27,26,25]\r\n\t\t# 10 lines version\r\n\t\t# LINKS = [(0,1),(1,2),(3,4),(4,5),(6,8),(8,9),(13,14),(14,15),(12,11),(11,10)]\r\n\t\tfor i in range(len(LINKS)):\r\n\t\t\tself.links[i] = {\'link\' : LINKS[i], \'color\' : self.palette[self.color_name[color_id[i]]]}\r\n\t\r\n\t# ----------------------------TOOLS----------------------------------------\r\n\tdef col2RGB(self, col):\r\n\t\t"""""" \r\n\t\tArgs:\r\n\t\t\tcol \t: (int-tuple) Color code in BGR MODE\r\n\t\tReturns\r\n\t\t\tout \t: (int-tuple) Color code in RGB MODE\r\n\t\t""""""\r\n\t\treturn col[::-1]\r\n\t\r\n\tdef givePixel(self, link, joints):\r\n\t\t"""""" Returns the pixel corresponding to a link\r\n\t\tArgs:\r\n\t\t\tlink \t: (int-tuple) Tuple of linked joints\r\n\t\t\tjoints\t: (array) Array of joints position shape = outDim x 2\r\n\t\tReturns:\r\n\t\t\tout\t\t: (tuple) Tuple of joints position\r\n\t\t""""""\r\n\t\treturn (joints[link[0]].astype(np.int), joints[link[1]].astype(np.int))\r\n\t\r\n\t# ---------------------------MODEL METHODS---------------------------------\r\n\tdef model_init(self):\r\n\t\t"""""" Initialize the Hourglass Model\r\n\t\t""""""\r\n\t\tt = time()\r\n\t\twith self.graph.as_default():\r\n\t\t\tself.HG.generate_model()\r\n\t\tprint(\'Graph Generated in \', int(time() - t), \' sec.\')\r\n\t\r\n\tdef load_model(self, load = None):\r\n\t\t"""""" Load pretrained weights (See README)\r\n\t\tArgs:\r\n\t\t\tload : File to load\r\n\t\t""""""\r\n\t\twith self.graph.as_default():\r\n\t\t\tself.HG.restore(load)\r\n\t\t\r\n\tdef _create_joint_tensor(self, tensor, name = \'joint_tensor\',debug = False):\r\n\t\t"""""" TensorFlow Computation of Joint Position\r\n\t\tArgs:\r\n\t\t\ttensor\t\t: Prediction Tensor Shape [nbStack x 64 x 64 x outDim] or [64 x 64 x outDim]\r\n\t\t\tname\t\t: name of the tensor\r\n\t\tReturns:\r\n\t\t\tout\t\t\t: Tensor of joints position\r\n\t\t\r\n\t\tComment:\r\n\t\t\tGenuinely Agreeing this tensor is UGLY. If you don\'t trust me, look at\r\n\t\t\t\'prediction\' node in TensorBoard.\r\n\t\t\tIn my defence, I implement it to compare computation times with numpy.\r\n\t\t""""""\r\n\t\twith tf.name_scope(name):\r\n\t\t\tshape = tensor.get_shape().as_list()\r\n\t\t\tif debug:\r\n\t\t\t\tprint(shape)\r\n\t\t\tif len(shape) == 3:\r\n\t\t\t\tresh = tf.reshape(tensor[:,:,0], [-1])\r\n\t\t\telif len(shape) == 4:\r\n\t\t\t\tresh = tf.reshape(tensor[-1,:,:,0], [-1])\r\n\t\t\tif debug:\r\n\t\t\t\tprint(resh)\r\n\t\t\targ = tf.arg_max(resh,0)\r\n\t\t\tif debug:\r\n\t\t\t\tprint(arg, arg.get_shape(), arg.get_shape().as_list())\r\n\t\t\tjoints = tf.expand_dims(tf.stack([arg // tf.to_int64(shape[1]), arg % tf.to_int64(shape[1])], axis = -1), axis = 0)\r\n\t\t\tfor i in range(1, shape[-1]):\r\n\t\t\t\tif len(shape) == 3:\r\n\t\t\t\t\tresh = tf.reshape(tensor[:,:,i], [-1])\r\n\t\t\t\telif len(shape) == 4:\r\n\t\t\t\t\tresh = tf.reshape(tensor[-1,:,:,i], [-1])\r\n\t\t\t\targ = tf.arg_max(resh,0)\r\n\t\t\t\tj = tf.expand_dims(tf.stack([arg // tf.to_int64(shape[1]), arg % tf.to_int64(shape[1])], axis = -1), axis = 0)\r\n\t\t\t\tjoints = tf.concat([joints, j], axis = 0)\r\n\t\t\treturn tf.identity(joints, name = \'joints\')\r\n\t\t\t\r\n\tdef _create_prediction_tensor(self):\r\n\t\t"""""" Create Tensor for prediction purposes\r\n\t\t""""""\r\n\t\twith self.graph.as_default():\r\n\t\t\twith tf.name_scope(\'prediction\'):\r\n\t\t\t\tself.HG.pred_sigmoid = tf.nn.sigmoid(self.HG.output[:,self.HG.nStack - 1], name= \'sigmoid_final_prediction\')\r\n\t\t\t\tself.HG.pred_final = self.HG.output[:,self.HG.nStack - 1]\r\n\t\t\t\tself.HG.joint_tensor = self._create_joint_tensor(self.HG.output[0], name = \'joint_tensor\')\r\n\t\t\t\tself.HG.joint_tensor_final = self._create_joint_tensor(self.HG.output[0,-1] , name = \'joint_tensor_final\')\r\n\t\tprint(\'Prediction Tensors Ready!\')\r\n\t\r\n\t\r\n\t\r\n\t#----------------------------PREDICTION METHODS----------------------------\r\n\tdef predict_coarse(self, img, debug = False, sess = None):\r\n\t\t"""""" Given a 256 x 256 image, Returns prediction Tensor\r\n\t\tThis prediction method returns a non processed Output\r\n\t\tValues not Bounded\r\n\t\tArgs:\r\n\t\t\timg\t\t: Image -Shape (256 x256 x 3) -Type : float32\r\n\t\t\tdebug\t: (bool) True to output prediction time\r\n\t\tReturns:\r\n\t\t\tout\t\t: Array -Shape (nbStacks x 64 x 64 x outputDim) -Type : float32\r\n\t\t""""""\r\n\t\tif debug:\r\n\t\t\tt = time()\r\n\t\tif img.shape == (256,256,3):\r\n\t\t\tif sess is None:\r\n\t\t\t\tout = self.HG.Session.run(self.HG.output, feed_dict={self.HG.img : np.expand_dims(img, axis = 0)})\r\n\t\t\telse:\r\n\t\t\t\tout = sess.run(self.HG.output, feed_dict={self.HG.img : np.expand_dims(img, axis = 0)})\r\n\t\telse:\r\n\t\t\tprint(\'Image Size does not match placeholder shape\')\r\n\t\t\traise Exception\r\n\t\tif debug:\r\n\t\t\tprint(\'Pred: \', time() - t, \' sec.\')\r\n\t\treturn out\r\n\t\r\n\tdef pred(self, img, debug = False, sess = None):\r\n\t\t"""""" Given a 256 x 256 image, Returns prediction Tensor\r\n\t\tThis prediction method returns values in [0,1]\r\n\t\tUse this method for inference\r\n\t\tArgs:\r\n\t\t\timg\t\t: Image -Shape (256 x256 x 3) -Type : float32\r\n\t\t\tdebug\t: (bool) True to output prediction time\r\n\t\tReturns:\r\n\t\t\tout\t\t: Array -Shape (64 x 64 x outputDim) -Type : float32\r\n\t\t""""""\r\n\t\tif debug:\r\n\t\t\tt = time()\r\n\t\tif img.shape == (256,256,3):\r\n\t\t\tif sess is None:\r\n\t\t\t\tout = self.HG.Session.run(self.HG.pred_sigmoid, feed_dict={self.HG.img : np.expand_dims(img, axis = 0)})\r\n\t\t\telse:\r\n\t\t\t\tout = sess.run(self.HG.pred_sigmoid, feed_dict={self.HG.img : np.expand_dims(img, axis = 0)})\r\n\t\telse:\r\n\t\t\tprint(\'Image Size does not match placeholder shape\')\r\n\t\t\traise Exception\r\n\t\tif debug:\r\n\t\t\tprint(\'Pred: \', time() - t, \' sec.\')\r\n\t\treturn out\r\n\t\r\n\tdef joints_pred(self, img, coord = \'hm\', debug = False, sess = None):\r\n\t\t"""""" Given an Image, Returns an array with joints position\r\n\t\tArgs:\r\n\t\t\timg\t\t: Image -Shape (256 x 256 x 3) -Type : float32 \r\n\t\t\tcoord\t: \'hm\'/\'img\' Give pixel coordinates relative to heatMap(\'hm\') or Image(\'img\')\r\n\t\t\tdebug\t: (bool) True to output prediction time\r\n\t\tReturns\r\n\t\t\tout\t\t: Array -Shape(num_joints x 2) -Type : int\r\n\t\t""""""\r\n\t\tif debug:\r\n\t\t\tt = time()\r\n\t\t\tif sess is None:\r\n\t\t\t\tj1 = self.HG.Session.run(self.HG.joint_tensor, feed_dict = {self.HG.img: img})\r\n\t\t\telse:\r\n\t\t\t\tj1 = sess.run(self.HG.joint_tensor, feed_dict = {self.HG.img: img})\r\n\t\t\tprint(\'JT:\', time() - t)\r\n\t\t\tt = time()\r\n\t\t\tif sess is None:\r\n\t\t\t\tj2 = self.HG.Session.run(self.HG.joint_tensor_final, feed_dict = {self.HG.img: img})\r\n\t\t\telse:\r\n\t\t\t\tj2 = sess.run(self.HG.joint_tensor_final, feed_dict = {self.HG.img: img})\r\n\t\t\tprint(\'JTF:\', time() - t)\r\n\t\t\tif coord == \'hm\':\r\n\t\t\t\treturn j1, j2\r\n\t\t\telif coord == \'img\':\r\n\t\t\t\treturn j1 * self.params[\'img_size\'] / self.params[\'hm_size\'], j2 *self.params[\'img_size\'] / self.params[\'hm_size\']\r\n\t\t\telse:\r\n\t\t\t\tprint(""Error: \'coord\' argument different of [\'hm\',\'img\']"")\r\n\t\telse:\r\n\t\t\tif sess is None:\r\n\t\t\t\tj = self.HG.Session.run(self.HG.joint_tensor_final, feed_dict = {self.HG.img: img})\r\n\t\t\telse:\r\n\t\t\t\tj = sess.run(self.HG.joint_tensor_final, feed_dict = {self.HG.img: img})\r\n\t\t\tif coord == \'hm\':\r\n\t\t\t\treturn j\r\n\t\t\telif coord == \'img\':\r\n\t\t\t\treturn j * self.params[\'img_size\'] / self.params[\'hm_size\']\r\n\t\t\telse:\r\n\t\t\t\tprint(""Error: \'coord\' argument different of [\'hm\',\'img\']"")\r\n\t\t\t\t\r\n\tdef joints_pred_numpy(self, img, coord = \'hm\', thresh = 0.2, sess = None):\r\n\t\t"""""" Create Tensor for joint position prediction\r\n\t\tNON TRAINABLE\r\n\t\tTO CALL AFTER GENERATING GRAPH\r\n\t\tNotes:\r\n\t\t\tNot more efficient than Numpy, prefer Numpy for such operation!\r\n\t\t""""""\r\n\t\tif sess is None:\r\n\t\t\thm = self.HG.Session.run(self.HG.pred_sigmoid , feed_dict = {self.HG.img: img})\r\n\t\telse:\r\n\t\t\thm = sess.run(self.HG.pred_sigmoid , feed_dict = {self.HG.img: img})\r\n\t\tjoints = -1*np.ones(shape = (self.params[\'num_joints\'], 2))\r\n\t\tfor i in range(self.params[\'num_joints\']):\r\n\t\t\tindex = np.unravel_index(hm[0,:,:,i].argmax(), (self.params[\'hm_size\'],self.params[\'hm_size\']))\r\n\t\t\tif hm[0,index[0], index[1],i] > thresh:\r\n\t\t\t\tif coord == \'hm\':\r\n\t\t\t\t\tjoints[i] = np.array(index)\r\n\t\t\t\telif coord == \'img\':\r\n\t\t\t\t\tjoints[i] = np.array(index) * self.params[\'img_size\'] / self.params[\'hm_size\']\r\n\t\treturn joints\r\n\t\t\t\r\n\tdef batch_pred(self, batch, debug = False):\r\n\t\t"""""" Given a 256 x 256 images, Returns prediction Tensor\r\n\t\tThis prediction method returns values in [0,1]\r\n\t\tUse this method for inference\r\n\t\tArgs:\r\n\t\t\tbatch\t: Batch -Shape (batchSize x 256 x 256 x 3) -Type : float32\r\n\t\t\tdebug\t: (bool) True to output prediction time\r\n\t\tReturns:\r\n\t\t\tout\t\t: Array -Shape (batchSize x 64 x 64 x outputDim) -Type : float32\r\n\t\t""""""\r\n\t\tif debug:\r\n\t\t\tt = time()\r\n\t\tif batch[0].shape == (256,256,3):\r\n\t\t\tout = self.HG.Session.run(self.HG.pred_sigmoid, feed_dict={self.HG.img : batch})\r\n\t\telse:\r\n\t\t\tprint(\'Image Size does not match placeholder shape\')\r\n\t\t\traise Exception\r\n\t\tif debug:\r\n\t\t\tprint(\'Pred: \', time() - t, \' sec.\')\r\n\t\treturn out\r\n\t\r\n\t\r\n\t#-------------------------------PLOT FUNCTION------------------------------\r\n\tdef plt_skeleton(self, img, tocopy = True, debug = False, sess = None):\r\n\t\t"""""" Given an Image, returns Image with plotted limbs (TF VERSION)\r\n\t\tArgs:\r\n\t\t\timg \t: Source Image shape = (256,256,3)\r\n\t\t\ttocopy \t: (bool) False to write on source image / True to return a new array\r\n\t\t\tdebug\t: (bool) for testing puposes\r\n\t\t\tsess\t: KEEP NONE\r\n\t\t""""""\r\n\t\tjoints = self.joints_pred(np.expand_dims(img, axis = 0), coord = \'img\', debug = False, sess = sess)\r\n\t\tif tocopy:\r\n\t\t\timg = np.copy(img)\r\n\t\tfor i in range(len(self.links)):\r\n\t\t\tposition = self.givePixel(self.links[i][\'link\'],joints)\r\n\t\t\tcv2.line(img, tuple(position[0])[::-1], tuple(position[1])[::-1], self.links[i][\'color\'][::-1], thickness = 2)\r\n\t\tif tocopy:\r\n\t\t\treturn img\r\n\t\t\r\n\tdef plt_skeleton_numpy(self, img, tocopy = True, thresh = 0.2, sess = None, joint_plt = True):\r\n\t\t"""""" Given an Image, returns Image with plotted limbs (NUMPY VERSION)\r\n\t\tArgs:\r\n\t\t\timg\t\t\t: Source Image shape = (256,256,3)\r\n\t\t\ttocopy\t\t: (bool) False to write on source image / True to return a new array\r\n\t\t\tthresh\t\t: Joint Threshold\r\n\t\t\tsess\t\t: KEEP NONE\r\n\t\t\tjoint_plt\t: (bool) True to plot joints (as circles)\r\n\t\t""""""\r\n\t\tjoints = self.joints_pred_numpy(np.expand_dims(img, axis = 0), coord = \'img\', thresh = thresh, sess = sess)\r\n\t\tif tocopy:\r\n\t\t\timg = np.copy(img)*255\r\n\t\tfor i in range(len(self.links)):\r\n\t\t\tl = self.links[i][\'link\']\r\n\t\t\tgood_link = True\r\n\t\t\tfor p in l:\r\n\t\t\t\tif np.array_equal(joints[p], [-1,-1]):\r\n\t\t\t\t\tgood_link = False\r\n\t\t\tif good_link:\r\n\t\t\t\tposition = self.givePixel(self.links[i][\'link\'],joints)\r\n\t\t\t\tcv2.line(img, tuple(position[0])[::-1], tuple(position[1])[::-1], self.links[i][\'color\'][::-1], thickness = 2)\r\n\t\tif joint_plt:\r\n\t\t\tfor p in range(len(joints)):\r\n\t\t\t\tif not(np.array_equal(joints[p], [-1,-1])):\r\n\t\t\t\t\tcv2.circle(img, (int(joints[p,1]), int(joints[p,0])), radius = 3, color = self.color[p][::-1], thickness = -1)\r\n\t\tif tocopy:\r\n\t\t\treturn img\r\n\t\t\r\n\tdef pltSkeleton(self, img, thresh = 0.2, pltJ = True, pltL = True, tocopy = True, norm = True):\r\n\t\t"""""" Plot skeleton on Image (Single Detection)\r\n\t\tArgs:\r\n\t\t\timg\t\t\t: Input Image ( RGB IMAGE 256x256x3)\r\n\t\t\tthresh\t\t: Joints Threshold\r\n\t\t\tpltL\t\t: (bool) Plot Limbs\r\n\t\t\ttocopy\t\t: (bool) Plot on imput image or return a copy\r\n\t\t\tnorm\t\t: (bool) Normalize input Image (DON\'T MODIFY)\r\n\t\tReturns:\r\n\t\t\timg\t\t\t: Copy of input image if \'tocopy\'\r\n\t\t""""""\r\n\t\tif tocopy:\r\n\t\t\timg = np.copy(img)\r\n\t\tif norm:\r\n\t\t\timg_hg = img / 255\r\n\t\thg = self.HG.Session.run(self.HG.pred_sigmoid, feed_dict = {self.HG.img: np.expand_dims(img_hg, axis = 0)})\r\n\t\tj = np.ones(shape = (self.params[\'num_joints\'],2)) * -1\r\n\t\tfor i in range(len(j)):\r\n\t\t\tidx = np.unravel_index( hg[0,:,:,i].argmax(), (64,64))\r\n\t\t\tif hg[0, idx[0], idx[1], i] > thresh:\r\n\t\t\t\t\tj[i] = np.asarray(idx) * 256 / 64\r\n\t\t\t\t\tif pltJ:\r\n\t\t\t\t\t\tcv2.circle(img, center = tuple(j[i].astype(np.int))[::-1], radius= 5, color= self.color[i][::-1], thickness= -1)\r\n\t\tif pltL:\r\n\t\t\tfor i in range(len(self.links)):\r\n\t\t\t\t\tl = self.links[i][\'link\']\r\n\t\t\t\t\tgood_link = True\r\n\t\t\t\t\tfor p in l:\r\n\t\t\t\t\t\tif np.array_equal(j[p], [-1,-1]):\r\n\t\t\t\t\t\t\tgood_link = False\r\n\t\t\t\t\tif good_link:\r\n\t\t\t\t\t\tpos = self.givePixel(l, j)\r\n\t\t\t\t\t\tcv2.line(img, tuple(pos[0])[::-1], tuple(pos[1])[::-1], self.links[i][\'color\'][::-1], thickness = 5)\r\n\t\tif tocopy:\r\n\t\t\treturn img\r\n\t\t\r\n\t#------------------------Visualiazing Methods------------------------------\r\n\tdef jointsToMat(self, joints):\r\n\t\t"""""" Given a 16 Joints Matrix, returns a 13 joints Matrix\r\n\t\tMPII Formalism to PennAction DeepDragon Formalism\r\n\t\t(Neck, Torso, Pelvis) erased\r\n\t\tArgs:\r\n\t\t\tjoints : Joints of shape (16,2) (HAS TO FOLLOW MPII FORMALISM)\r\n\t\tReturns:\r\n\t\t\tposition : Joints of shape (13,2)\r\n\t\t""""""\r\n\t\tposition = np.zeros((13,2))\r\n\t\tposition[0:6,:] = joints[0:6,:]\r\n\t\tposition[6,:] = joints[9,:]\r\n\t\tposition[7:,:] = joints[10:,:]\r\n\t\tposition = np.reshape(position,(26,1),order = \'F\')\r\n\t\treturn position\r\n\t\r\n\tdef computeErr(self, history, frame = 3):\r\n\t\t"""""" Given frames, compute error vector to project into new basis\r\n\t\tArgs:\r\n\t\t\thistory\t: List of Joints detected in previous frames\r\n\t\t\tframe\t\t: Number of previous frames to consider\r\n\t\tReturns:\r\n\t\t\terr\t\t\t: Error vector\r\n\t\t""""""\r\n\t\terr = np.zeros((13*2*(frame-1),1))\r\n\t\trsho = 9\r\n\t\tlhip = 3\r\n\t\teps = 0.00000001\r\n\t\tfor i in range(frame-1):\r\n\t\t\tjf,js = history[-i-1], history[-i-2]\r\n\t\t\txf,yf = jf[:13], jf[13:]\r\n\t\t\txs,ys = js[:13], js[13:]\r\n\t\t\tx = xf / abs(xf[rsho] - xf[lhip] + eps) - xs / abs(xs[rsho] - xs[lhip] + eps)\r\n\t\t\ty = yf / abs(yf[rsho] - yf[lhip] + eps) - ys / abs(ys[rsho] - ys[lhip] + eps)\r\n\t\t\terr[26*(i):26*(i)+26,:] = np.concatenate([x,y],axis = 0)\r\n\t\treturn err\r\n\t\r\n\tdef errToJoints(self, err, frame, hFrame):\r\n\t\t"""""" Given an error vector and the frame considered, compute the joint\'s \r\n\t\tlocation from the error\r\n\t\tArgs:\r\n\t\t\terr\t\t\t: Error Vector\r\n\t\t\tframe\t\t: Current Frame Detected Joints\r\n\t\t\thFrame\t\t: Previous Frames Detected Joints\r\n\t\tReturns:\r\n\t\t\tjoints\t\t: Joints computed from error\r\n\t\t""""""\r\n\t\t# Don\'t change Matrix computed with those joints to normalize\r\n\t\tlhip = 3\r\n\t\trsho = 9\r\n\t\t# Epsilon to avoir ZeroDivision Exception\r\n\t\teps = 0.00000001\r\n\t\t# Compute X and Y coordinates\r\n\t\tx = err[:,0:13]\r\n\t\ty = err[:,13:26]\r\n\t\tx = np.transpose(x)\r\n\t\ty = np.transpose(y)\r\n\t\txh,yh = hFrame[0:13,:], hFrame[13:26,:]\r\n\t\txf,yf = frame[0:13,:], frame[13:26,:]\r\n\t\tx = (x + (xh /np.abs(xh[rsho,:] - xh[lhip,:] + eps))) * np.abs(xf[rsho,:] - xf[lhip,:] + eps)\r\n\t\ty = (y + (yh /np.abs(yh[rsho,:] - yh[lhip,:] + eps))) * np.abs(yf[rsho,:] - yf[lhip,:] + eps)\r\n\t\tjoints = np.concatenate([x,y], axis = 1).astype(np.int64)\r\n\t\treturn joints\r\n\t\r\n\tdef reconstructACPVideo(self, load = \'F:/Cours/DHPE/DHPE/hourglass_tiny/withyolo/p4frames.mat\',n = 5):\r\n\t\t"""""" Single Person Detection with Principle Componenent Analysis (PCA)\r\n\t\tThis method reconstructs joints given an error matrix (computed on MATLAB and available on GitHub)\r\n\t\tand try to use temporal information from previous frames to improve detection and reduce False Positive\r\n\t\t/!\\Work In Progress on this model\r\n\t\t/!\\Our PCA considers the current frames and the last 3 frames\r\n\t\t/!\\WORKS ONLY ON 13 JOINTS FOR 16 JOINTS TRAINED MODELS\r\n\t\tArgs:\r\n\t\t\tload\t: MATLAB file to load (the eigenvectors matrix should be called P for convenience)\r\n\t\t\tn\t\t: Numbers of Dimensions to consider\r\n\t\t""""""\r\n\t\t# OpenCv Video Capture : 0 is Webcam\r\n\t\tcam = cv2.VideoCapture(self.src)\r\n\t\tframe = 0\r\n\t\t#/!\\NOT USED YET\r\n\t\t#rsho = 9\r\n\t\t#lhip = 3\r\n\t\t# Keep History of previous frames\r\n\t\tjHistory = []\r\n\t\t# Load Eigenvectors MATLAB matrix\r\n\t\tP = scipy.io.loadmat(load)[\'P\']\r\n\t\twhile True:\r\n\t\t\t# Detection As Usual (See hpeWebcam)\r\n\t\t\tt = time()\r\n\t\t\trec = np.zeros((500,500,3)).astype(np.uint8)\r\n\t\t\tplt = np.zeros((500,500,3)).astype(np.uint8)\r\n\t\t\tret_val, img = cam.read()\r\n\t\t\timg = cv2.flip(img,1)\r\n\t\t\timg[:, self.cam_res[1]//2 - self.cam_res[0]//2:self.cam_res[1]//2 + self.cam_res[0]//2]\r\n\t\t\timg_hg = cv2.resize(img, (256,256))\r\n\t\t\timg_res = cv2.resize(img, (500,500))\r\n\t\t\timg_hg = cv2.cvtColor(img_hg, cv2.COLOR_BGR2RGB)\r\n\t\t\tj = self.HG.Session.run(self.HG.joint_tensor_final, feed_dict = {self.HG.img: np.expand_dims(img_hg/255, axis = 0)})\r\n\t\t\tj = np.asarray(j * 500 / 64).astype(np.int)\r\n\t\t\tjoints = self.jointsToMat(j)\r\n\t\t\tjHistory.append(joints)\r\n\t\t\t# When enough frames are available\r\n\t\t\tif frame > 4:\r\n\t\t\t\t#Compute Error\r\n\t\t\t\terr = np.transpose(self.computeErr(jHistory, frame = 4))\r\n\t\t\t\t# Dismiss useless frame\r\n\t\t\t\tjHistory.pop(0)\r\n\t\t\t\t# Project Error into New Basis (PCA new basis)\r\n\t\t\t\tprojectedErr = np.dot(err, P)\r\n\t\t\t\tnComp = n\r\n\t\t\t\t# Reconstruct Error by dimensionality reduction\r\n\t\t\t\trecErr = np.dot(projectedErr[:,:nComp], np.transpose(P[:,:nComp]))\r\n\t\t\t\t# Compute joints position from reconstructed error\r\n\t\t\t\tnewJ = self.errToJoints(recErr, jHistory[-1], jHistory[-2])\r\n\t\t\t\tfor i in range(8):\r\n\t\t\t\t#for i in [4,5,6,7]:\r\n\t\t\t\t\tpos = self.givePixel(self.LINKS_ACP[i], newJ)\r\n\t\t\t\t\tcv2.line(img_res, tuple(pos[0])[::-1], tuple(pos[1])[::-1], self.color[self.color_id_acp[i]][::-1], thickness = 8)\r\n\t\t\t\t\tcv2.line(rec, tuple(pos[0])[::-1], tuple(pos[1])[::-1], self.color[self.color_id_acp[i]][::-1], thickness = 8)\r\n\t\t\tfor i in range(13):\r\n\t\t\t#for i in [8,9,11,12]:\r\n\t\t\t\t\tl = self.links[i][\'link\']\r\n\t\t\t\t\tpos = self.givePixel(l, j)\r\n\t\t\t\t\tcv2.line(img_res, tuple(pos[0])[::-1], tuple(pos[1])[::-1], self.links[i][\'color\'][::-1], thickness = 5)\r\n\t\t\t\t\tcv2.line(plt, tuple(pos[0])[::-1], tuple(pos[1])[::-1], self.links[i][\'color\'][::-1], thickness = 5)\r\n\t\t\tfps = 1/(time()-t)\r\n\t\t\tcv2.putText(img_res, \'FPS: \' + str(fps)[:4], (60, 40), 2, 2, (0,0,0), thickness = 2)\r\n\t\t\ttoplot = np.concatenate([rec, img_res, plt], axis = 1)\r\n\t\t\tcv2.imshow(\'stream\', toplot)\r\n\t\t\tframe += 1\r\n\t\t\tif cv2.waitKey(1) == 27:\r\n\t\t\t\tprint(\'Stream Ended\')\r\n\t\t\t\tcv2.destroyAllWindows()\r\n\t\t\t\tbreak\r\n\t\tcv2.destroyAllWindows()\r\n\t\tcam.release()\r\n\r\n\t\r\n\t\r\n\tdef _singleDetection(self, plt_j = True, plt_l = True):\r\n\t\t"""""" /!\\/!\\DO NOT USE THIS FUNCTION/!\\/!\\\r\n\t\t/!\\/!\\METHOD FOR TEST PURPOSES ONLY/!\\/!\\\r\n\t\tPREFER HPE WEBCAM METHOD\r\n\t\t""""""\r\n\t\tcam = cv2.VideoCapture(self.src)\r\n\t\tframe = 0\r\n\t\tposition = np.zeros((32,1))\r\n\t\twhile True:\r\n\t\t\tt = time()\r\n\t\t\tret_val, img = cam.read()\r\n\t\t\timg = cv2.flip(img, 1)\r\n\t\t\timg[:, self.cam_res[1]//2 - self.cam_res[0]//2:self.cam_res[1]//2 + self.cam_res[0]//2]\r\n\t\t\timg_hg = cv2.resize(img, (256,256))\r\n\t\t\timg_res = cv2.resize(img, (400,400))\r\n\t\t\tcv2.imwrite(\'F:/Cours/DHPE/photos/acp/%04d.png\' % (frame,), img_res)\r\n\t\t\timg_hg = cv2.cvtColor(img_hg, cv2.COLOR_BGR2RGB)\r\n\t\t\tj = self.HG.Session.run(self.HG.joint_tensor_final, feed_dict = {self.HG.img: np.expand_dims(img_hg/255, axis = 0)})\r\n\t\t\tj = np.asarray(j * 400 / 64).astype(np.int)\r\n\t\t\tjoints = self.jointsToMat(j)\r\n\t\t\tX = j.reshape((32,1),order = \'F\')\r\n\t\t\tposition = np.hstack((position, X))\r\n\t\t\tif plt_j:\r\n\t\t\t\tfor i in range(len(j)):\r\n\t\t\t\t\tcv2.circle(img_res, center = tuple(j[i].astype(np.int))[::-1], radius= 5, color= self.color[i][::-1], thickness= -1)\r\n\t\t\tif plt_l:\r\n\t\t\t\tfor i in range(len(self.links)):\r\n\t\t\t\t\tl = self.links[i][\'link\']\r\n\t\t\t\t\tpos = self.givePixel(l, j)\r\n\t\t\t\t\tcv2.line(img_res, tuple(pos[0])[::-1], tuple(pos[1])[::-1], self.links[i][\'color\'][::-1], thickness = 5)\r\n\t\t\tfps = 1/(time()-t)\r\n\t\t\tcv2.putText(img_res, \'FPS: \' + str(fps)[:4], (60, 40), 2, 2, (0,0,0), thickness = 2)\r\n\t\t\tcv2.imshow(\'stream\', img_res)\r\n\t\t\tframe += 1\r\n\t\t\tif cv2.waitKey(1) == 27:\r\n\t\t\t\tprint(\'Stream Ended\')\r\n\t\t\t\tcv2.destroyAllWindows()\r\n\t\t\t\tscipy.io.savemat(\'acpTest2.mat\', dict(history = position))\r\n\t\t\t\tbreak\r\n\t\tcv2.destroyAllWindows()\r\n\t\tcam.release()\r\n\t\t\t\r\n\t\t\t\r\n\tdef hpeWebcam(self, thresh = 0.6, plt_j = True, plt_l = True, plt_hm = False, debug = True):\r\n\t\t"""""" Single Person Detector\r\n\t\tArgs:\r\n\t\t\tthresh\t\t: Threshold for joint plotting\r\n\t\t\tplt_j\t\t: (bool) To plot joints (as circles)\r\n\t\t\tplt_l\t\t: (bool) To plot links/limbs (as lines)\r\n\t\t\tplt_hm\t\t: (bool) To plot heat map\r\n\t\t""""""\r\n\t\tcam = cv2.VideoCapture(self.src)\r\n\t\tif debug:\r\n\t\t\tframerate = []\r\n\t\twhile True:\r\n\t\t\tt = time()\r\n\t\t\tret_val, img = cam.read()\r\n\t\t\timg = cv2.flip(img, 1)\r\n\t\t\timg[:, self.cam_res[1]//2 - self.cam_res[0]//2:self.cam_res[1]//2 + self.cam_res[0]//2]\r\n\t\t\timg_hg = cv2.resize(img, (256,256))\r\n\t\t\timg_res = cv2.resize(img, (800,800))\r\n\t\t\t#img_copy = np.copy(img_res)\r\n\t\t\timg_hg = cv2.cvtColor(img_hg, cv2.COLOR_BGR2RGB)\r\n\t\t\thg = self.HG.Session.run(self.HG.pred_sigmoid, feed_dict = {self.HG.img: np.expand_dims(img_hg/255, axis = 0)})\r\n\t\t\tj = np.ones(shape = (self.params[\'num_joints\'],2)) * -1\r\n\t\t\tif plt_hm:\r\n\t\t\t\thm = np.sum(hg[0], axis = 2)\r\n\t\t\t\thm = np.repeat(np.expand_dims(hm, axis = 2), 3, axis = 2)\r\n\t\t\t\thm = cv2.resize(hm, (800,800))\r\n\t\t\t\timg_res = img_res / 255 + hm\r\n\t\t\tfor i in range(len(j)):\r\n\t\t\t\tidx = np.unravel_index( hg[0,:,:,i].argmax(), (64,64))\r\n\t\t\t\tif hg[0, idx[0], idx[1], i] > thresh:\r\n\t\t\t\t\tj[i] = np.asarray(idx) * 800 / 64\r\n\t\t\t\t\tif plt_j:\r\n\t\t\t\t\t\tcv2.circle(img_res, center = tuple(j[i].astype(np.int))[::-1], radius= 5, color= self.color[i][::-1], thickness= -1)\r\n\t\t\tif plt_l:\r\n\t\t\t\tfor i in range(len(self.links)):\r\n\t\t\t\t\tl = self.links[i][\'link\']\r\n\t\t\t\t\tgood_link = True\r\n\t\t\t\t\tfor p in l:\r\n\t\t\t\t\t\tif np.array_equal(j[p], [-1,-1]):\r\n\t\t\t\t\t\t\tgood_link = False\r\n\t\t\t\t\tif good_link:\r\n\t\t\t\t\t\tpos = self.givePixel(l, j)\r\n\t\t\t\t\t\tcv2.line(img_res, tuple(pos[0])[::-1], tuple(pos[1])[::-1], self.links[i][\'color\'][::-1], thickness = 5)\r\n\t\t\tfps = 1/(time()-t)\r\n\t\t\tif debug:\r\n\t\t\t\tframerate.append(fps)\r\n\t\t\tcv2.putText(img_res, \'FPS: \' + str(fps)[:4], (60, 40), 2, 2, (0,0,0), thickness = 2)\r\n\t\t\tcv2.imshow(\'stream\', img_res)\r\n\t\t\tif cv2.waitKey(1) == 27:\r\n\t\t\t\tprint(\'Stream Ended\')\r\n\t\t\t\tcv2.destroyAllWindows()\r\n\t\t\t\tbreak\r\n\t\tcv2.destroyAllWindows()\r\n\t\tcam.release()\r\n\t\tif debug:\r\n\t\t\treturn framerate\r\n\t\r\n\tdef mpe(self, j_thresh = 0.5, nms_thresh = 0.5, plt_l = True, plt_j = True, plt_b = True, img_size = 800,  skeleton = False):\r\n\t\t"""""" Multiple Person Estimation (WebCam usage)\r\n\t\tArgs:\r\n\t\t\tj_thresh\t\t: Joint Threshold\r\n\t\t\tnms_thresh\t: Non Maxima Suppression Threshold\r\n\t\t\tplt_l\t\t\t: (bool) Plot Limbs\r\n\t\t\tplt_j\t\t\t: (bool) Plot Joints\r\n\t\t\tplt_b\t\t\t: (bool) Plot Bounding Boxes\r\n\t\t\timg_size\t\t: Resolution of Output Image\r\n\t\t\tskeleton\t\t: (bool) Plot Skeleton alone next to image\r\n\t\t""""""\r\n\t\tcam = cv2.VideoCapture(self.src)\r\n\t\tres = img_size\r\n\t\tfpsl = []\r\n\t\twhile True:\r\n\t\t\tt = time()\r\n\t\t\tif skeleton:\r\n\t\t\t\tskel = np.zeros((img_size,img_size,3)).astype(np.uint8)\r\n\t\t\tret_val, img = cam.read()\r\n\t\t\timg = cv2.flip(img,1)\r\n\t\t\timg[:, self.cam_res[1]//2 - self.cam_res[0]//2:self.cam_res[1]//2 + self.cam_res[0]//2]\r\n\t\t\timg_res = cv2.resize(img, (res,res))\r\n\t\t\timg_yolo = np.copy(img_res)\r\n\t\t\timg_yolo = cv2.cvtColor(img_yolo, cv2.COLOR_BGR2RGB)\r\n\t\t\tresults = self.detect(img_yolo)\r\n\t\t\tresults_person = []\r\n\t\t\tfor i in range(len(results)):\r\n\t\t\t\tif results[i][0] == \'person\':\r\n\t\t\t\t\tresults_person.append(results[i])\r\n\t\t\tresults_person = self.nms(results_person, nms_thresh)\r\n\t\t\tfor box in results_person:\r\n\t\t\t\tclass_name = box[0]\r\n\t\t\t\tx = int(box[1])\r\n\t\t\t\ty = int(box[2])\r\n\t\t\t\tw = int(box[3] / 2)\r\n\t\t\t\th = int(box[4] / 2)\r\n\t\t\t\tprob = box[5]\r\n\t\t\t\tbbox = np.asarray((max(0,x-w), max(0, y-h), min(img_size-1, x+w), min(img_size-1, y+h)))\r\n\t\t\t\tif plt_b:\r\n\t\t\t\t\tcv2.rectangle(img_res, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\r\n\t\t\t\t\tcv2.rectangle(img_res, (bbox[0], bbox[1] - 20),(bbox[2], bbox[1]), (125, 125, 125), -1)\r\n\t\t\t\t\tcv2.putText(img_res, class_name + \' : %.2f\' % prob, (bbox[0] + 5, bbox[1] - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\r\n\t\t\t\tmaxl = np.max([w+0,h+0])\r\n\t\t\t\tlenghtarray = np.array([maxl/h, maxl/w])\r\n\t\t\t\tnbox = np.array([x-maxl, y-maxl, x+maxl, y+maxl])\r\n\t\t\t\tpadding = np.abs(nbox-bbox).astype(np.int)\r\n\t\t\t\timg_person = np.copy(img_yolo[bbox[1]:bbox[3],bbox[0]:bbox[2],:])\r\n\t\t\t\tpadd = np.array([[padding[1],padding[3]],[padding[0],padding[2]],[0,0]])\r\n\t\t\t\timg_person = np.pad(img_person, padd, mode = \'constant\')\r\n\t\t\t\timg_person = cv2.resize(img_person, (256,256))\r\n\t\t\t\thm = self.HG.Session.run(self.HG.pred_sigmoid, feed_dict={self.HG.img: np.expand_dims(img_person/255, axis= 0)})\r\n\t\t\t\tj = -1*np.ones(shape = (self.params[\'num_joints\'],2))\r\n\t\t\t\tjoint = -1*np.ones(shape = (self.params[\'num_joints\'],2))\r\n\t\t\t\tfor i in range(self.params[\'num_joints\']):\r\n\t\t\t\t\tidx = np.unravel_index(hm[0,:,:,i].argmax(), (64,64))\r\n\t\t\t\t\tif hm[0, idx[0], idx[1],i] > j_thresh:\r\n\t\t\t\t\t\tj[i] = idx\r\n\t\t\t\t\t\tjoint[i] = np.asarray(np.array([y,x]) + ((j[i]-32)/32 * np.array([h,w])* lenghtarray )).astype(np.int)\r\n\t\t\t\t\t\tif plt_j:\r\n\t\t\t\t\t\t\tcv2.circle(img_res, tuple(joint[i].astype(np.int))[::-1], radius = 5, color = self.color[i][::-1], thickness = -1)\r\n\t\t\t\tif plt_l:\r\n\t\t\t\t\tfor k in range(len(self.links)):\r\n\t\t\t\t\t\tl = self.links[k][\'link\']\r\n\t\t\t\t\t\tgood_link = True\r\n\t\t\t\t\t\tfor p in l:\r\n\t\t\t\t\t\t\tif np.array_equal(joint[p], [-1,-1]):\r\n\t\t\t\t\t\t\t\tgood_link = False\r\n\t\t\t\t\t\tif good_link:\r\n\t\t\t\t\t\t\tcv2.line(img_res, tuple(joint[l[0]][::-1].astype(np.int)), tuple(joint[l[1]][::-1].astype(np.int)), self.links[k][\'color\'][::-1], thickness = 3)\r\n\t\t\t\t\t\t\tif skeleton:\r\n\t\t\t\t\t\t\t\tcv2.line(skel, tuple(joint[l[0]][::-1].astype(np.int)), tuple(joint[l[1]][::-1].astype(np.int)), self.links[k][\'color\'][::-1], thickness = 3)\r\n\t\t\tt_f = time()\r\n\t\t\tcv2.putText(img_res, \'FPS: \' + str(1/(t_f-t))[:4], (60, 40), 2, 2, (0,0,0), thickness = 2)\r\n\t\t\tfpsl.append(1/(t_f-t))\r\n\t\t\tif skeleton:\r\n\t\t\t\timg_res = np.concatenate([img_res,skel], axis = 1)\r\n\t\t\tcv2.imshow(\'stream\', img_res)\r\n\t\t\tif cv2.waitKey(1) == 27:\r\n\t\t\t\tprint(\'Stream Ended\')\r\n\t\t\t\tcv2.destroyAllWindows()\r\n\t\t\t\treturn fpsl\r\n\t\t\t\tbreak\r\n\t\tcv2.destroyAllWindows()\r\n\t\tcam.release()\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t# ---------------------------------MPE MULTITHREAD--------------------------\r\n\tdef threadProcessing(self, box, img_size, j_thresh, plt_l, plt_j, plt_b):\r\n\t\t#if not coord.should_stop():\r\n\t\tclass_name = box[0]\r\n\t\tx = int(box[1])\r\n\t\ty = int(box[2])\r\n\t\tw = int(box[3] / 2)\r\n\t\th = int(box[4] / 2)\r\n\t\tprob = box[5]\r\n\t\tbbox = np.asarray((max(0,x-w), max(0, y-h), min(img_size-1, x+w), min(img_size-1, y+h)))\r\n\t\tif plt_b:\r\n\t\t\tcv2.rectangle(self.img_res, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\r\n\t\t\tcv2.rectangle(self.img_res, (bbox[0], bbox[1] - 20),(bbox[2], bbox[1]), (125, 125, 125), -1)\r\n\t\t\tcv2.putText(self.img_res, class_name + \' : %.2f\' % prob, (bbox[0] + 5, bbox[1] - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\r\n\t\tmaxl = np.max([w+0,h+0])\r\n\t\tlenghtarray = np.array([maxl/h, maxl/w])\r\n\t\tnbox = np.array([x-maxl, y-maxl, x+maxl, y+maxl])\r\n\t\tpadding = np.abs(nbox-bbox).astype(np.int)\r\n\t\timg_person = np.copy(self.img_yolo[bbox[1]:bbox[3],bbox[0]:bbox[2],:])\r\n\t\tpadd = np.array([[padding[1],padding[3]],[padding[0],padding[2]],[0,0]])\r\n\t\timg_person = np.pad(img_person, padd, mode = \'constant\')\r\n\t\timg_person = cv2.resize(img_person, (256,256))\r\n\t\thm = self.HG.Session.run(self.HG.pred_sigmoid, feed_dict={self.HG.img: np.expand_dims(img_person/255, axis= 0)})\r\n\t\tj = -1*np.ones(shape = (self.params[\'num_joints\'],2))\r\n\t\tjoint = -1*np.ones(shape = (self.params[\'num_joints\'],2))\r\n\t\tfor i in range(self.params[\'num_joints\']):\r\n\t\t\tidx = np.unravel_index(hm[0,:,:,i].argmax(), (64,64))\r\n\t\t\tif hm[0, idx[0], idx[1],i] > j_thresh:\r\n\t\t\t\tj[i] = idx\r\n\t\t\t\tjoint[i] = np.asarray(np.array([y,x]) + ((j[i]-32)/32 * np.array([h,w])* lenghtarray )).astype(np.int)\r\n\t\t\t\tif plt_j:\r\n\t\t\t\t\tcv2.circle(self.img_res, tuple(joint[i].astype(np.int))[::-1], radius = 5, color = self.color[i][::-1], thickness = -1)\r\n\t\tif plt_l:\r\n\t\t\tfor k in range(len(self.links)):\r\n\t\t\t\tl = self.links[k][\'link\']\r\n\t\t\t\tgood_link = True\r\n\t\t\t\tfor p in l:\r\n\t\t\t\t\tif np.array_equal(joint[p], [-1,-1]):\r\n\t\t\t\t\t\tgood_link = False\r\n\t\t\t\tif good_link:\r\n\t\t\t\t\tcv2.line(self.img_res, tuple(joint[l[0]][::-1].astype(np.int)), tuple(joint[l[1]][::-1].astype(np.int)), self.links[k][\'color\'][::-1], thickness = 3)\r\n\t#\tself.pprocessed += 1\r\n\t#\tif self.pprocessed == self.pnum:\r\n\t#\t\tcoord.request_stop()\r\n\t\r\n\t\r\n\tdef imgPrepare(self, img, res):\r\n\t\timg = cv2.flip(img,1)\r\n\t\timg[:, self.cam_res[1]//2 - self.cam_res[0]//2:self.cam_res[1]//2 + self.cam_res[0]//2]\r\n\t\tself.img_res = cv2.resize(img, (res,res))\r\n\t\tself.img_yolo = np.copy(self.img_res)\r\n\t\tself.img_yolo = cv2.cvtColor(self.img_yolo, cv2.COLOR_BGR2RGB)\r\n\t\r\n\tdef yoloPrepare(self, nms):\r\n\t\tresults = self.detect(self.img_yolo)\r\n\t\tresults_person = []\r\n\t\tfor i in range(len(results)):\r\n\t\t\tif results[i][0] == \'person\':\r\n\t\t\t\tresults_person.append(results[i])\r\n\t\tresults_person = self.nms(results_person, nms)\r\n\t\treturn results_person\r\n\t\r\n\tdef mpeThread(self, jThresh = 0.2, nms = 0.5, plt_l = True, plt_j = True, plt_b = True, img_size = 800, wait = 0.07):\r\n\t\tcam = cv2.VideoCapture(self.src)\r\n\t\tres = img_size\r\n\t\twhile True:\r\n\t\t\t#coord = tf.train.Coordinator()\r\n\t\t\tt = time()\r\n\t\t\tret_val, img = cam.read()\r\n\t\t\tself.img_res, self.img_yolo = self.imgPrepare(img, res)\r\n\t\t\tresults_person = self.yoloPrepare(nms)\r\n\t\t\tself.pnum = len(results_person)\r\n\t\t\tself.pprocessed = 0\r\n\t\t\t#workers = []\r\n\t\t\tfor box in results_person:\r\n\t\t\t\tThr = threading.Thread(target = self.threadProcessing, args = (box,img_size, jThresh, plt_l, plt_j, plt_b))\r\n\t\t\t\tThr.start()\r\n\t\t\t\t#workers.append(Thr)\r\n\t\t\tsleep(wait)\r\n\t\t\t#coord.join(workers)\r\n\t\t\t#for i in workers:\r\n\t\t\t#\ti.join()\r\n\t\t\tt_f = time()\r\n\t\t\tcv2.putText(self.img_res, \'FPS: \' + str(1/(t_f-t))[:4], (60, 40), 2, 2, (0,0,0), thickness = 2)\r\n\t\t\tcv2.imshow(\'stream\', self.img_res)\r\n\t\t\tif cv2.waitKey(1) == 27:\r\n\t\t\t\tprint(\'Stream Ended\')\r\n\t\t\t\tcv2.destroyAllWindows()\r\n\t\t\t\tbreak\r\n\t\tcv2.destroyAllWindows()\r\n\t\tcam.release()\r\n\t\r\n\t\r\n\t\r\n\t#---------------------------Conversion Method------------------------------\r\n\t\r\n\tdef videoDetection(self, src = None, outName = None, codec = \'DIVX\', j_thresh = 0.5, nms_thresh = 0.5, show = True, plt_j = True, plt_l = True, plt_b = True):\r\n\t\t"""""" Process Video with Pose Estimation\r\n\t\tArgs:\r\n\t\t\tsrc\t\t\t\t: Source (video path or 0 for webcam)\r\n\t\t\toutName\t\t: outName (set name of output file, set to None if you don\'t want to save)\r\n\t\t\tcodec\t\t\t: Codec to use for video compression (see OpenCV documentation)\r\n\t\t\tj_thresh\t\t: Joint Threshold\r\n\t\t\tnms_thresh\t: Non Maxima Suppression Threshold\r\n\t\t\tshow\t\t\t: (bool) True to show the video\r\n\t\t\tplt_j\t\t\t: (bool) Plot Body Joints as circles\r\n\t\t\tplt_l\t\t\t: (bool) Plot Limbs\r\n\t\t\tplt_b\t\t\t: (bool) Plot Bounding Boxes\r\n\t\t""""""\r\n\t\tcam = cv2.VideoCapture(src)\r\n\t\tshape = np.asarray((cam.get(cv2.CAP_PROP_FRAME_HEIGHT),cam.get(cv2.CAP_PROP_FRAME_WIDTH))).astype(np.int)\r\n\t\tframes = cam.get(cv2.CAP_PROP_FRAME_COUNT)\r\n\t\tfps = cam.get(cv2.CAP_PROP_FPS)\r\n\t\tif outName != None:\r\n\t\t\tfourcc = cv2.VideoWriter_fourcc(*codec)\r\n\t\t\toutVid = cv2.VideoWriter( outName, fourcc, fps, tuple(shape.astype(np.int))[::-1], 1)\r\n\t\tcur_frame = 0\r\n\t\tstartT = time()\r\n\t\twhile (cur_frame < frames or frames == -1) and outVid.isOpened():\r\n\t\t\tRECONSTRUCT_IMG = np.zeros((shape[0].astype(np.int),shape[1].astype(np.int),3))\r\n\t\t\tret_val, IMG_BASE = cam.read()\r\n\t\t\tWIDTH = shape[1].astype(np.int)\r\n\t\t\tHEIGHT = shape[0].astype(np.int)\r\n\t\t\tXC = WIDTH // 2\r\n\t\t\tYC = HEIGHT // 2\r\n\t\t\tif WIDTH > HEIGHT:\r\n\t\t\t\ttop = np.copy(IMG_BASE[:,:XC - HEIGHT //2])\r\n\t\t\t\tbottom = np.copy(IMG_BASE[:,XC + HEIGHT //2:])\r\n\t\t\t\timg_square = np.copy(IMG_BASE[:,XC - HEIGHT //2:XC + HEIGHT //2])\r\n\t\t\telif HEIGHT > WIDTH:\r\n\t\t\t\ttop = np.copy(IMG_BASE[:YC - WIDTH //2])\r\n\t\t\t\tbottom = np.copy(IMG_BASE[YC + WIDTH //2:])\r\n\t\t\t\timg_square = np.copy(IMG_BASE[YC - WIDTH //2:YC + WIDTH //2])\r\n\t\t\telse:\r\n\t\t\t\timg_square = np.copy(IMG_BASE)\r\n\t\t\timg_od = cv2.cvtColor(np.copy(img_square), cv2.COLOR_BGR2RGB)\r\n\t\t\tshapeOd = img_od.shape\r\n\t\t\tresults = self.detect(img_od)\r\n\t\t\tresults_person = []\r\n\t\t\tfor i in range(len(results)):\r\n\t\t\t\tif results[i][0] == \'person\':\r\n\t\t\t\t\tresults_person.append(results[i])\r\n\t\t\tresults_person = self.nms(results_person, nms_thresh)\r\n\t\t\tfor box in results_person:\r\n\t\t\t\tclass_name = box[0]\r\n\t\t\t\tx = int(box[1])\r\n\t\t\t\ty = int(box[2])\r\n\t\t\t\tw = int(box[3] / 2)\r\n\t\t\t\th = int(box[4] / 2)\r\n\t\t\t\tprob = box[5]\r\n\t\t\t\tbbox = np.asarray((max(0,x-w), max(0, y-h), min(shapeOd[1]-1, x+w), min(shapeOd[0]-1, y+h)))\r\n\t\t\t\tif plt_b:\r\n\t\t\t\t\tcv2.rectangle(img_square, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\r\n\t\t\t\t\tcv2.rectangle(img_square, (bbox[0], bbox[1] - 20),(bbox[2], bbox[1]), (125, 125, 125), -1)\r\n\t\t\t\t\tcv2.putText(img_square, class_name + \' : %.2f\' % prob, (bbox[0] + 5, bbox[1] - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\r\n\t\t\t\tmaxl = np.max([w+0,h+0])\r\n\t\t\t\tlenghtarray = np.array([maxl/h, maxl/w])\r\n\t\t\t\tnbox = np.array([x-maxl, y-maxl, x+maxl, y+maxl])\r\n\t\t\t\tpadding = np.abs(nbox-bbox).astype(np.int)\r\n\t\t\t\timg_person = np.copy(img_od[bbox[1]:bbox[3],bbox[0]:bbox[2],:])\r\n\t\t\t\tpadd = np.array([[padding[1],padding[3]],[padding[0],padding[2]],[0,0]])\r\n\t\t\t\timg_person = np.pad(img_person, padd, mode = \'constant\')\r\n\t\t\t\timg_person = cv2.resize(img_person, (256,256))\r\n\t\t\t\thm = self.HG.Session.run(self.HG.pred_sigmoid, feed_dict={self.HG.img: np.expand_dims(img_person/255, axis= 0)})\r\n\t\t\t\tj = -1*np.ones(shape = (self.params[\'num_joints\'],2))\r\n\t\t\t\tjoint = -1*np.ones(shape = (self.params[\'num_joints\'],2))\r\n\t\t\t\tfor i in range(self.params[\'num_joints\']):\r\n\t\t\t\t\tidx = np.unravel_index(hm[0,:,:,i].argmax(), (64,64))\r\n\t\t\t\t\tif hm[0, idx[0], idx[1],i] > j_thresh:\r\n\t\t\t\t\t\tj[i] = idx\r\n\t\t\t\t\t\tjoint[i] = np.asarray(np.array([y,x]) + ((j[i]-32)/32 * np.array([h,w])* lenghtarray )).astype(np.int)\r\n\t\t\t\t\t\tif plt_j:\r\n\t\t\t\t\t\t\tcv2.circle(img_square, tuple(joint[i].astype(np.int))[::-1], radius = 5, color = self.color[i][::-1], thickness = -1)\r\n\t\t\t\tif plt_l:\r\n\t\t\t\t\tfor k in range(len(self.links)):\r\n\t\t\t\t\t\tl = self.links[k][\'link\']\r\n\t\t\t\t\t\tgood_link = True\r\n\t\t\t\t\t\tfor p in l:\r\n\t\t\t\t\t\t\tif np.array_equal(joint[p], [-1,-1]):\r\n\t\t\t\t\t\t\t\tgood_link = False\r\n\t\t\t\t\t\tif good_link:\r\n\t\t\t\t\t\t\tcv2.line(img_square, tuple(joint[l[0]][::-1].astype(np.int)), tuple(joint[l[1]][::-1].astype(np.int)), self.links[k][\'color\'][::-1], thickness = 3)\r\n\t\t\tif WIDTH > HEIGHT:\r\n\t\t\t\tRECONSTRUCT_IMG[:,:XC - HEIGHT //2] = top\r\n\t\t\t\tRECONSTRUCT_IMG[:,XC + HEIGHT //2:] = bottom\r\n\t\t\t\tRECONSTRUCT_IMG[:,XC - HEIGHT //2: XC + HEIGHT //2] = img_square\r\n\t\t\telif HEIGHT < WIDTH:\r\n\t\t\t\tRECONSTRUCT_IMG[:YC - WIDTH //2] = top\r\n\t\t\t\tRECONSTRUCT_IMG[YC + WIDTH //2:] = bottom\r\n\t\t\t\tRECONSTRUCT_IMG[YC - WIDTH //2:YC + WIDTH //2] = img_square\r\n\t\t\telse:\r\n\t\t\t\tRECONSTRUCT_IMG = img_square.astype(np.uint8)\r\n\t\t\tRECONSTRUCT_IMG = RECONSTRUCT_IMG.astype(np.uint8)\r\n\t\t\toutVid.write(np.uint8(RECONSTRUCT_IMG))\r\n\t\t\tcur_frame = cur_frame + 1\t\t\r\n\t\t\tif frames != -1:\r\n\t\t\t\tpercent = ((cur_frame+1)/frames) * 100\r\n\t\t\t\tnum = np.int(20*percent/100)\r\n\t\t\t\ttToEpoch = int((time() - startT) * (100 - percent)/(percent))\r\n\t\t\t\tsys.stdout.write(\'\\r Processing: {0}>\'.format(""=""*num) + ""{0}>"".format("" ""*(20-num)) + \'||\' + str(percent)[:4] + \'%\' + \' -timeToEnd: \' + str(tToEpoch) + \' sec.\')\r\n\t\t\t\tsys.stdout.flush()\r\n\t\t\tif show:\r\n\t\t\t\tcv2.imshow(\'stream\', RECONSTRUCT_IMG)\r\n\t\t\tif cv2.waitKey(1) == 27:\r\n\t\t\t\tprint(\'Stream Ended\')\r\n\t\t\t\tcv2.destroyAllWindows()\r\n\t\t\t\tcam.release()\r\n\t\t\t\tprint(time() - startT)\r\n\t\t\t\t#if outName != None:\r\n\t\t\t\t\t#outVid.release()\r\n\t\tcv2.destroyAllWindows()\r\n\t\tcam.release()\r\n\t\tif outName != None:\r\n\t\t\tprint(outVid.isOpened())\r\n\t\t\toutVid.release()\r\n\t\t\tprint(outVid.isOpened())\r\n\t\tprint(time() - startT)\t\r\n\r\n\t\r\n\t #-------------------------Benchmark Methods (PCK)-------------------------\r\n\t\r\n\tdef pcki(self, joint_id, gtJ, prJ, idlh = 3, idrs = 12):\r\n\t\t"""""" Compute PCK accuracy on a given joint\r\n\t\tArgs:\r\n\t\t\tjoint_id\t: Index of the joint considered\r\n\t\t\tgtJ\t\t\t: Ground Truth Joint\r\n\t\t\tprJ\t\t\t: Predicted Joint\r\n\t\t\tidlh\t\t: Index of Normalizer (Left Hip on PCK, neck on PCKh)\r\n\t\t\tidrs\t\t: Index of Normalizer (Right Shoulder on PCK, top head on PCKh)\r\n\t\tReturns:\r\n\t\t\t(float) NORMALIZED L2 ERROR\r\n\t\t""""""\r\n\t\treturn np.linalg.norm(gtJ[joint_id]-prJ[joint_id][::-1]) / np.linalg.norm(gtJ[idlh]-gtJ[idrs])\r\n\t\t\r\n\tdef pck(self, weight, gtJ, prJ, gtJFull, boxL, idlh = 3, idrs = 12):\r\n\t\t"""""" Compute PCK accuracy for a sample\r\n\t\tArgs:\r\n\t\t\tweight\t\t: Index of the joint considered\r\n\t\t\tgtJFull\t: Ground Truth (sampled on whole image)\r\n\t\t\tgtJ\t\t\t: Ground Truth (sampled on reduced image)\r\n\t\t\tprJ\t\t\t: Prediction\r\n\t\t\tboxL\t\t: Box Lenght\r\n\t\t\tidlh\t\t: Index of Normalizer (Left Hip on PCK, neck on PCKh)\r\n\t\t\tidrs\t\t: Index of Normalizer (Right Shoulder on PCK, top head on PCKh)\r\n\t\t""""""\r\n\t\tfor i in range(len(weight)):\r\n\t\t\tif weight[i] == 1:\r\n\t\t\t\tself.ratio_pck.append(self.pcki(i, gtJ, prJ, idlh=idlh, idrs = idrs))\r\n\t\t\t\tself.ratio_pck_full.append(self.pcki(i, gtJFull, np.asarray(prJ / 255 * boxL)))\r\n\t\t\t\tself.pck_id.append(i)\r\n\t\r\n\tdef compute_pck(self, datagen, idlh = 3, idrs = 12, testSet = None):\r\n\t\t"""""" Compute PCK on dataset\r\n\t\tArgs:\r\n\t\t\tdatagen\t: (DataGenerator)\r\n\t\t\tidlh\t\t: Index of Normalizer (Left Hip on PCK, neck on PCKh)\r\n\t\t\tidrs\t\t: Index of Normalizer (Right Shoulder on PCK, top head on PCKh)\r\n\t\t""""""\r\n\t\tdatagen.pck_ready(idlh = idlh, idrs = idrs, testSet = testSet)\r\n\t\tself.ratio_pck = []\r\n\t\tself.ratio_pck_full = []\r\n\t\tself.pck_id = []\r\n\t\tsamples = len(datagen.pck_samples)\r\n\t\tstartT = time()\r\n\t\tfor idx, sample in enumerate(datagen.pck_samples):\r\n\t\t\tpercent = ((idx+1)/samples) * 100\r\n\t\t\tnum = np.int(20*percent/100)\r\n\t\t\ttToEpoch = int((time() - startT) * (100 - percent)/(percent))\r\n\t\t\tsys.stdout.write(\'\\r PCK : {0}>\'.format(""=""*num) + ""{0}>"".format("" ""*(20-num)) + \'||\' + str(percent)[:4] + \'%\' + \' -timeToEnd: \' + str(tToEpoch) + \' sec.\')\r\n\t\t\tsys.stdout.flush()\r\n\t\t\tres = datagen.getSample(sample)\r\n\t\t\tif res != False:\r\n\t\t\t\timg, gtJoints, w, gtJFull, boxL = res\r\n\t\t\t\tprJoints = self.joints_pred_numpy(np.expand_dims(img/255, axis = 0), coord = \'img\', thresh = 0)\r\n\t\t\t\tself.pck(w, gtJoints, prJoints, gtJFull, boxL, idlh=idlh, idrs = idrs)\r\n\t\tprint(\'Done in \', int(time() - startT), \'sec.\')\r\n\t\t\t\r\n\t#-------------------------Object Detector (YOLO)-------------------------\r\n\t\r\n\t# YOLO MODEL\r\n\t# Source : https://github.com/hizhangp/yolo_tensorflow\r\n\t# Author : Peng Zhang (https://github.com/hizhangp/)\r\n\t# yolo_init, iou, detect, detect_from_cvmat, interpret_output are methods\r\n\t# to the credit of Peng Zhang\r\n\tdef yolo_init(self):\r\n\t\t""""""YOLO Initializer\r\n\t\tInitialize the YOLO Model\r\n\t\t""""""\r\n\t\tt = time()\r\n\t\tself.classes = cfg.CLASSES\r\n\t\tself.num_class = len(self.classes)\r\n\t\tself.image_size = cfg.IMAGE_SIZE\r\n\t\tself.cell_size = cfg.CELL_SIZE\r\n\t\tself.boxes_per_cell = cfg.BOXES_PER_CELL\r\n\t\tself.threshold = cfg.THRESHOLD\r\n\t\tself.iou_threshold = cfg.IOU_THRESHOLD\r\n\t\tself.boundary1 = self.cell_size * self.cell_size * self.num_class\r\n\t\tself.boundary2 = self.boundary1 + self.cell_size * self.cell_size * self.boxes_per_cell\r\n\t\twith self.graph.as_default():\r\n\t\t\tself.net = YOLONet(is_training=False)\r\n\t\tprint(\'YOLO created: \', time() - t, \' sec.\')\r\n\t\r\n\tdef restore_yolo(self, load = \'yolo_small.ckpt\'):\r\n\t\t"""""" Restore Weights\r\n\t\tArgs:\r\n\t\t\tload : File to load\r\n\t\t""""""\r\n\t\tprint(\'Loading YOLO...\')\r\n\t\tprint(\'Restoring weights from: \' + load)\r\n\t\tt = time()\r\n\t\twith self.graph.as_default():\r\n\t\t\tself.saver = tf.train.Saver(tf.contrib.framework.get_trainable_variables(scope=\'yolo\'))\r\n\t\t\tself.saver.restore(self.HG.Session, load)\r\n\t\tprint(\'Trained YOLO Loaded: \', time() - t, \' sec.\')\r\n\t\r\n\t\r\n\tdef iou(self, box1, box2):\r\n\t\t"""""" Intersection over Union (IoU)\r\n\t\tArgs:\r\n\t\t\tbox1 : Bounding Box\r\n\t\t\tbox2 : Bounding Box\r\n\t\tReturns:\r\n\t\t\tIoU\r\n\t\t""""""\r\n\t\ttb = min(box1[0] + 0.5 * box1[2], box2[0] + 0.5 * box2[2]) - max(box1[0] - 0.5 * box1[2], box2[0] - 0.5 * box2[2])\r\n\t\tlr = min(box1[1] + 0.5 * box1[3], box2[1] + 0.5 * box2[3]) - max(box1[1] - 0.5 * box1[3], box2[1] - 0.5 * box2[3])\r\n\t\tif tb < 0 or lr < 0:\r\n\t\t\tintersection = 0\r\n\t\telse:\r\n\t\t\tintersection = tb * lr\r\n\t\treturn intersection / (box1[2] * box1[3] + box2[2] * box2[3] - intersection)\r\n\t\r\n\tdef detect(self, img):\r\n\t\t"""""" Method for Object Detection\r\n\t\tArgs:\r\n\t\t\timg\t\t\t: Input Image (BGR Image)\r\n\t\tReturns:\r\n\t\t\tresult\t\t: List of Bounding Boxes\r\n\t\t""""""\r\n\t\timg_h, img_w, _ = img.shape\r\n\t\tinputs = cv2.resize(img, (self.image_size, self.image_size))\r\n\t\tinputs = cv2.cvtColor(inputs, cv2.COLOR_BGR2RGB).astype(np.float32)\r\n\t\tinputs = (inputs / 255.0) * 2.0 - 1.0\r\n\t\tinputs = np.reshape(inputs, (1, self.image_size, self.image_size, 3))\r\n\t\tresult = self.detect_from_cvmat(inputs)[0]\r\n\t\tfor i in range(len(result)):\r\n\t\t\tresult[i][1] *= (1.0 * img_w / self.image_size)\r\n\t\t\tresult[i][2] *= (1.0 * img_h / self.image_size)\r\n\t\t\tresult[i][3] *= (1.0 * img_w / self.image_size)\r\n\t\t\tresult[i][4] *= (1.0 * img_h / self.image_size)\r\n\t\treturn result\r\n\t\r\n\tdef detect_from_cvmat(self, inputs):\r\n\t\t"""""" Runs detection on Session (TENSORFLOW RELATED)\r\n\t\t""""""\r\n\t\tnet_output = self.HG.Session.run(self.net.logits,feed_dict={self.net.images: inputs})\r\n\t\tresults = []\r\n\t\tfor i in range(net_output.shape[0]):\r\n\t\t\tresults.append(self.interpret_output(net_output[i]))\r\n\t\treturn results\r\n\t\r\n\tdef interpret_output(self, output):\r\n\t\t"""""" Post Process the Output of the network\r\n\t\tArgs:\r\n\t\t\toutput : Network Prediction (Tensor)\r\n\t\t""""""\r\n\t\tprobs = np.zeros((self.cell_size, self.cell_size, self.boxes_per_cell, self.num_class))\r\n\t\tclass_probs = np.reshape(output[0:self.boundary1], (self.cell_size, self.cell_size, self.num_class))\r\n\t\tscales = np.reshape(output[self.boundary1:self.boundary2], (self.cell_size, self.cell_size, self.boxes_per_cell))\r\n\t\tboxes = np.reshape(output[self.boundary2:], (self.cell_size, self.cell_size, self.boxes_per_cell, 4))\r\n\t\toffset = np.transpose(np.reshape(np.array([np.arange(self.cell_size)] * self.cell_size * self.boxes_per_cell),[self.boxes_per_cell, self.cell_size, self.cell_size]), (1, 2, 0))\r\n\t\tboxes[:, :, :, 0] += offset\r\n\t\tboxes[:, :, :, 1] += np.transpose(offset, (1, 0, 2))\r\n\t\tboxes[:, :, :, :2] = 1.0 * boxes[:, :, :, 0:2] / self.cell_size\r\n\t\tboxes[:, :, :, 2:] = np.square(boxes[:, :, :, 2:])\r\n\t\tboxes *= self.image_size\r\n\t\tfor i in range(self.boxes_per_cell):\r\n\t\t\tfor j in range(self.num_class):\r\n\t\t\t\tprobs[:, :, i, j] = np.multiply(class_probs[:, :, j], scales[:, :, i])\r\n\t\tfilter_mat_probs = np.array(probs >= self.threshold, dtype=\'bool\')\r\n\t\tfilter_mat_boxes = np.nonzero(filter_mat_probs)\r\n\t\tboxes_filtered = boxes[filter_mat_boxes[0],filter_mat_boxes[1], filter_mat_boxes[2]]\r\n\t\tprobs_filtered = probs[filter_mat_probs]\r\n\t\tclasses_num_filtered = np.argmax(filter_mat_probs, axis=3)[filter_mat_boxes[0], filter_mat_boxes[1], filter_mat_boxes[2]]\r\n\t\targsort = np.array(np.argsort(probs_filtered))[::-1]\r\n\t\tboxes_filtered = boxes_filtered[argsort]\r\n\t\tprobs_filtered = probs_filtered[argsort]\r\n\t\tclasses_num_filtered = classes_num_filtered[argsort]\r\n\t\tfor i in range(len(boxes_filtered)):\r\n\t\t\tif probs_filtered[i] == 0:\r\n\t\t\t\tcontinue\r\n\t\t\tfor j in range(i + 1, len(boxes_filtered)):\r\n\t\t\t\tif self.iou(boxes_filtered[i], boxes_filtered[j]) > self.iou_threshold:\r\n\t\t\t\t\tprobs_filtered[j] = 0.0\r\n\t\tfilter_iou = np.array(probs_filtered > 0.0, dtype=\'bool\')\r\n\t\tboxes_filtered = boxes_filtered[filter_iou]\r\n\t\tprobs_filtered = probs_filtered[filter_iou]\r\n\t\tclasses_num_filtered = classes_num_filtered[filter_iou]\r\n\t\tresult = []\r\n\t\tfor i in range(len(boxes_filtered)):\r\n\t\t\tresult.append([self.classes[classes_num_filtered[i]], boxes_filtered[i][0], boxes_filtered[i][1], boxes_filtered[i][2], boxes_filtered[i][3], probs_filtered[i]])\r\n\t\treturn result\r\n\t\r\n\tdef nms(self, boxes, overlapThresh):\r\n\t\t"""""" Non Maxima Suppression\r\n\t\tArgs:\r\n\t\t\tboxes\t\t\t\t\t: List of Bounding Boxes\r\n\t\t\toverlapThreshold\t: Non Maxima Suppression Threshold\r\n\t\tReturns:\r\n\t\t\tret\t\t\t\t\t\t: List of processed Bounding Boxes\r\n\t\t""""""\r\n\t\tif len(boxes) == 0:\r\n\t\t\treturn []\r\n\t\tarray = []\r\n\t\tfor i in range(len(boxes)):\r\n\t\t\tarray.append(boxes[i][1:5])\r\n\t\tarray = np.array(array)\r\n\t\tpick = []\r\n\t\tx = array[:,0]\r\n\t\ty = array[:,1]\r\n\t\tw = array[:,2]\r\n\t\th = array[:,3]\r\n\t\tx1 = x - w / 2\r\n\t\tx2 = x + w / 2\r\n\t\ty1 = y - h / 2\r\n\t\ty2 = y + h / 2\r\n\t\tarea = (x2 - x1 + 1) * (y2 - y1 + 1)\r\n\t\tidxs = np.argsort(y2)\r\n\t\twhile len(idxs) > 0:\r\n\t\t\tlast = len(idxs) - 1\r\n\t\t\ti = idxs[last]\r\n\t\t\tpick.append(i)\r\n\t\t\txx1 = np.maximum(x1[i], x1[idxs[:last]])\r\n\t\t\tyy1 = np.maximum(y1[i], y1[idxs[:last]])\r\n\t\t\txx2 = np.minimum(x2[i], x2[idxs[:last]])\r\n\t\t\tyy2 = np.minimum(y2[i], y2[idxs[:last]])\r\n\t\t\tw = np.maximum(0, xx2 - xx1 + 1)\r\n\t\t\th = np.maximum(0, yy2 - yy1 + 1)\r\n\t\t\toverlap = (w * h) / area[idxs[:last]]\r\n\t\t\tidxs = np.delete(idxs, np.concatenate(([last],np.where(overlap > overlapThresh)[0])))\r\n\t\tret = []\r\n\t\tfor i in pick:\r\n\t\t\tret.append(boxes[i])\r\n\t\treturn ret\r\n\t\r\n\tdef camera_detector(self, cap, wait=10, mirror = True):\r\n\t\t"""""" YOLO Webcam Detector\r\n\t\tArgs:\r\n\t\t\tcap\t\t\t: Video Capture (OpenCv Related)\r\n\t\t\twait\t\t: Time between frames\r\n\t\t\tmirror\t\t: Apply mirror Effect\r\n\t\t""""""\r\n\t\twhile True:\r\n\t\t\tt = time()\r\n\t\t\tret, frame = cap.read()\r\n\t\t\tif mirror:\r\n\t\t\t\tframe = cv2.flip(frame, 1)\r\n\t\t\tresult = self.detect(frame)\r\n\t\t\tshapeOd = frame.shape\r\n\t\t\tfor box in result:\r\n\t\t\t\tclass_name = box[0]\r\n\t\t\t\tx = int(box[1])\r\n\t\t\t\ty = int(box[2])\r\n\t\t\t\tw = int(box[3] / 2)\r\n\t\t\t\th = int(box[4] / 2)\r\n\t\t\t\tprob = box[5]\r\n\t\t\t\tbbox = np.asarray((max(0,x-w), max(0, y-h), min(shapeOd[1]-1, x+w), min(shapeOd[0]-1, y+h)))\r\n\t\t\t\tcv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\r\n\t\t\t\tcv2.rectangle(frame, (bbox[0], bbox[1] - 20),(bbox[2], bbox[1]), (125, 125, 125), -1)\r\n\t\t\t\tcv2.putText(frame, class_name + \' : %.2f\' % prob, (bbox[0] + 5, bbox[1] - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\r\n\t\t\tfps = 1/(time() - t)\r\n\t\t\tcv2.putText(frame, str(fps)[:4] + \' fps\', (20, 20), 2, 1, (255,255,255), thickness = 2)\r\n\t\t\tcv2.imshow(\'Camera\', frame)\r\n\t\t\tret, frame = cap.read()\r\n\t\t\tif cv2.waitKey(1) == 27:\r\n\t\t\t\tprint(\'Stream Ended\')\r\n\t\t\t\tcv2.destroyAllWindows()\r\n\t\t\t\tcap.release()\r\n\t\tcv2.destroyAllWindows()\r\n\t\tcap.release()\r\n\t\t\r\n\tdef person_detector(self, wait=10, mirror = True, plot = True):\r\n\t\t"""""" YOLO Webcam Detector\r\n\t\tArgs:\r\n\t\t\tcap\t\t\t: Video Capture (OpenCv Related)\r\n\t\t\twait\t\t: Time between frames\r\n\t\t\tmirror\t\t: Apply mirror Effect\r\n\t\t""""""\r\n\t\tcap = cv2.VideoCapture(0)\r\n\t\twhile True:\r\n\t\t\tt = time()\r\n\t\t\tret, frame = cap.read()\r\n\t\t\tif mirror:\r\n\t\t\t\tframe = cv2.flip(frame, 1)\r\n\t\t\tresult_all = self.detect(frame)\r\n\t\t\tresult = []\r\n\t\t\tfor i in range(len(result_all)):\r\n\t\t\t\tif result_all[i][0] == \'person\':\r\n\t\t\t\t\tresult.append(result_all[i])\r\n\t\t\tshapeOd = frame.shape\r\n\t\t\tif plot:\r\n\t\t\t\tfor box in result:\r\n\t\t\t\t\tclass_name = box[0]\r\n\t\t\t\t\tx = int(box[1])\r\n\t\t\t\t\ty = int(box[2])\r\n\t\t\t\t\tw = int(box[3] / 2)\r\n\t\t\t\t\th = int(box[4] / 2)\r\n\t\t\t\t\tprob = box[5]\r\n\t\t\t\t\tbbox = np.asarray((max(0,x-w), max(0, y-h), min(shapeOd[1]-1, x+w), min(shapeOd[0]-1, y+h)))\r\n\t\t\t\t\tcv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\r\n\t\t\t\t\tcv2.rectangle(frame, (bbox[0], bbox[1] - 20),(bbox[2], bbox[1]), (125, 125, 125), -1)\r\n\t\t\t\t\tcv2.putText(frame, class_name + \' : %.2f\' % prob, (bbox[0] + 5, bbox[1] - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\r\n\t\t\tfps = 1/(time() - t)\r\n\t\t\tcv2.putText(frame, str(fps)[:4] + \' fps\' + \' PinS:\' + str(len(result)), (20, 20), 2, 1, (0,0,0), thickness = 2)\r\n\t\t\tcv2.imshow(\'Camera\', frame)\r\n\t\t\tret, frame = cap.read()\r\n\t\t\tif cv2.waitKey(1) == 27:\r\n\t\t\t\tprint(\'Stream Ended\')\r\n\t\t\t\tcv2.destroyAllWindows()\r\n\t\t\t\tcap.release()\r\n\t\tcv2.destroyAllWindows()\r\n\t\tcap.release()\r\n\t\t\r\nif __name__ == \'__main__\':\r\n\tt = time()\r\n\tparams = process_config(\'configTiny.cfg\')\r\n\tpredict = PredictProcessor(params)\r\n\tpredict.color_palette()\r\n\tpredict.LINKS_JOINTS()\r\n\tpredict.model_init()\r\n\tpredict.load_model(load = \'hg_refined_tiny_200\')\r\n\tpredict.yolo_init()\r\n\tpredict.restore_yolo(load = \'YOLO_small.ckpt\')\r\n\tpredict._create_prediction_tensor()\r\n\tprint(\'Done: \', time() - t, \' sec.\')'"
timer.py,0,"b""import time, datetime\r\n\r\nclass Timer(object):\r\n    '''\r\n    A simple timer.\r\n    '''\r\n    def __init__(self):\r\n        self.init_time = time.time()\r\n        self.total_time = 0.\r\n        self.calls = 0\r\n        self.start_time = 0.\r\n        self.diff = 0.\r\n        self.average_time = 0.\r\n        self.remain_time = 0.\r\n\r\n    def tic(self):\r\n        # using time.time instead of time.clock because time time.clock\r\n        # does not normalize for multithreading\r\n        self.start_time = time.time()\r\n\r\n    def toc(self, average=True):\r\n        self.diff = time.time() - self.start_time\r\n        self.total_time += self.diff\r\n        self.calls += 1\r\n        self.average_time = self.total_time / self.calls\r\n        if average:\r\n            return self.average_time\r\n        else:\r\n            return self.diff\r\n\r\n    def remain(self, iters, max_iters):\r\n        if iters == 0:\r\n            self.remain_time = 0\r\n        else:\r\n            self.remain_time = (time.time() - self.init_time) * \\\r\n                                (max_iters - iters) / iters\r\n        return str(datetime.timedelta(seconds=int(self.remain_time)))\r\n"""
train_launcher.py,0,"b'""""""\r\nTRAIN LAUNCHER \r\n\r\n""""""\r\n\r\nimport configparser\r\nfrom hourglass_tiny import HourglassModel\r\nfrom datagen import DataGenerator\r\n\r\ndef process_config(conf_file):\r\n\t""""""\r\n\t""""""\r\n\tparams = {}\r\n\tconfig = configparser.ConfigParser()\r\n\tconfig.read(conf_file)\r\n\tfor section in config.sections():\r\n\t\tif section == \'DataSetHG\':\r\n\t\t\tfor option in config.options(section):\r\n\t\t\t\tparams[option] = eval(config.get(section, option))\r\n\t\tif section == \'Network\':\r\n\t\t\tfor option in config.options(section):\r\n\t\t\t\tparams[option] = eval(config.get(section, option))\r\n\t\tif section == \'Train\':\r\n\t\t\tfor option in config.options(section):\r\n\t\t\t\tparams[option] = eval(config.get(section, option))\r\n\t\tif section == \'Validation\':\r\n\t\t\tfor option in config.options(section):\r\n\t\t\t\tparams[option] = eval(config.get(section, option))\r\n\t\tif section == \'Saver\':\r\n\t\t\tfor option in config.options(section):\r\n\t\t\t\tparams[option] = eval(config.get(section, option))\r\n\treturn params\r\n\r\n\r\nif __name__ == \'__main__\':\r\n\tprint(\'--Parsing Config File\')\r\n\tparams = process_config(\'config.cfg\')\r\n\t\r\n\tprint(\'--Creating Dataset\')\r\n\tdataset = DataGenerator(params[\'joint_list\'], params[\'img_directory\'], params[\'training_txt_file\'], remove_joints=params[\'remove_joints\'])\r\n\tdataset._create_train_table()\r\n\tdataset._randomize()\r\n\tdataset._create_sets()\r\n\t\r\n\tmodel = HourglassModel(nFeat=params[\'nfeats\'], nStack=params[\'nstacks\'], nModules=params[\'nmodules\'], nLow=params[\'nlow\'], outputDim=params[\'num_joints\'], batch_size=params[\'batch_size\'], attention = params[\'mcam\'],training=True, drop_rate= params[\'dropout_rate\'], lear_rate=params[\'learning_rate\'], decay=params[\'learning_rate_decay\'], decay_step=params[\'decay_step\'], dataset=dataset, name=params[\'name\'], logdir_train=params[\'log_dir_train\'], logdir_test=params[\'log_dir_test\'], tiny= params[\'tiny\'], w_loss=params[\'weighted_loss\'] , joints= params[\'joint_list\'],modif=False)\r\n\tmodel.generate_model()\r\n\tmodel.training_init(nEpochs=params[\'nepochs\'], epochSize=params[\'epoch_size\'], saveStep=params[\'saver_step\'], dataset = None)\r\n\t\r\n'"
yolo_net.py,62,"b'import numpy as np\nimport tensorflow as tf\nimport config as cfg\n\nslim = tf.contrib.slim\n\n\nclass YOLONet(object):\n\n    def __init__(self, is_training=True):\n        self.classes = cfg.CLASSES\n        self.num_class = len(self.classes)\n        self.image_size = cfg.IMAGE_SIZE\n        self.cell_size = cfg.CELL_SIZE\n        self.boxes_per_cell = cfg.BOXES_PER_CELL\n        self.output_size = (self.cell_size * self.cell_size) * (self.num_class + self.boxes_per_cell * 5)\n        self.scale = 1.0 * self.image_size / self.cell_size\n        self.boundary1 = self.cell_size * self.cell_size * self.num_class\n        self.boundary2 = self.boundary1 + self.cell_size * self.cell_size * self.boxes_per_cell\n\n        self.object_scale = cfg.OBJECT_SCALE\n        self.noobject_scale = cfg.NOOBJECT_SCALE\n        self.class_scale = cfg.CLASS_SCALE\n        self.coord_scale = cfg.COORD_SCALE\n\n        self.learning_rate = cfg.LEARNING_RATE\n        self.batch_size = cfg.BATCH_SIZE\n        self.alpha = cfg.ALPHA\n\n        self.offset = np.transpose(np.reshape(np.array(\n            [np.arange(self.cell_size)] * self.cell_size * self.boxes_per_cell),\n            (self.boxes_per_cell, self.cell_size, self.cell_size)), (1, 2, 0))\n\n        self.images = tf.placeholder(tf.float32, [None, self.image_size, self.image_size, 3], name=\'images\')\n        self.logits = self.build_network(self.images, num_outputs=self.output_size, alpha=self.alpha, is_training=is_training)\n\n        if is_training:\n            self.labels = tf.placeholder(tf.float32, [None, self.cell_size, self.cell_size, 5 + self.num_class])\n            self.loss_layer(self.logits, self.labels)\n            self.total_loss = tf.losses.get_total_loss()\n            tf.summary.scalar(\'total_loss\', self.total_loss)\n\n    def build_network(self,\n                      images,\n                      num_outputs,\n                      alpha,\n                      keep_prob=0.5,\n                      is_training=True,\n                      scope=\'yolo\'):\n        with tf.variable_scope(scope):\n            with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                                activation_fn=leaky_relu(alpha),\n                                weights_initializer=tf.truncated_normal_initializer(0.0, 0.01),\n                                weights_regularizer=slim.l2_regularizer(0.0005)):\n                net = tf.pad(images, np.array([[0, 0], [3, 3], [3, 3], [0, 0]]), name=\'pad_1\')\n                net = slim.conv2d(net, 64, 7, 2, padding=\'VALID\', scope=\'conv_2\')\n                net = slim.max_pool2d(net, 2, padding=\'SAME\', scope=\'pool_3\')\n                net = slim.conv2d(net, 192, 3, scope=\'conv_4\')\n                net = slim.max_pool2d(net, 2, padding=\'SAME\', scope=\'pool_5\')\n                net = slim.conv2d(net, 128, 1, scope=\'conv_6\')\n                net = slim.conv2d(net, 256, 3, scope=\'conv_7\')\n                net = slim.conv2d(net, 256, 1, scope=\'conv_8\')\n                net = slim.conv2d(net, 512, 3, scope=\'conv_9\')\n                net = slim.max_pool2d(net, 2, padding=\'SAME\', scope=\'pool_10\')\n                net = slim.conv2d(net, 256, 1, scope=\'conv_11\')\n                net = slim.conv2d(net, 512, 3, scope=\'conv_12\')\n                net = slim.conv2d(net, 256, 1, scope=\'conv_13\')\n                net = slim.conv2d(net, 512, 3, scope=\'conv_14\')\n                net = slim.conv2d(net, 256, 1, scope=\'conv_15\')\n                net = slim.conv2d(net, 512, 3, scope=\'conv_16\')\n                net = slim.conv2d(net, 256, 1, scope=\'conv_17\')\n                net = slim.conv2d(net, 512, 3, scope=\'conv_18\')\n                net = slim.conv2d(net, 512, 1, scope=\'conv_19\')\n                net = slim.conv2d(net, 1024, 3, scope=\'conv_20\')\n                net = slim.max_pool2d(net, 2, padding=\'SAME\', scope=\'pool_21\')\n                net = slim.conv2d(net, 512, 1, scope=\'conv_22\')\n                net = slim.conv2d(net, 1024, 3, scope=\'conv_23\')\n                net = slim.conv2d(net, 512, 1, scope=\'conv_24\')\n                net = slim.conv2d(net, 1024, 3, scope=\'conv_25\')\n                net = slim.conv2d(net, 1024, 3, scope=\'conv_26\')\n                net = tf.pad(net, np.array([[0, 0], [1, 1], [1, 1], [0, 0]]), name=\'pad_27\')\n                net = slim.conv2d(net, 1024, 3, 2, padding=\'VALID\', scope=\'conv_28\')\n                net = slim.conv2d(net, 1024, 3, scope=\'conv_29\')\n                net = slim.conv2d(net, 1024, 3, scope=\'conv_30\')\n                net = tf.transpose(net, [0, 3, 1, 2], name=\'trans_31\')\n                net = slim.flatten(net, scope=\'flat_32\')\n                net = slim.fully_connected(net, 512, scope=\'fc_33\')\n                net = slim.fully_connected(net, 4096, scope=\'fc_34\')\n                net = slim.dropout(net, keep_prob=keep_prob,\n                                   is_training=is_training, scope=\'dropout_35\')\n                net = slim.fully_connected(net, num_outputs,\n                                           activation_fn=None, scope=\'fc_36\')\n        return net\n\n    def calc_iou(self, boxes1, boxes2, scope=\'iou\'):\n        """"""calculate ious\n        Args:\n          boxes1: 4-D tensor [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL, 4]  ====> (x_center, y_center, w, h)\n          boxes2: 1-D tensor [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL, 4] ===> (x_center, y_center, w, h)\n        Return:\n          iou: 3-D tensor [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n        """"""\n        with tf.variable_scope(scope):\n            boxes1 = tf.stack([boxes1[:, :, :, :, 0] - boxes1[:, :, :, :, 2] / 2.0,\n                               boxes1[:, :, :, :, 1] - boxes1[:, :, :, :, 3] / 2.0,\n                               boxes1[:, :, :, :, 0] + boxes1[:, :, :, :, 2] / 2.0,\n                               boxes1[:, :, :, :, 1] + boxes1[:, :, :, :, 3] / 2.0])\n            boxes1 = tf.transpose(boxes1, [1, 2, 3, 4, 0])\n\n            boxes2 = tf.stack([boxes2[:, :, :, :, 0] - boxes2[:, :, :, :, 2] / 2.0,\n                               boxes2[:, :, :, :, 1] - boxes2[:, :, :, :, 3] / 2.0,\n                               boxes2[:, :, :, :, 0] + boxes2[:, :, :, :, 2] / 2.0,\n                               boxes2[:, :, :, :, 1] + boxes2[:, :, :, :, 3] / 2.0])\n            boxes2 = tf.transpose(boxes2, [1, 2, 3, 4, 0])\n\n            # calculate the left up point & right down point\n            lu = tf.maximum(boxes1[:, :, :, :, :2], boxes2[:, :, :, :, :2])\n            rd = tf.minimum(boxes1[:, :, :, :, 2:], boxes2[:, :, :, :, 2:])\n\n            # intersection\n            intersection = tf.maximum(0.0, rd - lu)\n            inter_square = intersection[:, :, :, :, 0] * intersection[:, :, :, :, 1]\n\n            # calculate the boxs1 square and boxs2 square\n            square1 = (boxes1[:, :, :, :, 2] - boxes1[:, :, :, :, 0]) * \\\n                (boxes1[:, :, :, :, 3] - boxes1[:, :, :, :, 1])\n            square2 = (boxes2[:, :, :, :, 2] - boxes2[:, :, :, :, 0]) * \\\n                (boxes2[:, :, :, :, 3] - boxes2[:, :, :, :, 1])\n\n            union_square = tf.maximum(square1 + square2 - inter_square, 1e-10)\n\n        return tf.clip_by_value(inter_square / union_square, 0.0, 1.0)\n\n    def loss_layer(self, predicts, labels, scope=\'loss_layer\'):\n        with tf.variable_scope(scope):\n            predict_classes = tf.reshape(predicts[:, :self.boundary1], [self.batch_size, self.cell_size, self.cell_size, self.num_class])\n            predict_scales = tf.reshape(predicts[:, self.boundary1:self.boundary2], [self.batch_size, self.cell_size, self.cell_size, self.boxes_per_cell])\n            predict_boxes = tf.reshape(predicts[:, self.boundary2:], [self.batch_size, self.cell_size, self.cell_size, self.boxes_per_cell, 4])\n\n            response = tf.reshape(labels[:, :, :, 0], [self.batch_size, self.cell_size, self.cell_size, 1])\n            boxes = tf.reshape(labels[:, :, :, 1:5], [self.batch_size, self.cell_size, self.cell_size, 1, 4])\n            boxes = tf.tile(boxes, [1, 1, 1, self.boxes_per_cell, 1]) / self.image_size\n            classes = labels[:, :, :, 5:]\n\n            offset = tf.constant(self.offset, dtype=tf.float32)\n            offset = tf.reshape(offset, [1, self.cell_size, self.cell_size, self.boxes_per_cell])\n            offset = tf.tile(offset, [self.batch_size, 1, 1, 1])\n            predict_boxes_tran = tf.stack([(predict_boxes[:, :, :, :, 0] + offset) / self.cell_size,\n                                           (predict_boxes[:, :, :, :, 1] + tf.transpose(offset, (0, 2, 1, 3))) / self.cell_size,\n                                           tf.square(predict_boxes[:, :, :, :, 2]),\n                                           tf.square(predict_boxes[:, :, :, :, 3])])\n            predict_boxes_tran = tf.transpose(predict_boxes_tran, [1, 2, 3, 4, 0])\n\n            iou_predict_truth = self.calc_iou(predict_boxes_tran, boxes)\n\n            # calculate I tensor [BATCH_SIZE, CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n            object_mask = tf.reduce_max(iou_predict_truth, 3, keep_dims=True)\n            object_mask = tf.cast((iou_predict_truth >= object_mask), tf.float32) * response\n\n            # calculate no_I tensor [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n            noobject_mask = tf.ones_like(object_mask, dtype=tf.float32) - object_mask\n\n            boxes_tran = tf.stack([boxes[:, :, :, :, 0] * self.cell_size - offset,\n                                   boxes[:, :, :, :, 1] * self.cell_size - tf.transpose(offset, (0, 2, 1, 3)),\n                                   tf.sqrt(boxes[:, :, :, :, 2]),\n                                   tf.sqrt(boxes[:, :, :, :, 3])])\n            boxes_tran = tf.transpose(boxes_tran, [1, 2, 3, 4, 0])\n\n            # class_loss\n            class_delta = response * (predict_classes - classes)\n            class_loss = tf.reduce_mean(tf.reduce_sum(tf.square(class_delta), axis=[1, 2, 3]), name=\'class_loss\') * self.class_scale\n\n            # object_loss\n            object_delta = object_mask * (predict_scales - iou_predict_truth)\n            object_loss = tf.reduce_mean(tf.reduce_sum(tf.square(object_delta), axis=[1, 2, 3]), name=\'object_loss\') * self.object_scale\n\n            # noobject_loss\n            noobject_delta = noobject_mask * predict_scales\n            noobject_loss = tf.reduce_mean(tf.reduce_sum(tf.square(noobject_delta), axis=[1, 2, 3]), name=\'noobject_loss\') * self.noobject_scale\n\n            # coord_loss\n            coord_mask = tf.expand_dims(object_mask, 4)\n            boxes_delta = coord_mask * (predict_boxes - boxes_tran)\n            coord_loss = tf.reduce_mean(tf.reduce_sum(tf.square(boxes_delta), axis=[1, 2, 3, 4]), name=\'coord_loss\') * self.coord_scale\n\n            tf.losses.add_loss(class_loss)\n            tf.losses.add_loss(object_loss)\n            tf.losses.add_loss(noobject_loss)\n            tf.losses.add_loss(coord_loss)\n\n            tf.summary.scalar(\'class_loss\', class_loss)\n            tf.summary.scalar(\'object_loss\', object_loss)\n            tf.summary.scalar(\'noobject_loss\', noobject_loss)\n            tf.summary.scalar(\'coord_loss\', coord_loss)\n\n            tf.summary.histogram(\'boxes_delta_x\', boxes_delta[:, :, :, :, 0])\n            tf.summary.histogram(\'boxes_delta_y\', boxes_delta[:, :, :, :, 1])\n            tf.summary.histogram(\'boxes_delta_w\', boxes_delta[:, :, :, :, 2])\n            tf.summary.histogram(\'boxes_delta_h\', boxes_delta[:, :, :, :, 3])\n            tf.summary.histogram(\'iou\', iou_predict_truth)\n\n\ndef leaky_relu(alpha):\n    def op(inputs):\n        return tf.maximum(alpha * inputs, inputs, name=\'leaky_relu\')\n    return op\n'"
v0.1/hourglassModel.py,18,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nCreated on Sun May 28 03:32:16 2017\r\n\r\n@author: Walid Benbihi\r\n@mail : w.benbihi (at) gmail.com\r\n""""""\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom layers import conv2d, convBnrelu, residual\r\n\r\n################################################\r\n#\t             Hourglass Model\r\n################################################\r\n\r\nclass HourglassModel():\r\n\t\r\n\tdef __init__(self, nbStacks = 8, nFeat = 256, nModules = 1, outDim = 16, nLow = 3, training = True, name = \'stacked_hourglass\'):\r\n\t\t""""""\r\n\t\t\targs:\r\n\t\t\t\tnbStack \t: (int) number of (single) hourlgass modules stacked together\r\n\t\t\t\tname \t \t: (str) Name of the layer (TensorFlow useful)\r\n\t\t\t\tnModules \t: (int)\r\n\t\t\t\toutDim \t: (int) number of output dimension (body joints)\r\n\t\t\t\ttrain \t \t: (bool) trigger Dropout layers (avoid overfitting)\r\n\t\t""""""\r\n\t\tself.nbStack = nbStacks\r\n\t\tself.name = name\r\n\t\tself.nFeat = nFeat\r\n\t\tself.nModules = nModules\r\n\t\tself.outDim = outDim\r\n\t\tself.train = training\r\n\t\tself.nLow = nLow\r\n\t\t\r\n\tdef __call__(self, inputs):\r\n\t\twith tf.name_scope(self.name):\r\n\t\t\twith tf.name_scope(\'preprocessing\'):\r\n\t\t\t\tpad_1 = tf.pad(inputs, np.array([[0,0],[2,2],[2,2],[0,0]]))\r\n\t\t\t\tconv_1 = conv2d(pad_1, 64, kernel_size=6, strides = 2, name = \'256to128\')\r\n\t\t\t\tres_1 = residual(conv_1, 128)\r\n\t\t\t\tpool_1 = tf.contrib.layers.max_pool2d(res_1, [2,2], [2,2], padding= \'VALID\')\r\n\t\t\t\tres_2 = residual(pool_1, 128)\r\n\t\t\t\tres_3 = residual(res_2, self.nFeat)\r\n\t\t\t# Supervision Table\r\n\t\t\thg = [None] * self.nbStack\r\n\t\t\tll = [None] * self.nbStack\r\n\t\t\tll_ = [None] * self.nbStack\r\n\t\t\tdrop = [None] * self.nbStack\r\n\t\t\tout = [None] * self.nbStack\r\n\t\t\tout_ = [None] * self.nbStack\r\n\t\t\tsum_ = [None] * self.nbStack\r\n\t\t\twith tf.name_scope(\'stacks\'):\r\n\t\t\t\twith tf.name_scope(\'hourglass.1\'):\r\n\t\t\t\t\thg[0] = self.hourglass(res_3, self.nLow, self.nFeat, \'hourglass\')\r\n\t\t\t\t\tll[0] = convBnrelu(hg[0], self.nFeat, name= \'conv_1\')\r\n\t\t\t\t\tll_[0] = conv2d(ll[0],self.nFeat,1,1,\'VALID\',\'ll\')\r\n\t\t\t\t\tdrop[0] = tf.layers.dropout(ll_[0], rate = 0.1, training = self.train)\r\n\t\t\t\t\tout[0] = conv2d(ll[0],self.outDim,1,1,\'VALID\',\'out\')\r\n\t\t\t\t\tout_[0] = conv2d(out[0],self.nFeat,1,1,\'VALID\',\'out_\')\r\n\t\t\t\t\tsum_[0] = tf.add_n([drop[0], out_[0], res_3])\r\n\t\t\t\tfor i in range(1, self.nbStack-1):\r\n\t\t\t\t\twith tf.name_scope(\'hourglass.\' + str(i+1)):\r\n\t\t\t\t\t\thg[i] = self.hourglass(sum_[i-1], self.nLow, self.nFeat, \'hourglass\')\r\n\t\t\t\t\t\tll[i] = convBnrelu(hg[i], self.nFeat, name=\'conv_1\')\r\n\t\t\t\t\t\tll_[i] = conv2d(ll[i],self.nFeat,1,1,\'VALID\',\'ll\')\r\n\t\t\t\t\t\tdrop[i] = tf.layers.dropout(ll_[i],rate=0.1, training = self.train)\r\n\t\t\t\t\t\tout[i] = conv2d(ll[i],self.outDim,1,1,\'VALID\',\'out\')\r\n\t\t\t\t\t\tout_[i] = conv2d(out[i],self.nFeat,1,1,\'VALID\',\'out_\')\r\n\t\t\t\t\t\tsum_[i] = tf.add_n([drop[i], out_[i], sum_[i-1]])\r\n\t\t\t\twith tf.name_scope(\'hourglass.\' + str(self.nbStack)):\r\n\t\t\t\t\thg[self.nbStack-1] = self.hourglass(sum_[self.nbStack - 2], self.nLow, self.nFeat, \'hourglass\')\r\n\t\t\t\t\tll[self.nbStack-1] = convBnrelu(hg[self.nbStack - 1], self.nFeat, name=\'conv_1\')\r\n\t\t\t\t\tdrop[self.nbStack-1] = tf.layers.dropout(ll[self.nbStack-1], rate=0.1, training = self.train)\r\n\t\t\t\t\tout[self.nbStack-1] = conv2d(drop[self.nbStack-1],self.outDim,1,1,\'VALID\', \'out\')\r\n\t\t\treturn tf.stack(out, name = \'output\')\r\n\t\t\r\n\tdef hourglass(self, inputs, n, numOut, name = \'hourglass\'):\r\n\t\twith tf.name_scope(name):\r\n\t\t\tup_1 = residual(inputs, numOut, name = \'up1\')\r\n\t\t\tlow_ = tf.contrib.layers.max_pool2d(inputs, [2,2],[2,2], \'VALID\')\r\n\t\t\tlow_1 = residual(low_, numOut, name = \'low1\')\r\n\t\t\tif n > 0:\r\n\t\t\t\tlow_2 = self.hourglass(low_1, n-1, numOut, name=\'low2\')\r\n\t\t\telse:\r\n\t\t\t\tlow_2 = residual(low_1, numOut, name=\'low2\')\r\n\t\t\tlow_3 = residual(low_2, numOut, name = \'low3\')\r\n\t\t\tup_2 = tf.image.resize_nearest_neighbor(low_3, tf.shape(low_3)[1:3]*2, name= \'upsampling\')\r\n\t\t\treturn tf.add([up_1, up_2])\r\n'"
v0.1/layers.py,26,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nCreated on Sun May 28 04:21:16 2017\r\n\r\n@author: Walid Benbihi\r\n@mail : w.benbihi (at) gmail.com\r\n""""""\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\n""""""\r\n\tTensorBoard Local Run Windows : http://192.168.56.1:6006/ \r\n\t\r\n\tDOCUMENTATION:\r\n\t\tCalculus for Convolution Dimension (W-K+2*P)/S + 1:\r\n\t\t\tW: input size, K: kernel size, P: padding, S: stride\r\n""""""\r\n\r\ndef conv2d(inputs, filters, kernel_size = 1, strides = 1, pad = \'VALID\', name = None):\r\n\t""""""\r\n\t\tCreate a Convolutional Layer\r\n\t\targs :\r\n\t\t\tinputs \t: (tensor) input Tensor\r\n\t\t\tfilters \t: (int) number of filters\r\n\t\t\tkernel_size : (int) size of the kernel\r\n\t\t\tstrides \t: (int) Value of stride\r\n\t\t\tpad \t \t: (\'VALID\'/\'SAME\')\r\n\t\treturn :\r\n\t\t\ttf.Tensor\r\n\t""""""\r\n\twith tf.name_scope(name):\r\n\t\tkernel = tf.Variable(tf.contrib.layers.xavier_initializer(uniform=False)([kernel_size,kernel_size, inputs.get_shape().as_list()[3], filters]), name= \'weights\')\r\n\t\tconv = tf.nn.conv2d(inputs, kernel, [1,strides,strides,1], padding=pad, data_format=\'NHWC\')\r\n\t\twith tf.device(\'/cpu:0\'):\r\n\t\t\ttf.summary.histogram(\'weights_summary\', kernel, collections = [\'train\'])\r\n\t\treturn conv\r\n\t\r\ndef convBnrelu(inputs, filters, kernel_size = 1, strides = 1, name = None):\r\n\t""""""\r\n\t\tCreate a Convolutional Layer + Batch Normalization + ReLU Activation \r\n\t\targs :\r\n\t\t\tinputs \t: (tf.Tensor) input Tensor\r\n\t\t\tfilters \t: (int) number of filters\r\n\t\t\tkernel_size : (int) size of the kernel\r\n\t\t\tstrides \t: (int) Value of stride\r\n\t\t\tpad \t \t: (\'VALID\'/\'SAME\')\r\n\t\treturn :\r\n\t\t\ttf.Tensor\r\n\t""""""\r\n\twith tf.name_scope(name):\r\n\t\tkernel = tf.Variable(tf.contrib.layers.xavier_initializer(uniform=False)([kernel_size,kernel_size, inputs.get_shape().as_list()[3], filters]), name= \'weights\')\r\n\t\tconv = tf.nn.conv2d(inputs, kernel, [1,strides,strides,1], padding=\'VALID\', data_format=\'NHWC\')\r\n\t\tnorm = tf.contrib.layers.batch_norm(conv, 0.9, epsilon=1e-5, activation_fn = tf.nn.relu, scope = \'_bn_relu\')\r\n\t\twith tf.device(\'/cpu:0\'):\r\n\t\t\ttf.summary.histogram(\'weights_summary\', kernel, collections = [\'train\'])\r\n\t\treturn norm\r\n\t\r\ndef convBlock(inputs, numOut, name = \'convBlock\'):\r\n\t""""""\r\n\t\tCreate a Convolutional Block Layer for Residual Units\r\n\t\targs:\r\n\t\t\tinputs : (tf.Tensor)\r\n\t\t\tnumOut : (int) number of output channels\r\n\t\treturn :\r\n\t\t\ttf.Tensor\r\n\t""""""\r\n\t# DIMENSION CONSERVED\r\n\twith tf.name_scope(name):\r\n\t\tnorm_1 = tf.contrib.layers.batch_norm(inputs, 0.9, epsilon=1e-5, activation_fn = tf.nn.relu)\r\n\t\tconv_1 = conv2d(norm_1, int(numOut/2), kernel_size=1, strides=1, pad = \'VALID\')\r\n\t\tnorm_2 = tf.contrib.layers.batch_norm(conv_1, 0.9, epsilon=1e-5, activation_fn = tf.nn.relu)\r\n\t\tpad = tf.pad(norm_2, np.array([[0,0],[1,1],[1,1],[0,0]]))\r\n\t\tconv_2 = conv2d(pad, int(numOut/2), kernel_size=3, strides=1, pad = \'VALID\')\r\n\t\tnorm_3 = tf.contrib.layers.batch_norm(conv_2, 0.9, epsilon=1e-5, activation_fn = tf.nn.relu)\r\n\t\tconv_3 = conv2d(norm_3, int(numOut), kernel_size=1, strides=1, pad = \'VALID\')\r\n\t\treturn conv_3\r\n\t\r\ndef skipLayer(inputs, numOut, name = \'skipLayer\'):\r\n\t""""""\r\n\t\tCreate a skip layer : Identity if number of input channel = numOut, convolution else\r\n\t\targs :\r\n\t\t\tinputs : (tf.Tensor)\r\n\t\t\tnumOut : (int)\r\n\t\treturn :\r\n\t\t\ttf.Tensor\r\n\t""""""\r\n\t# DIMENSION CONSERVED\r\n\twith tf.name_scope(name):\r\n\t\tif inputs.get_shape().as_list()[3] == numOut:\r\n\t\t\treturn inputs\r\n\t\telse:\r\n\t\t\tconv = conv2d(inputs, numOut, kernel_size=1, strides=1, name= \'skipLayer-conv\')\r\n\t\t\treturn conv\r\n\t\r\ndef residual(inputs, numOut, name = \'residual\'):\r\n\t# DIMENSION CONSERVED\r\n\twith tf.name_scope(name):\r\n\t\tconvb = convBlock(inputs, numOut)\r\n\t\tskip = skipLayer(inputs,numOut)\r\n\t\treturn tf.add_n([convb,skip])\r\n'"
v0.1/params.py,0,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nCreated on Sun May 28 17:39:00 2017\r\n\r\n@author: Walid Benbihi\r\n@mail: w.benbihi (at) gmail.com\r\n""""""\r\n\r\n########################################\r\n#\t           Parameters\r\n########################################\r\n\r\n\r\n# Directory (do not forget the last \'/\')\r\n\r\nimg_dir \t\t\t= \'~/images/\'\t\t  # Directory of image dataset (.png, .jpg)\r\ndata_dir \t\t\t= \'~/data/\'\t\t    # Directory of .csv files\r\ntrain_dir \t\t= \'~/logs/train/\'\t# Path to save training logs\r\ntest_dir \t\t\t= \'~/logs/test/\'\t# Path to save testing logs\r\nprocessed_dir \t= \'~/arrays/\'\t\t# Directory of processed images (.npy)\r\n\r\n# TensorFlow Parameters\r\n\r\ngpu = \'/gpu:0\'\t\t\t# Indicates which GPU to use (can be replace by a CPU)\r\ncpu = \'/cpu:0\'\t\t\t# Indicates which CPU to use (only use CPU)\r\n\r\n\r\n# Training Parameters\r\n\r\nlearning_rate \t\t= 2.5e-4 \t  # Learning Rate \r\nnEpochs \t\t\t\t  = 30\t\t    # Number of epochs\r\niter_by_epoch \t\t= 1000\t\t  # Number of batch to train in one epoch\r\nbatch_size \t\t\t  = 16\t\t    # Batch Size per iteration\r\nlimit_train_test \t= 24000\t    # Index of separation between training and testing set\r\n\r\nstep_to_save \t\t  = 500\t  # Step to save summaries on TensorBoard (should be lower than iter_by_epoch)\r\nrandom_angle_max \t= 30\t\t# Max of random rotation\r\nrandom_angle_min \t= -30\t\t# Min of random rotation\r\n\r\n# Hourglass Parameters\r\n\r\nnbStacks \t\t= 8\t \t\t\t\t# Number of stacks\r\noutDim \t\t  = 16\t\t\t\t# Number of output channels (how many joints)\r\nnFeat \t\t\t= 256\t\t\t\t# Number of feature channels \r\nnLow \t\t\t  = 4 \t\t\t\t# Number of downsampling by stacks (3 or 4 for better results)\r\nnModule \t\t= 1\t\t\t\t\t# Number of upsampling iterations (Not implemented yet)\r\n\r\n\r\n\r\n\r\n\r\n\r\n'"
v0.1/process.py,0,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nCreated on Sun May 28 03:28:07 2017\r\n\r\n@author: Walid Benbihi\r\n@mail : w.benbihi (at) gmail.com\r\n""""""\r\n\r\n###########################################\r\n#              PROCESS DATA\r\n###########################################\r\n\r\n# Import\r\nimport csv\r\nimport os\r\nimport numpy as np\r\nimport params\r\nimport tools\r\n\r\n\r\n\r\nfolder = params.img_dir\r\npath = params.data_dir\r\nsummarytrain = params.train_dir\r\nsummarytest = params.test_dir\r\narraypath = params.processed_dir\r\n\r\n\r\ndef openCsv(path, filename):\r\n\t""""""\r\n\t\tOpen a .csv file and convert it as a numpy array\r\n\t\tWARNING: csv file should contain only numbers\r\n\t\tDATASET : MPII only\r\n\t\targs : \r\n\t\t\tpath : (str) path to folder\r\n\t\t\tfilename: (str) name of .csv file\r\n\t\treturn : \r\n\t\t\tnp.array\r\n\t""""""\r\n\treturn np.array(np.genfromtxt(path + filename, delimiter = \',\'), np.float32)[0:]\r\n\r\n\r\n# Creation of Joints Position Arrays:\r\n\r\njoint00 = openCsv(path, \'00_rAnckle.csv\')\r\njoint01 = openCsv(path, \'01_rKnee.csv\')\r\njoint02 = openCsv(path, \'02_rHip.csv\')\r\njoint03 = openCsv(path, \'03_lAnckle.csv\')\r\njoint04 = openCsv(path, \'04_lKnee.csv\')\r\njoint05 = openCsv(path, \'05_lHip.csv\')\r\njoint06 = openCsv(path, \'06_pelvis.csv\')\r\njoint07 = openCsv(path, \'07_thorax.csv\')\r\njoint08 = openCsv(path, \'08_neck.csv\')\r\njoint09 = openCsv(path, \'09_head.csv\')\r\njoint10 = openCsv(path, \'10_rWrist.csv\')\r\njoint11 = openCsv(path, \'11_rElbow.csv\')\r\njoint12 = openCsv(path, \'12_rShoulder.csv\')\r\njoint13 = openCsv(path, \'13_lWrist.csv\')\r\njoint14 = openCsv(path, \'14_lElbow.csv\')\r\njoint15 = openCsv(path, \'15_lShoulder.csv\')\r\n\r\ndef getPosition(index):\r\n\t""""""\r\n\t\tGiven an Image Index, returns an array of body joints position\r\n\t\tDATASET : MPII only\r\n\t\targs :\r\n\t\t\tindex : (str) index of the image\r\n\t\treturn :\r\n\t\t\tnp.array : size 16x2 (body joints x coordinates)\r\n\t""""""\r\n\tpos = np.zeros((16,2))\r\n\tpos[0,:] = joint00[index,:]\r\n\tpos[1,:] = joint01[index,:]\r\n\tpos[2,:] = joint02[index,:]\r\n\tpos[3,:] = joint03[index,:]\r\n\tpos[4,:] = joint04[index,:]\r\n\tpos[5,:] = joint05[index,:]\r\n\tpos[6,:] = joint06[index,:]\r\n\tpos[7,:] = joint07[index,:]\r\n\tpos[8,:] = joint08[index,:]\r\n\tpos[9,:] = joint09[index,:]\r\n\tpos[10,:] = joint10[index,:]\r\n\tpos[11,:] = joint11[index,:]\r\n\tpos[12,:] = joint12[index,:]\r\n\tpos[13,:] = joint13[index,:]\r\n\tpos[14,:] = joint14[index,:]\r\n\tpos[15,:] = joint15[index,:]\r\n\treturn pos\r\n\r\n\r\ndef cleanList(L):\r\n\t""""""\r\n\t\tGiven a list L returns a list without duplicates\r\n\t\tDATASET : MPII only\r\n\t\targs:\r\n\t\t\tL : (list(str))\r\n\t\treturn:\r\n\t\t\tlist\r\n\t""""""\r\n\tclean = []\r\n\tfor i in range(len(L)):\r\n\t\tif not(L[i][:9] in clean):\r\n\t\t\tclean.append(L[i][9])\r\n\treturn clean\r\n\r\n\r\ndef toTrainList():\r\n\t""""""\r\n\t\tProcedure to generate intel about dataset\r\n\t\tDATASET : MPII only\r\n\t\treturn :\r\n\t\t\tlabels  : (list) list of labels (0/1) between testing and training data\r\n\t\t\timList  : (list) list of all numpy arrays in the \'arraypath\'\r\n\t\t\tname    : (list) list of all images indexed on the dataset\r\n\t\t\ttoTrain : (list) list of all images\' index with label 1 and in the \'arraypath\' directoty\t\t\t\r\n\t""""""\r\n\tlabels = np.array(np.genfromtxt( path + \'tset.csv\', delimiter = \',\'), np.uint8)[1:]\r\n\twith open(path + \'name.csv\', \'r\') as file:\r\n\t\treader = csv.reader(file)\r\n\t\tname = list(reader)[1:]\r\n\t\timList = os.listdir(arraypath)\r\n\t\ttoTrain = []\r\n\t\tfor i in range(len(imList)):\r\n\t\t\tif labels[name.index(imList[i])]:\r\n\t\t\t\ttoTrain.append(imList[i])\r\n\t\treturn labels, imList, name, toTrain\r\n\t\r\n\t\r\n'"
v0.1/tools.py,0,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nCreated on Sun May 28 17:59:37 2017\r\n\r\n@author: Walid Benbihi\r\n@mail: w.benbihi (at) gmail.com\r\n""""""\r\nimport numpy as np\r\nfrom skimage import transform\r\n\r\ndef fullTestSet(test, name, weight):\r\n\t""""""\r\n\t\tFilter the test list to salect only the test images with no missing values\r\n\t\targs :\r\n\t\t\ttest \t : (list) List of testing set\r\n\t\t\tname \t : (list) List of all images on the dataset\r\n\t\t\tweight : (list) List of weight (0 if missing value else 1)\r\n\t""""""\r\n\tfinal = []\r\n\tfor i in range(len(test)):\r\n\t\tif np.array_equal(weight[name.index([test[i]+\'.jpg\'])], np.ones((16,),np.uint8)):\r\n\t\t\tfinal.append(test[i])\r\n\treturn final\r\n\r\ndef rotatehm(hm, angle):\r\n\t""""""\r\n\t\tGiven a heatMap, returns a rotated heatMap\r\n\t\targs : \r\n\t\t\thm \t \t: (numpy.array) heatMap\r\n\t\t\tangle : (int) Angle\r\n\t""""""\r\n\trot_hm = np.zeros((16,64,64))\r\n\tfor i in range(16):\r\n\t\trot_hm[i] = transform.rotate(hm[i],angle)\r\n\treturn rot_hm\r\n\r\n\r\ndef modifyOutput(hm, stack):\r\n\t""""""\r\n\t\tGiven a heatMap, returns repeated stacked heatMaps as many time as \'stack\'\r\n\t\targs :\r\n\t\t\thm \t \t : (numpy.array) heatMap\r\n\t\t\tstacks : (int) number of stacks\r\n\t""""""\r\n\thm_rolled = np.rollaxis(hm, 0, 3)\r\n\thm_full = np.zeros((stack,1,64,64,16))\r\n\tfor i in range(stack):\r\n\t\thm_full[i,0,:,:,:] = hm_rolled\r\n\treturn hm_full\r\n'"
v0.1/trainer.py,27,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nCreated on Sun May 28 05:08:22 2017\r\n\r\n@author: Walid Benbihi\r\n@mail : w.benbihi (at) gmail.com\r\n""""""\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\nfrom hourglassModel import HourglassModel\r\nfrom time import time, strftime\r\nfrom random import shuffle, choice\r\nimport params\r\nimport process\r\nfrom tools import fullTestSet, modifyOutput, rotatehm\r\nimport sys\r\nfrom skimage.transform import rotate\r\n\r\n\r\n\r\n\r\nGPU = params.gpu\r\nCPU = params.cpu\r\n\r\ndate = strftime(\'%Y.%m.%d\')\r\nlr = params.learning_rate\r\nnepochs = params.nEpochs\r\nepochiter = params.iter_by_epoch\r\n_,_,name,_ = process.toTrainList()\r\nweights = np.array(np.genfromtxt(process.path + \'weight_joint.csv\', delimiter=\',\'), np.uint8)\r\nimageOnDisk = process.cleanList(os.listdir(process.arraypath))\r\nshuffle(imageOnDisk)\r\ntrainingData = imageOnDisk[:params.limit_train_test]\r\ntestingData = imageOnDisk[params.limit_train_test:]\r\ntestingData = fullTestSet(testingData, name, weights)\r\nbatchSize = params.batch_size\r\n\r\n\r\nif __name__ == \'__main__\':\r\n\twith tf.device(GPU):\r\n\t\tprint(\'Creating Model\')\r\n\t\tt_start = time()\r\n\t\twith tf.name_scope(\'inputs\'):\r\n\t\t\tx = tf.placeholder(tf.float32, [None, 256,256,3], name = \'x_train\')\r\n\t\t\ty = tf.placeholder(tf.float32, [params.nbStacks,None, 64,64, params.outDim], name= \'y_train\')\r\n\t\tprint(\'--Inputs : Done\')\r\n\t\twith tf.name_scope(\'model\'):\r\n\t\t\toutput = HourglassModel(params.nbStacks, params.nFeat, params.nModule, params.outDim, params.nLow, True, name = \'stacked_hourglass\')\r\n\t\tprint(\'--Model : Done\')\r\n\t\twith tf.name_scope(\'loss\'):\r\n\t\t\tloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output, labels= y), name = \'cross_entropy_loss\') *4096 * params.nbStacks * params.batch_size\r\n\t\tprint(\'--Loss : Done\')\r\n\t\twith tf.name_scope(\'rmsprop_optimizer\'):\r\n\t\t\trmsprop = tf.train.RMSPropOptimizer(lr)\r\n\t\tprint(\'--Optim : Done\')\r\n\twith tf.name_scope(\'steps\'):\r\n\t\ttrain_steps = tf.Variable(0, trainable=False)\r\n\twith tf.device(GPU):\r\n\t\twith tf.name_scope(\'minimize\'):\r\n\t\t\ttrain_rmsprop = rmsprop.minimize(loss, train_steps)\r\n\t\tprint(\'--Minimizer : Done\')\r\n\tinit = tf.global_variables_initializer()\r\n\tprint(\'--Init : Done\')\r\n\tprint(\'Model generation: \' + str(time()- t_start))\r\n\twith tf.name_scope(\'loss\'):\r\n\t\ttf.summary.scalar(\'loss\', loss, collections = [\'train\'])\r\n\tmerged_summary_op = tf.summary.merge_all(\'train\')\r\n\twith tf.name_scope(\'Session\'):\r\n\t\twith tf.device(GPU):\r\n\t\t\tsess = tf.Session()\r\n\t\t\tsess.run(init)\r\n\t\t\tprint(\'Session initilized\')\r\n\t\twith tf.device(CPU):\r\n\t\t\tsaver = tf.train.Saver()\r\n\t\twith tf.device(GPU):\r\n\t\t\tsummary_train = tf.summary.FileWriter(process.summarytrain , tf.get_default_graph())\r\n\t\t\tt_train_start = time()\r\n\t\t\tprint(\'Start training\')\r\n\t\t\twith tf.name_scope(\'training\'):\r\n\t\t\t\tfor epoch in range(nepochs):\r\n\t\t\t\t\tt_epoch_start = time()\r\n\t\t\t\t\tavg_cost = 0.\r\n\t\t\t\t\tprint(\'========Training Epoch: \', (epoch + 1))\r\n\t\t\t\t\twith tf.name_scope(\'epoch_\' + str(epoch)):\r\n\t\t\t\t\t\tfor i in range(epochiter):\r\n\t\t\t\t\t\t\tpercent = ((i+1)/epochiter )*100\r\n\t\t\t\t\t\t\tnum = np.int(20*percent/100)\r\n\t\t\t\t\t\t\tsys.stdout.write(""\\r Train: {0}>"".format(""=""*num) + ""{0}>"".format("" ""*(20-num)) + ""||"" + str(percent)[:3] + ""%"" + \' time :\' + str(time()- t_epoch_start)[:5] + \'sec  loss =\' + str(avg_cost)[:6])\r\n\t\t\t\t\t\t\tsys.stdout.flush()\r\n\t\t\t\t\t\t\ty_batch = np.zeros((params.nbStacks, batchSize, 64,64,16))\r\n\t\t\t\t\t\t\tx_batch = np.zeros((batchSize,256,256,3))\r\n\t\t\t\t\t\t\twith tf.name_scope(\'batch_train\'):\r\n\t\t\t\t\t\t\t\tfor k in range(batchSize):\r\n\t\t\t\t\t\t\t\t\titem = choice(trainingData)\r\n\t\t\t\t\t\t\t\t\tpath_img = process.arraypath + item + \'img.npy\'\r\n\t\t\t\t\t\t\t\t\tpath_hm = process.arraypath + item + \'hm.npy\'\r\n\t\t\t\t\t\t\t\t\tr_angle = np.random.randint(params.random_angle_min,params.random_angle_max)\r\n\t\t\t\t\t\t\t\t\timg = np.load(path_img)\r\n\t\t\t\t\t\t\t\t\thm = np.load(path_hm)\r\n\t\t\t\t\t\t\t\t\timg = rotate(img, r_angle)\r\n\t\t\t\t\t\t\t\t\thm =  rotatehm(hm, r_angle)\r\n\t\t\t\t\t\t\t\t\thm_f = modifyOutput(hm, params.nbStacks)\r\n\t\t\t\t\t\t\t\t\tx_batch[k,:,:,:] = img / 255\r\n\t\t\t\t\t\t\t\t\ty_batch[:,k,:,:,:] = hm_f[:,0,:,:,:]\r\n\t\t\t\t\t\t\t\t_,c,summary = sess.run([train_rmsprop, loss, merged_summary_op], feed_dict={x: x_batch, y: y_batch})\r\n\t\t\t\t\t\t\t\tavg_cost += c / (epochiter*batchSize)\r\n\t\t\t\t\t\t\t\tif (epoch * epochiter + i) % params.step_to_save == 0:\r\n\t\t\t\t\t\t\t\t\tsummary_train.add_summary(summary, epoch * epochiter + i)\r\n\t\t\t\t\t\t\t\t\tsummary_train.flush()\r\n\t\t\t\t\tt_epoch_finish = time()\r\n\t\t\t\t\tprint(""Epoch:"", (epoch + 1), \'  avg_cost= \', ""{:.9f}"".format(avg_cost),\' time_epoch=\', str(t_epoch_finish-t_epoch_start))\r\n\t\tt_end = time()\r\n\t\tprint(\'Training Done : \' + str(t_end - t_start))\r\n\t\t\r\n\t\t\r\n\t\t\r\n\t\t\r\n\t\t\r\n'"
