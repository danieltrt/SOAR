file_path,api_count,code
addnoise.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Fri Feb  9 16:02:33 2018\n\n@author: clausmichele\n""""""\nimport os\nimport numpy as np\nimport cv2\nimport glob\n\n \ndef main():\n  \n  src_dir=\'./data/\'\n  save_dir = \'./data/train\'\n  src_dir_test=\'./data/test\'\n  save_dir_test = \'./data/test\'\n  \n  filepaths = glob.glob(src_dir + \'/*.jpg\')\n  filepaths_test = glob.glob(src_dir_test + \'/*.jpg\')\n  def sortKeyFunc(s):\n    return int(os.path.basename(s)[:-4])\n    \n  filepaths_test.sort(key=sortKeyFunc)\n  filepaths.sort(key=sortKeyFunc)\n  \n  \n  print(""[*] Reading train files..."")  \n  \n  if not os.path.exists(save_dir):\n        os.mkdir(save_dir)\n        os.mkdir(save_dir_test)\n        os.mkdir(\'./data/train/noisy\')\n        os.mkdir(\'./data/train/original\')\n        os.mkdir(\'./data/test/noisy\')\n        os.mkdir(\'./data/test/original\')        \n\n  print(""[*] Applying noise..."")\n\n  sig = np.linspace(0,50,len(filepaths))\n  np.random.shuffle(sig)\n  sig_test = np.linspace(0,50,len(filepaths_test))\n  np.random.shuffle(sig_test)\n\n  for i in xrange(len(filepaths)):\n        image = cv2.imread(filepaths[i])\n        image = cv2.resize(image,(180,180), interpolation = cv2.INTER_CUBIC)\n        row,col,ch = image.shape\n        mean = 0\n        sigma = sig[i]\n        gauss = np.random.normal(mean,sigma,(row,col,ch))\n        gauss = gauss.reshape(row,col,ch)\n        noisy = image + gauss\n        noisy = np.clip(noisy, 0, 255)\n        noisy = noisy.astype(\'uint8\')\n        cv2.imwrite(os.path.join(save_dir, ""noisy/%04d.png"" %i), noisy)\n        cv2.imwrite(os.path.join(save_dir, ""original/%04d.png"" %i), image)\n        \n  for i in xrange(len(filepaths_test)):\n        image = cv2.imread(filepaths_test[i])\n        image = cv2.resize(image,(180,180), interpolation = cv2.INTER_CUBIC)\n        row,col,ch = image.shape\n        mean = 0\n        sigma = sig[i]\n        gauss = np.random.normal(mean,sigma,(row,col,ch))\n        gauss = gauss.reshape(row,col,ch)\n        noisy = image + gauss\n        noisy = np.clip(noisy, 0, 255)\n        noisy = noisy.astype(\'uint8\')     \n        cv2.imwrite(os.path.join(save_dir_test, ""noisy/%d.png"" %i), noisy)\n        cv2.imwrite(os.path.join(save_dir_test, ""original/%d.png"" %i), image)\n  \n  print(""[*] Noisy and original images saved"")\n\nif __name__ == ""__main__"":\n main()'"
main.py,6,"b'import argparse\nfrom glob import glob\nimport tensorflow as tf\nimport time\nfrom model import denoiser\nimport os\nimport numpy as np\n\nparser = argparse.ArgumentParser(description=\'\')\nparser.add_argument(\'--epoch\', dest=\'epoch\', type=int, default=10, help=\'# of epochs\')\nparser.add_argument(\'--batch_size\', dest=\'batch_size\', type=int, default=128, help=\'# images in batch\')\nparser.add_argument(\'--lr\', dest=\'lr\', type=float, default=0.001, help=\'initial learning rate for adam\')\nparser.add_argument(\'--use_gpu\', dest=\'use_gpu\', type=int, default=1, help=\'gpu flag, 1 for GPU and 0 for CPU\')\nparser.add_argument(\'--phase\', dest=\'phase\', default=\'train\', help=\'train or test\')\nparser.add_argument(\'--checkpoint_dir\', dest=\'ckpt_dir\', default=\'./checkpoint\', help=\'models are saved here\')\nparser.add_argument(\'--test_dir\', dest=\'test_dir\', default=\'./data/denoised\', help=\'denoised sample are saved here\')\nargs = parser.parse_args()\n\n\ndef denoiser_train(denoiser, lr):\n        noisy_eval_files = glob(\'./data/train/noisy/*.png\')\n        noisy_eval_files = sorted(noisy_eval_files)\n        eval_files = glob(\'./data/train/original/*.png\')\n        eval_files = sorted(eval_files)\n        denoiser.train(eval_files, noisy_eval_files, batch_size=args.batch_size, ckpt_dir=args.ckpt_dir, epoch=args.epoch, lr=lr)\n\n\ndef denoiser_test(denoiser):\n\n    noisy_eval_files = glob(\'./data/test/noisy/*.png\')\n#    n = [int(i) for i in map(lambda x: x.split(\'/\')[-1].split(\'.png\')[0], noisy_eval_files)]\n#    noisy_eval_files = [x for (y, x) in sorted(zip(n, noisy_eval_files))]\n    noisy_eval_files = sorted(noisy_eval_files)\n    eval_files = glob(\'./data/test/original/*.png\')\n    eval_files = sorted(eval_files)\n    start = time.time()\n    denoiser.test(eval_files, noisy_eval_files, ckpt_dir=args.ckpt_dir, save_dir=\'./data/denoised\', temporal=args.temporal)\n    end = time.time()\n    print ""Elapsed time:"", end-start\n\ndef main(_):\n    if not os.path.exists(args.ckpt_dir):\n        os.makedirs(args.ckpt_dir)\n    if not os.path.exists(args.test_dir):\n        os.makedirs(args.test_dir)\n\n    lr = args.lr * np.ones([args.epoch])\n    lr[30:] = lr[0] / 10.0\n    if args.use_gpu:\n        # added to control the gpu memory\n        print(""GPU\\n"")\n        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n            model = denoiser(sess)\n            if args.phase == \'train\':\n                denoiser_train(model, lr=lr)\n            elif args.phase == \'test\':\n                denoiser_test(model)\n            else:\n                print(\'[!]Unknown phase\')\n                exit(0)\n    else:\n        print(""CPU\\n"")\n        with tf.Session() as sess:\n            model = denoiser(sess)\n            if args.phase == \'train\':\n                denoiser_train(model, lr=lr)\n            elif args.phase == \'test\':\n                denoiser_test(model)\n            else:\n                print(\'[!]Unknown phase\')\n                exit(0)\n\ndef test():\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n      model = denoiser(sess)\n      denoiser_test(model)\n\n  \nif __name__ == \'__main__\':\n    tf.app.run()\n'"
model.py,32,"b'import time\nfrom glob import glob\nimport numpy as np\nimport tensorflow as tf\nimport random\nimport os\nimport cv2\n\ndef dncnn(input, is_training=True, output_channels=3):\n    with tf.variable_scope(\'block1\'):\n        output = tf.layers.conv2d(input, 64, 3, padding=\'same\', activation=tf.nn.relu)\n    for layers in xrange(2, 19+1):\n        with tf.variable_scope(\'block%d\' % layers):\n            output = tf.layers.conv2d(output, 64, 3, padding=\'same\', name=\'conv%d\' % layers, use_bias=False)\n            output = tf.nn.relu(tf.layers.batch_normalization(output, training=is_training))   \n    with tf.variable_scope(\'block17\'):\n        output = tf.layers.conv2d(output, output_channels, 3, padding=\'same\',use_bias=False)\n    return input - output\n\nfilepaths = glob(\'./data/train/original/*.png\') #takes all the paths of the png files in the train folder\nfilepaths = sorted(filepaths)                           #Order the list of files\nfilepaths_noisy = glob(\'./data/train/noisy/*.png\')\nfilepaths_noisy = sorted(filepaths_noisy)\nind = range(len(filepaths))\n\nclass denoiser(object):\n    def __init__(self, sess, input_c_dim=3, batch_size=128):\n        self.sess = sess\n        self.input_c_dim = input_c_dim\n        # build model\n        self.Y_ = tf.placeholder(tf.float32, [None, None, None, self.input_c_dim],\n                                 name=\'clean_image\')\n        self.is_training = tf.placeholder(tf.bool, name=\'is_training\')\n        self.X = tf.placeholder(tf.float32, [None, None, None, self.input_c_dim])\n        self.Y = dncnn(self.X, is_training=self.is_training)\n        self.loss = (1.0 / batch_size) * tf.nn.l2_loss(self.Y_ - self.Y)\n        self.lr = tf.placeholder(tf.float32, name=\'learning_rate\')\n        self.dataset = dataset(sess)\n        optimizer = tf.train.AdamOptimizer(self.lr, name=\'AdamOptimizer\')\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n            self.train_op = optimizer.minimize(self.loss)\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n        print(""[*] Initialize model successfully..."")\n\n    def evaluate(self, iter_num, eval_files, noisy_files, summary_writer):\n        print(""[*] Evaluating..."")\n        psnr_sum = 0\n        \n        for i in xrange(10):\n            clean_image = cv2.imread(eval_files[i])\n            clean_image = clean_image.astype(\'float32\') / 255.0\n            clean_image = clean_image[np.newaxis, ...]\n            noisy = cv2.imread(noisy_files[i])\n            noisy = noisy.astype(\'float32\') / 255.0\n            noisy = noisy[np.newaxis, ...]\n            \n            output_clean_image = self.sess.run(\n                [self.Y],feed_dict={self.Y_: clean_image,\n                           self.X: noisy,\n                           self.is_training: False})\n            psnr = psnr_scaled(clean_image, output_clean_image)\n            print(""img%d PSNR: %.2f"" % (i + 1, psnr))\n            psnr_sum += psnr\n\n        avg_psnr = psnr_sum / 10\n\n        print(""--- Test ---- Average PSNR %.2f ---"" % avg_psnr)\n\n\n    def train(self, eval_files, noisy_files, batch_size, ckpt_dir, epoch, lr, eval_every_epoch=1):\n\n        numBatch = int(len(filepaths) * 2)\n        # load pretrained model\n        load_model_status, global_step = self.load(ckpt_dir)\n        if load_model_status:\n            iter_num = global_step\n            start_epoch = global_step // numBatch\n            start_step = global_step % numBatch\n            print(""[*] Model restore success!"")\n        else:\n            iter_num = 0\n            start_epoch = 0\n            start_step = 0\n            print(""[*] Not find pretrained model!"")\n        # make summary\n        tf.summary.scalar(\'loss\', self.loss)\n        tf.summary.scalar(\'lr\', self.lr)\n        writer = tf.summary.FileWriter(\'./logs\', self.sess.graph)\n        merged = tf.summary.merge_all()\n        clip_all_weights = tf.get_collection(""max_norm"")        \n\n        print(""[*] Start training, with start epoch %d start iter %d : "" % (start_epoch, iter_num))\n        start_time = time.time()\n        self.evaluate(iter_num, eval_files, noisy_files, summary_writer=writer)  # eval_data value range is 0-255\n        for epoch in xrange(start_epoch, epoch):\n            batch_noisy = np.zeros((batch_size,64,64,3),dtype=\'float32\')\n            batch_images = np.zeros((batch_size,64,64,3),dtype=\'float32\')\n            for batch_id in xrange(start_step, numBatch):\n              try:\n                res = self.dataset.get_batch() # If we get an error retrieving a batch of patches we have to reinitialize the dataset\n              except KeyboardInterrupt:\n                raise\n              except:\n                self.dataset = dataset(self.sess) # Dataset re init\n                res = self.dataset.get_batch()\n              if batch_id==0:\n                batch_noisy = np.zeros((batch_size,64,64,3),dtype=\'float32\')\n                batch_images = np.zeros((batch_size,64,64,3),dtype=\'float32\')\n              ind1 = range(res.shape[0]/2)\n              ind1 = np.multiply(ind1,2)\n              for i in range(batch_size):\n                random.shuffle(ind1)\n                ind2 = random.randint(0,8-1)\n                batch_noisy[i] = res[ind1[0],ind2]\n                batch_images[i] = res[ind1[0]+1,ind2]\n#              for i in range(64):\n#                cv2.imshow(\'raw\',batch_images[i])\n#                cv2.imshow(\'noisy\',batch_noisy[i])\n              _, loss, summary = self.sess.run([self.train_op, self.loss, merged],\n                                                 feed_dict={self.Y_: batch_images, self.X: batch_noisy, self.lr: lr[epoch],\n                                                            self.is_training: True})\n              self.sess.run(clip_all_weights)          \n              \n              print(""Epoch: [%2d] [%4d/%4d] time: %4.4f, loss: %.6f""\n                    % (epoch + 1, batch_id + 1, numBatch, time.time() - start_time, loss))\n              iter_num += 1\n              writer.add_summary(summary, iter_num)\n              \n            if np.mod(epoch + 1, eval_every_epoch) == 0: ##Evaluate and save model\n                self.evaluate(iter_num, eval_files, noisy_files, summary_writer=writer)\n                self.save(iter_num, ckpt_dir)\n        print(""[*] Training finished."")\n\n    def save(self, iter_num, ckpt_dir, model_name=\'DnCNN-tensorflow\'):\n        saver = tf.train.Saver()\n        checkpoint_dir = ckpt_dir\n        if not os.path.exists(checkpoint_dir):\n            os.makedirs(checkpoint_dir)\n        print(""[*] Saving model..."")\n        saver.save(self.sess,\n                   os.path.join(checkpoint_dir, model_name),\n                   global_step=iter_num)\n\n    def load(self, checkpoint_dir):\n        print(""[*] Reading checkpoint..."")\n        saver = tf.train.Saver()\n        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n        if ckpt and ckpt.model_checkpoint_path:\n            full_path = tf.train.latest_checkpoint(checkpoint_dir)\n            global_step = int(full_path.split(\'/\')[-1].split(\'-\')[-1])\n            saver.restore(self.sess, full_path)\n            return True, global_step\n        else:\n            return False, 0\n\n    def test(self, eval_files, noisy_files, ckpt_dir, save_dir, temporal):\n        """"""Test DnCNN""""""\n        # init variables\n        tf.global_variables_initializer().run()\n        assert len(eval_files) != 0, \'No testing data!\'\n        load_model_status, global_step = self.load(ckpt_dir)\n        assert load_model_status == True, \'[!] Load weights FAILED...\'\n        print("" [*] Load weights SUCCESS..."")\n        psnr_sum = 0\n            \n        for i in xrange(len(eval_files)):\n            clean_image = cv2.imread(eval_files[i])\n            clean_image = clean_image.astype(\'float32\') / 255.0\n            clean_image = clean_image[np.newaxis, ...]\n            \n            noisy = cv2.imread(noisy_files[i])\n            noisy = noisy.astype(\'float32\') / 255.0\n            noisy = noisy[np.newaxis, ...] \n          \n            output_clean_image = self.sess.run(\n                [self.Y],feed_dict={self.Y_: clean_image, self.X: noisy,\n                                    self.is_training: False})\n            \n            out1 = np.asarray(output_clean_image)\n               \n            psnr = psnr_scaled(clean_image, out1[0,0])\n            psnr1 = psnr_scaled(clean_image, noisy)\n            \n            print(""img%d PSNR: %.2f , noisy PSNR: %.2f"" % (i + 1, psnr, psnr1))\n            psnr_sum += psnr\n\n            cv2.imwrite(\'./data/denoised/%04d.png\'%(i),out1[0,0]*255.0)\n\n        avg_psnr = psnr_sum / len(eval_files)\n        print(""--- Test ---- Average PSNR %.2f ---"" % avg_psnr)\n\n    \nclass dataset(object):\n  def __init__(self,sess):\n    self.sess = sess\n    seed = time.time()\n    random.seed(seed)\n\n    random.shuffle(ind)\n    \n    filenames = list()\n    for i in xrange(len(filepaths)):\n        filenames.append(filepaths_noisy[ind[i]])\n        filenames.append(filepaths[ind[i]])\n\n    # Parameters\n    num_patches = 8   # number of patches to extract from each image\n    patch_size = 64                 # size of the patches\n    num_parallel_calls = 1          # number of threads\n    batch_size = 32                # size of the batch\n    get_patches_fn = lambda image: get_patches(image, num_patches=num_patches, patch_size=patch_size)\n    dataset = (\n        tf.data.Dataset.from_tensor_slices(filenames)\n        .map(im_read, num_parallel_calls=num_parallel_calls)\n        .map(get_patches_fn, num_parallel_calls=num_parallel_calls)\n        .batch(batch_size)\n        .prefetch(batch_size)\n    )\n    \n    iterator = dataset.make_one_shot_iterator()\n    self.iter = iterator.get_next()\n  \n\n  def get_batch(self):\n        res = self.sess.run(self.iter)\n        return res\n        \ndef im_read(filename):\n    """"""Decode the png image from the filename and convert to [0, 1].""""""\n    image_string = tf.read_file(filename)\n    image_decoded = tf.image.decode_png(image_string, channels=3)\n    # This will convert to float values in [0, 1]\n    image = tf.image.convert_image_dtype(image_decoded, tf.float32)\n    return image\n    \ndef get_patches(image, num_patches=128, patch_size=64):\n    """"""Get `num_patches` from the image""""""\n    patches = []\n    for i in range(num_patches):\n      point1 = random.randint(0,116) # 116 comes from the image source size (180) - the patch dimension (64)\n      point2 = random.randint(0,116)\n      patch = tf.image.crop_to_bounding_box(image, point1, point2, patch_size, patch_size)\n      patches.append(patch)\n    patches = tf.stack(patches)\n    assert patches.get_shape().dims == [num_patches, patch_size, patch_size, 3]\n    return patches\n    \ndef cal_psnr(im1, im2): # PSNR function for 0-255 values\n    mse = ((im1.astype(np.float) - im2.astype(np.float)) ** 2).mean()\n    psnr = 10 * np.log10(255 ** 2 / mse)\n    return psnr\n    \ndef psnr_scaled(im1, im2): # PSNR function for 0-1 values\n    mse = ((im1 - im2) ** 2).mean()\n    mse = mse * (255 ** 2)\n    psnr = 10 * np.log10(255 **2 / mse)\n    return psnr\n'"
