file_path,api_count,code
src/charset.py,0,"b'# CNN-LSTM-CTC-OCR\n# Copyright (C) 2017,2018 Jerod Weinman, Abyaya Lamsal, Benjamin Gafford\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n# charset.py -- A suite of tools for converting strings to/from labels\n\n\n"""""" \nThe string of valid output characters for the model\nAny example with a character not found here will generate a runtime error\n""""""\nout_charset=""ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789""\n\n\n""""""\nDict for constant time string->label conversion\nAttribution: https://stackoverflow.com/a/36460020 [Abhijit] Terms: CC-BY-SA\nProduces a table of character->index mappings according to out_charset\n"""""" \nout_charset_dict = { key: val for val, key in enumerate( out_charset ) }\n\n\n""""""\nDict for constant time label->string conversion\nProduces a table of index->string mappings according to out_charset\n""""""\nint_to_string_dict = dict(enumerate(out_charset))\n\n\ndef num_classes():\n    """""" Returns length/size of out_charset """"""\n    return len( out_charset )\n\n\ndef label_to_string ( labels ):\n    """"""Convert a list of labels to the corresoponding string""""""\n    string = \'\'.join( [int_to_string_dict[c] for c in labels] )\n    return string\n\ndef string_to_label ( string ):\n    """"""Convert a string to a list of labels""""""\n    label = [out_charset_dict[c] for c in string]\n    return label\n'"
src/evaluate.py,23,"b'# CNN-LSTM-CTC-OCR\n# Copyright (C) 2017,2018 Jerod Weinman, Abyaya Lamsal, Benjamin Gafford\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n\n# evaluate.py -- Streams evaluation statistics (i.e., character error\n#   rate, sequence error rate) for a single batch whenever a new model\n#   checkpoint appears\n\nimport os\nimport tensorflow as tf\nfrom tensorflow.python.ops import control_flow_ops\nimport six\nimport model_fn\nimport pipeline\nimport filters\n\n# Filters out information to just show a stream of results\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\nFLAGS = tf.app.flags.FLAGS\n\n\ntf.app.flags.DEFINE_integer( \'batch_size\',2**9,\n                             """"""Eval batch size"""""" )\ntf.app.flags.DEFINE_integer(\'eval_interval_secs\', 120,\n                             """"""Time between test runs"""""")\n\ntf.app.flags.DEFINE_string( \'model\',\'../data/model\',\n                            """"""Directory for event logs and checkpoints"""""" )\ntf.app.flags.DEFINE_string( \'output\',\'test\',\n                            """"""Sub-directory of model for test summary events"""""" )\n\ntf.app.flags.DEFINE_string( \'test_path\',\'../data/\',\n                            """"""Base directory for test/validation data"""""" )\ntf.app.flags.DEFINE_string( \'filename_pattern\',\'val/words-*\',\n                            """"""File pattern for input data"""""" )\ntf.app.flags.DEFINE_integer( \'num_input_threads\',4,\n                             """"""Number of readers for input data"""""" )\n\ntf.app.flags.DEFINE_integer(\'min_image_width\',None,\n                            """"""Minimum allowable input image width"""""")\ntf.app.flags.DEFINE_integer(\'max_image_width\',None,\n                            """"""Maximum allowable input image width"""""")\ntf.app.flags.DEFINE_integer(\'min_string_length\',None,\n                            """"""Minimum allowable input string length"""""")\ntf.app.flags.DEFINE_integer(\'max_string_length\',None,\n                            """"""Maximum allowable input string_length"""""")\n\ntf.app.flags.DEFINE_boolean(\'bucket_data\',False,\n                            """"""Bucket training data by width for efficiency"""""")\n\n\ndef _get_input():\n    """"""\n    Get tf.data.Dataset object according to command-line flags for evaluation\n    using tf.estimator.Estimator\n\n    Note: Default behavior is bucketing according to default bucket boundaries\n    listed in pipeline.get_data\n\n    Returns:\n      features, labels\n                feature structure can be seen in postbatch_fn \n                in mjsynth.py or maptextsynth.py for static or dynamic\n                data pipelines respectively\n    """"""\n\n    # WARNING: More than two filters causes SEVERE throughput slowdown\n    filter_fn = filters.input_filter_fn \\\n                ( min_image_width=FLAGS.min_image_width,\n                  max_image_width=FLAGS.max_image_width,\n                  min_string_length=FLAGS.min_string_length,\n                  max_string_length=FLAGS.max_string_length )\n\n    # Pack keyword arguments into dictionary\n    data_args = { \'base_dir\': FLAGS.test_path,\n                  \'file_patterns\': str.split(FLAGS.filename_pattern, \',\'),\n                  \'num_threads\': FLAGS.num_input_threads,\n                  \'batch_size\': FLAGS.batch_size,\n                  \'filter_fn\': filter_fn\n    }\n\n    if not FLAGS.bucket_data: # Turn off bucketing (on by default in pipeline)\n        data_args[\'boundaries\']=None\n\n    # Get data according to flags\n    dataset = pipeline.get_data( use_static_data=True, **data_args)\n\n    return dataset\n\n\n# Taken from the official source code of Tensorflow\n# Licensed under the Apache License, Version 2.0\n# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py\ndef _extract_metric_update_ops( eval_dict ):\n  """"""Separate update operations from metric value operations.""""""\n  update_ops = []\n  value_ops = {}\n  # Sort metrics lexicographically so graph is identical every time.\n  for name, metric_ops in sorted( six.iteritems( eval_dict ) ):\n    value_ops[name] = metric_ops[0]\n    update_ops.append( metric_ops[1] )\n\n  if update_ops:\n    update_op = control_flow_ops.group( *update_ops )\n  else:\n    update_op = None\n\n  return update_op, value_ops\n\n\ndef _get_config():\n    """"""Setup session config to soften device placement""""""\n    device_config=tf.ConfigProto(\n        allow_soft_placement=True, \n        log_device_placement=False)\n\n    return device_config\n\n\ndef main(argv=None):\n    \n    dataset = _get_input()\n\n    # Extract input tensors for evaluation\n    iterator = dataset.make_one_shot_iterator()\n    features, labels = iterator.get_next()\n\n    # Construct the evaluation function \n    evaluate_fn = model_fn.evaluate_fn()\n\n    # Wrap the ops in an Estimator spec object\n    estimator_spec = evaluate_fn( features, labels, \n                                  tf.estimator.ModeKeys.EVAL, \n                                  {\'continuous_eval\': True})\n\n    # Extract the necessary ops and the final tensors from the estimator spec\n    update_op, value_ops = _extract_metric_update_ops(\n        estimator_spec.eval_metric_ops)\n  \n    # Specify to evaluate N number of batches (in this case N==1)\n    stop_hook = tf.contrib.training.StopAfterNEvalsHook( 1 )\n\n    # Create summaries of values added to tf.GraphKeys.SUMMARIES  \n    summary_writer = tf.summary.FileWriter (os.path.join(FLAGS.model,\n                                                         FLAGS.output))\n    summary_hook = tf.contrib.training.SummaryAtEndHook(summary_writer=\n                                                        summary_writer)\n    \n    # Evaluate repeatedly once a new checkpoint is found\n    tf.contrib.training.evaluate_repeatedly(\n        checkpoint_dir=FLAGS.model,eval_ops=update_op, final_ops=value_ops, \n        hooks = [stop_hook, summary_hook], config=_get_config(), \n        eval_interval_secs= FLAGS.eval_interval_secs )\n\nif __name__ == \'__main__\':\n    tf.app.run()   \n'"
src/filters.py,13,"b'# CNN-LSTM-CTC-OCR\n# Copyright (C) 2017,2018 Jerod Weinman, Abyaya Lamsal, Benjamin Gafford\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n# filters.py - Options/examples for building Dataset input filters based on\n# image or label string size\n\n\n# READ THIS BEFORE WRITING YOUR OWN FILTER FUNCTION\n#\n# To be used as filter_fn\'s passed into call to pipeline.get_data\n# Filters for must be written in the following structure:\n# * filter_fn(image, width, label, length, text)\n# VAR            : tf.dtype, shape\n#--------------------------------------------\n# image          : tf.float32, [32, ?, 1] HWC\n# width          : tf.int32, []\n# label          : tf.int32, [?]\n# length         : tf.int32, []\n# text           : tf.string, []\n#\n#\n# Note: For more detailed information, refer to `_preprocess_fn` in\n# mjsynth.py. Filtering is performed after these transformations have\n# been applied. Therefore, filter_fn args will correspond to the\n# return values of `preprocess_fn`.\n \nimport tensorflow as tf\nimport model\n\ndef input_filter_fn( min_image_width=None, max_image_width=None,\n                     min_string_length=None, max_string_length=None,\n                     check_input=False):\n    """"""Functor for filter based on string or image size\n    Input:\n      min_image_width  : Python numerical value (or None) representing the \n                         minimum allowable input image width \n      max_image_width  : Python numerical value (or None) representing the \n                         maximum allowable input image width \n      min_string_length : Python numerical value (or None) representing the \n                         minimum allowable input string length\n      max_string_length : Python numerical value (or None) representing the \n                         maximum allowable input string length\n      check_input: Whether to verify the feature sequence is as long as the \n                    input string\n   Returns:\n      keep_input : Boolean Tensor indicating whether to keep a given input \n                  with the specified image width and string length\n    """"""\n\n    if not (min_image_width or max_image_width or\n            min_string_length or max_string_length or check_input):\n        return None\n    \n    filter_fn = lambda image, width, label, length, text: \\\n                _get_filter( width, length,\n                             min_image_width, max_image_width,\n                             min_string_length, max_string_length,\n                             check_input)\n\n    return filter_fn\n\n\ndef _get_filter(width, length, min_width, max_width, min_length, max_length,\n                check_input):\n    """"""Function for filter based on string or image size\n    Input:\n      width      : Tensor representing the image width\n      length     : Tensor representing the ground truth string length\n      min_width  : Python numerical value (or None) representing the \n                     minimum allowable input image width \n      max_width  : Python numerical value (or None) representing the \n                     maximum allowable input image width \n      min_length : Python numerical value (or None) representing the \n                     minimum allowable input string length\n      max_length : Python numerical value (or None) representing the \n                     maximum allowable input string length\n      check_input: Whether to verify the feature sequence is as long as the \n                    input string\n   Returns:\n      keep_input : Boolean Tensor indicating whether to keep a given input \n                     with the specified image width and string length\n    """"""\n    \n    def add_filter(orig_filt, new_filt): # Helper to build conjunctions\n        if orig_filt is None:\n            return new_filt\n        else:\n            return tf.logical_and( orig_filt, new_filt )\n        \n    keep_input = None\n\n    if min_width:\n        keep_input = add_filter( keep_input,\n                                 tf.greater_equal(width, min_width) )\n    if max_width:\n        keep_input = add_filter( keep_input,\n                                 tf.less_equal(width, max_width) )\n    if min_length:\n        keep_input = add_filter( keep_input,\n                                tf.greater_equal(length, min_length) )\n    if max_length:\n        keep_input = add_filter( keep_input,\n                                tf.less_equal(length, max_length) )\n    if check_input:\n        keep_input = add_filter(\n            keep_input,\n            tf.less_equal( tf.cast(length,tf.int32),\n                              model.get_sequence_lengths(width) ) )\n        \n    if keep_input!=None:\n        keep_input = tf.reshape( keep_input, [] ) # explicitly make a scalar\n\n    return keep_input\n'"
src/mjsynth-tfrecord.py,12,"b'# CNN-LSTM-CTC-OCR\n# Copyright (C) 2017 Jerod Weinman\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nimport os\nimport tensorflow as tf\nimport math\n\nimport charset\n\n""""""Each record within the TFRecord file is a serialized Example proto. \nThe Example proto contains the following fields:\n  image/encoded: string containing JPEG encoded grayscale image\n  image/height: integer, image height in pixels\n  image/width: integer, image width in pixels\n  image/filename: string containing the basename of the image file\n  image/labels: list containing the sequence labels for the image text\n  image/text: string specifying the human-readable version of the text\n""""""\njpeg_data = tf.placeholder( dtype=tf.string )\njpeg_decoder = tf.image.decode_jpeg( jpeg_data,channels=1 )\n\nkernel_sizes = [3,3,3,3,3,3] # CNN kernels for image reduction\n\n# Minimum allowable width of image after CNN processing\nmin_width = 20\n\ndef calc_seq_len( image_width ):\n    """"""Calculate sequence length of given image after CNN processing""""""\n    \n    conv1_trim =  2 * (kernel_sizes[0] // 2)\n    fc6_trim = 2 * (kernel_sizes[5] // 2)\n    \n    after_conv1 = image_width - conv1_trim \n    after_pool2 = after_conv1 // 2\n    after_pool4 = after_pool2 - 1 # max without stride\n    after_pool6 = after_pool4 - 1\n    after_pool8 = after_pool6\n    seq_len = after_pool8\n    return seq_len\n\nseq_lens = [calc_seq_len( w ) for w in range( 1024 )]\n\n\ndef gen_data( input_base_dir, image_list_filename, output_filebase, \n              num_shards=1000, start_shard=0 ):\n    """""" Generate several shards worth of TFRecord data """"""\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth=True\n    sess = tf.Session( config=session_config )\n    image_filenames = get_image_filenames( \n        os.path.join( input_base_dir,\n                      image_list_filename ) )\n    num_digits = math.ceil( math.log10( num_shards - 1 ) )\n\n    # Use appropriate # leading zeros\n    shard_format = \'%0\'+ (\'%d\'%num_digits) + \'d\' \n    images_per_shard = int( math.ceil( len( image_filenames ) / \n                                       float( num_shards ) ) )\n    \n    for i in range( start_shard,num_shards ):\n        start = i * images_per_shard\n        end = (i + 1) * images_per_shard\n        out_filename = output_filebase+\'-\'+(shard_format % i)+\'.tfrecord\'\n        if os.path.isfile( out_filename ): # Don\'t recreate data if restarting\n            continue\n        print str( i ), \'of\', str( num_shards ),\\\n            \'[\', str( start ), \':\', str( end ), \']\', out_filename\n        gen_shard( sess, input_base_dir, \n                   image_filenames[start:end], out_filename )\n\n    # Clean up writing last shard\n    start = num_shards * images_per_shard\n    out_filename = output_filebase+\'-\'+(shard_format % num_shards)+\'.tfrecord\'\n    print str(i),\'of\',str(num_shards),\'[\',str(start),\':]\',out_filename\n    gen_shard(sess, input_base_dir, image_filenames[start:], out_filename)\n\n    sess.close()\n\n    \ndef gen_shard( sess, input_base_dir, image_filenames, output_filename ):\n    """"""Create a TFRecord file from a list of image filenames""""""\n    writer = tf.python_io.TFRecordWriter( output_filename )\n    \n    for filename in image_filenames:\n        path_filename = os.path.join( input_base_dir, filename )\n        if os.stat( path_filename ).st_size == 0:\n            print(\'SKIPPING\', filename)\n            continue\n        try:\n            image_data, height, width = get_image( sess, path_filename )\n            text, labels = get_text_and_labels( filename )\n            if is_writable( width, text ):\n                example = make_example( filename, image_data, \n                                        labels, text, \n                                        height, width )\n                writer.write( example.SerializeToString() )\n            else:\n                print( \'SKIPPING\', filename )\n        except:\n            # Some files have bogus payloads, \n            # catch and note the error, moving on\n            print( \'ERROR\', filename )\n    writer.close()\n\n\ndef get_image_filenames( image_list_filename ):\n    """""" Given input file, generate a list of relative filenames""""""\n    filenames = []\n    with open( image_list_filename ) as f:\n        for line in f:\n            # Carve out the ground truth string and file path from lines like:\n            # ./2697/6/466_MONIKER_49537.jpg 49537\n            filename = line.split( \' \', 1 )[0][2:] # split off ""./"" and number\n            filenames.append( filename )\n    return filenames\n\n\ndef get_image( sess, filename ):\n    """"""Given path to an image file, load its data and size""""""\n    with tf.gfile.GFile( filename, \'rb\' ) as f:\n        image_data = f.read()\n    image = sess.run( jpeg_decoder, feed_dict={ jpeg_data: image_data } )\n    height = image.shape[0]\n    width = image.shape[1]\n    return image_data, height, width\n\n\ndef is_writable( image_width, text ):\n    """"""Determine whether the CNN-processed image is longer than the string""""""\n    return (image_width > min_width) and (len( text ) <= seq_lens[image_width])\n\n\ndef get_text_and_labels( filename ):\n    """""" \n    Extract the human-readable text and label sequence from image filename\n    """"""\n    # Ground truth string lines embedded within base \n    # filename between underscores\n    # 2697/6/466_MONIKER_49537.jpg --> MONIKER\n    text = os.path.basename( filename ).split( \'_\', 2 )[1]\n\n    # Transform string text to sequence of indices using charset, e.g.,\n    # MONIKER -> [12, 14, 13, 8, 10, 4, 17]\n    labels = charset.string_to_label(text)\n    \n    return text, labels\n\n\ndef make_example( filename, image_data, labels, text, height, width ):\n    """"""Build an Example proto for an example.\n    Args:\n    filename: string, path to an image file, e.g., \'/path/to/example.JPG\'\n    image_data: string, JPEG encoding of grayscale image\n    labels: integer list, identifiers for the ground truth for the network\n    text: string, unique human-readable, e.g. \'dog\'\n    height: integer, image height in pixels\n    width: integer, image width in pixels\n  Returns:\n    Example proto\n  """"""\n    example = tf.train.Example( features=tf.train.Features( feature={\n        \'image/encoded\' : _bytes_feature( tf.compat.as_bytes( image_data ) ),\n        \'image/labels\'  : _int64_feature( labels ),\n        \'image/height\'  : _int64_feature( [height] ),\n        \'image/width\'   : _int64_feature( [width] ),\n        \'image/filename\': _bytes_feature( tf.compat.as_bytes( filename ) ),\n        \'text/string\'   : _bytes_feature( tf.compat.as_bytes( text ) ),\n        \'text/length\'   : _int64_feature( [len( text )] )\n    }))\n    return example\n\n\ndef _int64_feature( values ):\n    return tf.train.Feature( int64_list=tf.train.Int64List( value=values ) )\n\n\ndef _bytes_feature( values ):\n    return tf.train.Feature( bytes_list=tf.train.BytesList( value=[values] ) )\n\n\ndef main( argv=None ):\n    \n    gen_data( \'../data/images\', \'annotation_train.txt\', \'../data/train/words\')\n    gen_data( \'../data/images\', \'annotation_val.txt\',   \'../data/val/words\' )\n    gen_data( \'../data/images\', \'annotation_test.txt\',  \'../data/test/words\' )\n\n    \nif __name__ == \'__main__\':\n    main()\n'"
src/mjsynth.py,22,"b'# CNN-LSTM-CTC-OCR\n# Copyright (C) 2017,2018 Jerod Weinman, Abyaya Lamsal, Benjamin Gafford\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n# mjsynth.py -- Suite of routines for processing MJSynth data stored\n#   as Examples in in TFRecord files.\n\nimport os\nimport tensorflow as tf\nimport numpy as np\nimport pipeline\n\ndef get_dataset( args ):\n    """""" Get a Dataset from TFRecord files.\n    Parameters:\n      base_dir      : Directory containing the TFRecord files\n      file_patterns : List of wildcard patterns for TFRecord files to read\n      num_threads   : Number of threads to use for reading and processing\n      buffer_sz     : Number of Examples to prefetch and buffer\n    Returns:\n      image   : preprocessed image\n                  tf.float32 tensor of shape [32, ?, 1] (? = width)\n      width   : width (in pixels) of image\n                  tf.int32 tensor of shape []\n      labels  : list of indices of characters mapping text->out_charset\n                  tf.int32 tensor of shape [?] (? = length+1)\n      length  : length of labels (sans -1 EOS token)\n                  tf.int32 tensor of shape []\n      text    : ground truth string\n                  tf.string tensor of shape []\n    """"""\n\n    # Extract args\n    [ base_dir, file_patterns, num_threads, buffer_sz ] = args[0:4]\n\n    # Get filenames as list of tensors\n    tensor_filenames = _get_filenames( base_dir, file_patterns )\n\n    # Get filenames into a dataset format\n    ds_filenames = tf.data.Dataset.from_tensor_slices( tensor_filenames )\n\n    # Shuffle for some stochasticity\n    ds_filenames = ds_filenames.shuffle( buffer_size=len( tensor_filenames ),\n                                         reshuffle_each_iteration=True )\n    \n    dataset = tf.data.TFRecordDataset( ds_filenames, \n                                       num_parallel_reads=num_threads,\n                                       buffer_size=buffer_sz )\n    return dataset\n\n\ndef preprocess_fn( data ):\n    """"""Parse the elements of the dataset""""""\n\n    feature_map = {\n        \'image/encoded\'  :   tf.FixedLenFeature( [], dtype=tf.string, \n                                                 default_value=\'\' ),\n        \'image/labels\'   :   tf.VarLenFeature( dtype=tf.int64 ), \n        \'image/width\'    :   tf.FixedLenFeature( [1], dtype=tf.int64,\n                                                 default_value=1 ),\n        \'image/filename\' :   tf.FixedLenFeature( [], dtype=tf.string,\n                                                 default_value=\'\' ),\n        \'text/string\'    :   tf.FixedLenFeature( [], dtype=tf.string,\n                                                 default_value=\'\' ),\n        \'text/length\'    :   tf.FixedLenFeature( [1], dtype=tf.int64,\n                                                 default_value=1 )\n    }\n    \n    features = tf.parse_single_example( data, feature_map )\n    \n    # Initialize fields according to feature map\n\n    # Convert to grayscale\n    image = tf.image.decode_jpeg( features[\'image/encoded\'], channels=1 ) \n\n    width = tf.cast( features[\'image/width\'], tf.int32 ) # for ctc_loss\n    label = tf.serialize_sparse( features[\'image/labels\'] ) # for batching\n    length = features[\'text/length\']\n    text = features[\'text/string\']\n\n    image = preprocess_image( image )\n\n    return image, width, label, length, text\n\n\ndef element_length_fn( image, image_width, label, label_seq_length, text ):\n    return image_width\n\n\ndef postbatch_fn( image, width, label, length, text ):\n    """"""Post-batching, postprocessing: packs raw tensors into a dictionary for \n       Dataset\'s iterator output""""""\n\n    # Batching is complete, so now we can re-sparsify our labels for ctc_loss\n    label = tf.cast( tf.deserialize_many_sparse( label, tf.int64 ),\n                     tf.int32 )\n    \n    # Format relevant features for estimator ingestion\n    features = {\n        ""image""    : image, \n        ""width""    : width,\n        ""length""   : length,\n        ""text""     : text\n    }\n\n    return features, label\n\n\ndef _get_filenames( base_dir, file_patterns=[\'*.tfrecord\'] ):\n    """"""Get a list of record files""""""\n    \n    # List of lists ...\n    data_files = [tf.gfile.Glob( os.path.join( base_dir, file_pattern ) )\n                  for file_pattern in file_patterns]\n    # flatten\n    data_files = [data_file for sublist in data_files for data_file in sublist]\n\n    return data_files\n\n\ndef preprocess_image( image ):\n    """"""Preprocess image: Rescale and fix image height""""""\n\n    # Rescale from uint8([0,255]) to float([-0.5,0.5])\n    image = pipeline.rescale_image( image )\n\n    # Pad with copy of first row to expand to 32 pixels height\n    first_row = tf.slice( image, [0, 0, 0], [1, -1, -1] )\n    image = tf.concat( [first_row, image], 0 )\n\n    return image\n'"
src/model.py,40,"b'# CNN-LSTM-CTC-OCR\n# Copyright (C) 2017 Jerod Weinman\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n# model.py -- Constructs the graph representing the network\n#   model. Inputs start from convnet_layers(), whose outputs hook into\n#   rnn_layers(), which produces the logits for CTC loss (for\n#   training) and decoding (for prediction/evaluation).\n\nimport tensorflow as tf\nfrom tensorflow.contrib import learn\n\n# Layer params:   Filts K  Padding  Name     BatchNorm?\nlayer_params = [ [  64, 3, \'valid\', \'conv1\', False], \n                 [  64, 3, \'same\',  \'conv2\', True],  # pool\n                 [ 128, 3, \'same\',  \'conv3\', False], \n                 [ 128, 3, \'same\',  \'conv4\', True],  # hpool\n                 [ 256, 3, \'same\',  \'conv5\', False],\n                 [ 256, 3, \'same\',  \'conv6\', True],  # hpool\n                 [ 512, 3, \'same\',  \'conv7\', False], \n                 [ 512, 3, \'same\',  \'conv8\', True] ] # hpool 3\n\nrnn_size = 2**9    # Dimensionality of all RNN elements\' hidden layers\ndropout_rate = 0.5 # For RNN layers (currently not used--uncomment below)\n\ndef conv_layer( bottom, params, training ):\n    """"""Build a convolutional layer using entry from layer_params)""""""\n\n    batch_norm = params[4] # Boolean\n\n    if batch_norm:\n        activation = None\n    else:\n        activation = tf.nn.relu\n\n    kernel_initializer = tf.contrib.layers.variance_scaling_initializer()\n    bias_initializer = tf.constant_initializer( value=0.0 )\n\n    top = tf.layers.conv2d( bottom, \n                            filters=params[0],\n                            kernel_size=params[1],\n                            padding=params[2],\n                            activation=activation,\n                            kernel_initializer=kernel_initializer,\n                            bias_initializer=bias_initializer,\n                            name=params[3] )\n    if batch_norm:\n        top = norm_layer( top, training, params[3]+\'/batch_norm\' )\n        top = tf.nn.relu( top, name=params[3]+\'/relu\' )\n\n    return top\n\n\ndef pool_layer( bottom, wpool, padding, name ):\n    """"""Short function to build a pooling layer with less syntax""""""\n    top = tf.layers.max_pooling2d( bottom, \n                                   2, \n                                   [2, wpool], \n                                   padding=padding, \n                                   name=name )\n    return top\n\n\ndef norm_layer( bottom, training, name):\n    """"""Short function to build a batch normalization layer with less syntax""""""\n    top = tf.layers.batch_normalization( bottom, \n                                         axis=3, # channels last\n                                         training=training,\n                                         name=name )\n    return top\n\n\ndef convnet_layers( inputs, widths, mode ):\n    """"""\n    Build convolutional network layers attached to the given input tensor\n    """"""\n\n    training = (mode == learn.ModeKeys.TRAIN)\n\n    # inputs should have shape [ ?, 32, ?, 1 ]\n    with tf.variable_scope( ""convnet"" ): # h,w\n        \n        conv1 = conv_layer( inputs, layer_params[0], training ) # 30,30\n        conv2 = conv_layer( conv1, layer_params[1], training )  # 30,30\n        pool2 = pool_layer( conv2, 2, \'valid\', \'pool2\' )        # 15,15\n        conv3 = conv_layer( pool2, layer_params[2], training )  # 15,15\n        conv4 = conv_layer( conv3, layer_params[3], training )  # 15,15\n        pool4 = pool_layer( conv4, 1, \'valid\', \'pool4\' )        # 7,14\n        conv5 = conv_layer( pool4, layer_params[4], training )  # 7,14\n        conv6 = conv_layer( conv5, layer_params[5], training )  # 7,14\n        pool6 = pool_layer( conv6, 1, \'valid\', \'pool6\')         # 3,13\n        conv7 = conv_layer( pool6, layer_params[6], training )  # 3,13\n        conv8 = conv_layer( conv7, layer_params[7], training )  # 3,13\n        pool8 = tf.layers.max_pooling2d( conv8, [3, 1], [3, 1], \n                                         padding=\'valid\', \n                                         name=\'pool8\' )         # 1,13\n        # squeeze row dim\n        features = tf.squeeze( pool8, axis=1, name=\'features\' ) \n        \n        sequence_length = get_sequence_lengths( widths )\n        \n        # Vectorize\n        sequence_length = tf.reshape( sequence_length, [-1], name=\'seq_len\' ) \n\n        return features, sequence_length\n\n\ndef get_sequence_lengths( widths ):    \n    """"""Tensor calculating output sequence length from original image widths""""""\n    kernel_sizes = [params[1] for params in layer_params]\n\n    with tf.variable_scope(""sequence_length""):\n        conv1_trim = tf.constant( 2 * (kernel_sizes[0] // 2),\n                                  dtype=tf.int32,\n                                  name=\'conv1_trim\' )\n        one = tf.constant( 1, dtype=tf.int32, name=\'one\' )\n        two = tf.constant( 2, dtype=tf.int32, name=\'two\' )\n        after_conv1 = tf.subtract( widths, conv1_trim, name=\'after_conv1\' )\n        after_pool2 = tf.floor_div( after_conv1, two, name=\'after_pool2\' )\n        after_pool4 = tf.subtract( after_pool2, one, name=\'after_pool4\' )\n        after_pool6 = tf.subtract( after_pool4, one, name=\'after_pool6\' ) \n        after_pool8 = tf.identity( after_pool6, name=\'after_pool8\' )\n    return after_pool8\n\n\ndef rnn_layer( bottom_sequence, sequence_length, rnn_size, scope ):\n    """"""Build bidirectional (concatenated output) RNN layer""""""\n\n    weight_initializer = tf.truncated_normal_initializer( stddev=0.01 )\n\n    # Default activation is tanh\n    cell_fw = tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell( rnn_size )\n    cell_bw = tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell( rnn_size )\n\n    # Pre-CUDNN (slower) alternatve. Default activation is tanh .\n    #cell_fw = tf.contrib.rnn.LSTMCell( rnn_size, \n    #                                   initializer=weight_initializer)\n    #cell_bw = tf.contrib.rnn.LSTMCell( rnn_size, \n    #                                   initializer=weight_initializer)\n\n    # Include?\n    #cell_fw = tf.contrib.rnn.DropoutWrapper( cell_fw, \n    #                                         input_keep_prob=dropout_rate )\n    #cell_bw = tf.contrib.rnn.DropoutWrapper( cell_bw, \n    #                                         input_keep_prob=dropout_rate )\n    \n    rnn_output,_ = tf.nn.bidirectional_dynamic_rnn(\n        cell_fw, \n        cell_bw, \n        bottom_sequence,\n        sequence_length=sequence_length,\n        time_major=True,\n        dtype=tf.float32,\n        scope=scope )\n    \n    # Concatenation allows a single output op because [A B]*[x;y] = Ax+By\n    # [ paddedSeqLen batchSize 2*rnn_size]\n    rnn_output_stack = tf.concat( rnn_output, 2, name=\'output_stack\' )\n    \n    return rnn_output_stack\n\n\ndef rnn_layers( features, sequence_length, num_classes ):\n    """"""Build a stack of RNN layers from input features""""""\n\n    # Input features is [batchSize paddedSeqLen numFeatures]\n    logit_activation = tf.nn.relu\n    weight_initializer = tf.contrib.layers.variance_scaling_initializer()\n    bias_initializer = tf.constant_initializer( value=0.0 )\n\n    with tf.variable_scope( ""rnn"" ):\n        # Transpose to time-major order for efficiency\n        rnn_sequence = tf.transpose( features, \n                                     perm=[1, 0, 2], \n                                     name=\'time_major\' )\n        rnn1 = rnn_layer( rnn_sequence, sequence_length, rnn_size, \'bdrnn1\' )\n        rnn2 = rnn_layer( rnn1, sequence_length, rnn_size, \'bdrnn2\' )\n        rnn_logits = tf.layers.dense( rnn2, \n                                      num_classes+1, \n                                      activation=logit_activation,\n                                      kernel_initializer=weight_initializer,\n                                      bias_initializer=bias_initializer,\n                                      name=\'logits\' )\n        return rnn_logits\n    \n\ndef ctc_loss_layer( rnn_logits, sequence_labels, sequence_length,\n                    reduce_mean=True ):\n    """"""Build CTC Loss layer for training""""""\n    losses = tf.nn.ctc_loss( sequence_labels, \n                             rnn_logits, \n                             sequence_length,\n                             time_major=True, \n                             ignore_longer_outputs_than_inputs=True )\n    if (reduce_mean):\n        loss = tf.reduce_mean( losses )\n    else:\n        loss = tf.reduce_sum( losses )\n\n    return loss\n'"
src/model_fn.py,95,"b'# CNN-LSTM-CTC-OCR\n# Copyright (C) 2017,2018 Jerod Weinman, Abyaya Lamsal, Benjamin Gafford\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n# model_fn.py -- Provides functions necessary for using the Estimator\n#   API to control training, evaluation, and prediction.\n\nimport tensorflow as tf \nimport model\nimport mjsynth\nimport charset\nimport pipeline\nimport utils\n\n# Beam search width for prediction and evaluation modes using both the\n# custom, lexicon-driven CTCWordBeamSearch module and the open-lexicon\n# tf.nn.ctc_beam_search_decoder\n_ctc_beam_width = 2**7\n\ndef _get_image_info( features, mode ):\n    """"""Calculates the logits and sequence length""""""\n\n    image = features[\'image\']\n    width = features[\'width\']\n\n    conv_features,sequence_length = model.convnet_layers( image, \n                                                          width, \n                                                          mode )\n\n    logits = model.rnn_layers( conv_features, sequence_length,\n                               charset.num_classes() )\n\n    return logits, sequence_length\n\n\ndef _get_init_pretrained( tune_from ):\n    """"""Return lambda for reading pretrained initial model with a given session""""""\n    \n    if not tune_from:\n        return None\n    \n    # Extract the global variables\n    saver_reader = tf.train.Saver(\n        tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES ) )\n    \n    ckpt_path=tune_from\n\n    # Function to build the scaffold to initialize the training process\n    init_fn = lambda scaffold, sess: saver_reader.restore( sess, ckpt_path )\n\n    return init_fn\n\n\ndef _get_training( rnn_logits,label,sequence_length, tune_scope, \n                   learning_rate, decay_steps, decay_rate, decay_staircase, \n                   momentum ):\n    """"""Set up training ops""""""\n\n    with tf.name_scope( ""train"" ):\n\n        if tune_scope:\n            scope=tune_scope\n        else:            \n            scope=""convnet|rnn""\n\n        rnn_vars = tf.get_collection( tf.GraphKeys.TRAINABLE_VARIABLES,\n                                       scope=scope )        \n\n        loss = model.ctc_loss_layer( rnn_logits,label,sequence_length ) \n\n        # Update batch norm stats [http://stackoverflow.com/questions/43234667]\n        extra_update_ops = tf.get_collection( tf.GraphKeys.UPDATE_OPS )\n\n        with tf.control_dependencies( extra_update_ops ):\n            \n            # Calculate the learning rate given the parameters\n            learning_rate_tensor = tf.train.exponential_decay(\n                learning_rate,\n                tf.train.get_global_step(),\n                decay_steps,\n                decay_rate,\n                staircase=decay_staircase,\n                name=\'learning_rate\' )\n\n            optimizer = tf.train.AdamOptimizer(\n                learning_rate=learning_rate_tensor,\n                beta1=momentum )\n\n            train_op = tf.contrib.layers.optimize_loss(\n                loss=loss,\n                global_step=tf.train.get_global_step(),\n                learning_rate=learning_rate_tensor, \n                optimizer=optimizer,\n                variables=rnn_vars )\n\n            tf.summary.scalar( \'learning_rate\', learning_rate_tensor )\n\n    return train_op, loss\n\n\n\n\ndef _get_testing( rnn_logits,sequence_length,label,label_length,\n                  continuous_eval, lexicon, lexicon_prior ):\n    """"""Create ops for testing (all scalars): \n       loss: CTC loss function value, \n       label_error:   batch level edit distance on beam search max\n       sequence_error: batch level sequence error rate\n    """"""\n\n    with tf.name_scope( ""train"" ):\n        # Reduce by mean (rather than sum) if doing continuous evaluation\n        batch_loss = model.ctc_loss_layer( rnn_logits,label,sequence_length,\n                                           reduce_mean=continuous_eval) \n    with tf.name_scope( ""test"" ):\n        predictions,_ = _get_output( rnn_logits, sequence_length,\n                                     lexicon, lexicon_prior )\n\n        hypothesis = tf.cast( predictions[0], tf.int32 ) # for edit_distance\n\n        # Per-sequence statistic\n        num_label_errors = tf.edit_distance( hypothesis, label, \n                                             normalize=False )\n\n        # Per-batch summary counts\n        batch_num_label_errors = tf.reduce_sum( num_label_errors)\n        batch_num_sequence_errors = tf.count_nonzero( num_label_errors, axis=0 )\n        batch_num_labels = tf.reduce_sum( label_length )\n        \n        # Wide integer type casts (prefer unsigned, but truediv dislikes those)\n        batch_num_label_errors = tf.cast( batch_num_label_errors, tf.int64 )\n        batch_num_sequence_errors = tf.cast( batch_num_sequence_errors, \n                                             tf.int64 )\n        batch_num_labels = tf.cast( batch_num_labels, tf.int64)\n        \n    return batch_loss, batch_num_label_errors, batch_num_sequence_errors, \\\n        batch_num_labels, predictions\n\n\ndef _get_loss_ops( batch_loss ):\n    """"""Calculates the total loss by accumulating for batches and returns\n    the average""""""\n\n    var_collections=[tf.GraphKeys.LOCAL_VARIABLES]\n\n    # Variable to tally across batches (all initially zero)\n    total_loss = tf.Variable( 0, trainable=False,\n                              name=\'total_loss\',\n                              dtype=tf.float32,\n                              collections=var_collections )\n\n    # Create the ""+="" update op\n    update_op = tf.assign_add( total_loss, batch_loss )\n\n    return total_loss, update_op\n\n\ndef _get_label_err_ops( batch_num_label_error, batch_total_labels ):\n    """"""Calculates the label error by accumulating for batches and returns\n    the average""""""\n\n    var_collections=[tf.GraphKeys.LOCAL_VARIABLES]\n\n    # Variables to tally across batches (all initially zero)\n    total_num_label_errors = tf.Variable( 0, trainable=False,\n                                          name=\'total_num_label_errors\',\n                                          dtype=tf.int64,\n                                          collections=var_collections )\n\n    total_num_labels =  tf.Variable( 0, trainable=False,\n                                     name=\'total_num_labels\',\n                                     dtype=tf.int64,\n                                     collections=var_collections )\n\n    # Create the ""+="" update ops and group together as one\n    update_label_errors    = tf.assign_add( total_num_label_errors,\n                                            batch_num_label_error )\n    update_num_labels     = tf.assign_add( total_num_labels,\n                                           batch_total_labels )\n\n    update_op = tf.group(update_label_errors,update_num_labels )\n\n    \n    # Get the average label error across all inputs\n    label_error = tf.truediv( total_num_label_errors, \n                              total_num_labels,\n                              name=\'label_error\' )    \n   \n    return label_error, update_op, total_num_label_errors, total_num_labels\n\n\ndef _get_seq_err_ops( batch_num_sequence_errors, label_length ):\n    """"""Calculates the sequence error by accumulating for batches and returns\n    the average""""""\n\n    var_collections=[tf.GraphKeys.LOCAL_VARIABLES]\n\n    # Variables to tally across batches (all initially zero)\n    total_num_sequence_errors =  tf.Variable( 0, trainable=False,\n                                              name=\'total_num_sequence_errors\',\n                                              dtype=tf.int64,\n                                              collections=var_collections )\n\n    total_num_sequences =  tf.Variable( 0, trainable=False,\n                                        name=\'total_num_sequences\',\n                                        dtype=tf.int64,\n                                        collections=var_collections )\n\n    # Get the batch size and cast it appropriately\n    batch_size = tf.shape( label_length )[0]\n    batch_size = tf.cast( batch_size, tf.int64 )\n\n    # Create the ""+="" update ops and group together as one\n    update_sequence_errors = tf.assign_add( total_num_sequence_errors,\n                                            batch_num_sequence_errors )\n    update_num_sequences   = tf.assign_add( total_num_sequences,\n                                            batch_size )\n\n    update_op = tf.group(update_sequence_errors, update_num_sequences)\n\n    # Get the average sequence error across all inputs\n    sequence_error = tf.truediv( total_num_sequence_errors,\n                                 total_num_sequences,\n                                 name=\'sequence_error\' )\n    \n    return sequence_error, update_op, total_num_sequence_errors,\\\n        total_num_sequences\n\n\ndef _get_dictionary_tensor( dictionary_path, charset ):\n    return tf.sparse_tensor_to_dense( tf.to_int32(\n\tdictionary_from_file( dictionary_path, charset )))\n\ndef _get_lexicon_output( rnn_logits, sequence_length, lexicon ):\n    """"""Create lexicon-restricted output ops\n        prediction: Dense BxT tensor of predicted character indices\n        seq_prob: Bx1 tensor of output sequence probabilities\n    """"""\n    # Note: TFWordBeamSearch.so must be in LD_LIBRARY_PATH (on *nix)\n    # from github.com/weinman/CTCWordBeamSearch branch var_seq_len\n    word_beam_search_module = tf.load_op_library(\'TFWordBeamSearch.so\')\n    beam_width = _ctc_beam_width\n    with open(lexicon) as lexicon_fd:\n        corpus = lexicon_fd.read().encode(\'utf8\')\n\n    rnn_probs = tf.nn.softmax(rnn_logits, axis=2) # decodes in expspace\n\n    # CTCWordBeamSearch requires a non-word char. We hack this by\n    # prepending a zero-prob "" "" entry to the rnn_probs\n    rnn_probs = tf.pad( rnn_probs,\n                        [[0,0],[0,0],[1,0]], # Add one slice of zeros\n                        mode=\'CONSTANT\',\n                        constant_values=0.0 )\n    chars = (\' \'+charset.out_charset).encode(\'utf8\')\n\n    # Assume words can be formed from all chars--if punctuation is added\n    # or numbers (etc) are to be treated differently, more such \n    # categories should be added to the charset module\n    wordChars = chars[1:]\n            \n    prediction,seq_prob = word_beam_search_module.word_beam_search(\n        rnn_probs,\n        sequence_length,\n        beam_width,\n        \'Words\', # Use No LM\n        0.0, # Irrelevant: No LM to smooth\n        corpus, # aka lexicon [are unigrams ignored?]\n        chars,\n        wordChars )\n    prediction = prediction - 1 # Remove hacky prepended non-word char\n\n    return prediction, seq_prob\n\n\ndef _get_open_output( rnn_logits, sequence_length ):\n    """"""Create open-vocabulary output ops for validation (testing) and prediction\n       prediction: BxT sparse result of CTC beam search decoding\n       seq_prob: Score of prediction\n    """"""\n    prediction,log_prob = tf.nn.ctc_beam_search_decoder(\n        rnn_logits,\n        sequence_length,\n        beam_width=_ctc_beam_width,\n        top_paths=1,\n        merge_repeated=True )\n    seq_prob = tf.math.exp(log_prob)\n\n    return prediction, seq_prob\n\n\ndef _get_merged_output( lex_prediction, lex_seq_prob,\n                        open_prediction, open_seq_prob, lexicon_prior ):\n    """"""Create merged output ops based on maximum posterior probobability\n    """"""\n    # TODO: See whether tf.where with x and y would be faster/easier\n    # Calculate posterior probability for lexicon versus open prediction\n    seq_joint = tf.concat( [lexicon_prior * lex_seq_prob,\n                            (1-lexicon_prior) * open_seq_prob ],\n                           axis=1 )  # Bx2\n    #seq_post = seq_joint / tf.reduce_sum( seq_joint, axis=1, keepdims=True)\n    # argmax posterior to find most likely prediction\n    seq_class = tf.argmax( seq_joint, axis=1, output_type=tf.int32 )\n    # stack predictions for gathering\n    predictions = tf.stack( [lex_prediction, open_prediction], axis=0) # 2xBxT\n    # pair off classification (first index) and batch element [0,B) for gather\n    indices = tf.stack( [seq_class,\n                         tf.range( tf.shape(seq_class)[0]) ],\n                        axis=1) # Bx2\n    prediction = tf.gather_nd( predictions, indices) # BxT\n    seq_prob = tf.gather_nd( tf.transpose(seq_joint), indices) # Bx1\n\n    return prediction, seq_prob\n\ndef _get_output( rnn_logits, sequence_length, lexicon, lexicon_prior ):\n    """"""Create output ops for validation (testing) and prediction\n       prediction: Result of CTC beam search decoding\n       seq_prob: Score of prediction\n    """"""\n    with tf.name_scope(""test""):\n\tif lexicon:\n            ctc_blank = (rnn_logits.shape[2]-1)\n            lex_prediction,lex_seq_prob = _get_lexicon_output(rnn_logits,\n                                                      sequence_length, lexicon )\n            if lexicon_prior != None:\n                # Need to run both open and closed vocabulary modes\n                open_prediction, open_seq_prob = _get_open_output(\n                    rnn_logits, sequence_length)\n                # Convert top open output prediction to dense values\n                # NOTE: What to do if the sparse result is shorter than T?\n                # Reshape sparse version of open_prediction?\n                open_prediction = tf.cast(\n                    tf.sparse.to_dense(\n                        tf.sparse.reset_shape(\n                            open_prediction[0],\n                            new_shape=tf.shape(lex_prediction) ),\n                        default_value=ctc_blank),\n                    tf.int32)\n                prediction, seq_prob = _get_merged_output(\n                    lex_prediction, lex_seq_prob,\n                    open_prediction, open_seq_prob, lexicon_prior )\n            else:\n                prediction = lex_prediction\n                seq_prob = lex_seq_prob\n                \n            # Match tf.nn.ctc_beam_search_decoder outputs: list of sparse\n\n            # (1) CTCWordBeamSearch returns a dense tensor matching input \n            # sequence length (padded with ctc blanks).\n            # We convert to sparse tightly so trailing blanks are trimmed from \n            # the dense_shape of the resulting SparseTensor\n            prediction = utils.dense_to_sparse_tight(\n                prediction,\n                eos_token=ctc_blank )\n            # (2) CTCWordBeamSearch returns only top match, so convert to list\n            prediction = [prediction]\n\telse:\n            prediction, seq_prob = _get_open_output(rnn_logits, sequence_length)\n            \n    return prediction, seq_prob\n\n\ndef train_fn( scope, tune_from, learning_rate, \n                    decay_steps, decay_rate, decay_staircase, momentum ): \n    """"""Returns a function that trains the model""""""\n\n    def train( features, labels, mode ):\n\n        logits, sequence_length = _get_image_info( features, mode )\n\n        train_op, loss = _get_training( logits,labels,\n                                        sequence_length, \n                                        scope, learning_rate, \n                                        decay_steps, decay_rate, \n                                        decay_staircase, momentum )\n        \n        # Initialize weights from a pre-trained model\n        # NOTE: Does not work when num_gpus>1, cf. tensorflow issue 21615.\n        scaffold = tf.train.Scaffold( init_fn=\n                                      _get_init_pretrained( tune_from ) )\n\n        return tf.estimator.EstimatorSpec( mode=mode, \n                                           loss=loss, \n                                           train_op=train_op,\n                                           scaffold=scaffold )\n    return train\n\n\ndef evaluate_fn( lexicon=None, lexicon_prior=None ):\n    """"""Returns a function that evaluates the model for all batches at once or \n    continuously for one batch""""""\n\n    def evaluate( features, labels, mode, params ):\n                \n        logits, sequence_length = _get_image_info( features, mode )\n\n        continuous_eval = params[\'continuous_eval\']\n        length = features[\'length\']\n            \n        # Get the predictions\n        batch_loss,\\\n            batch_label_error,\\\n            batch_sequence_error, \\\n            batch_total_labels, \\\n            _ = _get_testing( logits,sequence_length,labels, length, \n                              continuous_eval, lexicon, lexicon_prior )\n        \n        # Label errors: mean over the batch and updated total number\n        mean_label_error, \\\n            update_op_label, \\\n            total_num_label_errors, \\\n            total_num_labels = _get_label_err_ops( batch_label_error, \n                                                  batch_total_labels )\n        \n        # Sequence errors: mean over the batch and updated total number\n        mean_sequence_error,\\\n            update_op_seq,\\\n            total_num_sequence_errs,\\\n            total_num_sequences = _get_seq_err_ops( batch_sequence_error, \n                                                   length )\n        \n        # Loss: Accumulated total loss over batches\n        total_loss, update_op_loss   = _get_loss_ops( batch_loss )\n        mean_loss =  tf.truediv( total_loss, \n                                 tf.cast( total_num_sequences, tf.float32 ),\n                                 name=\'mean_loss\' )    \n   \n\n        # Print the metrics while doing continuous evaluation (evaluate.py) \n        # Note: tf.Print is identical to tf.identity, except it prints\n        # the list of metrics as a side effect\n        if (continuous_eval):\n            global_step = tf.train.get_or_create_global_step()\n            mean_sequence_error = tf.Print( mean_sequence_error, \n                                            [global_step,\n                                             batch_loss,\n                                             mean_label_error,\n                                             mean_sequence_error] ,\n                                            first_n=1)\n            \n            # Create summaries for the metrics during continuous eval\n            tf.summary.scalar( \'loss\', tensor=batch_loss,\n                               family=\'test\' )\n            tf.summary.scalar( \'label_error\', tensor=mean_label_error,\n                               family=\'test\' )\n            tf.summary.scalar( \'sequence_error\',\n                               tensor=mean_sequence_error,\n                               family=\'test\' )\n            \n        # Convert to tensor from Variable in order to pass it to eval_metric_ops\n        total_num_label_errors  = tf.convert_to_tensor( total_num_label_errors )\n        total_num_labels        = tf.convert_to_tensor( total_num_labels )\n        total_num_sequence_errs = tf.convert_to_tensor( total_num_sequence_errs )\n        total_num_sequences     = tf.convert_to_tensor( total_num_sequences )\n        total_loss              = tf.convert_to_tensor( total_loss )\n            \n        # All the ops that will be passed to the EstimatorSpec object\n        eval_metric_ops = {\n            \'mean_loss\': ( mean_loss, update_op_loss ),\n            \'mean_label_error\': ( mean_label_error, update_op_label ),\n            \'mean_sequence_error\': ( mean_sequence_error, update_op_seq ),\n            \'total_loss\': ( total_loss, tf.no_op() ),\n            \'total_num_label_errors\': ( total_num_label_errors, tf.no_op() ),\n            \'total_num_labels\':( total_num_labels, tf.no_op() ),\n            \'total_num_sequence_errs\': ( total_num_sequence_errs, tf.no_op() ),\n            \'total_num_sequences\': ( total_num_sequences, tf.no_op() ) \n        }\n        \n        return tf.estimator.EstimatorSpec( mode=mode, \n                                           loss=batch_loss, \n                                           eval_metric_ops=eval_metric_ops )\n    return evaluate\n\n\ndef predict_fn( lexicon, lexicon_prior ):\n    """"""Returns a function that runs the model on the input data \n       (e.g., for validation)""""""\n\n    def predict( features, labels, mode ):\n\n        logits, sequence_length = _get_image_info(features, mode)\n        \n        predictions, log_probs = _get_output( logits, sequence_length,\n                                              lexicon, lexicon_prior )\n\n        if lexicon:\n            # TFWordBeamSearch produces only a single value, but its\n            # given dense shape is the original sequence length\n            # dense_to_sparse_tight in_get_output should filter out\n            # the excess, but we set the dense fill value to ctc_blank\n            # now to catch any potential errors/bugs downstream later\n            ctc_blank = (logits.shape[2]-1)\n            final_pred = tf.sparse.to_dense( predictions[0],\n                                             default_value=ctc_blank ) \n        else:\n        # tf.nn.ctc_beam_search produces SparseTensor but EstimatorSpec\n        # predictions only takes dense tensors\n            final_pred = tf.sparse.to_dense( predictions[0], \n                                             default_value=0 ) \n        \n        return tf.estimator.EstimatorSpec( mode=mode,\n                                           predictions={ \'labels\': final_pred,\n                                                         \'score\': log_probs })\n\n    return predict\n'"
src/pipeline.py,17,"b'# CNN-LSTM-CTC-OCR\n# Copyright (C) 2017,2018 Jerod Weinman, Abyaya Lamsal, Benjamin Gafford\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n# pipeline.py -- Constructs Dataset objects for use in training, testing, etc.\n\nimport tensorflow as tf\nimport numpy as np\nimport sys\n\ndef get_data( use_static_data,\n              base_dir=None,\n              file_patterns=None,\n              num_threads=4,\n              batch_size=32,\n              boundaries=[32, 64, 96, 128, 160, 192, 224, 256],\n              num_epochs=None,\n              filter_fn=None,\n              synth_config_file=None,\n              use_ipc_synth=True ):\n    """"""Get Dataset according to parameters\n    Parameters:\n      use_static_data   : boolean for whether to use static or dynamic data\n      base_dir          : string for static data locations (static data only)\n      file_patterns     : string for static data patterns  (static data only)\n      num_threads       : number of threads to use for IO / preprocessing\n      batch_size        : number of images to use in each batch \n      boundaries        : boundaries for bucketing. If None, no bucketing used\n      num_epochs        : if None, data repeats infinitely (static data only)\n      filter_fn         : filtering function\n      synth_config_file : string for synthesizer config file (dynamic data only)\n      use_ipc_synth:    : boolean for IPC versus single-thread synthesizer\n                      \n    Returns:\n      dataset : tf.data.Dataset object.\n                elements structured as [features, labels]\n                Example feature structure can be seen in postbatch_fn \n                in mjsynth.py\n    """"""    \n    # Elements to be buffered\n    num_buffered_elements = num_threads * batch_size * 2\n\n    # Get correct import and args for given pipeline\n    # `dpipe` will be a variable for the package name\n    if use_static_data:\n        import mjsynth as dpipe\n        dpipe_args = ( base_dir, \n                       file_patterns, \n                       num_threads, \n                       num_buffered_elements )\n    else:\n        # For dynamic data only -- refer to README.md for additional instructions\n        import maptextsynth as dpipe\n\n        # Ensure synth_config_file is specified\n        if not synth_config_file:\n            sys.stderr.write(""Dynamic data pipeline requires synth_config_file."")\n            sys.exit(1)\n            \n        # num_producers=0 uses single-threaded, nonbuffered synthesizer\n        num_producers = num_threads if use_ipc_synth else 0\n        \n        dpipe_args = ( synth_config_file,\n                       num_producers )\n\n    # Get raw data\n    dataset = dpipe.get_dataset( dpipe_args )\n    dataset = dataset.prefetch( num_buffered_elements )\n    \n    # Preprocess data\n    dataset = dataset.map( dpipe.preprocess_fn, \n                           num_parallel_calls=num_threads )\n    dataset = dataset.prefetch( num_buffered_elements )\n    \n    # Remove input that doesn\'t fit necessary specifications\n    if filter_fn:\n        dataset = dataset.filter( filter_fn )\n        dataset = dataset.prefetch( num_buffered_elements )\n\n    # Bucket and batch appropriately\n    if boundaries:\n        dataset = dataset.apply( tf.contrib.data.bucket_by_sequence_length(\n            element_length_func=dpipe.element_length_fn,\n            # Create numpy array as follows: [batch_size,...,batch_size]\n            bucket_batch_sizes=np.full( len( boundaries ) + 1, \n                                        batch_size ),\n            bucket_boundaries=boundaries ) ) \n    else:\n        # Dynamically pad batches to match largest in batch\n        dataset = dataset.padded_batch( batch_size, \n                                        padded_shapes=dataset.output_shapes )\n    \n    # Update to account for batching\n    num_buffered_elements = num_threads * 2\n    \n    dataset = dataset.prefetch( num_buffered_elements )\n    \n    # Repeat for num_epochs  \n    if num_epochs and use_static_data:\n        dataset = dataset.repeat( num_epochs )\n    # Repeat indefinitely if no num_epochs is specified\n    elif use_static_data:\n        dataset = dataset.repeat()\n    \n    # Prepare dataset for Estimator ingestion\n    # ie: sparsify labels for CTC operations (eg loss, decoder)\n    # and convert elements to be [features, label]\n    dataset = dataset.map( dpipe.postbatch_fn,\n                           num_parallel_calls=num_threads )\n    dataset = dataset.prefetch( num_buffered_elements )\n    \n    return dataset\n\n\ndef rescale_image( image ):\n    """"""Rescale from uint8([0,255]) to float([-0.5,0.5])""""""\n    image = tf.image.convert_image_dtype( image, tf.float32 )\n    image = tf.subtract( image, 0.5 )\n    return image\n\n\n\ndef pack_image( image ):\n    """"""\n    Pack the image in a dataset into the model_fn-appropriate dictionary with \n    features and labels, where features is a dictionary containing  \n    \'image\' and \'width\' values.\n    """"""\n    width = tf.size( image[1] )\n    \n    # Pre-process the images\n    proc_image = tf.reshape( image,[1,32,-1,1] ) # Make first dim batch\n\n    # Pack the modified image data into a dictionary\n    features = {\'image\': proc_image, \'width\': width}\n\n    # Labels unused for prediction-only validation; construct a NOP value instead\n    label = tf.constant(0)\n    \n    return features, label\n\n\ndef normalize_image( image ):\n    """"""Normalize: convert uint8 RGB to gray, rescale, and resize image height""""""\n\n    # Convert to grayscale\n    image = tf.image.rgb_to_grayscale( image )\n    \n    # Rescale from uint8([0,255]) to float([-0.5,0.5])\n    image = rescale_image( image )\n\n    # Resize to 32 pixels high\n    image_height = tf.cast(tf.shape(image)[0], tf.float64)\n    image_width = tf.shape(image)[1]\n\n    scaled_image_width = tf.cast(\n        tf.round(\n            tf.multiply(tf.cast(image_width,tf.float64),\n                        tf.divide(32.0,image_height)) ),\n        tf.int32)\n\n    image = tf.image.resize_images(image, [32, scaled_image_width],\n                                   tf.image.ResizeMethod.BICUBIC )\n\n    return image\n\n'"
src/test.py,18,"b'# CNN-LSTM-CTC-OCR\n# Copyright (C) 2017,2018 Jerod Weinman, Abyaya Lamsal, Benjamin Gafford\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n# test.py -- Calculates evaluation metrics on an entire Dataset\n\nimport tensorflow as tf\nimport pipeline\nimport filters\nimport model_fn\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string( \'model\',\'../data/model\',\n                            """"""Directory for model checkpoints"""""" )\ntf.app.flags.DEFINE_string( \'lexicon\',None,\n\t\t\t    """"""File containing lexicon of image words"""""" )\ntf.app.flags.DEFINE_float( \'lexicon_prior\',None,\n\t\t\t    """"""Prior bias [0,1] for lexicon word"""""" )\n\ntf.app.flags.DEFINE_integer( \'batch_size\',2**9,\n                             """"""Eval batch size"""""" )\n\ntf.app.flags.DEFINE_string( \'test_path\',\'../data/\',\n                            """"""Base directory for test/validation data"""""" )\ntf.app.flags.DEFINE_string( \'filename_pattern\',\'test/words-*\',\n                            """"""File pattern for test input data"""""" )\ntf.app.flags.DEFINE_integer( \'num_input_threads\',4,\n                             """"""Number of readers for input data"""""" )\n\ntf.app.flags.DEFINE_integer(\'min_image_width\',None,\n                            """"""Minimum allowable input image width"""""")\ntf.app.flags.DEFINE_integer(\'max_image_width\',None,\n                            """"""Maximum allowable input image width"""""")\ntf.app.flags.DEFINE_integer(\'min_string_length\',None,\n                            """"""Minimum allowable input string length"""""")\ntf.app.flags.DEFINE_integer(\'max_string_length\',None,\n                            """"""Maximum allowable input string_length"""""")\n\n\ndef _get_input():\n    """"""\n    Get tf.data.Dataset object according to command-line flags for testing\n    using tf.estimator.Estimator\n    Returns:\n      dataset : elements structured as [features, labels]\n                feature structure can be seen in postbatch_fn \n                in mjsynth.py\n    """"""\n\n    # WARNING: More than two filters causes SEVERE throughput slowdown\n    filter_fn = filters.input_filter_fn \\\n                ( min_image_width=FLAGS.min_image_width,\n                  max_image_width=FLAGS.max_image_width,\n                  min_string_length=FLAGS.min_string_length,\n                  max_string_length=FLAGS.max_string_length )\n\n    # Get data according to flags\n    dataset = pipeline.get_data( use_static_data=True,\n                                 base_dir=FLAGS.test_path,\n                                 file_patterns=str.split(\n                                     FLAGS.filename_pattern,\n                                     \',\'),\n                                 num_threads=FLAGS.num_input_threads,\n                                 batch_size=FLAGS.batch_size,\n                                 filter_fn=filter_fn,\n                                 num_epochs=1 )\n    return dataset\n\n\ndef _get_config():\n    """"""Setup config to soften device placement""""""\n\n    device_config=tf.ConfigProto(\n        allow_soft_placement=True, \n        log_device_placement=False )\n\n    custom_config = tf.estimator.RunConfig(session_config=device_config)\n\n    return custom_config\n\n\ndef main(argv=None):\n  \n    # Initialize the classifier\n    classifier = tf.estimator.Estimator(\n        config= _get_config(),\n        model_fn=model_fn.evaluate_fn( FLAGS.lexicon, FLAGS.lexicon_prior), \n        model_dir=FLAGS.model,\n        params={\'continuous_eval\': False} )\n\n    evaluations = classifier.evaluate( input_fn=_get_input )\n    \n    print(evaluations)\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
src/train.py,33,"b'# CNN-LSTM-CTC-OCR\n# Copyright (C) 2017,2018 Jerod Weinman, Abyaya Lamsal, Benjamin Gafford\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n# train.py -- Train all or only part of the model from scratch or an\n#   existing checkpoint.\n\nimport tensorflow as tf\nimport pipeline\nimport charset\nimport model_fn\nimport filters\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\'output\',\'../data/model\',\n                          """"""Directory for event logs and checkpoints"""""")\ntf.app.flags.DEFINE_string(\'tune_from\',\'\',\n                          """"""Path to pre-trained model checkpoint"""""")\ntf.app.flags.DEFINE_string(\'tune_scope\',\'\',\n                          """"""Variable scope for training"""""")\n\ntf.app.flags.DEFINE_integer(\'batch_size\',2**5,\n                            """"""Mini-batch size"""""")\ntf.app.flags.DEFINE_float(\'learning_rate\',1e-4,\n                          """"""Initial learning rate"""""")\ntf.app.flags.DEFINE_float(\'momentum\',0.9,\n                          """"""Optimizer gradient first-order momentum"""""")\ntf.app.flags.DEFINE_float(\'decay_rate\',0.9,\n                          """"""Learning rate decay base"""""")\ntf.app.flags.DEFINE_float(\'decay_steps\',2**16,\n                          """"""Learning rate decay exponent scale"""""")\ntf.app.flags.DEFINE_boolean(\'decay_staircase\',False,\n                          """"""Staircase learning rate decay by integer division"""""")\ntf.app.flags.DEFINE_integer(\'max_num_steps\', 2**21,\n                            """"""Number of optimization steps to run"""""")\ntf.app.flags.DEFINE_integer(\'save_checkpoint_secs\', 120,\n                            """"""Interval between daving checkpoints"""""")\n\ntf.app.flags.DEFINE_integer(\'num_input_threads\',4,\n                          """"""Number of readers/generators for input data"""""")\ntf.app.flags.DEFINE_integer(\'num_gpus\', 1,\n                            """"""Number of GPUs to use for distributed training"""""")\ntf.app.flags.DEFINE_boolean(\'bucket_data\',True,\n                            """"""Bucket training data by width for efficiency"""""")\n\ntf.app.flags.DEFINE_integer(\'min_image_width\',None,\n                            """"""Minimum allowable input image width"""""")\ntf.app.flags.DEFINE_integer(\'max_image_width\',None,\n                            """"""Maximum allowable input image width"""""")\ntf.app.flags.DEFINE_integer(\'min_string_length\',None,\n                            """"""Minimum allowable input string length"""""")\ntf.app.flags.DEFINE_integer(\'max_string_length\',None,\n                            """"""Maximum allowable input string_length"""""")\n\ntf.app.flags.DEFINE_boolean(\'static_data\', True,\n                            """"""Whether to use static data \n                            (false for dynamic data)"""""")\ntf.app.flags.DEFINE_string(\'train_path\',\'../data/train/\',\n                           """"""Base directory for training data"""""")\ntf.app.flags.DEFINE_string(\'filename_pattern\',\'words-*\',\n                           """"""File pattern for input data"""""")\n\ntf.app.flags.DEFINE_string(\'synth_config_file\', None,\n                           """"""Location of config file for map text synthesizer"""""")\ntf.app.flags.DEFINE_boolean(\'ipc_synth\',True,\n                            """"""Use multi-process dynamic image synthesis"""""")\n\n\n# For displaying various statistics while training\ntf.logging.set_verbosity( tf.logging.INFO )\n\n\ndef _get_input():\n    """"""\n    Get tf.data.Dataset according to command-line flags for training \n    using tf.estimator.Estimator\n\n    Note: Default behavior is bucketing according to default bucket boundaries\n    listed in pipeline.get_data\n\n    Returns:\n      dataset : elements structured as [features, labels]\n                feature structure can be seen in postbatch_fn \n                in mjsynth.py or maptextsynth.py for static or dynamic\n                data pipelines respectively\n    """"""\n\n    # WARNING: More than two filters causes SEVERE throughput slowdown\n    filter_fn = filters.input_filter_fn \\\n                ( min_image_width=FLAGS.min_image_width,\n                  max_image_width=FLAGS.max_image_width,\n                  min_string_length=FLAGS.min_string_length,\n                  max_string_length=FLAGS.max_string_length,\n                  check_input=(not FLAGS.static_data) )\n    \n    gpu_batch_size = FLAGS.batch_size / FLAGS.num_gpus\n    \n    # Pack keyword arguments into dictionary\n    data_args = { \'num_threads\': FLAGS.num_input_threads,\n                  \'batch_size\': gpu_batch_size,\n                  \'filter_fn\': filter_fn }\n\n    if FLAGS.static_data: # Pack data stream-specific parameters\n        data_args[\'base_dir\'] = FLAGS.train_path\n        data_args[\'file_patterns\'] = str.split(FLAGS.filename_pattern, \',\')\n    else:\n        data_args[\'synth_config_file\'] = FLAGS.synth_config_file\n        data_args[\'use_ipc_synth\'] = FLAGS.ipc_synth\n\n    if not FLAGS.bucket_data:\n        data_args[\'boundaries\']=None # Turn off bucketing (on by default)\n    elif not FLAGS.static_data: # Extra buckets for the wider synthetic data\n        data_args[\'boundaries\']=[32, 64, 96, 128, 160, 192, 224, 256,\n                                 288, 320, 352, 384, 416, 448, 480, 512]\n        \n    # Get data according to flags\n    dataset = pipeline.get_data( FLAGS.static_data, **data_args)\n\n    return dataset\n\n\ndef _get_distribution_strategy():\n    """"""Configure training distribution strategy""""""\n    \n#    if FLAGS.num_gpus == 1: # cannot restore until at least r1.10 (02ae1e2)\n#        return tf.contrib.distribute.OneDeviceStrategy(device=\'/gpu:0\')\n    if FLAGS.num_gpus > 1:\n        return tf.contrib.distribute.MirroredStrategy(num_gpus=FLAGS.num_gpus)\n    else:\n        return None\n\n    \ndef _get_config():\n    """"""Setup config to soften device placement and set chkpt saving intervals""""""\n    \n    device_config=tf.ConfigProto(\n        allow_soft_placement=True, \n        log_device_placement=False)\n\n    custom_config = tf.estimator.RunConfig(\n        session_config=device_config,\n        train_distribute=_get_distribution_strategy(),\n        save_checkpoints_secs=FLAGS.save_checkpoint_secs)\n\n    return custom_config \n\n\ndef main( argv=None ):\n\n    # Set up a dictionary of arguments to be passed for training\n    train_args = {\'scope\': FLAGS.tune_scope, \n                  \'tune_from\': FLAGS.tune_from, \n                  \'learning_rate\': FLAGS.learning_rate, \n                  \'decay_steps\': FLAGS.decay_steps, \n                  \'decay_rate\': FLAGS.decay_rate, \n                  \'decay_staircase\': FLAGS.decay_staircase, \n                  \'momentum\':FLAGS.momentum}\n\n    # Initialize the classifier\n    classifier = tf.estimator.Estimator( config=_get_config(), \n                                         model_fn=model_fn.train_fn(\n                                             **train_args),\n                                         model_dir=FLAGS.output )\n   \n    # Train the model\n    classifier.train( input_fn=_get_input, max_steps=FLAGS.max_num_steps )\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
src/utils.py,11,"b'# CNN-LSTM-CTC-OCR\n# Copyright (C) 2019 Jerod Weinman, Abyaya Lamsal, Benjamin Gafford\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nimport tensorflow as tf\n\n\n\n# The procedure dense_to_sparse_tight is a derivative work of the function\n# dense_to_sparse from tensorflow.contrib.layers.python.layers.layers\n# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/ \\\n# /layers/python/layers/layers.py which bears the following copyright notice:\n#\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# The original copyrighted work is licensed under Apache License 2.0, which is\n# available at http://www.apache.org/licenses/LICENSE-2.0, while the\n# derivative work is licensed under GPLv3, to the maximum extent warranted by\n# the original license.\nfrom tensorflow.contrib.layers.python.layers import utils as layers_utils\n\ndef dense_to_sparse_tight(tensor, eos_token=0,\n                          outputs_collections=None, scope=None):\n    """"""Converts a dense tensor into a sparse tensor whose shape is no larger than \n     strictly necessary.\n    Args:\n     tensor: An `int` `Tensor` to be converted to a `Sparse`.\n     eos_token: An integer.\n       It is part of the target label that signifies the end of a sentence.\n     outputs_collections: Collection to add the outputs.\n     scope: Optional scope for name_scope.\n    """"""\n    with tf.variable_scope(scope, \'dense_to_sparse_tight\', [tensor]) as sc:\n        tensor = tf.convert_to_tensor(tensor)\n        indices = tf.where(\n            tf.math.not_equal(tensor, tf.constant(eos_token,\n                                                tensor.dtype)))\n        # Need to verify there are *any* indices that are not eos_token\n        # If none, give shape [1,0].\n        shape = tf.cond( tf.not_equal(tf.shape(indices)[0],\n                                      tf.constant(0)), # Found valid indices?\n                         true_fn=lambda: tf.cast(tf.reduce_max(indices,axis=0),\\\n                                                 tf.int64) + 1,\n                         false_fn=lambda: tf.cast([1,0], tf.int64) )\n        values = tf.gather_nd(tensor, indices)\n        outputs = tf.SparseTensor(indices, values, shape)\n        return layers_utils.collect_named_outputs(outputs_collections,\n                                                  sc.name, outputs)\n    \n'"
src/validate.py,12,"b'# CNN-LSTM-CTC-OCR\n# Copyright (C) 2017,2018 Jerod Weinman, Abyaya Lamsal, Benjamin Gafford\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n# validate.py - Run model directly on from paths to image filenames. \n# NOTE: assumes mjsynth files are given by adding an extra row of padding\n\nimport sys\nimport numpy as np\nfrom PIL import Image\nimport tensorflow as tf\n\nimport model_fn\nimport charset\n\nimport mjsynth\nimport pipeline\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string( \'model\',\'../data/model\',\n                            """"""Directory for model checkpoints"""""" )\ntf.app.flags.DEFINE_boolean( \'print_score\', False,\n                             """"""Print log probability scores with predictions"""""" )\ntf.app.flags.DEFINE_string( \'lexicon\',\'\',\n\t\t\t    """"""File containing lexicon of image words"""""" )\ntf.app.flags.DEFINE_float( \'lexicon_prior\',None,\n\t\t\t    """"""Prior bias [0,1] for lexicon word"""""" )\n\n\ntf.logging.set_verbosity( tf.logging.INFO )\n\n\ndef _get_image( filename ):\n    """"""Load image data for placement in graph""""""\n\n    image = Image.open( filename ) \n    image = np.array( image )\n    # in mjsynth, all three channels are the same in these grayscale-cum-RGB data\n    image = image[:,:,:1] # so just extract first channel, preserving 3D shape\n\n    return image\n\n\ndef _get_input():\n    """"""Create a dataset of images by reading from stdin""""""\n\n    # Eliminate any trailing newline from filename\n    image_data = _get_image( raw_input().rstrip() )\n\n    # Initializing the dataset with one image\n    dataset = tf.data.Dataset.from_tensors( image_data )\n\n    # Add the rest of the images to the dataset (if any)\n    for line in sys.stdin:\n        image_data = _get_image( line.rstrip() )\n        temp_dataset = tf.data.Dataset.from_tensors( image_data )\n        dataset = dataset.concatenate( temp_dataset )\n\n    # mjsynth input images need preprocessing transformation (shape, range)\n    dataset = dataset.map( mjsynth.preprocess_image )\n\n    # pack results for model_fn.predict \n    dataset = dataset.map ( pipeline.pack_image )\n    return dataset\n\n\ndef _get_config():\n    """"""Setup session config to soften device placement""""""\n\n    device_config=tf.ConfigProto(\n        allow_soft_placement=True, \n        log_device_placement=False )\n\n    custom_config = tf.estimator.RunConfig( session_config=device_config ) \n\n    return custom_config\n\n\ndef main(argv=None):\n    \n    classifier = tf.estimator.Estimator( config=_get_config(),\n                                         model_fn=model_fn.predict_fn(\n                                             FLAGS.lexicon,\n                                             FLAGS.lexicon_prior), \n                                         model_dir=FLAGS.model )\n    \n    predictions = classifier.predict( input_fn=_get_input )\n    \n    # Get all the predictions in string format\n    while True:\n        try:\n            results = next( predictions )\n            print \'results =\',results\n            pred_str = charset.label_to_string( results[\'labels\'] )\n            if FLAGS.print_score:\n                print pred_str, results[\'score\'][0]\n            else:\n                print pred_str\n        except StopIteration:\n            sys.exit()\n    \nif __name__ == \'__main__\':\n    tf.app.run()\n'"
