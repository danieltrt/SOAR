file_path,api_count,code
setup.py,0,"b""#!/usr/bin/env python\nimport os\nfrom setuptools import find_packages, setup\n\nversion = '0.7.5'\nPATH_ROOT = os.path.dirname(__file__)\n\n\ndef load_requirements(path_dir=PATH_ROOT, comment_char='#'):\n    with open(os.path.join(path_dir, 'requirements.txt'), 'r') as file:\n        lines = [ln.strip() for ln in file.readlines()]\n    reqs = []\n    for ln in lines:\n        # filer all comments\n        if comment_char in ln:\n            ln = ln[:ln.index(comment_char)]\n        if ln:  # if requirement is not empty\n            reqs.append(ln)\n    return reqs\n\n\nsetup(\n    name='test_tube',\n    packages=find_packages(),\n    version=version,\n    description='Experiment logger and visualizer',\n    author='William Falcon',\n    install_requires=load_requirements(PATH_ROOT),\n    author_email='will@hacstudios.com',\n    url='https://github.com/williamFalcon/test_tube',\n    download_url='https://github.com/williamFalcon/test_tube/archive/{}.tar.gz'.format(version),\n    keywords=[\n        'testing',\n        'machine learning',\n        'deep learning',\n        'prototyping',\n        'experimenting',\n        'modeling',\n    ],\n)\n"""
examples/__init__.py,0,b''
examples/hpc_cpu_example.py,0,"b'""""""Example launcher for a hyperparameter search on SLURM.""""""\nfrom test_tube import Experiment, HyperOptArgumentParser, SlurmCluster\n\n\ndef train(hparams, *args):\n    """"""Train your awesome model.\n\n    :param hparams: The arguments to run the model with.\n    """"""\n    # Initialize experiments and track all the hyperparameters\n    exp = Experiment(\n        name=hparams.test_tube_exp_name,\n        # Location to save the metrics.\n        save_dir=hparams.log_path,\n        # The experiment version is optional, but using the one \n        # from SLURM means the exp will not collide with other\n        # versions if SLURM runs multiple at once.\n        version=hparams.hpc_exp_number,\n        autosave=False,\n    )\n    exp.argparse(hparams)\n\n    # Pretend to train.\n    x = hparams.x_val\n    for train_step in range(0, 100):\n        y = hparams.y_val\n        out = x * y\n        exp.log({\'fake_err\': out.item()})  # Log metrics.\n\n    # Save exp when done.\n    exp.save()\n\n\nif __name__ == \'__main__\':\n    # Set up our argparser and make the y_val tunable.\n    parser = HyperOptArgumentParser(strategy=\'random_search\')\n    parser.add_argument(\'--test_tube_exp_name\', default=\'my_test\')\n    parser.add_argument(\'--log_path\', default=\'/some/path/to/log\')\n    parser.opt_list(\'--y_val\',\n        default=12, options=[1, 2, 3, 4, 5, 6], tunable=True)\n    parser.opt_list(\'--x_val\',\n        default=12, options=[20, 12, 30, 45], tunable=True)\n    hyperparams = parser.parse_args()\n\n    # Enable cluster training.\n    cluster = SlurmCluster(\n        hyperparam_optimizer=hyperparams,\n        log_path=hyperparams.log_path,\n        python_cmd=\'python3\',\n        test_tube_exp_name=hyperparams.test_tube_exp_name\n    )\n\n    # Email results if your hpc supports it.\n    cluster.notify_job_status(\n        email=\'some@email.com\', on_done=True, on_fail=True)\n\n    # SLURM Module to load.\n    cluster.load_modules([\n        \'python-3\',\n        \'anaconda3\'\n    ])\n\n    # Add commands to the non-SLURM portion.\n    cluster.add_command(\'source activate myCondaEnv\')\n\n    # Add custom SLURM commands which show up as:\n    # #comment\n    # #SBATCH --cmd=value\n    # ############\n    # cluster.add_slurm_cmd(\n    #    cmd=\'cpus-per-task\', value=\'1\', comment=\'CPUS per task.\')\n\n    # Set job compute details (this will apply PER set of hyperparameters.)\n    cluster.per_experiment_nb_cpus = 20\n    cluster.per_experiment_nb_nodes = 10\n\n    # Each hyperparameter combination will use 200 cpus.\n    cluster.optimize_parallel_cluster_cpu(\n        # Function to execute:\n        train,\n        # Number of hyperparameter combinations to search:\n        nb_trials=24,\n        job_name=\'first_tt_job\',\n        # This is what will display in the slurm queue:\n        job_display_name=\'short_name\')\n'"
examples/pytorch_hpc_example.py,0,"b'""""""Example launcher for a hyperparameter search on SLURM.\n\nThis example shows how to use gpus on SLURM with PyTorch.\n""""""\nimport torch\n\nfrom test_tube import Experiment, HyperOptArgumentParser, SlurmCluster\n\n\ndef train(hparams, *args):\n    """"""Train your awesome model.\n\n    :param hparams: The arguments to run the model with.\n    """"""\n    # Initialize experiments and track all the hyperparameters\n    exp = Experiment(\n        name=hparams.test_tube_exp_name,\n        # Location to save the metrics.\n        save_dir=hparams.log_path,\n        autosave=False,\n    )\n    exp.argparse(hparams)\n\n    # Pretend to train.\n    x = torch.rand((1, hparams.x_val))\n    for train_step in range(0, 100):\n        y = torch.rand((hparams.x_val, 1))\n        out = x.mm(y)\n        exp.log({\'fake_err\': out.item()})\n\n    # Save exp when .\n    exp.save()\n\n\nif __name__ == \'__main__\':\n    # Set up our argparser and make the y_val tunable.\n    parser = HyperOptArgumentParser(strategy=\'random_search\')\n    parser.add_argument(\'--test_tube_exp_name\', default=\'my_test\')\n    parser.add_argument(\'--log_path\', default=\'/some/path/to/log\')\n    parser.opt_list(\'--y_val\',\n        default=12, options=[1, 2, 3, 4, 5, 6], tunable=True)\n    parser.opt_list(\'--x_val\',\n        default=12, options=[20, 12, 30, 45], tunable=True)\n    hyperparams = parser.parse_args()\n\n    # Enable cluster training.\n    cluster = SlurmCluster(\n        hyperparam_optimizer=hyperparams,\n        log_path=hyperparams.log_path,\n        python_cmd=\'python3\',\n        test_tube_exp_name=hyperparams.test_tube_exp_name\n    )\n\n    # Email results if your hpc supports it.\n    cluster.notify_job_status(\n        email=\'some@email.com\', on_done=True, on_fail=True)\n\n    # SLURM Module to load.\n    cluster.load_modules([\n        \'python-3\',\n        \'anaconda3\'\n    ])\n\n    # Add commands to the non-SLURM portion.\n    cluster.add_command(\'source activate myCondaEnv\')\n\n    # Add custom SLURM commands which show up as:\n    # #comment\n    # #SBATCH --cmd=value\n    # ############\n    # cluster.add_slurm_cmd(\n    #    cmd=\'cpus-per-task\', value=\'1\', comment=\'CPUS per task.\')\n\n    # Set job compute details (this will apply PER set of hyperparameters.)\n    cluster.per_experiment_nb_gpus = 4\n    cluster.per_experiment_nb_nodes = 2\n    cluster.gpu_type = \'1080ti\'\n\n    # Each hyperparameter combination will use 8 gpus.\n    cluster.optimize_parallel_cluster_gpu(\n        # Function to execute:\n        train,\n        # Number of hyperparameter combinations to search:\n        nb_trials=24,\n        # This is what will display in the slurm queue:\n        job_name=\'first_tt_job\')\n'"
examples/tensorflow_example.py,3,"b'import tensorflow as tf\n\nfrom test_tube import Experiment, HyperOptArgumentParser\n\n""""""\nThis script demonstrates how to do a hyperparameter search over 2 parameters in tensorflow\non 4 simultaneous GPUs. Each trial will also save its own experiment logs.   \n\nA single trial gets allocated on a single GPU until all trials have completed.   \nThis means for 10 trials and 4 GPUs, we\'ll run 4 in parallel twice and the last 2 trials in parallel.   \n""""""\n\n\n# main training function (very simple)\ndef train(hparams):\n    # init exp and track all the parameters from the HyperOptArgumentParser\n    exp = Experiment(\n        name=hparams.test_tube_exp_name,\n        save_dir=hparams.log_path,\n        autosave=False,\n    )\n    exp.argparse(hparams)\n\n    # define tensorflow graph\n    x = tf.placeholder(dtype=tf.int32, name=\'x\')\n    y = tf.placeholder(dtype=tf.int32, name=\'y\')\n    out = x * y\n\n    sess = tf.Session()\n\n    # Run the tf op\n    for train_step in range(0, 100):\n        output = sess.run(out, feed_dict={x: hparams.x_val, y: hparams.y_val})\n        exp.log({\'fake_err\': output})\n\n    # save exp when we\'re done\n    exp.save()\n\n\n# set up our argparser and make the y_val tunable\nparser = HyperOptArgumentParser(strategy=\'random_search\')\nparser.add_argument(\'--test_tube_exp_name\', default=\'my_test\')\nparser.add_argument(\'--log_path\', default=\'/Users/waf/Desktop/test\')\nparser.opt_list(\'--y_val\', default=12, options=[1, 2, 3, 4], tunable=True)\nparser.opt_list(\'--x_val\', default=12, options=[20, 12, 30, 45], tunable=True)\nhyperparams = parser.parse_args()\n\n\n# optimize on 4 gpus at the same time\n# each gpu will get 1 experiment with a set of hyperparams\nhyperparams.optimize_parallel_gpu(train, gpu_ids=[\'1\', \'0\', \'3\', \'2\'], nb_trials=4, nb_workers=4)\n'"
test_tube/__init__.py,0,"b'""""""\nExperiment logger module\n""""""\n\nfrom .argparse_hopt import HyperOptArgumentParser\nfrom .hpc import SlurmCluster\nfrom .hyperopt import HyperParamOptimizer\nfrom .log import Experiment\n'"
test_tube/argparse_hopt.py,0,"b'import argparse\nimport json\nimport math\nimport os\nimport random\nimport re\nimport traceback\nfrom argparse import ArgumentParser\nfrom copy import deepcopy\nfrom gettext import gettext as _\nfrom multiprocessing import Pool, Queue\nfrom time import sleep\n\nimport numpy as np\n\nfrom .hyper_opt_utils import strategies\n\n# needed to work with pytorch multiprocess\ntry:\n    import torch\n    import multiprocessing\n    # multiprocessing.set_start_method(\'spawn\', force=True)\nexcept ModuleNotFoundError:\n    pass\n\n\ndef optimize_parallel_gpu_private(args):\n    trial_params, train_function = args[0], args[1]\n\n    # get set of gpu ids\n    gpu_id_set = g_gpu_id_q.get(block=True)\n\n    try:\n\n        # enable the proper gpus\n        os.environ[""CUDA_VISIBLE_DEVICES""] = gpu_id_set\n\n        # run training fx on the specific gpus\n        results = train_function(trial_params, gpu_id_set)\n\n        return [trial_params, results]\n\n    except Exception as e:\n        print(\'Caught exception in worker thread\', e)\n\n        # This prints the type, value, and stack trace of the\n        # current exception being handled.\n        traceback.print_exc()\n        return [trial_params, None]\n\n    finally:\n        g_gpu_id_q.put(gpu_id_set)\n\n\ndef optimize_parallel_cpu_private(args):\n    trial_params, train_function = args[0], args[1]\n\n    sleep(random.randint(0, 4))\n\n    # run training fx on the specific gpus\n    results = train_function(trial_params)\n\n    # True = completed\n    return [trial_params, results]\n\n\nclass HyperOptArgumentParser(ArgumentParser):\n    """"""\n    Subclass of argparse ArgumentParser which adds optional calls to sample from lists or ranges\n    Also enables running optimizations across parallel processes\n    """"""\n\n    # these are commands injected by test tube from cluster operations\n    TRIGGER_CMD = \'test_tube_from_cluster_hopt\'\n    SLURM_CMD_PATH = \'test_tube_slurm_cmd_path\'\n    SLURM_EXP_CMD = \'hpc_exp_number\'\n    SLURM_LOAD_CMD = \'test_tube_do_checkpoint_load\'\n    CMD_MAP = {\n        TRIGGER_CMD: bool,\n        SLURM_CMD_PATH: str,\n        SLURM_EXP_CMD: int,\n        SLURM_LOAD_CMD: bool\n    }\n\n    def __init__(self, strategy=\'grid_search\', **kwargs):\n        """"""\n\n        :param strategy: \'grid_search\', \'random_search\'\n        :param enabled:\n        :param experiment:\n        :param kwargs:\n        """"""\n        ArgumentParser.__init__(self, **kwargs)\n\n        self.strategy = strategy\n        self.trials = []\n        self.parsed_args = None\n        self.opt_args = {}\n        self.json_config_arg_name = None\n        self.pool = None\n\n    def __getstate__(self):\n        # capture what is normally pickled\n        state = self.__dict__.copy()\n\n        # remove all functions from the namespace\n        clean_state = {}\n        for k, v in state.items():\n            if not hasattr(v, \'__call__\'):\n                clean_state[k] = v\n\n        # what we return here will be stored in the pickle\n        return clean_state\n\n    def __setstate__(self, newstate):\n        # re-instate our __dict__ state from the pickled state\n        self.__dict__.update(newstate)\n\n    def add_argument(self, *args, **kwargs):\n        super(HyperOptArgumentParser, self).add_argument(*args, **kwargs)\n\n    def opt_list(self, *args, **kwargs):\n        options = kwargs.pop(""options"", None)\n        tunable = kwargs.pop(""tunable"", False)\n        self.add_argument(*args, **kwargs)\n        for i in range(len(args)):\n            arg_name = args[i]\n            self.opt_args[arg_name] = OptArg(obj_id=arg_name, opt_values=options, tunable=tunable)\n\n    def opt_range(\n            self,\n            *args,\n            **kwargs\n    ):\n        low = kwargs.pop(""low"", None)\n        high = kwargs.pop(""high"", None)\n        arg_type = kwargs[""type""]\n        nb_samples = kwargs.pop(""nb_samples"", 10)\n        tunable = kwargs.pop(""tunable"", False)\n        log_base = kwargs.pop(""log_base"", None)\n\n        self.add_argument(*args, **kwargs)\n        arg_name = args[-1]\n        self.opt_args[arg_name] = OptArg(\n            obj_id=arg_name,\n            opt_values=[low, high],\n            arg_type=arg_type,\n            nb_samples=nb_samples,\n            tunable=tunable,\n            log_base=log_base,\n        )\n\n    def json_config(self, *args, **kwargs):\n        self.add_argument(*args, **kwargs)\n        self.json_config_arg_name = re.sub(\'-\', \'\', args[-1])\n\n    def __parse_args(self, args=None, namespace=None):\n        # allow bypassing certain missing params which other parts of test tube may introduce\n        args, argv = self.parse_known_args(args, namespace)\n        args, argv = self.__whitelist_cluster_commands(args, argv)\n        if argv:\n            msg = _(\'unrecognized arguments: %s\')\n            self.error(msg % \' \'.join(argv))\n        return args\n\n    def __whitelist_cluster_commands(self, args, argv):\n        parsed = {}\n\n        # build a dict where key = arg, value = value of the arg or None if just a flag\n        for i, arg_candidate in enumerate(argv):\n            arg = None\n            value = None\n\n            # only look at --keys\n            if \'--\' not in arg_candidate:\n                continue\n\n            # skip items not on the white list\n            if arg_candidate[2:] not in HyperOptArgumentParser.CMD_MAP:\n                continue\n\n            arg = arg_candidate[2:]\n            # pull out the value of the argument if given\n            if i + 1 <= len(argv) - 1:\n                if \'--\' not in argv[i + 1]:\n                    value = argv[i + 1]\n\n                if arg is not None:\n                    parsed[arg] = value\n            else:\n                if arg is not None:\n                    parsed[arg] = value\n\n        # add the whitelist cmds to the args\n        all_values = set()\n        for k, v in args.__dict__.items():\n            all_values.add(k)\n            all_values.add(v)\n\n        for arg, v in parsed.items():\n            v_parsed = self.__parse_primitive_arg_val(v)\n            all_values.add(v)\n            all_values.add(arg)\n            args.__setattr__(arg, v_parsed)\n\n        # make list with only the unknown args\n        unk_args = []\n        for arg in argv:\n            arg_candidate = re.sub(\'--\', \'\', arg)\n            is_bool = arg_candidate == \'True\' or arg_candidate == \'False\'\n            if is_bool: continue\n\n            if arg_candidate not in all_values:\n                unk_args.append(arg)\n\n        # when no bad args are left, return none to be consistent with super api\n        if len(unk_args) == 0:\n            unk_args = None\n\n        # add hpc_exp_number if not passed in so we can never get None\n        if HyperOptArgumentParser.SLURM_EXP_CMD not in args:\n            args.__setattr__(HyperOptArgumentParser.SLURM_EXP_CMD, None)\n\n        return args, unk_args\n\n    def __parse_primitive_arg_val(self, val):\n        if val is None:\n            return True\n        try:\n            return int(val)\n        except ValueError:\n            try:\n                return float(val)\n            except ValueError:\n                return val\n\n    def parse_args(self, args=None, namespace=None):\n        # call superclass arg first\n        results = self.__parse_args(args, namespace)\n\n        # extract vals\n        old_args = vars(results)\n\n        # override with json args if given\n        if self.json_config_arg_name and old_args[self.json_config_arg_name]:\n            for arg, v in self.__read_json_config(old_args[self.json_config_arg_name]).items():\n                old_args[arg] = v\n\n        # track args\n        self.parsed_args = deepcopy(old_args)\n        # attach optimization fx\n        old_args[\'trials\'] = self.opt_trials\n        old_args[\'optimize_parallel\'] = self.optimize_parallel\n        old_args[\'optimize_parallel_gpu\'] = self.optimize_parallel_gpu\n        old_args[\'optimize_parallel_cpu\'] = self.optimize_parallel_cpu\n        old_args[\'generate_trials\'] = self.generate_trials\n        old_args[\'optimize_trials_parallel_gpu\'] = self.optimize_trials_parallel_gpu\n\n        return TTNamespace(**old_args)\n\n    def __read_json_config(self, file_path):\n        with open(file_path) as json_data:\n            json_args = json.load(json_data)\n            return json_args\n\n    def opt_trials(self, num):\n        self.trials = strategies.generate_trials(\n            strategy=self.strategy,\n            flat_params=self.__flatten_params(self.opt_args),\n            nb_trials=num,\n        )\n\n        for trial in self.trials:\n            ns = self.__namespace_from_trial(trial)\n            yield ns\n\n    def generate_trials(self, nb_trials):\n        trials = strategies.generate_trials(\n            strategy=self.strategy,\n            flat_params=self.__flatten_params(self.opt_args),\n            nb_trials=nb_trials,\n        )\n\n        trials = [self.__namespace_from_trial(x) for x in trials]\n        return trials\n\n    def optimize_parallel_gpu(\n            self,\n            train_function,\n            gpu_ids,\n            max_nb_trials=None,\n    ):\n        """"""\n        Runs optimization across gpus with cuda drivers\n        :param train_function:\n        :param max_nb_trials:\n        :param gpu_ids: List of strings like: [\'0\', \'1, 3\']\n        :return:\n        """"""\n        self.trials = strategies.generate_trials(\n            strategy=self.strategy,\n            flat_params=self.__flatten_params(self.opt_args),\n            nb_trials=max_nb_trials,\n        )\n\n        self.trials = [(self.__namespace_from_trial(x), train_function) for x in self.trials]\n\n        # build q of gpu ids so we can use them in each process\n        # this is thread safe so each process can pull out a gpu id, run its task and put it back when done\n        if self.pool is None:\n            gpu_q = Queue()\n            for gpu_id in gpu_ids:\n                gpu_q.put(gpu_id)\n\n            # called by the Pool when a process starts\n            def init(local_gpu_q):\n                global g_gpu_id_q\n                g_gpu_id_q = local_gpu_q\n\n            # init a pool with the nb of worker threads we want\n            nb_workers = len(gpu_ids)\n            self.pool = Pool(processes=nb_workers, initializer=init, initargs=(gpu_q,))\n\n        # apply parallelization\n        results = self.pool.map(optimize_parallel_gpu_private, self.trials)\n        return results\n\n    def optimize_trials_parallel_gpu(\n            self,\n            train_function,\n            nb_trials,\n            trials,\n            gpu_ids,\n            nb_workers=4,\n    ):\n        """"""\n        Runs optimization across gpus with cuda drivers\n        :param train_function:\n        :param nb_trials:\n        :param gpu_ids: List of strings like: [\'0\', \'1, 3\']\n        :param nb_workers:\n        :return:\n        """"""\n        self.trials = trials\n        self.trials = [(x, train_function) for x in self.trials]\n\n        # build q of gpu ids so we can use them in each process\n        # this is thread safe so each process can pull out a gpu id, run its task and put it back when done\n        if self.pool is None:\n            gpu_q = Queue()\n            for gpu_id in gpu_ids:\n                gpu_q.put(gpu_id)\n\n            # called by the Pool when a process starts\n            def init(local_gpu_q):\n                global g_gpu_id_q\n                g_gpu_id_q = local_gpu_q\n\n            # init a pool with the nb of worker threads we want\n            self.pool = Pool(processes=nb_workers, initializer=init, initargs=(gpu_q,))\n\n        # apply parallelization\n        results = self.pool.map(optimize_parallel_gpu_private, self.trials)\n        return results\n\n    def optimize_parallel_cpu(\n            self,\n            train_function,\n            nb_trials,\n            nb_workers=4,\n    ):\n        """"""\n        Runs optimization across n cpus\n        :param train_function:\n        :param nb_trials:\n        :param nb_workers:\n        :return:\n        """"""\n        self.trials = strategies.generate_trials(\n            strategy=self.strategy,\n            flat_params=self.__flatten_params(self.opt_args),\n            nb_trials=nb_trials\n        )\n\n        self.trials = [(self.__namespace_from_trial(x), train_function) for x in self.trials]\n\n        # init a pool with the nb of worker threads we want\n        if self.pool is None:\n            self.pool = Pool(processes=nb_workers)\n\n        # apply parallelization\n        results = self.pool.map(optimize_parallel_cpu_private, self.trials)\n        return results\n\n    def optimize_parallel(\n            self,\n            train_function,\n            nb_trials,\n            nb_parallel=4,\n    ):\n        self.trials = strategies.generate_trials(\n            strategy=self.strategy,\n            flat_params=self.__flatten_params(self.opt_args),\n            nb_trials=nb_trials\n        )\n\n        # nb of runs through all parallel systems\n        fork_batches = [\n            self.trials[i:i + nb_parallel] for i in range(0, len(self.trials), nb_parallel)\n        ]\n\n        for fork_batch in fork_batches:\n            children = []\n\n            # run n parallel forks\n            for parallel_nb, trial in enumerate(fork_batch):\n\n                # q up the trial and convert to a namespace\n                ns = self.__namespace_from_trial(trial)\n\n                # split new fork\n                pid = os.fork()\n\n                # when the process is a parent\n                if pid:\n                    children.append(pid)\n\n                # when process is a child\n                else:\n                    # slight delay to make sure we don\'t overwrite over test tube log versions\n                    sleep(parallel_nb * 0.5)\n                    train_function(ns, parallel_nb)\n                    os._exit(0)\n\n            for i, child in enumerate(children):\n                os.waitpid(child, 0)\n\n    def __namespace_from_trial(self, trial):\n        trial_dict = {d[\'name\']: d[\'val\'] for d in trial}\n        for k, v in self.parsed_args.items():\n            if k not in trial_dict:\n                trial_dict[k] = v\n\n        return TTNamespace(**trial_dict)\n\n    def __flatten_params(self, params):\n        """"""\n        Turns a list of parameters with values into a flat tuple list of lists\n        so we can permute\n        :param params:\n        :return:\n        """"""\n        flat_params = []\n        for i, (opt_name, opt_arg) in enumerate(params.items()):\n            if opt_arg.tunable:\n                clean_name = opt_name.strip(\'-\')\n                clean_name = re.sub(\'-\', \'_\', clean_name)\n                param_groups = []\n                for val in opt_arg.opt_values:\n                    param_groups.append({\'idx\': i, \'val\': val, \'name\': clean_name})\n                flat_params.append(param_groups)\n        return flat_params\n\n\nclass TTNamespace(argparse.Namespace):\n\n    def __str__(self):\n        result = \'-\' * 100 + \'\\nHyperparameters:\\n\'\n        for k, v in self.__dict__.items():\n            result += \'{0:20}: {1}\\n\'.format(k, v)\n        return result\n\n    def __getstate__(self):\n        # capture what is normally pickled\n        state = self.__dict__.copy()\n\n        # remove all functions from the namespace\n        clean_state = {}\n        for k, v in state.items():\n            if not hasattr(v, \'__call__\'):\n                clean_state[k] = v\n\n        # what we return here will be stored in the pickle\n        return clean_state\n\n    def __setstate__(self, newstate):\n        # re-instate our __dict__ state from the pickled state\n        self.__dict__.update(newstate)\n\n\nclass OptArg(object):\n    def __init__(\n            self,\n            obj_id,\n            opt_values,\n            arg_type=None,\n            nb_samples=None,\n            tunable=False,\n            log_base=None,\n    ):\n        self.opt_values = opt_values\n        self.obj_id = obj_id\n        self.tunable = tunable\n\n        # convert range to list of values\n        if nb_samples:\n            low, high = opt_values\n\n            if log_base is None:\n                # random search on uniform scale\n                if arg_type is int:\n                    self.opt_values = [int(_) for _ in np.random.choice(np.arange(low, high), nb_samples, replace=False)]\n                elif arg_type is float:\n                    self.opt_values = np.random.uniform(low, high, nb_samples)\n            else:\n                # random search on log scale with specified base\n                assert high >= low > 0, ""`opt_values` must be positive to do log-scale search.""\n\n                log_low, log_high = math.log(low, log_base), math.log(high, log_base)\n\n                self.opt_values = log_base ** np.random.uniform(log_low, log_high, nb_samples)\n\n'"
test_tube/hpc.py,0,"b'import datetime\nimport os\nimport signal\nimport sys\nimport time\nimport traceback\nfrom subprocess import call\n\nfrom .argparse_hopt import HyperOptArgumentParser\n\n\ndef exit():\n    time.sleep(1)\n    os._exit(1)\n\n\nclass AbstractCluster(object):\n\n    RUN_CMD = \'sbatch\'\n    def __init__(\n            self,\n            hyperparam_optimizer=None,\n            log_path=None,\n            python_cmd=\'python3\',\n            enable_log_err=True,\n            enable_log_out=True,\n    ):\n        self.hyperparam_optimizer = hyperparam_optimizer\n        self.log_path = log_path\n\n        self.enable_log_err = enable_log_err\n        self.enable_log_out = enable_log_out\n        self.slurm_files_log_path = None\n        self.err_log_path = None\n        self.out_log_path = None\n        self.modules = []\n        self.script_name = os.path.realpath(sys.argv[0])\n        self.job_time = \'15:00\'\n        self.minutes_to_checkpoint_before_walltime = 5\n        self.per_experiment_nb_gpus = 1\n        self.per_experiment_nb_cpus = 1\n        self.per_experiment_nb_nodes = 1\n        self.memory_mb_per_node = 2000\n        self.email = None\n        self.notify_on_end = False\n        self.notify_on_fail = False\n        self.job_name = None\n        self.python_cmd = python_cmd\n        self.gpu_type = None\n        self.on_gpu = False\n        self.call_load_checkpoint = False\n        self.commands = []\n        self.slurm_commands = []\n        self.hpc_exp_number = 0\n\n        # these are set via getters and setters so we can use a BaseManager which can be shared across processes\n        self.checkpoint_save_function = None\n        self.checkpoint_load_function = None\n\n        # detect when this was called because a slurm object started a hopt.\n        # if true, remove the flag so tt logs don\'t show it\n        if hyperparam_optimizer is not None:\n\n            self.is_from_slurm_object = HyperOptArgumentParser.TRIGGER_CMD in vars(self.hyperparam_optimizer) and vars(self.hyperparam_optimizer)[HyperOptArgumentParser.TRIGGER_CMD] == True\n            if self.is_from_slurm_object:\n                self.hyperparam_optimizer.__delattr__(HyperOptArgumentParser.TRIGGER_CMD)\n\n            self.call_load_checkpoint = HyperOptArgumentParser.SLURM_LOAD_CMD in vars(self.hyperparam_optimizer)\n            if self.call_load_checkpoint:\n                self.hyperparam_optimizer.__delattr__(HyperOptArgumentParser.SLURM_LOAD_CMD)\n\n            self.hpc_exp_number = self.hyperparam_optimizer.hpc_exp_number\n\n    def set_checkpoint_save_function(self, fx, kwargs):\n        self.checkpoint_save_function = [fx, kwargs]\n\n    def get_checkpoint_save_function(self):\n        return self.checkpoint_save_function\n\n    def set_checkpoint_load_function(self, fx, kwargs):\n        # if we were passed in the load flag, then we call the load function as soon as it\'s added\n        if self.call_load_checkpoint:\n            fx(**kwargs)\n\n        self.checkpoint_load_function = [fx, kwargs]\n\n    def get_checkpoint_load_function(self):\n        return self.checkpoint_load_function\n\n    def add_slurm_cmd(self, cmd, value, comment):\n        self.slurm_commands.append((cmd, value, comment))\n\n    def add_command(self, cmd):\n        self.commands.append(cmd)\n\n    def load_modules(self, modules):\n        self.modules = modules\n\n    def notify_job_status(self, email, on_done, on_fail):\n        self.email = email\n        self.notify_on_end = on_done\n        self.notify_on_fail = on_fail\n\n    def optimize_parallel_cluster(self, train_function, nb_trials, job_name):\n        raise NotImplementedError\n\n    def optimize_parallel_slurm(self, job_name, output_file, error_file, job_time, nb_gpus, nb_nodes, memory, notifications_email, gpu_types):\n        pass\n\n\nclass SlurmCluster(AbstractCluster):\n    def __init__(self, *args, **kwargs):\n        super(SlurmCluster, self).__init__(*args, **kwargs)\n\n    def optimize_parallel_cluster_gpu(\n            self,\n            train_function,\n            nb_trials,\n            job_name,\n            enable_auto_resubmit=False,\n            job_display_name=None\n    ):\n        if job_display_name is None:\n            job_display_name = job_name\n\n        self.__optimize_parallel_cluster_internal(train_function, nb_trials, job_name, job_display_name,\n                                                  enable_auto_resubmit, on_gpu=True)\n\n    def optimize_parallel_cluster_cpu(\n            self,\n            train_function,\n            nb_trials,\n            job_name,\n            enable_auto_resubmit=False,\n            job_display_name=None\n    ):\n        if job_display_name is None:\n            job_display_name = job_name\n\n        self.__optimize_parallel_cluster_internal(train_function, nb_trials, job_name, job_display_name,\n                                                  enable_auto_resubmit, on_gpu=False)\n\n    def __optimize_parallel_cluster_internal(\n            self,\n            train_function,\n            nb_trials,\n            job_name,\n            job_display_name,\n            enable_auto_resubmit,\n            on_gpu\n    ):\n        """"""\n        Runs optimization on the attached cluster\n        :param train_function:\n        :param nb_trials:\n        :param job_name:\n        :return:\n        """"""\n        self.job_name = job_name\n        self.job_display_name = job_display_name\n        self.on_gpu = on_gpu\n        self.enable_auto_resubmit = enable_auto_resubmit\n\n        # layout logging structure\n        self.__layout_logging_dir()\n\n        if self.is_from_slurm_object:\n            # Script is called by slurm: it\'s an actual experiment.\n            self.__run_experiment(train_function)\n        else:\n            # Launcher script. Generate trials and launch jobs.\n\n            # generate hopt trials\n            trials = self.hyperparam_optimizer.generate_trials(nb_trials)\n\n            # get the max test tube exp version so far if it\'s there\n            scripts_path = os.path.join(self.log_path, \'slurm_out_logs\')\n            next_trial_version = self.__get_max_trial_version(scripts_path)\n\n            # for each trial, generate a slurm command\n            for i, trial_params in enumerate(trials):\n                exp_i = i + next_trial_version\n                self.schedule_experiment(trial_params, exp_i)\n\n    def schedule_experiment(self, trial_params, exp_i):\n        timestamp = datetime.datetime.now().strftime(""%Y-%m-%d__%H-%M-%S"")\n        timestamp = \'trial_{}_{}\'.format(exp_i, timestamp)\n\n        # generate command\n        slurm_cmd_script_path = os.path.join(self.slurm_files_log_path, \'{}_slurm_cmd.sh\'.format(timestamp))\n        slurm_cmd = self.__build_slurm_command(trial_params, slurm_cmd_script_path, timestamp, exp_i, self.on_gpu)\n        self.__save_slurm_cmd(slurm_cmd, slurm_cmd_script_path)\n\n        # run script to launch job\n        print(\'\\nlaunching exp...\')\n        result = call(\'{} {}\'.format(AbstractCluster.RUN_CMD, slurm_cmd_script_path), shell=True)\n        if result == 0:\n            print(\'launched exp \', slurm_cmd_script_path)\n        else:\n            print(\'launch failed...\')\n\n    def slurm_time_to_seconds(self, job_time):\n        seconds = 0\n        time_component = job_time\n        if \'-\' in job_time:\n            days, time_component = job_time.split(\'-\')\n            seconds += int(days) * 24 * 60 * 60\n\n        time_components = time_component.split(\':\')\n        if len(time_components) == 3:\n            hours, minutes, secs = time_components\n            time_seconds = int(secs) + (int(minutes) * 60) + (int(hours) * 60 * 60)\n            seconds += time_seconds\n\n        elif len(time_components) == 2:\n            minutes, secs = time_components\n            time_seconds = int(secs) + (int(minutes) * 60)\n            seconds += time_seconds\n\n        elif len(time_components) == 1:\n            secs = time_components[0]\n            seconds += int(secs)\n\n        return seconds\n\n    def call_save(self):\n        print(\'calling save\')\n\n        # if save function was passed, call it\n        if self.get_checkpoint_save_function() is not None:\n            save_fx, kwargs = self.get_checkpoint_save_function()\n            save_fx(**kwargs)\n\n            # if we\'re here, the job didn\'t finish and we were given a save function\n            # if we were given a load function, then schedule the program again and pass in the load function\n            if self.get_checkpoint_load_function() is not None:\n                job_id = os.environ[\'SLURM_JOB_ID\']\n                cmd = \'scontrol requeue {}\'.format(job_id)\n\n                print(\'\\nrequeing job {}...\'.format(job_id))\n                result = call(cmd, shell=True)\n                if result == 0:\n                    print(\'requeued exp \', job_id)\n                else:\n                    print(\'requeue failed...\')\n\n        # stop program\n        os._exit(0)\n\n    def sig_handler(self, signum, frame):\n        print(""caught signal"", signum)\n        self.call_save()\n        # sys.exit(-1)\n\n    # ------------------------\n    # HANDLE SLURM SIGNALS\n    # ------------------------\n    def term_handler(self, signum, frame):\n        print(""bypassing sigterm"")\n\n    def __run_experiment(self, train_function):\n        if self.enable_auto_resubmit:\n            print(\'setting signal\')\n            signal.signal(signal.SIGUSR1, self.sig_handler)\n            signal.signal(signal.SIGTERM, self.term_handler)\n\n        try:\n            # run training\n            train_function(self.hyperparam_optimizer, self)\n\n        except Exception as e:\n            print(\'Caught exception in worker thread\', e)\n\n            # This prints the type, value, and stack trace of the\n            # current exception being handled.\n            traceback.print_exc()\n            raise SystemExit\n\n    def __save_slurm_cmd(self, slurm_cmd, slurm_cmd_script_path):\n        with open(slurm_cmd_script_path, mode=\'w\') as file:\n            file.write(slurm_cmd)\n\n    def __get_max_trial_version(self, path):\n        files = os.listdir(path)\n        version_files = [f for f in files if \'trial_\' in f]\n        if len(version_files) > 0:\n            # regex out everything except file version for ve\n            versions = [int(f_name.split(\'_\')[1]) for f_name in version_files]\n            max_version = max(versions)\n            return max_version + 1\n        else:\n            return 0\n\n    def __layout_logging_dir(self):\n        """"""\n        Generates dir structure for logging errors and outputs\n        :return:\n        """"""\n\n        # format the logging folder path\n        slurm_out_path = os.path.join(self.log_path, self.job_name)\n\n        self.log_path = slurm_out_path\n\n        # if we have a test tube name, make the folder and set as the logging destination\n        if not os.path.exists(slurm_out_path):\n            os.makedirs(slurm_out_path)\n\n        # when err logging is enabled, build add the err logging folder\n        if self.enable_log_err:\n            err_path = os.path.join(slurm_out_path, \'slurm_err_logs\')\n            if not os.path.exists(err_path):\n                os.makedirs(err_path)\n            self.err_log_path = err_path\n\n        # when out logging is enabled, build add the out logging folder\n        if self.enable_log_out:\n            out_path = os.path.join(slurm_out_path, \'slurm_out_logs\')\n            if not os.path.exists(out_path):\n                os.makedirs(out_path)\n            self.out_log_path = out_path\n\n        # place where slurm files log to\n        self.slurm_files_log_path = os.path.join(slurm_out_path, \'slurm_scripts\')\n        if not os.path.exists(self.slurm_files_log_path):\n            os.makedirs(self.slurm_files_log_path)\n\n    def __get_hopt_params(self, trial):\n        """"""\n        Turns hopt trial into script params\n        :param trial:\n        :return:\n        """"""\n\n        params = []\n        for k in trial.__dict__:\n            v = trial.__dict__[k]\n\n            # don\'t add None params\n            if v is None or v is False:\n                continue\n\n            # put everything in quotes except bools\n            if self.__should_escape(v):\n                cmd = \'--{} \\""{}\\""\'.format(k, v)\n            else:\n                cmd = \'--{} {}\'.format(k, v)\n            params.append(cmd)\n\n        # this arg lets the hyperparameter optimizer do its thing\n        params.append(\'--{}\'.format(HyperOptArgumentParser.TRIGGER_CMD))\n\n        full_cmd = \' \'.join(params)\n        return full_cmd\n\n    def __should_escape(self, v):\n        v = str(v)\n        return \'[\' in v or \';\' in v or \' \' in v\n\n    def __build_slurm_command(self, trial, slurm_cmd_script_path, timestamp, exp_i, on_gpu):\n        sub_commands = []\n\n        command =[\n            \'#!/bin/bash\',\n            \'#\',\n            \'# Auto-generated by test-tube (https://github.com/williamFalcon/test-tube)\',\n            \'#################\\n\'\n        ]\n        sub_commands.extend(command)\n\n        # add job name\n        job_with_version = \'{}v{}\'.format(self.job_display_name, exp_i)\n        command = [\n            \'# set a job name\',\n            \'#SBATCH --job-name={}\'.format(job_with_version),\n            \'#################\\n\',\n        ]\n        sub_commands.extend(command)\n\n        # add out output\n        if self.enable_log_out:\n            out_path = os.path.join(self.out_log_path, \'{}_slurm_output_%j.out\'.format(timestamp))\n            command = [\n                \'# a file for job output, you can check job progress\',\n                \'#SBATCH --output={}\'.format(out_path),\n                \'#################\\n\',\n            ]\n            sub_commands.extend(command)\n\n        # add err output\n        if self.enable_log_err:\n            err_path = os.path.join(self.err_log_path, \'{}_slurm_output_%j.err\'.format(timestamp))\n            command = [\n                \'# a file for errors\',\n                \'#SBATCH --error={}\'.format(err_path),\n                \'#################\\n\',\n            ]\n            sub_commands.extend(command)\n\n        # add job time\n        command = [\n            \'# time needed for job\',\n            \'#SBATCH --time={}\'.format(self.job_time),\n            \'#################\\n\'\n        ]\n        sub_commands.extend(command)\n\n        # add nb of gpus\n        if self.per_experiment_nb_gpus > 0 and on_gpu:\n            command = [\n                \'# gpus per node\',\n                \'#SBATCH --gres=gpu:{}\'.format(self.per_experiment_nb_gpus),\n                \'#################\\n\'\n            ]\n            if self.gpu_type is not None:\n                command = [\n                    \'# gpus per node\',\n                    \'#SBATCH --gres=gpu:{}:{}\'.format(self.gpu_type, self.per_experiment_nb_gpus),\n                    \'#################\\n\'\n                ]\n            sub_commands.extend(command)\n\n        # add nb of cpus if not looking at a gpu job\n        if self.per_experiment_nb_cpus > 0:\n            command = [\n                \'# cpus per job\',\n                \'#SBATCH --cpus-per-task={}\'.format(self.per_experiment_nb_cpus),\n                \'#################\\n\'\n            ]\n            sub_commands.extend(command)\n\n        # pick nb nodes\n        command = [\n            \'# number of requested nodes\',\n            \'#SBATCH --nodes={}\'.format(self.per_experiment_nb_nodes),\n            \'#################\\n\'\n        ]\n        sub_commands.extend(command)\n\n        # pick memory per node\n        command = [\n            \'# memory per node\',\n            \'#SBATCH --mem={}\'.format(self.memory_mb_per_node),\n            \'#################\\n\'\n        ]\n        sub_commands.extend(command)\n\n        # add signal command to catch job termination\n        command = [\n            \'# slurm will send a signal this far out before it kills the job\',\n            f\'#SBATCH --signal=USR1@{self.minutes_to_checkpoint_before_walltime * 60}\',\n            \'#################\\n\'\n        ]\n\n        sub_commands.extend(command)\n\n        # Subscribe to email if requested\n        mail_type = []\n        if self.notify_on_end:\n            mail_type.append(\'END\')\n        if self.notify_on_fail:\n            mail_type.append(\'FAIL\')\n        if len(mail_type) > 0:\n            mail_type_query = [\n                \'# Have SLURM send you an email when the job ends or fails\',\n                \'#SBATCH --mail-type={}\'.format(\',\'.join(mail_type))\n            ]\n            sub_commands.extend(mail_type_query)\n\n            email_query = [\n                \'#SBATCH --mail-user={}\'.format(self.email),\n            ]\n            sub_commands.extend(email_query)\n\n        # add custom sbatch commands\n        sub_commands.append(\'\\n\')\n        for (cmd, value, comment) in self.slurm_commands:\n            comment = \'# {}\'.format(comment)\n            cmd = \'#SBATCH --{}={}\'.format(cmd, value)\n            spaces = \'#################\\n\'\n            sub_commands.extend([comment, cmd, spaces])\n\n        # load modules\n        sub_commands.append(\'\\n\')\n        for module in self.modules:\n            cmd = \'module load {}\'.format(module)\n            sub_commands.append(cmd)\n\n        # remove spaces before the hash\n        sub_commands = [x.lstrip() for x in sub_commands]\n\n        # add additional commands\n        for cmd in self.commands:\n            sub_commands.append(cmd)\n            sub_commands.append(\'\\n\')\n\n        # add run command\n        trial_args = self.__get_hopt_params(trial)\n        trial_args = \'{} --{} {} --{} {}\'.format(trial_args,\n                                                 HyperOptArgumentParser.SLURM_CMD_PATH,\n                                                 slurm_cmd_script_path,\n                                                 HyperOptArgumentParser.SLURM_EXP_CMD,\n                                                 exp_i)\n\n        cmd = \'srun {} {} {}\'.format(self.python_cmd, self.script_name, trial_args)\n        sub_commands.append(cmd)\n\n        # build full command with empty lines in between\n        full_command = \'\\n\'.join(sub_commands)\n        return full_command\n'"
test_tube/hyperopt.py,0,"b'import itertools\nimport json\nimport random\n\n\nclass HyperParamOptimizer(object):\n\n    def __init__(self, method=\'grid_search\', enabled=True, experiment=None):\n        """"""\n        :param method: \'grid_search\', \'random_search\'\n        :param enabled:\n        """"""\n        self.method = method\n        self.enabled = enabled\n        self.experiment = experiment\n        self.seen_params = {}\n        self.current_iteration = 0\n\n        # the params to use at each trial\n        self.trials = None\n\n        # total iterations we\'re doing\n        self.nb_iterations = None\n\n        # details about each param\n        self.params = []\n\n    # -----------------------------\n    # PARAMETER CHOICES\n    # -----------------------------\n    def tune_uniform(self, low, high, samples, default, name):\n        # how this fx samples for the data\n        def gen_samples():\n            vals = [random.uniform(low, high) for i in range(samples)]\n            return vals\n\n        return self.__resolve_param(gen_samples, default, name)\n\n    def tune_odds(self, low, high, default, name):\n        start = low if low %2 != 0 else low + 1\n        def gen_samples():\n            return range(start, high+1, 2)\n\n        return self.__resolve_param(gen_samples, default, name)\n\n    def tune_evens(self, low, high, default, name):\n        start = low if low %2 == 0 else low + 1\n        def gen_samples():\n            return range(start, high+1, 2)\n\n        return self.__resolve_param(gen_samples, default, name)\n\n    def tune_choice(self, options, default, name):\n        def gen_samples():\n            return options\n\n        return self.__resolve_param(gen_samples, default, name)\n\n    def __resolve_param(self, gen_fx, default, name):\n        # case when no action was requested\n        if not self.enabled:\n            return default\n\n        # create the param when it\'s new\n        # return the first value in this case\n        if name not in self.seen_params:\n            vals = gen_fx()\n            param = {\'vals\': vals, \'name\': name}\n            self.seen_params[name] = {\'idx\': len(self.params)}\n            self.params.append(param)\n            return vals[0]\n\n        # not the first iteration so return the ith element\n        # in the possible values\n        iteration_params = self.trials[self.current_iteration]\n        param_i = self.seen_params[name][\'idx\']\n        param = iteration_params[param_i]\n        return param[\'val\']\n\n    # -----------------------------\n    # OPTIMIZATION\n    # -----------------------------\n    def optimize(self, fx, nb_iterations=None):\n        """"""\n        Primary entry point into the optimization\n        :param fx:\n        :param nb_iterations:\n        :return:\n        """"""\n        self.nb_iterations = nb_iterations\n\n        # run first iteration\n        result = fx(self)\n\n        # log if requested\n        if self.experiment is not None:\n            result[\'hypo_iter_nb\'] = self.current_iteration\n            self.experiment.log(result)\n\n        self.current_iteration += 1\n\n        # generate the rest of the training seq\n        # we couldn\'t do this before because we don\'t know\n        # how many params the user needed\n        self.__generate_trials()\n\n        # run trials for the rest of the iterations\n        # we either know the iterations or they\'re\n        # calculated from the strategy used\n        for i in range(1, len(self.trials)):\n            result = fx(self)\n            result[\'hypo_iter_nb\'] = self.current_iteration\n\n            # log if requested\n            if self.experiment is not None:\n                self.experiment.log(result)\n\n            self.current_iteration += 1\n\n    # -----------------------------\n    # INTERFACE WITH LOGGER\n    # -----------------------------\n    def get_current_trial_meta(self):\n        meta_results = []\n\n        # when we have trials, means we\'ve already done 1 run\n        # we can just get the params that are about to be run\n        # otherwise we need to infer params from the current param list\n        # this assumes the user feeds the opt into the experiment after\n        # they\'re done setting up the params\n        is_first_trial = self.trials is not None and len(self.trials) > 0\n        if is_first_trial:\n            trial_params = self.trials[self.current_iteration]\n            for trial_param in trial_params:\n                root_param = self.params[trial_param[\'idx\']]\n                meta_results.append({\'hypo_\' + root_param[\'name\']: trial_param[\'val\']})\n\n        # if we haven\'t done a pass through the data yet,\n        # we need to infer from the params in the list\n        else:\n            for param in self.params:\n                meta_results.append({\'hypo_\' + param[\'name\']: param[\'vals\'][0]})\n\n        # add shared meta\n        meta_results.append({\'hypo_iter_nb\': self.current_iteration})\n        return meta_results\n\n    # -----------------------------\n    # TRIALS HELPER\n    # -----------------------------\n    def __generate_trials(self):\n        """"""\n        Generates the parameter combinations for each requested trial\n        :return:\n        """"""\n        flat_params = self.__flatten_params(self.params)\n\n        # permute for grid search\n        if self.method == \'grid_search\':\n            self.trials = list(itertools.product(*flat_params))\n\n            if self.nb_iterations is not None:\n                self.trials = self.trials[0: self.nb_iterations]\n\n        if self.method == \'random_search\':\n            self.trials = self.__generate_random_search_trials(flat_params)\n\n    def __flatten_params(self, params):\n        """"""\n        Turns a list of parameters with values into a flat tuple list of lists\n        so we can permute\n        :param params:\n        :return:\n        """"""\n        flat_params = []\n        for i, param in enumerate(params):\n            param_groups = []\n            for val in param[\'vals\']:\n                param_groups.append({\'idx\': i, \'val\': val})\n            flat_params.append(param_groups)\n        return flat_params\n\n    def __generate_random_search_trials(self, params):\n        results = []\n\n        # ensures we have unique results\n        seen_trials = set()\n\n        # shuffle each param list\n        potential_trials = 1\n        for p in params:\n            random.shuffle(p)\n            potential_trials *= len(p)\n\n        # we can\'t sample more trials than are possible\n        max_iters = min(potential_trials, self.nb_iterations)\n\n        # then for the nb of trials requested, create a new param tuple\n        # by picking a random integer at each param level\n        while len(results) < max_iters:\n            trial = []\n            for param in params:\n                p = random.sample(param, 1)[0]\n                trial.append(p)\n\n            # verify this is a unique trial so we\n            # don\'t duplicate work\n            trial_str = json.dumps(trial)\n            if trial_str not in seen_trials:\n                seen_trials.add(trial_str)\n                results.append(trial)\n\n        return results\n'"
test_tube/log.py,0,"b'import contextlib\nimport json\nimport os\nimport shutil\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\nfrom imageio import imwrite\nfrom tensorboard.compat.proto.event_pb2 import Event\nfrom tensorboard.compat.proto.event_pb2 import SessionLog\nfrom torch.utils.tensorboard import SummaryWriter, FileWriter\n\n# constants\n_ROOT = os.path.abspath(os.path.dirname(__file__))\n\n# -----------------------------\n# Experiment object\n# -----------------------------\n\n\nclass DDPExperiment(object):\n    def __init__(\n        self,\n        exp\n    ):\n        """"""\n        Used as meta_data storage if the experiment needs to be pickled\n        :param name:\n        :param debug:\n        :param version:\n        :param save_dir:\n        :param autosave:\n        :param description:\n        :param create_git_tag:\n        :param args:\n        :param kwargs:\n        """"""\n\n        self.tag_markdown_saved = exp.tag_markdown_saved\n        self.no_save_dir = exp.no_save_dir\n        self.metrics = exp.metrics\n        self.tags = exp.tags\n        self.name = exp.name\n        self.debug = exp.debug\n        self.version = exp.version\n        self.autosave = exp.autosave\n        self.description = exp.description\n        self.create_git_tag = exp.create_git_tag\n        self.exp_hash = exp.exp_hash\n        self.created_at = exp.created_at\n        self.save_dir = exp.save_dir\n\n\n    def get_non_ddp_exp(self):\n        return Experiment(\n            name=self.name,\n            debug=self.debug,\n            version=self.version,\n            save_dir=self.save_dir,\n            autosave=self.autosave,\n            description=self.description,\n            create_git_tag=self.create_git_tag\n        )\n\nclass Experiment(SummaryWriter):\n\n    def __init__(\n        self,\n        save_dir=None,\n        name=\'default\',\n        debug=False,\n        version=None,\n        autosave=False,\n        description=None,\n        create_git_tag=False,\n        rank=0,\n        *args, **kwargs\n    ):\n        """"""\n        A new Experiment object defaults to \'default\' unless a specific name is provided\n        If a known name is already provided, then the file version is changed\n        :param name:\n        :param debug:\n        """"""\n\n        # change where the save dir is if requested\n\n        if save_dir is not None:\n            global _ROOT\n            _ROOT = save_dir\n\n        self.save_dir = save_dir\n        self.tag_markdown_saved = False\n        self.no_save_dir = save_dir is None\n        self.metrics = []\n        self.tags = {}\n        self.name = name\n        self.debug = debug\n        self.version = version\n        self.autosave = autosave\n        self.description = description\n        self.create_git_tag = create_git_tag\n        self.exp_hash = \'{}_v{}\'.format(self.name, version)\n        self.created_at = str(datetime.utcnow())\n        self.rank = rank\n        self.process = os.getpid()\n\n        # when debugging don\'t do anything else\n        if debug:\n            return\n\n        # update version hash if we need to increase version on our own\n        # we will increase the previous version, so do it now so the hash\n        # is accurate\n        if version is None:\n            old_version = self.__get_last_experiment_version()\n            self.exp_hash = \'{}_v{}\'.format(self.name, old_version + 1)\n            self.version = old_version + 1\n\n        # create a new log file\n        self.__init_cache_file_if_needed()\n\n        # when we have a version, load it\n        if self.version is not None:\n\n            # when no version and no file, create it\n            if not os.path.exists(self.__get_log_name()):\n                self.__create_exp_file(self.version)\n            else:\n                # otherwise load it\n                try:\n                    self.__load()\n                except Exception as e:\n                    self.debug = True\n        else:\n            # if no version given, increase the version to a new exp\n            # create the file if not exists\n            old_version = self.__get_last_experiment_version()\n            self.version = old_version\n            self.__create_exp_file(self.version + 1)\n\n        # create a git tag if requested\n        if self.create_git_tag:\n            desc = description if description is not None else \'no description\'\n            tag_msg = \'Test tube exp: {} - {}\'.format(self.name, desc)\n            cmd = \'git tag -a tt_{} -m ""{}""\'.format(self.exp_hash, tag_msg)\n            os.system(cmd)\n            print(\'Test tube created git tag:\', \'tt_{}\'.format(self.exp_hash))\n\n        # set the tensorboardx log path to the /tf folder in the exp folder\n        log_dir = self.get_tensorboardx_path(self.name, self.version)\n        # this is a fix for pytorch 1.1 since it does not have this attribute\n        for attr, val in [(\'purge_step\', None),\n                          (\'max_queue\', 10),\n                          (\'flush_secs\', 120),\n                          (\'filename_suffix\', \'\')]:\n            if not hasattr(self, attr):\n                setattr(self, attr, val)\n        super().__init__(log_dir=log_dir, *args, **kwargs)\n\n        # register on exit fx so we always close the writer\n        # atexit.register(self.on_exit)\n\n    def get_meta_copy(self):\n        """"""\n        Gets a meta-version only copy of this module\n        :return:\n        """"""\n        return DDPExperiment(self)\n\n    def on_exit(self):\n        pass\n\n\n    def __clean_dir(self):\n        files = os.listdir(self.save_dir)\n\n        if self.rank == 0:\n            return\n\n        for f in files:\n            if str(self.process) in f:\n                os.remove(os.path.join(self.save_dir, f))\n\n    def argparse(self, argparser):\n        parsed = vars(argparser)\n        to_add = {}\n\n        # don\'t store methods\n        for k, v in parsed.items():\n            if not callable(v):\n                to_add[k] = v\n\n        self.tag(to_add)\n\n    def add_meta_from_hyperopt(self, hypo):\n        """"""\n        Transfers meta data about all the params from the\n        hyperoptimizer to the log\n        :param hypo:\n        :return:\n        """"""\n        meta = hypo.get_current_trial_meta()\n        for tag in meta:\n            self.tag(tag)\n\n    # --------------------------------\n    # FILE IO UTILS\n    # --------------------------------\n    def __init_cache_file_if_needed(self):\n        """"""\n        Inits a file that we log historical experiments\n        :return:\n        """"""\n        try:\n            exp_cache_file = self.get_data_path(self.name, self.version)\n            if not os.path.isdir(exp_cache_file):\n                os.makedirs(exp_cache_file, exist_ok=True)\n        except Exception as e:\n            # file already exists (likely written by another exp. In this case disable the experiment\n            self.debug = True\n\n    def __create_exp_file(self, version):\n        """"""\n        Recreates the old file with this exp and version\n        :param version:\n        :return:\n        """"""\n\n        try:\n            exp_cache_file = self.get_data_path(self.name, self.version)\n            # if no exp, then make it\n            path = \'{}/meta.experiment\'.format(exp_cache_file)\n            open(path, \'w\').close()\n            self.version = version\n\n            # make the directory for the experiment media assets name\n            os.makedirs(self.get_media_path(self.name, self.version), exist_ok=True)\n\n            # make the directory for tensorboardx stuff\n            os.makedirs(self.get_tensorboardx_path(self.name, self.version), exist_ok=True)\n        except Exception as e:\n            # file already exists (likely written by another exp. In this case disable the experiment\n            self.debug = True\n\n\n    def __get_last_experiment_version(self):\n        try:\n            exp_cache_file = os.sep.join(self.get_data_path(self.name, self.version).split(os.sep)[:-1])\n            return find_last_experiment_version(exp_cache_file)\n        except Exception as e:\n            return -1\n\n    def __get_log_name(self):\n        exp_cache_file = self.get_data_path(self.name, self.version)\n        return \'{}/meta.experiment\'.format(exp_cache_file)\n\n    def tag(self, tag_dict):\n        """"""\n        Adds a tag to the experiment.\n        Tags are metadata for the exp.\n\n        >> e.tag({""model"": ""Convnet A""})\n\n        :param key:\n        :param val:\n        :return:\n        """"""\n        if self.debug or self.rank > 0: return\n\n        # parse tags\n        for k, v in tag_dict.items():\n            self.tags[k] = v\n\n        # save if needed\n        if self.autosave == True:\n            self.save()\n\n    def log(self, metrics_dict, global_step=None, walltime=None):\n        """"""\n        Adds a json dict of metrics.\n\n        >> e.log({""loss"": 23, ""coeff_a"": 0.2})\n\n        :param metrics_dict:\n        :tag optional tfx tag\n        :return:\n        """"""\n        if self.debug or self.rank > 0: return\n\n        # handle tfx metrics\n        if global_step is None:\n            global_step = len(self.metrics)\n\n        new_metrics_dict = metrics_dict.copy()\n        for k, v in metrics_dict.items():\n            if isinstance(v, dict):\n                self.add_scalars(main_tag=k, tag_scalar_dict=v, global_step=global_step, walltime=walltime)\n                tmp_metrics_dict = new_metrics_dict.pop(k)\n                new_metrics_dict.update(tmp_metrics_dict)\n            else:\n                self.add_scalar(tag=k, scalar_value=v, global_step=global_step, walltime=walltime)\n\n        metrics_dict = new_metrics_dict\n\n        # timestamp\n        if \'created_at\' not in metrics_dict:\n            metrics_dict[\'created_at\'] = str(datetime.utcnow())\n\n        self.__convert_numpy_types(metrics_dict)\n\n        self.metrics.append(metrics_dict)\n\n        if self.autosave:\n            self.save()\n\n    def __convert_numpy_types(self, metrics_dict):\n        for k, v in metrics_dict.items():\n            if v.__class__.__name__ == \'float32\':\n                metrics_dict[k] = float(v)\n\n            if v.__class__.__name__ == \'float64\':\n                metrics_dict[k] = float(v)\n\n    def save(self):\n        """"""\n        Saves current experiment progress\n        :return:\n        """"""\n        if self.debug or self.rank > 0: return\n\n        # save images and replace the image array with the\n        # file name\n        self.__save_images(self.metrics)\n        metrics_file_path = self.get_data_path(self.name, self.version) + \'/metrics.csv\'\n        meta_tags_path = self.get_data_path(self.name, self.version) + \'/meta_tags.csv\'\n\n        obj = {\n            \'name\': self.name,\n            \'version\': self.version,\n            \'tags_path\': meta_tags_path,\n            \'metrics_path\': metrics_file_path,\n            \'autosave\': self.autosave,\n            \'description\': self.description,\n            \'created_at\': self.created_at,\n            \'exp_hash\': self.exp_hash\n        }\n\n        # save the experiment meta file\n        with atomic_write(self.__get_log_name()) as tmp_path:\n            with open(tmp_path, \'w\') as file:\n                json.dump(obj, file, ensure_ascii=False)\n\n        # save the metatags file\n        df = pd.DataFrame({\'key\': list(self.tags.keys()), \'value\': list(self.tags.values())})\n        with atomic_write(meta_tags_path) as tmp_path:\n            df.to_csv(tmp_path, index=False)\n\n        # save the metrics data\n        df = pd.DataFrame(self.metrics)\n        with atomic_write(metrics_file_path) as tmp_path:\n            df.to_csv(tmp_path, index=False)\n\n        # write new vals to disk\n        self.flush()\n\n        # until hparam plugin is fixed, generate hparams as text\n        if not self.tag_markdown_saved and len(self.tags) > 0:\n            self.tag_markdown_saved = True\n            self.add_text(\'hparams\', self.__generate_tfx_meta_log())\n\n    def __generate_tfx_meta_log(self):\n        header = f\'\'\'###### {self.name}, version {self.version}\\n---\\n\'\'\'\n        desc = \'\'\n        if self.description is not None:\n            desc = f\'\'\'#####*{self.description}*\\n\'\'\'\n        params = f\'\'\'##### Hyperparameters\\n\'\'\'\n\n        row_header = \'\'\'parameter|value\\n-|-\\n\'\'\'\n        rows = [row_header]\n        for k, v in self.tags.items():\n            row = f\'\'\'{k}|{v}\\n\'\'\'\n            rows.append(row)\n\n        all_rows = [\n            header,\n            desc,\n            params\n        ]\n        all_rows.extend(rows)\n        mkdown_log = \'\'.join(all_rows)\n        return mkdown_log\n\n    def __save_images(self, metrics):\n        """"""\n        Save tags that have a png_ prefix (as images)\n        and replace the meta tag with the file name\n        :param metrics:\n        :return:\n        """"""\n        # iterate all metrics and find keys with a specific prefix\n        for i, metric in enumerate(metrics):\n            for k, v in metric.items():\n                # if the prefix is a png, save the image and replace the value with the path\n                img_extension = None\n                img_extension = \'png\' if \'png_\' in k else img_extension\n                img_extension = \'jpg\' if \'jpg\' in k else img_extension\n                img_extension = \'jpeg\' if \'jpeg\' in k else img_extension\n\n                if img_extension is not None:\n                    # determine the file name\n                    img_name = \'_\'.join(k.split(\'_\')[1:])\n                    save_path = self.get_media_path(self.name, self.version)\n                    save_path = \'{}/{}_{}.{}\'.format(save_path, img_name, i, img_extension)\n\n                    # save image to disk\n                    if type(metric[k]) is not str:\n                        imwrite(save_path, metric[k])\n\n                    # replace the image in the metric with the file path\n                    metric[k] = save_path\n\n    def __load(self):\n        # load .experiment file\n        with open(self.__get_log_name(), \'r\') as file:\n            data = json.load(file)\n            self.name = data[\'name\']\n            self.version = data[\'version\']\n            self.autosave = data[\'autosave\']\n            self.created_at = data[\'created_at\']\n            self.description = data[\'description\']\n            self.exp_hash = data[\'exp_hash\']\n\n        # load .tags file\n        meta_tags_path = self.get_data_path(self.name, self.version) + \'/meta_tags.csv\'\n        df = pd.read_csv(meta_tags_path)\n        self.tags_list = df.to_dict(orient=\'records\')\n        self.tags = {}\n        for d in self.tags_list:\n            k, v = d[\'key\'], d[\'value\']\n            self.tags[k] = v\n\n        # load metrics\n        metrics_file_path = self.get_data_path(self.name, self.version) + \'/metrics.csv\'\n        try:\n            df = pd.read_csv(metrics_file_path)\n            self.metrics = df.to_dict(orient=\'records\')\n\n            # remove nans\n            for metric in self.metrics:\n                to_delete = []\n                for k, v in metric.items():\n                    try:\n                        if np.isnan(v):\n                            to_delete.append(k)\n                    except Exception as e:\n                        pass\n\n                for k in to_delete:\n                    del metric[k]\n        except Exception as e:\n            # metrics was empty...\n            self.metrics = []\n\n    def get_data_path(self, exp_name, exp_version):\n        """"""\n        Returns the path to the local package cache\n        :param path:\n        :return:\n        """"""\n        if self.no_save_dir:\n            return os.path.join(_ROOT, \'test_tube_data\', exp_name, \'version_{}\'.format(exp_version))\n        else:\n            return os.path.join(_ROOT, exp_name, \'version_{}\'.format(exp_version))\n\n    def get_media_path(self, exp_name, exp_version):\n        """"""\n        Returns the path to the local package cache\n        :param path:\n        :return:\n        """"""\n        return os.path.join(self.get_data_path(exp_name, exp_version), \'media\')\n\n    def get_tensorboardx_path(self, exp_name, exp_version):\n        """"""\n        Returns the path to the local package cache\n        :param path:\n        :return:\n        """"""\n        return os.path.join(self.get_data_path(exp_name, exp_version), \'tf\')\n\n    def get_tensorboardx_scalars_path(self, exp_name, exp_version):\n        """"""\n        Returns the path to the local package cache\n        :param path:\n        :return:\n        """"""\n        tfx_path = self.get_tensorboardx_path(exp_name, exp_version)\n        return os.path.join(tfx_path, \'scalars.json\')\n\n\n    # ----------------------------\n    # OVERWRITES\n    # ----------------------------\n    def _get_file_writer(self):\n        """"""Returns the default FileWriter instance. Recreates it if closed.""""""\n        if self.rank > 0:\n            return TTDummyFileWriter()\n\n        if self.all_writers is None or self.file_writer is None:\n            if self.purge_step is not None:\n                most_recent_step = self.purge_step\n                self.file_writer = FileWriter(self.log_dir, self.max_queue,\n                                          self.flush_secs, self.filename_suffix)\n                self.file_writer.debug = self.debug\n                self.file_writer.rank = self.rank\n\n                self.file_writer.add_event(\n                    Event(step=most_recent_step, file_version=\'brain.Event:2\'))\n                self.file_writer.add_event(\n                    Event(step=most_recent_step, session_log=SessionLog(status=SessionLog.START)))\n            else:\n                self.file_writer = FileWriter(self.log_dir, self.max_queue,\n                                          self.flush_secs, self.filename_suffix)\n            self.all_writers = {self.file_writer.get_logdir(): self.file_writer}\n        return self.file_writer\n\n\n    def __str__(self):\n        return \'Exp: {}, v: {}\'.format(self.name, self.version)\n\n    def __hash__(self):\n        return \'Exp: {}, v: {}\'.format(self.name, self.version)\n\n    def flush(self):\n        if self.rank > 0:\n            return\n\n        if self.all_writers is None:\n            return  # ignore double close\n\n        for writer in self.all_writers.values():\n            writer.flush()\n\n\nclass TTDummyFileWriter(object):\n\n    def add_summary(self, summary, global_step=None, walltime=None):\n        """"""\n        Overwrite tf add summary so we can ignore when other non-zero processes call it\n        Avoids overwriting logs from multiple processes\n        :param summary:\n        :param global_step:\n        :param walltime:\n        :return:\n        """"""\n        return\n\n\n@contextlib.contextmanager\ndef atomic_write(dst_path):\n    """"""A context manager to simplify atomic writing.\n\n    Usage:\n    >>> with atomic_write(dst_path) as tmp_path:\n    >>>     # write to tmp_path\n    >>> # Here tmp_path renamed to dst_path, if no exception happened.\n    """"""\n    tmp_path = str(dst_path) + \'.tmp\'\n    try:\n        yield tmp_path\n    except:\n        if os.path.exists(tmp_path):\n            os.remove(tmp_path)\n        raise\n    else:\n        # If everything is fine, move tmp file to the destination.\n        shutil.move(tmp_path, str(dst_path))\n\n\ndef find_last_experiment_version(path):\n    last_version = -1\n    for f in os.listdir(path):\n        if \'version_\' in f:\n            file_parts = f.split(\'_\')\n            version = int(file_parts[-1])\n            last_version = max(last_version, version)\n    return last_version\n\n\nif __name__ == \'__main__\':\n    from time import sleep\n    e = Experiment(description=\'my description\')\n    e.tag({\'lr\': 0.02, \'layers\': 4})\n\n    for n_iter in range(20):\n        sleep(0.3)\n        e.log({\'loss/xsinx\': n_iter * np.sin(n_iter)})\n        if n_iter % 10 == 0:\n            print(\'saved\')\n            e.save()\n\n    e.close()\n    os._exit(1)\n\n'"
tests/argparse_hopt_test.py,0,"b""import pytest\n\n\ndef test_hello():\n    assert 4==4\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
tests/hpc_test.py,0,"b""import pytest\n\nfrom test_tube.argparse_hopt import HyperOptArgumentParser\nfrom test_tube.hpc import SlurmCluster\n\n\ndef test_slurm_time_to_seconds():\n    parser = HyperOptArgumentParser()\n    parsed = parser.parse_args()\n    cluster = SlurmCluster(log_path='/home/travis', hyperparam_optimizer=parsed)\n\n    assert cluster.slurm_time_to_seconds('15:00') == 900\n    assert cluster.slurm_time_to_seconds('1-12:20:12') == 130812\n    assert cluster.slurm_time_to_seconds('1:20:12') == 4812\n    assert cluster.slurm_time_to_seconds('00:20:12') == 1212\n    assert cluster.slurm_time_to_seconds('00:00:12') == 12\n    assert cluster.slurm_time_to_seconds('12') == 12\n\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
tests/log_test.py,0,"b""import pytest\n\n\ndef test_hello():\n    assert 4==4\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
tests/strategies_test.py,0,"b""import pytest\n\nfrom test_tube.hyper_opt_utils import strategies\n\nGRID_SEARCH = 'grid_search'\nRANDOM_SEARCH = 'random_search'\n\nFLAT_PARAMS = [\n    [\n        {'idx': 0, 'val': 0.0001, 'name': 'learning_rate'},\n        {'idx': 1, 'val': 0.001, 'name': 'learning_rate'},\n        {'idx': 2, 'val': 0.01, 'name': 'learning_rate'},\n        {'idx': 3, 'val': 0.1, 'name': 'learning_rate'}\n    ],\n    [\n        {'idx': 4, 'val': 0.99, 'name': 'decay'},\n        {'idx': 5, 'val': 0.999, 'name': 'decay'},\n    ]\n]\ndef test_unknown_strategy():\n    with pytest.raises(ValueError):\n        strategies.generate_trials(\n            'unknown_strategy', FLAT_PARAMS, nb_trials=None)\n\ndef test_grid_search_no_limit():\n    trials = strategies.generate_trials(\n        GRID_SEARCH, FLAT_PARAMS, nb_trials=None)\n    assert len(trials) == len(FLAT_PARAMS[0]) * len(FLAT_PARAMS[1])\n\ndef test_grid_search_limit():\n    trials = strategies.generate_trials(\n        GRID_SEARCH, FLAT_PARAMS, nb_trials=5)\n    assert len(trials) == 5\n\n\ndef test_random_search():\n    trials = strategies.generate_trials(\n        RANDOM_SEARCH, FLAT_PARAMS, nb_trials=5)\n    assert len(trials) == 5\n\ndef test_random_search_unbounded_error():\n    with pytest.raises(TypeError):\n        trials = strategies.generate_trials(\n            RANDOM_SEARCH, FLAT_PARAMS, nb_trials=None)\n\n"""
test_tube/hyper_opt_utils/__init__.py,0,b''
test_tube/hyper_opt_utils/strategies.py,0,"b'""""""Hyperparameter search strategies.""""""\nimport itertools\nimport json\nimport random\n\n\ndef generate_trials(strategy, flat_params, nb_trials=None):\n    r""""""Generates the parameter combinations to search.\n\n    Two search strategies are implemented:\n    1. `grid_search`: creates a search space that consists of the\n        product of all flat_params. If `nb_trials` is specified\n        the first `nb_trials` combinations are searched.\n    2. `random_search`: Creates random combinations of the\n        hyperparameters. Can be used for a more efficient search.\n        See (Bergstra and Bengio, 2012) for more details.\n\n    :param strategy: The hyperparameter search to strategy. Can be\n        one of: {`grid_search`, `random`}.\n    :param flat_params: The hyperparameter arguments to iterate over.\n    :param nb_trials: The number of hyperparameter combinations to try.\n    Generates the parameter combinations for each requested trial\n    :param strategy:\n    :param flat_params:\n    :param nb_trials: The number of trials to un.\n    :return:\n    """"""\n    if strategy == \'grid_search\':\n        trials = generate_grid_search_trials(flat_params, nb_trials)\n        return trials\n    elif strategy == \'random_search\':\n        trials = generate_random_search_trials(flat_params, nb_trials)\n        return trials\n    else:\n        raise ValueError(\n            (\'Unknown strategy ""{}"". Must be one of \'\n             \'{{grid_search, random_search}}\').format(strategy))\n\n\ndef generate_grid_search_trials(flat_params, nb_trials):\n    """"""\n    Standard grid search. Takes the product of `flat_params`\n    to generate the search space.\n\n    :param params: The hyperparameters options to search.\n    :param nb_trials: Returns the first `nb_trials` from the\n        combinations space. If this is None, all combinations\n        are returned.\n    :return: A dict containing the hyperparameters.\n    """"""\n    trials = list(itertools.product(*flat_params))\n    if nb_trials:\n        trials = trials[0:nb_trials]\n    return trials\n\n\ndef generate_random_search_trials(params, nb_trials):\n    """"""\n    Generates random combination of hyperparameters to try.\n    See (Bergstra and Bengio, 2012) for more details.\n\n    :param params: The hyperparameters options to search.\n    :param nb_trials: The number of trials to run.\n    :return: A dict containing the hyperparameters.\n    """"""\n    if nb_trials is None:\n        raise TypeError(\n            \'`random_search` strategy requires nb_trails to be an int.\')\n    results = []\n\n    # ensures we have unique results\n    seen_trials = set()\n\n    # shuffle each param list\n    potential_trials = 1\n    for param in params:\n        random.shuffle(param)\n        potential_trials *= len(param)\n\n    # we can\'t sample more trials than are possible\n    max_iters = min(potential_trials, nb_trials)\n\n    # then for the nb of trials requested, create a new param tuple\n    # by picking a random integer at each param level\n    while len(results) < max_iters:\n        trial = []\n        for param in params:\n            sampled_param = random.sample(param, 1)[0]\n            trial.append(sampled_param)\n\n        # verify this is a unique trial so we\n        # don\'t duplicate work\n        trial_str = json.dumps(trial)\n        if trial_str not in seen_trials:\n            seen_trials.add(trial_str)\n            results.append(trial)\n\n    return results\n'"
