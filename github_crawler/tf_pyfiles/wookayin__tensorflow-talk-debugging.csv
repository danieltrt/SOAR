file_path,api_count,code
codes/10-mnist.py,11,"b'import tensorflow as tf\nimport tensorflow.contrib.layers as layers\nfrom datetime import datetime\n\n# MNIST input data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)\n\ndef multilayer_perceptron(x):\n    fc1 = layers.fully_connected(x, 256, activation_fn=tf.nn.relu, scope=\'fc1\')\n    fc2 = layers.fully_connected(fc1, 256, activation_fn=tf.nn.relu, scope=\'fc2\')\n    out = layers.fully_connected(fc2, 10, activation_fn=None, scope=\'out\')\n    return out\n\n# build model, loss, and train op\nx = tf.placeholder(tf.float32, [None, 784])\ny = tf.placeholder(tf.float32, [None, 10])\npred = multilayer_perceptron(x)\n\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\ntrain_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n\ndef train(session):\n    batch_size = 200\n    session.run(tf.global_variables_initializer())\n\n    # Training cycle\n    for epoch in range(10):\n        epoch_loss = 0.0\n        batch_steps = mnist.train.num_examples / batch_size\n        for i in range(batch_steps):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n            _, c = session.run([train_op, loss],\n                               feed_dict={x: batch_x, y: batch_y})\n            epoch_loss += c / batch_steps\n        print ""[%s] Epoch %02d, Loss = %.6f"" % (datetime.now(), epoch, epoch_loss)\n\n    # Test model\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print ""Accuracy:"", accuracy.eval({x: mnist.test.images, y: mnist.test.labels})\n\ndef main():\n    with tf.Session(config=tf.ConfigProto(\n        gpu_options=tf.GPUOptions(allow_growth=True),\n        device_count={\'GPU\': 1})) as session:\n        train(session)\n\nif __name__ == \'__main__\':\n    main()\n'"
codes/11-mnist-fetch.py,11,"b'import tensorflow as tf\nimport tensorflow.contrib.layers as layers\nfrom datetime import datetime\n\n# MNIST input data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)\n\ndef multilayer_perceptron(x):\n    fc1 = layers.fully_connected(x, 256, activation_fn=tf.nn.relu, scope=\'fc1\')\n    fc2 = layers.fully_connected(fc1, 256, activation_fn=tf.nn.relu, scope=\'fc2\')\n    out = layers.fully_connected(fc2, 10, activation_fn=None, scope=\'out\')\n    return out, fc1, fc2\n\n# build model, loss, and train op\nnet = {}\nx = tf.placeholder(tf.float32, [None, 784])\ny = tf.placeholder(tf.float32, [None, 10])\npred, net[\'fc1\'], net[\'fc2\'] = multilayer_perceptron(x)         # (*) obtain fc1, fc2 as well\n\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\ntrain_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n\ndef train(session):\n    batch_size = 200\n    session.run(tf.global_variables_initializer())\n\n    # Training cycle\n    for epoch in range(10):\n        epoch_loss = 0.0\n        batch_steps = mnist.train.num_examples / batch_size\n        for i in range(batch_steps):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n            _, c, fc1, fc2, out = session.run(\n                [train_op, loss, net[\'fc1\'], net[\'fc2\'], pred],\n                feed_dict={x: batch_x, y: batch_y})\n            epoch_loss += c / batch_steps\n            if i == 0: # XXX Debug\n                print fc1[0].mean(), fc2[0].mean(), out[0]\n        print ""[%s] Epoch %02d, Loss = %.6f"" % (datetime.now(), epoch, epoch_loss)\n\n    # Test model\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print ""Accuracy:"", accuracy.eval({x: mnist.test.images, y: mnist.test.labels})\n\ndef main():\n    with tf.Session(config=tf.ConfigProto(\n        gpu_options=tf.GPUOptions(allow_growth=True),\n        device_count={\'GPU\': 1})) as session:\n        train(session)\n\nif __name__ == \'__main__\':\n    main()\n'"
codes/12-mnist-summary.py,21,"b'import tensorflow as tf\nimport tensorflow.contrib.layers as layers\nfrom datetime import datetime\n\n# MNIST input data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)\n\ndef multilayer_perceptron(x):\n    fc1 = layers.fully_connected(x, 256, activation_fn=tf.nn.relu, scope=\'fc1\')\n    tf.summary.histogram(\'fc1\', fc1)\n    tf.summary.histogram(\'fc1/sparsity\', tf.nn.zero_fraction(fc1))\n    fc2 = layers.fully_connected(fc1, 256, activation_fn=tf.nn.relu, scope=\'fc2\')\n    tf.summary.histogram(\'fc2\', fc2)\n    tf.summary.histogram(\'fc2/sparsity\', tf.nn.zero_fraction(fc2))\n    out = layers.fully_connected(fc2, 10, activation_fn=None, scope=\'out\')\n    return out\n\n# build model, loss, and train op\nx = tf.placeholder(tf.float32, [None, 784])\ny = tf.placeholder(tf.float32, [None, 10])\npred = multilayer_perceptron(x)\n\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\ntf.summary.scalar(\'loss\', loss)\n\nglobal_step = tf.Variable(0, dtype=tf.int32, trainable=False)   # (*)\ntrain_op = tf.train.AdamOptimizer(learning_rate=0.001)\\\n                   .minimize(loss, global_step=global_step)     # (*)\n\n# histogram summary for all trainable variables (slow?)\nfor v in tf.trainable_variables():\n    tf.summary.histogram(v.name, v)\n\ndef train(session):\n    batch_size = 200\n    session.run(tf.global_variables_initializer())\n    merged_summary_op = tf.summary.merge_all()\n    summary_writer = tf.summary.FileWriter(\'/tmp/mnist\', session.graph)\n\n    # Training cycle\n    for epoch in range(10):\n        epoch_loss = 0.0\n        batch_steps = mnist.train.num_examples / batch_size\n        for step in range(batch_steps):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n            _, c, summary = session.run(\n                [train_op, loss, merged_summary_op],\n                feed_dict={x: batch_x, y: batch_y})\n            summary_writer.add_summary(summary, global_step.eval(session=session))\n            epoch_loss += c / batch_steps\n        print ""[%s] Epoch %02d, Loss = %.6f"" % (datetime.now(), epoch, epoch_loss)\n\n    # Test model\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print ""Accuracy:"", accuracy.eval({x: mnist.test.images, y: mnist.test.labels})\n\ndef main():\n    with tf.Session(config=tf.ConfigProto(\n        gpu_options=tf.GPUOptions(allow_growth=True),\n        device_count={\'GPU\': 1})) as session:\n        train(session)\n\nif __name__ == \'__main__\':\n    main()\n'"
codes/13-mnist-print.py,12,"b'import tensorflow as tf\nimport tensorflow.contrib.layers as layers\nfrom datetime import datetime\n\n# MNIST input data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)\n\ndef multilayer_perceptron(x):\n    fc1 = layers.fully_connected(x, 256, activation_fn=tf.nn.relu, scope=\'fc1\')\n    fc2 = layers.fully_connected(fc1, 256, activation_fn=tf.nn.relu, scope=\'fc2\')\n    out = layers.fully_connected(fc2, 10, activation_fn=None, scope=\'out\')\n    out = tf.Print(out, [tf.argmax(out, 1)],\n                   \'argmax(out) = \', summarize=20, first_n=7)\n    return out\n\n# build model, loss, and train op\nx = tf.placeholder(tf.float32, [None, 784])\ny = tf.placeholder(tf.float32, [None, 10])\npred = multilayer_perceptron(x)\n\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\ntrain_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n\ndef train(session):\n    batch_size = 200\n    session.run(tf.global_variables_initializer())\n\n    # Training cycle\n    for epoch in range(10):\n        epoch_loss = 0.0\n        batch_steps = mnist.train.num_examples / batch_size\n        for step in range(batch_steps):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n            _, c = session.run(\n                [train_op, loss],\n                feed_dict={x: batch_x, y: batch_y})\n            epoch_loss += c / batch_steps\n        print ""[%s] Epoch %02d, Loss = %.6f"" % (datetime.now(), epoch, epoch_loss)\n\n    # Test model\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print ""Accuracy:"", accuracy.eval({x: mnist.test.images, y: mnist.test.labels})\n\ndef main():\n    with tf.Session(config=tf.ConfigProto(\n        gpu_options=tf.GPUOptions(allow_growth=True),\n        device_count={\'GPU\': 1})) as session:\n        train(session)\n\nif __name__ == \'__main__\':\n    main()\n'"
codes/14-mnist-assert-alternative.py,13,"b'import tensorflow as tf\nimport tensorflow.contrib.layers as layers\nfrom datetime import datetime\n\n# MNIST input data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)\n\ndef multilayer_perceptron(x):\n    fc1 = layers.fully_connected(x, 256, activation_fn=tf.nn.relu, scope=\'fc1\')\n    fc2 = layers.fully_connected(fc1, 256, activation_fn=tf.nn.relu, scope=\'fc2\')\n    out = layers.fully_connected(fc2, 10, activation_fn=None, scope=\'out\')\n    tf.add_to_collection(\'Asserts\', tf.Assert(tf.reduce_all(out > 0), [out], name=\'assert_out_positive\'))     # (*)\n    return out\n\n# build model, loss, and train op\nx = tf.placeholder(tf.float32, [None, 784])\ny = tf.placeholder(tf.float32, [None, 10])\npred = multilayer_perceptron(x)\n\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\ntrain_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n\ndef train(session):\n    batch_size = 200\n    session.run(tf.global_variables_initializer())\n    assert_op = tf.group(*tf.get_collection(\'Asserts\'))     # (*)\n\n    # Training cycle\n    for epoch in range(10):\n        epoch_loss = 0.0\n        batch_steps = mnist.train.num_examples / batch_size\n        for step in range(batch_steps):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n            _, c, _ = session.run(\n                [train_op, loss, assert_op],     # (*)\n                feed_dict={x: batch_x, y: batch_y})\n            epoch_loss += c / batch_steps\n        print ""[%s] Epoch %02d, Loss = %.6f"" % (datetime.now(), epoch, epoch_loss)\n\n    # Test model\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print ""Accuracy:"", accuracy.eval({x: mnist.test.images, y: mnist.test.labels})\n\ndef main():\n    with tf.Session(config=tf.ConfigProto(\n        gpu_options=tf.GPUOptions(allow_growth=True),\n        device_count={\'GPU\': 1})) as session:\n        train(session)\n\nif __name__ == \'__main__\':\n    main()\n'"
codes/14-mnist-assert-wrong.py,12,"b'import tensorflow as tf\nimport tensorflow.contrib.layers as layers\nfrom datetime import datetime\n\n# MNIST input data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)\n\ndef multilayer_perceptron(x):\n    fc1 = layers.fully_connected(x, 256, activation_fn=tf.nn.relu, scope=\'fc1\')\n    fc2 = layers.fully_connected(fc1, 256, activation_fn=tf.nn.relu, scope=\'fc2\')\n    out = layers.fully_connected(fc2, 10, activation_fn=None, scope=\'out\')\n    tf.Assert(out > 0, [out], name=\'assert_out_positive\')\n    return out\n\n# build model, loss, and train op\nx = tf.placeholder(tf.float32, [None, 784])\ny = tf.placeholder(tf.float32, [None, 10])\npred = multilayer_perceptron(x)\n\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\ntrain_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n\ndef train(session):\n    batch_size = 200\n    session.run(tf.global_variables_initializer())\n\n    # Training cycle\n    for epoch in range(10):\n        epoch_loss = 0.0\n        batch_steps = mnist.train.num_examples / batch_size\n        for step in range(batch_steps):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n            _, c = session.run(\n                [train_op, loss],\n                feed_dict={x: batch_x, y: batch_y})\n            epoch_loss += c / batch_steps\n        print ""[%s] Epoch %02d, Loss = %.6f"" % (datetime.now(), epoch, epoch_loss)\n\n    # Test model\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print ""Accuracy:"", accuracy.eval({x: mnist.test.images, y: mnist.test.labels})\n\ndef main():\n    with tf.Session(config=tf.ConfigProto(\n        gpu_options=tf.GPUOptions(allow_growth=True),\n        device_count={\'GPU\': 1})) as session:\n        train(session)\n\nif __name__ == \'__main__\':\n    main()\n'"
codes/14-mnist-assert.py,15,"b'import tensorflow as tf\nimport tensorflow.contrib.layers as layers\nfrom datetime import datetime\n\n# MNIST input data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)\n\ndef multilayer_perceptron(x):\n    fc1 = layers.fully_connected(x, 256, activation_fn=tf.nn.relu, scope=\'fc1\')\n    fc2 = layers.fully_connected(fc1, 256, activation_fn=tf.nn.relu, scope=\'fc2\')\n    out = layers.fully_connected(fc2, 10, activation_fn=None, scope=\'out\')\n    assert_op = tf.Assert(tf.reduce_all(out > 0), [out], name=\'assert_out_positive\')\n    #out = tf.with_dependencies([assert_op], out)\n    with tf.control_dependencies([assert_op]):\n        out = tf.identity(out, name=\'out\')\n    return out\n\n# build model, loss, and train op\nx = tf.placeholder(tf.float32, [None, 784])\ny = tf.placeholder(tf.float32, [None, 10])\npred = multilayer_perceptron(x)\n\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\ntrain_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n\ndef train(session):\n    batch_size = 200\n    session.run(tf.global_variables_initializer())\n\n    # Training cycle\n    for epoch in range(10):\n        epoch_loss = 0.0\n        batch_steps = mnist.train.num_examples / batch_size\n        for step in range(batch_steps):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n            _, c = session.run(\n                [train_op, loss],\n                feed_dict={x: batch_x, y: batch_y})\n            epoch_loss += c / batch_steps\n        print ""[%s] Epoch %02d, Loss = %.6f"" % (datetime.now(), epoch, epoch_loss)\n\n    # Test model\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print ""Accuracy:"", accuracy.eval({x: mnist.test.images, y: mnist.test.labels})\n\ndef main():\n    with tf.Session(config=tf.ConfigProto(\n        gpu_options=tf.GPUOptions(allow_growth=True),\n        device_count={\'GPU\': 1})) as session:\n        train(session)\n\nif __name__ == \'__main__\':\n    main()\n'"
codes/19-mnist-profiling.py,16,"b'import tensorflow as tf\nimport tensorflow.contrib.layers as layers\nfrom datetime import datetime\n\n# MNIST input data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)\n\ndef multilayer_perceptron(x):\n    fc1 = layers.fully_connected(x, 256, activation_fn=tf.nn.relu)\n    fc2 = layers.fully_connected(fc1, 256, activation_fn=tf.nn.relu)\n    out = layers.fully_connected(fc2, 10, activation_fn=None)\n    return out\n\n# build model, loss, and train op\nx = tf.placeholder(tf.float32, [None, 784])\ny = tf.placeholder(tf.float32, [None, 10])\npred = multilayer_perceptron(x)\n\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\ntrain_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n\ndef train(session):\n    batch_size = 200\n    session.run(tf.global_variables_initializer())\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)   # (*)\n    run_metadata = tf.RunMetadata()\n\n    # Training cycle\n    for epoch in range(10):\n        epoch_loss = 0.0\n        batch_steps = mnist.train.num_examples / batch_size\n        for step in range(batch_steps):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n            _, c = session.run(\n                [train_op, loss],\n                feed_dict={x: batch_x, y: batch_y},\n                options=run_options, run_metadata=run_metadata          # (*)\n            )\n            epoch_loss += c / batch_steps\n        print ""[%s] Epoch %02d, Loss = %.6f"" % (datetime.now(), epoch, epoch_loss)\n\n    # Dump profiling data (*)\n    prof_timeline = tf.python.client.timeline.Timeline(run_metadata.step_stats)\n    prof_ctf = prof_timeline.generate_chrome_trace_format()\n    with open(\'./prof_ctf.json\', \'w\') as fp:\n        print \'Dumped to prof_ctf.json\'\n        fp.write(prof_ctf)\n\n    # Test model\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print ""Accuracy:"", accuracy.eval({x: mnist.test.images, y: mnist.test.labels})\n\ndef main():\n    with tf.Session(config=tf.ConfigProto(\n        gpu_options=tf.GPUOptions(allow_growth=True),\n        device_count={\'GPU\': 1})) as session:\n        train(session)\n\nif __name__ == \'__main__\':\n    main()\n'"
codes/20-mnist-pdb.py,11,"b'import numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.layers as layers\nfrom datetime import datetime\n\n# MNIST input data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)\n\ndef multilayer_perceptron(x):\n    fc1 = layers.fully_connected(x, 256, activation_fn=tf.nn.relu, scope=\'fc1\')\n    fc2 = layers.fully_connected(fc1, 256, activation_fn=tf.nn.relu, scope=\'fc2\')\n    out = layers.fully_connected(fc2, 10, activation_fn=None, scope=\'out\')\n    return out\n\n# build model, loss, and train op\nx = tf.placeholder(tf.float32, [None, 784])\ny = tf.placeholder(tf.float32, [None, 10])\npred = multilayer_perceptron(x)\n\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\ntrain_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n\ndef train(session):\n    batch_size = 200\n    session.run(tf.global_variables_initializer())\n\n    # Training cycle\n    for epoch in range(10):\n        epoch_loss = 0.0\n        batch_steps = mnist.train.num_examples / batch_size\n        for i in range(batch_steps):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n            # ---------------\n            if (np.argmax(batch_y, axis=1)[:7] == [4, 9, 6, 2, 9, 6, 5]).all():\n                import pudb; pudb.set_trace()  # XXX BREAKPOINT\n            # ---------------\n            _, c = session.run([train_op, loss],\n                               feed_dict={x: batch_x, y: batch_y})\n            epoch_loss += c / batch_steps\n        print ""[%s] Epoch %02d, Loss = %.6f"" % (datetime.now(), epoch, epoch_loss)\n\n    # Test model\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print ""Accuracy:"", accuracy.eval({x: mnist.test.images, y: mnist.test.labels})\n\ndef main():\n    with tf.Session(config=tf.ConfigProto(\n        gpu_options=tf.GPUOptions(allow_growth=True),\n        device_count={\'GPU\': 1})) as session:\n        train(session)\n\nif __name__ == \'__main__\':\n    main()\n'"
codes/31-mnist-func-print.py,13,"b'import tensorflow as tf\nimport tensorflow.contrib.layers as layers\nfrom datetime import datetime\n\n# MNIST input data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)\n\ndef multilayer_perceptron(x):\n    fc1 = layers.fully_connected(x, 256, activation_fn=tf.nn.relu, scope=\'fc1\')\n    fc2 = layers.fully_connected(fc1, 256, activation_fn=tf.nn.relu, scope=\'fc2\')\n\n    def _debug_print_func(fc1_val, fc2_val):\n        print \'FC1 : {}, FC2 : {}\'.format(fc1_val.shape, fc2_val.shape)\n        return False\n    debug_print_op = tf.py_func(_debug_print_func, [fc1, fc2], [tf.bool])\n    with tf.control_dependencies(debug_print_op):\n        out = layers.fully_connected(fc2, 10, activation_fn=None, scope=\'out\')\n    return out\n\n# build model, loss, and train op\nx = tf.placeholder(tf.float32, [None, 784])\ny = tf.placeholder(tf.float32, [None, 10])\npred = multilayer_perceptron(x)\n\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\ntrain_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n\ndef train(session):\n    batch_size = 200\n    session.run(tf.global_variables_initializer())\n\n    # Training cycle\n    for epoch in range(10):\n        epoch_loss = 0.0\n        batch_steps = mnist.train.num_examples / batch_size\n        for i in range(batch_steps):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n            _, c = session.run([train_op, loss],\n                               feed_dict={x: batch_x, y: batch_y})\n            epoch_loss += c / batch_steps\n        print ""[%s] Epoch %02d, Loss = %.6f"" % (datetime.now(), epoch, epoch_loss)\n\n    # Test model\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print ""Accuracy:"", accuracy.eval({x: mnist.test.images, y: mnist.test.labels})\n\ndef main():\n    with tf.Session(config=tf.ConfigProto(\n        gpu_options=tf.GPUOptions(allow_growth=True),\n        device_count={\'GPU\': 1})) as session:\n        train(session)\n\nif __name__ == \'__main__\':\n    main()\n'"
codes/32-mnist-func-pdb.py,15,"b'import tensorflow as tf\nimport tensorflow.contrib.layers as layers\nfrom datetime import datetime\n\n# MNIST input data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)\n\ndef multilayer_perceptron(x):\n    global fc1, fc2\n    fc1 = layers.fully_connected(x, 256, activation_fn=tf.nn.relu, scope=\'fc1\')\n    fc2 = layers.fully_connected(fc1, 256, activation_fn=tf.nn.relu, scope=\'fc2\')\n    out = layers.fully_connected(fc2, 10, activation_fn=tf.nn.relu, scope=\'out\') #\n\n    def _debug_func(x_val, fc1_val, fc2_val, out_val):\n        if (out_val == 0.0).any():\n            import ipdb; ipdb.set_trace()  # XXX BREAKPOINT\n        #from IPython import embed; embed()  # XXX DEBUG\n        return False\n\n    debug_op = tf.py_func(_debug_func, [x, fc1, fc2, out], [tf.bool])\n    with tf.control_dependencies(debug_op):\n        out = tf.identity(out, name=\'out\')\n    return out\n\n# build model, loss, and train op\nx = tf.placeholder(tf.float32, [None, 784])\ny = tf.placeholder(tf.float32, [None, 10])\npred = multilayer_perceptron(x)\n\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\ntrain_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n\ndef train(session):\n    batch_size = 200\n    session.run(tf.global_variables_initializer())\n\n    # Training cycle\n    for epoch in range(10):\n        epoch_loss = 0.0\n        batch_steps = mnist.train.num_examples / batch_size\n        for i in range(batch_steps):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n            _, c = session.run([train_op, loss],\n                               feed_dict={x: batch_x, y: batch_y})\n            epoch_loss += c / batch_steps\n        print ""[%s] Epoch %02d, Loss = %.6f"" % (datetime.now(), epoch, epoch_loss)\n\n    # Test model\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print ""Accuracy:"", accuracy.eval({x: mnist.test.images, y: mnist.test.labels})\n\ndef main():\n    global session\n    with tf.Session(config=tf.ConfigProto(\n        gpu_options=tf.GPUOptions(allow_growth=True),\n        device_count={\'GPU\': 1})) as session:\n        train(session)\n\nif __name__ == \'__main__\':\n    main()\n'"
codes/40-mnist-name.py,13,"b'import tensorflow as tf\nimport tensorflow.contrib.layers as layers\nfrom datetime import datetime\n\n# MNIST input data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)\n\ndef multilayer_perceptron(x):\n    W_fc1 = tf.Variable(tf.random_normal([784, 256], mean=0, stddev=1))\n    b_fc1 = tf.Variable([0] * 256) # ???????\n    fc1 = tf.nn.xw_plus_b(x, W_fc1, b_fc1)\n    fc2 = layers.fully_connected(fc1, 256, activation_fn=tf.nn.relu, scope=\'fc2\')\n    out = layers.fully_connected(fc2, 10, activation_fn=None, scope=\'out\')\n    return out\n\n# build model, loss, and train op\nx = tf.placeholder(tf.float32, [None, 784], name=\'placeholder_x\')\ny = tf.placeholder(tf.float32, [None, 10], name=\'placeholder_y\')\npred = multilayer_perceptron(x)\n\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\ntrain_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n\ndef train(session):\n    batch_size = 200\n    session.run(tf.global_variables_initializer())\n\n    # Training cycle\n    for epoch in range(10):\n        epoch_loss = 0.0\n        batch_steps = mnist.train.num_examples / batch_size\n        for i in range(batch_steps):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n            _, c = session.run([train_op, loss],\n                               feed_dict={x: batch_x, y: batch_y})\n            epoch_loss += c / batch_steps\n        print ""[%s] Epoch %02d, Loss = %.6f"" % (datetime.now(), epoch, epoch_loss)\n\n    # Test model\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print ""Accuracy:"", accuracy.eval({x: mnist.test.images, y: mnist.test.labels})\n\ndef main():\n    with tf.Session(config=tf.ConfigProto(\n        gpu_options=tf.GPUOptions(allow_growth=True),\n        device_count={\'GPU\': 1})) as session:\n        train(session)\n\nif __name__ == \'__main__\':\n    main()\n'"
codes/50-mnist-tfdbg.py,11,"b'import tensorflow as tf\nimport tensorflow.contrib.layers as layers\nfrom datetime import datetime\n\nfrom tensorflow.python import debug as tf_debug\n\n# MNIST input data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)\n\ndef multilayer_perceptron(x):\n    fc1 = layers.fully_connected(x, 256, activation_fn=tf.nn.relu, scope=\'fc1\')\n    fc2 = layers.fully_connected(fc1, 256, activation_fn=tf.nn.relu, scope=\'fc2\')\n    out = layers.fully_connected(fc2, 10, activation_fn=None, scope=\'out\')\n    return out\n\n# build model, loss, and train op\nx = tf.placeholder(tf.float32, [None, 784], name=\'placeholder_x\')\ny = tf.placeholder(tf.float32, [None, 10], name=\'placeholder_y\')\npred = multilayer_perceptron(x)\n\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\ntrain_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n\ndef train(session):\n    # wrap in a local command-line debug session {{{\n    if FLAGS.debug:\n        session = tf_debug.LocalCLIDebugWrapperSession(session)\n        session.add_tensor_filter(""has_inf_or_nan"", tf_debug.has_inf_or_nan)\n    # }}}\n\n    batch_size = 200\n    session.run(tf.global_variables_initializer())\n\n    # Training cycle\n    for epoch in range(10):\n        epoch_loss = 0.0\n        batch_steps = mnist.train.num_examples / batch_size\n        for i in range(batch_steps):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n            _, c = session.run([train_op, loss],\n                               feed_dict={x: batch_x, y: batch_y})\n            epoch_loss += c / batch_steps\n        print ""[%s] Epoch %02d, Loss = %.6f"" % (datetime.now(), epoch, epoch_loss)\n\n    # Test model\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print ""Accuracy:"", accuracy.eval({x: mnist.test.images, y: mnist.test.labels})\n\ndef main():\n    with tf.Session(config=tf.ConfigProto(\n        gpu_options=tf.GPUOptions(allow_growth=True),\n        device_count={\'GPU\': 1})) as session:\n        train(session)\n\nif __name__ == \'__main__\':\n    import argparse\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.register(""type"", ""bool"", lambda v: v.lower() == ""true"")\n    parser.add_argument(""--debug"", type=""bool"", nargs=""?"", const=True, default=False)\n    FLAGS = parser.parse_args()\n\n    main()\n'"
codes/debug_mnist.py,30,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Demo of the tfdbg curses CLI: Locating the source of bad numerical values.\n\nThe neural network in this demo is larged based on the tutorial at:\n  tensorflow/examples/tutorials/mnist/mnist_with_summaries.py\n\nBut modifications are made so that problematic numerical values (infs and nans)\nappear in nodes of the graph during training.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\n\nimport tensorflow as tf\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nfrom tensorflow.python import debug as tf_debug\n\n\nIMAGE_SIZE = 28\nHIDDEN_SIZE = 500\nNUM_LABELS = 10\nRAND_SEED = 42\n\n\ndef main(_):\n  # Import data\n  mnist = input_data.read_data_sets(FLAGS.data_dir,\n                                    one_hot=True,\n                                    fake_data=FLAGS.fake_data)\n\n  def feed_dict(train):\n    if train or FLAGS.fake_data:\n      xs, ys = mnist.train.next_batch(FLAGS.train_batch_size,\n                                      fake_data=FLAGS.fake_data)\n    else:\n      xs, ys = mnist.test.images, mnist.test.labels\n\n    return {x: xs, y_: ys}\n\n  sess = tf.InteractiveSession()\n\n  # Create the MNIST neural network graph.\n\n  # Input placeholders.\n  with tf.name_scope(""input""):\n    x = tf.placeholder(\n        tf.float32, [None, IMAGE_SIZE * IMAGE_SIZE], name=""x-input"")\n    y_ = tf.placeholder(tf.float32, [None, NUM_LABELS], name=""y-input"")\n\n  def weight_variable(shape):\n    """"""Create a weight variable with appropriate initialization.""""""\n    initial = tf.truncated_normal(shape, stddev=0.1, seed=RAND_SEED)\n    return tf.Variable(initial)\n\n  def bias_variable(shape):\n    """"""Create a bias variable with appropriate initialization.""""""\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\n  def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n    """"""Reusable code for making a simple neural net layer.""""""\n    # Adding a name scope ensures logical grouping of the layers in the graph.\n    with tf.name_scope(layer_name):\n      # This Variable will hold the state of the weights for the layer\n      with tf.name_scope(""weights""):\n        weights = weight_variable([input_dim, output_dim])\n      with tf.name_scope(""biases""):\n        biases = bias_variable([output_dim])\n      with tf.name_scope(""Wx_plus_b""):\n        preactivate = tf.matmul(input_tensor, weights) + biases\n\n      activations = act(preactivate)\n      return activations\n\n  hidden = nn_layer(x, IMAGE_SIZE**2, HIDDEN_SIZE, ""hidden"")\n  y = nn_layer(hidden, HIDDEN_SIZE, NUM_LABELS, ""softmax"", act=tf.nn.softmax)\n\n  with tf.name_scope(""cross_entropy""):\n    # The following line is the culprit of the bad numerical values that appear\n    # during training of this graph. Log of zero gives inf, which is first seen\n    # in the intermediate tensor ""cross_entropy/Log:0"" during the 4th run()\n    # call. A multiplication of the inf values with zeros leads to nans,\n    # which is first in ""cross_entropy/mul:0"".\n    #\n    # You can use clipping to fix this issue, e.g.,\n    #   diff = y_ * tf.log(tf.clip_by_value(y, 1e-8, 1.0))\n\n    diff = y_ * tf.log(y)\n    with tf.name_scope(""total""):\n      cross_entropy = -tf.reduce_mean(diff)\n\n  with tf.name_scope(""train""):\n    train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(\n        cross_entropy)\n\n  with tf.name_scope(""accuracy""):\n    with tf.name_scope(""correct_prediction""):\n      correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    with tf.name_scope(""accuracy""):\n      accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n  sess.run(tf.global_variables_initializer())\n\n  if FLAGS.debug:\n    sess = tf_debug.LocalCLIDebugWrapperSession(sess, ui_type=FLAGS.ui_type)\n    sess.add_tensor_filter(""has_inf_or_nan"", tf_debug.has_inf_or_nan)\n\n  # Add this point, sess is a debug wrapper around the actual Session if\n  # FLAGS.debug is true. In that case, calling run() will launch the CLI.\n  for i in range(FLAGS.max_steps):\n    acc = sess.run(accuracy, feed_dict=feed_dict(False))\n    print(""Accuracy at step %d: %s"" % (i, acc))\n\n    sess.run(train_step, feed_dict=feed_dict(True))\n\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.register(""type"", ""bool"", lambda v: v.lower() == ""true"")\n  parser.add_argument(\n      ""--max_steps"",\n      type=int,\n      default=10,\n      help=""Number of steps to run trainer."")\n  parser.add_argument(\n      ""--train_batch_size"",\n      type=int,\n      default=100,\n      help=""Batch size used during training."")\n  parser.add_argument(\n      ""--learning_rate"",\n      type=float,\n      default=0.025,\n      help=""Initial learning rate."")\n  parser.add_argument(\n      ""--data_dir"",\n      type=str,\n      default=""/tmp/mnist_data"",\n      help=""Directory for storing data"")\n  parser.add_argument(\n      ""--ui_type"",\n      type=str,\n      default=""curses"",\n      help=""Command-line user interface type (curses | readline)"")\n  parser.add_argument(\n      ""--fake_data"",\n      type=""bool"",\n      nargs=""?"",\n      const=True,\n      default=False,\n      help=""Use fake MNIST data for unit testing"")\n  parser.add_argument(\n      ""--debug"",\n      type=""bool"",\n      nargs=""?"",\n      const=True,\n      default=False,\n      help=""Use debugger to track down bad values during training"")\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
