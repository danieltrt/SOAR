file_path,api_count,code
main.py,4,"b'import os\r\nimport random\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport importlib\r\nfrom data.dataset import Dataset\r\nfrom util import Configurator, tool\r\n\r\n\r\nnp.random.seed(2018)\r\nrandom.seed(2018)\r\ntf.set_random_seed(2017)\r\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\r\n\r\nif __name__ == ""__main__"":\r\n    conf = Configurator(""NeuRec.properties"", default_section=""hyperparameters"")\r\n    gpu_id = str(conf[""gpu_id""])\r\n    os.environ[""CUDA_VISIBLE_DEVICES""] = gpu_id\r\n\r\n    recommender = conf[""recommender""]\r\n    # num_thread = int(conf[""rec.number.thread""])\r\n\r\n    # if Tool.get_available_gpus(gpu_id):\r\n    #     os.environ[""CUDA_VISIBLE_DEVICES""] = gpu_id\r\n    dataset = Dataset(conf)\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    config.gpu_options.per_process_gpu_memory_fraction = conf[""gpu_mem""]\r\n    with tf.Session(config=config) as sess:\r\n        if importlib.util.find_spec(""model.general_recommender."" + recommender) is not None:\r\n            my_module = importlib.import_module(""model.general_recommender."" + recommender)\r\n            \r\n        elif importlib.util.find_spec(""model.social_recommender."" + recommender) is not None:\r\n            \r\n            my_module = importlib.import_module(""model.social_recommender."" + recommender)\r\n            \r\n        else:\r\n            my_module = importlib.import_module(""model.sequential_recommender."" + recommender)\r\n        \r\n        MyClass = getattr(my_module, recommender)\r\n        model = MyClass(sess, dataset, conf)\r\n\r\n        model.build_graph()\r\n        sess.run(tf.global_variables_initializer())\r\n        model.train_model()\r\n'"
setup.py,0,"b'from distutils.core import setup, Extension\nfrom Cython.Build import cythonize\nimport numpy as np\nimport os\n\npyx_directories = [""evaluator/backend/cpp/"", ""util/cython""]\ncpp_dirs = [""evaluator/backend/cpp/include"", ""util/cython/include""]\n\nextensions = [\n    Extension(\n        \'*\',\n        [""*.pyx""],\n        extra_compile_args=[""-std=c++11""])\n]\n\npwd = os.getcwd()\n\nadditional_dirs = [os.path.join(pwd, d) for d in cpp_dirs]\n\nfor t_dir in pyx_directories:\n    target_dir = os.path.join(pwd, t_dir)\n    os.chdir(target_dir)\n    ori_files = set(os.listdir(""./""))\n    setup(\n        ext_modules=cythonize(extensions,\n                              language=""c++""),\n        include_dirs=[np.get_include()]+additional_dirs\n    )\n\n    new_files = set(os.listdir(""./""))\n    for n_file in new_files:\n        if n_file not in ori_files and n_file.split(""."")[-1] in (""c"", ""cpp""):\n            os.remove(n_file)\n\n    os.chdir(pwd)\n'"
data/__init__.py,0,b'from .sampler import PointwiseSampler\r\nfrom .sampler import PairwiseSampler\r\nfrom .sampler import TimeOrderPointwiseSampler\r\nfrom .sampler import TimeOrderPairwiseSampler\r\n'
data/dataset.py,0,"b'""""""\nCreated on Aug 8, 2016\nProcessing datasets. \n@author: Xiangnan He (xiangnanhe@gmail.com)\n""""""\n\nimport os\nimport pandas as pd\nfrom scipy.sparse import csr_matrix\nfrom util.tool import csr_to_user_dict_bytime, csr_to_user_dict\nfrom .utils import check_md5\nfrom util.logger import Logger\nfrom util import randint_choice\nimport numpy as np\nfrom .utils import filter_data, split_by_ratio, split_by_loo\n\n\nclass Dataset(object):\n    def __init__(self, conf):\n        """"""Constructor\n        """"""\n        self.train_matrix = None\n        self.test_matrix = None\n        self.time_matrix = None\n        self.negative_matrix = None\n        self.userids = None\n        self.itemids = None\n        self.num_users = None\n        self.num_items = None\n        self.dataset_name = conf[""data.input.dataset""]\n\n        # self._split_data(conf)\n        self._load_data(conf)\n\n    def _get_data_path(self, config):\n        data_path = config[""data.input.path""]\n        ori_prefix = os.path.join(data_path, self.dataset_name)\n\n        saved_path = os.path.join(data_path, ""_tmp_""+self.dataset_name)\n        saved_prefix = ""%s_%s_u%d_i%d"" % (self.dataset_name, config[""splitter""], config[""user_min""], config[""item_min""])\n        if ""by_time"" in config and config[""by_time""] is True:\n            saved_prefix += ""_by_time""\n\n        saved_prefix = os.path.join(saved_path, saved_prefix)\n\n        return ori_prefix, saved_prefix\n\n    def _check_saved_data(self, splitter, ori_prefix, saved_prefix):\n        check_state = False\n        # get md5\n        if splitter in (""loo"", ""ratio""):\n            rating_file = ori_prefix + "".rating""\n            ori_file_md5 = [check_md5(rating_file)]\n        elif splitter == ""given"":\n            train_file = ori_prefix + "".train""\n            test_file = ori_prefix + "".test""\n            ori_file_md5 = [check_md5(file) for file in [train_file, test_file]]\n        else:\n            raise ValueError(""\'%s\' is an invalid splitter!"" % splitter)\n\n        # check md5\n        if os.path.isfile(saved_prefix + "".md5""):\n            with open(saved_prefix + "".md5"", \'r\') as md5_fin:\n                saved_md5 = md5_fin.readlines()\n            if ori_file_md5 == saved_md5:\n                check_state = True\n\n        # check saved files\n        for postfix in ["".train"", "".test"", "".user2id"", "".item2id""]:\n            if not os.path.isfile(saved_prefix + postfix):\n                check_state = False\n\n        return check_state\n\n    def _load_data(self, config):\n        format_dict = {""UIRT"": [""user"", ""item"", ""rating"", ""time""],\n                       ""UIR"": [""user"", ""item"", ""rating""],\n                       ""UI"": [""user"", ""item""]}\n        file_format = config[""data.column.format""]\n        if file_format not in format_dict:\n            raise ValueError(""\'%s\' is an invalid data column format!"" % file_format)\n\n        ori_prefix, saved_prefix = self._get_data_path(config)\n        splitter = config[""splitter""]\n        sep = config[""data.convert.separator""]\n        columns = format_dict[file_format]\n        train_file = saved_prefix + "".train""\n        test_file = saved_prefix + "".test""\n        user_map_file = saved_prefix + "".user2id""\n        item_map_file = saved_prefix + "".item2id""\n\n        if self._check_saved_data(splitter, ori_prefix, saved_prefix):\n            # load saved data\n            train_data = pd.read_csv(train_file, sep=sep, header=None, names=columns)\n            test_data = pd.read_csv(test_file, sep=sep, header=None, names=columns)\n\n            user_map = pd.read_csv(user_map_file, sep=sep, header=None, names=[""user"", ""id""])\n            item_map = pd.read_csv(item_map_file, sep=sep, header=None, names=[""item"", ""id""])\n            self.userids = {user: uid for user, uid in zip(user_map[""user""], user_map[""id""])}\n            self.itemids = {item: iid for item, iid in zip(item_map[""item""], item_map[""id""])}\n        else:  # split and save data\n            by_time = config[""by_time""] if file_format == ""UIRT"" else False\n            train_data, test_data = self._split_data(ori_prefix, saved_prefix, columns, by_time, config)\n\n        all_data = pd.concat([train_data, test_data])\n        self.num_users = max(all_data[""user""]) + 1\n        self.num_items = max(all_data[""item""]) + 1\n        self.num_ratings = len(all_data)\n\n        if file_format == ""UI"":\n            train_ratings = [1.0] * len(train_data[""user""])\n            test_ratings = [1.0] * len(test_data[""user""])\n        else:\n            train_ratings = train_data[""rating""]\n            test_ratings = test_data[""rating""]\n\n        self.train_matrix = csr_matrix((train_ratings, (train_data[""user""], train_data[""item""])),\n                                       shape=(self.num_users, self.num_items))\n        self.test_matrix = csr_matrix((test_ratings, (test_data[""user""], test_data[""item""])),\n                                      shape=(self.num_users, self.num_items))\n\n        if file_format == ""UIRT"":\n            self.time_matrix = csr_matrix((train_data[""time""], (train_data[""user""], train_data[""item""])),\n                                          shape=(self.num_users, self.num_items))\n\n        self.negative_matrix = self._load_test_neg_items(all_data, config, saved_prefix)\n\n    def _split_data(self, ori_prefix, saved_prefix, columns, by_time, config):\n        splitter = config[""splitter""]\n        user_min = config[""user_min""]\n        item_min = config[""item_min""]\n        sep = config[""data.convert.separator""]\n\n        dir_name = os.path.dirname(saved_prefix)\n        if not os.path.exists(dir_name):\n            os.makedirs(dir_name)\n\n        if splitter in (""loo"", ""ratio""):\n            rating_file = ori_prefix + "".rating""\n            all_data = pd.read_csv(rating_file, sep=sep, header=None, names=columns)\n            filtered_data = filter_data(all_data, user_min=user_min, item_min=item_min)\n            if splitter == ""ratio"":\n                ratio = config[""ratio""]\n                train_data, test_data = split_by_ratio(filtered_data, ratio=ratio, by_time=by_time)\n            elif splitter == ""loo"":\n                train_data, test_data = split_by_loo(filtered_data, by_time=by_time)\n            else:\n                raise ValueError(""There is not splitter \'%s\'"" % splitter)\n            with open(saved_prefix+"".md5"", ""w"") as md5_out:\n                md5_out.writelines(check_md5(rating_file))\n        elif splitter == ""given"":\n            train_file = ori_prefix + "".train""\n            test_file = ori_prefix + "".test""\n            train_data = pd.read_csv(train_file, sep=sep, header=None, names=columns)\n            test_data = pd.read_csv(test_file, sep=sep, header=None, names=columns)\n            with open(saved_prefix+"".md5"", ""w"") as md5_out:\n                md5_out.writelines(check_md5(train_file))\n                md5_out.writelines(check_md5(test_file))\n        else:\n            raise ValueError(""\'%s\' is an invalid splitter!"" % splitter)\n\n        # remap id\n        all_data = pd.concat([train_data, test_data])\n        unique_user = all_data[""user""].unique()\n        self.userids = pd.Series(data=range(len(unique_user)), index=unique_user).to_dict()\n        train_data[""user""] = train_data[""user""].map(self.userids)\n        test_data[""user""] = test_data[""user""].map(self.userids)\n\n        unique_item = all_data[""item""].unique()\n        self.itemids = pd.Series(data=range(len(unique_item)), index=unique_item).to_dict()\n        train_data[""item""] = train_data[""item""].map(self.itemids)\n        test_data[""item""] = test_data[""item""].map(self.itemids)\n\n        # save files\n        np.savetxt(saved_prefix+"".train"", train_data, fmt=\'%d\', delimiter=sep)\n        np.savetxt(saved_prefix+"".test"", test_data, fmt=\'%d\', delimiter=sep)\n\n        user2id = [[user, id] for user, id in self.userids.items()]\n        item2id = [[item, id] for item, id in self.itemids.items()]\n        np.savetxt(saved_prefix+"".user2id"", user2id, fmt=\'%s\', delimiter=sep)\n        np.savetxt(saved_prefix+"".item2id"", item2id, fmt=\'%s\', delimiter=sep)\n\n        # remap test negative items and save to a file\n        neg_item_file = ori_prefix + "".neg""\n        if os.path.isfile(neg_item_file):\n            neg_item_list = []\n            with open(neg_item_file, \'r\') as fin:\n                for line in fin.readlines():\n                    line = line.strip().split(sep)\n                    user_items = [self.userids[line[0]]]\n                    user_items.extend([self.itemids[i] for i in line[1:]])\n                    neg_item_list.append(user_items)\n\n            test_neg = len(neg_item_list[0]) - 1\n            np.savetxt(""%s.neg%d"" % (saved_prefix, test_neg), neg_item_list, fmt=\'%d\', delimiter=sep)\n\n        all_remapped_data = pd.concat([train_data, test_data])\n        self.num_users = max(all_remapped_data[""user""]) + 1\n        self.num_items = max(all_remapped_data[""item""]) + 1\n        self.num_ratings = len(all_remapped_data)\n\n        logger = Logger(saved_prefix+"".info"")\n        logger.info(os.path.basename(saved_prefix))\n        logger.info(self.__str__())\n\n        return train_data, test_data\n\n    def _load_test_neg_items(self, all_data, config, saved_prefix):\n        number_neg = config[""rec.evaluate.neg""]\n        sep = config[""data.convert.separator""]\n        neg_matrix = None\n        if number_neg > 0:\n            neg_items_file = ""%s.neg%d"" % (saved_prefix, number_neg)\n            if not os.path.isfile(neg_items_file):\n                # sampling\n                neg_items = []\n                grouped_user = all_data.groupby([""user""])\n                for user, u_data in grouped_user:\n                    line = [user]\n                    line.extend(randint_choice(self.num_items, size=number_neg,\n                                               replace=False, exclusion=u_data[""item""].tolist()))\n                    neg_items.append(line)\n\n                neg_items = pd.DataFrame(neg_items)\n                np.savetxt(""%s.neg%d"" % (saved_prefix, number_neg), neg_items, fmt=\'%d\', delimiter=sep)\n            else:\n                # load file\n                neg_items = pd.read_csv(neg_items_file, sep=sep, header=None)\n\n            user_list, item_list = [], []\n            for line in neg_items.values:\n                user_list.extend([line[0]] * (len(line) - 1))\n                item_list.extend(line[1:])\n\n            neg_matrix = csr_matrix(([1] * len(user_list), (user_list, item_list)),\n                                    shape=(self.num_users, self.num_items))\n\n        return neg_matrix\n\n    def __str__(self):\n        num_users, num_items = self.num_users, self.num_items\n        num_ratings = self.num_ratings\n        sparsity = 1 - 1.0*num_ratings/(num_users*num_items)\n        data_info = [""Dataset name: %s"" % self.dataset_name,\n                     ""The number of users: %d"" % num_users,\n                     ""The number of items: %d"" % num_items,\n                     ""The number of ratings: %d"" % num_ratings,\n                     ""Average actions of users: %.2f"" % (1.0*num_ratings/num_users),\n                     ""Average actions of items: %.2f"" % (1.0*num_ratings/num_items),\n                     ""The sparsity of the dataset: %.6f%%"" % (sparsity * 100)]\n        data_info = ""\\n"".join(data_info)\n        return data_info\n\n    def __repr__(self):\n        return self.__str__()\n\n    def get_user_train_dict(self, by_time=False):\n        if by_time:\n            train_dict = csr_to_user_dict_bytime(self.time_matrix, self.train_matrix)\n        else:\n            train_dict = csr_to_user_dict(self.train_matrix)\n\n        return train_dict\n\n    def get_user_test_dict(self):\n        test_dict = csr_to_user_dict(self.test_matrix)\n        return test_dict\n\n    def get_user_test_neg_dict(self):\n        test_neg_dict = None\n        if self.negative_matrix is not None:\n            test_neg_dict = csr_to_user_dict(self.negative_matrix)\n        return test_neg_dict\n\n    def get_train_interactions(self):\n        dok_matrix = self.train_matrix.todok()\n        users_list, items_list = [], []\n        for (user, item), value in dok_matrix.items():\n            users_list.append(user)\n            items_list.append(item)\n\n        return users_list, items_list\n\n    def to_csr_matrix(self):\n        return self.train_matrix.copy()\n'"
data/sampler.py,0,"b'""""""\n@author: Zhongchuan Sun\n""""""\nfrom util import DataIterator\nfrom util.cython.random_choice import batch_randint_choice\nfrom collections import Iterable\nimport numpy as np\n\n\nclass Sampler(object):\n    """"""Base class for all sampler to sample negative items.\n    """"""\n\n    def __init__(self):\n        pass\n\n    def __len__(self):\n        raise NotImplementedError\n\n    def __iter__(self):\n        raise NotImplementedError\n\n\ndef _generate_positive_items(user_pos_dict):\n    if not isinstance(user_pos_dict, dict):\n        raise TypeError(""\'user_pos_dict\' must be a dict."")\n\n    if not user_pos_dict:\n        raise ValueError(""\'user_pos_dict\' cannot be empty."")\n\n    users_list, pos_items_list = [], []\n    user_pos_len = []\n    for user, pos_items in user_pos_dict.items():\n        pos_len = len(pos_items)\n        user_pos_len.append([user, pos_len])\n        users_list.extend([user] * len(pos_items))\n        pos_items_list.extend(pos_items)\n\n    return user_pos_len, users_list, pos_items_list\n\n\ndef _generative_time_order_positive_items(user_pos_dict, high_order=1):\n    if high_order <= 0:\n        raise ValueError(""\'high_order\' must be a positive integer."")\n\n    if not isinstance(user_pos_dict, dict):\n        raise TypeError(""\'user_pos_dict\' must be a dict."")\n\n    if not user_pos_dict:\n        raise ValueError(""\'user_pos_dict\' cannot be empty."")\n\n    users_list, recent_items_list, pos_items_list = [], [], []\n    user_pos_len = []\n    for user, seq_items in user_pos_dict.items():\n        if len(seq_items) - high_order <= 0:\n            continue\n        num_instance = len(seq_items) - high_order\n        user_pos_len.append([user, num_instance])\n        users_list.extend([user] * num_instance)\n        if high_order == 1:\n            r_items = [seq_items[idx] for idx in range(num_instance)]\n        else:\n            r_items = [seq_items[idx:][:high_order] for idx in range(num_instance)]\n\n        recent_items_list.extend(r_items)\n        pos_items_list.extend(seq_items[high_order:])\n\n    return user_pos_len, users_list, recent_items_list, pos_items_list\n\n\ndef _sampling_negative_items(user_pos_len, neg_num, item_num, user_pos_dict):\n    if neg_num <= 0:\n        raise ValueError(""\'neg_num\' must be a positive integer."")\n\n    users, n_pos = list(zip(*user_pos_len))\n    users_n_pos = DataIterator(users, n_pos, batch_size=1024, shuffle=False, drop_last=False)\n    neg_items_list = []\n    for bat_user, batch_num in users_n_pos:\n        batch_num = [num * neg_num for num in batch_num]\n        exclusion = [user_pos_dict[u] for u in bat_user]\n        bat_neg_items = batch_randint_choice(item_num, batch_num, replace=True, exclusion=exclusion)\n\n        for user, neg_items, n_item in zip(bat_user, bat_neg_items, batch_num):\n            if isinstance(neg_items, Iterable):\n                if neg_num > 1:\n                    neg_items = np.reshape(neg_items, newshape=[-1, neg_num])\n                neg_items_list.extend(neg_items)\n            else:\n                neg_items_list.append(neg_items)\n    return neg_items_list\n\n\nclass PointwiseSampler(Sampler):\n    """"""Sampling negative items and construct pointwise training instances.\n\n    The training instances consist of `batch_user`, `batch_item` and\n    `batch_label`, which are lists of users, items and labels. All lengths of\n    them are `batch_size`.\n    Positive and negative items are labeled as `1` and  `0`, respectively.\n    """"""\n\n    def __init__(self, dataset, neg_num=1, batch_size=1024, shuffle=True, drop_last=False):\n        """"""Initializes a new `PointwiseSampler` instance.\n\n        Args:\n            dataset (data.Dataset): An instance of `Dataset`.\n            neg_num (int): How many negative items for each positive item.\n                Defaults to `1`.\n            batch_size (int): How many samples per batch to load.\n                Defaults to `1`.\n            shuffle (bool): Whether reshuffling the samples at every epoch.\n                Defaults to `False`.\n            drop_last (bool): Whether dropping the last incomplete batch.\n                Defaults to `False`.\n        """"""\n        super(Sampler, self).__init__()\n        if neg_num <= 0:\n            raise ValueError(""\'neg_num\' must be a positive integer."")\n\n        self.batch_size = batch_size\n        self.drop_last = drop_last\n        self.shuffle = shuffle\n        self.neg_num = neg_num\n        self.item_num = dataset.num_items\n        self.user_pos_dict = dataset.get_user_train_dict()\n        self.user_pos_len, users_list, self.pos_items_list = \\\n            _generate_positive_items(self.user_pos_dict)\n\n        self.users_list = users_list * (self.neg_num+1)\n        len_pos_items = len(self.pos_items_list)\n        pos_labels_list = [1.0] * len_pos_items\n        neg_labels_list = [0.0] * (len_pos_items * self.neg_num)\n        self.all_labels = pos_labels_list + neg_labels_list\n\n    def __iter__(self):\n        neg_items_list = _sampling_negative_items(self.user_pos_len, self.neg_num,\n                                                  self.item_num, self.user_pos_dict)\n\n        neg_items = np.array(neg_items_list, dtype=np.int32)\n        neg_items = np.reshape(neg_items.T, [-1]).tolist()\n        all_items = self.pos_items_list + neg_items\n\n        data_iter = DataIterator(self.users_list, all_items, self.all_labels,\n                                 batch_size=self.batch_size,\n                                 shuffle=self.shuffle, drop_last=self.drop_last)\n\n        for bat_users, bat_items, bat_labels in data_iter:\n            yield bat_users, bat_items, bat_labels\n\n    def __len__(self):\n        n_sample = len(self.users_list)\n        if self.drop_last:\n            return n_sample // self.batch_size\n        else:\n            return (n_sample + self.batch_size - 1) // self.batch_size\n\n\nclass PairwiseSampler(Sampler):\n    """"""Sampling negative items and construct pairwise training instances.\n\n    The training instances consist of `batch_user`, `batch_pos_item` and\n    `batch_neg_items`, where `batch_user` and `batch_pos_item` are lists\n    of users and positive items with length `batch_size`, and `neg_items`\n    does not interact with `user`.\n\n    If `neg_num == 1`, `batch_neg_items` is also a list of negative items\n    with length `batch_size`;  If `neg_num > 1`, `batch_neg_items` is an\n    array like list with shape `(batch_size, neg_num)`.\n    """"""\n    def __init__(self, dataset, neg_num=1, batch_size=1024, shuffle=True, drop_last=False):\n        """"""Initializes a new `PairwiseSampler` instance.\n\n        Args:\n            dataset (data.Dataset): An instance of `Dataset`.\n            neg_num (int): How many negative items for each positive item.\n                Defaults to `1`.\n            batch_size (int): How many samples per batch to load.\n                Defaults to `1`.\n            shuffle (bool): Whether reshuffling the samples at every epoch.\n                Defaults to `False`.\n            drop_last (bool): Whether dropping the last incomplete batch.\n                Defaults to `False`.\n        """"""\n        super(PairwiseSampler, self).__init__()\n        if neg_num <= 0:\n            raise ValueError(""\'neg_num\' must be a positive integer."")\n\n        self.batch_size = batch_size\n        self.drop_last = drop_last\n        self.shuffle = shuffle\n        self.neg_num = neg_num\n        self.item_num = dataset.num_items\n        self.user_pos_dict = dataset.get_user_train_dict()\n\n        self.user_pos_len, self.users_list, self.pos_items_list = \\\n            _generate_positive_items(self.user_pos_dict)\n\n    def __iter__(self):\n        neg_items_list = _sampling_negative_items(self.user_pos_len, self.neg_num,\n                                                  self.item_num, self.user_pos_dict)\n\n        data_iter = DataIterator(self.users_list, self.pos_items_list, neg_items_list,\n                                 batch_size=self.batch_size,\n                                 shuffle=self.shuffle, drop_last=self.drop_last)\n        for bat_users, bat_pos_items, bat_neg_items in data_iter:\n            yield bat_users, bat_pos_items, bat_neg_items\n\n    def __len__(self):\n        n_sample = len(self.users_list)\n        if self.drop_last:\n            return n_sample // self.batch_size\n        else:\n            return (n_sample + self.batch_size - 1) // self.batch_size\n\n\nclass TimeOrderPointwiseSampler(Sampler):\n    """"""Sampling negative items and construct time ordered pointwise instances.\n\n    The training instances consist of `batch_user`, `batch_recent_items`,\n    `batch_item` and `batch_label`. For each instance, positive `label`\n    indicates that `user` interacts with `item` immediately following\n    `recent_items`; and negative `label` indicates that `item` does not\n    interact with `user`.\n\n    If `high_order == 1`, `batch_recent_items` is a list of items with length\n    `batch_size`; If `high_order > 1`, `batch_recent_items` is an array like\n    list with shape `(batch_size, high_order)`.\n    Positive and negative items are labeled as `1` and  `0`, respectively.\n    """"""\n\n    def __init__(self, dataset, high_order=1, neg_num=1, batch_size=1024, shuffle=True, drop_last=False):\n        """"""Initializes a new `TimeOrderPointwiseSampler` instance.\n\n        Args:\n            dataset (data.Dataset): An instance of `Dataset`.\n            high_order (int): The number of recent items. Defaults to `1`.\n            neg_num (int): How many negative items for each positive item.\n                Defaults to `1`.\n            batch_size (int): How many samples per batch to load.\n                Defaults to `1`.\n            shuffle (bool): Whether reshuffling the samples at every epoch.\n                Defaults to `False`.\n            drop_last (bool): Whether dropping the last incomplete batch.\n                Defaults to `False`.\n        """"""\n        super(TimeOrderPointwiseSampler, self).__init__()\n        if high_order < 0:\n            raise ValueError(""\'high_order\' must be a positive integer."")\n        if neg_num <= 0:\n            raise ValueError(""\'neg_num\' must be a positive integer."")\n\n        self.batch_size = batch_size\n        self.drop_last = drop_last\n        self.shuffle = shuffle\n        self.neg_num = neg_num\n        self.item_num = dataset.num_items\n        self.user_pos_dict = dataset.get_user_train_dict(by_time=True)\n\n        self.user_pos_len, users_list, recent_items_list, self.pos_items_list = \\\n            _generative_time_order_positive_items(self.user_pos_dict, high_order=high_order)\n\n        self.users_list = users_list * (self.neg_num + 1)\n        self.recent_items_list = recent_items_list * (self.neg_num + 1)\n\n        len_pos_items = len(self.pos_items_list)\n        pos_labels_list = [1.0] * len_pos_items\n        neg_labels_list = [0.0] * (len_pos_items * self.neg_num)\n        self.all_labels = pos_labels_list + neg_labels_list\n\n    def __iter__(self):\n        neg_items_list = _sampling_negative_items(self.user_pos_len, self.neg_num,\n                                                  self.item_num, self.user_pos_dict)\n\n        neg_items = np.array(neg_items_list, dtype=np.int32)\n        neg_items = np.reshape(neg_items.T, [-1]).tolist()\n        all_next_items = self.pos_items_list + neg_items\n\n        data_iter = DataIterator(self.users_list, self.recent_items_list, all_next_items, self.all_labels,\n                                 batch_size=self.batch_size, shuffle=self.shuffle, drop_last=self.drop_last)\n\n        for bat_users, bat_recent_items, bat_next_items, bat_labels in data_iter:\n            yield bat_users, bat_recent_items, bat_next_items, bat_labels\n\n    def __len__(self):\n        n_sample = len(self.users_list)\n        if self.drop_last:\n            return n_sample // self.batch_size\n        else:\n            return (n_sample + self.batch_size - 1) // self.batch_size\n\n\nclass TimeOrderPairwiseSampler(Sampler):\n    """"""Sampling negative items and construct time ordered pairwise instances.\n\n    The training instances consist of `batch_user`, `batch_recent_items`,\n    `batch_next_item` and `batch_neg_items`. For each instance, `user`\n    interacts with `next_item` immediately following `recent_items`, and\n    `neg_items` does not interact with `user`.\n\n    If `high_order == 1`, `batch_recent_items` is a list of items with length\n    `batch_size`; If `high_order > 1`, `batch_recent_items` is an array like\n    list with shape `(batch_size, high_order)`.\n\n    If `neg_num == 1`, `batch_neg_items` is a list of negative items with length\n    `batch_size`; If `neg_num > 1`, `batch_neg_items` is an array like list with\n    shape `(batch_size, neg_num)`.\n    """"""\n    def __init__(self, dataset, high_order=1, neg_num=1, batch_size=1024, shuffle=True, drop_last=False):\n        """"""Initializes a new `TimeOrderPairwiseSampler` instance.\n\n        Args:\n            dataset (data.Dataset): An instance of `Dataset`.\n            high_order (int): The number of recent items. Defaults to `1`.\n            neg_num (int): How many negative items for each positive item.\n                Defaults to `1`.\n            batch_size (int): How many samples per batch to load.\n                Defaults to `1`.\n            shuffle (bool): Whether reshuffling the samples at every epoch.\n                Defaults to `False`.\n            drop_last (bool): Whether dropping the last incomplete batch.\n                Defaults to `False`.\n        """"""\n        super(TimeOrderPairwiseSampler, self).__init__()\n        if high_order < 0:\n            raise ValueError(""\'high_order\' must be a positive integer."")\n        if neg_num <= 0:\n            raise ValueError(""\'neg_num\' must be a positive integer."")\n\n        self.batch_size = batch_size\n        self.drop_last = drop_last\n        self.shuffle = shuffle\n        self.neg_num = neg_num\n        self.item_num = dataset.num_items\n        self.user_pos_dict = dataset.get_user_train_dict(by_time=True)\n\n        self.user_pos_len, self.users_list, self.recent_items_list, self.pos_items_list = \\\n            _generative_time_order_positive_items(self.user_pos_dict, high_order=high_order)\n\n    def __iter__(self):\n        neg_items_list = _sampling_negative_items(self.user_pos_len, self.neg_num,\n                                                  self.item_num, self.user_pos_dict)\n\n        data_iter = DataIterator(self.users_list, self.recent_items_list, self.pos_items_list, neg_items_list,\n                                 batch_size=self.batch_size, shuffle=self.shuffle, drop_last=self.drop_last)\n\n        for bat_users, bat_recent_items, bat_pos_items, bat_neg_items in data_iter:\n            yield bat_users, bat_recent_items, bat_pos_items, bat_neg_items\n\n    def __len__(self):\n        n_sample = len(self.users_list)\n        if self.drop_last:\n            return n_sample // self.batch_size\n        else:\n            return (n_sample + self.batch_size - 1) // self.batch_size\n'"
data/utils.py,0,"b'""""""\n@author: Zhongchuan Sun\n""""""\nimport pandas as pd\nimport math\nimport hashlib\nimport os\n\n\ndef check_md5(file_name):\n    if not os.path.isfile(file_name):\n        raise FileNotFoundError(""There is not file named \'%s\'!"" % file_name)\n    with open(file_name, ""rb"") as fin:\n        bytes = fin.read()  # read file as bytes\n        readable_hash = hashlib.md5(bytes).hexdigest()\n\n    return readable_hash\n\n\ndef load_data(filename, sep, columns):\n    data = pd.read_csv(filename, sep=sep, header=None, names=columns)\n    return data\n\n\ndef filter_data(data, user_min=None, item_min=None):\n    data.dropna(how=""any"", inplace=True)\n    if item_min is not None and item_min > 0:\n        item_count = data[""item""].value_counts(sort=False)\n        filtered_idx = data[""item""].map(lambda x: item_count[x] >= item_min)\n        data = data[filtered_idx]\n\n    if user_min is not None and user_min > 0:\n        user_count = data[""user""].value_counts(sort=False)\n        filtered_idx = data[""user""].map(lambda x: user_count[x] >= user_min)\n        data = data[filtered_idx]\n    return data\n\n\ndef remap_id(data):\n    unique_user = data[""user""].unique()\n    user2id = pd.Series(data=range(len(unique_user)), index=unique_user)\n    data[""user""] = data[""user""].map(user2id)\n\n    unique_item = data[""item""].unique()\n    item2id = pd.Series(data=range(len(unique_item)), index=unique_item)\n    data[""item""] = data[""item""].map(item2id)\n\n    return data, user2id, item2id\n\n\ndef get_map_id(data):\n    unique_user = data[""user""].unique()\n    user2id = pd.Series(data=range(len(unique_user)), index=unique_user)\n\n    unique_item = data[""item""].unique()\n    item2id = pd.Series(data=range(len(unique_item)), index=unique_item)\n    return user2id.to_dict(), item2id.to_dict()\n\n\ndef split_by_ratio(data, ratio=0.8, by_time=True):\n    if by_time:\n        data.sort_values(by=[""user"", ""time""], inplace=True)\n    else:\n        data.sort_values(by=[""user"", ""item""], inplace=True)\n\n    first_section = []\n    second_section = []\n    user_grouped = data.groupby(by=[""user""])\n    for user, u_data in user_grouped:\n        u_data_len = len(u_data)\n        if not by_time:\n            u_data = u_data.sample(frac=1)\n        idx = math.ceil(ratio*u_data_len)\n        first_section.append(u_data.iloc[:idx])\n        second_section.append(u_data.iloc[idx:])\n\n    first_section = pd.concat(first_section, ignore_index=True)\n    second_section = pd.concat(second_section, ignore_index=True)\n\n    return first_section, second_section\n\n\ndef split_by_loo(data, by_time=True):\n    if by_time:\n        data.sort_values(by=[""user"", ""time""], inplace=True)\n    else:\n        data.sort_values(by=[""user"", ""item""], inplace=True)\n\n    first_section = []\n    second_section = []\n    user_grouped = data.groupby(by=[""user""])\n    for user, u_data in user_grouped:\n        u_data_len = len(u_data)\n        if u_data_len <= 3:\n            first_section.append(u_data)\n        else:\n            if not by_time:\n                u_data = u_data.sample(frac=1)\n            first_section.append(u_data.iloc[:-1])\n            second_section.append(u_data.iloc[-1:])\n\n    first_section = pd.concat(first_section, ignore_index=True)\n    second_section = pd.concat(second_section, ignore_index=True)\n\n    return first_section, second_section\n'"
evaluator/__init__.py,0,b'from .proxy_evaluator import ProxyEvaluator\n'
evaluator/abstract_evaluator.py,0,"b'""""""\r\n@author: Zhongchuan Sun\r\n""""""\r\n\r\n\r\nclass AbstractEvaluator(object):\r\n    """"""Base class for all evaluator.\r\n    """"""\r\n\r\n    def __init__(self):\r\n        pass\r\n\r\n    def metrics_info(self):\r\n        """"""Get all metrics information.\r\n\r\n        Returns:\r\n            str: A string consist of all metrics information\xef\xbc\x8c such as\r\n            `""Precision@10    Precision@20    NDCG@10    NDCG@20""`.\r\n        """"""\r\n        raise NotImplementedError\r\n\r\n    def evaluate(self, model):\r\n        """"""Evaluate `model`.\r\n\r\n        Args:\r\n            model: The model need to be evaluated. This model must have\r\n                a method `predict_for_eval(self, users)`, where the argument\r\n                `users` is a list of users and the return is a 2-D array that\r\n                contains `users` rating/ranking scores on all items.\r\n\r\n        Returns:\r\n            str: A string consist of all results, such as\r\n            `""0.18663847    0.11239596    0.35824192    0.21479650""`.\r\n        """"""\r\n        raise NotImplementedError\r\n'"
evaluator/grouped_evaluator.py,0,"b'""""""\r\n@author: Zhongchuan Sun\r\n""""""\r\nfrom util import typeassert\r\nimport numpy as np\r\nfrom collections import OrderedDict\r\nimport pandas as pd\r\nfrom .abstract_evaluator import AbstractEvaluator\r\nfrom .backend import UniEvaluator\r\n\r\n\r\nclass GroupedEvaluator(AbstractEvaluator):\r\n    """"""`GroupedEvaluator` evaluates models in user groups.\r\n\r\n    This class evaluates the ranking performance of models in user groups,\r\n    which are split according to the numbers of users\' interactions in\r\n    **training data**. This function can be activated by the argument\r\n    `group_view`, which must be a list of integers.\r\n    For example, if `group_view = [10,30,50,100]`, users will be split into\r\n    four groups: `(0, 10]`, `(10, 30]`, `(30, 50]` and `(50, 100]`. And the\r\n    users whose interacted items more than `100` will be discard.\r\n    """"""\r\n    @typeassert(user_train_dict=dict, user_test_dict=dict, group_view=list)\r\n    def __init__(self, user_train_dict, user_test_dict, user_neg_test=None,\r\n                 metric=None, group_view=None, top_k=50, batch_size=1024, num_thread=8):\r\n        """"""Initializes a new `GroupedEvaluator` instance.\r\n\r\n        Args:\r\n            user_train_dict (dict): Each key is user ID and the corresponding\r\n                value is the list of **training items**.\r\n            user_test_dict (dict): Each key is user ID and the corresponding\r\n                value is the list of **test items**.\r\n            metric (None or list of str): If `metric == None`, metric will\r\n                be set to `[""Precision"", ""Recall"", ""MAP"", ""NDCG"", ""MRR""]`.\r\n                Otherwise, `metric` must be one or a sublist of metrics\r\n                mentioned above. Defaults to `None`.\r\n            group_view (list of int): A list of integers.\r\n            top_k (int or list of int): `top_k` controls the Top-K item ranking\r\n                performance. If `top_k` is an integer, K ranges from `1` to\r\n                `top_k`; If `top_k` is a list of integers, K are only assigned\r\n                these values. Defaults to `50`.\r\n            batch_size (int): An integer to control the test batch size.\r\n                Defaults to `1024`.\r\n            num_thread (int): An integer to control the test thread number.\r\n                Defaults to `8`.\r\n\r\n        Raises:\r\n             TypeError: If `group_view` is not a list.\r\n             ValueError: If user splitting with `group_view` is not suitable.\r\n        """"""\r\n        super(GroupedEvaluator, self).__init__()\r\n\r\n        if not isinstance(group_view, list):\r\n            raise TypeError(""The type of \'group_view\' must be `list`!"")\r\n\r\n        self.evaluator = UniEvaluator(user_train_dict, user_test_dict, user_neg_test,\r\n                                      metric=metric, top_k=top_k,\r\n                                      batch_size=batch_size,\r\n                                      num_thread=num_thread)\r\n        self.user_pos_train = user_train_dict\r\n        self.user_pos_test = user_test_dict\r\n\r\n        group_list = [0] + group_view\r\n        group_info = [(""(%d,%d]:"" % (g_l, g_h)).ljust(12)\r\n                      for g_l, g_h in zip(group_list[:-1], group_list[1:])]\r\n\r\n        all_test_user = list(self.user_pos_test.keys())\r\n        num_interaction = [len(self.user_pos_train[u]) for u in all_test_user]\r\n        group_idx = np.searchsorted(group_list[1:], num_interaction)\r\n        user_group = pd.DataFrame(list(zip(all_test_user, group_idx)),\r\n                                  columns=[""user"", ""group""])\r\n        grouped = user_group.groupby(by=[""group""])\r\n\r\n        self.grouped_user = OrderedDict()\r\n        for idx, users in grouped:\r\n            if idx < len(group_info):\r\n                self.grouped_user[group_info[idx]] = users[""user""].tolist()\r\n\r\n        if not self.grouped_user:\r\n            raise ValueError(""The splitting of user groups is not suitable!"")\r\n\r\n    def metrics_info(self):\r\n        """"""Get all metrics information.\r\n\r\n        Returns:\r\n            str: A string consist of all metrics information\xef\xbc\x8c such as\r\n            `""Precision@10    Precision@20    NDCG@10    NDCG@20""`.\r\n        """"""\r\n        return self.evaluator.metrics_info()\r\n\r\n    def evaluate(self, model):\r\n        """"""Evaluate `model` in user groups.\r\n\r\n        Args:\r\n            model: The model need to be evaluated. This model must have\r\n                a method `predict_for_eval(self, users)`, where the argument\r\n                `users` is a list of users and the return is a 2-D array that\r\n                contains `users` rating/ranking scores on all items.\r\n\r\n        Returns:\r\n            str: A multi-line string consist of all results of groups, such as:\r\n                `""(0,10]:   0.00648002   0.00421617   0.00301847   0.00261693\\n\r\n                (10,30]:  0.00686600   0.00442968   0.00310077   0.00249169\\n\r\n                (30,50]:  0.00653595   0.00326797   0.00217865   0.00163399\\n\r\n                (50,100]: 0.00423729   0.00211864   0.00141243   0.00105932""`\r\n        """"""\r\n        result_to_show = """"\r\n        for group, users in self.grouped_user.items():\r\n            tmp_result = self.evaluator.evaluate(model, users)\r\n            result_to_show = ""%s\\n%s\\t%s"" % (result_to_show, group, tmp_result)\r\n\r\n        return result_to_show\r\n'"
evaluator/proxy_evaluator.py,0,"b'""""""\n@author: Zhongchuan Sun\n""""""\nfrom util import typeassert\nfrom .abstract_evaluator import AbstractEvaluator\nfrom .backend import UniEvaluator\nfrom .grouped_evaluator import GroupedEvaluator\n\n\nclass ProxyEvaluator(AbstractEvaluator):\n    """"""`ProxyEvaluator` is the interface to evaluate models.\n\n    `ProxyEvaluator` contains various evaluation protocols:\n\n    * **First**, evaluation metrics of this class are configurable via the\n      argument `metric`. Now there are five configurable metrics: `Precision`,\n      `Recall`, `MAP`, `NDCG` and `MRR`.\n\n    * **Second**, this class and its evaluation metrics can automatically fit\n      both leave-one-out and fold-out data splitting without specific indication.\n      In **leave-one-out** evaluation, 1) `Recall` is equal to `HitRatio`;\n      2) The implementation of `NDCG` is compatible with fold-out; 3) `MAP` and\n      `MRR` have same numeric values; 4) `Precision` is meaningless.\n\n    * **Furthermore**, the ranking performance of models can be viewed in user\n      groups, which are split according to the numbers of users\' interactions\n      in **training data**. This function can be activated by the argument\n      `group_view`. Specifically, if `group_view == None`, the ranking performance\n      will be viewed without groups; If `group_view` is a list of integers,\n      the ranking performance will be view in groups.\n      For example, if `group_view = [10,30,50,100]`, users will be split into\n      four groups: `(0, 10]`, `(10, 30]`, `(30, 50]` and `(50, 100]`. And the\n      users whose interacted items more than `100` will be discarded.\n\n    * **Finally and importantly**, all the functions mentioned above depend on\n      `UniEvaluator`, which is implemented by **python** and **cpp**.\n      And both of the two versions are **multi-threaded**.\n    """"""\n\n    @typeassert(user_train_dict=dict, user_test_dict=dict)\n    def __init__(self, user_train_dict, user_test_dict, user_neg_test=None, metric=None,\n                 group_view=None, top_k=50, batch_size=1024, num_thread=8):\n        """"""Initializes a new `ProxyEvaluator` instance.\n\n        Args:\n            user_train_dict (dict): Each key is user ID and the corresponding\n                value is the list of **training items**.\n            user_test_dict (dict): Each key is user ID and the corresponding\n                value is the list of **test items**.\n            metric (None or list of str): If `metric == None`, metric will\n                be set to `[""Precision"", ""Recall"", ""MAP"", ""NDCG"", ""MRR""]`.\n                Otherwise, `metric` must be one or a sublist of metrics\n                mentioned above. Defaults to `None`.\n            group_view (None or list of int): If `group_view == None`, the ranking\n                performance will be viewed without groups. If `group_view` is a\n                list of integers, ranking performance will be viewed in groups.\n                Defaults to `None`.\n            top_k (int or list of int): `top_k` controls the Top-K item ranking\n                performance. If `top_k` is an integer, K ranges from `1` to\n                `top_k`; If `top_k` is a list of integers, K are only assigned\n                these values. Defaults to `50`.\n            batch_size (int): An integer to control the test batch size.\n                Defaults to `1024`.\n            num_thread (int): An integer to control the test thread number.\n                Defaults to `8`.\n\n        Raises:\n            ValueError: If `metric` or one of its element is not in\n                `[""Precision"", ""Recall"", ""MAP"", ""NDCG"", ""MRR""]`.\n\n        TODO:\n            * Check the validation of `num_thread` in cpp implementation.\n        """"""\n        super(ProxyEvaluator, self).__init__()\n        if group_view is not None:\n            self.evaluator = GroupedEvaluator(user_train_dict, user_test_dict, user_neg_test,\n                                              metric=metric, group_view=group_view,\n                                              top_k=top_k, batch_size=batch_size,\n                                              num_thread=num_thread)\n        else:\n            self.evaluator = UniEvaluator(user_train_dict, user_test_dict, user_neg_test,\n                                          metric=metric, top_k=top_k,\n                                          batch_size=batch_size,\n                                          num_thread=num_thread)\n\n    def metrics_info(self):\n        """"""Get all metrics information.\n\n        Returns:\n            str: A string consist of all metrics information\xef\xbc\x8c such as\n                `""Precision@10    Precision@20    NDCG@10    NDCG@20""`.\n        """"""\n        return self.evaluator.metrics_info()\n\n    def evaluate(self, model):\n        """"""Evaluate `model`.\n\n        Args:\n            model: The model need to be evaluated. This model must have\n                a method `predict_for_eval(self, users)`, where the argument\n                `users` is a list of users and the return is a 2-D array that\n                contains `users` rating/ranking scores on all items.\n\n        Returns:\n            str: A string consist of all results, such as\n                `""0.18663847    0.11239596    0.35824192    0.21479650""`.\n        """"""\n        return self.evaluator.evaluate(model)\n'"
model/AbstractRecommender.py,0,"b'from evaluator import ProxyEvaluator\r\nimport pandas as pd\r\nimport numpy as np\r\nimport scipy.sparse as sp\r\nfrom util import Logger\r\nimport os\r\nimport time\r\n\r\ndef _create_logger(config, data_name):\r\n    # create a logger\r\n    timestamp = time.time()\r\n    param_str = ""%s_%s"" % (data_name, config.params_str())\r\n    run_id = ""%s_%.8f"" % (param_str[:150], timestamp)\r\n\r\n    model_name = config[""recommender""]\r\n    log_dir = os.path.join(""log"", data_name, model_name)\r\n    logger_name = os.path.join(log_dir, run_id + "".log"")\r\n    logger = Logger(logger_name)\r\n\r\n    return logger\r\n\r\n\r\nclass AbstractRecommender(object):\r\n    def __init__(self, dataset, conf):\r\n        self.evaluator = ProxyEvaluator(dataset.get_user_train_dict(),\r\n                                        dataset.get_user_test_dict(),\r\n                                        dataset.get_user_test_neg_dict(),\r\n                                        metric=conf[""metric""],\r\n                                        group_view=conf[""group_view""],\r\n                                        top_k=conf[""topk""],\r\n                                        batch_size=conf[""test_batch_size""],\r\n                                        num_thread=conf[""num_thread""])\r\n\r\n        self.logger = _create_logger(conf, dataset.dataset_name)\r\n        self.logger.info(dataset)\r\n        self.logger.info(conf)\r\n\r\n    def build_graph(self):\r\n        raise NotImplementedError\r\n\r\n    def train_model(self):\r\n        raise NotImplementedError\r\n    \r\n    def predict(self, user_ids, items):\r\n        raise NotImplementedError\r\n\r\n\r\nclass SeqAbstractRecommender(AbstractRecommender):\r\n    def __init__(self, dataset, conf):\r\n        if dataset.time_matrix is None:\r\n            raise ValueError(""Dataset does not contant time infomation!"")\r\n        super(SeqAbstractRecommender, self).__init__(dataset, conf)\r\n\r\n\r\nclass SocialAbstractRecommender(AbstractRecommender):\r\n    def __init__(self, dataset, conf):\r\n        super(SocialAbstractRecommender, self).__init__(dataset, conf)\r\n        social_users = pd.read_csv(conf[""social_file""], sep=conf[""data.convert.separator""],\r\n                                   header=None, names=[""user"", ""friend""])\r\n        users_key = np.array(list(dataset.userids.keys()))\r\n        index = np.in1d(social_users[""user""], users_key)\r\n        social_users = social_users[index]\r\n\r\n        index = np.in1d(social_users[""friend""], users_key)\r\n        social_users = social_users[index]\r\n\r\n        user = social_users[""user""]\r\n        user_id = [dataset.userids[u] for u in user]\r\n        friend = social_users[""friend""]\r\n        friend_id = [dataset.userids[u] for u in friend]\r\n        num_users, num_items = dataset.train_matrix.shape\r\n        self.social_matrix = sp.csr_matrix(([1] * len(user_id), (user_id, friend_id)),\r\n                                           shape=(num_users, num_users))\r\n'"
util/__init__.py,0,b'from util.configurator import Configurator\nfrom util.data_iterator import DataIterator\nfrom util.tool import randint_choice\nfrom util.tool import csr_to_user_dict\nfrom util.tool import typeassert\nfrom util.tool import argmax_top_k\nfrom util.tool import timer\nfrom util.tool import pad_sequences\nfrom util.tool import inner_product\n# from util.tool import batch_random_choice\nfrom util.tool import l2_loss\nfrom util.tool import log_loss\nfrom .logger import Logger\nfrom .cython.random_choice import batch_randint_choice'
util/configurator.py,0,"b'""""""\n@author: Zhongchuan Sun\n""""""\n\nimport os\nimport sys\nfrom configparser import ConfigParser\nfrom collections import OrderedDict\n\n\nclass Configurator(object):\n    """"""A configurator class.\n\n    This class can read arguments from ini-style configuration file and parse\n    arguments from command line simultaneously. This class can also convert\n    the argument value from `str` to `int`, `float`, `bool`, `list` and `None`\n    automatically. The priority of arguments from command line is higher than\n    that from configuration file. That is, if there are same argument name in\n    configuration file and command line, the value in the former will be\n    overwritten by that in the latter. Moreover:\n\n    * Command line: The format of arguments is ``--arg_name=arg_value``,\n      there cannot be any space in the inner of an argument string.\n      For example::\n\n        python main.py --model=Pop --num_thread=128 --group_view=[10,30,50,100]\n\n    * Configuration file: This file must be ini-style. If there is only one\n      section and whatever the name is, this class will read arguments from\n      that section. If there are more than one sections, this class will read\n      arguments from the section named `default_section`.\n\n    After initialization successful, the objective of this class can be used as\n    a dictionary::\n\n        config = Configurator(""./NeuRec.properties"")\n        num_thread = config[""num_thread""]\n        group_view = config[""group_view""]\n\n    Here, the types of `num_thread` and `group_view` are `int` and `list`,\n    respectively.\n    """"""\n\n    def __init__(self, config_file, default_section=""default""):\n        """"""Initializes a new `Configurator` instance.\n\n        Args:\n            config_file (str): The path of ini-style configuration file.\n            default_section (str): The default section if there are more than\n                one sections in configuration file.\n\n        Raises:\n             FileNotFoundError: If `config_file` is not existing.\n             SyntaxError: If the format of arguments in commend line is invalid.\n             ValueError: If there is more than one section but no one section\n                named `default_section` in ini-style file.\n        """"""\n        if not os.path.isfile(config_file):\n            raise FileNotFoundError(""There is not config file named \'%s\'!"" % config_file)\n\n        self._default_section = default_section\n        self.cmd_arg = self._read_cmd_arg()\n        self.lib_arg = self._read_config_file(config_file)\n        config_dir = self.lib_arg[""config_dir""]\n        model_name = self.lib_arg[""recommender""]\n        arg_file = os.path.join(config_dir, model_name+\'.properties\')\n        self.alg_arg = self._read_config_file(arg_file)\n\n    def _read_cmd_arg(self):\n        cmd_arg = OrderedDict()\n        if ""ipykernel_launcher"" not in sys.argv[0]:\n            for arg in sys.argv[1:]:\n                if not arg.startswith(""--""):\n                    raise SyntaxError(""Commend arg must start with \'--\', but \'%s\' is not!"" % arg)\n                arg_name, arg_value = arg[2:].split(""="")\n                cmd_arg[arg_name] = arg_value\n\n        return cmd_arg\n\n    def _read_config_file(self, filename):\n        config = ConfigParser()\n        config.optionxform = str\n        config.read(filename, encoding=""utf-8"")\n        sections = config.sections()\n\n        if len(sections) == 0:\n            raise ValueError(""\'%s\' is empty!"" % filename)\n        elif len(sections) == 1:\n            config_sec = sections[0]\n        elif self._default_section in sections:\n            config_sec = self._default_section\n        else:\n            raise ValueError(""\'%s\' has more than one sections but there is no ""\n                             ""section named \'%s\'"" % filename, self._default_section)\n\n        config_arg = OrderedDict(config[config_sec].items())\n        for arg in self.cmd_arg:\n            if arg in config_arg:\n                config_arg[arg] = self.cmd_arg[arg]\n\n        return config_arg\n\n    def params_str(self):\n        """"""Get a summary of parameters.\n\n        Returns:\n            str: A string summary of parameters.\n        """"""\n        params_id = \'_\'.join([""{}={}"".format(arg, value) for arg, value in self.alg_arg.items() if len(value) < 20])\n        special_char = {\'/\', \'\\\\\', \'\\""\', \':\', \'*\', \'?\', \'<\', \'>\', \'|\', \'\\t\'}\n        params_id = [c if c not in special_char else \'_\' for c in params_id]\n        params_id = \'\'.join(params_id)\n        params_id = ""%s_%s"" % (self[""recommender""], params_id)\n        return params_id\n\n    def __getitem__(self, item):\n        if not isinstance(item, str):\n            raise TypeError(""index must be a str"")\n\n        if item in self.lib_arg:\n            param = self.lib_arg[item]\n        elif item in self.alg_arg:\n            param = self.alg_arg[item]\n        elif item in self.cmd_arg:\n            param = self.cmd_arg[item]\n        else:\n            raise KeyError(""There are not the parameter named \'%s\'"" % item)\n\n        # convert param from str to value, i.e. int, float or list etc.\n        try:\n            value = eval(param)\n            if not isinstance(value, (str, int, float, list, tuple, bool, None.__class__)):\n                value = param\n        except:\n            if param.lower() == ""true"":\n                value = True\n            elif param.lower() == ""false"":\n                value = False\n            else:\n                value = param\n\n        return value\n\n    def __getattr__(self, item):\n        return self[item]\n\n    def __contains__(self, o):\n        return o in self.lib_arg or o in self.alg_arg or o in self.cmd_arg\n\n    def __str__(self):\n        lib_info = \'\\n\'.join([""{}={}"".format(arg, value) for arg, value in self.lib_arg.items()])\n        alg_info = \'\\n\'.join([""{}={}"".format(arg, value) for arg, value in self.alg_arg.items()])\n        info = ""\\n\\nNeuRec hyperparameters:\\n%s\\n\\n%s\'s hyperparameters:\\n%s\\n"" % (lib_info, self[""recommender""], alg_info)\n        return info\n\n    def __repr__(self):\n        return self.__str__()\n'"
util/data_generator.py,0,"b'import numpy as np\nfrom util.tool import randint_choice\n\n\ndef _get_pairwise_all_likefism_data(dataset):\n    user_input_pos, user_input_neg, num_idx_pos, num_idx_neg, item_input_pos, item_input_neg = [], [], [], [], [], []\n    num_items = dataset.num_items\n    num_users = dataset.num_users\n    train_matrix = dataset.train_matrix\n    for u in range(num_users):\n        items_by_u = train_matrix[u].indices.copy().tolist()\n        num_items_by_u = len(items_by_u)\n        if num_items_by_u > 1: \n            negative_items = randint_choice(num_items, num_items_by_u, replace=True, exclusion = items_by_u)\n        \n            for index, i in enumerate(items_by_u):\n                j = negative_items[index]\n                user_input_neg.append(items_by_u)\n                num_idx_neg.append(num_items_by_u)\n                item_input_neg.append(j)\n                \n                items_by_u.remove(i)\n                user_input_pos.append(items_by_u)\n                num_idx_pos.append(num_items_by_u-1)\n                item_input_pos.append(i)  \n                \n    return user_input_pos, user_input_neg, num_idx_pos, num_idx_neg, item_input_pos, item_input_neg\n\ndef _get_pointwise_all_likefism_data(dataset, num_negatives, train_dict):\n    user_input,num_idx,item_input,labels = [],[],[],[]\n    num_users = dataset.num_users\n    num_items = dataset.num_items\n    for u in range(num_users):\n        items_by_user = train_dict[u].copy()\n        items_set = set(items_by_user)\n        size = len(items_by_user)   \n        for i in items_by_user:\n            # negative instances\n            for _ in range(num_negatives):\n                j = np.random.randint(num_items)\n                while j in items_set:\n                    j = np.random.randint(num_items)\n                user_input.append(items_by_user)\n                item_input.append(j)\n                num_idx.append(size)\n                labels.append(0)\n            items_by_user.remove(i)\n            user_input.append(items_by_user)\n            item_input.append(i)\n            num_idx.append(size-1)\n            labels.append(1)\n    return user_input,num_idx,item_input,labels\n\ndef _get_pairwise_all_likefossil_data(dataset, high_order, train_dict):\n    user_input_id,user_input_pos,user_input_neg, num_idx_pos, num_idx_neg, item_input_pos,item_input_neg,item_input_recents = [],[], [], [],[],[],[],[]\n    for u in range(dataset.num_users):\n        items_by_user = train_dict[u].copy()\n        num_items_by_u = len(items_by_user)\n        if  num_items_by_u > high_order: \n            negative_items = randint_choice(dataset.num_items, num_items_by_u, replace=True, exclusion = items_by_user)\n            for idx in range(high_order,len(train_dict[u])):\n                i = train_dict[u][idx] # item id \n                item_input_recent = []\n                for t in range(1,high_order+1):\n                    item_input_recent.append(train_dict[u][idx-t])\n                item_input_recents.append(item_input_recent)\n                j = negative_items[idx]\n                user_input_neg.append(items_by_user)\n                num_idx_neg.append(num_items_by_u)\n                item_input_neg.append(j)\n                \n                items_by_user.remove(i)\n                user_input_id.append(u)\n                user_input_pos.append(items_by_user)\n                num_idx_pos.append(num_items_by_u-1)\n                item_input_pos.append(i)\n                \n    return user_input_id,user_input_pos,user_input_neg, num_idx_pos, num_idx_neg, item_input_pos,item_input_neg,item_input_recents\n\ndef _get_pointwise_all_likefossil_data(dataset, high_order, num_negatives, train_dict):\n    user_input_id,user_input,num_idx,item_input,item_input_recents,labels = [],[],[],[],[],[]\n    for u in range(dataset.num_users):\n        items_by_user = train_dict[u].copy()\n        items_set = set(items_by_user)\n        size = len(items_by_user)   \n        for idx in range(high_order,len(train_dict[u])):\n            i = train_dict[u][idx] # item id \n            item_input_recent = []\n            for t in range(1,high_order+1):\n                item_input_recent.append(train_dict[u][idx-t])\n            # negative instances\n            for _ in range(num_negatives):\n                j = np.random.randint(dataset.num_items)\n                while j in items_set:\n                    j = np.random.randint(dataset.num_items)\n                user_input_id.append(u)\n                user_input.append(items_by_user)\n                item_input_recents.append(item_input_recent)\n                item_input.append(j)\n                num_idx.append(size)\n                labels.append(0)\n            items_by_user.remove(i)\n            user_input.append(items_by_user)\n            user_input_id.append(u)\n            item_input_recents.append(item_input_recent)\n            item_input.append(i)\n            num_idx.append(size-1)\n            labels.append(1)\n    return user_input_id,user_input,num_idx,item_input,item_input_recents,labels'"
util/data_iterator.py,0,"b'""""""\n@author: Zhongchuan Sun\n""""""\nimport numpy as np\n\n\nclass Sampler(object):\n    """"""Base class for all Samplers.\n\n    Every Sampler subclass has to provide an __iter__ method, providing a way\n    to iterate over indices of dataset elements, and a __len__ method that\n    returns the length of the returned iterators.\n    """"""\n\n    def __init__(self):\n        pass\n\n    def __iter__(self):\n        raise NotImplementedError\n\n    def __len__(self):\n        raise NotImplementedError\n\n\nclass SequentialSampler(Sampler):\n    """"""Samples elements sequentially, always in the same order.\n    """"""\n\n    def __init__(self, data_source):\n        """"""Initializes a new `SequentialSampler` instance.\n\n        Args:\n            data_source (_Dataset): Dataset to sample from.\n        """"""\n        super(SequentialSampler, self).__init__()\n        self.data_source = data_source\n\n    def __iter__(self):\n        return iter(range(len(self.data_source)))\n\n    def __len__(self):\n        return len(self.data_source)\n\n\nclass RandomSampler(Sampler):\n    """"""Samples elements randomly, without replacement.\n    """"""\n\n    def __init__(self, data_source):\n        """"""Initializes a new `SequentialSampler` instance.\n\n        Args:\n            data_source (_Dataset): Dataset to sample from.\n        """"""\n        super(RandomSampler, self).__init__()\n        self.data_source = data_source\n\n    def __iter__(self):\n        perm = np.random.permutation(len(self.data_source)).tolist()\n        return iter(perm)\n\n    def __len__(self):\n        return len(self.data_source)\n\n\nclass BatchSampler(Sampler):\n    """"""Wraps another sampler to yield a mini-batch of indices.\n    """"""\n\n    def __init__(self, sampler, batch_size, drop_last):\n        """"""Initializes a new `BatchSampler` instance.\n\n        Args:\n            sampler (Sampler): Base sampler.\n            batch_size (int): Size of mini-batch.\n            drop_last (bool): If `True`, the sampler will drop the last batch\n                if its size would be less than `batch_size`.\n        """"""\n        super(BatchSampler, self).__init__()\n        if not isinstance(sampler, Sampler):\n            raise ValueError(""sampler should be an instance of ""\n                             ""torch.utils.data.Sampler, but got sampler={}""\n                             .format(sampler))\n        if not isinstance(batch_size, int) or isinstance(batch_size, bool) or \\\n                batch_size <= 0:\n            raise ValueError(""batch_size should be a positive integeral value, ""\n                             ""but got batch_size={}"".format(batch_size))\n        if not isinstance(drop_last, bool):\n            raise ValueError(""drop_last should be a boolean value, but got ""\n                             ""drop_last={}"".format(drop_last))\n        self.sampler = sampler\n        self.batch_size = batch_size\n        self.drop_last = drop_last\n\n    def __iter__(self):\n        batch = []\n        for idx in self.sampler:\n            batch.append(idx)\n            if len(batch) == self.batch_size:\n                yield batch\n                batch = []\n        if len(batch) > 0 and not self.drop_last:\n            yield batch\n\n    def __len__(self):\n        if self.drop_last:\n            return len(self.sampler) // self.batch_size\n        else:\n            return (len(self.sampler) + self.batch_size - 1) // self.batch_size\n\n\nclass _Dataset(object):\n    """"""Pack the given data to one dataset.\n\n    Args:\n        data (list or tuple): a list of \'data\'.\n    """"""\n\n    def __init__(self, data):\n        for d in data:\n            if len(d) != len(data[0]):\n                raise ValueError(""The length of the given data are not equal!"")\n            # assert len(d) == len(data[0])\n        self.data = data\n\n    def __len__(self):\n        return len(self.data[0])\n\n    def __getitem__(self, idx):\n        return [data[idx] for data in self.data]\n\n\nclass _DataLoaderIter(object):\n    """"""Iterates once over the dataset, as specified by the sampler.\n    """"""\n\n    def __init__(self, loader):\n        self.dataset = loader.dataset\n        self.batch_sampler = loader.batch_sampler\n        self.sample_iter = iter(self.batch_sampler)\n\n    def __len__(self):\n        return len(self.batch_sampler)\n\n    def __next__(self):\n        indices = next(self.sample_iter)  # may raise StopIteration\n        batch = [self.dataset[i] for i in indices]\n\n        transposed = [list(samples) for samples in zip(*batch)]\n        if len(transposed) == 1:\n            transposed = transposed[0]\n        return transposed\n\n    def __iter__(self):\n        return self\n\n\nclass DataIterator(object):\n    """"""`DataIterator` provides iterators over the dataset.\n\n    This class combines some data sets and provides a batch iterator over them.\n    For example::\n\n        users = list(range(10))\n        items = list(range(10, 20))\n        labels = list(range(20, 30))\n\n        data_iter = DataIterator(users, items, labels, batch_size=4, shuffle=False)\n        for bat_user, bat_item, bat_label in data_iter:\n            print(bat_user, bat_item, bat_label)\n\n        data_iter = DataIterator(users, items, batch_size=4, shuffle=True, drop_last=True)\n        for bat_user, bat_item in data_iter:\n            print(bat_user, bat_item)\n\n    """"""\n\n    def __init__(self, *data, batch_size=1, shuffle=False, drop_last=False):\n        """"""\n        Args:\n            *data: Variable length data list.\n            batch_size (int): How many samples per batch to load. Defaults to `1`.\n            shuffle (bool): Set to `True` to have the data reshuffled at every\n                epoch. Defaults to `False`.\n            drop_last (bool): Set to `True` to drop the last incomplete batch,\n                if the dataset size is not divisible by the batch size.\n                If `False` and the size of dataset is not divisible by the\n                batch size, then the last batch will be smaller.\n                Defaults to `False`.\n\n        Raises:\n            ValueError: If the length of the given data are not equal.\n        """"""\n        dataset = _Dataset(list(data))\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.drop_last = drop_last\n\n        if shuffle:\n            sampler = RandomSampler(dataset)\n        else:\n            sampler = SequentialSampler(dataset)\n\n        self.batch_sampler = BatchSampler(sampler, batch_size, drop_last)\n\n    def __iter__(self):\n        return _DataLoaderIter(self)\n\n    def __len__(self):\n        return len(self.batch_sampler)\n\n\nif __name__ == ""__main__"":\n    users = list(range(10))\n    items = list(range(10, 20))\n    labels = list(range(20, 30))\n\n    data_iter = DataIterator(users, items, labels, batch_size=4, shuffle=False)\n    for bat_user, bat_item, bat_label in data_iter:\n        print(bat_user, bat_item, bat_label)\n\n    data_iter = DataIterator(users, items, batch_size=4, shuffle=True, drop_last=True)\n    for bat_user, bat_item in data_iter:\n        print(bat_user, bat_item)\n'"
util/learner.py,12,"b'import tensorflow as tf\r\ndef optimizer(learner,loss,learning_rate,momentum=0.9):\r\n    optimizer=None\r\n    if learner.lower() == ""adagrad"": \r\n        optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate,\\\r\n                     initial_accumulator_value=1e-8).minimize(loss)\r\n    elif learner.lower() == ""rmsprop"":\r\n        optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\r\n    elif learner.lower() == ""adam"":\r\n        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\r\n    elif learner.lower() == ""gd"" :\r\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)  \r\n    elif learner.lower() == ""momentum"" :\r\n        optimizer = tf.train.MomentumOptimizer(learning_rate,momentum).minimize(loss)  \r\n    else :\r\n        raise ValueError(""please select a suitable optimizer"")  \r\n    return optimizer\r\n\r\ndef pairwise_loss(loss_function,y,margin=1):\r\n    loss=None\r\n    if loss_function.lower() == ""bpr"":\r\n        loss = -tf.reduce_sum(tf.log_sigmoid(y))\r\n    elif loss_function.lower() == ""hinge"":\r\n        loss = tf.reduce_sum(tf.maximum(y+margin, 0))\r\n    elif loss_function.lower() == ""square"":  \r\n        loss = tf.reduce_sum(tf.square(1-y))   \r\n    else:\r\n        raise Exception(""please choose a suitable loss function"")\r\n    return loss\r\n\r\ndef pointwise_loss(loss_function,y_rea,y_pre):\r\n    loss=None\r\n    if loss_function.lower() == ""cross_entropy"":\r\n        loss = tf.losses.sigmoid_cross_entropy(y_rea,y_pre)\r\n#         loss = - tf.reduce_sum(\r\n#             y_rea * tf.log(y_pre) + (1 - y_rea) * tf.log(1 - y_pre)) \r\n    elif loss_function.lower() == ""square"":  \r\n        loss = tf.reduce_sum(tf.square(y_rea-y_pre))  \r\n    else:\r\n        raise Exception(""please choose a suitable loss function"") \r\n    return loss\r\n'"
util/logger.py,0,"b'""""""\n@author: Zhongchuan Sun\n""""""\nimport sys\nimport os\nimport logging\nfrom util import Configurator\n\n\nclass Logger(object):\n    """"""`Logger` is a simple encapsulation of python logger.\n\n    This class can show a message on standard output and write it into the\n    file named `filename` simultaneously. This is convenient for observing\n    and saving training results.\n    """"""\n\n    def __init__(self, filename):\n        """"""Initializes a new `Logger` instance.\n\n        Args:\n            filename (str): File name to create. The directory component of this\n                file will be created automatically if it is not existing.\n        """"""\n        dir_name = os.path.dirname(filename)\n        if not os.path.exists(dir_name):\n            os.makedirs(dir_name)\n\n        self.logger = logging.getLogger(filename)\n        self.logger.setLevel(logging.DEBUG)\n        formatter = logging.Formatter(\'%(asctime)s.%(msecs)03d: %(message)s\',\n                                      datefmt=\'%Y-%m-%d %H:%M:%S\')\n\n        # write into file\n        fh = logging.FileHandler(filename)\n        fh.setLevel(logging.DEBUG)\n        fh.setFormatter(formatter)\n\n        # show on console\n        ch = logging.StreamHandler(sys.stdout)\n        ch.setLevel(logging.DEBUG)\n        ch.setFormatter(formatter)\n\n        # add to Handler\n        self.logger.addHandler(fh)\n        self.logger.addHandler(ch)\n\n    def _flush(self):\n        for handler in self.logger.handlers:\n            handler.flush()\n\n    def debug(self, message):\n        self.logger.debug(message)\n        self._flush()\n\n    def info(self, message):\n        self.logger.info(message)\n        self._flush()\n\n    def warning(self, message):\n        self.logger.warning(message)\n        self._flush()\n\n    def error(self, message):\n        self.logger.error(message)\n        self._flush()\n\n    def critical(self, message):\n        self.logger.critical(message)\n        self._flush()\n\n\nif __name__ == \'__main__\':\n    log = Logger(\'NeuRec_test.log\')\n    log.debug(\'debug\')\n    log.info(\'info\')\n    log.warning(\'warning\')\n    log.error(\'error\')\n    log.critical(\'critical\')\n\n'"
util/tool.py,20,"b'import tensorflow as tf\nimport numpy as np\nfrom inspect import signature\nfrom functools import wraps\nimport heapq\nimport itertools\nimport time\n\n\ndef activation_function(act,act_input):\n        act_func = None\n        if act == ""sigmoid"":\n            act_func = tf.nn.sigmoid(act_input)\n        elif act == ""tanh"":\n            act_func = tf.nn.tanh(act_input)\n            \n        elif act == ""relu"":\n            act_func = tf.nn.relu(act_input)\n        \n        elif act == ""elu"":\n            act_func = tf.nn.elu(act_input)\n           \n        elif act == ""identity"":\n            act_func = tf.identity(act_input)\n            \n        elif act == ""softmax"":\n            act_func = tf.nn.softmax(act_input)\n         \n        elif act == ""selu"":\n            act_func = tf.nn.selu(act_input) \n        \n        else:\n            raise NotImplementedError(""ERROR"")\n        return act_func  \n\n\ndef get_data_format(data_format):\n    if data_format == ""UIRT"":\n        columns = [""user"", ""item"", ""rating"", ""time""]\n        \n    elif data_format == ""UIR"":\n        columns = [""user"", ""item"", ""rating""]\n    \n    elif data_format == ""UIT"":\n        columns = [""user"", ""item"", ""time""] \n        \n    elif data_format == ""UI"":\n        columns = [""user"", ""item""]    \n    \n    else:\n        raise ValueError(""please choose a correct data format. "")\n    \n    return columns\n\n\ndef csr_to_user_dict(train_matrix):\n    """"""convert a scipy.sparse.csr_matrix to a dict,\n    where the key is row number, and value is the\n    non-empty index in each row.\n    """"""\n    train_dict = {}\n    for idx, value in enumerate(train_matrix):\n        if any(value.indices):\n            train_dict[idx] = value.indices.copy().tolist()\n    return train_dict\n\n\ndef csr_to_user_dict_bytime(time_matrix,train_matrix):\n    train_dict = {}\n    time_matrix = time_matrix\n    user_pos_items = csr_to_user_dict(train_matrix)\n    for u, items in user_pos_items.items():\n        sorted_items = sorted(items, key=lambda x: time_matrix[u,x])\n        train_dict[u] = np.array(sorted_items, dtype=np.int32).tolist()\n\n    return train_dict\n\n\ndef get_initializer(init_method, stddev):\n        if init_method == \'tnormal\':\n            return tf.truncated_normal_initializer(stddev=stddev)\n        elif init_method == \'uniform\':\n            return tf.random_uniform_initializer(-stddev, stddev)\n        elif init_method == \'normal\':\n            return tf.random_normal_initializer(stddev=stddev)\n        elif init_method == \'xavier_normal\':\n            return tf.contrib.layers.xavier_initializer(uniform=False)\n        elif init_method == \'xavier_uniform\':\n            return tf.contrib.layers.xavier_initializer(uniform=True)\n        elif init_method == \'he_normal\':\n            return tf.contrib.layers.variance_scaling_initializer(\n                factor=2.0, mode=\'FAN_IN\', uniform=False)\n        elif init_method == \'he_uniform\':\n            return tf.contrib.layers.variance_scaling_initializer(\n                factor=2.0, mode=\'FAN_IN\', uniform=True)\n        else:\n            return tf.truncated_normal_initializer(stddev=stddev)  \n\n\ndef noise_validator(noise, allowed_noises):\n    \'\'\'Validates the noise provided\'\'\'\n    try:\n        if noise in allowed_noises:\n            return True\n        elif noise.split(\'-\')[0] == \'mask\' and float(noise.split(\'-\')[1]):\n            t = float(noise.split(\'-\')[1])\n            if t >= 0.0 and t <= 1.0:\n                return True\n            else:\n                return False\n    except:\n        return False\n    pass \n\n\ndef randint_choice(high, size=None, replace=True, p=None, exclusion=None):\n    """"""Return random integers from `0` (inclusive) to `high` (exclusive).\n    """"""\n    a = np.arange(high)\n    if exclusion is not None:\n        if p is None:\n            p = np.ones_like(a)\n        else:\n            p = np.array(p, copy=True)\n        p = p.flatten()\n        p[exclusion] = 0\n        p = p / np.sum(p)\n    sample = np.random.choice(a, size=size, replace=replace, p=p)\n    return sample\n\n\ndef typeassert(*type_args, **type_kwargs):\n    def decorate(func):\n        sig = signature(func)\n        bound_types = sig.bind_partial(*type_args, **type_kwargs).arguments\n\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            bound_values = sig.bind(*args, **kwargs)\n            for name, value in bound_values.arguments.items():\n                if name in bound_types:\n                    if not isinstance(value, bound_types[name]):\n                        raise TypeError(\'Argument {} must be {}\'.format(name, bound_types[name]))\n            return func(*args, **kwargs)\n        return wrapper\n    return decorate\n\n\ndef argmax_top_k(a, top_k=50):\n    ele_idx = heapq.nlargest(top_k, zip(a, itertools.count()))\n    return np.array([idx for ele, idx in ele_idx], dtype=np.intc)\n\n\ndef pad_sequences(sequences, value=0., max_len=None,\n                  padding=\'post\', truncating=\'post\', dtype=np.int32):\n    """"""Pads sequences to the same length.\n\n    Args:\n        sequences (list): A list of lists, where each element is a sequence.\n        value (int or float): Padding value. Defaults to `0.`.\n        max_len (int or None): Maximum length of all sequences.\n        padding (str): `""pre""` or `""post""`: pad either before or after each\n            sequence. Defaults to `post`.\n        truncating (str): `""pre""` or `""post""`: remove values from sequences\n            larger than `max_len`, either at the beginning or at the end of\n            the sequences. Defaults to `post`.\n        dtype (int or float): Type of the output sequences. Defaults to `np.int32`.\n\n    Returns:\n        np.ndarray: Numpy array with shape `(len(sequences), max_len)`.\n\n    Raises:\n        ValueError: If `padding` or `truncating` is not understood.\n    """"""\n    if max_len is None:\n        max_len = np.max([len(x) for x in sequences])\n\n    x = np.full([len(sequences), max_len], value, dtype=dtype)\n    for idx, s in enumerate(sequences):\n        if not len(s):\n            continue  # empty list/array was found\n        if truncating == \'pre\':\n            trunc = s[-max_len:]\n        elif truncating == \'post\':\n            trunc = s[:max_len]\n        else:\n            raise ValueError(\'Truncating type ""%s"" not understood\' % truncating)\n\n        if padding == \'post\':\n            x[idx, :len(trunc)] = trunc\n        elif padding == \'pre\':\n            x[idx, -len(trunc):] = trunc\n        else:\n            raise ValueError(\'Padding type ""%s"" not understood\' % padding)\n    return x\n\n\ndef inner_product(a, b, name=""inner_product""):\n    with tf.name_scope(name=name):\n        return tf.reduce_sum(tf.multiply(a, b), axis=-1)\n\n\ndef timer(func):\n    """"""The timer decorator\n    """"""\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time = time.time()\n        result = func(*args, **kwargs)\n        end_time = time.time()\n        print(""%s function cost: %fs"" % (func.__name__, end_time - start_time))\n        return result\n    return wrapper\n\n\ndef l2_loss(*params):\n    return tf.add_n([tf.nn.l2_loss(w) for w in params])\n\n\ndef log_loss(yij, name=""log_loss""):\n    """""" bpr loss\n    """"""\n    with tf.name_scope(name):\n        return -tf.log_sigmoid(yij)\n'"
evaluator/backend/__init__.py,0,"b'try:\n    from evaluator.backend.cpp.uni_evaluator import UniEvaluator\n    print(""Evaluate model with cpp"")\nexcept:\n    from evaluator.backend.python.uni_evaluator import UniEvaluator\n    print(""Evaluate model with python"")\n'"
model/general_recommender/APR.py,39,"b'""""""\nReference: Xiangnan He, et al., Adversarial Personalized Ranking for Recommendation"" in SIGIR2018\n@author: wubin\n""""""\nimport tensorflow as tf\nimport numpy as np\nfrom time import time\nfrom util import learner, tool\nfrom model.AbstractRecommender import AbstractRecommender\nfrom util import timer\nfrom util import l2_loss\nfrom data import PairwiseSampler\n\n\nclass APR(AbstractRecommender):\n    def __init__(self, sess, dataset, conf):  \n        super(APR, self).__init__(dataset, conf)\n        self.learning_rate = conf[""learning_rate""]\n        self.embedding_size = conf[""embedding_size""]\n        self.learner = conf[""learner""]\n        self.num_epochs = conf[""epochs""]\n        self.eps = conf[""eps""]\n        self.adv = conf[""adv""]\n        self.adver = conf[""adver""]\n        self.adv_epoch = conf[""adv_epoch""]\n        self.reg = conf[""reg""]\n        self.reg_adv = conf[""reg_adv""]\n        self.batch_size = conf[""batch_size""]\n        self.init_method = conf[""init_method""]\n        self.stddev = conf[""stddev""]\n        self.verbose = conf[""verbose""]\n        self.dataset = dataset\n        self.num_users = dataset.num_users\n        self.num_items = dataset.num_items\n        self.sess = sess\n    \n    def _create_placeholders(self):\n        with tf.name_scope(""input_data""):\n            self.user_input = tf.placeholder(tf.int32, shape=[None,], name=""user_input"")\n            self.item_input_pos = tf.placeholder(tf.int32, shape=[None,], name=""item_input_pos"")\n            self.item_input_neg = tf.placeholder(tf.int32, shape=[None,], name=""item_input_neg"")\n            \n    def _create_variables(self):\n        with tf.name_scope(""embedding""):\n            initializer = tool.get_initializer(self.init_method, self.stddev)\n            self.embedding_P = tf.Variable(initializer([self.num_users, self.embedding_size]),\n                                           name=\'embedding_P\', dtype=tf.float32)  # (users, embedding_size)\n            self.embedding_Q = tf.Variable(initializer([self.num_items, self.embedding_size]),\n                                           name=\'embedding_Q\', dtype=tf.float32)  # (items, embedding_size)\n\n            self.delta_P = tf.Variable(tf.zeros(shape=[self.num_users, self.embedding_size]),\n                                       name=\'delta_P\', dtype=tf.float32, trainable=False)  # (users, embedding_size)\n            self.delta_Q = tf.Variable(tf.zeros(shape=[self.num_items, self.embedding_size]),\n                                       name=\'delta_Q\', dtype=tf.float32, trainable=False)  # (items, embedding_size)\n\n    def _create_inference(self, item_input):\n        with tf.name_scope(""inference""):\n            # embedding look up\n            self.embedding_p = tf.nn.embedding_lookup(self.embedding_P, self.user_input)\n            self.embedding_q = tf.nn.embedding_lookup(self.embedding_Q, item_input)  # (b, embedding_size)\n            return tf.reduce_sum(self.embedding_p * self.embedding_q,1) # (b, embedding_size) * (embedding_size, 1)\n    \n    def _create_inference_adv(self, item_input):\n        with tf.name_scope(""inference_adv""):\n            # embedding look up\n            self.embedding_p = tf.nn.embedding_lookup(self.embedding_P, self.user_input)\n            self.embedding_q = tf.nn.embedding_lookup(self.embedding_Q, item_input)  # (b, embedding_size)\n            # add adversarial noise\n            self.P_plus_delta = self.embedding_p + tf.nn.embedding_lookup(self.delta_P, self.user_input)\n            self.Q_plus_delta = self.embedding_q + tf.nn.embedding_lookup(self.delta_Q, item_input)\n            return tf.reduce_sum(self.P_plus_delta * self.Q_plus_delta,1) # (b, embedding_size) * (embedding_size, 1)\n    \n    def _create_loss(self):\n        with tf.name_scope(""loss""):\n            # loss for L(Theta)\n            self.output = self._create_inference(self.item_input_pos)\n            self.output_neg = self._create_inference(self.item_input_neg)\n            self.result = self.output - self.output_neg\n            # self.loss = tf.reduce_sum(tf.log(1 + tf.exp(-self.result))) # this is numerically unstable\n            self.loss = tf.reduce_sum(tf.nn.softplus(-self.result))\n\n            # loss to be omptimized\n            self.opt_loss = self.loss + self.reg * l2_loss(self.embedding_P, self.embedding_Q)\n\n            if self.adver:\n                # loss for L(Theta + adv_Delta)\n                self.output_adv = self._create_inference_adv(self.item_input_pos)\n                self.output_neg_adv = self._create_inference_adv(self.item_input_neg)\n                self.result_adv = self.output_adv - self.output_neg_adv\n                # self.loss_adv = tf.reduce_sum(tf.log(1 + tf.exp(-self.result_adv)))\n                self.loss_adv = tf.reduce_sum(tf.nn.softplus(-self.result_adv))\n                self.opt_loss += self.reg_adv * self.loss_adv\n\n    def _create_adversarial(self):\n        with tf.name_scope(""adversarial""):\n            # generate the adversarial weights by random method\n            if self.adv == ""random"":\n                # generation\n                self.adv_P = tf.truncated_normal(shape=[self.num_users, self.embedding_size], mean=0.0, stddev=0.01)\n                self.adv_Q = tf.truncated_normal(shape=[self.num_items, self.embedding_size], mean=0.0, stddev=0.01)\n\n                # normalization and multiply epsilon\n                self.update_P = self.delta_P.assign(tf.nn.l2_normalize(self.adv_P, 1) * self.eps)\n                self.update_Q = self.delta_Q.assign(tf.nn.l2_normalize(self.adv_Q, 1) * self.eps)\n\n            # generate the adversarial weights by gradient-based method\n            elif self.adv == ""grad"":\n                # return the IndexedSlice Data: [(values, indices, dense_shape)]\n                # grad_var_P: [grad,var], grad_var_Q: [grad, var]\n                self.grad_P, self.grad_Q = tf.gradients(self.loss, [self.embedding_P, self.embedding_Q])\n\n                # convert the IndexedSlice Data to Dense Tensor\n                self.grad_P_dense = tf.stop_gradient(self.grad_P)\n                self.grad_Q_dense = tf.stop_gradient(self.grad_Q)\n\n                # normalization: new_grad = (grad / |grad|) * eps\n                self.update_P = self.delta_P.assign(tf.nn.l2_normalize(self.grad_P_dense, 1) * self.eps)\n                self.update_Q = self.delta_Q.assign(tf.nn.l2_normalize(self.grad_Q_dense, 1) * self.eps)\n\n    def _create_optimizer(self):\n        with tf.name_scope(""learner""):\n            self.optimizer = learner.optimizer(self.learner, self.loss, self.learning_rate)\n    \n    def build_graph(self):\n        self._create_placeholders()\n        self._create_variables()\n        self._create_loss()\n        self._create_optimizer()\n        self._create_adversarial()\n\n    # ---------- training process -------\n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n        data_iter = PairwiseSampler(self.dataset, neg_num=1, batch_size=self.batch_size, shuffle=True)\n        for epoch in range(1, self.num_epochs+1):\n            # Generate training instances\n            total_loss = 0.0\n            training_start_time = time()\n            num_training_instances = len(data_iter)\n            for bat_users, bat_items_pos, bat_items_neg in data_iter:\n                    feed_dict = {self.user_input: bat_users,\n                                 self.item_input_pos: bat_items_pos,\n                                 self.item_input_neg: bat_items_neg}\n                    loss, _ = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n                    total_loss += loss\n\n            self.logger.info(""[iter %d : loss : %f, time: %f]"" % (epoch, total_loss/num_training_instances,\n                                                             time()-training_start_time))\n            if epoch % self.verbose == 0:\n                self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate()))\n                \n    @timer\n    def evaluate(self):\n        self._cur_user_embeddings, self._cur_item_embeddings = self.sess.run([self.embedding_P, self.embedding_Q])\n        return self.evaluator.evaluate(self)\n\n    def predict(self, user_ids, candidate_items_userids):\n        if candidate_items_userids is None:\n            user_embed = self._cur_user_embeddings[user_ids]\n            ratings = np.matmul(user_embed, self._cur_item_embeddings.T)\n        else:\n            ratings = []\n            for userid, items_by_userid in zip(user_ids, candidate_items_userids):\n                user_embed = self._cur_user_embeddings[userid]\n                items_embed = self._cur_item_embeddings[items_by_userid]\n                ratings.append(np.squeeze(np.matmul(user_embed, items_embed.T)))\n            \n        return ratings\n'"
model/general_recommender/CDAE.py,18,"b'""""""\nReference: Wu, Yao, et al. ""Collaborative denoising auto-encoders for top-n recommender systems."" in WSDM2016\n@author: wubin\n""""""\nfrom model.AbstractRecommender import AbstractRecommender\nimport tensorflow as tf\nimport numpy as np\nfrom time import time\nfrom util import learner, tool\nfrom util import timer\nfrom util import l2_loss\nfrom util.data_iterator import DataIterator\n\n\nclass CDAE(AbstractRecommender):\n    def __init__(self, sess, dataset, conf):  \n        super(CDAE, self).__init__(dataset, conf)\n        self.hidden_neuron = conf[""hidden_neuron""]\n        self.learning_rate = conf[""learning_rate""]\n        self.learner = conf[""learner""]\n        self.reg = conf[""reg""]\n        self.num_epochs = conf[""epochs""]\n        self.batch_size = conf[""batch_size""]\n        self.verbose = conf[""verbose""]\n        self.h_act = conf[""h_act""]\n        self.g_act = conf[""g_act""]\n        self.corruption_level = conf[""corruption_level""]\n        self.init_method = conf[""init_method""]\n        self.stddev = conf[""stddev""]\n        self.num_users = dataset.num_users\n        self.num_items = dataset.num_items\n        self.train_matrix = dataset.to_csr_matrix()\n        self.train_dict = dataset.get_user_train_dict()\n        self.sess = sess\n        \n    def _create_placeholders(self):\n        with tf.name_scope(""input_data""):\n            self.user_input = tf.placeholder(tf.int32, shape=[None,],name = \'user_input\')\n            self.input_R = tf.placeholder(tf.float32, [None, self.num_items])\n            self.mask_corruption = tf.placeholder(tf.float32, [None, self.num_items])\n            \n    def _create_variables(self):\n        with tf.name_scope(""embedding""):  # The embedding initialization is unknown now\n            initializer = tool.get_initializer(self.init_method, self.stddev)\n            self.V = tf.Variable(initializer([self.num_users, self.hidden_neuron]))\n             \n            self.weights = {\'encoder\': tf.Variable(initializer([self.num_items, self.hidden_neuron])),\n                            \'decoder\': tf.Variable(initializer([self.hidden_neuron, self.num_items]))}\n            self.biases = {\'encoder\': tf.Variable(initializer([self.hidden_neuron])),\n                           \'decoder\': tf.Variable(initializer([self.num_items]))}\n            \n    def _create_inference(self):\n        with tf.name_scope(""inference""):\n            \n            self.user_latent = tf.nn.embedding_lookup(self.V, self.user_input)\n            \n            corrupted_input = tf.multiply(self.input_R, self.mask_corruption)\n            encoder_op = tool.activation_function(self.h_act,\n                                                  tf.matmul(corrupted_input, self.weights[\'encoder\'])\n                                                  + self.biases[\'encoder\'] + self.user_latent)\n              \n            self.decoder_op = tf.matmul(encoder_op, self.weights[\'decoder\'])+self.biases[\'decoder\']\n            self.output = tool.activation_function(self.g_act, self.decoder_op)\n            \n    def _create_loss(self):\n        with tf.name_scope(""loss""):\n            \n            self.loss = - tf.reduce_sum(self.input_R*tf.log(self.output) + (1 - self.input_R)*tf.log(1 - self.output))\n\n            self.reg_loss = self.reg * l2_loss(self.weights[\'encoder\'], self.weights[\'decoder\'],\n                                               self.biases[\'encoder\'], self.biases[\'decoder\'],\n                                               self.user_latent)\n            self.loss = self.loss + self.reg_loss\n    \n    def _create_optimizer(self):\n        with tf.name_scope(""learner""):\n            self.optimizer = learner.optimizer(self.learner, self.loss, self.learning_rate)\n            \n    def build_graph(self):\n        self._create_placeholders()\n        self._create_variables()\n        self._create_inference()\n        self._create_loss()\n        self._create_optimizer()\n                                               \n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n        for epoch in range(1, self.num_epochs+1):\n            # Generate training instances\n            mask_corruption_np = np.random.binomial(1, 1-self.corruption_level, (self.num_users, self.num_items))\n            total_loss = 0.0\n            training_start_time = time()\n            all_users = np.arange(self.num_users)\n            users_iter = DataIterator(all_users, batch_size=self.batch_size, shuffle=True, drop_last=False)\n            for batch_set_idx in users_iter:\n                batch_matrix = np.zeros((len(batch_set_idx), self.num_items))\n                for idx, user_id in enumerate(batch_set_idx):\n                    items_by_user_id = self.train_dict[user_id]\n                    batch_matrix[idx, items_by_user_id] = 1\n\n                feed_dict = {self.mask_corruption: mask_corruption_np[batch_set_idx, :],\n                             self.input_R: batch_matrix,\n                             self.user_input: batch_set_idx}\n                _, loss = self.sess.run([self.optimizer, self.loss], feed_dict=feed_dict)\n                total_loss += loss\n\n            self.logger.info(""[iter %d : loss : %f, time: %f]"" % (epoch, total_loss/self.num_users,\n                                                             time()-training_start_time))\n            if epoch % self.verbose == 0:\n                self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate()))\n    \n    @timer\n    def evaluate(self):\n        return self.evaluator.evaluate(self)\n\n    def predict(self, user_ids, candidate_items_user_ids):\n        ratings = []\n        mask = np.ones((1, self.num_items), dtype=np.int32)\n        if candidate_items_user_ids is not None:\n            rating_matrix = np.zeros((1, self.num_items), dtype=np.int32)\n            for user_id, candidate_items_user_id in zip(user_ids, candidate_items_user_ids):\n                items_by_user_id = self.train_matrix[user_id].indices\n                for item_id in items_by_user_id:\n                    rating_matrix[0, item_id] = 1\n                output = self.sess.run(self.output, \n                                       feed_dict={self.mask_corruption: mask,\n                                                  self.input_R: rating_matrix,\n                                                  self.user_input: [user_id]})\n                ratings.append(output[0, candidate_items_user_id])\n                \n        else:\n            rating_matrix = np.zeros((1,self.num_items), dtype=np.int32)\n            all_items = np.arange(self.num_items)\n            for user_id in user_ids:\n                items_by_user_id = self.train_matrix[user_id].indices\n                for item_id in items_by_user_id:\n                    rating_matrix[0, item_id] = 1\n\n                feed_dict = {self.mask_corruption: mask,\n                             self.input_R: rating_matrix,\n                             self.user_input: [user_id]}\n                output = self.sess.run(self.output, feed_dict=feed_dict)\n                ratings.append(output[0, all_items])\n        return ratings\n'"
model/general_recommender/CFGAN.py,21,"b'""""""\nReference: Dong-Kyu Chae, et al., ""CFGAN: A Generic Collaborative Filtering Framework \nbased on Generative Adversarial Networks."" in CIKM2018\n@author: Zhongchuan Sun\n""""""\n# ZP: Hybrid of zero-reconstruction regularization and partial-masking\n\nfrom model.AbstractRecommender import AbstractRecommender\nimport numpy as np\nimport tensorflow as tf\nfrom scipy.sparse import csr_matrix\nfrom util import csr_to_user_dict\nfrom util import randint_choice\nfrom util import l2_loss\nfrom util.data_iterator import DataIterator\n\n\nclass CFGAN(AbstractRecommender):\n    def __init__(self, sess, dataset, conf):\n        super(CFGAN, self).__init__(dataset, conf)\n        self.dataset = dataset\n        self.epochs = conf[""epochs""]\n        self.mode = conf[""mode""]\n        self.reg_G = conf[""reg_G""]\n        self.reg_D = conf[""reg_D""]\n        self.lr_G = conf[""lr_G""]\n        self.lr_D = conf[""lr_D""]\n        self.batchSize_G = conf[""batchSize_G""]\n        self.batchSize_D = conf[""batchSize_D""]\n\n        self.opt_g = conf[""opt_G""]\n        self.opt_d = conf[""opt_D""]\n        self.hiddenLayer_G = conf[""hiddenLayer_G""]\n        self.hiddenLayer_D = conf[""hiddenLayer_D""]\n        self.step_G = conf[""step_G""]\n        self.step_D = conf[""step_D""]\n\n        self.ZR_ratio = conf[""ZR_ratio""]\n        self.ZP_ratio = conf[""ZP_ratio""]\n        self.ZR_coefficient = conf[""ZR_coefficient""]\n        self.verbose = conf[""verbose""]\n        \n        train_matrix = dataset.train_matrix\n        self.train_matrix = train_matrix.copy()\n        if self.mode == ""itemBased"":\n            self.train_matrix = self.train_matrix.transpose(copy=True).tocsr()\n\n        self.num_users, self.num_items = self.train_matrix.shape\n        self.user_pos_train = csr_to_user_dict(self.train_matrix)\n        self.all_items = np.arange(self.num_items)\n        self.sess = sess\n\n    def build_graph(self):\n        self._create_layer()\n\n        # generator\n        self.condition = tf.placeholder(tf.float32, [None, self.num_items])\n        self.g_zr_dims = tf.placeholder(tf.float32, [None, self.num_items])\n        self.g_output = self.gen(self.condition)\n        self.g_zr_loss = tf.reduce_mean(tf.reduce_sum(tf.square(self.g_output - 0) * self.g_zr_dims, 1, keepdims=True))\n\n        # discriminator\n        self.mask = tf.placeholder(tf.float32, [None, self.num_items])  # purchased = 1, otherwise 0\n        fake_data = self.g_output * self.mask\n        fake_data = tf.concat([self.condition, fake_data], 1)\n\n        self.real_data = tf.placeholder(tf.float32, [None, self.num_items])\n        real_data = tf.concat([self.condition, self.real_data], 1)\n\n        d_fake = self.dis(fake_data)\n        d_real = self.dis(real_data)\n\n        g_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'gen\')\n        d_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'dis\')\n\n        # define loss & optimizer for G\n        g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_fake, labels=tf.ones_like(d_fake)))\n\n        g_loss = g_loss + self.reg_G * l2_loss(*g_vars)\n        g_loss = g_loss + self.ZR_coefficient*self.g_zr_loss\n\n        if self.opt_g == \'sgd\':\n            self.trainer_g = tf.train.GradientDescentOptimizer(self.lr_G).minimize(g_loss, var_list=g_vars)\n        elif self.opt_g == \'adam\':\n            self.trainer_g = tf.train.AdamOptimizer(self.lr_G).minimize(g_loss, var_list=g_vars)\n\n        # define loss & optimizer for D\n        d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_real, labels=tf.ones_like(d_real)))\n        d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_fake, labels=tf.zeros_like(d_fake)))\n        d_loss = d_loss_real + d_loss_fake + self.reg_D * l2_loss(*d_vars)\n\n        if self.opt_d == \'sgd\':\n            self.trainer_d = tf.train.GradientDescentOptimizer(self.lr_D).minimize(d_loss, var_list=d_vars)\n        elif self.opt_d == \'adam\':\n            self.trainer_d = tf.train.AdamOptimizer(self.lr_D).minimize(d_loss, var_list=d_vars)\n\n    def _create_layer(self):\n        # Generator\'s layers\n        self.gen_layers = []\n        xavier_init = tf.contrib.layers.xavier_initializer()\n        # stacked hidden layers\n        for i, unit in enumerate(self.hiddenLayer_G):\n            hidden_layer = tf.layers.Dense(unit, activation=tf.sigmoid,\n                                           kernel_initializer=xavier_init, name=""gen_h%d""%i)\n            self.gen_layers.append(hidden_layer)\n\n        # hidden -> output\n        output_layer = tf.layers.Dense(self.num_items, activation=tf.identity,\n                                       kernel_initializer=xavier_init, name=""gen_out"")\n        self.gen_layers.append(output_layer)\n\n        # Discriminator\'s layers\n        self.dis_layers = []\n        # stacked hidden layers\n        for i, unit in enumerate(self.hiddenLayer_D):\n            hidden_layer = tf.layers.Dense(unit, activation=tf.sigmoid,\n                                           kernel_initializer=xavier_init, name=""dis_h%d""%i)\n            self.dis_layers.append(hidden_layer)\n\n        # hidden -> output\n        output_layer = tf.layers.Dense(1, activation=tf.identity,\n                                       kernel_initializer=xavier_init, name=""dis_out"")\n        self.dis_layers.append(output_layer)\n\n    def gen(self, x):\n        for layer in self.gen_layers:\n            x = layer.apply(x)\n        return x\n\n    def dis(self, x):\n        for layer in self.dis_layers:\n            x = layer.apply(x)\n        return x\n\n    def get_train_data(self):\n        train_matrix = self.train_matrix.copy()\n        zr_matrix = self.train_matrix.todense()\n        pm_matrix = self.train_matrix.todense()\n\n        for u, pos_items in self.user_pos_train.items():\n            num = int((self.num_items-len(pos_items)) * self.ZR_ratio)\n            sample = randint_choice(self.num_items, size=num, replace=False, exclusion=pos_items).tolist()\n            zr_matrix[u, sample] = 1\n\n            num = int((self.num_items-len(pos_items)) * self.ZP_ratio)\n            sample = randint_choice(self.num_items, size=num, replace=False, exclusion=pos_items).tolist()\n            pm_matrix[u, sample] = 1\n        return train_matrix, csr_matrix(zr_matrix), csr_matrix(pm_matrix)\n\n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n        g_iter = DataIterator(np.arange(self.num_users), batch_size=self.batchSize_G, shuffle=True, drop_last=False)\n        d_iter = DataIterator(np.arange(self.num_users), batch_size=self.batchSize_D, shuffle=True, drop_last=False)\n\n        total_epochs = self.epochs\n        total_epochs = int(total_epochs / self.step_G)\n        for epoch in range(total_epochs):\n            train_matrix, zr_matrix, pm_matrix = self.get_train_data()\n            # training discriminator\n            for d_epoch in range(self.step_D):\n                for idx in d_iter:\n                    train_data = train_matrix[idx].toarray()\n                    train_mask = pm_matrix[idx].toarray()\n                    feed = {self.real_data: train_data, self.mask: train_mask, self.condition: train_data}\n                    self.sess.run(self.trainer_d, feed_dict=feed)\n\n            # training generator\n            for g_epoch in range(self.step_G):\n                for idx in g_iter:\n                    train_data = train_matrix[idx].toarray()\n                    train_z_mask = zr_matrix[idx].toarray()\n                    train_p_mask = pm_matrix[idx].toarray()\n                    feed = {self.real_data: train_data, self.condition: train_data,\n                            self.mask: train_p_mask, self.g_zr_dims: train_z_mask}\n                    self.sess.run(self.trainer_g, feed_dict=feed)\n            if epoch % self.verbose == 0:\n                self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate()))\n\n    def evaluate(self):\n        self.eval_rating_matrix()\n        return self.evaluator.evaluate(self)\n\n    def eval_rating_matrix(self):\n        all_ratings = self.sess.run(self.g_output, feed_dict={self.condition: self.train_matrix.toarray()})\n        if self.mode == ""itemBased"":\n            all_ratings = np.transpose(all_ratings)\n        self.all_ratings_for_test = all_ratings\n\n    def predict(self, users, items):\n        all_ratings = self.all_ratings_for_test[users]\n        if items is not None:\n            all_ratings = [all_ratings[idx][item] for idx, item in enumerate(items)]\n        return all_ratings\n'"
model/general_recommender/ConvNCF.py,32,"b'""""""\nXiangnan He et al., ""Outer Product-based Neural Collaborative Filtering"", In IJCAI 2018.  \n@author: wubin\n""""""\nfrom model.AbstractRecommender import AbstractRecommender\nimport tensorflow as tf\nimport numpy as np\nfrom time import time\nfrom util import learner, tool\nfrom util import timer\nimport pickle\nfrom util import l2_loss\nfrom data import PairwiseSampler\n\n\nclass ConvNCF(AbstractRecommender):\n    def __init__(self, sess, dataset, conf):  \n        super(ConvNCF, self).__init__(dataset, conf)\n        self.embedding_size = conf[""embedding_size""]\n        regs = conf[""regs""]\n        self.lambda_bilinear = regs[0]\n        self.gamma_bilinear = regs[1]\n        self.lambda_weight = regs[2]\n        self.keep = conf[""keep""]\n        self.num_epochs = conf[""epochs""]\n        self.batch_size = conf[""batch_size""]\n        self.nc = conf[""net_channel""]\n        self.lr_embed = conf[""lr_embed""]\n        self.lr_net = conf[""lr_net""]\n        self.verbose = conf[""verbose""]\n        self.loss_function = conf[""loss_function""]\n        self.num_negatives = conf[""num_negatives""]\n        self.embed_init_method = conf[""embed_init_method""]\n        self.weight_init_method = conf[""weight_init_method""]\n        self.stddev = conf[""stddev""]\n        self.num_users = dataset.num_users\n        self.num_items = dataset.num_items \n        self.dataset = dataset\n        self.sess = sess\n        \n    def _create_placeholders(self):\n        with tf.name_scope(""input_data""):\n            self.user_input = tf.placeholder(tf.int32, shape = [None,], name = ""user_input"")\n            self.item_input_pos = tf.placeholder(tf.int32, shape = [None,], name = ""item_input_pos"")\n            self.item_input_neg = tf.placeholder(tf.int32, shape = [None,], name = ""item_input_neg"")\n            self.keep_prob = tf.placeholder_with_default(1.0, shape=None,name = ""keep_prob"")\n\n    # ---------- model definition -------\n    def weight_variable(self, shape):\n        initializer = tool.get_initializer(self.weight_init_method, self.stddev)\n        return tf.Variable(initializer(shape))\n\n    def bias_variable(self, shape):\n        initial = tf.constant(0.1, shape=shape)\n        return tf.Variable(initial)\n\n    def _conv_weight(self, isz, osz):\n        return self.weight_variable([2,2,isz,osz]), self.bias_variable([osz])\n\n    def _conv_layer(self, x, P):\n        conv = tf.nn.conv2d(x, P[0], strides=[1, 2, 2, 1], padding=\'SAME\')\n        return tf.nn.tanh(conv + P[1])\n\n    def _create_variables(self, params=None):\n        with tf.name_scope(""embedding""):\n            if params is None:\n                initializer = tool.get_initializer(self.embed_init_method, self.stddev)\n                self.embedding_P = tf.Variable(initializer([self.num_users, self.embedding_size]),\n                                               name=\'embedding_P\', dtype=tf.float32)  # (users, embedding_size)\n                self.embedding_Q = tf.Variable(initializer([self.num_items, self.embedding_size]),\n                                               name=\'embedding_Q\', dtype=tf.float32)  # (items, embedding_size)\n            \n            else:\n                self.embedding_P = tf.Variable(params[0], name=\'embedding_P\', dtype=tf.float32)  # (users, embedding_size)\n                self.embedding_Q = tf.Variable(params[1], name=\'embedding_Q\', dtype=tf.float32)  # (items, embedding_size)\n\n            # here should have 6 iszs due to the size of outer products is 64x64\n            iszs = [1] + self.nc[:-1]\n            oszs = self.nc\n            self.P = []\n            for isz, osz in zip(iszs, oszs):\n                self.P.append(self._conv_weight(isz, osz))\n\n            self.W = self.weight_variable([self.nc[-1], 1])  # 32x1\n            self.b = self.weight_variable([1])  # 1\n\n    def _create_inference(self, item_input):\n        with tf.name_scope(""inference""):\n            # embedding look up\n            self.embedding_p = tf.nn.embedding_lookup(self.embedding_P, self.user_input)\n            self.embedding_q = tf.nn.embedding_lookup(self.embedding_Q, item_input)\n\n            # outer product of P_u and Q_i\n            self.relation = tf.matmul(tf.expand_dims(self.embedding_p,2), tf.expand_dims(self.embedding_q,1))\n            self.net_input = tf.expand_dims(self.relation, -1)\n\n            # CNN\n            self.layer = []\n            input = self.net_input\n            for p in self.P:\n                self.layer.append(self._conv_layer(input, p))\n                input = self.layer[-1]\n            # prediction\n            self.dropout = tf.nn.dropout(self.layer[-1], self.keep_prob)\n            self.output_layer = tf.matmul(tf.reshape(self.dropout,[-1,self.nc[-1]]), self.W) + self.b\n\n            return self.embedding_p, self.embedding_q, self.output_layer\n\n    def _regular(self, params):\n        res = 0\n        for param in params:\n            res += l2_loss(param[0], param[1])\n        return res\n\n    def _create_loss(self):\n        with tf.name_scope(""loss""):\n            # BPR loss for L(Theta)\n            self.p1, self.q1, self.output = self._create_inference(self.item_input_pos)\n            self.p2, self.q2, self.output_neg = self._create_inference(self.item_input_neg)\n            self.result = self.output - self.output_neg\n            self.loss = learner.pairwise_loss(self.loss_function, self.result)\n\n            self.opt_loss = self.loss + self.lambda_bilinear * l2_loss(self.p1, self.q2, self.q1) + \\\n                            self.gamma_bilinear * self._regular([(self.W, self.b)]) + \\\n                            self.lambda_weight * (self._regular(self.P) + self._regular([(self.W, self.b)]))\n\n    # used at the first time when emgeddings are pretrained yet network are randomly initialized\n    # if not, the parameters may be NaN.\n    def _create_pre_optimizer(self):\n        self.pre_opt = tf.train.AdagradOptimizer(learning_rate=0.01).minimize(self.loss)\n\n    def _create_optimizer(self):\n        with tf.name_scope(""learner""):\n            # seperated optimizer\n            var_list1 = [self.embedding_P, self.embedding_Q]\n            # [self.W1,self.W2,self.W3,self.W4,self.b1,self.b2,self.b3,self.b4,self.P1,self.P2,self.P3]\n            var_list2 = list(set(tf.trainable_variables()) - set(var_list1))\n            opt1 = tf.train.AdagradOptimizer(self.lr_embed)\n            opt2 = tf.train.AdagradOptimizer(self.lr_net)\n            grads = tf.gradients(self.opt_loss, var_list1 + var_list2)\n            grads1 = grads[:len(var_list1)]\n            grads2 = grads[len(var_list1):]\n            train_op1 = opt1.apply_gradients(zip(grads1, var_list1))\n            train_op2 = opt2.apply_gradients(zip(grads2, var_list2))\n            self.optimizer = tf.group(train_op1, train_op2)   \n            \n    def build_graph(self):\n        self._create_placeholders()\n        try:\n            pretrained_params = []\n            with open(self.mf_pretrain, ""rb"") as fin:\n                pretrained_params.append(pickle.load(fin, encoding=""utf-8""))\n            with open(self.mlp_pretrain, ""rb"") as fin:\n                pretrained_params.append(pickle.load(fin, encoding=""utf-8""))\n            self.logger.info(""load pretrained params successful!"")\n        except:\n            pretrained_params = None\n            self.logger.info(""load pretrained params unsuccessful!"")\n        self._create_variables(pretrained_params)\n        self._create_loss()\n        if pretrained_params is None:\n            self._create_pre_optimizer()\n        self._create_optimizer()\n\n    # ---------- training process -------\n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n        data_iter = PairwiseSampler(self.dataset, neg_num=1, batch_size=self.batch_size, shuffle=True)\n        for epoch in range(1, self.num_epochs+1):\n            total_loss = 0.0\n            training_start_time = time()\n            num_training_instances = len(data_iter)\n            for bat_users, bat_items_pos, bat_items_neg in data_iter:\n                    feed_dict = {self.user_input: bat_users,\n                                 self.item_input_pos: bat_items_pos,\n                                 self.item_input_neg: bat_items_neg}\n                    loss, _ = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n                    total_loss += loss\n            self.logger.info(""[iter %d : loss : %f, time: %f]"" % (epoch, total_loss / num_training_instances,\n                                                             time() - training_start_time))\n            if epoch % self.verbose == 0:\n                self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate()))\n\n    @timer\n    def evaluate(self):\n        return self.evaluator.evaluate(self)\n\n    def predict(self, user_ids, candidate_items_user_ids):\n        ratings = []\n        if candidate_items_user_ids is not None:\n            for u, i in zip(user_ids, candidate_items_user_ids):\n                users = np.full(len(i), u, dtype=np.int32)\n                feed_dict = {self.user_input: users, self.item_input_pos: i}\n                ratings.append(self.sess.run(self.output, feed_dict=feed_dict))\n        else:\n            for u in user_ids:\n                users = np.full(self.num_items, u, dtype=np.int32)\n                feed_dict = {self.user_input: users, self.item_input_pos: np.arange(self.num_items)}\n                ratings.append(self.sess.run(self.output, feed_dict=feed_dict))\n        return ratings\n'"
model/general_recommender/DAE.py,17,"b'""""""\nReference: Wu, Yao, et al. ""Collaborative denoising auto-encoders for top-n recommender systems."" in WSDM2016\n@author: wubin\n""""""\nfrom model.AbstractRecommender import AbstractRecommender\nimport tensorflow as tf\nimport numpy as np\nfrom time import time\nfrom util import learner, tool\nfrom util import timer\nfrom util.tool import csr_to_user_dict\nfrom util import l2_loss\nfrom util.data_iterator import DataIterator\n\n\nclass DAE(AbstractRecommender):\n    def __init__(self, sess, dataset, conf):  \n        super(DAE, self).__init__(dataset, conf)\n        self.hidden_neuron = conf[""hidden_neuron""]\n        self.learning_rate = conf[""learning_rate""]\n        self.learner = conf[""learner""]\n        self.reg = conf[""reg""]\n        self.num_epochs = conf[""epochs""]\n        self.batch_size = conf[""batch_size""]\n        self.verbose = conf[""verbose""]\n        self.h_act = conf[""h_act""]\n        self.g_act = conf[""g_act""]\n        self.corruption_level = conf[""corruption_level""]\n        self.init_method = conf[""init_method""]\n        self.stddev = conf[""stddev""]\n        self.num_users = dataset.num_users\n        self.num_items = dataset.num_items \n        self.dataset = dataset \n        self.train_dict = csr_to_user_dict(dataset.train_matrix) \n        self.sess = sess\n        \n    def _create_placeholders(self):\n        with tf.name_scope(""input_data""):\n            self.input_R = tf.placeholder(tf.float32, [None, self.num_items])\n            self.mask_corruption = tf.placeholder(tf.float32, [None, self.num_items])\n            \n    def _create_variables(self):\n        with tf.name_scope(""embedding""):  # The embedding initialization is unknown now\n            initializer = tool.get_initializer(self.init_method, self.stddev)\n            self.V = tf.Variable(initializer([self.num_users, self.hidden_neuron]))\n             \n            self.weights = {\'encoder\': tf.Variable(initializer([self.num_items, self.hidden_neuron])),\n                            \'decoder\': tf.Variable(initializer([self.hidden_neuron, self.num_items]))}\n            self.biases = {\'encoder\': tf.Variable(initializer([self.hidden_neuron])),\n                           \'decoder\': tf.Variable(initializer([self.num_items]))}\n            \n    def _create_inference(self):\n        with tf.name_scope(""inference""):\n            \n            corrupted_input = tf.multiply(self.input_R, self.mask_corruption)\n            encoder_op = tool.activation_function(self.h_act, tf.matmul(corrupted_input, self.weights[\'encoder\']) +\n                                                  self.biases[\'encoder\'])\n              \n            self.decoder_op = tf.matmul(encoder_op, self.weights[\'decoder\'])+self.biases[\'decoder\']\n            self.output = tool.activation_function(self.g_act, self.decoder_op)\n            \n    def _create_loss(self):\n        with tf.name_scope(""loss""):\n            self.loss = - tf.reduce_sum(self.input_R * tf.log(self.output) +\n                                        (1 - self.input_R) * tf.log(1 - self.output))\n \n            self.reg_loss = self.reg*l2_loss(self.weights[\'encoder\'], self.weights[\'decoder\'],\n                                             self.biases[\'encoder\'], self.biases[\'decoder\'])\n\n            self.loss = self.loss + self.reg_loss\n\n    def _create_optimizer(self):\n        with tf.name_scope(""learner""):\n            self.optimizer = learner.optimizer(self.learner, self.loss, self.learning_rate)\n            \n    def build_graph(self):\n        self._create_placeholders()\n        self._create_variables()\n        self._create_inference()\n        self._create_loss()\n        self._create_optimizer()\n                                               \n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n        for epoch in range(self.num_epochs):\n            # Generate training instances\n            mask_corruption_np = np.random.binomial(1, 1-self.corruption_level, (self.num_users, self.num_items))\n\n            total_loss = 0.0\n            all_users = np.arange(self.num_users)\n            users_iter = DataIterator(all_users, batch_size=self.batch_size, shuffle=True, drop_last=False)\n            training_start_time = time()\n            for batch_set_idx in users_iter:\n                batch_matrix = np.zeros((len(batch_set_idx), self.num_items))\n                for idx, user_id in enumerate(batch_set_idx):\n                    items_by_user_id = self.train_dict[user_id]\n                    batch_matrix[idx, items_by_user_id] = 1\n\n                feed_dict = {self.mask_corruption: mask_corruption_np[batch_set_idx, :],\n                             self.input_R: batch_matrix}\n                _, loss = self.sess.run([self.optimizer, self.loss], feed_dict=feed_dict)\n                total_loss += loss\n            self.logger.info(""[iter %d : loss : %f, time: %f]"" % (epoch, total_loss/self.num_users,\n                                                             time()-training_start_time))\n            if epoch % self.verbose == 0:\n                self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate()))\n    \n    @timer\n    def evaluate(self):\n        return self.evaluator.evaluate(self)\n\n    def predict(self, user_ids, candidate_items_user_ids):\n        ratings = []\n        mask = np.ones((1, self.num_items), dtype=np.int32)\n        if candidate_items_user_ids is not None:\n            rating_matrix = np.zeros((1, self.num_items), dtype=np.int32)\n            for user_id, candidate_items_user_id in zip(user_ids, candidate_items_user_ids):\n                items_by_user_id = self.dataset.train_matrix[user_id].indices\n                for item_id in items_by_user_id:\n                    rating_matrix[0, item_id] = 1\n\n                feed_dict = {self.mask_corruption: mask,\n                             self.input_R: rating_matrix}\n                output = self.sess.run(self.output, feed_dict=feed_dict)\n                ratings.append(output[0, candidate_items_user_id])\n                \n        else:\n            rating_matrix = np.zeros((1, self.num_items), dtype=np.int32)\n            all_items = np.arange(self.num_items)\n            for user_id in user_ids:\n                items_by_user_id = self.dataset.train_matrix[user_id].indices\n                for item_id in items_by_user_id:\n                    rating_matrix[0, item_id] = 1\n\n                feed_dict = {self.mask_corruption: mask,\n                             self.input_R: rating_matrix}\n                output = self.sess.run(self.output, feed_dict=feed_dict)\n                ratings.append(output[0, all_items])\n        return ratings\n'"
model/general_recommender/DMF.py,24,"b'""""""\nReference: Hong-Jian Xue et al., ""Deep Matrix Factorization Models for Recommender Systems."" In IJCAI2017.  \n@author: wubin\n""""""\nimport tensorflow as tf\nimport numpy as np\nfrom time import time\nfrom util import learner, tool\nfrom model.AbstractRecommender import AbstractRecommender\nfrom util import timer\n\n\nclass DMF(AbstractRecommender):\n    def __init__(self, sess, dataset, conf):\n        super(DMF, self).__init__(dataset, conf)\n        self.learning_rate = conf[""learning_rate""]\n        self.learner = conf[""learner""]\n        self.num_epochs = conf[""epochs""]\n        self.num_negatives = conf[""num_negatives""]\n        self.batch_size = conf[""batch_size""]\n        self.verbose = conf[""verbose""]\n        layers = conf[""layers""]\n        self.loss_function = conf[""loss_function""]\n        self.fist_layer_size = layers[0]\n        self.last_layer_size = layers[1]\n        self.init_method = conf[""init_method""]\n        self.stddev = conf[""stddev""]\n        self.neg_sample_size = self.num_negatives\n        self.num_users = dataset.num_users\n        self.num_items = dataset.num_items \n        self.dataset = dataset \n        self.user_matrix = self.dataset.train_matrix\n        self.item_matrix = self.dataset.train_matrix.tocsc()\n        self.trainMatrix = self.dataset.train_matrix.todok()\n        self.sess = sess\n        \n    def _create_placeholders(self):\n        with tf.name_scope(""input_data""):\n            self.one_hot_u = tf.placeholder(tf.float32, shape=[None, None], name=\'user_input\')\n            self.one_hot_v = tf.placeholder(tf.float32, shape=[None, None], name=\'item_input\')\n            self.labels = tf.placeholder(tf.float32, shape=[None, ], name=""labels"")\n            \n    def _create_variables(self):\n        with tf.name_scope(""embedding""):  # The embedding initialization is unknown now\n            initializer = tool.get_initializer(self.init_method, self.stddev)\n            self.u_w1 = tf.Variable(initializer([self.num_items, self.fist_layer_size]), name=""u_w1"")\n            self.u_b1 = tf.Variable(initializer([self.fist_layer_size]), name=""u_b1"")\n            self.u_w2 = tf.Variable(initializer([self.fist_layer_size, self.last_layer_size]), name=""u_w2"")\n            self.u_b2 = tf.Variable(initializer([self.last_layer_size]), name=""u_b2"")\n        \n            self.v_w1 = tf.Variable(initializer([self.num_users, self.fist_layer_size]), name=""v_w1"")\n            self.v_b1 = tf.Variable(initializer([self.fist_layer_size]), name=""v_b1"")\n            self.v_w2 = tf.Variable(initializer([self.fist_layer_size, self.last_layer_size]), name=""v_w2"")\n            self.v_b2 = tf.Variable(initializer([self.last_layer_size]), name=""v_b2"")\n\n    def _create_inference(self):\n        with tf.name_scope(""inference""):\n            net_u_1 = tf.nn.relu(tf.matmul(self.one_hot_u, self.u_w1) + self.u_b1)\n            net_u_2 = tf.matmul(net_u_1, self.u_w2) + self.u_b2\n        \n            net_v_1 = tf.nn.relu(tf.matmul(self.one_hot_v, self.v_w1) + self.v_b1)\n            net_v_2 = tf.matmul(net_v_1, self.v_w2) + self.v_b2\n        \n            fen_zhi = tf.reduce_sum(net_u_2 * net_v_2, 1)\n        \n            norm_u = tf.reduce_sum(tf.square(net_u_2), 1)\n            norm_v = tf.reduce_sum(tf.square(net_v_2), 1)\n            fen_mu = norm_u * norm_v\n            self.output = tf.nn.relu(fen_zhi / fen_mu)\n            \n    def _create_loss(self):\n        with tf.name_scope(""loss""):\n            self.loss = learner.pointwise_loss(self.loss_function, self.labels, self.output)\n\n    def _create_optimizer(self):\n        with tf.name_scope(""learner""):\n            self.optimizer = learner.optimizer(self.learner, self.loss, self.learning_rate)\n            \n    def build_graph(self):\n        self._create_placeholders()\n        self._create_variables()\n        self._create_inference()\n        self._create_loss()\n        self._create_optimizer()\n                                               \n    def train_model(self):\n\n        for epoch in range(self.num_epochs):\n            # Generate training instances\n            user_input, item_input, labels = self._get_input_all_data()\n            \n            total_loss = 0.0\n            training_start_time = time()\n            num_training_instances = len(user_input)\n            for num_batch in np.arange(int(num_training_instances/self.batch_size)):\n                num_training_instances = len(user_input)\n                id_start = num_batch * self.batch_size\n                id_end = (num_batch + 1) * self.batch_size\n                if id_end > num_training_instances:\n                    id_end = num_training_instances\n                bat_users = user_input[id_start:id_end].tolist()\n                bat_items = item_input[id_start:id_end].tolist()\n                bat_labels = np.array(labels[id_start:id_end])\n                feed_dict = {self.one_hot_u: bat_users, self.one_hot_v: bat_items,\n                             self.labels: bat_labels}\n                loss, _ = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n                total_loss += loss\n            self.logger.info(""[iter %d : loss : %f, time: %f]"" % (epoch, total_loss/num_training_instances,\n                                                             time()-training_start_time))\n            if epoch % self.verbose == 0:\n                self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate()))\n    \n    @timer\n    def evaluate(self):\n        return self.evaluator.evaluate(self)\n    \n    def predict(self, user_ids, candidate_items_user_ids):\n        ratings = []\n        if candidate_items_user_ids is not None:\n            for user_id, candidate_items_user_id in zip(user_ids, candidate_items_user_ids):\n                user_input, item_input = [], []\n                u_vector = np.reshape(self.user_matrix.getrow(user_id.toarray(), [self.num_items]))\n                for i in candidate_items_user_id:\n                    user_input.append(u_vector)\n                    i_vector = np.reshape(self.item_matrix.getcol(i).toarray(), [self.num_users])\n                    item_input.append(i_vector)\n                    feed_dict = {self.one_hot_u: user_input, self.one_hot_v: item_input}\n                    ratings.append(self.sess.run(self.output, feed_dict=feed_dict))\n        else:\n            user_input, item_input = [], []\n            u_vector = np.reshape(self.user_matrix.getrow(user_id.toarray(), [self.num_items]))\n            for i in range(self.num_items):\n                user_input.append(u_vector)\n                i_vector = np.reshape(self.item_matrix.getcol(i).toarray(), [self.num_users])\n                item_input.append(i_vector)\n                feed_dict = {self.one_hot_u: user_input, self.one_hot_v: item_input}\n                ratings.append(self.sess.run(self.output, feed_dict=feed_dict))\n        return ratings\n\n    def _get_input_all_data(self):\n        user_input, item_input, labels = [], [], []\n        for u in range(self.num_users):\n            # positive instance\n            items_by_user = self.user_matrix[u].indices\n            u_vector = np.reshape(self.user_matrix.getrow(u).toarray(), [self.num_items])\n            for i in items_by_user:\n                i_vector = np.reshape(self.item_matrix.getcol(i).toarray(), [self.num_users])\n                user_input.append(u_vector)\n                item_input.append(i_vector)\n                labels.append(1)\n                # negative instance\n                for _ in range(self.num_negatives):\n                    j = np.random.randint(self.num_items)\n                    while (u, j) in self.trainMatrix.keys():\n                        j = np.random.randint(self.num_items)\n                    j_vector = np.reshape(self.item_matrix.getcol(i).toarray(), [self.num_users])\n                    user_input.append(u_vector)\n                    item_input.append(j_vector)\n                    labels.append(0)\n        user_input = np.array(user_input, dtype=np.int32)\n        item_input = np.array(item_input, dtype=np.int32)\n        labels = np.array(labels, dtype=np.float32)\n        num_training_instances = len(user_input)\n        shuffle_index = np.arange(num_training_instances, dtype=np.int32)\n        np.random.shuffle(shuffle_index)\n        user_input = user_input[shuffle_index]\n        item_input = item_input[shuffle_index]\n        labels = labels[shuffle_index]\n        return user_input, item_input, labels\n'"
model/general_recommender/DeepICF.py,61,"b'""""""\nReference: Feng Xue et al., ""Deep Item-based Collaborative Filtering for Top-N Recommendation"" in TOIS2019\n@author: wubin\n""""""\n\nfrom model.AbstractRecommender import AbstractRecommender\nimport tensorflow as tf\nimport numpy as np\nfrom time import time\nfrom util import learner,data_generator, tool\nfrom util import timer\nimport pickle\nfrom tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\nfrom util.tool import csr_to_user_dict, pad_sequences\nfrom util import l2_loss\nfrom util.data_iterator import DataIterator\n\n\nclass DeepICF(AbstractRecommender):\n    def __init__(self, sess, dataset, conf):\n        super(DeepICF, self).__init__(dataset, conf)\n        self.pretrain_file = conf[""pretrain_file""]\n        self.verbose = conf[""verbose""]\n        self.batch_size = conf[""batch_size""]\n        self.use_batch_norm = conf[""batch_norm""]\n        self.num_epochs = conf[""epochs""]\n        self.weight_size = conf[""weight_size""]\n        self.embedding_size = conf[""embedding_size""]\n        self.n_hidden = conf[""layers""]\n        regs = conf[""regs""]\n        self.reg_W = conf[""regw""]\n        self.lambda_bilinear = regs[0]\n        self.gamma_bilinear = regs[1]\n        self.eta_bilinear = regs[2] \n        self.alpha = conf[""alpha""]\n        self.beta = conf[""beta""]\n        self.num_negatives = conf[""num_neg""]\n        self.learning_rate = conf[""learning_rate""]\n        self.activation = conf[""activation""]\n        self.algorithm = conf[""algorithm""]\n        self.learner = conf[""learner""]\n        self.embed_init_method = conf[""embed_init_method""]\n        self.weight_init_method = conf[""weight_init_method""]\n        self.bias_init_method = conf[""bias_init_method""]\n        self.stddev = conf[""stddev""]\n        self.dataset = dataset\n        self.num_items = dataset.num_items\n        self.num_users = dataset.num_users\n        self.train_dict = csr_to_user_dict(dataset.train_matrix)\n        self.sess = sess\n\n    # batch norm\n    def batch_norm_layer(self, x, train_phase, scope_bn):\n        bn_train = batch_norm(x, decay=0.9, center=True, scale=True, updates_collections=None,\n                              is_training=True, reuse=None, trainable=True, scope=scope_bn)\n        bn_inference = batch_norm(x, decay=0.9, center=True, scale=True, updates_collections=None,\n                                  is_training=False, reuse=True, trainable=True, scope=scope_bn)\n        z = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)\n        return z\n\n    def _create_placeholders(self):\n        with tf.name_scope(""input_data""):\n            self.user_input = tf.placeholder(tf.int32, shape=[None, None])  # the index of users\n            self.num_idx = tf.placeholder(tf.float32, shape=[None, ])  # the number of items rated by users\n            self.item_input = tf.placeholder(tf.int32, shape=[None, ])  # the index of items\n            self.labels = tf.placeholder(tf.float32, shape=[None, ])  # the ground truth\n            self.is_train_phase = tf.placeholder(tf.bool)  # mark is training or testing\n\n    def _create_variables(self, params=None):\n        with tf.name_scope(""embedding""):  # The embedding initialization is unknown now\n            if params is None:\n                embed_initializer = tool.get_initializer(self.embed_init_method, self.stddev)\n                \n                self.c1 = tf.Variable(embed_initializer([self.num_items, self.embedding_size]),\n                                      name=\'c1\', dtype=tf.float32)\n                self.embedding_Q = tf.Variable(embed_initializer([self.num_items, self.embedding_size]),\n                                               name=\'embedding_Q\', dtype=tf.float32)\n                self.bias = tf.Variable(tf.zeros(self.num_items), name=\'bias\')\n            else:\n                self.c1 = tf.Variable(params[0], name=\'c1\', dtype=tf.float32)\n                self.embedding_Q = tf.Variable(params[1], name=\'embedding_Q\', dtype=tf.float32)\n                self.bias = tf.Variable(params[2], name=""bias"", dtype=tf.float32)\n                \n            self.c2 = tf.constant(0.0, tf.float32, [1, self.embedding_size], name=\'c2\')\n            self.embedding_Q_ = tf.concat([self.c1, self.c2], axis=0, name=\'embedding_Q_\')\n            \n            # Variables for attention\n            weight_initializer = tool.get_initializer(self.weight_init_method, self.stddev)\n            bias_initializer = tool.get_initializer(self.bias_init_method, self.stddev)\n            if self.algorithm == 0:\n                self.W = tf.Variable(weight_initializer([self.embedding_size, self.weight_size]),\n                                     name=\'Weights_for_MLP\', dtype=tf.float32, trainable=True)\n            else:\n                self.W = tf.Variable(weight_initializer([2 * self.embedding_size, self.weight_size]), \n                                     name=\'Weights_for_MLP\', dtype=tf.float32, trainable=True)\n            self.b = tf.Variable(bias_initializer([1, self.weight_size]), name=\'Bias_for_MLP\',\n                                 dtype=tf.float32, trainable=True)\n            self.h = tf.Variable(tf.ones([self.weight_size, 1]), name=\'H_for_MLP\', dtype=tf.float32)\n\n            # Variables for DeepICF+a\n            self.weights = {\'out\': tf.Variable(weight_initializer([self.n_hidden[-1], 1]), name=\'weights_out\')}\n            self.biases = {\'out\': tf.Variable(tf.random_normal([1]), name=\'biases_out\')}\n            n_hidden_0 = self.embedding_size\n            for i in range(len(self.n_hidden)):\n                if i > 0:\n                    n_hidden_0 = self.n_hidden[i - 1]\n                n_hidden_1 = self.n_hidden[i]\n                self.weights[\'h%d\' % i] = tf.Variable(weight_initializer([n_hidden_0, n_hidden_1]),\n                                                      name=\'weights_h%d\' % i)\n                self.biases[\'b%d\' % i] = tf.Variable(tf.random_normal([n_hidden_1]), name=\'biases_b%d\' % i)\n\n    def _attention_MLP(self, q_):\n        with tf.name_scope(""attention_MLP""):\n            b = tf.shape(q_)[0]\n            n = tf.shape(q_)[1]\n            r = (self.algorithm + 1) * self.embedding_size\n\n            mlp_output = tf.matmul(tf.reshape(q_, [-1, r]), self.W) + self.b  # (b*n, e or 2*e) * (e or 2*e, w) + (1, w)\n            if self.activation == 0:\n                mlp_output = tf.nn.relu(mlp_output)\n            elif self.activation == 1:\n                mlp_output = tf.nn.sigmoid(mlp_output)\n            elif self.activation == 2:\n                mlp_output = tf.nn.tanh(mlp_output)\n\n            A_ = tf.reshape(tf.matmul(mlp_output, self.h), [b, n])  # (b*n, w) * (w, 1) => (None, 1) => (b, n)\n\n            # softmax for not mask features\n            exp_A_ = tf.exp(A_)\n            num_idx = self.num_idx\n            mask_mat = tf.sequence_mask(num_idx, maxlen=n, dtype=tf.float32)  # (b, n)\n            exp_A_ = mask_mat * exp_A_\n            exp_sum = tf.reduce_sum(exp_A_, 1, keepdims=True)  # (b, 1)\n            exp_sum = tf.pow(exp_sum, tf.constant(self.beta, tf.float32, [1]))\n\n            A = tf.expand_dims(tf.div(exp_A_, exp_sum), 2)  # (b, n, 1)\n\n            return A, tf.reduce_sum(A * self.embedding_q_, 1)\n\n    def _create_inference(self):\n        with tf.name_scope(""inference""):\n            self.embedding_q_ = tf.nn.embedding_lookup(self.embedding_Q_, self.user_input)  # (b, n, e)\n            self.embedding_q = tf.nn.embedding_lookup(self.embedding_Q, self.item_input)  # (b, 1, e)\n\n            if self.algorithm == 0:  # prod\n                # (?, k)\n                self.A, self.embedding_p = self._attention_MLP(self.embedding_q_ * tf.expand_dims(self.embedding_q, 1))\n            else:  # concat\n                n = tf.shape(self.user_input)[1]\n                self.A, self.embedding_p = self._attention_MLP(tf.concat([self.embedding_q_,\n                                                                          tf.tile(tf.expand_dims(self.embedding_q, 1),\n                                                                                  tf.stack([1, n, 1]))], 2))  # (?, k)\n\n            self.bias_i = tf.nn.embedding_lookup(self.bias, self.item_input)\n            self.coeff = tf.pow(tf.expand_dims(self.num_idx, 1), tf.constant(self.alpha, tf.float32, [1]))\n            self.embedding_p = self.coeff * self.embedding_p  # (?, k)\n\n            # DeepICF+a\n            layer1 = tf.multiply(self.embedding_p, self.embedding_q)  # (?, k)\n            for i in range(0, len(self.n_hidden)):\n                layer1 = tf.add(tf.matmul(layer1, self.weights[\'h%d\' % i]), self.biases[\'b%d\' % i])\n                if self.use_batch_norm:\n                    layer1 = self.batch_norm_layer(layer1, train_phase=self.is_train_phase, scope_bn=\'bn_%d\' % i)\n                layer1 = tf.nn.relu(layer1)\n            out_layer = tf.reduce_sum(tf.matmul(layer1, self.weights[\'out\']) + self.biases[\'out\'], 1)  # (?, 1)\n\n            self.output = tf.sigmoid(tf.add_n([out_layer, self.bias_i]))  # (?, 1)\n\n    def _create_loss(self):\n        with tf.name_scope(""loss""):\n            self.loss = tf.losses.log_loss(self.labels, self.output) + \\\n                        self.lambda_bilinear * l2_loss(self.embedding_Q) + \\\n                        self.gamma_bilinear * l2_loss(self.embedding_Q_) + \\\n                        self.eta_bilinear * l2_loss(self.W)\n\n            for i in range(min(len(self.n_hidden), len(self.reg_W))):\n                if self.reg_W[i] > 0:\n                    self.loss = self.loss + self.reg_W[i] * l2_loss(self.weights[\'h%d\' % i])\n\n    def _create_optimizer(self):\n        with tf.name_scope(""learner""):\n            self.optimizer = learner.optimizer(self.learner, self.loss, self.learning_rate)\n            \n    def build_graph(self):\n        self._create_placeholders()\n        try:\n            pretrained_params = []\n            with open(self.pretrain_file, ""rb"") as fin:\n                pretrained_params.append(pickle.load(fin, encoding=""utf-8""))\n            with open(self.mlp_pretrain, ""rb"") as fin:\n                pretrained_params.append(pickle.load(fin, encoding=""utf-8""))\n            self.logger.info(""load pretrained params successful!"")\n        except:\n            pretrained_params = None\n            self.logger.info(""load pretrained params unsuccessful!"")\n            \n        self._create_variables(pretrained_params)\n        self._create_inference()\n        self._create_loss()\n        self._create_optimizer()\n\n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n        for epoch in range(1, self.num_epochs+1):\n            user_input, num_idx, item_input, labels = \\\n                data_generator._get_pointwise_all_likefism_data(self.dataset, self.num_negatives, self.train_dict)\n            data_iter = DataIterator(user_input, num_idx, item_input, labels,\n                                     batch_size=self.batch_size, shuffle=True)\n                    \n            num_training_instances = len(user_input)\n            total_loss = 0.0\n            training_start_time = time()\n            for bat_users, bat_idx, bat_items, bat_labels in data_iter:\n                    bat_users = pad_sequences(bat_users, value=self.num_items)\n                    feed_dict = {self.user_input: bat_users,\n                                 self.num_idx: bat_idx,\n                                 self.item_input: bat_items,\n                                 self.labels: bat_labels,\n                                 self.is_train_phase: True}\n                    loss, _ = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n                    total_loss += loss\n            self.logger.info(""[iter %d : loss : %f, time: %f]"" % (epoch, total_loss/num_training_instances,\n                                                             time()-training_start_time))\n            if epoch % self.verbose == 0:\n                self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate()))\n    \n    @timer\n    def evaluate(self):\n        return self.evaluator.evaluate(self)\n    \n    def predict(self, user_ids, candidate_items_userids):      \n        ratings = []\n        if candidate_items_userids is not None:\n            for u, eval_items_by_u in zip(user_ids, candidate_items_userids):\n                user_input = []\n                cand_items_by_u = self.train_dict[u]\n                num_idx = len(cand_items_by_u)\n                item_idx = np.full(len(eval_items_by_u), num_idx, dtype=np.int32)\n                user_input.extend([cand_items_by_u]*len(eval_items_by_u))\n                feed_dict = {self.user_input: user_input,\n                             self.num_idx: item_idx, \n                             self.item_input: eval_items_by_u,\n                             self.is_train_phase: False}\n                ratings.append(self.sess.run(self.output, feed_dict=feed_dict))\n                \n        else:\n            eval_items = np.arange(self.num_items)\n            for u in user_ids:\n                user_input = []\n                cand_items_by_u = self.train_dict[u]\n                num_idx = len(cand_items_by_u)\n                item_idx = np.full(self.num_items, num_idx, dtype=np.int32)\n                user_input.extend([cand_items_by_u]*self.num_items)\n                feed_dict = {self.user_input: user_input,\n                             self.num_idx: item_idx, \n                             self.item_input: eval_items,\n                             self.is_train_phase: False}\n                ratings.append(self.sess.run(self.output, feed_dict=feed_dict))\n        return ratings\n'"
model/general_recommender/FISM.py,24,"b'""""""\nReference: Santosh Kabbur et al., ""FISM: Factored Item Similarity Models for Top-N Recommender Systems."" in KDD 2013.\n@author: wubin\n""""""\nfrom model.AbstractRecommender import AbstractRecommender\nimport tensorflow as tf\nimport numpy as np\nfrom time import time\nfrom util import learner, data_generator, tool\nfrom util import timer\nfrom util.tool import csr_to_user_dict\nfrom util import l2_loss\nfrom util.data_iterator import DataIterator\nfrom util import pad_sequences\n\n\nclass FISM(AbstractRecommender):\n    def __init__(self, sess, dataset, conf):\n        super(FISM, self).__init__(dataset, conf)\n        self.batch_size = conf[""batch_size""]\n        self.num_epochs = conf[""epochs""]\n        self.embedding_size = conf[""embedding_size""]\n        self.regs = conf[""regs""]\n        self.lambda_bilinear = self.regs[0]\n        self.gamma_bilinear = self.regs[1]\n        self.alpha = conf[""alpha""]\n        self.num_negatives = conf[""num_neg""]\n        self.learning_rate = conf[""learning_rate""]\n        self.learner = conf[""learner""]\n        self.topK = conf[""topk""]\n        self.loss_function = conf[""loss_function""]\n        self.is_pairwise = conf[""is_pairwise""]\n        self.num_negatives = conf[""num_neg""]\n        self.init_method = conf[""init_method""]\n        self.stddev = conf[""stddev""]\n        self.verbose = conf[""verbose""]\n        self.num_users = dataset.num_users\n        self.num_items = dataset.num_items\n        self.dataset = dataset\n        self.train_dict = csr_to_user_dict(self.dataset.train_matrix)\n        self.sess = sess\n\n    def _create_placeholders(self):\n        with tf.name_scope(""input_data""):\n            self.user_input = tf.placeholder(tf.int32, shape=[None, None], name=""user_input"")  # the index of users\n            self.num_idx = tf.placeholder(tf.float32, shape=[None], name=""num_idx"")  # the number of items rated by users\n            self.item_input = tf.placeholder(tf.int32, shape=[None], name=""item_input_pos"")  # the index of items\n            if self.is_pairwise is True:\n                self.user_input_neg = tf.placeholder(tf.int32, shape=[None, None], name=""user_input_neg"")\n                self.item_input_neg = tf.placeholder(tf.int32, shape=[None], name=""item_input_neg"")\n                self.num_idx_neg = tf.placeholder(tf.float32, shape=[None], name=""num_idx_neg"")\n            else:\n                self.labels = tf.placeholder(tf.float32, shape=[None], name=""labels"")\n\n    def _create_variables(self):\n        with tf.name_scope(""embedding""):  # The embedding initialization is unknown now\n            initializer = tool.get_initializer(self.init_method, self.stddev)\n            self.c1 = tf.Variable(initializer([self.num_items, self.embedding_size]),\n                                  name=\'c1\', dtype=tf.float32)\n            self.c2 = tf.constant(0.0, tf.float32, [1, self.embedding_size], name=\'c2\')\n            self.embedding_Q_ = tf.concat([self.c1,self.c2], 0, name=\'embedding_Q_\')\n            self.embedding_Q = tf.Variable(initializer([self.num_items, self.embedding_size]),\n                                           name=\'embedding_Q\', dtype=tf.float32)\n            self.bias = tf.Variable(tf.zeros(self.num_items),name=\'bias\')\n\n    def _create_inference(self, user_input, item_input, num_idx):\n        with tf.name_scope(""inference""):\n            embedding_p = tf.reduce_sum(tf.nn.embedding_lookup(self.embedding_Q_, user_input), 1)\n            embedding_q = tf.nn.embedding_lookup(self.embedding_Q, item_input)\n            bias_i = tf.nn.embedding_lookup(self.bias, item_input)\n            coeff = tf.pow(num_idx, -tf.constant(self.alpha, tf.float32, [1]))\n            output = coeff * tf.reduce_sum(tf.multiply(embedding_p,embedding_q), 1) + bias_i\n        return embedding_p, embedding_q, output\n\n    def _create_loss(self):\n        with tf.name_scope(""loss""):\n            p1, q1, self.output = self._create_inference(self.user_input, self.item_input, self.num_idx)\n            if self.is_pairwise is True:\n                _, q2, output_neg = self._create_inference(self.user_input_neg, self.item_input_neg, self.num_idx_neg)\n                self.result = self.output - output_neg\n                self.loss = learner.pairwise_loss(self.loss_function, self.result) + \\\n                            self.lambda_bilinear * l2_loss(p1) + \\\n                            self.gamma_bilinear * l2_loss(q2, q1)\n            \n            else:\n                self.loss = learner.pointwise_loss(self.loss_function, self.labels, self.output) + \\\n                            self.lambda_bilinear * l2_loss(p1) + \\\n                            self.gamma_bilinear * l2_loss(q1)\n\n    def _create_optimizer(self):\n        with tf.name_scope(""learner""):\n            self.optimizer = learner.optimizer(self.learner, self.loss, self.learning_rate)\n              \n    def build_graph(self):\n        self._create_placeholders()\n        self._create_variables()\n        self._create_loss()\n        self._create_optimizer()\n\n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n        for epoch in range(1, self.num_epochs+1):\n            if self.is_pairwise is True:\n                user_input, user_input_neg, num_idx_pos, num_idx_neg, item_input_pos, item_input_neg = \\\n                    data_generator._get_pairwise_all_likefism_data(self.dataset)\n                data_iter = DataIterator(user_input, user_input_neg, num_idx_pos,\n                                         num_idx_neg, item_input_pos, item_input_neg,\n                                         batch_size=self.batch_size, shuffle=True)\n            else:\n                user_input, num_idx, item_input, labels = \\\n                data_generator._get_pointwise_all_likefism_data(self.dataset, self.num_negatives, self.train_dict)\n                data_iter = DataIterator(user_input, num_idx, item_input, labels,\n                                         batch_size=self.batch_size, shuffle=True)\n\n            total_loss = 0.0\n            training_start_time = time()\n\n            if self.is_pairwise is True:\n                for bat_users_pos, bat_users_neg, bat_idx_pos, bat_idx_neg, bat_items_pos, bat_items_neg in data_iter:\n                    bat_users_pos = pad_sequences(bat_users_pos, value=self.num_items)\n                    bat_users_neg = pad_sequences(bat_users_neg, value=self.num_items)\n                    feed_dict = {self.user_input: bat_users_pos,\n                                 self.user_input_neg: bat_users_neg,\n                                 self.num_idx: bat_idx_pos,\n                                 self.num_idx_neg: bat_idx_neg,\n                                 self.item_input: bat_items_pos,\n                                 self.item_input_neg: bat_items_neg}\n\n                    loss, _ = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n                    total_loss += loss\n            else:\n                for bat_users, bat_idx, bat_items, bat_labels in data_iter:\n                    bat_users = pad_sequences(bat_users, value=self.num_items)\n                    feed_dict = {self.user_input: bat_users,\n                                 self.num_idx: bat_idx,\n                                 self.item_input: bat_items,\n                                 self.labels: bat_labels}\n                    loss, _ = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n                    total_loss += loss\n\n            self.logger.info(""[iter %d : loss : %f, time: %f]"" % (epoch, total_loss/len(user_input),\n                                                             time()-training_start_time))\n            if epoch % self.verbose == 0:\n                self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate()))\n        \n        # save model\n        # params = self.sess.run([self.c1, self.embedding_Q, self.bias])\n        # with open(""./pretrained/%s_epoch=%d_fism.pkl"" % (self.dataset_name, self.num_epochs), ""wb"") as fout:\n        #     pickle.dump(params, fout)\n    @timer\n    def evaluate(self):\n        return self.evaluator.evaluate(self)\n    \n    def predict(self, user_ids, candidate_items_userids):      \n        ratings = []\n        if candidate_items_userids is not None:\n            for u, eval_items_by_u in zip(user_ids, candidate_items_userids):\n                user_input = []\n                cand_items_by_u = self.train_dict[u]\n                num_idx = len(cand_items_by_u)\n                item_idx = np.full(len(eval_items_by_u), num_idx, dtype=np.int32)\n                user_input.extend([cand_items_by_u]*len(eval_items_by_u))\n                feed_dict = {self.user_input: user_input,\n                             self.num_idx: item_idx, \n                             self.item_input: eval_items_by_u}\n                ratings.append(self.sess.run(self.output, feed_dict=feed_dict))\n                \n        else:\n            eval_items = np.arange(self.num_items)\n            for u in user_ids:\n                user_input = []\n                cand_items_by_u = self.train_dict[u]\n                num_idx = len(cand_items_by_u)\n                item_idx = np.full(self.num_items, num_idx, dtype=np.int32)\n                user_input.extend([cand_items_by_u]*self.num_items)\n                feed_dict = {self.user_input: user_input,\n                             self.num_idx: item_idx, \n                             self.item_input: eval_items}\n                ratings.append(self.sess.run(self.output, feed_dict=feed_dict))\n        return ratings\n'"
model/general_recommender/IRGAN.py,50,"b'""""""\nReference: Jun Wang, et al., ""IRGAN: A Minimax Game for Unifying Generative and \nDiscriminative Information Retrieval Models."" SIGIR 2017.\n@author: Zhongchuan Sun\n""""""\nfrom model.AbstractRecommender import AbstractRecommender\nimport tensorflow as tf\nimport pickle\nimport numpy as np\nfrom concurrent.futures import ThreadPoolExecutor\nfrom util import l2_loss\nfrom util.data_iterator import DataIterator\n\n\nclass GEN(object):\n    def __init__(self, item_num, user_num, emb_dim, lamda, param=None, init_delta=0.05, learning_rate=0.05):\n        self.itemNum = item_num\n        self.userNum = user_num\n        self.emb_dim = emb_dim\n        self.lamda = lamda  # regularization parameters\n        self.param = param\n        self.init_delta = init_delta\n        self.learning_rate = learning_rate\n        self.g_params = []\n\n        with tf.variable_scope(\'generator\'):\n            if self.param is None:\n                self.user_embeddings = tf.Variable(\n                    tf.random_uniform([self.userNum, self.emb_dim], minval=-self.init_delta, maxval=self.init_delta,\n                                      dtype=tf.float32))\n                self.item_embeddings = tf.Variable(\n                    tf.random_uniform([self.itemNum, self.emb_dim], minval=-self.init_delta, maxval=self.init_delta,\n                                      dtype=tf.float32))\n                self.item_bias = tf.Variable(tf.zeros([self.itemNum]))\n            else:\n                self.user_embeddings = tf.Variable(self.param[0])\n                self.item_embeddings = tf.Variable(self.param[1])\n                self.item_bias = tf.Variable(param[2])\n\n            self.g_params = [self.user_embeddings, self.item_embeddings, self.item_bias]\n\n        self.u = tf.placeholder(tf.int32)\n        self.i = tf.placeholder(tf.int32)\n        self.reward = tf.placeholder(tf.float32)\n\n        self.u_embedding = tf.nn.embedding_lookup(self.user_embeddings, self.u)\n        self.i_embedding = tf.nn.embedding_lookup(self.item_embeddings, self.i)\n        self.i_bias = tf.gather(self.item_bias, self.i)\n\n        self.all_logits = tf.reduce_sum(tf.multiply(self.u_embedding, self.item_embeddings), 1) + self.item_bias\n        self.i_prob = tf.gather(\n            tf.reshape(tf.nn.softmax(tf.reshape(self.all_logits, [1, -1])), [-1]),\n            self.i)\n\n        self.gan_loss = -tf.reduce_mean(tf.log(self.i_prob) * self.reward) + \\\n                        self.lamda * l2_loss(self.u_embedding, self.i_embedding, self.i_bias)\n\n        g_opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n        self.gan_updates = g_opt.minimize(self.gan_loss, var_list=self.g_params)\n\n        # for test stage, self.u: [batch_size]\n        self.all_rating = tf.matmul(self.u_embedding, self.item_embeddings, transpose_a=False,\n                                    transpose_b=True) + self.item_bias\n\n\nclass DIS(object):\n    def __init__(self, item_num, user_num, emb_dim, lamda, param=None, init_delta=0.05, learning_rate=0.05):\n        self.itemNum = item_num\n        self.userNum = user_num\n        self.emb_dim = emb_dim\n        self.lamda = lamda  # regularization parameters\n        self.param = param\n        self.init_delta = init_delta\n        self.learning_rate = learning_rate\n        self.d_params = []\n\n        with tf.variable_scope(\'discriminator\'):\n            if self.param is None:\n                self.user_embeddings = tf.Variable(\n                    tf.random_uniform([self.userNum, self.emb_dim], minval=-self.init_delta, maxval=self.init_delta,\n                                      dtype=tf.float32))\n                self.item_embeddings = tf.Variable(\n                    tf.random_uniform([self.itemNum, self.emb_dim], minval=-self.init_delta, maxval=self.init_delta,\n                                      dtype=tf.float32))\n                self.item_bias = tf.Variable(tf.zeros([self.itemNum]))\n            else:\n                self.user_embeddings = tf.Variable(self.param[0])\n                self.item_embeddings = tf.Variable(self.param[1])\n                self.item_bias = tf.Variable(self.param[2])\n\n        self.d_params = [self.user_embeddings, self.item_embeddings, self.item_bias]\n\n        # placeholder definition\n        self.u = tf.placeholder(tf.int32)\n        self.i = tf.placeholder(tf.int32)\n        self.label = tf.placeholder(tf.float32)\n\n        self.u_embedding = tf.nn.embedding_lookup(self.user_embeddings, self.u)\n        self.i_embedding = tf.nn.embedding_lookup(self.item_embeddings, self.i)\n        self.i_bias = tf.gather(self.item_bias, self.i)\n\n        self.pre_logits = tf.reduce_sum(tf.multiply(self.u_embedding, self.i_embedding), 1) + self.i_bias\n        self.pre_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.label, logits=self.pre_logits) + \\\n                        self.lamda * l2_loss(self.u_embedding, self.i_embedding, self.i_bias)\n\n        d_opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n        self.d_updates = d_opt.minimize(self.pre_loss, var_list=self.d_params)\n\n        self.reward_logits = tf.reduce_sum(tf.multiply(self.u_embedding, self.i_embedding),\n                                           1) + self.i_bias\n        self.reward = 2 * (tf.sigmoid(self.reward_logits) - 0.5)\n\n        # for test stage, self.u: [batch_size]\n        self.all_rating = tf.matmul(self.u_embedding, self.item_embeddings, transpose_b=True) + self.item_bias\n\n        self.all_logits = tf.reduce_sum(tf.multiply(self.u_embedding, self.item_embeddings), 1) + self.item_bias\n        self.NLL = -tf.reduce_mean(tf.log(\n            tf.gather(tf.reshape(tf.nn.softmax(tf.reshape(self.all_logits, [1, -1])), [-1]), self.i))\n        )\n        # for dns sample\n        self.dns_rating = tf.reduce_sum(tf.multiply(self.u_embedding, self.item_embeddings), 1) + self.item_bias\n\n\nclass IRGAN(AbstractRecommender):\n    def __init__(self, sess, dataset, conf):\n        super(IRGAN, self).__init__(dataset, conf)\n\n        train_matrix = dataset.trainMatrix.tocsr()\n        self.num_users, self.num_items = train_matrix.shape\n        self.factors_num = conf[""factors_num""]\n        self.lr = conf[""lr""]\n        self.g_reg = conf[""g_reg""]\n        self.d_reg = conf[""d_reg""]\n        self.epochs = conf[""epochs""]\n        self.g_epoch = conf[""g_epoch""]\n        self.d_epoch = conf[""d_epoch""]\n        self.batch_size = conf[""batch_size""]\n        self.d_tau = conf[""d_tau""]\n        self.topK = conf[""topk""]\n        self.pretrain_file = conf[""pretrain_file""]\n\n        idx_value_dict = {}\n        for idx, value in enumerate(train_matrix):\n            if any(value.indices):\n                idx_value_dict[idx] = value.indices\n\n        self.user_pos_train = idx_value_dict\n\n        self.num_users, self.num_items = dataset.num_users, dataset.num_items\n        self.dataset = dataset\n        self.sess = sess\n        self.all_items = np.arange(self.num_items)\n\n    def build_graph(self):\n        with open(self.pretrain_file, ""rb"") as fin:\n            pretrain_params = pickle.load(fin, encoding=""latin"")\n        self.generator = GEN(self.num_items, self.num_users, self.factors_num, self.g_reg, param=pretrain_params,\n                             learning_rate=self.lr)\n        self.discriminator = DIS(self.num_items, self.num_users, self.factors_num, self.d_reg, param=None,\n                                 learning_rate=self.lr)\n\n    def get_train_data(self):\n        users_list, items_list, labels_list = [], [], []\n        train_users = list(self.user_pos_train.keys())\n        with ThreadPoolExecutor() as executor:\n            data = executor.map(self.get_train_data_one_user, train_users)\n        data = list(data)\n        for users, items, labels in data:\n            users_list.extend(users)\n            items_list.extend(items)\n            labels_list.extend(labels)\n\n        return users_list, items_list, labels_list\n\n    def get_train_data_one_user(self, user):\n        user_list, items_list, label_list = [], [], []\n        pos = self.user_pos_train[user]\n\n        rating = self.sess.run(self.generator.all_rating, {self.generator.u: [user]})\n        rating = np.reshape(rating, [-1])\n        rating = np.array(rating) / self.d_tau  # Temperature\n        exp_rating = np.exp(rating)\n        prob = exp_rating / np.sum(exp_rating)\n        neg = np.random.choice(self.all_items, size=len(pos), p=prob)\n        for i, j in zip(pos, neg):\n            user_list.append(user)\n            items_list.append(i)\n            label_list.append(1.0)\n\n            user_list.append(user)\n            items_list.append(j)\n            label_list.append(0.0)\n        return user_list, items_list, label_list\n\n    def train_model(self):\n        for _ in range(self.epochs):\n            for _ in range(self.d_epoch):\n                self.training_discriminator()\n            for _ in range(self.g_epoch):\n                self.training_generator()\n                self.logger.info(""%s"" % (self.evaluate()))\n\n    def training_discriminator(self):\n        users_list, items_list, labels_list = self.get_train_data()\n        data_iter = DataIterator(users_list, items_list, labels_list,\n                                 batch_size=self.batch_size, shuffle=True)\n        for bat_users, bat_items, bat_labels in data_iter:\n            feed = {self.discriminator.u: bat_users,\n                    self.discriminator.i: bat_items,\n                    self.discriminator.label: bat_labels}\n            self.sess.run(self.discriminator.d_updates, feed_dict=feed)\n\n    def training_generator(self):\n        for user, pos in self.user_pos_train.items():\n            sample_lambda = 0.2\n            rating = self.sess.run(self.generator.all_logits, {self.generator.u: user})\n            exp_rating = np.exp(rating)\n            prob = exp_rating / np.sum(exp_rating)  # prob is generator distribution p_\\theta\n\n            pn = (1 - sample_lambda) * prob\n            pn[pos] += sample_lambda * 1.0 / len(pos)\n            # Now, pn is the Pn in importance sampling, prob is generator distribution p_\\theta\n\n            sample = np.random.choice(self.all_items, 2 * len(pos), p=pn)\n            ###########################################################################\n            # Get reward and adapt it with importance sampling\n            ###########################################################################\n            feed = {self.discriminator.u: user, self.discriminator.i: sample}\n            reward = self.sess.run(self.discriminator.reward, feed_dict=feed)\n            reward = reward * prob[sample] / pn[sample]\n            ###########################################################################\n            # Update G\n            ###########################################################################\n            feed = {self.generator.u: user, self.generator.i: sample, self.generator.reward: reward}\n            self.sess.run(self.generator.gan_updates, feed_dict=feed)\n\n    def evaluate(self):\n        self._cur_user_embedding, self._cur_item_embedding, self._cur_item_bias = self.sess.run(self.generator.g_params)\n        return self.evaluator.evaluate(self)\n\n    def predict(self, users, items):\n        user_embedding = self._cur_user_embedding[users]\n        item_embedding = self._cur_item_embedding\n        item_bias = self._cur_item_bias\n\n        all_ratings = np.matmul(user_embedding, item_embedding.T) + item_bias\n        if items is not None:\n            all_ratings = [all_ratings[idx][item] for idx, item in enumerate(items)]\n        return all_ratings\n'"
model/general_recommender/JCA.py,32,"b'""""""\nReference: Ziwei Zhu, et al. ""Improving Top-K Recommendation via Joint\nCollaborative Autoencoders."" in WWW2019\n@author: wubin\n""""""\nfrom model.AbstractRecommender import AbstractRecommender\nimport tensorflow as tf\nimport numpy as np\nfrom time import time\nfrom util import learner, tool\nfrom util import timer\nfrom util import l2_loss\n\n\nclass JCA(AbstractRecommender):\n    def __init__(self, sess, dataset, conf):\n        super(JCA, self).__init__(dataset, conf)\n        self.hidden_neuron = conf[""hidden_neuron""]\n        self.learning_rate = conf[""learning_rate""]\n        self.learner = conf[""learner""]\n        self.reg = conf[""reg""]\n        self.num_epochs = conf[""epochs""]\n        self.batch_size = conf[""batch_size""]\n        self.verbose = conf[""verbose""]\n        self.f_act = conf[""f_act""]  # the activation function for the output layer\n        self.g_act = conf[""g_act""]  # the activation function for the hidden layer\n        self.margin = conf[""margin""]\n        self.corruption_level = conf[""corruption_level""]\n        self.init_method = conf[""init_method""]\n        self.stddev = conf[""stddev""]\n        self.neg_sample_rate = conf[""num_neg""]\n        self.num_users = dataset.num_users\n        self.num_items = dataset.num_items \n        self.dataset = dataset\n        self.train_R = dataset.train_matrix.toarray()\n        self.U_OH_mat = np.eye(self.num_users, dtype=float)\n        self.I_OH_mat = np.eye(self.num_items, dtype=float)\n        self.num_batch_U = int(self.num_users / float(self.batch_size)) + 1\n        self.num_batch_I = int(self.num_items / float(self.batch_size)) + 1\n        self.sess = sess\n        \n    def _create_placeholders(self):\n        with tf.name_scope(""input_data""):\n            # input rating vector\n            self.input_R_U = tf.placeholder(dtype=tf.float32, shape=[None, self.num_items], name=""input_R_U"")\n            self.input_R_I = tf.placeholder(dtype=tf.float32, shape=[self.num_users, None], name=""input_R_I"")\n            self.input_OH_I = tf.placeholder(dtype=tf.float32, shape=[None, self.num_items], name=""input_OH_I"")\n            self.input_P_cor = tf.placeholder(dtype=tf.int32, shape=[None, 2], name=""input_P_cor"")\n            self.input_N_cor = tf.placeholder(dtype=tf.int32, shape=[None, 2], name=""input_N_cor"")\n    \n            # input indicator vector indicator\n            self.row_idx = tf.placeholder(dtype=tf.int32, shape=[None, 1], name=""row_idx"")\n            self.col_idx = tf.placeholder(dtype=tf.int32, shape=[None, 1], name=""col_idx"")\n            \n    def _create_variables(self):\n        with tf.name_scope(""embedding""):  # The embedding initialization is unknown now\n            initializer = tool.get_initializer(self.init_method, self.stddev)\n            # user component\n            # first layer weights\n            self.UV = tf.Variable(initializer([self.num_items, self.hidden_neuron]), name=""UV"", dtype=tf.float32)\n            # second layer weights\n            self.UW = tf.Variable(initializer([self.hidden_neuron, self.num_items]), name=""UW"", dtype=tf.float32)\n            # first layer bias\n            self.Ub1 = tf.Variable(initializer([1, self.hidden_neuron]), name=""Ub1"", dtype=tf.float32)\n            # second layer bias\n            self.Ub2 = tf.Variable(initializer([1, self.num_items]), name=""Ub2"", dtype=tf.float32)\n    \n            # item component\n            # first layer weights\n            self.IV = tf.Variable(initializer([self.num_users, self.hidden_neuron]), name=""IV"", dtype=tf.float32)\n            # second layer weights\n            self.IW = tf.Variable(initializer([self.hidden_neuron, self.num_users]), name=""IW"", dtype=tf.float32)\n            # first layer bias\n            self.Ib1 = tf.Variable(initializer([1, self.hidden_neuron]), name=""Ib1"", dtype=tf.float32)\n            # second layer bias\n            self.Ib2 = tf.Variable(initializer([1, self.num_users]), name=""Ib2"", dtype=tf.float32)\n\n            self.I_factor_vector = tf.Variable(initializer([1, self.num_items]),name=""I_factor_vector"", dtype=tf.float32)\n\n    def _create_inference(self):\n        with tf.name_scope(""inference""):\n            \n            # user component\n            u_pre_encoder = tf.matmul(self.input_R_U, self.UV) + self.Ub1  # input to the hidden layer\n            self.u_encoder = tool.activation_function(self.g_act, u_pre_encoder)  # output of the hidden layer\n            u_pre_decoder = tf.matmul(self.u_encoder, self.UW) + self.Ub2  # input to the output layer\n            self.u_decoder = tool.activation_function(self.f_act, u_pre_decoder)  # output of the output layer\n    \n            # item component\n            i_pre_mul = tf.transpose(tf.matmul(self.I_factor_vector, tf.transpose(self.input_OH_I)))\n            i_pre_encoder = tf.matmul(tf.transpose(self.input_R_I), self.IV) + self.Ib1  # input to the hidden layer\n            self.I_Encoder = tool.activation_function(self.g_act, i_pre_encoder * i_pre_mul)  # output of the hidden layer\n            i_pre_decoder = tf.matmul(self.I_Encoder, self.IW) + self.Ib2  # input to the output layer\n            self.I_Decoder = tool.activation_function(self.f_act, i_pre_decoder)  # output of the output layer\n    \n            # final output\n            self.Decoder = ((tf.transpose(tf.gather_nd(tf.transpose(self.u_decoder), self.col_idx)))\n                            + tf.gather_nd(tf.transpose(self.I_Decoder), self.row_idx)) / 2.0\n    \n            pos_data = tf.gather_nd(self.Decoder, self.input_P_cor)\n            neg_data = tf.gather_nd(self.Decoder, self.input_N_cor)\n            \n            self.pre_cost1 = tf.maximum(neg_data - pos_data + self.margin, tf.zeros(tf.shape(neg_data)[0]))\n            \n    def _create_loss(self):\n        with tf.name_scope(""loss""):\n            \n            cost1 = tf.reduce_sum(self.pre_cost1)  # prediction squared error\n            pre_cost2 = l2_loss(self.UW, self.UV, self.IW, self.IV, self.Ib1, self.Ib2, self.Ub1, self.Ub2)\n            cost2 = self.reg * 0.5 * pre_cost2  # regularization term\n    \n            self.cost = cost1 + cost2  # the loss function\n\n    def _create_optimizer(self):\n        with tf.name_scope(""learner""):\n            self.optimizer = learner.optimizer(self.learner, self.cost, self.learning_rate)\n            \n    def build_graph(self):\n        self._create_placeholders()\n        self._create_variables()\n        self._create_inference()\n        self._create_loss()\n        self._create_optimizer()\n                                               \n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n        for epoch in  range(1,self.num_epochs+1):\n            random_row_idx = np.random.permutation(self.num_users)  # randomly permute the rows\n            random_col_idx = np.random.permutation(self.num_items)  # randomly permute the cols\n            training_start_time = time()\n            total_loss = 0.0\n            for i in range(self.num_batch_U):  # iterate each batch\n                if i == self.num_batch_U - 1:\n                    row_idx = random_row_idx[i * self.batch_size:]\n                else:\n                    row_idx = random_row_idx[(i * self.batch_size):((i + 1) * self.batch_size)]\n                for j in range(self.num_batch_I):\n                    # get the indices of the current batch\n                    if j == self.num_batch_I - 1:\n                        col_idx = random_col_idx[j * self.batch_size:]\n                    else:\n                        col_idx = random_col_idx[(j * self.batch_size):((j + 1) * self.batch_size)]\n                    \n                    p_input, n_input = self.pairwise_neg_sampling(row_idx, col_idx)\n                    input_tmp = self.train_R[row_idx, :]\n                    input_tmp = input_tmp[:, col_idx]\n    \n                    input_r_u = self.train_R[row_idx, :]\n                    input_r_i = self.train_R[:, col_idx]\n                    _, loss = self.sess.run(  # do the optimization by the minibatch\n                        [self.optimizer, self.cost],\n                        feed_dict={\n                            self.input_R_U: input_r_u,\n                            self.input_R_I: input_r_i,\n                            self.input_OH_I: self.I_OH_mat[col_idx, :],\n                            self.input_P_cor: p_input,\n                            self.input_N_cor: n_input,\n                            self.row_idx: np.reshape(row_idx, (len(row_idx), 1)),\n                            self.col_idx: np.reshape(col_idx, (len(col_idx), 1))})\n                total_loss += loss\n            self.logger.info(""[iter %d : loss : %f, time: %f]"" %\n                             (epoch, total_loss, time()-training_start_time))\n            if epoch % self.verbose == 0:\n                self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate()))\n    \n    @timer\n    def evaluate(self):\n        feed_dict = {self.input_R_U: self.train_R,\n                     self.input_R_I: self.train_R,\n                     self.input_OH_I: self.I_OH_mat,\n                     self.input_P_cor: [[0, 0]],\n                     self.input_N_cor: [[0, 0]],\n                     self.row_idx: np.reshape(range(self.num_users), (self.num_users, 1)),\n                     self.col_idx: np.reshape(range(self.num_items), (self.num_items, 1))}\n\n        self.all_ratings = self.sess.run(self.Decoder, feed_dict=feed_dict)\n        return self.evaluator.evaluate(self)\n                \n    def pairwise_neg_sampling(self,row_idx, col_idx):\n        R = self.train_R[row_idx, :]\n        R = R[:, col_idx]\n        p_input, n_input = [], []\n        obsv_list = np.where(R == 1)\n    \n        unobsv_mat = []\n        for r in range(R.shape[0]):\n            unobsv_list = np.where(R[r, :] == 0)\n            unobsv_list = unobsv_list[0]\n            unobsv_mat.append(unobsv_list)\n    \n        for i in range(len(obsv_list[1])):\n            # positive instance\n            u = obsv_list[0][i]\n            # negative instances\n            unobsv_list = unobsv_mat[u]\n            neg_sample_list = np.random.choice(unobsv_list, size=self.neg_sample_rate, replace=False)\n            for ns in neg_sample_list:\n                p_input.append([u, obsv_list[1][i]])\n                n_input.append([u, ns])\n        return np.array(p_input), np.array(n_input)\n\n    def predict(self, user_ids, candidate_items_user_ids):\n        ratings = []\n        if candidate_items_user_ids is None:\n            all_items = np.arange(self.num_items)\n            for user_id in user_ids:\n                ratings.append(self.all_ratings[user_id, all_items])\n             \n        else:\n            for user_id, candidate_items_user_id in zip(user_ids, candidate_items_user_ids):\n                ratings.append(self.all_ratings[user_id, candidate_items_user_id])\n             \n        return ratings\n'"
model/general_recommender/LightGCN.py,28,"b'""""""\nPaper: LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation\nAuthor: Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang\nReference: https://github.com/hexiangnan/LightGCN\n""""""\n\nimport scipy.sparse as sp\nimport tensorflow as tf\nimport numpy as np\nfrom model.AbstractRecommender import AbstractRecommender\nfrom util import timer\nfrom util import l2_loss, inner_product, log_loss\nfrom data import PairwiseSampler\n\n\nclass LightGCN(AbstractRecommender):\n    def __init__(self, sess, dataset, config):\n        super(LightGCN, self).__init__(dataset, config)\n        self.lr = config[\'lr\']\n        self.reg = config[\'reg\']\n        self.emb_dim = config[\'embed_size\']\n        self.batch_size = config[\'batch_size\']\n        self.epochs = config[""epochs""]\n        self.n_layers = config[\'n_layers\']\n\n        self.dataset = dataset\n        self.n_users, self.n_items = self.dataset.num_users, self.dataset.num_items\n        self.user_pos_train = self.dataset.get_user_train_dict(by_time=False)\n        self.all_users = list(self.user_pos_train.keys())\n\n        self.norm_adj = self.create_adj_mat(config[\'adj_type\'])\n        self.sess = sess\n\n    @timer\n    def create_adj_mat(self, adj_type):\n        user_list, item_list = self.dataset.get_train_interactions()\n        user_np = np.array(user_list, dtype=np.int32)\n        item_np = np.array(item_list, dtype=np.int32)\n        ratings = np.ones_like(user_np, dtype=np.float32)\n        n_nodes = self.n_users + self.n_items\n        tmp_adj = sp.csr_matrix((ratings, (user_np, item_np+self.n_users)), shape=(n_nodes, n_nodes))\n        adj_mat = tmp_adj + tmp_adj.T\n\n        def normalized_adj_single(adj):\n            rowsum = np.array(adj.sum(1))\n            d_inv = np.power(rowsum, -1).flatten()\n            d_inv[np.isinf(d_inv)] = 0.\n            d_mat_inv = sp.diags(d_inv)\n\n            norm_adj = d_mat_inv.dot(adj)\n            print(\'generate single-normalized adjacency matrix.\')\n            return norm_adj.tocoo()\n\n        if adj_type == \'plain\':\n            adj_matrix = adj_mat\n            print(\'use the plain adjacency matrix\')\n        elif adj_type == \'norm\':\n            adj_matrix = normalized_adj_single(adj_mat + sp.eye(adj_mat.shape[0]))\n            print(\'use the normalized adjacency matrix\')\n        elif adj_type == \'gcmc\':\n            adj_matrix = normalized_adj_single(adj_mat)\n            print(\'use the gcmc adjacency matrix\')\n        elif adj_type == \'pre\':\n            # pre adjcency matrix\n            rowsum = np.array(adj_mat.sum(1))\n            d_inv = np.power(rowsum, -0.5).flatten()\n            d_inv[np.isinf(d_inv)] = 0.\n            d_mat_inv = sp.diags(d_inv)\n\n            norm_adj_tmp = d_mat_inv.dot(adj_mat)\n            adj_matrix = norm_adj_tmp.dot(d_mat_inv)\n            print(\'use the pre adjcency matrix\')\n        else:\n            mean_adj = normalized_adj_single(adj_mat)\n            adj_matrix = mean_adj + sp.eye(mean_adj.shape[0])\n            print(\'use the mean adjacency matrix\')\n\n        return adj_matrix\n\n    def _create_variable(self):\n\n        self.users = tf.placeholder(tf.int32, shape=(None,))\n        self.pos_items = tf.placeholder(tf.int32, shape=(None,))\n        self.neg_items = tf.placeholder(tf.int32, shape=(None,))\n\n        self.weights = dict()\n        initializer = tf.contrib.layers.xavier_initializer()\n        self.weights[\'user_embedding\'] = tf.Variable(initializer([self.n_users, self.emb_dim]), name=\'user_embedding\')\n        self.weights[\'item_embedding\'] = tf.Variable(initializer([self.n_items, self.emb_dim]), name=\'item_embedding\')\n\n    def build_graph(self):\n        self._create_variable()\n        self.ua_embeddings, self.ia_embeddings = self._create_lightgcn_embed()\n\n        """"""\n        *********************************************************\n        Establish the final representations for user-item pairs in batch.\n        """"""\n        self.u_g_embeddings = tf.nn.embedding_lookup(self.ua_embeddings, self.users)\n        self.pos_i_g_embeddings = tf.nn.embedding_lookup(self.ia_embeddings, self.pos_items)\n        self.neg_i_g_embeddings = tf.nn.embedding_lookup(self.ia_embeddings, self.neg_items)\n        self.u_g_embeddings_pre = tf.nn.embedding_lookup(self.weights[\'user_embedding\'], self.users)\n        self.pos_i_g_embeddings_pre = tf.nn.embedding_lookup(self.weights[\'item_embedding\'], self.pos_items)\n        self.neg_i_g_embeddings_pre = tf.nn.embedding_lookup(self.weights[\'item_embedding\'], self.neg_items)\n\n        """"""\n        *********************************************************\n        Inference for the testing phase.\n        """"""\n        self.item_embeddings_final = tf.Variable(tf.zeros([self.n_items, self.emb_dim]),\n                                                 dtype=tf.float32, name=""item_embeddings_final"", trainable=False)\n        self.user_embeddings_final = tf.Variable(tf.zeros([self.n_users, self.emb_dim]),\n                                                 dtype=tf.float32, name=""user_embeddings_final"", trainable=False)\n\n        self.assign_opt = [tf.assign(self.user_embeddings_final, self.ua_embeddings),\n                           tf.assign(self.item_embeddings_final, self.ia_embeddings)]\n\n        u_embed = tf.nn.embedding_lookup(self.user_embeddings_final, self.users)\n        self.batch_ratings = tf.matmul(u_embed, self.item_embeddings_final, transpose_a=False, transpose_b=True)\n\n        """"""\n        *********************************************************\n        Generate Predictions & Optimize via BPR loss.\n        """"""\n        self.mf_loss, self.emb_loss = self.create_bpr_loss(self.u_g_embeddings,\n                                                           self.pos_i_g_embeddings,\n                                                           self.neg_i_g_embeddings)\n        self.loss = self.mf_loss + self.emb_loss\n\n        self.opt = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss)\n\n    def _create_lightgcn_embed(self):\n        adj_mat = self._convert_sp_mat_to_sp_tensor(self.norm_adj)\n\n        ego_embeddings = tf.concat([self.weights[\'user_embedding\'], self.weights[\'item_embedding\']], axis=0)\n\n        all_embeddings = [ego_embeddings]\n\n        for k in range(0, self.n_layers):\n            side_embeddings = tf.sparse_tensor_dense_matmul(adj_mat, ego_embeddings, name=""sparse_dense"")\n\n            # transformed sum messages of neighbors.\n            ego_embeddings = side_embeddings\n            all_embeddings += [ego_embeddings]\n\n        all_embeddings = tf.stack(all_embeddings, 1)\n        all_embeddings = tf.reduce_mean(all_embeddings, axis=1, keepdims=False)\n        u_g_embeddings, i_g_embeddings = tf.split(all_embeddings, [self.n_users, self.n_items], 0)\n        return u_g_embeddings, i_g_embeddings\n\n    def _convert_sp_mat_to_sp_tensor(self, X):\n        coo = X.tocoo().astype(np.float32)\n        indices = np.mat([coo.row, coo.col]).transpose()\n        return tf.SparseTensor(indices, coo.data, coo.shape)\n\n    def create_bpr_loss(self, users, pos_items, neg_items):\n        pos_scores = inner_product(users, pos_items)\n        neg_scores = inner_product(users, neg_items)\n\n        regularizer = l2_loss(self.u_g_embeddings_pre, self.pos_i_g_embeddings_pre, self.neg_i_g_embeddings_pre)\n\n        mf_loss = tf.reduce_sum(log_loss(pos_scores - neg_scores))\n\n        emb_loss = self.reg * regularizer\n\n        return mf_loss, emb_loss\n\n    def train_model(self):\n\n        data_iter = PairwiseSampler(self.dataset, neg_num=1, batch_size=self.batch_size, shuffle=True)\n\n        self.logger.info(self.evaluator.metrics_info())\n        for epoch in range(self.epochs):\n            for bat_users, bat_pos_items, bat_neg_items in data_iter:\n                feed = {self.users: bat_users,\n                        self.pos_items: bat_pos_items,\n                        self.neg_items: bat_neg_items}\n                self.sess.run(self.opt, feed_dict=feed)\n            result = self.evaluate_model()\n            self.logger.info(""epoch %d:\\t%s"" % (epoch, result))\n\n    # @timer\n    def evaluate_model(self):\n        self.sess.run(self.assign_opt)\n        return self.evaluator.evaluate(self)\n\n    def predict(self, users, candidate_items=None):\n        feed_dict = {self.users: users}\n        ratings = self.sess.run(self.batch_ratings, feed_dict=feed_dict)\n        if candidate_items is not None:\n            ratings = [ratings[idx][u_item] for idx, u_item in enumerate(candidate_items)]\n        return ratings\n'"
model/general_recommender/MF.py,16,"b'""""""\nReference: Steffen Rendle et al., ""BPR: Bayesian Personalized Ranking from Implicit Feedback."" in UAI 2009.\n    GMF: Xiangnan He et al., ""Neural Collaborative Filtering."" in WWW 2017.\n@author: wubin\n""""""\nimport tensorflow as tf\nimport numpy as np\nfrom time import time\nfrom util import learner, tool\nfrom model.AbstractRecommender import AbstractRecommender\nfrom util import timer\nfrom util import l2_loss\nfrom data import PairwiseSampler, PointwiseSampler\n\n\nclass MF(AbstractRecommender):\n    def __init__(self, sess, dataset, conf):\n        super(MF, self).__init__(dataset, conf)\n        self.learning_rate = conf[""learning_rate""]\n        self.embedding_size = conf[""embedding_size""]\n        self.learner = conf[""learner""]\n        self.loss_function = conf[""loss_function""]\n        self.is_pairwise = conf[""is_pairwise""]\n        self.num_epochs = conf[""epochs""]\n        self.reg_mf = conf[""reg_mf""]\n        self.batch_size = conf[""batch_size""]\n        self.verbose = conf[""verbose""]\n        self.num_negatives = conf[""num_negatives""]\n        self.init_method = conf[""init_method""]\n        self.stddev = conf[""stddev""]\n        self.dataset = dataset\n        self.num_users = dataset.num_users\n        self.num_items = dataset.num_items\n        self.sess = sess\n    \n    def _create_placeholders(self):\n        with tf.name_scope(""input_data""):\n            self.user_input = tf.placeholder(tf.int32, shape=[None], name=""user_input"")\n            self.item_input = tf.placeholder(tf.int32, shape=[None], name=""item_input"")\n            if self.is_pairwise is True:\n                self.item_input_neg = tf.placeholder(tf.int32, shape=[None], name=""item_input_neg"")\n            else:\n                self.labels = tf.placeholder(tf.float32, shape=[None], name=""labels"")\n\n    def _create_variables(self):\n        with tf.name_scope(""embedding""):\n            initializer = tool.get_initializer(self.init_method, self.stddev)\n            \n            self.user_embeddings = tf.Variable(initializer([self.num_users, self.embedding_size]), \n                                               name=\'user_embeddings\', dtype=tf.float32)  # (users, embedding_size)\n            self.item_embeddings = tf.Variable(initializer([self.num_items, self.embedding_size]),\n                                               name=\'item_embeddings\', dtype=tf.float32)  # (items, embedding_size)\n\n    def _create_inference(self, item_input):\n        with tf.name_scope(""inference""):\n            # embedding look up\n            user_embedding = tf.nn.embedding_lookup(self.user_embeddings, self.user_input)\n            item_embedding = tf.nn.embedding_lookup(self.item_embeddings, item_input)\n            predict = tf.reduce_sum(tf.multiply(user_embedding, item_embedding), 1)\n            return user_embedding, item_embedding, predict\n\n    def _create_loss(self):\n        with tf.name_scope(""loss""):\n            # loss for L(Theta)\n            p1, q1, self.output = self._create_inference(self.item_input)\n            if self.is_pairwise is True:\n                _, q2, self.output_neg = self._create_inference(self.item_input_neg)\n                result = self.output - self.output_neg\n                self.loss = learner.pairwise_loss(self.loss_function, result) + self.reg_mf * l2_loss(p1, q2, q1)\n            else:\n                self.loss = learner.pointwise_loss(self.loss_function, self.labels, self.output) + \\\n                            self.reg_mf * l2_loss(p1, q1)\n\n    def _create_optimizer(self):\n        with tf.name_scope(""learner""):\n            self.optimizer = learner.optimizer(self.learner, self.loss, self.learning_rate)\n    \n    def build_graph(self):\n        self._create_placeholders()\n        self._create_variables()\n        self._create_loss()\n        self._create_optimizer()\n        \n    # ---------- training process -------\n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n        if self.is_pairwise is True:\n            data_iter = PairwiseSampler(self.dataset, neg_num=1, batch_size=self.batch_size, shuffle=True)\n        else:\n            data_iter = PointwiseSampler(self.dataset, neg_num=self.num_negatives, batch_size=self.batch_size, shuffle=True)\n\n        for epoch in range(1, self.num_epochs+1):\n            total_loss = 0.0\n            training_start_time = time()\n\n            if self.is_pairwise is True:\n                for bat_users, bat_items_pos, bat_items_neg in data_iter:\n                    feed_dict = {self.user_input: bat_users,\n                                 self.item_input: bat_items_pos,\n                                 self.item_input_neg: bat_items_neg}\n                    loss, _ = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n                    total_loss += loss\n            else:\n                for bat_users, bat_items, bat_labels in data_iter:\n                    feed_dict = {self.user_input: bat_users,\n                                 self.item_input: bat_items,\n                                 self.labels: bat_labels}\n                    loss, _ = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n                    total_loss += loss\n            self.logger.info(""[iter %d : loss : %f, time: %f]"" % (epoch, total_loss/len(data_iter),\n                                                             time()-training_start_time))\n            if epoch % self.verbose == 0:\n                self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate()))\n    \n    @timer\n    def evaluate(self):\n        self._cur_user_embeddings, self._cur_item_embeddings = self.sess.run([self.user_embeddings, self.item_embeddings])\n        return self.evaluator.evaluate(self)\n\n    def predict(self, user_ids, candidate_items=None):\n        if candidate_items is None:\n            user_embed = self._cur_user_embeddings[user_ids]\n            ratings = np.matmul(user_embed, self._cur_item_embeddings.T)\n        else:\n            ratings = []\n            for user_id, items_by_user_id in zip(user_ids, candidate_items):\n                user_embed = self._cur_user_embeddings[user_id]\n                items_embed = self._cur_item_embeddings[items_by_user_id]\n                ratings.append(np.squeeze(np.matmul(user_embed, items_embed.T)))\n            \n        return ratings\n'"
model/general_recommender/MLP.py,20,"b'""""""\nXiangnan He et al., ""Neural Collaborative Filtering."" in WWW 2017.\n@author: WuBin\n""""""\nfrom model.AbstractRecommender import AbstractRecommender\nimport tensorflow as tf\nimport numpy as np\nfrom time import time\nfrom util import data_generator, learner, tool\nfrom util.data_iterator import DataIterator\nfrom util import l2_loss\nfrom data import PointwiseSampler, PairwiseSampler\n\n\nclass MLP(AbstractRecommender):\n    def __init__(self, sess, dataset, conf):\n        super(MLP, self).__init__(dataset, conf)\n        self.layers = conf[""layers""]\n        self.learning_rate = conf[""learning_rate""]\n        self.is_pairwise = conf[""is_pairwise""]\n        self.learner = conf[""learner""]\n        self.num_epochs = conf[""epochs""]\n        self.num_negatives = conf[""num_neg""]\n        self.reg_mlp = conf[""reg_mlp""]\n        self.batch_size = conf[""batch_size""]\n        self.loss_function = conf[""loss_function""]\n        self.verbose = conf[""verbose""]\n        self.num_negatives = conf[""num_neg""]\n        self.init_method = conf[""init_method""]\n        self.stddev = conf[""stddev""]\n        self.dataset = dataset\n        self.num_users = dataset.num_users\n        self.num_items = dataset.num_items\n        self.sess = sess\n        \n    def _create_placeholders(self):\n        with tf.name_scope(""input_data""):\n            self.user_input = tf.placeholder(tf.int32, shape=[None], name=\'user_input\')\n            self.item_input = tf.placeholder(tf.int32, shape=[None], name=\'item_input\')\n            if self.is_pairwise is True:\n                self.item_input_neg = tf.placeholder(tf.int32, shape=[None], name=""item_input_neg"")\n            else:\n                self.labels = tf.placeholder(tf.float32, shape=[None], name=""labels"")\n    \n    def _create_variables(self):\n        with tf.name_scope(""embedding""):  # The embedding initialization is unknown now\n            initializer = tool.get_initializer(self.init_method, self.stddev)\n            self.mlp_embedding_user = tf.Variable(initializer([self.num_users, int(self.layers[0]/2)]), \n                                                  name=""mlp_embedding_user"", dtype=tf.float32)\n            self.mlp_embedding_item = tf.Variable(initializer([self.num_items, int(self.layers[0]/2)]),\n                                                  name=""mlp_embedding_item"", dtype=tf.float32)\n\n        self.dense_layer = [tf.layers.Dense(units=n_units, activation=tf.nn.relu, name=""layer%d"" % idx)\n                            for idx, n_units in enumerate(self.layers)]\n\n    def _create_inference(self, item_input):\n        with tf.name_scope(""inference""):\n            # Crucial to flatten an embedding vector!\n            mlp_user_latent = tf.nn.embedding_lookup(self.mlp_embedding_user, self.user_input)\n            mlp_item_latent = tf.nn.embedding_lookup(self.mlp_embedding_item, item_input)\n            # The 0-th layer is the concatenation of embedding layers\n            mlp_vector = tf.concat([mlp_user_latent, mlp_item_latent], axis=1)\n            # MLP layers\n            for layer in self.dense_layer:\n                mlp_vector = layer(mlp_vector)\n            # for idx in np.arange(len(self.layers)):\n            #     mlp_vector = tf.layers.dense(mlp_vector, units=self.layers[idx],\n            #                                  activation=tf.nn.relu, name=""layer%d"" % idx)\n                \n            # Final prediction layer\n        predict = tf.reduce_sum(mlp_vector, 1)\n        return mlp_user_latent, mlp_item_latent, predict\n             \n    def _create_loss(self):\n        with tf.name_scope(""loss""):  \n            p1, q1, self.output = self._create_inference(self.item_input)\n            if self.is_pairwise is True:\n                _, q2, self.output_neg = self._create_inference(self.item_input_neg)\n                result = self.output - self.output_neg\n                self.loss = learner.pairwise_loss(self.loss_function, result) + self.reg_mlp * l2_loss(p1, q2, q1)\n            else:\n                self.loss = learner.pointwise_loss(self.loss_function, self.labels, self.output) + \\\n                            self.reg_mlp * l2_loss(p1, q1)\n\n    def _create_optimizer(self):\n        with tf.name_scope(""learner""):\n            self.optimizer = learner.optimizer(self.learner, self.loss, self.learning_rate)\n            \n    def build_graph(self):\n        self._create_placeholders()\n        self._create_variables()\n        self._create_loss()\n        self._create_optimizer()\n            \n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n        for epoch in range(1,self.num_epochs+1):\n            # Generate training instances\n            if self.is_pairwise is True:\n                data_iter = PairwiseSampler(self.dataset, neg_num=1, batch_size=self.batch_size, shuffle=True, drop_last=False)\n            else:\n                data_iter = PointwiseSampler(self.dataset, neg_num=1, batch_size=self.batch_size, shuffle=True, drop_last=False)\n            \n            total_loss = 0.0\n            training_start_time = time()\n            num_training_instances = len(data_iter)\n            if self.is_pairwise is True:\n                for bat_users, bat_items_pos, bat_items_neg in data_iter:\n                    feed_dict = {self.user_input: bat_users,\n                                 self.item_input: bat_items_pos,\n                                 self.item_input_neg: bat_items_neg}\n                    loss, _ = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n                    total_loss += loss\n            else:\n                for bat_users, bat_items, bat_labels in data_iter:\n                    feed_dict = {self.user_input: bat_users,\n                                 self.item_input: bat_items,\n                                 self.labels: bat_labels}\n                    loss, _ = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n                    total_loss += loss\n            self.logger.info(""[iter %d : loss : %f, time: %f]"" % (epoch, total_loss/num_training_instances,\n                                                             time()-training_start_time))\n            if epoch % self.verbose == 0:\n                self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate()))\n                \n    # @timer\n    def evaluate(self):\n        return self.evaluator.evaluate(self)\n\n    def predict(self, user_ids, candidate_items_userids=None):\n        ratings = []\n        if candidate_items_userids is not None:\n            for u, i in zip(user_ids, candidate_items_userids):\n                users = np.full(len(i), u, dtype=np.int32)\n                feed_dict = {self.user_input: users, self: i}\n                ratings.append(self.sess.run(self.output, feed_dict=feed_dict))\n        else:\n            for u in user_ids:\n                users = np.full(self.num_items, u, dtype=np.int32)\n                feed_dict = {self.user_input: users, self.item_input: np.arange(self.num_items)}\n                ratings.append(self.sess.run(self.output, feed_dict=feed_dict))\n        return ratings\n'"
model/general_recommender/MultiDAE.py,14,"b'""""""\nReference: Dawen, Liang, et al. ""Variational autoencoders for collaborative filtering."" in WWW2018\n@author: wubin\n""""""\nimport tensorflow as tf\nimport numpy as np\nfrom time import time\nfrom util import learner, tool\nfrom tensorflow.contrib.layers import apply_regularization, l2_regularizer\nfrom model.AbstractRecommender import AbstractRecommender\nfrom util import timer\nfrom util.tool import csr_to_user_dict\n\n\nclass MultiDAE(AbstractRecommender):\n    def __init__(self, sess, dataset, conf):\n        super(MultiDAE, self).__init__(dataset, conf)\n        self.learning_rate = conf[""learning_rate""]\n        self.learner = conf[""learner""]\n        self.batch_size = conf[""batch_size""]\n        self.dataset = dataset\n        self.num_users = dataset.num_users\n        self.num_items = dataset.num_items  \n        self.p_dims = conf[""p_dim""] + [self.num_items]\n        self.q_dims = self.p_dims[::-1]\n        self.dims = self.q_dims + self.p_dims[1:]\n        self.act = conf[""activation""]\n        self.reg = conf[""reg""]\n        self.num_epochs = conf[""epochs""]\n        self.weight_init_method = conf[""weight_init_method""]\n        self.bias_init_method = conf[""bias_init_method""]\n        self.stddev = conf[""stddev""]\n        self.verbose = conf[""verbose""]\n        self.train_dict = csr_to_user_dict(dataset.train_matrix)\n        self.sess = sess\n\n    def _create_placeholders(self):\n        with tf.name_scope(""input_data""):\n            self.input_ph = tf.placeholder(dtype=tf.float32, shape=[None, self.num_items])\n            self.keep_prob_ph = tf.placeholder_with_default(1.0, shape=None)\n\n    def _create_variables(self):\n        with tf.name_scope(""embedding""):  # The embedding initialization is unknown now   \n            self.weights = []\n            self.biases = []\n            weight_initializer = tool.get_initializer(self.weight_init_method, self.stddev)\n            bias_initializer = tool.get_initializer(self.bias_init_method, self.stddev)\n            # define weights\n            for i, (d_in, d_out) in enumerate(zip(self.dims[:-1], self.dims[1:])):\n                weight_key = ""weight_{}to{}"".format(i, i+1)\n                bias_key = ""bias_{}"".format(i+1)\n                \n                self.weights.append(tf.Variable(weight_initializer([d_in, d_out]), name=weight_key, dtype=tf.float32))\n                \n                self.biases.append(tf.Variable(bias_initializer([d_out]), name=bias_key, dtype=tf.float32))\n    \n    def _create_inference(self):\n        with tf.name_scope(""inference""):\n            # construct forward graph        \n            self.h = tf.nn.l2_normalize(self.input_ph, 1)\n            self.h = tf.nn.dropout(self.h, self.keep_prob_ph)\n            \n            for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n                self.h = tf.matmul(self.h, w) + b\n                \n                if i != len(self.weights) - 1:\n                    self.h = tool.activation_function(self.act, self.h)\n                    \n            self.log_softmax_var = tf.nn.log_softmax(self.h)\n        \n    def _create_loss(self):\n        with tf.name_scope(""loss""):  \n            # per-user average negative log-likelihood \n            neg_ll = -tf.reduce_mean(tf.reduce_sum(self.log_softmax_var * self.input_ph, axis=1))\n            # apply regularization to weights\n            regularization = l2_regularizer(self.reg)\n            reg_var = apply_regularization(regularization, self.weights)\n            # tensorflow l2 regularization multiply 0.5 to the l2 norm\n            # multiply 2 so that it is back in the same scale\n            self.loss = neg_ll + 2 * reg_var   \n                \n    def _create_optimizer(self):\n        with tf.name_scope(""learner""):\n            self.optimizer = learner.optimizer(self.learner, self.loss, self.learning_rate)\n    \n    def build_graph(self):\n        self._create_placeholders()\n        self._create_variables()\n        self._create_inference()\n        self._create_loss()\n        self._create_optimizer()        \n            \n    def train_model(self):\n\n        for epoch in range(1, self.num_epochs+1):\n            random_perm_doc_idx = np.random.permutation(self.num_users)\n            self.total_batch = self.num_users\n            total_loss = 0.0\n            training_start_time = time()\n            num_training_instances = self.num_users\n            for num_batch in np.arange(int(num_training_instances/self.batch_size)):\n                if num_batch == self.total_batch - 1:\n                    batch_set_idx = random_perm_doc_idx[num_batch * self.batch_size:]\n                elif num_batch < self.total_batch - 1:\n                    batch_set_idx = random_perm_doc_idx[num_batch * self.batch_size: (num_batch + 1) * self.batch_size]\n                \n                batch_matrix = np.zeros((len(batch_set_idx), self.num_items))\n                \n                batch_uid = 0\n                for user_id in batch_set_idx:\n                    items_by_user_id = self.train_dict[user_id]\n                    for item_id in items_by_user_id:\n                        batch_matrix[batch_uid, item_id] = 1\n                        \n                    batch_uid = batch_uid+1\n                 \n                feed_dict = {self.input_ph: batch_matrix, self.keep_prob_ph: 0.5}\n                _, loss = self.sess.run([self.optimizer, self.loss], feed_dict=feed_dict)\n                total_loss += loss\n            self.logger.info(""[iter %d : loss : %f, time: %f]"" % (epoch, total_loss/num_training_instances,\n                                                             time()-training_start_time))\n            if epoch % self.verbose == 0:\n                self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate()))\n\n    @timer\n    def evaluate(self):\n        return self.evaluator.evaluate(self)\n\n    def predict(self, user_ids, candidate_items_user_ids):\n        ratings = []\n        if candidate_items_user_ids is not None:\n            rating_matrix = np.zeros((1, self.num_items), dtype=np.int32)\n            for user_id, candidate_items_user_id in zip(user_ids, candidate_items_user_ids):\n                items_by_user_id = self.dataset.train_matrix[user_id].indices\n                for item_id in items_by_user_id:\n                    rating_matrix[0, item_id] = 1\n                output = self.sess.run(self.h, feed_dict={self.input_ph:rating_matrix})\n                ratings.append(output[0, candidate_items_user_id])\n                \n        else:\n            rating_matrix = np.zeros((1, self.num_items), dtype=np.int32)\n            all_items = np.arange(self.num_items)\n            for user_id in user_ids:\n                items_by_user_id = self.dataset.train_matrix[user_id].indices\n                for item_id in items_by_user_id:\n                    rating_matrix[0, item_id] = 1\n                output = self.sess.run(self.h, feed_dict={self.input_ph:rating_matrix})\n                ratings.append(output[0, all_items])\n        return ratings\n'"
model/general_recommender/MultiVAE.py,22,"b'""""""\nReference: Dawen, Liang, et al. ""Variational autoencoders for collaborative filtering."" in WWW2018\n@author: wubin\n""""""\nimport tensorflow as tf\nimport numpy as np\nfrom time import time\nfrom util import learner, tool\nfrom tensorflow.contrib.layers import apply_regularization, l2_regularizer\nfrom model.AbstractRecommender import AbstractRecommender\nfrom util import timer\nfrom util.tool import csr_to_user_dict\n\n\nclass MultiVAE(AbstractRecommender):\n    def __init__(self, sess, dataset, conf):\n        super(MultiVAE, self).__init__(dataset, conf)\n        self.learning_rate = conf[""learning_rate""]\n        self.learner = conf[""learner""]\n        self.batch_size = conf[""batch_size""]\n        self.act = conf[""activation""]\n        self.reg = conf[""reg""]\n        self.num_epochs = conf[""epochs""]\n        self.anneal_cap = conf[""anneal_cap""]\n        self.total_anneal_steps = conf[""total_anneal_steps""]\n        self.weight_init_method = conf[""weight_init_method""]\n        self.bias_init_method = conf[""bias_init_method""]\n        self.stddev = conf[""stddev""]\n        self.verbose = conf[""verbose""]\n        self.dataset = dataset\n        self.num_users = dataset.num_users\n        self.num_items = dataset.num_items  \n        self.p_dims = conf[""p_dim""] + [self.num_items]\n        self.q_dims = self.p_dims[::-1]\n        self.dims = self.q_dims + self.p_dims[1:]\n        self.train_dict = csr_to_user_dict(dataset.train_matrix)\n        self.sess = sess\n\n    def _create_placeholders(self):\n        with tf.name_scope(""input_data""):\n            self.input_ph = tf.placeholder(dtype=tf.float32, shape=[None, self.num_items])\n            self.keep_prob_ph = tf.placeholder_with_default(1.0, shape=None)\n            self.is_training_ph = tf.placeholder_with_default(0., shape=None)\n            self.anneal_ph = tf.placeholder_with_default(1., shape=None)\n\n    def _create_variables(self):\n        with tf.name_scope(""embedding""):  # The embedding initialization is unknown now   \n            self.weights_q, self.biases_q = [], []\n            weight_initializer = tool.get_initializer(self.weight_init_method, self.stddev)\n            bias_initializer = tool.get_initializer(self.bias_init_method, self.stddev)\n            for i, (d_in, d_out) in enumerate(zip(self.q_dims[:-1], self.q_dims[1:])):\n                if i == len(self.q_dims[:-1]) - 1:\n                    # we need two sets of parameters for mean and variance,\n                    # respectively\n                    d_out *= 2\n                weight_key = ""weight_q_{}to{}"".format(i, i+1)\n                bias_key = ""bias_q_{}"".format(i+1)\n                \n                self.weights_q.append(tf.Variable(weight_initializer([d_in, d_out]), name=weight_key, dtype=tf.float32))\n                \n                self.biases_q.append(tf.Variable(bias_initializer([d_out]), name=bias_key, dtype=tf.float32))\n\n            self.weights_p, self.biases_p = [], []\n    \n            for i, (d_in, d_out) in enumerate(zip(self.p_dims[:-1], self.p_dims[1:])):\n                weight_key = ""weight_p_{}to{}"".format(i, i+1)\n                bias_key = ""bias_p_{}"".format(i+1)\n                \n                self.weights_p.append(tf.Variable(weight_initializer([d_in, d_out]), name=weight_key, dtype=tf.float32))\n                \n                self.biases_p.append(tf.Variable(bias_initializer([d_out]), name=bias_key, dtype=tf.float32))\n\n    def q_graph(self):\n        mu_q, std_q, KL = None, None, None\n        \n        h = tf.nn.l2_normalize(self.input_ph, 1)\n        h = tf.nn.dropout(h, self.keep_prob_ph)\n        \n        for i, (w, b) in enumerate(zip(self.weights_q, self.biases_q)):\n            h = tf.matmul(h, w) + b\n            \n            if i != len(self.weights_q) - 1:\n                h = tool.activation_function(self.act, h)\n            else:\n                mu_q = h[:, :self.q_dims[-1]]\n                logvar_q = h[:, self.q_dims[-1]:]  # log sigmod^2  batch x 200\n\n                std_q = tf.exp(0.5 * logvar_q)  # sigmod batch x 200\n                KL = tf.reduce_mean(tf.reduce_sum(0.5 * (-logvar_q + tf.exp(logvar_q) + tf.pow(mu_q,2) - 1), axis=1))\n        return mu_q, std_q, KL\n\n    def p_graph(self, z):\n        self.h = z\n        \n        for i, (w, b) in enumerate(zip(self.weights_p, self.biases_p)):\n            self.h = tf.matmul(self.h, w) + b\n            \n            if i != len(self.weights_p) - 1:\n                self.h = tool.activation_function(self.act, self.h)\n        return self.h\n\n    def _create_inference(self):\n        with tf.name_scope(""inference""):\n            # q-network\n            mu_q, std_q, self.KL = self.q_graph()\n            epsilon = tf.random_normal(tf.shape(std_q), mean=0, stddev=0.01)\n    \n            sampled_z = mu_q + self.is_training_ph * epsilon * std_q  # batch x 200\n    \n            # p-network\n            logits = self.p_graph(sampled_z)\n            \n            self.log_softmax_var = tf.nn.log_softmax(logits)\n               \n    def _create_loss(self):\n        with tf.name_scope(""loss""):  \n            neg_ll = -tf.reduce_mean(tf.reduce_sum(self.log_softmax_var * self.input_ph, axis=1))\n            # apply regularization to weights\n            reg = l2_regularizer(self.reg)\n            \n            reg_var = apply_regularization(reg, self.weights_q + self.weights_p)\n            # tensorflow l2 regularization multiply 0.5 to the l2 norm\n            # multiply 2 so that it is back in the same scale\n            self.loss = neg_ll + self.anneal_ph * self.KL + 2 * reg_var  # neg_ELBO\n                \n    def _create_optimizer(self):\n        with tf.name_scope(""learner""):\n            self.optimizer = learner.optimizer(self.learner, self.loss, self.learning_rate)\n    \n    def build_graph(self):\n        self._create_placeholders()\n        self._create_variables()\n        self._create_inference()\n        self._create_loss()\n        self._create_optimizer()        \n            \n    def train_model(self):\n        update_count = 0.0\n        self.logger.info(self.evaluator.metrics_info())\n        for epoch in range(1,self.num_epochs+1):\n            random_perm_doc_idx = np.random.permutation(self.num_users)\n            self.total_batch = self.num_users\n            total_loss = 0.0\n            training_start_time = time()\n            num_training_instances = self.num_users\n            for num_batch in np.arange(int(num_training_instances/self.batch_size)):\n                if num_batch == self.total_batch - 1:\n                    batch_set_idx = random_perm_doc_idx[num_batch * self.batch_size:]\n                elif num_batch < self.total_batch - 1:\n                    batch_set_idx = random_perm_doc_idx[num_batch * self.batch_size: (num_batch + 1) * self.batch_size]\n                \n                batch_matrix = np.zeros((len(batch_set_idx), self.num_items))\n                \n                if self.total_anneal_steps > 0:\n                    anneal = min(self.anneal_cap, 1. * update_count / self.total_anneal_steps)\n                else:\n                    anneal = self.anneal_cap\n                \n                batch_uid = 0\n                for user_id in batch_set_idx:\n                    items_by_user_id = self.train_dict[user_id]\n                    for item_id in items_by_user_id:\n                        batch_matrix[batch_uid, item_id] = 1\n                        \n                    batch_uid = batch_uid+1\n                 \n                feed_dict = {self.input_ph: batch_matrix,\n                             self.keep_prob_ph: 0.8,\n                             self.anneal_ph: anneal,\n                             self.is_training_ph: 1}\n                _, loss = self.sess.run([self.optimizer, self.loss],feed_dict=feed_dict)\n                total_loss += loss\n                \n                update_count += 1\n            self.logger.info(""[iter %d : loss : %f, time: %f]"" % (epoch, total_loss/num_training_instances,\n                                                             time()-training_start_time))\n            if epoch % self.verbose == 0:\n                self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate()))\n\n    @timer\n    def evaluate(self):\n        return self.evaluator.evaluate(self)\n\n    def predict(self, user_ids, candidate_items_user_ids):\n        ratings = []\n        if candidate_items_user_ids is not None:\n            rating_matrix = np.zeros((1, self.num_items), dtype=np.int32)\n            for user_id, candidate_items_user_id in zip(user_ids, candidate_items_user_ids):\n                items_by_user_id = self.dataset.train_matrix[user_id].indices\n                for item_id in items_by_user_id:\n                    rating_matrix[0, item_id] = 1\n                output = self.sess.run(self.h, feed_dict={self.input_ph: rating_matrix})\n                ratings.append(output[0, candidate_items_user_id])\n                \n        else:\n            rating_matrix = np.zeros((1, self.num_items), dtype=np.int32)\n            all_items = np.arange(self.num_items)\n            for user_id in user_ids:\n                items_by_user_id = self.dataset.train_matrix[user_id].indices\n                for item_id in items_by_user_id:\n                    rating_matrix[0, item_id] = 1\n                output = self.sess.run(self.h, feed_dict={self.input_ph: rating_matrix})\n                ratings.append(output[0, all_items])\n        return ratings\n'"
model/general_recommender/NAIS.py,51,"b'""""""\nReference: Xiangnan He et al., ""NAIS: Neural Attentive Item Similarity Model for Recommendation."" in TKDE2018\n@author: wubin\n""""""\nfrom model.AbstractRecommender import AbstractRecommender\nimport tensorflow as tf\nimport numpy as np\nfrom time import time\nfrom util import learner,data_generator, tool\nfrom util import timer\nfrom util.tool import csr_to_user_dict\nimport pickle\nfrom util import l2_loss\nfrom util import pad_sequences\nfrom util.data_iterator import DataIterator\n\n\nclass NAIS(AbstractRecommender):\n    def __init__(self, sess, dataset, conf):\n        super(NAIS, self).__init__(dataset, conf)\n        self.pretrain = conf[""pretrain""]\n        self.verbose = conf[""verbose""]\n        self.batch_size = conf[""batch_size""]\n        self.num_epochs = conf[""epochs""]\n        self.weight_size = conf[""weight_size""]\n        self.embedding_size = conf[""embedding_size""]\n        self.data_alpha = conf[""data_alpha""]\n        self.regs = conf[""regs""]\n        self.is_pairwise = conf[""is_pairwise""]\n        self.topK = conf[""topk""]\n        self.lambda_bilinear = self.regs[0]\n        self.gamma_bilinear = self.regs[1]\n        self.eta_bilinear = self.regs[2] \n        self.alpha = conf[""alpha""]\n        self.beta = conf[""beta""]\n        self.num_negatives = conf[""num_neg""]\n        self.learning_rate = conf[""learning_rate""]\n        self.activation = conf[""activation""]\n        self.loss_function = conf[""loss_function""]\n        self.algorithm = conf[""algorithm""]\n        self.learner = conf[""learner""]\n        self.embed_init_method = conf[""embed_init_method""]\n        self.weight_init_method = conf[""weight_init_method""]\n        self.stddev = conf[""stddev""]\n        self.pretrain_file = conf[""pretrain_file""]\n        self.dataset = dataset\n        self.num_items = dataset.num_items\n        self.num_users = dataset.num_users\n        self.train_dict = csr_to_user_dict(self.dataset.train_matrix)\n        self.sess = sess\n\n    def _create_placeholders(self):\n        with tf.name_scope(""input_data""):\n            self.user_input = tf.placeholder(tf.int32, shape=[None, None], name=""user_input"")  # the index of users\n            self.num_idx = tf.placeholder(tf.float32, shape=[None], name=""num_idx"")  # the number of items rated by users\n            self.item_input = tf.placeholder(tf.int32, shape=[None], name=""item_input_pos"")  # the index of items\n            if self.is_pairwise is True:\n                self.user_input_neg = tf.placeholder(tf.int32, shape=[None, None], name=""user_input_neg"")\n                self.item_input_neg = tf.placeholder(tf.int32, shape=[None], name=""item_input_neg"")\n                self.num_idx_neg = tf.placeholder(tf.float32, shape=[None], name=""num_idx_neg"")\n            else:\n                self.labels = tf.placeholder(tf.float32, shape=[None], name=""labels"")\n\n    def _create_variables(self, params=None):\n        with tf.name_scope(""embedding""):  # The embedding initialization is unknown now\n            if params is None:\n                embed_initializer = tool.get_initializer(self.embed_init_method, self.stddev)\n                \n                self.c1 = tf.Variable(embed_initializer([self.num_items, self.embedding_size]),\n                                      name=\'c1\', dtype=tf.float32)\n                self.embedding_Q = tf.Variable(embed_initializer([self.num_items, self.embedding_size]),\n                                               name=\'embedding_Q\', dtype=tf.float32)\n                self.bias = tf.Variable(tf.zeros(self.num_items), name=\'bias\')\n            else:\n                self.c1 = tf.Variable(params[0], name=\'c1\', dtype=tf.float32)\n                self.embedding_Q = tf.Variable(params[1], name=\'embedding_Q\', dtype=tf.float32)\n                self.bias = tf.Variable(params[2], name=""bias"", dtype=tf.float32)\n                \n            self.c2 = tf.constant(0.0, tf.float32, [1, self.embedding_size], name=\'c2\')\n            self.embedding_Q_ = tf.concat([self.c1, self.c2], axis=0, name=\'embedding_Q_\')\n\n            # Variables for attention\n            weight_initializer = tool.get_initializer(self.weight_init_method, self.stddev)\n            if self.algorithm == 0:\n                self.W = tf.Variable(weight_initializer([self.embedding_size, self.weight_size]),\n                                     name=\'Weights_for_MLP\', dtype=tf.float32, trainable=True)\n            else:    \n                self.W = tf.Variable(weight_initializer([2*self.embedding_size, self.weight_size]),\n                                     name=\'Weights_for_MLP\', dtype=tf.float32, trainable=True)\n            \n            self.b = tf.Variable(weight_initializer([1, self.weight_size]),\n                                 name=\'Bias_for_MLP\', dtype=tf.float32, trainable=True)\n            \n            self.h = tf.Variable(tf.ones([self.weight_size, 1]), name=\'H_for_MLP\', dtype=tf.float32)\n            \n    def _create_inference(self, user_input, item_input, num_idx):\n        with tf.name_scope(""inference""):\n            embedding_q_ = tf.nn.embedding_lookup(self.embedding_Q_, user_input)  # (b, n, e)\n            embedding_q = tf.expand_dims(tf.nn.embedding_lookup(self.embedding_Q, item_input), 1)  # (b, 1, e)\n            \n            if self.algorithm == 0:\n                embedding_p = self._attention_mlp(embedding_q_ * embedding_q, embedding_q_, num_idx)\n            else:\n                n = tf.shape(user_input)[1]\n                embedding_p = self._attention_mlp(tf.concat([embedding_q_, tf.tile(embedding_q, tf.stack([1, n, 1]))], 2),\n                                                  embedding_q_, num_idx)\n\n            embedding_q = tf.reduce_sum(embedding_q, 1)\n            bias_i = tf.nn.embedding_lookup(self.bias, item_input)\n            coeff = tf.pow(num_idx, tf.constant(self.alpha, tf.float32, [1]))\n            output = coeff * tf.reduce_sum(embedding_p*embedding_q, 1) + bias_i\n            \n            return embedding_q_, embedding_q, output\n    \n    def _create_loss(self):\n        with tf.name_scope(""loss""):\n            p1, q1, self.output = self._create_inference(self.user_input,self.item_input,self.num_idx)\n            if self.is_pairwise is True:\n                _, q2, output_neg = self._create_inference(self.user_input_neg, self.item_input_neg, self.num_idx_neg)\n                self.result = self.output - output_neg\n                self.loss = learner.pairwise_loss(self.loss_function, self.result) + \\\n                            self.lambda_bilinear * l2_loss(p1) + \\\n                            self.gamma_bilinear * l2_loss(q2, q1)\n            \n            else:\n                self.loss = learner.pointwise_loss(self.loss_function, self.labels, self.output) + \\\n                            self.lambda_bilinear * l2_loss(p1) + \\\n                            self.gamma_bilinear * l2_loss(q1)\n\n    def _create_optimizer(self):\n        with tf.name_scope(""learner""):\n            self.optimizer = learner.optimizer(self.learner, self.loss, self.learning_rate)\n            \n    def build_graph(self):\n        self._create_placeholders()\n        try:\n            pre_trained_params = []\n            with open(self.pretrain_file, ""rb"") as fin:\n                pre_trained_params.append(pickle.load(fin, encoding=""utf-8""))\n            with open(self.mlp_pretrain, ""rb"") as fin:\n                pre_trained_params.append(pickle.load(fin, encoding=""utf-8""))\n            self.logger.info(""load pretrained params successful!"")\n        except:\n            pre_trained_params = None\n            self.logger.info(""load pretrained params unsuccessful!"")\n            \n        self._create_variables(pre_trained_params)\n        self._create_loss()\n        self._create_optimizer()\n\n    def _attention_mlp(self, q_, embedding_q_, num_idx):\n            with tf.name_scope(""attention_MLP""):\n                b = tf.shape(q_)[0]\n                n = tf.shape(q_)[1]\n                r = (self.algorithm + 1)*self.embedding_size\n\n                mlp_output = tf.matmul(tf.reshape(q_, [-1, r]), self.W) + self.b  # (b*n, e or 2*e) * (e or 2*e, w) + (1, w)\n                if self.activation == 0:\n                    mlp_output = tf.nn.relu(mlp_output)\n                elif self.activation == 1:\n                    mlp_output = tf.nn.sigmoid(mlp_output)\n                elif self.activation == 2:\n                    mlp_output = tf.nn.tanh(mlp_output)\n    \n                A_ = tf.reshape(tf.matmul(mlp_output, self.h), [b,n])  # (b*n, w) * (w, 1) => (None, 1) => (b, n)\n    \n                # softmax for not mask features\n                exp_A_ = tf.exp(A_)\n                mask_mat = tf.sequence_mask(num_idx, maxlen = n, dtype=tf.float32)  # (b, n)\n                exp_A_ = mask_mat * exp_A_\n                exp_sum = tf.reduce_sum(exp_A_, 1, keepdims=True)  # (b, 1)\n                exp_sum = tf.pow(exp_sum, tf.constant(self.beta, tf.float32, [1]))\n    \n                A = tf.expand_dims(tf.div(exp_A_, exp_sum), 2)  # (b, n, 1)\n    \n                return tf.reduce_sum(A * embedding_q_, 1)\n\n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n        for epoch in range(1, self.num_epochs+1):\n            if self.is_pairwise is True:\n                user_input, user_input_neg, num_idx_pos, num_idx_neg, item_input_pos, item_input_neg = \\\n                    data_generator._get_pairwise_all_likefism_data(self.dataset)\n                data_iter = DataIterator(user_input, user_input_neg, num_idx_pos,\n                                         num_idx_neg, item_input_pos, item_input_neg,\n                                         batch_size=self.batch_size, shuffle=True)\n            else:\n                user_input, num_idx, item_input, labels = \\\n                 data_generator._get_pointwise_all_likefism_data(self.dataset, self.num_negatives, self.train_dict)\n                data_iter = DataIterator(user_input, num_idx, item_input, labels,\n                                         batch_size=self.batch_size, shuffle=True)\n           \n            num_training_instances = len(user_input)\n            total_loss = 0.0\n            training_start_time = time()\n            if self.is_pairwise is True:\n                for bat_users_pos, bat_users_neg, bat_idx_pos, bat_idx_neg, bat_items_pos, bat_items_neg in data_iter:\n                    bat_users_pos = pad_sequences(bat_users_pos, value=self.num_items)\n                    bat_users_neg = pad_sequences(bat_users_neg, value=self.num_items)\n                    feed_dict = {self.user_input: bat_users_pos,\n                                 self.user_input_neg: bat_users_neg,\n                                 self.num_idx: bat_idx_pos,\n                                 self.num_idx_neg: bat_idx_neg,\n                                 self.item_input: bat_items_pos,\n                                 self.item_input_neg: bat_items_neg}\n\n                    loss, _ = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n                    total_loss += loss\n            else:\n                for bat_users, bat_idx, bat_items, bat_labels in data_iter:\n                    bat_users = pad_sequences(bat_users, value=self.num_items)\n                    feed_dict = {self.user_input: bat_users,\n                                 self.num_idx: bat_idx,\n                                 self.item_input: bat_items,\n                                 self.labels: bat_labels}\n                    loss, _ = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n                    total_loss += loss\n\n            self.logger.info(""[iter %d : loss : %f, time: %f]"" % (epoch, total_loss/num_training_instances,\n                                                             time()-training_start_time))\n            if epoch % self.verbose == 0:\n                self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate()))\n        \n        # save model\n        # params = self.sess.run([self.c1, self.embedding_Q, self.bias])\n        # with open(""./pretrained/%s_epoch=%d_fism.pkl"" % (self.dataset_name, self.num_epochs), ""wb"") as fout:\n        #     pickle.dump(params, fout)\n    @timer\n    def evaluate(self):\n        return self.evaluator.evaluate(self)\n    \n    def predict(self, user_ids, candidate_items_userids):      \n        ratings = []\n        if candidate_items_userids is not None:\n            for u, eval_items_by_u in zip(user_ids, candidate_items_userids):\n                user_input = []\n                cand_items_by_u = self.train_dict[u]\n                num_idx = len(cand_items_by_u)\n                item_idx = np.full(len(eval_items_by_u), num_idx, dtype=np.int32)\n                user_input.extend([cand_items_by_u]*len(eval_items_by_u))\n                feed_dict = {self.user_input: user_input,\n                             self.num_idx: item_idx, \n                             self.item_input: eval_items_by_u}\n                ratings.append(self.sess.run(self.output, feed_dict=feed_dict))\n                \n        else:\n            eval_items = np.arange(self.num_items)\n            for u in user_ids:\n                user_input = []\n                cand_items_by_u = self.train_dict[u]\n                num_idx = len(cand_items_by_u)\n                item_idx = np.full(self.num_items, num_idx, dtype=np.int32)\n                user_input.extend([cand_items_by_u]*self.num_items)\n                feed_dict = {self.user_input: user_input,\n                             self.num_idx: item_idx, \n                             self.item_input: eval_items}\n                ratings.append(self.sess.run(self.output, feed_dict=feed_dict))\n        return ratings\n'"
model/general_recommender/NGCF.py,64,"b'""""""\nReference: Wang Xiang et al. ""Neural Graph Collaborative Filtering."" in SIGIR2019\nSource code: https://github.com/xiangwang1223/neural_graph_collaborative_filtering\n@author: wubin\n""""""\nimport tensorflow as tf\nimport numpy as np\nimport scipy.sparse as sp\nfrom time import time\nfrom util import learner, tool\nfrom model.AbstractRecommender import AbstractRecommender\nfrom util import timer\nfrom util import l2_loss\nfrom data import PairwiseSampler\n\nclass NGCF(AbstractRecommender):\n    def __init__(self, sess, dataset, conf):\n        super(NGCF, self).__init__(dataset, conf)\n        self.learning_rate = conf[""learning_rate""]\n        self.learner = conf[""learner""]\n        self.batch_size = conf[""batch_size""]\n        self.emb_dim = conf[""embedding_size""]\n        self.weight_size = conf[""layer_size""]\n        self.n_layers = len(self.weight_size)\n        self.num_epochs = conf[""epochs""]\n        self.reg = conf[""reg""]\n        self.adj_type = conf[""adj_type""]\n        self.alg_type = conf[""alg_type""]\n        self.node_dropout_flag = conf[""node_dropout_flag""]\n        self.node_dropout_ratio = conf[""node_dropout_ratio""]\n        self.mess_dropout_ratio = conf[""mess_dropout_ratio""]\n        self.n_fold = 100\n        self.embed_init_method = conf[""embed_init_method""]\n        self.weight_init_method = conf[""weight_init_method""]\n        self.stddev = conf[""stddev""]\n        self.verbose = conf[""verbose""]\n        self.dataset = dataset\n        self.num_users = dataset.num_users\n        self.num_items = dataset.num_items    \n        self.graph = dataset.train_matrix.toarray()\n        self.norm_adj = self.get_adj_mat()\n        self.n_nonzero_elems = self.norm_adj.count_nonzero()\n        self.pre_train_data = None\n        self.sess = sess\n\n    def _create_placeholders(self):\n        with tf.name_scope(""input_data""):\n            # placeholder definition\n            self.user_input = tf.placeholder(tf.int32, shape=(None,))\n            self.pos_items = tf.placeholder(tf.int32, shape=(None,))\n            self.neg_items = tf.placeholder(tf.int32, shape=(None,))\n            \n    def _create_variables(self):\n        with tf.name_scope(""embedding""):  # The embedding initialization is unknown now   \n            """"""\n            *********************************************************\n            Compute Graph-based Representations of all users & items via Message-Passing Mechanism of Graph Neural Networks.\n            Different Convolutional Layers:\n                1. ngcf: defined in \'Neural Graph Collaborative Filtering\', SIGIR2019;\n                2. gcn:  defined in \'Semi-Supervised Classification with Graph Convolutional Networks\', ICLR2018;\n                3. gcmc: defined in \'Graph Convolutional Matrix Completion\', KDD2018;\n            """"""\n            # initialization of model parameters\n            self.weights = self._init_weights()\n            \n            if self.alg_type in [\'ngcf\']:\n                self.ua_embeddings, self.ia_embeddings = self._create_ngcf_embed()\n    \n            elif self.alg_type in [\'gcn\']:\n                self.ua_embeddings, self.ia_embeddings = self._create_gcn_embed()\n    \n            elif self.alg_type in [\'gcmc\']:\n                self.ua_embeddings, self.ia_embeddings = self._create_gcmc_embed()\n\n    def _create_inference(self):\n        with tf.name_scope(""inference""):\n            """"""\n            *********************************************************\n            Establish the final representations for user-item pairs in batch.\n            """"""\n            self.u_g_embeddings = tf.nn.embedding_lookup(self.ua_embeddings, self.user_input)\n            self.pos_i_g_embeddings = tf.nn.embedding_lookup(self.ia_embeddings, self.pos_items)\n            self.neg_i_g_embeddings = tf.nn.embedding_lookup(self.ia_embeddings, self.neg_items)\n    \n            """"""\n            *********************************************************\n            Inference for the testing phase.\n            """"""\n            self.batch_ratings = tf.matmul(self.u_g_embeddings, self.pos_i_g_embeddings, transpose_a=False, transpose_b=True)\n\n    def _create_loss(self):\n        with tf.name_scope(""loss""): \n            \n            self.pos_scores = tf.reduce_sum(tf.multiply(self.u_g_embeddings, self.pos_i_g_embeddings), axis=1)\n            neg_scores = tf.reduce_sum(tf.multiply(self.u_g_embeddings, self.neg_i_g_embeddings), axis=1)\n    \n            embedding_regularizer = l2_loss(self.u_g_embeddings, self.pos_i_g_embeddings, self.neg_i_g_embeddings)\n\n            maxi = tf.nn.softplus(-(self.pos_scores - neg_scores))\n            mf_loss = tf.reduce_sum(maxi)\n    \n            emb_loss = self.reg * embedding_regularizer\n            \n#             for k in range(self.n_layers):\n#                 emb_loss = emb_loss + self.reg_w*(tf.reduce_sum(tf.square(self.weights[\'W_gc_%d\' % k])) +\n#                                                   tf.reduce_sum(tf.square(self.weights[\'W_bi_%d\' % k])) +\n#                                                   tf.reduce_sum(tf.square(self.weights[\'b_gc_%d\' % k])) +\n#                                                   tf.reduce_sum(tf.square(self.weights[\'b_bi_%d\' % k])))\n\n            self.loss = mf_loss + emb_loss\n\n    def _create_optimizer(self):\n        with tf.name_scope(""learner""):\n            self.optimizer = learner.optimizer(self.learner, self.loss, self.learning_rate)\n    \n    def build_graph(self):\n        self._create_placeholders()\n        self._create_variables()\n        self._create_inference()\n        self._create_loss()\n        self._create_optimizer()\n        \n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n        data_iter = PairwiseSampler(self.dataset, neg_num=1, batch_size=self.batch_size, shuffle=True)\n        for epoch in  range(1,self.num_epochs+1):\n            total_loss = 0.0\n            training_start_time = time()\n            num_training_instances = len(data_iter)\n            for bat_users, bat_items_pos, bat_items_neg in data_iter:\n                    feed_dict = {self.user_input: bat_users,\n                                 self.pos_items: bat_items_pos,\n                                 self.neg_items: bat_items_neg}\n                    loss, _ = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n                    total_loss += loss\n\n            self.logger.info(""[iter %d : loss : %f, time: %f]"" % (epoch, total_loss/num_training_instances,\n                                                             time()-training_start_time))\n            if epoch % self.verbose == 0:\n                self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate()))\n                \n    @timer\n    def evaluate(self):\n        self._cur_user_embeddings, self._cur_item_embeddings = self.sess.run([self.ua_embeddings, self.ia_embeddings])\n        return self.evaluator.evaluate(self)\n\n    def predict(self, user_ids, candidate_items_userids):\n        if candidate_items_userids is None:\n            user_embed = self._cur_user_embeddings[user_ids]\n            ratings = np.matmul(user_embed, self._cur_item_embeddings.T)\n        else:\n            ratings = []\n            for userid, items_by_userid in zip(user_ids, candidate_items_userids):\n                user_embed = self._cur_user_embeddings[userid]\n                items_embed = self._cur_item_embeddings[items_by_userid]\n                ratings.append(np.squeeze(np.matmul(user_embed, items_embed.T)))\n            \n        return ratings\n    \n    def _create_ngcf_embed(self):\n        # Generate a set of adjacency sub-matrix.\n        if self.node_dropout_flag is True:\n            # node dropout.\n            A_fold_hat = self._split_A_hat_node_dropout(self.norm_adj)\n        else:\n            A_fold_hat = self._split_A_hat(self.norm_adj)\n\n        ego_embeddings = tf.concat([self.weights[\'user_embedding\'], self.weights[\'item_embedding\']], axis=0)\n\n        all_embeddings = [ego_embeddings]\n\n        for k in range(0, self.n_layers):\n\n            temp_embed = []\n            for f in range(self.n_fold):\n                temp_embed.append(tf.sparse_tensor_dense_matmul(A_fold_hat[f], ego_embeddings))\n\n            # sum messages of neighbors.\n            side_embeddings = tf.concat(temp_embed, 0)\n            # transformed sum messages of neighbors.\n            sum_embeddings = tf.nn.leaky_relu(\n                tf.matmul(side_embeddings, self.weights[\'W_gc_%d\' % k]) + self.weights[\'b_gc_%d\' % k])\n\n            # bi messages of neighbors.\n            bi_embeddings = tf.multiply(ego_embeddings, side_embeddings)\n            # transformed bi messages of neighbors.\n            bi_embeddings = tf.nn.leaky_relu(\n                tf.matmul(bi_embeddings, self.weights[\'W_bi_%d\' % k]) + self.weights[\'b_bi_%d\' % k])\n\n            # non-linear activation.\n            ego_embeddings = sum_embeddings + bi_embeddings\n            # message dropout.\n            ego_embeddings = tf.nn.dropout(ego_embeddings, 1-self.mess_dropout_ratio)\n\n            # normalize the distribution of embeddings.\n            norm_embeddings = tf.nn.l2_normalize(ego_embeddings, axis=1)\n\n            all_embeddings += [norm_embeddings]\n\n        all_embeddings = tf.concat(all_embeddings, 1)\n        u_g_embeddings, i_g_embeddings = tf.split(all_embeddings, [self.num_users, self.num_items], 0)\n        return u_g_embeddings, i_g_embeddings\n\n    def _create_gcn_embed(self):\n        A_fold_hat = self._split_A_hat(self.norm_adj)\n        embeddings = tf.concat([self.weights[\'user_embedding\'], self.weights[\'item_embedding\']], axis=0)\n\n        all_embeddings = [embeddings]\n\n        for k in range(0, self.n_layers):\n            temp_embed = []\n            for f in range(self.n_fold):\n                temp_embed.append(tf.sparse_tensor_dense_matmul(A_fold_hat[f], embeddings))\n\n            embeddings = tf.concat(temp_embed, 0)\n            embeddings = tf.nn.leaky_relu(tf.matmul(embeddings, self.weights[\'W_gc_%d\' %k]) + self.weights[\'b_gc_%d\' %k])\n                \n            embeddings = tf.nn.dropout(embeddings, 1 - self.mess_dropout_ratio)\n\n            all_embeddings += [embeddings]\n\n        all_embeddings = tf.concat(all_embeddings, 1)\n        u_g_embeddings, i_g_embeddings = tf.split(all_embeddings, [self.num_users, self.num_items], 0)\n        return u_g_embeddings, i_g_embeddings\n\n    def _create_gcmc_embed(self):\n        A_fold_hat = self._split_A_hat(self.norm_adj)\n\n        embeddings = tf.concat([self.weights[\'user_embedding\'], self.weights[\'item_embedding\']], axis=0)\n\n        all_embeddings = []\n\n        for k in range(0, self.n_layers):\n            temp_embed = []\n            for f in range(self.n_fold):\n                temp_embed.append(tf.sparse_tensor_dense_matmul(A_fold_hat[f], embeddings))\n            embeddings = tf.concat(temp_embed, 0)\n            # convolutional layer.\n            embeddings = tf.nn.leaky_relu(tf.matmul(embeddings, self.weights[\'W_gc_%d\' % k]) + self.weights[\'b_gc_%d\' % k])\n            # dense layer.\n            mlp_embeddings = tf.matmul(embeddings, self.weights[\'W_mlp_%d\' %k]) + self.weights[\'b_mlp_%d\' %k]\n            mlp_embeddings = tf.nn.dropout(mlp_embeddings, 1 - self.mess_dropout_ratio)\n\n            all_embeddings += [mlp_embeddings]\n        all_embeddings = tf.concat(all_embeddings, 1)\n\n        u_g_embeddings, i_g_embeddings = tf.split(all_embeddings, [self.num_users, self.num_items], 0)\n        return u_g_embeddings, i_g_embeddings\n        \n    def _init_weights(self):\n        all_weights = dict()\n        embed_initializer = tool.get_initializer(self.embed_init_method, self.stddev)\n        weight_initializer = tool.get_initializer(self.weight_init_method, self.stddev)\n\n        if self.pre_train_data is None:\n            all_weights[\'user_embedding\'] = tf.Variable(embed_initializer([self.num_users, self.emb_dim]),\n                                                        name=\'user_embedding\')\n            all_weights[\'item_embedding\'] = tf.Variable(embed_initializer([self.num_items, self.emb_dim]),\n                                                        name=\'item_embedding\')\n            self.logger.info(\'using xavier initialization\')\n        else:\n            all_weights[\'user_embedding\'] = tf.Variable(initial_value=self.pre_train_data[\'user_embed\'], trainable=True,\n                                                        name=\'user_embedding\', dtype=tf.float32)\n            all_weights[\'item_embedding\'] = tf.Variable(initial_value=self.pre_train_data[\'item_embed\'], trainable=True,\n                                                        name=\'item_embedding\', dtype=tf.float32)\n            self.logger.info(\'using pretrained initialization\')\n\n        self.weight_size_list = [self.emb_dim] + self.weight_size\n\n        for k in range(self.n_layers):\n            all_weights[\'W_gc_%d\' % k] = tf.Variable(\n                weight_initializer([self.weight_size_list[k], self.weight_size_list[k+1]]), name=\'W_gc_%d\' % k)\n            all_weights[\'b_gc_%d\' % k] = tf.Variable(\n                weight_initializer([1, self.weight_size_list[k+1]]), name=\'b_gc_%d\' % k)\n\n            all_weights[\'W_bi_%d\' % k] = tf.Variable(\n                weight_initializer([self.weight_size_list[k], self.weight_size_list[k + 1]]), name=\'W_bi_%d\' % k)\n            all_weights[\'b_bi_%d\' % k] = tf.Variable(\n                weight_initializer([1, self.weight_size_list[k + 1]]), name=\'b_bi_%d\' % k)\n\n            all_weights[\'W_mlp_%d\' % k] = tf.Variable(\n                weight_initializer([self.weight_size_list[k], self.weight_size_list[k+1]]), name=\'W_mlp_%d\' % k)\n            all_weights[\'b_mlp_%d\' % k] = tf.Variable(\n                weight_initializer([1, self.weight_size_list[k+1]]), name=\'b_mlp_%d\' % k)\n\n        return all_weights\n     \n    def normalized_adj_single(self,adj):\n        rowsum = np.array(adj.sum(1))\n\n        d_inv = np.power(rowsum, -1).flatten()\n        d_inv[np.isinf(d_inv)] = 0.\n        d_mat_inv = sp.diags(d_inv)\n        norm_adj = d_mat_inv.dot(adj)\n        # norm_adj = adj.dot(d_mat_inv)\n        self.logger.info(\'generate single-normalized adjacency matrix.\')\n        return norm_adj.tocoo()\n\n    def get_adj_mat(self):\n        A = sp.dok_matrix((self.num_users + self.num_items, self.num_users + self.num_items), dtype=np.float32)\n        A = A.tolil()\n        A[:self.num_users, self.num_users:] = self.graph\n        A[self.num_users:, :self.num_users] = self.graph.transpose()\n        A = A.todok()\n        if self.adj_type == \'plain\':\n            adj_mat = A\n            self.logger.info(\'use the plain adjacency matrix\')\n        elif self.adj_type == \'norm\':  \n            adj_mat = self.normalized_adj_single(A + sp.eye(A.shape[0]))\n            self.logger.info(\'use the normalized adjacency matrix\')\n        elif self.adj_type == \'gcmc\':\n            adj_mat = self.normalized_adj_single(A)\n            self.logger.info(\'use the gcmc adjacency matrix\')\n        else:\n            adj_mat = self.normalized_adj_single(A) + sp.eye(A.shape[0])\n            self.logger.info(\'use the mean adjacency matrix\')\n    \n        return adj_mat.tocsr()\n    \n    def _split_A_hat(self, X):\n        A_fold_hat = []\n\n        fold_len = (self.num_users + self.num_items) // self.n_fold\n        for i_fold in range(self.n_fold):\n            start = i_fold * fold_len\n            if i_fold == self.n_fold -1:\n                end = self.num_users + self.num_items\n            else:\n                end = (i_fold + 1) * fold_len\n\n            A_fold_hat.append(self._convert_sp_mat_to_sp_tensor(X[start:end]))\n        return A_fold_hat\n\n    def _split_A_hat_node_dropout(self, X):\n        A_fold_hat = []\n\n        fold_len = (self.num_users + self.num_items) // self.n_fold\n        for i_fold in range(self.n_fold):\n            start = i_fold * fold_len\n            if i_fold == self.n_fold -1:\n                end = self.num_users + self.num_items\n            else:\n                end = (i_fold + 1) * fold_len\n\n            # A_fold_hat.append(self._convert_sp_mat_to_sp_tensor(X[start:end]))\n            temp = self._convert_sp_mat_to_sp_tensor(X[start:end])\n            n_nonzero_temp = X[start:end].count_nonzero()\n            A_fold_hat.append(self._dropout_sparse(temp, 1 - self.node_dropout_ratio, n_nonzero_temp))\n        return A_fold_hat\n    \n    def _dropout_sparse(self, X, keep_prob, n_nonzero_elems):\n        """"""\n        Dropout for sparse tensors.\n        """"""\n        noise_shape = [n_nonzero_elems]\n        random_tensor = keep_prob\n        random_tensor += tf.random_uniform(noise_shape)\n        dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n        pre_out = tf.sparse_retain(X, dropout_mask)\n\n        return pre_out * tf.div(1., keep_prob)\n    \n    def _convert_sp_mat_to_sp_tensor(self, X):\n        coo = X.tocoo().astype(np.float32)\n        indices = np.mat([coo.row, coo.col]).transpose()\n        return tf.SparseTensor(indices, coo.data, coo.shape)\n'"
model/general_recommender/NeuMF.py,29,"b'""""""\nXiangnan He et al., ""Neural Collaborative Filtering."" in WWW 2017.\n@author: WuBin\n""""""\nfrom model.AbstractRecommender import AbstractRecommender\nimport tensorflow as tf\nimport numpy as np\nfrom time import time\nfrom util import learner, tool\nfrom util import timer\nimport pickle\nfrom util import l2_loss\nfrom data import PairwiseSampler, PointwiseSampler\n\n\nclass NeuMF(AbstractRecommender):\n    def __init__(self, sess, dataset, conf):\n        super(NeuMF, self).__init__(dataset, conf)\n        self.embedding_size = conf[""embedding_size""]\n        self.layers = conf[""layers""]\n        self.reg_mf = conf[""reg_mf""]\n        self.reg_mlp = conf[""reg_mlp""]\n        self.learning_rate = conf[""learning_rate""]\n        self.learner = conf[""learner""]\n        self.loss_function = conf[""loss_function""]\n        self.num_epochs = conf[""epochs""]\n        self.num_negatives = conf[""num_neg""]\n        self.batch_size = conf[""batch_size""]\n        self.verbose = conf[""verbose""]\n        self.is_pairwise = conf[""is_pairwise""]\n        self.mf_pretrain = conf[""mf_pretrain""]\n        self.mlp_pretrain = conf[""mlp_pretrain""]\n        self.init_method = conf[""init_method""]\n        self.stddev = conf[""stddev""]\n        self.num_users = dataset.num_users\n        self.num_items = dataset.num_items \n        self.dataset = dataset \n        self.sess = sess\n        \n    def _create_placeholders(self):\n        with tf.name_scope(""input_data""):\n            self.user_input = tf.placeholder(tf.int32, shape=[None], name=\'user_input\')\n            self.item_input = tf.placeholder(tf.int32, shape=[None], name=\'item_input\')\n            if self.is_pairwise is True:\n                self.item_input_neg = tf.placeholder(tf.int32, shape=[None], name=""item_input_neg"")\n            else:\n                self.labels = tf.placeholder(tf.float32, shape=[None], name=""labels"")\n            \n    def _create_variables(self, params=None):\n        with tf.name_scope(""embedding""):  # The embedding initialization is unknown now\n            if params is None:\n                initializer = tool.get_initializer(self.init_method, self.stddev)\n                \n                self.mf_embedding_user = tf.Variable(initializer([self.num_users, self.embedding_size]),\n                                                     name=\'mf_embedding_user\', dtype=tf.float32)\n                self.mf_embedding_item = tf.Variable(initializer([self.num_items, self.embedding_size]),\n                                                     name=\'mf_embedding_item\', dtype=tf.float32)\n                self.mlp_embedding_user = tf.Variable(initializer([self.num_users, int(self.layers[0]/2)]),\n                                                      name=""mlp_embedding_user"", dtype=tf.float32)\n                self.mlp_embedding_item = tf.Variable(initializer([self.num_items, int(self.layers[0]/2)]),\n                                                      name=""mlp_embedding_item"", dtype=tf.float32)\n            else:\n                self.mf_embedding_user = tf.Variable(params[0][0], name=\'mf_embedding_user\', dtype=tf.float32)\n                self.mf_embedding_item = tf.Variable(params[0][1], name=\'mf_embedding_item\', dtype=tf.float32)\n                \n                self.mlp_embedding_user = tf.Variable(params[1][0], name=""mlp_embedding_user"", dtype=tf.float32)\n                self.mlp_embedding_item = tf.Variable(params[1][1], name=""mlp_embedding_item"", dtype=tf.float32)\n\n    def _create_inference(self, item_input):\n        with tf.name_scope(""inference""):\n            \n            mf_user_latent = tf.nn.embedding_lookup(self.mf_embedding_user, self.user_input)\n            mf_item_latent = tf.nn.embedding_lookup(self.mf_embedding_item, item_input)\n            mlp_user_latent = tf.nn.embedding_lookup(self.mlp_embedding_user, self.user_input)\n            mlp_item_latent = tf.nn.embedding_lookup(self.mlp_embedding_item, item_input)\n            \n            mf_vector = tf.multiply(mf_user_latent, mf_item_latent)  # element-wise multiply\n            \n            mlp_vector = tf.concat([mlp_user_latent, mlp_item_latent], axis=1)\n\n            for idx in np.arange(len(self.layers)):\n                mlp_vector = tf.layers.dense(mlp_vector, units=self.layers[idx], activation=tf.nn.relu)\n    \n            # Concatenate MF and MLP parts\n            predict = tf.reduce_sum(tf.concat([mf_vector, mlp_vector], axis=1), 1)\n            return mf_user_latent, mf_item_latent, mlp_user_latent, mlp_item_latent, predict\n\n    def _create_loss(self):\n        with tf.name_scope(""loss""):\n            p1, q1, m1, n1, self.output = self._create_inference(self.item_input)\n            if self.is_pairwise is True:\n                _, q2, _, n2, output_neg = self._create_inference(self.item_input_neg)\n                result = self.output - output_neg\n                self.loss = learner.pairwise_loss(self.loss_function, result) + \\\n                            self.reg_mf * l2_loss(p1, q2, q1) + \\\n                            self.reg_mlp * l2_loss(m1, n2, n1)\n            else:\n                self.loss = learner.pointwise_loss(self.loss_function, self.labels, self.output) + \\\n                            self.reg_mf * l2_loss(p1, q1) + \\\n                            self.reg_mlp * l2_loss(m1, n1)\n\n    def _create_optimizer(self):\n        with tf.name_scope(""learner""):\n            self.optimizer = learner.optimizer(self.learner, self.loss, self.learning_rate)\n            \n    def build_graph(self):\n        self._create_placeholders()\n        try:\n            pre_trained_params = []\n            with open(self.mf_pretrain, ""rb"") as fin:\n                pre_trained_params.append(pickle.load(fin, encoding=""utf-8""))\n            with open(self.mlp_pretrain, ""rb"") as fin:\n                pre_trained_params.append(pickle.load(fin, encoding=""utf-8""))\n            self.logger.info(""load pretrained params successful!"")\n        except:\n            pre_trained_params = None\n            self.logger.info(""load pretrained params unsuccessful!"")\n            \n        self._create_variables(pre_trained_params)\n        self._create_loss()\n        self._create_optimizer()\n                                               \n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n        if self.is_pairwise is True:\n            data_iter = PairwiseSampler(self.dataset, neg_num=1, batch_size=self.batch_size, shuffle=True)\n        else:\n            data_iter = PointwiseSampler(self.dataset, neg_num=self.num_negatives, batch_size=self.batch_size, shuffle=True)\n\n        for epoch in range(1, self.num_epochs+1):\n            total_loss = 0.0\n            training_start_time = time()\n            num_training_instances = len(data_iter)\n            if self.is_pairwise is True:\n                for bat_users, bat_items_pos, bat_items_neg in data_iter:\n                    feed_dict = {self.user_input: bat_users,\n                                 self.item_input: bat_items_pos,\n                                 self.item_input_neg: bat_items_neg}\n                    loss, _ = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n                    total_loss += loss\n            else:\n                for bat_users, bat_items, bat_labels in data_iter:\n                    feed_dict = {self.user_input: bat_users,\n                                 self.item_input: bat_items,\n                                 self.labels: bat_labels}\n                    loss, _ = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n                    total_loss += loss\n            self.logger.info(""[iter %d : loss : %f, time: %f]"" % (epoch, total_loss / num_training_instances,\n                                                             time() - training_start_time))\n            if epoch % self.verbose == 0:\n                self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate()))\n\n    @timer\n    def evaluate(self):\n        return self.evaluator.evaluate(self)\n\n    def predict(self, user_ids, candidate_items_user_ids):\n        ratings = []\n        if candidate_items_user_ids is not None:\n            for u, i in zip(user_ids, candidate_items_user_ids):\n                users = np.full(len(i), u, dtype=np.int32)\n                feed_dict = {self.user_input: users, self.item_input: i}\n                ratings.append(self.sess.run(self.output, feed_dict=feed_dict))\n        else:\n            for u in user_ids:\n                users = np.full(self.num_items, u, dtype=np.int32)\n                feed_dict = {self.user_input: users, self.item_input: np.arange(self.num_items)}\n                ratings.append(self.sess.run(self.output, feed_dict=feed_dict))\n        return ratings\n'"
model/general_recommender/Pop.py,0,"b'import numpy as np\nfrom model.AbstractRecommender import AbstractRecommender\n\n\nclass Pop(AbstractRecommender):\n    def __init__(self, sess, dataset, config):\n        super(Pop, self).__init__(dataset, config)\n        self.dataset = dataset\n        self.users_num, self.items_num = self.dataset.num_users, self.dataset.num_items\n\n    def build_graph(self):\n        pass\n\n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n        item_csr = self.dataset.to_csr_matrix().transpose(copy=False)\n        items_count = [item_csr[i].getnnz() for i in range(item_csr.shape[0])]\n        self.ranking_score = np.array(items_count, dtype=np.float32)\n        result = self.evaluate_model()\n        self.logger.info(""result:\\t%s"" % result)\n\n    def evaluate_model(self):\n        return self.evaluator.evaluate(self)\n\n    def predict(self, users, items=None):\n        ratings = np.tile(self.ranking_score, len(users))\n        ratings = np.reshape(ratings, newshape=[len(users), self.items_num])\n\n        if items is not None:\n            ratings = [ratings[idx][u_item] for idx, u_item in enumerate(items)]\n        return ratings\n'"
model/general_recommender/SpectralCF.py,24,"b'""""""\nReference: Lei, Zheng, et al. ""Spectral Collaborative Filtering."" in RecSys2018\n@author: wubin\n""""""\nimport tensorflow as tf\nimport numpy as np\nfrom time import time\nfrom util import learner, tool\nfrom model.AbstractRecommender import AbstractRecommender\nfrom util import timer\nfrom util import l2_loss\nfrom data import PairwiseSampler\n\n\nclass SpectralCF(AbstractRecommender):\n    def __init__(self, sess, dataset, conf):\n        super(SpectralCF, self).__init__(dataset, conf)\n        self.learning_rate = conf[""learning_rate""]\n        self.learner = conf[""learner""]\n        self.batch_size = conf[""batch_size""]\n        self.num_layers = conf[""num_layers""]\n        self.activation = conf[""activation""]\n        self.embedding_size = conf[""embedding_size""]\n        self.num_epochs = conf[""epochs""]\n        self.reg = conf[""reg""]\n        self.loss_function = conf[""loss_function""]\n        self.dropout = conf[""dropout""]\n        self.embed_init_method = conf[""embed_init_method""]\n        self.weight_init_method = conf[""weight_init_method""]\n        self.stddev = conf[""stddev""]\n        self.verbose = conf[""verbose""]\n        self.dataset = dataset\n        self.num_users = dataset.num_users\n        self.num_items = dataset.num_items  \n        self.graph = dataset.train_matrix.toarray()\n        self.A = self.adjacient_matrix(self_connection=True)\n        self.D = self.degree_matrix()\n        self.L = self.laplacian_matrix(normalized=True)\n        self.lamda, self.U = np.linalg.eig(self.L)\n        self.lamda = np.diag(self.lamda)\n        self.sess = sess\n\n    def _create_placeholders(self):\n        with tf.name_scope(""input_data""):\n            self.input_user = tf.placeholder(dtype=tf.int32, shape=[None, ])\n            self.input_item_pos = tf.placeholder(dtype=tf.int32, shape=[None, ])\n            self.input_item_neg = tf.placeholder(dtype=tf.int32, shape=[None, ])\n\n    def _create_variables(self):\n        with tf.name_scope(""embedding""):  # The embedding initialization is unknown now  \n            embed_initializer = tool.get_initializer(self.embed_init_method, self.stddev)\n            self.user_embeddings = tf.Variable(embed_initializer([self.num_users, self.embedding_size]),\n                                               dtype=tf.float32, name=\'user_embeddings\')\n            self.item_embeddings = tf.Variable(embed_initializer([self.num_items, self.embedding_size]),\n                                               dtype=tf.float32, name=\'item_embeddings\')\n            \n        weight_initializer = tool.get_initializer(self.weight_init_method, self.stddev)\n        self.filters = []\n        for _ in range(self.num_layers):\n            self.filters.append(\n                tf.Variable(weight_initializer([self.embedding_size, self.embedding_size]), dtype=tf.float32))\n    \n    def _create_inference(self):\n        with tf.name_scope(""inference""):\n            A_hat = np.dot(self.U, self.U.T) + np.dot(np.dot(self.U, self.lamda), self.U.T)\n            # A_hat += np.dot(np.dot(self.U, self.lamda_2), self.U.T)\n            A_hat = A_hat.astype(np.float32)\n    \n            embeddings = tf.concat([self.user_embeddings, self.item_embeddings], axis=0)\n            all_embeddings = [embeddings]\n            for k in range(0, self.num_layers):\n    \n                embeddings = tf.matmul(A_hat, embeddings)\n    \n                # filters = self.filters[k]#tf.squeeze(tf.gather(self.filters, k))\n                embeddings = tool.activation_function(self.activation, (tf.matmul(embeddings, self.filters[k])))\n                all_embeddings += [embeddings]\n            all_embeddings = tf.concat(all_embeddings, 1)\n            self.user_new_embeddings, self.item_new_embeddings = tf.split(all_embeddings, [self.num_users, self.num_items], 0)\n            self.u_embeddings = tf.nn.embedding_lookup(self.user_new_embeddings, self.input_user)\n            self.pos_i_embeddings = tf.nn.embedding_lookup(self.item_new_embeddings, self.input_item_pos)\n            self.neg_j_embeddings = tf.nn.embedding_lookup(self.item_new_embeddings, self.input_item_neg)\n            \n    def _create_loss(self):\n        with tf.name_scope(""loss""): \n            self.output = tf.reduce_sum(tf.multiply(self.u_embeddings, self.pos_i_embeddings), axis=1)\n            output_neg = tf.reduce_sum(tf.multiply(self.u_embeddings, self.neg_j_embeddings), axis=1)\n            regularizer = self.reg * l2_loss(self.u_embeddings, self.pos_i_embeddings, self.neg_j_embeddings)\n\n            self.loss = learner.pairwise_loss(self.loss_function, self.output - output_neg) + regularizer\n                \n    def _create_optimizer(self):\n        with tf.name_scope(""learner""):\n            self.optimizer = learner.optimizer(self.learner, self.loss, self.learning_rate)\n    \n    def build_graph(self):\n        self._create_placeholders()\n        self._create_variables()\n        self._create_inference()\n        self._create_loss()\n        self._create_optimizer()\n    \n    def adjacient_matrix(self, self_connection=False):\n        A = np.zeros([self.num_users+self.num_items, self.num_users+self.num_items], dtype=np.float32)\n        A[:self.num_users, self.num_users:] = self.graph\n        A[self.num_users:, :self.num_users] = self.graph.transpose()\n        if self_connection is True:\n            return np.identity(self.num_users+self.num_items,dtype=np.float32) + A\n        return A\n\n    def degree_matrix(self):\n        degree = np.sum(self.A, axis=1, keepdims=False)\n        # degree = np.diag(degree)\n        return degree\n\n    def laplacian_matrix(self, normalized=False):\n        if normalized is False:\n            return self.D - self.A\n\n        temp = np.dot(np.diag(np.power(self.D, -1)), self.A)\n        # temp = np.dot(temp, np.power(self.D, -0.5))\n        return np.identity(self.num_users+self.num_items,dtype=np.float32) - temp\n        \n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n        data_iter = PairwiseSampler(self.dataset, neg_num=1, batch_size=self.batch_size, shuffle=True)\n        for epoch in range(1,self.num_epochs+1):\n            total_loss = 0.0\n            training_start_time = time()\n            num_training_instances = len(data_iter)\n            for bat_users, bat_items_pos, bat_items_neg in data_iter:\n                    feed_dict = {self.input_user: bat_users,\n                                 self.input_item_pos: bat_items_pos,\n                                 self.input_item_neg: bat_items_neg}\n                    loss, _ = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n                    total_loss += loss\n    \n            self.logger.info(""[iter %d : loss : %f, time: %f]"" % (epoch, total_loss/num_training_instances,\n                                                             time()-training_start_time))\n            if epoch % self.verbose == 0:\n                self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate()))\n                \n    @timer\n    def evaluate(self):\n        self._cur_user_embeddings, self._cur_item_embeddings = \\\n            self.sess.run([self.user_new_embeddings, self.item_new_embeddings])\n        return self.evaluator.evaluate(self)\n\n    def predict(self, user_ids, candidate_items_userids):\n        if candidate_items_userids is None:\n            user_embed = self._cur_user_embeddings[user_ids]\n            ratings = np.matmul(user_embed, self._cur_item_embeddings.T)\n        else:\n            ratings = []\n            for user_id, items_by_user_id in zip(user_ids, candidate_items_userids):\n                user_embed = self._cur_user_embeddings[user_id]\n                items_embed = self._cur_item_embeddings[items_by_user_id]\n                ratings.append(np.squeeze(np.matmul(user_embed, items_embed.T)))\n            \n        return ratings\n'"
model/general_recommender/WRMF.py,19,"b'""""""\nReference: Yifan Hu et al., ""Collaborative Filtering for Implicit Feedback Datasets"" in ICDM 2008.\n@author: wubin\n""""""\nimport numpy as np\nfrom time import time\nfrom model.AbstractRecommender import AbstractRecommender\nimport tensorflow as tf\nfrom util import timer, tool\n\n\nclass WRMF(AbstractRecommender):\n    def __init__(self, sess, dataset, conf):\n        super(WRMF, self).__init__(dataset, conf)\n        self.embedding_size = conf[""embedding_size""]\n        self.alpha = conf[""alpha""]\n        self.topK = conf[""topk""]\n        self.num_epochs = conf[""epochs""]\n        self.reg_mf = conf[""reg_mf""]\n        self.init_method = conf[""init_method""]\n        self.stddev = conf[""stddev""]\n        self.verbose = conf[""verbose""]\n        self.dataset = dataset\n        self.num_users = dataset.num_users\n        self.num_items = dataset.num_items\n        self.sess = sess\n        self.Cui = np.zeros(shape=[self.num_users, self.num_items], dtype=np.float32)\n        self.Pui = np.zeros(shape=[self.num_users, self.num_items], dtype=np.float32)\n        for u in np.arange(self.num_users):\n            items_by_user_id = self.dataset.train_matrix[u].indices\n            for i in items_by_user_id:\n                self.Cui[u, i] = self.alpha\n                self.Pui[u, i] = 1.0\n        self.lambda_eye = self.reg_mf * tf.eye(self.embedding_size)\n    \n    def _create_placeholders(self):\n        self.user_id = tf.placeholder(tf.int32, [1])\n        self.Cu = tf.placeholder(tf.float32, [self.num_items, 1])  \n        self.Pu = tf.placeholder(tf.float32, [self.num_items, 1])\n\n        self.item_id = tf.placeholder(tf.int32, [1])\n        self.Ci = tf.placeholder(tf.float32, [self.num_users, 1])  \n        self.Pi = tf.placeholder(tf.float32, [self.num_users, 1])\n    \n    def _create_variables(self):\n        initializer = tool.get_initializer(self.init_method, self.stddev)\n        self.user_embeddings = tf.Variable(initializer([self.num_users, self.embedding_size]), name=\'user_embeddings\', dtype=tf.float32)\n        self.item_embeddings = tf.Variable(initializer([self.num_items, self.embedding_size]), name=\'item_embeddings\', dtype=tf.float32)\n        \n    def _create_optimizer(self):    \n        YTY = tf.matmul(self.item_embeddings, self.item_embeddings, transpose_a=True)\n        YTCuIY = tf.matmul(self.item_embeddings, tf.multiply(self.Cu, self.item_embeddings), transpose_a=True)  \n        YTCupu = tf.matmul(self.item_embeddings, tf.multiply(self.Cu+1, self.Pu), transpose_a=True)\n        xu = tf.linalg.solve(YTY + YTCuIY + self.lambda_eye, YTCupu)\n        self.update_user = tf.scatter_update(self.user_embeddings, self.user_id, tf.transpose(xu))\n\n        XTX = tf.matmul(self.user_embeddings, self.user_embeddings, transpose_a=True)\n        XTCIIX = tf.matmul(self.user_embeddings, tf.multiply(self.Ci, self.user_embeddings), transpose_a=True)  \n        XTCIpi = tf.matmul(self.user_embeddings, tf.multiply(self.Ci+1, self.Pi), transpose_a=True)\n        xi = tf.linalg.solve(XTX + XTCIIX + self.lambda_eye, XTCIpi)\n        self.update_item = tf.scatter_update(self.item_embeddings, self.item_id, tf.transpose(xi))\n    \n    def build_graph(self):\n        self._create_placeholders()\n        self._create_variables()\n        self._create_optimizer()\n        \n    # ---------- training process -------\n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n        for epoch in range(1, self.num_epochs+1):\n            training_start_time = time()\n            print(\'solving for user vectors...\')\n            for user_id in range(self.num_users):\n                feed = {self.user_id: [user_id],\n                        self.Pu: self.Pui[user_id].T.reshape([-1, 1]),\n                        self.Cu: self.Cui[user_id].T.reshape([-1, 1])}\n                self.sess.run(self.update_user, feed_dict=feed)\n\n            print(\'solving for item vectors...\')\n            for item_id in range(self.num_items):\n                feed = {self.item_id: [item_id],\n                        self.Pi: self.Pui[:,item_id].reshape([-1, 1]),\n                        self.Ci: self.Cui[:,item_id].reshape([-1, 1])}\n                self.sess.run(self.update_item, feed_dict=feed)\n           \n            self.logger.info(\'iteration %i finished in %f seconds\' % (epoch, time()-training_start_time))\n            if epoch % self.verbose == 0:\n                self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate()))\n                \n    @timer\n    def evaluate(self):\n        self._cur_user_embeddings, self._cur_item_embeddings = self.sess.run([self.user_embeddings, self.item_embeddings])\n        return self.evaluator.evaluate(self)\n\n    def predict(self, user_ids, candidate_items_userids):\n        if candidate_items_userids is None:\n            user_embed = self._cur_user_embeddings[user_ids]\n            ratings = np.matmul(user_embed, self._cur_item_embeddings.T)\n        else:\n            ratings = []\n            for user_id, items_by_user_id in zip(user_ids, candidate_items_userids):\n                user_embed = self._cur_user_embeddings[user_id]\n                items_embed = self._cur_item_embeddings[items_by_user_id]\n                ratings.append(np.squeeze(np.matmul(user_embed, items_embed.T)))\n        return ratings\n'"
model/sequential_recommender/Caser.py,39,"b'""""""\nPaper: Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding\nAuthor: Jiaxi Tang, and Ke Wang\nReference: https://github.com/graytowne/caser_pytorch\n@author: Zhongchuan Sun\n""""""\n\nimport numpy as np\nfrom model.AbstractRecommender import SeqAbstractRecommender\nfrom util import DataIterator\nfrom util.tool import csr_to_user_dict_bytime\nimport tensorflow as tf\nfrom util import batch_randint_choice\nfrom util import pad_sequences\n\n\nclass Caser(SeqAbstractRecommender):\n    def __init__(self, sess, dataset, conf):\n        super(Caser, self).__init__(dataset, conf)\n        self.dataset = dataset\n        self.users_num, self.items_num = dataset.train_matrix.shape\n\n        self.lr = conf[""lr""]\n        self.l2_reg = conf[""l2_reg""]\n        self.factors_num = conf[""factors_num""]\n        self.batch_size = conf[""batch_size""]\n        self.epochs = conf[""epochs""]\n        self.seq_L = conf[""seq_L""]\n        self.seq_T = conf[""seq_T""]\n        self.nv = conf[""nv""]\n        self.nh = conf[""nh""]\n        self.dropout = conf[""dropout""]\n        self.neg_samples = conf[""neg_samples""]\n\n        self.sess = sess\n\n    def _create_variable(self):\n        self.user_ph = tf.placeholder(tf.int32, [None], name=""user"")\n        self.item_seq_ph = tf.placeholder(tf.int32, [None, self.seq_L], name=""item_seq"")\n        self.item_pos_ph = tf.placeholder(tf.int32, [None, self.seq_T], name=""item_pos"")\n        self.item_neg_ph = tf.placeholder(tf.int32, [None, self.neg_samples], name=""item_neg"")\n        self.is_training = tf.placeholder(tf.bool, name=""training_flag"")\n\n        l2_regularizer = tf.contrib.layers.l2_regularizer(self.l2_reg)\n\n        self.user_embeddings = tf.get_variable(\'user_embeddings\', dtype=tf.float32,\n                                               shape=[self.users_num, self.factors_num],\n                                               regularizer=l2_regularizer)\n\n        item_embeddings = tf.get_variable(\'seq_item_embeddings\', dtype=tf.float32,\n                                          shape=[self.items_num, self.factors_num],\n                                          regularizer=l2_regularizer)\n        zero_pad = tf.zeros([1, self.factors_num], name=""padding"")\n        self.seq_item_embeddings = tf.concat([item_embeddings, zero_pad], axis=0)\n\n        self.conv_v = tf.layers.Conv2D(self.nv, [self.seq_L, 1])\n        self.conv_h = [tf.layers.Conv2D(self.nh, [i, self.factors_num], activation=""relu"")\n                       for i in np.arange(1, self.seq_L + 1)]\n\n        self.fc1_ly = tf.layers.Dense(self.factors_num, activation=""relu"")\n        self.dropout_ly = tf.layers.Dropout(self.dropout)\n\n        # predication embedding\n        self.item_embeddings = tf.get_variable(\'item_embeddings\', dtype=tf.float32,\n                                               shape=[self.items_num, self.factors_num * 2],\n                                               regularizer=l2_regularizer)\n        self.item_biases = tf.get_variable(\'item_biases\', dtype=tf.float32, shape=[self.items_num],\n                                           initializer=tf.initializers.zeros(), regularizer=l2_regularizer)\n\n    def build_graph(self):\n        self._create_variable()\n        # embedding lookup\n        batch_size = tf.shape(self.item_seq_ph)[0]\n        item_embs = tf.nn.embedding_lookup(self.seq_item_embeddings, self.item_seq_ph)  # (b, L, d)\n        item_embs = tf.expand_dims(item_embs, axis=3)  # (b, L, d, 1)\n\n        # vertical conv layer\n        out_v = self.conv_v(item_embs)  # (b, 1, d, nv)\n        out_v = tf.reshape(out_v, shape=[batch_size, self.nv*self.factors_num])  # (b, nv*d)\n\n        # horizontal conv layer\n        out_hs = []\n        for conv_h in self.conv_h:\n            conv_out = conv_h(item_embs)  # (b, ?, 1, nh)\n            conv_out = tf.squeeze(conv_out, axis=2)  # (b, ?, nh)\n            pool_out = tf.reduce_max(conv_out, axis=1)  # (b, nh)\n            out_hs.append(pool_out)\n        out_h = tf.concat(out_hs, axis=1)  # (b, nh*L)\n\n        out = tf.concat([out_v, out_h], axis=1)  # (b, nv*d+nh*L)\n        # apply dropout\n        out = self.dropout_ly(out, training=self.is_training)\n\n        # fully-connected Layers\n        z = self.fc1_ly(out)  # (b, d)\n\n        # rating calculation\n        user_embs = tf.nn.embedding_lookup(self.user_embeddings, self.user_ph)  # (b, d)\n        user_embs = tf.concat([z, user_embs], axis=1)  # (b, 2d)\n        user_embs = tf.expand_dims(user_embs, axis=1)  # (b, 1, 2d)\n\n        tar_item = tf.concat([self.item_pos_ph, self.item_neg_ph], axis=-1)  # (b, 2T)\n        tar_item_embs = tf.nn.embedding_lookup(self.item_embeddings, tar_item)  # (b, 2T, 2d)\n        tar_item_bias = tf.gather(self.item_biases, tar_item)  # (b, 2T)\n        logits = tf.squeeze(tf.matmul(user_embs, tar_item_embs, transpose_b=True), axis=1) + tar_item_bias  # (b, 2T)\n        pos_logits, neg_logits = tf.split(logits, [self.seq_T, self.neg_samples], axis=1)\n\n        # loss\n        pos_loss = tf.reduce_mean(-tf.log(tf.sigmoid(pos_logits) + 1e-24))\n        neg_loss = tf.reduce_mean(-tf.log(1 - tf.sigmoid(neg_logits) + 1e-24))\n        loss = pos_loss + neg_loss\n        try:\n            reg_losses = tf.add_n(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n            loss = loss + reg_losses\n        except:\n            pass\n\n        self.train_opt = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(loss)\n\n        # for predict/test\n        user_embs = tf.squeeze(user_embs, axis=1)  # (b, 2d)\n        self.all_logits = tf.matmul(user_embs, self.item_embeddings, transpose_b=True)  # (b, items_num)\n\n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n        self.user_pos_train = csr_to_user_dict_bytime(self.dataset.time_matrix, self.dataset.train_matrix)\n        users_list, item_seq_list, item_pos_list = self._generate_sequences()\n        for epoch in range(self.epochs):\n            item_neg_list = self._sample_negative(users_list)\n            data = DataIterator(users_list, item_seq_list, item_pos_list, item_neg_list,\n                                batch_size=self.batch_size, shuffle=True)\n            for bat_user, bat_item_seq, bat_item_pos, bat_item_neg in data:\n                feed = {self.user_ph: bat_user,\n                        self.item_seq_ph: bat_item_seq,\n                        self.item_pos_ph: bat_item_pos,\n                        self.item_neg_ph: bat_item_neg,\n                        self.is_training: True}\n\n                self.sess.run(self.train_opt, feed_dict=feed)\n\n            result = self.evaluate_model()\n            self.logger.info(""epoch %d:\\t%s"" % (epoch, result))\n\n    def _generate_sequences(self):\n        self.user_test_seq = {}\n        users_list, item_seq_list, item_pos_list = [], [], []\n        seq_len = self.seq_L + self.seq_T\n        uni_users = np.unique(list(self.user_pos_train.keys()))\n        for user in uni_users:\n            seq_items = self.user_pos_train[user]\n            if len(seq_items) - seq_len >= 0:\n                for i in range(len(seq_items), 0, -1):\n                    if i-seq_len >= 0:\n                        seq_i = seq_items[i-seq_len:i]\n                        if user not in self.user_test_seq:\n                            self.user_test_seq[user] = seq_i[-self.seq_L:]\n                        users_list.append(user)\n                        item_seq_list.append(seq_i[:self.seq_L])\n                        item_pos_list.append(seq_i[-self.seq_T:])\n                    else:\n                        break\n            else:\n                seq_items = np.reshape(seq_items, newshape=[1, -1]).astype(np.int32)\n                seq_items = pad_sequences(seq_items, value=self.items_num, max_len=seq_len,\n                                          padding=\'pre\', truncating=\'pre\')\n                seq_i = np.reshape(seq_items, newshape=[-1])\n                if user not in self.user_test_seq:\n                    self.user_test_seq[user] = seq_i[-self.seq_L:]\n                users_list.append(user)\n                item_seq_list.append(seq_i[:self.seq_L])\n                item_pos_list.append(seq_i[-self.seq_T:])\n        return users_list, item_seq_list, item_pos_list\n\n    def _sample_negative(self, users_list):\n        neg_items_list = []\n        user_neg_items_dict = {}\n        all_uni_user, all_counts = np.unique(users_list, return_counts=True)\n        user_count = DataIterator(all_uni_user, all_counts, batch_size=1024, shuffle=False)\n        for bat_users, bat_counts in user_count:\n            n_neg_items = [c*self.neg_samples for c in bat_counts]\n            exclusion = [self.user_pos_train[u] for u in bat_users]\n            bat_neg = batch_randint_choice(self.items_num, n_neg_items, replace=True, exclusion=exclusion)\n            for u, neg in zip(bat_users, bat_neg):\n                user_neg_items_dict[u] = neg\n\n        for u, c in zip(all_uni_user, all_counts):\n            neg_items = np.reshape(user_neg_items_dict[u], newshape=[c, self.neg_samples])\n            neg_items_list.extend(neg_items)\n        return neg_items_list\n\n    def evaluate_model(self):\n        return self.evaluator.evaluate(self)\n\n    def predict(self, users, items=None):\n        users = DataIterator(users, batch_size=512, shuffle=False, drop_last=False)\n        all_ratings = []\n        for bat_user in users:\n            bat_seq = [self.user_test_seq[u] for u in bat_user]\n            feed = {self.user_ph: bat_user,\n                    self.item_seq_ph: bat_seq,\n                    self.is_training: False}\n            bat_ratings = self.sess.run(self.all_logits, feed_dict=feed)\n            all_ratings.extend(bat_ratings)\n        all_ratings = np.array(all_ratings, dtype=np.float32)\n\n        if items is not None:\n            all_ratings = [all_ratings[idx][item] for idx, item in enumerate(items)]\n\n        return all_ratings\n'"
model/sequential_recommender/FPMC.py,24,"b'""""""\nReference: Steffen Rendle et al., ""Factorizing Personalized Markov Chains\nfor Next-Basket Recommendation."" in WWW 2010.\n@author: wubin\n""""""\nfrom model.AbstractRecommender import SeqAbstractRecommender\nimport tensorflow as tf\nimport numpy as np\nfrom time import time\nfrom util import timer\nfrom util import learner, tool\nfrom util.tool import csr_to_user_dict_bytime\nfrom util import l2_loss\nfrom data import TimeOrderPointwiseSampler, TimeOrderPairwiseSampler\n\nclass FPMC(SeqAbstractRecommender):\n    def __init__(self, sess, dataset, conf):\n        super(FPMC, self).__init__(dataset, conf)\n        self.learning_rate = conf[""learning_rate""]\n        self.embedding_size = conf[""embedding_size""]\n        self.learner = conf[""learner""]\n        self.loss_function = conf[""loss_function""]\n        self.is_pairwise = conf[""is_pairwise""]\n        self.topK = conf[""topk""]\n        self.num_epochs = conf[""epochs""]\n        self.reg_mf = conf[""reg_mf""]\n        self.batch_size = conf[""batch_size""]\n        self.init_method = conf[""init_method""]\n        self.stddev = conf[""stddev""]\n        self.verbose = conf[""verbose""]\n        self.num_negatives = conf[""num_neg""]\n        self.num_users = dataset.num_users\n        self.num_items = dataset.num_items\n        self.dataset = dataset\n        self.train_matrix = dataset.train_matrix\n        self.train_dict = csr_to_user_dict_bytime(dataset.time_matrix, dataset.train_matrix)\n        self.sess = sess\n\n    def _create_placeholders(self):\n        with tf.name_scope(""input_data""):\n            self.user_input = tf.placeholder(tf.int32, shape=[None], name=""user_input"")\n            self.item_input = tf.placeholder(tf.int32, shape=[None], name=""item_input_pos"")\n            self.item_input_recent = tf.placeholder(tf.int32, shape=[None], name=""item_input_recent"")\n            if self.is_pairwise is True:\n                self.item_input_neg = tf.placeholder(tf.int32, shape=[None], name=""item_input_neg"")\n            else:\n                self.labels = tf.placeholder(tf.float32, shape=[None], name=""labels"")\n\n    def _create_variables(self):\n        with tf.name_scope(""embedding""):\n            initializer = tool.get_initializer(self.init_method, self.stddev)\n            self.embeddings_UI = tf.Variable(initializer([self.num_users, self.embedding_size]), \n                                             name=\'embeddings_UI\', dtype=tf.float32)  # (users, embedding_size)\n            self.embeddings_IU = tf.Variable(initializer([self.num_items, self.embedding_size]),\n                                             name=\'embeddings_IU\', dtype=tf.float32)  # (items, embedding_size)\n            self.embeddings_IL = tf.Variable(initializer([self.num_items, self.embedding_size]),\n                                             name=\'embeddings_IL\', dtype=tf.float32)\n            self.embeddings_LI = tf.Variable(initializer([self.num_items, self.embedding_size]),\n                                             name=\'embeddings_LI\', dtype=tf.float32)  # (items, embedding_size)\n            \n    def _create_inference(self, item_input):\n        with tf.name_scope(""inference""):\n            # embedding look up\n            embeddings_UI_u = tf.nn.embedding_lookup(self.embeddings_UI, self.user_input)\n            embeddings_IU_i = tf.nn.embedding_lookup(self.embeddings_IU, item_input)\n            embeddings_IL_i = tf.nn.embedding_lookup(self.embeddings_IL, item_input)\n            embeddings_LI_l = tf.nn.embedding_lookup(self.embeddings_LI, self.item_input_recent)\n            predict_vector = tf.multiply(embeddings_UI_u, embeddings_IU_i) + tf.multiply(embeddings_IL_i, embeddings_LI_l)\n            predict = tf.reduce_sum(predict_vector, 1)\n            return embeddings_UI_u, embeddings_IU_i, embeddings_IL_i, embeddings_LI_l, predict\n\n    def _create_loss(self):\n        with tf.name_scope(""loss""):\n            # loss for L(Theta)\n            UI_u, IU_i, IL_i, LI_l, self.output = self._create_inference(self.item_input)\n            if self.is_pairwise is True:\n                _, IU_j, IL_j, _, output_neg = self._create_inference(self.item_input_neg)\n                self.result = self.output - output_neg\n                self.loss = learner.pairwise_loss(self.loss_function, self.result) + \\\n                            self.reg_mf * l2_loss(UI_u, IU_i, IL_i, LI_l, IU_j, IL_j)\n\n            else:\n                self.loss = learner.pointwise_loss(self.loss_function, self.labels, self.output) + \\\n                            self.reg_mf * l2_loss(UI_u, IU_i, IL_i, LI_l)\n\n    def _create_optimizer(self):\n        with tf.name_scope(""learner""):\n            self.optimizer = learner.optimizer(self.learner, self.loss, self.learning_rate)\n                \n    def build_graph(self):\n        self._create_placeholders()\n        self._create_variables()\n        self._create_loss()\n        self._create_optimizer()\n\n    # ---------- training process -------\n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n        if self.is_pairwise is True:\n            data_iter = TimeOrderPairwiseSampler(self.dataset, high_order=1, neg_num=1,\n                                                 batch_size=self.batch_size, shuffle=True)\n        else:\n            data_iter = TimeOrderPointwiseSampler(self.dataset, high_order=1,\n                                                  neg_num=self.num_negatives,\n                                                  batch_size=self.batch_size, shuffle=True)\n        for epoch in range(1, self.num_epochs+1):\n            num_training_instances = len(data_iter)\n            total_loss = 0.0\n            training_start_time = time()\n\n            if self.is_pairwise is True:\n                for bat_users, bat_items_recent, bat_items_pos, bat_items_neg in data_iter:\n                    feed_dict = {self.user_input: bat_users,\n                                 self.item_input: bat_items_pos,\n                                 self.item_input_recent: bat_items_recent,\n                                 self.item_input_neg: bat_items_neg}\n\n                    loss, _ = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n                    total_loss += loss\n            else:\n                for bat_users, bat_items_recent, bat_items, bat_labels in data_iter:\n                    feed_dict = {self.user_input: bat_users,\n                                 self.item_input: bat_items,\n                                 self.item_input_recent: bat_items_recent,\n                                 self.labels: bat_labels}\n\n                    loss, _ = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n                    total_loss += loss\n\n            self.logger.info(""[iter %d : loss : %f, time: %f]"" %\n                             (epoch, total_loss / num_training_instances, time() - training_start_time))\n            \n            if epoch % self.verbose == 0:\n                self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate()))\n\n    @timer\n    def evaluate(self):\n        return self.evaluator.evaluate(self)\n    \n    def predict(self, user_ids, candidate_items_userids=None):\n        ratings = []\n        if candidate_items_userids is None:\n            all_items = np.arange(self.num_items)\n            for user_id in user_ids:\n                cand_items = self.train_dict[user_id]\n                item_recent = np.full(self.num_items, cand_items[-1], dtype=np.int32)\n    \n                users = np.full(self.num_items, user_id, dtype=np.int32)\n                feed_dict = {self.user_input: users,\n                             self.item_input_recent: item_recent,\n                             self.item_input: all_items}\n                ratings.append(self.sess.run(self.output, feed_dict=feed_dict))   \n                \n        else:\n            for user_id, items_by_user_id in zip(user_ids, candidate_items_userids):\n                cand_items = self.train_dict[user_id]\n                item_recent = np.full(len(candidate_items_userids), cand_items[-1], dtype=np.int32)\n    \n                users = np.full(len(items_by_user_id), user_id, dtype=np.int32)\n                feed_dict = {self.user_input: users,\n                             self.item_input_recent: item_recent,\n                             self.item_input: items_by_user_id}\n                ratings.append(self.sess.run(self.output, feed_dict=feed_dict))\n\n        return np.vstack(ratings)\n'"
model/sequential_recommender/FPMCplus.py,42,"b'""""""\nReference: Steffen Rendle et al., ""Factorizing Personalized Markov Chains\nfor Next-Basket Recommendation."" in WWW 2010.\n@author: wubin\n""""""\nfrom model.AbstractRecommender import SeqAbstractRecommender\nimport tensorflow as tf\nimport numpy as np\nfrom time import time\nfrom util import learner, tool\nfrom util.tool import csr_to_user_dict_bytime\nfrom util import timer\nfrom util import l2_loss\nfrom data import TimeOrderPairwiseSampler, TimeOrderPointwiseSampler\n\n\nclass FPMCplus(SeqAbstractRecommender):\n    def __init__(self, sess, dataset, conf):\n        super(FPMCplus, self).__init__(dataset, conf)\n        self.learning_rate = conf[""learning_rate""]\n        self.embedding_size = conf[""embedding_size""]\n        self.weight_size = conf[""weight_size""]\n        self.learner = conf[""learner""]\n        self.loss_function = conf[""loss_function""]\n        self.is_pairwise = conf[""is_pairwise""]\n        self.num_epochs = conf[""epochs""]\n        self.reg_mf = conf[""reg_mf""]\n        self.reg_w = conf[""reg_w""]\n        self.batch_size = conf[""batch_size""]\n        self.high_order = conf[""high_order""]\n        self.verbose = conf[""verbose""]\n        self.embed_init_method = conf[""embed_init_method""]\n        self.weight_init_method = conf[""weight_init_method""]\n        self.stddev = float(conf[""stddev""])\n        self.num_negatives = conf[""num_neg""]\n        self.num_users = dataset.num_users\n        self.num_items = dataset.num_items\n        self.dataset = dataset\n        self.train_matrix = dataset.train_matrix\n        self.train_dict = csr_to_user_dict_bytime(dataset.time_matrix, dataset.train_matrix)\n        self.sess = sess\n\n    def _create_placeholders(self):\n        with tf.name_scope(""input_data""):\n            self.user_input = tf.placeholder(tf.int32, shape=[None], name=""user_input"")\n            self.item_input = tf.placeholder(tf.int32, shape=[None], name=""item_input_pos"")\n            self.item_input_recent = tf.placeholder(tf.int32, shape=[None, None], name=""item_input_recent"")\n            if self.is_pairwise is True:\n                self.item_input_neg = tf.placeholder(tf.int32, shape=[None], name=""item_input_neg"")\n            else:\n                self.labels = tf.placeholder(tf.float32, shape=[None], name=""labels"")\n\n    def _create_variables(self):\n        with tf.name_scope(""embedding""):\n            embed_initializer = tool.get_initializer(self.embed_init_method, self.stddev)\n            weight_initializer = tool.get_initializer(self.weight_init_method, self.stddev)\n            \n            self.embeddings_UI = tf.Variable(embed_initializer([self.num_users, self.embedding_size]), \n                                             name=\'embeddings_UI\', dtype=tf.float32)  # (users, embedding_size)\n            self.embeddings_IU = tf.Variable(embed_initializer([self.num_items, self.embedding_size]),\n                                             name=\'embeddings_IU\', dtype=tf.float32)  # (items, embedding_size)\n            self.embeddings_IL = tf.Variable(embed_initializer([self.num_items, self.embedding_size]),\n                                             name=\'embeddings_IL\', dtype=tf.float32)\n            self.embeddings_LI = tf.Variable(embed_initializer([self.num_items, self.embedding_size]),\n                                             name=\'embeddings_LI\', dtype=tf.float32)  # (items, embedding_size)\n            \n            self.W = tf.Variable(weight_initializer([3*self.embedding_size, self.weight_size]),\n                                 name=\'Weights_for_MLP\', dtype=tf.float32, trainable=True)\n            self.b = tf.Variable(weight_initializer([1, self.weight_size]),\n                                 name=\'Bias_for_MLP\', dtype=tf.float32, trainable=True)\n            self.h = tf.Variable(tf.ones([self.weight_size, 1]), name=\'H_for_MLP\', dtype=tf.float32)\n\n    def _attention_mlp(self, embeddings_UI_u, embeddings_IL_i, item_embedding_recent):\n        with tf.name_scope(""attention_MLP""):\n            UI_u = tf.tile(tf.expand_dims(embeddings_UI_u, 1), tf.stack([1, self.high_order, 1]))\n            IL_i = tf.tile(tf.expand_dims(embeddings_IL_i, 1), tf.stack([1, self.high_order, 1]))\n            embedding_short = tf.concat([UI_u,IL_i,item_embedding_recent], 2)\n            batch_users = tf.shape(embedding_short)[0]\n\n            # (b*n, e or 2*e) * (e or 2*e, w) + (1, w)\n            MLP_output = tf.matmul(tf.reshape(embedding_short, shape=[-1, 3*self.embedding_size]), self.W) + self.b\n            MLP_output = tf.nn.tanh(MLP_output)\n\n            # (b*n, w) * (w, 1) => (None, 1) => (b, n)\n            A_ = tf.reshape(tf.matmul(MLP_output, self.h), shape=[batch_users, self.high_order])\n\n            # softmax for not mask features\n            exp_A_ = tf.exp(A_)\n            exp_sum = tf.reduce_sum(exp_A_, 1, keepdims=True)  # (b, 1)\n\n            A = tf.expand_dims(tf.div(exp_A_, exp_sum), 2)  # (b, n, 1)\n\n            return tf.reduce_sum(A * item_embedding_recent, 1)    \n            \n    def _create_inference(self, item_input):\n        with tf.name_scope(""inference""):\n            # embedding look up\n            embeddings_UI_u = tf.nn.embedding_lookup(self.embeddings_UI, self.user_input)\n            embeddings_IU_i = tf.nn.embedding_lookup(self.embeddings_IU, item_input)\n            embeddings_IL_i = tf.nn.embedding_lookup(self.embeddings_IL, item_input)\n            embeddings_LI_l = tf.nn.embedding_lookup(self.embeddings_LI, self.item_input_recent)\n            item_embedding_short = self._attention_mlp(embeddings_UI_u, embeddings_IL_i, embeddings_LI_l)\n            predict_vector = tf.multiply(embeddings_UI_u, embeddings_IU_i) + \\\n                             tf.multiply(embeddings_IL_i, item_embedding_short)\n            predict = tf.reduce_sum(predict_vector, 1)\n            return embeddings_UI_u, embeddings_IU_i, embeddings_IL_i, embeddings_LI_l, predict\n\n    def _create_loss(self):\n        with tf.name_scope(""loss""):\n            UI_u, IU_i, IL_i, LI_l, self.output = self._create_inference(self.item_input)\n            if self.is_pairwise is True:\n                _, IU_j, IL_j, _, output_neg = self._create_inference(self.item_input_neg)\n                self.result = self.output - output_neg\n                self.loss = learner.pairwise_loss(self.loss_function, self.result) + \\\n                            self.reg_mf * l2_loss(UI_u, IU_i, IL_i, LI_l, IU_j, IL_j) + \\\n                            self.reg_w * l2_loss(self.W, self.h)\n            else:\n                self.loss = learner.pointwise_loss(self.loss_function, self.labels, self.output) + \\\n                            self.reg_mf * l2_loss(UI_u, IU_i, IL_i, LI_l)\n\n    def _create_optimizer(self):\n        with tf.name_scope(""learner""):\n            self.optimizer = learner.optimizer(self.learner, self.loss, self.learning_rate)\n                \n    def build_graph(self):\n        self._create_placeholders()\n        self._create_variables()\n        self._create_loss()\n        self._create_optimizer()\n\n    # ---------- training process -------\n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n        if self.is_pairwise is True:\n            data_iter = TimeOrderPairwiseSampler(self.dataset, high_order=self.high_order,\n                                                 batch_size=self.batch_size, shuffle=True)\n        else:\n            data_iter = TimeOrderPointwiseSampler(self.dataset, high_order=self.high_order,\n                                                  neg_num=self.num_negatives,\n                                                  batch_size=self.batch_size, shuffle=True)\n        for epoch in range(1, self.num_epochs+1):\n            num_training_instances = len(data_iter)\n            total_loss = 0.0\n            training_start_time = time()\n      \n            if self.is_pairwise is True:\n                for bat_users, bat_items_recent, bat_items_pos, bat_items_neg in data_iter:\n                    \n                    feed_dict = {self.user_input: bat_users, \n                                 self.item_input: bat_items_pos,\n                                 self.item_input_recent: bat_items_recent,\n                                 self.item_input_neg: bat_items_neg}\n                    \n                    loss, _ = self.sess.run((self.loss,self.optimizer), feed_dict=feed_dict)\n                    total_loss += loss\n            else:\n                for bat_users, bat_items_recent, bat_items, bat_labels in data_iter:\n                    \n                    feed_dict = {self.user_input: bat_users,\n                                 self.item_input: bat_items,\n                                 self.item_input_recent: bat_items_recent,\n                                 self.labels: bat_labels}\n    \n                    loss, _ = self.sess.run((self.loss,self.optimizer),feed_dict=feed_dict)\n                    total_loss += loss\n                \n            self.logger.info(""[iter %d : loss : %f, time: %f]"" % (epoch, total_loss/num_training_instances,\n                                                             time()-training_start_time))\n            \n            if epoch % self.verbose == 0:\n                self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate()))\n\n    @timer\n    def evaluate(self):\n        return self.evaluator.evaluate(self)\n                \n    def predict(self, user_ids, candidate_items_user_ids):\n        ratings = []\n        if candidate_items_user_ids is None:\n            all_items = np.arange(self.num_items)\n            for user_id in user_ids:\n                cand_items = self.train_dict[user_id]\n                item_recent = []\n    \n                for _ in range(self.num_items):\n                    item_recent.append(cand_items[len(cand_items)-self.high_order:])\n                users = np.full(self.num_items, user_id, dtype=np.int32)\n                feed_dict = {self.user_input: users,\n                             self.item_input_recent: item_recent,\n                             self.item_input: all_items}\n                ratings.append(self.sess.run(self.output, feed_dict=feed_dict))  \n        \n        else:\n            for user_id, items_by_user_id in zip(user_ids, candidate_items_user_ids):\n                cand_items = self.train_dict[user_id]\n                item_recent = []\n    \n                for _ in range(len(items_by_user_id)):\n                    item_recent.append(cand_items[len(cand_items)-self.high_order:])\n                users = np.full(len(items_by_user_id), user_id, dtype=np.int32)\n                feed_dict = {self.user_input: users,\n                             self.item_input_recent: item_recent,\n                             self.item_input: items_by_user_id}\n                ratings.append(self.sess.run(self.output, feed_dict=feed_dict))\n        return ratings\n'"
model/sequential_recommender/Fossil.py,32,"b'""""""\nReference: Ruining He et al., ""Fusing similarity models with Markov chains for sparse sequential recommendation."" in ICDM 2016.\n@author: wubin\n""""""\n\nfrom model.AbstractRecommender import SeqAbstractRecommender\nimport tensorflow as tf\nimport numpy as np\nfrom time import time\nfrom util import learner, tool, data_generator\nfrom util.tool import csr_to_user_dict_bytime, timer, pad_sequences\nfrom util import l2_loss\nfrom util.data_iterator import DataIterator\n\n\nclass Fossil(SeqAbstractRecommender):\n    def __init__(self, sess, dataset, conf):\n        super(Fossil, self).__init__(dataset, conf)\n        self.verbose = conf[""verbose""]\n        self.batch_size = conf[""batch_size""]\n        self.num_epochs = conf[""epochs""]\n        self.embedding_size = conf[""embedding_size""]\n        regs = conf[""regs""]\n        self.lambda_bilinear = regs[0]\n        self.gamma_bilinear = regs[1]\n        self.reg_eta = regs[2]\n        self.alpha = conf[""alpha""]\n        self.num_negatives = conf[""num_neg""]\n        self.learning_rate = conf[""learning_rate""]\n        self.learner = conf[""learner""]\n        self.loss_function = conf[""loss_function""]\n        self.is_pairwise = conf[""is_pairwise""]\n        self.high_order = conf[""high_order""]\n        self.num_negatives = conf[""num_neg""]\n        self.init_method = conf[""init_method""]\n        self.stddev = conf[""stddev""]\n        self.num_negatives = conf[""num_neg""]\n        self.num_users = dataset.num_users\n        self.num_items = dataset.num_items\n        self.dataset = dataset\n        self.train_matrix = self.dataset.train_matrix\n        self.train_dict = csr_to_user_dict_bytime(self.dataset.time_matrix,self.train_matrix)\n        self.sess = sess\n\n    def _create_placeholders(self):\n        with tf.name_scope(""input_data""):\n            self.user_input_id = tf.placeholder(tf.int32, shape=[None, ], name=""user_input_id"")  # the index of users\n            self.user_input = tf.placeholder(tf.int32, shape=[None, None], name=""user_input"")  # the index of users\n            self.num_idx = tf.placeholder(tf.float32, shape=[None, ], name=""num_idx"")  # the number of items rated by users\n            self.item_input = tf.placeholder(tf.int32, shape=[None, ], name=""item_input_pos"")  # the index of items\n            self.item_input_recent = tf.placeholder(tf.int32, shape=[None, None], name=""item_input_recent"")\n            if self.is_pairwise is True:\n                self.user_input_neg = tf.placeholder(tf.int32, shape=[None, None], name=""user_input_neg"")\n                self.item_input_neg = tf.placeholder(tf.int32, shape=[None, ], name=""item_input_neg"")\n                self.num_idx_neg = tf.placeholder(tf.float32, shape=[None, ], name=""num_idx_neg"")\n            else :\n                self.labels = tf.placeholder(tf.float32, shape=[None, ], name=""labels"")\n\n    def _create_variables(self):\n        with tf.name_scope(""embedding""):  # The embedding initialization is unknown now\n            initializer = tool.get_initializer(self.init_method, self.stddev)\n            \n            self.c1 = tf.Variable(initializer([self.num_items, self.embedding_size]), name=\'c1\', dtype=tf.float32)\n            self.c2 = tf.constant(0.0, tf.float32, [1, self.embedding_size], name=\'c2\')\n            self.embedding_P = tf.concat([self.c1, self.c2], 0, name=\'embedding_P\')\n            self.embedding_Q = tf.Variable(initializer([self.num_items, self.embedding_size]),\n                                           name=\'embedding_Q\', dtype=tf.float32)\n            \n            self.eta = tf.Variable(initializer([self.num_users, self.high_order]), name=\'eta\')\n            self.eta_bias = tf.Variable(initializer([1, self.high_order]), name=\'eta_bias\')\n            \n            self.bias = tf.Variable(tf.zeros(self.num_items), name=\'bias\')\n\n    def _create_inference(self, user_input, item_input, num_idx):\n        with tf.name_scope(""inference""):\n            embedding_p = tf.reduce_sum(tf.nn.embedding_lookup(self.embedding_P, user_input), 1)\n            embedding_eta_u = tf.nn.embedding_lookup(self.eta, self.user_input_id)\n            batch_size = tf.shape(embedding_eta_u)[0]\n            eta = tf.expand_dims(tf.tile(self.eta_bias, tf.stack([batch_size, 1])) + embedding_eta_u, -1)\n            embeddings_short = tf.nn.embedding_lookup(self.embedding_P, self.item_input_recent)\n            embedding_q = tf.nn.embedding_lookup(self.embedding_Q, item_input)\n            bias_i = tf.nn.embedding_lookup(self.bias, item_input)\n            coeff = tf.pow(num_idx, -tf.constant(self.alpha, tf.float32, [1]))\n            output = coeff * tf.reduce_sum(tf.multiply(embedding_p, embedding_q), 1) +\\\n                     tf.reduce_sum(tf.multiply(tf.reduce_sum(eta*embeddings_short, 1), embedding_q), 1) + bias_i\n        return embedding_p, embedding_q, embedding_eta_u, embeddings_short, output\n\n    def _create_loss(self):\n        with tf.name_scope(""loss""):\n            p1, q1, eta_u, short, self.output = self._create_inference(self.user_input, self.item_input, self.num_idx)\n            if self.is_pairwise is True:\n                _, q2, _, _, output_neg = self._create_inference(self.user_input_neg, self.item_input_neg, self.num_idx_neg)\n                self.result = self.output - output_neg\n                self.loss = learner.pairwise_loss(self.loss_function, self.result) + \\\n                            self.lambda_bilinear * l2_loss(p1) + \\\n                            self.gamma_bilinear * l2_loss(q2, q1, short) + \\\n                            self.reg_eta * l2_loss(eta_u, self.eta_bias)\n            else:\n                self.loss = learner.pointwise_loss(self.loss_function, self.labels, self.output) + \\\n                            self.lambda_bilinear * l2_loss(p1) + \\\n                            self.gamma_bilinear * l2_loss(q1, short) + \\\n                            self.reg_eta * l2_loss(eta_u, self.eta_bias)\n\n    def _create_optimizer(self):\n        with tf.name_scope(""learner""):\n            self.optimizer = learner.optimizer(self.learner, self.loss, self.learning_rate)\n              \n    def build_graph(self):\n        self._create_placeholders()\n        self._create_variables()\n        self._create_loss()\n        self._create_optimizer()\n\n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n        self.evaluate()\n        for epoch in range(1, self.num_epochs+1):\n            if self.is_pairwise is True:\n                user_input_id, user_input, user_input_neg, num_idx_pos,\\\n                    num_idx_neg, item_input_pos, item_input_neg, item_input_recent = \\\n                    data_generator._get_pairwise_all_likefossil_data(self.dataset, self.high_order, self.train_dict)\n                \n                data_iter = DataIterator(user_input_id, user_input, user_input_neg, num_idx_pos, num_idx_neg,\n                                         item_input_pos, item_input_neg, item_input_recent,\n                                         batch_size=self.batch_size, shuffle=True)\n            else:\n                user_input_id, user_input, num_idx, item_input, item_input_recent, labels = \\\n                    data_generator._get_pointwise_all_likefossil_data(self.dataset, self.high_order,\n                                                                      self.num_negatives, self.train_dict)\n\n                data_iter = DataIterator(user_input_id, user_input, num_idx, item_input, item_input_recent, labels,\n                                         batch_size=self.batch_size, shuffle=True)\n           \n            num_training_instances = len(user_input)\n            total_loss = 0.0\n            training_start_time = time()\n            \n            if self.is_pairwise is True:\n                for bat_user_input_id, bat_users_pos, bat_users_neg, bat_idx_pos, bat_idx_neg, \\\n                        bat_items_pos, bat_items_neg, bat_item_input_recent in data_iter:\n                    bat_users_pos = pad_sequences(bat_users_pos, value=self.num_items)\n                    bat_users_neg = pad_sequences(bat_users_neg, value=self.num_items)\n                    feed_dict = {self.user_input_id: bat_user_input_id,\n                                 self.user_input: bat_users_pos,\n                                 self.user_input_neg: bat_users_neg,\n                                 self.num_idx: bat_idx_pos,\n                                 self.num_idx_neg: bat_idx_neg,\n                                 self.item_input: bat_items_pos,\n                                 self.item_input_neg: bat_items_neg,\n                                 self.item_input_recent: bat_item_input_recent}\n\n                    loss, _ = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n                    total_loss += loss\n            else:\n                for bat_user_input_id, bat_users, bat_idx, bat_items, bat_item_input_recent, bat_labels in data_iter:\n                    bat_users = pad_sequences(bat_users, value=self.num_items)\n                    feed_dict = {self.user_input_id: bat_user_input_id,\n                                 self.user_input: bat_users,\n                                 self.num_idx: bat_idx,\n                                 self.item_input: bat_items,\n                                 self.item_input_recent: bat_item_input_recent,\n                                 self.labels: bat_labels}\n                    \n                    loss, _ = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n                    total_loss += loss\n                \n            self.logger.info(""[iter %d : loss : %f, time: %f]"" %\n                             (epoch, total_loss/num_training_instances, time()-training_start_time))\n            \n            if epoch % self.verbose == 0:\n                self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate()))\n\n    @timer\n    def evaluate(self):\n        return self.evaluator.evaluate(self)                \n\n    def predict(self, user_ids, candidate_items_userids):\n        ratings = []\n        if candidate_items_userids is None:\n            all_items = np.arange(self.num_items)\n            for user_id in user_ids:\n                items_by_user_id = self.train_dict[user_id]\n                num_idx = len(items_by_user_id)\n                # Get prediction scores\n                item_idx = np.full(self.num_items, num_idx, dtype=np.int32)\n                item_recent = []\n                user_input = []\n                user_input.extend([items_by_user_id]*self.num_items)\n                for _ in range(self.num_items):\n                    item_recent.append(items_by_user_id[len(items_by_user_id)-self.high_order:])\n                users = np.full(self.num_items, user_id, dtype=np.int32)\n                feed_dict = {self.user_input_id: users,\n                             self.user_input: user_input,\n                             self.num_idx: item_idx,\n                             self.item_input: all_items,\n                             self.item_input_recent: item_recent}\n                ratings.append(self.sess.run(self.output, feed_dict=feed_dict))\n                \n        else:\n            for user_id, eval_items_by_user_id in zip(user_ids, candidate_items_userids):\n                items_by_user_id = self.train_dict[user_id]\n                num_idx = len(items_by_user_id)\n                # Get prediction scores\n                item_idx = np.full(len(eval_items_by_user_id), num_idx, dtype=np.int32)\n                user_input = []\n                user_input.extend([items_by_user_id]*len(eval_items_by_user_id))\n                item_recent = []\n                for _ in range(len(eval_items_by_user_id)):\n                    item_recent.append(items_by_user_id[len(items_by_user_id)-self.high_order:])\n                users = np.full(len(eval_items_by_user_id), user_id, dtype=np.int32)\n                feed_dict = {self.user_input_id: users,\n                             self.user_input: user_input,\n                             self.num_idx: item_idx,\n                             self.item_input: eval_items_by_user_id,\n                             self.item_input_recent: item_recent}\n                ratings.append(self.sess.run(self.output, feed_dict=feed_dict))\n        return ratings\n'"
model/sequential_recommender/GRU4Rec.py,33,"b'""""""\nPaper: Session-based Recommendations with Recurrent Neural Networks\nAuthor: Bal\xc3\xa1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk\nReference: https://github.com/hidasib/GRU4Rec\n           https://github.com/Songweiping/GRU4Rec_TensorFlow\n@author: Zhongchuan Sun\n""""""\n\nimport numpy as np\nfrom model.AbstractRecommender import SeqAbstractRecommender\nimport tensorflow as tf\nfrom util import log_loss, l2_loss\n\n\nclass GRU4Rec(SeqAbstractRecommender):\n    def __init__(self, sess, dataset, conf):\n        super(GRU4Rec, self).__init__(dataset, conf)\n        self.train_matrix = dataset.train_matrix\n        self.dataset = dataset\n\n        self.users_num, self.items_num = self.train_matrix.shape\n\n        self.lr = conf[""lr""]\n        self.reg = conf[""reg""]\n        self.layers = conf[""layers""]\n        self.batch_size = conf[""batch_size""]\n        self.epochs = conf[""epochs""]\n\n        if conf[""hidden_act""] == ""relu"":\n            self.hidden_act = tf.nn.relu\n        elif conf[""hidden_act""] == ""tanh"":\n            self.hidden_act = tf.nn.tanh\n        else:\n            raise ValueError(""There is not hidden_act named \'%s\'."" % conf[""hidden_act""])\n\n        # final_act = leaky-relu\n        if conf[""final_act""] == ""relu"":\n            self.final_act = tf.nn.relu\n        elif conf[""final_act""] == ""linear"":\n            self.final_act = tf.identity\n        elif conf[""final_act""] == ""leaky_relu"":\n            self.final_act = tf.nn.leaky_relu\n        else:\n            raise ValueError(""There is not final_act named \'%s\'."" % conf[""final_act""])\n\n        if conf[""loss""] == ""bpr"":\n            self.loss_fun = self._bpr_loss\n        elif conf[""loss""] == ""top1"":\n            self.loss_fun = self._top1_loss\n        else:\n            raise ValueError(""There is not loss named \'%s\'."" % conf[""loss""])\n\n        self.data_uit, self.offset_idx = self._init_data()\n\n        # for sampling negative items\n        _, pop = np.unique(self.data_uit[:, 1], return_counts=True)\n        pop_cumsum = np.cumsum(pop)\n        self.pop_cumsum = pop_cumsum / pop_cumsum[-1]\n\n        self.sess = sess\n\n    def _init_data(self):\n        time_dok = self.dataset.time_matrix.todok()\n        data_uit = [[row, col, time] for (row, col), time in time_dok.items()]\n        data_uit.sort(key=lambda x: (x[0], x[-1]))\n        data_uit = np.array(data_uit, dtype=np.int32)\n        _, idx = np.unique(data_uit[:, 0], return_index=True)\n        offset_idx = np.zeros(len(idx)+1, dtype=np.int32)\n        offset_idx[:-1] = idx\n        offset_idx[-1] = len(data_uit)\n\n        return data_uit, offset_idx\n\n    def _create_variable(self):\n        self.X_ph = tf.placeholder(tf.int32, [self.batch_size], name=\'input\')\n        self.Y_ph = tf.placeholder(tf.int32, [self.batch_size], name=\'output\')\n        self.state_ph = [tf.placeholder(tf.float32, [self.batch_size, n_unit], name=\'layer_%d_state\' % idx)\n                         for idx, n_unit in enumerate(self.layers)]\n\n        init = tf.random.truncated_normal([self.items_num, self.layers[0]], mean=0.0, stddev=0.01)\n        self.input_embeddings = tf.Variable(init, dtype=tf.float32, name=""input_embeddings"")\n\n        init = tf.random.truncated_normal([self.items_num, self.layers[-1]], mean=0.0, stddev=0.01)\n        self.item_embeddings = tf.Variable(init, dtype=tf.float32, name=""item_embeddings"")\n        self.item_biases = tf.Variable(tf.zeros([self.items_num]), dtype=tf.float32, name=""item_biases"")\n\n    def _bpr_loss(self, logits):\n        # logits: (b, size_y)\n        pos_logits = tf.matrix_diag_part(logits)  # (b,)\n        pos_logits = tf.reshape(pos_logits, shape=[-1, 1])  # (b, 1)\n        loss = tf.reduce_mean(log_loss(pos_logits-logits))\n        return loss\n\n    def _top1_loss(self, logits):\n        # logits: (b, size_y)\n        pos_logits = tf.matrix_diag_part(logits)  # (b,)\n        pos_logits = tf.reshape(pos_logits, shape=[-1, 1])  # (b, 1)\n        loss1 = tf.reduce_mean(tf.sigmoid(-pos_logits + logits), axis=-1)  # (b,)\n        loss2 = tf.reduce_mean(tf.sigmoid(tf.pow(logits, 2)), axis=-1) - \\\n                tf.squeeze(tf.sigmoid(tf.pow(pos_logits, 2))/self.batch_size)  # (b,)\n        return tf.reduce_mean(loss1+loss2)\n\n    def build_graph(self):\n        self._create_variable()\n        # get embedding and bias\n        # b: batch size\n        # l1: the dim of the first layer\n        # ln: the dim of the last layer\n        # size_y: the length of Y_ph, i.e., n_sample+batch_size\n\n        cells = [tf.nn.rnn_cell.GRUCell(size, activation=self.hidden_act) for size in self.layers]\n        drop_cell = [tf.nn.rnn_cell.DropoutWrapper(cell) for cell in cells]\n        stacked_cell = tf.nn.rnn_cell.MultiRNNCell(drop_cell)\n        inputs = tf.nn.embedding_lookup(self.input_embeddings, self.X_ph)  # (b, l1)\n        outputs, state = stacked_cell(inputs, state=self.state_ph)\n        self.u_emb = outputs  # outputs: (b, ln)\n        self.final_state = state  # [(b, l1), (b, l2), ..., (b, ln)]\n\n        # for training\n        items_embed = tf.nn.embedding_lookup(self.item_embeddings, self.Y_ph)  # (size_y, ln)\n        items_bias = tf.gather(self.item_biases, self.Y_ph)  # (size_y,)\n\n        logits = tf.matmul(outputs, items_embed, transpose_b=True) + items_bias  # (b, size_y)\n        logits = self.final_act(logits)\n\n        loss = self.loss_fun(logits)\n\n        # reg loss\n\n        reg_loss = l2_loss(inputs, items_embed, items_bias)\n        final_loss = loss + self.reg*reg_loss\n        self.update_opt = tf.train.AdamOptimizer(self.lr).minimize(final_loss)\n\n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n\n        data_uit, offset_idx = self.data_uit, self.offset_idx\n        data_items = data_uit[:, 1]\n\n        for epoch in range(self.epochs):\n            state = [np.zeros([self.batch_size, n_unit], dtype=np.float32) for n_unit in self.layers]\n            user_idx = np.random.permutation(len(offset_idx) - 1)\n            iters = np.arange(self.batch_size, dtype=np.int32)\n            maxiter = iters.max()\n            start = offset_idx[user_idx[iters]]\n            end = offset_idx[user_idx[iters]+1]\n            finished = False\n            while not finished:\n                min_len = (end - start).min()\n                out_idx = data_items[start]\n                for i in range(min_len-1):\n                    in_idx = out_idx\n                    out_idx = data_items[start+i+1]\n                    out_items = out_idx\n\n                    feed = {self.X_ph: in_idx, self.Y_ph: out_items}\n                    for l in range(len(self.layers)):\n                        feed[self.state_ph[l]] = state[l]\n\n                    _, state = self.sess.run([self.update_opt, self.final_state], feed_dict=feed)\n\n                start = start+min_len-1\n                mask = np.arange(len(iters))[(end - start) <= 1]\n                for idx in mask:\n                    maxiter += 1\n                    if maxiter >= len(offset_idx)-1:\n                        finished = True\n                        break\n                    iters[idx] = maxiter\n                    start[idx] = offset_idx[user_idx[maxiter]]\n                    end[idx] = offset_idx[user_idx[maxiter]+1]\n                if len(mask):\n                    for i in range(len(self.layers)):\n                        state[i][mask] = 0\n\n            result = self.evaluate_model()\n            self.logger.info(""epoch %d:\\t%s"" % (epoch, result))\n\n    def _get_user_embeddings(self):\n        users = np.arange(self.users_num, dtype=np.int32)\n        u_nnz = np.array([self.train_matrix[u].nnz for u in users], dtype=np.int32)\n        users = users[np.argsort(-u_nnz)]\n        user_embeddings = np.zeros([self.users_num, self.layers[-1]], dtype=np.float32)  # saving user embedding\n\n        data_uit, offset_idx = self.data_uit, self.offset_idx\n        data_items = data_uit[:, 1]\n\n        state = [np.zeros([self.batch_size, n_unit], dtype=np.float32) for n_unit in self.layers]\n        batch_iter = np.arange(self.batch_size, dtype=np.int32)\n        next_iter = batch_iter.max() + 1\n\n        start = offset_idx[users[batch_iter]]\n        end = offset_idx[users[batch_iter] + 1]  # the start index of next user\n\n        batch_mask = np.ones([self.batch_size], dtype=np.int32)\n        while np.sum(batch_mask) > 0:\n            min_len = (end - start).min()\n\n            for i in range(min_len):\n                cur_items = data_items[start + i]\n                feed = {self.X_ph: cur_items}\n                for l in range(len(self.layers)):\n                    feed[self.state_ph[l]] = state[l]\n\n                u_emb, state = self.sess.run([self.u_emb, self.final_state], feed_dict=feed)\n\n            start = start + min_len\n            mask = np.arange(self.batch_size)[(end - start) == 0]\n            for idx in mask:\n                u = users[batch_iter[idx]]\n                user_embeddings[u] = u_emb[idx]  # saving user embedding\n                if next_iter < self.users_num:\n                    batch_iter[idx] = next_iter\n                    start[idx] = offset_idx[users[next_iter]]\n                    end[idx] = offset_idx[users[next_iter] + 1]\n                    next_iter += 1\n                else:\n                    batch_mask[idx] = 0\n                    start[idx] = 0\n                    end[idx] = offset_idx[-1]\n\n            for i, _ in enumerate(self.layers):\n                state[i][mask] = 0\n\n        return user_embeddings\n\n    def evaluate_model(self):\n        self.cur_user_embeddings = self._get_user_embeddings()\n        self.cur_item_embeddings, self.cur_item_biases = self.sess.run([self.item_embeddings, self.item_biases])\n        return self.evaluator.evaluate(self)\n\n    def predict(self, users, items=None):\n        user_embeddings = self.cur_user_embeddings[users]\n        all_ratings = np.matmul(user_embeddings, self.cur_item_embeddings.T) + self.cur_item_biases\n\n        # final_act = leaky-relu\n        if self.final_act == tf.nn.relu:\n            all_ratings = np.maximum(all_ratings, 0)\n        elif self.final_act == tf.identity:\n            all_ratings = all_ratings\n        elif self.final_act == tf.nn.leaky_relu:\n            all_ratings = np.maximum(all_ratings, all_ratings*0.2)\n        else:\n            pass\n\n        all_ratings = np.array(all_ratings, dtype=np.float32)\n        if items is not None:\n            all_ratings = [all_ratings[idx][item] for idx, item in enumerate(items)]\n\n        return all_ratings\n'"
model/sequential_recommender/GRU4RecPlus.py,40,"b'""""""\nPaper: Recurrent Neural Networks with Top-k Gains for Session-based Recommendations\nAuthor: Bal\xc3\xa1zs Hidasi, and Alexandros Karatzoglou\nReference: https://github.com/hidasib/GRU4Rec\n           https://github.com/Songweiping/GRU4Rec_TensorFlow\n@author: Zhongchuan Sun\n""""""\n\nimport numpy as np\nfrom model.AbstractRecommender import SeqAbstractRecommender\nimport tensorflow as tf\nfrom util import l2_loss\n\n\nclass GRU4RecPlus(SeqAbstractRecommender):\n    def __init__(self, sess, dataset, conf):\n        super(GRU4RecPlus, self).__init__(dataset, conf)\n        self.train_matrix = dataset.train_matrix\n        self.dataset = dataset\n\n        self.users_num, self.items_num = self.train_matrix.shape\n\n        self.lr = conf[""lr""]\n        self.reg = conf[""reg""]\n        self.layers = conf[""layers""]\n        self.batch_size = conf[""batch_size""]\n        self.n_sample = conf[""n_sample""]\n        self.sample_alpha = conf[""sample_alpha""]\n        self.epochs = conf[""epochs""]\n        self.bpr_reg = conf[""bpr_reg""]\n\n        if conf[""hidden_act""] == ""relu"":\n            self.hidden_act = tf.nn.relu\n        elif conf[""hidden_act""] == ""tanh"":\n            self.hidden_act = tf.nn.tanh\n        else:\n            raise ValueError(""There is not hidden_act named \'%s\'."" % conf[""hidden_act""])\n\n        # final_act = leaky-relu\n        if conf[""final_act""] == ""relu"":\n            self.final_act = tf.nn.relu\n        elif conf[""final_act""] == ""linear"":\n            self.final_act = tf.identity\n        elif conf[""final_act""] == ""leaky_relu"":\n            self.final_act = tf.nn.leaky_relu\n        else:\n            raise ValueError(""There is not final_act named \'%s\'."" % conf[""final_act""])\n\n        if conf[""loss""] == ""bpr_max"":\n            self.loss_fun = self._bpr_max_loss\n        elif conf[""loss""] == ""top1_max"":\n            self.loss_fun = self._top1_max_loss\n        else:\n            raise ValueError(""There is not loss named \'%s\'."" % conf[""loss""])\n\n        self.data_uit, self.offset_idx = self._init_data()\n\n        # for sampling negative items\n        _, pop = np.unique(self.data_uit[:, 1], return_counts=True)\n        pop = np.power(pop, self.sample_alpha)\n        pop_cumsum = np.cumsum(pop)\n        self.pop_cumsum = pop_cumsum / pop_cumsum[-1]\n\n        self.sess = sess\n\n    def _init_data(self):\n        time_dok = self.dataset.time_matrix.todok()\n        data_uit = [[row, col, time] for (row, col), time in time_dok.items()]\n        data_uit.sort(key=lambda x: (x[0], x[-1]))\n        data_uit = np.array(data_uit, dtype=np.int32)\n        _, idx = np.unique(data_uit[:, 0], return_index=True)\n        offset_idx = np.zeros(len(idx)+1, dtype=np.int32)\n        offset_idx[:-1] = idx\n        offset_idx[-1] = len(data_uit)\n\n        return data_uit, offset_idx\n\n    def _create_variable(self):\n        self.X_ph = tf.placeholder(tf.int32, [self.batch_size], name=\'input\')\n        self.Y_ph = tf.placeholder(tf.int32, [self.batch_size+self.n_sample], name=\'output\')\n        self.state_ph = [tf.placeholder(tf.float32, [self.batch_size, n_unit], name=\'layer_%d_state\' % idx)\n                         for idx, n_unit in enumerate(self.layers)]\n\n        init = tf.random.truncated_normal([self.items_num, self.layers[0]], mean=0.0, stddev=0.01)\n        self.input_embeddings = tf.Variable(init, dtype=tf.float32, name=""input_embeddings"")\n\n        init = tf.random.truncated_normal([self.items_num, self.layers[-1]], mean=0.0, stddev=0.01)\n        self.item_embeddings = tf.Variable(init, dtype=tf.float32, name=""item_embeddings"")\n        self.item_biases = tf.Variable(tf.zeros([self.items_num]), dtype=tf.float32, name=""item_biases"")\n\n    def _softmax_neg(self, logits):\n        # logits: (b, size_y)\n        hm = 1.0 - tf.eye(tf.shape(logits)[0], tf.shape(logits)[1])\n        logits = logits * hm\n        logits = logits - tf.reduce_max(logits, axis=1, keep_dims=True)\n        e_x = tf.exp(logits) * hm  # (b, size_y)\n        e_x = e_x / tf.reduce_sum(e_x, axis=1, keep_dims=True)\n        return e_x  # (b, size_y)\n\n    def _bpr_max_loss(self, logits):\n        # logits: (b, size_y)\n        softmax_scores = self._softmax_neg(logits)  # (b, size_y)\n        pos_logits = tf.matrix_diag_part(logits)  # (b,)\n        pos_logits = tf.reshape(pos_logits, shape=[-1, 1])  # (b, 1)\n        prob = tf.sigmoid((pos_logits - logits))  # (b, size_y)\n        prob = tf.reduce_sum(tf.multiply(prob, softmax_scores), axis=1)  # (b,)\n        loss = -tf.log(prob + 1e-24)\n        reg_loss = tf.reduce_sum(tf.multiply(tf.pow(logits, 2), softmax_scores), axis=1)  # (b,)\n\n        return tf.reduce_mean(loss + self.bpr_reg*reg_loss)\n\n    def _top1_max_loss(self, logits):\n        softmax_scores = self._softmax_neg(logits)  # (b, size_y)\n\n        pos_logits = tf.matrix_diag_part(logits)  # (b,)\n        pos_logits = tf.reshape(pos_logits, shape=[-1, 1])  # (b, 1)\n        prob = tf.sigmoid(-pos_logits + logits) + tf.sigmoid(tf.pow(logits, 2))\n        loss = tf.reduce_sum(tf.multiply(prob, softmax_scores), axis=1)\n\n        return tf.reduce_mean(loss)\n\n    def build_graph(self):\n        self._create_variable()\n        # get embedding and bias\n        # b: batch size\n        # l1: the dim of the first layer\n        # ln: the dim of the last layer\n        # size_y: the length of Y_ph, i.e., n_sample+batch_size\n\n        cells = [tf.nn.rnn_cell.GRUCell(size, activation=self.hidden_act) for size in self.layers]\n        drop_cell = [tf.nn.rnn_cell.DropoutWrapper(cell) for cell in cells]\n        stacked_cell = tf.nn.rnn_cell.MultiRNNCell(drop_cell)\n        inputs = tf.nn.embedding_lookup(self.input_embeddings, self.X_ph)  # (b, l1)\n        outputs, state = stacked_cell(inputs, state=self.state_ph)\n        self.u_emb = outputs  # outputs: (b, ln)\n        self.final_state = state  # [(b, l1), (b, l2), ..., (b, ln)]\n\n        # for training\n        items_embed = tf.nn.embedding_lookup(self.item_embeddings, self.Y_ph)  # (size_y, ln)\n        items_bias = tf.gather(self.item_biases, self.Y_ph)  # (size_y,)\n\n        logits = tf.matmul(outputs, items_embed, transpose_b=True) + items_bias  # (b, size_y)\n        logits = self.final_act(logits)\n\n        loss = self.loss_fun(logits)\n\n        # reg loss\n\n        reg_loss = l2_loss(inputs, items_embed, items_bias)\n        final_loss = loss + self.reg*reg_loss\n        self.update_opt = tf.train.AdamOptimizer(self.lr).minimize(final_loss)\n\n    def _sample_neg_items(self, size):\n        samples = np.searchsorted(self.pop_cumsum, np.random.rand(size))\n        return samples\n\n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n\n        data_uit, offset_idx = self.data_uit, self.offset_idx\n        data_items = data_uit[:, 1]\n\n        for epoch in range(self.epochs):\n            state = [np.zeros([self.batch_size, n_unit], dtype=np.float32) for n_unit in self.layers]\n            user_idx = np.random.permutation(len(offset_idx) - 1)\n            iters = np.arange(self.batch_size, dtype=np.int32)\n            maxiter = iters.max()\n            start = offset_idx[user_idx[iters]]\n            end = offset_idx[user_idx[iters]+1]\n            finished = False\n            while not finished:\n                min_len = (end - start).min()\n                out_idx = data_items[start]\n                for i in range(min_len-1):\n                    in_idx = out_idx\n                    out_idx = data_items[start+i+1]\n                    out_items = out_idx\n                    if self.n_sample:\n                        neg_items = self._sample_neg_items(self.n_sample)\n                        out_items = np.hstack([out_items, neg_items])\n\n                    feed = {self.X_ph: in_idx, self.Y_ph: out_items}\n                    for l in range(len(self.layers)):\n                        feed[self.state_ph[l]] = state[l]\n\n                    _, state = self.sess.run([self.update_opt, self.final_state], feed_dict=feed)\n\n                start = start+min_len-1\n                mask = np.arange(len(iters))[(end - start) <= 1]\n                for idx in mask:\n                    maxiter += 1\n                    if maxiter >= len(offset_idx)-1:\n                        finished = True\n                        break\n                    iters[idx] = maxiter\n                    start[idx] = offset_idx[user_idx[maxiter]]\n                    end[idx] = offset_idx[user_idx[maxiter]+1]\n                if len(mask):\n                    for i in range(len(self.layers)):\n                        state[i][mask] = 0\n\n            result = self.evaluate_model()\n            self.logger.info(""epoch %d:\\t%s"" % (epoch, result))\n\n    def _get_user_embeddings(self):\n        users = np.arange(self.users_num, dtype=np.int32)\n        u_nnz = np.array([self.train_matrix[u].nnz for u in users], dtype=np.int32)\n        users = users[np.argsort(-u_nnz)]\n        user_embeddings = np.zeros([self.users_num, self.layers[-1]], dtype=np.float32)  # saving user embedding\n\n        data_uit, offset_idx = self.data_uit, self.offset_idx\n        data_items = data_uit[:, 1]\n\n        state = [np.zeros([self.batch_size, n_unit], dtype=np.float32) for n_unit in self.layers]\n        batch_iter = np.arange(self.batch_size, dtype=np.int32)\n        next_iter = batch_iter.max() + 1\n\n        start = offset_idx[users[batch_iter]]\n        end = offset_idx[users[batch_iter] + 1]  # the start index of next user\n\n        batch_mask = np.ones([self.batch_size], dtype=np.int32)\n        while np.sum(batch_mask) > 0:\n            min_len = (end - start).min()\n\n            for i in range(min_len):\n                cur_items = data_items[start + i]\n                feed = {self.X_ph: cur_items}\n                for l in range(len(self.layers)):\n                    feed[self.state_ph[l]] = state[l]\n\n                u_emb, state = self.sess.run([self.u_emb, self.final_state], feed_dict=feed)\n\n            start = start + min_len\n            mask = np.arange(self.batch_size)[(end - start) == 0]\n            for idx in mask:\n                u = users[batch_iter[idx]]\n                user_embeddings[u] = u_emb[idx]  # saving user embedding\n                if next_iter < self.users_num:\n                    batch_iter[idx] = next_iter\n                    start[idx] = offset_idx[users[next_iter]]\n                    end[idx] = offset_idx[users[next_iter] + 1]\n                    next_iter += 1\n                else:\n                    batch_mask[idx] = 0\n                    start[idx] = 0\n                    end[idx] = offset_idx[-1]\n\n            for i, _ in enumerate(self.layers):\n                state[i][mask] = 0\n\n        return user_embeddings\n\n    def evaluate_model(self):\n        self.cur_user_embeddings = self._get_user_embeddings()\n        self.cur_item_embeddings, self.cur_item_biases = self.sess.run([self.item_embeddings, self.item_biases])\n        return self.evaluator.evaluate(self)\n\n    def predict(self, users, items=None):\n        user_embeddings = self.cur_user_embeddings[users]\n        all_ratings = np.matmul(user_embeddings, self.cur_item_embeddings.T) + self.cur_item_biases\n\n        # final_act = leaky-relu\n        if self.final_act == tf.nn.relu:\n            all_ratings = np.maximum(all_ratings, 0)\n        elif self.final_act == tf.identity:\n            all_ratings = all_ratings\n        elif self.final_act == tf.nn.leaky_relu:\n            all_ratings = np.maximum(all_ratings, all_ratings*0.2)\n        else:\n            pass\n        all_ratings = np.array(all_ratings, dtype=np.float32)\n\n        if items is not None:\n            all_ratings = [all_ratings[idx][item] for idx, item in enumerate(items)]\n        return all_ratings\n'"
model/sequential_recommender/HRM.py,24,"b'""""""\nReference: Pengfei Wang et al., ""Learning Hierarchical Representation Model for NextBasket Recommendation."" in SIGIR 2015.\n@author: wubin\n""""""\nimport tensorflow as tf\nimport numpy as np\nfrom time import time\nfrom util import timer\nfrom util import learner, tool\nfrom util.tool import csr_to_user_dict_bytime\nfrom model.AbstractRecommender import SeqAbstractRecommender\nfrom util import l2_loss\nfrom data import TimeOrderPointwiseSampler\n\n\nclass HRM(SeqAbstractRecommender):\n    def __init__(self, sess, dataset, conf):\n        super(HRM, self).__init__(dataset, conf)\n        self.learning_rate = conf[""learning_rate""]\n        self.embedding_size = conf[""embedding_size""]\n        self.learner = conf[""learner""]\n        self.num_epochs = conf[""epochs""]\n        self.reg_mf = conf[""reg_mf""]\n        self.pre_agg = conf[""pre_agg""]\n        self.loss_function = conf[""loss_function""]\n        self.session_agg = conf[""session_agg""]\n        self.batch_size = conf[""batch_size""]\n        self.high_order = conf[""high_order""]\n        self.verbose = conf[""verbose""]\n        self.num_negatives = conf[""num_neg""]\n        self.init_method = conf[""init_method""]\n        self.stddev = conf[""stddev""]\n        self.num_users = dataset.num_users\n        self.num_items = dataset.num_items\n        self.dataset = dataset\n        self.train_dict = csr_to_user_dict_bytime(dataset.time_matrix, dataset.train_matrix)\n        self.sess = sess\n\n    def _create_placeholders(self):\n        with tf.name_scope(""input_data""):\n            self.user_input = tf.placeholder(tf.int32, shape=[None, ], name=""user_input"")\n            self.item_input = tf.placeholder(tf.int32, shape=[None, ], name=""item_input_pos"")\n            self.item_input_recent = tf.placeholder(tf.int32, shape=[None, None], name = ""item_input_recent"")\n            self.labels = tf.placeholder(tf.float32, shape=[None, ], name=""labels"")\n\n    def _create_variables(self):\n        with tf.name_scope(""embedding""):\n            initializer = tool.get_initializer(self.init_method, self.stddev)\n            self.user_embeddings = tf.Variable(initializer([self.num_users, self.embedding_size]),\n                                               name=\'user_embeddings\', dtype=tf.float32)  # (users, embedding_size)\n            self.item_embeddings = tf.Variable(initializer([self.num_items, self.embedding_size]),\n                                               name=\'item_embeddings\', dtype=tf.float32)  # (items, embedding_size)\n    \n    def avg_pooling(self, input_embedding):\n        output = tf.reduce_mean(input_embedding, axis=1)\n        return output\n\n    def max_pooling(self, input_embedding):\n        output = tf.reduce_max(input_embedding, axis=1)\n        return output\n    \n    def _create_inference(self):\n        with tf.name_scope(""inference""):\n            # embedding look up\n            user_embedding = tf.nn.embedding_lookup(self.user_embeddings, self.user_input)\n            item_embedding_recent = tf.nn.embedding_lookup(self.item_embeddings, self.item_input_recent)\n            item_embedding = tf.nn.embedding_lookup(self.item_embeddings, self.item_input)\n            if self.high_order > 1:\n                if self.session_agg == ""max"":\n                    item_embedding_short = self.max_pooling(item_embedding_recent)  \n                else :\n                    item_embedding_short = self.avg_pooling(item_embedding_recent) \n                concat_user_item = tf.concat([tf.expand_dims(user_embedding, 1),\n                                              tf.expand_dims(item_embedding_short, 1)], axis=1)\n            else:\n                concat_user_item = tf.concat([tf.expand_dims(user_embedding, 1),\n                                              tf.expand_dims(item_embedding_recent, 1)], axis=1)\n            if self.pre_agg == ""max"":\n                hybrid_user_embedding = self.max_pooling(concat_user_item)\n            else:\n                hybrid_user_embedding = self.avg_pooling(concat_user_item)\n            predict_vector = tf.multiply(hybrid_user_embedding, item_embedding)\n            predict = tf.reduce_sum(predict_vector, 1)\n            return user_embedding, item_embedding, item_embedding_recent, predict\n\n    def _create_loss(self):\n        with tf.name_scope(""loss""):\n            # loss for L(Theta)\n            p1, q1, r1, self.output = self._create_inference()\n            self.loss = learner.pointwise_loss(self.loss_function, self.labels, self.output) + \\\n                        self.reg_mf * l2_loss(p1, r1, q1)\n\n    def _create_optimizer(self):\n        with tf.name_scope(""learner""):\n            self.optimizer = learner.optimizer(self.learner, self.loss, self.learning_rate)\n                \n    def build_graph(self):\n        self._create_placeholders()\n        self._create_variables()\n        self._create_loss()\n        self._create_optimizer()\n\n    # ---------- training process -------\n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n        data_iter = TimeOrderPointwiseSampler(self.dataset, high_order=self.high_order,\n                                              neg_num=self.num_negatives,\n                                              batch_size=self.batch_size, shuffle=True)\n\n        for epoch in  range(1, self.num_epochs+1):\n            num_training_instances = len(data_iter)\n            total_loss = 0.0\n            training_start_time = time()\n      \n            for bat_users, bat_items_recent, bat_items, bat_labels in data_iter:\n                    \n                    feed_dict = {self.user_input: bat_users,\n                                 self.item_input: bat_items,\n                                 self.item_input_recent: bat_items_recent,\n                                 self.labels: bat_labels}\n    \n                    loss, _ = self.sess.run((self.loss,self.optimizer),feed_dict=feed_dict)\n                    total_loss += loss\n                    \n            self.logger.info(""[iter %d : loss : %f, time: %f]"" %\n                             (epoch, total_loss/num_training_instances, time()-training_start_time))\n            \n            if epoch % self.verbose == 0:\n                self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate()))\n\n    @timer\n    def evaluate(self):\n        return self.evaluator.evaluate(self)\n    \n    def predict(self, user_ids, candidate_items_user_ids):\n        ratings = []\n        if candidate_items_user_ids is None:\n            all_items = np.arange(self.num_items)\n            for user_id in user_ids:\n                cand_items = self.train_dict[user_id]\n                item_recent = []\n    \n                for _ in range(self.num_items):\n                    item_recent.append(cand_items[len(cand_items)-self.high_order:])\n                users = np.full(self.num_items, user_id, dtype=np.int32)\n                feed_dict = {self.user_input: users,\n                             self.item_input_recent: item_recent,\n                             self.item_input: all_items}\n                ratings.append(self.sess.run(self.output, feed_dict=feed_dict))  \n        \n        else:\n            for user_id, items_by_user_id in zip(user_ids, candidate_items_user_ids):\n                cand_items = self.train_dict[user_id]\n                item_recent = []\n    \n                for _ in range(len(items_by_user_id)):\n                    item_recent.append(cand_items[len(cand_items)-self.high_order:])\n                users = np.full(len(items_by_user_id), user_id, dtype=np.int32)\n                feed_dict = {self.user_input: users,\n                             self.item_input_recent: item_recent,\n                             self.item_input: items_by_user_id}\n                ratings.append(self.sess.run(self.output, feed_dict=feed_dict))\n        return ratings\n'"
model/sequential_recommender/NPE.py,22,"b'""""""\nReference: ThaiBinh Nguyen, et al. ""NPE: Neural Personalized Embedding for Collaborative Filtering"" in ijcai2018\n@author: wubin\n""""""\nfrom model.AbstractRecommender import SeqAbstractRecommender\nimport tensorflow as tf\nimport numpy as np\nfrom time import time\nfrom util import learner, tool\nfrom util.tool import csr_to_user_dict_bytime\nfrom util import timer\nfrom util import l2_loss\nfrom data import TimeOrderPointwiseSampler\n\n\nclass NPE(SeqAbstractRecommender):\n    def __init__(self, sess, dataset, conf):  \n        super(NPE, self).__init__(dataset, conf)\n        self.learning_rate = conf[""learning_rate""]\n        self.embedding_size = conf[""embedding_size""]\n        self.learner = conf[""learner""]\n        self.loss_function = conf[""loss_function""]\n        self.num_epochs = conf[""epochs""]\n        self.reg = conf[""reg""]\n        self.batch_size = conf[""batch_size""]\n        self.high_order = conf[""high_order""]\n        self.verbose = conf[""verbose""]\n        self.num_negatives = conf[""num_neg""]\n        self.init_method = conf[""init_method""]\n        self.stddev = conf[""stddev""]\n        self.num_users = dataset.num_users\n        self.num_items = dataset.num_items\n        self.dataset = dataset\n        self.train_dict = csr_to_user_dict_bytime(dataset.time_matrix, dataset.train_matrix)\n        self.sess = sess\n\n    def _create_placeholders(self):\n        with tf.name_scope(""input_data""):\n            self.user_input = tf.placeholder(tf.int32, shape=[None, ], name=""user_input"")\n            self.item_input = tf.placeholder(tf.int32, shape=[None, ], name=""item_input"")\n            self.item_input_recent = tf.placeholder(tf.int32, shape=[None, None], name=""item_input_recent"")\n            self.labels = tf.placeholder(tf.float32, shape=[None, ], name=""labels"")\n\n    def _create_variables(self):\n        with tf.name_scope(""embedding""):\n            initializer = tool.get_initializer(self.init_method, self.stddev)\n            self.embeddings_UI = tf.Variable(initializer([self.num_users, self.embedding_size]),\n                                             name=\'embeddings_UI\', dtype=tf.float32)  # (users, embedding_size)\n            self.embeddings_IU = tf.Variable(initializer([self.num_items, self.embedding_size]),\n                                             name=\'embeddings_IU\', dtype=tf.float32)  # (items, embedding_size)\n            self.embeddings_IL = tf.Variable(initializer([self.num_items, self.embedding_size]),\n                                             name=\'embeddings_IL\', dtype=tf.float32)\n\n    def _create_inference(self):\n        with tf.name_scope(""inference""):\n            # embedding look up\n            embeddings_UI_u = tf.nn.embedding_lookup(self.embeddings_UI, self.user_input)\n            embeddings_IU_i = tf.nn.embedding_lookup(self.embeddings_IU,self.item_input)\n            embeddings_LI_l = tf.nn.embedding_lookup(self.embeddings_IL, self.item_input_recent)\n            \n            context_embedding = tf.reduce_sum(embeddings_LI_l, 1)\n            predict_vector = tf.multiply(tf.nn.relu(embeddings_UI_u), tf.nn.relu(embeddings_IU_i)) + \\\n                             tf.multiply(tf.nn.relu(embeddings_IU_i), tf.nn.relu(context_embedding))\n            predict = tf.reduce_sum(predict_vector, 1)\n            return embeddings_UI_u, embeddings_IU_i, embeddings_LI_l, predict\n\n    def _create_loss(self):\n        with tf.name_scope(""loss""):\n            UI_u, IU_i, LI_l, self.output = self._create_inference()\n            self.loss = learner.pointwise_loss(self.loss_function, self.labels, self.output) + \\\n                        self.reg * l2_loss(UI_u, IU_i, LI_l)\n\n    def _create_optimizer(self):\n        with tf.name_scope(""learner""):\n            self.optimizer = learner.optimizer(self.learner, self.loss, self.learning_rate)\n                \n    def build_graph(self):\n        self._create_placeholders()\n        self._create_variables()\n        self._create_loss()\n        self._create_optimizer()\n\n    # ---------- training process -------\n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n        data_iter = TimeOrderPointwiseSampler(self.dataset, high_order=self.high_order,\n                                              neg_num=self.num_negatives,\n                                              batch_size=self.batch_size, shuffle=True)\n        for epoch in range(1, self.num_epochs+1):\n            num_training_instances = len(data_iter)\n            total_loss = 0.0\n            training_start_time = time()\n\n            for bat_users, bat_items_recent, bat_items, bat_labels in data_iter:\n                    \n                    feed_dict = {self.user_input: bat_users,\n                                 self.item_input: bat_items,\n                                 self.item_input_recent: bat_items_recent,\n                                 self.labels: bat_labels}\n    \n                    loss, _ = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n                    total_loss+=loss\n                \n            self.logger.info(""[iter %d : loss : %f, time: %f]"" % (epoch, total_loss/num_training_instances,\n                                                             time()-training_start_time))\n            \n            if epoch % self.verbose == 0:\n                self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate()))\n\n    @timer\n    def evaluate(self):\n        return self.evaluator.evaluate(self)\n                \n    def predict(self, user_ids, candidate_items_userids):\n        ratings = []\n        if candidate_items_userids is None:\n            all_items = np.arange(self.num_items)\n            for user_id in user_ids:\n                cand_items = self.train_dict[user_id]\n                item_recent = []\n    \n                for _ in range(self.num_items):\n                    item_recent.append(cand_items[len(cand_items)-self.high_order:])\n                users = np.full(self.num_items, user_id, dtype=np.int32)\n                feed_dict = {self.user_input: users,\n                             self.item_input_recent: item_recent,\n                             self.item_input: all_items}\n                ratings.append(self.sess.run(self.output, feed_dict=feed_dict))  \n        \n        else:\n            for user_id, items_by_user_id in zip(user_ids, candidate_items_userids):\n                cand_items = self.train_dict[user_id]\n                item_recent = []\n    \n                for _ in range(len(items_by_user_id)):\n                    item_recent.append(cand_items[len(cand_items)-self.high_order:])\n                users = np.full(len(items_by_user_id), user_id, dtype=np.int32)\n                feed_dict = {self.user_input: users,\n                             self.item_input_recent: item_recent,\n                             self.item_input: items_by_user_id}\n                ratings.append(self.sess.run(self.output, feed_dict=feed_dict))\n        return ratings\n'"
model/sequential_recommender/SASRec.py,83,"b'""""""\nPaper: Self-Attentive Sequential Recommendation\nAuthor: Wang-Cheng Kang, and Julian McAuley\nReference: https://github.com/kang205/SASRec\n@author: Zhongchuan Sun\n""""""\n\nimport numpy as np\nfrom model.AbstractRecommender import SeqAbstractRecommender\nfrom util import DataIterator\nfrom util.tool import csr_to_user_dict_bytime\nimport tensorflow as tf\nfrom util import inner_product\nfrom util import batch_randint_choice\nfrom util import pad_sequences\n\n\ndef normalize(inputs,\n              epsilon=1e-8,\n              scope=""ln"",\n              reuse=None):\n    \'\'\'Applies layer normalization.\n\n    Args:\n      inputs: A tensor with 2 or more dimensions, where the first dimension has\n        `batch_size`.\n      epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n      scope: Optional scope for `variable_scope`.\n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n\n    Returns:\n      A tensor with the same shape and data dtype as `inputs`.\n    \'\'\'\n    with tf.variable_scope(scope, reuse=reuse):\n        inputs_shape = inputs.get_shape()\n        params_shape = inputs_shape[-1:]\n\n        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n        beta = tf.Variable(tf.zeros(params_shape))\n        gamma = tf.Variable(tf.ones(params_shape))\n        normalized = (inputs - mean) / ((variance + epsilon) ** (.5))\n        outputs = gamma * normalized + beta\n\n    return outputs\n\n\ndef embedding(inputs,\n              vocab_size,\n              num_units,\n              zero_pad=True,\n              scale=True,\n              l2_reg=0.0,\n              scope=""embedding"",\n              with_t=False,\n              reuse=None):\n    \'\'\'Embeds a given tensor.\n\n    Args:\n      inputs: A `Tensor` with type `int32` or `int64` containing the ids\n         to be looked up in `lookup table`.\n      vocab_size: An int. Vocabulary size.\n      num_units: An int. Number of embedding hidden units.\n      zero_pad: A boolean. If True, all the values of the fist row (id 0)\n        should be constant zeros.\n      scale: A boolean. If True. the outputs is multiplied by sqrt num_units.\n      scope: Optional scope for `variable_scope`.\n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n\n    Returns:\n      A `Tensor` with one more rank than inputs\'s. The last dimensionality\n        should be `num_units`.\n\n    For example,\n\n    ```\n    import tensorflow as tf\n\n    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n    outputs = embedding(inputs, 6, 2, zero_pad=True)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        print sess.run(outputs)\n    >>\n    [[[ 0.          0.        ]\n      [ 0.09754146  0.67385566]\n      [ 0.37864095 -0.35689294]]\n\n     [[-1.01329422 -1.09939694]\n      [ 0.7521342   0.38203377]\n      [-0.04973143 -0.06210355]]]\n    ```\n\n    ```\n    import tensorflow as tf\n\n    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n    outputs = embedding(inputs, 6, 2, zero_pad=False)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        print sess.run(outputs)\n    >>\n    [[[-0.19172323 -0.39159766]\n      [-0.43212751 -0.66207761]\n      [ 1.03452027 -0.26704335]]\n\n     [[-0.11634696 -0.35983452]\n      [ 0.50208133  0.53509563]\n      [ 1.22204471 -0.96587461]]]\n    ```\n    \'\'\'\n    with tf.variable_scope(scope, reuse=reuse):\n        lookup_table = tf.get_variable(\'lookup_table\',\n                                       dtype=tf.float32,\n                                       shape=[vocab_size, num_units],\n                                       # initializer=tf.contrib.layers.xavier_initializer(),\n                                       regularizer=tf.contrib.layers.l2_regularizer(l2_reg))\n        if zero_pad:\n            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n                                      lookup_table[1:, :]), 0)\n        outputs = tf.nn.embedding_lookup(lookup_table, inputs)\n\n        if scale:\n            outputs = outputs * (num_units ** 0.5)\n    if with_t:\n        return outputs, lookup_table\n    else:\n        return outputs\n\n\ndef multihead_attention(queries,\n                        keys,\n                        num_units=None,\n                        num_heads=8,\n                        dropout_rate=0,\n                        is_training=True,\n                        causality=False,\n                        scope=""multihead_attention"",\n                        reuse=None,\n                        with_qk=False):\n    \'\'\'Applies multihead attention.\n\n    Args:\n      queries: A 3d tensor with shape of [N, T_q, C_q].\n      keys: A 3d tensor with shape of [N, T_k, C_k].\n      num_units: A scalar. Attention size.\n      dropout_rate: A floating point number.\n      is_training: Boolean. Controller of mechanism for dropout.\n      causality: Boolean. If true, units that reference the future are masked.\n      num_heads: An int. Number of heads.\n      scope: Optional scope for `variable_scope`.\n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n\n    Returns\n      A 3d tensor with shape of (N, T_q, C)\n    \'\'\'\n    with tf.variable_scope(scope, reuse=reuse):\n        # Set the fall back option for num_units\n        if num_units is None:\n            num_units = queries.get_shape().as_list[-1]\n\n        # Linear projections\n        # Q = tf.layers.dense(queries, num_units, activation=tf.nn.relu) # (N, T_q, C)\n        # K = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n        # V = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n        Q = tf.layers.dense(queries, num_units, activation=None)  # (N, T_q, C)\n        K = tf.layers.dense(keys, num_units, activation=None)  # (N, T_k, C)\n        V = tf.layers.dense(keys, num_units, activation=None)  # (N, T_k, C)\n\n        # Split and concat\n        Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0)  # (h*N, T_q, C/h)\n        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0)  # (h*N, T_k, C/h)\n        V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0)  # (h*N, T_k, C/h)\n\n        # Multiplication\n        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))  # (h*N, T_q, T_k)\n\n        # Scale\n        outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)\n\n        # Key Masking\n        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1)))  # (N, T_k)\n        key_masks = tf.tile(key_masks, [num_heads, 1])  # (h*N, T_k)\n        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1])  # (h*N, T_q, T_k)\n\n        paddings = tf.ones_like(outputs) * (-2 ** 32 + 1)\n        outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs)  # (h*N, T_q, T_k)\n\n        # Causality = Future blinding\n        if causality:\n            diag_vals = tf.ones_like(outputs[0, :, :])  # (T_q, T_k)\n            tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()  # (T_q, T_k)\n            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1])  # (h*N, T_q, T_k)\n\n            paddings = tf.ones_like(masks) * (-2 ** 32 + 1)\n            outputs = tf.where(tf.equal(masks, 0), paddings, outputs)  # (h*N, T_q, T_k)\n\n        # Activation\n        outputs = tf.nn.softmax(outputs)  # (h*N, T_q, T_k)\n\n        # Query Masking\n        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1)))  # (N, T_q)\n        query_masks = tf.tile(query_masks, [num_heads, 1])  # (h*N, T_q)\n        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]])  # (h*N, T_q, T_k)\n        outputs *= query_masks  # broadcasting. (N, T_q, C)\n\n        # Dropouts\n        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n\n        # Weighted sum\n        outputs = tf.matmul(outputs, V_)  # ( h*N, T_q, C/h)\n\n        # Restore shape\n        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2)  # (N, T_q, C)\n\n        # Residual connection\n        outputs += queries\n\n        # Normalize\n        # outputs = normalize(outputs) # (N, T_q, C)\n\n    if with_qk:\n        return Q, K\n    else:\n        return outputs\n\n\ndef feedforward(inputs,\n                num_units=[2048, 512],\n                scope=""multihead_attention"",\n                dropout_rate=0.2,\n                is_training=True,\n                reuse=None):\n    \'\'\'Point-wise feed forward net.\n\n    Args:\n      inputs: A 3d tensor with shape of [N, T, C].\n      num_units: A list of two integers.\n      scope: Optional scope for `variable_scope`.\n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n\n    Returns:\n      A 3d tensor with the same shape and dtype as inputs\n    \'\'\'\n    with tf.variable_scope(scope, reuse=reuse):\n        # Inner layer\n        params = {""inputs"": inputs, ""filters"": num_units[0], ""kernel_size"": 1,\n                  ""activation"": tf.nn.relu, ""use_bias"": True}\n        outputs = tf.layers.conv1d(**params)\n        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n        # Readout layer\n        params = {""inputs"": outputs, ""filters"": num_units[1], ""kernel_size"": 1,\n                  ""activation"": None, ""use_bias"": True}\n        outputs = tf.layers.conv1d(**params)\n        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n\n        # Residual connection\n        outputs += inputs\n\n        # Normalize\n        # outputs = normalize(outputs)\n\n    return outputs\n\n\nclass SASRec(SeqAbstractRecommender):\n    def __init__(self, sess, dataset, conf):\n        super(SASRec, self).__init__(dataset, conf)\n        train_matrix, time_matrix = dataset.train_matrix, dataset.time_matrix\n        self.dataset = dataset\n\n        self.users_num, self.items_num = train_matrix.shape\n\n        self.lr = conf[""lr""]\n        self.l2_emb = conf[""l2_emb""]\n        self.hidden_units = conf[""hidden_units""]\n        self.batch_size = conf[""batch_size""]\n        self.epochs = conf[""epochs""]\n        self.dropout_rate = conf[""dropout_rate""]\n        self.max_len = conf[""max_len""]\n        self.num_blocks = conf[""num_blocks""]\n        self.num_heads = conf[""num_heads""]\n\n        self.user_pos_train = csr_to_user_dict_bytime(time_matrix, train_matrix)\n\n        self.sess = sess\n\n    def _create_variable(self):\n        # self.user_ph = tf.placeholder(tf.int32, [None], name=""user"")\n        self.item_seq_ph = tf.placeholder(tf.int32, [None, self.max_len], name=""item_seq"")\n        self.item_pos_ph = tf.placeholder(tf.int32, [None, self.max_len], name=""item_pos"")\n        self.item_neg_ph = tf.placeholder(tf.int32, [None, self.max_len], name=""item_neg"")\n        self.is_training = tf.placeholder(tf.bool, name=""training_flag"")\n\n        l2_regularizer = tf.contrib.layers.l2_regularizer(self.l2_emb)\n        item_embeddings = tf.get_variable(\'item_embeddings\', dtype=tf.float32,\n                                          shape=[self.items_num, self.hidden_units],\n                                          regularizer=l2_regularizer)\n\n        zero_pad = tf.zeros([1, self.hidden_units], name=""padding"")\n        item_embeddings = tf.concat([item_embeddings, zero_pad], axis=0)\n        self.item_embeddings = item_embeddings * (self.hidden_units ** 0.5)\n\n        self.position_embeddings = tf.get_variable(\'position_embeddings\', dtype=tf.float32,\n                                                   shape=[self.max_len, self.hidden_units],\n                                                   regularizer=l2_regularizer)\n\n    def build_graph(self):\n        self._create_variable()\n        reuse = None\n        with tf.variable_scope(""SASRec"", reuse=reuse):\n            # sequence embedding, item embedding table\n            self.seq = tf.nn.embedding_lookup(self.item_embeddings, self.item_seq_ph)\n            item_emb_table = self.item_embeddings\n\n            # Positional Encoding\n            position = tf.tile(tf.expand_dims(tf.range(tf.shape(self.item_seq_ph)[1]), 0),\n                               [tf.shape(self.item_seq_ph)[0], 1])\n            t = tf.nn.embedding_lookup(self.position_embeddings, position)\n            # pos_emb_table = self.position_embeddings\n\n            self.seq += t\n\n            # Dropout\n            self.seq = tf.layers.dropout(self.seq,\n                                         rate=self.dropout_rate,\n                                         training=tf.convert_to_tensor(self.is_training))\n\n            mask = tf.expand_dims(tf.to_float(tf.not_equal(self.item_seq_ph, self.items_num)), -1)\n            self.seq *= mask\n\n            # Build blocks\n\n            for i in range(self.num_blocks):\n                with tf.variable_scope(""num_blocks_%d"" % i):\n                    # Self-attention\n                    self.seq = multihead_attention(queries=normalize(self.seq),\n                                                   keys=self.seq,\n                                                   num_units=self.hidden_units,\n                                                   num_heads=self.num_heads,\n                                                   dropout_rate=self.dropout_rate,\n                                                   is_training=self.is_training,\n                                                   causality=True,\n                                                   scope=""self_attention"")\n\n                    # Feed forward\n                    self.seq = feedforward(normalize(self.seq),\n                                           num_units=[self.hidden_units, self.hidden_units],\n                                           dropout_rate=self.dropout_rate,\n                                           is_training=self.is_training)\n                    self.seq *= mask\n\n            self.seq = normalize(self.seq)  # (b, l, d)\n            last_emb = self.seq[:, -1, :]  # (b, d), the embedding of last item of each session\n\n        pos = tf.reshape(self.item_pos_ph, [tf.shape(self.item_seq_ph)[0] * self.max_len])  # (b*l,)\n        neg = tf.reshape(self.item_neg_ph, [tf.shape(self.item_seq_ph)[0] * self.max_len])  # (b*l,)\n        pos_emb = tf.nn.embedding_lookup(item_emb_table, pos)  # (b*l, d)\n        neg_emb = tf.nn.embedding_lookup(item_emb_table, neg)  # (b*l, d)\n        seq_emb = tf.reshape(self.seq, [tf.shape(self.item_seq_ph)[0] * self.max_len, self.hidden_units])  # (b*l, d)\n\n        # prediction layer\n        self.pos_logits = inner_product(pos_emb, seq_emb)  # (b*l,)\n        self.neg_logits = inner_product(neg_emb, seq_emb)  # (b*l,)\n\n        # ignore padding items (self.items_num)\n        is_target = tf.reshape(tf.to_float(tf.not_equal(pos, self.items_num)),\n                               [tf.shape(self.item_seq_ph)[0] * self.max_len])\n\n        pos_loss = -tf.log(tf.sigmoid(self.pos_logits) + 1e-24) * is_target\n        neg_loss = -tf.log(1 - tf.sigmoid(self.neg_logits) + 1e-24) * is_target\n        self.loss = tf.reduce_sum(pos_loss + neg_loss) / tf.reduce_sum(is_target)\n\n        try:\n            reg_losses = tf.add_n(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n            self.loss = self.loss + reg_losses\n        except:\n            pass\n\n        self.train_opt = tf.train.AdamOptimizer(learning_rate=self.lr, beta2=0.98).minimize(self.loss)\n\n        # for predication/test\n        items_embeddings = item_emb_table[:-1]  # remove the padding item\n        self.all_logits = tf.matmul(last_emb, items_embeddings, transpose_b=True)\n\n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n\n        for epoch in range(self.epochs):\n            item_seq_list, item_pos_list, item_neg_list = self.get_train_data()\n            data = DataIterator(item_seq_list, item_pos_list, item_neg_list,\n                                batch_size=self.batch_size, shuffle=True)\n            for bat_item_seq, bat_item_pos, bat_item_neg in data:\n                feed = {self.item_seq_ph: bat_item_seq,\n                        self.item_pos_ph: bat_item_pos,\n                        self.item_neg_ph: bat_item_neg,\n                        self.is_training: True}\n\n                self.sess.run(self.train_opt, feed_dict=feed)\n\n            result = self.evaluate_model()\n            self.logger.info(""epoch %d:\\t%s"" % (epoch, result))\n\n    def get_train_data(self):\n        item_seq_list, item_pos_list, item_neg_list = [], [], []\n        all_users = DataIterator(list(self.user_pos_train.keys()), batch_size=1024, shuffle=False)\n        for bat_users in all_users:\n            bat_seq = [self.user_pos_train[u][:-1] for u in bat_users]\n            bat_pos = [self.user_pos_train[u][1:] for u in bat_users]\n            n_neg_items = [len(pos) for pos in bat_pos]\n            exclusion = [self.user_pos_train[u] for u in bat_users]\n            bat_neg = batch_randint_choice(self.items_num, n_neg_items, replace=True, exclusion=exclusion)\n\n            # padding\n            bat_seq = pad_sequences(bat_seq, value=self.items_num, max_len=self.max_len, padding=\'pre\', truncating=\'pre\')\n            bat_pos = pad_sequences(bat_pos, value=self.items_num, max_len=self.max_len, padding=\'pre\', truncating=\'pre\')\n            bat_neg = pad_sequences(bat_neg, value=self.items_num, max_len=self.max_len, padding=\'pre\', truncating=\'pre\')\n\n            item_seq_list.extend(bat_seq)\n            item_pos_list.extend(bat_pos)\n            item_neg_list.extend(bat_neg)\n        return item_seq_list, item_pos_list, item_neg_list\n\n    def evaluate_model(self):\n        return self.evaluator.evaluate(self)\n\n    def predict(self, users, items=None):\n        users = DataIterator(users, batch_size=512, shuffle=False, drop_last=False)\n        all_ratings = []\n        for bat_user in users:\n            bat_seq = [self.user_pos_train[u] for u in bat_user]\n            bat_seq = pad_sequences(bat_seq, value=self.items_num, max_len=self.max_len, padding=\'pre\', truncating=\'pre\')\n            feed = {self.item_seq_ph: bat_seq,\n                    self.is_training: False}\n            bat_ratings = self.sess.run(self.all_logits, feed_dict=feed)\n            all_ratings.extend(bat_ratings)\n        all_ratings = np.array(all_ratings, dtype=np.float32)\n        if items is not None:\n            all_ratings = [all_ratings[idx][item] for idx, item in enumerate(items)]\n        return all_ratings\n'"
model/sequential_recommender/SRGNN.py,61,"b'""""""\nPaper: Session-Based Recommendation with Graph Neural Networks\nAuthor: Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan\nReference: https://github.com/CRIPAC-DIG/SR-GNN\n@author: Zhongchuan Sun\n""""""\n\nimport numpy as np\nfrom model.AbstractRecommender import SeqAbstractRecommender\nfrom util import DataIterator\nimport tensorflow as tf\nfrom util import pad_sequences\nimport math\n\n\nclass SRGNN(SeqAbstractRecommender):\n    def __init__(self, sess, dataset, config):\n        super(SRGNN, self).__init__(dataset, config)\n        self.lr = config[""lr""]\n        self.L2 = config[""L2""]\n        self.hidden_size = config[""hidden_size""]\n        self.step = config[""step""]\n        self.batch_size = config[""batch_size""]\n        self.epochs = config[""epochs""]\n        self.lr_dc = config[""lr_dc""]\n        self.lr_dc_step = config[""lr_dc_step""]\n        self.nonhybrid = config[""nonhybrid""]\n        self.max_seq_len = config[""max_seq_len""]\n\n        self.num_users, self.num_item = dataset.num_users, dataset.num_items\n        self.user_pos_train = dataset.get_user_train_dict(by_time=True)\n\n        self.train_seq = []\n        self.train_tar = []\n        for user, seqs in self.user_pos_train.items():\n            for i in range(1, len(seqs)):\n                self.train_seq.append(seqs[-i - self.max_seq_len:-i])\n                self.train_tar.append(seqs[-i])\n\n        self.sess = sess\n\n    def _create_variable(self):\n        self.mask_ph = tf.placeholder(dtype=tf.float32, shape=[self.batch_size, None])\n        self.alias_ph = tf.placeholder(dtype=tf.int32, shape=[self.batch_size, None])  # \xe7\xbb\x99\xe7\xbb\x99\xe6\xaf\x8f\xe4\xb8\xaa\xe8\xbe\x93\xe5\x85\xa5\xe9\x87\x8d\xe6\x96\xb0\n        self.item_ph = tf.placeholder(dtype=tf.int32, shape=[self.batch_size, None])  # \xe5\x8e\x9f\xe5\xa7\x8bID+padID, pad \xe5\x9c\xa8\xe5\x90\x8e\xe9\x9d\xa2\n        self.target_ph = tf.placeholder(dtype=tf.int32, shape=[self.batch_size])\n\n        self.adj_in_ph = tf.placeholder(dtype=tf.float32, shape=[self.batch_size, None, None])\n        self.adj_out_ph = tf.placeholder(dtype=tf.float32, shape=[self.batch_size, None, None])\n\n        stdv = 1.0 / math.sqrt(self.hidden_size)\n        w_init = tf.random_uniform_initializer(-stdv, stdv)\n        self.nasr_w1 = tf.get_variable(\'nasr_w1\', [self.hidden_size, self.hidden_size], dtype=tf.float32,\n                                       initializer=w_init)\n        self.nasr_w2 = tf.get_variable(\'nasr_w2\', [self.hidden_size, self.hidden_size], dtype=tf.float32,\n                                       initializer=w_init)\n        self.nasr_v = tf.get_variable(\'nasrv\', [1, self.hidden_size], dtype=tf.float32, initializer=w_init)\n        self.nasr_b = tf.get_variable(\'nasr_b\', [self.hidden_size], dtype=tf.float32,\n                                      initializer=tf.zeros_initializer())\n\n        embedding = tf.get_variable(shape=[self.num_item, self.hidden_size], name=\'embedding\',\n                                    dtype=tf.float32, initializer=w_init)\n        zero_pad = tf.zeros([1, self.hidden_size], name=""padding"")\n        self.embedding = tf.concat([embedding, zero_pad], axis=0)\n\n        self.W_in = tf.get_variable(\'W_in\', shape=[self.hidden_size, self.hidden_size], dtype=tf.float32,\n                                    initializer=w_init)\n        self.b_in = tf.get_variable(\'b_in\', [self.hidden_size], dtype=tf.float32, initializer=w_init)\n        self.W_out = tf.get_variable(\'W_out\', [self.hidden_size, self.hidden_size], dtype=tf.float32,\n                                     initializer=w_init)\n        self.b_out = tf.get_variable(\'b_out\', [self.hidden_size], dtype=tf.float32, initializer=w_init)\n\n        self.B = tf.get_variable(\'B\', [2 * self.hidden_size, self.hidden_size], initializer=w_init)\n\n    def ggnn(self):\n        fin_state = tf.nn.embedding_lookup(self.embedding, self.item_ph)  # (b,l,d)\n        cell = tf.nn.rnn_cell.GRUCell(self.hidden_size)\n        with tf.variable_scope(\'gru\'):\n            for i in range(self.step):\n                fin_state = tf.reshape(fin_state, [self.batch_size, -1, self.hidden_size])  # (b,l,d)\n                fin_state_tmp = tf.reshape(fin_state, [-1, self.hidden_size])  # (b*l,d)\n\n                fin_state_in = tf.reshape(tf.matmul(fin_state_tmp, self.W_in) + self.b_in,\n                                          [self.batch_size, -1, self.hidden_size])  # (b,l,d)\n\n                # fin_state_tmp = tf.reshape(fin_state, [-1, self.hidden_size])  # (b*l,d)\n                fin_state_out = tf.reshape(tf.matmul(fin_state_tmp, self.W_out) + self.b_out,\n                                           [self.batch_size, -1, self.hidden_size])  # (b,l,d)\n\n                av_in = tf.matmul(self.adj_in_ph, fin_state_in)  # (b,l,d)\n                av_out = tf.matmul(self.adj_out_ph, fin_state_out)  # (b,l,d)\n                av = tf.concat([av_in, av_out], axis=-1)  # (b,l,2d)\n\n                av = tf.expand_dims(tf.reshape(av, [-1, 2 * self.hidden_size]), axis=1)  # (b*l,1,2d)\n                # fin_state_tmp = tf.reshape(fin_state, [-1, self.hidden_size])  # (b*l,d)\n\n                state_output, fin_state = tf.nn.dynamic_rnn(cell, av, initial_state=fin_state_tmp)\n        return tf.reshape(fin_state, [self.batch_size, -1, self.hidden_size])  # (b,l,d)\n\n    def _session_embedding(self, re_embedding):\n        # re_embedding  (b,l,d)\n        rm = tf.reduce_sum(self.mask_ph, 1)  # (b,), length of each session\n        last_idx = tf.stack([tf.range(self.batch_size), tf.to_int32(rm) - 1], axis=1)  # (b, 2) index of last item\n        last_id = tf.gather_nd(self.alias_ph, last_idx)  # (b,) alias id of last item\n        last_h = tf.gather_nd(re_embedding, tf.stack([tf.range(self.batch_size), last_id], axis=1))  # (b,d) embedding of last item\n\n        seq_h = [tf.nn.embedding_lookup(re_embedding[i], self.alias_ph[i]) for i in range(self.batch_size)]\n        seq_h = tf.stack(seq_h, axis=0)  # batch_size*T*d\n        last = tf.matmul(last_h, self.nasr_w1)\n        seq = tf.matmul(tf.reshape(seq_h, [-1, self.hidden_size]), self.nasr_w2)\n        last = tf.reshape(last, [self.batch_size, 1, -1])\n        m = tf.nn.sigmoid(last + tf.reshape(seq, [self.batch_size, -1, self.hidden_size]) + self.nasr_b)\n        coef = tf.matmul(tf.reshape(m, [-1, self.hidden_size]), self.nasr_v, transpose_b=True) * tf.reshape(self.mask_ph, [-1, 1])\n        if not self.nonhybrid:\n            ma = tf.concat([tf.reduce_sum(tf.reshape(coef, [self.batch_size, -1, 1]) * seq_h, 1),\n                            tf.reshape(last, [-1, self.hidden_size])], -1)\n            sess_embedding = tf.matmul(ma, self.B)\n        else:\n            sess_embedding = tf.reduce_sum(tf.reshape(coef, [self.batch_size, -1, 1]) * seq_h, 1)\n\n        return sess_embedding\n\n    def build_graph(self):\n        self._create_variable()\n        with tf.variable_scope(\'ggnn_model\', reuse=None):\n            node_embedding = self.ggnn()\n            sess_embedding = self._session_embedding(node_embedding)\n\n        item_embedding = self.embedding[:-1]\n        self.all_logits = tf.matmul(sess_embedding, item_embedding, transpose_b=True)\n        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.target_ph, logits=self.all_logits)\n        loss = tf.reduce_mean(loss)\n\n        vars = tf.trainable_variables()\n        lossL2 = [tf.nn.l2_loss(v) for v in vars if v.name not in [\'bias\', \'gamma\', \'b\', \'g\', \'beta\']]\n        loss_train = loss + self.L2 * tf.add_n(lossL2)\n\n        global_step = tf.Variable(0)\n        decay = self.lr_dc_step * len(self.train_seq) / self.batch_size\n        learning_rate = tf.train.exponential_decay(self.lr, global_step=global_step, decay_steps=decay,\n                                                   decay_rate=self.lr_dc, staircase=True)\n        self.train_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss_train, global_step=global_step)\n\n    def train_model(self):\n        train_seq_len = [(idx, len(seq)) for idx, seq in enumerate(self.train_seq)]\n        train_seq_len = sorted(train_seq_len, key=lambda x: x[1], reverse=True)\n        train_seq_index, _ = list(zip(*train_seq_len))\n\n        self.logger.info(self.evaluator.metrics_info())\n        for epoch in range(self.epochs):\n            for bat_index in self._shuffle_index(train_seq_index):\n                item_seqs = [self.train_seq[idx] for idx in bat_index]\n                bat_tars = [self.train_tar[idx] for idx in bat_index]\n                bat_adj_in, bat_adj_out, bat_alias, bat_items, bat_mask = self._build_session_graph(item_seqs)\n                feed = {self.target_ph: bat_tars,\n                        self.item_ph: bat_items,\n                        self.adj_in_ph: bat_adj_in,\n                        self.adj_out_ph: bat_adj_out,\n                        self.alias_ph: bat_alias,\n                        self.mask_ph: bat_mask}\n\n                self.sess.run(self.train_opt, feed_dict=feed)\n\n            self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate_model()))\n\n    def _shuffle_index(self, seq_index):\n        index_chunks = DataIterator(seq_index, batch_size=self.batch_size*32, shuffle=False, drop_last=False)  # chunking\n        index_chunks = list(index_chunks)\n        index_chunks_iter = DataIterator(index_chunks, batch_size=1, shuffle=True, drop_last=False)  # shuffle index chunk\n        for indexes in index_chunks_iter:\n            indexes = indexes[0]\n            indexes_iter = DataIterator(indexes, batch_size=self.batch_size, shuffle=True, drop_last=True)  # shuffle batch index\n            for bat_index in indexes_iter:\n                yield bat_index\n\n    def _build_session_graph(self, bat_items):\n        A_in, A_out, alias_inputs = [], [], []\n        all_mask = [[1] * len(items) for items in bat_items]\n        bat_items = pad_sequences(bat_items, value=self.num_item)\n\n        unique_nodes = [np.unique(items).tolist() for items in bat_items]\n        max_n_node = np.max([len(nodes) for nodes in unique_nodes])\n        for u_seq, u_node, mask in zip(bat_items, unique_nodes, all_mask):\n            adj_mat = np.zeros((max_n_node, max_n_node))\n            id_map = {node: idx for idx, node in enumerate(u_node)}\n            if len(u_seq) > 1:\n                alias_previous = [id_map[i] for i in u_seq[:len(mask) - 1]]\n                alias_next = [id_map[i] for i in u_seq[1:len(mask)]]\n                adj_mat[alias_previous, alias_next] = 1\n\n            u_sum_in = np.sum(adj_mat, axis=0)\n            u_sum_in[np.where(u_sum_in == 0)] = 1\n            u_A_in = np.divide(adj_mat, u_sum_in)\n\n            u_sum_out = np.sum(adj_mat, 1)\n            u_sum_out[np.where(u_sum_out == 0)] = 1\n            u_A_out = np.divide(adj_mat.transpose(), u_sum_out)\n\n            A_in.append(u_A_in)\n            A_out.append(u_A_out)\n            alias_inputs.append([id_map[i] for i in u_seq])\n\n        items = pad_sequences(unique_nodes, value=self.num_item)\n        all_mask = pad_sequences(all_mask, value=0)\n        return A_in, A_out, alias_inputs, items, all_mask\n\n    def evaluate_model(self):\n        return self.evaluator.evaluate(self)\n\n    def predict(self, users, items):\n        users = DataIterator(users, batch_size=self.batch_size, shuffle=False, drop_last=False)\n        all_ratings = []\n        for bat_user in users:\n            cur_batch_size = len(bat_user)\n            bat_items = [self.user_pos_train[user][-self.max_seq_len:] for user in bat_user]\n            bat_adj_in, bat_adj_out, bat_alias, bat_items, bat_mask = self._build_session_graph(bat_items)\n            if cur_batch_size < self.batch_size:  # padding\n                pad_size = self.batch_size - cur_batch_size\n                bat_adj_in = np.concatenate([bat_adj_in, [bat_adj_in[-1]] * pad_size], axis=0)\n                bat_adj_out = np.concatenate([bat_adj_out, [bat_adj_out[-1]] * pad_size], axis=0)\n                bat_alias = np.concatenate([bat_alias, [bat_alias[-1]] * pad_size], axis=0)\n                bat_items = np.concatenate([bat_items, [bat_items[-1]] * pad_size], axis=0)\n                bat_mask = np.concatenate([bat_mask, [bat_mask[-1]] * pad_size], axis=0)\n\n            feed = {self.item_ph: bat_items,\n                    self.adj_in_ph: bat_adj_in,\n                    self.adj_out_ph: bat_adj_out,\n                    self.alias_ph: bat_alias,\n                    self.mask_ph: bat_mask}\n            bat_ratings = self.sess.run(self.all_logits, feed_dict=feed)\n            all_ratings.extend(bat_ratings[:cur_batch_size])\n        all_ratings = np.array(all_ratings)\n        if items is not None:\n            all_ratings = [all_ratings[idx][u_item] for idx, u_item in enumerate(items)]\n\n        return all_ratings\n'"
model/sequential_recommender/TransRec.py,30,"b'""""""\nReference: Ruining He et al., ""Translation-based Recommendation."" in RecSys 2017\n@author: wubin\n""""""\n\nfrom model.AbstractRecommender import SeqAbstractRecommender\nimport tensorflow as tf\nimport numpy as np\nfrom time import time\nfrom util import learner, tool\nfrom util.data_iterator import DataIterator\nfrom util.tool import csr_to_user_dict_bytime\nfrom util import timer\nfrom util import l2_loss\nfrom data import TimeOrderPointwiseSampler, TimeOrderPairwiseSampler\n\n\ndef l2_distance(a, b, name=""euclidean_distance""):\n    return tf.norm(a - b, ord=\'euclidean\', axis=-1, name=name)\n\n\nclass TransRec(SeqAbstractRecommender):\n    def __init__(self, sess, dataset, conf):\n        super(TransRec, self).__init__(dataset, conf)\n        self.learning_rate = conf[""learning_rate""]\n        self.embedding_size = conf[""embedding_size""]\n        self.learner = conf[""learner""]\n        self.loss_function = conf[""loss_function""]\n        self.is_pairwise = conf[""is_pairwise""]\n        self.num_epochs = conf[""epochs""]\n        self.reg_mf = conf[""reg_mf""]\n        self.batch_size = conf[""batch_size""]\n        self.verbose = conf[""verbose""]\n        self.num_negatives = conf[""num_neg""]\n        self.init_method = conf[""init_method""]\n        self.stddev = conf[""stddev""]\n        self.num_users = dataset.num_users\n        self.num_items = dataset.num_items\n        self.dataset = dataset \n        self.train_matrix = dataset.train_matrix\n        self.train_dict = csr_to_user_dict_bytime(dataset.time_matrix, dataset.train_matrix)\n        self.sess = sess\n\n    def _create_placeholders(self):\n        with tf.name_scope(""input_data""):\n            self.user_input = tf.placeholder(tf.int32, shape=[None], name=""user_input"")\n            self.item_input = tf.placeholder(tf.int32, shape=[None], name=""item_input_pos"")\n            self.item_input_recent = tf.placeholder(tf.int32, shape=[None], name=""item_input_recent"")\n            if self.is_pairwise is True:\n                self.item_input_neg = tf.placeholder(tf.int32, shape=[None], name=""item_input_neg"")\n            else:\n                self.labels = tf.placeholder(tf.float32, shape=[None, ], name=""labels"")\n\n    def _create_variables(self):\n        with tf.name_scope(""embedding""):\n            initializer = tool.get_initializer(self.init_method, self.stddev)\n            self.user_embeddings = tf.Variable(initializer([self.num_users, self.embedding_size]),\n                                               name=\'user_embeddings\', dtype=tf.float32)  # (users, embedding_size)\n            self.item_embeddings = tf.Variable(initializer([self.num_items, self.embedding_size]),\n                                               name=\'item_embeddings\', dtype=tf.float32)  # (items, embedding_size)\n            self.item_biases = tf.Variable(initializer([self.num_items]),\n                                           name=\'item_biases\', dtype=tf.float32)  # (items)\n            self.global_embedding = tf.Variable(initializer([1, self.embedding_size]),\n                                                name=\'global_embedding\', dtype=tf.float32)\n    \n    def _create_inference(self, item_input):\n        with tf.name_scope(""inference""):\n            # embedding look up\n            user_embedding = tf.nn.embedding_lookup(self.user_embeddings, self.user_input)\n            batch_size = tf.shape(user_embedding)[0]\n            item_embedding_recent = tf.nn.embedding_lookup(self.item_embeddings, self.item_input_recent)\n            item_embedding = tf.nn.embedding_lookup(self.item_embeddings, item_input)\n            \n            item_bias = tf.nn.embedding_lookup(self.item_biases, item_input)\n            predict_vector = user_embedding + tf.tile(self.global_embedding, tf.stack([batch_size, 1])) + \\\n                             item_embedding_recent - item_embedding\n            predict = item_bias-tf.reduce_sum(tf.square(predict_vector), 1)\n            return user_embedding, item_embedding_recent, item_embedding, item_bias, predict\n\n    def _create_loss(self):\n        with tf.name_scope(""loss""):\n            # loss for L(Theta)\n            p1, r1, q1, b1, self.output = self._create_inference(self.item_input)\n            if self.is_pairwise is True:\n                _, _, q2, b2, output_neg = self._create_inference(self.item_input_neg)\n                self.result = self.output - output_neg\n                self.loss = learner.pairwise_loss(self.loss_function, self.result) + \\\n                            self.reg_mf * l2_loss(p1, r1, q2, q1, b1, b2, self.global_embedding)\n            else:\n                self.loss = learner.pointwise_loss(self.loss_function, self.labels, self.output) + \\\n                            self.reg_mf * l2_loss(p1, r1, q1, b1, self.global_embedding)\n\n    def _create_optimizer(self):\n        with tf.name_scope(""learner""):\n            self.optimizer = learner.optimizer(self.learner, self.loss, self.learning_rate)\n                \n    def build_graph(self):\n        self._create_placeholders()\n        self._create_variables()\n        self._create_loss()\n        self._create_optimizer()\n        u_emb = tf.nn.embedding_lookup(self.user_embeddings, self.user_input)\n        last_i_emb = tf.nn.embedding_lookup(self.item_embeddings, self.item_input_recent)\n        pre_emb = u_emb + self.global_embedding + last_i_emb\n        pre_emb = tf.expand_dims(pre_emb, axis=1)  # b*1*d\n        j_emb = tf.expand_dims(self.item_embeddings, axis=0)  # 1*n*d\n        self.prediction = -l2_distance(pre_emb, j_emb) + self.item_biases  # b*n\n\n    # ---------- training process -------\n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n        if self.is_pairwise is True:\n            data_iter = TimeOrderPairwiseSampler(self.dataset, high_order=1, neg_num=1,\n                                                 batch_size=self.batch_size, shuffle=True)\n        else:\n            data_iter = TimeOrderPointwiseSampler(self.dataset, high_order=1,\n                                                  neg_num=self.num_negatives,\n                                                  batch_size=self.batch_size, shuffle=True)\n        for epoch in range(self.num_epochs):\n            num_training_instances = len(data_iter)\n            total_loss = 0.0\n            training_start_time = time()\n\n            if self.is_pairwise is True:\n                for bat_users, bat_items_recent, bat_items_pos, bat_items_neg in data_iter:\n                    feed_dict = {self.user_input: bat_users,\n                                 self.item_input: bat_items_pos,\n                                 self.item_input_recent: bat_items_recent,\n                                 self.item_input_neg: bat_items_neg}\n\n                    loss, _ = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n                    total_loss += loss\n            else:\n                for bat_users, bat_items_recent, bat_items, bat_labels in data_iter:\n                    feed_dict = {self.user_input: bat_users,\n                                 self.item_input: bat_items,\n                                 self.item_input_recent: bat_items_recent,\n                                 self.labels: bat_labels}\n\n                    loss, _ = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n                    total_loss += loss\n\n            # logger.info(""[iter %d : loss : %f, time: %f]"" % (epoch, total_loss / num_training_instances,\n            #                                                  time() - training_start_time))\n            \n            if epoch % self.verbose == 0:\n                self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate()))\n\n    @timer\n    def evaluate(self):\n        return self.evaluator.evaluate(self)\n\n    def predict(self, user_ids, items=None):\n        users = DataIterator(user_ids, batch_size=64, shuffle=False, drop_last=False)\n        all_ratings = []\n        for bat_user in users:\n            last_items = [self.train_dict[u][-1] for u in bat_user]\n            feed = {self.user_input: bat_user, self.item_input_recent: last_items}\n            bat_ratings = self.sess.run(self.prediction, feed_dict=feed)\n            all_ratings.append(bat_ratings)\n        all_ratings = np.vstack(all_ratings)\n\n        if items is not None:\n            all_ratings = [all_ratings[idx][item] for idx, item in enumerate(items)]\n\n        return all_ratings\n'"
model/social_recommender/DiffNet.py,28,"b'from model.AbstractRecommender import SocialAbstractRecommender\nimport tensorflow as tf\nimport numpy as np\nfrom time import time\nfrom util import learner, tool\nfrom util.tool import csr_to_user_dict\nfrom util import timer\nfrom util import l2_loss\nfrom data import PointwiseSampler\n\n\nclass DiffNet(SocialAbstractRecommender):\n    def __init__(self, sess, dataset, conf):\n        super(DiffNet, self).__init__(dataset, conf)\n        self.learning_rate = conf[""learning_rate""]\n        self.embedding_size = conf[""embedding_size""]\n        self.learner = conf[""learner""]\n        self.loss_function = conf[""loss_function""]\n        self.num_epochs = conf[""epochs""]\n        self.reg_mf = conf[""reg_mf""]\n        self.batch_size = conf[""batch_size""]\n        self.num_negatives = conf[""num_negatives""]\n        self.user_feature_file = conf[""user_feature_file""]\n        self.item_feature_file = conf[""item_feature_file""]\n        self.feature_dimension = conf[""feature_dimension""]\n        self.init_method = conf[""init_method""]\n        self.stddev = conf[""stddev""]\n        self.verbose = conf[""verbose""]\n        self.num_users = dataset.num_users\n        self.num_items = dataset.num_items\n        self.userids = dataset.userids\n        self.itemids = dataset.itemids\n        self.dataset = dataset\n        self.trainMatrix = dataset.trainMatrix\n        self.trainDict = csr_to_user_dict(self.trainMatrix)\n        self.social_matrix = self.social_matrix + self.social_matrix.transpose()\n\n        self.consumed_items_sparse_matrix, self.social_neighbors_sparse_matrix = self.input_supply()\n        self.sess = sess\n        \n    def input_supply(self):\n        consumed_items_indices_list = []\n        consumed_items_values_list = []\n        for (u, i) in self.trainMatrix.keys():\n            consumed_items_indices_list.append([u, i])\n            consumed_items_values_list.append(1.0/len(self.trainDict[u]))\n        self.consumed_items_indices_list = np.array(consumed_items_indices_list).astype(np.int32)\n        self.consumed_items_values_list = np.array(consumed_items_values_list).astype(np.float32)\n\n        social_neighbors_indices_list = []\n        social_neighbors_values_list = []\n        for u in range(self.num_users):\n            friends = self.social_matrix[u].indices\n            for v in friends:\n                social_neighbors_indices_list.append([u,v])\n                social_neighbors_values_list.append(1.0/len(friends))\n                    \n        self.social_neighbors_indices_list = np.array(social_neighbors_indices_list).astype(np.int32)\n        self.social_neighbors_values_list = np.array(social_neighbors_values_list).astype(np.float32)\n\n        # prepare sparse matrix, in order to compute user\'s embedding from social neighbors and consumed items\n        self.social_neighbors_dense_shape = np.array([self.num_users, self.num_users]).astype(np.int32)\n        self.consumed_items_dense_shape = np.array([self.num_users, self.num_items]).astype(np.int32)\n\n        self.social_neighbors_sparse_matrix = tf.SparseTensor(indices=self.social_neighbors_indices_list,\n                                                              values=self.social_neighbors_values_list,\n                                                              dense_shape=self.social_neighbors_dense_shape)\n        self.consumed_items_sparse_matrix = tf.SparseTensor(indices=self.consumed_items_indices_list,\n                                                            values=self.consumed_items_values_list,\n                                                            dense_shape=self.consumed_items_dense_shape)\n        return self.consumed_items_sparse_matrix, self.social_neighbors_sparse_matrix\n    \n    def _create_placeholders(self):\n        with tf.name_scope(""input_data""):\n            self.user_input = tf.placeholder(tf.int32, shape=[None], name=""user_input"")\n            self.item_input = tf.placeholder(tf.int32, shape=[None], name=""item_input_pos"")\n            self.labels = tf.placeholder(tf.float32, shape=[None], name=""labels_input"")\n\n    def _create_variables(self):\n        with tf.name_scope(""embedding""):\n            initializer = tool.get_initializer(self.init_method, self.stddev)\n               \n            self.user_embedding = tf.Variable(initializer([self.num_users, self.embedding_size]),\n                                              name=\'user_embedding\')\n            self.item_embedding = tf.Variable(initializer([self.num_items, self.embedding_size]),\n                                              name=\'item_embedding\')\n            \n        user_review_vectors = np.zeros((self.num_users, self.feature_dimension))\n        with open(self.user_feature_file, \'r\') as f:\n            for line in f.readlines():\n                user_idx, data = line.strip().split(""::::"")\n                if user_idx in self.userids:\n                    inner_user_idx = self.userids[user_idx]\n                    user_review_vectors[inner_user_idx] = eval(data)\n            \n        self.user_review_vector_matrix = tf.constant(user_review_vectors, dtype=tf.float32)\n\n        item_review_vectors = np.zeros((self.num_items, self.feature_dimension))\n        with open(self.item_feature_file, \'r\') as f:\n            for line in f.readlines():\n                item_idx, data = line.strip().split(""::::"")\n                if item_idx in self.itemids:\n                    inner_item_idx = self.itemids[item_idx]\n                    item_review_vectors[inner_item_idx] = eval(data)\n            \n        self.item_review_vector_matrix = tf.constant(item_review_vectors, dtype=tf.float32)\n        self.reduce_dimension_layer = tf.layers.Dense(self.embedding_size, activation=tf.nn.sigmoid,\n                                                      name=\'reduce_dimension_layer\')\n        \n        self.item_fusion_layer = tf.layers.Dense(self.embedding_size, activation=tf.nn.sigmoid,\n                                                 name=\'item_fusion_layer\')\n        self.user_fusion_layer = tf.layers.Dense(self.embedding_size, activation=tf.nn.sigmoid,\n                                                 name=\'user_fusion_layer\')\n\n    def convertDistribution(self, x):\n        mean, var = tf.nn.moments(x, axes=[0, 1])\n        y = (x - mean) * 0.1 / tf.sqrt(var)\n        return y\n      \n    def generateUserEmbeddingFromSocialNeighbors(self, current_user_embedding):\n        user_embedding_from_social_neighbors = tf.sparse_tensor_dense_matmul(self.social_neighbors_sparse_matrix,\n                                                                             current_user_embedding)\n        return user_embedding_from_social_neighbors\n\n    def generateUserEmebddingFromConsumedItems(self, current_item_embedding):\n        user_embedding_from_consumed_items = tf.sparse_tensor_dense_matmul(self.consumed_items_sparse_matrix,\n                                                                           current_item_embedding)\n        return user_embedding_from_consumed_items\n    \n    def _create_inference(self):\n        with tf.name_scope(""inference""):\n            first_user_review_vector_matrix = self.convertDistribution(self.user_review_vector_matrix)\n            first_item_review_vector_matrix = self.convertDistribution(self.item_review_vector_matrix)\n            \n            self.user_reduce_dim_vector_matrix = self.reduce_dimension_layer(first_user_review_vector_matrix)\n            self.item_reduce_dim_vector_matrix = self.reduce_dimension_layer(first_item_review_vector_matrix)\n    \n            second_user_review_vector_matrix = self.convertDistribution(self.user_reduce_dim_vector_matrix)\n            second_item_review_vector_matrix = self.convertDistribution(self.item_reduce_dim_vector_matrix)\n            \n            # compute item embedding\n            #self.fusion_item_embedding = self.item_fusion_layer(\\\n            #   tf.concat([self.item_embedding, second_item_review_vector_matrix], 1))\n            self.final_item_embedding = self.fusion_item_embedding = \\\n                self.item_embedding + second_item_review_vector_matrix  # TODO ?\n            #self.final_item_embedding = self.fusion_item_embedding = second_item_review_vector_matrix\n    \n            # compute user embedding\n            user_embedding_from_consumed_items = self.generateUserEmebddingFromConsumedItems(self.final_item_embedding)\n    \n            #self.fusion_user_embedding = self.user_fusion_layer(\\\n            #    tf.concat([self.user_embedding, second_user_review_vector_matrix], 1))\n            self.fusion_user_embedding = self.user_embedding  # + second_user_review_vector_matrix\n            first_gcn_user_embedding = self.generateUserEmbeddingFromSocialNeighbors(self.fusion_user_embedding)\n            second_gcn_user_embedding = self.generateUserEmbeddingFromSocialNeighbors(first_gcn_user_embedding)\n    \n            self.final_user_embedding = second_gcn_user_embedding + user_embedding_from_consumed_items\n            \n            # embedding look up\n            self.latest_user_latent = tf.nn.embedding_lookup(self.final_user_embedding, self.user_input)\n            self.latest_item_latent = tf.nn.embedding_lookup(self.final_item_embedding, self.item_input)\n            \n#             self.output = tf.sigmoid(tf.reduce_sum(tf.multiply(self.latest_user_latent, self.latest_item_latent),1))\n            self.output = tf.reduce_sum(tf.multiply(self.latest_user_latent, self.latest_item_latent), 1)\n            \n    def _create_loss(self):\n        with tf.name_scope(""loss""):\n            # loss for L(Theta)\n            # reg = l2_regularizer(self.reg_mf)\n            #\n            # reg_var = apply_regularization(reg, self.user_embedding + self.item_embedding)\n            \n            self.loss = tf.losses.sigmoid_cross_entropy(self.labels, self.output) + \\\n            self.reg_mf*l2_loss(self.latest_user_latent, self.latest_item_latent)\n\n    def _create_optimizer(self):\n        with tf.name_scope(""learner""):\n            self.optimizer = learner.optimizer(self.learner, self.loss, self.learning_rate)\n                \n    def build_graph(self):\n        self._create_placeholders()  \n        self._create_variables()\n        self._create_inference()\n        self._create_loss()\n        self._create_optimizer()\n\n    # ---------- training process -------\n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n        data_iter = PointwiseSampler(self.dataset, neg_num=self.num_negatives, batch_size=self.batch_size, shuffle=True)\n        for epoch in range(1, self.num_epochs+1):\n            total_loss = 0.0\n            start_time = time()\n            num_instances = len(data_iter)\n            for bat_users, bat_items, bat_labels in data_iter:\n                feed_dict = {self.user_input: bat_users,\n                             self.item_input: bat_items,\n                             self.labels: bat_labels}\n                loss, _ = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n                total_loss += loss\n            self.logger.info(""[iter %d : loss : %f, time: %f]"" %\n                             (epoch, total_loss / num_instances,time() - start_time))\n            if epoch % self.verbose == 0:\n                self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate()))\n\n    @timer\n    def evaluate(self):\n        self._cur_user_embeddings, self._cur_item_embeddings = \\\n            self.sess.run([self.latest_user_latent, self.latest_item_latent])\n        return self.evaluator.evaluate(self)\n\n    def predict(self, user_ids, candidate_items_userids):\n        if candidate_items_userids is None:\n            user_embed = self._cur_user_embeddings[user_ids]\n            ratings = np.matmul(user_embed, self._cur_item_embeddings.T)\n        else:\n            ratings = []\n            for user_id, items_by_user_id in zip(user_ids, candidate_items_userids):\n                user_embed = self._cur_user_embeddings[user_id]\n                items_embed = self._cur_item_embeddings[items_by_user_id]\n                ratings.append(np.squeeze(np.matmul(user_embed, items_embed.T)))\n            \n        return ratings\n'"
model/social_recommender/SBPR.py,21,"b'""""""\nReference: Tong Zhao et al., ""Leveraging Social Connections to Improve \nPersonalized Ranking for Collaborative Filtering."" in CIKM 2014\n@author: wubin\n""""""\nimport tensorflow as tf\nimport numpy as np\nfrom time import time\nfrom util import learner, randint_choice, tool\nfrom model.AbstractRecommender import SocialAbstractRecommender\nfrom util import timer\nfrom util.data_iterator import DataIterator\nfrom util.tool import csr_to_user_dict\nfrom util import l2_loss\n\n\nclass SBPR(SocialAbstractRecommender):\n    def __init__(self, sess, dataset, conf):\n        super(SBPR, self).__init__(dataset, conf)\n        self.learning_rate = conf[""learning_rate""]\n        self.embedding_size = conf[""embedding_size""]\n        self.learner = conf[""learner""]\n        self.loss_function = conf[""loss_function""]\n        self.num_epochs = conf[""num_epochs""]\n        self.reg_mf = conf[""reg_mf""]\n        self.batch_size = conf[""batch_size""]\n\n        self.init_method = conf[""init_method""]\n        self.stddev = conf[""stddev""]\n        self.verbose = conf[""verbose""]\n        self.dataset = dataset\n        self.num_users = dataset.num_users\n        self.num_items = dataset.num_items\n        self.userids = self.dataset.userids\n        self.train_dict = csr_to_user_dict(self.dataset.train_matrix)\n        # self.social_matrix = self._get_social_matrix()\n        self.userSocialItemsSetList = self._get_SocialItemsSet()\n        self.sess = sess\n\n    def _get_SocialItemsSet(self):\n        # find items rated by trusted neighbors only\n        userSocialItemsSetList = {}\n        for u, _ in self.train_dict.items():\n            trustors = self.social_matrix[u].indices\n            items = [item for f_u in trustors for item in self.train_dict[f_u] if item not in self.train_dict[u]]\n            items = set(items)\n            if len(items) > 0:\n                userSocialItemsSetList[u] = list(items)\n        return userSocialItemsSetList\n\n    def _create_placeholders(self):\n        with tf.name_scope(""input_data""):\n            self.user_input = tf.placeholder(tf.int32, shape=[None], name=""user_input"")\n            self.item_input_pos = tf.placeholder(tf.int32, shape=[None], name=""item_input_pos"")\n            self.item_input_social = tf.placeholder(tf.int32, shape=[None], name=""item_input_social"")\n            self.suk = tf.placeholder(tf.float32, shape=[None], name=""suk"")\n            self.item_input_neg = tf.placeholder(tf.int32, shape=[None], name=""item_input_neg"")\n\n    def _create_variables(self):\n        with tf.name_scope(""embedding""):\n            initializer = tool.get_initializer(self.init_method, self.stddev)\n            self.user_embeddings = tf.Variable(initializer([self.num_users, self.embedding_size]),\n                                               name=\'user_embeddings\', dtype=tf.float32)  # (users, embedding_size)\n            self.item_embeddings = tf.Variable(initializer([self.num_items, self.embedding_size]),\n                                               name=\'item_embeddings\', dtype=tf.float32)  # (items, embedding_size)\n            self.bias = tf.Variable(initializer([self.num_items]), name=\'bias\')\n\n    def _create_inference(self, item_input):\n        with tf.name_scope(""inference""):\n            # embedding look up\n            user_embedding = tf.nn.embedding_lookup(self.user_embeddings, self.user_input)\n            item_embedding = tf.nn.embedding_lookup(self.item_embeddings, item_input)\n            # item_bias = tf.nn.embedding_lookup(self.bias, item_input)\n            item_bias = tf.gather(self.bias, item_input)\n            output = tf.reduce_sum(tf.multiply(user_embedding, item_embedding), 1) + item_bias\n            return user_embedding, item_embedding, item_bias, output\n\n    def _create_loss(self):\n        with tf.name_scope(""loss""):\n            # loss for L(Theta)\n            p1, q1, b1, self.output = self._create_inference(self.item_input_pos)\n            _, q2, b2, output_social = self._create_inference(self.item_input_social)\n            _, q3, b3, output_neg = self._create_inference(self.item_input_neg)\n            result1 = tf.divide(self.output - output_social, self.suk)\n            result2 = output_social - output_neg\n            self.loss = learner.pairwise_loss(self.loss_function, result1) + \\\n                        learner.pairwise_loss(self.loss_function, result2) + \\\n                        self.reg_mf * l2_loss(p1, q2, q1, q3, b1, b2, b3)\n\n    def _create_optimizer(self):\n        with tf.name_scope(""learner""):\n            self.optimizer = learner.optimizer(self.learner, self.loss, self.learning_rate)\n    \n    def build_graph(self):\n        self._create_placeholders()\n        self._create_variables()\n        self._create_loss()\n        self._create_optimizer()\n\n    # ---------- training process -------\n    def train_model(self):\n        self.logger.info(self.evaluator.metrics_info())\n        for epoch in range(self.num_epochs):\n            user_input, item_input_pos, item_input_social, item_input_neg, suk_input = self._get_pairwise_all_data()\n            data_iter = DataIterator(user_input, item_input_pos, item_input_social, item_input_neg, suk_input,\n                                     batch_size=self.batch_size, shuffle=True)\n            total_loss = 0.0\n            training_start_time = time()\n            num_training_instances = len(user_input)\n            for bat_users, bat_items_pos, bat_items_social, bat_items_neg, bat_suk_input in data_iter:\n                feed_dict = {self.user_input: bat_users, self.item_input_pos: bat_items_pos,\n                             self.item_input_social: bat_items_social,\n                             self.item_input_neg: bat_items_neg, self.suk: bat_suk_input}\n                      \n                loss, _ = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n                total_loss += loss\n            self.logger.info(""[iter %d : loss : %f, time: %f]"" % (epoch, total_loss/num_training_instances,\n                                                             time()-training_start_time))\n            if epoch % self.verbose == 0:\n                self.logger.info(""epoch %d:\\t%s"" % (epoch, self.evaluate()))\n\n    def _get_pairwise_all_data(self):\n        user_input, item_input_pos, item_input_social, item_input_neg, suk_input = [], [], [], [], []\n        num_items = self.dataset.num_items\n\n        for u, pos_item in self.train_dict.items():\n            if u not in self.userSocialItemsSetList:\n                continue\n            # pos_item = pos_item.indices.tolist()\n            pos_len = len(pos_item)\n            user_input.extend([u]*pos_len)\n            item_input_pos.extend(pos_item)\n\n            socialItemsList = self.userSocialItemsSetList[u]\n            # a, size = None, replace = True, p = None, exclusion = None\n            neg_excl = np.concatenate([socialItemsList, list(pos_item)], axis=0)\n\n            neg_item = randint_choice(num_items, pos_len, replace=True, exclusion=neg_excl)\n            item_input_neg.extend(neg_item)\n            social_item = np.random.choice(socialItemsList, size=pos_len)\n            item_input_social.extend(social_item)\n\n            trustedUserIdices = self.social_matrix[u].indices\n            socialWeight_bool = [[1 if k in self.train_dict[f_u] else 0 for f_u in trustedUserIdices] for k in social_item]\n            socialWeight = np.sum(socialWeight_bool, axis=-1) + 1\n            suk_input.extend(socialWeight)\n            \n        return user_input, item_input_pos, item_input_social, item_input_neg, suk_input\n            \n    @timer\n    def evaluate(self):\n        self._cur_user_embeddings, self._cur_item_embeddings = self.sess.run([self.user_embeddings, self.item_embeddings])\n        return self.evaluator.evaluate(self)\n\n    def predict(self, user_ids, candidate_items_userids):\n        if candidate_items_userids is None:\n            user_embed = self._cur_user_embeddings[user_ids]\n            ratings = np.matmul(user_embed, self._cur_item_embeddings.T)\n        else:\n            ratings = []\n            for user_id, items_by_user_id in zip(user_ids, candidate_items_userids):\n                user_embed = self._cur_user_embeddings[user_id]\n                items_embed = self._cur_item_embeddings[items_by_user_id]\n                ratings.append(np.squeeze(np.matmul(user_embed, items_embed.T)))\n            \n        return ratings\n'"
evaluator/backend/cpp/uni_evaluator.py,0,"b'""""""\r\n@author: Zhongchuan Sun\r\n""""""\r\nimport numpy as np\r\nfrom util import DataIterator\r\nfrom util import typeassert\r\nfrom .cpp_evaluator import CPPEvaluator\r\nfrom util.cython.tools import float_type, is_ndarray\r\nfrom util import pad_sequences\r\n\r\n\r\nmetric_dict = {""Precision"": 1, ""Recall"": 2, ""MAP"": 3, ""NDCG"": 4, ""MRR"": 5}\r\nre_metric_dict = {value: key for key, value in metric_dict.items()}\r\n\r\n\r\nclass UniEvaluator(CPPEvaluator):\r\n    """"""Cpp implementation `UniEvaluator` for item ranking task.\r\n\r\n    Evaluation metrics of `UniEvaluator` are configurable and can\r\n    automatically fit both leave-one-out and fold-out data splitting\r\n    without specific indication:\r\n\r\n    * **First**, evaluation metrics of this class are configurable via the\r\n      argument `metric`. Now there are five configurable metrics: `Precision`,\r\n      `Recall`, `MAP`, `NDCG` and `MRR`.\r\n\r\n    * **Second**, this class and its evaluation metrics can automatically fit\r\n      both leave-one-out and fold-out data splitting without specific indication.\r\n      In **leave-one-out** evaluation, 1) `Recall` is equal to `HitRatio`;\r\n      2) The implementation of `NDCG` is compatible with fold-out; 3) `MAP` and\r\n      `MRR` have same numeric values; 4) `Precision` is meaningless.\r\n    """"""\r\n\r\n    @typeassert(user_train_dict=dict, user_test_dict=(dict, None.__class__))\r\n    def __init__(self, user_train_dict, user_test_dict, user_neg_test=None,\r\n                 metric=None, top_k=50, batch_size=1024, num_thread=8):\r\n        """"""Initializes a new `UniEvaluator` instance.\r\n\r\n        Args:\r\n            user_train_dict (dict): Each key is user ID and the corresponding\r\n                value is the list of **training items**.\r\n            user_test_dict (dict): Each key is user ID and the corresponding\r\n                value is the list of **test items**.\r\n            metric (None or list of str): If `metric == None`, metric will\r\n                be set to `[""Precision"", ""Recall"", ""MAP"", ""NDCG"", ""MRR""]`.\r\n                Otherwise, `metric` must be one or a sublist of metrics\r\n                mentioned above. Defaults to `None`.\r\n            top_k (int or list of int): `top_k` controls the Top-K item ranking\r\n                performance. If `top_k` is an integer, K ranges from `1` to\r\n                `top_k`; If `top_k` is a list of integers, K are only assigned\r\n                these values. Defaults to `50`.\r\n            batch_size (int): An integer to control the test batch size.\r\n                Defaults to `1024`.\r\n            num_thread (int): An integer to control the test thread number.\r\n                Defaults to `8`.\r\n\r\n        Raises:\r\n             ValueError: If `metric` or one of its element is invalid.\r\n        """"""\r\n        # super(UniEvaluator, self).__init__(user_test_dict)\r\n        super(UniEvaluator, self).__init__()\r\n        if metric is None:\r\n            metric = [""Precision"", ""Recall"", ""MAP"", ""NDCG"", ""MRR""]\r\n        elif isinstance(metric, str):\r\n            metric = [metric]\r\n        elif isinstance(metric, (set, tuple, list)):\r\n            pass\r\n        else:\r\n            raise TypeError(""The type of \'metric\' (%s) is invalid!"" % metric.__class__.__name__)\r\n\r\n        for m in metric:\r\n            if m not in metric_dict:\r\n                raise ValueError(""There is not the metric named \'%s\'!"" % metric)\r\n\r\n        self.user_pos_train = user_train_dict\r\n        self.user_pos_test = user_test_dict\r\n        self.user_neg_test = user_neg_test\r\n        self.metrics_num = len(metric)\r\n        self.metrics = [metric_dict[m] for m in metric]\r\n        self.num_thread = num_thread\r\n        self.batch_size = batch_size\r\n\r\n        self.max_top = top_k if isinstance(top_k, int) else max(top_k)\r\n        if isinstance(top_k, int):\r\n            self.top_show = np.arange(top_k) + 1\r\n        else:\r\n            self.top_show = np.sort(top_k)\r\n\r\n    def metrics_info(self):\r\n        """"""Get all metrics information.\r\n\r\n        Returns:\r\n            str: A string consist of all metrics information\xef\xbc\x8c such as\r\n                `""Precision@10    Precision@20    NDCG@10    NDCG@20""`.\r\n        """"""\r\n        metrics_show = [\'\\t\'.join([(""%s@""%re_metric_dict[metric] + str(k)).ljust(12) for k in self.top_show])\r\n                        for metric in self.metrics]\r\n        metric = \'\\t\'.join(metrics_show)\r\n        return ""metrics:\\t%s"" % metric\r\n\r\n    def evaluate(self, model, test_users=None):\r\n        """"""Evaluate `model`.\r\n\r\n        Args:\r\n            model: The model need to be evaluated. This model must have\r\n                a method `predict_for_eval(self, users)`, where the argument\r\n                `users` is a list of users and the return is a 2-D array that\r\n                contains `users` rating/ranking scores on all items.\r\n\r\n        Returns:\r\n            str: A single-line string consist of all results, such as\r\n                `""0.18663847    0.11239596    0.35824192    0.21479650""`.\r\n        """"""\r\n        # B: batch size\r\n        # N: the number of items\r\n        test_users = test_users if test_users is not None else list(self.user_pos_test.keys())\r\n        if not isinstance(test_users, (list, tuple, set, np.ndarray)):\r\n            raise TypeError(""\'test_user\' must be a list, tuple, set or numpy array!"")\r\n\r\n        test_users = DataIterator(test_users, batch_size=self.batch_size, shuffle=False, drop_last=False)\r\n        batch_result = []\r\n        for batch_users in test_users:\r\n            if self.user_neg_test is not None:\r\n                candidate_items = [list(self.user_pos_test[u]) + self.user_neg_test[u] for u in batch_users]\r\n                test_items = [set(range(len(self.user_pos_test[u]))) for u in batch_users]\r\n\r\n                ranking_score = model.predict(batch_users, candidate_items)  # (B,N)\r\n                ranking_score = pad_sequences(ranking_score, value=-np.inf, dtype=float_type)\r\n\r\n                if not is_ndarray(ranking_score, float_type):\r\n                    ranking_score = np.array(ranking_score, dtype=float_type)\r\n            else:\r\n                test_items = [self.user_pos_test[u] for u in batch_users]\r\n                ranking_score = model.predict(batch_users, None)  # (B,N)\r\n                if not is_ndarray(ranking_score, float_type):\r\n                    ranking_score = np.array(ranking_score, dtype=float_type)\r\n\r\n                # set the ranking scores of training items to -inf,\r\n                # then the training items will be sorted at the end of the ranking list.\r\n                for idx, user in enumerate(batch_users):\r\n                    train_items = self.user_pos_train[user]\r\n                    ranking_score[idx][train_items] = -np.inf\r\n\r\n            result = self.eval_score_matrix(ranking_score, test_items, self.metrics,\r\n                                            top_k=self.max_top, thread_num=self.num_thread)  # (B,k*metric_num)\r\n            batch_result.append(result)\r\n\r\n        # concatenate the batch results to a matrix\r\n        all_user_result = np.concatenate(batch_result, axis=0)  # (num_users, metrics_num*max_top)\r\n        final_result = np.mean(all_user_result, axis=0)  # (1, metrics_num*max_top)\r\n\r\n        final_result = np.reshape(final_result, newshape=[self.metrics_num, self.max_top])  # (metrics_num, max_top)\r\n        final_result = final_result[:, self.top_show - 1]\r\n        final_result = np.reshape(final_result, newshape=[-1])\r\n        buf = \'\\t\'.join([(""%.8f"" % x).ljust(12) for x in final_result])\r\n        return buf\r\n'"
evaluator/backend/python/metric.py,0,"b'""""""\r\n@author: Zhongchuan Sun\r\n""""""\r\nimport numpy as np\r\nimport sys\r\n\r\n\r\ndef hit(rank, ground_truth):\r\n    # HR is equal to Recall when dataset is loo split.\r\n    last_idx = sys.maxsize\r\n    for idx, item in enumerate(rank):\r\n        if item == ground_truth:\r\n            last_idx = idx\r\n            break\r\n    result = np.zeros(len(rank), dtype=np.float32)\r\n    result[last_idx:] = 1.0\r\n    return result\r\n\r\n\r\ndef precision(rank, ground_truth):\r\n    # Precision is meaningless when dataset is loo split.\r\n    hits = [1 if item in ground_truth else 0 for item in rank]\r\n    result = np.cumsum(hits, dtype=np.float32)/np.arange(1, len(rank)+1)\r\n    return result\r\n\r\n\r\ndef recall(rank, ground_truth):\r\n    # Recall is equal to HR when dataset is loo split.\r\n    hits = [1 if item in ground_truth else 0 for item in rank]\r\n    result = np.cumsum(hits, dtype=np.float32) / len(ground_truth)\r\n    return result\r\n\r\n\r\ndef map(rank, ground_truth):\r\n    # Reference: https://blog.csdn.net/u010138758/article/details/69936041\r\n    # MAP is equal to MRR when dataset is loo split.\r\n    # \xe6\x8c\x89\xe7\x85\xa7\xe5\xae\x9a\xe4\xb9\x89, MAP\xe5\xa5\xbd\xe5\x83\x8f\xe6\xb2\xa1\xe6\x9c\x89MAP@N\xe8\xbf\x99\xe4\xb8\x80\xe8\xaf\xb4\r\n    pre = precision(rank, ground_truth)\r\n    pre = [pre[idx] if item in ground_truth else 0 for idx, item in enumerate(rank)]\r\n    sum_pre = np.cumsum(pre, dtype=np.float32)\r\n    relevant_num = np.cumsum([1 if item in ground_truth else 0 for item in rank])\r\n    result = [p/r_num if r_num!=0 else 0 for p, r_num in zip(sum_pre, relevant_num)]\r\n    return result\r\n\r\n\r\ndef ndcg(rank, ground_truth):\r\n    len_rank = len(rank)\r\n    idcg_len = min(len(ground_truth), len_rank)\r\n    idcg = np.cumsum(1.0 / np.log2(np.arange(2, len_rank + 2)))\r\n    idcg[idcg_len:] = idcg[idcg_len - 1]\r\n\r\n    dcg = np.cumsum([1.0/np.log2(idx+2) if item in ground_truth else 0.0 for idx, item in enumerate(rank)])\r\n    result = dcg/idcg\r\n    return result\r\n\r\n\r\ndef mrr(rank, ground_truth):\r\n    # MRR is equal to MAP when dataset is loo split.\r\n    last_idx = sys.maxsize\r\n    for idx, item in enumerate(rank):\r\n        if item in ground_truth:\r\n            last_idx = idx\r\n            break\r\n    result = np.zeros(len(rank), dtype=np.float32)\r\n    result[last_idx:] = 1.0/(last_idx+1)\r\n    return result\r\n\r\n\r\nmetric_dict = {""Precision"": precision,\r\n               ""Recall"": recall,\r\n               ""MAP"": map,\r\n               ""NDCG"": ndcg,\r\n               ""MRR"": mrr}\r\n'"
evaluator/backend/python/uni_evaluator.py,0,"b'""""""\r\n@author: Zhongchuan Sun\r\n""""""\r\nimport numpy as np\r\nfrom concurrent.futures import ThreadPoolExecutor\r\nfrom util import DataIterator\r\nfrom util import typeassert, argmax_top_k\r\nfrom evaluator.abstract_evaluator import AbstractEvaluator\r\nfrom .metric import metric_dict\r\nfrom util import pad_sequences\r\n\r\n\r\nclass UniEvaluator(AbstractEvaluator):\r\n    """"""Python implementation `UniEvaluator` for item ranking task.\r\n\r\n    Evaluation metrics of `UniEvaluator` are configurable and can\r\n    automatically fit both leave-one-out and fold-out data splitting\r\n    without specific indication:\r\n\r\n    * **First**, evaluation metrics of this class are configurable via the\r\n      argument `metric`. Now there are five configurable metrics: `Precision`,\r\n      `Recall`, `MAP`, `NDCG` and `MRR`.\r\n\r\n    * **Second**, this class and its evaluation metrics can automatically fit\r\n      both leave-one-out and fold-out data splitting without specific indication.\r\n      In **leave-one-out** evaluation, 1) `Recall` is equal to `HitRatio`;\r\n      2) The implementation of `NDCG` is compatible with fold-out; 3) `MAP` and\r\n      `MRR` have same numeric values; 4) `Precision` is meaningless.\r\n    """"""\r\n\r\n    @typeassert(user_train_dict=dict, user_test_dict=(dict, None.__class__))\r\n    def __init__(self, user_train_dict, user_test_dict, user_neg_test=None,\r\n                 metric=None, top_k=50, batch_size=1024, num_thread=8):\r\n        """"""Initializes a new `UniEvaluator` instance.\r\n\r\n        Args:\r\n            user_train_dict (dict): Each key is user ID and the corresponding\r\n                value is the list of **training items**.\r\n            user_test_dict (dict): Each key is user ID and the corresponding\r\n                value is the list of **test items**.\r\n            metric (None or list of str): If `metric == None`, metric will\r\n                be set to `[""Precision"", ""Recall"", ""MAP"", ""NDCG"", ""MRR""]`.\r\n                Otherwise, `metric` must be one or a sublist of metrics\r\n                mentioned above. Defaults to `None`.\r\n            top_k (int or list of int): `top_k` controls the Top-K item ranking\r\n                performance. If `top_k` is an integer, K ranges from `1` to\r\n                `top_k`; If `top_k` is a list of integers, K are only assigned\r\n                these values. Defaults to `50`.\r\n            batch_size (int): An integer to control the test batch size.\r\n                Defaults to `1024`.\r\n            num_thread (int): An integer to control the test thread number.\r\n                Defaults to `8`.\r\n\r\n        Raises:\r\n             ValueError: If `metric` or one of its element is invalid.\r\n        """"""\r\n        super(UniEvaluator, self).__init__()\r\n        if metric is None:\r\n            metric = [""Precision"", ""Recall"", ""MAP"", ""NDCG"", ""MRR""]\r\n        elif isinstance(metric, str):\r\n            metric = [metric]\r\n        elif isinstance(metric, (set, tuple, list)):\r\n            pass\r\n        else:\r\n            raise TypeError(""The type of \'metric\' (%s) is invalid!"" % (metric.__class__.__name__))\r\n\r\n        for m in metric:\r\n            if m not in metric_dict:\r\n                raise ValueError(""There is not the metric named \'%s\'!"" % (metric))\r\n\r\n        self.user_pos_train = user_train_dict\r\n        self.user_pos_test = {user: set(items) for user, items in user_test_dict.items()}\r\n        self.user_neg_test = user_neg_test\r\n        self.metrics_num = len(metric)\r\n        self.metrics = metric\r\n        self.num_thread = num_thread\r\n        self.batch_size = batch_size\r\n\r\n        self.max_top = top_k if isinstance(top_k, int) else max(top_k)\r\n        if isinstance(top_k, int):\r\n            self.top_show = np.arange(top_k) + 1\r\n        else:\r\n            self.top_show = np.sort(top_k)\r\n\r\n    def metrics_info(self):\r\n        """"""Get all metrics information.\r\n\r\n        Returns:\r\n            str: A string consist of all metrics information\xef\xbc\x8c such as\r\n                `""Precision@10    Precision@20    NDCG@10    NDCG@20""`.\r\n        """"""\r\n        metrics_show = [\'\\t\'.join([(""%s@""%metric + str(k)).ljust(12) for k in self.top_show])\r\n                        for metric in self.metrics]\r\n        metric = \'\\t\'.join(metrics_show)\r\n        return ""metrics:\\t%s"" % metric\r\n\r\n    def evaluate(self, model, test_users=None):\r\n        """"""Evaluate `model`.\r\n\r\n        Args:\r\n            model: The model need to be evaluated. This model must have\r\n                a method `predict_for_eval(self, users)`, where the argument\r\n                `users` is a list of users and the return is a 2-D array that\r\n                contains `users` rating/ranking scores on all items.\r\n\r\n        Returns:\r\n            str: A single-line string consist of all results, such as\r\n                `""0.18663847    0.11239596    0.35824192    0.21479650""`.\r\n        """"""\r\n        # B: batch size\r\n        # N: the number of items\r\n        test_users = test_users if test_users is not None else list(self.user_pos_test.keys())\r\n        if not isinstance(test_users, (list, tuple, set, np.ndarray)):\r\n            raise TypeError(""\'test_user\' must be a list, tuple, set or numpy array!"")\r\n\r\n        test_users = DataIterator(test_users, batch_size=self.batch_size,\r\n                                  shuffle=False, drop_last=False)\r\n        batch_result = []\r\n        for batch_users in test_users:\r\n            if self.user_neg_test is not None:\r\n                candidate_items = [list(self.user_pos_test[u]) + self.user_neg_test[u] for u in batch_users]\r\n                test_items = [set(range(len(self.user_pos_test[u]))) for u in batch_users]\r\n\r\n                ranking_score = model.predict(batch_users, candidate_items)  # (B,N)\r\n                ranking_score = pad_sequences(ranking_score, value=-np.inf, dtype=np.float32)\r\n\r\n                ranking_score = np.array(ranking_score)\r\n            else:\r\n                test_items = [self.user_pos_test[u] for u in batch_users]\r\n                ranking_score = model.predict(batch_users, None)  # (B,N)\r\n                ranking_score = np.array(ranking_score)\r\n\r\n                # set the ranking scores of training items to -inf,\r\n                # then the training items will be sorted at the end of the ranking list.\r\n                for idx, user in enumerate(batch_users):\r\n                    train_items = self.user_pos_train[user]\r\n                    ranking_score[idx][train_items] = -np.inf\r\n\r\n            result = self.eval_score_matrix(ranking_score, test_items, self.metrics,\r\n                                            top_k=self.max_top, thread_num=self.num_thread)  # (B,k*metric_num)\r\n            batch_result.append(result)\r\n\r\n        # concatenate the batch results to a matrix\r\n        all_user_result = np.concatenate(batch_result, axis=0)  # (num_users, metrics_num*max_top)\r\n        final_result = np.mean(all_user_result, axis=0)  # (1, metrics_num*max_top)\r\n\r\n        final_result = np.reshape(final_result, newshape=[self.metrics_num, self.max_top])  # (metrics_num, max_top)\r\n        final_result = final_result[:, self.top_show - 1]\r\n        final_result = np.reshape(final_result, newshape=[-1])\r\n        buf = \'\\t\'.join([(""%.8f"" % x).ljust(12) for x in final_result])\r\n        return buf\r\n\r\n    @typeassert(score_matrix=np.ndarray, test_items=list)\r\n    def eval_score_matrix(self, score_matrix, test_items, metric, top_k, thread_num):\r\n        def _eval_one_user(idx):\r\n            scores = score_matrix[idx]  # all scores of the test user\r\n            test_item = test_items[idx]\r\n\r\n            ranking = argmax_top_k(scores, top_k)  # Top-K items\r\n            result = [metric_dict[m](ranking, test_item) for m in metric]\r\n\r\n            result = np.array(result, dtype=np.float32).flatten()\r\n            return result\r\n\r\n        with ThreadPoolExecutor(max_workers=thread_num) as executor:\r\n            batch_result = executor.map(_eval_one_user, range(len(test_items)))\r\n\r\n        result = list(batch_result)  # generator to list\r\n        return np.array(result)  # list to ndarray\r\n'"
