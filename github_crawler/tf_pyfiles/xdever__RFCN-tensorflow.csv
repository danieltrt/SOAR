file_path,api_count,code
BoxInceptionResnet.py,6,"b'# Copyright 2017 Robert Csordas. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# ==============================================================================\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nimport Utils.RandomSelect\n\nfrom InceptionResnetV2 import *\nfrom BoxEngine.BoxNetwork import BoxNetwork\n\n\nclass BoxInceptionResnet(BoxNetwork):\n\tLAYER_NAMES = [\'Conv2d_1a_3x3\',\'Conv2d_2a_3x3\',\'Conv2d_2b_3x3\',\'MaxPool_3a_3x3\',\'Conv2d_3b_1x1\',\'Conv2d_4a_3x3\',\n\t\t\t  \'MaxPool_5a_3x3\',\'Mixed_5b\',\'Repeat\',\'Mixed_6a\',\'Repeat_1\',\'Mixed_7a\',\'Repeat_2\',\'Block8\',\'Conv2d_7b_1x1\']\n\n\tdef __init__(self, inputs, nCategories, name=""BoxNetwork"", weightDecay=0.00004, freezeBatchNorm=False, reuse=False, isTraining=True, trainFrom=None, hardMining=True):\n\t\tself.boxThreshold = 0.5\n\n\t\ttry:\n\t\t\ttrainFrom = int(trainFrom)\n\t\texcept:\n\t\t\tpass\n\n\t\tif isinstance(trainFrom, int):\n\t\t\ttrainFrom = self.LAYER_NAMES[trainFrom]\n\n\n\t\tprint(""Training network from ""+(trainFrom if trainFrom is not None else ""end""))\n\n\t\twith tf.variable_scope(name, reuse=reuse) as scope:\n\t\t\tself.googleNet = InceptionResnetV2(""features"", inputs, trainFrom=trainFrom, freezeBatchNorm=freezeBatchNorm)\n\t\t\tself.scope=scope\n\t\t\n\t\t\twith tf.variable_scope(""Box""):\n\t\t\t\t#Pepeat_1 - last 1/16 layer, Mixed_6a - first 1/16 layer\n\t\t\t\tscale_16 = self.googleNet.getOutput(""Repeat_1"")[:,1:-1,1:-1,:]\n\t\t\t\t#scale_16 = self.googleNet.getOutput(""Mixed_6a"")[:,1:-1,1:-1,:]\n\t\t\t\tscale_32 = self.googleNet.getOutput(""PrePool"")\n\n\t\t\t\twith slim.arg_scope([slim.conv2d],\n\t\t\t\t\t\tweights_regularizer=slim.l2_regularizer(weightDecay),\n\t\t\t\t\t\tbiases_regularizer=slim.l2_regularizer(weightDecay),\n\t\t\t\t\t\tpadding=\'SAME\',\n\t\t\t\t\t\tactivation_fn = tf.nn.relu):\n\n\t\t\t\t\tnet = tf.concat([ tf.image.resize_bilinear(scale_32, tf.shape(scale_16)[1:3]), scale_16], 3)\n\t\t\t\t\trpnInput = slim.conv2d(net, 1024, 1)\n\t\t\t\t\t\n\t\t\t\t\t#BoxNetwork.__init__(self, nCategories, rpnInput, 16, [32,32], scale_32, 32, [32,32], weightDecay=weightDecay, hardMining=hardMining)\n\t\t\t\t\tfeatureInput = slim.conv2d(net, 1536, 1)\n\t\t\t\t\tBoxNetwork.__init__(self, nCategories, rpnInput, 16, [32,32], featureInput, 16, [32,32], weightDecay=weightDecay, hardMining=hardMining)\n\t\n\tdef getVariables(self, includeFeatures=False):\n\t\tif includeFeatures:\n\t\t\treturn tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.scope.name)\n\t\telse:\n\t\t\tvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.scope.name+""/Box/"")\n\t\t\tvars += self.googleNet.getTrainableVars()\n\n\t\t\tprint(""Training variables: "", [v.op.name for v in vars])\n\t\t\treturn vars\n\n\tdef importWeights(self, sess, filename):\n\t\tself.googleNet.importWeights(sess, filename, includeTraining=True)\n'"
InceptionResnetV2.py,38,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Modified by Robert Csordas, 2017.\n# ==============================================================================\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom Utils import CheckpointLoader\n\nclass InceptionResnetV2:\n\t@staticmethod\n\tdef block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n\t\t""""""Builds the 35x35 resnet block.""""""\n\t\twith tf.variable_scope(scope, \'Block35\', [net], reuse=reuse):\n\t\t\twith tf.variable_scope(\'Branch_0\'):\n\t\t\t\ttower_conv = slim.conv2d(net, 32, 1, scope=\'Conv2d_1x1\')\n\t\t\twith tf.variable_scope(\'Branch_1\'):\n\t\t\t\ttower_conv1_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n\t\t\t\ttower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n\t\t\twith tf.variable_scope(\'Branch_2\'):\n\t\t\t\ttower_conv2_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n\t\t\t\ttower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope=\'Conv2d_0b_3x3\')\n\t\t\t\ttower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope=\'Conv2d_0c_3x3\')\n\t\t\tmixed = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2], 3)\n\t\t\tup = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None, activation_fn=None, scope=\'Conv2d_1x1\')\n\t\t\tnet += scale * up\n\t\t\tif activation_fn:\n\t\t\t\tnet = activation_fn(net)\n\t\treturn net\n\n\t@staticmethod\n\tdef block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n\t\t""""""Builds the 17x17 resnet block.""""""\n\t\twith tf.variable_scope(scope, \'Block17\', [net], reuse=reuse):\n\t\t\twith tf.variable_scope(\'Branch_0\'):\n\t\t\t\ttower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n\t\t\twith tf.variable_scope(\'Branch_1\'):\n\t\t\t\ttower_conv1_0 = slim.conv2d(net, 128, 1, scope=\'Conv2d_0a_1x1\')\n\t\t\t\ttower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7], scope=\'Conv2d_0b_1x7\')\n\t\t\t\ttower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1], scope=\'Conv2d_0c_7x1\')\n\t\t\tmixed = tf.concat([tower_conv, tower_conv1_2], 3)\n\t\t\tup = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None, activation_fn=None, scope=\'Conv2d_1x1\')\n\t\t\tnet += scale * up\n\t\t\tif activation_fn:\n\t\t\t\tnet = activation_fn(net)\n\t\treturn net\n\n\t@staticmethod\n\tdef block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n\t\t""""""Builds the 8x8 resnet block.""""""\n\t\twith tf.variable_scope(scope, \'Block8\', [net], reuse=reuse):\n\t\t\twith tf.variable_scope(\'Branch_0\'):\n\t\t\t\ttower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n\t\t\twith tf.variable_scope(\'Branch_1\'):\n\t\t\t\ttower_conv1_0 = slim.conv2d(net, 192, 1, scope=\'Conv2d_0a_1x1\')\n\t\t\t\ttower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3], scope=\'Conv2d_0b_1x3\')\n\t\t\t\ttower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1], scope=\'Conv2d_0c_3x1\')\n\t\t\tmixed = tf.concat([tower_conv, tower_conv1_2], 3)\n\t\t\tup = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None, activation_fn=None, scope=\'Conv2d_1x1\')\n\t\t\tnet += scale * up\n\t\t\tif activation_fn:\n\t\t\t\tnet = activation_fn(net)\n\t\treturn net\n\n\t@staticmethod\n\tdef define(inputs, reuse, weightDecay, scope=\'InceptionResnetV2\', trainFrom=None, freezeBatchNorm=False):\n\t\t""""""Creates the Inception Resnet V2 model.\n\t\tArgs:\n\t\t\tinputs: a 4-D tensor of size [batch_size, height, width, 3].\n\t\t\tnum_classes: number of predicted classes.\n\t\t\tis_training: whether is training or not.\n\t\t\treuse: whether or not the network and its variables should be reused. To be\n\t\t\t  able to reuse \'scope\' must be given.\n\t\t\tscope: Optional variable_scope.\n\t\tReturns:\n\t\t\tlogits: the logits outputs of the model.\n\t\t\tend_points: the set of end_points from the inception model.\n\t\t""""""\n\n\t\twith tf.name_scope(\'preprocess\'):\n\t\t\t#BGR -> RGB\n\t\t\tinputs = tf.reverse(inputs, axis=[3])\n\t\t\t#Normalize\n\t\t\tinputs = 2.0*(inputs/255.0 - 0.5)\n\n\t\tend_points = {}\n\t\tscopes = []\n\n\n\t\ttrainBatchNormScope = slim.arg_scope([slim.batch_norm], is_training=True)\n\t\tweightDecayScope = slim.arg_scope([slim.conv2d, slim.fully_connected],\n\t\t\t\tweights_regularizer=slim.l2_regularizer(weightDecay),\n\t\t\t\tbiases_regularizer=slim.l2_regularizer(weightDecay))\n\n\t\tnonlocalTemp = {\n\t\t\t""trainBnEntered"" : False,\n\t\t\t""currBlock"": """"\n\t\t}\n\n\t\tdef beginBlock(name):\n\t\t\tnonlocalTemp[""currBlock""] = name\n\t\t\tif (trainFrom is not None) and (not nonlocalTemp[""trainBnEntered""]) and (trainFrom==name or trainFrom==""start""):\n\t\t\t\tprint(""Enabling training on ""+trainFrom)\n\t\t\t\tif not freezeBatchNorm:\n\t\t\t\t\ttrainBatchNormScope.__enter__()\n\t\t\t\tweightDecayScope.__enter__()\n\t\t\t\tnonlocalTemp[""trainBnEntered""]=True\n\n\t\tdef endBlock(net, scope=True, name=None):\n\t\t\tif name is None:\n\t\t\t\tname = nonlocalTemp[""currBlock""]\n\t\t\tend_points[name]=net\n\t\t\tif scope:\n\t\t\t\tscopes.append(name)\n\t\t\n\t\tdef endAll():\n\t\t\tif nonlocalTemp[""trainBnEntered""]:\n\t\t\t\tif not freezeBatchNorm:\n\t\t\t\t\ttrainBatchNormScope.__exit__(None, None, None)\n\t\t\t\tweightDecayScope.__exit__(None,None,None)\n\n\t\twith tf.variable_scope(scope, \'InceptionResnetV2\', [inputs], reuse=reuse) as scope:\n\t\t\twith slim.arg_scope([slim.batch_norm], is_training=False):\n\t\t\t\twith slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], stride=1, padding=\'SAME\'):\n\n\t\t\t\t\t# 149 x 149 x 32\n\t\t\t\t\tbeginBlock(\'Conv2d_1a_3x3\')\n\t\t\t\t\tnet = slim.conv2d(inputs, 32, 3, stride=2, padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n\t\t\t\t\tendBlock(net)\n\t\t\t\t\t# 147 x 147 x 32\n\t\t\t\t\tbeginBlock(\'Conv2d_2a_3x3\')\n\t\t\t\t\tnet = slim.conv2d(net, 32, 3, padding=\'VALID\', scope=\'Conv2d_2a_3x3\')\n\t\t\t\t\tendBlock(net)\n\t\t\t\t\t# 147 x 147 x 64\n\t\t\t\t\tbeginBlock(\'Conv2d_2b_3x3\')\n\t\t\t\t\tnet = slim.conv2d(net, 64, 3, scope=\'Conv2d_2b_3x3\')\n\t\t\t\t\tendBlock(net)\n\t\t\t\t\t# 73 x 73 x 64\n\t\t\t\t\tbeginBlock(\'MaxPool_3a_3x3\')\n\t\t\t\t\tnet = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\', scope=\'MaxPool_3a_3x3\')\n\t\t\t\t\tendBlock(net)\n\t\t\t\t\t# 73 x 73 x 80\n\t\t\t\t\tbeginBlock(\'Conv2d_3b_1x1\')\n\t\t\t\t\tnet = slim.conv2d(net, 80, 1, padding=\'VALID\', scope=\'Conv2d_3b_1x1\')\n\t\t\t\t\tendBlock(net)\n\t\t\t\t\t# 71 x 71 x 192\n\t\t\t\t\tbeginBlock(\'Conv2d_4a_3x3\')\n\t\t\t\t\tnet = slim.conv2d(net, 192, 3, padding=\'VALID\', scope=\'Conv2d_4a_3x3\')\n\t\t\t\t\tendBlock(net)\n\t\t\t\t\t# 35 x 35 x 192\n\t\t\t\t\tbeginBlock(\'MaxPool_5a_3x3\')\n\t\t\t\t\tnet = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\', scope=\'MaxPool_5a_3x3\')\n\t\t\t\t\tendBlock(net)\n\n\t\t\t\t\t# 35 x 35 x 320\n\t\t\t\t\tbeginBlock(\'Mixed_5b\')\n\t\t\t\t\twith tf.variable_scope(\'Mixed_5b\'):\n\t\t\t\t\t\twith tf.variable_scope(\'Branch_0\'):\n\t\t\t\t\t\t\ttower_conv = slim.conv2d(net, 96, 1, scope=\'Conv2d_1x1\')\n\t\t\t\t\t\twith tf.variable_scope(\'Branch_1\'):\n\t\t\t\t\t\t\ttower_conv1_0 = slim.conv2d(net, 48, 1, scope=\'Conv2d_0a_1x1\')\n\t\t\t\t\t\t\ttower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5, scope=\'Conv2d_0b_5x5\')\n\t\t\t\t\t\twith tf.variable_scope(\'Branch_2\'):\n\t\t\t\t\t\t\ttower_conv2_0 = slim.conv2d(net, 64, 1, scope=\'Conv2d_0a_1x1\')\n\t\t\t\t\t\t\ttower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3, scope=\'Conv2d_0b_3x3\')\n\t\t\t\t\t\t\ttower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3, scope=\'Conv2d_0c_3x3\')\n\t\t\t\t\t\twith tf.variable_scope(\'Branch_3\'):\n\t\t\t\t\t\t\ttower_pool = slim.avg_pool2d(net, 3, stride=1, padding=\'SAME\', scope=\'AvgPool_0a_3x3\')\n\t\t\t\t\t\t\ttower_pool_1 = slim.conv2d(tower_pool, 64, 1, scope=\'Conv2d_0b_1x1\')\n\t\t\t\t\t\tnet = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2, tower_pool_1], 3)\n\n\t\t\t\t\tendBlock(net)\n\t\t\t\t\tbeginBlock(\'Repeat\')\n\t\t\t\t\tnet = slim.repeat(net, 10, InceptionResnetV2.block35, scale=0.17)\n\t\t\t\t\tendBlock(net)\n\n\t\t\t\t\t# 17 x 17 x 1024\n\t\t\t\t\tbeginBlock(\'Mixed_6a\')\n\t\t\t\t\twith tf.variable_scope(\'Mixed_6a\'):\n\t\t\t\t\t\twith tf.variable_scope(\'Branch_0\'):\n\t\t\t\t\t\t\ttower_conv = slim.conv2d(net, 384, 3, stride=2, padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n\t\t\t\t\t\twith tf.variable_scope(\'Branch_1\'):\n\t\t\t\t\t\t\ttower_conv1_0 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n\t\t\t\t\t\t\ttower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3, scope=\'Conv2d_0b_3x3\')\n\t\t\t\t\t\t\ttower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3, stride=2, padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n\t\t\t\t\t\twith tf.variable_scope(\'Branch_2\'):\n\t\t\t\t\t\t\ttower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\', scope=\'MaxPool_1a_3x3\')\n\t\t\t\t\t\tnet = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\n\t\t\t\t\tendBlock(net)\n\n\t\t\t\t\tbeginBlock(\'Repeat_1\')\n\t\t\t\t\tnet = slim.repeat(net, 20, InceptionResnetV2.block17, scale=0.10)\n\t\t\t\t\tendBlock(net)\n\t\t\t\t\tendBlock(net, scope=False, name=\'aux\')\n\n\t\t\t\t\tbeginBlock(\'Mixed_7a\')\n\t\t\t\t\twith tf.variable_scope(\'Mixed_7a\'):\n\t\t\t\t\t\twith tf.variable_scope(\'Branch_0\'):\n\t\t\t\t\t\t\ttower_conv = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n\t\t\t\t\t\t\ttower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2, padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n\t\t\t\t\t\twith tf.variable_scope(\'Branch_1\'):\n\t\t\t\t\t\t\ttower_conv1 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n\t\t\t\t\t\t\ttower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2, padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n\t\t\t\t\t\twith tf.variable_scope(\'Branch_2\'):\n\t\t\t\t\t\t\ttower_conv2 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n\t\t\t\t\t\t\ttower_conv2_1 = slim.conv2d(tower_conv2, 288, 3, scope=\'Conv2d_0b_3x3\')\n\t\t\t\t\t\t\ttower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2, padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n\t\t\t\t\t\twith tf.variable_scope(\'Branch_3\'):\n\t\t\t\t\t\t\ttower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\', scope=\'MaxPool_1a_3x3\')\n\t\t\t\t\t\tnet = tf.concat([tower_conv_1, tower_conv1_1, tower_conv2_2, tower_pool], 3)\n\t\t\t\t\tendBlock(net)\n\n\t\t\t\t\tbeginBlock(\'Repeat_2\')\n\t\t\t\t\tnet = slim.repeat(net, 9, InceptionResnetV2.block8, scale=0.20)\n\t\t\t\t\tendBlock(net)\n\n\t\t\t\t\tbeginBlock(\'Block8\')\n\t\t\t\t\tnet = InceptionResnetV2.block8(net, activation_fn=None)\n\t\t\t\t\tendBlock(net)\n\n\t\t\t\t\tbeginBlock(\'Conv2d_7b_1x1\')\n\t\t\t\t\tnet = slim.conv2d(net, 1536, 1, scope=\'Conv2d_7b_1x1\')\n\t\t\t\t\tendBlock(net)\n\t\t\t\t\tendBlock(net, scope=False, name=\'PrePool\')\n\n\t\t\t\t\tendAll()\n\n\t\t\treturn end_points, scope, scopes\n\n\n\tdef __init__(self, name, inputs, trainFrom = None, reuse=False, weightDecay=0.00004, batchNormDecay=0.9997, batchNormEpsilon=0.001, freezeBatchNorm=False):\n\t\tself.name = name\n\t\tself.inputs = inputs\n\t\tself.trainFrom = trainFrom\n\n\t\twith slim.arg_scope([slim.conv2d, slim.fully_connected],\n\t\t\t\tweights_regularizer=None,\n\t\t\t\tbiases_regularizer=None):\n\n\t\t\tbatch_norm_params = {\n\t\t\t\t\'decay\': batchNormDecay,\n\t\t\t\t\'epsilon\': batchNormEpsilon,\n\t\t\t}\n\t\t\t# Set activation_fn and parameters for batch_norm.\n\t\t\twith slim.arg_scope([slim.conv2d],\n\t\t\t\t\tactivation_fn=tf.nn.relu,\n\t\t\t\t\tnormalizer_fn=slim.batch_norm,\n\t\t\t\t\tnormalizer_params=batch_norm_params) as scope:\n\n\t\t\t\tself.endPoints, self.scope, self.scopeList = InceptionResnetV2.define(inputs, weightDecay = weightDecay, trainFrom=trainFrom, scope=name, reuse = reuse, freezeBatchNorm = freezeBatchNorm)\n\n\tdef importWeights(self, sess, filename, includeTraining=False):\n\t\tignores = [] if includeTraining or (self.trainFrom is None) else self.getScopes(fromLayer = self.trainFrom, inclusive = True)\n\t\tprint(""Ignoring blocks:"")\n\t\tprint(ignores)\n\t\tCheckpointLoader.importIntoScope(sess, filename, fromScope=""InceptionResnetV2"", toScope=self.scope.name, ignore=ignores)\n\n\tdef getOutput(self, name=None):\n\t\tif name is None:\n\t\t\treturn self.endPoints\n\t\telse:\n\t\t\treturn self.endPoints[name]\n\n\tdef getScopes(self, fromLayer = None, toLayer = None, inclusive=False):\n\t\tl=[]\n\t\tprint(fromLayer)\n\t\tif fromLayer is not None:\n\t\t\tassert(toLayer is None)\n\t\t\ti = self.scopeList.index(fromLayer)\n\t\t\tif not inclusive:\n\t\t\t\ti += 1\n\t\t\tl=self.scopeList[i:]\n\t\telif toLayer is not None:\n\t\t\tassert(fromLayer is None)\n\t\t\ti = self.scopeList.index(toLayer)\n\t\t\tif not inclusive:\n\t\t\t\ti -= 1\n\t\t\tif i<0:\n\t\t\t\tl=[]\n\t\t\telse:\n\t\t\t\tl=self.scopeList[:(i+1)]\n\t\telse:\n\t\t\tl=self.scopeList\n\n\t\treturn [self.scope.name+""/""+s+""/"" for s in l]\n\n\tdef getTrainableVars(self):\n\t\tif self.trainFrom==None:\n\t\t\treturn []\n\n\t\tvars=[]\n\t\tscopes = self.getScopes(fromLayer=self.trainFrom, inclusive=True)\n\t\tfor s in scopes:\n\t\t\tvars +=  tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=s)\n\n\t\treturn vars\n'"
main.py,14,"b'#!/usr/bin/python\n#\n# Copyright 2017 Robert Csordas. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# ==============================================================================\n\n\nfrom Utils.ArgSave import *\nimport sys\nimport os\n\nparser = StorableArgparse(description=\'RFCN trainer.\')\nparser.add_argument(\'-learningRate\', type=float, default=0.0001, help=\'Learning rate\')\nparser.add_argument(\'-adamEps\', type=float, default=1e-8, help=\'Adam epsilon\')\nparser.add_argument(\'-dataset\', type=str, default=""/data/Datasets/COCO"", help=""Path to COCO dataset"")\nparser.add_argument(\'-name\', type=str, default=""save"", help=""Directory to save checkpoints"")\nparser.add_argument(\'-saveInterval\', type=int, default=10000, help=\'Save model for this amount of iterations\')\nparser.add_argument(\'-reportInterval\', type=int, default=30, help=\'Repeat after this amount of iterations\')\nparser.add_argument(\'-displayInterval\', type=int, default=60, help=\'Display after this amount of iterations\')\nparser.add_argument(\'-optimizer\', type=str, default=\'adam\', help=\'sgd/adam/rmsprop\')\nparser.add_argument(\'-resume\', type=str, help=\'Resume from this file\', save=False)\nparser.add_argument(\'-report\', type=str, default="""", help=\'Create report here\', save=False)\nparser.add_argument(\'-trainFrom\', type=str, default=""-1"", help=\'Train from this layer. Use 0 for all, -1 for just the added layers\')\nparser.add_argument(\'-hardMining\', type=int, default=1, help=""Enable hard example mining."")\nparser.add_argument(\'-gpu\', type=str, default=""0"", help=\'Train on this GPU(s)\')\nparser.add_argument(\'-mergeValidationSet\', type=int, default=1, help=\'Merge validation set to training set.\')\nparser.add_argument(\'-profile\', type=int, default=0, help=\'Enable profiling\', save=False)\nparser.add_argument(\'-randZoom\', type=int, default=1, help=\'Enable box aware random zooming and cropping\')\nparser.add_argument(\'-freezeBatchNorm\', type=int, default=1, help=\'Freeze batch normalization during finetuning.\')\nparser.add_argument(\'-export\', type=str, help=\'Export model here.\', save=False)\nparser.add_argument(\'-cocoVariant\', type=str, default=""2014"", help=\'Coco variant to load. 2014 or 2017\')\n\nopt=parser.parse_args()\n\nif not os.path.isdir(opt.name):\n\tos.makedirs(opt.name)\n\nopt = parser.load(opt.name+""/args.json"")\nparser.save(opt.name+""/args.json"")\n\nif not os.path.isdir(opt.name+""/log""):\n\tos.makedirs(opt.name+""/log"")\n\nif not os.path.isdir(opt.name+""/save""):\n\tos.makedirs(opt.name+""/save"")\n\nif not os.path.isdir(opt.name+""/preview""):\n\tos.makedirs(opt.name+""/preview"")\n\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = opt.gpu\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import control_flow_ops\nfrom Dataset.CocoDataset import *\nfrom Dataset.BoxLoader import *\nfrom Utils.RunManager import *\nfrom Utils.CheckpointLoader import *\nfrom BoxInceptionResnet import *\nfrom Dataset import Augment\nfrom Visualize import VisualizeOutput\nfrom Utils import Model\nfrom Utils import Export\nfrom tensorflow.python.client import timeline\nimport re\n\nglobalStep = tf.Variable(0, name=\'globalStep\', trainable=False)\nglobalStepInc=tf.assign_add(globalStep,1)\n\nModel.download()\n\ndataset = BoxLoader()\ndataset.add(CocoDataset(opt.dataset, randomZoom=opt.randZoom==1, set=""train""+opt.cocoVariant))\nif opt.mergeValidationSet==1:\n\tdataset.add(CocoDataset(opt.dataset, set=""val""+opt.cocoVariant))\n\n\nimages, boxes, classes = Augment.augment(*dataset.get())\n\n\nprint(""Number of categories: ""+str(dataset.categoryCount()))\nprint(dataset.getCaptionMap())\n\n\nnet = BoxInceptionResnet(images, dataset.categoryCount(), name=""boxnet"", trainFrom=opt.trainFrom, hardMining=opt.hardMining==1, freezeBatchNorm=opt.freezeBatchNorm==1)\ntf.losses.add_loss(net.getLoss(boxes, classes))\n\ndef createUpdateOp(gradClip=1):\n\twith tf.name_scope(""optimizer""):\n\t\toptimizer=tf.train.AdamOptimizer(learning_rate=opt.learningRate, epsilon=opt.adamEps)\n\t\tupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\t\ttotalLoss = tf.losses.get_total_loss()\n\t\tgrads = optimizer.compute_gradients(totalLoss, var_list=net.getVariables())\n\t\tif gradClip is not None:\n\t\t\tcGrads = []\n\t\t\tfor g, v in grads:\n\t\t\t\tif g is None:\n\t\t\t\t\tprint(""WARNING: no grad for variable ""+v.op.name)\n\t\t\t\t\tcontinue\n\t\t\t\tcGrads.append((tf.clip_by_value(g, -float(gradClip), float(gradClip)), v))\n\t\t\tgrads = cGrads\n\n\t\tupdate_ops.append(optimizer.apply_gradients(grads))\n\t\treturn control_flow_ops.with_dependencies([tf.group(*update_ops)], totalLoss, name=\'train_op\')\n\ntrainOp=createUpdateOp()\n\nsaver=tf.train.Saver(keep_checkpoint_every_n_hours=4, max_to_keep=100)\n\n\nif opt.profile==1:\n\trunOptions = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\trunMetadata = tf.RunMetadata()\n\titerationsSinceStart=0\nelse:\n\trunOptions=None\n\trunMetadata=None\n\nwith tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=8)) as sess:\n\tif not loadCheckpoint(sess, opt.name+""/save/"", opt.resume):\n\t\tprint(""Loading GoogleNet"")\n\t\tnet.importWeights(sess, ""./inception_resnet_v2_2016_08_30.ckpt"")\n\t\t#net.importWeights(sess, ""initialWeights/"", permutateRgb=False)\n\t\tprint(""Done."")\n\n\tif opt.export is not None:\n\t\tExport.exportModel(sess, opt.export, [lambda name: name.split(""/"")[0]==""boxnet"" and not re.match(""^[Aa]dam(_.*)?$"",name.split(""/"")[-1])])\n\t\tsys.exit(-1)\n\n\tdataset.startThreads(sess)\n\n\trunManager = RunManager(sess, options=runOptions, run_metadata=runMetadata)\n\trunManager.add(""train"", [globalStepInc,trainOp], modRun=1)\n\n\n\tvisualizer = VisualizeOutput.OutputVisualizer(opt, runManager, dataset, net, images, boxes, classes)\n\n\ti=1\n\tcycleCnt=0\n\tlossSum=0\n\n\twhile True:\n\t\t#run various parts of the network\n\t\tres = runManager.modRun(i)\n\n\t\tif opt.profile==1:\n\t\t\tprint(""Profiling step %d"" % iterationsSinceStart)\n\t\t\titerationsSinceStart+=1\n\t\t\tif iterationsSinceStart==5:\n\t\t\t\tprint(""Writing profile data..."")\n\t\t\t\ttl = timeline.Timeline(runMetadata.step_stats)\n\t\t\t\tctf = tl.generate_chrome_trace_format()\n\t\t\t\twith open(\'timeline.json\', \'w\') as f:\n\t\t\t\t\tf.write(ctf)\n\n\t\t\t\tprint(""Done."")\n\t\t\t\tsys.exit(0)\n\n\t\t\t\n\t\ti, loss=res[""train""]\n\n\t\tlossSum+=loss\n\t\tcycleCnt+=1\n\n\t\tvisualizer.draw(res)\n\n\t\tif i % opt.reportInterval == 0:\n\t\t\tif cycleCnt>0:\n\t\t\t\tloss=lossSum/cycleCnt\n\n\t\t\t# lossS=sess.run(trainLossSum, feed_dict={\n\t\t\t# \ttrainLossFeed: loss\n\t\t\t# })\n\t\t\t# log.add_summary(lossS, global_step=samplesSeen)\n\n\t\t\tepoch=""%.2f"" % (float(i) / dataset.count())\n\t\t\tprint(""Iteration ""+str(i)+"" (epoch: ""+epoch+""): loss: ""+str(loss))\n\t\t\tlossSum=0\n\t\t\tcycleCnt=0\n\n\t\tif i % opt.saveInterval == 0:\n\t\t\tprint(""Saving checkpoint ""+str(i))\n\t\t\tsaver.save(sess, opt.name+""/save/model_""+str(i), write_meta_graph=False)\n'"
test.py,2,"b'#!/usr/bin/python\n#\n# Copyright 2017 Robert Csordas. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# ==============================================================================\n\nimport argparse\nimport os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom BoxInceptionResnet import BoxInceptionResnet\nfrom Visualize import Visualize\nfrom Utils import CheckpointLoader\nfrom Utils import PreviewIO\n\nparser = argparse.ArgumentParser(description=""RFCN tester"")\nparser.add_argument(\'-gpu\', type=str, default=""0"", help=\'Train on this GPU(s)\')\nparser.add_argument(\'-n\', type=str, help=\'Network checkpoint file\')\nparser.add_argument(\'-i\', type=str, help=\'Input file.\')\nparser.add_argument(\'-o\', type=str, default="""", help=\'Write output here.\')\nparser.add_argument(\'-p\', type=int, default=1, help=\'Show preview\')\nparser.add_argument(\'-threshold\', type=float, default=0.5, help=\'Detection threshold\')\nparser.add_argument(\'-delay\', type=int, default=-1, help=\'Delay between frames in visualization. -1 for automatic, 0 for wait for keypress.\')\n\nopt=parser.parse_args()\nos.environ[""CUDA_VISIBLE_DEVICES""] = opt.gpu\n\ncategories = [\'person\', \'bicycle\', \'car\', \'motorcycle\', \'airplane\', \'bus\', \'train\', \'truck\', \'boat\', \'traffic light\', \'fire hydrant\', \'stop sign\', \'parking meter\',\n \'bench\', \'bird\', \'cat\', \'dog\', \'horse\', \'sheep\', \'cow\', \'elephant\', \'bear\', \'zebra\', \'giraffe\', \'backpack\', \'umbrella\', \'handbag\', \'tie\', \'suitcase\',\n \'frisbee\', \'skis\', \'snowboard\', \'sports ball\', \'kite\', \'baseball bat\', \'baseball glove\', \'skateboard\', \'surfboard\', \'tennis racket\', \'bottle\', \'wine glass\',\n \'cup\', \'fork\', \'knife\', \'spoon\', \'bowl\', \'banana\', \'apple\', \'sandwich\', \'orange\', \'broccoli\', \'carrot\', \'hot dog\', \'pizza\', \'donut\', \'cake\', \'chair\',\n \'couch\', \'potted plant\', \'bed\', \'dining table\', \'toilet\', \'tv\', \'laptop\', \'mouse\', \'remote\', \'keyboard\', \'cell phone\', \'microwave\', \'oven\', \'toaster\',\n \'sink\', \'refrigerator\', \'book\', \'clock\', \'vase\', \'scissors\', \'teddy bear\', \'hair drier\', \'toothbrush\']\n\npalette = Visualize.Palette(len(categories))\n\nimage = tf.placeholder(tf.float32, [None, None, None, 3])\nnet = BoxInceptionResnet(image, len(categories), name=""boxnet"")\n\nboxes, scores, classes = net.getBoxes(scoreThreshold=opt.threshold)\n\n\ninput = PreviewIO.PreviewInput(opt.i)\noutput = PreviewIO.PreviewOutput(opt.o, input.getFps())\n\ndef preprocessInput(img):\n\tdef calcPad(size):\n\t\tm = size % 32\n\t\tp = int(m/2)\n\t\ts = size - m\n\t\treturn s,p\n\n\tzoom = max(640.0 / img.shape[0], 640.0 / img.shape[1])\n\timg = cv2.resize(img, (int(zoom*img.shape[1]), int(zoom*img.shape[0])))\n\n\tif img.shape[0] % 32 != 0:\n\t\ts,p = calcPad(img.shape[0])\n\t\timg = img[p:p+s]\n\n\tif img.shape[1] % 32 != 0:\n\t\ts,p = calcPad(img.shape[1])\n\t\timg = img[:,p:p+s]\n\n\treturn img\n\nwith tf.Session() as sess:\n\tif not CheckpointLoader.loadCheckpoint(sess, None, opt.n, ignoreVarsInFileNotInSess=True):\n\t\tprint(""Failed to load network."")\n\t\tsys.exit(-1)\n\n\twhile True:\n\t\timg = input.get()\n\t\tif img is None:\n\t\t\tbreak\n\n\t\timg = preprocessInput(img)\t\n\n\t\trBoxes, rScores, rClasses = sess.run([boxes, scores, classes], feed_dict={image: np.expand_dims(img, 0)})\n\n\t\tres = Visualize.drawBoxes(img, rBoxes, rClasses, [categories[i] for i in rClasses.tolist()], palette, scores=rScores)\n\n\t\toutput.put(input.getName(), res)\n\n\t\tif opt.p==1:\n\t\t\tcv2.imshow(""result"", res)\n\t\t\tif opt.o=="""":\n\t\t\t\tcv2.waitKey(input.getDelay() if opt.delay <0 else opt.delay)\n\t\t\telse:\n\t\t\t\tcv2.waitKey(1)\n'"
testCheckpoint.py,1,"b'#!/usr/bin/python\n#\n# Copyright 2017 Robert Csordas. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# ==============================================================================\n\nimport tensorflow as tf\nimport os\nimport sys\nimport numpy as np\nimport argparse\n\nparser = argparse.ArgumentParser(description=\'Checkpoint tester.\')\nparser.add_argument(\'-stats\', type=int, default=0, help=\'Enable statistics\')\nparser.add_argument(\'-n\', type=str, default="""", help=\'Network checkpoint\')\nopt=parser.parse_args()\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = """"\n\n\nreader = tf.contrib.framework.load_checkpoint(opt.n)\n\nsumSize=0.0\n\nsizes = []\nnames = []\n\nsMap = reader.get_variable_to_shape_map()\n\nfor v, s in sMap.items():\n\tsize=0.0\n\tif len(s)>0:\n\t\tsize = np.prod(s)\n\t\n\tsize *= 4.0\n\tsumSize += size\n\tsize /= 1024.0*1024.0\n\tsizes.append(size)\n\tnames.append(v)\n\n\ni = np.argsort(sizes)[::-1]\nsizes = np.array(sizes)[i]\nnames = np.array(names)[i]\n\ncumulativeSize = 0\n\nfor i in range(len(sizes)):\n\tsSize= ""%.2f Mb"" % sizes[i]\n\tsName = names[i]\n\n\tcumulativeSize += sizes[i]\n\n\tsTotalSize= ""%.2f Mb"" % cumulativeSize\n\n\tprint(""%10s %20s %15s \\t %s"" % (sSize, str(sMap[sName]), sTotalSize, sName))\n\nprint(""Total size: %.2f Mb"" % (sumSize/(1024.0*1024.0)))\n\t\t\n\nif opt.stats==1:\n\tprint(""-------------------------------------------------"")\n\tprint(""Statistics:"")\n\tfor i in range(len(sizes)):\n\t\tt = reader.get_tensor(names[i])\n\t\t\n\t\ts = "" %f    %f    %f"" % (t.min(), t.mean(), t.max())\n\t\tprint(""%60s \\t %s"" % (s, names[i]))\n\t'"
BoxEngine/BoxNetwork.py,3,"b'# Copyright 2017 Robert Csordas. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# ==============================================================================\n\nfrom BoxEngine.BoxRefinementNetwork import BoxRefinementNetwork\nfrom BoxEngine.RPN import RPN\nimport tensorflow as tf\n\nclass BoxNetwork:\n\tdef __init__(self, nCategories, rpnLayer, rpnDownscale, rpnOffset, featureLayer=None, featureDownsample=None, featureOffset=None, weightDecay=1e-6, hardMining=True):\n\t\tif featureLayer is None:\n\t\t\tfeatureLayer=rpnLayer\n\n\t\tif featureDownsample is None:\n\t\t\tfeatureDownsample=rpnDownscale\n\t\t\t\n\t\tif featureOffset is None:\n\t\t\trpnOffset=featureOffset\n\n\t\twith tf.name_scope(""BoxNetwork""):\n\t\t\tself.rpn = RPN(rpnLayer, immediateSize=512, weightDecay=weightDecay, inputDownscale=rpnDownscale, offset=rpnOffset)\n\t\t\tself.boxRefiner = BoxRefinementNetwork(featureLayer, nCategories, downsample=featureDownsample, offset=featureOffset, hardMining=hardMining)\n\n\t\t\tself.proposals, self.proposalScores = self.rpn.getPositiveOutputs(maxOutSize=300)\n\n\t\t\n\tdef getProposals(self, threshold=None):\n\t\tif threshold is not None and threshold>0:\n\t\t\ts = tf.cast(tf.where(self.proposalScores > threshold), tf.int32)\n\t\t\treturn tf.gather_nd(self.proposals, s), tf.gather_nd(self.proposalScores, s)\n\t\telse:\n\t\t\treturn self.proposals, self.proposalScores\n\t\t\n\tdef getBoxes(self, nmsThreshold=0.3, scoreThreshold=0.8):\n\t\treturn self.boxRefiner.getBoxes(self.proposals, self.proposalScores, maxOutputs=50, nmsThreshold=nmsThreshold, scoreThreshold=scoreThreshold)\n\n\tdef getLoss(self, refBoxes, refClasses):\n\t\treturn self.rpn.loss(refBoxes) + self.boxRefiner.loss(self.proposals, refBoxes, refClasses)\n'"
BoxEngine/BoxRefinementNetwork.py,53,"b'# Copyright 2017 Robert Csordas. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# ==============================================================================\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom BoxEngine.ROIPooling import positionSensitiveRoiPooling\nimport math\nimport Utils.RandomSelect\nimport BoxEngine.BoxUtils as BoxUtils\nimport Utils.MultiGather as MultiGather\nimport BoxEngine.Loss as Loss\n\nclass BoxRefinementNetwork:\n\tPOOL_SIZE=3\n\n\tdef __init__(self, input, nCategories, downsample=16, offset=[32,32], hardMining=True):\n\t\tself.downsample = downsample\n\t\tself.offset = offset\n\t\tself.nCategories = nCategories\n\t\tself.classMaps = slim.conv2d(input, (self.POOL_SIZE**2)*(1+nCategories), 3, activation_fn=None, scope=\'classMaps\')\n\t\tself.regressionMap = slim.conv2d(input, (self.POOL_SIZE**2)*4, 3, activation_fn=None, scope=\'regressionMaps\')\n\n\t\tself.hardMining=hardMining\n\n\t\t#Magic parameters.\n\t\tself.posIouTheshold = 0.5\n\t\tself.negIouThesholdHi = 0.5\n\t\tself.negIouThesholdLo = 0.1\n\t\tself.nTrainBoxes = 128\n\t\tself.nTrainPositives = 32\n\t\tself.falseValue = 0.0002\n\n\tdef roiPooling(self, layer, boxes):\n\t\treturn positionSensitiveRoiPooling(layer, boxes, offset=self.offset, downsample=self.downsample, roiSize=self.POOL_SIZE)\n\n\tdef roiMean(self, layer, boxes):\n\t\twith tf.name_scope(""roiMean""):\n\t\t\treturn tf.reduce_mean(self.roiPooling(layer, boxes), axis=[1,2])\n\n\tdef getBoxScores(self, boxes):\n\t\twith tf.name_scope(""getBoxScores""):\n\t\t\treturn self.roiMean(self.classMaps, boxes)\n\n\tdef classRefinementLoss(self, boxes, refs):\n\t\twith tf.name_scope(""classRefinementLoss""):\n\t\t\tnetScores = self.getBoxScores(boxes)\n\t\t\trefOnehot = tf.one_hot(refs, self.nCategories+1, on_value=1.0 - self.nCategories*self.falseValue, off_value=self.falseValue)\n\t\t\n\t\t\treturn tf.nn.softmax_cross_entropy_with_logits(logits=netScores, labels=refOnehot)\n\n\tdef refineBoxes(self, boxes, needSizes):\n\t\twith tf.name_scope(""refineBoxes""):\n\t\t\tboxFineData = self.roiMean(self.regressionMap, boxes)\n\n\t\t\tx,y,w,h = BoxUtils.x0y0x1y1_to_xywh(*tf.unstack(boxes, axis=1))\n\t\t\tx_rel, y_rel, w_rel, h_rel = tf.unstack(boxFineData, axis=1)\n\n\t\t\tif needSizes:\n\t\t\t\trefSizes = tf.stack([h,w], axis=1)\n\n\t\t\tx = x + x_rel * w\n\t\t\ty = y + y_rel * h\n\n\t\t\tw = w * tf.exp(w_rel)\n\t\t\th = h * tf.exp(h_rel)\n\n\t\t\trefinedBoxes = tf.stack(BoxUtils.xywh_to_x0y0x1y1(x,y,w,h), axis=1)\n\n\t\t\tif needSizes:\n\t\t\t\treturn refinedBoxes, refSizes, boxFineData[:,2:4]\n\t\t\telse:\n\t\t\t\treturn refinedBoxes\n\n\tdef boxRefinementLoss(self, boxes, refBoxes):\n\t\twith tf.name_scope(""boxesRefinementLoss""):\n\t\t\trefinedBoxes, refSizes, rawSizes = self.refineBoxes(boxes, True)\n\t\t\treturn Loss.boxRegressionLoss(refinedBoxes, rawSizes, refBoxes, refSizes)\n\n\tdef loss(self, proposals, refBoxes, refClasses):\n\t\twith tf.name_scope(""BoxRefinementNetworkLoss""):\n\t\t\tproposals = tf.stop_gradient(proposals)\n\n\t\t\tdef getPosLoss(positiveBoxes, positiveRefIndices, nPositive):\n\t\t\t\twith tf.name_scope(""getPosLoss""):\n\t\t\t\t\tpositiveRefIndices =  tf.reshape(positiveRefIndices,[-1,1])\n\n\t\t\t\t\tpositiveClasses, positiveRefBoxes = MultiGather.gather([refClasses, refBoxes], positiveRefIndices)\n\t\t\t\t\tpositiveClasses = tf.cast(tf.cast(positiveClasses,tf.int8) + 1, tf.uint8)\n\n\t\t\t\t\tif not self.hardMining:\n\t\t\t\t\t\tselected = Utils.RandomSelect.randomSelectIndex(tf.shape(positiveBoxes)[0], nPositive)\n\t\t\t\t\t\tpositiveBoxes, positiveClasses, positiveRefBoxes = MultiGather.gather([positiveBoxes, positiveClasses, positiveRefBoxes], selected)\n\n\t\t\t\t\treturn tf.tuple([self.classRefinementLoss(positiveBoxes, positiveClasses) + self.boxRefinementLoss(positiveBoxes, positiveRefBoxes), tf.shape(positiveBoxes)[0]])\n\n\t\t\tdef getNegLoss(negativeBoxes, nNegative):\n\t\t\t\twith tf.name_scope(""getNetLoss""):\n\t\t\t\t\tif not self.hardMining:\n\t\t\t\t\t\tnegativeIndices = Utils.RandomSelect.randomSelectIndex(tf.shape(negativeBoxes)[0], nNegative)\n\t\t\t\t\t\tnegativeBoxes = tf.gather_nd(negativeBoxes, negativeIndices)\n\n\t\t\t\t\treturn self.classRefinementLoss(negativeBoxes, tf.zeros(tf.stack([tf.shape(negativeBoxes)[0],1]), dtype=tf.uint8))\n\t\t\t\n\t\t\tdef getRefinementLoss():\n\t\t\t\twith tf.name_scope(""getRefinementLoss""):\n\t\t\t\t\tiou = BoxUtils.iou(proposals, refBoxes)\n\t\t\t\t\t\n\t\t\t\t\tmaxIou = tf.reduce_max(iou, axis=1)\n\t\t\t\t\tbestIou = tf.expand_dims(tf.cast(tf.argmax(iou, axis=1), tf.int32), axis=-1)\n\n\t\t\t\t\t#Find positive and negative indices based on their IOU\n\t\t\t\t\tposBoxIndices = tf.cast(tf.where(maxIou > self.posIouTheshold), tf.int32)\n\t\t\t\t\tnegBoxIndices = tf.cast(tf.where(tf.logical_and(maxIou < self.negIouThesholdHi, maxIou > self.negIouThesholdLo)), tf.int32)\n\n\t\t\t\t\t#Split the boxes and references\n\t\t\t\t\tposBoxes, posRefIndices = MultiGather.gather([proposals, bestIou], posBoxIndices)\n\t\t\t\t\tnegBoxes = tf.gather_nd(proposals, negBoxIndices)\n\n\t\t\t\t\t#Add GT boxes\n\t\t\t\t\tposBoxes = tf.concat([posBoxes,refBoxes], 0)\n\t\t\t\t\tposRefIndices = tf.concat([posRefIndices, tf.reshape(tf.range(tf.shape(refClasses)[0]), [-1,1])], 0)\n\n\t\t\t\t\t#Call the loss if the box collection is not empty\n\t\t\t\t\tnPositive = tf.shape(posBoxes)[0]\n\t\t\t\t\tnNegative = tf.shape(negBoxes)[0]\n\n\t\t\t\t\tif self.hardMining:\n\t\t\t\t\t\tposLoss = tf.cond(nPositive > 0, lambda: getPosLoss(posBoxes, posRefIndices, 0)[0], lambda: tf.zeros((0,), tf.float32))\n\t\t\t\t\t\tnegLoss = tf.cond(nNegative > 0, lambda: getNegLoss(negBoxes, 0), lambda: tf.zeros((0,), tf.float32))\n\n\t\t\t\t\t\tallLoss = tf.concat([posLoss, negLoss], 0)\n\t\t\t\t\t\treturn tf.cond(tf.shape(allLoss)[0]>0, lambda: tf.reduce_mean(Utils.MultiGather.gatherTopK(allLoss, self.nTrainBoxes)), lambda: tf.constant(0.0))\n\t\t\t\t\telse:\n\t\t\t\t\t\tposLoss, posCount = tf.cond(nPositive > 0, lambda: getPosLoss(posBoxes, posRefIndices, self.nTrainPositives), lambda: tf.tuple([tf.constant(0.0), tf.constant(0,tf.int32)]))\n\t\t\t\t\t\tnegLoss = tf.cond(nNegative > 0, lambda: getNegLoss(negBoxes, self.nTrainBoxes-posCount), lambda: tf.constant(0.0))\n\n\t\t\t\t\t\tnPositive = tf.cast(tf.shape(posLoss)[0], tf.float32)\n\t\t\t\t\t\tnNegative = tf.cond(nNegative > 0, lambda: tf.cast(tf.shape(negLoss)[0], tf.float32), lambda: tf.constant(0.0))\n\t\t\t\t\t\t\n\t\t\t\t\t\treturn (tf.reduce_mean(posLoss)*nPositive + tf.reduce_mean(negLoss)*nNegative)/(nNegative+nPositive)\n\t\n\n\t\treturn tf.cond(tf.logical_and(tf.shape(proposals)[0] > 0, tf.shape(refBoxes)[0] > 0), lambda: getRefinementLoss(), lambda:tf.constant(0.0))\n\n\tdef getBoxes(self, proposals, proposal_scores, maxOutputs=30, nmsThreshold=0.3, scoreThreshold=0.8):\n\t\tif scoreThreshold is None:\n\t\t\tscoreThreshold = 0\n\n\t\twith tf.name_scope(""getBoxes""):\n\t\t\tscores = tf.nn.softmax(self.getBoxScores(proposals))\n\t\t\t\n\t\t\tclasses = tf.argmax(scores, 1)\n\t\t\tscores = tf.reduce_max(scores, axis=1)\n\t\t\tposIndices = tf.cast(tf.where(tf.logical_and(classes > 0, scores>scoreThreshold)), tf.int32)\n\n\t\t\tpositives, scores, classes = MultiGather.gather([proposals, scores, classes], posIndices)\n\t\t\tpositives = self.refineBoxes(positives, False)\n\n\t\t\t#Final NMS\n\t\t\tposIndices = tf.image.non_max_suppression(positives, scores, iou_threshold=nmsThreshold, max_output_size=maxOutputs)\n\t\t\tposIndices = tf.expand_dims(posIndices, axis=-1)\n\t\t\tpositives, scores, classes = MultiGather.gather([positives, scores, classes], posIndices)\t\n\t\t\t\n\t\t\tclasses = tf.cast(tf.cast(classes,tf.int32) - 1, tf.uint8)\n\n\t\t\treturn positives, scores, classes'"
BoxEngine/BoxUtils.py,36,"b'# Copyright 2017 Robert Csordas. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# ==============================================================================\n\nimport tensorflow as tf\n\ndef iou(boxes, refBoxes, oneToAll=True):\n\twith tf.name_scope(""IOU""):\n\t\tx0, y0, x1, y1 = tf.unstack(boxes, axis=1)\n\t\tref_x0, ref_y0, ref_x1, ref_y1 = tf.unstack(refBoxes, axis=1)\n\n\t\t#Calculate box IOU\n\t\tx0=tf.reshape(x0,[-1,1])\n\t\ty0=tf.reshape(y0,[-1,1])\n\t\tx1=tf.reshape(x1,[-1,1])\n\t\ty1=tf.reshape(y1,[-1,1])\n\n\t\tif oneToAll:\n\t\t\tboxShape = [1,-1]\n\t\telse:\n\t\t\tboxShape = [-1,1]\n\n\t\tref_x0=tf.reshape(ref_x0,boxShape)\n\t\tref_y0=tf.reshape(ref_y0,boxShape)\n\t\tref_x1=tf.reshape(ref_x1,boxShape)\n\t\tref_y1=tf.reshape(ref_y1,boxShape)\n\n\t\tmax_x0 = tf.maximum(x0, ref_x0)\n\t\tmax_y0 = tf.maximum(y0, ref_y0)\n\t\tmin_x1 = tf.minimum(x1, ref_x1)\n\t\tmin_y1 = tf.minimum(y1, ref_y1)\n\n\t\tintersect = tf.maximum(min_x1 - max_x0 + 1, 0.0) * tf.maximum(min_y1 - max_y0 + 1, 0.0)\n\t\tunion = (x1 - x0 + 1) * (y1 - y0 + 1) + (ref_x1 - ref_x0 + 1) * (ref_y1 - ref_y0 + 1) - intersect\n\n\t\tiou = intersect / union\n\n\t\treturn iou\n\ndef filterSmallBoxes(boxes, others=None, minSize=16.0):\n\twith tf.name_scope(""filterSmallBoxes""):\n\t\tx0,y0,x1,y1 = tf.unstack(boxes, axis=1)\n\t\t\n\t\tokIndices = tf.where(tf.logical_and((x1-x0) >= (minSize-1), (y1-y0) >= (minSize-1)))\n\t\tokIndices = tf.cast(okIndices, tf.int32)\n\n\t\tres = [tf.gather_nd(boxes, okIndices)]\n\n\t\tif others is not None:\n\t\t\tfor o in others:\n\t\t\t\tres.append(tf.gather_nd(o, okIndices))\n\n\t\treturn res\n\ndef x0y0x1y1_to_xywh(x0,y0,x1,y1):\n\twith tf.name_scope(""x0y0x1y1_to_xywh""):\n\t\tx = (x0+x1)/2.0\n\t\ty = (y0+y1)/2.0\n\t\tw = x1-x0+1\n\t\th = y1-y0+1\n\t\treturn x,y,w,h\n\ndef xywh_to_x0y0x1y1(x,y,w,h):\n\twith tf.name_scope(""xywh_to_x0y0x1y1""):\n\t\tw_per_2=w/2.0\n\t\th_per_2=h/2.0\n\n\t\treturn x-w_per_2+0.5, y-h_per_2+0.5, x+w_per_2-0.5, y+h_per_2-0.5\n\ndef nnToCenteredBox(x_raw, y_raw, w_raw, h_raw, wA, hA, inputDownscale, offset):\n\twith tf.name_scope(\'nnToCenteredBox\'):\n\t\ts = tf.shape(x_raw)\n\t\twIn = s[2]\n\t\thIn = s[1]\n\t\tx = x_raw*wA + tf.reshape((tf.range(0.0,tf.cast(wIn, tf.float32), dtype=tf.float32)+0.5)*inputDownscale, [-1,1]) + offset[1]\n\t\ty = y_raw*hA + tf.reshape((tf.range(0.0,tf.cast(hIn, tf.float32), dtype=tf.float32)+0.5)*inputDownscale, [-1,1,1]) + offset[0]\n\t\tw = tf.exp(w_raw) * wA\n\t\th = tf.exp(h_raw) * hA\n\n\t\treturn x,y,w,h\n\ndef mergeBoxData(list):\n\twith tf.name_scope(\'mergeBoxData\'):\n\t\tl2 = []\n\t\tfor l in list:\n\t\t\tl2.append(tf.expand_dims(l, -1))\n\t\t\n\t\tres = tf.concat(l2, tf.rank(list[0]))\n\t\treturn tf.reshape(res, [-1,len(list)])\n\t\n\n\ndef mergeCoordinates(x,y,w,h):\n\twith tf.name_scope(\'mergeCoordinates\'):\n\t\treturn mergeBoxData(list(xywh_to_x0y0x1y1(x,y,w,h)))\n\n\ndef nnToImageBoxes(x_raw, y_raw, w_raw, h_raw, wA, hA, inputDownscale, offset):\n\twith tf.name_scope(""nnToImageBoxes""):\n\t\treturn mergeCoordinates(*nnToCenteredBox(x_raw, y_raw, w_raw, h_raw, wA, hA, inputDownscale, offset))\n'"
BoxEngine/Loss.py,13,"b'# Copyright 2017 Robert Csordas. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# ==============================================================================\n\nimport tensorflow as tf \nimport BoxEngine.BoxUtils as BoxUtils\n\ndef smooth_l1(x):\n    with tf.name_scope(""smooth_l1""):\n        abs_x = tf.abs(x)\n        lessMask = tf.cast(abs_x < 1.0, tf.float32)\n\n        return lessMask * (0.5 * tf.square(x)) + (1.0 - lessMask) * (abs_x - 0.5)\n\ndef reshapeAll(l, shape=[-1]):\n    with tf.name_scope(""reshapeAll""):\n        res=[]\n        for e in l:\n            res.append(tf.reshape(e, shape))\n        return res\n\ndef boxRegressionLoss(boxes, rawSizes, refBoxes, boxSizes):\n    with tf.name_scope(""rawBoxRegressionLoss""):\n        x, y, w, h = BoxUtils.x0y0x1y1_to_xywh(*tf.unstack(boxes, axis=1))\n        wRel, hRel = tf.unstack(rawSizes, axis=1)\n        boxH, boxW = tf.unstack(boxSizes, axis=1)\n        ref_x, ref_y, ref_w, ref_h = BoxUtils.x0y0x1y1_to_xywh(*tf.unstack(refBoxes, axis=1))\n\n        x,y,wRel,hRel, boxH,boxW, ref_x,ref_y,ref_w,ref_h = reshapeAll([x,y,wRel,hRel, boxH,boxW, ref_x,ref_y,ref_w,ref_h])\n        \n        wrelRef = tf.log(ref_w/boxW)\n        hrelRef = tf.log(ref_h/boxH)\n\n        # Smooth L1 loss is defined on NN output values, but only the box sizes are available here. However\n        # we can transform back the coordinates in a numerically stable way in the NN output space:\n        #\n        # tx-tx\' = (x-x\')/wa\n\n        return smooth_l1((x-ref_x)/boxW) + smooth_l1((y-ref_y)/boxH) + smooth_l1(wRel - wrelRef) + smooth_l1(hRel - hrelRef)\n        '"
BoxEngine/RPN.py,72,"b'# Copyright 2017 Robert Csordas. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# ==============================================================================\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport math\nimport Utils.RandomSelect\nimport BoxEngine.BoxUtils as BoxUtils\nimport BoxEngine.Loss as Loss\nimport Utils.MultiGather as MultiGather\n\nclass RPN:\n\tdef __init__(self, input, anchors=None, immediateSize=512, weightDecay=1e-5, inputDownscale=16, offset=[32,32]):\n\t\tself.input = input\n\t\tself.anchors = anchors\n\t\tself.inputDownscale = inputDownscale\n\t\tself.offset = offset\n\t\tself.anchors = anchors if anchors is not None else self.makeAnchors([64,128,256,512])\n\t\tprint(""Anchors: "", self.anchors)\n\t\tself.tfAnchors = tf.constant(self.anchors, dtype=tf.float32)\n\n\t\tself.hA=tf.reshape(self.tfAnchors[:,0],[-1])\n\t\tself.wA=tf.reshape(self.tfAnchors[:,1],[-1])\n\n\t\tself.nAnchors = len(self.anchors)\n\n\t\tself.positiveIouThreshold=0.7\n\t\tself.negativeIouThreshold=0.3\n\t\tself.regressionWeight=1.0\n\t\t\n\t\tself.nBoxLosses=256\n\t\tself.nPositiveLosses=128\n\n\t\t#dimensions\n\t\twith tf.name_scope(\'dimension_info\'):\n\t\t\ts = tf.shape(self.input)\n\t\t\tself.hIn = s[1]\n\t\t\tself.wIn = s[2]\n\n\t\t\n\t\tself.imageH = tf.cast(self.hIn*self.inputDownscale+self.offset[0]*2, tf.float32)\n\t\tself.imageW = tf.cast(self.wIn*self.inputDownscale+self.offset[1]*2, tf.float32)\n\n\t\tself.define(immediateSize, weightDecay)\n\n\n\tdef define(self, immediateSize, weightDecay):\n\t\twith tf.name_scope(\'RPN\'):\n\t\t\twith slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(weightDecay), padding=\'SAME\'):\n\t\t\t\t#box prediction layers\n\t\t\t\twith tf.name_scope(\'NN\'):\n\t\t\t\t\tnet = slim.conv2d(self.input, immediateSize, 3, activation_fn=tf.nn.relu)\n\t\t\t\t\tscores = slim.conv2d(net, 2*self.nAnchors, 1, activation_fn=None)\n\t\t\t\t\tboxRelativeCoordinates = slim.conv2d(net, 4*self.nAnchors, 1, activation_fn=None)\n\n\t\t\t\t#split coordinates\n\t\t\t\tx_raw, y_raw, w_raw, h_raw = tf.split(boxRelativeCoordinates, 4, axis=3)\n\n\t\t\t\t#Save raw box sizes for loss\n\t\t\t\tself.rawSizes = BoxUtils.mergeBoxData([w_raw, h_raw])\n\t\t\t\t\t\t\t\n\t\t\t\t#Convert NN outputs to BBox coordinates\n\t\t\t\tself.boxes = BoxUtils.nnToImageBoxes(x_raw, y_raw, w_raw, h_raw, self.wA, self.hA, self.inputDownscale, self.offset)\n\n\t\t\t\t#store the size of every box\n\t\t\t\twith tf.name_scope(\'box_sizes\'):\n\t\t\t\t\tboxSizes = tf.reshape(self.tfAnchors, [1,1,1,-1,2])\n\t\t\t\t\tboxSizes = tf.tile(boxSizes, tf.stack([1,self.hIn,self.wIn,1,1]))\n\t\t\t\t\tself.boxSizes = tf.reshape(boxSizes, [-1,2])\n\n\t\t\t\t#scores\n\t\t\t\tself.scores = tf.reshape(scores, [-1,2])\n\n\tdef genAllAnchors(self):\n\t\twith tf.name_scope(\'genAllAnchors\'):\n\t\t\tz = tf.zeros([1, self.hIn, self.wIn, self.nAnchors], tf.float32)\n\t\t\treturn BoxUtils.nnToImageBoxes(z, z, z, z, self.wA, self.hA, self.inputDownscale, self.offset)\n\n\tdef loss(self, refBoxes):\n\t\tdef getPositiveBoxes(boxes):\n\t\t\twith tf.name_scope(\'getPositiveBoxes\'):\n\t\t\t\tiou = BoxUtils.iou(boxes, refBoxes)\n\t\t\t\t\n\t\t\t\tmaxIou = tf.reduce_max(iou, axis=1)\n\t\t\t\tbestIou = tf.expand_dims(tf.cast(tf.argmax(iou, axis=1), tf.int32), axis=-1)\n\n\t\t\t\tbestAnchors = tf.argmax(iou, axis=0)\n\t\t\t\t#Box matching matrix\n\t\t\t\tboxMatches = tf.cast(iou > self.positiveIouThreshold, tf.float32)\n\n\t\t\t\tboxMatches = tf.minimum(boxMatches + tf.transpose(tf.one_hot(bestAnchors, tf.shape(boxMatches)[0])), 1.0)\n\n\t\t\t\tboxMatchMatrix = tf.stop_gradient(boxMatches)\n\n\t\t\t\t#Find positive boxes\n\t\t\t\toneIfPositive = tf.reduce_max(boxMatchMatrix, axis=1)\n\t\t\t\toneIfPositive = tf.stop_gradient(oneIfPositive)\n\n\t\t\t\treturn oneIfPositive, maxIou, bestIou\n\n\n\t\tdef getPositiveLoss(boxes, rawSizes, boxSizes, positiveIndices, bestIou, classificationLoss):\n\t\t\twith tf.name_scope(\'getPositiveLoss\'):\n\t\t\t\tpositiveBoxes, positiveRawSizes, positiveBoxSizes, positiveRefIndices, positiveClassificationLoss = \\\n\t\t\t\t\tMultiGather.gather([boxes, rawSizes, boxSizes, bestIou, classificationLoss], positiveIndices)\n\t\t\t\n\t\t\t\t#Regression loss\n\t\t\t\tpositiveRefs = tf.gather_nd(refBoxes, positiveRefIndices)\n\t\t\t\treturn Loss.boxRegressionLoss(positiveBoxes, positiveRawSizes, positiveRefs, positiveBoxSizes)*self.regressionWeight  + positiveClassificationLoss\n\n\t\tdef emptyPositiveLoss():\n\t\t\twith tf.name_scope(\'emptyPositiveLoss\'):\n\t\t\t\treturn tf.zeros((0,),tf.float32)\n\n\t\tdef getNegativeLosses(boxes, negativeIndices, classificationLoss):\n\t\t\twith tf.name_scope(\'getNegativeLosses\'):\n\t\t\t\treturn tf.gather_nd(classificationLoss, negativeIndices)\n\t\t\t\n\t\tdef emptyNegativeLoss():\n\t\t\twith tf.name_scope(\'emptyNegativeLoss\'):\n\t\t\t\treturn tf.zeros((0,),tf.float32)\n\n\t\tdef calcAllLosses(refAnchors, boxes, rawSizes, scores, boxSizes):\n\t\t\twith tf.name_scope(\'calcAllRPNLosses\'):\n\t\t\t\toneIfPositive, maxIou, bestIou = getPositiveBoxes(refAnchors)\n\t\t\t\n\t\t\t\t#Classification loss\n\t\t\t\trefScores = tf.one_hot(tf.cast(oneIfPositive>0.5, tf.uint8), 2, on_value=0.999, off_value=0.001)\n\t\t\t\tclassificationLoss = tf.nn.softmax_cross_entropy_with_logits(logits=scores, labels=refScores)\n\n\t\t\t\t#Split to positive and negative\n\t\t\t\tpositiveIndices = tf.stop_gradient(tf.cast(tf.where(oneIfPositive >= 0.5), tf.int32))\n\t\t\t\tnegativeIndices = tf.stop_gradient(tf.cast(tf.where(tf.logical_and(oneIfPositive < 0.5, maxIou < self.negativeIouThreshold)), tf.int32))\n\n\t\t\t\tp = tf.cond(tf.shape(positiveIndices)[0]>0, lambda:getPositiveLoss(boxes, rawSizes, boxSizes, positiveIndices, bestIou, classificationLoss), lambda: emptyPositiveLoss())\n\t\t\t\tn = tf.cond(tf.shape(negativeIndices)[0]>0, lambda:getNegativeLosses(boxes, negativeIndices, classificationLoss), lambda: emptyNegativeLoss())\n\n\t\t\t\t#return positive losses, negative losses, positive boxes, positive reference indices, negative boxes\n\t\t\t\treturn p, n\n\n\t\tdef selectAndSum(losses, n):\n\t\t\twith tf.name_scope(\'selectAndSum\'):\n\t\t\t\tl = Utils.RandomSelect.randomSelectBatch(losses, n)\n\t\t\t\treturn tf.reduce_mean(l)\n\n\t\tdef calcLoss():\n\t\t\twith tf.name_scope(\'calcRPNLoss\'):\n\t\t\t\t#Filter cross boundary boxes and get positives\n\t\t\t\tinAnchros, inBoxes, inScores, inBoxSizes, inRawSizes = self.filterCrossBoundaryBoxes(self.genAllAnchors(), [self.boxes, self.scores, self.boxSizes, self.rawSizes])\n\t\t\t\tpositiveLosses, negativeLosses = calcAllLosses(inAnchros, inBoxes, inRawSizes, inScores, inBoxSizes)\n\n\t\t\t\tpCount = tf.shape(positiveLosses)[0]\n\t\t\t\tnCount = tf.shape(negativeLosses)[0]\n\n\t\t\t\tnPositive = tf.minimum(pCount, self.nPositiveLosses)\n\t\t\t\tnNegative = tf.minimum(self.nBoxLosses - nPositive, nCount)\n\t\t\t\t\n\t\t\t\tn = tf.cond(nNegative > 0, lambda: selectAndSum(negativeLosses, nNegative), lambda: tf.constant(0.0))\n\t\t\t\tp = tf.cond(nPositive > 0, lambda: selectAndSum(positiveLosses, nPositive), lambda: tf.constant(0.0))\n\t\t\t\treturn n+p\n\t\t\n\t\twith tf.name_scope(\'RPNloss\'):\n\t\t\treturn tf.cond(tf.shape(refBoxes)[0]>0, lambda: calcLoss(), lambda: tf.constant(0.0))\n\t\t\n\tdef getInsideMask(self, boxes, boxInsideRate=1.0):\n\t\twith tf.name_scope(\'getInsideMask\'):\n\t\t\tx0, y0, x1, y1 = tf.unstack(boxes, axis=1)\n\t\t\t\n\t\t\tif boxInsideRate!=1.0:\n\t\t\t\tw = x1-x0+1.0\n\t\t\t\th = y1-y0+1.0\n\n\t\t\t\txOutside = (1.0 - boxInsideRate) * w\n\t\t\t\tyOutside = (1.0 - boxInsideRate) * h\n\t\t\telse:\n\t\t\t\txOutside = 0\n\t\t\t\tyOutside = 0\n\t\t\n\t\t\treturn tf.logical_and(tf.logical_and(x0 >= -xOutside, y0 > -yOutside), tf.logical_and(x1 < (tf.cast(self.imageW, tf.float32)+xOutside), y1 < (tf.cast(self.imageH, tf.float32)+yOutside)))\n\t\n\tdef filterCrossBoundaryBoxes(self, boxes, others=[], boxInsideRate=1.0):\n\t\twith tf.name_scope(\'filterCrossBoundaryBoxes\'):\n\t\t\tokIndices = tf.where(self.getInsideMask(boxes, boxInsideRate))\n\t\t\tokIndices = tf.cast(okIndices, tf.int32)\n\n\t\t\treturn MultiGather.gather([boxes]+others, okIndices)\n\n\tdef clipBoxesToEdge(self, boxes):\n\t\twith tf.name_scope(""clipBoxesToEdge""):\n\t\t\tx0,y0,x1,y1 = tf.unstack(boxes, axis=1)\n\t\t\tx0 = tf.maximum(tf.minimum(x0, self.imageW), 0.0)\n\t\t\tx1 = tf.maximum(tf.minimum(x1, self.imageW), 0.0)\n\t\t\ty0 = tf.maximum(tf.minimum(y0, self.imageH), 0.0)\n\t\t\ty1 = tf.maximum(tf.minimum(y1, self.imageH), 0.0)\n\t\t\treturn tf.stack([x0,y0,x1,y1], axis=1)\n\t\t\n\n\tdef filterOutputBoxes(self, boxes, scores, others=[], preNmsCount=6000, maxOutSize=300, nmsThreshold=0.7): \n\t\twith tf.name_scope(""filter_output_boxes""):\n\t\t\tscores = tf.nn.softmax(scores)[:,1]\n\t\t\tscores = tf.reshape(scores,[-1])\n\n\t\t\t#Clip boxes to edge\n\t\t\tboxes = self.clipBoxesToEdge(boxes)\n\n\t\t\t#Remove empty boxes\n\t\t\tboxes, scores = BoxUtils.filterSmallBoxes(boxes, [scores])\n\t\t\tscores, boxes = tf.cond(tf.shape(scores)[0] > preNmsCount , lambda: tf.tuple(MultiGather.gatherTopK(scores, preNmsCount, [boxes])), lambda: tf.tuple([scores, boxes]))\n\n\t\t\t#NMS filter\n\t\t\tnmsIndices = tf.image.non_max_suppression(boxes, scores, iou_threshold=nmsThreshold, max_output_size=maxOutSize)\n\t\t\tnmsIndices = tf.expand_dims(nmsIndices, axis=-1)\n\n\t\t\treturn MultiGather.gather([boxes, scores]+others, nmsIndices)\n\t\t\n\tdef getPositiveOutputs(self, preNmsCount=6000, maxOutSize=300, nmsThreshold=0.7):\n\t\tboxes, scores = self.filterOutputBoxes(self.boxes, self.scores, preNmsCount=preNmsCount, nmsThreshold=nmsThreshold, maxOutSize=maxOutSize)\n\t\treturn boxes, scores\n\n\t@staticmethod\n\tdef makeAnchors(sizeList, sizeLim=[1024, 1024]):\n\t\tres = []\n\t\tfor s in sizeList:\n\t\t\tres.append([s,s])\n\t\t\tif s*2 <= sizeLim[0]:\n\t\t\t\tres.append([int(s*math.sqrt(2)), int(s/math.sqrt(2))])\n\t\t\tif s*2 <= sizeLim[1]:\n\t\t\t\tres.append([int(s/math.sqrt(2)), int(s*math.sqrt(2))])\n\n\t\treturn res\n\t'"
BoxEngine/__init__.py,0,b'from BoxEngine.BoxNetwork import *\n'
Dataset/Augment.py,16,"b'# Copyright 2017 Robert Csordas. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# ==============================================================================\n\nimport tensorflow as tf\nimport threading\n\ndef nonlinear(imageList, lower, upper):\n\twith tf.name_scope(\'nonlinear\') as scope:\n\t\tfactor = tf.random_uniform([], lower, upper)\n\n\t\tres=[]\n\t\tfor i in imageList:\n\t\t\tres.append(tf.pow(i, factor))\n\n\t\treturn res\n\ndef randomNormal(imageList, stddev):\n\twith tf.name_scope(\'randomNormal\') as scope:\n\t\tfactor = tf.random_uniform([], 0, stddev)\n\n\t\tres=[]\n\t\tfor i in imageList:\n\t\t\tres.append(i+tf.random_normal(tf.shape(i), mean=0.0, stddev=factor))\n\n\t\treturn res\n\ndef mirror(image, boxes):\n\tdef doMirror(image, boxes):\n\t\timage = tf.reverse(image, axis=[2])\n\t\tx0,y0,x1,y1 = tf.unstack(boxes, axis=1)\n\n\t\tw=tf.cast(tf.shape(image)[2], tf.float32)\n\t\tx0_m=w-x1\n\t\tx1_m=w-x0\n\n\t\treturn image, tf.stack([x0_m,y0,x1_m,y1], axis=1)\n\t\t\t\n\tuniform_random = tf.random_uniform([], 0, 1.0)\n\treturn tf.cond(uniform_random < 0.5, lambda: (image, boxes), lambda: doMirror(image, boxes))\n\ndef augment(image, boxes, classes):\n\twith tf.name_scope(\'augmentation\') as scope:\n\t\timage=image/255.0\n\t\timage = nonlinear([image], 0.8, 1.2)[0]\n\n\t\timage, boxes = mirror(image, boxes)\n\n\t\timage = tf.image.random_contrast(image, lower=0.3, upper=1.3)\n\t\timage = tf.image.random_brightness(image, max_delta=0.3)\n\n\t\timage = randomNormal([image], 0.025)[0]\n\n\t\timage = tf.clip_by_value(image, 0, 1.0)*255\n\n\t\treturn image, boxes, classes\n'"
Dataset/BoxAwareRandZoom.py,0,"b'# Copyright 2017 Robert Csordas. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# ==============================================================================\n\nimport cv2\nimport random\nimport numpy as np\n\ndef randZoom(img, boxes, minImageRatio=0.6, minBoxRatio=1.0, keepOriginalRatio=True, keepOriginalSize=True, keepBoxes=False):\n\th=img.shape[0]\n\tw=img.shape[1]\n\n\tdef sampleStaringPoint(dim, boxPos, boxDim):\n\t\tmaxVal = np.min([\n\t\t\tint(dim *(1.0 - minImageRatio)),\n\t\t\tint(boxPos+(1.0 - minBoxRatio)*boxDim)\n\t\t])\n\n\t\treturn random.randint(0, maxVal)\n\n\tdef sampleEndPoint(startPoint, dim, boxPos, boxDim):\n\t\tminBoxSize = int(boxDim * minBoxRatio)\n\t\tminImageSize = int(dim * minImageRatio)\n\t\tminEndPos = np.max([\n\t\t\tstartPoint + minBoxSize + (0 if startPoint >= boxPos else boxPos - startPoint),\n\t\t\tstartPoint + minImageSize\n\t\t])\n\n\t\treturn minEndPos + random.randint(0, dim - minEndPos)\n\n\n\tdef growBox(left, top, right, bottom):\n\t\tnewW = right - left + 1\n\t\tnewH = bottom - top + 1\n\n\t\taspectRatio = w/h\n\n\t\tif int(newH*aspectRatio) > newW:\n\t\t\t#Need to grow in X direction\n\t\t\tnewW2 = int(newH*aspectRatio)\n\t\t\tassert(newW2 > newW)\n\n\t\t\tminLeft = np.max([right - newW2, 0])\n\t\t\tmaxDiff = np.min([\n\t\t\t\tleft - minLeft,\n\t\t\t\tw - right\n\t\t\t])\n\n\t\t\tleft = minLeft + random.randint(0, maxDiff)\n\t\t\tright = left + newW2\n\n\t\telif int(newW/aspectRatio) >= newH:\n\t\t\t#Need to grow in Y direction\n\t\t\tnewH2 = int(newW/aspectRatio)\n\t\t\t\n\t\t\tassert(newH2 >= newH)\n\t\t\tminTop = np.max([bottom - newH2, 0])\n\n\t\t\tmaxDiff = np.min([\n\t\t\t\ttop - minTop,\n\t\t\t\th - bottom\n\t\t\t])\n\t\t\t\n\t\t\ttop = minTop + random.randint(0, maxDiff)\n\t\t\tbottom = top + newH2\n\n\t\treturn left, top, right, bottom\n\n\tdef sampleNoBox():\n\t\tleft = random.randint(0, int((1.0-minImageRatio)*w))\n\t\tright = w-random.randint(0, int(w - left - minImageRatio*w))\n\n\t\ttop = random.randint(0, int((1.0-minImageRatio)*h))\n\t\tbottom = h-random.randint(0, int(h - top - minImageRatio*h))\n\n\t\treturn left, top, right, bottom\n\n\tdef limitBoxSize(box):\n\t\tx=np.min([np.max([box[""x""], 0]),w-2])\n\t\ty=np.min([np.max([box[""y""], 0]),h-2])\n\n\t\tbw = np.min([box[""w""] + np.min([box[""x""],0]), w-x])\n\t\tbw = np.max([bw, 1])\n\t\tbh = np.min([box[""h""] + np.min([box[""y""],0]), h-y])\n\t\tbh = np.max([bh, 1])\n\t\treturn {\n\t\t\t""x"": x,\n\t\t\t""y"": y,\n\t\t\t""w"": bw,\n\t\t\t""h"": bh\n\t\t}\n\n\tdef checkBox(left, top, right, bottom, box):\n\t\tif box is None:\n\t\t\treturn\n\n\t\tcL = np.max([left, box[""x""]])\n\t\tcR = np.min([right, box[""x""]+box[""w""]])\n\n\t\tcT = np.max([top, box[""y""]])\n\t\tcB = np.min([bottom, box[""y""]+box[""h""]])\n\n\t\tif (cR - cL) < int(minBoxRatio*box[""w""]) or (cB - cT) < int(minBoxRatio*box[""h""]):\n\t\t\tprint(""box:"",box)\n\t\t\tprint(""Remaining W: "", cR - cL, ""minW:"", minBoxRatio*box[""w""])\n\t\t\tprint(""Remaining H: "", cB - cT, ""minH:"", minBoxRatio*box[""h""])\n\t\t\tassert(False)\n\t\tpass\n\n\tdef filterBoxes(top, bottom, left, right, zoom):\n\t\tnewBoxes = []\n\t\troiW = right - left\n\t\troiH = bottom - top\n\t\tfor b in boxes:\n\t\t\tif b[""x""] >= right or (b[""x""]+b[""w""]) <= left or b[""y""] >= bottom or (b[""y""]+b[""h""]) <= top:\n\t\t\t\tif keepBoxes:\n\t\t\t\t\tnewBoxes.append({""x"":0,""y"":0,""w"":0,""h"":0})\n\t\t\t\tcontinue\n\n\t\t\tnewBox = b.copy()\n\t\t\tnewBox[""x""] -= left\n\t\t\tif newBox[""x""] < 0:\n\t\t\t\tnewBox[""w""] += newBox[""x""]\n\t\t\t\tnewBox[""x""] = 0\n\n\t\t\tnewBox[""y""] -= top\n\t\t\tif newBox[""y""] < 0:\n\t\t\t\tnewBox[""h""] += newBox[""y""]\n\t\t\t\tnewBox[""y""] = 0\n\n\t\t\tif (newBox[""w""] + newBox[""x""]) > roiW:\n\t\t\t\tnewBox[""w""] = roiW - newBox[""x""]\n\n\t\t\tif (newBox[""h""] + newBox[""y""]) > roiH:\n\t\t\t\tnewBox[""h""] = roiH - newBox[""y""]\n\n\t\t\tnewBox[""x""]=int( float(newBox[""x""]) * zoom )\n\t\t\tnewBox[""y""]=int( float(newBox[""y""]) * zoom )\n\t\t\tnewBox[""w""]=int( float(newBox[""w""]) * zoom )\n\t\t\tnewBox[""h""]=int( float(newBox[""h""]) * zoom )\n\n\t\t\tnewBoxes.append(newBox)\n\n\t\treturn newBoxes\n\n\tif len(boxes)>1:\n\t\tbox = boxes[random.randint(0, len(boxes)-1)]\n\telif len(boxes)==1:\n\t\tbox = boxes[0]\n\telse:\n\t\tbox = None\n\n\tif box is not None:\n\t\tbox = limitBoxSize(box)\n\t\tleft = sampleStaringPoint(w, box[""x""], box[""w""])\n\t\ttop = sampleStaringPoint(h, box[""y""], box[""h""])\n\n\t\tright = sampleEndPoint(left, w, box[""x""], box[""w""])\n\t\tbottom = sampleEndPoint(top, h, box[""y""], box[""h""])\n\telse:\n\t\tleft, top, right, bottom = sampleNoBox()\n\n\t#checkBox(left, top, right, bottom, box)\n\n\tif keepOriginalRatio:\n\t\tleft, top, right, bottom = growBox(left, top, right, bottom)\n\t\n\t#checkBox(left, top, right, bottom, box)\n\n\timg = img[top:bottom, left:right]\n\n\tif keepOriginalSize:\n\t\tzoom = (float(w)) / img.shape[1]\n\t\timg = cv2.resize(img, (w,h), cv2.INTER_CUBIC)\n\telse:\n\t\tzoom = 1\n\n\treturn img, filterBoxes(top, bottom, left, right, zoom)'"
Dataset/BoxLoader.py,7,"b'# Copyright 2017 Robert Csordas. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# ==============================================================================\n\n\nimport Dataset.coco.pycocotools.coco as coco\nimport random\nimport tensorflow as tf\nimport threading\nimport time\nimport numpy as np\n\nclass BoxLoader:\n\tQUEUE_CAPACITY=16\n\n\tdef __init__(self, sources=[], initOnStart=True):\n\t\tself.totalCount = 0\n\t\tself.counts=[]\n\t\tself.sources=[]\n\t\tself.initDone=False\n\t\tself.initOnStart=initOnStart\n\n\t\twith tf.name_scope(\'dataset\') as scope:\n\t\t\tself.queue = tf.FIFOQueue(dtypes=[tf.float32, tf.float32, tf.uint8],\n\t\t\t\t\tcapacity=self.QUEUE_CAPACITY)\n\n\t\t\tself.image = tf.placeholder(dtype=tf.float32, shape=[None, None, 3], name=""image"")\n\t\t\tself.boxes = tf.placeholder(dtype=tf.float32, shape=[None,4], name=""boxes"")\n\t\t\tself.classes = tf.placeholder(dtype=tf.uint8, shape=[None], name=""classes"")\n\t\t\t\n\t\t\tself.enqueueOp = self.queue.enqueue([self.image, self.boxes, self.classes])\n\n\t\tself.sources=sources[:]\n\t\n\tdef categoryCount(self):\n\t\treturn 80\n\t\n\tdef threadFn(self, tid, sess):\n\t\tif tid==0:\n\t\t\tself.init()\n\t\telse:\n\t\t\twhile not self.initDone:\n\t\t\t\ttime.sleep(1)\n\n\t\twhile True:\n\t\t\timg, boxes, classes=self.selectSource().load()\n\t\t\ttry:\n\t\t\t\tsess.run(self.enqueueOp,feed_dict={self.image:img, self.boxes:boxes, self.classes:classes})\n\t\t\texcept tf.errors.CancelledError:\n\t\t\t\treturn\n\n\tdef init(self):\n\t\tif not self.initOnStart:\n\t\t\tfor s in self.sources:\n\t\t\t\ts.init()\n\n\t\tfor s in self.sources:\n\t\t\tc = s.count()\n\t\t\tself.counts.append(c)\n\t\t\tself.totalCount+=c\n\n\t\tprint(""BoxLoader: Loaded %d files."" % self.totalCount)\n\t\tself.initDone=True\n\n\tdef startThreads(self, sess, nThreads=4):\n\t\tself.threads=[]\n\t\tfor n in range(nThreads):\n\t\t\tt=threading.Thread(target=self.threadFn, args=(n,sess))\n\t\t\tt.daemon = True\n\t\t\tt.start()\n\t\t\tself.threads.append(t)\n\n\tdef get(self):\n\t\timages, boxes, classes = self.queue.dequeue()\n\t\timages = tf.expand_dims(images, axis=0)\n\n\t\timages.set_shape([None, None, None, 3])\n\t\tboxes.set_shape([None,4])\n\n\t\treturn images, boxes, classes\n\n\tdef add(self, source):\n\t\tassert self.initDone==False\n\t\tif self.initOnStart:\n\t\t\tsource.init()\n\t\tself.sources.append(source)\n\n\tdef selectSource(self):\n\t\ti = random.randint(0, self.totalCount-1)\n\t\tacc = 0\n\t\tfor j in range(len(self.counts)):\n\t\t\tacc += self.counts[j]\n\t\t\tif acc >= i:\n\t\t\t\treturn self.sources[j]\n\n\tdef count(self):\n\t\treturn self.totalCount\n\n\tdef getCaptions(self, categories):\n\t\treturn self.sources[0].getCaptions(categories)\n\n\tdef getCaptionMap(self):\n\t\treturn self.getCaptions(np.arange(0,self.categoryCount()))'"
Dataset/CocoDataset.py,0,"b'# Copyright 2017 Robert Csordas. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# ==============================================================================\n\nfrom .coco.pycocotools import coco\nimport random\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nfrom . import BoxAwareRandZoom\n\nclass CocoDataset:\n\tdef __init__(self, path, set=""train2017"", normalizeSize=True, randomZoom=True):\n\t\tprint(path)\n\t\tself.path=path\n\t\tself.coco=None\n\t\tself.normalizeSize=normalizeSize\n\t\tself.set=set\n\t\tself.randomZoom=randomZoom\n\n\tdef init(self):\n\t\tself.coco=coco.COCO(self.path+""/annotations/instances_""+self.set+"".json"")\n\t\tself.images=self.coco.getImgIds()\n\n\t\tself.toCocoCategory=[]\n\t\tself.fromCocoCategory={}\n\n\t\tcats = self.coco.dataset[\'categories\']\n\t\tfor i in range(len(cats)):\n\t\t\tself.fromCocoCategory[cats[i][""id""]] = i\n\t\t\tself.toCocoCategory.append(cats[i][""id""])\n\n\t\tprint(""Loaded ""+str(len(self.images))+"" COCO images"")\n\t\n  \n\tdef getCaptions(self, categories):\n\t\tif categories is None:\n\t\t\treturn None\n\n\t\tres = []\n\t\tif isinstance(categories, np.ndarray):\n\t\t\tcategories = categories.tolist()\n\n\t\tfor c in categories:\n\t\t\tres.append(self.coco.cats[self.toCocoCategory[c]][""name""])\n\n\t\treturn res\n\n\tdef load(self):\n\t\twhile True:\n\t\t\t#imgId=self.images[1]\n\t\t\t#imgId=self.images[3456]\n\t\t\timgId=self.images[random.randint(0, len(self.images)-1)]\n\t  \n\t\t\tinstances = self.coco.loadAnns(self.coco.getAnnIds(imgId, iscrowd=False))\n\t\t\n\t\t\t#Ignore crowd images\n\t\t\tcrowd = self.coco.loadAnns(self.coco.getAnnIds(imgId, iscrowd=True))\n\t\t\tif len(crowd)>0:\n\t\t\t\tcontinue;\n\n\t\t\timgFile=self.path+""/""+self.set+""/""+self.coco.loadImgs(imgId)[0][""file_name""]\n\t\t\timg = cv2.imread(imgFile)\n\n\t\t\tif img is None:\n\t\t\t\tprint(""ERROR: Failed to load ""+imgFile)\n\t\t\t\tcontinue\n\n\t\t\tsizeMul = 1.0\n\t\t\tpadTop = 0\n\t\t\tpadLeft = 0\n\n\t\t\tif len(instances)<=0:\n\t\t\t\tcontinue\n\n\t\t\tiBoxes=[{\n\t\t\t\t\t\t""x"":int(i[""bbox""][0]),\n\t\t\t\t\t\t""y"":int(i[""bbox""][1]),\n\t\t\t\t\t\t""w"":int(i[""bbox""][2]),\n\t\t\t\t\t\t""h"":int(i[""bbox""][3])\n\t\t\t\t\t} for i in instances]\n\t\n\t\t\tif self.randomZoom:\n\t\t\t\timg, iBoxes = BoxAwareRandZoom.randZoom(img, iBoxes, keepOriginalRatio=False, keepOriginalSize=False, keepBoxes=True)\n\n\t\t\tif self.normalizeSize:\n\t\t\t\tsizeMul = 640.0 / min(img.shape[0], img.shape[1])\n\t\t\t\timg = cv2.resize(img, (int(img.shape[1]*sizeMul), int(img.shape[0]*sizeMul)))\n\n\t\t\tm = img.shape[1] % 32\n\t\t\tif m != 0:\n\t\t\t\tpadLeft = int(m/2)\n\t\t\t\timg = img[:,padLeft : padLeft + img.shape[1] - m]\n\n\t\t\tm = img.shape[0] % 32\n\t\t\tif m != 0:\n\t\t\t\tm = img.shape[0] % 32\n\t\t\t\tpadTop = int(m/2)\n\t\t\t\timg = img[padTop : padTop + img.shape[0] - m]\n\n\t\t\tif img.shape[0]<256 or img.shape[1]<256:\n\t\t\t\tprint(""Warning: Image to small, skipping: ""+str(img.shape))\n\t\t\t\tcontinue\n\n\t\t\tboxes=[]\n\t\t\tcategories=[]\n\t\t\tfor i in range(len(instances)):\n\t\t\t\tx1,y1,w,h = iBoxes[i][""x""],iBoxes[i][""y""],iBoxes[i][""w""],iBoxes[i][""h""]\n\t\t\t\tnewBox=[int(x1*sizeMul) - padLeft, int(y1*sizeMul) - padTop, int((x1+w)*sizeMul) - padLeft, int((y1+h)*sizeMul) - padTop]\n\t\t\t\tnewBox[0] = max(min(newBox[0], img.shape[1]),0)\n\t\t\t\tnewBox[1] = max(min(newBox[1], img.shape[0]),0)\n\t\t\t\tnewBox[2] = max(min(newBox[2], img.shape[1]),0)\n\t\t\t\tnewBox[3] = max(min(newBox[3], img.shape[0]),0)\n\n\t\t\t\tif (newBox[2]-newBox[0]) >= 16 and (newBox[3]-newBox[1]) >= 16:\n\t\t\t\t\tboxes.append(newBox)\n\t\t\t\t\tcategories.append(self.fromCocoCategory[instances[i][""category_id""]])\n\n\t\t\tif len(boxes)==0:\n\t\t\t\tprint(""Warning: No boxes on image. Skipping."")\n\t\t\t\tcontinue;\n\n\t\t\tboxes=np.array(boxes, dtype=np.float32)\n\t\t\tboxes=np.reshape(boxes, [-1,4])\n\t\t\tcategories=np.array(categories, dtype=np.uint8)\n\n\t\t\treturn img, boxes, categories\n\n\tdef count(self):\n\t\treturn len(self.images)\n'"
Dataset/__init__.py,0,b'from BoxEngine.BoxNetwork import *\n'
Utils/ArgSave.py,0,"b'# Copyright 2017 Robert Csordas. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# ==============================================================================\n\nimport os\nimport json\nimport argparse\n\nclass StorableArgparse:\n\tdef __init__(self, description):\n\t\tself.parser = argparse.ArgumentParser(description=description)\n\t\tself.loaded={}\n\t\tself.args={}\n\t\tself.parsed=None\n\n\tdef add_argument(self, name, type, default=None, help="""", save=True):\n\t\tself.parser.add_argument(name, type=type, default=None, help=help)\n\t\tif name[0]==\'-\':\n\t\t\tname = name[1:]\n\n\t\tself.args[name]={\n\t\t\t""type"": type,\n\t\t\t""default"": default,\n\t\t\t""save"": save\n\t\t}\n\n\tdef do_parse_args(self, loaded={}):\n\t\tself.parsed=self.parser.parse_args()\n\t\tfor k, v in self.parsed.__dict__.items():\n\t\t\tif v is None:\n\t\t\t\tif k in loaded and self.args[k][""save""]:\n\t\t\t\t\tself.parsed.__dict__[k] = loaded[k]\n\t\t\t\telse:\n\t\t\t\t\tself.parsed.__dict__[k] = self.args[k][""default""]\n\t\treturn self.parsed\n\n\tdef parse_or_cache(self):\n\t\tif self.parsed is None:\n\t\t\tself.do_parse_args()\n\n\tdef parse_args(self):\n\t\tself.parse_or_cache()\n\t\treturn self.parsed\n\n\tdef save(self, fname):\n\t\tself.parse_or_cache()\n\t\twith open(fname, \'w\') as outfile:\n\t\t\tjson.dump(self.parsed.__dict__, outfile, indent=4)\n\t\t\treturn True\n\n\tdef load(self, fname):\n\t\tif os.path.isfile(fname):\n\t\t\tmap={}\n\t\t\twith open(fname,""r"") as data_file:\n\t\t\t\tmap=json.load(data_file)\n\n\t\t\tself.do_parse_args(map)\n\t\treturn self.parsed'"
Utils/CheckpointLoader.py,12,"b'# Copyright 2017 Robert Csordas. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# ==============================================================================\n\nimport tensorflow as tf\n\ndef getCheckpointVarList(file):\n\treader = tf.contrib.framework.load_checkpoint(file)\n\tvarsToRead = []\n\tloadedVars = []\n\tfor v in reader.get_variable_to_shape_map().keys():\n\t\ttfVar = tf.contrib.slim.get_variables_by_name(v)\n\t\ttfVarFitlered=[]\n\t\tfor var in tfVar:\n\t\t\tif var.op.name==v:\n\t\t\t\ttfVarFitlered.append(var)\n \n\t\tif len(tfVarFitlered)==0:\n\t\t\tcontinue\n \n\t\tvarsToRead += tfVarFitlered\n\t\tloadedVars.append(v)\n\t\n\tdel reader\n \n\treturn varsToRead, loadedVars\n\ndef loadVarsFromCheckpoint(sess, vars, file):\n\trestorer=tf.train.Saver(var_list = vars)\n\trestorer.restore(sess, file)\n\tdel restorer\n\ndef loadExitingFromCheckpoint(file, sess):\n\tvarsToRead, loadedVars = getCheckpointVarList(file)\n\tloadVarsFromCheckpoint(sess, varsToRead, file)\n\treturn loadedVars\n\ndef loadCheckpoint(sess, saveDir, resume, ignoreVarsInFileNotInSess=False):\n\tdef initGlobalVars():\n\t\tif ""global_variables_initializer"" in tf.__dict__:\n\t\t\tsess.run(tf.global_variables_initializer())\n\t\telse:\n\t\t\tsess.run(tf.initialize_all_variables())\n\n\tfirstError = True\n\twith tf.name_scope(\'checkpoint_load\') as scope:\n\t\tif resume is not None:\n\t\t\tlast=resume\n\t\telse:\n\t\t\tlast=tf.train.latest_checkpoint(saveDir)\n\n\t\tif last is not None:\n\t\t\tprint(""Resuming ""+last)\n\n\t\t\tvarsToRead, loadedVars = getCheckpointVarList(last)\n\t\t\t\n\t\t\tallVars=tf.global_variables()\n\t\t\tallVars = [v.op.name for v in allVars]\n\n\t\t\tallRestored = True\n\t\t\tfor v in allVars:\n\t\t\t\tif v not in loadedVars:\n\t\t\t\t\tif firstError:\n\t\t\t\t\t\tprint(""!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"")\n\t\t\t\t\t\tfirstError = False\n\t\t\t\t\tprint(""   WARNING: Not loaded: ""+v)\n\t\t\t\t\tallRestored = False\n\n\t\t\tif not ignoreVarsInFileNotInSess:\n\t\t\t\tfor v in loadedVars:\n\t\t\t\t\tif v not in allVars:\n\t\t\t\t\t\tif firstError:\n\t\t\t\t\t\t\tprint(""!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"")\n\t\t\t\t\t\t\tfirstError = False\n\t\t\t\t\t\tprint(""   WARNING: Variable doesn\'t exists: ""+v)\n\n\t\t\tif not allRestored:\n\t\t\t\tprint(""Missing variable found. Initializing variables first."")\n\t\t\t\tinitGlobalVars()\n\n\t\t\tloadVarsFromCheckpoint(sess, varsToRead, last)\n\t\t\treturn True\n\t\telse:\n\t\t\tprint(""Checkpoint not found. Initializing variables."")\n\t\t\tinitGlobalVars()\n\t\t\treturn False\n\ndef importIntoScope(sess, file, fromScope=None, toScope=None, collection=tf.GraphKeys.GLOBAL_VARIABLES, ignore=[]):\n\ttoRun=[]\n\treader = tf.contrib.framework.load_checkpoint(file)\n\n\tvlist=tf.get_collection(collection, scope=toScope)\n\tknownNames = reader.get_variable_to_shape_map().keys()\n\tloaded = []\n\tvarsToLoad = {}\n\n\tfor va in vlist:\n\t\tif fromScope is not None and toScope is not None:\n\t\t\tname = va.op.name.replace(toScope+""/"", fromScope+""/"", 1)\n\t\telse:\n\t\t\tname = va.op.name\n\n\t\tif name not in knownNames:\n\t\t\tprint(""WARNING: Variable \\""""+name+""\\"" not found in file to load."")\n\t\t\tcontinue\n\n\t\tignored = False\n\t\tfor i in ignore:\n\t\t\tif va.op.name.startswith(i) or name.startswith(i):\n\t\t\t\tprint(""WARNING: ignoring loading of variable \\""""+name+""\\"""")\n\t\t\t\tignored = True\n\t\t\t\tbreak\n\n\t\tif not ignored:\n\t\t\tloaded.append(name)\n\t\t\tvarsToLoad[name] = va\n\n\tfor name in knownNames:\n\t\tif name not in loaded:\n\t\t\tprint(""WARNING: Unused variable: ""+name)\n\n\tloadVarsFromCheckpoint(sess, varsToLoad, file)\n'"
Utils/Export.py,2,"b'import tensorflow as tf\nimport re\n\ndef exportModel(sess, filename, filters=[\'.*\']):\n    def matchName(name):\n        for f in filters:\n            if isinstance(f, str):\n                if re.match(""^""+f+""$"", name):\n                    return True\n            else:\n                if f(name):\n                    return True\n                    \n        return False\n\n    vars = []\n    print(""Exporting..."")\n    for v in tf.get_collection(tf.GraphKeys.VARIABLES):\n        if matchName(v.op.name):\n            print(""   ""+v.op.name)\n            vars.append(v)\n    print(""Done."")\n        \n    saver = tf.train.Saver(var_list=vars)\n    saver.save(sess, filename, write_meta_graph=False)'"
Utils/Model.py,0,"b'import os\n\nURL=""http://download.tensorflow.org/models/inception_resnet_v2_2016_08_30.tar.gz""\nFILENAME=""./inception_resnet_v2_2016_08_30.ckpt""\n\ndef download():\n\tif not os.path.isfile(FILENAME):\n\t\tprint(""Checkpoint file doesn\'t exists. Downloading it from tensorflow slim model list."")\n\t\timport requests, tarfile, io\n\t\trequest = requests.get(URL)\n\t\tdecompressedFile = tarfile.open(fileobj=io.BytesIO(request.content), mode=\'r|gz\')\n\t\tdecompressedFile.extractall()\n\t\tprint(""Done."")\n'"
Utils/MultiGather.py,7,"b'# Copyright 2017 Robert Csordas. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# ==============================================================================\n\nimport tensorflow as tf \n\ndef gather(tensors, indices):\n    with tf.name_scope(""multiGather""):\n        res = []\n        for a in tensors:\n            res.append(tf.gather_nd(a, indices))\n        return res\n\ndef gatherTopK(t, k, others=[], sorted=False):\n    res=[]\n    with tf.name_scope(""gather_top_k""):\n        isMoreThanK = tf.shape(t)[-1]>k\n        values, indices = tf.cond(isMoreThanK, lambda: tuple(tf.nn.top_k(t, k=k, sorted=sorted)), lambda: (t, tf.zeros((0,1), tf.int32)))\n        indices = tf.reshape(indices, [-1,1])\n        res.append(values)\n\n        for o in others:\n            res.append(tf.cond(isMoreThanK, lambda: tf.gather_nd(o, indices), lambda: o))\n\n    return res\n'"
Utils/PreviewIO.py,0,"b'import os\nimport cv2\nimport glob\nimport sys\n\nclass PreviewInput:\n\tIMG = 0\n\tDIR = 1\n\tVID = 2\n\tNONE = 3\n\t\n\tdef __init__(self, path):\n\t\tself.path = path\n\t\tself.fps = 30\n\t\tself.currName=""unknown""\n\n\t\tif os.path.isdir(self.path):\n\t\t\tself.type=self.DIR\n\t\t\tself.files = glob.glob(self.path+\'/*.*\')\n\t\t\tself.currFile = 0\n\t\telif self.path.split(\'.\')[-1].lower() in [\'avi\', \'mp4\', \'mpeg\', ""mov""]:\n\t\t\tself.cap = cv2.VideoCapture(opt.i)\n\t\t\tself.frameIndex = 0\n\t\t\tself.type=self.VID\n\t\t\tif int((cv2.__version__).split(\'.\')[0]) < 3:\n\t\t\t\tself.fps = cap.get(cv2.cv.CV_CAP_PROP_FPS)\n\t\t\telse:\n\t\t\t\tself.fps = cap.get(cv2.CAP_PROP_FPS)\n \n\t\t\tif self.fps<1:\n\t\t\t\tself.fps=1\n\t\telif self.path.split(\'.\')[-1].lower() in [\'png\',\'bmp\',\'jpg\',\'jpeg\']:\n\t\t\tself.type=self.IMG\n\t\t\tself.fps=0\n\t\telse:\n\t\t\tprint(""Invalid file: ""+self.path)\n\t\t\tsys.exit(-1)\n\n\tdef get(self):\n\t\tif self.type==self.DIR:\n\t\t\twhile True:\n\t\t\t\tif self.currFile >= len(self.files):\n\t\t\t\t\treturn None\n \n\t\t\t\tf = self.files[self.currFile]\n\t\t\t\tself.currFile+=1\n\n\t\t\t\tif f.split(\'.\')[-1].lower() not in [\'png\',\'bmp\',\'jpg\',\'jpeg\']:\n\t\t\t\t\tprint(""Unknown file: ""+f)\n\t\t\t\t\tcontinue\n\n\t\t\t\tself.currName = f.split(""/"")[-1]\n\t\t\t\t\n\t\t\t\timg = cv2.imread(f)\n\t\t\t\tif img is None:\n\t\t\t\t\tprint(""Failed to load image: ""+f)\n\t\t\t\t\tcontinue\n\n\t\t\t\treturn img\n\t\telif self.type==self.IMG:\n\t\t\tself.type=self.NONE\n\t\t\tself.currName = self.path.split(""/"")[-1]\n\t\t\treturn cv2.imread(self.path)\n\t\telif self.type==self.VID:\n\t\t\tret, frame = self.cap.read()\n\t\t\tif ret==False:\n\t\t\t\treturn None\n\n\t\t\tself.currName=""frame%.6d.jpg"" % self.frameIndex\n\t\t\tself.frameIndex += 1\n\t\t\treturn frame\n\t\telse:\n\t\t\treturn None\n\n\tdef getFps(self):\n\t\treturn self.fps\n\n\tdef getDelay(self):\n\t\tif self.fps==0:\n\t\t\treturn 0\n\t\telse:\n\t\t\treturn int(1.0/self.fps)\n\n\tdef getName(self):\n\t\treturn self.currName\n\nclass PreviewOutput:\n\tIMG = 0\n\tDIR = 1\n\tVID = 2\n\tNONE = 3\n\n\tdef __init__(self, path, fps=30):\n\t\tself.path=path\n\t\tself.fps=fps\n\t\tif path is None or path=="""":\n\t\t\tself.type=self.NONE\n\t\telif os.path.isdir(self.path):\n\t\t\tself.type=self.DIR\n\t\telif self.path.split(\'.\')[-1].lower() in [\'avi\', \'mp4\', \'mpeg\', ""mov""]:\n\t\t\tself.type=self.VID\n\t\t\tself.writer=None\n\t\telif self.path.split(\'.\')[-1].lower() in [\'png\',\'bmp\',\'jpg\',\'jpeg\']:\n\t\t\tself.type=self.IMG\n\t\telse:\n\t\t\tprint(""Invalid file: ""+self.path)\n\t\t\tsys.exit(-1)\n\n\tdef put(self, name, frame):\n\t\tif self.type==self.DIR:\n\t\t\tcv2.imwrite(self.path+""/""+name, frame)\n\t\telif self.type==self.IMG:\n\t\t\tcv2.imwrite(self.path, frame)\n\t\telif self.type==self.VID:\n\t\t\tif self.writer is None:\n\t\t\t\tif hasattr(cv2, \'VideoWriter_fourcc\'):\n\t\t\t\t\tfcc=cv2.VideoWriter_fourcc(*\'MJPG\')\n\t\t\t\telse:\n\t\t\t\t\tfcc=cv2.cv.CV_FOURCC(*\'MJPG\')\n\n\t\t\t\tself.writer = cv2.VideoWriter(self.path, fcc, int(self.fps), (frame.shape[1], frame.shape[0]))\n\t\t\tself.writer.write(frame)\n\t\telse:\n\t\t\tpass\n'"
Utils/RandomSelect.py,7,"b'# Copyright 2017 Robert Csordas. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# ==============================================================================\n\nimport tensorflow as tf\n\ndef randomSelectIndex(fromCount, n):\n\twith tf.name_scope(""randomSelectIndex""):\n\t\tn = tf.minimum(fromCount, n)\n\t\ti = tf.random_shuffle(tf.range(fromCount, dtype=tf.int32))[0:n]\n\t\treturn tf.expand_dims(i,-1)\n\ndef randomSelectBatch(t, n):\n\twith tf.name_scope(""randomSelectBatch""):\n\t\tcount = tf.shape(t)[0]\n\t\treturn tf.gather_nd(t, randomSelectIndex(count,n))\n'"
Utils/RunManager.py,0,"b'# Copyright 2017 Robert Csordas. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# ==============================================================================\n\nimport sys\n\nclass RunManager():\n\tdef __init__(self, sess, options=None, run_metadata=None):\n\t\tself.sess=sess\n\t\tself.groups={}\n\t\tself.options=options\n\t\tself.run_metadata=run_metadata\n\n\tdef add(self, name, tensorList, enabled=True, modRun=0):\n\t\tself.groups[name]={\n\t\t\t""inList"": tensorList,\n\t\t\t""enabled"": enabled,\n\t\t\t""modRun"": modRun\n\t\t}\n\n\tdef appendToInput(self, name, list):\n\t\tstartIndex = len(self.inputTensors)\n\t\tself.inputTensors+=self.groups[name][""inList""]\n\t\tself.indexList.append({\n\t\t\t""name"": name,\n\t\t\t""index"": startIndex\n\t\t})\n\n\tdef clearInput(self):\n\t\tself.indexList=[]\n\t\tself.inputTensors=[]\n\n\tdef buildInputFromNames(self, names):\n\t\tself.clearInput()\n\t\tfor k in names:\n\t\t\tself.appendToInput(k, self.groups[k][""inList""])\n\n\tdef buildInputFromEnabled(self):\n\t\tself.clearInput()\n\t\tfor k in self.groups:\n\t\t\tg=self.groups[k]\n\t\t\tif g[""enabled""] != True:\n\t\t\t\tcontinue\n\n\t\t\tself.appendToInput(k, g[""inList""])\n\n\tdef buildInputMod(self, counter):\n\t\tself.clearInput()\n\t\tfor k in self.groups:\n\t\t\tg=self.groups[k]\n\t\t\tm=g[""modRun""]\n\t\t\tif m<=0 or g[""enabled""] != True or counter % m != 0:\n\t\t\t\tcontinue\n\n\t\t\tself.appendToInput(k, g[""inList""])\n\n\tdef runAndMerge(self, feed_dict=None, options=None, run_metadata=None):\n\t\ttry:\n\t\t\tres = self.sess.run(self.inputTensors, feed_dict=feed_dict, options=options, run_metadata=run_metadata)\n\t\texcept KeyboardInterrupt:\n\t\t\tprint(""Keyboard interrupt. Shutting down."")\n\t\t\tsys.exit(0)\n\n\t\tresult = {}\n\n\t\tfor i in self.indexList:\n\t\t\tname=i[""name""]\n\t\t\tcnt=len(self.groups[name][""inList""])\n\t\t\tstartIndex=i[""index""]\n\t\t\tr=res[startIndex:startIndex+cnt]\n\t\t\tresult[name]=r if cnt > 1 else r[0]\n\n\t\treturn result\n\n\tdef run(self, names=None, feed_dict=None, options=None, run_metadata=None):\n\t\tif names is None:\n\t\t\tself.buildInputFromEnabled()\n\t\telse:\n\t\t\tself.buildInputFromNames(names)\n\n\t\treturn self.runAndMerge(feed_dict, options=options, run_metadata=run_metadata)\n\n\tdef modRun(self, counter, feed_dict=None, options=None, run_metadata=None):\n\t\tself.buildInputMod(counter=counter)\n\t\treturn self.runAndMerge(feed_dict, options=options if options is not None else self.options, run_metadata=run_metadata if run_metadata is not None else self.run_metadata)\n\n\tdef enable(self, name, enabled=True):\n\t\tself.groups[name][""enabled""]=enabled\n\n\tdef disable(self, name):\n\t\tself.enable(name, False)\n'"
Utils/Summary.py,13,"b'# Copyright 2017 Robert Csordas. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# ==============================================================================\n\nimport tensorflow as tf\n\ndef variableSummary(var):\n  if not isinstance(var, list):\n    var=[var]\n\n  for v in var:\n    with tf.name_scope(\'summaries\'):\n      mean = tf.reduce_mean(v)\n      tf.summary.scalar(\'mean/\' + v.op.name, mean)\n      with tf.name_scope(\'stddev\'):\n        stddev = tf.sqrt(tf.reduce_mean(tf.square(v - mean)))\n      tf.summary.scalar(\'stddev/\' + v.op.name, stddev)\n      tf.summary.scalar(\'max/\' + v.op.name, tf.reduce_max(v))\n      tf.summary.scalar(\'min/\' + v.op.name, tf.reduce_min(v))\n      tf.histogram_summary(v.op.name, v)\n\ndef createSummaryForAllVars():\n  variableSummary(tf.trainable_variables())\n\ndef pyhtonFloatSummary(name):\n  p=tf.placeholder(tf.float32)\n  s=tf.summary.scalar(name, p)\n  return s, p\n\ndef imageSummary(var):\n  res=[]\n  for name in var:\n    res.append(tf.image_summary(name, var[name]))\n\n  return res'"
Utils/__init__.py,0,b''
Visualize/Visualize.py,0,"b'# Copyright 2017 Robert Csordas. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# ==============================================================================\n\nimport cv2\nimport numpy as np\n\nclass Palette:\n    @staticmethod\n    def bitShift(v,s):\n        if s>0:\n            return v<<s\n        else:\n            return v>>(-s)\n \n    def setFixColor(self, category, color):\n        if len(self.cmap)<=category:\n            return\n \n        if self.bgr:\n            self.cmap[category][2]=color[0]\n            self.cmap[category][1]=color[1]\n            self.cmap[category][0]=color[2]\n        else:\n            self.cmap[category]=color\n \n    def modify(self, p):\n        for k in p:\n            self.setFixColor(k, p[k])\n \n    def __init__(self, N, bgr=True, modifier=None):\n        self.list=None\n        self.bgr=bgr\n        self.cmap=np.zeros((N, 3), dtype=np.uint8)\n        for i in range(N):\n            id=(i+1)\n            r=0\n            g=0\n            b=0\n \n            for j in range(8):\n                r=r | Palette.bitShift((id >> 0) & 1, 7-j)\n                g=g | Palette.bitShift((id >> 1) & 1, 7-j)\n                b=b | Palette.bitShift((id >> 2) & 1, 7-j)\n                id = Palette.bitShift(id, -3)\n \n            if bgr:\n                self.cmap[i]=[b,g,r]\n            else:\n                self.cmap[i]=[r,g,b]\n \n        if modifier is not None:\n            self.modify(modifier)\n \n    def getMap(self, list=False):\n        if list:\n            if self.list is None:\n                self.list=self.cmap.tolist()\n            return self.list\n        else:\n            return self.cmap\n\ndef drawBoxes(img, boxes, categories, names, palette, scores=None, fade=False):\n    def clipCoord(xy):\n        return np.minimum(np.maximum(np.array(xy,dtype=np.int32),0),[img.shape[1]-1, img.shape[0]-1]).tolist()\n\n    cmap = palette.getMap(list=True)\n    font = cv2.FONT_HERSHEY_COMPLEX_SMALL \n    fontSize = 0.8\n    fontThickness = 1\n    pad=5\n\n\n    img=np.copy(img)\n\n    for box in range(boxes.shape[0]):\n        if fade and scores is not None:\n            iOrig = img\n            img=np.copy(img)\n\n        topleft = tuple(clipCoord(boxes[box][0:2]))\n        if categories is not None:\n            color = tuple(cmap[categories[box]])\n        else:\n            color = (0,0,255)\n        cv2.rectangle(img, topleft, tuple(clipCoord(boxes[box][2:5])), color, thickness=4)\n        if names:\n            title=names[box]\n            if scores is not None:\n                title+="": %.2f"" % scores[box]\n            textpos=[topleft[0], topleft[1]-pad]\n            size = cv2.getTextSize(title, font, fontSize, fontThickness)[0]\n\n            boxTL = textpos[:]\n            boxTL[1] = boxTL[1] - size[1]\n            boxBR = list(topleft)\n            boxBR[0] = boxBR[0] + size[0]\n            \n            cv2.rectangle(img, tuple(boxTL), tuple(boxBR), color, thickness=-1)\n            cv2.rectangle(img, tuple(boxTL), tuple(boxBR), color, thickness=4)\n            cv2.putText(img, title, tuple(textpos), font, fontSize, (255,255,255), thickness=fontThickness)\n        \n        if fade and scores is not None:\n            img = scores[box] * img + (1.0-scores[box]) * iOrig\n    return img\n\ndef tile(cols, rows, imgs, titles=None):\n    font = cv2.FONT_HERSHEY_COMPLEX_SMALL\n    fontSize = 1\n    fontThickness = 2\n    pad=10\n    titleColor = (255,192,0)\n\n    hImg = imgs[0]\n    i = 0\n    z = None\n    row = []\n    for c in range(cols):\n        col = []\n        for r in range(rows):\n            if i<len(imgs):\n                img = imgs[i]\n                if titles is not None and i<len(titles):\n                    img = img.copy()\n                    size = cv2.getTextSize(titles[i], font, fontSize, fontThickness)[0]\n                    cv2.putText(img, titles[i], (pad, size[1]+pad), font, fontSize, titleColor, thickness=fontThickness)\n\n                col.append(img)\n            else:\n                if z is None:\n                    z = np.zeros_like(imgs[0])\n                col.append(z)\n            i+=1\n        row.append(np.concatenate(col, axis=0))\n\n    return np.concatenate(row, axis=1)\n'"
Visualize/VisualizeOutput.py,0,"b'# Copyright 2017 Robert Csordas. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# ==============================================================================\n\nfrom . import Visualize\nimport threading\nimport cv2\n\ntry:\n\timport queue\nexcept:\n\timport Queue as queue\n\nclass OutputVisualizer:\n\tdef __init__(self, opt, runManager, dataset, net, images, boxes, classes):\n\t\tself.opt = opt\n\t\tself.queue = queue.Queue()\n\t\tself.dataset = dataset\n\t\tself.palette = Visualize.Palette(dataset.categoryCount())\n\n\t\tpredBoxes, predScores, predClasses = net.getBoxes()\n\t\tallPredBoxes, allPredScores, allPredClasses = net.getBoxes(scoreThreshold=0)\n\t\tproposals, proposalScores = net.getProposals()\n\n\t\trunManager.add(""preview"", [images, boxes, classes, predBoxes, predClasses, predScores, proposals, proposalScores, allPredBoxes, allPredScores, allPredClasses], modRun=self.opt.displayInterval)\t\t\n\t\tself.startThread()\n\n\n\tdef threadFn(self):\n\t\twhile True:\n\t\t\trefImg, refBox, refClasses, pBoxes, pClasses, pScores, pProposals, pProposalScores, pAllBoxes, pAllScores, pAllClasses = self.queue.get()\n\t\t\trefImg=refImg[0]\n\n\t\t\ta = Visualize.drawBoxes(refImg, refBox, refClasses, self.dataset.getCaptions(refClasses), self.palette)\n\t\t\tb = Visualize.drawBoxes(refImg, pBoxes, pClasses, self.dataset.getCaptions(pClasses), self.palette, scores=pScores)\n\t\t\tc = Visualize.drawBoxes(refImg, pProposals, None, None, self.palette, scores=pProposalScores*0.3)\n\t\t\td = Visualize.drawBoxes(refImg, pAllBoxes, pAllClasses, self.dataset.getCaptions(pAllClasses), self.palette, scores=pAllScores)\n\n\t\t\tpreview = Visualize.tile(2,2, [a,b,c,d], [""input"", ""output"", ""proposals"", ""all detections""])\n\n\t\t\tcv2.imwrite(self.opt.name+""/preview/preview.jpg"", preview)\n\n\t\t\tself.queue.task_done()\n \n\tdef startThread(self):\n\t\tself.thread=threading.Thread(target=self.threadFn)\n\t\tself.thread.daemon = True\n\t\tself.thread.start()\n\n\tdef draw(self, res):\n\t\tif ""preview"" not in res:\n\t\t\treturn\n\n\t\twhile not self.queue.empty():\n\t\t\ttry:\n\t\t\t\tself.queue.get(False)\n\t\t\t\tself.queue.task_done()\n\t\t\texcept queue.Empty:\n\t\t\t\tcontinue\n \n\t\tself.queue.put(res[""preview""])\n\t\t'"
Visualize/__init__.py,0,b'from BoxEngine.BoxNetwork import *\n'
BoxEngine/ROIPooling/ROIPoolingWrapper.py,9,"b'# Copyright 2017 Robert Csordas. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# ==============================================================================\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\n\ntry:\n\troiPoolingModule = tf.load_op_library(""BoxEngine/ROIPooling/roi_pooling.so"")\nexcept:\n\troiPoolingModule = tf.load_op_library(""./roi_pooling.so"")\n\ndef positionSensitiveRoiPooling(features, boxes, offset=[0,0], downsample=16, roiSize=3):\n\twith tf.name_scope(""positionSensitiveRoiPooling""):\n\t\tfeatureCount = features.get_shape().as_list()[-1]\n\t\t\n\t\twith tf.name_scope(""imgCoordinatesToHeatmapCoordinates""):\n\t\t\tboxes=tf.stop_gradient(boxes)\n\t\t\tboxes = boxes - [offset[1], offset[0], offset[1]-downsample+0.1, offset[0]-downsample+0.1]\n\t\t\tboxes = boxes / downsample\n\t\t\tboxes = tf.cast(boxes, tf.int32)\n\n\t\t\n\t\twith tf.name_scope(""NHWC2NCHW""):\n\t\t\tfeatures = tf.transpose(features, [0,3,1,2])\n\n\t\tres = roiPoolingModule.pos_roi_pooling(features, boxes, [roiSize,roiSize])\n\t\t\n\t\tres.set_shape([None, roiSize, roiSize, None if featureCount is None else int(featureCount/(roiSize*roiSize))])\n\t\treturn res\n\n@ops.RegisterGradient(""PosRoiPooling"")\ndef _pos_roi_pooling_grad(op, grad):\n\tg_features = roiPoolingModule.pos_roi_pooling_grad(grad, tf.shape(op.inputs[0]), op.inputs[1], op.inputs[2])\n\treturn g_features, None, None\n\t'"
BoxEngine/ROIPooling/__init__.py,0,"b""__author__ = 'Robert Csordas'\nfrom .ROIPoolingWrapper import *\n"""
BoxEngine/ROIPooling/test.py,6,"b'import numpy as np\n\nif __name__ == ""__main__"":\n\timport tensorflow as tf\n\tfrom .ROIPoolingWrapper import *\n\n\twith tf.Session() as sess:\n\t\timg = np.zeros((1,8,8, 9), np.float32)\n\t\tboxes = tf.constant([[0,0,2*16,5*16]], dtype=tf.float32)\n\t\tprint(boxes.get_shape().as_list())\n\n\t\tyOffset=0\n\t\txOffset=0\n\t\tchOffset=0\n\t\timg[0,yOffset+0:yOffset+1,xOffset+0:xOffset+1,chOffset+0:chOffset+1]=1;\n\t\t#img[:,:,:,:]=1\n\t\tp = tf.placeholder(tf.float32, shape=img.shape)\n\n\t\tnp.set_printoptions(threshold=5000, linewidth=150)\n\n\t\tpooled=positionSensitiveRoiPooling(p, boxes)\n\t\tpooled=tf.Print(pooled,[tf.shape(pooled)],""pooled shape"", summarize=100)\n\t\tprint(sess.run(pooled, feed_dict={p: img}))\n\n\n\t\tloss = tf.reduce_sum(pooled)\n\n\t\tg = tf.gradients(loss, p)\n\n\t\tprint(img)\n\t\tprint(sess.run(g, feed_dict={p: img})[0])\n\t\tprint(sess.run(g, feed_dict={p: img})[0][:,:,:,1])\n\n\n'"
Dataset/coco/__init__.py,0,b''
Dataset/coco/setup.py,0,"b'from distutils.core import setup\nfrom Cython.Build import cythonize\nfrom distutils.extension import Extension\nimport numpy as np\n\n# To compile and install locally run ""python setup.py build_ext --inplace""\n# To install library to Python site-packages run ""python setup.py build_ext install""\n\next_modules = [\n    Extension(\n        \'pycocotools._mask\',\n        sources=[\'common/maskApi.c\', \'pycocotools/_mask.pyx\'],\n        include_dirs = [np.get_include(), \'common\'],\n        extra_compile_args=[\'-Wno-cpp\', \'-Wno-unused-function\', \'-std=c99\'],\n    )\n]\n\nsetup(name=\'pycocotools\',\n      packages=[\'pycocotools\'],\n      package_dir = {\'pycocotools\': \'pycocotools\'},\n      version=\'2.0\',\n      ext_modules=\n          cythonize(ext_modules)\n      )\n'"
Dataset/coco/pycocotools/__init__.py,0,"b""__author__ = 'tylin'\n"""
Dataset/coco/pycocotools/coco.py,0,"b'__author__ = \'tylin\'\n__version__ = \'2.0\'\n# Interface for accessing the Microsoft COCO dataset.\n\n# Microsoft COCO is a large image dataset designed for object detection,\n# segmentation, and caption generation. pycocotools is a Python API that\n# assists in loading, parsing and visualizing the annotations in COCO.\n# Please visit http://mscoco.org/ for more information on COCO, including\n# for the data, paper, and tutorials. The exact format of the annotations\n# is also described on the COCO website. For example usage of the pycocotools\n# please see pycocotools_demo.ipynb. In addition to this API, please download both\n# the COCO images and annotations in order to run the demo.\n\n# An alternative to using the API is to load the annotations directly\n# into Python dictionary\n# Using the API provides additional utility functions. Note that this API\n# supports both *instance* and *caption* annotations. In the case of\n# captions not all functions are defined (e.g. categories are undefined).\n\n# The following API functions are defined:\n#  COCO       - COCO api class that loads COCO annotation file and prepare data structures.\n#  decodeMask - Decode binary mask M encoded via run-length encoding.\n#  encodeMask - Encode binary mask M using run-length encoding.\n#  getAnnIds  - Get ann ids that satisfy given filter conditions.\n#  getCatIds  - Get cat ids that satisfy given filter conditions.\n#  getImgIds  - Get img ids that satisfy given filter conditions.\n#  loadAnns   - Load anns with the specified ids.\n#  loadCats   - Load cats with the specified ids.\n#  loadImgs   - Load imgs with the specified ids.\n#  annToMask  - Convert segmentation in an annotation to binary mask.\n#  showAnns   - Display the specified annotations.\n#  loadRes    - Load algorithm results and create API for accessing them.\n#  download   - Download COCO images from mscoco.org server.\n# Throughout the API ""ann""=annotation, ""cat""=category, and ""img""=image.\n# Help on each functions can be accessed by: ""help COCO>function"".\n\n# See also COCO>decodeMask,\n# COCO>encodeMask, COCO>getAnnIds, COCO>getCatIds,\n# COCO>getImgIds, COCO>loadAnns, COCO>loadCats,\n# COCO>loadImgs, COCO>annToMask, COCO>showAnns\n\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2014.\n# Licensed under the Simplified BSD License [see bsd.txt]\n\nimport json\nimport time\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import PatchCollection\nfrom matplotlib.patches import Polygon\nimport numpy as np\nimport copy\nimport itertools\nfrom . import mask as maskUtils\nimport os\nfrom collections import defaultdict\nimport sys\nPYTHON_VERSION = sys.version_info[0]\nif PYTHON_VERSION == 2:\n    from urllib import urlretrieve\nelif PYTHON_VERSION == 3:\n    from urllib.request import urlretrieve\n\nclass COCO:\n    def __init__(self, annotation_file=None):\n        """"""\n        Constructor of Microsoft COCO helper class for reading and visualizing annotations.\n        :param annotation_file (str): location of annotation file\n        :param image_folder (str): location to the folder that hosts images.\n        :return:\n        """"""\n        # load dataset\n        self.dataset,self.anns,self.cats,self.imgs = dict(),dict(),dict(),dict()\n        self.imgToAnns, self.catToImgs = defaultdict(list), defaultdict(list)\n        if not annotation_file == None:\n            print(\'loading annotations into memory...\')\n            tic = time.time()\n            dataset = json.load(open(annotation_file, \'r\'))\n            assert type(dataset)==dict, \'annotation file format {} not supported\'.format(type(dataset))\n            print(\'Done (t={:0.2f}s)\'.format(time.time()- tic))\n            self.dataset = dataset\n            self.createIndex()\n\n    def createIndex(self):\n        # create index\n        print(\'creating index...\')\n        anns, cats, imgs = {}, {}, {}\n        imgToAnns,catToImgs = defaultdict(list),defaultdict(list)\n        if \'annotations\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                imgToAnns[ann[\'image_id\']].append(ann)\n                anns[ann[\'id\']] = ann\n\n        if \'images\' in self.dataset:\n            for img in self.dataset[\'images\']:\n                imgs[img[\'id\']] = img\n\n        if \'categories\' in self.dataset:\n            for cat in self.dataset[\'categories\']:\n                cats[cat[\'id\']] = cat\n\n        if \'annotations\' in self.dataset and \'categories\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                catToImgs[ann[\'category_id\']].append(ann[\'image_id\'])\n\n        print(\'index created!\')\n\n        # create class members\n        self.anns = anns\n        self.imgToAnns = imgToAnns\n        self.catToImgs = catToImgs\n        self.imgs = imgs\n        self.cats = cats\n\n    def info(self):\n        """"""\n        Print information about the annotation file.\n        :return:\n        """"""\n        for key, value in self.dataset[\'info\'].items():\n            print(\'{}: {}\'.format(key, value))\n\n    def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n        """"""\n        Get ann ids that satisfy given filter conditions. default skips that filter\n        :param imgIds  (int array)     : get anns for given imgs\n               catIds  (int array)     : get anns for given cats\n               areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n               iscrowd (boolean)       : get anns for given crowd label (False or True)\n        :return: ids (int array)       : integer array of ann ids\n        """"""\n        imgIds = imgIds if type(imgIds) == list else [imgIds]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(imgIds) == len(catIds) == len(areaRng) == 0:\n            anns = self.dataset[\'annotations\']\n        else:\n            if not len(imgIds) == 0:\n                lists = [self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns]\n                anns = list(itertools.chain.from_iterable(lists))\n            else:\n                anns = self.dataset[\'annotations\']\n            anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann[\'category_id\'] in catIds]\n            anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann[\'area\'] > areaRng[0] and ann[\'area\'] < areaRng[1]]\n        if not iscrowd == None:\n            ids = [ann[\'id\'] for ann in anns if ann[\'iscrowd\'] == iscrowd]\n        else:\n            ids = [ann[\'id\'] for ann in anns]\n        return ids\n\n    def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n        """"""\n        filtering parameters. default skips that filter.\n        :param catNms (str array)  : get cats for given cat names\n        :param supNms (str array)  : get cats for given supercategory names\n        :param catIds (int array)  : get cats for given cat ids\n        :return: ids (int array)   : integer array of cat ids\n        """"""\n        catNms = catNms if type(catNms) == list else [catNms]\n        supNms = supNms if type(supNms) == list else [supNms]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(catNms) == len(supNms) == len(catIds) == 0:\n            cats = self.dataset[\'categories\']\n        else:\n            cats = self.dataset[\'categories\']\n            cats = cats if len(catNms) == 0 else [cat for cat in cats if cat[\'name\']          in catNms]\n            cats = cats if len(supNms) == 0 else [cat for cat in cats if cat[\'supercategory\'] in supNms]\n            cats = cats if len(catIds) == 0 else [cat for cat in cats if cat[\'id\']            in catIds]\n        ids = [cat[\'id\'] for cat in cats]\n        return ids\n\n    def getImgIds(self, imgIds=[], catIds=[]):\n        \'\'\'\n        Get img ids that satisfy given filter conditions.\n        :param imgIds (int array) : get imgs for given ids\n        :param catIds (int array) : get imgs with all given cats\n        :return: ids (int array)  : integer array of img ids\n        \'\'\'\n        imgIds = imgIds if type(imgIds) == list else [imgIds]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(imgIds) == len(catIds) == 0:\n            ids = self.imgs.keys()\n        else:\n            ids = set(imgIds)\n            for i, catId in enumerate(catIds):\n                if i == 0 and len(ids) == 0:\n                    ids = set(self.catToImgs[catId])\n                else:\n                    ids &= set(self.catToImgs[catId])\n        return list(ids)\n\n    def loadAnns(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying anns\n        :return: anns (object array) : loaded ann objects\n        """"""\n        if type(ids) == list:\n            return [self.anns[id] for id in ids]\n        elif type(ids) == int:\n            return [self.anns[ids]]\n\n    def loadCats(self, ids=[]):\n        """"""\n        Load cats with the specified ids.\n        :param ids (int array)       : integer ids specifying cats\n        :return: cats (object array) : loaded cat objects\n        """"""\n        if type(ids) == list:\n            return [self.cats[id] for id in ids]\n        elif type(ids) == int:\n            return [self.cats[ids]]\n\n    def loadImgs(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying img\n        :return: imgs (object array) : loaded img objects\n        """"""\n        if type(ids) == list:\n            return [self.imgs[id] for id in ids]\n        elif type(ids) == int:\n            return [self.imgs[ids]]\n\n    def showAnns(self, anns):\n        """"""\n        Display the specified annotations.\n        :param anns (array of object): annotations to display\n        :return: None\n        """"""\n        if len(anns) == 0:\n            return 0\n        if \'segmentation\' in anns[0] or \'keypoints\' in anns[0]:\n            datasetType = \'instances\'\n        elif \'caption\' in anns[0]:\n            datasetType = \'captions\'\n        else:\n            raise Exception(\'datasetType not supported\')\n        if datasetType == \'instances\':\n            ax = plt.gca()\n            ax.set_autoscale_on(False)\n            polygons = []\n            color = []\n            for ann in anns:\n                c = (np.random.random((1, 3))*0.6+0.4).tolist()[0]\n                if \'segmentation\' in ann:\n                    if type(ann[\'segmentation\']) == list:\n                        # polygon\n                        for seg in ann[\'segmentation\']:\n                            poly = np.array(seg).reshape((int(len(seg)/2), 2))\n                            polygons.append(Polygon(poly))\n                            color.append(c)\n                    else:\n                        # mask\n                        t = self.imgs[ann[\'image_id\']]\n                        if type(ann[\'segmentation\'][\'counts\']) == list:\n                            rle = maskUtils.frPyObjects([ann[\'segmentation\']], t[\'height\'], t[\'width\'])\n                        else:\n                            rle = [ann[\'segmentation\']]\n                        m = maskUtils.decode(rle)\n                        img = np.ones( (m.shape[0], m.shape[1], 3) )\n                        if ann[\'iscrowd\'] == 1:\n                            color_mask = np.array([2.0,166.0,101.0])/255\n                        if ann[\'iscrowd\'] == 0:\n                            color_mask = np.random.random((1, 3)).tolist()[0]\n                        for i in range(3):\n                            img[:,:,i] = color_mask[i]\n                        ax.imshow(np.dstack( (img, m*0.5) ))\n                if \'keypoints\' in ann and type(ann[\'keypoints\']) == list:\n                    # turn skeleton into zero-based index\n                    sks = np.array(self.loadCats(ann[\'category_id\'])[0][\'skeleton\'])-1\n                    kp = np.array(ann[\'keypoints\'])\n                    x = kp[0::3]\n                    y = kp[1::3]\n                    v = kp[2::3]\n                    for sk in sks:\n                        if np.all(v[sk]>0):\n                            plt.plot(x[sk],y[sk], linewidth=3, color=c)\n                    plt.plot(x[v>0], y[v>0],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=\'k\',markeredgewidth=2)\n                    plt.plot(x[v>1], y[v>1],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=c, markeredgewidth=2)\n            p = PatchCollection(polygons, facecolor=color, linewidths=0, alpha=0.4)\n            ax.add_collection(p)\n            p = PatchCollection(polygons, facecolor=\'none\', edgecolors=color, linewidths=2)\n            ax.add_collection(p)\n        elif datasetType == \'captions\':\n            for ann in anns:\n                print(ann[\'caption\'])\n\n    def loadRes(self, resFile):\n        """"""\n        Load result file and return a result api object.\n        :param   resFile (str)     : file name of result file\n        :return: res (obj)         : result api object\n        """"""\n        res = COCO()\n        res.dataset[\'images\'] = [img for img in self.dataset[\'images\']]\n\n        print(\'Loading and preparing results...\')\n        tic = time.time()\n        if type(resFile) == str or type(resFile) == unicode:\n            anns = json.load(open(resFile))\n        elif type(resFile) == np.ndarray:\n            anns = self.loadNumpyAnnotations(resFile)\n        else:\n            anns = resFile\n        assert type(anns) == list, \'results in not an array of objects\'\n        annsImgIds = [ann[\'image_id\'] for ann in anns]\n        assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \\\n               \'Results do not correspond to current coco set\'\n        if \'caption\' in anns[0]:\n            imgIds = set([img[\'id\'] for img in res.dataset[\'images\']]) & set([ann[\'image_id\'] for ann in anns])\n            res.dataset[\'images\'] = [img for img in res.dataset[\'images\'] if img[\'id\'] in imgIds]\n            for id, ann in enumerate(anns):\n                ann[\'id\'] = id+1\n        elif \'bbox\' in anns[0] and not anns[0][\'bbox\'] == []:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                bb = ann[\'bbox\']\n                x1, x2, y1, y2 = [bb[0], bb[0]+bb[2], bb[1], bb[1]+bb[3]]\n                if not \'segmentation\' in ann:\n                    ann[\'segmentation\'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n                ann[\'area\'] = bb[2]*bb[3]\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'segmentation\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                # now only support compressed RLE format as segmentation results\n                ann[\'area\'] = maskUtils.area(ann[\'segmentation\'])\n                if not \'bbox\' in ann:\n                    ann[\'bbox\'] = maskUtils.toBbox(ann[\'segmentation\'])\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'keypoints\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                s = ann[\'keypoints\']\n                x = s[0::3]\n                y = s[1::3]\n                x0,x1,y0,y1 = np.min(x), np.max(x), np.min(y), np.max(y)\n                ann[\'area\'] = (x1-x0)*(y1-y0)\n                ann[\'id\'] = id + 1\n                ann[\'bbox\'] = [x0,y0,x1-x0,y1-y0]\n        print(\'DONE (t={:0.2f}s)\'.format(time.time()- tic))\n\n        res.dataset[\'annotations\'] = anns\n        res.createIndex()\n        return res\n\n    def download(self, tarDir = None, imgIds = [] ):\n        \'\'\'\n        Download COCO images from mscoco.org server.\n        :param tarDir (str): COCO results directory name\n               imgIds (list): images to be downloaded\n        :return:\n        \'\'\'\n        if tarDir is None:\n            print(\'Please specify target directory\')\n            return -1\n        if len(imgIds) == 0:\n            imgs = self.imgs.values()\n        else:\n            imgs = self.loadImgs(imgIds)\n        N = len(imgs)\n        if not os.path.exists(tarDir):\n            os.makedirs(tarDir)\n        for i, img in enumerate(imgs):\n            tic = time.time()\n            fname = os.path.join(tarDir, img[\'file_name\'])\n            if not os.path.exists(fname):\n                urlretrieve(img[\'coco_url\'], fname)\n            print(\'downloaded {}/{} images (t={:0.1f}s)\'.format(i, N, time.time()- tic))\n\n    def loadNumpyAnnotations(self, data):\n        """"""\n        Convert result data from a numpy array [Nx7] where each row contains {imageID,x1,y1,w,h,score,class}\n        :param  data (numpy.ndarray)\n        :return: annotations (python nested list)\n        """"""\n        print(\'Converting ndarray to lists...\')\n        assert(type(data) == np.ndarray)\n        print(data.shape)\n        assert(data.shape[1] == 7)\n        N = data.shape[0]\n        ann = []\n        for i in range(N):\n            if i % 1000000 == 0:\n                print(\'{}/{}\'.format(i,N))\n            ann += [{\n                \'image_id\'  : int(data[i, 0]),\n                \'bbox\'  : [ data[i, 1], data[i, 2], data[i, 3], data[i, 4] ],\n                \'score\' : data[i, 5],\n                \'category_id\': int(data[i, 6]),\n                }]\n        return ann\n\n    def annToRLE(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE to RLE.\n        :return: binary mask (numpy 2D array)\n        """"""\n        t = self.imgs[ann[\'image_id\']]\n        h, w = t[\'height\'], t[\'width\']\n        segm = ann[\'segmentation\']\n        if type(segm) == list:\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            rles = maskUtils.frPyObjects(segm, h, w)\n            rle = maskUtils.merge(rles)\n        elif type(segm[\'counts\']) == list:\n            # uncompressed RLE\n            rle = maskUtils.frPyObjects(segm, h, w)\n        else:\n            # rle\n            rle = ann[\'segmentation\']\n        return rle\n\n    def annToMask(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n        :return: binary mask (numpy 2D array)\n        """"""\n        rle = self.annToRLE(ann)\n        m = maskUtils.decode(rle)\n        return m'"
Dataset/coco/pycocotools/cocoeval.py,0,"b'__author__ = \'tsungyi\'\n\nimport numpy as np\nimport datetime\nimport time\nfrom collections import defaultdict\nfrom . import mask as maskUtils\nimport copy\n\nclass COCOeval:\n    # Interface for evaluating detection on the Microsoft COCO dataset.\n    #\n    # The usage for CocoEval is as follows:\n    #  cocoGt=..., cocoDt=...       # load dataset and results\n    #  E = CocoEval(cocoGt,cocoDt); # initialize CocoEval object\n    #  E.params.recThrs = ...;      # set parameters as desired\n    #  E.evaluate();                # run per image evaluation\n    #  E.accumulate();              # accumulate per image results\n    #  E.summarize();               # display summary metrics of results\n    # For example usage see evalDemo.m and http://mscoco.org/.\n    #\n    # The evaluation parameters are as follows (defaults in brackets):\n    #  imgIds     - [all] N img ids to use for evaluation\n    #  catIds     - [all] K cat ids to use for evaluation\n    #  iouThrs    - [.5:.05:.95] T=10 IoU thresholds for evaluation\n    #  recThrs    - [0:.01:1] R=101 recall thresholds for evaluation\n    #  areaRng    - [...] A=4 object area ranges for evaluation\n    #  maxDets    - [1 10 100] M=3 thresholds on max detections per image\n    #  iouType    - [\'segm\'] set iouType to \'segm\', \'bbox\' or \'keypoints\'\n    #  iouType replaced the now DEPRECATED useSegm parameter.\n    #  useCats    - [1] if true use category labels for evaluation\n    # Note: if useCats=0 category labels are ignored as in proposal scoring.\n    # Note: multiple areaRngs [Ax2] and maxDets [Mx1] can be specified.\n    #\n    # evaluate(): evaluates detections on every image and every category and\n    # concats the results into the ""evalImgs"" with fields:\n    #  dtIds      - [1xD] id for each of the D detections (dt)\n    #  gtIds      - [1xG] id for each of the G ground truths (gt)\n    #  dtMatches  - [TxD] matching gt id at each IoU or 0\n    #  gtMatches  - [TxG] matching dt id at each IoU or 0\n    #  dtScores   - [1xD] confidence of each dt\n    #  gtIgnore   - [1xG] ignore flag for each gt\n    #  dtIgnore   - [TxD] ignore flag for each dt at each IoU\n    #\n    # accumulate(): accumulates the per-image, per-category evaluation\n    # results in ""evalImgs"" into the dictionary ""eval"" with fields:\n    #  params     - parameters used for evaluation\n    #  date       - date evaluation was performed\n    #  counts     - [T,R,K,A,M] parameter dimensions (see above)\n    #  precision  - [TxRxKxAxM] precision for every evaluation setting\n    #  recall     - [TxKxAxM] max recall for every evaluation setting\n    # Note: precision and recall==-1 for settings with no gt objects.\n    #\n    # See also coco, mask, pycocoDemo, pycocoEvalDemo\n    #\n    # Microsoft COCO Toolbox.      version 2.0\n    # Data, paper, and tutorials available at:  http://mscoco.org/\n    # Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n    # Licensed under the Simplified BSD License [see coco/license.txt]\n    def __init__(self, cocoGt=None, cocoDt=None, iouType=\'segm\'):\n        \'\'\'\n        Initialize CocoEval using coco APIs for gt and dt\n        :param cocoGt: coco object with ground truth annotations\n        :param cocoDt: coco object with detection results\n        :return: None\n        \'\'\'\n        if not iouType:\n            print(\'iouType not specified. use default iouType segm\')\n        self.cocoGt   = cocoGt              # ground truth COCO API\n        self.cocoDt   = cocoDt              # detections COCO API\n        self.params   = {}                  # evaluation parameters\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results [KxAxI] elements\n        self.eval     = {}                  # accumulated evaluation results\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        self.params = Params(iouType=iouType) # parameters\n        self._paramsEval = {}               # parameters for evaluation\n        self.stats = []                     # result summarization\n        self.ious = {}                      # ious between all gts and dts\n        if not cocoGt is None:\n            self.params.imgIds = sorted(cocoGt.getImgIds())\n            self.params.catIds = sorted(cocoGt.getCatIds())\n\n\n    def _prepare(self):\n        \'\'\'\n        Prepare ._gts and ._dts for evaluation based on params\n        :return: None\n        \'\'\'\n        def _toMask(anns, coco):\n            # modify ann[\'segmentation\'] by reference\n            for ann in anns:\n                rle = coco.annToRLE(ann)\n                ann[\'segmentation\'] = rle\n        p = self.params\n        if p.useCats:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n        else:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds))\n\n        # convert ground truth to mask if iouType == \'segm\'\n        if p.iouType == \'segm\':\n            _toMask(gts, self.cocoGt)\n            _toMask(dts, self.cocoDt)\n        # set ignore flag\n        for gt in gts:\n            gt[\'ignore\'] = gt[\'ignore\'] if \'ignore\' in gt else 0\n            gt[\'ignore\'] = \'iscrowd\' in gt and gt[\'iscrowd\']\n            if p.iouType == \'keypoints\':\n                gt[\'ignore\'] = (gt[\'num_keypoints\'] == 0) or gt[\'ignore\']\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        for gt in gts:\n            self._gts[gt[\'image_id\'], gt[\'category_id\']].append(gt)\n        for dt in dts:\n            self._dts[dt[\'image_id\'], dt[\'category_id\']].append(dt)\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results\n        self.eval     = {}                  # accumulated evaluation results\n\n    def evaluate(self):\n        \'\'\'\n        Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n        :return: None\n        \'\'\'\n        tic = time.time()\n        print(\'Running per image evaluation...\')\n        p = self.params\n        # add backward compatibility if useSegm is specified in params\n        if not p.useSegm is None:\n            p.iouType = \'segm\' if p.useSegm == 1 else \'bbox\'\n            print(\'useSegm (deprecated) is not None. Running {} evaluation\'.format(p.iouType))\n        print(\'Evaluate annotation type *{}*\'.format(p.iouType))\n        p.imgIds = list(np.unique(p.imgIds))\n        if p.useCats:\n            p.catIds = list(np.unique(p.catIds))\n        p.maxDets = sorted(p.maxDets)\n        self.params=p\n\n        self._prepare()\n        # loop through images, area range, max detection number\n        catIds = p.catIds if p.useCats else [-1]\n\n        if p.iouType == \'segm\' or p.iouType == \'bbox\':\n            computeIoU = self.computeIoU\n        elif p.iouType == \'keypoints\':\n            computeIoU = self.computeOks\n        self.ious = {(imgId, catId): computeIoU(imgId, catId) \\\n                        for imgId in p.imgIds\n                        for catId in catIds}\n\n        evaluateImg = self.evaluateImg\n        maxDet = p.maxDets[-1]\n        self.evalImgs = [evaluateImg(imgId, catId, areaRng, maxDet)\n                 for catId in catIds\n                 for areaRng in p.areaRng\n                 for imgId in p.imgIds\n             ]\n        self._paramsEval = copy.deepcopy(self.params)\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format(toc-tic))\n\n    def computeIoU(self, imgId, catId):\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return []\n        inds = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in inds]\n        if len(dt) > p.maxDets[-1]:\n            dt=dt[0:p.maxDets[-1]]\n\n        if p.iouType == \'segm\':\n            g = [g[\'segmentation\'] for g in gt]\n            d = [d[\'segmentation\'] for d in dt]\n        elif p.iouType == \'bbox\':\n            g = [g[\'bbox\'] for g in gt]\n            d = [d[\'bbox\'] for d in dt]\n        else:\n            raise Exception(\'unknown iouType for iou computation\')\n\n        # compute iou between each dt and gt region\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        ious = maskUtils.iou(d,g,iscrowd)\n        return ious\n\n    def computeOks(self, imgId, catId):\n        p = self.params\n        # dimention here should be Nxm\n        gts = self._gts[imgId, catId]\n        dts = self._dts[imgId, catId]\n        inds = np.argsort([-d[\'score\'] for d in dts], kind=\'mergesort\')\n        dts = [dts[i] for i in inds]\n        if len(dts) > p.maxDets[-1]:\n            dts = dts[0:p.maxDets[-1]]\n        # if len(gts) == 0 and len(dts) == 0:\n        if len(gts) == 0 or len(dts) == 0:\n            return []\n        ious = np.zeros((len(dts), len(gts)))\n        sigmas = np.array([.26, .25, .25, .35, .35, .79, .79, .72, .72, .62,.62, 1.07, 1.07, .87, .87, .89, .89])/10.0\n        vars = (sigmas * 2)**2\n        k = len(sigmas)\n        # compute oks between each detection and ground truth object\n        for j, gt in enumerate(gts):\n            # create bounds for ignore regions(double the gt bbox)\n            g = np.array(gt[\'keypoints\'])\n            xg = g[0::3]; yg = g[1::3]; vg = g[2::3]\n            k1 = np.count_nonzero(vg > 0)\n            bb = gt[\'bbox\']\n            x0 = bb[0] - bb[2]; x1 = bb[0] + bb[2] * 2\n            y0 = bb[1] - bb[3]; y1 = bb[1] + bb[3] * 2\n            for i, dt in enumerate(dts):\n                d = np.array(dt[\'keypoints\'])\n                xd = d[0::3]; yd = d[1::3]\n                if k1>0:\n                    # measure the per-keypoint distance if keypoints visible\n                    dx = xd - xg\n                    dy = yd - yg\n                else:\n                    # measure minimum distance to keypoints in (x0,y0) & (x1,y1)\n                    z = np.zeros((k))\n                    dx = np.max((z, x0-xd),axis=0)+np.max((z, xd-x1),axis=0)\n                    dy = np.max((z, y0-yd),axis=0)+np.max((z, yd-y1),axis=0)\n                e = (dx**2 + dy**2) / vars / (gt[\'area\']+np.spacing(1)) / 2\n                if k1 > 0:\n                    e=e[vg > 0]\n                ious[i, j] = np.sum(np.exp(-e)) / e.shape[0]\n        return ious\n\n    def evaluateImg(self, imgId, catId, aRng, maxDet):\n        \'\'\'\n        perform evaluation for single category and image\n        :return: dict (single image results)\n        \'\'\'\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return None\n\n        for g in gt:\n            if g[\'ignore\'] or (g[\'area\']<aRng[0] or g[\'area\']>aRng[1]):\n                g[\'_ignore\'] = 1\n            else:\n                g[\'_ignore\'] = 0\n\n        # sort dt highest score first, sort gt ignore last\n        gtind = np.argsort([g[\'_ignore\'] for g in gt], kind=\'mergesort\')\n        gt = [gt[i] for i in gtind]\n        dtind = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in dtind[0:maxDet]]\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        # load computed ious\n        ious = self.ious[imgId, catId][:, gtind] if len(self.ious[imgId, catId]) > 0 else self.ious[imgId, catId]\n\n        T = len(p.iouThrs)\n        G = len(gt)\n        D = len(dt)\n        gtm  = np.zeros((T,G))\n        dtm  = np.zeros((T,D))\n        gtIg = np.array([g[\'_ignore\'] for g in gt])\n        dtIg = np.zeros((T,D))\n        if not len(ious)==0:\n            for tind, t in enumerate(p.iouThrs):\n                for dind, d in enumerate(dt):\n                    # information about best match so far (m=-1 -> unmatched)\n                    iou = min([t,1-1e-10])\n                    m   = -1\n                    for gind, g in enumerate(gt):\n                        # if this gt already matched, and not a crowd, continue\n                        if gtm[tind,gind]>0 and not iscrowd[gind]:\n                            continue\n                        # if dt matched to reg gt, and on ignore gt, stop\n                        if m>-1 and gtIg[m]==0 and gtIg[gind]==1:\n                            break\n                        # continue to next gt unless better match made\n                        if ious[dind,gind] < iou:\n                            continue\n                        # if match successful and best so far, store appropriately\n                        iou=ious[dind,gind]\n                        m=gind\n                    # if match made store id of match for both dt and gt\n                    if m ==-1:\n                        continue\n                    dtIg[tind,dind] = gtIg[m]\n                    dtm[tind,dind]  = gt[m][\'id\']\n                    gtm[tind,m]     = d[\'id\']\n        # set unmatched detections outside of area range to ignore\n        a = np.array([d[\'area\']<aRng[0] or d[\'area\']>aRng[1] for d in dt]).reshape((1, len(dt)))\n        dtIg = np.logical_or(dtIg, np.logical_and(dtm==0, np.repeat(a,T,0)))\n        # store results for given image and category\n        return {\n                \'image_id\':     imgId,\n                \'category_id\':  catId,\n                \'aRng\':         aRng,\n                \'maxDet\':       maxDet,\n                \'dtIds\':        [d[\'id\'] for d in dt],\n                \'gtIds\':        [g[\'id\'] for g in gt],\n                \'dtMatches\':    dtm,\n                \'gtMatches\':    gtm,\n                \'dtScores\':     [d[\'score\'] for d in dt],\n                \'gtIgnore\':     gtIg,\n                \'dtIgnore\':     dtIg,\n            }\n\n    def accumulate(self, p = None):\n        \'\'\'\n        Accumulate per image evaluation results and store the result in self.eval\n        :param p: input params for evaluation\n        :return: None\n        \'\'\'\n        print(\'Accumulating evaluation results...\')\n        tic = time.time()\n        if not self.evalImgs:\n            print(\'Please run evaluate() first\')\n        # allows input customized parameters\n        if p is None:\n            p = self.params\n        p.catIds = p.catIds if p.useCats == 1 else [-1]\n        T           = len(p.iouThrs)\n        R           = len(p.recThrs)\n        K           = len(p.catIds) if p.useCats else 1\n        A           = len(p.areaRng)\n        M           = len(p.maxDets)\n        precision   = -np.ones((T,R,K,A,M)) # -1 for the precision of absent categories\n        recall      = -np.ones((T,K,A,M))\n\n        # create dictionary for future indexing\n        _pe = self._paramsEval\n        catIds = _pe.catIds if _pe.useCats else [-1]\n        setK = set(catIds)\n        setA = set(map(tuple, _pe.areaRng))\n        setM = set(_pe.maxDets)\n        setI = set(_pe.imgIds)\n        # get inds to evaluate\n        k_list = [n for n, k in enumerate(p.catIds)  if k in setK]\n        m_list = [m for n, m in enumerate(p.maxDets) if m in setM]\n        a_list = [n for n, a in enumerate(map(lambda x: tuple(x), p.areaRng)) if a in setA]\n        i_list = [n for n, i in enumerate(p.imgIds)  if i in setI]\n        I0 = len(_pe.imgIds)\n        A0 = len(_pe.areaRng)\n        # retrieve E at each category, area range, and max number of detections\n        for k, k0 in enumerate(k_list):\n            Nk = k0*A0*I0\n            for a, a0 in enumerate(a_list):\n                Na = a0*I0\n                for m, maxDet in enumerate(m_list):\n                    E = [self.evalImgs[Nk + Na + i] for i in i_list]\n                    E = [e for e in E if not e is None]\n                    if len(E) == 0:\n                        continue\n                    dtScores = np.concatenate([e[\'dtScores\'][0:maxDet] for e in E])\n\n                    # different sorting method generates slightly different results.\n                    # mergesort is used to be consistent as Matlab implementation.\n                    inds = np.argsort(-dtScores, kind=\'mergesort\')\n\n                    dtm  = np.concatenate([e[\'dtMatches\'][:,0:maxDet] for e in E], axis=1)[:,inds]\n                    dtIg = np.concatenate([e[\'dtIgnore\'][:,0:maxDet]  for e in E], axis=1)[:,inds]\n                    gtIg = np.concatenate([e[\'gtIgnore\'] for e in E])\n                    npig = np.count_nonzero(gtIg==0 )\n                    if npig == 0:\n                        continue\n                    tps = np.logical_and(               dtm,  np.logical_not(dtIg) )\n                    fps = np.logical_and(np.logical_not(dtm), np.logical_not(dtIg) )\n\n                    tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\n                    fp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)\n                    for t, (tp, fp) in enumerate(zip(tp_sum, fp_sum)):\n                        tp = np.array(tp)\n                        fp = np.array(fp)\n                        nd = len(tp)\n                        rc = tp / npig\n                        pr = tp / (fp+tp+np.spacing(1))\n                        q  = np.zeros((R,))\n\n                        if nd:\n                            recall[t,k,a,m] = rc[-1]\n                        else:\n                            recall[t,k,a,m] = 0\n\n                        # numpy is slow without cython optimization for accessing elements\n                        # use python array gets significant speed improvement\n                        pr = pr.tolist(); q = q.tolist()\n\n                        for i in range(nd-1, 0, -1):\n                            if pr[i] > pr[i-1]:\n                                pr[i-1] = pr[i]\n\n                        inds = np.searchsorted(rc, p.recThrs, side=\'left\')\n                        try:\n                            for ri, pi in enumerate(inds):\n                                q[ri] = pr[pi]\n                        except:\n                            pass\n                        precision[t,:,k,a,m] = np.array(q)\n        self.eval = {\n            \'params\': p,\n            \'counts\': [T, R, K, A, M],\n            \'date\': datetime.datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\'),\n            \'precision\': precision,\n            \'recall\':   recall,\n        }\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format( toc-tic))\n\n    def summarize(self):\n        \'\'\'\n        Compute and display summary metrics for evaluation results.\n        Note this functin can *only* be applied on the default parameter setting\n        \'\'\'\n        def _summarize( ap=1, iouThr=None, areaRng=\'all\', maxDets=100 ):\n            p = self.params\n            iStr = \' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}\'\n            titleStr = \'Average Precision\' if ap == 1 else \'Average Recall\'\n            typeStr = \'(AP)\' if ap==1 else \'(AR)\'\n            iouStr = \'{:0.2f}:{:0.2f}\'.format(p.iouThrs[0], p.iouThrs[-1]) \\\n                if iouThr is None else \'{:0.2f}\'.format(iouThr)\n\n            aind = [i for i, aRng in enumerate(p.areaRngLbl) if aRng == areaRng]\n            mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]\n            if ap == 1:\n                # dimension of precision: [TxRxKxAxM]\n                s = self.eval[\'precision\']\n                # IoU\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,:,aind,mind]\n            else:\n                # dimension of recall: [TxKxAxM]\n                s = self.eval[\'recall\']\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,aind,mind]\n            if len(s[s>-1])==0:\n                mean_s = -1\n            else:\n                mean_s = np.mean(s[s>-1])\n            print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n            return mean_s\n        def _summarizeDets():\n            stats = np.zeros((12,))\n            stats[0] = _summarize(1)\n            stats[1] = _summarize(1, iouThr=.5, maxDets=self.params.maxDets[2])\n            stats[2] = _summarize(1, iouThr=.75, maxDets=self.params.maxDets[2])\n            stats[3] = _summarize(1, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[4] = _summarize(1, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[5] = _summarize(1, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            stats[6] = _summarize(0, maxDets=self.params.maxDets[0])\n            stats[7] = _summarize(0, maxDets=self.params.maxDets[1])\n            stats[8] = _summarize(0, maxDets=self.params.maxDets[2])\n            stats[9] = _summarize(0, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[10] = _summarize(0, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[11] = _summarize(0, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            return stats\n        def _summarizeKps():\n            stats = np.zeros((10,))\n            stats[0] = _summarize(1, maxDets=20)\n            stats[1] = _summarize(1, maxDets=20, iouThr=.5)\n            stats[2] = _summarize(1, maxDets=20, iouThr=.75)\n            stats[3] = _summarize(1, maxDets=20, areaRng=\'medium\')\n            stats[4] = _summarize(1, maxDets=20, areaRng=\'large\')\n            stats[5] = _summarize(0, maxDets=20)\n            stats[6] = _summarize(0, maxDets=20, iouThr=.5)\n            stats[7] = _summarize(0, maxDets=20, iouThr=.75)\n            stats[8] = _summarize(0, maxDets=20, areaRng=\'medium\')\n            stats[9] = _summarize(0, maxDets=20, areaRng=\'large\')\n            return stats\n        if not self.eval:\n            raise Exception(\'Please run accumulate() first\')\n        iouType = self.params.iouType\n        if iouType == \'segm\' or iouType == \'bbox\':\n            summarize = _summarizeDets\n        elif iouType == \'keypoints\':\n            summarize = _summarizeKps\n        self.stats = summarize()\n\n    def __str__(self):\n        self.summarize()\n\nclass Params:\n    \'\'\'\n    Params for coco evaluation api\n    \'\'\'\n    def setDetParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [1, 10, 100]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [0 ** 2, 32 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'small\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def setKpParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [20]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def __init__(self, iouType=\'segm\'):\n        if iouType == \'segm\' or iouType == \'bbox\':\n            self.setDetParams()\n        elif iouType == \'keypoints\':\n            self.setKpParams()\n        else:\n            raise Exception(\'iouType not supported\')\n        self.iouType = iouType\n        # useSegm is deprecated\n        self.useSegm = None'"
Dataset/coco/pycocotools/mask.py,0,"b'__author__ = \'tsungyi\'\n\nimport Dataset.coco.pycocotools._mask as _mask\n\n# Interface for manipulating masks stored in RLE format.\n#\n# RLE is a simple yet efficient format for storing binary masks. RLE\n# first divides a vector (or vectorized image) into a series of piecewise\n# constant regions and then for each piece simply stores the length of\n# that piece. For example, given M=[0 0 1 1 1 0 1] the RLE counts would\n# be [2 3 1 1], or for M=[1 1 1 1 1 1 0] the counts would be [0 6 1]\n# (note that the odd counts are always the numbers of zeros). Instead of\n# storing the counts directly, additional compression is achieved with a\n# variable bitrate representation based on a common scheme called LEB128.\n#\n# Compression is greatest given large piecewise constant regions.\n# Specifically, the size of the RLE is proportional to the number of\n# *boundaries* in M (or for an image the number of boundaries in the y\n# direction). Assuming fairly simple shapes, the RLE representation is\n# O(sqrt(n)) where n is number of pixels in the object. Hence space usage\n# is substantially lower, especially for large simple objects (large n).\n#\n# Many common operations on masks can be computed directly using the RLE\n# (without need for decoding). This includes computations such as area,\n# union, intersection, etc. All of these operations are linear in the\n# size of the RLE, in other words they are O(sqrt(n)) where n is the area\n# of the object. Computing these operations on the original mask is O(n).\n# Thus, using the RLE can result in substantial computational savings.\n#\n# The following API functions are defined:\n#  encode         - Encode binary masks using RLE.\n#  decode         - Decode binary masks encoded via RLE.\n#  merge          - Compute union or intersection of encoded masks.\n#  iou            - Compute intersection over union between masks.\n#  area           - Compute area of encoded masks.\n#  toBbox         - Get bounding boxes surrounding encoded masks.\n#  frPyObjects    - Convert polygon, bbox, and uncompressed RLE to encoded RLE mask.\n#\n# Usage:\n#  Rs     = encode( masks )\n#  masks  = decode( Rs )\n#  R      = merge( Rs, intersect=false )\n#  o      = iou( dt, gt, iscrowd )\n#  a      = area( Rs )\n#  bbs    = toBbox( Rs )\n#  Rs     = frPyObjects( [pyObjects], h, w )\n#\n# In the API the following formats are used:\n#  Rs      - [dict] Run-length encoding of binary masks\n#  R       - dict Run-length encoding of binary mask\n#  masks   - [hxwxn] Binary mask(s) (must have type np.ndarray(dtype=uint8) in column-major order)\n#  iscrowd - [nx1] list of np.ndarray. 1 indicates corresponding gt image has crowd region to ignore\n#  bbs     - [nx4] Bounding box(es) stored as [x y w h]\n#  poly    - Polygon stored as [[x1 y1 x2 y2...],[x1 y1 ...],...] (2D list)\n#  dt,gt   - May be either bounding boxes or encoded masks\n# Both poly and bbs are 0-indexed (bbox=[0 0 1 1] encloses first pixel).\n#\n# Finally, a note about the intersection over union (iou) computation.\n# The standard iou of a ground truth (gt) and detected (dt) object is\n#  iou(gt,dt) = area(intersect(gt,dt)) / area(union(gt,dt))\n# For ""crowd"" regions, we use a modified criteria. If a gt object is\n# marked as ""iscrowd"", we allow a dt to match any subregion of the gt.\n# Choosing gt\' in the crowd gt that best matches the dt can be done using\n# gt\'=intersect(dt,gt). Since by definition union(gt\',dt)=dt, computing\n#  iou(gt,dt,iscrowd) = iou(gt\',dt) = area(intersect(gt,dt)) / area(dt)\n# For crowd gt regions we use this modified criteria above for the iou.\n#\n# To compile run ""python setup.py build_ext --inplace""\n# Please do not contact us for help with compiling.\n#\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n# Licensed under the Simplified BSD License [see coco/license.txt]\n\niou         = _mask.iou\nmerge       = _mask.merge\nfrPyObjects = _mask.frPyObjects\n\ndef encode(bimask):\n    if len(bimask.shape) == 3:\n        return _mask.encode(bimask)\n    elif len(bimask.shape) == 2:\n        h, w = bimask.shape\n        return _mask.encode(bimask.reshape((h, w, 1), order=\'F\'))[0]\n\ndef decode(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.decode(rleObjs)\n    else:\n        return _mask.decode([rleObjs])[:,:,0]\n\ndef area(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.area(rleObjs)\n    else:\n        return _mask.area([rleObjs])[0]\n\ndef toBbox(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.toBbox(rleObjs)\n    else:\n        return _mask.toBbox([rleObjs])[0]'"
