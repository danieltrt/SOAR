file_path,api_count,code
main.py,4,"b""import argparse\nimport os\nimport tensorflow as tf\ntf.set_random_seed(19)\nfrom model import cyclegan\n\nparser = argparse.ArgumentParser(description='')\nparser.add_argument('--dataset_dir', dest='dataset_dir', default='horse2zebra', help='path of the dataset')\nparser.add_argument('--epoch', dest='epoch', type=int, default=200, help='# of epoch')\nparser.add_argument('--epoch_step', dest='epoch_step', type=int, default=100, help='# of epoch to decay lr')\nparser.add_argument('--batch_size', dest='batch_size', type=int, default=1, help='# images in batch')\nparser.add_argument('--train_size', dest='train_size', type=int, default=1e8, help='# images used to train')\nparser.add_argument('--load_size', dest='load_size', type=int, default=286, help='scale images to this size')\nparser.add_argument('--fine_size', dest='fine_size', type=int, default=256, help='then crop to this size')\nparser.add_argument('--ngf', dest='ngf', type=int, default=64, help='# of gen filters in first conv layer')\nparser.add_argument('--ndf', dest='ndf', type=int, default=64, help='# of discri filters in first conv layer')\nparser.add_argument('--input_nc', dest='input_nc', type=int, default=3, help='# of input image channels')\nparser.add_argument('--output_nc', dest='output_nc', type=int, default=3, help='# of output image channels')\nparser.add_argument('--lr', dest='lr', type=float, default=0.0002, help='initial learning rate for adam')\nparser.add_argument('--beta1', dest='beta1', type=float, default=0.5, help='momentum term of adam')\nparser.add_argument('--which_direction', dest='which_direction', default='AtoB', help='AtoB or BtoA')\nparser.add_argument('--phase', dest='phase', default='train', help='train, test')\nparser.add_argument('--save_freq', dest='save_freq', type=int, default=1000, help='save a model every save_freq iterations')\nparser.add_argument('--print_freq', dest='print_freq', type=int, default=100, help='print the debug information every print_freq iterations')\nparser.add_argument('--continue_train', dest='continue_train', type=bool, default=False, help='if continue training, load the latest model: 1: true, 0: false')\nparser.add_argument('--checkpoint_dir', dest='checkpoint_dir', default='./checkpoint', help='models are saved here')\nparser.add_argument('--sample_dir', dest='sample_dir', default='./sample', help='sample are saved here')\nparser.add_argument('--test_dir', dest='test_dir', default='./test', help='test sample are saved here')\nparser.add_argument('--L1_lambda', dest='L1_lambda', type=float, default=10.0, help='weight on L1 term in objective')\nparser.add_argument('--use_resnet', dest='use_resnet', type=bool, default=True, help='generation network using reidule block')\nparser.add_argument('--use_lsgan', dest='use_lsgan', type=bool, default=True, help='gan loss defined in lsgan')\nparser.add_argument('--max_size', dest='max_size', type=int, default=50, help='max size of image pool, 0 means do not use image pool')\n\nargs = parser.parse_args()\n\n\ndef main(_):\n    if not os.path.exists(args.checkpoint_dir):\n        os.makedirs(args.checkpoint_dir)\n    if not os.path.exists(args.sample_dir):\n        os.makedirs(args.sample_dir)\n    if not os.path.exists(args.test_dir):\n        os.makedirs(args.test_dir)\n\n    tfconfig = tf.ConfigProto(allow_soft_placement=True)\n    tfconfig.gpu_options.allow_growth = True\n    with tf.Session(config=tfconfig) as sess:\n        model = cyclegan(sess, args)\n        model.train(args) if args.phase == 'train' \\\n            else model.test(args)\n\nif __name__ == '__main__':\n    tf.app.run()\n"""
model.py,34,"b'from __future__ import division\nimport os\nimport time\nfrom glob import glob\nimport tensorflow as tf\nimport numpy as np\nfrom collections import namedtuple\n\nfrom module import *\nfrom utils import *\n\n\nclass cyclegan(object):\n    def __init__(self, sess, args):\n        self.sess = sess\n        self.batch_size = args.batch_size\n        self.image_size = args.fine_size\n        self.input_c_dim = args.input_nc\n        self.output_c_dim = args.output_nc\n        self.L1_lambda = args.L1_lambda\n        self.dataset_dir = args.dataset_dir\n\n        self.discriminator = discriminator\n        if args.use_resnet:\n            self.generator = generator_resnet\n        else:\n            self.generator = generator_unet\n        if args.use_lsgan:\n            self.criterionGAN = mae_criterion\n        else:\n            self.criterionGAN = sce_criterion\n\n        OPTIONS = namedtuple(\'OPTIONS\', \'batch_size image_size \\\n                              gf_dim df_dim output_c_dim is_training\')\n        self.options = OPTIONS._make((args.batch_size, args.fine_size,\n                                      args.ngf, args.ndf, args.output_nc,\n                                      args.phase == \'train\'))\n\n        self._build_model()\n        self.saver = tf.train.Saver()\n        self.pool = ImagePool(args.max_size)\n\n    def _build_model(self):\n        self.real_data = tf.placeholder(tf.float32,\n                                        [None, self.image_size, self.image_size,\n                                         self.input_c_dim + self.output_c_dim],\n                                        name=\'real_A_and_B_images\')\n\n        self.real_A = self.real_data[:, :, :, :self.input_c_dim]\n        self.real_B = self.real_data[:, :, :, self.input_c_dim:self.input_c_dim + self.output_c_dim]\n\n        self.fake_B = self.generator(self.real_A, self.options, False, name=""generatorA2B"")\n        self.fake_A_ = self.generator(self.fake_B, self.options, False, name=""generatorB2A"")\n        self.fake_A = self.generator(self.real_B, self.options, True, name=""generatorB2A"")\n        self.fake_B_ = self.generator(self.fake_A, self.options, True, name=""generatorA2B"")\n\n        self.DB_fake = self.discriminator(self.fake_B, self.options, reuse=False, name=""discriminatorB"")\n        self.DA_fake = self.discriminator(self.fake_A, self.options, reuse=False, name=""discriminatorA"")\n        self.g_loss_a2b = self.criterionGAN(self.DB_fake, tf.ones_like(self.DB_fake)) \\\n            + self.L1_lambda * abs_criterion(self.real_A, self.fake_A_) \\\n            + self.L1_lambda * abs_criterion(self.real_B, self.fake_B_)\n        self.g_loss_b2a = self.criterionGAN(self.DA_fake, tf.ones_like(self.DA_fake)) \\\n            + self.L1_lambda * abs_criterion(self.real_A, self.fake_A_) \\\n            + self.L1_lambda * abs_criterion(self.real_B, self.fake_B_)\n        self.g_loss = self.criterionGAN(self.DA_fake, tf.ones_like(self.DA_fake)) \\\n            + self.criterionGAN(self.DB_fake, tf.ones_like(self.DB_fake)) \\\n            + self.L1_lambda * abs_criterion(self.real_A, self.fake_A_) \\\n            + self.L1_lambda * abs_criterion(self.real_B, self.fake_B_)\n\n        self.fake_A_sample = tf.placeholder(tf.float32,\n                                            [None, self.image_size, self.image_size,\n                                             self.input_c_dim], name=\'fake_A_sample\')\n        self.fake_B_sample = tf.placeholder(tf.float32,\n                                            [None, self.image_size, self.image_size,\n                                             self.output_c_dim], name=\'fake_B_sample\')\n        self.DB_real = self.discriminator(self.real_B, self.options, reuse=True, name=""discriminatorB"")\n        self.DA_real = self.discriminator(self.real_A, self.options, reuse=True, name=""discriminatorA"")\n        self.DB_fake_sample = self.discriminator(self.fake_B_sample, self.options, reuse=True, name=""discriminatorB"")\n        self.DA_fake_sample = self.discriminator(self.fake_A_sample, self.options, reuse=True, name=""discriminatorA"")\n\n        self.db_loss_real = self.criterionGAN(self.DB_real, tf.ones_like(self.DB_real))\n        self.db_loss_fake = self.criterionGAN(self.DB_fake_sample, tf.zeros_like(self.DB_fake_sample))\n        self.db_loss = (self.db_loss_real + self.db_loss_fake) / 2\n        self.da_loss_real = self.criterionGAN(self.DA_real, tf.ones_like(self.DA_real))\n        self.da_loss_fake = self.criterionGAN(self.DA_fake_sample, tf.zeros_like(self.DA_fake_sample))\n        self.da_loss = (self.da_loss_real + self.da_loss_fake) / 2\n        self.d_loss = self.da_loss + self.db_loss\n\n        self.g_loss_a2b_sum = tf.summary.scalar(""g_loss_a2b"", self.g_loss_a2b)\n        self.g_loss_b2a_sum = tf.summary.scalar(""g_loss_b2a"", self.g_loss_b2a)\n        self.g_loss_sum = tf.summary.scalar(""g_loss"", self.g_loss)\n        self.g_sum = tf.summary.merge([self.g_loss_a2b_sum, self.g_loss_b2a_sum, self.g_loss_sum])\n        self.db_loss_sum = tf.summary.scalar(""db_loss"", self.db_loss)\n        self.da_loss_sum = tf.summary.scalar(""da_loss"", self.da_loss)\n        self.d_loss_sum = tf.summary.scalar(""d_loss"", self.d_loss)\n        self.db_loss_real_sum = tf.summary.scalar(""db_loss_real"", self.db_loss_real)\n        self.db_loss_fake_sum = tf.summary.scalar(""db_loss_fake"", self.db_loss_fake)\n        self.da_loss_real_sum = tf.summary.scalar(""da_loss_real"", self.da_loss_real)\n        self.da_loss_fake_sum = tf.summary.scalar(""da_loss_fake"", self.da_loss_fake)\n        self.d_sum = tf.summary.merge(\n            [self.da_loss_sum, self.da_loss_real_sum, self.da_loss_fake_sum,\n             self.db_loss_sum, self.db_loss_real_sum, self.db_loss_fake_sum,\n             self.d_loss_sum]\n        )\n\n        self.test_A = tf.placeholder(tf.float32,\n                                     [None, self.image_size, self.image_size,\n                                      self.input_c_dim], name=\'test_A\')\n        self.test_B = tf.placeholder(tf.float32,\n                                     [None, self.image_size, self.image_size,\n                                      self.output_c_dim], name=\'test_B\')\n        self.testB = self.generator(self.test_A, self.options, True, name=""generatorA2B"")\n        self.testA = self.generator(self.test_B, self.options, True, name=""generatorB2A"")\n\n        t_vars = tf.trainable_variables()\n        self.d_vars = [var for var in t_vars if \'discriminator\' in var.name]\n        self.g_vars = [var for var in t_vars if \'generator\' in var.name]\n        for var in t_vars: print(var.name)\n\n    def train(self, args):\n        """"""Train cyclegan""""""\n        self.lr = tf.placeholder(tf.float32, None, name=\'learning_rate\')\n        self.d_optim = tf.train.AdamOptimizer(self.lr, beta1=args.beta1) \\\n            .minimize(self.d_loss, var_list=self.d_vars)\n        self.g_optim = tf.train.AdamOptimizer(self.lr, beta1=args.beta1) \\\n            .minimize(self.g_loss, var_list=self.g_vars)\n\n        init_op = tf.global_variables_initializer()\n        self.sess.run(init_op)\n        self.writer = tf.summary.FileWriter(""./logs"", self.sess.graph)\n\n        counter = 1\n        start_time = time.time()\n\n        if args.continue_train:\n            if self.load(args.checkpoint_dir):\n                print("" [*] Load SUCCESS"")\n            else:\n                print("" [!] Load failed..."")\n\n        for epoch in range(args.epoch):\n            dataA = glob(\'./datasets/{}/*.*\'.format(self.dataset_dir + \'/trainA\'))\n            dataB = glob(\'./datasets/{}/*.*\'.format(self.dataset_dir + \'/trainB\'))\n            np.random.shuffle(dataA)\n            np.random.shuffle(dataB)\n            batch_idxs = min(min(len(dataA), len(dataB)), args.train_size) // self.batch_size\n            lr = args.lr if epoch < args.epoch_step else args.lr*(args.epoch-epoch)/(args.epoch-args.epoch_step)\n\n            for idx in range(0, batch_idxs):\n                batch_files = list(zip(dataA[idx * self.batch_size:(idx + 1) * self.batch_size],\n                                       dataB[idx * self.batch_size:(idx + 1) * self.batch_size]))\n                batch_images = [load_train_data(batch_file, args.load_size, args.fine_size) for batch_file in batch_files]\n                batch_images = np.array(batch_images).astype(np.float32)\n\n                # Update G network and record fake outputs\n                fake_A, fake_B, _, summary_str = self.sess.run(\n                    [self.fake_A, self.fake_B, self.g_optim, self.g_sum],\n                    feed_dict={self.real_data: batch_images, self.lr: lr})\n                self.writer.add_summary(summary_str, counter)\n                [fake_A, fake_B] = self.pool([fake_A, fake_B])\n\n                # Update D network\n                _, summary_str = self.sess.run(\n                    [self.d_optim, self.d_sum],\n                    feed_dict={self.real_data: batch_images,\n                               self.fake_A_sample: fake_A,\n                               self.fake_B_sample: fake_B,\n                               self.lr: lr})\n                self.writer.add_summary(summary_str, counter)\n\n                counter += 1\n                print((""Epoch: [%2d] [%4d/%4d] time: %4.4f"" % (\n                    epoch, idx, batch_idxs, time.time() - start_time)))\n\n                if np.mod(counter, args.print_freq) == 1:\n                    self.sample_model(args.sample_dir, epoch, idx)\n\n                if np.mod(counter, args.save_freq) == 2:\n                    self.save(args.checkpoint_dir, counter)\n\n    def save(self, checkpoint_dir, step):\n        model_name = ""cyclegan.model""\n        model_dir = ""%s_%s"" % (self.dataset_dir, self.image_size)\n        checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n\n        if not os.path.exists(checkpoint_dir):\n            os.makedirs(checkpoint_dir)\n\n        self.saver.save(self.sess,\n                        os.path.join(checkpoint_dir, model_name),\n                        global_step=step)\n\n    def load(self, checkpoint_dir):\n        print("" [*] Reading checkpoint..."")\n\n        model_dir = ""%s_%s"" % (self.dataset_dir, self.image_size)\n        checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n\n        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n        if ckpt and ckpt.model_checkpoint_path:\n            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n            return True\n        else:\n            return False\n\n    def sample_model(self, sample_dir, epoch, idx):\n        dataA = glob(\'./datasets/{}/*.*\'.format(self.dataset_dir + \'/testA\'))\n        dataB = glob(\'./datasets/{}/*.*\'.format(self.dataset_dir + \'/testB\'))\n        np.random.shuffle(dataA)\n        np.random.shuffle(dataB)\n        batch_files = list(zip(dataA[:self.batch_size], dataB[:self.batch_size]))\n        sample_images = [load_train_data(batch_file, is_testing=True) for batch_file in batch_files]\n        sample_images = np.array(sample_images).astype(np.float32)\n\n        fake_A, fake_B = self.sess.run(\n            [self.fake_A, self.fake_B],\n            feed_dict={self.real_data: sample_images}\n        )\n        save_images(fake_A, [self.batch_size, 1],\n                    \'./{}/A_{:02d}_{:04d}.jpg\'.format(sample_dir, epoch, idx))\n        save_images(fake_B, [self.batch_size, 1],\n                    \'./{}/B_{:02d}_{:04d}.jpg\'.format(sample_dir, epoch, idx))\n\n    def test(self, args):\n        """"""Test cyclegan""""""\n        init_op = tf.global_variables_initializer()\n        self.sess.run(init_op)\n        if args.which_direction == \'AtoB\':\n            sample_files = glob(\'./datasets/{}/*.*\'.format(self.dataset_dir + \'/testA\'))\n        elif args.which_direction == \'BtoA\':\n            sample_files = glob(\'./datasets/{}/*.*\'.format(self.dataset_dir + \'/testB\'))\n        else:\n            raise Exception(\'--which_direction must be AtoB or BtoA\')\n\n        if self.load(args.checkpoint_dir):\n            print("" [*] Load SUCCESS"")\n        else:\n            print("" [!] Load failed..."")\n\n        # write html for visual comparison\n        index_path = os.path.join(args.test_dir, \'{0}_index.html\'.format(args.which_direction))\n        index = open(index_path, ""w"")\n        index.write(""<html><body><table><tr>"")\n        index.write(""<th>name</th><th>input</th><th>output</th></tr>"")\n\n        out_var, in_var = (self.testB, self.test_A) if args.which_direction == \'AtoB\' else (\n            self.testA, self.test_B)\n\n        for sample_file in sample_files:\n            print(\'Processing image: \' + sample_file)\n            sample_image = [load_test_data(sample_file, args.fine_size)]\n            sample_image = np.array(sample_image).astype(np.float32)\n            image_path = os.path.join(args.test_dir,\n                                      \'{0}_{1}\'.format(args.which_direction, os.path.basename(sample_file)))\n            fake_img = self.sess.run(out_var, feed_dict={in_var: sample_image})\n            save_images(fake_img, [1, 1], image_path)\n            index.write(""<td>%s</td>"" % os.path.basename(image_path))\n            index.write(""<td><img src=\'%s\'></td>"" % (sample_file if os.path.isabs(sample_file) else (\n                \'..\' + os.path.sep + sample_file)))\n            index.write(""<td><img src=\'%s\'></td>"" % (image_path if os.path.isabs(image_path) else (\n                \'..\' + os.path.sep + image_path)))\n            index.write(""</tr>"")\n        index.close()\n'"
module.py,41,"b'from __future__ import division\nimport tensorflow as tf\nfrom ops import *\nfrom utils import *\n\n\ndef discriminator(image, options, reuse=False, name=""discriminator""):\n\n    with tf.variable_scope(name):\n        # image is 256 x 256 x input_c_dim\n        if reuse:\n            tf.get_variable_scope().reuse_variables()\n        else:\n            assert tf.get_variable_scope().reuse is False\n\n        h0 = lrelu(conv2d(image, options.df_dim, name=\'d_h0_conv\'))\n        # h0 is (128 x 128 x self.df_dim)\n        h1 = lrelu(instance_norm(conv2d(h0, options.df_dim*2, name=\'d_h1_conv\'), \'d_bn1\'))\n        # h1 is (64 x 64 x self.df_dim*2)\n        h2 = lrelu(instance_norm(conv2d(h1, options.df_dim*4, name=\'d_h2_conv\'), \'d_bn2\'))\n        # h2 is (32x 32 x self.df_dim*4)\n        h3 = lrelu(instance_norm(conv2d(h2, options.df_dim*8, s=1, name=\'d_h3_conv\'), \'d_bn3\'))\n        # h3 is (32 x 32 x self.df_dim*8)\n        h4 = conv2d(h3, 1, s=1, name=\'d_h3_pred\')\n        # h4 is (32 x 32 x 1)\n        return h4\n\n\ndef generator_unet(image, options, reuse=False, name=""generator""):\n\n    dropout_rate = 0.5 if options.is_training else 1.0\n    with tf.variable_scope(name):\n        # image is 256 x 256 x input_c_dim\n        if reuse:\n            tf.get_variable_scope().reuse_variables()\n        else:\n            assert tf.get_variable_scope().reuse is False\n\n        # image is (256 x 256 x input_c_dim)\n        e1 = instance_norm(conv2d(image, options.gf_dim, name=\'g_e1_conv\'))\n        # e1 is (128 x 128 x self.gf_dim)\n        e2 = instance_norm(conv2d(lrelu(e1), options.gf_dim*2, name=\'g_e2_conv\'), \'g_bn_e2\')\n        # e2 is (64 x 64 x self.gf_dim*2)\n        e3 = instance_norm(conv2d(lrelu(e2), options.gf_dim*4, name=\'g_e3_conv\'), \'g_bn_e3\')\n        # e3 is (32 x 32 x self.gf_dim*4)\n        e4 = instance_norm(conv2d(lrelu(e3), options.gf_dim*8, name=\'g_e4_conv\'), \'g_bn_e4\')\n        # e4 is (16 x 16 x self.gf_dim*8)\n        e5 = instance_norm(conv2d(lrelu(e4), options.gf_dim*8, name=\'g_e5_conv\'), \'g_bn_e5\')\n        # e5 is (8 x 8 x self.gf_dim*8)\n        e6 = instance_norm(conv2d(lrelu(e5), options.gf_dim*8, name=\'g_e6_conv\'), \'g_bn_e6\')\n        # e6 is (4 x 4 x self.gf_dim*8)\n        e7 = instance_norm(conv2d(lrelu(e6), options.gf_dim*8, name=\'g_e7_conv\'), \'g_bn_e7\')\n        # e7 is (2 x 2 x self.gf_dim*8)\n        e8 = instance_norm(conv2d(lrelu(e7), options.gf_dim*8, name=\'g_e8_conv\'), \'g_bn_e8\')\n        # e8 is (1 x 1 x self.gf_dim*8)\n\n        d1 = deconv2d(tf.nn.relu(e8), options.gf_dim*8, name=\'g_d1\')\n        d1 = tf.nn.dropout(d1, dropout_rate)\n        d1 = tf.concat([instance_norm(d1, \'g_bn_d1\'), e7], 3)\n        # d1 is (2 x 2 x self.gf_dim*8*2)\n\n        d2 = deconv2d(tf.nn.relu(d1), options.gf_dim*8, name=\'g_d2\')\n        d2 = tf.nn.dropout(d2, dropout_rate)\n        d2 = tf.concat([instance_norm(d2, \'g_bn_d2\'), e6], 3)\n        # d2 is (4 x 4 x self.gf_dim*8*2)\n\n        d3 = deconv2d(tf.nn.relu(d2), options.gf_dim*8, name=\'g_d3\')\n        d3 = tf.nn.dropout(d3, dropout_rate)\n        d3 = tf.concat([instance_norm(d3, \'g_bn_d3\'), e5], 3)\n        # d3 is (8 x 8 x self.gf_dim*8*2)\n\n        d4 = deconv2d(tf.nn.relu(d3), options.gf_dim*8, name=\'g_d4\')\n        d4 = tf.concat([instance_norm(d4, \'g_bn_d4\'), e4], 3)\n        # d4 is (16 x 16 x self.gf_dim*8*2)\n\n        d5 = deconv2d(tf.nn.relu(d4), options.gf_dim*4, name=\'g_d5\')\n        d5 = tf.concat([instance_norm(d5, \'g_bn_d5\'), e3], 3)\n        # d5 is (32 x 32 x self.gf_dim*4*2)\n\n        d6 = deconv2d(tf.nn.relu(d5), options.gf_dim*2, name=\'g_d6\')\n        d6 = tf.concat([instance_norm(d6, \'g_bn_d6\'), e2], 3)\n        # d6 is (64 x 64 x self.gf_dim*2*2)\n\n        d7 = deconv2d(tf.nn.relu(d6), options.gf_dim, name=\'g_d7\')\n        d7 = tf.concat([instance_norm(d7, \'g_bn_d7\'), e1], 3)\n        # d7 is (128 x 128 x self.gf_dim*1*2)\n\n        d8 = deconv2d(tf.nn.relu(d7), options.output_c_dim, name=\'g_d8\')\n        # d8 is (256 x 256 x output_c_dim)\n\n        return tf.nn.tanh(d8)\n\n\ndef generator_resnet(image, options, reuse=False, name=""generator""):\n\n    with tf.variable_scope(name):\n        # image is 256 x 256 x input_c_dim\n        if reuse:\n            tf.get_variable_scope().reuse_variables()\n        else:\n            assert tf.get_variable_scope().reuse is False\n\n        def residule_block(x, dim, ks=3, s=1, name=\'res\'):\n            p = int((ks - 1) / 2)\n            y = tf.pad(x, [[0, 0], [p, p], [p, p], [0, 0]], ""REFLECT"")\n            y = instance_norm(conv2d(y, dim, ks, s, padding=\'VALID\', name=name+\'_c1\'), name+\'_bn1\')\n            y = tf.pad(tf.nn.relu(y), [[0, 0], [p, p], [p, p], [0, 0]], ""REFLECT"")\n            y = instance_norm(conv2d(y, dim, ks, s, padding=\'VALID\', name=name+\'_c2\'), name+\'_bn2\')\n            return y + x\n\n        # Justin Johnson\'s model from https://github.com/jcjohnson/fast-neural-style/\n        # The network with 9 blocks consists of: c7s1-32, d64, d128, R128, R128, R128,\n        # R128, R128, R128, R128, R128, R128, u64, u32, c7s1-3\n        c0 = tf.pad(image, [[0, 0], [3, 3], [3, 3], [0, 0]], ""REFLECT"")\n        c1 = tf.nn.relu(instance_norm(conv2d(c0, options.gf_dim, 7, 1, padding=\'VALID\', name=\'g_e1_c\'), \'g_e1_bn\'))\n        c2 = tf.nn.relu(instance_norm(conv2d(c1, options.gf_dim*2, 3, 2, name=\'g_e2_c\'), \'g_e2_bn\'))\n        c3 = tf.nn.relu(instance_norm(conv2d(c2, options.gf_dim*4, 3, 2, name=\'g_e3_c\'), \'g_e3_bn\'))\n        # define G network with 9 resnet blocks\n        r1 = residule_block(c3, options.gf_dim*4, name=\'g_r1\')\n        r2 = residule_block(r1, options.gf_dim*4, name=\'g_r2\')\n        r3 = residule_block(r2, options.gf_dim*4, name=\'g_r3\')\n        r4 = residule_block(r3, options.gf_dim*4, name=\'g_r4\')\n        r5 = residule_block(r4, options.gf_dim*4, name=\'g_r5\')\n        r6 = residule_block(r5, options.gf_dim*4, name=\'g_r6\')\n        r7 = residule_block(r6, options.gf_dim*4, name=\'g_r7\')\n        r8 = residule_block(r7, options.gf_dim*4, name=\'g_r8\')\n        r9 = residule_block(r8, options.gf_dim*4, name=\'g_r9\')\n\n        d1 = deconv2d(r9, options.gf_dim*2, 3, 2, name=\'g_d1_dc\')\n        d1 = tf.nn.relu(instance_norm(d1, \'g_d1_bn\'))\n        d2 = deconv2d(d1, options.gf_dim, 3, 2, name=\'g_d2_dc\')\n        d2 = tf.nn.relu(instance_norm(d2, \'g_d2_bn\'))\n        d2 = tf.pad(d2, [[0, 0], [3, 3], [3, 3], [0, 0]], ""REFLECT"")\n        pred = tf.nn.tanh(conv2d(d2, options.output_c_dim, 7, 1, padding=\'VALID\', name=\'g_pred_c\'))\n\n        return pred\n\n\ndef abs_criterion(in_, target):\n    return tf.reduce_mean(tf.abs(in_ - target))\n\n\ndef mae_criterion(in_, target):\n    return tf.reduce_mean((in_-target)**2)\n\n\ndef sce_criterion(logits, labels):\n    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n'"
ops.py,18,"b'import math\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.python.framework import ops\n\nfrom utils import *\n\ndef batch_norm(x, name=""batch_norm""):\n    return tf.contrib.layers.batch_norm(x, decay=0.9, updates_collections=None, epsilon=1e-5, scale=True, scope=name)\n\ndef instance_norm(input, name=""instance_norm""):\n    with tf.variable_scope(name):\n        depth = input.get_shape()[3]\n        scale = tf.get_variable(""scale"", [depth], initializer=tf.random_normal_initializer(1.0, 0.02, dtype=tf.float32))\n        offset = tf.get_variable(""offset"", [depth], initializer=tf.constant_initializer(0.0))\n        mean, variance = tf.nn.moments(input, axes=[1,2], keep_dims=True)\n        epsilon = 1e-5\n        inv = tf.rsqrt(variance + epsilon)\n        normalized = (input-mean)*inv\n        return scale*normalized + offset\n\ndef conv2d(input_, output_dim, ks=4, s=2, stddev=0.02, padding=\'SAME\', name=""conv2d""):\n    with tf.variable_scope(name):\n        return slim.conv2d(input_, output_dim, ks, s, padding=padding, activation_fn=None,\n                            weights_initializer=tf.truncated_normal_initializer(stddev=stddev),\n                            biases_initializer=None)\n\ndef deconv2d(input_, output_dim, ks=4, s=2, stddev=0.02, name=""deconv2d""):\n    with tf.variable_scope(name):\n        return slim.conv2d_transpose(input_, output_dim, ks, s, padding=\'SAME\', activation_fn=None,\n                                    weights_initializer=tf.truncated_normal_initializer(stddev=stddev),\n                                    biases_initializer=None)\n\ndef lrelu(x, leak=0.2, name=""lrelu""):\n    return tf.maximum(x, leak*x)\n\ndef linear(input_, output_size, scope=None, stddev=0.02, bias_start=0.0, with_w=False):\n\n    with tf.variable_scope(scope or ""Linear""):\n        matrix = tf.get_variable(""Matrix"", [input_.get_shape()[-1], output_size], tf.float32,\n                                 tf.random_normal_initializer(stddev=stddev))\n        bias = tf.get_variable(""bias"", [output_size],\n            initializer=tf.constant_initializer(bias_start))\n        if with_w:\n            return tf.matmul(input_, matrix) + bias, matrix, bias\n        else:\n            return tf.matmul(input_, matrix) + bias\n'"
utils.py,0,"b'""""""\nSome codes from https://github.com/Newmu/dcgan_code\n""""""\nfrom __future__ import division\nimport math\nimport pprint\nimport scipy.misc\nimport numpy as np\nimport copy\ntry:\n    _imread = scipy.misc.imread\nexcept AttributeError:\n    from imageio import imread as _imread\n\npp = pprint.PrettyPrinter()\n\nget_stddev = lambda x, k_h, k_w: 1/math.sqrt(k_w*k_h*x.get_shape()[-1])\n\n# -----------------------------\n# new added functions for cyclegan\nclass ImagePool(object):\n    def __init__(self, maxsize=50):\n        self.maxsize = maxsize\n        self.num_img = 0\n        self.images = []\n\n    def __call__(self, image):\n        if self.maxsize <= 0:\n            return image\n        if self.num_img < self.maxsize:\n            self.images.append(image)\n            self.num_img += 1\n            return image\n        if np.random.rand() > 0.5:\n            idx = int(np.random.rand()*self.maxsize)\n            tmp1 = copy.copy(self.images[idx])[0]\n            self.images[idx][0] = image[0]\n            idx = int(np.random.rand()*self.maxsize)\n            tmp2 = copy.copy(self.images[idx])[1]\n            self.images[idx][1] = image[1]\n            return [tmp1, tmp2]\n        else:\n            return image\n\ndef load_test_data(image_path, fine_size=256):\n    img = imread(image_path)\n    img = scipy.misc.imresize(img, [fine_size, fine_size])\n    img = img/127.5 - 1\n    return img\n\ndef load_train_data(image_path, load_size=286, fine_size=256, is_testing=False):\n    img_A = imread(image_path[0])\n    img_B = imread(image_path[1])\n    if not is_testing:\n        img_A = scipy.misc.imresize(img_A, [load_size, load_size])\n        img_B = scipy.misc.imresize(img_B, [load_size, load_size])\n        h1 = int(np.ceil(np.random.uniform(1e-2, load_size-fine_size)))\n        w1 = int(np.ceil(np.random.uniform(1e-2, load_size-fine_size)))\n        img_A = img_A[h1:h1+fine_size, w1:w1+fine_size]\n        img_B = img_B[h1:h1+fine_size, w1:w1+fine_size]\n\n        if np.random.random() > 0.5:\n            img_A = np.fliplr(img_A)\n            img_B = np.fliplr(img_B)\n    else:\n        img_A = scipy.misc.imresize(img_A, [fine_size, fine_size])\n        img_B = scipy.misc.imresize(img_B, [fine_size, fine_size])\n\n    img_A = img_A/127.5 - 1.\n    img_B = img_B/127.5 - 1.\n\n    img_AB = np.concatenate((img_A, img_B), axis=2)\n    # img_AB shape: (fine_size, fine_size, input_c_dim + output_c_dim)\n    return img_AB\n\n# -----------------------------\n\ndef get_image(image_path, image_size, is_crop=True, resize_w=64, is_grayscale = False):\n    return transform(imread(image_path, is_grayscale), image_size, is_crop, resize_w)\n\ndef save_images(images, size, image_path):\n    return imsave(inverse_transform(images), size, image_path)\n\ndef imread(path, is_grayscale = False):\n    if (is_grayscale):\n        return _imread(path, flatten=True).astype(np.float)\n    else:\n        return _imread(path, mode=\'RGB\').astype(np.float)\n\ndef merge_images(images, size):\n    return inverse_transform(images)\n\ndef merge(images, size):\n    h, w = images.shape[1], images.shape[2]\n    img = np.zeros((h * size[0], w * size[1], 3))\n    for idx, image in enumerate(images):\n        i = idx % size[1]\n        j = idx // size[1]\n        img[j*h:j*h+h, i*w:i*w+w, :] = image\n\n    return img\n\ndef imsave(images, size, path):\n    return scipy.misc.imsave(path, merge(images, size))\n\ndef center_crop(x, crop_h, crop_w,\n                resize_h=64, resize_w=64):\n  if crop_w is None:\n    crop_w = crop_h\n  h, w = x.shape[:2]\n  j = int(round((h - crop_h)/2.))\n  i = int(round((w - crop_w)/2.))\n  return scipy.misc.imresize(\n      x[j:j+crop_h, i:i+crop_w], [resize_h, resize_w])\n\ndef transform(image, npx=64, is_crop=True, resize_w=64):\n    # npx : # of pixels width/height of image\n    if is_crop:\n        cropped_image = center_crop(image, npx, resize_w=resize_w)\n    else:\n        cropped_image = image\n    return np.array(cropped_image)/127.5 - 1.\n\ndef inverse_transform(images):\n    return (images+1.)/2.\n'"
