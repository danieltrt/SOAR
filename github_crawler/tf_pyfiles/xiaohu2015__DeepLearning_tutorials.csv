file_path,api_count,code
CNNs/MobileNet.py,35,"b'""""""\r\n2017/11/24 ref:https://github.com/Zehaos/MobileNet/blob/master/nets/mobilenet.py\r\n""""""\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.training import moving_averages\r\n\r\nUPDATE_OPS_COLLECTION = ""_update_ops_""\r\n\r\n# create variable\r\ndef create_variable(name, shape, initializer,\r\n    dtype=tf.float32, trainable=True):\r\n    return tf.get_variable(name, shape=shape, dtype=dtype,\r\n            initializer=initializer, trainable=trainable)\r\n\r\n# batchnorm layer\r\ndef bacthnorm(inputs, scope, epsilon=1e-05, momentum=0.99, is_training=True):\r\n    inputs_shape = inputs.get_shape().as_list()\r\n    params_shape = inputs_shape[-1:]\r\n    axis = list(range(len(inputs_shape) - 1))\r\n\r\n    with tf.variable_scope(scope):\r\n        beta = create_variable(""beta"", params_shape,\r\n                               initializer=tf.zeros_initializer())\r\n        gamma = create_variable(""gamma"", params_shape,\r\n                                initializer=tf.ones_initializer())\r\n        # for inference\r\n        moving_mean = create_variable(""moving_mean"", params_shape,\r\n                            initializer=tf.zeros_initializer(), trainable=False)\r\n        moving_variance = create_variable(""moving_variance"", params_shape,\r\n                            initializer=tf.ones_initializer(), trainable=False)\r\n    if is_training:\r\n        mean, variance = tf.nn.moments(inputs, axes=axis)\r\n        update_move_mean = moving_averages.assign_moving_average(moving_mean,\r\n                                                mean, decay=momentum)\r\n        update_move_variance = moving_averages.assign_moving_average(moving_variance,\r\n                                                variance, decay=momentum)\r\n        tf.add_to_collection(UPDATE_OPS_COLLECTION, update_move_mean)\r\n        tf.add_to_collection(UPDATE_OPS_COLLECTION, update_move_variance)\r\n    else:\r\n        mean, variance = moving_mean, moving_variance\r\n    return tf.nn.batch_normalization(inputs, mean, variance, beta, gamma, epsilon)\r\n\r\n# depthwise conv2d layer\r\ndef depthwise_conv2d(inputs, scope, filter_size=3, channel_multiplier=1, strides=1):\r\n    inputs_shape = inputs.get_shape().as_list()\r\n    in_channels = inputs_shape[-1]\r\n    with tf.variable_scope(scope):\r\n        filter = create_variable(""filter"", shape=[filter_size, filter_size,\r\n                                                  in_channels, channel_multiplier],\r\n                       initializer=tf.truncated_normal_initializer(stddev=0.01))\r\n\r\n    return tf.nn.depthwise_conv2d(inputs, filter, strides=[1, strides, strides, 1],\r\n                                padding=""SAME"", rate=[1, 1])\r\n\r\n# conv2d layer\r\ndef conv2d(inputs, scope, num_filters, filter_size=1, strides=1):\r\n    inputs_shape = inputs.get_shape().as_list()\r\n    in_channels = inputs_shape[-1]\r\n    with tf.variable_scope(scope):\r\n        filter = create_variable(""filter"", shape=[filter_size, filter_size,\r\n                                                  in_channels, num_filters],\r\n                        initializer=tf.truncated_normal_initializer(stddev=0.01))\r\n    return tf.nn.conv2d(inputs, filter, strides=[1, strides, strides, 1],\r\n                        padding=""SAME"")\r\n\r\n# avg pool layer\r\ndef avg_pool(inputs, pool_size, scope):\r\n    with tf.variable_scope(scope):\r\n        return tf.nn.avg_pool(inputs, [1, pool_size, pool_size, 1],\r\n                strides=[1, pool_size, pool_size, 1], padding=""VALID"")\r\n\r\n# fully connected layer\r\ndef fc(inputs, n_out, scope, use_bias=True):\r\n    inputs_shape = inputs.get_shape().as_list()\r\n    n_in = inputs_shape[-1]\r\n    with tf.variable_scope(scope):\r\n        weight = create_variable(""weight"", shape=[n_in, n_out],\r\n                    initializer=tf.random_normal_initializer(stddev=0.01))\r\n        if use_bias:\r\n            bias = create_variable(""bias"", shape=[n_out,],\r\n                                   initializer=tf.zeros_initializer())\r\n            return tf.nn.xw_plus_b(inputs, weight, bias)\r\n        return tf.matmul(inputs, weight)\r\n\r\n\r\nclass MobileNet(object):\r\n    def __init__(self, inputs, num_classes=1000, is_training=True,\r\n                 width_multiplier=1, scope=""MobileNet""):\r\n        """"""\r\n        The implement of MobileNet(ref:https://arxiv.org/abs/1704.04861)\r\n        :param inputs: 4-D Tensor of [batch_size, height, width, channels]\r\n        :param num_classes: number of classes\r\n        :param is_training: Boolean, whether or not the model is training\r\n        :param width_multiplier: float, controls the size of model\r\n        :param scope: Optional scope for variables\r\n        """"""\r\n        self.inputs = inputs\r\n        self.num_classes = num_classes\r\n        self.is_training = is_training\r\n        self.width_multiplier = width_multiplier\r\n\r\n        # construct model\r\n        with tf.variable_scope(scope):\r\n            # conv1\r\n            net = conv2d(inputs, ""conv_1"", round(32 * width_multiplier), filter_size=3,\r\n                         strides=2)  # ->[N, 112, 112, 32]\r\n            net = tf.nn.relu(bacthnorm(net, ""conv_1/bn"", is_training=self.is_training))\r\n            net = self._depthwise_separable_conv2d(net, 64, self.width_multiplier,\r\n                                ""ds_conv_2"") # ->[N, 112, 112, 64]\r\n            net = self._depthwise_separable_conv2d(net, 128, self.width_multiplier,\r\n                                ""ds_conv_3"", downsample=True) # ->[N, 56, 56, 128]\r\n            net = self._depthwise_separable_conv2d(net, 128, self.width_multiplier,\r\n                                ""ds_conv_4"") # ->[N, 56, 56, 128]\r\n            net = self._depthwise_separable_conv2d(net, 256, self.width_multiplier,\r\n                                ""ds_conv_5"", downsample=True) # ->[N, 28, 28, 256]\r\n            net = self._depthwise_separable_conv2d(net, 256, self.width_multiplier,\r\n                                ""ds_conv_6"") # ->[N, 28, 28, 256]\r\n            net = self._depthwise_separable_conv2d(net, 512, self.width_multiplier,\r\n                                ""ds_conv_7"", downsample=True) # ->[N, 14, 14, 512]\r\n            net = self._depthwise_separable_conv2d(net, 512, self.width_multiplier,\r\n                                ""ds_conv_8"") # ->[N, 14, 14, 512]\r\n            net = self._depthwise_separable_conv2d(net, 512, self.width_multiplier,\r\n                                ""ds_conv_9"")  # ->[N, 14, 14, 512]\r\n            net = self._depthwise_separable_conv2d(net, 512, self.width_multiplier,\r\n                                ""ds_conv_10"")  # ->[N, 14, 14, 512]\r\n            net = self._depthwise_separable_conv2d(net, 512, self.width_multiplier,\r\n                                ""ds_conv_11"")  # ->[N, 14, 14, 512]\r\n            net = self._depthwise_separable_conv2d(net, 512, self.width_multiplier,\r\n                                ""ds_conv_12"")  # ->[N, 14, 14, 512]\r\n            net = self._depthwise_separable_conv2d(net, 1024, self.width_multiplier,\r\n                                ""ds_conv_13"", downsample=True) # ->[N, 7, 7, 1024]\r\n            net = self._depthwise_separable_conv2d(net, 1024, self.width_multiplier,\r\n                                ""ds_conv_14"") # ->[N, 7, 7, 1024]\r\n            net = avg_pool(net, 7, ""avg_pool_15"")\r\n            net = tf.squeeze(net, [1, 2], name=""SpatialSqueeze"")\r\n            self.logits = fc(net, self.num_classes, ""fc_16"")\r\n            self.predictions = tf.nn.softmax(self.logits)\r\n\r\n    def _depthwise_separable_conv2d(self, inputs, num_filters, width_multiplier,\r\n                                    scope, downsample=False):\r\n        """"""depthwise separable convolution 2D function""""""\r\n        num_filters = round(num_filters * width_multiplier)\r\n        strides = 2 if downsample else 1\r\n\r\n        with tf.variable_scope(scope):\r\n            # depthwise conv2d\r\n            dw_conv = depthwise_conv2d(inputs, ""depthwise_conv"", strides=strides)\r\n            # batchnorm\r\n            bn = bacthnorm(dw_conv, ""dw_bn"", is_training=self.is_training)\r\n            # relu\r\n            relu = tf.nn.relu(bn)\r\n            # pointwise conv2d (1x1)\r\n            pw_conv = conv2d(relu, ""pointwise_conv"", num_filters)\r\n            # bn\r\n            bn = bacthnorm(pw_conv, ""pw_bn"", is_training=self.is_training)\r\n            return tf.nn.relu(bn)\r\n\r\nif __name__ == ""__main__"":\r\n    # test data\r\n    inputs = tf.random_normal(shape=[4, 224, 224, 3])\r\n    mobileNet = MobileNet(inputs)\r\n    writer = tf.summary.FileWriter(""./logs"", graph=tf.get_default_graph())\r\n    init = tf.global_variables_initializer()\r\n    with tf.Session() as sess:\r\n        sess.run(init)\r\n        pred = sess.run(mobileNet.predictions)\r\n        print(pred.shape)\r\n\r\n'"
CNNs/ResNet50.py,31,"b'""""""\r\nResNet50\r\n2017/12/06\r\n""""""\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.training import moving_averages\r\n\r\nfc_initializer = tf.contrib.layers.xavier_initializer\r\nconv2d_initializer = tf.contrib.layers.xavier_initializer_conv2d\r\n\r\n# create weight variable\r\ndef create_var(name, shape, initializer, trainable=True):\r\n    return tf.get_variable(name, shape=shape, dtype=tf.float32,\r\n                           initializer=initializer, trainable=trainable)\r\n\r\n# conv2d layer\r\ndef conv2d(x, num_outputs, kernel_size, stride=1, scope=""conv2d""):\r\n    num_inputs = x.get_shape()[-1]\r\n    with tf.variable_scope(scope):\r\n        kernel = create_var(""kernel"", [kernel_size, kernel_size,\r\n                                       num_inputs, num_outputs],\r\n                            conv2d_initializer())\r\n        return tf.nn.conv2d(x, kernel, strides=[1, stride, stride, 1],\r\n                            padding=""SAME"")\r\n\r\n# fully connected layer\r\ndef fc(x, num_outputs, scope=""fc""):\r\n    num_inputs = x.get_shape()[-1]\r\n    with tf.variable_scope(scope):\r\n        weight = create_var(""weight"", [num_inputs, num_outputs],\r\n                            fc_initializer())\r\n        bias = create_var(""bias"", [num_outputs,],\r\n                          tf.zeros_initializer())\r\n        return tf.nn.xw_plus_b(x, weight, bias)\r\n\r\n\r\n# batch norm layer\r\ndef batch_norm(x, decay=0.999, epsilon=1e-03, is_training=True,\r\n               scope=""scope""):\r\n    x_shape = x.get_shape()\r\n    num_inputs = x_shape[-1]\r\n    reduce_dims = list(range(len(x_shape) - 1))\r\n    with tf.variable_scope(scope):\r\n        beta = create_var(""beta"", [num_inputs,],\r\n                               initializer=tf.zeros_initializer())\r\n        gamma = create_var(""gamma"", [num_inputs,],\r\n                                initializer=tf.ones_initializer())\r\n        # for inference\r\n        moving_mean = create_var(""moving_mean"", [num_inputs,],\r\n                                 initializer=tf.zeros_initializer(),\r\n                                 trainable=False)\r\n        moving_variance = create_var(""moving_variance"", [num_inputs],\r\n                                     initializer=tf.ones_initializer(),\r\n                                     trainable=False)\r\n    if is_training:\r\n        mean, variance = tf.nn.moments(x, axes=reduce_dims)\r\n        update_move_mean = moving_averages.assign_moving_average(moving_mean,\r\n                                                mean, decay=decay)\r\n        update_move_variance = moving_averages.assign_moving_average(moving_variance,\r\n                                                variance, decay=decay)\r\n        tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, update_move_mean)\r\n        tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, update_move_variance)\r\n    else:\r\n        mean, variance = moving_mean, moving_variance\r\n    return tf.nn.batch_normalization(x, mean, variance, beta, gamma, epsilon)\r\n\r\n\r\n# avg pool layer\r\ndef avg_pool(x, pool_size, scope):\r\n    with tf.variable_scope(scope):\r\n        return tf.nn.avg_pool(x, [1, pool_size, pool_size, 1],\r\n                strides=[1, pool_size, pool_size, 1], padding=""VALID"")\r\n\r\n# max pool layer\r\ndef max_pool(x, pool_size, stride, scope):\r\n    with tf.variable_scope(scope):\r\n        return tf.nn.max_pool(x, [1, pool_size, pool_size, 1],\r\n                              [1, stride, stride, 1], padding=""SAME"")\r\n\r\nclass ResNet50(object):\r\n    def __init__(self, inputs, num_classes=1000, is_training=True,\r\n                 scope=""resnet50""):\r\n        self.inputs =inputs\r\n        self.is_training = is_training\r\n        self.num_classes = num_classes\r\n\r\n        with tf.variable_scope(scope):\r\n            # construct the model\r\n            net = conv2d(inputs, 64, 7, 2, scope=""conv1"") # -> [batch, 112, 112, 64]\r\n            net = tf.nn.relu(batch_norm(net, is_training=self.is_training, scope=""bn1""))\r\n            net = max_pool(net, 3, 2, scope=""maxpool1"")  # -> [batch, 56, 56, 64]\r\n            net = self._block(net, 256, 3, init_stride=1, is_training=self.is_training,\r\n                              scope=""block2"")           # -> [batch, 56, 56, 256]\r\n            net = self._block(net, 512, 4, is_training=self.is_training, scope=""block3"")\r\n                                                        # -> [batch, 28, 28, 512]\r\n            net = self._block(net, 1024, 6, is_training=self.is_training, scope=""block4"")\r\n                                                        # -> [batch, 14, 14, 1024]\r\n            net = self._block(net, 2048, 3, is_training=self.is_training, scope=""block5"")\r\n                                                        # -> [batch, 7, 7, 2048]\r\n            net = avg_pool(net, 7, scope=""avgpool5"")    # -> [batch, 1, 1, 2048]\r\n            net = tf.squeeze(net, [1, 2], name=""SpatialSqueeze"") # -> [batch, 2048]\r\n            self.logits = fc(net, self.num_classes, ""fc6"")       # -> [batch, num_classes]\r\n            self.predictions = tf.nn.softmax(self.logits)\r\n\r\n\r\n    def _block(self, x, n_out, n, init_stride=2, is_training=True, scope=""block""):\r\n        with tf.variable_scope(scope):\r\n            h_out = n_out // 4\r\n            out = self._bottleneck(x, h_out, n_out, stride=init_stride,\r\n                                   is_training=is_training, scope=""bottlencek1"")\r\n            for i in range(1, n):\r\n                out = self._bottleneck(out, h_out, n_out, is_training=is_training,\r\n                                       scope=(""bottlencek%s"" % (i + 1)))\r\n            return out\r\n\r\n    def _bottleneck(self, x, h_out, n_out, stride=None, is_training=True, scope=""bottleneck""):\r\n        """""" A residual bottleneck unit""""""\r\n        n_in = x.get_shape()[-1]\r\n        if stride is None:\r\n            stride = 1 if n_in == n_out else 2\r\n\r\n        with tf.variable_scope(scope):\r\n            h = conv2d(x, h_out, 1, stride=stride, scope=""conv_1"")\r\n            h = batch_norm(h, is_training=is_training, scope=""bn_1"")\r\n            h = tf.nn.relu(h)\r\n            h = conv2d(h, h_out, 3, stride=1, scope=""conv_2"")\r\n            h = batch_norm(h, is_training=is_training, scope=""bn_2"")\r\n            h = tf.nn.relu(h)\r\n            h = conv2d(h, n_out, 1, stride=1, scope=""conv_3"")\r\n            h = batch_norm(h, is_training=is_training, scope=""bn_3"")\r\n\r\n            if n_in != n_out:\r\n                shortcut = conv2d(x, n_out, 1, stride=stride, scope=""conv_4"")\r\n                shortcut = batch_norm(shortcut, is_training=is_training, scope=""bn_4"")\r\n            else:\r\n                shortcut = x\r\n            return tf.nn.relu(shortcut + h)\r\n\r\nif __name__ == ""__main__"":\r\n    x = tf.random_normal([32, 224, 224, 3])\r\n    resnet50 = ResNet50(x)\r\n    print(resnet50.logits)'"
CNNs/ShuffleNet.py,0,"b'""""""\nimplement a shuffleNet by pytorch\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\ndtype = torch.FloatTensor\n\ndef shuffle_channels(x, groups):\n    """"""shuffle channels of a 4-D Tensor""""""\n    batch_size, channels, height, width = x.size()\n    assert channels % groups == 0\n    channels_per_group = channels // groups\n    # split into groups\n    x = x.view(batch_size, groups, channels_per_group,\n               height, width)\n    # transpose 1, 2 axis\n    x = x.transpose(1, 2).contiguous()\n    # reshape into orignal\n    x = x.view(batch_size, channels, height, width)\n    return x\n\nclass ShuffleNetUnitA(nn.Module):\n    """"""ShuffleNet unit for stride=1""""""\n    def __init__(self, in_channels, out_channels, groups=3):\n        super(ShuffleNetUnitA, self).__init__()\n        assert in_channels == out_channels\n        assert out_channels % 4 == 0\n        bottleneck_channels = out_channels // 4\n        self.groups = groups\n        self.group_conv1 = nn.Conv2d(in_channels, bottleneck_channels,\n                                        1, groups=groups, stride=1)\n        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n        self.depthwise_conv3 = nn.Conv2d(bottleneck_channels,\n                                         bottleneck_channels,\n                                         3, padding=1, stride=1,\n                                         groups=bottleneck_channels)\n        self.bn4 = nn.BatchNorm2d(bottleneck_channels)\n        self.group_conv5 = nn.Conv2d(bottleneck_channels, out_channels,\n                                     1, stride=1, groups=groups)\n        self.bn6 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        out = self.group_conv1(x)\n        out = F.relu(self.bn2(out))\n        out = shuffle_channels(out, groups=self.groups)\n        out = self.depthwise_conv3(out)\n        out = self.bn4(out)\n        out = self.group_conv5(out)\n        out = self.bn6(out)\n        out = F.relu(x + out)\n        return out\n\nclass ShuffleNetUnitB(nn.Module):\n    """"""ShuffleNet unit for stride=2""""""\n    def __init__(self, in_channels, out_channels, groups=3):\n        super(ShuffleNetUnitB, self).__init__()\n        out_channels -= in_channels\n        assert out_channels % 4 == 0\n        bottleneck_channels = out_channels // 4\n        self.groups = groups\n        self.group_conv1 = nn.Conv2d(in_channels, bottleneck_channels,\n                                     1, groups=groups, stride=1)\n        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n        self.depthwise_conv3 = nn.Conv2d(bottleneck_channels,\n                                         bottleneck_channels,\n                                         3, padding=1, stride=2,\n                                         groups=bottleneck_channels)\n        self.bn4 = nn.BatchNorm2d(bottleneck_channels)\n        self.group_conv5 = nn.Conv2d(bottleneck_channels, out_channels,\n                                     1, stride=1, groups=groups)\n        self.bn6 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        out = self.group_conv1(x)\n        out = F.relu(self.bn2(out))\n        out = shuffle_channels(out, groups=self.groups)\n        out = self.depthwise_conv3(out)\n        out = self.bn4(out)\n        out = self.group_conv5(out)\n        out = self.bn6(out)\n        x = F.avg_pool2d(x, 3, stride=2, padding=1)\n        out = F.relu(torch.cat([x, out], dim=1))\n        return out\n\nclass ShuffleNet(nn.Module):\n    """"""ShuffleNet for groups=3""""""\n    def __init__(self, groups=3, in_channels=3, num_classes=1000):\n        super(ShuffleNet, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels, 24, 3, stride=2, padding=1)\n        stage2_seq = [ShuffleNetUnitB(24, 240, groups=3)] + \\\n            [ShuffleNetUnitA(240, 240, groups=3) for i in range(3)]\n        self.stage2 = nn.Sequential(*stage2_seq)\n        stage3_seq = [ShuffleNetUnitB(240, 480, groups=3)] + \\\n            [ShuffleNetUnitA(480, 480, groups=3) for i in range(7)]\n        self.stage3 = nn.Sequential(*stage3_seq)\n        stage4_seq = [ShuffleNetUnitB(480, 960, groups=3)] + \\\n                     [ShuffleNetUnitA(960, 960, groups=3) for i in range(3)]\n        self.stage4 = nn.Sequential(*stage4_seq)\n        self.fc = nn.Linear(960, num_classes)\n\n    def forward(self, x):\n        net = self.conv1(x)\n        net = F.max_pool2d(net, 3, stride=2, padding=1)\n        net = self.stage2(net)\n        net = self.stage3(net)\n        net = self.stage4(net)\n        net = F.avg_pool2d(net, 7)\n        net = net.view(net.size(0), -1)\n        net = self.fc(net)\n        logits = F.softmax(net)\n        return logits\n\nif __name__ == ""__main__"":\n    x = Variable(torch.randn([32, 3, 224, 224]).type(dtype),\n                 requires_grad=False)\n    shuffleNet = ShuffleNet()\n    out = shuffleNet(x)\n    print(out.size())\n'"
CNNs/SqueezeNet.py,20,"b'""""""\n2017/12/02\n""""""\n\nimport tensorflow as tf\nimport numpy as np\n\n\nclass SqueezeNet(object):\n    def __init__(self, inputs, nb_classes=1000, is_training=True):\n        # conv1\n        net = tf.layers.conv2d(inputs, 96, [7, 7], strides=[2, 2],\n                                 padding=""SAME"", activation=tf.nn.relu,\n                                 name=""conv1"")\n        # maxpool1\n        net = tf.layers.max_pooling2d(net, [3, 3], strides=[2, 2],\n                                      name=""maxpool1"")\n        # fire2\n        net = self._fire(net, 16, 64, ""fire2"")\n        # fire3\n        net = self._fire(net, 16, 64, ""fire3"")\n        # fire4\n        net = self._fire(net, 32, 128, ""fire4"")\n        # maxpool4\n        net = tf.layers.max_pooling2d(net, [3, 3], strides=[2, 2],\n                                      name=""maxpool4"")\n        # fire5\n        net = self._fire(net, 32, 128, ""fire5"")\n        # fire6\n        net = self._fire(net, 48, 192, ""fire6"")\n        # fire7\n        net = self._fire(net, 48, 192, ""fire7"")\n        # fire8\n        net = self._fire(net, 64, 256, ""fire8"")\n        # maxpool8\n        net = tf.layers.max_pooling2d(net, [3, 3], strides=[2, 2],\n                                      name=""maxpool8"")\n        # fire9\n        net = self._fire(net, 64, 256, ""fire9"")\n        # dropout\n        net = tf.layers.dropout(net, 0.5, training=is_training)\n        # conv10\n        net = tf.layers.conv2d(net, 1000, [1, 1], strides=[1, 1],\n                               padding=""SAME"", activation=tf.nn.relu,\n                               name=""conv10"")\n        # avgpool10\n        net = tf.layers.average_pooling2d(net, [13, 13], strides=[1, 1],\n                                          name=""avgpool10"")\n        # squeeze the axis\n        net = tf.squeeze(net, axis=[1, 2])\n\n        self.logits = net\n        self.prediction = tf.nn.softmax(net)\n\n\n    def _fire(self, inputs, squeeze_depth, expand_depth, scope):\n        with tf.variable_scope(scope):\n            squeeze = tf.layers.conv2d(inputs, squeeze_depth, [1, 1],\n                                       strides=[1, 1], padding=""SAME"",\n                                       activation=tf.nn.relu, name=""squeeze"")\n            # squeeze\n            expand_1x1 = tf.layers.conv2d(squeeze, expand_depth, [1, 1],\n                                          strides=[1, 1], padding=""SAME"",\n                                          activation=tf.nn.relu, name=""expand_1x1"")\n            expand_3x3 = tf.layers.conv2d(squeeze, expand_depth, [3, 3],\n                                          strides=[1, 1], padding=""SAME"",\n                                          activation=tf.nn.relu, name=""expand_3x3"")\n            return tf.concat([expand_1x1, expand_3x3], axis=3)\n\n\nif __name__ == ""__main__"":\n    inputs = tf.random_normal([32, 224, 224, 3])\n    net = SqueezeNet(inputs)\n    print(net.prediction)\n'"
CNNs/densenet.py,0,"b'""""""\r\nDenseNet, original: https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py\r\n""""""\r\nimport re\r\nfrom collections import OrderedDict\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.utils.model_zoo as model_zoo\r\nimport torchvision.transforms as transforms\r\n\r\nfrom PIL import Image\r\nimport numpy as np\r\n\r\nmodel_urls = {\r\n    \'densenet121\': \'https://download.pytorch.org/models/densenet121-a639ec97.pth\',\r\n    \'densenet169\': \'https://download.pytorch.org/models/densenet169-b2777c0a.pth\',\r\n    \'densenet201\': \'https://download.pytorch.org/models/densenet201-c1103571.pth\',\r\n    \'densenet161\': \'https://download.pytorch.org/models/densenet161-8d451a50.pth\',\r\n}\r\n\r\n\r\nclass _DenseLayer(nn.Sequential):\r\n    """"""Basic unit of DenseBlock (using bottleneck layer) """"""\r\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\r\n        super(_DenseLayer, self).__init__()\r\n        self.add_module(""norm1"", nn.BatchNorm2d(num_input_features))\r\n        self.add_module(""relu1"", nn.ReLU(inplace=True))\r\n        self.add_module(""conv1"", nn.Conv2d(num_input_features, bn_size*growth_rate,\r\n                                           kernel_size=1, stride=1, bias=False))\r\n        self.add_module(""norm2"", nn.BatchNorm2d(bn_size*growth_rate))\r\n        self.add_module(""relu2"", nn.ReLU(inplace=True))\r\n        self.add_module(""conv2"", nn.Conv2d(bn_size*growth_rate, growth_rate,\r\n                                           kernel_size=3, stride=1, padding=1, bias=False))\r\n        self.drop_rate = drop_rate\r\n\r\n    def forward(self, x):\r\n        new_features = super(_DenseLayer, self).forward(x)\r\n        if self.drop_rate > 0:\r\n            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\r\n        return torch.cat([x, new_features], 1)\r\n\r\nclass _DenseBlock(nn.Sequential):\r\n    """"""DenseBlock""""""\r\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\r\n        super(_DenseBlock, self).__init__()\r\n        for i in range(num_layers):\r\n            layer = _DenseLayer(num_input_features+i*growth_rate, growth_rate, bn_size,\r\n                                drop_rate)\r\n            self.add_module(""denselayer%d"" % (i+1,), layer)\r\n\r\n\r\nclass _Transition(nn.Sequential):\r\n    """"""Transition layer between two adjacent DenseBlock""""""\r\n    def __init__(self, num_input_feature, num_output_features):\r\n        super(_Transition, self).__init__()\r\n        self.add_module(""norm"", nn.BatchNorm2d(num_input_feature))\r\n        self.add_module(""relu"", nn.ReLU(inplace=True))\r\n        self.add_module(""conv"", nn.Conv2d(num_input_feature, num_output_features,\r\n                                          kernel_size=1, stride=1, bias=False))\r\n        self.add_module(""pool"", nn.AvgPool2d(2, stride=2))\r\n\r\n\r\nclass DenseNet(nn.Module):\r\n    ""DenseNet-BC model""\r\n    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16), num_init_features=64,\r\n                 bn_size=4, compression_rate=0.5, drop_rate=0, num_classes=1000):\r\n        """"""\r\n        :param growth_rate: (int) number of filters used in DenseLayer, `k` in the paper\r\n        :param block_config: (list of 4 ints) number of layers in each DenseBlock\r\n        :param num_init_features: (int) number of filters in the first Conv2d\r\n        :param bn_size: (int) the factor using in the bottleneck layer\r\n        :param compression_rate: (float) the compression rate used in Transition Layer\r\n        :param drop_rate: (float) the drop rate after each DenseLayer\r\n        :param num_classes: (int) number of classes for classification\r\n        """"""\r\n        super(DenseNet, self).__init__()\r\n        # first Conv2d\r\n        self.features = nn.Sequential(OrderedDict([\r\n            (""conv0"", nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\r\n            (""norm0"", nn.BatchNorm2d(num_init_features)),\r\n            (""relu0"", nn.ReLU(inplace=True)),\r\n            (""pool0"", nn.MaxPool2d(3, stride=2, padding=1))\r\n        ]))\r\n\r\n        # DenseBlock\r\n        num_features = num_init_features\r\n        for i, num_layers in enumerate(block_config):\r\n            block = _DenseBlock(num_layers, num_features, bn_size, growth_rate, drop_rate)\r\n            self.features.add_module(""denseblock%d"" % (i + 1), block)\r\n            num_features += num_layers*growth_rate\r\n            if i != len(block_config) - 1:\r\n                transition = _Transition(num_features, int(num_features*compression_rate))\r\n                self.features.add_module(""transition%d"" % (i + 1), transition)\r\n                num_features = int(num_features * compression_rate)\r\n\r\n        # final bn+ReLU\r\n        self.features.add_module(""norm5"", nn.BatchNorm2d(num_features))\r\n        self.features.add_module(""relu5"", nn.ReLU(inplace=True))\r\n\r\n        # classification layer\r\n        self.classifier = nn.Linear(num_features, num_classes)\r\n\r\n        # params initialization\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                nn.init.kaiming_normal_(m.weight)\r\n            elif isinstance(m, nn.BatchNorm2d):\r\n                nn.init.constant_(m.bias, 0)\r\n                nn.init.constant_(m.weight, 1)\r\n            elif isinstance(m, nn.Linear):\r\n                nn.init.constant_(m.bias, 0)\r\n\r\n    def forward(self, x):\r\n        features = self.features(x)\r\n        out = F.avg_pool2d(features, 7, stride=1).view(features.size(0), -1)\r\n        out = self.classifier(out)\r\n        return out\r\n\r\nclass DenseNet_MNIST(nn.Module):\r\n    """"""DenseNet for MNIST dataset""""""\r\n    def __init__(self, growth_rate=12, block_config=(6, 6, 6), num_init_features=16,\r\n                 bn_size=4, compression_rate=0.5, drop_rate=0, num_classes=10):\r\n        """"""\r\n        :param growth_rate: (int) number of filters used in DenseLayer, `k` in the paper\r\n        :param block_config: (list of 2 ints) number of layers in each DenseBlock\r\n        :param num_init_features: (int) number of filters in the first Conv2d\r\n        :param bn_size: (int) the factor using in the bottleneck layer\r\n        :param compression_rate: (float) the compression rate used in Transition Layer\r\n        :param drop_rate: (float) the drop rate after each DenseLayer\r\n        :param num_classes: (int) number of classes for classification\r\n        """"""\r\n        super(DenseNet_MNIST, self).__init__()\r\n        # first Conv2d\r\n        self.features = nn.Sequential(OrderedDict([\r\n            (""conv0"", nn.Conv2d(1, num_init_features, kernel_size=3, stride=1, padding=1, bias=False)),\r\n            (""norm0"", nn.BatchNorm2d(num_init_features)),\r\n            (""relu0"", nn.ReLU(inplace=True)),\r\n        ]))\r\n\r\n        # DenseBlock\r\n        num_features = num_init_features\r\n        for i, num_layers in enumerate(block_config):\r\n            block = _DenseBlock(num_layers, num_features, bn_size, growth_rate, drop_rate)\r\n            self.features.add_module(""denseblock%d"" % (i + 1), block)\r\n            num_features += num_layers * growth_rate\r\n            if i != len(block_config) - 1:\r\n                transition = _Transition(num_features, int(num_features * compression_rate))\r\n                self.features.add_module(""transition%d"" % (i + 1), transition)\r\n                num_features = int(num_features * compression_rate)\r\n\r\n        # final bn+ReLU\r\n        self.features.add_module(""norm5"", nn.BatchNorm2d(num_features))\r\n        self.features.add_module(""relu5"", nn.ReLU(inplace=True))\r\n\r\n        # classification layer\r\n        self.classifier = nn.Linear(num_features, num_classes)\r\n\r\n        # params initialization\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                nn.init.kaiming_normal_(m.weight)\r\n            elif isinstance(m, nn.BatchNorm2d):\r\n                nn.init.constant_(m.bias, 0)\r\n                nn.init.constant_(m.weight, 1)\r\n            elif isinstance(m, nn.Linear):\r\n                nn.init.constant_(m.bias, 0)\r\n\r\n    def forward(self, x):\r\n        features = self.features(x)\r\n        out = F.avg_pool2d(features, 7, stride=1).view(features.size(0), -1)\r\n        out = self.classifier(out)\r\n        return out\r\n\r\n\r\ndef densenet121(pretrained=False, **kwargs):\r\n    """"""DenseNet121""""""\r\n    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16),\r\n                     **kwargs)\r\n\r\n    if pretrained:\r\n        # \'.\'s are no longer allowed in module names, but pervious _DenseLayer\r\n        # has keys \'norm.1\', \'relu.1\', \'conv.1\', \'norm.2\', \'relu.2\', \'conv.2\'.\r\n        # They are also in the checkpoints in model_urls. This pattern is used\r\n        # to find such keys.\r\n        pattern = re.compile(\r\n            r\'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\')\r\n        state_dict = model_zoo.load_url(model_urls[\'densenet121\'])\r\n        for key in list(state_dict.keys()):\r\n            res = pattern.match(key)\r\n            if res:\r\n                new_key = res.group(1) + res.group(2)\r\n                state_dict[new_key] = state_dict[key]\r\n                del state_dict[key]\r\n        model.load_state_dict(state_dict)\r\n    return model\r\n\r\nif __name__ == ""__main__"":\r\n    densenet = densenet121(pretrained=True)\r\n    densenet.eval()\r\n\r\n    img = Image.open(""./images/cat.jpg"")\r\n\r\n    trans_ops = transforms.Compose([\r\n        transforms.Resize(256),\r\n        transforms.CenterCrop(224),\r\n        transforms.ToTensor(),\r\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\r\n                             std=[0.229, 0.224, 0.225])\r\n    ])\r\n\r\n    images = trans_ops(img).view(-1, 3, 224, 224)\r\n    print(images)\r\n    outputs = densenet(images)\r\n\r\n    _, predictions = outputs.topk(5, dim=1)\r\n\r\n    labels = list(map(lambda s: s.strip(), open(""./data/imagenet/synset_words.txt"").readlines()))\r\n    for idx in predictions.numpy()[0]:\r\n        print(""Predicted labels:"", labels[idx])\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n'"
CNNs/mobilenet_v2.py,24,"b'""""""\r\n2018-11-24\r\n""""""\r\n\r\nfrom collections import namedtuple\r\nimport copy\r\n\r\nimport tensorflow as tf\r\n\r\nslim = tf.contrib.slim\r\n\r\ndef _make_divisible(v, divisor, min_value=None):\r\n    """"""make `v` is divided exactly by `divisor`, but keep the min_value""""""\r\n    if min_value is None:\r\n        min_value = divisor\r\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\r\n    # Make sure that round down does not go down by more than 10%.\r\n    if new_v < 0.9 * v:\r\n        new_v += divisor\r\n    return new_v\r\n\r\n\r\n@slim.add_arg_scope\r\ndef _depth_multiplier_func(params,\r\n                           multiplier,\r\n                           divisible_by=8,\r\n                           min_depth=8):\r\n    """"""get the new channles""""""\r\n    if \'num_outputs\' not in params:\r\n        return\r\n    d = params[\'num_outputs\']\r\n    params[\'num_outputs\'] = _make_divisible(d * multiplier, divisible_by,\r\n                                                   min_depth)\r\n\r\ndef _fixed_padding(inputs, kernel_size, rate=1):\r\n    """"""Pads the input along the spatial dimensions independently of input size.\r\n      Pads the input such that if it was used in a convolution with \'VALID\' padding,\r\n      the output would have the same dimensions as if the unpadded input was used\r\n      in a convolution with \'SAME\' padding.\r\n      Args:\r\n        inputs: A tensor of size [batch, height_in, width_in, channels].\r\n        kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\r\n        rate: An integer, rate for atrous convolution.\r\n      Returns:\r\n        output: A tensor of size [batch, height_out, width_out, channels] with the\r\n        input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).\r\n    """"""\r\n    kernel_size_effective = [kernel_size[0] + (kernel_size[0] - 1) * (rate - 1),\r\n                               kernel_size[0] + (kernel_size[0] - 1) * (rate - 1)]\r\n    pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]\r\n    pad_beg = [pad_total[0] // 2, pad_total[1] // 2]\r\n    pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]\r\n    padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg[0], pad_end[0]],\r\n                                      [pad_beg[1], pad_end[1]], [0, 0]])\r\n    return padded_inputs\r\n\r\n\r\n@slim.add_arg_scope\r\ndef expanded_conv(x,\r\n                  num_outputs,\r\n                  expansion=6,\r\n                  stride=1,\r\n                  rate=1,\r\n                  normalizer_fn=slim.batch_norm,\r\n                  project_activation_fn=tf.identity,\r\n                  padding=""SAME"",\r\n                  scope=None):\r\n    """"""The expand conv op in MobileNetv2\r\n        1x1 conv -> depthwise 3x3 conv -> 1x1 linear conv\r\n    """"""\r\n    with tf.variable_scope(scope, default_name=""expanded_conv"") as s, \\\r\n       tf.name_scope(s.original_name_scope):\r\n        prev_depth = x.get_shape().as_list()[3]\r\n        # the filters of expanded conv\r\n        inner_size = prev_depth * expansion\r\n        net = x\r\n        # only inner_size > prev_depth, use expanded conv\r\n        if inner_size > prev_depth:\r\n            net = slim.conv2d(net, inner_size, 1, normalizer_fn=normalizer_fn,\r\n                              scope=""expand"")\r\n        # depthwise conv\r\n        net = slim.separable_conv2d(net, num_outputs=None, kernel_size=3,\r\n                                    depth_multiplier=1, stride=stride,\r\n                                    rate=rate, normalizer_fn=normalizer_fn,\r\n                                    padding=padding, scope=""depthwise"")\r\n        # projection\r\n        net = slim.conv2d(net, num_outputs, 1, normalizer_fn=normalizer_fn,\r\n                          activation_fn=project_activation_fn, scope=""project"")\r\n\r\n        # residual connection\r\n        if stride == 1 and net.get_shape().as_list()[-1] == prev_depth:\r\n            net += x\r\n\r\n        return net\r\n\r\ndef global_pool(x, pool_op=tf.nn.avg_pool):\r\n    """"""Applies avg pool to produce 1x1 output.\r\n    NOTE: This function is funcitonally equivalenet to reduce_mean, but it has\r\n        baked in average pool which has better support across hardware.\r\n    Args:\r\n        input_tensor: input tensor\r\n        pool_op: pooling op (avg pool is default)\r\n    Returns:\r\n        a tensor batch_size x 1 x 1 x depth.\r\n    """"""\r\n    shape = x.get_shape().as_list()\r\n    if shape[1] is None or shape[2] is None:\r\n        kernel_size = tf.convert_to_tensor(\r\n            [1, tf.shape(x)[1], tf.shape(x)[2], 1])\r\n    else:\r\n        kernel_size = [1, shape[1], shape[2], 1]\r\n    output = pool_op(x, ksize=kernel_size, strides=[1, 1, 1, 1], padding=\'VALID\')\r\n    # Recover output shape, for unknown shape.\r\n    output.set_shape([None, 1, 1, None])\r\n    return output\r\n\r\n\r\n_Op = namedtuple(""Op"", [\'op\', \'params\', \'multiplier_func\'])\r\n\r\ndef op(op_func, **params):\r\n    return _Op(op=op_func, params=params,\r\n               multiplier_func=_depth_multiplier_func)\r\n\r\n\r\nCONV_DEF = [op(slim.conv2d, num_outputs=32, stride=2, kernel_size=3),\r\n            op(expanded_conv, num_outputs=16, expansion=1),\r\n            op(expanded_conv, num_outputs=24, stride=2),\r\n            op(expanded_conv, num_outputs=24, stride=1),\r\n            op(expanded_conv, num_outputs=32, stride=2),\r\n            op(expanded_conv, num_outputs=32, stride=1),\r\n            op(expanded_conv, num_outputs=32, stride=1),\r\n            op(expanded_conv, num_outputs=64, stride=2),\r\n            op(expanded_conv, num_outputs=64, stride=1),\r\n            op(expanded_conv, num_outputs=64, stride=1),\r\n            op(expanded_conv, num_outputs=64, stride=1),\r\n            op(expanded_conv, num_outputs=96, stride=1),\r\n            op(expanded_conv, num_outputs=96, stride=1),\r\n            op(expanded_conv, num_outputs=96, stride=1),\r\n            op(expanded_conv, num_outputs=160, stride=2),\r\n            op(expanded_conv, num_outputs=160, stride=1),\r\n            op(expanded_conv, num_outputs=160, stride=1),\r\n            op(expanded_conv, num_outputs=320, stride=1),\r\n            op(slim.conv2d, num_outputs=1280, stride=1, kernel_size=1),\r\n            ]\r\n\r\n\r\ndef mobilenet_arg_scope(is_training=True,\r\n                        weight_decay=0.00004,\r\n                        stddev=0.09,\r\n                        dropout_keep_prob=0.8,\r\n                        bn_decay=0.997):\r\n    """"""Defines Mobilenet default arg scope.\r\n    Usage:\r\n     with tf.contrib.slim.arg_scope(mobilenet.training_scope()):\r\n       logits, endpoints = mobilenet_v2.mobilenet(input_tensor)\r\n     # the network created will be trainble with dropout/batch norm\r\n     # initialized appropriately.\r\n    Args:\r\n        is_training: if set to False this will ensure that all customizations are\r\n            set to non-training mode. This might be helpful for code that is reused\r\n        across both training/evaluation, but most of the time training_scope with\r\n        value False is not needed. If this is set to None, the parameters is not\r\n        added to the batch_norm arg_scope.\r\n        weight_decay: The weight decay to use for regularizing the model.\r\n        stddev: Standard deviation for initialization, if negative uses xavier.\r\n        dropout_keep_prob: dropout keep probability (not set if equals to None).\r\n        bn_decay: decay for the batch norm moving averages (not set if equals to\r\n            None).\r\n    Returns:\r\n        An argument scope to use via arg_scope.\r\n    """"""\r\n    # Note: do not introduce parameters that would change the inference\r\n    # model here (for example whether to use bias), modify conv_def instead.\r\n    batch_norm_params = {\r\n        \'center\': True,\r\n        \'scale\': True,\r\n        \'decay\': bn_decay,\r\n        \'is_training\': is_training\r\n    }\r\n    if stddev < 0:\r\n        weight_intitializer = slim.initializers.xavier_initializer()\r\n    else:\r\n        weight_intitializer = tf.truncated_normal_initializer(stddev=stddev)\r\n\r\n    # Set weight_decay for weights in Conv and FC layers.\r\n    with slim.arg_scope(\r\n        [slim.conv2d, slim.fully_connected, slim.separable_conv2d],\r\n        weights_initializer=weight_intitializer,\r\n        normalizer_fn=slim.batch_norm,\r\n        activation_fn=tf.nn.relu6), \\\r\n        slim.arg_scope([slim.batch_norm], **batch_norm_params), \\\r\n        slim.arg_scope([slim.dropout], is_training=is_training,\r\n                     keep_prob=dropout_keep_prob), \\\r\n        slim.arg_scope([slim.conv2d, slim.separable_conv2d],\r\n                       biases_initializer=None,\r\n                       padding=""SAME""), \\\r\n        slim.arg_scope([slim.conv2d],\r\n                     weights_regularizer=slim.l2_regularizer(weight_decay)), \\\r\n        slim.arg_scope([slim.separable_conv2d], weights_regularizer=None) as s:\r\n        return s\r\n\r\n\r\ndef mobilenetv2(x,\r\n                num_classes=1001,\r\n                depth_multiplier=1.0,\r\n                scope=\'MobilenetV2\',\r\n                finegrain_classification_mode=False,\r\n                min_depth=8,\r\n                divisible_by=8,\r\n                output_stride=None,\r\n                ):\r\n    """"""Mobilenet v2\r\n    Args:\r\n        x: The input tensor\r\n        num_classes: number of classes\r\n        depth_multiplier: The multiplier applied to scale number of\r\n            channels in each layer. Note: this is called depth multiplier in the\r\n            paper but the name is kept for consistency with slim\'s model builder.\r\n        scope: Scope of the operator\r\n        finegrain_classification_mode: When set to True, the model\r\n            will keep the last layer large even for small multipliers.\r\n            The paper suggests that it improves performance for ImageNet-type of problems.\r\n        min_depth: If provided, will ensure that all layers will have that\r\n          many channels after application of depth multiplier.\r\n       divisible_by: If provided will ensure that all layers # channels\r\n          will be divisible by this number.\r\n    """"""\r\n    conv_defs = CONV_DEF\r\n\r\n    # keep the last conv layer very larger channel\r\n    if finegrain_classification_mode:\r\n        conv_defs = copy.deepcopy(conv_defs)\r\n        if depth_multiplier < 1:\r\n            conv_defs[-1].params[\'num_outputs\'] /= depth_multiplier\r\n\r\n    depth_args = {}\r\n    # NB: do not set depth_args unless they are provided to avoid overriding\r\n    # whatever default depth_multiplier might have thanks to arg_scope.\r\n    if min_depth is not None:\r\n        depth_args[\'min_depth\'] = min_depth\r\n    if divisible_by is not None:\r\n        depth_args[\'divisible_by\'] = divisible_by\r\n\r\n    with slim.arg_scope([_depth_multiplier_func], **depth_args):\r\n        with tf.variable_scope(scope, default_name=\'Mobilenet\'):\r\n            # The current_stride variable keeps track of the output stride of the\r\n            # activations, i.e., the running product of convolution strides up to the\r\n            # current network layer. This allows us to invoke atrous convolution\r\n            # whenever applying the next convolution would result in the activations\r\n            # having output stride larger than the target output_stride.\r\n            current_stride = 1\r\n\r\n            # The atrous convolution rate parameter.\r\n            rate = 1\r\n\r\n            net = x\r\n            # Insert default parameters before the base scope which includes\r\n            # any custom overrides set in mobilenet.\r\n            end_points = {}\r\n            scopes = {}\r\n            for i, opdef in enumerate(conv_defs):\r\n                params = dict(opdef.params)\r\n                opdef.multiplier_func(params, depth_multiplier)\r\n                stride = params.get(\'stride\', 1)\r\n                if output_stride is not None and current_stride == output_stride:\r\n                    # If we have reached the target output_stride, then we need to employ\r\n                    # atrous convolution with stride=1 and multiply the atrous rate by the\r\n                    # current unit\'s stride for use in subsequent layers.\r\n                    layer_stride = 1\r\n                    layer_rate = rate\r\n                    rate *= stride\r\n                else:\r\n                    layer_stride = stride\r\n                    layer_rate = 1\r\n                    current_stride *= stride\r\n                # Update params.\r\n                params[\'stride\'] = layer_stride\r\n                # Only insert rate to params if rate > 1.\r\n                if layer_rate > 1:\r\n                    params[\'rate\'] = layer_rate\r\n\r\n                try:\r\n                    net = opdef.op(net, **params)\r\n                except Exception:\r\n                    raise ValueError(\'Failed to create op %i: %r params: %r\' % (i, opdef, params))\r\n\r\n            with tf.variable_scope(\'Logits\'):\r\n                net = global_pool(net)\r\n                end_points[\'global_pool\'] = net\r\n                if not num_classes:\r\n                    return net, end_points\r\n                net = slim.dropout(net, scope=\'Dropout\')\r\n                # 1 x 1 x num_classes\r\n                # Note: legacy scope name.\r\n                logits = slim.conv2d(\r\n                    net,\r\n                    num_classes, [1, 1],\r\n                    activation_fn=None,\r\n                    normalizer_fn=None,\r\n                    biases_initializer=tf.zeros_initializer(),\r\n                    scope=\'Conv2d_1c_1x1\')\r\n\r\n                logits = tf.squeeze(logits, [1, 2])\r\n\r\n                return logits\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    import cv2\r\n    import numpy as np\r\n\r\n    inputs = tf.placeholder(tf.uint8, [None, None, 3])\r\n    images = tf.expand_dims(inputs, 0)\r\n    images = tf.cast(images, tf.float32) / 128. - 1\r\n    images.set_shape((None, None, None, 3))\r\n    images = tf.image.resize_images(images, (224, 224))\r\n\r\n    with slim.arg_scope(mobilenet_arg_scope(is_training=False)):\r\n        logits = mobilenetv2(images)\r\n\r\n    # Restore using exponential moving average since it produces (1.5-2%) higher\r\n    # accuracy\r\n    ema = tf.train.ExponentialMovingAverage(0.999)\r\n    vars = ema.variables_to_restore()\r\n\r\n    saver = tf.train.Saver(vars)\r\n\r\n    print(len(tf.global_variables()))\r\n    for var in tf.global_variables():\r\n        print(var)\r\n    checkpoint_path = r""C:\\Users\\xiaoh\\Desktop\\temp\\mobilenet_v2_1.0_224\\mobilenet_v2_1.0_224.ckpt""\r\n    image_file = ""C:/Users/xiaoh/Desktop/temp/pandas.jpg""\r\n    with tf.Session() as sess:\r\n        saver.restore(sess, checkpoint_path)\r\n\r\n        img = cv2.imread(image_file)\r\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n\r\n        print(np.argmax(sess.run(logits, feed_dict={inputs: img})[0]))\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n'"
CNNs/shufflenet_v2.py,23,"b'""""""\r\nThe implement of shufflenet_v2 by Keras\r\n""""""\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Conv2D, DepthwiseConv2D\r\nfrom tensorflow.keras.layers import MaxPool2D, GlobalAveragePooling2D, Dense\r\nfrom tensorflow.keras.layers import BatchNormalization, Activation\r\n\r\n\r\ndef channle_shuffle(inputs, group):\r\n    """"""Shuffle the channel\r\n    Args:\r\n        inputs: 4D Tensor\r\n        group: int, number of groups\r\n    Returns:\r\n        Shuffled 4D Tensor\r\n    """"""\r\n    in_shape = inputs.get_shape().as_list()\r\n    h, w, in_channel = in_shape[1:]\r\n    assert in_channel % group == 0\r\n    l = tf.reshape(inputs, [-1, h, w, in_channel // group, group])\r\n    l = tf.transpose(l, [0, 1, 2, 4, 3])\r\n    l = tf.reshape(l, [-1, h, w, in_channel])\r\n\r\n    return l\r\n\r\nclass Conv2D_BN_ReLU(tf.keras.Model):\r\n    """"""Conv2D -> BN -> ReLU""""""\r\n    def __init__(self, channel, kernel_size=1, stride=1):\r\n        super(Conv2D_BN_ReLU, self).__init__()\r\n\r\n        self.conv = Conv2D(channel, kernel_size, strides=stride,\r\n                            padding=""SAME"", use_bias=False)\r\n        self.bn = BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)\r\n        self.relu = Activation(""relu"")\r\n\r\n    def call(self, inputs, training=True):\r\n        x = self.conv(inputs)\r\n        x = self.bn(x, training=training)\r\n        x = self.relu(x)\r\n        return x\r\n\r\nclass DepthwiseConv2D_BN(tf.keras.Model):\r\n    """"""DepthwiseConv2D -> BN""""""\r\n    def __init__(self, kernel_size=3, stride=1):\r\n        super(DepthwiseConv2D_BN, self).__init__()\r\n\r\n        self.dconv = DepthwiseConv2D(kernel_size, strides=stride,\r\n                                     depth_multiplier=1,\r\n                                     padding=""SAME"", use_bias=False)\r\n        self.bn = BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)\r\n\r\n    def call(self, inputs, training=True):\r\n        x = self.dconv(inputs)\r\n        x = self.bn(x, training=training)\r\n        return x\r\n\r\n\r\nclass ShufflenetUnit1(tf.keras.Model):\r\n    def __init__(self, out_channel):\r\n        """"""The unit of shufflenetv2 for stride=1\r\n        Args:\r\n            out_channel: int, number of channels\r\n        """"""\r\n        super(ShufflenetUnit1, self).__init__()\r\n\r\n        assert out_channel % 2 == 0\r\n        self.out_channel = out_channel\r\n\r\n        self.conv1_bn_relu = Conv2D_BN_ReLU(out_channel // 2, 1, 1)\r\n        self.dconv_bn = DepthwiseConv2D_BN(3, 1)\r\n        self.conv2_bn_relu = Conv2D_BN_ReLU(out_channel // 2, 1, 1)\r\n\r\n    def call(self, inputs, training=False):\r\n        # split the channel\r\n        shortcut, x = tf.split(inputs, 2, axis=3)\r\n\r\n        x = self.conv1_bn_relu(x, training=training)\r\n        x = self.dconv_bn(x, training=training)\r\n        x = self.conv2_bn_relu(x, training=training)\r\n\r\n        x = tf.concat([shortcut, x], axis=3)\r\n        x = channle_shuffle(x, 2)\r\n        return x\r\n\r\nclass ShufflenetUnit2(tf.keras.Model):\r\n    """"""The unit of shufflenetv2 for stride=2""""""\r\n    def __init__(self, in_channel, out_channel):\r\n        super(ShufflenetUnit2, self).__init__()\r\n\r\n        assert out_channel % 2 == 0\r\n        self.in_channel = in_channel\r\n        self.out_channel = out_channel\r\n\r\n        self.conv1_bn_relu = Conv2D_BN_ReLU(out_channel // 2, 1, 1)\r\n        self.dconv_bn = DepthwiseConv2D_BN(3, 2)\r\n        self.conv2_bn_relu = Conv2D_BN_ReLU(out_channel - in_channel, 1, 1)\r\n\r\n        # for shortcut\r\n        self.shortcut_dconv_bn = DepthwiseConv2D_BN(3, 2)\r\n        self.shortcut_conv_bn_relu = Conv2D_BN_ReLU(in_channel, 1, 1)\r\n\r\n    def call(self, inputs, training=False):\r\n        shortcut, x = inputs, inputs\r\n\r\n        x = self.conv1_bn_relu(x, training=training)\r\n        x = self.dconv_bn(x, training=training)\r\n        x = self.conv2_bn_relu(x, training=training)\r\n\r\n        shortcut = self.shortcut_dconv_bn(shortcut, training=training)\r\n        shortcut = self.shortcut_conv_bn_relu(shortcut, training=training)\r\n\r\n        x = tf.concat([shortcut, x], axis=3)\r\n        x = channle_shuffle(x, 2)\r\n        return x\r\n\r\nclass ShufflenetStage(tf.keras.Model):\r\n    """"""The stage of shufflenet""""""\r\n    def __init__(self, in_channel, out_channel, num_blocks):\r\n        super(ShufflenetStage, self).__init__()\r\n\r\n        self.in_channel = in_channel\r\n        self.out_channel = out_channel\r\n\r\n        self.ops = []\r\n        for i in range(num_blocks):\r\n            if i == 0:\r\n                op = ShufflenetUnit2(in_channel, out_channel)\r\n            else:\r\n                op = ShufflenetUnit1(out_channel)\r\n            self.ops.append(op)\r\n\r\n    def call(self, inputs, training=False):\r\n        x = inputs\r\n        for op in self.ops:\r\n            x = op(x, training=training)\r\n        return x\r\n\r\n\r\nclass ShuffleNetv2(tf.keras.Model):\r\n    """"""Shufflenetv2""""""\r\n    def __init__(self, num_classes, first_channel=24, channels_per_stage=(116, 232, 464)):\r\n        super(ShuffleNetv2, self).__init__()\r\n\r\n        self.num_classes = num_classes\r\n\r\n        self.conv1_bn_relu = Conv2D_BN_ReLU(first_channel, 3, 2)\r\n        self.pool1 = MaxPool2D(3, strides=2, padding=""SAME"")\r\n        self.stage2 = ShufflenetStage(first_channel, channels_per_stage[0], 4)\r\n        self.stage3 = ShufflenetStage(channels_per_stage[0], channels_per_stage[1], 8)\r\n        self.stage4 = ShufflenetStage(channels_per_stage[1], channels_per_stage[2], 4)\r\n        self.conv5_bn_relu = Conv2D_BN_ReLU(1024, 1, 1)\r\n        self.gap = GlobalAveragePooling2D()\r\n        self.linear = Dense(num_classes)\r\n\r\n    def call(self, inputs, training=False):\r\n        x = self.conv1_bn_relu(inputs, training=training)\r\n        x = self.pool1(x)\r\n        x = self.stage2(x, training=training)\r\n        x = self.stage3(x, training=training)\r\n        x = self.stage4(x, training=training)\r\n        x = self.conv5_bn_relu(x, training=training)\r\n        x = self.gap(x)\r\n        x = self.linear(x)\r\n        return x\r\n\r\n\r\nif __name__ ==""__main__"":\r\n    """"""\r\n    inputs = tf.placeholder(tf.float32, [None, 224, 224, 3])\r\n\r\n    model = ShuffleNetv2(1000)\r\n    outputs = model(inputs)\r\n\r\n    print(model.summary())\r\n\r\n    with tf.Session() as sess:\r\n        pass\r\n    \r\n\r\n    vars = []\r\n    for v in tf.global_variables():\r\n\r\n        vars.append((v.name, v))\r\n        print(v.name)\r\n    print(len(vars))\r\n\r\n\r\n    import numpy as np\r\n\r\n    path = ""C:/models/ShuffleNetV2-1x.npz""\r\n    weights = np.load(path)\r\n    np_vars = []\r\n    for k in weights:\r\n        k_ = k.replace(""beta"", ""gbeta"")\r\n        k_ = k_.replace(""/dconv"", ""/conv10_dconv"")\r\n        k_ = k_.replace(""shortcut_dconv"", ""shortcut_a_dconv"")\r\n        k_ = k_.replace(""conv5"", ""su_conv5"")\r\n        k_ = k_.replace(""linear"", ""t_linear"")\r\n        np_vars.append((k_, weights[k]))\r\n    np_vars.sort(key=lambda x: x[0])\r\n\r\n    for k, _ in np_vars:\r\n        print(k)\r\n\r\n    saver = tf.train.Saver(tf.global_variables())\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        assign_ops = []\r\n        for id in range(len(vars)):\r\n            print(vars[id][0], np_vars[id][0])\r\n            assign_ops.append(tf.assign(vars[id][1], np_vars[id][1]))\r\n\r\n        sess.run(assign_ops)\r\n        saver.save(sess, ""./models/shufflene_v2_1.0.ckpt"")\r\n\r\n        model.save(""./models/shufflenet_v2_1.0.hdf5"")\r\n    \r\n    """"""\r\n\r\n    import numpy as np\r\n    from tensorflow.keras.preprocessing import image\r\n    from tensorflow.keras.applications.densenet import preprocess_input, decode_predictions\r\n\r\n    img_path = \'./images/cat.jpg\'\r\n    img = image.load_img(img_path, target_size=(224, 224))\r\n    x = image.img_to_array(img)\r\n    x = np.expand_dims(x, axis=0)\r\n    x = preprocess_input(x)\r\n\r\n    inputs = tf.placeholder(tf.float32, [None, 224, 224, 3])\r\n    model = ShuffleNetv2(1000)\r\n    outputs = model(inputs, training=False)\r\n    outputs = tf.nn.softmax(outputs)\r\n\r\n    saver = tf.train.Saver()\r\n    with tf.Session() as sess:\r\n        saver.restore(sess, ""./models/shufflene_v2_1.0.ckpt"")\r\n        preds = sess.run(outputs, feed_dict={inputs: x})\r\n        print(decode_predictions(preds, top=3)[0])\r\n\r\n'"
examples/test.py,0,b'# test\n'
models/cnn.py,20,"b'""""""\r\nConvolution neural network\r\nauthor: Ye Hu\r\n2016/12/15\r\n""""""\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport input_data\r\nfrom logisticRegression import LogisticRegression\r\nfrom mlp import HiddenLayer\r\n\r\nclass ConvLayer(object):\r\n    """"""\r\n    A convolution layer\r\n    """"""\r\n    def __init__(self, inpt, filter_shape, strides=(1, 1, 1, 1),\r\n                 padding=""SAME"", activation=tf.nn.relu, bias_setting=True):\r\n        """"""\r\n        inpt: tf.Tensor, shape [n_examples, witdth, height, channels]\r\n        filter_shape: list or tuple, [witdth, height. channels, filter_nums]\r\n        strides: list or tuple, the step of filter\r\n        padding:\r\n        activation:\r\n        bias_setting:\r\n        """"""\r\n        self.input = inpt\r\n        # initializes the filter\r\n        self.W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), dtype=tf.float32)\r\n        if bias_setting:\r\n            self.b = tf.Variable(tf.truncated_normal(filter_shape[-1:], stddev=0.1),\r\n                                 dtype=tf.float32)\r\n        else:\r\n            self.b = None\r\n        conv_output = tf.nn.conv2d(self.input, filter=self.W, strides=strides,\r\n                                   padding=padding)\r\n        conv_output = conv_output + self.b if self.b is not None else conv_output\r\n        # the output\r\n        self.output = conv_output if activation is None else activation(conv_output)\r\n        # the params\r\n        self.params = [self.W, self.b] if self.b is not None else [self.W, ]\r\n\r\n\r\nclass MaxPoolLayer(object):\r\n    """"""pool layer""""""\r\n    def __init__(self, inpt, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1), padding=""SAME""):\r\n        """"""\r\n        """"""\r\n        self.input = inpt\r\n        # the output\r\n        self.output = tf.nn.max_pool(self.input, ksize=ksize, strides=strides, padding=padding)\r\n        self.params = []\r\n\r\n\r\nclass FlattenLayer(object):\r\n    """"""Flatten layer""""""\r\n    def __init__(self, inpt, shape):\r\n        self.input = inpt\r\n        self.output = tf.reshape(self.input, shape=shape)\r\n        self.params = []\r\n\r\nclass DropoutLayer(object):\r\n    """"""Dropout layer""""""\r\n    def __init__(self, inpt, keep_prob):\r\n        """"""\r\n        keep_prob: float (0, 1]\r\n        """"""\r\n        self.keep_prob = tf.placeholder(tf.float32)\r\n        self.input = inpt\r\n        self.output = tf.nn.dropout(self.input, keep_prob=self.keep_prob)\r\n        self.train_dicts = {self.keep_prob: keep_prob}\r\n        self.pred_dicts = {self.keep_prob: 1.0}\r\n\r\nif __name__ == ""__main__"":\r\n    # mnist examples\r\n    mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\r\n    # define input and output placehoders\r\n    x = tf.placeholder(tf.float32, shape=[None, 784])\r\n    y_ = tf.placeholder(tf.float32, shape=[None, 10])\r\n    # reshape\r\n    inpt = tf.reshape(x, shape=[-1, 28, 28, 1])\r\n\r\n    # create network\r\n    # params for training\r\n    # conv and pool layer0\r\n    layer0_conv = ConvLayer(inpt, filter_shape=[5, 5, 1, 32], strides=[1, 1, 1, 1], activation=tf.nn.relu,\r\n                            padding=""SAME"")              # [?, 28, 28, 32]\r\n    layer0_pool = MaxPoolLayer(layer0_conv.output, ksize=[1, 2, 2, 1],\r\n                               strides=[1, 2, 2, 1])                       # [?, 14, 14, 32]\r\n    # conv and pool layer1\r\n    layer1_conv = ConvLayer(layer0_pool.output, filter_shape=[5, 5, 32, 64], strides=[1, 1, 1, 1],\r\n                            activation=tf.nn.relu, padding=""SAME"")  # [?, 14, 14, 64]\r\n    layer1_pool = MaxPoolLayer(layer1_conv.output, ksize=[1, 2, 2, 1],\r\n                               strides=[1, 2, 2, 1])              # [?, 7, 7, 64]\r\n    # flatten layer\r\n    layer2_flatten = FlattenLayer(layer1_pool.output, shape=[-1, 7*7*64])\r\n    # fully-connected layer\r\n    layer3_fullyconn = HiddenLayer(layer2_flatten.output, n_in=7*7*64, n_out=256, activation=tf.nn.relu)\r\n    # dropout layer\r\n    layer3_dropout = DropoutLayer(layer3_fullyconn.output, keep_prob=0.5)\r\n    # the output layer\r\n    layer4_output = LogisticRegression(layer3_dropout.output, n_in=256, n_out=10)\r\n\r\n    # params for training\r\n    params = layer0_conv.params + layer1_conv.params + layer3_fullyconn.params + layer4_output.params\r\n    # train dicts for dropout\r\n    train_dicts = layer3_dropout.train_dicts\r\n    # prediction dicts for dropout\r\n    pred_dicts = layer3_dropout.pred_dicts\r\n\r\n    # get cost\r\n    cost = layer4_output.cost(y_)\r\n    # accuracy\r\n    accuracy = layer4_output.accuarcy(y_)\r\n    predictor = layer4_output.y_pred\r\n    # \xe5\xae\x9a\xe4\xb9\x89\xe8\xae\xad\xe7\xbb\x83\xe5\x99\xa8\r\n    train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(\r\n        cost, var_list=params)\r\n\r\n    # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\x98\xe9\x87\x8f\r\n    init = tf.global_variables_initializer()\r\n\r\n    # \xe5\xae\x9a\xe4\xb9\x89\xe8\xae\xad\xe7\xbb\x83\xe5\x8f\x82\xe6\x95\xb0\r\n    training_epochs = 10\r\n    batch_size = 100\r\n    display_step = 1\r\n\r\n    # \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\r\n    print(""Start to train..."")\r\n    with tf.Session() as sess:\r\n        sess.run(init)\r\n        for epoch in range(training_epochs):\r\n            avg_cost = 0.0\r\n            batch_num = int(mnist.train.num_examples / batch_size)\r\n            for i in range(batch_num):\r\n                x_batch, y_batch = mnist.train.next_batch(batch_size)\r\n                # \xe8\xae\xad\xe7\xbb\x83\r\n                train_dicts.update({x: x_batch, y_: y_batch})\r\n\r\n                sess.run(train_op, feed_dict=train_dicts)\r\n                # \xe8\xae\xa1\xe7\xae\x97cost\r\n                pred_dicts.update({x: x_batch, y_: y_batch})\r\n                avg_cost += sess.run(cost, feed_dict=pred_dicts) / batch_num\r\n            # \xe8\xbe\x93\xe5\x87\xba\r\n            if epoch % display_step == 0:\r\n                pred_dicts.update({x: mnist.validation.images,\r\n                                   y_: mnist.validation.labels})\r\n                val_acc = sess.run(accuracy, feed_dict=pred_dicts)\r\n                print(""Epoch {0} cost: {1}, validation accuacy: {2}"".format(epoch,\r\n                                                                            avg_cost, val_acc))\r\n\r\n        print(""Finished!"")\r\n        test_x = mnist.test.images[:10]\r\n        test_y = mnist.test.labels[:10]\r\n        print(""Ture lables:"")\r\n        print(""  "", np.argmax(test_y, 1))\r\n        print(""Prediction:"")\r\n        pred_dicts.update({x: test_x})\r\n        print(""  "", sess.run(predictor, feed_dict=pred_dicts))\r\n        tf.scan()\r\n\r\n\r\n\r\n\r\n\r\n'"
models/da.py,22,"b'""""""\r\nDenoising Autoencoder (DA)\r\nauthor: Ye Hu\r\n2016/12/16\r\n""""""\r\nimport os\r\nimport timeit\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom PIL import Image\r\n\r\nimport input_data\r\nfrom utils import tile_raster_images\r\n\r\n\r\n\r\nclass DA(object):\r\n    """"""A denoising autoencoder class (using tied weight)""""""\r\n    def __init__(self, inpt, n_visiable=784, n_hidden=500, W=None, bhid=None,\r\n                 bvis=None, activation=tf.nn.sigmoid):\r\n        """"""\r\n        inpt: tf.Tensor, the input\r\n        :param n_visiable: int, number of hidden units\r\n        :param n_hidden: int, number of visable units\r\n        :param W, bhid, bvis: tf.Tensor, the weight, bias tensor\r\n        """"""\r\n        self.n_visiable = n_visiable\r\n        self.n_hidden = n_hidden\r\n        # initialize the weight and bias if not given\r\n        if W is None:\r\n            bound = -4*np.sqrt(6.0 / (self.n_hidden + self.n_visiable))\r\n            W = tf.Variable(tf.random_uniform([self.n_visiable, self.n_hidden], minval=-bound,\r\n                                              maxval=bound), dtype=tf.float32)\r\n        if bhid is None:\r\n            bhid = tf.Variable(tf.zeros([n_hidden,]), dtype=tf.float32)\r\n        if bvis is None:\r\n            bvis = tf.Variable(tf.zeros([n_visiable,]), dtype=tf.float32)\r\n        self.W = W\r\n        self.b = bhid\r\n        # reconstruct params\r\n        self.b_prime = bvis\r\n        self.W_prime = tf.transpose(self.W)\r\n        # keep track of input and params\r\n        self.input = inpt\r\n        self.params = [self.W, self.b, self.b_prime]\r\n        # activation\r\n        self.activation = activation\r\n\r\n    def get_encode_values(self, inpt):\r\n        """"""Compute the encode values""""""\r\n        return self.activation(tf.matmul(inpt, self.W) + self.b)\r\n\r\n    def get_decode_values(self, encode_input):\r\n        """"""Get the reconstructed values""""""\r\n        return self.activation(tf.matmul(encode_input, self.W_prime) + self.b_prime)\r\n\r\n    def get_corrupted_input(self, inpt, corruption_level):\r\n        """"""\r\n        Randomly zero the element of input\r\n        corruption_level: float, (0,1]\r\n        """"""\r\n        # the shape of input\r\n        input_shape = tf.shape(inpt)\r\n        # the probablity for corruption\r\n        probs = tf.tile(tf.log([[corruption_level, 1-corruption_level]]),\r\n                        multiples=[input_shape[0], 1])\r\n        return tf.mul(tf.cast(tf.multinomial(probs, num_samples=input_shape[1]),\r\n                              dtype=tf.float32), inpt)\r\n\r\n    def get_cost(self, corruption_level=0.3):\r\n        """"""Get the cost for training""""""\r\n        corrupted_input = self.get_corrupted_input(self.input, corruption_level)\r\n        encode_output = self.get_encode_values(corrupted_input)\r\n        decode_output = self.get_decode_values(encode_output)\r\n        # use cross_entropy\r\n        cross = tf.mul(self.input, tf.log(decode_output)) + \\\r\n                tf.mul(1.0-self.input, tf.log(1.0-decode_output))\r\n        cost = -tf.reduce_mean(tf.reduce_sum(cross, axis=1))\r\n        return cost\r\n\r\nif __name__ == ""__main__"":\r\n    # mnist examples\r\n    mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\r\n    # define input\r\n    x = tf.placeholder(tf.float32, shape=[None, 784])\r\n    # set random_seed\r\n    tf.set_random_seed(seed=99999)\r\n    # the DA model\r\n    da = DA(x, n_visiable=784, n_hidden=500)\r\n    # corruption level\r\n    corruption_level = 0.0\r\n    learning_rate = 0.1\r\n    cost = da.get_cost(corruption_level)\r\n    params = da.params\r\n    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost, var_list=params)\r\n    init = tf.global_variables_initializer()\r\n\r\n    output_folder = ""dA_plots""\r\n    if not os.path.isdir(output_folder):\r\n        os.makedirs(output_folder)\r\n    os.chdir(output_folder)\r\n\r\n    training_epochs = 10\r\n    batch_size = 100\r\n    display_step = 1\r\n    print(""Start training..."")\r\n    start_time = timeit.default_timer()\r\n    with tf.Session() as sess:\r\n        sess.run(init)\r\n        for epoch in range(training_epochs):\r\n            avg_cost = 0.0\r\n            batch_num = int(mnist.train.num_examples / batch_size)\r\n            for i in range(batch_num):\r\n                x_batch, _ = mnist.train.next_batch(batch_size)\r\n                # \xe8\xae\xad\xe7\xbb\x83\r\n                sess.run(train_op, feed_dict={x: x_batch})\r\n                # \xe8\xae\xa1\xe7\xae\x97cost\r\n                avg_cost += sess.run(cost, feed_dict={x: x_batch,}) / batch_num\r\n            # \xe8\xbe\x93\xe5\x87\xba\r\n            if epoch % display_step == 0:\r\n                print(""Epoch {0} cost: {1}"".format(epoch, avg_cost))\r\n\r\n        end_time = timeit.default_timer()\r\n        training_time = end_time - start_time\r\n        print(""Finished!"")\r\n        print(""  The {0}%% corruption code ran for {1}."".format(corruption_level*100, training_time/60,))\r\n        W_value = sess.run(da.W_prime)\r\n        image = Image.fromarray(tile_raster_images(\r\n            X=W_value,\r\n            img_shape=(28, 28), tile_shape=(10, 10),\r\n            tile_spacing=(1, 1)))\r\n        image.save(\'filters_corruption_{0}.png\'.format(int(corruption_level*100)))\r\n\r\n\r\n\r\n\r\n'"
models/dbn.py,8,"b'""""""\r\nDeep Belief Network\r\nauthor: Ye Hu\r\n2016/12/20\r\n""""""\r\nimport timeit\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport input_data\r\nfrom logisticRegression import LogisticRegression\r\nfrom mlp import HiddenLayer\r\nfrom rbm import RBM\r\n\r\nclass DBN(object):\r\n    """"""\r\n    An implement of deep belief network\r\n    The hidden layers are firstly pretrained by RBM, then DBN is treated as a normal\r\n    MLP by adding a output layer.\r\n    """"""\r\n    def __init__(self, n_in=784, n_out=10, hidden_layers_sizes=[500, 500]):\r\n        """"""\r\n        :param n_in: int, the dimension of input\r\n        :param n_out: int, the dimension of output\r\n        :param hidden_layers_sizes: list or tuple, the hidden layer sizes\r\n        """"""\r\n        # Number of layers\r\n        assert len(hidden_layers_sizes) > 0\r\n        self.n_layers = len(hidden_layers_sizes)\r\n        self.layers = []    # normal sigmoid layer\r\n        self.rbm_layers = []   # RBM layer\r\n        self.params = []       # keep track of params for training\r\n\r\n        # Define the input and output\r\n        self.x = tf.placeholder(tf.float32, shape=[None, n_in])\r\n        self.y = tf.placeholder(tf.float32, shape=[None, n_out])\r\n\r\n        # Contruct the layers of DBN\r\n        for i in range(self.n_layers):\r\n            if i == 0:\r\n                layer_input = self.x\r\n                input_size = n_in\r\n            else:\r\n                layer_input = self.layers[i-1].output\r\n                input_size = hidden_layers_sizes[i-1]\r\n            # Sigmoid layer\r\n            sigmoid_layer = HiddenLayer(inpt=layer_input, n_in=input_size, n_out=hidden_layers_sizes[i],\r\n                                    activation=tf.nn.sigmoid)\r\n            self.layers.append(sigmoid_layer)\r\n            # Add the parameters for finetuning\r\n            self.params.extend(sigmoid_layer.params)\r\n            # Create the RBM layer\r\n            self.rbm_layers.append(RBM(inpt=layer_input, n_visiable=input_size, n_hidden=hidden_layers_sizes[i],\r\n                                        W=sigmoid_layer.W, hbias=sigmoid_layer.b))\r\n        # We use the LogisticRegression layer as the output layer\r\n        self.output_layer = LogisticRegression(inpt=self.layers[-1].output, n_in=hidden_layers_sizes[-1],\r\n                                                n_out=n_out)\r\n        self.params.extend(self.output_layer.params)\r\n        # The finetuning cost\r\n        self.cost = self.output_layer.cost(self.y)\r\n        # The accuracy\r\n        self.accuracy = self.output_layer.accuarcy(self.y)\r\n    \r\n    def pretrain(self, sess, X_train, batch_size=50, pretraining_epochs=10, lr=0.1, k=1, \r\n                    display_step=1):\r\n        """"""\r\n        Pretrain the layers (just train the RBM layers)\r\n        :param sess: tf.Session\r\n        :param X_train: the input of the train set (You might modidy this function if you do not use the desgined mnist)\r\n        :param batch_size: int\r\n        :param lr: float\r\n        :param k: int, use CD-k\r\n        :param pretraining_epoch: int\r\n        :param display_step: int\r\n        """"""\r\n        print(\'Starting pretraining...\\n\')\r\n        start_time = timeit.default_timer()\r\n        batch_num = int(X_train.train.num_examples / batch_size)\r\n        # Pretrain layer by layer\r\n        for i in range(self.n_layers):\r\n            cost = self.rbm_layers[i].get_reconstruction_cost()\r\n            train_ops = self.rbm_layers[i].get_train_ops(learning_rate=lr, k=k, persistent=None)\r\n            for epoch in range(pretraining_epochs):\r\n                avg_cost = 0.0\r\n                for j in range(batch_num):\r\n                    x_batch, _ = X_train.train.next_batch(batch_size)\r\n                    # \xe8\xae\xad\xe7\xbb\x83\r\n                    sess.run(train_ops, feed_dict={self.x: x_batch})\r\n                    # \xe8\xae\xa1\xe7\xae\x97cost\r\n                    avg_cost += sess.run(cost, feed_dict={self.x: x_batch,}) / batch_num\r\n                # \xe8\xbe\x93\xe5\x87\xba\r\n                if epoch % display_step == 0:\r\n                    print(""\\tPretraing layer {0} Epoch {1} cost: {2}"".format(i, epoch, avg_cost))\r\n\r\n        end_time = timeit.default_timer()\r\n        print(""\\nThe pretraining process ran for {0} minutes"".format((end_time - start_time) / 60))\r\n    \r\n    def finetuning(self, sess, trainSet, training_epochs=10, batch_size=100, lr=0.1,\r\n                   display_step=1):\r\n        """"""\r\n        Finetuing the network\r\n        """"""\r\n        print(""\\nStart finetuning...\\n"")\r\n        start_time = timeit.default_timer()\r\n        train_op = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(\r\n            self.cost, var_list=self.params)\r\n        for epoch in range(training_epochs):\r\n            avg_cost = 0.0\r\n            batch_num = int(trainSet.train.num_examples / batch_size)\r\n            for i in range(batch_num):\r\n                x_batch, y_batch = trainSet.train.next_batch(batch_size)\r\n                # \xe8\xae\xad\xe7\xbb\x83\r\n                sess.run(train_op, feed_dict={self.x: x_batch, self.y: y_batch})\r\n                # \xe8\xae\xa1\xe7\xae\x97cost\r\n                avg_cost += sess.run(self.cost, feed_dict=\r\n                {self.x: x_batch, self.y: y_batch}) / batch_num\r\n            # \xe8\xbe\x93\xe5\x87\xba\r\n            if epoch % display_step == 0:\r\n                val_acc = sess.run(self.accuracy, feed_dict={self.x: trainSet.validation.images,\r\n                                                       self.y: trainSet.validation.labels})\r\n                print(""\\tEpoch {0} cost: {1}, validation accuacy: {2}"".format(epoch, avg_cost, val_acc))\r\n\r\n        end_time = timeit.default_timer()\r\n        print(""\\nThe finetuning process ran for {0} minutes"".format((end_time - start_time) / 60))\r\n\r\nif __name__ == ""__main__"":\r\n    # mnist examples\r\n    mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\r\n    dbn = DBN(n_in=784, n_out=10, hidden_layers_sizes=[500, 500, 500])\r\n    sess = tf.Session()\r\n    init = tf.global_variables_initializer()\r\n    sess.run(init)\r\n    # set random_seed\r\n    tf.set_random_seed(seed=1111)\r\n    dbn.pretrain(sess, X_train=mnist)\r\n    dbn.finetuning(sess, trainSet=mnist)'"
models/gbrbm.py,14,"b'""""""\r\nRestricted Boltzmann Machines (RBM)\r\nauthor: Ye Hu\r\n2016/12/18\r\n""""""\r\nimport os\r\nimport timeit\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom PIL import Image\r\nfrom utils import tile_raster_images\r\nimport input_data\r\nfrom rbm import RBM\r\n\r\n\r\nclass GBRBM(RBM):\r\n    """"""\r\n    Gaussian-binary Restricted Boltzmann Machines\r\n    Note we assume that the standard deviation is a constant (not training parameter)\r\n    You better normalize you data with range of [0, 1.0].\r\n    """"""\r\n    def __init__(self, inpt=None, n_visiable=784, n_hidden=500, sigma=1.0, W=None,\r\n                 hbias=None, vbias=None, sample_visible=True):\r\n        """"""\r\n        :param inpt: Tensor, the input tensor [None, n_visiable]\r\n        :param n_visiable: int, number of visiable units\r\n        :param n_hidden: int, number of hidden units\r\n        :param sigma: float, the standard deviation (note we use the same \xcf\x83 for all visible units)\r\n        :param W, hbias, vbias: Tensor, the parameters of RBM (tf.Variable)\r\n        :param sample_visble: bool, if True, do gaussian sampling.\r\n        """"""\r\n        super(GBRBM, self).__init__(inpt, n_visiable, n_hidden, W, hbias, vbias)\r\n        self.sigma = sigma\r\n        self.sample_visible = sample_visible\r\n    \r\n    @staticmethod\r\n    def sample_gaussian(x, sigma):\r\n        return x + tf.random_normal(tf.shape(x), mean=0.0, stddev=sigma)\r\n\r\n    def propdown(self, h):\r\n        """"""Compute the mean for visible units given hidden units""""""\r\n        return tf.matmul(h, tf.transpose(self.W)) + self.vbias\r\n    \r\n    def sample_v_given_h(self, h0_sample):\r\n        """"""Sampling the visiable units given hidden sample""""""\r\n        v1_mean = self.propdown(h0_sample)\r\n        v1_sample = v1_mean\r\n        if self.sample_visible:\r\n            v1_sample = GBRBM.sample_gaussian(v1_mean, self.sigma)\r\n        return (v1_mean, v1_sample)\r\n    \r\n    def propup(self, v):\r\n        """"""Compute the sigmoid activation for hidden units given visible units""""""\r\n        return tf.nn.sigmoid(tf.matmul(v, self.W) / self.sigma**2 + self.hbias)\r\n    \r\n    def free_energy(self, v_sample):\r\n        """"""Compute the free energy""""""\r\n        wx_b = tf.matmul(v_sample, self.W) / self.sigma**2 + self.hbias\r\n        vbias_term = tf.reduce_sum(0.5 * tf.square(v_sample - self.vbias) / self.sigma**2, axis=1)\r\n        hidden_term = tf.reduce_sum(tf.log(1.0 + tf.exp(wx_b)), axis=1)\r\n        return -hidden_term + vbias_term\r\n    \r\n    def get_reconstruction_cost(self):\r\n        """"""Compute the mse of the original input and the reconstruction""""""\r\n        activation_h = self.propup(self.input)\r\n        activation_v = self.propdown(activation_h)\r\n        mse = tf.reduce_mean(tf.reduce_sum(tf.square(self.input - activation_v), axis=1))\r\n        return mse  \r\n        \r\n    \r\n\r\nif __name__ == ""__main__"":\r\n    # mnist examples\r\n    mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\r\n    # define input\r\n    x = tf.placeholder(tf.float32, shape=[None, 784])\r\n    # set random_seed\r\n    tf.set_random_seed(seed=99999)\r\n    np.random.seed(123)\r\n    # the rbm model\r\n    n_visiable, n_hidden = 784, 500\r\n    rbm = GBRBM(x, n_visiable=n_visiable, n_hidden=n_hidden)\r\n    \r\n    learning_rate = 0.01\r\n    batch_size = 50\r\n    cost = rbm.get_reconstruction_cost()\r\n    # Create the persistent variable\r\n    #persistent_chain = tf.Variable(tf.zeros([batch_size, n_hidden]), dtype=tf.float32)\r\n    persistent_chain = None\r\n    train_ops = rbm.get_train_ops(learning_rate=learning_rate, k=1, persistent=persistent_chain)\r\n    init = tf.global_variables_initializer()\r\n\r\n    output_folder = ""rbm_plots""\r\n    if not os.path.isdir(output_folder):\r\n        os.makedirs(output_folder)\r\n    os.chdir(output_folder)\r\n\r\n    training_epochs = 15\r\n    display_step = 1\r\n    print(""Start training..."")\r\n   \r\n    with tf.Session() as sess:\r\n        start_time = timeit.default_timer()\r\n        sess.run(init)\r\n        for epoch in range(training_epochs):\r\n            avg_cost = 0.0\r\n            batch_num = int(mnist.train.num_examples / batch_size)\r\n            for i in range(batch_num):\r\n                x_batch, _ = mnist.train.next_batch(batch_size)\r\n                # \xe8\xae\xad\xe7\xbb\x83\r\n                sess.run(train_ops, feed_dict={x: x_batch})\r\n                # \xe8\xae\xa1\xe7\xae\x97cost\r\n                avg_cost += sess.run(cost, feed_dict={x: x_batch,}) / batch_num\r\n            # \xe8\xbe\x93\xe5\x87\xba\r\n            if epoch % display_step == 0:\r\n                print(""Epoch {0} cost: {1}"".format(epoch, avg_cost))\r\n            # Construct image from the weight matrix\r\n            image = Image.fromarray(\r\n            tile_raster_images(\r\n                X=sess.run(tf.transpose(rbm.W)),\r\n                img_shape=(28, 28),\r\n                tile_shape=(10, 10),\r\n                tile_spacing=(1, 1)))\r\n            image.save(""test_filters_at_epoch_{0}.png"".format(epoch))\r\n\r\n        end_time = timeit.default_timer()\r\n        training_time = end_time - start_time\r\n        print(""Finished!"")\r\n        print(""  The training ran for {0} minutes."".format(training_time/60,))\r\n        \r\n        # Randomly select the \'n_chains\' examples\r\n        n_chains = 20\r\n        n_batch = 10\r\n        n_samples = n_batch*2\r\n        number_test_examples = mnist.test.num_examples\r\n        test_indexs = np.random.randint(number_test_examples - n_chains*n_batch)\r\n        test_samples = mnist.test.images[test_indexs:test_indexs+n_chains*n_batch]\r\n        image_data = np.zeros((29*(n_samples+1)+1, 29*(n_chains)-1),\r\n                          dtype=""uint8"")\r\n        # Add the original images\r\n        for i in range(n_batch):\r\n            image_data[2*i*29:2*i*29+28,:] = tile_raster_images(X=test_samples[i*n_batch:(i+1)*n_chains],\r\n                                            img_shape=(28, 28),\r\n                                            tile_shape=(1, n_chains),\r\n                                            tile_spacing=(1, 1))\r\n            samples = sess.run(rbm.reconstruct(x), feed_dict={x:test_samples[i*n_batch:(i+1)*n_chains]})\r\n            image_data[(2*i+1)*29:(2*i+1)*29+28,:] = tile_raster_images(X=samples,\r\n                                            img_shape=(28, 28),\r\n                                            tile_shape=(1, n_chains),\r\n                                            tile_spacing=(1, 1))\r\n        \r\n        image = Image.fromarray(image_data)\r\n        image.save(""original_and_reconstruct.png"")\r\n\r\n\r\n    '"
models/input_data.py,0,"b'# ---\n# Fichero descargado de https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/g3doc/tutorials/mnist/input_data.py\n# ---\n# Copyright 2015 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Functions for downloading and reading MNIST data.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport gzip\nimport os\nimport numpy\nfrom six.moves import urllib\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nSOURCE_URL = \'http://yann.lecun.com/exdb/mnist/\'\ndef maybe_download(filename, work_directory):\n  """"""Download the data from Yann\'s website, unless it\'s already here.""""""\n  if not os.path.exists(work_directory):\n    os.mkdir(work_directory)\n  filepath = os.path.join(work_directory, filename)\n  if not os.path.exists(filepath):\n    filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n    statinfo = os.stat(filepath)\n    print(\'Successfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n  return filepath\ndef _read32(bytestream):\n  dt = numpy.dtype(numpy.uint32).newbyteorder(\'>\')\n  return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]\ndef extract_images(filename):\n  """"""Extract the images into a 4D uint8 numpy array [index, y, x, depth].""""""\n  print(\'Extracting\', filename)\n  with gzip.open(filename) as bytestream:\n    magic = _read32(bytestream)\n    if magic != 2051:\n      raise ValueError(\n          \'Invalid magic number %d in MNIST image file: %s\' %\n          (magic, filename))\n    num_images = _read32(bytestream)\n    rows = _read32(bytestream)\n    cols = _read32(bytestream)\n    buf = bytestream.read(rows * cols * num_images)\n    data = numpy.frombuffer(buf, dtype=numpy.uint8)\n    data = data.reshape(num_images, rows, cols, 1)\n    return data\ndef dense_to_one_hot(labels_dense, num_classes=10):\n  """"""Convert class labels from scalars to one-hot vectors.""""""\n  num_labels = labels_dense.shape[0]\n  index_offset = numpy.arange(num_labels) * num_classes\n  labels_one_hot = numpy.zeros((num_labels, num_classes))\n  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n  return labels_one_hot\ndef extract_labels(filename, one_hot=False):\n  """"""Extract the labels into a 1D uint8 numpy array [index].""""""\n  print(\'Extracting\', filename)\n  with gzip.open(filename) as bytestream:\n    magic = _read32(bytestream)\n    if magic != 2049:\n      raise ValueError(\n          \'Invalid magic number %d in MNIST label file: %s\' %\n          (magic, filename))\n    num_items = _read32(bytestream)\n    buf = bytestream.read(num_items)\n    labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n    if one_hot:\n      return dense_to_one_hot(labels)\n    return labels\nclass DataSet(object):\n  def __init__(self, images, labels, fake_data=False):\n    if fake_data:\n      self._num_examples = 10000\n    else:\n      assert images.shape[0] == labels.shape[0], (\n          ""images.shape: %s labels.shape: %s"" % (images.shape,\n                                                 labels.shape))\n      self._num_examples = images.shape[0]\n      # Convert shape from [num examples, rows, columns, depth]\n      # to [num examples, rows*columns] (assuming depth == 1)\n      assert images.shape[3] == 1\n      images = images.reshape(images.shape[0],\n                              images.shape[1] * images.shape[2])\n      # Convert from [0, 255] -> [0.0, 1.0].\n      images = images.astype(numpy.float32)\n      images = numpy.multiply(images, 1.0 / 255.0)\n    self._images = images\n    self._labels = labels\n    self._epochs_completed = 0\n    self._index_in_epoch = 0\n  @property\n  def images(self):\n    return self._images\n  @property\n  def labels(self):\n    return self._labels\n  @property\n  def num_examples(self):\n    return self._num_examples\n  @property\n  def epochs_completed(self):\n    return self._epochs_completed\n  def next_batch(self, batch_size, fake_data=False):\n    """"""Return the next `batch_size` examples from this data set.""""""\n    if fake_data:\n      fake_image = [1.0 for _ in xrange(784)]\n      fake_label = 0\n      return [fake_image for _ in xrange(batch_size)], [\n          fake_label for _ in xrange(batch_size)]\n    start = self._index_in_epoch\n    self._index_in_epoch += batch_size\n    if self._index_in_epoch > self._num_examples:\n      # Finished epoch\n      self._epochs_completed += 1\n      # Shuffle the data\n      perm = numpy.arange(self._num_examples)\n      numpy.random.shuffle(perm)\n      self._images = self._images[perm]\n      self._labels = self._labels[perm]\n      # Start next epoch\n      start = 0\n      self._index_in_epoch = batch_size\n      assert batch_size <= self._num_examples\n    end = self._index_in_epoch\n    return self._images[start:end], self._labels[start:end]\ndef read_data_sets(train_dir, fake_data=False, one_hot=False):\n  class DataSets(object):\n    pass\n  data_sets = DataSets()\n  if fake_data:\n    data_sets.train = DataSet([], [], fake_data=True)\n    data_sets.validation = DataSet([], [], fake_data=True)\n    data_sets.test = DataSet([], [], fake_data=True)\n    return data_sets\n  TRAIN_IMAGES = \'train-images-idx3-ubyte.gz\'\n  TRAIN_LABELS = \'train-labels-idx1-ubyte.gz\'\n  TEST_IMAGES = \'t10k-images-idx3-ubyte.gz\'\n  TEST_LABELS = \'t10k-labels-idx1-ubyte.gz\'\n  VALIDATION_SIZE = 5000\n  local_file = maybe_download(TRAIN_IMAGES, train_dir)\n  train_images = extract_images(local_file)\n  local_file = maybe_download(TRAIN_LABELS, train_dir)\n  train_labels = extract_labels(local_file, one_hot=one_hot)\n  local_file = maybe_download(TEST_IMAGES, train_dir)\n  test_images = extract_images(local_file)\n  local_file = maybe_download(TEST_LABELS, train_dir)\n  test_labels = extract_labels(local_file, one_hot=one_hot)\n  validation_images = train_images[:VALIDATION_SIZE]\n  validation_labels = train_labels[:VALIDATION_SIZE]\n  train_images = train_images[VALIDATION_SIZE:]\n  train_labels = train_labels[VALIDATION_SIZE:]\n  data_sets.train = DataSet(train_images, train_labels)\n  data_sets.validation = DataSet(validation_images, validation_labels)\n  data_sets.test = DataSet(test_images, test_labels)\n  return data_sets\n'"
models/logisticRegression.py,14,"b'""""""\r\nLogistic Regression\r\nauthor: Ye Hu\r\n2016/12/14  update 2017/02/16\r\n""""""\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport input_data\r\n\r\nclass LogisticRegression(object):\r\n    """"""Multi-class logistic regression class""""""\r\n    def __init__(self, inpt, n_in, n_out):\r\n        """"""\r\n        inpt: tf.Tensor, (one minibatch) [None, n_in]\r\n        n_in: int, number of input units\r\n        n_out: int, number of output units\r\n        """"""\r\n        # weight\r\n        self.W = tf.Variable(tf.zeros([n_in, n_out], dtype=tf.float32))\r\n        # bias\r\n        self.b = tf.Variable(tf.zeros([n_out,]), dtype=tf.float32)\r\n        # activation output\r\n        self.output = tf.nn.softmax(tf.matmul(inpt, self.W) + self.b)\r\n        # prediction\r\n        self.y_pred = tf.argmax(self.output, axis=1)\r\n        # keep track of variables\r\n        self.params = [self.W, self.b]\r\n\r\n    def cost(self, y):\r\n        """"""\r\n        y: tf.Tensor, the target of the input\r\n        """"""\r\n        # cross_entropy\r\n        return -tf.reduce_mean(tf.reduce_sum(y * tf.log(self.output), axis=1))\r\n\r\n    def accuarcy(self, y):\r\n        """"""errors""""""\r\n        correct_pred = tf.equal(self.y_pred, tf.argmax(y, axis=1))\r\n        return tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n\r\n\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    # Load mnist dataset\r\n    mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\r\n    # Define placeholder for input and target\r\n    x = tf.placeholder(tf.float32, shape=[None, 784])\r\n    y_ = tf.placeholder(tf.float32, shape=[None, 10])\r\n\r\n    # Construct model\r\n    classifier = LogisticRegression(x, n_in=784, n_out=10)\r\n    cost = classifier.cost(y_)\r\n    accuracy = classifier.accuarcy(y_)\r\n    predictor = classifier.y_pred\r\n    # Define the train operation\r\n    train_op = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(\r\n        cost, var_list=classifier.params)\r\n\r\n    # Initialize all variables\r\n    init = tf.global_variables_initializer()\r\n\r\n    # Training settings\r\n    training_epochs = 50\r\n    batch_size = 100\r\n    display_step = 5\r\n\r\n    # Train loop\r\n    print(""Start to train..."")\r\n    with tf.Session() as sess:\r\n        sess.run(init)\r\n        for epoch in range(training_epochs):\r\n            avg_cost = 0.0\r\n            batch_num = int(mnist.train.num_examples/batch_size)\r\n            for i in range(batch_num):\r\n                x_batch, y_batch = mnist.train.next_batch(batch_size)\r\n                # Run train op\r\n                c, _ = sess.run([cost, train_op], feed_dict={x: x_batch, y_: y_batch})\r\n                # Sum up cost\r\n                avg_cost += c/batch_num\r\n\r\n            if epoch % display_step == 0:\r\n                val_acc = sess.run(accuracy, feed_dict={x: mnist.validation.images,\r\n                                                       y_: mnist.validation.labels})\r\n                print(""Epoch {0} cost: {1}, validation accuacy: {2}"".format(epoch,\r\n                      avg_cost, val_acc))\r\n\r\n        print(""Finished!"")\r\n        test_x = mnist.test.images[:10]\r\n        test_y = mnist.test.labels[:10]\r\n        print(""Ture lables:"")\r\n        print(""  "", np.argmax(test_y, 1))\r\n        print(""Prediction:"")\r\n        print(""  "", sess.run(predictor, feed_dict={x: test_x}))\r\n\r\n'"
models/mlp.py,18,"b'""""""\r\nMulti-Layer Perceptron Class\r\nauthor: Ye Hu\r\n2016/12/15\r\n""""""\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport input_data\r\nfrom logisticRegression import LogisticRegression\r\n\r\nclass HiddenLayer(object):\r\n    """"""Typical hidden layer of MLP""""""\r\n    def __init__(self, inpt, n_in, n_out, W=None, b=None,\r\n                 activation=tf.nn.sigmoid):\r\n        """"""\r\n        inpt: tf.Tensor, shape [n_examples, n_in]\r\n        n_in: int, the dimensionality of input\r\n        n_out: int, number of hidden units\r\n        W, b: tf.Tensor, weight and bias\r\n        activation: tf.op, activation function\r\n        """"""\r\n        if W is None:\r\n            bound_val = 4.0*np.sqrt(6.0/(n_in + n_out))\r\n            W = tf.Variable(tf.random_uniform([n_in, n_out], minval=-bound_val, maxval=bound_val),\r\n                            dtype=tf.float32, name=""W"")\r\n        if b is None:\r\n            b = tf.Variable(tf.zeros([n_out,]), dtype=tf.float32, name=""b"")\r\n\r\n        self.W = W\r\n        self.b = b\r\n        # the output\r\n        sum_W = tf.matmul(inpt, self.W) + self.b\r\n        self.output = activation(sum_W) if activation is not None else sum_W\r\n        # params\r\n        self.params = [self.W, self.b]\r\n\r\n\r\nclass MLP(object):\r\n    """"""Multi-layer perceptron class""""""\r\n    def __init__(self, inpt, n_in, n_hidden, n_out):\r\n        """"""\r\n        inpt: tf.Tensor, shape [n_examples, n_in]\r\n        n_in: int, the dimensionality of input\r\n        n_hidden: int, number of hidden units\r\n        n_out: int, number of output units\r\n        """"""\r\n        # hidden layer\r\n        self.hiddenLayer = HiddenLayer(inpt, n_in=n_in, n_out=n_hidden)\r\n        # output layer (logistic layer)\r\n        self.outputLayer = LogisticRegression(self.hiddenLayer.output, n_in=n_hidden,\r\n                                              n_out=n_out)\r\n        # L1 norm\r\n        self.L1 = tf.reduce_sum(tf.abs(self.hiddenLayer.W)) + \\\r\n                  tf.reduce_sum(tf.abs(self.outputLayer.W))\r\n        # L2 norm\r\n        self.L2 = tf.reduce_sum(tf.square(self.hiddenLayer.W)) + \\\r\n                  tf.reduce_sum(tf.square(self.outputLayer.W))\r\n        # cross_entropy cost function\r\n        self.cost = self.outputLayer.cost\r\n        # accuracy function\r\n        self.accuracy = self.outputLayer.accuarcy\r\n\r\n        # params\r\n        self.params = self.hiddenLayer.params + self.outputLayer.params\r\n        # keep track of input\r\n        self.input = inpt\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    # mnist examples\r\n    mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\r\n    # define input and output placehoders\r\n    x = tf.placeholder(tf.float32, shape=[None, 784])\r\n    y_ = tf.placeholder(tf.float32, shape=[None, 10])\r\n    # create mlp model\r\n    mlp_classifier = MLP(inpt=x, n_in=784, n_hidden=500, n_out=10)\r\n    # get cost\r\n    l2_reg = 0.0001\r\n    cost = mlp_classifier.cost(y_) + l2_reg*mlp_classifier.L2\r\n    # accuracy\r\n    accuracy = mlp_classifier.accuracy(y_)\r\n    predictor = mlp_classifier.outputLayer.y_pred\r\n    # \xe5\xae\x9a\xe4\xb9\x89\xe8\xae\xad\xe7\xbb\x83\xe5\x99\xa8\r\n    train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(\r\n        cost, var_list=mlp_classifier.params)\r\n\r\n    # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\x98\xe9\x87\x8f\r\n    init = tf.global_variables_initializer()\r\n\r\n    # \xe5\xae\x9a\xe4\xb9\x89\xe8\xae\xad\xe7\xbb\x83\xe5\x8f\x82\xe6\x95\xb0\r\n    training_epochs = 10\r\n    batch_size = 100\r\n    display_step = 1\r\n\r\n    # \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\r\n    print(""Start to train..."")\r\n    with tf.Session() as sess:\r\n        sess.run(init)\r\n        for epoch in range(training_epochs):\r\n            avg_cost = 0.0\r\n            batch_num = int(mnist.train.num_examples / batch_size)\r\n            for i in range(batch_num):\r\n                x_batch, y_batch = mnist.train.next_batch(batch_size)\r\n                # \xe8\xae\xad\xe7\xbb\x83\r\n                sess.run(train_op, feed_dict={x: x_batch, y_: y_batch})\r\n                # \xe8\xae\xa1\xe7\xae\x97cost\r\n                avg_cost += sess.run(cost, feed_dict={x: x_batch, y_: y_batch}) / batch_num\r\n            # \xe8\xbe\x93\xe5\x87\xba\r\n            if epoch % display_step == 0:\r\n                val_acc = sess.run(accuracy, feed_dict={x: mnist.validation.images,\r\n                                                       y_: mnist.validation.labels})\r\n                print(""Epoch {0} cost: {1}, validation accuacy: {2}"".format(epoch,\r\n                                                                            avg_cost, val_acc))\r\n\r\n        print(""Finished!"")\r\n        test_x = mnist.test.images[:10]\r\n        test_y = mnist.test.labels[:10]\r\n        print(""Ture lables:"")\r\n        print(""  "", np.argmax(test_y, 1))\r\n        print(""Prediction:"")\r\n        print(""  "", sess.run(predictor, feed_dict={x: test_x}))\r\n\r\n\r\n\r\n\r\n'"
models/rbm.py,42,"b'""""""\r\nRestricted Boltzmann Machines (RBM)\r\nauthor: Ye Hu\r\n2016/12/18\r\n""""""\r\nimport os\r\nimport timeit\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom PIL import Image\r\nfrom utils import tile_raster_images\r\nimport input_data\r\n\r\nclass RBM(object):\r\n    """"""A Restricted Boltzmann Machines class""""""\r\n    def __init__(self, inpt=None, n_visiable=784, n_hidden=500, W=None,\r\n                 hbias=None, vbias=None):\r\n        """"""\r\n        :param inpt: Tensor, the input tensor [None, n_visiable]\r\n        :param n_visiable: int, number of visiable units\r\n        :param n_hidden: int, number of hidden units\r\n        :param W, hbias, vbias: Tensor, the parameters of RBM (tf.Variable)\r\n        """"""\r\n        self.n_visiable = n_visiable\r\n        self.n_hidden = n_hidden\r\n        # Optionally initialize input\r\n        if inpt is None:\r\n            inpt = tf.placeholder(dtype=tf.float32, shape=[None, self.n_visiable])\r\n        self.input = inpt\r\n        # Initialize the parameters if not given\r\n        if W is None:\r\n            bounds = -4.0 * np.sqrt(6.0 / (self.n_visiable + self.n_hidden))\r\n            W = tf.Variable(tf.random_uniform([self.n_visiable, self.n_hidden], minval=-bounds,\r\n                                              maxval=bounds), dtype=tf.float32)\r\n        if hbias is None:\r\n            hbias = tf.Variable(tf.zeros([self.n_hidden,]), dtype=tf.float32)\r\n        if vbias is None:\r\n            vbias = tf.Variable(tf.zeros([self.n_visiable,]), dtype=tf.float32)\r\n        self.W = W\r\n        self.hbias = hbias\r\n        self.vbias = vbias\r\n        # keep track of parameters for training (DBN)\r\n        self.params = [self.W, self.hbias, self.vbias]\r\n    \r\n    def propup(self, v):\r\n        """"""Compute the sigmoid activation for hidden units given visible units""""""\r\n        return tf.nn.sigmoid(tf.matmul(v, self.W) + self.hbias)\r\n\r\n    def propdown(self, h):\r\n        """"""Compute the sigmoid activation for visible units given hidden units""""""\r\n        return tf.nn.sigmoid(tf.matmul(h, tf.transpose(self.W)) + self.vbias)\r\n    \r\n    def sample_prob(self, prob):\r\n        """"""Do sampling with the given probability (you can use binomial in Theano)""""""\r\n        return tf.nn.relu(tf.sign(prob - tf.random_uniform(tf.shape(prob))))\r\n    \r\n    def sample_h_given_v(self, v0_sample):\r\n        """"""Sampling the hidden units given visiable sample""""""\r\n        h1_mean = self.propup(v0_sample)\r\n        h1_sample = self.sample_prob(h1_mean)\r\n        return (h1_mean, h1_sample)\r\n    \r\n    def sample_v_given_h(self, h0_sample):\r\n        """"""Sampling the visiable units given hidden sample""""""\r\n        v1_mean = self.propdown(h0_sample)\r\n        v1_sample = self.sample_prob(v1_mean)\r\n        return (v1_mean, v1_sample)\r\n    \r\n    def gibbs_vhv(self, v0_sample):\r\n        """"""Implement one step of Gibbs sampling from the visiable state""""""\r\n        h1_mean, h1_sample = self.sample_h_given_v(v0_sample)\r\n        v1_mean, v1_sample = self.sample_v_given_h(h1_sample)\r\n        return (h1_mean, h1_sample, v1_mean, v1_sample)\r\n\r\n    def gibbs_hvh(self, h0_sample):\r\n        """"""Implement one step of Gibbs sampling from the hidden state""""""\r\n        v1_mean, v1_sample = self.sample_v_given_h(h0_sample)\r\n        h1_mean, h1_sample = self.sample_h_given_v(v1_sample)\r\n        return (v1_mean, v1_sample, h1_mean, h1_sample)\r\n\r\n    def free_energy(self, v_sample):\r\n        """"""Compute the free energy""""""\r\n        wx_b = tf.matmul(v_sample, self.W) + self.hbias\r\n        vbias_term = tf.matmul(v_sample, tf.expand_dims(self.vbias, axis=1))\r\n        hidden_term = tf.reduce_sum(tf.log(1.0 + tf.exp(wx_b)), axis=1)\r\n        return -hidden_term - vbias_term\r\n\r\n    def get_train_ops(self, learning_rate=0.1, k=1, persistent=None):\r\n        """"""\r\n        Get the training opts by CD-k\r\n        :params learning_rate: float\r\n        :params k: int, the number of Gibbs step (Note k=1 has been shown work surprisingly well)\r\n        :params persistent: Tensor, PCD-k (TO DO:)\r\n        """"""\r\n        # Compute the positive phase\r\n        ph_mean, ph_sample = self.sample_h_given_v(self.input)\r\n        # The old state of the chain\r\n        if persistent is None:\r\n            chain_start = ph_sample\r\n        else:\r\n            chain_start = persistent\r\n\r\n        # Use tf.while_loop to do the CD-k\r\n        cond = lambda i, nv_mean, nv_sample, nh_mean, nh_sample: i < k\r\n        body = lambda i, nv_mean, nv_sample, nh_mean, nh_sample: (i+1, ) + self.gibbs_hvh(nh_sample)\r\n        i, nv_mean, nv_sample, nh_mean, nh_sample = tf.while_loop(cond, body, loop_vars=[tf.constant(0), tf.zeros(tf.shape(self.input)), \r\n                                                            tf.zeros(tf.shape(self.input)), tf.zeros(tf.shape(chain_start)), chain_start])\r\n        """"""\r\n        # Compute the update values for each parameter\r\n        update_W = self.W + learning_rate * (tf.matmul(tf.transpose(self.input), ph_mean) - \r\n                                tf.matmul(tf.transpose(nv_sample), nh_mean)) / tf.to_float(tf.shape(self.input)[0])  # use probability\r\n        update_vbias = self.vbias + learning_rate * (tf.reduce_mean(self.input - nv_sample, axis=0))   # use binary value\r\n        update_hbias = self.hbias + learning_rate * (tf.reduce_mean(ph_mean - nh_mean, axis=0))       # use probability\r\n        # Assign the parameters new values\r\n        new_W = tf.assign(self.W, update_W)\r\n        new_vbias = tf.assign(self.vbias, update_vbias)\r\n        new_hbias = tf.assign(self.hbias, update_hbias)\r\n        """"""\r\n        chain_end = tf.stop_gradient(nv_sample)   # do not compute the gradients\r\n        cost = tf.reduce_mean(self.free_energy(self.input)) - tf.reduce_mean(self.free_energy(chain_end))\r\n        # Compute the gradients\r\n        gparams = tf.gradients(ys=[cost], xs=self.params)\r\n        new_params = []\r\n        for gparam, param in zip(gparams, self.params):\r\n            new_params.append(tf.assign(param, param - gparam*learning_rate))\r\n\r\n        if persistent is not None:\r\n            new_persistent = [tf.assign(persistent, nh_sample)]\r\n        else:\r\n            new_persistent = []\r\n        return new_params + new_persistent  # use for training\r\n\r\n    def get_reconstruction_cost(self):\r\n        """"""Compute the cross-entropy of the original input and the reconstruction""""""\r\n        activation_h = self.propup(self.input)\r\n        activation_v = self.propdown(activation_h)\r\n        # Do this to not get Nan\r\n        activation_v_clip = tf.clip_by_value(activation_v, clip_value_min=1e-30, clip_value_max=1.0)\r\n        reduce_activation_v_clip = tf.clip_by_value(1.0 - activation_v, clip_value_min=1e-30, clip_value_max=1.0)\r\n        cross_entropy = -tf.reduce_mean(tf.reduce_sum(self.input*(tf.log(activation_v_clip)) + \r\n                                    (1.0 - self.input)*(tf.log(reduce_activation_v_clip)), axis=1))\r\n        return cross_entropy   \r\n    def reconstruct(self, v):\r\n        """"""Reconstruct the original input by RBM""""""\r\n        h = self.propup(v)\r\n        return self.propdown(h) \r\n\r\nif __name__ == ""__main__"":\r\n    # mnist examples\r\n    mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\r\n    # define input\r\n    x = tf.placeholder(tf.float32, shape=[None, 784])\r\n    # set random_seed\r\n    tf.set_random_seed(seed=99999)\r\n    np.random.seed(123)\r\n    # the rbm model\r\n    n_visiable, n_hidden = 784, 500\r\n    rbm = RBM(x, n_visiable=n_visiable, n_hidden=n_hidden)\r\n    \r\n    learning_rate = 0.1\r\n    batch_size = 20\r\n    cost = rbm.get_reconstruction_cost()\r\n    # Create the persistent variable\r\n    persistent_chain = tf.Variable(tf.zeros([batch_size, n_hidden]), dtype=tf.float32)\r\n    train_ops = rbm.get_train_ops(learning_rate=learning_rate, k=15, persistent=persistent_chain)\r\n    init = tf.global_variables_initializer()\r\n\r\n    output_folder = ""rbm_plots""\r\n    if not os.path.isdir(output_folder):\r\n        os.makedirs(output_folder)\r\n    os.chdir(output_folder)\r\n\r\n    training_epochs = 15\r\n    display_step = 1\r\n    print(""Start training..."")\r\n   \r\n    with tf.Session() as sess:\r\n        start_time = timeit.default_timer()\r\n        sess.run(init)\r\n        for epoch in range(training_epochs):\r\n            avg_cost = 0.0\r\n            batch_num = int(mnist.train.num_examples / batch_size)\r\n            for i in range(batch_num):\r\n                x_batch, _ = mnist.train.next_batch(batch_size)\r\n                # \xe8\xae\xad\xe7\xbb\x83\r\n                sess.run(train_ops, feed_dict={x: x_batch})\r\n                # \xe8\xae\xa1\xe7\xae\x97cost\r\n                avg_cost += sess.run(cost, feed_dict={x: x_batch,}) / batch_num\r\n            # \xe8\xbe\x93\xe5\x87\xba\r\n            if epoch % display_step == 0:\r\n                print(""Epoch {0} cost: {1}"".format(epoch, avg_cost))\r\n            # Construct image from the weight matrix\r\n            image = Image.fromarray(\r\n            tile_raster_images(\r\n                X=sess.run(tf.transpose(rbm.W)),\r\n                img_shape=(28, 28),\r\n                tile_shape=(10, 10),\r\n                tile_spacing=(1, 1)))\r\n            image.save(""new_filters_at_epoch_{0}.png"".format(epoch))\r\n\r\n        end_time = timeit.default_timer()\r\n        training_time = end_time - start_time\r\n        print(""Finished!"")\r\n        print(""  The training ran for {0} minutes."".format(training_time/60,))\r\n        # Reconstruct the image by sampling\r\n        print(""...Sampling from the RBM"")\r\n        # the \r\n        n_chains = 20\r\n        n_samples = 10\r\n        number_test_examples = mnist.test.num_examples\r\n        # Randomly select the \'n_chains\' examples\r\n        test_indexs = np.random.randint(number_test_examples - n_chains)\r\n        test_samples = mnist.test.images[test_indexs:test_indexs+n_chains]\r\n        # Create the persistent variable saving the visiable state\r\n        persistent_v_chain = tf.Variable(tf.to_float(test_samples), dtype=tf.float32)\r\n        # The step of Gibbs\r\n        step_every = 1000\r\n        # Inplement the Gibbs\r\n        cond = lambda i, h_mean, h_sample, v_mean, v_sample: i < step_every\r\n        body = lambda i, h_mean, h_sample, v_mean, v_sample: (i+1, ) + rbm.gibbs_vhv(v_sample)\r\n        i, h_mean, h_sample, v_mean, v_sample = tf.while_loop(cond, body, loop_vars=[tf.constant(0), tf.zeros([n_chains, n_hidden]), \r\n                                                            tf.zeros([n_chains, n_hidden]), tf.zeros(tf.shape(persistent_v_chain)), persistent_v_chain])\r\n        # Update the persistent_v_chain\r\n        new_persistent_v_chain = tf.assign(persistent_v_chain, v_sample)\r\n        # Store the image by sampling\r\n        image_data = np.zeros((29*(n_samples+1)+1, 29*(n_chains)-1),\r\n                          dtype=""uint8"")\r\n        # Add the original images\r\n        image_data[0:28,:] = tile_raster_images(X=test_samples,\r\n                                            img_shape=(28, 28),\r\n                                            tile_shape=(1, n_chains),\r\n                                            tile_spacing=(1, 1))\r\n        # Initialize the variable\r\n        sess.run(tf.variables_initializer(var_list=[persistent_v_chain]))\r\n        # Do successive sampling\r\n        for idx in range(1, n_samples+1):\r\n            sample = sess.run(v_mean)\r\n            sess.run(new_persistent_v_chain)\r\n            print(""...plotting sample"", idx)\r\n            image_data[idx*29:idx*29+28,:] = tile_raster_images(X=sample,\r\n                                            img_shape=(28, 28),\r\n                                            tile_shape=(1, n_chains),\r\n                                            tile_spacing=(1, 1))\r\n        image = Image.fromarray(image_data)\r\n        image.save(""new_original_and_{0}samples.png"".format(n_samples))\r\n'"
models/sda.py,9,"b'""""""\r\nStacked Denoising Autoencoders (SDA)\r\nauthor: Ye Hu\r\n2016/12/16\r\n""""""\r\nimport timeit\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport input_data\r\n\r\nfrom logisticRegression import LogisticRegression\r\nfrom mlp import HiddenLayer\r\nfrom da import DA\r\n\r\nclass SdA(object):\r\n    """"""\r\n    Stacked denoising autoencoder class\r\n    the model is constructed by stacking several dAs\r\n    the dA layers are used to initialize the network, after pre-training,\r\n    the SdA is similar to a normal MLP\r\n    """"""\r\n    def __init__(self, n_in=784, n_out=10, hidden_layers_sizes=(500, 500),\r\n                 corruption_levels=(0.1, 0.1)):\r\n        """"""\r\n        :param n_in: int, the dimension of input\r\n        :param n_out: int, the dimension of output\r\n        :param hidden_layers_sizes: list or tuple, the hidden layer sizes\r\n        :param corruption_levels: list or tuple, the corruption lever for each layer\r\n        """"""\r\n        assert len(hidden_layers_sizes) >= 1\r\n        assert len(hidden_layers_sizes) == len(corruption_levels)\r\n        self.corruption_levels = corruption_levels\r\n        self.n_layers = len(hidden_layers_sizes)\r\n        # define the layers\r\n        self.layers = []   # the normal layers\r\n        self.dA_layers = []  # the dA layers\r\n        self.params = []     # params\r\n        # define the input and output\r\n        self.x = tf.placeholder(tf.float32, shape=[None, n_in])\r\n        self.y = tf.placeholder(tf.float32, shape=[None, n_out])\r\n        # construct the layers\r\n        for i in range(self.n_layers):\r\n            if i == 0:  # the input layer\r\n                input_size = n_in\r\n                layer_input = self.x\r\n            else:\r\n                input_size = hidden_layers_sizes[i-1]\r\n                layer_input = self.layers[i-1].output\r\n            # create the sigmoid layer\r\n            sigmoid_layer = HiddenLayer(inpt=layer_input, n_in=input_size,\r\n                                         n_out=hidden_layers_sizes[i], activation=tf.nn.sigmoid)\r\n            self.layers.append(sigmoid_layer)\r\n            # create the da layer\r\n            dA_layer = DA(inpt=layer_input, n_hidden=hidden_layers_sizes[i], n_visiable=input_size,\r\n                          W=sigmoid_layer.W, bhid=sigmoid_layer.b)\r\n            self.dA_layers.append(dA_layer)\r\n\r\n            # collect the params\r\n            self.params.extend(sigmoid_layer.params)\r\n\r\n        # add the output layer\r\n        self.output_layer = LogisticRegression(inpt=self.layers[-1].output, n_in=hidden_layers_sizes[-1],\r\n                                               n_out=n_out)\r\n        self.params.extend(self.output_layer.params)\r\n\r\n        # the finetuning cost\r\n        self.finetune_cost = self.output_layer.cost(self.y)\r\n        # the accuracy\r\n        self.accuracy = self.output_layer.accuarcy(self.y)\r\n\r\n    def pretrain(self, sess, X_train, pretraining_epochs=10, batch_size=100, learning_rate=0.001,\r\n                 display_step=1):\r\n        """"""\r\n        Pretrain the layers\r\n        :param sess: tf.Session\r\n        :param X_train: the input of the train set\r\n        :param batch_size: int\r\n        :param learning_rate: float\r\n        """"""\r\n        print(\'Starting pretraining...\')\r\n        start_time = timeit.default_timer()\r\n        batch_num = int(X_train.train.num_examples / batch_size)\r\n        for i in range(self.n_layers):\r\n            # pretraining layer by layer\r\n            cost = self.dA_layers[i].get_cost(corruption_level=self.corruption_levels[i])\r\n            params = self.dA_layers[i].params\r\n            train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost, var_list=params)\r\n            for epoch in range(pretraining_epochs):\r\n                avg_cost = 0.0\r\n                for j in range(batch_num):\r\n                    x_batch, _ = X_train.train.next_batch(batch_size)\r\n                    # \xe8\xae\xad\xe7\xbb\x83\r\n                    sess.run(train_op, feed_dict={self.x: x_batch})\r\n                    # \xe8\xae\xa1\xe7\xae\x97cost\r\n                    avg_cost += sess.run(cost, feed_dict={self.x: x_batch,}) / batch_num\r\n                # \xe8\xbe\x93\xe5\x87\xba\r\n                if epoch % display_step == 0:\r\n                    print(""Pretraing layer {0} Epoch {1} cost: {2}"".format(i, epoch, avg_cost))\r\n\r\n        end_time = timeit.default_timer()\r\n        print(""The pretraining process ran for {0}m"".format((end_time - start_time) / 60))\r\n\r\n    def finetuning(self, sess, trainSet, training_epochs=10, batch_size=100, learning_rate=0.1,\r\n                   display_step=1):\r\n        """"""Finetuing the network""""""\r\n        print(""Start finetuning..."")\r\n        start_time = timeit.default_timer()\r\n        train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(\r\n            self.finetune_cost, var_list=self.params)\r\n        for epoch in range(training_epochs):\r\n            avg_cost = 0.0\r\n            batch_num = int(trainSet.train.num_examples / batch_size)\r\n            for i in range(batch_num):\r\n                x_batch, y_batch = trainSet.train.next_batch(batch_size)\r\n                # \xe8\xae\xad\xe7\xbb\x83\r\n                sess.run(train_op, feed_dict={self.x: x_batch, self.y: y_batch})\r\n                # \xe8\xae\xa1\xe7\xae\x97cost\r\n                avg_cost += sess.run(self.finetune_cost, feed_dict=\r\n                {self.x: x_batch, self.y: y_batch}) / batch_num\r\n            # \xe8\xbe\x93\xe5\x87\xba\r\n            if epoch % display_step == 0:\r\n                val_acc = sess.run(self.accuracy, feed_dict={self.x: trainSet.validation.images,\r\n                                                       self.y: trainSet.validation.labels})\r\n                print(""  Epoch {0} cost: {1}, validation accuacy: {2}"".format(epoch, avg_cost, val_acc))\r\n\r\n        end_time = timeit.default_timer()\r\n        print(""The finetuning process ran for {0}m"".format((end_time - start_time) / 60))\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    # mnist examples\r\n    mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\r\n    sda = SdA(n_in=784, n_out=10, hidden_layers_sizes=[500, 500, 500], corruption_levels=[0.1, 0.2, 0.2])\r\n    sess = tf.Session()\r\n    init = tf.global_variables_initializer()\r\n    sess.run(init)\r\n    # set random_seed\r\n    tf.set_random_seed(seed=1111)\r\n    sda.pretrain(sess, X_train=mnist)\r\n    sda.finetuning(sess, trainSet=mnist)\r\n\r\n'"
models/utils.py,0,"b'"""""" This file contains different utility functions that are not connected\r\nin anyway to the networks presented in the tutorials, but rather help in\r\nprocessing the outputs into a more understandable way.\r\n\r\nFor example ``tile_raster_images`` helps in generating a easy to grasp\r\nimage from a set of samples or weights.\r\n""""""\r\n\r\nimport numpy\r\n\r\n\r\ndef scale_to_unit_interval(ndar, eps=1e-8):\r\n    """""" Scales all values in the ndarray ndar to be between 0 and 1 """"""\r\n    ndar = ndar.copy()\r\n    ndar -= ndar.min()\r\n    ndar *= 1.0 / (ndar.max() + eps)\r\n    return ndar\r\n\r\n\r\ndef tile_raster_images(X, img_shape, tile_shape, tile_spacing=(0, 0),\r\n                       scale_rows_to_unit_interval=True,\r\n                       output_pixel_vals=True):\r\n    """"""\r\n    Transform an array with one flattened image per row, into an array in\r\n    which images are reshaped and layed out like tiles on a floor.\r\n\r\n    This function is useful for visualizing datasets whose rows are images,\r\n    and also columns of matrices for transforming those rows\r\n    (such as the first layer of a neural net).\r\n\r\n    :type X: a 2-D ndarray or a tuple of 4 channels, elements of which can\r\n    be 2-D ndarrays or None;\r\n    :param X: a 2-D array in which every row is a flattened image.\r\n\r\n    :type img_shape: tuple; (height, width)\r\n    :param img_shape: the original shape of each image\r\n\r\n    :type tile_shape: tuple; (rows, cols)\r\n    :param tile_shape: the number of images to tile (rows, cols)\r\n\r\n    :param output_pixel_vals: if output should be pixel values (i.e. int8\r\n    values) or floats\r\n\r\n    :param scale_rows_to_unit_interval: if the values need to be scaled before\r\n    being plotted to [0,1] or not\r\n\r\n\r\n    :returns: array suitable for viewing as an image.\r\n    (See:`Image.fromarray`.)\r\n    :rtype: a 2-d array with same dtype as X.\r\n\r\n    """"""\r\n\r\n    assert len(img_shape) == 2\r\n    assert len(tile_shape) == 2\r\n    assert len(tile_spacing) == 2\r\n\r\n    # The expression below can be re-written in a more C style as\r\n    # follows :\r\n    #\r\n    # out_shape    = [0,0]\r\n    # out_shape[0] = (img_shape[0]+tile_spacing[0])*tile_shape[0] -\r\n    #                tile_spacing[0]\r\n    # out_shape[1] = (img_shape[1]+tile_spacing[1])*tile_shape[1] -\r\n    #                tile_spacing[1]\r\n    out_shape = [\r\n        (ishp + tsp) * tshp - tsp\r\n        for ishp, tshp, tsp in zip(img_shape, tile_shape, tile_spacing)\r\n    ]\r\n\r\n    if isinstance(X, tuple):\r\n        assert len(X) == 4\r\n        # Create an output numpy ndarray to store the image\r\n        if output_pixel_vals:\r\n            out_array = numpy.zeros((out_shape[0], out_shape[1], 4),\r\n                                    dtype=\'uint8\')\r\n        else:\r\n            out_array = numpy.zeros((out_shape[0], out_shape[1], 4),\r\n                                    dtype=X.dtype)\r\n\r\n        #colors default to 0, alpha defaults to 1 (opaque)\r\n        if output_pixel_vals:\r\n            channel_defaults = [0, 0, 0, 255]\r\n        else:\r\n            channel_defaults = [0., 0., 0., 1.]\r\n\r\n        for i in range(4):\r\n            if X[i] is None:\r\n                # if channel is None, fill it with zeros of the correct\r\n                # dtype\r\n                dt = out_array.dtype\r\n                if output_pixel_vals:\r\n                    dt = \'uint8\'\r\n                out_array[:, :, i] = numpy.zeros(\r\n                    out_shape,\r\n                    dtype=dt\r\n                ) + channel_defaults[i]\r\n            else:\r\n                # use a recurrent call to compute the channel and store it\r\n                # in the output\r\n                out_array[:, :, i] = tile_raster_images(\r\n                    X[i], img_shape, tile_shape, tile_spacing,\r\n                    scale_rows_to_unit_interval, output_pixel_vals)\r\n        return out_array\r\n\r\n    else:\r\n        # if we are dealing with only one channel\r\n        H, W = img_shape\r\n        Hs, Ws = tile_spacing\r\n\r\n        # generate a matrix to store the output\r\n        dt = X.dtype\r\n        if output_pixel_vals:\r\n            dt = \'uint8\'\r\n        out_array = numpy.zeros(out_shape, dtype=dt)\r\n\r\n        for tile_row in range(tile_shape[0]):\r\n            for tile_col in range(tile_shape[1]):\r\n                if tile_row * tile_shape[1] + tile_col < X.shape[0]:\r\n                    this_x = X[tile_row * tile_shape[1] + tile_col]\r\n                    if scale_rows_to_unit_interval:\r\n                        # if we should scale values to be between 0 and 1\r\n                        # do this by calling the `scale_to_unit_interval`\r\n                        # function\r\n                        this_img = scale_to_unit_interval(\r\n                            this_x.reshape(img_shape))\r\n                    else:\r\n                        this_img = this_x.reshape(img_shape)\r\n                    # add the slice to the corresponding position in the\r\n                    # output array\r\n                    c = 1\r\n                    if output_pixel_vals:\r\n                        c = 255\r\n                    out_array[\r\n                        tile_row * (H + Hs): tile_row * (H + Hs) + H,\r\n                        tile_col * (W + Ws): tile_col * (W + Ws) + W\r\n                    ] = this_img * c\r\n        return out_array'"
ObjectDetections/SSD/SSD_demo.py,3,"b'""""""\r\nSSD demo\r\n""""""\r\n\r\nimport cv2\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport matplotlib.image as mpimg\r\n\r\nfrom ssd_300_vgg import SSD\r\nfrom utils import preprocess_image, process_bboxes\r\nfrom visualization import plt_bboxes\r\n\r\n\r\nssd_net = SSD()\r\nclasses, scores, bboxes = ssd_net.detections()\r\nimages = ssd_net.images()\r\n\r\nsess = tf.Session()\r\n# Restore SSD model.\r\nckpt_filename = \'./ssd_checkpoints/ssd_vgg_300_weights.ckpt\'\r\nsess.run(tf.global_variables_initializer())\r\nsaver = tf.train.Saver()\r\nsaver.restore(sess, ckpt_filename)\r\n\r\nimg = cv2.imread(\'./demo/dog.jpg\')\r\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\nimg_prepocessed = preprocess_image(img)\r\nrclasses, rscores, rbboxes = sess.run([classes, scores, bboxes],\r\n                                      feed_dict={images: img_prepocessed})\r\nrclasses, rscores, rbboxes = process_bboxes(rclasses, rscores, rbboxes)\r\n\r\nplt_bboxes(img, rclasses, rscores, rbboxes)\r\n'"
ObjectDetections/SSD/ssd_300_vgg.py,19,"b'""""""\r\nSSD net (vgg_based) 300x300\r\n""""""\r\nfrom collections import namedtuple\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom ssd_layers import conv2d, max_pool2d, l2norm, dropout, \\\r\n    pad2d, ssd_multibox_layer\r\nfrom ssd_anchors import ssd_anchors_all_layers\r\n\r\n# SSD parameters\r\nSSDParams = namedtuple(\'SSDParameters\', [\'img_shape\',  # the input image size: 300x300\r\n                                         \'num_classes\',  # number of classes: 20+1\r\n                                         \'no_annotation_label\',\r\n                                         \'feat_layers\', # list of names of layer for detection\r\n                                         \'feat_shapes\', # list of feature map sizes of layer for detection\r\n                                         \'anchor_size_bounds\', # the down and upper bounds of anchor sizes\r\n                                         \'anchor_sizes\',   # list of anchor sizes of layer for detection\r\n                                         \'anchor_ratios\',  # list of rations used in layer for detection\r\n                                         \'anchor_steps\',   # list of cell size (pixel size) of layer for detection\r\n                                         \'anchor_offset\',  # the center point offset\r\n                                         \'normalizations\', # list of normalizations of layer for detection\r\n                                         \'prior_scaling\'   #\r\n                                         ])\r\nclass SSD(object):\r\n    """"""SSD net 300""""""\r\n    def __init__(self, is_training=True):\r\n        self.is_training = is_training\r\n        self.threshold = 0.5  # class score threshold\r\n        self.ssd_params = SSDParams(img_shape=(300, 300),\r\n                                    num_classes=21,\r\n                                    no_annotation_label=21,\r\n                                    feat_layers=[""block4"", ""block7"", ""block8"", ""block9"", ""block10"", ""block11""],\r\n                                    feat_shapes=[(38, 38), (19, 19), (10, 10), (5, 5), (3, 3), (1, 1)],\r\n                                    anchor_size_bounds=[0.15, 0.90],  # diff from the original paper\r\n                                    anchor_sizes=[(21., 45.),\r\n                                                  (45., 99.),\r\n                                                  (99., 153.),\r\n                                                  (153., 207.),\r\n                                                  (207., 261.),\r\n                                                  (261., 315.)],\r\n                                    anchor_ratios=[[2, .5],\r\n                                                   [2, .5, 3, 1. / 3],\r\n                                                   [2, .5, 3, 1. / 3],\r\n                                                   [2, .5, 3, 1. / 3],\r\n                                                   [2, .5],\r\n                                                   [2, .5]],\r\n                                    anchor_steps=[8, 16, 32, 64, 100, 300],\r\n                                    anchor_offset=0.5,\r\n                                    normalizations=[20, -1, -1, -1, -1, -1],\r\n                                    prior_scaling=[0.1, 0.1, 0.2, 0.2]\r\n                                    )\r\n\r\n        predictions, logits, locations = self._built_net()\r\n        #self._update_feat_shapes_from_net()\r\n        classes, scores, bboxes = self._bboxes_select(predictions, locations)\r\n        self._classes = classes\r\n        self._scores = scores\r\n        self._bboxes = bboxes\r\n\r\n    def _built_net(self):\r\n        """"""Construct the SSD net""""""\r\n        self.end_points = {}  # record the detection layers output\r\n        self._images = tf.placeholder(tf.float32, shape=[None, self.ssd_params.img_shape[0],\r\n                                                        self.ssd_params.img_shape[1], 3])\r\n        with tf.variable_scope(""ssd_300_vgg""):\r\n            # original vgg layers\r\n            # block 1\r\n            net = conv2d(self._images, 64, 3, scope=""conv1_1"")\r\n            net = conv2d(net, 64, 3, scope=""conv1_2"")\r\n            self.end_points[""block1""] = net\r\n            net = max_pool2d(net, 2, scope=""pool1"")\r\n            # block 2\r\n            net = conv2d(net, 128, 3, scope=""conv2_1"")\r\n            net = conv2d(net, 128, 3, scope=""conv2_2"")\r\n            self.end_points[""block2""] = net\r\n            net = max_pool2d(net, 2, scope=""pool2"")\r\n            # block 3\r\n            net = conv2d(net, 256, 3, scope=""conv3_1"")\r\n            net = conv2d(net, 256, 3, scope=""conv3_2"")\r\n            net = conv2d(net, 256, 3, scope=""conv3_3"")\r\n            self.end_points[""block3""] = net\r\n            net = max_pool2d(net, 2, scope=""pool3"")\r\n            # block 4\r\n            net = conv2d(net, 512, 3, scope=""conv4_1"")\r\n            net = conv2d(net, 512, 3, scope=""conv4_2"")\r\n            net = conv2d(net, 512, 3, scope=""conv4_3"")\r\n            self.end_points[""block4""] = net\r\n            net = max_pool2d(net, 2, scope=""pool4"")\r\n            # block 5\r\n            net = conv2d(net, 512, 3, scope=""conv5_1"")\r\n            net = conv2d(net, 512, 3, scope=""conv5_2"")\r\n            net = conv2d(net, 512, 3, scope=""conv5_3"")\r\n            self.end_points[""block5""] = net\r\n            print(net)\r\n            net = max_pool2d(net, 3, stride=1, scope=""pool5"")\r\n            print(net)\r\n\r\n            # additional SSD layers\r\n            # block 6: use dilate conv\r\n            net = conv2d(net, 1024, 3, dilation_rate=6, scope=""conv6"")\r\n            self.end_points[""block6""] = net\r\n            #net = dropout(net, is_training=self.is_training)\r\n            # block 7\r\n            net = conv2d(net, 1024, 1, scope=""conv7"")\r\n            self.end_points[""block7""] = net\r\n            # block 8\r\n            net = conv2d(net, 256, 1, scope=""conv8_1x1"")\r\n            net = conv2d(pad2d(net, 1), 512, 3, stride=2, scope=""conv8_3x3"",\r\n                         padding=""valid"")\r\n            self.end_points[""block8""] = net\r\n            # block 9\r\n            net = conv2d(net, 128, 1, scope=""conv9_1x1"")\r\n            net = conv2d(pad2d(net, 1), 256, 3, stride=2, scope=""conv9_3x3"",\r\n                         padding=""valid"")\r\n            self.end_points[""block9""] = net\r\n            # block 10\r\n            net = conv2d(net, 128, 1, scope=""conv10_1x1"")\r\n            net = conv2d(net, 256, 3, scope=""conv10_3x3"", padding=""valid"")\r\n            self.end_points[""block10""] = net\r\n            # block 11\r\n            net = conv2d(net, 128, 1, scope=""conv11_1x1"")\r\n            net = conv2d(net, 256, 3, scope=""conv11_3x3"", padding=""valid"")\r\n            self.end_points[""block11""] = net\r\n\r\n            # class and location predictions\r\n            predictions = []\r\n            logits = []\r\n            locations = []\r\n            for i, layer in enumerate(self.ssd_params.feat_layers):\r\n                cls, loc = ssd_multibox_layer(self.end_points[layer], self.ssd_params.num_classes,\r\n                                              self.ssd_params.anchor_sizes[i],\r\n                                              self.ssd_params.anchor_ratios[i],\r\n                                              self.ssd_params.normalizations[i], scope=layer+""_box"")\r\n                predictions.append(tf.nn.softmax(cls))\r\n                logits.append(cls)\r\n                locations.append(loc)\r\n            return predictions, logits, locations\r\n\r\n    def _update_feat_shapes_from_net(self, predictions):\r\n        """""" Obtain the feature shapes from the prediction layers""""""\r\n        new_feat_shapes = []\r\n        for l in predictions:\r\n            new_feat_shapes.append(l.get_shape().as_list()[1:])\r\n        self.ssd_params._replace(feat_shapes=new_feat_shapes)\r\n\r\n    def anchors(self):\r\n        """"""Get sSD anchors""""""\r\n        return ssd_anchors_all_layers(self.ssd_params.img_shape,\r\n                                      self.ssd_params.feat_shapes,\r\n                                      self.ssd_params.anchor_sizes,\r\n                                      self.ssd_params.anchor_ratios,\r\n                                      self.ssd_params.anchor_steps,\r\n                                      self.ssd_params.anchor_offset,\r\n                                      np.float32)\r\n\r\n    def _bboxes_decode_layer(self, feat_locations, anchor_bboxes, prior_scaling):\r\n        """"""\r\n        Decode the feat location of one layer\r\n        params:\r\n         feat_locations: 5D Tensor, [batch_size, size, size, n_anchors, 4]\r\n         anchor_bboxes: list of Tensors(y, x, w, h)\r\n                        shape: [size,size,1], [size, size,1], [n_anchors], [n_anchors]\r\n         prior_scaling: list of 4 floats\r\n        """"""\r\n        yref, xref, href, wref = anchor_bboxes\r\n        print(yref)\r\n        # Compute center, height and width\r\n        cx = feat_locations[:, :, :, :, 0] * wref * prior_scaling[0] + xref\r\n        cy = feat_locations[:, :, :, :, 1] * href * prior_scaling[1] + yref\r\n        w = wref * tf.exp(feat_locations[:, :, :, :, 2] * prior_scaling[2])\r\n        h = href * tf.exp(feat_locations[:, :, :, :, 3] * prior_scaling[3])\r\n        # compute boxes coordinates (ymin, xmin, ymax,,xmax)\r\n        bboxes = tf.stack([cy - h / 2., cx - w / 2.,\r\n                           cy + h / 2., cx + w / 2.], axis=-1)\r\n        # shape [batch_size, size, size, n_anchors, 4]\r\n        return bboxes\r\n\r\n    def _bboxes_select_layer(self, feat_predictions, feat_locations, anchor_bboxes,\r\n                             prior_scaling):\r\n        """"""Select boxes from the feat layer, only for bacth_size=1""""""\r\n        n_bboxes = np.product(feat_predictions.get_shape().as_list()[1:-1])\r\n        # decode the location\r\n        bboxes = self._bboxes_decode_layer(feat_locations, anchor_bboxes, prior_scaling)\r\n        bboxes = tf.reshape(bboxes, [n_bboxes, 4])\r\n        predictions = tf.reshape(feat_predictions, [n_bboxes, self.ssd_params.num_classes])\r\n        # remove the background predictions\r\n        sub_predictions = predictions[:, 1:]\r\n        # choose the max score class\r\n        classes = tf.argmax(sub_predictions, axis=1) + 1  # class labels\r\n        scores = tf.reduce_max(sub_predictions, axis=1)   # max_class scores\r\n        # Boxes selection: use threshold\r\n        filter_mask = scores > self.threshold\r\n        classes = tf.boolean_mask(classes, filter_mask)\r\n        scores = tf.boolean_mask(scores, filter_mask)\r\n        bboxes = tf.boolean_mask(bboxes, filter_mask)\r\n        return classes, scores, bboxes\r\n\r\n    def _bboxes_select(self, predictions, locations):\r\n        """"""Select all bboxes predictions, only for bacth_size=1""""""\r\n        anchor_bboxes_list = self.anchors()\r\n        classes_list = []\r\n        scores_list = []\r\n        bboxes_list = []\r\n        # select bboxes for each feat layer\r\n        for n in range(len(predictions)):\r\n            anchor_bboxes = list(map(tf.convert_to_tensor, anchor_bboxes_list[n]))\r\n            classes, scores, bboxes = self._bboxes_select_layer(predictions[n],\r\n                            locations[n], anchor_bboxes, self.ssd_params.prior_scaling)\r\n            classes_list.append(classes)\r\n            scores_list.append(scores)\r\n            bboxes_list.append(bboxes)\r\n        # combine all feat layers\r\n        classes = tf.concat(classes_list, axis=0)\r\n        scores = tf.concat(scores_list, axis=0)\r\n        bboxes = tf.concat(bboxes_list, axis=0)\r\n        return classes, scores, bboxes\r\n\r\n    def images(self):\r\n        return self._images\r\n\r\n    def detections(self):\r\n        return self._classes, self._scores, self._bboxes\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    ssd = SSD()\r\n    sess = tf.Session()\r\n    saver_ = tf.train.Saver()\r\n    saver_.restore(sess, ""../SSD-Tensorflow-master/ssd_checkpoints/ssd_vgg_300_weights.ckpt"")\r\n\r\n'"
ObjectDetections/SSD/ssd_anchors.py,0,"b'""""""\r\nSSD anchors\r\n""""""\r\nimport math\r\n\r\nimport numpy as np\r\n\r\ndef ssd_size_bounds_to_values(size_bounds,\r\n                              n_feat_layers,\r\n                              img_shape=(300, 300)):\r\n    """"""Compute the reference sizes of the anchor boxes from relative bounds.\r\n    The absolute values are measured in pixels, based on the network\r\n    default size (300 pixels).\r\n\r\n    This function follows the computation performed in the original\r\n    implementation of SSD in Caffe.\r\n\r\n    Return:\r\n      list of list containing the absolute sizes at each scale. For each scale,\r\n      the ratios only apply to the first value.\r\n    """"""\r\n    assert img_shape[0] == img_shape[1]\r\n\r\n    img_size = img_shape[0]\r\n    min_ratio = int(size_bounds[0] * 100)\r\n    max_ratio = int(size_bounds[1] * 100)\r\n    step = int(math.floor((max_ratio - min_ratio) / (n_feat_layers - 2)))\r\n    # Start with the following smallest sizes.\r\n    sizes = [[img_size * size_bounds[0] / 2, img_size * size_bounds[0]]]\r\n    for ratio in range(min_ratio, max_ratio + 1, step):\r\n        sizes.append((img_size * ratio / 100.,\r\n                      img_size * (ratio + step) / 100.))\r\n    return sizes\r\n\r\ndef ssd_anchor_one_layer(img_shape,\r\n                         feat_shape,\r\n                         sizes,\r\n                         ratios,\r\n                         step,\r\n                         offset=0.5,\r\n                         dtype=np.float32):\r\n    """"""Computer SSD default anchor boxes for one feature layer.\r\n\r\n    Determine the relative position grid of the centers, and the relative\r\n    width and height.\r\n\r\n    Arguments:\r\n      feat_shape: Feature shape, used for computing relative position grids;\r\n      size: Absolute reference sizes;\r\n      ratios: Ratios to use on these features;\r\n      img_shape: Image shape, used for computing height, width relatively to the\r\n        former;\r\n      offset: Grid offset.\r\n\r\n    Return:\r\n      y, x, h, w: Relative x and y grids, and height and width.\r\n    """"""\r\n    # Compute the position grid: simple way.\r\n    # y, x = np.mgrid[0:feat_shape[0], 0:feat_shape[1]]\r\n    # y = (y.astype(dtype) + offset) / feat_shape[0]\r\n    # x = (x.astype(dtype) + offset) / feat_shape[1]\r\n    # Weird SSD-Caffe computation using steps values...\r\n    y, x = np.mgrid[0:feat_shape[0], 0:feat_shape[1]]\r\n    y = (y.astype(dtype) + offset) * step / img_shape[0]\r\n    x = (x.astype(dtype) + offset) * step / img_shape[1]\r\n\r\n    # Expand dims to support easy broadcasting.\r\n    y = np.expand_dims(y, axis=-1)  # [size, size, 1]\r\n    x = np.expand_dims(x, axis=-1)  # [size, size, 1]\r\n\r\n    # Compute relative height and width.\r\n    # Tries to follow the original implementation of SSD for the order.\r\n    num_anchors = len(sizes) + len(ratios)\r\n    h = np.zeros((num_anchors, ), dtype=dtype)  # [n_anchors]\r\n    w = np.zeros((num_anchors, ), dtype=dtype)  # [n_anchors]\r\n    # Add first anchor boxes with ratio=1.\r\n    h[0] = sizes[0] / img_shape[0]\r\n    w[0] = sizes[0] / img_shape[1]\r\n    di = 1\r\n    if len(sizes) > 1:\r\n        h[1] = math.sqrt(sizes[0] * sizes[1]) / img_shape[0]\r\n        w[1] = math.sqrt(sizes[0] * sizes[1]) / img_shape[1]\r\n        di += 1\r\n    for i, r in enumerate(ratios):\r\n        h[i+di] = sizes[0] / img_shape[0] / math.sqrt(r)\r\n        w[i+di] = sizes[0] / img_shape[1] * math.sqrt(r)\r\n    return y, x, h, w\r\n\r\n\r\ndef ssd_anchors_all_layers(img_shape,\r\n                           layers_shape,\r\n                           anchor_sizes,\r\n                           anchor_ratios,\r\n                           anchor_steps,\r\n                           offset=0.5,\r\n                           dtype=np.float32):\r\n    """"""Compute anchor boxes for all feature layers.\r\n    """"""\r\n    layers_anchors = []\r\n    for i, s in enumerate(layers_shape):\r\n        anchor_bboxes = ssd_anchor_one_layer(img_shape, s,\r\n                                             anchor_sizes[i],\r\n                                             anchor_ratios[i],\r\n                                             anchor_steps[i],\r\n                                             offset=offset, dtype=dtype)\r\n        layers_anchors.append(anchor_bboxes)\r\n    return layers_anchors'"
ObjectDetections/SSD/ssd_layers.py,12,"b'""""""\r\nLayers for SSD\r\n""""""\r\n\r\nimport tensorflow as tf\r\n\r\n# Conv2d: for stride = 1\r\ndef conv2d(x, filters, kernel_size, stride=1, padding=""same"",\r\n           dilation_rate=1, activation=tf.nn.relu, scope=""conv2d""):\r\n    kernel_sizes = [kernel_size] * 2\r\n    strides = [stride] * 2\r\n    dilation_rate = [dilation_rate] * 2\r\n    return tf.layers.conv2d(x, filters, kernel_sizes, strides=strides,\r\n                            dilation_rate=dilation_rate, padding=padding,\r\n                            name=scope, activation=activation)\r\n\r\n# max pool2d: default pool_size = stride\r\ndef max_pool2d(x, pool_size, stride=None, scope=""max_pool2d""):\r\n    pool_sizes = [pool_size] * 2\r\n    strides = [pool_size] * 2 if stride is None else [stride] * 2\r\n    return tf.layers.max_pooling2d(x, pool_sizes, strides, name=scope, padding=""same"")\r\n\r\n# pad2d: for conv2d with stride > 1\r\ndef pad2d(x, pad):\r\n    return tf.pad(x, paddings=[[0, 0], [pad, pad], [pad, pad], [0, 0]])\r\n\r\n# dropout\r\ndef dropout(x, rate=0.5, is_training=True):\r\n    return tf.layers.dropout(x, rate=rate, training=is_training)\r\n\r\n# l2norm (not bacth norm, spatial normalization)\r\ndef l2norm(x, scale, trainable=True, scope=""L2Normalization""):\r\n    n_channels = x.get_shape().as_list()[-1]\r\n    l2_norm = tf.nn.l2_normalize(x, [3], epsilon=1e-12)\r\n    with tf.variable_scope(scope):\r\n        gamma = tf.get_variable(""gamma"", shape=[n_channels, ], dtype=tf.float32,\r\n                                initializer=tf.constant_initializer(scale),\r\n                                trainable=trainable)\r\n        return l2_norm * gamma\r\n\r\n\r\n# multibox layer: get class and location predicitions from detection layer\r\ndef ssd_multibox_layer(x, num_classes, sizes, ratios, normalization=-1, scope=""multibox""):\r\n    pre_shape = x.get_shape().as_list()[1:-1]\r\n    pre_shape = [-1] + pre_shape\r\n    with tf.variable_scope(scope):\r\n        # l2 norm\r\n        if normalization > 0:\r\n            x = l2norm(x, normalization)\r\n            print(x)\r\n        # numbers of anchors\r\n        n_anchors = len(sizes) + len(ratios)\r\n        # location predictions\r\n        loc_pred = conv2d(x, n_anchors*4, 3, activation=None, scope=""conv_loc"")\r\n        loc_pred = tf.reshape(loc_pred, pre_shape + [n_anchors, 4])\r\n        # class prediction\r\n        cls_pred = conv2d(x, n_anchors*num_classes, 3, activation=None, scope=""conv_cls"")\r\n        cls_pred = tf.reshape(cls_pred, pre_shape + [n_anchors, num_classes])\r\n        return cls_pred, loc_pred\r\n\r\n\r\n'"
ObjectDetections/SSD/utils.py,0,"b'""""""\r\nHelp functions for SSD\r\n""""""\r\n\r\nimport cv2\r\nimport numpy as np\r\n\r\n\r\n############## preprocess image ##################\r\n# whiten the image\r\ndef whiten_image(image, means=(123., 117., 104.)):\r\n    """"""Subtracts the given means from each image channel""""""\r\n    if image.ndim != 3:\r\n        raise ValueError(\'Input must be of size [height, width, C>0]\')\r\n    num_channels = image.shape[-1]\r\n    if len(means) != num_channels:\r\n        raise ValueError(\'len(means) must match the number of channels\')\r\n\r\n    mean = np.array(means, dtype=image.dtype)\r\n    image = image - mean\r\n    return image\r\n\r\ndef resize_image(image, size=(300, 300)):\r\n    return cv2.resize(image, size)\r\n\r\ndef preprocess_image(image):\r\n    """"""Preprocess a image to inference""""""\r\n    image_cp = np.copy(image).astype(np.float32)\r\n    # whiten the image\r\n    image_whitened = whiten_image(image_cp)\r\n    # resize the image\r\n    image_resized = resize_image(image_whitened)\r\n    # expand the batch_size dim\r\n    image_expanded = np.expand_dims(image_resized, axis=0)\r\n    return image_expanded\r\n\r\n############## process bboxes ##################\r\ndef bboxes_clip(bbox_ref, bboxes):\r\n    """"""Clip bounding boxes with respect to reference bbox.\r\n    """"""\r\n    bboxes = np.copy(bboxes)\r\n    bboxes = np.transpose(bboxes)\r\n    bbox_ref = np.transpose(bbox_ref)\r\n    bboxes[0] = np.maximum(bboxes[0], bbox_ref[0])\r\n    bboxes[1] = np.maximum(bboxes[1], bbox_ref[1])\r\n    bboxes[2] = np.minimum(bboxes[2], bbox_ref[2])\r\n    bboxes[3] = np.minimum(bboxes[3], bbox_ref[3])\r\n    bboxes = np.transpose(bboxes)\r\n    return bboxes\r\n\r\ndef bboxes_sort(classes, scores, bboxes, top_k=400):\r\n    """"""Sort bounding boxes by decreasing order and keep only the top_k\r\n    """"""\r\n    # if priority_inside:\r\n    #     inside = (bboxes[:, 0] > margin) & (bboxes[:, 1] > margin) & \\\r\n    #         (bboxes[:, 2] < 1-margin) & (bboxes[:, 3] < 1-margin)\r\n    #     idxes = np.argsort(-scores)\r\n    #     inside = inside[idxes]\r\n    #     idxes = np.concatenate([idxes[inside], idxes[~inside]])\r\n    idxes = np.argsort(-scores)\r\n    classes = classes[idxes][:top_k]\r\n    scores = scores[idxes][:top_k]\r\n    bboxes = bboxes[idxes][:top_k]\r\n    return classes, scores, bboxes\r\n\r\ndef bboxes_iou(bboxes1, bboxes2):\r\n    """"""Computing iou between bboxes1 and bboxes2.\r\n    Note: bboxes1 and bboxes2 can be multi-dimensional, but should broacastable.\r\n    """"""\r\n    bboxes1 = np.transpose(bboxes1)\r\n    bboxes2 = np.transpose(bboxes2)\r\n    # Intersection bbox and volume.\r\n    int_ymin = np.maximum(bboxes1[0], bboxes2[0])\r\n    int_xmin = np.maximum(bboxes1[1], bboxes2[1])\r\n    int_ymax = np.minimum(bboxes1[2], bboxes2[2])\r\n    int_xmax = np.minimum(bboxes1[3], bboxes2[3])\r\n\r\n    int_h = np.maximum(int_ymax - int_ymin, 0.)\r\n    int_w = np.maximum(int_xmax - int_xmin, 0.)\r\n    int_vol = int_h * int_w\r\n    # Union volume.\r\n    vol1 = (bboxes1[2] - bboxes1[0]) * (bboxes1[3] - bboxes1[1])\r\n    vol2 = (bboxes2[2] - bboxes2[0]) * (bboxes2[3] - bboxes2[1])\r\n    iou = int_vol / (vol1 + vol2 - int_vol)\r\n    return iou\r\n\r\ndef bboxes_nms(classes, scores, bboxes, nms_threshold=0.5):\r\n    """"""Apply non-maximum selection to bounding boxes.\r\n    """"""\r\n    keep_bboxes = np.ones(scores.shape, dtype=np.bool)\r\n    for i in range(scores.size-1):\r\n        if keep_bboxes[i]:\r\n            # Computer overlap with bboxes which are following.\r\n            overlap = bboxes_iou(bboxes[i], bboxes[(i+1):])\r\n            # Overlap threshold for keeping + checking part of the same class\r\n            keep_overlap = np.logical_or(overlap < nms_threshold, classes[(i+1):] != classes[i])\r\n            keep_bboxes[(i+1):] = np.logical_and(keep_bboxes[(i+1):], keep_overlap)\r\n\r\n    idxes = np.where(keep_bboxes)\r\n    return classes[idxes], scores[idxes], bboxes[idxes]\r\n\r\ndef bboxes_resize(bbox_ref, bboxes):\r\n    """"""Resize bounding boxes based on a reference bounding box,\r\n    assuming that the latter is [0, 0, 1, 1] after transform.\r\n    """"""\r\n    bboxes = np.copy(bboxes)\r\n    # Translate.\r\n    bboxes[:, 0] -= bbox_ref[0]\r\n    bboxes[:, 1] -= bbox_ref[1]\r\n    bboxes[:, 2] -= bbox_ref[0]\r\n    bboxes[:, 3] -= bbox_ref[1]\r\n    # Resize.\r\n    resize = [bbox_ref[2] - bbox_ref[0], bbox_ref[3] - bbox_ref[1]]\r\n    bboxes[:, 0] /= resize[0]\r\n    bboxes[:, 1] /= resize[1]\r\n    bboxes[:, 2] /= resize[0]\r\n    bboxes[:, 3] /= resize[1]\r\n    return bboxes\r\n\r\ndef process_bboxes(rclasses, rscores, rbboxes, rbbox_img = (0.0, 0.0, 1.0, 1.0),\r\n                   top_k=400, nms_threshold=0.5):\r\n    """"""Process the bboxes including sort and nms""""""\r\n    rbboxes = bboxes_clip(rbbox_img, rbboxes)\r\n    rclasses, rscores, rbboxes = bboxes_sort(rclasses, rscores, rbboxes, top_k)\r\n    rclasses, rscores, rbboxes = bboxes_nms(rclasses, rscores, rbboxes, nms_threshold)\r\n    rbboxes = bboxes_resize(rbbox_img, rbboxes)\r\n    return rclasses, rscores, rbboxes\r\n\r\n\r\n\r\n'"
ObjectDetections/SSD/visualization.py,0,"b'# Copyright 2017 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport cv2\nimport random\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport matplotlib.cm as mpcm\n\n\n# class names\nCLASSES = [""aeroplane"", ""bicycle"", ""bird"", ""boat"", ""bottle"",\n                        ""bus"", ""car"", ""cat"", ""chair"", ""cow"", ""diningtable"",\n                        ""dog"", ""horse"", ""motorbike"", ""person"", ""pottedplant"",\n                        ""sheep"", ""sofa"", ""train"",""tvmonitor""]\n# =========================================================================== #\n# Some colormaps.\n# =========================================================================== #\ndef colors_subselect(colors, num_classes=21):\n    dt = len(colors) // num_classes\n    sub_colors = []\n    for i in range(num_classes):\n        color = colors[i*dt]\n        if isinstance(color[0], float):\n            sub_colors.append([int(c * 255) for c in color])\n        else:\n            sub_colors.append([c for c in color])\n    return sub_colors\n\ncolors_plasma = colors_subselect(mpcm.plasma.colors, num_classes=21)\ncolors_tableau = [(255, 255, 255), (31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),\n                  (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),\n                  (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),\n                  (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),\n                  (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]\n\n\n# =========================================================================== #\n# OpenCV drawing.\n# =========================================================================== #\ndef draw_lines(img, lines, color=[255, 0, 0], thickness=2):\n    """"""Draw a collection of lines on an image.\n    """"""\n    for line in lines:\n        for x1, y1, x2, y2 in line:\n            cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n\n\ndef draw_rectangle(img, p1, p2, color=[255, 0, 0], thickness=2):\n    cv2.rectangle(img, p1[::-1], p2[::-1], color, thickness)\n\n\ndef draw_bbox(img, bbox, shape, label, color=[255, 0, 0], thickness=2):\n    p1 = (int(bbox[0] * shape[0]), int(bbox[1] * shape[1]))\n    p2 = (int(bbox[2] * shape[0]), int(bbox[3] * shape[1]))\n    cv2.rectangle(img, p1[::-1], p2[::-1], color, thickness)\n    p1 = (p1[0]+15, p1[1])\n    cv2.putText(img, str(label), p1[::-1], cv2.FONT_HERSHEY_DUPLEX, 0.5, color, 1)\n\n\ndef bboxes_draw_on_img(img, classes, scores, bboxes, colors, thickness=2):\n    shape = img.shape\n    for i in range(bboxes.shape[0]):\n        bbox = bboxes[i]\n        color = colors[classes[i]]\n        # Draw bounding box...\n        p1 = (int(bbox[0] * shape[0]), int(bbox[1] * shape[1]))\n        p2 = (int(bbox[2] * shape[0]), int(bbox[3] * shape[1]))\n        cv2.rectangle(img, p1[::-1], p2[::-1], color, thickness)\n        # Draw text...\n        s = \'%s/%.3f\' % (classes[i], scores[i])\n        p1 = (p1[0]-5, p1[1])\n        cv2.putText(img, s, p1[::-1], cv2.FONT_HERSHEY_DUPLEX, 0.4, color, 1)\n\n\n# =========================================================================== #\n# Matplotlib show...\n# =========================================================================== #\ndef plt_bboxes(img, classes, scores, bboxes, figsize=(10,10), linewidth=1.5, show_class_name=True):\n    """"""Visualize bounding boxes. Largely inspired by SSD-MXNET!\n    """"""\n    fig = plt.figure(figsize=figsize)\n    plt.imshow(img)\n    height = img.shape[0]\n    width = img.shape[1]\n    colors = dict()\n    for i in range(classes.shape[0]):\n        cls_id = int(classes[i])\n        if cls_id >= 0:\n            score = scores[i]\n            if cls_id not in colors:\n                colors[cls_id] = (random.random(), random.random(), random.random())\n            ymin = int(bboxes[i, 0] * height)\n            xmin = int(bboxes[i, 1] * width)\n            ymax = int(bboxes[i, 2] * height)\n            xmax = int(bboxes[i, 3] * width)\n            rect = plt.Rectangle((xmin, ymin), xmax - xmin,\n                                 ymax - ymin, fill=False,\n                                 edgecolor=colors[cls_id],\n                                 linewidth=linewidth)\n            plt.gca().add_patch(rect)\n            class_name = CLASSES[cls_id-1] if show_class_name else str(cls_id)\n            plt.gca().text(xmin, ymin - 2,\n                           \'{:s} | {:.3f}\'.format(class_name, score),\n                           bbox=dict(facecolor=colors[cls_id], alpha=0.5),\n                           fontsize=12, color=\'white\')\n    plt.show()\n'"
ObjectDetections/yolo/yolo.py,15,"b'""""""\r\nYolo V1 by tensorflow\r\n""""""\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport cv2\r\n\r\n\r\ndef leak_relu(x, alpha=0.1):\r\n    return tf.maximum(alpha * x, x)\r\n\r\nclass Yolo(object):\r\n    def __init__(self, weights_file):\r\n        self.verbose = True\r\n        # detection params\r\n        self.S = 7  # cell size\r\n        self.B = 2  # boxes_per_cell\r\n        self.classes = [""aeroplane"", ""bicycle"", ""bird"", ""boat"", ""bottle"",\r\n                        ""bus"", ""car"", ""cat"", ""chair"", ""cow"", ""diningtable"",\r\n                        ""dog"", ""horse"", ""motorbike"", ""person"", ""pottedplant"",\r\n                        ""sheep"", ""sofa"", ""train"",""tvmonitor""]\r\n        self.C = len(self.classes) # number of classes\r\n        # offset for box center (top left point of each cell)\r\n        self.x_offset = np.transpose(np.reshape(np.array([np.arange(self.S)]*self.S*self.B),\r\n                                              [self.B, self.S, self.S]), [1, 2, 0])\r\n        self.y_offset = np.transpose(self.x_offset, [1, 0, 2])\r\n\r\n        self.threshold = 0.2  # confidence scores threshold\r\n        self.iou_threshold = 0.5\r\n\r\n        self.sess = tf.Session()\r\n        self._build_net()\r\n        self._load_weights(weights_file)\r\n\r\n    def _build_net(self):\r\n        """"""build the network""""""\r\n        if self.verbose:\r\n            print(""Start to build the network ..."")\r\n        self.images = tf.placeholder(tf.float32, [None, 448, 448, 3])\r\n        net = self._conv_layer(self.images, 1, 64, 7, 2)\r\n        net = self._maxpool_layer(net, 1, 2, 2)\r\n        net = self._conv_layer(net, 2, 192, 3, 1)\r\n        net = self._maxpool_layer(net, 2, 2, 2)\r\n        net = self._conv_layer(net, 3, 128, 1, 1)\r\n        net = self._conv_layer(net, 4, 256, 3, 1)\r\n        net = self._conv_layer(net, 5, 256, 1, 1)\r\n        net = self._conv_layer(net, 6, 512, 3, 1)\r\n        net = self._maxpool_layer(net, 6, 2, 2)\r\n        net = self._conv_layer(net, 7, 256, 1, 1)\r\n        net = self._conv_layer(net, 8, 512, 3, 1)\r\n        net = self._conv_layer(net, 9, 256, 1, 1)\r\n        net = self._conv_layer(net, 10, 512, 3, 1)\r\n        net = self._conv_layer(net, 11, 256, 1, 1)\r\n        net = self._conv_layer(net, 12, 512, 3, 1)\r\n        net = self._conv_layer(net, 13, 256, 1, 1)\r\n        net = self._conv_layer(net, 14, 512, 3, 1)\r\n        net = self._conv_layer(net, 15, 512, 1, 1)\r\n        net = self._conv_layer(net, 16, 1024, 3, 1)\r\n        net = self._maxpool_layer(net, 16, 2, 2)\r\n        net = self._conv_layer(net, 17, 512, 1, 1)\r\n        net = self._conv_layer(net, 18, 1024, 3, 1)\r\n        net = self._conv_layer(net, 19, 512, 1, 1)\r\n        net = self._conv_layer(net, 20, 1024, 3, 1)\r\n        net = self._conv_layer(net, 21, 1024, 3, 1)\r\n        net = self._conv_layer(net, 22, 1024, 3, 2)\r\n        net = self._conv_layer(net, 23, 1024, 3, 1)\r\n        net = self._conv_layer(net, 24, 1024, 3, 1)\r\n        net = self._flatten(net)\r\n        net = self._fc_layer(net, 25, 512, activation=leak_relu)\r\n        net = self._fc_layer(net, 26, 4096, activation=leak_relu)\r\n        net = self._fc_layer(net, 27, self.S*self.S*(self.C+5*self.B))\r\n        self.predicts = net\r\n\r\n    def _conv_layer(self, x, id, num_filters, filter_size, stride):\r\n        """"""Conv layer""""""\r\n        in_channels = x.get_shape().as_list()[-1]\r\n        weight = tf.Variable(tf.truncated_normal([filter_size, filter_size,\r\n                                                  in_channels, num_filters], stddev=0.1))\r\n        bias = tf.Variable(tf.zeros([num_filters,]))\r\n        # padding, note: not using padding=""SAME""\r\n        pad_size = filter_size // 2\r\n        pad_mat = np.array([[0, 0], [pad_size, pad_size], [pad_size, pad_size], [0, 0]])\r\n        x_pad = tf.pad(x, pad_mat)\r\n        conv = tf.nn.conv2d(x_pad, weight, strides=[1, stride, stride, 1], padding=""VALID"")\r\n        output = leak_relu(tf.nn.bias_add(conv, bias))\r\n        if self.verbose:\r\n            print(""    Layer %d: type=Conv, num_filter=%d, filter_size=%d, stride=%d, output_shape=%s"" \\\r\n                  % (id, num_filters, filter_size, stride, str(output.get_shape())))\r\n        return output\r\n\r\n    def _fc_layer(self, x, id, num_out, activation=None):\r\n        """"""fully connected layer""""""\r\n        num_in = x.get_shape().as_list()[-1]\r\n        weight = tf.Variable(tf.truncated_normal([num_in, num_out], stddev=0.1))\r\n        bias = tf.Variable(tf.zeros([num_out,]))\r\n        output = tf.nn.xw_plus_b(x, weight, bias)\r\n        if activation:\r\n            output = activation(output)\r\n        if self.verbose:\r\n            print(""    Layer %d: type=Fc, num_out=%d, output_shape=%s"" \\\r\n                  % (id, num_out, str(output.get_shape())))\r\n        return output\r\n\r\n    def _maxpool_layer(self, x, id, pool_size, stride):\r\n        output = tf.nn.max_pool(x, [1, pool_size, pool_size, 1],\r\n                                strides=[1, stride, stride, 1], padding=""SAME"")\r\n        if self.verbose:\r\n            print(""    Layer %d: type=MaxPool, pool_size=%d, stride=%d, output_shape=%s"" \\\r\n                  % (id, pool_size, stride, str(output.get_shape())))\r\n        return output\r\n\r\n    def _flatten(self, x):\r\n        """"""flatten the x""""""\r\n        tran_x = tf.transpose(x, [0, 3, 1, 2])  # channle first mode\r\n        nums = np.product(x.get_shape().as_list()[1:])\r\n        return tf.reshape(tran_x, [-1, nums])\r\n\r\n    def _load_weights(self, weights_file):\r\n        """"""Load weights from file""""""\r\n        if self.verbose:\r\n            print(""Start to load weights from file:%s"" % (weights_file))\r\n        saver = tf.train.Saver()\r\n        saver.restore(self.sess, weights_file)\r\n\r\n    def detect_from_file(self, image_file, imshow=True, deteted_boxes_file=""boxes.txt"",\r\n                     detected_image_file=""detected_image.jpg""):\r\n        """"""Do detection given a image file""""""\r\n        # read image\r\n        image = cv2.imread(image_file)\r\n        img_h, img_w, _ = image.shape\r\n        predicts = self._detect_from_image(image)\r\n        predict_boxes = self._interpret_predicts(predicts, img_h, img_w)\r\n        self.show_results(image, predict_boxes, imshow, deteted_boxes_file, detected_image_file)\r\n\r\n    def _detect_from_image(self, image):\r\n        """"""Do detection given a cv image""""""\r\n        img_resized = cv2.resize(image, (448, 448))\r\n        img_RGB = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\r\n        img_resized_np = np.asarray(img_RGB)\r\n        _images = np.zeros((1, 448, 448, 3), dtype=np.float32)\r\n        _images[0] = (img_resized_np / 255.0) * 2.0 - 1.0\r\n        predicts = self.sess.run(self.predicts, feed_dict={self.images: _images})[0]\r\n        return predicts\r\n\r\n    def _interpret_predicts(self, predicts, img_h, img_w):\r\n        """"""Interpret the predicts and get the detetction boxes""""""\r\n        idx1 = self.S*self.S*self.C\r\n        idx2 = idx1 + self.S*self.S*self.B\r\n        # class prediction\r\n        class_probs = np.reshape(predicts[:idx1], [self.S, self.S, self.C])\r\n        # confidence\r\n        confs = np.reshape(predicts[idx1:idx2], [self.S, self.S, self.B])\r\n        # boxes -> (x, y, w, h)\r\n        boxes = np.reshape(predicts[idx2:], [self.S, self.S, self.B, 4])\r\n\r\n        # convert the x, y to the coordinates relative to the top left point of the image\r\n        boxes[:, :, :, 0] += self.x_offset\r\n        boxes[:, :, :, 1] += self.y_offset\r\n        boxes[:, :, :, :2] /= self.S\r\n\r\n        # the predictions of w, h are the square root\r\n        boxes[:, :, :, 2:] = np.square(boxes[:, :, :, 2:])\r\n\r\n        # multiply the width and height of image\r\n        boxes[:, :, :, 0] *= img_w\r\n        boxes[:, :, :, 1] *= img_h\r\n        boxes[:, :, :, 2] *= img_w\r\n        boxes[:, :, :, 3] *= img_h\r\n\r\n        # class-specific confidence scores [S, S, B, C]\r\n        scores = np.expand_dims(confs, -1) * np.expand_dims(class_probs, 2)\r\n\r\n        scores = np.reshape(scores, [-1, self.C]) # [S*S*B, C]\r\n        boxes = np.reshape(boxes, [-1, 4])        # [S*S*B, 4]\r\n\r\n        # filter the boxes when score < threhold\r\n        scores[scores < self.threshold] = 0.0\r\n\r\n        # non max suppression\r\n        self._non_max_suppression(scores, boxes)\r\n\r\n        # report the boxes\r\n        predict_boxes = [] # (class, x, y, w, h, scores)\r\n        max_idxs = np.argmax(scores, axis=1)\r\n        for i in range(len(scores)):\r\n            max_idx = max_idxs[i]\r\n            if scores[i, max_idx] > 0.0:\r\n                predict_boxes.append((self.classes[max_idx], boxes[i, 0], boxes[i, 1],\r\n                                      boxes[i, 2], boxes[i, 3], scores[i, max_idx]))\r\n        return predict_boxes\r\n\r\n    def _non_max_suppression(self, scores, boxes):\r\n        """"""Non max suppression""""""\r\n        # for each class\r\n        for c in range(self.C):\r\n            sorted_idxs = np.argsort(scores[:, c])\r\n            last = len(sorted_idxs) - 1\r\n            while last > 0:\r\n                if scores[sorted_idxs[last], c] < 1e-6:\r\n                    break\r\n                for i in range(last):\r\n                    if scores[sorted_idxs[i], c] < 1e-6:\r\n                        continue\r\n                    if self._iou(boxes[sorted_idxs[i]], boxes[sorted_idxs[last]]) > self.iou_threshold:\r\n                        scores[sorted_idxs[i], c] = 0.0\r\n                last -= 1\r\n\r\n    def _iou(self, box1, box2):\r\n        """"""Compute the iou of two boxes""""""\r\n\r\n        inter_w = np.minimum(box1[0]+0.5*box1[2], box2[0]+0.5*box2[2]) - \\\r\n                  np.maximum(box1[0]-0.5*box2[2], box2[0]-0.5*box2[2])\r\n        inter_h = np.minimum(box1[1]+0.5*box1[3], box2[1]+0.5*box2[3]) - \\\r\n                  np.maximum(box1[1]-0.5*box2[3], box2[1]-0.5*box2[3])\r\n        if inter_h < 0 or inter_w < 0:\r\n            inter = 0\r\n        else:\r\n            inter = inter_w * inter_h\r\n        union = box1[2]*box1[3] + box2[2]*box2[3] - inter\r\n        return inter / union\r\n\r\n    def show_results(self, image, results, imshow=True, deteted_boxes_file=None,\r\n                     detected_image_file=None):\r\n        """"""Show the detection boxes""""""\r\n        img_cp = image.copy()\r\n        if deteted_boxes_file:\r\n            f = open(deteted_boxes_file, ""w"")\r\n        #  draw boxes\r\n        for i in range(len(results)):\r\n            x = int(results[i][1])\r\n            y = int(results[i][2])\r\n            w = int(results[i][3]) // 2\r\n            h = int(results[i][4]) // 2\r\n            if self.verbose:\r\n                print(""   class: %s, [x, y, w, h]=[%d, %d, %d, %d], confidence=%f"" % (results[i][0],\r\n                            x, y, w, h, results[i][-1]))\r\n\r\n                cv2.rectangle(img_cp, (x - w, y - h), (x + w, y + h), (0, 255, 0), 2)\r\n                cv2.rectangle(img_cp, (x - w, y - h - 20), (x + w, y - h), (125, 125, 125), -1)\r\n                cv2.putText(img_cp, results[i][0] + \' : %.2f\' % results[i][5], (x - w + 5, y - h - 7),\r\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\r\n            if deteted_boxes_file:\r\n                f.write(results[i][0] + \',\' + str(x) + \',\' + str(y) + \',\' +\r\n                        str(w) + \',\' + str(h)+\',\' + str(results[i][5]) + \'\\n\')\r\n        if imshow:\r\n            cv2.imshow(\'YOLO_small detection\', img_cp)\r\n            cv2.waitKey(1)\r\n        if detected_image_file:\r\n            cv2.imwrite(detected_image_file, img_cp)\r\n        if deteted_boxes_file:\r\n            f.close()\r\n\r\nif __name__ == ""__main__"":\r\n    yolo_net = Yolo(""./weights/YOLO_small.ckpt"")\r\n    yolo_net.detect_from_file(""./test/car.jpg"")\r\n\r\n\r\n\r\n'"
ObjectDetections/yolo/yolo_tf.py,37,"b'""""""\r\nYolo V1 by tensorflow\r\n""""""\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport cv2\r\n\r\n\r\ndef leak_relu(x, alpha=0.1):\r\n    return tf.maximum(alpha * x, x)\r\n\r\nclass Yolo(object):\r\n    def __init__(self, weights_file, verbose=True):\r\n        self.verbose = verbose\r\n        # detection params\r\n        self.S = 7  # cell size\r\n        self.B = 2  # boxes_per_cell\r\n        self.classes = [""aeroplane"", ""bicycle"", ""bird"", ""boat"", ""bottle"",\r\n                        ""bus"", ""car"", ""cat"", ""chair"", ""cow"", ""diningtable"",\r\n                        ""dog"", ""horse"", ""motorbike"", ""person"", ""pottedplant"",\r\n                        ""sheep"", ""sofa"", ""train"",""tvmonitor""]\r\n        self.C = len(self.classes) # number of classes\r\n        # offset for box center (top left point of each cell)\r\n        self.x_offset = np.transpose(np.reshape(np.array([np.arange(self.S)]*self.S*self.B),\r\n                                              [self.B, self.S, self.S]), [1, 2, 0])\r\n        self.y_offset = np.transpose(self.x_offset, [1, 0, 2])\r\n\r\n        self.threshold = 0.2  # confidence scores threhold\r\n        self.iou_threshold = 0.4\r\n        #  the maximum number of boxes to be selected by non max suppression\r\n        self.max_output_size = 10\r\n\r\n        self.sess = tf.Session()\r\n        self._build_net()\r\n        self._build_detector()\r\n        self._load_weights(weights_file)\r\n\r\n    def _build_net(self):\r\n        """"""build the network""""""\r\n        if self.verbose:\r\n            print(""Start to build the network ..."")\r\n        self.images = tf.placeholder(tf.float32, [None, 448, 448, 3])\r\n        net = self._conv_layer(self.images, 1, 64, 7, 2)\r\n        net = self._maxpool_layer(net, 1, 2, 2)\r\n        net = self._conv_layer(net, 2, 192, 3, 1)\r\n        net = self._maxpool_layer(net, 2, 2, 2)\r\n        net = self._conv_layer(net, 3, 128, 1, 1)\r\n        net = self._conv_layer(net, 4, 256, 3, 1)\r\n        net = self._conv_layer(net, 5, 256, 1, 1)\r\n        net = self._conv_layer(net, 6, 512, 3, 1)\r\n        net = self._maxpool_layer(net, 6, 2, 2)\r\n        net = self._conv_layer(net, 7, 256, 1, 1)\r\n        net = self._conv_layer(net, 8, 512, 3, 1)\r\n        net = self._conv_layer(net, 9, 256, 1, 1)\r\n        net = self._conv_layer(net, 10, 512, 3, 1)\r\n        net = self._conv_layer(net, 11, 256, 1, 1)\r\n        net = self._conv_layer(net, 12, 512, 3, 1)\r\n        net = self._conv_layer(net, 13, 256, 1, 1)\r\n        net = self._conv_layer(net, 14, 512, 3, 1)\r\n        net = self._conv_layer(net, 15, 512, 1, 1)\r\n        net = self._conv_layer(net, 16, 1024, 3, 1)\r\n        net = self._maxpool_layer(net, 16, 2, 2)\r\n        net = self._conv_layer(net, 17, 512, 1, 1)\r\n        net = self._conv_layer(net, 18, 1024, 3, 1)\r\n        net = self._conv_layer(net, 19, 512, 1, 1)\r\n        net = self._conv_layer(net, 20, 1024, 3, 1)\r\n        net = self._conv_layer(net, 21, 1024, 3, 1)\r\n        net = self._conv_layer(net, 22, 1024, 3, 2)\r\n        net = self._conv_layer(net, 23, 1024, 3, 1)\r\n        net = self._conv_layer(net, 24, 1024, 3, 1)\r\n        net = self._flatten(net)\r\n        net = self._fc_layer(net, 25, 512, activation=leak_relu)\r\n        net = self._fc_layer(net, 26, 4096, activation=leak_relu)\r\n        net = self._fc_layer(net, 27, self.S*self.S*(self.C+5*self.B))\r\n        self.predicts = net\r\n\r\n    def _build_detector(self):\r\n        """"""Interpret the net output and get the predicted boxes""""""\r\n        # the width and height of orignal image\r\n        self.width = tf.placeholder(tf.float32, name=""img_w"")\r\n        self.height = tf.placeholder(tf.float32, name=""img_h"")\r\n        # get class prob, confidence, boxes from net output\r\n        idx1 = self.S * self.S * self.C\r\n        idx2 = idx1 + self.S * self.S * self.B\r\n        # class prediction\r\n        class_probs = tf.reshape(self.predicts[0, :idx1], [self.S, self.S, self.C])\r\n        # confidence\r\n        confs = tf.reshape(self.predicts[0, idx1:idx2], [self.S, self.S, self.B])\r\n        # boxes -> (x, y, w, h)\r\n        boxes = tf.reshape(self.predicts[0, idx2:], [self.S, self.S, self.B, 4])\r\n\r\n        # convert the x, y to the coordinates relative to the top left point of the image\r\n        # the predictions of w, h are the square root\r\n        # multiply the width and height of image\r\n        boxes = tf.stack([(boxes[:, :, :, 0] + tf.constant(self.x_offset, dtype=tf.float32)) / self.S * self.width,\r\n                          (boxes[:, :, :, 1] + tf.constant(self.y_offset, dtype=tf.float32)) / self.S * self.height,\r\n                          tf.square(boxes[:, :, :, 2]) * self.width,\r\n                          tf.square(boxes[:, :, :, 3]) * self.height], axis=3)\r\n\r\n        # class-specific confidence scores [S, S, B, C]\r\n        scores = tf.expand_dims(confs, -1) * tf.expand_dims(class_probs, 2)\r\n\r\n        scores = tf.reshape(scores, [-1, self.C])  # [S*S*B, C]\r\n        boxes = tf.reshape(boxes, [-1, 4])  # [S*S*B, 4]\r\n\r\n        # find each box class, only select the max score\r\n        box_classes = tf.argmax(scores, axis=1)\r\n        box_class_scores = tf.reduce_max(scores, axis=1)\r\n\r\n        # filter the boxes by the score threshold\r\n        filter_mask = box_class_scores >= self.threshold\r\n        scores = tf.boolean_mask(box_class_scores, filter_mask)\r\n        boxes = tf.boolean_mask(boxes, filter_mask)\r\n        box_classes = tf.boolean_mask(box_classes, filter_mask)\r\n\r\n        # non max suppression (do not distinguish different classes)\r\n        # ref: https://tensorflow.google.cn/api_docs/python/tf/image/non_max_suppression\r\n        # box (x, y, w, h) -> box (x1, y1, x2, y2)\r\n        _boxes = tf.stack([boxes[:, 0] - 0.5 * boxes[:, 2], boxes[:, 1] - 0.5 * boxes[:, 3],\r\n                           boxes[:, 0] + 0.5 * boxes[:, 2], boxes[:, 1] + 0.5 * boxes[:, 3]], axis=1)\r\n        nms_indices = tf.image.non_max_suppression(_boxes, scores,\r\n                                                   self.max_output_size, self.iou_threshold)\r\n        self.scores = tf.gather(scores, nms_indices)\r\n        self.boxes = tf.gather(boxes, nms_indices)\r\n        self.box_classes = tf.gather(box_classes, nms_indices)\r\n\r\n    def _conv_layer(self, x, id, num_filters, filter_size, stride):\r\n        """"""Conv layer""""""\r\n        in_channels = x.get_shape().as_list()[-1]\r\n        weight = tf.Variable(tf.truncated_normal([filter_size, filter_size,\r\n                                                  in_channels, num_filters], stddev=0.1))\r\n        bias = tf.Variable(tf.zeros([num_filters,]))\r\n        # padding, note: not using padding=""SAME""\r\n        pad_size = filter_size // 2\r\n        pad_mat = np.array([[0, 0], [pad_size, pad_size], [pad_size, pad_size], [0, 0]])\r\n        x_pad = tf.pad(x, pad_mat)\r\n        conv = tf.nn.conv2d(x_pad, weight, strides=[1, stride, stride, 1], padding=""VALID"")\r\n        output = leak_relu(tf.nn.bias_add(conv, bias))\r\n        if self.verbose:\r\n            print(""    Layer %d: type=Conv, num_filter=%d, filter_size=%d, stride=%d, output_shape=%s"" \\\r\n                  % (id, num_filters, filter_size, stride, str(output.get_shape())))\r\n        return output\r\n\r\n    def _fc_layer(self, x, id, num_out, activation=None):\r\n        """"""fully connected layer""""""\r\n        num_in = x.get_shape().as_list()[-1]\r\n        weight = tf.Variable(tf.truncated_normal([num_in, num_out], stddev=0.1))\r\n        bias = tf.Variable(tf.zeros([num_out,]))\r\n        output = tf.nn.xw_plus_b(x, weight, bias)\r\n        if activation:\r\n            output = activation(output)\r\n        if self.verbose:\r\n            print(""    Layer %d: type=Fc, num_out=%d, output_shape=%s"" \\\r\n                  % (id, num_out, str(output.get_shape())))\r\n        return output\r\n\r\n    def _maxpool_layer(self, x, id, pool_size, stride):\r\n        output = tf.nn.max_pool(x, [1, pool_size, pool_size, 1],\r\n                                strides=[1, stride, stride, 1], padding=""SAME"")\r\n        if self.verbose:\r\n            print(""    Layer %d: type=MaxPool, pool_size=%d, stride=%d, output_shape=%s"" \\\r\n                  % (id, pool_size, stride, str(output.get_shape())))\r\n        return output\r\n\r\n    def _flatten(self, x):\r\n        """"""flatten the x""""""\r\n        tran_x = tf.transpose(x, [0, 3, 1, 2])  # channle first mode\r\n        nums = np.product(x.get_shape().as_list()[1:])\r\n        return tf.reshape(tran_x, [-1, nums])\r\n\r\n    def _load_weights(self, weights_file):\r\n        """"""Load weights from file""""""\r\n        if self.verbose:\r\n            print(""Start to load weights from file:%s"" % (weights_file))\r\n        saver = tf.train.Saver()\r\n        saver.restore(self.sess, weights_file)\r\n\r\n    def detect_from_file(self, image_file, imshow=True, deteted_boxes_file=""boxes.txt"",\r\n                     detected_image_file=""detected_image.jpg""):\r\n        """"""Do detection given a image file""""""\r\n        # read image\r\n        image = cv2.imread(image_file)\r\n        img_h, img_w, _ = image.shape\r\n        scores, boxes, box_classes = self._detect_from_image(image)\r\n        predict_boxes = []\r\n        for i in range(len(scores)):\r\n            predict_boxes.append((self.classes[box_classes[i]], boxes[i, 0],\r\n                                boxes[i, 1], boxes[i, 2], boxes[i, 3], scores[i]))\r\n        self.show_results(image, predict_boxes, imshow, deteted_boxes_file, detected_image_file)\r\n\r\n    def _detect_from_image(self, image):\r\n        """"""Do detection given a cv image""""""\r\n        img_h, img_w, _ = image.shape\r\n        img_resized = cv2.resize(image, (448, 448))\r\n        img_RGB = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\r\n        img_resized_np = np.asarray(img_RGB)\r\n        _images = np.zeros((1, 448, 448, 3), dtype=np.float32)\r\n        _images[0] = (img_resized_np / 255.0) * 2.0 - 1.0\r\n        scores, boxes, box_classes = self.sess.run([self.scores, self.boxes, self.box_classes],\r\n                    feed_dict={self.images: _images, self.width: img_w, self.height: img_h})\r\n        return scores, boxes, box_classes\r\n\r\n    def show_results(self, image, results, imshow=True, deteted_boxes_file=None,\r\n                     detected_image_file=None):\r\n        """"""Show the detection boxes""""""\r\n        img_cp = image.copy()\r\n        if deteted_boxes_file:\r\n            f = open(deteted_boxes_file, ""w"")\r\n        #  draw boxes\r\n        for i in range(len(results)):\r\n            x = int(results[i][1])\r\n            y = int(results[i][2])\r\n            w = int(results[i][3]) // 2\r\n            h = int(results[i][4]) // 2\r\n            if self.verbose:\r\n                print(""   class: %s, [x, y, w, h]=[%d, %d, %d, %d], confidence=%f"" % (results[i][0],\r\n                            x, y, w, h, results[i][-1]))\r\n\r\n                cv2.rectangle(img_cp, (x - w, y - h), (x + w, y + h), (0, 255, 0), 2)\r\n                cv2.rectangle(img_cp, (x - w, y - h - 20), (x + w, y - h), (125, 125, 125), -1)\r\n                cv2.putText(img_cp, results[i][0] + \' : %.2f\' % results[i][5], (x - w + 5, y - h - 7),\r\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\r\n            if deteted_boxes_file:\r\n                f.write(results[i][0] + \',\' + str(x) + \',\' + str(y) + \',\' +\r\n                        str(w) + \',\' + str(h)+\',\' + str(results[i][5]) + \'\\n\')\r\n        if imshow:\r\n            cv2.imshow(\'YOLO_small detection\', img_cp)\r\n            cv2.waitKey(1)\r\n        if detected_image_file:\r\n            cv2.imwrite(detected_image_file, img_cp)\r\n        if deteted_boxes_file:\r\n            f.close()\r\n\r\nif __name__ == ""__main__"":\r\n    yolo_net = Yolo(""./weights/YOLO_small.ckpt"")\r\n    yolo_net.detect_from_file(""./test/car.jpg"")\r\n'"
ObjectDetections/yolo2/config.py,0,"b'""""""\r\nYolov2 anchors and coco classes\r\n""""""\r\n\r\n""""""\r\nanchors = [[0.738768, 0.874946],\r\n           [2.42204, 2.65704],\r\n           [4.30971, 7.04493],\r\n           [10.246, 4.59428],\r\n           [12.6868, 11.8741]]\r\n""""""\r\nanchors = [[0.57273, 0.677385],\r\n           [1.87446, 2.06253],\r\n           [3.33843, 5.47434],\r\n           [7.88282, 3.52778],\r\n           [9.77052, 9.16828]]\r\n\r\ndef read_coco_labels():\r\n    f = open(""./data/coco_classes.txt"")\r\n    class_names = []\r\n    for l in f.readlines():\r\n        class_names.append(l[:-1])\r\n    return class_names\r\n\r\nclass_names = read_coco_labels()'"
ObjectDetections/yolo2/demo.py,3,"b'""""""\r\nDemo for yolov2\r\n""""""\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport cv2\r\nfrom PIL import Image\r\n\r\nfrom model import darknet\r\nfrom detect_ops import decode\r\nfrom utils import preprocess_image, postprocess, draw_detection\r\nfrom config import anchors, class_names\r\n\r\n\r\ninput_size = (416, 416)\r\nimage_file = ""./images/car.jpg""\r\nimage = cv2.imread(image_file)\r\nimage_shape = image.shape[:2]\r\nimage_cp = preprocess_image(image, input_size)\r\n""""""\r\nimage = Image.open(image_file)\r\nimage_cp = image.resize(input_size, Image.BICUBIC)\r\nimage_cp = np.array(image_cp, dtype=np.float32)/255.0\r\nimage_cp = np.expand_dims(image_cp, 0)\r\n#print(image_cp)\r\n""""""\r\n\r\n\r\nimages = tf.placeholder(tf.float32, [1, input_size[0], input_size[1], 3])\r\ndetection_feat = darknet(images)\r\nfeat_sizes = input_size[0] // 32, input_size[1] // 32\r\ndetection_results = decode(detection_feat, feat_sizes, len(class_names), anchors)\r\n\r\ncheckpoint_path = ""./checkpoint_dir/yolo2_coco.ckpt""\r\nsaver = tf.train.Saver()\r\nwith tf.Session() as sess:\r\n    saver.restore(sess, checkpoint_path)\r\n    bboxes, obj_probs, class_probs = sess.run(detection_results, feed_dict={images: image_cp})\r\n\r\nbboxes, scores, class_inds = postprocess(bboxes, obj_probs, class_probs,\r\n                                         image_shape=image_shape)\r\nimg_detection = draw_detection(image, bboxes, scores, class_inds, class_names)\r\ncv2.imwrite(""detection.jpg"", img_detection)\r\ncv2.imshow(""detection results"", img_detection)\r\n\r\ncv2.waitKey(0)\r\n\r\n\r\n\r\n'"
ObjectDetections/yolo2/detect_ops.py,12,"b'""""""\r\nDetection ops for Yolov2\r\n""""""\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\ndef decode(detection_feat, feat_sizes=(13, 13), num_classes=80,\r\n           anchors=None):\r\n    """"""decode from the detection feature""""""\r\n    H, W = feat_sizes\r\n    num_anchors = len(anchors)\r\n    detetion_results = tf.reshape(detection_feat, [-1, H * W, num_anchors,\r\n                                        num_classes + 5])\r\n\r\n    bbox_xy = tf.nn.sigmoid(detetion_results[:, :, :, 0:2])\r\n    bbox_wh = tf.exp(detetion_results[:, :, :, 2:4])\r\n    obj_probs = tf.nn.sigmoid(detetion_results[:, :, :, 4])\r\n    class_probs = tf.nn.softmax(detetion_results[:, :, :, 5:])\r\n\r\n    anchors = tf.constant(anchors, dtype=tf.float32)\r\n\r\n    height_ind = tf.range(H, dtype=tf.float32)\r\n    width_ind = tf.range(W, dtype=tf.float32)\r\n    x_offset, y_offset = tf.meshgrid(height_ind, width_ind)\r\n    x_offset = tf.reshape(x_offset, [1, -1, 1])\r\n    y_offset = tf.reshape(y_offset, [1, -1, 1])\r\n\r\n    # decode\r\n    bbox_x = (bbox_xy[:, :, :, 0] + x_offset) / W\r\n    bbox_y = (bbox_xy[:, :, :, 1] + y_offset) / H\r\n    bbox_w = bbox_wh[:, :, :, 0] * anchors[:, 0] / W * 0.5\r\n    bbox_h = bbox_wh[:, :, :, 1] * anchors[:, 1] / H * 0.5\r\n\r\n    bboxes = tf.stack([bbox_x - bbox_w, bbox_y - bbox_h,\r\n                       bbox_x + bbox_w, bbox_y + bbox_h], axis=3)\r\n\r\n    return bboxes, obj_probs, class_probs\r\n'"
ObjectDetections/yolo2/loss.py,26,"b'""""""\r\nLoss function for YOLOv2\r\n""""""\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef compute_loss(predictions, targets, anchors, scales, num_classes=20, feat_sizes=(13, 13)):\r\n    """"""\r\n    Compute the loss of Yolov2 for training\r\n    """"""\r\n    H, W = feat_sizes\r\n    C = num_classes\r\n    B = len(anchors)\r\n    anchors = tf.constant(anchors, dtype=tf.float32)\r\n    anchors = tf.reshape(anchors, [1, 1, B, 2])\r\n\r\n    sprob, sconf, snoob, scoor = scales  # the scales for different parts\r\n\r\n    _coords = targets[""coords""]  # ground truth [-1, H*W, B, 4]\r\n    _probs = targets[""probs""]    # class probability [-1, H*W, B, C] one hot\r\n    _confs = targets[""confs""]    # 1 for object, 0 for background, [-1, H*W, B]\r\n\r\n    # decode the net output\r\n    predictions = tf.reshape(predictions, [-1, H, W, B, (5 + C)])\r\n    coords = predictions[:, :, :, :, 0:4]   # t_x, t_y, t_w, t_h\r\n    coords = tf.reshape(coords, [-1, H*W, B, 4])\r\n    coords_xy = tf.nn.sigmoid(coords[:, :, :, 0:2])  # (0, 1) relative cell top left\r\n    coords_wh = tf.sqrt(tf.exp(coords[:, :, :, 2:4]) * anchors /\r\n                        np.reshape([W, H], [1, 1, 1, 2])) # sqrt of w, h (0, 1)\r\n    coords = tf.concat([coords_xy, coords_wh], axis=3)  # [batch_size, H*W, B, 4]\r\n\r\n    confs = tf.nn.sigmoid(predictions[:, :, :, :, 4])  # object confidence\r\n    confs = tf.reshape(confs, [-1, H*W, B, 1])\r\n\r\n    probs = tf.nn.softmax(predictions[:, :, :, :, 5:])  # class probability\r\n    probs = tf.reshape(probs, [-1, H*W, B, C])\r\n\r\n    preds = tf.concat([coords, confs, probs], axis=3)  # [-1, H*W, B, (4+1+C)]\r\n\r\n    # match ground truths with anchors (predictions in fact)\r\n    # assign ground truths to the predictions with the best IOU (select 1 among 5 anchors)\r\n    wh = tf.pow(coords[:, :, :, 2:4], 2) * np.reshape([W, H], [1, 1, 1, 2])\r\n    areas = wh[:, :, :, 0] * wh[:, :, :, 1]\r\n    centers = coords[:, :, :, 0:2]\r\n    up_left, down_right = centers - (wh * 0.5), centers + (wh * 0.5)\r\n\r\n    # the ground truth\r\n    _wh = tf.pow(_coords[:, :, :, 2:4], 2) * np.reshape([W, H], [1, 1, 1, 2])\r\n    _areas = _wh[:, :, :, 0] * _wh[:, :, :, 1]\r\n    _centers = _coords[:, :, :, 0:2]\r\n    _up_left, _down_right = _centers - (_wh * 0.5), _centers + (_wh * 0.5)\r\n\r\n    # compute IOU\r\n    inter_upleft = tf.maximum(up_left, _up_left)\r\n    inter_downright = tf.minimum(down_right, _down_right)\r\n    inter_wh = tf.maximum(inter_downright - inter_upleft, 0.0)\r\n    intersects = inter_wh[:, :, :, 0] * inter_wh[:, :, :, 1]\r\n    ious = tf.truediv(intersects, areas + _areas - intersects)\r\n\r\n    best_iou_mask = tf.equal(ious, tf.reduce_max(ious, axis=2, keep_dims=True))\r\n    best_iou_mask = tf.cast(best_iou_mask, tf.float32)\r\n    mask = best_iou_mask * _confs  # [-1, H*W, B]\r\n    mask = tf.expand_dims(mask, -1)  # [-1, H*W, B, 1]\r\n\r\n    # compute weight terms\r\n    confs_w = snoob * (1 - mask) + sconf * mask\r\n    coords_w = scoor * mask\r\n    probs_w = sprob * mask\r\n    weights = tf.concat([coords_w, confs_w, probs_w], axis=3)\r\n\r\n    truths = tf.concat([_coords, tf.expand_dims(_confs, -1), _probs], 3)\r\n\r\n    loss = tf.pow(preds - truths, 2) * weights\r\n    loss = tf.reduce_sum(loss, axis=[1, 2, 3])\r\n    loss = 0.5 * tf.reduce_mean(loss)\r\n    return loss\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n'"
ObjectDetections/yolo2/model.py,10,"b'""""""\r\nYOLOv2 implemented by Tensorflow, only for predicting\r\n""""""\r\nimport os\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\n\r\n######## basic layers #######\r\n\r\ndef leaky_relu(x):\r\n    return tf.nn.leaky_relu(x, alpha=0.1, name=""leaky_relu"")\r\n\r\n# Conv2d\r\ndef conv2d(x, filters, size, pad=0, stride=1, batch_normalize=1,\r\n           activation=leaky_relu, use_bias=False, name=""conv2d""):\r\n    if pad > 0:\r\n        x = tf.pad(x, [[0, 0], [pad, pad], [pad, pad], [0, 0]])\r\n    out = tf.layers.conv2d(x, filters, size, strides=stride, padding=""VALID"",\r\n                           activation=None, use_bias=use_bias, name=name)\r\n    if batch_normalize == 1:\r\n        out = tf.layers.batch_normalization(out, axis=-1, momentum=0.9,\r\n                                            training=False, name=name+""_bn"")\r\n    if activation:\r\n        out = activation(out)\r\n    return out\r\n\r\n# maxpool2d\r\ndef maxpool(x, size=2, stride=2, name=""maxpool""):\r\n    return tf.layers.max_pooling2d(x, size, stride)\r\n\r\n# reorg layer\r\ndef reorg(x, stride):\r\n    return tf.extract_image_patches(x, [1, stride, stride, 1],\r\n                        [1, stride, stride, 1], [1,1,1,1], padding=""VALID"")\r\n\r\n\r\ndef darknet(images, n_last_channels=425):\r\n    """"""Darknet19 for YOLOv2""""""\r\n    net = conv2d(images, 32, 3, 1, name=""conv1"")\r\n    net = maxpool(net, name=""pool1"")\r\n    net = conv2d(net, 64, 3, 1, name=""conv2"")\r\n    net = maxpool(net, name=""pool2"")\r\n    net = conv2d(net, 128, 3, 1, name=""conv3_1"")\r\n    net = conv2d(net, 64, 1, name=""conv3_2"")\r\n    net = conv2d(net, 128, 3, 1, name=""conv3_3"")\r\n    net = maxpool(net, name=""pool3"")\r\n    net = conv2d(net, 256, 3, 1, name=""conv4_1"")\r\n    net = conv2d(net, 128, 1, name=""conv4_2"")\r\n    net = conv2d(net, 256, 3, 1, name=""conv4_3"")\r\n    net = maxpool(net, name=""pool4"")\r\n    net = conv2d(net, 512, 3, 1, name=""conv5_1"")\r\n    net = conv2d(net, 256, 1, name=""conv5_2"")\r\n    net = conv2d(net, 512, 3, 1, name=""conv5_3"")\r\n    net = conv2d(net, 256, 1, name=""conv5_4"")\r\n    net = conv2d(net, 512, 3, 1, name=""conv5_5"")\r\n    shortcut = net\r\n    net = maxpool(net, name=""pool5"")\r\n    net = conv2d(net, 1024, 3, 1, name=""conv6_1"")\r\n    net = conv2d(net, 512, 1, name=""conv6_2"")\r\n    net = conv2d(net, 1024, 3, 1, name=""conv6_3"")\r\n    net = conv2d(net, 512, 1, name=""conv6_4"")\r\n    net = conv2d(net, 1024, 3, 1, name=""conv6_5"")\r\n    # ---------\r\n    net = conv2d(net, 1024, 3, 1, name=""conv7_1"")\r\n    net = conv2d(net, 1024, 3, 1, name=""conv7_2"")\r\n    # shortcut\r\n    shortcut = conv2d(shortcut, 64, 1, name=""conv_shortcut"")\r\n    shortcut = reorg(shortcut, 2)\r\n    net = tf.concat([shortcut, net], axis=-1)\r\n    net = conv2d(net, 1024, 3, 1, name=""conv8"")\r\n    # detection layer\r\n    net = conv2d(net, n_last_channels, 1, batch_normalize=0,\r\n                 activation=None, use_bias=True, name=""conv_dec"")\r\n    return net\r\n\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    x = tf.random_normal([1, 416, 416, 3])\r\n    model = darknet(x)\r\n\r\n    saver = tf.train.Saver()\r\n    with tf.Session() as sess:\r\n        saver.restore(sess, ""./checkpoint_dir/yolo2_coco.ckpt"")\r\n        print(sess.run(model).shape)\r\n\r\n'"
ObjectDetections/yolo2/utils.py,0,"b'""""""\r\nHelp functions for YOLOv2\r\n""""""\r\nimport random\r\nimport colorsys\r\n\r\nimport cv2\r\nimport numpy as np\r\n\r\n\r\n\r\n############## preprocess image ##################\r\n\r\n\r\ndef preprocess_image(image, image_size=(416, 416)):\r\n    """"""Preprocess a image to inference""""""\r\n    image_cp = np.copy(image).astype(np.float32)\r\n    # resize the image\r\n    image_rgb = cv2.cvtColor(image_cp, cv2.COLOR_BGR2RGB)\r\n    image_resized = cv2.resize(image_rgb, image_size)\r\n    # normalize\r\n    image_normalized = image_resized.astype(np.float32) / 255.0\r\n    # expand the batch_size dim\r\n    image_expanded = np.expand_dims(image_normalized, axis=0)\r\n    return image_expanded\r\n\r\ndef postprocess(bboxes, obj_probs, class_probs, image_shape=(416, 416),\r\n                threshold=0.5):\r\n    """"""post process the detection results""""""\r\n    bboxes = np.reshape(bboxes, [-1, 4])\r\n    bboxes[:, 0::2] *= float(image_shape[1])\r\n    bboxes[:, 1::2] *= float(image_shape[0])\r\n    bboxes = bboxes.astype(np.int32)\r\n\r\n    # clip the bboxs\r\n    bbox_ref = [0, 0, image_shape[1] - 1, image_shape[0] - 1]\r\n    bboxes = bboxes_clip(bbox_ref, bboxes)\r\n\r\n    obj_probs = np.reshape(obj_probs, [-1])\r\n    class_probs = np.reshape(class_probs, [len(obj_probs), -1])\r\n    class_inds = np.argmax(class_probs, axis=1)\r\n    class_probs = class_probs[np.arange(len(obj_probs)), class_inds]\r\n    scores = obj_probs * class_probs\r\n\r\n    # filter bboxes with scores > threshold\r\n    keep_inds = scores > threshold\r\n    bboxes = bboxes[keep_inds]\r\n    scores = scores[keep_inds]\r\n    class_inds = class_inds[keep_inds]\r\n\r\n    # sort top K\r\n    class_inds, scores, bboxes = bboxes_sort(class_inds, scores, bboxes)\r\n    # nms\r\n    class_inds, scores, bboxes = bboxes_nms(class_inds, scores, bboxes)\r\n\r\n    return bboxes, scores, class_inds\r\n\r\ndef draw_detection(im, bboxes, scores, cls_inds, labels, thr=0.3):\r\n    # for display\r\n    ############################\r\n    # Generate colors for drawing bounding boxes.\r\n    hsv_tuples = [(x / float(len(labels)), 1., 1.)\r\n                  for x in range(len(labels))]\r\n    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\r\n    colors = list(\r\n        map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)),\r\n            colors))\r\n    random.seed(10101)  # Fixed seed for consistent colors across runs.\r\n    random.shuffle(colors)  # Shuffle colors to decorrelate adjacent classes.\r\n    random.seed(None)  # Reset seed to default.\r\n    # draw image\r\n    imgcv = np.copy(im)\r\n    h, w, _ = imgcv.shape\r\n    for i, box in enumerate(bboxes):\r\n        if scores[i] < thr:\r\n            continue\r\n        cls_indx = cls_inds[i]\r\n\r\n        thick = int((h + w) / 300)\r\n        cv2.rectangle(imgcv,\r\n                      (box[0], box[1]), (box[2], box[3]),\r\n                      colors[cls_indx], thick)\r\n        mess = \'%s: %.3f\' % (labels[cls_indx], scores[i])\r\n        if box[1] < 20:\r\n            text_loc = (box[0] + 2, box[1] + 15)\r\n        else:\r\n            text_loc = (box[0], box[1] - 10)\r\n        cv2.putText(imgcv, mess, text_loc,\r\n                    cv2.FONT_HERSHEY_SIMPLEX, 1e-3 * h, colors[cls_indx], thick // 3)\r\n\r\n    return imgcv\r\n\r\n\r\n############## process bboxes ##################\r\ndef bboxes_clip(bbox_ref, bboxes):\r\n    """"""Clip bounding boxes with respect to reference bbox.\r\n    """"""\r\n    bboxes = np.copy(bboxes)\r\n    bboxes = np.transpose(bboxes)\r\n    bbox_ref = np.transpose(bbox_ref)\r\n    bboxes[0] = np.maximum(bboxes[0], bbox_ref[0])\r\n    bboxes[1] = np.maximum(bboxes[1], bbox_ref[1])\r\n    bboxes[2] = np.minimum(bboxes[2], bbox_ref[2])\r\n    bboxes[3] = np.minimum(bboxes[3], bbox_ref[3])\r\n    bboxes = np.transpose(bboxes)\r\n    return bboxes\r\n\r\ndef bboxes_sort(classes, scores, bboxes, top_k=400):\r\n    """"""Sort bounding boxes by decreasing order and keep only the top_k\r\n    """"""\r\n    # if priority_inside:\r\n    #     inside = (bboxes[:, 0] > margin) & (bboxes[:, 1] > margin) & \\\r\n    #         (bboxes[:, 2] < 1-margin) & (bboxes[:, 3] < 1-margin)\r\n    #     idxes = np.argsort(-scores)\r\n    #     inside = inside[idxes]\r\n    #     idxes = np.concatenate([idxes[inside], idxes[~inside]])\r\n    idxes = np.argsort(-scores)\r\n    classes = classes[idxes][:top_k]\r\n    scores = scores[idxes][:top_k]\r\n    bboxes = bboxes[idxes][:top_k]\r\n    return classes, scores, bboxes\r\n\r\ndef bboxes_iou(bboxes1, bboxes2):\r\n    """"""Computing iou between bboxes1 and bboxes2.\r\n    Note: bboxes1 and bboxes2 can be multi-dimensional, but should broacastable.\r\n    """"""\r\n    bboxes1 = np.transpose(bboxes1)\r\n    bboxes2 = np.transpose(bboxes2)\r\n    # Intersection bbox and volume.\r\n    int_ymin = np.maximum(bboxes1[0], bboxes2[0])\r\n    int_xmin = np.maximum(bboxes1[1], bboxes2[1])\r\n    int_ymax = np.minimum(bboxes1[2], bboxes2[2])\r\n    int_xmax = np.minimum(bboxes1[3], bboxes2[3])\r\n\r\n    int_h = np.maximum(int_ymax - int_ymin, 0.)\r\n    int_w = np.maximum(int_xmax - int_xmin, 0.)\r\n    int_vol = int_h * int_w\r\n    # Union volume.\r\n    vol1 = (bboxes1[2] - bboxes1[0]) * (bboxes1[3] - bboxes1[1])\r\n    vol2 = (bboxes2[2] - bboxes2[0]) * (bboxes2[3] - bboxes2[1])\r\n    iou = int_vol / (vol1 + vol2 - int_vol)\r\n    return iou\r\n\r\ndef bboxes_nms(classes, scores, bboxes, nms_threshold=0.5):\r\n    """"""Apply non-maximum selection to bounding boxes.\r\n    """"""\r\n    keep_bboxes = np.ones(scores.shape, dtype=np.bool)\r\n    for i in range(scores.size-1):\r\n        if keep_bboxes[i]:\r\n            # Computer overlap with bboxes which are following.\r\n            overlap = bboxes_iou(bboxes[i], bboxes[(i+1):])\r\n            # Overlap threshold for keeping + checking part of the same class\r\n            keep_overlap = np.logical_or(overlap < nms_threshold, classes[(i+1):] != classes[i])\r\n            keep_bboxes[(i+1):] = np.logical_and(keep_bboxes[(i+1):], keep_overlap)\r\n\r\n    idxes = np.where(keep_bboxes)\r\n    return classes[idxes], scores[idxes], bboxes[idxes]\r\n\r\n\r\n\r\n\r\n\r\n\r\n'"
examples/Resnet/resent.py,40,"b'""""""\r\nDeep Residual Learning\r\nsource: \'https://github.com/wenxinxu/resnet_in_tensorflow\'\r\n2016/12/27\r\n""""""\r\nimport sys\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorlayer as tl\r\nfrom tensorlayer.files import load_cifar10_dataset\r\n\r\ndef tensor_summary(tensor):\r\n    """"""Add histogram and scalar summary of the tensor""""""\r\n    tensor_name = tensor.op.name\r\n    tf.summary.histogram(tensor_name+""/activations"", tensor)\r\n    tf.summary.scalar(tensor_name+""/sparsity"", tf.nn.zero_fraction(tensor))\r\n\r\nclass ResnetConfig(object):\r\n    """"""\r\n    The default hyper parameters config\r\n    """"""\r\n    # The batch normalization variance epsilon\r\n    bn_var_epsilon = 0.001\r\n    # weight decay for regularization in fully connected layer\r\n    fc_weight_decay = 0.0002\r\n    # weight decay for regularization in convolutional layer\r\n    conv_weight_decay = 0.0002\r\n    # Default initializer\r\n    initializer = tf.contrib.layers.xavier_initializer\r\n\r\nclass Resnet(object):\r\n    """"""\r\n    A deep residual learning network class\r\n    """"""\r\n    def __init__(self, input_tensor, n, is_training=True, config=ResnetConfig()):\r\n        """"""\r\n        :param input_tensor: 4-D input tensor\r\n        :param n: int, the number of residual blocks\r\n        :param is_training: bool, create new variables if True, otherwise, reuse the variables\r\n        :param config: The hyper parameters config class\r\n        """"""\r\n        self.input = input_tensor\r\n        self.n = n\r\n        self.is_training = is_training\r\n        self.config = ResnetConfig()\r\n        self.__build__model__()\r\n    \r\n    def __build__model__(self):\r\n        """"""\r\n        This function will bulid the resnet model.\r\n        """"""\r\n        if self.is_training:\r\n            reuse = False\r\n        else:\r\n            reuse = True\r\n        # Keep track of all layers\r\n        layers = []\r\n        # The first layer\r\n        with tf.variable_scope(""conv0"", reuse=reuse):\r\n            conv0 = self._conv_bn_relu_layer(self.input, 16, 3, strides=1)\r\n            tensor_summary(conv0)\r\n            layers.append(conv0)\r\n        \r\n        # The first residual blocks\r\n        for i in range(self.n):\r\n            with tf.variable_scope(""conv1_%d"" % i, reuse=reuse):\r\n                if i == 0:\r\n                    conv1 = self._residual_block(layers[-1], 16, is_first_block=True)\r\n                else:\r\n                    conv1 = self._residual_block(layers[-1], 16)   #[None, 32, 32, 16]\r\n                tensor_summary(conv1)\r\n                layers.append(conv1)\r\n        \r\n        # The second residual blocks\r\n        for i in range(self.n):\r\n            with tf.variable_scope(""conv2_%d"" % i, reuse=reuse):\r\n                conv2 = self._residual_block(layers[-1], 32) #[None, 16, 16, 32]\r\n                tensor_summary(conv2)\r\n                layers.append(conv2)\r\n        \r\n        # The 3th residual blocks\r\n        for i in range(self.n):\r\n            with tf.variable_scope(""conv3_%d"" % i, reuse=reuse):\r\n                conv3 = self._residual_block(layers[-1], 64)  #[None, 8, 8, 64]\r\n                tensor_summary(conv3)\r\n                layers.append(conv3)\r\n        \r\n        # The fully connected layer\r\n        with tf.variable_scope(""fc"", reuse=reuse):\r\n            in_channels = layers[-1].get_shape().as_list()[-1]\r\n            bn = self._batch_normalization_layer(layers[-1], in_channels)\r\n            relu = tf.nn.relu(bn)\r\n            global_pool = tf.reduce_mean(relu, axis=[1, 2])\r\n            output = self._fc_layer(global_pool, 10)\r\n            layers.append(output)\r\n        \r\n        self._output = output\r\n        self._prediction = tf.cast(tf.argmax(tf.nn.softmax(output), axis=1), tf.int32)\r\n\r\n\r\n    def _get_variable(self, name, shape, initializer=None, is_fc_layer=False):\r\n        """"""\r\n        Create the variable of layers\r\n        :param name: string, variable name\r\n        :param shape: list or tuple, the shape of variable\r\n        :param initializer: default initializer\r\n        :param is_fc_layer: use different regularization for different layers\r\n        """"""\r\n        if is_fc_layer:\r\n            scale = self.config.fc_weight_decay\r\n        else:\r\n            scale = self.config.conv_weight_decay\r\n\r\n        if initializer is None:\r\n            initializer = self.config.initializer()\r\n\r\n        var = tf.get_variable(name, shape, initializer=initializer, \r\n                                regularizer=tf.contrib.layers.l2_regularizer(scale=scale))\r\n        return var\r\n\r\n    def _batch_normalization_layer(self, input_tensor, depth_dim=None):\r\n        """"""\r\n        The batch normalization layer\r\n        :param input_tensor: 4-D tensor\r\n        :param depth_dim: the last dimension of the input_tensor\r\n        :return: the normalized tensor with the same shape of input tensor\r\n        """"""\r\n        if depth_dim is None:\r\n            depth_dim = input_tensor.get_shape().as_list()[-1]\r\n        mean, variance = tf.nn.moments(input_tensor, axes=[0, 1, 2], keep_dims=False)\r\n        # Define variables of batch normalization\r\n        beta = tf.get_variable(""beta"", [depth_dim,], dtype=tf.float32, \r\n                            initializer=tf.constant_initializer(0.0))\r\n        gamma = tf.get_variable(""gamma"", [depth_dim,], dtype=tf.float32,\r\n                            initializer=tf.constant_initializer(1.0))\r\n        output_tensor = tf.nn.batch_normalization(input_tensor, mean, variance, \r\n                                                beta, gamma, self.config.bn_var_epsilon)\r\n        return output_tensor\r\n    \r\n    def _fc_layer(self, input_tensor, n_out, n_in=None, activation=tf.identity):\r\n        """"""\r\n        The fully connected layer\r\n        :param input_tensor: 2-D tensor \r\n        :param n_in: int, the number of input units\r\n        :param n_out: int, the number of output units\r\n        :param activation: activation function, default you use identity activation\r\n        """"""\r\n        if n_in is None:\r\n            n_in = input_tensor.get_shape().as_list()[-1]\r\n        weights = self._get_variable(""fc_weight"", [n_in, n_out], initializer=tf.uniform_unit_scaling_initializer(factor=1.0),\r\n                                    is_fc_layer=True)\r\n        biases = self._get_variable(""fc_bias"", [n_out,], initializer=tf.zeros_initializer, is_fc_layer=True)\r\n        wx_b = tf.matmul(input_tensor, weights) + biases\r\n        return activation(wx_b)\r\n    \r\n    def _conv_bn_relu_layer(self, input_tensor, nb_filter, filter_size, strides=1):\r\n        """"""\r\n        This function implements a sequence layes with convolution, batch normalize, and relu\r\n        :param input_tensor: 4-D tensor\r\n        :param nb_filter: int, the number of filters\r\n        :param filter_size: int,  the size of filters\r\n        :param strides: int, the strides of convolution operation\r\n        """"""\r\n        in_channels = input_tensor.get_shape().as_list()[-1]\r\n        filter = self._get_variable(""conv"", shape=[filter_size, filter_size, in_channels, nb_filter])\r\n        conv = tf.nn.conv2d(input_tensor, filter, strides=[1, strides, strides, 1], padding=""SAME"")\r\n        bn = self._batch_normalization_layer(conv, nb_filter)\r\n        return tf.nn.relu(bn)\r\n    \r\n    def _bn_relu_conv_layer(self, input_tensor, nb_filter, filter_size, strides=1):\r\n        """"""\r\n        This function implements a sequence layers with batch normalize, relu and convolution\r\n        :param input_tensor: 4-D tensor\r\n        :param nb_filter: int, the number of filters\r\n        :param filter_size: int,  the size of filters\r\n        :param strides: int, the strides of convolution operation\r\n        """"""\r\n        in_channels = input_tensor.get_shape().as_list()[-1]\r\n        bn = self._batch_normalization_layer(input_tensor, in_channels)\r\n        relu = tf.nn.relu(bn)\r\n        filter = self._get_variable(""conv"", shape=[filter_size, filter_size, in_channels, nb_filter])\r\n        conv = tf.nn.conv2d(relu, filter, strides=[1, strides, strides, 1], padding=""SAME"")\r\n        return conv\r\n\r\n    def _residual_block(self, input_tensor, out_channels, is_first_block=False):\r\n        """"""\r\n        A residual block of resnet\r\n        :param input_tensor: 4-D tensor\r\n        :param out_channels: int, the number of output channels\r\n        :param is_first_block: bool, if it is first residual block of Resnet\r\n        """"""\r\n        in_channels = input_tensor.get_shape().as_list()[-1]\r\n        # If the map feature size reduces, you should use strides =2, also\r\n        # you must average pool over the input, and pad the input at last dimension\r\n        if in_channels*2 == out_channels:\r\n            strides = 2\r\n        elif in_channels == out_channels:\r\n            strides = 1\r\n        else:\r\n            raise ValueError(""There is mismatch betwwen input and output channels"")\r\n        \r\n        # The first conv layer in the first residual block only implments the conv operation\r\n        with tf.variable_scope(""block_conv1""):\r\n            if is_first_block:\r\n                filter = self._get_variable(""conv"", shape=[3, 3, in_channels, out_channels])\r\n                conv1 = tf.nn.conv2d(input_tensor, filter, strides=[1, 1, 1, 1], padding=""SAME"")\r\n            else:\r\n                conv1 = self._bn_relu_conv_layer(input_tensor, out_channels, 3, strides=strides)\r\n\r\n        # The second conv layer\r\n        with tf.variable_scope(""block_conv2""):\r\n            conv2 = self._bn_relu_conv_layer(conv1, out_channels, 3, strides=1)\r\n        \r\n        # Deal with input\r\n        if strides > 1:\r\n            pooled_input = tf.nn.avg_pool(input_tensor, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\r\n                                            padding=""VALID"")\r\n            padded_input = tf.pad(pooled_input, [[0, 0], [0, 0], [0, 0], [in_channels//2, in_channels//2]])\r\n        \r\n        else:\r\n            padded_input = input_tensor\r\n        \r\n        return conv2 + padded_input\r\n        \r\n    @property\r\n    def prediction(self):\r\n        return self._prediction\r\n    \r\n    def get_cost(self, y):\r\n        """"""\r\n        Get the cost for training\r\n        :param y: the target tensor (1-D, [None])\r\n        """"""\r\n        assert y.get_shape().as_list()[0] == self.input.get_shape().as_list()[0]\r\n        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(self._output, y)\r\n        return tf.reduce_mean(cross_entropy)\r\n\r\nif __name__ == ""__main__"":\r\n    X_train, y_train, X_test, y_test = load_cifar10_dataset(shape=(-1, 32, 32, 3), plotable=False)\r\n    train_dir = sys.path[0] + ""/train_dir""\r\n    input_tensor = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\r\n    y = tf.placeholder(tf.int32, shape=[None,])\r\n    resent = Resnet(input_tensor, 2, is_training=True)\r\n    cost = resent.get_cost(y)\r\n    train_op = tf.train.GradientDescentOptimizer(0.1).minimize(cost)\r\n    accuracy = tf.reduce_mean(tf.cast(tf.equal(resent.prediction, y), tf.float32))\r\n    init = tf.global_variables_initializer()\r\n    sess = tf.Session()\r\n    sess.run(init)\r\n\r\n    print(""Start training..."")\r\n    n_epochs = 10\r\n    for epoch in range(n_epochs):\r\n        for X_train_a, y_train_a in tl.iterate.minibatches(X_train, y_train, batch_size=128, shuffle=True):\r\n            sess.run(train_op, feed_dict={resent.input: X_train_a, y: y_train_a})\r\n        n_batchs = 0\r\n        acc = 0\r\n        for X_test_a, y_test_a in tl.iterate.minibatches(X_test, y_test, 128, shuffle=True):\r\n            acc += sess.run(accuracy, feed_dict={resent.input: X_test_a, y: y_test_a})\r\n            n_batchs += 1\r\n        print(""Epoch {0}, test accuracy {1}"".format(epoch, acc/n_batchs))\r\n\r\n'"
examples/VAE/vae_mnist.py,30,"b'""""""\r\nVariational Autoencoder for MNIST data\r\nreference: https://jmetzen.github.io/2015-11-27/vae.html\r\n2017/01/17\r\n""""""\r\nimport sys\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\n\r\nfrom input_data import read_data_sets\r\n\r\n# Random seeds for reproduce\r\nnp.random.seed(2017)\r\ntf.set_random_seed(2017)\r\n\r\nclass VAE(object):\r\n    """"""A simple class of variational autoencoder""""""\r\n    def __init__(self, input_dim=784, z_dim=50, batch_size=100, encoder_hidden_size=[500, 500], \r\n                    decoder_hidden_size=[500, 500], act_fn=tf.nn.softplus):\r\n        """"""\r\n        :param input_dim: int, the dimension of input\r\n        :param z_dim: int, the dimension of latent space\r\n        :param batch_size: int, batch size\r\n        :param encoder_hidden_size: list or tuple, the number of hidden units in encoder\r\n        :param decoder_hidden_size: list or tuple, the number of hidden units in decoder\r\n        :param act_fn: the activation function\r\n        """"""\r\n        self.input_dim = input_dim\r\n        self.z_dim = z_dim\r\n        self.batch_size = batch_size\r\n        self.encoder_hidden_size = encoder_hidden_size\r\n        self.decoder_hidden_size = decoder_hidden_size\r\n        self.act_fn = act_fn\r\n        \r\n        self._bulid_model()\r\n\r\n    def _bulid_model(self):\r\n        """"""The inner function to build the model""""""\r\n        # Input placeholder\r\n        self.x = tf.placeholder(tf.float32, shape=[self.batch_size, self.input_dim])\r\n        # The encoder: determine the mean and (log) variance of Gaussian distribution\r\n        self.z_mean, self.z_log_sigma_sq = self._encoder(self.x)\r\n        # Sampling from Gaussian distribution\r\n        eps = tf.random_normal([self.batch_size, self.z_dim], mean=0.0, stddev=1.0)\r\n        # z = mean + sigma*epsilon\r\n        self.z = tf.add(self.z_mean, tf.mul(tf.sqrt(tf.exp(self.z_log_sigma_sq)), eps))\r\n\r\n        # Decoder: determine the mean of Bernoulli distribution of reconstructed input\r\n        self.x_reconstr_mean = self._decoder(self.z)\r\n        \r\n        # Compute the loss\r\n        with tf.name_scope(""loss""):\r\n            # The reconstruction loss: cross entropy\r\n            reconstr_loss = -tf.reduce_sum(self.x * tf.log(1e-10 + self.x_reconstr_mean) + \\\r\n                            (1.0 - self.x) * tf.log(1e-10 + 1.0 - self.x_reconstr_mean), axis=1)\r\n            # The latent loss: KL divergence\r\n            latent_loss = -0.5 * tf.reduce_sum(1.0 + self.z_log_sigma_sq - tf.square(self.z_mean) - \\\r\n                                    tf.exp(self.z_log_sigma_sq), axis=1)\r\n            # Average over the batch\r\n            self.cost = tf.reduce_mean(reconstr_loss + latent_loss)\r\n        \r\n        # The optimizer\r\n        self.lr = tf.Variable(0.001, trainable=False)\r\n        vars = tf.trainable_variables()\r\n        self.train_op = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.cost, var_list=vars)\r\n        \r\n    def _encoder(self, x, name=""encoder""):\r\n        """"""Encoder""""""\r\n        with tf.variable_scope(name):\r\n            n_in = self.input_dim\r\n            for i, s in enumerate(self.encoder_hidden_size):\r\n                w, b = self._get_vars(n_in, s, name=""h{0}"".format(i))\r\n                if i == 0:\r\n                    h = self.act_fn(tf.nn.xw_plus_b(x, w, b))\r\n                else:\r\n                    h = self.act_fn(tf.nn.xw_plus_b(h, w, b))\r\n                n_in = s\r\n            w, b = self._get_vars(n_in, self.z_dim, name=""out_mean"")\r\n            z_mean = tf.nn.xw_plus_b(h, w, b)\r\n            w, b = self._get_vars(n_in, self.z_dim, name=""out_log_sigma"")\r\n            z_log_sigma_sq = tf.nn.xw_plus_b(h, w, b)\r\n            return z_mean, z_log_sigma_sq\r\n        \r\n    def _decoder(self, z, name=""decoder""):\r\n        """"""Decoder""""""\r\n        with tf.variable_scope(name):\r\n            n_in = self.z_dim\r\n            for i, s in enumerate(self.decoder_hidden_size):\r\n                w, b = self._get_vars(n_in, s, name=""h{0}"".format(i))\r\n                if i == 0:\r\n                    h = self.act_fn(tf.nn.xw_plus_b(z, w, b))\r\n                else:\r\n                    h = self.act_fn(tf.nn.xw_plus_b(h, w, b))\r\n                n_in = s\r\n            # Use sigmoid for Bernoulli distribution\r\n            w, b = self._get_vars(n_in, self.input_dim, name=""out_mean"")\r\n            x_reconstr_mean = tf.nn.sigmoid(tf.nn.xw_plus_b(h, w, b))\r\n            return x_reconstr_mean\r\n\r\n    def _get_vars(self, n_in, n_out, name=""""):\r\n        """"""\r\n        Create weight and bias variables \r\n        """"""\r\n        with tf.variable_scope(name):\r\n            w = tf.get_variable(""w"", [n_in, n_out], initializer=tf.contrib.layers.xavier_initializer())\r\n            b = tf.get_variable(""b"", [n_out,], initializer=tf.constant_initializer(0.1))\r\n            return w, b\r\n\r\nif __name__ == ""__main__"":\r\n    n_epochs = 30\r\n    lr = 0.001\r\n    batch_size = 100\r\n    display_every = 1\r\n\r\n    path = sys.path[0]\r\n    mnist = read_data_sets(""MNIST_data/"", one_hot=True)\r\n    with tf.Session() as sess:\r\n        vae = VAE(input_dim=784, z_dim=2, batch_size=batch_size, encoder_hidden_size=[500, 500],\r\n                    decoder_hidden_size=[500, 500], act_fn=tf.nn.softplus)\r\n        sess.run(tf.global_variables_initializer())\r\n        saver = tf.train.Saver()\r\n        #saver.restore(sess, save_path=path+""/model/model.ckpt"")\r\n        # Start training\r\n        print(""Start training..."")\r\n        total_batch = int(mnist.train.num_examples/batch_size)\r\n        for epoch in range(n_epochs):\r\n            avg_cost = 0.0\r\n            # For each batch \r\n            for i in range(total_batch):\r\n                batch_xs, _ = mnist.train.next_batch(batch_size)\r\n                c, _ = sess.run([vae.cost, vae.train_op], feed_dict={vae.x: batch_xs})\r\n                avg_cost += c/total_batch\r\n            if epoch % display_every == 0:\r\n                save_path = saver.save(sess, path+""/model/model.ckpt"")\r\n                #print(""\\tModel saved in file: {0}"".format(save_path))\r\n                print(""\\tEpoch {0}, cost {1}"".format(epoch, avg_cost))\r\n        \r\n        # Sampling\r\n        x_sample, _ = mnist.test.next_batch(batch_size)\r\n        x_reconstr = sess.run(vae.x_reconstr_mean, feed_dict={vae.x: x_sample})\r\n        plt.figure(figsize=(8, 12))\r\n        for i in range(5):\r\n            plt.subplot(5, 2, 2*i + 1)\r\n            plt.imshow(np.reshape(x_sample[i],(28, 28)), vmin=0, vmax=1, cmap=""gray"")\r\n            plt.title(""Test input"")\r\n            plt.colorbar()\r\n            plt.subplot(5, 2, 2*i + 2)\r\n            plt.imshow(np.reshape(x_reconstr[i], [28, 28]), vmin=0, vmax=1, cmap=""gray"")\r\n            plt.title(""Reconstruction"")\r\n            plt.colorbar()\r\n        plt.tight_layout()\r\n        plt.savefig(path+""/results/img_epoch{0}.jpg"".format(n_epochs))\r\n        plt.show()\r\n\r\n        # Random sampling\r\n        nx, ny = 20, 20\r\n        xs = np.linspace(-3, 3, nx)\r\n        ys = np.linspace(-3, 3, ny)\r\n        xs, ys = np.meshgrid(xs, ys)\r\n        xs = np.reshape(xs, [-1, 1])\r\n        ys = np.reshape(ys, [-1, 1])\r\n        zs = np.concatenate((xs, ys), axis=1)\r\n\r\n        canvas = np.zeros((28*ny, 28*nx))\r\n        xs_recon = np.zeros((batch_size*4, 28*28))\r\n        for i in range(4):\r\n            z_mu = zs[batch_size*i:batch_size*(i+1), :]\r\n            x_mean = sess.run(vae.x_reconstr_mean, feed_dict={vae.z: z_mu})\r\n            xs_recon[i*batch_size:(i+1)*batch_size] = x_mean\r\n        \r\n        n = 0\r\n        for i in range(nx):\r\n            for j in range(ny):\r\n                canvas[(ny-i-1)*28:(ny-i)*28, j*28:(j+1)*28] = xs_recon[n].reshape(28, 28)\r\n                n = n + 1\r\n        \r\n        plt.figure(figsize=(8, 10))\r\n        plt.imshow(canvas, origin=""upper"", vmin=0, vmax=1, interpolation=\'none\', cmap=\'gray\')\r\n        plt.tight_layout()\r\n        plt.savefig(path+""/results/rand_img_epoch{0}.jpg"".format(n_epochs))\r\n        plt.show()\r\n'"
examples/VGG/imagenet_classes.py,0,"b'class_names = \'\'\'tench, Tinca tinca\ngoldfish, Carassius auratus\ngreat white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias\ntiger shark, Galeocerdo cuvieri\nhammerhead, hammerhead shark\nelectric ray, crampfish, numbfish, torpedo\nstingray\ncock\nhen\nostrich, Struthio camelus\nbrambling, Fringilla montifringilla\ngoldfinch, Carduelis carduelis\nhouse finch, linnet, Carpodacus mexicanus\njunco, snowbird\nindigo bunting, indigo finch, indigo bird, Passerina cyanea\nrobin, American robin, Turdus migratorius\nbulbul\njay\nmagpie\nchickadee\nwater ouzel, dipper\nkite\nbald eagle, American eagle, Haliaeetus leucocephalus\nvulture\ngreat grey owl, great gray owl, Strix nebulosa\nEuropean fire salamander, Salamandra salamandra\ncommon newt, Triturus vulgaris\neft\nspotted salamander, Ambystoma maculatum\naxolotl, mud puppy, Ambystoma mexicanum\nbullfrog, Rana catesbeiana\ntree frog, tree-frog\ntailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui\nloggerhead, loggerhead turtle, Caretta caretta\nleatherback turtle, leatherback, leathery turtle, Dermochelys coriacea\nmud turtle\nterrapin\nbox turtle, box tortoise\nbanded gecko\ncommon iguana, iguana, Iguana iguana\nAmerican chameleon, anole, Anolis carolinensis\nwhiptail, whiptail lizard\nagama\nfrilled lizard, Chlamydosaurus kingi\nalligator lizard\nGila monster, Heloderma suspectum\ngreen lizard, Lacerta viridis\nAfrican chameleon, Chamaeleo chamaeleon\nKomodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis\nAfrican crocodile, Nile crocodile, Crocodylus niloticus\nAmerican alligator, Alligator mississipiensis\ntriceratops\nthunder snake, worm snake, Carphophis amoenus\nringneck snake, ring-necked snake, ring snake\nhognose snake, puff adder, sand viper\ngreen snake, grass snake\nking snake, kingsnake\ngarter snake, grass snake\nwater snake\nvine snake\nnight snake, Hypsiglena torquata\nboa constrictor, Constrictor constrictor\nrock python, rock snake, Python sebae\nIndian cobra, Naja naja\ngreen mamba\nsea snake\nhorned viper, cerastes, sand viper, horned asp, Cerastes cornutus\ndiamondback, diamondback rattlesnake, Crotalus adamanteus\nsidewinder, horned rattlesnake, Crotalus cerastes\ntrilobite\nharvestman, daddy longlegs, Phalangium opilio\nscorpion\nblack and gold garden spider, Argiope aurantia\nbarn spider, Araneus cavaticus\ngarden spider, Aranea diademata\nblack widow, Latrodectus mactans\ntarantula\nwolf spider, hunting spider\ntick\ncentipede\nblack grouse\nptarmigan\nruffed grouse, partridge, Bonasa umbellus\nprairie chicken, prairie grouse, prairie fowl\npeacock\nquail\npartridge\nAfrican grey, African gray, Psittacus erithacus\nmacaw\nsulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita\nlorikeet\ncoucal\nbee eater\nhornbill\nhummingbird\njacamar\ntoucan\ndrake\nred-breasted merganser, Mergus serrator\ngoose\nblack swan, Cygnus atratus\ntusker\nechidna, spiny anteater, anteater\nplatypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus\nwallaby, brush kangaroo\nkoala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus\nwombat\njellyfish\nsea anemone, anemone\nbrain coral\nflatworm, platyhelminth\nnematode, nematode worm, roundworm\nconch\nsnail\nslug\nsea slug, nudibranch\nchiton, coat-of-mail shell, sea cradle, polyplacophore\nchambered nautilus, pearly nautilus, nautilus\nDungeness crab, Cancer magister\nrock crab, Cancer irroratus\nfiddler crab\nking crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica\nAmerican lobster, Northern lobster, Maine lobster, Homarus americanus\nspiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish\ncrayfish, crawfish, crawdad, crawdaddy\nhermit crab\nisopod\nwhite stork, Ciconia ciconia\nblack stork, Ciconia nigra\nspoonbill\nflamingo\nlittle blue heron, Egretta caerulea\nAmerican egret, great white heron, Egretta albus\nbittern\ncrane\nlimpkin, Aramus pictus\nEuropean gallinule, Porphyrio porphyrio\nAmerican coot, marsh hen, mud hen, water hen, Fulica americana\nbustard\nruddy turnstone, Arenaria interpres\nred-backed sandpiper, dunlin, Erolia alpina\nredshank, Tringa totanus\ndowitcher\noystercatcher, oyster catcher\npelican\nking penguin, Aptenodytes patagonica\nalbatross, mollymawk\ngrey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus\nkiller whale, killer, orca, grampus, sea wolf, Orcinus orca\ndugong, Dugong dugon\nsea lion\nChihuahua\nJapanese spaniel\nMaltese dog, Maltese terrier, Maltese\nPekinese, Pekingese, Peke\nShih-Tzu\nBlenheim spaniel\npapillon\ntoy terrier\nRhodesian ridgeback\nAfghan hound, Afghan\nbasset, basset hound\nbeagle\nbloodhound, sleuthhound\nbluetick\nblack-and-tan coonhound\nWalker hound, Walker foxhound\nEnglish foxhound\nredbone\nborzoi, Russian wolfhound\nIrish wolfhound\nItalian greyhound\nwhippet\nIbizan hound, Ibizan Podenco\nNorwegian elkhound, elkhound\notterhound, otter hound\nSaluki, gazelle hound\nScottish deerhound, deerhound\nWeimaraner\nStaffordshire bullterrier, Staffordshire bull terrier\nAmerican Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier\nBedlington terrier\nBorder terrier\nKerry blue terrier\nIrish terrier\nNorfolk terrier\nNorwich terrier\nYorkshire terrier\nwire-haired fox terrier\nLakeland terrier\nSealyham terrier, Sealyham\nAiredale, Airedale terrier\ncairn, cairn terrier\nAustralian terrier\nDandie Dinmont, Dandie Dinmont terrier\nBoston bull, Boston terrier\nminiature schnauzer\ngiant schnauzer\nstandard schnauzer\nScotch terrier, Scottish terrier, Scottie\nTibetan terrier, chrysanthemum dog\nsilky terrier, Sydney silky\nsoft-coated wheaten terrier\nWest Highland white terrier\nLhasa, Lhasa apso\nflat-coated retriever\ncurly-coated retriever\ngolden retriever\nLabrador retriever\nChesapeake Bay retriever\nGerman short-haired pointer\nvizsla, Hungarian pointer\nEnglish setter\nIrish setter, red setter\nGordon setter\nBrittany spaniel\nclumber, clumber spaniel\nEnglish springer, English springer spaniel\nWelsh springer spaniel\ncocker spaniel, English cocker spaniel, cocker\nSussex spaniel\nIrish water spaniel\nkuvasz\nschipperke\ngroenendael\nmalinois\nbriard\nkelpie\nkomondor\nOld English sheepdog, bobtail\nShetland sheepdog, Shetland sheep dog, Shetland\ncollie\nBorder collie\nBouvier des Flandres, Bouviers des Flandres\nRottweiler\nGerman shepherd, German shepherd dog, German police dog, alsatian\nDoberman, Doberman pinscher\nminiature pinscher\nGreater Swiss Mountain dog\nBernese mountain dog\nAppenzeller\nEntleBucher\nboxer\nbull mastiff\nTibetan mastiff\nFrench bulldog\nGreat Dane\nSaint Bernard, St Bernard\nEskimo dog, husky\nmalamute, malemute, Alaskan malamute\nSiberian husky\ndalmatian, coach dog, carriage dog\naffenpinscher, monkey pinscher, monkey dog\nbasenji\npug, pug-dog\nLeonberg\nNewfoundland, Newfoundland dog\nGreat Pyrenees\nSamoyed, Samoyede\nPomeranian\nchow, chow chow\nkeeshond\nBrabancon griffon\nPembroke, Pembroke Welsh corgi\nCardigan, Cardigan Welsh corgi\ntoy poodle\nminiature poodle\nstandard poodle\nMexican hairless\ntimber wolf, grey wolf, gray wolf, Canis lupus\nwhite wolf, Arctic wolf, Canis lupus tundrarum\nred wolf, maned wolf, Canis rufus, Canis niger\ncoyote, prairie wolf, brush wolf, Canis latrans\ndingo, warrigal, warragal, Canis dingo\ndhole, Cuon alpinus\nAfrican hunting dog, hyena dog, Cape hunting dog, Lycaon pictus\nhyena, hyaena\nred fox, Vulpes vulpes\nkit fox, Vulpes macrotis\nArctic fox, white fox, Alopex lagopus\ngrey fox, gray fox, Urocyon cinereoargenteus\ntabby, tabby cat\ntiger cat\nPersian cat\nSiamese cat, Siamese\nEgyptian cat\ncougar, puma, catamount, mountain lion, painter, panther, Felis concolor\nlynx, catamount\nleopard, Panthera pardus\nsnow leopard, ounce, Panthera uncia\njaguar, panther, Panthera onca, Felis onca\nlion, king of beasts, Panthera leo\ntiger, Panthera tigris\ncheetah, chetah, Acinonyx jubatus\nbrown bear, bruin, Ursus arctos\nAmerican black bear, black bear, Ursus americanus, Euarctos americanus\nice bear, polar bear, Ursus Maritimus, Thalarctos maritimus\nsloth bear, Melursus ursinus, Ursus ursinus\nmongoose\nmeerkat, mierkat\ntiger beetle\nladybug, ladybeetle, lady beetle, ladybird, ladybird beetle\nground beetle, carabid beetle\nlong-horned beetle, longicorn, longicorn beetle\nleaf beetle, chrysomelid\ndung beetle\nrhinoceros beetle\nweevil\nfly\nbee\nant, emmet, pismire\ngrasshopper, hopper\ncricket\nwalking stick, walkingstick, stick insect\ncockroach, roach\nmantis, mantid\ncicada, cicala\nleafhopper\nlacewing, lacewing fly\ndragonfly, darning needle, devil\'s darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\ndamselfly\nadmiral\nringlet, ringlet butterfly\nmonarch, monarch butterfly, milkweed butterfly, Danaus plexippus\ncabbage butterfly\nsulphur butterfly, sulfur butterfly\nlycaenid, lycaenid butterfly\nstarfish, sea star\nsea urchin\nsea cucumber, holothurian\nwood rabbit, cottontail, cottontail rabbit\nhare\nAngora, Angora rabbit\nhamster\nporcupine, hedgehog\nfox squirrel, eastern fox squirrel, Sciurus niger\nmarmot\nbeaver\nguinea pig, Cavia cobaya\nsorrel\nzebra\nhog, pig, grunter, squealer, Sus scrofa\nwild boar, boar, Sus scrofa\nwarthog\nhippopotamus, hippo, river horse, Hippopotamus amphibius\nox\nwater buffalo, water ox, Asiatic buffalo, Bubalus bubalis\nbison\nram, tup\nbighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis\nibex, Capra ibex\nhartebeest\nimpala, Aepyceros melampus\ngazelle\nArabian camel, dromedary, Camelus dromedarius\nllama\nweasel\nmink\npolecat, fitch, foulmart, foumart, Mustela putorius\nblack-footed ferret, ferret, Mustela nigripes\notter\nskunk, polecat, wood pussy\nbadger\narmadillo\nthree-toed sloth, ai, Bradypus tridactylus\norangutan, orang, orangutang, Pongo pygmaeus\ngorilla, Gorilla gorilla\nchimpanzee, chimp, Pan troglodytes\ngibbon, Hylobates lar\nsiamang, Hylobates syndactylus, Symphalangus syndactylus\nguenon, guenon monkey\npatas, hussar monkey, Erythrocebus patas\nbaboon\nmacaque\nlangur\ncolobus, colobus monkey\nproboscis monkey, Nasalis larvatus\nmarmoset\ncapuchin, ringtail, Cebus capucinus\nhowler monkey, howler\ntiti, titi monkey\nspider monkey, Ateles geoffroyi\nsquirrel monkey, Saimiri sciureus\nMadagascar cat, ring-tailed lemur, Lemur catta\nindri, indris, Indri indri, Indri brevicaudatus\nIndian elephant, Elephas maximus\nAfrican elephant, Loxodonta africana\nlesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens\ngiant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca\nbarracouta, snoek\neel\ncoho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch\nrock beauty, Holocanthus tricolor\nanemone fish\nsturgeon\ngar, garfish, garpike, billfish, Lepisosteus osseus\nlionfish\npuffer, pufferfish, blowfish, globefish\nabacus\nabaya\nacademic gown, academic robe, judge\'s robe\naccordion, piano accordion, squeeze box\nacoustic guitar\naircraft carrier, carrier, flattop, attack aircraft carrier\nairliner\nairship, dirigible\naltar\nambulance\namphibian, amphibious vehicle\nanalog clock\napiary, bee house\napron\nashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin\nassault rifle, assault gun\nbackpack, back pack, knapsack, packsack, rucksack, haversack\nbakery, bakeshop, bakehouse\nbalance beam, beam\nballoon\nballpoint, ballpoint pen, ballpen, Biro\nBand Aid\nbanjo\nbannister, banister, balustrade, balusters, handrail\nbarbell\nbarber chair\nbarbershop\nbarn\nbarometer\nbarrel, cask\nbarrow, garden cart, lawn cart, wheelbarrow\nbaseball\nbasketball\nbassinet\nbassoon\nbathing cap, swimming cap\nbath towel\nbathtub, bathing tub, bath, tub\nbeach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon\nbeacon, lighthouse, beacon light, pharos\nbeaker\nbearskin, busby, shako\nbeer bottle\nbeer glass\nbell cote, bell cot\nbib\nbicycle-built-for-two, tandem bicycle, tandem\nbikini, two-piece\nbinder, ring-binder\nbinoculars, field glasses, opera glasses\nbirdhouse\nboathouse\nbobsled, bobsleigh, bob\nbolo tie, bolo, bola tie, bola\nbonnet, poke bonnet\nbookcase\nbookshop, bookstore, bookstall\nbottlecap\nbow\nbow tie, bow-tie, bowtie\nbrass, memorial tablet, plaque\nbrassiere, bra, bandeau\nbreakwater, groin, groyne, mole, bulwark, seawall, jetty\nbreastplate, aegis, egis\nbroom\nbucket, pail\nbuckle\nbulletproof vest\nbullet train, bullet\nbutcher shop, meat market\ncab, hack, taxi, taxicab\ncaldron, cauldron\ncandle, taper, wax light\ncannon\ncanoe\ncan opener, tin opener\ncardigan\ncar mirror\ncarousel, carrousel, merry-go-round, roundabout, whirligig\ncarpenter\'s kit, tool kit\ncarton\ncar wheel\ncash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM\ncassette\ncassette player\ncastle\ncatamaran\nCD player\ncello, violoncello\ncellular telephone, cellular phone, cellphone, cell, mobile phone\nchain\nchainlink fence\nchain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour\nchain saw, chainsaw\nchest\nchiffonier, commode\nchime, bell, gong\nchina cabinet, china closet\nChristmas stocking\nchurch, church building\ncinema, movie theater, movie theatre, movie house, picture palace\ncleaver, meat cleaver, chopper\ncliff dwelling\ncloak\nclog, geta, patten, sabot\ncocktail shaker\ncoffee mug\ncoffeepot\ncoil, spiral, volute, whorl, helix\ncombination lock\ncomputer keyboard, keypad\nconfectionery, confectionary, candy store\ncontainer ship, containership, container vessel\nconvertible\ncorkscrew, bottle screw\ncornet, horn, trumpet, trump\ncowboy boot\ncowboy hat, ten-gallon hat\ncradle\ncrane\ncrash helmet\ncrate\ncrib, cot\nCrock Pot\ncroquet ball\ncrutch\ncuirass\ndam, dike, dyke\ndesk\ndesktop computer\ndial telephone, dial phone\ndiaper, nappy, napkin\ndigital clock\ndigital watch\ndining table, board\ndishrag, dishcloth\ndishwasher, dish washer, dishwashing machine\ndisk brake, disc brake\ndock, dockage, docking facility\ndogsled, dog sled, dog sleigh\ndome\ndoormat, welcome mat\ndrilling platform, offshore rig\ndrum, membranophone, tympan\ndrumstick\ndumbbell\nDutch oven\nelectric fan, blower\nelectric guitar\nelectric locomotive\nentertainment center\nenvelope\nespresso maker\nface powder\nfeather boa, boa\nfile, file cabinet, filing cabinet\nfireboat\nfire engine, fire truck\nfire screen, fireguard\nflagpole, flagstaff\nflute, transverse flute\nfolding chair\nfootball helmet\nforklift\nfountain\nfountain pen\nfour-poster\nfreight car\nFrench horn, horn\nfrying pan, frypan, skillet\nfur coat\ngarbage truck, dustcart\ngasmask, respirator, gas helmet\ngas pump, gasoline pump, petrol pump, island dispenser\ngoblet\ngo-kart\ngolf ball\ngolfcart, golf cart\ngondola\ngong, tam-tam\ngown\ngrand piano, grand\ngreenhouse, nursery, glasshouse\ngrille, radiator grille\ngrocery store, grocery, food market, market\nguillotine\nhair slide\nhair spray\nhalf track\nhammer\nhamper\nhand blower, blow dryer, blow drier, hair dryer, hair drier\nhand-held computer, hand-held microcomputer\nhandkerchief, hankie, hanky, hankey\nhard disc, hard disk, fixed disk\nharmonica, mouth organ, harp, mouth harp\nharp\nharvester, reaper\nhatchet\nholster\nhome theater, home theatre\nhoneycomb\nhook, claw\nhoopskirt, crinoline\nhorizontal bar, high bar\nhorse cart, horse-cart\nhourglass\niPod\niron, smoothing iron\njack-o\'-lantern\njean, blue jean, denim\njeep, landrover\njersey, T-shirt, tee shirt\njigsaw puzzle\njinrikisha, ricksha, rickshaw\njoystick\nkimono\nknee pad\nknot\nlab coat, laboratory coat\nladle\nlampshade, lamp shade\nlaptop, laptop computer\nlawn mower, mower\nlens cap, lens cover\nletter opener, paper knife, paperknife\nlibrary\nlifeboat\nlighter, light, igniter, ignitor\nlimousine, limo\nliner, ocean liner\nlipstick, lip rouge\nLoafer\nlotion\nloudspeaker, speaker, speaker unit, loudspeaker system, speaker system\nloupe, jeweler\'s loupe\nlumbermill, sawmill\nmagnetic compass\nmailbag, postbag\nmailbox, letter box\nmaillot\nmaillot, tank suit\nmanhole cover\nmaraca\nmarimba, xylophone\nmask\nmatchstick\nmaypole\nmaze, labyrinth\nmeasuring cup\nmedicine chest, medicine cabinet\nmegalith, megalithic structure\nmicrophone, mike\nmicrowave, microwave oven\nmilitary uniform\nmilk can\nminibus\nminiskirt, mini\nminivan\nmissile\nmitten\nmixing bowl\nmobile home, manufactured home\nModel T\nmodem\nmonastery\nmonitor\nmoped\nmortar\nmortarboard\nmosque\nmosquito net\nmotor scooter, scooter\nmountain bike, all-terrain bike, off-roader\nmountain tent\nmouse, computer mouse\nmousetrap\nmoving van\nmuzzle\nnail\nneck brace\nnecklace\nnipple\nnotebook, notebook computer\nobelisk\noboe, hautboy, hautbois\nocarina, sweet potato\nodometer, hodometer, mileometer, milometer\noil filter\norgan, pipe organ\noscilloscope, scope, cathode-ray oscilloscope, CRO\noverskirt\noxcart\noxygen mask\npacket\npaddle, boat paddle\npaddlewheel, paddle wheel\npadlock\npaintbrush\npajama, pyjama, pj\'s, jammies\npalace\npanpipe, pandean pipe, syrinx\npaper towel\nparachute, chute\nparallel bars, bars\npark bench\nparking meter\npassenger car, coach, carriage\npatio, terrace\npay-phone, pay-station\npedestal, plinth, footstall\npencil box, pencil case\npencil sharpener\nperfume, essence\nPetri dish\nphotocopier\npick, plectrum, plectron\npickelhaube\npicket fence, paling\npickup, pickup truck\npier\npiggy bank, penny bank\npill bottle\npillow\nping-pong ball\npinwheel\npirate, pirate ship\npitcher, ewer\nplane, carpenter\'s plane, woodworking plane\nplanetarium\nplastic bag\nplate rack\nplow, plough\nplunger, plumber\'s helper\nPolaroid camera, Polaroid Land camera\npole\npolice van, police wagon, paddy wagon, patrol wagon, wagon, black Maria\nponcho\npool table, billiard table, snooker table\npop bottle, soda bottle\npot, flowerpot\npotter\'s wheel\npower drill\nprayer rug, prayer mat\nprinter\nprison, prison house\nprojectile, missile\nprojector\npuck, hockey puck\npunching bag, punch bag, punching ball, punchball\npurse\nquill, quill pen\nquilt, comforter, comfort, puff\nracer, race car, racing car\nracket, racquet\nradiator\nradio, wireless\nradio telescope, radio reflector\nrain barrel\nrecreational vehicle, RV, R.V.\nreel\nreflex camera\nrefrigerator, icebox\nremote control, remote\nrestaurant, eating house, eating place, eatery\nrevolver, six-gun, six-shooter\nrifle\nrocking chair, rocker\nrotisserie\nrubber eraser, rubber, pencil eraser\nrugby ball\nrule, ruler\nrunning shoe\nsafe\nsafety pin\nsaltshaker, salt shaker\nsandal\nsarong\nsax, saxophone\nscabbard\nscale, weighing machine\nschool bus\nschooner\nscoreboard\nscreen, CRT screen\nscrew\nscrewdriver\nseat belt, seatbelt\nsewing machine\nshield, buckler\nshoe shop, shoe-shop, shoe store\nshoji\nshopping basket\nshopping cart\nshovel\nshower cap\nshower curtain\nski\nski mask\nsleeping bag\nslide rule, slipstick\nsliding door\nslot, one-armed bandit\nsnorkel\nsnowmobile\nsnowplow, snowplough\nsoap dispenser\nsoccer ball\nsock\nsolar dish, solar collector, solar furnace\nsombrero\nsoup bowl\nspace bar\nspace heater\nspace shuttle\nspatula\nspeedboat\nspider web, spider\'s web\nspindle\nsports car, sport car\nspotlight, spot\nstage\nsteam locomotive\nsteel arch bridge\nsteel drum\nstethoscope\nstole\nstone wall\nstopwatch, stop watch\nstove\nstrainer\nstreetcar, tram, tramcar, trolley, trolley car\nstretcher\nstudio couch, day bed\nstupa, tope\nsubmarine, pigboat, sub, U-boat\nsuit, suit of clothes\nsundial\nsunglass\nsunglasses, dark glasses, shades\nsunscreen, sunblock, sun blocker\nsuspension bridge\nswab, swob, mop\nsweatshirt\nswimming trunks, bathing trunks\nswing\nswitch, electric switch, electrical switch\nsyringe\ntable lamp\ntank, army tank, armored combat vehicle, armoured combat vehicle\ntape player\nteapot\nteddy, teddy bear\ntelevision, television system\ntennis ball\nthatch, thatched roof\ntheater curtain, theatre curtain\nthimble\nthresher, thrasher, threshing machine\nthrone\ntile roof\ntoaster\ntobacco shop, tobacconist shop, tobacconist\ntoilet seat\ntorch\ntotem pole\ntow truck, tow car, wrecker\ntoyshop\ntractor\ntrailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi\ntray\ntrench coat\ntricycle, trike, velocipede\ntrimaran\ntripod\ntriumphal arch\ntrolleybus, trolley coach, trackless trolley\ntrombone\ntub, vat\nturnstile\ntypewriter keyboard\numbrella\nunicycle, monocycle\nupright, upright piano\nvacuum, vacuum cleaner\nvase\nvault\nvelvet\nvending machine\nvestment\nviaduct\nviolin, fiddle\nvolleyball\nwaffle iron\nwall clock\nwallet, billfold, notecase, pocketbook\nwardrobe, closet, press\nwarplane, military plane\nwashbasin, handbasin, washbowl, lavabo, wash-hand basin\nwasher, automatic washer, washing machine\nwater bottle\nwater jug\nwater tower\nwhiskey jug\nwhistle\nwig\nwindow screen\nwindow shade\nWindsor tie\nwine bottle\nwing\nwok\nwooden spoon\nwool, woolen, woollen\nworm fence, snake fence, snake-rail fence, Virginia fence\nwreck\nyawl\nyurt\nweb site, website, internet site, site\ncomic book\ncrossword puzzle, crossword\nstreet sign\ntraffic light, traffic signal, stoplight\nbook jacket, dust cover, dust jacket, dust wrapper\nmenu\nplate\nguacamole\nconsomme\nhot pot, hotpot\ntrifle\nice cream, icecream\nice lolly, lolly, lollipop, popsicle\nFrench loaf\nbagel, beigel\npretzel\ncheeseburger\nhotdog, hot dog, red hot\nmashed potato\nhead cabbage\nbroccoli\ncauliflower\nzucchini, courgette\nspaghetti squash\nacorn squash\nbutternut squash\ncucumber, cuke\nartichoke, globe artichoke\nbell pepper\ncardoon\nmushroom\nGranny Smith\nstrawberry\norange\nlemon\nfig\npineapple, ananas\nbanana\njackfruit, jak, jack\ncustard apple\npomegranate\nhay\ncarbonara\nchocolate sauce, chocolate syrup\ndough\nmeat loaf, meatloaf\npizza, pizza pie\npotpie\nburrito\nred wine\nespresso\ncup\neggnog\nalp\nbubble\ncliff, drop, drop-off\ncoral reef\ngeyser\nlakeside, lakeshore\npromontory, headland, head, foreland\nsandbar, sand bar\nseashore, coast, seacoast, sea-coast\nvalley, vale\nvolcano\nballplayer, baseball player\ngroom, bridegroom\nscuba diver\nrapeseed\ndaisy\nyellow lady\'s slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\ncorn\nacorn\nhip, rose hip, rosehip\nbuckeye, horse chestnut, conker\ncoral fungus\nagaric\ngyromitra\nstinkhorn, carrion fungus\nearthstar\nhen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa\nbolete\near, spike, capitulum\ntoilet tissue, toilet paper, bathroom tissue\'\'\'.split(""\\n"")'"
examples/VGG/vgg16.py,34,"b'""""""\r\nThe VGG16 Model for cifra10 dataset\r\nauthor: Ye Hu\r\n2016/12/26\r\n""""""\r\nimport sys\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom scipy.misc import imread, imresize, toimage\r\nimport matplotlib.pyplot as plt\r\nimport skimage\r\nimport skimage.io\r\nimport skimage.transform\r\nfrom imagenet_classes import class_names\r\n\r\nVGG_MEAN = [103.939, 116.779, 123.68]\r\n\r\nclass VGG16(object):\r\n    """"""\r\n    The VGG16 model for image classification\r\n    """"""\r\n    def __init__(self, vgg16_npy_path=None, trainable=True):\r\n        """"""\r\n        :param vgg16_npy_path: string, vgg16_npz path\r\n        :param trainable: bool, construct a trainable model if True\r\n        """"""\r\n        # The pretained data\r\n        if vgg16_npy_path is None:\r\n            self._data_dict = None\r\n        else:\r\n            self._data_dict = np.load(vgg16_npy_path, encoding=""latin1"").item()\r\n        self.trainable = trainable\r\n        # Keep all trainable parameters\r\n        self._var_dict = {}\r\n        self.__bulid__()\r\n\r\n    def __bulid__(self):\r\n        """"""\r\n        The inner method to build VGG16 model\r\n        """"""\r\n        # input and output\r\n        self._x = tf.placeholder(tf.float32, shape=[None, 224, 224, 3])\r\n        self._y = tf.placeholder(tf.int64, shape=[None,])\r\n        # Data preprocessiing\r\n        mean = tf.constant([103.939, 116.779, 123.68], dtype=tf.float32, shape=[1, 1, 1, 3])\r\n        x = self._x - mean\r\n        self._train_mode = tf.placeholder(tf.bool)   # use training model is True, otherwise test model\r\n        # construct model\r\n        conv1_1 = self._conv_layer(x, 3, 64, ""conv1_1"")\r\n        conv1_2 = self._conv_layer(conv1_1, 64, 64, ""conv1_2"")\r\n        pool1 = self._max_pool(conv1_2, ""pool1"")\r\n\r\n        conv2_1 = self._conv_layer(pool1, 64, 128, ""conv2_1"")\r\n        conv2_2 = self._conv_layer(conv2_1, 128, 128, ""conv2_2"")\r\n        pool2 = self._max_pool(conv2_2, ""pool2"")\r\n\r\n        conv3_1 = self._conv_layer(pool2, 128, 256, ""conv3_1"")\r\n        conv3_2 = self._conv_layer(conv3_1, 256, 256, ""conv3_2"")\r\n        conv3_3 = self._conv_layer(conv3_2, 256, 256, ""conv3_3"")\r\n        pool3 = self._max_pool(conv3_3, ""pool3"")\r\n\r\n        conv4_1 = self._conv_layer(pool3, 256, 512, ""conv4_1"")\r\n        conv4_2 = self._conv_layer(conv4_1, 512, 512, ""conv4_2"")\r\n        conv4_3 = self._conv_layer(conv4_2, 512, 512, ""conv4_3"")\r\n        pool4 = self._max_pool(conv4_3, ""pool4"")\r\n\r\n        conv5_1 = self._conv_layer(pool4, 512, 512, ""conv5_1"")\r\n        conv5_2 = self._conv_layer(conv5_1, 512, 512, ""conv5_2"")\r\n        conv5_3 = self._conv_layer(conv5_2, 512, 512, ""conv5_3"")\r\n        pool5 = self._max_pool(conv5_3, ""pool5"")\r\n\r\n        # n_in = ((224 / (2**5)) ** 2) * 512\r\n        fc6 = self._fc_layer(pool5, 25088, 4096, ""fc6"", act=tf.nn.relu, reshaped=False)\r\n        # Use train_mode to control\r\n        fc6 = tf.cond(self._train_mode, lambda : tf.nn.dropout(fc6, 0.5), lambda: fc6)\r\n        fc7 = self._fc_layer(fc6, 4096, 4096, ""fc7"", act=tf.nn.relu)\r\n        fc7 = tf.cond(self._train_mode, lambda : tf.nn.dropout(fc7, 0.5), lambda: fc7)\r\n        fc8 = self._fc_layer(fc7, 4096, 1000, ""fc8"", act=tf.identity)\r\n\r\n        self._prob = tf.nn.softmax(fc8, name=""prob"")\r\n\r\n        if self.trainable:\r\n            self._cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(fc8, self._y))\r\n            correct_pred = tf.equal(self._y, tf.argmax(self._prob, 1))\r\n            self._accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n        else:\r\n            self._cost = None\r\n            self._accuracy = None\r\n\r\n    def _conv_layer(self, inpt, in_channels, out_channels, name):\r\n        """"""\r\n        Create conv layer\r\n        """"""\r\n        with tf.variable_scope(name):\r\n            filters, biases = self._get_conv_var(3, in_channels, out_channels, name)\r\n            conv_output = tf.nn.conv2d(inpt, filters, strides=[1, 1, 1, 1], padding=""SAME"")\r\n            conv_output = tf.nn.bias_add(conv_output, biases)\r\n            conv_output = tf.nn.relu(conv_output)\r\n        return conv_output\r\n    \r\n    def _fc_layer(self, inpt, n_in, n_out, name, act=tf.nn.relu, reshaped=True):\r\n        """"""Create fully connected layer""""""\r\n        if not reshaped:\r\n            inpt = tf.reshape(inpt, shape=[-1, n_in])\r\n        with tf.variable_scope(name):\r\n            weights, biases = self._get_fc_var(n_in, n_out, name)\r\n            output = tf.matmul(inpt, weights) + biases\r\n        return act(output)\r\n\r\n    \r\n    def _avg_pool(self, inpt, name):\r\n        return tf.nn.avg_pool(inpt, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=""SAME"",\r\n                                name=name)\r\n    \r\n    def _max_pool(self, inpt, name):\r\n        return tf.nn.max_pool(inpt, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=""SAME"",\r\n                                name=name)\r\n    \r\n    def _get_fc_var(self, n_in, n_out, name):\r\n        """"""Get the weights and biases of fully connected layer""""""\r\n        if self.trainable:\r\n            init_weights = tf.truncated_normal([n_in, n_out], 0.0, 0.001)\r\n            init_biases = tf.truncated_normal([n_out,], 0.0, 0.001)\r\n        else:\r\n            init_weights = None\r\n            init_biases = None\r\n        weights = self._get_var(init_weights, name, 0, name+""_weights"")\r\n        biases = self._get_var(init_biases, name, 1, name+""_biases"")\r\n        return weights, biases\r\n\r\n    def _get_conv_var(self, filter_size, in_channels, out_channels, name):\r\n        """"""\r\n        Get the filter and bias of conv layer \r\n        """"""\r\n        if self.trainable:\r\n            initial_value_filter = tf.truncated_normal([filter_size, filter_size, in_channels, out_channels], 0.0, 0.001)\r\n            initial_value_bias = tf.truncated_normal([out_channels,], 0.0, 0.001)\r\n        else:\r\n            initial_value_filter = None\r\n            initial_value_bias = None\r\n        filters = self._get_var(initial_value_filter, name, 0, name+""_filters"")\r\n        biases = self._get_var(initial_value_bias, name, 1, name+""_biases"")\r\n        return filters, biases\r\n\r\n    \r\n    def _get_var(self, initial_value, name, idx, var_name):\r\n        """"""\r\n        Use this method to construct variable parameters\r\n        """"""\r\n        if self._data_dict is not None:\r\n            value = self._data_dict[name][idx]\r\n        else:\r\n            value = initial_value\r\n\r\n        if self.trainable:\r\n            var = tf.Variable(value, dtype=tf.float32, name=var_name)\r\n        else:\r\n            var = tf.constant(value, dtype=tf.float32, name=""var_name"")\r\n        # Save\r\n        self._var_dict[(name, idx)] = var\r\n        return var\r\n    \r\n    def get_train_op(self, lr=0.01):\r\n        if not self.trainable:\r\n            return\r\n        return tf.train.GradientDescentOptimizer(lr).minimize(self.cost, \r\n                                                    var_list=list(self._var_dict.values()))\r\n    \r\n    def save_npy(self, sess, npy_path=""./vgg16_save.npy""):\r\n        data_dict = {}\r\n        for (name, idx), var in self._var_dict.items():\r\n            var_out = sess.run(var)\r\n            if not data_dict.has_key(name):\r\n                data_dict[name] = {}\r\n            data_dict[name][idx] = var_out\r\n        \r\n        np.save(npy_path, data_dict)\r\n        print(""File saved"", npy_path)\r\n\r\n    @property\r\n    def input(self):\r\n        return self._x\r\n    \r\n    @property\r\n    def target(self):\r\n        return self._y\r\n    \r\n    @property\r\n    def train_mode(self):\r\n        return self._train_mode\r\n    \r\n    @property\r\n    def accuracy(self):\r\n        return self._accuracy\r\n\r\n    @property\r\n    def cost(self):\r\n        return self._cost\r\n    \r\n    @property\r\n    def prob(self):\r\n        return self._prob\r\n\r\n# returns image of shape [224, 224, 3]\r\n# [height, width, depth]\r\ndef load_image(path):\r\n    # load image\r\n    img = skimage.io.imread(path)\r\n    img = img / 255.0\r\n    # assert (0 <= img).all() and (img <= 1.0).all()\r\n    # print ""Original Image Shape: "", img.shape\r\n    # we crop image from center\r\n    short_edge = min(img.shape[:2])\r\n    yy = int((img.shape[0] - short_edge) / 2)\r\n    xx = int((img.shape[1] - short_edge) / 2)\r\n    crop_img = img[yy: yy + short_edge, xx: xx + short_edge]\r\n    # resize to 224, 224\r\n    resized_img = skimage.transform.resize(crop_img, (224, 224))\r\n    return resized_img\r\n\r\n\r\n\r\ndef test_not_trainable_vgg16():\r\n    path = sys.path[0]\r\n    img1 = load_image(path+""/tiger.jpeg"")*255.0\r\n    img2 = load_image(path+""/puzzle.jpeg"")*255.0\r\n    batch1 = img1.reshape((1, 224, 224, 3))\r\n    batch2 = img2.reshape((1, 224, 224, 3))\r\n\r\n    batch = np.concatenate((batch1, batch2), 0)\r\n\r\n    with tf.Graph().as_default(), tf.Session() as sess:\r\n        vgg = VGG16(path+""/vgg16.npy"", trainable=False)\r\n        probs = sess.run(vgg.prob, feed_dict={vgg.input: batch, vgg.train_mode: False})\r\n        for i, prob in enumerate([probs[0], probs[1]]):\r\n            preds = (np.argsort(prob)[::-1])[0:5]\r\n            print(""The"" +str(i) + "" image:"")\r\n            for p in preds:\r\n                print(""\\t"", p, class_names[p], prob[p])\r\n\r\nif __name__ == ""__main__"":\r\n    path = sys.path[0]\r\n    img1 = load_image(path+""/tiger.jpeg"")*255.0\r\n    img2 = load_image(path+""/puzzle.jpeg"")*255.0\r\n    batch1 = img1.reshape((1, 224, 224, 3))\r\n    batch2 = img2.reshape((1, 224, 224, 3))\r\n    x = np.concatenate((batch1, batch2), 0)\r\n    y = np.array([292, 611], dtype=np.int64)\r\n    with tf.Graph().as_default():\r\n        with tf.Session() as sess:\r\n            vgg = VGG16(path+""/vgg16.npy"", trainable=True)\r\n            sess.run(tf.global_variables_initializer())\r\n\r\n            train_op = vgg.get_train_op(lr=0.0001)\r\n            _, cost = sess.run([train_op, vgg.cost], feed_dict={vgg.input: x,\r\n                                                            vgg.target: y, vgg.train_mode:True})\r\n            accuracy = sess.run(vgg.accuracy, feed_dict={vgg.input: x,\r\n                                                        vgg.target: y, vgg.train_mode:False})\r\n            print(cost, accuracy)\r\n\r\n\r\n\r\n'"
examples/cnn_setence_classification/data_helpers.py,0,"b'import numpy as np\nimport re\nimport itertools\nfrom collections import Counter\n\n\ndef clean_str(string):\n    """"""\n    Tokenization/string cleaning for all datasets except for SST.\n    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n    """"""\n    string = re.sub(r""[^A-Za-z0-9(),!?\\\'\\`]"", "" "", string)\n    string = re.sub(r""\\\'s"", "" \\\'s"", string)\n    string = re.sub(r""\\\'ve"", "" \\\'ve"", string)\n    string = re.sub(r""n\\\'t"", "" n\\\'t"", string)\n    string = re.sub(r""\\\'re"", "" \\\'re"", string)\n    string = re.sub(r""\\\'d"", "" \\\'d"", string)\n    string = re.sub(r""\\\'ll"", "" \\\'ll"", string)\n    string = re.sub(r"","", "" , "", string)\n    string = re.sub(r""!"", "" ! "", string)\n    string = re.sub(r""\\("", "" \\( "", string)\n    string = re.sub(r""\\)"", "" \\) "", string)\n    string = re.sub(r""\\?"", "" \\? "", string)\n    string = re.sub(r""\\s{2,}"", "" "", string)\n    return string.strip().lower()\n\n\ndef load_data_and_labels(positive_data_file, negative_data_file):\n    """"""\n    Loads MR polarity data from files, splits the data into words and generates labels.\n    Returns split sentences and labels.\n    """"""\n    # Load data from files\n    positive_examples = list(open(positive_data_file, ""r"", encoding=""utf-8"").readlines())\n    positive_examples = [s.strip() for s in positive_examples]\n    negative_examples = list(open(negative_data_file, ""r"", encoding=""utf-8"").readlines())\n    negative_examples = [s.strip() for s in negative_examples]\n    # Split by words\n    x_text = positive_examples + negative_examples\n    x_text = [clean_str(sent) for sent in x_text]\n    # Generate labels\n    positive_labels = [[0, 1] for _ in positive_examples]\n    negative_labels = [[1, 0] for _ in negative_examples]\n    y = np.concatenate([positive_labels, negative_labels], 0)\n    return [x_text, y]\n\n\ndef batch_iter(data, batch_size, num_epochs, shuffle=True):\n    """"""\n    Generates a batch iterator for a dataset.\n    """"""\n    data = np.array(data)\n    data_size = len(data)\n    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n    for epoch in range(num_epochs):\n        # Shuffle the data at each epoch\n        if shuffle:\n            shuffle_indices = np.random.permutation(np.arange(data_size))\n            shuffled_data = data[shuffle_indices]\n        else:\n            shuffled_data = data\n        for batch_num in range(num_batches_per_epoch):\n            start_index = batch_num * batch_size\n            end_index = min((batch_num + 1) * batch_size, data_size)\n            yield shuffled_data[start_index:end_index]\n'"
examples/cnn_setence_classification/text_cnn.py,36,"b'""""""\r\nA CNN model for sentence classification\r\nsource: \'https://github.com/dennybritz/cnn-text-classification-tf/blob/master/text_cnn.py\'\r\n2016/12/21\r\n""""""\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclass TextCNN(object):\r\n    """"""\r\n    A CNN class for sentence classification\r\n    The model includes an embedding layer, a convolutional layer, a max-pooling layer and\r\n    a softmax layer as the output.\r\n    """"""\r\n    def __init__(self, seq_len, vocab_size, embedding_size, filter_sizes, num_filters,\r\n                    num_classes=2, l2_reg_lambda=0.0):\r\n        """"""\r\n        :param seq_len: int, the sequence length (i.e. the length of the sentences, \r\n                        keep all length same by zero-padding)\r\n        :param vocab_size: int, the size of vocabulary to define the embedding layer\r\n        :param embedding_size: int, the dimensionality of the embeddings (word vector). \r\n        :param filter_sizes: list or tuple, The number of words we want our convolutional filters to cover. \r\n                            For example, [3, 4, 5] means that we will have filters that slide over 3, 4 \r\n                            and 5 words respectively\r\n        :param num_filters: int, the number of each filter with different filter_size, hence, we have a total of\r\n                            len(filter_sizes) * num_filters filters\r\n        :param num_classes: the number of classes we want to predict in the output layer, default 2\r\n        :param l2_reg_lambda: float, the ratio of L2 loss\r\n        """"""\r\n        # keep track of all parameters\r\n        self.seq_len = seq_len\r\n        self.vocab_size = vocab_size\r\n        self.embedding_szie = embedding_size\r\n        self.filter_sizes = filter_sizes\r\n        self.num_filters = num_filters\r\n        self.num_classes = num_classes\r\n        self.l2_reg_lambda = l2_reg_lambda\r\n        # Define the input and output\r\n        self.x = tf.placeholder(tf.int32, shape=[None, seq_len], name=""x"")\r\n        self.y = tf.placeholder(tf.float32, shape=[None, num_classes], name=""y"")\r\n        # The dropout probability\r\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\r\n        # Compute the L2 regularization loss\r\n        L2_loss = tf.constant(0.0)   # initial value 0.0\r\n\r\n        # The Embedding layer\r\n        with tf.device(""/cpu:0""):   # embedding implementation not support GPU\r\n            with tf.name_scope(""embedding""):\r\n                # The embedding matrix\r\n                self.W_embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\r\n                                            dtype=tf.float32, name=""W_embedding"")\r\n                # The embedding results   \r\n                self.embedded_chars = tf.nn.embedding_lookup(self.W_embedding, self.x)   #[None, seq_len, embedding_size]\r\n                # Expand it to use conv2D operation\r\n                self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, axis=-1) # [None, seq_len, embedding_size, 1]\r\n        \r\n        # The convolution and maxpool layer\r\n        pooled_outputs = []\r\n        self.Ws_conv = []\r\n        self.bs_conv = []\r\n        # For each filter\r\n        for i, filter_size in enumerate(filter_sizes):\r\n            with tf.name_scope(""conv_maxpool_{0}"".format(filter_size)):\r\n                # Convolution layer\r\n                filter_shape = [filter_size, embedding_size, 1, num_filters]\r\n                # Conv params\r\n                W_conv = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1),\r\n                                            dtype=tf.float32, name=""W_conv"")\r\n                self.Ws_conv.append(W_conv)\r\n                b_conv = tf.Variable(tf.constant(0.1, shape=[num_filters,]), dtype=tf.float32,\r\n                                                name=""b_conv"")\r\n                self.bs_conv.append(b_conv)\r\n                # conv result\r\n                conv_output = tf.nn.conv2d(self.embedded_chars_expanded, W_conv, strides=[1, 1, 1, 1],\r\n                                            padding=""VALID"", name=""conv"")   # [None, seq_len-filter_size+1, 1, num_filters]\r\n                # use relu as activation\r\n                conv_h = tf.nn.relu(tf.nn.bias_add(conv_output, b_conv), name=""relu"")\r\n                # Use max-pooling\r\n                pool_output = tf.nn.max_pool(conv_h, ksize=[1, seq_len-filter_size+1, 1, 1],\r\n                                            strides=[1, 1, 1, 1], padding=""VALID"", name=""max_pooling"")\r\n                pooled_outputs.append(pool_output)   # [None, 1, 1, num_filters]\r\n        # Combine all pooled features\r\n        num_filters_total = num_filters * len(filter_sizes)\r\n        self.h_pool = tf.concat( pooled_outputs,3)  # [None, 1, 1, num_filters_total]\r\n        self.h_pool_flat = tf.reshape(self.h_pool, shape=[-1, num_filters_total]) # [None, num_filters_total]\r\n\r\n        # The dropout layer\r\n        with tf.name_scope(""dropout""):\r\n            self.h_dropout = tf.nn.dropout(self.h_pool_flat, keep_prob=self.dropout_keep_prob, name=""dropout"")\r\n        \r\n        # The output layer (softmax)\r\n        with tf.name_scope(""output""):\r\n            self.W_fullyconn = tf.get_variable(""W_fullyconn"", shape=[num_filters_total, num_classes],\r\n                                                initializer=tf.contrib.layers.xavier_initializer())\r\n            self.b_fullyconn = tf.Variable(tf.constant(0.1, shape=[num_classes,]), dtype=tf.float32, name=""b_fullyconn"")\r\n            # L2_loss\r\n            L2_loss += tf.nn.l2_loss(self.W_fullyconn)\r\n            self.scores = tf.nn.xw_plus_b(self.h_dropout, self.W_fullyconn, self.b_fullyconn, name=""scores"")\r\n            self.preds = tf.argmax(self.scores, axis=1, name=""preds"")\r\n        \r\n        # The loss\r\n        with tf.name_scope(""loss""):\r\n            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.y)\r\n            self.loss = tf.reduce_mean(losses) + L2_loss * l2_reg_lambda\r\n        \r\n        # Accuracy\r\n        with tf.name_scope(""accuracy""):\r\n            correct_preds = tf.equal(self.preds, tf.argmax(self.y, axis=1))\r\n            self.accuracy = tf.reduce_mean(tf.cast(correct_preds, tf.float32))\r\n    \r\n    def save_weights(self, sess, filename, name=""TextRNN""):\r\n        """"""""""""\r\n        save_dicts = {name+""_W_embedding"": self.W_embedding}\r\n        for i in range(len(self.Ws_conv)):\r\n            save_dicts.update({name+""_W_conv_""+str(i): self.Ws_conv[i],\r\n                                name+""_b_conv_""+str(i): self.bs_conv[i]})\r\n        save_dicts.update({name+""_W_fullyconn"": self.W_fullyconn,\r\n                            name+""_b_fullyconn"": self.b_fullyconn})\r\n        saver = tf.train.Saver(save_dicts)\r\n        return saver.save(sess, filename)\r\n    \r\n    def load_weights(self, sess, filename, name=""TextRNN""):\r\n        """"""""""""\r\n        save_dicts = {name+""_W_embedding"": self.W_embedding}\r\n        for i in range(len(self.Ws_conv)):\r\n            save_dicts.update({name+""_W_conv_""+str(i): self.Ws_conv[i],\r\n                                name+""_b_conv_""+str(i): self.bs_conv[i]})\r\n        save_dicts.update({name+""_W_fullyconn"": self.W_fullyconn,\r\n                            name+""_b_fullyconn"": self.b_fullyconn})\r\n        saver = tf.train.Saver(save_dicts)\r\n        saver.restore(sess)'"
examples/cnn_setence_classification/train_cnn.py,3,"b'""""""\nTest the TextRNN class\n2016/12/22\n""""""\nimport os\nimport sys\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.contrib import learn\n\nfrom data_helpers import load_data_and_labels, batch_iter\nfrom text_cnn import TextCNN\nimport pudb;pu.db\n\n# Load original data\npath = sys.path[0]\npos_filename = path + ""/data/rt-polarity.pos""\nneg_filename = path + ""/data/rt-polarity.neg""\n\nX_data, y_data = load_data_and_labels(pos_filename, neg_filename)\nmax_document_length = max([len(sen.split("" "")) for sen in X_data])\nprint(""Max_document_length:,"", max_document_length)\n# Create the vacabulary\nvocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n# The idx data\nx = np.array(list(vocab_processor.fit_transform(X_data)), dtype=np.float32)\ny = np.array(y_data, dtype=np.int32)\nvocabulary_size = len(vocab_processor.vocabulary_)\nprint(""The size of vocabulary:"", vocabulary_size)\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=1111)\nprint(""X_train shape {0}, y_train shape {1}"".format(X_train.shape, y_train.shape))\nprint(""X_test shape {0}, y_test shape {1}"".format(X_test.shape, y_test.shape))\n\n# The parameters of RNN\nseq_len = X_train.shape[1]\nvocab_size = vocabulary_size\nembedding_size = 128\nfilter_sizes = [2, 3, 4]\nnum_filters = 128\nnum_classes = y_train.shape[1]\nl2_reg_lambda = 0.0\n\n# Construct RNN model\ntext_rnn_model = TextCNN(seq_len=seq_len, vocab_size=vocab_size, embedding_size=embedding_size, filter_sizes=\n                        filter_sizes, num_filters=num_filters, num_classes=num_classes)\nloss = text_rnn_model.loss\ntrain_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\naccuracy = text_rnn_model.accuracy\n# The parameters for training\nbatch_size = 64\ntraining_epochs = 10\ndispaly_every = 1\ndropout_keep_prob = 0.5\n\nbatch_num = int(X_train.shape[0]/batch_size)\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\nprint(""Starting training..."")\nfor epoch in range(training_epochs):\n    avg_cost = 0\n    for batch in range(batch_num):\n        _, cost = sess.run([train_op, loss], feed_dict={text_rnn_model.x: X_train[batch*batch_size:(batch+1)*batch_size],\n                                    text_rnn_model.y: y_train[batch*batch_size:(batch+1)*batch_size],\n                                    text_rnn_model.dropout_keep_prob:dropout_keep_prob})\n        avg_cost += cost\n    if epoch % dispaly_every == 0:\n        cost, acc = sess.run([loss, accuracy], feed_dict={text_rnn_model.x: X_test,\n                                    text_rnn_model.y: y_test,\n                                    text_rnn_model.dropout_keep_prob: 1.0})\n        print(""\\nEpoch {0} : loss {1}, accuracy {2}"".format(epoch, cost, acc))\n\n'"
examples/gan/DCGAN.py,38,"b'""""""\r\n2017/01/09\r\n""""""\r\nimport sys\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom keras.datasets import mnist\r\nfrom PIL import Image\r\n\r\n# Batch normalization\r\ndef batch_norm(inpt, epsilon=1e-05, decay=0.9, is_training=True, name=""batch_norm""):\r\n    """"""\r\n    Implements the bacth normalization\r\n    The input is 4-D tensor\r\n    """"""\r\n    bn = tf.contrib.layers.batch_norm(inpt, decay=decay, updates_collections=None,\r\n                                    epsilon=epsilon, scale=True, is_training=is_training, scope=name)\r\n    return bn\r\n\r\n# Convolution 2-D \r\ndef conv2d(inpt, nb_filter, filter_size=5, strides=2, bias=True, stddev=0.02, padding=""SAME"", \r\n            name=""conv2d""):\r\n    in_channels = inpt.get_shape().as_list()[-1]\r\n    with tf.variable_scope(name):\r\n        w = tf.get_variable(""w"", shape=[filter_size, filter_size, in_channels, nb_filter],\r\n                        initializer=tf.truncated_normal_initializer(mean=0.0, stddev=stddev))\r\n        conv = tf.nn.conv2d(inpt, w, strides=[1, strides, strides, 1], padding=padding)\r\n        if bias:\r\n            b = tf.get_variable(""b"", shape=[nb_filter,], initializer=tf.constant_initializer(0.0))\r\n            conv = tf.nn.bias_add(conv, b)\r\n        return conv\r\n\r\n# Convolution 2D Transpose\r\ndef deconv2d(inpt, output_shape, filter_size=5, strides=2, bias=True, stddev=0.02,\r\n              padding=""SAME"", name=""deconv2d""):\r\n    in_channels = inpt.get_shape().as_list()[-1]\r\n    with tf.variable_scope(name):\r\n        # Note: filter with shape [height, width, output_channels, in_channels]\r\n        w = tf.get_variable(""w"", shape=[filter_size, filter_size, output_shape[-1], in_channels],\r\n                            initializer=tf.truncated_normal_initializer(mean=0.0, stddev=stddev))\r\n        deconv = tf.nn.conv2d_transpose(inpt, w, output_shape=output_shape, strides=[1, strides, strides, 1],\r\n                                        padding=padding)\r\n        if bias:\r\n            b = tf.get_variable(""b"", shape=[output_shape[-1]], initializer=tf.constant_initializer(0.0))\r\n            deconv = tf.nn.bias_add(deconv, b)\r\n        return deconv\r\n\r\n# Leaky ReLU\r\ndef lrelu(x, leak=0.2, name=""lrelu""):\r\n    return tf.maximum(x, x*leak)\r\n\r\n# Linear \r\ndef linear(x, output_dim, stddev=0.02, name=""linear""):\r\n    input_dim = x.get_shape().as_list()[-1]\r\n    with tf.variable_scope(name):\r\n        w = tf.get_variable(""w"", shape=[input_dim, output_dim], initializer=\\\r\n                        tf.random_normal_initializer(stddev=stddev))\r\n        b = tf.get_variable(""b"", shape=[output_dim,], initializer=tf.constant_initializer(0.0))\r\n        return tf.nn.xw_plus_b(x, w, b)\r\n\r\nclass DCGAN(object):\r\n    """"""A class of DCGAN model""""""\r\n    def __init__(self, z_dim=100, output_dim=28, batch_size=100, c_dim=1, df_dim=64, gf_dim=64, dfc_dim=1024,\r\n                  n_conv=3, n_deconv=2):\r\n        """"""\r\n        :param z_dim: int, the dimension of z (the noise input of generator)\r\n        :param output_dim: int, the resolution in pixels of the images (height, width)\r\n        :param batch_size: int, the size of the mini-batch\r\n        :param c_dim: int, the dimension of image color, for minist, it is 1 (grayscale)\r\n        :param df_dim: int, the number of filters in the first convolution layer of discriminator\r\n        :param gf_dim: int, the number of filters in the penultimate deconvolution layer of generator (last is 1)\r\n        :param dfc_dim: int, the number of units in the penultimate fully-connected layer of discriminator (last is 1)\r\n        :param n_conv: int, number of convolution layer in discriminator (the number of filters is double increased)\r\n        :param n_deconv: int, number of deconvolution layer in generator (the number of filters is double reduced)\r\n        """"""\r\n        self.z_dim = z_dim\r\n        self.output_dim = output_dim\r\n        self.c_dim = c_dim\r\n        self.df_dim = df_dim\r\n        self.gf_dim = gf_dim\r\n        self.dfc_dim = dfc_dim\r\n        self.n_conv = n_conv\r\n        self.n_deconv = n_deconv\r\n        self.batch_size = batch_size\r\n\r\n        self._build_model()\r\n    \r\n    def _build_model(self):\r\n        # input \r\n        self.z = tf.placeholder(tf.float32, shape=[self.batch_size, self.z_dim])\r\n        self.x = tf.placeholder(tf.float32, shape=[self.batch_size, self.output_dim, \r\n                                                    self.output_dim, self.c_dim])\r\n        \r\n        # G\r\n        self.G = self._generator(self.z)\r\n        # D\r\n        self.D1, d1_logits = self._discriminator(self.x, reuse=False)\r\n        self.D2, d2_logits = self._discriminator(self.G, reuse=True)\r\n\r\n        self.g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(d2_logits, tf.ones_like(self.D2)))\r\n        real_loss = tf.nn.sigmoid_cross_entropy_with_logits(d1_logits, tf.ones_like(self.D1))\r\n        fake_loss = tf.nn.sigmoid_cross_entropy_with_logits(d2_logits, tf.zeros_like(self.D2))\r\n        self.d_loss = tf.reduce_mean(real_loss + fake_loss)\r\n\r\n        vars = tf.trainable_variables()\r\n        self.d_vars = [v for v in vars if ""D"" in v.name]\r\n        self.g_vars = [v for v in vars if ""G"" in v.name]\r\n\r\n    def _discriminator(self, input, reuse=False):\r\n        with tf.variable_scope(""D"", reuse=reuse):\r\n            h = lrelu(conv2d(input, nb_filter=self.df_dim, name=""d_conv0""))\r\n            for i in range(1, self.n_conv):\r\n                conv = conv2d(h, nb_filter=self.df_dim*(2**i), name=""d_conv{0}"".format(i))\r\n                h = lrelu(batch_norm(conv, name=""d_bn{0}"".format(i)))\r\n            h = linear(tf.reshape(h, shape=[self.batch_size, -1]), self.dfc_dim, name=""d_lin0"")\r\n            h = linear(tf.nn.tanh(h), 1, name=""d_lin1"")\r\n            return tf.nn.sigmoid(h), h\r\n\r\n    def _generator(self, input):\r\n        with tf.variable_scope(""G""):\r\n            nb_fliters = [self.gf_dim]\r\n            f_size = [self.output_dim//2]\r\n            for i in range(1, self.n_deconv):\r\n                nb_fliters.append(nb_fliters[-1]*2)\r\n                f_size.append(f_size[-1]//2)\r\n    \r\n            h = linear(input, nb_fliters[-1]*f_size[-1]*f_size[-1], name=""g_lin0"")\r\n            h = tf.nn.relu(batch_norm(tf.reshape(h, shape=[-1, f_size[-1], f_size[-1], nb_fliters[-1]]),\r\n                            name=""g_bn0""))\r\n            for i in range(1, self.n_deconv):\r\n                h = deconv2d(h, [self.batch_size, f_size[-i-1], f_size[-i-1], nb_fliters[-i-1]], \r\n                                    name=""g_deconv{0}"".format(i-1))\r\n                h = tf.nn.relu(batch_norm(h, name=""g_bn{0}"".format(i)))\r\n            \r\n            h = deconv2d(h, [self.batch_size, self.output_dim, self.output_dim, self.c_dim], \r\n                            name=""g_deconv{0}"".format(self.n_deconv-1))\r\n            return tf.nn.tanh(h)\r\n\r\ndef combine_images(images):\r\n    """"""Combine the bacth images""""""\r\n    num = images.shape[0]\r\n    width = int(np.sqrt(num))\r\n    height = int(np.ceil(num/width))\r\n    h, w = images.shape[1:-1]\r\n    img = np.zeros((height*h, width*w), dtype=images.dtype)\r\n    for index, m in enumerate(images):\r\n        i = int(index/width)\r\n        j = index % width\r\n        img[i*h:(i+1)*h, j*w:(j+1)*w] = m[:, :, 0]\r\n    return img\r\n\r\nif __name__ == ""__main__"":\r\n    # Load minist data\r\n    (X_train, y_train), (X_test, y_test) = mnist.load_data()\r\n    X_train = (np.asarray(X_train, dtype=np.float32) - 127.5)/127.5\r\n    X_train = np.reshape(X_train, [-1, 28, 28, 1])\r\n\r\n    z_dim = 100\r\n    batch_size = 128\r\n    lr = 0.0002\r\n    n_epochs = 10\r\n\r\n    sess = tf.Session()\r\n    dcgan = DCGAN(z_dim=z_dim, output_dim=28, batch_size=128, c_dim=1)\r\n    # The optimizers\r\n    d_train_op = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(dcgan.d_loss, \r\n                                                var_list=dcgan.d_vars)\r\n    g_train_op = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(dcgan.g_loss,\r\n                                                var_list=dcgan.g_vars)\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    num_batches = int(len(X_train)/batch_size)\r\n    for epoch in range(n_epochs):\r\n        print(""Epoch"", epoch)\r\n        d_losses = 0\r\n        g_losses = 0\r\n        for idx in range(num_batches):\r\n            # Train D\r\n            z = np.random.uniform(-1, 1, size=[batch_size, z_dim])\r\n            x = X_train[idx*batch_size:(idx+1)*batch_size]\r\n            _, d_loss = sess.run([d_train_op, dcgan.d_loss], feed_dict={dcgan.z: z,\r\n                                                        dcgan.x: x})\r\n            d_losses += d_loss/num_batches\r\n            # Train G\r\n            z = np.random.uniform(-1, 1, size=[batch_size, z_dim])\r\n            _, g_loss = sess.run([g_train_op, dcgan.g_loss], feed_dict={dcgan.z: z})\r\n            g_losses += g_loss/num_batches\r\n        \r\n        print(""\\td_loss {0}, g_loss {1}"".format(d_losses, g_losses))\r\n        # Generate images\r\n        z = np.random.uniform(-1, 1, size=[batch_size, z_dim])\r\n        images = sess.run(dcgan.G, feed_dict={dcgan.z: z})\r\n        img = combine_images(images)\r\n        img = img*127.5 + 127.5\r\n        Image.fromarray(img.astype(np.uint8)).save(""epoch{0}_g_images.png"".format(epoch))\r\n\r\n\r\n\r\n        '"
examples/gan/GAN_simple.py,38,"b'""""""\r\nA simple generative adversarial networks (GAN)\r\nsource: https://github.com/AYLIEN/gan-intro/blob/master/gan.py\r\n2017/01/12\r\n""""""\r\nimport numpy as np\r\nfrom scipy.stats import norm\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\n\r\n# Set random seed to reproduce\r\nseed = 24\r\nnp.random.seed(seed)\r\ntf.set_random_seed(seed)\r\n\r\nclass NormDistribution(object):\r\n    """""" 1-D Guassian Distribution""""""\r\n    def __init__(self, mu=-1, sigma=1):\r\n        self.mu = mu\r\n        self.sigma = sigma\r\n    \r\n    def sample(self, n):\r\n        """"""\r\n        Sample form the norm distribution\r\n        :param n: int, the number of samples\r\n        """"""\r\n        samples = np.random.normal(loc=self.mu, scale=self.sigma, size=[n,])\r\n        samples.sort()   # stratified sampling by sorting the samples\r\n        return samples\r\n\r\nclass NoiseInput(object):\r\n    """"""\r\n    The nosie input `z` for the generator. \r\n    """"""\r\n    def __init__(self, scope):\r\n        """"""\r\n        :param scope: int, `z` are generated in the range of [-scope, scope]\r\n        """"""\r\n        self.scope = scope\r\n    \r\n    def sample(self, n):\r\n        """"""\r\n        Sample form the noise input, the samples are sorted with some noise.\r\n        :param n: int, the number of samples\r\n        """"""\r\n        return np.linspace(-self.scope, self.scope, n) + np.random.random(n)*0.01\r\n\r\n# linear layer\r\ndef linear(input, output_dim, stddev=1.0, scope=""linear""):\r\n    input_dim = input.get_shape().as_list()[-1]\r\n    with tf.variable_scope(scope):\r\n        w = tf.get_variable(""w"", shape=[input_dim, output_dim], initializer=\r\n                                tf.random_normal_initializer(mean=0.0, stddev=stddev))\r\n        b = tf.get_variable(""b"", shape=[output_dim,], initializer=tf.constant_initializer(0.0))\r\n        return tf.nn.xw_plus_b(input, w, b)\r\n\r\n# Minibatch for discriminator\r\ndef minibatch(input, num_kernels=5, kernel_dim=3):\r\n    """"""\r\n    The minibatch method for the discriminator\r\n    """"""\r\n    x = linear(input, num_kernels*kernel_dim, stddev=0.02, scope=""minibatch"")\r\n    activation = tf.reshape(x, shape=[-1, num_kernels, kernel_dim])\r\n    # Compute the L1 distance over rows\r\n    diffs = tf.expand_dims(activation, -1) - tf.expand_dims(tf.transpose(activation, [1, 2, 0]), 0)\r\n    abs_diffs = tf.reduce_sum(tf.abs(diffs), axis=2)\r\n    minibatch_features = tf.reduce_sum(tf.exp(-abs_diffs), axis=2)\r\n    return tf.concat(1, [x, minibatch_features])\r\n\r\nclass Generator(object):\r\n    """"""A class of generator""""""\r\n    def __init__(self, hidden_size):\r\n        self.hidden_size = hidden_size\r\n\r\n    def __call__(self, input):\r\n        """"""We only use two layers""""""\r\n        h0 = tf.nn.softplus(linear(input, self.hidden_size, scope=""g0""))\r\n        h1 = tf.tanh(linear(h0, 1, scope=""g1""))\r\n        return h1\r\n\r\nclass Discriminator(object):\r\n    """"""A class of discriminator""""""\r\n    def __init__(self, hidden_size, minibatch_layer=True):\r\n        self.hidden_size = hidden_size\r\n        self.minibatch_layer = minibatch_layer\r\n    \r\n    def __call__(self, input):\r\n        """"""We use more hidden layers""""""\r\n        h0 = tf.tanh(linear(input, self.hidden_size*2, scope=""d0""))\r\n        h1 = tf.tanh(linear(h0, self.hidden_size*2, scope=""d1""))\r\n        # We add a layer if you don not use minibatch method\r\n        if self.minibatch_layer:\r\n            h2 = minibatch(h1)\r\n        else:\r\n            h2 = tf.tanh(linear(h1, self.hidden_size*2, scope=""d2""))\r\n        h3 = tf.sigmoid(linear(h2, 1, scope=""d3""))\r\n        return h3\r\n\r\ndef optimizer(loss, var_list, init_lr):\r\n    decay = 0.95\r\n    num_decay_steps = 150\r\n    global_step = tf.Variable(0, trainable=False)\r\n    lr = tf.train.exponential_decay(init_lr, global_step, num_decay_steps, decay, \r\n                                    staircase=True)\r\n    train_op = tf.train.GradientDescentOptimizer(lr).minimize(loss, global_step=global_step,\r\n                                                                var_list=var_list)\r\n    return train_op\r\n\r\nclass GAN(object):\r\n    """"""A simple generative adversarial network to train 1-D norm distribution""""""\r\n    def __init__(self, data, z_data, hidden_size=4, is_minibatch=True):\r\n        """"""\r\n        :param data: a object to generate the true data distribution\r\n        :param z_data: a object to generate nosie input for Generator\r\n        :param hidden_size: int, the number of units in mlp\r\n        :param is_minibatch: bool, if use minibatch method in discriminator\r\n        """"""\r\n        self.data = data\r\n        self.z_data = z_data\r\n        self.hidden_size = hidden_size\r\n        self.is_minibatch = is_minibatch\r\n        if is_minibatch:\r\n            self.lr = 0.005\r\n        else:\r\n            self.lr = 0.03\r\n        self._bulid_model()\r\n    \r\n    def _bulid_model(self):\r\n        """"""The inner function to build the model""""""\r\n        # Pretrain the discriminator is helpful to GAN\r\n        with tf.variable_scope(""D_pre""):\r\n            self.pre_x = tf.placeholder(tf.float32, shape=[None, 1])\r\n            self.pre_y = tf.placeholder(tf.float32, shape=[None, 1])\r\n            D_pre = Discriminator(self.hidden_size, self.is_minibatch)\r\n            y = D_pre(self.pre_x)\r\n            # Use mse loss\r\n            self.pre_loss = tf.reduce_mean(tf.square(y - self.pre_y))\r\n\r\n        # Generator model\r\n        with tf.variable_scope(""G""):\r\n            self.z = tf.placeholder(tf.float32, shape=[None, 1])\r\n            G = Generator(self.hidden_size)(self.z)\r\n            self.G = tf.mul(G, self.z_data.scope)\r\n            #self.G = tf.clip_by_value(self.G, 0.01, 0.999)\r\n        \r\n        # Discriminator model\r\n        with tf.variable_scope(""D"") as scope:\r\n            self.x = tf.placeholder(tf.float32, shape=[None, 1])\r\n            self.D1 = Discriminator(self.hidden_size, self.is_minibatch)(self.x)\r\n            #self.D1 = tf.clip_by_value(self.D1, 0.01, 0.99)\r\n            # Reuse the model\r\n            scope.reuse_variables()\r\n            self.D2 = Discriminator(self.hidden_size, self.is_minibatch)(self.G)\r\n            #self.D2 = tf.clip_by_value(self.D2, 0.01, 0.999)\r\n\r\n        # Compute the loss\r\n        self.d_loss = -tf.reduce_mean(tf.log(self.D1) + tf.log(1.0 - self.D2))\r\n        self.g_loss = -tf.reduce_mean(tf.log(self.D2))\r\n\r\n        # Get the trainable vars for each model\r\n        vars = tf.trainable_variables()\r\n        self.d_pre_vars = sorted([v for v in vars if v.name.startswith(""D_pre/"")], key=lambda v: v.name)\r\n        self.d_vars = sorted([v for v in vars if v.name.startswith(""D/"")], key=lambda v: v.name)\r\n        self.g_vars = [v for v in vars if v.name.startswith(""G/"")]\r\n\r\n        # Train_ops\r\n        self.d_pre_train_op = optimizer(self.pre_loss, self.d_pre_vars, self.lr)\r\n        self.d_train_op = optimizer(self.d_loss, self.d_vars, self.lr)\r\n        self.g_train_op = optimizer(self.g_loss, self.g_vars, self.lr)\r\n\r\n    def pretrain_discriminator(self, sess, batch_size=20, n_epochs=1000, display_every=50):\r\n        """"""Pretrain the discriminator""""""\r\n        losses = []\r\n        for epoch in range(n_epochs):\r\n            x = (np.random.random(batch_size) - 0.5)*10.0\r\n            y = norm.pdf(x, loc=self.data.mu, scale=self.data.sigma)\r\n            loss, _ = sess.run([self.pre_loss, self.d_pre_train_op], feed_dict={self.pre_x: np.reshape(x, [-1, 1]), \r\n                                                                     self.pre_y: np.reshape(y, [-1, 1])})\r\n            losses.append(loss)\r\n            if epoch % display_every == 0:\r\n                print(""Epoch {0}, pretrain loss: {1}"".format(epoch, loss))\r\n\r\n        pretrain_vars = sess.run(self.d_pre_vars)\r\n        for pre_v, v in zip(pretrain_vars, self.d_vars):\r\n            sess.run(tf.assign(v, pre_v))\r\n\r\n        # Plot the losses\r\n        f, ax = plt.subplots(1)\r\n        ax.plot(np.arange(n_epochs), np.array(losses))\r\n        plt.xlabel(""Epoch"")\r\n        plt.ylabel(""Loss"")\r\n        plt.title(""Pretrain losses"")\r\n        plt.show()\r\n\r\n    def train(self, sess, batch_size=20, n_epochs=100, d_k=1, display_every=10):\r\n        """"""Train GAN""""""\r\n\r\n        for epoch in range(n_epochs):\r\n            # train D\r\n            d_losses = 0.0\r\n            for i in range(d_k):\r\n                x = np.reshape(self.data.sample(batch_size), [-1, 1])\r\n                z = np.reshape(self.z_data.sample(batch_size), [-1, 1])\r\n                d_loss, _ = sess.run([self.d_loss, self.d_train_op], feed_dict={self.x: x,\r\n                                        self.z: z})\r\n                d_losses += d_loss/d_k\r\n            # train G\r\n            z = np.reshape(self.z_data.sample(batch_size), [-1, 1])\r\n            g_loss, _ = sess.run([self.g_loss, self.g_train_op], feed_dict={self.z: z})\r\n            if epoch % display_every == 0:\r\n                print(""Epoch {0}, d_loss {1}, g_loss {2}"".format(epoch, d_losses, g_loss)) \r\n\r\n    def _sample(self, sess, batch_size =20, num_points=10000, num_bins=100):\r\n        """"""Sampler""""""\r\n        # Decision boundary given by Discriminator\r\n        xs = np.linspace(-self.z_data.scope, self.z_data.scope, num_points)\r\n        dbs = np.zeros((num_points,))\r\n        for i in range(num_points // batch_size):\r\n            x = np.reshape(xs[i*batch_size:(i+1)*batch_size], [-1, 1])\r\n            db = sess.run(self.D1, feed_dict={self.x: x})\r\n            dbs[i*batch_size:(i+1)*batch_size] = np.reshape(db, [-1])\r\n        \r\n        # True data distribution\r\n        bins = np.linspace(-self.z_data.scope, self.z_data.scope, num_bins)\r\n        d = self.data.sample(num_points)\r\n        pds, _ = np.histogram(d, bins=bins, density=True)\r\n        \r\n        # The generated distribution\r\n        zs = np.linspace(-self.z_data.scope, self.z_data.scope, num_points)\r\n        gds = np.zeros((num_points))\r\n        for i in range(num_points // batch_size):\r\n            z = np.reshape(zs[i*batch_size:(i+1)*batch_size], [-1, 1])\r\n            gd = sess.run(self.G, feed_dict={self.z: z})\r\n            gds[i*batch_size:(i+1)*batch_size] = np.reshape(gd, [-1])\r\n        \r\n        gds, _ = np.histogram(gds, bins=bins, density=True)\r\n    \r\n        return (dbs, pds, gds)\r\n\r\n    def ploter(self, sess, num_points=10000, num_bins=100):\r\n        """"""Plot decision boundary, true data distribution, \r\n        generated distribution""""""\r\n        dbs, pds, gds = self._sample(sess, batch_size =20, num_points=num_points,\r\n                                     num_bins=num_bins)\r\n        f, ax = plt.subplots(1)\r\n        x1 = np.linspace(-self.z_data.scope, self.z_data.scope, len(dbs))\r\n        x2 = np.linspace(-self.z_data.scope, self.z_data.scope, len(pds))\r\n        ax.plot(x1, dbs, label=""Decision boundary"")\r\n        ax.plot(x2, pds, label=""Data"")\r\n        ax.plot(x2, gds, label=""G_data"")\r\n        ax.set_ylim(0, 1.2)\r\n        plt.title(""1-D Norm Distribution"")\r\n        plt.xlabel(""Random variable"")\r\n        plt.ylabel(""Probability density"")\r\n        plt.legend()\r\n        plt.show()\r\n\r\nif __name__ == ""__main__"":\r\n    sess = tf.Session()\r\n    \r\n    gan = GAN(NormDistribution(-1, 1), NoiseInput(5), hidden_size=4, is_minibatch=False)\r\n    sess.run(tf.global_variables_initializer())\r\n    gan.pretrain_discriminator(sess, batch_size=12, n_epochs=1000)\r\n    gan.ploter(sess)\r\n    gan.train(sess, batch_size=10, n_epochs=1000, d_k=1, display_every=10)\r\n    gan.ploter(sess)\r\n\r\n\r\n\r\n    \r\n    '"
examples/lstm_model_ptb/ptb_lstm_model.py,35,"b'""""""\r\nA lstm model for PTB data\r\nsource: ""https://github.com/xiaohu2015/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py""\r\n2016/12/25\r\n""""""\r\nimport sys\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom reader import ptb_raw_data, ptb_iterator\r\n\r\nclass LSTM_Model(object):\r\n    """"""\r\n    A LSTM class for language model on PTB data\r\n    """"""\r\n    def __init__(self, num_steps=20, vocab_size=10000, batch_size=20, hidden_size=1500, num_lstm_layers=2,\r\n                keep_prob=0.5, max_grad_norm=5, is_training=True):\r\n        """"""\r\n        :param num_steps: int, the number of time steps (also sequence length)\r\n        :param vocab_size: int, vocabulary size\r\n        :param batch_size: int, batch size, you can also not give the batch_size\r\n        :param hidden_size: int, the number of hidden units in lstm\r\n        :param num_lstm_layers: int, the number of lstm layers\r\n        :param keep_prob: float, the keep probability of dropout layer\r\n        :param max_grad_norm: int, regularize gradients by norm\r\n        :param is_training: bool, set True for training model, but False for test model\r\n                            Note we construct three models with shared weight variables\r\n        """"""\r\n        # Keep all parameters\r\n        self.num_steps = num_steps\r\n        self.vocab_size = vocab_size\r\n        self.hidden_size = hidden_size\r\n        self.num_lstm_layers = num_lstm_layers\r\n        self.batch_size = batch_size\r\n        self.max_grad_norm = max_grad_norm\r\n        self.is_training = is_training\r\n\r\n        # The input and output \r\n        self._x = tf.placeholder(tf.int32, shape=[batch_size, num_steps])\r\n        self._y = tf.placeholder(tf.int32, shape=[batch_size, num_steps])\r\n        if batch_size is None:\r\n            batch_size = tf.shape(self._x)[0]\r\n        \r\n        # Construct lstm cell\r\n        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=0.0)\r\n        if is_training and keep_prob < 1.0:  # use dropout\r\n            lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\r\n        cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell]*num_lstm_layers)\r\n        # The initial state\r\n        self._initial_state = cell.zero_state(batch_size, tf.float32)\r\n\r\n        # The embedding layer\r\n        with tf.device(""/cpu:0""):\r\n            embedding = tf.get_variable(""embedding"", shape=[vocab_size, hidden_size])\r\n            inputs = tf.nn.embedding_lookup(embedding, self._x)  # [batch_size, num_steps, hidden_size]\r\n        # Dropout\r\n        if is_training and keep_prob < 1.0:\r\n            inputs = tf.nn.dropout(inputs, keep_prob=keep_prob)\r\n        \r\n        # The lstm layer\r\n        # Note: we compute the outputs by unrolling the lstm\r\n        # you can also use tf.nn.rnn to simplify the following codes\r\n        """"""\r\n        inputs = tf.unstack(inputs, num_steps, axis=1) # list of [batch_size, hidden_size] from time step 0 to the end\r\n        outputs, state = tf.nn.rnn(cell, inputs, initial_state=self._initial_state)\r\n        """"""\r\n        outputs = []\r\n        state = self._initial_state\r\n        with tf.variable_scope(""RNN""):\r\n            for time_step in range(num_steps):\r\n                # Note: we use shared variables (The cell creates the variables when it firstly starts running)\r\n                if time_step > 0: tf.get_variable_scope().reuse_variables()\r\n                cell_output, state = cell(inputs[:, time_step, :], state)\r\n                outputs.append(cell_output)\r\n        # Reshape\r\n        output = tf.reshape(tf.concat(1, outputs), [-1, hidden_size])  # [num_steps*batch_size, hidden_size]\r\n        # The softmax layer\r\n        softmax_W = tf.get_variable(""softmax_W"", shape=[hidden_size, vocab_size], dtype=tf.float32)\r\n        softmax_b = tf.get_variable(""softmax_b"", shape=[vocab_size, ], dtype=tf.float32)\r\n        logits = tf.matmul(output, softmax_W) + softmax_b\r\n        # The loss\r\n        loss = tf.nn.seq2seq.sequence_loss_by_example([logits], [tf.reshape(self._y, [-1,])], \r\n                                                        weights=[tf.ones(batch_size*num_steps)])\r\n        self._cost = tf.reduce_sum(loss) / tf.to_float(batch_size)\r\n        self._final_state = state\r\n\r\n        if not is_training:\r\n            return\r\n        \r\n        # The training operations\r\n        # learning rate\r\n        self._lr = tf.Variable(0.0, trainable=False)\r\n        tvars = tf.trainable_variables()  # The variables for training\r\n        grads, _ = tf.clip_by_global_norm(tf.gradients(self._cost, tvars), max_grad_norm)\r\n        self._train_op = tf.train.GradientDescentOptimizer(self._lr).apply_gradients(zip(grads, tvars))\r\n        \r\n    def assign_lr(self, sess, lr_value):\r\n        if self.is_training:\r\n            sess.run(tf.assign(self._lr, lr_value))\r\n    \r\n    \r\n    @property\r\n    def input(self):\r\n        return self._x\r\n\r\n    @property\r\n    def target(self):\r\n        return self._y\r\n    \r\n    @property\r\n    def initial_state(self):\r\n        return self._initial_state\r\n    \r\n    @property\r\n    def final_state(self):\r\n        return self._final_state\r\n    \r\n    @property\r\n    def lr(self):\r\n        return self._lr\r\n\r\n    @property\r\n    def cost(self):\r\n        return self._cost\r\n    \r\n    @property\r\n    def train_op(self):\r\n        return self._train_op\r\n    \r\n\r\nclass SmallConfig(object):\r\n    """"""Small config.""""""\r\n    init_scale = 0.1\r\n    learning_rate = 1.0\r\n    max_grad_norm = 5\r\n    num_layers = 2\r\n    num_steps = 20\r\n    hidden_size = 200\r\n    max_epoch = 4\r\n    max_max_epoch = 13\r\n    keep_prob = 1.0\r\n    lr_decay = 0.5\r\n    batch_size = 20\r\n    vocab_size = 10000\r\n\r\n\r\nclass MediumConfig(object):\r\n    """"""Medium config.""""""\r\n    init_scale = 0.05\r\n    learning_rate = 1.0\r\n    max_grad_norm = 5\r\n    num_layers = 2\r\n    num_steps = 35\r\n    hidden_size = 650\r\n    max_epoch = 6\r\n    max_max_epoch = 39\r\n    keep_prob = 0.5\r\n    lr_decay = 0.8\r\n    batch_size = 20\r\n    vocab_size = 10000\r\n\r\n\r\nclass LargeConfig(object):\r\n    """"""Large config.""""""\r\n    init_scale = 0.04\r\n    learning_rate = 1.0\r\n    max_grad_norm = 10\r\n    num_layers = 2\r\n    num_steps = 35\r\n    hidden_size = 1500\r\n    max_epoch = 14\r\n    max_max_epoch = 55\r\n    keep_prob = 0.35\r\n    lr_decay = 1 / 1.15\r\n    batch_size = 20\r\n    vocab_size = 10000\r\n\r\n\r\nclass TestConfig(object):\r\n    """"""Tiny config, for testing.""""""\r\n    init_scale = 0.1\r\n    learning_rate = 1.0\r\n    max_grad_norm = 1\r\n    num_layers = 1\r\n    num_steps = 2\r\n    hidden_size = 2\r\n    max_epoch = 1\r\n    max_max_epoch = 1\r\n    keep_prob = 1.0\r\n    lr_decay = 0.5\r\n    batch_size = 20\r\n    vocab_size = 10000\r\n\r\ndef model_run_epoch(sess, model, data, eval_op, verbose=True):\r\n    """"""Runs the model for one epoch on the given data""""""\r\n    epoch_size = ((len(data)// model.batch_size) - 1) // model.num_steps\r\n    start_time = time.time()\r\n    costs = 0.0\r\n    iters = 0\r\n    state = sess.run(model.initial_state)\r\n    for step, (x, y) in enumerate(ptb_iterator(data, model.batch_size, model.num_steps)):\r\n        feed_dict = {model.input: x, model.target: y, model.initial_state: state}\r\n        cost, state, _ = sess.run([model.cost, model.final_state, eval_op],\r\n                                    feed_dict=feed_dict)\r\n        costs += cost\r\n        iters += model.num_steps\r\n        if verbose and step % (epoch_size // 10) == 10:\r\n            print(""%.3f perplexity: %.3f speed: %.0f wps"" %\r\n            (step * 1.0 / epoch_size, np.exp(costs / iters), iters * m.batch_size / (time.time() - start_time)))\r\n    return np.exp(costs/iters)\r\n\r\nif __name__ == ""__main__"":\r\n    # Load the PTB data\r\n    data_path = sys.path[0] + ""/data/""\r\n    train_data, valid_data, test_data, vocab= ptb_raw_data(data_path=data_path)\r\n    print(len(train_data), len(valid_data), len(test_data), vocab)\r\n    # Configs\r\n    config = LargeConfig()\r\n    eval_config = LargeConfig()\r\n    eval_config.batch_size = 1\r\n    eval_config.num_steps = 1\r\n\r\n    with tf.Graph().as_default(), tf.Session() as sess:\r\n        initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\r\n        with tf.variable_scope(""model"", reuse=None, initializer=initializer):\r\n            model = LSTM_Model(num_steps=config.num_steps, vocab_size=config.vocab_size, batch_size=\r\n                                config.batch_size, hidden_size=config.hidden_size, num_lstm_layers=config.num_layers,\r\n                                keep_prob=config.keep_prob, max_grad_norm=config.max_grad_norm, is_training=True)\r\n        with tf.variable_scope(""model"", reuse=True, initializer=initializer):\r\n            val_model = LSTM_Model(num_steps=config.num_steps, vocab_size=config.vocab_size, batch_size=\r\n                                config.batch_size, hidden_size=config.hidden_size, num_lstm_layers=config.num_layers,\r\n                                keep_prob=config.keep_prob, max_grad_norm=config.max_grad_norm, is_training=False)\r\n            test_model = LSTM_Model(num_steps=eval_config.num_steps, vocab_size=eval_config.vocab_size, batch_size=\r\n                                eval_config.batch_size, hidden_size=eval_config.hidden_size, num_lstm_layers=eval_config.num_layers,\r\n                                keep_prob=eval_config.keep_prob, max_grad_norm=eval_config.max_grad_norm, is_training=False)\r\n        \r\n        sess.run(tf.global_variables_initializer())\r\n        for i in range(config.max_max_epoch):\r\n            lr_decay = config.lr_decay ** max(i - config.max_epoch, 0.0)\r\n            model.assign_lr(sess, config.learning_rate * lr_decay)\r\n\r\n            print(""Epoch: %d Learning rate: %.3f"" % (i + 1, sess.run(model.lr)))\r\n            train_perplexity = model_run_epoch(sess, model, train_data, model.train_op,\r\n                                   verbose=True)\r\n            print(""Epoch: %d Train Perplexity: %.3f"" % (i + 1, train_perplexity))\r\n            valid_perplexity = model_run_epoch(sess, val_model, valid_data, tf.no_op())\r\n            print(""Epoch: %d Valid Perplexity: %.3f"" % (i + 1, valid_perplexity))\r\n\r\n        test_perplexity = model_run_epoch(session, test_model, test_data, tf.no_op())\r\n        print(""Test Perplexity: %.3f"" % test_perplexity)\r\n\r\n\r\n\r\n'"
examples/lstm_model_ptb/reader.py,1,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\n""""""Utilities for parsing PTB text files.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef _read_words(filename):\n  with tf.gfile.GFile(filename, ""r"") as f:\n    return f.read().decode(""utf-8"").replace(""\\n"", ""<eos>"").split()\n\n\ndef _build_vocab(filename):\n  data = _read_words(filename)\n  counter = collections.Counter(data)\n  count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n\n  words, _ = list(zip(*count_pairs))\n  word_to_id = dict(zip(words, range(len(words))))\n\n  return word_to_id\n\n\ndef _file_to_word_ids(filename, word_to_id):\n  data = _read_words(filename)\n  return [word_to_id[word] for word in data]\n\n\ndef ptb_raw_data(data_path=None):\n  """"""Load PTB raw data from data directory ""data_path"".\n  Reads PTB text files, converts strings to integer ids,\n  and performs mini-batching of the inputs.\n  The PTB dataset comes from Tomas Mikolov\'s webpage:\n  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n  Args:\n    data_path: string path to the directory where simple-examples.tgz has\n      been extracted.\n  Returns:\n    tuple (train_data, valid_data, test_data, vocabulary)\n    where each of the data objects can be passed to PTBIterator.\n  """"""\n\n  train_path = os.path.join(data_path, ""ptb.train.txt"")\n  valid_path = os.path.join(data_path, ""ptb.valid.txt"")\n  test_path = os.path.join(data_path, ""ptb.test.txt"")\n\n  word_to_id = _build_vocab(train_path)\n  train_data = _file_to_word_ids(train_path, word_to_id)\n  valid_data = _file_to_word_ids(valid_path, word_to_id)\n  test_data = _file_to_word_ids(test_path, word_to_id)\n  vocabulary = len(word_to_id)\n  return train_data, valid_data, test_data, vocabulary\n\n\ndef ptb_iterator(raw_data, batch_size, num_steps):\n  """"""Iterate on the raw PTB data.\n  This generates batch_size pointers into the raw PTB data, and allows\n  minibatch iteration along these pointers.\n  Args:\n    raw_data: one of the raw data outputs from ptb_raw_data.\n    batch_size: int, the batch size.\n    num_steps: int, the number of unrolls.\n  Yields:\n    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\n    The second element of the tuple is the same data time-shifted to the\n    right by one.\n  Raises:\n    ValueError: if batch_size or num_steps are too high.\n  """"""\n  raw_data = np.array(raw_data, dtype=np.int32)\n\n  data_len = len(raw_data)\n  batch_len = data_len // batch_size\n  data = np.zeros([batch_size, batch_len], dtype=np.int32)\n  for i in range(batch_size):\n    data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\n\n  epoch_size = (batch_len - 1) // num_steps\n\n  if epoch_size == 0:\n    raise ValueError(""epoch_size == 0, decrease batch_size or num_steps"")\n\n  for i in range(epoch_size):\n    x = data[:, i*num_steps:(i+1)*num_steps]\n    y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n    yield (x, y)'"
examples/lstm_time_series_regression/lstm_regression.py,31,"b'""""""\r\nLSTM Model for Time Series Prediction/Regression\r\nsource: \'https://github.com/MorvanZhou/tutorials/blob/master/tensorflowTUT/tf20_RNN2.2/full_code.py\'\r\n2017/01/03\r\n""""""\r\nimport sys\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\n\r\ndef batch_iterate(num_batchs, batch_size, num_steps):\r\n    """"""\r\n    Generate the mini batch about sin and cos function\r\n    """"""\r\n    start = 0\r\n    for i in range(num_batchs):\r\n        xo = np.arange(start, start+batch_size*num_steps).reshape(\r\n                            [batch_size, num_steps])/(10.0*np.pi)\r\n        x = np.sin(xo)\r\n        y = np.cos(xo)\r\n        start += num_steps\r\n        yield (x[:, :, np.newaxis], y[:, :, np.newaxis], xo)\r\n\r\nclass LstmRegression(object):\r\n    """"""\r\n    A lstm class for time series prediction\r\n    """"""\r\n    def __init__(self, in_size, out_size, num_steps=20, cell_size=20, batch_size=50,\r\n                    num_lstm_layers=2, keep_prob=0.5, is_training=True):\r\n        """"""\r\n        :param in_size: int, the dimension of input\r\n        :param out_size: int, the dimension of output\r\n        :param num_steps: int, the number of time steps\r\n        :param cell_size: int, the size of lstm cell\r\n        :param batch_size: int, the size of mini bacth\r\n        :param num_lstm_layers: int, the number of lstm cells\r\n        :param keep_prob: float, the keep probability of dropout layer\r\n        :param is_training: bool, set True for training model, but False for test model\r\n        """"""\r\n        self.in_size = in_size\r\n        self.out_size = out_size\r\n        self.num_steps = num_steps\r\n        self.cell_size = cell_size\r\n        self.batch_size = batch_size\r\n        self.num_lstm_layers = num_lstm_layers\r\n        self.keep_prob = keep_prob\r\n        self.is_training = is_training\r\n        self.__build_model__()\r\n\r\n    def __build_model__(self):\r\n        """"""\r\n        The inner method to construct the lstm model.\r\n        """"""\r\n        # Input and output placeholders\r\n        self.x = tf.placeholder(tf.float32, shape=[None, self.num_steps, self.in_size])\r\n        self.y = tf.placeholder(tf.float32, shape=[None, self.num_steps, self.out_size])\r\n\r\n        # Add the first input layer\r\n        with tf.variable_scope(""input""):\r\n            # Reshape x to 2-D tensor\r\n            inputs = tf.reshape(self.x, shape=[-1, self.in_size])  #[batch_size*num_steps, in_size]\r\n            W, b = self._get_weight_bias(self.in_size, self.cell_size)\r\n            inputs = tf.nn.xw_plus_b(inputs, W, b, name=""input_xW_plus_b"")\r\n        # Reshep to 3-D tensor\r\n        inputs = tf.reshape(inputs, shape=[-1, self.num_steps, self.cell_size]) #[batch_size, num_steps, in_size]\r\n\r\n        # Dropout the inputs\r\n        if self.is_training and self.keep_prob < 1.0:\r\n            inputs = tf.nn.dropout(inputs, keep_prob=self.keep_prob)\r\n        \r\n        # Construct lstm cells\r\n        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(self.cell_size, forget_bias=1.0, state_is_tuple=True)\r\n        if self.is_training and self.keep_prob < 1.0:\r\n            lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=self.keep_prob)\r\n        cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell]*self.num_lstm_layers)\r\n        # The initial state\r\n        self.init_state = cell.zero_state(self.batch_size, dtype=tf.float32)\r\n\r\n        # Add the lstm layer\r\n        with tf.variable_scope(""LSTM""):\r\n            outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=self.init_state)\r\n        self.final_state = final_state\r\n\r\n        # Add the output layer\r\n        with tf.variable_scope(""output""):\r\n            output = tf.reshape(outputs, shape=[-1, self.cell_size])\r\n            W, b = self._get_weight_bias(self.cell_size, self.out_size)\r\n            output = tf.nn.xw_plus_b(output, W, b, name=""output"")\r\n        \r\n        self.pred = output\r\n        losses = tf.nn.seq2seq.sequence_loss_by_example([tf.reshape(self.pred, [-1,])], [tf.reshape(self.y, [-1,])],\r\n                                    [tf.ones([self.batch_size*self.num_steps])], average_across_timesteps=True,\r\n                                    softmax_loss_function=self._ms_cost)\r\n        self.cost = tf.reduce_sum(losses)/tf.to_float(self.batch_size)\r\n\r\n    def _ms_cost(self, y_pred, y_target):\r\n        """"""The quadratic cost function""""""\r\n        return 0.5*tf.square(y_pred - y_target)\r\n\r\n    def _get_weight_bias(self, in_size, out_size):\r\n        """"""\r\n        Create weight and bias variables\r\n        """"""\r\n        weights = tf.get_variable(""weight"", shape=[in_size, out_size], \r\n                                    initializer=tf.random_normal_initializer(mean=0.0, stddev=1.0))\r\n        biases = tf.get_variable(""bias"", shape=[out_size,], initializer=tf.constant_initializer(0.1))\r\n        return weights, biases\r\n\r\nif __name__ == ""__main__"":\r\n    batch_size = 50\r\n    in_size = 1\r\n    out_size = 1\r\n    cell_size = 10\r\n    num_steps = 20\r\n    lr = 0.002\r\n    num_batchs = 200\r\n    n_epochs = 10\r\n\r\n    with tf.Session() as sess:\r\n        with tf.variable_scope(""model"", reuse=None):\r\n            model = LstmRegression(in_size, out_size, num_steps=num_steps, cell_size=cell_size, \r\n                            batch_size=batch_size, num_lstm_layers=2, keep_prob=0.5, is_training=True)\r\n        with tf.variable_scope(""model"", reuse=True):\r\n            pred_model = LstmRegression(in_size, out_size, num_steps=num_steps, cell_size=cell_size, \r\n                            batch_size=batch_size, num_lstm_layers=2, keep_prob=1.0, is_training=False)\r\n        \r\n        train_op = tf.train.AdamOptimizer(lr).minimize(model.cost)\r\n        tf.summary.scalar(""cost"", model.cost)\r\n        merged = tf.merge_all_summaries()\r\n        writer = tf.train.SummaryWriter(""logs"", sess.graph)\r\n        sess.run(tf.global_variables_initializer())\r\n        \r\n        global_steps = 0\r\n        state = sess.run(model.init_state)\r\n        for epoch in range(n_epochs):\r\n            losses = 0\r\n            for x, y, xo in batch_iterate(num_batchs, batch_size, num_steps):\r\n                _, cost, state = sess.run([train_op, model.cost, model.final_state], feed_dict={model.x: x,\r\n                                            model.y: y, model.init_state: state})\r\n                losses += cost/num_batchs\r\n            print(""Epoch {0}, cost {1}"".format(epoch, losses))\r\n        \r\n        # The prediction\r\n        plt.ion()\r\n        plt.show()\r\n        state = sess.run(pred_model.init_state)\r\n        for x, y, xo in batch_iterate(num_batchs, batch_size, num_steps):\r\n            pred, state = sess.run([pred_model.pred, pred_model.final_state], feed_dict={pred_model.x: x,\r\n                                    pred_model.y: y, pred_model.init_state: state })\r\n\r\n            # plotting\r\n            plt.plot(xo[0, :], y[0].flatten(), \'r\', xo[0, :], pred.flatten()[:num_steps], \'b--\')\r\n            plt.ylim((-1.2, 1.2))\r\n            plt.draw()\r\n            plt.pause(0.3)\r\n            \r\n'"
examples/rnn_language_model/input_data_rnn.py,0,"b'""""""\r\nThe data used in RNN language model.\r\n""""""\r\nimport csv\r\nimport itertools\r\nimport operator\r\nimport numpy as np\r\nimport nltk\r\nimport sys\r\nfrom datetime import datetime\r\n\r\n\r\ndef get_data(fileName=\'/data/reddit-comments-2015-08.csv\', vocabulary_size = 8000, unknown_token = ""UNKNOWN_TOKEN"",\r\n             sentence_start_token=""SENTENCE_START"", sentence_end_token = ""SENTENCE_END""):\r\n    # Read the data and append SENTENCE_START and SENTENCE_END tokens\r\n    print(""Reading CSV file..."")\r\n    with open(sys.path[0]+fileName, \'r\', encoding=\'utf-8\') as f:\r\n        reader = csv.reader(f, skipinitialspace=True)\r\n        reader.__next__()\r\n        # Split full comments into sentences\r\n        sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\r\n        # Append SENTENCE_START and SENTENCE_END\r\n        sentences = [""%s %s %s"" % (sentence_start_token, x, sentence_end_token) for x in sentences]\r\n    print(""Parsed %d sentences."" % (len(sentences)))\r\n\r\n    # Tokenize the sentences into words\r\n    tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\r\n\r\n    # Count the word frequencies\r\n    word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\r\n    print(""Found %d unique words tokens."" % len(word_freq.items()))\r\n\r\n    # Get the most common words and build index_to_word and word_to_index vectors\r\n    vocab = word_freq.most_common(vocabulary_size - 1)\r\n    index_to_word = [x[0] for x in vocab]\r\n    index_to_word.append(unknown_token)\r\n    word_to_index = dict([(w, i) for i, w in enumerate(index_to_word)])\r\n\r\n    print(""Using vocabulary size %d."" % vocabulary_size)\r\n    print(""The least frequent word in our vocabulary is \'%s\' and appeared %d times."" % (vocab[-1][0], vocab[-1][1]))\r\n\r\n    # Replace all words not in our vocabulary with the unknown token\r\n    for i, sent in enumerate(tokenized_sentences):\r\n        tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\r\n    print(""\\nExample sentence: \'%s\'"" % sentences[0])\r\n    print(""\\nExample sentence after Pre-processing: \'%s\'"" % tokenized_sentences[0])\r\n    # get the training set\r\n    X_train = []\r\n    y_train = []\r\n    for sen in tokenized_sentences:\r\n        X_train.append(list([word_to_index[w] for w in sen[:-1]]))\r\n        y_train.append(list([word_to_index[w] for w in sen[1:]]))\r\n\r\n    X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\r\n    y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\r\n\r\n    x_example, y_example = X_train[17], y_train[17]\r\n    print(""x:\\n%s\\n%s"" % ("" "".join([index_to_word[x] for x in x_example]), x_example))\r\n    print(""\\ny:\\n%s\\n%s"" % ("" "".join([index_to_word[x] for x in y_example]), y_example))\r\n    return (X_train, y_train)\r\n\r\nif __name__ == ""__main__"":\r\n    X_train, y_train = get_data()\r\n    print(type(X_train[0]))'"
examples/rnn_language_model/rnn_numpy.py,0,"b'""""""\r\nRNN model implemented by Numpy library\r\nauthor: Ye Hu\r\n2016/12/17\r\nfrom "" https://github.com/dennybritz/rnn-tutorial-rnnlm/blob/master/RNNLM.ipynb ""\r\n""""""\r\nimport sys\r\nimport operator\r\nfrom datetime import datetime\r\nimport timeit\r\nimport numpy as np\r\n\r\nfrom input_data_rnn import get_data\r\n\r\ndef softmax(x):\r\n    xt = np.exp(x - np.max(x))\r\n    return xt / np.sum(xt)\r\n\r\nclass RNN_np(object):\r\n    """"""A simple rnn class with numpy""""""\r\n    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=-1):\r\n        """"""\r\n        """"""\r\n        # keep\r\n        self.word_dim = word_dim\r\n        self.hidden_dim = hidden_dim\r\n        self.bptt_truncate = bptt_truncate\r\n        # Randomly initialize the params\r\n        bound = np.sqrt(1.0 / self.word_dim)\r\n        self.U = np.random.uniform(-bound, bound, size=[self.word_dim, self.hidden_dim]) # input\r\n        bound = np.sqrt(1.0 / self.hidden_dim)\r\n        self.V = np.random.uniform(-bound, bound, size=[self.hidden_dim, self.word_dim])  # output\r\n        self.W = np.random.uniform(-bound, bound, size=[self.hidden_dim, self.hidden_dim]) # old memeory\r\n\r\n    def forward_propagation(self, x):\r\n        """"""\r\n        Forward propagation\r\n        """"""\r\n        sequence_dim = len(x)  # time steps, also sequence dim\r\n        # keep the hidden states\r\n        s = np.zeros((sequence_dim+1, self.hidden_dim))\r\n        # the initial hidden of time step 0 (last)\r\n        s[-1] = np.zeros((self.hidden_dim))\r\n        # the output of each time step\r\n        o = np.zeros((sequence_dim, self.word_dim))\r\n        # for each time step\r\n        for t in range(sequence_dim):\r\n            # indeing with one-hot vector\r\n            s[t] = np.tanh(self.U[x[t], :] + np.dot(s[t-1], self.W))\r\n            o[t] = softmax(np.dot(s[t], self.V))\r\n\r\n        return (o, s)\r\n\r\n    def predict(self, x):\r\n        """"""Give word with the highest probability """"""\r\n        o, s = self.forward_propagation(x)\r\n        return np.argmax(o, axis=1)  # for each time step\r\n\r\n    def calculate_total_loss(self, xs, ys):\r\n        """"""Cross entropy loss""""""\r\n        loss = 0\r\n        # for each sequence\r\n        for i in range(len(ys)):\r\n            o, _ = self.forward_propagation(xs[i])\r\n            correct_predictions = o[np.arange(len(ys[i])), ys[i]]\r\n            loss += -1.0*np.sum(np.log(correct_predictions))\r\n        return loss\r\n\r\n    def calculate_loss(self, xs, ys):\r\n        """"""""""""\r\n        # the training examples\r\n        N = np.sum((len(e) for e in ys))\r\n        return self.calculate_total_loss(xs, ys)/float(N)\r\n\r\n    def bptt(self, x, y):\r\n        """"""Compute the gradients by BPTT""""""\r\n        N = len(x) # time steps, also sequence dim\r\n        # Perform forward propagation\r\n        o, s = self.forward_propagation(x)\r\n        # the initial gradients\r\n        dLdU = np.zeros(self.U.shape)\r\n        dLdW = np.zeros(self.W.shape)\r\n        dLdV = np.zeros(self.V.shape)\r\n        # dL/do\r\n        delta_o = o\r\n        delta_o[np.arange(N), y] += -1.0\r\n        # for each time step (also each output)\r\n        for t in np.arange(N)[::-1]:\r\n            # dL/dV\r\n            dLdV += np.outer(s[t], delta_o[t])\r\n            # dL/ds\r\n            delta_t = np.dot(self.V, delta_o[t])*(1 - (s[t]**2))\r\n            # Backpropagation through time (for at most self.bptt_truncate steps)\r\n            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\r\n                #print(""Backpropagation step t=%d bptt step=%d "" % (t, bptt_step))\r\n                dLdW += np.outer(s[bptt_step-1], delta_t)\r\n                dLdU[x[bptt_step], :] += delta_t\r\n                # Update delta for next time step\r\n                delta_t = np.dot(self.W, delta_t)*(1 - (s[bptt_step-1]**2))\r\n        return (dLdU, dLdV, dLdW)\r\n\r\n    def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\r\n        # Calculate the gradients using backpropagation. We want to checker if these are correct.\r\n        bptt_gradients = model.bptt(x, y)\r\n        # List of all parameters we want to check.\r\n        model_parameters = [\'U\', \'V\', \'W\']\r\n        # Gradient check for each parameter\r\n        for pidx, pname in enumerate(model_parameters):\r\n            # Get the actual parameter value from the mode, e.g. model.W\r\n            parameter = operator.attrgetter(pname)(self)\r\n            print(""Performing gradient check for parameter %s with shape %s."" % (pname, str(parameter.shape)))\r\n            # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\r\n            it = np.nditer(parameter, flags=[\'multi_index\'], op_flags=[\'readwrite\'])\r\n            while not it.finished:\r\n                ix = it.multi_index\r\n                # Save the original value so we can reset it later\r\n                original_value = parameter[ix]\r\n                # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\r\n                parameter[ix] = original_value + h\r\n                gradplus = model.calculate_total_loss([x], [y])\r\n                parameter[ix] = original_value - h\r\n                gradminus = model.calculate_total_loss([x], [y])\r\n                estimated_gradient = (gradplus - gradminus) / (2 * h)\r\n                # Reset parameter to original value\r\n                parameter[ix] = original_value\r\n                # The gradient for this parameter calculated using backpropagation\r\n                backprop_gradient = bptt_gradients[pidx][ix]\r\n                # calculate The relative error: (|x - y|/(|x| + |y|))\r\n                relative_error = np.abs(backprop_gradient - estimated_gradient) / (\r\n                np.abs(backprop_gradient) + np.abs(estimated_gradient))\r\n                # If the error is to large fail the gradient check\r\n                if relative_error > error_threshold:\r\n                    print(""Gradient Check ERROR: parameter=%s ix=%s"" % (pname, ix))\r\n                    print(""+h Loss: %f"" % gradplus)\r\n                    print(""-h Loss: %f"" % gradminus)\r\n                    print(""Estimated_gradient: %f"" % estimated_gradient)\r\n                    print(""Backpropagation gradient: %f"" % backprop_gradient)\r\n                    print(""Relative Error: %f"" % relative_error)\r\n                    return\r\n                it.iternext()\r\n            print(""Gradient check for parameter %s passed."" % (pname))\r\n\r\n    def sgd(self, x, y, learning_rate):\r\n        """"""Train the model with SGD""""""\r\n        # Compute the gradients\r\n        dLdU, dLdV, dLdW = self.bptt(x, y)\r\n        # Update the parameters\r\n        self.U += -learning_rate * dLdU\r\n        self.W += -learning_rate * dLdW\r\n        self.V += -learning_rate * dLdV\r\n\r\n\r\ndef train_rnn_with_sgd(model, X_train, y_train, learning_rate=0.005, n_epochs=100,\r\n                       evaluate_loss_after=5):\r\n    """"""""""""\r\n    N = len(X_train)  # number of training examples\r\n    losses = []\r\n    num_examples_seen = 0\r\n    for epoch in range(n_epochs):\r\n        # if evaluate the loss\r\n        if epoch % evaluate_loss_after == 0:\r\n            loss = model.calculate_loss(X_train, y_train)\r\n            losses.append((num_examples_seen, loss))\r\n            time = datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\')\r\n            print(""%s: Loss after num_examples_seen=%d epoch=%d: %f"" %\r\n                  (time, num_examples_seen, epoch, loss))\r\n            # Adjust the learning rate if loss increases\r\n            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\r\n                learning_rate *= 0.5\r\n                print(""Setting learning rate to %f"" % learning_rate)\r\n            sys.stdout.flush()\r\n        # Training\r\n        for i in range(N):\r\n            model.sgd(X_train[i], y_train[i], learning_rate=learning_rate)\r\n            num_examples_seen += 1\r\n\r\n\r\nif __name__ == ""__main__"":\r\n\r\n    np.random.seed(10)\r\n    vocabulary_size = 8000\r\n    X_train, y_train = get_data(vocabulary_size=vocabulary_size)\r\n\r\n    model = RNN_np(word_dim=8000, bptt_truncate=-1)\r\n    start_time = timeit.default_timer()\r\n    train_rnn_with_sgd(model, X_train[:1000], y_train[:1000], n_epochs=10, evaluate_loss_after=1)\r\n    end_time = timeit.default_timer()\r\n    print(""Time elapsed {0} seconds"".format((end_time-start_time)))\r\n\r\n\r\n\r\n'"
examples/rnn_language_model/rnn_tensorflow.py,18,"b'""""""\r\nA simple RNN model implemented by Tensorflow\r\nauthor: Ye Hu\r\n2016/12/24\r\n""""""\r\nimport timeit\r\nfrom datetime import datetime\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom input_data_rnn import get_data\r\n\r\nclass RNN_tf(object):\r\n    """"""\r\n    A RNN class for the language model\r\n    """"""\r\n    def __init__(self, inpt=None, word_dim=8000, hidden_dim=100, bptt_truncate=4):\r\n        """"""\r\n        :param inpt: tf.Tensor, the input tensor\r\n        :param word_dim: int, the number of word in the input sentence\r\n        :param hidden_dim: int, the size of hidden units\r\n        :param bptt_truncate: int, (TO DO:)\r\n        """"""\r\n        self.word_dim = word_dim\r\n        self.hidden_dim = hidden_dim\r\n        self.bptt_truncate = bptt_truncate\r\n        if inpt is None:\r\n            inpt = tf.placeholder(tf.int32, shape=[None, ])\r\n        self.x = inpt\r\n        self.y = tf.placeholder(tf.int32, shape=[None, ])\r\n\r\n        # Initialize the network parameters\r\n        bounds = np.sqrt(1.0/self.word_dim)\r\n        # Input weight matrix\r\n        self.U = tf.Variable(tf.random_uniform([self.word_dim, self.hidden_dim], minval=-bounds, maxval=bounds), \r\n                             name=""U"")\r\n        bounds = np.sqrt(1.0/self.hidden_dim)\r\n        self.W = tf.Variable(tf.random_uniform([self.hidden_dim, self.hidden_dim], minval=-bounds, maxval=bounds),\r\n                                name=""W"")         # old state weight matrix\r\n        self.V = tf.Variable(tf.random_uniform([self.hidden_dim, self.word_dim], minval=-bounds, maxval=bounds),\r\n                                name=""V"")         # the output weight matrix\r\n        # Keep track of all parameters for training\r\n        self.params = [self.U, self.W, self.V]\r\n        # Build the model\r\n        self.__model_build__()\r\n    \r\n    def __model_build__(self):\r\n        """"""\r\n        A private method to build the RNN model\r\n        """"""\r\n        # The inner function for forward propagation\r\n        def forward_propagation(s_t_prv, x_t):\r\n            s_t = tf.nn.tanh(tf.slice(self.U, [x_t, 0], [1, -1]) + tf.matmul(s_t_prv, self.W))\r\n            return s_t\r\n        # Use scan function to get the hidden state of all times\r\n        s = tf.scan(forward_propagation, self.x, initializer=tf.zeros([1, self.hidden_dim]))  # [seq_len, 1, hidden_dim]\r\n        s = tf.squeeze(s)  # [seq_len, hidden_dim]\r\n        # The output\r\n        o_wx = tf.matmul(s, self.V)\r\n        o = tf.nn.softmax(o_wx)\r\n        # The right prediction\r\n        self.prediction = tf.argmax(o, axis=1)\r\n        # The cost for training\r\n        self.cost = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(o_wx, self.y))\r\n        self.loss = self.cost / tf.cast(tf.size(self.x), tf.float32)\r\n\r\n        \r\n\r\ndef train_rnn_with_sgd(sess, model, X_train, y_train, learning_rate=0.005, n_epochs=100,\r\n                       evaluate_loss_after=5):\r\n    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(model.cost, var_list=model.params)\r\n    N = len(X_train)  # number of training examples\r\n    print(""Start training..."")\r\n    start_time = timeit.default_timer()\r\n    for epoch in range(n_epochs):\r\n        # If output the loss for all training examples\r\n        if epoch % evaluate_loss_after == 0:\r\n            losses = 0\r\n            for i in range(N):\r\n                losses += sess.run(model.loss, feed_dict={model.x: X_train[i], model.y: y_train[i]})\r\n            time = datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\')\r\n            print(""\\t{0}:Loss after Epoch {1} is {2}"".format(time, epoch, losses/N))\r\n        # Traing each by each\r\n        for i in range(N):\r\n            sess.run(train_op, feed_dict={model.x: X_train[i], model.y: y_train[i]})\r\n    end_time = timeit.default_timer()\r\n    print(""Finished!"")\r\n    print(""Time elapsed {0} minutes."".format((end_time-start_time)/60.0))\r\n\r\nif __name__ == ""__main__"":\r\n    np.random.seed(10)\r\n    tf.set_random_seed(1111)\r\n    vocabulary_size = 8000\r\n    X_train, y_train = get_data(vocabulary_size=vocabulary_size)\r\n    \r\n    with tf.Session() as sess:\r\n        model = RNN_tf(inpt=None, word_dim=8000, hidden_dim=100)\r\n        sess.run(tf.global_variables_initializer())\r\n        train_rnn_with_sgd(sess, model, X_train[:1000], y_train[:1000], n_epochs=10, evaluate_loss_after=1)'"
