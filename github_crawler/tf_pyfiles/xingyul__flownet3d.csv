file_path,api_count,code
evaluate.py,7,"b'\n\nimport argparse\nimport math\nfrom datetime import datetime\nimport numpy as np\nimport tensorflow as tf\nimport socket\nimport importlib\nimport os\nimport sys\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(BASE_DIR)\nimport pickle\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--gpu\', type=int, default=0, help=\'GPU to use [default: GPU 0]\')\nparser.add_argument(\'--model\', default=\'model_concat_upsa\', help=\'Model name [default: model_concat_upsa]\')\nparser.add_argument(\'--dataset\', default=\'flying_things_dataset\', help=\'Dataset name [default: flying_things_dataset]\')\nparser.add_argument(\'--data\', default=\'data_preprocessing/data_processed_maxcut_35_20k_2k_8192\', help=\'Dataset directory [default: /data_preprocessing/data_processed_maxcut_35_20k_2k_8192]\')\nparser.add_argument(\'--model_path\', default=\'log_train/model.ckpt\', help=\'model checkpoint file path [default: log_train/model.ckpt]\')\nparser.add_argument(\'--log_dir\', default=\'log_evaluate\', help=\'Log dir [default: log_evaluate]\')\nparser.add_argument(\'--num_point\', type=int, default=2048, help=\'Point Number [default: 2048]\')\nparser.add_argument(\'--batch_size\', type=int, default=16, help=\'Batch Size during training [default: 16]\')\nFLAGS = parser.parse_args()\n\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = str(FLAGS.gpu)\n\nBATCH_SIZE = FLAGS.batch_size\nNUM_POINT = FLAGS.num_point\nDATA = FLAGS.data\nGPU_INDEX = FLAGS.gpu\n\nMODEL = importlib.import_module(FLAGS.model) # import network module\nMODEL_FILE = os.path.join(BASE_DIR, FLAGS.model+\'.py\')\nMODEL_PATH = FLAGS.model_path\nLOG_DIR = FLAGS.log_dir\nif not os.path.exists(LOG_DIR): os.mkdir(LOG_DIR)\nos.system(\'cp %s %s\' % (MODEL_FILE, LOG_DIR)) # bkp of model def\nos.system(\'cp %s %s\' % (__file__, LOG_DIR)) # bkp of train procedure\nLOG_FOUT = open(os.path.join(LOG_DIR, \'log_evaluate.txt\'), \'w\')\nLOG_FOUT.write(str(FLAGS)+\'\\n\')\n\nDATASET = importlib.import_module(FLAGS.dataset)\nTEST_DATASET = DATASET.SceneflowDataset(DATA, npoints=NUM_POINT, train=False)\n\ndef log_string(out_str):\n    LOG_FOUT.write(out_str+\'\\n\')\n    LOG_FOUT.flush()\n    print(out_str)\n\ndef evaluate():\n    with tf.Graph().as_default():\n        with tf.device(\'/gpu:\'+str(GPU_INDEX)):\n            pointclouds_pl, labels_pl, masks_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)\n            is_training_pl = tf.placeholder(tf.bool, shape=())\n\n            print(""--- Get model and loss"")\n            # Get model and loss\n            pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=None)\n            loss = MODEL.get_loss(pred, labels_pl, masks_pl, end_points)\n            tf.summary.scalar(\'loss\', loss)\n\n            # Add ops to save and restore all the variables.\n            saver = tf.train.Saver()\n\n        # Create a session\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        config.allow_soft_placement = True\n        config.log_device_placement = False\n        sess = tf.Session(config=config)\n\n        saver.restore(sess, MODEL_PATH)\n        log_string(""Model restored."")\n\n        ops = {\'pointclouds_pl\': pointclouds_pl,\n               \'labels_pl\': labels_pl,\n               \'masks_pl\': masks_pl,\n               \'is_training_pl\': is_training_pl,\n               \'pred\': pred,\n               \'loss\': loss}\n\n        eval_one_epoch(sess, ops)\n\ndef get_batch(dataset, idxs, start_idx, end_idx):\n    bsize = end_idx-start_idx\n    batch_data = np.zeros((bsize, NUM_POINT*2, 6))\n    batch_label = np.zeros((bsize, NUM_POINT, 3))\n    batch_mask = np.zeros((bsize, NUM_POINT))\n    # shuffle idx to change point order (change FPS behavior)\n    shuffle_idx = np.arange(NUM_POINT)\n    np.random.shuffle(shuffle_idx)\n    for i in range(bsize):\n        pc1, pc2, color1, color2, vel, mask1 = dataset[idxs[i+start_idx]]\n\n        batch_data[i,:NUM_POINT,:3] = pc1[shuffle_idx,:]\n        batch_data[i,:NUM_POINT,3:] = color1[shuffle_idx,:]\n        batch_data[i,NUM_POINT:,:3] = pc2[shuffle_idx,:]\n        batch_data[i,NUM_POINT:,3:] = color2[shuffle_idx,:]\n        batch_label[i,:,:] = vel[shuffle_idx,:]\n        batch_mask[i,:] = mask1[shuffle_idx]\n    return batch_data, batch_label, batch_mask\n\ndef scene_flow_EPE_np(pred, labels, mask):\n    error = np.sqrt(np.sum((pred - labels)**2, 2) + 1e-20)\n\n    gtflow_len = np.sqrt(np.sum(labels*labels, 2) + 1e-20) # B,N\n    acc1 = np.sum(np.logical_or((error <= 0.05)*mask, (error/gtflow_len <= 0.05)*mask), axis=1)\n    acc2 = np.sum(np.logical_or((error <= 0.1)*mask, (error/gtflow_len <= 0.1)*mask), axis=1)\n\n    mask_sum = np.sum(mask, 1)\n    acc1 = acc1[mask_sum > 0] / mask_sum[mask_sum > 0]\n    acc1 = np.mean(acc1)\n    acc2 = acc2[mask_sum > 0] / mask_sum[mask_sum > 0]\n    acc2 = np.mean(acc2)\n\n    EPE = np.sum(error * mask, 1)[mask_sum > 0] / mask_sum[mask_sum > 0]\n    EPE = np.mean(EPE)\n    return EPE, acc1, acc2\n\ndef eval_one_epoch(sess, ops):\n    """""" ops: dict mapping from string to tf ops """"""\n    is_training = False\n    test_idxs = np.arange(0, len(TEST_DATASET))\n    # Test on all data: last batch might be smaller than BATCH_SIZE\n    num_batches = (len(TEST_DATASET)+BATCH_SIZE-1) // BATCH_SIZE\n\n    loss_sum = 0\n    epe_3d_sum = 0\n    acc_3d_sum = 0\n    acc_3d_2_sum = 0\n\n    log_string(str(datetime.now()))\n    log_string(\'---- EVALUATION ----\')\n\n    batch_data = np.zeros((BATCH_SIZE, NUM_POINT*2, 3))\n    batch_label = np.zeros((BATCH_SIZE, NUM_POINT, 3))\n    batch_mask = np.zeros((BATCH_SIZE, NUM_POINT))\n    for batch_idx in range(num_batches):\n        if batch_idx %20==0:\n            log_string(\'%03d/%03d\'%(batch_idx, num_batches))\n        start_idx = batch_idx * BATCH_SIZE\n        end_idx = min(len(TEST_DATASET), (batch_idx+1) * BATCH_SIZE)\n        cur_batch_size = end_idx-start_idx\n        cur_batch_data, cur_batch_label, cur_batch_mask = get_batch(TEST_DATASET, test_idxs, start_idx, end_idx)\n        if cur_batch_size == BATCH_SIZE:\n            batch_data = cur_batch_data\n            batch_label = cur_batch_label\n            batch_mask = cur_batch_mask\n        else:\n            batch_data[0:cur_batch_size] = cur_batch_data\n            batch_label[0:cur_batch_size] = cur_batch_label\n            batch_mask[0:cur_batch_size] = cur_batch_mask\n\n        # ---------------------------------------------------------------------\n        # ---- INFERENCE BELOW ----\n        pred_val_sum = np.zeros((BATCH_SIZE, NUM_POINT, 3))\n        SHUFFLE_TIMES = 10\n        RECURRENT_TIMES = 0\n        for shuffle_cnt in range(SHUFFLE_TIMES):\n            shuffle_idx = np.arange(NUM_POINT)\n            np.random.shuffle(shuffle_idx)\n            batch_data_new = np.copy(batch_data)\n            batch_data_new[:,0:NUM_POINT,:] = batch_data[:,shuffle_idx,:]\n            batch_data_new[:,NUM_POINT:,:] = batch_data[:,NUM_POINT+shuffle_idx,:]\n            feed_dict = {ops[\'pointclouds_pl\']: batch_data_new,\n                         ops[\'labels_pl\']: batch_label[:,shuffle_idx,:],\n                         ops[\'masks_pl\']: batch_mask[:,shuffle_idx],\n                         ops[\'is_training_pl\']: is_training}\n            loss_val, pred_val = sess.run([ops[\'loss\'], ops[\'pred\']], feed_dict=feed_dict)\n            for recurrent_cnt in range(RECURRENT_TIMES):\n                batch_data_new[:,0:NUM_POINT,0:3] += pred_val\n                batch_label_new = np.copy(batch_label)\n                batch_label_new[:,:,:] = batch_label - pred_val\n                feed_dict = {ops[\'pointclouds_pl\']: batch_data_new,\n                             ops[\'labels_pl\']: batch_label_new[:,shuffle_idx,:],\n                             ops[\'masks_pl\']: batch_mask[:,shuffle_idx],\n                             ops[\'is_training_pl\']: is_training}\n                loss_val, pred_val_new = sess.run([ops[\'loss\'], ops[\'pred\']], feed_dict=feed_dict)\n                pred_val += pred_val_new\n            pred_val_sum[:,shuffle_idx,:] += pred_val\n        # ---- INFERENCE ABOVE ----\n        # ---------------------------------------------------------------------\n        pred_val = pred_val_sum / float(SHUFFLE_TIMES)\n        tmp = np.sum((pred_val - batch_label)**2, 2) / 2.0\n        loss_val_np = np.mean(batch_mask * tmp)\n        loss_val = loss_val_np\n        print(\'batch loss: %f\' % (loss_val))\n        if cur_batch_size==BATCH_SIZE:\n            loss_sum += loss_val\n\n        epe_3d, acc_3d, acc_3d_2 = scene_flow_EPE_np(pred_val, batch_label, batch_mask)\n        print(\'batch EPE 3D: %f\\tACC 3D: %f\\tACC 3D 2: %f\' % (epe_3d, acc_3d, acc_3d_2))\n\n        if cur_batch_size==BATCH_SIZE:\n            epe_3d_sum += epe_3d\n            acc_3d_sum += acc_3d\n            acc_3d_2_sum += acc_3d_2\n\n    log_string(\'eval mean loss: %f\' % (loss_sum / float(len(TEST_DATASET)/BATCH_SIZE)))\n    log_string(\'eval mean EPE 3D: %f\' % (epe_3d_sum / float(len(TEST_DATASET)/BATCH_SIZE)))\n    log_string(\'eval mean ACC 3D: %f\' % (acc_3d_sum / float(len(TEST_DATASET)/BATCH_SIZE)))\n    log_string(\'eval mean ACC 3D 2: %f\' % (acc_3d_2_sum / float(len(TEST_DATASET)/BATCH_SIZE)))\n\n    return loss_sum/float(len(TEST_DATASET)/BATCH_SIZE)\n\n\nif __name__ == ""__main__"":\n    log_string(\'pid: %s\'%(str(os.getpid())))\n    evaluate()\n    LOG_FOUT.close()\n'"
flying_things_dataset.py,0,"b""'''\n    Provider for duck dataset from xingyu liu\n'''\n\nimport os\nimport os.path\nimport json\nimport numpy as np\nimport sys\nimport pickle\nimport glob\n\n\nclass SceneflowDataset():\n    def __init__(self, root='data_preprocessing/data_processed_maxcut_35_both_mask_20k_2k', npoints=2048, train=True):\n        self.npoints = npoints\n        self.train = train\n        self.root = root\n        if self.train:\n            self.datapath = glob.glob(os.path.join(self.root, 'TRAIN*.npz'))\n        else:\n            self.datapath = glob.glob(os.path.join(self.root, 'TEST*.npz'))\n        self.cache = {}\n        self.cache_size = 30000\n\n        ###### deal with one bad datapoint with nan value\n        self.datapath = [d for d in self.datapath if 'TRAIN_C_0140_left_0006-0' not in d]\n        ######\n\n    def __getitem__(self, index):\n        if index in self.cache:\n            pos1, pos2, color1, color2, flow, mask1 = self.cache[index]\n        else:\n            fn = self.datapath[index]\n            with open(fn, 'rb') as fp:\n                data = np.load(fp)\n                pos1 = data['points1']\n                pos2 = data['points2']\n                color1 = data['color1'] / 255\n                color2 = data['color2'] / 255\n                flow = data['flow']\n                mask1 = data['valid_mask1']\n\n            if len(self.cache) < self.cache_size:\n                self.cache[index] = (pos1, pos2, color1, color2, flow, mask1)\n\n        if self.train:\n            n1 = pos1.shape[0]\n            sample_idx1 = np.random.choice(n1, self.npoints, replace=False)\n            n2 = pos2.shape[0]\n            sample_idx2 = np.random.choice(n2, self.npoints, replace=False)\n\n            pos1_ = np.copy(pos1[sample_idx1, :])\n            pos2_ = np.copy(pos2[sample_idx2, :])\n            color1_ = np.copy(color1[sample_idx1, :])\n            color2_ = np.copy(color2[sample_idx2, :])\n            flow_ = np.copy(flow[sample_idx1, :])\n            mask1_ = np.copy(mask1[sample_idx1])\n        else:\n            pos1_ = np.copy(pos1[:self.npoints, :])\n            pos2_ = np.copy(pos2[:self.npoints, :])\n            color1_ = np.copy(color1[:self.npoints, :])\n            color2_ = np.copy(color2[:self.npoints, :])\n            flow_ = np.copy(flow[:self.npoints, :])\n            mask1_ = np.copy(mask1[:self.npoints])\n\n        return pos1_, pos2_, color1_, color2_, flow_, mask1_\n\n    def __len__(self):\n        return len(self.datapath)\n\n\nif __name__ == '__main__':\n    # import mayavi.mlab as mlab\n    d = SceneflowDataset(npoints=2048)\n    print(len(d))\n    import time\n    tic = time.time()\n    for i in range(100):\n        pc1, pc2, c1, c2, flow, m1, m2 = d[i]\n\n        print(pc1.shape)\n        print(pc2.shape)\n        print(flow.shape)\n        print(np.sum(m1))\n        print(np.sum(m2))\n        pc1_m1 = pc1[m1==1,:]\n        pc1_m1_n = pc1[m1==0,:]\n        print(pc1_m1.shape)\n        print(pc1_m1_n.shape)\n        mlab.points3d(pc1_m1[:,0], pc1_m1[:,1], pc1_m1[:,2], scale_factor=0.05, color=(1,0,0))\n        mlab.points3d(pc1_m1_n[:,0], pc1_m1_n[:,1], pc1_m1_n[:,2], scale_factor=0.05, color=(0,1,0))\n        raw_input()\n\n        mlab.points3d(pc1[:,0], pc1[:,1], pc1[:,2], scale_factor=0.05, color=(1,0,0))\n        mlab.points3d(pc2[:,0], pc2[:,1], pc2[:,2], scale_factor=0.05, color=(0,1,0))\n        raw_input()\n        mlab.quiver3d(pc1[:,0], pc1[:,1], pc1[:,2], flow[:,0], flow[:,1], flow[:,2], scale_factor=1)\n        raw_input()\n\n    print(time.time() - tic)\n    print(pc1.shape, type(pc1))\n\n\n"""
kitti_dataset.py,0,"b""'''\n    Provider for duck dataset from xingyu liu\n'''\n\nimport os\nimport os.path\nimport json\nimport numpy as np\nimport sys\nimport pickle\nimport glob\n\n\nclass SceneflowDataset():\n    def __init__(self, root='kitti_rm_ground', npoints=16384, train=True):\n        self.npoints = npoints\n        self.root = root\n        self.train = train\n        self.datapath = glob.glob(os.path.join(self.root, '*.npz'))\n        self.cache = {}\n        self.cache_size = 30000\n\n    def __getitem__(self, index):\n        if index in self.cache:\n            pos1, pos2, flow = self.cache[index]\n        else:\n            fn = self.datapath[index]\n            with open(fn, 'rb') as fp:\n                data = np.load(fp)\n                pos1 = data['pos1']\n                pos2 = data['pos2']\n                flow = data['gt']\n\n            if len(self.cache) < self.cache_size:\n                self.cache[index] = (pos1, pos2, flow)\n\n            n1 = pos1.shape[0]\n            n2 = pos2.shape[0]\n            if n1 >= self.npoints:\n                sample_idx1 = np.random.choice(n1, self.npoints, replace=False)\n            else:\n                sample_idx1 = np.concatenate((np.arange(n1), np.random.choice(n1, self.npoints - n1, replace=True)), axis=-1)\n            if n2 >= self.npoints:\n                sample_idx2 = np.random.choice(n2, self.npoints, replace=False)\n            else:\n                sample_idx2 = np.concatenate((np.arange(n2), np.random.choice(n2, self.npoints - n2, replace=True)), axis=-1)\n\n            pos1_ = np.copy(pos1)[sample_idx1, :]\n            pos2_ = np.copy(pos2)[sample_idx2, :]\n            flow_ = np.copy(flow)[sample_idx1, :]\n\n        color1 = np.zeros([self.npoints, 3])\n        color2 = np.zeros([self.npoints, 3])\n        mask = np.ones([self.npoints])\n\n        return pos1_, pos2_, color1, color2, flow_, mask\n\n    def __len__(self):\n        return len(self.datapath)\n\n\nif __name__ == '__main__':\n    import mayavi.mlab as mlab\n    d = SceneflowDataset(root='kitti_rm_ground', npoints=16384)\n    print(len(d))\n    import time\n    tic = time.time()\n    for i in range(1, 100):\n        pc1, pc2, color1, color2, flow, mask = d[i]\n        print(pc1.shape, pc2.shape)\n        continue\n\n        mlab.figure(bgcolor=(1,1,1))\n        mlab.points3d(pc1[:,0], pc1[:,1], pc1[:,2], scale_factor=0.05, color=(1,0,0))\n        mlab.points3d(pc2[:,0], pc2[:,1], pc2[:,2], scale_factor=0.05, color=(0,1,0))\n        input()\n\n        mlab.figure(bgcolor=(1,1,1))\n        mlab.points3d(pc1[:,0], pc1[:,1], pc1[:,2], scale_factor=0.05, color=(1,0,0))\n        mlab.points3d(pc2[:,0], pc2[:,1], pc2[:,2], scale_factor=0.05, color=(0,1,0))\n        mlab.quiver3d(pc1[:,0], pc1[:,1], pc1[:,2], flow[:,0], flow[:,1], flow[:,2], scale_factor=1, color=(0,0,1), line_width=0.2)\n        input()\n\n    print(time.time() - tic)\n    print(pc1.shape, type(pc1))\n\n\n"""
model_concat_upsa.py,14,"b'""""""\n    FlowNet3D model with up convolution\n""""""\n\nimport tensorflow as tf\nimport numpy as np\nimport math\nimport sys\nimport os\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.join(BASE_DIR, \'utils\'))\nimport tf_util\nfrom pointnet_util import *\n\ndef placeholder_inputs(batch_size, num_point):\n    pointclouds_pl = tf.placeholder(tf.float32, shape=(batch_size, num_point * 2, 6))\n    labels_pl = tf.placeholder(tf.float32, shape=(batch_size, num_point, 3))\n    masks_pl = tf.placeholder(tf.float32, shape=(batch_size, num_point))\n    return pointclouds_pl, labels_pl, masks_pl\n\n\ndef get_model(point_cloud, is_training, bn_decay=None):\n    """""" FlowNet3D, for training\n        input: Bx(N1+N2)x3,\n        output: BxN1x3 """"""\n    end_points = {}\n    batch_size = point_cloud.get_shape()[0].value\n    num_point = point_cloud.get_shape()[1].value // 2\n\n    l0_xyz_f1 = point_cloud[:, :num_point, 0:3]\n    l0_points_f1 = point_cloud[:, :num_point, 3:]\n    l0_xyz_f2 = point_cloud[:, num_point:, 0:3]\n    l0_points_f2 = point_cloud[:, num_point:, 3:]\n\n    RADIUS1 = 0.5\n    RADIUS2 = 1.0\n    RADIUS3 = 2.0\n    RADIUS4 = 4.0\n    with tf.variable_scope(\'sa1\') as scope:\n        # Frame 1, Layer 1\n        l1_xyz_f1, l1_points_f1, l1_indices_f1 = pointnet_sa_module(l0_xyz_f1, l0_points_f1, npoint=1024, radius=RADIUS1, nsample=16, mlp=[32,32,64], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope=\'layer1\')\n        end_points[\'l1_indices_f1\'] = l1_indices_f1\n\n        # Frame 1, Layer 2\n        l2_xyz_f1, l2_points_f1, l2_indices_f1 = pointnet_sa_module(l1_xyz_f1, l1_points_f1, npoint=256, radius=RADIUS2, nsample=16, mlp=[64,64,128], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope=\'layer2\')\n        end_points[\'l2_indices_f1\'] = l2_indices_f1\n\n        scope.reuse_variables()\n        # Frame 2, Layer 1\n        l1_xyz_f2, l1_points_f2, l1_indices_f2 = pointnet_sa_module(l0_xyz_f2, l0_points_f2, npoint=1024, radius=RADIUS1, nsample=16, mlp=[32,32,64], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope=\'layer1\')\n        # Frame 2, Layer 2\n        l2_xyz_f2, l2_points_f2, l2_indices_f2 = pointnet_sa_module(l1_xyz_f2, l1_points_f2, npoint=256, radius=RADIUS2, nsample=16, mlp=[64,64,128], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope=\'layer2\')\n\n    _, l2_points_f1_new = flow_embedding_module(l2_xyz_f1, l2_xyz_f2, l2_points_f1, l2_points_f2, radius=10.0, nsample=64, mlp=[128,128,128], is_training=is_training, bn_decay=bn_decay, scope=\'flow_embedding\', bn=True, pooling=\'max\', knn=True, corr_func=\'concat\')\n\n    # Layer 3\n    l3_xyz_f1, l3_points_f1, l3_indices_f1 = pointnet_sa_module(l2_xyz_f1, l2_points_f1_new, npoint=64, radius=RADIUS3, nsample=8, mlp=[128,128,256], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope=\'layer3\')\n    end_points[\'l3_indices_f1\'] = l3_indices_f1\n\n    # Layer 4\n    l4_xyz_f1, l4_points_f1, l4_indices_f1 = pointnet_sa_module(l3_xyz_f1, l3_points_f1, npoint=16, radius=RADIUS4, nsample=8, mlp=[256,256,512], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope=\'layer4\')\n    end_points[\'l4_indices_f1\'] = l4_indices_f1\n\n    # Feature Propagation\n    l3_feat_f1 = set_upconv_module(l3_xyz_f1, l4_xyz_f1, l3_points_f1, l4_points_f1, nsample=8, radius=2.4, mlp=[], mlp2=[256,256], scope=\'up_sa_layer1\', is_training=is_training, bn_decay=bn_decay, knn=True)\n    l2_feat_f1 = set_upconv_module(l2_xyz_f1, l3_xyz_f1, tf.concat(axis=-1, values=[l2_points_f1, l2_points_f1_new]), l3_feat_f1, nsample=8, radius=1.2, mlp=[128,128,256], mlp2=[256], scope=\'up_sa_layer2\', is_training=is_training, bn_decay=bn_decay, knn=True)\n    l1_feat_f1 = set_upconv_module(l1_xyz_f1, l2_xyz_f1, l1_points_f1, l2_feat_f1, nsample=8, radius=0.6, mlp=[128,128,256], mlp2=[256], scope=\'up_sa_layer3\', is_training=is_training, bn_decay=bn_decay, knn=True)\n    l0_feat_f1 = pointnet_fp_module(l0_xyz_f1, l1_xyz_f1, l0_points_f1, l1_feat_f1, [256,256], is_training, bn_decay, scope=\'fa_layer4\')\n\n    # FC layers\n    net = tf_util.conv1d(l0_feat_f1, 128, 1, padding=\'VALID\', bn=True, is_training=is_training, scope=\'fc1\', bn_decay=bn_decay)\n    net = tf_util.conv1d(net, 3, 1, padding=\'VALID\', activation_fn=None, scope=\'fc2\')\n\n    return net, end_points\n\n\ndef huber_loss(error, delta):\n    abs_error = tf.abs(error)\n    quadratic = tf.minimum(abs_error, delta)\n    linear = (abs_error - quadratic)\n    losses = 0.5 * quadratic**2 + delta * linear\n    return tf.reduce_mean(losses)\n\n\ndef get_loss(pred, label, mask, end_points):\n    """""" pred: BxNx3,\n        label: BxNx3,\n        mask: BxN\n    """"""\n    batch_size = pred.get_shape()[0].value\n    num_point = pred.get_shape()[1].value\n    l2_loss = tf.reduce_mean(mask * tf.reduce_sum((pred-label) * (pred-label), axis=2) / 2.0)\n    tf.summary.scalar(\'l2 loss\', l2_loss)\n    tf.add_to_collection(\'losses\', l2_loss)\n    return l2_loss\n\nif __name__==\'__main__\':\n    with tf.Graph().as_default():\n        inputs = tf.zeros((32,1024*2,6))\n        outputs = get_model(inputs, tf.constant(True))\n        print(outputs)\n'"
model_concat_upsa_eval_kitti.py,14,"b'""""""\n    FlowNet3D model with up convolution\n""""""\n\nimport tensorflow as tf\nimport numpy as np\nimport math\nimport sys\nimport os\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.join(BASE_DIR, \'utils\'))\nimport tf_util\nfrom pointnet_util import *\n\ndef placeholder_inputs(batch_size, num_point):\n    pointclouds_pl = tf.placeholder(tf.float32, shape=(batch_size, num_point * 2, 6))\n    labels_pl = tf.placeholder(tf.float32, shape=(batch_size, num_point, 3))\n    masks_pl = tf.placeholder(tf.float32, shape=(batch_size, num_point))\n    return pointclouds_pl, labels_pl, masks_pl\n\n\ndef get_model(point_cloud, is_training, bn_decay=None, reuse=False):\n    """""" FlowNet3D, for evaluating on KITTI\n        input: Bx(N1+N2)x3,\n        output: BxN1x3 """"""\n    end_points = {}\n    batch_size = point_cloud.get_shape()[0].value\n    num_point = point_cloud.get_shape()[1].value // 2\n\n    l0_xyz_f1 = point_cloud[:, :num_point, 0:3]\n    l0_points_f1 = point_cloud[:, :num_point, 3:]\n    l0_xyz_f2 = point_cloud[:, num_point:, 0:3]\n    l0_points_f2 = point_cloud[:, num_point:, 3:]\n\n    RADIUS1 = 0.5\n    RADIUS2 = 1.0\n    RADIUS3 = 2.0\n    RADIUS4 = 4.0\n    with tf.variable_scope(\'sa1\') as scope:\n        if reuse:\n            scope.reuse_variables()\n\n        # Frame 1, Layer 1\n        l1_xyz_f1, l1_points_f1, l1_indices_f1 = pointnet_sa_module(l0_xyz_f1, l0_points_f1, npoint=8192, radius=RADIUS1, nsample=256, mlp=[32,32,64], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope=\'layer1\')\n        end_points[\'l1_indices_f1\'] = l1_indices_f1\n\n        # Frame 1, Layer 2\n        l2_xyz_f1, l2_points_f1, l2_indices_f1 = pointnet_sa_module(l1_xyz_f1, l1_points_f1, npoint=2048, radius=RADIUS2, nsample=256, mlp=[64,64,128], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope=\'layer2\')\n        end_points[\'l2_indices_f1\'] = l2_indices_f1\n\n        if not reuse:\n            scope.reuse_variables()\n\n        # Frame 2, Layer 1\n        l1_xyz_f2, l1_points_f2, l1_indices_f2 = pointnet_sa_module(l0_xyz_f2, l0_points_f2, npoint=8192, radius=RADIUS1, nsample=256, mlp=[32,32,64], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope=\'layer1\')\n        # Frame 2, Layer 2\n        l2_xyz_f2, l2_points_f2, l2_indices_f2 = pointnet_sa_module(l1_xyz_f2, l1_points_f2, npoint=2048, radius=RADIUS2, nsample=256, mlp=[64,64,128], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope=\'layer2\')\n\n    _, l2_points_f1_new = flow_embedding_module(l2_xyz_f1, l2_xyz_f2, l2_points_f1, l2_points_f2, radius=3.0, nsample=256, mlp=[128,128,128], is_training=is_training, bn_decay=bn_decay, scope=\'flow_embedding\', bn=True, pooling=\'max\', knn=True, corr_func=\'concat\')\n\n    # Layer 3\n    l3_xyz_f1, l3_points_f1, l3_indices_f1 = pointnet_sa_module(l2_xyz_f1, l2_points_f1_new, npoint=512, radius=RADIUS3, nsample=64, mlp=[128,128,256], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope=\'layer3\')\n    end_points[\'l3_indices_f1\'] = l3_indices_f1\n\n    # Layer 4\n    l4_xyz_f1, l4_points_f1, l4_indices_f1 = pointnet_sa_module(l3_xyz_f1, l3_points_f1, npoint=256, radius=RADIUS4, nsample=64, mlp=[256,256,512], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope=\'layer4\')\n    end_points[\'l4_indices_f1\'] = l4_indices_f1\n\n    # Feature Propagation\n    l3_feat_f1 = set_upconv_module(l3_xyz_f1, l4_xyz_f1, l3_points_f1, l4_points_f1, nsample=4, radius=2.4, mlp=[], mlp2=[256,256], scope=\'up_sa_layer1\', is_training=is_training, bn_decay=bn_decay, knn=True)\n    l2_feat_f1 = set_upconv_module(l2_xyz_f1, l3_xyz_f1, tf.concat(axis=-1, values=[l2_points_f1, l2_points_f1_new]), l3_feat_f1, nsample=4, radius=1.2, mlp=[128,128,256], mlp2=[256], scope=\'up_sa_layer2\', is_training=is_training, bn_decay=bn_decay, knn=True)\n    l1_feat_f1 = set_upconv_module(l1_xyz_f1, l2_xyz_f1, l1_points_f1, l2_feat_f1, nsample=4, radius=0.6, mlp=[128,128,256], mlp2=[256], scope=\'up_sa_layer3\', is_training=is_training, bn_decay=bn_decay, knn=True)\n    l0_feat_f1 = pointnet_fp_module(l0_xyz_f1, l1_xyz_f1, l0_points_f1, l1_feat_f1, [256,256], is_training, bn_decay, scope=\'fa_layer4\')\n\n    # FC layers\n    net = tf_util.conv1d(l0_feat_f1, 128, 1, padding=\'VALID\', bn=True, is_training=is_training, scope=\'fc1\', bn_decay=bn_decay)\n    net = tf_util.conv1d(net, 3, 1, padding=\'VALID\', activation_fn=None, scope=\'fc2\')\n\n    return net, end_points\n\n\ndef huber_loss(error, delta):\n    abs_error = tf.abs(error)\n    quadratic = tf.minimum(abs_error, delta)\n    linear = (abs_error - quadratic)\n    losses = 0.5 * quadratic**2 + delta * linear\n    return tf.reduce_mean(losses)\n\n\ndef get_loss(pred, label, mask, end_points):\n    """""" pred: BxNx3,\n        label: BxNx3,\n        mask: BxN\n    """"""\n    batch_size = pred.get_shape()[0].value\n    num_point = pred.get_shape()[1].value\n    l2_loss = tf.reduce_mean(mask * tf.reduce_sum((pred-label) * (pred-label), axis=2) / 2.0)\n    tf.summary.scalar(\'l2 loss\', l2_loss)\n    tf.add_to_collection(\'losses\', l2_loss)\n    return l2_loss\n\nif __name__==\'__main__\':\n    with tf.Graph().as_default():\n        inputs = tf.zeros((32,1024*2,6))\n        outputs = get_model(inputs, tf.constant(True))\n        print(outputs)\n'"
train.py,20,"b'\'\'\'\n    Single-GPU training code\n\'\'\'\n\nimport argparse\nimport math\nfrom datetime import datetime\nimport numpy as np\nimport tensorflow as tf\nimport socket\nimport importlib\nimport os\nimport sys\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(BASE_DIR)\nimport flying_things_dataset\nimport pickle\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--gpu\', type=int, default=0, help=\'GPU to use [default: GPU 0]\')\nparser.add_argument(\'--model\', default=\'model_concat_upsa\', help=\'Model name [default: model_concat_upsa]\')\nparser.add_argument(\'--data\', default=\'data_preprocessing/data_processed_maxcut_35_20k_2k_8192\', help=\'Dataset directory [default: data_preprocessing/data_processed_maxcut_35_20k_2k_8192]\')\nparser.add_argument(\'--log_dir\', default=\'log_train\', help=\'Log dir [default: log_train]\')\nparser.add_argument(\'--num_point\', type=int, default=2048, help=\'Point Number [default: 2048]\')\nparser.add_argument(\'--max_epoch\', type=int, default=151, help=\'Epoch to run [default: 151]\')\nparser.add_argument(\'--batch_size\', type=int, default=16, help=\'Batch Size during training [default: 16]\')\nparser.add_argument(\'--learning_rate\', type=float, default=0.001, help=\'Initial learning rate [default: 0.001]\')\nparser.add_argument(\'--momentum\', type=float, default=0.9, help=\'Initial learning rate [default: 0.9]\')\nparser.add_argument(\'--optimizer\', default=\'adam\', help=\'adam or momentum [default: adam]\')\nparser.add_argument(\'--decay_step\', type=int, default=200000, help=\'Decay step for lr decay [default: 200000]\')\nparser.add_argument(\'--decay_rate\', type=float, default=0.7, help=\'Decay rate for lr decay [default: 0.7]\')\nFLAGS = parser.parse_args()\n\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = str(FLAGS.gpu)\n\nEPOCH_CNT = 0\n\nBATCH_SIZE = FLAGS.batch_size\nNUM_POINT = FLAGS.num_point\nDATA = FLAGS.data\nMAX_EPOCH = FLAGS.max_epoch\nBASE_LEARNING_RATE = FLAGS.learning_rate\nGPU_INDEX = FLAGS.gpu\nMOMENTUM = FLAGS.momentum\nOPTIMIZER = FLAGS.optimizer\nDECAY_STEP = FLAGS.decay_step\nDECAY_RATE = FLAGS.decay_rate\n\nMODEL = importlib.import_module(FLAGS.model) # import network module\nMODEL_FILE = os.path.join(BASE_DIR, FLAGS.model+\'.py\')\nLOG_DIR = FLAGS.log_dir\nif not os.path.exists(LOG_DIR): os.mkdir(LOG_DIR)\nos.system(\'cp %s %s\' % (MODEL_FILE, LOG_DIR)) # bkp of model def\nos.system(\'cp %s %s\' % (__file__, LOG_DIR)) # bkp of train procedure\nos.system(\'cp %s %s\' % (\'flying_things_dataset.py\', LOG_DIR)) # bkp of dataset file\nLOG_FOUT = open(os.path.join(LOG_DIR, \'log_train.txt\'), \'w\')\nLOG_FOUT.write(str(FLAGS)+\'\\n\')\n\nBN_INIT_DECAY = 0.5\nBN_DECAY_DECAY_RATE = 0.5\nBN_DECAY_DECAY_STEP = float(DECAY_STEP)\nBN_DECAY_CLIP = 0.99\n\nTRAIN_DATASET = flying_things_dataset.SceneflowDataset(DATA, npoints=NUM_POINT)\nTEST_DATASET = flying_things_dataset.SceneflowDataset(DATA, npoints=NUM_POINT, train=False)\n\ndef log_string(out_str):\n    LOG_FOUT.write(out_str+\'\\n\')\n    LOG_FOUT.flush()\n    print(out_str)\n\ndef get_learning_rate(batch):\n    learning_rate = tf.train.exponential_decay(\n                        BASE_LEARNING_RATE,  # Base learning rate.\n                        batch * BATCH_SIZE,  # Current index into the dataset.\n                        DECAY_STEP,          # Decay step.\n                        DECAY_RATE,          # Decay rate.\n                        staircase=True)\n    learing_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!\n    return learning_rate\n\ndef get_bn_decay(batch):\n    bn_momentum = tf.train.exponential_decay(\n                      BN_INIT_DECAY,\n                      batch*BATCH_SIZE,\n                      BN_DECAY_DECAY_STEP,\n                      BN_DECAY_DECAY_RATE,\n                      staircase=True)\n    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n    return bn_decay\n\ndef train():\n    with tf.Graph().as_default():\n        with tf.device(\'/gpu:\'+str(GPU_INDEX)):\n            pointclouds_pl, labels_pl, masks_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)\n            is_training_pl = tf.placeholder(tf.bool, shape=())\n\n            # Note the global_step=batch parameter to minimize.\n            # That tells the optimizer to helpfully increment the \'batch\' parameter for you every time it trains.\n            batch = tf.Variable(0)\n            bn_decay = get_bn_decay(batch)\n            tf.summary.scalar(\'bn_decay\', bn_decay)\n\n            print(""--- Get model and loss"")\n            # Get model and loss\n            pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)\n            loss = MODEL.get_loss(pred, labels_pl, masks_pl, end_points)\n            tf.summary.scalar(\'loss\', loss)\n\n\n            print(""--- Get training operator"")\n            # Get training operator\n            learning_rate = get_learning_rate(batch)\n            tf.summary.scalar(\'learning_rate\', learning_rate)\n            if OPTIMIZER == \'momentum\':\n                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n            elif OPTIMIZER == \'adam\':\n                optimizer = tf.train.AdamOptimizer(learning_rate)\n            train_op = optimizer.minimize(loss, global_step=batch)\n\n            # Add ops to save and restore all the variables.\n            saver = tf.train.Saver()\n\n        # Create a session\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        config.allow_soft_placement = True\n        config.log_device_placement = False\n        sess = tf.Session(config=config)\n\n        # Add summary writers\n        merged = tf.summary.merge_all()\n        train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, \'train\'), sess.graph)\n        test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, \'test\'), sess.graph)\n\n        # Init variables\n        init = tf.global_variables_initializer()\n        sess.run(init)\n\n        ops = {\'pointclouds_pl\': pointclouds_pl,\n               \'labels_pl\': labels_pl,\n               \'masks_pl\': masks_pl,\n               \'is_training_pl\': is_training_pl,\n               \'pred\': pred,\n               \'loss\': loss,\n               \'train_op\': train_op,\n               \'merged\': merged,\n               \'step\': batch,\n               \'end_points\': end_points}\n\n        for epoch in range(MAX_EPOCH):\n            log_string(\'**** EPOCH %03d ****\' % (epoch))\n            sys.stdout.flush()\n\n            train_one_epoch(sess, ops, train_writer)\n            eval_one_epoch(sess, ops, test_writer)\n\n            # Save the variables to disk.\n            if epoch % 10 == 0:\n                save_path = saver.save(sess, os.path.join(LOG_DIR, ""model.ckpt""))\n                log_string(""Model saved in file: %s"" % save_path)\n\n\ndef get_batch(dataset, idxs, start_idx, end_idx):\n    bsize = end_idx-start_idx\n    batch_data = np.zeros((bsize, NUM_POINT*2, 6))\n    batch_label = np.zeros((bsize, NUM_POINT, 3))\n    batch_mask = np.zeros((bsize, NUM_POINT))\n    # shuffle idx to change point order (change FPS behavior)\n    shuffle_idx = np.arange(NUM_POINT)\n    np.random.shuffle(shuffle_idx)\n    for i in range(bsize):\n        pc1, pc2, color1, color2, flow, mask1 = dataset[idxs[i+start_idx]]\n        # move pc1 to center\n        pc1_center = np.mean(pc1, 0)\n        pc1 -= pc1_center\n        pc2 -= pc1_center\n        batch_data[i,:NUM_POINT,:3] = pc1[shuffle_idx]\n        batch_data[i,:NUM_POINT,3:] = color1[shuffle_idx]\n        batch_data[i,NUM_POINT:,:3] = pc2[shuffle_idx]\n        batch_data[i,NUM_POINT:,3:] = color2[shuffle_idx]\n        batch_label[i] = flow[shuffle_idx]\n        batch_mask[i] = mask1[shuffle_idx]\n    return batch_data, batch_label, batch_mask\n\ndef train_one_epoch(sess, ops, train_writer):\n    """""" ops: dict mapping from string to tf ops """"""\n    is_training = True\n\n    # Shuffle train samples\n    train_idxs = np.arange(0, len(TRAIN_DATASET))\n    np.random.shuffle(train_idxs)\n    num_batches = len(TRAIN_DATASET) // BATCH_SIZE\n\n    log_string(str(datetime.now()))\n\n    loss_sum = 0\n    for batch_idx in range(num_batches):\n        start_idx = batch_idx * BATCH_SIZE\n        end_idx = (batch_idx+1) * BATCH_SIZE\n        batch_data, batch_label, batch_mask = get_batch(TRAIN_DATASET, train_idxs, start_idx, end_idx)\n\n        feed_dict = {ops[\'pointclouds_pl\']: batch_data,\n                     ops[\'labels_pl\']: batch_label,\n                     ops[\'masks_pl\']: batch_mask,\n                     ops[\'is_training_pl\']: is_training,}\n        summary, step, _, loss_val, pred_val = sess.run([ops[\'merged\'], ops[\'step\'],\n            ops[\'train_op\'], ops[\'loss\'], ops[\'pred\']], feed_dict=feed_dict)\n        train_writer.add_summary(summary, step)\n        loss_sum += loss_val\n\n        if (batch_idx+1)%10 == 0:\n            log_string(\' -- %03d / %03d --\' % (batch_idx+1, num_batches))\n            log_string(\'mean loss: %f\' % (loss_sum / 10))\n            loss_sum = 0\n\ndef eval_one_epoch(sess, ops, test_writer):\n    """""" ops: dict mapping from string to tf ops """"""\n    global EPOCH_CNT\n    is_training = False\n    test_idxs = np.arange(0, len(TEST_DATASET))\n\n    # Test on all data: last batch might be smaller than BATCH_SIZE\n    num_batches = (len(TEST_DATASET)+BATCH_SIZE-1) // BATCH_SIZE\n\n    loss_sum = 0\n    loss_sum_l2 = 0\n\n    log_string(str(datetime.now()))\n    log_string(\'---- EPOCH %03d EVALUATION ----\'%(EPOCH_CNT))\n\n    batch_data = np.zeros((BATCH_SIZE, NUM_POINT*2, 3))\n    batch_label = np.zeros((BATCH_SIZE, NUM_POINT, 3))\n    batch_mask = np.zeros((BATCH_SIZE, NUM_POINT))\n    for batch_idx in range(num_batches):\n        if batch_idx %20==0:\n            log_string(\'%03d/%03d\'%(batch_idx, num_batches))\n        start_idx = batch_idx * BATCH_SIZE\n        end_idx = min(len(TEST_DATASET), (batch_idx+1) * BATCH_SIZE)\n        cur_batch_size = end_idx-start_idx\n        cur_batch_data, cur_batch_label, cur_batch_mask = get_batch(TEST_DATASET, test_idxs, start_idx, end_idx)\n        if cur_batch_size == BATCH_SIZE:\n            batch_data = cur_batch_data\n            batch_label = cur_batch_label\n            batch_mask = cur_batch_mask\n        else:\n            batch_data[0:cur_batch_size] = cur_batch_data\n            batch_label[0:cur_batch_size] = cur_batch_label\n            batch_mask[0:cur_batch_size] = cur_batch_mask\n\n        # ---------------------------------------------------------------------\n        # ---- INFERENCE BELOW ----\n        feed_dict = {ops[\'pointclouds_pl\']: batch_data,\n                     ops[\'labels_pl\']: batch_label,\n                     ops[\'masks_pl\']: batch_mask,\n                     ops[\'is_training_pl\']: is_training}\n        summary, step, loss_val, pred_val = sess.run([ops[\'merged\'], ops[\'step\'],\n            ops[\'loss\'], ops[\'pred\']], feed_dict=feed_dict)\n        test_writer.add_summary(summary, step)\n        # ---- INFERENCE ABOVE ----\n        # ---------------------------------------------------------------------\n\n        tmp = np.sum((pred_val - batch_label)**2, 2) / 2.0\n        loss_val_np = np.mean(batch_mask * tmp)\n        if cur_batch_size==BATCH_SIZE:\n            loss_sum += loss_val\n            loss_sum_l2 += loss_val_np\n\n        # Dump some results\n        if batch_idx == 0:\n            with open(\'test_results.pkl\', \'wb\') as fp:\n                pickle.dump([batch_data, batch_label, pred_val], fp)\n\n    log_string(\'eval mean loss: %f\' % (loss_sum / float(len(TEST_DATASET)/BATCH_SIZE)))\n    log_string(\'eval mean loss: %f\' % (loss_sum_l2 / float(len(TEST_DATASET)/BATCH_SIZE)))\n\n    EPOCH_CNT += 1\n    return loss_sum/float(len(TEST_DATASET)/BATCH_SIZE)\n\n\nif __name__ == ""__main__"":\n    log_string(\'pid: %s\'%(str(os.getpid())))\n    train()\n    LOG_FOUT.close()\n'"
data_preprocessing/dbg_view.py,0,"b""\n\n\n\n\n\nimport numpy as np\nimport os\n\ndata_dir = '.'\nfilename = 'test.npz'\n# data_dir = '/scr-ssd/Projects/flownet3d_private/data_preprocessing/data_processed_maxcut_35_both_mask_20k_2k_12288'\n# filename = 'TRAIN_A_0376_right_0009-0.npz'\n# data_dir = '/scr-ssd/Projects/flownet3d_private/data_processed_maxcut_35_color_both_mask_20k_2k'\n# filename = 'TRAIN_A_0376_right_0009-2.npz'\n\ndata = np.load(os.path.join(data_dir, filename), filename)\n\npoints1 = data['points1']\npoints2 = data['points2']\ncolor1 = data['color1']\ncolor2 = data['color2']\nflow = data['flow']\nvalid_mask1 = data['valid_mask1']\n\npoints1_valid = points1[valid_mask1]\npoints1_nonvalid = points1[np.logical_not(valid_mask1)]\n\nn1_valid = points1_valid.shape[0]\nn1_nonvalid = points1_nonvalid.shape[0]\nn2 = points2.shape[0]\n\nf = open('view.pts', 'w')\n\nfor i in range(n1_valid):\n    # f.write('{} {} {} {} {} {}\\n'.format(points1_valid[i, 0], points1_valid[i, 1], points1_valid[i, 2], 2*color1[i, 0]-1, 2*color1[i, 1]-1, 2*color1[i, 2]-1))\n    f.write('{} {} {} {} {} {}\\n'.format(points1_valid[i, 0], points1_valid[i, 1], points1_valid[i, 2], 1, -1, -1))\nfor i in range(n1_nonvalid):\n    f.write('{} {} {} {} {} {}\\n'.format(points1_nonvalid[i, 0], points1_nonvalid[i, 1], points1_nonvalid[i, 2], -1, -1, -1))\n\nfor i in range(n1_valid + n1_nonvalid):\n    f.write('{} {} {} {} {} {}\\n'.format((points1 + flow)[i, 0], (points1 + flow)[i, 1], (points1 + flow)[i, 2], -1, -1, 1))\n    pass\n\nfor i in range(n2):\n    # f.write('{} {} {} {} {} {}\\n'.format(points2[i, 0], points2[i, 1], points2[i, 2], 2*color2[i, 0]-1, 2*color2[i, 1]-1, 2*color2[i, 2]-1))\n    f.write('{} {} {} {} {} {}\\n'.format(points2[i, 0], points2[i, 1], points2[i, 2], -1, 1, -1))\n    pass\n\n"""
data_preprocessing/load_pfm.py,0,"b""#!/usr/bin/python\n# Adapted from https://gist.github.com/chpatrick/8935738\n\n\nimport numpy as np\nimport re\n\ndef load_pfm(filename):\n     file = open(filename, 'r', newline='', encoding='latin-1')\n     color = None\n     width = None\n     height = None\n     scale = None\n     endian = None\n\n     header = file.readline().rstrip()\n     if header == 'PF':\n         color = True\n     elif header == 'Pf':\n         color = False\n     else:\n         raise Exception('Not a PFM file.')\n\n     dim_match = re.match(r'^(\\d+)\\s(\\d+)\\s$', file.readline())\n     if dim_match:\n         width, height = map(int, dim_match.groups())\n     else:\n         raise Exception('Malformed PFM header.')\n\n     scale = float(file.readline().rstrip())\n     if scale < 0: # little-endian\n         endian = '<'\n         scale = -scale\n     else:\n         endian = '>' # big-endian\n\n     data = np.fromfile(file, endian + 'f')\n     shape = (height, width, 3) if color else (height, width)\n\n     file.close()\n\n     return np.reshape(data, shape), scale\n\nif __name__ == '__main__':\n    import matplotlib.pyplot as plt\n    import sys\n    img, _ = load_pfm(sys.argv[1])\n    img = img[::-1, :]\n    imgplot = plt.imshow( 1050 / img)\n    plt.colorbar()\n    plt.show()\n"""
data_preprocessing/proc_dataset_gen_point_pairs_color.py,0,"b""#!/usr/bin/python\n'''\n    FlyingThings3D data preprocessing.\n'''\n\nimport numpy as np\nimport os\nimport re\nimport sys\nimport cv2\nimport glob\nimport itertools\nimport load_pfm\nimport pickle\nimport argparse\nimport random\nimport multiprocessing\n\nimport warnings\nwarnings.filterwarnings('error')\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--input_dir', default='/scr2/flyingthings3d/', type=str, help='input root dir')\nparser.add_argument('--output_dir', default='data_processed_maxcut_35_20k_2k_8192', type=str, help='output dir')\nFLAGS = parser.parse_args()\n\nINPUT_DIR = FLAGS.input_dir\nOUTPUT_DIR = FLAGS.output_dir\n\nif not os.path.exists(OUTPUT_DIR):\n    os.system('mkdir -p {}'.format(OUTPUT_DIR))\n\nnp.random.seed(0)\nrandom.seed(0)\n\ndisparity_scenes_train = glob.glob(os.path.join(INPUT_DIR, 'disparity/TRAIN/*/*/*/'))\ndisparity_scenes_test = glob.glob(os.path.join(INPUT_DIR, 'disparity/TEST/*/*/*/'))\ndisparity_scenes = random.sample(disparity_scenes_train, 2223) + random.sample(disparity_scenes_test, 223)\n\n\ndef bilinear_interp_val(vmap, y, x):\n    '''\n        bilinear interpolation on a 2D map\n    '''\n    h, w = vmap.shape\n    x1 = int(x)\n    x2 = x1 + 1\n    x2 = w-1 if x2 > (w-1) else x2\n    y1 = int(y)\n    y2 = y1 + 1\n    y2 = h-1 if y2 > (h-1) else y2\n    Q11 = vmap[y1,x1]\n    Q21 = vmap[y1,x2]\n    Q12 = vmap[y2,x1]\n    Q22 = vmap[y2,x2]\n    return Q11 * (x2-x) * (y2-y) + Q21 * (x-x1) * (y2-y) + Q12 * (x2-x) * (y-y1) + Q22 * (x-x1) * (y-y1)\n\ndef get_3d_pos_xy(y_prime, x_prime, depth, focal_length=1050., w=960, h=540):\n    '''\n        depth pop up\n    '''\n    y = (y_prime - h / 2.) * depth / focal_length\n    x = (x_prime - w / 2.) * depth / focal_length\n    return [x, y, depth]\n\ndef gen_datapoint(fname_disparity, fname_disparity_next_frame, fname_disparity_change, fname_optical_flow, image, image_next_frame, n = 8192, max_cut = 35, focal_length=1050.):\n\n    np.random.seed(0)\n\n    ##### generate needed data\n    disparity_np, _ = load_pfm.load_pfm(fname_disparity)\n    disparity_next_frame_np, _ = load_pfm.load_pfm(fname_disparity_next_frame)\n    disparity_change_np, _ = load_pfm.load_pfm(fname_disparity_change)\n    optical_flow_np, _ = load_pfm.load_pfm(fname_optical_flow)\n    rgb_np = cv2.imread(image)[:, :, ::-1] / 255.\n    rgb_next_frame_np = cv2.imread(image_next_frame)[:, :, ::-1] / 255.\n\n    depth_np = focal_length / disparity_np\n    depth_next_frame_np = focal_length / disparity_next_frame_np\n    future_depth_np = focal_length / (disparity_np + disparity_change_np)\n    ##### generate needed data\n    h, w = disparity_np.shape\n\n    ##### point set 1 current pos\n    try:\n        depth_requirement = depth_np < max_cut\n    except:\n        return None\n\n    satisfy_pix1 = np.column_stack(np.where(depth_requirement))\n    if satisfy_pix1.shape[0] < n:\n        return None\n    sample_choice1 = np.random.choice(satisfy_pix1.shape[0], size=n, replace=False)\n    sampled_pix1_y = satisfy_pix1[sample_choice1, 0]\n    sampled_pix1_x = satisfy_pix1[sample_choice1, 1]\n\n    current_pos1 = np.array([get_3d_pos_xy( sampled_pix1_y[i], sampled_pix1_x[i], depth_np[int(sampled_pix1_y[i]), int(sampled_pix1_x[i])] ) for i in range(n)])\n    current_rgb1 = np.array([[rgb_np[h-1-int(sampled_pix1_y[i]), int(sampled_pix1_x[i]), 0], rgb_np[h-1-int(sampled_pix1_y[i]), int(sampled_pix1_x[i]), 1], rgb_np[h-1-int(sampled_pix1_y[i]), int(sampled_pix1_x[i]), 2]] for i in range(n)])\n    ##### point set 1 current pos\n\n    ##### point set 1 future pos\n    sampled_optical_flow_x = np.array([ optical_flow_np[ int( sampled_pix1_y[i] ), int( sampled_pix1_x[i] ) ][0] for i in range(n)])\n    sampled_optical_flow_y = np.array([ optical_flow_np[ int( sampled_pix1_y[i] ), int( sampled_pix1_x[i] ) ][1] for i in range(n)])\n    future_pix1_x = sampled_pix1_x + sampled_optical_flow_x\n    future_pix1_y = sampled_pix1_y - sampled_optical_flow_y\n    future_pos1 = np.array([get_3d_pos_xy( future_pix1_y[i], future_pix1_x[i], future_depth_np[int(sampled_pix1_y[i]), int(sampled_pix1_x[i])] ) for i in range(n)])\n    ##### point set 1 future pos\n\n    flow = future_pos1 - current_pos1\n\n    ##### point set 2 current pos\n    try:\n        depth_requirement = depth_next_frame_np < max_cut\n    except:\n        return None\n\n    satisfy_pix2 = np.column_stack(np.where(depth_next_frame_np < max_cut))\n    if satisfy_pix2.shape[0] < n:\n        return None\n    sample_choice2 = np.random.choice(satisfy_pix2.shape[0], size=n, replace=False)\n    sampled_pix2_y = satisfy_pix2[sample_choice2, 0]\n    sampled_pix2_x = satisfy_pix2[sample_choice2, 1]\n\n    current_pos2 = np.array([get_3d_pos_xy( sampled_pix2_y[i], sampled_pix2_x[i], depth_next_frame_np[int(sampled_pix2_y[i]), int(sampled_pix2_x[i])] ) for i in range(n)])\n    current_rgb2 = np.array([[rgb_next_frame_np[h-1-int(sampled_pix2_y[i]), int(sampled_pix2_x[i]), 0], rgb_next_frame_np[h-1-int(sampled_pix2_y[i]), int(sampled_pix2_x[i]), 1], rgb_next_frame_np[h-1-int(sampled_pix2_y[i]), int(sampled_pix2_x[i]), 2]] for i in range(n)])\n    ##### point set 2 current pos\n\n    ##### mask, judge whether point move out of fov or occluded by other object after motion\n    future_pos1_depth = future_depth_np[sampled_pix1_y, sampled_pix1_x]\n    future_pos1_foreground_depth = np.zeros_like(future_pos1_depth)\n    valid_mask_fov1 = np.ones_like(future_pos1_depth, dtype=bool)\n    for i in range(future_pos1_depth.shape[0]):\n        if future_pix1_y[i] > 0 and future_pix1_y[i] < h and future_pix1_x[i] > 0 and future_pix1_x[i] < w:\n            future_pos1_foreground_depth[i] = bilinear_interp_val(depth_next_frame_np, future_pix1_y[i], future_pix1_x[i])\n        else:\n            valid_mask_fov1[i] = False\n    valid_mask_occ1 = (future_pos1_foreground_depth - future_pos1_depth) > -5e-1\n\n    mask1 = valid_mask_occ1 & valid_mask_fov1\n    ##### mask, judge whether point move out of fov or occluded by other object after motion\n\n    return current_pos1, current_pos2, current_rgb1, current_rgb2, flow, mask1\n\ndisparity =                 os.path.join(INPUT_DIR, 'disparity/TRAIN/A/0376/right/0009.pfm')\ndisparity_next_frame =      os.path.join(INPUT_DIR, 'disparity/TRAIN/A/0376/right/0010.pfm')\ndisparity_change =          os.path.join(INPUT_DIR, 'disparity_change/TRAIN/A/0376/into_future/right/0009.pfm')\noptical_flow =              os.path.join(INPUT_DIR, 'optical_flow/TRAIN/A/0376/into_future/right/OpticalFlowIntoFuture_0009_R.pfm')\nimage =                     os.path.join(INPUT_DIR, 'frames_finalpass/TRAIN/A/0376/right/0009.png')\nimage_next_frame =          os.path.join(INPUT_DIR, 'frames_finalpass/TRAIN/A/0376/right/0010.png')\n\nd = gen_datapoint(disparity, disparity_next_frame, disparity_change, optical_flow, image, image_next_frame)\nnp.savez_compressed('test.npz', points1=d[0], \\\n                                points2=d[1], \\\n                                color1=d[2], \\\n                                color2=d[3], \\\n                                flow=d[4], \\\n                                valid_mask1=d[5] )\n\ndef proc_one_scene(s, input_dir, output_dir):\n    if s[-1] == '/':\n        s = s[:-1]\n    dis_split = s.split('/')\n    train_or_test = dis_split[-4]\n    ABC = dis_split[-3]\n    scene_idx = dis_split[-2]\n    left_right = dis_split[-1]\n    for v in range(6, 15):\n        fname = os.path.join(output_dir, train_or_test + '_' + ABC + '_' + scene_idx + '_' + left_right + '_' + str(v).zfill(4) + '-{}'.format(0) + '.npz')\n        if os.path.exists(fname):\n            continue\n\n        fname_disparity = os.path.join(input_dir, 'disparity', train_or_test, ABC, scene_idx, left_right, str(v).zfill(4) + '.pfm')\n        fname_disparity_next_frame = os.path.join(input_dir, 'disparity', train_or_test, ABC, scene_idx, left_right, str(v+1).zfill(4) + '.pfm')\n        fname_image = os.path.join(input_dir, 'frames_finalpass', train_or_test, ABC, scene_idx, left_right, str(v).zfill(4) + '.png')\n        fname_image_next_frame = os.path.join(input_dir, 'frames_finalpass', train_or_test, ABC, scene_idx, left_right, str(v+1).zfill(4) + '.png')\n        fname_disparity_change = os.path.join(input_dir, 'disparity_change', train_or_test, ABC, scene_idx, 'into_future', left_right, str(v).zfill(4) + '.pfm')\n        L_R = 'L' if left_right == 'left' else 'R'\n        fname_optical_flow = os.path.join(input_dir, 'optical_flow', train_or_test, ABC, scene_idx, 'into_future', left_right, 'OpticalFlowIntoFuture_' + str(v).zfill(4) + '_' + L_R + '.pfm')\n\n        d = gen_datapoint(fname_disparity, fname_disparity_next_frame, fname_disparity_change, fname_optical_flow, fname_image, fname_image_next_frame, focal_length=1050.)\n        if d is not None:\n            np.savez_compressed(fname, points1=d[0], \\\n                                       points2=d[1], \\\n                                       color1=d[2], \\\n                                       color2=d[3], \\\n                                       flow=d[4], \\\n                                       valid_mask1=d[5] )\n\n\npool = multiprocessing.Pool(processes=8)\n\nfor s in disparity_scenes:\n    print(s)\n    # proc_one_scene(s, INPUT_DIR, OUTPUT_DIR)\n    pool.apply_async(proc_one_scene, (s, INPUT_DIR, OUTPUT_DIR))\n\npool.close()\npool.join()\n\n\n"""
utils/pointnet_util.py,63,"b'"""""" PointNet++ Layers\n\nOriginal Author: Charles R. Qi\nModified by Xingyu Liu\nDate: April 2019\n""""""\n\nimport os\nimport sys\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nROOT_DIR = os.path.dirname(BASE_DIR)\nsys.path.append(os.path.join(ROOT_DIR, \'utils\'))\nsys.path.append(os.path.join(ROOT_DIR, \'tf_ops/sampling\'))\nsys.path.append(os.path.join(ROOT_DIR, \'tf_ops/grouping\'))\nsys.path.append(os.path.join(ROOT_DIR, \'tf_ops/3d_interpolation\'))\nfrom tf_sampling import farthest_point_sample, gather_point\nfrom tf_grouping import query_ball_point, group_point, knn_point\nfrom tf_interpolate import three_nn, three_interpolate\nimport tensorflow as tf\nimport numpy as np\nimport tf_util\n\ndef sample_and_group(npoint, radius, nsample, xyz, points, knn=False, use_xyz=True):\n    \'\'\'\n    Input:\n        npoint: int32\n        radius: float32\n        nsample: int32\n        xyz: (batch_size, ndataset, 3) TF tensor\n        points: (batch_size, ndataset, channel) TF tensor, if None will just use xyz as points\n        knn: bool, if True use kNN instead of radius search\n        use_xyz: bool, if True concat XYZ with local point features, otherwise just use point features\n    Output:\n        new_xyz: (batch_size, npoint, 3) TF tensor\n        new_points: (batch_size, npoint, nsample, 3+channel) TF tensor\n        idx: (batch_size, npoint, nsample) TF tensor, indices of local points as in ndataset points\n        grouped_xyz: (batch_size, npoint, nsample, 3) TF tensor, normalized point XYZs\n            (subtracted by seed point XYZ) in local regions\n    \'\'\'\n\n    new_xyz = gather_point(xyz, farthest_point_sample(npoint, xyz)) # (batch_size, npoint, 3)\n    if knn:\n        _,idx = knn_point(nsample, xyz, new_xyz)\n    else:\n        idx, pts_cnt = query_ball_point(radius, nsample, xyz, new_xyz)\n    grouped_xyz = group_point(xyz, idx) # (batch_size, npoint, nsample, 3)\n    grouped_xyz -= tf.tile(tf.expand_dims(new_xyz, 2), [1,1,nsample,1]) # translation normalization\n    if points is not None:\n        grouped_points = group_point(points, idx) # (batch_size, npoint, nsample, channel)\n        if use_xyz:\n            new_points = tf.concat([grouped_xyz, grouped_points], axis=-1) # (batch_size, npoint, nample, 3+channel)\n        else:\n            new_points = grouped_points\n    else:\n        new_points = grouped_xyz\n\n    return new_xyz, new_points, idx, grouped_xyz\n\n\ndef sample_and_group_all(xyz, points, use_xyz=True):\n    \'\'\'\n    Inputs:\n        xyz: (batch_size, ndataset, 3) TF tensor\n        points: (batch_size, ndataset, channel) TF tensor, if None will just use xyz as points\n        use_xyz: bool, if True concat XYZ with local point features, otherwise just use point features\n    Outputs:\n        new_xyz: (batch_size, 1, 3) as (0,0,0)\n        new_points: (batch_size, 1, ndataset, 3+channel) TF tensor\n    Note:\n        Equivalent to sample_and_group with npoint=1, radius=inf, use (0,0,0) as the centroid\n    \'\'\'\n    batch_size = xyz.get_shape()[0].value\n    nsample = xyz.get_shape()[1].value\n    new_xyz = tf.constant(np.tile(np.array([0,0,0]).reshape((1,1,3)), (batch_size,1,1)),dtype=tf.float32) # (batch_size, 1, 3)\n    idx = tf.constant(np.tile(np.array(range(nsample)).reshape((1,1,nsample)), (batch_size,1,1)))\n    grouped_xyz = tf.reshape(xyz, (batch_size, 1, nsample, 3)) # (batch_size, npoint=1, nsample, 3)\n    if points is not None:\n        if use_xyz:\n            new_points = tf.concat([xyz, points], axis=2) # (batch_size, 16, 259)\n        else:\n            new_points = points\n        new_points = tf.expand_dims(new_points, 1) # (batch_size, 1, 16, 259)\n    else:\n        new_points = grouped_xyz\n    return new_xyz, new_points, idx, grouped_xyz\n\n\ndef pointnet_sa_module(xyz, points, npoint, radius, nsample, mlp, mlp2, group_all, is_training, bn_decay, scope, bn=True, pooling=\'max\', knn=False, use_xyz=True, use_nchw=False):\n    \'\'\' PointNet Set Abstraction (SA) Module\n        Input:\n            xyz: (batch_size, ndataset, 3) TF tensor\n            points: (batch_size, ndataset, channel) TF tensor\n            npoint: int32 -- #points sampled in farthest point sampling\n            radius: float32 -- search radius in local region\n            nsample: int32 -- how many points in each local region\n            mlp: list of int32 -- output size for MLP on each point\n            mlp2: list of int32 -- output size for MLP on each region\n            group_all: bool -- group all points into one PC if set true, OVERRIDE\n                npoint, radius and nsample settings\n            use_xyz: bool, if True concat XYZ with local point features, otherwise just use point features\n            use_nchw: bool, if True, use NCHW data format for conv2d, which is usually faster than NHWC format\n        Return:\n            new_xyz: (batch_size, npoint, 3) TF tensor\n            new_points: (batch_size, npoint, mlp[-1] or mlp2[-1]) TF tensor\n            idx: (batch_size, npoint, nsample) int32 -- indices for local regions\n    \'\'\'\n    data_format = \'NCHW\' if use_nchw else \'NHWC\'\n    with tf.variable_scope(scope) as sc:\n        # Sample and Grouping\n        if group_all:\n            nsample = xyz.get_shape()[1].value\n            new_xyz, new_points, idx, grouped_xyz = sample_and_group_all(xyz, points, use_xyz)\n        else:\n            new_xyz, new_points, idx, grouped_xyz = sample_and_group(npoint, radius, nsample, xyz, points, knn, use_xyz)\n\n        # Point Feature Embedding\n        if use_nchw: new_points = tf.transpose(new_points, [0,3,1,2])\n        for i, num_out_channel in enumerate(mlp):\n            new_points = tf_util.conv2d(new_points, num_out_channel, [1,1],\n                                        padding=\'VALID\', stride=[1,1],\n                                        bn=bn, is_training=is_training,\n                                        scope=\'conv%d\'%(i), bn_decay=bn_decay,\n                                        data_format=data_format)\n        if use_nchw: new_points = tf.transpose(new_points, [0,2,3,1])\n\n        # Pooling in Local Regions\n        if pooling==\'max\':\n            new_points = tf.reduce_max(new_points, axis=[2], keep_dims=True, name=\'maxpool\')\n        elif pooling==\'avg\':\n            new_points = tf.reduce_mean(new_points, axis=[2], keep_dims=True, name=\'avgpool\')\n        elif pooling==\'weighted_avg\':\n            with tf.variable_scope(\'weighted_avg\'):\n                dists = tf.norm(grouped_xyz,axis=-1,ord=2,keep_dims=True)\n                exp_dists = tf.exp(-dists * 5)\n                weights = exp_dists/tf.reduce_sum(exp_dists,axis=2,keep_dims=True) # (batch_size, npoint, nsample, 1)\n                new_points *= weights # (batch_size, npoint, nsample, mlp[-1])\n                new_points = tf.reduce_sum(new_points, axis=2, keep_dims=True)\n        elif pooling==\'max_and_avg\':\n            max_points = tf.reduce_max(new_points, axis=[2], keep_dims=True, name=\'maxpool\')\n            avg_points = tf.reduce_mean(new_points, axis=[2], keep_dims=True, name=\'avgpool\')\n            new_points = tf.concat([avg_points, max_points], axis=-1)\n\n        # [Optional] Further Processing\n        if mlp2 is not None:\n            if use_nchw: new_points = tf.transpose(new_points, [0,3,1,2])\n            for i, num_out_channel in enumerate(mlp2):\n                new_points = tf_util.conv2d(new_points, num_out_channel, [1,1],\n                                            padding=\'VALID\', stride=[1,1],\n                                            bn=bn, is_training=is_training,\n                                            scope=\'conv_post_%d\'%(i), bn_decay=bn_decay,\n                                            data_format=data_format)\n            if use_nchw: new_points = tf.transpose(new_points, [0,2,3,1])\n\n        new_points = tf.squeeze(new_points, [2]) # (batch_size, npoints, mlp2[-1])\n        return new_xyz, new_points, idx\n\ndef pointnet_sa_module_msg(xyz, points, npoint, radius_list, nsample_list, mlp_list, is_training, bn_decay, scope, bn=True, use_xyz=True, use_nchw=False):\n    \'\'\' PointNet Set Abstraction (SA) module with Multi-Scale Grouping (MSG)\n        Input:\n            xyz: (batch_size, ndataset, 3) TF tensor\n            points: (batch_size, ndataset, channel) TF tensor\n            npoint: int32 -- #points sampled in farthest point sampling\n            radius: list of float32 -- search radius in local region\n            nsample: list of int32 -- how many points in each local region\n            mlp: list of list of int32 -- output size for MLP on each point\n            use_xyz: bool, if True concat XYZ with local point features, otherwise just use point features\n            use_nchw: bool, if True, use NCHW data format for conv2d, which is usually faster than NHWC format\n        Return:\n            new_xyz: (batch_size, npoint, 3) TF tensor\n            new_points: (batch_size, npoint, \\sum_k{mlp[k][-1]}) TF tensor\n    \'\'\'\n    data_format = \'NCHW\' if use_nchw else \'NHWC\'\n    with tf.variable_scope(scope) as sc:\n        new_xyz = gather_point(xyz, farthest_point_sample(npoint, xyz))\n        new_points_list = []\n        for i in range(len(radius_list)):\n            radius = radius_list[i]\n            nsample = nsample_list[i]\n            idx, pts_cnt = query_ball_point(radius, nsample, xyz, new_xyz)\n            grouped_xyz = group_point(xyz, idx)\n            grouped_xyz -= tf.tile(tf.expand_dims(new_xyz, 2), [1,1,nsample,1])\n            if points is not None:\n                grouped_points = group_point(points, idx)\n                if use_xyz:\n                    grouped_points = tf.concat([grouped_points, grouped_xyz], axis=-1)\n            else:\n                grouped_points = grouped_xyz\n            if use_nchw: grouped_points = tf.transpose(grouped_points, [0,3,1,2])\n            for j,num_out_channel in enumerate(mlp_list[i]):\n                grouped_points = tf_util.conv2d(grouped_points, num_out_channel, [1,1],\n                                                padding=\'VALID\', stride=[1,1], bn=bn, is_training=is_training,\n                                                scope=\'conv%d_%d\'%(i,j), bn_decay=bn_decay)\n            if use_nchw: grouped_points = tf.transpose(grouped_points, [0,2,3,1])\n            new_points = tf.reduce_max(grouped_points, axis=[2])\n            new_points_list.append(new_points)\n        new_points_concat = tf.concat(new_points_list, axis=-1)\n        return new_xyz, new_points_concat\n\n\ndef pointnet_fp_module(xyz1, xyz2, points1, points2, mlp, is_training, bn_decay, scope, bn=True, last_mlp_activation=True):\n    \'\'\' PointNet Feature Propogation (FP) Module\n        Input:\n            xyz1: (batch_size, ndataset1, 3) TF tensor\n            xyz2: (batch_size, ndataset2, 3) TF tensor, sparser than xyz1\n            points1: (batch_size, ndataset1, nchannel1) TF tensor\n            points2: (batch_size, ndataset2, nchannel2) TF tensor\n            mlp: list of int32 -- output size for MLP on each point\n        Return:\n            new_points: (batch_size, ndataset1, mlp[-1]) TF tensor\n    \'\'\'\n    with tf.variable_scope(scope) as sc:\n        dist, idx = three_nn(xyz1, xyz2)\n        dist = tf.maximum(dist, 1e-10)\n        norm = tf.reduce_sum((1.0/dist),axis=2,keep_dims=True)\n        norm = tf.tile(norm,[1,1,3])\n        weight = (1.0/dist) / norm\n        interpolated_points = three_interpolate(points2, idx, weight)\n\n        if points1 is not None:\n            new_points1 = tf.concat(axis=2, values=[interpolated_points, points1]) # B,ndataset1,nchannel1+nchannel2\n        else:\n            new_points1 = interpolated_points\n        new_points1 = tf.expand_dims(new_points1, 2)\n        for i, num_out_channel in enumerate(mlp):\n            if i == len(mlp)-1 and not(last_mlp_activation):\n                activation_fn = None\n            else:\n                activation_fn = tf.nn.relu\n            new_points1 = tf_util.conv2d(new_points1, num_out_channel, [1,1],\n                                         padding=\'VALID\', stride=[1,1],\n                                         bn=bn, is_training=is_training,\n                                         scope=\'conv_%d\'%(i), bn_decay=bn_decay, activation_fn=activation_fn)\n        new_points1 = tf.squeeze(new_points1, [2]) # B,ndataset1,mlp[-1]\n        return new_points1\n\ndef flow_embedding_module(xyz1, xyz2, feat1, feat2, radius, nsample, mlp, is_training, bn_decay, scope, bn=True, pooling=\'max\', knn=True, corr_func=\'elementwise_product\'):\n    """"""\n    Input:\n        xyz1: (batch_size, npoint, 3)\n        xyz2: (batch_size, npoint, 3)\n        feat1: (batch_size, npoint, channel)\n        feat2: (batch_size, npoint, channel)\n    Output:\n        xyz1: (batch_size, npoint, 3)\n        feat1_new: (batch_size, npoint, mlp[-1])\n    """"""\n    if knn:\n        _, idx = knn_point(nsample, xyz2, xyz1)\n    else:\n        idx, cnt = query_ball_point(radius, nsample, xyz2, xyz1)\n        _, idx_knn = knn_point(nsample, xyz2, xyz1)\n        cnt = tf.tile(tf.expand_dims(cnt, -1), [1,1,nsample])\n        idx = tf.where(cnt > (nsample-1), idx, idx_knn)\n\n    xyz2_grouped = group_point(xyz2, idx) # batch_size, npoint, nsample, 3\n    xyz1_expanded = tf.expand_dims(xyz1, 2) # batch_size, npoint, 1, 3\n    xyz_diff = xyz2_grouped - xyz1_expanded # batch_size, npoint, nsample, 3\n\n    feat2_grouped = group_point(feat2, idx) # batch_size, npoint, nsample, channel\n    feat1_expanded = tf.expand_dims(feat1, 2) # batch_size, npoint, 1, channel\n    # TODO: change distance function\n    if corr_func == \'elementwise_product\':\n        feat_diff = feat2_grouped * feat1_expanded # batch_size, npoint, nsample, channel\n    elif corr_func == \'concat\':\n        feat_diff = tf.concat(axis=-1, values=[feat2_grouped, tf.tile(feat1_expanded,[1,1,nsample,1])]) # batch_size, npoint, sample, channel*2\n    elif corr_func == \'dot_product\':\n        feat_diff = tf.reduce_sum(feat2_grouped * feat1_expanded, axis=[-1], keep_dims=True) # batch_size, npoint, nsample, 1\n    elif corr_func == \'cosine_dist\':\n        feat2_grouped = tf.nn.l2_normalize(feat2_grouped, -1)\n        feat1_expanded = tf.nn.l2_normalize(feat1_expanded, -1)\n        feat_diff = tf.reduce_sum(feat2_grouped * feat1_expanded, axis=[-1], keep_dims=True) # batch_size, npoint, nsample, 1\n    elif corr_func == \'flownet_like\': # assuming square patch size k = 0 as the FlowNet paper\n        batch_size = xyz1.get_shape()[0].value\n        npoint = xyz1.get_shape()[1].value\n        feat_diff = tf.reduce_sum(feat2_grouped * feat1_expanded, axis=[-1], keep_dims=True) # batch_size, npoint, nsample, 1\n        total_diff = tf.concat(axis=-1, values=[xyz_diff, feat_diff]) # batch_size, npoint, nsample, 4\n        feat1_new = tf.reshape(total_diff, [batch_size, npoint, -1]) # batch_size, npoint, nsample*4\n        #feat1_new = tf.concat(axis=[-1], values=[feat1_new, feat1]) # batch_size, npoint, nsample*4+channel\n        return xyz1, feat1_new\n\n\n    feat1_new = tf.concat([feat_diff, xyz_diff], axis=3) # batch_size, npoint, nsample, [channel or 1] + 3\n    # TODO: move scope to outer indent\n    with tf.variable_scope(scope) as sc:\n        for i, num_out_channel in enumerate(mlp):\n            feat1_new = tf_util.conv2d(feat1_new, num_out_channel, [1,1],\n                                       padding=\'VALID\', stride=[1,1],\n                                       bn=True, is_training=is_training,\n                                       scope=\'conv_diff_%d\'%(i), bn_decay=bn_decay)\n    if pooling==\'max\':\n        feat1_new = tf.reduce_max(feat1_new, axis=[2], keep_dims=False, name=\'maxpool_diff\')\n    elif pooling==\'avg\':\n        feat1_new = tf.reduce_mean(feat1_new, axis=[2], keep_dims=False, name=\'avgpool_diff\')\n    return xyz1, feat1_new\n\ndef set_upconv_module(xyz1, xyz2, feat1, feat2, nsample, mlp, mlp2, is_training, scope, bn_decay=None, bn=True, pooling=\'max\', radius=None, knn=True):\n    """"""\n        Feature propagation from xyz2 (less points) to xyz1 (more points)\n\n    Inputs:\n        xyz1: (batch_size, npoint1, 3)\n        xyz2: (batch_size, npoint2, 3)\n        feat1: (batch_size, npoint1, channel1) features for xyz1 points (earlier layers)\n        feat2: (batch_size, npoint2, channel2) features for xyz2 points\n    Output:\n        feat1_new: (batch_size, npoint2, mlp[-1] or mlp2[-1] or channel1+3)\n\n        TODO: Add support for skip links. Study how delta(XYZ) plays a role in feature updating.\n    """"""\n    with tf.variable_scope(scope) as sc:\n        if knn:\n            l2_dist, idx = knn_point(nsample, xyz2, xyz1)\n        else:\n            idx, pts_cnt = query_ball_point(radius, nsample, xyz2, xyz1)\n        xyz2_grouped = group_point(xyz2, idx) # batch_size, npoint1, nsample, 3\n        xyz1_expanded = tf.expand_dims(xyz1, 2) # batch_size, npoint1, 1, 3\n        xyz_diff = xyz2_grouped - xyz1_expanded # batch_size, npoint1, nsample, 3\n\n        feat2_grouped = group_point(feat2, idx) # batch_size, npoint1, nsample, channel2\n        net = tf.concat([feat2_grouped, xyz_diff], axis=3) # batch_size, npoint1, nsample, channel2+3\n\n        if mlp is None: mlp=[]\n        for i, num_out_channel in enumerate(mlp):\n            net = tf_util.conv2d(net, num_out_channel, [1,1],\n                                 padding=\'VALID\', stride=[1,1],\n                                 bn=True, is_training=is_training,\n                                 scope=\'conv%d\'%(i), bn_decay=bn_decay)\n        if pooling==\'max\':\n            feat1_new = tf.reduce_max(net, axis=[2], keep_dims=False, name=\'maxpool\') # batch_size, npoint1, mlp[-1]\n        elif pooling==\'avg\':\n            feat1_new = tf.reduce_mean(net, axis=[2], keep_dims=False, name=\'avgpool\') # batch_size, npoint1, mlp[-1]\n\n        if feat1 is not None:\n            feat1_new = tf.concat([feat1_new, feat1], axis=2) # batch_size, npoint1, mlp[-1]+channel1\n\n        feat1_new = tf.expand_dims(feat1_new, 2) # batch_size, npoint1, 1, mlp[-1]+channel2\n        if mlp2 is None: mlp2=[]\n        for i, num_out_channel in enumerate(mlp2):\n            feat1_new = tf_util.conv2d(feat1_new, num_out_channel, [1,1],\n                                       padding=\'VALID\', stride=[1,1],\n                                       bn=True, is_training=is_training,\n                                       scope=\'post-conv%d\'%(i), bn_decay=bn_decay)\n        feat1_new = tf.squeeze(feat1_new, [2]) # batch_size, npoint1, mlp2[-1]\n        return feat1_new\n\n'"
utils/tf_util.py,63,"b'"""""" Wrapper functions for TensorFlow layers.\n\nAuthor: Charles R. Qi\nDate: November 2017\n""""""\n\nimport numpy as np\nimport tensorflow as tf\n\ndef _variable_on_cpu(name, shape, initializer, use_fp16=False):\n  """"""Helper to create a Variable stored on CPU memory.\n  Args:\n    name: name of the variable\n    shape: list of ints\n    initializer: initializer for Variable\n  Returns:\n    Variable Tensor\n  """"""\n  with tf.device(""/cpu:0""):\n    dtype = tf.float16 if use_fp16 else tf.float32\n    var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n  return var\n\ndef _variable_with_weight_decay(name, shape, stddev, wd, use_xavier=True):\n  """"""Helper to create an initialized Variable with weight decay.\n\n  Note that the Variable is initialized with a truncated normal distribution.\n  A weight decay is added only if one is specified.\n\n  Args:\n    name: name of the variable\n    shape: list of ints\n    stddev: standard deviation of a truncated Gaussian\n    wd: add L2Loss weight decay multiplied by this float. If None, weight\n        decay is not added for this Variable.\n    use_xavier: bool, whether to use xavier initializer\n\n  Returns:\n    Variable Tensor\n  """"""\n  if use_xavier:\n    initializer = tf.contrib.layers.xavier_initializer()\n  else:\n    initializer = tf.truncated_normal_initializer(stddev=stddev)\n  var = _variable_on_cpu(name, shape, initializer)\n  if wd is not None:\n    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name=\'weight_loss\')\n    tf.add_to_collection(\'losses\', weight_decay)\n  return var\n\n\ndef conv1d(inputs,\n           num_output_channels,\n           kernel_size,\n           scope,\n           stride=1,\n           padding=\'SAME\',\n           data_format=\'NHWC\',\n           use_xavier=True,\n           stddev=1e-3,\n           weight_decay=None,\n           activation_fn=tf.nn.relu,\n           bn=False,\n           bn_decay=None,\n           is_training=None):\n  """""" 1D convolution with non-linear operation.\n\n  Args:\n    inputs: 3-D tensor variable BxLxC\n    num_output_channels: int\n    kernel_size: int\n    scope: string\n    stride: int\n    padding: \'SAME\' or \'VALID\'\n    data_format: \'NHWC\' or \'NCHW\'\n    use_xavier: bool, use xavier_initializer if true\n    stddev: float, stddev for truncated_normal init\n    weight_decay: float\n    activation_fn: function\n    bn: bool, whether to use batch norm\n    bn_decay: float or float tensor variable in [0,1]\n    is_training: bool Tensor variable\n\n  Returns:\n    Variable tensor\n  """"""\n  with tf.variable_scope(scope) as sc:\n    assert(data_format==\'NHWC\' or data_format==\'NCHW\')\n    if data_format == \'NHWC\':\n      num_in_channels = inputs.get_shape()[-1].value\n    elif data_format==\'NCHW\':\n      num_in_channels = inputs.get_shape()[1].value\n    kernel_shape = [kernel_size,\n                    num_in_channels, num_output_channels]\n    kernel = _variable_with_weight_decay(\'weights\',\n                                         shape=kernel_shape,\n                                         use_xavier=use_xavier,\n                                         stddev=stddev,\n                                         wd=weight_decay)\n    outputs = tf.nn.conv1d(inputs, kernel,\n                           stride=stride,\n                           padding=padding,\n                           data_format=data_format)\n    biases = _variable_on_cpu(\'biases\', [num_output_channels],\n                              tf.constant_initializer(0.0))\n    outputs = tf.nn.bias_add(outputs, biases, data_format=data_format)\n\n    if bn:\n      outputs = batch_norm_for_conv1d(outputs, is_training,\n                                      bn_decay=bn_decay, scope=\'bn\',\n                                      data_format=data_format)\n\n    if activation_fn is not None:\n      outputs = activation_fn(outputs)\n    return outputs\n\n\n\n\ndef conv2d(inputs,\n           num_output_channels,\n           kernel_size,\n           scope,\n           stride=[1, 1],\n           padding=\'SAME\',\n           data_format=\'NHWC\',\n           use_xavier=True,\n           stddev=1e-3,\n           weight_decay=None,\n           activation_fn=tf.nn.relu,\n           bn=False,\n           bn_decay=None,\n           is_training=None):\n  """""" 2D convolution with non-linear operation.\n\n  Args:\n    inputs: 4-D tensor variable BxHxWxC\n    num_output_channels: int\n    kernel_size: a list of 2 ints\n    scope: string\n    stride: a list of 2 ints\n    padding: \'SAME\' or \'VALID\'\n    data_format: \'NHWC\' or \'NCHW\'\n    use_xavier: bool, use xavier_initializer if true\n    stddev: float, stddev for truncated_normal init\n    weight_decay: float\n    activation_fn: function\n    bn: bool, whether to use batch norm\n    bn_decay: float or float tensor variable in [0,1]\n    is_training: bool Tensor variable\n\n  Returns:\n    Variable tensor\n  """"""\n  with tf.variable_scope(scope) as sc:\n      kernel_h, kernel_w = kernel_size\n      assert(data_format==\'NHWC\' or data_format==\'NCHW\')\n      if data_format == \'NHWC\':\n        num_in_channels = inputs.get_shape()[-1].value\n      elif data_format==\'NCHW\':\n        num_in_channels = inputs.get_shape()[1].value\n      kernel_shape = [kernel_h, kernel_w,\n                      num_in_channels, num_output_channels]\n      kernel = _variable_with_weight_decay(\'weights\',\n                                           shape=kernel_shape,\n                                           use_xavier=use_xavier,\n                                           stddev=stddev,\n                                           wd=weight_decay)\n      stride_h, stride_w = stride\n      outputs = tf.nn.conv2d(inputs, kernel,\n                             [1, stride_h, stride_w, 1],\n                             padding=padding,\n                             data_format=data_format)\n      biases = _variable_on_cpu(\'biases\', [num_output_channels],\n                                tf.constant_initializer(0.0))\n      outputs = tf.nn.bias_add(outputs, biases, data_format=data_format)\n\n      if bn:\n        outputs = batch_norm_for_conv2d(outputs, is_training,\n                                        bn_decay=bn_decay, scope=\'bn\',\n                                        data_format=data_format)\n\n      if activation_fn is not None:\n        outputs = activation_fn(outputs)\n      return outputs\n\n\ndef conv2d_transpose(inputs,\n                     num_output_channels,\n                     kernel_size,\n                     scope,\n                     stride=[1, 1],\n                     padding=\'SAME\',\n                     use_xavier=True,\n                     stddev=1e-3,\n                     weight_decay=None,\n                     activation_fn=tf.nn.relu,\n                     bn=False,\n                     bn_decay=None,\n                     is_training=None):\n  """""" 2D convolution transpose with non-linear operation.\n\n  Args:\n    inputs: 4-D tensor variable BxHxWxC\n    num_output_channels: int\n    kernel_size: a list of 2 ints\n    scope: string\n    stride: a list of 2 ints\n    padding: \'SAME\' or \'VALID\'\n    use_xavier: bool, use xavier_initializer if true\n    stddev: float, stddev for truncated_normal init\n    weight_decay: float\n    activation_fn: function\n    bn: bool, whether to use batch norm\n    bn_decay: float or float tensor variable in [0,1]\n    is_training: bool Tensor variable\n\n  Returns:\n    Variable tensor\n\n  Note: conv2d(conv2d_transpose(a, num_out, ksize, stride), a.shape[-1], ksize, stride) == a\n  """"""\n  with tf.variable_scope(scope) as sc:\n      kernel_h, kernel_w = kernel_size\n      num_in_channels = inputs.get_shape()[-1].value\n      kernel_shape = [kernel_h, kernel_w,\n                      num_output_channels, num_in_channels] # reversed to conv2d\n      kernel = _variable_with_weight_decay(\'weights\',\n                                           shape=kernel_shape,\n                                           use_xavier=use_xavier,\n                                           stddev=stddev,\n                                           wd=weight_decay)\n      stride_h, stride_w = stride\n      \n      # from slim.convolution2d_transpose\n      def get_deconv_dim(dim_size, stride_size, kernel_size, padding):\n          dim_size *= stride_size\n\n          if padding == \'VALID\' and dim_size is not None:\n            dim_size += max(kernel_size - stride_size, 0)\n          return dim_size\n\n      # caculate output shape\n      batch_size = inputs.get_shape()[0].value\n      height = inputs.get_shape()[1].value\n      width = inputs.get_shape()[2].value\n      out_height = get_deconv_dim(height, stride_h, kernel_h, padding)\n      out_width = get_deconv_dim(width, stride_w, kernel_w, padding)\n      output_shape = [batch_size, out_height, out_width, num_output_channels]\n\n      outputs = tf.nn.conv2d_transpose(inputs, kernel, output_shape,\n                             [1, stride_h, stride_w, 1],\n                             padding=padding)\n      biases = _variable_on_cpu(\'biases\', [num_output_channels],\n                                tf.constant_initializer(0.0))\n      outputs = tf.nn.bias_add(outputs, biases)\n\n      if bn:\n        outputs = batch_norm_for_conv2d(outputs, is_training,\n                                        bn_decay=bn_decay, scope=\'bn\')\n\n      if activation_fn is not None:\n        outputs = activation_fn(outputs)\n      return outputs\n\n   \n\ndef conv3d(inputs,\n           num_output_channels,\n           kernel_size,\n           scope,\n           stride=[1, 1, 1],\n           padding=\'SAME\',\n           use_xavier=True,\n           stddev=1e-3,\n           weight_decay=None,\n           activation_fn=tf.nn.relu,\n           bn=False,\n           bn_decay=None,\n           is_training=None):\n  """""" 3D convolution with non-linear operation.\n\n  Args:\n    inputs: 5-D tensor variable BxDxHxWxC\n    num_output_channels: int\n    kernel_size: a list of 3 ints\n    scope: string\n    stride: a list of 3 ints\n    padding: \'SAME\' or \'VALID\'\n    use_xavier: bool, use xavier_initializer if true\n    stddev: float, stddev for truncated_normal init\n    weight_decay: float\n    activation_fn: function\n    bn: bool, whether to use batch norm\n    bn_decay: float or float tensor variable in [0,1]\n    is_training: bool Tensor variable\n\n  Returns:\n    Variable tensor\n  """"""\n  with tf.variable_scope(scope) as sc:\n    kernel_d, kernel_h, kernel_w = kernel_size\n    num_in_channels = inputs.get_shape()[-1].value\n    kernel_shape = [kernel_d, kernel_h, kernel_w,\n                    num_in_channels, num_output_channels]\n    kernel = _variable_with_weight_decay(\'weights\',\n                                         shape=kernel_shape,\n                                         use_xavier=use_xavier,\n                                         stddev=stddev,\n                                         wd=weight_decay)\n    stride_d, stride_h, stride_w = stride\n    outputs = tf.nn.conv3d(inputs, kernel,\n                           [1, stride_d, stride_h, stride_w, 1],\n                           padding=padding)\n    biases = _variable_on_cpu(\'biases\', [num_output_channels],\n                              tf.constant_initializer(0.0))\n    outputs = tf.nn.bias_add(outputs, biases)\n    \n    if bn:\n      outputs = batch_norm_for_conv3d(outputs, is_training,\n                                      bn_decay=bn_decay, scope=\'bn\')\n\n    if activation_fn is not None:\n      outputs = activation_fn(outputs)\n    return outputs\n\ndef fully_connected(inputs,\n                    num_outputs,\n                    scope,\n                    use_xavier=True,\n                    stddev=1e-3,\n                    weight_decay=None,\n                    activation_fn=tf.nn.relu,\n                    bn=False,\n                    bn_decay=None,\n                    is_training=None):\n  """""" Fully connected layer with non-linear operation.\n  \n  Args:\n    inputs: 2-D tensor BxN\n    num_outputs: int\n  \n  Returns:\n    Variable tensor of size B x num_outputs.\n  """"""\n  with tf.variable_scope(scope) as sc:\n    num_input_units = inputs.get_shape()[-1].value\n    weights = _variable_with_weight_decay(\'weights\',\n                                          shape=[num_input_units, num_outputs],\n                                          use_xavier=use_xavier,\n                                          stddev=stddev,\n                                          wd=weight_decay)\n    outputs = tf.matmul(inputs, weights)\n    biases = _variable_on_cpu(\'biases\', [num_outputs],\n                             tf.constant_initializer(0.0))\n    outputs = tf.nn.bias_add(outputs, biases)\n     \n    if bn:\n      outputs = batch_norm_for_fc(outputs, is_training, bn_decay, \'bn\')\n\n    if activation_fn is not None:\n      outputs = activation_fn(outputs)\n    return outputs\n\n\ndef max_pool2d(inputs,\n               kernel_size,\n               scope,\n               stride=[2, 2],\n               padding=\'VALID\'):\n  """""" 2D max pooling.\n\n  Args:\n    inputs: 4-D tensor BxHxWxC\n    kernel_size: a list of 2 ints\n    stride: a list of 2 ints\n  \n  Returns:\n    Variable tensor\n  """"""\n  with tf.variable_scope(scope) as sc:\n    kernel_h, kernel_w = kernel_size\n    stride_h, stride_w = stride\n    outputs = tf.nn.max_pool(inputs,\n                             ksize=[1, kernel_h, kernel_w, 1],\n                             strides=[1, stride_h, stride_w, 1],\n                             padding=padding,\n                             name=sc.name)\n    return outputs\n\ndef avg_pool2d(inputs,\n               kernel_size,\n               scope,\n               stride=[2, 2],\n               padding=\'VALID\'):\n  """""" 2D avg pooling.\n\n  Args:\n    inputs: 4-D tensor BxHxWxC\n    kernel_size: a list of 2 ints\n    stride: a list of 2 ints\n  \n  Returns:\n    Variable tensor\n  """"""\n  with tf.variable_scope(scope) as sc:\n    kernel_h, kernel_w = kernel_size\n    stride_h, stride_w = stride\n    outputs = tf.nn.avg_pool(inputs,\n                             ksize=[1, kernel_h, kernel_w, 1],\n                             strides=[1, stride_h, stride_w, 1],\n                             padding=padding,\n                             name=sc.name)\n    return outputs\n\n\ndef max_pool3d(inputs,\n               kernel_size,\n               scope,\n               stride=[2, 2, 2],\n               padding=\'VALID\'):\n  """""" 3D max pooling.\n\n  Args:\n    inputs: 5-D tensor BxDxHxWxC\n    kernel_size: a list of 3 ints\n    stride: a list of 3 ints\n  \n  Returns:\n    Variable tensor\n  """"""\n  with tf.variable_scope(scope) as sc:\n    kernel_d, kernel_h, kernel_w = kernel_size\n    stride_d, stride_h, stride_w = stride\n    outputs = tf.nn.max_pool3d(inputs,\n                               ksize=[1, kernel_d, kernel_h, kernel_w, 1],\n                               strides=[1, stride_d, stride_h, stride_w, 1],\n                               padding=padding,\n                               name=sc.name)\n    return outputs\n\ndef avg_pool3d(inputs,\n               kernel_size,\n               scope,\n               stride=[2, 2, 2],\n               padding=\'VALID\'):\n  """""" 3D avg pooling.\n\n  Args:\n    inputs: 5-D tensor BxDxHxWxC\n    kernel_size: a list of 3 ints\n    stride: a list of 3 ints\n  \n  Returns:\n    Variable tensor\n  """"""\n  with tf.variable_scope(scope) as sc:\n    kernel_d, kernel_h, kernel_w = kernel_size\n    stride_d, stride_h, stride_w = stride\n    outputs = tf.nn.avg_pool3d(inputs,\n                               ksize=[1, kernel_d, kernel_h, kernel_w, 1],\n                               strides=[1, stride_d, stride_h, stride_w, 1],\n                               padding=padding,\n                               name=sc.name)\n    return outputs\n\n\ndef batch_norm_template_unused(inputs, is_training, scope, moments_dims, bn_decay):\n  """""" NOTE: this is older version of the util func. it is deprecated.\n  Batch normalization on convolutional maps and beyond...\n  Ref.: http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow\n  \n  Args:\n      inputs:        Tensor, k-D input ... x C could be BC or BHWC or BDHWC\n      is_training:   boolean tf.Varialbe, true indicates training phase\n      scope:         string, variable scope\n      moments_dims:  a list of ints, indicating dimensions for moments calculation\n      bn_decay:      float or float tensor variable, controling moving average weight\n  Return:\n      normed:        batch-normalized maps\n  """"""\n  with tf.variable_scope(scope) as sc:\n    num_channels = inputs.get_shape()[-1].value\n    beta = _variable_on_cpu(name=\'beta\',shape=[num_channels],\n                            initializer=tf.constant_initializer(0))\n    gamma = _variable_on_cpu(name=\'gamma\',shape=[num_channels],\n                            initializer=tf.constant_initializer(1.0))\n    batch_mean, batch_var = tf.nn.moments(inputs, moments_dims, name=\'moments\')\n    decay = bn_decay if bn_decay is not None else 0.9\n    ema = tf.train.ExponentialMovingAverage(decay=decay)\n    # Operator that maintains moving averages of variables.\n    # Need to set reuse=False, otherwise if reuse, will see moments_1/mean/ExponentialMovingAverage/ does not exist\n    # https://github.com/shekkizh/WassersteinGAN.tensorflow/issues/3\n    with tf.variable_scope(tf.get_variable_scope(), reuse=False):\n        ema_apply_op = tf.cond(is_training,\n                               lambda: ema.apply([batch_mean, batch_var]),\n                               lambda: tf.no_op())\n    \n    # Update moving average and return current batch\'s avg and var.\n    def mean_var_with_update():\n      with tf.control_dependencies([ema_apply_op]):\n        return tf.identity(batch_mean), tf.identity(batch_var)\n    \n    # ema.average returns the Variable holding the average of var.\n    mean, var = tf.cond(is_training,\n                        mean_var_with_update,\n                        lambda: (ema.average(batch_mean), ema.average(batch_var)))\n    normed = tf.nn.batch_normalization(inputs, mean, var, beta, gamma, 1e-3)\n  return normed\n\n\ndef batch_norm_template(inputs, is_training, scope, moments_dims_unused, bn_decay, data_format=\'NHWC\'):\n  """""" Batch normalization on convolutional maps and beyond...\n  Ref.: http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow\n  \n  Args:\n      inputs:        Tensor, k-D input ... x C could be BC or BHWC or BDHWC\n      is_training:   boolean tf.Varialbe, true indicates training phase\n      scope:         string, variable scope\n      moments_dims:  a list of ints, indicating dimensions for moments calculation\n      bn_decay:      float or float tensor variable, controling moving average weight\n      data_format:   \'NHWC\' or \'NCHW\'\n  Return:\n      normed:        batch-normalized maps\n  """"""\n  bn_decay = bn_decay if bn_decay is not None else 0.9\n  return tf.contrib.layers.batch_norm(inputs, \n                                      center=True, scale=True,\n                                      is_training=is_training, decay=bn_decay,updates_collections=None,\n                                      scope=scope,\n                                      data_format=data_format)\n\n\ndef batch_norm_for_fc(inputs, is_training, bn_decay, scope):\n  """""" Batch normalization on FC data.\n  \n  Args:\n      inputs:      Tensor, 2D BxC input\n      is_training: boolean tf.Varialbe, true indicates training phase\n      bn_decay:    float or float tensor variable, controling moving average weight\n      scope:       string, variable scope\n  Return:\n      normed:      batch-normalized maps\n  """"""\n  return batch_norm_template(inputs, is_training, scope, [0,], bn_decay)\n\n\ndef batch_norm_for_conv1d(inputs, is_training, bn_decay, scope, data_format):\n  """""" Batch normalization on 1D convolutional maps.\n  \n  Args:\n      inputs:      Tensor, 3D BLC input maps\n      is_training: boolean tf.Varialbe, true indicates training phase\n      bn_decay:    float or float tensor variable, controling moving average weight\n      scope:       string, variable scope\n      data_format: \'NHWC\' or \'NCHW\'\n  Return:\n      normed:      batch-normalized maps\n  """"""\n  return batch_norm_template(inputs, is_training, scope, [0,1], bn_decay, data_format)\n\n\n\n  \ndef batch_norm_for_conv2d(inputs, is_training, bn_decay, scope, data_format):\n  """""" Batch normalization on 2D convolutional maps.\n  \n  Args:\n      inputs:      Tensor, 4D BHWC input maps\n      is_training: boolean tf.Varialbe, true indicates training phase\n      bn_decay:    float or float tensor variable, controling moving average weight\n      scope:       string, variable scope\n      data_format: \'NHWC\' or \'NCHW\'\n  Return:\n      normed:      batch-normalized maps\n  """"""\n  return batch_norm_template(inputs, is_training, scope, [0,1,2], bn_decay, data_format)\n\n\ndef batch_norm_for_conv3d(inputs, is_training, bn_decay, scope):\n  """""" Batch normalization on 3D convolutional maps.\n  \n  Args:\n      inputs:      Tensor, 5D BDHWC input maps\n      is_training: boolean tf.Varialbe, true indicates training phase\n      bn_decay:    float or float tensor variable, controling moving average weight\n      scope:       string, variable scope\n  Return:\n      normed:      batch-normalized maps\n  """"""\n  return batch_norm_template(inputs, is_training, scope, [0,1,2,3], bn_decay)\n\n\ndef dropout(inputs,\n            is_training,\n            scope,\n            keep_prob=0.5,\n            noise_shape=None):\n  """""" Dropout layer.\n\n  Args:\n    inputs: tensor\n    is_training: boolean tf.Variable\n    scope: string\n    keep_prob: float in [0,1]\n    noise_shape: list of ints\n\n  Returns:\n    tensor variable\n  """"""\n  with tf.variable_scope(scope) as sc:\n    outputs = tf.cond(is_training,\n                      lambda: tf.nn.dropout(inputs, keep_prob, noise_shape),\n                      lambda: inputs)\n    return outputs\n'"
tf_ops/3d_interpolation/tf_interpolate.py,8,"b""import tensorflow as tf\nfrom tensorflow.python.framework import ops\nimport sys\nimport os\nBASE_DIR = os.path.dirname(__file__)\nsys.path.append(BASE_DIR)\ninterpolate_module=tf.load_op_library(os.path.join(BASE_DIR, 'tf_interpolate_so.so'))\ndef three_nn(xyz1, xyz2):\n    '''\n    Input:\n        xyz1: (b,n,3) float32 array, unknown points\n        xyz2: (b,m,3) float32 array, known points\n    Output:\n        dist: (b,n,3) float32 array, distances to known points\n        idx: (b,n,3) int32 array, indices to known points\n    '''\n    return interpolate_module.three_nn(xyz1, xyz2)\nops.NoGradient('ThreeNN')\ndef three_interpolate(points, idx, weight):\n    '''\n    Input:\n        points: (b,m,c) float32 array, known points\n        idx: (b,n,3) int32 array, indices to known points\n        weight: (b,n,3) float32 array, weights on known points\n    Output:\n        out: (b,n,c) float32 array, interpolated point values\n    '''\n    return interpolate_module.three_interpolate(points, idx, weight)\n@tf.RegisterGradient('ThreeInterpolate')\ndef _three_interpolate_grad(op, grad_out):\n    points = op.inputs[0]\n    idx = op.inputs[1]\n    weight = op.inputs[2]\n    return [interpolate_module.three_interpolate_grad(points, idx, weight, grad_out), None, None]\n\nif __name__=='__main__':\n    import numpy as np\n    import time\n    np.random.seed(100)\n    pts = np.random.random((32,128,64)).astype('float32')\n    tmp1 = np.random.random((32,512,3)).astype('float32')\n    tmp2 = np.random.random((32,128,3)).astype('float32')\n    with tf.device('/cpu:0'):\n        points = tf.constant(pts)\n        xyz1 = tf.constant(tmp1)\n        xyz2 = tf.constant(tmp2)\n        dist, idx = three_nn(xyz1, xyz2)\n        weight = tf.ones_like(dist)/3.0\n        interpolated_points = three_interpolate(points, idx, weight)\n    with tf.Session('') as sess:\n        now = time.time()\n        for _ in range(100):\n            ret = sess.run(interpolated_points)\n        print(time.time() - now)\n        print(ret.shape, ret.dtype)\n        #print ret\n\n\n\n"""
tf_ops/3d_interpolation/tf_interpolate_op_test.py,7,"b""import tensorflow as tf\nimport numpy as np\nfrom tf_interpolate import three_nn, three_interpolate\n\nclass GroupPointTest(tf.test.TestCase):\n  def test(self):\n    pass\n\n  def test_grad(self):\n    with self.test_session():\n      points = tf.constant(np.random.random((1,8,16)).astype('float32'))\n      print points\n      xyz1 = tf.constant(np.random.random((1,128,3)).astype('float32'))\n      xyz2 = tf.constant(np.random.random((1,8,3)).astype('float32'))\n      dist, idx = three_nn(xyz1, xyz2)\n      weight = tf.ones_like(dist)/3.0\n      interpolated_points = three_interpolate(points, idx, weight)\n      print interpolated_points\n      err = tf.test.compute_gradient_error(points, (1,8,16), interpolated_points, (1,128,16))\n      print err\n      self.assertLess(err, 1e-4) \n\nif __name__=='__main__':\n  tf.test.main() \n"""
tf_ops/3d_interpolation/visu_interpolation.py,9,"b""''' Visualize part segmentation '''\nimport os\nimport sys\nROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nsys.path.append('/home/rqi/Projects/toolkits/visualization')\nfrom show3d_balls import showpoints\nimport numpy as np\nfrom tf_interpolate import three_nn, three_interpolate\nimport tensorflow as tf\n\n\npts2 = np.array([[0,0,1],[1,0,0],[0,1,0],[1,1,0]]).astype('float32')\nxyz1 = np.random.random((100,3)).astype('float32')\nxyz2 = np.array([[0,0,0],[1,0,0],[0,1,0],[1,1,1]]).astype('float32')\n\ndef fun(xyz1,xyz2,pts2):\n    with tf.device('/cpu:0'):\n        points = tf.constant(np.expand_dims(pts2,0))\n        xyz1 = tf.constant(np.expand_dims(xyz1,0))\n        xyz2 = tf.constant(np.expand_dims(xyz2,0))\n        dist, idx = three_nn(xyz1, xyz2)\n        #weight = tf.ones_like(dist)/3.0\n        dist = tf.maximum(dist, 1e-10)\n        norm = tf.reduce_sum((1.0/dist),axis=2,keep_dims=True)\n        norm = tf.tile(norm, [1,1,3])\n        print norm\n        weight = (1.0/dist) / norm\n        interpolated_points = three_interpolate(points, idx, weight)\n    with tf.Session('') as sess:\n        tmp,pts1,d,w = sess.run([xyz1, interpolated_points, dist, weight])\n        #print w\n        pts1 = pts1.squeeze()\n    return pts1\n\npts1 = fun(xyz1,xyz2,pts2) \nall_pts = np.zeros((104,3))\nall_pts[0:100,:] = pts1\nall_pts[100:,:] = pts2\nall_xyz = np.zeros((104,3))\nall_xyz[0:100,:]=xyz1\nall_xyz[100:,:]=xyz2\nshowpoints(xyz2, pts2, ballradius=8)\nshowpoints(xyz1, pts1, ballradius=8)\nshowpoints(all_xyz, all_pts, ballradius=8)\n"""
tf_ops/grouping/tf_grouping.py,15,"b""import tensorflow as tf\nfrom tensorflow.python.framework import ops\nimport sys\nimport os\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(BASE_DIR)\ngrouping_module=tf.load_op_library(os.path.join(BASE_DIR, 'tf_grouping_so.so'))\ndef query_ball_point(radius, nsample, xyz1, xyz2):\n    '''\n    Input:\n        radius: float32, ball search radius\n        nsample: int32, number of points selected in each ball region\n        xyz1: (batch_size, ndataset, 3) float32 array, input points\n        xyz2: (batch_size, npoint, 3) float32 array, query points\n    Output:\n        idx: (batch_size, npoint, nsample) int32 array, indices to input points\n        pts_cnt: (batch_size, npoint) int32 array, number of unique points in each local region\n    '''\n    #return grouping_module.query_ball_point(radius, nsample, xyz1, xyz2)\n    return grouping_module.query_ball_point(xyz1, xyz2, radius, nsample)\nops.NoGradient('QueryBallPoint')\ndef select_top_k(k, dist):\n    '''\n    Input:\n        k: int32, number of k SMALLEST elements selected\n        dist: (b,m,n) float32 array, distance matrix, m query points, n dataset points\n    Output:\n        idx: (b,m,n) int32 array, first k in n are indices to the top k\n        dist_out: (b,m,n) float32 array, first k in n are the top k\n    '''\n    return grouping_module.selection_sort(dist, k)\nops.NoGradient('SelectionSort')\ndef group_point(points, idx):\n    '''\n    Input:\n        points: (batch_size, ndataset, channel) float32 array, points to sample from\n        idx: (batch_size, npoint, nsample) int32 array, indices to points\n    Output:\n        out: (batch_size, npoint, nsample, channel) float32 array, values sampled from points\n    '''\n    return grouping_module.group_point(points, idx)\n@tf.RegisterGradient('GroupPoint')\ndef _group_point_grad(op, grad_out):\n    points = op.inputs[0]\n    idx = op.inputs[1]\n    return [grouping_module.group_point_grad(points, idx, grad_out), None]\n\ndef knn_point(k, xyz1, xyz2):\n    '''\n    Input:\n        k: int32, number of k in k-nn search\n        xyz1: (batch_size, ndataset, c) float32 array, input points\n        xyz2: (batch_size, npoint, c) float32 array, query points\n    Output:\n        val: (batch_size, npoint, k) float32 array, L2 distances\n        idx: (batch_size, npoint, k) int32 array, indices to input points\n    '''\n    b = xyz1.get_shape()[0].value\n    n = xyz1.get_shape()[1].value\n    c = xyz1.get_shape()[2].value\n    m = xyz2.get_shape()[1].value\n\n    xyz1 = tf.tile(tf.reshape(xyz1, (b,1,n,c)), [1,m,1,1])\n    xyz2 = tf.tile(tf.reshape(xyz2, (b,m,1,c)), [1,1,n,1])\n    dist = tf.reduce_sum((xyz1-xyz2)**2, -1)\n\n    outi, out = select_top_k(k, dist)\n    idx = tf.slice(outi, [0,0,0], [-1,-1,k])\n    val = tf.slice(out, [0,0,0], [-1,-1,k])\n    #val, idx = tf.nn.top_k(-dist, k=k) # ONLY SUPPORT CPU\n    return val, idx\n\nif __name__=='__main__':\n    knn=True\n    import numpy as np\n    import time\n    np.random.seed(100)\n    pts = np.random.random((32,512,64)).astype('float32')\n    tmp1 = np.random.random((32,512,3)).astype('float32')\n    tmp2 = np.random.random((32,128,3)).astype('float32')\n    with tf.device('/gpu:1'):\n        points = tf.constant(pts)\n        xyz1 = tf.constant(tmp1)\n        xyz2 = tf.constant(tmp2)\n        radius = 0.1\n        nsample = 64\n        if knn:\n            _, idx = knn_point(nsample, xyz1, xyz2)\n            grouped_points = group_point(points, idx)\n        else:\n            idx, _ = query_ball_point(radius, nsample, xyz1, xyz2)\n            grouped_points = group_point(points, idx)\n            #grouped_points_grad = tf.ones_like(grouped_points)\n            #points_grad = tf.gradients(grouped_points, points, grouped_points_grad)\n    with tf.Session('') as sess:\n        now = time.time()\n        for _ in range(100):\n            ret = sess.run(grouped_points)\n        print(time.time() - now)\n        print(ret.shape, ret.dtype)\n        print(ret)\n\n\n"""
tf_ops/grouping/tf_grouping_op_test.py,7,"b'import tensorflow as tf\nimport numpy as np\nfrom tf_grouping import query_ball_point, group_point\n\nclass GroupPointTest(tf.test.TestCase):\n  def test(self):\n    pass\n\n  def test_grad(self):\n    with tf.device(\'/gpu:0\'):\n      points = tf.constant(np.random.random((1,128,16)).astype(\'float32\'))\n      print points\n      xyz1 = tf.constant(np.random.random((1,128,3)).astype(\'float32\'))\n      xyz2 = tf.constant(np.random.random((1,8,3)).astype(\'float32\'))\n      radius = 0.3 \n      nsample = 32\n      idx, pts_cnt = query_ball_point(radius, nsample, xyz1, xyz2)\n      grouped_points = group_point(points, idx)\n      print grouped_points\n\n    with self.test_session():\n      print ""---- Going to compute gradient error""\n      err = tf.test.compute_gradient_error(points, (1,128,16), grouped_points, (1,8,32,16))\n      print err\n      self.assertLess(err, 1e-4) \n\nif __name__==\'__main__\':\n  tf.test.main() \n'"
tf_ops/sampling/tf_sampling.py,15,"b""''' Furthest point sampling\nOriginal author: Haoqiang Fan\nModified by Charles R. Qi\nAll Rights Reserved. 2017.\n'''\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nimport sys\nimport os\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(BASE_DIR)\nsampling_module=tf.load_op_library(os.path.join(BASE_DIR, 'tf_sampling_so.so'))\ndef prob_sample(inp,inpr):\n    '''\ninput:\n    batch_size * ncategory float32\n    batch_size * npoints   float32\nreturns:\n    batch_size * npoints   int32\n    '''\n    return sampling_module.prob_sample(inp,inpr)\nops.NoGradient('ProbSample')\n# TF1.0 API requires set shape in C++\n#@tf.RegisterShape('ProbSample')\n#def _prob_sample_shape(op):\n#    shape1=op.inputs[0].get_shape().with_rank(2)\n#    shape2=op.inputs[1].get_shape().with_rank(2)\n#    return [tf.TensorShape([shape2.dims[0],shape2.dims[1]])]\ndef gather_point(inp,idx):\n    '''\ninput:\n    batch_size * ndataset * 3   float32\n    batch_size * npoints        int32\nreturns:\n    batch_size * npoints * 3    float32\n    '''\n    return sampling_module.gather_point(inp,idx)\n#@tf.RegisterShape('GatherPoint')\n#def _gather_point_shape(op):\n#    shape1=op.inputs[0].get_shape().with_rank(3)\n#    shape2=op.inputs[1].get_shape().with_rank(2)\n#    return [tf.TensorShape([shape1.dims[0],shape2.dims[1],shape1.dims[2]])]\n@tf.RegisterGradient('GatherPoint')\ndef _gather_point_grad(op,out_g):\n    inp=op.inputs[0]\n    idx=op.inputs[1]\n    return [sampling_module.gather_point_grad(inp,idx,out_g),None]\ndef farthest_point_sample(npoint,inp):\n    '''\ninput:\n    int32\n    batch_size * ndataset * 3   float32\nreturns:\n    batch_size * npoint         int32\n    '''\n    return sampling_module.farthest_point_sample(inp, npoint)\nops.NoGradient('FarthestPointSample')\n\n\nif __name__=='__main__':\n    import numpy as np\n    np.random.seed(100)\n    triangles=np.random.rand(1,5,3,3).astype('float32')\n    with tf.device('/gpu:1'):\n        inp=tf.constant(triangles)\n        tria=inp[:,:,0,:]\n        trib=inp[:,:,1,:]\n        tric=inp[:,:,2,:]\n        areas=tf.sqrt(tf.reduce_sum(tf.cross(trib-tria,tric-tria)**2,2)+1e-9)\n        randomnumbers=tf.random_uniform((1,8192))\n        triids=prob_sample(areas,randomnumbers)\n        tria_sample=gather_point(tria,triids)\n        trib_sample=gather_point(trib,triids)\n        tric_sample=gather_point(tric,triids)\n        us=tf.random_uniform((1,8192))\n        vs=tf.random_uniform((1,8192))\n        uplusv=1-tf.abs(us+vs-1)\n        uminusv=us-vs\n        us=(uplusv+uminusv)*0.5\n        vs=(uplusv-uminusv)*0.5\n        pt_sample=tria_sample+(trib_sample-tria_sample)*tf.expand_dims(us,-1)+(tric_sample-tria_sample)*tf.expand_dims(vs,-1)\n        print('pt_sample: ', pt_sample)\n        reduced_sample=gather_point(pt_sample,farthest_point_sample(1024,pt_sample))\n        print(reduced_sample)\n    with tf.Session('') as sess:\n        ret=sess.run(reduced_sample)\n    print(ret.shape,ret.dtype)\n    import cPickle as pickle\n    pickle.dump(ret,open('1.pkl','wb'),-1)\n"""
