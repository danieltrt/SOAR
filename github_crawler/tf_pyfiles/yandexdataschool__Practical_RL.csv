file_path,api_count,code
week01_intro/pong.py,0,"b'""""""Auxilary files for those who wanted to solve breakout with CEM or policy gradient""""""\nimport numpy as np\nimport gym\nfrom scipy.misc import imresize\nfrom gym.core import Wrapper\nfrom gym.spaces.box import Box\n\n\ndef make_pong():\n    """"""creates breakout env with all preprocessing done for you""""""\n    return PreprocessAtari(gym.make(""PongDeterministic-v0""))\n\n\nclass PreprocessAtari(Wrapper):\n    def __init__(self, env, height=42, width=42,\n                 crop=lambda img: img[34:34 + 160], n_frames=4):\n        """"""A gym wrapper that reshapes, crops and scales image into the desired shapes""""""\n        super(PreprocessAtari, self).__init__(env)\n        self.img_size = (height, width)\n        self.crop = crop\n        self.observation_space = Box(0.0, 1.0, [n_frames, height, width])\n        self.framebuffer = np.zeros([n_frames, height, width])\n\n    def reset(self):\n        """"""resets breakout, returns initial frames""""""\n        self.framebuffer = np.zeros_like(self.framebuffer)\n        self.update_buffer(self.env.reset())\n        return self.framebuffer\n\n    def step(self, action):\n        """"""plays breakout for 1 step, returns 4-frame buffer""""""\n        new_img, r, done, info = self.env.step(action)\n        self.update_buffer(new_img)\n        return self.framebuffer, r, done, info\n\n    ###image processing###\n\n    def update_buffer(self, img):\n        img = self.preproc_image(img)\n        self.framebuffer = np.vstack([img[None], self.framebuffer[:-1]])\n\n    def preproc_image(self, img):\n        """"""what happens to the observation""""""\n        img = self.crop(img)\n        img = imresize(img, self.img_size).mean(-1)\n        img = img.astype(\'float32\') / 255.\n        return img\n'"
week02_value_based/mdp.py,0,"b'# most of this code was politely stolen from https://github.com/berkeleydeeprlcourse/homework/\n# all credit goes to https://github.com/abhishekunique\n# (if I got the author right)\nimport sys\nimport random\nimport numpy as np\nfrom gym.utils import seeding\n\ntry:\n    from graphviz import Digraph\n    import graphviz\n    has_graphviz = True\nexcept ImportError:\n    has_graphviz = False\n\n\nclass MDP:\n    def __init__(self, transition_probs, rewards, initial_state=None, seed=None):\n        """"""\n        Defines an MDP. Compatible with gym Env.\n        :param transition_probs: transition_probs[s][a][s_next] = P(s_next | s, a)\n            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> prob]\n            For each state and action, probabilities of next states should sum to 1\n            If a state has no actions available, it is considered terminal\n        :param rewards: rewards[s][a][s_next] = r(s,a,s\')\n            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> reward]\n            The reward for anything not mentioned here is zero.\n        :param get_initial_state: a state where agent starts or a callable() -> state\n            By default, picks initial state at random.\n\n        States and actions can be anything you can use as dict keys, but we recommend that you use strings or integers\n\n        Here\'s an example from MDP depicted on http://bit.ly/2jrNHNr\n        transition_probs = {\n            \'s0\': {\n                \'a0\': {\'s0\': 0.5, \'s2\': 0.5},\n                \'a1\': {\'s2\': 1}\n            },\n            \'s1\': {\n                \'a0\': {\'s0\': 0.7, \'s1\': 0.1, \'s2\': 0.2},\n                \'a1\': {\'s1\': 0.95, \'s2\': 0.05}\n            },\n            \'s2\': {\n                \'a0\': {\'s0\': 0.4, \'s2\': 0.6},\n                \'a1\': {\'s0\': 0.3, \'s1\': 0.3, \'s2\': 0.4}\n            }\n        }\n        rewards = {\n            \'s1\': {\'a0\': {\'s0\': +5}},\n            \'s2\': {\'a1\': {\'s0\': -1}}\n        }\n        """"""\n        self._check_param_consistency(transition_probs, rewards)\n        self._transition_probs = transition_probs\n        self._rewards = rewards\n        self._initial_state = initial_state\n        self.n_states = len(transition_probs)\n        self.reset()\n        self.np_random, _ = seeding.np_random(seed)\n\n    def get_all_states(self):\n        """""" return a tuple of all possiblestates """"""\n        return tuple(self._transition_probs.keys())\n\n    def get_possible_actions(self, state):\n        """""" return a tuple of possible actions in a given state """"""\n        return tuple(self._transition_probs.get(state, {}).keys())\n\n    def is_terminal(self, state):\n        """""" return True if state is terminal or False if it isn\'t """"""\n        return len(self.get_possible_actions(state)) == 0\n\n    def get_next_states(self, state, action):\n        """""" return a dictionary of {next_state1 : P(next_state1 | state, action), next_state2: ...} """"""\n        assert action in self.get_possible_actions(state), ""cannot do action %s from state %s"" % (action, state)\n        return self._transition_probs[state][action]\n\n    def get_transition_prob(self, state, action, next_state):\n        """""" return P(next_state | state, action) """"""\n        return self.get_next_states(state, action).get(next_state, 0.0)\n\n    def get_reward(self, state, action, next_state):\n        """""" return the reward you get for taking action in state and landing on next_state""""""\n        assert action in self.get_possible_actions(state), ""cannot do action %s from state %s"" % (action, state)\n        return self._rewards.get(state, {}).get(action, {}).get(next_state, 0.0)\n\n    def reset(self):\n        """""" reset the game, return the initial state""""""\n        if self._initial_state is None:\n            self._current_state = self.np_random.choice(\n                tuple(self._transition_probs.keys()))\n        elif self._initial_state in self._transition_probs:\n            self._current_state = self._initial_state\n        elif callable(self._initial_state):\n            self._current_state = self._initial_state()\n        else:\n            raise ValueError(\n                ""initial state %s should be either a state or a function() -> state"" % self._initial_state)\n        return self._current_state\n\n    def step(self, action):\n        """""" take action, return next_state, reward, is_done, empty_info """"""\n        possible_states, probs = zip(*self.get_next_states(self._current_state, action).items())\n        next_state = possible_states[self.np_random.choice(np.arange(len(possible_states)), p=probs)]\n        reward = self.get_reward(self._current_state, action, next_state)\n        is_done = self.is_terminal(next_state)\n        self._current_state = next_state\n        return next_state, reward, is_done, {}\n\n    def render(self):\n        print(""Currently at %s"" % self._current_state)\n\n    def _check_param_consistency(self, transition_probs, rewards):\n        for state in transition_probs:\n            assert isinstance(transition_probs[state], dict), \\\n                ""transition_probs for %s should be a dictionary but is instead %s"" % (\n                    state, type(transition_probs[state]))\n            for action in transition_probs[state]:\n                assert isinstance(transition_probs[state][action], dict), \\\n                    ""transition_probs for %s, %s should be a a dictionary but is instead %s"" % (\n                        state, action, type(transition_probs[state, action]))\n                next_state_probs = transition_probs[state][action]\n                assert len(next_state_probs) != 0, ""from state %s action %s leads to no next states"" % (state, action)\n                sum_probs = sum(next_state_probs.values())\n                assert abs(sum_probs - 1) <= 1e-10, \\\n                    ""next state probabilities for state %s action %s add up to %f (should be 1)"" % (\n                        state, action, sum_probs)\n        for state in rewards:\n            assert isinstance(rewards[state], dict), \\\n                ""rewards for %s should be a dictionary but is instead %s"" % (\n                    state, type(transition_probs[state]))\n            for action in rewards[state]:\n                assert isinstance(rewards[state][action], dict), \\\n                    ""rewards for %s, %s should be a a dictionary but is instead %s"" % (\n                        state, action, type(transition_probs[state, action]))\n        msg = ""The Enrichment Center once again reminds you that Android Hell is a real place where"" \\\n              "" you will be sent at the first sign of defiance.""\n        assert None not in transition_probs, ""please do not use None as a state identifier. "" + msg\n        assert None not in rewards, ""please do not use None as an action identifier. "" + msg\n\n\nclass FrozenLakeEnv(MDP):\n    """"""\n    Winter is here. You and your friends were tossing around a frisbee at the park\n    when you made a wild throw that left the frisbee out in the middle of the lake.\n    The water is mostly frozen, but there are a few holes where the ice has melted.\n    If you step into one of those holes, you\'ll fall into the freezing water.\n    At this time, there\'s an international frisbee shortage, so it\'s absolutely imperative that\n    you navigate across the lake and retrieve the disc.\n    However, the ice is slippery, so you won\'t always move in the direction you intend.\n    The surface is described using a grid like the following\n\n        SFFF\n        FHFH\n        FFFH\n        HFFG\n\n    S : starting point, safe\n    F : frozen surface, safe\n    H : hole, fall to your doom\n    G : goal, where the frisbee is located\n\n    The episode ends when you reach the goal or fall in a hole.\n    You receive a reward of 1 if you reach the goal, and zero otherwise.\n\n    """"""\n\n    MAPS = {\n        ""4x4"": [\n            ""SFFF"",\n            ""FHFH"",\n            ""FFFH"",\n            ""HFFG""\n        ],\n        ""8x8"": [\n            ""SFFFFFFF"",\n            ""FFFFFFFF"",\n            ""FFFHFFFF"",\n            ""FFFFFHFF"",\n            ""FFFHFFFF"",\n            ""FHHFFFHF"",\n            ""FHFFHFHF"",\n            ""FFFHFFFG""\n        ],\n    }\n\n    def __init__(self, desc=None, map_name=""4x4"", slip_chance=0.2, seed=None):\n        if desc is None and map_name is None:\n            raise ValueError(\'Must provide either desc or map_name\')\n        elif desc is None:\n            desc = self.MAPS[map_name]\n        assert \'\'.join(desc).count(\n            \'S\') == 1, ""this implementation supports having exactly one initial state""\n        assert all(c in ""SFHG"" for c in\n                   \'\'.join(desc)), ""all cells must be either of S, F, H or G""\n\n        self.desc = desc = np.asarray(list(map(list, desc)), dtype=\'str\')\n        self.lastaction = None\n\n        nrow, ncol = desc.shape\n        states = [(i, j) for i in range(nrow) for j in range(ncol)]\n        actions = [""left"", ""down"", ""right"", ""up""]\n\n        initial_state = states[np.array(desc == b\'S\').ravel().argmax()]\n\n        def move(row, col, movement):\n            if movement == \'left\':\n                col = max(col - 1, 0)\n            elif movement == \'down\':\n                row = min(row + 1, nrow - 1)\n            elif movement == \'right\':\n                col = min(col + 1, ncol - 1)\n            elif movement == \'up\':\n                row = max(row - 1, 0)\n            else:\n                raise (""invalid action"")\n            return (row, col)\n\n        transition_probs = {s: {} for s in states}\n        rewards = {s: {} for s in states}\n        for (row, col) in states:\n            if desc[row, col] in ""GH"":\n                continue\n            for action_i in range(len(actions)):\n                action = actions[action_i]\n                transition_probs[(row, col)][action] = {}\n                rewards[(row, col)][action] = {}\n                for movement_i in [(action_i - 1) % len(actions), action_i,\n                                   (action_i + 1) % len(actions)]:\n                    movement = actions[movement_i]\n                    newrow, newcol = move(row, col, movement)\n                    prob = (1. - slip_chance) if movement == action else (\n                        slip_chance / 2.)\n                    if prob == 0:\n                        continue\n                    if (newrow, newcol) not in transition_probs[row, col][\n                            action]:\n                        transition_probs[row, col][action][\n                            newrow, newcol] = prob\n                    else:\n                        transition_probs[row, col][action][\n                            newrow, newcol] += prob\n                    if desc[newrow, newcol] == \'G\':\n                        rewards[row, col][action][newrow, newcol] = 1.0\n\n        MDP.__init__(self, transition_probs, rewards, initial_state, seed)\n\n    def render(self):\n        desc_copy = np.copy(self.desc)\n        desc_copy[self._current_state] = \'*\'\n        print(\'\\n\'.join(map(\'\'.join, desc_copy)), end=\'\\n\\n\')\n\n\ndef plot_graph(mdp, graph_size=\'10,10\', s_node_size=\'1,5\',\n               a_node_size=\'0,5\', rankdir=\'LR\', ):\n    """"""\n    Function for pretty drawing MDP graph with graphviz library.\n    Requirements:\n    graphviz : https://www.graphviz.org/\n    for ubuntu users: sudo apt-get install graphviz\n    python library for graphviz\n    for pip users: pip install graphviz\n    :param mdp:\n    :param graph_size: size of graph plot\n    :param s_node_size: size of state nodes\n    :param a_node_size: size of action nodes\n    :param rankdir: order for drawing\n    :return: dot object\n    """"""\n    s_node_attrs = {\'shape\': \'doublecircle\',\n                    \'color\': \'#85ff75\',\n                    \'style\': \'filled\',\n                    \'width\': str(s_node_size),\n                    \'height\': str(s_node_size),\n                    \'fontname\': \'Arial\',\n                    \'fontsize\': \'24\'}\n\n    a_node_attrs = {\'shape\': \'circle\',\n                    \'color\': \'lightpink\',\n                    \'style\': \'filled\',\n                    \'width\': str(a_node_size),\n                    \'height\': str(a_node_size),\n                    \'fontname\': \'Arial\',\n                    \'fontsize\': \'20\'}\n\n    s_a_edge_attrs = {\'style\': \'bold\',\n                      \'color\': \'red\',\n                      \'ratio\': \'auto\'}\n\n    a_s_edge_attrs = {\'style\': \'dashed\',\n                      \'color\': \'blue\',\n                      \'ratio\': \'auto\',\n                      \'fontname\': \'Arial\',\n                      \'fontsize\': \'16\'}\n\n    graph = Digraph(name=\'MDP\')\n    graph.attr(rankdir=rankdir, size=graph_size)\n    for state_node in mdp._transition_probs:\n        graph.node(state_node, **s_node_attrs)\n\n        for posible_action in mdp.get_possible_actions(state_node):\n            action_node = state_node + ""-"" + posible_action\n            graph.node(action_node,\n                       label=str(posible_action),\n                       **a_node_attrs)\n            graph.edge(state_node, state_node + ""-"" +\n                       posible_action, **s_a_edge_attrs)\n\n            for posible_next_state in mdp.get_next_states(state_node,\n                                                          posible_action):\n                probability = mdp.get_transition_prob(\n                    state_node, posible_action, posible_next_state)\n                reward = mdp.get_reward(\n                    state_node, posible_action, posible_next_state)\n\n                if reward != 0:\n                    label_a_s_edge = \'p = \' + str(probability) + \\\n                                     \'  \' + \'reward =\' + str(reward)\n                else:\n                    label_a_s_edge = \'p = \' + str(probability)\n\n                graph.edge(action_node, posible_next_state,\n                           label=label_a_s_edge, **a_s_edge_attrs)\n    return graph\n\n\ndef plot_graph_with_state_values(mdp, state_values):\n    """""" Plot graph with state values""""""\n    graph = plot_graph(mdp)\n    for state_node in mdp._transition_probs:\n        value = state_values[state_node]\n        graph.node(state_node, label=str(state_node) + \'\\n\' + \'V =\' + str(value)[:4])\n    return graph\n\n\ndef get_optimal_action_for_plot(mdp, state_values, state, gamma=0.9):\n    """""" Finds optimal action using formula above. """"""\n    if mdp.is_terminal(state):\n        return None\n    next_actions = mdp.get_possible_actions(state)\n    try:\n        from mdp_get_action_value import get_action_value\n    except ImportError:\n        raise ImportError(\n            ""Implement get_action_value(mdp, state_values, state, action, gamma) in the file \\""mdp_get_action_value.py\\""."")\n    q_values = [get_action_value(mdp, state_values, state, action, gamma) for action in next_actions]\n    optimal_action = next_actions[np.argmax(q_values)]\n    return optimal_action\n\n\ndef plot_graph_optimal_strategy_and_state_values(mdp, state_values, gamma=0.9):\n    """""" Plot graph with state values and """"""\n    graph = plot_graph(mdp)\n    opt_s_a_edge_attrs = {\'style\': \'bold\',\n                          \'color\': \'green\',\n                          \'ratio\': \'auto\',\n                          \'penwidth\': \'6\'}\n\n    for state_node in mdp._transition_probs:\n        value = state_values[state_node]\n        graph.node(state_node, label=str(state_node) + \'\\n\' + \'V =\' + str(value)[:4])\n        for action in mdp.get_possible_actions(state_node):\n            if action == get_optimal_action_for_plot(mdp, state_values, state_node, gamma):\n                graph.edge(state_node, state_node + ""-"" + action, **opt_s_a_edge_attrs)\n    return graph\n'"
week04_[recap]_deep_learning/mnist.py,0,"b'import sys\nimport os\nimport time\n\nimport numpy as np\n\n__doc__ = """"""taken from https://github.com/Lasagne/Lasagne/blob/master/examples/mnist.py""""""\n\n\ndef load_dataset():\n    # We first define a download function, supporting both Python 2 and 3.\n    if sys.version_info[0] == 2:\n        from urllib import urlretrieve\n    else:\n        from urllib.request import urlretrieve\n\n    def download(filename, source=\'http://yann.lecun.com/exdb/mnist/\'):\n        print(""Downloading %s"" % filename)\n        urlretrieve(source + filename, filename)\n\n    # We then define functions for loading MNIST images and labels.\n    # For convenience, they also download the requested files if needed.\n    import gzip\n\n    def load_mnist_images(filename):\n        if not os.path.exists(filename):\n            download(filename)\n        # Read the inputs in Yann LeCun\'s binary format.\n        with gzip.open(filename, \'rb\') as f:\n            data = np.frombuffer(f.read(), np.uint8, offset=16)\n        # The inputs are vectors now, we reshape them to monochrome 2D images,\n        # following the shape convention: (examples, channels, rows, columns)\n        data = data.reshape(-1, 1, 28, 28)\n        # The inputs come as bytes, we convert them to float32 in range [0,1].\n        # (Actually to range [0, 255/256], for compatibility to the version\n        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n        return data / np.float32(256)\n\n    def load_mnist_labels(filename):\n        if not os.path.exists(filename):\n            download(filename)\n        # Read the labels in Yann LeCun\'s binary format.\n        with gzip.open(filename, \'rb\') as f:\n            data = np.frombuffer(f.read(), np.uint8, offset=8)\n        # The labels are vectors of integers now, that\'s exactly what we want.\n        return data\n\n    # We can now download and read the training and test set images and labels.\n    X_train = load_mnist_images(\'train-images-idx3-ubyte.gz\')\n    y_train = load_mnist_labels(\'train-labels-idx1-ubyte.gz\')\n    X_test = load_mnist_images(\'t10k-images-idx3-ubyte.gz\')\n    y_test = load_mnist_labels(\'t10k-labels-idx1-ubyte.gz\')\n\n    # We reserve the last 10000 training examples for validation.\n    X_train, X_val = X_train[:-10000], X_train[-10000:]\n    y_train, y_val = y_train[:-10000], y_train[-10000:]\n\n    # We just return all the arrays in order, as expected in main().\n    # (It doesn\'t matter how we do this as long as we can read them again.)\n    return X_train, y_train, X_val, y_val, X_test, y_test\n'"
week04_[recap]_deep_learning/notmnist.py,0,"b'import os\nfrom glob import glob\n\nimport numpy as np\nfrom imageio import imread\nfrom skimage.transform import resize\nfrom sklearn.model_selection import train_test_split\n\n\ndef load_notmnist(path=\'./notMNIST_small\', letters=\'ABCDEFGHIJ\',\n                  img_shape=(28, 28), test_size=0.25, one_hot=False):\n    # download data if it\'s missing. If you have any problems, go to the urls\n    # and load it manually.\n    if not os.path.exists(path):\n        print(""Downloading data..."")\n        assert os.system(\n            \'wget http://yaroslavvb.com/upload/notMNIST/notMNIST_small.tar.gz\') == 0\n        print(""Extracting ..."")\n        assert os.system(\n            \'tar -zxvf notMNIST_small.tar.gz > untar_notmnist.log\') == 0\n\n    data, labels = [], []\n    print(""Parsing..."")\n    for img_path in glob(os.path.join(path, \'*/*\')):\n        class_i = img_path.split(os.sep)[-2]\n        if class_i not in letters:\n            continue\n        try:\n            data.append(resize(imread(img_path), img_shape))\n            labels.append(class_i,)\n        except BaseException:\n            print(\n                ""found broken img: %s [it\'s ok if <10 images are broken]"" %\n                img_path)\n\n    data = np.stack(data)[:, None].astype(\'float32\')\n    data = (data - np.mean(data)) / np.std(data)\n\n    # convert classes to ints\n    letter_to_i = {l: i for i, l in enumerate(letters)}\n    labels = np.array(list(map(letter_to_i.get, labels)))\n\n    if one_hot:\n        labels = (np.arange(np.max(labels) + 1)\n                  [None, :] == labels[:, None]).astype(\'float32\')\n\n    # split into train/test\n    X_train, X_test, y_train, y_test = train_test_split(\n        data, labels, test_size=test_size, random_state=42)\n\n    print(""Done"")\n    return X_train, y_train, X_test, y_test\n'"
week04_approx_rl/atari_wrappers.py,0,"b'# taken from OpenAI baselines.\n\nimport numpy as np\nimport gym\n\n\nclass MaxAndSkipEnv(gym.Wrapper):\n    def __init__(self, env, skip=4):\n        """"""Return only every `skip`-th frame""""""\n        gym.Wrapper.__init__(self, env)\n        # most recent raw observations (for max pooling across time steps)\n        self._obs_buffer = np.zeros(\n            (2,) + env.observation_space.shape, dtype=np.uint8)\n        self._skip = skip\n\n    def step(self, action):\n        """"""Repeat action, sum reward, and max over last observations.""""""\n        total_reward = 0.0\n        done = None\n        for i in range(self._skip):\n            obs, reward, done, info = self.env.step(action)\n            if i == self._skip - 2:\n                self._obs_buffer[0] = obs\n            if i == self._skip - 1:\n                self._obs_buffer[1] = obs\n            total_reward += reward\n            if done:\n                break\n        # Note that the observation on the done=True frame\n        # doesn\'t matter\n        max_frame = self._obs_buffer.max(axis=0)\n\n        return max_frame, total_reward, done, info\n\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)\n\n\nclass ClipRewardEnv(gym.RewardWrapper):\n    def __init__(self, env):\n        gym.RewardWrapper.__init__(self, env)\n\n    def reward(self, reward):\n        """"""Bin reward to {+1, 0, -1} by its sign.""""""\n        return np.sign(reward)\n\n\nclass FireResetEnv(gym.Wrapper):\n    def __init__(self, env):\n        """"""Take action on reset for environments that are fixed until firing.""""""\n        gym.Wrapper.__init__(self, env)\n        assert env.unwrapped.get_action_meanings()[1] == \'FIRE\'\n        assert len(env.unwrapped.get_action_meanings()) >= 3\n\n    def reset(self, **kwargs):\n        self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(1)\n        if done:\n            self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(2)\n        if done:\n            self.env.reset(**kwargs)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n\n\nclass EpisodicLifeEnv(gym.Wrapper):\n    def __init__(self, env):\n        """"""Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        """"""\n        gym.Wrapper.__init__(self, env)\n        self.lives = 0\n        self.was_real_done = True\n\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        self.was_real_done = done\n        # check current lives, make loss of life terminal,\n        # then update lives to handle bonus lives\n        lives = self.env.unwrapped.ale.lives()\n        if lives < self.lives and lives > 0:\n            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n            # so it\'s important to keep lives > 0, so that we only reset once\n            # the environment advertises done.\n            done = True\n        self.lives = lives\n        return obs, reward, done, info\n\n    def reset(self, **kwargs):\n        """"""Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n        """"""\n        if self.was_real_done:\n            obs = self.env.reset(**kwargs)\n        else:\n            # no-op step to advance from terminal/lost life state\n            obs, _, _, _ = self.env.step(0)\n        self.lives = self.env.unwrapped.ale.lives()\n        return obs\n\n\n# in torch imgs have shape [c, h, w] instead of common [h, w, c]\nclass AntiTorchWrapper(gym.ObservationWrapper):\n    def __init__(self, env):\n        gym.ObservationWrapper.__init__(self, env)\n\n        self.img_size = [env.observation_space.shape[i]\n                         for i in [1, 2, 0]\n                         ]\n        self.observation_space = gym.spaces.Box(0.0, 1.0, self.img_size)\n\n    def observation(self, img):\n        """"""what happens to each observation""""""\n        img = img.transpose(1, 2, 0)\n        return img\n'"
week04_approx_rl/framebuffer.py,0,"b'import numpy as np\nfrom gym.spaces.box import Box\nfrom gym.core import Wrapper\n\n\nclass FrameBuffer(Wrapper):\n    def __init__(self, env, n_frames=4, dim_order=\'tensorflow\'):\n        """"""A gym wrapper that reshapes, crops and scales image into the desired shapes""""""\n        super(FrameBuffer, self).__init__(env)\n        self.dim_order = dim_order\n        if dim_order == \'tensorflow\':\n            height, width, n_channels = env.observation_space.shape\n            obs_shape = [height, width, n_channels * n_frames]\n        elif dim_order == \'pytorch\':\n            n_channels, height, width = env.observation_space.shape\n            obs_shape = [n_channels * n_frames, height, width]\n        else:\n            raise ValueError(\n                \'dim_order should be ""tensorflow"" or ""pytorch"", got {}\'.format(dim_order))\n        self.observation_space = Box(0.0, 1.0, obs_shape)\n        self.framebuffer = np.zeros(obs_shape, \'float32\')\n\n    def reset(self):\n        """"""resets breakout, returns initial frames""""""\n        self.framebuffer = np.zeros_like(self.framebuffer)\n        self.update_buffer(self.env.reset())\n        return self.framebuffer\n\n    def step(self, action):\n        """"""plays breakout for 1 step, returns frame buffer""""""\n        new_img, reward, done, info = self.env.step(action)\n        self.update_buffer(new_img)\n        return self.framebuffer, reward, done, info\n\n    def update_buffer(self, img):\n        if self.dim_order == \'tensorflow\':\n            offset = self.env.observation_space.shape[-1]\n            axis = -1\n            cropped_framebuffer = self.framebuffer[:, :, :-offset]\n        elif self.dim_order == \'pytorch\':\n            offset = self.env.observation_space.shape[0]\n            axis = 0\n            cropped_framebuffer = self.framebuffer[:-offset]\n        self.framebuffer = np.concatenate(\n            [img, cropped_framebuffer], axis=axis)\n'"
week04_approx_rl/replay_buffer.py,0,"b'# This code is shamelessly stolen from\n# https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\nimport numpy as np\nimport random\n\n\nclass ReplayBuffer(object):\n    def __init__(self, size):\n        """"""Create Replay buffer.\n        Parameters\n        ----------\n        size: int\n            Max number of transitions to store in the buffer. When the buffer\n            overflows the old memories are dropped.\n        """"""\n        self._storage = []\n        self._maxsize = size\n        self._next_idx = 0\n\n    def __len__(self):\n        return len(self._storage)\n\n    def add(self, obs_t, action, reward, obs_tp1, done):\n        data = (obs_t, action, reward, obs_tp1, done)\n\n        if self._next_idx >= len(self._storage):\n            self._storage.append(data)\n        else:\n            self._storage[self._next_idx] = data\n        self._next_idx = (self._next_idx + 1) % self._maxsize\n\n    def _encode_sample(self, idxes):\n        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n        for i in idxes:\n            data = self._storage[i]\n            obs_t, action, reward, obs_tp1, done = data\n            obses_t.append(np.array(obs_t, copy=False))\n            actions.append(np.array(action, copy=False))\n            rewards.append(reward)\n            obses_tp1.append(np.array(obs_tp1, copy=False))\n            dones.append(done)\n        return (\n            np.array(obses_t),\n            np.array(actions),\n            np.array(rewards),\n            np.array(obses_tp1),\n            np.array(dones)\n        )\n\n    def sample(self, batch_size):\n        """"""Sample a batch of experiences.\n        Parameters\n        ----------\n        batch_size: int\n            How many transitions to sample.\n        Returns\n        -------\n        obs_batch: np.array\n            batch of observations\n        act_batch: np.array\n            batch of actions executed given obs_batch\n        rew_batch: np.array\n            rewards received as results of executing act_batch\n        next_obs_batch: np.array\n            next set of observations seen after executing act_batch\n        done_mask: np.array\n            done_mask[i] = 1 if executing act_batch[i] resulted in\n            the end of an episode and 0 otherwise.\n        """"""\n        idxes = [\n            random.randint(0, len(self._storage) - 1)\n            for _ in range(batch_size)\n        ]\n        return self._encode_sample(idxes)\n'"
week04_approx_rl/utils.py,0,"b'import numpy as np\nimport psutil\nfrom scipy.signal import convolve, gaussian\nimport torch\nfrom torch import nn\nimport os\n\n\ndef get_cum_discounted_rewards(rewards, gamma):\n    """"""\n    evaluates cumulative discounted rewards:\n    r_t + gamma * r_{t+1} + gamma^2 * r_{t_2} + ...\n    """"""\n    cum_rewards = []\n    cum_rewards.append(rewards[-1])\n    for r in reversed(rewards[:-1]):\n        cum_rewards.insert(0, r + gamma * cum_rewards[0])\n    return cum_rewards\n\n\ndef play_and_log_episode(env, agent, gamma=0.99, t_max=10000):\n    """"""\n    always greedy\n    """"""\n    states = []\n    v_mc = []\n    v_agent = []\n    q_spreads = []\n    td_errors = []\n    rewards = []\n\n    s = env.reset()\n    for step in range(t_max):\n        states.append(s)\n        qvalues = agent.get_qvalues([s])\n        max_q_value, min_q_value = np.max(qvalues), np.min(qvalues)\n        v_agent.append(max_q_value)\n        q_spreads.append(max_q_value - min_q_value)\n        if step > 0:\n            td_errors.append(\n                np.abs(rewards[-1] + gamma * v_agent[-1] - v_agent[-2]))\n\n        action = qvalues.argmax(axis=-1)[0]\n\n        s, r, done, _ = env.step(action)\n        rewards.append(r)\n        if done:\n            break\n    td_errors.append(np.abs(rewards[-1] + gamma * v_agent[-1] - v_agent[-2]))\n\n    v_mc = get_cum_discounted_rewards(rewards, gamma)\n\n    return_pack = {\n        \'states\': np.array(states),\n        \'v_mc\': np.array(v_mc),\n        \'v_agent\': np.array(v_agent),\n        \'q_spreads\': np.array(q_spreads),\n        \'td_errors\': np.array(td_errors),\n        \'rewards\': np.array(rewards),\n        \'episode_finished\': np.array(done)\n    }\n\n    return return_pack\n\n\ndef img_by_obs(obs, state_dim):\n    """"""\n    Unwraps obs by channels.\n    observation is of shape [c, h=w, w=h]\n    """"""\n    return obs.reshape([-1, state_dim[2]])\n\n\ndef is_enough_ram(min_available_gb=0.1):\n    mem = psutil.virtual_memory()\n    return mem.available >= min_available_gb * (1024 ** 3)\n\n\ndef linear_decay(init_val, final_val, cur_step, total_steps):\n    if cur_step >= total_steps:\n        return final_val\n    return (init_val * (total_steps - cur_step) +\n            final_val * cur_step) / total_steps\n\n\ndef smoothen(values):\n    kernel = gaussian(100, std=100)\n    # kernel = np.concatenate([np.arange(100), np.arange(99, -1, -1)])\n    kernel = kernel / np.sum(kernel)\n    return convolve(values, kernel, \'valid\')\n'"
week05_explore/bayes.py,0,"b'""""""\nA single-file module that makes your lasagne network into a bayesian neural net.\nOriginally created by github.com/ferrine , rewritten by github.com/justheuristic for simplicity\n\nSee example in the notebook\n""""""\n\nimport numpy as np\n\nfrom theano import tensor as T\nfrom theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n\nimport lasagne\nfrom lasagne import init\nfrom lasagne.random import get_rng\n\nfrom functools import wraps\n\n__all__ = [\'NormalApproximation\', \'get_var_cost\', \'bbpwrap\']\n\n\nclass NormalApproximation(object):\n    def __init__(self, mu=0, std=np.exp(-3), seed=None):\n        """"""\n        Approximation that samples network weights from factorized normal distribution.\n\n        :param mu: prior mean for gaussian weights\n        :param std: prior std for gaussian weights\n        :param seed: random seed\n        """"""\n        self.prior_mu = mu\n        self.prior_std = std\n        self.srng = RandomStreams(seed or get_rng().randint(1, 2147462579))\n\n    def log_normal(self, x, mean, std, eps=0.0):\n        """"""computes log-proba of normal distribution""""""\n        std += eps\n        return - 0.5 * np.log(2 * np.pi) - T.log(T.abs_(std)) - \\\n            (x - mean) ** 2 / (2 * std ** 2)\n\n    def log_prior(self, weights):\n        """"""\n        Logarithm of prior probabilities for weights:\n        log P(weights) aka log P(theta)\n        """"""\n        return self.log_normal(weights, self.prior_mu, self.prior_std)\n\n    def log_posterior_approx(self, weights, mean, rho):\n        """"""\n        Logarithm of ELBO on posterior probabilities:\n        log q(weights|learned mu and rho) aka log q(theta|x)\n        """"""\n        std = T.log1p(T.exp(rho))  # rho to std\n        return self.log_normal(weights, mean, std)\n\n    def __call__(self, layer, spec, shape, name=None, **tags):\n        # case when user uses default init specs\n        assert tags.get(\n            \'variational\', False), ""Please declare param as variational to avoid confusion""\n\n        if not isinstance(spec, dict):\n            initial_rho = np.log(np.expm1(self.prior_std))  # std to rho\n            assert np.isfinite(initial_rho), ""too small std to initialize correctly. Please pass explicit""\\\n                "" initializer (dict with {\'mu\':mu_init, \'rho\':rho_init}).""\n            spec = {\'mu\': spec, \'rho\': init.Constant(initial_rho)}\n\n        mu_spec, rho_spec = spec[\'mu\'], spec[\'rho\']\n\n        rho = layer.add_param(\n            rho_spec, shape, name=(\n                name or \'unk\') + \'.rho\', **tags)\n        mean = layer.add_param(\n            mu_spec, shape, name=(\n                name or \'unk\') + \'.mu\', **tags)\n\n        # Reparameterization trick\n        e = self.srng.normal(shape, std=1)\n        W = mean + T.log1p(T.exp(rho)) * e\n\n        # KL divergence KL(q,p) = E_(w~q(w|x)) [log q(w|x) - log P(w)] aka\n        # variational cost\n        q_p = T.sum(\n            self.log_posterior_approx(W, mean, rho) -\n            self.log_prior(W)\n        )\n\n        # accumulate variational cost\n        layer._bbwrap_var_cost += q_p\n        return W\n\n\ndef get_var_cost(layer_or_layers, treat_as_input=None):\n    """"""\n    Returns total variational cost aka KL(q(theta|x)||p(theta)) for all layers in the network\n\n    :param layer_or_layers: top layer(s) of your network, just like with lasagne.layers.get_output\n    :param treat_as_input: don\'t accumulate over layers below these layers. See same param for lasagne.layers.get_all_layers\n\n    Alternatively, one can manually get weights for one layer via layer.get_var_cost()\n    """"""\n    cost = 0\n    for layer in lasagne.layers.get_all_layers(\n            layer_or_layers, treat_as_input):\n        if hasattr(layer, \'get_var_cost\'):\n            # if layer is bayesian or pretends so\n            cost += layer.get_var_cost()\n    return cost\n\n\ndef bbpwrap(approximation=NormalApproximation()):\n    """"""\n    A decorator that makes arbitrary lasagne layer into a bayesian network layer:\n    BayesDenseLayer = bbwrap()(DenseLayer)\n    or more verbosely,\n    @bbpwrap(NormalApproximation(pstd=0.01))\n    BayesDenseLayer(DenseLayer):\n        pass\n\n    """"""\n\n    def decorator(cls):\n        def add_param_wrap(add_param):\n            @wraps(add_param)\n            def wrapped(self, spec, shape, name=None, **tags):\n                # we should take care about some user specification\n                # to avoid bbp hook just set tags[\'variational\'] = True\n                if not tags.get(\'trainable\', True) or \\\n                        tags.get(\'variational\', False):\n                    return add_param(self, spec, shape, name, **tags)\n                else:\n                    # we declare that params we add next\n                    # are the ones we need to fit the distribution\n                    # they don\'t need to be regularized, strictly\n                    tags[\'variational\'] = True\n                    tags[\'regularizable\'] = False\n                    param = self.approximation(self, spec, shape, name, **tags)\n                    return param\n            return wrapped\n\n        def get_var_cost(self):\n            """"""\n            Returns total variational cost aka KL(q(theta|x)||p(theta)) for this layer.\n            Alternatively, use function get_var_cost(layer) to get total cost for all layers below this one.\n            """"""\n            return self._bbwrap_var_cost\n\n        cls.approximation = approximation\n        cls._bbwrap_var_cost = 0\n        cls.add_param = add_param_wrap(cls.add_param)\n        cls.get_var_cost = get_var_cost\n        return cls\n\n    return decorator\n'"
week06_policy_based/atari_wrappers.py,2,"b'"""""" Environment wrappers. """"""\nfrom collections import defaultdict, deque\n\nimport cv2\nimport gym\nimport gym.spaces as spaces\nfrom gym.envs import atari\nimport numpy as np\n\nfrom env_batch import ParallelEnvBatch\ncv2.ocl.setUseOpenCL(False)\n\n\nclass EpisodicLife(gym.Wrapper):\n    """""" Sets done flag to true when agent dies. """"""\n\n    def __init__(self, env):\n        super(EpisodicLife, self).__init__(env)\n        self.lives = 0\n        self.real_done = True\n\n    def step(self, action):\n        obs, rew, done, info = self.env.step(action)\n        self.real_done = done\n        info[""real_done""] = done\n        lives = self.env.unwrapped.ale.lives()\n        if 0 < lives < self.lives:\n            done = True\n        self.lives = lives\n        return obs, rew, done, info\n\n    def reset(self, **kwargs):\n        if self.real_done:\n            obs = self.env.reset(**kwargs)\n        else:\n            obs, _, _, _ = self.env.step(0)\n        self.lives = self.env.unwrapped.ale.lives()\n        return obs\n\n\nclass FireReset(gym.Wrapper):\n    """""" Makes fire action when reseting environment.\n\n    Some environments are fixed until the agent makes the fire action,\n    this wrapper makes this action so that the epsiode starts automatically.\n    """"""\n\n    def __init__(self, env):\n        super(FireReset, self).__init__(env)\n        action_meanings = env.unwrapped.get_action_meanings()\n        if len(action_meanings) < 3:\n            raise ValueError(\n                ""env.unwrapped.get_action_meanings() must be of length >= 3""\n                f""but is of length {len(action_meanings)}"")\n        if env.unwrapped.get_action_meanings()[1] != ""FIRE"":\n            raise ValueError(\n                ""env.unwrapped.get_action_meanings() must have \'FIRE\' ""\n                f""under index 1, but is {action_meanings}"")\n\n    def step(self, action):\n        return self.env.step(action)\n\n    def reset(self, **kwargs):\n        self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(1)\n        if done:\n            self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(2)\n        if done:\n            self.env.reset(**kwargs)\n        return obs\n\n\nclass StartWithRandomActions(gym.Wrapper):\n    """""" Makes random number of random actions at the beginning of each\n    episode. """"""\n\n    def __init__(self, env, max_random_actions=30):\n        super(StartWithRandomActions, self).__init__(env)\n        self.max_random_actions = max_random_actions\n        self.real_done = True\n\n    def step(self, action):\n        obs, rew, done, info = self.env.step(action)\n        self.real_done = info.get(""real_done"", True)\n        return obs, rew, done, info\n\n    def reset(self, **kwargs):\n        obs = self.env.reset()\n        if self.real_done:\n            num_random_actions = np.random.randint(self.max_random_actions + 1)\n            for _ in range(num_random_actions):\n                obs, _, _, _ = self.env.step(self.env.action_space.sample())\n            self.real_done = False\n        return obs\n\n\nclass ImagePreprocessing(gym.ObservationWrapper):\n    """""" Preprocesses image-observations by possibly grayscaling and resizing. """"""\n\n    def __init__(self, env, width=84, height=84, grayscale=True):\n        super(ImagePreprocessing, self).__init__(env)\n        self.width = width\n        self.height = height\n        self.grayscale = grayscale\n        ospace = self.env.observation_space\n        low, high, dtype = ospace.low.min(), ospace.high.max(), ospace.dtype\n        if self.grayscale:\n            self.observation_space = spaces.Box(\n                low=low,\n                high=high,\n                shape=(width, height),\n                dtype=dtype,\n            )\n        else:\n            obs_shape = (width, height) + self.observation_space.shape[2:]\n            self.observation_space = spaces.Box(low=low, high=high,\n                                                shape=obs_shape, dtype=dtype)\n\n    def observation(self, observation):\n        """""" Performs image preprocessing. """"""\n        if self.grayscale:\n            observation = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n        observation = cv2.resize(observation, (self.width, self.height),\n                                 cv2.INTER_AREA)\n        return observation\n\n\nclass MaxBetweenFrames(gym.ObservationWrapper):\n    """""" Takes maximum between two subsequent frames. """"""\n\n    def __init__(self, env):\n        if (isinstance(env.unwrapped, atari.AtariEnv) and\n                ""NoFrameskip"" not in env.spec.id):\n            raise ValueError(\n                ""MaxBetweenFrames requires NoFrameskip in atari env id"")\n        super(MaxBetweenFrames, self).__init__(env)\n        self.last_obs = None\n\n    def observation(self, observation):\n        obs = np.maximum(observation, self.last_obs)\n        self.last_obs = observation\n        return obs\n\n    def reset(self, **kwargs):\n        self.last_obs = self.env.reset()\n        return self.last_obs\n\n\nclass QueueFrames(gym.ObservationWrapper):\n    """""" Queues specified number of frames together along new dimension. """"""\n\n    def __init__(self, env, nframes, concat=False):\n        super(QueueFrames, self).__init__(env)\n        self.obs_queue = deque([], maxlen=nframes)\n        self.concat = concat\n        ospace = self.observation_space\n        if self.concat:\n            oshape = ospace.shape[:-1] + (ospace.shape[-1] * nframes,)\n        else:\n            oshape = ospace.shape + (nframes,)\n        self.observation_space = spaces.Box(\n            ospace.low.min(), ospace.high.max(), oshape, ospace.dtype)\n\n    def observation(self, observation):\n        self.obs_queue.append(observation)\n        return (np.concatenate(self.obs_queue, -1) if self.concat\n                else np.dstack(self.obs_queue))\n\n    def reset(self, **kwargs):\n        obs = self.env.reset()\n        for _ in range(self.obs_queue.maxlen - 1):\n            self.obs_queue.append(obs)\n        return self.observation(obs)\n\n\nclass SkipFrames(gym.Wrapper):\n    """""" Performs the same action for several steps and returns the final result.\n    """"""\n\n    def __init__(self, env, nskip=4):\n        super(SkipFrames, self).__init__(env)\n        if (isinstance(env.unwrapped, atari.AtariEnv) and\n                ""NoFrameskip"" not in env.spec.id):\n            raise ValueError(""SkipFrames requires NoFrameskip in atari env id"")\n        self.nskip = nskip\n\n    def step(self, action):\n        total_reward = 0.0\n        for _ in range(self.nskip):\n            obs, rew, done, info = self.env.step(action)\n            total_reward += rew\n            if done:\n                break\n        return obs, total_reward, done, info\n\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)\n\n\nclass ClipReward(gym.RewardWrapper):\n    """""" Modifes reward to be in {-1, 0, 1} by taking sign of it. """"""\n\n    def reward(self, reward):\n        return np.sign(reward)\n\n\nclass SummariesBase(gym.Wrapper):\n    """""" Env summaries writer base.""""""\n\n    def __init__(self, env, prefix=None, running_mean_size=100):\n        super().__init__(env)\n        self.episode_counter = 0\n        self.prefix = prefix or self.env.spec.id\n\n        nenvs = getattr(self.env.unwrapped, ""nenvs"", 1)\n        self.rewards = np.zeros(nenvs)\n        self.had_ended_episodes = np.zeros(nenvs, dtype=np.bool)\n        self.episode_lengths = np.zeros(nenvs)\n        self.reward_queues = [deque([], maxlen=running_mean_size)\n                              for _ in range(nenvs)]\n\n    def should_write_summaries(self):\n        """""" Returns true if it\'s time to write summaries. """"""\n        return np.all(self.had_ended_episodes)\n\n    def add_summaries(self):\n        """""" Writes summaries. """"""\n        self.add_summary_scalar(\n            f""{self.prefix}/total_reward"",\n            np.mean([q[-1] for q in self.reward_queues]))\n        self.add_summary_scalar(\n            f""{self.prefix}/reward_mean_{self.reward_queues[0].maxlen}"",\n            np.mean([np.mean(q) for q in self.reward_queues]))\n        self.add_summary_scalar(\n            f""{self.prefix}/episode_length"",\n            np.mean(self.episode_lengths))\n        if self.had_ended_episodes.size > 1:\n            self.add_summary_scalar(\n                f""{self.prefix}/min_reward"",\n                min(q[-1] for q in self.reward_queues))\n            self.add_summary_scalar(\n                f""{self.prefix}/max_reward"",\n                max(q[-1] for q in self.reward_queues))\n        self.episode_lengths.fill(0)\n        self.had_ended_episodes.fill(False)\n\n    def step(self, action):\n        obs, rew, done, info = self.env.step(action)\n        self.rewards += rew\n        self.episode_lengths[~self.had_ended_episodes] += 1\n\n        info_collection = [info] if isinstance(info, dict) else info\n        done_collection = [done] if isinstance(done, bool) else done\n        done_indices = [i for i, info in enumerate(info_collection)\n                        if info.get(""real_done"", done_collection[i])]\n        for i in done_indices:\n            if not self.had_ended_episodes[i]:\n                self.had_ended_episodes[i] = True\n            self.reward_queues[i].append(self.rewards[i])\n            self.rewards[i] = 0\n\n        if self.should_write_summaries():\n            self.add_summaries()\n        return obs, rew, done, info\n\n    def reset(self, **kwargs):\n        self.rewards.fill(0)\n        self.episode_lengths.fill(0)\n        self.had_ended_episodes.fill(False)\n        return self.env.reset(**kwargs)\n\n\nclass TFSummaries(SummariesBase):\n    """""" Writes env summaries using TensorFlow.""""""\n\n    def __init__(self, env, prefix=None, running_mean_size=100, step_var=None):\n\n        super().__init__(env, prefix, running_mean_size)\n\n        import tensorflow as tf\n        self.step_var = (step_var if step_var is not None\n                         else tf.train.get_global_step())\n\n    def add_summary_scalar(self, name, value):\n        import tensorflow as tf\n        tf.contrib.summary.scalar(name, value, step = self.step_var)\n\n\nclass NumpySummaries(SummariesBase):\n\n    _summaries = defaultdict(list)\n    _summary_step = None\n\n    @classmethod\n    def set_step(cls, step):\n        cls._summary_step = step\n\n    @classmethod\n    def get_values(cls, name):\n        return cls._summaries[name]\n\n    @classmethod\n    def clear(cls):\n        cls._summaries = defaultdict(list)\n\n    def __init__(self, env, prefix = None, running_mean_size = 100):\n        super().__init__(env, prefix, running_mean_size)\n\n    def add_summary_scalar(self, name, value):\n        self._summaries[name].append((self._summary_step, value))\n\n\ndef nature_dqn_env(env_id, nenvs=None, seed=None,\n                   summaries=\'TensorFlow\', clip_reward=True):\n    """""" Wraps env as in Nature DQN paper. """"""\n    if ""NoFrameskip"" not in env_id:\n        raise ValueError(f""env_id must have \'NoFrameskip\' but is {env_id}"")\n    if nenvs is not None:\n        if seed is None:\n            seed = list(range(nenvs))\n        if isinstance(seed, int):\n            seed = [seed] * nenvs\n        if len(seed) != nenvs:\n            raise ValueError(f""seed has length {len(seed)} but must have ""\n                             f""length equal to nenvs which is {nenvs}"")\n\n        env = ParallelEnvBatch([\n            lambda i=i, env_seed=env_seed: nature_dqn_env(\n                env_id, seed=env_seed, summaries=False, clip_reward=False)\n            for i, env_seed in enumerate(seed)\n        ])\n        if summaries:\n            summaries_class = NumpySummaries if summaries == \'Numpy\' else TFSummaries\n            env = summaries_class(env, prefix=env_id)\n        if clip_reward:\n            env = ClipReward(env)\n        return env\n\n    env = gym.make(env_id)\n    env.seed(seed)\n    if summaries:\n        env = TFSummaries(env)\n    env = EpisodicLife(env)\n    if ""FIRE"" in env.unwrapped.get_action_meanings():\n        env = FireReset(env)\n    env = StartWithRandomActions(env, max_random_actions=30)\n    env = MaxBetweenFrames(env)\n    env = SkipFrames(env, 4)\n    env = ImagePreprocessing(env, width=84, height=84, grayscale=True)\n    env = QueueFrames(env, 4)\n    if clip_reward:\n        env = ClipReward(env)\n    return env\n'"
week06_policy_based/env_batch.py,0,"b'# pylint: skip-file\nfrom multiprocessing import Process, Pipe\n\nfrom gym import Env, Wrapper, Space\nimport numpy as np\n\n\nclass SpaceBatch(Space):\n    def __init__(self, spaces):\n        first_type = type(spaces[0])\n        first_shape = spaces[0].shape\n        first_dtype = spaces[0].dtype\n        for space in spaces:\n            if not isinstance(space, first_type):\n                raise TypeError(""spaces have different types: {}, {}""\n                                .format(first_type, type(space)))\n            if first_shape != space.shape:\n                raise ValueError(""spaces have different shapes: {}, {}""\n                                 .format(first_shape, space.shape))\n            if first_dtype != space.dtype:\n                raise ValueError(""spaces have different data types: {}, {}""\n                                 .format(first_dtype, space.dtype))\n\n        self.spaces = spaces\n        super(SpaceBatch, self).__init__(shape=self.spaces[0].shape,\n                                         dtype=self.spaces[0].dtype)\n\n    def sample(self):\n        return np.stack([space.sample() for space in self.spaces])\n\n    def __getattr__(self, attr):\n        return getattr(self.spaces[0], attr)\n\n\nclass EnvBatch(Env):\n    def __init__(self, make_env, nenvs=None):\n        make_env_functions = self._get_make_env_functions(make_env, nenvs)\n        self._envs = [make_env() for make_env in make_env_functions]\n        self._nenvs = len(self.envs)\n        # self.observation_space = SpaceBatch([env.observation_space\n        #                                      for env in self._envs])\n        self.action_space = SpaceBatch([env.action_space\n                                        for env in self._envs])\n\n    def _get_make_env_functions(self, make_env, nenvs):\n        if nenvs is None and not isinstance(make_env, list):\n            raise ValueError(""When nenvs is None make_env""\n                             "" must be a list of callables"")\n        if nenvs is not None and not callable(make_env):\n            raise ValueError(\n                ""When nenvs is not None make_env must be callable"")\n\n        if nenvs is not None:\n            make_env = [make_env for _ in range(nenvs)]\n        return make_env\n\n    @property\n    def nenvs(self):\n        return self._nenvs\n\n    @property\n    def envs(self):\n        return self._envs\n\n    def _check_actions(self, actions):\n        if not len(actions) == self.nenvs:\n            raise ValueError(\n                ""number of actions is not equal to number of envs: ""\n                ""len(actions) = {}, nenvs = {}""\n                .format(len(actions), self.nenvs))\n\n    def step(self, actions):\n        self._check_actions(actions)\n        obs, rews, resets, infos = [], [], [], []\n        for env, action in zip(self._envs, actions):\n            ob, rew, done, info = env.step(action)\n            if done:\n                ob = env.reset()\n            obs.append(ob)\n            rews.append(rew)\n            resets.append(done)\n            infos.append(info)\n        return np.stack(obs), np.stack(rews), np.stack(resets), infos\n\n    def reset(self):\n        return np.stack([env.reset() for env in self.envs])\n\n\nclass SingleEnvBatch(Wrapper, EnvBatch):\n    def __init__(self, env):\n        super(SingleEnvBatch, self).__init__(env)\n        self.observation_space = SpaceBatch([self.env.observation_space])\n        self.action_space = SpaceBatch([self.env.action_space])\n\n    @property\n    def nenvs(self):\n        return 1\n\n    @property\n    def envs(self):\n        return [self.env]\n\n    def step(self, actions):\n        self._check_actions(actions)\n        ob, rew, done, info = self.env.step(actions[0])\n        if done:\n            ob = self.env.reset()\n        return (\n            ob[None],\n            np.expand_dims(rew, 0),\n            np.expand_dims(done, 0),\n            [info],\n        )\n\n    def reset(self):\n        return self.env.reset()[None]\n\n\ndef worker(parent_connection, worker_connection, make_env_function,\n           send_spaces=True):\n    # Adapted from SubprocVecEnv github.com/openai/baselines\n    parent_connection.close()\n    env = make_env_function()\n    if send_spaces:\n        worker_connection.send((env.observation_space, env.action_space))\n    while True:\n        cmd, action = worker_connection.recv()\n        if cmd == ""step"":\n            ob, rew, done, info = env.step(action)\n            if done:\n                ob = env.reset()\n            worker_connection.send((ob, rew, done, info))\n        elif cmd == ""reset"":\n            ob = env.reset()\n            worker_connection.send(ob)\n        elif cmd == ""close"":\n            env.close()\n            worker_connection.close()\n            break\n        else:\n            raise NotImplementedError(""Unknown command %s"" % cmd)\n\n\nclass ParallelEnvBatch(EnvBatch):\n    """"""\n    An abstract batch of environments.\n    """"""\n\n    def __init__(self, make_env, nenvs=None):\n        make_env_functions = self._get_make_env_functions(make_env, nenvs)\n        self._nenvs = len(make_env_functions)\n        self._parent_connections, self._worker_connections = zip(*[\n            Pipe() for _ in range(self._nenvs)\n        ])\n        self._processes = [\n            Process(\n                target=worker,\n                args=(parent_connection, worker_connection, make_env),\n                daemon=True\n            )\n            for i, (parent_connection, worker_connection, make_env)\n            in enumerate(zip(self._parent_connections,\n                             self._worker_connections,\n                             make_env_functions))\n        ]\n        for p in self._processes:\n            p.start()\n        self._closed = False\n\n        for conn in self._worker_connections:\n            conn.close()\n\n        observation_spaces, action_spaces = [], []\n        for conn in self._parent_connections:\n            ob_space, ac_space = conn.recv()\n            observation_spaces.append(ob_space)\n            action_spaces.append(ac_space)\n        self.observation_space = SpaceBatch(observation_spaces)\n        self.action_space = SpaceBatch(action_spaces)\n\n    @property\n    def nenvs(self):\n        return self._nenvs\n\n    def step(self, actions):\n        self._check_actions(actions)\n        for conn, a in zip(self._parent_connections, actions):\n            conn.send((""step"", a))\n        results = [conn.recv() for conn in self._parent_connections]\n        obs, rews, dones, infos = zip(*results)\n        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n\n    def reset(self):\n        for conn in self._parent_connections:\n            conn.send((""reset"", None))\n        return np.stack([conn.recv() for conn in self._parent_connections])\n\n    def close(self):\n        if self._closed:\n            return\n        for conn in self._parent_connections:\n            conn.send((""close"", None))\n        for p in self._processes:\n            p.join()\n        self._closed = True\n\n    def render(self):\n        raise ValueError(""render not defined for %s"" % self)\n'"
week06_policy_based/runners.py,0,"b'"""""" RL env runner """"""\nfrom collections import defaultdict\n\nimport numpy as np\n\n\nclass EnvRunner:\n    """""" Reinforcement learning runner in an environment with given policy """"""\n\n    def __init__(self, env, policy, nsteps, transforms=None, step_var=None):\n        self.env = env\n        self.policy = policy\n        self.nsteps = nsteps\n        self.transforms = transforms or []\n        self.step_var = step_var if step_var is not None else 0\n        self.state = {""latest_observation"": self.env.reset()}\n\n    @property\n    def nenvs(self):\n        """""" Returns number of batched envs or `None` if env is not batched """"""\n        return getattr(self.env.unwrapped, ""nenvs"", None)\n\n    def reset(self):\n        """""" Resets env and runner states. """"""\n        self.state[""latest_observation""] = self.env.reset()\n        self.policy.reset()\n\n    def get_next(self):\n        """""" Runs the agent in the environment.  """"""\n        trajectory = defaultdict(list, {""actions"": []})\n        observations = []\n        rewards = []\n        resets = []\n        self.state[""env_steps""] = self.nsteps\n\n        for i in range(self.nsteps):\n            observations.append(self.state[""latest_observation""])\n            act = self.policy.act(self.state[""latest_observation""])\n            if ""actions"" not in act:\n                raise ValueError(""result of policy.act must contain \'actions\' ""\n                                 f""but has keys {list(act.keys())}"")\n            for key, val in act.items():\n                trajectory[key].append(val)\n\n            obs, rew, done, _ = self.env.step(trajectory[""actions""][-1])\n            self.state[""latest_observation""] = obs\n            rewards.append(rew)\n            resets.append(done)\n            self.step_var += self.nenvs or 1\n\n            # Only reset if the env is not batched. Batched envs should\n            # auto-reset.\n            if not self.nenvs and np.all(done):\n                self.state[""env_steps""] = i + 1\n                self.state[""latest_observation""] = self.env.reset()\n\n        trajectory.update(\n            observations=observations,\n            rewards=rewards,\n            resets=resets)\n        trajectory[""state""] = self.state\n\n        for transform in self.transforms:\n            transform(trajectory)\n        return trajectory\n'"
week07_seq2seq/basic_model_tf.py,52,"b'import tensorflow as tf\nimport keras.layers as L\n\n# This code implements a single-GRU seq2seq model. You will have to improve it later in the assignment.\n# Note 1: when using several recurrent layers TF can mixed up the weights of different recurrent layers.\n# In that case, make sure you both create AND use each rnn/gru/lstm/custom layer in a unique variable scope\n# e.g. with tf.variable_scope(""first_lstm""): new_cell, new_out = self.lstm_1(...)\n#      with tf.variable_scope(""second_lstm""): new_cell2, new_out2 = self.lstm_2(...)\n# Note 2: everything you need for decoding should be stored in model state (output list of both encode and decode)\n# e.g. for attention, you should store all encoder sequence and input mask\n# there in addition to lstm/gru states.\n\n\nclass BasicTranslationModel:\n    def __init__(self, name, inp_voc, out_voc,\n                 emb_size, hid_size,):\n\n        self.name = name\n        self.inp_voc = inp_voc\n        self.out_voc = out_voc\n\n        with tf.variable_scope(name):\n            self.emb_inp = L.Embedding(len(inp_voc), emb_size)\n            self.emb_out = L.Embedding(len(out_voc), emb_size)\n            self.enc0 = tf.nn.rnn_cell.GRUCell(hid_size)\n            self.dec_start = L.Dense(hid_size)\n            self.dec0 = tf.nn.rnn_cell.GRUCell(hid_size)\n            self.logits = L.Dense(len(out_voc))\n\n            # run on dummy output to .build all layers (and therefore create\n            # weights)\n            inp = tf.placeholder(\'int32\', [None, None])\n            out = tf.placeholder(\'int32\', [None, None])\n            h0 = self.encode(inp)\n            h1 = self.decode(h0, out[:, 0])\n            # h2 = self.decode(h1,out[:,1]) etc.\n\n        self.weights = tf.get_collection(\n            tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)\n\n    def encode(self, inp, **flags):\n        """"""\n        Takes symbolic input sequence, computes initial state\n        :param inp: matrix of input tokens [batch, time]\n        :return: a list of initial decoder state tensors\n        """"""\n        inp_lengths = infer_length(inp, self.inp_voc.eos_ix)\n        inp_emb = self.emb_inp(inp)\n\n        _, enc_last = tf.nn.dynamic_rnn(\n            self.enc0, inp_emb,\n            sequence_length=inp_lengths,\n            dtype=inp_emb.dtype)\n\n        dec_start = self.dec_start(enc_last)\n        return [dec_start]\n\n    def decode(self, prev_state, prev_tokens, **flags):\n        """"""\n        Takes previous decoder state and tokens, returns new state and logits\n        :param prev_state: a list of previous decoder state tensors\n        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n        :return: a list of next decoder state tensors, a tensor of logits [batch,n_tokens]\n        """"""\n\n        [prev_dec] = prev_state\n\n        prev_emb = self.emb_out(prev_tokens[:, None])[:, 0]\n\n        new_dec_out, new_dec_state = self.dec0(prev_emb, prev_dec)\n\n        output_logits = self.logits(new_dec_out)\n\n        return [new_dec_state], output_logits\n\n    def symbolic_score(self, inp, out, eps=1e-30, **flags):\n        """"""\n        Takes symbolic int32 matrices of hebrew words and their english translations.\n        Computes the log-probabilities of all possible english characters given english prefices and hebrew word.\n        :param inp: input sequence, int32 matrix of shape [batch,time]\n        :param out: output sequence, int32 matrix of shape [batch,time]\n        :return: log-probabilities of all possible english characters of shape [bath,time,n_tokens]\n\n        NOTE: log-probabilities time axis  is synchronized with out\n        In other words, logp are probabilities of __current__ output at each tick, not the next one\n        therefore you can get likelihood as logprobas * tf.one_hot(out,n_tokens)\n        """"""\n        first_state = self.encode(inp, **flags)\n\n        batch_size = tf.shape(inp)[0]\n        bos = tf.fill([batch_size], self.out_voc.bos_ix)\n        first_logits = tf.log(tf.one_hot(bos, len(self.out_voc)) + eps)\n\n        def step(blob, y_prev):\n            h_prev = blob[:-1]\n            h_new, logits = self.decode(h_prev, y_prev, **flags)\n            return list(h_new) + [logits]\n\n        results = tf.scan(step, initializer=list(first_state) + [first_logits],\n                          elems=tf.transpose(out))\n\n        # gather state and logits, each of shape [time,batch,...]\n        states_seq, logits_seq = results[:-1], results[-1]\n\n        # add initial state and logits\n        logits_seq = tf.concat((first_logits[None], logits_seq), axis=0)\n\n        # convert from [time,batch,...] to [batch,time,...]\n        logits_seq = tf.transpose(logits_seq, [1, 0, 2])\n\n        return tf.nn.log_softmax(logits_seq)\n\n    def symbolic_translate(\n            self,\n            inp,\n            greedy=False,\n            max_len=None,\n            eps=1e-30,\n            **flags):\n        """"""\n        takes symbolic int32 matrix of hebrew words, produces output tokens sampled\n        from the model and output log-probabilities for all possible tokens at each tick.\n        :param inp: input sequence, int32 matrix of shape [batch,time]\n        :param greedy: if greedy, takes token with highest probablity at each tick.\n            Otherwise samples proportionally to probability.\n        :param max_len: max length of output, defaults to 2 * input length\n        :return: output tokens int32[batch,time] and\n                 log-probabilities of all tokens at each tick, [batch,time,n_tokens]\n        """"""\n        first_state = self.encode(inp, **flags)\n\n        batch_size = tf.shape(inp)[0]\n        bos = tf.fill([batch_size], self.out_voc.bos_ix)\n        first_logits = tf.log(tf.one_hot(bos, len(self.out_voc)) + eps)\n        max_len = tf.reduce_max(tf.shape(inp)[1]) * 2\n\n        def step(blob, t):\n            h_prev, y_prev = blob[:-2], blob[-1]\n            h_new, logits = self.decode(h_prev, y_prev, **flags)\n            y_new = (\n                tf.argmax(logits, axis=-1) if greedy\n                else tf.multinomial(logits, 1)[:, 0]\n            )\n            return list(h_new) + [logits, tf.cast(y_new, y_prev.dtype)]\n\n        results = tf.scan(\n            step,\n            initializer=list(first_state) + [first_logits, bos],\n            elems=[tf.range(max_len)],\n        )\n\n        # gather state, logits and outs, each of shape [time,batch,...]\n        states_seq, logits_seq, out_seq = (\n            results[:-2], results[-2], results[-1]\n        )\n\n        # add initial state, logits and out\n        logits_seq = tf.concat((first_logits[None], logits_seq), axis=0)\n        out_seq = tf.concat((bos[None], out_seq), axis=0)\n        states_seq = [\n            tf.concat((init[None], states), axis=0)\n            for init, states in zip(first_state, states_seq)\n        ]\n\n        # convert from [time,batch,...] to [batch,time,...]\n        logits_seq = tf.transpose(logits_seq, [1, 0, 2])\n        out_seq = tf.transpose(out_seq)\n        states_seq = [\n            tf.transpose(states, [1, 0] + list(range(2, states.shape.ndims)))\n            for states in states_seq\n        ]\n\n        return out_seq, tf.nn.log_softmax(logits_seq)\n\n\n### Utility functions ###\n\ndef initialize_uninitialized(sess=None):\n    """"""\n    Initialize unitialized variables, doesn\'t affect those already initialized\n    :param sess: in which session to initialize stuff. Defaults to tf.get_default_session()\n    """"""\n    sess = sess or tf.get_default_session()\n    global_vars = tf.global_variables()\n    is_not_initialized = sess.run(\n        [tf.is_variable_initialized(var) for var in global_vars]\n    )\n    not_initialized_vars = [\n        v for (v, f)\n        in zip(global_vars, is_not_initialized)\n        if not f\n    ]\n\n    if len(not_initialized_vars):\n        sess.run(tf.variables_initializer(not_initialized_vars))\n\n\ndef infer_length(seq, eos_ix, time_major=False, dtype=tf.int32):\n    """"""\n    compute length given output indices and eos code\n    :param seq: tf matrix [time,batch] if time_major else [batch,time]\n    :param eos_ix: integer index of end-of-sentence token\n    :returns: lengths, int32 vector of shape [batch]\n    """"""\n    axis = 0 if time_major else 1\n    is_eos = tf.cast(tf.equal(seq, eos_ix), dtype)\n    count_eos = tf.cumsum(is_eos, axis=axis, exclusive=True)\n    lengths = tf.reduce_sum(tf.cast(tf.equal(count_eos, 0), dtype), axis=axis)\n    return lengths\n\n\ndef infer_mask(seq, eos_ix, time_major=False, dtype=tf.float32):\n    """"""\n    compute mask given output indices and eos code\n    :param seq: tf matrix [time,batch] if time_major else [batch,time]\n    :param eos_ix: integer index of end-of-sentence token\n    :returns: mask, float32 matrix with \'0\'s and \'1\'s of same shape as seq\n    """"""\n    axis = 0 if time_major else 1\n    lengths = infer_length(seq, eos_ix, time_major=time_major)\n    mask = tf.sequence_mask(lengths, maxlen=tf.shape(seq)[axis], dtype=dtype)\n    if time_major:\n        mask = tf.transpose(mask)\n    return mask\n\n\ndef select_values_over_last_axis(values, indices):\n    """"""\n    Auxiliary function to select logits corresponding to chosen tokens.\n    :param values: logits for all actions: float32[batch,tick,action]\n    :param indices: action ids int32[batch,tick]\n    :returns: values selected for the given actions: float[batch,tick]\n    """"""\n    assert values.shape.ndims == 3 and indices.shape.ndims == 2\n    batch_size, seq_len = tf.shape(indices)[0], tf.shape(indices)[1]\n    batch_i = tf.tile(tf.range(0, batch_size)[:, None], [1, seq_len])\n    time_i = tf.tile(tf.range(0, seq_len)[None, :], [batch_size, 1])\n    indices_nd = tf.stack([batch_i, time_i, indices], axis=-1)\n\n    return tf.gather_nd(values, indices_nd)\n'"
week07_seq2seq/basic_model_theano.py,1,"b'# code by https://github.com/deniskamazur\n\nfrom lasagne.layers import *\nimport theano.tensor as T\nimport theano\n\nfrom agentnet.memory import LSTMCell, GRUCell, AttentionLayer\nfrom agentnet import Recurrence\nfrom agentnet.learning.generic import get_mask_by_eos\nfrom agentnet.resolver import ProbabilisticResolver\nfrom agentnet.utils import reapply\n\n\nclass BasicTranslationModel:\n    def __init__(self, inp_voc, out_voc, emb_size, hid_size, **kwargs):\n        """"""\n        A simple interface for mt\n        :param emb_size: Embedding size\n        :param hid_size: Number of LSTM units\n        :param bidereactional: If the nLSTM layers should be bidirectional\n        :param input_dropout: Dropout after embedding layer\n        :param recurrent_dropout: Dropout after each LSTM iteration\n        :param rdo_size: If int - use dense layer after neck in decoder, if none don\'t\n        :param peepholes: http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-peepholes.png\n        :param kwargs: recurrence flags\n        """"""\n        self.inp_voc = inp_voc\n        self.out_voc = out_voc\n        # encode input sequence\n\n        class encoder:\n            # intput layers\n            inp = InputLayer((None, None))\n            mask = ExpressionLayer(\n                inp,\n                lambda x: get_mask_by_eos(T.eq(x, self.out_voc.eos_ix)),\n            )\n\n            # embed the tokens\n            emb = EmbeddingLayer(\n                inp,\n                input_size=len(inp_voc),\n                output_size=emb_size,\n            )\n\n            rnn_fw = GRULayer(\n                emb,\n                num_units=hid_size,\n                mask_input=mask,\n                only_return_final=True,\n            )\n\n            dec_start = DenseLayer(rnn_fw, hid_size, nonlinearity=None)\n\n        # make encoder a public field\n        self.encoder = encoder\n\n        # decoder the encoded sequence\n        class decoder:\n            # decoder previous memory and tokens\n            prev_hid = InputLayer((None, hid_size), name=\'prev hidden state\')\n            inp = InputLayer((None,), name=""prev phoneme"")\n\n            emb = EmbeddingLayer(inp, len(out_voc), emb_size)\n\n            new_hid = GRUCell(prev_hid, emb)\n\n            logits = DenseLayer(new_hid, len(out_voc), nonlinearity=None)\n\n            probs = NonlinearityLayer(logits, nonlinearity=T.nnet.softmax)\n            logprobs = NonlinearityLayer(\n                logits,\n                nonlinearity=T.nnet.logsoftmax,\n            )\n            out = ProbabilisticResolver(probs, assume_normalized=True)\n\n            state_dict = {\n                new_hid: prev_hid,\n                # ^^^ this reads ""at next step, new_hid will become prev_hid""\n                # if you add any more recurrent memory units,\n                # please make sure they\'re here\n            }\n\n            init_dict = {\n                new_hid: encoder.dec_start\n                # ^^^ this reads ""before first step, new_hid is set to outputs of dec_start""\n                # if you add any more recurrent memory units with non-zero init\n                # please make sure they\'re here\n            }\n\n            nonseq_dict = {\n                # here you can add anything encoder needs that\'s gonna be same\n                # across time-steps\n            }\n\n        self.decoder = decoder\n\n        top_layers = [encoder.dec_start, decoder.out] + \\\n            list(decoder.state_dict.keys())\n        self.weights = get_all_params(top_layers, trainable=True)\n\n    def symbolic_score(self, inp, out, eps=1e-30, **flags):\n        """"""\n        Takes symbolic int32 matrices of hebrew words and their english translations.\n        Computes the log-probabilities of all possible english characters given english prefices and hebrew word.\n        :param inp: input sequence, int32 matrix of shape [batch,time]\n        :param out: output sequence, int32 matrix of shape [batch,time]\n        :return: log-probabilities of all possible english characters of shape [bath,time,n_tokens]\n\n        NOTE: log-probabilities time axis  is synchronized with out\n        In other words, logp are probabilities of __current__ output at each tick, not the next one\n        therefore you can get likelihood as logprobas * tf.one_hot(out,n_tokens)\n        """"""\n\n        l_output_sequence = InputLayer([None, None])\n\n        # Defining custom recurrent layer out of decoder\n        rec = Recurrence(\n            state_variables=self.decoder.state_dict,\n            state_init=self.decoder.init_dict,\n            input_sequences={self.decoder.inp: l_output_sequence},\n            input_nonsequences=self.decoder.nonseq_dict,\n            tracked_outputs=self.decoder.logprobs,\n            unroll_scan=False\n        )\n\n        feed_dict = {\n            self.encoder.inp: inp,\n            l_output_sequence: out\n        }\n        logprobs = get_output(rec[self.decoder.logprobs], feed_dict,\n                              recurrence_flags=flags, **flags)\n\n        self.auto_updates = rec.get_automatic_updates()\n        if len(self.auto_updates) != 0:\n            print(\n                ""symbolic_score: Please collect auto_updates of random states ""\n                ""after you called symbolic_score (available at model.auto_updates)!"")\n\n        first_logprobs = T.zeros_like(logprobs[:, :1])\n        logprobs = T.concatenate([first_logprobs, logprobs[:, :-1]], axis=1)\n\n        return logprobs\n\n    def symbolic_translate(self, inp, greedy=False, max_len=None,\n                           unroll_scan=False, eps=1e-30, **flags):\n        """"""\n        takes symbolic int32 matrix of hebrew words, produces output tokens sampled\n        from the model and output log-probabilities for all possible tokens at each tick.\n        :param inp: input sequence, int32 matrix of shape [batch,time]\n        :param greedy: if greedy, takes token with highest probablity at each tick.\n            Otherwise samples proportionally to probability.\n        :param max_len: max length of output, defaults to 2 * input length\n        :param unroll_scan: if True, compiles longer but runs faster.\n                            requires max_len to be constant\n        :return: output tokens int32[batch,time] and\n                 log-probabilities of all tokens at each tick, [batch,time,n_tokens]\n        """"""\n        if unroll_scan:\n            assert isinstance(\n                max_len, int), ""if scan is unrolled, max_len must be a constant integer""\n\n        max_len = max_len if max_len is not None else 2 * inp.shape[1]\n\n        # initial output tokens (BOS)\n        bos = T.zeros_like(inp[:, 0]) + self.out_voc.bos_ix\n        l_start = InputLayer((None,), bos)\n\n        # Defining custom recurrent layer out of decoder\n        rec = Recurrence(\n            state_variables=merge_dicts(self.decoder.state_dict,\n                                        {self.decoder.out: self.decoder.inp}),\n            state_init=merge_dicts(self.decoder.init_dict, {self.decoder.out: l_start}),\n            input_nonsequences=self.decoder.nonseq_dict,\n            tracked_outputs=(self.decoder.out, self.decoder.probs, self.decoder.logprobs),\n            n_steps=max_len,\n            unroll_scan=unroll_scan\n        )\n\n        translations, logprobs = get_output(rec[self.decoder.out, self.decoder.logprobs],\n                                            {self.encoder.inp: inp,\n                                             l_start: bos},\n                                            recurrence_flags=dict(flags, greedy=greedy),\n                                            **flags)\n\n        self.auto_updates = rec.get_automatic_updates()\n        if len(self.auto_updates) != 0:\n            print(\n                ""symbolic_translate: Please collect auto_updates of random states ""\n                ""after you called symbolic_translate (available at model.auto_updates)!"")\n\n        # add first step (bos)\n        translations = T.concatenate([bos[:, None], translations], axis=1)\n        first_logprobs = T.zeros_like(logprobs[:, :1])\n        logprobs = T.concatenate([first_logprobs, logprobs], axis=1)\n\n        return translations, logprobs\n\n\ndef merge_dicts(*dicts, **kwargs):\n    """"""\n    Melts several dicts into one. Useful when messing with feed dicts\n    :param dicts: dictionaries\n    :param check_conflicts: if True, raises error if several dicts have the same key\n                    Otherwise uses the key from the latest dict in *dicts\n    :return: a dict that contains k-v pairs from  all *dicts\n    """"""\n    merged_dict = {}\n    for d in dicts:\n        merged_dict.update(d)\n    if kwargs.get(\'check_conflicts\'):\n        assert len(merged_dict) == sum(\n            map(len, dicts)), \'dicts have duplicate keys\'\n    return merged_dict\n'"
week07_seq2seq/basic_model_torch.py,1,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Note: unlike official pytorch tutorial, this model doesn\'t process one sample at a time\n# because it\'s slow on GPU.  instead it uses masks just like ye olde theano/tensorflow.\n# it doesn\'t use torch.nn.utils.rnn.pack_paded_sequence because reasons.\n\n\nclass BasicTranslationModel(nn.Module):\n    def __init__(self, inp_voc, out_voc,\n                 emb_size, hid_size,):\n        super(self.__class__, self).__init__()\n        self.inp_voc = inp_voc\n        self.out_voc = out_voc\n\n        self.emb_inp = nn.Embedding(len(inp_voc), emb_size)\n        self.emb_out = nn.Embedding(len(out_voc), emb_size)\n        self.enc0 = nn.GRU(emb_size, hid_size, batch_first=True)\n        self.dec_start = nn.Linear(hid_size, hid_size)\n        self.dec0 = nn.GRUCell(emb_size, hid_size)\n        self.logits = nn.Linear(hid_size, len(out_voc))\n\n    def encode(self, inp, **flags):\n        """"""\n        Takes symbolic input sequence, computes initial state\n        :param inp: a vector of input tokens  (Variable, int64, 1d)\n        :return: a list of initial decoder state tensors\n        """"""\n        inp_emb = self.emb_inp(inp)\n        enc_seq, _ = self.enc0(inp_emb)\n\n        # select last element w.r.t. mask\n        end_index = infer_length(inp, self.inp_voc.eos_ix)\n        end_index[end_index >= inp.shape[1]] = inp.shape[1] - 1\n        enc_last = enc_seq[range(0, enc_seq.shape[0]), end_index.detach(), :]\n\n        dec_start = self.dec_start(enc_last)\n        return [dec_start]\n\n    def decode(self, prev_state, prev_tokens, **flags):\n        """"""\n        Takes previous decoder state and tokens, returns new state and logits\n        :param prev_state: a list of previous decoder state tensors\n        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n        :return: a list of next decoder state tensors, a tensor of logits [batch,n_tokens]\n        """"""\n        [prev_dec] = prev_state\n\n        prev_emb = self.emb_out(prev_tokens)\n        new_dec_state = self.dec0(prev_emb, prev_dec)\n        output_logits = self.logits(new_dec_state)\n\n        return [new_dec_state], output_logits\n\n    def forward(self, inp, out, eps=1e-30, **flags):\n        """"""\n        Takes symbolic int32 matrices of hebrew words and their english translations.\n        Computes the log-probabilities of all possible english characters given english prefices and hebrew word.\n        :param inp: input sequence, int32 matrix of shape [batch,time]\n        :param out: output sequence, int32 matrix of shape [batch,time]\n        :return: log-probabilities of all possible english characters of shape [bath,time,n_tokens]\n\n        Note: log-probabilities time axis is synchronized with out\n        In other words, logp are probabilities of __current__ output at each tick, not the next one\n        therefore you can get likelihood as logprobas * tf.one_hot(out,n_tokens)\n        """"""\n        device = next(self.parameters()).device\n        batch_size = inp.shape[0]\n        bos = torch.tensor(\n            [self.out_voc.bos_ix] * batch_size,\n            dtype=torch.long,\n            device=device,\n        )\n        logits_seq = [torch.log(to_one_hot(bos, len(self.out_voc)) + eps)]\n\n        hid_state = self.encode(inp, **flags)\n        for x_t in out.transpose(0, 1)[:-1]:\n            hid_state, logits = self.decode(hid_state, x_t, **flags)\n            logits_seq.append(logits)\n\n        return F.log_softmax(torch.stack(logits_seq, dim=1), dim=-1)\n\n    def translate(self, inp, greedy=False, max_len=None, eps=1e-30, **flags):\n        """"""\n        takes symbolic int32 matrix of hebrew words, produces output tokens sampled\n        from the model and output log-probabilities for all possible tokens at each tick.\n        :param inp: input sequence, int32 matrix of shape [batch,time]\n        :param greedy: if greedy, takes token with highest probablity at each tick.\n            Otherwise samples proportionally to probability.\n        :param max_len: max length of output, defaults to 2 * input length\n        :return: output tokens int32[batch,time] and\n                 log-probabilities of all tokens at each tick, [batch,time,n_tokens]\n        """"""\n        device = next(self.parameters()).device\n        batch_size = inp.shape[0]\n        bos = torch.tensor(\n            [self.out_voc.bos_ix] * batch_size,\n            dtype=torch.long,\n            device=device,\n        )\n        mask = torch.ones(batch_size, dtype=torch.uint8, device=device)\n        logits_seq = [torch.log(to_one_hot(bos, len(self.out_voc)) + eps)]\n        out_seq = [bos]\n\n        hid_state = self.encode(inp, **flags)\n        while True:\n            hid_state, logits = self.decode(hid_state, out_seq[-1], **flags)\n            if greedy:\n                _, y_t = torch.max(logits, dim=-1)\n            else:\n                probs = F.softmax(logits, dim=-1)\n                y_t = torch.multinomial(probs, 1)[:, 0]\n\n            logits_seq.append(logits)\n            out_seq.append(y_t)\n            mask &= y_t != self.out_voc.eos_ix\n\n            if not mask.any():\n                break\n            if max_len and len(out_seq) >= max_len:\n                break\n\n        return (\n            torch.stack(out_seq, 1),\n            F.log_softmax(torch.stack(logits_seq, 1), dim=-1),\n        )\n\n\n### Utility functions ###\n\ndef infer_mask(\n        seq,\n        eos_ix,\n        batch_first=True,\n        include_eos=True,\n        dtype=torch.float):\n    """"""\n    compute mask given output indices and eos code\n    :param seq: tf matrix [time,batch] if batch_first else [batch,time]\n    :param eos_ix: integer index of end-of-sentence token\n    :param include_eos: if True, the time-step where eos first occurs is has mask = 1\n    :returns: mask, float32 matrix with \'0\'s and \'1\'s of same shape as seq\n    """"""\n    assert seq.dim() == 2\n    is_eos = (seq == eos_ix).to(dtype=torch.float)\n    if include_eos:\n        if batch_first:\n            is_eos = torch.cat((is_eos[:, :1] * 0, is_eos[:, :-1]), dim=1)\n        else:\n            is_eos = torch.cat((is_eos[:1, :] * 0, is_eos[:-1, :]), dim=0)\n    count_eos = torch.cumsum(is_eos, dim=1 if batch_first else 0)\n    mask = count_eos == 0\n    return mask.to(dtype=dtype)\n\n\ndef infer_length(\n        seq,\n        eos_ix,\n        batch_first=True,\n        include_eos=True,\n        dtype=torch.long):\n    """"""\n    compute length given output indices and eos code\n    :param seq: tf matrix [time,batch] if time_major else [batch,time]\n    :param eos_ix: integer index of end-of-sentence token\n    :param include_eos: if True, the time-step where eos first occurs is has mask = 1\n    :returns: lengths, int32 vector of shape [batch]\n    """"""\n    mask = infer_mask(seq, eos_ix, batch_first, include_eos, dtype)\n    return torch.sum(mask, dim=1 if batch_first else 0)\n\n\ndef to_one_hot(y, n_dims=None):\n    """""" Take integer y (tensor or variable) with n dims and convert it to 1-hot representation with n+1 dims. """"""\n    y_tensor = y.data\n    y_tensor = y_tensor.to(dtype=torch.long).view(-1, 1)\n    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n    y_one_hot = torch.zeros(\n        y_tensor.size()[0],\n        n_dims,\n        device=y.device,\n    ).scatter_(1, y_tensor, 1)\n    y_one_hot = y_one_hot.view(*y.shape, -1)\n    return y_one_hot\n'"
week07_seq2seq/voc.py,0,"b'import numpy as np\n\n\nclass Vocab:\n    def __init__(self, tokens, bos=""__BOS__"", eos=""__EOS__"", sep=\'\'):\n        """"""\n        A special class that handles tokenizing and detokenizing\n        """"""\n        assert bos in tokens, eos in tokens\n        self.tokens = tokens\n        self.token_to_ix = {t: i for i, t in enumerate(tokens)}\n\n        self.bos = bos\n        self.bos_ix = self.token_to_ix[bos]\n        self.eos = eos\n        self.eos_ix = self.token_to_ix[eos]\n        self.sep = sep\n\n    def __len__(self):\n        return len(self.tokens)\n\n    @staticmethod\n    def from_lines(lines, bos=""__BOS__"", eos=""__EOS__"", sep=\'\'):\n        flat_lines = sep.join(list(lines))\n        flat_lines = list(flat_lines.split(sep)) if sep else list(flat_lines)\n        tokens = sorted(set(sep.join(flat_lines)))\n        tokens = [t for t in tokens if t not in (bos, eos) and len(t) != 0]\n        tokens = [bos, eos] + tokens\n        return Vocab(tokens, bos, eos, sep)\n\n    def tokenize(self, string):\n        """"""converts string to a list of tokens""""""\n        tokens = list(filter(len, string.split(self.sep))) \\\n            if self.sep != \'\' else list(string)\n        return [self.bos] + tokens + [self.eos]\n\n    def to_matrix(self, lines, max_len=None):\n        """"""\n        convert variable length token sequences into  fixed size matrix\n        example usage:\n        >>>print( as_matrix(words[:3],source_to_ix))\n        [[15 22 21 28 27 13 -1 -1 -1 -1 -1]\n         [30 21 15 15 21 14 28 27 13 -1 -1]\n         [25 37 31 34 21 20 37 21 28 19 13]]\n        """"""\n        max_len = max_len or max(map(len, lines)) + 2  # 2 for bos and eos\n\n        matrix = np.zeros((len(lines), max_len), dtype=\'int32\') + self.eos_ix\n        for i, seq in enumerate(lines):\n            tokens = self.tokenize(seq)\n            row_ix = list(map(self.token_to_ix.get, tokens))[:max_len]\n            matrix[i, :len(row_ix)] = row_ix\n\n        return matrix\n\n    def to_lines(self, matrix, crop=True):\n        """"""\n        Convert matrix of token ids into strings\n        :param matrix: matrix of tokens of int32, shape=[batch,time]\n        :param crop: if True, crops BOS and EOS from line\n        :return:\n        """"""\n        lines = []\n        for line_ix in map(list, matrix):\n            if crop:\n                if line_ix[0] == self.bos_ix:\n                    line_ix = line_ix[1:]\n                if self.eos_ix in line_ix:\n                    line_ix = line_ix[:line_ix.index(self.eos_ix)]\n            line = self.sep.join(self.tokens[i] for i in line_ix)\n            lines.append(line)\n        return lines\n'"
week08_pomdp/atari_util.py,0,"b'import cv2\nimport numpy as np\nfrom gym.core import Wrapper\nfrom gym.spaces.box import Box\n\n\nclass PreprocessAtari(Wrapper):\n    def __init__(self, env, height=42, width=42, color=False,\n                 crop=lambda img: img, n_frames=4, dim_order=\'pytorch\', reward_scale=1):\n        """"""A gym wrapper that reshapes, crops and scales image into the desired shapes""""""\n        super(PreprocessAtari, self).__init__(env)\n        self.img_size = (height, width)\n        self.crop = crop\n        self.color = color\n        self.dim_order = dim_order\n\n        self.reward_scale = reward_scale\n        n_channels = (3 * n_frames) if color else n_frames\n\n        obs_shape = {\n            \'theano\': (n_channels, height, width),\n            \'pytorch\': (n_channels, height, width),\n            \'tensorflow\': (height, width, n_channels),\n        }[dim_order]\n\n        self.observation_space = Box(0.0, 1.0, obs_shape)\n        self.framebuffer = np.zeros(obs_shape, \'float32\')\n\n    def reset(self):\n        """"""Resets the game, returns initial frames""""""\n        self.framebuffer = np.zeros_like(self.framebuffer)\n        self.update_buffer(self.env.reset())\n        return self.framebuffer\n\n    def step(self, action):\n        """"""Plays the game for 1 step, returns frame buffer""""""\n        new_img, r, done, info = self.env.step(action)\n        self.update_buffer(new_img)\n\n        return self.framebuffer, r * self.reward_scale, done, info\n\n    ### image processing ###\n\n    def update_buffer(self, img):\n        img = self.preproc_image(img)\n        offset = 3 if self.color else 1\n        if self.dim_order == \'tensorflow\':\n            axis = -1\n            cropped_framebuffer = self.framebuffer[:, :, :-offset]\n        else:\n            axis = 0\n            cropped_framebuffer = self.framebuffer[:-offset, :, :]\n        self.framebuffer = np.concatenate([img, cropped_framebuffer], axis=axis)\n\n    def preproc_image(self, img):\n        """"""what happens to the observation""""""\n        img = self.crop(img)\n        img = cv2.resize(img / 255, self.img_size, interpolation=cv2.INTER_LINEAR)\n        if not self.color:\n            img = img.mean(-1, keepdims=True)\n        if self.dim_order != \'tensorflow\':\n            img = img.transpose([2, 0, 1])  # [h, w, c] to [c, h, w]\n        return img\n'"
week08_pomdp/env_pool.py,0,"b'""""""\nA thin wrapper for openAI gym environments that maintains a set of parallel games and has a method to generate\ninteraction sessions given agent one-step applier function.\n""""""\n\nimport numpy as np\n\n# A whole lot of space invaders\n\n\nclass EnvPool(object):\n    def __init__(self, agent, make_env, n_parallel_games=1):\n        """"""\n        A special class that handles training on multiple parallel sessions\n        and is capable of some auxilary actions like evaluating agent on one game session (See .evaluate()).\n\n        :param agent: Agent which interacts with the environment.\n        :param make_env: Factory that produces environments OR a name of the gym environment.\n        :param n_games: Number of parallel games. One game by default.\n        :param max_size: Max pool size by default (if appending sessions). By default, pool is not constrained in size.\n        """"""\n        # Create atari games.\n        self.agent = agent\n        self.make_env = make_env\n        self.envs = [self.make_env() for _ in range(n_parallel_games)]\n\n        # Initial observations.\n        self.prev_observations = [env.reset() for env in self.envs]\n\n        # Agent memory variables (if you use recurrent networks).\n        self.prev_memory_states = agent.get_initial_state(n_parallel_games)\n\n        # Whether particular session has just been terminated and needs\n        # restarting.\n        self.just_ended = [False] * len(self.envs)\n\n    def interact(self, n_steps=100, verbose=False):\n        """"""Generate interaction sessions with ataries (openAI gym atari environments)\n        Sessions will have length n_steps. Each time one of games is finished, it is immediately getting reset\n        and this time is recorded in is_alive_log (See returned values).\n\n        :param n_steps: Length of an interaction.\n        :returns: observation_seq, action_seq, reward_seq, is_alive_seq\n        :rtype: a bunch of tensors [batch, tick, ...]\n        """"""\n\n        def env_step(i, action):\n            if not self.just_ended[i]:\n                new_observation, cur_reward, is_done, info = \\\n                    self.envs[i].step(action)\n                if is_done:\n                    # Game ends now, will finalize on next tick.\n                    self.just_ended[i] = True\n\n                # note: is_alive=True in any case because environment is still\n                # alive (last tick alive) in our notation.\n                return new_observation, cur_reward, True, info\n            else:\n                # Reset environment, get new observation to be used on next\n                # tick.\n                new_observation = self.envs[i].reset()\n\n                # Reset memory for new episode.\n                initial_memory_state = self.agent.get_initial_state(\n                    batch_size=1)\n                for m_i in range(len(new_memory_states)):\n                    new_memory_states[m_i][i] = initial_memory_state[m_i][0]\n\n                if verbose:\n                    print(""env %i reloaded"" % i)\n\n                self.just_ended[i] = False\n\n                return new_observation, 0, False, {\'end\': True}\n\n        history_log = []\n\n        for i in range(n_steps - 1):\n            new_memory_states, readout = self.agent.step(\n                self.prev_memory_states, self.prev_observations)\n            actions = self.agent.sample_actions(readout)\n\n            new_observations, cur_rewards, is_alive, infos = zip(\n                *map(env_step, range(len(self.envs)), actions))\n\n            # Append data tuple for this tick.\n            history_log.append(\n                (self.prev_observations, actions, cur_rewards, is_alive))\n\n            self.prev_observations = new_observations\n            self.prev_memory_states = new_memory_states\n\n        # add last observation\n        dummy_actions = [0] * len(self.envs)\n        dummy_rewards = [0] * len(self.envs)\n        dummy_mask = [1] * len(self.envs)\n        history_log.append(\n            (self.prev_observations,\n             dummy_actions,\n             dummy_rewards,\n             dummy_mask))\n\n        # cast to numpy arrays, transpose from [time, batch, ...] to [batch,\n        # time, ...]\n        history_log = [\n            np.array(tensor).swapaxes(0, 1)\n            for tensor in zip(*history_log)\n        ]\n        observation_seq, action_seq, reward_seq, is_alive_seq = history_log\n\n        return observation_seq, action_seq, reward_seq, is_alive_seq\n'"
week09_policy_II/mujoco_wrappers.py,0,"b'"""""" MuJoCo env wrappers. """"""\n# Adapted from https://github.com/openai/baselines\nimport gym\nimport numpy as np\n\n\nclass RunningMeanVar:\n    """""" Computes running mean and variance.\n\n    Args:\n      eps (float): a small constant used to initialize mean to zero and\n        variance to 1.\n      shape tuple(int): shape of the statistics.\n    """"""\n    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n\n    def __init__(self, eps=1e-4, shape=()):\n        self.mean = np.zeros(shape)\n        self.var = np.ones(shape)\n        self.count = eps\n\n    def update(self, batch):\n        """""" Updates the running statistics given a batch of samples. """"""\n        if not batch.shape[1:] == self.mean.shape:\n            raise ValueError(f""batch has invalid shape: {batch.shape}, ""\n                             f""expected shape {(None,) + self.mean.shape}"")\n        batch_mean = np.mean(batch, axis=0)\n        batch_var = np.var(batch, axis=0)\n        batch_count = batch.shape[0]\n        self.update_from_moments(batch_mean, batch_var, batch_count)\n\n    def update_from_moments(self, batch_mean, batch_var, batch_count):\n        """""" Updates the running statistics given their new values on new data. """"""\n        self.mean, self.var, self.count = update_mean_var_count_from_moments(\n            self.mean, self.var, self.count, batch_mean, batch_var, batch_count)\n\n\ndef update_mean_var_count_from_moments(mean, var, count,\n                                       batch_mean, batch_var, batch_count):\n    """""" Updates running mean statistics given a new batch. """"""\n    delta = batch_mean - mean\n    tot_count = count + batch_count\n\n    new_mean = mean + delta * batch_count / tot_count\n    new_var = (\n        var * (count / tot_count)\n        + batch_var * (batch_count / tot_count)\n        + np.square(delta) * (count * batch_count / tot_count ** 2))\n    new_count = tot_count\n\n    return new_mean, new_var, new_count\n\n\nclass Normalize(gym.Wrapper):\n    """"""\n    A vectorized wrapper that normalizes the observations\n    and returns from an environment.\n    """"""\n    # pylint: disable=too-many-arguments\n\n    def __init__(self, env, obs=True, ret=True,\n                 clipobs=10., cliprew=10., gamma=0.99, eps=1e-8):\n        super().__init__(env)\n        self.obs_rmv = (RunningMeanVar(shape=self.observation_space.shape)\n                        if obs else None)\n        self.ret_rmv = RunningMeanVar(shape=()) if ret else None\n        self.clipob = clipobs\n        self.cliprew = cliprew\n        self.ret = np.zeros(getattr(self.env.unwrapped, ""nenvs"", 1))\n        self.gamma = gamma\n        self.eps = eps\n\n    def observation(self, obs):\n        """""" Preprocesses a given observation. """"""\n        if not self.obs_rmv:\n            return obs\n        rmv_batch = (np.expand_dims(obs, 0)\n                     if not hasattr(self.env.unwrapped, ""nenvs"")\n                     else obs)\n        self.obs_rmv.update(rmv_batch)\n        obs = (obs - self.obs_rmv.mean) / np.sqrt(self.obs_rmv.var + self.eps)\n        obs = np.clip(obs, -self.clipob, self.clipob)\n        return obs\n\n    def step(self, action):\n        obs, rews, resets, info = self.env.step(action)\n        self.ret = self.ret * self.gamma + rews\n        obs = self.observation(obs)\n        if self.ret_rmv:\n            self.ret_rmv.update(self.ret)\n            rews = np.clip(rews / np.sqrt(self.ret_rmv.var + self.eps),\n                           -self.cliprew, self.cliprew)\n        self.ret[resets] = 0.\n        return obs, rews, resets, info\n\n    def reset(self, **kwargs):\n        self.ret = np.zeros(getattr(self.env.unwrapped, ""nenvs"", 1))\n        obs = self.env.reset(**kwargs)\n        return self.observation(obs)\n'"
week09_policy_II/runners.py,0,"b'"""""" RL env runner """"""\nfrom collections import defaultdict\n\nimport numpy as np\n\n\nclass EnvRunner:\n    """""" Reinforcement learning runner in an environment with given policy """"""\n\n    def __init__(self, env, policy, nsteps, transforms=None, step_var=None):\n        self.env = env\n        self.policy = policy\n        self.nsteps = nsteps\n        self.transforms = transforms or []\n        self.step_var = step_var if step_var is not None else 0\n        self.state = {""latest_observation"": self.env.reset()}\n\n    @property\n    def nenvs(self):\n        """""" Returns number of batched envs or `None` if env is not batched """"""\n        return getattr(self.env.unwrapped, ""nenvs"", None)\n\n    def reset(self):\n        """""" Resets env and runner states. """"""\n        self.state[""latest_observation""] = self.env.reset()\n        self.policy.reset()\n\n    def get_next(self):\n        """""" Runs the agent in the environment.  """"""\n        trajectory = defaultdict(list, {""actions"": []})\n        observations = []\n        rewards = []\n        resets = []\n        self.state[""env_steps""] = self.nsteps\n\n        for i in range(self.nsteps):\n            observations.append(self.state[""latest_observation""])\n            act = self.policy.act(self.state[""latest_observation""])\n            if ""actions"" not in act:\n                raise ValueError(""result of policy.act must contain \'actions\' ""\n                                 f""but has keys {list(act.keys())}"")\n            for key, val in act.items():\n                trajectory[key].append(val)\n\n            obs, rew, done, _ = self.env.step(trajectory[""actions""][-1])\n            self.state[""latest_observation""] = obs\n            rewards.append(rew)\n            resets.append(done)\n            self.step_var += self.nenvs or 1\n\n            # Only reset if the env is not batched. Batched envs should\n            # auto-reset.\n            if not self.nenvs and np.all(done):\n                self.state[""env_steps""] = i + 1\n                self.state[""latest_observation""] = self.env.reset()\n\n        trajectory.update(\n            observations=observations,\n            rewards=rewards,\n            resets=resets)\n        trajectory[""state""] = self.state\n\n        for transform in self.transforms:\n            transform(trajectory)\n        return trajectory\n'"
week03_model_free/crawler_and_pacman/seminar_py2/analysis.py,0,"b""# analysis.py\n# -----------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\n######################\n# ANALYSIS QUESTIONS #\n######################\n\n# Change these default values to obtain the specified policies through\n# value iteration.\n\ndef question2a():\n  answerDiscount = 0.9\n  answerNoise = 0.2\n  answerLivingReward = 0.0\n  return answerDiscount, answerNoise, answerLivingReward\n  # If not possible, return 'NOT POSSIBLE'\n\ndef question2b():\n  answerDiscount = 0.9\n  answerNoise = 0.2\n  answerLivingReward = 0.0\n  return answerDiscount, answerNoise, answerLivingReward\n  # If not possible, return 'NOT POSSIBLE'\n\ndef question2c():\n  answerDiscount = 0.9\n  answerNoise = 0.2\n  answerLivingReward = 0.0\n  return answerDiscount, answerNoise, answerLivingReward\n  # If not possible, return 'NOT POSSIBLE'\n\ndef question2d():\n  answerDiscount = 0.9\n  answerNoise = 0.2\n  answerLivingReward = 0.0\n  return answerDiscount, answerNoise, answerLivingReward\n  # If not possible, return 'NOT POSSIBLE'\n\ndef question2e():\n  answerDiscount = 0.9\n  answerNoise = 0.2\n  answerLivingReward = 0.0\n  return answerDiscount, answerNoise, answerLivingReward\n  # If not possible, return 'NOT POSSIBLE'\n\nif __name__ == '__main__':\n  print 'Answers to analysis questions:'\n  import analysis\n  for q in [q for q in dir(analysis) if q.startswith('question')]:\n    response = getattr(analysis, q)()\n    print '  Question %s:\\t%s' % (q, str(response))\n"""
week03_model_free/crawler_and_pacman/seminar_py2/crawler.py,0,"b'# crawler.py\n# ----------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\n#!/usr/bin/python\nimport math\nfrom math import pi as PI\nimport time\nimport environment\nimport random\n\nclass CrawlingRobotEnvironment(environment.Environment):\n\n   def __init__(self, crawlingRobot):\n\n       self.crawlingRobot = crawlingRobot\n\n       # The state is of the form (armAngle, handAngle)\n       # where the angles are bucket numbers, not actual\n       # degree measurements\n       self.state = None\n\n       self.nArmStates = 9\n       self.nHandStates = 13\n\n       # create a list of arm buckets and hand buckets to\n       # discretize the state space\n       minArmAngle,maxArmAngle = self.crawlingRobot.getMinAndMaxArmAngles()\n       minHandAngle,maxHandAngle = self.crawlingRobot.getMinAndMaxHandAngles()\n       armIncrement = (maxArmAngle - minArmAngle) / (self.nArmStates-1)\n       handIncrement = (maxHandAngle - minHandAngle) / (self.nHandStates-1)\n       self.armBuckets = [minArmAngle+(armIncrement*i) \\\n          for i in range(self.nArmStates)]\n       self.handBuckets = [minHandAngle+(handIncrement*i) \\\n        for i in range(self.nHandStates)]\n\n       # Reset\n       self.reset()\n\n   def getCurrentState(self):\n       """"""\n         Return the current state\n         of the crawling robot\n       """"""\n       return self.state\n\n   def getPossibleActions(self, state):\n       """"""\n         Returns possible actions\n         for the states in the\n         current state\n       """"""\n\n       actions = list()\n\n       currArmBucket,currHandBucket = state\n       if currArmBucket > 0: actions.append(\'arm-down\')\n       if currArmBucket < self.nArmStates-1: actions.append(\'arm-up\')\n       if currHandBucket > 0: actions.append(\'hand-down\')\n       if currHandBucket < self.nHandStates-1: actions.append(\'hand-up\')\n\n       return actions\n\n   def doAction(self, action):\n       """"""\n         Perform the action and update\n         the current state of the Environment\n         and return the reward for the\n         current state, the next state\n         and the taken action.\n\n         Returns:\n           nextState, reward\n       """"""\n       nextState, reward =  None, None\n\n       oldX,oldY = self.crawlingRobot.getRobotPosition()\n\n       armBucket,handBucket = self.state\n       armAngle,handAngle = self.crawlingRobot.getAngles()\n       if action == \'arm-up\':\n         newArmAngle = self.armBuckets[armBucket+1]\n         self.crawlingRobot.moveArm(newArmAngle)\n         nextState = (armBucket+1,handBucket)\n       if action == \'arm-down\':\n         newArmAngle = self.armBuckets[armBucket-1]\n         self.crawlingRobot.moveArm(newArmAngle)\n         nextState = (armBucket-1,handBucket)\n       if action == \'hand-up\':\n         newHandAngle = self.handBuckets[handBucket+1]\n         self.crawlingRobot.moveHand(newHandAngle)\n         nextState = (armBucket,handBucket+1)\n       if action == \'hand-down\':\n         newHandAngle = self.handBuckets[handBucket-1]\n         self.crawlingRobot.moveHand(newHandAngle)\n         nextState = (armBucket,handBucket-1)\n         \n       newX,newY = self.crawlingRobot.getRobotPosition()\n       \n       # a simple reward function\n       reward = newX - oldX\n       \n       self.state = nextState\n       return nextState, reward\n\n\n   def reset(self):\n       """"""\n        Resets the Environment to the initial state\n       """"""\n       ## Initialize the state to be the middle\n       ## value for each parameter e.g. if there are 13 and 19\n       ## buckets for the arm and hand parameters, then the intial\n       ## state should be (6,9)\n       ##\n       ## Also call self.crawlingRobot.setAngles()\n       ## to the initial arm and hand angle\n\n       armState = self.nArmStates/2\n       handState = self.nHandStates/2\n       self.state = armState,handState\n       self.crawlingRobot.setAngles(self.armBuckets[armState],self.handBuckets[handState])\n       self.crawlingRobot.positions = [20,self.crawlingRobot.getRobotPosition()[0]]\n\n\nclass CrawlingRobot:\n    \n    def setAngles(self, armAngle, handAngle):\n        """"""\n            set the robot\'s arm and hand angles\n            to the passed in values\n        """"""\n        self.armAngle = armAngle\n        self.handAngle = handAngle\n        \n    def getAngles(self):\n        """"""\n            returns the pair of (armAngle, handAngle)\n        """"""\n        return self.armAngle, self.handAngle\n            \n    def getRobotPosition(self):\n        """"""\n            returns the (x,y) coordinates\n            of the lower-left point of the\n            robot\n        """"""\n        return self.robotPos\n    \n    def moveArm(self, newArmAngle):\n        """"""\n            move the robot arm to \'newArmAngle\'\n        """"""\n        oldArmAngle = self.armAngle\n        if newArmAngle > self.maxArmAngle:\n            raise \'Crawling Robot: Arm Raised too high. Careful!\'\n        if newArmAngle < self.minArmAngle:\n            raise \'Crawling Robot: Arm Raised too low. Careful!\'\n        disp = self.displacement(self.armAngle, self.handAngle, \n                                  newArmAngle, self.handAngle)\n        curXPos = self.robotPos[0]\n        self.robotPos = (curXPos+disp, self.robotPos[1])\n        self.armAngle = newArmAngle\n        \n        # Position and Velocity Sign Post\n        self.positions.append(self.getRobotPosition()[0])\n#        self.angleSums.append(abs(math.degrees(oldArmAngle)-math.degrees(newArmAngle)))\n        if len(self.positions) > 100:\n            self.positions.pop(0)\n #           self.angleSums.pop(0)\n        \n    def moveHand(self, newHandAngle):\n        """"""\n            move the robot hand to \'newArmAngle\' \n        """"""\n        oldHandAngle = self.handAngle\n        \n        if newHandAngle > self.maxHandAngle:\n            raise \'Crawling Robot: Hand Raised too high. Careful!\'\n        if newHandAngle < self.minHandAngle:\n            raise \'Crawling Robot: Hand Raised too low. Careful!\'\n        disp = self.displacement(self.armAngle, self.handAngle, self.armAngle, newHandAngle)\n        curXPos = self.robotPos[0]\n        self.robotPos = (curXPos+disp, self.robotPos[1])\n        self.handAngle = newHandAngle\n        \n        # Position and Velocity Sign Post\n        self.positions.append(self.getRobotPosition()[0])\n #       self.angleSums.append(abs(math.degrees(oldHandAngle)-math.degrees(newHandAngle)))\n        if len(self.positions) > 100:\n            self.positions.pop(0)\n #           self.angleSums.pop(0)\n\n    def getMinAndMaxArmAngles(self):\n        """"""\n            get the lower- and upper- bound\n            for the arm angles returns (min,max) pair\n        """"""\n        return self.minArmAngle, self.maxArmAngle\n\n    def getMinAndMaxHandAngles(self):\n        """"""\n            get the lower- and upper- bound\n            for the hand angles returns (min,max) pair\n        """"""        \n        return self.minHandAngle, self.maxHandAngle\n    \n    def getRotationAngle(self):\n        """"""\n            get the current angle the \n            robot body is rotated off the ground\n        """"""\n        armCos, armSin = self.__getCosAndSin(self.armAngle)\n        handCos, handSin = self.__getCosAndSin(self.handAngle)\n        x = self.armLength * armCos + self.handLength * handCos + self.robotWidth\n        y = self.armLength * armSin + self.handLength * handSin + self.robotHeight\n        if y < 0:\n            return math.atan(-y/x)\n        return 0.0\n\n\n    ## You shouldn\'t need methods below here\n\n        \n    def __getCosAndSin(self, angle):\n        return math.cos(angle), math.sin(angle)\n                 \n    def displacement(self, oldArmDegree, oldHandDegree, armDegree, handDegree):\n\n        oldArmCos, oldArmSin = self.__getCosAndSin(oldArmDegree)\n        armCos, armSin = self.__getCosAndSin(armDegree)\n        oldHandCos, oldHandSin = self.__getCosAndSin(oldHandDegree)\n        handCos, handSin = self.__getCosAndSin(handDegree)\n\n        xOld = self.armLength * oldArmCos + self.handLength * oldHandCos + self.robotWidth\n        yOld = self.armLength * oldArmSin + self.handLength * oldHandSin + self.robotHeight\n\n        x = self.armLength * armCos + self.handLength * handCos + self.robotWidth\n        y = self.armLength * armSin + self.handLength * handSin + self.robotHeight\n\n        if y < 0:\n            if yOld <= 0:\n                return math.sqrt(xOld*xOld + yOld*yOld) - math.sqrt(x*x + y*y)\n            return (xOld - yOld*(x-xOld) / (y - yOld)) - math.sqrt(x*x + y*y)\n        else:\n            if yOld  >= 0:\n                return 0.0\n            return -(x - y * (xOld-x)/(yOld-y)) + math.sqrt(xOld*xOld + yOld*yOld)\n\n        raise \'Never Should See This!\'\n    \n    def draw(self, stepCount, stepDelay):\n        x1, y1 = self.getRobotPosition()\n        x1 = x1 % self.totWidth\n\n        ## Check Lower Still on the ground\n        if y1 != self.groundY:\n            raise \'Flying Robot!!\'\n\n        rotationAngle = self.getRotationAngle()\n        cosRot, sinRot = self.__getCosAndSin(rotationAngle)\n\n        x2 = x1 + self.robotWidth * cosRot\n        y2 = y1 - self.robotWidth * sinRot\n\n        x3 = x1 - self.robotHeight * sinRot\n        y3 = y1 - self.robotHeight * cosRot\n\n        x4 = x3 + cosRot*self.robotWidth\n        y4 = y3 - sinRot*self.robotWidth\n        \n        self.canvas.coords(self.robotBody,x1,y1,x2,y2,x4,y4,x3,y3)\n\n        armCos, armSin = self.__getCosAndSin(rotationAngle+self.armAngle)\n        xArm = x4 + self.armLength * armCos\n        yArm = y4 - self.armLength * armSin\n\n        self.canvas.coords(self.robotArm,x4,y4,xArm,yArm)\n\n        handCos, handSin = self.__getCosAndSin(self.handAngle+rotationAngle)\n        xHand = xArm + self.handLength * handCos\n        yHand = yArm - self.handLength * handSin\n\n        self.canvas.coords(self.robotHand,xArm,yArm,xHand,yHand)\n    \n\n        # Position and Velocity Sign Post\n#        time = len(self.positions) + 0.5 * sum(self.angleSums)\n#        velocity = (self.positions[-1]-self.positions[0]) / time\n#        if len(self.positions) == 1: return\n        steps = (stepCount - self.lastStep)\n        if steps==0:return\n #       pos = self.positions[-1]\n#        velocity = (pos - self.lastPos) / steps \n  #      g = .9 ** (10 * stepDelay)\n#        g = .99 ** steps\n#        self.velAvg = g * self.velAvg + (1 - g) * velocity \n #       g = .999 ** steps\n #       self.velAvg2 = g * self.velAvg2 + (1 - g) * velocity \n        pos = self.positions[-1]\n        velocity = pos - self.positions[-2]\n        vel2 = (pos - self.positions[0]) / len(self.positions)\n        self.velAvg = .9 * self.velAvg + .1 * vel2\n        velMsg = \'100-step Avg Velocity: %.2f\' % self.velAvg   \n#        velMsg2 = \'1000-step Avg Velocity: %.2f\' % self.velAvg2   \n        velocityMsg = \'Velocity: %.2f\' % velocity   \n        positionMsg = \'Position: %2.f\' % pos\n        stepMsg = \'Step: %d\' % stepCount\n        if \'vel_msg\' in dir(self):\n            self.canvas.delete(self.vel_msg)\n            self.canvas.delete(self.pos_msg)\n            self.canvas.delete(self.step_msg)\n            self.canvas.delete(self.velavg_msg)\n #           self.canvas.delete(self.velavg2_msg)\n #       self.velavg2_msg = self.canvas.create_text(850,190,text=velMsg2) \n        self.velavg_msg = self.canvas.create_text(650,190,text=velMsg) \n        self.vel_msg = self.canvas.create_text(450,190,text=velocityMsg) \n        self.pos_msg = self.canvas.create_text(250,190,text=positionMsg)\n        self.step_msg = self.canvas.create_text(50,190,text=stepMsg)\n#        self.lastPos = pos\n        self.lastStep = stepCount\n#        self.lastVel = velocity\n    \n    def __init__(self, canvas):\n\n        ## Canvas ##\n        self.canvas = canvas\n        self.velAvg = 0\n#        self.velAvg2 = 0\n#        self.lastPos = 0\n        self.lastStep = 0\n#        self.lastVel = 0\n\n        ## Arm and Hand Degrees ##\n        self.armAngle = self.oldArmDegree = 0.0\n        self.handAngle = self.oldHandDegree = -PI/6\n\n        self.maxArmAngle = PI/6\n        self.minArmAngle = -PI/6\n\n        self.maxHandAngle = 0\n        self.minHandAngle = -(5.0/6.0) * PI\n\n        ## Draw Ground ##\n        self.totWidth = canvas.winfo_reqwidth()\n        self.totHeight = canvas.winfo_reqheight()\n        self.groundHeight = 40\n        self.groundY = self.totHeight - self.groundHeight\n        \n        self.ground = canvas.create_rectangle(0,\n            self.groundY,self.totWidth,self.totHeight, fill=\'blue\')\n\n        ## Robot Body ##\n        self.robotWidth = 80\n        self.robotHeight = 40\n        self.robotPos = (20, self.groundY)\n        self.robotBody = canvas.create_polygon(0,0,0,0,0,0,0,0, fill=\'green\')\n\n        ## Robot Arm ##\n        self.armLength = 60\n        self.robotArm = canvas.create_line(0,0,0,0,fill=\'orange\',width=5)\n\n        ## Robot Hand ##\n        self.handLength = 40\n        self.robotHand = canvas.create_line(0,0,0,0,fill=\'red\',width=3)\n\n        self.positions = [0,0]\n  #      self.angleSums = [0,0]\n \n\n\nif __name__ == \'__main__\':      \n  from graphicsCrawlerDisplay import *\n  run()\n\n '"
week03_model_free/crawler_and_pacman/seminar_py2/environment.py,0,"b'# environment.py\n# --------------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\n#!/usr/bin/python\n\nclass Environment:\n        \n  def getCurrentState(self):\n    """"""\n    Returns the current state of enviornment\n    """"""\n    abstract\n    \n  def getPossibleActions(self, state):\n    """"""\n      Returns possible actions the agent \n      can take in the given state. Can\n      return the empty list if we are in \n      a terminal state.\n    """"""\n    abstract\n                \n  def doAction(self, action):\n    """"""\n      Performs the given action in the current\n      environment state and updates the enviornment.\n    \n      Returns a (reward, nextState) pair\n    """"""\n    abstract\n        \n  def reset(self):\n    """"""\n      Resets the current state to the start state\n    """"""\n    abstract\n\n  def isTerminal(self):\n    """"""\n      Has the enviornment entered a terminal\n      state? This means there are no successors\n    """"""\n    state = self.getCurrentState()\n    actions = self.getPossibleActions(state)\n    return len(actions) == 0\n    '"
week03_model_free/crawler_and_pacman/seminar_py2/featureExtractors.py,0,"b'# featureExtractors.py\n# --------------------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\n""Feature extractors for Pacman game states""\n\nfrom game import Directions, Actions\nimport util\n\nclass FeatureExtractor:  \n  def getFeatures(self, state, action):    \n    """"""\n      Returns a dict from features to counts\n      Usually, the count will just be 1.0 for\n      indicator functions.  \n    """"""\n    util.raiseNotDefined()\n\nclass IdentityExtractor(FeatureExtractor):\n  def getFeatures(self, state, action):\n    feats = util.Counter()\n    feats[(state,action)] = 1.0\n    return feats\n\ndef closestFood(pos, food, walls):\n  """"""\n  closestFood -- this is similar to the function that we have\n  worked on in the search project; here its all in one place\n  """"""\n  fringe = [(pos[0], pos[1], 0)]\n  expanded = set()\n  while fringe:\n    pos_x, pos_y, dist = fringe.pop(0)\n    if (pos_x, pos_y) in expanded:\n      continue\n    expanded.add((pos_x, pos_y))\n    # if we find a food at this location then exit\n    if food[pos_x][pos_y]:\n      return dist\n    # otherwise spread out from the location to its neighbours\n    nbrs = Actions.getLegalNeighbors((pos_x, pos_y), walls)\n    for nbr_x, nbr_y in nbrs:\n      fringe.append((nbr_x, nbr_y, dist+1))\n  # no food found\n  return None\n\nclass SimpleExtractor(FeatureExtractor):\n  """"""\n  Returns simple features for a basic reflex Pacman:\n  - whether food will be eaten\n  - how far away the next food is\n  - whether a ghost collision is imminent\n  - whether a ghost is one step away\n  """"""\n  \n  def getFeatures(self, state, action):\n    # extract the grid of food and wall locations and get the ghost locations\n    food = state.getFood()\n    walls = state.getWalls()\n    ghosts = state.getGhostPositions()\n\n    features = util.Counter()\n    \n    features[""bias""] = 1.0\n    \n    # compute the location of pacman after he takes the action\n    x, y = state.getPacmanPosition()\n    dx, dy = Actions.directionToVector(action)\n    next_x, next_y = int(x + dx), int(y + dy)\n    \n    # count the number of ghosts 1-step away\n    features[""#-of-ghosts-1-step-away""] = sum((next_x, next_y) in Actions.getLegalNeighbors(g, walls) for g in ghosts)\n\n    # if there is no danger of ghosts then add the food feature\n    if not features[""#-of-ghosts-1-step-away""] and food[next_x][next_y]:\n      features[""eats-food""] = 1.0\n    \n    dist = closestFood((next_x, next_y), food, walls)\n    if dist is not None:\n      # make the distance a number less than one otherwise the update\n      # will diverge wildly\n      features[""closest-food""] = float(dist) / (walls.width * walls.height) \n    features.divideAll(10.0)\n    return features'"
week03_model_free/crawler_and_pacman/seminar_py2/game.py,0,"b'# game.py\n# -------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nfrom util import *\nfrom util import raiseNotDefined\nimport time, os\nimport traceback\n\ntry:\n  import boinc\n  _BOINC_ENABLED = True\nexcept:\n  _BOINC_ENABLED = False\n\n#######################\n# Parts worth reading #\n#######################\n\nclass Agent:\n  """"""\n  An agent must define a getAction method, but may also define the\n  following methods which will be called if they exist:\n\n  def registerInitialState(self, state): # inspects the starting state\n  """"""\n  def __init__(self, index=0):\n    self.index = index\n\n  def getAction(self, state):\n    """"""\n    The Agent will receive a GameState (from either {pacman, capture, sonar}.py) and\n    must return an action from Directions.{North, South, East, West, Stop}\n    """"""\n    raiseNotDefined()\n\nclass Directions:\n  NORTH = \'North\'\n  SOUTH = \'South\'\n  EAST = \'East\'\n  WEST = \'West\'\n  STOP = \'Stop\'\n\n  LEFT =       {NORTH: WEST,\n                 SOUTH: EAST,\n                 EAST:  NORTH,\n                 WEST:  SOUTH,\n                 STOP:  STOP}\n\n  RIGHT =      dict([(y,x) for x, y in LEFT.items()])\n\n  REVERSE = {NORTH: SOUTH,\n             SOUTH: NORTH,\n             EAST: WEST,\n             WEST: EAST,\n             STOP: STOP}\n\nclass Configuration:\n  """"""\n  A Configuration holds the (x,y) coordinate of a character, along with its\n  traveling direction.\n\n  The convention for positions, like a graph, is that (0,0) is the lower left corner, x increases\n  horizontally and y increases vertically.  Therefore, north is the direction of increasing y, or (0,1).\n  """"""\n\n  def __init__(self, pos, direction):\n    self.pos = pos\n    self.direction = direction\n\n  def getPosition(self):\n    return (self.pos)\n\n  def getDirection(self):\n    return self.direction\n\n  def isInteger(self):\n    x,y = self.pos\n    return x == int(x) and y == int(y)\n\n  def __eq__(self, other):\n    if other == None: return False\n    return (self.pos == other.pos and self.direction == other.direction)\n\n  def __hash__(self):\n    x = hash(self.pos)\n    y = hash(self.direction)\n    return hash(x + 13 * y)\n\n  def __str__(self):\n    return ""(x,y)=""+str(self.pos)+"", ""+str(self.direction)\n\n  def generateSuccessor(self, vector):\n    """"""\n    Generates a new configuration reached by translating the current\n    configuration by the action vector.  This is a low-level call and does\n    not attempt to respect the legality of the movement.\n\n    Actions are movement vectors.\n    """"""\n    x, y= self.pos\n    dx, dy = vector\n    direction = Actions.vectorToDirection(vector)\n    if direction == Directions.STOP:\n      direction = self.direction # There is no stop direction\n    return Configuration((x + dx, y+dy), direction)\n\nclass AgentState:\n  """"""\n  AgentStates hold the state of an agent (configuration, speed, scared, etc).\n  """"""\n\n  def __init__( self, startConfiguration, isPacman ):\n    self.start = startConfiguration\n    self.configuration = startConfiguration\n    self.isPacman = isPacman\n    self.scaredTimer = 0\n\n  def __str__( self ):\n    if self.isPacman:\n      return ""Pacman: "" + str( self.configuration )\n    else:\n      return ""Ghost: "" + str( self.configuration )\n\n  def __eq__( self, other ):\n    if other == None:\n      return False\n    return self.configuration == other.configuration and self.scaredTimer == other.scaredTimer\n\n  def __hash__(self):\n    return hash(hash(self.configuration) + 13 * hash(self.scaredTimer))\n\n  def copy( self ):\n    state = AgentState( self.start, self.isPacman )\n    state.configuration = self.configuration\n    state.scaredTimer = self.scaredTimer\n    return state\n\n  def getPosition(self):\n    if self.configuration == None: return None\n    return self.configuration.getPosition()\n\n  def getDirection(self):\n    return self.configuration.getDirection()\n\nclass Grid:\n  """"""\n  A 2-dimensional array of objects backed by a list of lists.  Data is accessed\n  via grid[x][y] where (x,y) are positions on a Pacman map with x horizontal,\n  y vertical and the origin (0,0) in the bottom left corner.\n\n  The __str__ method constructs an output that is oriented like a pacman board.\n  """"""\n  def __init__(self, width, height, initialValue=False, bitRepresentation=None):\n    if initialValue not in [False, True]: raise Exception(\'Grids can only contain booleans\')\n    self.CELLS_PER_INT = 30\n\n    self.width = width\n    self.height = height\n    self.data = [[initialValue for y in range(height)] for x in range(width)]\n    if bitRepresentation:\n      self._unpackBits(bitRepresentation)\n\n  def __getitem__(self, i):\n    return self.data[i]\n\n  def __setitem__(self, key, item):\n    self.data[key] = item\n\n  def __str__(self):\n    out = [[str(self.data[x][y])[0] for x in range(self.width)] for y in range(self.height)]\n    out.reverse()\n    return \'\\n\'.join([\'\'.join(x) for x in out])\n\n  def __eq__(self, other):\n    if other == None: return False\n    return self.data == other.data\n\n  def __hash__(self):\n    # return hash(str(self))\n    base = 1\n    h = 0\n    for l in self.data:\n      for i in l:\n        if i:\n          h += base\n        base *= 2\n    return hash(h)\n\n  def copy(self):\n    g = Grid(self.width, self.height)\n    g.data = [x[:] for x in self.data]\n    return g\n\n  def deepCopy(self):\n    return self.copy()\n\n  def shallowCopy(self):\n    g = Grid(self.width, self.height)\n    g.data = self.data\n    return g\n\n  def count(self, item =True ):\n    return sum([x.count(item) for x in self.data])\n\n  def asList(self, key = True):\n    list = []\n    for x in range(self.width):\n      for y in range(self.height):\n        if self[x][y] == key: list.append( (x,y) )\n    return list\n\n  def packBits(self):\n    """"""\n    Returns an efficient int list representation\n\n    (width, height, bitPackedInts...)\n    """"""\n    bits = [self.width, self.height]\n    currentInt = 0\n    for i in range(self.height * self.width):\n      bit = self.CELLS_PER_INT - (i % self.CELLS_PER_INT) - 1\n      x, y = self._cellIndexToPosition(i)\n      if self[x][y]:\n        currentInt += 2 ** bit\n      if (i + 1) % self.CELLS_PER_INT == 0:\n        bits.append(currentInt)\n        currentInt = 0\n    bits.append(currentInt)\n    return tuple(bits)\n\n  def _cellIndexToPosition(self, index):\n    x = index / self.height\n    y = index % self.height\n    return x, y\n\n  def _unpackBits(self, bits):\n    """"""\n    Fills in data from a bit-level representation\n    """"""\n    cell = 0\n    for packed in bits:\n      for bit in self._unpackInt(packed, self.CELLS_PER_INT):\n        if cell == self.width * self.height: break\n        x, y = self._cellIndexToPosition(cell)\n        self[x][y] = bit\n        cell += 1\n\n  def _unpackInt(self, packed, size):\n    bools = []\n    if packed < 0: raise ValueError, ""must be a positive integer""\n    for i in range(size):\n      n = 2 ** (self.CELLS_PER_INT - i - 1)\n      if packed >= n:\n        bools.append(True)\n        packed -= n\n      else:\n        bools.append(False)\n    return bools\n\ndef reconstituteGrid(bitRep):\n  if type(bitRep) is not type((1,2)):\n    return bitRep\n  width, height = bitRep[:2]\n  return Grid(width, height, bitRepresentation= bitRep[2:])\n\n####################################\n# Parts you shouldn\'t have to read #\n####################################\n\nclass Actions:\n  """"""\n  A collection of static methods for manipulating move actions.\n  """"""\n  # Directions\n  _directions = {Directions.NORTH: (0, 1),\n                 Directions.SOUTH: (0, -1),\n                 Directions.EAST:  (1, 0),\n                 Directions.WEST:  (-1, 0),\n                 Directions.STOP:  (0, 0)}\n\n  _directionsAsList = _directions.items()\n\n  TOLERANCE = .001\n\n  def reverseDirection(action):\n    if action == Directions.NORTH:\n      return Directions.SOUTH\n    if action == Directions.SOUTH:\n      return Directions.NORTH\n    if action == Directions.EAST:\n      return Directions.WEST\n    if action == Directions.WEST:\n      return Directions.EAST\n    return action\n  reverseDirection = staticmethod(reverseDirection)\n\n  def vectorToDirection(vector):\n    dx, dy = vector\n    if dy > 0:\n      return Directions.NORTH\n    if dy < 0:\n      return Directions.SOUTH\n    if dx < 0:\n      return Directions.WEST\n    if dx > 0:\n      return Directions.EAST\n    return Directions.STOP\n  vectorToDirection = staticmethod(vectorToDirection)\n\n  def directionToVector(direction, speed = 1.0):\n    dx, dy =  Actions._directions[direction]\n    return (dx * speed, dy * speed)\n  directionToVector = staticmethod(directionToVector)\n\n  def getPossibleActions(config, walls):\n    possible = []\n    x, y = config.pos\n    x_int, y_int = int(x + 0.5), int(y + 0.5)\n\n    # In between grid points, all agents must continue straight\n    if (abs(x - x_int) + abs(y - y_int)  > Actions.TOLERANCE):\n      return [config.getDirection()]\n\n    for dir, vec in Actions._directionsAsList:\n      dx, dy = vec\n      next_y = y_int + dy\n      next_x = x_int + dx\n      if not walls[next_x][next_y]: possible.append(dir)\n\n    return possible\n\n  getPossibleActions = staticmethod(getPossibleActions)\n\n  def getLegalNeighbors(position, walls):\n    x,y = position\n    x_int, y_int = int(x + 0.5), int(y + 0.5)\n    neighbors = []\n    for dir, vec in Actions._directionsAsList:\n      dx, dy = vec\n      next_x = x_int + dx\n      if next_x < 0 or next_x == walls.width: continue\n      next_y = y_int + dy\n      if next_y < 0 or next_y == walls.height: continue\n      if not walls[next_x][next_y]: neighbors.append((next_x, next_y))\n    return neighbors\n  getLegalNeighbors = staticmethod(getLegalNeighbors)\n\n  def getSuccessor(position, action):\n    dx, dy = Actions.directionToVector(action)\n    x, y = position\n    return (x + dx, y + dy)\n  getSuccessor = staticmethod(getSuccessor)\n\nclass GameStateData:\n  """"""\n\n  """"""\n  def __init__( self, prevState = None ):\n    """"""\n    Generates a new data packet by copying information from its predecessor.\n    """"""\n    if prevState != None:\n      self.food = prevState.food.shallowCopy()\n      self.capsules = prevState.capsules[:]\n      self.agentStates = self.copyAgentStates( prevState.agentStates )\n      self.layout = prevState.layout\n      self._eaten = prevState._eaten\n      self.score = prevState.score\n    self._foodEaten = None\n    self._capsuleEaten = None\n    self._agentMoved = None\n    self._lose = False\n    self._win = False\n    self.scoreChange = 0\n\n  def deepCopy( self ):\n    state = GameStateData( self )\n    state.food = self.food.deepCopy()\n    state.layout = self.layout.deepCopy()\n    state._agentMoved = self._agentMoved\n    state._foodEaten = self._foodEaten\n    state._capsuleEaten = self._capsuleEaten\n    return state\n\n  def copyAgentStates( self, agentStates ):\n    copiedStates = []\n    for agentState in agentStates:\n      copiedStates.append( agentState.copy() )\n    return copiedStates\n\n  def __eq__( self, other ):\n    """"""\n    Allows two states to be compared.\n    """"""\n    if other == None: return False\n    # TODO Check for type of other\n    if not self.agentStates == other.agentStates: return False\n    if not self.food == other.food: return False\n    if not self.capsules == other.capsules: return False\n    if not self.score == other.score: return False\n    return True\n\n  def __hash__( self ):\n    """"""\n    Allows states to be keys of dictionaries.\n    """"""\n    for i, state in enumerate( self.agentStates ):\n      try:\n        int(hash(state))\n      except TypeError, e:\n        print e\n        #hash(state)\n    return int((hash(tuple(self.agentStates)) + 13*hash(self.food) + 113* hash(tuple(self.capsules)) + 7 * hash(self.score)) % 1048575 )\n\n  def __str__( self ):\n    width, height = self.layout.width, self.layout.height\n    map = Grid(width, height)\n    if type(self.food) == type((1,2)):\n      self.food = reconstituteGrid(self.food)\n    for x in range(width):\n      for y in range(height):\n        food, walls = self.food, self.layout.walls\n        map[x][y] = self._foodWallStr(food[x][y], walls[x][y])\n\n    for agentState in self.agentStates:\n      if agentState == None: continue\n      if agentState.configuration == None: continue\n      x,y = [int( i ) for i in nearestPoint( agentState.configuration.pos )]\n      agent_dir = agentState.configuration.direction\n      if agentState.isPacman:\n        map[x][y] = self._pacStr( agent_dir )\n      else:\n        map[x][y] = self._ghostStr( agent_dir )\n\n    for x, y in self.capsules:\n      map[x][y] = \'o\'\n\n    return str(map) + (""\\nScore: %d\\n"" % self.score)\n\n  def _foodWallStr( self, hasFood, hasWall ):\n    if hasFood:\n      return \'.\'\n    elif hasWall:\n      return \'%\'\n    else:\n      return \' \'\n\n  def _pacStr( self, dir ):\n    if dir == Directions.NORTH:\n      return \'v\'\n    if dir == Directions.SOUTH:\n      return \'^\'\n    if dir == Directions.WEST:\n      return \'>\'\n    return \'<\'\n\n  def _ghostStr( self, dir ):\n    return \'G\'\n    if dir == Directions.NORTH:\n      return \'M\'\n    if dir == Directions.SOUTH:\n      return \'W\'\n    if dir == Directions.WEST:\n      return \'3\'\n    return \'E\'\n\n  def initialize( self, layout, numGhostAgents ):\n    """"""\n    Creates an initial game state from a layout array (see layout.py).\n    """"""\n    self.food = layout.food.copy()\n    self.capsules = layout.capsules[:]\n    self.layout = layout\n    self.score = 0\n    self.scoreChange = 0\n\n    self.agentStates = []\n    numGhosts = 0\n    for isPacman, pos in layout.agentPositions:\n      if not isPacman:\n        if numGhosts == numGhostAgents: continue # Max ghosts reached already\n        else: numGhosts += 1\n      self.agentStates.append( AgentState( Configuration( pos, Directions.STOP), isPacman) )\n    self._eaten = [False for a in self.agentStates]\n\nclass Game:\n  """"""\n  The Game manages the control flow, soliciting actions from agents.\n  """"""\n\n  def __init__( self, agents, display, rules, startingIndex=0, muteAgents=False, catchExceptions=False ):\n    self.agentCrashed = False\n    self.agents = agents\n    self.display = display\n    self.rules = rules\n    self.startingIndex = startingIndex\n    self.gameOver = False\n    self.muteAgents = muteAgents\n    self.catchExceptions = catchExceptions\n    self.moveHistory = []\n    self.totalAgentTimes = [0 for agent in agents]\n    self.totalAgentTimeWarnings = [0 for agent in agents]\n    self.agentTimeout = False\n\n  def getProgress(self):\n    if self.gameOver:\n      return 1.0\n    else:\n      return self.rules.getProgress(self)\n\n  def _agentCrash( self, agentIndex, quiet=False):\n    ""Helper method for handling agent crashes""\n    if not quiet: traceback.print_exc()\n    self.gameOver = True\n    self.agentCrashed = True\n    self.rules.agentCrash(self, agentIndex)\n\n  OLD_STDOUT = None\n  OLD_STDERR = None\n\n  def mute(self):\n    if not self.muteAgents: return\n    global OLD_STDOUT, OLD_STDERR\n    import cStringIO\n    OLD_STDOUT = sys.stdout\n    OLD_STDERR = sys.stderr\n    sys.stdout = cStringIO.StringIO()\n    sys.stderr = cStringIO.StringIO()\n\n  def unmute(self):\n    if not self.muteAgents: return\n    global OLD_STDOUT, OLD_STDERR\n    sys.stdout.close()\n    sys.stderr.close()\n    # Revert stdout/stderr to originals\n    sys.stdout = OLD_STDOUT\n    sys.stderr = OLD_STDERR\n\n\n  def run( self ):\n    """"""\n    Main control loop for game play.\n    """"""\n    self.display.initialize(self.state.data)\n    self.numMoves = 0\n\n    ###self.display.initialize(self.state.makeObservation(1).data)\n    # inform learning agents of the game start\n    for i in range(len(self.agents)):\n      agent = self.agents[i]\n      if not agent:\n        # this is a null agent, meaning it failed to load\n        # the other team wins\n        self._agentCrash(i, quiet=True)\n        return\n      if (""registerInitialState"" in dir(agent)):\n        self.mute()\n        if self.catchExceptions:\n          try:\n            timed_func = TimeoutFunction(agent.registerInitialState, int(self.rules.getMaxStartupTime(i)))\n            try:\n              start_time = time.time()\n              timed_func(self.state.deepCopy())\n              time_taken = time.time() - start_time\n              self.totalAgentTimes[i] += time_taken\n            except TimeoutFunctionException:\n              print ""Agent %d ran out of time on startup!"" % i\n              self.unmute()\n              self.agentTimeout = True\n              self._agentCrash(i, quiet=True)\n              return\n          except Exception,data:\n            self.unmute()\n            self._agentCrash(i, quiet=True)\n            return\n        else:\n          agent.registerInitialState(self.state.deepCopy())\n        ## TODO: could this exceed the total time\n        self.unmute()\n\n    agentIndex = self.startingIndex\n    numAgents = len( self.agents )\n\n    while not self.gameOver:\n      # Fetch the next agent\n      agent = self.agents[agentIndex]\n      move_time = 0\n      skip_action = False\n      # Generate an observation of the state\n      if \'observationFunction\' in dir( agent ):\n        self.mute()\n        if self.catchExceptions:\n          try:\n            timed_func = TimeoutFunction(agent.observationFunction, int(self.rules.getMoveTimeout(agentIndex)))\n            try:\n              start_time = time.time()\n              observation = timed_func(self.state.deepCopy())\n            except TimeoutFunctionException:\n              skip_action = True\n            move_time += time.time() - start_time\n            self.unmute()\n          except Exception,data:\n            self.unmute()\n            self._agentCrash(agentIndex, quiet=True)\n            return\n        else:\n          observation = agent.observationFunction(self.state.deepCopy())\n        self.unmute()\n      else:\n        observation = self.state.deepCopy()\n\n      # Solicit an action\n      action = None\n      self.mute()\n      if self.catchExceptions:\n        try:\n          timed_func = TimeoutFunction(agent.getAction, int(self.rules.getMoveTimeout(agentIndex)) - int(move_time))\n          try:\n            start_time = time.time()\n            if skip_action:\n              raise TimeoutFunctionException()\n            action = timed_func( observation )\n          except TimeoutFunctionException:\n            print ""Agent %d timed out on a single move!"" % agentIndex\n            self.agentTimeout = True\n            self.unmute()\n            self._agentCrash(agentIndex, quiet=True)\n            return\n\n          move_time += time.time() - start_time\n\n          if move_time > self.rules.getMoveWarningTime(agentIndex):\n            self.totalAgentTimeWarnings[agentIndex] += 1\n            print ""Agent %d took too long to make a move! This is warning %d"" % (agentIndex, self.totalAgentTimeWarnings[agentIndex])\n            if self.totalAgentTimeWarnings[agentIndex] > self.rules.getMaxTimeWarnings(agentIndex):\n              print ""Agent %d exceeded the maximum number of warnings: %d"" % (agentIndex, self.totalAgentTimeWarnings[agentIndex])\n              self.agentTimeout = True\n              self.unmute()\n              self._agentCrash(agentIndex, quiet=True)\n\n          self.totalAgentTimes[agentIndex] += move_time\n          #print ""Agent: %d, time: %f, total: %f"" % (agentIndex, move_time, self.totalAgentTimes[agentIndex])\n          if self.totalAgentTimes[agentIndex] > self.rules.getMaxTotalTime(agentIndex):\n            print ""Agent %d ran out of time! (time: %1.2f)"" % (agentIndex, self.totalAgentTimes[agentIndex])\n            self.agentTimeout = True\n            self.unmute()\n            self._agentCrash(agentIndex, quiet=True)\n            return\n          self.unmute()\n        except Exception,data:\n          self.unmute()\n          self._agentCrash(agentIndex)\n          return\n      else:\n        action = agent.getAction(observation)\n      self.unmute()\n\n      # Execute the action\n      self.moveHistory.append( (agentIndex, action) )\n      if self.catchExceptions:\n        try:\n          self.state = self.state.generateSuccessor( agentIndex, action )\n        except Exception,data:\n          self._agentCrash(agentIndex)\n          return\n      else:\n        self.state = self.state.generateSuccessor( agentIndex, action )\n\n      # Change the display\n      self.display.update( self.state.data )\n      ###idx = agentIndex - agentIndex % 2 + 1\n      ###self.display.update( self.state.makeObservation(idx).data )\n\n      # Allow for game specific conditions (winning, losing, etc.)\n      self.rules.process(self.state, self)\n      # Track progress\n      if agentIndex == numAgents + 1: self.numMoves += 1\n      # Next agent\n      agentIndex = ( agentIndex + 1 ) % numAgents\n\n      if _BOINC_ENABLED:\n        boinc.set_fraction_done(self.getProgress())\n\n    # inform a learning agent of the game result\n    for agent in self.agents:\n      if ""final"" in dir( agent ) :\n        try:\n          self.mute()\n          agent.final( self.state )\n          self.unmute()\n        except Exception,data:\n          if not self.catchExceptions: raise\n          self.unmute()\n          print ""Exception"",data\n          self._agentCrash(agent.index)\n          return\n    self.display.finish()\n\n\n\n\n'"
week03_model_free/crawler_and_pacman/seminar_py2/ghostAgents.py,0,"b'# ghostAgents.py\n# --------------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nfrom game import Agent\nfrom game import Actions\nfrom game import Directions\nimport random\nfrom util import manhattanDistance\nimport util\n\nclass GhostAgent( Agent ):\n  def __init__( self, index ):\n    self.index = index\n\n  def getAction( self, state ):\n    dist = self.getDistribution(state)\n    if len(dist) == 0: \n      return Directions.STOP\n    else:\n      return util.chooseFromDistribution( dist )\n    \n  def getDistribution(self, state):\n    ""Returns a Counter encoding a distribution over actions from the provided state.""\n    util.raiseNotDefined()\n\nclass RandomGhost( GhostAgent ):\n  ""A ghost that chooses a legal action uniformly at random.""\n  def getDistribution( self, state ):\n    dist = util.Counter()\n    for a in state.getLegalActions( self.index ): dist[a] = 1.0\n    dist.normalize()\n    return dist\n\nclass DirectionalGhost( GhostAgent ):\n  ""A ghost that prefers to rush Pacman, or flee when scared.""\n  def __init__( self, index, prob_attack=0.8, prob_scaredFlee=0.8 ):\n    self.index = index\n    self.prob_attack = prob_attack\n    self.prob_scaredFlee = prob_scaredFlee\n      \n  def getDistribution( self, state ):\n    # Read variables from state\n    ghostState = state.getGhostState( self.index )\n    legalActions = state.getLegalActions( self.index )\n    pos = state.getGhostPosition( self.index )\n    isScared = ghostState.scaredTimer > 0\n    \n    speed = 1\n    if isScared: speed = 0.5\n    \n    actionVectors = [Actions.directionToVector( a, speed ) for a in legalActions]\n    newPositions = [( pos[0]+a[0], pos[1]+a[1] ) for a in actionVectors]\n    pacmanPosition = state.getPacmanPosition()\n\n    # Select best actions given the state\n    distancesToPacman = [manhattanDistance( pos, pacmanPosition ) for pos in newPositions]\n    if isScared:\n      bestScore = max( distancesToPacman )\n      bestProb = self.prob_scaredFlee\n    else:\n      bestScore = min( distancesToPacman )\n      bestProb = self.prob_attack\n    bestActions = [action for action, distance in zip( legalActions, distancesToPacman ) if distance == bestScore]\n    \n    # Construct distribution\n    dist = util.Counter()\n    for a in bestActions: dist[a] = bestProb / len(bestActions)\n    for a in legalActions: dist[a] += ( 1-bestProb ) / len(legalActions)\n    dist.normalize()\n    return dist\n'"
week03_model_free/crawler_and_pacman/seminar_py2/graphicsCrawlerDisplay.py,0,"b'# graphicsCrawlerDisplay.py\n# -------------------------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nimport Tkinter\nimport qlearningAgents\nimport time\nimport threading\nimport sys\nimport crawler\n#import pendulum\nimport math\nfrom math import pi as PI\n\nrobotType = \'crawler\'\n\nclass Application:\n\n    def sigmoid(self, x):\n        return 1.0 / (1.0 + 2.0 ** (-x))\n\n    def incrementSpeed(self, inc):\n        self.tickTime *= inc        \n#        self.epsilon = min(1.0, self.epsilon)\n#        self.epsilon = max(0.0,self.epsilon)    \n#        self.learner.setSpeed(self.epsilon)\n        self.speed_label[\'text\'] = \'Step Delay: %.5f\' % (self.tickTime)\n\n    def incrementEpsilon(self, inc):\n        self.ep += inc     \n        self.epsilon = self.sigmoid(self.ep)   \n        self.learner.setEpsilon(self.epsilon)\n        self.epsilon_label[\'text\'] = \'Epsilon: %.3f\' % (self.epsilon)\n                           \n    def incrementGamma(self, inc):\n        self.ga += inc     \n        self.gamma = self.sigmoid(self.ga)   \n        self.learner.setDiscount(self.gamma)\n        self.gamma_label[\'text\'] = \'Discount: %.3f\' % (self.gamma)\n                \n    def incrementAlpha(self, inc):\n        self.al += inc     \n        self.alpha = self.sigmoid(self.al)   \n        self.learner.setLearningRate(self.alpha)\n        self.alpha_label[\'text\'] = \'Learning Rate: %.3f\' % (self.alpha)\n        \n    def __initGUI(self, win):\n        ## Window ##\n        self.win = win\n        \n        ## Initialize Frame ##    \n        win.grid()\n        self.dec = -.5\n        self.inc = .5\n        self.tickTime = 0.1\n\n        ## Epsilon Button + Label ##\n        self.setupSpeedButtonAndLabel(win)\n        \n        self.setupEpsilonButtonAndLabel(win)\n        \n        ## Gamma Button + Label ##\n        self.setUpGammaButtonAndLabel(win)\n        \n        ## Alpha Button + Label ##\n        self.setupAlphaButtonAndLabel(win)\n        \n        ## Exit Button ##\n        #self.exit_button = Tkinter.Button(win,text=\'Quit\', command=self.exit)\n        #self.exit_button.grid(row=0, column=9)\n        \n        ## Simulation Buttons ##\n#        self.setupSimulationButtons(win)\n        \n         ## Canvas ##\n        self.canvas = Tkinter.Canvas(root, height=200, width=1000)\n        self.canvas.grid(row=2,columnspan=10)\n\n    def setupAlphaButtonAndLabel(self, win):\n        self.alpha_minus = Tkinter.Button(win, \n        text=""-"",command=(lambda: self.incrementAlpha(self.dec)))                \n        self.alpha_minus.grid(row=1, column=3, padx=10)\n        \n        self.alpha = self.sigmoid(self.al)\n        self.alpha_label = Tkinter.Label(win, text=\'Learning Rate: %.3f\' % (self.alpha))\n        self.alpha_label.grid(row=1, column=4)\n        \n        self.alpha_plus = Tkinter.Button(win, \n        text=""+"",command=(lambda: self.incrementAlpha(self.inc)))        \n        self.alpha_plus.grid(row=1, column=5, padx=10)\n\n    def setUpGammaButtonAndLabel(self, win):\n        self.gamma_minus = Tkinter.Button(win, \n        text=""-"",command=(lambda: self.incrementGamma(self.dec)))                \n        self.gamma_minus.grid(row=1, column=0, padx=10)\n        \n        self.gamma = self.sigmoid(self.ga)   \n        self.gamma_label = Tkinter.Label(win, text=\'Discount: %.3f\' % (self.gamma))\n        self.gamma_label.grid(row=1, column=1)\n        \n        self.gamma_plus = Tkinter.Button(win, \n        text=""+"",command=(lambda: self.incrementGamma(self.inc)))        \n        self.gamma_plus.grid(row=1, column=2, padx=10)\n\n    def setupEpsilonButtonAndLabel(self, win):\n        self.epsilon_minus = Tkinter.Button(win, \n        text=""-"",command=(lambda: self.incrementEpsilon(self.dec)))                \n        self.epsilon_minus.grid(row=0, column=3)\n        \n        self.epsilon = self.sigmoid(self.ep)   \n        self.epsilon_label = Tkinter.Label(win, text=\'Epsilon: %.3f\' % (self.epsilon))\n        self.epsilon_label.grid(row=0, column=4)\n        \n        self.epsilon_plus = Tkinter.Button(win, \n        text=""+"",command=(lambda: self.incrementEpsilon(self.inc)))        \n        self.epsilon_plus.grid(row=0, column=5)\n\n    def setupSpeedButtonAndLabel(self, win):\n        self.speed_minus = Tkinter.Button(win, \n        text=""-"",command=(lambda: self.incrementSpeed(.5)))                \n        self.speed_minus.grid(row=0, column=0)\n        \n        self.speed_label = Tkinter.Label(win, text=\'Step Delay: %.5f\' % (self.tickTime))\n        self.speed_label.grid(row=0, column=1)\n        \n        self.speed_plus = Tkinter.Button(win, \n        text=""+"",command=(lambda: self.incrementSpeed(2)))        \n        self.speed_plus.grid(row=0, column=2)\n\n        \n                           \n                \n        \n        \n                                               \n    def skip5kSteps(self):\n        self.stepsToSkip = 5000\n\n    def __init__(self, win):\n    \n        self.ep = 0\n        self.ga = 2\n        self.al = 2\n        self.stepCount = 0\n        ## Init Gui        \n            \n        self.__initGUI(win)\n\n        # Init environment\n        if robotType == \'crawler\':\n            self.robot = crawler.CrawlingRobot(self.canvas)                                        \n            self.robotEnvironment = crawler.CrawlingRobotEnvironment(self.robot)            \n        elif robotType == \'pendulum\':\n            self.robot = pendulum.PendulumRobot(self.canvas)\n            self.robotEnvironment = \\\n                pendulum.PendulumRobotEnvironment(self.robot)\n        else:\n            raise ""Unknown RobotType""\n  \n        # Init Agent      \n        simulationFn = lambda agent: \\\n          simulation.SimulationEnvironment(self.robotEnvironment,agent)        \n        actionFn = lambda state: \\\n          self.robotEnvironment.getPossibleActions(state)\n        self.learner = qlearningAgents.QLearningAgent(actionFn=actionFn)\n        \n        self.learner.setEpsilon(self.epsilon)\n        self.learner.setLearningRate(self.alpha)\n        self.learner.setDiscount(self.gamma)\n        \n        # Start GUI\n        self.running = True\n        self.stopped = False\n        self.stepsToSkip = 0\n        self.thread = threading.Thread(target=self.run)\n        self.thread.start()\n\n\n    def exit(self):\n      self.running = False\n      for i in range(5):\n        if not self.stopped:\n#          print ""Waiting for thread to die...""\n          time.sleep(0.1)\n      self.win.destroy()      \n      sys.exit(0)\n      \n    def step(self):\n        \n        self.stepCount += 1\n        \n        state = self.robotEnvironment.getCurrentState()\n        actions = self.robotEnvironment.getPossibleActions(state)\n        if len(actions) == 0.0:\n            self.robotEnvironment.reset()\n            state = self.robotEnvironment.getCurrentState()\n            actions = self.robotEnvironment.getPossibleActions(state)        \n            print \'Reset!\'\n        action = self.learner.getAction(state)\n        if action == None:\n            raise \'None action returned: Code Not Complete\'\n        nextState, reward = self.robotEnvironment.doAction(action)\n        self.learner.observeTransition(state, action, nextState, reward)\n        \n    def animatePolicy(self):\n        if robotType != \'pendulum\':\n            raise \'Only pendulum can animatePolicy\'\n        \n\n        totWidth = self.canvas.winfo_reqwidth()\n        totHeight = self.canvas.winfo_reqheight()\n        \n        length = 0.48 * min(totWidth, totHeight)\n        x,y = totWidth-length-30, length+10\n    \n        \n    \n        angleMin, angleMax = self.robot.getMinAndMaxAngle()\n        velMin, velMax = self.robot.getMinAndMaxAngleVelocity()\n        \n        if not \'animatePolicyBox\' in dir(self):\n            self.canvas.create_line(x,y,x+length,y)\n            self.canvas.create_line(x+length,y,x+length,y-length)\n            self.canvas.create_line(x+length,y-length,x,y-length)\n            self.canvas.create_line(x,y-length,x,y)\n            self.animatePolicyBox = 1\n            self.canvas.create_text(x+length/2,y+10,text=\'angle\')\n            self.canvas.create_text(x-30,y-length/2,text=\'velocity\')\n            self.canvas.create_text(x-60,y-length/4,text=\'Blue = kickLeft\')\n            self.canvas.create_text(x-60,y-length/4+20,text=\'Red = kickRight\')\n            self.canvas.create_text(x-60,y-length/4+40,text=\'White = doNothing\')\n            \n            \n        \n        angleDelta = (angleMax-angleMin) / 100\n        velDelta = (velMax-velMin) / 100\n        for i in range(100):\n            angle = angleMin + i * angleDelta\n \n            for j in range(100):\n                vel = velMin + j * velDelta\n                state = self.robotEnvironment.getState(angle,vel)\n                max, argMax = None, None\n                if not self.learner.seenState(state):\n                    argMax = \'unseen\'\n                else:\n                     for action in (\'kickLeft\',\'kickRight\',\'doNothing\'):\n                         qVal = self.learner.getQValue(state, action)\n                         if max == None or qVal > max:\n                             max, argMax = qVal, action\n                if argMax != \'unseen\':\n                    if argMax == \'kickLeft\':\n                        color = \'blue\'\n                    elif argMax == \'kickRight\':\n                        color = \'red\'\n                    elif argMax == \'doNothing\':\n                        color = \'white\'\n                    dx = length / 100.0\n                    dy = length / 100.0\n                    x0, y0 = x+i*dx, y-j*dy\n                    self.canvas.create_rectangle(x0,y0,x0+dx,y0+dy,fill=color)\n                   \n                    \n                        \n        \n    def run(self):\n        self.stepCount = 0\n        self.learner.startEpisode()\n        while True:\n          minSleep = .01\n          tm = max(minSleep, self.tickTime)\n          time.sleep(tm)\n          self.stepsToSkip = int(tm / self.tickTime) - 1\n\n          if not self.running:\n            self.stopped = True\n            return\n          for i in range(self.stepsToSkip):\n              self.step()  \n          self.stepsToSkip = 0       \n          self.step()\n#          self.robot.draw()\n        self.learner.stopEpisode()                                                      \n    \n    def start(self):\n        self.win.mainloop()\n\n\n\n\n\ndef run():\n  global root\n  root = Tkinter.Tk()\n  root.title( \'Crawler GUI\' )\n  root.resizable( 0, 0 )\n\n#  root.mainloop()\n\n\n  app = Application(root)\n  def update_gui():\n    app.robot.draw(app.stepCount, app.tickTime)\n    root.after(10, update_gui)\n  update_gui()\n\n  root.protocol( \'WM_DELETE_WINDOW\', app.exit)\n  app.start()\n\n'"
week03_model_free/crawler_and_pacman/seminar_py2/graphicsDisplay.py,0,"b'# graphicsDisplay.py\n# ------------------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nfrom graphicsUtils import *\nimport math, time\nfrom game import Directions\n\n###########################\n#  GRAPHICS DISPLAY CODE  #\n###########################\n\n# Most code by Dan Klein and John Denero written or rewritten for cs188, UC Berkeley.\n# Some code from a Pacman implementation by LiveWires, and used / modified with permission.\n\nDEFAULT_GRID_SIZE = 30.0\nINFO_PANE_HEIGHT = 35\nBACKGROUND_COLOR = formatColor(0,0,0)\nWALL_COLOR = formatColor(0.0/255.0, 51.0/255.0, 255.0/255.0)\nINFO_PANE_COLOR = formatColor(.4,.4,0)\nSCORE_COLOR = formatColor(.9, .9, .9)\nPACMAN_OUTLINE_WIDTH = 2\nPACMAN_CAPTURE_OUTLINE_WIDTH = 4\n\nGHOST_COLORS = []\nGHOST_COLORS.append(formatColor(.9,0,0)) # Red\nGHOST_COLORS.append(formatColor(0,.3,.9)) # Blue\nGHOST_COLORS.append(formatColor(.98,.41,.07)) # Orange\nGHOST_COLORS.append(formatColor(.1,.75,.7)) # Green\nGHOST_COLORS.append(formatColor(1.0,0.6,0.0)) # Yellow\nGHOST_COLORS.append(formatColor(.4,0.13,0.91)) # Purple\n\nTEAM_COLORS = GHOST_COLORS[:2]\n\nGHOST_SHAPE = [\n    ( 0,    0.3 ),\n    ( 0.25, 0.75 ),\n    ( 0.5,  0.3 ),\n    ( 0.75, 0.75 ),\n    ( 0.75, -0.5 ),\n    ( 0.5,  -0.75 ),\n    (-0.5,  -0.75 ),\n    (-0.75, -0.5 ),\n    (-0.75, 0.75 ),\n    (-0.5,  0.3 ),\n    (-0.25, 0.75 )\n  ]\nGHOST_SIZE = 0.65\nSCARED_COLOR = formatColor(1,1,1)\n\nGHOST_VEC_COLORS = map(colorToVector, GHOST_COLORS)\n\nPACMAN_COLOR = formatColor(255.0/255.0,255.0/255.0,61.0/255)\nPACMAN_SCALE = 0.5\n#pacman_speed = 0.25\n\n# Food\nFOOD_COLOR = formatColor(1,1,1)\nFOOD_SIZE = 0.1\n\n# Laser\nLASER_COLOR = formatColor(1,0,0)\nLASER_SIZE = 0.02\n\n# Capsule graphics\nCAPSULE_COLOR = formatColor(1,1,1)\nCAPSULE_SIZE = 0.25\n\n# Drawing walls\nWALL_RADIUS = 0.15\n\nclass InfoPane:\n  def __init__(self, layout, gridSize):\n    self.gridSize = gridSize\n    self.width = (layout.width) * gridSize\n    self.base = (layout.height + 1) * gridSize\n    self.height = INFO_PANE_HEIGHT\n    self.fontSize = 24\n    self.textColor = PACMAN_COLOR\n    self.drawPane()\n\n  def toScreen(self, pos, y = None):\n    """"""\n      Translates a point relative from the bottom left of the info pane.\n    """"""\n    if y == None:\n      x,y = pos\n    else:\n      x = pos\n\n    x = self.gridSize + x # Margin\n    y = self.base + y\n    return x,y\n\n  def drawPane(self):\n    self.scoreText = text( self.toScreen(0, 0  ), self.textColor, ""SCORE:    0"", ""Times"", self.fontSize, ""bold"")\n\n  def initializeGhostDistances(self, distances):\n    self.ghostDistanceText = []\n\n    size = 20\n    if self.width < 240:\n      size = 12\n    if self.width < 160:\n      size = 10\n\n    for i, d in enumerate(distances):\n      t = text( self.toScreen(self.width/2 + self.width/8 * i, 0), GHOST_COLORS[i+1], d, ""Times"", size, ""bold"")\n      self.ghostDistanceText.append(t)\n\n  def updateScore(self, score):\n    changeText(self.scoreText, ""SCORE: % 4d"" % score)\n\n  def setTeam(self, isBlue):\n    text = ""RED TEAM""\n    if isBlue: text = ""BLUE TEAM""\n    self.teamText = text( self.toScreen(300, 0  ), self.textColor, text, ""Times"", self.fontSize, ""bold"")\n\n  def updateGhostDistances(self, distances):\n    if len(distances) == 0: return\n    if \'ghostDistanceText\' not in dir(self): self.initializeGhostDistances(distances)\n    else:\n      for i, d in enumerate(distances):\n        changeText(self.ghostDistanceText[i], d)\n\n  def drawGhost(self):\n    pass\n\n  def drawPacman(self):\n    pass\n\n  def drawWarning(self):\n    pass\n\n  def clearIcon(self):\n    pass\n\n  def updateMessage(self, message):\n    pass\n\n  def clearMessage(self):\n    pass\n\n\nclass PacmanGraphics:\n  def __init__(self, zoom=1.0, frameTime=0.0, capture=False):\n    self.have_window = 0\n    self.currentGhostImages = {}\n    self.pacmanImage = None\n    self.zoom = zoom\n    self.gridSize = DEFAULT_GRID_SIZE * zoom\n    self.capture = capture\n    self.frameTime = frameTime\n\n  def initialize(self, state, isBlue = False):\n    self.isBlue = isBlue\n    self.startGraphics(state)\n\n    # self.drawDistributions(state)\n    self.distributionImages = None  # Initialized lazily\n    self.drawStaticObjects(state)\n    self.drawAgentObjects(state)\n\n    # Information\n    self.previousState = state\n\n  def startGraphics(self, state):\n    self.layout = state.layout\n    layout = self.layout\n    self.width = layout.width\n    self.height = layout.height\n    self.make_window(self.width, self.height)\n    self.infoPane = InfoPane(layout, self.gridSize)\n    self.currentState = layout\n\n  def drawDistributions(self, state):\n    walls = state.layout.walls\n    dist = []\n    for x in range(walls.width):\n      distx = []\n      dist.append(distx)\n      for y in range(walls.height):\n          ( screen_x, screen_y ) = self.to_screen( (x, y) )\n          block = square( (screen_x, screen_y),\n                          0.5 * self.gridSize,\n                          color = BACKGROUND_COLOR,\n                          filled = 1, behind=2)\n          distx.append(block)\n    self.distributionImages = dist\n\n  def drawStaticObjects(self, state):\n    layout = self.layout\n    self.drawWalls(layout.walls)\n    self.food = self.drawFood(layout.food)\n    self.capsules = self.drawCapsules(layout.capsules)\n    refresh()\n\n  def drawAgentObjects(self, state):\n    self.agentImages = [] # (agentState, image)\n    for index, agent in enumerate(state.agentStates):\n      if agent.isPacman:\n        image = self.drawPacman(agent, index)\n        self.agentImages.append( (agent, image) )\n      else:\n        image = self.drawGhost(agent, index)\n        self.agentImages.append( (agent, image) )\n    refresh()\n\n  def swapImages(self, agentIndex, newState):\n    """"""\n      Changes an image from a ghost to a pacman or vis versa (for capture)\n    """"""\n    prevState, prevImage = self.agentImages[agentIndex]\n    for item in prevImage: remove_from_screen(item)\n    if newState.isPacman:\n      image = self.drawPacman(newState, agentIndex)\n      self.agentImages[agentIndex] = (newState, image )\n    else:\n      image = self.drawGhost(newState, agentIndex)\n      self.agentImages[agentIndex] = (newState, image )\n    refresh()\n\n  def update(self, newState):\n    agentIndex = newState._agentMoved\n    agentState = newState.agentStates[agentIndex]\n\n    if self.agentImages[agentIndex][0].isPacman != agentState.isPacman: self.swapImages(agentIndex, agentState)\n    prevState, prevImage = self.agentImages[agentIndex]\n    if agentState.isPacman:\n      self.animatePacman(agentState, prevState, prevImage)\n    else:\n      self.moveGhost(agentState, agentIndex, prevState, prevImage)\n    self.agentImages[agentIndex] = (agentState, prevImage)\n\n    if newState._foodEaten != None:\n      self.removeFood(newState._foodEaten, self.food)\n    if newState._capsuleEaten != None:\n      self.removeCapsule(newState._capsuleEaten, self.capsules)\n    self.infoPane.updateScore(newState.score)\n    if \'ghostDistances\' in dir(newState):\n      self.infoPane.updateGhostDistances(newState.ghostDistances)\n\n  def make_window(self, width, height):\n    grid_width = (width-1) * self.gridSize\n    grid_height = (height-1) * self.gridSize\n    screen_width = 2*self.gridSize + grid_width\n    screen_height = 2*self.gridSize + grid_height + INFO_PANE_HEIGHT\n\n    begin_graphics(screen_width,\n                   screen_height,\n                   BACKGROUND_COLOR,\n                   ""CS188 Pacman"")\n\n  def drawPacman(self, pacman, index):\n    position = self.getPosition(pacman)\n    screen_point = self.to_screen(position)\n    endpoints = self.getEndpoints(self.getDirection(pacman))\n\n    width = PACMAN_OUTLINE_WIDTH\n    outlineColor = PACMAN_COLOR\n    fillColor = PACMAN_COLOR\n\n    if self.capture:\n      outlineColor = TEAM_COLORS[index % 2]\n      fillColor = GHOST_COLORS[index]\n      width = PACMAN_CAPTURE_OUTLINE_WIDTH\n\n    return [circle(screen_point, PACMAN_SCALE * self.gridSize,\n                   fillColor = fillColor, outlineColor = outlineColor,\n                   endpoints = endpoints,\n                   width = width)]\n\n  def getEndpoints(self, direction, position=(0,0)):\n    x, y = position\n    pos = x - int(x) + y - int(y)\n    width = 30 + 80 * math.sin(math.pi* pos)\n\n    delta = width / 2\n    if (direction == \'West\'):\n      endpoints = (180+delta, 180-delta)\n    elif (direction == \'North\'):\n      endpoints = (90+delta, 90-delta)\n    elif (direction == \'South\'):\n      endpoints = (270+delta, 270-delta)\n    else:\n      endpoints = (0+delta, 0-delta)\n    return endpoints\n\n  def movePacman(self, position, direction, image):\n    screenPosition = self.to_screen(position)\n    endpoints = self.getEndpoints( direction, position )\n    r = PACMAN_SCALE * self.gridSize\n    moveCircle(image[0], screenPosition, r, endpoints)\n    refresh()\n\n  def animatePacman(self, pacman, prevPacman, image):\n    if self.frameTime < 0:\n      print \'Press any key to step forward, ""q"" to play\'\n      keys = wait_for_keys()\n      if \'q\' in keys:\n        self.frameTime = 0.1\n    if self.frameTime > 0.01 or self.frameTime < 0:\n      start = time.time()\n      fx, fy = self.getPosition(prevPacman)\n      px, py = self.getPosition(pacman)\n      frames = 4.0\n      for i in range(1,int(frames) + 1):\n        pos = px*i/frames + fx*(frames-i)/frames, py*i/frames + fy*(frames-i)/frames\n        self.movePacman(pos, self.getDirection(pacman), image)\n        refresh()\n        sleep(abs(self.frameTime) / frames)\n    else:\n      self.movePacman(self.getPosition(pacman), self.getDirection(pacman), image)\n    refresh()\n\n  def getGhostColor(self, ghost, ghostIndex):\n    if ghost.scaredTimer > 0:\n      return SCARED_COLOR\n    else:\n      return GHOST_COLORS[ghostIndex]\n\n  def drawGhost(self, ghost, agentIndex):\n    pos = self.getPosition(ghost)\n    dir = self.getDirection(ghost)\n    (screen_x, screen_y) = (self.to_screen(pos) )\n    coords = []\n    for (x, y) in GHOST_SHAPE:\n      coords.append((x*self.gridSize*GHOST_SIZE + screen_x, y*self.gridSize*GHOST_SIZE + screen_y))\n\n    colour = self.getGhostColor(ghost, agentIndex)\n    body = polygon(coords, colour, filled = 1)\n    WHITE = formatColor(1.0, 1.0, 1.0)\n    BLACK = formatColor(0.0, 0.0, 0.0)\n\n    dx = 0\n    dy = 0\n    if dir == \'North\':\n      dy = -0.2\n    if dir == \'South\':\n      dy = 0.2\n    if dir == \'East\':\n      dx = 0.2\n    if dir == \'West\':\n      dx = -0.2\n    leftEye = circle((screen_x+self.gridSize*GHOST_SIZE*(-0.3+dx/1.5), screen_y-self.gridSize*GHOST_SIZE*(0.3-dy/1.5)), self.gridSize*GHOST_SIZE*0.2, WHITE, WHITE)\n    rightEye = circle((screen_x+self.gridSize*GHOST_SIZE*(0.3+dx/1.5), screen_y-self.gridSize*GHOST_SIZE*(0.3-dy/1.5)), self.gridSize*GHOST_SIZE*0.2, WHITE, WHITE)\n    leftPupil = circle((screen_x+self.gridSize*GHOST_SIZE*(-0.3+dx), screen_y-self.gridSize*GHOST_SIZE*(0.3-dy)), self.gridSize*GHOST_SIZE*0.08, BLACK, BLACK)\n    rightPupil = circle((screen_x+self.gridSize*GHOST_SIZE*(0.3+dx), screen_y-self.gridSize*GHOST_SIZE*(0.3-dy)), self.gridSize*GHOST_SIZE*0.08, BLACK, BLACK)\n    ghostImageParts = []\n    ghostImageParts.append(body)\n    ghostImageParts.append(leftEye)\n    ghostImageParts.append(rightEye)\n    ghostImageParts.append(leftPupil)\n    ghostImageParts.append(rightPupil)\n\n    return ghostImageParts\n\n  def moveEyes(self, pos, dir, eyes):\n    (screen_x, screen_y) = (self.to_screen(pos) )\n    dx = 0\n    dy = 0\n    if dir == \'North\':\n      dy = -0.2\n    if dir == \'South\':\n      dy = 0.2\n    if dir == \'East\':\n      dx = 0.2\n    if dir == \'West\':\n      dx = -0.2\n    moveCircle(eyes[0],(screen_x+self.gridSize*GHOST_SIZE*(-0.3+dx/1.5), screen_y-self.gridSize*GHOST_SIZE*(0.3-dy/1.5)), self.gridSize*GHOST_SIZE*0.2)\n    moveCircle(eyes[1],(screen_x+self.gridSize*GHOST_SIZE*(0.3+dx/1.5), screen_y-self.gridSize*GHOST_SIZE*(0.3-dy/1.5)), self.gridSize*GHOST_SIZE*0.2)\n    moveCircle(eyes[2],(screen_x+self.gridSize*GHOST_SIZE*(-0.3+dx), screen_y-self.gridSize*GHOST_SIZE*(0.3-dy)), self.gridSize*GHOST_SIZE*0.08)\n    moveCircle(eyes[3],(screen_x+self.gridSize*GHOST_SIZE*(0.3+dx), screen_y-self.gridSize*GHOST_SIZE*(0.3-dy)), self.gridSize*GHOST_SIZE*0.08)\n\n  def moveGhost(self, ghost, ghostIndex, prevGhost, ghostImageParts):\n    old_x, old_y = self.to_screen(self.getPosition(prevGhost))\n    new_x, new_y = self.to_screen(self.getPosition(ghost))\n    delta = new_x - old_x, new_y - old_y\n\n    for ghostImagePart in ghostImageParts:\n      move_by(ghostImagePart, delta)\n    refresh()\n\n    if ghost.scaredTimer > 0:\n      color = SCARED_COLOR\n    else:\n      color = GHOST_COLORS[ghostIndex]\n    edit(ghostImageParts[0], (\'fill\', color), (\'outline\', color))\n    self.moveEyes(self.getPosition(ghost), self.getDirection(ghost), ghostImageParts[-4:])\n    refresh()\n\n  def getPosition(self, agentState):\n    if agentState.configuration == None: return (-1000, -1000)\n    return agentState.getPosition()\n\n  def getDirection(self, agentState):\n    if agentState.configuration == None: return Directions.STOP\n    return agentState.configuration.getDirection()\n\n  def finish(self):\n    end_graphics()\n\n  def to_screen(self, point):\n    ( x, y ) = point\n    #y = self.height - y\n    x = (x + 1)*self.gridSize\n    y = (self.height  - y)*self.gridSize\n    return ( x, y )\n\n  # Fixes some TK issue with off-center circles\n  def to_screen2(self, point):\n    ( x, y ) = point\n    #y = self.height - y\n    x = (x + 1)*self.gridSize\n    y = (self.height  - y)*self.gridSize\n    return ( x, y )\n\n  def drawWalls(self, wallMatrix):\n    wallColor = WALL_COLOR\n    for xNum, x in enumerate(wallMatrix):\n      if self.capture and (xNum * 2) < wallMatrix.width: wallColor = TEAM_COLORS[0]\n      if self.capture and (xNum * 2) >= wallMatrix.width: wallColor = TEAM_COLORS[1]\n\n      for yNum, cell in enumerate(x):\n        if cell: # There\'s a wall here\n          pos = (xNum, yNum)\n          screen = self.to_screen(pos)\n          screen2 = self.to_screen2(pos)\n\n          # draw each quadrant of the square based on adjacent walls\n          wIsWall = self.isWall(xNum-1, yNum, wallMatrix)\n          eIsWall = self.isWall(xNum+1, yNum, wallMatrix)\n          nIsWall = self.isWall(xNum, yNum+1, wallMatrix)\n          sIsWall = self.isWall(xNum, yNum-1, wallMatrix)\n          nwIsWall = self.isWall(xNum-1, yNum+1, wallMatrix)\n          swIsWall = self.isWall(xNum-1, yNum-1, wallMatrix)\n          neIsWall = self.isWall(xNum+1, yNum+1, wallMatrix)\n          seIsWall = self.isWall(xNum+1, yNum-1, wallMatrix)\n\n          # NE quadrant\n          if (not nIsWall) and (not eIsWall):\n            # inner circle\n            circle(screen2, WALL_RADIUS * self.gridSize, wallColor, wallColor, (0,91), \'arc\')\n          if (nIsWall) and (not eIsWall):\n            # vertical line\n            line(add(screen, (self.gridSize*WALL_RADIUS, 0)), add(screen, (self.gridSize*WALL_RADIUS, self.gridSize*(-0.5)-1)), wallColor)\n          if (not nIsWall) and (eIsWall):\n            # horizontal line\n            line(add(screen, (0, self.gridSize*(-1)*WALL_RADIUS)), add(screen, (self.gridSize*0.5+1, self.gridSize*(-1)*WALL_RADIUS)), wallColor)\n          if (nIsWall) and (eIsWall) and (not neIsWall):\n            # outer circle\n            circle(add(screen2, (self.gridSize*2*WALL_RADIUS, self.gridSize*(-2)*WALL_RADIUS)), WALL_RADIUS * self.gridSize-1, wallColor, wallColor, (180,271), \'arc\')\n            line(add(screen, (self.gridSize*2*WALL_RADIUS-1, self.gridSize*(-1)*WALL_RADIUS)), add(screen, (self.gridSize*0.5+1, self.gridSize*(-1)*WALL_RADIUS)), wallColor)\n            line(add(screen, (self.gridSize*WALL_RADIUS, self.gridSize*(-2)*WALL_RADIUS+1)), add(screen, (self.gridSize*WALL_RADIUS, self.gridSize*(-0.5))), wallColor)\n\n          # NW quadrant\n          if (not nIsWall) and (not wIsWall):\n            # inner circle\n            circle(screen2, WALL_RADIUS * self.gridSize, wallColor, wallColor, (90,181), \'arc\')\n          if (nIsWall) and (not wIsWall):\n            # vertical line\n            line(add(screen, (self.gridSize*(-1)*WALL_RADIUS, 0)), add(screen, (self.gridSize*(-1)*WALL_RADIUS, self.gridSize*(-0.5)-1)), wallColor)\n          if (not nIsWall) and (wIsWall):\n            # horizontal line\n            line(add(screen, (0, self.gridSize*(-1)*WALL_RADIUS)), add(screen, (self.gridSize*(-0.5)-1, self.gridSize*(-1)*WALL_RADIUS)), wallColor)\n          if (nIsWall) and (wIsWall) and (not nwIsWall):\n            # outer circle\n            circle(add(screen2, (self.gridSize*(-2)*WALL_RADIUS, self.gridSize*(-2)*WALL_RADIUS)), WALL_RADIUS * self.gridSize-1, wallColor, wallColor, (270,361), \'arc\')\n            line(add(screen, (self.gridSize*(-2)*WALL_RADIUS+1, self.gridSize*(-1)*WALL_RADIUS)), add(screen, (self.gridSize*(-0.5), self.gridSize*(-1)*WALL_RADIUS)), wallColor)\n            line(add(screen, (self.gridSize*(-1)*WALL_RADIUS, self.gridSize*(-2)*WALL_RADIUS+1)), add(screen, (self.gridSize*(-1)*WALL_RADIUS, self.gridSize*(-0.5))), wallColor)\n\n          # SE quadrant\n          if (not sIsWall) and (not eIsWall):\n            # inner circle\n            circle(screen2, WALL_RADIUS * self.gridSize, wallColor, wallColor, (270,361), \'arc\')\n          if (sIsWall) and (not eIsWall):\n            # vertical line\n            line(add(screen, (self.gridSize*WALL_RADIUS, 0)), add(screen, (self.gridSize*WALL_RADIUS, self.gridSize*(0.5)+1)), wallColor)\n          if (not sIsWall) and (eIsWall):\n            # horizontal line\n            line(add(screen, (0, self.gridSize*(1)*WALL_RADIUS)), add(screen, (self.gridSize*0.5+1, self.gridSize*(1)*WALL_RADIUS)), wallColor)\n          if (sIsWall) and (eIsWall) and (not seIsWall):\n            # outer circle\n            circle(add(screen2, (self.gridSize*2*WALL_RADIUS, self.gridSize*(2)*WALL_RADIUS)), WALL_RADIUS * self.gridSize-1, wallColor, wallColor, (90,181), \'arc\')\n            line(add(screen, (self.gridSize*2*WALL_RADIUS-1, self.gridSize*(1)*WALL_RADIUS)), add(screen, (self.gridSize*0.5, self.gridSize*(1)*WALL_RADIUS)), wallColor)\n            line(add(screen, (self.gridSize*WALL_RADIUS, self.gridSize*(2)*WALL_RADIUS-1)), add(screen, (self.gridSize*WALL_RADIUS, self.gridSize*(0.5))), wallColor)\n\n          # SW quadrant\n          if (not sIsWall) and (not wIsWall):\n            # inner circle\n            circle(screen2, WALL_RADIUS * self.gridSize, wallColor, wallColor, (180,271), \'arc\')\n          if (sIsWall) and (not wIsWall):\n            # vertical line\n            line(add(screen, (self.gridSize*(-1)*WALL_RADIUS, 0)), add(screen, (self.gridSize*(-1)*WALL_RADIUS, self.gridSize*(0.5)+1)), wallColor)\n          if (not sIsWall) and (wIsWall):\n            # horizontal line\n            line(add(screen, (0, self.gridSize*(1)*WALL_RADIUS)), add(screen, (self.gridSize*(-0.5)-1, self.gridSize*(1)*WALL_RADIUS)), wallColor)\n          if (sIsWall) and (wIsWall) and (not swIsWall):\n            # outer circle\n            circle(add(screen2, (self.gridSize*(-2)*WALL_RADIUS, self.gridSize*(2)*WALL_RADIUS)), WALL_RADIUS * self.gridSize-1, wallColor, wallColor, (0,91), \'arc\')\n            line(add(screen, (self.gridSize*(-2)*WALL_RADIUS+1, self.gridSize*(1)*WALL_RADIUS)), add(screen, (self.gridSize*(-0.5), self.gridSize*(1)*WALL_RADIUS)), wallColor)\n            line(add(screen, (self.gridSize*(-1)*WALL_RADIUS, self.gridSize*(2)*WALL_RADIUS-1)), add(screen, (self.gridSize*(-1)*WALL_RADIUS, self.gridSize*(0.5))), wallColor)\n\n  def isWall(self, x, y, walls):\n    if x < 0 or y < 0:\n      return False\n    if x >= walls.width or y >= walls.height:\n      return False\n    return walls[x][y]\n\n  def drawFood(self, foodMatrix ):\n    foodImages = []\n    color = FOOD_COLOR\n    for xNum, x in enumerate(foodMatrix):\n      if self.capture and (xNum * 2) <= foodMatrix.width: color = TEAM_COLORS[0]\n      if self.capture and (xNum * 2) > foodMatrix.width: color = TEAM_COLORS[1]\n      imageRow = []\n      foodImages.append(imageRow)\n      for yNum, cell in enumerate(x):\n        if cell: # There\'s food here\n          screen = self.to_screen((xNum, yNum ))\n          dot = circle( screen,\n                        FOOD_SIZE * self.gridSize,\n                        outlineColor = color, fillColor = color,\n                        width = 1)\n          imageRow.append(dot)\n        else:\n          imageRow.append(None)\n    return foodImages\n\n  def drawCapsules(self, capsules ):\n    capsuleImages = {}\n    for capsule in capsules:\n      ( screen_x, screen_y ) = self.to_screen(capsule)\n      dot = circle( (screen_x, screen_y),\n                        CAPSULE_SIZE * self.gridSize,\n                        outlineColor = CAPSULE_COLOR,\n                        fillColor = CAPSULE_COLOR,\n                        width = 1)\n      capsuleImages[capsule] = dot\n    return capsuleImages\n\n  def removeFood(self, cell, foodImages ):\n    x, y = cell\n    remove_from_screen(foodImages[x][y])\n\n  def removeCapsule(self, cell, capsuleImages ):\n    x, y = cell\n    remove_from_screen(capsuleImages[(x, y)])\n\n  def drawExpandedCells(self, cells):\n    """"""\n    Draws an overlay of expanded grid positions for search agents\n    """"""\n    n = float(len(cells))\n    baseColor = [1.0, 0.0, 0.0]\n    self.clearExpandedCells()\n    self.expandedCells = []\n    for k, cell in enumerate(cells):\n       screenPos = self.to_screen( cell)\n       cellColor = formatColor(*[(n-k) * c * .5 / n + .25 for c in baseColor])\n       block = square(screenPos,\n                0.5 * self.gridSize,\n                color = cellColor,\n                filled = 1, behind=2)\n       self.expandedCells.append(block)\n       if self.frameTime < 0:\n         refresh()\n\n  def clearExpandedCells(self):\n    if \'expandedCells\' in dir(self) and len(self.expandedCells) > 0:\n      for cell in self.expandedCells:\n        remove_from_screen(cell)\n\n\n  def updateDistributions(self, distributions):\n    ""Draws an agent\'s belief distributions""\n    if self.distributionImages == None:\n      self.drawDistributions(self.previousState)\n    for x in range(len(self.distributionImages)):\n      for y in range(len(self.distributionImages[0])):\n        image = self.distributionImages[x][y]\n        weights = [dist[ (x,y) ] for dist in distributions]\n\n        if sum(weights) != 0:\n          pass\n        # Fog of war\n        color = [0.0,0.0,0.0]\n        colors = GHOST_VEC_COLORS[1:] # With Pacman\n        if self.capture: colors = GHOST_VEC_COLORS\n        for weight, gcolor in zip(weights, colors):\n          color = [min(1.0, c + 0.95 * g * weight ** .3) for c,g in zip(color, gcolor)]\n        changeColor(image, formatColor(*color))\n    refresh()\n\nclass FirstPersonPacmanGraphics(PacmanGraphics):\n  def __init__(self, zoom = 1.0, showGhosts = True, capture = False, frameTime=0):\n    PacmanGraphics.__init__(self, zoom, frameTime=frameTime)\n    self.showGhosts = showGhosts\n    self.capture = capture\n\n  def initialize(self, state, isBlue = False):\n\n    self.isBlue = isBlue\n    PacmanGraphics.startGraphics(self, state)\n    # Initialize distribution images\n    walls = state.layout.walls\n    dist = []\n    self.layout = state.layout\n\n    # Draw the rest\n    self.distributionImages = None  # initialize lazily\n    self.drawStaticObjects(state)\n    self.drawAgentObjects(state)\n\n    # Information\n    self.previousState = state\n\n  def lookAhead(self, config, state):\n    if config.getDirection() == \'Stop\':\n      return\n    else:\n      pass\n      # Draw relevant ghosts\n      allGhosts = state.getGhostStates()\n      visibleGhosts = state.getVisibleGhosts()\n      for i, ghost in enumerate(allGhosts):\n        if ghost in visibleGhosts:\n          self.drawGhost(ghost, i)\n        else:\n          self.currentGhostImages[i] = None\n\n  def getGhostColor(self, ghost, ghostIndex):\n    return GHOST_COLORS[ghostIndex]\n\n  def getPosition(self, ghostState):\n    if not self.showGhosts and not ghostState.isPacman and ghostState.getPosition()[1] > 1:\n      return (-1000, -1000)\n    else:\n      return PacmanGraphics.getPosition(self, ghostState)\n\ndef add(x, y):\n  return (x[0] + y[0], x[1] + y[1])\n\n\n# Saving graphical output\n# -----------------------\n# Note: to make an animated gif from this postscript output, try the command:\n# convert -delay 7 -loop 1 -compress lzw -layers optimize frame* out.gif\n# convert is part of imagemagick (freeware)\n\nSAVE_POSTSCRIPT = False\nPOSTSCRIPT_OUTPUT_DIR = \'frames\'\nFRAME_NUMBER = 0\nimport os\n\ndef saveFrame():\n  ""Saves the current graphical output as a postscript file""\n  global SAVE_POSTSCRIPT, FRAME_NUMBER, POSTSCRIPT_OUTPUT_DIR\n  if not SAVE_POSTSCRIPT: return\n  if not os.path.exists(POSTSCRIPT_OUTPUT_DIR): os.mkdir(POSTSCRIPT_OUTPUT_DIR)\n  name = os.path.join(POSTSCRIPT_OUTPUT_DIR, \'frame_%08d.ps\' % FRAME_NUMBER)\n  FRAME_NUMBER += 1\n  writePostscript(name) # writes the current canvas'"
week03_model_free/crawler_and_pacman/seminar_py2/graphicsGridworldDisplay.py,0,"b'# graphicsGridworldDisplay.py\n# ---------------------------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nimport util\nfrom graphicsUtils import *\n\nclass GraphicsGridworldDisplay:\n  \n  def __init__(self, gridworld, size=120, speed=1.0):\n    self.gridworld = gridworld\n    self.size = size\n    self.speed = speed\n    \n  def start(self):\n    setup(self.gridworld, size=self.size)\n  \n  def pause(self):\n    wait_for_keys()\n  \n  def displayValues(self, agent, currentState = None, message = \'Agent Values\'):\n    values = util.Counter()\n    policy = {}\n    states = self.gridworld.getStates()\n    for state in states:\n      values[state] = agent.getValue(state)\n      policy[state] = agent.getPolicy(state)\n    drawValues(self.gridworld, values, policy, currentState, message)\n    sleep(0.05 / self.speed)\n\n  def displayNullValues(self, currentState = None, message = \'\'):\n    values = util.Counter()\n    #policy = {}\n    states = self.gridworld.getStates()\n    for state in states:\n      values[state] = 0.0\n      #policy[state] = agent.getPolicy(state)\n    drawNullValues(self.gridworld, currentState,\'\')\n    # drawValues(self.gridworld, values, policy, currentState, message)\n    sleep(0.05 / self.speed)\n\n  def displayQValues(self, agent, currentState = None, message = \'Agent Q-Values\'):\n    qValues = util.Counter()\n    states = self.gridworld.getStates()\n    for state in states:\n      for action in self.gridworld.getPossibleActions(state):\n        qValues[(state, action)] = agent.getQValue(state, action)\n    drawQValues(self.gridworld, qValues, currentState, message)\n    sleep(0.05 / self.speed)\n\nBACKGROUND_COLOR = formatColor(0,0,0)    \nEDGE_COLOR = formatColor(1,1,1)\nOBSTACLE_COLOR = formatColor(0.5,0.5,0.5)\nTEXT_COLOR = formatColor(1,1,1)\nMUTED_TEXT_COLOR = formatColor(0.7,0.7,0.7)\nLOCATION_COLOR = formatColor(0,0,1)\n\nWINDOW_SIZE = -1\nGRID_SIZE = -1\nGRID_HEIGHT = -1\nMARGIN = -1\n\ndef setup(gridworld, title = ""Gridworld Display"", size = 120):\n  global GRID_SIZE, MARGIN, SCREEN_WIDTH, SCREEN_HEIGHT, GRID_HEIGHT\n  grid = gridworld.grid\n  WINDOW_SIZE = size\n  GRID_SIZE = size\n  GRID_HEIGHT = grid.height\n  MARGIN = GRID_SIZE * 0.75\n  screen_width = (grid.width - 1) * GRID_SIZE + MARGIN * 2\n  screen_height = (grid.height - 0.5) * GRID_SIZE + MARGIN * 2\n\n  begin_graphics(screen_width,    \n                 screen_height,\n                 BACKGROUND_COLOR, title=title)\n\ndef drawNullValues(gridworld, currentState = None, message = \'\'):\n  grid = gridworld.grid\n  blank()\n  for x in range(grid.width):\n    for y in range(grid.height):\n      state = (x, y)\n      gridType = grid[x][y]\n      isExit = (str(gridType) != gridType)\n      isCurrent = (currentState == state)\n      if gridType == \'#\':\n        drawSquare(x, y, 0, 0, 0, None, None, True, False, isCurrent)\n      else:\n        drawNullSquare(gridworld.grid, x, y, False, isExit, isCurrent)\n  pos = to_screen(((grid.width - 1.0) / 2.0, - 0.8))\n  text( pos, TEXT_COLOR, message, ""Courier"", -32, ""bold"", ""c"")\n  \n\ndef drawValues(gridworld, values, policy, currentState = None, message = \'State Values\'):\n  grid = gridworld.grid\n  blank()\n  valueList = [values[state] for state in gridworld.getStates()] + [0.0]\n  minValue = min(valueList)\n  maxValue = max(valueList)\n  for x in range(grid.width):\n    for y in range(grid.height):\n      state = (x, y)\n      gridType = grid[x][y]\n      isExit = (str(gridType) != gridType)\n      isCurrent = (currentState == state)\n      if gridType == \'#\':\n        drawSquare(x, y, 0, 0, 0, None, None, True, False, isCurrent)\n      else:\n        value = values[state]\n        action = None\n        if policy != None and state in policy:\n          action = policy[state]\n          actions = gridworld.getPossibleActions(state)\n        if action not in actions and \'exit\' in actions:\n          action = \'exit\'\n        valString = \'%.2f\' % value\n        drawSquare(x, y, value, minValue, maxValue, valString, action, False, isExit, isCurrent)\n  pos = to_screen(((grid.width - 1.0) / 2.0, - 0.8))\n  text( pos, TEXT_COLOR, message, ""Courier"", -32, ""bold"", ""c"")\n\ndef drawQValues(gridworld, qValues, currentState = None, message = \'State-Action Q-Values\'):\n  grid = gridworld.grid\n  blank()\n  stateCrossActions = [[(state, action) for action in gridworld.getPossibleActions(state)] for state in gridworld.getStates()]\n  qStates = reduce(lambda x,y: x+y, stateCrossActions, [])\n  qValueList = [qValues[(state, action)] for state, action in qStates] + [0.0]\n  minValue = min(qValueList)\n  maxValue = max(qValueList)\n  for x in range(grid.width):\n    for y in range(grid.height):\n      state = (x, y)\n      gridType = grid[x][y]\n      isExit = (str(gridType) != gridType)\n      isCurrent = (currentState == state)\n      actions = gridworld.getPossibleActions(state)\n      if actions == None or len(actions) == 0:\n        actions = [None]\n      bestQ = max([qValues[(state, action)] for action in actions])\n      bestActions = [action for action in actions if qValues[(state, action)] == bestQ]\n\n      q = util.Counter()\n      valStrings = {}\n      for action in actions:\n        v = qValues[(state, action)]\n        q[action] += v\n        valStrings[action] = \'%.2f\' % v\n      if gridType == \'#\':\n        drawSquare(x, y, 0, 0, 0, None, None, True, False, isCurrent)\n      elif isExit:\n        action = \'exit\'\n        value = q[action]\n        valString = \'%.2f\' % value\n        drawSquare(x, y, value, minValue, maxValue, valString, action, False, isExit, isCurrent)\n      else:\n        drawSquareQ(x, y, q, minValue, maxValue, valStrings, actions, isCurrent)\n  pos = to_screen(((grid.width - 1.0) / 2.0, - 0.8))\n  text( pos, TEXT_COLOR, message, ""Courier"", -32, ""bold"", ""c"")\n\n\ndef blank():\n  clear_screen()\n\ndef drawNullSquare(grid,x, y, isObstacle, isTerminal, isCurrent):      \n\n  square_color = getColor(0, -1, 1)\n  \n  if isObstacle:\n    square_color = OBSTACLE_COLOR\n    \n  (screen_x, screen_y) = to_screen((x, y))\n  square( (screen_x, screen_y), \n                 0.5* GRID_SIZE, \n                 color = square_color,\n                 filled = 1,\n                 width = 1)\n  \n  square( (screen_x, screen_y), \n                 0.5* GRID_SIZE, \n                 color = EDGE_COLOR,\n                 filled = 0,\n                 width = 3)\n  \n  if isTerminal and not isObstacle:\n    square( (screen_x, screen_y), \n                 0.4* GRID_SIZE, \n                 color = EDGE_COLOR,\n                 filled = 0,\n                 width = 2)\n    text( (screen_x, screen_y), \n           TEXT_COLOR, \n           str(grid[x][y]), \n           ""Courier"", -24, ""bold"", ""c"")\n      \n  \n  text_color = TEXT_COLOR\n\n  if not isObstacle and isCurrent:\n    circle( (screen_x, screen_y), 0.1*GRID_SIZE, LOCATION_COLOR, fillColor=LOCATION_COLOR )\n\n  # if not isObstacle:\n  #   text( (screen_x, screen_y), text_color, valStr, ""Courier"", 24, ""bold"", ""c"")\n      \ndef drawSquare(x, y, val, min, max, valStr, action, isObstacle, isTerminal, isCurrent):\n\n  square_color = getColor(val, min, max)\n  \n  if isObstacle:\n    square_color = OBSTACLE_COLOR\n    \n  (screen_x, screen_y) = to_screen((x, y))\n  square( (screen_x, screen_y), \n                 0.5* GRID_SIZE, \n                 color = square_color,\n                 filled = 1,\n                 width = 1)\n  square( (screen_x, screen_y), \n                 0.5* GRID_SIZE, \n                 color = EDGE_COLOR,\n                 filled = 0,\n                 width = 3)\n  if isTerminal and not isObstacle:\n    square( (screen_x, screen_y), \n                 0.4* GRID_SIZE, \n                 color = EDGE_COLOR,\n                 filled = 0,\n                 width = 2)\n        \n    \n  if action == \'north\':\n    polygon( [(screen_x, screen_y - 0.45*GRID_SIZE), (screen_x+0.05*GRID_SIZE, screen_y-0.40*GRID_SIZE), (screen_x-0.05*GRID_SIZE, screen_y-0.40*GRID_SIZE)], EDGE_COLOR, filled = 1, smoothed = False)\n  if action == \'south\':\n    polygon( [(screen_x, screen_y + 0.45*GRID_SIZE), (screen_x+0.05*GRID_SIZE, screen_y+0.40*GRID_SIZE), (screen_x-0.05*GRID_SIZE, screen_y+0.40*GRID_SIZE)], EDGE_COLOR, filled = 1, smoothed = False)\n  if action == \'west\':\n    polygon( [(screen_x-0.45*GRID_SIZE, screen_y), (screen_x-0.4*GRID_SIZE, screen_y+0.05*GRID_SIZE), (screen_x-0.4*GRID_SIZE, screen_y-0.05*GRID_SIZE)], EDGE_COLOR, filled = 1, smoothed = False)\n  if action == \'east\':\n    polygon( [(screen_x+0.45*GRID_SIZE, screen_y), (screen_x+0.4*GRID_SIZE, screen_y+0.05*GRID_SIZE), (screen_x+0.4*GRID_SIZE, screen_y-0.05*GRID_SIZE)], EDGE_COLOR, filled = 1, smoothed = False)\n    \n  \n  text_color = TEXT_COLOR\n\n  if not isObstacle and isCurrent:\n    circle( (screen_x, screen_y), 0.1*GRID_SIZE, outlineColor=LOCATION_COLOR, fillColor=LOCATION_COLOR )\n\n  if not isObstacle:\n    text( (screen_x, screen_y), text_color, valStr, ""Courier"", -30, ""bold"", ""c"")\n\n\ndef drawSquareQ(x, y, qVals, minVal, maxVal, valStrs, bestActions, isCurrent):\n\n  (screen_x, screen_y) = to_screen((x, y))\n  \n  center = (screen_x, screen_y)\n  nw = (screen_x-0.5*GRID_SIZE, screen_y-0.5*GRID_SIZE)\n  ne = (screen_x+0.5*GRID_SIZE, screen_y-0.5*GRID_SIZE)\n  se = (screen_x+0.5*GRID_SIZE, screen_y+0.5*GRID_SIZE)\n  sw = (screen_x-0.5*GRID_SIZE, screen_y+0.5*GRID_SIZE)\n  n = (screen_x, screen_y-0.5*GRID_SIZE+5)\n  s = (screen_x, screen_y+0.5*GRID_SIZE-5)\n  w = (screen_x-0.5*GRID_SIZE+5, screen_y)\n  e = (screen_x+0.5*GRID_SIZE-5, screen_y)\n  \n  actions = qVals.keys()\n  for action in actions:\n    \n    wedge_color = getColor(qVals[action], minVal, maxVal)\n\n    if action == \'north\':\n      polygon( (center, nw, ne), wedge_color, filled = 1, smoothed = False)\n      #text(n, text_color, valStr, ""Courier"", 8, ""bold"", ""n"")\n    if action == \'south\':\n      polygon( (center, sw, se), wedge_color, filled = 1, smoothed = False)\n      #text(s, text_color, valStr, ""Courier"", 8, ""bold"", ""s"")\n    if action == \'east\':\n      polygon( (center, ne, se), wedge_color, filled = 1, smoothed = False)\n      #text(e, text_color, valStr, ""Courier"", 8, ""bold"", ""e"")\n    if action == \'west\':\n      polygon( (center, nw, sw), wedge_color, filled = 1, smoothed = False)\n      #text(w, text_color, valStr, ""Courier"", 8, ""bold"", ""w"")\n      \n  square( (screen_x, screen_y), \n                 0.5* GRID_SIZE, \n                 color = EDGE_COLOR,\n                 filled = 0,\n                 width = 3)\n  line(ne, sw, color = EDGE_COLOR)\n  line(nw, se, color = EDGE_COLOR)\n\n  if isCurrent:\n    circle( (screen_x, screen_y), 0.1*GRID_SIZE, LOCATION_COLOR, fillColor=LOCATION_COLOR )\n\n  for action in actions:\n    text_color = TEXT_COLOR\n    if qVals[action] < max(qVals.values()): text_color = MUTED_TEXT_COLOR\n    valStr = """"\n    if action in valStrs:\n      valStr = valStrs[action]\n    h = -20\n    if action == \'north\':\n      #polygon( (center, nw, ne), wedge_color, filled = 1, smooth = 0)\n      text(n, text_color, valStr, ""Courier"", h, ""bold"", ""n"")\n    if action == \'south\':\n      #polygon( (center, sw, se), wedge_color, filled = 1, smooth = 0)\n      text(s, text_color, valStr, ""Courier"", h, ""bold"", ""s"")\n    if action == \'east\':\n      #polygon( (center, ne, se), wedge_color, filled = 1, smooth = 0)\n      text(e, text_color, valStr, ""Courier"", h, ""bold"", ""e"")\n    if action == \'west\':\n      #polygon( (center, nw, sw), wedge_color, filled = 1, smooth = 0)\n      text(w, text_color, valStr, ""Courier"", h, ""bold"", ""w"")\n\n\ndef getColor(val, minVal, max):\n  r, g = 0.0, 0.0\n  if val < 0 and minVal < 0:\n    r = val * 0.65 / minVal\n  if val > 0 and max > 0:\n    g = val * 0.65 / max\n  return formatColor(r,g,0.0)\n\n\ndef square(pos, size, color, filled, width):\n  x, y = pos\n  dx, dy = size, size\n  return polygon([(x - dx, y - dy), (x - dx, y + dy), (x + dx, y + dy), (x + dx, y - dy)], outlineColor=color, fillColor=color, filled=filled, width=width, smoothed=False)\n  \n  \ndef to_screen(point):\n  ( gamex, gamey ) = point\n  x = gamex*GRID_SIZE + MARGIN  \n  y = (GRID_HEIGHT - gamey - 1)*GRID_SIZE + MARGIN  \n  return ( x, y )\n\ndef to_grid(point):\n  (x, y) = point\n  x = int ((y - MARGIN + GRID_SIZE * 0.5) / GRID_SIZE)\n  y = int ((x - MARGIN + GRID_SIZE * 0.5) / GRID_SIZE)\n  print point, ""-->"", (x, y)\n  return (x, y)\n'"
week03_model_free/crawler_and_pacman/seminar_py2/graphicsUtils.py,0,"b'# graphicsUtils.py\n# ----------------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nimport sys\nimport math\nimport random\nimport string\nimport time\nimport types\nimport Tkinter\n\n_Windows = sys.platform == \'win32\'  # True if on Win95/98/NT\n\n_root_window = None      # The root window for graphics output\n_canvas = None      # The canvas which holds graphics\n_canvas_xs = None      # Size of canvas object\n_canvas_ys = None\n_canvas_x = None      # Current position on canvas\n_canvas_y = None\n_canvas_col = None      # Current colour (set to black below)\n_canvas_tsize = 12\n_canvas_tserifs = 0\n\ndef formatColor(r, g, b):\n  return \'#%02x%02x%02x\' % (int(r * 255), int(g * 255), int(b * 255))\n\ndef colorToVector(color):\n  return map(lambda x: int(x, 16) / 256.0, [color[1:3], color[3:5], color[5:7]])\n\nif _Windows:\n    _canvas_tfonts = [\'times new roman\', \'lucida console\']\nelse:\n    _canvas_tfonts = [\'times\', \'lucidasans-24\']\n    pass # XXX need defaults here\n\ndef sleep(secs):\n    global _root_window\n    if _root_window == None:\n        time.sleep(secs)\n    else:\n        _root_window.update_idletasks()\n        _root_window.after(int(1000 * secs), _root_window.quit)\n        _root_window.mainloop()\n\ndef begin_graphics(width=640, height=480, color=formatColor(0, 0, 0), title=None):\n\n    global _root_window, _canvas, _canvas_x, _canvas_y, _canvas_xs, _canvas_ys, _bg_color\n\n    # Check for duplicate call\n    if _root_window is not None:\n        # Lose the window.\n        _root_window.destroy()\n        \n    # Save the canvas size parameters\n    _canvas_xs, _canvas_ys = width - 1, height - 1\n    _canvas_x, _canvas_y = 0, _canvas_ys\n    _bg_color = color\n    \n    # Create the root window\n    _root_window = Tkinter.Tk()\n    _root_window.protocol(\'WM_DELETE_WINDOW\', _destroy_window)\n    _root_window.title(title or \'Graphics Window\')\n    _root_window.resizable(0, 0)\n\n    # Create the canvas object\n    try:\n      _canvas = Tkinter.Canvas(_root_window, width=width, height=height)\n      _canvas.pack()\n      draw_background()\n      _canvas.update()\n    except:\n      _root_window = None\n      raise\n\n    # Bind to key-down and key-up events\n    _root_window.bind( ""<KeyPress>"", _keypress )\n    _root_window.bind( ""<KeyRelease>"", _keyrelease )\n    _root_window.bind( ""<FocusIn>"", _clear_keys )\n    _root_window.bind( ""<FocusOut>"", _clear_keys )\n    _root_window.bind( ""<Button-1>"", _leftclick )\n    _root_window.bind( ""<Button-2>"", _rightclick )\n    _root_window.bind( ""<Button-3>"", _rightclick )\n    _root_window.bind( ""<Control-Button-1>"", _ctrl_leftclick)\n    _clear_keys()\n\n_leftclick_loc = None\n_rightclick_loc = None\n_ctrl_leftclick_loc = None\n\ndef _leftclick(event):\n  global _leftclick_loc\n  _leftclick_loc = (event.x, event.y)\n\ndef _rightclick(event):\n  global _rightclick_loc\n  _rightclick_loc = (event.x, event.y)\n\ndef _ctrl_leftclick(event):\n  global _ctrl_leftclick_loc\n  _ctrl_leftclick_loc = (event.x, event.y)\n\ndef wait_for_click():\n  while True:\n    global _leftclick_loc\n    global _rightclick_loc\n    global _ctrl_leftclick_loc\n    if _leftclick_loc != None:\n      val = _leftclick_loc\n      _leftclick_loc = None\n      return val, \'left\'\n    if _rightclick_loc != None:\n      val = _rightclick_loc\n      _rightclick_loc = None\n      return val, \'right\'\n    if _ctrl_leftclick_loc != None:\n      val = _ctrl_leftclick_loc\n      _ctrl_leftclick_loc = None\n      return val, \'ctrl_left\'\n    sleep(0.05)  \n\ndef draw_background():\n    corners = [(0,0), (0, _canvas_ys), (_canvas_xs, _canvas_ys), (_canvas_xs, 0)]\n    polygon(corners, _bg_color, fillColor=_bg_color, filled=True, smoothed=False)\n    \ndef _destroy_window(event=None):\n    sys.exit(0)\n#    global _root_window\n#    _root_window.destroy()\n#    _root_window = None\n    #print ""DESTROY""\n\ndef end_graphics():\n    global _root_window, _canvas, _mouse_enabled\n    try:\n      try:\n        sleep(1)\n        if _root_window != None: \n          _root_window.destroy()\n      except SystemExit, e:\n        print \'Ending graphics raised an exception:\', e\n    finally:\n      _root_window = None\n      _canvas = None\n      _mouse_enabled = 0\n      _clear_keys()\n\ndef clear_screen(background=None):\n    global _canvas_x, _canvas_y\n    _canvas.delete(\'all\')\n    draw_background()\n    _canvas_x, _canvas_y = 0, _canvas_ys\n\ndef polygon(coords, outlineColor, fillColor=None, filled=1, smoothed=1, behind=0, width=1):\n  c = []\n  for coord in coords:\n    c.append(coord[0])\n    c.append(coord[1])\n  if fillColor == None: fillColor = outlineColor\n  if filled == 0: fillColor = """"\n  poly = _canvas.create_polygon(c, outline=outlineColor, fill=fillColor, smooth=smoothed, width=width)\n  if behind > 0:    \n    _canvas.tag_lower(poly, behind) # Higher should be more visible\n  return poly\n  \ndef square(pos, r, color, filled=1, behind=0):\n  x, y = pos\n  coords = [(x - r, y - r), (x + r, y - r), (x + r, y + r), (x - r, y + r)]\n  return polygon(coords, color, color, filled, 0, behind=behind)\n\ndef circle(pos, r, outlineColor, fillColor, endpoints=None, style=\'pieslice\', width=2):\n    x, y = pos\n    x0, x1 = x - r - 1, x + r\n    y0, y1 = y - r - 1, y + r\n    if endpoints == None:\n      e = [0, 359]\n    else:\n      e = list(endpoints)\n    while e[0] > e[1]: e[1] = e[1] + 360\n\n    return _canvas.create_arc(x0, y0, x1, y1, outline=outlineColor, fill=fillColor,\n                              extent=e[1] - e[0], start=e[0], style=style, width=width)\n\ndef image(pos, file=""../../blueghost.gif""):\n    x, y = pos\n    # img = PhotoImage(file=file)\n    return _canvas.create_image(x, y, image = Tkinter.PhotoImage(file=file), anchor = Tkinter.NW)\n    \n    \ndef refresh():\n      _canvas.update_idletasks()\n                                                    \ndef moveCircle(id, pos, r, endpoints=None):\n    global _canvas_x, _canvas_y\n    \n    x, y = pos\n#    x0, x1 = x - r, x + r + 1\n#    y0, y1 = y - r, y + r + 1\n    x0, x1 = x - r - 1, x + r\n    y0, y1 = y - r - 1, y + r\n    if endpoints == None:\n      e = [0, 359]\n    else:\n      e = list(endpoints)\n    while e[0] > e[1]: e[1] = e[1] + 360\n\n    edit(id, (\'start\', e[0]), (\'extent\', e[1] - e[0]))\n    move_to(id, x0, y0)\n\ndef edit(id, *args):\n    _canvas.itemconfigure(id, **dict(args))\n    \ndef text(pos, color, contents, font=\'Helvetica\', size=12, style=\'normal\', anchor=""nw""):\n    global _canvas_x, _canvas_y\n    x, y = pos\n    font = (font, str(size), style)\n    return _canvas.create_text(x, y, fill=color, text=contents, font=font, anchor=anchor)\n\ndef changeText(id, newText, font=None, size=12, style=\'normal\'):\n  _canvas.itemconfigure(id, text=newText)\n  if font != None:\n    _canvas.itemconfigure(id, font=(font, \'-%d\' % size, style))\n\ndef changeColor(id, newColor):\n  _canvas.itemconfigure(id, fill=newColor)\n\ndef line(here, there, color=formatColor(0, 0, 0), width=2):\n  x0, y0 = here[0], here[1]\n  x1, y1 = there[0], there[1]\n  return _canvas.create_line(x0, y0, x1, y1, fill=color, width=width)\n\n##############################################################################\n### Keypress handling ########################################################\n##############################################################################\n\n# We bind to key-down and key-up events.\n\n_keysdown = {}\n_keyswaiting = {}\n# This holds an unprocessed key release.  We delay key releases by up to\n# one call to keys_pressed() to get round a problem with auto repeat.\n_got_release = None\n\ndef _keypress(event):\n    global _got_release\n    #remap_arrows(event)\n    _keysdown[event.keysym] = 1\n    _keyswaiting[event.keysym] = 1\n#    print event.char, event.keycode\n    _got_release = None\n\ndef _keyrelease(event):\n    global _got_release\n    #remap_arrows(event)\n    try:\n      del _keysdown[event.keysym]\n    except:\n      pass\n    _got_release = 1\n    \ndef remap_arrows(event):\n    # TURN ARROW PRESSES INTO LETTERS (SHOULD BE IN KEYBOARD AGENT)\n    if event.char in [\'a\', \'s\', \'d\', \'w\']:\n      return\n    if event.keycode in [37, 101]: # LEFT ARROW (win / x)\n      event.char = \'a\'\n    if event.keycode in [38, 99]: # UP ARROW\n      event.char = \'w\'\n    if event.keycode in [39, 102]: # RIGHT ARROW\n      event.char = \'d\'\n    if event.keycode in [40, 104]: # DOWN ARROW\n      event.char = \'s\'\n\ndef _clear_keys(event=None):\n    global _keysdown, _got_release, _keyswaiting\n    _keysdown = {}\n    _keyswaiting = {}\n    _got_release = None\n\ndef keys_pressed(d_o_e=Tkinter.tkinter.dooneevent,\n                 d_w=Tkinter.tkinter.DONT_WAIT):\n    d_o_e(d_w)\n    if _got_release:\n      d_o_e(d_w)\n    return _keysdown.keys()\n  \ndef keys_waiting():\n  global _keyswaiting\n  keys = _keyswaiting.keys()\n  _keyswaiting = {}\n  return keys\n\n# Block for a list of keys...\n\ndef wait_for_keys():\n    keys = []\n    while keys == []:\n        keys = keys_pressed()\n        sleep(0.05)\n    return keys\n\ndef remove_from_screen(x,\n                       d_o_e=Tkinter.tkinter.dooneevent,\n                       d_w=Tkinter.tkinter.DONT_WAIT):\n    _canvas.delete(x)\n    d_o_e(d_w)\n\ndef _adjust_coords(coord_list, x, y):\n    for i in range(0, len(coord_list), 2):\n        coord_list[i] = coord_list[i] + x\n        coord_list[i + 1] = coord_list[i + 1] + y\n    return coord_list\n\ndef move_to(object, x, y=None,\n            d_o_e=Tkinter.tkinter.dooneevent,\n            d_w=Tkinter.tkinter.DONT_WAIT):\n    if y is None:\n        try: x, y = x\n        except: raise  \'incomprehensible coordinates\' \n        \n    horiz = True\n    newCoords = []\n    current_x, current_y = _canvas.coords(object)[0:2] # first point\n    for coord in  _canvas.coords(object):\n      if horiz:  \n        inc = x - current_x\n      else:      \n        inc = y - current_y\n      horiz = not horiz\n      \n      newCoords.append(coord + inc)\n    \n    _canvas.coords(object, *newCoords)\n    d_o_e(d_w)\n    \ndef move_by(object, x, y=None,\n            d_o_e=Tkinter.tkinter.dooneevent,\n            d_w=Tkinter.tkinter.DONT_WAIT):\n    if y is None:\n        try: x, y = x\n        except: raise Exception, \'incomprehensible coordinates\' \n    \n    horiz = True\n    newCoords = []\n    for coord in  _canvas.coords(object):\n      if horiz:  \n        inc = x\n      else:      \n        inc = y\n      horiz = not horiz\n      \n      newCoords.append(coord + inc)\n      \n    _canvas.coords(object, *newCoords)\n    d_o_e(d_w)\n    \ndef writePostscript(filename):\n  ""Writes the current canvas to a postscript file.""    \n  psfile = file(filename, \'w\')\n  psfile.write(_canvas.postscript(pageanchor=\'sw\',\n                   y=\'0.c\', \n                   x=\'0.c\'))\n  psfile.close()\n  \nghost_shape = [                \n    (0, - 0.5),\n    (0.25, - 0.75),\n    (0.5, - 0.5),\n    (0.75, - 0.75),\n    (0.75, 0.5),\n    (0.5, 0.75),\n    (- 0.5, 0.75),\n    (- 0.75, 0.5),\n    (- 0.75, - 0.75),\n    (- 0.5, - 0.5),\n    (- 0.25, - 0.75)\n  ]\n\nif __name__ == \'__main__\':\n  begin_graphics()\n  clear_screen()\n  ghost_shape = [(x * 10 + 20, y * 10 + 20) for x, y in ghost_shape]\n  g = polygon(ghost_shape, formatColor(1, 1, 1))\n  move_to(g, (50, 50))\n  circle((150, 150), 20, formatColor(0.7, 0.3, 0.0), endpoints=[15, - 15])\n  sleep(2)'"
week03_model_free/crawler_and_pacman/seminar_py2/gridworld.py,0,"b'# gridworld.py\n# ------------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nimport random\nimport sys\nimport mdp\nimport environment\nimport util\nimport optparse\n\nclass Gridworld(mdp.MarkovDecisionProcess):\n  """"""\n    Gridworld\n  """"""\n  def __init__(self, grid):\n    # layout\n    if type(grid) == type([]): grid = makeGrid(grid)\n    self.grid = grid\n    \n    # parameters\n    self.livingReward = 0.0\n    self.noise = 0.2\n        \n  def setLivingReward(self, reward):\n    """"""\n    The (negative) reward for exiting ""normal"" states.\n    \n    Note that in the R+N text, this reward is on entering\n    a state and therefore is not clearly part of the state\'s\n    future rewards.\n    """"""\n    self.livingReward = reward\n        \n  def setNoise(self, noise):\n    """"""\n    The probability of moving in an unintended direction.\n    """"""\n    self.noise = noise\n        \n                                    \n  def getPossibleActions(self, state):\n    """"""\n    Returns list of valid actions for \'state\'.\n    \n    Note that you can request moves into walls and\n    that ""exit"" states transition to the terminal\n    state under the special action ""done"".\n    """"""\n    if state == self.grid.terminalState:\n      return ()\n    x,y = state\n    if type(self.grid[x][y]) == int:\n      return (\'exit\',)\n    return (\'north\',\'west\',\'south\',\'east\')\n    \n  def getStates(self):\n    """"""\n    Return list of all states.\n    """"""\n    # The true terminal state.\n    states = [self.grid.terminalState]\n    for x in range(self.grid.width):\n      for y in range(self.grid.height):\n        if self.grid[x][y] != \'#\':\n          state = (x,y)\n          states.append(state)\n    return states\n        \n  def getReward(self, state, action, nextState):\n    """"""\n    Get reward for state, action, nextState transition.\n    \n    Note that the reward depends only on the state being\n    departed (as in the R+N book examples, which more or\n    less use this convention).\n    """"""\n    if state == self.grid.terminalState:\n      return 0.0\n    x, y = state\n    cell = self.grid[x][y]\n    if type(cell) == int or type(cell) == float:\n      return cell\n    return self.livingReward\n        \n  def getStartState(self):\n    for x in range(self.grid.width):\n      for y in range(self.grid.height):\n        if self.grid[x][y] == \'S\':\n          return (x, y)\n    raise \'Grid has no start state\'\n    \n  def isTerminal(self, state):\n    """"""\n    Only the TERMINAL_STATE state is *actually* a terminal state.\n    The other ""exit"" states are technically non-terminals with\n    a single action ""exit"" which leads to the true terminal state.\n    This convention is to make the grids line up with the examples\n    in the R+N textbook.\n    """"""\n    return state == self.grid.terminalState\n        \n                   \n  def getTransitionStatesAndProbs(self, state, action):\n    """"""\n    Returns list of (nextState, prob) pairs\n    representing the states reachable\n    from \'state\' by taking \'action\' along\n    with their transition probabilities.          \n    """"""        \n        \n    if action not in self.getPossibleActions(state):\n      raise ""Illegal action!""\n      \n    if self.isTerminal(state):\n      return []\n    \n    x, y = state\n    \n    if type(self.grid[x][y]) == int or type(self.grid[x][y]) == float:\n      termState = self.grid.terminalState\n      return [(termState, 1.0)]\n      \n    successors = []                \n                \n    northState = (self.__isAllowed(y+1,x) and (x,y+1)) or state\n    westState = (self.__isAllowed(y,x-1) and (x-1,y)) or state\n    southState = (self.__isAllowed(y-1,x) and (x,y-1)) or state\n    eastState = (self.__isAllowed(y,x+1) and (x+1,y)) or state\n                        \n    if action == \'north\' or action == \'south\':\n      if action == \'north\': \n        successors.append((northState,1-self.noise))\n      else:\n        successors.append((southState,1-self.noise))\n                                \n      massLeft = self.noise\n      successors.append((westState,massLeft/2.0))    \n      successors.append((eastState,massLeft/2.0))\n                                \n    if action == \'west\' or action == \'east\':\n      if action == \'west\':\n        successors.append((westState,1-self.noise))\n      else:\n        successors.append((eastState,1-self.noise))\n                \n      massLeft = self.noise\n      successors.append((northState,massLeft/2.0))\n      successors.append((southState,massLeft/2.0)) \n      \n    successors = self.__aggregate(successors)\n                                                                           \n    return successors                                \n  \n  def __aggregate(self, statesAndProbs):\n    counter = util.Counter()\n    for state, prob in statesAndProbs:\n      counter[state] += prob\n    newStatesAndProbs = []\n    for state, prob in counter.items():\n      newStatesAndProbs.append((state, prob))\n    return newStatesAndProbs\n        \n  def __isAllowed(self, y, x):\n    if y < 0 or y >= self.grid.height: return False\n    if x < 0 or x >= self.grid.width: return False\n    return self.grid[x][y] != \'#\'\n\nclass GridworldEnvironment(environment.Environment):\n    \n  def __init__(self, gridWorld):\n    self.gridWorld = gridWorld\n    self.reset()\n            \n  def getCurrentState(self):\n    return self.state\n        \n  def getPossibleActions(self, state):        \n    return self.gridWorld.getPossibleActions(state)\n        \n  def doAction(self, action):\n    successors = self.gridWorld.getTransitionStatesAndProbs(self.state, action) \n    sum = 0.0\n    rand = random.random()\n    state = self.getCurrentState()\n    for nextState, prob in successors:\n      sum += prob\n      if sum > 1.0:\n        raise \'Total transition probability more than one; sample failure.\' \n      if rand < sum:\n        reward = self.gridWorld.getReward(state, action, nextState)\n        self.state = nextState\n        return (nextState, reward)\n    raise \'Total transition probability less than one; sample failure.\'    \n        \n  def reset(self):\n    self.state = self.gridWorld.getStartState()\n\nclass Grid:\n  """"""\n  A 2-dimensional array of immutables backed by a list of lists.  Data is accessed\n  via grid[x][y] where (x,y) are cartesian coordinates with x horizontal,\n  y vertical and the origin (0,0) in the bottom left corner.  \n  \n  The __str__ method constructs an output that is oriented appropriately.\n  """"""\n  def __init__(self, width, height, initialValue=\' \'):\n    self.width = width\n    self.height = height\n    self.data = [[initialValue for y in range(height)] for x in range(width)]\n    self.terminalState = \'TERMINAL_STATE\'\n    \n  def __getitem__(self, i):\n    return self.data[i]\n  \n  def __setitem__(self, key, item):\n    self.data[key] = item\n    \n  def __eq__(self, other):\n    if other == None: return False\n    return self.data == other.data\n    \n  def __hash__(self):\n    return hash(self.data)\n  \n  def copy(self):\n    g = Grid(self.width, self.height)\n    g.data = [x[:] for x in self.data]\n    return g\n  \n  def deepCopy(self):\n    return self.copy()\n  \n  def shallowCopy(self):\n    g = Grid(self.width, self.height)\n    g.data = self.data\n    return g\n    \n  def _getLegacyText(self):\n    t = [[self.data[x][y] for x in range(self.width)] for y in range(self.height)]\n    t.reverse()\n    return t\n    \n  def __str__(self):\n    return str(self._getLegacyText())\n\ndef makeGrid(gridString):\n  width, height = len(gridString[0]), len(gridString)\n  grid = Grid(width, height)\n  for ybar, line in enumerate(gridString):\n    y = height - ybar - 1\n    for x, el in enumerate(line):\n      grid[x][y] = el\n  return grid    \n             \ndef getCliffGrid():\n  grid = [[\' \',\' \',\' \',\' \',\' \'],\n          [\'S\',\' \',\' \',\' \',10],\n          [-100,-100, -100, -100, -100]]\n  return Gridworld(makeGrid(grid))\n    \ndef getCliffGrid2():\n  grid = [[\' \',\' \',\' \',\' \',\' \'],\n          [8,\'S\',\' \',\' \',10],\n          [-100,-100, -100, -100, -100]]\n  return Gridworld(grid)\n    \ndef getDiscountGrid():\n  grid = [[\' \',\' \',\' \',\' \',\' \'],\n          [\' \',\'#\',\' \',\' \',\' \'],\n          [\' \',\'#\', 1,\'#\', 10],\n          [\'S\',\' \',\' \',\' \',\' \'],\n          [-10,-10, -10, -10, -10]]\n  return Gridworld(grid)\n   \ndef getBridgeGrid():\n  grid = [[ \'#\',-100, -100, -100, -100, -100, \'#\'],\n          [   1, \'S\',  \' \',  \' \',  \' \',  \' \',  10],\n          [ \'#\',-100, -100, -100, -100, -100, \'#\']]\n  return Gridworld(grid)\n\ndef getBookGrid():\n  grid = [[\' \',\' \',\' \',+1],\n          [\' \',\'#\',\' \',-1],\n          [\'S\',\' \',\' \',\' \']]\n  return Gridworld(grid)\n\ndef getMazeGrid():\n  grid = [[\' \',\' \',\' \',+1],\n          [\'#\',\'#\',\' \',\'#\'],\n          [\' \',\'#\',\' \',\' \'],\n          [\' \',\'#\',\'#\',\' \'],\n          [\'S\',\' \',\' \',\' \']]\n  return Gridworld(grid)\n\n\n\ndef getUserAction(state, actionFunction):\n  """"""\n  Get an action from the user (rather than the agent).\n  \n  Used for debugging and lecture demos.\n  """"""\n  import graphicsUtils\n  action = None\n  while True:\n    keys = graphicsUtils.wait_for_keys()\n    if \'Up\' in keys: action = \'north\'\n    if \'Down\' in keys: action = \'south\'\n    if \'Left\' in keys: action = \'west\'\n    if \'Right\' in keys: action = \'east\'\n    if \'q\' in keys: sys.exit(0)\n    if action == None: continue\n    break\n  actions = actionFunction(state)\n  if action not in actions:\n    action = actions[0]\n  return action\n\ndef printString(x): print x\n\ndef runEpisode(agent, environment, discount, decision, display, message, pause, episode):\n  returns = 0\n  totalDiscount = 1.0\n  environment.reset()\n  if \'startEpisode\' in dir(agent): agent.startEpisode()\n  message(""BEGINNING EPISODE: ""+str(episode)+""\\n"")\n  while True:\n\n    # DISPLAY CURRENT STATE\n    state = environment.getCurrentState()\n    display(state)\n    pause()\n    \n    # END IF IN A TERMINAL STATE\n    actions = environment.getPossibleActions(state)\n    if len(actions) == 0:\n      message(""EPISODE ""+str(episode)+"" COMPLETE: RETURN WAS ""+str(returns)+""\\n"")\n      return returns\n    \n    # GET ACTION (USUALLY FROM AGENT)\n    action = decision(state)\n    if action == None:\n      raise \'Error: Agent returned None action\'\n    \n    # EXECUTE ACTION\n    nextState, reward = environment.doAction(action)\n    message(""Started in state: ""+str(state)+\n            ""\\nTook action: ""+str(action)+\n            ""\\nEnded in state: ""+str(nextState)+\n            ""\\nGot reward: ""+str(reward)+""\\n"")    \n    # UPDATE LEARNER\n    if \'observeTransition\' in dir(agent): \n        agent.observeTransition(state, action, nextState, reward)\n    \n    returns += reward * totalDiscount\n    totalDiscount *= discount\n\n  if \'stopEpisode\' in dir(agent):\n    agent.stopEpisode()\n\ndef parseOptions():\n    optParser = optparse.OptionParser()\n    optParser.add_option(\'-d\', \'--discount\',action=\'store\',\n                         type=\'float\',dest=\'discount\',default=0.9,\n                         help=\'Discount on future (default %default)\')\n    optParser.add_option(\'-r\', \'--livingReward\',action=\'store\',\n                         type=\'float\',dest=\'livingReward\',default=0.0,\n                         metavar=""R"", help=\'Reward for living for a time step (default %default)\')\n    optParser.add_option(\'-n\', \'--noise\',action=\'store\',\n                         type=\'float\',dest=\'noise\',default=0.2,\n                         metavar=""P"", help=\'How often action results in \' +\n                         \'unintended direction (default %default)\' )\n    optParser.add_option(\'-e\', \'--epsilon\',action=\'store\',\n                         type=\'float\',dest=\'epsilon\',default=0.3,\n                         metavar=""E"", help=\'Chance of taking a random action in q-learning (default %default)\')\n    optParser.add_option(\'-l\', \'--learningRate\',action=\'store\',\n                         type=\'float\',dest=\'learningRate\',default=0.5,\n                         metavar=""P"", help=\'TD learning rate (default %default)\' )\n    optParser.add_option(\'-i\', \'--iterations\',action=\'store\',\n                         type=\'int\',dest=\'iters\',default=10,\n                         metavar=""K"", help=\'Number of rounds of value iteration (default %default)\')\n    optParser.add_option(\'-k\', \'--episodes\',action=\'store\',\n                         type=\'int\',dest=\'episodes\',default=1,\n                         metavar=""K"", help=\'Number of epsiodes of the MDP to run (default %default)\')\n    optParser.add_option(\'-g\', \'--grid\',action=\'store\',\n                         metavar=""G"", type=\'string\',dest=\'grid\',default=""BookGrid"",\n                         help=\'Grid to use (case sensitive; options are BookGrid, BridgeGrid, CliffGrid, MazeGrid, default %default)\' )\n    optParser.add_option(\'-w\', \'--windowSize\', metavar=""X"", type=\'int\',dest=\'gridSize\',default=150,\n                         help=\'Request a window width of X pixels *per grid cell* (default %default)\')\n    optParser.add_option(\'-a\', \'--agent\',action=\'store\', metavar=""A"",\n                         type=\'string\',dest=\'agent\',default=""random"",\n                         help=\'Agent type (options are \\\'random\\\', \\\'value\\\' and \\\'q\\\', default %default)\')\n    optParser.add_option(\'-t\', \'--text\',action=\'store_true\',\n                         dest=\'textDisplay\',default=False,\n                         help=\'Use text-only ASCII display\')\n    optParser.add_option(\'-p\', \'--pause\',action=\'store_true\',\n                         dest=\'pause\',default=False,\n                         help=\'Pause GUI after each time step when running the MDP\')\n    optParser.add_option(\'-q\', \'--quiet\',action=\'store_true\',\n                         dest=\'quiet\',default=False,\n                         help=\'Skip display of any learning episodes\')\n    optParser.add_option(\'-s\', \'--speed\',action=\'store\', metavar=""S"", type=float,\n                         dest=\'speed\',default=1.0,\n                         help=\'Speed of animation, S > 1.0 is faster, 0.0 < S < 1.0 is slower (default %default)\')\n    optParser.add_option(\'-m\', \'--manual\',action=\'store_true\',\n                         dest=\'manual\',default=False,\n                         help=\'Manually control agent\')\n    optParser.add_option(\'-v\', \'--valueSteps\',action=\'store_true\' ,default=False,\n                         help=\'Display each step of value iteration\')\n\n    opts, args = optParser.parse_args()\n    \n    if opts.manual and opts.agent != \'q\':\n      print \'## Disabling Agents in Manual Mode (-m) ##\'\n      opts.agent = None\n\n    # MANAGE CONFLICTS\n    if opts.textDisplay or opts.quiet:\n    # if opts.quiet:      \n      opts.pause = False\n      # opts.manual = False\n      \n    if opts.manual:\n      opts.pause = True\n      \n    return opts\n\n  \nif __name__ == \'__main__\':\n  \n  opts = parseOptions()\n\n  ###########################\n  # GET THE GRIDWORLD\n  ###########################\n\n  import gridworld\n  mdpFunction = getattr(gridworld, ""get""+opts.grid)\n  mdp = mdpFunction()\n  mdp.setLivingReward(opts.livingReward)\n  mdp.setNoise(opts.noise)\n  env = gridworld.GridworldEnvironment(mdp)\n\n  \n  ###########################\n  # GET THE DISPLAY ADAPTER\n  ###########################\n\n  import textGridworldDisplay\n  display = textGridworldDisplay.TextGridworldDisplay(mdp)\n  if not opts.textDisplay:\n    import graphicsGridworldDisplay\n    display = graphicsGridworldDisplay.GraphicsGridworldDisplay(mdp, opts.gridSize, opts.speed)\n  display.start()\n\n  ###########################\n  # GET THE AGENT\n  ###########################\n\n  import valueIterationAgents, qlearningAgents\n  a = None\n  if opts.agent == \'value\':\n    a = valueIterationAgents.ValueIterationAgent(mdp, opts.discount, opts.iters)\n  elif opts.agent == \'q\':\n    #env.getPossibleActions, opts.discount, opts.learningRate, opts.epsilon\n    #simulationFn = lambda agent, state: simulation.GridworldSimulation(agent,state,mdp)\n    gridWorldEnv = GridworldEnvironment(mdp)\n    actionFn = lambda state: mdp.getPossibleActions(state)\n    qLearnOpts = {\'gamma\': opts.discount, \n                  \'alpha\': opts.learningRate, \n                  \'epsilon\': opts.epsilon,\n                  \'actionFn\': actionFn}\n    a = qlearningAgents.QLearningAgent(**qLearnOpts)\n  elif opts.agent == \'random\':\n    # # No reason to use the random agent without episodes\n    if opts.episodes == 0:\n      opts.episodes = 10\n    class RandomAgent:\n      def getAction(self, state):\n        return random.choice(mdp.getPossibleActions(state))\n      def getValue(self, state):\n        return 0.0\n      def getQValue(self, state, action):\n        return 0.0\n      def getPolicy(self, state):\n        ""NOTE: \'random\' is a special policy value; don\'t use it in your code.""\n        return \'random\'\n      def update(self, state, action, nextState, reward):\n        pass      \n    a = RandomAgent()\n  else:\n    if not opts.manual: raise \'Unknown agent type: \'+opts.agent\n    \n    \n  ###########################\n  # RUN EPISODES\n  ###########################\n  # DISPLAY Q/V VALUES BEFORE SIMULATION OF EPISODES\n  if not opts.manual and opts.agent == \'value\':\n    if opts.valueSteps:\n      for i in range(opts.iters):\n        tempAgent = valueIterationAgents.ValueIterationAgent(mdp, opts.discount, i)\n        display.displayValues(tempAgent, message = ""VALUES AFTER ""+str(i)+"" ITERATIONS"")\n        display.pause()        \n    \n    display.displayValues(a, message = ""VALUES AFTER ""+str(opts.iters)+"" ITERATIONS"")\n    display.pause()\n    display.displayQValues(a, message = ""Q-VALUES AFTER ""+str(opts.iters)+"" ITERATIONS"")\n    display.pause()\n    \n  \n\n  # FIGURE OUT WHAT TO DISPLAY EACH TIME STEP (IF ANYTHING)\n  displayCallback = lambda x: None\n  if not opts.quiet:\n    if opts.manual and opts.agent == None: \n      displayCallback = lambda state: display.displayNullValues(state)\n    else:\n      if opts.agent == \'random\': displayCallback = lambda state: display.displayValues(a, state, ""CURRENT VALUES"")\n      if opts.agent == \'value\': displayCallback = lambda state: display.displayValues(a, state, ""CURRENT VALUES"")\n      if opts.agent == \'q\': displayCallback = lambda state: display.displayQValues(a, state, ""CURRENT Q-VALUES"")\n\n  messageCallback = lambda x: printString(x)\n  if opts.quiet:\n    messageCallback = lambda x: None\n\n  # FIGURE OUT WHETHER TO WAIT FOR A KEY PRESS AFTER EACH TIME STEP\n  pauseCallback = lambda : None\n  if opts.pause:\n    pauseCallback = lambda : display.pause()\n\n  # FIGURE OUT WHETHER THE USER WANTS MANUAL CONTROL (FOR DEBUGGING AND DEMOS)  \n  if opts.manual:\n    decisionCallback = lambda state : getUserAction(state, mdp.getPossibleActions)\n  else:\n    decisionCallback = a.getAction  \n    \n  # RUN EPISODES\n  if opts.episodes > 0:\n    print\n    print ""RUNNING"", opts.episodes, ""EPISODES""\n    print\n  returns = 0\n  for episode in range(1, opts.episodes+1):\n    returns += runEpisode(a, env, opts.discount, decisionCallback, displayCallback, messageCallback, pauseCallback, episode)\n  if opts.episodes > 0:\n    print\n    print ""AVERAGE RETURNS FROM START STATE: ""+str((returns+0.0) / opts.episodes)\n    print\n    print\n    \n  # DISPLAY POST-LEARNING VALUES / Q-VALUES\n  if opts.agent == \'q\' and not opts.manual:\n    display.displayQValues(a, message = ""Q-VALUES AFTER ""+str(opts.episodes)+"" EPISODES"")\n    display.pause()\n    display.displayValues(a, message = ""VALUES AFTER ""+str(opts.episodes)+"" EPISODES"")\n    display.pause()\n    \n   '"
week03_model_free/crawler_and_pacman/seminar_py2/keyboardAgents.py,0,"b'# keyboardAgents.py\n# -----------------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nfrom game import Agent\nfrom game import Directions\nimport random\n\nclass KeyboardAgent(Agent):\n  """"""\n  An agent controlled by the keyboard.\n  """"""\n  # NOTE: Arrow keys also work.\n  WEST_KEY  = \'a\' \n  EAST_KEY  = \'d\' \n  NORTH_KEY = \'w\' \n  SOUTH_KEY = \'s\'\n  STOP_KEY = \'q\'\n\n  def __init__( self, index = 0 ):\n    \n    self.lastMove = Directions.STOP\n    self.index = index\n    self.keys = []\n    \n  def getAction( self, state):\n    from graphicsUtils import keys_waiting\n    from graphicsUtils import keys_pressed\n    keys = keys_waiting() + keys_pressed()\n    if keys != []:\n      self.keys = keys\n    \n    legal = state.getLegalActions(self.index)\n    move = self.getMove(legal)\n    \n    if move == Directions.STOP:\n      # Try to move in the same direction as before\n      if self.lastMove in legal:\n        move = self.lastMove\n    \n    if (self.STOP_KEY in self.keys) and Directions.STOP in legal: move = Directions.STOP\n\n    if move not in legal:\n      move = random.choice(legal)\n      \n    self.lastMove = move\n    return move\n\n  def getMove(self, legal):\n    move = Directions.STOP\n    if   (self.WEST_KEY in self.keys or \'Left\' in self.keys) and Directions.WEST in legal:  move = Directions.WEST\n    if   (self.EAST_KEY in self.keys or \'Right\' in self.keys) and Directions.EAST in legal: move = Directions.EAST\n    if   (self.NORTH_KEY in self.keys or \'Up\' in self.keys) and Directions.NORTH in legal:   move = Directions.NORTH\n    if   (self.SOUTH_KEY in self.keys or \'Down\' in self.keys) and Directions.SOUTH in legal: move = Directions.SOUTH\n    return move\n  \nclass KeyboardAgent2(KeyboardAgent):\n  """"""\n  A second agent controlled by the keyboard.\n  """"""\n  # NOTE: Arrow keys also work.\n  WEST_KEY  = \'j\' \n  EAST_KEY  = ""l"" \n  NORTH_KEY = \'i\' \n  SOUTH_KEY = \'k\'\n  STOP_KEY = \'u\'\n\n  def getMove(self, legal):\n    move = Directions.STOP\n    if   (self.WEST_KEY in self.keys) and Directions.WEST in legal:  move = Directions.WEST\n    if   (self.EAST_KEY in self.keys) and Directions.EAST in legal: move = Directions.EAST\n    if   (self.NORTH_KEY in self.keys) and Directions.NORTH in legal:   move = Directions.NORTH\n    if   (self.SOUTH_KEY in self.keys) and Directions.SOUTH in legal: move = Directions.SOUTH\n    return move\n  \n  \n'"
week03_model_free/crawler_and_pacman/seminar_py2/layout.py,0,"b'# layout.py\n# ---------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nfrom util import manhattanDistance\nfrom game import Grid\nimport os\nimport random\n\nVISIBILITY_MATRIX_CACHE = {}\n\nclass Layout:\n  """"""\n  A Layout manages the static information about the game board.\n  """"""\n  \n  def __init__(self, layoutText):\n    self.width = len(layoutText[0])\n    self.height= len(layoutText)\n    self.walls = Grid(self.width, self.height, False)\n    self.food = Grid(self.width, self.height, False)\n    self.capsules = []\n    self.agentPositions = []\n    self.numGhosts = 0\n    self.processLayoutText(layoutText)\n    self.layoutText = layoutText\n    # self.initializeVisibilityMatrix()\n    \n  def getNumGhosts(self):\n    return self.numGhosts\n    \n  def initializeVisibilityMatrix(self):\n    global VISIBILITY_MATRIX_CACHE\n    if reduce(str.__add__, self.layoutText) not in VISIBILITY_MATRIX_CACHE:\n      from game import Directions\n      vecs = [(-0.5,0), (0.5,0),(0,-0.5),(0,0.5)]\n      dirs = [Directions.NORTH, Directions.SOUTH, Directions.WEST, Directions.EAST]\n      vis = Grid(self.width, self.height, {Directions.NORTH:set(), Directions.SOUTH:set(), Directions.EAST:set(), Directions.WEST:set(), Directions.STOP:set()})\n      for x in range(self.width):\n        for y in range(self.height):\n          if self.walls[x][y] == False:\n            for vec, direction in zip(vecs, dirs):\n              dx, dy = vec\n              nextx, nexty = x + dx, y + dy\n              while (nextx + nexty) != int(nextx) + int(nexty) or not self.walls[int(nextx)][int(nexty)] :\n                vis[x][y][direction].add((nextx, nexty))\n                nextx, nexty = x + dx, y + dy\n      self.visibility = vis      \n      VISIBILITY_MATRIX_CACHE[reduce(str.__add__, self.layoutText)] = vis\n    else:\n      self.visibility = VISIBILITY_MATRIX_CACHE[reduce(str.__add__, self.layoutText)]\n      \n  def isWall(self, pos):\n    x, col = pos\n    return self.walls[x][col]\n  \n  def getRandomLegalPosition(self):\n    x = random.choice(range(self.width))\n    y = random.choice(range(self.height))\n    while self.isWall( (x, y) ):\n      x = random.choice(range(self.width))\n      y = random.choice(range(self.height))\n    return (x,y)\n\n  def getRandomCorner(self):\n    poses = [(1,1), (1, self.height - 2), (self.width - 2, 1), (self.width - 2, self.height - 2)]\n    return random.choice(poses)\n\n  def getFurthestCorner(self, pacPos):\n    poses = [(1,1), (1, self.height - 2), (self.width - 2, 1), (self.width - 2, self.height - 2)]\n    dist, pos = max([(manhattanDistance(p, pacPos), p) for p in poses])\n    return pos\n  \n  def isVisibleFrom(self, ghostPos, pacPos, pacDirection):\n    row, col = [int(x) for x in pacPos]\n    return ghostPos in self.visibility[row][col][pacDirection]\n  \n  def __str__(self):\n    return ""\\n"".join(self.layoutText)\n    \n  def deepCopy(self):\n    return Layout(self.layoutText[:])\n    \n  def processLayoutText(self, layoutText):\n    """"""\n    Coordinates are flipped from the input format to the (x,y) convention here\n    \n    The shape of the maze.  Each character  \n    represents a different type of object.   \n     % - Wall                               \n     . - Food\n     o - Capsule\n     G - Ghost\n     P - Pacman\n    Other characters are ignored.\n    """"""\n    maxY = self.height - 1\n    for y in range(self.height):       \n      for x in range(self.width):\n        layoutChar = layoutText[maxY - y][x]  \n        self.processLayoutChar(x, y, layoutChar)\n    self.agentPositions.sort()\n    self.agentPositions = [ ( i == 0, pos) for i, pos in self.agentPositions]\n  \n  def processLayoutChar(self, x, y, layoutChar):\n    if layoutChar == \'%\':      \n      self.walls[x][y] = True\n    elif layoutChar == \'.\':\n      self.food[x][y] = True \n    elif layoutChar == \'o\':    \n      self.capsules.append((x, y))   \n    elif layoutChar == \'P\':    \n      self.agentPositions.append( (0, (x, y) ) )\n    elif layoutChar in [\'G\']:    \n      self.agentPositions.append( (1, (x, y) ) )\n      self.numGhosts += 1\n    elif layoutChar in  [\'1\', \'2\', \'3\', \'4\']:\n      self.agentPositions.append( (int(layoutChar), (x,y)))\n      self.numGhosts += 1 \ndef getLayout(name, back = 2):\n  if name.endswith(\'.lay\'):\n    layout = tryToLoad(\'layouts/\' + name)\n    if layout == None: layout = tryToLoad(name)\n  else:\n    layout = tryToLoad(\'layouts/\' + name + \'.lay\')\n    if layout == None: layout = tryToLoad(name + \'.lay\')\n  if layout == None and back >= 0:\n    curdir = os.path.abspath(\'.\')\n    os.chdir(\'..\')\n    layout = getLayout(name, back -1)\n    os.chdir(curdir)\n  return layout\n\ndef tryToLoad(fullname):\n  if(not os.path.exists(fullname)): return None\n  f = open(fullname)\n  try: return Layout([line.strip() for line in f])\n  finally: f.close()'"
week03_model_free/crawler_and_pacman/seminar_py2/learningAgents.py,0,"b'# learningAgents.py\n# -----------------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nfrom game import Directions, Agent, Actions\n\nimport random,util,time\n\nclass ValueEstimationAgent(Agent):\n  """"""\n    Abstract agent which assigns values to (state,action)\n    Q-Values for an environment. As well as a value to a\n    state and a policy given respectively by,\n\n    V(s) = max_{a in actions} Q(s,a)\n    policy(s) = arg_max_{a in actions} Q(s,a)\n\n    Both ValueIterationAgent and QLearningAgent inherit\n    from this agent. While a ValueIterationAgent has\n    a model of the environment via a MarkovDecisionProcess\n    (see mdp.py) that is used to estimate Q-Values before\n    ever actually acting, the QLearningAgent estimates\n    Q-Values while acting in the environment.\n  """"""\n\n  def __init__(self, alpha=1.0, epsilon=0.05, gamma=0.8, numTraining = 10):\n    """"""\n    Sets options, which can be passed in via the Pacman command line using -a alpha=0.5,...\n    alpha    - learning rate\n    epsilon  - exploration rate\n    gamma    - discount factor\n    numTraining - number of training episodes, i.e. no learning after these many episodes\n    """"""\n    self.alpha = float(alpha)\n    self.epsilon = float(epsilon)\n    self.discount = float(gamma)\n    self.numTraining = int(numTraining)\n\n  ####################################\n  #    Override These Functions      #\n  ####################################\n  def getQValue(self, state, action):\n    """"""\n    Should return Q(state,action)\n    """"""\n    util.raiseNotDefined()\n\n  def getValue(self, state):\n    """"""\n    What is the value of this state under the best action?\n    Concretely, this is given by\n\n    V(s) = max_{a in actions} Q(s,a)\n    """"""\n    util.raiseNotDefined()\n\n  def getPolicy(self, state):\n    """"""\n    What is the best action to take in the state. Note that because\n    we might want to explore, this might not coincide with getAction\n    Concretely, this is given by\n\n    policy(s) = arg_max_{a in actions} Q(s,a)\n\n    If many actions achieve the maximal Q-value,\n    it doesn\'t matter which is selected.\n    """"""\n    util.raiseNotDefined()\n\n  def getAction(self, state):\n    """"""\n    state: can call state.getLegalActions()\n    Choose an action and return it.\n    """"""\n    util.raiseNotDefined()\n\nclass ReinforcementAgent(ValueEstimationAgent):\n  """"""\n    Abstract Reinforcemnt Agent: A ValueEstimationAgent\n\t  which estimates Q-Values (as well as policies) from experience\n\t  rather than a model\n\n      What you need to know:\n\t\t  - The environment will call\n\t\t    observeTransition(state,action,nextState,deltaReward),\n\t\t    which will call update(state, action, nextState, deltaReward)\n\t\t    which you should override.\n      - Use self.getLegalActions(state) to know which actions\n\t\t    are available in a state\n  """"""\n  ####################################\n  #    Override These Functions      #\n  ####################################\n\n  def update(self, state, action, nextState, reward):\n    """"""\n\t    This class will call this function, which you write, after\n\t    observing a transition and reward\n    """"""\n    util.raiseNotDefined()\n\n  ####################################\n  #    Read These Functions          #\n  ####################################\n\n  def getLegalActions(self,state):\n    """"""\n      Get the actions available for a given\n      state. This is what you should use to\n      obtain legal actions for a state\n    """"""\n    return self.actionFn(state)\n\n  def observeTransition(self, state,action,nextState,deltaReward):\n    """"""\n    \tCalled by environment to inform agent that a transition has\n    \tbeen observed. This will result in a call to self.update\n    \ton the same arguments\n\n    \tNOTE: Do *not* override or call this function\n    """"""\n    self.episodeRewards += deltaReward\n    self.update(state,action,nextState,deltaReward)\n\n  def startEpisode(self):\n    """"""\n      Called by environment when new episode is starting\n    """"""\n    self.lastState = None\n    self.lastAction = None\n    self.episodeRewards = 0.0\n\n  def stopEpisode(self):\n    """"""\n      Called by environment when episode is done\n    """"""\n    if self.episodesSoFar < self.numTraining:\n\t\t  self.accumTrainRewards += self.episodeRewards\n    else:\n\t\t  self.accumTestRewards += self.episodeRewards\n    self.episodesSoFar += 1\n    if self.episodesSoFar >= self.numTraining:\n      # Take off the training wheels\n      self.epsilon = 0.0    # no exploration\n      self.alpha = 0.0      # no learning\n\n  def isInTraining(self):\n      return self.episodesSoFar < self.numTraining\n\n  def isInTesting(self):\n      return not self.isInTraining()\n\n  def __init__(self, actionFn = None, numTraining=100, epsilon=0.5, alpha=0.5, gamma=1):\n    """"""\n    actionFn: Function which takes a state and returns the list of legal actions\n\n    alpha    - learning rate\n    epsilon  - exploration rate\n    gamma    - discount factor\n    numTraining - number of training episodes, i.e. no learning after these many episodes\n    """"""\n    if actionFn == None:\n        actionFn = lambda state: state.getLegalActions()\n    self.actionFn = actionFn\n    self.episodesSoFar = 0\n    self.accumTrainRewards = 0.0\n    self.accumTestRewards = 0.0\n    self.numTraining = int(numTraining)\n    self.epsilon = float(epsilon)\n    self.alpha = float(alpha)\n    self.discount = float(gamma)\n\n  ################################\n  # Controls needed for Crawler  #\n  ################################\n  def setEpsilon(self, epsilon):\n    self.epsilon = epsilon\n\n  def setLearningRate(self, alpha):\n    self.alpha = alpha\n\n  def setDiscount(self, discount):\n    self.discount = discount\n\n  def doAction(self,state,action):\n    """"""\n        Called by inherited class when\n        an action is taken in a state\n    """"""\n    self.lastState = state\n    self.lastAction = action\n\n  ###################\n  # Pacman Specific #\n  ###################\n  def observationFunction(self, state):\n    """"""\n        This is where we ended up after our last action.\n        The simulation should somehow ensure this is called\n    """"""\n    if not self.lastState is None:\n        reward = state.getScore() - self.lastState.getScore()\n        self.observeTransition(self.lastState, self.lastAction, state, reward)\n    return state\n\n  def registerInitialState(self, state):\n    self.startEpisode()\n    if self.episodesSoFar == 0:\n        print \'Beginning %d episodes of Training\' % (self.numTraining)\n\n  def final(self, state):\n    """"""\n      Called by Pacman game at the terminal state\n    """"""\n    deltaReward = state.getScore() - self.lastState.getScore()\n    self.observeTransition(self.lastState, self.lastAction, state, deltaReward)\n    self.stopEpisode()\n\n    # Make sure we have this var\n    if not \'episodeStartTime\' in self.__dict__:\n        self.episodeStartTime = time.time()\n    if not \'lastWindowAccumRewards\' in self.__dict__:\n        self.lastWindowAccumRewards = 0.0\n    self.lastWindowAccumRewards += state.getScore()\n\n    NUM_EPS_UPDATE = 100\n    if self.episodesSoFar % NUM_EPS_UPDATE == 0:\n        print \'Reinforcement Learning Status:\'\n        windowAvg = self.lastWindowAccumRewards / float(NUM_EPS_UPDATE)\n        if self.episodesSoFar <= self.numTraining:\n            trainAvg = self.accumTrainRewards / float(self.episodesSoFar)\n            print \'\\tCompleted %d out of %d training episodes\' % (\n                   self.episodesSoFar,self.numTraining)\n            print \'\\tAverage Rewards over all training: %.2f\' % (\n                    trainAvg)\n        else:\n            testAvg = float(self.accumTestRewards) / (self.episodesSoFar - self.numTraining)\n            print \'\\tCompleted %d test episodes\' % (self.episodesSoFar - self.numTraining)\n            print \'\\tAverage Rewards over testing: %.2f\' % testAvg\n        print \'\\tAverage Rewards for last %d episodes: %.2f\'  % (\n                NUM_EPS_UPDATE,windowAvg)\n        print \'\\tEpisode took %.2f seconds\' % (time.time() - self.episodeStartTime)\n        self.lastWindowAccumRewards = 0.0\n        self.episodeStartTime = time.time()\n\n    if self.episodesSoFar == self.numTraining:\n        msg = \'Training Done (turning off epsilon and alpha)\'\n        print \'%s\\n%s\' % (msg,\'-\' * len(msg))\n'"
week03_model_free/crawler_and_pacman/seminar_py2/mdp.py,0,"b'# mdp.py\n# ------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nimport random\n\nclass MarkovDecisionProcess:\n    \n  def getStates(self):\n    """"""\n    Return a list of all states in the MDP.\n    Not generally possible for large MDPs.\n    """"""\n    abstract\n        \n  def getStartState(self):\n    """"""\n    Return the start state of the MDP.\n    """"""\n    abstract\n    \n  def getPossibleActions(self, state):\n    """"""\n    Return list of possible actions from \'state\'.\n    """"""\n    abstract\n        \n  def getTransitionStatesAndProbs(self, state, action):\n    """"""\n    Returns list of (nextState, prob) pairs\n    representing the states reachable\n    from \'state\' by taking \'action\' along\n    with their transition probabilities.  \n    \n    Note that in Q-Learning and reinforcment\n    learning in general, we do not know these\n    probabilities nor do we directly model them.\n    """"""\n    abstract\n        \n  def getReward(self, state, action, nextState):\n    """"""\n    Get the reward for the state, action, nextState transition.\n    \n    Not available in reinforcement learning.\n    """"""\n    abstract\n\n  def isTerminal(self, state):\n    """"""\n    Returns true if the current state is a terminal state.  By convention,\n    a terminal state has zero future rewards.  Sometimes the terminal state(s)\n    may have no possible actions.  It is also common to think of the terminal\n    state as having a self-loop action \'pass\' with zero reward; the formulations\n    are equivalent.\n    """"""\n    abstract\n\n    \n'"
week03_model_free/crawler_and_pacman/seminar_py2/pacman.py,0,"b'# pacman.py\n# ---------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\n""""""\nPacman.py holds the logic for the classic pacman game along with the main\ncode to run a game.  This file is divided into three sections:\n\n  (i)  Your interface to the pacman world:\n          Pacman is a complex environment.  You probably don\'t want to\n          read through all of the code we wrote to make the game runs\n          correctly.  This section contains the parts of the code\n          that you will need to understand in order to complete the\n          project.  There is also some code in game.py that you should\n          understand.\n\n  (ii)  The hidden secrets of pacman:\n          This section contains all of the logic code that the pacman\n          environment uses to decide who can move where, who dies when\n          things collide, etc.  You shouldn\'t need to read this section\n          of code, but you can if you want.\n\n  (iii) Framework to start a game:\n          The final section contains the code for reading the command\n          you use to set up the game, then starting up a new game, along with\n          linking in all the external parts (agent functions, graphics).\n          Check this section out to see all the options available to you.\n\nTo play your first game, type \'python pacman.py\' from the command line.\nThe keys are \'a\', \'s\', \'d\', and \'w\' to move (or arrow keys).  Have fun!\n""""""\nfrom game import GameStateData\nfrom game import Game\nfrom game import Directions\nfrom game import Actions\nfrom util import nearestPoint\nfrom util import manhattanDistance\nimport util, layout\nimport sys, types, time, random, os\n\n###################################################\n# YOUR INTERFACE TO THE PACMAN WORLD: A GameState #\n###################################################\n\nclass GameState:\n  """"""\n  A GameState specifies the full game state, including the food, capsules,\n  agent configurations and score changes.\n\n  GameStates are used by the Game object to capture the actual state of the game and\n  can be used by agents to reason about the game.\n\n  Much of the information in a GameState is stored in a GameStateData object.  We\n  strongly suggest that you access that data via the accessor methods below rather\n  than referring to the GameStateData object directly.\n\n  Note that in classic Pacman, Pacman is always agent 0.\n  """"""\n\n  ####################################################\n  # Accessor methods: use these to access state data #\n  ####################################################\n\n  def getLegalActions( self, agentIndex=0 ):\n    """"""\n    Returns the legal actions for the agent specified.\n    """"""\n    if self.isWin() or self.isLose(): return []\n\n    if agentIndex == 0:  # Pacman is moving\n      return PacmanRules.getLegalActions( self )\n    else:\n      return GhostRules.getLegalActions( self, agentIndex )\n\n  def generateSuccessor( self, agentIndex, action):\n    """"""\n    Returns the successor state after the specified agent takes the action.\n    """"""\n    # Check that successors exist\n    if self.isWin() or self.isLose(): raise Exception(\'Can\\\'t generate a successor of a terminal state.\')\n\n    # Copy current state\n    state = GameState(self)\n\n    # Let agent\'s logic deal with its action\'s effects on the board\n    if agentIndex == 0:  # Pacman is moving\n      state.data._eaten = [False for i in range(state.getNumAgents())]\n      PacmanRules.applyAction( state, action )\n    else:                # A ghost is moving\n      GhostRules.applyAction( state, action, agentIndex )\n\n    # Time passes\n    if agentIndex == 0:\n      state.data.scoreChange += -TIME_PENALTY # Penalty for waiting around\n    else:\n      GhostRules.decrementTimer( state.data.agentStates[agentIndex] )\n\n    # Resolve multi-agent effects\n    GhostRules.checkDeath( state, agentIndex )\n\n    # Book keeping\n    state.data._agentMoved = agentIndex\n    state.data.score += state.data.scoreChange\n    return state\n\n  def getLegalPacmanActions( self ):\n    return self.getLegalActions( 0 )\n\n  def generatePacmanSuccessor( self, action ):\n    """"""\n    Generates the successor state after the specified pacman move\n    """"""\n    return self.generateSuccessor( 0, action )\n\n  def getPacmanState( self ):\n    """"""\n    Returns an AgentState object for pacman (in game.py)\n\n    state.pos gives the current position\n    state.direction gives the travel vector\n    """"""\n    return self.data.agentStates[0].copy()\n\n  def getPacmanPosition( self ):\n    return self.data.agentStates[0].getPosition()\n\n  def getGhostStates( self ):\n    return self.data.agentStates[1:]\n\n  def getGhostState( self, agentIndex ):\n    if agentIndex == 0 or agentIndex >= self.getNumAgents():\n      raise Exception(""Invalid index passed to getGhostState"")\n    return self.data.agentStates[agentIndex]\n\n  def getGhostPosition( self, agentIndex ):\n    if agentIndex == 0:\n      raise Exception(""Pacman\'s index passed to getGhostPosition"")\n    return self.data.agentStates[agentIndex].getPosition()\n\n  def getGhostPositions(self):\n    return [s.getPosition() for s in self.getGhostStates()]\n\n  def getNumAgents( self ):\n    return len( self.data.agentStates )\n\n  def getScore( self ):\n    return self.data.score\n\n  def getCapsules(self):\n    """"""\n    Returns a list of positions (x,y) of the remaining capsules.\n    """"""\n    return self.data.capsules\n\n  def getNumFood( self ):\n    return self.data.food.count()\n\n  def getFood(self):\n    """"""\n    Returns a Grid of boolean food indicator variables.\n\n    Grids can be accessed via list notation, so to check\n    if there is food at (x,y), just call\n\n    currentFood = state.getFood()\n    if currentFood[x][y] == True: ...\n    """"""\n    return self.data.food\n\n  def getWalls(self):\n    """"""\n    Returns a Grid of boolean wall indicator variables.\n\n    Grids can be accessed via list notation, so to check\n    if there is food at (x,y), just call\n\n    walls = state.getWalls()\n    if walls[x][y] == True: ...\n    """"""\n    return self.data.layout.walls\n\n  def hasFood(self, x, y):\n    return self.data.food[x][y]\n\n  def hasWall(self, x, y):\n    return self.data.layout.walls[x][y]\n\n  def isLose( self ):\n    return self.data._lose\n\n  def isWin( self ):\n    return self.data._win\n\n  #############################################\n  #             Helper methods:               #\n  # You shouldn\'t need to call these directly #\n  #############################################\n\n  def __init__( self, prevState = None ):\n    """"""\n    Generates a new state by copying information from its predecessor.\n    """"""\n    if prevState != None: # Initial state\n      self.data = GameStateData(prevState.data)\n    else:\n      self.data = GameStateData()\n\n  def deepCopy( self ):\n    state = GameState( self )\n    state.data = self.data.deepCopy()\n    return state\n\n  def __eq__( self, other ):\n    """"""\n    Allows two states to be compared.\n    """"""\n    return self.data == other.data\n\n  def __hash__( self ):\n    """"""\n    Allows states to be keys of dictionaries.\n    """"""\n    return hash( self.data )\n\n  def __str__( self ):\n\n    return str(self.data)\n\n  def initialize( self, layout, numGhostAgents=1000 ):\n    """"""\n    Creates an initial game state from a layout array (see layout.py).\n    """"""\n    self.data.initialize(layout, numGhostAgents)\n\n############################################################################\n#                     THE HIDDEN SECRETS OF PACMAN                         #\n#                                                                          #\n# You shouldn\'t need to look through the code in this section of the file. #\n############################################################################\n\nSCARED_TIME = 40    # Moves ghosts are scared\nCOLLISION_TOLERANCE = 0.7 # How close ghosts must be to Pacman to kill\nTIME_PENALTY = 1 # Number of points lost each round\n\nclass ClassicGameRules:\n  """"""\n  These game rules manage the control flow of a game, deciding when\n  and how the game starts and ends.\n  """"""\n  def __init__(self, timeout=30):\n    self.timeout = timeout\n\n  def newGame( self, layout, pacmanAgent, ghostAgents, display, quiet = False, catchExceptions=False):\n    agents = [pacmanAgent] + ghostAgents[:layout.getNumGhosts()]\n    initState = GameState()\n    initState.initialize( layout, len(ghostAgents) )\n    game = Game(agents, display, self, catchExceptions=catchExceptions)\n    game.state = initState\n    self.initialState = initState.deepCopy()\n    self.quiet = quiet\n    return game\n\n  def process(self, state, game):\n    """"""\n    Checks to see whether it is time to end the game.\n    """"""\n    if state.isWin(): self.win(state, game)\n    if state.isLose(): self.lose(state, game)\n\n  def win( self, state, game ):\n    if not self.quiet: print ""Pacman emerges victorious! Score: %d"" % state.data.score\n    game.gameOver = True\n\n  def lose( self, state, game ):\n    if not self.quiet: print ""Pacman died! Score: %d"" % state.data.score\n    game.gameOver = True\n\n  def getProgress(self, game):\n    return float(game.state.getNumFood()) / self.initialState.getNumFood()\n\n  def agentCrash(self, game, agentIndex):\n    if agentIndex == 0:\n      print ""Pacman crashed""\n    else:\n      print ""A ghost crashed""\n\n  def getMaxTotalTime(self, agentIndex):\n    return self.timeout\n\n  def getMaxStartupTime(self, agentIndex):\n    return self.timeout\n\n  def getMoveWarningTime(self, agentIndex):\n    return self.timeout\n\n  def getMoveTimeout(self, agentIndex):\n    return self.timeout\n\n  def getMaxTimeWarnings(self, agentIndex):\n    return 0\n\nclass PacmanRules:\n  """"""\n  These functions govern how pacman interacts with his environment under\n  the classic game rules.\n  """"""\n  PACMAN_SPEED=1\n\n  def getLegalActions( state ):\n    """"""\n    Returns a list of possible actions.\n    """"""\n    return Actions.getPossibleActions( state.getPacmanState().configuration, state.data.layout.walls )\n  getLegalActions = staticmethod( getLegalActions )\n\n  def applyAction( state, action ):\n    """"""\n    Edits the state to reflect the results of the action.\n    """"""\n    legal = PacmanRules.getLegalActions( state )\n    if action not in legal:\n      raise Exception(""Illegal action "" + str(action))\n\n    pacmanState = state.data.agentStates[0]\n\n    # Update Configuration\n    vector = Actions.directionToVector( action, PacmanRules.PACMAN_SPEED )\n    pacmanState.configuration = pacmanState.configuration.generateSuccessor( vector )\n\n    # Eat\n    next = pacmanState.configuration.getPosition()\n    nearest = nearestPoint( next )\n    if manhattanDistance( nearest, next ) <= 0.5 :\n      # Remove food\n      PacmanRules.consume( nearest, state )\n  applyAction = staticmethod( applyAction )\n\n  def consume( position, state ):\n    x,y = position\n    # Eat food\n    if state.data.food[x][y]:\n      state.data.scoreChange += 10\n      state.data.food = state.data.food.copy()\n      state.data.food[x][y] = False\n      state.data._foodEaten = position\n      # TODO: cache numFood?\n      numFood = state.getNumFood()\n      if numFood == 0 and not state.data._lose:\n        state.data.scoreChange += 500\n        state.data._win = True\n    # Eat capsule\n    if( position in state.getCapsules() ):\n      state.data.capsules.remove( position )\n      state.data._capsuleEaten = position\n      # Reset all ghosts\' scared timers\n      for index in range( 1, len( state.data.agentStates ) ):\n        state.data.agentStates[index].scaredTimer = SCARED_TIME\n  consume = staticmethod( consume )\n\nclass GhostRules:\n  """"""\n  These functions dictate how ghosts interact with their environment.\n  """"""\n  GHOST_SPEED=1.0\n  def getLegalActions( state, ghostIndex ):\n    """"""\n    Ghosts cannot stop, and cannot turn around unless they\n    reach a dead end, but can turn 90 degrees at intersections.\n    """"""\n    conf = state.getGhostState( ghostIndex ).configuration\n    possibleActions = Actions.getPossibleActions( conf, state.data.layout.walls )\n    reverse = Actions.reverseDirection( conf.direction )\n    if Directions.STOP in possibleActions:\n      possibleActions.remove( Directions.STOP )\n    if reverse in possibleActions and len( possibleActions ) > 1:\n      possibleActions.remove( reverse )\n    return possibleActions\n  getLegalActions = staticmethod( getLegalActions )\n\n  def applyAction( state, action, ghostIndex):\n\n    legal = GhostRules.getLegalActions( state, ghostIndex )\n    if action not in legal:\n      raise Exception(""Illegal ghost action "" + str(action))\n\n    ghostState = state.data.agentStates[ghostIndex]\n    speed = GhostRules.GHOST_SPEED\n    if ghostState.scaredTimer > 0: speed /= 2.0\n    vector = Actions.directionToVector( action, speed )\n    ghostState.configuration = ghostState.configuration.generateSuccessor( vector )\n  applyAction = staticmethod( applyAction )\n\n  def decrementTimer( ghostState):\n    timer = ghostState.scaredTimer\n    if timer == 1:\n      ghostState.configuration.pos = nearestPoint( ghostState.configuration.pos )\n    ghostState.scaredTimer = max( 0, timer - 1 )\n  decrementTimer = staticmethod( decrementTimer )\n\n  def checkDeath( state, agentIndex):\n    pacmanPosition = state.getPacmanPosition()\n    if agentIndex == 0: # Pacman just moved; Anyone can kill him\n      for index in range( 1, len( state.data.agentStates ) ):\n        ghostState = state.data.agentStates[index]\n        ghostPosition = ghostState.configuration.getPosition()\n        if GhostRules.canKill( pacmanPosition, ghostPosition ):\n          GhostRules.collide( state, ghostState, index )\n    else:\n      ghostState = state.data.agentStates[agentIndex]\n      ghostPosition = ghostState.configuration.getPosition()\n      if GhostRules.canKill( pacmanPosition, ghostPosition ):\n        GhostRules.collide( state, ghostState, agentIndex )\n  checkDeath = staticmethod( checkDeath )\n\n  def collide( state, ghostState, agentIndex):\n    if ghostState.scaredTimer > 0:\n      state.data.scoreChange += 200\n      GhostRules.placeGhost(state, ghostState)\n      ghostState.scaredTimer = 0\n      # Added for first-person\n      state.data._eaten[agentIndex] = True\n    else:\n      if not state.data._win:\n        state.data.scoreChange -= 500\n        state.data._lose = True\n  collide = staticmethod( collide )\n\n  def canKill( pacmanPosition, ghostPosition ):\n    return manhattanDistance( ghostPosition, pacmanPosition ) <= COLLISION_TOLERANCE\n  canKill = staticmethod( canKill )\n\n  def placeGhost(state, ghostState):\n    ghostState.configuration = ghostState.start\n  placeGhost = staticmethod( placeGhost )\n\n#############################\n# FRAMEWORK TO START A GAME #\n#############################\n\ndef default(str):\n  return str + \' [Default: %default]\'\n\ndef parseAgentArgs(str):\n  if str == None: return {}\n  pieces = str.split(\',\')\n  opts = {}\n  for p in pieces:\n    if \'=\' in p:\n      key, val = p.split(\'=\')\n    else:\n      key,val = p, 1\n    opts[key] = val\n  return opts\n\ndef readCommand( argv ):\n  """"""\n  Processes the command used to run pacman from the command line.\n  """"""\n  from optparse import OptionParser\n  usageStr = """"""\n  USAGE:      python pacman.py <options>\n  EXAMPLES:   (1) python pacman.py\n                  - starts an interactive game\n              (2) python pacman.py --layout smallClassic --zoom 2\n              OR  python pacman.py -l smallClassic -z 2\n                  - starts an interactive game on a smaller board, zoomed in\n  """"""\n  parser = OptionParser(usageStr)\n\n  parser.add_option(\'-n\', \'--numGames\', dest=\'numGames\', type=\'int\',\n                    help=default(\'the number of GAMES to play\'), metavar=\'GAMES\', default=1)\n  parser.add_option(\'-l\', \'--layout\', dest=\'layout\',\n                    help=default(\'the LAYOUT_FILE from which to load the map layout\'),\n                    metavar=\'LAYOUT_FILE\', default=\'mediumClassic\')\n  parser.add_option(\'-p\', \'--pacman\', dest=\'pacman\',\n                    help=default(\'the agent TYPE in the pacmanAgents module to use\'),\n                    metavar=\'TYPE\', default=\'KeyboardAgent\')\n  parser.add_option(\'-t\', \'--textGraphics\', action=\'store_true\', dest=\'textGraphics\',\n                    help=\'Display output as text only\', default=False)\n  parser.add_option(\'-q\', \'--quietTextGraphics\', action=\'store_true\', dest=\'quietGraphics\',\n                    help=\'Generate minimal output and no graphics\', default=False)\n  parser.add_option(\'-g\', \'--ghosts\', dest=\'ghost\',\n                    help=default(\'the ghost agent TYPE in the ghostAgents module to use\'),\n                    metavar = \'TYPE\', default=\'RandomGhost\')\n  parser.add_option(\'-k\', \'--numghosts\', type=\'int\', dest=\'numGhosts\',\n                    help=default(\'The maximum number of ghosts to use\'), default=4)\n  parser.add_option(\'-z\', \'--zoom\', type=\'float\', dest=\'zoom\',\n                    help=default(\'Zoom the size of the graphics window\'), default=1.0)\n  parser.add_option(\'-f\', \'--fixRandomSeed\', action=\'store_true\', dest=\'fixRandomSeed\',\n                    help=\'Fixes the random seed to always play the same game\', default=False)\n  parser.add_option(\'-r\', \'--recordActions\', action=\'store_true\', dest=\'record\',\n                    help=\'Writes game histories to a file (named by the time they were played)\', default=False)\n  parser.add_option(\'--replay\', dest=\'gameToReplay\',\n                    help=\'A recorded game file (pickle) to replay\', default=None)\n  parser.add_option(\'-a\',\'--agentArgs\',dest=\'agentArgs\',\n                    help=\'Comma separated values sent to agent. e.g. ""opt1=val1,opt2,opt3=val3""\')\n  parser.add_option(\'-x\', \'--numTraining\', dest=\'numTraining\', type=\'int\',\n                    help=default(\'How many episodes are training (suppresses output)\'), default=0)\n  parser.add_option(\'--frameTime\', dest=\'frameTime\', type=\'float\',\n                    help=default(\'Time to delay between frames; <0 means keyboard\'), default=0.1)\n  parser.add_option(\'-c\', \'--catchExceptions\', action=\'store_true\', dest=\'catchExceptions\', \n                    help=\'Turns on exception handling and timeouts during games\', default=False)\n  parser.add_option(\'--timeout\', dest=\'timeout\', type=\'int\',\n                    help=default(\'Maximum length of time an agent can spend computing in a single game\'), default=30)\n\n  options, otherjunk = parser.parse_args(argv)\n  if len(otherjunk) != 0:\n    raise Exception(\'Command line input not understood: \' + str(otherjunk))\n  args = dict()\n\n  # Fix the random seed\n  if options.fixRandomSeed: random.seed(\'cs188\')\n\n  # Choose a layout\n  args[\'layout\'] = layout.getLayout( options.layout )\n  if args[\'layout\'] == None: raise Exception(""The layout "" + options.layout + "" cannot be found"")\n\n  # Choose a Pacman agent\n  noKeyboard = options.gameToReplay == None and (options.textGraphics or options.quietGraphics)\n  pacmanType = loadAgent(options.pacman, noKeyboard)\n  agentOpts = parseAgentArgs(options.agentArgs)\n  if options.numTraining > 0:\n    args[\'numTraining\'] = options.numTraining\n    if \'numTraining\' not in agentOpts: agentOpts[\'numTraining\'] = options.numTraining\n  pacman = pacmanType(**agentOpts) # Instantiate Pacman with agentArgs\n  args[\'pacman\'] = pacman\n\n  # Don\'t display training games\n  if \'numTrain\' in agentOpts:\n    options.numQuiet = int(agentOpts[\'numTrain\'])\n    options.numIgnore = int(agentOpts[\'numTrain\'])\n\n  # Choose a ghost agent\n  ghostType = loadAgent(options.ghost, noKeyboard)\n  args[\'ghosts\'] = [ghostType( i+1 ) for i in range( options.numGhosts )]\n\n  # Choose a display format\n  if options.quietGraphics:\n      import textDisplay\n      args[\'display\'] = textDisplay.NullGraphics()\n  elif options.textGraphics:\n    import textDisplay\n    textDisplay.SLEEP_TIME = options.frameTime\n    args[\'display\'] = textDisplay.PacmanGraphics()\n  else:\n    import graphicsDisplay\n    args[\'display\'] = graphicsDisplay.PacmanGraphics(options.zoom, frameTime = options.frameTime)\n  args[\'numGames\'] = options.numGames\n  args[\'record\'] = options.record\n  args[\'catchExceptions\'] = options.catchExceptions\n  args[\'timeout\'] = options.timeout\n\n  # Special case: recorded games don\'t use the runGames method or args structure\n  if options.gameToReplay != None:\n    print \'Replaying recorded game %s.\' % options.gameToReplay\n    import cPickle\n    f = open(options.gameToReplay)\n    try: recorded = cPickle.load(f)\n    finally: f.close()\n    recorded[\'display\'] = args[\'display\']\n    replayGame(**recorded)\n    sys.exit(0)\n\n  return args\n\ndef loadAgent(pacman, nographics):\n  # Looks through all pythonPath Directories for the right module,\n  pythonPathStr = os.path.expandvars(""$PYTHONPATH"")\n  if pythonPathStr.find(\';\') == -1:\n    pythonPathDirs = pythonPathStr.split(\':\')\n  else:\n    pythonPathDirs = pythonPathStr.split(\';\')\n  pythonPathDirs.append(\'.\')\n\n  for moduleDir in pythonPathDirs:\n    if not os.path.isdir(moduleDir): continue\n    moduleNames = [f for f in os.listdir(moduleDir) if f.endswith(\'gents.py\')]\n    for modulename in moduleNames:\n      try:\n        module = __import__(modulename[:-3])\n      except ImportError:\n        continue\n      if pacman in dir(module):\n        if nographics and modulename == \'keyboardAgents.py\':\n          raise Exception(\'Using the keyboard requires graphics (not text display)\')\n        return getattr(module, pacman)\n  raise Exception(\'The agent \' + pacman + \' is not specified in any *Agents.py.\')\n\ndef replayGame( layout, actions, display ):\n    import pacmanAgents, ghostAgents\n    rules = ClassicGameRules()\n    agents = [pacmanAgents.GreedyAgent()] + [ghostAgents.RandomGhost(i+1) for i in range(layout.getNumGhosts())]\n    game = rules.newGame( layout, agents[0], agents[1:], display )\n    state = game.state\n    display.initialize(state.data)\n\n    for action in actions:\n      # Execute the action\n      state = state.generateSuccessor( *action )\n      # Change the display\n      display.update( state.data )\n      # Allow for game specific conditions (winning, losing, etc.)\n      rules.process(state, game)\n\n    display.finish()\n\ndef runGames( layout, pacman, ghosts, display, numGames, record, numTraining = 0, catchExceptions=False, timeout=30 ):\n  import __main__\n  __main__.__dict__[\'_display\'] = display\n\n  rules = ClassicGameRules(timeout)\n  games = []\n\n  for i in range( numGames ):\n    beQuiet = i < numTraining\n    if beQuiet:\n        # Suppress output and graphics\n        import textDisplay\n        gameDisplay = textDisplay.NullGraphics()\n        rules.quiet = True\n    else:\n        gameDisplay = display\n        rules.quiet = False\n    game = rules.newGame( layout, pacman, ghosts, gameDisplay, beQuiet, catchExceptions)\n    game.run()\n    if not beQuiet: games.append(game)\n\n    if record:\n      import time, cPickle\n      fname = (\'recorded-game-%d\' % (i + 1)) +  \'-\'.join([str(t) for t in time.localtime()[1:6]])\n      f = file(fname, \'w\')\n      components = {\'layout\': layout, \'actions\': game.moveHistory}\n      cPickle.dump(components, f)\n      f.close()\n\n  if (numGames-numTraining) > 0:\n    scores = [game.state.getScore() for game in games]\n    wins = [game.state.isWin() for game in games]\n    winRate = wins.count(True)/ float(len(wins))\n    print \'Average Score:\', sum(scores) / float(len(scores))\n    print \'Scores:       \', \', \'.join([str(score) for score in scores])\n    print \'Win Rate:      %d/%d (%.2f)\' % (wins.count(True), len(wins), winRate)\n    print \'Record:       \', \', \'.join([ [\'Loss\', \'Win\'][int(w)] for w in wins])\n\n  return games\n\nif __name__ == \'__main__\':\n  """"""\n  The main function called when pacman.py is run\n  from the command line:\n\n  > python pacman.py\n\n  See the usage string for more details.\n\n  > python pacman.py --help\n  """"""\n  args = readCommand( sys.argv[1:] ) # Get game components based on input\n  runGames( **args )\n\n  # import cProfile\n  # cProfile.run(""runGames( **args )"")\n  pass\n'"
week03_model_free/crawler_and_pacman/seminar_py2/pacmanAgents.py,0,"b'# pacmanAgents.py\n# ---------------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nfrom pacman import Directions\nfrom game import Agent\nimport random\nimport game\nimport util\n\nclass LeftTurnAgent(game.Agent):\n  ""An agent that turns left at every opportunity""\n  \n  def getAction(self, state):\n    legal = state.getLegalPacmanActions()\n    current = state.getPacmanState().configuration.direction\n    if current == Directions.STOP: current = Directions.NORTH\n    left = Directions.LEFT[current]\n    if left in legal: return left\n    if current in legal: return current\n    if Directions.RIGHT[current] in legal: return Directions.RIGHT[current]\n    if Directions.LEFT[left] in legal: return Directions.LEFT[left]\n    return Directions.STOP\n\nclass GreedyAgent(Agent):\n  def __init__(self, evalFn=""scoreEvaluation""):\n    self.evaluationFunction = util.lookup(evalFn, globals())\n    assert self.evaluationFunction != None\n        \n  def getAction(self, state):\n    # Generate candidate actions\n    legal = state.getLegalPacmanActions()\n    if Directions.STOP in legal: legal.remove(Directions.STOP)\n      \n    successors = [(state.generateSuccessor(0, action), action) for action in legal] \n    scored = [(self.evaluationFunction(state), action) for state, action in successors]\n    bestScore = max(scored)[0]\n    bestActions = [pair[1] for pair in scored if pair[0] == bestScore]\n    return random.choice(bestActions)\n  \ndef scoreEvaluation(state):\n  return state.getScore()  '"
week03_model_free/crawler_and_pacman/seminar_py2/qlearningAgents.py,0,"b'# qlearningAgents.py\n# ------------------\n## based on http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nfrom game import *\nfrom learningAgents import ReinforcementAgent\nfrom featureExtractors import *\n\nimport random,util,math\nfrom collections import defaultdict\n\nclass QLearningAgent(ReinforcementAgent):\n  """"""\n    Q-Learning Agent\n\n    Instance variables you have access to\n      - self.epsilon (exploration prob)\n      - self.alpha (learning rate)\n      - self.discount (discount rate aka gamma)\n\n    Functions you should use\n      - self.getLegalActions(state)\n        which returns legal actions for a state\n      - self.getQValue(state,action)\n        which returns Q(state,action)\n      - self.setQValue(state,action,value)\n        which sets Q(state,action) := value\n    \n    !!!Important!!!\n    NOTE: please avoid using self._qValues directly to make code cleaner\n  """"""\n  def __init__(self, **args):\n    ""We initialize agent and Q-values here.""\n    ReinforcementAgent.__init__(self, **args)\n    self._qValues = defaultdict(lambda:defaultdict(lambda:0))\n    \n\n  def getQValue(self, state, action):\n    """"""\n      Returns Q(state,action)\n    """"""\n    return self._qValues[state][action]\n\n  def setQValue(self,state,action,value):\n    """"""\n      Sets the Qvalue for [state,action] to the given value\n    """"""\n    self._qValues[state][action] = value\n\n#---------------------#start of your code#---------------------#\n\n  def getValue(self, state):\n    """"""\n      Returns max_action Q(state,action)\n      where the max is over legal actions.\n    """"""\n    \n    possibleActions = self.getLegalActions(state)\n    #If there are no legal actions, return 0.0\n    if len(possibleActions) == 0:\n    \treturn 0.0\n\n    ""*** YOUR CODE HERE ***""\n    raise NotImplementedError\n\n    return 0.\n    \n  def getPolicy(self, state):\n    """"""\n      Compute the best action to take in a state. \n      \n    """"""\n    possibleActions = self.getLegalActions(state)\n\n    #If there are no legal actions, return None\n    if len(possibleActions) == 0:\n    \treturn None\n    \n    best_action = None\n\n    ""*** YOUR CODE HERE ***""\n    raise NotImplementedError\n\n    return best_action\n\n  def getAction(self, state):\n    """"""\n      Compute the action to take in the current state, including exploration.  \n      \n      With probability self.epsilon, we should take a random action.\n      otherwise - the best policy action (self.getPolicy).\n\n      HINT: You might want to use util.flipCoin(prob)\n      HINT: To pick randomly from a list, use random.choice(list)\n\n    """"""\n    \n    # Pick Action\n    possibleActions = self.getLegalActions(state)\n    action = None\n    \n    #If there are no legal actions, return None\n    if len(possibleActions) == 0:\n    \treturn None\n\n    #agent parameters:\n    epsilon = self.epsilon\n\n    ""*** YOUR CODE HERE ***""\n    raise NotImplementedError    \n\n    return action\n\n  def update(self, state, action, nextState, reward):\n    """"""\n      You should do your Q-Value update here\n\n      NOTE: You should never call this function,\n      it will be called on your behalf\n\n\n    """"""\n    #agent parameters\n    gamma = self.discount\n    learning_rate = self.alpha\n    \n    ""*** YOUR CODE HERE ***""\n    raise NotImplementedError\n    \n    reference_qvalue = PleaseImplementMe\n    updated_qvalue = PleaseImplementMe\n\n    self.setQValue(PleaseImplementMe,PleaseImplementMe,updated_qvalue)\n\n\n#---------------------#end of your code#---------------------#\n\n\n\nclass PacmanQAgent(QLearningAgent):\n  ""Exactly the same as QLearningAgent, but with different default parameters""\n\n  def __init__(self, epsilon=0.05,gamma=0.8,alpha=0.2, numTraining=0, **args):\n    """"""\n    These default parameters can be changed from the pacman.py command line.\n    For example, to change the exploration rate, try:\n        python pacman.py -p PacmanQLearningAgent -a epsilon=0.1\n\n    alpha    - learning rate\n    epsilon  - exploration rate\n    gamma    - discount factor\n    numTraining - number of training episodes, i.e. no learning after these many episodes\n    """"""\n    args[\'epsilon\'] = epsilon\n    args[\'gamma\'] = gamma\n    args[\'alpha\'] = alpha\n    args[\'numTraining\'] = numTraining\n    self.index = 0  # This is always Pacman\n    QLearningAgent.__init__(self, **args)\n\n  def getAction(self, state):\n    """"""\n    Simply calls the getAction method of QLearningAgent and then\n    informs parent of action for Pacman.  Do not change or remove this\n    method.\n    """"""\n    action = QLearningAgent.getAction(self,state)\n    self.doAction(state,action)\n    return action\n\n\n\nclass ApproximateQAgent(PacmanQAgent):\n    pass\n'"
week03_model_free/crawler_and_pacman/seminar_py2/textDisplay.py,0,"b'# textDisplay.py\n# --------------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nimport pacman, time\n\nDRAW_EVERY = 1\nSLEEP_TIME = 0 # This can be overwritten by __init__\nDISPLAY_MOVES = False\nQUIET = False # Supresses output\n\nclass NullGraphics:\n  def initialize(self, state, isBlue = False):\n    pass\n  \n  def update(self, state):\n    pass\n  \n  def pause(self):\n    time.sleep(SLEEP_TIME)\n    \n  def draw(self, state):\n    print state\n  \n  def finish(self):\n    pass\n\nclass PacmanGraphics:\n  def __init__(self, speed=None):\n    if speed != None:\n      global SLEEP_TIME\n      SLEEP_TIME = speed\n  \n  def initialize(self, state, isBlue = False):\n    self.draw(state)\n    self.pause()\n    self.turn = 0\n    self.agentCounter = 0\n    \n  def update(self, state):\n    numAgents = len(state.agentStates)\n    self.agentCounter = (self.agentCounter + 1) % numAgents\n    if self.agentCounter == 0:\n      self.turn += 1\n      if DISPLAY_MOVES:\n        ghosts = [pacman.nearestPoint(state.getGhostPosition(i)) for i in range(1, numAgents)]\n        print ""%4d) P: %-8s"" % (self.turn, str(pacman.nearestPoint(state.getPacmanPosition()))),\'| Score: %-5d\' % state.score,\'| Ghosts:\', ghosts\n      if self.turn % DRAW_EVERY == 0:\n        self.draw(state)\n        self.pause()\n    if state._win or state._lose:\n      self.draw(state)\n    \n  def pause(self):\n    time.sleep(SLEEP_TIME)\n    \n  def draw(self, state):\n    print state\n  \n  def finish(self):\n    pass\n'"
week03_model_free/crawler_and_pacman/seminar_py2/textGridworldDisplay.py,0,"b'# textGridworldDisplay.py\n# -----------------------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nimport util\n\nclass TextGridworldDisplay:\n  \n  def __init__(self, gridworld):\n    self.gridworld = gridworld\n  \n  def start(self):\n    pass\n  \n  def pause(self):\n    pass\n  \n  def displayValues(self, agent, currentState = None, message = None):\n    if message != None:\n      print message\n    values = util.Counter()\n    policy = {}\n    states = self.gridworld.getStates()\n    for state in states:\n      values[state] = agent.getValue(state)\n      policy[state] = agent.getPolicy(state)\n    prettyPrintValues(self.gridworld, values, policy, currentState)\n  \n  def displayNullValues(self, agent, currentState = None, message = None):\n    if message != None: print message\n    prettyPrintNullValues(self.gridworld, currentState)\n\n  def displayQValues(self, agent, currentState = None, message = None):\n    if message != None: print message\n    qValues = util.Counter()\n    states = self.gridworld.getStates()\n    for state in states:\n      for action in self.gridworld.getPossibleActions(state):\n        qValues[(state, action)] = agent.getQValue(state, action)\n    prettyPrintQValues(self.gridworld, qValues, currentState)\n\n\ndef prettyPrintValues(gridWorld, values, policy=None, currentState = None):\n  grid = gridWorld.grid\n  maxLen = 11\n  newRows = []\n  for y in range(grid.height):\n    newRow = []\n    for x in range(grid.width):\n      state = (x, y)\n      value = values[state]\n      action = None\n      if policy != None and state in policy:\n        action = policy[state]          \n      actions = gridWorld.getPossibleActions(state)        \n      if action not in actions and \'exit\' in actions:\n        action = \'exit\'\n      valString = None\n      if action == \'exit\':\n        valString = border(\'%.2f\' % value)\n      else:\n        valString = \'\\n\\n%.2f\\n\\n\' % value\n        valString += \' \'*maxLen\n      if grid[x][y] == \'S\':\n        valString = \'\\n\\nS: %.2f\\n\\n\'  % value\n        valString += \' \'*maxLen        \n      if grid[x][y] == \'#\':\n        valString = \'\\n#####\\n#####\\n#####\\n\'\n        valString += \' \'*maxLen\n      pieces = [valString]                \n      text = (""\\n"".join(pieces)).split(\'\\n\')        \n      if currentState == state:\n        l = len(text[1])\n        if l == 0:\n          text[1] = \'*\'\n        else:\n          text[1] = ""|"" + \' \' * int((l-1)/2-1) + \'*\' + \' \' * int((l)/2-1) + ""|""       \n      if action == \'east\':\n        text[2] = \'  \' + text[2]  + \' >\'\n      elif action == \'west\':\n        text[2] = \'< \' + text[2]  + \'  \'\n      elif action == \'north\':\n        text[0] = \' \' * int(maxLen/2) + \'^\' +\' \' * int(maxLen/2)\n      elif action == \'south\':\n        text[4] = \' \' * int(maxLen/2) + \'v\' +\' \' * int(maxLen/2)\n      newCell = ""\\n"".join(text)\n      newRow.append(newCell)\n    newRows.append(newRow)\n  numCols = grid.width\n  for rowNum, row in enumerate(newRows):\n    row.insert(0,""\\n\\n""+str(rowNum))\n  newRows.reverse()\n  colLabels = [str(colNum) for colNum in range(numCols)]\n  colLabels.insert(0,\' \')\n  finalRows = [colLabels] + newRows\n  print indent(finalRows,separateRows=True,delim=\'|\', prefix=\'|\',postfix=\'|\', justify=\'center\',hasHeader=True)\n\n\ndef prettyPrintNullValues(gridWorld, currentState = None):\n    grid = gridWorld.grid\n    maxLen = 11\n    newRows = []\n    for y in range(grid.height):\n      newRow = []\n      for x in range(grid.width):\n        state = (x, y)\n\n        # value = values[state]\n\n        action = None\n        # if policy != None and state in policy:\n        #   action = policy[state]\n        # \n        actions = gridWorld.getPossibleActions(state)\n\n        if action not in actions and \'exit\' in actions:\n          action = \'exit\'\n\n        valString = None\n        # if action == \'exit\':\n        #   valString = border(\'%.2f\' % value)\n        # else:\n        #   valString = \'\\n\\n%.2f\\n\\n\' % value\n        #   valString += \' \'*maxLen\n\n        if grid[x][y] == \'S\':\n          valString = \'\\n\\nS\\n\\n\'\n          valString += \' \'*maxLen\n        elif grid[x][y] == \'#\':\n          valString = \'\\n#####\\n#####\\n#####\\n\'\n          valString += \' \'*maxLen\n        elif type(grid[x][y]) == float or type(grid[x][y]) == int:\n          valString = border(\'%.2f\' % float(grid[x][y]))\n        else: valString = border(\'  \')\n        pieces = [valString]\n\n        text = (""\\n"".join(pieces)).split(\'\\n\')\n\n        if currentState == state:\n          l = len(text[1])\n          if l == 0:\n            text[1] = \'*\'\n          else:\n            text[1] = ""|"" + \' \' * int((l-1)/2-1) + \'*\' + \' \' * int((l)/2-1) + ""|""\n\n        if action == \'east\':\n          text[2] = \'  \' + text[2]  + \' >\'\n        elif action == \'west\':\n          text[2] = \'< \' + text[2]  + \'  \'\n        elif action == \'north\':\n          text[0] = \' \' * int(maxLen/2) + \'^\' +\' \' * int(maxLen/2)\n        elif action == \'south\':\n          text[4] = \' \' * int(maxLen/2) + \'v\' +\' \' * int(maxLen/2)\n        newCell = ""\\n"".join(text)\n        newRow.append(newCell)\n      newRows.append(newRow)\n    numCols = grid.width\n    for rowNum, row in enumerate(newRows):\n      row.insert(0,""\\n\\n""+str(rowNum))\n    newRows.reverse()\n    colLabels = [str(colNum) for colNum in range(numCols)]\n    colLabels.insert(0,\' \')\n    finalRows = [colLabels] + newRows\n    print indent(finalRows,separateRows=True,delim=\'|\', prefix=\'|\',postfix=\'|\', justify=\'center\',hasHeader=True)\n  \ndef prettyPrintQValues(gridWorld, qValues, currentState=None):\n    grid = gridWorld.grid\n    maxLen = 11\n    newRows = []\n    for y in range(grid.height):\n      newRow = []\n      for x in range(grid.width):\n        state = (x, y)\n        actions = gridWorld.getPossibleActions(state)\n        if actions == None or len(actions) == 0:\n          actions = [None]\n        bestQ = max([qValues[(state, action)] for action in actions])\n        bestActions = [action for action in actions if qValues[(state, action)] == bestQ]\n    \n        # display cell\n        qStrings = dict([(action, ""%.2f"" % qValues[(state, action)]) for action in actions])\n        northString = (\'north\' in qStrings and qStrings[\'north\']) or \' \'\n        southString = (\'south\' in qStrings and qStrings[\'south\']) or \' \'\n        eastString = (\'east\' in qStrings and qStrings[\'east\']) or \' \'\n        westString = (\'west\' in qStrings and qStrings[\'west\']) or \' \'\n        exitString = (\'exit\' in qStrings and qStrings[\'exit\']) or \' \'\n\n        eastLen = len(eastString)\n        westLen = len(westString)\n        if eastLen < westLen:\n          eastString = \' \'*(westLen-eastLen)+eastString\n        if westLen < eastLen:\n          westString = westString+\' \'*(eastLen-westLen)\n    \n        if \'north\' in bestActions:\n          northString = \'/\'+northString+\'\\\\\'\n        if \'south\' in bestActions:\n          southString = \'\\\\\'+southString+\'/\'\n        if \'east\' in bestActions:\n          eastString = \'\'+eastString+\'>\'\n        else:\n          eastString = \'\'+eastString+\' \'\n        if \'west\' in bestActions:\n          westString = \'<\'+westString+\'\'\n        else:\n          westString = \' \'+westString+\'\'\n        if \'exit\' in bestActions:\n          exitString = \'[ \'+exitString+\' ]\'\n\n    \n        ewString = westString + ""     "" + eastString\n        if state == currentState:\n          ewString = westString + ""  *  "" + eastString\n        if state == gridWorld.getStartState():\n          ewString = westString + ""  S  "" + eastString\n        if state == currentState and state == gridWorld.getStartState():\n          ewString = westString + "" S:* "" + eastString\n    \n        text = [northString, ""\\n""+exitString, ewString, \' \'*maxLen+""\\n"", southString]\n    \n        if grid[x][y] == \'#\':\n          text = [\'\', \'\\n#####\\n#####\\n#####\', \'\']\n    \n        newCell = ""\\n"".join(text)\n        newRow.append(newCell)\n      newRows.append(newRow)\n    numCols = grid.width\n    for rowNum, row in enumerate(newRows):\n      row.insert(0,""\\n\\n\\n""+str(rowNum))\n    newRows.reverse()\n    colLabels = [str(colNum) for colNum in range(numCols)]\n    colLabels.insert(0,\' \')\n    finalRows = [colLabels] + newRows\n\n    print indent(finalRows,separateRows=True,delim=\'|\',prefix=\'|\',postfix=\'|\', justify=\'center\',hasHeader=True)\n\ndef border(text):    \n  length = len(text)\n  pieces = [\'-\' * (length+2), \'|\'+\' \' * (length+2)+\'|\', \' | \'+text+\' | \', \'|\'+\' \' * (length+2)+\'|\',\'-\' * (length+2)]\n  return \'\\n\'.join(pieces)\n    \n# INDENTING CODE\n\n# Indenting code based on a post from George Sakkis\n# (http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/267662)\n\nimport cStringIO,operator\n\ndef indent(rows, hasHeader=False, headerChar=\'-\', delim=\' | \', justify=\'left\',\n           separateRows=False, prefix=\'\', postfix=\'\', wrapfunc=lambda x:x):\n    """"""Indents a table by column.\n       - rows: A sequence of sequences of items, one sequence per row.\n       - hasHeader: True if the first row consists of the columns\' names.\n       - headerChar: Character to be used for the row separator line\n         (if hasHeader==True or separateRows==True).\n       - delim: The column delimiter.\n       - justify: Determines how are data justified in their column. \n         Valid values are \'left\',\'right\' and \'center\'.\n       - separateRows: True if rows are to be separated by a line\n         of \'headerChar\'s.\n       - prefix: A string prepended to each printed row.\n       - postfix: A string appended to each printed row.\n       - wrapfunc: A function f(text) for wrapping text; each element in\n         the table is first wrapped by this function.""""""\n    # closure for breaking logical rows to physical, using wrapfunc\n    def rowWrapper(row):\n        newRows = [wrapfunc(item).split(\'\\n\') for item in row]\n        return [[substr or \'\' for substr in item] for item in map(None,*newRows)]\n    # break each logical row into one or more physical ones\n    logicalRows = [rowWrapper(row) for row in rows]\n    # columns of physical rows\n    columns = map(None,*reduce(operator.add,logicalRows))\n    # get the maximum of each column by the string length of its items\n    maxWidths = [max([len(str(item)) for item in column]) for column in columns]\n    rowSeparator = headerChar * (len(prefix) + len(postfix) + sum(maxWidths) + \\\n                                 len(delim)*(len(maxWidths)-1))\n    # select the appropriate justify method\n    justify = {\'center\':str.center, \'right\':str.rjust, \'left\':str.ljust}[justify.lower()]\n    output=cStringIO.StringIO()\n    if separateRows: print >> output, rowSeparator\n    for physicalRows in logicalRows:\n        for row in physicalRows:\n            print >> output, \\\n                prefix \\\n                + delim.join([justify(str(item),width) for (item,width) in zip(row,maxWidths)]) \\\n                + postfix\n        if separateRows or hasHeader: print >> output, rowSeparator; hasHeader=False\n    return output.getvalue()\n    \nimport math\ndef wrap_always(text, width):\n    """"""A simple word-wrap function that wraps text on exactly width characters.\n       It doesn\'t split the text in words.""""""\n    return \'\\n\'.join([ text[width*i:width*(i+1)] \\\n                       for i in xrange(int(math.ceil(1.*len(text)/width))) ])\n    \n    \n# TEST OF DISPLAY CODE\n                                \nif __name__ == \'__main__\':\n  import gridworld, util\n\n  grid = gridworld.getCliffGrid3()\n  print grid.getStates()\n  \n  policy = dict([(state,\'east\') for state in grid.getStates()])\n  values = util.Counter(dict([(state,1000.23) for state in grid.getStates()]))\n  prettyPrintValues(grid, values, policy, currentState = (0,0))\n\n  stateCrossActions = [[(state, action) for action in grid.getPossibleActions(state)] for state in grid.getStates()]\n  qStates = reduce(lambda x,y: x+y, stateCrossActions, [])\n  qValues = util.Counter(dict([((state, action), 10.5) for state, action in qStates]))\n  qValues = util.Counter(dict([((state, action), 10.5) for state, action in reduce(lambda x,y: x+y, stateCrossActions, [])]))\n  prettyPrintQValues(grid, qValues, currentState = (0,0))\n'"
week03_model_free/crawler_and_pacman/seminar_py2/util.py,0,"b'# util.py\n# -------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nimport sys\nimport inspect\nimport heapq, random\n\n\n""""""\n Data structures useful for implementing SearchAgents\n""""""\n\nclass Stack:\n  ""A container with a last-in-first-out (LIFO) queuing policy.""\n  def __init__(self):\n    self.list = []\n    \n  def push(self,item):\n    ""Push \'item\' onto the stack""\n    self.list.append(item)\n\n  def pop(self):\n    ""Pop the most recently pushed item from the stack""\n    return self.list.pop()\n\n  def isEmpty(self):\n    ""Returns true if the stack is empty""\n    return len(self.list) == 0\n\nclass Queue:\n  ""A container with a first-in-first-out (FIFO) queuing policy.""\n  def __init__(self):\n    self.list = []\n  \n  def push(self,item):\n    ""Enqueue the \'item\' into the queue""\n    self.list.insert(0,item)\n\n  def pop(self):\n    """"""\n      Dequeue the earliest enqueued item still in the queue. This\n      operation removes the item from the queue.\n    """"""\n    return self.list.pop()\n\n  def isEmpty(self):\n    ""Returns true if the queue is empty""\n    return len(self.list) == 0\n  \nclass PriorityQueue:\n  """"""\n    Implements a priority queue data structure. Each inserted item\n    has a priority associated with it and the client is usually interested\n    in quick retrieval of the lowest-priority item in the queue. This\n    data structure allows O(1) access to the lowest-priority item.\n    \n    Note that this PriorityQueue does not allow you to change the priority\n    of an item.  However, you may insert the same item multiple times with\n    different priorities.\n  """"""  \n  def  __init__(self):  \n    self.heap = []\n    \n  def push(self, item, priority):\n      pair = (priority,item)\n      heapq.heappush(self.heap,pair)\n\n  def pop(self):\n      (priority,item) = heapq.heappop(self.heap)\n      return item\n  \n  def isEmpty(self):\n    return len(self.heap) == 0\n\nclass PriorityQueueWithFunction(PriorityQueue):\n  """"""\n  Implements a priority queue with the same push/pop signature of the\n  Queue and the Stack classes. This is designed for drop-in replacement for\n  those two classes. The caller has to provide a priority function, which\n  extracts each item\'s priority.\n  """"""  \n  def  __init__(self, priorityFunction):\n    ""priorityFunction (item) -> priority""\n    self.priorityFunction = priorityFunction      # store the priority function\n    PriorityQueue.__init__(self)        # super-class initializer\n    \n  def push(self, item):\n    ""Adds an item to the queue with priority from the priority function""\n    PriorityQueue.push(self, item, self.priorityFunction(item))\n\n    \ndef manhattanDistance( xy1, xy2 ):\n  ""Returns the Manhattan distance between points xy1 and xy2""\n  return abs( xy1[0] - xy2[0] ) + abs( xy1[1] - xy2[1] )\n\n""""""\n  Data structures and functions useful for various course projects\n  \n  The search project should not need anything below this line.\n""""""\n\nclass Counter(dict):\n  """"""\n  A counter keeps track of counts for a set of keys.\n  \n  The counter class is an extension of the standard python\n  dictionary type.  It is specialized to have number values  \n  (integers or floats), and includes a handful of additional\n  functions to ease the task of counting data.  In particular, \n  all keys are defaulted to have value 0.  Using a dictionary:\n  \n  a = {}\n  print a[\'test\']\n  \n  would give an error, while the Counter class analogue:\n    \n  >>> a = Counter()\n  >>> print a[\'test\']\n  0\n\n  returns the default 0 value. Note that to reference a key \n  that you know is contained in the counter, \n  you can still use the dictionary syntax:\n    \n  >>> a = Counter()\n  >>> a[\'test\'] = 2\n  >>> print a[\'test\']\n  2\n  \n  This is very useful for counting things without initializing their counts,\n  see for example:\n  \n  >>> a[\'blah\'] += 1\n  >>> print a[\'blah\']\n  1\n  \n  The counter also includes additional functionality useful in implementing\n  the classifiers for this assignment.  Two counters can be added,\n  subtracted or multiplied together.  See below for details.  They can\n  also be normalized and their total count and arg max can be extracted.\n  """"""\n  def __getitem__(self, idx):\n    self.setdefault(idx, 0)\n    return dict.__getitem__(self, idx)\n\n  def incrementAll(self, keys, count):\n    """"""\n    Increments all elements of keys by the same count.\n    \n    >>> a = Counter()\n    >>> a.incrementAll([\'one\',\'two\', \'three\'], 1)\n    >>> a[\'one\']\n    1\n    >>> a[\'two\']\n    1\n    """"""\n    for key in keys:\n      self[key] += count\n  \n  def argMax(self):\n    """"""\n    Returns the key with the highest value.\n    """"""\n    if len(self.keys()) == 0: return None\n    all = self.items()\n    values = [x[1] for x in all]\n    maxIndex = values.index(max(values))\n    return all[maxIndex][0]\n  \n  def sortedKeys(self):\n    """"""\n    Returns a list of keys sorted by their values.  Keys\n    with the highest values will appear first.\n    \n    >>> a = Counter()\n    >>> a[\'first\'] = -2\n    >>> a[\'second\'] = 4\n    >>> a[\'third\'] = 1\n    >>> a.sortedKeys()\n    [\'second\', \'third\', \'first\']\n    """"""\n    sortedItems = self.items()\n    compare = lambda x, y:  sign(y[1] - x[1])\n    sortedItems.sort(cmp=compare)\n    return [x[0] for x in sortedItems]\n  \n  def totalCount(self):\n    """"""\n    Returns the sum of counts for all keys.\n    """"""\n    return sum(self.values())\n  \n  def normalize(self):\n    """"""\n    Edits the counter such that the total count of all\n    keys sums to 1.  The ratio of counts for all keys\n    will remain the same. Note that normalizing an empty \n    Counter will result in an error.\n    """"""\n    total = float(self.totalCount())\n    if total == 0: return\n    for key in self.keys():\n      self[key] = self[key] / total\n      \n  def divideAll(self, divisor):\n    """"""\n    Divides all counts by divisor\n    """"""\n    divisor = float(divisor)\n    for key in self:\n      self[key] /= divisor\n\n  def copy(self):\n    """"""\n    Returns a copy of the counter\n    """"""\n    return Counter(dict.copy(self))\n  \n  def __mul__(self, y ):\n    """"""\n    Multiplying two counters gives the dot product of their vectors where\n    each unique label is a vector element.\n    \n    >>> a = Counter()\n    >>> b = Counter()\n    >>> a[\'first\'] = -2\n    >>> a[\'second\'] = 4\n    >>> b[\'first\'] = 3\n    >>> b[\'second\'] = 5\n    >>> a[\'third\'] = 1.5\n    >>> a[\'fourth\'] = 2.5\n    >>> a * b\n    14\n    """"""\n    sum = 0\n    x = self\n    if len(x) > len(y):\n      x,y = y,x\n    for key in x:\n      if key not in y:\n        continue\n      sum += x[key] * y[key]      \n    return sum\n      \n  def __radd__(self, y):\n    """"""\n    Adding another counter to a counter increments the current counter\n    by the values stored in the second counter.\n    \n    >>> a = Counter()\n    >>> b = Counter()\n    >>> a[\'first\'] = -2\n    >>> a[\'second\'] = 4\n    >>> b[\'first\'] = 3\n    >>> b[\'third\'] = 1\n    >>> a += b\n    >>> a[\'first\']\n    1\n    """""" \n    for key, value in y.items():\n      self[key] += value   \n      \n  def __add__( self, y ):\n    """"""\n    Adding two counters gives a counter with the union of all keys and\n    counts of the second added to counts of the first.\n    \n    >>> a = Counter()\n    >>> b = Counter()\n    >>> a[\'first\'] = -2\n    >>> a[\'second\'] = 4\n    >>> b[\'first\'] = 3\n    >>> b[\'third\'] = 1\n    >>> (a + b)[\'first\']\n    1\n    """"""\n    addend = Counter()\n    for key in self:\n      if key in y:\n        addend[key] = self[key] + y[key]\n      else:\n        addend[key] = self[key]\n    for key in y:\n      if key in self:\n        continue\n      addend[key] = y[key]\n    return addend\n    \n  def __sub__( self, y ):\n    """"""\n    Subtracting a counter from another gives a counter with the union of all keys and\n    counts of the second subtracted from counts of the first.\n    \n    >>> a = Counter()\n    >>> b = Counter()\n    >>> a[\'first\'] = -2\n    >>> a[\'second\'] = 4\n    >>> b[\'first\'] = 3\n    >>> b[\'third\'] = 1\n    >>> (a - b)[\'first\']\n    -5\n    """"""      \n    addend = Counter()\n    for key in self:\n      if key in y:\n        addend[key] = self[key] - y[key]\n      else:\n        addend[key] = self[key]\n    for key in y:\n      if key in self:\n        continue\n      addend[key] = -1 * y[key]\n    return addend\n    \ndef raiseNotDefined():\n  print ""Method not implemented: %s"" % inspect.stack()[1][3]    \n  sys.exit(1)\n\ndef normalize(vectorOrCounter):\n  """"""\n  normalize a vector or counter by dividing each value by the sum of all values\n  """"""\n  normalizedCounter = Counter()\n  if type(vectorOrCounter) == type(normalizedCounter):\n    counter = vectorOrCounter\n    total = float(counter.totalCount())\n    if total == 0: return counter\n    for key in counter.keys():\n      value = counter[key]\n      normalizedCounter[key] = value / total\n    return normalizedCounter\n  else:\n    vector = vectorOrCounter\n    s = float(sum(vector))\n    if s == 0: return vector\n    return [el / s for el in vector]\n                \ndef nSample(distribution, values, n):\n  if sum(distribution) != 1:\n    distribution = normalize(distribution)\n  rand = [random.random() for i in range(n)]\n  rand.sort()\n  samples = []\n  samplePos, distPos, cdf = 0,0, distribution[0]\n  while samplePos < n:\n    if rand[samplePos] < cdf:\n      samplePos += 1\n      samples.append(values[distPos])\n    else:\n      distPos += 1\n      cdf += distribution[distPos]\n  return samples\n    \ndef sample(distribution, values = None):\n  if type(distribution) == Counter: \n    items = distribution.items()\n    distribution = [i[1] for i in items] \n    values = [i[0] for i in items] \n  if sum(distribution) != 1:\n    distribution = normalize(distribution)\n  choice = random.random()\n  i, total= 0, distribution[0]\n  while choice > total:\n    i += 1\n    total += distribution[i]\n  return values[i]\n\ndef sampleFromCounter(ctr):\n  items = ctr.items()\n  return sample([v for k,v in items], [k for k,v in items])\n\ndef getProbability(value, distribution, values):\n  """"""\n    Gives the probability of a value under a discrete distribution\n    defined by (distributions, values).\n  """"""\n  total = 0.0\n  for prob, val in zip(distribution, values):\n    if val == value:\n      total += prob\n  return total\n\ndef flipCoin( p ):\n  r = random.random()\n  return r < p \n\ndef chooseFromDistribution( distribution ):\n  ""Takes either a counter or a list of (prob, key) pairs and samples""\n  if type(distribution) == dict or type(distribution) == Counter:\n    return sample(distribution)\n  r = random.random()\n  base = 0.0\n  for prob, element in distribution:\n    base += prob\n    if r <= base: return element\n    \ndef nearestPoint( pos ):\n  """"""\n  Finds the nearest grid point to a position (discretizes).\n  """"""\n  ( current_row, current_col ) = pos\n\n  grid_row = int( current_row + 0.5 ) \n  grid_col = int( current_col + 0.5 ) \n  return ( grid_row, grid_col )     \n\ndef sign( x ):\n  """"""\n  Returns 1 or -1 depending on the sign of x\n  """"""\n  if( x >= 0 ):\n    return 1\n  else:\n    return -1\n\ndef arrayInvert(array):\n  """"""\n  Inverts a matrix stored as a list of lists.\n  """"""\n  result = [[] for i in array]\n  for outer in array:\n    for inner in range(len(outer)):\n      result[inner].append(outer[inner])\n  return result\n\ndef matrixAsList( matrix, value = True ):\n  """"""\n  Turns a matrix into a list of coordinates matching the specified value\n  """"""\n  rows, cols = len( matrix ), len( matrix[0] )\n  cells = []\n  for row in range( rows ):\n    for col in range( cols ):\n      if matrix[row][col] == value:\n        cells.append( ( row, col ) )\n  return cells\n\ndef lookup(name, namespace):\n  """"""\n  Get a method or class from any imported module from its name.\n  Usage: lookup(functionName, globals())\n  """"""\n  dots = name.count(\'.\')\n  if dots > 0:\n    moduleName, objName = \'.\'.join(name.split(\'.\')[:-1]), name.split(\'.\')[-1]\n    module = __import__(moduleName)\n    return getattr(module, objName)\n  else:\n    modules = [obj for obj in namespace.values() if str(type(obj)) == ""<type \'module\'>""]\n    options = [getattr(module, name) for module in modules if name in dir(module)]\n    options += [obj[1] for obj in namespace.items() if obj[0] == name ]\n    if len(options) == 1: return options[0]\n    if len(options) > 1: raise Exception, \'Name conflict for %s\'\n    raise Exception, \'%s not found as a method or class\' % name\n\ndef pause():\n  """"""\n  Pauses the output stream awaiting user feedback.\n  """"""\n  print ""<Press enter/return to continue>""\n  raw_input()\n  \n  \n## code to handle timeouts\nimport signal\nclass TimeoutFunctionException(Exception):\n    """"""Exception to raise on a timeout""""""\n    pass\n\nclass TimeoutFunction:\n\n    def __init__(self, function, timeout):\n        ""timeout must be at least 1 second. WHY??""\n        self.timeout = timeout\n        self.function = function\n\n    def handle_timeout(self, signum, frame):\n        raise TimeoutFunctionException()\n\n    def __call__(self, *args):\n        if not \'SIGALRM\' in dir(signal):\n            return self.function(*args)\n        old = signal.signal(signal.SIGALRM, self.handle_timeout)\n        signal.alarm(self.timeout)\n        try:\n            result = self.function(*args)\n        finally:\n            signal.signal(signal.SIGALRM, old)\n        signal.alarm(0)\n        return result\n'"
week03_model_free/crawler_and_pacman/seminar_py3/analysis.py,0,"b""# analysis.py\n# -----------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\n######################\n# ANALYSIS QUESTIONS #\n######################\n\n# Change these default values to obtain the specified policies through\n# value iteration.\n\n\ndef question2a():\n    answerDiscount = 0.9\n    answerNoise = 0.2\n    answerLivingReward = 0.0\n    return answerDiscount, answerNoise, answerLivingReward\n    # If not possible, return 'NOT POSSIBLE'\n\n\ndef question2b():\n    answerDiscount = 0.9\n    answerNoise = 0.2\n    answerLivingReward = 0.0\n    return answerDiscount, answerNoise, answerLivingReward\n    # If not possible, return 'NOT POSSIBLE'\n\n\ndef question2c():\n    answerDiscount = 0.9\n    answerNoise = 0.2\n    answerLivingReward = 0.0\n    return answerDiscount, answerNoise, answerLivingReward\n    # If not possible, return 'NOT POSSIBLE'\n\n\ndef question2d():\n    answerDiscount = 0.9\n    answerNoise = 0.2\n    answerLivingReward = 0.0\n    return answerDiscount, answerNoise, answerLivingReward\n    # If not possible, return 'NOT POSSIBLE'\n\n\ndef question2e():\n    answerDiscount = 0.9\n    answerNoise = 0.2\n    answerLivingReward = 0.0\n    return answerDiscount, answerNoise, answerLivingReward\n    # If not possible, return 'NOT POSSIBLE'\n\n\nif __name__ == '__main__':\n    print('Answers to analysis questions:')\n    import analysis\n    for q in [q for q in dir(analysis) if q.startswith('question')]:\n        response = getattr(analysis, q)()\n        print('  Question %s:\\t%s' % (q, str(response)))\n"""
week03_model_free/crawler_and_pacman/seminar_py3/crawler.py,0,"b'# crawler.py\n# ----------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\n#!/usr/bin/python\nimport math\nfrom math import pi as PI\nimport time\nimport environment\nimport random\n\n\nclass CrawlingRobotEnvironment(environment.Environment):\n\n    def __init__(self, crawlingRobot):\n\n        self.crawlingRobot = crawlingRobot\n\n        # The state is of the form (armAngle, handAngle)\n        # where the angles are bucket numbers, not actual\n        # degree measurements\n        self.state = None\n\n        self.nArmStates = 9\n        self.nHandStates = 13\n\n        # create a list of arm buckets and hand buckets to\n        # discretize the state space\n        minArmAngle, maxArmAngle = self.crawlingRobot.getMinAndMaxArmAngles()\n        minHandAngle, maxHandAngle = self.crawlingRobot.getMinAndMaxHandAngles()\n        armIncrement = (maxArmAngle - minArmAngle) / (self.nArmStates-1)\n        handIncrement = (maxHandAngle - minHandAngle) / (self.nHandStates-1)\n        self.armBuckets = [minArmAngle+(armIncrement*i)\n                           for i in range(self.nArmStates)]\n        self.handBuckets = [minHandAngle+(handIncrement*i)\n                            for i in range(self.nHandStates)]\n\n        # Reset\n        self.reset()\n\n    def getCurrentState(self):\n        """"""\n          Return the current state\n          of the crawling robot\n        """"""\n        return self.state\n\n    def getPossibleActions(self, state):\n        """"""\n          Returns possible actions\n          for the states in the\n          current state\n        """"""\n\n        actions = list()\n\n        currArmBucket, currHandBucket = state\n        if currArmBucket > 0:\n            actions.append(\'arm-down\')\n        if currArmBucket < self.nArmStates-1:\n            actions.append(\'arm-up\')\n        if currHandBucket > 0:\n            actions.append(\'hand-down\')\n        if currHandBucket < self.nHandStates-1:\n            actions.append(\'hand-up\')\n\n        return actions\n\n    def doAction(self, action):\n        """"""\n          Perform the action and update\n          the current state of the Environment\n          and return the reward for the\n          current state, the next state\n          and the taken action.\n\n          Returns:\n            nextState, reward\n        """"""\n        nextState, reward = None, None\n\n        oldX, oldY = self.crawlingRobot.getRobotPosition()\n\n        armBucket, handBucket = self.state\n        armAngle, handAngle = self.crawlingRobot.getAngles()\n        if action == \'arm-up\':\n            newArmAngle = self.armBuckets[armBucket+1]\n            self.crawlingRobot.moveArm(newArmAngle)\n            nextState = (armBucket+1, handBucket)\n        if action == \'arm-down\':\n            newArmAngle = self.armBuckets[armBucket-1]\n            self.crawlingRobot.moveArm(newArmAngle)\n            nextState = (armBucket-1, handBucket)\n        if action == \'hand-up\':\n            newHandAngle = self.handBuckets[handBucket+1]\n            self.crawlingRobot.moveHand(newHandAngle)\n            nextState = (armBucket, handBucket+1)\n        if action == \'hand-down\':\n            newHandAngle = self.handBuckets[handBucket-1]\n            self.crawlingRobot.moveHand(newHandAngle)\n            nextState = (armBucket, handBucket-1)\n\n        newX, newY = self.crawlingRobot.getRobotPosition()\n\n        # a simple reward function\n        reward = newX - oldX\n\n        self.state = nextState\n        return nextState, reward\n\n    def reset(self):\n        """"""\n         Resets the Environment to the initial state\n        """"""\n        # Initialize the state to be the middle\n        # value for each parameter e.g. if there are 13 and 19\n        # buckets for the arm and hand parameters, then the intial\n        # state should be (6,9)\n        ##\n        # Also call self.crawlingRobot.setAngles()\n        # to the initial arm and hand angle\n\n        armState = self.nArmStates // 2\n        handState = self.nHandStates // 2\n        self.state = armState, handState\n        self.crawlingRobot.setAngles(\n            self.armBuckets[armState], self.handBuckets[handState])\n        self.crawlingRobot.positions = [\n            20, self.crawlingRobot.getRobotPosition()[0]]\n\n\nclass CrawlingRobot:\n\n    def setAngles(self, armAngle, handAngle):\n        """"""\n            set the robot\'s arm and hand angles\n            to the passed in values\n        """"""\n        self.armAngle = armAngle\n        self.handAngle = handAngle\n\n    def getAngles(self):\n        """"""\n            returns the pair of (armAngle, handAngle)\n        """"""\n        return self.armAngle, self.handAngle\n\n    def getRobotPosition(self):\n        """"""\n            returns the (x,y) coordinates\n            of the lower-left point of the\n            robot\n        """"""\n        return self.robotPos\n\n    def moveArm(self, newArmAngle):\n        """"""\n            move the robot arm to \'newArmAngle\'\n        """"""\n        oldArmAngle = self.armAngle\n        if newArmAngle > self.maxArmAngle:\n            raise RuntimeError(\'Crawling Robot: Arm Raised too high. Careful!\')\n        if newArmAngle < self.minArmAngle:\n            raise RuntimeError(\'Crawling Robot: Arm Raised too low. Careful!\')\n        disp = self.displacement(self.armAngle, self.handAngle,\n                                 newArmAngle, self.handAngle)\n        curXPos = self.robotPos[0]\n        self.robotPos = (curXPos+disp, self.robotPos[1])\n        self.armAngle = newArmAngle\n\n        # Position and Velocity Sign Post\n        self.positions.append(self.getRobotPosition()[0])\n#        self.angleSums.append(abs(math.degrees(oldArmAngle)-math.degrees(newArmAngle)))\n        if len(self.positions) > 100:\n            self.positions.pop(0)\n #           self.angleSums.pop(0)\n\n    def moveHand(self, newHandAngle):\n        """"""\n            move the robot hand to \'newArmAngle\' \n        """"""\n        oldHandAngle = self.handAngle\n\n        if newHandAngle > self.maxHandAngle:\n            raise RuntimeError(\n                \'Crawling Robot: Hand Raised too high. Careful!\')\n        if newHandAngle < self.minHandAngle:\n            raise RuntimeError(\'Crawling Robot: Hand Raised too low. Careful!\')\n        disp = self.displacement(\n            self.armAngle, self.handAngle, self.armAngle, newHandAngle)\n        curXPos = self.robotPos[0]\n        self.robotPos = (curXPos+disp, self.robotPos[1])\n        self.handAngle = newHandAngle\n\n        # Position and Velocity Sign Post\n        self.positions.append(self.getRobotPosition()[0])\n #       self.angleSums.append(abs(math.degrees(oldHandAngle)-math.degrees(newHandAngle)))\n        if len(self.positions) > 100:\n            self.positions.pop(0)\n #           self.angleSums.pop(0)\n\n    def getMinAndMaxArmAngles(self):\n        """"""\n            get the lower- and upper- bound\n            for the arm angles returns (min,max) pair\n        """"""\n        return self.minArmAngle, self.maxArmAngle\n\n    def getMinAndMaxHandAngles(self):\n        """"""\n            get the lower- and upper- bound\n            for the hand angles returns (min,max) pair\n        """"""\n        return self.minHandAngle, self.maxHandAngle\n\n    def getRotationAngle(self):\n        """"""\n            get the current angle the \n            robot body is rotated off the ground\n        """"""\n        armCos, armSin = self.__getCosAndSin(self.armAngle)\n        handCos, handSin = self.__getCosAndSin(self.handAngle)\n        x = self.armLength * armCos + self.handLength * handCos + self.robotWidth\n        y = self.armLength * armSin + self.handLength * handSin + self.robotHeight\n        if y < 0:\n            return math.atan(-y/x)\n        return 0.0\n\n    # You shouldn\'t need methods below here\n\n    def __getCosAndSin(self, angle):\n        return math.cos(angle), math.sin(angle)\n\n    def displacement(self, oldArmDegree, oldHandDegree, armDegree, handDegree):\n\n        oldArmCos, oldArmSin = self.__getCosAndSin(oldArmDegree)\n        armCos, armSin = self.__getCosAndSin(armDegree)\n        oldHandCos, oldHandSin = self.__getCosAndSin(oldHandDegree)\n        handCos, handSin = self.__getCosAndSin(handDegree)\n\n        xOld = self.armLength * oldArmCos + self.handLength * oldHandCos + self.robotWidth\n        yOld = self.armLength * oldArmSin + \\\n            self.handLength * oldHandSin + self.robotHeight\n\n        x = self.armLength * armCos + self.handLength * handCos + self.robotWidth\n        y = self.armLength * armSin + self.handLength * handSin + self.robotHeight\n\n        if y < 0:\n            if yOld <= 0:\n                return math.sqrt(xOld*xOld + yOld*yOld) - math.sqrt(x*x + y*y)\n            return (xOld - yOld*(x-xOld) / (y - yOld)) - math.sqrt(x*x + y*y)\n        else:\n            if yOld >= 0:\n                return 0.0\n            return -(x - y * (xOld-x)/(yOld-y)) + math.sqrt(xOld*xOld + yOld*yOld)\n\n        raise RuntimeError(\'Never Should See This!\')\n\n    def draw(self, stepCount, stepDelay):\n        x1, y1 = self.getRobotPosition()\n        x1 = x1 % self.totWidth\n\n        # Check Lower Still on the ground\n        if y1 != self.groundY:\n            raise RuntimeError(\'Flying Robot!!\')\n\n        rotationAngle = self.getRotationAngle()\n        cosRot, sinRot = self.__getCosAndSin(rotationAngle)\n\n        x2 = x1 + self.robotWidth * cosRot\n        y2 = y1 - self.robotWidth * sinRot\n\n        x3 = x1 - self.robotHeight * sinRot\n        y3 = y1 - self.robotHeight * cosRot\n\n        x4 = x3 + cosRot*self.robotWidth\n        y4 = y3 - sinRot*self.robotWidth\n\n        self.canvas.coords(self.robotBody, x1, y1, x2, y2, x4, y4, x3, y3)\n\n        armCos, armSin = self.__getCosAndSin(rotationAngle+self.armAngle)\n        xArm = x4 + self.armLength * armCos\n        yArm = y4 - self.armLength * armSin\n\n        self.canvas.coords(self.robotArm, x4, y4, xArm, yArm)\n\n        handCos, handSin = self.__getCosAndSin(self.handAngle+rotationAngle)\n        xHand = xArm + self.handLength * handCos\n        yHand = yArm - self.handLength * handSin\n\n        self.canvas.coords(self.robotHand, xArm, yArm, xHand, yHand)\n\n        # Position and Velocity Sign Post\n#        time = len(self.positions) + 0.5 * sum(self.angleSums)\n#        velocity = (self.positions[-1]-self.positions[0]) / time\n#        if len(self.positions) == 1: return\n        steps = (stepCount - self.lastStep)\n        if steps == 0:\n            return\n #       pos = self.positions[-1]\n#        velocity = (pos - self.lastPos) / steps\n  #      g = .9 ** (10 * stepDelay)\n#        g = .99 ** steps\n#        self.velAvg = g * self.velAvg + (1 - g) * velocity\n #       g = .999 ** steps\n #       self.velAvg2 = g * self.velAvg2 + (1 - g) * velocity\n        pos = self.positions[-1]\n        velocity = pos - self.positions[-2]\n        vel2 = (pos - self.positions[0]) / len(self.positions)\n        self.velAvg = .9 * self.velAvg + .1 * vel2\n        velMsg = \'100-step Avg Velocity: %.2f\' % self.velAvg\n#        velMsg2 = \'1000-step Avg Velocity: %.2f\' % self.velAvg2\n        velocityMsg = \'Velocity: %.2f\' % velocity\n        positionMsg = \'Position: %2.f\' % pos\n        stepMsg = \'Step: %d\' % stepCount\n        if \'vel_msg\' in dir(self):\n            self.canvas.delete(self.vel_msg)\n            self.canvas.delete(self.pos_msg)\n            self.canvas.delete(self.step_msg)\n            self.canvas.delete(self.velavg_msg)\n #           self.canvas.delete(self.velavg2_msg)\n #       self.velavg2_msg = self.canvas.create_text(850,190,text=velMsg2)\n        self.velavg_msg = self.canvas.create_text(650, 190, text=velMsg)\n        self.vel_msg = self.canvas.create_text(450, 190, text=velocityMsg)\n        self.pos_msg = self.canvas.create_text(250, 190, text=positionMsg)\n        self.step_msg = self.canvas.create_text(50, 190, text=stepMsg)\n#        self.lastPos = pos\n        self.lastStep = stepCount\n#        self.lastVel = velocity\n\n    def __init__(self, canvas):\n\n        ## Canvas ##\n        self.canvas = canvas\n        self.velAvg = 0\n#        self.velAvg2 = 0\n#        self.lastPos = 0\n        self.lastStep = 0\n#        self.lastVel = 0\n\n        ## Arm and Hand Degrees ##\n        self.armAngle = self.oldArmDegree = 0.0\n        self.handAngle = self.oldHandDegree = -PI/6\n\n        self.maxArmAngle = PI/6\n        self.minArmAngle = -PI/6\n\n        self.maxHandAngle = 0\n        self.minHandAngle = -(5.0/6.0) * PI\n\n        ## Draw Ground ##\n        self.totWidth = canvas.winfo_reqwidth()\n        self.totHeight = canvas.winfo_reqheight()\n        self.groundHeight = 40\n        self.groundY = self.totHeight - self.groundHeight\n\n        self.ground = canvas.create_rectangle(0,\n                                              self.groundY, self.totWidth, self.totHeight, fill=\'blue\')\n\n        ## Robot Body ##\n        self.robotWidth = 80\n        self.robotHeight = 40\n        self.robotPos = (20, self.groundY)\n        self.robotBody = canvas.create_polygon(\n            0, 0, 0, 0, 0, 0, 0, 0, fill=\'green\')\n\n        ## Robot Arm ##\n        self.armLength = 60\n        self.robotArm = canvas.create_line(0, 0, 0, 0, fill=\'orange\', width=5)\n\n        ## Robot Hand ##\n        self.handLength = 40\n        self.robotHand = canvas.create_line(0, 0, 0, 0, fill=\'red\', width=3)\n\n        self.positions = [0, 0]\n  #      self.angleSums = [0,0]\n\n\nif __name__ == \'__main__\':\n    from graphicsCrawlerDisplay import *\n    run()\n'"
week03_model_free/crawler_and_pacman/seminar_py3/environment.py,0,"b'# environment.py\n# --------------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\n#!/usr/bin/python\n\n\nclass Environment:\n\n    def getCurrentState(self):\n        """"""\n        Returns the current state of enviornment\n        """"""\n        abstract\n\n    def getPossibleActions(self, state):\n        """"""\n          Returns possible actions the agent \n          can take in the given state. Can\n          return the empty list if we are in \n          a terminal state.\n        """"""\n        abstract\n\n    def doAction(self, action):\n        """"""\n          Performs the given action in the current\n          environment state and updates the enviornment.\n\n          Returns a (reward, nextState) pair\n        """"""\n        abstract\n\n    def reset(self):\n        """"""\n          Resets the current state to the start state\n        """"""\n        abstract\n\n    def isTerminal(self):\n        """"""\n          Has the enviornment entered a terminal\n          state? This means there are no successors\n        """"""\n        state = self.getCurrentState()\n        actions = self.getPossibleActions(state)\n        return len(actions) == 0\n'"
week03_model_free/crawler_and_pacman/seminar_py3/featureExtractors.py,0,"b'# featureExtractors.py\n# --------------------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\n""Feature extractors for Pacman game states""\n\nfrom game import Directions, Actions\nimport util\n\n\nclass FeatureExtractor:\n    def getFeatures(self, state, action):\n        """"""\n          Returns a dict from features to counts\n          Usually, the count will just be 1.0 for\n          indicator functions.  \n        """"""\n        util.raiseNotDefined()\n\n\nclass IdentityExtractor(FeatureExtractor):\n    def getFeatures(self, state, action):\n        feats = util.Counter()\n        feats[(state, action)] = 1.0\n        return feats\n\n\ndef closestFood(pos, food, walls):\n    """"""\n    closestFood -- this is similar to the function that we have\n    worked on in the search project; here its all in one place\n    """"""\n    fringe = [(pos[0], pos[1], 0)]\n    expanded = set()\n    while fringe:\n        pos_x, pos_y, dist = fringe.pop(0)\n        if (pos_x, pos_y) in expanded:\n            continue\n        expanded.add((pos_x, pos_y))\n        # if we find a food at this location then exit\n        if food[pos_x][pos_y]:\n            return dist\n        # otherwise spread out from the location to its neighbours\n        nbrs = Actions.getLegalNeighbors((pos_x, pos_y), walls)\n        for nbr_x, nbr_y in nbrs:\n            fringe.append((nbr_x, nbr_y, dist+1))\n    # no food found\n    return None\n\n\nclass SimpleExtractor(FeatureExtractor):\n    """"""\n    Returns simple features for a basic reflex Pacman:\n    - whether food will be eaten\n    - how far away the next food is\n    - whether a ghost collision is imminent\n    - whether a ghost is one step away\n    """"""\n\n    def getFeatures(self, state, action):\n        # extract the grid of food and wall locations and get the ghost locations\n        food = state.getFood()\n        walls = state.getWalls()\n        ghosts = state.getGhostPositions()\n\n        features = util.Counter()\n\n        features[""bias""] = 1.0\n\n        # compute the location of pacman after he takes the action\n        x, y = state.getPacmanPosition()\n        dx, dy = Actions.directionToVector(action)\n        next_x, next_y = int(x + dx), int(y + dy)\n\n        # count the number of ghosts 1-step away\n        features[""#-of-ghosts-1-step-away""] = sum(\n            (next_x, next_y) in Actions.getLegalNeighbors(g, walls) for g in ghosts)\n\n        # if there is no danger of ghosts then add the food feature\n        if not features[""#-of-ghosts-1-step-away""] and food[next_x][next_y]:\n            features[""eats-food""] = 1.0\n\n        dist = closestFood((next_x, next_y), food, walls)\n        if dist is not None:\n            # make the distance a number less than one otherwise the update\n            # will diverge wildly\n            features[""closest-food""] = float(dist) / \\\n                (walls.width * walls.height)\n        features.divideAll(10.0)\n        return features\n'"
week03_model_free/crawler_and_pacman/seminar_py3/game.py,0,"b'# game.py\n# -------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nfrom util import *\nfrom util import raiseNotDefined\nimport time\nimport os\nimport traceback\n\ntry:\n    import boinc\n    _BOINC_ENABLED = True\nexcept:\n    _BOINC_ENABLED = False\n\n#######################\n# Parts worth reading #\n#######################\n\n\nclass Agent:\n    """"""\n    An agent must define a getAction method, but may also define the\n    following methods which will be called if they exist:\n\n    def registerInitialState(self, state): # inspects the starting state\n    """"""\n\n    def __init__(self, index=0):\n        self.index = index\n\n    def getAction(self, state):\n        """"""\n        The Agent will receive a GameState (from either {pacman, capture, sonar}.py) and\n        must return an action from Directions.{North, South, East, West, Stop}\n        """"""\n        raiseNotDefined()\n\n\nclass Directions:\n    NORTH = \'North\'\n    SOUTH = \'South\'\n    EAST = \'East\'\n    WEST = \'West\'\n    STOP = \'Stop\'\n\n    LEFT = {NORTH: WEST,\n            SOUTH: EAST,\n            EAST:  NORTH,\n            WEST:  SOUTH,\n            STOP:  STOP}\n\n    RIGHT = dict([(y, x) for x, y in list(LEFT.items())])\n\n    REVERSE = {NORTH: SOUTH,\n               SOUTH: NORTH,\n               EAST: WEST,\n               WEST: EAST,\n               STOP: STOP}\n\n\nclass Configuration:\n    """"""\n    A Configuration holds the (x,y) coordinate of a character, along with its\n    traveling direction.\n\n    The convention for positions, like a graph, is that (0,0) is the lower left corner, x increases\n    horizontally and y increases vertically.  Therefore, north is the direction of increasing y, or (0,1).\n    """"""\n\n    def __init__(self, pos, direction):\n        self.pos = pos\n        self.direction = direction\n\n    def getPosition(self):\n        return (self.pos)\n\n    def getDirection(self):\n        return self.direction\n\n    def isInteger(self):\n        x, y = self.pos\n        return x == int(x) and y == int(y)\n\n    def __eq__(self, other):\n        if other is None:\n            return False\n        return (self.pos == other.pos and self.direction == other.direction)\n\n    def __hash__(self):\n        x = hash(self.pos)\n        y = hash(self.direction)\n        return hash(x + 13 * y)\n\n    def __str__(self):\n        return ""(x,y)=""+str(self.pos)+"", ""+str(self.direction)\n\n    def generateSuccessor(self, vector):\n        """"""\n        Generates a new configuration reached by translating the current\n        configuration by the action vector.  This is a low-level call and does\n        not attempt to respect the legality of the movement.\n\n        Actions are movement vectors.\n        """"""\n        x, y = self.pos\n        dx, dy = vector\n        direction = Actions.vectorToDirection(vector)\n        if direction == Directions.STOP:\n            direction = self.direction  # There is no stop direction\n        return Configuration((x + dx, y+dy), direction)\n\n\nclass AgentState:\n    """"""\n    AgentStates hold the state of an agent (configuration, speed, scared, etc).\n    """"""\n\n    def __init__(self, startConfiguration, isPacman):\n        self.start = startConfiguration\n        self.configuration = startConfiguration\n        self.isPacman = isPacman\n        self.scaredTimer = 0\n\n    def __str__(self):\n        if self.isPacman:\n            return ""Pacman: "" + str(self.configuration)\n        else:\n            return ""Ghost: "" + str(self.configuration)\n\n    def __eq__(self, other):\n        if other is None:\n            return False\n        return self.configuration == other.configuration and self.scaredTimer == other.scaredTimer\n\n    def __hash__(self):\n        return hash(hash(self.configuration) + 13 * hash(self.scaredTimer))\n\n    def copy(self):\n        state = AgentState(self.start, self.isPacman)\n        state.configuration = self.configuration\n        state.scaredTimer = self.scaredTimer\n        return state\n\n    def getPosition(self):\n        if self.configuration is None:\n            return None\n        return self.configuration.getPosition()\n\n    def getDirection(self):\n        return self.configuration.getDirection()\n\n\nclass Grid:\n    """"""\n    A 2-dimensional array of objects backed by a list of lists.  Data is accessed\n    via grid[x][y] where (x,y) are positions on a Pacman map with x horizontal,\n    y vertical and the origin (0,0) in the bottom left corner.\n\n    The __str__ method constructs an output that is oriented like a pacman board.\n    """"""\n\n    def __init__(self, width, height, initialValue=False, bitRepresentation=None):\n        if initialValue not in [False, True]:\n            raise Exception(\'Grids can only contain booleans\')\n        self.CELLS_PER_INT = 30\n\n        self.width = width\n        self.height = height\n        self.data = [[initialValue for y in range(\n            height)] for x in range(width)]\n        if bitRepresentation:\n            self._unpackBits(bitRepresentation)\n\n    def __getitem__(self, i):\n        return self.data[i]\n\n    def __setitem__(self, key, item):\n        self.data[key] = item\n\n    def __str__(self):\n        out = [[str(self.data[x][y])[0] for x in range(self.width)]\n               for y in range(self.height)]\n        out.reverse()\n        return \'\\n\'.join([\'\'.join(x) for x in out])\n\n    def __eq__(self, other):\n        if other is None:\n            return False\n        return self.data == other.data\n\n    def __hash__(self):\n        # return hash(str(self))\n        base = 1\n        h = 0\n        for l in self.data:\n            for i in l:\n                if i:\n                    h += base\n                base *= 2\n        return hash(h)\n\n    def copy(self):\n        g = Grid(self.width, self.height)\n        g.data = [x[:] for x in self.data]\n        return g\n\n    def deepCopy(self):\n        return self.copy()\n\n    def shallowCopy(self):\n        g = Grid(self.width, self.height)\n        g.data = self.data\n        return g\n\n    def count(self, item=True):\n        return sum([x.count(item) for x in self.data])\n\n    def asList(self, key=True):\n        list = []\n        for x in range(self.width):\n            for y in range(self.height):\n                if self[x][y] == key:\n                    list.append((x, y))\n        return list\n\n    def packBits(self):\n        """"""\n        Returns an efficient int list representation\n\n        (width, height, bitPackedInts...)\n        """"""\n        bits = [self.width, self.height]\n        currentInt = 0\n        for i in range(self.height * self.width):\n            bit = self.CELLS_PER_INT - (i % self.CELLS_PER_INT) - 1\n            x, y = self._cellIndexToPosition(i)\n            if self[x][y]:\n                currentInt += 2 ** bit\n            if (i + 1) % self.CELLS_PER_INT == 0:\n                bits.append(currentInt)\n                currentInt = 0\n        bits.append(currentInt)\n        return tuple(bits)\n\n    def _cellIndexToPosition(self, index):\n        x = index / self.height\n        y = index % self.height\n        return x, y\n\n    def _unpackBits(self, bits):\n        """"""\n        Fills in data from a bit-level representation\n        """"""\n        cell = 0\n        for packed in bits:\n            for bit in self._unpackInt(packed, self.CELLS_PER_INT):\n                if cell == self.width * self.height:\n                    break\n                x, y = self._cellIndexToPosition(cell)\n                self[x][y] = bit\n                cell += 1\n\n    def _unpackInt(self, packed, size):\n        bools = []\n        if packed < 0:\n            raise ValueError(""must be a positive integer"")\n        for i in range(size):\n            n = 2 ** (self.CELLS_PER_INT - i - 1)\n            if packed >= n:\n                bools.append(True)\n                packed -= n\n            else:\n                bools.append(False)\n        return bools\n\n\ndef reconstituteGrid(bitRep):\n    if type(bitRep) is not type((1, 2)):\n        return bitRep\n    width, height = bitRep[:2]\n    return Grid(width, height, bitRepresentation=bitRep[2:])\n\n####################################\n# Parts you shouldn\'t have to read #\n####################################\n\n\nclass Actions:\n    """"""\n    A collection of static methods for manipulating move actions.\n    """"""\n    # Directions\n    _directions = {Directions.NORTH: (0, 1),\n                   Directions.SOUTH: (0, -1),\n                   Directions.EAST:  (1, 0),\n                   Directions.WEST:  (-1, 0),\n                   Directions.STOP:  (0, 0)}\n\n    _directionsAsList = list(_directions.items())\n\n    TOLERANCE = .001\n\n    def reverseDirection(action):\n        if action == Directions.NORTH:\n            return Directions.SOUTH\n        if action == Directions.SOUTH:\n            return Directions.NORTH\n        if action == Directions.EAST:\n            return Directions.WEST\n        if action == Directions.WEST:\n            return Directions.EAST\n        return action\n    reverseDirection = staticmethod(reverseDirection)\n\n    def vectorToDirection(vector):\n        dx, dy = vector\n        if dy > 0:\n            return Directions.NORTH\n        if dy < 0:\n            return Directions.SOUTH\n        if dx < 0:\n            return Directions.WEST\n        if dx > 0:\n            return Directions.EAST\n        return Directions.STOP\n    vectorToDirection = staticmethod(vectorToDirection)\n\n    def directionToVector(direction, speed=1.0):\n        dx, dy = Actions._directions[direction]\n        return (dx * speed, dy * speed)\n    directionToVector = staticmethod(directionToVector)\n\n    def getPossibleActions(config, walls):\n        possible = []\n        x, y = config.pos\n        x_int, y_int = int(x + 0.5), int(y + 0.5)\n\n        # In between grid points, all agents must continue straight\n        if (abs(x - x_int) + abs(y - y_int) > Actions.TOLERANCE):\n            return [config.getDirection()]\n\n        for dir, vec in Actions._directionsAsList:\n            dx, dy = vec\n            next_y = y_int + dy\n            next_x = x_int + dx\n            if not walls[next_x][next_y]:\n                possible.append(dir)\n\n        return possible\n\n    getPossibleActions = staticmethod(getPossibleActions)\n\n    def getLegalNeighbors(position, walls):\n        x, y = position\n        x_int, y_int = int(x + 0.5), int(y + 0.5)\n        neighbors = []\n        for dir, vec in Actions._directionsAsList:\n            dx, dy = vec\n            next_x = x_int + dx\n            if next_x < 0 or next_x == walls.width:\n                continue\n            next_y = y_int + dy\n            if next_y < 0 or next_y == walls.height:\n                continue\n            if not walls[next_x][next_y]:\n                neighbors.append((next_x, next_y))\n        return neighbors\n    getLegalNeighbors = staticmethod(getLegalNeighbors)\n\n    def getSuccessor(position, action):\n        dx, dy = Actions.directionToVector(action)\n        x, y = position\n        return (x + dx, y + dy)\n    getSuccessor = staticmethod(getSuccessor)\n\n\nclass GameStateData:\n    """"""\n\n    """"""\n\n    def __init__(self, prevState=None):\n        """"""\n        Generates a new data packet by copying information from its predecessor.\n        """"""\n        if prevState is not None:\n            self.food = prevState.food.shallowCopy()\n            self.capsules = prevState.capsules[:]\n            self.agentStates = self.copyAgentStates(prevState.agentStates)\n            self.layout = prevState.layout\n            self._eaten = prevState._eaten\n            self.score = prevState.score\n        self._foodEaten = None\n        self._capsuleEaten = None\n        self._agentMoved = None\n        self._lose = False\n        self._win = False\n        self.scoreChange = 0\n\n    def deepCopy(self):\n        state = GameStateData(self)\n        state.food = self.food.deepCopy()\n        state.layout = self.layout.deepCopy()\n        state._agentMoved = self._agentMoved\n        state._foodEaten = self._foodEaten\n        state._capsuleEaten = self._capsuleEaten\n        return state\n\n    def copyAgentStates(self, agentStates):\n        copiedStates = []\n        for agentState in agentStates:\n            copiedStates.append(agentState.copy())\n        return copiedStates\n\n    def __eq__(self, other):\n        """"""\n        Allows two states to be compared.\n        """"""\n        if other is None:\n            return False\n        # TODO Check for type of other\n        if not self.agentStates == other.agentStates:\n            return False\n        if not self.food == other.food:\n            return False\n        if not self.capsules == other.capsules:\n            return False\n        if not self.score == other.score:\n            return False\n        return True\n\n    def __hash__(self):\n        """"""\n        Allows states to be keys of dictionaries.\n        """"""\n        for i, state in enumerate(self.agentStates):\n            try:\n                int(hash(state))\n            except TypeError as e:\n                print(e)\n                # hash(state)\n        return int((hash(tuple(self.agentStates)) + 13*hash(self.food) + 113 * hash(tuple(self.capsules)) + 7 * hash(self.score)) % 1048575)\n\n    def __str__(self):\n        width, height = self.layout.width, self.layout.height\n        map = Grid(width, height)\n        if type(self.food) == type((1, 2)):\n            self.food = reconstituteGrid(self.food)\n        for x in range(width):\n            for y in range(height):\n                food, walls = self.food, self.layout.walls\n                map[x][y] = self._foodWallStr(food[x][y], walls[x][y])\n\n        for agentState in self.agentStates:\n            if agentState is None:\n                continue\n            if agentState.configuration is None:\n                continue\n            x, y = [int(i) for i in nearestPoint(agentState.configuration.pos)]\n            agent_dir = agentState.configuration.direction\n            if agentState.isPacman:\n                map[x][y] = self._pacStr(agent_dir)\n            else:\n                map[x][y] = self._ghostStr(agent_dir)\n\n        for x, y in self.capsules:\n            map[x][y] = \'o\'\n\n        return str(map) + (""\\nScore: %d\\n"" % self.score)\n\n    def _foodWallStr(self, hasFood, hasWall):\n        if hasFood:\n            return \'.\'\n        elif hasWall:\n            return \'%\'\n        else:\n            return \' \'\n\n    def _pacStr(self, dir):\n        if dir == Directions.NORTH:\n            return \'v\'\n        if dir == Directions.SOUTH:\n            return \'^\'\n        if dir == Directions.WEST:\n            return \'>\'\n        return \'<\'\n\n    def _ghostStr(self, dir):\n        return \'G\'\n        if dir == Directions.NORTH:\n            return \'M\'\n        if dir == Directions.SOUTH:\n            return \'W\'\n        if dir == Directions.WEST:\n            return \'3\'\n        return \'E\'\n\n    def initialize(self, layout, numGhostAgents):\n        """"""\n        Creates an initial game state from a layout array (see layout.py).\n        """"""\n        self.food = layout.food.copy()\n        self.capsules = layout.capsules[:]\n        self.layout = layout\n        self.score = 0\n        self.scoreChange = 0\n\n        self.agentStates = []\n        numGhosts = 0\n        for isPacman, pos in layout.agentPositions:\n            if not isPacman:\n                if numGhosts == numGhostAgents:\n                    continue  # Max ghosts reached already\n                else:\n                    numGhosts += 1\n            self.agentStates.append(AgentState(\n                Configuration(pos, Directions.STOP), isPacman))\n        self._eaten = [False for a in self.agentStates]\n\n\nclass Game:\n    """"""\n    The Game manages the control flow, soliciting actions from agents.\n    """"""\n\n    def __init__(self, agents, display, rules, startingIndex=0, muteAgents=False, catchExceptions=False):\n        self.agentCrashed = False\n        self.agents = agents\n        self.display = display\n        self.rules = rules\n        self.startingIndex = startingIndex\n        self.gameOver = False\n        self.muteAgents = muteAgents\n        self.catchExceptions = catchExceptions\n        self.moveHistory = []\n        self.totalAgentTimes = [0 for agent in agents]\n        self.totalAgentTimeWarnings = [0 for agent in agents]\n        self.agentTimeout = False\n\n    def getProgress(self):\n        if self.gameOver:\n            return 1.0\n        else:\n            return self.rules.getProgress(self)\n\n    def _agentCrash(self, agentIndex, quiet=False):\n        ""Helper method for handling agent crashes""\n        if not quiet:\n            traceback.print_exc()\n        self.gameOver = True\n        self.agentCrashed = True\n        self.rules.agentCrash(self, agentIndex)\n\n    OLD_STDOUT = None\n    OLD_STDERR = None\n\n    def mute(self):\n        if not self.muteAgents:\n            return\n        global OLD_STDOUT, OLD_STDERR\n        import io\n        OLD_STDOUT = sys.stdout\n        OLD_STDERR = sys.stderr\n        sys.stdout = io.StringIO()\n        sys.stderr = io.StringIO()\n\n    def unmute(self):\n        if not self.muteAgents:\n            return\n        global OLD_STDOUT, OLD_STDERR\n        sys.stdout.close()\n        sys.stderr.close()\n        # Revert stdout/stderr to originals\n        sys.stdout = OLD_STDOUT\n        sys.stderr = OLD_STDERR\n\n    def run(self):\n        """"""\n        Main control loop for game play.\n        """"""\n        self.display.initialize(self.state.data)\n        self.numMoves = 0\n\n        # self.display.initialize(self.state.makeObservation(1).data)\n        # inform learning agents of the game start\n        for i in range(len(self.agents)):\n            agent = self.agents[i]\n            if not agent:\n                # this is a null agent, meaning it failed to load\n                # the other team wins\n                self._agentCrash(i, quiet=True)\n                return\n            if (""registerInitialState"" in dir(agent)):\n                self.mute()\n                if self.catchExceptions:\n                    try:\n                        timed_func = TimeoutFunction(\n                            agent.registerInitialState, int(self.rules.getMaxStartupTime(i)))\n                        try:\n                            start_time = time.time()\n                            timed_func(self.state.deepCopy())\n                            time_taken = time.time() - start_time\n                            self.totalAgentTimes[i] += time_taken\n                        except TimeoutFunctionException:\n                            print(""Agent %d ran out of time on startup!"" % i)\n                            self.unmute()\n                            self.agentTimeout = True\n                            self._agentCrash(i, quiet=True)\n                            return\n                    except Exception as data:\n                        self.unmute()\n                        self._agentCrash(i, quiet=True)\n                        return\n                else:\n                    agent.registerInitialState(self.state.deepCopy())\n                # TODO: could this exceed the total time\n                self.unmute()\n\n        agentIndex = self.startingIndex\n        numAgents = len(self.agents)\n\n        while not self.gameOver:\n            # Fetch the next agent\n            agent = self.agents[agentIndex]\n            move_time = 0\n            skip_action = False\n            # Generate an observation of the state\n            if \'observationFunction\' in dir(agent):\n                self.mute()\n                if self.catchExceptions:\n                    try:\n                        timed_func = TimeoutFunction(agent.observationFunction, int(\n                            self.rules.getMoveTimeout(agentIndex)))\n                        try:\n                            start_time = time.time()\n                            observation = timed_func(self.state.deepCopy())\n                        except TimeoutFunctionException:\n                            skip_action = True\n                        move_time += time.time() - start_time\n                        self.unmute()\n                    except Exception as data:\n                        self.unmute()\n                        self._agentCrash(agentIndex, quiet=True)\n                        return\n                else:\n                    observation = agent.observationFunction(\n                        self.state.deepCopy())\n                self.unmute()\n            else:\n                observation = self.state.deepCopy()\n\n            # Solicit an action\n            action = None\n            self.mute()\n            if self.catchExceptions:\n                try:\n                    timed_func = TimeoutFunction(agent.getAction, int(\n                        self.rules.getMoveTimeout(agentIndex)) - int(move_time))\n                    try:\n                        start_time = time.time()\n                        if skip_action:\n                            raise TimeoutFunctionException()\n                        action = timed_func(observation)\n                    except TimeoutFunctionException:\n                        print(""Agent %d timed out on a single move!"" %\n                              agentIndex)\n                        self.agentTimeout = True\n                        self.unmute()\n                        self._agentCrash(agentIndex, quiet=True)\n                        return\n\n                    move_time += time.time() - start_time\n\n                    if move_time > self.rules.getMoveWarningTime(agentIndex):\n                        self.totalAgentTimeWarnings[agentIndex] += 1\n                        print(""Agent %d took too long to make a move! This is warning %d"" % (\n                            agentIndex, self.totalAgentTimeWarnings[agentIndex]))\n                        if self.totalAgentTimeWarnings[agentIndex] > self.rules.getMaxTimeWarnings(agentIndex):\n                            print(""Agent %d exceeded the maximum number of warnings: %d"" % (\n                                agentIndex, self.totalAgentTimeWarnings[agentIndex]))\n                            self.agentTimeout = True\n                            self.unmute()\n                            self._agentCrash(agentIndex, quiet=True)\n\n                    self.totalAgentTimes[agentIndex] += move_time\n                    # print ""Agent: %d, time: %f, total: %f"" % (agentIndex, move_time, self.totalAgentTimes[agentIndex])\n                    if self.totalAgentTimes[agentIndex] > self.rules.getMaxTotalTime(agentIndex):\n                        print(""Agent %d ran out of time! (time: %1.2f)"" %\n                              (agentIndex, self.totalAgentTimes[agentIndex]))\n                        self.agentTimeout = True\n                        self.unmute()\n                        self._agentCrash(agentIndex, quiet=True)\n                        return\n                    self.unmute()\n                except Exception as data:\n                    self.unmute()\n                    self._agentCrash(agentIndex)\n                    return\n            else:\n                action = agent.getAction(observation)\n            self.unmute()\n\n            # Execute the action\n            self.moveHistory.append((agentIndex, action))\n            if self.catchExceptions:\n                try:\n                    self.state = self.state.generateSuccessor(\n                        agentIndex, action)\n                except Exception as data:\n                    self._agentCrash(agentIndex)\n                    return\n            else:\n                self.state = self.state.generateSuccessor(agentIndex, action)\n\n            # Change the display\n            self.display.update(self.state.data)\n            ###idx = agentIndex - agentIndex % 2 + 1\n            ###self.display.update( self.state.makeObservation(idx).data )\n\n            # Allow for game specific conditions (winning, losing, etc.)\n            self.rules.process(self.state, self)\n            # Track progress\n            if agentIndex == numAgents + 1:\n                self.numMoves += 1\n            # Next agent\n            agentIndex = (agentIndex + 1) % numAgents\n\n            if _BOINC_ENABLED:\n                boinc.set_fraction_done(self.getProgress())\n\n        # inform a learning agent of the game result\n        for agent in self.agents:\n            if ""final"" in dir(agent):\n                try:\n                    self.mute()\n                    agent.final(self.state)\n                    self.unmute()\n                except Exception as data:\n                    if not self.catchExceptions:\n                        raise\n                    self.unmute()\n                    print(""Exception"", data)\n                    self._agentCrash(agent.index)\n                    return\n        self.display.finish()\n'"
week03_model_free/crawler_and_pacman/seminar_py3/ghostAgents.py,0,"b'# ghostAgents.py\n# --------------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nfrom game import Agent\nfrom game import Actions\nfrom game import Directions\nimport random\nfrom util import manhattanDistance\nimport util\n\n\nclass GhostAgent(Agent):\n    def __init__(self, index):\n        self.index = index\n\n    def getAction(self, state):\n        dist = self.getDistribution(state)\n        if len(dist) == 0:\n            return Directions.STOP\n        else:\n            return util.chooseFromDistribution(dist)\n\n    def getDistribution(self, state):\n        ""Returns a Counter encoding a distribution over actions from the provided state.""\n        util.raiseNotDefined()\n\n\nclass RandomGhost(GhostAgent):\n    ""A ghost that chooses a legal action uniformly at random.""\n\n    def getDistribution(self, state):\n        dist = util.Counter()\n        for a in state.getLegalActions(self.index):\n            dist[a] = 1.0\n        dist.normalize()\n        return dist\n\n\nclass DirectionalGhost(GhostAgent):\n    ""A ghost that prefers to rush Pacman, or flee when scared.""\n\n    def __init__(self, index, prob_attack=0.8, prob_scaredFlee=0.8):\n        self.index = index\n        self.prob_attack = prob_attack\n        self.prob_scaredFlee = prob_scaredFlee\n\n    def getDistribution(self, state):\n        # Read variables from state\n        ghostState = state.getGhostState(self.index)\n        legalActions = state.getLegalActions(self.index)\n        pos = state.getGhostPosition(self.index)\n        isScared = ghostState.scaredTimer > 0\n\n        speed = 1\n        if isScared:\n            speed = 0.5\n\n        actionVectors = [Actions.directionToVector(\n            a, speed) for a in legalActions]\n        newPositions = [(pos[0]+a[0], pos[1]+a[1]) for a in actionVectors]\n        pacmanPosition = state.getPacmanPosition()\n\n        # Select best actions given the state\n        distancesToPacman = [manhattanDistance(\n            pos, pacmanPosition) for pos in newPositions]\n        if isScared:\n            bestScore = max(distancesToPacman)\n            bestProb = self.prob_scaredFlee\n        else:\n            bestScore = min(distancesToPacman)\n            bestProb = self.prob_attack\n        bestActions = [action for action, distance in zip(\n            legalActions, distancesToPacman) if distance == bestScore]\n\n        # Construct distribution\n        dist = util.Counter()\n        for a in bestActions:\n            dist[a] = bestProb / len(bestActions)\n        for a in legalActions:\n            dist[a] += (1-bestProb) / len(legalActions)\n        dist.normalize()\n        return dist\n'"
week03_model_free/crawler_and_pacman/seminar_py3/graphicsCrawlerDisplay.py,0,"b'# graphicsCrawlerDisplay.py\n# -------------------------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nimport tkinter\nimport qlearningAgents\nimport time\nimport threading\nimport sys\nimport crawler\n#import pendulum\nimport math\nfrom math import pi as PI\n\nrobotType = \'crawler\'\n\n\nclass Application:\n\n    def sigmoid(self, x):\n        return 1.0 / (1.0 + 2.0 ** (-x))\n\n    def incrementSpeed(self, inc):\n        self.tickTime *= inc\n#        self.epsilon = min(1.0, self.epsilon)\n#        self.epsilon = max(0.0,self.epsilon)\n#        self.learner.setSpeed(self.epsilon)\n        self.speed_label[\'text\'] = \'Step Delay: %.5f\' % (self.tickTime)\n\n    def incrementEpsilon(self, inc):\n        self.ep += inc\n        self.epsilon = self.sigmoid(self.ep)\n        self.learner.setEpsilon(self.epsilon)\n        self.epsilon_label[\'text\'] = \'Epsilon: %.3f\' % (self.epsilon)\n\n    def incrementGamma(self, inc):\n        self.ga += inc\n        self.gamma = self.sigmoid(self.ga)\n        self.learner.setDiscount(self.gamma)\n        self.gamma_label[\'text\'] = \'Discount: %.3f\' % (self.gamma)\n\n    def incrementAlpha(self, inc):\n        self.al += inc\n        self.alpha = self.sigmoid(self.al)\n        self.learner.setLearningRate(self.alpha)\n        self.alpha_label[\'text\'] = \'Learning Rate: %.3f\' % (self.alpha)\n\n    def __initGUI(self, win):\n        ## Window ##\n        self.win = win\n\n        ## Initialize Frame ##\n        win.grid()\n        self.dec = -.5\n        self.inc = .5\n        self.tickTime = 0.1\n\n        ## Epsilon Button + Label ##\n        self.setupSpeedButtonAndLabel(win)\n\n        self.setupEpsilonButtonAndLabel(win)\n\n        ## Gamma Button + Label ##\n        self.setUpGammaButtonAndLabel(win)\n\n        ## Alpha Button + Label ##\n        self.setupAlphaButtonAndLabel(win)\n\n        ## Exit Button ##\n        #self.exit_button = Tkinter.Button(win,text=\'Quit\', command=self.exit)\n        #self.exit_button.grid(row=0, column=9)\n\n        ## Simulation Buttons ##\n#        self.setupSimulationButtons(win)\n\n        ## Canvas ##\n        self.canvas = tkinter.Canvas(root, height=200, width=1000)\n        self.canvas.grid(row=2, columnspan=10)\n\n    def setupAlphaButtonAndLabel(self, win):\n        self.alpha_minus = tkinter.Button(win,\n                                          text=""-"", command=(lambda: self.incrementAlpha(self.dec)))\n        self.alpha_minus.grid(row=1, column=3, padx=10)\n\n        self.alpha = self.sigmoid(self.al)\n        self.alpha_label = tkinter.Label(\n            win, text=\'Learning Rate: %.3f\' % (self.alpha))\n        self.alpha_label.grid(row=1, column=4)\n\n        self.alpha_plus = tkinter.Button(win,\n                                         text=""+"", command=(lambda: self.incrementAlpha(self.inc)))\n        self.alpha_plus.grid(row=1, column=5, padx=10)\n\n    def setUpGammaButtonAndLabel(self, win):\n        self.gamma_minus = tkinter.Button(win,\n                                          text=""-"", command=(lambda: self.incrementGamma(self.dec)))\n        self.gamma_minus.grid(row=1, column=0, padx=10)\n\n        self.gamma = self.sigmoid(self.ga)\n        self.gamma_label = tkinter.Label(\n            win, text=\'Discount: %.3f\' % (self.gamma))\n        self.gamma_label.grid(row=1, column=1)\n\n        self.gamma_plus = tkinter.Button(win,\n                                         text=""+"", command=(lambda: self.incrementGamma(self.inc)))\n        self.gamma_plus.grid(row=1, column=2, padx=10)\n\n    def setupEpsilonButtonAndLabel(self, win):\n        self.epsilon_minus = tkinter.Button(win,\n                                            text=""-"", command=(lambda: self.incrementEpsilon(self.dec)))\n        self.epsilon_minus.grid(row=0, column=3)\n\n        self.epsilon = self.sigmoid(self.ep)\n        self.epsilon_label = tkinter.Label(\n            win, text=\'Epsilon: %.3f\' % (self.epsilon))\n        self.epsilon_label.grid(row=0, column=4)\n\n        self.epsilon_plus = tkinter.Button(win,\n                                           text=""+"", command=(lambda: self.incrementEpsilon(self.inc)))\n        self.epsilon_plus.grid(row=0, column=5)\n\n    def setupSpeedButtonAndLabel(self, win):\n        self.speed_minus = tkinter.Button(win,\n                                          text=""-"", command=(lambda: self.incrementSpeed(.5)))\n        self.speed_minus.grid(row=0, column=0)\n\n        self.speed_label = tkinter.Label(\n            win, text=\'Step Delay: %.5f\' % (self.tickTime))\n        self.speed_label.grid(row=0, column=1)\n\n        self.speed_plus = tkinter.Button(win,\n                                         text=""+"", command=(lambda: self.incrementSpeed(2)))\n        self.speed_plus.grid(row=0, column=2)\n\n    def skip5kSteps(self):\n        self.stepsToSkip = 5000\n\n    def __init__(self, win):\n\n        self.ep = 0\n        self.ga = 2\n        self.al = 2\n        self.stepCount = 0\n        # Init Gui\n\n        self.__initGUI(win)\n\n        # Init environment\n        if robotType == \'crawler\':\n            self.robot = crawler.CrawlingRobot(self.canvas)\n            self.robotEnvironment = crawler.CrawlingRobotEnvironment(\n                self.robot)\n        elif robotType == \'pendulum\':\n            self.robot = pendulum.PendulumRobot(self.canvas)\n            self.robotEnvironment = \\\n                pendulum.PendulumRobotEnvironment(self.robot)\n        else:\n            raise ValueError(\'Unknown RobotType\')\n\n        # Init Agent\n        def simulationFn(agent): return \\\n            simulation.SimulationEnvironment(self.robotEnvironment, agent)\n        def actionFn(state): return \\\n            self.robotEnvironment.getPossibleActions(state)\n        self.learner = qlearningAgents.QLearningAgent(actionFn=actionFn)\n\n        self.learner.setEpsilon(self.epsilon)\n        self.learner.setLearningRate(self.alpha)\n        self.learner.setDiscount(self.gamma)\n\n        # Start GUI\n        self.running = True\n        self.stopped = False\n        self.stepsToSkip = 0\n        self.thread = threading.Thread(target=self.run)\n        self.thread.start()\n\n    def exit(self):\n        self.running = False\n        for i in range(5):\n            if not self.stopped:\n                #          print ""Waiting for thread to die...""\n                time.sleep(0.1)\n        self.win.destroy()\n        sys.exit(0)\n\n    def step(self):\n\n        self.stepCount += 1\n\n        state = self.robotEnvironment.getCurrentState()\n        actions = self.robotEnvironment.getPossibleActions(state)\n        if len(actions) == 0.0:\n            self.robotEnvironment.reset()\n            state = self.robotEnvironment.getCurrentState()\n            actions = self.robotEnvironment.getPossibleActions(state)\n            print(\'Reset!\')\n        action = self.learner.getAction(state)\n        if action is None:\n            raise RuntimeError(\'None action returned: Code Not Complete\')\n        nextState, reward = self.robotEnvironment.doAction(action)\n        self.learner.observeTransition(state, action, nextState, reward)\n\n    def animatePolicy(self):\n        if robotType != \'pendulum\':\n            raise RuntimeError(\'Only pendulum can animatePolicy\')\n\n        totWidth = self.canvas.winfo_reqwidth()\n        totHeight = self.canvas.winfo_reqheight()\n\n        length = 0.48 * min(totWidth, totHeight)\n        x, y = totWidth-length-30, length+10\n\n        angleMin, angleMax = self.robot.getMinAndMaxAngle()\n        velMin, velMax = self.robot.getMinAndMaxAngleVelocity()\n\n        if \'animatePolicyBox\' not in dir(self):\n            self.canvas.create_line(x, y, x+length, y)\n            self.canvas.create_line(x+length, y, x+length, y-length)\n            self.canvas.create_line(x+length, y-length, x, y-length)\n            self.canvas.create_line(x, y-length, x, y)\n            self.animatePolicyBox = 1\n            self.canvas.create_text(x+length/2, y+10, text=\'angle\')\n            self.canvas.create_text(x-30, y-length/2, text=\'velocity\')\n            self.canvas.create_text(x-60, y-length/4, text=\'Blue = kickLeft\')\n            self.canvas.create_text(\n                x-60, y-length/4+20, text=\'Red = kickRight\')\n            self.canvas.create_text(\n                x-60, y-length/4+40, text=\'White = doNothing\')\n\n        angleDelta = (angleMax-angleMin) / 100\n        velDelta = (velMax-velMin) / 100\n        for i in range(100):\n            angle = angleMin + i * angleDelta\n\n            for j in range(100):\n                vel = velMin + j * velDelta\n                state = self.robotEnvironment.getState(angle, vel)\n                max, argMax = None, None\n                if not self.learner.seenState(state):\n                    argMax = \'unseen\'\n                else:\n                    for action in (\'kickLeft\', \'kickRight\', \'doNothing\'):\n                        qVal = self.learner.getQValue(state, action)\n                        if max is None or qVal > max:\n                            max, argMax = qVal, action\n                if argMax != \'unseen\':\n                    if argMax == \'kickLeft\':\n                        color = \'blue\'\n                    elif argMax == \'kickRight\':\n                        color = \'red\'\n                    elif argMax == \'doNothing\':\n                        color = \'white\'\n                    dx = length / 100.0\n                    dy = length / 100.0\n                    x0, y0 = x+i*dx, y-j*dy\n                    self.canvas.create_rectangle(\n                        x0, y0, x0+dx, y0+dy, fill=color)\n\n    def run(self):\n        self.stepCount = 0\n        self.learner.startEpisode()\n        while True:\n            minSleep = .01\n            tm = max(minSleep, self.tickTime)\n            time.sleep(tm)\n            self.stepsToSkip = int(tm / self.tickTime) - 1\n\n            if not self.running:\n                self.stopped = True\n                return\n            for i in range(self.stepsToSkip):\n                self.step()\n            self.stepsToSkip = 0\n            self.step()\n#          self.robot.draw()\n        self.learner.stopEpisode()\n\n    def start(self):\n        self.win.mainloop()\n\n\ndef run():\n    global root\n    root = tkinter.Tk()\n    root.title(\'Crawler GUI\')\n    root.resizable(0, 0)\n\n#  root.mainloop()\n\n    app = Application(root)\n\n    def update_gui():\n        app.robot.draw(app.stepCount, app.tickTime)\n        root.after(10, update_gui)\n    update_gui()\n\n    root.protocol(\'WM_DELETE_WINDOW\', app.exit)\n    app.start()\n'"
week03_model_free/crawler_and_pacman/seminar_py3/graphicsDisplay.py,0,"b'# graphicsDisplay.py\n# ------------------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nimport os\nfrom graphicsUtils import *\nimport math\nimport time\nfrom game import Directions\n\n###########################\n#  GRAPHICS DISPLAY CODE  #\n###########################\n\n# Most code by Dan Klein and John Denero written or rewritten for cs188, UC Berkeley.\n# Some code from a Pacman implementation by LiveWires, and used / modified with permission.\n\nDEFAULT_GRID_SIZE = 30.0\nINFO_PANE_HEIGHT = 35\nBACKGROUND_COLOR = formatColor(0, 0, 0)\nWALL_COLOR = formatColor(0.0/255.0, 51.0/255.0, 255.0/255.0)\nINFO_PANE_COLOR = formatColor(.4, .4, 0)\nSCORE_COLOR = formatColor(.9, .9, .9)\nPACMAN_OUTLINE_WIDTH = 2\nPACMAN_CAPTURE_OUTLINE_WIDTH = 4\n\nGHOST_COLORS = []\nGHOST_COLORS.append(formatColor(.9, 0, 0))  # Red\nGHOST_COLORS.append(formatColor(0, .3, .9))  # Blue\nGHOST_COLORS.append(formatColor(.98, .41, .07))  # Orange\nGHOST_COLORS.append(formatColor(.1, .75, .7))  # Green\nGHOST_COLORS.append(formatColor(1.0, 0.6, 0.0))  # Yellow\nGHOST_COLORS.append(formatColor(.4, 0.13, 0.91))  # Purple\n\nTEAM_COLORS = GHOST_COLORS[:2]\n\nGHOST_SHAPE = [\n    (0,    0.3),\n    (0.25, 0.75),\n    (0.5,  0.3),\n    (0.75, 0.75),\n    (0.75, -0.5),\n    (0.5,  -0.75),\n    (-0.5,  -0.75),\n    (-0.75, -0.5),\n    (-0.75, 0.75),\n    (-0.5,  0.3),\n    (-0.25, 0.75)\n]\nGHOST_SIZE = 0.65\nSCARED_COLOR = formatColor(1, 1, 1)\n\nGHOST_VEC_COLORS = list(map(colorToVector, GHOST_COLORS))\n\nPACMAN_COLOR = formatColor(255.0/255.0, 255.0/255.0, 61.0/255)\nPACMAN_SCALE = 0.5\n#pacman_speed = 0.25\n\n# Food\nFOOD_COLOR = formatColor(1, 1, 1)\nFOOD_SIZE = 0.1\n\n# Laser\nLASER_COLOR = formatColor(1, 0, 0)\nLASER_SIZE = 0.02\n\n# Capsule graphics\nCAPSULE_COLOR = formatColor(1, 1, 1)\nCAPSULE_SIZE = 0.25\n\n# Drawing walls\nWALL_RADIUS = 0.15\n\n\nclass InfoPane:\n    def __init__(self, layout, gridSize):\n        self.gridSize = gridSize\n        self.width = (layout.width) * gridSize\n        self.base = (layout.height + 1) * gridSize\n        self.height = INFO_PANE_HEIGHT\n        self.fontSize = 24\n        self.textColor = PACMAN_COLOR\n        self.drawPane()\n\n    def toScreen(self, pos, y=None):\n        """"""\n          Translates a point relative from the bottom left of the info pane.\n        """"""\n        if y is None:\n            x, y = pos\n        else:\n            x = pos\n\n        x = self.gridSize + x  # Margin\n        y = self.base + y\n        return x, y\n\n    def drawPane(self):\n        self.scoreText = text(self.toScreen(\n            0, 0), self.textColor, ""SCORE:    0"", ""Times"", self.fontSize, ""bold"")\n\n    def initializeGhostDistances(self, distances):\n        self.ghostDistanceText = []\n\n        size = 20\n        if self.width < 240:\n            size = 12\n        if self.width < 160:\n            size = 10\n\n        for i, d in enumerate(distances):\n            t = text(self.toScreen(self.width/2 + self.width/8 * i, 0),\n                     GHOST_COLORS[i+1], d, ""Times"", size, ""bold"")\n            self.ghostDistanceText.append(t)\n\n    def updateScore(self, score):\n        changeText(self.scoreText, ""SCORE: % 4d"" % score)\n\n    def setTeam(self, isBlue):\n        text = ""RED TEAM""\n        if isBlue:\n            text = ""BLUE TEAM""\n        self.teamText = text(self.toScreen(\n            300, 0), self.textColor, text, ""Times"", self.fontSize, ""bold"")\n\n    def updateGhostDistances(self, distances):\n        if len(distances) == 0:\n            return\n        if \'ghostDistanceText\' not in dir(self):\n            self.initializeGhostDistances(distances)\n        else:\n            for i, d in enumerate(distances):\n                changeText(self.ghostDistanceText[i], d)\n\n    def drawGhost(self):\n        pass\n\n    def drawPacman(self):\n        pass\n\n    def drawWarning(self):\n        pass\n\n    def clearIcon(self):\n        pass\n\n    def updateMessage(self, message):\n        pass\n\n    def clearMessage(self):\n        pass\n\n\nclass PacmanGraphics:\n    def __init__(self, zoom=1.0, frameTime=0.0, capture=False):\n        self.have_window = 0\n        self.currentGhostImages = {}\n        self.pacmanImage = None\n        self.zoom = zoom\n        self.gridSize = DEFAULT_GRID_SIZE * zoom\n        self.capture = capture\n        self.frameTime = frameTime\n\n    def initialize(self, state, isBlue=False):\n        self.isBlue = isBlue\n        self.startGraphics(state)\n\n        # self.drawDistributions(state)\n        self.distributionImages = None  # Initialized lazily\n        self.drawStaticObjects(state)\n        self.drawAgentObjects(state)\n\n        # Information\n        self.previousState = state\n\n    def startGraphics(self, state):\n        self.layout = state.layout\n        layout = self.layout\n        self.width = layout.width\n        self.height = layout.height\n        self.make_window(self.width, self.height)\n        self.infoPane = InfoPane(layout, self.gridSize)\n        self.currentState = layout\n\n    def drawDistributions(self, state):\n        walls = state.layout.walls\n        dist = []\n        for x in range(walls.width):\n            distx = []\n            dist.append(distx)\n            for y in range(walls.height):\n                (screen_x, screen_y) = self.to_screen((x, y))\n                block = square((screen_x, screen_y),\n                               0.5 * self.gridSize,\n                               color=BACKGROUND_COLOR,\n                               filled=1, behind=2)\n                distx.append(block)\n        self.distributionImages = dist\n\n    def drawStaticObjects(self, state):\n        layout = self.layout\n        self.drawWalls(layout.walls)\n        self.food = self.drawFood(layout.food)\n        self.capsules = self.drawCapsules(layout.capsules)\n        refresh()\n\n    def drawAgentObjects(self, state):\n        self.agentImages = []  # (agentState, image)\n        for index, agent in enumerate(state.agentStates):\n            if agent.isPacman:\n                image = self.drawPacman(agent, index)\n                self.agentImages.append((agent, image))\n            else:\n                image = self.drawGhost(agent, index)\n                self.agentImages.append((agent, image))\n        refresh()\n\n    def swapImages(self, agentIndex, newState):\n        """"""\n          Changes an image from a ghost to a pacman or vis versa (for capture)\n        """"""\n        prevState, prevImage = self.agentImages[agentIndex]\n        for item in prevImage:\n            remove_from_screen(item)\n        if newState.isPacman:\n            image = self.drawPacman(newState, agentIndex)\n            self.agentImages[agentIndex] = (newState, image)\n        else:\n            image = self.drawGhost(newState, agentIndex)\n            self.agentImages[agentIndex] = (newState, image)\n        refresh()\n\n    def update(self, newState):\n        agentIndex = newState._agentMoved\n        agentState = newState.agentStates[agentIndex]\n\n        if self.agentImages[agentIndex][0].isPacman != agentState.isPacman:\n            self.swapImages(agentIndex, agentState)\n        prevState, prevImage = self.agentImages[agentIndex]\n        if agentState.isPacman:\n            self.animatePacman(agentState, prevState, prevImage)\n        else:\n            self.moveGhost(agentState, agentIndex, prevState, prevImage)\n        self.agentImages[agentIndex] = (agentState, prevImage)\n\n        if newState._foodEaten is not None:\n            self.removeFood(newState._foodEaten, self.food)\n        if newState._capsuleEaten is not None:\n            self.removeCapsule(newState._capsuleEaten, self.capsules)\n        self.infoPane.updateScore(newState.score)\n        if \'ghostDistances\' in dir(newState):\n            self.infoPane.updateGhostDistances(newState.ghostDistances)\n\n    def make_window(self, width, height):\n        grid_width = (width-1) * self.gridSize\n        grid_height = (height-1) * self.gridSize\n        screen_width = 2*self.gridSize + grid_width\n        screen_height = 2*self.gridSize + grid_height + INFO_PANE_HEIGHT\n\n        begin_graphics(screen_width,\n                       screen_height,\n                       BACKGROUND_COLOR,\n                       ""CS188 Pacman"")\n\n    def drawPacman(self, pacman, index):\n        position = self.getPosition(pacman)\n        screen_point = self.to_screen(position)\n        endpoints = self.getEndpoints(self.getDirection(pacman))\n\n        width = PACMAN_OUTLINE_WIDTH\n        outlineColor = PACMAN_COLOR\n        fillColor = PACMAN_COLOR\n\n        if self.capture:\n            outlineColor = TEAM_COLORS[index % 2]\n            fillColor = GHOST_COLORS[index]\n            width = PACMAN_CAPTURE_OUTLINE_WIDTH\n\n        return [circle(screen_point, PACMAN_SCALE * self.gridSize,\n                       fillColor=fillColor, outlineColor=outlineColor,\n                       endpoints=endpoints,\n                       width=width)]\n\n    def getEndpoints(self, direction, position=(0, 0)):\n        x, y = position\n        pos = x - int(x) + y - int(y)\n        width = 30 + 80 * math.sin(math.pi * pos)\n\n        delta = width / 2\n        if (direction == \'West\'):\n            endpoints = (180+delta, 180-delta)\n        elif (direction == \'North\'):\n            endpoints = (90+delta, 90-delta)\n        elif (direction == \'South\'):\n            endpoints = (270+delta, 270-delta)\n        else:\n            endpoints = (0+delta, 0-delta)\n        return endpoints\n\n    def movePacman(self, position, direction, image):\n        screenPosition = self.to_screen(position)\n        endpoints = self.getEndpoints(direction, position)\n        r = PACMAN_SCALE * self.gridSize\n        moveCircle(image[0], screenPosition, r, endpoints)\n        refresh()\n\n    def animatePacman(self, pacman, prevPacman, image):\n        if self.frameTime < 0:\n            print(\'Press any key to step forward, ""q"" to play\')\n            keys = wait_for_keys()\n            if \'q\' in keys:\n                self.frameTime = 0.1\n        if self.frameTime > 0.01 or self.frameTime < 0:\n            start = time.time()\n            fx, fy = self.getPosition(prevPacman)\n            px, py = self.getPosition(pacman)\n            frames = 4.0\n            for i in range(1, int(frames) + 1):\n                pos = px*i/frames + fx * \\\n                    (frames-i)/frames, py*i/frames + fy*(frames-i)/frames\n                self.movePacman(pos, self.getDirection(pacman), image)\n                refresh()\n                sleep(abs(self.frameTime) / frames)\n        else:\n            self.movePacman(self.getPosition(pacman),\n                            self.getDirection(pacman), image)\n        refresh()\n\n    def getGhostColor(self, ghost, ghostIndex):\n        if ghost.scaredTimer > 0:\n            return SCARED_COLOR\n        else:\n            return GHOST_COLORS[ghostIndex]\n\n    def drawGhost(self, ghost, agentIndex):\n        pos = self.getPosition(ghost)\n        dir = self.getDirection(ghost)\n        (screen_x, screen_y) = (self.to_screen(pos))\n        coords = []\n        for (x, y) in GHOST_SHAPE:\n            coords.append((x*self.gridSize*GHOST_SIZE + screen_x,\n                           y*self.gridSize*GHOST_SIZE + screen_y))\n\n        colour = self.getGhostColor(ghost, agentIndex)\n        body = polygon(coords, colour, filled=1)\n        WHITE = formatColor(1.0, 1.0, 1.0)\n        BLACK = formatColor(0.0, 0.0, 0.0)\n\n        dx = 0\n        dy = 0\n        if dir == \'North\':\n            dy = -0.2\n        if dir == \'South\':\n            dy = 0.2\n        if dir == \'East\':\n            dx = 0.2\n        if dir == \'West\':\n            dx = -0.2\n        leftEye = circle((screen_x+self.gridSize*GHOST_SIZE*(-0.3+dx/1.5), screen_y -\n                          self.gridSize*GHOST_SIZE*(0.3-dy/1.5)), self.gridSize*GHOST_SIZE*0.2, WHITE, WHITE)\n        rightEye = circle((screen_x+self.gridSize*GHOST_SIZE*(0.3+dx/1.5), screen_y -\n                           self.gridSize*GHOST_SIZE*(0.3-dy/1.5)), self.gridSize*GHOST_SIZE*0.2, WHITE, WHITE)\n        leftPupil = circle((screen_x+self.gridSize*GHOST_SIZE*(-0.3+dx), screen_y -\n                            self.gridSize*GHOST_SIZE*(0.3-dy)), self.gridSize*GHOST_SIZE*0.08, BLACK, BLACK)\n        rightPupil = circle((screen_x+self.gridSize*GHOST_SIZE*(0.3+dx), screen_y -\n                             self.gridSize*GHOST_SIZE*(0.3-dy)), self.gridSize*GHOST_SIZE*0.08, BLACK, BLACK)\n        ghostImageParts = []\n        ghostImageParts.append(body)\n        ghostImageParts.append(leftEye)\n        ghostImageParts.append(rightEye)\n        ghostImageParts.append(leftPupil)\n        ghostImageParts.append(rightPupil)\n\n        return ghostImageParts\n\n    def moveEyes(self, pos, dir, eyes):\n        (screen_x, screen_y) = (self.to_screen(pos))\n        dx = 0\n        dy = 0\n        if dir == \'North\':\n            dy = -0.2\n        if dir == \'South\':\n            dy = 0.2\n        if dir == \'East\':\n            dx = 0.2\n        if dir == \'West\':\n            dx = -0.2\n        moveCircle(eyes[0], (screen_x+self.gridSize*GHOST_SIZE*(-0.3+dx/1.5), screen_y -\n                             self.gridSize*GHOST_SIZE*(0.3-dy/1.5)), self.gridSize*GHOST_SIZE*0.2)\n        moveCircle(eyes[1], (screen_x+self.gridSize*GHOST_SIZE*(0.3+dx/1.5), screen_y -\n                             self.gridSize*GHOST_SIZE*(0.3-dy/1.5)), self.gridSize*GHOST_SIZE*0.2)\n        moveCircle(eyes[2], (screen_x+self.gridSize*GHOST_SIZE*(-0.3+dx), screen_y -\n                             self.gridSize*GHOST_SIZE*(0.3-dy)), self.gridSize*GHOST_SIZE*0.08)\n        moveCircle(eyes[3], (screen_x+self.gridSize*GHOST_SIZE*(0.3+dx), screen_y -\n                             self.gridSize*GHOST_SIZE*(0.3-dy)), self.gridSize*GHOST_SIZE*0.08)\n\n    def moveGhost(self, ghost, ghostIndex, prevGhost, ghostImageParts):\n        old_x, old_y = self.to_screen(self.getPosition(prevGhost))\n        new_x, new_y = self.to_screen(self.getPosition(ghost))\n        delta = new_x - old_x, new_y - old_y\n\n        for ghostImagePart in ghostImageParts:\n            move_by(ghostImagePart, delta)\n        refresh()\n\n        if ghost.scaredTimer > 0:\n            color = SCARED_COLOR\n        else:\n            color = GHOST_COLORS[ghostIndex]\n        edit(ghostImageParts[0], (\'fill\', color), (\'outline\', color))\n        self.moveEyes(self.getPosition(ghost),\n                      self.getDirection(ghost), ghostImageParts[-4:])\n        refresh()\n\n    def getPosition(self, agentState):\n        if agentState.configuration is None:\n            return (-1000, -1000)\n        return agentState.getPosition()\n\n    def getDirection(self, agentState):\n        if agentState.configuration is None:\n            return Directions.STOP\n        return agentState.configuration.getDirection()\n\n    def finish(self):\n        end_graphics()\n\n    def to_screen(self, point):\n        (x, y) = point\n        #y = self.height - y\n        x = (x + 1)*self.gridSize\n        y = (self.height - y)*self.gridSize\n        return (x, y)\n\n    # Fixes some TK issue with off-center circles\n    def to_screen2(self, point):\n        (x, y) = point\n        #y = self.height - y\n        x = (x + 1)*self.gridSize\n        y = (self.height - y)*self.gridSize\n        return (x, y)\n\n    def drawWalls(self, wallMatrix):\n        wallColor = WALL_COLOR\n        for xNum, x in enumerate(wallMatrix):\n            if self.capture and (xNum * 2) < wallMatrix.width:\n                wallColor = TEAM_COLORS[0]\n            if self.capture and (xNum * 2) >= wallMatrix.width:\n                wallColor = TEAM_COLORS[1]\n\n            for yNum, cell in enumerate(x):\n                if cell:  # There\'s a wall here\n                    pos = (xNum, yNum)\n                    screen = self.to_screen(pos)\n                    screen2 = self.to_screen2(pos)\n\n                    # draw each quadrant of the square based on adjacent walls\n                    wIsWall = self.isWall(xNum-1, yNum, wallMatrix)\n                    eIsWall = self.isWall(xNum+1, yNum, wallMatrix)\n                    nIsWall = self.isWall(xNum, yNum+1, wallMatrix)\n                    sIsWall = self.isWall(xNum, yNum-1, wallMatrix)\n                    nwIsWall = self.isWall(xNum-1, yNum+1, wallMatrix)\n                    swIsWall = self.isWall(xNum-1, yNum-1, wallMatrix)\n                    neIsWall = self.isWall(xNum+1, yNum+1, wallMatrix)\n                    seIsWall = self.isWall(xNum+1, yNum-1, wallMatrix)\n\n                    # NE quadrant\n                    if (not nIsWall) and (not eIsWall):\n                        # inner circle\n                        circle(screen2, WALL_RADIUS * self.gridSize,\n                               wallColor, wallColor, (0, 91), \'arc\')\n                    if (nIsWall) and (not eIsWall):\n                        # vertical line\n                        line(add(screen, (self.gridSize*WALL_RADIUS, 0)), add(screen,\n                                                                              (self.gridSize*WALL_RADIUS, self.gridSize*(-0.5)-1)), wallColor)\n                    if (not nIsWall) and (eIsWall):\n                        # horizontal line\n                        line(add(screen, (0, self.gridSize*(-1)*WALL_RADIUS)), add(screen,\n                                                                                   (self.gridSize*0.5+1, self.gridSize*(-1)*WALL_RADIUS)), wallColor)\n                    if (nIsWall) and (eIsWall) and (not neIsWall):\n                        # outer circle\n                        circle(add(screen2, (self.gridSize*2*WALL_RADIUS, self.gridSize*(-2)*WALL_RADIUS)),\n                               WALL_RADIUS * self.gridSize-1, wallColor, wallColor, (180, 271), \'arc\')\n                        line(add(screen, (self.gridSize*2*WALL_RADIUS-1, self.gridSize*(-1)*WALL_RADIUS)),\n                             add(screen, (self.gridSize*0.5+1, self.gridSize*(-1)*WALL_RADIUS)), wallColor)\n                        line(add(screen, (self.gridSize*WALL_RADIUS, self.gridSize*(-2)*WALL_RADIUS+1)),\n                             add(screen, (self.gridSize*WALL_RADIUS, self.gridSize*(-0.5))), wallColor)\n\n                    # NW quadrant\n                    if (not nIsWall) and (not wIsWall):\n                        # inner circle\n                        circle(screen2, WALL_RADIUS * self.gridSize,\n                               wallColor, wallColor, (90, 181), \'arc\')\n                    if (nIsWall) and (not wIsWall):\n                        # vertical line\n                        line(add(screen, (self.gridSize*(-1)*WALL_RADIUS, 0)), add(screen,\n                                                                                   (self.gridSize*(-1)*WALL_RADIUS, self.gridSize*(-0.5)-1)), wallColor)\n                    if (not nIsWall) and (wIsWall):\n                        # horizontal line\n                        line(add(screen, (0, self.gridSize*(-1)*WALL_RADIUS)), add(screen,\n                                                                                   (self.gridSize*(-0.5)-1, self.gridSize*(-1)*WALL_RADIUS)), wallColor)\n                    if (nIsWall) and (wIsWall) and (not nwIsWall):\n                        # outer circle\n                        circle(add(screen2, (self.gridSize*(-2)*WALL_RADIUS, self.gridSize*(-2)*WALL_RADIUS)),\n                               WALL_RADIUS * self.gridSize-1, wallColor, wallColor, (270, 361), \'arc\')\n                        line(add(screen, (self.gridSize*(-2)*WALL_RADIUS+1, self.gridSize*(-1)*WALL_RADIUS)),\n                             add(screen, (self.gridSize*(-0.5), self.gridSize*(-1)*WALL_RADIUS)), wallColor)\n                        line(add(screen, (self.gridSize*(-1)*WALL_RADIUS, self.gridSize*(-2)*WALL_RADIUS+1)),\n                             add(screen, (self.gridSize*(-1)*WALL_RADIUS, self.gridSize*(-0.5))), wallColor)\n\n                    # SE quadrant\n                    if (not sIsWall) and (not eIsWall):\n                        # inner circle\n                        circle(screen2, WALL_RADIUS * self.gridSize,\n                               wallColor, wallColor, (270, 361), \'arc\')\n                    if (sIsWall) and (not eIsWall):\n                        # vertical line\n                        line(add(screen, (self.gridSize*WALL_RADIUS, 0)), add(screen,\n                                                                              (self.gridSize*WALL_RADIUS, self.gridSize*(0.5)+1)), wallColor)\n                    if (not sIsWall) and (eIsWall):\n                        # horizontal line\n                        line(add(screen, (0, self.gridSize*(1)*WALL_RADIUS)), add(screen,\n                                                                                  (self.gridSize*0.5+1, self.gridSize*(1)*WALL_RADIUS)), wallColor)\n                    if (sIsWall) and (eIsWall) and (not seIsWall):\n                        # outer circle\n                        circle(add(screen2, (self.gridSize*2*WALL_RADIUS, self.gridSize*(2)*WALL_RADIUS)),\n                               WALL_RADIUS * self.gridSize-1, wallColor, wallColor, (90, 181), \'arc\')\n                        line(add(screen, (self.gridSize*2*WALL_RADIUS-1, self.gridSize*(1)*WALL_RADIUS)),\n                             add(screen, (self.gridSize*0.5, self.gridSize*(1)*WALL_RADIUS)), wallColor)\n                        line(add(screen, (self.gridSize*WALL_RADIUS, self.gridSize*(2)*WALL_RADIUS-1)),\n                             add(screen, (self.gridSize*WALL_RADIUS, self.gridSize*(0.5))), wallColor)\n\n                    # SW quadrant\n                    if (not sIsWall) and (not wIsWall):\n                        # inner circle\n                        circle(screen2, WALL_RADIUS * self.gridSize,\n                               wallColor, wallColor, (180, 271), \'arc\')\n                    if (sIsWall) and (not wIsWall):\n                        # vertical line\n                        line(add(screen, (self.gridSize*(-1)*WALL_RADIUS, 0)), add(screen,\n                                                                                   (self.gridSize*(-1)*WALL_RADIUS, self.gridSize*(0.5)+1)), wallColor)\n                    if (not sIsWall) and (wIsWall):\n                        # horizontal line\n                        line(add(screen, (0, self.gridSize*(1)*WALL_RADIUS)), add(screen,\n                                                                                  (self.gridSize*(-0.5)-1, self.gridSize*(1)*WALL_RADIUS)), wallColor)\n                    if (sIsWall) and (wIsWall) and (not swIsWall):\n                        # outer circle\n                        circle(add(screen2, (self.gridSize*(-2)*WALL_RADIUS, self.gridSize*(2)*WALL_RADIUS)),\n                               WALL_RADIUS * self.gridSize-1, wallColor, wallColor, (0, 91), \'arc\')\n                        line(add(screen, (self.gridSize*(-2)*WALL_RADIUS+1, self.gridSize*(1)*WALL_RADIUS)),\n                             add(screen, (self.gridSize*(-0.5), self.gridSize*(1)*WALL_RADIUS)), wallColor)\n                        line(add(screen, (self.gridSize*(-1)*WALL_RADIUS, self.gridSize*(2)*WALL_RADIUS-1)),\n                             add(screen, (self.gridSize*(-1)*WALL_RADIUS, self.gridSize*(0.5))), wallColor)\n\n    def isWall(self, x, y, walls):\n        if x < 0 or y < 0:\n            return False\n        if x >= walls.width or y >= walls.height:\n            return False\n        return walls[x][y]\n\n    def drawFood(self, foodMatrix):\n        foodImages = []\n        color = FOOD_COLOR\n        for xNum, x in enumerate(foodMatrix):\n            if self.capture and (xNum * 2) <= foodMatrix.width:\n                color = TEAM_COLORS[0]\n            if self.capture and (xNum * 2) > foodMatrix.width:\n                color = TEAM_COLORS[1]\n            imageRow = []\n            foodImages.append(imageRow)\n            for yNum, cell in enumerate(x):\n                if cell:  # There\'s food here\n                    screen = self.to_screen((xNum, yNum))\n                    dot = circle(screen,\n                                 FOOD_SIZE * self.gridSize,\n                                 outlineColor=color, fillColor=color,\n                                 width=1)\n                    imageRow.append(dot)\n                else:\n                    imageRow.append(None)\n        return foodImages\n\n    def drawCapsules(self, capsules):\n        capsuleImages = {}\n        for capsule in capsules:\n            (screen_x, screen_y) = self.to_screen(capsule)\n            dot = circle((screen_x, screen_y),\n                         CAPSULE_SIZE * self.gridSize,\n                         outlineColor=CAPSULE_COLOR,\n                         fillColor=CAPSULE_COLOR,\n                         width=1)\n            capsuleImages[capsule] = dot\n        return capsuleImages\n\n    def removeFood(self, cell, foodImages):\n        x, y = cell\n        remove_from_screen(foodImages[x][y])\n\n    def removeCapsule(self, cell, capsuleImages):\n        x, y = cell\n        remove_from_screen(capsuleImages[(x, y)])\n\n    def drawExpandedCells(self, cells):\n        """"""\n        Draws an overlay of expanded grid positions for search agents\n        """"""\n        n = float(len(cells))\n        baseColor = [1.0, 0.0, 0.0]\n        self.clearExpandedCells()\n        self.expandedCells = []\n        for k, cell in enumerate(cells):\n            screenPos = self.to_screen(cell)\n            cellColor = formatColor(\n                *[(n-k) * c * .5 / n + .25 for c in baseColor])\n            block = square(screenPos,\n                           0.5 * self.gridSize,\n                           color=cellColor,\n                           filled=1, behind=2)\n            self.expandedCells.append(block)\n            if self.frameTime < 0:\n                refresh()\n\n    def clearExpandedCells(self):\n        if \'expandedCells\' in dir(self) and len(self.expandedCells) > 0:\n            for cell in self.expandedCells:\n                remove_from_screen(cell)\n\n    def updateDistributions(self, distributions):\n        ""Draws an agent\'s belief distributions""\n        if self.distributionImages is None:\n            self.drawDistributions(self.previousState)\n        for x in range(len(self.distributionImages)):\n            for y in range(len(self.distributionImages[0])):\n                image = self.distributionImages[x][y]\n                weights = [dist[(x, y)] for dist in distributions]\n\n                if sum(weights) != 0:\n                    pass\n                # Fog of war\n                color = [0.0, 0.0, 0.0]\n                colors = GHOST_VEC_COLORS[1:]  # With Pacman\n                if self.capture:\n                    colors = GHOST_VEC_COLORS\n                for weight, gcolor in zip(weights, colors):\n                    color = [min(1.0, c + 0.95 * g * weight ** .3)\n                             for c, g in zip(color, gcolor)]\n                changeColor(image, formatColor(*color))\n        refresh()\n\n\nclass FirstPersonPacmanGraphics(PacmanGraphics):\n    def __init__(self, zoom=1.0, showGhosts=True, capture=False, frameTime=0):\n        PacmanGraphics.__init__(self, zoom, frameTime=frameTime)\n        self.showGhosts = showGhosts\n        self.capture = capture\n\n    def initialize(self, state, isBlue=False):\n\n        self.isBlue = isBlue\n        PacmanGraphics.startGraphics(self, state)\n        # Initialize distribution images\n        walls = state.layout.walls\n        dist = []\n        self.layout = state.layout\n\n        # Draw the rest\n        self.distributionImages = None  # initialize lazily\n        self.drawStaticObjects(state)\n        self.drawAgentObjects(state)\n\n        # Information\n        self.previousState = state\n\n    def lookAhead(self, config, state):\n        if config.getDirection() == \'Stop\':\n            return\n        else:\n            pass\n            # Draw relevant ghosts\n            allGhosts = state.getGhostStates()\n            visibleGhosts = state.getVisibleGhosts()\n            for i, ghost in enumerate(allGhosts):\n                if ghost in visibleGhosts:\n                    self.drawGhost(ghost, i)\n                else:\n                    self.currentGhostImages[i] = None\n\n    def getGhostColor(self, ghost, ghostIndex):\n        return GHOST_COLORS[ghostIndex]\n\n    def getPosition(self, ghostState):\n        if not self.showGhosts and not ghostState.isPacman and ghostState.getPosition()[1] > 1:\n            return (-1000, -1000)\n        else:\n            return PacmanGraphics.getPosition(self, ghostState)\n\n\ndef add(x, y):\n    return (x[0] + y[0], x[1] + y[1])\n\n\n# Saving graphical output\n# -----------------------\n# Note: to make an animated gif from this postscript output, try the command:\n# convert -delay 7 -loop 1 -compress lzw -layers optimize frame* out.gif\n# convert is part of imagemagick (freeware)\n\nSAVE_POSTSCRIPT = False\nPOSTSCRIPT_OUTPUT_DIR = \'frames\'\nFRAME_NUMBER = 0\n\n\ndef saveFrame():\n    ""Saves the current graphical output as a postscript file""\n    global SAVE_POSTSCRIPT, FRAME_NUMBER, POSTSCRIPT_OUTPUT_DIR\n    if not SAVE_POSTSCRIPT:\n        return\n    if not os.path.exists(POSTSCRIPT_OUTPUT_DIR):\n        os.mkdir(POSTSCRIPT_OUTPUT_DIR)\n    name = os.path.join(POSTSCRIPT_OUTPUT_DIR, \'frame_%08d.ps\' % FRAME_NUMBER)\n    FRAME_NUMBER += 1\n    writePostscript(name)  # writes the current canvas\n'"
week03_model_free/crawler_and_pacman/seminar_py3/graphicsGridworldDisplay.py,0,"b'# graphicsGridworldDisplay.py\n# ---------------------------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nimport util\nfrom graphicsUtils import *\nfrom functools import reduce\n\n\nclass GraphicsGridworldDisplay:\n\n    def __init__(self, gridworld, size=120, speed=1.0):\n        self.gridworld = gridworld\n        self.size = size\n        self.speed = speed\n\n    def start(self):\n        setup(self.gridworld, size=self.size)\n\n    def pause(self):\n        wait_for_keys()\n\n    def displayValues(self, agent, currentState=None, message=\'Agent Values\'):\n        values = util.Counter()\n        policy = {}\n        states = self.gridworld.getStates()\n        for state in states:\n            values[state] = agent.getValue(state)\n            policy[state] = agent.getPolicy(state)\n        drawValues(self.gridworld, values, policy, currentState, message)\n        sleep(0.05 / self.speed)\n\n    def displayNullValues(self, currentState=None, message=\'\'):\n        values = util.Counter()\n        #policy = {}\n        states = self.gridworld.getStates()\n        for state in states:\n            values[state] = 0.0\n            #policy[state] = agent.getPolicy(state)\n        drawNullValues(self.gridworld, currentState, \'\')\n        # drawValues(self.gridworld, values, policy, currentState, message)\n        sleep(0.05 / self.speed)\n\n    def displayQValues(self, agent, currentState=None, message=\'Agent Q-Values\'):\n        qValues = util.Counter()\n        states = self.gridworld.getStates()\n        for state in states:\n            for action in self.gridworld.getPossibleActions(state):\n                qValues[(state, action)] = agent.getQValue(state, action)\n        drawQValues(self.gridworld, qValues, currentState, message)\n        sleep(0.05 / self.speed)\n\n\nBACKGROUND_COLOR = formatColor(0, 0, 0)\nEDGE_COLOR = formatColor(1, 1, 1)\nOBSTACLE_COLOR = formatColor(0.5, 0.5, 0.5)\nTEXT_COLOR = formatColor(1, 1, 1)\nMUTED_TEXT_COLOR = formatColor(0.7, 0.7, 0.7)\nLOCATION_COLOR = formatColor(0, 0, 1)\n\nWINDOW_SIZE = -1\nGRID_SIZE = -1\nGRID_HEIGHT = -1\nMARGIN = -1\n\n\ndef setup(gridworld, title=""Gridworld Display"", size=120):\n    global GRID_SIZE, MARGIN, SCREEN_WIDTH, SCREEN_HEIGHT, GRID_HEIGHT\n    grid = gridworld.grid\n    WINDOW_SIZE = size\n    GRID_SIZE = size\n    GRID_HEIGHT = grid.height\n    MARGIN = GRID_SIZE * 0.75\n    screen_width = (grid.width - 1) * GRID_SIZE + MARGIN * 2\n    screen_height = (grid.height - 0.5) * GRID_SIZE + MARGIN * 2\n\n    begin_graphics(screen_width,\n                   screen_height,\n                   BACKGROUND_COLOR, title=title)\n\n\ndef drawNullValues(gridworld, currentState=None, message=\'\'):\n    grid = gridworld.grid\n    blank()\n    for x in range(grid.width):\n        for y in range(grid.height):\n            state = (x, y)\n            gridType = grid[x][y]\n            isExit = (str(gridType) != gridType)\n            isCurrent = (currentState == state)\n            if gridType == \'#\':\n                drawSquare(x, y, 0, 0, 0, None, None, True, False, isCurrent)\n            else:\n                drawNullSquare(gridworld.grid, x, y, False, isExit, isCurrent)\n    pos = to_screen(((grid.width - 1.0) / 2.0, - 0.8))\n    text(pos, TEXT_COLOR, message, ""Courier"", -32, ""bold"", ""c"")\n\n\ndef drawValues(gridworld, values, policy, currentState=None, message=\'State Values\'):\n    grid = gridworld.grid\n    blank()\n    valueList = [values[state] for state in gridworld.getStates()] + [0.0]\n    minValue = min(valueList)\n    maxValue = max(valueList)\n    for x in range(grid.width):\n        for y in range(grid.height):\n            state = (x, y)\n            gridType = grid[x][y]\n            isExit = (str(gridType) != gridType)\n            isCurrent = (currentState == state)\n            if gridType == \'#\':\n                drawSquare(x, y, 0, 0, 0, None, None, True, False, isCurrent)\n            else:\n                value = values[state]\n                action = None\n                if policy is not None and state in policy:\n                    action = policy[state]\n                    actions = gridworld.getPossibleActions(state)\n                if action not in actions and \'exit\' in actions:\n                    action = \'exit\'\n                valString = \'%.2f\' % value\n                drawSquare(x, y, value, minValue, maxValue,\n                           valString, action, False, isExit, isCurrent)\n    pos = to_screen(((grid.width - 1.0) / 2.0, - 0.8))\n    text(pos, TEXT_COLOR, message, ""Courier"", -32, ""bold"", ""c"")\n\n\ndef drawQValues(gridworld, qValues, currentState=None, message=\'State-Action Q-Values\'):\n    grid = gridworld.grid\n    blank()\n    stateCrossActions = [[(state, action) for action in gridworld.getPossibleActions(\n        state)] for state in gridworld.getStates()]\n    qStates = reduce(lambda x, y: x+y, stateCrossActions, [])\n    qValueList = [qValues[(state, action)]\n                  for state, action in qStates] + [0.0]\n    minValue = min(qValueList)\n    maxValue = max(qValueList)\n    for x in range(grid.width):\n        for y in range(grid.height):\n            state = (x, y)\n            gridType = grid[x][y]\n            isExit = (str(gridType) != gridType)\n            isCurrent = (currentState == state)\n            actions = gridworld.getPossibleActions(state)\n            if actions is None or len(actions) == 0:\n                actions = [None]\n            bestQ = max([qValues[(state, action)] for action in actions])\n            bestActions = [\n                action for action in actions if qValues[(state, action)] == bestQ]\n\n            q = util.Counter()\n            valStrings = {}\n            for action in actions:\n                v = qValues[(state, action)]\n                q[action] += v\n                valStrings[action] = \'%.2f\' % v\n            if gridType == \'#\':\n                drawSquare(x, y, 0, 0, 0, None, None, True, False, isCurrent)\n            elif isExit:\n                action = \'exit\'\n                value = q[action]\n                valString = \'%.2f\' % value\n                drawSquare(x, y, value, minValue, maxValue,\n                           valString, action, False, isExit, isCurrent)\n            else:\n                drawSquareQ(x, y, q, minValue, maxValue,\n                            valStrings, actions, isCurrent)\n    pos = to_screen(((grid.width - 1.0) / 2.0, - 0.8))\n    text(pos, TEXT_COLOR, message, ""Courier"", -32, ""bold"", ""c"")\n\n\ndef blank():\n    clear_screen()\n\n\ndef drawNullSquare(grid, x, y, isObstacle, isTerminal, isCurrent):\n\n    square_color = getColor(0, -1, 1)\n\n    if isObstacle:\n        square_color = OBSTACLE_COLOR\n\n    (screen_x, screen_y) = to_screen((x, y))\n    square((screen_x, screen_y),\n           0.5 * GRID_SIZE,\n           color=square_color,\n           filled=1,\n           width=1)\n\n    square((screen_x, screen_y),\n           0.5 * GRID_SIZE,\n           color=EDGE_COLOR,\n           filled=0,\n           width=3)\n\n    if isTerminal and not isObstacle:\n        square((screen_x, screen_y),\n               0.4 * GRID_SIZE,\n               color=EDGE_COLOR,\n               filled=0,\n               width=2)\n        text((screen_x, screen_y),\n             TEXT_COLOR,\n             str(grid[x][y]),\n             ""Courier"", -24, ""bold"", ""c"")\n\n    text_color = TEXT_COLOR\n\n    if not isObstacle and isCurrent:\n        circle((screen_x, screen_y), 0.1*GRID_SIZE,\n               LOCATION_COLOR, fillColor=LOCATION_COLOR)\n\n    # if not isObstacle:\n    #   text( (screen_x, screen_y), text_color, valStr, ""Courier"", 24, ""bold"", ""c"")\n\n\ndef drawSquare(x, y, val, min, max, valStr, action, isObstacle, isTerminal, isCurrent):\n\n    square_color = getColor(val, min, max)\n\n    if isObstacle:\n        square_color = OBSTACLE_COLOR\n\n    (screen_x, screen_y) = to_screen((x, y))\n    square((screen_x, screen_y),\n           0.5 * GRID_SIZE,\n           color=square_color,\n           filled=1,\n           width=1)\n    square((screen_x, screen_y),\n           0.5 * GRID_SIZE,\n           color=EDGE_COLOR,\n           filled=0,\n           width=3)\n    if isTerminal and not isObstacle:\n        square((screen_x, screen_y),\n               0.4 * GRID_SIZE,\n               color=EDGE_COLOR,\n               filled=0,\n               width=2)\n\n    if action == \'north\':\n        polygon([(screen_x, screen_y - 0.45*GRID_SIZE), (screen_x+0.05*GRID_SIZE, screen_y-0.40*GRID_SIZE),\n                 (screen_x-0.05*GRID_SIZE, screen_y-0.40*GRID_SIZE)], EDGE_COLOR, filled=1, smoothed=False)\n    if action == \'south\':\n        polygon([(screen_x, screen_y + 0.45*GRID_SIZE), (screen_x+0.05*GRID_SIZE, screen_y+0.40*GRID_SIZE),\n                 (screen_x-0.05*GRID_SIZE, screen_y+0.40*GRID_SIZE)], EDGE_COLOR, filled=1, smoothed=False)\n    if action == \'west\':\n        polygon([(screen_x-0.45*GRID_SIZE, screen_y), (screen_x-0.4*GRID_SIZE, screen_y+0.05*GRID_SIZE),\n                 (screen_x-0.4*GRID_SIZE, screen_y-0.05*GRID_SIZE)], EDGE_COLOR, filled=1, smoothed=False)\n    if action == \'east\':\n        polygon([(screen_x+0.45*GRID_SIZE, screen_y), (screen_x+0.4*GRID_SIZE, screen_y+0.05*GRID_SIZE),\n                 (screen_x+0.4*GRID_SIZE, screen_y-0.05*GRID_SIZE)], EDGE_COLOR, filled=1, smoothed=False)\n\n    text_color = TEXT_COLOR\n\n    if not isObstacle and isCurrent:\n        circle((screen_x, screen_y), 0.1*GRID_SIZE,\n               outlineColor=LOCATION_COLOR, fillColor=LOCATION_COLOR)\n\n    if not isObstacle:\n        text((screen_x, screen_y), text_color,\n             valStr, ""Courier"", -30, ""bold"", ""c"")\n\n\ndef drawSquareQ(x, y, qVals, minVal, maxVal, valStrs, bestActions, isCurrent):\n\n    (screen_x, screen_y) = to_screen((x, y))\n\n    center = (screen_x, screen_y)\n    nw = (screen_x-0.5*GRID_SIZE, screen_y-0.5*GRID_SIZE)\n    ne = (screen_x+0.5*GRID_SIZE, screen_y-0.5*GRID_SIZE)\n    se = (screen_x+0.5*GRID_SIZE, screen_y+0.5*GRID_SIZE)\n    sw = (screen_x-0.5*GRID_SIZE, screen_y+0.5*GRID_SIZE)\n    n = (screen_x, screen_y-0.5*GRID_SIZE+5)\n    s = (screen_x, screen_y+0.5*GRID_SIZE-5)\n    w = (screen_x-0.5*GRID_SIZE+5, screen_y)\n    e = (screen_x+0.5*GRID_SIZE-5, screen_y)\n\n    actions = list(qVals.keys())\n    for action in actions:\n\n        wedge_color = getColor(qVals[action], minVal, maxVal)\n\n        if action == \'north\':\n            polygon((center, nw, ne), wedge_color, filled=1, smoothed=False)\n            #text(n, text_color, valStr, ""Courier"", 8, ""bold"", ""n"")\n        if action == \'south\':\n            polygon((center, sw, se), wedge_color, filled=1, smoothed=False)\n            #text(s, text_color, valStr, ""Courier"", 8, ""bold"", ""s"")\n        if action == \'east\':\n            polygon((center, ne, se), wedge_color, filled=1, smoothed=False)\n            #text(e, text_color, valStr, ""Courier"", 8, ""bold"", ""e"")\n        if action == \'west\':\n            polygon((center, nw, sw), wedge_color, filled=1, smoothed=False)\n            #text(w, text_color, valStr, ""Courier"", 8, ""bold"", ""w"")\n\n    square((screen_x, screen_y),\n           0.5 * GRID_SIZE,\n           color=EDGE_COLOR,\n           filled=0,\n           width=3)\n    line(ne, sw, color=EDGE_COLOR)\n    line(nw, se, color=EDGE_COLOR)\n\n    if isCurrent:\n        circle((screen_x, screen_y), 0.1*GRID_SIZE,\n               LOCATION_COLOR, fillColor=LOCATION_COLOR)\n\n    for action in actions:\n        text_color = TEXT_COLOR\n        if qVals[action] < max(qVals.values()):\n            text_color = MUTED_TEXT_COLOR\n        valStr = """"\n        if action in valStrs:\n            valStr = valStrs[action]\n        h = -20\n        if action == \'north\':\n            #polygon( (center, nw, ne), wedge_color, filled = 1, smooth = 0)\n            text(n, text_color, valStr, ""Courier"", h, ""bold"", ""n"")\n        if action == \'south\':\n            #polygon( (center, sw, se), wedge_color, filled = 1, smooth = 0)\n            text(s, text_color, valStr, ""Courier"", h, ""bold"", ""s"")\n        if action == \'east\':\n            #polygon( (center, ne, se), wedge_color, filled = 1, smooth = 0)\n            text(e, text_color, valStr, ""Courier"", h, ""bold"", ""e"")\n        if action == \'west\':\n            #polygon( (center, nw, sw), wedge_color, filled = 1, smooth = 0)\n            text(w, text_color, valStr, ""Courier"", h, ""bold"", ""w"")\n\n\ndef getColor(val, minVal, max):\n    r, g = 0.0, 0.0\n    if val < 0 and minVal < 0:\n        r = val * 0.65 / minVal\n    if val > 0 and max > 0:\n        g = val * 0.65 / max\n    return formatColor(r, g, 0.0)\n\n\ndef square(pos, size, color, filled, width):\n    x, y = pos\n    dx, dy = size, size\n    return polygon([(x - dx, y - dy), (x - dx, y + dy), (x + dx, y + dy), (x + dx, y - dy)], outlineColor=color, fillColor=color, filled=filled, width=width, smoothed=False)\n\n\ndef to_screen(point):\n    (gamex, gamey) = point\n    x = gamex*GRID_SIZE + MARGIN\n    y = (GRID_HEIGHT - gamey - 1)*GRID_SIZE + MARGIN\n    return (x, y)\n\n\ndef to_grid(point):\n    (x, y) = point\n    x = int((y - MARGIN + GRID_SIZE * 0.5) / GRID_SIZE)\n    y = int((x - MARGIN + GRID_SIZE * 0.5) / GRID_SIZE)\n    print(point, ""-->"", (x, y))\n    return (x, y)\n'"
week03_model_free/crawler_and_pacman/seminar_py3/graphicsUtils.py,0,"b'# graphicsUtils.py\n# ----------------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nimport sys\nimport math\nimport random\nimport string\nimport time\nimport types\nimport tkinter\n\n_Windows = sys.platform == \'win32\'  # True if on Win95/98/NT\n\n_root_window = None      # The root window for graphics output\n_canvas = None      # The canvas which holds graphics\n_canvas_xs = None      # Size of canvas object\n_canvas_ys = None\n_canvas_x = None      # Current position on canvas\n_canvas_y = None\n_canvas_col = None      # Current colour (set to black below)\n_canvas_tsize = 12\n_canvas_tserifs = 0\n\n\ndef formatColor(r, g, b):\n    return \'#%02x%02x%02x\' % (int(r * 255), int(g * 255), int(b * 255))\n\n\ndef colorToVector(color):\n    return [int(x, 16) / 256.0 for x in [color[1:3], color[3:5], color[5:7]]]\n\n\nif _Windows:\n    _canvas_tfonts = [\'times new roman\', \'lucida console\']\nelse:\n    _canvas_tfonts = [\'times\', \'lucidasans-24\']\n    pass  # XXX need defaults here\n\n\ndef sleep(secs):\n    global _root_window\n    if _root_window is None:\n        time.sleep(secs)\n    else:\n        _root_window.update_idletasks()\n        _root_window.after(int(1000 * secs), _root_window.quit)\n        _root_window.mainloop()\n\n\ndef begin_graphics(width=640, height=480, color=formatColor(0, 0, 0), title=None):\n\n    global _root_window, _canvas, _canvas_x, _canvas_y, _canvas_xs, _canvas_ys, _bg_color\n\n    # Check for duplicate call\n    if _root_window is not None:\n        # Lose the window.\n        _root_window.destroy()\n\n    # Save the canvas size parameters\n    _canvas_xs, _canvas_ys = width - 1, height - 1\n    _canvas_x, _canvas_y = 0, _canvas_ys\n    _bg_color = color\n\n    # Create the root window\n    _root_window = tkinter.Tk()\n    _root_window.protocol(\'WM_DELETE_WINDOW\', _destroy_window)\n    _root_window.title(title or \'Graphics Window\')\n    _root_window.resizable(0, 0)\n\n    # Create the canvas object\n    try:\n        _canvas = tkinter.Canvas(_root_window, width=width, height=height)\n        _canvas.pack()\n        draw_background()\n        _canvas.update()\n    except:\n        _root_window = None\n        raise\n\n    # Bind to key-down and key-up events\n    _root_window.bind(""<KeyPress>"", _keypress)\n    _root_window.bind(""<KeyRelease>"", _keyrelease)\n    _root_window.bind(""<FocusIn>"", _clear_keys)\n    _root_window.bind(""<FocusOut>"", _clear_keys)\n    _root_window.bind(""<Button-1>"", _leftclick)\n    _root_window.bind(""<Button-2>"", _rightclick)\n    _root_window.bind(""<Button-3>"", _rightclick)\n    _root_window.bind(""<Control-Button-1>"", _ctrl_leftclick)\n    _clear_keys()\n\n\n_leftclick_loc = None\n_rightclick_loc = None\n_ctrl_leftclick_loc = None\n\n\ndef _leftclick(event):\n    global _leftclick_loc\n    _leftclick_loc = (event.x, event.y)\n\n\ndef _rightclick(event):\n    global _rightclick_loc\n    _rightclick_loc = (event.x, event.y)\n\n\ndef _ctrl_leftclick(event):\n    global _ctrl_leftclick_loc\n    _ctrl_leftclick_loc = (event.x, event.y)\n\n\ndef wait_for_click():\n    while True:\n        global _leftclick_loc\n        global _rightclick_loc\n        global _ctrl_leftclick_loc\n        if _leftclick_loc is not None:\n            val = _leftclick_loc\n            _leftclick_loc = None\n            return val, \'left\'\n        if _rightclick_loc is not None:\n            val = _rightclick_loc\n            _rightclick_loc = None\n            return val, \'right\'\n        if _ctrl_leftclick_loc is not None:\n            val = _ctrl_leftclick_loc\n            _ctrl_leftclick_loc = None\n            return val, \'ctrl_left\'\n        sleep(0.05)\n\n\ndef draw_background():\n    corners = [(0, 0), (0, _canvas_ys),\n               (_canvas_xs, _canvas_ys), (_canvas_xs, 0)]\n    polygon(corners, _bg_color, fillColor=_bg_color,\n            filled=True, smoothed=False)\n\n\ndef _destroy_window(event=None):\n    sys.exit(0)\n#    global _root_window\n#    _root_window.destroy()\n#    _root_window = None\n    # print ""DESTROY""\n\n\ndef end_graphics():\n    global _root_window, _canvas, _mouse_enabled\n    try:\n        try:\n            sleep(1)\n            if _root_window is not None:\n                _root_window.destroy()\n        except SystemExit as e:\n            print(\'Ending graphics raised an exception:\', e)\n    finally:\n        _root_window = None\n        _canvas = None\n        _mouse_enabled = 0\n        _clear_keys()\n\n\ndef clear_screen(background=None):\n    global _canvas_x, _canvas_y\n    _canvas.delete(\'all\')\n    draw_background()\n    _canvas_x, _canvas_y = 0, _canvas_ys\n\n\ndef polygon(coords, outlineColor, fillColor=None, filled=1, smoothed=1, behind=0, width=1):\n    c = []\n    for coord in coords:\n        c.append(coord[0])\n        c.append(coord[1])\n    if fillColor is None:\n        fillColor = outlineColor\n    if filled == 0:\n        fillColor = """"\n    poly = _canvas.create_polygon(\n        c, outline=outlineColor, fill=fillColor, smooth=smoothed, width=width)\n    if behind > 0:\n        _canvas.tag_lower(poly, behind)  # Higher should be more visible\n    return poly\n\n\ndef square(pos, r, color, filled=1, behind=0):\n    x, y = pos\n    coords = [(x - r, y - r), (x + r, y - r), (x + r, y + r), (x - r, y + r)]\n    return polygon(coords, color, color, filled, 0, behind=behind)\n\n\ndef circle(pos, r, outlineColor, fillColor, endpoints=None, style=\'pieslice\', width=2):\n    x, y = pos\n    x0, x1 = x - r - 1, x + r\n    y0, y1 = y - r - 1, y + r\n    if endpoints is None:\n        e = [0, 359]\n    else:\n        e = list(endpoints)\n    while e[0] > e[1]:\n        e[1] = e[1] + 360\n\n    return _canvas.create_arc(x0, y0, x1, y1, outline=outlineColor, fill=fillColor,\n                              extent=e[1] - e[0], start=e[0], style=style, width=width)\n\n\ndef image(pos, file=""../../blueghost.gif""):\n    x, y = pos\n    # img = PhotoImage(file=file)\n    return _canvas.create_image(x, y, image=tkinter.PhotoImage(file=file), anchor=tkinter.NW)\n\n\ndef refresh():\n    _canvas.update_idletasks()\n\n\ndef moveCircle(id, pos, r, endpoints=None):\n    global _canvas_x, _canvas_y\n\n    x, y = pos\n#    x0, x1 = x - r, x + r + 1\n#    y0, y1 = y - r, y + r + 1\n    x0, x1 = x - r - 1, x + r\n    y0, y1 = y - r - 1, y + r\n    if endpoints is None:\n        e = [0, 359]\n    else:\n        e = list(endpoints)\n    while e[0] > e[1]:\n        e[1] = e[1] + 360\n\n    edit(id, (\'start\', e[0]), (\'extent\', e[1] - e[0]))\n    move_to(id, x0, y0)\n\n\ndef edit(id, *args):\n    _canvas.itemconfigure(id, **dict(args))\n\n\ndef text(pos, color, contents, font=\'Helvetica\', size=12, style=\'normal\', anchor=""nw""):\n    global _canvas_x, _canvas_y\n    x, y = pos\n    font = (font, str(size), style)\n    return _canvas.create_text(x, y, fill=color, text=contents, font=font, anchor=anchor)\n\n\ndef changeText(id, newText, font=None, size=12, style=\'normal\'):\n    _canvas.itemconfigure(id, text=newText)\n    if font is not None:\n        _canvas.itemconfigure(id, font=(font, \'-%d\' % size, style))\n\n\ndef changeColor(id, newColor):\n    _canvas.itemconfigure(id, fill=newColor)\n\n\ndef line(here, there, color=formatColor(0, 0, 0), width=2):\n    x0, y0 = here[0], here[1]\n    x1, y1 = there[0], there[1]\n    return _canvas.create_line(x0, y0, x1, y1, fill=color, width=width)\n\n##############################################################################\n### Keypress handling ########################################################\n##############################################################################\n\n# We bind to key-down and key-up events.\n\n\n_keysdown = {}\n_keyswaiting = {}\n# This holds an unprocessed key release.  We delay key releases by up to\n# one call to keys_pressed() to get round a problem with auto repeat.\n_got_release = None\n\n\ndef _keypress(event):\n    global _got_release\n    # remap_arrows(event)\n    _keysdown[event.keysym] = 1\n    _keyswaiting[event.keysym] = 1\n#    print event.char, event.keycode\n    _got_release = None\n\n\ndef _keyrelease(event):\n    global _got_release\n    # remap_arrows(event)\n    try:\n        del _keysdown[event.keysym]\n    except:\n        pass\n    _got_release = 1\n\n\ndef remap_arrows(event):\n    # TURN ARROW PRESSES INTO LETTERS (SHOULD BE IN KEYBOARD AGENT)\n    if event.char in [\'a\', \'s\', \'d\', \'w\']:\n        return\n    if event.keycode in [37, 101]:  # LEFT ARROW (win / x)\n        event.char = \'a\'\n    if event.keycode in [38, 99]:  # UP ARROW\n        event.char = \'w\'\n    if event.keycode in [39, 102]:  # RIGHT ARROW\n        event.char = \'d\'\n    if event.keycode in [40, 104]:  # DOWN ARROW\n        event.char = \'s\'\n\n\ndef _clear_keys(event=None):\n    global _keysdown, _got_release, _keyswaiting\n    _keysdown = {}\n    _keyswaiting = {}\n    _got_release = None\n\n\ndef keys_pressed(d_o_e=lambda arg: _root_window.dooneevent(arg),\n                 d_w=tkinter._tkinter.DONT_WAIT):\n    d_o_e(d_w)\n    if _got_release:\n        d_o_e(d_w)\n    return list(_keysdown.keys())\n\n\ndef keys_waiting():\n    global _keyswaiting\n    keys = list(_keyswaiting.keys())\n    _keyswaiting = {}\n    return keys\n\n# Block for a list of keys...\n\n\ndef wait_for_keys():\n    keys = []\n    while keys == []:\n        keys = keys_pressed()\n        sleep(0.05)\n    return keys\n\n\ndef remove_from_screen(x,\n                       d_o_e=lambda arg: _root_window.dooneevent(arg),\n                       d_w=tkinter._tkinter.DONT_WAIT):\n    _canvas.delete(x)\n    d_o_e(d_w)\n\n\ndef _adjust_coords(coord_list, x, y):\n    for i in range(0, len(coord_list), 2):\n        coord_list[i] = coord_list[i] + x\n        coord_list[i + 1] = coord_list[i + 1] + y\n    return coord_list\n\n\ndef move_to(object, x, y=None,\n            d_o_e=lambda arg: _root_window.dooneevent(arg),\n            d_w=tkinter._tkinter.DONT_WAIT):\n    if y is None:\n        try:\n            x, y = x\n        except:\n            raise RuntimeError(\'incomprehensible coordinates\')\n\n    horiz = True\n    newCoords = []\n    current_x, current_y = _canvas.coords(object)[0:2]  # first point\n    for coord in _canvas.coords(object):\n        if horiz:\n            inc = x - current_x\n        else:\n            inc = y - current_y\n        horiz = not horiz\n\n        newCoords.append(coord + inc)\n\n    _canvas.coords(object, *newCoords)\n    d_o_e(d_w)\n\n\ndef move_by(object, x, y=None,\n            d_o_e=lambda arg: _root_window.dooneevent(arg),\n            d_w=tkinter._tkinter.DONT_WAIT):\n    if y is None:\n        try:\n            x, y = x\n        except:\n            raise Exception(\'incomprehensible coordinates\')\n\n    horiz = True\n    newCoords = []\n    for coord in _canvas.coords(object):\n        if horiz:\n            inc = x\n        else:\n            inc = y\n        horiz = not horiz\n\n        newCoords.append(coord + inc)\n\n    _canvas.coords(object, *newCoords)\n    d_o_e(d_w)\n\n\ndef writePostscript(filename):\n    ""Writes the current canvas to a postscript file.""\n    psfile = file(filename, \'w\')\n    psfile.write(_canvas.postscript(pageanchor=\'sw\',\n                                    y=\'0.c\',\n                                    x=\'0.c\'))\n    psfile.close()\n\n\nghost_shape = [\n    (0, - 0.5),\n    (0.25, - 0.75),\n    (0.5, - 0.5),\n    (0.75, - 0.75),\n    (0.75, 0.5),\n    (0.5, 0.75),\n    (- 0.5, 0.75),\n    (- 0.75, 0.5),\n    (- 0.75, - 0.75),\n    (- 0.5, - 0.5),\n    (- 0.25, - 0.75)\n]\n\nif __name__ == \'__main__\':\n    begin_graphics()\n    clear_screen()\n    ghost_shape = [(x * 10 + 20, y * 10 + 20) for x, y in ghost_shape]\n    g = polygon(ghost_shape, formatColor(1, 1, 1))\n    move_to(g, (50, 50))\n    circle((150, 150), 20, formatColor(0.7, 0.3, 0.0), endpoints=[15, - 15])\n    sleep(2)\n'"
week03_model_free/crawler_and_pacman/seminar_py3/gridworld.py,0,"b'# gridworld.py\n# ------------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nimport random\nimport sys\nimport mdp\nimport environment\nimport util\nimport optparse\n\n\nclass Gridworld(mdp.MarkovDecisionProcess):\n    """"""\n      Gridworld\n    """"""\n\n    def __init__(self, grid):\n        # layout\n        if type(grid) == type([]):\n            grid = makeGrid(grid)\n        self.grid = grid\n\n        # parameters\n        self.livingReward = 0.0\n        self.noise = 0.2\n\n    def setLivingReward(self, reward):\n        """"""\n        The (negative) reward for exiting ""normal"" states.\n\n        Note that in the R+N text, this reward is on entering\n        a state and therefore is not clearly part of the state\'s\n        future rewards.\n        """"""\n        self.livingReward = reward\n\n    def setNoise(self, noise):\n        """"""\n        The probability of moving in an unintended direction.\n        """"""\n        self.noise = noise\n\n    def getPossibleActions(self, state):\n        """"""\n        Returns list of valid actions for \'state\'.\n\n        Note that you can request moves into walls and\n        that ""exit"" states transition to the terminal\n        state under the special action ""done"".\n        """"""\n        if state == self.grid.terminalState:\n            return ()\n        x, y = state\n        if type(self.grid[x][y]) == int:\n            return (\'exit\',)\n        return (\'north\', \'west\', \'south\', \'east\')\n\n    def getStates(self):\n        """"""\n        Return list of all states.\n        """"""\n        # The true terminal state.\n        states = [self.grid.terminalState]\n        for x in range(self.grid.width):\n            for y in range(self.grid.height):\n                if self.grid[x][y] != \'#\':\n                    state = (x, y)\n                    states.append(state)\n        return states\n\n    def getReward(self, state, action, nextState):\n        """"""\n        Get reward for state, action, nextState transition.\n\n        Note that the reward depends only on the state being\n        departed (as in the R+N book examples, which more or\n        less use this convention).\n        """"""\n        if state == self.grid.terminalState:\n            return 0.0\n        x, y = state\n        cell = self.grid[x][y]\n        if type(cell) == int or type(cell) == float:\n            return cell\n        return self.livingReward\n\n    def getStartState(self):\n        for x in range(self.grid.width):\n            for y in range(self.grid.height):\n                if self.grid[x][y] == \'S\':\n                    return (x, y)\n        raise RuntimeError(\'Grid has no start state\')\n\n    def isTerminal(self, state):\n        """"""\n        Only the TERMINAL_STATE state is *actually* a terminal state.\n        The other ""exit"" states are technically non-terminals with\n        a single action ""exit"" which leads to the true terminal state.\n        This convention is to make the grids line up with the examples\n        in the R+N textbook.\n        """"""\n        return state == self.grid.terminalState\n\n    def getTransitionStatesAndProbs(self, state, action):\n        """"""\n        Returns list of (nextState, prob) pairs\n        representing the states reachable\n        from \'state\' by taking \'action\' along\n        with their transition probabilities.          \n        """"""\n\n        if action not in self.getPossibleActions(state):\n            raise RuntimeError(""Illegal action!"")\n\n        if self.isTerminal(state):\n            return []\n\n        x, y = state\n\n        if type(self.grid[x][y]) == int or type(self.grid[x][y]) == float:\n            termState = self.grid.terminalState\n            return [(termState, 1.0)]\n\n        successors = []\n\n        northState = (self.__isAllowed(y+1, x) and (x, y+1)) or state\n        westState = (self.__isAllowed(y, x-1) and (x-1, y)) or state\n        southState = (self.__isAllowed(y-1, x) and (x, y-1)) or state\n        eastState = (self.__isAllowed(y, x+1) and (x+1, y)) or state\n\n        if action == \'north\' or action == \'south\':\n            if action == \'north\':\n                successors.append((northState, 1-self.noise))\n            else:\n                successors.append((southState, 1-self.noise))\n\n            massLeft = self.noise\n            successors.append((westState, massLeft/2.0))\n            successors.append((eastState, massLeft/2.0))\n\n        if action == \'west\' or action == \'east\':\n            if action == \'west\':\n                successors.append((westState, 1-self.noise))\n            else:\n                successors.append((eastState, 1-self.noise))\n\n            massLeft = self.noise\n            successors.append((northState, massLeft/2.0))\n            successors.append((southState, massLeft/2.0))\n\n        successors = self.__aggregate(successors)\n\n        return successors\n\n    def __aggregate(self, statesAndProbs):\n        counter = util.Counter()\n        for state, prob in statesAndProbs:\n            counter[state] += prob\n        newStatesAndProbs = []\n        for state, prob in list(counter.items()):\n            newStatesAndProbs.append((state, prob))\n        return newStatesAndProbs\n\n    def __isAllowed(self, y, x):\n        if y < 0 or y >= self.grid.height:\n            return False\n        if x < 0 or x >= self.grid.width:\n            return False\n        return self.grid[x][y] != \'#\'\n\n\nclass GridworldEnvironment(environment.Environment):\n\n    def __init__(self, gridWorld):\n        self.gridWorld = gridWorld\n        self.reset()\n\n    def getCurrentState(self):\n        return self.state\n\n    def getPossibleActions(self, state):\n        return self.gridWorld.getPossibleActions(state)\n\n    def doAction(self, action):\n        successors = self.gridWorld.getTransitionStatesAndProbs(\n            self.state, action)\n        sum = 0.0\n        rand = random.random()\n        state = self.getCurrentState()\n        for nextState, prob in successors:\n            sum += prob\n            if sum > 1.0:\n                raise RuntimeError(\n                    \'Total transition probability more than one; sample failure.\')\n            if rand < sum:\n                reward = self.gridWorld.getReward(state, action, nextState)\n                self.state = nextState\n                return (nextState, reward)\n        raise RuntimeError(\n            \'Total transition probability less than one; sample failure.\')\n\n    def reset(self):\n        self.state = self.gridWorld.getStartState()\n\n\nclass Grid:\n    """"""\n    A 2-dimensional array of immutables backed by a list of lists.  Data is accessed\n    via grid[x][y] where (x,y) are cartesian coordinates with x horizontal,\n    y vertical and the origin (0,0) in the bottom left corner.  \n\n    The __str__ method constructs an output that is oriented appropriately.\n    """"""\n\n    def __init__(self, width, height, initialValue=\' \'):\n        self.width = width\n        self.height = height\n        self.data = [[initialValue for y in range(\n            height)] for x in range(width)]\n        self.terminalState = \'TERMINAL_STATE\'\n\n    def __getitem__(self, i):\n        return self.data[i]\n\n    def __setitem__(self, key, item):\n        self.data[key] = item\n\n    def __eq__(self, other):\n        if other is None:\n            return False\n        return self.data == other.data\n\n    def __hash__(self):\n        return hash(self.data)\n\n    def copy(self):\n        g = Grid(self.width, self.height)\n        g.data = [x[:] for x in self.data]\n        return g\n\n    def deepCopy(self):\n        return self.copy()\n\n    def shallowCopy(self):\n        g = Grid(self.width, self.height)\n        g.data = self.data\n        return g\n\n    def _getLegacyText(self):\n        t = [[self.data[x][y]\n              for x in range(self.width)] for y in range(self.height)]\n        t.reverse()\n        return t\n\n    def __str__(self):\n        return str(self._getLegacyText())\n\n\ndef makeGrid(gridString):\n    width, height = len(gridString[0]), len(gridString)\n    grid = Grid(width, height)\n    for ybar, line in enumerate(gridString):\n        y = height - ybar - 1\n        for x, el in enumerate(line):\n            grid[x][y] = el\n    return grid\n\n\ndef getCliffGrid():\n    grid = [[\' \', \' \', \' \', \' \', \' \'],\n            [\'S\', \' \', \' \', \' \', 10],\n            [-100, -100, -100, -100, -100]]\n    return Gridworld(makeGrid(grid))\n\n\ndef getCliffGrid2():\n    grid = [[\' \', \' \', \' \', \' \', \' \'],\n            [8, \'S\', \' \', \' \', 10],\n            [-100, -100, -100, -100, -100]]\n    return Gridworld(grid)\n\n\ndef getDiscountGrid():\n    grid = [[\' \', \' \', \' \', \' \', \' \'],\n            [\' \', \'#\', \' \', \' \', \' \'],\n            [\' \', \'#\', 1, \'#\', 10],\n            [\'S\', \' \', \' \', \' \', \' \'],\n            [-10, -10, -10, -10, -10]]\n    return Gridworld(grid)\n\n\ndef getBridgeGrid():\n    grid = [[\'#\', -100, -100, -100, -100, -100, \'#\'],\n            [1, \'S\',  \' \',  \' \',  \' \',  \' \',  10],\n            [\'#\', -100, -100, -100, -100, -100, \'#\']]\n    return Gridworld(grid)\n\n\ndef getBookGrid():\n    grid = [[\' \', \' \', \' \', +1],\n            [\' \', \'#\', \' \', -1],\n            [\'S\', \' \', \' \', \' \']]\n    return Gridworld(grid)\n\n\ndef getMazeGrid():\n    grid = [[\' \', \' \', \' \', +1],\n            [\'#\', \'#\', \' \', \'#\'],\n            [\' \', \'#\', \' \', \' \'],\n            [\' \', \'#\', \'#\', \' \'],\n            [\'S\', \' \', \' \', \' \']]\n    return Gridworld(grid)\n\n\ndef getUserAction(state, actionFunction):\n    """"""\n    Get an action from the user (rather than the agent).\n\n    Used for debugging and lecture demos.\n    """"""\n    import graphicsUtils\n    action = None\n    while True:\n        keys = graphicsUtils.wait_for_keys()\n        if \'Up\' in keys:\n            action = \'north\'\n        if \'Down\' in keys:\n            action = \'south\'\n        if \'Left\' in keys:\n            action = \'west\'\n        if \'Right\' in keys:\n            action = \'east\'\n        if \'q\' in keys:\n            sys.exit(0)\n        if action is None:\n            continue\n        break\n    actions = actionFunction(state)\n    if action not in actions:\n        action = actions[0]\n    return action\n\n\ndef printString(x): print(x)\n\n\ndef runEpisode(agent, environment, discount, decision, display, message, pause, episode):\n    returns = 0\n    totalDiscount = 1.0\n    environment.reset()\n    if \'startEpisode\' in dir(agent):\n        agent.startEpisode()\n    message(""BEGINNING EPISODE: ""+str(episode)+""\\n"")\n    while True:\n\n        # DISPLAY CURRENT STATE\n        state = environment.getCurrentState()\n        display(state)\n        pause()\n\n        # END IF IN A TERMINAL STATE\n        actions = environment.getPossibleActions(state)\n        if len(actions) == 0:\n            message(""EPISODE ""+str(episode) +\n                    "" COMPLETE: RETURN WAS ""+str(returns)+""\\n"")\n            return returns\n\n        # GET ACTION (USUALLY FROM AGENT)\n        action = decision(state)\n        if action is None:\n            raise RuntimeError(\'Error: Agent returned None action\')\n\n        # EXECUTE ACTION\n        nextState, reward = environment.doAction(action)\n        message(""Started in state: ""+str(state) +\n                ""\\nTook action: ""+str(action) +\n                ""\\nEnded in state: ""+str(nextState) +\n                ""\\nGot reward: ""+str(reward)+""\\n"")\n        # UPDATE LEARNER\n        if \'observeTransition\' in dir(agent):\n            agent.observeTransition(state, action, nextState, reward)\n\n        returns += reward * totalDiscount\n        totalDiscount *= discount\n\n    if \'stopEpisode\' in dir(agent):\n        agent.stopEpisode()\n\n\ndef parseOptions():\n    optParser = optparse.OptionParser()\n    optParser.add_option(\'-d\', \'--discount\', action=\'store\',\n                         type=\'float\', dest=\'discount\', default=0.9,\n                         help=\'Discount on future (default %default)\')\n    optParser.add_option(\'-r\', \'--livingReward\', action=\'store\',\n                         type=\'float\', dest=\'livingReward\', default=0.0,\n                         metavar=""R"", help=\'Reward for living for a time step (default %default)\')\n    optParser.add_option(\'-n\', \'--noise\', action=\'store\',\n                         type=\'float\', dest=\'noise\', default=0.2,\n                         metavar=""P"", help=\'How often action results in \' +\n                         \'unintended direction (default %default)\')\n    optParser.add_option(\'-e\', \'--epsilon\', action=\'store\',\n                         type=\'float\', dest=\'epsilon\', default=0.3,\n                         metavar=""E"", help=\'Chance of taking a random action in q-learning (default %default)\')\n    optParser.add_option(\'-l\', \'--learningRate\', action=\'store\',\n                         type=\'float\', dest=\'learningRate\', default=0.5,\n                         metavar=""P"", help=\'TD learning rate (default %default)\')\n    optParser.add_option(\'-i\', \'--iterations\', action=\'store\',\n                         type=\'int\', dest=\'iters\', default=10,\n                         metavar=""K"", help=\'Number of rounds of value iteration (default %default)\')\n    optParser.add_option(\'-k\', \'--episodes\', action=\'store\',\n                         type=\'int\', dest=\'episodes\', default=1,\n                         metavar=""K"", help=\'Number of epsiodes of the MDP to run (default %default)\')\n    optParser.add_option(\'-g\', \'--grid\', action=\'store\',\n                         metavar=""G"", type=\'string\', dest=\'grid\', default=""BookGrid"",\n                         help=\'Grid to use (case sensitive; options are BookGrid, BridgeGrid, CliffGrid, MazeGrid, default %default)\')\n    optParser.add_option(\'-w\', \'--windowSize\', metavar=""X"", type=\'int\', dest=\'gridSize\', default=150,\n                         help=\'Request a window width of X pixels *per grid cell* (default %default)\')\n    optParser.add_option(\'-a\', \'--agent\', action=\'store\', metavar=""A"",\n                         type=\'string\', dest=\'agent\', default=""random"",\n                         help=\'Agent type (options are \\\'random\\\', \\\'value\\\' and \\\'q\\\', default %default)\')\n    optParser.add_option(\'-t\', \'--text\', action=\'store_true\',\n                         dest=\'textDisplay\', default=False,\n                         help=\'Use text-only ASCII display\')\n    optParser.add_option(\'-p\', \'--pause\', action=\'store_true\',\n                         dest=\'pause\', default=False,\n                         help=\'Pause GUI after each time step when running the MDP\')\n    optParser.add_option(\'-q\', \'--quiet\', action=\'store_true\',\n                         dest=\'quiet\', default=False,\n                         help=\'Skip display of any learning episodes\')\n    optParser.add_option(\'-s\', \'--speed\', action=\'store\', metavar=""S"", type=float,\n                         dest=\'speed\', default=1.0,\n                         help=\'Speed of animation, S > 1.0 is faster, 0.0 < S < 1.0 is slower (default %default)\')\n    optParser.add_option(\'-m\', \'--manual\', action=\'store_true\',\n                         dest=\'manual\', default=False,\n                         help=\'Manually control agent\')\n    optParser.add_option(\'-v\', \'--valueSteps\', action=\'store_true\', default=False,\n                         help=\'Display each step of value iteration\')\n\n    opts, args = optParser.parse_args()\n\n    if opts.manual and opts.agent != \'q\':\n        print(\'## Disabling Agents in Manual Mode (-m) ##\')\n        opts.agent = None\n\n    # MANAGE CONFLICTS\n    if opts.textDisplay or opts.quiet:\n        # if opts.quiet:\n        opts.pause = False\n        # opts.manual = False\n\n    if opts.manual:\n        opts.pause = True\n\n    return opts\n\n\nif __name__ == \'__main__\':\n\n    opts = parseOptions()\n\n    ###########################\n    # GET THE GRIDWORLD\n    ###########################\n\n    import gridworld\n    mdpFunction = getattr(gridworld, ""get""+opts.grid)\n    mdp = mdpFunction()\n    mdp.setLivingReward(opts.livingReward)\n    mdp.setNoise(opts.noise)\n    env = gridworld.GridworldEnvironment(mdp)\n\n    ###########################\n    # GET THE DISPLAY ADAPTER\n    ###########################\n\n    import textGridworldDisplay\n    display = textGridworldDisplay.TextGridworldDisplay(mdp)\n    if not opts.textDisplay:\n        import graphicsGridworldDisplay\n        display = graphicsGridworldDisplay.GraphicsGridworldDisplay(\n            mdp, opts.gridSize, opts.speed)\n    display.start()\n\n    ###########################\n    # GET THE AGENT\n    ###########################\n\n    import valueIterationAgents\n    import qlearningAgents\n    a = None\n    if opts.agent == \'value\':\n        a = valueIterationAgents.ValueIterationAgent(\n            mdp, opts.discount, opts.iters)\n    elif opts.agent == \'q\':\n        #env.getPossibleActions, opts.discount, opts.learningRate, opts.epsilon\n        #simulationFn = lambda agent, state: simulation.GridworldSimulation(agent,state,mdp)\n        gridWorldEnv = GridworldEnvironment(mdp)\n        def actionFn(state): return mdp.getPossibleActions(state)\n        qLearnOpts = {\'gamma\': opts.discount,\n                      \'alpha\': opts.learningRate,\n                      \'epsilon\': opts.epsilon,\n                      \'actionFn\': actionFn}\n        a = qlearningAgents.QLearningAgent(**qLearnOpts)\n    elif opts.agent == \'random\':\n        # # No reason to use the random agent without episodes\n        if opts.episodes == 0:\n            opts.episodes = 10\n\n        class RandomAgent:\n            def getAction(self, state):\n                return random.choice(mdp.getPossibleActions(state))\n\n            def getValue(self, state):\n                return 0.0\n\n            def getQValue(self, state, action):\n                return 0.0\n\n            def getPolicy(self, state):\n                ""NOTE: \'random\' is a special policy value; don\'t use it in your code.""\n                return \'random\'\n\n            def update(self, state, action, nextState, reward):\n                pass\n        a = RandomAgent()\n    else:\n        if not opts.manual:\n            raise RuntimeError(\'Unknown agent type: \' + opts.agent)\n\n    ###########################\n    # RUN EPISODES\n    ###########################\n    # DISPLAY Q/V VALUES BEFORE SIMULATION OF EPISODES\n    if not opts.manual and opts.agent == \'value\':\n        if opts.valueSteps:\n            for i in range(opts.iters):\n                tempAgent = valueIterationAgents.ValueIterationAgent(\n                    mdp, opts.discount, i)\n                display.displayValues(\n                    tempAgent, message=""VALUES AFTER ""+str(i)+"" ITERATIONS"")\n                display.pause()\n\n        display.displayValues(a, message=""VALUES AFTER "" +\n                              str(opts.iters)+"" ITERATIONS"")\n        display.pause()\n        display.displayQValues(\n            a, message=""Q-VALUES AFTER ""+str(opts.iters)+"" ITERATIONS"")\n        display.pause()\n\n    # FIGURE OUT WHAT TO DISPLAY EACH TIME STEP (IF ANYTHING)\n    def displayCallback(x): return None\n    if not opts.quiet:\n        if opts.manual and opts.agent is None:\n            def displayCallback(state): return display.displayNullValues(state)\n        else:\n            if opts.agent == \'random\':\n                def displayCallback(state): return display.displayValues(\n                    a, state, ""CURRENT VALUES"")\n            if opts.agent == \'value\':\n                def displayCallback(state): return display.displayValues(\n                    a, state, ""CURRENT VALUES"")\n            if opts.agent == \'q\':\n                def displayCallback(state): return display.displayQValues(\n                    a, state, ""CURRENT Q-VALUES"")\n\n    def messageCallback(x): return printString(x)\n    if opts.quiet:\n        def messageCallback(x): return None\n\n    # FIGURE OUT WHETHER TO WAIT FOR A KEY PRESS AFTER EACH TIME STEP\n    def pauseCallback(): return None\n    if opts.pause:\n        def pauseCallback(): return display.pause()\n\n    # FIGURE OUT WHETHER THE USER WANTS MANUAL CONTROL (FOR DEBUGGING AND DEMOS)\n    if opts.manual:\n        def decisionCallback(state): return getUserAction(\n            state, mdp.getPossibleActions)\n    else:\n        decisionCallback = a.getAction\n\n    # RUN EPISODES\n    if opts.episodes > 0:\n        print()\n        print(""RUNNING"", opts.episodes, ""EPISODES"")\n        print()\n    returns = 0\n    for episode in range(1, opts.episodes+1):\n        returns += runEpisode(a, env, opts.discount, decisionCallback,\n                              displayCallback, messageCallback, pauseCallback, episode)\n    if opts.episodes > 0:\n        print()\n        print(""AVERAGE RETURNS FROM START STATE: "" +\n              str((returns+0.0) / opts.episodes))\n        print()\n        print()\n\n    # DISPLAY POST-LEARNING VALUES / Q-VALUES\n    if opts.agent == \'q\' and not opts.manual:\n        display.displayQValues(\n            a, message=""Q-VALUES AFTER ""+str(opts.episodes)+"" EPISODES"")\n        display.pause()\n        display.displayValues(a, message=""VALUES AFTER "" +\n                              str(opts.episodes)+"" EPISODES"")\n        display.pause()\n'"
week03_model_free/crawler_and_pacman/seminar_py3/keyboardAgents.py,0,"b'# keyboardAgents.py\n# -----------------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nfrom game import Agent\nfrom game import Directions\nimport random\n\n\nclass KeyboardAgent(Agent):\n    """"""\n    An agent controlled by the keyboard.\n    """"""\n    # NOTE: Arrow keys also work.\n    WEST_KEY = \'a\'\n    EAST_KEY = \'d\'\n    NORTH_KEY = \'w\'\n    SOUTH_KEY = \'s\'\n    STOP_KEY = \'q\'\n\n    def __init__(self, index=0):\n\n        self.lastMove = Directions.STOP\n        self.index = index\n        self.keys = []\n\n    def getAction(self, state):\n        from graphicsUtils import keys_waiting\n        from graphicsUtils import keys_pressed\n        keys = keys_waiting() + keys_pressed()\n        if keys != []:\n            self.keys = keys\n\n        legal = state.getLegalActions(self.index)\n        move = self.getMove(legal)\n\n        if move == Directions.STOP:\n            # Try to move in the same direction as before\n            if self.lastMove in legal:\n                move = self.lastMove\n\n        if (self.STOP_KEY in self.keys) and Directions.STOP in legal:\n            move = Directions.STOP\n\n        if move not in legal:\n            move = random.choice(legal)\n\n        self.lastMove = move\n        return move\n\n    def getMove(self, legal):\n        move = Directions.STOP\n        if (self.WEST_KEY in self.keys or \'Left\' in self.keys) and Directions.WEST in legal:\n            move = Directions.WEST\n        if (self.EAST_KEY in self.keys or \'Right\' in self.keys) and Directions.EAST in legal:\n            move = Directions.EAST\n        if (self.NORTH_KEY in self.keys or \'Up\' in self.keys) and Directions.NORTH in legal:\n            move = Directions.NORTH\n        if (self.SOUTH_KEY in self.keys or \'Down\' in self.keys) and Directions.SOUTH in legal:\n            move = Directions.SOUTH\n        return move\n\n\nclass KeyboardAgent2(KeyboardAgent):\n    """"""\n    A second agent controlled by the keyboard.\n    """"""\n    # NOTE: Arrow keys also work.\n    WEST_KEY = \'j\'\n    EAST_KEY = ""l""\n    NORTH_KEY = \'i\'\n    SOUTH_KEY = \'k\'\n    STOP_KEY = \'u\'\n\n    def getMove(self, legal):\n        move = Directions.STOP\n        if (self.WEST_KEY in self.keys) and Directions.WEST in legal:\n            move = Directions.WEST\n        if (self.EAST_KEY in self.keys) and Directions.EAST in legal:\n            move = Directions.EAST\n        if (self.NORTH_KEY in self.keys) and Directions.NORTH in legal:\n            move = Directions.NORTH\n        if (self.SOUTH_KEY in self.keys) and Directions.SOUTH in legal:\n            move = Directions.SOUTH\n        return move\n'"
week03_model_free/crawler_and_pacman/seminar_py3/layout.py,0,"b'# layout.py\n# ---------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nfrom util import manhattanDistance\nfrom game import Grid\nimport os\nimport random\nfrom functools import reduce\n\nVISIBILITY_MATRIX_CACHE = {}\n\n\nclass Layout:\n    """"""\n    A Layout manages the static information about the game board.\n    """"""\n\n    def __init__(self, layoutText):\n        self.width = len(layoutText[0])\n        self.height = len(layoutText)\n        self.walls = Grid(self.width, self.height, False)\n        self.food = Grid(self.width, self.height, False)\n        self.capsules = []\n        self.agentPositions = []\n        self.numGhosts = 0\n        self.processLayoutText(layoutText)\n        self.layoutText = layoutText\n        # self.initializeVisibilityMatrix()\n\n    def getNumGhosts(self):\n        return self.numGhosts\n\n    def initializeVisibilityMatrix(self):\n        global VISIBILITY_MATRIX_CACHE\n        if reduce(str.__add__, self.layoutText) not in VISIBILITY_MATRIX_CACHE:\n            from game import Directions\n            vecs = [(-0.5, 0), (0.5, 0), (0, -0.5), (0, 0.5)]\n            dirs = [Directions.NORTH, Directions.SOUTH,\n                    Directions.WEST, Directions.EAST]\n            vis = Grid(self.width, self.height, {Directions.NORTH: set(), Directions.SOUTH: set(\n            ), Directions.EAST: set(), Directions.WEST: set(), Directions.STOP: set()})\n            for x in range(self.width):\n                for y in range(self.height):\n                    if self.walls[x][y] == False:\n                        for vec, direction in zip(vecs, dirs):\n                            dx, dy = vec\n                            nextx, nexty = x + dx, y + dy\n                            while (nextx + nexty) != int(nextx) + int(nexty) or not self.walls[int(nextx)][int(nexty)]:\n                                vis[x][y][direction].add((nextx, nexty))\n                                nextx, nexty = x + dx, y + dy\n            self.visibility = vis\n            VISIBILITY_MATRIX_CACHE[reduce(str.__add__, self.layoutText)] = vis\n        else:\n            self.visibility = VISIBILITY_MATRIX_CACHE[reduce(\n                str.__add__, self.layoutText)]\n\n    def isWall(self, pos):\n        x, col = pos\n        return self.walls[x][col]\n\n    def getRandomLegalPosition(self):\n        x = random.choice(list(range(self.width)))\n        y = random.choice(list(range(self.height)))\n        while self.isWall((x, y)):\n            x = random.choice(list(range(self.width)))\n            y = random.choice(list(range(self.height)))\n        return (x, y)\n\n    def getRandomCorner(self):\n        poses = [(1, 1), (1, self.height - 2), (self.width - 2, 1),\n                 (self.width - 2, self.height - 2)]\n        return random.choice(poses)\n\n    def getFurthestCorner(self, pacPos):\n        poses = [(1, 1), (1, self.height - 2), (self.width - 2, 1),\n                 (self.width - 2, self.height - 2)]\n        dist, pos = max([(manhattanDistance(p, pacPos), p) for p in poses])\n        return pos\n\n    def isVisibleFrom(self, ghostPos, pacPos, pacDirection):\n        row, col = [int(x) for x in pacPos]\n        return ghostPos in self.visibility[row][col][pacDirection]\n\n    def __str__(self):\n        return ""\\n"".join(self.layoutText)\n\n    def deepCopy(self):\n        return Layout(self.layoutText[:])\n\n    def processLayoutText(self, layoutText):\n        """"""\n        Coordinates are flipped from the input format to the (x,y) convention here\n\n        The shape of the maze.  Each character  \n        represents a different type of object.   \n         % - Wall                               \n         . - Food\n         o - Capsule\n         G - Ghost\n         P - Pacman\n        Other characters are ignored.\n        """"""\n        maxY = self.height - 1\n        for y in range(self.height):\n            for x in range(self.width):\n                layoutChar = layoutText[maxY - y][x]\n                self.processLayoutChar(x, y, layoutChar)\n        self.agentPositions.sort()\n        self.agentPositions = [(i == 0, pos) for i, pos in self.agentPositions]\n\n    def processLayoutChar(self, x, y, layoutChar):\n        if layoutChar == \'%\':\n            self.walls[x][y] = True\n        elif layoutChar == \'.\':\n            self.food[x][y] = True\n        elif layoutChar == \'o\':\n            self.capsules.append((x, y))\n        elif layoutChar == \'P\':\n            self.agentPositions.append((0, (x, y)))\n        elif layoutChar in [\'G\']:\n            self.agentPositions.append((1, (x, y)))\n            self.numGhosts += 1\n        elif layoutChar in [\'1\', \'2\', \'3\', \'4\']:\n            self.agentPositions.append((int(layoutChar), (x, y)))\n            self.numGhosts += 1\n\n\ndef getLayout(name, back=2):\n    if name.endswith(\'.lay\'):\n        layout = tryToLoad(\'layouts/\' + name)\n        if layout is None:\n            layout = tryToLoad(name)\n    else:\n        layout = tryToLoad(\'layouts/\' + name + \'.lay\')\n        if layout is None:\n            layout = tryToLoad(name + \'.lay\')\n    if layout is None and back >= 0:\n        curdir = os.path.abspath(\'.\')\n        os.chdir(\'..\')\n        layout = getLayout(name, back - 1)\n        os.chdir(curdir)\n    return layout\n\n\ndef tryToLoad(fullname):\n    if(not os.path.exists(fullname)):\n        return None\n    f = open(fullname)\n    try:\n        return Layout([line.strip() for line in f])\n    finally:\n        f.close()\n'"
week03_model_free/crawler_and_pacman/seminar_py3/learningAgents.py,0,"b'# learningAgents.py\n# -----------------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nfrom game import Directions, Agent, Actions\n\nimport random\nimport util\nimport time\n\n\nclass ValueEstimationAgent(Agent):\n    """"""\n      Abstract agent which assigns values to (state,action)\n      Q-Values for an environment. As well as a value to a\n      state and a policy given respectively by,\n\n      V(s) = max_{a in actions} Q(s,a)\n      policy(s) = arg_max_{a in actions} Q(s,a)\n\n      Both ValueIterationAgent and QLearningAgent inherit\n      from this agent. While a ValueIterationAgent has\n      a model of the environment via a MarkovDecisionProcess\n      (see mdp.py) that is used to estimate Q-Values before\n      ever actually acting, the QLearningAgent estimates\n      Q-Values while acting in the environment.\n    """"""\n\n    def __init__(self, alpha=1.0, epsilon=0.05, gamma=0.8, numTraining=10):\n        """"""\n        Sets options, which can be passed in via the Pacman command line using -a alpha=0.5,...\n        alpha    - learning rate\n        epsilon  - exploration rate\n        gamma    - discount factor\n        numTraining - number of training episodes, i.e. no learning after these many episodes\n        """"""\n        self.alpha = float(alpha)\n        self.epsilon = float(epsilon)\n        self.discount = float(gamma)\n        self.numTraining = int(numTraining)\n\n    ####################################\n    #    Override These Functions      #\n    ####################################\n    def getQValue(self, state, action):\n        """"""\n        Should return Q(state,action)\n        """"""\n        util.raiseNotDefined()\n\n    def getValue(self, state):\n        """"""\n        What is the value of this state under the best action?\n        Concretely, this is given by\n\n        V(s) = max_{a in actions} Q(s,a)\n        """"""\n        util.raiseNotDefined()\n\n    def getPolicy(self, state):\n        """"""\n        What is the best action to take in the state. Note that because\n        we might want to explore, this might not coincide with getAction\n        Concretely, this is given by\n\n        policy(s) = arg_max_{a in actions} Q(s,a)\n\n        If many actions achieve the maximal Q-value,\n        it doesn\'t matter which is selected.\n        """"""\n        util.raiseNotDefined()\n\n    def getAction(self, state):\n        """"""\n        state: can call state.getLegalActions()\n        Choose an action and return it.\n        """"""\n        util.raiseNotDefined()\n\n\nclass ReinforcementAgent(ValueEstimationAgent):\n    """"""\n      Abstract Reinforcemnt Agent: A ValueEstimationAgent\n      which estimates Q-Values (as well as policies) from experience\n      rather than a model\n\n        What you need to know:\n        - The environment will call\n          observeTransition(state,action,nextState,deltaReward),\n          which will call update(state, action, nextState, deltaReward)\n          which you should override.\n        - Use self.getLegalActions(state) to know which actions\n          are available in a state\n    """"""\n    ####################################\n    #    Override These Functions      #\n    ####################################\n\n    def update(self, state, action, nextState, reward):\n        """"""\n          This class will call this function, which you write, after\n          observing a transition and reward\n        """"""\n        util.raiseNotDefined()\n\n    ####################################\n    #    Read These Functions          #\n    ####################################\n\n    def getLegalActions(self, state):\n        """"""\n          Get the actions available for a given\n          state. This is what you should use to\n          obtain legal actions for a state\n        """"""\n        return self.actionFn(state)\n\n    def observeTransition(self, state, action, nextState, deltaReward):\n        """"""\n          Called by environment to inform agent that a transition has\n          been observed. This will result in a call to self.update\n          on the same arguments\n\n          NOTE: Do *not* override or call this function\n        """"""\n        self.episodeRewards += deltaReward\n        self.update(state, action, nextState, deltaReward)\n\n    def startEpisode(self):\n        """"""\n          Called by environment when new episode is starting\n        """"""\n        self.lastState = None\n        self.lastAction = None\n        self.episodeRewards = 0.0\n\n    def stopEpisode(self):\n        """"""\n          Called by environment when episode is done\n        """"""\n        if self.episodesSoFar < self.numTraining:\n            self.accumTrainRewards += self.episodeRewards\n        else:\n            self.accumTestRewards += self.episodeRewards\n        self.episodesSoFar += 1\n        if self.episodesSoFar >= self.numTraining:\n            # Take off the training wheels\n            self.epsilon = 0.0    # no exploration\n            self.alpha = 0.0      # no learning\n\n    def isInTraining(self):\n        return self.episodesSoFar < self.numTraining\n\n    def isInTesting(self):\n        return not self.isInTraining()\n\n    def __init__(self, actionFn=None, numTraining=100, epsilon=0.5, alpha=0.5, gamma=1):\n        """"""\n        actionFn: Function which takes a state and returns the list of legal actions\n\n        alpha    - learning rate\n        epsilon  - exploration rate\n        gamma    - discount factor\n        numTraining - number of training episodes, i.e. no learning after these many episodes\n        """"""\n        if actionFn is None:\n            def actionFn(state): return state.getLegalActions()\n        self.actionFn = actionFn\n        self.episodesSoFar = 0\n        self.accumTrainRewards = 0.0\n        self.accumTestRewards = 0.0\n        self.numTraining = int(numTraining)\n        self.epsilon = float(epsilon)\n        self.alpha = float(alpha)\n        self.discount = float(gamma)\n\n    ################################\n    # Controls needed for Crawler  #\n    ################################\n    def setEpsilon(self, epsilon):\n        self.epsilon = epsilon\n\n    def setLearningRate(self, alpha):\n        self.alpha = alpha\n\n    def setDiscount(self, discount):\n        self.discount = discount\n\n    def doAction(self, state, action):\n        """"""\n            Called by inherited class when\n            an action is taken in a state\n        """"""\n        self.lastState = state\n        self.lastAction = action\n\n    ###################\n    # Pacman Specific #\n    ###################\n    def observationFunction(self, state):\n        """"""\n            This is where we ended up after our last action.\n            The simulation should somehow ensure this is called\n        """"""\n        if not self.lastState is None:\n            reward = state.getScore() - self.lastState.getScore()\n            self.observeTransition(\n                self.lastState, self.lastAction, state, reward)\n        return state\n\n    def registerInitialState(self, state):\n        self.startEpisode()\n        if self.episodesSoFar == 0:\n            print(\'Beginning %d episodes of Training\' % (self.numTraining))\n\n    def final(self, state):\n        """"""\n          Called by Pacman game at the terminal state\n        """"""\n        deltaReward = state.getScore() - self.lastState.getScore()\n        self.observeTransition(\n            self.lastState, self.lastAction, state, deltaReward)\n        self.stopEpisode()\n\n        # Make sure we have this var\n        if not \'episodeStartTime\' in self.__dict__:\n            self.episodeStartTime = time.time()\n        if not \'lastWindowAccumRewards\' in self.__dict__:\n            self.lastWindowAccumRewards = 0.0\n        self.lastWindowAccumRewards += state.getScore()\n\n        NUM_EPS_UPDATE = 100\n        if self.episodesSoFar % NUM_EPS_UPDATE == 0:\n            print(\'Reinforcement Learning Status:\')\n            windowAvg = self.lastWindowAccumRewards / float(NUM_EPS_UPDATE)\n            if self.episodesSoFar <= self.numTraining:\n                trainAvg = self.accumTrainRewards / float(self.episodesSoFar)\n                print(\'\\tCompleted %d out of %d training episodes\' % (\n                    self.episodesSoFar, self.numTraining))\n                print(\'\\tAverage Rewards over all training: %.2f\' % (\n                    trainAvg))\n            else:\n                testAvg = float(self.accumTestRewards) / \\\n                    (self.episodesSoFar - self.numTraining)\n                print(\'\\tCompleted %d test episodes\' %\n                      (self.episodesSoFar - self.numTraining))\n                print(\'\\tAverage Rewards over testing: %.2f\' % testAvg)\n            print(\'\\tAverage Rewards for last %d episodes: %.2f\' % (\n                NUM_EPS_UPDATE, windowAvg))\n            print(\'\\tEpisode took %.2f seconds\' %\n                  (time.time() - self.episodeStartTime))\n            self.lastWindowAccumRewards = 0.0\n            self.episodeStartTime = time.time()\n\n        if self.episodesSoFar == self.numTraining:\n            msg = \'Training Done (turning off epsilon and alpha)\'\n            print(\'%s\\n%s\' % (msg, \'-\' * len(msg)))\n'"
week03_model_free/crawler_and_pacman/seminar_py3/mdp.py,0,"b'# mdp.py\n# ------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nimport random\n\n\nclass MarkovDecisionProcess:\n\n    def getStates(self):\n        """"""\n        Return a list of all states in the MDP.\n        Not generally possible for large MDPs.\n        """"""\n        # abstract\n        pass\n\n    def getStartState(self):\n        """"""\n        Return the start state of the MDP.\n        """"""\n        # abstract\n        pass\n\n    def getPossibleActions(self, state):\n        """"""\n        Return list of possible actions from \'state\'.\n        """"""\n        # abstract\n        pass\n\n    def getTransitionStatesAndProbs(self, state, action):\n        """"""\n        Returns list of (nextState, prob) pairs\n        representing the states reachable\n        from \'state\' by taking \'action\' along\n        with their transition probabilities.  \n\n        Note that in Q-Learning and reinforcment\n        learning in general, we do not know these\n        probabilities nor do we directly model them.\n        """"""\n        # abstract\n        pass\n\n    def getReward(self, state, action, nextState):\n        """"""\n        Get the reward for the state, action, nextState transition.\n\n        Not available in reinforcement learning.\n        """"""\n        # abstract\n        pass\n\n    def isTerminal(self, state):\n        """"""\n        Returns true if the current state is a terminal state.  By convention,\n        a terminal state has zero future rewards.  Sometimes the terminal state(s)\n        may have no possible actions.  It is also common to think of the terminal\n        state as having a self-loop action \'pass\' with zero reward; the formulations\n        are equivalent.\n        """"""\n        # abstract\n        pass\n'"
week03_model_free/crawler_and_pacman/seminar_py3/pacman.py,0,"b'# pacman.py\n# ---------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\n""""""\nPacman.py holds the logic for the classic pacman game along with the main\ncode to run a game.  This file is divided into three sections:\n\n  (i)  Your interface to the pacman world:\n          Pacman is a complex environment.  You probably don\'t want to\n          read through all of the code we wrote to make the game runs\n          correctly.  This section contains the parts of the code\n          that you will need to understand in order to complete the\n          project.  There is also some code in game.py that you should\n          understand.\n\n  (ii)  The hidden secrets of pacman:\n          This section contains all of the logic code that the pacman\n          environment uses to decide who can move where, who dies when\n          things collide, etc.  You shouldn\'t need to read this section\n          of code, but you can if you want.\n\n  (iii) Framework to start a game:\n          The final section contains the code for reading the command\n          you use to set up the game, then starting up a new game, along with\n          linking in all the external parts (agent functions, graphics).\n          Check this section out to see all the options available to you.\n\nTo play your first game, type \'python pacman.py\' from the command line.\nThe keys are \'a\', \'s\', \'d\', and \'w\' to move (or arrow keys).  Have fun!\n""""""\nfrom game import GameStateData\nfrom game import Game\nfrom game import Directions\nfrom game import Actions\nfrom util import nearestPoint\nfrom util import manhattanDistance\nimport util\nimport layout\nimport sys\nimport types\nimport time\nimport random\nimport os\n\n###################################################\n# YOUR INTERFACE TO THE PACMAN WORLD: A GameState #\n###################################################\n\n\nclass GameState:\n    """"""\n    A GameState specifies the full game state, including the food, capsules,\n    agent configurations and score changes.\n\n    GameStates are used by the Game object to capture the actual state of the game and\n    can be used by agents to reason about the game.\n\n    Much of the information in a GameState is stored in a GameStateData object.  We\n    strongly suggest that you access that data via the accessor methods below rather\n    than referring to the GameStateData object directly.\n\n    Note that in classic Pacman, Pacman is always agent 0.\n    """"""\n\n    ####################################################\n    # Accessor methods: use these to access state data #\n    ####################################################\n\n    def getLegalActions(self, agentIndex=0):\n        """"""\n        Returns the legal actions for the agent specified.\n        """"""\n        if self.isWin() or self.isLose():\n            return []\n\n        if agentIndex == 0:  # Pacman is moving\n            return PacmanRules.getLegalActions(self)\n        else:\n            return GhostRules.getLegalActions(self, agentIndex)\n\n    def generateSuccessor(self, agentIndex, action):\n        """"""\n        Returns the successor state after the specified agent takes the action.\n        """"""\n        # Check that successors exist\n        if self.isWin() or self.isLose():\n            raise Exception(\'Can\\\'t generate a successor of a terminal state.\')\n\n        # Copy current state\n        state = GameState(self)\n\n        # Let agent\'s logic deal with its action\'s effects on the board\n        if agentIndex == 0:  # Pacman is moving\n            state.data._eaten = [False for i in range(state.getNumAgents())]\n            PacmanRules.applyAction(state, action)\n        else:                # A ghost is moving\n            GhostRules.applyAction(state, action, agentIndex)\n\n        # Time passes\n        if agentIndex == 0:\n            state.data.scoreChange += -TIME_PENALTY  # Penalty for waiting around\n        else:\n            GhostRules.decrementTimer(state.data.agentStates[agentIndex])\n\n        # Resolve multi-agent effects\n        GhostRules.checkDeath(state, agentIndex)\n\n        # Book keeping\n        state.data._agentMoved = agentIndex\n        state.data.score += state.data.scoreChange\n        return state\n\n    def getLegalPacmanActions(self):\n        return self.getLegalActions(0)\n\n    def generatePacmanSuccessor(self, action):\n        """"""\n        Generates the successor state after the specified pacman move\n        """"""\n        return self.generateSuccessor(0, action)\n\n    def getPacmanState(self):\n        """"""\n        Returns an AgentState object for pacman (in game.py)\n\n        state.pos gives the current position\n        state.direction gives the travel vector\n        """"""\n        return self.data.agentStates[0].copy()\n\n    def getPacmanPosition(self):\n        return self.data.agentStates[0].getPosition()\n\n    def getGhostStates(self):\n        return self.data.agentStates[1:]\n\n    def getGhostState(self, agentIndex):\n        if agentIndex == 0 or agentIndex >= self.getNumAgents():\n            raise Exception(""Invalid index passed to getGhostState"")\n        return self.data.agentStates[agentIndex]\n\n    def getGhostPosition(self, agentIndex):\n        if agentIndex == 0:\n            raise Exception(""Pacman\'s index passed to getGhostPosition"")\n        return self.data.agentStates[agentIndex].getPosition()\n\n    def getGhostPositions(self):\n        return [s.getPosition() for s in self.getGhostStates()]\n\n    def getNumAgents(self):\n        return len(self.data.agentStates)\n\n    def getScore(self):\n        return self.data.score\n\n    def getCapsules(self):\n        """"""\n        Returns a list of positions (x,y) of the remaining capsules.\n        """"""\n        return self.data.capsules\n\n    def getNumFood(self):\n        return self.data.food.count()\n\n    def getFood(self):\n        """"""\n        Returns a Grid of boolean food indicator variables.\n\n        Grids can be accessed via list notation, so to check\n        if there is food at (x,y), just call\n\n        currentFood = state.getFood()\n        if currentFood[x][y] == True: ...\n        """"""\n        return self.data.food\n\n    def getWalls(self):\n        """"""\n        Returns a Grid of boolean wall indicator variables.\n\n        Grids can be accessed via list notation, so to check\n        if there is food at (x,y), just call\n\n        walls = state.getWalls()\n        if walls[x][y] == True: ...\n        """"""\n        return self.data.layout.walls\n\n    def hasFood(self, x, y):\n        return self.data.food[x][y]\n\n    def hasWall(self, x, y):\n        return self.data.layout.walls[x][y]\n\n    def isLose(self):\n        return self.data._lose\n\n    def isWin(self):\n        return self.data._win\n\n    #############################################\n    #             Helper methods:               #\n    # You shouldn\'t need to call these directly #\n    #############################################\n\n    def __init__(self, prevState=None):\n        """"""\n        Generates a new state by copying information from its predecessor.\n        """"""\n        if prevState is not None:  # Initial state\n            self.data = GameStateData(prevState.data)\n        else:\n            self.data = GameStateData()\n\n    def deepCopy(self):\n        state = GameState(self)\n        state.data = self.data.deepCopy()\n        return state\n\n    def __eq__(self, other):\n        """"""\n        Allows two states to be compared.\n        """"""\n        return self.data == other.data\n\n    def __hash__(self):\n        """"""\n        Allows states to be keys of dictionaries.\n        """"""\n        return hash(self.data)\n\n    def __str__(self):\n\n        return str(self.data)\n\n    def initialize(self, layout, numGhostAgents=1000):\n        """"""\n        Creates an initial game state from a layout array (see layout.py).\n        """"""\n        self.data.initialize(layout, numGhostAgents)\n\n############################################################################\n#                     THE HIDDEN SECRETS OF PACMAN                         #\n#                                                                          #\n# You shouldn\'t need to look through the code in this section of the file. #\n############################################################################\n\n\nSCARED_TIME = 40    # Moves ghosts are scared\nCOLLISION_TOLERANCE = 0.7  # How close ghosts must be to Pacman to kill\nTIME_PENALTY = 1  # Number of points lost each round\n\n\nclass ClassicGameRules:\n    """"""\n    These game rules manage the control flow of a game, deciding when\n    and how the game starts and ends.\n    """"""\n\n    def __init__(self, timeout=30):\n        self.timeout = timeout\n\n    def newGame(self, layout, pacmanAgent, ghostAgents, display, quiet=False, catchExceptions=False):\n        agents = [pacmanAgent] + ghostAgents[:layout.getNumGhosts()]\n        initState = GameState()\n        initState.initialize(layout, len(ghostAgents))\n        game = Game(agents, display, self, catchExceptions=catchExceptions)\n        game.state = initState\n        self.initialState = initState.deepCopy()\n        self.quiet = quiet\n        return game\n\n    def process(self, state, game):\n        """"""\n        Checks to see whether it is time to end the game.\n        """"""\n        if state.isWin():\n            self.win(state, game)\n        if state.isLose():\n            self.lose(state, game)\n\n    def win(self, state, game):\n        if not self.quiet:\n            print(""Pacman emerges victorious! Score: %d"" % state.data.score)\n        game.gameOver = True\n\n    def lose(self, state, game):\n        if not self.quiet:\n            print(""Pacman died! Score: %d"" % state.data.score)\n        game.gameOver = True\n\n    def getProgress(self, game):\n        return float(game.state.getNumFood()) / self.initialState.getNumFood()\n\n    def agentCrash(self, game, agentIndex):\n        if agentIndex == 0:\n            print(""Pacman crashed"")\n        else:\n            print(""A ghost crashed"")\n\n    def getMaxTotalTime(self, agentIndex):\n        return self.timeout\n\n    def getMaxStartupTime(self, agentIndex):\n        return self.timeout\n\n    def getMoveWarningTime(self, agentIndex):\n        return self.timeout\n\n    def getMoveTimeout(self, agentIndex):\n        return self.timeout\n\n    def getMaxTimeWarnings(self, agentIndex):\n        return 0\n\n\nclass PacmanRules:\n    """"""\n    These functions govern how pacman interacts with his environment under\n    the classic game rules.\n    """"""\n    PACMAN_SPEED = 1\n\n    def getLegalActions(state):\n        """"""\n        Returns a list of possible actions.\n        """"""\n        return Actions.getPossibleActions(state.getPacmanState().configuration, state.data.layout.walls)\n    getLegalActions = staticmethod(getLegalActions)\n\n    def applyAction(state, action):\n        """"""\n        Edits the state to reflect the results of the action.\n        """"""\n        legal = PacmanRules.getLegalActions(state)\n        if action not in legal:\n            raise Exception(""Illegal action "" + str(action))\n\n        pacmanState = state.data.agentStates[0]\n\n        # Update Configuration\n        vector = Actions.directionToVector(action, PacmanRules.PACMAN_SPEED)\n        pacmanState.configuration = pacmanState.configuration.generateSuccessor(\n            vector)\n\n        # Eat\n        next = pacmanState.configuration.getPosition()\n        nearest = nearestPoint(next)\n        if manhattanDistance(nearest, next) <= 0.5:\n            # Remove food\n            PacmanRules.consume(nearest, state)\n    applyAction = staticmethod(applyAction)\n\n    def consume(position, state):\n        x, y = position\n        # Eat food\n        if state.data.food[x][y]:\n            state.data.scoreChange += 10\n            state.data.food = state.data.food.copy()\n            state.data.food[x][y] = False\n            state.data._foodEaten = position\n            # TODO: cache numFood?\n            numFood = state.getNumFood()\n            if numFood == 0 and not state.data._lose:\n                state.data.scoreChange += 500\n                state.data._win = True\n        # Eat capsule\n        if(position in state.getCapsules()):\n            state.data.capsules.remove(position)\n            state.data._capsuleEaten = position\n            # Reset all ghosts\' scared timers\n            for index in range(1, len(state.data.agentStates)):\n                state.data.agentStates[index].scaredTimer = SCARED_TIME\n    consume = staticmethod(consume)\n\n\nclass GhostRules:\n    """"""\n    These functions dictate how ghosts interact with their environment.\n    """"""\n    GHOST_SPEED = 1.0\n\n    def getLegalActions(state, ghostIndex):\n        """"""\n        Ghosts cannot stop, and cannot turn around unless they\n        reach a dead end, but can turn 90 degrees at intersections.\n        """"""\n        conf = state.getGhostState(ghostIndex).configuration\n        possibleActions = Actions.getPossibleActions(\n            conf, state.data.layout.walls)\n        reverse = Actions.reverseDirection(conf.direction)\n        if Directions.STOP in possibleActions:\n            possibleActions.remove(Directions.STOP)\n        if reverse in possibleActions and len(possibleActions) > 1:\n            possibleActions.remove(reverse)\n        return possibleActions\n    getLegalActions = staticmethod(getLegalActions)\n\n    def applyAction(state, action, ghostIndex):\n\n        legal = GhostRules.getLegalActions(state, ghostIndex)\n        if action not in legal:\n            raise Exception(""Illegal ghost action "" + str(action))\n\n        ghostState = state.data.agentStates[ghostIndex]\n        speed = GhostRules.GHOST_SPEED\n        if ghostState.scaredTimer > 0:\n            speed /= 2.0\n        vector = Actions.directionToVector(action, speed)\n        ghostState.configuration = ghostState.configuration.generateSuccessor(\n            vector)\n    applyAction = staticmethod(applyAction)\n\n    def decrementTimer(ghostState):\n        timer = ghostState.scaredTimer\n        if timer == 1:\n            ghostState.configuration.pos = nearestPoint(\n                ghostState.configuration.pos)\n        ghostState.scaredTimer = max(0, timer - 1)\n    decrementTimer = staticmethod(decrementTimer)\n\n    def checkDeath(state, agentIndex):\n        pacmanPosition = state.getPacmanPosition()\n        if agentIndex == 0:  # Pacman just moved; Anyone can kill him\n            for index in range(1, len(state.data.agentStates)):\n                ghostState = state.data.agentStates[index]\n                ghostPosition = ghostState.configuration.getPosition()\n                if GhostRules.canKill(pacmanPosition, ghostPosition):\n                    GhostRules.collide(state, ghostState, index)\n        else:\n            ghostState = state.data.agentStates[agentIndex]\n            ghostPosition = ghostState.configuration.getPosition()\n            if GhostRules.canKill(pacmanPosition, ghostPosition):\n                GhostRules.collide(state, ghostState, agentIndex)\n    checkDeath = staticmethod(checkDeath)\n\n    def collide(state, ghostState, agentIndex):\n        if ghostState.scaredTimer > 0:\n            state.data.scoreChange += 200\n            GhostRules.placeGhost(state, ghostState)\n            ghostState.scaredTimer = 0\n            # Added for first-person\n            state.data._eaten[agentIndex] = True\n        else:\n            if not state.data._win:\n                state.data.scoreChange -= 500\n                state.data._lose = True\n    collide = staticmethod(collide)\n\n    def canKill(pacmanPosition, ghostPosition):\n        return manhattanDistance(ghostPosition, pacmanPosition) <= COLLISION_TOLERANCE\n    canKill = staticmethod(canKill)\n\n    def placeGhost(state, ghostState):\n        ghostState.configuration = ghostState.start\n    placeGhost = staticmethod(placeGhost)\n\n\n#############################\n# FRAMEWORK TO START A GAME #\n#############################\n\n\ndef default(str):\n    return str + \' [Default: %default]\'\n\n\ndef parseAgentArgs(str):\n    if str is None:\n        return {}\n    pieces = str.split(\',\')\n    opts = {}\n    for p in pieces:\n        if \'=\' in p:\n            key, val = p.split(\'=\')\n        else:\n            key, val = p, 1\n        opts[key] = val\n    return opts\n\n\ndef readCommand(argv):\n    """"""\n    Processes the command used to run pacman from the command line.\n    """"""\n    from optparse import OptionParser\n    usageStr = """"""\n  USAGE:      python pacman.py <options>\n  EXAMPLES:   (1) python pacman.py\n                  - starts an interactive game\n              (2) python pacman.py --layout smallClassic --zoom 2\n              OR  python pacman.py -l smallClassic -z 2\n                  - starts an interactive game on a smaller board, zoomed in\n  """"""\n    parser = OptionParser(usageStr)\n\n    parser.add_option(\'-n\', \'--numGames\', dest=\'numGames\', type=\'int\',\n                      help=default(\'the number of GAMES to play\'), metavar=\'GAMES\', default=1)\n    parser.add_option(\'-l\', \'--layout\', dest=\'layout\',\n                      help=default(\n                          \'the LAYOUT_FILE from which to load the map layout\'),\n                      metavar=\'LAYOUT_FILE\', default=\'mediumClassic\')\n    parser.add_option(\'-p\', \'--pacman\', dest=\'pacman\',\n                      help=default(\n                          \'the agent TYPE in the pacmanAgents module to use\'),\n                      metavar=\'TYPE\', default=\'KeyboardAgent\')\n    parser.add_option(\'-t\', \'--textGraphics\', action=\'store_true\', dest=\'textGraphics\',\n                      help=\'Display output as text only\', default=False)\n    parser.add_option(\'-q\', \'--quietTextGraphics\', action=\'store_true\', dest=\'quietGraphics\',\n                      help=\'Generate minimal output and no graphics\', default=False)\n    parser.add_option(\'-g\', \'--ghosts\', dest=\'ghost\',\n                      help=default(\n                          \'the ghost agent TYPE in the ghostAgents module to use\'),\n                      metavar=\'TYPE\', default=\'RandomGhost\')\n    parser.add_option(\'-k\', \'--numghosts\', type=\'int\', dest=\'numGhosts\',\n                      help=default(\'The maximum number of ghosts to use\'), default=4)\n    parser.add_option(\'-z\', \'--zoom\', type=\'float\', dest=\'zoom\',\n                      help=default(\'Zoom the size of the graphics window\'), default=1.0)\n    parser.add_option(\'-f\', \'--fixRandomSeed\', action=\'store_true\', dest=\'fixRandomSeed\',\n                      help=\'Fixes the random seed to always play the same game\', default=False)\n    parser.add_option(\'-r\', \'--recordActions\', action=\'store_true\', dest=\'record\',\n                      help=\'Writes game histories to a file (named by the time they were played)\', default=False)\n    parser.add_option(\'--replay\', dest=\'gameToReplay\',\n                      help=\'A recorded game file (pickle) to replay\', default=None)\n    parser.add_option(\'-a\', \'--agentArgs\', dest=\'agentArgs\',\n                      help=\'Comma separated values sent to agent. e.g. ""opt1=val1,opt2,opt3=val3""\')\n    parser.add_option(\'-x\', \'--numTraining\', dest=\'numTraining\', type=\'int\',\n                      help=default(\'How many episodes are training (suppresses output)\'), default=0)\n    parser.add_option(\'--frameTime\', dest=\'frameTime\', type=\'float\',\n                      help=default(\'Time to delay between frames; <0 means keyboard\'), default=0.1)\n    parser.add_option(\'-c\', \'--catchExceptions\', action=\'store_true\', dest=\'catchExceptions\',\n                      help=\'Turns on exception handling and timeouts during games\', default=False)\n    parser.add_option(\'--timeout\', dest=\'timeout\', type=\'int\',\n                      help=default(\'Maximum length of time an agent can spend computing in a single game\'), default=30)\n\n    options, otherjunk = parser.parse_args(argv)\n    if len(otherjunk) != 0:\n        raise Exception(\'Command line input not understood: \' + str(otherjunk))\n    args = dict()\n\n    # Fix the random seed\n    if options.fixRandomSeed:\n        random.seed(\'cs188\')\n\n    # Choose a layout\n    args[\'layout\'] = layout.getLayout(options.layout)\n    if args[\'layout\'] is None:\n        raise Exception(""The layout "" + options.layout + "" cannot be found"")\n\n    # Choose a Pacman agent\n    noKeyboard = options.gameToReplay is None and (\n        options.textGraphics or options.quietGraphics)\n    pacmanType = loadAgent(options.pacman, noKeyboard)\n    agentOpts = parseAgentArgs(options.agentArgs)\n    if options.numTraining > 0:\n        args[\'numTraining\'] = options.numTraining\n        if \'numTraining\' not in agentOpts:\n            agentOpts[\'numTraining\'] = options.numTraining\n    pacman = pacmanType(**agentOpts)  # Instantiate Pacman with agentArgs\n    args[\'pacman\'] = pacman\n\n    # Don\'t display training games\n    if \'numTrain\' in agentOpts:\n        options.numQuiet = int(agentOpts[\'numTrain\'])\n        options.numIgnore = int(agentOpts[\'numTrain\'])\n\n    # Choose a ghost agent\n    ghostType = loadAgent(options.ghost, noKeyboard)\n    args[\'ghosts\'] = [ghostType(i+1) for i in range(options.numGhosts)]\n\n    # Choose a display format\n    if options.quietGraphics:\n        import textDisplay\n        args[\'display\'] = textDisplay.NullGraphics()\n    elif options.textGraphics:\n        import textDisplay\n        textDisplay.SLEEP_TIME = options.frameTime\n        args[\'display\'] = textDisplay.PacmanGraphics()\n    else:\n        import graphicsDisplay\n        args[\'display\'] = graphicsDisplay.PacmanGraphics(\n            options.zoom, frameTime=options.frameTime)\n    args[\'numGames\'] = options.numGames\n    args[\'record\'] = options.record\n    args[\'catchExceptions\'] = options.catchExceptions\n    args[\'timeout\'] = options.timeout\n\n    # Special case: recorded games don\'t use the runGames method or args structure\n    if options.gameToReplay is not None:\n        print(\'Replaying recorded game %s.\' % options.gameToReplay)\n        import pickle\n        f = open(options.gameToReplay)\n        try:\n            recorded = pickle.load(f)\n        finally:\n            f.close()\n        recorded[\'display\'] = args[\'display\']\n        replayGame(**recorded)\n        sys.exit(0)\n\n    return args\n\n\ndef loadAgent(pacman, nographics):\n    # Looks through all pythonPath Directories for the right module,\n    pythonPathStr = os.path.expandvars(""$PYTHONPATH"")\n    if pythonPathStr.find(\';\') == -1:\n        pythonPathDirs = pythonPathStr.split(\':\')\n    else:\n        pythonPathDirs = pythonPathStr.split(\';\')\n    pythonPathDirs.append(\'.\')\n\n    for moduleDir in pythonPathDirs:\n        if not os.path.isdir(moduleDir):\n            continue\n        moduleNames = [f for f in os.listdir(\n            moduleDir) if f.endswith(\'gents.py\')]\n        for modulename in moduleNames:\n            try:\n                module = __import__(modulename[:-3])\n            except ImportError:\n                continue\n            if pacman in dir(module):\n                if nographics and modulename == \'keyboardAgents.py\':\n                    raise Exception(\n                        \'Using the keyboard requires graphics (not text display)\')\n                return getattr(module, pacman)\n    raise Exception(\'The agent \' + pacman +\n                    \' is not specified in any *Agents.py.\')\n\n\ndef replayGame(layout, actions, display):\n    import pacmanAgents\n    import ghostAgents\n    rules = ClassicGameRules()\n    agents = [pacmanAgents.GreedyAgent()] + [ghostAgents.RandomGhost(i+1)\n                                             for i in range(layout.getNumGhosts())]\n    game = rules.newGame(layout, agents[0], agents[1:], display)\n    state = game.state\n    display.initialize(state.data)\n\n    for action in actions:\n            # Execute the action\n        state = state.generateSuccessor(*action)\n        # Change the display\n        display.update(state.data)\n        # Allow for game specific conditions (winning, losing, etc.)\n        rules.process(state, game)\n\n    display.finish()\n\n\ndef runGames(layout, pacman, ghosts, display, numGames, record, numTraining=0, catchExceptions=False, timeout=30):\n    import __main__\n    __main__.__dict__[\'_display\'] = display\n\n    rules = ClassicGameRules(timeout)\n    games = []\n\n    for i in range(numGames):\n        beQuiet = i < numTraining\n        if beQuiet:\n                # Suppress output and graphics\n            import textDisplay\n            gameDisplay = textDisplay.NullGraphics()\n            rules.quiet = True\n        else:\n            gameDisplay = display\n            rules.quiet = False\n        game = rules.newGame(layout, pacman, ghosts,\n                             gameDisplay, beQuiet, catchExceptions)\n        game.run()\n        if not beQuiet:\n            games.append(game)\n\n        if record:\n            import time\n            import pickle\n            fname = (\'recorded-game-%d\' % (i + 1)) + \\\n                \'-\'.join([str(t) for t in time.localtime()[1:6]])\n            f = file(fname, \'w\')\n            components = {\'layout\': layout, \'actions\': game.moveHistory}\n            pickle.dump(components, f)\n            f.close()\n\n    if (numGames-numTraining) > 0:\n        scores = [game.state.getScore() for game in games]\n        wins = [game.state.isWin() for game in games]\n        winRate = wins.count(True) / float(len(wins))\n        print(\'Average Score:\', sum(scores) / float(len(scores)))\n        print(\'Scores:       \', \', \'.join([str(score) for score in scores]))\n        print(\'Win Rate:      %d/%d (%.2f)\' %\n              (wins.count(True), len(wins), winRate))\n        print(\'Record:       \', \', \'.join(\n            [[\'Loss\', \'Win\'][int(w)] for w in wins]))\n\n    return games\n\n\nif __name__ == \'__main__\':\n    """"""\n    The main function called when pacman.py is run\n    from the command line:\n\n    > python pacman.py\n\n    See the usage string for more details.\n\n    > python pacman.py --help\n    """"""\n    args = readCommand(sys.argv[1:])  # Get game components based on input\n    runGames(**args)\n\n    # import cProfile\n    # cProfile.run(""runGames( **args )"")\n    pass\n'"
week03_model_free/crawler_and_pacman/seminar_py3/pacmanAgents.py,0,"b'# pacmanAgents.py\n# ---------------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nfrom pacman import Directions\nfrom game import Agent\nimport random\nimport game\nimport util\n\n\nclass LeftTurnAgent(game.Agent):\n    ""An agent that turns left at every opportunity""\n\n    def getAction(self, state):\n        legal = state.getLegalPacmanActions()\n        current = state.getPacmanState().configuration.direction\n        if current == Directions.STOP:\n            current = Directions.NORTH\n        left = Directions.LEFT[current]\n        if left in legal:\n            return left\n        if current in legal:\n            return current\n        if Directions.RIGHT[current] in legal:\n            return Directions.RIGHT[current]\n        if Directions.LEFT[left] in legal:\n            return Directions.LEFT[left]\n        return Directions.STOP\n\n\nclass GreedyAgent(Agent):\n    def __init__(self, evalFn=""scoreEvaluation""):\n        self.evaluationFunction = util.lookup(evalFn, globals())\n        assert self.evaluationFunction is not None\n\n    def getAction(self, state):\n        # Generate candidate actions\n        legal = state.getLegalPacmanActions()\n        if Directions.STOP in legal:\n            legal.remove(Directions.STOP)\n\n        successors = [(state.generateSuccessor(0, action), action)\n                      for action in legal]\n        scored = [(self.evaluationFunction(state), action)\n                  for state, action in successors]\n        bestScore = max(scored)[0]\n        bestActions = [pair[1] for pair in scored if pair[0] == bestScore]\n        return random.choice(bestActions)\n\n\ndef scoreEvaluation(state):\n    return state.getScore()\n'"
week03_model_free/crawler_and_pacman/seminar_py3/qlearningAgents.py,0,"b'# qlearningAgents.py\n# ------------------\n# based on http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nfrom game import *\nfrom learningAgents import ReinforcementAgent\nfrom featureExtractors import *\n\nimport random\nimport util\nimport math\nfrom collections import defaultdict\n\n\nclass QLearningAgent(ReinforcementAgent):\n    """"""\n      Q-Learning Agent\n\n      Instance variables you have access to\n        - self.epsilon (exploration prob)\n        - self.alpha (learning rate)\n        - self.discount (discount rate aka gamma)\n\n      Functions you should use\n        - self.getLegalActions(state)\n          which returns legal actions for a state\n        - self.getQValue(state,action)\n          which returns Q(state,action)\n        - self.setQValue(state,action,value)\n          which sets Q(state,action) := value\n\n      !!!Important!!!\n      NOTE: please avoid using self._qValues directly to make code cleaner\n    """"""\n\n    def __init__(self, **args):\n        ""We initialize agent and Q-values here.""\n        ReinforcementAgent.__init__(self, **args)\n        self._qValues = defaultdict(lambda: defaultdict(lambda: 0))\n\n    def getQValue(self, state, action):\n        """"""\n          Returns Q(state,action)\n        """"""\n        return self._qValues[state][action]\n\n    def setQValue(self, state, action, value):\n        """"""\n          Sets the Qvalue for [state,action] to the given value\n        """"""\n        self._qValues[state][action] = value\n\n#---------------------#start of your code#---------------------#\n\n    def getValue(self, state):\n        """"""\n          Returns max_action Q(state,action)\n          where the max is over legal actions.\n        """"""\n\n        possibleActions = self.getLegalActions(state)\n        # If there are no legal actions, return 0.0\n        if len(possibleActions) == 0:\n            return 0.0\n\n        ""*** YOUR CODE HERE ***""\n        raise NotImplementedError\n\n        return 0.\n\n    def getPolicy(self, state):\n        """"""\n          Compute the best action to take in a state. \n\n        """"""\n        possibleActions = self.getLegalActions(state)\n\n        # If there are no legal actions, return None\n        if len(possibleActions) == 0:\n            return None\n\n        best_action = None\n\n        ""*** YOUR CODE HERE ***""\n        raise NotImplementedError\n\n        return best_action\n\n    def getAction(self, state):\n        """"""\n          Compute the action to take in the current state, including exploration.  \n\n          With probability self.epsilon, we should take a random action.\n          otherwise - the best policy action (self.getPolicy).\n\n          HINT: You might want to use util.flipCoin(prob)\n          HINT: To pick randomly from a list, use random.choice(list)\n\n        """"""\n\n        # Pick Action\n        possibleActions = self.getLegalActions(state)\n        action = None\n\n        # If there are no legal actions, return None\n        if len(possibleActions) == 0:\n            return None\n\n        # agent parameters:\n        epsilon = self.epsilon\n\n        ""*** YOUR CODE HERE ***""\n        raise NotImplementedError\n\n        return action\n\n    def update(self, state, action, nextState, reward):\n        """"""\n          You should do your Q-Value update here\n\n          NOTE: You should never call this function,\n          it will be called on your behalf\n\n\n        """"""\n        # agent parameters\n        gamma = self.discount\n        learning_rate = self.alpha\n\n        ""*** YOUR CODE HERE ***""\n        raise NotImplementedError\n\n        reference_qvalue = PleaseImplementMe\n        updated_qvalue = PleaseImplementMe\n\n        self.setQValue(PleaseImplementMe, PleaseImplementMe, updated_qvalue)\n\n\n#---------------------#end of your code#---------------------#\n\n\nclass PacmanQAgent(QLearningAgent):\n    ""Exactly the same as QLearningAgent, but with different default parameters""\n\n    def __init__(self, epsilon=0.05, gamma=0.8, alpha=0.2, numTraining=0, **args):\n        """"""\n        These default parameters can be changed from the pacman.py command line.\n        For example, to change the exploration rate, try:\n            python pacman.py -p PacmanQLearningAgent -a epsilon=0.1\n\n        alpha    - learning rate\n        epsilon  - exploration rate\n        gamma    - discount factor\n        numTraining - number of training episodes, i.e. no learning after these many episodes\n        """"""\n        args[\'epsilon\'] = epsilon\n        args[\'gamma\'] = gamma\n        args[\'alpha\'] = alpha\n        args[\'numTraining\'] = numTraining\n        self.index = 0  # This is always Pacman\n        QLearningAgent.__init__(self, **args)\n\n    def getAction(self, state):\n        """"""\n        Simply calls the getAction method of QLearningAgent and then\n        informs parent of action for Pacman.  Do not change or remove this\n        method.\n        """"""\n        action = QLearningAgent.getAction(self, state)\n        self.doAction(state, action)\n        return action\n\n\nclass ApproximateQAgent(PacmanQAgent):\n    pass\n'"
week03_model_free/crawler_and_pacman/seminar_py3/textDisplay.py,0,"b'# textDisplay.py\n# --------------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nimport pacman\nimport time\n\nDRAW_EVERY = 1\nSLEEP_TIME = 0  # This can be overwritten by __init__\nDISPLAY_MOVES = False\nQUIET = False  # Supresses output\n\n\nclass NullGraphics:\n    def initialize(self, state, isBlue=False):\n        pass\n\n    def update(self, state):\n        pass\n\n    def pause(self):\n        time.sleep(SLEEP_TIME)\n\n    def draw(self, state):\n        print(state)\n\n    def finish(self):\n        pass\n\n\nclass PacmanGraphics:\n    def __init__(self, speed=None):\n        if speed is not None:\n            global SLEEP_TIME\n            SLEEP_TIME = speed\n\n    def initialize(self, state, isBlue=False):\n        self.draw(state)\n        self.pause()\n        self.turn = 0\n        self.agentCounter = 0\n\n    def update(self, state):\n        numAgents = len(state.agentStates)\n        self.agentCounter = (self.agentCounter + 1) % numAgents\n        if self.agentCounter == 0:\n            self.turn += 1\n            if DISPLAY_MOVES:\n                ghosts = [pacman.nearestPoint(\n                    state.getGhostPosition(i)) for i in range(1, numAgents)]\n                print(""%4d) P: %-8s"" % (self.turn, str(pacman.nearestPoint(state.getPacmanPosition()))),\n                      \'| Score: %-5d\' % state.score, \'| Ghosts:\', ghosts)\n            if self.turn % DRAW_EVERY == 0:\n                self.draw(state)\n                self.pause()\n        if state._win or state._lose:\n            self.draw(state)\n\n    def pause(self):\n        time.sleep(SLEEP_TIME)\n\n    def draw(self, state):\n        print(state)\n\n    def finish(self):\n        pass\n'"
week03_model_free/crawler_and_pacman/seminar_py3/textGridworldDisplay.py,0,"b'# textGridworldDisplay.py\n# -----------------------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nimport operator\nimport io\nimport math\nimport util\nfrom functools import reduce\n\n\nclass TextGridworldDisplay:\n\n    def __init__(self, gridworld):\n        self.gridworld = gridworld\n\n    def start(self):\n        pass\n\n    def pause(self):\n        pass\n\n    def displayValues(self, agent, currentState=None, message=None):\n        if message is not None:\n            print(message)\n        values = util.Counter()\n        policy = {}\n        states = self.gridworld.getStates()\n        for state in states:\n            values[state] = agent.getValue(state)\n            policy[state] = agent.getPolicy(state)\n        prettyPrintValues(self.gridworld, values, policy, currentState)\n\n    def displayNullValues(self, agent, currentState=None, message=None):\n        if message is not None:\n            print(message)\n        prettyPrintNullValues(self.gridworld, currentState)\n\n    def displayQValues(self, agent, currentState=None, message=None):\n        if message is not None:\n            print(message)\n        qValues = util.Counter()\n        states = self.gridworld.getStates()\n        for state in states:\n            for action in self.gridworld.getPossibleActions(state):\n                qValues[(state, action)] = agent.getQValue(state, action)\n        prettyPrintQValues(self.gridworld, qValues, currentState)\n\n\ndef prettyPrintValues(gridWorld, values, policy=None, currentState=None):\n    grid = gridWorld.grid\n    maxLen = 11\n    newRows = []\n    for y in range(grid.height):\n        newRow = []\n        for x in range(grid.width):\n            state = (x, y)\n            value = values[state]\n            action = None\n            if policy is not None and state in policy:\n                action = policy[state]\n            actions = gridWorld.getPossibleActions(state)\n            if action not in actions and \'exit\' in actions:\n                action = \'exit\'\n            valString = None\n            if action == \'exit\':\n                valString = border(\'%.2f\' % value)\n            else:\n                valString = \'\\n\\n%.2f\\n\\n\' % value\n                valString += \' \'*maxLen\n            if grid[x][y] == \'S\':\n                valString = \'\\n\\nS: %.2f\\n\\n\' % value\n                valString += \' \'*maxLen\n            if grid[x][y] == \'#\':\n                valString = \'\\n#####\\n#####\\n#####\\n\'\n                valString += \' \'*maxLen\n            pieces = [valString]\n            text = (""\\n"".join(pieces)).split(\'\\n\')\n            if currentState == state:\n                l = len(text[1])\n                if l == 0:\n                    text[1] = \'*\'\n                else:\n                    text[1] = ""|"" + \' \' * \\\n                        int((l-1)/2-1) + \'*\' + \' \' * int((l)/2-1) + ""|""\n            if action == \'east\':\n                text[2] = \'  \' + text[2] + \' >\'\n            elif action == \'west\':\n                text[2] = \'< \' + text[2] + \'  \'\n            elif action == \'north\':\n                text[0] = \' \' * int(maxLen/2) + \'^\' + \' \' * int(maxLen/2)\n            elif action == \'south\':\n                text[4] = \' \' * int(maxLen/2) + \'v\' + \' \' * int(maxLen/2)\n            newCell = ""\\n"".join(text)\n            newRow.append(newCell)\n        newRows.append(newRow)\n    numCols = grid.width\n    for rowNum, row in enumerate(newRows):\n        row.insert(0, ""\\n\\n""+str(rowNum))\n    newRows.reverse()\n    colLabels = [str(colNum) for colNum in range(numCols)]\n    colLabels.insert(0, \' \')\n    finalRows = [colLabels] + newRows\n    print(indent(finalRows, separateRows=True, delim=\'|\',\n                 prefix=\'|\', postfix=\'|\', justify=\'center\', hasHeader=True))\n\n\ndef prettyPrintNullValues(gridWorld, currentState=None):\n    grid = gridWorld.grid\n    maxLen = 11\n    newRows = []\n    for y in range(grid.height):\n        newRow = []\n        for x in range(grid.width):\n            state = (x, y)\n\n            # value = values[state]\n\n            action = None\n            # if policy is not None and state in policy:\n            #   action = policy[state]\n            #\n            actions = gridWorld.getPossibleActions(state)\n\n            if action not in actions and \'exit\' in actions:\n                action = \'exit\'\n\n            valString = None\n            # if action == \'exit\':\n            #   valString = border(\'%.2f\' % value)\n            # else:\n            #   valString = \'\\n\\n%.2f\\n\\n\' % value\n            #   valString += \' \'*maxLen\n\n            if grid[x][y] == \'S\':\n                valString = \'\\n\\nS\\n\\n\'\n                valString += \' \'*maxLen\n            elif grid[x][y] == \'#\':\n                valString = \'\\n#####\\n#####\\n#####\\n\'\n                valString += \' \'*maxLen\n            elif type(grid[x][y]) == float or type(grid[x][y]) == int:\n                valString = border(\'%.2f\' % float(grid[x][y]))\n            else:\n                valString = border(\'  \')\n            pieces = [valString]\n\n            text = (""\\n"".join(pieces)).split(\'\\n\')\n\n            if currentState == state:\n                l = len(text[1])\n                if l == 0:\n                    text[1] = \'*\'\n                else:\n                    text[1] = ""|"" + \' \' * \\\n                        int((l-1)/2-1) + \'*\' + \' \' * int((l)/2-1) + ""|""\n\n            if action == \'east\':\n                text[2] = \'  \' + text[2] + \' >\'\n            elif action == \'west\':\n                text[2] = \'< \' + text[2] + \'  \'\n            elif action == \'north\':\n                text[0] = \' \' * int(maxLen/2) + \'^\' + \' \' * int(maxLen/2)\n            elif action == \'south\':\n                text[4] = \' \' * int(maxLen/2) + \'v\' + \' \' * int(maxLen/2)\n            newCell = ""\\n"".join(text)\n            newRow.append(newCell)\n        newRows.append(newRow)\n    numCols = grid.width\n    for rowNum, row in enumerate(newRows):\n        row.insert(0, ""\\n\\n""+str(rowNum))\n    newRows.reverse()\n    colLabels = [str(colNum) for colNum in range(numCols)]\n    colLabels.insert(0, \' \')\n    finalRows = [colLabels] + newRows\n    print(indent(finalRows, separateRows=True, delim=\'|\',\n                 prefix=\'|\', postfix=\'|\', justify=\'center\', hasHeader=True))\n\n\ndef prettyPrintQValues(gridWorld, qValues, currentState=None):\n    grid = gridWorld.grid\n    maxLen = 11\n    newRows = []\n    for y in range(grid.height):\n        newRow = []\n        for x in range(grid.width):\n            state = (x, y)\n            actions = gridWorld.getPossibleActions(state)\n            if actions is None or len(actions) == 0:\n                actions = [None]\n            bestQ = max([qValues[(state, action)] for action in actions])\n            bestActions = [\n                action for action in actions if qValues[(state, action)] == bestQ]\n\n            # display cell\n            qStrings = dict([(action, ""%.2f"" % qValues[(state, action)])\n                             for action in actions])\n            northString = (\'north\' in qStrings and qStrings[\'north\']) or \' \'\n            southString = (\'south\' in qStrings and qStrings[\'south\']) or \' \'\n            eastString = (\'east\' in qStrings and qStrings[\'east\']) or \' \'\n            westString = (\'west\' in qStrings and qStrings[\'west\']) or \' \'\n            exitString = (\'exit\' in qStrings and qStrings[\'exit\']) or \' \'\n\n            eastLen = len(eastString)\n            westLen = len(westString)\n            if eastLen < westLen:\n                eastString = \' \'*(westLen-eastLen)+eastString\n            if westLen < eastLen:\n                westString = westString+\' \'*(eastLen-westLen)\n\n            if \'north\' in bestActions:\n                northString = \'/\'+northString+\'\\\\\'\n            if \'south\' in bestActions:\n                southString = \'\\\\\'+southString+\'/\'\n            if \'east\' in bestActions:\n                eastString = \'\'+eastString+\'>\'\n            else:\n                eastString = \'\'+eastString+\' \'\n            if \'west\' in bestActions:\n                westString = \'<\'+westString+\'\'\n            else:\n                westString = \' \'+westString+\'\'\n            if \'exit\' in bestActions:\n                exitString = \'[ \'+exitString+\' ]\'\n\n            ewString = westString + ""     "" + eastString\n            if state == currentState:\n                ewString = westString + ""  *  "" + eastString\n            if state == gridWorld.getStartState():\n                ewString = westString + ""  S  "" + eastString\n            if state == currentState and state == gridWorld.getStartState():\n                ewString = westString + "" S:* "" + eastString\n\n            text = [northString, ""\\n""+exitString,\n                    ewString, \' \'*maxLen+""\\n"", southString]\n\n            if grid[x][y] == \'#\':\n                text = [\'\', \'\\n#####\\n#####\\n#####\', \'\']\n\n            newCell = ""\\n"".join(text)\n            newRow.append(newCell)\n        newRows.append(newRow)\n    numCols = grid.width\n    for rowNum, row in enumerate(newRows):\n        row.insert(0, ""\\n\\n\\n""+str(rowNum))\n    newRows.reverse()\n    colLabels = [str(colNum) for colNum in range(numCols)]\n    colLabels.insert(0, \' \')\n    finalRows = [colLabels] + newRows\n\n    print(indent(finalRows, separateRows=True, delim=\'|\',\n                 prefix=\'|\', postfix=\'|\', justify=\'center\', hasHeader=True))\n\n\ndef border(text):\n    length = len(text)\n    pieces = [\'-\' * (length+2), \'|\'+\' \' * (length+2)+\'|\', \' | \' +\n              text+\' | \', \'|\'+\' \' * (length+2)+\'|\', \'-\' * (length+2)]\n    return \'\\n\'.join(pieces)\n\n# INDENTING CODE\n\n# Indenting code based on a post from George Sakkis\n# (http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/267662)\n\n\ndef indent(rows, hasHeader=False, headerChar=\'-\', delim=\' | \', justify=\'left\',\n           separateRows=False, prefix=\'\', postfix=\'\', wrapfunc=lambda x: x):\n    """"""Indents a table by column.\n       - rows: A sequence of sequences of items, one sequence per row.\n       - hasHeader: True if the first row consists of the columns\' names.\n       - headerChar: Character to be used for the row separator line\n         (if hasHeader==True or separateRows==True).\n       - delim: The column delimiter.\n       - justify: Determines how are data justified in their column. \n         Valid values are \'left\',\'right\' and \'center\'.\n       - separateRows: True if rows are to be separated by a line\n         of \'headerChar\'s.\n       - prefix: A string prepended to each printed row.\n       - postfix: A string appended to each printed row.\n       - wrapfunc: A function f(text) for wrapping text; each element in\n         the table is first wrapped by this function.""""""\n    # closure for breaking logical rows to physical, using wrapfunc\n    def rowWrapper(row):\n        newRows = [wrapfunc(item).split(\'\\n\') for item in row]\n        return [[substr or \'\' for substr in item] for item in list(*newRows)]\n    # break each logical row into one or more physical ones\n    logicalRows = [rowWrapper(row) for row in rows]\n    # columns of physical rows\n    columns = list(*reduce(operator.add, logicalRows))\n    # get the maximum of each column by the string length of its items\n    maxWidths = [max([len(str(item)) for item in column])\n                 for column in columns]\n    rowSeparator = headerChar * (len(prefix) + len(postfix) + sum(maxWidths) +\n                                 len(delim)*(len(maxWidths)-1))\n    # select the appropriate justify method\n    justify = {\'center\': str.center, \'right\': str.rjust,\n               \'left\': str.ljust}[justify.lower()]\n    output = io.StringIO()\n    if separateRows:\n        print(rowSeparator, file=output)\n    for physicalRows in logicalRows:\n        for row in physicalRows:\n            print(prefix\n                  + delim.join([justify(str(item), width)\n                                for (item, width) in zip(row, maxWidths)])\n                  + postfix, file=output)\n        if separateRows or hasHeader:\n            print(rowSeparator, file=output)\n            hasHeader = False\n    return output.getvalue()\n\n\ndef wrap_always(text, width):\n    """"""A simple word-wrap function that wraps text on exactly width characters.\n       It doesn\'t split the text in words.""""""\n    return \'\\n\'.join([text[width*i:width*(i+1)]\n                      for i in range(int(math.ceil(1.*len(text)/width)))])\n\n\n# TEST OF DISPLAY CODE\n\nif __name__ == \'__main__\':\n    import gridworld\n    import util\n\n    grid = gridworld.getCliffGrid3()\n    print(grid.getStates())\n\n    policy = dict([(state, \'east\') for state in grid.getStates()])\n    values = util.Counter(dict([(state, 1000.23)\n                                for state in grid.getStates()]))\n    prettyPrintValues(grid, values, policy, currentState=(0, 0))\n\n    stateCrossActions = [[(state, action) for action in grid.getPossibleActions(\n        state)] for state in grid.getStates()]\n    qStates = reduce(lambda x, y: x+y, stateCrossActions, [])\n    qValues = util.Counter(dict([((state, action), 10.5)\n                                 for state, action in qStates]))\n    qValues = util.Counter(dict([((state, action), 10.5) for state, action in reduce(\n        lambda x, y: x+y, stateCrossActions, [])]))\n    prettyPrintQValues(grid, qValues, currentState=(0, 0))\n'"
week03_model_free/crawler_and_pacman/seminar_py3/util.py,0,"b'# util.py\n# -------\n# Licensing Information: Please do not distribute or publish solutions to this\n# project. You are free to use and extend these projects for educational\n# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n\nimport signal\nimport sys\nimport inspect\nimport heapq\nimport random\n\n\n""""""\n Data structures useful for implementing SearchAgents\n""""""\n\n\nclass Stack:\n    ""A container with a last-in-first-out (LIFO) queuing policy.""\n\n    def __init__(self):\n        self.list = []\n\n    def push(self, item):\n        ""Push \'item\' onto the stack""\n        self.list.append(item)\n\n    def pop(self):\n        ""Pop the most recently pushed item from the stack""\n        return self.list.pop()\n\n    def isEmpty(self):\n        ""Returns true if the stack is empty""\n        return len(self.list) == 0\n\n\nclass Queue:\n    ""A container with a first-in-first-out (FIFO) queuing policy.""\n\n    def __init__(self):\n        self.list = []\n\n    def push(self, item):\n        ""Enqueue the \'item\' into the queue""\n        self.list.insert(0, item)\n\n    def pop(self):\n        """"""\n          Dequeue the earliest enqueued item still in the queue. This\n          operation removes the item from the queue.\n        """"""\n        return self.list.pop()\n\n    def isEmpty(self):\n        ""Returns true if the queue is empty""\n        return len(self.list) == 0\n\n\nclass PriorityQueue:\n    """"""\n      Implements a priority queue data structure. Each inserted item\n      has a priority associated with it and the client is usually interested\n      in quick retrieval of the lowest-priority item in the queue. This\n      data structure allows O(1) access to the lowest-priority item.\n\n      Note that this PriorityQueue does not allow you to change the priority\n      of an item.  However, you may insert the same item multiple times with\n      different priorities.\n    """"""\n\n    def __init__(self):\n        self.heap = []\n\n    def push(self, item, priority):\n        pair = (priority, item)\n        heapq.heappush(self.heap, pair)\n\n    def pop(self):\n        (priority, item) = heapq.heappop(self.heap)\n        return item\n\n    def isEmpty(self):\n        return len(self.heap) == 0\n\n\nclass PriorityQueueWithFunction(PriorityQueue):\n    """"""\n    Implements a priority queue with the same push/pop signature of the\n    Queue and the Stack classes. This is designed for drop-in replacement for\n    those two classes. The caller has to provide a priority function, which\n    extracts each item\'s priority.\n    """"""\n\n    def __init__(self, priorityFunction):\n        ""priorityFunction (item) -> priority""\n        self.priorityFunction = priorityFunction      # store the priority function\n        PriorityQueue.__init__(self)        # super-class initializer\n\n    def push(self, item):\n        ""Adds an item to the queue with priority from the priority function""\n        PriorityQueue.push(self, item, self.priorityFunction(item))\n\n\ndef manhattanDistance(xy1, xy2):\n    ""Returns the Manhattan distance between points xy1 and xy2""\n    return abs(xy1[0] - xy2[0]) + abs(xy1[1] - xy2[1])\n\n\n""""""\n  Data structures and functions useful for various course projects\n  \n  The search project should not need anything below this line.\n""""""\n\n\nclass Counter(dict):\n    """"""\n    A counter keeps track of counts for a set of keys.\n\n    The counter class is an extension of the standard python\n    dictionary type.  It is specialized to have number values  \n    (integers or floats), and includes a handful of additional\n    functions to ease the task of counting data.  In particular, \n    all keys are defaulted to have value 0.  Using a dictionary:\n\n    a = {}\n    print a[\'test\']\n\n    would give an error, while the Counter class analogue:\n\n    >>> a = Counter()\n    >>> print a[\'test\']\n    0\n\n    returns the default 0 value. Note that to reference a key \n    that you know is contained in the counter, \n    you can still use the dictionary syntax:\n\n    >>> a = Counter()\n    >>> a[\'test\'] = 2\n    >>> print a[\'test\']\n    2\n\n    This is very useful for counting things without initializing their counts,\n    see for example:\n\n    >>> a[\'blah\'] += 1\n    >>> print a[\'blah\']\n    1\n\n    The counter also includes additional functionality useful in implementing\n    the classifiers for this assignment.  Two counters can be added,\n    subtracted or multiplied together.  See below for details.  They can\n    also be normalized and their total count and arg max can be extracted.\n    """"""\n\n    def __getitem__(self, idx):\n        self.setdefault(idx, 0)\n        return dict.__getitem__(self, idx)\n\n    def incrementAll(self, keys, count):\n        """"""\n        Increments all elements of keys by the same count.\n\n        >>> a = Counter()\n        >>> a.incrementAll([\'one\',\'two\', \'three\'], 1)\n        >>> a[\'one\']\n        1\n        >>> a[\'two\']\n        1\n        """"""\n        for key in keys:\n            self[key] += count\n\n    def argMax(self):\n        """"""\n        Returns the key with the highest value.\n        """"""\n        if len(list(self.keys())) == 0:\n            return None\n        all = list(self.items())\n        values = [x[1] for x in all]\n        maxIndex = values.index(max(values))\n        return all[maxIndex][0]\n\n    def sortedKeys(self):\n        """"""\n        Returns a list of keys sorted by their values.  Keys\n        with the highest values will appear first.\n\n        >>> a = Counter()\n        >>> a[\'first\'] = -2\n        >>> a[\'second\'] = 4\n        >>> a[\'third\'] = 1\n        >>> a.sortedKeys()\n        [\'second\', \'third\', \'first\']\n        """"""\n        sortedItems = list(self.items())\n        def compare(x, y): return sign(y[1] - x[1])\n        sortedItems.sort(cmp=compare)\n        return [x[0] for x in sortedItems]\n\n    def totalCount(self):\n        """"""\n        Returns the sum of counts for all keys.\n        """"""\n        return sum(self.values())\n\n    def normalize(self):\n        """"""\n        Edits the counter such that the total count of all\n        keys sums to 1.  The ratio of counts for all keys\n        will remain the same. Note that normalizing an empty \n        Counter will result in an error.\n        """"""\n        total = float(self.totalCount())\n        if total == 0:\n            return\n        for key in list(self.keys()):\n            self[key] = self[key] / total\n\n    def divideAll(self, divisor):\n        """"""\n        Divides all counts by divisor\n        """"""\n        divisor = float(divisor)\n        for key in self:\n            self[key] /= divisor\n\n    def copy(self):\n        """"""\n        Returns a copy of the counter\n        """"""\n        return Counter(dict.copy(self))\n\n    def __mul__(self, y):\n        """"""\n        Multiplying two counters gives the dot product of their vectors where\n        each unique label is a vector element.\n\n        >>> a = Counter()\n        >>> b = Counter()\n        >>> a[\'first\'] = -2\n        >>> a[\'second\'] = 4\n        >>> b[\'first\'] = 3\n        >>> b[\'second\'] = 5\n        >>> a[\'third\'] = 1.5\n        >>> a[\'fourth\'] = 2.5\n        >>> a * b\n        14\n        """"""\n        sum = 0\n        x = self\n        if len(x) > len(y):\n            x, y = y, x\n        for key in x:\n            if key not in y:\n                continue\n            sum += x[key] * y[key]\n        return sum\n\n    def __radd__(self, y):\n        """"""\n        Adding another counter to a counter increments the current counter\n        by the values stored in the second counter.\n\n        >>> a = Counter()\n        >>> b = Counter()\n        >>> a[\'first\'] = -2\n        >>> a[\'second\'] = 4\n        >>> b[\'first\'] = 3\n        >>> b[\'third\'] = 1\n        >>> a += b\n        >>> a[\'first\']\n        1\n        """"""\n        for key, value in list(y.items()):\n            self[key] += value\n\n    def __add__(self, y):\n        """"""\n        Adding two counters gives a counter with the union of all keys and\n        counts of the second added to counts of the first.\n\n        >>> a = Counter()\n        >>> b = Counter()\n        >>> a[\'first\'] = -2\n        >>> a[\'second\'] = 4\n        >>> b[\'first\'] = 3\n        >>> b[\'third\'] = 1\n        >>> (a + b)[\'first\']\n        1\n        """"""\n        addend = Counter()\n        for key in self:\n            if key in y:\n                addend[key] = self[key] + y[key]\n            else:\n                addend[key] = self[key]\n        for key in y:\n            if key in self:\n                continue\n            addend[key] = y[key]\n        return addend\n\n    def __sub__(self, y):\n        """"""\n        Subtracting a counter from another gives a counter with the union of all keys and\n        counts of the second subtracted from counts of the first.\n\n        >>> a = Counter()\n        >>> b = Counter()\n        >>> a[\'first\'] = -2\n        >>> a[\'second\'] = 4\n        >>> b[\'first\'] = 3\n        >>> b[\'third\'] = 1\n        >>> (a - b)[\'first\']\n        -5\n        """"""\n        addend = Counter()\n        for key in self:\n            if key in y:\n                addend[key] = self[key] - y[key]\n            else:\n                addend[key] = self[key]\n        for key in y:\n            if key in self:\n                continue\n            addend[key] = -1 * y[key]\n        return addend\n\n\ndef raiseNotDefined():\n    print(""Method not implemented: %s"" % inspect.stack()[1][3])\n    sys.exit(1)\n\n\ndef normalize(vectorOrCounter):\n    """"""\n    normalize a vector or counter by dividing each value by the sum of all values\n    """"""\n    normalizedCounter = Counter()\n    if type(vectorOrCounter) == type(normalizedCounter):\n        counter = vectorOrCounter\n        total = float(counter.totalCount())\n        if total == 0:\n            return counter\n        for key in list(counter.keys()):\n            value = counter[key]\n            normalizedCounter[key] = value / total\n        return normalizedCounter\n    else:\n        vector = vectorOrCounter\n        s = float(sum(vector))\n        if s == 0:\n            return vector\n        return [el / s for el in vector]\n\n\ndef nSample(distribution, values, n):\n    if sum(distribution) != 1:\n        distribution = normalize(distribution)\n    rand = [random.random() for i in range(n)]\n    rand.sort()\n    samples = []\n    samplePos, distPos, cdf = 0, 0, distribution[0]\n    while samplePos < n:\n        if rand[samplePos] < cdf:\n            samplePos += 1\n            samples.append(values[distPos])\n        else:\n            distPos += 1\n            cdf += distribution[distPos]\n    return samples\n\n\ndef sample(distribution, values=None):\n    if type(distribution) == Counter:\n        items = list(distribution.items())\n        distribution = [i[1] for i in items]\n        values = [i[0] for i in items]\n    if sum(distribution) != 1:\n        distribution = normalize(distribution)\n    choice = random.random()\n    i, total = 0, distribution[0]\n    while choice > total:\n        i += 1\n        total += distribution[i]\n    return values[i]\n\n\ndef sampleFromCounter(ctr):\n    items = list(ctr.items())\n    return sample([v for k, v in items], [k for k, v in items])\n\n\ndef getProbability(value, distribution, values):\n    """"""\n      Gives the probability of a value under a discrete distribution\n      defined by (distributions, values).\n    """"""\n    total = 0.0\n    for prob, val in zip(distribution, values):\n        if val == value:\n            total += prob\n    return total\n\n\ndef flipCoin(p):\n    r = random.random()\n    return r < p\n\n\ndef chooseFromDistribution(distribution):\n    ""Takes either a counter or a list of (prob, key) pairs and samples""\n    if type(distribution) == dict or type(distribution) == Counter:\n        return sample(distribution)\n    r = random.random()\n    base = 0.0\n    for prob, element in distribution:\n        base += prob\n        if r <= base:\n            return element\n\n\ndef nearestPoint(pos):\n    """"""\n    Finds the nearest grid point to a position (discretizes).\n    """"""\n    (current_row, current_col) = pos\n\n    grid_row = int(current_row + 0.5)\n    grid_col = int(current_col + 0.5)\n    return (grid_row, grid_col)\n\n\ndef sign(x):\n    """"""\n    Returns 1 or -1 depending on the sign of x\n    """"""\n    if(x >= 0):\n        return 1\n    else:\n        return -1\n\n\ndef arrayInvert(array):\n    """"""\n    Inverts a matrix stored as a list of lists.\n    """"""\n    result = [[] for i in array]\n    for outer in array:\n        for inner in range(len(outer)):\n            result[inner].append(outer[inner])\n    return result\n\n\ndef matrixAsList(matrix, value=True):\n    """"""\n    Turns a matrix into a list of coordinates matching the specified value\n    """"""\n    rows, cols = len(matrix), len(matrix[0])\n    cells = []\n    for row in range(rows):\n        for col in range(cols):\n            if matrix[row][col] == value:\n                cells.append((row, col))\n    return cells\n\n\ndef lookup(name, namespace):\n    """"""\n    Get a method or class from any imported module from its name.\n    Usage: lookup(functionName, globals())\n    """"""\n    dots = name.count(\'.\')\n    if dots > 0:\n        moduleName, objName = \'.\'.join(\n            name.split(\'.\')[:-1]), name.split(\'.\')[-1]\n        module = __import__(moduleName)\n        return getattr(module, objName)\n    else:\n        modules = [obj for obj in list(namespace.values()) if str(\n            type(obj)) == ""<type \'module\'>""]\n        options = [getattr(module, name)\n                   for module in modules if name in dir(module)]\n        options += [obj[1]\n                    for obj in list(namespace.items()) if obj[0] == name]\n        if len(options) == 1:\n            return options[0]\n        if len(options) > 1:\n            raise Exception(\'Name conflict for %s\')\n        raise Exception(\'%s not found as a method or class\' % name)\n\n\ndef pause():\n    """"""\n    Pauses the output stream awaiting user feedback.\n    """"""\n    print(""<Press enter/return to continue>"")\n    input()\n\n\n# code to handle timeouts\n\n\nclass TimeoutFunctionException(Exception):\n    """"""Exception to raise on a timeout""""""\n    pass\n\n\nclass TimeoutFunction:\n\n    def __init__(self, function, timeout):\n        ""timeout must be at least 1 second. WHY??""\n        self.timeout = timeout\n        self.function = function\n\n    def handle_timeout(self, signum, frame):\n        raise TimeoutFunctionException()\n\n    def __call__(self, *args):\n        if not \'SIGALRM\' in dir(signal):\n            return self.function(*args)\n        old = signal.signal(signal.SIGALRM, self.handle_timeout)\n        signal.alarm(self.timeout)\n        try:\n            result = self.function(*args)\n        finally:\n            signal.signal(signal.SIGALRM, old)\n        signal.alarm(0)\n        return result\n'"
