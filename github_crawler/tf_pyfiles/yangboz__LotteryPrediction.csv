file_path,api_count,code
Keras/PM25Beijing.py,0,"b""#@see: https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/\nfrom pandas import read_csv\nfrom datetime import datetime\n# from matplotlib import pyplot\nimport matplotlib.pyplot as pyplot\n#load data\n# def parse(x):\n#     return datetime.strptime(x,'%Y %m %d %H')\n# dataset = read_csv('PRSA_data_2010.1.1-2014.12.31.csv',parse_dates=[['year', 'month', 'day', 'hour']], index_col=0, date_parser=parse)\n# dataset.drop('No', axis=1, inplace=True)\n# # manually specify column names\n# dataset.columns = ['pollution', 'dew', 'temp', 'press', 'wnd_dir', 'wnd_spd', 'snow', 'rain']\n# dataset.index.name = 'date'\n# # mark all NA values with 0\n# dataset['pollution'].fillna(0, inplace=True)\n# # drop the first 24 hours\n# dataset = dataset[24:]\n# # summarize first 5 rows\n# print(dataset.head(5))\n# # save to file\n# dataset.to_csv('pollution.csv')\n\n# load dataset\ndataset = read_csv('pollution.csv', header=0, index_col=0)\nvalues = dataset.values\n# specify columns to plot\ngroups = [0, 1, 2, 3, 5, 6, 7]\ni = 1\n# plot each column\npyplot.figure()\nfor group in groups:\n\tpyplot.subplot(len(groups), 1, i)\n\tpyplot.plot(values[:, group])\n\tpyplot.title(dataset.columns[group], y=0.5, loc='right')\n\ti += 1\npyplot.show()\n\n"""
Tensorflow/DailyFemaleBirthDataset.py,0,b'#@see: https://machinelearningmastery.com/make-predictions-time-series-forecasting-python/\n'
Tensorflow/TimeSeriels2SupervisedLearning.py,0,"b'import pandas\nimport numpy\n# @see: https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n\ndf_csv = pandas.read_csv(\'./data/red_bule_balls_2003.csv\',names=[\'id\', \'r1\', \'r2\', \'r3\', \'r4\', \'r5\',\'r6\',\'b1\'])\nprint(df_csv)\n\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n\t""""""\n\tFrame a time series as a supervised learning dataset.\n\tArguments:\n\t\tdata: Sequence of observations as a list or NumPy array.\n\t\tn_in: Number of lag observations as input (X).\n\t\tn_out: Number of observations as output (y).\n\t\tdropnan: Boolean whether or not to drop rows with NaN values.\n\tReturns:\n\t\tPandas DataFrame of series framed for supervised learning.\n\t""""""\n\tn_vars = 1 if type(data) is list else data.shape[1]\n\tdf = pandas.DataFrame(data)\n\tcols, names = list(), list()\n\t# input sequence (t-n, ... t-1)\n\tfor i in range(n_in, 0, -1):\n\t\tcols.append(df.shift(i))\n\t\tnames += [(\'var%d(t-%d)\' % (j+1, i)) for j in range(n_vars)]\n\t# forecast sequence (t, t+1, ... t+n)\n\tfor i in range(0, n_out):\n\t\tcols.append(df.shift(-i))\n\t\tif i == 0:\n\t\t\tnames += [(\'var%d(t)\' % (j+1)) for j in range(n_vars)]\n\t\telse:\n\t\t\tnames += [(\'var%d(t+%d)\' % (j+1, i)) for j in range(n_vars)]\n\t# put it all together\n\tagg = pandas.concat(cols, axis=1)\n\tagg.columns = names\n\t# drop rows with NaN values\n\tif dropnan:\n\t\tagg.dropna(inplace=True)\n\treturn agg\n\n# 1.One-Step Univariate Forecasting\n# values = [x for x in range(10)]\n# data = series_to_supervised(values)\n# data = series_to_supervised(values, 3)\nvalues = df_csv.values\n\n# 2.Multi-Step or Sequence Forecasting\n\ndata = series_to_supervised(values, 2, 2)\nprint(data)\n\n# 3.Multivariate Forecasting\n# raw = pandas.DataFrame()\n# raw[\'ob1\'] = [x for x in range(10)]\n# raw[\'ob2\'] = [x for x in range(50, 60)]\n# values = raw.values\ndata = series_to_supervised(values)\nprint(data)\n\ndata = series_to_supervised(values, 1, 2)\nprint(data)\n'"
Tensorflow/lstm.py,19,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A more advanced example, of building an RNN-based time series model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os import path\n\nimport numpy\nimport tensorflow as tf\n\nfrom tensorflow.contrib.timeseries.python.timeseries import estimators as ts_estimators\nfrom tensorflow.contrib.timeseries.python.timeseries import model as ts_model\n\ntry:\n  import matplotlib  # pylint: disable=g-import-not-at-top\n  matplotlib.use(""TkAgg"")  # Need Tk for interactive plots.\n  from matplotlib import pyplot  # pylint: disable=g-import-not-at-top\n  HAS_MATPLOTLIB = True\nexcept ImportError:\n  # Plotting requires matplotlib, but the unit test running this code may\n  # execute in an environment without it (i.e. matplotlib is not a build\n  # dependency). We\'d still like to test the TensorFlow-dependent parts of this\n  # example.\n  HAS_MATPLOTLIB = False\n\n_MODULE_PATH = path.dirname(__file__)\n_DATA_FILE = path.join(_MODULE_PATH, ""data/multivariate_periods.csv"")\n\n\nclass _LSTMModel(ts_model.SequentialTimeSeriesModel):\n  """"""A time series model-building example using an RNNCell.""""""\n\n  def __init__(self, num_units, num_features, dtype=tf.float32):\n    """"""Initialize/configure the model object.\n\n    Note that we do not start graph building here. Rather, this object is a\n    configurable factory for TensorFlow graphs which are run by an Estimator.\n\n    Args:\n      num_units: The number of units in the model\'s LSTMCell.\n      num_features: The dimensionality of the time series (features per\n        timestep).\n      dtype: The floating point data type to use.\n    """"""\n    super(_LSTMModel, self).__init__(\n        # Pre-register the metrics we\'ll be outputting (just a mean here).\n        train_output_names=[""mean""],\n        predict_output_names=[""mean""],\n        num_features=num_features,\n        dtype=dtype)\n    self._num_units = num_units\n    # Filled in by initialize_graph()\n    self._lstm_cell = None\n    self._lstm_cell_run = None\n    self._predict_from_lstm_output = None\n\n  def initialize_graph(self, input_statistics):\n    """"""Save templates for components, which can then be used repeatedly.\n\n    This method is called every time a new graph is created. It\'s safe to start\n    adding ops to the current default graph here, but the graph should be\n    constructed from scratch.\n\n    Args:\n      input_statistics: A math_utils.InputStatistics object.\n    """"""\n    super(_LSTMModel, self).initialize_graph(input_statistics=input_statistics)\n    self._lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=self._num_units)\n    # Create templates so we don\'t have to worry about variable reuse.\n    self._lstm_cell_run = tf.make_template(\n        name_=""lstm_cell"",\n        func_=self._lstm_cell,\n        create_scope_now_=True)\n    # Transforms LSTM output into mean predictions.\n    self._predict_from_lstm_output = tf.make_template(\n        name_=""predict_from_lstm_output"",\n        func_=\n        lambda inputs: tf.layers.dense(inputs=inputs, units=self.num_features),\n        create_scope_now_=True)\n\n  def get_start_state(self):\n    """"""Return initial state for the time series model.""""""\n    return (\n        # Keeps track of the time associated with this state for error checking.\n        tf.zeros([], dtype=tf.int64),\n        # The previous observation or prediction.\n        tf.zeros([self.num_features], dtype=self.dtype),\n        # The state of the RNNCell (batch dimension removed since this parent\n        # class will broadcast).\n        [tf.squeeze(state_element, axis=0)\n         for state_element\n         in self._lstm_cell.zero_state(batch_size=1, dtype=self.dtype)])\n\n  def _filtering_step(self, current_times, current_values, state, predictions):\n    """"""Update model state based on observations.\n\n    Note that we don\'t do much here aside from computing a loss. In this case\n    it\'s easier to update the RNN state in _prediction_step, since that covers\n    running the RNN both on observations (from this method) and our own\n    predictions. This distinction can be important for probabilistic models,\n    where repeatedly predicting without filtering should lead to low-confidence\n    predictions.\n\n    Args:\n      current_times: A [batch size] integer Tensor.\n      current_values: A [batch size, self.num_features] floating point Tensor\n        with new observations.\n      state: The model\'s state tuple.\n      predictions: The output of the previous `_prediction_step`.\n    Returns:\n      A tuple of new state and a predictions dictionary updated to include a\n      loss (note that we could also return other measures of goodness of fit,\n      although only ""loss"" will be optimized).\n    """"""\n    state_from_time, prediction, lstm_state = state\n    with tf.control_dependencies(\n        [tf.assert_equal(current_times, state_from_time)]):\n      # Subtract the mean and divide by the variance of the series.  Slightly\n      # more efficient if done for a whole window (using the normalize_features\n      # argument to SequentialTimeSeriesModel).\n      transformed_values = self._scale_data(current_values)\n      # Use mean squared error across features for the loss.\n      predictions[""loss""] = tf.reduce_mean(\n          (prediction - transformed_values) ** 2, axis=-1)\n      # Keep track of the new observation in model state. It won\'t be run\n      # through the LSTM until the next _imputation_step.\n      new_state_tuple = (current_times, transformed_values, lstm_state)\n    return (new_state_tuple, predictions)\n\n  def _prediction_step(self, current_times, state):\n    """"""Advance the RNN state using a previous observation or prediction.""""""\n    _, previous_observation_or_prediction, lstm_state = state\n    lstm_output, new_lstm_state = self._lstm_cell_run(\n        inputs=previous_observation_or_prediction, state=lstm_state)\n    next_prediction = self._predict_from_lstm_output(lstm_output)\n    new_state_tuple = (current_times, next_prediction, new_lstm_state)\n    return new_state_tuple, {""mean"": self._scale_back_data(next_prediction)}\n\n  def _imputation_step(self, current_times, state):\n    """"""Advance model state across a gap.""""""\n    # Does not do anything special if we\'re jumping across a gap. More advanced\n    # models, especially probabilistic ones, would want a special case that\n    # depends on the gap size.\n    return state\n\n  def _exogenous_input_step(\n      self, current_times, current_exogenous_regressors, state):\n    """"""Update model state based on exogenous regressors.""""""\n    raise NotImplementedError(\n        ""Exogenous inputs are not implemented for this example."")\n\n\ndef train_and_predict(csv_file_name=_DATA_FILE, training_steps=200):\n  """"""Train and predict using a custom time series model.""""""\n  # Construct an Estimator from our LSTM model.\n  estimator = ts_estimators._TimeSeriesRegressor(\n      model=_LSTMModel(num_features=5, num_units=128),\n      optimizer=tf.train.AdamOptimizer(0.001))\n  reader = tf.contrib.timeseries.CSVReader(\n      csv_file_name,\n      column_names=((tf.contrib.timeseries.TrainEvalFeatures.TIMES,)\n                    + (tf.contrib.timeseries.TrainEvalFeatures.VALUES,) * 5))\n  train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(\n      reader, batch_size=4, window_size=32)\n  estimator.train(input_fn=train_input_fn, steps=training_steps)\n  evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)\n  evaluation = estimator.evaluate(input_fn=evaluation_input_fn, steps=1)\n  # Predict starting after the evaluation\n  (predictions,) = tuple(estimator.predict(\n      input_fn=tf.contrib.timeseries.predict_continuation_input_fn(\n          evaluation, steps=100)))\n  times = evaluation[""times""][0]\n  observed = evaluation[""observed""][0, :, :]\n  predicted_mean = numpy.squeeze(numpy.concatenate(\n      [evaluation[""mean""][0], predictions[""mean""]], axis=0))\n  all_times = numpy.concatenate([times, predictions[""times""]], axis=0)\n  return times, observed, all_times, predicted_mean\n\n\ndef main(unused_argv):\n  if not HAS_MATPLOTLIB:\n    raise ImportError(\n        ""Please install matplotlib to generate a plot from this example."")\n  (observed_times, observations,\n   all_times, predictions) = train_and_predict()\n  pyplot.axvline(99, linestyle=""dotted"")\n  observed_lines = pyplot.plot(\n      observed_times, observations, label=""Observed"", color=""k"")\n  predicted_lines = pyplot.plot(\n      all_times, predictions, label=""Predicted"", color=""b"")\n  pyplot.legend(handles=[observed_lines[0], predicted_lines[0]],\n                loc=""upper left"")\n  pyplot.show()\n\n\nif __name__ == ""__main__"":\n  tf.app.run(main=main)\n'"
Tensorflow/test_csv_data.py,10,"b""# coding: utf-8\nfrom __future__ import print_function\nimport tensorflow as tf\nfrom tensorflow.contrib.timeseries.python.timeseries import CSVReader\n\ncsv_file_name = './data/red_bule_balls_2003.csv'\nreader = tf.contrib.timeseries.CSVReader(\n      csv_file_name,\n      column_names=((tf.contrib.timeseries.TrainEvalFeatures.TIMES,)\n                    + (tf.contrib.timeseries.TrainEvalFeatures.VALUES,) * 8))\n\nwith tf.Session() as sess:\n    data = reader.read_full()\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n    print(sess.run(data))\n    coord.request_stop()\n\ntrain_input_fn = tf.contrib.timeseries.RandomWindowInputFn(reader, batch_size=4, window_size=16)\nwith tf.Session() as sess:\n    data = train_input_fn.create_batch()\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n    batch1 = sess.run(data[0])\n    batch2 = sess.run(data[0])\n    coord.request_stop()\n\nprint('batch1:', batch1)\nprint('batch2:', batch2)\n"""
Tensorflow/test_input_array.py,9,"b""# coding: utf-8\nfrom __future__ import print_function\nimport numpy as np\nimport matplotlib\nmatplotlib.use('agg')\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.contrib.timeseries.python.timeseries import  NumpyReader\n\n\nx = np.array(range(1000))\nnoise = np.random.uniform(-0.2, 0.2, 1000)\ny = np.sin(np.pi * x / 100) + x / 200. + noise\nplt.plot(x, y)\nplt.savefig('imgss/timeseries_y.png')\n\ndata = {\n    tf.contrib.timeseries.TrainEvalFeatures.TIMES: x,\n    tf.contrib.timeseries.TrainEvalFeatures.VALUES: y,\n}\n\nreader = NumpyReader(data)\n\nwith tf.Session() as sess:\n    full_data = reader.read_full()\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n    print(sess.run(full_data))\n    coord.request_stop()\n\ntrain_input_fn = tf.contrib.timeseries.RandomWindowInputFn(\n    reader, batch_size=2, window_size=10)\n\nwith tf.Session() as sess:\n    batch_data = train_input_fn.create_batch()\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n    one_batch = sess.run(batch_data[0])\n    coord.request_stop()\n\nprint('one_batch_data:', one_batch)\n"""
Tensorflow/test_input_csv.py,11,"b""# coding: utf-8\nfrom __future__ import print_function\nimport tensorflow as tf\n\ncsv_file_name = './data/multivariate_periods.csv'\n# reader = tf.contrib.timeseries.CSVReader(csv_file_name)\n# csv_file_name = './data/red_bule_balls_2003.csv'\nreader = tf.contrib.timeseries.CSVReader(\n      csv_file_name,\n      column_names=((tf.contrib.timeseries.TrainEvalFeatures.TIMES,)\n                    + (tf.contrib.timeseries.TrainEvalFeatures.VALUES,) * 8))\n\nwith tf.Session() as sess:\n    data = reader.read_full()\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n    print(sess.run(data))\n    coord.request_stop()\n\ntrain_input_fn = tf.contrib.timeseries.RandomWindowInputFn(reader, batch_size=4, window_size=16)\nwith tf.Session() as sess:\n    data = train_input_fn.create_batch()\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n    batch1 = sess.run(data[0])\n    batch2 = sess.run(data[0])\n    coord.request_stop()\n\nprint('batch1:', batch1)\nprint('batch2:', batch2)\n"""
Tensorflow/test_input_csv_multi_variables.py,16,"b'# coding: utf-8\nfrom __future__ import print_function\nfrom os import path\nimport matplotlib.pyplot as plt\n\nimport numpy\nimport tensorflow as tf\n\nfrom tensorflow.contrib.timeseries.python.timeseries import estimators as ts_estimators\nfrom tensorflow.contrib.timeseries.python.timeseries import model as ts_model\n\nimport matplotlib\nmatplotlib.use(""agg"")\n\n\nclass _LSTMModel(ts_model.SequentialTimeSeriesModel):\n  """"""A time series model-building example using an RNNCell.""""""\n\n  def __init__(self, num_units, num_features, dtype=tf.float32):\n    """"""Initialize/configure the model object.\n    Note that we do not start graph building here. Rather, this object is a\n    configurable factory for TensorFlow graphs which are run by an Estimator.\n    Args:\n      num_units: The number of units in the model\'s LSTMCell.\n      num_features: The dimensionality of the time series (features per\n        timestep).\n      dtype: The floating point data type to use.\n    """"""\n    super(_LSTMModel, self).__init__(\n        # Pre-register the metrics we\'ll be outputting (just a mean here).\n        train_output_names=[""mean""],\n        predict_output_names=[""mean""],\n        num_features=num_features,\n        dtype=dtype)\n    self._num_units = num_units\n    # Filled in by initialize_graph()\n    self._lstm_cell = None\n    self._lstm_cell_run = None\n    self._predict_from_lstm_output = None\n\n  def initialize_graph(self, input_statistics):\n    """"""Save templates for components, which can then be used repeatedly.\n    This method is called every time a new graph is created. It\'s safe to start\n    adding ops to the current default graph here, but the graph should be\n    constructed from scratch.\n    Args:\n      input_statistics: A math_utils.InputStatistics object.\n    """"""\n    super(_LSTMModel, self).initialize_graph(input_statistics=input_statistics)\n    self._lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=self._num_units)\n    # Create templates so we don\'t have to worry about variable reuse.\n    self._lstm_cell_run = tf.make_template(\n        name_=""lstm_cell"",\n        func_=self._lstm_cell,\n        create_scope_now_=True)\n    # Transforms LSTM output into mean predictions.\n    self._predict_from_lstm_output = tf.make_template(\n        name_=""predict_from_lstm_output"",\n        func_=lambda inputs: tf.layers.dense(inputs=inputs, units=self.num_features),\n        create_scope_now_=True)\n\n  def get_start_state(self):\n    """"""Return initial state for the time series model.""""""\n    return (\n        # Keeps track of the time associated with this state for error checking.\n        tf.zeros([], dtype=tf.int64),\n        # The previous observation or prediction.\n        tf.zeros([self.num_features], dtype=self.dtype),\n        # The state of the RNNCell (batch dimension removed since this parent\n        # class will broadcast).\n        [tf.squeeze(state_element, axis=0)\n         for state_element\n         in self._lstm_cell.zero_state(batch_size=1, dtype=self.dtype)])\n\n  def _transform(self, data):\n    """"""Normalize data based on input statistics to encourage stable training.""""""\n    mean, variance = self._input_statistics.overall_feature_moments\n    return (data - mean) / variance\n\n  def _de_transform(self, data):\n    """"""Transform data back to the input scale.""""""\n    mean, variance = self._input_statistics.overall_feature_moments\n    return data * variance + mean\n\n  def _filtering_step(self, current_times, current_values, state, predictions):\n    """"""Update model state based on observations.\n    Note that we don\'t do much here aside from computing a loss. In this case\n    it\'s easier to update the RNN state in _prediction_step, since that covers\n    running the RNN both on observations (from this method) and our own\n    predictions. This distinction can be important for probabilistic models,\n    where repeatedly predicting without filtering should lead to low-confidence\n    predictions.\n    Args:\n      current_times: A [batch size] integer Tensor.\n      current_values: A [batch size, self.num_features] floating point Tensor\n        with new observations.\n      state: The model\'s state tuple.\n      predictions: The output of the previous `_prediction_step`.\n    Returns:\n      A tuple of new state and a predictions dictionary updated to include a\n      loss (note that we could also return other measures of goodness of fit,\n      although only ""loss"" will be optimized).\n    """"""\n    state_from_time, prediction, lstm_state = state\n    with tf.control_dependencies(\n            [tf.assert_equal(current_times, state_from_time)]):\n      transformed_values = self._transform(current_values)\n      # Use mean squared error across features for the loss.\n      predictions[""loss""] = tf.reduce_mean(\n          (prediction - transformed_values) ** 2, axis=-1)\n      # Keep track of the new observation in model state. It won\'t be run\n      # through the LSTM until the next _imputation_step.\n      new_state_tuple = (current_times, transformed_values, lstm_state)\n    return (new_state_tuple, predictions)\n\n  def _prediction_step(self, current_times, state):\n    """"""Advance the RNN state using a previous observation or prediction.""""""\n    _, previous_observation_or_prediction, lstm_state = state\n    lstm_output, new_lstm_state = self._lstm_cell_run(\n        inputs=previous_observation_or_prediction, state=lstm_state)\n    next_prediction = self._predict_from_lstm_output(lstm_output)\n    new_state_tuple = (current_times, next_prediction, new_lstm_state)\n    return new_state_tuple, {""mean"": self._de_transform(next_prediction)}\n\n  def _imputation_step(self, current_times, state):\n    """"""Advance model state across a gap.""""""\n    # Does not do anything special if we\'re jumping across a gap. More advanced\n    # models, especially probabilistic ones, would want a special case that\n    # depends on the gap size.\n    return state\n\n  def _exogenous_input_step(\n          self, current_times, current_exogenous_regressors, state):\n    """"""Update model state based on exogenous regressors.""""""\n    raise NotImplementedError(\n        ""Exogenous inputs are not implemented for this example."")\n\n\n\n\ncsv_file_name = \'./data/multivariate_periods.csv\'\n# reader = tf.contrib.timeseries.CSVReader(csv_file_name)\n# csv_file_name = \'./data/red_bule_balls_2003.csv\'\nreader = tf.contrib.timeseries.CSVReader(\n      csv_file_name,\n      column_names=((tf.contrib.timeseries.TrainEvalFeatures.TIMES,)\n                    + (tf.contrib.timeseries.TrainEvalFeatures.VALUES,) * 5))\nprint(reader)\ndata = reader.read_full()\nprint(data)\n# estimator\n# @see:https://mp.weixin.qq.com/s?__biz=MzI0ODcxODk5OA==&mid=2247489403&idx=1&sn=9f004963889104fd83307e48e9ec8709&chksm=e99d2482deeaad94ae1f1154b7c4de91eafa678d2feaebe9183987efd9b38287fdf8d2f3f04a&mpshare=1&scene=1&srcid=0925uD5dshUAN3UoUe9PD4mV&key=f32e811df94f16bf6c056cd27bde2d6cf2de557b042b10eebff3ec8204d94d8b19d5502a0f108ced8ea7006b76087c025b93cb3864bafee6a311dfd9990f974fe965643704ab1c791d9a2355d4b21f1e&ascene=0&uin=MTkyNTE5ODcwMw%3D%3D&devicetype=iMac+MacBookAir6%2C2+OSX+OSX+10.12.6+build(16G29)&version=12020810&nettype=WIFI&fontScale=100&pass_ticket=LQXTGodGb%2B7%2BPVkkVqV4W1EEZLoNH7Wi%2BnJxlbUYnFVsrEsp3ZsvKzcliL9Ezrm7\nestimator = ts_estimators._TimeSeriesRegressor(\n    model= _LSTMModel(num_features=5,num_units=128),\n    optimizer = tf.train.AdamOptimizer(0.001)\n)\nprint(estimator)\n\nplt.figure(figsize=(15, 5))\n# plt.plot(data[\'times\'], data[\'values\'], label=\'origin\')\n# plt.plot(evaluation[\'times\'].reshape(-1), evaluation[\'mean\'].reshape(-1), label=\'evaluation\')\n# plt.plot(predictions[\'times\'].reshape(-1), predictions[\'mean\'].reshape(-1), label=\'prediction\')\n# plt.xlabel(\'time_step\')\nplt.show()\n'"
Tensorflow/train_array.py,9,"b""# coding: utf-8\nfrom __future__ import print_function\nimport numpy as np\nimport matplotlib\nmatplotlib.use('agg')\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.contrib.timeseries.python.timeseries import  NumpyReader\n\n\ndef main(_):\n    x = np.array(range(1000))\n    noise = np.random.uniform(-0.2, 0.2, 1000)\n    y = np.sin(np.pi * x / 100) + x / 200. + noise\n    plt.plot(x, y)\n    plt.savefig('timeseries_y.jpg')\n\n    data = {\n        tf.contrib.timeseries.TrainEvalFeatures.TIMES: x,\n        tf.contrib.timeseries.TrainEvalFeatures.VALUES: y,\n    }\n\n    reader = NumpyReader(data)\n\n    train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(\n        reader, batch_size=16, window_size=40)\n\n    ar = tf.contrib.timeseries.ARRegressor(\n        periodicities=200, input_window_size=30, output_window_size=10,\n        num_features=1,\n        loss=tf.contrib.timeseries.ARModel.NORMAL_LIKELIHOOD_LOSS)\n\n    ar.train(input_fn=train_input_fn, steps=6000)\n\n    evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)\n    # keys of evaluation: ['covariance', 'loss', 'mean', 'observed', 'start_tuple', 'times', 'global_step']\n    evaluation = ar.evaluate(input_fn=evaluation_input_fn, steps=1)\n\n    (predictions,) = tuple(ar.predict(\n        input_fn=tf.contrib.timeseries.predict_continuation_input_fn(\n            evaluation, steps=250)))\n\n    plt.figure(figsize=(15, 5))\n    plt.plot(data['times'].reshape(-1), data['values'].reshape(-1), label='origin')\n    plt.plot(evaluation['times'].reshape(-1), evaluation['mean'].reshape(-1), label='evaluation')\n    plt.plot(predictions['times'].reshape(-1), predictions['mean'].reshape(-1), label='prediction')\n    plt.xlabel('time_step')\n    plt.ylabel('values')\n    plt.legend(loc=4)\n    plt.savefig('predict_result.jpg')\n\n\nif __name__ == '__main__':\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run()\n"""
Tensorflow/train_csv.py,11,"b""# coding: utf-8\nfrom __future__ import print_function\nimport numpy as np\nimport matplotlib\nmatplotlib.use('agg')\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\n\ndef main(_):\n    csv_file_name = './data/period_trend.csv'\n    reader = tf.contrib.timeseries.CSVReader(csv_file_name)\n    train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(reader, batch_size=16, window_size=16)\n    with tf.Session() as sess:\n        data = reader.read_full()\n        coord = tf.train.Coordinator()\n        tf.train.start_queue_runners(sess=sess, coord=coord)\n        data = sess.run(data)\n        coord.request_stop()\n\n    ar = tf.contrib.timeseries.ARRegressor(\n        periodicities=100, input_window_size=10, output_window_size=6,\n        num_features=1,\n        loss=tf.contrib.timeseries.ARModel.NORMAL_LIKELIHOOD_LOSS)\n\n    ar.train(input_fn=train_input_fn, steps=1000)\n\n    evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)\n    # keys of evaluation: ['covariance', 'loss', 'mean', 'observed', 'start_tuple', 'times', 'global_step']\n    evaluation = ar.evaluate(input_fn=evaluation_input_fn, steps=1)\n\n    (predictions,) = tuple(ar.predict(\n        input_fn=tf.contrib.timeseries.predict_continuation_input_fn(\n            evaluation, steps=250)))\n\n    plt.figure(figsize=(15, 5))\n    plt.plot(data['times'].reshape(-1), data['values'].reshape(-1), label='origin')\n    plt.plot(evaluation['times'].reshape(-1), evaluation['mean'].reshape(-1), label='evaluation')\n    plt.plot(predictions['times'].reshape(-1), predictions['mean'].reshape(-1), label='prediction')\n    plt.xlabel('time_step')\n    plt.ylabel('values')\n    plt.legend(loc=4)\n    plt.savefig('predict_result_period_trend.png')\n\n\nif __name__ == '__main__':\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run()\n"""
Tensorflow/train_lstm.py,18,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os import path\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.contrib.timeseries.python.timeseries import estimators as ts_estimators\nfrom tensorflow.contrib.timeseries.python.timeseries import model as ts_model\nfrom tensorflow.contrib.timeseries.python.timeseries import  NumpyReader\n\nimport matplotlib\nmatplotlib.use(""agg"")\nimport matplotlib.pyplot as plt\n\n\nclass _LSTMModel(ts_model.SequentialTimeSeriesModel):\n  """"""A time series model-building example using an RNNCell.""""""\n\n  def __init__(self, num_units, num_features, dtype=tf.float32):\n    """"""Initialize/configure the model object.\n    Note that we do not start graph building here. Rather, this object is a\n    configurable factory for TensorFlow graphs which are run by an Estimator.\n    Args:\n      num_units: The number of units in the model\'s LSTMCell.\n      num_features: The dimensionality of the time series (features per\n        timestep).\n      dtype: The floating point data type to use.\n    """"""\n    super(_LSTMModel, self).__init__(\n        # Pre-register the metrics we\'ll be outputting (just a mean here).\n        train_output_names=[""mean""],\n        predict_output_names=[""mean""],\n        num_features=num_features,\n        dtype=dtype)\n    self._num_units = num_units\n    # Filled in by initialize_graph()\n    self._lstm_cell = None\n    self._lstm_cell_run = None\n    self._predict_from_lstm_output = None\n\n  def initialize_graph(self, input_statistics):\n    """"""Save templates for components, which can then be used repeatedly.\n    This method is called every time a new graph is created. It\'s safe to start\n    adding ops to the current default graph here, but the graph should be\n    constructed from scratch.\n    Args:\n      input_statistics: A math_utils.InputStatistics object.\n    """"""\n    super(_LSTMModel, self).initialize_graph(input_statistics=input_statistics)\n    self._lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=self._num_units)\n    # Create templates so we don\'t have to worry about variable reuse.\n    self._lstm_cell_run = tf.make_template(\n        name_=""lstm_cell"",\n        func_=self._lstm_cell,\n        create_scope_now_=True)\n    # Transforms LSTM output into mean predictions.\n    self._predict_from_lstm_output = tf.make_template(\n        name_=""predict_from_lstm_output"",\n        func_=lambda inputs: tf.layers.dense(inputs=inputs, units=self.num_features),\n        create_scope_now_=True)\n\n  def get_start_state(self):\n    """"""Return initial state for the time series model.""""""\n    return (\n        # Keeps track of the time associated with this state for error checking.\n        tf.zeros([], dtype=tf.int64),\n        # The previous observation or prediction.\n        tf.zeros([self.num_features], dtype=self.dtype),\n        # The state of the RNNCell (batch dimension removed since this parent\n        # class will broadcast).\n        [tf.squeeze(state_element, axis=0)\n         for state_element\n         in self._lstm_cell.zero_state(batch_size=1, dtype=self.dtype)])\n\n  def _transform(self, data):\n    """"""Normalize data based on input statistics to encourage stable training.""""""\n    mean, variance = self._input_statistics.overall_feature_moments\n    return (data - mean) / variance\n\n  def _de_transform(self, data):\n    """"""Transform data back to the input scale.""""""\n    mean, variance = self._input_statistics.overall_feature_moments\n    return data * variance + mean\n\n  def _filtering_step(self, current_times, current_values, state, predictions):\n    """"""Update model state based on observations.\n    Note that we don\'t do much here aside from computing a loss. In this case\n    it\'s easier to update the RNN state in _prediction_step, since that covers\n    running the RNN both on observations (from this method) and our own\n    predictions. This distinction can be important for probabilistic models,\n    where repeatedly predicting without filtering should lead to low-confidence\n    predictions.\n    Args:\n      current_times: A [batch size] integer Tensor.\n      current_values: A [batch size, self.num_features] floating point Tensor\n        with new observations.\n      state: The model\'s state tuple.\n      predictions: The output of the previous `_prediction_step`.\n    Returns:\n      A tuple of new state and a predictions dictionary updated to include a\n      loss (note that we could also return other measures of goodness of fit,\n      although only ""loss"" will be optimized).\n    """"""\n    state_from_time, prediction, lstm_state = state\n    with tf.control_dependencies(\n            [tf.assert_equal(current_times, state_from_time)]):\n      transformed_values = self._transform(current_values)\n      # Use mean squared error across features for the loss.\n      predictions[""loss""] = tf.reduce_mean(\n          (prediction - transformed_values) ** 2, axis=-1)\n      # Keep track of the new observation in model state. It won\'t be run\n      # through the LSTM until the next _imputation_step.\n      new_state_tuple = (current_times, transformed_values, lstm_state)\n    return (new_state_tuple, predictions)\n\n  def _prediction_step(self, current_times, state):\n    """"""Advance the RNN state using a previous observation or prediction.""""""\n    _, previous_observation_or_prediction, lstm_state = state\n    lstm_output, new_lstm_state = self._lstm_cell_run(\n        inputs=previous_observation_or_prediction, state=lstm_state)\n    next_prediction = self._predict_from_lstm_output(lstm_output)\n    new_state_tuple = (current_times, next_prediction, new_lstm_state)\n    return new_state_tuple, {""mean"": self._de_transform(next_prediction)}\n\n  def _imputation_step(self, current_times, state):\n    """"""Advance model state across a gap.""""""\n    # Does not do anything special if we\'re jumping across a gap. More advanced\n    # models, especially probabilistic ones, would want a special case that\n    # depends on the gap size.\n    return state\n\n  def _exogenous_input_step(\n          self, current_times, current_exogenous_regressors, state):\n    """"""Update model state based on exogenous regressors.""""""\n    raise NotImplementedError(\n        ""Exogenous inputs are not implemented for this example."")\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  x = np.array(range(1000))\n  noise = np.random.uniform(-0.2, 0.2, 1000)\n  y = np.sin(np.pi * x / 50 ) + np.cos(np.pi * x / 50) + np.sin(np.pi * x / 25) + noise\n\n  data = {\n      tf.contrib.timeseries.TrainEvalFeatures.TIMES: x,\n      tf.contrib.timeseries.TrainEvalFeatures.VALUES: y,\n  }\n\n  reader = NumpyReader(data)\n\n  train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(\n      reader, batch_size=4, window_size=100)\n\n  estimator = ts_estimators.TimeSeriesRegressor(\n      model=_LSTMModel(num_features=1, num_units=128),\n      optimizer=tf.train.AdamOptimizer(0.001))\n\n  estimator.train(input_fn=train_input_fn, steps=2000)\n  evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)\n  evaluation = estimator.evaluate(input_fn=evaluation_input_fn, steps=1)\n  # Predict starting after the evaluation\n  (predictions,) = tuple(estimator.predict(\n      input_fn=tf.contrib.timeseries.predict_continuation_input_fn(\n          evaluation, steps=200)))\n\n  observed_times = evaluation[""times""][0]\n  observed = evaluation[""observed""][0, :, :]\n  evaluated_times = evaluation[""times""][0]\n  evaluated = evaluation[""mean""][0]\n  predicted_times = predictions[\'times\']\n  predicted = predictions[""mean""]\n\n  plt.figure(figsize=(15, 5))\n  plt.axvline(999, linestyle=""dotted"", linewidth=4, color=\'r\')\n  observed_lines = plt.plot(observed_times, observed, label=""observation"", color=""k"")\n  evaluated_lines = plt.plot(evaluated_times, evaluated, label=""evaluation"", color=""g"")\n  predicted_lines = plt.plot(predicted_times, predicted, label=""prediction"", color=""r"")\n  plt.legend(handles=[observed_lines[0], evaluated_lines[0], predicted_lines[0]],\n             loc=""upper left"")\n  plt.savefig(\'predict_result.jpg\')\n'"
Tensorflow/train_lstm_multivariate.py,19,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os import path\n\nimport numpy\nimport tensorflow as tf\n\nfrom tensorflow.contrib.timeseries.python.timeseries import estimators as ts_estimators\nfrom tensorflow.contrib.timeseries.python.timeseries import model as ts_model\nimport matplotlib\nmatplotlib.use(""agg"")\nimport matplotlib.pyplot as plt\n\nclass _LSTMModel(ts_model.SequentialTimeSeriesModel):\n  """"""A time series model-building example using an RNNCell.""""""\n\n  def __init__(self, num_units, num_features, dtype=tf.float32):\n    """"""Initialize/configure the model object.\n    Note that we do not start graph building here. Rather, this object is a\n    configurable factory for TensorFlow graphs which are run by an Estimator.\n    Args:\n      num_units: The number of units in the model\'s LSTMCell.\n      num_features: The dimensionality of the time series (features per\n        timestep).\n      dtype: The floating point data type to use.\n    """"""\n    super(_LSTMModel, self).__init__(\n        # Pre-register the metrics we\'ll be outputting (just a mean here).\n        train_output_names=[""mean""],\n        predict_output_names=[""mean""],\n        num_features=num_features,\n        dtype=dtype)\n    self._num_units = num_units\n    # Filled in by initialize_graph()\n    self._lstm_cell = None\n    self._lstm_cell_run = None\n    self._predict_from_lstm_output = None\n\n  def initialize_graph(self, input_statistics):\n    """"""Save templates for components, which can then be used repeatedly.\n    This method is called every time a new graph is created. It\'s safe to start\n    adding ops to the current default graph here, but the graph should be\n    constructed from scratch.\n    Args:\n      input_statistics: A math_utils.InputStatistics object.\n    """"""\n    super(_LSTMModel, self).initialize_graph(input_statistics=input_statistics)\n    self._lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=self._num_units)\n    # Create templates so we don\'t have to worry about variable reuse.\n    self._lstm_cell_run = tf.make_template(\n        name_=""lstm_cell"",\n        func_=self._lstm_cell,\n        create_scope_now_=True)\n    # Transforms LSTM output into mean predictions.\n    self._predict_from_lstm_output = tf.make_template(\n        name_=""predict_from_lstm_output"",\n        func_=lambda inputs: tf.layers.dense(inputs=inputs, units=self.num_features),\n        create_scope_now_=True)\n\n  def get_start_state(self):\n    """"""Return initial state for the time series model.""""""\n    return (\n        # Keeps track of the time associated with this state for error checking.\n        tf.zeros([], dtype=tf.int64),\n        # The previous observation or prediction.\n        tf.zeros([self.num_features], dtype=self.dtype),\n        # The state of the RNNCell (batch dimension removed since this parent\n        # class will broadcast).\n        [tf.squeeze(state_element, axis=0)\n         for state_element\n         in self._lstm_cell.zero_state(batch_size=1, dtype=self.dtype)])\n\n  def _transform(self, data):\n    """"""Normalize data based on input statistics to encourage stable training.""""""\n    mean, variance = self._input_statistics.overall_feature_moments\n    return (data - mean) / variance\n\n  def _de_transform(self, data):\n    """"""Transform data back to the input scale.""""""\n    mean, variance = self._input_statistics.overall_feature_moments\n    return data * variance + mean\n\n  def _filtering_step(self, current_times, current_values, state, predictions):\n    """"""Update model state based on observations.\n    Note that we don\'t do much here aside from computing a loss. In this case\n    it\'s easier to update the RNN state in _prediction_step, since that covers\n    running the RNN both on observations (from this method) and our own\n    predictions. This distinction can be important for probabilistic models,\n    where repeatedly predicting without filtering should lead to low-confidence\n    predictions.\n    Args:\n      current_times: A [batch size] integer Tensor.\n      current_values: A [batch size, self.num_features] floating point Tensor\n        with new observations.\n      state: The model\'s state tuple.\n      predictions: The output of the previous `_prediction_step`.\n    Returns:\n      A tuple of new state and a predictions dictionary updated to include a\n      loss (note that we could also return other measures of goodness of fit,\n      although only ""loss"" will be optimized).\n    """"""\n    state_from_time, prediction, lstm_state = state\n    with tf.control_dependencies(\n            [tf.assert_equal(current_times, state_from_time)]):\n      transformed_values = self._transform(current_values)\n      # Use mean squared error across features for the loss.\n      predictions[""loss""] = tf.reduce_mean(\n          (prediction - transformed_values) ** 2, axis=-1)\n      # Keep track of the new observation in model state. It won\'t be run\n      # through the LSTM until the next _imputation_step.\n      new_state_tuple = (current_times, transformed_values, lstm_state)\n    return (new_state_tuple, predictions)\n\n  def _prediction_step(self, current_times, state):\n    """"""Advance the RNN state using a previous observation or prediction.""""""\n    _, previous_observation_or_prediction, lstm_state = state\n    lstm_output, new_lstm_state = self._lstm_cell_run(\n        inputs=previous_observation_or_prediction, state=lstm_state)\n    next_prediction = self._predict_from_lstm_output(lstm_output)\n    new_state_tuple = (current_times, next_prediction, new_lstm_state)\n    return new_state_tuple, {""mean"": self._de_transform(next_prediction)}\n\n  def _imputation_step(self, current_times, state):\n    """"""Advance model state across a gap.""""""\n    # Does not do anything special if we\'re jumping across a gap. More advanced\n    # models, especially probabilistic ones, would want a special case that\n    # depends on the gap size.\n    return state\n\n  def _exogenous_input_step(\n          self, current_times, current_exogenous_regressors, state):\n    """"""Update model state based on exogenous regressors.""""""\n    raise NotImplementedError(\n        ""Exogenous inputs are not implemented for this example."")\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  csv_file_name = path.join(""./data/multivariate_periods.csv"")\n  reader = tf.contrib.timeseries.CSVReader(\n      csv_file_name,\n      column_names=((tf.contrib.timeseries.TrainEvalFeatures.TIMES,)\n                    + (tf.contrib.timeseries.TrainEvalFeatures.VALUES,) * 5))\n  train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(\n      reader, batch_size=4, window_size=32)\n\n  estimator = ts_estimators._TimeSeriesRegressor(\n      model=_LSTMModel(num_features=5, num_units=128),\n      optimizer=tf.train.AdamOptimizer(0.001))\n\n  estimator.train(input_fn=train_input_fn, steps=200)\n  evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)\n  evaluation = estimator.evaluate(input_fn=evaluation_input_fn, steps=1)\n  # Predict starting after the evaluation\n  (predictions,) = tuple(estimator.predict(\n      input_fn=tf.contrib.timeseries.predict_continuation_input_fn(\n          evaluation, steps=100)))\n\n  observed_times = evaluation[""times""][0]\n  observed = evaluation[""observed""][0, :, :]\n  evaluated_times = evaluation[""times""][0]\n  evaluated = evaluation[""mean""][0]\n  predicted_times = predictions[\'times\']\n  predicted = predictions[""mean""]\n\n  plt.figure(figsize=(15, 5))\n  plt.axvline(99, linestyle=""dotted"", linewidth=4, color=\'r\')\n  observed_lines = plt.plot(observed_times, observed, label=""observation"", color=""k"")\n  evaluated_lines = plt.plot(evaluated_times, evaluated, label=""evaluation"", color=""g"")\n  predicted_lines = plt.plot(predicted_times, predicted, label=""prediction"", color=""r"")\n  plt.legend(handles=[observed_lines[0], evaluated_lines[0], predicted_lines[0]],\n             loc=""upper left"")\n  plt.savefig(\'predict_result.png\')\n'"
Tensorflow/train_lstm_redblueballs.py,19,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os import path\n\nimport numpy\nimport tensorflow as tf\n\nfrom tensorflow.contrib.timeseries.python.timeseries import estimators as ts_estimators\nfrom tensorflow.contrib.timeseries.python.timeseries import model as ts_model\nimport matplotlib\nmatplotlib.use(""agg"")\nimport matplotlib.pyplot as plt\n\nclass _LSTMModel(ts_model.SequentialTimeSeriesModel):\n  """"""A time series model-building example using an RNNCell.""""""\n\n  def __init__(self, num_units, num_features, dtype=tf.float32):\n    """"""Initialize/configure the model object.\n    Note that we do not start graph building here. Rather, this object is a\n    configurable factory for TensorFlow graphs which are run by an Estimator.\n    Args:\n      num_units: The number of units in the model\'s LSTMCell.\n      num_features: The dimensionality of the time series (features per\n        timestep).\n      dtype: The floating point data type to use.\n    """"""\n    super(_LSTMModel, self).__init__(\n        # Pre-register the metrics we\'ll be outputting (just a mean here).\n        train_output_names=[""mean""],\n        predict_output_names=[""mean""],\n        num_features=num_features,\n        dtype=dtype)\n    self._num_units = num_units\n    # Filled in by initialize_graph()\n    self._lstm_cell = None\n    self._lstm_cell_run = None\n    self._predict_from_lstm_output = None\n\n  def initialize_graph(self, input_statistics):\n    """"""Save templates for components, which can then be used repeatedly.\n    This method is called every time a new graph is created. It\'s safe to start\n    adding ops to the current default graph here, but the graph should be\n    constructed from scratch.\n    Args:\n      input_statistics: A math_utils.InputStatistics object.\n    """"""\n    super(_LSTMModel, self).initialize_graph(input_statistics=input_statistics)\n    self._lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=self._num_units)\n    # Create templates so we don\'t have to worry about variable reuse.\n    self._lstm_cell_run = tf.make_template(\n        name_=""lstm_cell"",\n        func_=self._lstm_cell,\n        create_scope_now_=True)\n    # Transforms LSTM output into mean predictions.\n    self._predict_from_lstm_output = tf.make_template(\n        name_=""predict_from_lstm_output"",\n        func_=lambda inputs: tf.layers.dense(inputs=inputs, units=self.num_features),\n        create_scope_now_=True)\n\n  def get_start_state(self):\n    """"""Return initial state for the time series model.""""""\n    return (\n        # Keeps track of the time associated with this state for error checking.\n        tf.zeros([], dtype=tf.int64),\n        # The previous observation or prediction.\n        tf.zeros([self.num_features], dtype=self.dtype),\n        # The state of the RNNCell (batch dimension removed since this parent\n        # class will broadcast).\n        [tf.squeeze(state_element, axis=0)\n         for state_element\n         in self._lstm_cell.zero_state(batch_size=1, dtype=self.dtype)])\n\n  def _transform(self, data):\n    """"""Normalize data based on input statistics to encourage stable training.""""""\n    mean, variance = self._input_statistics.overall_feature_moments\n    return (data - mean) / variance\n\n  def _de_transform(self, data):\n    """"""Transform data back to the input scale.""""""\n    mean, variance = self._input_statistics.overall_feature_moments\n    return data * variance + mean\n\n  def _filtering_step(self, current_times, current_values, state, predictions):\n    """"""Update model state based on observations.\n    Note that we don\'t do much here aside from computing a loss. In this case\n    it\'s easier to update the RNN state in _prediction_step, since that covers\n    running the RNN both on observations (from this method) and our own\n    predictions. This distinction can be important for probabilistic models,\n    where repeatedly predicting without filtering should lead to low-confidence\n    predictions.\n    Args:\n      current_times: A [batch size] integer Tensor.\n      current_values: A [batch size, self.num_features] floating point Tensor\n        with new observations.\n      state: The model\'s state tuple.\n      predictions: The output of the previous `_prediction_step`.\n    Returns:\n      A tuple of new state and a predictions dictionary updated to include a\n      loss (note that we could also return other measures of goodness of fit,\n      although only ""loss"" will be optimized).\n    """"""\n    state_from_time, prediction, lstm_state = state\n    with tf.control_dependencies(\n            [tf.assert_equal(current_times, state_from_time)]):\n      transformed_values = self._transform(current_values)\n      # Use mean squared error across features for the loss.\n      predictions[""loss""] = tf.reduce_mean(\n          (prediction - transformed_values) ** 2, axis=-1)\n      # Keep track of the new observation in model state. It won\'t be run\n      # through the LSTM until the next _imputation_step.\n      new_state_tuple = (current_times, transformed_values, lstm_state)\n    return (new_state_tuple, predictions)\n\n  def _prediction_step(self, current_times, state):\n    """"""Advance the RNN state using a previous observation or prediction.""""""\n    _, previous_observation_or_prediction, lstm_state = state\n    lstm_output, new_lstm_state = self._lstm_cell_run(\n        inputs=previous_observation_or_prediction, state=lstm_state)\n    next_prediction = self._predict_from_lstm_output(lstm_output)\n    new_state_tuple = (current_times, next_prediction, new_lstm_state)\n    return new_state_tuple, {""mean"": self._de_transform(next_prediction)}\n\n  def _imputation_step(self, current_times, state):\n    """"""Advance model state across a gap.""""""\n    # Does not do anything special if we\'re jumping across a gap. More advanced\n    # models, especially probabilistic ones, would want a special case that\n    # depends on the gap size.\n    return state\n\n  def _exogenous_input_step(\n          self, current_times, current_exogenous_regressors, state):\n    """"""Update model state based on exogenous regressors.""""""\n    raise NotImplementedError(\n        ""Exogenous inputs are not implemented for this example."")\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  csv_file_name = path.join(""./data/red_bule_balls_2003.csv"")\n  reader = tf.contrib.timeseries.CSVReader(\n      csv_file_name,\n      column_names=((tf.contrib.timeseries.TrainEvalFeatures.TIMES,)\n                    + (tf.contrib.timeseries.TrainEvalFeatures.VALUES,) * 7))\n  train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(\n      reader, batch_size=4, window_size=32)\n\n  estimator = ts_estimators._TimeSeriesRegressor(\n      model=_LSTMModel(num_features=7, num_units=128),\n      optimizer=tf.train.AdamOptimizer(0.001))\n\n  estimator.train(input_fn=train_input_fn, steps=100)\n  evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)\n  evaluation = estimator.evaluate(input_fn=evaluation_input_fn, steps=1)\n  # Predict starting after the evaluation\n  (predictions,) = tuple(estimator.predict(\n      input_fn=tf.contrib.timeseries.predict_continuation_input_fn(\n          evaluation, steps=4)))\n\n  observed_times = evaluation[""times""][0]\n  observed = evaluation[""observed""][0, :, :]\n  evaluated_times = evaluation[""times""][0]\n  evaluated = evaluation[""mean""][0]\n  predicted_times = predictions[\'times\']\n  predicted = predictions[""mean""]\n\n  plt.figure(figsize=(15, 5))\n  plt.axvline(99, linestyle=""dotted"", linewidth=4, color=\'r\')\n  observed_lines = plt.plot(observed_times, observed, label=""observation"", color=""k"")\n  evaluated_lines = plt.plot(evaluated_times, evaluated, label=""evaluation"", color=""g"")\n  predicted_lines = plt.plot(predicted_times, predicted, label=""prediction"", color=""r"")\n  plt.legend(handles=[observed_lines[0], evaluated_lines[0], predicted_lines[0]],\n             loc=""upper left"")\n  plt.savefig(\'predict_result_redblueballs.png\')\n'"
Python/src/RandomNumber.py,0,"b""import random\nfrom scipy import *\n#from pylab import *\nimport pylab as pylab\n#variables\nredBalls=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32]\nblueBalls=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n#test code\n#import itertools\n#print(list(itertools.permutations(redBalls,6)))\n#print( random.random() )\nfor x in range(0,6):#NUM_OF_RED=6\n    choice_num_red = random.choice( redBalls )\n    print( choice_num_red )\n    redBalls.remove(choice_num_red)\nfor y in range(0,1):#NUM_OF_BLUE=1\n    choice_num_blue = random.choice( blueBalls )\n    print( choice_num_blue )\n#scipy test code\na = zeros(1000)\na[:100] = 1\nprint(a)\nb = fft(a)\nprint(b)\n#matplotlib test \nprint(pylab.plot(abs(b)))\n#show()\n#from matplotlib.mlab import normpdf\n#import matplotlib.numerix as nx\n#import pylab as p\n#\n#x = nx.arange(-4, 4, 0.01)\n#y = normpdf(x, 0, 1) # unit normal\n#p.plot(x,y, color='red', lw=2)\n#p.show()"""
Python/src/RedAndBlueBalls.py,0,"b'import pandas\nimport time\nimport matplotlib.pyplot as plt\nimport random\nimport numpy\n#See http://www.red-dove.com/python_logging.html\nimport logging\nlogging.basicConfig()\nlog = logging.getLogger(None)\nlog.setLevel(logging.DEBUG) #set verbosity to show all messages of severity >= DEBUG\nlog.info(""Starting PandasCSV!"")\nclass UTCFormatter(logging.Formatter):\n    converter = time.gmtime # not documented, had to read the module\'s source code ;-)\n#read CSV\n#balls = pd.read_csv(\'../../lottery-data/red_bule_balls_2003.csv\')\nballs = pandas.read_csv(\'../../lottery-data/red_bule_balls.csv\',sep=\';\',header=None)\nprint(balls)\n#Dot plot,a very simple way to gain an initial sense of the data set is to create a dot plot.\n##Dot plot display\n###Blue balls statistics\nblue_balls = balls[\'X7\']\nprint(""blue_balls.describe:"")\nprint(blue_balls.describe())\nseries_blue_balls = pandas.Series(blue_balls)\nprint(""series_blue_balls.value_counts():"")\nseries_blue_balls_value_counts = series_blue_balls.value_counts()\n##sort_index\nseries_blue_balls_value_counts = series_blue_balls_value_counts.sort_index(ascending=True)\nprint(series_blue_balls_value_counts)\n##Dot plot display\ndfs_blue_balls_count_values = series_blue_balls_value_counts.values\nprint(""Dot,dfs_blue_balls_count_values:"")\nprint(dfs_blue_balls_count_values)\nplt.plot(dfs_blue_balls_count_values,\'x\',label=\'Dot plot\')\nplt.legend()\nplt.ylabel(\'Y-axis,number of blue balls\')\nplt.xlabel(\'X-axis,number of duplication\')\nplt.show()\n#Jitter plot\nidx_min = min(dfs_blue_balls_count_values)\nidx_max = max(dfs_blue_balls_count_values)\nidx_len = idx_max-idx_min\nprint(""min:"",idx_min,""max:"",idx_max)\nnum_jitter = 0\nsamplers = random.sample(range(idx_min,idx_max),idx_len)\nwhile num_jitter < 5:\n    samplers += random.sample(range(idx_min,idx_max),idx_len)\n    num_jitter += 1\n##lots of jitter effect\nprint(""samplers:"",samplers)\n#plt.plot(samplers,\'ro\',label=\'Jitter plot\')\n#plt.ylabel(\'Y-axis,number of blue balls\')\n#plt.xlabel(\'X-axis,number of duplication\')\n#plt.legend()\n#plt.show()\n#Histograms and Kernel Density Estimates:\n#Scott rule,\n#This rule assumes that the data follows a Gaussian distribution;\n\n#Plotting the blue balls appear frequency histograms(x-axis:frequency,y-axis:VIPs)\n##@see http://pandas.pydata.org/pandas-docs/dev/basics.html#value-counts-histogramming\nnum_of_bin = len(series_blue_balls_value_counts)\narray_of_ball_names = series_blue_balls_value_counts.keys()\nprint(""Blue ball names:"",array_of_ball_names)\nlist_merged_by_ball_id = []\nfor x in xrange(0,num_of_bin):\n    num_index = x+1.5\n    list_merged_by_ball_id += [num_index]*dfs_blue_balls_count_values[x]\nprint(""list_merged_by_ball_id:"",list_merged_by_ball_id)  \n##Histograms plotting\nplt.hist(list_merged_by_ball_id, bins=num_of_bin)\nplt.legend()\nplt.xlabel(\'Histograms,number of appear time by blue ball number\')\nplt.ylabel(\'Histograms,counter of appear time by blue ball number\')\nplt.show()\n###Gaussian_KDE\nfrom scipy.stats import gaussian_kde\ndensity = gaussian_kde(list_merged_by_ball_id)\nxs = numpy.linspace(0,8,200)\ndensity.covariance_factor = lambda : .25\ndensity._compute_covariance()\nplt.plot(xs,density(xs))\nplt.xlabel(\'KDE,number of appear time by blue ball number\')\nplt.ylabel(\'KDE,counter of appear time by blue ball number\')\nplt.show()\n##CDF(The Cumulative Distribution Function\nfrom scipy.stats import cumfreq\nidx_max = max(dfs_blue_balls_count_values)\nhi = idx_max\na = numpy.arange(hi) ** 2\n#    for nbins in ( 2, 20, 100 ):\nfor nbins in dfs_blue_balls_count_values:    \n    cf = cumfreq(a, nbins)  # bin values, lowerlimit, binsize, extrapoints\n    w = hi / nbins\n    x = numpy.linspace( w/2, hi - w/2, nbins )  # care\n    # print x, cf\n    plt.plot( x, cf[0], label=str(nbins) )\n\nplt.legend()\nplt.xlabel(\'CDF,number of appear time by blue ball number\')\nplt.ylabel(\'CDF,counter of appear time by blue ball number\')\nplt.show()\n\n###Optional: Comparing Distributions with Probability Plots and QQ Plots\n###Quantile plot of the server data. A quantile plot is a graph of the CDF with the x and y axes interchanged.\n###Probability plot for the data set shown,a standard normal distribution:\n###@see: http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html\nimport scipy.stats as stats\nprob_measurements = numpy.random.normal(loc = 20, scale = 5, size=num_of_bin)   \nstats.probplot(prob_measurements, dist=""norm"", plot=plt)\nplt.show()'"
Python/src/ThePitch.py,0,"b'#from IPython.display import YouTubeVideo\n#YouTubeVideo(""65FLP1ChetI"")\n#How to choose your lottery numbers ? ""Not all combinations are born equal"" [0]\n#see http://nbviewer.ipython.org/url/www.onewinner.me/en/devoxxML.ipynb\n\nfrom __future__ import print_function\n\nimport pandas as pd\nimport numpy as np\n\n# Read ""human generated"" combinations, and keep only numbers (not stars in this tutorial)\nhumandata = pd.read_csv(""OneWinnerMe.csv"", sep="";"")\nhumandata = humandata[[\'c1\', \'c2\', \'c3\', \'c4\', \'c5\']]\nhumandatalen = humandata.shape[0]\n\n# Create a set of random combinations, with the same size as the ""human generated"" dataset\nrandomdata = []\nfor i in range(humandatalen):\n    randomdata.append(sorted(np.random.choice(50,5, replace=False) + 1))\nrandomdata = np.array(randomdata)\nrandomdata = pd.DataFrame(randomdata, \n                          index = [i + humandatalen for i in range(humandatalen)], \n                          columns=[\'c1\', \'c2\', \'c3\', \'c4\', \'c5\'])\nrandomdatalen = randomdata.shape[0]\n\n# Concatenate the 2 datasets : human + random\nalldata = humandata.append(randomdata)\n\nprint(""Number of human euromillions combinations : "", humandatalen)\nprint(""Number of random euromillions combinations : "", randomdatalen)\nprint(randomdata.head(10))'"
Flex/bin-release/python/RandomNumber.py,0,"b""import random\nfrom scipy import *\n#from pylab import *\nimport pylab as pylab\n#variables\nredBalls=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32]\nblueBalls=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n#test code\n#import itertools\n#print(list(itertools.permutations(redBalls,6)))\n#print( random.random() )\nfor x in range(0,6):#NUM_OF_RED=6\n    choice_num_red = random.choice( redBalls )\n    print( choice_num_red )\n    redBalls.remove(choice_num_red)\nfor y in range(0,1):#NUM_OF_BLUE=1\n    choice_num_blue = random.choice( blueBalls )\n    print( choice_num_blue )\n#scipy test code\na = zeros(1000)\na[:100] = 1\nprint(a)\nb = fft(a)\nprint(b)\n#matplotlib test \nprint(pylab.plot(abs(b)))\n#show()\n#from matplotlib.mlab import normpdf\n#import matplotlib.numerix as nx\n#import pylab as p\n#\n#x = nx.arange(-4, 4, 0.01)\n#y = normpdf(x, 0, 1) # unit normal\n#p.plot(x,y, color='red', lw=2)\n#p.show()"""
Python/src/TimeSeries/FemaleBIrthDataset.py,0,"b'#@see: https://machinelearningmastery.com/make-predictions-time-series-forecasting-python/\nfrom pandas import Series\nfrom matplotlib import pyplot\nimport numpy as np\nfrom statsmodels.tsa.ar_model import AR\nfrom statsmodels.tsa.ar_model import ARResults\nfrom sklearn.metrics import mean_squared_error\n\nseries = Series.from_csv(\'daily-total-female-births-in-cal.csv\',header=0)\nprint(series.head())\nseries.plot()\npyplot.show()\n# pyplot.savefig(\'./FemaleBirthDataset.png\')\n\n#1. Select Time Series Forecast Model\n# value(t) = obs(t) - obs(t - 1)\n# create a difference of the dataset\ndef difference(dataset):\n    diff = list()\n    for i in range(1,len(dataset)):\n        value = dataset[i]-dataset[i-1]\n        diff.append(value)\n    return np.array(diff)\n# make prediction with regressive coefficients and lag observe value\ndef prediction(coef,history):\n    yhat = coef[0]\n    for i in range(1,len(coef)):\n        yhat += coef[i]* history[-i]\n    return yhat\n# split dataset\nX = difference(series.values)\nsize = int(len(X)*0.66)\ntrain,test = X[0:size],X[size:]\n# train autoregression model\nmodel = AR(train)\nmodel_fit = model.fit(maxlag=6,disp=False)\nwindow = model_fit.k_ar\ncoef = model_fit.params\n# walk forward over the time steps in the test\nhistory = [train[i] for i in range(len(train))]\npredictions = list()\nfor t in range(len(test)):\n    yhat = prediction(coef,history)\n    obs = test[t]\n    predictions.append(yhat)\n    history.append(obs)\nerror = mean_squared_error(test,predictions)\nprint(\'Test MSE: %.3f\' % error)\n# plot\npyplot.plot(test)\npyplot.plot(predictions, color=\'red\')\npyplot.show()\n# pyplot.savefig(\'./FemaleBirthPrediction.png\')\n\n# 2. Finalize and Save Time Series Forecast Model\n# save model to file\nmodel_fit.save(\'ar_model.pkl\')\n# save different data_set\nnp.save(\'ar_data.npy\',X)\n# save last observe value.\nnp.save(\'ar_obs.npy\',[series.values[:-1]])\n\n# load AR model\n\nloaded = ARResults.load(\'ar_model.pkl\')\nprint(loaded.params)\n# get real observation\nobservation = 48\n# load the saved data\ndata = np.load(\'ar_data.npy\')\nlast_ob = np.load(\'ar_obs.npy\')\n# update and save differenced observation\ndiffed = observation - last_ob[0]\ndata = np.append(data, [diffed], axis=0)\nnp.save(\'ar_data.npy\', data)\n# update and save real observation\nlast_ob[0] = observation\nnp.save(\'ar_obs.npy\', last_ob)\n# 3. Make a Time Series Forecast\n# predictions = model.predict(start=len(last_ob),end=len(lag),params=coef)\npredictions = prediction(coef,data)\n# transform prediction\nyhat = predictions+last_ob[0]\nprint(""transform predictions: %f"" % yhat)\n# 4. Update Forecast Model\n\n'"
Python/src/listings/ch02_lst1.py,0,"b'\nimport numpy as np\n\n# From a Python list\nvec1 = np.array( [ 0., 1., 2., 3., 4. ] )\n\n# arange( start inclusive, stop exclusive, step size )\nvec2 = np.arange( 0, 5, 1, dtype=float )\n\n# linspace( start inclusive, stop inclusive, number of elements )\nvec3 = np.linspace( 0, 4, 5 )\n\n# zeros( n ) returns a vector filled with n zeros\nvec4 = np.zeros( 5 )\nfor i in range( 5 ):\n    vec4[i] = i\n\n# read from a text file, one number per row\n#vec5 = np.loadtxt( ""data"" )\nvec5 = np.loadtxt( ""../datasets/ch02_presidents.txt"", usecols=(2,))\n\n# Add a vector to another\nv1 = vec1 + vec2\n\n# Unnecessary: adding two vectors using an explicit loop\nv2 = np.zeros( 5 )\nfor i in range( 5 ):\n    v2[i] = vec1[i] + vec2[i]\n\n# Adding a vector to another in place\nvec1 += vec2\n\n# Broadcasting: combining scalars and vectors\nv3 = 2*vec3\nv4 = vec4 + 3\n\n# Ufuncs: applying a function to a vector, element-by-element\nv5 = np.sin(vec5)\n\n# Converting to Python list object again\nlst = v5.tolist()\n'"
Python/src/listings/ch02_lst2.py,0,"b'\nfrom numpy import *\n\n# z: position, w: bandwidth, xv: vector of points\ndef kde( z, w, xv ):\n    return sum( exp(-0.5*((z-xv)/w)**2)/sqrt(2*pi*w**2) )\n\nd = loadtxt( ""../datasets/ch02_presidents.txt"", usecols=(2,) )\n\nw = 2.5\nalist = []\nfor x in linspace( min(d)-w, max(d)+w, 1000 ):\n    print x, kde( x, w, d );alist.append(kde( x, w, d ))\nprint alist\n\nimport matplotlib.pyplot as plt\ndata = [1.5]*7 + [2.5]*2 + [3.5]*8 + [4.5]*3 + [5.5]*1 + [6.5]*8\nplt.hist(data, bins=6)\nplt.show()\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import gaussian_kde\ndata = [1.5]*7 + [2.5]*2 + [3.5]*8 + [4.5]*3 + [5.5]*1 + [6.5]*8\ndensity = gaussian_kde(data)\nxs = np.linspace(0,8,200)\ndensity.covariance_factor = lambda : .25\ndensity._compute_covariance()\nplt.plot(xs,density(xs))\nplt.show()'"
Python/src/listings/ch02_lst3.py,0,"b'\nimport numpy as np\n\n# Generate two vectors with 12 elements each\nd1 = np.linspace( 0, 11, 12 )\nd2 = np.linspace( 0, 11, 12 )\n\n# Reshape the first vector to a 3x4 (row x col) matrix\nd1.shape = ( 3, 4 )\nprint d1\n\n# Generate a matrix VIEW to the second vector\nview = d2.reshape( (3,4) )\n\n# Now: possible to combine the matrix and the view\ntotal = d1 + view\n\n\n# Element access: [row,col] for matrix\nprint d1[0,1]\nprint view[0,1]\n\n# ... and [pos] for vector\nprint d2[1]\n\n\n# Shape or layout information\nprint d1.shape\nprint d2.shape\nprint view.shape\n\n# Number of elements (both commands equivalent)\nprint d1.size\nprint len(d2)\n\n# Number of dimensions (both commands equivalent)\nprint d1.ndim\nprint np.rank(d2)\n\n'"
Python/src/listings/ch02_lst4.py,0,"b'\nimport numpy as np\n\n# Create a 12 element vector and reshape into 3x4 matrix\nd = np.linspace( 0, 11, 12 )\nd.shape = ( 3,4 )\nprint d\n\n\n# Slicing...\n# First row\nprint d[0,:]\n\n# Second col\nprint d[:,1]\n\n\n# Individual element: scalar\nprint d[0,1]\n\n# Sub-vector of shape 1\nprint d[0:1,1]\n\n# Sub-array of shape 1x1\nprint d[0:1,1:2]\n\n\n# Indexing...\n# Integer indexing: third and first column\nprint d[ :, [2,0] ]\n\n# Boolean indexing: second and third column\nk = np.array( [False, True, True] )\nprint d[ k, : ]\n\n'"
Python/src/listings/ch03_lst1.py,0,"b'\nfrom pylab import *\n\n# x: location; h: bandwidth; xp, yp: data points (vectors)\ndef loess( x, h, xp, yp ):\n    w = exp( -0.5*( ((x-xp)/h)**2 )/sqrt(2*pi*h**2) )\n\n    b = sum(w*xp)*sum(w*yp) - sum(w)*sum(w*xp*yp)\n    b /= sum(w*xp)**2 - sum(w)*sum(w*xp**2)\n    a = ( sum(w*yp) - b*sum(w*xp) )/sum(w)\n\n    return a + b*x\n\nd = loadtxt( ""draftlottery"" )\n\ns1, s2 = [], []\nfor k in d[:,0]:\n    s1.append( loess( k,   5, d[:,0], d[:,1] ) )\n    s2.append( loess( k, 100, d[:,0], d[:,1] ) )\n\nxlabel( ""Day in Year"" )\nylabel( ""Draft Number"" )\n\ngca().set_aspect( \'equal\' )\n\nplot( d[:,0], d[:,1], \'o\', color=""white"", markersize=7, linewidth=3 )\nplot( d[:,0], array(s1), \'k-\', d[:,0], array(s2), \'k--\' )\n\nq = 4\naxis( [1-q, 366+q, 1-q, 366+q] )\n\nsavefig( ""draftlottery.eps"" )\n'"
Python/src/listings/ch04_lst1.py,0,"b'\nfrom scipy import *\nfrom scipy.signal import *\nfrom matplotlib.pyplot import *\n\nfilename = \'data\'\n\n\n# Read data from a text file, retaining only the third column.\n# (Column indexes start at 0.)\n# The default delimiter is any whitespace.\ndata = loadtxt( filename, comments=\'#\', delimiter=None, usecols=(2,) )\n\n# The number of points in the time series. We will need it later.\nn = data.shape[0]\n\n\n# Finding a smoothed version of the time series:\n# 1) Construct a 31-point Gaussian filter with standard deviation = 4\nfilt = gaussian( 31, 4 )\n# 2) Normalize the filter through dividing by the sum of its elements\nfilt /= sum( filt )\n# 3) Pad data on both sides with half the filter length of the last value\n#    (The function ones(k) returns a vector of length k, with all elements 1.)\npadded = concatenate( (data[0]*ones(31//2), data, data[n-1]*ones(31//2)) )\n# 4) Convolve the data with the filter. See text for the meaning of ""mode"".\nsmooth = convolve( padded, filt, mode=\'valid\' )\n\n# Plot the raw together with the smoothed data:\n# 1) Create a figure, sized to 7x5 inches\nfigure( 1, figsize=( 7, 5 ) )\n# 2) Plot the raw data in red\nplot( data, \'r\' )\n# 3) Plot the smoothed data in blue\nplot( smooth, \'b\' )\n# 4) Save the figure to file\nsavefig( filename + ""_smooth.png"" )\n# 5) Clear the figure\nclf()\n\n\n# Calculate the autocorrelation function:\n# 1) Subtract the mean \ntmp = data - mean(data)\n# 2) Pad one copy of data on the right with zeros, then form correlation fct\n#    The function zeros_like(v) creates a vector with the same dimensions\n#    as the input vector v, but with all elements zero.\ncorr = correlate( tmp, concatenate( (tmp, zeros_like(tmp)) ), mode=\'valid\' )\n# 3) Retain only some of the elements\ncorr = corr[:500]\n# 4) Normalize by dividing by the first element\ncorr /= corr[0]\n\n\n# Plot the correlation function:\nfigure( 2, figsize=( 7, 5 ) )\nplot( corr )\nsavefig( filename + ""_corr.png"" )\nclf()\n'"
Python/src/listings/ch09_lst1.py,0,"b'\nimport sys, random\n\ndef pareto( alpha ):\n    y = random.random()\n    return 1.0/pow( 1-y, 1.0/alpha )\n\n\nalpha = float( sys.argv[1] )\n\nn, ttl, mx = 0, 0, 0\n\nwhile n<1e7:\n    n += 1\n\n    v = pareto( alpha )\n\n    ttl += v\n    mx = max( mx, v )\n    \n    if( n%50000 == 0 ):\n        print n, ttl/n, mx\n'"
Python/src/listings/ch12_lst1.py,0,"b'\nimport random\n\nrepeats, tosses = 60, 8\n\ndef heads( tosses, p ):\n    h = 0\n    for x in range( 0, tosses ):\n        if random.random() < p: h += 1\n    return h\n\np = 0\nwhile p < 1.01:\n    for t in range( 0, repeats ):\n        print p, ""\\t"", heads( tosses, p )\n    p += 0.05\n'"
Python/src/listings/ch12_lst2.py,0,"b""\nimport sys\nimport random as rnd\n\nstrategy = sys.argv[1]   # must be 'stick', 'choose', or 'switch'\n\nwins = 0\nfor trial in range( 1000 ):    \n    # The prize is always in envelope 0 ... but we don't know that!\n    envelopes = [0, 1, 2]\n\n    first_choice = rnd.choice( envelopes )\n\n    if first_choice == 0:\n        envelopes = [0, rnd.choice( [1,2] ) ] # Randomly retain 1 or 2\n    else:\n        envelopes = [0, first_choice] # Retain winner and first choice\n\n    if strategy == 'stick':\n        second_choice = first_choice\n    elif strategy == 'choose':\n        second_choice = rnd.choice( envelopes )\n    elif strategy == 'switch':\n        envelopes.remove( first_choice )\n        second_choice = envelopes[0]\n\n    # Remember that the prize is in envelope 0\n    if second_choice == 0:\n        wins += 1\n\nprint wins\n"""
Python/src/listings/ch12_lst3.py,0,"b'\nimport random as rnd\n\nn = 1000    # total visitors\nk = 100     # avg visitors per day\ns = 50      # daily variation\n\ndef trial():\n    visitors_for_day = [0]  # No visitors on day 0\n\n    has_visited = [0]*n     # A flag for each visitor\n    for day in range( 31 ):\n        visitors_today = max( 0, int(rnd.gauss( k, s )) )\n\n        # Pick the individuals who visited today and mark them \n        for i in rnd.sample( range( n ), visitors_today ):\n            has_visited[i] = 1  \n\n        # Find the total number of unique visitors so far\n        visitors_for_day.append( sum(has_visited) )\n\n    return visitors_for_day\n    \n    \nfor t in range( 25 ):\n    r = trial()\n    for i in range( len(r) ):\n        print i, r[i]\n\n    print\n    print\n'"
Python/src/listings/ch12_lst4.py,0,"b'\nfrom SimPy.Simulation import *\n\nclass Customer( Process ):\n    def doit( self ):\n        print ""Arriving""\n        yield request, self, bank\n\n        print ""Being served""\n        yield hold, self, 100.0\n\n        print ""Leaving""\n        yield release, self, bank\n\n\n# Begin of main simulation program\ninitialize()\n\nbank = Resource()\n\ncust = Customer()\ncust.start( cust.doit() )\n\nsimulate( until=1000 )\n'"
Python/src/listings/ch12_lst5.py,0,"b'\nfrom SimPy.Simulation import *\nimport random as rnd\n\ninterarrival_time = 10.0\nservice_time = 8.0\n\n\nclass CustomerGenerator( Process ):\n    def produce( self, b ):\n        while True:\n            c = Customer( b )\n            c.start( c.doit() )\n            yield hold, self, rnd.expovariate(1.0/interarrival_time)\n\n\nclass Customer( Process ):\n    def __init__( self, resource ):\n        Process.__init__( self )\n        self.bank = resource\n    \n    def doit( self ):\n        yield request, self, self.bank\n        yield hold, self, self.bank.servicetime()\n        yield release, self, self.bank\n\n\nclass Bank( Resource ):\n    def servicetime( self ):\n        return rnd.expovariate(1.0/service_time)\n\n\ninitialize()\n\nbank = Bank( capacity=1, monitored=True, monitorType=Monitor )\n\nsrc = CustomerGenerator()\nactivate( src, src.produce( bank ) )\n\nsimulate( until=500 )\n\nprint bank.waitMon.mean()\nprint\n\nfor evt in bank.waitMon:\n    print evt[0], evt[1]\n'"
Python/src/listings/ch12_lst6.py,0,"b'\nfrom SimPy.Simulation import *\nimport random as rnd\n\ninterarrival_time = 10.0\n\nclass CustomerGenerator( Process ):\n    def produce( self, bank ):\n        while True:\n            c = Customer( bank, sim=self.sim )\n            c.start( c.doit() )\n            yield hold, self, rnd.expovariate(1.0/interarrival_time)\n\n\nclass Customer( Process ):\n    def __init__( self, resource, sim=None ):\n        Process.__init__( self, sim=sim )\n        self.bank = resource\n    \n    def doit( self ):\n        yield request, self, self.bank\n        yield hold, self, self.bank.servicetime()\n        yield release, self, self.bank\n\n\nclass Bank( Resource ):\n    def setServicetime( self, s ):\n        self.service_time = s\n        \n    def servicetime( self ):\n        return rnd.expovariate(1.0/self.service_time )\n\n\ndef run_simulation( t, steps, runs ):\n    for r in range( runs ):\n        sim = Simulation()\n        sim.initialize()\n        \n        bank = Bank( monitored=True, monitorType=Tally, sim=sim )\n        bank.setServicetime( t )\n\n        src = CustomerGenerator( sim=sim )\n        sim.activate( src, src.produce( bank ) )\n\n        sim.startCollection( when=steps//2 )\n        sim.simulate( until=steps )\n        \n        print t, bank.waitMon.mean()\n    \n\nt = 0\nwhile t <= 11.0:\n    t += 0.5\n    run_simulation( t, 100000, 10 )\n'"
Python/src/listings/ch13_lst1.py,0,"b'\nimport Pycluster as pc\nimport numpy as np\nimport sys\n\n# Read data filename and desired number of clusters from command line\nfilename, n = sys.argv[1], int( sys.argv[2] )\n\ndata = np.loadtxt( filename )\n\n# Perform clustering and find centroids\nclustermap, _, _ = pc.kcluster( data, nclusters=n, npass=50 )\ncentroids, _ = pc.clustercentroids( data, clusterid=clustermap )\n\n# Obtain distance matrix\nm = pc.distancematrix( data )\n\n# Find the masses of all clusters\nmass = np.zeros( n )\nfor c in clustermap:\n    mass[c] += 1\n\n# Create a matrix for individual silhouette coefficients\nsil = np.zeros( n*len(data) )\nsil.shape = ( len(data), n )\n\n# Evaluate the distance for all pairs of points\nfor i in range( 0, len(data) ):\n    for j in range( i+1, len(data) ):\n        d = m[j][i]\n\n        sil[i, clustermap[j] ] += d\n        sil[j, clustermap[i] ] += d\n\n# Normalize by cluster size (that is: form average over cluster)\nfor i in range( 0, len(data) ):\n    sil[i,:] /= mass\n\n# Evaluate the silhouette coefficient\ns = 0\nfor i in range( 0, len(data) ):\n    c = clustermap[i]\n    a = sil[i,c]\n    b = min( sil[i, range(0,c)+range(c+1,n) ] )\n    si = (b-a)/max(b,a) # This is the silhouette coeff of point i\n    s += si\n\n# Print overall silhouette coefficient\nprint n, s/len(data)\n'"
Python/src/listings/ch13_lst2.py,0,"b'\nimport Pycluster as pc\nimport numpy as np\nimport sys\n\n# Our own distance function: maximum norm\ndef dist( a, b ):\n    return max( abs( a - b ) )\n\n# Read data filename and desired number of clusters from command line\nfilename, n = sys.argv[1], int( sys.argv[2] )\n\ndata = np.loadtxt( filename )\nk = len(data)\n\n# Calculate the distance matrix\nm = np.zeros( k*k )\nm.shape = ( k, k )\n\nfor i in range( 0, k ):\n    for j in range( i, k ):\n        d = dist( data[i], data[j] )\n        m[i][j] = d\n        m[j][i] = d\n\n# Perform the actual clustering\nclustermap, _, _ = pc.kmedoids( m, n, npass=20 )\n\n# Find the indices of the points used as medoids, and the cluster masses\nmedoids = {}\nfor i in clustermap:\n    medoids[i] = medoids.get(i,0) + 1\n\n# Print points, grouped by cluster\nfor i in medoids.keys():\n    print ""Cluster="", i, "" Mass="", medoids[i], "" Centroid: "", data[i]\n\n    for j in range( 0, len(data) ):\n        if clustermap[j] == i:\n            print ""\\t"", data[j]\n'"
Python/src/listings/ch15_lst1.py,0,"b'\nimport sys, time\n\ndef permutations( v ):\n    if len(v) == 1: return [ [v[0]] ]\n\n    res = []\n    for i in range( 0, len(v) ):\n        w = permutations( v[:i] + v[i+1:] )\n        for k in w:\n            k.append( v[i] )\n        res += w\n        \n    return res\n\n\nn = int(sys.argv[1])\nv = range(n)\n\nt0 = time.clock()\nz = permutations( v )\nt1 = time.clock();\n\nprint n, t1-t0\n'"
Python/src/listings/ch16_lst1.py,0,"b'\nimport dbm\n\ndb = dbm.open( ""data.db"", \'c\' )\n\ndb[ \'abc\' ] = ""123""\ndb[ \'xyz\' ] = ""Hello, World!""\ndb[ \'42\' ] = ""42""\n    \nprint db[ \'abc\' ]\n\ndel db[ \'xyz\' ]\n\nfor k in db.keys():\n    print db[k]\n\ndb.close()\n'"
Python/src/listings/ch16_lst2.py,0,"b'\nimport sqlite3\n\n# Connect and obtain a cursor \nconn = sqlite3.connect( \'data.dbl\' )\nconn.isolation_level = None            # use autocommit!\nc = conn.cursor()\n\n\n# Create tables\nc.execute( """"""CREATE TABLE orders\n              ( id INTEGER PRIMARY KEY AUTOINCREMENT,\n                customer )"""""" )\nc.execute( """"""CREATE TABLE lineitems\n              ( id INTEGER PRIMARY KEY AUTOINCREMENT,\n                orderid, description, quantity )"""""" )\n\n# Insert values\nc.execute( ""INSERT INTO orders ( customer ) VALUES ( \'Joe Blo\' )"" )\nid = str( c.lastrowid )\nc.execute( """"""INSERT INTO lineitems ( orderid, description, quantity )\n              VALUES ( ?, \'Widget 1\', \'2\' )"""""", ( id, ) )\nc.execute( """"""INSERT INTO lineitems ( orderid, description, quantity )\n              VALUES ( ?, \'Fidget 2\', \'1\' )"""""", ( id, ) )\nc.execute( """"""INSERT INTO lineitems ( orderid, description, quantity )\n              VALUES ( ?, \'Part 17\', \'5\' )"""""", ( id, ) )\n\nc.execute( ""INSERT INTO orders ( customer ) VALUES ( \'Jane Doe\' )"" )\nid = str( c.lastrowid )\nc.execute( """"""INSERT INTO lineitems ( orderid, description, quantity )\n              VALUES ( ?, \'Fidget 2\', \'3\' )"""""", ( id, ) )\nc.execute( """"""INSERT INTO lineitems ( orderid, description, quantity )\n              VALUES ( ?, \'Part 9\', \'2\' )"""""", ( id, ) )\n\n\n# Query\nc.execute( """"""SELECT li.description FROM orders o, lineitems li\n              WHERE o.id = li.orderid AND o.customer LIKE \'%Blo\'"""""" )\nfor r in c.fetchall():\n    print r[0]\n\nc.execute( """"""SELECT orderid, sum(quantity) FROM lineitems\n              GROUP BY orderid ORDER BY orderid desc"""""" )\nfor r in c.fetchall():\n    print ""OrderID: "", r[0], ""\\tItems: "", r[1]\n\n\n# Disconnect\nconn.close()\n'"
Python/src/listings/ch17_lst1.py,0,"b'\nfrom sys import argv\nfrom random import gauss\n\nc0, c1 = 1.0, float( argv[1] )\nmu, sigma = 100, 10\nmaxtrials = 1000\n\nfor n in range( mu-5*sigma, mu+5*sigma ):\n    avg = 0\n    for trial in range( maxtrials ):\n        m = int( 0.5 + gauss( mu, sigma ) )       \n        r = c1*min( n, m ) - c0*n\n        avg += r\n        \n    print c1, n, avg/maxtrials\n'"
Python/src/listings/ch18_lst1.py,0,"b'\nfrom numpy import *\n\ntrain = loadtxt( ""iris.trn"", delimiter=\',\', usecols=(0,1,2,3) )\ntrainlabel = loadtxt( ""iris.trn"", delimiter=\',\', usecols=(4,), dtype=str )\n\ntest = loadtxt( ""iris.tst"", delimiter=\',\', usecols=(0,1,2,3) )\ntestlabel = loadtxt( ""iris.tst"", delimiter=\',\', usecols=(4,), dtype=str )\n\nhit, miss = 0, 0\nfor i in range( test.shape[0] ):\n    dist = sqrt( sum( (test[i] - train)**2, axis=1 ) )\n    k = argmin( dist )\n\n    if trainlabel[k] == testlabel[i]:\n        flag = \'+\'\n        hit += 1\n    else:\n        flag = \'-\'\n        miss += 1\n        \n    print flag, ""\\tPredicted: "", trainlabel[k], ""\\tTrue: "", testlabel[i]\n\nprint\nprint hit, ""out of"", hit+miss, ""correct - Accuracy: "", hit/(hit+miss+0.0)\n'"
Python/src/listings/ch18_lst2.py,0,"b'\ntotal = {}  # Training instances per class label\nhisto = {}  # Histogram\n\n# Read the training set and build up a histogram\ntrain = open( ""iris.trn"" )\nfor line in train:\n    # seplen, sepwid, petlen, petwid, label\n    f = line.rstrip().split( \',\' )\n    label = f.pop()\n\n    if not total.has_key( label ):\n        total[ label ] = 0\n        histo[ label ] = [ {}, {}, {}, {} ]\n\n    # Count training instances for the current label\n    total[label] += 1\n\n    # Iterate over features\n    for i in range( 4 ):\n        histo[label][i][f[i]] = 1 + histo[label][i].get( f[i], 0.0 )\n        \ntrain.close()\n\n\n# Read the test set and evaluate the probabilities\nhit, miss = 0, 0\ntest = open( ""iris.tst"" )\nfor line in test:\n    f = line.rstrip().split( \',\' )\n    true = f.pop()\n\n    p = {} # Probability for class label, given the test features\n    for label in total.keys():\n        p[label] = 1\n        for i in range( 4 ):\n            p[label] *= histo[label][i].get(f[i],0.0)/total[label]\n\n    # Find the label with the largest probability\n    mx, predicted = 0, -1\n    for k in p.keys():\n        if p[k] >= mx:\n            mx, predicted = p[k], k\n\n    if true == predicted:\n        flag = \'+\'\n        hit += 1\n    else:\n        flag = \'-\'\n        miss += 1\n        \n    print flag, ""\\t"", true, ""\\t"", predicted, ""\\t"",\n    for label in p.keys():\n        print label, "":"", p[label], ""\\t"",\n    print\n\nprint\nprint hit, ""out of"", hit+miss, ""correct - Accuracy: "", hit/(hit+miss+0.0)\n\ntest.close()\n'"
Python/src/machine-learning/BigML.py,0,"b'#@see: http://bigml.readthedocs.org/en/latest/#local-predictions\nfrom bigml.api import BigML\napi = BigML(\'smarkit\',""37b903bf765414b5e1c3164061cee5fa57e7e6ad"",storage=\'./storage\')\n\nsource = api.create_source(\'./data/red_bule_balls_2003.csv\')\napi.pprint(api.get_fields(source))\ndataset = api.create_dataset(source)\nmodel = api.create_model(dataset)\nprediction = api.create_prediction(model, {\'red\':[1,2,3,4,5,6],\'blue\':7})\n#prediction\napi.pprint(prediction)'"
Python/src/machine-learning/StatsModels.py,0,"b""import numpy as np\nimport pandas as pd\nimport statsmodels.formula.api as smf\n#Load data\nurl = './data/Guerry.csv'\ndata = pd.read_csv(url)\n#Fit regression model(using natural log of one of the regression)\nresults = smf.ols('Lottery ~ Literacy + np.log(Pop1831)', data=data).fit()\n#Inspect the results\nprint results.summary()\n#Using numpy\nimport numpy as np\nimport statsmodels.api as sm\n#Generate artifical data(2 regressors+constant)\nnobs = 100\nX = np.random.random((nobs,2))\nX = sm.add_constant(X)\nbeta = [1,.1,.5]\ne = np.random.random(nobs)\ny = np.dot(X,beta) + e\n#Fit regression model\nresults = sm.OLS(y,X).fit()\n#Inspect the results\nprint results.summary()"""
Python/src/scikit-learn/ModelPersistence.py,0,"b'from sklearn import svm\nfrom sklearn import datasets\nclf = svm.SVC()\niris = datasets.load_iris()\nX,y = iris.data,iris.target\nprint clf.fit(X, y)\nimport pickle\ns = pickle.dumps(clf)\nclf2 = pickle.loads(s)\nprint clf2.predict(X[0])\nprint y[0]'"
Python/src/scikit-learn/ModelSelection.py,0,"b""#2.2.3.1,the bigger is better\nfrom sklearn import datasets,svm\ndigits = datasets.load_digits()\nX_digits = digits.data\ny_digits = digits.target\nsvc = svm.SVC(C=1,kernel='linear')\nprint svc.fit(X_digits[:-100], y_digits[:-100]).score(X_digits[-100:],y_digits[-100:])\n#Split the dataset in folds to get better measure of prediction accuracy\nimport numpy as np\nX_folders = np.array_split(X_digits, 3)\ny_folders = np.array_split(y_digits,3)\nscores = list()\n#KFold cross validation\nfor k in range(3):\n    #We use 'list' to copy ,in order to 'pop' later on\n    X_train = list(X_folders)\n    X_test = X_train.pop(k)\n    X_train = np.concatenate(X_train)\n    y_train = list(y_folders)\n    y_test = y_train.pop(k)\n    y_train = np.concatenate(y_train)\n    scores.append(svc.fit(X_train,y_train).score(X_test,y_test))\nprint scores\n#2.2.3.2. Cross-validation generators\nfrom sklearn import cross_validation\nk_fold = cross_validation.KFold(n=6,n_folds=3,indices=True)\nfor train_indices,test_indices in k_fold:\n    print 'Train:%s | test:%s' % (train_indices,test_indices)\n#\nkfold = cross_validation.KFold(len(X_digits), n_folds=3)\nprint [svc.fit(X_digits[train], y_digits[train]).score(X_digits[test], y_digits[test]) for train, test in kfold]\n#print cross_validation.cross_val_score(svc, X_digits, y_digits, cv=kfold, n_jobs=-1)\n#2.2.3.3.1. Grid-search\nfrom sklearn.grid_search import GridSearchCV\ngammas = np.logspace(-6, -1, 10)\nclf = GridSearchCV(estimator=svc,param_grid=dict(gamma=gammas))\nprint clf.fit(X_digits[:1000], y_digits[:1000])\nprint clf.best_score_\nprint clf.best_estimator_.gamma\n#Prediction performace on test set is not as good as on train set\nprint clf.score(X_digits[1000:], y_digits[1000:])\n#2.2.3.3.2. Cross-validated estimators\nfrom sklearn import linear_model,datasets\nlasso = linear_model.LassoCV()\ndiabetes = datasets.load_diabetes()\nX_diabetes = diabetes.data\ny_diabetes = diabetes.target\nprint lasso.fit(X_diabetes,y_diabetes)\n#The estimator chose automatically its lambda:\nprint lasso.alpha_"""
Python/src/scikit-learn/Pipeline.py,0,"b""import pylab as pl\nimport numpy as np\n\nfrom sklearn import linear_model, decomposition, datasets\n\nlogistic = linear_model.LogisticRegression()\n\npca = decomposition.PCA()\nfrom sklearn.pipeline import Pipeline\npipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n\ndigits = datasets.load_digits()\nX_digits = digits.data\ny_digits = digits.target\n\n###############################################################################\n# Plot the PCA spectrum\npca.fit(X_digits)\n\npl.figure(1, figsize=(4, 3))\npl.clf()\npl.axes([.2, .2, .7, .7])\npl.plot(pca.explained_variance_, linewidth=2)\npl.axis('tight')\npl.xlabel('n_components')\npl.ylabel('explained_variance_')\n\n###############################################################################\n# Prediction\n\nfrom sklearn.grid_search import GridSearchCV\n\nn_components = [20, 40, 64]\nCs = np.logspace(-4, 4, 3)\n\n#Parameters of pipelines can be set using '__' separated parameter names:\n\nestimator = GridSearchCV(pipe,\n                         dict(pca__n_components=n_components,\n                              logistic__C=Cs))\nestimator.fit(X_digits, y_digits)\n\npl.axvline(estimator.best_estimator_.named_steps['pca'].n_components,\n           linestyle=':', label='n_components chosen')\npl.legend(prop=dict(size=12))\n\npl.show()"""
Python/src/scikit-learn/StandardKernelizedRegression.py,0,"b""import numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.svm import SVR\n\nX = np.arange(0,100)\nY = np.sin(X)\n\nsvr_rbf = SVR(kernel='rbf', C=1e5, gamma=1e5)\ny_rbf = svr_rbf.fit(X[:-10, np.newaxis], Y[:-10]).predict(X[:, np.newaxis])\n\nfigure = plt.figure()\ntick_plot = figure.add_subplot(1, 1, 1)\ntick_plot.plot(X, Y, label='data', color='green', linestyle='-')\ntick_plot.axvline(x=X[-10], alpha=0.2, color='gray')\ntick_plot.plot(X, y_rbf, label='data', color='blue', linestyle='--')\nplt.show()"""
Python/src/scikit-learn/StatisticalInference.py,0,"b'from sklearn import datasets\niris = datasets.load_iris()\ndata = iris.data\nprint data.shape\ndigits = datasets.load_digits()\nprint digits.images.shape\n#Plot it\nimport pylab as pl\npl.imshow(digits.images[-1], cmap=pl.cm.gray_r)\npl.show()\n#To use this dataset with the scikit, we transform each 8x8 image into a feature vector of length 64\ndata = digits.images.reshape((digits.images.shape[0],-1))'"
Python/src/scikit-learn/StockMarketVisualization.py,0,"b""print __doc__\n\n# Author: Gael Varoquaux gael.varoquaux@normalesup.org\n# License: BSD\n\nimport datetime\n\nimport numpy as np\nimport pylab as pl\nfrom matplotlib import finance\nfrom matplotlib.collections import LineCollection\n\nfrom sklearn import cluster, covariance, manifold\n\n###############################################################################\n# Retrieve the data from Internet\n\n# Choose a time period reasonnably calm (not too long ago so that we get\n# high-tech firms, and before the 2008 crash)\nd1 = datetime.datetime(2003, 01, 01)\nd2 = datetime.datetime(2008, 01, 01)\n\nsymbol_dict = {\n    'TOT': 'Total',\n    'XOM': 'Exxon',\n    'CVX': 'Chevron',\n    'COP': 'ConocoPhillips',\n    'VLO': 'Valero Energy',\n    'MSFT': 'Microsoft',\n    'IBM': 'IBM',\n    'TWX': 'Time Warner',\n    'CMCSA': 'Comcast',\n    'CVC': 'Cablevision',\n    'YHOO': 'Yahoo',\n    'DELL': 'Dell',\n    'HPQ': 'HP',\n    'AMZN': 'Amazon',\n    'TM': 'Toyota',\n    'CAJ': 'Canon',\n    'MTU': 'Mitsubishi',\n    'SNE': 'Sony',\n    'F': 'Ford',\n    'HMC': 'Honda',\n    'NAV': 'Navistar',\n    'NOC': 'Northrop Grumman',\n    'BA': 'Boeing',\n    'KO': 'Coca Cola',\n    'MMM': '3M',\n    'MCD': 'Mc Donalds',\n    'PEP': 'Pepsi',\n    'KFT': 'Kraft Foods',\n    'K': 'Kellogg',\n    'UN': 'Unilever',\n    'MAR': 'Marriott',\n    'PG': 'Procter Gamble',\n    'CL': 'Colgate-Palmolive',\n    'NWS': 'News Corp',\n    'GE': 'General Electrics',\n    'WFC': 'Wells Fargo',\n    'JPM': 'JPMorgan Chase',\n    'AIG': 'AIG',\n    'AXP': 'American express',\n    'BAC': 'Bank of America',\n    'GS': 'Goldman Sachs',\n    'AAPL': 'Apple',\n    'SAP': 'SAP',\n    'CSCO': 'Cisco',\n    'TXN': 'Texas instruments',\n    'XRX': 'Xerox',\n    'LMT': 'Lookheed Martin',\n    'WMT': 'Wal-Mart',\n    'WAG': 'Walgreen',\n    'HD': 'Home Depot',\n    'GSK': 'GlaxoSmithKline',\n    'PFE': 'Pfizer',\n    'SNY': 'Sanofi-Aventis',\n    'NVS': 'Novartis',\n    'KMB': 'Kimberly-Clark',\n    'R': 'Ryder',\n    'GD': 'General Dynamics',\n    'RTN': 'Raytheon',\n    'CVS': 'CVS',\n    'CAT': 'Caterpillar',\n    'DD': 'DuPont de Nemours'}\n\nsymbols, names = np.array(symbol_dict.items()).T\n\nquotes = [finance.quotes_historical_yahoo(symbol, d1, d2, asobject=True)\n          for symbol in symbols]\n\nopen = np.array([q.open for q in quotes]).astype(np.float)\nclose = np.array([q.close for q in quotes]).astype(np.float)\n\n# The daily variations of the quotes are what carry most information\nvariation = close - open\n\n###############################################################################\n# Learn a graphical structure from the correlations\nedge_model = covariance.GraphLassoCV()\n\n# standardize the time series: using correlations rather than covariance\n# is more efficient for structure recovery\nX = variation.copy().T\nX /= X.std(axis=0)\nedge_model.fit(X)\n\n###############################################################################\n# Cluster using affinity propagation\n\n_, labels = cluster.affinity_propagation(edge_model.covariance_)\nn_labels = labels.max()\n\nfor i in range(n_labels + 1):\n    print 'Cluster %i: %s' % ((i + 1), ', '.join(names[labels == i]))\n\n###############################################################################\n# Find a low-dimension embedding for visualization: find the best position of\n# the nodes (the stocks) on a 2D plane\n\n# We use a dense eigen_solver to achieve reproducibility (arpack is\n# initiated with random vectors that we don't control). In addition, we\n# use a large number of neighbors to capture the large-scale structure.\nnode_position_model = manifold.LocallyLinearEmbedding(\n    n_components=2, eigen_solver='dense', n_neighbors=6)\n\nembedding = node_position_model.fit_transform(X.T).T\n\n###############################################################################\n# Visualization\npl.figure(1, facecolor='w', figsize=(10, 8))\npl.clf()\nax = pl.axes([0., 0., 1., 1.])\npl.axis('off')\n\n# Display a graph of the partial correlations\npartial_correlations = edge_model.precision_.copy()\nd = 1 / np.sqrt(np.diag(partial_correlations))\npartial_correlations *= d\npartial_correlations *= d[:, np.newaxis]\nnon_zero = (np.abs(np.triu(partial_correlations, k=1)) > 0.02)\n\n# Plot the nodes using the coordinates of our embedding\npl.scatter(embedding[0], embedding[1], s=100 * d ** 2, c=labels,\n           cmap=pl.cm.spectral)\n\n# Plot the edges\nstart_idx, end_idx = np.where(non_zero)\n#a sequence of (*line0*, *line1*, *line2*), where::\n#            linen = (x0, y0), (x1, y1), ... (xm, ym)\nsegments = [[embedding[:, start], embedding[:, stop]]\n            for start, stop in zip(start_idx, end_idx)]\nvalues = np.abs(partial_correlations[non_zero])\nlc = LineCollection(segments,\n                    zorder=0, cmap=pl.cm.hot_r,\n                    norm=pl.Normalize(0, .7 * values.max()))\nlc.set_array(values)\nlc.set_linewidths(15 * values)\nax.add_collection(lc)\n\n# Add a label to each node. The challenge here is that we want to\n# position the labels to avoid overlap with other labels\nfor index, (name, label, (x, y)) in enumerate(\n        zip(names, labels, embedding.T)):\n\n    dx = x - embedding[0]\n    dx[index] = 1\n    dy = y - embedding[1]\n    dy[index] = 1\n    this_dx = dx[np.argmin(np.abs(dy))]\n    this_dy = dy[np.argmin(np.abs(dx))]\n    if this_dx > 0:\n        horizontalalignment = 'left'\n        x = x + .002\n    else:\n        horizontalalignment = 'right'\n        x = x - .002\n    if this_dy > 0:\n        verticalalignment = 'bottom'\n        y = y + .002\n    else:\n        verticalalignment = 'top'\n        y = y - .002\n    pl.text(x, y, name, size=10,\n            horizontalalignment=horizontalalignment,\n            verticalalignment=verticalalignment,\n            bbox=dict(facecolor='w',\n                      edgecolor=pl.cm.spectral(label / float(n_labels)),\n                      alpha=.6))\n\npl.xlim(embedding[0].min() - .15 * embedding[0].ptp(),\n        embedding[0].max() + .10 * embedding[0].ptp(),)\npl.ylim(embedding[1].min() - .03 * embedding[1].ptp(),\n        embedding[1].max() + .03 * embedding[1].ptp())\n\npl.show()"""
Python/src/scikit-learn/SupervisedLearning.py,0,"b""import numpy as np\nfrom sklearn import datasets\n\niris = datasets.load_iris()\niris_X = iris.data\niris_y = iris.target\nprint np.unique(iris_y)\n#Split the data in train and test data\n#A random permutation, to split the data randomly\nnp.random.seed(0)\nindics = np.random.permutation(len(iris_X))\niris_X_train = iris_X[indics[:-10]]\niris_y_train = iris_y[indics[:-10]]\niris_X_test = iris_X[indics[-10:]]\niris_y_test = iris_y[indics[-10:]]\n#Create and fit a nearest-neighbor classifer\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\n#Trainning\nprint knn.fit(iris_X_train,iris_y_train)\n#Testing\nprint knn.predict(iris_X_test)\n#Diabetes data prepare(age, sex, weight, blood pressure)\ndiabetes = datasets.load_diabetes()\ndiabetes_X_train = diabetes.data[:-20]\ndiabetes_X_test  = diabetes.data[-20:]\ndiabetes_y_train = diabetes.target[:-20]\ndiabetes_y_test  = diabetes.target[-20:]\nprint iris_y_test\n#Curse of dimensionality\n#2.2.2.2 Linear Model:from regression to sparsity\nfrom sklearn import linear_model\nregr = linear_model.LinearRegression()\nprint regr.fit(diabetes_X_train,diabetes_y_train)\nprint regr.coef_\n#The mean square error\nprint np.mean((regr.predict(diabetes_X_test)-diabetes_y_test)**2)\n#Explained variance score: 1 is perfect prediction\n#and o means that there is no linear relationship between X and y.\nprint regr.score(diabetes_X_test, diabetes_y_test)\n\n#Shrinkage\nX = np.c_[.5,1].T\ny = [.5,1]\ntest = np.c_[0,2].T\nregr = linear_model.LinearRegression()\n#Plot it\nimport pylab as pl\npl.figure()\n\nnp.random.seed(0)\nfor _ in range(6):\n    this_X = .1*np.random.normal(size=(2,1))+X\n    regr.fit(this_X,y)\n    pl.plot(test,regr.predict(test))\n    pl.scatter(this_X, y, s=3)\n\npl.show()\n#Ridge regression\nregr = linear_model.Ridge(alpha=.1)\npl.figure()\n\nnp.random.seed(0)\nfor _ in range(6):\n    this_X = .1*np.random.normal(size=(2,1))+X\n    regr.fit(this_X,y)\n    pl.plot(test,regr.predict(test))\n    pl.scatter(this_X,y,s=3)\npl.show()\n\n#bias/variance tradeoff\nalphas = np.logspace(-4, -1, 6)\nprint [regr.set_params(alpha=alpha).fit(diabetes_X_train,diabetes_y_train).score(diabetes_X_test,diabetes_y_test) for alpha in alphas]\n\n#Sparsity\nregr = linear_model.Lasso()\nscores = [regr.set_params(alpha=alpha).fit(diabetes_X_train, diabetes_y_train).score(diabetes_X_test, diabetes_y_test) for alpha in alphas]\nbest_alpha = alphas[scores.index(max(scores))]\nregr.alpha = best_alpha\nprint regr.fit(diabetes_X_train, diabetes_y_train)\n\nprint regr.coef_\n\n#2.2.2.4. Classification\n##LogisticRegression\nlogistic = linear_model.LogisticRegression(C=1e5)\nprint logistic.fit(iris_X_train, iris_y_train)\n\n#2.2.2.3. Support vector machines(SVMs)\n#2.2.2.3.1. Linear SVMs\n#For many estimators, including the SVMs, having datasets with unit standard deviation for each feature is important to get good prediction.\nfrom sklearn import svm\nsvc = svm.SVC(kernel='linear')\nprint svc.fit(iris_X_train, iris_y_train)\n\n"""
Python/src/scikit-learn/Tutorials.py,0,"b'from sklearn import datasets\niris = datasets.load_iris()\ndigits = datasets.load_digits()\nprint digits.data\nprint digits.target\nprint digits.images[0]\n\n\n# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>\n# License: Simplified BSD\n\n# Standard scientific Python imports\nimport pylab as pl\n\n# Import datasets, classifiers and performance metrics\nfrom sklearn import datasets, svm, metrics\n\n# The digits dataset\ndigits = datasets.load_digits()\n\n# The data that we are interested in is made of 8x8 images of digits,\n# let\'s have a look at the first 3 images, stored in the `images`\n# attribute of the dataset. If we were working from image files, we\n# could load them using pylab.imread. For these images know which\n# digit they represent: it is given in the \'target\' of the dataset.\nfor index, (image, label) in enumerate(zip(digits.images, digits.target)[:4]):\n    pl.subplot(2, 4, index + 1)\n    pl.axis(\'off\')\n    pl.imshow(image, cmap=pl.cm.gray_r, interpolation=\'nearest\')\n    pl.title(\'Training: %i\' % label)\n\n# To apply an classifier on this data, we need to flatten the image, to\n# turn the data in a (samples, feature) matrix:\nn_samples = len(digits.images)\ndata = digits.images.reshape((n_samples, -1))\n\n# Create a classifier: a support vector classifier\nclassifier = svm.SVC(gamma=0.001)\n\n# We learn the digits on the first half of the digits\nclassifier.fit(data[:n_samples / 2], digits.target[:n_samples / 2])\n\n# Now predict the value of the digit on the second half:\nexpected = digits.target[n_samples / 2:]\npredicted = classifier.predict(data[n_samples / 2:])\n\nprint ""Classification report for classifier %s:\\n%s\\n"" % (\n    classifier, metrics.classification_report(expected, predicted))\nprint ""Confusion matrix:\\n%s"" % metrics.confusion_matrix(expected, predicted)\n\nfor index, (image, prediction) in enumerate(\n        zip(digits.images[n_samples / 2:], predicted)[:4]):\n    pl.subplot(2, 4, index + 5)\n    pl.axis(\'off\')\n    pl.imshow(image, cmap=pl.cm.gray_r, interpolation=\'nearest\')\n    pl.title(\'Prediction: %i\' % prediction)\n\npl.show()'"
Python/src/scikit-learn/UnsupervisedLearning.py,0,"b'#2.2.4.1.1. K-means clustering\nimport numpy as np\nfrom sklearn import cluster,datasets\niris = datasets.load_iris()\nX_iris = iris.data\ny_iris = iris.target\n\nk_means = cluster.KMeans(n_clusters=3)\nprint k_means.fit(X_iris)\nprint k_means.labels_[::10]\nprint y_iris[::10]\n\n#Application example: vector quantization\nimport scipy as sp\ntry:\n    lena = sp.lena()\nexcept AttributeError:\n    from scipy import misc\n    lena = misc.lena()\nX = lena.reshape((-1,1))\nk_means = cluster.KMeans(n_clusters=5,n_init=1)\nprint k_means.fit(X)\nvalues = k_means.cluster_centers_.squeeze()\nlabels = k_means.labels_\nlena_compressed = np.choose(labels,values)\nlena_compressed.shape = lena.shape\n#2.2.4.1.2. Hierarchical agglomerative clustering: Ward\n#2.2.4.2.1. Principal component analysis: PCA\n#Create a signal with only 2 useful dimensions\\\nx1 = np.random.normal(size=100)\nx2 = np.random.normal(size=100)\nx3 = x1 + x2\nX = np.c_[x1,x2,x3]\n#\nfrom sklearn import decomposition\npca = decomposition.PCA()\nprint pca.fit(X)\nprint pca.explained_variance_\n#\npca.n_components = 2\nX_reduced = pca.fit_transform(X)\nprint X_reduced.shape\n#ICA\n# Generate sample data\ntime = np.linspace(0, 10, 2000)\ns1 = np.sin(2 * time)  # Signal 1 : sinusoidal signal\ns2 = np.sign(np.sin(3 * time))  # Signal 2 : square signal\nS = np.c_[s1, s2]\nS += 0.2 * np.random.normal(size=S.shape)  # Add noise\nS /= S.std(axis=0)  # Standardize data\n# Mix data\nA = np.array([[1, 1], [0.5, 2]])  # Mixing matrix\nX = np.dot(S, A.T)  # Generate observations\n\n# Compute ICA\nica = decomposition.FastICA()\nS_ = ica.fit(X).transform(X)  # Get the estimated sources\nA_ = ica.get_mixing_matrix()  # Get estimated mixing matrix\nprint np.allclose(X, np.dot(S_, A_.T))'"
Python/src/tempodb/tempodb-read-demo.py,0,"b'""""""\nhttp://tempo-db.com/api/read-series/#read-series-by-key\n""""""\n\nimport datetime\nfrom tempodb import Client\n\n# Modify these with your settings found at: http://tempo-db.com/manage/\nAPI_KEY = \'680d1adbbb0c42a398b5846c8e1517dd\'\nAPI_SECRET = \'f2db65d178634a36b4c25467f8bc2099\'\nSERIES_KEY = \'your-custom-key\'\n\nclient = Client(API_KEY, API_SECRET)\n\nstart = datetime.date(2012, 1, 1)\nend = start + datetime.timedelta(days=1)\n\ndata = client.read_key(SERIES_KEY, start, end)\n\nfor datapoint in data.data:\n    print datapoint'"
Python/src/tempodb/tempodb-write-demo.py,0,"b'""""""\nhttp://tempo-db.com/api/write-series/#write-series-by-key\n""""""\n\nimport datetime\nimport random\nfrom tempodb import Client, DataPoint\n\n# Modify these with your credentials found at: http://tempo-db.com/manage/\nAPI_KEY = \'680d1adbbb0c42a398b5846c8e1517dd\'\nAPI_SECRET = \'f2db65d178634a36b4c25467f8bc2099\'\nSERIES_KEY = \'your-custom-key\'\n\nclient = Client(API_KEY, API_SECRET)\n\ndate = datetime.datetime(2012, 1, 1)\n\nfor day in range(1, 10):\n    # print out the current day we are sending data for\n    print date\n\n    data = []\n    # 1440 minutes in one day\n    for min in range (1, 1441):\n        data.append(DataPoint(date, random.random() * 50.0))\n        date = date + datetime.timedelta(minutes=1)\n\n    client.write_key(SERIES_KEY, data)'"
