file_path,api_count,code
__init__.py,0,b''
configs/__init__.py,0,b''
configs/config_inception_resnet.py,15,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n######################\n#    data set\n####################\n\ntf.app.flags.DEFINE_string(\n    \'dataset_tfrecord\',\n    \'../data/tfrecords\',\n    \'tfrecord of fruits dataset\'\n)\ntf.app.flags.DEFINE_integer(\n    \'new_img_size\',\n    224,\n    \'the value of new height and new width, new_height = new_width\'\n)\n\n###########################\n#  data batch\n##########################\ntf.app.flags.DEFINE_integer(\n    \'num_classes\',\n    100,\n    \'num of classes\'\n)\ntf.app.flags.DEFINE_integer(\n    \'batch_size\',\n    32, #64\n    \'num of imgs in a batch\'\n)\ntf.app.flags.DEFINE_integer(\n    \'val_batch_size\',\n    16,\n    \'val or test batch\'\n)\n###########################\n## learning rate\n#########################\ntf.app.flags.DEFINE_float(\n    \'lr_begin\',\n    0.0001, # 0.01 # 0.001 for without prepocess\n    \'the value of learning rate start with\'\n)\ntf.app.flags.DEFINE_integer(\n    \'decay_steps\',\n    2000, # 5000\n    ""after \'decay_steps\' steps, learning rate begin decay""\n)\ntf.app.flags.DEFINE_float(\n    \'decay_rate\',\n    0.1,\n    \'decay rate\'\n)\n\n###############################\n# optimizer-- MomentumOptimizer\n###############################\ntf.app.flags.DEFINE_float(\n    \'momentum\',\n    0.9,\n    \'accumulation = momentum * accumulation + gradient\'\n)\n\n############################\n#  train\n########################\ntf.app.flags.DEFINE_integer(\n    \'max_steps\',\n    30050,\n    \'max iterate steps\'\n)\n\ntf.app.flags.DEFINE_string(\n    \'pretrained_model_path\',\n    \'../data/pretrained_weights/inception_resnet_v2_2016_08_30.ckpt\',\n    \'the path of pretrained weights\'\n)\ntf.app.flags.DEFINE_float(\n    \'weight_decay\',\n    0.00004,\n    \'weight_decay in regulation\'\n)\n################################\n# summary and save_weights_checkpoint\n##################################\ntf.app.flags.DEFINE_string(\n    \'summary_path\',\n    \'../output/inception_res_summary\',\n    \'the path of summary write to \'\n)\ntf.app.flags.DEFINE_string(\n    \'trained_checkpoint\',\n    \'../output/inception_res_trainedweights\',\n    \'the path to save trained_weights\'\n)\nFLAGS = tf.app.flags.FLAGS'"
configs/config_res101.py,15,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n######################\n#    data set\n####################\ntf.app.flags.DEFINE_string(\n    \'dataset_tfrecord\',\n    \'../data/tfrecords\',\n    \'tfrecord of fruits dataset\'\n)\ntf.app.flags.DEFINE_integer(\n    \'new_img_size\',\n    224,\n    \'the value of new height and new width, new_height = new_width\'\n)\n\n###########################\n#  data batch\n##########################\ntf.app.flags.DEFINE_integer(\n    \'num_classes\',\n    134,\n    \'num of classes\'\n)\ntf.app.flags.DEFINE_integer(\n    \'batch_size\',\n    64, #64\n    \'num of imgs in a batch\'\n)\ntf.app.flags.DEFINE_integer(\n    \'val_batch_size\',\n    32,\n    \'val or test batch\'\n)\n###########################\n## learning rate\n#########################\ntf.app.flags.DEFINE_float(\n    \'lr_begin\',\n    0.001, # 0.01 # 0.001 for without prepocess\n    \'the value of learning rate start with\'\n)\ntf.app.flags.DEFINE_integer(\n    \'decay_steps\',\n    20000, # 5000\n    ""after \'decay_steps\' steps, learning rate begin decay""\n)\ntf.app.flags.DEFINE_float(\n    \'decay_rate\',\n    0.1,\n    \'decay rate\'\n)\n\n###############################\n# optimizer-- MomentumOptimizer\n###############################\ntf.app.flags.DEFINE_float(\n    \'momentum\',\n    0.9,\n    \'accumulation = momentum * accumulation + gradient\'\n)\n\n############################\n#  train\n########################\ntf.app.flags.DEFINE_integer(\n    \'max_steps\',\n    4003,\n    \'max iterate steps\'\n)\n\ntf.app.flags.DEFINE_string(\n    \'pretrained_model_path\',\n    \'../data/pretrained_weights/resnet_v1_101.ckpt\',\n    \'the path of pretrained weights\'\n)\ntf.app.flags.DEFINE_float(\n    \'weight_decay\',\n    0.0001,\n    \'weight_decay in regulation\'\n)\n################################\n# summary and save_weights_checkpoint\n##################################\ntf.app.flags.DEFINE_string(\n    \'summary_path\',\n    \'../output/res101_summary\',\n    \'the path of summary write to \'\n)\ntf.app.flags.DEFINE_string(\n    \'trained_checkpoint\',\n    \'../output/res101_trained_weights\',\n    \'the path to save trained_weights\'\n)\nFLAGS = tf.app.flags.FLAGS'"
configs/config_resnet_50.py,11,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\ntf.app.flags.DEFINE_string(\n    'dataset_tfrecord',\n    '../data/tfrecords',\n    'tfrecord of fruits dataset'\n)\ntf.app.flags.DEFINE_integer(\n    'shortside_size',\n    600,\n    'the value of new height and new width, new_height = new_width'\n)\n\n###########################\n#  data batch\n##########################\ntf.app.flags.DEFINE_integer(\n    'num_classes',\n    20,\n    'num of classes'\n)\ntf.app.flags.DEFINE_integer(\n    'batch_size',\n    1, #64\n    'num of imgs in a batch'\n)\n\n###############################\n# optimizer-- MomentumOptimizer\n###############################\ntf.app.flags.DEFINE_float(\n    'momentum',\n    0.9,\n    'accumulation = momentum * accumulation + gradient'\n)\n\n############################\n#  train\n########################\ntf.app.flags.DEFINE_integer(\n    'max_steps',\n    900000,\n    'max iterate steps'\n)\n\ntf.app.flags.DEFINE_string(\n    'pretrained_model_path',\n    '../data/pretrained_weights/resnet_50.ckpt',\n    'the path of pretrained weights'\n)\ntf.app.flags.DEFINE_float(\n    'weight_decay',\n    0.0001,\n    'weight_decay in regulation'\n)\n################################\n# summary and save_weights_checkpoint\n##################################\ntf.app.flags.DEFINE_string(\n    'summary_path',\n    '../output/resnet_summary',\n    'the path of summary write to '\n)\ntf.app.flags.DEFINE_string(\n    'trained_checkpoint',\n    '../output/resnet_trained_weights',\n    'the path to save trained_weights'\n)\nFLAGS = tf.app.flags.FLAGS"""
configs/config_vgg16.py,15,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n######################\n#    data set\n####################\n\ntf.app.flags.DEFINE_string(\n    \'dataset_tfrecord\',\n    \'../data/tfrecords\',\n    \'tfrecord of dog dataset\'\n)\ntf.app.flags.DEFINE_integer(\n    \'new_img_size\',\n    224,\n    \'the value of new height and new width, new_height = new_width\'\n)\n\n###########################\n#  data batch\n##########################\ntf.app.flags.DEFINE_integer(\n    \'num_classes\',\n    134,\n    \'num of classes\'\n)\ntf.app.flags.DEFINE_integer(\n    \'batch_size\',\n    32, #64\n    \'num of imgs in a batch\'\n)\ntf.app.flags.DEFINE_integer(\n    \'val_batch_size\',\n    8,\n    \'val or test batch\'\n)\n###########################\n## learning rate\n#########################\ntf.app.flags.DEFINE_float(\n    \'lr_begin\',\n    0.001, # 0.01 # 0.001 for without prepocess # 0.01 for inception\n    \'the value of learning rate start with\'\n)\ntf.app.flags.DEFINE_integer(\n    \'decay_steps\',\n    3000, # 5000\n    ""after \'decay_steps\' steps, learning rate begin decay""\n)\ntf.app.flags.DEFINE_float(\n    \'decay_rate\',\n    0.1,\n    \'decay rate\'\n)\n\n###############################\n# optimizer-- MomentumOptimizer\n###############################\ntf.app.flags.DEFINE_float(\n    \'momentum\',\n    0.9,\n    \'accumulation = momentum * accumulation + gradient\'\n)\n\n############################\n#  train\n########################\ntf.app.flags.DEFINE_integer(\n    \'max_steps\',\n    20010,\n    \'max iterate steps\'\n)\n\ntf.app.flags.DEFINE_string(\n    \'pretrained_model_path\',\n    \'../data/pretrained_weights/vgg_16.ckpt\',\n    \'the path of pretrained weights\'\n)\ntf.app.flags.DEFINE_float(\n    \'weight_decay\',\n    0.0005,\n    \'weight_decay in regulation\'\n)\n################################\n# summary and save_weights_checkpoint\n##################################\ntf.app.flags.DEFINE_string(\n    \'summary_path\',\n    \'../output/vgg16_summary\',\n    \'the path of summary write to \'\n)\ntf.app.flags.DEFINE_string(\n    \'trained_checkpoint\',\n    \'../output/vgg16_trainedweights\',\n    \'the path to save trained_weights\'\n)\nFLAGS = tf.app.flags.FLAGS'"
data/__init__.py,0,b''
help_utils/__init__.py,0,b''
help_utils/help_utils.py,5,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\nimport numpy as np\nimport cv2\nfrom libs.label_name_dict.label_dict import LABEl_NAME_MAP\n\n\ndef show_boxes_in_img(img, boxes_and_label):\n    \'\'\'\n\n    :param img:\n    :param boxes: must be int\n    :return:\n    \'\'\'\n    boxes_and_label = boxes_and_label.astype(np.int64)\n    img = np.array(img, np.float32)\n    img = np.array(img*255/np.max(img), np.uint8)\n    for box in boxes_and_label:\n        ymin, xmin, ymax, xmax, label = box[0], box[1], box[2], box[3], box[4]\n\n        category = LABEl_NAME_MAP[label]\n\n        color = (np.random.randint(255), np.random.randint(255), np.random.randint(255))\n        cv2.rectangle(img,\n                      pt1=(xmin, ymin),\n                      pt2=(xmax, ymax),\n                      color=color)\n        cv2.putText(img,\n                    text=category,\n                    org=((xmin+xmax)//2, (ymin+ymax)//2),\n                    fontFace=1,\n                    fontScale=1,\n                    color=(0, 0, 255))\n\n    cv2.imshow(\'img_\', img)\n    cv2.waitKey(0)\n\n\ndef draw_box_cv(img, boxes, labels, scores):\n    img = img + np.array([103.939, 116.779, 123.68])\n    boxes = boxes.astype(np.int64)\n    labels = labels.astype(np.int32)\n    img = np.array(img, np.float32)\n    img = np.array(img*255/np.max(img), np.uint8)\n\n    num_of_object = 0\n    for i, box in enumerate(boxes):\n        ymin, xmin, ymax, xmax = box[0], box[1], box[2], box[3]\n\n        label = labels[i]\n        if label != 0:\n            num_of_object += 1\n            color = (np.random.randint(255), np.random.randint(255), np.random.randint(255))\n            cv2.rectangle(img,\n                          pt1=(xmin, ymin),\n                          pt2=(xmax, ymax),\n                          color=color,\n                          thickness=2)\n\n            category = LABEl_NAME_MAP[label]\n\n            if scores is not None:\n                cv2.rectangle(img,\n                              pt1=(xmin, ymin),\n                              pt2=(xmin+150, ymin+15),\n                              color=color,\n                              thickness=-1)\n                cv2.putText(img,\n                            text=category+"": ""+str(scores[i]),\n                            org=(xmin, ymin+10),\n                            fontFace=1,\n                            fontScale=1,\n                            thickness=2,\n                            color=(color[1], color[2], color[0]))\n            else:\n                cv2.rectangle(img,\n                              pt1=(xmin, ymin),\n                              pt2=(xmin + 40, ymin + 15),\n                              color=color,\n                              thickness=-1)\n                cv2.putText(img,\n                            text=category,\n                            org=(xmin, ymin + 10),\n                            fontFace=1,\n                            fontScale=1,\n                            thickness=2,\n                            color=(color[1], color[2], color[0]))\n    cv2.putText(img,\n                text=str(num_of_object),\n                org=((img.shape[1]) // 2, (img.shape[0]) // 2),\n                fontFace=3,\n                fontScale=1,\n                color=(255, 0, 0))\n    return img\n\n\ndef draw_rotate_box_cv(img, boxes, labels, scores):\n    img = img + np.array([103.939, 116.779, 123.68])\n    boxes = boxes.astype(np.int64)\n    labels = labels.astype(np.int32)\n    img = np.array(img, np.float32)\n    img = np.array(img*255/np.max(img), np.uint8)\n\n    num_of_object = 0\n    for i, box in enumerate(boxes):\n        y_c, x_c, h, w, theta = box[0], box[1], box[2], box[3], box[4]\n\n        label = labels[i]\n        if label != 0:\n            num_of_object += 1\n            color = (np.random.randint(255), np.random.randint(255), np.random.randint(255))\n            rect = ((x_c, y_c), (w, h), theta)\n            rect = cv2.boxPoints(rect)\n            rect = np.int0(rect)\n            cv2.drawContours(img, [rect], -1, color, 3)\n\n            category = LABEl_NAME_MAP[label]\n\n            if scores is not None:\n                cv2.rectangle(img,\n                              pt1=(x_c, y_c),\n                              pt2=(x_c + 120, y_c + 15),\n                              color=color,\n                              thickness=-1)\n                cv2.putText(img,\n                            text=category+"": ""+str(scores[i]),\n                            org=(x_c, y_c+10),\n                            fontFace=1,\n                            fontScale=1,\n                            thickness=2,\n                            color=(color[1], color[2], color[0]))\n            else:\n                cv2.rectangle(img,\n                              pt1=(x_c, y_c),\n                              pt2=(x_c + 40, y_c + 15),\n                              color=color,\n                              thickness=-1)\n                cv2.putText(img,\n                            text=category,\n                            org=(x_c, y_c + 10),\n                            fontFace=1,\n                            fontScale=1,\n                            thickness=2,\n                            color=(color[1], color[2], color[0]))\n    cv2.putText(img,\n                text=str(num_of_object),\n                org=((img.shape[1]) // 2, (img.shape[0]) // 2),\n                fontFace=3,\n                fontScale=1,\n                color=(255, 0, 0))\n    return img\n\n\ndef print_tensors(tensor, tensor_name):\n\n    def np_print(ary):\n        ary = ary + np.zeros_like(ary)\n        print(tensor_name + \':\', ary)\n\n        print(\'shape is: \',ary.shape)\n        print(10*""%%%%%"")\n        return ary\n    result = tf.py_func(np_print,\n                        [tensor],\n                        [tensor.dtype])\n    result = tf.reshape(result, tf.shape(tensor))\n    result = tf.cast(result, tf.float32)\n    sum_ = tf.reduce_sum(result)\n    tf.summary.scalar(\'print_s/{}\'.format(tensor_name), sum_)\n'"
help_utils/tools.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport math\nimport sys\nimport os\n\n\ndef view_bar(message, num, total):\n    rate = num / total\n    rate_num = int(rate * 40)\n    rate_nums = math.ceil(rate * 100)\n    r = \'\\r%s:[%s%s]%d%%\\t%d/%d\' % (message, "">"" * rate_num, "" "" * (40 - rate_num), rate_nums, num, total,)\n    sys.stdout.write(r)\n    sys.stdout.flush()\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)'"
libs/__init__.py,0,b''
tools/__init__.py,0,b''
tools/demo.py,11,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\n\nimport sys\nsys.path.append(\'../\')\nimport random\nimport matplotlib.pyplot as plt\n# from osgeo import gdal, gdalconst\nimport xml.dom.minidom\nimport time\nfrom timeit import default_timer as timer\nimport cv2\nfrom data.io import image_preprocess\nfrom libs.networks.network_factory import get_network_byname\nfrom libs.rpn import build_rpn\nfrom libs.fast_rcnn import build_fast_rcnn\nfrom tools import restore_model\nfrom libs.configs import cfgs\nfrom help_utils.tools import *\nfrom help_utils.help_utils import *\nimport argparse\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""1""\n\n\ndef chw2hwc(img):\n    """""" Convert image data from [channel, height, weight] to [height, weight, channel],\n    and the origianl image should be [channel, height, weight]\n    :param img:\n    :return:\n    """"""\n    res = np.swapaxes(img, 0, 2) # w,h,c\n    res = np.swapaxes(res, 0, 1) # h,w,c\n    return res\n\n\ndef get_file_paths_recursive(folder=None, file_ext=None):\n    """""" Get the absolute path of all files in given folder recursively\n    :param folder:\n    :param file_ext:\n    :return:\n    """"""\n    file_list = []\n    if folder is None:\n        return file_list\n\n    for dir_path, dir_names, file_names in os.walk(folder):\n        for file_name in file_names:\n            if file_ext is None:\n                file_list.append(os.path.join(dir_path, file_name))\n                continue\n            if file_name.endswith(file_ext):\n                file_list.append(os.path.join(dir_path, file_name))\n    return file_list\n\n\ndef visualize_detection(src_img, boxes, scores):\n    """""" visualize detections in one image\n    :param src_img: numpy.array\n    :param boxes: [[x1, y1, x2, y2]...], each row is one object\n    :param class_names: class names, and each row is one object\n    :param scores: score for each object (each row)\n    :return:\n    """"""\n    plt.imshow(src_img)\n    box_num = len(boxes)\n    color = (1.0, 0.0, 0.0)\n    for i in range(box_num):\n        box = boxes[i]\n        rect = plt.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], fill=False,\n                             edgecolor=color, linewidth=1.5)\n        plt.gca().add_patch(rect)\n        plt.gca().text(box[0], box[1] - 2, \'{:.3f}\'.format(scores[i]),\n                       bbox=dict(facecolor=color, alpha=0.5), fontsize=6, color=\'white\')\n    plt.show()\n\n\n# def obj_to_det_xml(img_path, box_res, label_res, score_res, det_xml_path):\n#     """""" Save detection results to det.xml\n#     :param img_path:\n#     :param box_res:\n#     :param label_res:\n#     :param score_res:\n#     :param det_xml_path:\n#     :return:\n#     """"""\n#     gdal.AllRegister()\n#     ds = gdal.Open(img_path, gdalconst.GA_Update)\n#     if ds is None:\n#         print(""Image %s open failed!"" % img_path)\n#         sys.exit()\n#     proj_str = ds.GetProjection()\n#     geoTf = ds.GetGeoTransform()\n#\n#     obj_n = len(box_res)\n#\n#     doc = xml.dom.minidom.Document()\n#     root_node = doc.createElement(""ImageInfo"")\n#     root_node.setAttribute(""resolution"", str(geoTf[1]))\n#     root_node.setAttribute(""imagingtime"", time.strftime(\'%Y-%m-%dT%H:%M:%S\'))\n#     doc.appendChild(root_node)\n#\n#     BaseInfo_node = doc.createElement(""BaseInfo"")\n#     BaseInfo_node.setAttribute(""description"", "" "")\n#     BaseInfo_node.setAttribute(""ID"", "" "")\n#     BaseInfo_node.setAttribute(""name"", ""sewage"")\n#     root_node.appendChild(BaseInfo_node)\n#\n#     result_node = doc.createElement(""result"")\n#     DetectNumber_node = doc.createElement(""DetectNumber"")\n#     DetectNumber_value = doc.createTextNode(str(obj_n))\n#     DetectNumber_node.appendChild(DetectNumber_value)\n#     result_node.appendChild(DetectNumber_node)\n#\n#     for ii in range(obj_n):\n#         box = box_res[ii]\n#         xmin = geoTf[0] + geoTf[1] * box[1]\n#         ymin = geoTf[3] + geoTf[5] * box[0]\n#         xmax = geoTf[0] + geoTf[1] * box[3]\n#         ymax = geoTf[3] + geoTf[5] * box[2]\n#\n#         DetectResult_node = doc.createElement(""DetectResult"")\n#\n#         ResultID_node = doc.createElement(""ResultID"")\n#         ResultID_value = doc.createTextNode(str(ii))\n#         ResultID_node.appendChild(ResultID_value)\n#\n#         Shape_node = doc.createElement(""Shape"")\n#         Point1_node = doc.createElement(""Point"")\n#         Point1_value = doc.createTextNode(""%.6f, %.6f"" % (xmin, ymin))\n#         Point1_node.appendChild(Point1_value)\n#\n#         Point2_node = doc.createElement(""Point"")\n#         Point2_value = doc.createTextNode(""%.6f, %.6f"" % (xmax, ymin))\n#         Point2_node.appendChild(Point2_value)\n#\n#         Point3_node = doc.createElement(""Point"")\n#         Point3_value = doc.createTextNode(""%.6f, %.6f"" % (xmax, ymax))\n#         Point3_node.appendChild(Point3_value)\n#\n#         Point4_node = doc.createElement(""Point"")\n#         Point4_value = doc.createTextNode(""%.6f, %.6f"" % (xmin, ymax))\n#         Point4_node.appendChild(Point4_value)\n#\n#         Point5_node = doc.createElement(""Point"")\n#         Point5_value = doc.createTextNode(""%.6f, %.6f"" % (xmin, ymin))\n#         Point5_node.appendChild(Point5_value)\n#\n#         Shape_node.appendChild(Point1_node)\n#         Shape_node.appendChild(Point2_node)\n#         Shape_node.appendChild(Point3_node)\n#         Shape_node.appendChild(Point4_node)\n#         Shape_node.appendChild(Point5_node)\n#\n#         Location_node = doc.createElement(""Location"")\n#         Location_value = doc.createTextNode(""unknown"")\n#         Location_node.appendChild(Location_value)\n#\n#         CenterLonLat_node = doc.createElement(""CenterLonLat"")\n#         CenterLonLat_value = doc.createTextNode(""0.000000, 0.000000"")\n#         CenterLonLat_node.appendChild(CenterLonLat_value)\n#\n#         Length_node = doc.createElement(""Length"")\n#         Length_value = doc.createTextNode(""0"")\n#         Length_node.appendChild(Length_value)\n#\n#         Width_node = doc.createElement(""Width"")\n#         Width_value = doc.createTextNode(""0"")\n#         Width_node.appendChild(Width_value)\n#\n#         Area_node = doc.createElement(""Area"")\n#         Area_value = doc.createTextNode(""0"")\n#         Area_node.appendChild(Area_value)\n#\n#         Angle_node = doc.createElement(""Angle"")\n#         Angle_value = doc.createTextNode(""0"")\n#         Angle_node.appendChild(Angle_value)\n#\n#         Probability_node = doc.createElement(""Probability"")\n#         Probability_value = doc.createTextNode(""1.0"")\n#         Probability_node.appendChild(Probability_value)\n#\n#         ResultImagePath_node = doc.createElement(""ResultImagePath"")\n#         ResultImagePath_value = doc.createTextNode("" "")\n#         ResultImagePath_node.appendChild(ResultImagePath_value)\n#\n#         ValidationName_node = doc.createElement(""ValidationName"")\n#         ValidationName_value = doc.createTextNode("" "")\n#         ValidationName_node.appendChild(ValidationName_value)\n#\n#         PossibleResults_node = doc.createElement(""PossibleResults"")\n#\n#         Type_node = doc.createElement(""Type"")\n#         Type_value = doc.createTextNode(""%s"" % label_res[ii])\n#         Type_node.appendChild(Type_value)\n#\n#         Reliability_node = doc.createElement(""Reliability"")\n#         Reliability_value = doc.createTextNode(""%.3f"" % score_res[ii])\n#         Reliability_node.appendChild(Reliability_value)\n#\n#         PossibleResults_node.appendChild(Type_node)\n#         PossibleResults_node.appendChild(Reliability_node)\n#\n#         DetectResult_node.appendChild(ResultID_node)\n#         DetectResult_node.appendChild(Shape_node)\n#         DetectResult_node.appendChild(Location_node)\n#         DetectResult_node.appendChild(CenterLonLat_node)\n#         DetectResult_node.appendChild(Length_node)\n#         DetectResult_node.appendChild(Width_node)\n#         DetectResult_node.appendChild(Area_node)\n#         DetectResult_node.appendChild(Angle_node)\n#         DetectResult_node.appendChild(Probability_node)\n#         DetectResult_node.appendChild(ResultImagePath_node)\n#         DetectResult_node.appendChild(ValidationName_node)\n#         DetectResult_node.appendChild(PossibleResults_node)\n#\n#         result_node.appendChild(DetectResult_node)\n#     root_node.appendChild(result_node)\n#\n#     with open(det_xml_path, ""w+"") as f:\n#         f.write(doc.toprettyxml(indent=""\\t"", newl=""\\n"", encoding=""utf-8""))\n\n\ndef clip_obj_imgs(src_img, boxes, classes, scores, des_folder):\n    """""" Clip image by target information\n    :param src_img:\n    :param boxes:\n    :param classes:\n    :param scores:\n    :param des_folder:\n    :return:\n    """"""\n    box_num = len(boxes)\n    ii = 0\n    off_size = 20\n    img_height = src_img.shape[0]\n    img_width = src_img.shape[1]\n\n    while ii < box_num:\n        box = boxes[ii]\n        xpos = max(box[0] - off_size, 0)\n        ypos = max(box[1] - off_size, 0)\n        clip_w = min(box[2]-box[0]+2*off_size, img_width-xpos)\n        clip_h = min(box[3]-box[1]+2*off_size, img_height-ypos)\n        img = np.zeros((clip_h, clip_w, 3))\n        img[0:clip_h, 0:clip_w, :] = src_img[ypos:ypos+clip_h, xpos:xpos+clip_w, :]\n        #plt.imshow(img)\n        #plt.show()\n        clip_path = os.path.join(des_folder, \'%s-%d_%.3f.jpg\' % (classes[ii], ii, scores[ii]))\n        cv2.imwrite(clip_path, img)\n        ii = ii + 1\n\n\ndef detect_img(file_paths, des_folder, det_th, h_len, w_len, show_res=False):\n    with tf.Graph().as_default():\n\n        img_plac = tf.placeholder(shape=[None, None, 3], dtype=tf.uint8)\n\n        img_tensor = tf.cast(img_plac, tf.float32) - tf.constant([103.939, 116.779, 123.68])\n        img_batch = image_preprocess.short_side_resize_for_inference_data(img_tensor,\n                                                                          target_shortside_len=cfgs.SHORT_SIDE_LEN,\n                                                                          is_resize=False)\n\n        # ***********************************************************************************************\n        # *                                         share net                                           *\n        # ***********************************************************************************************\n        _, share_net = get_network_byname(net_name=cfgs.NET_NAME,\n                                          inputs=img_batch,\n                                          num_classes=None,\n                                          is_training=True,\n                                          output_stride=None,\n                                          global_pool=False,\n                                          spatial_squeeze=False)\n        # ***********************************************************************************************\n        # *                                            RPN                                              *\n        # ***********************************************************************************************\n        rpn = build_rpn.RPN(net_name=cfgs.NET_NAME,\n                            inputs=img_batch,\n                            gtboxes_and_label=None,\n                            is_training=False,\n                            share_head=cfgs.SHARE_HEAD,\n                            share_net=share_net,\n                            stride=cfgs.STRIDE,\n                            anchor_ratios=cfgs.ANCHOR_RATIOS,\n                            anchor_scales=cfgs.ANCHOR_SCALES,\n                            scale_factors=cfgs.SCALE_FACTORS,\n                            base_anchor_size_list=cfgs.BASE_ANCHOR_SIZE_LIST,  # P2, P3, P4, P5, P6\n                            level=cfgs.LEVEL,\n                            top_k_nms=cfgs.RPN_TOP_K_NMS,\n                            rpn_nms_iou_threshold=cfgs.RPN_NMS_IOU_THRESHOLD,\n                            max_proposals_num=cfgs.MAX_PROPOSAL_NUM,\n                            rpn_iou_positive_threshold=cfgs.RPN_IOU_POSITIVE_THRESHOLD,\n                            rpn_iou_negative_threshold=cfgs.RPN_IOU_NEGATIVE_THRESHOLD,\n                            rpn_mini_batch_size=cfgs.RPN_MINIBATCH_SIZE,\n                            rpn_positives_ratio=cfgs.RPN_POSITIVE_RATE,\n                            remove_outside_anchors=False,  # whether remove anchors outside\n                            rpn_weight_decay=cfgs.WEIGHT_DECAY[cfgs.NET_NAME])\n\n        # rpn predict proposals\n        rpn_proposals_boxes, rpn_proposals_scores = rpn.rpn_proposals()  # rpn_score shape: [300, ]\n\n        # ***********************************************************************************************\n        # *                                         Fast RCNN                                           *\n        # ***********************************************************************************************\n        fast_rcnn = build_fast_rcnn.FastRCNN(feature_pyramid=rpn.feature_pyramid,\n                                             rpn_proposals_boxes=rpn_proposals_boxes,\n                                             rpn_proposals_scores=rpn_proposals_scores,\n                                             img_shape=tf.shape(img_batch),\n                                             roi_size=cfgs.ROI_SIZE,\n                                             roi_pool_kernel_size=cfgs.ROI_POOL_KERNEL_SIZE,\n                                             scale_factors=cfgs.SCALE_FACTORS,\n                                             gtboxes_and_label=None,\n                                             gtboxes_and_label_minAreaRectangle=None,\n                                             fast_rcnn_nms_iou_threshold=cfgs.FAST_RCNN_NMS_IOU_THRESHOLD,\n                                             fast_rcnn_maximum_boxes_per_img=100,\n                                             fast_rcnn_nms_max_boxes_per_class=cfgs.FAST_RCNN_NMS_MAX_BOXES_PER_CLASS,\n                                             show_detections_score_threshold=det_th,\n                                             # show detections which score >= 0.6\n                                             num_classes=cfgs.CLASS_NUM,\n                                             fast_rcnn_minibatch_size=cfgs.FAST_RCNN_MINIBATCH_SIZE,\n                                             fast_rcnn_positives_ratio=cfgs.FAST_RCNN_POSITIVE_RATE,\n                                             fast_rcnn_positives_iou_threshold=cfgs.FAST_RCNN_IOU_POSITIVE_THRESHOLD,\n                                             # iou>0.5 is positive, iou<0.5 is negative\n                                             use_dropout=cfgs.USE_DROPOUT,\n                                             weight_decay=cfgs.WEIGHT_DECAY[cfgs.NET_NAME],\n                                             is_training=False,\n                                             level=cfgs.LEVEL)\n\n        fast_rcnn_decode_boxes, fast_rcnn_score, num_of_objects, detection_category = \\\n            fast_rcnn.fast_rcnn_predict()\n\n        init_op = tf.group(\n            tf.global_variables_initializer(),\n            tf.local_variables_initializer()\n        )\n\n        restorer, restore_ckpt = restore_model.get_restorer()\n\n        config = tf.ConfigProto()\n        # config.gpu_options.per_process_gpu_memory_fraction = 0.5\n        config.gpu_options.allow_growth = True\n\n        with tf.Session(config=config) as sess:\n            sess.run(init_op)\n            if not restorer is None:\n                restorer.restore(sess, restore_ckpt)\n                print(\'restore model\')\n\n            coord = tf.train.Coordinator()\n            threads = tf.train.start_queue_runners(sess, coord)\n\n            for img_path in file_paths:\n                start = timer()\n                # gdal.AllRegister()\n                # ds = gdal.Open(img_path, gdalconst.GA_ReadOnly)\n                # if ds is None:\n                #     print(""Image %s open failed!"" % img_path)\n                #     sys.exit()\n                img = cv2.imread(img_path)\n\n                box_res = []\n                label_res = []\n                score_res = []\n                # imgH = ds.RasterYSize\n                # imgW = ds.RasterXSize\n                imgH = img.shape[0]\n                imgW = img.shape[1]\n                for hh in range(0, imgH, h_len):\n                    h_size = min(h_len, imgH - hh)\n                    if h_size < 10:\n                        break\n                    for ww in range(0, imgW, w_len):\n                        w_size = min(w_len, imgW - ww)\n                        if w_size < 10:\n                            break\n\n                        # src_img = ds.ReadAsArray(ww, hh, w_size, h_size)\n                        src_img = img[hh:(hh + h_size), ww:(ww + w_size), :]\n                        # if len(src_img.shape) == 2:\n                        #     src_img = cv2.cvtColor(src_img, cv2.COLOR_GRAY2RGB)\n                        # else:\n                        #     src_img = chw2hwc(src_img)\n\n                        boxes, labels, scores = sess.run([fast_rcnn_decode_boxes, detection_category, fast_rcnn_score],\n                                                         feed_dict={img_plac: src_img})\n\n                        if show_res:\n                            visualize_detection(src_img, boxes, scores)\n                        if len(boxes) > 0:\n                            for ii in range(len(boxes)):\n                                box = boxes[ii]\n                                box[0] = box[0] + hh\n                                box[1] = box[1] + ww\n                                box[2] = box[2] + hh\n                                box[3] = box[3] + ww\n                                box_res.append(box)\n                                label_res.append(labels[ii])\n                                score_res.append(scores[ii])\n                # ds = None\n                time_elapsed = timer() - start\n                print(""{} detection time : {:.4f} sec"".format(img_path.split(\'/\')[-1].split(\'.\')[0], time_elapsed))\n\n                # if target_name == \'aircraft\':\n                # img = cv2.imread(img_path)\n                # if len(img.shape) == 2:\n                #     img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n                # elif len(img.shape) == 3:\n                #     img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n                #     img[:, :, 0] = img[:, :, 1] = img[:, :, 2] = img_gray\n                mkdir(des_folder)\n                img_np = draw_box_cv(img,\n                                     boxes=np.array(box_res),\n                                     labels=np.array(label_res),\n                                     scores=np.array(score_res)) - np.array([103.939, 116.779, 123.68])\n                cv2.imwrite(des_folder + \'/{}_fpn.jpg\'.format(img_path.split(\'/\')[-1].split(\'.\')[0]), img_np)\n                # clip_obj_imgs(src_img, box_res, label_res, score_res, des_folder)\n                # print(img_path)\n                # det_xml_path =img_path.replace("".tif"", "".det.xml"")\n                # obj_to_det_xml(img_path, box_res, label_res, score_res, det_xml_path)\n\n            coord.request_stop()\n            coord.join(threads)\n\n\ndef parse_args():\n    """"""\n    Parse input arguments\n    """"""\n    parser = argparse.ArgumentParser(description=\'Train a Fast R-CNN network\')\n    parser.add_argument(\'--src_folder\', dest=\'src_folder\',\n                        help=\'images path\',\n                        default=None, type=str)\n    parser.add_argument(\'--des_folder\', dest=\'des_folder\',\n                        help=\'output path\',\n                        default=None, type=str)\n    parser.add_argument(\'--det_th\', dest=\'det_th\',\n                        help=\'detection threshold\',\n                        default=0.7,\n                        type=float)\n    parser.add_argument(\'--h_len\', dest=\'h_len\',\n                        help=\'image height\',\n                        default=600, type=int)\n    parser.add_argument(\'--w_len\', dest=\'w_len\',\n                        help=\'image width\',\n                        default=1000, type=int)\n    parser.add_argument(\'--image_ext\', dest=\'image_ext\',\n                        help=\'image format\',\n                        default=\'.tif\', type=str)\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n\n    args = parser.parse_args()\n    return args\n\nif __name__ == ""__main__"":\n    args = parse_args()\n    print(\'Called with args:\')\n    print(args)\n    file_paths = get_file_paths_recursive(args.src_folder, args.image_ext)\n\n    detect_img(file_paths, args.des_folder, args.det_th, args.h_len, args.w_len, False)\n\n'"
tools/demo1.py,11,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\n\nimport sys\nsys.path.append(\'../\')\nimport random\nimport matplotlib.pyplot as plt\n# from osgeo import gdal, gdalconst\nimport xml.dom.minidom\nimport time\nfrom timeit import default_timer as timer\nimport cv2\nfrom data.io import image_preprocess\nfrom libs.networks.network_factory import get_network_byname\nfrom libs.rpn import build_rpn\nfrom libs.fast_rcnn import build_fast_rcnn1\nfrom tools import restore_model\nfrom libs.configs import cfgs\nfrom help_utils.tools import *\nfrom help_utils.help_utils import *\nimport argparse\nfrom libs.box_utils import nms_rotate\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""1""\n\n\ndef get_file_paths_recursive(folder=None, file_ext=None):\n    """""" Get the absolute path of all files in given folder recursively\n    :param folder:\n    :param file_ext:\n    :return:\n    """"""\n    file_list = []\n    if folder is None:\n        return file_list\n\n    for dir_path, dir_names, file_names in os.walk(folder):\n        for file_name in file_names:\n            if file_ext is None:\n                file_list.append(os.path.join(dir_path, file_name))\n                continue\n            if file_name.endswith(file_ext):\n                file_list.append(os.path.join(dir_path, file_name))\n    return file_list\n\n\ndef clip_obj_imgs(src_img, boxes, classes, scores, des_folder):\n    """""" Clip image by target information\n    :param src_img:\n    :param boxes:\n    :param classes:\n    :param scores:\n    :param des_folder:\n    :return:\n    """"""\n    box_num = len(boxes)\n    ii = 0\n    off_size = 20\n    img_height = src_img.shape[0]\n    img_width = src_img.shape[1]\n\n    while ii < box_num:\n        box = boxes[ii]\n        xpos = max(box[0] - off_size, 0)\n        ypos = max(box[1] - off_size, 0)\n        clip_w = min(box[2]-box[0]+2*off_size, img_width-xpos)\n        clip_h = min(box[3]-box[1]+2*off_size, img_height-ypos)\n        img = np.zeros((clip_h, clip_w, 3))\n        img[0:clip_h, 0:clip_w, :] = src_img[ypos:ypos+clip_h, xpos:xpos+clip_w, :]\n        #plt.imshow(img)\n        #plt.show()\n        clip_path = os.path.join(des_folder, \'%s-%d_%.3f.jpg\' % (classes[ii], ii, scores[ii]))\n        cv2.imwrite(clip_path, img)\n        ii = ii + 1\n\n\ndef detect_img(file_paths, des_folder, det_th, h_len, w_len, h_overlap, w_overlap, show_res=False):\n    with tf.Graph().as_default():\n\n        img_plac = tf.placeholder(shape=[None, None, 3], dtype=tf.uint8)\n\n        img_tensor = tf.cast(img_plac, tf.float32) - tf.constant([103.939, 116.779, 123.68])\n        img_batch = image_preprocess.short_side_resize_for_inference_data(img_tensor,\n                                                                          target_shortside_len=cfgs.SHORT_SIDE_LEN,\n                                                                          is_resize=False)\n\n        # ***********************************************************************************************\n        # *                                         share net                                           *\n        # ***********************************************************************************************\n        _, share_net = get_network_byname(net_name=cfgs.NET_NAME,\n                                          inputs=img_batch,\n                                          num_classes=None,\n                                          is_training=True,\n                                          output_stride=None,\n                                          global_pool=False,\n                                          spatial_squeeze=False)\n        # ***********************************************************************************************\n        # *                                            RPN                                              *\n        # ***********************************************************************************************\n        rpn = build_rpn.RPN(net_name=cfgs.NET_NAME,\n                            inputs=img_batch,\n                            gtboxes_and_label=None,\n                            is_training=False,\n                            share_head=cfgs.SHARE_HEAD,\n                            share_net=share_net,\n                            stride=cfgs.STRIDE,\n                            anchor_ratios=cfgs.ANCHOR_RATIOS,\n                            anchor_scales=cfgs.ANCHOR_SCALES,\n                            scale_factors=cfgs.SCALE_FACTORS,\n                            base_anchor_size_list=cfgs.BASE_ANCHOR_SIZE_LIST,  # P2, P3, P4, P5, P6\n                            level=cfgs.LEVEL,\n                            top_k_nms=cfgs.RPN_TOP_K_NMS,\n                            rpn_nms_iou_threshold=cfgs.RPN_NMS_IOU_THRESHOLD,\n                            max_proposals_num=cfgs.MAX_PROPOSAL_NUM,\n                            rpn_iou_positive_threshold=cfgs.RPN_IOU_POSITIVE_THRESHOLD,\n                            rpn_iou_negative_threshold=cfgs.RPN_IOU_NEGATIVE_THRESHOLD,\n                            rpn_mini_batch_size=cfgs.RPN_MINIBATCH_SIZE,\n                            rpn_positives_ratio=cfgs.RPN_POSITIVE_RATE,\n                            remove_outside_anchors=False,  # whether remove anchors outside\n                            rpn_weight_decay=cfgs.WEIGHT_DECAY[cfgs.NET_NAME])\n\n        # rpn predict proposals\n        rpn_proposals_boxes, rpn_proposals_scores = rpn.rpn_proposals()  # rpn_score shape: [300, ]\n\n        # ***********************************************************************************************\n        # *                                         Fast RCNN                                           *\n        # ***********************************************************************************************\n        fast_rcnn = build_fast_rcnn1.FastRCNN(feature_pyramid=rpn.feature_pyramid,\n                                              rpn_proposals_boxes=rpn_proposals_boxes,\n                                              rpn_proposals_scores=rpn_proposals_scores,\n                                              img_shape=tf.shape(img_batch),\n                                              roi_size=cfgs.ROI_SIZE,\n                                              roi_pool_kernel_size=cfgs.ROI_POOL_KERNEL_SIZE,\n                                              scale_factors=cfgs.SCALE_FACTORS,\n                                              gtboxes_and_label=None,\n                                              gtboxes_and_label_minAreaRectangle=None,\n                                              fast_rcnn_nms_iou_threshold=cfgs.FAST_RCNN_NMS_IOU_THRESHOLD,\n                                              fast_rcnn_maximum_boxes_per_img=100,\n                                              fast_rcnn_nms_max_boxes_per_class=cfgs.FAST_RCNN_NMS_MAX_BOXES_PER_CLASS,\n                                              show_detections_score_threshold=det_th,\n                                              # show detections which score >= 0.6\n                                              num_classes=cfgs.CLASS_NUM,\n                                              fast_rcnn_minibatch_size=cfgs.FAST_RCNN_MINIBATCH_SIZE,\n                                              fast_rcnn_positives_ratio=cfgs.FAST_RCNN_POSITIVE_RATE,\n                                              fast_rcnn_positives_iou_threshold=cfgs.FAST_RCNN_IOU_POSITIVE_THRESHOLD,\n                                              # iou>0.5 is positive, iou<0.5 is negative\n                                              use_dropout=cfgs.USE_DROPOUT,\n                                              weight_decay=cfgs.WEIGHT_DECAY[cfgs.NET_NAME],\n                                              is_training=False,\n                                              level=cfgs.LEVEL)\n\n        fast_rcnn_decode_boxes, fast_rcnn_score, num_of_objects, detection_category, \\\n        fast_rcnn_decode_boxes_rotate, fast_rcnn_score_rotate, num_of_objects_rotate, detection_category_rotate = \\\n            fast_rcnn.fast_rcnn_predict()\n\n        init_op = tf.group(\n            tf.global_variables_initializer(),\n            tf.local_variables_initializer()\n        )\n\n        restorer, restore_ckpt = restore_model.get_restorer()\n\n        config = tf.ConfigProto()\n        # config.gpu_options.per_process_gpu_memory_fraction = 0.5\n        config.gpu_options.allow_growth = True\n\n        with tf.Session(config=config) as sess:\n            sess.run(init_op)\n            if not restorer is None:\n                restorer.restore(sess, restore_ckpt)\n                print(\'restore model\')\n\n            coord = tf.train.Coordinator()\n            threads = tf.train.start_queue_runners(sess, coord)\n\n            for img_path in file_paths:\n                start = timer()\n                img = cv2.imread(img_path)\n\n                box_res = []\n                label_res = []\n                score_res = []\n                box_res_rotate = []\n                label_res_rotate = []\n                score_res_rotate = []\n\n                imgH = img.shape[0]\n                imgW = img.shape[1]\n                for hh in range(0, imgH, h_len - h_overlap):\n                    h_size = min(h_len, imgH - hh)\n                    if h_size < 10:\n                        break\n                    for ww in range(0, imgW, w_len - w_overlap):\n                        w_size = min(w_len, imgW - ww)\n                        if w_size < 10:\n                            break\n\n                        src_img = img[hh:(hh + h_size), ww:(ww + w_size), :]\n\n                        boxes, labels, scores = sess.run([fast_rcnn_decode_boxes, detection_category, fast_rcnn_score],\n                                                         feed_dict={img_plac: src_img})\n\n                        boxes_rotate, labels_rotate, scores_rotate = sess.run([fast_rcnn_decode_boxes_rotate,\n                                                                               detection_category_rotate,\n                                                                               fast_rcnn_score_rotate],\n                                                                              feed_dict={img_plac: src_img})\n\n                        if len(boxes) > 0:\n                            for ii in range(len(boxes)):\n                                box = boxes[ii]\n                                box[0] = box[0] + hh\n                                box[1] = box[1] + ww\n                                box[2] = box[2] + hh\n                                box[3] = box[3] + ww\n                                box_res.append(box)\n                                label_res.append(labels[ii])\n                                score_res.append(scores[ii])\n                        if len(boxes_rotate) > 0:\n                            for ii in range(len(boxes_rotate)):\n                                box_rotate = boxes_rotate[ii]\n                                box_rotate[0] = box_rotate[0] + hh\n                                box_rotate[1] = box_rotate[1] + ww\n                                box_res_rotate.append(box_rotate)\n                                label_res_rotate.append(labels_rotate[ii])\n                                score_res_rotate.append(scores_rotate[ii])\n\n                # inx = nms_rotate.nms_rotate_cpu(boxes=np.array(box_res_rotate), scores=np.array(score_res_rotate),\n                #                                 iou_threshold=0.5, max_output_size=100)\n                # box_res_rotate = np.array(box_res_rotate)[inx]\n                # score_res_rotate = np.array(score_res_rotate)[inx]\n                # label_res_rotate = np.array(label_res_rotate)[inx]\n\n                time_elapsed = timer() - start\n                print(""{} detection time : {:.4f} sec"".format(img_path.split(\'/\')[-1].split(\'.\')[0], time_elapsed))\n\n                mkdir(des_folder)\n                img_np = draw_box_cv(np.array(img, np.float32) - np.array([103.939, 116.779, 123.68]),\n                                     boxes=np.array(box_res),\n                                     labels=np.array(label_res),\n                                     scores=np.array(score_res))\n                img_np_rotate = draw_rotate_box_cv(np.array(img, np.float32) - np.array([103.939, 116.779, 123.68]),\n                                                   boxes=np.array(box_res_rotate),\n                                                   labels=np.array(label_res_rotate),\n                                                   scores=np.array(score_res_rotate))\n                cv2.imwrite(des_folder + \'/{}_horizontal_fpn.jpg\'.format(img_path.split(\'/\')[-1].split(\'.\')[0]), img_np)\n                cv2.imwrite(des_folder + \'/{}_rotate_fpn.jpg\'.format(img_path.split(\'/\')[-1].split(\'.\')[0]), img_np_rotate)\n\n            coord.request_stop()\n            coord.join(threads)\n\n\ndef parse_args():\n    """"""\n    Parse input arguments\n    """"""\n    parser = argparse.ArgumentParser(description=\'Train a Fast R-CNN network\')\n    parser.add_argument(\'--src_folder\', dest=\'src_folder\',\n                        help=\'images path\',\n                        default=None, type=str)\n    parser.add_argument(\'--des_folder\', dest=\'des_folder\',\n                        help=\'output path\',\n                        default=None, type=str)\n    parser.add_argument(\'--det_th\', dest=\'det_th\',\n                        help=\'detection threshold\',\n                        default=0.7,\n                        type=float)\n    parser.add_argument(\'--h_len\', dest=\'h_len\',\n                        help=\'image height\',\n                        default=600, type=int)\n    parser.add_argument(\'--w_len\', dest=\'w_len\',\n                        help=\'image width\',\n                        default=1000, type=int)\n    parser.add_argument(\'--h_overlap\', dest=\'h_overlap\',\n                        help=\'height overlap\',\n                        default=0, type=int)\n    parser.add_argument(\'--w_overlap\', dest=\'w_overlap\',\n                        help=\'width overlap\',\n                        default=0, type=int)\n    parser.add_argument(\'--image_ext\', dest=\'image_ext\',\n                        help=\'image format\',\n                        default=\'.tif\', type=str)\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n\n    args = parser.parse_args()\n    return args\n\nif __name__ == ""__main__"":\n    args = parse_args()\n    print(\'Called with args:\')\n    print(args)\n    file_paths = get_file_paths_recursive(args.src_folder, args.image_ext)\n\n    detect_img(file_paths, args.des_folder, args.det_th, args.h_len, args.w_len,\n               args.h_overlap, args.w_overlap, False)\n\n'"
tools/eval.py,14,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\nimport sys\nsys.path.append(\'../\')\n\nimport tensorflow as tf\nimport os\nimport time\nfrom data.io.read_tfrecord import next_batch\nfrom libs.networks.network_factory import get_network_byname\nfrom libs.label_name_dict.label_dict import *\nfrom libs.rpn import build_rpn\nfrom help_utils.tools import view_bar\nfrom tools import restore_model\nimport pickle\nfrom libs.box_utils.coordinate_convert import *\nfrom libs.box_utils.boxes_utils import get_horizen_minAreaRectangle\nfrom libs.fast_rcnn import build_fast_rcnn\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = \'1\'\n\n\ndef make_dict_packle(_gtboxes_and_label, _fast_rcnn_decode_boxes, _fast_rcnn_score, _detection_category):\n\n    gtbox_list = []\n    predict_list = []\n\n    for j, box in enumerate(_gtboxes_and_label):\n        bbox_dict = {}\n        bbox_dict[\'bbox\'] = np.array(_gtboxes_and_label[j, :-1], np.float64)\n        bbox_dict[\'name\'] = LABEl_NAME_MAP[int(_gtboxes_and_label[j, -1])]\n        gtbox_list.append(bbox_dict)\n\n    for label in NAME_LABEL_MAP.keys():\n        if label == \'back_ground\':\n            continue\n        else:\n            temp_dict = {}\n            temp_dict[\'name\'] = label\n\n            ind = np.where(_detection_category == NAME_LABEL_MAP[label])[0]\n            temp_boxes = _fast_rcnn_decode_boxes[ind]\n            temp_score = np.reshape(_fast_rcnn_score[ind], [-1, 1])\n            temp_dict[\'bbox\'] = np.array(np.concatenate([temp_boxes, temp_score], axis=1), np.float64)\n            predict_list.append(temp_dict)\n    return gtbox_list, predict_list\n\n\ndef eval_ship(img_num):\n    with tf.Graph().as_default():\n\n        img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch = \\\n            next_batch(dataset_name=cfgs.DATASET_NAME,\n                       batch_size=cfgs.BATCH_SIZE,\n                       shortside_len=cfgs.SHORT_SIDE_LEN,\n                       is_training=False)\n\n        gtboxes_and_label = tf.py_func(back_forward_convert,\n                                       inp=[tf.squeeze(gtboxes_and_label_batch, 0)],\n                                       Tout=tf.float32)\n        gtboxes_and_label = tf.reshape(gtboxes_and_label, [-1, 6])\n\n        gtboxes_and_label_minAreaRectangle = get_horizen_minAreaRectangle(gtboxes_and_label)\n\n        gtboxes_and_label_minAreaRectangle = tf.reshape(gtboxes_and_label_minAreaRectangle, [-1, 5])\n\n        # ***********************************************************************************************\n        # *                                         share net                                           *\n        # ***********************************************************************************************\n        _, share_net = get_network_byname(net_name=cfgs.NET_NAME,\n                                          inputs=img_batch,\n                                          num_classes=None,\n                                          is_training=True,\n                                          output_stride=None,\n                                          global_pool=False,\n                                          spatial_squeeze=False)\n\n        # ***********************************************************************************************\n        # *                                            RPN                                              *\n        # ***********************************************************************************************\n        rpn = build_rpn.RPN(net_name=cfgs.NET_NAME,\n                            inputs=img_batch,\n                            gtboxes_and_label=None,\n                            is_training=False,\n                            share_head=cfgs.SHARE_HEAD,\n                            share_net=share_net,\n                            stride=cfgs.STRIDE,\n                            anchor_ratios=cfgs.ANCHOR_RATIOS,\n                            anchor_scales=cfgs.ANCHOR_SCALES,\n                            scale_factors=cfgs.SCALE_FACTORS,\n                            base_anchor_size_list=cfgs.BASE_ANCHOR_SIZE_LIST,  # P2, P3, P4, P5, P6\n                            level=cfgs.LEVEL,\n                            top_k_nms=cfgs.RPN_TOP_K_NMS,\n                            rpn_nms_iou_threshold=cfgs.RPN_NMS_IOU_THRESHOLD,\n                            max_proposals_num=cfgs.MAX_PROPOSAL_NUM,\n                            rpn_iou_positive_threshold=cfgs.RPN_IOU_POSITIVE_THRESHOLD,\n                            rpn_iou_negative_threshold=cfgs.RPN_IOU_NEGATIVE_THRESHOLD,\n                            rpn_mini_batch_size=cfgs.RPN_MINIBATCH_SIZE,\n                            rpn_positives_ratio=cfgs.RPN_POSITIVE_RATE,\n                            remove_outside_anchors=False,  # whether remove anchors outside\n                            rpn_weight_decay=cfgs.WEIGHT_DECAY[cfgs.NET_NAME])\n\n        # rpn predict proposals\n        rpn_proposals_boxes, rpn_proposals_scores = rpn.rpn_proposals()  # rpn_score shape: [300, ]\n\n        # ***********************************************************************************************\n        # *                                         Fast RCNN                                           *\n        # ***********************************************************************************************\n        fast_rcnn = build_fast_rcnn.FastRCNN(feature_pyramid=rpn.feature_pyramid,\n                                             rpn_proposals_boxes=rpn_proposals_boxes,\n                                             rpn_proposals_scores=rpn_proposals_scores,\n                                             img_shape=tf.shape(img_batch),\n                                             roi_size=cfgs.ROI_SIZE,\n                                             roi_pool_kernel_size=cfgs.ROI_POOL_KERNEL_SIZE,\n                                             scale_factors=cfgs.SCALE_FACTORS,\n                                             gtboxes_and_label=None,\n                                             gtboxes_and_label_minAreaRectangle=gtboxes_and_label_minAreaRectangle,\n                                             fast_rcnn_nms_iou_threshold=cfgs.FAST_RCNN_NMS_IOU_THRESHOLD,\n                                             fast_rcnn_maximum_boxes_per_img=100,\n                                             fast_rcnn_nms_max_boxes_per_class=cfgs.FAST_RCNN_NMS_MAX_BOXES_PER_CLASS,\n                                             show_detections_score_threshold=cfgs.FINAL_SCORE_THRESHOLD,\n                                             # show detections which score >= 0.6\n                                             num_classes=cfgs.CLASS_NUM,\n                                             fast_rcnn_minibatch_size=cfgs.FAST_RCNN_MINIBATCH_SIZE,\n                                             fast_rcnn_positives_ratio=cfgs.FAST_RCNN_POSITIVE_RATE,\n                                             fast_rcnn_positives_iou_threshold=cfgs.FAST_RCNN_IOU_POSITIVE_THRESHOLD,\n                                             # iou>0.5 is positive, iou<0.5 is negative\n                                             use_dropout=cfgs.USE_DROPOUT,\n                                             weight_decay=cfgs.WEIGHT_DECAY[cfgs.NET_NAME],\n                                             is_training=False,\n                                             level=cfgs.LEVEL)\n\n        fast_rcnn_decode_boxes, fast_rcnn_score, num_of_objects, detection_category = fast_rcnn.fast_rcnn_predict()\n\n        # train\n        init_op = tf.group(\n            tf.global_variables_initializer(),\n            tf.local_variables_initializer()\n        )\n\n        restorer, restore_ckpt = restore_model.get_restorer()\n\n        config = tf.ConfigProto()\n        # config.gpu_options.per_process_gpu_memory_fraction = 0.5\n        config.gpu_options.allow_growth = True\n        with tf.Session(config=config) as sess:\n            sess.run(init_op)\n            if not restorer is None:\n                restorer.restore(sess, restore_ckpt)\n                print(\'restore model\')\n\n            coord = tf.train.Coordinator()\n            threads = tf.train.start_queue_runners(sess, coord)\n\n            gtboxes_horizontal_dict = {}\n            predict_horizontal_dict = {}\n\n            for i in range(img_num):\n\n                start = time.time()\n\n                _img_name_batch, _img_batch, _gtboxes_and_label, _gtboxes_and_label_minAreaRectangle, \\\n                _fast_rcnn_decode_boxes, _fast_rcnn_score, _detection_category \\\n                    = sess.run([img_name_batch, img_batch, gtboxes_and_label, gtboxes_and_label_minAreaRectangle,\n                                fast_rcnn_decode_boxes, fast_rcnn_score, detection_category])\n                end = time.time()\n\n                # gtboxes convert dict\n                gtboxes_horizontal_dict[str(_img_name_batch[0])] = []\n                predict_horizontal_dict[str(_img_name_batch[0])] = []\n\n                gtbox_horizontal_list, predict_horizontal_list = \\\n                    make_dict_packle(_gtboxes_and_label_minAreaRectangle, _fast_rcnn_decode_boxes,\n                                     _fast_rcnn_score, _detection_category)\n\n                gtboxes_horizontal_dict[str(_img_name_batch[0])].extend(gtbox_horizontal_list)\n                predict_horizontal_dict[str(_img_name_batch[0])].extend(predict_horizontal_list)\n\n                view_bar(\'{} image cost {}s\'.format(str(_img_name_batch[0]), (end - start)), i + 1, img_num)\n\n            fw1 = open(\'gtboxes_horizontal_dict.pkl\', \'w\')\n            fw2 = open(\'predict_horizontal_dict.pkl\', \'w\')\n            pickle.dump(gtboxes_horizontal_dict, fw1)\n            pickle.dump(predict_horizontal_dict, fw2)\n            fw1.close()\n            fw2.close()\n            coord.request_stop()\n            coord.join(threads)\n\n\ndef voc_ap(rec, prec, use_07_metric=False):\n    """"""\n    average precision calculations\n    [precision integrated to recall]\n    :param rec: recall\n    :param prec: precision\n    :param use_07_metric: 2007 metric is 11-recall-point based AP\n    :return: average precision\n    """"""\n    if use_07_metric:\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap += p / 11.\n    else:\n        # append sentinel values at both ends\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute precision integration ladder\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # look for recall value changes\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # sum (\\delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\n\ndef get_single_label_dict(predict_dict, gtboxes_dict, label):\n    rboxes = {}\n    gboxes = {}\n    rbox_images = predict_dict.keys()\n    for i in range(len(rbox_images)):\n        rbox_image = rbox_images[i]\n        for pre_box in predict_dict[rbox_image]:\n            if pre_box[\'name\'] == label and len(pre_box[\'bbox\']) != 0:\n                rboxes[rbox_image] = [pre_box]\n\n                gboxes[rbox_image] = []\n\n                for gt_box in gtboxes_dict[rbox_image]:\n                    if gt_box[\'name\'] == label:\n                        gboxes[rbox_image].append(gt_box)\n    return rboxes, gboxes\n\n\ndef eval(rboxes, gboxes, iou_th, use_07_metric):\n    rbox_images = rboxes.keys()\n    fp = np.zeros(len(rbox_images))\n    tp = np.zeros(len(rbox_images))\n    box_num = 0\n\n    for i in range(len(rbox_images)):\n        rbox_image = rbox_images[i]\n        if len(rboxes[rbox_image][0][\'bbox\']) > 0:\n\n            rbox_lists = np.array(rboxes[rbox_image][0][\'bbox\'])\n            if len(gboxes[rbox_image]) > 0:\n                gbox_list = np.array([obj[\'bbox\'] for obj in gboxes[rbox_image]])\n                box_num = box_num + len(gbox_list)\n                gbox_list = np.concatenate((gbox_list, np.zeros((np.shape(gbox_list)[0], 1))), axis=1)\n                confidence = rbox_lists[:, -1]\n                box_index = np.argsort(-confidence)\n\n                rbox_lists = rbox_lists[box_index, :]\n                for rbox_list in rbox_lists:\n                    ixmin = np.maximum(gbox_list[:, 0], rbox_list[0])\n                    iymin = np.maximum(gbox_list[:, 1], rbox_list[1])\n                    ixmax = np.minimum(gbox_list[:, 2], rbox_list[2])\n                    iymax = np.minimum(gbox_list[:, 3], rbox_list[3])\n                    iw = np.maximum(ixmax - ixmin + 1., 0.)\n                    ih = np.maximum(iymax - iymin + 1., 0.)\n                    inters = iw * ih\n                    # union\n                    uni = ((rbox_list[2] - rbox_list[0] + 1.) * (rbox_list[3] - rbox_list[1] + 1.) +\n                           (gbox_list[:, 2] - gbox_list[:, 0] + 1.) *\n                           (gbox_list[:, 3] - gbox_list[:, 1] + 1.) - inters)\n                    overlaps = inters / uni\n\n                    ovmax = np.max(overlaps)\n                    jmax = np.argmax(overlaps)\n                    if ovmax > iou_th:\n                        if gbox_list[jmax, -1] == 0:\n                            tp[i] += 1\n                            gbox_list[jmax, -1] = 1\n                        else:\n                            fp[i] += 1\n                    else:\n                        fp[i] += 1\n            else:\n                fp[i] += len(rboxes[rbox_image][0][\'bbox\'])\n        else:\n            continue\n    rec = np.zeros(len(rbox_images))\n    prec = np.zeros(len(rbox_images))\n    if box_num == 0:\n        for i in range(len(fp)):\n            if fp[i] != 0:\n                prec[i] = 0\n            else:\n                prec[i] = 1\n\n    else:\n\n        fp = np.cumsum(fp)\n        tp = np.cumsum(tp)\n\n        prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n        rec = tp / box_num\n    ap = voc_ap(rec, prec, use_07_metric)\n\n    return rec, prec, ap, box_num\n\n\nif __name__ == \'__main__\':\n    img_num = 1000\n    mode = 0\n    eval_ship(img_num)\n\n    fr1 = open(\'gtboxes_horizontal_dict.pkl\', \'r\')\n    fr2 = open(\'predict_horizontal_dict.pkl\', \'r\')\n    gtboxes_horizontal_dict = pickle.load(fr1)\n    predict_horizontal_dict = pickle.load(fr2)\n\n    R, P, AP, F, num = [], [], [], [], []\n\n    for label in NAME_LABEL_MAP.keys():\n        if label == \'back_ground\':\n            continue\n\n        rboxes, gboxes = get_single_label_dict(predict_horizontal_dict, gtboxes_horizontal_dict, label)\n\n        rec, prec, ap, box_num = eval(rboxes, gboxes, 0.5, False)\n\n        recall = rec[-1]\n        precision = prec[-1]\n        F_measure = (2 * precision * recall) / (recall + precision)\n        print(\'\\n{}\\tR:{}\\tP:{}\\tap:{}\\tF:{}\'.format(label, recall, precision, ap, F_measure))\n        R.append(recall)\n        P.append(precision)\n        AP.append(ap)\n        F.append(F_measure)\n        num.append(box_num)\n\n    R = np.array(R)\n    P = np.array(P)\n    AP = np.array(AP)\n    F = np.array(F)\n    num = np.array(num)\n    weights = num / np.sum(num)\n    Recall = np.sum(R * weights)\n    Precision = np.sum(P * weights)\n    mAP = np.sum(AP * weights)\n    F_measure = np.sum(F * weights)\n    print(\'\\n{}\\tR:{}\\tP:{}\\tmAP:{}\\tF:{}\'.format(\'Final\', Recall, Precision, mAP, F_measure))\n\n    fr1.close()\n    fr2.close()\n\n\n\n\n\n\n\n\n\n\n'"
tools/eval1.py,14,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\nimport sys\nsys.path.append(\'../\')\n\nimport tensorflow as tf\nimport os\nimport time\nfrom data.io.read_tfrecord import next_batch\nfrom libs.networks.network_factory import get_network_byname\nfrom libs.label_name_dict.label_dict import *\nfrom libs.rpn import build_rpn\nfrom help_utils.tools import view_bar\nfrom tools import restore_model\nimport pickle\nfrom libs.box_utils.coordinate_convert import *\nfrom libs.box_utils.boxes_utils import get_horizen_minAreaRectangle\nfrom libs.fast_rcnn import build_fast_rcnn1\nfrom libs.box_utils import iou_rotate\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = \'1\'\n\n\ndef make_dict_packle(_gtboxes_and_label, _fast_rcnn_decode_boxes, _fast_rcnn_score, _detection_category):\n\n    gtbox_list = []\n    predict_list = []\n\n    for j, box in enumerate(_gtboxes_and_label):\n        bbox_dict = {}\n        bbox_dict[\'bbox\'] = np.array(_gtboxes_and_label[j, :-1], np.float64)\n        bbox_dict[\'name\'] = LABEl_NAME_MAP[int(_gtboxes_and_label[j, -1])]\n        gtbox_list.append(bbox_dict)\n\n    for label in NAME_LABEL_MAP.keys():\n        if label == \'back_ground\':\n            continue\n        else:\n            temp_dict = {}\n            temp_dict[\'name\'] = label\n\n            ind = np.where(_detection_category == NAME_LABEL_MAP[label])[0]\n            temp_boxes = _fast_rcnn_decode_boxes[ind]\n            temp_score = np.reshape(_fast_rcnn_score[ind], [-1, 1])\n            temp_dict[\'bbox\'] = np.array(np.concatenate([temp_boxes, temp_score], axis=1), np.float64)\n            predict_list.append(temp_dict)\n    return gtbox_list, predict_list\n\n\ndef eval_ship(img_num, mode):\n    with tf.Graph().as_default():\n\n        img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch = \\\n            next_batch(dataset_name=cfgs.DATASET_NAME,\n                       batch_size=cfgs.BATCH_SIZE,\n                       shortside_len=cfgs.SHORT_SIDE_LEN,\n                       is_training=False)\n\n        gtboxes_and_label = tf.py_func(back_forward_convert,\n                                       inp=[tf.squeeze(gtboxes_and_label_batch, 0)],\n                                       Tout=tf.float32)\n        gtboxes_and_label = tf.reshape(gtboxes_and_label, [-1, 6])\n\n        gtboxes_and_label_minAreaRectangle = get_horizen_minAreaRectangle(gtboxes_and_label)\n\n        gtboxes_and_label_minAreaRectangle = tf.reshape(gtboxes_and_label_minAreaRectangle, [-1, 5])\n\n        # ***********************************************************************************************\n        # *                                         share net                                           *\n        # ***********************************************************************************************\n        _, share_net = get_network_byname(net_name=cfgs.NET_NAME,\n                                          inputs=img_batch,\n                                          num_classes=None,\n                                          is_training=True,\n                                          output_stride=None,\n                                          global_pool=False,\n                                          spatial_squeeze=False)\n\n        # ***********************************************************************************************\n        # *                                            RPN                                              *\n        # ***********************************************************************************************\n        rpn = build_rpn.RPN(net_name=cfgs.NET_NAME,\n                            inputs=img_batch,\n                            gtboxes_and_label=None,\n                            is_training=False,\n                            share_head=cfgs.SHARE_HEAD,\n                            share_net=share_net,\n                            stride=cfgs.STRIDE,\n                            anchor_ratios=cfgs.ANCHOR_RATIOS,\n                            anchor_scales=cfgs.ANCHOR_SCALES,\n                            scale_factors=cfgs.SCALE_FACTORS,\n                            base_anchor_size_list=cfgs.BASE_ANCHOR_SIZE_LIST,  # P2, P3, P4, P5, P6\n                            level=cfgs.LEVEL,\n                            top_k_nms=cfgs.RPN_TOP_K_NMS,\n                            rpn_nms_iou_threshold=cfgs.RPN_NMS_IOU_THRESHOLD,\n                            max_proposals_num=cfgs.MAX_PROPOSAL_NUM,\n                            rpn_iou_positive_threshold=cfgs.RPN_IOU_POSITIVE_THRESHOLD,\n                            rpn_iou_negative_threshold=cfgs.RPN_IOU_NEGATIVE_THRESHOLD,\n                            rpn_mini_batch_size=cfgs.RPN_MINIBATCH_SIZE,\n                            rpn_positives_ratio=cfgs.RPN_POSITIVE_RATE,\n                            remove_outside_anchors=False,  # whether remove anchors outside\n                            rpn_weight_decay=cfgs.WEIGHT_DECAY[cfgs.NET_NAME])\n\n        # rpn predict proposals\n        rpn_proposals_boxes, rpn_proposals_scores = rpn.rpn_proposals()  # rpn_score shape: [300, ]\n\n        # ***********************************************************************************************\n        # *                                         Fast RCNN                                           *\n        # ***********************************************************************************************\n        fast_rcnn = build_fast_rcnn1.FastRCNN(feature_pyramid=rpn.feature_pyramid,\n                                              rpn_proposals_boxes=rpn_proposals_boxes,\n                                              rpn_proposals_scores=rpn_proposals_scores,\n                                              img_shape=tf.shape(img_batch),\n                                              roi_size=cfgs.ROI_SIZE,\n                                              roi_pool_kernel_size=cfgs.ROI_POOL_KERNEL_SIZE,\n                                              scale_factors=cfgs.SCALE_FACTORS,\n                                              gtboxes_and_label=None,\n                                              gtboxes_and_label_minAreaRectangle=gtboxes_and_label_minAreaRectangle,\n                                              fast_rcnn_nms_iou_threshold=cfgs.FAST_RCNN_NMS_IOU_THRESHOLD,\n                                              fast_rcnn_maximum_boxes_per_img=100,\n                                              fast_rcnn_nms_max_boxes_per_class=cfgs.FAST_RCNN_NMS_MAX_BOXES_PER_CLASS,\n                                              show_detections_score_threshold=cfgs.FINAL_SCORE_THRESHOLD,\n                                              # show detections which score >= 0.6\n                                              num_classes=cfgs.CLASS_NUM,\n                                              fast_rcnn_minibatch_size=cfgs.FAST_RCNN_MINIBATCH_SIZE,\n                                              fast_rcnn_positives_ratio=cfgs.FAST_RCNN_POSITIVE_RATE,\n                                              fast_rcnn_positives_iou_threshold=cfgs.FAST_RCNN_IOU_POSITIVE_THRESHOLD,\n                                              # iou>0.5 is positive, iou<0.5 is negative\n                                              use_dropout=cfgs.USE_DROPOUT,\n                                              weight_decay=cfgs.WEIGHT_DECAY[cfgs.NET_NAME],\n                                              is_training=False,\n                                              level=cfgs.LEVEL)\n\n        fast_rcnn_decode_boxes, fast_rcnn_score, num_of_objects, detection_category, \\\n        fast_rcnn_decode_boxes_rotate, fast_rcnn_score_rotate, num_of_objects_rotate, detection_category_rotate = \\\n            fast_rcnn.fast_rcnn_predict()\n\n        if mode == 0:\n            fast_rcnn_decode_boxes_rotate = get_horizen_minAreaRectangle(fast_rcnn_decode_boxes_rotate, False)\n\n        # train\n        init_op = tf.group(\n            tf.global_variables_initializer(),\n            tf.local_variables_initializer()\n        )\n\n        restorer, restore_ckpt = restore_model.get_restorer()\n\n        config = tf.ConfigProto()\n        # config.gpu_options.per_process_gpu_memory_fraction = 0.5\n        config.gpu_options.allow_growth = True\n        with tf.Session(config=config) as sess:\n            sess.run(init_op)\n            if not restorer is None:\n                restorer.restore(sess, restore_ckpt)\n                print(\'restore model\')\n\n            coord = tf.train.Coordinator()\n            threads = tf.train.start_queue_runners(sess, coord)\n\n            gtboxes_horizontal_dict = {}\n            predict_horizontal_dict = {}\n            gtboxes_rotate_dict = {}\n            predict_rotate_dict = {}\n\n            for i in range(img_num):\n\n                start = time.time()\n\n                _img_name_batch, _img_batch, _gtboxes_and_label, _gtboxes_and_label_minAreaRectangle, \\\n                _fast_rcnn_decode_boxes, _fast_rcnn_score, _detection_category, _fast_rcnn_decode_boxes_rotate, \\\n                _fast_rcnn_score_rotate, _detection_category_rotate \\\n                    = sess.run([img_name_batch, img_batch, gtboxes_and_label, gtboxes_and_label_minAreaRectangle,\n                                fast_rcnn_decode_boxes, fast_rcnn_score, detection_category, fast_rcnn_decode_boxes_rotate,\n                                fast_rcnn_score_rotate, detection_category_rotate])\n                end = time.time()\n\n                # gtboxes convert dict\n                gtboxes_horizontal_dict[str(_img_name_batch[0])] = []\n                predict_horizontal_dict[str(_img_name_batch[0])] = []\n                gtboxes_rotate_dict[str(_img_name_batch[0])] = []\n                predict_rotate_dict[str(_img_name_batch[0])] = []\n\n                gtbox_horizontal_list, predict_horizontal_list = \\\n                    make_dict_packle(_gtboxes_and_label_minAreaRectangle, _fast_rcnn_decode_boxes,\n                                     _fast_rcnn_score, _detection_category)\n\n                if mode == 0:\n                    gtbox_rotate_list, predict_rotate_list = \\\n                        make_dict_packle(_gtboxes_and_label_minAreaRectangle, _fast_rcnn_decode_boxes_rotate,\n                                         _fast_rcnn_score_rotate, _detection_category_rotate)\n                else:\n                    gtbox_rotate_list, predict_rotate_list = \\\n                        make_dict_packle(_gtboxes_and_label, _fast_rcnn_decode_boxes_rotate,\n                                         _fast_rcnn_score_rotate, _detection_category_rotate)\n\n                gtboxes_horizontal_dict[str(_img_name_batch[0])].extend(gtbox_horizontal_list)\n                predict_horizontal_dict[str(_img_name_batch[0])].extend(predict_horizontal_list)\n                gtboxes_rotate_dict[str(_img_name_batch[0])].extend(gtbox_rotate_list)\n                predict_rotate_dict[str(_img_name_batch[0])].extend(predict_rotate_list)\n\n                view_bar(\'{} image cost {}s\'.format(str(_img_name_batch[0]), (end - start)), i + 1, img_num)\n\n            fw1 = open(\'gtboxes_horizontal_dict.pkl\', \'w\')\n            fw2 = open(\'predict_horizontal_dict.pkl\', \'w\')\n            fw3 = open(\'gtboxes_rotate_dict.pkl\', \'w\')\n            fw4 = open(\'predict_rotate_dict.pkl\', \'w\')\n            pickle.dump(gtboxes_horizontal_dict, fw1)\n            pickle.dump(predict_horizontal_dict, fw2)\n            pickle.dump(gtboxes_rotate_dict, fw3)\n            pickle.dump(predict_rotate_dict, fw4)\n            fw1.close()\n            fw2.close()\n            fw3.close()\n            fw4.close()\n            coord.request_stop()\n            coord.join(threads)\n\n\ndef voc_ap(rec, prec, use_07_metric=False):\n    """"""\n    average precision calculations\n    [precision integrated to recall]\n    :param rec: recall\n    :param prec: precision\n    :param use_07_metric: 2007 metric is 11-recall-point based AP\n    :return: average precision\n    """"""\n    if use_07_metric:\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap += p / 11.\n    else:\n        # append sentinel values at both ends\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute precision integration ladder\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # look for recall value changes\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # sum (\\delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\n\ndef get_single_label_dict(predict_dict, gtboxes_dict, label):\n    rboxes = {}\n    gboxes = {}\n    rbox_images = predict_dict.keys()\n    for i in range(len(rbox_images)):\n        rbox_image = rbox_images[i]\n        for pre_box in predict_dict[rbox_image]:\n            if pre_box[\'name\'] == label and len(pre_box[\'bbox\']) != 0:\n                rboxes[rbox_image] = [pre_box]\n\n                gboxes[rbox_image] = []\n\n                for gt_box in gtboxes_dict[rbox_image]:\n                    if gt_box[\'name\'] == label:\n                        gboxes[rbox_image].append(gt_box)\n    return rboxes, gboxes\n\n\ndef eval(rboxes, gboxes, iou_th, use_07_metric, mode):\n    rbox_images = rboxes.keys()\n    fp = np.zeros(len(rbox_images))\n    tp = np.zeros(len(rbox_images))\n    box_num = 0\n\n    for i in range(len(rbox_images)):\n        rbox_image = rbox_images[i]\n        if len(rboxes[rbox_image][0][\'bbox\']) > 0:\n\n            rbox_lists = np.array(rboxes[rbox_image][0][\'bbox\'])\n            if len(gboxes[rbox_image]) > 0:\n                gbox_list = np.array([obj[\'bbox\'] for obj in gboxes[rbox_image]])\n                box_num = box_num + len(gbox_list)\n                gbox_list = np.concatenate((gbox_list, np.zeros((np.shape(gbox_list)[0], 1))), axis=1)\n                confidence = rbox_lists[:, -1]\n                box_index = np.argsort(-confidence)\n\n                rbox_lists = rbox_lists[box_index, :]\n                for rbox_list in rbox_lists:\n                    if mode == 0:\n                        ixmin = np.maximum(gbox_list[:, 0], rbox_list[0])\n                        iymin = np.maximum(gbox_list[:, 1], rbox_list[1])\n                        ixmax = np.minimum(gbox_list[:, 2], rbox_list[2])\n                        iymax = np.minimum(gbox_list[:, 3], rbox_list[3])\n                        iw = np.maximum(ixmax - ixmin + 1., 0.)\n                        ih = np.maximum(iymax - iymin + 1., 0.)\n                        inters = iw * ih\n                        # union\n                        uni = ((rbox_list[2] - rbox_list[0] + 1.) * (rbox_list[3] - rbox_list[1] + 1.) +\n                               (gbox_list[:, 2] - gbox_list[:, 0] + 1.) *\n                               (gbox_list[:, 3] - gbox_list[:, 1] + 1.) - inters)\n                        overlaps = inters / uni\n                    else:\n                        overlaps = iou_rotate.iou_rotate_calculate1(np.array([rbox_list[:-1]]),\n                                                                    gbox_list,\n                                                                    use_gpu=False)[0]\n\n                    ovmax = np.max(overlaps)\n                    jmax = np.argmax(overlaps)\n                    if ovmax > iou_th:\n                        if gbox_list[jmax, -1] == 0:\n                            tp[i] += 1\n                            gbox_list[jmax, -1] = 1\n                        else:\n                            fp[i] += 1\n                    else:\n                        fp[i] += 1\n            else:\n                fp[i] += len(rboxes[rbox_image][0][\'bbox\'])\n        else:\n            continue\n    rec = np.zeros(len(rbox_images))\n    prec = np.zeros(len(rbox_images))\n    if box_num == 0:\n        for i in range(len(fp)):\n            if fp[i] != 0:\n                prec[i] = 0\n            else:\n                prec[i] = 1\n\n    else:\n\n        fp = np.cumsum(fp)\n        tp = np.cumsum(tp)\n\n        prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n        rec = tp / box_num\n    ap = voc_ap(rec, prec, use_07_metric)\n\n    return rec, prec, ap, box_num\n\n\nif __name__ == \'__main__\':\n    img_num = 1\n    # 0: horizontal standard 1: rotate standard\n    mode = 0\n    eval_ship(img_num, mode)\n\n    fr1 = open(\'gtboxes_horizontal_dict.pkl\', \'r\')\n    fr2 = open(\'predict_horizontal_dict.pkl\', \'r\')\n    fr3 = open(\'gtboxes_rotate_dict.pkl\', \'r\')\n    fr4 = open(\'predict_rotate_dict.pkl\', \'r\')\n    gtboxes_horizontal_dict = pickle.load(fr1)\n    predict_horizontal_dict = pickle.load(fr2)\n    gtboxes_rotate_dict = pickle.load(fr3)\n    predict_rotate_dict = pickle.load(fr4)\n\n    R, P, AP, F, num = [], [], [], [], []\n    R1, P1, AP1, F1, num1 = [], [], [], [], []\n\n    for label in NAME_LABEL_MAP.keys():\n        if label == \'back_ground\':\n            continue\n\n        rboxes, gboxes = get_single_label_dict(predict_horizontal_dict, gtboxes_horizontal_dict, label)\n        rboxes1, gboxes1 = get_single_label_dict(predict_rotate_dict, gtboxes_rotate_dict, label)\n\n        rec, prec, ap, box_num = eval(rboxes, gboxes, 0.5, False, mode=0)\n        rec1, prec1, ap1, box_num1 = eval(rboxes1, gboxes1, 0.5, False, mode=mode)\n\n        recall = rec[-1]\n        recall1 = rec1[-1]\n        precision = prec[-1]\n        precision1 = prec1[-1]\n        F_measure = (2 * precision * recall) / (recall + precision)\n        F_measure1 = (2 * precision1 * recall1) / (recall1 + precision1)\n        print(\'\\n{}\\tR:{}\\tP:{}\\tap:{}\\tF:{}\'.format(label, recall, precision, ap, F_measure))\n        print(\'\\n{}\\tR:{}\\tP:{}\\tap:{}\\tF:{}\'.format(label, recall1, precision1, ap1, F_measure1))\n        R.append(recall)\n        P.append(precision)\n        AP.append(ap)\n        F.append(F_measure)\n        num.append(box_num)\n\n        R1.append(recall1)\n        P1.append(precision1)\n        AP1.append(ap1)\n        F1.append(F_measure1)\n        num1.append(box_num1)\n\n    R = np.array(R)\n    P = np.array(P)\n    AP = np.array(AP)\n    F = np.array(F)\n    num = np.array(num)\n    weights = num / np.sum(num)\n    Recall = np.sum(R * weights)\n    Precision = np.sum(P * weights)\n    mAP = np.sum(AP * weights)\n    F_measure = np.sum(F * weights)\n\n    R1 = np.array(R1)\n    P1 = np.array(P1)\n    AP1 = np.array(AP1)\n    F1 = np.array(F1)\n    num1 = np.array(num1)\n    weights1 = num1 / np.sum(num1)\n    Recall1 = np.sum(R1 * weights1)\n    Precision1 = np.sum(P1 * weights1)\n    mAP1 = np.sum(AP1 * weights1)\n    F_measure1 = np.sum(F1 * weights1)\n\n    print(\'\\n{}\\tR:{}\\tP:{}\\tmAP:{}\\tF:{}\'.format(\'horizontal standard\', Recall, Precision, mAP, F_measure))\n    print(\'\\n{}\\tR:{}\\tP:{}\\tmAP:{}\\tF:{}\'.format(\'rotate standard\', Recall1, Precision1, mAP1, F_measure1))\n\n    fr1.close()\n    fr2.close()\n    fr3.close()\n    fr4.close()\n\n\n\n\n\n\n\n\n\n'"
tools/inference.py,11,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\nimport sys\nsys.path.append(\'../\')\n\nimport time\nfrom data.io import image_preprocess\nfrom libs.networks.network_factory import get_network_byname\nfrom libs.rpn import build_rpn\nfrom help_utils.help_utils import *\nfrom help_utils.tools import *\nfrom libs.configs import cfgs\nfrom tools import restore_model\nfrom libs.fast_rcnn import build_fast_rcnn\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\n\n\ndef get_imgs():\n    mkdir(cfgs.INFERENCE_IMAGE_PATH)\n    root_dir = cfgs.INFERENCE_IMAGE_PATH\n    img_name_list = os.listdir(root_dir)\n    if len(img_name_list) == 0:\n        assert \'no test image in {}!\'.format(cfgs.INFERENCE_IMAGE_PATH)\n    img_list = [cv2.imread(os.path.join(root_dir, img_name))\n                for img_name in img_name_list]\n    return img_list, img_name_list\n\n\ndef inference():\n    with tf.Graph().as_default():\n\n        img_plac = tf.placeholder(shape=[None, None, 3], dtype=tf.uint8)\n\n        img_tensor = tf.cast(img_plac, tf.float32) - tf.constant([103.939, 116.779, 123.68])\n        img_batch = image_preprocess.short_side_resize_for_inference_data(img_tensor,\n                                                                          target_shortside_len=cfgs.SHORT_SIDE_LEN)\n\n        # ***********************************************************************************************\n        # *                                         share net                                           *\n        # ***********************************************************************************************\n        _, share_net = get_network_byname(net_name=cfgs.NET_NAME,\n                                          inputs=img_batch,\n                                          num_classes=None,\n                                          is_training=True,\n                                          output_stride=None,\n                                          global_pool=False,\n                                          spatial_squeeze=False)\n        # ***********************************************************************************************\n        # *                                            RPN                                              *\n        # ***********************************************************************************************\n        rpn = build_rpn.RPN(net_name=cfgs.NET_NAME,\n                            inputs=img_batch,\n                            gtboxes_and_label=None,\n                            is_training=False,\n                            share_head=cfgs.SHARE_HEAD,\n                            share_net=share_net,\n                            stride=cfgs.STRIDE,\n                            anchor_ratios=cfgs.ANCHOR_RATIOS,\n                            anchor_scales=cfgs.ANCHOR_SCALES,\n                            scale_factors=cfgs.SCALE_FACTORS,\n                            base_anchor_size_list=cfgs.BASE_ANCHOR_SIZE_LIST,  # P2, P3, P4, P5, P6\n                            level=cfgs.LEVEL,\n                            top_k_nms=cfgs.RPN_TOP_K_NMS,\n                            rpn_nms_iou_threshold=cfgs.RPN_NMS_IOU_THRESHOLD,\n                            max_proposals_num=cfgs.MAX_PROPOSAL_NUM,\n                            rpn_iou_positive_threshold=cfgs.RPN_IOU_POSITIVE_THRESHOLD,\n                            rpn_iou_negative_threshold=cfgs.RPN_IOU_NEGATIVE_THRESHOLD,\n                            rpn_mini_batch_size=cfgs.RPN_MINIBATCH_SIZE,\n                            rpn_positives_ratio=cfgs.RPN_POSITIVE_RATE,\n                            remove_outside_anchors=False,  # whether remove anchors outside\n                            rpn_weight_decay=cfgs.WEIGHT_DECAY[cfgs.NET_NAME])\n\n        # rpn predict proposals\n        rpn_proposals_boxes, rpn_proposals_scores = rpn.rpn_proposals()  # rpn_score shape: [300, ]\n\n        # ***********************************************************************************************\n        # *                                         Fast RCNN                                           *\n        # ***********************************************************************************************\n        fast_rcnn = build_fast_rcnn.FastRCNN(feature_pyramid=rpn.feature_pyramid,\n                                             rpn_proposals_boxes=rpn_proposals_boxes,\n                                             rpn_proposals_scores=rpn_proposals_scores,\n                                             img_shape=tf.shape(img_batch),\n                                             roi_size=cfgs.ROI_SIZE,\n                                             roi_pool_kernel_size=cfgs.ROI_POOL_KERNEL_SIZE,\n                                             scale_factors=cfgs.SCALE_FACTORS,\n                                             gtboxes_and_label=None,\n                                             gtboxes_and_label_minAreaRectangle=None,\n                                             fast_rcnn_nms_iou_threshold=cfgs.FAST_RCNN_NMS_IOU_THRESHOLD,\n                                             fast_rcnn_maximum_boxes_per_img=100,\n                                             fast_rcnn_nms_max_boxes_per_class=cfgs.FAST_RCNN_NMS_MAX_BOXES_PER_CLASS,\n                                             show_detections_score_threshold=cfgs.FINAL_SCORE_THRESHOLD,\n                                             # show detections which score >= 0.6\n                                             num_classes=cfgs.CLASS_NUM,\n                                             fast_rcnn_minibatch_size=cfgs.FAST_RCNN_MINIBATCH_SIZE,\n                                             fast_rcnn_positives_ratio=cfgs.FAST_RCNN_POSITIVE_RATE,\n                                             fast_rcnn_positives_iou_threshold=cfgs.FAST_RCNN_IOU_POSITIVE_THRESHOLD,\n                                             # iou>0.5 is positive, iou<0.5 is negative\n                                             use_dropout=cfgs.USE_DROPOUT,\n                                             weight_decay=cfgs.WEIGHT_DECAY[cfgs.NET_NAME],\n                                             is_training=False,\n                                             level=cfgs.LEVEL)\n\n        fast_rcnn_decode_boxes, fast_rcnn_score, num_of_objects, detection_category = \\\n            fast_rcnn.fast_rcnn_predict()\n\n        init_op = tf.group(\n            tf.global_variables_initializer(),\n            tf.local_variables_initializer()\n        )\n\n        restorer, restore_ckpt = restore_model.get_restorer()\n\n        config = tf.ConfigProto()\n        # config.gpu_options.per_process_gpu_memory_fraction = 0.5\n        config.gpu_options.allow_growth = True\n        with tf.Session(config=config) as sess:\n            sess.run(init_op)\n            if not restorer is None:\n                restorer.restore(sess, restore_ckpt)\n                print(\'restore model\')\n\n            coord = tf.train.Coordinator()\n            threads = tf.train.start_queue_runners(sess, coord)\n\n            imgs, img_names = get_imgs()\n            for i, img in enumerate(imgs):\n\n                start = time.time()\n\n                _img_batch, _fast_rcnn_decode_boxes, _fast_rcnn_score, _detection_category = \\\n                    sess.run([img_batch, fast_rcnn_decode_boxes, fast_rcnn_score, detection_category],\n                             feed_dict={img_plac: img})\n                end = time.time()\n\n                img_np = np.squeeze(_img_batch, axis=0)\n\n                img_horizontal_np = draw_box_cv(img_np,\n                                                boxes=_fast_rcnn_decode_boxes,\n                                                labels=_detection_category,\n                                                scores=_fast_rcnn_score)\n                mkdir(cfgs.INFERENCE_SAVE_PATH)\n                cv2.imwrite(cfgs.INFERENCE_SAVE_PATH + \'/{}_horizontal_fpn.jpg\'.format(img_names[i]), img_horizontal_np)\n                view_bar(\'{} cost {}s\'.format(img_names[i], (end - start)), i + 1, len(imgs))\n            coord.request_stop()\n            coord.join(threads)\n\nif __name__ == \'__main__\':\n    inference()'"
tools/inference1.py,11,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\nimport sys\nsys.path.append(\'../\')\n\nimport time\nfrom data.io import image_preprocess\nfrom libs.networks.network_factory import get_network_byname\nfrom libs.rpn import build_rpn\nfrom help_utils.help_utils import *\nfrom help_utils.tools import *\nfrom libs.configs import cfgs\nfrom tools import restore_model\nfrom libs.fast_rcnn import build_fast_rcnn1\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\n\n\ndef get_imgs():\n    mkdir(cfgs.INFERENCE_IMAGE_PATH)\n    root_dir = cfgs.INFERENCE_IMAGE_PATH\n    img_name_list = os.listdir(root_dir)\n    if len(img_name_list) == 0:\n        assert \'no test image in {}!\'.format(cfgs.INFERENCE_IMAGE_PATH)\n    img_list = [cv2.imread(os.path.join(root_dir, img_name))\n                for img_name in img_name_list]\n    return img_list, img_name_list\n\n\ndef inference():\n    with tf.Graph().as_default():\n\n        img_plac = tf.placeholder(shape=[None, None, 3], dtype=tf.uint8)\n\n        img_tensor = tf.cast(img_plac, tf.float32) - tf.constant([103.939, 116.779, 123.68])\n        img_batch = image_preprocess.short_side_resize_for_inference_data(img_tensor,\n                                                                          target_shortside_len=cfgs.SHORT_SIDE_LEN)\n\n        # ***********************************************************************************************\n        # *                                         share net                                           *\n        # ***********************************************************************************************\n        _, share_net = get_network_byname(net_name=cfgs.NET_NAME,\n                                          inputs=img_batch,\n                                          num_classes=None,\n                                          is_training=True,\n                                          output_stride=None,\n                                          global_pool=False,\n                                          spatial_squeeze=False)\n        # ***********************************************************************************************\n        # *                                            RPN                                              *\n        # ***********************************************************************************************\n        rpn = build_rpn.RPN(net_name=cfgs.NET_NAME,\n                            inputs=img_batch,\n                            gtboxes_and_label=None,\n                            is_training=False,\n                            share_head=cfgs.SHARE_HEAD,\n                            share_net=share_net,\n                            stride=cfgs.STRIDE,\n                            anchor_ratios=cfgs.ANCHOR_RATIOS,\n                            anchor_scales=cfgs.ANCHOR_SCALES,\n                            scale_factors=cfgs.SCALE_FACTORS,\n                            base_anchor_size_list=cfgs.BASE_ANCHOR_SIZE_LIST,  # P2, P3, P4, P5, P6\n                            level=cfgs.LEVEL,\n                            top_k_nms=cfgs.RPN_TOP_K_NMS,\n                            rpn_nms_iou_threshold=cfgs.RPN_NMS_IOU_THRESHOLD,\n                            max_proposals_num=cfgs.MAX_PROPOSAL_NUM,\n                            rpn_iou_positive_threshold=cfgs.RPN_IOU_POSITIVE_THRESHOLD,\n                            rpn_iou_negative_threshold=cfgs.RPN_IOU_NEGATIVE_THRESHOLD,\n                            rpn_mini_batch_size=cfgs.RPN_MINIBATCH_SIZE,\n                            rpn_positives_ratio=cfgs.RPN_POSITIVE_RATE,\n                            remove_outside_anchors=False,  # whether remove anchors outside\n                            rpn_weight_decay=cfgs.WEIGHT_DECAY[cfgs.NET_NAME])\n\n        # rpn predict proposals\n        rpn_proposals_boxes, rpn_proposals_scores = rpn.rpn_proposals()  # rpn_score shape: [300, ]\n\n        # ***********************************************************************************************\n        # *                                         Fast RCNN                                           *\n        # ***********************************************************************************************\n        fast_rcnn = build_fast_rcnn1.FastRCNN(feature_pyramid=rpn.feature_pyramid,\n                                              rpn_proposals_boxes=rpn_proposals_boxes,\n                                              rpn_proposals_scores=rpn_proposals_scores,\n                                              img_shape=tf.shape(img_batch),\n                                              roi_size=cfgs.ROI_SIZE,\n                                              roi_pool_kernel_size=cfgs.ROI_POOL_KERNEL_SIZE,\n                                              scale_factors=cfgs.SCALE_FACTORS,\n                                              gtboxes_and_label=None,\n                                              gtboxes_and_label_minAreaRectangle=None,\n                                              fast_rcnn_nms_iou_threshold=cfgs.FAST_RCNN_NMS_IOU_THRESHOLD,\n                                              fast_rcnn_maximum_boxes_per_img=100,\n                                              fast_rcnn_nms_max_boxes_per_class=cfgs.FAST_RCNN_NMS_MAX_BOXES_PER_CLASS,\n                                              show_detections_score_threshold=cfgs.FINAL_SCORE_THRESHOLD,\n                                              # show detections which score >= 0.6\n                                              num_classes=cfgs.CLASS_NUM,\n                                              fast_rcnn_minibatch_size=cfgs.FAST_RCNN_MINIBATCH_SIZE,\n                                              fast_rcnn_positives_ratio=cfgs.FAST_RCNN_POSITIVE_RATE,\n                                              fast_rcnn_positives_iou_threshold=cfgs.FAST_RCNN_IOU_POSITIVE_THRESHOLD,\n                                              # iou>0.5 is positive, iou<0.5 is negative\n                                              use_dropout=cfgs.USE_DROPOUT,\n                                              weight_decay=cfgs.WEIGHT_DECAY[cfgs.NET_NAME],\n                                              is_training=False,\n                                              level=cfgs.LEVEL)\n\n        fast_rcnn_decode_boxes, fast_rcnn_score, num_of_objects, detection_category, \\\n        fast_rcnn_decode_boxes_rotate, fast_rcnn_score_rotate, num_of_objects_rotate, detection_category_rotate = \\\n            fast_rcnn.fast_rcnn_predict()\n\n        init_op = tf.group(\n            tf.global_variables_initializer(),\n            tf.local_variables_initializer()\n        )\n\n        restorer, restore_ckpt = restore_model.get_restorer()\n\n        config = tf.ConfigProto()\n        # config.gpu_options.per_process_gpu_memory_fraction = 0.5\n        config.gpu_options.allow_growth = True\n        with tf.Session(config=config) as sess:\n            sess.run(init_op)\n            if not restorer is None:\n                restorer.restore(sess, restore_ckpt)\n                print(\'restore model\')\n\n            coord = tf.train.Coordinator()\n            threads = tf.train.start_queue_runners(sess, coord)\n\n            imgs, img_names = get_imgs()\n            for i, img in enumerate(imgs):\n\n                start = time.time()\n\n                _img_batch, _fast_rcnn_decode_boxes, _fast_rcnn_score, _detection_category, \\\n                _fast_rcnn_decode_boxes_rotate,  _fast_rcnn_score_rotate, _detection_category_rotate = \\\n                    sess.run([img_batch, fast_rcnn_decode_boxes, fast_rcnn_score, detection_category,\n                              fast_rcnn_decode_boxes_rotate, fast_rcnn_score_rotate, detection_category_rotate],\n                             feed_dict={img_plac: img})\n                end = time.time()\n\n                img_np = np.squeeze(_img_batch, axis=0)\n\n                img_horizontal_np = draw_box_cv(img_np,\n                                                boxes=_fast_rcnn_decode_boxes,\n                                                labels=_detection_category,\n                                                scores=_fast_rcnn_score)\n\n                img_rotate_np = draw_rotate_box_cv(img_np,\n                                                   boxes=_fast_rcnn_decode_boxes_rotate,\n                                                   labels=_detection_category_rotate,\n                                                   scores=_fast_rcnn_score_rotate)\n                mkdir(cfgs.INFERENCE_SAVE_PATH)\n                cv2.imwrite(cfgs.INFERENCE_SAVE_PATH + \'/{}_horizontal_fpn.jpg\'.format(img_names[i]), img_horizontal_np)\n                cv2.imwrite(cfgs.INFERENCE_SAVE_PATH + \'/{}_rotate_fpn.jpg\'.format(img_names[i]), img_rotate_np)\n                view_bar(\'{} cost {}s\'.format(img_names[i], (end - start)), i + 1, len(imgs))\n            coord.request_stop()\n            coord.join(threads)\n\nif __name__ == \'__main__\':\n    inference()'"
tools/restore_model.py,4,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport os\n\nfrom libs.configs import cfgs\nfrom libs.networks.network_factory import get_flags_byname\n\n\nRESTORE_FROM_RPN = False\nFLAGS = get_flags_byname(cfgs.NET_NAME)\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""6""\n\n\ndef get_restorer():\n\n    checkpoint_path = tf.train.latest_checkpoint(os.path.join(FLAGS.trained_checkpoint, cfgs.VERSION))\n\n    if checkpoint_path != None:\n        if RESTORE_FROM_RPN:\n            print(\'___restore from rpn___\')\n            model_variables = slim.get_model_variables()\n            restore_variables = [var for var in model_variables if not var.name.startswith(\'Fast_Rcnn\')] + [slim.get_or_create_global_step()]\n            for var in restore_variables:\n                print(var.name)\n            restorer = tf.train.Saver(restore_variables)\n        else:\n            restorer = tf.train.Saver()\n        print(""model restore from :"", checkpoint_path)\n    else:\n        checkpoint_path = FLAGS.pretrained_model_path\n        print(""model restore from pretrained mode, path is :"", checkpoint_path)\n\n        model_variables = slim.get_model_variables()\n\n        restore_variables = [var for var in model_variables\n                             if (var.name.startswith(cfgs.NET_NAME)\n                                 and not var.name.startswith(\'{}/logits\'.format(cfgs.NET_NAME)))]\n        for var in restore_variables:\n            print(var.name)\n        restorer = tf.train.Saver(restore_variables)\n    return restorer, checkpoint_path'"
tools/test.py,14,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\nimport sys\nsys.path.append(\'../\')\n\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport time\nfrom data.io.read_tfrecord import next_batch\nfrom libs.configs import cfgs\nfrom libs.networks.network_factory import get_network_byname\nfrom help_utils.tools import *\nfrom libs.rpn import build_rpn\nimport cv2\nfrom help_utils import help_utils\nfrom tools import restore_model\nfrom libs.box_utils.coordinate_convert import back_forward_convert\nfrom libs.box_utils.boxes_utils import get_horizen_minAreaRectangle\nfrom libs.fast_rcnn import build_fast_rcnn\n\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = \'1\'\n\n\ndef test(img_num):\n    with tf.Graph().as_default():\n\n        img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch = \\\n            next_batch(dataset_name=cfgs.DATASET_NAME,\n                       batch_size=cfgs.BATCH_SIZE,\n                       shortside_len=cfgs.SHORT_SIDE_LEN,\n                       is_training=False)\n\n        gtboxes_and_label = tf.py_func(back_forward_convert,\n                                       inp=[tf.squeeze(gtboxes_and_label_batch, 0)],\n                                       Tout=tf.float32)\n        gtboxes_and_label = tf.reshape(gtboxes_and_label, [-1, 6])\n\n        gtboxes_and_label_minAreaRectangle = get_horizen_minAreaRectangle(gtboxes_and_label)\n\n        gtboxes_and_label_minAreaRectangle = tf.reshape(gtboxes_and_label_minAreaRectangle, [-1, 5])\n\n        # ***********************************************************************************************\n        # *                                         share net                                           *\n        # ***********************************************************************************************\n        _, share_net = get_network_byname(net_name=cfgs.NET_NAME,\n                                          inputs=img_batch,\n                                          num_classes=None,\n                                          is_training=True,\n                                          output_stride=None,\n                                          global_pool=False,\n                                          spatial_squeeze=False)\n\n        # ***********************************************************************************************\n        # *                                            RPN                                              *\n        # ***********************************************************************************************\n        rpn = build_rpn.RPN(net_name=cfgs.NET_NAME,\n                            inputs=img_batch,\n                            gtboxes_and_label=None,\n                            is_training=False,\n                            share_head=cfgs.SHARE_HEAD,\n                            share_net=share_net,\n                            stride=cfgs.STRIDE,\n                            anchor_ratios=cfgs.ANCHOR_RATIOS,\n                            anchor_scales=cfgs.ANCHOR_SCALES,\n                            scale_factors=cfgs.SCALE_FACTORS,\n                            base_anchor_size_list=cfgs.BASE_ANCHOR_SIZE_LIST,  # P2, P3, P4, P5, P6\n                            level=cfgs.LEVEL,\n                            top_k_nms=cfgs.RPN_TOP_K_NMS,\n                            rpn_nms_iou_threshold=cfgs.RPN_NMS_IOU_THRESHOLD,\n                            max_proposals_num=cfgs.MAX_PROPOSAL_NUM,\n                            rpn_iou_positive_threshold=cfgs.RPN_IOU_POSITIVE_THRESHOLD,\n                            rpn_iou_negative_threshold=cfgs.RPN_IOU_NEGATIVE_THRESHOLD,\n                            rpn_mini_batch_size=cfgs.RPN_MINIBATCH_SIZE,\n                            rpn_positives_ratio=cfgs.RPN_POSITIVE_RATE,\n                            remove_outside_anchors=False,  # whether remove anchors outside\n                            rpn_weight_decay=cfgs.WEIGHT_DECAY[cfgs.NET_NAME])\n\n        # rpn predict proposals\n        rpn_proposals_boxes, rpn_proposals_scores = rpn.rpn_proposals()  # rpn_score shape: [300, ]\n\n        # ***********************************************************************************************\n        # *                                         Fast RCNN                                           *\n        # ***********************************************************************************************\n        fast_rcnn = build_fast_rcnn.FastRCNN(feature_pyramid=rpn.feature_pyramid,\n                                             rpn_proposals_boxes=rpn_proposals_boxes,\n                                             rpn_proposals_scores=rpn_proposals_scores,\n                                             img_shape=tf.shape(img_batch),\n                                             roi_size=cfgs.ROI_SIZE,\n                                             roi_pool_kernel_size=cfgs.ROI_POOL_KERNEL_SIZE,\n                                             scale_factors=cfgs.SCALE_FACTORS,\n                                             gtboxes_and_label=None,\n                                             gtboxes_and_label_minAreaRectangle=gtboxes_and_label_minAreaRectangle,\n                                             fast_rcnn_nms_iou_threshold=cfgs.FAST_RCNN_NMS_IOU_THRESHOLD,\n                                             fast_rcnn_maximum_boxes_per_img=100,\n                                             fast_rcnn_nms_max_boxes_per_class=cfgs.FAST_RCNN_NMS_MAX_BOXES_PER_CLASS,\n                                             show_detections_score_threshold=cfgs.FINAL_SCORE_THRESHOLD,\n                                             # show detections which score >= 0.6\n                                             num_classes=cfgs.CLASS_NUM,\n                                             fast_rcnn_minibatch_size=cfgs.FAST_RCNN_MINIBATCH_SIZE,\n                                             fast_rcnn_positives_ratio=cfgs.FAST_RCNN_POSITIVE_RATE,\n                                             fast_rcnn_positives_iou_threshold=cfgs.FAST_RCNN_IOU_POSITIVE_THRESHOLD,\n                                             # iou>0.5 is positive, iou<0.5 is negative\n                                             use_dropout=cfgs.USE_DROPOUT,\n                                             weight_decay=cfgs.WEIGHT_DECAY[cfgs.NET_NAME],\n                                             is_training=False,\n                                             level=cfgs.LEVEL)\n\n        fast_rcnn_decode_boxes, fast_rcnn_score, num_of_objects, detection_category = fast_rcnn.fast_rcnn_predict()\n\n        # train\n        init_op = tf.group(\n            tf.global_variables_initializer(),\n            tf.local_variables_initializer()\n        )\n\n        restorer, restore_ckpt = restore_model.get_restorer()\n\n        config = tf.ConfigProto()\n        # config.gpu_options.per_process_gpu_memory_fraction = 0.5\n        config.gpu_options.allow_growth = True\n        with tf.Session(config=config) as sess:\n            sess.run(init_op)\n            if not restorer is None:\n                restorer.restore(sess, restore_ckpt)\n                print(\'restore model\')\n\n            coord = tf.train.Coordinator()\n            threads = tf.train.start_queue_runners(sess, coord)\n\n            for i in range(img_num):\n\n                start = time.time()\n\n                _img_name_batch, _img_batch, _gtboxes_and_label, _gtboxes_and_label_minAreaRectangle, \\\n                _fast_rcnn_decode_boxes, _fast_rcnn_score, _detection_category \\\n                    = sess.run([img_name_batch, img_batch, gtboxes_and_label, gtboxes_and_label_minAreaRectangle,\n                                fast_rcnn_decode_boxes, fast_rcnn_score, detection_category])\n                end = time.time()\n\n                _img_batch = np.squeeze(_img_batch, axis=0)\n\n                _img_batch_fpn_horizonal = help_utils.draw_box_cv(_img_batch,\n                                                                  boxes=_fast_rcnn_decode_boxes,\n                                                                  labels=_detection_category,\n                                                                  scores=_fast_rcnn_score)\n                mkdir(cfgs.TEST_SAVE_PATH)\n                cv2.imwrite(cfgs.TEST_SAVE_PATH + \'/{}_horizontal_fpn.jpg\'.format(str(_img_name_batch[0])), _img_batch_fpn_horizonal)\n\n                temp_label_horizontal = np.reshape(_gtboxes_and_label[:, -1:], [-1, ]).astype(np.int64)\n\n                _img_batch_gt_horizontal = help_utils.draw_box_cv(_img_batch,\n                                                                  boxes=_gtboxes_and_label_minAreaRectangle[:, :-1],\n                                                                  labels=temp_label_horizontal,\n                                                                  scores=None)\n\n                cv2.imwrite(cfgs.TEST_SAVE_PATH + \'/{}_horizontal_gt.jpg\'.format(str(_img_name_batch[0])), _img_batch_gt_horizontal)\n\n                view_bar(\'{} image cost {}s\'.format(str(_img_name_batch[0]), (end - start)), i + 1, img_num)\n\n            coord.request_stop()\n            coord.join(threads)\n\n\nif __name__ == \'__main__\':\n    img_num = 2000\n    test(img_num)\n\n\n\n\n\n\n\n\n\n\n'"
tools/test1.py,14,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\nimport sys\nsys.path.append(\'../\')\n\nimport tensorflow as tf\nimport numpy as np\nimport time\nfrom data.io.read_tfrecord import next_batch\nfrom libs.configs import cfgs\nfrom libs.networks.network_factory import get_network_byname\nfrom help_utils.tools import *\nfrom libs.rpn import build_rpn\nimport cv2\nfrom help_utils import help_utils\nfrom tools import restore_model\nfrom libs.box_utils.coordinate_convert import back_forward_convert\nfrom libs.box_utils.boxes_utils import get_horizen_minAreaRectangle\nfrom libs.fast_rcnn import build_fast_rcnn1\n\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = \'1\'\n\n\ndef test(img_num):\n    with tf.Graph().as_default():\n\n        img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch = \\\n            next_batch(dataset_name=cfgs.DATASET_NAME,\n                       batch_size=cfgs.BATCH_SIZE,\n                       shortside_len=cfgs.SHORT_SIDE_LEN,\n                       is_training=False)\n\n        gtboxes_and_label = tf.py_func(back_forward_convert,\n                                       inp=[tf.squeeze(gtboxes_and_label_batch, 0)],\n                                       Tout=tf.float32)\n        gtboxes_and_label = tf.reshape(gtboxes_and_label, [-1, 6])\n\n        gtboxes_and_label_minAreaRectangle = get_horizen_minAreaRectangle(gtboxes_and_label)\n\n        gtboxes_and_label_minAreaRectangle = tf.reshape(gtboxes_and_label_minAreaRectangle, [-1, 5])\n\n        # ***********************************************************************************************\n        # *                                         share net                                           *\n        # ***********************************************************************************************\n        _, share_net = get_network_byname(net_name=cfgs.NET_NAME,\n                                          inputs=img_batch,\n                                          num_classes=None,\n                                          is_training=True,\n                                          output_stride=None,\n                                          global_pool=False,\n                                          spatial_squeeze=False)\n\n        # ***********************************************************************************************\n        # *                                            RPN                                              *\n        # ***********************************************************************************************\n        rpn = build_rpn.RPN(net_name=cfgs.NET_NAME,\n                            inputs=img_batch,\n                            gtboxes_and_label=None,\n                            is_training=False,\n                            share_head=cfgs.SHARE_HEAD,\n                            share_net=share_net,\n                            stride=cfgs.STRIDE,\n                            anchor_ratios=cfgs.ANCHOR_RATIOS,\n                            anchor_scales=cfgs.ANCHOR_SCALES,\n                            scale_factors=cfgs.SCALE_FACTORS,\n                            base_anchor_size_list=cfgs.BASE_ANCHOR_SIZE_LIST,  # P2, P3, P4, P5, P6\n                            level=cfgs.LEVEL,\n                            top_k_nms=cfgs.RPN_TOP_K_NMS,\n                            rpn_nms_iou_threshold=cfgs.RPN_NMS_IOU_THRESHOLD,\n                            max_proposals_num=cfgs.MAX_PROPOSAL_NUM,\n                            rpn_iou_positive_threshold=cfgs.RPN_IOU_POSITIVE_THRESHOLD,\n                            rpn_iou_negative_threshold=cfgs.RPN_IOU_NEGATIVE_THRESHOLD,\n                            rpn_mini_batch_size=cfgs.RPN_MINIBATCH_SIZE,\n                            rpn_positives_ratio=cfgs.RPN_POSITIVE_RATE,\n                            remove_outside_anchors=False,  # whether remove anchors outside\n                            rpn_weight_decay=cfgs.WEIGHT_DECAY[cfgs.NET_NAME])\n\n        # rpn predict proposals\n        rpn_proposals_boxes, rpn_proposals_scores = rpn.rpn_proposals()  # rpn_score shape: [300, ]\n\n        # ***********************************************************************************************\n        # *                                         Fast RCNN                                           *\n        # ***********************************************************************************************\n        fast_rcnn = build_fast_rcnn1.FastRCNN(feature_pyramid=rpn.feature_pyramid,\n                                              rpn_proposals_boxes=rpn_proposals_boxes,\n                                              rpn_proposals_scores=rpn_proposals_scores,\n                                              img_shape=tf.shape(img_batch),\n                                              roi_size=cfgs.ROI_SIZE,\n                                              roi_pool_kernel_size=cfgs.ROI_POOL_KERNEL_SIZE,\n                                              scale_factors=cfgs.SCALE_FACTORS,\n                                              gtboxes_and_label=None,\n                                              gtboxes_and_label_minAreaRectangle=gtboxes_and_label_minAreaRectangle,\n                                              fast_rcnn_nms_iou_threshold=cfgs.FAST_RCNN_NMS_IOU_THRESHOLD,\n                                              fast_rcnn_maximum_boxes_per_img=100,\n                                              fast_rcnn_nms_max_boxes_per_class=cfgs.FAST_RCNN_NMS_MAX_BOXES_PER_CLASS,\n                                              show_detections_score_threshold=cfgs.FINAL_SCORE_THRESHOLD,\n                                              # show detections which score >= 0.6\n                                              num_classes=cfgs.CLASS_NUM,\n                                              fast_rcnn_minibatch_size=cfgs.FAST_RCNN_MINIBATCH_SIZE,\n                                              fast_rcnn_positives_ratio=cfgs.FAST_RCNN_POSITIVE_RATE,\n                                              fast_rcnn_positives_iou_threshold=cfgs.FAST_RCNN_IOU_POSITIVE_THRESHOLD,\n                                              # iou>0.5 is positive, iou<0.5 is negative\n                                              use_dropout=cfgs.USE_DROPOUT,\n                                              weight_decay=cfgs.WEIGHT_DECAY[cfgs.NET_NAME],\n                                              is_training=False,\n                                              level=cfgs.LEVEL)\n\n        fast_rcnn_decode_boxes, fast_rcnn_score, num_of_objects, detection_category, \\\n        fast_rcnn_decode_boxes_rotate, fast_rcnn_score_rotate, num_of_objects_rotate, detection_category_rotate = \\\n            fast_rcnn.fast_rcnn_predict()\n\n        # train\n        init_op = tf.group(\n            tf.global_variables_initializer(),\n            tf.local_variables_initializer()\n        )\n\n        restorer, restore_ckpt = restore_model.get_restorer()\n\n        config = tf.ConfigProto()\n        # config.gpu_options.per_process_gpu_memory_fraction = 0.5\n        config.gpu_options.allow_growth = True\n        with tf.Session(config=config) as sess:\n            sess.run(init_op)\n            if not restorer is None:\n                restorer.restore(sess, restore_ckpt)\n                print(\'restore model\')\n\n            coord = tf.train.Coordinator()\n            threads = tf.train.start_queue_runners(sess, coord)\n\n            for i in range(img_num):\n\n                start = time.time()\n\n                _img_name_batch, _img_batch, _gtboxes_and_label, _gtboxes_and_label_minAreaRectangle, \\\n                _fast_rcnn_decode_boxes, _fast_rcnn_score, _detection_category, _fast_rcnn_decode_boxes_rotate, \\\n                _fast_rcnn_score_rotate, _detection_category_rotate \\\n                    = sess.run([img_name_batch, img_batch, gtboxes_and_label, gtboxes_and_label_minAreaRectangle,\n                                fast_rcnn_decode_boxes, fast_rcnn_score, detection_category, fast_rcnn_decode_boxes_rotate,\n                                fast_rcnn_score_rotate, detection_category_rotate])\n                end = time.time()\n\n                _img_batch = np.squeeze(_img_batch, axis=0)\n\n                _img_batch_fpn_horizonal = help_utils.draw_box_cv(_img_batch,\n                                                                  boxes=_fast_rcnn_decode_boxes,\n                                                                  labels=_detection_category,\n                                                                  scores=_fast_rcnn_score)\n\n                _img_batch_fpn_rotate = help_utils.draw_rotate_box_cv(_img_batch,\n                                                                      boxes=_fast_rcnn_decode_boxes_rotate,\n                                                                      labels=_detection_category_rotate,\n                                                                      scores=_fast_rcnn_score_rotate)\n                mkdir(cfgs.TEST_SAVE_PATH)\n                cv2.imwrite(cfgs.TEST_SAVE_PATH + \'/{}_horizontal_fpn.jpg\'.format(str(_img_name_batch[0])), _img_batch_fpn_horizonal)\n                cv2.imwrite(cfgs.TEST_SAVE_PATH + \'/{}_rotate_fpn.jpg\'.format(str(_img_name_batch[0])), _img_batch_fpn_rotate)\n\n                temp_label_horizontal = np.reshape(_gtboxes_and_label[:, -1:], [-1, ]).astype(np.int64)\n                temp_label_rotate = np.reshape(_gtboxes_and_label[:, -1:], [-1, ]).astype(np.int64)\n\n                _img_batch_gt_horizontal = help_utils.draw_box_cv(_img_batch,\n                                                                  boxes=_gtboxes_and_label_minAreaRectangle[:, :-1],\n                                                                  labels=temp_label_horizontal,\n                                                                  scores=None)\n\n                _img_batch_gt_rotate = help_utils.draw_rotate_box_cv(_img_batch,\n                                                                     boxes=_gtboxes_and_label[:, :-1],\n                                                                     labels=temp_label_rotate,\n                                                                     scores=None)\n\n                cv2.imwrite(cfgs.TEST_SAVE_PATH + \'/{}_horizontal_gt.jpg\'.format(str(_img_name_batch[0])), _img_batch_gt_horizontal)\n                cv2.imwrite(cfgs.TEST_SAVE_PATH + \'/{}_rotate_gt.jpg\'.format(str(_img_name_batch[0])), _img_batch_gt_rotate)\n\n                view_bar(\'{} image cost {}s\'.format(str(_img_name_batch[0]), (end - start)), i + 1, img_num)\n\n            coord.request_stop()\n            coord.join(threads)\n\n\nif __name__ == \'__main__\':\n    img_num = 2000\n    test(img_num)\n\n\n\n\n\n\n\n\n\n\n'"
tools/train.py,42,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\nimport sys\nsys.path.append(\'../\')\n\nimport tensorflow.contrib.slim as slim\nimport time\nfrom data.io.read_tfrecord import next_batch\nfrom libs.networks.network_factory import get_flags_byname\nfrom libs.networks.network_factory import get_network_byname\nfrom libs.configs import cfgs\nfrom libs.rpn import build_rpn\nfrom libs.fast_rcnn import build_fast_rcnn\nfrom help_utils.tools import *\nfrom libs.box_utils.show_box_in_tensor import *\nfrom tools import restore_model\nfrom libs.box_utils.coordinate_convert import back_forward_convert\nfrom libs.box_utils.boxes_utils import get_horizen_minAreaRectangle\n\n\nRESTORE_FROM_RPN = False\nFLAGS = get_flags_byname(cfgs.NET_NAME)\nos.environ[""CUDA_VISIBLE_DEVICES""] = cfgs.GPU_GROUP\n\n\ndef train():\n    with tf.Graph().as_default():\n        with tf.name_scope(\'get_batch\'):\n            img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch = \\\n                next_batch(dataset_name=cfgs.DATASET_NAME,\n                           batch_size=cfgs.BATCH_SIZE,\n                           shortside_len=cfgs.SHORT_SIDE_LEN,\n                           is_training=True)\n            gtboxes_and_label = tf.py_func(back_forward_convert,\n                                           inp=[tf.squeeze(gtboxes_and_label_batch, 0)],\n                                           Tout=tf.float32)\n            gtboxes_and_label = tf.reshape(gtboxes_and_label, [-1, 6])\n\n            gtboxes_and_label_minAreaRectangle = get_horizen_minAreaRectangle(gtboxes_and_label)\n\n            gtboxes_and_label_minAreaRectangle = tf.reshape(gtboxes_and_label_minAreaRectangle, [-1, 5])\n\n        with tf.name_scope(\'draw_gtboxes\'):\n            gtboxes_in_img = draw_box_with_color(img_batch, tf.reshape(gtboxes_and_label_minAreaRectangle, [-1, 5])[:, :-1],\n                                                 text=tf.shape(gtboxes_and_label_minAreaRectangle)[0])\n\n        # ***********************************************************************************************\n        # *                                         share net                                           *\n        # ***********************************************************************************************\n        _, share_net = get_network_byname(net_name=cfgs.NET_NAME,\n                                          inputs=img_batch,\n                                          num_classes=None,\n                                          is_training=True,\n                                          output_stride=None,\n                                          global_pool=False,\n                                          spatial_squeeze=False)\n\n        # ***********************************************************************************************\n        # *                                            rpn                                              *\n        # ***********************************************************************************************\n        rpn = build_rpn.RPN(net_name=cfgs.NET_NAME,\n                            inputs=img_batch,\n                            gtboxes_and_label=gtboxes_and_label_minAreaRectangle,\n                            is_training=True,\n                            share_head=cfgs.SHARE_HEAD,\n                            share_net=share_net,\n                            stride=cfgs.STRIDE,\n                            anchor_ratios=cfgs.ANCHOR_RATIOS,\n                            anchor_scales=cfgs.ANCHOR_SCALES,\n                            scale_factors=cfgs.SCALE_FACTORS,\n                            base_anchor_size_list=cfgs.BASE_ANCHOR_SIZE_LIST,  # P2, P3, P4, P5, P6\n                            level=cfgs.LEVEL,\n                            top_k_nms=cfgs.RPN_TOP_K_NMS,\n                            rpn_nms_iou_threshold=cfgs.RPN_NMS_IOU_THRESHOLD,\n                            max_proposals_num=cfgs.MAX_PROPOSAL_NUM,\n                            rpn_iou_positive_threshold=cfgs.RPN_IOU_POSITIVE_THRESHOLD,\n                            rpn_iou_negative_threshold=cfgs.RPN_IOU_NEGATIVE_THRESHOLD,  # iou>=0.7 is positive box, iou< 0.3 is negative\n                            rpn_mini_batch_size=cfgs.RPN_MINIBATCH_SIZE,\n                            rpn_positives_ratio=cfgs.RPN_POSITIVE_RATE,\n                            remove_outside_anchors=False,  # whether remove anchors outside\n                            rpn_weight_decay=cfgs.WEIGHT_DECAY[cfgs.NET_NAME])\n\n        rpn_proposals_boxes, rpn_proposals_scores = rpn.rpn_proposals()  # rpn_score shape: [300, ]\n\n        rpn_location_loss, rpn_classification_loss = rpn.rpn_losses()\n        rpn_total_loss = rpn_classification_loss + rpn_location_loss\n\n        with tf.name_scope(\'draw_proposals\'):\n            # score > 0.5 is object\n            rpn_object_boxes_indices = tf.reshape(tf.where(tf.greater(rpn_proposals_scores, 0.5)), [-1])\n            rpn_object_boxes = tf.gather(rpn_proposals_boxes, rpn_object_boxes_indices)\n\n            rpn_proposals_objcet_boxes_in_img = draw_box_with_color(img_batch, rpn_object_boxes,\n                                                                    text=tf.shape(rpn_object_boxes)[0])\n            rpn_proposals_boxes_in_img = draw_box_with_color(img_batch, rpn_proposals_boxes,\n                                                             text=tf.shape(rpn_proposals_boxes)[0])\n        # ***********************************************************************************************\n        # *                                         Fast RCNN                                           *\n        # ***********************************************************************************************\n\n        fast_rcnn = build_fast_rcnn.FastRCNN(feature_pyramid=rpn.feature_pyramid,\n                                             rpn_proposals_boxes=rpn_proposals_boxes,\n                                             rpn_proposals_scores=rpn_proposals_scores,\n                                             img_shape=tf.shape(img_batch),\n                                             roi_size=cfgs.ROI_SIZE,\n                                             roi_pool_kernel_size=cfgs.ROI_POOL_KERNEL_SIZE,\n                                             scale_factors=cfgs.SCALE_FACTORS,\n                                             gtboxes_and_label=gtboxes_and_label,\n                                             gtboxes_and_label_minAreaRectangle=gtboxes_and_label_minAreaRectangle,\n                                             fast_rcnn_nms_iou_threshold=cfgs.FAST_RCNN_NMS_IOU_THRESHOLD,\n                                             fast_rcnn_maximum_boxes_per_img=100,\n                                             fast_rcnn_nms_max_boxes_per_class=cfgs.FAST_RCNN_NMS_MAX_BOXES_PER_CLASS,\n                                             show_detections_score_threshold=cfgs.FINAL_SCORE_THRESHOLD,  # show detections which score >= 0.6\n                                             num_classes=cfgs.CLASS_NUM,\n                                             fast_rcnn_minibatch_size=cfgs.FAST_RCNN_MINIBATCH_SIZE,\n                                             fast_rcnn_positives_ratio=cfgs.FAST_RCNN_POSITIVE_RATE,\n                                             fast_rcnn_positives_iou_threshold=cfgs.FAST_RCNN_IOU_POSITIVE_THRESHOLD,  # iou>0.5 is positive, iou<0.5 is negative\n                                             use_dropout=cfgs.USE_DROPOUT,\n                                             weight_decay=cfgs.WEIGHT_DECAY[cfgs.NET_NAME],\n                                             is_training=True,\n                                             level=cfgs.LEVEL)\n\n        fast_rcnn_decode_boxes, fast_rcnn_score, num_of_objects, detection_category = \\\n            fast_rcnn.fast_rcnn_predict()\n        fast_rcnn_location_loss, fast_rcnn_classification_loss = fast_rcnn.fast_rcnn_loss()\n        fast_rcnn_total_loss = fast_rcnn_location_loss + fast_rcnn_classification_loss\n\n        with tf.name_scope(\'draw_boxes_with_categories\'):\n            fast_rcnn_predict_boxes_in_imgs = draw_boxes_with_categories(img_batch=img_batch,\n                                                                         boxes=fast_rcnn_decode_boxes,\n                                                                         labels=detection_category,\n                                                                         scores=fast_rcnn_score)\n\n        # train\n        total_loss = slim.losses.get_total_loss()\n\n        global_step = slim.get_or_create_global_step()\n\n        lr = tf.train.piecewise_constant(global_step,\n                                         boundaries=[np.int64(20000), np.int64(40000)],\n                                         values=[cfgs.LR, cfgs.LR/10, cfgs.LR/100])\n        tf.summary.scalar(\'lr\', lr)\n        optimizer = tf.train.MomentumOptimizer(lr, momentum=cfgs.MOMENTUM)\n\n        train_op = slim.learning.create_train_op(total_loss, optimizer, global_step)  # rpn_total_loss,\n        # train_op = optimizer.minimize(second_classification_loss, global_step)\n\n        # ***********************************************************************************************\n        # *                                          Summary                                            *\n        # ***********************************************************************************************\n        # ground truth and predict\n        tf.summary.image(\'img/gtboxes\', gtboxes_in_img)\n        tf.summary.image(\'img/faster_rcnn_predict\', fast_rcnn_predict_boxes_in_imgs)\n        # rpn loss and image\n        tf.summary.scalar(\'rpn/rpn_location_loss\', rpn_location_loss)\n        tf.summary.scalar(\'rpn/rpn_classification_loss\', rpn_classification_loss)\n        tf.summary.scalar(\'rpn/rpn_total_loss\', rpn_total_loss)\n\n        tf.summary.scalar(\'fast_rcnn/fast_rcnn_location_loss\', fast_rcnn_location_loss)\n        tf.summary.scalar(\'fast_rcnn/fast_rcnn_classification_loss\', fast_rcnn_classification_loss)\n        tf.summary.scalar(\'fast_rcnn/fast_rcnn_total_loss\', fast_rcnn_total_loss)\n\n        tf.summary.scalar(\'loss/total_loss\', total_loss)\n\n        tf.summary.image(\'rpn/rpn_all_boxes\', rpn_proposals_boxes_in_img)\n        tf.summary.image(\'rpn/rpn_object_boxes\', rpn_proposals_objcet_boxes_in_img)\n        # learning_rate\n        tf.summary.scalar(\'learning_rate\', lr)\n\n        summary_op = tf.summary.merge_all()\n        init_op = tf.group(\n            tf.global_variables_initializer(),\n            tf.local_variables_initializer()\n        )\n\n        restorer, restore_ckpt = restore_model.get_restorer()\n        saver = tf.train.Saver(max_to_keep=10)\n\n        config = tf.ConfigProto()\n        # config.gpu_options.per_process_gpu_memory_fraction = 0.5\n        config.gpu_options.allow_growth = True\n        with tf.Session(config=config) as sess:\n            sess.run(init_op)\n            if not restorer is None:\n                restorer.restore(sess, restore_ckpt)\n                print(\'restore model\')\n            coord = tf.train.Coordinator()\n            threads = tf.train.start_queue_runners(sess, coord)\n\n            summary_path = os.path.join(FLAGS.summary_path, cfgs.VERSION)\n            mkdir(summary_path)\n            summary_writer = tf.summary.FileWriter(summary_path, graph=sess.graph)\n\n            for step in range(cfgs.MAX_ITERATION):\n                training_time = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(time.time()))\n                start = time.time()\n\n                _global_step, _img_name_batch, _rpn_location_loss, _rpn_classification_loss, \\\n                _rpn_total_loss, _fast_rcnn_location_loss, _fast_rcnn_classification_loss, \\\n                _fast_rcnn_total_loss, _total_loss, _ = \\\n                    sess.run([global_step, img_name_batch, rpn_location_loss, rpn_classification_loss,\n                              rpn_total_loss, fast_rcnn_location_loss, fast_rcnn_classification_loss,\n                              fast_rcnn_total_loss, total_loss, train_op])\n                end = time.time()\n\n                if step % 10 == 0:\n                    print("""""" {}: step{}    image_name:{} |\\t\n                                rpn_loc_loss:{} |\\t rpn_cla_loss:{} |\\t rpn_total_loss:{} |\n                                fast_rcnn_loc_loss:{} |\\t fast_rcnn_cla_loss:{} |\\t fast_rcnn_total_loss:{} |\n                                total_loss:{} |\\t pre_cost_time:{}s"""""" \\\n                          .format(training_time, _global_step, str(_img_name_batch[0]), _rpn_location_loss,\n                                  _rpn_classification_loss, _rpn_total_loss, _fast_rcnn_location_loss,\n                                  _fast_rcnn_classification_loss, _fast_rcnn_total_loss, _total_loss,\n                                  (end - start)))\n\n                if step % 50 == 0:\n                    summary_str = sess.run(summary_op)\n                    summary_writer.add_summary(summary_str, _global_step)\n                    summary_writer.flush()\n\n                if (step > 0 and step % 1000 == 0) or (step == cfgs.MAX_ITERATION - 1):\n                    save_dir = os.path.join(FLAGS.trained_checkpoint, cfgs.VERSION)\n                    mkdir(save_dir)\n\n                    save_ckpt = os.path.join(save_dir, \'voc_\'+str(_global_step)+\'model.ckpt\')\n                    saver.save(sess, save_ckpt)\n                    print(\' weights had been saved\')\n\n            coord.request_stop()\n            coord.join(threads)\n\n\nif __name__ == \'__main__\':\n\n    train()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tools/train1.py,48,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\nimport sys\nsys.path.append(\'../\')\n\nimport tensorflow.contrib.slim as slim\nimport time\nfrom data.io.read_tfrecord import next_batch\nfrom libs.networks.network_factory import get_flags_byname\nfrom libs.networks.network_factory import get_network_byname\nfrom libs.configs import cfgs\nfrom libs.rpn import build_rpn\nfrom libs.fast_rcnn import build_fast_rcnn1\nfrom help_utils.tools import *\nfrom libs.box_utils.show_box_in_tensor import *\nfrom tools import restore_model\nfrom libs.box_utils.coordinate_convert import back_forward_convert\nfrom libs.box_utils.boxes_utils import get_horizen_minAreaRectangle\n\n\nRESTORE_FROM_RPN = False\nFLAGS = get_flags_byname(cfgs.NET_NAME)\nos.environ[""CUDA_VISIBLE_DEVICES""] = cfgs.GPU_GROUP\n\n\ndef train():\n    with tf.Graph().as_default():\n        with tf.name_scope(\'get_batch\'):\n            img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch = \\\n                next_batch(dataset_name=cfgs.DATASET_NAME,\n                           batch_size=cfgs.BATCH_SIZE,\n                           shortside_len=cfgs.SHORT_SIDE_LEN,\n                           is_training=True)\n            gtboxes_and_label = tf.py_func(back_forward_convert,\n                                           inp=[tf.squeeze(gtboxes_and_label_batch, 0)],\n                                           Tout=tf.float32)\n            gtboxes_and_label = tf.reshape(gtboxes_and_label, [-1, 6])\n\n            gtboxes_and_label_minAreaRectangle = get_horizen_minAreaRectangle(gtboxes_and_label)\n\n            gtboxes_and_label_minAreaRectangle = tf.reshape(gtboxes_and_label_minAreaRectangle, [-1, 5])\n\n        with tf.name_scope(\'draw_gtboxes\'):\n            gtboxes_in_img = draw_box_with_color(img_batch, tf.reshape(gtboxes_and_label_minAreaRectangle, [-1, 5])[:, :-1],\n                                                 text=tf.shape(gtboxes_and_label_minAreaRectangle)[0])\n\n            gtboxes_rotate_in_img = draw_box_with_color_rotate(img_batch, tf.reshape(gtboxes_and_label, [-1, 6])[:, :-1],\n                                                               text=tf.shape(gtboxes_and_label)[0])\n\n        # ***********************************************************************************************\n        # *                                         share net                                           *\n        # ***********************************************************************************************\n        _, share_net = get_network_byname(net_name=cfgs.NET_NAME,\n                                          inputs=img_batch,\n                                          num_classes=None,\n                                          is_training=True,\n                                          output_stride=None,\n                                          global_pool=False,\n                                          spatial_squeeze=False)\n\n        # ***********************************************************************************************\n        # *                                            rpn                                              *\n        # ***********************************************************************************************\n        rpn = build_rpn.RPN(net_name=cfgs.NET_NAME,\n                            inputs=img_batch,\n                            gtboxes_and_label=gtboxes_and_label_minAreaRectangle,\n                            is_training=True,\n                            share_head=cfgs.SHARE_HEAD,\n                            share_net=share_net,\n                            stride=cfgs.STRIDE,\n                            anchor_ratios=cfgs.ANCHOR_RATIOS,\n                            anchor_scales=cfgs.ANCHOR_SCALES,\n                            scale_factors=cfgs.SCALE_FACTORS,\n                            base_anchor_size_list=cfgs.BASE_ANCHOR_SIZE_LIST,  # P2, P3, P4, P5, P6\n                            level=cfgs.LEVEL,\n                            top_k_nms=cfgs.RPN_TOP_K_NMS,\n                            rpn_nms_iou_threshold=cfgs.RPN_NMS_IOU_THRESHOLD,\n                            max_proposals_num=cfgs.MAX_PROPOSAL_NUM,\n                            rpn_iou_positive_threshold=cfgs.RPN_IOU_POSITIVE_THRESHOLD,\n                            rpn_iou_negative_threshold=cfgs.RPN_IOU_NEGATIVE_THRESHOLD,  # iou>=0.7 is positive box, iou< 0.3 is negative\n                            rpn_mini_batch_size=cfgs.RPN_MINIBATCH_SIZE,\n                            rpn_positives_ratio=cfgs.RPN_POSITIVE_RATE,\n                            remove_outside_anchors=False,  # whether remove anchors outside\n                            rpn_weight_decay=cfgs.WEIGHT_DECAY[cfgs.NET_NAME])\n\n        rpn_proposals_boxes, rpn_proposals_scores = rpn.rpn_proposals()  # rpn_score shape: [300, ]\n\n        rpn_location_loss, rpn_classification_loss = rpn.rpn_losses()\n        rpn_total_loss = rpn_classification_loss + rpn_location_loss\n\n        with tf.name_scope(\'draw_proposals\'):\n            # score > 0.5 is object\n            rpn_object_boxes_indices = tf.reshape(tf.where(tf.greater(rpn_proposals_scores, 0.5)), [-1])\n            rpn_object_boxes = tf.gather(rpn_proposals_boxes, rpn_object_boxes_indices)\n\n            rpn_proposals_objcet_boxes_in_img = draw_box_with_color(img_batch, rpn_object_boxes,\n                                                                    text=tf.shape(rpn_object_boxes)[0])\n            rpn_proposals_boxes_in_img = draw_box_with_color(img_batch, rpn_proposals_boxes,\n                                                             text=tf.shape(rpn_proposals_boxes)[0])\n        # ***********************************************************************************************\n        # *                                         Fast RCNN                                           *\n        # ***********************************************************************************************\n\n        fast_rcnn = build_fast_rcnn1.FastRCNN(feature_pyramid=rpn.feature_pyramid,\n                                              rpn_proposals_boxes=rpn_proposals_boxes,\n                                              rpn_proposals_scores=rpn_proposals_scores,\n                                              img_shape=tf.shape(img_batch),\n                                              roi_size=cfgs.ROI_SIZE,\n                                              roi_pool_kernel_size=cfgs.ROI_POOL_KERNEL_SIZE,\n                                              scale_factors=cfgs.SCALE_FACTORS,\n                                              gtboxes_and_label=gtboxes_and_label,\n                                              gtboxes_and_label_minAreaRectangle=gtboxes_and_label_minAreaRectangle,\n                                              fast_rcnn_nms_iou_threshold=cfgs.FAST_RCNN_NMS_IOU_THRESHOLD,\n                                              fast_rcnn_maximum_boxes_per_img=100,\n                                              fast_rcnn_nms_max_boxes_per_class=cfgs.FAST_RCNN_NMS_MAX_BOXES_PER_CLASS,\n                                              show_detections_score_threshold=cfgs.FINAL_SCORE_THRESHOLD,  # show detections which score >= 0.6\n                                              num_classes=cfgs.CLASS_NUM,\n                                              fast_rcnn_minibatch_size=cfgs.FAST_RCNN_MINIBATCH_SIZE,\n                                              fast_rcnn_positives_ratio=cfgs.FAST_RCNN_POSITIVE_RATE,\n                                              fast_rcnn_positives_iou_threshold=cfgs.FAST_RCNN_IOU_POSITIVE_THRESHOLD,  # iou>0.5 is positive, iou<0.5 is negative\n                                              use_dropout=cfgs.USE_DROPOUT,\n                                              weight_decay=cfgs.WEIGHT_DECAY[cfgs.NET_NAME],\n                                              is_training=True,\n                                              level=cfgs.LEVEL)\n\n        fast_rcnn_decode_boxes, fast_rcnn_score, num_of_objects, detection_category, \\\n        fast_rcnn_decode_boxes_rotate, fast_rcnn_score_rotate, num_of_objects_rotate, detection_category_rotate = \\\n            fast_rcnn.fast_rcnn_predict()\n        fast_rcnn_location_loss, fast_rcnn_classification_loss, \\\n        fast_rcnn_location_rotate_loss, fast_rcnn_classification_rotate_loss = fast_rcnn.fast_rcnn_loss()\n\n        fast_rcnn_total_loss = fast_rcnn_location_loss + fast_rcnn_classification_loss + \\\n                               fast_rcnn_location_rotate_loss + fast_rcnn_classification_rotate_loss\n\n        with tf.name_scope(\'draw_boxes_with_categories\'):\n            fast_rcnn_predict_boxes_in_imgs = draw_boxes_with_categories(img_batch=img_batch,\n                                                                         boxes=fast_rcnn_decode_boxes,\n                                                                         labels=detection_category,\n                                                                         scores=fast_rcnn_score)\n\n            fast_rcnn_predict_rotate_boxes_in_imgs = draw_boxes_with_categories_rotate(img_batch=img_batch,\n                                                                                       boxes=fast_rcnn_decode_boxes_rotate,\n                                                                                       labels=detection_category_rotate,\n                                                                                       scores=fast_rcnn_score_rotate)\n\n        # train\n        total_loss = slim.losses.get_total_loss()\n\n        global_step = slim.get_or_create_global_step()\n\n        lr = tf.train.piecewise_constant(global_step,\n                                         boundaries=[np.int64(20000), np.int64(40000)],\n                                         values=[cfgs.LR, cfgs.LR/10, cfgs.LR/100])\n        tf.summary.scalar(\'lr\', lr)\n        optimizer = tf.train.MomentumOptimizer(lr, momentum=cfgs.MOMENTUM)\n\n        train_op = slim.learning.create_train_op(total_loss, optimizer, global_step)  # rpn_total_loss,\n        # train_op = optimizer.minimize(second_classification_loss, global_step)\n\n        # ***********************************************************************************************\n        # *                                          Summary                                            *\n        # ***********************************************************************************************\n        # ground truth and predict\n        tf.summary.image(\'img/gtboxes\', gtboxes_in_img)\n        tf.summary.image(\'img/gtboxes_rotate\', gtboxes_rotate_in_img)\n        tf.summary.image(\'img/faster_rcnn_predict\', fast_rcnn_predict_boxes_in_imgs)\n        tf.summary.image(\'img/faster_rcnn_predict_rotate\', fast_rcnn_predict_rotate_boxes_in_imgs)\n        # rpn loss and image\n        tf.summary.scalar(\'rpn/rpn_location_loss\', rpn_location_loss)\n        tf.summary.scalar(\'rpn/rpn_classification_loss\', rpn_classification_loss)\n        tf.summary.scalar(\'rpn/rpn_total_loss\', rpn_total_loss)\n\n        tf.summary.scalar(\'fast_rcnn/fast_rcnn_location_loss\', fast_rcnn_location_loss)\n        tf.summary.scalar(\'fast_rcnn/fast_rcnn_classification_loss\', fast_rcnn_classification_loss)\n        tf.summary.scalar(\'fast_rcnn/fast_rcnn_location_rotate_loss\', fast_rcnn_location_rotate_loss)\n        tf.summary.scalar(\'fast_rcnn/fast_rcnn_classification_rotate_loss\', fast_rcnn_classification_rotate_loss)\n        tf.summary.scalar(\'fast_rcnn/fast_rcnn_total_loss\', fast_rcnn_total_loss)\n\n        tf.summary.scalar(\'loss/total_loss\', total_loss)\n\n        tf.summary.image(\'rpn/rpn_all_boxes\', rpn_proposals_boxes_in_img)\n        tf.summary.image(\'rpn/rpn_object_boxes\', rpn_proposals_objcet_boxes_in_img)\n        # learning_rate\n        tf.summary.scalar(\'learning_rate\', lr)\n\n        summary_op = tf.summary.merge_all()\n        init_op = tf.group(\n            tf.global_variables_initializer(),\n            tf.local_variables_initializer()\n        )\n\n        restorer, restore_ckpt = restore_model.get_restorer()\n        saver = tf.train.Saver(max_to_keep=10)\n\n        config = tf.ConfigProto()\n        # config.gpu_options.per_process_gpu_memory_fraction = 0.5\n        config.gpu_options.allow_growth = True\n        with tf.Session(config=config) as sess:\n            sess.run(init_op)\n            if not restorer is None:\n                restorer.restore(sess, restore_ckpt)\n                print(\'restore model\')\n            coord = tf.train.Coordinator()\n            threads = tf.train.start_queue_runners(sess, coord)\n\n            summary_path = os.path.join(FLAGS.summary_path, cfgs.VERSION)\n            mkdir(summary_path)\n            summary_writer = tf.summary.FileWriter(summary_path, graph=sess.graph)\n\n            for step in range(cfgs.MAX_ITERATION):\n                training_time = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(time.time()))\n                start = time.time()\n\n                _global_step, _img_name_batch, _rpn_location_loss, _rpn_classification_loss, \\\n                _rpn_total_loss, _fast_rcnn_location_loss, _fast_rcnn_classification_loss, \\\n                _fast_rcnn_location_rotate_loss, _fast_rcnn_classification_rotate_loss, \\\n                _fast_rcnn_total_loss, _total_loss, _ = \\\n                    sess.run([global_step, img_name_batch, rpn_location_loss, rpn_classification_loss,\n                              rpn_total_loss, fast_rcnn_location_loss, fast_rcnn_classification_loss,\n                              fast_rcnn_location_rotate_loss, fast_rcnn_classification_rotate_loss,\n                              fast_rcnn_total_loss, total_loss, train_op])\n\n                end = time.time()\n\n                if step % 10 == 0:\n\n                    print("""""" {}: step{}    image_name:{} |\\t\n                                rpn_loc_loss:{} |\\t rpn_cla_loss:{} |\\t\n                                rpn_total_loss:{} |\n                                fast_rcnn_loc_loss:{} |\\t fast_rcnn_cla_loss:{} |\\t\n                                fast_rcnn_loc_rotate_loss:{} |\\t fast_rcnn_cla_rotate_loss:{} |\\t\n                                fast_rcnn_total_loss:{} |\\t\n                                total_loss:{} |\\t pre_cost_time:{}s"""""" \\\n                          .format(training_time, _global_step, str(_img_name_batch[0]), _rpn_location_loss,\n                                  _rpn_classification_loss, _rpn_total_loss, _fast_rcnn_location_loss,\n                                  _fast_rcnn_classification_loss, _fast_rcnn_location_rotate_loss,\n                                  _fast_rcnn_classification_rotate_loss,  _fast_rcnn_total_loss, _total_loss,\n                                  (end - start)))\n\n                if step % 50 == 0:\n                    summary_str = sess.run(summary_op)\n                    summary_writer.add_summary(summary_str, _global_step)\n                    summary_writer.flush()\n\n                if (step > 0 and step % 1000 == 0) or (step == cfgs.MAX_ITERATION - 1):\n                    save_dir = os.path.join(FLAGS.trained_checkpoint, cfgs.VERSION)\n                    mkdir(save_dir)\n\n                    save_ckpt = os.path.join(save_dir, \'voc_\'+str(_global_step)+\'model.ckpt\')\n                    saver.save(sess, save_ckpt)\n                    print(\' weights had been saved\')\n\n            coord.request_stop()\n            coord.join(threads)\n\n\nif __name__ == \'__main__\':\n\n    train()'"
data/io/__init__.py,0,b''
data/io/convert_data_to_tfrecord.py,15,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport sys\nsys.path.append(\'../../\')\nimport xml.etree.cElementTree as ET\nfrom libs.configs import cfgs\nimport numpy as np\nimport tensorflow as tf\nimport glob\nimport cv2\nfrom libs.label_name_dict.label_dict import *\nfrom help_utils.tools import *\n\ntf.app.flags.DEFINE_string(\'VOC_dir\', None, \'Voc dir\')\ntf.app.flags.DEFINE_string(\'xml_dir\', \'Annotations\', \'xml dir\')\ntf.app.flags.DEFINE_string(\'image_dir\', \'JPEGImages\', \'image dir\')\ntf.app.flags.DEFINE_string(\'save_name\', \'train\', \'save name\')\ntf.app.flags.DEFINE_string(\'save_dir\', cfgs.ROOT_PATH + \'/data/tfrecords/\', \'save name\')\ntf.app.flags.DEFINE_string(\'img_format\', \'.tif\', \'format of image\')\ntf.app.flags.DEFINE_string(\'dataset\', \'pascal\', \'dataset\')\nFLAGS = tf.app.flags.FLAGS\n\n\ndef _int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef _bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef read_xml_gtbox_and_label(xml_path):\n    """"""\n    :param xml_path: the path of voc xml\n    :return: a list contains gtboxes and labels, shape is [num_of_gtboxes, 9],\n           and has [x1, y1, x2, y2, x3, y3, x4, y4, label] in a per row\n    """"""\n\n    tree = ET.parse(xml_path)\n    root = tree.getroot()\n    img_width = None\n    img_height = None\n    box_list = []\n    for child_of_root in root:\n        # if child_of_root.tag == \'filename\':\n        #     assert child_of_root.text == xml_path.split(\'/\')[-1].split(\'.\')[0] \\\n        #                                  + FLAGS.img_format, \'xml_name and img_name cannot match\'\n\n        if child_of_root.tag == \'size\':\n            for child_item in child_of_root:\n                if child_item.tag == \'width\':\n                    img_width = int(child_item.text)\n                if child_item.tag == \'height\':\n                    img_height = int(child_item.text)\n\n        if child_of_root.tag == \'object\':\n            label = None\n            for child_item in child_of_root:\n                if child_item.tag == \'name\':\n                    label = NAME_LABEL_MAP[child_item.text]\n                if child_item.tag == \'bndbox\':\n                    tmp_box = []\n                    for node in child_item:\n                        tmp_box.append(int(node.text))\n                    assert label is not None, \'label is none, error\'\n                    tmp_box.append(label)\n                    box_list.append(tmp_box)\n\n    gtbox_label = np.array(box_list, dtype=np.int32)\n\n    return img_height, img_width, gtbox_label\n\n\ndef convert_pascal_to_tfrecord():\n    xml_path = FLAGS.VOC_dir + FLAGS.xml_dir\n    image_path = FLAGS.VOC_dir + FLAGS.image_dir\n    save_path = FLAGS.save_dir + FLAGS.dataset + \'_\' + FLAGS.save_name + \'.tfrecord\'\n    mkdir(FLAGS.save_dir)\n\n    # writer_options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n    # writer = tf.python_io.TFRecordWriter(path=save_path, options=writer_options)\n    writer = tf.python_io.TFRecordWriter(path=save_path)\n    for count, xml in enumerate(glob.glob(xml_path + \'/*.xml\')):\n        # to avoid path error in different development platform\n        xml = xml.replace(\'\\\\\', \'/\')\n\n        img_name = xml.split(\'/\')[-1].split(\'.\')[0] + FLAGS.img_format\n        img_path = image_path + \'/\' + img_name\n\n        if not os.path.exists(img_path):\n            print(\'{} is not exist!\'.format(img_path))\n            continue\n\n        img_height, img_width, gtbox_label = read_xml_gtbox_and_label(xml)\n\n        # img = np.array(Image.open(img_path))\n        img = cv2.imread(img_path)\n\n        feature = tf.train.Features(feature={\n            # do not need encode() in linux\n            # \'img_name\': _bytes_feature(img_name.encode()),\n            \'img_name\': _bytes_feature(img_name),\n            \'img_height\': _int64_feature(img_height),\n            \'img_width\': _int64_feature(img_width),\n            \'img\': _bytes_feature(img.tostring()),\n            \'gtboxes_and_label\': _bytes_feature(gtbox_label.tostring()),\n            \'num_objects\': _int64_feature(gtbox_label.shape[0])\n        })\n\n        example = tf.train.Example(features=feature)\n\n        writer.write(example.SerializeToString())\n\n        view_bar(\'Conversion progress\', count + 1, len(glob.glob(xml_path + \'/*.xml\')))\n\n    print(\'\\nConversion is complete!\')\n\n\nif __name__ == \'__main__\':\n    # xml_path = \'../data/dataset/VOCdevkit/VOC2007/Annotations/000005.xml\'\n    # read_xml_gtbox_and_label(xml_path)\n\n    convert_pascal_to_tfrecord()\n'"
data/io/divide_data.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport sys\nsys.path.append(\'../../\')\nimport shutil\nimport os\nimport random\nimport math\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\ndivide_rate = 0.7\n\nroot_path = \'/mnt/ExtraDisk/yangxue/data_ship_clean\'\n\nimage_path = root_path + \'/VOCdevkit/JPEGImages\'\nxml_path = root_path + \'/VOCdevkit/Annotations\'\n\nimage_list = os.listdir(image_path)\n\nimage_name = [n.split(\'.\')[0] for n in image_list]\n\nrandom.shuffle(image_name)\n\ntrain_image = image_name[:int(math.ceil(len(image_name)) * divide_rate)]\ntest_image = image_name[int(math.ceil(len(image_name)) * divide_rate):]\n\nimage_output_train = os.path.join(root_path, \'VOCdevkit_train/JPEGImages\')\nmkdir(image_output_train)\nimage_output_test = os.path.join(root_path, \'VOCdevkit_test/JPEGImages\')\nmkdir(image_output_test)\n\nxml_train = os.path.join(root_path, \'VOCdevkit_train/Annotations\')\nmkdir(xml_train)\nxml_test = os.path.join(root_path, \'VOCdevkit_test/Annotations\')\nmkdir(xml_test)\n\n\ncount = 0\nfor i in train_image:\n    shutil.copy(os.path.join(image_path, i + \'.tif\'), image_output_train)\n    shutil.copy(os.path.join(xml_path, i + \'.xml\'), xml_train)\n    if count % 1000 == 0:\n        print(""process step {}"".format(count))\n    count += 1\n\nfor i in test_image:\n    shutil.copy(os.path.join(image_path, i + \'.tif\'), image_output_test)\n    shutil.copy(os.path.join(xml_path, i + \'.xml\'), xml_test)\n    if count % 1000 == 0:\n        print(""process step {}"".format(count))\n    count += 1\n\n\n\n\n\n\n\n\n'"
data/io/image_preprocess.py,16,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\n\nimport numpy as np\n\n\ndef short_side_resize(img_tensor, gtboxes_and_label, target_shortside_len):\n    '''\n\n    :param img_tensor:[h, w, c], gtboxes_and_label:[-1, 9]\n    :param target_shortside_len:\n    :return:\n    '''\n\n    h, w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n\n    new_h, new_w = tf.cond(tf.less(h, w),\n                           true_fn=lambda: (target_shortside_len, target_shortside_len * w//h),\n                           false_fn=lambda: (target_shortside_len * h//w,  target_shortside_len))\n\n    img_tensor = tf.expand_dims(img_tensor, axis=0)\n    img_tensor = tf.image.resize_bilinear(img_tensor, [new_h, new_w])\n\n    x1, y1, x2, y2, x3, y3, x4, y4, label = tf.unstack(gtboxes_and_label, axis=1)\n\n    x1, x2, x3, x4 = x1 * new_w//w, x2 * new_w//w, x3 * new_w//w, x4 * new_w//w\n    y1, y2, y3, y4 = y1 * new_h//h, y2 * new_h//h, y3 * new_h//h, y4 * new_h//h\n\n    img_tensor = tf.squeeze(img_tensor, axis=0)  # ensure image tensor rank is 3\n    return img_tensor, tf.transpose(tf.stack([x1, y1, x2, y2, x3, y3, x4, y4, label], axis=0))\n\n\ndef short_side_resize_for_inference_data(img_tensor, target_shortside_len, is_resize=True):\n    h, w, = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n\n    img_tensor = tf.expand_dims(img_tensor, axis=0)\n\n    if is_resize:\n        new_h, new_w = tf.cond(tf.less(h, w),\n                               true_fn=lambda: (target_shortside_len, target_shortside_len*w//h),\n                               false_fn=lambda: (target_shortside_len*h//w, target_shortside_len))\n        img_tensor = tf.image.resize_bilinear(img_tensor, [new_h, new_w])\n\n    return img_tensor  # [1, h, w, c]\n\n\ndef flip_left_right(img_tensor, gtboxes_and_label):\n    h, w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n    img_tensor = tf.image.flip_left_right(img_tensor)\n\n    x1, y1, x2, y2, x3, y3, x4, y4, label = tf.unstack(gtboxes_and_label, axis=1)\n    new_x1 = w - x1\n    new_x2 = w - x2\n    new_x3 = w - x3\n    new_x4 = w - x4\n    return img_tensor, tf.transpose(tf.stack([new_x1, y1, new_x2, y2, new_x3, y3, new_x4, y4, label], axis=0))\n\n\ndef random_flip_left_right(img_tensor, gtboxes_and_label):\n\n    img_tensor, gtboxes_and_label = tf.cond(tf.less(tf.random_uniform(shape=[], minval=0, maxval=1), 0.5),\n                                            lambda: flip_left_right(img_tensor, gtboxes_and_label),\n                                            lambda: (img_tensor, gtboxes_and_label))\n\n    return img_tensor,  gtboxes_and_label\n\n\n"""
data/io/read_tfrecord.py,23,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport os\nfrom data.io import image_preprocess\n\n\ndef read_single_example_and_decode(filename_queue):\n\n    # tfrecord_options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n\n    # reader = tf.TFRecordReader(options=tfrecord_options)\n    reader = tf.TFRecordReader()\n\n    _, serialized_example = reader.read(filename_queue)\n\n    features = tf.parse_single_example(\n        serialized=serialized_example,\n        features={\n            'img_name': tf.FixedLenFeature([], tf.string),\n            'img_height': tf.FixedLenFeature([], tf.int64),\n            'img_width': tf.FixedLenFeature([], tf.int64),\n            'img': tf.FixedLenFeature([], tf.string),\n            'gtboxes_and_label': tf.FixedLenFeature([], tf.string),\n            'num_objects': tf.FixedLenFeature([], tf.int64)\n        }\n    )\n    img_name = features['img_name']\n    img_height = tf.cast(features['img_height'], tf.int32)\n    img_width = tf.cast(features['img_width'], tf.int32)\n    img = tf.decode_raw(features['img'], tf.uint8)\n\n    img = tf.reshape(img, shape=[img_height, img_width, 3])\n\n    gtboxes_and_label = tf.decode_raw(features['gtboxes_and_label'], tf.int32)\n    gtboxes_and_label = tf.reshape(gtboxes_and_label, [-1, 9])\n\n    num_objects = tf.cast(features['num_objects'], tf.int32)\n    return img_name, img, gtboxes_and_label, num_objects\n\n\ndef read_and_prepocess_single_img(filename_queue, shortside_len, is_training):\n\n    img_name, img, gtboxes_and_label, num_objects = read_single_example_and_decode(filename_queue)\n    # img = tf.image.per_image_standardization(img)\n    img = tf.cast(img, tf.float32)\n    img = img - tf.constant([103.939, 116.779, 123.68])\n    if is_training:\n        img, gtboxes_and_label = image_preprocess.short_side_resize(img_tensor=img, gtboxes_and_label=gtboxes_and_label,\n                                                                    target_shortside_len=shortside_len)\n        img, gtboxes_and_label = image_preprocess.random_flip_left_right(img_tensor=img, gtboxes_and_label=gtboxes_and_label)\n\n    else:\n        img, gtboxes_and_label = image_preprocess.short_side_resize(img_tensor=img, gtboxes_and_label=gtboxes_and_label,\n                                                                    target_shortside_len=shortside_len)\n\n    return img_name, img, gtboxes_and_label, num_objects\n\n\ndef next_batch(dataset_name, batch_size, shortside_len, is_training):\n    if dataset_name not in ['ship', 'spacenet', 'pascal', 'coco']:\n        raise ValueError('dataSet name must be in pascal or coco')\n\n    if is_training:\n        pattern = os.path.join('../data/tfrecords', dataset_name + '_train*')\n    else:\n        pattern = os.path.join('../data/tfrecords', dataset_name + '_test*')\n\n    print('tfrecord path is -->', os.path.abspath(pattern))\n    filename_tensorlist = tf.train.match_filenames_once(pattern)\n\n    filename_queue = tf.train.string_input_producer(filename_tensorlist)\n\n    img_name, img, gtboxes_and_label, num_obs = read_and_prepocess_single_img(filename_queue, shortside_len,\n                                                                              is_training=is_training)\n    img_name_batch, img_batch, gtboxes_and_label_batch, num_obs_batch = \\\n        tf.train.batch(\n                       [img_name, img, gtboxes_and_label, num_obs],\n                       batch_size=batch_size,\n                       capacity=100,\n                       num_threads=16,\n                       dynamic_pad=True)\n    return img_name_batch, img_batch, gtboxes_and_label_batch, num_obs_batch\n\n"""
libs/box_utils/__init__.py,0,b''
libs/box_utils/anchor_utils_pyfunc.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport cv2\nfrom help_utils.help_utils import show_boxes_in_img\n\n\ndef make_anchors(base_anchor_size, anchor_scales, anchor_ratios, featuremaps_height,\n                 featuremaps_width, stride\n                 ):\n    \'\'\'\n    :param base_anchor_size:\n    :param anchor_scales:\n    :param anchor_ratios:\n    :param featuremaps_width:\n    :param featuremaps_height:\n    :param stride\n    :return: anchors of shape: [w*h*9, 4]\n    \'\'\'\n    base_anchor = [0, 0, base_anchor_size, base_anchor_size]  # [y_center, x_center, h, w]\n    per_location_anchors = enum_ratios(enum_scales(base_anchor, anchor_scales),\n                                       anchor_ratios)\n\n    ws, hs = per_location_anchors[:, 2], per_location_anchors[:, 3]\n\n    x_centers = np.arange(featuremaps_width) * stride\n    y_centers = np.arange(featuremaps_height) * stride\n\n    x_centers, y_centers = np.meshgrid(x_centers, y_centers)\n\n    ws, x_centers = np.meshgrid(ws, x_centers)\n    hs, y_centers = np.meshgrid(hs, y_centers)\n\n    box_centers = np.stack([y_centers, x_centers], axis=2)\n    box_centers = np.reshape(box_centers, [-1, 2])\n\n    box_sizes = np.stack([hs, ws], axis=2)\n    box_sizes = np.reshape(box_sizes, [-1, 2])\n    final_anchors = np.concatenate([box_centers - 0.5*box_sizes, box_centers+0.5*box_sizes], axis=1)\n    final_anchors = final_anchors.astype(dtype=np.float32)\n    return final_anchors\n\n\ndef enum_scales(base_anchor, anchor_scales):\n    \'\'\'\n    for baseanchor : center point is zero\n    :param base_anchor: [y_center, x_center, h, w] -->may[0, 0, 256, 256]\n    :param anchor_scales: maybe [0.5, 1., 2.0]\n    :return:\n    \'\'\'\n\n    base_anchor = np.array(base_anchor)\n    anchor_scales = np.array(anchor_scales).reshape(len(anchor_scales), 1)\n\n    return base_anchor * anchor_scales\n\n\ndef enum_ratios(anchors, anchor_ratios):\n    \'\'\'\n    h / w = ratio\n    :param anchors:\n    :param anchor_ratios:\n    :return:\n    \'\'\'\n\n    ws = anchors[:, 3]  # for base anchor, w == h\n    hs = anchors[:, 2]\n    sqrt_ratios = np.sqrt(np.array(anchor_ratios))\n    ws = np.reshape(ws / sqrt_ratios[:, np.newaxis], [-1])\n    hs = np.reshape(hs * sqrt_ratios[:, np.newaxis], [-1])\n    assert ws.shape == hs.shape, \'h shape is not equal w shape\'\n\n    num_anchors_per_location = ws.shape[0]\n\n    return np.hstack([np.zeros((num_anchors_per_location, 1)),\n                     np.zeros((num_anchors_per_location, 1)),\n                     ws[:, np.newaxis],\n                     hs[:, np.newaxis]])\n\n\ndef filter_outside_boxes(anchors, img_h, img_w):\n    \'\'\'\n\n    :param anchors:[-1, 4] ... [ymin, xmin, ymax, xmax]\n    :param img_h:\n    :param img_w:\n    :return:\n    \'\'\'\n\n    index = (anchors[:, 0] > 0) & (anchors[:, 0] < img_h) & \\\n            (anchors[:, 1] > 0) & (anchors[:, 1] < img_w) & \\\n            (anchors[:, 2] < img_h) & (anchors[:, 2] > 0) & \\\n            (anchors[:, 3] < img_w) & (anchors[:, 3] > 0)\n\n    valid_indices = np.where(index == True)[0]\n\n    return valid_indices\n\n\ndef show_anchors_in_img(anchors):\n    img = cv2.imread(\'1.jpg\')\n    img = cv2.resize(img, (800, 600), interpolation=cv2.INTER_AREA)\n    img = show_boxes_in_img(img, anchors)\n\n    cv2.imshow(\'resize_img\', img)\n    cv2.waitKey(0)\n\nif __name__ == \'__main__\':\n    print(enum_scales([0, 0, 256, 256], [0.5, 1.0, 2.0]))\n    print(""_______________"")\n    anchors = make_anchors(256,\n                           [1.0],\n                           [0.5, 1.0, 2.0],\n                           featuremaps_height=38,\n                           featuremaps_width=50,\n                           stride=16)\n    indices = filter_outside_boxes(anchors, img_h=600, img_w=800)\n    show_anchors_in_img(np.column_stack([anchors[indices], np.ones(shape=(anchors[indices].shape[0], 1))]))\n\n\n'"
libs/box_utils/boxes_utils.py,47,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom libs.box_utils.coordinate_convert import forward_convert\n\n\ndef clip_boxes_to_img_boundaries(decode_boxes, img_shape):\n    '''\n\n    :param decode_boxes:\n    :return: decode boxes, and already clip to boundaries\n    '''\n\n    with tf.name_scope('clip_boxes_to_img_boundaries'):\n\n        ymin, xmin, ymax, xmax = tf.unstack(decode_boxes, axis=1)\n        img_h, img_w = img_shape[1], img_shape[2]\n\n        xmin = tf.maximum(xmin, 0.0)\n        xmin = tf.minimum(xmin, tf.cast(img_w, tf.float32))\n\n        ymin = tf.maximum(ymin, 0.0)\n        ymin = tf.minimum(ymin, tf.cast(img_h, tf.float32))  # avoid xmin > img_w, ymin > img_h\n\n        xmax = tf.minimum(xmax, tf.cast(img_w, tf.float32))\n        ymax = tf.minimum(ymax, tf.cast(img_h, tf.float32))\n\n        return tf.transpose(tf.stack([ymin, xmin, ymax, xmax]))\n\n\ndef filter_outside_boxes(boxes, img_w, img_h):\n    '''\n    :param anchors:boxes with format [xmin, ymin, xmax, ymax]\n    :param img_h: height of image\n    :param img_w: width of image\n    :return: indices of anchors that not outside the image boundary\n    '''\n\n    with tf.name_scope('filter_outside_boxes'):\n\n        ymin, xmin, ymax, xmax = tf.unstack(boxes, axis=1)\n        xmin_index = tf.greater_equal(xmin, 0)\n        ymin_index = tf.greater_equal(ymin, 0)\n        xmax_index = tf.less_equal(xmax, img_w)\n        ymax_index = tf.less_equal(ymax, img_h)\n\n        indices = tf.transpose(tf.stack([ymin_index, xmin_index, ymax_index, xmax_index]))\n        indices = tf.cast(indices, dtype=tf.int32)\n        indices = tf.reduce_sum(indices, axis=1)\n        indices = tf.where(tf.equal(indices, tf.shape(boxes)[1]))\n\n        return tf.reshape(indices, [-1, ])\n\n\ndef nms_boxes(decode_boxes, scores, iou_threshold, max_output_size, name):\n    '''\n    1) NMS\n    2) get maximum num of proposals\n    :return: valid_indices\n    '''\n\n    valid_index = tf.image.non_max_suppression(\n        boxes=decode_boxes,\n        scores=scores,\n        max_output_size=max_output_size,\n        iou_threshold=iou_threshold,\n        name=name\n    )\n\n    return valid_index\n\n\ndef padd_boxes_with_zeros(boxes, scores, max_num_of_boxes):\n\n    '''\n    num of boxes less than max num of boxes, so it need to pad with zeros[0, 0, 0, 0]\n    :param boxes:\n    :param scores: [-1]\n    :param max_num_of_boxes:\n    :return:\n    '''\n\n    pad_num = tf.cast(max_num_of_boxes, tf.int32) - tf.shape(boxes)[0]\n\n    zero_boxes = tf.zeros(shape=[pad_num, 4], dtype=boxes.dtype)\n    zero_scores = tf.zeros(shape=[pad_num], dtype=scores.dtype)\n\n    final_boxes = tf.concat([boxes, zero_boxes], axis=0)\n\n    final_scores = tf.concat([scores, zero_scores], axis=0)\n\n    return final_boxes, final_scores\n\n\ndef get_horizen_minAreaRectangle(boxs, with_label=True):\n\n    rpn_proposals_boxes_convert = tf.py_func(forward_convert,\n                                             inp=[boxs, with_label],\n                                             Tout=tf.float32)\n    if with_label:\n        rpn_proposals_boxes_convert = tf.reshape(rpn_proposals_boxes_convert, [-1, 9])\n\n        boxes_shape = tf.shape(rpn_proposals_boxes_convert)\n        y_list = tf.strided_slice(rpn_proposals_boxes_convert, begin=[0, 0], end=[boxes_shape[0], boxes_shape[1] - 1],\n                                  strides=[1, 2])\n        x_list = tf.strided_slice(rpn_proposals_boxes_convert, begin=[0, 1], end=[boxes_shape[0], boxes_shape[1] - 1],\n                                  strides=[1, 2])\n\n        label = tf.unstack(rpn_proposals_boxes_convert, axis=1)[-1]\n\n        y_max = tf.reduce_max(y_list, axis=1)\n        y_min = tf.reduce_min(y_list, axis=1)\n        x_max = tf.reduce_max(x_list, axis=1)\n        x_min = tf.reduce_min(x_list, axis=1)\n        return tf.transpose(tf.stack([y_min, x_min, y_max, x_max, label], axis=0))\n    else:\n        rpn_proposals_boxes_convert = tf.reshape(rpn_proposals_boxes_convert, [-1, 8])\n\n        boxes_shape = tf.shape(rpn_proposals_boxes_convert)\n        y_list = tf.strided_slice(rpn_proposals_boxes_convert, begin=[0, 0], end=[boxes_shape[0], boxes_shape[1]],\n                                  strides=[1, 2])\n        x_list = tf.strided_slice(rpn_proposals_boxes_convert, begin=[0, 1], end=[boxes_shape[0], boxes_shape[1]],\n                                  strides=[1, 2])\n\n        y_max = tf.reduce_max(y_list, axis=1)\n        y_min = tf.reduce_min(y_list, axis=1)\n        x_max = tf.reduce_max(x_list, axis=1)\n        x_min = tf.reduce_min(x_list, axis=1)\n\n    return tf.transpose(tf.stack([y_min, x_min, y_max, x_max], axis=0))"""
libs/box_utils/coordinate_convert.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cv2\nimport numpy as np\n\n\ndef forward_convert(coordinate, with_label=True):\n    """"""\n    :param coordinate: format [y_c, x_c, h, w, theta]\n    :return: format [y1, x1, y2, x2, y3, x3, y4, x4]\n    """"""\n    boxes = []\n    if with_label:\n        for rect in coordinate:\n            box = cv2.boxPoints(((rect[1], rect[0]), (rect[3], rect[2]), rect[4]))\n            box = np.reshape(box, [-1, ])\n            boxes.append([box[1], box[0], box[3], box[2], box[5], box[4], box[7], box[6], rect[5]])\n    else:\n        for rect in coordinate:\n            box = cv2.boxPoints(((rect[1], rect[0]), (rect[3], rect[2]), rect[4]))\n            box = np.reshape(box, [-1, ])\n            boxes.append([box[1], box[0], box[3], box[2], box[5], box[4], box[7], box[6]])\n\n    return np.array(boxes, dtype=np.float32)\n\n\ndef back_forward_convert(coordinate, with_label=True):\n    """"""\n    :param coordinate: format [x1, y1, x2, y2, x3, y3, x4, y4, (label)] \n    :param with_label: default True\n    :return: format [y_c, x_c, h, w, theta, (label)]\n    """"""\n\n    boxes = []\n    if with_label:\n        for rect in coordinate:\n            box = np.int0(rect[:-1])\n            box = box.reshape([4, 2])\n            rect1 = cv2.minAreaRect(box)\n\n            x, y, w, h, theta = rect1[0][0], rect1[0][1], rect1[1][0], rect1[1][1], rect1[2]\n            boxes.append([y, x, h, w, theta, rect[-1]])\n\n    else:\n        for rect in coordinate:\n            box = np.int0(rect)\n            box = box.reshape([4, 2])\n            rect1 = cv2.minAreaRect(box)\n\n            x, y, w, h, theta = rect1[0][0], rect1[0][1], rect1[1][0], rect1[1][1], rect1[2]\n            boxes.append([y, x, h, w, theta])\n\n    return np.array(boxes, dtype=np.float32)\n\n\nif __name__ == \'__main__\':\n    coord = np.array([[150, 150, 50, 100, -90, 1],\n                      [150, 150, 100, 50, -90, 1],\n                      [150, 150, 50, 100, -45, 1],\n                      [150, 150, 100, 50, -45, 1]])\n\n    coord1 = np.array([[150, 150, 100, 50, 0],\n                      [150, 150, 100, 50, -90],\n                      [150, 150, 100, 50, 45],\n                      [150, 150, 100, 50, -45]])\n\n    coord2 = forward_convert(coord)\n    # coord3 = forward_convert(coord1, mode=-1)\n    print(coord2)\n    # print(coord3-coord2)\n    # coord_label = np.array([[167., 203., 96., 132., 132., 96., 203., 167., 1.]])\n    #\n    # coord4 = back_forward_convert(coord_label, mode=1)\n    # coord5 = back_forward_convert(coord_label)\n\n    # print(coord4)\n    # print(coord5)\n\n    # coord3 = coordinate_present_convert(coord, -1)\n    # print(coord3)\n    # coord4 = coordinate_present_convert(coord3, mode=1)\n    # print(coord4)\n\n'"
libs/box_utils/encode_and_decode.py,51,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\n\nimport numpy as np\nimport math\n\n\ndef decode_boxes(encode_boxes, reference_boxes, scale_factors=None, name='decode'):\n    '''\n\n    :param encode_boxes:[N, 4]\n    :param reference_boxes: [N, 4] .\n    :param scale_factors: use for scale\n    in the first stage, reference_boxes  are anchors\n    in the second stage, reference boxes are proposals(decode) produced by rpn stage\n    :return:decode boxes [N, 4]\n    '''\n\n    with tf.variable_scope(name):\n        t_ycenter, t_xcenter, t_h, t_w = tf.unstack(encode_boxes, axis=1)\n        if scale_factors:\n            t_xcenter /= scale_factors[0]\n            t_ycenter /= scale_factors[1]\n            t_w /= scale_factors[2]\n            t_h /= scale_factors[3]\n\n        reference_ymin, reference_xmin, reference_ymax, reference_xmax = tf.unstack(reference_boxes, axis=1)\n\n        reference_xcenter = (reference_xmin + reference_xmax) / 2.\n        reference_ycenter = (reference_ymin + reference_ymax) / 2.\n        reference_w = reference_xmax - reference_xmin\n        reference_h = reference_ymax - reference_ymin\n\n        predict_xcenter = t_xcenter * reference_w + reference_xcenter\n        predict_ycenter = t_ycenter * reference_h + reference_ycenter\n        predict_w = tf.exp(t_w) * reference_w\n        predict_h = tf.exp(t_h) * reference_h\n\n        predict_xmin = predict_xcenter - predict_w / 2.\n        predict_xmax = predict_xcenter + predict_w / 2.\n        predict_ymin = predict_ycenter - predict_h / 2.\n        predict_ymax = predict_ycenter + predict_h / 2.\n\n        return tf.transpose(tf.stack([predict_ymin, predict_xmin,\n                                      predict_ymax, predict_xmax]))\n\n\ndef decode_boxes_rotate(encode_boxes, reference_boxes, scale_factors=None, name='decode'):\n    '''\n\n    :param encode_boxes:[N, 5]\n    :param reference_boxes: [N, 5] .\n    :param scale_factors: use for scale\n    in the rpn stage, reference_boxes are anchors\n    in the fast_rcnn stage, reference boxes are proposals(decode) produced by rpn stage\n    :return:decode boxes [N, 5]\n    '''\n\n    with tf.variable_scope(name):\n        t_ycenter, t_xcenter, t_h, t_w, t_theta = tf.unstack(encode_boxes, axis=1)\n        if scale_factors:\n            t_xcenter /= scale_factors[0]\n            t_ycenter /= scale_factors[1]\n            t_w /= scale_factors[2]\n            t_h /= scale_factors[3]\n            t_theta /= scale_factors[4]\n\n        reference_ymin, reference_xmin, reference_ymax, reference_xmax = tf.unstack(reference_boxes, axis=1)\n        reference_x_center = (reference_xmin + reference_xmax) / 2.\n        reference_y_center = (reference_ymin + reference_ymax) / 2.\n        reference_w = reference_xmax - reference_xmin\n        reference_h = reference_ymax - reference_ymin\n        reference_theta = tf.ones(tf.shape(reference_xmin)) * -90\n\n        predict_x_center = t_xcenter * reference_w + reference_x_center\n        predict_y_center = t_ycenter * reference_h + reference_y_center\n        predict_w = tf.exp(t_w) * reference_w\n        predict_h = tf.exp(t_h) * reference_h\n\n        predict_theta = t_theta * 180 / math.pi + reference_theta\n\n        mask1 = tf.less(predict_theta, -90)\n        mask2 = tf.greater_equal(predict_theta, -180)\n        mask7 = tf.less(predict_theta, -180)\n        mask8 = tf.greater_equal(predict_theta, -270)\n\n        mask3 = tf.greater_equal(predict_theta, 0)\n        mask4 = tf.less(predict_theta, 90)\n        mask5 = tf.greater_equal(predict_theta, 90)\n        mask6 = tf.less(predict_theta, 180)\n\n        # to keep range in [-90, 0)\n        # [-180, -90)\n        convert_mask = tf.logical_and(mask1, mask2)\n        remain_mask = tf.logical_not(convert_mask)\n        predict_theta += tf.cast(convert_mask, tf.float32) * 90.\n\n        remain_h = tf.cast(remain_mask, tf.float32) * predict_h\n        remain_w = tf.cast(remain_mask, tf.float32) * predict_w\n        convert_h = tf.cast(convert_mask, tf.float32) * predict_h\n        convert_w = tf.cast(convert_mask, tf.float32) * predict_w\n\n        predict_h = remain_h + convert_w\n        predict_w = remain_w + convert_h\n\n        # [-270, -180)\n        cond4 = tf.cast(tf.logical_and(mask7, mask8), tf.float32) * 180.\n        predict_theta += cond4\n\n        # [0, 90)\n        # cond2 = tf.cast(tf.logical_and(mask3, mask4), tf.float32) * 90.\n        # predict_theta -= cond2\n\n        convert_mask1 = tf.logical_and(mask3, mask4)\n        remain_mask1 = tf.logical_not(convert_mask1)\n        predict_theta -= tf.cast(convert_mask1, tf.float32) * 90.\n\n        remain_h = tf.cast(remain_mask1, tf.float32) * predict_h\n        remain_w = tf.cast(remain_mask1, tf.float32) * predict_w\n        convert_h = tf.cast(convert_mask1, tf.float32) * predict_h\n        convert_w = tf.cast(convert_mask1, tf.float32) * predict_w\n\n        predict_h = remain_h + convert_w\n        predict_w = remain_w + convert_h\n\n        # [90, 180)\n        cond3 = tf.cast(tf.logical_and(mask5, mask6), tf.float32) * 180.\n        predict_theta -= cond3\n\n        decode_boxes = tf.transpose(tf.stack([predict_y_center, predict_x_center,\n                                              predict_h, predict_w, predict_theta]))\n\n        return decode_boxes\n\n\ndef encode_boxes(unencode_boxes, reference_boxes, scale_factors=None, name='encode'):\n    '''\n\n    :param unencode_boxes: [batch_size*H*W*num_anchors_per_location, 4]\n    :param reference_boxes: [H*W*num_anchors_per_location, 4]\n    :return: encode_boxes [-1, 4]\n    '''\n\n    with tf.variable_scope(name):\n        ymin, xmin, ymax, xmax = tf.unstack(unencode_boxes, axis=1)\n\n        reference_ymin, reference_xmin, reference_ymax, reference_xmax = tf.unstack(reference_boxes, axis=1)\n\n        x_center = (xmin + xmax) / 2.\n        y_center = (ymin + ymax) / 2.\n        w = xmax - xmin\n        h = ymax - ymin\n\n        reference_xcenter = (reference_xmin + reference_xmax) / 2.\n        reference_ycenter = (reference_ymin + reference_ymax) / 2.\n        reference_w = reference_xmax - reference_xmin\n        reference_h = reference_ymax - reference_ymin\n\n        reference_w += 1e-8\n        reference_h += 1e-8\n        w += 1e-8\n        h += 1e-8  # to avoid NaN in division and log below\n\n        t_xcenter = (x_center - reference_xcenter) / reference_w\n        t_ycenter = (y_center - reference_ycenter) / reference_h\n        t_w = tf.log(w / reference_w)\n        t_h = tf.log(h / reference_h)\n\n        if scale_factors:\n            t_xcenter *= scale_factors[0]\n            t_ycenter *= scale_factors[1]\n            t_w *= scale_factors[2]\n            t_h *= scale_factors[3]\n\n        return tf.transpose(tf.stack([t_ycenter, t_xcenter, t_h, t_w]))\n\n\ndef encode_boxes_rotate(unencode_boxes, reference_boxes, scale_factors=None, name='encode'):\n    '''\n    :param unencode_boxes: [batch_size*H*W*num_anchors_per_location, 5]\n    :param reference_boxes: [H*W*num_anchors_per_location, 5]\n    :return: encode_boxes [-1, 5]\n    '''\n\n    with tf.variable_scope(name):\n        y_center, x_center, h, w, theta = tf.unstack(unencode_boxes, axis=1)\n\n        reference_ymin, reference_xmin, reference_ymax, reference_xmax = tf.unstack(reference_boxes, axis=1)\n\n        reference_x_center = (reference_xmin + reference_xmax) / 2.\n        reference_y_center = (reference_ymin + reference_ymax) / 2.\n        # here maybe have logical error, reference_w and reference_h should exchange,\n        # but it doesn't seem to affect the result.\n        reference_w = reference_xmax - reference_xmin\n        reference_h = reference_ymax - reference_ymin\n        reference_theta = tf.ones(tf.shape(reference_xmin)) * -90\n\n        reference_w += 1e-8\n        reference_h += 1e-8\n        w += 1e-8\n        h += 1e-8  # to avoid NaN in division and log below\n\n        t_xcenter = (x_center - reference_x_center) / reference_w\n        t_ycenter = (y_center - reference_y_center) / reference_h\n        t_w = tf.log(w / reference_w)\n        t_h = tf.log(h / reference_h)\n        t_theta = (theta - reference_theta) * math.pi / 180\n\n        if scale_factors:\n            t_xcenter *= scale_factors[0]\n            t_ycenter *= scale_factors[1]\n            t_w *= scale_factors[2]\n            t_h *= scale_factors[3]\n            t_theta *= scale_factors[4]\n\n        return tf.transpose(tf.stack([t_ycenter, t_xcenter, t_h, t_w, t_theta]))"""
libs/box_utils/iou.py,9,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\n\ndef iou_calculate(boxes_1, boxes_2):\n    '''\n\n    :param boxes_1: [N, 4] [ymin, xmin, ymax, xmax]\n    :param boxes_2: [M, 4] [ymin, xmin. ymax, xmax]\n    :return:\n    '''\n    with tf.name_scope('iou_caculate'):\n\n        ymin_1, xmin_1, ymax_1, xmax_1 = tf.split(boxes_1, 4, axis=1)  # ymin_1 shape is [N, 1]..\n\n        ymin_2, xmin_2, ymax_2, xmax_2 = tf.unstack(boxes_2, axis=1)  # ymin_2 shape is [M, ]..\n\n        max_xmin = tf.maximum(xmin_1, xmin_2)\n        min_xmax = tf.minimum(xmax_1, xmax_2)\n\n        max_ymin = tf.maximum(ymin_1, ymin_2)\n        min_ymax = tf.minimum(ymax_1, ymax_2)\n\n        overlap_h = tf.maximum(0., min_ymax - max_ymin)  # avoid h < 0\n        overlap_w = tf.maximum(0., min_xmax - max_xmin)\n\n        overlaps = overlap_h * overlap_w\n\n        area_1 = (xmax_1 - xmin_1) * (ymax_1 - ymin_1)  # [N, 1]\n        area_2 = (xmax_2 - xmin_2) * (ymax_2 - ymin_2)  # [M, ]\n\n        iou = overlaps / (area_1 + area_2 - overlaps)\n\n        return iou\n"""
libs/box_utils/iou_rotate.py,8,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\nimport tensorflow as tf\nfrom libs.box_utils.coordinate_convert import *\nfrom libs.box_utils.rbbox_overlaps import rbbx_overlaps\nfrom libs.box_utils.iou_cpu import get_iou_matrix\n\n\ndef iou_rotate_calculate(boxes1, boxes2, use_gpu=True, gpu_id=0):\n    \'\'\'\n\n    :param boxes_list1:[N, 8] tensor\n    :param boxes_list2: [M, 8] tensor\n    :return:\n    \'\'\'\n\n    boxes1 = tf.cast(boxes1, tf.float32)\n    boxes2 = tf.cast(boxes2, tf.float32)\n    if use_gpu:\n\n        iou_matrix = tf.py_func(rbbx_overlaps,\n                                inp=[boxes1, boxes2, gpu_id],\n                                Tout=tf.float32)\n    else:\n        iou_matrix = tf.py_func(get_iou_matrix, inp=[boxes1, boxes2],\n                                Tout=tf.float32)\n\n    iou_matrix = tf.reshape(iou_matrix, [tf.shape(boxes1)[0], tf.shape(boxes2)[0]])\n\n    return iou_matrix\n\n\ndef iou_rotate_calculate1(boxes1, boxes2, use_gpu=True, gpu_id=0):\n\n    # start = time.time()\n    if use_gpu:\n        ious = rbbx_overlaps(boxes1, boxes2, gpu_id)\n    else:\n        area1 = boxes1[:, 2] * boxes1[:, 3]\n        area2 = boxes2[:, 2] * boxes2[:, 3]\n        ious = []\n        for i, box1 in enumerate(boxes1):\n            temp_ious = []\n            r1 = ((box1[0], box1[1]), (box1[2], box1[3]), box1[4])\n            for j, box2 in enumerate(boxes2):\n                r2 = ((box2[0], box2[1]), (box2[2], box2[3]), box2[4])\n\n                int_pts = cv2.rotatedRectangleIntersection(r1, r2)[1]\n                if int_pts is not None:\n                    order_pts = cv2.convexHull(int_pts, returnPoints=True)\n\n                    int_area = cv2.contourArea(order_pts)\n\n                    inter = int_area * 1.0 / (area1[i] + area2[j] - int_area)\n                    temp_ious.append(inter)\n                else:\n                    temp_ious.append(0.0)\n            ious.append(temp_ious)\n\n    # print(\'{}s\'.format(time.time() - start))\n\n    return np.array(ious, dtype=np.float32)\n\n\nif __name__ == \'__main__\':\n    import os\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'13\'\n    boxes1 = np.array([[50, 50, 100, 300, 0],\n                       [60, 60, 100, 200, 0]], np.float32)\n\n    boxes2 = np.array([[50, 50, 100, 300, -45.],\n                       [200, 200, 100, 200, 0.]], np.float32)\n\n    start = time.time()\n    with tf.Session() as sess:\n        ious = iou_rotate_calculate1(boxes1, boxes2, use_gpu=False)\n        print(sess.run(ious))\n        print(\'{}s\'.format(time.time() - start))\n\n    # start = time.time()\n    # for _ in range(10):\n    #     ious = rbbox_overlaps.rbbx_overlaps(boxes1, boxes2)\n    # print(\'{}s\'.format(time.time() - start))\n    # print(ious)\n\n    # print(ovr)\n\n\n\n'"
libs/box_utils/make_anchor.py,29,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef enum_scales(base_anchor, anchor_scales, name='enum_scales'):\n\n    '''\n    :param base_anchor: [y_center, x_center, h, w]\n    :param anchor_scales: different scales, like [0.5, 1., 2.0]\n    :return: return base anchors in different scales.\n            Example:[[0, 0, 128, 128],[0, 0, 256, 256],[0, 0, 512, 512]]\n    '''\n    with tf.variable_scope(name):\n        anchor_scales = tf.reshape(anchor_scales, [-1, 1])\n\n        return base_anchor * anchor_scales\n\n\ndef enum_ratios(anchors, anchor_ratios, name='enum_ratios'):\n\n    '''\n    :param anchors: base anchors in different scales\n    :param anchor_ratios:  ratio = h / w\n    :return: base anchors in different scales and ratios\n    '''\n\n    with tf.variable_scope(name):\n        _, _, hs, ws = tf.unstack(anchors, axis=1)  # for base anchor, w == h\n        sqrt_ratios = tf.sqrt(anchor_ratios)\n        sqrt_ratios = tf.expand_dims(sqrt_ratios, axis=1)\n        ws = tf.reshape(ws / sqrt_ratios, [-1])\n        hs = tf.reshape(hs * sqrt_ratios, [-1])\n        # assert tf.shape(ws) == tf.shape(hs), 'h shape is not equal w shape'\n\n        num_anchors_per_location = tf.shape(ws)[0]\n\n        return tf.transpose(tf.stack([tf.zeros([num_anchors_per_location, ]),\n                                      tf.zeros([num_anchors_per_location,]),\n                                      ws, hs]))\n\n\ndef make_anchors(base_anchor_size, anchor_scales, anchor_ratios, featuremaps_height,\n                 featuremaps_width, stride, name='make_anchors'):\n\n    '''\n    :param base_anchor_size: base anchor size in different scales\n    :param anchor_scales: anchor scales\n    :param anchor_ratios: anchor ratios\n    :param featuremaps_width: width of featuremaps\n    :param featuremaps_height: height of featuremaps\n    :return: anchors of shape [w * h * len(anchor_scales) * len(anchor_ratios), 4]\n    '''\n\n    with tf.variable_scope(name):\n        # [y_center, x_center, h, w]\n        base_anchor = tf.constant([0, 0, base_anchor_size, base_anchor_size], dtype=tf.float32)\n        base_anchors = enum_ratios(enum_scales(base_anchor, anchor_scales), anchor_ratios)\n\n        _, _, ws, hs = tf.unstack(base_anchors, axis=1)\n\n        x_centers = tf.range(tf.cast(featuremaps_width, tf.float32), dtype=tf.float32) * stride\n        y_centers = tf.range(tf.cast(featuremaps_height, tf.float32), dtype=tf.float32) * stride\n\n        x_centers, y_centers = tf.meshgrid(x_centers, y_centers)\n\n        ws, x_centers = tf.meshgrid(ws, x_centers)\n        hs, y_centers = tf.meshgrid(hs, y_centers)\n\n        box_centers = tf.stack([y_centers, x_centers], axis=2)\n        box_centers = tf.reshape(box_centers, [-1, 2])\n\n        box_sizes = tf.stack([hs, ws], axis=2)\n        box_sizes = tf.reshape(box_sizes, [-1, 2])\n        final_anchors = tf.concat([box_centers - 0.5*box_sizes, box_centers+0.5*box_sizes], axis=1)\n        return final_anchors\n\nif __name__ == '__main__':\n    base_anchor = tf.constant([256], dtype=tf.float32)\n    anchor_scales = tf.constant([1.0], dtype=tf.float32)\n    anchor_ratios = tf.constant([0.5, 1.0, 2.0], dtype=tf.float32)\n    # print(enum_scales(base_anchor, anchor_scales))\n    sess = tf.Session()\n    # print(sess.run(enum_ratios(enum_scales(base_anchor, anchor_scales), anchor_ratios)))\n    anchors = make_anchors(256, anchor_scales, anchor_ratios,\n                           featuremaps_height=38,\n                           featuremaps_width=50, stride=16)\n\n    _anchors = sess.run(anchors)\n    print(_anchors)\n\n"""
libs/box_utils/nms.py,2,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef non_maximal_suppression(boxes, scores, iou_threshold, max_output_size, name='non_maximal_suppression'):\n    with tf.variable_scope(name):\n        nms_index = tf.image.non_max_suppression(\n            boxes=boxes,\n            scores=scores,\n            max_output_size=max_output_size,\n            iou_threshold=iou_threshold,\n            name=name\n        )\n        return nms_index"""
libs/box_utils/nms_rotate.py,18,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport cv2\nfrom libs.configs import cfgs\nimport tensorflow as tf\nif cfgs.ROTATE_NMS_USE_GPU:\n    from libs.box_utils.rotate_polygon_nms import rotate_gpu_nms\n\n\ndef nms_rotate(decode_boxes, scores, iou_threshold, max_output_size,\n               use_angle_condition=False, angle_threshold=0, use_gpu=True, gpu_id=0):\n    """"""\n    :param boxes: format [x_c, y_c, w, h, theta]\n    :param scores: scores of boxes\n    :param threshold: iou threshold (0.7 or 0.5)\n    :param max_output_size: max number of output\n    :return: the remaining index of boxes\n    """"""\n\n    if use_gpu:\n        keep = nms_rotate_gpu(boxes_list=decode_boxes,\n                              scores=scores,\n                              iou_threshold=iou_threshold,\n                              angle_gap_threshold=angle_threshold,\n                              use_angle_condition=use_angle_condition,\n                              device_id=gpu_id)\n\n        keep = tf.cond(\n            tf.greater(tf.shape(keep)[0], max_output_size),\n            true_fn=lambda: tf.slice(keep, [0], [max_output_size]),\n            false_fn=lambda: keep)\n\n    else:\n        keep = tf.py_func(nms_rotate_cpu,\n                          inp=[decode_boxes, scores, iou_threshold, max_output_size],\n                          Tout=tf.int64)\n    return keep\n\n\ndef nms_rotate_cpu(boxes, scores, iou_threshold, max_output_size):\n\n    keep = []\n\n    order = scores.argsort()[::-1]\n    num = boxes.shape[0]\n\n    suppressed = np.zeros((num), dtype=np.int)\n\n    for _i in range(num):\n        if len(keep) >= max_output_size:\n            break\n\n        i = order[_i]\n        if suppressed[i] == 1:\n            continue\n        keep.append(i)\n        r1 = ((boxes[i, 1], boxes[i, 0]), (boxes[i, 3], boxes[i, 2]), boxes[i, 4])\n        area_r1 = boxes[i, 2] * boxes[i, 3]\n        for _j in range(_i + 1, num):\n            j = order[_j]\n            if suppressed[i] == 1:\n                continue\n            r2 = ((boxes[j, 1], boxes[j, 0]), (boxes[j, 3], boxes[j, 2]), boxes[j, 4])\n            area_r2 = boxes[j, 2] * boxes[j, 3]\n            inter = 0.0\n\n            int_pts = cv2.rotatedRectangleIntersection(r1, r2)[1]\n            if int_pts is not None:\n                order_pts = cv2.convexHull(int_pts, returnPoints=True)\n\n                int_area = cv2.contourArea(order_pts)\n\n                inter = int_area * 1.0 / (area_r1 + area_r2 - int_area + cfgs.EPSILON)\n\n            if inter >= iou_threshold:\n                suppressed[j] = 1\n\n    return np.array(keep, np.int64)\n\n\ndef nms_rotate_gpu(boxes_list, scores, iou_threshold, use_angle_condition=False, angle_gap_threshold=0, device_id=0):\n    if use_angle_condition:\n        y_c, x_c, h, w, theta = tf.unstack(boxes_list, axis=1)\n        boxes_list = tf.transpose(tf.stack([x_c, y_c, w, h, theta]))\n        det_tensor = tf.concat([boxes_list, tf.expand_dims(scores, axis=1)], axis=1)\n        keep = tf.py_func(rotate_gpu_nms,\n                          inp=[det_tensor, iou_threshold, device_id],\n                          Tout=tf.int64)\n        return keep\n    else:\n        y_c, x_c, h, w, theta = tf.unstack(boxes_list, axis=1)\n        boxes_list = tf.transpose(tf.stack([x_c, y_c, w, h, theta]))\n        det_tensor = tf.concat([boxes_list, tf.expand_dims(scores, axis=1)], axis=1)\n        keep = tf.py_func(rotate_gpu_nms,\n                          inp=[det_tensor, iou_threshold, device_id],\n                          Tout=tf.int64)\n        keep = tf.reshape(keep, [-1])\n        return keep\n\n\nif __name__ == \'__main__\':\n    boxes = np.array([[50, 50, 100, 100, 0],\n                      [60, 60, 100, 100, 0],\n                      [50, 50, 100, 100, -45.],\n                      [200, 200, 100, 100, 0.]])\n\n    scores = np.array([0.99, 0.88, 0.66, 0.77])\n\n    keep = nms_rotate(tf.convert_to_tensor(boxes, dtype=tf.float32), tf.convert_to_tensor(scores, dtype=tf.float32),\n                      0.7, 5)\n\n    import os\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'0\'\n    with tf.Session() as sess:\n        print(sess.run(keep))\n'"
libs/box_utils/setup.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nfrom os.path import join as pjoin\nfrom setuptools import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\nimport subprocess\nimport numpy as np\n\n\ndef find_in_path(name, path):\n    ""Find a file in a search path""\n    # Adapted fom\n    # http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\n\ndef locate_cuda():\n    """"""Locate the CUDA environment on the system\n\n    Returns a dict with keys \'home\', \'nvcc\', \'include\', and \'lib64\'\n    and values giving the absolute path to each directory.\n\n    Starts by looking for the CUDAHOME env variable. If not found, everything\n    is based on finding \'nvcc\' in the PATH.\n    """"""\n\n    # first check if the CUDAHOME env variable is in use\n    if \'CUDAHOME\' in os.environ:\n        home = os.environ[\'CUDAHOME\']\n        nvcc = pjoin(home, \'bin\', \'nvcc\')\n    else:\n        # otherwise, search the PATH for NVCC\n        default_path = pjoin(os.sep, \'usr\', \'local\', \'cuda\', \'bin\')\n        nvcc = find_in_path(\'nvcc\', os.environ[\'PATH\'] + os.pathsep + default_path)\n        if nvcc is None:\n            raise EnvironmentError(\'The nvcc binary could not be \'\n                \'located in your $PATH. Either add it to your path, or set $CUDAHOME\')\n        home = os.path.dirname(os.path.dirname(nvcc))\n\n    cudaconfig = {\'home\':home, \'nvcc\':nvcc,\n                  \'include\': pjoin(home, \'include\'),\n                  \'lib64\': pjoin(home, \'lib64\')}\n    for k, v in cudaconfig.items():\n        if not os.path.exists(v):\n            raise EnvironmentError(\'The CUDA %s path could not be located in %s\' % (k, v))\n\n    return cudaconfig\nCUDA = locate_cuda()\n\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to gcc/nvcc works.\n\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        if os.path.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', CUDA[\'nvcc\'])\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'gcc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\n\next_modules = [\n    Extension(\'rbbox_overlaps\',\n              [\'rbbox_overlaps_kernel.cu\', \'rbbox_overlaps.pyx\'],\n              library_dirs=[CUDA[\'lib64\']],\n              libraries=[\'cudart\'],\n              language=\'c++\',\n              runtime_library_dirs=[CUDA[\'lib64\']],\n              # this syntax is specific to this build system\n              # we\'re only going to use certain compiler args with nvcc and not with\n              # gcc the implementation of this trick is in customize_compiler() below\n              extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n                                  \'nvcc\': [\'-arch=sm_35\',\n                                           \'--ptxas-options=-v\',\n                                           \'-c\',\n                                           \'--compiler-options\',\n                                           ""\'-fPIC\'""]},\n              include_dirs=[numpy_include, CUDA[\'include\']]\n              ),\n    Extension(\'rotate_polygon_nms\',\n        [\'rotate_polygon_nms_kernel.cu\', \'rotate_polygon_nms.pyx\'],\n        library_dirs=[CUDA[\'lib64\']],\n        libraries=[\'cudart\'],\n        language=\'c++\',\n        runtime_library_dirs=[CUDA[\'lib64\']],\n        # this syntax is specific to this build system\n        # we\'re only going to use certain compiler args with nvcc and not with\n        # gcc the implementation of this trick is in customize_compiler() below\n        extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n                            \'nvcc\': [\'-arch=sm_35\',\n                                     \'--ptxas-options=-v\',\n                                     \'-c\',\n                                     \'--compiler-options\',\n                                     ""\'-fPIC\'""]},\n        include_dirs=[numpy_include, CUDA[\'include\']]\n    ),\n    Extension(\'iou_cpu\',\n              [\'iou_cpu.pyx\'],\n              library_dirs=[CUDA[\'lib64\']],\n              libraries=[\'cudart\'],\n              language=\'c++\',\n              runtime_library_dirs=[CUDA[\'lib64\']],\n              # this syntax is specific to this build system\n              # we\'re only going to use certain compiler args with nvcc and not with\n              # gcc the implementation of this trick is in customize_compiler() below\n              extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n                                  \'nvcc\': [\'-arch=sm_35\',\n                                           \'--ptxas-options=-v\',\n                                           \'-c\',\n                                           \'--compiler-options\',\n                                           ""\'-fPIC\'""]},\n              include_dirs=[numpy_include, CUDA[\'include\']])\n]\n\nsetup(\n    name=\'fast_rcnn\',\n    ext_modules=ext_modules,\n    # inject our custom trigger\n    cmdclass={\'build_ext\': custom_build_ext},\n)\n'"
libs/box_utils/show_box_in_tensor.py,31,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport cv2\nfrom libs.label_name_dict.label_dict import LABEl_NAME_MAP\n\n\ndef draw_box_in_img_batch(img_batch, boxes):\n    boxes = tf.cast(boxes, tf.float32)\n\n    ymin, xmin, ymax, xmax = tf.unstack(boxes, axis=1)\n\n    img_h, img_w = tf.shape(img_batch)[1], tf.shape(img_batch)[2]\n    abs_xmin = xmin / tf.cast(img_w, tf.float32)\n    abs_ymin = ymin / tf.cast(img_h, tf.float32)\n    abs_xmax = xmax / tf.cast(img_w, tf.float32)\n    abs_ymax = ymax / tf.cast(img_h, tf.float32)\n\n    return tf.image.draw_bounding_boxes(img_batch,\n                                        boxes=tf.expand_dims(tf.transpose(tf.stack([abs_ymin, abs_xmin,\n                                                                                    abs_ymax, abs_xmax])), 0))\n\n\ndef draw_box_with_color(img_batch, boxes, text):\n\n    def draw_box_cv(img, boxes, text):\n        img = img + np.array([103.939, 116.779, 123.68])\n        boxes = boxes.astype(np.int64)\n        img = np.array(img * 255 / np.max(img), np.uint8)\n        for box in boxes:\n            ymin, xmin, ymax, xmax = box[0], box[1], box[2], box[3]\n\n            color = (np.random.randint(255), np.random.randint(255), np.random.randint(255))\n            cv2.rectangle(img,\n                          pt1=(xmin, ymin),\n                          pt2=(xmax, ymax),\n                          color=color,\n                          thickness=2)\n\n        text = str(text)\n        cv2.putText(img,\n                    text=text,\n                    org=((img.shape[1]) // 2, (img.shape[0]) // 2),\n                    fontFace=3,\n                    fontScale=1,\n                    color=(255, 0, 0))\n\n        # img = np.transpose(img, [2, 1, 0])\n        img = img[:, :, -1::-1]\n        return img\n\n    img_tensor = tf.squeeze(img_batch, 0)\n    # color = tf.constant([0, 0, 255])\n    img_tensor_with_boxes = tf.py_func(draw_box_cv,\n                                       inp=[img_tensor, boxes, text],\n                                       Tout=[tf.uint8])\n\n    img_tensor_with_boxes = tf.reshape(img_tensor_with_boxes, tf.shape(img_batch))\n\n    return img_tensor_with_boxes\n\n\ndef draw_boxes_with_categories(img_batch, boxes, labels, scores):\n\n    def draw_box_cv(img, boxes, labels, scores):\n        img = img + np.array([103.939, 116.779, 123.68])\n        boxes = boxes.astype(np.int64)\n        labels = labels.astype(np.int32)\n        img = np.array(img*255/np.max(img), np.uint8)\n\n        num_of_object = 0\n        for i, box in enumerate(boxes):\n            ymin, xmin, ymax, xmax = box[0], box[1], box[2], box[3]\n\n            label = labels[i]\n            score = scores[i]\n            if label != 0:\n                num_of_object += 1\n                color = (np.random.randint(255), np.random.randint(255), np.random.randint(255))\n                cv2.rectangle(img,\n                              pt1=(xmin, ymin),\n                              pt2=(xmax, ymax),\n                              color=color,\n                              thickness=2)\n                cv2.rectangle(img,\n                              pt1=(xmin, ymin),\n                              pt2=(xmin+120, ymin+15),\n                              color=color,\n                              thickness=-1)\n                category = LABEl_NAME_MAP[label]\n                cv2.putText(img,\n                            text=category+"": ""+str(score),\n                            org=(xmin, ymin+10),\n                            fontFace=1,\n                            fontScale=1,\n                            thickness=2,\n                            color=(color[1], color[2], color[0]))\n        cv2.putText(img,\n                    text=str(num_of_object),\n                    org=((img.shape[1]) // 2, (img.shape[0]) // 2),\n                    fontFace=3,\n                    fontScale=1,\n                    color=(255, 0, 0))\n        img = img[:, :, ::-1]\n        return img\n\n    img_tensor = tf.squeeze(img_batch, 0)\n    img_tensor_with_boxes = tf.py_func(draw_box_cv,\n                                       inp=[img_tensor, boxes, labels, scores],\n                                       Tout=[tf.uint8])\n    img_tensor_with_boxes = tf.reshape(img_tensor_with_boxes, tf.shape(img_batch))\n    return img_tensor_with_boxes\n\n\ndef draw_boxes_with_categories_rotate(img_batch, boxes, labels, scores):\n\n    def draw_box_cv(img, boxes, labels, scores):\n        img = img + np.array([103.939, 116.779, 123.68])\n        boxes = boxes.astype(np.int64)\n        labels = labels.astype(np.int32)\n        img = np.array(img*255/np.max(img), np.uint8)\n\n        num_of_object = 0\n        for i, box in enumerate(boxes):\n\n            y_c, x_c, h, w, theta = box[0], box[1], box[2], box[3], box[4]\n            label = labels[i]\n            score = scores[i]\n            if label != 0:\n                num_of_object += 1\n\n                rect = ((x_c, y_c), (w, h), theta)\n                rect = cv2.boxPoints(rect)\n                rect = np.int0(rect)\n                color = (np.random.randint(255), np.random.randint(255), np.random.randint(255))\n                cv2.drawContours(img, [rect], -1, color, 3)\n\n                cv2.rectangle(img,\n                              pt1=(x_c, y_c),\n                              pt2=(x_c+120, y_c+15),\n                              color=color,\n                              thickness=-1)\n                category = LABEl_NAME_MAP[label]\n                cv2.putText(img,\n                            text=category+"": ""+str(score),\n                            org=(x_c, y_c+10),\n                            fontFace=1,\n                            fontScale=1,\n                            thickness=2,\n                            color=(color[1], color[2], color[0]))\n        cv2.putText(img,\n                    text=str(num_of_object),\n                    org=((img.shape[1]) // 2, (img.shape[0]) // 2),\n                    fontFace=3,\n                    fontScale=1,\n                    color=(255, 0, 0))\n        img = img[:, :, ::-1]\n        return img\n\n    img_tensor = tf.squeeze(img_batch, 0)\n    img_tensor_with_boxes = tf.py_func(draw_box_cv,\n                                       inp=[img_tensor, boxes, labels, scores],\n                                       Tout=[tf.uint8])\n    img_tensor_with_boxes = tf.reshape(img_tensor_with_boxes, tf.shape(img_batch))\n    return img_tensor_with_boxes\n\n\ndef draw_box_with_color_rotate(img_batch, boxes, text):\n\n    def draw_box_cv(img, boxes, text):\n        img = img + np.array([103.939, 116.779, 123.68])\n        boxes = boxes.astype(np.int64)\n        img = np.array(img * 255 / np.max(img), np.uint8)\n        for box in boxes:\n            y_c, x_c, h, w, theta = box[0], box[1], box[2], box[3], box[4]\n            rect = ((x_c, y_c), (w, h), theta)\n            rect = cv2.boxPoints(rect)\n            rect = np.int0(rect)\n            color = (np.random.randint(255), np.random.randint(255), np.random.randint(255))\n            cv2.drawContours(img, [rect], -1, color, 3)\n\n        text = str(text)\n        cv2.putText(img,\n                    text=text,\n                    org=((img.shape[1]) // 2, (img.shape[0]) // 2),\n                    fontFace=3,\n                    fontScale=1,\n                    color=(255, 0, 0))\n\n        img = img[:, :, ::-1]\n        return img\n\n    img_tensor = tf.squeeze(img_batch, 0)\n    img_tensor_with_boxes = tf.py_func(draw_box_cv,\n                                       inp=[img_tensor, boxes, text],\n                                       Tout=[tf.uint8])\n\n    img_tensor_with_boxes = tf.reshape(img_tensor_with_boxes, tf.shape(img_batch))\n\n    return img_tensor_with_boxes\n\n\nif __name__ == ""__main__"":\n\n    img = cv2.imread(\'1.jpg\')\n    img = tf.constant(np.expand_dims(img, 0), tf.float32)\n    boxes = tf.constant([[30, 30, 230, 230]])\n    labels = tf.constant([1])\n    scores = tf.constant([0.6])\n    img_ten = draw_boxes_with_categories(img, boxes, labels, scores)\n\n    with tf.Session() as sess:\n        img_np = sess.run(img_ten)\n        img_np = np.squeeze(img_np, 0)\n        cv2.imshow(\'test\', img_np)\n        cv2.waitKey(0)'"
libs/configs/__init__.py,0,b''
libs/configs/cfgs.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\n\n""""""\nv4-dense_feature_pyramid, v5-feature_pyramid\nTest(\xe5\xbf\xab\xe9\x80\x9f\xe8\xae\xad\xe7\xbb\x83):\nv4\n*********horizontal eval*********\n\xe6\xb0\xb4\xe5\xb9\xb3\xef\xbc\x8c 2000\xe5\xbc\xa0  3000 nms\nR: 0.803773584906\nP: 0.963236661407\nmAP: 0.776797872684\nF: 0.876309784923\n\n***********rotate eval***********\n\xe6\x97\x8b\xe8\xbd\xac\xef\xbc\x8c 2000\xe5\xbc\xa0  3000 nms\nR: 0.769496855346\nP: 0.89145996997\nmAP: 0.690753310464\nF: 0.826000571602\n\n\xe6\xb0\xb4\xe5\xb9\xb3\xe6\xa0\x87\xe5\x87\x86\xef\xbc\x9a\n*********horizontal eval*********\n\xe6\xb0\xb4\xe5\xb9\xb3\xef\xbc\x8c 2000\xe5\xbc\xa0  3000 nms iou 0.4\nR: 0.806289308176\nP: 0.967120965901\nmAP: 0.781793358603\nF: 0.879412176548\n\xe6\xb0\xb4\xe5\xb9\xb3\xef\xbc\x8c 2000\xe5\xbc\xa0  3000 nms iou 0.5\nR: 0.803773584906\nP: 0.963236661407\nmAP: 0.776797872684\nF: 0.876309784923\n\xe6\xb0\xb4\xe5\xb9\xb3\xef\xbc\x8c 2000\xe5\xbc\xa0  3000 nms iou 0.6\nR: 0.792452830189\nP: 0.951222839664\nmAP: 0.756438964669\nF: 0.864609450559\n\xe6\xb0\xb4\xe5\xb9\xb3\xef\xbc\x8c 2000\xe5\xbc\xa0  3000 nms iou 0.7\nR: 0.751257861635\nP: 0.893903438016\nmAP: 0.679421925197\nF: 0.816396526583\n***********rotate eval***********\n\xe6\x97\x8b\xe8\xbd\xac\xef\xbc\x8c 2000\xe5\xbc\xa0  3000 nms  iou 0.4\nR: 0.817924528302\nP: 0.942568041236\nmAP: 0.775231905212\nF: 0.87583388179\n\xe6\x97\x8b\xe8\xbd\xac\xef\xbc\x8c 2000\xe5\xbc\xa0  3000 nms  iou 0.5\nR: 0.808805031447\nP: 0.934789235475\nmAP: 0.760185750655\nF: 0.867245610218\n\xe6\x97\x8b\xe8\xbd\xac\xef\xbc\x8c 2000\xe5\xbc\xa0  3000 nms  iou 0.6\nR: 0.790880503145\nP: 0.915325169604\nmAP: 0.728443572557\nF: 0.848564557298\n\xe6\x97\x8b\xe8\xbd\xac\xef\xbc\x8c 2000\xe5\xbc\xa0  3000 nms  iou 0.7\nR: 0.730188679245\nP: 0.844232716838\nmAP: 0.62203781869\nF: 0.783080278275\n\nv5\n*********horizontal eval*********\n\xe6\xb0\xb4\xe5\xb9\xb3\xef\xbc\x8c 2000\xe5\xbc\xa0  3000 nms\nR: 0.81320754717\nP: 0.962663777256\nmAP: 0.785794751386\nF: 0.881646590363\n***********rotate eval***********\n\xe6\x97\x8b\xe8\xbd\xac\xef\xbc\x8c 2000\xe5\xbc\xa0  3000 nms\nR: 0.786163522013\nP: 0.885101755072\nmAP: 0.700995138344\nF: 0.832704086715\n\n\n\n\n\xe6\xb0\xb4\xe5\xb9\xb3\xe6\xa0\x87\xe5\x87\x86\xef\xbc\x9a\n*********horizontal eval*********\n\xe6\xb0\xb4\xe5\xb9\xb3\xef\xbc\x8c 2000\xe5\xbc\xa0  3000 nms  iou 0.4\nR: 0.814150943396\nP: 0.962856197701\nmAP: 0.786882346129\nF: 0.882281521085\n\xe6\xb0\xb4\xe5\xb9\xb3\xef\xbc\x8c 2000\xe5\xbc\xa0  3000 nms  iou 0.5\nR: 0.81320754717\nP: 0.962663777256\nmAP: 0.785794751386\nF: 0.881646590363\n\xe6\xb0\xb4\xe5\xb9\xb3\xef\xbc\x8c 2000\xe5\xbc\xa0  3000 nms  iou 0.6\nR: 0.802201257862\nP: 0.949192944629\nmAP: 0.76520138242\nF: 0.869528713812\n\xe6\xb0\xb4\xe5\xb9\xb3\xef\xbc\x8c 2000\xe5\xbc\xa0  3000 nms  iou 0.7\nR: 0.757547169811\nP: 0.900415522966\nmAP: 0.687343366523\nF: 0.822825789806\n***********rotate eval***********\n\xe6\x97\x8b\xe8\xbd\xac\xef\xbc\x8c 2000\xe5\xbc\xa0  3000 nms  iou 0.4\nR: 0.83679245283\nP: 0.942812670644\nmAP: 0.792656319873\nF: 0.886644477273\n\xe6\x97\x8b\xe8\xbd\xac\xef\xbc\x8c 2000\xe5\xbc\xa0  3000 nms  iou 0.5\nR: 0.825471698113\nP: 0.933186075468\nmAP: 0.773740173667\nF: 0.876030238451\n\xe6\x97\x8b\xe8\xbd\xac\xef\xbc\x8c 2000\xe5\xbc\xa0  3000 nms  iou 0.6\nR: 0.804402515723\nP: 0.907360638473\nmAP: 0.733757840362\nF: 0.852785244812\n\xe6\x97\x8b\xe8\xbd\xac\xef\xbc\x8c 2000\xe5\xbc\xa0  3000 nms  iou 0.7\nR: 0.740566037736\nP: 0.837118962891\nmAP: 0.624056832155\nF: 0.785888023548\n""""""\n\n# root path\nROOT_PATH = os.path.abspath(r\'C:\\Users\\yangxue\\Documents\\GitHub\\R2CNN_FPN_Tensorflow\')\n\n# pretrain weights path\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\nINFERENCE_IMAGE_PATH = ROOT_PATH + \'/tools/inference_image\'\nINFERENCE_SAVE_PATH = ROOT_PATH + \'/tools/inference_result\'\n\nNET_NAME = \'resnet_v1_101\'\nVERSION = \'v5\'\nCLASS_NUM = 1\nLEVEL = [\'P2\', \'P3\', \'P4\', \'P5\', \'P6\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nSTRIDE = [4, 8, 16, 32, 64]\nANCHOR_SCALES = [1.]\nANCHOR_RATIOS = [1 / 3., 1., 3.0]\nSCALE_FACTORS = [10., 10., 5., 5., 5.]\nOUTPUT_STRIDE = 16\nSHORT_SIDE_LEN = 600\nDATASET_NAME = \'ship\'\n\nBATCH_SIZE = 1\nWEIGHT_DECAY = {\'resnet_v1_50\': 0.0001, \'resnet_v1_101\': 0.0001}\nEPSILON = 1e-5\nMOMENTUM = 0.9\nMAX_ITERATION = 40000\nGPU_GROUP = ""1""\nLR = 0.001\n\n# rpn\nSHARE_HEAD = False\nRPN_NMS_IOU_THRESHOLD = 0.7\nMAX_PROPOSAL_NUM = 300\nRPN_IOU_POSITIVE_THRESHOLD = 0.7\nRPN_IOU_NEGATIVE_THRESHOLD = 0.3\nRPN_MINIBATCH_SIZE = 256\nRPN_POSITIVE_RATE = 0.5\nIS_FILTER_OUTSIDE_BOXES = True\nRPN_TOP_K_NMS = 3000\nFEATURE_PYRAMID_MODE = 0  # {0: \'feature_pyramid\', 1: \'dense_feature_pyramid\'}\n\n# fast rcnn\nROTATE_NMS_USE_GPU = True\nFAST_RCNN_MODE = \'build_fast_rcnn1\'\nROI_SIZE = 14\nROI_POOL_KERNEL_SIZE = 2\nUSE_DROPOUT = False\nKEEP_PROB = 0.5\nFAST_RCNN_NMS_IOU_THRESHOLD = 0.15\nFAST_RCNN_NMS_MAX_BOXES_PER_CLASS = 20\nFINAL_SCORE_THRESHOLD = 0.9\nFAST_RCNN_IOU_POSITIVE_THRESHOLD = 0.5\nFAST_RCNN_MINIBATCH_SIZE = 128\nFAST_RCNN_POSITIVE_RATE = 0.25\n'"
libs/fast_rcnn/__init__.py,0,b''
libs/fast_rcnn/build_fast_rcnn.py,85,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom libs.box_utils import encode_and_decode\nfrom libs.box_utils import boxes_utils\nfrom libs.box_utils import iou\nfrom libs.losses import losses\nfrom help_utils.help_utils import print_tensors\nimport numpy as np\n\nDEBUG = False\n\n\nclass FastRCNN(object):\n    def __init__(self,\n                 feature_pyramid, rpn_proposals_boxes, rpn_proposals_scores,\n                 img_shape,\n                 roi_size,\n                 scale_factors,\n                 roi_pool_kernel_size,  # roi size = initial_crop_size / max_pool_kernel size\n                 gtboxes_and_label,\n                 gtboxes_and_label_minAreaRectangle, # [M, 5]\n                 fast_rcnn_nms_iou_threshold,\n                 fast_rcnn_maximum_boxes_per_img,\n                 fast_rcnn_nms_max_boxes_per_class,\n                 show_detections_score_threshold,  # show box scores larger than this threshold\n\n                 num_classes,  # exclude background\n                 fast_rcnn_minibatch_size,\n                 fast_rcnn_positives_ratio,\n                 fast_rcnn_positives_iou_threshold,\n                 use_dropout,\n                 is_training,\n                 weight_decay,\n                 level):\n\n        self.feature_pyramid = feature_pyramid\n        self.rpn_proposals_boxes = rpn_proposals_boxes  # [N, 4]\n        self.rpn_proposals_scores = rpn_proposals_scores\n\n        self.img_shape = img_shape\n        self.roi_size = roi_size\n        self.roi_pool_kernel_size = roi_pool_kernel_size\n        self.level = level\n        self.min_level = int(level[0][1])\n        self.max_level = min(int(level[-1][1]), 5)\n\n        self.fast_rcnn_nms_iou_threshold = fast_rcnn_nms_iou_threshold\n        self.fast_rcnn_nms_max_boxes_per_class = fast_rcnn_nms_max_boxes_per_class\n        self.fast_rcnn_maximum_boxes_per_img = fast_rcnn_maximum_boxes_per_img\n        self.show_detections_score_threshold = show_detections_score_threshold\n\n        self.scale_factors = scale_factors\n        # larger than 0.5 is positive, others are negative\n        self.fast_rcnn_positives_iou_threshold = fast_rcnn_positives_iou_threshold\n\n        self.fast_rcnn_minibatch_size = fast_rcnn_minibatch_size\n        self.fast_rcnn_positives_ratio = fast_rcnn_positives_ratio\n\n        self.gtboxes_and_label = gtboxes_and_label\n        self.gtboxes_and_label_minAreaRectangle = gtboxes_and_label_minAreaRectangle\n        self.num_classes = num_classes\n        self.use_dropout = use_dropout\n        self.is_training = is_training\n        self.weight_decay = weight_decay\n\n        self.fast_rcnn_all_level_rois, self.fast_rcnn_all_level_proposals = self.get_rois()\n        self.fast_rcnn_encode_boxes, self.fast_rcnn_scores = self.fast_rcnn_net()\n\n    def assign_level(self):\n        with tf.name_scope('assign_levels'):\n            ymin, xmin, ymax, xmax = tf.unstack(self.rpn_proposals_boxes, axis=1)\n\n            w = tf.maximum(xmax - xmin, 0.)  # avoid w is negative\n            h = tf.maximum(ymax - ymin, 0.)  # avoid h is negative\n\n            levels = tf.round(4. + tf.log(tf.sqrt(w*h + 1e-8)/224.0) / tf.log(2.))  # 4 + log_2(***)\n\n            levels = tf.maximum(levels, tf.ones_like(levels)*(np.float32(self.min_level)))  # level minimum is 2\n            levels = tf.minimum(levels, tf.ones_like(levels)*(np.float32(self.max_level)))  # level maximum is 5\n\n            return tf.cast(levels, tf.int32)\n\n    def get_rois(self):\n        '''\n        1)get roi from feature map\n        2)roi align or roi pooling. Here is roi align\n        :return:\n        all_level_rois: [N, 7, 7, C]\n        all_level_proposals : [N, 4]\n        all_level_proposals is matched with all_level_rois\n        '''\n        levels = self.assign_level()\n\n        all_level_roi_list = []\n        all_level_proposal_list = []\n        if DEBUG:\n            print_tensors(levels, 'levels')\n        with tf.variable_scope('fast_rcnn_roi'):\n            # P6 is not used by the Fast R-CNN detector.\n            for i in range(self.min_level, self.max_level + 1):\n                level_i_proposal_indices = tf.reshape(tf.where(tf.equal(levels, i)), [-1])\n                level_i_proposals = tf.gather(self.rpn_proposals_boxes, level_i_proposal_indices)\n\n                level_i_proposals = tf.cond(\n                    tf.equal(tf.shape(level_i_proposals)[0], 0),\n                    lambda: tf.constant([[0, 0, 0, 0]], dtype=tf.float32),\n                    lambda: level_i_proposals\n                )  # to avoid level_i_proposals batch is 0, or it will broken when gradient BP\n\n                all_level_proposal_list.append(level_i_proposals)\n\n                ymin, xmin, ymax, xmax = tf.unstack(level_i_proposals, axis=1)\n                img_h, img_w = tf.cast(self.img_shape[1], tf.float32), tf.cast(self.img_shape[2], tf.float32)\n                normalize_ymin = ymin / img_h\n                normalize_xmin = xmin / img_w\n                normalize_ymax = ymax / img_h\n                normalize_xmax = xmax / img_w\n\n                level_i_cropped_rois = tf.image.crop_and_resize(self.feature_pyramid['P%d' % i],\n                                                                boxes=tf.transpose(tf.stack([normalize_ymin, normalize_xmin,\n                                                                                             normalize_ymax, normalize_xmax])),\n                                                                box_ind=tf.zeros(shape=[tf.shape(level_i_proposals)[0], ],\n                                                                                 dtype=tf.int32),\n                                                                crop_size=[self.roi_size, self.roi_size]\n                                                                )\n                level_i_rois = slim.max_pool2d(level_i_cropped_rois,\n                                               [self.roi_pool_kernel_size, self.roi_pool_kernel_size],\n                                               stride=self.roi_pool_kernel_size)\n                all_level_roi_list.append(level_i_rois)\n\n            all_level_rois = tf.concat(all_level_roi_list, axis=0)\n            all_level_proposals = tf.concat(all_level_proposal_list, axis=0)\n\n            return all_level_rois, all_level_proposals\n\n    def fast_rcnn_net(self):\n\n        with tf.variable_scope('fast_rcnn_net'):\n            with slim.arg_scope([slim.fully_connected], weights_regularizer=slim.l2_regularizer(self.weight_decay)):\n\n                flatten_rois_features = slim.flatten(self.fast_rcnn_all_level_rois)\n\n                net = slim.fully_connected(flatten_rois_features, 1024, scope='fc_1')\n                if self.use_dropout:\n                    net = slim.dropout(net, keep_prob=0.5, is_training=self.is_training, scope='dropout')\n\n                net = slim.fully_connected(net, 1024, scope='fc_2')\n\n                fast_rcnn_scores = slim.fully_connected(net, self.num_classes + 1, activation_fn=None,\n                                                          scope='classifier')\n\n                fast_rcnn_encode_boxes = slim.fully_connected(net, self.num_classes * 4, activation_fn=None,\n                                                                 scope='regressor')\n            if DEBUG:\n                print_tensors(fast_rcnn_encode_boxes, 'fast_rcnn_encode_bxes')\n\n            return fast_rcnn_encode_boxes, fast_rcnn_scores\n\n    def fast_rcnn_find_positive_negative_samples(self, reference_boxes):\n        '''\n        when training, we should know each reference box's label and gtbox,\n        in second stage\n        iou >= 0.5 is object\n        iou < 0.5 is background\n        :param reference_boxes: [num_of_input_boxes, 4]\n        :return:\n        reference_boxes_mattached_gtboxes: each reference box mattched gtbox, shape: [num_of_input_boxes, 4]\n        object_mask: indicate box(a row) weather is a object, 1 is object, 0 is background\n        category_label: indicate box's class, one hot encoding. shape: [num_of_input_boxes, num_classes+1]\n        '''\n\n        with tf.variable_scope('fast_rcnn_find_positive_negative_samples'):\n            gtboxes = tf.cast(\n                tf.reshape(self.gtboxes_and_label_minAreaRectangle[:, :-1], [-1, 4]), tf.float32)  # [M, 4]\n            ious = iou.iou_calculate(reference_boxes, gtboxes)  # [N, M]\n\n            matchs = tf.cast(tf.argmax(ious, axis=1), tf.int32)  # [N, ]\n            max_iou_each_row = tf.reduce_max(ious, axis=1)\n            # [N, ]\n            positives = tf.cast(tf.greater_equal(max_iou_each_row, self.fast_rcnn_positives_iou_threshold), tf.int32)\n\n            # matchs = matchs * greater_than_threshold_indicator\n            # in matchs, negative is 0, object is 0, 1, 2, ... ,num_of_classes\n\n            reference_boxes_mattached_gtboxes = tf.gather(gtboxes, matchs)  # [N, 4]\n\n            object_mask = tf.cast(positives, tf.float32)  # [N, ]\n            # when box is background, not caculate gradient, so give a weight 0 to avoid caculate gradient\n\n            label = tf.gather(self.gtboxes_and_label_minAreaRectangle[:, -1], matchs)  # [N, ]\n            label = tf.cast(label, tf.int32) * positives  # background is 0\n            # label = tf.one_hot(category_label, depth=self.num_classes + 1)\n\n            return reference_boxes_mattached_gtboxes, object_mask, label\n\n    def fast_rcnn_minibatch(self, reference_boxes):\n        with tf.variable_scope('fast_rcnn_minibatch'):\n\n            reference_boxes_mattached_gtboxes, object_mask, label = \\\n                self.fast_rcnn_find_positive_negative_samples(reference_boxes)\n\n            positive_indices = tf.reshape(tf.where(tf.not_equal(object_mask, 0.)), [-1])\n\n            num_of_positives = tf.minimum(tf.shape(positive_indices)[0],\n                                          tf.cast(self.fast_rcnn_minibatch_size*self.fast_rcnn_positives_ratio, tf.int32))\n\n            positive_indices = tf.random_shuffle(positive_indices)\n            positive_indices = tf.slice(positive_indices, begin=[0], size=[num_of_positives])\n\n            negative_indices = tf.reshape(tf.where(tf.equal(object_mask, 0.)), [-1])\n            num_of_negatives = tf.minimum(tf.shape(negative_indices)[0],\n                                          self.fast_rcnn_minibatch_size - num_of_positives)\n\n            negative_indices = tf.random_shuffle(negative_indices)\n            negative_indices = tf.slice(negative_indices, begin=[0], size=[num_of_negatives])\n\n            minibatch_indices = tf.concat([positive_indices, negative_indices], axis=0)\n            minibatch_indices = tf.random_shuffle(minibatch_indices)\n\n            minibatch_reference_boxes_mattached_gtboxes = tf.gather(reference_boxes_mattached_gtboxes,\n                                                                      minibatch_indices)\n            object_mask = tf.gather(object_mask, minibatch_indices)\n            label = tf.gather(label, minibatch_indices)\n            label_one_hot = tf.one_hot(label, self.num_classes + 1)\n\n            return minibatch_indices, minibatch_reference_boxes_mattached_gtboxes, object_mask, label_one_hot\n\n    def fast_rcnn_loss(self):\n        with tf.variable_scope('fast_rcnn_loss'):\n            minibatch_indices, minibatch_reference_boxes_mattached_gtboxes, minibatch_object_mask, \\\n            minibatch_label_one_hot = self.fast_rcnn_minibatch(self.fast_rcnn_all_level_proposals)\n\n            minibatch_reference_boxes = tf.gather(self.fast_rcnn_all_level_proposals, minibatch_indices)\n\n            minibatch_encode_boxes = tf.gather(self.fast_rcnn_encode_boxes,\n                                               minibatch_indices)  # [minibatch_size, num_classes*4]\n            minibatch_scores = tf.gather(self.fast_rcnn_scores, minibatch_indices)\n\n            # encode gtboxes\n            minibatch_encode_gtboxes = \\\n                encode_and_decode.encode_boxes(\n                    unencode_boxes=minibatch_reference_boxes_mattached_gtboxes,\n                    reference_boxes=minibatch_reference_boxes,\n                    scale_factors=self.scale_factors\n                )\n\n            # [minibatch_size, num_classes*4]\n            minibatch_encode_gtboxes = tf.tile(minibatch_encode_gtboxes, [1, self.num_classes])\n\n            class_weights_list = []\n            category_list = tf.unstack(minibatch_label_one_hot, axis=1)\n            for i in range(1, self.num_classes+1):\n                tmp_class_weights = tf.ones(shape=[tf.shape(minibatch_encode_boxes)[0], 4], dtype=tf.float32)\n                tmp_class_weights = tmp_class_weights * tf.expand_dims(category_list[i], axis=1)\n                class_weights_list.append(tmp_class_weights)\n            class_weights = tf.concat(class_weights_list, axis=1)  # [minibatch_size, num_classes*4]\n\n            # loss\n            with tf.variable_scope('fast_rcnn_classification_loss'):\n                fast_rcnn_classification_loss = slim.losses.softmax_cross_entropy(logits=minibatch_scores,\n                                                                                  onehot_labels=minibatch_label_one_hot)\n            with tf.variable_scope('fast_rcnn_location_loss'):\n                fast_rcnn_location_loss = losses.l1_smooth_losses(predict_boxes=minibatch_encode_boxes,\n                                                                  gtboxes=minibatch_encode_gtboxes,\n                                                                  object_weights=minibatch_object_mask,\n                                                                  classes_weights=class_weights)\n                slim.losses.add_loss(fast_rcnn_location_loss)\n\n            return fast_rcnn_location_loss, fast_rcnn_classification_loss\n\n    def fast_rcnn_proposals(self, decode_boxes, scores):\n        '''\n        mutilclass NMS\n        :param decode_boxes: [N, num_classes*4]\n        :param scores: [N, num_classes+1]\n        :return:\n        detection_boxes : [-1, 4]\n        scores : [-1, ]\n\n        '''\n\n        with tf.variable_scope('fast_rcnn_proposals'):\n            category = tf.argmax(scores, axis=1)\n\n            object_mask = tf.cast(tf.not_equal(category, 0), tf.float32)\n\n            decode_boxes = decode_boxes * tf.expand_dims(object_mask, axis=1)  # make background box is [0 0 0 0]\n            scores = scores * tf.expand_dims(object_mask, axis=1)\n\n            decode_boxes = tf.reshape(decode_boxes, [-1, self.num_classes, 4])  # [N, num_classes, 4]\n\n            decode_boxes_list = tf.unstack(decode_boxes, axis=1)\n            score_list = tf.unstack(scores[:, 1:], axis=1)\n            after_nms_boxes = []\n            after_nms_scores = []\n            category_list = []\n            for per_class_decode_boxes, per_class_scores in zip(decode_boxes_list, score_list):\n\n                valid_indices = boxes_utils.nms_boxes(per_class_decode_boxes, per_class_scores,\n                                                      iou_threshold=self.fast_rcnn_nms_iou_threshold,\n                                                      max_output_size=self.fast_rcnn_nms_max_boxes_per_class,\n                                                      name='second_stage_NMS')\n\n                after_nms_boxes.append(tf.gather(per_class_decode_boxes, valid_indices))\n                after_nms_scores.append(tf.gather(per_class_scores, valid_indices))\n                tmp_category = tf.gather(category, valid_indices)\n\n                category_list.append(tmp_category)\n\n            all_nms_boxes = tf.concat(after_nms_boxes, axis=0)\n            all_nms_scores = tf.concat(after_nms_scores, axis=0)\n            all_category = tf.concat(category_list, axis=0)\n\n            all_nms_boxes = boxes_utils.clip_boxes_to_img_boundaries(all_nms_boxes,\n                                                                     img_shape=self.img_shape)\n\n            scores_large_than_threshold_indices = tf.reshape(tf.where(tf.greater(all_nms_scores,\n                                                                                 self.show_detections_score_threshold)), [-1])\n\n            all_nms_boxes = tf.gather(all_nms_boxes, scores_large_than_threshold_indices)\n            all_nms_scores = tf.gather(all_nms_scores, scores_large_than_threshold_indices)\n            all_category = tf.gather(all_category, scores_large_than_threshold_indices)\n\n            return all_nms_boxes, all_nms_scores, tf.shape(all_nms_boxes)[0], all_category  # num of objects\n\n    def fast_rcnn_predict(self):\n\n        with tf.variable_scope('fast_rcnn_predict'):\n            fast_rcnn_softmax_scores = slim.softmax(self.fast_rcnn_scores)  # [-1, num_classes+1]\n\n            fast_rcnn_encode_boxes = tf.reshape(self.fast_rcnn_encode_boxes, [-1, 4])\n\n            reference_boxes = tf.tile(self.fast_rcnn_all_level_proposals, [1, self.num_classes])  # [N, 4*num_classes]\n            reference_boxes = tf.reshape(reference_boxes, [-1, 4])   # [N*num_classes, 4]\n            fast_rcnn_decode_boxes = encode_and_decode.decode_boxes(encode_boxes=fast_rcnn_encode_boxes,\n                                                                    reference_boxes=reference_boxes,\n                                                                    scale_factors=self.scale_factors)\n\n            fast_rcnn_decode_boxes = boxes_utils.clip_boxes_to_img_boundaries(fast_rcnn_decode_boxes,\n                                                                              img_shape=self.img_shape)\n\n            # mutilclass NMS\n            fast_rcnn_decode_boxes = tf.reshape(fast_rcnn_decode_boxes, [-1, self.num_classes*4])\n            fast_rcnn_decode_boxes, fast_rcnn_score, num_of_objects, detection_category = \\\n                self.fast_rcnn_proposals(fast_rcnn_decode_boxes, scores=fast_rcnn_softmax_scores)\n\n            return fast_rcnn_decode_boxes, fast_rcnn_score, num_of_objects, detection_category\n\n\n\n\n\n\n\n\n\n\n\n\n"""
libs/fast_rcnn/build_fast_rcnn1.py,119,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom libs.box_utils import encode_and_decode\nfrom libs.box_utils import boxes_utils\nfrom libs.box_utils import iou\nfrom libs.losses import losses\nfrom help_utils.help_utils import print_tensors\nfrom libs.box_utils import nms_rotate\nimport numpy as np\nfrom libs.configs import cfgs\n\nDEBUG = False\n\n\nclass FastRCNN(object):\n    def __init__(self,\n                 feature_pyramid, rpn_proposals_boxes, rpn_proposals_scores,\n                 img_shape,\n                 roi_size,\n                 scale_factors,\n                 roi_pool_kernel_size,  # roi size = initial_crop_size / max_pool_kernel size\n                 gtboxes_and_label,\n                 gtboxes_and_label_minAreaRectangle,  # [M, 5]\n                 fast_rcnn_nms_iou_threshold,\n                 fast_rcnn_maximum_boxes_per_img,\n                 fast_rcnn_nms_max_boxes_per_class,\n                 show_detections_score_threshold,  # show box scores larger than this threshold\n\n                 num_classes,  # exclude background\n                 fast_rcnn_minibatch_size,\n                 fast_rcnn_positives_ratio,\n                 fast_rcnn_positives_iou_threshold,\n                 use_dropout,\n                 is_training,\n                 weight_decay,\n                 level):\n\n        self.feature_pyramid = feature_pyramid\n        self.rpn_proposals_boxes = rpn_proposals_boxes  # [N, 4]\n        self.rpn_proposals_scores = rpn_proposals_scores\n\n        self.img_shape = img_shape\n        self.roi_size = roi_size\n        self.roi_pool_kernel_size = roi_pool_kernel_size\n        self.level = level\n        self.min_level = int(level[0][1])\n        self.max_level = min(int(level[-1][1]), 5)\n\n        self.fast_rcnn_nms_iou_threshold = fast_rcnn_nms_iou_threshold\n        self.fast_rcnn_nms_max_boxes_per_class = fast_rcnn_nms_max_boxes_per_class\n        self.fast_rcnn_maximum_boxes_per_img = fast_rcnn_maximum_boxes_per_img\n        self.show_detections_score_threshold = show_detections_score_threshold\n\n        self.scale_factors = scale_factors\n        # larger than 0.5 is positive, others are negative\n        self.fast_rcnn_positives_iou_threshold = fast_rcnn_positives_iou_threshold\n\n        self.fast_rcnn_minibatch_size = fast_rcnn_minibatch_size\n        self.fast_rcnn_positives_ratio = fast_rcnn_positives_ratio\n\n        self.gtboxes_and_label = gtboxes_and_label\n        self.gtboxes_and_label_minAreaRectangle = gtboxes_and_label_minAreaRectangle\n        self.num_classes = num_classes\n        self.use_dropout = use_dropout\n        self.is_training = is_training\n        self.weight_decay = weight_decay\n\n        self.fast_rcnn_all_level_rois, self.fast_rcnn_all_level_proposals = self.get_rois()\n        self.fast_rcnn_encode_boxes, self.fast_rcnn_scores, \\\n        self.fast_rcnn_encode_boxes_rotate, self.fast_rcnn_scores_rotate = self.fast_rcnn_net()\n\n    def assign_level(self):\n        with tf.name_scope('assign_levels'):\n            ymin, xmin, ymax, xmax = tf.unstack(self.rpn_proposals_boxes, axis=1)\n\n            w = tf.maximum(xmax - xmin, 0.)  # avoid w is negative\n            h = tf.maximum(ymax - ymin, 0.)  # avoid h is negative\n\n            levels = tf.round(4. + tf.log(tf.sqrt(w*h + 1e-8)/224.0) / tf.log(2.))\n\n            levels = tf.maximum(levels, tf.ones_like(levels) * (np.float32(self.min_level)))  # level minimum is 2\n            levels = tf.minimum(levels, tf.ones_like(levels) * (np.float32(self.max_level)))  # level maximum is 5\n\n            return tf.cast(levels, tf.int32)\n\n    def get_rois(self):\n        '''\n           1)get roi from feature map\n           2)roi align or roi pooling. Here is roi align\n           :return:\n           all_level_rois: [N, 7, 7, C]\n           all_level_proposals : [N, 4]\n           all_level_proposals is matched with all_level_rois\n        '''\n        levels = self.assign_level()\n\n        all_level_roi_list = []\n        all_level_proposal_list = []\n        if DEBUG:\n            print_tensors(levels, 'levels')\n        with tf.variable_scope('fast_rcnn_roi'):\n            # P6 is not used by the Fast R-CNN detector.\n            for i in range(self.min_level, self.max_level + 1):\n                level_i_proposal_indices = tf.reshape(tf.where(tf.equal(levels, i)), [-1])\n                level_i_proposals = tf.gather(self.rpn_proposals_boxes, level_i_proposal_indices)\n\n                level_i_proposals = tf.cond(\n                    tf.equal(tf.shape(level_i_proposals)[0], 0),\n                    lambda: tf.constant([[0, 0, 0, 0]], dtype=tf.float32),\n                    lambda: level_i_proposals\n                )  # to avoid level_i_proposals batch is 0, or it will broken when gradient BP\n\n                all_level_proposal_list.append(level_i_proposals)\n\n                ymin, xmin, ymax, xmax = tf.unstack(level_i_proposals, axis=1)\n                img_h, img_w = tf.cast(self.img_shape[1], tf.float32), tf.cast(self.img_shape[2], tf.float32)\n                normalize_ymin = ymin / img_h\n                normalize_xmin = xmin / img_w\n                normalize_ymax = ymax / img_h\n                normalize_xmax = xmax / img_w\n\n                level_i_cropped_rois = tf.image.crop_and_resize(self.feature_pyramid['P%d' % i],\n                                                                boxes=tf.transpose(tf.stack([normalize_ymin, normalize_xmin,\n                                                                                             normalize_ymax, normalize_xmax])),\n                                                                box_ind=tf.zeros(shape=[tf.shape(level_i_proposals)[0], ],\n                                                                                 dtype=tf.int32),\n                                                                crop_size=[self.roi_size, self.roi_size]\n                                                                )\n                level_i_rois = slim.max_pool2d(level_i_cropped_rois,\n                                               [self.roi_pool_kernel_size, self.roi_pool_kernel_size],\n                                               stride=self.roi_pool_kernel_size)\n                all_level_roi_list.append(level_i_rois)\n\n            all_level_rois = tf.concat(all_level_roi_list, axis=0)\n            all_level_proposals = tf.concat(all_level_proposal_list, axis=0)\n            return all_level_rois, all_level_proposals\n\n    def fast_rcnn_net(self):\n\n        with tf.variable_scope('fast_rcnn_net'):\n            with slim.arg_scope([slim.fully_connected], weights_regularizer=slim.l2_regularizer(self.weight_decay)):\n\n                flatten_rois_features = slim.flatten(self.fast_rcnn_all_level_rois)\n\n                net = slim.fully_connected(flatten_rois_features, 1024, scope='fc_1')\n                if self.use_dropout:\n                    net = slim.dropout(net, keep_prob=0.5, is_training=self.is_training, scope='dropout')\n\n                net = slim.fully_connected(net, 1024, scope='fc_2')\n\n                fast_rcnn_scores = slim.fully_connected(net, self.num_classes + 1, activation_fn=None,\n                                                          scope='classifier')\n\n                fast_rcnn_encode_boxes = slim.fully_connected(net, self.num_classes * 4, activation_fn=None,\n                                                                 scope='regressor')\n            if DEBUG:\n                print_tensors(fast_rcnn_encode_boxes, 'fast_rcnn_encode_bxes')\n\n        with tf.variable_scope('fast_rcnn_net_rotate'):\n            with slim.arg_scope([slim.fully_connected], weights_regularizer=slim.l2_regularizer(self.weight_decay)):\n\n                flatten_rois_features_rotate = slim.flatten(self.fast_rcnn_all_level_rois)\n\n                net_rotate = slim.fully_connected(flatten_rois_features_rotate, 1024, scope='fc_1')\n                if self.use_dropout:\n                    net_rotate = slim.dropout(net_rotate, keep_prob=0.5, is_training=self.is_training, scope='dropout')\n\n                net_rotate = slim.fully_connected(net_rotate, 1024, scope='fc_2')\n\n                fast_rcnn_scores_rotate = slim.fully_connected(net_rotate, self.num_classes + 1, activation_fn=None,\n                                                               scope='classifier')\n\n                fast_rcnn_encode_boxes_rotate = slim.fully_connected(net_rotate, self.num_classes * 5,\n                                                                     activation_fn=None,\n                                                                     scope='regressor')\n\n            return fast_rcnn_encode_boxes, fast_rcnn_scores, fast_rcnn_encode_boxes_rotate, fast_rcnn_scores_rotate\n\n    def fast_rcnn_find_positive_negative_samples(self, reference_boxes):\n        '''\n        when training, we should know each reference box's label and gtbox,\n        in second stage\n        iou >= 0.5 is object\n        iou < 0.5 is background\n        :param reference_boxes: [num_of_input_boxes, 4]\n        :return:\n        reference_boxes_mattached_gtboxes: each reference box mattched gtbox, shape: [num_of_input_boxes, 4]\n        object_mask: indicate box(a row) weather is a object, 1 is object, 0 is background\n        category_label: indicate box's class, one hot encoding. shape: [num_of_input_boxes, num_classes+1]\n        '''\n\n        with tf.variable_scope('fast_rcnn_find_positive_negative_samples'):\n            gtboxes = tf.cast(\n                tf.reshape(self.gtboxes_and_label_minAreaRectangle[:, :-1], [-1, 4]), tf.float32)  # [M, 4]\n\n            gtboxes_rotate = tf.cast(\n                tf.reshape(self.gtboxes_and_label[:, :-1], [-1, 5]), tf.float32)  # [M, 5]\n\n            ious = iou.iou_calculate(reference_boxes, gtboxes)  # [N, M]\n\n            matchs = tf.cast(tf.argmax(ious, axis=1), tf.int32)  # [N, ]\n            max_iou_each_row = tf.reduce_max(ious, axis=1)\n            # [N, ]\n            positives = tf.cast(tf.greater_equal(max_iou_each_row, self.fast_rcnn_positives_iou_threshold), tf.int32)\n\n            reference_boxes_mattached_gtboxes = tf.gather(gtboxes, matchs)  # [N, 4]\n            reference_boxes_mattached_gtboxes_rotate = tf.gather(gtboxes_rotate, matchs)\n\n            object_mask = tf.cast(positives, tf.float32)  # [N, ]\n\n            label = tf.gather(self.gtboxes_and_label_minAreaRectangle[:, -1], matchs)  # [N, ]\n            label = tf.cast(label, tf.int32) * positives  # background is 0\n\n            return reference_boxes_mattached_gtboxes, reference_boxes_mattached_gtboxes_rotate, object_mask, label\n\n    def fast_rcnn_minibatch(self, reference_boxes):\n        with tf.variable_scope('fast_rcnn_minibatch'):\n\n            reference_boxes_mattached_gtboxes, reference_boxes_mattached_gtboxes_rotate, object_mask, label = \\\n                self.fast_rcnn_find_positive_negative_samples(reference_boxes)\n\n            positive_indices = tf.reshape(tf.where(tf.not_equal(object_mask, 0.)), [-1])\n\n            num_of_positives = tf.minimum(tf.shape(positive_indices)[0],\n                                          tf.cast(self.fast_rcnn_minibatch_size*self.fast_rcnn_positives_ratio, tf.int32))\n\n            positive_indices = tf.random_shuffle(positive_indices)\n            positive_indices = tf.slice(positive_indices, begin=[0], size=[num_of_positives])\n\n            negative_indices = tf.reshape(tf.where(tf.equal(object_mask, 0.)), [-1])\n            num_of_negatives = tf.minimum(tf.shape(negative_indices)[0],\n                                          self.fast_rcnn_minibatch_size - num_of_positives)\n\n            negative_indices = tf.random_shuffle(negative_indices)\n            negative_indices = tf.slice(negative_indices, begin=[0], size=[num_of_negatives])\n\n            minibatch_indices = tf.concat([positive_indices, negative_indices], axis=0)\n            minibatch_indices = tf.random_shuffle(minibatch_indices)\n\n            minibatch_reference_boxes_mattached_gtboxes = tf.gather(reference_boxes_mattached_gtboxes,\n                                                                    minibatch_indices)\n\n            minibatch_reference_boxes_mattached_gtboxes_rotate \\\n                = tf.gather(reference_boxes_mattached_gtboxes_rotate, minibatch_indices)\n\n            object_mask = tf.gather(object_mask, minibatch_indices)\n            label = tf.gather(label, minibatch_indices)\n            label_one_hot = tf.one_hot(label, self.num_classes + 1)\n\n            return minibatch_indices, minibatch_reference_boxes_mattached_gtboxes, \\\n                   minibatch_reference_boxes_mattached_gtboxes_rotate, object_mask, label_one_hot\n\n    def fast_rcnn_loss(self):\n        with tf.variable_scope('fast_rcnn_loss'):\n            minibatch_indices, minibatch_reference_boxes_mattached_gtboxes, \\\n            minibatch_reference_boxes_mattached_gtboxes_rotate, minibatch_object_mask, \\\n            minibatch_label_one_hot = self.fast_rcnn_minibatch(self.fast_rcnn_all_level_proposals)\n\n            minibatch_reference_boxes = tf.gather(self.fast_rcnn_all_level_proposals, minibatch_indices)\n\n            minibatch_encode_boxes = tf.gather(self.fast_rcnn_encode_boxes,\n                                               minibatch_indices)  # [minibatch_size, num_classes*4]\n\n            minibatch_encode_boxes_rotate = tf.gather(self.fast_rcnn_encode_boxes_rotate,\n                                                      minibatch_indices)  # [minibatch_size, num_classes*5]\n\n            minibatch_scores = tf.gather(self.fast_rcnn_scores, minibatch_indices)\n            minibatch_scores_rotate = tf.gather(self.fast_rcnn_scores_rotate, minibatch_indices)\n\n            # encode gtboxes\n            minibatch_encode_gtboxes = \\\n                encode_and_decode.encode_boxes(\n                    unencode_boxes=minibatch_reference_boxes_mattached_gtboxes,\n                    reference_boxes=minibatch_reference_boxes,\n                    scale_factors=self.scale_factors\n                )\n\n            minibatch_encode_gtboxes_rotate = encode_and_decode.encode_boxes_rotate(\n                unencode_boxes=minibatch_reference_boxes_mattached_gtboxes_rotate,\n                reference_boxes=minibatch_reference_boxes,\n                scale_factors=self.scale_factors\n            )\n\n            # [minibatch_size, num_classes*4]\n            minibatch_encode_gtboxes = tf.tile(minibatch_encode_gtboxes, [1, self.num_classes])\n            # [minibatch_size, num_classes*5]\n            minibatch_encode_gtboxes_rotate = tf.tile(minibatch_encode_gtboxes_rotate, [1, self.num_classes])\n\n            class_weights_list = []\n            category_list = tf.unstack(minibatch_label_one_hot, axis=1)\n            for i in range(1, self.num_classes+1):\n                tmp_class_weights = tf.ones(shape=[tf.shape(minibatch_encode_boxes)[0], 4], dtype=tf.float32)\n                tmp_class_weights = tmp_class_weights * tf.expand_dims(category_list[i], axis=1)\n                class_weights_list.append(tmp_class_weights)\n            class_weights = tf.concat(class_weights_list, axis=1)  # [minibatch_size, num_classes*4]\n\n            class_weights_list_rotate = []\n            category_list_rotate = tf.unstack(minibatch_label_one_hot, axis=1)\n            for i in range(1, self.num_classes + 1):\n                tmp_class_weights_rotate = tf.ones(shape=[tf.shape(minibatch_encode_boxes_rotate)[0], 5], dtype=tf.float32)\n                tmp_class_weights_rotate = tmp_class_weights_rotate * tf.expand_dims(category_list_rotate[i], axis=1)\n                class_weights_list_rotate.append(tmp_class_weights_rotate)\n            class_weights_rotate = tf.concat(class_weights_list_rotate, axis=1)  # [minibatch_size, num_classes*5]\n\n            # loss\n            with tf.variable_scope('fast_rcnn_classification_loss'):\n                fast_rcnn_classification_loss = slim.losses.softmax_cross_entropy(logits=minibatch_scores,\n                                                                                  onehot_labels=minibatch_label_one_hot)\n            with tf.variable_scope('fast_rcnn_location_loss'):\n                fast_rcnn_location_loss = losses.l1_smooth_losses(predict_boxes=minibatch_encode_boxes,\n                                                                  gtboxes=minibatch_encode_gtboxes,\n                                                                  object_weights=minibatch_object_mask,\n                                                                  classes_weights=class_weights)\n                slim.losses.add_loss(fast_rcnn_location_loss)\n\n            with tf.variable_scope('fast_rcnn_classification_rotate_loss'):\n                fast_rcnn_classification_rotate_loss = slim.losses.softmax_cross_entropy(logits=minibatch_scores_rotate,\n                                                                                         onehot_labels=minibatch_label_one_hot)\n\n            with tf.variable_scope('fast_rcnn_location_rotate_loss'):\n                fast_rcnn_location_rotate_loss = losses.l1_smooth_losses(predict_boxes=minibatch_encode_boxes_rotate,\n                                                                         gtboxes=minibatch_encode_gtboxes_rotate,\n                                                                         object_weights=minibatch_object_mask,\n                                                                         classes_weights=class_weights_rotate)\n                slim.losses.add_loss(fast_rcnn_location_rotate_loss)\n\n            return fast_rcnn_location_loss, fast_rcnn_classification_loss, \\\n                   fast_rcnn_location_rotate_loss, fast_rcnn_classification_rotate_loss\n\n    def fast_rcnn_proposals(self, decode_boxes, scores):\n        '''\n        mutilclass NMS\n        :param decode_boxes: [N, num_classes*4]\n        :param scores: [N, num_classes+1]\n        :return:\n        detection_boxes : [-1, 4]\n        scores : [-1, ]\n\n        '''\n\n        with tf.variable_scope('fast_rcnn_proposals'):\n            category = tf.argmax(scores, axis=1)\n\n            object_mask = tf.cast(tf.not_equal(category, 0), tf.float32)\n\n            decode_boxes = decode_boxes * tf.expand_dims(object_mask, axis=1)  # make background box is [0 0 0 0]\n            scores = scores * tf.expand_dims(object_mask, axis=1)\n\n            decode_boxes = tf.reshape(decode_boxes, [-1, self.num_classes, 4])  # [N, num_classes, 4]\n\n            decode_boxes_list = tf.unstack(decode_boxes, axis=1)\n            score_list = tf.unstack(scores[:, 1:], axis=1)\n            after_nms_boxes = []\n            after_nms_scores = []\n            category_list = []\n            for per_class_decode_boxes, per_class_scores in zip(decode_boxes_list, score_list):\n\n                valid_indices = boxes_utils.nms_boxes(per_class_decode_boxes, per_class_scores,\n                                                      iou_threshold=self.fast_rcnn_nms_iou_threshold,\n                                                      max_output_size=self.fast_rcnn_nms_max_boxes_per_class,\n                                                      name='second_stage_NMS')\n\n                after_nms_boxes.append(tf.gather(per_class_decode_boxes, valid_indices))\n                after_nms_scores.append(tf.gather(per_class_scores, valid_indices))\n                tmp_category = tf.gather(category, valid_indices)\n\n                category_list.append(tmp_category)\n\n            all_nms_boxes = tf.concat(after_nms_boxes, axis=0)\n            all_nms_scores = tf.concat(after_nms_scores, axis=0)\n            all_category = tf.concat(category_list, axis=0)\n\n            all_nms_boxes = boxes_utils.clip_boxes_to_img_boundaries(all_nms_boxes,\n                                                                     img_shape=self.img_shape)\n\n            scores_large_than_threshold_indices = tf.reshape(tf.where(tf.greater(all_nms_scores,\n                                                                                 self.show_detections_score_threshold)), [-1])\n\n            all_nms_boxes = tf.gather(all_nms_boxes, scores_large_than_threshold_indices)\n            all_nms_scores = tf.gather(all_nms_scores, scores_large_than_threshold_indices)\n            all_category = tf.gather(all_category, scores_large_than_threshold_indices)\n\n            return all_nms_boxes, all_nms_scores, tf.shape(all_nms_boxes)[0], all_category\n\n    def fast_rcnn_proposals_rotate(self, decode_boxes, scores):\n        '''\n        mutilclass NMS\n        :param decode_boxes: [N, num_classes*5]\n        :param scores: [N, num_classes+1]\n        :return:\n        detection_boxes : [-1, 5]\n        scores : [-1, ]\n\n        '''\n\n        with tf.variable_scope('fast_rcnn_proposals'):\n            category = tf.argmax(scores, axis=1)\n\n            object_mask = tf.cast(tf.not_equal(category, 0), tf.float32)\n\n            decode_boxes = decode_boxes * tf.expand_dims(object_mask, axis=1)  # make background box is [0 0 0 0, 0]\n            scores = scores * tf.expand_dims(object_mask, axis=1)\n\n            decode_boxes = tf.reshape(decode_boxes, [-1, self.num_classes, 5])  # [N, num_classes, 5]\n\n            decode_boxes_list = tf.unstack(decode_boxes, axis=1)\n            score_list = tf.unstack(scores[:, 1:], axis=1)\n            after_nms_boxes = []\n            after_nms_scores = []\n            category_list = []\n            for per_class_decode_boxes, per_class_scores in zip(decode_boxes_list, score_list):\n\n                valid_indices = nms_rotate.nms_rotate(decode_boxes=per_class_decode_boxes,\n                                                      scores=per_class_scores,\n                                                      iou_threshold=self.fast_rcnn_nms_iou_threshold,\n                                                      max_output_size=self.fast_rcnn_nms_max_boxes_per_class,\n                                                      use_angle_condition=False,\n                                                      angle_threshold=15,\n                                                      use_gpu=cfgs.ROTATE_NMS_USE_GPU)\n\n                after_nms_boxes.append(tf.gather(per_class_decode_boxes, valid_indices))\n                after_nms_scores.append(tf.gather(per_class_scores, valid_indices))\n                tmp_category = tf.gather(category, valid_indices)\n\n                category_list.append(tmp_category)\n\n            all_nms_boxes = tf.concat(after_nms_boxes, axis=0)\n            all_nms_scores = tf.concat(after_nms_scores, axis=0)\n            all_category = tf.concat(category_list, axis=0)\n\n            # all_nms_boxes = boxes_utils.clip_boxes_to_img_boundaries(all_nms_boxes,\n            #                                                          img_shape=self.img_shape)\n\n            scores_large_than_threshold_indices = \\\n                tf.reshape(tf.where(tf.greater(all_nms_scores, self.show_detections_score_threshold)), [-1])\n\n            all_nms_boxes = tf.gather(all_nms_boxes, scores_large_than_threshold_indices)\n            all_nms_scores = tf.gather(all_nms_scores, scores_large_than_threshold_indices)\n            all_category = tf.gather(all_category, scores_large_than_threshold_indices)\n\n            return all_nms_boxes, all_nms_scores, tf.shape(all_nms_boxes)[0], all_category\n\n    def fast_rcnn_predict(self):\n\n        with tf.variable_scope('fast_rcnn_predict'):\n            fast_rcnn_softmax_scores = slim.softmax(self.fast_rcnn_scores)  # [-1, num_classes+1]\n            fast_rcnn_softmax_scores_rotate = slim.softmax(self.fast_rcnn_scores_rotate)  # [-1, num_classes+1]\n\n            fast_rcnn_encode_boxes = tf.reshape(self.fast_rcnn_encode_boxes, [-1, 4])\n            fast_rcnn_encode_boxes_rotate = tf.reshape(self.fast_rcnn_encode_boxes_rotate, [-1, 5])\n\n            reference_boxes = tf.tile(self.fast_rcnn_all_level_proposals, [1, self.num_classes])  # [N, 4*num_classes]\n            reference_boxes = tf.reshape(reference_boxes, [-1, 4])   # [N*num_classes, 4]\n            fast_rcnn_decode_boxes = encode_and_decode.decode_boxes(encode_boxes=fast_rcnn_encode_boxes,\n                                                                    reference_boxes=reference_boxes,\n                                                                    scale_factors=self.scale_factors)\n\n            fast_rcnn_decode_boxes_rotate = \\\n                encode_and_decode.decode_boxes_rotate(encode_boxes=fast_rcnn_encode_boxes_rotate,\n                                                      reference_boxes=reference_boxes,\n                                                      scale_factors=self.scale_factors)\n\n            fast_rcnn_decode_boxes = boxes_utils.clip_boxes_to_img_boundaries(fast_rcnn_decode_boxes,\n                                                                              img_shape=self.img_shape)\n\n            # mutilclass NMS\n            fast_rcnn_decode_boxes = tf.reshape(fast_rcnn_decode_boxes, [-1, self.num_classes*4])\n            fast_rcnn_decode_boxes_rotate = tf.reshape(fast_rcnn_decode_boxes_rotate, [-1, self.num_classes * 5])\n\n            fast_rcnn_decode_boxes, fast_rcnn_score, num_of_objects, detection_category = \\\n                self.fast_rcnn_proposals(fast_rcnn_decode_boxes, scores=fast_rcnn_softmax_scores)\n\n            fast_rcnn_decode_boxes_rotate, fast_rcnn_score_rotate, num_of_objects_rotate, detection_category_rotate = \\\n                self.fast_rcnn_proposals_rotate(fast_rcnn_decode_boxes_rotate, scores=fast_rcnn_softmax_scores_rotate)\n\n            return fast_rcnn_decode_boxes, fast_rcnn_score, num_of_objects, detection_category,\\\n                   fast_rcnn_decode_boxes_rotate, fast_rcnn_score_rotate, num_of_objects_rotate, detection_category_rotate\n\n\n\n\n\n\n\n\n\n\n\n\n"""
libs/label_name_dict/__init__.py,0,b''
libs/label_name_dict/label_dict.py,0,"b""# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\n\nfrom libs.configs import cfgs\n\nif cfgs.DATASET_NAME == 'ship':\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'ship': 1\n    }\nelif cfgs.DATASET_NAME == 'aeroplane':\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'aeroplane': 1\n    }\nelif cfgs.DATASET_NAME == 'pascal':\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'aeroplane': 1,\n        'bicycle': 2,\n        'bird': 3,\n        'boat': 4,\n        'bottle': 5,\n        'bus': 6,\n        'car': 7,\n        'cat': 8,\n        'chair': 9,\n        'cow': 10,\n        'diningtable': 11,\n        'dog': 12,\n        'horse': 13,\n        'motorbike': 14,\n        'person': 15,\n        'pottedplant': 16,\n        'sheep': 17,\n        'sofa': 18,\n        'train': 19,\n        'tvmonitor': 20\n    }\nelse:\n    assert 'please set label dict!'\n\n\ndef get_label_name_map():\n    reverse_dict = {}\n    for name, label in NAME_LABEL_MAP.items():\n        reverse_dict[label] = name\n    return reverse_dict\n\nLABEl_NAME_MAP = get_label_name_map()"""
libs/losses/__init__.py,0,b''
libs/losses/losses.py,12,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef l1_smooth_losses(predict_boxes, gtboxes, object_weights, classes_weights=None):\n    '''\n\n    :param predict_boxes: [minibatch_size, -1]\n    :param gtboxes: [minibatch_size, -1]\n    :param object_weights: [minibatch_size, ]. 1.0 represent object, 0.0 represent others(ignored or background)\n    :return:\n    '''\n\n    diff = predict_boxes - gtboxes\n    abs_diff = tf.cast(tf.abs(diff), tf.float32)\n\n    if classes_weights is None:\n        '''\n        first_stage:\n        predict_boxes :[minibatch_size, 4]\n        gtboxes: [minibatchs_size, 4]\n        '''\n        anchorwise_smooth_l1norm = tf.reduce_sum(\n            tf.where(tf.less(abs_diff, 1), 0.5 * tf.square(abs_diff), abs_diff - 0.5),\n            axis=1) * object_weights\n    else:\n        '''\n        fast_rcnn:\n        predict_boxes: [minibatch_size, 4*num_classes]\n        gtboxes: [minibatch_size, 4*num_classes]\n        classes_weights : [minibatch_size, 4*num_classes]\n        '''\n        anchorwise_smooth_l1norm = tf.reduce_sum(\n            tf.where(tf.less(abs_diff, 1), 0.5*tf.square(abs_diff)*classes_weights,\n                     (abs_diff - 0.5)*classes_weights),\n            axis=1)*object_weights\n    return tf.reduce_mean(anchorwise_smooth_l1norm, axis=0)  # reduce mean\n\n\ndef weighted_softmax_cross_entropy_loss(predictions, labels, weights):\n    '''\n\n    :param predictions:\n    :param labels:\n    :param weights: [N, ] 1 -> should be sampled , 0-> not should be sampled\n    :return:\n    # '''\n    per_row_cross_ent = tf.nn.softmax_cross_entropy_with_logits(logits=predictions,\n                                                                labels=labels)\n\n    weighted_cross_ent = tf.reduce_sum(per_row_cross_ent * weights)\n    return weighted_cross_ent / tf.reduce_sum(weights)\n\n\ndef test_smoothl1():\n\n    predict_boxes = tf.constant([[1, 1, 2, 2],\n                                [2, 2, 2, 2],\n                                [3, 3, 3, 3]])\n    gtboxes = tf.constant([[1, 1, 1, 1],\n                          [2, 1, 1, 1],\n                          [3, 3, 2, 1]])\n\n    loss = l1_smooth_losses(predict_boxes, gtboxes, [1, 1, 1])\n\n    with tf.Session() as sess:\n        print(sess.run(loss))\n\nif __name__ == '__main__':\n    test_smoothl1()\n"""
libs/networks/__init__.py,0,b''
libs/networks/network_factory.py,0,"b'from __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nimport os,sys\nsys.path.insert(0, \'../../\')\n\nfrom libs.networks.slim_nets import resnet_v1\nfrom libs.networks.slim_nets import mobilenet_v1\nfrom libs.networks.slim_nets import inception_resnet_v2\nfrom libs.networks.slim_nets import vgg\n\n# FLAGS = get_flags_byname()\n\n\ndef get_flags_byname(net_name):\n    if net_name not in [\'resnet_v1_50\', \'mobilenet_224\', \'inception_resnet\', \'vgg16\', \'resnet_v1_101\']:\n        raise ValueError(""not include network: {}, we allow resnet_50, mobilenet_224, inception_resnet,""\n                         "" vgg16, resnet_v1_101""\n                         """")\n\n    if net_name == \'resnet_v1_50\':\n        from configs import config_resnet_50\n        return config_resnet_50.FLAGS\n    if net_name == \'mobilenet_224\':\n        from configs import config_mobilenet_224\n        return config_mobilenet_224.FLAGS\n    if net_name == \'inception_resnet\':\n        from configs import config_inception_resnet\n        return config_inception_resnet.FLAGS\n    if net_name == \'vgg16\':\n        from configs import config_vgg16\n        return config_vgg16.FLAGS\n    if net_name == \'resnet_v1_101\':\n        from configs import config_res101\n        return config_res101.FLAGS\n\n\ndef get_network_byname(net_name,\n                       inputs,\n                       num_classes=None,\n                       is_training=True,\n                       global_pool=True,\n                       output_stride=None,\n                       spatial_squeeze=True):\n    if net_name == \'resnet_v1_50\':\n        FLAGS = get_flags_byname(net_name)\n        with slim.arg_scope(resnet_v1.resnet_arg_scope(weight_decay=FLAGS.weight_decay)):\n            logits, end_points = resnet_v1.resnet_v1_50(inputs=inputs,\n                                                        num_classes=num_classes,\n                                                        is_training=is_training,\n                                                        global_pool=global_pool,\n                                                        output_stride=output_stride,\n                                                        spatial_squeeze=spatial_squeeze\n                                                        )\n\n        return logits, end_points\n    if net_name == \'resnet_v1_101\':\n        FLAGS = get_flags_byname(net_name)\n        with slim.arg_scope(resnet_v1.resnet_arg_scope(weight_decay=FLAGS.weight_decay)):\n            logits, end_points = resnet_v1.resnet_v1_101(inputs=inputs,\n                                                         num_classes=num_classes,\n                                                         is_training=is_training,\n                                                         global_pool=global_pool,\n                                                         output_stride=output_stride,\n                                                         spatial_squeeze=spatial_squeeze\n                                                         )\n        return logits, end_points\n\n    # if net_name == \'inception_resnet\':\n    #     FLAGS = get_flags_byname(net_name)\n    #     arg_sc = inception_resnet_v2.inception_resnet_v2_arg_scope(weight_decay=FLAGS.weight_decay)\n    #     with slim.arg_scope(arg_sc):\n    #         logits, end_points = inception_resnet_v2.inception_resnet_v2(inputs=inputs,\n    #                                                                      num_classes=num_classes,\n    #                                                                      is_training=is_training)\n    #\n    #     return logits, end_points\n    #\n    # if net_name == \'vgg16\':\n    #     FLAGS = get_flags_byname(net_name)\n    #     arg_sc = vgg.vgg_arg_scope(weight_decay=FLAGS.weight_decay)\n    #     with slim.arg_scope(arg_sc):\n    #         logits, end_points = vgg.vgg_16(inputs=inputs,\n    #                                         num_classes=num_classes,\n    #                                         is_training=is_training)\n    #     return logits, end_points\n'"
libs/rpn/__init__.py,0,b''
libs/rpn/build_rpn.py,90,"b""# # -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport tensorflow.contrib.slim as slim\nfrom libs.configs import cfgs\nfrom libs.box_utils import anchor_utils_pyfunc, make_anchor, nms\nfrom libs.box_utils import boxes_utils\nfrom libs.box_utils import iou\nfrom libs.box_utils import encode_and_decode\nfrom libs.box_utils.show_box_in_tensor import draw_box_with_color\nfrom libs.losses import losses\nfrom help_utils.help_utils import print_tensors\nDEBUG = True\n\n\nclass RPN(object):\n    def __init__(self, net_name, inputs, gtboxes_and_label,\n                 is_training,\n                 share_net,\n                 anchor_ratios,\n                 anchor_scales,\n                 scale_factors,\n                 base_anchor_size_list,  # P2, P3, P4, P5, P6\n                 stride,\n                 level,\n                 top_k_nms,\n                 share_head=False,\n                 rpn_nms_iou_threshold=0.7,\n                 max_proposals_num=300,\n                 rpn_iou_positive_threshold=0.7,\n                 rpn_iou_negative_threshold=0.3,  # iou>=0.7 is positive box, iou< 0.3 is negative\n                 rpn_mini_batch_size=256,\n                 rpn_positives_ratio=0.5,\n                 remove_outside_anchors=False,  # whether remove anchors outside\n                 rpn_weight_decay=0.0001,\n                 ):\n\n        self.net_name = net_name\n        self.img_batch = inputs\n        self.gtboxes_and_label = gtboxes_and_label  # shape is [M. 5],\n\n        self.base_anchor_size_list = base_anchor_size_list\n\n        self.anchor_ratios = tf.constant(anchor_ratios, dtype=tf.float32)\n        self.anchor_scales = tf.constant(anchor_scales, dtype=tf.float32)\n        self.share_head = share_head\n        self.num_of_anchors_per_location = len(anchor_scales) * len(anchor_ratios)\n\n        self.scale_factors = scale_factors\n        self.stride = stride\n        self.level = level\n        self.top_k_nms = top_k_nms\n\n        self.rpn_nms_iou_threshold = rpn_nms_iou_threshold\n        self.max_proposals_num = max_proposals_num\n\n        self.rpn_iou_positive_threshold = rpn_iou_positive_threshold\n        self.rpn_iou_negative_threshold = rpn_iou_negative_threshold\n        self.rpn_mini_batch_size = rpn_mini_batch_size\n        self.rpn_positives_ratio = rpn_positives_ratio\n        self.remove_outside_anchors = remove_outside_anchors\n        self.rpn_weight_decay = rpn_weight_decay\n        self.is_training = is_training\n        self.share_net = share_net\n\n        self.feature_maps_dict = self.get_feature_maps()\n        if cfgs.FEATURE_PYRAMID_MODE == 0:\n            self.feature_pyramid = self.build_feature_pyramid()\n        else:\n            self.feature_pyramid = self.build_dense_feature_pyramid()\n\n        self.anchors, self.rpn_encode_boxes, self.rpn_scores = self.get_anchors_and_rpn_predict()\n\n    def get_feature_maps(self):\n\n        '''\n            Compared to https://github.com/KaimingHe/deep-residual-networks, the implementation of resnet_50 in slim\n            subsample the output activations in the last residual unit of each block,\n            instead of subsampling the input activations in the first residual unit of each block.\n            The two implementations give identical results but the implementation of slim is more memory efficient.\n\n            SO, when we build feature_pyramid, we should modify the value of 'C_*' to get correct spatial size feature maps.\n            :return: feature maps\n        '''\n\n        with tf.variable_scope('get_feature_maps'):\n            if self.net_name == 'resnet_v1_50':\n                feature_maps_dict = {\n                    'C2': self.share_net['resnet_v1_50/block1/unit_2/bottleneck_v1'],  # [56, 56]\n                    'C3': self.share_net['resnet_v1_50/block2/unit_3/bottleneck_v1'],  # [28, 28]\n                    'C4': self.share_net['resnet_v1_50/block3/unit_5/bottleneck_v1'],  # [14, 14]\n                    'C5': self.share_net['resnet_v1_50/block4']  # [7, 7]\n                }\n            elif self.net_name == 'resnet_v1_101':\n                feature_maps_dict = {\n                    'C2': self.share_net['resnet_v1_101/block1/unit_2/bottleneck_v1'],  # [56, 56]\n                    'C3': self.share_net['resnet_v1_101/block2/unit_3/bottleneck_v1'],  # [28, 28]\n                    'C4': self.share_net['resnet_v1_101/block3/unit_22/bottleneck_v1'],  # [14, 14]\n                    'C5': self.share_net['resnet_v1_101/block4']  # [7, 7]\n                }\n            else:\n                raise Exception('get no feature maps')\n\n            return feature_maps_dict\n\n    def build_dense_feature_pyramid(self):\n        '''\n        reference: DenseNet\n        build P2, P3, P4, P5, P6\n        :return: multi-scale feature map\n        '''\n\n        feature_pyramid = {}\n        with tf.variable_scope('dense_feature_pyramid'):\n            with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(self.rpn_weight_decay)):\n                feature_pyramid['P5'] = slim.conv2d(self.feature_maps_dict['C5'],\n                                                    num_outputs=256,\n                                                    kernel_size=[1, 1],\n                                                    stride=1,\n                                                    scope='build_P5')\n\n                feature_pyramid['P6'] = slim.max_pool2d(feature_pyramid['P5'],\n                                                        kernel_size=[2, 2], stride=2, scope='build_P6')\n                # P6 is down sample of P5\n\n                for layer in range(4, 1, -1):\n                    c = self.feature_maps_dict['C' + str(layer)]\n                    c_conv = slim.conv2d(c, num_outputs=256, kernel_size=[1, 1], stride=1,\n                                         scope='build_P%d/reduce_dimension' % layer)\n                    p_concat = [c_conv]\n                    up_sample_shape = tf.shape(c)\n                    for layer_top in range(5, layer, -1):\n                        p_temp = feature_pyramid['P' + str(layer_top)]\n\n                        p_sub = tf.image.resize_nearest_neighbor(p_temp, [up_sample_shape[1], up_sample_shape[2]],\n                                                                 name='build_P%d/up_sample_nearest_neighbor' % layer)\n                        p_concat.append(p_sub)\n\n                    p = tf.concat(p_concat, axis=3)\n\n                    p_conv = slim.conv2d(p, 256, kernel_size=[3, 3], stride=[1, 1],\n                                         padding='SAME', scope='build_P%d/avoid_aliasing' % layer)\n                    feature_pyramid['P' + str(layer)] = p_conv\n\n        return feature_pyramid\n\n    def build_feature_pyramid(self):\n\n        '''\n        reference: https://github.com/CharlesShang/FastMaskRCNN\n        build P2, P3, P4, P5\n        :return: multi-scale feature map\n        '''\n\n        feature_pyramid = {}\n        with tf.variable_scope('build_feature_pyramid'):\n            with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(self.rpn_weight_decay)):\n                feature_pyramid['P5'] = slim.conv2d(self.feature_maps_dict['C5'],\n                                                    num_outputs=256,\n                                                    kernel_size=[1, 1],\n                                                    stride=1,\n                                                    scope='build_P5')\n\n                feature_pyramid['P6'] = slim.max_pool2d(feature_pyramid['P5'],\n                                                        kernel_size=[2, 2], stride=2, scope='build_P6')\n                # P6 is down sample of P5\n\n                for layer in range(4, 1, -1):\n                    p, c = feature_pyramid['P' + str(layer + 1)], self.feature_maps_dict['C' + str(layer)]\n                    up_sample_shape = tf.shape(c)\n                    up_sample = tf.image.resize_nearest_neighbor(p, [up_sample_shape[1], up_sample_shape[2]],\n                                                                 name='build_P%d/up_sample_nearest_neighbor' % layer)\n\n                    c = slim.conv2d(c, num_outputs=256, kernel_size=[1, 1], stride=1,\n                                    scope='build_P%d/reduce_dimension' % layer)\n                    p = up_sample + c\n                    p = slim.conv2d(p, 256, kernel_size=[3, 3], stride=1,\n                                    padding='SAME', scope='build_P%d/avoid_aliasing' % layer)\n                    feature_pyramid['P' + str(layer)] = p\n\n        return feature_pyramid\n\n    def make_anchors(self):\n        with tf.variable_scope('make_anchors'):\n            anchor_list = []\n            level_list = self.level\n            with tf.name_scope('make_anchors_all_level'):\n                for level, base_anchor_size, stride in zip(level_list, self.base_anchor_size_list, self.stride):\n                    '''\n                    (level, base_anchor_size) tuple:\n                    (P2, 32), (P3, 64), (P4, 128), (P5, 256), (P6, 512)\n                    '''\n                    featuremap_height, featuremap_width = tf.shape(self.feature_pyramid[level])[1], \\\n                                                          tf.shape(self.feature_pyramid[level])[2]\n                    # stride = base_anchor_size / 8.\n\n                    # tmp_anchors = tf.py_func(\n                    #     anchor_utils_pyfunc.make_anchors,\n                    #     inp=[base_anchor_size, self.anchor_scales, self.anchor_ratios,\n                    #          featuremap_height, featuremap_width, stride],\n                    #     Tout=tf.float32\n                    # )\n\n                    tmp_anchors = make_anchor.make_anchors(base_anchor_size, self.anchor_scales, self.anchor_ratios,\n                                                           featuremap_height,  featuremap_width, stride,\n                                                           name='make_anchors_{}'.format(level))\n                    tmp_anchors = tf.reshape(tmp_anchors, [-1, 4])\n                    anchor_list.append(tmp_anchors)\n\n                all_level_anchors = tf.concat(anchor_list, axis=0)\n            return all_level_anchors\n\n    def rpn_net(self):\n\n        rpn_encode_boxes_list = []\n        rpn_scores_list = []\n        with tf.variable_scope('rpn_net'):\n            with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(self.rpn_weight_decay)):\n                for level in self.level:\n\n                    if self.share_head:\n                        reuse_flag = None if level == 'P2' else True\n                        scope_list = ['conv2d_3x3', 'rpn_classifier', 'rpn_regressor']\n                    else:\n                        reuse_flag = None\n                        scope_list = ['conv2d_3x3_'+level, 'rpn_classifier_'+level, 'rpn_regressor_'+level]\n\n                    rpn_conv2d_3x3 = slim.conv2d(inputs=self.feature_pyramid[level],\n                                                 num_outputs=256,\n                                                 kernel_size=[3, 3],\n                                                 stride=1,\n                                                 scope=scope_list[0],\n                                                 reuse=reuse_flag)\n                    rpn_box_scores = slim.conv2d(rpn_conv2d_3x3,\n                                                 num_outputs=2 * self.num_of_anchors_per_location,\n                                                 kernel_size=[1, 1],\n                                                 stride=1,\n                                                 scope=scope_list[1],\n                                                 activation_fn=None,\n                                                 reuse=reuse_flag)\n                    rpn_encode_boxes = slim.conv2d(rpn_conv2d_3x3,\n                                                   num_outputs=4 * self.num_of_anchors_per_location,\n                                                   kernel_size=[1, 1],\n                                                   stride=1,\n                                                   scope=scope_list[2],\n                                                   activation_fn=None,\n                                                   reuse=reuse_flag)\n\n                    rpn_box_scores = tf.reshape(rpn_box_scores, [-1, 2])\n                    rpn_encode_boxes = tf.reshape(rpn_encode_boxes, [-1, 4])\n\n                    rpn_scores_list.append(rpn_box_scores)\n                    rpn_encode_boxes_list.append(rpn_encode_boxes)\n\n                rpn_all_encode_boxes = tf.concat(rpn_encode_boxes_list, axis=0)\n                rpn_all_boxes_scores = tf.concat(rpn_scores_list, axis=0)\n\n            return rpn_all_encode_boxes, rpn_all_boxes_scores\n\n    def get_anchors_and_rpn_predict(self):\n\n        anchors = self.make_anchors()\n        rpn_encode_boxes, rpn_scores = self.rpn_net()\n\n        with tf.name_scope('get_anchors_and_rpn_predict'):\n            if self.is_training:\n                if self.remove_outside_anchors:\n                    valid_indices = boxes_utils.filter_outside_boxes(boxes=anchors,\n                                                                     img_h=tf.shape(self.img_batch)[1],\n                                                                     img_w=tf.shape(self.img_batch)[2])\n                    valid_anchors = tf.gather(anchors, valid_indices)\n                    rpn_valid_encode_boxes = tf.gather(rpn_encode_boxes, valid_indices)\n                    rpn_valid_scores = tf.gather(rpn_scores, valid_indices)\n\n                    return valid_anchors, rpn_valid_encode_boxes, rpn_valid_scores\n                else:\n                    return anchors, rpn_encode_boxes, rpn_scores\n            else:\n                return anchors, rpn_encode_boxes, rpn_scores\n\n    def rpn_find_positive_negative_samples(self, anchors):\n        '''\n        assign anchors targets: object or background.\n        :param anchors: [valid_num_of_anchors, 4]. use N to represent valid_num_of_anchors\n\n        :return:labels. anchors_matched_gtboxes, object_mask\n\n        labels shape is [N, ].  positive is 1, negative is 0, ignored is -1\n        anchor_matched_gtboxes. each anchor's gtbox(only positive box has gtbox)shape is [N, 4]\n        object_mask. tf.float32. 1.0 represent box is object, 0.0 is others. shape is [N, ]\n        '''\n        with tf.variable_scope('rpn_find_positive_negative_samples'):\n            gtboxes = tf.reshape(self.gtboxes_and_label[:, :-1], [-1, 4])\n            gtboxes = tf.cast(gtboxes, tf.float32)\n\n            ious = iou.iou_calculate(anchors, gtboxes)  # [N, M]\n\n            max_iou_each_row = tf.reduce_max(ious, axis=1)\n\n            labels = tf.ones(shape=[tf.shape(anchors)[0], ], dtype=tf.float32) * (-1)  # [N, ] # ignored is -1\n\n            matchs = tf.cast(tf.argmax(ious, axis=1), tf.int32)\n\n            # an anchor that has an IoU overlap higher than 0.7 with any ground-truth box\n            positives1 = tf.greater_equal(max_iou_each_row, self.rpn_iou_positive_threshold)  # iou >= 0.7 is positive\n\n            # to avoid none of boxes iou >= 0.7, use max iou boxes as positive\n            max_iou_each_column = tf.reduce_max(ious, 0)\n            # the anchor/anchors with the highest Intersection-over-Union (IoU) overlap with a ground-truth box\n            positives2 = tf.reduce_sum(tf.cast(tf.equal(ious, max_iou_each_column), tf.float32), axis=1)\n\n            positives = tf.logical_or(positives1, tf.cast(positives2, tf.bool))\n\n            labels += 2 * tf.cast(positives, tf.float32)  # Now, positive is 1, ignored and background is -1\n\n            anchors_matched_gtboxes = tf.gather(gtboxes, matchs)  # [N, 4]\n\n            negatives = tf.less(max_iou_each_row, self.rpn_iou_negative_threshold)\n            negatives = tf.logical_and(negatives, tf.greater_equal(max_iou_each_row, 0.1))\n\n            labels = labels + tf.cast(negatives, tf.float32)  # [N, ] positive is >=1.0, negative is 0, ignored is -1.0\n            '''\n                Need to note: when opsitive, labels may >= 1.0.\n                Because, when all the iou< 0.7, we set anchors having max iou each column as positive.\n                these anchors may have iou < 0.3.\n                In the begining, labels is [-1, -1, -1...-1]\n                then anchors having iou<0.3 as well as are max iou each column will be +1.0.\n                when decide negatives, because of iou<0.3, they add 1.0 again.\n                So, the final result will be 2.0\n    \n                So, when opsitive, labels may in [1.0, 2.0]. that is labels >=1.0\n            '''\n            positives = tf.cast(tf.greater_equal(labels, 1.0), tf.float32)\n            ignored = tf.cast(tf.equal(labels, -1.0), tf.float32) * -1\n\n            labels = positives + ignored\n            object_mask = tf.cast(positives, tf.float32)  # 1.0 is object, 0.0 is others\n\n            return labels, anchors_matched_gtboxes, object_mask\n\n    def make_minibatch(self, valid_anchors):\n        with tf.variable_scope('rpn_minibatch'):\n\n            # in labels(shape is [N, ]): 1 is positive, 0 is negative, -1 is ignored\n            labels, anchor_matched_gtboxes, object_mask = \\\n                self.rpn_find_positive_negative_samples(valid_anchors)  # [num_of_valid_anchors, ]\n\n            positive_indices = tf.reshape(tf.where(tf.equal(labels, 1.0)), [-1])  # use labels is same as object_mask\n\n            num_of_positives = tf.minimum(tf.shape(positive_indices)[0],\n                                          tf.cast(self.rpn_mini_batch_size * self.rpn_positives_ratio, tf.int32))\n\n            positive_indices = tf.random_shuffle(positive_indices)\n            positive_indices = tf.slice(positive_indices,\n                                        begin=[0],\n                                        size=[num_of_positives])\n\n            negatives_indices = tf.reshape(tf.where(tf.equal(labels, 0.0)), [-1])\n            num_of_negatives = tf.minimum(self.rpn_mini_batch_size - num_of_positives,\n                                          tf.shape(negatives_indices)[0])\n\n            negatives_indices = tf.random_shuffle(negatives_indices)\n            negatives_indices = tf.slice(negatives_indices, begin=[0], size=[num_of_negatives])\n\n            minibatch_indices = tf.concat([positive_indices, negatives_indices], axis=0)\n            minibatch_indices = tf.random_shuffle(minibatch_indices)\n\n            minibatch_anchor_matched_gtboxes = tf.gather(anchor_matched_gtboxes, minibatch_indices)\n            object_mask = tf.gather(object_mask, minibatch_indices)\n            labels = tf.cast(tf.gather(labels, minibatch_indices), tf.int32)\n            labels_one_hot = tf.one_hot(labels, depth=2)\n            return minibatch_indices, minibatch_anchor_matched_gtboxes, object_mask, labels_one_hot\n\n    def rpn_losses(self):\n        with tf.variable_scope('rpn_losses'):\n            minibatch_indices, minibatch_anchor_matched_gtboxes, object_mask, minibatch_labels_one_hot = \\\n                self.make_minibatch(self.anchors)\n\n            minibatch_anchors = tf.gather(self.anchors, minibatch_indices)\n            minibatch_encode_boxes = tf.gather(self.rpn_encode_boxes, minibatch_indices)\n            minibatch_boxes_scores = tf.gather(self.rpn_scores, minibatch_indices)\n\n            # encode gtboxes\n            minibatch_encode_gtboxes = encode_and_decode.encode_boxes(unencode_boxes=minibatch_anchor_matched_gtboxes,\n                                                                      reference_boxes=minibatch_anchors,\n                                                                      scale_factors=self.scale_factors)\n\n            positive_anchors_in_img = draw_box_with_color(self.img_batch,\n                                                          minibatch_anchors * tf.expand_dims(object_mask, 1),\n                                                          text=tf.shape(tf.where(tf.equal(object_mask, 1.0)))[0])\n\n            negative_mask = tf.cast(tf.logical_not(tf.cast(object_mask, tf.bool)), tf.float32)\n            negative_anchors_in_img = draw_box_with_color(self.img_batch,\n                                                          minibatch_anchors * tf.expand_dims(negative_mask, 1),\n                                                          text=tf.shape(tf.where(tf.equal(object_mask, 0.0)))[0])\n\n            minibatch_decode_boxes = encode_and_decode.decode_boxes(encode_boxes=minibatch_encode_boxes,\n                                                                    reference_boxes=minibatch_anchors,\n                                                                    scale_factors=self.scale_factors)\n\n            tf.summary.image('/positive_anchors', positive_anchors_in_img)\n            tf.summary.image('/negative_anchors', negative_anchors_in_img)\n            top_k_scores, top_k_indices = tf.nn.top_k(minibatch_boxes_scores[:, 1], k=5)\n\n            top_detections_in_img = draw_box_with_color(self.img_batch,\n                                                        tf.gather(minibatch_decode_boxes, top_k_indices),\n                                                        text=tf.shape(top_k_scores)[0])\n            tf.summary.image('/top_5', top_detections_in_img)\n\n            # losses\n            with tf.variable_scope('rpn_location_loss'):\n                location_loss = losses.l1_smooth_losses(predict_boxes=minibatch_encode_boxes,\n                                                        gtboxes=minibatch_encode_gtboxes,\n                                                        object_weights=object_mask)\n                slim.losses.add_loss(location_loss)  # add smooth l1 loss to losses collection\n\n            with tf.variable_scope('rpn_classification_loss'):\n                classification_loss = slim.losses.softmax_cross_entropy(logits=minibatch_boxes_scores,\n                                                                        onehot_labels=minibatch_labels_one_hot)\n\n            return location_loss, classification_loss\n\n    def rpn_proposals(self):\n        with tf.variable_scope('rpn_proposals'):\n            rpn_decode_boxes = encode_and_decode.decode_boxes(encode_boxes=self.rpn_encode_boxes,\n                                                              reference_boxes=self.anchors,\n                                                              scale_factors=self.scale_factors)\n\n            if not self.is_training:  # when test, clip proposals to img boundaries\n                img_shape = tf.shape(self.img_batch)\n                rpn_decode_boxes = boxes_utils.clip_boxes_to_img_boundaries(rpn_decode_boxes, img_shape)\n\n            rpn_softmax_scores = slim.softmax(self.rpn_scores)\n            rpn_object_score = rpn_softmax_scores[:, 1]  # second column represent object\n\n            if self.top_k_nms:\n                rpn_object_score, top_k_indices = tf.nn.top_k(rpn_object_score, k=self.top_k_nms)\n                rpn_decode_boxes = tf.gather(rpn_decode_boxes, top_k_indices)\n\n            valid_indices = nms.non_maximal_suppression(boxes=rpn_decode_boxes,\n                                                        scores=rpn_object_score,\n                                                        max_output_size=self.max_proposals_num,\n                                                        iou_threshold=self.rpn_nms_iou_threshold)\n\n            valid_boxes = tf.gather(rpn_decode_boxes, valid_indices)\n            valid_scores = tf.gather(rpn_object_score, valid_indices)\n            # print_tensors(valid_scores, 'rpn_score')\n            rpn_proposals_boxes, rpn_proposals_scores = tf.cond(\n                tf.less(tf.shape(valid_boxes)[0], self.max_proposals_num),\n                lambda: boxes_utils.padd_boxes_with_zeros(valid_boxes, valid_scores,\n                                                          self.max_proposals_num),\n                lambda: (valid_boxes, valid_scores))\n\n            return rpn_proposals_boxes, rpn_proposals_scores\n"""
libs/networks/nets/__init__.py,0,b''
libs/networks/nets/vggnet16.py,29,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport inspect\nimport os\n\nimport numpy as np\nimport tensorflow as tf\nimport cv2\nimport matplotlib\nmatplotlib.use(\'TkAgg\')\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom libs.configs import cfgs\n\nVGG_MEAN = [103.939, 116.779, 123.68]\n\n\nclass Vgg16:\n    def __init__(self, vgg16_npy_path=cfgs.VGG16_WEIGHT_PATH):\n        if vgg16_npy_path is None:\n            path = inspect.getfile(Vgg16)\n            path = os.path.abspath(os.path.join(path, os.pardir))\n            path = os.path.join(path, ""vgg16_part.npy"")\n            vgg16_npy_path = path\n\n        self.data_dict = np.load(vgg16_npy_path, encoding=\'latin1\').item()\n        print(""vgg.npy file loaded"")\n\n    def conv_op(self, input_op, name, kh, kw, n_out, dh, dw):\n        n_in = input_op.get_shape()[-1].value\n\n        with tf.variable_scope(name):\n            weights = tf.get_variable(name=\'weights\',\n                                      dtype=tf.float32,\n                                      initializer=tf.constant(self.data_dict[name][\'weights\'], dtype=tf.float32))\n            conv = tf.nn.conv2d(input_op, weights, (1, dh, dw, 1), padding=\'SAME\')\n            biases = tf.get_variable(name=\'biases\',\n                                     dtype=tf.float32,\n                                     initializer=tf.constant(self.data_dict[name][\'biases\'], dtype=tf.float32))\n            return tf.nn.relu(tf.nn.bias_add(conv, biases))\n\n    def fc_op(self, input_op, name, n_out):\n        n_in = input_op.get_shape()[-1].value\n\n        with tf.variable_scope(name):\n            weights = tf.get_variable(name=\'weights\',\n                                      dtype=tf.float32,\n                                      initializer=tf.Variable(tf.constant(self.data_dict[name][\'weights\'],\n                                                                          name=""weights"")))\n\n            biases = tf.get_variable(name=\'biases\',\n                                     dtype=tf.float32,\n                                     initializer=tf.Variable(tf.constant(self.data_dict[name][\'biases\'],\n                                                                         name=""biases"")))\n\n            fc = tf.nn.relu_layer(input_op, weights, biases)\n\n            return fc\n\n    def mpool_op(self, input_op, name, kh, kw, dh, dw):\n        return tf.nn.max_pool(input_op,\n                              ksize=[1, kh, kw, 1],\n                              strides=[1, dh, dw, 1],\n                              padding=\'SAME\',\n                              name=name)\n\n    def build(self, rgb, rgb2gbr=False):\n\n        self.color = rgb\n\n        # if use cv2 read image, the channel is gbr, others are rgb\n        if rgb2gbr:\n            # Convert RGB to BGR\n            red, green, blue = tf.split(self.color, num_or_size_splits=3, axis=3)\n            self.color = tf.concat([blue - VGG_MEAN[0],\n                                    green - VGG_MEAN[1],\n                                    red - VGG_MEAN[2]], axis=3)\n            self.conv1_1 = self.conv_op(input_op=self.color, name=""conv1_1"", kh=3, kw=3,\n                                        n_out=64, dh=1, dw=1)\n\n        else:\n\n            blue, green, red = tf.split(self.color, num_or_size_splits=3, axis=3)\n            self.color = tf.concat([blue - VGG_MEAN[0],\n                                    green - VGG_MEAN[1],\n                                    red - VGG_MEAN[2]], axis=3)\n            self.conv1_1 = self.conv_op(input_op=self.color, name=""conv1_1"", kh=3, kw=3,\n                                        n_out=64, dh=1, dw=1)\n\n        self.conv1_2 = self.conv_op(input_op=self.conv1_1, name=""conv1_2"", kh=3, kw=3,\n                                    n_out=64, dh=1, dw=1)\n        self.pool1 = self.mpool_op(input_op=self.conv1_2, name=\'pool1\',\n                                   kh=2, kw=2, dw=2, dh=2)\n\n        self.conv2_1 = self.conv_op(input_op=self.pool1, name=""conv2_1"", kh=3, kw=3,\n                                    n_out=128, dh=1, dw=1)\n        self.conv2_2 = self.conv_op(input_op=self.conv2_1, name=""conv2_2"", kh=3, kw=3,\n                                    n_out=128, dh=1, dw=1)\n        self.pool2 = self.mpool_op(input_op=self.conv2_2, name=\'pool2\',\n                                   kh=2, kw=2, dw=2, dh=2)\n\n        self.conv3_1 = self.conv_op(input_op=self.pool2, name=""conv3_1"", kh=3, kw=3,\n                                    n_out=256, dh=1, dw=1)\n        self.conv3_2 = self.conv_op(input_op=self.conv3_1, name=""conv3_2"", kh=3, kw=3,\n                                    n_out=256, dh=1, dw=1)\n        self.conv3_3 = self.conv_op(input_op=self.conv3_2, name=""conv3_3"", kh=3, kw=3,\n                                    n_out=256, dh=1, dw=1)\n        self.pool3 = self.mpool_op(input_op=self.conv3_3, name=\'pool3\',\n                                   kh=2, kw=2, dw=2, dh=2)\n\n        self.conv4_1 = self.conv_op(input_op=self.pool3, name=""conv4_1"", kh=3, kw=3,\n                                    n_out=512, dh=1, dw=1)\n        self.conv4_2 = self.conv_op(input_op=self.conv4_1, name=""conv4_2"", kh=3, kw=3,\n                                    n_out=512, dh=1, dw=1)\n        self.conv4_3 = self.conv_op(input_op=self.conv4_2, name=""conv4_3"", kh=3, kw=3,\n                                    n_out=512, dh=1, dw=1)\n        self.pool4 = self.mpool_op(input_op=self.conv4_3, name=\'pool4\',\n                                   kh=2, kw=2, dw=2, dh=2)\n\n        self.conv5_1 = self.conv_op(input_op=self.pool4, name=""conv5_1"", kh=3, kw=3,\n                                    n_out=512, dh=1, dw=1)\n        self.conv5_2 = self.conv_op(input_op=self.conv5_1, name=""conv5_2"", kh=3, kw=3,\n                                    n_out=512, dh=1, dw=1)\n        self.conv5_3 = self.conv_op(input_op=self.conv5_2, name=""conv5_3"", kh=3, kw=3,\n                                    n_out=512, dh=1, dw=1)\n        # self.pool5 = self.mpool_op(input_op=self.conv5_3, name=\'pool5\',\n        #                            kh=2, kw=2, dw=2, dh=2)\n        #\n        # shape = self.pool5.get_shape()\n        # flattened_shape = shape[1].value * shape[2].value * shape[3].value\n        #\n        # flatten = tf.reshape(self.pool5, [-1, flattened_shape], name=\'flatten\')\n        #\n        # self.fc6 = self.fc_op(input_op=self.pool5, name=""fc6"", n_out=4096)\n        #\n        # assert self.fc6.get_shape().as_list()[1:] == [4096]\n        #\n        # self.fc6_drop = tf.nn.dropout(self.fc6, 0.5, name=\'fc6_drop\')\n        #\n        # self.fc7 = self.fc_op(input_op=self.fc6_drop, name=""fc7"", n_out=4096)\n\n        # self.fc7_drop = tf.nn.dropout(self.fc7, 0.5, name=\'fc7_drop\')\n        #\n        # self.fc8 = self.fc_op(input_op=self.fc7_drop, name=""fc8"", n_out=1000)\n        # self.prob = tf.nn.softmax(self.fc8, name=""prob"")\n\n\n# if __name__ == \'__main__\':\n#     img_path = cfgs._ROO_PATH + \'/demo/0000fdee4208b8b7e12074c920bc6166-0.jpg\'\n#     img1 = Image.open(img_path)\n#     img1 = np.array(img1)\n#     # plt.imshow(img1)\n#     # plt.show()\n#\n#     # img2 = cv2.imread(img_path)\n#     # plt.imshow(img2)\n#     # plt.show()\n#     images = tf.placeholder(""float32"", [1, 96, 96, 3])\n#\n#     vgg = Vgg16()\n#\n#     vgg.build(images, rgb2gbr=True)\n#\n#     with tf.Session() as sess:\n#         init = tf.global_variables_initializer()\n#         sess.run(init)\n#\n#         img = np.array([img1], dtype=np.float32)\n#         _relu5_3, temp = sess.run([vgg.conv5_3, vgg.color], feed_dict={images: img})\n#\n#         print(\'ooo\')'"
libs/networks/slim_nets/__init__.py,0,b'\n'
libs/networks/slim_nets/alexnet.py,8,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a model definition for AlexNet.\n\nThis work was first described in:\n  ImageNet Classification with Deep Convolutional Neural Networks\n  Alex Krizhevsky, Ilya Sutskever and Geoffrey E. Hinton\n\nand later refined in:\n  One weird trick for parallelizing convolutional neural networks\n  Alex Krizhevsky, 2014\n\nHere we provide the implementation proposed in ""One weird trick"" and not\n""ImageNet Classification"", as per the paper, the LRN layers have been removed.\n\nUsage:\n  with slim.arg_scope(alexnet.alexnet_v2_arg_scope()):\n    outputs, end_points = alexnet.alexnet_v2(inputs)\n\n@@alexnet_v2\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef alexnet_v2_arg_scope(weight_decay=0.0005):\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      biases_initializer=tf.constant_initializer(0.1),\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\'):\n      with slim.arg_scope([slim.max_pool2d], padding=\'VALID\') as arg_sc:\n        return arg_sc\n\n\ndef alexnet_v2(inputs,\n               num_classes=1000,\n               is_training=True,\n               dropout_keep_prob=0.5,\n               spatial_squeeze=True,\n               scope=\'alexnet_v2\'):\n  """"""AlexNet version 2.\n\n  Described in: http://arxiv.org/pdf/1404.5997v2.pdf\n  Parameters from:\n  github.com/akrizhevsky/cuda-convnet2/blob/master/layers/\n  layers-imagenet-1gpu.cfg\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224. To use in fully\n        convolutional mode, set spatial_squeeze to false.\n        The LRN layers have been removed and change the initializers from\n        random_normal_initializer to xavier_initializer.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'alexnet_v2\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=[end_points_collection]):\n      net = slim.conv2d(inputs, 64, [11, 11], 4, padding=\'VALID\',\n                        scope=\'conv1\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool1\')\n      net = slim.conv2d(net, 192, [5, 5], scope=\'conv2\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool2\')\n      net = slim.conv2d(net, 384, [3, 3], scope=\'conv3\')\n      net = slim.conv2d(net, 384, [3, 3], scope=\'conv4\')\n      net = slim.conv2d(net, 256, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool5\')\n\n      # Use conv2d instead of fully_connected layers.\n      with slim.arg_scope([slim.conv2d],\n                          weights_initializer=trunc_normal(0.005),\n                          biases_initializer=tf.constant_initializer(0.1)):\n        net = slim.conv2d(net, 4096, [5, 5], padding=\'VALID\',\n                          scope=\'fc6\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout6\')\n        net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout7\')\n        net = slim.conv2d(net, num_classes, [1, 1],\n                          activation_fn=None,\n                          normalizer_fn=None,\n                          biases_initializer=tf.zeros_initializer(),\n                          scope=\'fc8\')\n\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nalexnet_v2.default_image_size = 224\n'"
libs/networks/slim_nets/alexnet_test.py,16,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.slim_nets.alexnet.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import alexnet\n\nslim = tf.contrib.slim\n\n\nclass AlexnetV2Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'alexnet_v2/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 300, 400\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'alexnet_v2/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 4, 7, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = alexnet.alexnet_v2(inputs, num_classes)\n      expected_names = [\'alexnet_v2/conv1\',\n                        \'alexnet_v2/pool1\',\n                        \'alexnet_v2/conv2\',\n                        \'alexnet_v2/pool2\',\n                        \'alexnet_v2/conv3\',\n                        \'alexnet_v2/conv4\',\n                        \'alexnet_v2/conv5\',\n                        \'alexnet_v2/pool5\',\n                        \'alexnet_v2/fc6\',\n                        \'alexnet_v2/fc7\',\n                        \'alexnet_v2/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      alexnet.alexnet_v2(inputs, num_classes)\n      expected_names = [\'alexnet_v2/conv1/weights\',\n                        \'alexnet_v2/conv1/biases\',\n                        \'alexnet_v2/conv2/weights\',\n                        \'alexnet_v2/conv2/biases\',\n                        \'alexnet_v2/conv3/weights\',\n                        \'alexnet_v2/conv3/biases\',\n                        \'alexnet_v2/conv4/weights\',\n                        \'alexnet_v2/conv4/biases\',\n                        \'alexnet_v2/conv5/weights\',\n                        \'alexnet_v2/conv5/biases\',\n                        \'alexnet_v2/fc6/weights\',\n                        \'alexnet_v2/fc6/biases\',\n                        \'alexnet_v2/fc7/weights\',\n                        \'alexnet_v2/fc7/biases\',\n                        \'alexnet_v2/fc8/weights\',\n                        \'alexnet_v2/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 300, 400\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = alexnet.alexnet_v2(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = alexnet.alexnet_v2(eval_inputs, is_training=False,\n                                     spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 4, 7, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/cifarnet.py,12,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a variant of the CIFAR-10 model definition.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(stddev=stddev)\n\n\ndef cifarnet(images, num_classes=10, is_training=False,\n             dropout_keep_prob=0.5,\n             prediction_fn=slim.softmax,\n             scope=\'CifarNet\'):\n  """"""Creates a variant of the CifarNet model.\n\n  Note that since the output is a set of \'logits\', the values fall in the\n  interval of (-infinity, infinity). Consequently, to convert the outputs to a\n  probability distribution over the characters, one will need to convert them\n  using the softmax function:\n\n        logits = cifarnet.cifarnet(images, is_training=False)\n        probabilities = tf.nn.softmax(logits)\n        predictions = tf.argmax(logits, 1)\n\n  Args:\n    images: A batch of `Tensors` of size [batch_size, height, width, channels].\n    num_classes: the number of classes in the dataset.\n    is_training: specifies whether or not we\'re currently training the model.\n      This variable will determine the behaviour of the dropout layer.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, `num_classes`]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'CifarNet\', [images, num_classes]):\n    net = slim.conv2d(images, 64, [5, 5], scope=\'conv1\')\n    end_points[\'conv1\'] = net\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool1\')\n    end_points[\'pool1\'] = net\n    net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001/9.0, beta=0.75, name=\'norm1\')\n    net = slim.conv2d(net, 64, [5, 5], scope=\'conv2\')\n    end_points[\'conv2\'] = net\n    net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001/9.0, beta=0.75, name=\'norm2\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool2\')\n    end_points[\'pool2\'] = net\n    net = slim.flatten(net)\n    end_points[\'Flatten\'] = net\n    net = slim.fully_connected(net, 384, scope=\'fc3\')\n    end_points[\'fc3\'] = net\n    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                       scope=\'dropout3\')\n    net = slim.fully_connected(net, 192, scope=\'fc4\')\n    end_points[\'fc4\'] = net\n    logits = slim.fully_connected(net, num_classes,\n                                  biases_initializer=tf.zeros_initializer(),\n                                  weights_initializer=trunc_normal(1/192.0),\n                                  weights_regularizer=None,\n                                  activation_fn=None,\n                                  scope=\'logits\')\n\n    end_points[\'Logits\'] = logits\n    end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n\n  return logits, end_points\ncifarnet.default_image_size = 32\n\n\ndef cifarnet_arg_scope(weight_decay=0.004):\n  """"""Defines the default cifarnet argument scope.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n\n  Returns:\n    An `arg_scope` to use for the inception v3 model.\n  """"""\n  with slim.arg_scope(\n      [slim.conv2d],\n      weights_initializer=tf.truncated_normal_initializer(stddev=5e-2),\n      activation_fn=tf.nn.relu):\n    with slim.arg_scope(\n        [slim.fully_connected],\n        biases_initializer=tf.constant_initializer(0.1),\n        weights_initializer=trunc_normal(0.04),\n        weights_regularizer=slim.l2_regularizer(weight_decay),\n        activation_fn=tf.nn.relu) as sc:\n      return sc\n'"
libs/networks/slim_nets/inception.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Brings all inception models under one namespace.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# pylint: disable=unused-import\nfrom nets.inception_resnet_v2 import inception_resnet_v2\nfrom nets.inception_resnet_v2 import inception_resnet_v2_arg_scope\nfrom nets.inception_resnet_v2 import inception_resnet_v2_base\nfrom nets.inception_v1 import inception_v1\nfrom nets.inception_v1 import inception_v1_arg_scope\nfrom nets.inception_v1 import inception_v1_base\nfrom nets.inception_v2 import inception_v2\nfrom nets.inception_v2 import inception_v2_arg_scope\nfrom nets.inception_v2 import inception_v2_base\nfrom nets.inception_v3 import inception_v3\nfrom nets.inception_v3 import inception_v3_arg_scope\nfrom nets.inception_v3 import inception_v3_base\nfrom nets.inception_v4 import inception_v4\nfrom nets.inception_v4 import inception_v4_arg_scope\nfrom nets.inception_v4 import inception_v4_base\n# pylint: enable=unused-import\n'"
libs/networks/slim_nets/inception_resnet_v2.py,41,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition of the Inception Resnet V2 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 35x35 resnet block.""""""\n  with tf.variable_scope(scope, \'Block35\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 32, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n    with tf.variable_scope(\'Branch_2\'):\n      tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope=\'Conv2d_0b_3x3\')\n      tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope=\'Conv2d_0c_3x3\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_1, tower_conv2_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 17x17 resnet block.""""""\n  with tf.variable_scope(scope, \'Block17\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],\n                                  scope=\'Conv2d_0b_1x7\')\n      tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1],\n                                  scope=\'Conv2d_0c_7x1\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 8x8 resnet block.""""""\n  with tf.variable_scope(scope, \'Block8\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\n                                  scope=\'Conv2d_0b_1x3\')\n      tower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1],\n                                  scope=\'Conv2d_0c_3x1\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef inception_resnet_v2_base(inputs,\n                             final_endpoint=\'Conv2d_7b_1x1\',\n                             output_stride=16,\n                             align_feature_maps=False,\n                             scope=None):\n  """"""Inception model from  http://arxiv.org/abs/1602.07261.\n\n  Constructs an Inception Resnet v2 network from inputs to the given final\n  endpoint. This method can construct the network up to the final inception\n  block Conv2d_7b_1x1.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\', \'MaxPool_5a_3x3\',\n      \'Mixed_5b\', \'Mixed_6a\', \'PreAuxLogits\', \'Mixed_7a\', \'Conv2d_7b_1x1\']\n    output_stride: A scalar that specifies the requested ratio of input to\n      output spatial resolution. Only supports 8 and 16.\n    align_feature_maps: When true, changes all the VALID paddings in the network\n      to SAME padding so that the feature maps are aligned.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n      or if the output_stride is not 8 or 16, or if the output_stride is 8 and\n      we request an end point after \'PreAuxLogits\'.\n  """"""\n  if output_stride != 8 and output_stride != 16:\n    raise ValueError(\'output_stride must be 8 or 16.\')\n\n  padding = \'SAME\' if align_feature_maps else \'VALID\'\n\n  end_points = {}\n\n  def add_and_check_final(name, net):\n    end_points[name] = net\n    return name == final_endpoint\n\n  with tf.variable_scope(scope, \'InceptionResnetV2\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'SAME\'):\n      # 149 x 149 x 32\n      net = slim.conv2d(inputs, 32, 3, stride=2, padding=padding,\n                        scope=\'Conv2d_1a_3x3\')\n      if add_and_check_final(\'Conv2d_1a_3x3\', net): return net, end_points\n\n      # 147 x 147 x 32\n      net = slim.conv2d(net, 32, 3, padding=padding,\n                        scope=\'Conv2d_2a_3x3\')\n      if add_and_check_final(\'Conv2d_2a_3x3\', net): return net, end_points\n      # 147 x 147 x 64\n      net = slim.conv2d(net, 64, 3, scope=\'Conv2d_2b_3x3\')\n      if add_and_check_final(\'Conv2d_2b_3x3\', net): return net, end_points\n      # 73 x 73 x 64\n      net = slim.max_pool2d(net, 3, stride=2, padding=padding,\n                            scope=\'MaxPool_3a_3x3\')\n      if add_and_check_final(\'MaxPool_3a_3x3\', net): return net, end_points\n      # 73 x 73 x 80\n      net = slim.conv2d(net, 80, 1, padding=padding,\n                        scope=\'Conv2d_3b_1x1\')\n      if add_and_check_final(\'Conv2d_3b_1x1\', net): return net, end_points\n      # 71 x 71 x 192\n      net = slim.conv2d(net, 192, 3, padding=padding,\n                        scope=\'Conv2d_4a_3x3\')\n      if add_and_check_final(\'Conv2d_4a_3x3\', net): return net, end_points\n      # 35 x 35 x 192\n      net = slim.max_pool2d(net, 3, stride=2, padding=padding,\n                            scope=\'MaxPool_5a_3x3\')\n      if add_and_check_final(\'MaxPool_5a_3x3\', net): return net, end_points\n\n      # 35 x 35 x 320\n      with tf.variable_scope(\'Mixed_5b\'):\n        with tf.variable_scope(\'Branch_0\'):\n          tower_conv = slim.conv2d(net, 96, 1, scope=\'Conv2d_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          tower_conv1_0 = slim.conv2d(net, 48, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,\n                                      scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          tower_conv2_0 = slim.conv2d(net, 64, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,\n                                      scope=\'Conv2d_0b_3x3\')\n          tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,\n                                      scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          tower_pool = slim.avg_pool2d(net, 3, stride=1, padding=\'SAME\',\n                                       scope=\'AvgPool_0a_3x3\')\n          tower_pool_1 = slim.conv2d(tower_pool, 64, 1,\n                                     scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(\n            [tower_conv, tower_conv1_1, tower_conv2_2, tower_pool_1], 3)\n\n      if add_and_check_final(\'Mixed_5b\', net): return net, end_points\n      # TODO(alemi): Register intermediate endpoints\n      net = slim.repeat(net, 10, block35, scale=0.17)\n\n      # 17 x 17 x 1088 if output_stride == 8,\n      # 33 x 33 x 1088 if output_stride == 16\n      use_atrous = output_stride == 8\n\n      with tf.variable_scope(\'Mixed_6a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          tower_conv = slim.conv2d(net, 384, 3, stride=1 if use_atrous else 2,\n                                   padding=padding,\n                                   scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          tower_conv1_0 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,\n                                      scope=\'Conv2d_0b_3x3\')\n          tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3,\n                                      stride=1 if use_atrous else 2,\n                                      padding=padding,\n                                      scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          tower_pool = slim.max_pool2d(net, 3, stride=1 if use_atrous else 2,\n                                       padding=padding,\n                                       scope=\'MaxPool_1a_3x3\')\n        net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\n\n      if add_and_check_final(\'Mixed_6a\', net): return net, end_points\n\n      # TODO(alemi): register intermediate endpoints\n      with slim.arg_scope([slim.conv2d], rate=2 if use_atrous else 1):\n        net = slim.repeat(net, 20, block17, scale=0.10)\n      if add_and_check_final(\'PreAuxLogits\', net): return net, end_points\n\n      if output_stride == 8:\n        # TODO(gpapan): Properly support output_stride for the rest of the net.\n        raise ValueError(\'output_stride==8 is only supported up to the \'\n                         \'PreAuxlogits end_point for now.\')\n\n      # 8 x 8 x 2080\n      with tf.variable_scope(\'Mixed_7a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          tower_conv = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n                                     padding=padding,\n                                     scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          tower_conv1 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,\n                                      padding=padding,\n                                      scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          tower_conv2 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\n                                      scope=\'Conv2d_0b_3x3\')\n          tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,\n                                      padding=padding,\n                                      scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          tower_pool = slim.max_pool2d(net, 3, stride=2,\n                                       padding=padding,\n                                       scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(\n            [tower_conv_1, tower_conv1_1, tower_conv2_2, tower_pool], 3)\n\n      if add_and_check_final(\'Mixed_7a\', net): return net, end_points\n\n      # TODO(alemi): register intermediate endpoints\n      net = slim.repeat(net, 9, block8, scale=0.20)\n      net = block8(net, activation_fn=None)\n\n      # 8 x 8 x 1536\n      net = slim.conv2d(net, 1536, 1, scope=\'Conv2d_7b_1x1\')\n      if add_and_check_final(\'Conv2d_7b_1x1\', net): return net, end_points\n\n    raise ValueError(\'final_endpoint (%s) not recognized\', final_endpoint)\n\n\ndef inception_resnet_v2(inputs, num_classes=1001, is_training=True,\n                        dropout_keep_prob=0.8,#0.8\n                        reuse=None,\n                        scope=\'InceptionResnetV2\',\n                        create_aux_logits=True):\n  """"""Creates the Inception Resnet V2 model.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: float, the fraction to keep before final layer.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    create_aux_logits: Whether to include the auxilliary logits.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'InceptionResnetV2\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n\n      net, end_points = inception_resnet_v2_base(inputs, scope=scope)\n\n      if create_aux_logits:\n        with tf.variable_scope(\'AuxLogits\'):\n          aux = end_points[\'PreAuxLogits\']\n          aux = slim.avg_pool2d(aux, 5, stride=3, padding=\'VALID\',\n                                scope=\'Conv2d_1a_3x3\')\n          aux = slim.conv2d(aux, 128, 1, scope=\'Conv2d_1b_1x1\')\n          aux = slim.conv2d(aux, 768, aux.get_shape()[1:3],\n                            padding=\'VALID\', scope=\'Conv2d_2a_5x5\')\n          aux = slim.flatten(aux)\n          aux = slim.fully_connected(aux, num_classes, activation_fn=None,\n                                     scope=\'Logits\')\n          end_points[\'AuxLogits\'] = aux\n\n      with tf.variable_scope(\'Logits\'):\n        net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n                              scope=\'AvgPool_1a_8x8\')\n        net = slim.flatten(net)\n\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'Dropout\')\n\n        end_points[\'PreLogitsFlatten\'] = net\n        # end_points[\'yjr_feature\'] = tf.squeeze(net, axis=0)\n\n        logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                      scope=\'Logits\')\n        end_points[\'Logits\'] = logits\n        end_points[\'Predictions\'] = tf.nn.softmax(logits, name=\'Predictions\')\n\n    return logits, end_points\ninception_resnet_v2.default_image_size = 299\n\n\ndef inception_resnet_v2_arg_scope(weight_decay=0.00004,\n                                  batch_norm_decay=0.9997,\n                                  batch_norm_epsilon=0.001):\n  """"""Yields the scope with the default parameters for inception_resnet_v2.\n\n  Args:\n    weight_decay: the weight decay for weights variables.\n    batch_norm_decay: decay for the moving average of batch_norm momentums.\n    batch_norm_epsilon: small float added to variance to avoid dividing by zero.\n\n  Returns:\n    a arg_scope with the parameters needed for inception_resnet_v2.\n  """"""\n  # Set weight_decay for weights in conv2d and fully_connected layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_regularizer=slim.l2_regularizer(weight_decay)):\n\n    batch_norm_params = {\n        \'decay\': batch_norm_decay,\n        \'epsilon\': batch_norm_epsilon,\n    }\n    # Set activation_fn and parameters for batch_norm.\n    with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu,\n                        normalizer_fn=slim.batch_norm,\n                        normalizer_params=batch_norm_params) as scope:\n      return scope\n'"
libs/networks/slim_nets/inception_resnet_v2_test.py,27,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.inception_resnet_v2.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception\n\n\nclass InceptionTest(tf.test.TestCase):\n\n  def testBuildLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, endpoints = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(\'AuxLogits\' in endpoints)\n      auxlogits = endpoints[\'AuxLogits\']\n      self.assertTrue(\n          auxlogits.op.name.startswith(\'InceptionResnetV2/AuxLogits\'))\n      self.assertListEqual(auxlogits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testBuildWithoutAuxLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, endpoints = inception.inception_resnet_v2(inputs, num_classes,\n                                                        create_aux_logits=False)\n      self.assertTrue(\'AuxLogits\' not in endpoints)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testBuildEndPoints(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(\'Logits\' in end_points)\n      logits = end_points[\'Logits\']\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      self.assertTrue(\'AuxLogits\' in end_points)\n      aux_logits = end_points[\'AuxLogits\']\n      self.assertListEqual(aux_logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Conv2d_7b_1x1\']\n      self.assertListEqual(pre_pool.get_shape().as_list(),\n                           [batch_size, 8, 8, 1536])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    net, end_points = inception.inception_resnet_v2_base(inputs)\n    self.assertTrue(net.op.name.startswith(\'InceptionResnetV2/Conv2d_7b_1x1\'))\n    self.assertListEqual(net.get_shape().as_list(),\n                         [batch_size, 8, 8, 1536])\n    expected_endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                          \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                          \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_6a\',\n                          \'PreAuxLogits\', \'Mixed_7a\', \'Conv2d_7b_1x1\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 299, 299\n    endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                 \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                 \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_6a\',\n                 \'PreAuxLogits\', \'Mixed_7a\', \'Conv2d_7b_1x1\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_resnet_v2_base(\n            inputs, final_endpoint=endpoint)\n        if endpoint != \'PreAuxLogits\':\n          self.assertTrue(out_tensor.op.name.startswith(\n              \'InceptionResnetV2/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoPreAuxLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_resnet_v2_base(\n        inputs, final_endpoint=\'PreAuxLogits\')\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [5, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [5, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [5, 147, 147, 64],\n                        \'MaxPool_3a_3x3\': [5, 73, 73, 64],\n                        \'Conv2d_3b_1x1\': [5, 73, 73, 80],\n                        \'Conv2d_4a_3x3\': [5, 71, 71, 192],\n                        \'MaxPool_5a_3x3\': [5, 35, 35, 192],\n                        \'Mixed_5b\': [5, 35, 35, 320],\n                        \'Mixed_6a\': [5, 17, 17, 1088],\n                        \'PreAuxLogits\': [5, 17, 17, 1088]\n                       }\n\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testBuildAndCheckAllEndPointsUptoPreAuxLogitsWithAlignedFeatureMaps(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_resnet_v2_base(\n        inputs, final_endpoint=\'PreAuxLogits\', align_feature_maps=True)\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [5, 150, 150, 32],\n                        \'Conv2d_2a_3x3\': [5, 150, 150, 32],\n                        \'Conv2d_2b_3x3\': [5, 150, 150, 64],\n                        \'MaxPool_3a_3x3\': [5, 75, 75, 64],\n                        \'Conv2d_3b_1x1\': [5, 75, 75, 80],\n                        \'Conv2d_4a_3x3\': [5, 75, 75, 192],\n                        \'MaxPool_5a_3x3\': [5, 38, 38, 192],\n                        \'Mixed_5b\': [5, 38, 38, 320],\n                        \'Mixed_6a\': [5, 19, 19, 1088],\n                        \'PreAuxLogits\': [5, 19, 19, 1088]\n                       }\n\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testBuildAndCheckAllEndPointsUptoPreAuxLogitsWithOutputStrideEight(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_resnet_v2_base(\n        inputs, final_endpoint=\'PreAuxLogits\', output_stride=8)\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [5, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [5, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [5, 147, 147, 64],\n                        \'MaxPool_3a_3x3\': [5, 73, 73, 64],\n                        \'Conv2d_3b_1x1\': [5, 73, 73, 80],\n                        \'Conv2d_4a_3x3\': [5, 71, 71, 192],\n                        \'MaxPool_5a_3x3\': [5, 35, 35, 192],\n                        \'Mixed_5b\': [5, 35, 35, 320],\n                        \'Mixed_6a\': [5, 33, 33, 1088],\n                        \'PreAuxLogits\': [5, 33, 33, 1088]\n                       }\n\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testVariablesSetDevice(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      # Force all Variables to reside on the device.\n      with tf.variable_scope(\'on_cpu\'), tf.device(\'/cpu:0\'):\n        inception.inception_resnet_v2(inputs, num_classes)\n      with tf.variable_scope(\'on_gpu\'), tf.device(\'/gpu:0\'):\n        inception.inception_resnet_v2(inputs, num_classes)\n      for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_cpu\'):\n        self.assertDeviceEqual(v.device, \'/cpu:0\')\n      for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_gpu\'):\n        self.assertDeviceEqual(v.device, \'/gpu:0\')\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, end_points = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Conv2d_7b_1x1\']\n      self.assertListEqual(pre_pool.get_shape().as_list(),\n                           [batch_size, 3, 3, 1536])\n\n  def testUnknownBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [None, num_classes])\n      images = tf.random_uniform((batch_size, height, width, 3))\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(eval_inputs,\n                                                num_classes,\n                                                is_training=False)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session() as sess:\n      train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n      inception.inception_resnet_v2(train_inputs, num_classes)\n      eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(eval_inputs,\n                                                num_classes,\n                                                is_training=False,\n                                                reuse=True)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/inception_utils.py,3,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains common code shared by all inception models.\n\nUsage of arg scope:\n  with slim.arg_scope(inception_arg_scope()):\n    logits, end_points = inception.inception_v3(images, num_classes,\n                                                is_training=is_training)\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef inception_arg_scope(weight_decay=0.00004,\n                        use_batch_norm=True,\n                        batch_norm_decay=0.9997,\n                        batch_norm_epsilon=0.001):\n  """"""Defines the default arg scope for inception models.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    use_batch_norm: ""If `True`, batch_norm is applied after each convolution.\n    batch_norm_decay: Decay for batch norm moving average.\n    batch_norm_epsilon: Small float added to variance to avoid dividing by zero\n      in batch norm.\n\n  Returns:\n    An `arg_scope` to use for the inception models.\n  """"""\n  batch_norm_params = {\n      # Decay for the moving averages.\n      \'decay\': batch_norm_decay,\n      # epsilon to prevent 0s in variance.\n      \'epsilon\': batch_norm_epsilon,\n      # collection containing update_ops.\n      \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n  }\n  if use_batch_norm:\n    normalizer_fn = slim.batch_norm\n    normalizer_params = batch_norm_params\n  else:\n    normalizer_fn = None\n    normalizer_params = {}\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope(\n        [slim.conv2d],\n        weights_initializer=slim.variance_scaling_initializer(),\n        activation_fn=tf.nn.relu,\n        normalizer_fn=normalizer_fn,\n        normalizer_params=normalizer_params) as sc:\n      return sc\n'"
libs/networks/slim_nets/inception_v1.py,60,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v1 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v1_base(inputs,\n                      final_endpoint=\'Mixed_5c\',\n                      scope=\'InceptionV1\'):\n  """"""Defines the Inception V1 base architecture.\n\n  This architecture is defined in:\n    Going deeper with convolutions\n    Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n    http://arxiv.org/pdf/1409.4842v1.pdf.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n      \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n      \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\',\n      \'Mixed_4f\', \'MaxPool_5a_2x2\', \'Mixed_5b\', \'Mixed_5c\']\n    scope: Optional variable_scope.\n\n  Returns:\n    A dictionary from components of the network to the corresponding activation.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values.\n  """"""\n  end_points = {}\n  with tf.variable_scope(scope, \'InceptionV1\', [inputs]):\n    with slim.arg_scope(\n        [slim.conv2d, slim.fully_connected],\n        weights_initializer=trunc_normal(0.01)):\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                          stride=1, padding=\'SAME\'):\n        end_point = \'Conv2d_1a_7x7\'\n        net = slim.conv2d(inputs, 64, [7, 7], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'MaxPool_2a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'Conv2d_2b_1x1\'\n        net = slim.conv2d(net, 64, [1, 1], scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'Conv2d_2c_3x3\'\n        net = slim.conv2d(net, 192, [3, 3], scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'MaxPool_3a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_3b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 16, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 32, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 32, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_3c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 192, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'MaxPool_4a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 208, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 16, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 48, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 112, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 24, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4d\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 256, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 24, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4e\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 112, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 144, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 288, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4f\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'MaxPool_5a_2x2\'\n        net = slim.max_pool2d(net, [2, 2], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_5b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0a_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_5c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 384, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 48, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v1(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV1\'):\n  """"""Defines the Inception V1 architecture.\n\n  This architecture is defined in:\n\n    Going deeper with convolutions\n    Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n    http://arxiv.org/pdf/1409.4842v1.pdf.\n\n  The default image size used to train this network is 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  # Final pooling and prediction\n  with tf.variable_scope(scope, \'InceptionV1\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v1_base(inputs, scope=scope)\n      with tf.variable_scope(\'Logits\'):\n        net = slim.avg_pool2d(net, [7, 7], stride=1, scope=\'AvgPool_0a_7x7\')\n        net = slim.dropout(net,\n                           dropout_keep_prob, scope=\'Dropout_0b\')\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_0c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n\n        end_points[\'Logits\'] = logits\n        end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v1.default_image_size = 224\n\ninception_v1_arg_scope = inception_utils.inception_arg_scope\n'"
libs/networks/slim_nets/inception_v1_test.py,25,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim_nets.inception_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV1Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_6c, end_points = inception.inception_v1_base(inputs)\n    self.assertTrue(mixed_6c.op.name.startswith(\'InceptionV1/Mixed_5c\'))\n    self.assertListEqual(mixed_6c.get_shape().as_list(),\n                         [batch_size, 7, 7, 1024])\n    expected_endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                          \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\',\n                          \'Mixed_3c\', \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\',\n                          \'Mixed_4d\', \'Mixed_4e\', \'Mixed_4f\', \'MaxPool_5a_2x2\',\n                          \'Mixed_5b\', \'Mixed_5c\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 224, 224\n    endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                 \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n                 \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\',\n                 \'Mixed_4e\', \'Mixed_4f\', \'MaxPool_5a_2x2\', \'Mixed_5b\',\n                 \'Mixed_5c\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v1_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV1/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed5c(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v1_base(inputs,\n                                                final_endpoint=\'Mixed_5c\')\n    endpoints_shapes = {\'Conv2d_1a_7x7\': [5, 112, 112, 64],\n                        \'MaxPool_2a_3x3\': [5, 56, 56, 64],\n                        \'Conv2d_2b_1x1\': [5, 56, 56, 64],\n                        \'Conv2d_2c_3x3\': [5, 56, 56, 192],\n                        \'MaxPool_3a_3x3\': [5, 28, 28, 192],\n                        \'Mixed_3b\': [5, 28, 28, 256],\n                        \'Mixed_3c\': [5, 28, 28, 480],\n                        \'MaxPool_4a_3x3\': [5, 14, 14, 480],\n                        \'Mixed_4b\': [5, 14, 14, 512],\n                        \'Mixed_4c\': [5, 14, 14, 512],\n                        \'Mixed_4d\': [5, 14, 14, 512],\n                        \'Mixed_4e\': [5, 14, 14, 528],\n                        \'Mixed_4f\': [5, 14, 14, 832],\n                        \'MaxPool_5a_2x2\': [5, 7, 7, 832],\n                        \'Mixed_5b\': [5, 7, 7, 832],\n                        \'Mixed_5c\': [5, 7, 7, 1024]}\n\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 224, 224\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v1_arg_scope()):\n      inception.inception_v1_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(5607184, total_params)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 112, 112\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_5c, _ = inception.inception_v1_base(inputs)\n    self.assertTrue(mixed_5c.op.name.startswith(\'InceptionV1/Mixed_5c\'))\n    self.assertListEqual(mixed_5c.get_shape().as_list(),\n                         [batch_size, 4, 4, 1024])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v1(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_5c\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v1(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v1(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v1(eval_inputs, num_classes, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 224, 224, 3])\n    logits, _ = inception.inception_v1(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/inception_v2.py,68,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v2 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v2_base(inputs,\n                      final_endpoint=\'Mixed_5c\',\n                      min_depth=16,\n                      depth_multiplier=1.0,\n                      scope=None):\n  """"""Inception v2 (6a2).\n\n  Constructs an Inception v2 network from inputs to the given final endpoint.\n  This method can construct the network up to the layer inception(5b) as\n  described in http://arxiv.org/abs/1502.03167.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n      \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\', \'Mixed_4a\',\n      \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\', \'Mixed_5a\', \'Mixed_5b\',\n      \'Mixed_5c\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n\n  # end_points will collect relevant activations for external use, for example\n  # summaries or losses.\n  end_points = {}\n\n  # Used to find thinned depths for each layer.\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV2\', [inputs]):\n    with slim.arg_scope(\n        [slim.conv2d, slim.max_pool2d, slim.avg_pool2d, slim.separable_conv2d],\n        stride=1, padding=\'SAME\'):\n\n      # Note that sizes in the comments below assume an input spatial size of\n      # 224x224, however, the inputs can be of any size greater 32x32.\n\n      # 224 x 224 x 3\n      end_point = \'Conv2d_1a_7x7\'\n      # depthwise_multiplier here is different from depth_multiplier.\n      # depthwise_multiplier determines the output channels of the initial\n      # depthwise conv (see docs for tf.nn.separable_conv2d), while\n      # depth_multiplier controls the # channels of the subsequent 1x1\n      # convolution. Must have\n      #   in_channels * depthwise_multipler <= out_channels\n      # so that the separable convolution is not overparameterized.\n      depthwise_multiplier = min(int(depth(64) / 3), 8)\n      net = slim.separable_conv2d(\n          inputs, depth(64), [7, 7], depth_multiplier=depthwise_multiplier,\n          stride=2, weights_initializer=trunc_normal(1.0),\n          scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 112 x 112 x 64\n      end_point = \'MaxPool_2a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], scope=end_point, stride=2)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 64\n      end_point = \'Conv2d_2b_1x1\'\n      net = slim.conv2d(net, depth(64), [1, 1], scope=end_point,\n                        weights_initializer=trunc_normal(0.1))\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 64\n      end_point = \'Conv2d_2c_3x3\'\n      net = slim.conv2d(net, depth(192), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 192\n      end_point = \'MaxPool_3a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], scope=end_point, stride=2)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 192\n      # Inception module.\n      end_point = \'Mixed_3b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(32), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 256\n      end_point = \'Mixed_3c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 320\n      end_point = \'Mixed_4a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(160), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], stride=2, scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(\n              net, [3, 3], stride=2, scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(224), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n\n      # 14 x 14 x 576\n      end_point = \'Mixed_4e\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(96), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(160), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_5a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(192), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(256), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(branch_1, depth(256), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2,\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 7 x 7 x 1024\n      end_point = \'Mixed_5b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(352), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(320), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(160), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n\n      # 7 x 7 x 1024\n      end_point = \'Mixed_5c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(352), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(320), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v2(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 min_depth=16,\n                 depth_multiplier=1.0,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV2\'):\n  """"""Inception v2 model for classification.\n\n  Constructs an Inception v2 network for classification as described in\n  http://arxiv.org/abs/1502.03167.\n\n  The default image size used to train this network is 224x224.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n\n  # Final pooling and prediction\n  with tf.variable_scope(scope, \'InceptionV2\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v2_base(\n          inputs, scope=scope, min_depth=min_depth,\n          depth_multiplier=depth_multiplier)\n      with tf.variable_scope(\'Logits\'):\n        kernel_size = _reduced_kernel_size_for_small_input(net, [7, 7])\n        net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                              scope=\'AvgPool_1a_{}x{}\'.format(*kernel_size))\n        # 1 x 1 x 1024\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n      end_points[\'Logits\'] = logits\n      end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v2.default_image_size = 224\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are is large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n\n  TODO(jrru): Make this function work with unknown shapes. Theoretically, this\n  can be done with the code below. Problems are two-fold: (1) If the shape was\n  known, it will be lost. (2) inception.slim.ops._two_element_tuple cannot\n  handle tensors that define the kernel size.\n      shape = tf.shape(input_tensor)\n      return = tf.pack([tf.minimum(shape[1], kernel_size[0]),\n                        tf.minimum(shape[2], kernel_size[1])])\n\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ninception_v2_arg_scope = inception_utils.inception_arg_scope\n'"
libs/networks/slim_nets/inception_v2_test.py,28,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim_nets.inception_v2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV2Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_5c, end_points = inception.inception_v2_base(inputs)\n    self.assertTrue(mixed_5c.op.name.startswith(\'InceptionV2/Mixed_5c\'))\n    self.assertListEqual(mixed_5c.get_shape().as_list(),\n                         [batch_size, 7, 7, 1024])\n    expected_endpoints = [\'Mixed_3b\', \'Mixed_3c\', \'Mixed_4a\', \'Mixed_4b\',\n                          \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\', \'Mixed_5a\',\n                          \'Mixed_5b\', \'Mixed_5c\', \'Conv2d_1a_7x7\',\n                          \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\', \'Conv2d_2c_3x3\',\n                          \'MaxPool_3a_3x3\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 224, 224\n    endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                 \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n                 \'Mixed_4a\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\',\n                 \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v2_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV2/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed5c(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2_base(inputs,\n                                                final_endpoint=\'Mixed_5c\')\n    endpoints_shapes = {\'Mixed_3b\': [batch_size, 28, 28, 256],\n                        \'Mixed_3c\': [batch_size, 28, 28, 320],\n                        \'Mixed_4a\': [batch_size, 14, 14, 576],\n                        \'Mixed_4b\': [batch_size, 14, 14, 576],\n                        \'Mixed_4c\': [batch_size, 14, 14, 576],\n                        \'Mixed_4d\': [batch_size, 14, 14, 576],\n                        \'Mixed_4e\': [batch_size, 14, 14, 576],\n                        \'Mixed_5a\': [batch_size, 7, 7, 1024],\n                        \'Mixed_5b\': [batch_size, 7, 7, 1024],\n                        \'Mixed_5c\': [batch_size, 7, 7, 1024],\n                        \'Conv2d_1a_7x7\': [batch_size, 112, 112, 64],\n                        \'MaxPool_2a_3x3\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2b_1x1\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2c_3x3\': [batch_size, 56, 56, 192],\n                        \'MaxPool_3a_3x3\': [batch_size, 28, 28, 192]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 224, 224\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v2_arg_scope()):\n      inception.inception_v2_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(10173112, total_params)\n\n  def testBuildEndPointsWithDepthMultiplierLessThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v2(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=0.5)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(0.5 * original_depth, new_depth)\n\n  def testBuildEndPointsWithDepthMultiplierGreaterThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v2(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=2.0)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(2.0 * original_depth, new_depth)\n\n  def testRaiseValueErrorWithInvalidDepthMultiplier(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v2(inputs, num_classes, depth_multiplier=-0.1)\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v2(inputs, num_classes, depth_multiplier=0.0)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 112, 112\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_5c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 4, 4, 1024])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_5c\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v2(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v2(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v2(eval_inputs, num_classes, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 224, 224, 3])\n    logits, _ = inception.inception_v2(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/inception_v3.py,79,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v3 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v3_base(inputs,\n                      final_endpoint=\'Mixed_7c\',\n                      min_depth=16,\n                      depth_multiplier=1.0,\n                      scope=None):\n  """"""Inception model from http://arxiv.org/abs/1512.00567.\n\n  Constructs an Inception v3 network from inputs to the given final endpoint.\n  This method can construct the network up to the final inception block\n  Mixed_7c.\n\n  Note that the names of the layers in the paper do not correspond to the names\n  of the endpoints registered by this function although they build the same\n  network.\n\n  Here is a mapping from the old_names to the new names:\n  Old name          | New name\n  =======================================\n  conv0             | Conv2d_1a_3x3\n  conv1             | Conv2d_2a_3x3\n  conv2             | Conv2d_2b_3x3\n  pool1             | MaxPool_3a_3x3\n  conv3             | Conv2d_3b_1x1\n  conv4             | Conv2d_4a_3x3\n  pool2             | MaxPool_5a_3x3\n  mixed_35x35x256a  | Mixed_5b\n  mixed_35x35x288a  | Mixed_5c\n  mixed_35x35x288b  | Mixed_5d\n  mixed_17x17x768a  | Mixed_6a\n  mixed_17x17x768b  | Mixed_6b\n  mixed_17x17x768c  | Mixed_6c\n  mixed_17x17x768d  | Mixed_6d\n  mixed_17x17x768e  | Mixed_6e\n  mixed_8x8x1280a   | Mixed_7a\n  mixed_8x8x2048a   | Mixed_7b\n  mixed_8x8x2048b   | Mixed_7c\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\', \'MaxPool_5a_3x3\',\n      \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\',\n      \'Mixed_6d\', \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n  # end_points will collect relevant activations for external use, for example\n  # summaries or losses.\n  end_points = {}\n\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV3\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'VALID\'):\n      # 299 x 299 x 3\n      end_point = \'Conv2d_1a_3x3\'\n      net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 149 x 149 x 32\n      end_point = \'Conv2d_2a_3x3\'\n      net = slim.conv2d(net, depth(32), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 147 x 147 x 32\n      end_point = \'Conv2d_2b_3x3\'\n      net = slim.conv2d(net, depth(64), [3, 3], padding=\'SAME\', scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 147 x 147 x 64\n      end_point = \'MaxPool_3a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 73 x 73 x 64\n      end_point = \'Conv2d_3b_1x1\'\n      net = slim.conv2d(net, depth(80), [1, 1], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 73 x 73 x 80.\n      end_point = \'Conv2d_4a_3x3\'\n      net = slim.conv2d(net, depth(192), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 71 x 71 x 192.\n      end_point = \'MaxPool_5a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 35 x 35 x 192.\n\n    # Inception blocks\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'SAME\'):\n      # mixed: 35 x 35 x 256.\n      end_point = \'Mixed_5b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(32), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_1: 35 x 35 x 288.\n      end_point = \'Mixed_5c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0b_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv_1_0c_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1],\n                                 scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_2: 35 x 35 x 288.\n      end_point = \'Mixed_5d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_3: 17 x 17 x 768.\n      end_point = \'Mixed_6a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(384), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_1x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed4: 17 x 17 x 768.\n      end_point = \'Mixed_6b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(128), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(128), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(128), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_5: 17 x 17 x 768.\n      end_point = \'Mixed_6c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # mixed_6: 17 x 17 x 768.\n      end_point = \'Mixed_6d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_7: 17 x 17 x 768.\n      end_point = \'Mixed_6e\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_8: 8 x 8 x 1280.\n      end_point = \'Mixed_7a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(320), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # mixed_9: 8 x 8 x 2048.\n      end_point = \'Mixed_7b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(320), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(384), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_1, depth(384), [1, 3], scope=\'Conv2d_0b_1x3\'),\n              slim.conv2d(branch_1, depth(384), [3, 1], scope=\'Conv2d_0b_3x1\')])\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(448), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(\n              branch_2, depth(384), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_2, depth(384), [1, 3], scope=\'Conv2d_0c_1x3\'),\n              slim.conv2d(branch_2, depth(384), [3, 1], scope=\'Conv2d_0d_3x1\')])\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(192), [1, 1], scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_10: 8 x 8 x 2048.\n      end_point = \'Mixed_7c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(320), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(384), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_1, depth(384), [1, 3], scope=\'Conv2d_0b_1x3\'),\n              slim.conv2d(branch_1, depth(384), [3, 1], scope=\'Conv2d_0c_3x1\')])\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(448), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(\n              branch_2, depth(384), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_2, depth(384), [1, 3], scope=\'Conv2d_0c_1x3\'),\n              slim.conv2d(branch_2, depth(384), [3, 1], scope=\'Conv2d_0d_3x1\')])\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(192), [1, 1], scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v3(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 min_depth=16,\n                 depth_multiplier=1.0,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV3\'):\n  """"""Inception model from http://arxiv.org/abs/1512.00567.\n\n  ""Rethinking the Inception Architecture for Computer Vision""\n\n  Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens,\n  Zbigniew Wojna.\n\n  With the default arguments this method constructs the exact model defined in\n  the paper. However, one can experiment with variations of the inception_v3\n  network by changing arguments dropout_keep_prob, min_depth and\n  depth_multiplier.\n\n  The default image size used to train this network is 299x299.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: if \'depth_multiplier\' is less than or equal to zero.\n  """"""\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV3\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v3_base(\n          inputs, scope=scope, min_depth=min_depth,\n          depth_multiplier=depth_multiplier)\n\n      # Auxiliary Head logits\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding=\'SAME\'):\n        aux_logits = end_points[\'Mixed_6e\']\n        with tf.variable_scope(\'AuxLogits\'):\n          aux_logits = slim.avg_pool2d(\n              aux_logits, [5, 5], stride=3, padding=\'VALID\',\n              scope=\'AvgPool_1a_5x5\')\n          aux_logits = slim.conv2d(aux_logits, depth(128), [1, 1],\n                                   scope=\'Conv2d_1b_1x1\')\n\n          # Shape of feature map before the final layer.\n          kernel_size = _reduced_kernel_size_for_small_input(\n              aux_logits, [5, 5])\n          aux_logits = slim.conv2d(\n              aux_logits, depth(768), kernel_size,\n              weights_initializer=trunc_normal(0.01),\n              padding=\'VALID\', scope=\'Conv2d_2a_{}x{}\'.format(*kernel_size))\n          aux_logits = slim.conv2d(\n              aux_logits, num_classes, [1, 1], activation_fn=None,\n              normalizer_fn=None, weights_initializer=trunc_normal(0.001),\n              scope=\'Conv2d_2b_1x1\')\n          if spatial_squeeze:\n            aux_logits = tf.squeeze(aux_logits, [1, 2], name=\'SpatialSqueeze\')\n          end_points[\'AuxLogits\'] = aux_logits\n\n      # Final pooling and prediction\n      with tf.variable_scope(\'Logits\'):\n        kernel_size = _reduced_kernel_size_for_small_input(net, [8, 8])\n        net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                              scope=\'AvgPool_1a_{}x{}\'.format(*kernel_size))\n        # 1 x 1 x 2048\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        end_points[\'PreLogits\'] = net\n        # 2048\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n        # 1000\n      end_points[\'Logits\'] = logits\n      end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v3.default_image_size = 299\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are is large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n\n  TODO(jrru): Make this function work with unknown shapes. Theoretically, this\n  can be done with the code below. Problems are two-fold: (1) If the shape was\n  known, it will be lost. (2) inception.slim.ops._two_element_tuple cannot\n  handle tensors that define the kernel size.\n      shape = tf.shape(input_tensor)\n      return = tf.pack([tf.minimum(shape[1], kernel_size[0]),\n                        tf.minimum(shape[2], kernel_size[1])])\n\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ninception_v3_arg_scope = inception_utils.inception_arg_scope\n'"
libs/networks/slim_nets/inception_v3_test.py,29,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim_nets.inception_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV3Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    final_endpoint, end_points = inception.inception_v3_base(inputs)\n    self.assertTrue(final_endpoint.op.name.startswith(\n        \'InceptionV3/Mixed_7c\'))\n    self.assertListEqual(final_endpoint.get_shape().as_list(),\n                         [batch_size, 8, 8, 2048])\n    expected_endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                          \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                          \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n                          \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n                          \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 299, 299\n    endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                 \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                 \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n                 \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n                 \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\']\n\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v3_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV3/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed7c(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3_base(\n        inputs, final_endpoint=\'Mixed_7c\')\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [batch_size, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [batch_size, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [batch_size, 147, 147, 64],\n                        \'MaxPool_3a_3x3\': [batch_size, 73, 73, 64],\n                        \'Conv2d_3b_1x1\': [batch_size, 73, 73, 80],\n                        \'Conv2d_4a_3x3\': [batch_size, 71, 71, 192],\n                        \'MaxPool_5a_3x3\': [batch_size, 35, 35, 192],\n                        \'Mixed_5b\': [batch_size, 35, 35, 256],\n                        \'Mixed_5c\': [batch_size, 35, 35, 288],\n                        \'Mixed_5d\': [batch_size, 35, 35, 288],\n                        \'Mixed_6a\': [batch_size, 17, 17, 768],\n                        \'Mixed_6b\': [batch_size, 17, 17, 768],\n                        \'Mixed_6c\': [batch_size, 17, 17, 768],\n                        \'Mixed_6d\': [batch_size, 17, 17, 768],\n                        \'Mixed_6e\': [batch_size, 17, 17, 768],\n                        \'Mixed_7a\': [batch_size, 8, 8, 1280],\n                        \'Mixed_7b\': [batch_size, 8, 8, 2048],\n                        \'Mixed_7c\': [batch_size, 8, 8, 2048]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 299, 299\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v3_arg_scope()):\n      inception.inception_v3_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(21802784, total_params)\n\n  def testBuildEndPoints(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(\'Logits\' in end_points)\n    logits = end_points[\'Logits\']\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'AuxLogits\' in end_points)\n    aux_logits = end_points[\'AuxLogits\']\n    self.assertListEqual(aux_logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Mixed_7c\' in end_points)\n    pre_pool = end_points[\'Mixed_7c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 8, 8, 2048])\n    self.assertTrue(\'PreLogits\' in end_points)\n    pre_logits = end_points[\'PreLogits\']\n    self.assertListEqual(pre_logits.get_shape().as_list(),\n                         [batch_size, 1, 1, 2048])\n\n  def testBuildEndPointsWithDepthMultiplierLessThanOne(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v3(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=0.5)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(0.5 * original_depth, new_depth)\n\n  def testBuildEndPointsWithDepthMultiplierGreaterThanOne(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v3(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=2.0)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(2.0 * original_depth, new_depth)\n\n  def testRaiseValueErrorWithInvalidDepthMultiplier(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v3(inputs, num_classes, depth_multiplier=-0.1)\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v3(inputs, num_classes, depth_multiplier=0.0)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_7c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 3, 3, 2048])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v3(inputs, num_classes)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_7c\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 8, 8, 2048])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v3(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v3(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v3(eval_inputs, num_classes,\n                                       is_training=False, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 299, 299, 3])\n    logits, _ = inception.inception_v3(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/inception_v4.py,48,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition of the Inception V4 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\n\n\ndef block_inception_a(inputs, scope=None, reuse=None):\n  """"""Builds Inception-A block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionA\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n        branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0c_3x3\')\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 96, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef block_reduction_a(inputs, scope=None, reuse=None):\n  """"""Builds Reduction-A block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockReductionA\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 384, [3, 3], stride=2, padding=\'VALID\',\n                               scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope=\'Conv2d_0b_3x3\')\n        branch_1 = slim.conv2d(branch_1, 256, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding=\'VALID\',\n                                   scope=\'MaxPool_1a_3x3\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n\n\ndef block_inception_b(inputs, scope=None, reuse=None):\n  """"""Builds Inception-B block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionB\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 224, [1, 7], scope=\'Conv2d_0b_1x7\')\n        branch_1 = slim.conv2d(branch_1, 256, [7, 1], scope=\'Conv2d_0c_7x1\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope=\'Conv2d_0b_7x1\')\n        branch_2 = slim.conv2d(branch_2, 224, [1, 7], scope=\'Conv2d_0c_1x7\')\n        branch_2 = slim.conv2d(branch_2, 224, [7, 1], scope=\'Conv2d_0d_7x1\')\n        branch_2 = slim.conv2d(branch_2, 256, [1, 7], scope=\'Conv2d_0e_1x7\')\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef block_reduction_b(inputs, scope=None, reuse=None):\n  """"""Builds Reduction-B block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockReductionB\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_0 = slim.conv2d(branch_0, 192, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 256, [1, 7], scope=\'Conv2d_0b_1x7\')\n        branch_1 = slim.conv2d(branch_1, 320, [7, 1], scope=\'Conv2d_0c_7x1\')\n        branch_1 = slim.conv2d(branch_1, 320, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding=\'VALID\',\n                                   scope=\'MaxPool_1a_3x3\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n\n\ndef block_inception_c(inputs, scope=None, reuse=None):\n  """"""Builds Inception-C block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionC\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = tf.concat(axis=3, values=[\n            slim.conv2d(branch_1, 256, [1, 3], scope=\'Conv2d_0b_1x3\'),\n            slim.conv2d(branch_1, 256, [3, 1], scope=\'Conv2d_0c_3x1\')])\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 448, [3, 1], scope=\'Conv2d_0b_3x1\')\n        branch_2 = slim.conv2d(branch_2, 512, [1, 3], scope=\'Conv2d_0c_1x3\')\n        branch_2 = tf.concat(axis=3, values=[\n            slim.conv2d(branch_2, 256, [1, 3], scope=\'Conv2d_0d_1x3\'),\n            slim.conv2d(branch_2, 256, [3, 1], scope=\'Conv2d_0e_3x1\')])\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 256, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef inception_v4_base(inputs, final_endpoint=\'Mixed_7d\', scope=None):\n  """"""Creates the Inception V4 network up to the given final endpoint.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    final_endpoint: specifies the endpoint to construct the network up to.\n      It can be one of [ \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'Mixed_3a\', \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n      \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\', \'Mixed_6e\',\n      \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\',\n      \'Mixed_7d\']\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n  """"""\n  end_points = {}\n\n  def add_and_check_final(name, net):\n    end_points[name] = net\n    return name == final_endpoint\n\n  with tf.variable_scope(scope, \'InceptionV4\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'SAME\'):\n      # 299 x 299 x 3\n      net = slim.conv2d(inputs, 32, [3, 3], stride=2,\n                        padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      if add_and_check_final(\'Conv2d_1a_3x3\', net): return net, end_points\n      # 149 x 149 x 32\n      net = slim.conv2d(net, 32, [3, 3], padding=\'VALID\',\n                        scope=\'Conv2d_2a_3x3\')\n      if add_and_check_final(\'Conv2d_2a_3x3\', net): return net, end_points\n      # 147 x 147 x 32\n      net = slim.conv2d(net, 64, [3, 3], scope=\'Conv2d_2b_3x3\')\n      if add_and_check_final(\'Conv2d_2b_3x3\', net): return net, end_points\n      # 147 x 147 x 64\n      with tf.variable_scope(\'Mixed_3a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_0a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, 96, [3, 3], stride=2, padding=\'VALID\',\n                                 scope=\'Conv2d_0a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_3a\', net): return net, end_points\n\n      # 73 x 73 x 160\n      with tf.variable_scope(\'Mixed_4a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, 96, [3, 3], padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, 64, [1, 7], scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, 64, [7, 1], scope=\'Conv2d_0c_7x1\')\n          branch_1 = slim.conv2d(branch_1, 96, [3, 3], padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_4a\', net): return net, end_points\n\n      # 71 x 71 x 192\n      with tf.variable_scope(\'Mixed_5a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, 192, [3, 3], stride=2, padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_5a\', net): return net, end_points\n\n      # 35 x 35 x 384\n      # 4 x Inception-A blocks\n      for idx in range(4):\n        block_scope = \'Mixed_5\' + chr(ord(\'b\') + idx)\n        net = block_inception_a(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n\n      # 35 x 35 x 384\n      # Reduction-A block\n      net = block_reduction_a(net, \'Mixed_6a\')\n      if add_and_check_final(\'Mixed_6a\', net): return net, end_points\n\n      # 17 x 17 x 1024\n      # 7 x Inception-B blocks\n      for idx in range(7):\n        block_scope = \'Mixed_6\' + chr(ord(\'b\') + idx)\n        net = block_inception_b(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n\n      # 17 x 17 x 1024\n      # Reduction-B block\n      net = block_reduction_b(net, \'Mixed_7a\')\n      if add_and_check_final(\'Mixed_7a\', net): return net, end_points\n\n      # 8 x 8 x 1536\n      # 3 x Inception-C blocks\n      for idx in range(3):\n        block_scope = \'Mixed_7\' + chr(ord(\'b\') + idx)\n        net = block_inception_c(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n  raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v4(inputs, num_classes=1001, is_training=True,\n                 dropout_keep_prob=0.8,\n                 reuse=None,\n                 scope=\'InceptionV4\',\n                 create_aux_logits=True):\n  """"""Creates the Inception V4 model.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: float, the fraction to keep before final layer.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    create_aux_logits: Whether to include the auxiliary logits.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n  """"""\n  end_points = {}\n  with tf.variable_scope(scope, \'InceptionV4\', [inputs], reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v4_base(inputs, scope=scope)\n\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding=\'SAME\'):\n        # Auxiliary Head logits\n        if create_aux_logits:\n          with tf.variable_scope(\'AuxLogits\'):\n            # 17 x 17 x 1024\n            aux_logits = end_points[\'Mixed_6h\']\n            aux_logits = slim.avg_pool2d(aux_logits, [5, 5], stride=3,\n                                         padding=\'VALID\',\n                                         scope=\'AvgPool_1a_5x5\')\n            aux_logits = slim.conv2d(aux_logits, 128, [1, 1],\n                                     scope=\'Conv2d_1b_1x1\')\n            aux_logits = slim.conv2d(aux_logits, 768,\n                                     aux_logits.get_shape()[1:3],\n                                     padding=\'VALID\', scope=\'Conv2d_2a\')\n            aux_logits = slim.flatten(aux_logits)\n            aux_logits = slim.fully_connected(aux_logits, num_classes,\n                                              activation_fn=None,\n                                              scope=\'Aux_logits\')\n            end_points[\'AuxLogits\'] = aux_logits\n\n        # Final pooling and prediction\n        with tf.variable_scope(\'Logits\'):\n          # 8 x 8 x 1536\n          net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n                                scope=\'AvgPool_1a\')\n          # 1 x 1 x 1536\n          net = slim.dropout(net, dropout_keep_prob, scope=\'Dropout_1b\')\n          net = slim.flatten(net, scope=\'PreLogitsFlatten\')\n          end_points[\'PreLogitsFlatten\'] = net\n          # 1536\n          logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                        scope=\'Logits\')\n          end_points[\'Logits\'] = logits\n          end_points[\'Predictions\'] = tf.nn.softmax(logits, name=\'Predictions\')\n    return logits, end_points\ninception_v4.default_image_size = 299\n\n\ninception_v4_arg_scope = inception_utils.inception_arg_scope\n'"
libs/networks/slim_nets/inception_v4_test.py,24,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.inception_v4.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception\n\n\nclass InceptionTest(tf.test.TestCase):\n\n  def testBuildLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v4(inputs, num_classes)\n    auxlogits = end_points[\'AuxLogits\']\n    predictions = end_points[\'Predictions\']\n    self.assertTrue(auxlogits.op.name.startswith(\'InceptionV4/AuxLogits\'))\n    self.assertListEqual(auxlogits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(predictions.op.name.startswith(\n        \'InceptionV4/Logits/Predictions\'))\n    self.assertListEqual(predictions.get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildWithoutAuxLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, endpoints = inception.inception_v4(inputs, num_classes,\n                                               create_aux_logits=False)\n    self.assertFalse(\'AuxLogits\' in endpoints)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testAllEndPointsShapes(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v4(inputs, num_classes)\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [batch_size, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [batch_size, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [batch_size, 147, 147, 64],\n                        \'Mixed_3a\': [batch_size, 73, 73, 160],\n                        \'Mixed_4a\': [batch_size, 71, 71, 192],\n                        \'Mixed_5a\': [batch_size, 35, 35, 384],\n                        # 4 x Inception-A blocks\n                        \'Mixed_5b\': [batch_size, 35, 35, 384],\n                        \'Mixed_5c\': [batch_size, 35, 35, 384],\n                        \'Mixed_5d\': [batch_size, 35, 35, 384],\n                        \'Mixed_5e\': [batch_size, 35, 35, 384],\n                        # Reduction-A block\n                        \'Mixed_6a\': [batch_size, 17, 17, 1024],\n                        # 7 x Inception-B blocks\n                        \'Mixed_6b\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6c\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6d\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6e\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6f\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6g\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6h\': [batch_size, 17, 17, 1024],\n                        # Reduction-A block\n                        \'Mixed_7a\': [batch_size, 8, 8, 1536],\n                        # 3 x Inception-C blocks\n                        \'Mixed_7b\': [batch_size, 8, 8, 1536],\n                        \'Mixed_7c\': [batch_size, 8, 8, 1536],\n                        \'Mixed_7d\': [batch_size, 8, 8, 1536],\n                        # Logits and predictions\n                        \'AuxLogits\': [batch_size, num_classes],\n                        \'PreLogitsFlatten\': [batch_size, 1536],\n                        \'Logits\': [batch_size, num_classes],\n                        \'Predictions\': [batch_size, num_classes]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    net, end_points = inception.inception_v4_base(inputs)\n    self.assertTrue(net.op.name.startswith(\n        \'InceptionV4/Mixed_7d\'))\n    self.assertListEqual(net.get_shape().as_list(), [batch_size, 8, 8, 1536])\n    expected_endpoints = [\n        \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\', \'Mixed_3a\',\n        \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n        \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n        \'Mixed_6e\', \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\',\n        \'Mixed_7b\', \'Mixed_7c\', \'Mixed_7d\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n    for name, op in end_points.iteritems():\n      self.assertTrue(op.name.startswith(\'InceptionV4/\' + name))\n\n  def testBuildOnlyUpToFinalEndpoint(self):\n    batch_size = 5\n    height, width = 299, 299\n    all_endpoints = [\n        \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\', \'Mixed_3a\',\n        \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n        \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n        \'Mixed_6e\', \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\',\n        \'Mixed_7b\', \'Mixed_7c\', \'Mixed_7d\']\n    for index, endpoint in enumerate(all_endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v4_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV4/\' + endpoint))\n        self.assertItemsEqual(all_endpoints[:index+1], end_points)\n\n  def testVariablesSetDevice(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    # Force all Variables to reside on the device.\n    with tf.variable_scope(\'on_cpu\'), tf.device(\'/cpu:0\'):\n      inception.inception_v4(inputs, num_classes)\n    with tf.variable_scope(\'on_gpu\'), tf.device(\'/gpu:0\'):\n      inception.inception_v4(inputs, num_classes)\n    for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_cpu\'):\n      self.assertDeviceEqual(v.device, \'/cpu:0\')\n    for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_gpu\'):\n      self.assertDeviceEqual(v.device, \'/gpu:0\')\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v4(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_7d\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 3, 3, 1536])\n\n  def testUnknownBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n      logits, _ = inception.inception_v4(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [None, num_classes])\n      images = tf.random_uniform((batch_size, height, width, 3))\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = inception.inception_v4(eval_inputs,\n                                         num_classes,\n                                         is_training=False)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session() as sess:\n      train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n      inception.inception_v4(train_inputs, num_classes)\n      eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n      logits, _ = inception.inception_v4(eval_inputs,\n                                         num_classes,\n                                         is_training=False,\n                                         reuse=True)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/lenet.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a variant of the LeNet model definition.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef lenet(images, num_classes=10, is_training=False,\n          dropout_keep_prob=0.5,\n          prediction_fn=slim.softmax,\n          scope=\'LeNet\'):\n  """"""Creates a variant of the LeNet model.\n\n  Note that since the output is a set of \'logits\', the values fall in the\n  interval of (-infinity, infinity). Consequently, to convert the outputs to a\n  probability distribution over the characters, one will need to convert them\n  using the softmax function:\n\n        logits = lenet.lenet(images, is_training=False)\n        probabilities = tf.nn.softmax(logits)\n        predictions = tf.argmax(logits, 1)\n\n  Args:\n    images: A batch of `Tensors` of size [batch_size, height, width, channels].\n    num_classes: the number of classes in the dataset.\n    is_training: specifies whether or not we\'re currently training the model.\n      This variable will determine the behaviour of the dropout layer.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, `num_classes`]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'LeNet\', [images, num_classes]):\n    net = slim.conv2d(images, 32, [5, 5], scope=\'conv1\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool1\')\n    net = slim.conv2d(net, 64, [5, 5], scope=\'conv2\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool2\')\n    net = slim.flatten(net)\n    end_points[\'Flatten\'] = net\n\n    net = slim.fully_connected(net, 1024, scope=\'fc3\')\n    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                       scope=\'dropout3\')\n    logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                  scope=\'fc4\')\n\n  end_points[\'Logits\'] = logits\n  end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n\n  return logits, end_points\nlenet.default_image_size = 28\n\n\ndef lenet_arg_scope(weight_decay=0.0):\n  """"""Defines the default lenet argument scope.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n\n  Returns:\n    An `arg_scope` to use for the inception v3 model.\n  """"""\n  with slim.arg_scope(\n      [slim.conv2d, slim.fully_connected],\n      weights_regularizer=slim.l2_regularizer(weight_decay),\n      weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n      activation_fn=tf.nn.relu) as sc:\n    return sc\n'"
libs/networks/slim_nets/mobilenet_v1.py,9,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n""""""MobileNet v1.\n\nMobileNet is a general architecture and can be used for multiple use cases.\nDepending on the use case, it can use different input layer size and different\nhead (for example: embeddings, localization and classification).\n\nAs described in https://arxiv.org/abs/1704.04861.\n\n  MobileNets: Efficient Convolutional Neural Networks for\n    Mobile Vision Applications\n  Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,\n    Tobias Weyand, Marco Andreetto, Hartwig Adam\n\n100% Mobilenet V1 (base) with input size 224x224:\n\nLayer                                                     params           macs\n--------------------------------------------------------------------------------\nMobilenetV1/Conv2d_0/Conv2D:                                 864      10,838,016\nMobilenetV1/Conv2d_1_depthwise/depthwise:                    288       3,612,672\nMobilenetV1/Conv2d_1_pointwise/Conv2D:                     2,048      25,690,112\nMobilenetV1/Conv2d_2_depthwise/depthwise:                    576       1,806,336\nMobilenetV1/Conv2d_2_pointwise/Conv2D:                     8,192      25,690,112\nMobilenetV1/Conv2d_3_depthwise/depthwise:                  1,152       3,612,672\nMobilenetV1/Conv2d_3_pointwise/Conv2D:                    16,384      51,380,224\nMobilenetV1/Conv2d_4_depthwise/depthwise:                  1,152         903,168\nMobilenetV1/Conv2d_4_pointwise/Conv2D:                    32,768      25,690,112\nMobilenetV1/Conv2d_5_depthwise/depthwise:                  2,304       1,806,336\nMobilenetV1/Conv2d_5_pointwise/Conv2D:                    65,536      51,380,224\nMobilenetV1/Conv2d_6_depthwise/depthwise:                  2,304         451,584\nMobilenetV1/Conv2d_6_pointwise/Conv2D:                   131,072      25,690,112\nMobilenetV1/Conv2d_7_depthwise/depthwise:                  4,608         903,168\nMobilenetV1/Conv2d_7_pointwise/Conv2D:                   262,144      51,380,224\nMobilenetV1/Conv2d_8_depthwise/depthwise:                  4,608         903,168\nMobilenetV1/Conv2d_8_pointwise/Conv2D:                   262,144      51,380,224\nMobilenetV1/Conv2d_9_depthwise/depthwise:                  4,608         903,168\nMobilenetV1/Conv2d_9_pointwise/Conv2D:                   262,144      51,380,224\nMobilenetV1/Conv2d_10_depthwise/depthwise:                 4,608         903,168\nMobilenetV1/Conv2d_10_pointwise/Conv2D:                  262,144      51,380,224\nMobilenetV1/Conv2d_11_depthwise/depthwise:                 4,608         903,168\nMobilenetV1/Conv2d_11_pointwise/Conv2D:                  262,144      51,380,224\nMobilenetV1/Conv2d_12_depthwise/depthwise:                 4,608         225,792\nMobilenetV1/Conv2d_12_pointwise/Conv2D:                  524,288      25,690,112\nMobilenetV1/Conv2d_13_depthwise/depthwise:                 9,216         451,584\nMobilenetV1/Conv2d_13_pointwise/Conv2D:                1,048,576      51,380,224\n--------------------------------------------------------------------------------\nTotal:                                                 3,185,088     567,716,352\n\n\n75% Mobilenet V1 (base) with input size 128x128:\n\nLayer                                                     params           macs\n--------------------------------------------------------------------------------\nMobilenetV1/Conv2d_0/Conv2D:                                 648       2,654,208\nMobilenetV1/Conv2d_1_depthwise/depthwise:                    216         884,736\nMobilenetV1/Conv2d_1_pointwise/Conv2D:                     1,152       4,718,592\nMobilenetV1/Conv2d_2_depthwise/depthwise:                    432         442,368\nMobilenetV1/Conv2d_2_pointwise/Conv2D:                     4,608       4,718,592\nMobilenetV1/Conv2d_3_depthwise/depthwise:                    864         884,736\nMobilenetV1/Conv2d_3_pointwise/Conv2D:                     9,216       9,437,184\nMobilenetV1/Conv2d_4_depthwise/depthwise:                    864         221,184\nMobilenetV1/Conv2d_4_pointwise/Conv2D:                    18,432       4,718,592\nMobilenetV1/Conv2d_5_depthwise/depthwise:                  1,728         442,368\nMobilenetV1/Conv2d_5_pointwise/Conv2D:                    36,864       9,437,184\nMobilenetV1/Conv2d_6_depthwise/depthwise:                  1,728         110,592\nMobilenetV1/Conv2d_6_pointwise/Conv2D:                    73,728       4,718,592\nMobilenetV1/Conv2d_7_depthwise/depthwise:                  3,456         221,184\nMobilenetV1/Conv2d_7_pointwise/Conv2D:                   147,456       9,437,184\nMobilenetV1/Conv2d_8_depthwise/depthwise:                  3,456         221,184\nMobilenetV1/Conv2d_8_pointwise/Conv2D:                   147,456       9,437,184\nMobilenetV1/Conv2d_9_depthwise/depthwise:                  3,456         221,184\nMobilenetV1/Conv2d_9_pointwise/Conv2D:                   147,456       9,437,184\nMobilenetV1/Conv2d_10_depthwise/depthwise:                 3,456         221,184\nMobilenetV1/Conv2d_10_pointwise/Conv2D:                  147,456       9,437,184\nMobilenetV1/Conv2d_11_depthwise/depthwise:                 3,456         221,184\nMobilenetV1/Conv2d_11_pointwise/Conv2D:                  147,456       9,437,184\nMobilenetV1/Conv2d_12_depthwise/depthwise:                 3,456          55,296\nMobilenetV1/Conv2d_12_pointwise/Conv2D:                  294,912       4,718,592\nMobilenetV1/Conv2d_13_depthwise/depthwise:                 6,912         110,592\nMobilenetV1/Conv2d_13_pointwise/Conv2D:                  589,824       9,437,184\n--------------------------------------------------------------------------------\nTotal:                                                 1,800,144     106,002,432\n\n""""""\n\n# Tensorflow mandates these.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import namedtuple\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n# Conv and DepthSepConv namedtuple define layers of the MobileNet architecture\n# Conv defines 3x3 convolution layers\n# DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.\n# stride is the stride of the convolution\n# depth is the number of channels or filters in a layer\nConv = namedtuple(\'Conv\', [\'kernel\', \'stride\', \'depth\'])\nDepthSepConv = namedtuple(\'DepthSepConv\', [\'kernel\', \'stride\', \'depth\'])\n\n# _CONV_DEFS specifies the MobileNet body\n_CONV_DEFS = [\n    Conv(kernel=[3, 3], stride=2, depth=32),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=64),\n    DepthSepConv(kernel=[3, 3], stride=2, depth=128),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=128),\n    DepthSepConv(kernel=[3, 3], stride=2, depth=256),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=256),\n    DepthSepConv(kernel=[3, 3], stride=2, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=2, depth=1024),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=1024)\n]\n\n\ndef mobilenet_v1_base(inputs,\n                      final_endpoint=\'Conv2d_13_pointwise\',\n                      min_depth=8,\n                      depth_multiplier=1.0,\n                      conv_defs=None,\n                      output_stride=None,\n                      scope=None):\n  """"""Mobilenet v1.\n\n  Constructs a Mobilenet v1 network from inputs to the given final endpoint.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_0\', \'Conv2d_1_pointwise\', \'Conv2d_2_pointwise\',\n      \'Conv2d_3_pointwise\', \'Conv2d_4_pointwise\', \'Conv2d_5\'_pointwise,\n      \'Conv2d_6_pointwise\', \'Conv2d_7_pointwise\', \'Conv2d_8_pointwise\',\n      \'Conv2d_9_pointwise\', \'Conv2d_10_pointwise\', \'Conv2d_11_pointwise\',\n      \'Conv2d_12_pointwise\', \'Conv2d_13_pointwise\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    conv_defs: A list of ConvDef namedtuples specifying the net architecture.\n    output_stride: An integer that specifies the requested ratio of input to\n      output spatial resolution. If not None, then we invoke atrous convolution\n      if necessary to prevent the network from reducing the spatial resolution\n      of the activation maps. Allowed values are 8 (accurate fully convolutional\n      mode), 16 (fast fully convolutional mode), 32 (classification mode).\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0, or the target output_stride is not\n                allowed.\n  """"""\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n  end_points = {}\n\n  # Used to find thinned depths for each layer.\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n\n  if conv_defs is None:\n    conv_defs = _CONV_DEFS\n\n  if output_stride is not None and output_stride not in [8, 16, 32]:\n    raise ValueError(\'Only allowed output_stride values are 8, 16, 32.\')\n\n  with tf.variable_scope(scope, \'MobilenetV1\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d], padding=\'SAME\'):\n      # The current_stride variable keeps track of the output stride of the\n      # activations, i.e., the running product of convolution strides up to the\n      # current network layer. This allows us to invoke atrous convolution\n      # whenever applying the next convolution would result in the activations\n      # having output stride larger than the target output_stride.\n      current_stride = 1\n\n      # The atrous convolution rate parameter.\n      rate = 1\n\n      net = inputs\n      for i, conv_def in enumerate(conv_defs):\n        end_point_base = \'Conv2d_%d\' % i\n\n        if output_stride is not None and current_stride == output_stride:\n          # If we have reached the target output_stride, then we need to employ\n          # atrous convolution with stride=1 and multiply the atrous rate by the\n          # current unit\'s stride for use in subsequent layers.\n          layer_stride = 1\n          layer_rate = rate\n          rate *= conv_def.stride\n        else:\n          layer_stride = conv_def.stride\n          layer_rate = 1\n          current_stride *= conv_def.stride\n\n        if isinstance(conv_def, Conv):\n          end_point = end_point_base\n          net = slim.conv2d(net, depth(conv_def.depth), conv_def.kernel,\n                            stride=conv_def.stride,\n                            normalizer_fn=slim.batch_norm,\n                            scope=end_point)\n          end_points[end_point] = net\n          if end_point == final_endpoint:\n            return net, end_points\n\n        elif isinstance(conv_def, DepthSepConv):\n          end_point = end_point_base + \'_depthwise\'\n\n          # By passing filters=None\n          # separable_conv2d produces only a depthwise convolution layer\n          net = slim.separable_conv2d(net, None, conv_def.kernel,\n                                      depth_multiplier=1,\n                                      stride=layer_stride,\n                                      rate=layer_rate,\n                                      normalizer_fn=slim.batch_norm,\n                                      scope=end_point)\n\n          end_points[end_point] = net\n          if end_point == final_endpoint:\n            return net, end_points\n\n          end_point = end_point_base + \'_pointwise\'\n\n          net = slim.conv2d(net, depth(conv_def.depth), [1, 1],\n                            stride=1,\n                            normalizer_fn=slim.batch_norm,\n                            scope=end_point)\n\n          end_points[end_point] = net\n          if end_point == final_endpoint:\n            return net, end_points\n        else:\n          raise ValueError(\'Unknown convolution type %s for layer %d\'\n                           % (conv_def.ltype, i))\n  raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef mobilenet_v1(inputs,\n                 num_classes=1000,\n                 dropout_keep_prob=0.999,\n                 is_training=True,\n                 min_depth=8,\n                 depth_multiplier=1.0,\n                 conv_defs=None,\n                 prediction_fn=tf.contrib.layers.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'MobilenetV1\'):\n  """"""Mobilenet v1 model for classification.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    is_training: whether is training or not.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    conv_defs: A list of ConvDef namedtuples specifying the net architecture.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape is [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: Input rank is invalid.\n  """"""\n  input_shape = inputs.get_shape().as_list()\n  if len(input_shape) != 4:\n    raise ValueError(\'Invalid input tensor rank, expected 4, was: %d\' %\n                     len(input_shape))\n\n  with tf.variable_scope(scope, \'MobilenetV1\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = mobilenet_v1_base(inputs, scope=scope,\n                                          min_depth=min_depth,\n                                          depth_multiplier=depth_multiplier,\n                                          conv_defs=conv_defs)\n      with tf.variable_scope(\'Logits\'):\n        kernel_size = _reduced_kernel_size_for_small_input(net, [7, 7])\n        net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                              scope=\'AvgPool_1a\')\n        end_points[\'AvgPool_1a\'] = net\n        # 1 x 1 x 1024\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n      end_points[\'Logits\'] = logits\n      if prediction_fn:\n        end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\n\nmobilenet_v1.default_image_size = 224\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ndef mobilenet_v1_arg_scope(is_training=True,\n                           weight_decay=0.00004,\n                           stddev=0.09,\n                           regularize_depthwise=False):\n  """"""Defines the default MobilenetV1 arg scope.\n\n  Args:\n    is_training: Whether or not we\'re training the model.\n    weight_decay: The weight decay to use for regularizing the model.\n    stddev: The standard deviation of the trunctated normal weight initializer.\n    regularize_depthwise: Whether or not apply regularization on depthwise.\n\n  Returns:\n    An `arg_scope` to use for the mobilenet v1 model.\n  """"""\n  batch_norm_params = {\n      \'is_training\': is_training,\n      \'center\': True,\n      \'scale\': True,\n      \'decay\': 0.9997,\n      \'epsilon\': 0.001,\n  }\n\n  # Set weight_decay for weights in Conv and DepthSepConv layers.\n  weights_init = tf.truncated_normal_initializer(stddev=stddev)\n  regularizer = tf.contrib.layers.l2_regularizer(weight_decay)\n  if regularize_depthwise:\n    depthwise_regularizer = regularizer\n  else:\n    depthwise_regularizer = None\n  with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                      weights_initializer=weights_init,\n                      activation_fn=tf.nn.relu6, normalizer_fn=slim.batch_norm):\n    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n      with slim.arg_scope([slim.conv2d], weights_regularizer=regularizer):\n        with slim.arg_scope([slim.separable_conv2d],\n                            weights_regularizer=depthwise_regularizer) as sc:\n          return sc\n'"
libs/networks/slim_nets/mobilenet_v1_test.py,32,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n""""""Tests for MobileNet v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import mobilenet_v1\n\nslim = tf.contrib.slim\n\n\nclass MobilenetV1Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'MobilenetV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    net, end_points = mobilenet_v1.mobilenet_v1_base(inputs)\n    self.assertTrue(net.op.name.startswith(\'MobilenetV1/Conv2d_13\'))\n    self.assertListEqual(net.get_shape().as_list(),\n                         [batch_size, 7, 7, 1024])\n    expected_endpoints = [\'Conv2d_0\',\n                          \'Conv2d_1_depthwise\', \'Conv2d_1_pointwise\',\n                          \'Conv2d_2_depthwise\', \'Conv2d_2_pointwise\',\n                          \'Conv2d_3_depthwise\', \'Conv2d_3_pointwise\',\n                          \'Conv2d_4_depthwise\', \'Conv2d_4_pointwise\',\n                          \'Conv2d_5_depthwise\', \'Conv2d_5_pointwise\',\n                          \'Conv2d_6_depthwise\', \'Conv2d_6_pointwise\',\n                          \'Conv2d_7_depthwise\', \'Conv2d_7_pointwise\',\n                          \'Conv2d_8_depthwise\', \'Conv2d_8_pointwise\',\n                          \'Conv2d_9_depthwise\', \'Conv2d_9_pointwise\',\n                          \'Conv2d_10_depthwise\', \'Conv2d_10_pointwise\',\n                          \'Conv2d_11_depthwise\', \'Conv2d_11_pointwise\',\n                          \'Conv2d_12_depthwise\', \'Conv2d_12_pointwise\',\n                          \'Conv2d_13_depthwise\', \'Conv2d_13_pointwise\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 224, 224\n    endpoints = [\'Conv2d_0\',\n                 \'Conv2d_1_depthwise\', \'Conv2d_1_pointwise\',\n                 \'Conv2d_2_depthwise\', \'Conv2d_2_pointwise\',\n                 \'Conv2d_3_depthwise\', \'Conv2d_3_pointwise\',\n                 \'Conv2d_4_depthwise\', \'Conv2d_4_pointwise\',\n                 \'Conv2d_5_depthwise\', \'Conv2d_5_pointwise\',\n                 \'Conv2d_6_depthwise\', \'Conv2d_6_pointwise\',\n                 \'Conv2d_7_depthwise\', \'Conv2d_7_pointwise\',\n                 \'Conv2d_8_depthwise\', \'Conv2d_8_pointwise\',\n                 \'Conv2d_9_depthwise\', \'Conv2d_9_pointwise\',\n                 \'Conv2d_10_depthwise\', \'Conv2d_10_pointwise\',\n                 \'Conv2d_11_depthwise\', \'Conv2d_11_pointwise\',\n                 \'Conv2d_12_depthwise\', \'Conv2d_12_pointwise\',\n                 \'Conv2d_13_depthwise\', \'Conv2d_13_pointwise\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = mobilenet_v1.mobilenet_v1_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'MobilenetV1/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildCustomNetworkUsingConvDefs(self):\n    batch_size = 5\n    height, width = 224, 224\n    conv_defs = [\n        mobilenet_v1.Conv(kernel=[3, 3], stride=2, depth=32),\n        mobilenet_v1.DepthSepConv(kernel=[3, 3], stride=1, depth=64),\n        mobilenet_v1.DepthSepConv(kernel=[3, 3], stride=2, depth=128),\n        mobilenet_v1.DepthSepConv(kernel=[3, 3], stride=1, depth=512)\n    ]\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    net, end_points = mobilenet_v1.mobilenet_v1_base(\n        inputs, final_endpoint=\'Conv2d_3_pointwise\', conv_defs=conv_defs)\n    self.assertTrue(net.op.name.startswith(\'MobilenetV1/Conv2d_3\'))\n    self.assertListEqual(net.get_shape().as_list(),\n                         [batch_size, 56, 56, 512])\n    expected_endpoints = [\'Conv2d_0\',\n                          \'Conv2d_1_depthwise\', \'Conv2d_1_pointwise\',\n                          \'Conv2d_2_depthwise\', \'Conv2d_2_pointwise\',\n                          \'Conv2d_3_depthwise\', \'Conv2d_3_pointwise\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildAndCheckAllEndPointsUptoConv2d_13(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                        normalizer_fn=slim.batch_norm):\n      _, end_points = mobilenet_v1.mobilenet_v1_base(\n          inputs, final_endpoint=\'Conv2d_13_pointwise\')\n    endpoints_shapes = {\'Conv2d_0\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_depthwise\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_pointwise\': [batch_size, 112, 112, 64],\n                        \'Conv2d_2_depthwise\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_depthwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_4_depthwise\': [batch_size, 28, 28, 128],\n                        \'Conv2d_4_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_depthwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_6_depthwise\': [batch_size, 14, 14, 256],\n                        \'Conv2d_6_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_7_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_7_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_8_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_8_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_9_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_9_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_10_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_10_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_11_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_11_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_12_depthwise\': [batch_size, 7, 7, 512],\n                        \'Conv2d_12_pointwise\': [batch_size, 7, 7, 1024],\n                        \'Conv2d_13_depthwise\': [batch_size, 7, 7, 1024],\n                        \'Conv2d_13_pointwise\': [batch_size, 7, 7, 1024]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name, expected_shape in endpoints_shapes.iteritems():\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testOutputStride16BuildAndCheckAllEndPointsUptoConv2d_13(self):\n    batch_size = 5\n    height, width = 224, 224\n    output_stride = 16\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                        normalizer_fn=slim.batch_norm):\n      _, end_points = mobilenet_v1.mobilenet_v1_base(\n          inputs, output_stride=output_stride,\n          final_endpoint=\'Conv2d_13_pointwise\')\n    endpoints_shapes = {\'Conv2d_0\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_depthwise\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_pointwise\': [batch_size, 112, 112, 64],\n                        \'Conv2d_2_depthwise\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_depthwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_4_depthwise\': [batch_size, 28, 28, 128],\n                        \'Conv2d_4_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_depthwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_6_depthwise\': [batch_size, 14, 14, 256],\n                        \'Conv2d_6_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_7_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_7_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_8_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_8_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_9_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_9_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_10_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_10_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_11_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_11_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_12_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_12_pointwise\': [batch_size, 14, 14, 1024],\n                        \'Conv2d_13_depthwise\': [batch_size, 14, 14, 1024],\n                        \'Conv2d_13_pointwise\': [batch_size, 14, 14, 1024]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name, expected_shape in endpoints_shapes.iteritems():\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testOutputStride8BuildAndCheckAllEndPointsUptoConv2d_13(self):\n    batch_size = 5\n    height, width = 224, 224\n    output_stride = 8\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                        normalizer_fn=slim.batch_norm):\n      _, end_points = mobilenet_v1.mobilenet_v1_base(\n          inputs, output_stride=output_stride,\n          final_endpoint=\'Conv2d_13_pointwise\')\n    endpoints_shapes = {\'Conv2d_0\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_depthwise\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_pointwise\': [batch_size, 112, 112, 64],\n                        \'Conv2d_2_depthwise\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_depthwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_4_depthwise\': [batch_size, 28, 28, 128],\n                        \'Conv2d_4_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_depthwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_6_depthwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_6_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_7_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_7_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_8_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_8_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_9_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_9_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_10_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_10_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_11_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_11_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_12_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_12_pointwise\': [batch_size, 28, 28, 1024],\n                        \'Conv2d_13_depthwise\': [batch_size, 28, 28, 1024],\n                        \'Conv2d_13_pointwise\': [batch_size, 28, 28, 1024]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name, expected_shape in endpoints_shapes.iteritems():\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testBuildAndCheckAllEndPointsApproximateFaceNet(self):\n    batch_size = 5\n    height, width = 128, 128\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                        normalizer_fn=slim.batch_norm):\n      _, end_points = mobilenet_v1.mobilenet_v1_base(\n          inputs, final_endpoint=\'Conv2d_13_pointwise\', depth_multiplier=0.75)\n    # For the Conv2d_0 layer FaceNet has depth=16\n    endpoints_shapes = {\'Conv2d_0\': [batch_size, 64, 64, 24],\n                        \'Conv2d_1_depthwise\': [batch_size, 64, 64, 24],\n                        \'Conv2d_1_pointwise\': [batch_size, 64, 64, 48],\n                        \'Conv2d_2_depthwise\': [batch_size, 32, 32, 48],\n                        \'Conv2d_2_pointwise\': [batch_size, 32, 32, 96],\n                        \'Conv2d_3_depthwise\': [batch_size, 32, 32, 96],\n                        \'Conv2d_3_pointwise\': [batch_size, 32, 32, 96],\n                        \'Conv2d_4_depthwise\': [batch_size, 16, 16, 96],\n                        \'Conv2d_4_pointwise\': [batch_size, 16, 16, 192],\n                        \'Conv2d_5_depthwise\': [batch_size, 16, 16, 192],\n                        \'Conv2d_5_pointwise\': [batch_size, 16, 16, 192],\n                        \'Conv2d_6_depthwise\': [batch_size, 8, 8, 192],\n                        \'Conv2d_6_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_7_depthwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_7_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_8_depthwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_8_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_9_depthwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_9_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_10_depthwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_10_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_11_depthwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_11_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_12_depthwise\': [batch_size, 4, 4, 384],\n                        \'Conv2d_12_pointwise\': [batch_size, 4, 4, 768],\n                        \'Conv2d_13_depthwise\': [batch_size, 4, 4, 768],\n                        \'Conv2d_13_pointwise\': [batch_size, 4, 4, 768]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name, expected_shape in endpoints_shapes.iteritems():\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 224, 224\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                        normalizer_fn=slim.batch_norm):\n      mobilenet_v1.mobilenet_v1_base(inputs)\n      total_params, _ = slim.model_analyzer.analyze_vars(\n          slim.get_model_variables())\n      self.assertAlmostEqual(3217920L, total_params)\n\n  def testBuildEndPointsWithDepthMultiplierLessThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys() if key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = mobilenet_v1.mobilenet_v1(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=0.5)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(0.5 * original_depth, new_depth)\n\n  def testBuildEndPointsWithDepthMultiplierGreaterThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = mobilenet_v1.mobilenet_v1(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=2.0)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(2.0 * original_depth, new_depth)\n\n  def testRaiseValueErrorWithInvalidDepthMultiplier(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with self.assertRaises(ValueError):\n      _ = mobilenet_v1.mobilenet_v1(\n          inputs, num_classes, depth_multiplier=-0.1)\n    with self.assertRaises(ValueError):\n      _ = mobilenet_v1.mobilenet_v1(\n          inputs, num_classes, depth_multiplier=0.0)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 112, 112\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'MobilenetV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Conv2d_13_pointwise\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 4, 4, 1024])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'MobilenetV1/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Conv2d_13_pointwise\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'MobilenetV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = mobilenet_v1.mobilenet_v1(eval_inputs, num_classes,\n                                          is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    mobilenet_v1.mobilenet_v1(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = mobilenet_v1.mobilenet_v1(eval_inputs, num_classes,\n                                          reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 224, 224, 3])\n    logits, _ = mobilenet_v1.mobilenet_v1(images,\n                                          num_classes=num_classes,\n                                          spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/nets_factory.py,1,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a factory for building various models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport functools\n\nimport tensorflow as tf\n\nfrom nets import alexnet\nfrom nets import cifarnet\nfrom nets import inception\nfrom nets import lenet\nfrom nets import mobilenet_v1\nfrom nets import overfeat\nfrom nets import resnet_v1\nfrom nets import resnet_v2\nfrom nets import vgg\n\nslim = tf.contrib.slim\n\nnetworks_map = {\'alexnet_v2\': alexnet.alexnet_v2,\n                \'cifarnet\': cifarnet.cifarnet,\n                \'overfeat\': overfeat.overfeat,\n                \'vgg_a\': vgg.vgg_a,\n                \'vgg_16\': vgg.vgg_16,\n                \'vgg_19\': vgg.vgg_19,\n                \'inception_v1\': inception.inception_v1,\n                \'inception_v2\': inception.inception_v2,\n                \'inception_v3\': inception.inception_v3,\n                \'inception_v4\': inception.inception_v4,\n                \'inception_resnet_v2\': inception.inception_resnet_v2,\n                \'lenet\': lenet.lenet,\n                \'resnet_v1_50\': resnet_v1.resnet_v1_50,\n                \'resnet_v1_101\': resnet_v1.resnet_v1_101,\n                \'resnet_v1_152\': resnet_v1.resnet_v1_152,\n                \'resnet_v1_200\': resnet_v1.resnet_v1_200,\n                \'resnet_v2_50\': resnet_v2.resnet_v2_50,\n                \'resnet_v2_101\': resnet_v2.resnet_v2_101,\n                \'resnet_v2_152\': resnet_v2.resnet_v2_152,\n                \'resnet_v2_200\': resnet_v2.resnet_v2_200,\n                \'mobilenet_v1\': mobilenet_v1.mobilenet_v1,\n               }\n\narg_scopes_map = {\'alexnet_v2\': alexnet.alexnet_v2_arg_scope,\n                  \'cifarnet\': cifarnet.cifarnet_arg_scope,\n                  \'overfeat\': overfeat.overfeat_arg_scope,\n                  \'vgg_a\': vgg.vgg_arg_scope,\n                  \'vgg_16\': vgg.vgg_arg_scope,\n                  \'vgg_19\': vgg.vgg_arg_scope,\n                  \'inception_v1\': inception.inception_v3_arg_scope,\n                  \'inception_v2\': inception.inception_v3_arg_scope,\n                  \'inception_v3\': inception.inception_v3_arg_scope,\n                  \'inception_v4\': inception.inception_v4_arg_scope,\n                  \'inception_resnet_v2\':\n                  inception.inception_resnet_v2_arg_scope,\n                  \'lenet\': lenet.lenet_arg_scope,\n                  \'resnet_v1_50\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_101\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_152\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_200\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v2_50\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_101\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_152\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_200\': resnet_v2.resnet_arg_scope,\n                  \'mobilenet_v1\': mobilenet_v1.mobilenet_v1_arg_scope,\n                 }\n\n\ndef get_network_fn(name, num_classes, weight_decay=0.0, is_training=False):\n  """"""Returns a network_fn such as `logits, end_points = network_fn(images)`.\n\n  Args:\n    name: The name of the network.\n    num_classes: The number of classes to use for classification.\n    weight_decay: The l2 coefficient for the model weights.\n    is_training: `True` if the model is being used for training and `False`\n      otherwise.\n\n  Returns:\n    network_fn: A function that applies the model to a batch of images. It has\n      the following signature:\n        logits, end_points = network_fn(images)\n  Raises:\n    ValueError: If network `name` is not recognized.\n  """"""\n  if name not in networks_map:\n    raise ValueError(\'Name of network unknown %s\' % name)\n  arg_scope = arg_scopes_map[name](weight_decay=weight_decay)\n  func = networks_map[name]\n  @functools.wraps(func)\n  def network_fn(images):\n    with slim.arg_scope(arg_scope):\n      return func(images, num_classes, is_training=is_training)\n  if hasattr(func, \'default_image_size\'):\n    network_fn.default_image_size = func.default_image_size\n\n  return network_fn\n'"
libs/networks/slim_nets/nets_factory_test.py,7,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for slim.inception.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import nets_factory\n\nslim = tf.contrib.slim\n\n\nclass NetworksTest(tf.test.TestCase):\n\n  def testGetNetworkFn(self):\n    batch_size = 5\n    num_classes = 1000\n    for net in nets_factory.networks_map:\n      with self.test_session():\n        net_fn = nets_factory.get_network_fn(net, num_classes)\n        # Most networks use 224 as their default_image_size\n        image_size = getattr(net_fn, \'default_image_size\', 224)\n        inputs = tf.random_uniform((batch_size, image_size, image_size, 3))\n        logits, end_points = net_fn(inputs)\n        self.assertTrue(isinstance(logits, tf.Tensor))\n        self.assertTrue(isinstance(end_points, dict))\n        self.assertEqual(logits.get_shape().as_list()[0], batch_size)\n        self.assertEqual(logits.get_shape().as_list()[-1], num_classes)\n\n  def testGetNetworkFnArgScope(self):\n    batch_size = 5\n    num_classes = 10\n    net = \'cifarnet\'\n    with self.test_session(use_gpu=True):\n      net_fn = nets_factory.get_network_fn(net, num_classes)\n      image_size = getattr(net_fn, \'default_image_size\', 224)\n      with slim.arg_scope([slim.model_variable, slim.variable],\n                          device=\'/CPU:0\'):\n        inputs = tf.random_uniform((batch_size, image_size, image_size, 3))\n        net_fn(inputs)\n      weights = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \'CifarNet/conv1\')[0]\n      self.assertDeviceEqual(\'/CPU:0\', weights.device)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/overfeat.py,8,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the model definition for the OverFeat network.\n\nThe definition for the network was obtained from:\n  OverFeat: Integrated Recognition, Localization and Detection using\n  Convolutional Networks\n  Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus and\n  Yann LeCun, 2014\n  http://arxiv.org/abs/1312.6229\n\nUsage:\n  with slim.arg_scope(overfeat.overfeat_arg_scope()):\n    outputs, end_points = overfeat.overfeat(inputs)\n\n@@overfeat\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef overfeat_arg_scope(weight_decay=0.0005):\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_initializer=tf.zeros_initializer()):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\'):\n      with slim.arg_scope([slim.max_pool2d], padding=\'VALID\') as arg_sc:\n        return arg_sc\n\n\ndef overfeat(inputs,\n             num_classes=1000,\n             is_training=True,\n             dropout_keep_prob=0.5,\n             spatial_squeeze=True,\n             scope=\'overfeat\'):\n  """"""Contains the model definition for the OverFeat network.\n\n  The definition for the network was obtained from:\n    OverFeat: Integrated Recognition, Localization and Detection using\n    Convolutional Networks\n    Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus and\n    Yann LeCun, 2014\n    http://arxiv.org/abs/1312.6229\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 231x231. To use in fully\n        convolutional mode, set spatial_squeeze to false.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n\n  """"""\n  with tf.variable_scope(scope, \'overfeat\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.conv2d(inputs, 64, [11, 11], 4, padding=\'VALID\',\n                        scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.conv2d(net, 256, [5, 5], padding=\'VALID\', scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.conv2d(net, 512, [3, 3], scope=\'conv3\')\n      net = slim.conv2d(net, 1024, [3, 3], scope=\'conv4\')\n      net = slim.conv2d(net, 1024, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      with slim.arg_scope([slim.conv2d],\n                          weights_initializer=trunc_normal(0.005),\n                          biases_initializer=tf.constant_initializer(0.1)):\n        # Use conv2d instead of fully_connected layers.\n        net = slim.conv2d(net, 3072, [6, 6], padding=\'VALID\', scope=\'fc6\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout6\')\n        net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout7\')\n        net = slim.conv2d(net, num_classes, [1, 1],\n                          activation_fn=None,\n                          normalizer_fn=None,\n                          biases_initializer=tf.zeros_initializer(),\n                          scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\noverfeat.default_image_size = 231\n'"
libs/networks/slim_nets/overfeat_test.py,16,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.slim_nets.overfeat.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import overfeat\n\nslim = tf.contrib.slim\n\n\nclass OverFeatTest(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'overfeat/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 281, 281\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'overfeat/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = overfeat.overfeat(inputs, num_classes)\n      expected_names = [\'overfeat/conv1\',\n                        \'overfeat/pool1\',\n                        \'overfeat/conv2\',\n                        \'overfeat/pool2\',\n                        \'overfeat/conv3\',\n                        \'overfeat/conv4\',\n                        \'overfeat/conv5\',\n                        \'overfeat/pool5\',\n                        \'overfeat/fc6\',\n                        \'overfeat/fc7\',\n                        \'overfeat/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      overfeat.overfeat(inputs, num_classes)\n      expected_names = [\'overfeat/conv1/weights\',\n                        \'overfeat/conv1/biases\',\n                        \'overfeat/conv2/weights\',\n                        \'overfeat/conv2/biases\',\n                        \'overfeat/conv3/weights\',\n                        \'overfeat/conv3/biases\',\n                        \'overfeat/conv4/weights\',\n                        \'overfeat/conv4/biases\',\n                        \'overfeat/conv5/weights\',\n                        \'overfeat/conv5/biases\',\n                        \'overfeat/fc6/weights\',\n                        \'overfeat/fc6/biases\',\n                        \'overfeat/fc7/weights\',\n                        \'overfeat/fc7/biases\',\n                        \'overfeat/fc8/weights\',\n                        \'overfeat/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 231, 231\n    eval_height, eval_width = 281, 281\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = overfeat.overfeat(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = overfeat.overfeat(eval_inputs, is_training=False,\n                                    spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 231, 231\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/resnet_utils.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains building blocks for various versions of Residual Networks.\n\nResidual networks (ResNets) were proposed in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Deep Residual Learning for Image Recognition. arXiv:1512.03385, 2015\n\nMore variants were introduced in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Identity Mappings in Deep Residual Networks. arXiv: 1603.05027, 2016\n\nWe can obtain different ResNet variants by changing the network depth, width,\nand form of residual unit. This module implements the infrastructure for\nbuilding them. Concrete ResNet units and full ResNet networks are implemented in\nthe accompanying resnet_v1.py and resnet_v2.py modules.\n\nCompared to https://github.com/KaimingHe/deep-residual-networks, in the current\nimplementation we subsample the output activations in the last residual unit of\neach block, instead of subsampling the input activations in the first residual\nunit of each block. The two implementations give identical results but our\nimplementation is more memory efficient.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\nclass Block(collections.namedtuple(\'Block\', [\'scope\', \'unit_fn\', \'args\'])):\n  """"""A named tuple describing a ResNet block.\n\n  Its parts are:\n    scope: The scope of the `Block`.\n    unit_fn: The ResNet unit function which takes as input a `Tensor` and\n      returns another `Tensor` with the output of the ResNet unit.\n    args: A list of length equal to the number of units in the `Block`. The list\n      contains one (depth, depth_bottleneck, stride) tuple for each unit in the\n      block to serve as argument to unit_fn.\n  """"""\n\n\ndef subsample(inputs, factor, scope=None):\n  """"""Subsamples the input along the spatial dimensions.\n\n  Args:\n    inputs: A `Tensor` of size [batch, height_in, width_in, channels].\n    factor: The subsampling factor.\n    scope: Optional variable_scope.\n\n  Returns:\n    output: A `Tensor` of size [batch, height_out, width_out, channels] with the\n      input, either intact (if factor == 1) or subsampled (if factor > 1).\n  """"""\n  if factor == 1:\n    return inputs\n  else:\n    return slim.max_pool2d(inputs, [1, 1], stride=factor, scope=scope)\n\n\ndef conv2d_same(inputs, num_outputs, kernel_size, stride, rate=1, scope=None):\n  """"""Strided 2-D convolution with \'SAME\' padding.\n\n  When stride > 1, then we do explicit zero-padding, followed by conv2d with\n  \'VALID\' padding.\n\n  Note that\n\n     net = conv2d_same(inputs, num_outputs, 3, stride=stride)\n\n  is equivalent to\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=1, padding=\'SAME\')\n     net = subsample(net, factor=stride)\n\n  whereas\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=stride, padding=\'SAME\')\n\n  is different when the input\'s height or width is even, which is why we add the\n  current function. For more details, see ResnetUtilsTest.testConv2DSameEven().\n\n  Args:\n    inputs: A 4-D tensor of size [batch, height_in, width_in, channels].\n    num_outputs: An integer, the number of output filters.\n    kernel_size: An int with the kernel_size of the filters.\n    stride: An integer, the output stride.\n    rate: An integer, rate for atrous convolution.\n    scope: Scope.\n\n  Returns:\n    output: A 4-D tensor of size [batch, height_out, width_out, channels] with\n      the convolution output.\n  """"""\n  if stride == 1:\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=1, rate=rate,\n                       padding=\'SAME\', scope=scope)\n  else:\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    inputs = tf.pad(inputs,\n                    [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=stride,\n                       rate=rate, padding=\'VALID\', scope=scope)\n\n\n@slim.add_arg_scope\ndef stack_blocks_dense(net, blocks, output_stride=None,\n                       outputs_collections=None):\n  """"""Stacks ResNet `Blocks` and controls output feature density.\n\n  First, this function creates scopes for the ResNet in the form of\n  \'block_name/unit_1\', \'block_name/unit_2\', etc.\n\n  Second, this function allows the user to explicitly control the ResNet\n  output_stride, which is the ratio of the input to output spatial resolution.\n  This is useful for dense prediction tasks such as semantic segmentation or\n  object detection.\n\n  Most ResNets consist of 4 ResNet blocks and subsample the activations by a\n  factor of 2 when transitioning between consecutive ResNet blocks. This results\n  to a nominal ResNet output_stride equal to 8. If we set the output_stride to\n  half the nominal network stride (e.g., output_stride=4), then we compute\n  responses twice.\n\n  Control of the output feature density is implemented by atrous convolution.\n\n  Args:\n    net: A `Tensor` of size [batch, height, width, channels].\n    blocks: A list of length equal to the number of ResNet `Blocks`. Each\n      element is a ResNet `Block` object describing the units in the `Block`.\n    output_stride: If `None`, then the output will be computed at the nominal\n      network stride. If output_stride is not `None`, it specifies the requested\n      ratio of input to output spatial resolution, which needs to be equal to\n      the product of unit strides from the start up to some level of the ResNet.\n      For example, if the ResNet employs units with strides 1, 2, 1, 3, 4, 1,\n      then valid values for the output_stride are 1, 2, 6, 24 or None (which\n      is equivalent to output_stride=24).\n    outputs_collections: Collection to add the ResNet block outputs.\n\n  Returns:\n    net: Output tensor with stride equal to the specified output_stride.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  # The current_stride variable keeps track of the effective stride of the\n  # activations. This allows us to invoke atrous convolution whenever applying\n  # the next residual unit would result in the activations having stride larger\n  # than the target output_stride.\n  current_stride = 1\n\n  # The atrous convolution rate parameter.\n  rate = 1\n\n  for block in blocks:\n    with tf.variable_scope(block.scope, \'block\', [net]) as sc:\n      for i, unit in enumerate(block.args):\n        if output_stride is not None and current_stride > output_stride:\n          raise ValueError(\'The target output_stride cannot be reached.\')\n\n        with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n          # If we have reached the target output_stride, then we need to employ\n          # atrous convolution with stride=1 and multiply the atrous rate by the\n          # current unit\'s stride for use in subsequent layers.\n          if output_stride is not None and current_stride == output_stride:\n            net = block.unit_fn(net, rate=rate, **dict(unit, stride=1))\n            rate *= unit.get(\'stride\', 1)\n\n          else:\n            net = block.unit_fn(net, rate=1, **unit)\n            current_stride *= unit.get(\'stride\', 1)\n      net = slim.utils.collect_named_outputs(outputs_collections, sc.name, net)\n\n  if output_stride is not None and current_stride != output_stride:\n    raise ValueError(\'The target output_stride cannot be reached.\')\n\n  return net\n\n\ndef resnet_arg_scope(weight_decay=0.0001,\n                     batch_norm_decay=0.997, #0.997\n                     batch_norm_epsilon=1e-5,\n                     batch_norm_scale=True):\n  """"""Defines the default ResNet arg scope.\n\n  TODO(gpapan): The batch-normalization related default values above are\n    appropriate for use in conjunction with the reference ResNet models\n    released at https://github.com/KaimingHe/deep-residual-networks. When\n    training ResNets from scratch, they might need to be tuned.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    batch_norm_decay: The moving average decay when estimating layer activation\n      statistics in batch normalization.\n    batch_norm_epsilon: Small constant to prevent division by zero when\n      normalizing activations by their variance in batch normalization.\n    batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the\n      activations in the batch normalization layer.\n\n  Returns:\n    An `arg_scope` to use for the resnet models.\n  """"""\n  batch_norm_params = {\n      \'decay\': batch_norm_decay,\n      \'epsilon\': batch_norm_epsilon,\n      \'scale\': batch_norm_scale,\n      \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n  }\n\n  with slim.arg_scope(\n      [slim.conv2d],\n      weights_regularizer=slim.l2_regularizer(weight_decay),\n      weights_initializer=slim.variance_scaling_initializer(),\n      activation_fn=tf.nn.relu,\n      normalizer_fn=slim.batch_norm,\n      normalizer_params=batch_norm_params):\n    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n      # The following implies padding=\'SAME\' for pool1, which makes feature\n      # alignment easier for dense prediction tasks. This is also used in\n      # https://github.com/facebook/fb.resnet.torch. However the accompanying\n      # code of \'Deep Residual Learning for Image Recognition\' uses\n      # padding=\'VALID\' for pool1. You can switch to that choice by setting\n      # slim.arg_scope([slim.max_pool2d], padding=\'VALID\').\n      with slim.arg_scope([slim.max_pool2d], padding=\'SAME\') as arg_sc:\n        return arg_sc\n'"
libs/networks/slim_nets/resnet_v1.py,7,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the original form of Residual Networks.\n\nThe \'v1\' residual networks (ResNets) implemented in this module were proposed\nby:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nOther variants were introduced in:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe networks defined in this module utilize the bottleneck building block of\n[1] with projection shortcuts only for increasing depths. They employ batch\nnormalization *after* every weight layer. This is the architecture used by\nMSRA in the Imagenet and MSCOCO 2016 competition models ResNet-101 and\nResNet-152. See [2; Fig. 1a] for a comparison between the current \'v1\'\narchitecture and the alternative \'v2\' architecture of [2] which uses batch\nnormalization *before* every weight layer in the so-called full pre-activation\nunits.\n\nTypical use:\n\n   from tensorflow.contrib.slim.slim_nets import resnet_v1\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom libs.networks.slim_nets import resnet_utils\n\n\nresnet_arg_scope = resnet_utils.resnet_arg_scope\nslim = tf.contrib.slim\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n  """"""Bottleneck residual unit variant with BN after convolutions.\n\n  This is the original residual unit proposed in [1]. See Fig. 1(a) of [2] for\n  its definition. Note that we use here the bottleneck variant which has an\n  extra bottleneck layer.\n\n  When putting together two consecutive ResNet blocks that use this unit, one\n  should use stride = 2 in the last unit of the first block.\n\n  Args:\n    inputs: A tensor of size [batch, height, width, channels].\n    depth: The depth of the ResNet unit output.\n    depth_bottleneck: The depth of the bottleneck layers.\n    stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n      the units output compared to its input.\n    rate: An integer, rate for atrous convolution.\n    outputs_collections: Collection to add the ResNet unit output.\n    scope: Optional variable_scope.\n\n  Returns:\n    The ResNet unit\'s output.\n  """"""\n  with tf.variable_scope(scope, \'bottleneck_v1\', [inputs]) as sc:\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    if depth == depth_in:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut = slim.conv2d(inputs, depth, [1, 1], stride=stride,\n                             activation_fn=None, scope=\'shortcut\')\n\n    residual = slim.conv2d(inputs, depth_bottleneck, [1, 1], stride=1,\n                           scope=\'conv1\')\n    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                        rate=rate, scope=\'conv2\')\n    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                           activation_fn=None, scope=\'conv3\')\n\n    output = tf.nn.relu(shortcut + residual)\n\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.original_name_scope,\n                                            output)\n\n\ndef resnet_v1(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=False,\n              reuse=None,\n              scope=None):\n  """"""Generator for v1 ResNet models.\n\n  This function generates a family of ResNet v1 models. See the resnet_v1_*()\n  methods for specific model instantiations, obtained by selecting different\n  block instantiations that produce ResNets of various depths.\n\n  Training for image classification on Imagenet is usually done with [224, 224]\n  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n  block for the ResNets defined in [1] that have nominal stride equal to 32.\n  However, for dense prediction tasks we advise that one uses inputs with\n  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n  this case the feature maps at the ResNet output will have spatial shape\n  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n  and corners exactly aligned with the input image corners, which greatly\n  facilitates alignment of the features to the image. Using as input [225, 225]\n  images results in [8, 8] feature maps at the output of the last ResNet block.\n\n  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n  have nominal stride equal to 32 and a good choice in FCN mode is to use\n  output_stride=16 in order to increase the density of the computed features at\n  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    blocks: A list of length equal to the number of ResNet blocks. Each element\n      is a resnet_utils.Block object describing the units in the block.\n    num_classes: Number of predicted classes for classification tasks. If None\n      we return the features before the logit layer.\n    is_training: whether is training or not.\n    global_pool: If True, we perform global average pooling before computing the\n      logits. Set to True for image classification, False for dense prediction.\n    output_stride: If None, then the output will be computed at the nominal\n      network stride. If output_stride is not None, it specifies the requested\n      ratio of input to output spatial resolution.\n    include_root_block: If True, include the initial convolution followed by\n      max-pooling, if False excludes it.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n      If global_pool is False, then height_out and width_out are reduced by a\n      factor of output_stride compared to the respective height_in and width_in,\n      else both height_out and width_out equal one. If num_classes is None, then\n      net is the output of the last ResNet block, potentially after global\n      average pooling. If num_classes is not None, net contains the pre-softmax\n      activations.\n    end_points: A dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  with tf.variable_scope(scope, \'resnet_v1\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    with slim.arg_scope([slim.conv2d, bottleneck,\n                         resnet_utils.stack_blocks_dense],\n                        outputs_collections=end_points_collection):\n      with slim.arg_scope([slim.batch_norm], is_training=is_training):\n        net = inputs\n        if include_root_block:\n          if output_stride is not None:\n            if output_stride % 4 != 0:\n              raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n            output_stride /= 4\n          net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n          net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n        if global_pool:\n          # Global average pooling.\n          net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n          # yjr_feature = tf.squeeze(net, [0, 1, 2])\n        if num_classes is not None:\n          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                            normalizer_fn=None, scope=\'logits\')\n        if spatial_squeeze:\n          logits = tf.squeeze(net, [1, 2], name=\'SpatialSqueeze\')\n        else:\n          logits = net\n        # Convert end_points_collection into a dictionary of end_points.\n        end_points = slim.utils.convert_collection_to_dict(\n            end_points_collection)\n        if num_classes is not None:\n          end_points[\'predictions\'] = slim.softmax(logits, scope=\'predictions\')\n\n        ###\n        # end_points[\'yjr_feature\'] = yjr_feature\n        return logits, end_points\nresnet_v1.default_image_size = 224\n\n\ndef resnet_v1_block(scope, base_depth, num_units, stride):\n  """"""Helper function for creating a resnet_v1 bottleneck block.\n\n  Args:\n    scope: The scope of the block.\n    base_depth: The depth of the bottleneck layer for each unit.\n    num_units: The number of units in the block.\n    stride: The stride of the block, implemented as a stride in the last unit.\n      All other units have stride=1.\n\n  Returns:\n    A resnet_v1 bottleneck block.\n  """"""\n  return resnet_utils.Block(scope, bottleneck, [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': 1\n  }] * (num_units - 1) + [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': stride\n  }])\n\n\ndef resnet_v1_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'resnet_v1_50\'):\n  """"""ResNet-50 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v1_block(\'block2\', base_depth=128, num_units=4, stride=2),\n      resnet_v1_block(\'block3\', base_depth=256, num_units=6, stride=2),\n      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v1_50.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_101\'):\n  """"""ResNet-101 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v1_block(\'block2\', base_depth=128, num_units=4, stride=2),\n      resnet_v1_block(\'block3\', base_depth=256, num_units=23, stride=2),\n      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v1_101.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_152\'):\n  """"""ResNet-152 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v1_block(\'block2\', base_depth=128, num_units=8, stride=2),\n      resnet_v1_block(\'block3\', base_depth=256, num_units=36, stride=2),\n      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v1_152.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_200\'):\n  """"""ResNet-200 model of [2]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v1_block(\'block2\', base_depth=128, num_units=24, stride=2),\n      resnet_v1_block(\'block3\', base_depth=256, num_units=36, stride=2),\n      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v1_200.default_image_size = resnet_v1.default_image_size\n'"
libs/networks/slim_nets/resnet_v1_test.py,44,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.slim_nets.resnet_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import resnet_utils\nfrom nets import resnet_v1\n\nslim = tf.contrib.slim\n\n\ndef create_test_input(batch_size, height, width, channels):\n  """"""Create test input tensor.\n\n  Args:\n    batch_size: The number of images per batch or `None` if unknown.\n    height: The height of each image or `None` if unknown.\n    width: The width of each image or `None` if unknown.\n    channels: The number of channels per image or `None` if unknown.\n\n  Returns:\n    Either a placeholder `Tensor` of dimension\n      [batch_size, height, width, channels] if any of the inputs are `None` or a\n    constant `Tensor` with the mesh grid values along the spatial dimensions.\n  """"""\n  if None in [batch_size, height, width, channels]:\n    return tf.placeholder(tf.float32, (batch_size, height, width, channels))\n  else:\n    return tf.to_float(\n        np.tile(\n            np.reshape(\n                np.reshape(np.arange(height), [height, 1]) +\n                np.reshape(np.arange(width), [1, width]),\n                [1, height, width, 1]),\n            [batch_size, 1, 1, channels]))\n\n\nclass ResnetUtilsTest(tf.test.TestCase):\n\n  def testSubsampleThreeByThree(self):\n    x = tf.reshape(tf.to_float(tf.range(9)), [1, 3, 3, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 6, 8]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testSubsampleFourByFour(self):\n    x = tf.reshape(tf.to_float(tf.range(16)), [1, 4, 4, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 8, 10]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testConv2DSameEven(self):\n    n, n2 = 4, 2\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 26],\n                               [28, 48, 66, 37],\n                               [43, 66, 84, 46],\n                               [26, 37, 46, 22]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43],\n                               [43, 84]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = tf.to_float([[48, 37],\n                               [37, 22]])\n    y4_expected = tf.reshape(y4_expected, [1, n2, n2, 1])\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def testConv2DSameOdd(self):\n    n, n2 = 5, 3\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 58, 34],\n                               [28, 48, 66, 84, 46],\n                               [43, 66, 84, 102, 55],\n                               [58, 84, 102, 120, 64],\n                               [34, 46, 55, 64, 30]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43, 34],\n                               [43, 84, 55],\n                               [34, 55, 30]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = y2_expected\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def _resnet_plain(self, inputs, blocks, output_stride=None, scope=None):\n    """"""A plain ResNet without extra layers before or after the ResNet blocks.""""""\n    with tf.variable_scope(scope, values=[inputs]):\n      with slim.arg_scope([slim.conv2d], outputs_collections=\'end_points\'):\n        net = resnet_utils.stack_blocks_dense(inputs, blocks, output_stride)\n        end_points = slim.utils.convert_collection_to_dict(\'end_points\')\n        return net, end_points\n\n  def testEndPointsV1(self):\n    """"""Test the end points of a tiny v1 bottleneck network.""""""\n    blocks = [\n        resnet_v1.resnet_v1_block(\n            \'block1\', base_depth=1, num_units=2, stride=2),\n        resnet_v1.resnet_v1_block(\n            \'block2\', base_depth=2, num_units=2, stride=1),\n    ]\n    inputs = create_test_input(2, 32, 16, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_plain(inputs, blocks, scope=\'tiny\')\n    expected = [\n        \'tiny/block1/unit_1/bottleneck_v1/shortcut\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv1\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv2\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv3\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv1\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv2\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv3\',\n        \'tiny/block2/unit_1/bottleneck_v1/shortcut\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv1\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv2\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv3\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv1\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv2\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv3\']\n    self.assertItemsEqual(expected, end_points)\n\n  def _stack_blocks_nondense(self, net, blocks):\n    """"""A simplified ResNet Block stacker without output stride control.""""""\n    for block in blocks:\n      with tf.variable_scope(block.scope, \'block\', [net]):\n        for i, unit in enumerate(block.args):\n          with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n            net = block.unit_fn(net, rate=1, **unit)\n    return net\n\n  def testAtrousValuesBottleneck(self):\n    """"""Verify the values of dense feature extraction by atrous convolution.\n\n    Make sure that dense feature extraction by stack_blocks_dense() followed by\n    subsampling gives identical results to feature extraction at the nominal\n    network output stride using the simple self._stack_blocks_nondense() above.\n    """"""\n    block = resnet_v1.resnet_v1_block\n    blocks = [\n        block(\'block1\', base_depth=1, num_units=2, stride=2),\n        block(\'block2\', base_depth=2, num_units=2, stride=2),\n        block(\'block3\', base_depth=4, num_units=2, stride=2),\n        block(\'block4\', base_depth=8, num_units=2, stride=1),\n    ]\n    nominal_stride = 8\n\n    # Test both odd and even input dimensions.\n    height = 30\n    width = 31\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      with slim.arg_scope([slim.batch_norm], is_training=False):\n        for output_stride in [1, 2, 4, 8, None]:\n          with tf.Graph().as_default():\n            with self.test_session() as sess:\n              tf.set_random_seed(0)\n              inputs = create_test_input(1, height, width, 3)\n              # Dense feature extraction followed by subsampling.\n              output = resnet_utils.stack_blocks_dense(inputs,\n                                                       blocks,\n                                                       output_stride)\n              if output_stride is None:\n                factor = 1\n              else:\n                factor = nominal_stride // output_stride\n\n              output = resnet_utils.subsample(output, factor)\n              # Make the two networks use the same weights.\n              tf.get_variable_scope().reuse_variables()\n              # Feature extraction at the nominal network rate.\n              expected = self._stack_blocks_nondense(inputs, blocks)\n              sess.run(tf.global_variables_initializer())\n              output, expected = sess.run([output, expected])\n              self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)\n\n\nclass ResnetCompleteNetworkTest(tf.test.TestCase):\n  """"""Tests with complete small ResNet v1 networks.""""""\n\n  def _resnet_small(self,\n                    inputs,\n                    num_classes=None,\n                    is_training=True,\n                    global_pool=True,\n                    output_stride=None,\n                    include_root_block=True,\n                    reuse=None,\n                    scope=\'resnet_v1_small\'):\n    """"""A shallow and thin ResNet v1 for faster tests.""""""\n    block = resnet_v1.resnet_v1_block\n    blocks = [\n        block(\'block1\', base_depth=1, num_units=3, stride=2),\n        block(\'block2\', base_depth=2, num_units=3, stride=2),\n        block(\'block3\', base_depth=4, num_units=3, stride=2),\n        block(\'block4\', base_depth=8, num_units=2, stride=1),\n    ]\n    return resnet_v1.resnet_v1(inputs, blocks, num_classes,\n                               is_training=is_training,\n                               global_pool=global_pool,\n                               output_stride=output_stride,\n                               include_root_block=include_root_block,\n                               reuse=reuse,\n                               scope=scope)\n\n  def testClassificationEndPoints(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, end_points = self._resnet_small(inputs, num_classes,\n                                              global_pool=global_pool,\n                                              scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(), [2, 1, 1, num_classes])\n    self.assertTrue(\'predictions\' in end_points)\n    self.assertListEqual(end_points[\'predictions\'].get_shape().as_list(),\n                         [2, 1, 1, num_classes])\n\n  def testClassificationShapes(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 28, 28, 4],\n          \'resnet/block2\': [2, 14, 14, 8],\n          \'resnet/block3\': [2, 7, 7, 16],\n          \'resnet/block4\': [2, 7, 7, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 21, 21, 8],\n          \'resnet/block3\': [2, 11, 11, 16],\n          \'resnet/block4\': [2, 11, 11, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testRootlessFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 128, 128, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         include_root_block=False,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 64, 64, 4],\n          \'resnet/block2\': [2, 32, 32, 8],\n          \'resnet/block3\': [2, 16, 16, 16],\n          \'resnet/block4\': [2, 16, 16, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    output_stride = 8\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs,\n                                         num_classes,\n                                         global_pool=global_pool,\n                                         output_stride=output_stride,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 41, 41, 8],\n          \'resnet/block3\': [2, 41, 41, 16],\n          \'resnet/block4\': [2, 41, 41, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalValues(self):\n    """"""Verify dense feature extraction with atrous convolution.""""""\n    nominal_stride = 32\n    for output_stride in [4, 8, 16, 32, None]:\n      with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n        with tf.Graph().as_default():\n          with self.test_session() as sess:\n            tf.set_random_seed(0)\n            inputs = create_test_input(2, 81, 81, 3)\n            # Dense feature extraction followed by subsampling.\n            output, _ = self._resnet_small(inputs, None, is_training=False,\n                                           global_pool=False,\n                                           output_stride=output_stride)\n            if output_stride is None:\n              factor = 1\n            else:\n              factor = nominal_stride // output_stride\n            output = resnet_utils.subsample(output, factor)\n            # Make the two networks use the same weights.\n            tf.get_variable_scope().reuse_variables()\n            # Feature extraction at the nominal network rate.\n            expected, _ = self._resnet_small(inputs, None, is_training=False,\n                                             global_pool=False)\n            sess.run(tf.global_variables_initializer())\n            self.assertAllClose(output.eval(), expected.eval(),\n                                atol=1e-4, rtol=1e-4)\n\n  def testUnknownBatchSize(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(None, height, width, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, _ = self._resnet_small(inputs, num_classes,\n                                     global_pool=global_pool,\n                                     scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, 1, 1, num_classes])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 1, 1, num_classes))\n\n  def testFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs, None, global_pool=global_pool)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 3, 3, 32))\n\n  def testAtrousFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    output_stride = 8\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs,\n                                     None,\n                                     global_pool=global_pool,\n                                     output_stride=output_stride)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 9, 9, 32))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/resnet_v2.py,7,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the preactivation form of Residual Networks.\n\nResidual networks (ResNets) were originally proposed in:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nThe full preactivation \'v2\' ResNet variant implemented in this module was\nintroduced by:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe key difference of the full preactivation \'v2\' variant compared to the\n\'v1\' variant in [1] is the use of batch normalization before every weight layer.\n\nTypical use:\n\n   from tensorflow.contrib.slim.slim_nets import resnet_v2\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n      net, end_points = resnet_v2.resnet_v2_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v2.resnet_arg_scope(is_training)):\n      net, end_points = resnet_v2.resnet_v2_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import resnet_utils\n\nslim = tf.contrib.slim\nresnet_arg_scope = resnet_utils.resnet_arg_scope\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n  """"""Bottleneck residual unit variant with BN before convolutions.\n\n  This is the full preactivation residual unit variant proposed in [2]. See\n  Fig. 1(b) of [2] for its definition. Note that we use here the bottleneck\n  variant which has an extra bottleneck layer.\n\n  When putting together two consecutive ResNet blocks that use this unit, one\n  should use stride = 2 in the last unit of the first block.\n\n  Args:\n    inputs: A tensor of size [batch, height, width, channels].\n    depth: The depth of the ResNet unit output.\n    depth_bottleneck: The depth of the bottleneck layers.\n    stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n      the units output compared to its input.\n    rate: An integer, rate for atrous convolution.\n    outputs_collections: Collection to add the ResNet unit output.\n    scope: Optional variable_scope.\n\n  Returns:\n    The ResNet unit\'s output.\n  """"""\n  with tf.variable_scope(scope, \'bottleneck_v2\', [inputs]) as sc:\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    preact = slim.batch_norm(inputs, activation_fn=tf.nn.relu, scope=\'preact\')\n    if depth == depth_in:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut = slim.conv2d(preact, depth, [1, 1], stride=stride,\n                             normalizer_fn=None, activation_fn=None,\n                             scope=\'shortcut\')\n\n    residual = slim.conv2d(preact, depth_bottleneck, [1, 1], stride=1,\n                           scope=\'conv1\')\n    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                        rate=rate, scope=\'conv2\')\n    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                           normalizer_fn=None, activation_fn=None,\n                           scope=\'conv3\')\n\n    output = shortcut + residual\n\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.original_name_scope,\n                                            output)\n\n\ndef resnet_v2(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=False,\n              reuse=None,\n              scope=None):\n  """"""Generator for v2 (preactivation) ResNet models.\n\n  This function generates a family of ResNet v2 models. See the resnet_v2_*()\n  methods for specific model instantiations, obtained by selecting different\n  block instantiations that produce ResNets of various depths.\n\n  Training for image classification on Imagenet is usually done with [224, 224]\n  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n  block for the ResNets defined in [1] that have nominal stride equal to 32.\n  However, for dense prediction tasks we advise that one uses inputs with\n  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n  this case the feature maps at the ResNet output will have spatial shape\n  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n  and corners exactly aligned with the input image corners, which greatly\n  facilitates alignment of the features to the image. Using as input [225, 225]\n  images results in [8, 8] feature maps at the output of the last ResNet block.\n\n  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n  have nominal stride equal to 32 and a good choice in FCN mode is to use\n  output_stride=16 in order to increase the density of the computed features at\n  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    blocks: A list of length equal to the number of ResNet blocks. Each element\n      is a resnet_utils.Block object describing the units in the block.\n    num_classes: Number of predicted classes for classification tasks. If None\n      we return the features before the logit layer.\n    is_training: whether is training or not.\n    global_pool: If True, we perform global average pooling before computing the\n      logits. Set to True for image classification, False for dense prediction.\n    output_stride: If None, then the output will be computed at the nominal\n      network stride. If output_stride is not None, it specifies the requested\n      ratio of input to output spatial resolution.\n    include_root_block: If True, include the initial convolution followed by\n      max-pooling, if False excludes it. If excluded, `inputs` should be the\n      results of an activation-less convolution.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n\n  Returns:\n    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n      If global_pool is False, then height_out and width_out are reduced by a\n      factor of output_stride compared to the respective height_in and width_in,\n      else both height_out and width_out equal one. If num_classes is None, then\n      net is the output of the last ResNet block, potentially after global\n      average pooling. If num_classes is not None, net contains the pre-softmax\n      activations.\n    end_points: A dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  with tf.variable_scope(scope, \'resnet_v2\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    with slim.arg_scope([slim.conv2d, bottleneck,\n                         resnet_utils.stack_blocks_dense],\n                        outputs_collections=end_points_collection):\n      with slim.arg_scope([slim.batch_norm], is_training=is_training):\n        net = inputs\n        if include_root_block:\n          if output_stride is not None:\n            if output_stride % 4 != 0:\n              raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n            output_stride /= 4\n          # We do not include batch normalization or activation functions in\n          # conv1 because the first ResNet unit will perform these. Cf.\n          # Appendix of [2].\n          with slim.arg_scope([slim.conv2d],\n                              activation_fn=None, normalizer_fn=None):\n            net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n          net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n        # This is needed because the pre-activation variant does not have batch\n        # normalization or activation functions in the residual unit output. See\n        # Appendix of [2].\n        net = slim.batch_norm(net, activation_fn=tf.nn.relu, scope=\'postnorm\')\n        if global_pool:\n          # Global average pooling.\n          net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n        if num_classes is not None:\n          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                            normalizer_fn=None, scope=\'logits\')\n        if spatial_squeeze:\n          logits = tf.squeeze(net, [1, 2], name=\'SpatialSqueeze\')\n        else:\n          logits = net\n        # Convert end_points_collection into a dictionary of end_points.\n        end_points = slim.utils.convert_collection_to_dict(\n            end_points_collection)\n        if num_classes is not None:\n          end_points[\'predictions\'] = slim.softmax(logits, scope=\'predictions\')\n        return logits, end_points\nresnet_v2.default_image_size = 224\n\n\ndef resnet_v2_block(scope, base_depth, num_units, stride):\n  """"""Helper function for creating a resnet_v2 bottleneck block.\n\n  Args:\n    scope: The scope of the block.\n    base_depth: The depth of the bottleneck layer for each unit.\n    num_units: The number of units in the block.\n    stride: The stride of the block, implemented as a stride in the last unit.\n      All other units have stride=1.\n\n  Returns:\n    A resnet_v2 bottleneck block.\n  """"""\n  return resnet_utils.Block(scope, bottleneck, [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': 1\n  }] * (num_units - 1) + [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': stride\n  }])\nresnet_v2.default_image_size = 224\n\n\ndef resnet_v2_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 spatial_squeeze=False,\n                 reuse=None,\n                 scope=\'resnet_v2_50\'):\n  """"""ResNet-50 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=4, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=6, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_50.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=False,\n                  reuse=None,\n                  scope=\'resnet_v2_101\'):\n  """"""ResNet-101 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=4, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=23, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_101.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=False,\n                  reuse=None,\n                  scope=\'resnet_v2_152\'):\n  """"""ResNet-152 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=8, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=36, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_152.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=False,\n                  reuse=None,\n                  scope=\'resnet_v2_200\'):\n  """"""ResNet-200 model of [2]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=24, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=36, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_200.default_image_size = resnet_v2.default_image_size\n'"
libs/networks/slim_nets/resnet_v2_test.py,44,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.slim_nets.resnet_v2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import resnet_utils\nfrom nets import resnet_v2\n\nslim = tf.contrib.slim\n\n\ndef create_test_input(batch_size, height, width, channels):\n  """"""Create test input tensor.\n\n  Args:\n    batch_size: The number of images per batch or `None` if unknown.\n    height: The height of each image or `None` if unknown.\n    width: The width of each image or `None` if unknown.\n    channels: The number of channels per image or `None` if unknown.\n\n  Returns:\n    Either a placeholder `Tensor` of dimension\n      [batch_size, height, width, channels] if any of the inputs are `None` or a\n    constant `Tensor` with the mesh grid values along the spatial dimensions.\n  """"""\n  if None in [batch_size, height, width, channels]:\n    return tf.placeholder(tf.float32, (batch_size, height, width, channels))\n  else:\n    return tf.to_float(\n        np.tile(\n            np.reshape(\n                np.reshape(np.arange(height), [height, 1]) +\n                np.reshape(np.arange(width), [1, width]),\n                [1, height, width, 1]),\n            [batch_size, 1, 1, channels]))\n\n\nclass ResnetUtilsTest(tf.test.TestCase):\n\n  def testSubsampleThreeByThree(self):\n    x = tf.reshape(tf.to_float(tf.range(9)), [1, 3, 3, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 6, 8]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testSubsampleFourByFour(self):\n    x = tf.reshape(tf.to_float(tf.range(16)), [1, 4, 4, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 8, 10]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testConv2DSameEven(self):\n    n, n2 = 4, 2\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 26],\n                               [28, 48, 66, 37],\n                               [43, 66, 84, 46],\n                               [26, 37, 46, 22]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43],\n                               [43, 84]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = tf.to_float([[48, 37],\n                               [37, 22]])\n    y4_expected = tf.reshape(y4_expected, [1, n2, n2, 1])\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def testConv2DSameOdd(self):\n    n, n2 = 5, 3\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 58, 34],\n                               [28, 48, 66, 84, 46],\n                               [43, 66, 84, 102, 55],\n                               [58, 84, 102, 120, 64],\n                               [34, 46, 55, 64, 30]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43, 34],\n                               [43, 84, 55],\n                               [34, 55, 30]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = y2_expected\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def _resnet_plain(self, inputs, blocks, output_stride=None, scope=None):\n    """"""A plain ResNet without extra layers before or after the ResNet blocks.""""""\n    with tf.variable_scope(scope, values=[inputs]):\n      with slim.arg_scope([slim.conv2d], outputs_collections=\'end_points\'):\n        net = resnet_utils.stack_blocks_dense(inputs, blocks, output_stride)\n        end_points = slim.utils.convert_collection_to_dict(\'end_points\')\n        return net, end_points\n\n  def testEndPointsV2(self):\n    """"""Test the end points of a tiny v2 bottleneck network.""""""\n    blocks = [\n        resnet_v2.resnet_v2_block(\n            \'block1\', base_depth=1, num_units=2, stride=2),\n        resnet_v2.resnet_v2_block(\n            \'block2\', base_depth=2, num_units=2, stride=1),\n    ]\n    inputs = create_test_input(2, 32, 16, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_plain(inputs, blocks, scope=\'tiny\')\n    expected = [\n        \'tiny/block1/unit_1/bottleneck_v2/shortcut\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv1\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv2\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv3\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv1\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv2\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv3\',\n        \'tiny/block2/unit_1/bottleneck_v2/shortcut\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv1\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv2\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv3\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv1\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv2\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv3\']\n    self.assertItemsEqual(expected, end_points)\n\n  def _stack_blocks_nondense(self, net, blocks):\n    """"""A simplified ResNet Block stacker without output stride control.""""""\n    for block in blocks:\n      with tf.variable_scope(block.scope, \'block\', [net]):\n        for i, unit in enumerate(block.args):\n          with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n            net = block.unit_fn(net, rate=1, **unit)\n    return net\n\n  def testAtrousValuesBottleneck(self):\n    """"""Verify the values of dense feature extraction by atrous convolution.\n\n    Make sure that dense feature extraction by stack_blocks_dense() followed by\n    subsampling gives identical results to feature extraction at the nominal\n    network output stride using the simple self._stack_blocks_nondense() above.\n    """"""\n    block = resnet_v2.resnet_v2_block\n    blocks = [\n        block(\'block1\', base_depth=1, num_units=2, stride=2),\n        block(\'block2\', base_depth=2, num_units=2, stride=2),\n        block(\'block3\', base_depth=4, num_units=2, stride=2),\n        block(\'block4\', base_depth=8, num_units=2, stride=1),\n    ]\n    nominal_stride = 8\n\n    # Test both odd and even input dimensions.\n    height = 30\n    width = 31\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      with slim.arg_scope([slim.batch_norm], is_training=False):\n        for output_stride in [1, 2, 4, 8, None]:\n          with tf.Graph().as_default():\n            with self.test_session() as sess:\n              tf.set_random_seed(0)\n              inputs = create_test_input(1, height, width, 3)\n              # Dense feature extraction followed by subsampling.\n              output = resnet_utils.stack_blocks_dense(inputs,\n                                                       blocks,\n                                                       output_stride)\n              if output_stride is None:\n                factor = 1\n              else:\n                factor = nominal_stride // output_stride\n\n              output = resnet_utils.subsample(output, factor)\n              # Make the two networks use the same weights.\n              tf.get_variable_scope().reuse_variables()\n              # Feature extraction at the nominal network rate.\n              expected = self._stack_blocks_nondense(inputs, blocks)\n              sess.run(tf.global_variables_initializer())\n              output, expected = sess.run([output, expected])\n              self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)\n\n\nclass ResnetCompleteNetworkTest(tf.test.TestCase):\n  """"""Tests with complete small ResNet v2 networks.""""""\n\n  def _resnet_small(self,\n                    inputs,\n                    num_classes=None,\n                    is_training=True,\n                    global_pool=True,\n                    output_stride=None,\n                    include_root_block=True,\n                    reuse=None,\n                    scope=\'resnet_v2_small\'):\n    """"""A shallow and thin ResNet v2 for faster tests.""""""\n    block = resnet_v2.resnet_v2_block\n    blocks = [\n        block(\'block1\', base_depth=1, num_units=3, stride=2),\n        block(\'block2\', base_depth=2, num_units=3, stride=2),\n        block(\'block3\', base_depth=4, num_units=3, stride=2),\n        block(\'block4\', base_depth=8, num_units=2, stride=1),\n    ]\n    return resnet_v2.resnet_v2(inputs, blocks, num_classes,\n                               is_training=is_training,\n                               global_pool=global_pool,\n                               output_stride=output_stride,\n                               include_root_block=include_root_block,\n                               reuse=reuse,\n                               scope=scope)\n\n  def testClassificationEndPoints(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, end_points = self._resnet_small(inputs, num_classes,\n                                              global_pool=global_pool,\n                                              scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(), [2, 1, 1, num_classes])\n    self.assertTrue(\'predictions\' in end_points)\n    self.assertListEqual(end_points[\'predictions\'].get_shape().as_list(),\n                         [2, 1, 1, num_classes])\n\n  def testClassificationShapes(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 28, 28, 4],\n          \'resnet/block2\': [2, 14, 14, 8],\n          \'resnet/block3\': [2, 7, 7, 16],\n          \'resnet/block4\': [2, 7, 7, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 21, 21, 8],\n          \'resnet/block3\': [2, 11, 11, 16],\n          \'resnet/block4\': [2, 11, 11, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testRootlessFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 128, 128, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         include_root_block=False,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 64, 64, 4],\n          \'resnet/block2\': [2, 32, 32, 8],\n          \'resnet/block3\': [2, 16, 16, 16],\n          \'resnet/block4\': [2, 16, 16, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    output_stride = 8\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs,\n                                         num_classes,\n                                         global_pool=global_pool,\n                                         output_stride=output_stride,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 41, 41, 8],\n          \'resnet/block3\': [2, 41, 41, 16],\n          \'resnet/block4\': [2, 41, 41, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalValues(self):\n    """"""Verify dense feature extraction with atrous convolution.""""""\n    nominal_stride = 32\n    for output_stride in [4, 8, 16, 32, None]:\n      with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n        with tf.Graph().as_default():\n          with self.test_session() as sess:\n            tf.set_random_seed(0)\n            inputs = create_test_input(2, 81, 81, 3)\n            # Dense feature extraction followed by subsampling.\n            output, _ = self._resnet_small(inputs, None,\n                                           is_training=False,\n                                           global_pool=False,\n                                           output_stride=output_stride)\n            if output_stride is None:\n              factor = 1\n            else:\n              factor = nominal_stride // output_stride\n            output = resnet_utils.subsample(output, factor)\n            # Make the two networks use the same weights.\n            tf.get_variable_scope().reuse_variables()\n            # Feature extraction at the nominal network rate.\n            expected, _ = self._resnet_small(inputs, None,\n                                             is_training=False,\n                                             global_pool=False)\n            sess.run(tf.global_variables_initializer())\n            self.assertAllClose(output.eval(), expected.eval(),\n                                atol=1e-4, rtol=1e-4)\n\n  def testUnknownBatchSize(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(None, height, width, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, _ = self._resnet_small(inputs, num_classes,\n                                     global_pool=global_pool,\n                                     scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, 1, 1, num_classes])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 1, 1, num_classes))\n\n  def testFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs, None,\n                                     global_pool=global_pool)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 3, 3, 32))\n\n  def testAtrousFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    output_stride = 8\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs,\n                                     None,\n                                     global_pool=global_pool,\n                                     output_stride=output_stride)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 9, 9, 32))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/vgg.py,10,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains model definitions for versions of the Oxford VGG network.\n\nThese model definitions were introduced in the following technical report:\n\n  Very Deep Convolutional Networks For Large-Scale Image Recognition\n  Karen Simonyan and Andrew Zisserman\n  arXiv technical report, 2015\n  PDF: http://arxiv.org/pdf/1409.1556.pdf\n  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n  CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n\nUsage:\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_a(inputs)\n\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_16(inputs)\n\n@@vgg_a\n@@vgg_16\n@@vgg_19\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef vgg_arg_scope(weight_decay=0.0005):\n  """"""Defines the VGG arg scope.\n\n  Args:\n    weight_decay: The l2 regularization coefficient.\n\n  Returns:\n    An arg_scope.\n  """"""\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_initializer=tf.zeros_initializer()):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\') as arg_sc:\n      return arg_sc\n\n\ndef vgg_a(inputs,\n          num_classes=1000,\n          is_training=True,\n          dropout_keep_prob=0.5,\n          spatial_squeeze=True,\n          scope=\'vgg_a\',\n          fc_conv_padding=\'VALID\'):\n  """"""Oxford Net VGG 11-Layers version A Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n    fc_conv_padding: the type of padding to use for the fully connected layer\n      that is implemented as a convolutional layer. Use \'SAME\' padding if you\n      are applying the network in a fully convolutional manner and want to\n      get a prediction map downsampled by a factor of 32 as an output. Otherwise,\n      the output prediction map will be (input / 32) - 6 in case of \'VALID\' padding.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_a\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 1, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 1, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 2, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_a.default_image_size = 224\n\n\ndef vgg_16(inputs,\n           num_classes=1000,\n           is_training=True,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope=\'vgg_16\',\n           fc_conv_padding=\'VALID\'):\n  """"""Oxford Net VGG 16-Layers version D Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n    fc_conv_padding: the type of padding to use for the fully connected layer\n      that is implemented as a convolutional layer. Use \'SAME\' padding if you\n      are applying the network in a fully convolutional manner and want to\n      get a prediction map downsampled by a factor of 32 as an output. Otherwise,\n      the output prediction map will be (input / 32) - 6 in case of \'VALID\' padding.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_16\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      # yjr_feature = tf.squeeze(net)\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      # end_points[\'yjr_feature\'] = yjr_feature\n      end_points[\'predictions\'] = slim.softmax(net, scope=\'predictions\')\n      return net, end_points\nvgg_16.default_image_size = 224\n\n\ndef vgg_19(inputs,\n           num_classes=1000,\n           is_training=True,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope=\'vgg_19\',\n           fc_conv_padding=\'VALID\'):\n  """"""Oxford Net VGG 19-Layers version E Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n    fc_conv_padding: the type of padding to use for the fully connected layer\n      that is implemented as a convolutional layer. Use \'SAME\' padding if you\n      are applying the network in a fully convolutional manner and want to\n      get a prediction map downsampled by a factor of 32 as an output. Otherwise,\n      the output prediction map will be (input / 32) - 6 in case of \'VALID\' padding.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_19\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 4, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_19.default_image_size = 224\n\n# Alias\nvgg_d = vgg_16\nvgg_e = vgg_19\n'"
libs/networks/slim_nets/vgg_test.py,44,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.slim_nets.vgg.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import vgg\n\nslim = tf.contrib.slim\n\n\nclass VGGATest(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_a/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_a/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_a(inputs, num_classes)\n      expected_names = [\'vgg_a/conv1/conv1_1\',\n                        \'vgg_a/pool1\',\n                        \'vgg_a/conv2/conv2_1\',\n                        \'vgg_a/pool2\',\n                        \'vgg_a/conv3/conv3_1\',\n                        \'vgg_a/conv3/conv3_2\',\n                        \'vgg_a/pool3\',\n                        \'vgg_a/conv4/conv4_1\',\n                        \'vgg_a/conv4/conv4_2\',\n                        \'vgg_a/pool4\',\n                        \'vgg_a/conv5/conv5_1\',\n                        \'vgg_a/conv5/conv5_2\',\n                        \'vgg_a/pool5\',\n                        \'vgg_a/fc6\',\n                        \'vgg_a/fc7\',\n                        \'vgg_a/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_a(inputs, num_classes)\n      expected_names = [\'vgg_a/conv1/conv1_1/weights\',\n                        \'vgg_a/conv1/conv1_1/biases\',\n                        \'vgg_a/conv2/conv2_1/weights\',\n                        \'vgg_a/conv2/conv2_1/biases\',\n                        \'vgg_a/conv3/conv3_1/weights\',\n                        \'vgg_a/conv3/conv3_1/biases\',\n                        \'vgg_a/conv3/conv3_2/weights\',\n                        \'vgg_a/conv3/conv3_2/biases\',\n                        \'vgg_a/conv4/conv4_1/weights\',\n                        \'vgg_a/conv4/conv4_1/biases\',\n                        \'vgg_a/conv4/conv4_2/weights\',\n                        \'vgg_a/conv4/conv4_2/biases\',\n                        \'vgg_a/conv5/conv5_1/weights\',\n                        \'vgg_a/conv5/conv5_1/biases\',\n                        \'vgg_a/conv5/conv5_2/weights\',\n                        \'vgg_a/conv5/conv5_2/biases\',\n                        \'vgg_a/fc6/weights\',\n                        \'vgg_a/fc6/biases\',\n                        \'vgg_a/fc7/weights\',\n                        \'vgg_a/fc7/biases\',\n                        \'vgg_a/fc8/weights\',\n                        \'vgg_a/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_a(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_a(eval_inputs, is_training=False,\n                            spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\n\nclass VGG16Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_16/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_16/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_16(inputs, num_classes)\n      expected_names = [\'vgg_16/conv1/conv1_1\',\n                        \'vgg_16/conv1/conv1_2\',\n                        \'vgg_16/pool1\',\n                        \'vgg_16/conv2/conv2_1\',\n                        \'vgg_16/conv2/conv2_2\',\n                        \'vgg_16/pool2\',\n                        \'vgg_16/conv3/conv3_1\',\n                        \'vgg_16/conv3/conv3_2\',\n                        \'vgg_16/conv3/conv3_3\',\n                        \'vgg_16/pool3\',\n                        \'vgg_16/conv4/conv4_1\',\n                        \'vgg_16/conv4/conv4_2\',\n                        \'vgg_16/conv4/conv4_3\',\n                        \'vgg_16/pool4\',\n                        \'vgg_16/conv5/conv5_1\',\n                        \'vgg_16/conv5/conv5_2\',\n                        \'vgg_16/conv5/conv5_3\',\n                        \'vgg_16/pool5\',\n                        \'vgg_16/fc6\',\n                        \'vgg_16/fc7\',\n                        \'vgg_16/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_16(inputs, num_classes)\n      expected_names = [\'vgg_16/conv1/conv1_1/weights\',\n                        \'vgg_16/conv1/conv1_1/biases\',\n                        \'vgg_16/conv1/conv1_2/weights\',\n                        \'vgg_16/conv1/conv1_2/biases\',\n                        \'vgg_16/conv2/conv2_1/weights\',\n                        \'vgg_16/conv2/conv2_1/biases\',\n                        \'vgg_16/conv2/conv2_2/weights\',\n                        \'vgg_16/conv2/conv2_2/biases\',\n                        \'vgg_16/conv3/conv3_1/weights\',\n                        \'vgg_16/conv3/conv3_1/biases\',\n                        \'vgg_16/conv3/conv3_2/weights\',\n                        \'vgg_16/conv3/conv3_2/biases\',\n                        \'vgg_16/conv3/conv3_3/weights\',\n                        \'vgg_16/conv3/conv3_3/biases\',\n                        \'vgg_16/conv4/conv4_1/weights\',\n                        \'vgg_16/conv4/conv4_1/biases\',\n                        \'vgg_16/conv4/conv4_2/weights\',\n                        \'vgg_16/conv4/conv4_2/biases\',\n                        \'vgg_16/conv4/conv4_3/weights\',\n                        \'vgg_16/conv4/conv4_3/biases\',\n                        \'vgg_16/conv5/conv5_1/weights\',\n                        \'vgg_16/conv5/conv5_1/biases\',\n                        \'vgg_16/conv5/conv5_2/weights\',\n                        \'vgg_16/conv5/conv5_2/biases\',\n                        \'vgg_16/conv5/conv5_3/weights\',\n                        \'vgg_16/conv5/conv5_3/biases\',\n                        \'vgg_16/fc6/weights\',\n                        \'vgg_16/fc6/biases\',\n                        \'vgg_16/fc7/weights\',\n                        \'vgg_16/fc7/biases\',\n                        \'vgg_16/fc8/weights\',\n                        \'vgg_16/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_16(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_16(eval_inputs, is_training=False,\n                             spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\n\nclass VGG19Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_19/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_19/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_19(inputs, num_classes)\n      expected_names = [\n          \'vgg_19/conv1/conv1_1\',\n          \'vgg_19/conv1/conv1_2\',\n          \'vgg_19/pool1\',\n          \'vgg_19/conv2/conv2_1\',\n          \'vgg_19/conv2/conv2_2\',\n          \'vgg_19/pool2\',\n          \'vgg_19/conv3/conv3_1\',\n          \'vgg_19/conv3/conv3_2\',\n          \'vgg_19/conv3/conv3_3\',\n          \'vgg_19/conv3/conv3_4\',\n          \'vgg_19/pool3\',\n          \'vgg_19/conv4/conv4_1\',\n          \'vgg_19/conv4/conv4_2\',\n          \'vgg_19/conv4/conv4_3\',\n          \'vgg_19/conv4/conv4_4\',\n          \'vgg_19/pool4\',\n          \'vgg_19/conv5/conv5_1\',\n          \'vgg_19/conv5/conv5_2\',\n          \'vgg_19/conv5/conv5_3\',\n          \'vgg_19/conv5/conv5_4\',\n          \'vgg_19/pool5\',\n          \'vgg_19/fc6\',\n          \'vgg_19/fc7\',\n          \'vgg_19/fc8\'\n      ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_19(inputs, num_classes)\n      expected_names = [\n          \'vgg_19/conv1/conv1_1/weights\',\n          \'vgg_19/conv1/conv1_1/biases\',\n          \'vgg_19/conv1/conv1_2/weights\',\n          \'vgg_19/conv1/conv1_2/biases\',\n          \'vgg_19/conv2/conv2_1/weights\',\n          \'vgg_19/conv2/conv2_1/biases\',\n          \'vgg_19/conv2/conv2_2/weights\',\n          \'vgg_19/conv2/conv2_2/biases\',\n          \'vgg_19/conv3/conv3_1/weights\',\n          \'vgg_19/conv3/conv3_1/biases\',\n          \'vgg_19/conv3/conv3_2/weights\',\n          \'vgg_19/conv3/conv3_2/biases\',\n          \'vgg_19/conv3/conv3_3/weights\',\n          \'vgg_19/conv3/conv3_3/biases\',\n          \'vgg_19/conv3/conv3_4/weights\',\n          \'vgg_19/conv3/conv3_4/biases\',\n          \'vgg_19/conv4/conv4_1/weights\',\n          \'vgg_19/conv4/conv4_1/biases\',\n          \'vgg_19/conv4/conv4_2/weights\',\n          \'vgg_19/conv4/conv4_2/biases\',\n          \'vgg_19/conv4/conv4_3/weights\',\n          \'vgg_19/conv4/conv4_3/biases\',\n          \'vgg_19/conv4/conv4_4/weights\',\n          \'vgg_19/conv4/conv4_4/biases\',\n          \'vgg_19/conv5/conv5_1/weights\',\n          \'vgg_19/conv5/conv5_1/biases\',\n          \'vgg_19/conv5/conv5_2/weights\',\n          \'vgg_19/conv5/conv5_2/biases\',\n          \'vgg_19/conv5/conv5_3/weights\',\n          \'vgg_19/conv5/conv5_3/biases\',\n          \'vgg_19/conv5/conv5_4/weights\',\n          \'vgg_19/conv5/conv5_4/biases\',\n          \'vgg_19/fc6/weights\',\n          \'vgg_19/fc6/biases\',\n          \'vgg_19/fc7/weights\',\n          \'vgg_19/fc7/biases\',\n          \'vgg_19/fc8/weights\',\n          \'vgg_19/fc8/biases\',\n      ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_19(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_19(eval_inputs, is_training=False,\n                             spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
