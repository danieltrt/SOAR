file_path,api_count,code
mcnn/data.py,38,"b'import os\nfrom PIL import Image\nimport numpy as np\nimport tensorflow as tf\nimport tensorlayer as tl\nfrom xml.etree.ElementTree import ElementTree as ET\n\nimg_height = 256\nimg_width = 128\n\n\ndef load_index(is_train):\n    if is_train == True:\n        index_file = ""../TRAIN/indexs.txt""\n    else:\n        index_file = ""../TEST/indexs.txt""\n\n    with open(index_file) as f:\n        indexs = f.readlines()\n\n    indexs = [i.strip() for i in indexs]\n\n    return indexs\n\n\ndef _load_img(idx, is_train):\n    if is_train == True:\n        img_path = ""../TRAIN/IMAGES_TRAIN""\n    else:\n        img_path = ""../TEST/IMAGES_TEST""\n\n    im = Image.open(""{}/{}.jpg"".format(img_path, idx))\n    im = im.resize((img_width, img_height))\n    in_ = np.array(im, dtype=np.uint8)\n\n    #in_ = in_.transpose((2, 0, 1))\n    return in_\n\n\ndef _load_label(idx):\n    xml_path = ""../TRAIN/ANNOTATIONS_TRAIN""\n\n    tree = ET()\n    tree.parse(""{}/{}.xml"".format(xml_path, idx))\n    labels = {}\n    labels[""hair""] = int(tree.find(""hairlength"").text)\n    labels[""gender""] = int(tree.find(""gender"").text)\n\n    objs = tree.findall(""subcomponent"")\n    for obj in objs:\n        name = obj.find(""name"").text\n        if name in [""top"", ""down"", ""shoes"", ""bag""]:\n            category = obj.find(""category"").text\n            if category == ""NULL"" and name in [""bag"", ""shoes""]:\n                category = 5  # represent there is no bag or shoes\n            labels[name] = int(category)\n        if name == ""hat"":\n            if obj.find(""bndbox"").find(""xmin"").text == ""NULL"":\n                labels[name] = 0 # represent here is no hat\n            else:\n                labels[name] = 1\n    return labels\n\n\ndef data_to_tfrecord(indexs, filename, is_train):\n    print(""Converting data into %s ...""%filename)\n    writer = tf.python_io.TFRecordWriter(filename)\n\n    for idx in indexs:\n        img = _load_img(idx, is_train)\n\t#tl.visualize.frame(I=img, second=5, saveable=False, name=\'frame\', fig_idx=12836)\n        img_raw = img.tobytes()\n        if is_train == True:\n            labels = _load_label(idx)\n            hat_label = int(labels[\'hat\'])\n            hair_label = int(labels[\'hair\'])\n            gender_label = int(labels[\'gender\'])\n            top_label = int(labels[\'top\'])\n            down_label = int(labels[\'down\'])\n            shoes_label = int(labels[\'shoes\'])\n            bag_label = int(labels[\'bag\'])\n\n            example = tf.train.Example(features=tf.train.Features(feature={\n                ""hat_label"": tf.train.Feature(int64_list=tf.train.Int64List(value=[hat_label])),\n                ""hair_label"": tf.train.Feature(int64_list=tf.train.Int64List(value=[hair_label])),\n                ""gender_label"": tf.train.Feature(int64_list=tf.train.Int64List(value=[gender_label])),\n                ""top_label"": tf.train.Feature(int64_list=tf.train.Int64List(value=[top_label])),\n                ""down_label"": tf.train.Feature(int64_list=tf.train.Int64List(value=[down_label])),\n                ""shoes_label"": tf.train.Feature(int64_list=tf.train.Int64List(value=[shoes_label])),\n                ""bag_label"": tf.train.Feature(int64_list=tf.train.Int64List(value=[bag_label])),\n                ""img_raw"": tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw]))\n            }))\n        else:\n            example = tf.train.Example(features=tf.train.Features(feature={\n                ""img_raw"": tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw]))\n            }))\n        writer.write(example.SerializeToString())\n    writer.close()\n\n\n\ndef read_and_decode(filename, is_train):\n    filename_queue = tf.train.string_input_producer([filename])\n    reader = tf.TFRecordReader()\n    _,serialized_example = reader.read(filename_queue)\n\n    if is_train == True:\n        features = tf.parse_single_example(serialized_example,\n                                           features={\n                                               ""hat_label"": tf.FixedLenFeature([], tf.int64),\n                                               ""hair_label"": tf.FixedLenFeature([], tf.int64),\n                                               ""gender_label"": tf.FixedLenFeature([], tf.int64),\n                                               ""top_label"": tf.FixedLenFeature([], tf.int64),\n                                               ""down_label"": tf.FixedLenFeature([], tf.int64),\n                                               ""shoes_label"": tf.FixedLenFeature([], tf.int64),\n                                               ""bag_label"": tf.FixedLenFeature([], tf.int64),\n                                               ""img_raw"": tf.FixedLenFeature([], tf.string),\n                                           })\n        img = tf.decode_raw(features[\'img_raw\'], tf.uint8)\n        img = tf.reshape(img, [128, 256, 3])\n\t#image = Image.frombytes(\'RGB\', (224, 224), img[0])\n\timg = tf.cast(img, tf.float32) * (1. / 255) - 0.5\n\t#print(type(img))\n\t#img = np.asarray(img, dtype=np.uint8)\n\t#print(type(img))\n\t#tl.visualize.frame(I=img, second=5, saveable=False, name=\'frame\', fig_idx=12836)\n\n        hat_label = tf.cast(features[\'hat_label\'], tf.int32)\n        hair_label = tf.cast(features[\'hair_label\'], tf.int32)\n        gender_label = tf.cast(features[\'gender_label\'], tf.int32)\n        top_label = tf.cast(features[\'top_label\'], tf.int32)\n        down_label = tf.cast(features[\'down_label\'], tf.int32)\n        shoes_label = tf.cast(features[\'shoes_label\'], tf.int32)\n        bag_label = tf.cast(features[\'bag_label\'], tf.int32)\n        labels = {""hat"":hat_label, ""hair"":hair_label, ""gender"":gender_label,\n                  ""top"":top_label, ""down"":down_label, ""shoes"":shoes_label,\n                  ""bag"":bag_label}\n\n        return img, labels\n    else:\n        features = tf.parse_single_example(serialized_example,\n                                           features={\n                                               ""img_raw"": tf.FixedLenFeature([], tf.string),\n                                           })\n        img = tf.decode_raw(features[\'img_raw\'], tf.uint8)\n        img = tf.reshape(img, [128, 256, 3])\n\timg = tf.cast(img, tf.float32) * (1. / 255) - 0.5\n\t#tl.visualize.frame(I=img, second=5, saveable=False, name=\'frame\', fig_idx=12833)\n\n        return img\n\nif __name__ == ""__main__"":\n    print(""Prepare Data ..."")\n    train_indexs = load_index(is_train=True)\n    test_indexs = load_index(is_train=False)\n    data_to_tfrecord(train_indexs, ""train_tfrecord"", is_train=True)\n    data_to_tfrecord(test_indexs, ""test_tfrecord"", is_train=False)\n    print(""Data Success."")\n'"
mcnn/model.py,23,"b'import tensorflow as tf\nimport tensorlayer as tl\nfrom tensorlayer.layers import set_keep\n\ndef conv_lrn_pool(input, conv_shape, conv_strides, pool_size, pool_strides, name):\n    with tf.variable_scope(""model"", None):\n        tl.layers.set_name_reuse(None)\n        network = tl.layers.Conv2dLayer(input,\n                                    act=tf.nn.relu,\n                                    shape=conv_shape,\n                                    strides=conv_strides,\n                                    padding=""SAME"",\n                                    W_init=tf.truncated_normal_initializer(stddev=5e-2),\n                                    b_init=tf.constant_initializer(value=0.0),\n                                    name=""conv_"" + name)\n        network.outputs = tf.nn.lrn(network.outputs, 5, bias=1.0,\n                                alpha=0.00005, beta=0.75, name=""norm_"" + name)\n        network = tl.layers.PoolLayer(network, ksize=pool_size,\n                                  strides=pool_strides,\n                                  padding=""SAME"",\n                                  pool=tf.nn.max_pool,\n                                  name=""pool_"" + name)\n        return network\n\n\ndef branch(input, name):\n    with tf.variable_scope(""model"", None):\n        tl.layers.set_name_reuse(None)\n        network = tl.layers.DenseLayer(input, n_units=512,\n                                   act=tf.nn.relu,\n                                   W_init=tf.truncated_normal_initializer(stddev=5e-2),\n                                   b_init=tf.constant_initializer(value=0.0),\n                                   name=""dense1_"" + name)\n        network = tl.layers.DropoutLayer(network, keep=0.5, name=""drop1_"" + name)\n        network = tl.layers.DenseLayer(network, n_units=512,\n                                   act=tf.nn.relu,\n                                   W_init=tf.truncated_normal_initializer(stddev=5e-2),\n                                   b_init=tf.constant_initializer(value=0.0),\n                                   name=""dense2_"" + name)\n        network = tl.layers.DropoutLayer(network, keep=0.5, name=""drop2_"" + name)\n        return network\n\n\ndef single_branch(input, name):\n    with tf.variable_scope(""model"", None):\n        tl.layers.set_name_reuse(None)\n        network = conv_lrn_pool(input, conv_shape=[3, 3, 200, 300],\n                            conv_strides=[1, 1, 1, 1],\n                            pool_size=[1, 5, 5, 1],\n                            pool_strides=[1, 3, 3, 1],name=name)\n\n        network = tl.layers.FlattenLayer(network, name=\'flatten_\'+name)\n        network = branch(network, name)\n \n        return network\n\n\ndef double_branch(input, name, name1, name2):\n    with tf.variable_scope(""model"", None):\n        tl.layers.set_name_reuse(None)\n        network = conv_lrn_pool(input, conv_shape=[3, 3, 200, 300],\n                            conv_strides=[1, 1, 1, 1],\n                            pool_size=[1, 5, 5, 1],\n                            pool_strides=[1, 3, 3, 1], name=name)\n\n        network = tl.layers.FlattenLayer(network, name=\'flatten_\'+name)\n\n        branch1 = branch(network, name1)\n        branch2 = branch(network, name2)\n\n        return branch1, branch2\n\n\ndef score(input, classes, name):\n    with tf.variable_scope(""model"", None):\n        tl.layers.set_name_reuse(None)\n        return  tl.layers.DenseLayer(input, n_units=classes,\n                                      W_init=tf.truncated_normal_initializer(stddev=0.001),\n                                      b_init=tf.constant_initializer(value=0.0),\n                                      name=name+""_score"")\n\n\ndef loss_acc(y, y_):\n     with tf.variable_scope(""model"", None):\n        tl.layers.set_name_reuse(None)\n        cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y, y_))\n        correct_prediction = tf.equal(tf.cast(tf.argmax(y, 1), tf.int32), y_)\n        acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n        return cost, acc\n\n\ndef inference(x, y_, reuse):\n    with tf.variable_scope(""model"", reuse=reuse):\n        tl.layers.set_name_reuse(reuse)\n\n        network = tl.layers.InputLayer(x, name=""input_layer"")\n        network = conv_lrn_pool(network, conv_shape=[7, 7, 3, 75],\n                                conv_strides=[1, 2, 2, 1],\n                                pool_size=[1, 3, 3, 1],\n                                pool_strides=[1, 2, 2, 1], name=""layer1"")\n\n        network = conv_lrn_pool(network, conv_shape=[5, 5, 75, 200],\n                                conv_strides=[1, 2, 2, 1],\n                                pool_size=[1, 3, 3, 1],\n                                pool_strides=[1, 2, 2, 1], name=""layer2"")\n\n        hair_output, hat_output = double_branch(network, ""head"", ""hair"", ""hat"")\n        hair_score = score(hair_output, 3, ""hair"")\n        hair_y = hair_score.outputs\n\tif y_ != None:\n        \thair_loss, hair_acc = loss_acc(hair_y, y_[""hair""])\n\n        hat_score = score(hat_output, 2, ""hat"")\n        hat_y = hat_score.outputs\n\tif y_ != None:\n        \that_loss, hat_acc = loss_acc(hat_y, y_[""hat""])\n\n        gender_output = single_branch(network, ""gender"")\n        gender_score = score(gender_output, 3, ""gender"")\n        gender_y = gender_score.outputs\n\tif y_ != None:\n        \tgender_loss, gender_acc = loss_acc(gender_y, y_[""gender""])\n\n        top_output = single_branch(network, ""top"")\n        top_score = score(top_output, 6, ""top"")\n        top_y = top_score.outputs\n\tif y_ != None:\n        \ttop_loss, top_acc = loss_acc(top_y, y_[""top""])\n\n        down_output = single_branch(network, ""down"")\n        down_score = score(down_output, 5, ""down"")\n        down_y = down_score.outputs\n\tif y_ != None:\n        \tdown_loss, down_acc = loss_acc(down_y, y_[""down""])\n\n        shoes_output = single_branch(network, ""shoes"")\n        shoes_score = score(shoes_output, 6, ""shoes"")\n        shoes_y = shoes_score.outputs\n\tif y_ != None:\n        \tshoes_loss, shoes_acc = loss_acc(shoes_y, y_[""shoes""])\n\n        bag_output = single_branch(network, ""bag"")\n        bag_score = score(bag_output, 6, ""bag"")\n        bag_y = bag_score.outputs\n\tif y_ != None:\n        \tbag_loss, bag_acc = loss_acc(bag_y, y_[""bag""])\n\n\tif y_!=None:\n\t\tcost = {""all"": hair_loss+hat_loss+gender_loss+top_loss+down_loss+shoes_loss+bag_loss,\n\t\t        ""hair"":hair_loss, ""hat"":hat_loss, ""gender"":gender_loss,\n\t\t        ""top"":top_loss, ""down"":down_loss, ""shoes"":shoes_loss, ""bag"":bag_loss}\n\t\tacc = {""all"":hair_acc+hat_acc+gender_acc+top_acc+down_acc+shoes_acc+bag_acc,\n\t\t        ""hair"":hair_acc, ""hat"":hat_acc, ""gender"":gender_acc,\n\t\t        ""top"":top_acc, ""down"":down_acc, ""shoes"":shoes_acc, ""bag"":bag_acc}\n        net = {""hair"":hair_score, ""hat"":hat_score, ""gender"":gender_score,\n                ""top"":top_score, ""down"":down_score, ""shoes"":shoes_score, ""bag"":bag_score}\n\n\tif y_!=None:\n        \treturn cost, acc, net\n\treturn net\n\n\n'"
mcnn/train_test.py,18,"b'import time\nimport tensorflow as tf\nimport tensorlayer as tl\nfrom data import load_index, data_to_tfrecord, read_and_decode\nfrom model import inference\n\nlearning_rate = 0.1\nbatch_size = 100\nepoches = 100\nn_step_epoch = int(20000/100)\nn_step = n_step_epoch*epoches\nprint_freq =1\n\nprint(""Start."")\nwith tf.device(""/gpu:3""):\n    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\n    #1.prepare data in cpu\n    #print(""Prepare Data ..."")\n    #train_indexs = load_index(is_train=True)\n    #test_indexs = load_index(is_train=False)\n    #data_to_tfrecord(train_indexs, ""train_tfrecord"", is_train=True)\n    #data_to_tfrecord(test_indexs, ""test_tfrecord"", is_train=False)\n    #print(""Data Success."")\n\n    x_train, y_train = read_and_decode(""train_tfrecord"", is_train=True)\n    x_test = read_and_decode(""test_tfrecord"", is_train=False)\n\n    x_train_batch, hair_batch, hat_batch, \\\n    gender_batch, top_batch, down_batch, \\\n    shoes_batch, bag_batch = tf.train.shuffle_batch([x_train, y_train[\'hair\'], \n                                                     y_train[\'hat\'], y_train[\'gender\'],\n                                                    y_train[\'top\'], y_train[\'down\'], \n                                                    y_train[\'shoes\'], y_train[\'bag\']],\n                                                          batch_size=batch_size,\n                                                          capacity=2000,\n                                                          min_after_dequeue=1000,\n                                                          num_threads=12)\n    y_train_batch = {""hat"":hat_batch, ""hair"":hair_batch, ""gender"":gender_batch,\n                  ""top"":top_batch, ""down"":down_batch, ""shoes"":shoes_batch,\n                  ""bag"":bag_batch}\n    x_test_train = tf.train.batch([x_test],\n                                  batch_size=batch_size,\n                                  capacity=2000,\n                                  num_threads=12)\n\n    #2.\nwith tf.device(""/gpu:3""):\n    cost, acc, network = inference(x_train_batch, y_train_batch, None)\n\n    #cost,\n    all_train_op = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999,\n                                      epsilon=1e-08, use_locking=False).minimize(cost[\'all\'])\n    hair_train_op = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999,\n                                      epsilon=1e-08, use_locking=False).minimize(cost[\'hair\'])\n    hat_train_op = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999,\n                                           epsilon=1e-08, use_locking=False).minimize(cost[\'hat\'])\n    gender_train_op = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999,\n                                           epsilon=1e-08, use_locking=False).minimize(cost[\'gender\'])\n    top_train_op = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999,\n                                           epsilon=1e-08, use_locking=False).minimize(cost[\'top\'])\n    down_train_op = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999,\n                                           epsilon=1e-08, use_locking=False).minimize(cost[\'down\'])\n    shoes_train_op = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999,\n                                           epsilon=1e-08, use_locking=False).minimize(cost[\'shoes\'])\n    bag_train_op = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999,\n                                           epsilon=1e-08, use_locking=False).minimize(cost[\'bag\'])\n    train_op = {""hair"": hair_train_op, ""hat"": hat_train_op, ""gender"": gender_train_op,\n            ""top"": top_train_op, ""down"": down_train_op, ""shoes"": shoes_train_op, ""bag"": bag_train_op}\n    all_drop = [network[""hair""].all_drop, network[""hat""].all_drop, network[""gender""].all_drop, \n                network[""top""].all_drop, network[""down""].all_drop, network[""shoes""].all_drop,\n                network[""bag""].all_drop]\n\nwith tf.device(""/gpu:3""):\n    sess.run(tf.initialize_all_variables())\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n    print(""Train Start ..."")\n    step = 0\n    for epoch in range(epoches):\n        start_time = time.time()\n        hat_loss, hair_loss, gender_loss, top_loss, down_loss, shoes_loss, bag_loss = 0, 0, 0, 0, 0, 0, 0\n        hat_acc, hair_acc, gender_acc, top_acc, down_acc, shoes_acc, bag_acc = 0, 0, 0, 0, 0, 0, 0\n        train_loss, train_acc, n_batch = 0, 0, 0\n        for s in range(n_step_epoch):\n            feed_dict = {}\n            for i in range(len(all_drop)):\n                feed_dict.update(all_drop[i])\n            hat_err, hat_ac, _ = sess.run([cost[\'hat\'], acc[\'hat\'], hat_train_op], feed_dict)\n            hair_err, hair_ac, _ = sess.run([cost[\'hair\'], acc[\'hair\'], hair_train_op], feed_dict)\n            gender_err, gender_ac, _ = sess.run([cost[\'gender\'], acc[\'gender\'], gender_train_op], feed_dict)\n            top_err, top_ac, _ = sess.run([cost[\'top\'], acc[\'top\'], top_train_op], feed_dict)\n            down_err, down_ac, _ = sess.run([cost[\'down\'], acc[\'down\'], down_train_op], feed_dict)\n            shoes_err, shoes_ac, _ = sess.run([cost[\'shoes\'], acc[\'shoes\'], shoes_train_op], feed_dict)\n            bag_err, bag_ac, _ = sess.run([cost[\'bag\'], acc[\'bag\'], bag_train_op], feed_dict)\n            #err, ac, _ = sess.run([cost[\'all\'], acc[\'all\'], all_train_op],feed_dict)\n            step += 1\n            \n            hat_loss += hat_err\n            hair_loss += hair_err\n            gender_loss += gender_err\n            top_loss += top_err\n            down_loss += down_err\n            shoes_loss += shoes_err\n            bag_loss += bag_err\n            \n            hat_acc += hat_ac\n            hair_acc += hair_ac\n            gender_acc += gender_ac\n            top_acc += top_ac\n            down_acc += down_ac\n            shoes_acc += shoes_ac\n            bag_acc += bag_ac\n            \n            train_loss += (hat_err+hair_err+gender_err+top_err+down_err+shoes_err+bag_err)\n            train_acc += (hat_ac+hair_ac+gender_ac+top_ac+down_ac+shoes_ac+bag_ac)\n            n_batch += 1\n\n        if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n            print(""Epoch %d : Step %d-%d of %d took %fs"" % (\n            epoch, step, step + n_step_epoch, n_step, time.time() - start_time))\n            print(""   train loss: %f"" % (train_loss / n_batch))\n            print(""   train acc: %f"" % (train_acc / n_batch))\n            print(""     hat loss: %f"" % (hat_loss / n_batch))\n            print(""     hat acc: %f"" % (hat_acc / n_batch))\n            print(""     hair loss: %f"" % (hair_loss / n_batch))\n            print(""     hair acc: %f"" % (hair_acc / n_batch))\n            print(""     gender loss: %f"" % (gender_loss / n_batch))\n            print(""     gender acc: %f"" % (gender_acc / n_batch))\n            print(""     top loss: %f"" % (top_loss / n_batch))\n            print(""     top acc: %f"" % (top_acc / n_batch))\n            print(""     down loss: %f"" % (down_loss / n_batch))\n            print(""     down acc: %f"" % (down_acc / n_batch))\n            print(""     shoes loss: %f"" % (shoes_loss / n_batch))\n            print(""     shoes acc: %f"" % (shoes_acc / n_batch))\n            print(""     bag loss: %f"" % (bag_loss / n_batch))\n            print(""     bag acc: %f"" % (bag_acc / n_batch))\n\n    saver = tf.train.Saver()\n    save_path = saver.save(sess, ""mcnn_model.ckpt"")\n    coord.request_stop()\n    coord.join(threads)\n    sess.close()\n\n\n'"
reading_data/example_tfrecords.py,17,"b'import os\nimport tensorflow as tf\nfrom PIL import Image\n\ncwd = os.getcwd()\n\ndef create_record():\n    \'\'\'\n    \xe6\xad\xa4\xe5\xa4\x84\xe6\x88\x91\xe5\x8a\xa0\xe8\xbd\xbd\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe7\x9b\xae\xe5\xbd\x95\xe5\xa6\x82\xe4\xb8\x8b\xef\xbc\x9a\n    0 -- img1.jpg\n         img2.jpg\n         img3.jpg\n         ...\n    1 -- img1.jpg\n         img2.jpg\n         ...\n    2 -- ...\n    ...\n    \'\'\'\n    writer = tf.python_io.TFRecordWriter(""train.tfrecords"")\n    for index, name in enumerate(num_classes):\n        class_path = cwd + name + ""/""\n        for img_name in os.listdir(class_path):\n            img_path = class_path + img_name\n                img = Image.open(img_path)\n                img = img.resize((224, 224))\n            img_raw = img.tobytes() #\xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba\xe5\x8e\x9f\xe7\x94\x9fbytes\n            example = tf.train.Example(features=tf.train.Features(feature={\n                ""label"": tf.train.Feature(int64_list=tf.train.Int64List(value=[index])),\n                \'img_raw\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw]))\n            }))\n            writer.write(example.SerializeToString())\n    writer.close()\n\ndef read_and_decode(filename):\n    filename_queue = tf.train.string_input_producer([filename])\n\n    reader = tf.TFRecordReader()\n    _, serialized_example = reader.read(filename_queue)\n    features = tf.parse_single_example(serialized_example,\n                                       features={\n                                           \'label\': tf.FixedLenFeature([], tf.int64),\n                                           \'img_raw\' : tf.FixedLenFeature([], tf.string),\n                                       })\n\n    img = tf.decode_raw(features[\'img_raw\'], tf.uint8)\n    img = tf.reshape(img, [224, 224, 3])\n    img = tf.cast(img, tf.float32) * (1. / 255) - 0.5\n    label = tf.cast(features[\'label\'], tf.int32)\n\n    return img, label\n\nif __name__ == \'__main__\':\n    img, label = read_and_decode(""train.tfrecords"")\n\n    img_batch, label_batch = tf.train.shuffle_batch([img, label],\n                                                    batch_size=30, capacity=2000,\n                                                    min_after_dequeue=1000)\n    #\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84op\n    init = tf.initialize_all_variables()\n\n    with tf.Session() as sess:\n        sess.run(init)\n\t#\xe5\x90\xaf\xe5\x8a\xa8\xe9\x98\x9f\xe5\x88\x97\n        threads = tf.train.start_queue_runners(sess=sess)\n        for i in range(3):\n            val, l= sess.run([img_batch, label_batch])\n            #l = to_categorical(l, 12)\n            print(val.shape, l)\n'"
reading_data/read_data_time_test.py,18,"b'import os\nimport tensorflow as tf\nfrom tensorflow.python import debug as tf_debug\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport time\n\nroot = ""./ADE20K/images/training/""\n\ndef get_filenames(path):\n    filenames = []\n    for root, dirs, files in os.walk(path):\n        for f in files:\n            if "".jpg"" in f:\n                filenames.append(os.path.join(root, f))\n    return filenames\n\ndef convert_to_tfrecord():\n    writer = tf.python_io.TFRecordWriter(""./training.tfrecords"")\n    filenames = get_filenames(root)\n    for name in filenames:\n        img = Image.open(name)\n        if img.mode == ""RGB"":\n            img = img.resize((256, 256), Image.NEAREST)\n            img_raw = img.tobytes()\n            example = tf.train.Example(features=tf.train.Features(feature={\n                      ""img_raw"":tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw]))\n            }))\n            writer.write(example.SerializeToString())\n    writer.close()\n\ndef read_img(filenames, num_epochs, shuffle=True):\n    filename_queue = tf.train.string_input_producer(filenames, num_epochs=num_epochs, shuffle=True)\n\n    reader = tf.WholeFileReader()\n    key, value = reader.read(filename_queue)\n    img = tf.image.decode_jpeg(value, channels=3)\n    img = tf.image.resize_images(img, size=(256, 256), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n    return img\n\ndef read_tfrecord(filenames, num_epochs, shuffle=True):\n    filename_queue = tf.train.string_input_producer([filenames], num_epochs=num_epochs, shuffle=True)\n\n    reader = tf.TFRecordReader()\n    _, serialized_example = reader.read(filename_queue)\n    features = tf.parse_single_example(serialized_example, features={\n               ""img_raw"": tf.FixedLenFeature([], tf.string),\n    })\n    img = tf.decode_raw(features[""img_raw""], tf.uint8)\n    img =  tf.reshape(img, [256, 256, 3])\n\n    return img\n\nif __name__ == \'__main__\':\n    #create_tfrecord_start_time = time.time()\n    #convert_to_tfrecord()\n    #create_tfrecord_duration = time.time() - create_tfrecord_start_time\n    #print(""Create TFrecord Duration:  %.3f"" % (create_tfrecord_duration))\n\n    with tf.Session() as sess:\n        min_after_dequeue = 1000\n        capacity = min_after_dequeue + 3*4\n\n        img = read_img(get_filenames(root), 1, True)\n        # img = read_tfrecord(""training.tfrecords"", 1, True)\n        img_batch = tf.train.shuffle_batch([img], batch_size=4, num_threads=8,\n                                           capacity=capacity,\n                                           min_after_dequeue=min_after_dequeue)\n\n\n        init = (tf.global_variables_initializer(), tf.local_variables_initializer())\n        sess.run(init)\n        # sess = tf_debug.LocalCLIDebugWrapperSession(sess)\n        # print(sess.run(img))\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(coord=coord)\n        i = 0\n        read_tfrecord_start_time = time.time()\n        try:\n            while not coord.should_stop():\n                imgs = sess.run([img_batch])\n                for img in imgs:\n                    print(img.shape)\n        except Exception, e:\n            coord.request_stop(e)\n        finally:\n            coord.request_stop()\n        coord.join(threads)\n        read_tfrecord_duration = time.time() - read_tfrecord_start_time\n        print(""Read TFrecord Duration:   %.3f"" % read_tfrecord_duration)\n'"
