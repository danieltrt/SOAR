file_path,api_count,code
inference_usbCam_face.py,6,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n# pylint: disable=C0103\n# pylint: disable=E1101\n\nimport sys\nimport time\nimport numpy as np\nimport tensorflow as tf\nimport cv2\n\nfrom utils import label_map_util\nfrom utils import visualization_utils_color as vis_util\n\n# Path to frozen detection graph. This is the actual model that is used for the object detection.\nPATH_TO_CKPT = \'./model/frozen_inference_graph_face.pb\'\n\n# List of the strings that is used to add correct label for each box.\nPATH_TO_LABELS = \'./protos/face_label_map.pbtxt\'\n\nNUM_CLASSES = 2\n\nlabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\ncategory_index = label_map_util.create_category_index(categories)\n\nclass TensoflowFaceDector(object):\n    def __init__(self, PATH_TO_CKPT):\n        """"""Tensorflow detector\n        """"""\n\n        self.detection_graph = tf.Graph()\n        with self.detection_graph.as_default():\n            od_graph_def = tf.compat.v1.GraphDef()\n            with tf.io.gfile.GFile(PATH_TO_CKPT, \'rb\') as fid:\n                serialized_graph = fid.read()\n                od_graph_def.ParseFromString(serialized_graph)\n                tf.import_graph_def(od_graph_def, name=\'\')\n\n\n        with self.detection_graph.as_default():\n            config = tf.compat.v1.ConfigProto()\n            config.gpu_options.allow_growth = True\n            self.sess = tf.compat.v1.Session(graph=self.detection_graph, config=config)\n            self.windowNotSet = True\n\n\n    def run(self, image):\n        """"""image: bgr image\n        return (boxes, scores, classes, num_detections)\n        """"""\n\n        image_np = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        # the array based representation of the image will be used later in order to prepare the\n        # result image with boxes and labels on it.\n        # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n        image_np_expanded = np.expand_dims(image_np, axis=0)\n        image_tensor = self.detection_graph.get_tensor_by_name(\'image_tensor:0\')\n        # Each box represents a part of the image where a particular object was detected.\n        boxes = self.detection_graph.get_tensor_by_name(\'detection_boxes:0\')\n        # Each score represent how level of confidence for each of the objects.\n        # Score is shown on the result image, together with the class label.\n        scores = self.detection_graph.get_tensor_by_name(\'detection_scores:0\')\n        classes = self.detection_graph.get_tensor_by_name(\'detection_classes:0\')\n        num_detections = self.detection_graph.get_tensor_by_name(\'num_detections:0\')\n        # Actual detection.\n        start_time = time.time()\n        (boxes, scores, classes, num_detections) = self.sess.run(\n            [boxes, scores, classes, num_detections],\n            feed_dict={image_tensor: image_np_expanded})\n        elapsed_time = time.time() - start_time\n        print(\'inference time cost: {}\'.format(elapsed_time))\n\n        return (boxes, scores, classes, num_detections)\n\n\nif __name__ == ""__main__"":\n    import sys\n    if len(sys.argv) != 2:\n        print (""usage:%s (cameraID | filename) Detect faces\\\n in the video example:%s 0""%(sys.argv[0], sys.argv[0]))\n        exit(1)\n\n    try:\n    \tcamID = int(sys.argv[1])\n    except:\n    \tcamID = sys.argv[1]\n    \n    tDetector = TensoflowFaceDector(PATH_TO_CKPT)\n\n    cap = cv2.VideoCapture(camID)\n    windowNotSet = True\n    while True:\n        ret, image = cap.read()\n        if ret == 0:\n            break\n\n        [h, w] = image.shape[:2]\n        print (h, w)\n        image = cv2.flip(image, 1)\n\n        (boxes, scores, classes, num_detections) = tDetector.run(image)\n\n        vis_util.visualize_boxes_and_labels_on_image_array(\n            image,\n            np.squeeze(boxes),\n            np.squeeze(classes).astype(np.int32),\n            np.squeeze(scores),\n            category_index,\n            use_normalized_coordinates=True,\n            line_thickness=4)\n\n        if windowNotSet is True:\n            cv2.namedWindow(""tensorflow based (%d, %d)"" % (w, h), cv2.WINDOW_NORMAL)\n            windowNotSet = False\n\n        cv2.imshow(""tensorflow based (%d, %d)"" % (w, h), image)\n        k = cv2.waitKey(1) & 0xff\n        if k == ord(\'q\') or k == 27:\n            break\n\n    cap.release()\n'"
inference_video_face.py,6,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n# pylint: disable=C0103\n# pylint: disable=E1101\n\nimport sys\nimport time\nimport numpy as np\nimport tensorflow as tf\nimport cv2\n\nsys.path.append("".."")\n\nfrom utils import label_map_util\nfrom utils import visualization_utils_color as vis_util\n\n\n# Path to frozen detection graph. This is the actual model that is used for the object detection.\nPATH_TO_CKPT = \'./model/frozen_inference_graph_face.pb\'\n\n# List of the strings that is used to add correct label for each box.\nPATH_TO_LABELS = \'./protos/face_label_map.pbtxt\'\n\nNUM_CLASSES = 2\n\nlabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\ncategory_index = label_map_util.create_category_index(categories)\n\ndef load_image_into_numpy_array(image):\n  (im_width, im_height) = image.size\n  return np.array(image.getdata()).reshape(\n      (im_height, im_width, 3)).astype(np.uint8)\n\ncap = cv2.VideoCapture(""./media/test.mp4"")\nout = None\n\ndetection_graph = tf.Graph()\nwith detection_graph.as_default():\n    od_graph_def = tf.compat.v1.GraphDef()\n    with tf.io.gfile.GFile(PATH_TO_CKPT, \'rb\') as fid:\n        serialized_graph = fid.read()\n        od_graph_def.ParseFromString(serialized_graph)\n        tf.import_graph_def(od_graph_def, name=\'\')\n\nwith detection_graph.as_default():\n  config = tf.compat.v1.ConfigProto()\n  config.gpu_options.allow_growth = True\n  with tf.compat.v1.Session(graph=detection_graph, config=config) as sess:\n    frame_num = 1490;\n    while frame_num:\n      frame_num -= 1\n      ret, image = cap.read()\n      if ret == 0:\n          break\n\n      if out is None:\n          [h, w] = image.shape[:2]\n          out = cv2.VideoWriter(""./media/test_out.avi"", 0, 25.0, (w, h))\n\n\n      image_np = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n      # the array based representation of the image will be used later in order to prepare the\n      # result image with boxes and labels on it.\n      # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n      image_np_expanded = np.expand_dims(image_np, axis=0)\n      image_tensor = detection_graph.get_tensor_by_name(\'image_tensor:0\')\n      # Each box represents a part of the image where a particular object was detected.\n      boxes = detection_graph.get_tensor_by_name(\'detection_boxes:0\')\n      # Each score represent how level of confidence for each of the objects.\n      # Score is shown on the result image, together with the class label.\n      scores = detection_graph.get_tensor_by_name(\'detection_scores:0\')\n      classes = detection_graph.get_tensor_by_name(\'detection_classes:0\')\n      num_detections = detection_graph.get_tensor_by_name(\'num_detections:0\')\n      # Actual detection.\n      start_time = time.time()\n      (boxes, scores, classes, num_detections) = sess.run(\n          [boxes, scores, classes, num_detections],\n          feed_dict={image_tensor: image_np_expanded})\n      elapsed_time = time.time() - start_time\n      print(\'inference time cost: {}\'.format(elapsed_time))\n      #print(boxes.shape, boxes)\n      #print(scores.shape,scores)\n      #print(classes.shape,classes)\n      #print(num_detections)\n      # Visualization of the results of a detection.\n      vis_util.visualize_boxes_and_labels_on_image_array(\n#          image_np,\n          image,\n          np.squeeze(boxes),\n          np.squeeze(classes).astype(np.int32),\n          np.squeeze(scores),\n          category_index,\n          use_normalized_coordinates=True,\n          line_thickness=4)\n      out.write(image)\n\n\n    cap.release()\n    out.release()\n'"
protos/__init__.py,0,b''
protos/string_int_label_map_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/string_int_label_map.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/string_int_label_map.proto\',\n  package=\'object_detection.protos\',\n  serialized_pb=_b(\'\\n2object_detection/protos/string_int_label_map.proto\\x12\\x17object_detection.protos\\""G\\n\\x15StringIntLabelMapItem\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\n\\n\\x02id\\x18\\x02 \\x01(\\x05\\x12\\x14\\n\\x0c\\x64isplay_name\\x18\\x03 \\x01(\\t\\""Q\\n\\x11StringIntLabelMap\\x12<\\n\\x04item\\x18\\x01 \\x03(\\x0b\\x32..object_detection.protos.StringIntLabelMapItem\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_STRINGINTLABELMAPITEM = _descriptor.Descriptor(\n  name=\'StringIntLabelMapItem\',\n  full_name=\'object_detection.protos.StringIntLabelMapItem\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'object_detection.protos.StringIntLabelMapItem.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'id\', full_name=\'object_detection.protos.StringIntLabelMapItem.id\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'display_name\', full_name=\'object_detection.protos.StringIntLabelMapItem.display_name\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=79,\n  serialized_end=150,\n)\n\n\n_STRINGINTLABELMAP = _descriptor.Descriptor(\n  name=\'StringIntLabelMap\',\n  full_name=\'object_detection.protos.StringIntLabelMap\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'item\', full_name=\'object_detection.protos.StringIntLabelMap.item\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=152,\n  serialized_end=233,\n)\n\n_STRINGINTLABELMAP.fields_by_name[\'item\'].message_type = _STRINGINTLABELMAPITEM\nDESCRIPTOR.message_types_by_name[\'StringIntLabelMapItem\'] = _STRINGINTLABELMAPITEM\nDESCRIPTOR.message_types_by_name[\'StringIntLabelMap\'] = _STRINGINTLABELMAP\n\nStringIntLabelMapItem = _reflection.GeneratedProtocolMessageType(\'StringIntLabelMapItem\', (_message.Message,), dict(\n  DESCRIPTOR = _STRINGINTLABELMAPITEM,\n  __module__ = \'object_detection.protos.string_int_label_map_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.StringIntLabelMapItem)\n  ))\n_sym_db.RegisterMessage(StringIntLabelMapItem)\n\nStringIntLabelMap = _reflection.GeneratedProtocolMessageType(\'StringIntLabelMap\', (_message.Message,), dict(\n  DESCRIPTOR = _STRINGINTLABELMAP,\n  __module__ = \'object_detection.protos.string_int_label_map_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.StringIntLabelMap)\n  ))\n_sym_db.RegisterMessage(StringIntLabelMap)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
utils/__init__.py,0,b''
utils/label_map_util.py,1,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Label map utility functions.""""""\n\nimport logging\n\nimport tensorflow as tf\nfrom google.protobuf import text_format\nfrom protos import string_int_label_map_pb2    \n\n\ndef _validate_label_map(label_map):\n  """"""Checks if a label map is valid.\n\n  Args:\n    label_map: StringIntLabelMap to validate.\n\n  Raises:\n    ValueError: if label map is invalid.\n  """"""\n  for item in label_map.item:\n    if item.id < 1:\n      raise ValueError(\'Label map ids should be >= 1.\')\n\n\ndef create_category_index(categories):\n  """"""Creates dictionary of COCO compatible categories keyed by category id.\n\n  Args:\n    categories: a list of dicts, each of which has the following keys:\n      \'id\': (required) an integer id uniquely identifying this category.\n      \'name\': (required) string representing category name\n        e.g., \'cat\', \'dog\', \'pizza\'.\n\n  Returns:\n    category_index: a dict containing the same entries as categories, but keyed\n      by the \'id\' field of each category.\n  """"""\n  category_index = {}\n  for cat in categories:\n    category_index[cat[\'id\']] = cat\n  return category_index\n\n\ndef convert_label_map_to_categories(label_map,\n                                    max_num_classes,\n                                    use_display_name=True):\n  """"""Loads label map proto and returns categories list compatible with eval.\n\n  This function loads a label map and returns a list of dicts, each of which\n  has the following keys:\n    \'id\': (required) an integer id uniquely identifying this category.\n    \'name\': (required) string representing category name\n      e.g., \'cat\', \'dog\', \'pizza\'.\n  We only allow class into the list if its id-label_id_offset is\n  between 0 (inclusive) and max_num_classes (exclusive).\n  If there are several items mapping to the same id in the label map,\n  we will only keep the first one in the categories list.\n\n  Args:\n    label_map: a StringIntLabelMapProto or None.  If None, a default categories\n      list is created with max_num_classes categories.\n    max_num_classes: maximum number of (consecutive) label indices to include.\n    use_display_name: (boolean) choose whether to load \'display_name\' field\n      as category name.  If False or if the display_name field does not exist,\n      uses \'name\' field as category names instead.\n  Returns:\n    categories: a list of dictionaries representing all possible categories.\n  """"""\n  categories = []\n  list_of_ids_already_added = []\n  if not label_map:\n    label_id_offset = 1\n    for class_id in range(max_num_classes):\n      categories.append({\n          \'id\': class_id + label_id_offset,\n          \'name\': \'category_{}\'.format(class_id + label_id_offset)\n      })\n    return categories\n  for item in label_map.item:\n    if not 0 < item.id <= max_num_classes:\n      logging.info(\'Ignore item %d since it falls outside of requested \'\n                   \'label range.\', item.id)\n      continue\n    if use_display_name and item.HasField(\'display_name\'):\n      name = item.display_name\n    else:\n      name = item.name\n    if item.id not in list_of_ids_already_added:\n      list_of_ids_already_added.append(item.id)\n      categories.append({\'id\': item.id, \'name\': name})\n  return categories\n\n\ndef load_labelmap(path):\n  """"""Loads label map proto.\n\n  Args:\n    path: path to StringIntLabelMap proto text file.\n  Returns:\n    a StringIntLabelMapProto\n  """"""\n  with tf.io.gfile.GFile(path, \'r\') as fid:\n    label_map_string = fid.read()\n    label_map = string_int_label_map_pb2.StringIntLabelMap()\n    try:\n      text_format.Merge(label_map_string, label_map)\n    except text_format.ParseError:\n      label_map.ParseFromString(label_map_string)\n  _validate_label_map(label_map)\n  return label_map\n\n\ndef get_label_map_dict(label_map_path):\n  """"""Reads a label map and returns a dictionary of label names to id.\n\n  Args:\n    label_map_path: path to label_map.\n\n  Returns:\n    A dictionary mapping label names to id.\n  """"""\n  label_map = load_labelmap(label_map_path)\n  label_map_dict = {}\n  for item in label_map.item:\n    label_map_dict[item.name] = item.id\n  return label_map_dict\n'"
utils/visualization_utils_color.py,1,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""A set of functions that are used for visualization.\n\nThese functions often receive an image, perform some visualization on the image.\nThe functions do not return a value, instead they modify the image itself.\n\n""""""\nimport collections\nimport numpy as np\nimport PIL.Image as Image\nimport PIL.ImageColor as ImageColor\nimport PIL.ImageDraw as ImageDraw\nimport PIL.ImageFont as ImageFont\nimport six\nimport tensorflow as tf\n\n\n_TITLE_LEFT_MARGIN = 10\n_TITLE_TOP_MARGIN = 10\nSTANDARD_COLORS = [\n    \'AliceBlue\', \'Chartreuse\', \'Aqua\', \'Aquamarine\', \'Azure\', \'Beige\', \'Bisque\',\n    \'BlanchedAlmond\', \'BlueViolet\', \'BurlyWood\', \'CadetBlue\', \'AntiqueWhite\',\n    \'Chocolate\', \'Coral\', \'CornflowerBlue\', \'Cornsilk\', \'Crimson\', \'Cyan\',\n    \'DarkCyan\', \'DarkGoldenRod\', \'DarkGrey\', \'DarkKhaki\', \'DarkOrange\',\n    \'DarkOrchid\', \'DarkSalmon\', \'DarkSeaGreen\', \'DarkTurquoise\', \'DarkViolet\',\n    \'DeepPink\', \'DeepSkyBlue\', \'DodgerBlue\', \'FireBrick\', \'FloralWhite\',\n    \'ForestGreen\', \'Fuchsia\', \'Gainsboro\', \'GhostWhite\', \'Gold\', \'GoldenRod\',\n    \'Salmon\', \'Tan\', \'HoneyDew\', \'HotPink\', \'IndianRed\', \'Ivory\', \'Khaki\',\n    \'Lavender\', \'LavenderBlush\', \'LawnGreen\', \'LemonChiffon\', \'LightBlue\',\n    \'LightCoral\', \'LightCyan\', \'LightGoldenRodYellow\', \'LightGray\', \'LightGrey\',\n    \'LightGreen\', \'LightPink\', \'LightSalmon\', \'LightSeaGreen\', \'LightSkyBlue\',\n    \'LightSlateGray\', \'LightSlateGrey\', \'LightSteelBlue\', \'LightYellow\', \'Lime\',\n    \'LimeGreen\', \'Linen\', \'Magenta\', \'MediumAquaMarine\', \'MediumOrchid\',\n    \'MediumPurple\', \'MediumSeaGreen\', \'MediumSlateBlue\', \'MediumSpringGreen\',\n    \'MediumTurquoise\', \'MediumVioletRed\', \'MintCream\', \'MistyRose\', \'Moccasin\',\n    \'NavajoWhite\', \'OldLace\', \'Olive\', \'OliveDrab\', \'Orange\', \'OrangeRed\',\n    \'Orchid\', \'PaleGoldenRod\', \'PaleGreen\', \'PaleTurquoise\', \'PaleVioletRed\',\n    \'PapayaWhip\', \'PeachPuff\', \'Peru\', \'Pink\', \'Plum\', \'PowderBlue\', \'Purple\',\n    \'Red\', \'RosyBrown\', \'RoyalBlue\', \'SaddleBrown\', \'Green\', \'SandyBrown\',\n    \'SeaGreen\', \'SeaShell\', \'Sienna\', \'Silver\', \'SkyBlue\', \'SlateBlue\',\n    \'SlateGray\', \'SlateGrey\', \'Snow\', \'SpringGreen\', \'SteelBlue\', \'GreenYellow\',\n    \'Teal\', \'Thistle\', \'Tomato\', \'Turquoise\', \'Violet\', \'Wheat\', \'White\',\n    \'WhiteSmoke\', \'Yellow\', \'YellowGreen\'\n]\n\n\ndef save_image_array_as_png(image, output_path):\n  """"""Saves an image (represented as a numpy array) to PNG.\n\n  Args:\n    image: a numpy array with shape [height, width, 3].\n    output_path: path to which image should be written.\n  """"""\n  image_pil = Image.fromarray(np.uint8(image)).convert(\'RGB\')\n  with tf.gfile.Open(output_path, \'w\') as fid:\n    image_pil.save(fid, \'PNG\')\n\n\ndef encode_image_array_as_png_str(image):\n  """"""Encodes a numpy array into a PNG string.\n\n  Args:\n    image: a numpy array with shape [height, width, 3].\n\n  Returns:\n    PNG encoded image string.\n  """"""\n  image_pil = Image.fromarray(np.uint8(image))\n  output = six.BytesIO()\n  image_pil.save(output, format=\'PNG\')\n  png_string = output.getvalue()\n  output.close()\n  return png_string\n\n\ndef draw_bounding_box_on_image_array(image,\n                                     ymin,\n                                     xmin,\n                                     ymax,\n                                     xmax,\n                                     color=\'red\',\n                                     thickness=4,\n                                     display_str_list=(),\n                                     use_normalized_coordinates=True):\n  """"""Adds a bounding box to an image (numpy array).\n\n  Args:\n    image: a numpy array with shape [height, width, 3].\n    ymin: ymin of bounding box in normalized coordinates (same below).\n    xmin: xmin of bounding box.\n    ymax: ymax of bounding box.\n    xmax: xmax of bounding box.\n    color: color to draw bounding box. Default is red.\n    thickness: line thickness. Default value is 4.\n    display_str_list: list of strings to display in box\n                      (each to be shown on its own line).\n    use_normalized_coordinates: If True (default), treat coordinates\n      ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n      coordinates as absolute.\n  """"""\n  image_pil = Image.fromarray(np.uint8(image)).convert(\'RGB\')\n  draw_bounding_box_on_image(image_pil, ymin, xmin, ymax, xmax, color,\n                             thickness, display_str_list,\n                             use_normalized_coordinates)\n  np.copyto(image, np.array(image_pil))\n\n\ndef draw_bounding_box_on_image(image,\n                               ymin,\n                               xmin,\n                               ymax,\n                               xmax,\n                               color=\'red\',\n                               thickness=4,\n                               display_str_list=(),\n                               use_normalized_coordinates=True):\n  """"""Adds a bounding box to an image.\n\n  Each string in display_str_list is displayed on a separate line above the\n  bounding box in black text on a rectangle filled with the input \'color\'.\n\n  Args:\n    image: a PIL.Image object.\n    ymin: ymin of bounding box.\n    xmin: xmin of bounding box.\n    ymax: ymax of bounding box.\n    xmax: xmax of bounding box.\n    color: color to draw bounding box. Default is red.\n    thickness: line thickness. Default value is 4.\n    display_str_list: list of strings to display in box\n                      (each to be shown on its own line).\n    use_normalized_coordinates: If True (default), treat coordinates\n      ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n      coordinates as absolute.\n  """"""\n  draw = ImageDraw.Draw(image)\n  im_width, im_height = image.size\n  if use_normalized_coordinates:\n    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n                                  ymin * im_height, ymax * im_height)\n  else:\n    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n  draw.line([(left, top), (left, bottom), (right, bottom),\n             (right, top), (left, top)], width=thickness, fill=color)\n  try:\n    font = ImageFont.truetype(\'arial.ttf\', 24)\n  except IOError:\n    font = ImageFont.load_default()\n\n  text_bottom = top\n  # Reverse list and print from bottom to top.\n  for display_str in display_str_list[::-1]:\n    text_width, text_height = font.getsize(display_str)\n    margin = np.ceil(0.05 * text_height)\n    draw.rectangle(\n        [(left, text_bottom - text_height - 2 * margin), (left + text_width,\n                                                          text_bottom)],\n        fill=color)\n    draw.text(\n        (left + margin, text_bottom - text_height - margin),\n        display_str,\n        fill=\'black\',\n        font=font)\n    text_bottom -= text_height - 2 * margin\n\n\ndef draw_bounding_boxes_on_image_array(image,\n                                       boxes,\n                                       color=\'red\',\n                                       thickness=4,\n                                       display_str_list_list=()):\n  """"""Draws bounding boxes on image (numpy array).\n\n  Args:\n    image: a numpy array object.\n    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n           The coordinates are in normalized format between [0, 1].\n    color: color to draw bounding box. Default is red.\n    thickness: line thickness. Default value is 4.\n    display_str_list_list: list of list of strings.\n                           a list of strings for each bounding box.\n                           The reason to pass a list of strings for a\n                           bounding box is that it might contain\n                           multiple labels.\n\n  Raises:\n    ValueError: if boxes is not a [N, 4] array\n  """"""\n  image_pil = Image.fromarray(image)\n  draw_bounding_boxes_on_image(image_pil, boxes, color, thickness,\n                               display_str_list_list)\n  np.copyto(image, np.array(image_pil))\n\n\ndef draw_bounding_boxes_on_image(image,\n                                 boxes,\n                                 color=\'red\',\n                                 thickness=4,\n                                 display_str_list_list=()):\n  """"""Draws bounding boxes on image.\n\n  Args:\n    image: a PIL.Image object.\n    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n           The coordinates are in normalized format between [0, 1].\n    color: color to draw bounding box. Default is red.\n    thickness: line thickness. Default value is 4.\n    display_str_list_list: list of list of strings.\n                           a list of strings for each bounding box.\n                           The reason to pass a list of strings for a\n                           bounding box is that it might contain\n                           multiple labels.\n\n  Raises:\n    ValueError: if boxes is not a [N, 4] array\n  """"""\n  boxes_shape = boxes.shape\n  if not boxes_shape:\n    return\n  if len(boxes_shape) != 2 or boxes_shape[1] != 4:\n    raise ValueError(\'Input must be of size [N, 4]\')\n  for i in range(boxes_shape[0]):\n    display_str_list = ()\n    if display_str_list_list:\n      display_str_list = display_str_list_list[i]\n    draw_bounding_box_on_image(image, boxes[i, 0], boxes[i, 1], boxes[i, 2],\n                               boxes[i, 3], color, thickness, display_str_list)\n\n\ndef draw_keypoints_on_image_array(image,\n                                  keypoints,\n                                  color=\'red\',\n                                  radius=2,\n                                  use_normalized_coordinates=True):\n  """"""Draws keypoints on an image (numpy array).\n\n  Args:\n    image: a numpy array with shape [height, width, 3].\n    keypoints: a numpy array with shape [num_keypoints, 2].\n    color: color to draw the keypoints with. Default is red.\n    radius: keypoint radius. Default value is 2.\n    use_normalized_coordinates: if True (default), treat keypoint values as\n      relative to the image.  Otherwise treat them as absolute.\n  """"""\n  image_pil = Image.fromarray(np.uint8(image)).convert(\'RGB\')\n  draw_keypoints_on_image(image_pil, keypoints, color, radius,\n                          use_normalized_coordinates)\n  np.copyto(image, np.array(image_pil))\n\n\ndef draw_keypoints_on_image(image,\n                            keypoints,\n                            color=\'red\',\n                            radius=2,\n                            use_normalized_coordinates=True):\n  """"""Draws keypoints on an image.\n\n  Args:\n    image: a PIL.Image object.\n    keypoints: a numpy array with shape [num_keypoints, 2].\n    color: color to draw the keypoints with. Default is red.\n    radius: keypoint radius. Default value is 2.\n    use_normalized_coordinates: if True (default), treat keypoint values as\n      relative to the image.  Otherwise treat them as absolute.\n  """"""\n  draw = ImageDraw.Draw(image)\n  im_width, im_height = image.size\n  keypoints_x = [k[1] for k in keypoints]\n  keypoints_y = [k[0] for k in keypoints]\n  if use_normalized_coordinates:\n    keypoints_x = tuple([im_width * x for x in keypoints_x])\n    keypoints_y = tuple([im_height * y for y in keypoints_y])\n  for keypoint_x, keypoint_y in zip(keypoints_x, keypoints_y):\n    draw.ellipse([(keypoint_x - radius, keypoint_y - radius),\n                  (keypoint_x + radius, keypoint_y + radius)],\n                 outline=color, fill=color)\n\n\ndef draw_mask_on_image_array(image, mask, color=\'red\', alpha=0.7):\n  """"""Draws mask on an image.\n\n  Args:\n    image: uint8 numpy array with shape (img_height, img_height, 3)\n    mask: a float numpy array of shape (img_height, img_height) with\n      values between 0 and 1\n    color: color to draw the keypoints with. Default is red.\n    alpha: transparency value between 0 and 1. (default: 0.7)\n\n  Raises:\n    ValueError: On incorrect data type for image or masks.\n  """"""\n  if image.dtype != np.uint8:\n    raise ValueError(\'`image` not of type np.uint8\')\n  if mask.dtype != np.float32:\n    raise ValueError(\'`mask` not of type np.float32\')\n  if np.any(np.logical_or(mask > 1.0, mask < 0.0)):\n    raise ValueError(\'`mask` elements should be in [0, 1]\')\n  rgb = ImageColor.getrgb(color)\n  pil_image = Image.fromarray(image)\n\n  solid_color = np.expand_dims(\n      np.ones_like(mask), axis=2) * np.reshape(list(rgb), [1, 1, 3])\n  pil_solid_color = Image.fromarray(np.uint8(solid_color)).convert(\'RGBA\')\n  pil_mask = Image.fromarray(np.uint8(255.0*alpha*mask)).convert(\'L\')\n  pil_image = Image.composite(pil_solid_color, pil_image, pil_mask)\n  np.copyto(image, np.array(pil_image.convert(\'RGB\')))\n\n\ndef visualize_boxes_and_labels_on_image_array(image,\n                                              boxes,\n                                              classes,\n                                              scores,\n                                              category_index,\n                                              instance_masks=None,\n                                              keypoints=None,\n                                              use_normalized_coordinates=False,\n                                              max_boxes_to_draw=20,\n                                              min_score_thresh=.7,\n                                              agnostic_mode=False,\n                                              line_thickness=4):\n  """"""Overlay labeled boxes on an image with formatted scores and label names.\n\n  This function groups boxes that correspond to the same location\n  and creates a display string for each detection and overlays these\n  on the image.  Note that this function modifies the image array in-place\n  and does not return anything.\n\n  Args:\n    image: uint8 numpy array with shape (img_height, img_width, 3)\n    boxes: a numpy array of shape [N, 4]\n    classes: a numpy array of shape [N]\n    scores: a numpy array of shape [N] or None.  If scores=None, then\n      this function assumes that the boxes to be plotted are groundtruth\n      boxes and plot all boxes as black with no classes or scores.\n    category_index: a dict containing category dictionaries (each holding\n      category index `id` and category name `name`) keyed by category indices.\n    instance_masks: a numpy array of shape [N, image_height, image_width], can\n      be None\n    keypoints: a numpy array of shape [N, num_keypoints, 2], can\n      be None\n    use_normalized_coordinates: whether boxes is to be interpreted as\n      normalized coordinates or not.\n    max_boxes_to_draw: maximum number of boxes to visualize.  If None, draw\n      all boxes.\n    min_score_thresh: minimum score threshold for a box to be visualized\n    agnostic_mode: boolean (default: False) controlling whether to evaluate in\n      class-agnostic mode or not.  This mode will display scores but ignore\n      classes.\n    line_thickness: integer (default: 4) controlling line width of the boxes.\n  """"""\n  # Create a display string (and color) for every box location, group any boxes\n  # that correspond to the same location.\n  box_to_display_str_map = collections.defaultdict(list)\n  box_to_color_map = collections.defaultdict(str)\n  box_to_instance_masks_map = {}\n  box_to_keypoints_map = collections.defaultdict(list)\n  if not max_boxes_to_draw:\n    max_boxes_to_draw = boxes.shape[0]\n  for i in range(min(max_boxes_to_draw, boxes.shape[0])):\n    if scores is None or scores[i] > min_score_thresh:\n      box = tuple(boxes[i].tolist())\n      if instance_masks is not None:\n        box_to_instance_masks_map[box] = instance_masks[i]\n      if keypoints is not None:\n        box_to_keypoints_map[box].extend(keypoints[i])\n      if scores is None:\n        box_to_color_map[box] = \'black\'\n      else:\n        if not agnostic_mode:\n          if classes[i] in category_index.keys():\n            class_name = category_index[classes[i]][\'name\']\n          else:\n            class_name = \'N/A\'\n          display_str = \'{}: {}%\'.format(\n              class_name,\n              int(100*scores[i]))\n        else:\n          display_str = \'score: {}%\'.format(int(100 * scores[i]))\n        box_to_display_str_map[box].append(display_str)\n        if agnostic_mode:\n          box_to_color_map[box] = \'DarkOrange\'\n        else:\n          box_to_color_map[box] = STANDARD_COLORS[\n              classes[i] % len(STANDARD_COLORS)]\n\n  # Draw all boxes onto image.\n  for box, color in box_to_color_map.items():\n    color = \'Violet\'\n    ymin, xmin, ymax, xmax = box\n    if instance_masks is not None:\n      draw_mask_on_image_array(\n          image,\n          box_to_instance_masks_map[box],\n          color=color\n      )\n    draw_bounding_box_on_image_array(\n        image,\n        ymin,\n        xmin,\n        ymax,\n        xmax,\n        color=color,\n        thickness=line_thickness,\n        display_str_list=box_to_display_str_map[box],\n        use_normalized_coordinates=use_normalized_coordinates)\n    if keypoints is not None:\n      draw_keypoints_on_image_array(\n          image,\n          box_to_keypoints_map[box],\n          color=color,\n          radius=line_thickness / 2,\n          use_normalized_coordinates=use_normalized_coordinates)\n'"
