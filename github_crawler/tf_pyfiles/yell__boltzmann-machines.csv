file_path,api_count,code
setup.py,0,"b""from setuptools import setup\nfrom setuptools import find_packages\n\nsetup(name='boltzmann_machines',\n      version='0.1',\n      url='https://github.com/monsta-hd/boltzmann-machines',\n      install_requires=[ 'tensorflow-gpu>=1.3.0', \n                        'scipy>=0.17', \n                        'keras>=2.0.8', \n                        'matplotlib>=1.5.3', \n                        'nose-exclude>=0.5.0', \n                        'nose>=1.3.4', \n                        'numpy>=1.13.0',\n                        'scikit-learn>=0.19.0',\n                        'seaborn>=0.8.1', \n                        'tqdm>=4.14.0' \n],\npackages=find_packages())\n"""
boltzmann_machines/__init__.py,0,"b""__author__ = 'Yelysei Bondarenko'\n__email__ = 'yell.bondarenko@gmail.com'\n\nfrom .dbm import *\nfrom . import dbm\nfrom . import ebm\nfrom . import layers\n\nfrom . import base\nfrom . import rbm\nfrom . import utils\n\n"""
boltzmann_machines/dbm.py,189,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.core.framework import summary_pb2\nfrom tensorflow.contrib.distributions import Bernoulli\n\nfrom .base import run_in_tf_session\nfrom .ebm import EnergyBasedModel\nfrom .layers import BernoulliLayer\nfrom .utils import (make_list_from, write_during_training,\n                   batch_iter, epoch_iter,\n                   log_sum_exp, log_diff_exp, log_mean_exp, log_std_exp)\n\n\nclass DBM(EnergyBasedModel):\n    """"""Deep Boltzmann Machine with EM-like learning algorithm\n    based on PCD and mean-field variational inference [1].\n\n    Parameters\n    ----------\n    rbms : [BaseRBM]\n        Array of already pretrained RBMs going from visible units\n        to the most hidden ones.\n    n_particles : positive int\n        Number of ""persistent"" Markov chains (i.e., ""negative"" or ""fantasy"" ""particles"").\n    v_particle_init : None or (n_particles, n_visible) np.ndarray\n        If provided, initialize visible particle from this matrix,\n        otherwise initialize using resp. stochastic layer initializer.\n    h_particles_init : None or iterable of None or (n_particles, n_hiddens[i]) np.ndarray\n        Same semantics as for `v_particle_init`, but for hidden particles for all layers\n    n_gibbs_steps : positive int or iterable\n        Number of Gibbs steps for PCD. Values are updated after each epoch.\n    max_mf_updates : positive int\n        Maximum number of mean-field updates per weight update.\n    mf_tol : positive float\n        Mean-field tolerance.\n    learning_rate, momentum : positive float or iterable\n        Gradient descent parameters. Values are updated after each epoch.\n    max_epoch : positive int\n        Train till this epoch.\n    batch_size : positive int\n        Input batch size for training. Total number of training examples should\n        be divisible by this number.\n    l2 : non-negative float\n        L2 weight decay coefficient.\n    max_norm : positive float\n        Maximum norm constraint. Might be useful to use for this model instead\n        of L2 weight decay as recommended in [3].\n    sample_v_states : bool\n        Whether to sample visible states, or to use probabilities\n        w/o sampling.\n    sample_h_states : (n_layers,) bool\n        Whether to sample hidden states, or to use probabilities\n        w/o sampling.\n    sparsity_target : float in (0, 1) or iterable\n        Desired probability of hidden activation (for different hidden layers).\n    sparsity_cost : non-negative float or iterable\n        Controls the amount of sparsity penalty (for different hidden layers).\n    sparsity_damping : float in (0, 1)\n        Decay rate for hidden activations probs.\n    train_metrics_every_iter, val_metrics_every_epoch : positive int\n        Control frequency of logging progress\n    verbose : bool\n        Whether to display progress during training.\n    save_after_each_epoch : bool\n        If False, save model only after the whole training is complete.\n    display_filters : non-negative int\n        Number of weights filters to display during training (in TensorBoard).\n    display_particles : non-negative int\n        Number of hidden activations to display during training (in TensorBoard).\n    v_shape : (H, W) or (H, W, C) positive integer tuple\n        Shape for displaying filters during training. C should be in {1, 3, 4}.\n\n    References\n    ----------\n    [1] R. Salakhutdinov and G. Hinton. Deep boltzmann machines.\n        In AISTATS, pp. 448-455. 2009\n    [2] Salakhutdinov, R. Learning Deep Boltzmann Machines, MATLAB code.\n        url: https://www.cs.toronto.edu/~rsalakhu/DBM.html\n    [3] I.J. Goodfellow, A. Courville, and Y. Bengio. Joint training deep\n        boltzmann machines for classification. arXiv preprint arXiv:1301.3568.\n        2013.\n    [4] G. Monvaton and K.-R. Mueller. Deep boltzmann machines and\n        centering trick. In Neural Networks: Tricks of the trade,\n        pp. 621-637, Springer, 2012.\n    [5] G. Hinton and R. Salakhutdinov. A better way to pretrain Deep\n        Boltzmann Machines. In Advances in Neural Information Processing\n        Systems, pp. 2447-2455, 2012.\n    """"""\n    def __init__(self, rbms=None,\n                 n_particles=100, v_particle_init=None, h_particles_init=None,\n                 n_gibbs_steps=5, max_mf_updates=10, mf_tol=1e-7,\n                 learning_rate=0.0005, momentum=0.9, max_epoch=10, batch_size=100,\n                 l2=0., max_norm=np.inf,\n                 sample_v_states=True, sample_h_states=None,\n                 sparsity_target=0.1, sparsity_cost=0., sparsity_damping=0.9,\n                 train_metrics_every_iter=10, val_metrics_every_epoch=1,\n                 verbose=False, save_after_each_epoch=True,\n                 display_filters=0, display_particles=0, v_shape=(28, 28),\n                 model_path=\'dbm_model/\', *args, **kwargs):\n        super(DBM, self).__init__(model_path=model_path, *args, **kwargs)\n        self.n_layers_ = len(rbms) if rbms is not None else None\n        self.n_visible_ = None\n        self.n_hiddens_ = []\n        self.load_rbms(rbms)\n\n        self.n_particles = n_particles\n        self._v_particle_init = v_particle_init\n        self._h_particles_init = h_particles_init\n\n        self.n_gibbs_steps = make_list_from(n_gibbs_steps)\n        self.max_mf_updates = max_mf_updates\n        self.mf_tol = mf_tol\n\n        self.learning_rate = make_list_from(learning_rate)\n        self.momentum = make_list_from(momentum)\n        self.max_epoch = max_epoch\n        self.batch_size = batch_size\n        self.l2 = l2\n        self.max_norm = max_norm\n\n        self.sample_v_states = sample_v_states\n        self.sample_h_states = sample_h_states or [True] * self.n_layers_\n\n        self.sparsity_target = make_list_from(sparsity_target)\n        self.sparsity_cost = make_list_from(sparsity_cost)\n        if self.n_layers_ is not None and self.n_layers_ > 1:\n            for x in (self.sparsity_target, self.sparsity_cost):\n                if len(x) == 1:\n                    x *= self.n_layers_\n        self.sparsity_damping = sparsity_damping\n\n        self.train_metrics_every_iter = train_metrics_every_iter\n        self.val_metrics_every_epoch = val_metrics_every_epoch\n        self.verbose = verbose\n        self.save_after_each_epoch = save_after_each_epoch\n\n        for nh in self.n_hiddens_:\n            assert nh >= display_filters\n        self.display_filters = display_filters\n\n        assert display_particles <= self.n_particles\n        self.display_particles = display_particles\n\n        self.v_shape = v_shape\n        if len(self.v_shape) == 2:\n            self.v_shape = (self.v_shape[0], self.v_shape[1], 1)\n\n        # additional attributes\n        self.epoch_ = 0\n        self.iter_ = 0\n        self.n_samples_generated_ = 0\n\n        # tf constants\n        self._n_visible = None\n        self._n_hiddens = []\n        self._n_particles = None\n        self._max_mf_updates = None\n        self._mf_tol = None\n\n        self._sparsity_targets = []\n        self._sparsity_costs = []\n        self._sparsity_damping = None\n\n        self._batch_size = None\n        self._l2 = None\n        self._max_norm = None\n        self._N = None\n        self._M = None\n\n        # tf input data\n        self._learning_rate = None\n        self._momentum = None\n        self._n_gibbs_steps = None\n        self._X_batch = None\n        self._delta_beta = None\n        self._n_ais_runs = None\n\n        # tf vars\n        self._W = []\n        self._vb = None\n        self._hb = []\n\n        self._dW = []\n        self._dvb = None\n        self._dhb = []\n\n        self._mu = []\n        self._mu_new = []\n        self._q_means = []\n        self._mu_means = []\n\n        self._v = None\n        self._v_new = None\n        self._H = []\n        self._H_new = []\n\n        # tf operations\n        self._train_op = None\n        self._transform_op = None\n        self._msre = None\n        self._reconstruction = None\n        self._n_mf_updates = None\n        self._sample_v = None\n        self._log_Z = None\n        self._log_proba = None\n\n    def load_rbms(self, rbms):\n        if rbms is not None:\n            self._rbms = rbms\n\n            # create some shortcuts\n            self.n_layers_ = len(self._rbms)\n            self.n_visible_ = self._rbms[0].n_visible\n            self.n_hiddens_ = [rbm.n_hidden for rbm in self._rbms]\n\n            # extract weights and biases\n            self._W_init, self._vb_init, self._hb_init = [], [], []\n            for i in range(self.n_layers_):\n                weights = self._rbms[i].get_tf_params(scope=\'weights\')\n                self._W_init.append(weights[\'W\'])\n                self._vb_init.append(weights[\'vb\'])\n                self._hb_init.append(weights[\'hb\'])\n\n            # collect resp. layers of units\n            self._v_layer = self._rbms[0]._v_layer\n            self._h_layers = [rbm._h_layer for rbm in self._rbms]\n\n            # ... and update their dtypes\n            self._v_layer.dtype = self.dtype\n            for h in self._h_layers:\n                h.dtype = self.dtype\n\n    def _make_constants(self):\n        with tf.name_scope(\'constants\'):\n            self._n_visible = tf.constant(self.n_visible_, dtype=tf.int32, name=\'n_visible\')\n            for i in range(self.n_layers_):\n                T = tf.constant(self.n_hiddens_[i], dtype=tf.int32, name=\'n_hidden\')\n                self._n_hiddens.append(T)\n            self._n_particles = tf.constant(self.n_particles, dtype=tf.int32, name=\'n_particles\')\n            self._max_mf_updates = tf.constant(self.max_mf_updates,\n                                               dtype=tf.int32, name=\'max_mf_updates\')\n            self._mf_tol = tf.constant(self.mf_tol, dtype=self._tf_dtype, name=\'mf_tol\')\n\n            for i in range(self.n_layers_):\n                T = tf.constant(self.sparsity_target[i], dtype=self._tf_dtype, name=\'sparsity_target\')\n                self._sparsity_targets.append(T)\n                C = tf.constant(self.sparsity_cost[i], dtype=self._tf_dtype, name=\'sparsity_cost\')\n                self._sparsity_costs.append(C)\n            self._sparsity_damping = tf.constant(self.sparsity_damping, dtype=self._tf_dtype, name=\'sparsity_damping\')\n\n            self._batch_size = tf.constant(self.batch_size, dtype=tf.int32, name=\'batch_size\')\n            self._l2 = tf.constant(self.l2, dtype=self._tf_dtype, name=\'L2_coef\')\n            self._max_norm = tf.constant(self.max_norm, dtype=self._tf_dtype, name=\'max_norm_coef\')\n            self._N = tf.cast(self._batch_size, dtype=self._tf_dtype, name=\'N\')\n            self._M = tf.cast(self._n_particles, dtype=self._tf_dtype, name=\'M\')\n\n    def _make_placeholders(self):\n        with tf.name_scope(\'input_data\'):\n            self._learning_rate = tf.placeholder(self._tf_dtype, [], name=\'learning_rate\')\n            self._momentum = tf.placeholder(self._tf_dtype, [], name=\'momentum\')\n            self._n_gibbs_steps = tf.placeholder(tf.int32, [], name=\'n_gibbs_steps\')\n            self._X_batch = tf.placeholder(self._tf_dtype, [None, self.n_visible_], name=\'X_batch\')\n            self._delta_beta = tf.placeholder(self._tf_dtype, [], name=\'delta_beta\')\n            self._n_ais_runs = tf.placeholder(tf.int32, [], name=\'n_ais_runs\')\n\n    def _make_vars(self):\n        # compose weights and biases of DBM from trained RBMs\' ones\n        # and account double-counting evidence problem [1]\n        W_init, hb_init = [], []\n        vb_init = self._vb_init[0]\n        for i in range(self.n_layers_):\n            W = self._W_init[i]\n            vb = self._vb_init[i]\n            hb = self._hb_init[i]\n\n            # halve weights and biases of intermediate RBMs\n            if 0 < i < self.n_layers_ - 1:\n                W *= 0.5\n                vb *= 0.5\n                hb *= 0.5\n\n            # initialize weights\n            W_init.append(W)\n\n            # initialize hidden biases as average of respective biases\n            # of respective RBMs, as in [2]\n            if i == 0:\n                hb_init.append(0.5 * hb)\n            else:  # i > 0\n                hb_init[i - 1] += 0.5 * vb\n                hb_init.append(0.5 * hb if i < self.n_layers_ - 1 else hb)\n\n        # initialize weights and biases\n        with tf.name_scope(\'weights\'):\n            t = tf.constant(vb_init, dtype=self._tf_dtype, name=\'vb_init\')\n            self._vb = tf.Variable(t, dtype=self._tf_dtype, name=\'vb\')\n            tf.summary.histogram(\'vb_hist\', self._vb)\n\n            for i in range(self.n_layers_):\n                T = tf.constant(W_init[i], dtype=self._tf_dtype, name=\'W_init\')\n                W = tf.Variable(T, dtype=self._tf_dtype, name=\'W\')\n                self._W.append(W)\n                tf.summary.histogram(\'W_hist\', W)\n\n            for i in range(self.n_layers_):\n                t = tf.constant(hb_init[i],  dtype=self._tf_dtype, name=\'hb_init\')\n                hb = tf.Variable(t,  dtype=self._tf_dtype, name=\'hb\')\n                self._hb.append(hb)\n                tf.summary.histogram(\'hb_hist\', hb)\n\n        # visualize filters\n        if self.display_filters:\n            with tf.name_scope(\'filters_visualization\'):\n                W = self._W[0]\n                for i in range(self.n_layers_):\n                    if i > 0:\n                        W = tf.matmul(W, self._W[i])\n                    W_display = tf.transpose(W, [1, 0])\n                    W_display = tf.reshape(W_display, [self.n_hiddens_[i], self.v_shape[2],\n                                                       self.v_shape[0], self.v_shape[1]])\n                    W_display = tf.transpose(W_display, [0, 2, 3, 1])\n                    tf.summary.image(\'W_filters\', W_display, max_outputs=self.display_filters)\n\n        # initialize gradients accumulators\n        with tf.name_scope(\'grads_accumulators\'):\n            t = tf.zeros(vb_init.shape, dtype=self._tf_dtype, name=\'dvb_init\')\n            self._dvb = tf.Variable(t, name=\'dvb\')\n            tf.summary.histogram(\'dvb_hist\', self._dvb)\n\n            for i in range(self.n_layers_):\n                T = tf.zeros(W_init[i].shape, dtype=self._tf_dtype, name=\'dW_init\')\n                dW = tf.Variable(T, name=\'dW\')\n                tf.summary.histogram(\'dW_hist\', dW)\n                self._dW.append(dW)\n\n            for i in range(self.n_layers_):\n                t = tf.zeros(hb_init[i].shape, dtype=self._tf_dtype, name=\'dhb_init\')\n                dhb = tf.Variable(t, name=\'dhb\')\n                tf.summary.histogram(\'dhb_hist\', dhb)\n                self._dhb.append(dhb)\n\n        # initialize variational parameters\n        with tf.name_scope(\'variational_params\'):\n            for i in range(self.n_layers_):\n                t = tf.zeros([self._batch_size, self.n_hiddens_[i]], dtype=self._tf_dtype)\n                mu = tf.Variable(t, name=\'mu\')\n                t_new = tf.zeros([self._batch_size, self.n_hiddens_[i]], dtype=self._tf_dtype)\n                mu_new = tf.Variable(t_new, name=\'mu_new\')\n                tf.summary.histogram(\'mu_hist\', mu)\n                self._mu.append(mu)\n                self._mu_new.append(mu_new)\n\n        # initialize running means of hidden activations means\n        with tf.name_scope(\'hidden_means_accumulators\'):\n            for i in range(self.n_layers_):\n                T = tf.Variable(tf.zeros([self.n_hiddens_[i]], dtype=self._tf_dtype), name=\'q_means\')\n                self._q_means.append(T)\n                S = tf.Variable(tf.zeros([self.n_hiddens_[i]], dtype=self._tf_dtype), name=\'mu_means\')\n                self._mu_means.append(S)\n\n        # initialize negative particles\n        with tf.name_scope(\'negative_particles\'):\n            if self._v_particle_init is not None:\n                t = tf.constant(self._v_particle_init, dtype=self._tf_dtype, name=\'v_init\')\n            else:\n                t = self._v_layer.init(batch_size=self._n_particles)\n            self._v = tf.Variable(t, dtype=self._tf_dtype, name=\'v\')\n            t_new = self._v_layer.init(batch_size=self._n_particles)\n            self._v_new = tf.Variable(t_new, dtype=self._tf_dtype, name=\'v_new\')\n\n            for i in range(self.n_layers_):\n                with tf.name_scope(\'h_particle\'):\n                    if self._h_particles_init is not None:\n                        q = tf.constant(self._h_particles_init[i],\n                                        shape=[self.n_particles, self.n_hiddens_[i]],\n                                        dtype=self._tf_dtype, name=\'h_init\')\n                    else:\n                        q = self._h_layers[i].init(batch_size=self._n_particles)\n                    h = tf.Variable(q, dtype=self._tf_dtype, name=\'h\')\n                    q_new = self._h_layers[i].init(batch_size=self._n_particles)\n                    h_new = tf.Variable(q_new, dtype=self._tf_dtype, name=\'h_new\')\n                    self._H.append(h)\n                    self._H_new.append(h_new)\n\n    def _make_gibbs_step(self, v, H, v_new, H_new, update_v=True, sample=True):\n        """"""Compute one Gibbs step.""""""\n        with tf.name_scope(\'gibbs_step\'):\n\n            # update first hidden layer\n            with tf.name_scope(\'means_h0_hat_given_v_h1\'):\n                T = tf.matmul(v, self._W[0])\n                if self.n_layers_ >= 2:\n                    T += tf.matmul(a=H[1], b=self._W[1], transpose_b=True)\n                H_new[0] = self._h_layers[0].activation(T, self._hb[0])\n            if sample and self.sample_h_states[0]:\n                with tf.name_scope(\'sample_h0_hat_given_v_h1\'):\n                    H_new[0] = self._h_layers[0].sample(means=H_new[0])\n\n            # update the intermediate hidden layers if any\n            for i in range(1, self.n_layers_ - 1):\n                with tf.name_scope(\'means_h{0}_hat_given_h{1}_hat_h{2}\'.format(i, i - 1, i + 1)):\n                    T1 = tf.matmul(H_new[i - 1], self._W[i])\n                    T2 = tf.matmul(a=H[i + 1], b=self._W[i + 1], transpose_b=True)\n                    H_new[i] = self._h_layers[i].activation(T1 + T2, self._hb[i])\n                if sample and self.sample_h_states[i]:\n                    with tf.name_scope(\'sample_h{0}_hat_given_h{1}_hat_h{2}\'.format(i, i - 1, i + 1)):\n                        H_new[i] = self._h_layers[i].sample(means=H_new[i])\n\n            # update last hidden layer\n            if self.n_layers_ >= 2:\n                with tf.name_scope(\'means_h{0}_hat_given_h{1}_hat\'.format(self.n_layers_ - 1, self.n_layers_ - 2)):\n                    T = tf.matmul(H_new[-2], self._W[-1])\n                    H_new[-1] = self._h_layers[-1].activation(T, self._hb[-1])\n                if sample and self.sample_h_states[-1]:\n                    with tf.name_scope(\'sample_h{0}_hat_given_h{1}_hat\'.format(self.n_layers_ - 1, self.n_layers_ - 2)):\n                        H_new[-1] = self._h_layers[-1].sample(means=H_new[-1])\n\n            # update visible layer if needed\n            if update_v:\n                with tf.name_scope(\'means_v_hat_given_h0_hat\'):\n                    T = tf.matmul(a=H_new[0], b=self._W[0], transpose_b=True)\n                    v_new = self._v_layer.activation(T, self._vb)\n                if sample and self.sample_v_states:\n                    with tf.name_scope(\'sample_v_hat_given_h_hat\'):\n                        v_new = self._v_layer.sample(means=v_new)\n\n        return v, H, v_new, H_new\n\n    def _make_mf(self):\n        """"""Run mean-field updates for current mini-batch""""""\n        with tf.name_scope(\'mean_field\'):\n            # initialize mu_new using approximate inference\n            # as suggested in [1]\n            init_ops = []\n            T = None\n            for i in range(self.n_layers_):\n                if i == 0:\n                    T = 2. * tf.matmul(self._X_batch, self._W[0])\n                else:\n                    T = tf.matmul(T, self._W[i])\n                    if i < self.n_layers_ - 1:\n                        T *= 2.\n                T = self._h_layers[i].activation(T, self._hb[i])\n                q_new = tf.identity(T, name=\'approx_inference\')\n                init_op = tf.assign(self._mu_new[i], q_new)\n                init_ops.append(init_op)\n\n            # run mean-field updates until convergence\n            def cond(step, max_step, tol, X_batch, mu, mu_new):\n                c1 = step < max_step\n                c2 = tf.reduce_max([tf.norm(u - v, ord=np.inf) for u, v in zip(mu, mu_new)]) > tol\n                return tf.logical_and(c1, c2)\n\n            def body(step, max_step, tol, X_batch, mu, mu_new):\n                _, mu, _, mu_new = self._make_gibbs_step(X_batch, mu, X_batch, mu_new,\n                                                         update_v=False, sample=False)\n                return step + 1, max_step, tol, X_batch, mu_new, mu  # swap mu and mu_new\n\n            with tf.control_dependencies(init_ops):  # make sure mu\'s are initialized\n                i = tf.constant(0)\n                n_mf_updates, _, _, _, mu, _ = \\\n                    tf.while_loop(cond=cond, body=body,\n                                  loop_vars=[i,\n                                             self._max_mf_updates,\n                                             self._mf_tol,\n                                             self._X_batch,\n                                             self._mu, self._mu_new],\n                                  shape_invariants=[i.get_shape(),\n                                                    self._max_mf_updates.get_shape(),\n                                                    self._mf_tol.get_shape(),\n                                                    self._X_batch.get_shape(),\n                                                    [tf.TensorShape([None, n]) for n in self.n_hiddens_],\n                                                    [tf.TensorShape([None, n]) for n in self.n_hiddens_]],\n                                  back_prop=False,\n                                  parallel_iterations=1,\n                                  name=\'mean_field_updates\')\n                mu_updates = [self._mu[i].assign(mu[i]) for i in range(self.n_layers_)]\n            return n_mf_updates, mu_updates\n\n    def _make_particles_update(self, n_steps=None, sample=True, G_fed=False):\n        """"""Update negative particles by running Gibbs sampler\n        for specified number of steps.\n        """"""\n        if n_steps is None:\n            n_steps = self._n_gibbs_steps\n\n        with tf.name_scope(\'gibbs_chain\'):\n            def cond(step, max_step, v, H, v_new, H_new):\n                return step < max_step\n\n            def body(step, max_step, v, H, v_new, H_new):\n                v, H, v_new, H_new = self._make_gibbs_step(v, H, v_new, H_new,\n                                                           update_v=True, sample=sample)\n                return step + 1, max_step, v_new, H_new, v, H  # swap particles\n\n            _, _, v, H, v_new, H_new = \\\n                tf.while_loop(cond=cond, body=body,\n                              loop_vars=[tf.constant(0),\n                                         n_steps,\n                                         self._v, self._H,\n                                         self._v_new, self._H_new],\n                              parallel_iterations=1,\n                              back_prop=False)\n\n            v_update = self._v.assign(v)\n            v_new_update = self._v_new.assign(v_new)\n            H_updates = [self._H[i].assign(H[i]) for i in range(self.n_layers_)]\n            H_new_updates = [self._H_new[i].assign(H_new[i]) for i in range(self.n_layers_)]\n        return v_update, H_updates, v_new_update, H_new_updates\n\n    def _apply_max_norm(self, T):\n        T_norm = tf.norm(T, axis=0)\n        return T * tf.minimum(T_norm, self._max_norm) / tf.maximum(T_norm, 1e-8), T_norm\n\n    def _make_train_op(self):\n        # run mean-field updates for current mini-batch\n        n_mf_updates, mu_updates = self._make_mf()\n\n        # update negative particles by running Gibbs sampler\n        # for specified number of steps\n        v_update, H_updates, v_new_update, H_new_updates = self._make_particles_update()\n\n        with tf.control_dependencies([v_update, v_new_update] + H_updates + H_new_updates + mu_updates):\n\n            # encoded data, used by the transform method\n            with tf.name_scope(\'transform\'):\n                transform_op = tf.identity(self._mu[-1])\n                tf.add_to_collection(\'transform_op\', transform_op)\n\n            # visualize particles\n            if self.display_particles:\n                with tf.name_scope(\'particles_visualization\'):\n                    v_means, H_means, _, _ = self._make_particles_update(sample=False)\n\n                    V = v_means[:self.display_particles, :]\n                    V_display = tf.reshape(V, [self.display_particles, self.v_shape[2],\n                                               self.v_shape[0], self.v_shape[1]])\n                    V_display = tf.transpose(V_display, [0, 2, 3, 1])\n                    V_display = tf.cast(V_display, tf.float32)\n                    tf.summary.image(\'visible_activations_means\', V_display, max_outputs=self.display_filters)\n\n                    for i in range(self.n_layers_):\n                        h_means_display = H_means[i][:, :self.display_particles]\n                        h_means_display = tf.cast(h_means_display, tf.float32)\n                        h_means_display = tf.expand_dims(h_means_display, 0)\n                        h_means_display = tf.expand_dims(h_means_display, -1)\n                        tf.summary.image(\'hidden_activations_means\', h_means_display)\n\n            # compute gradients estimates (= positive - negative associations)\n            with tf.name_scope(\'grads_estimates\'):\n                # visible bias\n                with tf.name_scope(\'dvb\'):\n                    dvb = tf.reduce_mean(self._X_batch, axis=0) - tf.reduce_mean(self._v, axis=0)\n\n                dW = []\n                # first layer of weights\n                with tf.name_scope(\'dW\'):\n                    dW_0_positive = tf.matmul(a=self._X_batch, b=self._mu[0], transpose_a=True) / self._N\n                    dW_0_negative = tf.matmul(a=self._v, b=self._H[0], transpose_a=True) / self._M\n                    dW_0 = (dW_0_positive - dW_0_negative) - self._l2 * self._W[0]\n                    dW.append(dW_0)\n\n                # ... rest of them\n                for i in range(1, self.n_layers_):\n                    with tf.name_scope(\'dW\'):\n                        dW_i_positive = tf.matmul(a=self._mu[i - 1], b=self._mu[i], transpose_a=True) / self._N\n                        dW_i_negative = tf.matmul(a=self._H[i - 1], b=self._H[i], transpose_a=True) / self._M\n                        dW_i = (dW_i_positive - dW_i_negative) - self._l2 * self._W[i]\n                        dW.append(dW_i)\n\n                dhb = []\n                # hidden biases\n                for i in range(self.n_layers_):\n                    with tf.name_scope(\'dhb\'):\n                        dhb_i = tf.reduce_mean(self._mu[i], axis=0) - tf.reduce_mean(self._H[i], axis=0)\n                        dhb.append(dhb_i)\n\n            # apply sparsity targets if needed\n            with tf.name_scope(\'sparsity_targets\'):\n                for i in range(self.n_layers_):\n                    q_means = tf.reduce_sum(self._H[i], axis=0)\n                    q_update = self._q_means[i].assign(self._sparsity_damping * self._q_means[i] + \\\n                                                       (1 - self._sparsity_damping) * q_means[i])\n                    mu_means = tf.reduce_sum(self._mu[i], axis=0)\n                    mu_update = self._mu_means[i].assign(self._sparsity_damping * self._mu_means[i] + \\\n                                                        (1 - self._sparsity_damping) * mu_means[i])\n                    sparsity_penalty = self._sparsity_costs[i] * (q_update - self._sparsity_targets[i])\n                    sparsity_penalty += self._sparsity_costs[i] * (mu_update - self._sparsity_targets[i])\n                    dW[i] -= sparsity_penalty\n                    dhb[i] -= sparsity_penalty\n\n            # update parameters\n            with tf.name_scope(\'momentum_updates\'):\n                with tf.name_scope(\'dvb\'):\n                    dvb_update = self._dvb.assign(self._learning_rate * (self._momentum * self._dvb + dvb))\n                    vb_update = self._vb.assign_add(dvb_update)\n\n                W_updates = []\n                W_norms = []\n                for i in range(self.n_layers_):\n                    with tf.name_scope(\'dW\'):\n                        dW_update = self._dW[i].assign(self._learning_rate * (self._momentum * self._dW[i] + dW[i]))\n                        W_update = self._W[i] + dW_update\n                        with tf.name_scope(\'max_norm\'):\n                            W_new, W_norm = self._apply_max_norm(W_update)\n                        W_update = self._W[i].assign(W_new)\n                        W_norms.append(tf.minimum(tf.reduce_max(W_norm), self._max_norm))\n                        W_updates.append(W_update)\n\n                hb_updates = []\n                for i in range(self.n_layers_):\n                    with tf.name_scope(\'dhb\'):\n                        dhb_update = self._dhb[i].assign(self._learning_rate * (self._momentum * self._dhb[i] + dhb[i]))\n                        hb_update = self._hb[i].assign_add(dhb_update)\n                        hb_updates.append(hb_update)\n\n            # assemble train_op\n            with tf.name_scope(\'training_step\'):\n                train_op = tf.group(vb_update,\n                                    tf.group(*W_updates),\n                                    tf.group(*hb_updates))\n                tf.add_to_collection(\'train_op\', train_op)\n\n            # compute metrics\n            with tf.name_scope(\'mean_squared_reconstruction_error\'):\n                T = tf.matmul(a=self._mu[0], b=self._W[0], transpose_b=True)\n                v_means = self._v_layer.activation(T, self._vb)\n                v_means = tf.identity(v_means, name=\'x_reconstruction\')\n                msre = tf.reduce_mean(tf.square(self._X_batch - v_means))\n                tf.add_to_collection(\'msre\', msre)\n\n            tf.add_to_collection(\'reconstruction\', v_means)\n            tf.add_to_collection(\'n_mf_updates\', n_mf_updates)\n\n            # collect summaries\n            tf.summary.scalar(\'mean_squared_recon_error\', msre)\n            tf.summary.scalar(\'n_mf_updates\', n_mf_updates)\n            for i in range(self.n_layers_):\n                tf.summary.scalar(\'W_norm\', W_norms[i])\n\n    def _make_sample_v(self):\n        with tf.name_scope(\'sample_v\'):\n            v_update, H_updates, v_new_update, H_new_updates = \\\n                self._make_particles_update(n_steps=self._n_gibbs_steps)\n            with tf.control_dependencies([v_update, v_new_update] + H_updates + H_new_updates):\n                v_means, _, _, _ = self._make_particles_update(sample=False)\n                sample_v = self._v.assign(v_means)\n        tf.add_to_collection(\'sample_v\', sample_v)\n\n    def _unnormalized_log_prob_H0(self, x, beta):\n        T1 = tf.einsum(\'ij,j->i\', x, self._hb[0])\n        T1 *= beta\n        log_p = T1\n        T2 = tf.matmul(x, b=self._W[0], transpose_b=True) + self._vb\n        T2 *= beta\n        log_p += tf.reduce_sum(tf.nn.softplus(T2), axis=1)\n        T3 = tf.matmul(x, self._W[1]) + self._hb[1]\n        T3 *= beta\n        log_p += tf.reduce_sum(tf.nn.softplus(T3), axis=1)\n        return log_p\n\n    def _make_ais_next_sample(self, x, beta):\n        def cond(step, max_step, x):\n            return step < max_step\n\n        def body(step, max_step, x):\n            # v_hat <- P(v|h=x)\n            T1 = tf.matmul(a=x, b=self._W[0], transpose_b=True)\n            v = self._v_layer.activation(beta * T1, beta * self._vb)\n            if self.sample_v_states:\n                v = self._v_layer.sample(means=v)\n\n            # h2_hat <- P(h2|h=x)\n            T2 = tf.matmul(x, self._W[1])\n            h2 = self._h_layers[1].activation(beta * T2, beta * self._hb[1])\n            if self.sample_h_states[1]:\n                h2 = self._h_layers[1].sample(means=h2)\n\n            # x_hat <- P(h|v=v_hat, h2=h2_hat)\n            T3 = tf.matmul(v, self._W[0])\n            T4 = tf.matmul(a=h2, b=self._W[1], transpose_b=True)\n            x_hat = self._h_layers[0].activation(beta * (T3 + T4), beta * self._hb[0])\n            if self.sample_h_states[0]:\n                x_hat = self._h_layers[0].sample(means=x_hat)\n\n            return step + 1, max_step, x_hat\n\n        _, _, x_new = tf.while_loop(cond=cond, body=body,\n                                    loop_vars=[tf.constant(0),\n                                               self._n_gibbs_steps,\n                                               x],\n                                    parallel_iterations=1,\n                                    back_prop=False)\n        return x_new\n\n    def _make_ais(self):\n        with tf.name_scope(\'annealed_importance_sampling\'):\n\n            # x_0 ~ Ber(0.5) of size (M, H_1)\n            logits = tf.zeros([self._n_ais_runs, self._n_hiddens[0]])\n            T = Bernoulli(logits=logits).sample(seed=self.make_random_seed())\n            x_0 = tf.cast(T, dtype=self._tf_dtype)\n\n            # x_1 ~ T_1(x_1 | x_0)\n            x_1 = self._make_ais_next_sample(x_0, self._delta_beta)\n\n            # -log p_0(x_1)\n            log_Z = -self._unnormalized_log_prob_H0(x_1, 0.)\n\n            def cond(log_Z, x, beta, delta_beta):\n                return beta < 1. - delta_beta + 1e-5\n\n            def body(log_Z, x, beta, delta_beta):\n                # + log p_i(x_i)\n                log_Z += self._unnormalized_log_prob_H0(x, beta)\n                # x_{i + 1} ~ T_{i + 1}(x_{i + 1} | x_i)\n                x_new = self._make_ais_next_sample(x, beta + delta_beta)\n                # - log p_i(x_{i + 1})\n                log_Z -= self._unnormalized_log_prob_H0(x_new, beta)\n                return log_Z, x_new, beta + delta_beta, delta_beta\n\n            log_Z, x_M, _, _ = tf.while_loop(cond=cond, body=body,\n                                             loop_vars=[log_Z, x_1, self._delta_beta,\n                                                                    self._delta_beta],\n                                             back_prop=False,\n                                             parallel_iterations=1)\n            # + log p_M(x_M)\n            log_Z += self._unnormalized_log_prob_H0(x_M, 1.)\n\n            # + log(Z_0) = (V + H_1 + H_2) * log(2)\n            log_Z0 = self._n_visible + self._n_hiddens[0] + self._n_hiddens[1]\n            log_Z0 = tf.cast(log_Z0, dtype=self._tf_dtype)\n            log_Z0 *= tf.cast(tf.log(2.), dtype=self._tf_dtype)\n            log_Z += log_Z0\n\n        tf.add_to_collection(\'log_Z\', log_Z)\n\n    def _make_log_proba(self):\n        with tf.name_scope(\'log_proba\'):\n\n            n_mf_updates, mu_updates = self._make_mf()\n            with tf.control_dependencies(mu_updates):\n                t1 = tf.matmul(self._X_batch, self._W[0])\n                minus_E = tf.reduce_sum(t1 * self._mu[0], axis=1)\n                t2 = tf.matmul(self._mu[0], self._W[1])\n                minus_E += tf.reduce_sum(t2 * self._mu[1], axis=1)\n                minus_E += tf.einsum(\'ij,j->i\', self._X_batch, self._vb)\n                minus_E += tf.einsum(\'ij,j->i\', self._mu[0], self._hb[0])\n                minus_E += tf.einsum(\'ij,j->i\', self._mu[1], self._hb[1])\n\n                s1 = tf.clip_by_value(self._mu[0], 1e-7, 1. - 1e-7)\n                s2 = tf.clip_by_value(self._mu[1], 1e-7, 1. - 1e-7)\n                S1 = -s1 * tf.log(s1) - (1. - s1) * tf.log(1. - s1)\n                S2 = -s2 * tf.log(s2) - (1. - s2) * tf.log(1. - s2)\n                H = tf.reduce_sum(S1, axis=1) + tf.reduce_sum(S2, axis=1)\n\n                log_p = minus_E + H\n\n        tf.add_to_collection(\'log_proba\', log_p)\n\n    def _make_tf_model(self):\n        self._make_constants()\n        self._make_placeholders()\n        self._make_vars()\n\n        self._make_train_op()\n        self._make_sample_v()\n        self._make_ais()\n        self._make_log_proba()\n\n    def _make_tf_feed_dict(self, X_batch=None, delta_beta=None, n_ais_runs=None, n_gibbs_steps=None):\n        d = {}\n        d[\'learning_rate\'] = self.learning_rate[min(self.epoch_, len(self.learning_rate) - 1)]\n        d[\'momentum\'] = self.momentum[min(self.epoch_, len(self.momentum) - 1)]\n\n        if X_batch is not None:\n            d[\'X_batch\'] = X_batch\n        if delta_beta is not None:\n            d[\'delta_beta\'] = delta_beta\n        if n_ais_runs is not None:\n            d[\'n_ais_runs\'] = n_ais_runs\n        if n_gibbs_steps is not None:\n            d[\'n_gibbs_steps\'] = n_gibbs_steps\n        else:\n            d[\'n_gibbs_steps\'] = self.n_gibbs_steps[min(self.epoch_, len(self.n_gibbs_steps) - 1)]\n\n        # prepend name of the scope, and append \':0\'\n        feed_dict = {}\n        for k, v in d.items():\n            feed_dict[\'input_data/{0}:0\'.format(k)] = v\n        return feed_dict\n\n    def _train_epoch(self, X):\n        train_msres, train_n_mf_updates = [], []\n        for X_batch in batch_iter(X, self.batch_size, verbose=self.verbose):\n            self.iter_ += 1\n            if self.iter_ % self.train_metrics_every_iter == 0:\n                msre, n_mf_upds, _, s = self._tf_session.run([self._msre, self._n_mf_updates,\n                                                              self._train_op, self._tf_merged_summaries],\n                                                              feed_dict=self._make_tf_feed_dict(X_batch))\n                train_msres.append(msre)\n                train_n_mf_updates.append(n_mf_upds)\n                self._tf_train_writer.add_summary(s, self.iter_)\n            else:\n                self._tf_session.run(self._train_op,\n                                     feed_dict=self._make_tf_feed_dict(X_batch))\n        return (np.mean(train_msres) if train_msres else None,\n                np.mean(train_n_mf_updates) if train_n_mf_updates else None)\n\n    def _run_val_metrics(self, X_val):\n        val_msres, val_n_mf_updates = [], []\n        for X_vb in batch_iter(X_val, batch_size=self.batch_size):\n            msre, n_mf_upds = self._tf_session.run([self._msre, self._n_mf_updates],\n                                                    feed_dict=self._make_tf_feed_dict(X_vb))\n            val_msres.append(msre)\n            val_n_mf_updates.append(n_mf_upds)\n        mean_msre = np.mean(val_msres)\n        mean_n_mf_updates = np.mean(val_n_mf_updates)\n        s = summary_pb2.Summary(value=[\n            summary_pb2.Summary.Value(tag=\'mean_squared_recon_error\', simple_value=mean_msre),\n            summary_pb2.Summary.Value(tag=\'n_mf_updates\', simple_value=mean_n_mf_updates),\n        ])\n        self._tf_val_writer.add_summary(s, self.iter_)\n        return mean_msre, mean_n_mf_updates\n\n    def _fit(self, X, X_val=None, *args, **kwargs):\n        # load ops requested\n        self._train_op = tf.get_collection(\'train_op\')[0]\n        self._msre = tf.get_collection(\'msre\')[0]\n        self._n_mf_updates = tf.get_collection(\'n_mf_updates\')[0]\n\n        # main loop\n        val_msre, val_n_mf_updates = None, None\n        for self.epoch_ in epoch_iter(start_epoch=self.epoch_, max_epoch=self.max_epoch,\n                                      verbose=self.verbose):\n            train_msre, train_n_mf_updates = self._train_epoch(X)\n\n            # run validation metrics if needed\n            if X_val is not None and self.epoch_ % self.val_metrics_every_epoch == 0:\n                val_msre, val_n_mf_updates = self._run_val_metrics(X_val)\n\n            # print progress\n            if self.verbose:\n                s = ""epoch: {0:{1}}/{2}"".format(self.epoch_, len(str(self.max_epoch)), self.max_epoch)\n                if train_msre:\n                    s += ""; msre: {0:.5f}"".format(train_msre)\n                if train_n_mf_updates:\n                    s += ""; n_mf_upds: {0:.1f}"".format(train_n_mf_updates)\n                if val_msre:\n                    s += ""; val.msre: {0:.5f}"".format(val_msre)\n                if val_n_mf_updates:\n                    s += ""; val.n_mf_upds: {0:.1f}"".format(val_n_mf_updates)\n                write_during_training(s)\n\n            # save if needed\n            if self.save_after_each_epoch:\n                self._save_model(global_step=self.epoch_)\n\n    @run_in_tf_session()\n    def transform(self, X, np_dtype=None):\n        """"""Compute hidden units\' (from last layer) activation probabilities.""""""\n        np_dtype = np_dtype or self._np_dtype\n\n        self._transform_op = tf.get_collection(\'transform_op\')[0]\n        G = np.zeros((len(X), self.n_hiddens_[-1]), dtype=np_dtype)\n        start = 0\n        for X_b in batch_iter(X, batch_size=self.batch_size,\n                              verbose=self.verbose, desc=\'transform\'):\n            G_b = self._transform_op.eval(feed_dict=self._make_tf_feed_dict(X_b))\n            G[start:(start + self.batch_size)] = G_b\n            start += self.batch_size\n        return G\n\n    @run_in_tf_session(update_seed=True)\n    def reconstruct(self, X):\n        """"""Compute p(v|h_0=q, h...)=p(v|h_0=q), where q=p(h_0|v=x)""""""\n        self._reconstruction = tf.get_collection(\'reconstruction\')[0]\n        X_recon = np.zeros_like(X)\n        start = 0\n        for X_b in batch_iter(X, batch_size=self.batch_size,\n                              verbose=self.verbose, desc=\'reconstruction\'):\n            X_recon_b = self._reconstruction.eval(feed_dict=self._make_tf_feed_dict(X_b))\n            X_recon[start:(start + self.batch_size)] = X_recon_b\n            start += self.batch_size\n        return X_recon\n\n    @run_in_tf_session(update_seed=True)\n    def sample_v(self, n_gibbs_steps=0, save_model=False):\n        """"""Compute visible particle activation probabilities\n        after `n_gibbs_steps` chain iterations.\n        """"""\n        self._sample_v = tf.get_collection(\'sample_v\')[0]\n        v = self._sample_v.eval(feed_dict=self._make_tf_feed_dict(n_gibbs_steps=n_gibbs_steps))\n        if save_model:\n            self.n_samples_generated_ += n_gibbs_steps\n            self._save_model()\n        return v\n\n    @run_in_tf_session(update_seed=True)\n    def log_Z(self, n_betas=100, n_runs=100, n_gibbs_steps=5):\n        """"""Estimate log partition function using Annealed Importance Sampling.\n        Currently implemented only for 2-layer binary BM.\n        AIS is run on a state space x = {h_1} with v and h_2\n        analytically summed out, as in [1] and using formulae from [4].\n        To obtain reasonable estimate, parameter `n_betas` should be at least 10000 or more.\n\n        Parameters\n        ----------\n        n_betas : >1 int\n            Number of intermediate distributions.\n        n_runs : positive int\n            Number of AIS runs.\n        n_gibbs_steps : positive int\n            Number of Gibbs steps per transition.\n\n        Returns\n        -------\n        log_mean, (log_low, log_high) : float\n            `log_mean` = log(Z_mean)\n            `log_low`  = log(Z_mean - std(Z))\n            `log_high` = log(Z_mean + std(Z))\n        values : (`n_runs`,) np.ndarray\n            All estimates.\n        """"""\n        assert self.n_layers_ == 2\n        for L in [self._v_layer] + self._h_layers:\n            assert isinstance(L, BernoulliLayer)\n\n        self._log_Z = tf.get_collection(\'log_Z\')[0]\n        values = self._tf_session.run(self._log_Z,\n                                      feed_dict=self._make_tf_feed_dict(delta_beta=1./n_betas,\n                                                                        n_ais_runs=n_runs,\n                                                                        n_gibbs_steps=n_gibbs_steps))\n\n        log_mean = log_mean_exp(values)\n        log_std  = log_std_exp(values, log_mean_exp_x=log_mean)\n        log_high = log_sum_exp([log_std, log_mean])\n        log_low  = log_diff_exp([log_std, log_mean])[0]\n        return log_mean, (log_low, log_high), values\n\n    @run_in_tf_session()\n    def log_proba(self, X_test, log_Z):\n        """"""Estimate variational lower-bound on a test set, as in [5].\n        Currently implemented only for 2-layer binary BM.\n        """"""\n        assert self.n_layers_ == 2\n        for L in [self._v_layer] + self._h_layers:\n            assert isinstance(L, BernoulliLayer)\n\n        self._log_proba = tf.get_collection(\'log_proba\')[0]\n        P = np.zeros(len(X_test))\n        start = 0\n        for X_b in batch_iter(X_test, batch_size=self.batch_size, verbose=self.verbose):\n            P_b = self._log_proba.eval(feed_dict=self._make_tf_feed_dict(X_b))\n            P[start:(start + self.batch_size)] = P_b\n            start += self.batch_size\n        return P - log_Z\n\n\nif __name__ == \'__main__\':\n    # run corresponding tests\n    from boltzmann_machines.utils.testing import run_tests\n    run_tests(__file__)\n'"
boltzmann_machines/ebm.py,1,"b'from .base import TensorFlowModel\n\n\nclass EnergyBasedModel(TensorFlowModel):\n    """"""A generic Energy-based model with hidden variables.""""""\n    def __init__(self, *args, **kwargs):\n        super(EnergyBasedModel, self).__init__(*args, **kwargs)\n\n    def _free_energy(self, v):\n        """"""\n        Compute (average) free energy of a visible vectors `v`.\n\n        Parameters\n        ----------\n        v : (batch_size, n_visible) tf.Tensor\n        """"""\n        raise NotImplementedError(\'`free_energy` is not implemented\')\n'"
boltzmann_machines/layers.py,13,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.distributions import Bernoulli, Multinomial, Normal\n\nfrom boltzmann_machines.base import DtypeMixin\n\n\nclass BaseLayer(DtypeMixin):\n    """"""Class encapsulating one layer of stochastic units.""""""\n    def __init__(self, n_units, *args, **kwargs):\n        super(BaseLayer, self).__init__(*args, **kwargs)\n        self.n_units = n_units\n\n    def init(self, batch_size):\n        """"""Randomly initialize states according to their distribution.""""""\n        raise NotImplementedError(\'`init` is not implemented\')\n\n    def activation(self, x, b):\n        """"""Compute activation of states according to their distribution.\n\n        Parameters\n        ----------\n        x : (n_units,) tf.Tensor\n            Total input received (excluding bias).\n        b : (n_units,) tf.Tensor\n            Bias.\n        """"""\n        raise NotImplementedError(\'`activation` is not implemented\')\n\n    def _sample(self, means):\n        """"""Sample states of the units by combining output from 2 previous functions.""""""\n        raise NotImplementedError(\'`sample` is not implemented\')\n\n    def sample(self, means):\n        T = self._sample(means).sample()\n        return tf.cast(T, dtype=self._tf_dtype)\n\n\nclass BernoulliLayer(BaseLayer):\n    def __init__(self, *args, **kwargs):\n        super(BernoulliLayer, self).__init__(*args, **kwargs)\n\n    def init(self, batch_size, random_seed=None):\n        return tf.random_uniform([batch_size, self.n_units], minval=0., maxval=1.,\n                                 dtype=self._tf_dtype, seed=random_seed, name=\'bernoulli_init\')\n\n    def activation(self, x, b):\n        return tf.nn.sigmoid(x + b)\n\n    def _sample(self, means):\n        return Bernoulli(probs=means)\n\n\nclass MultinomialLayer(BaseLayer):\n    def __init__(self, n_samples=100, *args, **kwargs):\n        super(MultinomialLayer, self).__init__(*args, **kwargs)\n        self.n_samples = float(n_samples)\n\n    def init(self, batch_size, random_seed=None):\n        t = tf.random_uniform([batch_size, self.n_units], minval=0., maxval=1.,\n                              dtype=self._tf_dtype, seed=random_seed)\n        t /= tf.reduce_sum(t)\n        return tf.identity(t, name=\'multinomial_init\')\n\n    def activation(self, x, b):\n        return self.n_samples * tf.nn.softmax(x + b)\n\n    def _sample(self, means):\n        probs = tf.to_float(means / tf.reduce_sum(means))\n        return Multinomial(total_count=self.n_samples, probs=probs)\n\n\nclass GaussianLayer(BaseLayer):\n    def __init__(self, sigma, *args, **kwargs):\n        super(GaussianLayer, self).__init__(*args, **kwargs)\n        self.sigma = np.asarray(sigma)\n\n    def init(self, batch_size, random_seed=None):\n        t = tf.random_normal([batch_size, self.n_units],\n                             dtype=self._tf_dtype, seed=random_seed)\n        t = tf.multiply(t, self.sigma, name=\'gaussian_init\')\n        return t\n\n    def activation(self, x, b):\n        t = x * self.sigma + b\n        return t\n\n    def _sample(self, means):\n        return Normal(loc=means, scale=tf.cast(self.sigma, dtype=self._tf_dtype))\n'"
examples/dbm_cifar.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n""""""\nTrain 3072-7800-512 Gaussian-Bernoulli-Multinomial DBM with pre-training\non CIFAR-10, augmented (x10) using shifts by 1 pixel in all directions\nand horizontal mirroring.\n\nGaussian RBM is initialized from 26 small RBMs trained on patches 8x8\nof images, as in [1]. Multinomial RBM trained with increasing k in CD-k and decreasing\nlearning rate over time.\n\nPer sample validation mean reconstruction error for DBM monotonically\ndecreases during training from ~0.3 to ~0.11 at the end.\n\nThe training took approx. 26 x 35m + 5h 52m + 4h 55m + 11h 11m =\n = 1d 13h 8m on GTX 1060.\n\nI also trained for longer with options\n```\n--small-l2 2e-3 --small-epochs 120 --small-sparsity-cost 0 \\\n--increase-n-gibbs-steps-every 20 --epochs 80 72 200 \\\n--l2 2e-3 0.01 1e-8 --max-mf-updates 70\n```\nwith a decrease of MSRE from ~0.6 to ~0.147 at the end and it took\n~3d 1h 41m on GTX 1060.\n\nNote that DBM is trained without centering.\n\nReferences\n----------\n[1] A. Krizhevsky and G. Hinton. Learning multiple layers of features\n    from tine images. 2009.\n""""""\nprint __doc__\n\nimport os\nimport argparse\nimport numpy as np\nfrom keras import regularizers\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom keras.initializers import glorot_uniform\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, BatchNormalization as BN\nfrom sklearn.metrics import accuracy_score\n\nimport env\nfrom boltzmann_machines import DBM\nfrom boltzmann_machines.rbm import GaussianRBM, MultinomialRBM\nfrom boltzmann_machines.utils import (RNG, Stopwatch,\n                                      one_hot, one_hot_decision_function, unhot)\nfrom boltzmann_machines.utils.augmentation import shift, horizontal_mirror\nfrom boltzmann_machines.utils.dataset import (load_cifar10,\n                                              im_flatten, im_unflatten)\nfrom boltzmann_machines.utils.optimizers import MultiAdam\n\n\ndef make_augmentation(X_train, y_train, n_train, args):\n    X_aug = None\n    X_aug_path = os.path.join(args.data_path, \'X_aug.npy\')\n    y_train = y_train.tolist() * 10\n    RNG(seed=1337).shuffle(y_train)\n\n    augment = True\n    if os.path.isfile(X_aug_path):\n        print ""\\nLoading augmented data ...""\n        X_aug = np.load(X_aug_path)\n        print ""Checking augmented data ...""\n        if len(X_aug) == 10 * n_train:\n            augment = False\n\n    if augment:\n        print ""\\nAugmenting data ...""\n        s = Stopwatch(verbose=True).start()\n\n        X_aug = np.zeros((10 * n_train, 32, 32, 3), dtype=np.float32)\n        X_train = im_unflatten(X_train)\n        X_aug[:n_train] = X_train\n        for i in xrange(n_train):\n            for k, offset in enumerate((\n                    ( 1,  0),\n                    (-1,  0),\n                    ( 0,  1),\n                    ( 0, -1)\n            )):\n                img = X_train[i].copy()\n                X_aug[(k + 1) * n_train + i] = shift(img, offset=offset)\n        for i in xrange(5 * n_train):\n            X_aug[5 * n_train + i] = horizontal_mirror(X_aug[i].copy())\n\n        # shuffle once again\n        RNG(seed=1337).shuffle(X_aug)\n\n        # convert to \'uint8\' type to save disk space\n        X_aug *= 255.\n        X_aug = X_aug.astype(\'uint8\')\n\n        # flatten to (10 * `n_train`, 3072) shape\n        X_aug = im_flatten(X_aug)\n\n        # save to disk\n        np.save(X_aug_path, X_aug)\n\n        s.elapsed()\n        print ""\\n""\n\n    return X_aug, y_train\n\ndef make_small_rbms((X_train, X_val), args):\n    X_train = im_unflatten(X_train)\n    X_val = im_unflatten(X_val)\n\n    small_rbm_config = dict(n_visible=8 * 8 * 3,\n                            n_hidden=300,\n                            sigma=1.,\n                            W_init=0.001,\n                            vb_init=0.,\n                            hb_init=0.,\n                            n_gibbs_steps=1,\n                            learning_rate=args.small_lr,\n                            momentum=np.geomspace(0.5, 0.9, 8),\n                            max_epoch=args.small_epochs,\n                            batch_size=args.small_batch_size,\n                            l2=args.small_l2,\n                            sample_v_states=True,\n                            sample_h_states=True,\n                            sparsity_target=args.small_sparsity_target,\n                            sparsity_cost=args.small_sparsity_cost,\n                            dbm_first=True,  # !!!\n                            metrics_config=dict(\n                                msre=True,\n                                feg=True,\n                                train_metrics_every_iter=2000,\n                                val_metrics_every_epoch=2,\n                                feg_every_epoch=2,\n                                n_batches_for_feg=100,\n                            ),\n                            verbose=True,\n                            display_filters=12,\n                            display_hidden_activations=36,\n                            v_shape=(8, 8, 3),\n                            dtype=\'float32\',\n                            tf_saver_params=dict(max_to_keep=1))\n    small_rbms = []\n\n    # first 16 ...\n    for i in xrange(4):\n        for j in xrange(4):\n            rbm_id = 4 * i + j\n            rbm_dirpath = args.small_dirpath_prefix + str(rbm_id) + \'/\'\n\n            if os.path.isdir(rbm_dirpath):\n                print ""\\nLoading small RBM #{0} ...\\n\\n"".format(rbm_id)\n                rbm = GaussianRBM.load_model(rbm_dirpath)\n            else:\n                print ""\\nTraining small RBM #{0} ...\\n\\n"".format(rbm_id)\n                X_patches = X_train[:, 8 * i:8 * (i + 1),\n                                       8 * j:8 * (j + 1), :]\n                X_patches_val = X_val[:, 8 * i:8 * (i + 1),\n                                         8 * j:8 * (j + 1), :]\n                X_patches = im_flatten(X_patches)\n                X_patches_val = im_flatten(X_patches_val)\n\n                rbm = GaussianRBM(random_seed=9000 + rbm_id,\n                                  model_path=rbm_dirpath,\n                                  **small_rbm_config)\n                rbm.fit(X_patches, X_patches_val)\n            small_rbms.append(rbm)\n\n    # next 9 ...\n    for i in xrange(3):\n        for j in xrange(3):\n            rbm_id = 16 + 3 * i + j\n            rbm_dirpath = args.small_dirpath_prefix + str(rbm_id) + \'/\'\n\n            if os.path.isdir(rbm_dirpath):\n                print ""\\nLoading small RBM #{0} ...\\n\\n"".format(rbm_id)\n                rbm = GaussianRBM.load_model(rbm_dirpath)\n            else:\n                print ""\\nTraining small RBM #{0} ...\\n\\n"".format(rbm_id)\n                X_patches = X_train[:, 4 + 8 * i:4 + 8 * (i + 1),\n                                       4 + 8 * j:4 + 8 * (j + 1), :]\n                X_patches_val = X_val[:, 4 + 8 * i:4 + 8 * (i + 1),\n                                         4 + 8 * j:4 + 8 * (j + 1), :]\n                X_patches = im_flatten(X_patches)\n                X_patches_val = im_flatten(X_patches_val)\n\n                rbm = GaussianRBM(random_seed=args.small_random_seed + rbm_id,\n                                  model_path=rbm_dirpath,\n                                  **small_rbm_config)\n                rbm.fit(X_patches, X_patches_val)\n            small_rbms.append(rbm)\n\n    # ... and the last one\n    rbm_id = 25\n    rbm_dirpath = args.small_dirpath_prefix + str(rbm_id) + \'/\'\n\n    if os.path.isdir(rbm_dirpath):\n        print ""\\nLoading small RBM #{0} ...\\n\\n"".format(rbm_id)\n        rbm = GaussianRBM.load_model(rbm_dirpath)\n    else:\n        print ""\\nTraining small RBM #{0} ...\\n\\n"".format(rbm_id)\n        X_patches = X_train.copy()  # (N, 32, 32, 3)\n        X_patches = X_patches.transpose(0, 3, 1, 2)  # (N, 3, 32, 32)\n        X_patches = X_patches.reshape((-1, 3, 4, 8, 4, 8)).mean(axis=4).mean(axis=2)  # (N, 3, 8, 8)\n        X_patches = X_patches.transpose(0, 2, 3, 1)  # (N, 8, 8, 3)\n        X_patches = im_flatten(X_patches)  # (N, 8*8*3)\n\n        X_patches_val = X_val.copy()\n        X_patches_val = X_patches_val.transpose(0, 3, 1, 2)\n        X_patches_val = X_patches_val.reshape((-1, 3, 4, 8, 4, 8)).mean(axis=4).mean(axis=2)\n        X_patches_val = X_patches_val.transpose(0, 2, 3, 1)\n        X_patches_val = im_flatten(X_patches_val)\n\n        rbm = GaussianRBM(random_seed=9000 + rbm_id,\n                          model_path=rbm_dirpath,\n                          **small_rbm_config)\n        rbm.fit(X_patches, X_patches_val)\n    small_rbms.append(rbm)\n    return small_rbms\n\ndef make_large_weights(small_rbms):\n    W = np.zeros((300 * 26, 32, 32, 3), dtype=np.float32)\n    W[...] = RNG(seed=1234).rand(*W.shape) * 5e-6\n    vb = np.zeros((32, 32, 3))\n    hb = np.zeros(300 * 26)\n\n    for i in xrange(4):\n        for j in xrange(4):\n            rbm_id = 4 * i + j\n            weights = small_rbms[rbm_id].get_tf_params(scope=\'weights\')\n            W_small = weights[\'W\']\n            W_small = W_small.T  # (300, 192)\n            W_small = im_unflatten(W_small)  # (300, 8, 8, 3)\n            W[300 * rbm_id: 300 * (rbm_id + 1), 8 * i:8 * (i + 1),\n                                                8 * j:8 * (j + 1), :] = W_small\n            vb[8 * i:8 * (i + 1),\n               8 * j:8 * (j + 1), :] += im_unflatten(weights[\'vb\'])\n            hb[300 * rbm_id: 300 * (rbm_id + 1)] = weights[\'hb\']\n\n    for i in xrange(3):\n        for j in xrange(3):\n            rbm_id = 16 + 3 * i + j\n            weights = small_rbms[rbm_id].get_tf_params(scope=\'weights\')\n            W_small = weights[\'W\']\n            W_small = W_small.T\n            W_small = im_unflatten(W_small)\n            W[300 * rbm_id: 300 * (rbm_id + 1), 4 + 8 * i:4 + 8 * (i + 1),\n                                                4 + 8 * j:4 + 8 * (j + 1), :] = W_small\n            vb[4 + 8 * i:4 + 8 * (i + 1),\n               4 + 8 * j:4 + 8 * (j + 1), :] += im_unflatten(weights[\'vb\'])\n            hb[300 * rbm_id: 300 * (rbm_id + 1)] = weights[\'hb\']\n\n    weights = small_rbms[25].get_tf_params(scope=\'weights\')\n    W_small = weights[\'W\']\n    W_small = W_small.T\n    W_small = im_unflatten(W_small)\n    vb_small = im_unflatten(weights[\'vb\'])\n    for i in xrange(8):\n        for j in xrange(8):\n            U = W_small[:, i, j, :]\n            U = np.expand_dims(U, -1)\n            U = np.expand_dims(U, -1)\n            U = U.transpose(0, 2, 3, 1)\n            W[-300:, 4 * i:4 * (i + 1),\n                     4 * j:4 * (j + 1), :] = U / 16.\n            vb[4 * i:4 * (i + 1),\n               4 * j:4 * (j + 1), :] += vb_small[i, j, :].reshape((1, 1, 3)) / 16.\n            hb[-300:] = weights[\'hb\']\n\n    W = im_flatten(W)\n    W = W.T\n    vb /= 2.\n    vb[4:-4, 4:-4, :] /= 1.5\n    vb = im_flatten(vb)\n\n    return W, vb, hb\n\ndef make_grbm((X_train, X_val), small_rbms, args):\n    if os.path.isdir(args.grbm_dirpath):\n        print ""\\nLoading G-RBM ...\\n\\n""\n        grbm = GaussianRBM.load_model(args.grbm_dirpath)\n    else:\n        print ""\\nAssembling weights for large Gaussian RBM ...\\n\\n""\n        W, vb, hb = make_large_weights(small_rbms)\n\n        print ""\\nTraining G-RBM ...\\n\\n""\n        grbm = GaussianRBM(n_visible=32 * 32 * 3,\n                           n_hidden=300 * 26,\n                           sigma=1.,\n                           W_init=W,\n                           vb_init=vb,\n                           hb_init=hb,\n                           n_gibbs_steps=args.n_gibbs_steps[0],\n                           learning_rate=args.lr[0],\n                           momentum=np.geomspace(0.5, 0.9, 8),\n                           max_epoch=args.epochs[0],\n                           batch_size=args.batch_size[0],\n                           l2=args.l2[0],\n                           sample_v_states=True,\n                           sample_h_states=True,\n                           sparsity_target=0.1,\n                           sparsity_cost=1e-4,\n                           dbm_first=True,  # !!!\n                           metrics_config=dict(\n                               msre=True,\n                               feg=True,\n                               train_metrics_every_iter=1000,\n                               val_metrics_every_epoch=1,\n                               feg_every_epoch=2,\n                               n_batches_for_feg=50,\n                           ),\n                           verbose=True,\n                           display_filters=24,\n                           display_hidden_activations=36,\n                           v_shape=(32, 32, 3),\n                           random_seed=args.random_seed[0],\n                           dtype=\'float32\',\n                           tf_saver_params=dict(max_to_keep=1),\n                           model_path=args.grbm_dirpath)\n        grbm.fit(X_train, X_val)\n    return grbm\n\ndef make_mrbm((Q_train, Q_val), args):\n    if os.path.isdir(args.mrbm_dirpath):\n        print ""\\nLoading M-RBM ...\\n\\n""\n        mrbm = MultinomialRBM.load_model(args.mrbm_dirpath)\n    else:\n        print ""\\nTraining M-RBM ...\\n\\n""\n\n        epochs = args.epochs[1]\n        n_every = args.increase_n_gibbs_steps_every\n\n        n_gibbs_steps = np.arange(args.n_gibbs_steps[1],\n                                  args.n_gibbs_steps[1] + epochs / n_every)\n        learning_rate = args.lr[1] / np.arange(1, 1 + epochs / n_every)\n        n_gibbs_steps = np.repeat(n_gibbs_steps, n_every)\n        learning_rate = np.repeat(learning_rate, n_every)\n\n        mrbm = MultinomialRBM(n_visible=300 * 26,\n                              n_hidden=512,\n                              n_samples=512,\n                              W_init=0.001,\n                              hb_init=0.,\n                              vb_init=0.,\n                              n_gibbs_steps=n_gibbs_steps,\n                              learning_rate=learning_rate,\n                              momentum=np.geomspace(0.5, 0.9, 8),\n                              max_epoch=max(args.epochs[1], n_every),\n                              batch_size=args.batch_size[1],\n                              l2=args.l2[1],\n                              sample_h_states=True,\n                              sample_v_states=True,\n                              sparsity_target=0.2,\n                              sparsity_cost=1e-4,\n                              dbm_last=True,  # !!!\n                              metrics_config=dict(\n                                  msre=True,\n                                  pll=True,\n                                  feg=True,\n                                  train_metrics_every_iter=1000,\n                                  val_metrics_every_epoch=2,\n                                  feg_every_epoch=2,\n                                  n_batches_for_feg=50,\n                              ),\n                              verbose=True,\n                              display_filters=0,\n                              display_hidden_activations=100,\n                              random_seed=args.random_seed[1],\n                              dtype=\'float32\',\n                              tf_saver_params=dict(max_to_keep=1),\n                              model_path=args.mrbm_dirpath)\n        mrbm.fit(Q_train, Q_val)\n    return mrbm\n\ndef make_rbm_transform(rbm, X, path, np_dtype=None):\n    H = None\n    transform = True\n    if os.path.isfile(path):\n        H = np.load(path)\n        if len(X) == len(H):\n            transform = False\n    if transform:\n        H = rbm.transform(X, np_dtype=np_dtype)\n        np.save(path, H)\n    return H\n\ndef make_dbm((X_train, X_val), rbms, (Q, G), args):\n    if os.path.isdir(args.dbm_dirpath):\n        print ""\\nLoading DBM ...\\n\\n""\n        dbm = DBM.load_model(args.dbm_dirpath)\n        dbm.load_rbms(rbms)  # !!!\n    else:\n        print ""\\nTraining DBM ...\\n\\n""\n        dbm = DBM(rbms=rbms,\n                  n_particles=args.n_particles,\n                  v_particle_init=X_train[:args.n_particles].copy(),\n                  h_particles_init=(Q[:args.n_particles].copy(),\n                                    G[:args.n_particles].copy()),\n                  n_gibbs_steps=args.n_gibbs_steps[2],\n                  max_mf_updates=args.max_mf_updates,\n                  mf_tol=args.mf_tol,\n                  learning_rate=np.geomspace(args.lr[2], 1e-6, args.epochs[2]),\n                  momentum=np.geomspace(0.5, 0.9, 10),\n                  max_epoch=args.epochs[2],\n                  batch_size=args.batch_size[2],\n                  l2=args.l2[2],\n                  max_norm=args.max_norm,\n                  sample_v_states=True,\n                  sample_h_states=(True, True),\n                  sparsity_target=args.sparsity_target,\n                  sparsity_cost=args.sparsity_cost,\n                  sparsity_damping=args.sparsity_damping,\n                  train_metrics_every_iter=1000,\n                  val_metrics_every_epoch=2,\n                  random_seed=args.random_seed[2],\n                  verbose=True,\n                  display_filters=12,\n                  display_particles=36,\n                  v_shape=(32, 32, 3),\n                  dtype=\'float32\',\n                  tf_saver_params=dict(max_to_keep=1),\n                  model_path=args.dbm_dirpath)\n        dbm.fit(X_train, X_val)\n    return dbm\n\ndef make_mlp((X_train, y_train), (X_val, y_val), (X_test, y_test),\n             (W, hb), args):\n    dense_params = {}\n    if W is not None and hb is not None:\n        dense_params[\'weights\'] = (W, hb)\n\n    # define and initialize MLP model\n    mlp = Sequential([\n        Dense(7800, input_shape=(3 * 32 * 32,),\n              kernel_regularizer=regularizers.l2(args.mlp_l2),\n              kernel_initializer=glorot_uniform(seed=3333),\n              **dense_params),\n        BN(),\n        Activation(\'relu\'),\n        Dropout(args.mlp_dropout, seed=4444),\n        Dense(10, kernel_initializer=glorot_uniform(seed=5555)),\n        Activation(\'softmax\'),\n    ])\n    mlp.compile(optimizer=MultiAdam(lr=0.001,\n                                    lr_multipliers={\'dense_1\': args.mlp_lrm[0],\n                                                    \'dense_2\': args.mlp_lrm[1]}),\n                loss=\'categorical_crossentropy\',\n                metrics=[\'accuracy\'])\n\n    # train and evaluate classifier\n    with Stopwatch(verbose=True) as s:\n        early_stopping = EarlyStopping(monitor=args.mlp_val_metric, patience=6, verbose=2)\n        reduce_lr = ReduceLROnPlateau(monitor=args.mlp_val_metric, factor=0.2, verbose=2,\n                                      patience=3, min_lr=1e-5)\n        callbacks = [early_stopping, reduce_lr]\n        try:\n            mlp.fit(X_train, one_hot(y_train, n_classes=10),\n                    epochs=args.mlp_epochs,\n                    batch_size=args.mlp_batch_size,\n                    shuffle=False,\n                    validation_data=(X_val, one_hot(y_val, n_classes=10)),\n                    callbacks=callbacks)\n        except KeyboardInterrupt:\n            pass\n\n    y_pred = mlp.predict(X_test)\n    y_pred = unhot(one_hot_decision_function(y_pred), n_classes=10)\n    print ""Test accuracy: {:.4f}"".format(accuracy_score(y_test, y_pred))\n\n    # save predictions, targets, and fine-tuned weights\n    np.save(args.mlp_save_prefix + \'y_pred.npy\', y_pred)\n    np.save(args.mlp_save_prefix + \'y_test.npy\', y_test)\n    W_finetuned, _ = mlp.layers[0].get_weights()\n    np.save(args.mlp_save_prefix + \'W_finetuned.npy\', W_finetuned)\n\n\ndef main():\n    # training settings\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    # general\n    parser.add_argument(\'--gpu\', type=str, default=\'0\', metavar=\'ID\',\n                        help=""ID of the GPU to train on (or \'\' to train on CPU)"")\n\n    # data\n    parser.add_argument(\'--n-train\', type=int, default=49000, metavar=\'N\',\n                        help=\'number of training examples\')\n    parser.add_argument(\'--n-val\', type=int, default=1000, metavar=\'N\',\n                        help=\'number of validation examples\')\n    parser.add_argument(\'--data-path\', type=str, default=\'../data/\', metavar=\'PATH\',\n                        help=\'directory for storing augmented data etc.\')\n    parser.add_argument(\'--no-aug\', action=\'store_true\',\n                        help=""if enabled, don\'t augment data"")\n\n    # small RBMs related\n    parser.add_argument(\'--small-lr\', type=float, default=1e-3, metavar=\'LR\', nargs=\'+\',\n                        help=\'learning rate or sequence of such (per epoch)\')\n    parser.add_argument(\'--small-epochs\', type=int, default=100, metavar=\'N\',\n                        help=\'number of epochs to train\')\n    parser.add_argument(\'--small-batch-size\', type=int, default=48, metavar=\'B\',\n                        help=\'input batch size for training\')\n    parser.add_argument(\'--small-l2\', type=float, default=1e-3, metavar=\'L2\',\n                        help=\'L2 weight decay coefficient\')\n    parser.add_argument(\'--small-sparsity-target\', type=float, default=0.1, metavar=\'T\',\n                        help=\'desired probability of hidden activation\')\n    parser.add_argument(\'--small-sparsity-cost\', type=float, default=1e-3, metavar=\'C\',\n                        help=\'controls the amount of sparsity penalty\')\n    parser.add_argument(\'--small-random-seed\', type=int, default=9000, metavar=\'N\',\n                        help=""random seeds for models training"")\n    parser.add_argument(\'--small-dirpath-prefix\', type=str, default=\'../models/rbm_cifar_small_\', metavar=\'PREFIX\',\n                        help=\'directory path prefix to save RBMs trained on patches\')\n\n    # M-RBM related\n    parser.add_argument(\'--increase-n-gibbs-steps-every\', type=int, default=16, metavar=\'I\',\n                        help=\'increase number of Gibbs steps every specified number of epochs for M-RBM\')\n\n    # common for RBMs and DBM\n    parser.add_argument(\'--n-gibbs-steps\', type=int, default=(1, 1, 1), metavar=\'N\', nargs=\'+\',\n                        help=\'(initial) number of Gibbs steps for CD/PCD\')\n    parser.add_argument(\'--lr\', type=float, default=(5e-4, 5e-5, 4e-5), metavar=\'LR\', nargs=\'+\',\n                        help=\'(initial) learning rates\')\n    parser.add_argument(\'--epochs\', type=int, default=(64, 33, 100), metavar=\'N\', nargs=\'+\',\n                        help=\'number of epochs to train\')\n    parser.add_argument(\'--batch-size\', type=int, default=(100, 100, 100), metavar=\'B\', nargs=\'+\',\n                        help=\'input batch size for training, `--n-train` and `--n-val`\' + \\\n                             \'must be divisible by this number (for DBM)\')\n    parser.add_argument(\'--l2\', type=float, default=(1e-3, 0.005, 0.), metavar=\'L2\', nargs=\'+\',\n                        help=\'L2 weight decay coefficients\')\n    parser.add_argument(\'--random-seed\', type=int, default=(1111, 2222, 3333), metavar=\'N\', nargs=\'+\',\n                        help=\'random seeds for models training\')\n\n    # save dirpaths\n    parser.add_argument(\'--grbm-dirpath\', type=str, default=\'../models/grbm_cifar/\', metavar=\'DIRPATH\',\n                        help=\'directory path to save Gaussian RBM\')\n    parser.add_argument(\'--mrbm-dirpath\', type=str, default=\'../models/mrbm_cifar/\', metavar=\'DIRPATH\',\n                        help=\'directory path to save Multinomial RBM\')\n    parser.add_argument(\'--dbm-dirpath\', type=str, default=\'../models/dbm_cifar/\', metavar=\'DIRPATH\',\n                        help=\'directory path to save DBM\')\n\n    # DBM related\n    parser.add_argument(\'--n-particles\', type=int, default=100, metavar=\'M\',\n                        help=\'number of persistent Markov chains\')\n    parser.add_argument(\'--max-mf-updates\', type=int, default=50, metavar=\'N\',\n                        help=\'maximum number of mean-field updates per weight update\')\n    parser.add_argument(\'--mf-tol\', type=float, default=1e-11, metavar=\'TOL\',\n                        help=\'mean-field tolerance\')\n    parser.add_argument(\'--max-norm\', type=float, default=4., metavar=\'C\',\n                        help=\'maximum norm constraint\')\n    parser.add_argument(\'--sparsity-target\', type=float, default=(0.2, 0.2), metavar=\'T\', nargs=\'+\',\n                        help=\'desired probability of hidden activation\')\n    parser.add_argument(\'--sparsity-cost\', type=float, default=(1e-4, 1e-3), metavar=\'C\', nargs=\'+\',\n                        help=\'controls the amount of sparsity penalty\')\n    parser.add_argument(\'--sparsity-damping\', type=float, default=0.9, metavar=\'D\',\n                        help=\'decay rate for hidden activations probs\')\n\n    # MLP related\n    parser.add_argument(\'--mlp-no-init\', action=\'store_true\',\n                        help=\'if enabled, use random initialization\')\n    parser.add_argument(\'--mlp-l2\', type=float, default=1e-4, metavar=\'L2\',\n                        help=\'L2 weight decay coefficient\')\n    parser.add_argument(\'--mlp-lrm\', type=float, default=(0.01, 1.), metavar=\'LRM\', nargs=\'+\',\n                        help=\'learning rate multipliers of 1e-3\')\n    parser.add_argument(\'--mlp-epochs\', type=int, default=100, metavar=\'N\',\n                        help=\'number of epochs to train\')\n    parser.add_argument(\'--mlp-val-metric\', type=str, default=\'val_acc\', metavar=\'S\',\n                        help=""metric on validation set to perform early stopping, {\'val_acc\', \'val_loss\'}"")\n    parser.add_argument(\'--mlp-batch-size\', type=int, default=128, metavar=\'N\',\n                        help=\'input batch size for training\')\n    parser.add_argument(\'--mlp-dropout\', type=float, default=0.7, metavar=\'P\',\n                        help=\'probability of visible units being set to zero\')\n    parser.add_argument(\'--mlp-save-prefix\', type=str, default=\'../data/grbm_\', metavar=\'PREFIX\',\n                        help=\'prefix to save MLP predictions and targets\')\n\n    # parse and check params\n    args = parser.parse_args()\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n    for x, m in (\n        (args.n_gibbs_steps, 3),\n        (args.lr, 3),\n        (args.epochs, 3),\n        (args.batch_size, 3),\n        (args.l2, 3),\n        (args.random_seed, 3),\n    ):\n        if len(x) == 1:\n            x *= m\n\n    # prepare data (load + scale + split)\n    print ""\\nPreparing data ...""\n    X, y = load_cifar10(mode=\'train\', path=args.data_path)\n    X = X.astype(np.float32)\n    X /= 255.\n    RNG(seed=42).shuffle(X)\n    RNG(seed=42).shuffle(y)\n    n_train = min(len(X), args.n_train)\n    n_val = min(len(X), args.n_val)\n    X_train = X[:n_train]\n    X_val = X[-n_val:]\n    y_train = y[:n_train]\n    y_val = y[-n_val:]\n\n    if not args.no_aug:\n        # augment data\n        X_aug, y_train = make_augmentation(X_train, y_train, n_train, args)\n\n        # convert + scale augmented data again\n        X_train = X_aug.astype(np.float32)\n        X_train /= 255.\n        print ""Augmented shape: {0}"".format(X_train.shape)\n        print ""Augmented range: {0}"".format((X_train.min(), X_train.max()))\n\n    # center and normalize training data\n    X_mean = X_train.mean(axis=0)\n    X_std = X_train.std(axis=0)\n\n    if not args.no_aug:\n        mean_path = os.path.join(args.data_path, \'X_aug_mean.npy\')\n        std_path = os.path.join(args.data_path, \'X_aug_std.npy\')\n        if not os.path.isfile(mean_path):\n            np.save(mean_path, X_mean)\n        if not os.path.isfile(std_path):\n            np.save(std_path, X_std)\n\n    X_train -= X_mean\n    X_train /= X_std\n    X_val -= X_mean\n    X_val /= X_std\n    print ""Augmented mean: ({0:.3f}, ...); std: ({1:.3f}, ...)"".format(X_train.mean(axis=0)[0],\n                                                                       X_train.std(axis=0)[0])\n    print ""Augmented range: ({0:.3f}, {1:.3f})\\n\\n"".format(X_train.min(), X_train.max())\n\n    # train 26 small Gaussian RBMs on patches\n    small_rbms = None\n    if not os.path.isdir(args.grbm_dirpath):\n        small_rbms = make_small_rbms((X_train, X_val), args)\n\n    # assemble large weight matrix and biases\n    # and pre-train large Gaussian RBM (G-RBM)\n    grbm = make_grbm((X_train, X_val), small_rbms, args)\n\n    # extract features Q = p_{G-RBM}(h|v=X)\n    print ""\\nExtracting features from G-RBM ...\\n\\n""\n    Q_train, Q_val = None, None\n    if not os.path.isdir(args.mrbm_dirpath) or not os.path.isdir(args.dbm_dirpath):\n        Q_train_path = os.path.join(args.data_path, \'Q_train_cifar.npy\')\n        Q_train = make_rbm_transform(grbm, X_train, Q_train_path, np_dtype=np.float16)\n    if not os.path.isdir(args.mrbm_dirpath):\n        Q_val_path = os.path.join(args.data_path, \'Q_val_cifar.npy\')\n        Q_val = make_rbm_transform(grbm, X_val, Q_val_path)\n\n    # pre-train Multinomial RBM (M-RBM)\n    mrbm = make_mrbm((Q_train, Q_val), args)\n\n    # extract features G = p_{M-RBM}(h|v=Q)\n    print ""\\nExtracting features from M-RBM ...\\n\\n""\n    Q, G = None, None\n    if not os.path.isdir(args.dbm_dirpath):\n        Q = Q_train[:args.n_particles]\n        G_path = os.path.join(args.data_path, \'G_train_cifar.npy\')\n        G = make_rbm_transform(mrbm, Q, G_path)\n\n    # jointly train DBM\n    dbm = make_dbm((X_train, X_val), (grbm, mrbm), (Q, G), args)\n\n    # load test data\n    X_test, y_test = load_cifar10(mode=\'test\', path=args.data_path)\n    X_test /= 255.\n    X_test -= X_mean\n    X_test /= X_std\n\n    # G-RBM discriminative fine-tuning:\n    # initialize MLP with learned weights, \n    # add FC layer and train using backprop\n    print ""\\nG-RBM Discriminative fine-tuning ...\\n\\n""\n\n    W, hb = None, None\n    if not args.mlp_no_init:\n        weights = grbm.get_tf_params(scope=\'weights\')\n        W = weights[\'W\']\n        hb = weights[\'hb\']\n\n    make_mlp((X_train, y_train), (X_val, y_val), (X_test, y_test),\n             (W, hb), args)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/dbm_cifar_naive.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n""""""\nTrain 3072-5000-1000 Gaussian-Bernoulli-Multinomial\nDBM with pre-training on ""smoothed"" CIFAR-10 (with 1000 least\nsignificant singular values removed), as suggested in [1].\n\nPer sample validation mean reconstruction error for DBM monotonically\ndecreases during training from ~0.99 to (only) ~0.5 after 1500 epochs.\n\nThe training took approx. 47m + 119m + 22h 40m ~ 1d 1h 30m on GTX 1060.\n\nNote that DBM is trained without centering.\n\nAfter models are trained, Gaussian RBM is discriminatively fine-tuned.\nIt achieves 59.78% accuracy on a test set.\n\nReferences\n----------\n[1] A. Krizhevsky and G. Hinton. Learning multiple layers of features\n    from tine images. 2009.\n""""""\nprint __doc__\n\nimport os\nimport argparse\nimport numpy as np\nfrom scipy.linalg import svd\nfrom keras import regularizers\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom keras.initializers import glorot_uniform\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, BatchNormalization as BN\nfrom sklearn.metrics import accuracy_score\n\nimport env\nfrom boltzmann_machines import DBM\nfrom boltzmann_machines.rbm import GaussianRBM, MultinomialRBM\nfrom boltzmann_machines.utils import (RNG, Stopwatch,\n                                      one_hot, one_hot_decision_function, unhot)\nfrom boltzmann_machines.utils.dataset import load_cifar10\nfrom boltzmann_machines.utils.optimizers import MultiAdam\n\n\ndef make_smoothing(X_train, n_train, args):\n    X_s = None\n    X_s_path = os.path.join(args.data_path, \'X_s.npy\')\n\n    do_smoothing = True\n    if os.path.isfile(X_s_path):\n        print ""\\nLoading smoothed data ...""\n        X_s = np.load(X_s_path)\n        print ""Checking augmented data ...""\n        if len(X_s) == n_train:\n            do_smoothing = False\n\n    if do_smoothing:\n        print ""\\nSmoothing data ...""\n        X_m = X_train.mean(axis=0)\n        X_train -= X_m\n        with Stopwatch(verbose=True) as s:\n            [U, s, Vh] = svd(X_train,\n                             full_matrices=False,\n                             compute_uv=True,\n                             overwrite_a=True,\n                             check_finite=False)\n            s[-1000:] = 0.\n            X_s = U.dot(np.diag(s).dot(Vh))\n            X_s += X_m\n\n        # save to disk\n        np.save(X_s_path, X_s)\n        print ""\\n""\n\n    return X_s\n\ndef make_grbm((X_train, X_val), args):\n    if os.path.isdir(args.grbm_dirpath):\n        print ""\\nLoading G-RBM ...\\n\\n""\n        grbm = GaussianRBM.load_model(args.grbm_dirpath)\n    else:\n        print ""\\nTraining G-RBM ...\\n\\n""\n        grbm = GaussianRBM(n_visible=32 * 32 * 3,\n                           n_hidden=5000,\n                           sigma=1.,\n                           W_init=0.0008,\n                           vb_init=0.,\n                           hb_init=0.,\n                           n_gibbs_steps=args.n_gibbs_steps[0],\n                           learning_rate=args.lr[0],\n                           momentum=np.geomspace(0.5, 0.9, 8),\n                           max_epoch=args.epochs[0],\n                           batch_size=args.batch_size[0],\n                           l2=args.l2[0],\n                           sample_v_states=True,\n                           sample_h_states=True,\n                           sparsity_cost=0.,\n                           dbm_first=True,  # !!!\n                           metrics_config=dict(\n                               msre=True,\n                               feg=True,\n                               train_metrics_every_iter=1000,\n                               val_metrics_every_epoch=2,\n                               feg_every_epoch=2,\n                               n_batches_for_feg=50,\n                           ),\n                           verbose=True,\n                           display_filters=12,\n                           display_hidden_activations=24,\n                           v_shape=(32, 32, 3),\n                           dtype=\'float32\',\n                           tf_saver_params=dict(max_to_keep=1),\n                           model_path=args.grbm_dirpath)\n        grbm.fit(X_train, X_val)\n    return grbm\n\ndef make_mrbm((Q_train, Q_val), args):\n    if os.path.isdir(args.mrbm_dirpath):\n        print ""\\nLoading M-RBM ...\\n\\n""\n        mrbm = MultinomialRBM.load_model(args.mrbm_dirpath)\n    else:\n        print ""\\nTraining M-RBM ...\\n\\n""\n        mrbm = MultinomialRBM(n_visible=5000,\n                              n_hidden=1000,\n                              n_samples=1000,\n                              W_init=0.01,\n                              hb_init=0.,\n                              vb_init=0.,\n                              n_gibbs_steps=args.n_gibbs_steps[1],\n                              learning_rate=args.lr[1],\n                              momentum=np.geomspace(0.5, 0.9, 8),\n                              max_epoch=args.epochs[1],\n                              batch_size=args.batch_size[1],\n                              l2=args.l2[1],\n                              sample_h_states=True,\n                              sample_v_states=False,\n                              sparsity_cost=0.,\n                              dbm_last=True,  # !!!\n                              metrics_config=dict(\n                                  msre=True,\n                                  pll=True,\n                                  feg=True,\n                                  train_metrics_every_iter=400,\n                                  val_metrics_every_epoch=2,\n                                  feg_every_epoch=2,\n                                  n_batches_for_feg=50,\n                              ),\n                              verbose=True,\n                              display_filters=0,\n                              display_hidden_activations=100,\n                              random_seed=1337,\n                              dtype=\'float32\',\n                              tf_saver_params=dict(max_to_keep=1),\n                              model_path=args.mrbm_dirpath)\n        mrbm.fit(Q_train, Q_val)\n    return mrbm\n\ndef make_rbm_transform(rbm, X, path, np_dtype=None):\n    H = None\n    transform = True\n    if os.path.isfile(path):\n        H = np.load(path)\n        if len(X) == len(H):\n            transform = False\n    if transform:\n        H = rbm.transform(X, np_dtype=np_dtype)\n        np.save(path, H)\n    return H\n\ndef make_dbm((X_train, X_val), rbms, (Q, G), args):\n    if os.path.isdir(args.dbm_dirpath):\n        print ""\\nLoading DBM ...\\n\\n""\n        dbm = DBM.load_model(args.dbm_dirpath)\n        dbm.load_rbms(rbms)  # !!!\n    else:\n        print ""\\nTraining DBM ...\\n\\n""\n        dbm = DBM(rbms=rbms,\n                  n_particles=args.n_particles,\n                  v_particle_init=X_train[:args.n_particles].copy(),\n                  h_particles_init=(Q[:args.n_particles].copy(),\n                                    G[:args.n_particles].copy()),\n                  n_gibbs_steps=args.n_gibbs_steps[2],\n                  max_mf_updates=args.max_mf_updates,\n                  mf_tol=args.mf_tol,\n                  learning_rate=np.geomspace(args.lr[2], 1e-5, args.epochs[2]),\n                  momentum=np.geomspace(0.5, 0.9, 10),\n                  max_epoch=args.epochs[2],\n                  batch_size=args.batch_size[2],\n                  l2=args.l2[2],\n                  max_norm=args.max_norm,\n                  sample_v_states=True,\n                  sample_h_states=(True, True),\n                  sparsity_cost=0.,\n                  train_metrics_every_iter=1000,\n                  val_metrics_every_epoch=2,\n                  random_seed=args.random_seed[2],\n                  verbose=True,\n                  save_after_each_epoch=True,\n                  display_filters=12,\n                  display_particles=36,\n                  v_shape=(32, 32, 3),\n                  dtype=\'float32\',\n                  tf_saver_params=dict(max_to_keep=1),\n                  model_path=args.dbm_dirpath)\n        dbm.fit(X_train, X_val)\n    return dbm\n\ndef make_mlp((X_train, y_train), (X_val, y_val), (X_test, y_test),\n             (W, hb), args):\n    dense_params = {}\n    if W is not None and hb is not None:\n        dense_params[\'weights\'] = (W, hb)\n\n    # define and initialize MLP model\n    mlp = Sequential([\n        Dense(5000, input_shape=(3 * 32 * 32,),\n              kernel_regularizer=regularizers.l2(args.mlp_l2),\n              kernel_initializer=glorot_uniform(seed=3333),\n              **dense_params),\n        BN(),\n        Activation(\'relu\'),\n        Dropout(args.mlp_dropout, seed=4444),\n        Dense(10, kernel_initializer=glorot_uniform(seed=5555)),\n        Activation(\'softmax\'),\n    ])\n    mlp.compile(optimizer=MultiAdam(lr=0.001,\n                                    lr_multipliers={\'dense_1\': args.mlp_lrm[0],\n                                                    \'dense_2\': args.mlp_lrm[1]}),\n                loss=\'categorical_crossentropy\',\n                metrics=[\'accuracy\'])\n\n    # train and evaluate classifier\n    with Stopwatch(verbose=True) as s:\n        early_stopping = EarlyStopping(monitor=args.mlp_val_metric, patience=12, verbose=2)\n        reduce_lr = ReduceLROnPlateau(monitor=args.mlp_val_metric, factor=0.2, verbose=2,\n                                      patience=6, min_lr=1e-5)\n        callbacks = [early_stopping, reduce_lr]\n        try:\n            mlp.fit(X_train, one_hot(y_train, n_classes=10),\n                    epochs=args.mlp_epochs,\n                    batch_size=args.mlp_batch_size,\n                    shuffle=False,\n                    validation_data=(X_val, one_hot(y_val, n_classes=10)),\n                    callbacks=callbacks)\n        except KeyboardInterrupt:\n            pass\n\n    y_pred = mlp.predict(X_test)\n    y_pred = unhot(one_hot_decision_function(y_pred), n_classes=10)\n    print ""Test accuracy: {:.4f}"".format(accuracy_score(y_test, y_pred))\n\n    # save predictions, targets, and fine-tuned weights\n    np.save(args.mlp_save_prefix + \'y_pred.npy\', y_pred)\n    np.save(args.mlp_save_prefix + \'y_test.npy\', y_test)\n    W_finetuned, _ = mlp.layers[0].get_weights()\n    np.save(args.mlp_save_prefix + \'W_finetuned.npy\', W_finetuned)\n\n\ndef main():\n    # training settings\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    # general\n    parser.add_argument(\'--gpu\', type=str, default=\'0\', metavar=\'ID\',\n                        help=""ID of the GPU to train on (or \'\' to train on CPU)"")\n\n    # data\n    parser.add_argument(\'--n-train\', type=int, default=49000, metavar=\'N\',\n                        help=\'number of training examples\')\n    parser.add_argument(\'--n-val\', type=int, default=1000, metavar=\'N\',\n                        help=\'number of validation examples\')\n    parser.add_argument(\'--data-path\', type=str, default=\'../data/\', metavar=\'PATH\',\n                        help=\'directory for storing augmented data etc.\')\n\n    # common for RBMs and DBM\n    parser.add_argument(\'--n-gibbs-steps\', type=int, default=(1, 1, 1), metavar=\'N\', nargs=\'+\',\n                        help=\'(initial) number of Gibbs steps for CD/PCD\')\n    parser.add_argument(\'--lr\', type=float, default=(5e-4, 1e-4, 8e-5), metavar=\'LR\', nargs=\'+\',\n                        help=\'(initial) learning rates\')\n    parser.add_argument(\'--epochs\', type=int, default=(120, 180, 1500), metavar=\'N\', nargs=\'+\',\n                        help=\'number of epochs to train\')\n    parser.add_argument(\'--batch-size\', type=int, default=(100, 100, 100), metavar=\'B\', nargs=\'+\',\n                        help=\'input batch size for training, `--n-train` and `--n-val`\' + \\\n                             \'must be divisible by this number (for DBM)\')\n    parser.add_argument(\'--l2\', type=float, default=(0.01, 0.05, 1e-8), metavar=\'L2\', nargs=\'+\',\n                        help=\'L2 weight decay coefficients\')\n    parser.add_argument(\'--random-seed\', type=int, default=(1337, 1111, 2222), metavar=\'N\', nargs=\'+\',\n                        help=\'random seeds for models training\')\n\n    # save dirpaths\n    parser.add_argument(\'--grbm-dirpath\', type=str, default=\'../models/grbm_cifar_naive/\', metavar=\'DIRPATH\',\n                        help=\'directory path to save Gaussian RBM\')\n    parser.add_argument(\'--mrbm-dirpath\', type=str, default=\'../models/mrbm_cifar_naive/\', metavar=\'DIRPATH\',\n                        help=\'directory path to save Multinomial RBM\')\n    parser.add_argument(\'--dbm-dirpath\', type=str, default=\'../models/dbm_cifar_naive/\', metavar=\'DIRPATH\',\n                        help=\'directory path to save DBM\')\n\n    # DBM related\n    parser.add_argument(\'--n-particles\', type=int, default=100, metavar=\'M\',\n                        help=\'number of persistent Markov chains\')\n    parser.add_argument(\'--max-mf-updates\', type=int, default=50, metavar=\'N\',\n                        help=\'maximum number of mean-field updates per weight update\')\n    parser.add_argument(\'--mf-tol\', type=float, default=1e-11, metavar=\'TOL\',\n                        help=\'mean-field tolerance\')\n    parser.add_argument(\'--max-norm\', type=float, default=4., metavar=\'C\',\n                        help=\'maximum norm constraint\')\n\n    # MLP related\n    parser.add_argument(\'--mlp-no-init\', action=\'store_true\',\n                        help=\'if enabled, use random initialization\')\n    parser.add_argument(\'--mlp-l2\', type=float, default=1e-4, metavar=\'L2\',\n                        help=\'L2 weight decay coefficient\')\n    parser.add_argument(\'--mlp-lrm\', type=float, default=(0.1, 1.), metavar=\'LRM\', nargs=\'+\',\n                        help=\'learning rate multipliers of 1e-3\')\n    parser.add_argument(\'--mlp-epochs\', type=int, default=100, metavar=\'N\',\n                        help=\'number of epochs to train\')\n    parser.add_argument(\'--mlp-val-metric\', type=str, default=\'val_acc\', metavar=\'S\',\n                        help=""metric on validation set to perform early stopping, {\'val_acc\', \'val_loss\'}"")\n    parser.add_argument(\'--mlp-batch-size\', type=int, default=128, metavar=\'N\',\n                        help=\'input batch size for training\')\n    parser.add_argument(\'--mlp-dropout\', type=float, default=0.64, metavar=\'P\',\n                        help=\'probability of visible units being set to zero\')\n    parser.add_argument(\'--mlp-save-prefix\', type=str, default=\'../data/grbm_naive_\', metavar=\'PREFIX\',\n                        help=\'prefix to save MLP predictions and targets\')\n\n    # parse and check params\n    args = parser.parse_args()\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n    for x, m in (\n            (args.n_gibbs_steps, 3),\n            (args.lr, 3),\n            (args.epochs, 3),\n            (args.batch_size, 3),\n            (args.l2, 3),\n            (args.random_seed, 3),\n    ):\n        if len(x) == 1:\n            x *= m\n\n    # prepare data (load + scale + split)\n    print ""\\nPreparing data ...""\n    X, y = load_cifar10(mode=\'train\', path=args.data_path)\n    X = X.astype(np.float32)\n    X /= 255.\n    RNG(seed=42).shuffle(X)\n    RNG(seed=42).shuffle(y)\n    n_train = min(len(X), args.n_train)\n    n_val = min(len(X), args.n_val)\n    X_train = X[:n_train]\n    X_val = X[-n_val:]\n    y_train = y[:n_train]\n    y_val = y[-n_val:]\n\n    # remove 1000 least significant singular values\n    X_train = make_smoothing(X_train, n_train, args)\n    print X_train.shape\n\n    # center and normalize training data\n    X_s_mean = X_train.mean(axis=0)\n    X_s_std = X_train.std(axis=0)\n    mean_path = os.path.join(args.data_path, \'X_s_mean.npy\')\n    std_path = os.path.join(args.data_path, \'X_s_std.npy\')\n    if not os.path.isfile(mean_path):\n        np.save(mean_path, X_s_mean)\n    if not os.path.isfile(std_path):\n        np.save(std_path, X_s_std)\n\n    X_train -= X_s_mean\n    X_train /= X_s_std\n    X_val -= X_s_mean\n    X_val /= X_s_std\n    print ""Mean: ({0:.3f}, ...); std: ({1:.3f}, ...)"".format(X_train.mean(axis=0)[0],\n                                                             X_train.std(axis=0)[0])\n    print ""Range: ({0:.3f}, {1:.3f})\\n\\n"".format(X_train.min(), X_train.max())\n\n    # pre-train Gaussian RBM\n    grbm = make_grbm((X_train, X_val), args)\n\n    # extract features Q = p_{G-RBM}(h|v=X)\n    print ""\\nExtracting features from G-RBM ...\\n\\n""\n    Q_train, Q_val = None, None\n    if not os.path.isdir(args.mrbm_dirpath) or not os.path.isdir(args.dbm_dirpath):\n        Q_train_path = os.path.join(args.data_path, \'Q_train_cifar_naive.npy\')\n        Q_train = make_rbm_transform(grbm, X_train, Q_train_path)\n    if not os.path.isdir(args.mrbm_dirpath):\n        Q_val_path = os.path.join(args.data_path, \'Q_val_cifar_naive.npy\')\n        Q_val = make_rbm_transform(grbm, X_val, Q_val_path)\n\n    # pre-train Multinomial RBM (M-RBM)\n    mrbm = make_mrbm((Q_train, Q_val), args)\n\n    # extract features G = p_{M-RBM}(h|v=Q)\n    print ""\\nExtracting features from M-RBM ...\\n\\n""\n    Q, G = None, None\n    if not os.path.isdir(args.dbm_dirpath):\n        Q = Q_train[:args.n_particles]\n        G_path = os.path.join(args.data_path, \'G_train_cifar_naive.npy\')\n        G = make_rbm_transform(mrbm, Q, G_path)\n\n    # jointly train DBM\n    dbm = make_dbm((X_train, X_val), (grbm, mrbm), (Q, G), args)\n\n    # load test data\n    X_test, y_test = load_cifar10(mode=\'test\', path=args.data_path)\n    X_test /= 255.\n    X_test -= X_s_mean\n    X_test /= X_s_std\n\n    # G-RBM discriminative fine-tuning:\n    # initialize MLP with learned weights,\n    # add FC layer and train using backprop\n    print ""\\nG-RBM Discriminative fine-tuning ...\\n\\n""\n\n    W, hb = None, None\n    if not args.mlp_no_init:\n        weights = grbm.get_tf_params(scope=\'weights\')\n        W = weights[\'W\']\n        hb = weights[\'hb\']\n\n    make_mlp((X_train, y_train), (X_val, y_val), (X_test, y_test),\n             (W, hb), args)\n\n\nif __name__ == \'__main__\':\n    main()'"
examples/dbm_mnist.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n""""""\nTrain 2-layer Bernoulli DBM on MNIST dataset with pre-training.\nHyper-parameters are similar to those in MATLAB code [1].\nSome of them were changed for more efficient computation on GPUs,\nanother ones to obtain more stable learning (lesser number of ""died"" units etc.)\nRBM #2 trained with increasing k in CD-k and decreasing learning rate\nover time.\n\nPer sample validation mean reconstruction error for DBM (mostly) monotonically\ndecreases during training and is about 5.27e-3 at the end.\n\nThe training took approx. 9 + 55 + 185 min = 4h 9m on GTX 1060.\n\nAfter the model is trained, it is discriminatively fine-tuned.\nThe code uses early stopping so max number of MLP epochs is often not reached.\nIt achieves 1.32% misclassification rate on the test set.\n\nNote that DBM is trained without centering.\n\nLinks\n-----\n[1] http://www.cs.toronto.edu/~rsalakhu/DBM.html\n""""""\nprint __doc__\n\nimport os\nimport argparse\nimport numpy as np\nfrom keras import regularizers\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom keras.initializers import glorot_uniform\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom sklearn.metrics import accuracy_score\n\nimport env\nfrom boltzmann_machines import DBM\nfrom boltzmann_machines.rbm import BernoulliRBM\nfrom boltzmann_machines.utils import (RNG, Stopwatch,\n                                      one_hot, one_hot_decision_function, unhot)\nfrom boltzmann_machines.utils.dataset import load_mnist\nfrom boltzmann_machines.utils.optimizers import MultiAdam\n\n\ndef make_rbm1(X, args):\n    if os.path.isdir(args.rbm1_dirpath):\n        print ""\\nLoading RBM #1 ...\\n\\n""\n        rbm1 = BernoulliRBM.load_model(args.rbm1_dirpath)\n    else:\n        print ""\\nTraining RBM #1 ...\\n\\n""\n        rbm1 = BernoulliRBM(n_visible=784,\n                            n_hidden=args.n_hiddens[0],\n                            W_init=0.001,\n                            vb_init=0.,\n                            hb_init=0.,\n                            n_gibbs_steps=args.n_gibbs_steps[0],\n                            learning_rate=args.lr[0],\n                            momentum=[0.5] * 5 + [0.9],\n                            max_epoch=args.epochs[0],\n                            batch_size=args.batch_size[0],\n                            l2=args.l2[0],\n                            sample_h_states=True,\n                            sample_v_states=True,\n                            sparsity_cost=0.,\n                            dbm_first=True,  # !!!\n                            metrics_config=dict(\n                                msre=True,\n                                pll=True,\n                                train_metrics_every_iter=500,\n                            ),\n                            verbose=True,\n                            display_filters=30,\n                            display_hidden_activations=24,\n                            v_shape=(28, 28),\n                            random_seed=args.random_seed[0],\n                            dtype=\'float32\',\n                            tf_saver_params=dict(max_to_keep=1),\n                            model_path=args.rbm1_dirpath)\n        rbm1.fit(X)\n    return rbm1\n\ndef make_rbm2(Q, args):\n    if os.path.isdir(args.rbm2_dirpath):\n        print ""\\nLoading RBM #2 ...\\n\\n""\n        rbm2 = BernoulliRBM.load_model(args.rbm2_dirpath)\n    else:\n        print ""\\nTraining RBM #2 ...\\n\\n""\n\n        epochs = args.epochs[1]\n        n_every = args.increase_n_gibbs_steps_every\n\n        n_gibbs_steps = np.arange(args.n_gibbs_steps[1],\n                                  args.n_gibbs_steps[1] + epochs / n_every)\n        learning_rate = args.lr[1] / np.arange(1, 1 + epochs / n_every)\n        n_gibbs_steps = np.repeat(n_gibbs_steps, n_every)\n        learning_rate = np.repeat(learning_rate, n_every)\n\n        rbm2 = BernoulliRBM(n_visible=args.n_hiddens[0],\n                            n_hidden=args.n_hiddens[1],\n                            W_init=0.005,\n                            vb_init=0.,\n                            hb_init=0.,\n                            n_gibbs_steps=n_gibbs_steps,\n                            learning_rate=learning_rate,\n                            momentum=[0.5] * 5 + [0.9],\n                            max_epoch=max(args.epochs[1], n_every),\n                            batch_size=args.batch_size[1],\n                            l2=args.l2[1],\n                            sample_h_states=True,\n                            sample_v_states=True,\n                            sparsity_cost=0.,\n                            dbm_last=True,  # !!!\n                            metrics_config=dict(\n                                msre=True,\n                                pll=True,\n                                train_metrics_every_iter=500,\n                            ),\n                            verbose=True,\n                            display_filters=0,\n                            display_hidden_activations=24,\n                            random_seed=args.random_seed[1],\n                            dtype=\'float32\',\n                            tf_saver_params=dict(max_to_keep=1),\n                            model_path=args.rbm2_dirpath)\n        rbm2.fit(Q)\n    return rbm2\n\ndef make_dbm((X_train, X_val), rbms, (Q, G), args):\n    if os.path.isdir(args.dbm_dirpath):\n        print ""\\nLoading DBM ...\\n\\n""\n        dbm = DBM.load_model(args.dbm_dirpath)\n        dbm.load_rbms(rbms)  # !!!\n    else:\n        print ""\\nTraining DBM ...\\n\\n""\n        dbm = DBM(rbms=rbms,\n                  n_particles=args.n_particles,\n                  v_particle_init=X_train[:args.n_particles].copy(),\n                  h_particles_init=(Q[:args.n_particles].copy(),\n                                    G[:args.n_particles].copy()),\n                  n_gibbs_steps=args.n_gibbs_steps[2],\n                  max_mf_updates=args.max_mf_updates,\n                  mf_tol=args.mf_tol,\n                  learning_rate=np.geomspace(args.lr[2], 5e-6, 400),\n                  momentum=np.geomspace(0.5, 0.9, 10),\n                  max_epoch=args.epochs[2],\n                  batch_size=args.batch_size[2],\n                  l2=args.l2[2],\n                  max_norm=args.max_norm,\n                  sample_v_states=True,\n                  sample_h_states=(True, True),\n                  sparsity_target=args.sparsity_target,\n                  sparsity_cost=args.sparsity_cost,\n                  sparsity_damping=args.sparsity_damping,\n                  train_metrics_every_iter=400,\n                  val_metrics_every_epoch=2,\n                  random_seed=args.random_seed[2],\n                  verbose=True,\n                  display_filters=10,\n                  display_particles=20,\n                  v_shape=(28, 28),\n                  dtype=\'float32\',\n                  tf_saver_params=dict(max_to_keep=1),\n                  model_path=args.dbm_dirpath)\n        dbm.fit(X_train, X_val)\n    return dbm\n\ndef make_mlp((X_train, y_train), (X_val, y_val), (X_test, y_test),\n             (W, hb), (W2, hb2), args):\n    dense_params = {}\n    if W is not None and hb is not None:\n        dense_params[\'weights\'] = (W, hb)\n\n    dense2_params = {}\n    if W2 is not None and hb2 is not None:\n        dense2_params[\'weights\'] = (W2, hb2)\n\n    # define and initialize MLP model\n    mlp = Sequential([\n        Dense(args.n_hiddens[0], input_shape=(784,),\n              kernel_regularizer=regularizers.l2(args.mlp_l2),\n              kernel_initializer=glorot_uniform(seed=3333),\n              **dense_params),\n        Activation(\'sigmoid\'),\n        Dense(args.n_hiddens[1],\n              kernel_regularizer=regularizers.l2(args.mlp_l2),\n              kernel_initializer=glorot_uniform(seed=4444),\n              **dense2_params),\n        Activation(\'sigmoid\'),\n        Dense(10, kernel_initializer=glorot_uniform(seed=5555)),\n        Activation(\'softmax\'),\n    ])\n    mlp.compile(optimizer=MultiAdam(lr=0.001,\n                                    lr_multipliers={\'dense_1\': args.mlp_lrm[0],\n                                                    \'dense_2\': args.mlp_lrm[1],\n                                                    \'dense_3\': args.mlp_lrm[2],}),\n                loss=\'categorical_crossentropy\',\n                metrics=[\'accuracy\'])\n\n    # train and evaluate classifier\n    with Stopwatch(verbose=True) as s:\n        early_stopping = EarlyStopping(monitor=args.mlp_val_metric, patience=12, verbose=2)\n        reduce_lr = ReduceLROnPlateau(monitor=args.mlp_val_metric, factor=0.2, verbose=2,\n                                      patience=6, min_lr=1e-5)\n        try:\n            mlp.fit(X_train, one_hot(y_train, n_classes=10),\n                    epochs=args.mlp_epochs,\n                    batch_size=args.mlp_batch_size,\n                    shuffle=False,\n                    validation_data=(X_val, one_hot(y_val, n_classes=10)),\n                    callbacks=[early_stopping, reduce_lr])\n        except KeyboardInterrupt:\n            pass\n\n    y_pred = mlp.predict(X_test)\n    y_pred = unhot(one_hot_decision_function(y_pred), n_classes=10)\n    print ""Test accuracy: {:.4f}"".format(accuracy_score(y_test, y_pred))\n\n    # save predictions, targets, and fine-tuned weights\n    np.save(args.mlp_save_prefix + \'y_pred.npy\', y_pred)\n    np.save(args.mlp_save_prefix + \'y_test.npy\', y_test)\n    W1_finetuned, _ = mlp.layers[0].get_weights()\n    W2_finetuned, _ = mlp.layers[2].get_weights()\n    np.save(args.mlp_save_prefix + \'W1_finetuned.npy\', W1_finetuned)\n    np.save(args.mlp_save_prefix + \'W2_finetuned.npy\', W2_finetuned)\n\n\ndef main():\n    # training settings\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    # general/data\n    parser.add_argument(\'--gpu\', type=str, default=\'0\', metavar=\'ID\',\n                        help=""ID of the GPU to train on (or \'\' to train on CPU)"")\n    parser.add_argument(\'--n-train\', type=int, default=59000, metavar=\'N\',\n                        help=\'number of training examples\')\n    parser.add_argument(\'--n-val\', type=int, default=1000, metavar=\'N\',\n                        help=\'number of validation examples\')\n\n    # RBM #2 related\n    parser.add_argument(\'--increase-n-gibbs-steps-every\', type=int, default=20, metavar=\'I\',\n                        help=\'increase number of Gibbs steps every specified number of epochs for RBM #2\')\n\n    # common for RBMs and DBM\n    parser.add_argument(\'--n-hiddens\', type=int, default=(512, 1024), metavar=\'N\', nargs=\'+\',\n                        help=\'numbers of hidden units\')\n    parser.add_argument(\'--n-gibbs-steps\', type=int, default=(1, 1, 1), metavar=\'N\', nargs=\'+\',\n                        help=\'(initial) number of Gibbs steps for CD/PCD\')\n    parser.add_argument(\'--lr\', type=float, default=(0.05, 0.01, 2e-3), metavar=\'LR\', nargs=\'+\',\n                        help=\'(initial) learning rates\')\n    parser.add_argument(\'--epochs\', type=int, default=(64, 120, 500), metavar=\'N\', nargs=\'+\',\n                        help=\'number of epochs to train\')\n    parser.add_argument(\'--batch-size\', type=int, default=(48, 48, 100), metavar=\'B\', nargs=\'+\',\n                        help=\'input batch size for training, `--n-train` and `--n-val`\' + \\\n                             \'must be divisible by this number (for DBM)\')\n    parser.add_argument(\'--l2\', type=float, default=(1e-3, 2e-4, 1e-7), metavar=\'L2\', nargs=\'+\',\n                        help=\'L2 weight decay coefficients\')\n    parser.add_argument(\'--random-seed\', type=int, default=(1337, 1111, 2222), metavar=\'N\', nargs=\'+\',\n                        help=\'random seeds for models training\')\n\n    # save dirpaths\n    parser.add_argument(\'--rbm1-dirpath\', type=str, default=\'../models/dbm_mnist_rbm1/\', metavar=\'DIRPATH\',\n                        help=\'directory path to save RBM #1\')\n    parser.add_argument(\'--rbm2-dirpath\', type=str, default=\'../models/dbm_mnist_rbm2/\', metavar=\'DIRPATH\',\n                        help=\'directory path to save RBM #2\')\n    parser.add_argument(\'--dbm-dirpath\', type=str, default=\'../models/dbm_mnist/\', metavar=\'DIRPATH\',\n                        help=\'directory path to save DBM\')\n\n    # DBM related\n    parser.add_argument(\'--n-particles\', type=int, default=100, metavar=\'M\',\n                        help=\'number of persistent Markov chains\')\n    parser.add_argument(\'--max-mf-updates\', type=int, default=50, metavar=\'N\',\n                        help=\'maximum number of mean-field updates per weight update\')\n    parser.add_argument(\'--mf-tol\', type=float, default=1e-7, metavar=\'TOL\',\n                        help=\'mean-field tolerance\')\n    parser.add_argument(\'--max-norm\', type=float, default=6., metavar=\'C\',\n                        help=\'maximum norm constraint\')\n    parser.add_argument(\'--sparsity-target\', type=float, default=(0.2, 0.1), metavar=\'T\', nargs=\'+\',\n                        help=\'desired probability of hidden activation\')\n    parser.add_argument(\'--sparsity-cost\', type=float, default=(1e-4, 5e-5), metavar=\'C\', nargs=\'+\',\n                        help=\'controls the amount of sparsity penalty\')\n    parser.add_argument(\'--sparsity-damping\', type=float, default=0.9, metavar=\'D\',\n                        help=\'decay rate for hidden activations probs\')\n\n    # MLP related\n    parser.add_argument(\'--mlp-no-init\', action=\'store_true\',\n                        help=\'if enabled, use random initialization\')\n    parser.add_argument(\'--mlp-l2\', type=float, default=1e-5, metavar=\'L2\',\n                        help=\'L2 weight decay coefficient\')\n    parser.add_argument(\'--mlp-lrm\', type=float, default=(0.01, 0.1, 1.), metavar=\'LRM\', nargs=\'+\',\n                        help=\'learning rate multipliers of 1e-3\')\n    parser.add_argument(\'--mlp-epochs\', type=int, default=100, metavar=\'N\',\n                        help=\'number of epochs to train\')\n    parser.add_argument(\'--mlp-val-metric\', type=str, default=\'val_acc\', metavar=\'S\',\n                        help=""metric on validation set to perform early stopping, {\'val_acc\', \'val_loss\'}"")\n    parser.add_argument(\'--mlp-batch-size\', type=int, default=128, metavar=\'N\',\n                        help=\'input batch size for training\')\n    parser.add_argument(\'--mlp-save-prefix\', type=str, default=\'../data/dbm_\', metavar=\'PREFIX\',\n                        help=\'prefix to save MLP predictions and targets\')\n\n    # parse and check params\n    args = parser.parse_args()\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n    for x, m in (\n        (args.n_gibbs_steps, 3),\n        (args.lr, 3),\n        (args.epochs, 3),\n        (args.batch_size, 3),\n        (args.l2, 3),\n        (args.random_seed, 3),\n        (args.sparsity_target, 2),\n        (args.sparsity_cost, 2),\n        (args.mlp_lrm, 3),\n    ):\n        if len(x) == 1:\n            x *= m\n\n    # prepare data (load + scale + split)\n    print ""\\nPreparing data ...\\n\\n""\n    X, y = load_mnist(mode=\'train\', path=\'../data/\')\n    X /= 255.\n    RNG(seed=42).shuffle(X)\n    RNG(seed=42).shuffle(y)\n    n_train = min(len(X), args.n_train)\n    n_val = min(len(X), args.n_val)\n    X_train = X[:n_train]\n    y_train = y[:n_train]\n    X_val = X[-n_val:]\n    y_val = y[-n_val:]\n    X = np.concatenate((X_train, X_val))\n\n    # pre-train RBM #1\n    rbm1 = make_rbm1(X, args)\n\n    # freeze RBM #1 and extract features Q = p_{RBM_1}(h|v=X)\n    Q = None\n    if not os.path.isdir(args.rbm2_dirpath) or not os.path.isdir(args.dbm_dirpath):\n        print ""\\nExtracting features from RBM #1 ...""\n        Q = rbm1.transform(X)\n        print ""\\n""\n\n    # pre-train RBM #2\n    rbm2 = make_rbm2(Q, args)\n\n    # freeze RBM #2 and extract features G = p_{RBM_2}(h|v=Q)\n    G = None\n    if not os.path.isdir(args.dbm_dirpath):\n        print ""\\nExtracting features from RBM #2 ...""\n        G = rbm2.transform(Q)\n        print ""\\n""\n\n    # jointly train DBM\n    dbm = make_dbm((X_train, X_val), (rbm1, rbm2), (Q, G), args)\n\n    # load test data\n    X_test, y_test = load_mnist(mode=\'test\', path=\'../data/\')\n    X_test /= 255.\n\n    # discriminative fine-tuning: initialize MLP with\n    # learned weights, add FC layer and train using backprop\n    print ""\\nDiscriminative fine-tuning ...\\n\\n""\n\n    W, hb = None, None\n    W2, hb2 = None, None\n    if not args.mlp_no_init:\n        weights = dbm.get_tf_params(scope=\'weights\')\n        W = weights[\'W\']\n        hb = weights[\'hb\']\n        W2 = weights[\'W_1\']\n        hb2 = weights[\'hb_1\']\n\n    make_mlp((X_train, y_train), (X_val, y_val), (X_test, y_test),\n             (W, hb), (W2, hb2), args)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/env.py,0,"b'import sys\nimport os.path as path\n# prepend parent directory to sys.path\nsys.path.insert(0, path.dirname(path.dirname(path.abspath(__file__))))\n'"
examples/rbm_mnist.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n""""""\nTrain Bernoulli-Bernoulli RBM on MNIST dataset and use for classification.\n\nMomentum is initially 0.5 and gradually increases to 0.9.\nTraining time is approx. 2.5 times faster using single-precision rather than double\nwith negligible difference in reconstruction error, pseudo log-likelihood is slightly\nmore noisy at the beginning of training though.\n\nPer sample validation pseudo log-likelihood is -0.08 after 28 epochs and -0.017 after 110\nepochs. It still slightly underfitting at that point, though (free energy gap at the end\nof training is -1.4 < 0). Average validation mean reconstruction error monotonically\ndecreases during training and is about 7.39e-3 at the end.\n\nThe training took approx. 38 min on GTX 1060.\n\nAfter the model is trained, it is discriminatively fine-tuned.\nThe code uses early stopping so max number of MLP epochs is often not reached.\nIt achieves 1.27% misclassification rate on the test set.\n""""""\nprint __doc__\n\nimport os\nimport argparse\nimport numpy as np\nfrom keras import regularizers\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom keras.initializers import glorot_uniform\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom sklearn.metrics import accuracy_score\n\nimport env\nfrom boltzmann_machines.rbm import BernoulliRBM, logit_mean\nfrom boltzmann_machines.utils import (RNG, Stopwatch,\n                                      one_hot, one_hot_decision_function, unhot)\nfrom boltzmann_machines.utils.dataset import load_mnist\nfrom boltzmann_machines.utils.optimizers import MultiAdam\n\n\ndef make_rbm(X_train, X_val, args):\n    if os.path.isdir(args.model_dirpath):\n        print ""\\nLoading model ...\\n\\n""\n        rbm = BernoulliRBM.load_model(args.model_dirpath)\n    else:\n        print ""\\nTraining model ...\\n\\n""\n        rbm = BernoulliRBM(n_visible=784,\n                           n_hidden=args.n_hidden,\n                           W_init=args.w_init,\n                           vb_init=logit_mean(X_train) if args.vb_init else 0.,\n                           hb_init=args.hb_init,\n                           n_gibbs_steps=args.n_gibbs_steps,\n                           learning_rate=args.lr,\n                           momentum=np.geomspace(0.5, 0.9, 8),\n                           max_epoch=args.epochs,\n                           batch_size=args.batch_size,\n                           l2=args.l2,\n                           sample_v_states=args.sample_v_states,\n                           sample_h_states=True,\n                           dropout=args.dropout,\n                           sparsity_target=args.sparsity_target,\n                           sparsity_cost=args.sparsity_cost,\n                           sparsity_damping=args.sparsity_damping,\n                           metrics_config=dict(\n                               msre=True,\n                               pll=True,\n                               feg=True,\n                               train_metrics_every_iter=1000,\n                               val_metrics_every_epoch=2,\n                               feg_every_epoch=4,\n                               n_batches_for_feg=50,\n                           ),\n                           verbose=True,\n                           display_filters=30,\n                           display_hidden_activations=24,\n                           v_shape=(28, 28),\n                           random_seed=args.random_seed,\n                           dtype=args.dtype,\n                           tf_saver_params=dict(max_to_keep=1),\n                           model_path=args.model_dirpath)\n        rbm.fit(X_train, X_val)\n    return rbm\n\ndef make_mlp((X_train, y_train), (X_val, y_val), (X_test, y_test),\n             (W, hb), args):\n    dense_params = {}\n    if W is not None and hb is not None:\n        dense_params[\'weights\'] = (W, hb)\n\n    # define and initialize MLP model\n    mlp = Sequential([\n        Dense(args.n_hidden, input_shape=(784,),\n              kernel_regularizer=regularizers.l2(args.mlp_l2),\n              kernel_initializer=glorot_uniform(seed=1111),\n              **dense_params),\n        Activation(\'sigmoid\'),\n        Dense(10, kernel_initializer=glorot_uniform(seed=2222)),\n        Activation(\'softmax\'),\n    ])\n    mlp.compile(optimizer=MultiAdam(lr=0.001,\n                                    lr_multipliers={\'dense_1\': args.mlp_lrm[0],\n                                                    \'dense_2\': args.mlp_lrm[1]}),\n                loss=\'categorical_crossentropy\',\n                metrics=[\'accuracy\'])\n\n    # train and evaluate classifier\n    with Stopwatch(verbose=True) as s:\n        early_stopping = EarlyStopping(monitor=args.mlp_val_metric, patience=12, verbose=2)\n        reduce_lr = ReduceLROnPlateau(monitor=args.mlp_val_metric, factor=0.2, verbose=2,\n                                      patience=6, min_lr=1e-5)\n        callbacks = [early_stopping, reduce_lr]\n        try:\n            mlp.fit(X_train, one_hot(y_train, n_classes=10),\n                    epochs=args.mlp_epochs,\n                    batch_size=args.mlp_batch_size,\n                    shuffle=False,\n                    validation_data=(X_val, one_hot(y_val, n_classes=10)),\n                    callbacks=callbacks)\n        except KeyboardInterrupt:\n            pass\n\n    y_pred = mlp.predict(X_test)\n    y_pred = unhot(one_hot_decision_function(y_pred), n_classes=10)\n    print ""Test accuracy: {:.4f}"".format(accuracy_score(y_test, y_pred))\n\n    # save predictions, targets, and fine-tuned weights\n    np.save(args.mlp_save_prefix + \'y_pred.npy\', y_pred)\n    np.save(args.mlp_save_prefix + \'y_test.npy\', y_test)\n    W_finetuned, _ = mlp.layers[0].get_weights()\n    np.save(args.mlp_save_prefix + \'W_finetuned.npy\', W_finetuned)\n\n\ndef main():\n    # training settings\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    # general/data\n    parser.add_argument(\'--gpu\', type=str, default=\'0\', metavar=\'ID\',\n                        help=""ID of the GPU to train on (or \'\' to train on CPU)"")\n    parser.add_argument(\'--n-train\', type=int, default=55000, metavar=\'N\',\n                        help=\'number of training examples\')\n    parser.add_argument(\'--n-val\', type=int, default=5000, metavar=\'N\',\n                        help=\'number of validation examples\')\n    parser.add_argument(\'--data-path\', type=str, default=\'../data/\', metavar=\'PATH\',\n                        help=\'directory for storing augmented data etc.\')\n\n    # RBM related\n    parser.add_argument(\'--n-hidden\', type=int, default=1024, metavar=\'N\',\n                        help=\'number of hidden units\')\n    parser.add_argument(\'--w-init\', type=float, default=0.01, metavar=\'STD\',\n                        help=\'initialize weights from zero-centered Gaussian with this standard deviation\')\n    parser.add_argument(\'--vb-init\', action=\'store_false\',\n                        help=\'initialize visible biases as logit of mean values of features\' + \\\n                             \', otherwise (if enabled) zero init\')\n    parser.add_argument(\'--hb-init\', type=float, default=0., metavar=\'HB\',\n                        help=\'initial hidden bias\')\n    parser.add_argument(\'--n-gibbs-steps\', type=int, default=1, metavar=\'N\', nargs=\'+\',\n                        help=\'number of Gibbs updates per weights update or sequence of such (per epoch)\')\n    parser.add_argument(\'--lr\', type=float, default=0.05, metavar=\'LR\', nargs=\'+\',\n                        help=\'learning rate or sequence of such (per epoch)\')\n    parser.add_argument(\'--epochs\', type=int, default=120, metavar=\'N\',\n                        help=\'number of epochs to train\')\n    parser.add_argument(\'--batch-size\', type=int, default=10, metavar=\'B\',\n                        help=\'input batch size for training\')\n    parser.add_argument(\'--l2\', type=float, default=1e-5, metavar=\'L2\',\n                        help=\'L2 weight decay coefficient\')\n    parser.add_argument(\'--sample-v-states\', action=\'store_true\',\n                        help=\'sample visible states, otherwise use probabilities w/o sampling\')\n    parser.add_argument(\'--dropout\', type=float, metavar=\'P\',\n                        help=\'probability of visible units being on\')\n    parser.add_argument(\'--sparsity-target\', type=float, default=0.1, metavar=\'T\',\n                        help=\'desired probability of hidden activation\')\n    parser.add_argument(\'--sparsity-cost\', type=float, default=1e-5, metavar=\'C\',\n                        help=\'controls the amount of sparsity penalty\')\n    parser.add_argument(\'--sparsity-damping\', type=float, default=0.9, metavar=\'D\',\n                        help=\'decay rate for hidden activations probs\')\n    parser.add_argument(\'--random-seed\', type=int, default=1337, metavar=\'N\',\n                        help=""random seed for model training"")\n    parser.add_argument(\'--dtype\', type=str, default=\'float32\', metavar=\'T\',\n                        help=""datatype precision to use"")\n    parser.add_argument(\'--model-dirpath\', type=str, default=\'../models/rbm_mnist/\', metavar=\'DIRPATH\',\n                        help=\'directory path to save the model\')\n\n    # MLP related\n    parser.add_argument(\'--mlp-no-init\', action=\'store_true\',\n                        help=\'if enabled, use random initialization\')\n    parser.add_argument(\'--mlp-l2\', type=float, default=1e-5, metavar=\'L2\',\n                        help=\'L2 weight decay coefficient\')\n    parser.add_argument(\'--mlp-lrm\', type=float, default=(0.1, 1.), metavar=\'LRM\', nargs=\'+\',\n                        help=\'learning rate multipliers of 1e-3\')\n    parser.add_argument(\'--mlp-epochs\', type=int, default=100, metavar=\'N\',\n                        help=\'number of epochs to train\')\n    parser.add_argument(\'--mlp-val-metric\', type=str, default=\'val_acc\', metavar=\'S\',\n                        help=""metric on validation set to perform early stopping, {\'val_acc\', \'val_loss\'}"")\n    parser.add_argument(\'--mlp-batch-size\', type=int, default=128, metavar=\'N\',\n                        help=\'input batch size for training\')\n    parser.add_argument(\'--mlp-save-prefix\', type=str, default=\'../data/rbm_\', metavar=\'PREFIX\',\n                        help=\'prefix to save MLP predictions and targets\')\n\n    args = parser.parse_args()\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n    if len(args.mlp_lrm) == 1:\n        args.mlp_lrm *= 2\n\n    # prepare data (load + scale + split)\n    print ""\\nPreparing data ...\\n\\n""\n    X, y = load_mnist(mode=\'train\', path=args.data_path)\n    X /= 255.\n    RNG(seed=42).shuffle(X)\n    RNG(seed=42).shuffle(y)\n    n_train = min(len(X), args.n_train)\n    n_val = min(len(X), args.n_val)\n    X_train = X[:n_train]\n    y_train = y[:n_train]\n    X_val = X[-n_val:]\n    y_val = y[-n_val:]\n\n    # train and save the RBM model\n    rbm = make_rbm(X_train, X_val, args)\n\n    # load test data\n    X_test, y_test = load_mnist(mode=\'test\', path=args.data_path)\n    X_test /= 255.\n\n    # discriminative fine-tuning: initialize MLP with\n    # learned weights, add FC layer and train using backprop\n    print ""\\nDiscriminative fine-tuning ...\\n\\n""\n\n    W, hb = None, None\n    if not args.mlp_no_init:\n        weights = rbm.get_tf_params(scope=\'weights\')\n        W = weights[\'W\']\n        hb = weights[\'hb\']\n\n    make_mlp((X_train, y_train), (X_val, y_val), (X_test, y_test),\n             (W, hb), args)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
notebooks/env.py,0,"b'import sys\nimport os.path as path\n# prepend parent directory to sys.path\nsys.path.insert(0, path.dirname(path.dirname(path.abspath(__file__))))\n'"
boltzmann_machines/base/__init__.py,0,b'from .base import *\nfrom .base_model import *\nfrom .mixin import *\nfrom .tf_model import *\n'
boltzmann_machines/base/base.py,0,"b""def is_param_name(name):\n    return not name.startswith('_') and not name.endswith('_')\n\ndef is_attribute_name(name):\n    return not name.startswith('_') and name.endswith('_')\n"""
boltzmann_machines/base/base_model.py,0,"b'import numpy as np\nfrom copy import deepcopy\nfrom ..base.base import is_param_name, is_attribute_name\nfrom .mixin import SeedMixin\nfrom ..utils.utils import write_during_training\n\n\nclass BaseModel(SeedMixin):\n    def __init__(self, *args, **kwargs):\n        super(BaseModel, self).__init__(*args, **kwargs)\n\n    def get_params(self, deep=True, include_attributes=True):\n        """"""Get parameters (and attributes) of the model.\n\n        Parameters\n        ----------\n        deep : bool, optional\n            Whether to deepcopy all the parameters.\n\n        Returns\n        -------\n        params : dict\n            Parameters of the model.\n        """"""\n        params = vars(self)\n        p = lambda k: is_param_name(k) or (include_attributes and is_attribute_name(k))\n        params = {k: params[k] for k in params if p(k)}\n        if deep:\n            params = deepcopy(params)\n        return params\n\n    def set_params(self, **params):\n        """"""Set parameters (and attributes) of the model.\n\n        Parameters\n        ----------\n        params : kwargs\n            Parameters names and their new values.\n\n        Returns\n        -------\n        self\n        """"""\n        for k, v in params.items():\n            if (is_param_name(k) or is_attribute_name(k)) and hasattr(self, k):\n                setattr(self, k, v)\n            else:\n                raise ValueError(""invalid param name \'{0}\'"".format(k))\n        return self\n\n    def _serialize(self, params):\n        """"""Class-specific parameters serialization routine.""""""\n        for k, v in params.items():\n            if isinstance(v, np.ndarray):\n                if v.size > 1e6:\n                    msg = ""WARNING: parameter `{0}` won\'t be serialized because it is too large:""\n                    msg += \' ({1:.2f} > 1 Mio elements)\'\n                    msg = msg.format(k, 1e-6 * v.size)\n                    write_during_training(msg)\n                    params[k] = None\n                else:\n                    params[k] = v.tolist()\n        return params\n\n    def _deserialize(self, params):\n        """"""Class-specific parameters deserialization routine.""""""\n        return params\n'"
boltzmann_machines/base/mixin.py,0,"b""import numpy as np\nimport tensorflow as tf\n\nfrom ..utils import RNG\n\n\nclass BaseMixin(object):\n    def __init__(self, *args, **kwargs):\n        if args or kwargs:\n            raise AttributeError('Invalid parameters: {0}, {1}'.format(args, kwargs))\n        super(BaseMixin, self).__init__()\n\n\nclass DtypeMixin(BaseMixin):\n    def __init__(self, dtype='float32', *args, **kwargs):\n        super(DtypeMixin, self).__init__(*args, **kwargs)\n        self.dtype = dtype\n\n    @property\n    def _tf_dtype(self):\n        return getattr(tf, self.dtype)\n\n    @property\n    def _np_dtype(self):\n        return getattr(np, self.dtype)\n\n\nclass SeedMixin(BaseMixin):\n    def __init__(self, random_seed=None, *args, **kwargs):\n        super(SeedMixin, self).__init__(*args, **kwargs)\n        self.random_seed = random_seed\n        self._rng = RNG(seed=self.random_seed)\n\n    def make_random_seed(self):\n        return self._rng.randint(2 ** 31 - 1)\n"""
boltzmann_machines/base/tf_model.py,14,"b'import os\nimport json\nimport tensorflow as tf\nfrom functools import wraps\n\nfrom ..base import (BaseModel, DtypeMixin,\n                                     is_param_name)\n\n\ndef run_in_tf_session(check_initialized=True, update_seed=False):\n    """"""Decorator function that takes care to load appropriate graph/session,\n    depending on whether model can be loaded from disk or is just created,\n    and to execute `f` inside this session.\n    """"""\n    def wrap(f):\n        @wraps(f)  # preserve bound method properties\n        def wrapped_f(model, *args, **kwargs):\n            tf.reset_default_graph()\n            model._tf_graph = tf.get_default_graph()\n            if update_seed:\n                tf.set_random_seed(model.make_random_seed())\n            if model.initialized_:  # model should be loaded from disk\n                model._tf_saver = tf.train.import_meta_graph(model._tf_meta_graph_filepath)\n                with model._tf_graph.as_default():\n                    with tf.Session(config=model._tf_session_config) as model._tf_session:\n                        model._tf_saver.restore(model._tf_session, model._model_filepath)\n                        model._init_tf_writers()\n                        res = f(model, *args, **kwargs)\n            elif check_initialized:\n                raise RuntimeError(\'`fit` or `init` must be called before calling `{0}`\'.format(f.__name__))\n            else:\n                with model._tf_graph.as_default():\n                    with tf.Session(config=model._tf_session_config) as model._tf_session:\n                        model._make_tf_model()\n                        model._init_tf_ops()\n                        model._init_tf_writers()\n                        res = f(model, *args, **kwargs)\n            return res\n        return wrapped_f\n    return wrap\n\n\nclass TensorFlowModel(BaseModel, DtypeMixin):\n    def __init__(self, model_path=\'tf_model/\', paths=None,\n                 tf_session_config=None, tf_saver_params=None, json_params=None,\n                 *args, **kwargs):\n        super(TensorFlowModel, self).__init__(*args, **kwargs)\n        self._model_dirpath = None\n        self._model_filepath = None\n        self._params_filepath = None\n        self._random_state_filepath = None\n        self._train_summary_dirpath = None\n        self._val_summary_dirpath = None\n        self._tf_meta_graph_filepath = None\n        self.update_working_paths(model_path=model_path, paths=paths)\n\n        self._tf_session_config = tf_session_config or tf.ConfigProto()\n        self.tf_saver_params = tf_saver_params or {}\n        self.json_params = json_params or {}\n        self.json_params.setdefault(\'sort_keys\', True)\n        self.json_params.setdefault(\'indent\', 4)\n        self.initialized_ = False\n\n        self._tf_graph = tf.Graph()\n        self._tf_session = None\n        self._tf_saver = None\n        self._tf_merged_summaries = None\n        self._tf_train_writer = None\n        self._tf_val_writer = None\n\n    @staticmethod\n    def compute_working_paths(model_path):\n        """"""\n        Parameters\n        ----------\n        model_path : str\n            Model dirpath (should contain slash at the end) or filepath\n        """"""\n        head, tail = os.path.split(model_path)\n        if not head: head = \'.\'\n        if not head.endswith(\'/\'): head += \'/\'\n        if not tail: tail = \'model\'\n\n        paths = {}\n        paths[\'model_dirpath\'] = head\n        paths[\'model_filepath\'] = os.path.join(paths[\'model_dirpath\'], tail)\n        paths[\'params_filepath\'] = os.path.join(paths[\'model_dirpath\'], \'params.json\')\n        paths[\'random_state_filepath\'] = os.path.join(paths[\'model_dirpath\'], \'random_state.json\')\n        paths[\'train_summary_dirpath\'] = os.path.join(paths[\'model_dirpath\'], \'logs/train\')\n        paths[\'val_summary_dirpath\'] = os.path.join(paths[\'model_dirpath\'], \'logs/val\')\n        paths[\'tf_meta_graph_filepath\'] = paths[\'model_filepath\'] + \'.meta\'\n        return paths\n\n    def update_working_paths(self, model_path=None, paths=None):\n        paths = paths or {}\n        if not paths:\n            paths = TensorFlowModel.compute_working_paths(model_path=model_path)\n        for k, v in paths.items():\n            setattr(self, \'_{0}\'.format(k), v)\n\n    def _make_tf_model(self):\n        raise NotImplementedError(\'`_make_tf_model` is not implemented\')\n\n    def _init_tf_ops(self):\n        """"""Initialize all TF variables and Saver""""""\n        init_op = tf.global_variables_initializer()\n        self._tf_session.run(init_op)\n        self._tf_saver = tf.train.Saver(**self.tf_saver_params)\n\n    def _init_tf_writers(self):\n        self._tf_merged_summaries = tf.summary.merge_all()\n        self._tf_train_writer = tf.summary.FileWriter(self._train_summary_dirpath,\n                                                      self._tf_graph)\n        self._tf_val_writer = tf.summary.FileWriter(self._val_summary_dirpath,\n                                                    self._tf_graph)\n\n    def _save_model(self, global_step=None):\n        # (recursively) create all folders needed\n        for dirpath in (self._train_summary_dirpath, self._val_summary_dirpath):\n            if not os.path.exists(dirpath):\n                os.makedirs(dirpath)\n\n        # save params\n        params = self.get_params(deep=False)\n        params = self._serialize(params)\n        params[\'__class_name__\'] = self.__class__.__name__\n        with open(self._params_filepath, \'w\') as params_file:\n            json.dump(params, params_file, **self.json_params)\n\n        # dump random state if needed\n        if self.random_seed is not None:\n            random_state = self._rng.get_state()\n            with open(self._random_state_filepath, \'w\') as random_state_file:\n                json.dump(random_state, random_state_file)\n\n        # save tf model\n        self._tf_saver.save(self._tf_session,\n                            self._model_filepath,\n                            global_step=global_step)\n\n    @classmethod\n    def load_model(cls, model_path):\n        paths = TensorFlowModel.compute_working_paths(model_path)\n\n        # load params\n        with open(paths[\'params_filepath\'], \'r\') as params_file:\n            params = json.load(params_file)\n        class_name = params.pop(\'__class_name__\')\n        if class_name != cls.__name__:\n            raise RuntimeError(""attempt to load {0} with class {1}"".format(class_name, cls.__name__))\n        model = cls(paths=paths, **{k: params[k] for k in params if is_param_name(k)})\n        params = model._deserialize(params)\n        model.set_params(**params)  # set attributes and deserialized params\n\n        # restore random state if needed\n        if os.path.isfile(model._random_state_filepath):\n            with open(model._random_state_filepath, \'r\') as random_state_file:\n                random_state = json.load(random_state_file)\n            model._rng.set_state(random_state)\n\n        # (tf model will be loaded once any computation will be needed)\n        return model\n\n    def _fit(self, X, X_val=None, *args, **kwargs):\n        """"""Class-specific `fit` routine.""""""\n        raise NotImplementedError(\'`fit` is not implemented\')\n\n    @run_in_tf_session(check_initialized=False)\n    def init(self):\n        if not self.initialized_:\n            self.initialized_ = True\n            self._save_model()\n        return self\n\n    @run_in_tf_session(check_initialized=False, update_seed=True)\n    def fit(self, X, X_val=None, *args, **kwargs):\n        """"""Fit the model according to the given training data.""""""\n        self.initialized_ = True\n        self._fit(X, X_val=X_val, *args, **kwargs)\n        self._save_model()\n        return self\n\n    @run_in_tf_session()\n    def get_tf_params(self, scope=None):\n        """"""Get tf params of the model.\n\n        Returns\n        -------\n        params : dict[str] = np.ndarray\n            Evaluated parameters of the model.\n        """"""\n        weights = {}\n        for var in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope):\n            key = var.name\n            if scope and scope in key:\n                key = key.replace(scope, \'\')\n            if key.startswith(\'/\'):\n                key = key[1:]\n            if key.endswith(\':0\'):\n                key = key[:-2]\n            weights[key] = var.eval()\n        return weights\n\n\nif __name__ == \'__main__\':\n    # run corresponding tests\n    from ..utils.testing import run_tests\n    from .tests import test_tf_model as t\n    run_tests(__file__, t)\n'"
boltzmann_machines/rbm/__init__.py,0,b'from .base_rbm import *\nfrom .rbm import *\n'
boltzmann_machines/rbm/base_rbm.py,124,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.core.framework import summary_pb2\n\nfrom ..ebm import EnergyBasedModel\nfrom ..base import run_in_tf_session, is_attribute_name\nfrom ..utils import (make_list_from, batch_iter, epoch_iter,\n                                      write_during_training)\nfrom ..utils.testing import assert_len, assert_shape\n\n\nclass BaseRBM(EnergyBasedModel):\n    """"""\n    A generic implementation of Restricted Boltzmann Machine\n    with k-step Contrastive Divergence (CD-k) learning algorithm.\n\n    Parameters\n    ----------\n    n_visible : positive int\n        Number of visible units.\n    n_hidden : positive int\n        Number of hidden units.\n    W_init : float or (n_visible, n_hidden) iterable\n        Weight matrix initialization. If float, initialize from zero-centered\n        Gaussian with this standard deviation. If iterable, initialize from it.\n    vb_init, hb_init : float or iterable\n        Visible and hidden unit bias(es).\n    n_gibbs_steps : positive int\n        Number of Gibbs steps per iteration (per weight update).\n    learning_rate, momentum : positive float or iterable\n        Gradient descent parameters. Values are updated after each epoch.\n    max_epoch : positive int\n        Train till this epoch.\n    batch_size : positive int\n        Input batch size for training.\n    l2 : non-negative float\n        L2 weight decay coefficient.\n    sample_v_states, sample_h_states : bool\n        Whether to sample visible/hidden states, or to use probabilities\n        w/o sampling. Note that data driven states for hidden units will\n        be sampled regardless of the provided parameters.\n    dropout : None or float in [0, 1]\n        If float, interpreted as probability of visible units being on.\n    sparsity_target : float in (0, 1)\n        Desired probability of hidden activation.\n    sparsity_cost : non-negative float\n        Controls the amount of sparsity penalty.\n    sparsity_damping : float in (0, 1)\n        Decay rate for hidden activations probs.\n    dbm_first, dbm_last : bool\n        Flag whether RBM is first or last in a stack of RBMs used\n        for DBM pre-training to address ""double counting evidence"" problem [4].\n    metrics_config : dict\n        Parameters that controls which metrics and how often they are computed.\n        Possible (optional) commands:\n        * l2_loss : bool, default False\n            Whether to compute weight decay penalty.\n        * msre : bool, default False\n            Whether to compute MSRE = mean squared reconstruction error.\n        * pll : bool, default False\n            Whether to compute pseudo-loglikelihood estimation. Only makes sense\n            to compute for binary visible units (BernoulliRBM, MultinomialRBM).\n        * feg : bool, default False\n            Whether to compute free energy gap.\n        * l2_loss_fmt : str, default \'.2e\'\n        * msre_fmt : str, default \'.4f\'\n        * pll_fmt : str, default \'.3f\'\n        * feg_fmt : str, default \'.2f\'\n        * train_metrics_every_iter : non-negative int, default 10\n        * val_metrics_every_epoch : non-negative int, default 1\n        * feg_every_epoch : non-negative int, default 2\n        * n_batches_for_feg : non-negative int, default 10\n    verbose : bool\n        Whether to display progress during training.\n    save_after_each_epoch : bool\n        If False, save model only after the whole training is complete.\n    display_filters : non-negative int\n        Number of weights filters to display during training (in TensorBoard).\n    display_hidden_activations : non-negative int\n        Number of hidden activations to display during training (in TensorBoard).\n    v_shape : (H, W) or (H, W, C) positive integer tuple\n        Shape for displaying filters during training. C should be in {1, 3, 4}.\n\n    References\n    ----------\n    [1] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning.\n        MIT press, 2016.\n    [2] G. Hinton. A Practical Guide to Training Restricted Boltzmann\n        Machines. UTML TR 2010-003\n    [3] Restricted Boltzmann Machines (RBMs), Deep Learning Tutorial\n        url: http://deeplearning.net/tutorial/rbm.html\n    [4] R. Salakhutdinov and G. Hinton. Deep boltzmann machines.\n        In AISTATS, pp. 448-455. 2009\n    """"""\n    def __init__(self,\n                 n_visible=784, v_layer_cls=None, v_layer_params=None,\n                 n_hidden=256, h_layer_cls=None, h_layer_params=None,\n                 W_init=0.01, vb_init=0., hb_init=0., n_gibbs_steps=1,\n                 learning_rate=0.01, momentum=0.9, max_epoch=10, batch_size=10, l2=1e-4,\n                 sample_v_states=False, sample_h_states=True, dropout=None,\n                 sparsity_target=0.1, sparsity_cost=0., sparsity_damping=0.9,\n                 dbm_first=False, dbm_last=False,\n                 metrics_config=None, verbose=True, save_after_each_epoch=True,\n                 display_filters=0, display_hidden_activations=0, v_shape=(28, 28),\n                 model_path=\'rbm_model/\', *args, **kwargs):\n        super(BaseRBM, self).__init__(model_path=model_path, *args, **kwargs)\n        self.n_visible = n_visible\n        self.n_hidden = n_hidden\n\n        v_layer_params = v_layer_params or {}\n        v_layer_params.setdefault(\'n_units\', self.n_visible)\n        v_layer_params.setdefault(\'dtype\', self.dtype)\n        h_layer_params = h_layer_params or {}\n        h_layer_params.setdefault(\'n_units\', self.n_hidden)\n        h_layer_params.setdefault(\'dtype\', self.dtype)\n        self._v_layer = v_layer_cls(**v_layer_params)\n        self._h_layer = h_layer_cls(**h_layer_params)\n\n        self.W_init = W_init\n        if hasattr(self.W_init, \'__iter__\'):\n            self.W_init = np.asarray(self.W_init)\n            assert_shape(self, \'W_init\', (self.n_visible, self.n_hidden))\n\n        # Visible biases can be initialized with list of values,\n        # because it is often helpful to initialize i-th visible bias\n        # with value log(p_i / (1 - p_i)), p_i = fraction of training\n        # vectors where i-th unit is on, as proposed in [2]\n        self.vb_init = vb_init\n        if hasattr(self.vb_init, \'__iter__\'):\n            self.vb_init = np.asarray(self.vb_init)\n            assert_len(self, \'vb_init\', self.n_visible)\n\n        self.hb_init = hb_init\n        if hasattr(self.hb_init, \'__iter__\'):\n            self.hb_init = np.asarray(self.hb_init)\n            assert_len(self, \'hb_init\', self.n_hidden)\n\n        # these can be set in `init_from` method\n        self._dW_init = None\n        self._dvb_init = None\n        self._dhb_init = None\n\n        self.n_gibbs_steps = make_list_from(n_gibbs_steps)\n        self.learning_rate = make_list_from(learning_rate)\n        self.momentum = make_list_from(momentum)\n        self.max_epoch = max_epoch\n        self.batch_size = batch_size\n        self.l2 = l2\n\n        # According to [2], the training goes less noisy and slightly faster, if\n        # sampling used for states of hidden units driven by the data, and probabilities\n        # for ones driven by reconstructions, and if probabilities (means) used for visible units,\n        # both driven by data and by reconstructions. It is therefore recommended to set\n        # these parameter to False (default).\n        self.sample_h_states = sample_h_states\n        self.sample_v_states = sample_v_states\n        self.dropout = dropout\n\n        self.sparsity_target = sparsity_target\n        self.sparsity_cost = sparsity_cost\n        self.sparsity_damping = sparsity_damping\n\n        self.dbm_first = dbm_first\n        self.dbm_last = dbm_last\n\n        self.metrics_config = metrics_config or {}\n        self.metrics_config.setdefault(\'l2_loss\', False)\n        self.metrics_config.setdefault(\'msre\', False)\n        self.metrics_config.setdefault(\'pll\', False)\n        self.metrics_config.setdefault(\'feg\', False)\n        self.metrics_config.setdefault(\'l2_loss_fmt\', \'.2e\')\n        self.metrics_config.setdefault(\'msre_fmt\', \'.4f\')\n        self.metrics_config.setdefault(\'pll_fmt\', \'.3f\')\n        self.metrics_config.setdefault(\'feg_fmt\', \'.2f\')\n        self.metrics_config.setdefault(\'train_metrics_every_iter\', 10)\n        self.metrics_config.setdefault(\'val_metrics_every_epoch\', 1)\n        self.metrics_config.setdefault(\'feg_every_epoch\', 2)\n        self.metrics_config.setdefault(\'n_batches_for_feg\', 10)\n        self._metrics_names_map={\n            \'feg\': \'free_energy_gap\',\n            \'l2_loss\': \'l2_loss\',\n            \'msre\': \'mean_squared_reconstruction_error\',\n            \'pll\': \'pseudo_loglikelihood\'\n        }\n        self._train_metrics_names = (\'l2_loss\', \'msre\', \'pll\')\n        self._train_metrics_map = {}\n        self._val_metrics_names = (\'msre\', \'pll\')\n        self._val_metrics_map = {}\n\n        self.verbose = verbose\n        self.save_after_each_epoch = save_after_each_epoch\n\n        assert self.n_hidden >= display_filters\n        self.display_filters = display_filters\n\n        assert self.n_hidden >= display_hidden_activations\n        self.display_hidden_activations = display_hidden_activations\n\n        self.v_shape = v_shape\n        if len(self.v_shape) == 2:\n            self.v_shape = (self.v_shape[0], self.v_shape[1], 1)\n\n        # current epoch and iteration\n        self.epoch_ = 0\n        self.iter_ = 0\n\n        # tf constants\n        self._n_visible = None\n        self._n_hidden = None\n        self._l2 = None\n        self._dropout = None\n        self._sparsity_target = None\n        self._sparsity_cost = None\n        self._sparsity_damping = None\n        self._dbm_first = None\n        self._dbm_last = None\n        self._propup_multiplier = None\n        self._propdown_multiplier = None\n\n        # tf input data\n        self._learning_rate = None\n        self._momentum = None\n        self._n_gibbs_steps = None\n        self._X_batch = None\n\n        # tf vars\n        self._W = None\n        self._hb = None\n        self._vb = None\n\n        self._dW = None\n        self._dhb = None\n        self._dvb = None\n\n        self._q_means = None\n\n        # tf operations\n        self._train_op = None\n        self._transform_op = None\n        self._msre = None\n        self._pll = None\n        self._free_energy_op = None\n\n    def _make_constants(self):\n        with tf.name_scope(\'constants\'):\n            self._n_visible = tf.constant(self.n_visible, dtype=tf.int32, name=\'n_visible\')\n            self._n_hidden = tf.constant(self.n_hidden, dtype=tf.int32, name=\'n_hidden\')\n            self._l2 = tf.constant(self.l2, dtype=self._tf_dtype, name=\'L2_coef\')\n\n            if self.dropout is not None:\n                self._dropout = tf.constant(self.dropout, dtype=self._tf_dtype, name=\'dropout_prob\')\n            self._sparsity_target = tf.constant(self.sparsity_target, dtype=self._tf_dtype, name=\'sparsity_target\')\n            self._sparsity_cost = tf.constant(self.sparsity_cost, dtype=self._tf_dtype, name=\'sparsity_cost\')\n            self._sparsity_damping = tf.constant(self.sparsity_damping, dtype=self._tf_dtype, name=\'sparsity_damping\')\n\n            self._dbm_first = tf.constant(self.dbm_first, dtype=tf.bool, name=\'is_dbm_first\')\n            self._dbm_last = tf.constant(self.dbm_last, dtype=tf.bool, name=\'is_dbm_last\')\n            t = tf.constant(1., dtype=self._tf_dtype, name=""1"")\n            t1 = tf.cast(self._dbm_first, dtype=self._tf_dtype)\n            self._propup_multiplier = tf.identity(tf.add(t1, t), name=\'propup_multiplier\')\n            t2 = tf.cast(self._dbm_last, dtype=self._tf_dtype)\n            self._propdown_multiplier = tf.identity(tf.add(t2, t), name=\'propdown_multiplier\')\n\n    def _make_placeholders(self):\n        with tf.name_scope(\'input_data\'):\n            self._learning_rate = tf.placeholder(self._tf_dtype, [], name=\'learning_rate\')\n            self._momentum = tf.placeholder(self._tf_dtype, [], name=\'momentum\')\n            self._n_gibbs_steps = tf.placeholder(tf.int32, [], name=\'n_gibbs_steps\')\n            self._X_batch = tf.placeholder(self._tf_dtype, [None, self.n_visible], name=\'X_batch\')\n\n    def _make_vars(self):\n        # initialize weights and biases\n        with tf.name_scope(\'weights\'):\n            if hasattr(self.W_init, \'__iter__\'):\n                W_init = tf.constant(self.W_init, dtype=self._tf_dtype)\n            else:\n                W_init = tf.random_normal([self._n_visible, self._n_hidden],\n                                           mean=0.0, stddev=self.W_init,\n                                           seed=self.random_seed, dtype=self._tf_dtype)\n            W_init = tf.identity(W_init, name=\'W_init\')\n\n            vb_init = self.vb_init if hasattr(self.vb_init, \'__iter__\') else\\\n                      np.repeat(self.vb_init, self.n_visible)\n\n            hb_init = self.hb_init if hasattr(self.hb_init, \'__iter__\') else\\\n                      np.repeat(self.hb_init, self.n_hidden)\n\n            vb_init = tf.constant(vb_init, dtype=self._tf_dtype, name=\'vb_init\')\n            hb_init = tf.constant(hb_init, dtype=self._tf_dtype, name=\'hb_init\')\n\n            self._W = tf.Variable(W_init, dtype=self._tf_dtype, name=\'W\')\n            self._vb = tf.Variable(vb_init, dtype=self._tf_dtype, name=\'vb\')\n            self._hb = tf.Variable(hb_init, dtype=self._tf_dtype, name=\'hb\')\n\n            tf.summary.histogram(\'W\', self._W)\n            tf.summary.histogram(\'vb\', self._vb)\n            tf.summary.histogram(\'hb\', self._hb)\n\n        # visualize filters\n        if self.display_filters:\n            with tf.name_scope(\'filters_visualization\'):\n                W_display = tf.transpose(self._W, [1, 0])\n                W_display = tf.reshape(W_display, [self.n_hidden, self.v_shape[2],\n                                                   self.v_shape[0], self.v_shape[1]])\n                W_display = tf.transpose(W_display, [0, 2, 3, 1])\n                tf.summary.image(\'W_filters\', W_display, max_outputs=self.display_filters)\n\n        # initialize gradients accumulators\n        with tf.name_scope(\'grads_accumulators\'):\n            dW_init = tf.constant(self._dW_init, dtype=self._tf_dtype) if self._dW_init is not None else \\\n                      tf.zeros([self._n_visible, self._n_hidden], dtype=self._tf_dtype)\n            dvb_init = tf.constant(self._dvb_init, dtype=self._tf_dtype) if self._dvb_init is not None else \\\n                       tf.zeros([self._n_visible], dtype=self._tf_dtype)\n            dhb_init = tf.constant(self._dhb_init, dtype=self._tf_dtype) if self._dhb_init is not None else \\\n                       tf.zeros([self._n_hidden], dtype=self._tf_dtype)\n\n            self._dW = tf.Variable(dW_init, name=\'dW\')\n            self._dvb = tf.Variable(dvb_init, name=\'dvb\')\n            self._dhb = tf.Variable(dhb_init, name=\'dhb\')\n\n            tf.summary.histogram(\'dW\', self._dW)\n            tf.summary.histogram(\'dvb\', self._dvb)\n            tf.summary.histogram(\'dhb\', self._dhb)\n\n        # initialize running means of hidden activations means\n        with tf.name_scope(\'hidden_activations_means\'):\n            self._q_means = tf.Variable(tf.zeros([self._n_hidden], dtype=self._tf_dtype), name=\'q_means\')\n\n    def _propup(self, v):\n        with tf.name_scope(\'prop_up\'):\n            t = tf.matmul(v, self._W)\n        return t\n\n    def _propdown(self, h):\n        with tf.name_scope(\'prop_down\'):\n            t = tf.matmul(a=h, b=self._W, transpose_b=True)\n        return t\n\n    def _means_h_given_v(self, v):\n        """"""Compute means E(h|v).""""""\n        with tf.name_scope(\'means_h_given_v\'):\n            x  = self._propup_multiplier * self._propup(v)\n            hb = self._propup_multiplier * self._hb\n            h_means = self._h_layer.activation(x=x, b=hb)\n        return h_means\n\n    def _sample_h_given_v(self, h_means):\n        """"""Sample from P(h|v).""""""\n        with tf.name_scope(\'sample_h_given_v\'):\n            h_samples = self._h_layer.sample(means=h_means)\n        return h_samples\n\n    def _means_v_given_h(self, h):\n        """"""Compute means E(v|h).""""""\n        with tf.name_scope(\'means_v_given_h\'):\n            x  = self._propdown_multiplier * self._propdown(h)\n            vb = self._propdown_multiplier * self._vb\n            v_means = self._v_layer.activation(x=x, b=vb)\n        return v_means\n\n    def _sample_v_given_h(self, v_means):\n        """"""Sample from P(v|h).""""""\n        with tf.name_scope(\'sample_v_given_h\'):\n            v_samples = self._v_layer.sample(means=v_means)\n        return v_samples\n\n    def _make_gibbs_step(self, h_states):\n        """"""Compute one Gibbs step.""""""\n        with tf.name_scope(\'gibbs_step\'):\n            v_states = v_means = self._means_v_given_h(h_states)\n            if self.sample_v_states:\n                v_states = self._sample_v_given_h(v_means)\n\n            h_states = h_means = self._means_h_given_v(v_states)\n            if self.sample_h_states:\n                h_states = self._sample_h_given_v(h_means)\n\n        return v_states, v_means, h_states, h_means\n\n    def _make_gibbs_chain_fixed(self, h_states):\n        v_states = v_means = h_means = None\n        for _ in range(self.n_gibbs_steps[0]):\n            v_states, v_means, h_states, h_means = self._make_gibbs_step(h_states)\n        return v_states, v_means, h_states, h_means\n\n    def _make_gibbs_chain_variable(self, h_states):\n        def cond(step, max_step, v_states, v_means, h_states, h_means):\n            return step < max_step\n\n        def body(step, max_step, v_states, v_means, h_states, h_means):\n            v_states, v_means, h_states, h_means = self._make_gibbs_step(h_states)\n            return step + 1, max_step, v_states, v_means, h_states, h_means\n\n        _, _, v_states, v_means, h_states, h_means = \\\n            tf.while_loop(cond=cond, body=body,\n                          loop_vars=[tf.constant(0),\n                                     self._n_gibbs_steps,\n                                     tf.zeros_like(self._X_batch),\n                                     tf.zeros_like(self._X_batch),\n                                     h_states,\n                                     tf.zeros_like(h_states)],\n                          back_prop=False,\n                          parallel_iterations=1)\n\n        return v_states, v_means, h_states, h_means\n\n    def _make_gibbs_chain(self, *args, **kwargs):\n        # use faster implementation (w/o while loop) when\n        # number of Gibbs steps is fixed\n        if len(self.n_gibbs_steps) == 1:\n            return self._make_gibbs_chain_fixed(*args, **kwargs)\n        else:\n            return self._make_gibbs_chain_variable(*args, **kwargs)\n\n    def _make_train_op(self):\n        # apply dropout if necessary\n        if self.dropout is not None:\n            self._X_batch = tf.nn.dropout(self._X_batch, keep_prob=self._dropout)\n\n        # Run Gibbs chain for specified number of steps.\n        with tf.name_scope(\'gibbs_chain\'):\n            h0_means = self._means_h_given_v(self._X_batch)\n            h0_samples = self._sample_h_given_v(h0_means)\n            h_states = h0_samples if self.sample_h_states else h0_means\n\n            v_states, v_means, _, h_means = self._make_gibbs_chain(h_states)\n\n        # visualize hidden activation means\n        if self.display_hidden_activations:\n            with tf.name_scope(\'hidden_activations_visualization\'):\n                h_means_display = h_means[:, :self.display_hidden_activations]\n                h_means_display = tf.cast(h_means_display, tf.float32)\n                h_means_display = tf.expand_dims(h_means_display, 0)\n                h_means_display = tf.expand_dims(h_means_display, -1)\n                tf.summary.image(\'hidden_activation_means\', h_means_display)\n\n        # encoded data, used by the transform method\n        with tf.name_scope(\'transform\'):\n            transform_op = tf.identity(h_means)\n            tf.add_to_collection(\'transform_op\', transform_op)\n\n        # compute gradients estimates (= positive - negative associations)\n        with tf.name_scope(\'grads_estimates\'):\n            # number of training examples might not be divisible by batch size\n            N = tf.cast(tf.shape(self._X_batch)[0], dtype=self._tf_dtype)\n            with tf.name_scope(\'dW\'):\n                dW_positive = tf.matmul(self._X_batch, h0_means, transpose_a=True)\n                dW_negative = tf.matmul(v_states, h_means, transpose_a=True)\n                dW = (dW_positive - dW_negative) / N - self._l2 * self._W\n            with tf.name_scope(\'dvb\'):\n                dvb = tf.reduce_mean(self._X_batch - v_states, axis=0) # == sum / N\n            with tf.name_scope(\'dhb\'):\n                dhb = tf.reduce_mean(h0_means - h_means, axis=0) # == sum / N\n\n        # apply sparsity targets if needed\n        with tf.name_scope(\'sparsity_targets\'):\n            q_means = tf.reduce_sum(h_means, axis=0)\n            q_update = self._q_means.assign(self._sparsity_damping * self._q_means + \\\n                                            (1 - self._sparsity_damping) * q_means)\n            sparsity_penalty = self._sparsity_cost * (q_update - self._sparsity_target)\n            dhb -= sparsity_penalty\n            dW  -= sparsity_penalty\n\n        # update parameters\n        with tf.name_scope(\'momentum_updates\'):\n            with tf.name_scope(\'dW\'):\n                dW_update = self._dW.assign(self._learning_rate * (self._momentum * self._dW + dW))\n                W_update = self._W.assign_add(dW_update)\n            with tf.name_scope(\'dvb\'):\n                dvb_update = self._dvb.assign(self._learning_rate * (self._momentum * self._dvb + dvb))\n                vb_update = self._vb.assign_add(dvb_update)\n            with tf.name_scope(\'dhb\'):\n                dhb_update = self._dhb.assign(self._learning_rate * (self._momentum * self._dhb + dhb))\n                hb_update = self._hb.assign_add(dhb_update)\n\n        # assemble train_op\n        with tf.name_scope(\'training_step\'):\n            train_op = tf.group(W_update, vb_update, hb_update)\n            tf.add_to_collection(\'train_op\', train_op)\n\n        # compute metrics\n        with tf.name_scope(\'L2_loss\'):\n            l2_loss = self._l2 * tf.nn.l2_loss(self._W)\n            tf.add_to_collection(\'l2_loss\', l2_loss)\n\n        with tf.name_scope(\'mean_squared_recon_error\'):\n            msre = tf.reduce_mean(tf.square(self._X_batch - v_means))\n            tf.add_to_collection(\'msre\', msre)\n\n        # Since reconstruction error is fairly poor measure of performance,\n        # as this is not what CD-k learning algorithm aims to minimize [2],\n        # compute (per sample average) pseudo-loglikelihood (proxy to likelihood)\n        # instead, which not only is much more cheaper to compute, but also\n        # learning with PLL is asymptotically consistent [1].\n        # More specifically, PLL computed using approximation as in [3].\n        with tf.name_scope(\'pseudo_loglik\'):\n            x = self._X_batch\n            # randomly corrupt one feature in each sample\n            x_ = tf.identity(x)\n            batch_size = tf.shape(x)[0]\n            pll_rand = tf.random_uniform([batch_size], minval=0, maxval=self._n_visible,\n                                         dtype=tf.int32)\n            ind = tf.transpose([tf.range(batch_size), pll_rand])\n            m = tf.SparseTensor(indices=tf.to_int64(ind),\n                                values=tf.ones_like(pll_rand, dtype=self._tf_dtype),\n                                dense_shape=tf.to_int64(tf.shape(x_)))\n            x_ = tf.multiply(x_, -tf.sparse_tensor_to_dense(m, default_value=-1))\n            x_ = tf.sparse_add(x_, m)\n            x_ = tf.identity(x_, name=\'x_corrupted\')\n\n            pll = tf.cast(self._n_visible, dtype=self._tf_dtype) *\\\n                  tf.log_sigmoid(self._free_energy(x_)-self._free_energy(x))\n            tf.add_to_collection(\'pll\', pll)\n\n        # add also free energy of input batch to collection (for feg)\n        free_energy_op = self._free_energy(self._X_batch)\n        tf.add_to_collection(\'free_energy_op\', free_energy_op)\n\n        # collect summaries\n        if self.metrics_config[\'l2_loss\']:\n            tf.summary.scalar(self._metrics_names_map[\'l2_loss\'], l2_loss)\n        if self.metrics_config[\'msre\']:\n            tf.summary.scalar(self._metrics_names_map[\'msre\'], msre)\n        if self.metrics_config[\'pll\']:\n            tf.summary.scalar(self._metrics_names_map[\'pll\'], pll)\n\n    def _make_tf_model(self):\n        self._make_constants()\n        self._make_placeholders()\n        self._make_vars()\n        self._make_train_op()\n\n    def _make_tf_feed_dict(self, X_batch, n_gibbs_steps=None):\n        d = {}\n        d[\'learning_rate\'] = self.learning_rate[min(self.epoch_, len(self.learning_rate) - 1)]\n        d[\'momentum\'] = self.momentum[min(self.epoch_, len(self.momentum) - 1)]\n        d[\'X_batch\'] = X_batch\n        if n_gibbs_steps is not None:\n            d[\'n_gibbs_steps\'] = n_gibbs_steps\n        else:\n            d[\'n_gibbs_steps\'] = self.n_gibbs_steps[min(self.epoch_, len(self.n_gibbs_steps) - 1)]\n\n        # prepend name of the scope, and append \':0\'\n        feed_dict = {}\n        for k, v in d.items():\n            feed_dict[\'input_data/{0}:0\'.format(k)] = v\n        return feed_dict\n\n    def _train_epoch(self, X):\n        results = [[] for _ in range(len(self._train_metrics_map))]\n        for X_batch in batch_iter(X, self.batch_size,\n                                  verbose=self.verbose):\n            self.iter_ += 1\n            if self.iter_ % self.metrics_config[\'train_metrics_every_iter\'] == 0:\n                run_ops = [v for _, v in sorted(self._train_metrics_map.items())]\n                run_ops += [self._tf_merged_summaries, self._train_op]\n                outputs = \\\n                self._tf_session.run(run_ops,\n                                     feed_dict=self._make_tf_feed_dict(X_batch))\n                values = outputs[:len(self._train_metrics_map)]\n                for i, v in enumerate(values):\n                    results[i].append(v)\n                train_s = outputs[len(self._train_metrics_map)]\n                self._tf_train_writer.add_summary(train_s, self.iter_)\n            else:\n                self._tf_session.run(self._train_op,\n                                     feed_dict=self._make_tf_feed_dict(X_batch))\n\n        # aggregate and return metrics values\n        results = map(lambda r: np.mean(r) if r else None, results)\n        return dict(zip(sorted(self._train_metrics_map), results))\n\n    def _run_val_metrics(self, X_val):\n        results = [[] for _ in range(len(self._val_metrics_map))]\n        for X_vb in batch_iter(X_val, batch_size=self.batch_size):\n            run_ops = [v for _, v in sorted(self._val_metrics_map.items())]\n            values = \\\n            self._tf_session.run(run_ops,\n                                 feed_dict=self._make_tf_feed_dict(X_vb))\n            for i, v in enumerate(values):\n                results[i].append(v)\n        for i, r in enumerate(results):\n            results[i] = np.mean(r) if r else None\n        summary_value = []\n        for i, m in enumerate(sorted(self._val_metrics_map)):\n            summary_value.append(summary_pb2.Summary.Value(tag=self._metrics_names_map[m],\n                                                           simple_value=results[i]))\n        val_s = summary_pb2.Summary(value=summary_value)\n        self._tf_val_writer.add_summary(val_s, self.iter_)\n        return dict(zip(sorted(self._val_metrics_map), results))\n\n    def _run_feg(self, X, X_val):\n        """"""Calculate difference between average free energies of subsets\n        of validation and training sets to monitor overfitting,\n        as proposed in [2]. If the model is not overfitting at all, this\n        quantity should be close to zero. Once this value starts\n        growing, the model is overfitting and the value (""free energy gap"")\n        represents the amount of overfitting.\n        """"""\n        self._free_energy_op = tf.get_collection(\'free_energy_op\')[0]\n\n        train_fes = []\n        for _, X_b in zip(range(self.metrics_config[\'n_batches_for_feg\']),\n                          batch_iter(X, batch_size=self.batch_size)):\n            train_fe = self._tf_session.run(self._free_energy_op,\n                                            feed_dict=self._make_tf_feed_dict(X_b))\n            train_fes.append(train_fe)\n\n        val_fes = []\n        for _, X_vb in zip(range(self.metrics_config[\'n_batches_for_feg\']),\n                           batch_iter(X_val, batch_size=self.batch_size)):\n            val_fe = self._tf_session.run(self._free_energy_op,\n                                          feed_dict=self._make_tf_feed_dict(X_vb))\n            val_fes.append(val_fe)\n\n        feg = np.mean(val_fes) - np.mean(train_fes)\n        summary_value = [summary_pb2.Summary.Value(tag=self._metrics_names_map[\'feg\'],\n                                                   simple_value=feg)]\n        feg_s = summary_pb2.Summary(value=summary_value)\n        self._tf_val_writer.add_summary(feg_s, self.iter_)\n        return feg\n\n    def _fit(self, X, X_val=None, *args, **kwargs):\n        # load ops requested\n        self._train_op = tf.get_collection(\'train_op\')[0]\n\n        self._train_metrics_map = {}\n        for m in self._train_metrics_names:\n            if self.metrics_config[m]:\n                self._train_metrics_map[m] = tf.get_collection(m)[0]\n\n        self._val_metrics_map = {}\n        for m in self._val_metrics_names:\n            if self.metrics_config[m]:\n                self._val_metrics_map[m] = tf.get_collection(m)[0]\n\n        # main loop\n        for self.epoch_ in epoch_iter(start_epoch=self.epoch_, max_epoch=self.max_epoch,\n                                      verbose=self.verbose):\n            val_results = {}\n            feg = None\n            train_results = self._train_epoch(X)\n\n            # run validation metrics if needed\n            if X_val is not None and self.epoch_ % self.metrics_config[\'val_metrics_every_epoch\'] == 0:\n                val_results = self._run_val_metrics(X_val)\n            if X_val is not None and self.metrics_config[\'feg\'] and \\\n                    self.epoch_ % self.metrics_config[\'feg_every_epoch\'] == 0:\n                feg = self._run_feg(X, X_val)\n\n            # print progress\n            if self.verbose:\n                s = ""epoch: {0:{1}}/{2}"".format(self.epoch_, len(str(self.max_epoch)), self.max_epoch)\n                for m, v in sorted(train_results.items()):\n                    if v is not None:\n                        s += ""; {0}: {1:{2}}"".format(m, v, self.metrics_config[\'{0}_fmt\'.format(m)])\n                for m, v in sorted(val_results.items()):\n                    if v is not None:\n                        s += ""; val.{0}: {1:{2}}"".format(m, v, self.metrics_config[\'{0}_fmt\'.format(m)])\n                if feg is not None:\n                    s += "" ; feg: {0:{1}}"".format(feg, self.metrics_config[\'feg_fmt\'])\n                write_during_training(s)\n\n            # save if needed\n            if self.save_after_each_epoch:\n                self._save_model(global_step=self.epoch_)\n\n    def init_from(self, rbm):\n        if type(self) != type(rbm):\n            raise ValueError(\'an attempt to initialize `{0}` from `{1}`\'.\n                             format(self.__class__.__name__, rbm.__class__.__name__))\n        weights = rbm.get_tf_params(scope=\'weights\')\n        self.W_init = weights[\'W\']\n        self.vb_init = weights[\'vb\']\n        self.hb_init = weights[\'hb\']\n\n        grads_accumulators = rbm.get_tf_params(scope=\'grads_accumulators\')\n        self._dW_init = grads_accumulators[\'dW\']\n        self._dvb_init = grads_accumulators[\'dvb\']\n        self._dhb_init = grads_accumulators[\'dhb\']\n\n        # copy attributes\n        for k, v in vars(rbm).items():\n            if is_attribute_name(k):\n                setattr(self, k, v)\n\n    @run_in_tf_session(update_seed=True)\n    def transform(self, X, np_dtype=None):\n        """"""Compute hidden units\' activation probabilities.""""""\n        np_dtype = np_dtype or self._np_dtype\n\n        self._transform_op = tf.get_collection(\'transform_op\')[0]\n        H = np.zeros((len(X), self.n_hidden), dtype=np_dtype)\n        start = 0\n        for X_b in batch_iter(X, batch_size=self.batch_size,\n                              verbose=self.verbose, desc=\'transform\'):\n            H_b = self._transform_op.eval(feed_dict=self._make_tf_feed_dict(X_b))\n            H[start:(start + self.batch_size)] = H_b\n            start += self.batch_size\n        return H\n'"
boltzmann_machines/rbm/env.py,0,"b'import sys\nimport os.path as path\n# prepend parent directory to sys.path\nsys.path.insert(0, path.dirname(path.dirname(path.abspath(__file__))))\n'"
boltzmann_machines/rbm/rbm.py,21,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.distributions import Multinomial\n\nfrom .env import *\nfrom .base_rbm import BaseRBM\nfrom layers import BernoulliLayer, MultinomialLayer, GaussianLayer\n\n\nclass BernoulliRBM(BaseRBM):\n    """"""RBM with Bernoulli both visible and hidden units.""""""\n    def __init__(self, model_path=\'b_rbm_model/\', *args, **kwargs):\n        super(BernoulliRBM, self).__init__(v_layer_cls=BernoulliLayer,\n                                           h_layer_cls=BernoulliLayer,\n                                           model_path=model_path, *args, **kwargs)\n\n    def _free_energy(self, v):\n        with tf.name_scope(\'free_energy\'):\n            T1 = -tf.einsum(\'ij,j->i\', v, self._vb)\n            T2 = -tf.reduce_sum(tf.nn.softplus(self._propup(v) + self._hb), axis=1)\n            fe = tf.reduce_mean(T1 + T2, axis=0)\n        return fe\n\n\nclass MultinomialRBM(BaseRBM):\n    """"""RBM with Bernoulli visible and single Multinomial hidden unit\n    (= multiple softmax units with tied weights).\n\n    Parameters\n    ----------\n    n_hidden : int\n        Number of possible states of a multinomial unit.\n    n_samples : int\n        Number of softmax units with shared weights\n        (= number of samples from one softmax unit).\n\n    References\n    ----------\n    [1] R. Salakhutdinov, A. Mnih, and G. Hinton. Restricted boltzmann\n        machines for collaborative filtering, 2007.\n    """"""\n    def __init__(self, n_samples=100,\n                 model_path=\'m_rbm_model/\', *args, **kwargs):\n        self.n_samples = n_samples\n        super(MultinomialRBM, self).__init__(v_layer_cls=BernoulliLayer,\n                                             h_layer_cls=MultinomialLayer,\n                                             h_layer_params=dict(n_samples=self.n_samples),\n                                             model_path=model_path, *args, **kwargs)\n\n    def _free_energy(self, v):\n        K = float(self.n_hidden)\n        M = float(self.n_samples)\n        with tf.name_scope(\'free_energy\'):\n            T1 = -tf.einsum(\'ij,j->i\', v, self._vb)\n            T2 = -tf.matmul(v, self._W)\n            h_hat = Multinomial(total_count=M, logits=tf.ones([K])).sample()\n            T3 = tf.einsum(\'ij,j->i\', T2, h_hat)\n            fe = tf.reduce_mean(T1 + T3, axis=0)\n            fe += -tf.lgamma(M + K) + tf.lgamma(M + 1) + tf.lgamma(K)\n        return fe\n\n    def transform(self, *args, **kwargs):\n        H = super(MultinomialRBM, self).transform(*args, **kwargs)\n        H /= float(self.n_samples)\n        return H\n\n\nclass GaussianRBM(BaseRBM):\n    """"""RBM with Gaussian visible and Bernoulli hidden units.\n\n    This implementation does not learn variances, but instead uses\n    fixed, predetermined values. Input data should be pre-processed\n    to have zero mean (or, equivalently, initialize visible biases\n    to the negative mean of data). It can also be normalized to have\n    unit variance. In the latter case use `sigma` equal to 1., as\n    suggested in [1].\n\n    Parameters\n    ----------\n    sigma : float, or iterable of such\n        Standard deviations of visible units.\n\n    References\n    ----------\n    [1] Hinton, G. ""A Practical Guide to Training Restricted Boltzmann\n        Machines"" UTML TR 2010-003\n    """"""\n    def __init__(self, learning_rate=1e-3, sigma=1.,\n                 model_path=\'g_rbm_model/\', *args, **kwargs):\n        self.sigma = sigma\n        super(GaussianRBM, self).__init__(v_layer_cls=GaussianLayer,\n                                          v_layer_params=dict(sigma=self.sigma),\n                                          h_layer_cls=BernoulliLayer,\n                                          learning_rate=learning_rate,\n                                          model_path=model_path, *args, **kwargs)\n        if hasattr(self.sigma, \'__iter__\'):\n            self._sigma_tmp = self.sigma = np.asarray(self.sigma)\n        else:\n            self._sigma_tmp = np.repeat(self.sigma, self.n_visible)\n\n    def _make_placeholders(self):\n        super(GaussianRBM, self)._make_placeholders()\n        with tf.name_scope(\'input_data\'):\n            # divide by resp. sigmas before any operation\n            self._sigma = tf.Variable(self._sigma_tmp, dtype=self._tf_dtype, name=\'sigma\')\n            self._sigma = tf.reshape(self._sigma, [1, self.n_visible])\n            self._X_batch = tf.divide(self._X_batch, self._sigma)\n\n    def _free_energy(self, v):\n        with tf.name_scope(\'free_energy\'):\n            T1 = tf.divide(tf.reshape(self._vb, [1, self.n_visible]), self._sigma)\n            T2 = tf.square(tf.subtract(v, T1))\n            T3 = 0.5 * tf.reduce_sum(T2, axis=1)\n            T4 = -tf.reduce_sum(tf.nn.softplus(self._propup(v) + self._hb), axis=1)\n            fe = tf.reduce_mean(T3 + T4, axis=0)\n        return fe\n\n\ndef logit_mean(X):\n    p = np.mean(X, axis=0)\n    p = np.clip(p, 1e-7, 1. - 1e-7)\n    q = np.log(p / (1. - p))\n    return q\n\n\nif __name__ == \'__main__\':\n    # run corresponding tests\n    from utils.testing import run_tests\n    from tests import test_rbm as t\n    run_tests(__file__, t)\n'"
boltzmann_machines/utils/__init__.py,0,b'from .rng import *\nfrom .utils import *\nfrom .plot_utils import *\nfrom .stopwatch import *\n'
boltzmann_machines/utils/augmentation.py,0,"b""import numpy as np\nimport scipy.ndimage as nd\n\n\ndef shift(x, offset=(0, 0)):\n    if len(x.shape) == 3:\n        y = np.zeros_like(x)\n        for c in range(x.shape[2]):\n            y[:, :, c] = shift(x[:, :, c], offset=offset)\n        return y\n    y = nd.interpolation.shift(x, shift=offset, mode='nearest')\n    return y\n\ndef horizontal_mirror(x):\n    y = np.fliplr(x[:,:,...])\n    return y\n"""
boltzmann_machines/utils/dataset.py,0,"b'import struct\nimport pickle\nimport os.path\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom .rng import RNG\n\n\ndef load_mnist(mode=\'train\', path=\'.\'):\n    """"""\n    Load and return MNIST dataset.\n\n    Returns\n    -------\n    data : (n_samples, 784) np.ndarray\n        Data representing raw pixel intensities (in [0., 255.] range).\n    target : (n_samples,) np.ndarray\n        Labels vector (zero-based integers).\n    """"""\n    dirpath = os.path.join(path, \'mnist/\')\n    if mode == \'train\':\n        fname_data = os.path.join(dirpath, \'train-images-idx3-ubyte\')\n        fname_target = os.path.join(dirpath, \'train-labels-idx1-ubyte\')\n    elif mode == \'test\':\n        fname_data = os.path.join(dirpath, \'t10k-images-idx3-ubyte\')\n        fname_target = os.path.join(dirpath, \'t10k-labels-idx1-ubyte\')\n    else:\n        raise ValueError(""`mode` must be \'train\' or \'test\'"")\n\n    with open(fname_data, \'rb\') as fdata:\n        magic, n_samples, n_rows, n_cols = struct.unpack("">IIII"", fdata.read(16))\n        data = np.fromfile(fdata, dtype=np.uint8)\n        data = data.reshape(n_samples, n_rows * n_cols)\n\n    with open(fname_target, \'rb\') as ftarget:\n        magic, n_samples = struct.unpack("">II"", ftarget.read(8))\n        target = np.fromfile(ftarget, dtype=np.int8)\n\n    return data.astype(float), target\n\ndef load_cifar10(mode=\'train\', path=\'.\'):\n    """"""\n    Load and return CIFAR-10 dataset.\n\n    Returns\n    -------\n    data : (n_samples, 3 * 32 * 32) np.ndarray\n        Data representing raw pixel intensities (in [0., 255.] range).\n    target : (n_samples,) np.ndarray\n        Labels vector (zero-based integers).\n    """"""\n    dirpath = os.path.join(path, \'cifar-10-batches-py/\')\n    batch_size = 10000\n    if mode == \'train\':\n        fnames = [\'data_batch_{0}\'.format(i) for i in range(1, 5 + 1)]\n    elif mode == \'test\':\n        fnames = [\'test_batch\']\n    else:\n        raise ValueError(""`mode` must be \'train\' or \'test\'"")\n    n_samples = batch_size * len(fnames)\n    data = np.zeros(shape=(n_samples, 3 * 32 * 32), dtype=float)\n    target = np.zeros(shape=(n_samples,), dtype=int)\n    start = 0\n    for fname in fnames:\n        fname = os.path.join(dirpath, fname)\n        with open(fname, \'rb\') as fdata:\n            _data = pickle.load(fdata)\n            data[start:(start + batch_size)] = np.asarray(_data[\'data\'])\n            target[start:(start + batch_size)] = np.asarray(_data[\'labels\'])\n        start += 10000\n    return data, target\n\ndef im_flatten(X):\n    """"""Flatten batch of 3-channel images `X`\n    for learning.\n\n    Parameters\n    ----------\n    X : (n_samples, H, W, 3) np.ndarray\n\n    Returns\n    -------\n    X : (n_samples, H * W * 3) np.ndarray\n    """"""\n    X = np.asarray(X)\n    if len(X.shape) == 3:\n        X = np.expand_dims(X, 0)\n    n_samples = X.shape[0]\n    X = X.transpose(0, 3, 1, 2).reshape((n_samples, -1))\n    if X.shape[0] == 1:\n        X = X[0, ...]\n    return X\n\ndef im_unflatten(X):\n    """"""Convert batch of 3-channel images `X`\n    for visualization.\n\n    Parameters\n    ----------\n    X : (n_samples, D * D * 3) np.ndarray\n\n    Returns\n    -------\n    X : (n_samples, D, D, 3) np.ndarray\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.random.rand(10, 3072)\n    >>> Y = X.copy()\n    >>> np.testing.assert_allclose(X, im_flatten(im_unflatten(Y)))\n    >>> X = np.random.rand(3072)\n    >>> Y = X.copy()\n    >>> np.testing.assert_allclose(X, im_flatten(im_unflatten(Y)))\n    >>> X = np.random.rand(9, 8 * 8 * 3)\n    >>> Y = X.copy()\n    >>> np.testing.assert_allclose(X, im_flatten(im_unflatten(Y)))\n    >>> X = np.random.rand(7, 32, 32, 3)\n    >>> Y = X.copy()\n    >>> np.testing.assert_allclose(X, im_unflatten(im_flatten(Y)))\n    >>> X = np.random.rand(32, 32, 3)\n    >>> Y = X.copy()\n    >>> np.testing.assert_allclose(X, im_unflatten(im_flatten(Y)))\n    >>> X = np.random.rand(8, 8, 3)\n    >>> Y = X.copy()\n    >>> np.testing.assert_allclose(X, im_unflatten(im_flatten(Y)))\n    """"""\n    X = np.asarray(X)\n    if len(X.shape) == 1:\n        X = np.expand_dims(X, 0)\n    D = int(np.sqrt(X.shape[1]/3))\n    X = X.reshape((-1, 3, D, D)).transpose(0, 2, 3, 1)\n    if X.shape[0] == 1:\n        X = X[0, ...]\n    return X\n\ndef im_rescale(X, mean=0., std=1.):\n    """"""Same as `im_unflatten` but also scale range\n    of images for better visual perception.\n\n    Parameters\n    ----------\n    X : (n_samples, D * D * 3) np.ndarray\n\n    Returns\n    -------\n    X : (n_samples, D, D, 3) np.ndarray\n    """"""\n    X *= std\n    X += mean\n    X -= X.min(axis=1)[:, np.newaxis]\n    X /= X.ptp(axis=1)[:, np.newaxis]  # [0; 1] range for all images\n    X = im_unflatten(X)  # (n_samples, D, D, 3)\n    X *= 255.\n    X = X.astype(\'uint8\')\n    return X\n\ndef get_cifar10_labels():\n    return [\'airplane\', \'auto\', \'bird\', \'cat\', \'deer\',\n            \'dog\', \'frog\', \'horse\', \'ship\', \'truck\']\n\ndef get_cifar10_label(index):\n    return get_cifar10_labels()[index]\n\ndef plot_cifar10(X, y, samples_per_class=7,\n                 title=\'CIFAR-10 dataset\', title_params=None, imshow_params=None):\n    # check params\n    title_params = title_params or {}\n    title_params.setdefault(\'fontsize\', 20)\n    title_params.setdefault(\'y\', 0.95)\n\n    imshow_params = imshow_params or {}\n    imshow_params.setdefault(\'interpolation\', \'none\')\n\n    num_classes = 10\n    classes = range(num_classes)\n    for c in classes:\n        idxs = np.flatnonzero(y == c)\n        idxs = RNG(seed=1337).choice(idxs, samples_per_class, replace=False)\n        for i, idx in enumerate(idxs):\n            plt_idx = i * num_classes + c + 1\n            ax = plt.subplot(samples_per_class, num_classes, plt_idx)\n            ax.spines[\'bottom\'].set_linewidth(2.)\n            ax.spines[\'top\'].set_linewidth(2.)\n            ax.spines[\'left\'].set_linewidth(2.)\n            ax.spines[\'right\'].set_linewidth(2.)\n            plt.tick_params(axis=\'both\', which=\'both\',\n                            bottom=\'off\', top=\'off\', left=\'off\', right=\'off\',\n                            labelbottom=\'off\', labelleft=\'off\', labelright=\'off\')\n            plt.imshow(X[idx].astype(\'uint8\'), **imshow_params)\n            if i == 0:\n                plt.title(get_cifar10_label(c))\n    plt.suptitle(title, **title_params)\n    plt.subplots_adjust(wspace=0, hspace=0)\n\n\nif __name__ == \'__main__\':\n    # run corresponding tests\n    from .testing import run_tests\n    run_tests(__file__)\n'"
boltzmann_machines/utils/optimizers.py,0,"b'from keras import backend as K\nfrom keras.legacy import interfaces\nfrom keras.optimizers import Optimizer\n\n\nclass MultiAdam(Optimizer):\n    """"""Custom Adam optimizer that supports per-layer learning rates.""""""\n    def __init__(self, lr=0.001, lr_multipliers=None,\n                 beta_1=0.9, beta_2=0.999,\n                 epsilon=1e-8, decay=0., **kwargs):\n        super(MultiAdam, self).__init__(**kwargs)\n        self.lr_multipliers = lr_multipliers or {}\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype=\'int64\', name=\'iterations\')\n            self.lr = K.variable(lr, name=\'lr\')\n            self.beta_1 = K.variable(beta_1, name=\'beta_1\')\n            self.beta_2 = K.variable(beta_2, name=\'beta_2\')\n            self.decay = K.variable(decay, name=\'decay\')\n        self.epsilon = epsilon\n        self.initial_decay = decay\n\n    @interfaces.legacy_get_updates_support\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n        if self.initial_decay > 0:\n            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n                                                  K.dtype(self.decay))))\n\n        t = K.cast(self.iterations, K.floatx()) + 1\n        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n                     (1. - K.pow(self.beta_1, t)))\n\n        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        self.weights = [self.iterations] + ms + vs\n\n        for p, g, m, v in zip(params, grads, ms, vs):\n            lr_m = 1.\n            for key, value in self.lr_multipliers.iteritems():\n                if p.name.startswith(key):\n                    lr_m = value\n                    break\n            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n            p_t = p - lr_t * lr_m * m_t / (K.sqrt(v_t) + self.epsilon)\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(v, v_t))\n            new_p = p_t\n\n            # Apply constraints.\n            if getattr(p, \'constraint\', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {\'lr\': float(K.get_value(self.lr)),\n                  \'beta_1\': float(K.get_value(self.beta_1)),\n                  \'beta_2\': float(K.get_value(self.beta_2)),\n                  \'decay\': float(K.get_value(self.decay)),\n                  \'epsilon\': self.epsilon}\n        base_config = super(MultiAdam, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n'"
boltzmann_machines/utils/plot_utils.py,0,"b'import numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom matplotlib.animation import FuncAnimation\n\n\ndef tick_params():\n    """"""Tick params used in `plt.tick_params` or `im.axes.tick_params` to\n    plot images without labels, borders etc..\n    """"""\n    return dict(axis=\'both\', which=\'both\',\n                bottom=\'off\', top=\'off\', left=\'off\', right=\'off\',\n                labelbottom=\'off\', labelleft=\'off\', labelright=\'off\')\n\ndef im_plot(X, n_width=10, n_height=10, shape=None, title=None,\n            title_params=None, imshow_params=None):\n    """"""Plot batch of images `X` on a single graph.""""""\n    # check params\n    X = np.asarray(X)\n    if shape is None:\n        shape = X.shape[1:]\n\n    title_params = title_params or {}\n    title_params.setdefault(\'fontsize\', 22)\n    title_params.setdefault(\'y\', 0.95)\n\n    imshow_params = imshow_params or {}\n    imshow_params.setdefault(\'interpolation\', \'nearest\')\n\n    # plot\n    for i in range(n_height * n_width):\n        if i < len(X):\n            img = X[i]\n            if shape is not None:\n                img = img.reshape(shape)\n            ax = plt.subplot(n_height, n_width, i + 1)\n            for d in (\'bottom\', \'top\', \'left\', \'right\'):\n                ax.spines[d].set_linewidth(2.)\n            plt.tick_params(**tick_params())\n            plt.imshow(img, **imshow_params)\n    if title:\n        plt.suptitle(title, **title_params)\n    plt.subplots_adjust(wspace=0, hspace=0)\n\ndef im_reshape(X, n_width=10, n_height=10, shape=None, normalize=False):\n    """"""Reshape batch of images `X` to a single grid-image\n\n    Returns\n    -------\n    X_reshaped : (H, W, C) or (H, W) np.ndarray\n        Where H = `n_height` * `shape`[0],\n              W = `n_width` * `shape`[1],\n              C = `shape`[2] if `shape`[2] > 1 (3 or 4)\n    """"""\n    # check params\n    X = np.asarray(X)\n    if shape is None:\n        shape = X.shape[1:]\n\n    # reshape `X`\n    Y = X[:(n_width * n_height), ...].copy()\n    if len(shape) == 2:\n        shape = (shape[0], shape[1], 1)\n    Y = Y.reshape(-1, *shape)\n    Z = np.zeros((n_height * shape[0], n_width * shape[1], shape[2]), dtype=Y.dtype)\n\n    for i in range(n_height):\n        for j in range(n_width):\n            ind_Y = n_height * i + j\n            if ind_Y < len(Y):\n                Y_i = Y[ind_Y, ...]\n                if normalize:\n                    Y_i -= Y_i.min()\n                    Y_i /= max(Y_i.ptp(), 1e-5)\n                    Y_i /= Y_i.max()\n                Z[i * shape[0]:(i + 1) * shape[0],\n                  j * shape[1]:(j + 1) * shape[1], ...] = Y_i\n    if Z.shape[2] == 1:\n        Z = Z[:, :, 0]\n\n    return Z\n\ndef im_gif(matrices, im, fig, fname=None, title_func=None,\n           title_params=None, anim_params=None, save_params=None):\n    """"""Animate `matrices`.\n\n    Parameters\n    ----------\n    matrices : [np.ndarray]\n        list of matrices to animate\n\n    Returns\n    -------\n    anim : matplotlib.animation.FuncAnimation\n    """"""\n    if title_func is None:\n        title_func = lambda i: str(i)\n\n    title_params = title_params or {}\n    title_params.setdefault(\'fontsize\', 18)\n\n    anim_params = anim_params or {}\n    anim_params.setdefault(\'interval\', 250)\n    anim_params.setdefault(\'blit\', True)\n\n    save_params = save_params or {}\n    save_params.setdefault(\'dpi\', 80)\n    save_params.setdefault(\'writer\', \'imagemagick\')\n\n    def init():\n        im.set_array([[]])\n        return im,\n\n    def animate(i):\n        im.set_array(matrices[i])\n        title = title_func(i)\n        im.axes.set_title(title, **title_params)\n        return im,\n\n    anim = FuncAnimation(fig, animate, init_func=init, frames=len(matrices), **anim_params)\n    if fname:\n        anim.save(fname, **save_params)\n    return anim\n\ndef plot_confusion_matrix(C, labels=None, labels_fontsize=13, **heatmap_params):\n    # default params\n    labels = labels or range(C.shape[0])\n    annot_fontsize = 14\n    xy_label_fontsize = 21\n\n    # set default params where possible\n    if not \'annot\' in heatmap_params:\n        heatmap_params[\'annot\'] = True\n    if not \'fmt\' in heatmap_params:\n        heatmap_params[\'fmt\'] = \'d\' if C.dtype is np.dtype(\'int\') else \'.3f\'\n    if not \'annot_kws\' in heatmap_params:\n        heatmap_params[\'annot_kws\'] = {\'size\': annot_fontsize}\n    elif not \'size\' in heatmap_params[\'annot_kws\']:\n        heatmap_params[\'annot_kws\'][\'size\'] = annot_fontsize\n    if not \'xticklabels\' in heatmap_params:\n        heatmap_params[\'xticklabels\'] = labels\n    if not \'yticklabels\' in heatmap_params:\n        heatmap_params[\'yticklabels\'] = labels\n\n    # plot the stuff\n    with plt.rc_context(rc={\'xtick.labelsize\': labels_fontsize,\n                            \'ytick.labelsize\': labels_fontsize}):\n        ax = sns.heatmap(C, **heatmap_params)\n        plt.xlabel(\'predicted\', fontsize=xy_label_fontsize)\n        plt.ylabel(\'actual\', fontsize=xy_label_fontsize)\n        return ax\n'"
boltzmann_machines/utils/rng.py,0,"b'import numpy as np\n\n\nclass RNG(np.random.RandomState):\n    """"""Class encapsulating random number generator.\n\n    Creates RNG from `seed`:\n    If `seed` is None, return default RNG.\n    If `seed` is int or [int], return new RNG instance seeded with it.\n\n    Raises\n    ------\n    TypeError\n        If `seed` is none from the above.\n\n    Examples\n    --------\n    >>> rng = RNG(1337)\n    >>> state = rng.get_state()\n    >>> state1 = rng.get_state()\n    >>> rng.rand()\n    0.2620246750155817\n    >>> rng.rand()\n    0.1586839721544656\n    >>> _ = rng.reseed()\n    >>> rng.rand()\n    0.2620246750155817\n    >>> rng.rand()\n    0.1586839721544656\n    >>> _ = rng.set_state(state)\n    >>> rng.rand()\n    0.2620246750155817\n    >>> import json\n    >>> with open(\'random_state.json\', \'w\') as f:\n    ...     json.dump(state1, f)\n    >>> with open(\'random_state.json\', \'r\') as f:\n    ...     loaded_state = json.load(f)\n    >>> rng.set_state(loaded_state).rand()\n    0.2620246750155817\n    """"""\n    def __init__(self, seed=None):\n        self._seed = seed\n        super(RNG, self).__init__(self._seed)\n\n    def reseed(self):\n        if self._seed is not None:\n            self.seed(self._seed)\n        return self\n\n    def get_state(self):\n        """"""Get JSON-serializable inner state.""""""\n        state = super(RNG, self).get_state()\n        state = list(state)\n        state[1] = state[1].tolist()\n        return state\n\n    def set_state(self, state):\n        """"""Complementary method to `get_state`.""""""\n        state[1] = np.asarray(state[1], dtype=np.uint32)\n        state = tuple(state)\n        super(RNG, self).set_state(state)\n        return self\n\n\nif __name__ == \'__main__\':\n    # run corresponding tests\n    from .testing import run_tests\n    run_tests(__file__)\n'"
boltzmann_machines/utils/stopwatch.py,0,"b'import sys\nimport time\n\n\nclass Stopwatch(object):\n    """"""\n    A simple cross-platform\n    context-manager stopwatch.\n\n    Examples\n    --------\n    >>> import time\n    >>> with Stopwatch(verbose=True) as s:\n    ...     time.sleep(0.1) # doctest: +ELLIPSIS\n    Elapsed time: 0.10... sec\n    >>> with Stopwatch(verbose=False) as s:\n    ...     time.sleep(0.1)\n    >>> import math\n    >>> math.fabs(s.elapsed() - 0.1) < 0.05\n    True\n    """"""\n    def __init__(self, verbose=False):\n        self.verbose = verbose\n        if sys.platform == \'win32\':\n            # on Windows, the best timer is time.clock()\n            self._timer_func = time.clock\n        else:\n            # on most other platforms, the best timer is time.time()\n            self._timer_func = time.time\n        self.reset()\n\n    def __enter__(self, verbose=False):\n        return self.start()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.stop()\n        return self.elapsed()\n\n    def start(self):\n        if not self._is_running:\n            self._start = self._timer_func()\n            self._is_running = True\n        return self\n\n    def stop(self):\n        if self._is_running:\n            self._total += (self._timer_func() - self._start)\n            self._is_running = False\n        return self\n\n    def elapsed(self):\n        if self._is_running:\n            now = self._timer_func()\n            self._total += (now - self._start)\n            self._start = now\n        if self.verbose:\n            print(""Elapsed time: {0:.3f} sec"".format(self._total))\n        return self._total\n\n    def reset(self):\n        self._start = 0.\n        self._total = 0.\n        self._is_running = False\n        return self\n'"
boltzmann_machines/utils/testing.py,0,"b'import nose\n\n\n@nose.tools.nottest\ndef run_tests(script_path, test_module=None):\n    """"""\n    Run tests which are contained in `test_module` for script\n    whose location is specified in `script_path` (typically, is\n    called as __file__).\n    """"""\n    params = [\'\', script_path]\n    if test_module:\n        params.append(test_module.__file__)\n    params.append(\'--with-doctest\')\n    nose.run(argv=params)\n\ndef assert_shape(obj, name, desired_shape):\n    actual_shape = getattr(obj, name).shape\n    if actual_shape != desired_shape:\n        raise ValueError(\'`{0}` has invalid shape {1} != {2}\'.\\\n                         format(name, actual_shape, desired_shape))\n\ndef assert_len(obj, name, desired_len):\n    actual_len = len(getattr(obj, name))\n    if actual_len != desired_len:\n        raise ValueError(\'`{0}` has invalid len {1} != {2}\'.\\\n                         format(name, actual_len, desired_len))\n'"
boltzmann_machines/utils/utils.py,0,"b'import numpy as np\n\nfrom tqdm import tqdm, tqdm_notebook\ndef _is_in_ipython():\n    try: __IPYTHON__; return True\n    except NameError: return False\nprogress_bar = tqdm_notebook if _is_in_ipython() else tqdm\n\n\ndef write_during_training(s):\n    tqdm.write(s)\n\ndef batch_iter(X, batch_size=10, verbose=False, desc=\'epoch\'):\n    """"""Divide input data into batches, with optional\n    progress bar.\n\n    Examples\n    --------\n    >>> X = np.arange(36).reshape((12, 3))\n    >>> for X_b in batch_iter(X, batch_size=5):\n    ...     print(X_b)\n    [[ 0  1  2]\n     [ 3  4  5]\n     [ 6  7  8]\n     [ 9 10 11]\n     [12 13 14]]\n    [[15 16 17]\n     [18 19 20]\n     [21 22 23]\n     [24 25 26]\n     [27 28 29]]\n    [[30 31 32]\n     [33 34 35]]\n    """"""\n    X = np.asarray(X)\n    N = len(X)\n    n_batches = N // batch_size + (N % batch_size > 0)\n    gen = range(n_batches)\n    if verbose:\n        gen = progress_bar(gen, leave=False, ncols=64, desc=desc)\n    for i in gen:\n        yield X[i*batch_size:(i + 1)*batch_size]\n\ndef epoch_iter(start_epoch, max_epoch, verbose=False):\n    gen = range(start_epoch + 1, max_epoch + 1)\n    if verbose:\n        gen = progress_bar(gen, leave=True, ncols=84, desc=\'training\')\n    for epoch in gen:\n        yield epoch\n\ndef make_list_from(x):\n    return list(x) if hasattr(x, \'__iter__\') else [x]\n\ndef one_hot(y, n_classes=None):\n    """"""Convert `y` to one-hot encoding.\n\n    Examples\n    --------\n    >>> y = [2, 1, 0, 2, 0]\n    >>> one_hot(y)\n    array([[ 0.,  0.,  1.],\n           [ 0.,  1.,  0.],\n           [ 1.,  0.,  0.],\n           [ 0.,  0.,  1.],\n           [ 1.,  0.,  0.]])\n    """"""\n    n_classes = n_classes or np.max(y) + 1\n    return np.eye(n_classes)[y]\n\ndef one_hot_decision_function(y):\n    """"""\n    Examples\n    --------\n    >>> y = [[0.1, 0.4, 0.5],\n    ...      [0.8, 0.1, 0.1],\n    ...      [0.2, 0.2, 0.6],\n    ...      [0.3, 0.4, 0.3]]\n    >>> one_hot_decision_function(y)\n    array([[ 0.,  0.,  1.],\n           [ 1.,  0.,  0.],\n           [ 0.,  0.,  1.],\n           [ 0.,  1.,  0.]])\n    """"""\n    z = np.zeros_like(y)\n    z[np.arange(len(z)), np.argmax(y, axis=1)] = 1\n    return z\n\ndef unhot(y, n_classes=None):\n    """"""\n    Map `y` from one-hot encoding to {0, ..., `n_classes` - 1}.\n\n    Examples\n    --------\n    >>> y = [[0, 0, 1],\n    ...      [0, 1, 0],\n    ...      [1, 0, 0],\n    ...      [0, 0, 1],\n    ...      [1, 0, 0]]\n    >>> unhot(y)\n    array([2, 1, 0, 2, 0])\n    """"""\n    if not isinstance(y, np.ndarray):\n        y = np.asarray(y)\n    if not n_classes:\n        _, n_classes = y.shape\n    return y.dot(np.arange(n_classes))\n\ndef log_sum_exp(x):\n    """"""Compute log(sum(exp(x))) in a numerically stable way.\n\n    Examples\n    --------\n    >>> x = [0, 1, 0]\n    >>> log_sum_exp(x) #doctest: +ELLIPSIS\n    1.551...\n    >>> x = [1000, 1001, 1000]\n    >>> log_sum_exp(x) #doctest: +ELLIPSIS\n    1001.551...\n    >>> x = [-1000, -999, -1000]\n    >>> log_sum_exp(x) #doctest: +ELLIPSIS\n    -998.448...\n    """"""\n    x = np.asarray(x)\n    a = max(x)\n    return a + np.log(sum(np.exp(x - a)))\n\ndef log_mean_exp(x):\n    """"""Compute log(mean(exp(x))) in a numerically stable way.\n\n    Examples\n    --------\n    >>> x = [1, 2, 3]\n    >>> log_mean_exp(x) #doctest: +ELLIPSIS\n    2.308...\n    """"""\n    return log_sum_exp(x) - np.log(len(x))\n\ndef log_diff_exp(x):\n    """"""Compute log(diff(exp(x))) in a numerically stable way.\n\n    Examples\n    --------\n    >>> log_diff_exp([1, 2, 3]) #doctest: +ELLIPSIS\n    array([ 1.5413...,  2.5413...])\n    >>> [np.log(np.exp(2)-np.exp(1)), np.log(np.exp(3)-np.exp(2))] #doctest: +ELLIPSIS\n    [1.5413..., 2.5413...]\n    """"""\n    x = np.asarray(x)\n    a = max(x)\n    return a + np.log(np.diff(np.exp(x - a)))\n\ndef log_std_exp(x, log_mean_exp_x=None):\n    """"""Compute log(std(exp(x))) in a numerically stable way.\n\n    Examples\n    --------\n    >>> x = np.arange(8.)\n    >>> print x\n    [ 0.  1.  2.  3.  4.  5.  6.  7.]\n    >>> log_std_exp(x) #doctest: +ELLIPSIS\n    5.875416...\n    >>> np.log(np.std(np.exp(x))) #doctest: +ELLIPSIS\n    5.875416...\n    """"""\n    x = np.asarray(x)\n    m = log_mean_exp_x\n    if m is None:\n        m = log_mean_exp(x)\n    M = log_mean_exp(2. * x)\n    return 0.5 * log_diff_exp([2. * m, M])[0]\n\n\nif __name__ == \'__main__\':\n    # run corresponding tests\n    from .testing import run_tests\n    run_tests(__file__)\n'"
boltzmann_machines/base/tests/test_tf_model.py,0,"b""from nose.tools import eq_\n\nfrom boltzmann_machines.base import TensorFlowModel as TFM\n\n\nclass TestWorkingPaths(object):\n    def __init__(self):\n        pass\n\n    def test_filename_only(self):\n        tf_model = TFM(model_path='model')\n        eq_(tf_model._model_dirpath, './')\n        eq_(tf_model._model_filepath, './model')\n        eq_(tf_model._params_filepath, './params.json')\n        eq_(tf_model._random_state_filepath, './random_state.json')\n        eq_(tf_model._train_summary_dirpath, './logs/train')\n        eq_(tf_model._val_summary_dirpath, './logs/val')\n        eq_(tf_model._tf_meta_graph_filepath, './model.meta')\n\n        tf_model = TFM(model_path='model-1')\n        eq_(tf_model._model_dirpath, './')\n        eq_(tf_model._model_filepath, './model-1')\n        eq_(tf_model._params_filepath, './params.json')\n        eq_(tf_model._random_state_filepath, './random_state.json')\n        eq_(tf_model._train_summary_dirpath, './logs/train')\n        eq_(tf_model._val_summary_dirpath, './logs/val')\n        eq_(tf_model._tf_meta_graph_filepath, './model-1.meta')\n\n    def test_dirname_only(self):\n        tf_model = TFM(model_path='a/')\n        eq_(tf_model._model_dirpath, 'a/')\n        eq_(tf_model._model_filepath, 'a/model')\n        eq_(tf_model._params_filepath, 'a/params.json')\n        eq_(tf_model._random_state_filepath, 'a/random_state.json')\n        eq_(tf_model._train_summary_dirpath, 'a/logs/train')\n        eq_(tf_model._val_summary_dirpath, 'a/logs/val')\n        eq_(tf_model._tf_meta_graph_filepath, 'a/model.meta')\n\n        tf_model = TFM(model_path='./')\n        eq_(tf_model._model_dirpath, './')\n        eq_(tf_model._model_filepath, './model')\n        eq_(tf_model._params_filepath, './params.json')\n        eq_(tf_model._random_state_filepath, './random_state.json')\n        eq_(tf_model._train_summary_dirpath, './logs/train')\n        eq_(tf_model._val_summary_dirpath, './logs/val')\n        eq_(tf_model._tf_meta_graph_filepath, './model.meta')\n\n        tf_model = TFM(model_path='b/a/')\n        eq_(tf_model._model_dirpath, 'b/a/')\n        eq_(tf_model._model_filepath, 'b/a/model')\n        eq_(tf_model._params_filepath, 'b/a/params.json')\n        eq_(tf_model._random_state_filepath, 'b/a/random_state.json')\n        eq_(tf_model._train_summary_dirpath, 'b/a/logs/train')\n        eq_(tf_model._val_summary_dirpath, 'b/a/logs/val')\n        eq_(tf_model._tf_meta_graph_filepath, 'b/a/model.meta')\n\n    def test_nothing(self):\n        tf_model = TFM(model_path='')\n        eq_(tf_model._model_dirpath, './')\n        eq_(tf_model._model_filepath, './model')\n        eq_(tf_model._params_filepath, './params.json')\n        eq_(tf_model._random_state_filepath, './random_state.json')\n        eq_(tf_model._train_summary_dirpath, './logs/train')\n        eq_(tf_model._val_summary_dirpath, './logs/val')\n        eq_(tf_model._tf_meta_graph_filepath, './model.meta')\n\n    def test_all(self):\n        tf_model = TFM(model_path='a/b')\n        eq_(tf_model._model_dirpath, 'a/')\n        eq_(tf_model._model_filepath, 'a/b')\n        eq_(tf_model._params_filepath, 'a/params.json')\n        eq_(tf_model._random_state_filepath, 'a/random_state.json')\n        eq_(tf_model._train_summary_dirpath, 'a/logs/train')\n        eq_(tf_model._val_summary_dirpath, 'a/logs/val')\n        eq_(tf_model._tf_meta_graph_filepath, 'a/b.meta')\n\n        tf_model = TFM(model_path='./b')\n        eq_(tf_model._model_dirpath, './')\n        eq_(tf_model._model_filepath, './b')\n        eq_(tf_model._params_filepath, './params.json')\n        eq_(tf_model._random_state_filepath, './random_state.json')\n        eq_(tf_model._train_summary_dirpath, './logs/train')\n        eq_(tf_model._val_summary_dirpath, './logs/val')\n        eq_(tf_model._tf_meta_graph_filepath, './b.meta')\n\n        tf_model = TFM(model_path='a/b/c')\n        eq_(tf_model._model_dirpath, 'a/b/')\n        eq_(tf_model._model_filepath, 'a/b/c')\n        eq_(tf_model._params_filepath, 'a/b/params.json')\n        eq_(tf_model._random_state_filepath, 'a/b/random_state.json')\n        eq_(tf_model._train_summary_dirpath, 'a/b/logs/train')\n        eq_(tf_model._val_summary_dirpath, 'a/b/logs/val')\n        eq_(tf_model._tf_meta_graph_filepath, 'a/b/c.meta')\n"""
boltzmann_machines/rbm/tests/test_rbm.py,0,"b""import os\nimport numpy as np\nfrom shutil import rmtree\nfrom numpy.testing import (assert_allclose,\n                           assert_almost_equal,\n                           assert_raises)\n\nfrom rbm import BernoulliRBM, MultinomialRBM, GaussianRBM\nfrom utils import RNG\n\n\nclass TestRBM(object):\n    def __init__(self):\n        self.n_visible = 12\n        self.n_hidden = 8\n        self.X = RNG(seed=1337).rand(16, self.n_visible)\n        self.X_val = RNG(seed=42).rand(8, self.n_visible)\n        self.rbm_config = dict(n_visible=self.n_visible, n_hidden=self.n_hidden,\n                               sample_v_states=True, sample_h_states=True,\n                               dropout=0.9,\n                               verbose=False, display_filters=False,\n                               random_seed=1337)\n\n    def cleanup(self):\n        for d in ('test_rbm_1/', 'test_rbm_2/'):\n            if os.path.exists(d):\n                rmtree(d)\n\n    def test_W_init(self):\n        for C in (BernoulliRBM, MultinomialRBM, GaussianRBM):\n            assert_raises(ValueError, lambda: C(n_visible=4, n_hidden=3, W_init=np.zeros((4, 2))))\n            assert_raises(ValueError, lambda: C(n_visible=4, n_hidden=3, W_init=np.zeros((3, 3))))\n            assert_raises(ValueError, lambda: C(n_visible=4, n_hidden=3, W_init=np.zeros((3, 2))))\n            C(n_visible=4, n_hidden=3, W_init=np.zeros((4, 3)))\n            C(n_visible=3, n_hidden=3, W_init=np.zeros((3, 3)))\n            C(n_visible=1, n_hidden=1, W_init=np.zeros((1, 1)))\n\n    def compare_weights(self, rbm1, rbm2):\n        rbm1_weights = rbm1.get_tf_params(scope='weights')\n        rbm2_weights = rbm2.get_tf_params(scope='weights')\n        assert_allclose(rbm1_weights['W'], rbm2_weights['W'])\n        assert_allclose(rbm1_weights['hb'], rbm2_weights['hb'])\n        assert_allclose(rbm1_weights['vb'], rbm2_weights['vb'])\n\n    def compare_transforms(self, rbm1, rbm2):\n        H1 = rbm1.transform(self.X_val)\n        H2 = rbm2.transform(self.X_val)\n        assert H1.shape == (len(self.X_val), self.n_hidden)\n        assert H1.shape == H2.shape\n        assert_allclose(H1, H2)\n\n    def test_initialization(self):\n        for C, dtype in (\n                (BernoulliRBM, 'float32'),\n                (BernoulliRBM, 'float64'),\n                (MultinomialRBM, 'float32'),\n                (GaussianRBM, 'float32'),\n        ):\n            rbm = C(max_epoch=2,\n                    model_path='test_rbm_1/',\n                    dtype=dtype,\n                    **self.rbm_config)\n            rbm.init()\n            if dtype == 'float32':\n                assert_almost_equal(rbm.get_tf_params(scope='weights')['W'][0][0], -0.0094548017)\n            if dtype == 'float64':\n                assert_almost_equal(rbm.get_tf_params(scope='weights')['W'][0][0], -0.0077341544416)\n\n    def test_consistency(self):\n        for C, dtype in (\n            (BernoulliRBM, 'float32'),\n            (BernoulliRBM, 'float64'),\n            (MultinomialRBM, 'float32'),\n            (GaussianRBM, 'float32'),\n        ):\n            # train 2 RBMs with same params for 2 epochs\n            rbm1 = C(max_epoch=2,\n                     model_path='test_rbm_1/',\n                     dtype=dtype,\n                     **self.rbm_config)\n            rbm2 = C(max_epoch=2,\n                     model_path='test_rbm_2/',\n                     dtype=dtype,\n                     **self.rbm_config)\n\n            rbm1.fit(self.X)\n            rbm2.fit(self.X)\n\n            self.compare_weights(rbm1, rbm2)\n            self.compare_transforms(rbm1, rbm2)\n\n            # train for 1 more epoch\n            rbm1.set_params(max_epoch=rbm1.max_epoch + 1).fit(self.X)\n            rbm2.set_params(max_epoch=rbm2.max_epoch + 1).fit(self.X)\n\n            self.compare_weights(rbm1, rbm2)\n            self.compare_transforms(rbm1, rbm2)\n\n            # load from disk\n            rbm1 = C.load_model('test_rbm_1/')\n            rbm2 = C.load_model('test_rbm_2/')\n\n            self.compare_weights(rbm1, rbm2)\n            self.compare_transforms(rbm1, rbm2)\n\n            # train for 1 more epoch\n            rbm1.set_params(max_epoch=rbm1.max_epoch + 1).fit(self.X)\n            rbm2.set_params(max_epoch=rbm2.max_epoch + 1).fit(self.X)\n\n            self.compare_weights(rbm1, rbm2)\n            self.compare_transforms(rbm1, rbm2)\n\n            # cleanup\n            self.cleanup()\n\n    def test_consistency_val(self):\n        rbm1 = BernoulliRBM(max_epoch=2,\n                            model_path='test_rbm_1/',\n                            **self.rbm_config)\n        rbm2 = BernoulliRBM(max_epoch=2,\n                            model_path='test_rbm_2/',\n                            **self.rbm_config)\n\n        rbm1.fit(self.X, self.X_val)\n        rbm2.fit(self.X, self.X_val)\n\n        self.compare_weights(rbm1, rbm2)\n        self.compare_transforms(rbm1, rbm2)\n\n        # cleanup\n        self.cleanup()\n\n    def tearDown(self):\n        self.cleanup()\n"""
