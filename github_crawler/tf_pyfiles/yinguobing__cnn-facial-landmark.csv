file_path,api_count,code
landmark.py,43,"b'""""""\nConvolutional Neural Network for facial landmarks detection.\n""""""\nimport argparse\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\n\nfrom model import LandmarkModel\n\n# Add arguments parser to accept user specified arguments.\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--train_record\', default=\'train.record\', type=str,\n                    help=\'Training record file\')\nparser.add_argument(\'--val_record\', default=\'validation.record\', type=str,\n                    help=\'validation record file\')\nparser.add_argument(\'--model_dir\', default=\'train\', type=str,\n                    help=\'training model directory\')\nparser.add_argument(\'--export_dir\', default=None, type=str,\n                    help=\'directory to export the saved model\')\nparser.add_argument(\'--train_steps\', default=1000, type=int,\n                    help=\'training steps\')\nparser.add_argument(\'--num_epochs\', default=None, type=int,\n                    help=\'epochs of training dataset\')\nparser.add_argument(\'--batch_size\', default=16, type=int,\n                    help=\'training batch size\')\nparser.add_argument(\'--raw_input\', default=False, type=bool,\n                    help=\'Use raw tensor as model input.\')\n\n\n# CAUTION: The image width, height and channels should be consist with your\n# training data. Here they are set as 128 to be complied with the tutorial.\n# Mismatching of the image size will cause error of mismatching tensor shapes.\nIMG_WIDTH = 128\nIMG_HEIGHT = 128\nIMG_CHANNEL = 3\n\n\ndef cnn_model_fn(features, labels, mode):\n    """"""\n    The model function for the network.\n    """"""\n    # Construct the network.\n    model = LandmarkModel(output_size=68*2)\n    logits = model(features)\n\n    # Make prediction for PREDICATION mode.\n    predictions = logits\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(\n            mode=mode,\n            predictions=predictions,\n            export_outputs={\n                \'predict\': tf.estimator.export.PredictOutput(predictions)\n            })\n\n    # Calculate loss using mean squared error.\n    loss = tf.losses.mean_squared_error(labels=labels, predictions=predictions)\n\n    # Create a tensor logging purposes.\n    tf.identity(loss, name=\'loss\')\n    tf.summary.scalar(\'loss\', loss)\n\n    # Configure the train OP for TRAIN mode.\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n\n        train_op = optimizer.minimize(\n            loss=loss,\n            global_step=tf.train.get_global_step())\n    else:\n        train_op = None\n\n    # Create a metric.\n    rmse_metrics = tf.metrics.root_mean_squared_error(\n        labels=labels,\n        predictions=predictions)\n    metrics = {\'eval_mse\': rmse_metrics}\n\n    # A tensor for metric logging\n    tf.identity(rmse_metrics[1], name=\'root_mean_squared_error\')\n    tf.summary.scalar(\'root_mean_squared_error\', rmse_metrics[1])\n\n    # Generate a summary node for the images\n    tf.summary.image(\'images\', features, max_outputs=6)\n\n    return tf.estimator.EstimatorSpec(\n        mode=mode,\n        predictions=predictions,\n        loss=loss,\n        train_op=train_op,\n        eval_metric_ops=metrics\n    )\n\n\ndef _parse_function(record):\n    """"""\n    Extract data from a `tf.Example` protocol buffer.\n    """"""\n    # Defaults are not specified since both keys are required.\n    keys_to_features = {\n        \'image/filename\': tf.FixedLenFeature([], tf.string),\n        \'image/encoded\': tf.FixedLenFeature([], tf.string),\n        \'label/marks\': tf.FixedLenFeature([136], tf.float32),\n    }\n    parsed_features = tf.parse_single_example(record, keys_to_features)\n\n    # Extract features from single example\n    image_decoded = tf.image.decode_image(parsed_features[\'image/encoded\'])\n    image_reshaped = tf.reshape(\n        image_decoded, [IMG_HEIGHT, IMG_WIDTH, IMG_CHANNEL])\n    points = tf.cast(parsed_features[\'label/marks\'], tf.float32)\n\n    return image_reshaped, points\n\n\ndef input_fn(record_file, batch_size, num_epochs=None, shuffle=True):\n    """"""\n    Input function required for TensorFlow Estimator.\n    """"""\n    dataset = tf.data.TFRecordDataset(record_file)\n\n    # Use `Dataset.map()` to build a pair of a feature dictionary and a label\n    # tensor for each example.\n    dataset = dataset.map(_parse_function)\n    if shuffle is True:\n        dataset = dataset.shuffle(buffer_size=10000)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.repeat(num_epochs)\n\n    # Make dataset iterator.\n    iterator = dataset.make_one_shot_iterator()\n\n    # Return the feature and label.\n    image, label = iterator.get_next()\n    return image, label\n\n\ndef serving_input_receiver_fn():\n    """"""An input function for TensorFlow Serving.""""""\n\n    def _preprocess_image(image_bytes):\n        """"""Preprocess a single raw image.""""""\n        image = tf.image.decode_jpeg(image_bytes, channels=IMG_CHANNEL)\n        image.set_shape((None, None, IMG_CHANNEL))\n        image = tf.image.resize_images(image, [IMG_HEIGHT, IMG_WIDTH],\n                                       method=tf.image.ResizeMethod.BILINEAR,\n                                       align_corners=False)\n        return image\n    image_bytes_list = tf.compat.v1.placeholder(\n        shape=[None], dtype=tf.string,\n        name=\'encoded_image_string_tensor\')\n    image = tf.map_fn(_preprocess_image, image_bytes_list,\n                      dtype=tf.float32, back_prop=False)\n\n    return tf.estimator.export.TensorServingInputReceiver(\n        features=image,\n        receiver_tensors={\'image_bytes\': image_bytes_list})\n\n\ndef tensor_input_receiver_fn():\n    """"""An input function accept raw tensors.""""""\n    def _preprocess_image(image_tensor):\n        """"""Preprocess a single raw image tensor.""""""\n        image = tf.image.resize_images(image_tensor, [IMG_HEIGHT, IMG_WIDTH],\n                                       method=tf.image.ResizeMethod.BILINEAR,\n                                       align_corners=False)\n        return image\n\n    image_tensor = tf.compat.v1.placeholder(\n        shape=[None, None, None, 3], dtype=tf.uint8,\n        name=\'image_tensor\')\n    image = tf.map_fn(_preprocess_image, image_tensor,\n                      dtype=tf.float32, back_prop=False)\n\n    return tf.estimator.export.TensorServingInputReceiver(\n        features=image,\n        receiver_tensors={\'image\': image_tensor})\n\n\ndef main(unused_argv):\n    """"""Train, eval and export the model.""""""\n    # Parse the arguments.\n    args = parser.parse_args(unused_argv[1:])\n\n    # Create the Estimator\n    estimator = tf.estimator.Estimator(\n        model_fn=cnn_model_fn, model_dir=args.model_dir)\n\n    # Train for N steps.\n    tf.logging.info(\'Starting to train.\')\n    estimator.train(\n        input_fn=lambda: input_fn(record_file=args.train_record,\n                                  batch_size=args.batch_size,\n                                  num_epochs=args.num_epochs,\n                                  shuffle=True),\n        steps=args.train_steps)\n\n    # Do evaluation after training.\n    tf.logging.info(\'Starting to evaluate.\')\n    evaluation = estimator.evaluate(\n        input_fn=lambda: input_fn(record_file=args.val_record,\n                                  batch_size=1,\n                                  num_epochs=1,\n                                  shuffle=False))\n    print(evaluation)\n\n    # Export trained model as SavedModel.\n    receiver_fn = tensor_input_receiver_fn if args.raw_input else serving_input_receiver_fn\n    if args.export_dir is not None:\n        estimator.export_savedmodel(args.export_dir, receiver_fn)\n\n\nif __name__ == \'__main__\':\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run(main)\n'"
model.py,32,"b'import tensorflow as tf\n\n\nclass LandmarkModel(object):\n    def __init__(self, output_size):\n        self.output_size = output_size\n\n    def __call__(self, input_tensor):\n        # |== Layer 0: input layer ==|\n        # Input feature x should be of shape (batch_size, image_width, image_height,\n        # color_channels). As we will directly using the decoded image tensor of\n        # data type int8, a convertion should be performed.\n        inputs = tf.cast(input_tensor, tf.float32)\n\n        # |== Layer 1 ==|\n\n        with tf.variable_scope(\'layer1\'):\n            # Convolutional layer.\n            # Computes 32 features using a 3x3 filter with ReLU activation.\n            conv1 = tf.layers.conv2d(\n                inputs=inputs,\n                filters=32,\n                kernel_size=[3, 3],\n                strides=(1, 1),\n                padding=\'valid\',\n                activation=tf.nn.relu)\n\n            # Pooling layer.\n            # First max pooling layer with a 2x2 filter and stride of 2.\n            pool1 = tf.layers.max_pooling2d(\n                inputs=conv1,\n                pool_size=[2, 2],\n                strides=(2, 2),\n                padding=\'valid\')\n\n        # |== Layer 2 ==|\n\n        with tf.variable_scope(\'layer2\'):\n            # Convolutional layer\n            # Computes 64 features using a 3x3 filter with ReLU activation.\n            conv2 = tf.layers.conv2d(\n                inputs=pool1,\n                filters=64,\n                kernel_size=[3, 3],\n                strides=(1, 1),\n                padding=\'valid\',\n                activation=tf.nn.relu)\n\n            # Convolutional layer\n            # Computes 64 features using a 3x3 filter with ReLU activation.\n            conv3 = tf.layers.conv2d(\n                inputs=conv2,\n                filters=64,\n                kernel_size=[3, 3],\n                strides=(1, 1),\n                padding=\'valid\',\n                activation=tf.nn.relu)\n\n            # Pooling layer\n            # Second max pooling layer with a 2x2 filter and stride of 2.\n            pool2 = tf.layers.max_pooling2d(\n                inputs=conv3,\n                pool_size=[2, 2],\n                strides=(2, 2),\n                padding=\'valid\')\n\n        # |== Layer 3 ==|\n\n        with tf.variable_scope(\'layer3\'):\n            # Convolutional layer\n            # Computes 64 features using a 3x3 filter with ReLU activation.\n            conv4 = tf.layers.conv2d(\n                inputs=pool2,\n                filters=64,\n                kernel_size=[3, 3],\n                strides=(1, 1),\n                padding=\'valid\',\n                activation=tf.nn.relu)\n\n            # Convolutional layer\n            # Computes 64 features using a 3x3 filter with ReLU activation.\n            conv5 = tf.layers.conv2d(\n                inputs=conv4,\n                filters=64,\n                kernel_size=[3, 3],\n                strides=(1, 1),\n                padding=\'valid\',\n                activation=tf.nn.relu)\n\n            # Pooling layer\n            # Third max pooling layer with a 2x2 filter and stride of 2.\n            pool3 = tf.layers.max_pooling2d(\n                inputs=conv5,\n                pool_size=[2, 2],\n                strides=(2, 2),\n                padding=\'valid\')\n\n        # |== Layer 4 ==|\n\n        with tf.variable_scope(\'layer4\'):\n            # Convolutional layer\n            # Computes 128 features using a 3x3 filter with ReLU activation.\n            conv6 = tf.layers.conv2d(\n                inputs=pool3,\n                filters=128,\n                kernel_size=[3, 3],\n                strides=(1, 1),\n                padding=\'valid\',\n                activation=tf.nn.relu)\n\n            # Convolutional layer\n            # Computes 128 features using a 3x3 filter with ReLU activation.\n            conv7 = tf.layers.conv2d(\n                inputs=conv6,\n                filters=128,\n                kernel_size=[3, 3],\n                strides=(1, 1),\n                padding=\'valid\',\n                activation=tf.nn.relu)\n\n            # Pooling layer\n            # Fourth max pooling layer with a 2x2 filter and stride of 2.\n            pool4 = tf.layers.max_pooling2d(\n                inputs=conv7,\n                pool_size=[2, 2],\n                strides=(1, 1),\n                padding=\'valid\')\n\n        # |== Layer 5 ==|\n\n        with tf.variable_scope(\'layer5\'):\n            # Convolutional layer\n            conv8 = tf.layers.conv2d(\n                inputs=pool4,\n                filters=256,\n                kernel_size=[3, 3],\n                strides=(1, 1),\n                padding=\'valid\',\n                activation=tf.nn.relu)\n\n        # |== Layer 6 ==|\n\n        with tf.variable_scope(\'layer6\'):\n            # Flatten tensor into a batch of vectors\n            flatten = tf.layers.flatten(inputs=conv8)\n\n            # Dense layer 1, a fully connected layer.\n            dense1 = tf.layers.dense(\n                inputs=flatten,\n                units=1024,\n                activation=tf.nn.relu,\n                use_bias=True)\n\n            # Dense layer 2, also known as the output layer.\n            logits = tf.layers.dense(\n                inputs=dense1,\n                units=self.output_size,\n                activation=None,\n                use_bias=True,\n                name=""logits"")\n            logits = tf.identity(logits, \'final_dense\')\n\n        return logits\n'"
