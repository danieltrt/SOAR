file_path,api_count,code
estimate_head_pose.py,0,"b'""""""Demo code shows how to estimate human head pose.\nCurrently, human face is detected by a detector from an OpenCV DNN module.\nThen the face box is modified a little to suits the need of landmark\ndetection. The facial landmark detection is done by a custom Convolutional\nNeural Network trained with TensorFlow. After that, head pose is estimated\nby solving a PnP problem.\n""""""\nfrom argparse import ArgumentParser\nfrom multiprocessing import Process, Queue\n\nimport cv2\nimport numpy as np\n\nfrom mark_detector import MarkDetector\nfrom os_detector import detect_os\nfrom pose_estimator import PoseEstimator\nfrom stabilizer import Stabilizer\n\nprint(""OpenCV version: {}"".format(cv2.__version__))\n\n# multiprocessing may not work on Windows and macOS, check OS for safety.\ndetect_os()\n\nCNN_INPUT_SIZE = 128\n\n# Take arguments from user input.\nparser = ArgumentParser()\nparser.add_argument(""--video"", type=str, default=None,\n                    help=""Video file to be processed."")\nparser.add_argument(""--cam"", type=int, default=None,\n                    help=""The webcam index."")\nargs = parser.parse_args()\n\n\ndef get_face(detector, img_queue, box_queue):\n    """"""Get face from image queue. This function is used for multiprocessing""""""\n    while True:\n        image = img_queue.get()\n        box = detector.extract_cnn_facebox(image)\n        box_queue.put(box)\n\n\ndef main():\n    """"""MAIN""""""\n    # Video source from webcam or video file.\n    video_src = args.cam if args.cam is not None else args.video\n    if video_src is None:\n        print(""Warning: video source not assigned, default webcam will be used."")\n        video_src = 0\n\n    cap = cv2.VideoCapture(video_src)\n    if video_src == 0:\n        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n    _, sample_frame = cap.read()\n\n    # Introduce mark_detector to detect landmarks.\n    mark_detector = MarkDetector()\n\n    # Setup process and queues for multiprocessing.\n    img_queue = Queue()\n    box_queue = Queue()\n    img_queue.put(sample_frame)\n    box_process = Process(target=get_face, args=(\n        mark_detector, img_queue, box_queue,))\n    box_process.start()\n\n    # Introduce pose estimator to solve pose. Get one frame to setup the\n    # estimator according to the image size.\n    height, width = sample_frame.shape[:2]\n    pose_estimator = PoseEstimator(img_size=(height, width))\n\n    # Introduce scalar stabilizers for pose.\n    pose_stabilizers = [Stabilizer(\n        state_num=2,\n        measure_num=1,\n        cov_process=0.1,\n        cov_measure=0.1) for _ in range(6)]\n\n    tm = cv2.TickMeter()\n\n    while True:\n        # Read frame, crop it, flip it, suits your needs.\n        frame_got, frame = cap.read()\n        if frame_got is False:\n            break\n\n        # Crop it if frame is larger than expected.\n        # frame = frame[0:480, 300:940]\n\n        # If frame comes from webcam, flip it so it looks like a mirror.\n        if video_src == 0:\n            frame = cv2.flip(frame, 2)\n\n        # Pose estimation by 3 steps:\n        # 1. detect face;\n        # 2. detect landmarks;\n        # 3. estimate pose\n\n        # Feed frame to image queue.\n        img_queue.put(frame)\n\n        # Get face from box queue.\n        facebox = box_queue.get()\n\n        if facebox is not None:\n            # Detect landmarks from image of 128x128.\n            face_img = frame[facebox[1]: facebox[3],\n                             facebox[0]: facebox[2]]\n            face_img = cv2.resize(face_img, (CNN_INPUT_SIZE, CNN_INPUT_SIZE))\n            face_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n\n            tm.start()\n            marks = mark_detector.detect_marks([face_img])\n            tm.stop()\n\n            # Convert the marks locations from local CNN to global image.\n            marks *= (facebox[2] - facebox[0])\n            marks[:, 0] += facebox[0]\n            marks[:, 1] += facebox[1]\n\n            # Uncomment following line to show raw marks.\n            # mark_detector.draw_marks(\n            #     frame, marks, color=(0, 255, 0))\n\n            # Uncomment following line to show facebox.\n            # mark_detector.draw_box(frame, [facebox])\n\n            # Try pose estimation with 68 points.\n            pose = pose_estimator.solve_pose_by_68_points(marks)\n\n            # Stabilize the pose.\n            steady_pose = []\n            pose_np = np.array(pose).flatten()\n            for value, ps_stb in zip(pose_np, pose_stabilizers):\n                ps_stb.update([value])\n                steady_pose.append(ps_stb.state[0])\n            steady_pose = np.reshape(steady_pose, (-1, 3))\n\n            # Uncomment following line to draw pose annotation on frame.\n            # pose_estimator.draw_annotation_box(\n            #     frame, pose[0], pose[1], color=(255, 128, 128))\n\n            # Uncomment following line to draw stabile pose annotation on frame.\n            pose_estimator.draw_annotation_box(\n                frame, steady_pose[0], steady_pose[1], color=(128, 255, 128))\n\n            # Uncomment following line to draw head axes on frame.\n            # pose_estimator.draw_axes(frame, stabile_pose[0], stabile_pose[1])\n\n        # Show preview.\n        cv2.imshow(""Preview"", frame)\n        if cv2.waitKey(10) == 27:\n            break\n\n    # Clean up the multiprocessing process.\n    box_process.terminate()\n    box_process.join()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
mark_detector.py,4,"b'""""""Human facial landmark detector based on Convolutional Neural Network.""""""\nimport cv2\nimport numpy as np\nimport tensorflow as tf\n\n\nclass FaceDetector:\n    """"""Detect human face from image""""""\n\n    def __init__(self,\n                 dnn_proto_text=\'assets/deploy.prototxt\',\n                 dnn_model=\'assets/res10_300x300_ssd_iter_140000.caffemodel\'):\n        """"""Initialization""""""\n        self.face_net = cv2.dnn.readNetFromCaffe(dnn_proto_text, dnn_model)\n        self.detection_result = None\n\n    def get_faceboxes(self, image, threshold=0.5):\n        """"""\n        Get the bounding box of faces in image using dnn.\n        """"""\n        rows, cols, _ = image.shape\n\n        confidences = []\n        faceboxes = []\n\n        self.face_net.setInput(cv2.dnn.blobFromImage(\n            image, 1.0, (300, 300), (104.0, 177.0, 123.0), False, False))\n        detections = self.face_net.forward()\n\n        for result in detections[0, 0, :, :]:\n            confidence = result[2]\n            if confidence > threshold:\n                x_left_bottom = int(result[3] * cols)\n                y_left_bottom = int(result[4] * rows)\n                x_right_top = int(result[5] * cols)\n                y_right_top = int(result[6] * rows)\n                confidences.append(confidence)\n                faceboxes.append(\n                    [x_left_bottom, y_left_bottom, x_right_top, y_right_top])\n\n        self.detection_result = [faceboxes, confidences]\n\n        return confidences, faceboxes\n\n    def draw_all_result(self, image):\n        """"""Draw the detection result on image""""""\n        for facebox, conf in self.detection_result:\n            cv2.rectangle(image, (facebox[0], facebox[1]),\n                          (facebox[2], facebox[3]), (0, 255, 0))\n            label = ""face: %.4f"" % conf\n            label_size, base_line = cv2.getTextSize(\n                label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n\n            cv2.rectangle(image, (facebox[0], facebox[1] - label_size[1]),\n                          (facebox[0] + label_size[0],\n                           facebox[1] + base_line),\n                          (0, 255, 0), cv2.FILLED)\n            cv2.putText(image, label, (facebox[0], facebox[1]),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0))\n\n\nclass MarkDetector:\n    """"""Facial landmark detector by Convolutional Neural Network""""""\n\n    def __init__(self, saved_model=\'assets/pose_model\'):\n        """"""Initialization""""""\n        # A face detector is required for mark detection.\n        self.face_detector = FaceDetector()\n\n        self.cnn_input_size = 128\n        self.marks = None\n\n        # Get a TensorFlow session ready to do landmark detection\n        # Load a Tensorflow saved model into memory.\n        self.graph = tf.Graph()\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        self.sess = tf.Session(graph=self.graph, config=config)\n\n        # Restore model from the saved_model file, that is exported by\n        # TensorFlow estimator.\n        tf.saved_model.loader.load(self.sess, [""serve""], saved_model)\n\n    @staticmethod\n    def draw_box(image, boxes, box_color=(255, 255, 255)):\n        """"""Draw square boxes on image""""""\n        for box in boxes:\n            cv2.rectangle(image,\n                          (box[0], box[1]),\n                          (box[2], box[3]), box_color, 3)\n\n    @staticmethod\n    def move_box(box, offset):\n        """"""Move the box to direction specified by vector offset""""""\n        left_x = box[0] + offset[0]\n        top_y = box[1] + offset[1]\n        right_x = box[2] + offset[0]\n        bottom_y = box[3] + offset[1]\n        return [left_x, top_y, right_x, bottom_y]\n\n    @staticmethod\n    def get_square_box(box):\n        """"""Get a square box out of the given box, by expanding it.""""""\n        left_x = box[0]\n        top_y = box[1]\n        right_x = box[2]\n        bottom_y = box[3]\n\n        box_width = right_x - left_x\n        box_height = bottom_y - top_y\n\n        # Check if box is already a square. If not, make it a square.\n        diff = box_height - box_width\n        delta = int(abs(diff) / 2)\n\n        if diff == 0:                   # Already a square.\n            return box\n        elif diff > 0:                  # Height > width, a slim box.\n            left_x -= delta\n            right_x += delta\n            if diff % 2 == 1:\n                right_x += 1\n        else:                           # Width > height, a short box.\n            top_y -= delta\n            bottom_y += delta\n            if diff % 2 == 1:\n                bottom_y += 1\n\n        # Make sure box is always square.\n        assert ((right_x - left_x) == (bottom_y - top_y)), \'Box is not square.\'\n\n        return [left_x, top_y, right_x, bottom_y]\n\n    @staticmethod\n    def box_in_image(box, image):\n        """"""Check if the box is in image""""""\n        rows = image.shape[0]\n        cols = image.shape[1]\n        return box[0] >= 0 and box[1] >= 0 and box[2] <= cols and box[3] <= rows\n\n    def extract_cnn_facebox(self, image):\n        """"""Extract face area from image.""""""\n        _, raw_boxes = self.face_detector.get_faceboxes(\n            image=image, threshold=0.9)\n\n        for box in raw_boxes:\n            # Move box down.\n            # diff_height_width = (box[3] - box[1]) - (box[2] - box[0])\n            offset_y = int(abs((box[3] - box[1]) * 0.1))\n            box_moved = self.move_box(box, [0, offset_y])\n\n            # Make box square.\n            facebox = self.get_square_box(box_moved)\n\n            if self.box_in_image(facebox, image):\n                return facebox\n\n        return None\n\n    def detect_marks(self, image_np):\n        """"""Detect marks from image""""""\n        # Get result tensor by its name.\n        logits_tensor = self.graph.get_tensor_by_name(\n            \'layer6/final_dense:0\')\n\n        # Actual detection.\n        predictions = self.sess.run(\n            logits_tensor,\n            feed_dict={\'image_tensor:0\': image_np})\n\n        # Convert predictions to landmarks.\n        marks = np.array(predictions).flatten()[:136]\n        marks = np.reshape(marks, (-1, 2))\n\n        return marks\n\n    @staticmethod\n    def draw_marks(image, marks, color=(255, 255, 255)):\n        """"""Draw mark points on image""""""\n        for mark in marks:\n            cv2.circle(image, (int(mark[0]), int(\n                mark[1])), 1, color, -1, cv2.LINE_AA)\n'"
optical_flow_tracker.py,0,"b'\'\'\'\nLucas-Kanade sparse optical flow tracker. Uses goodFeaturesToTrack\nfor track initialization and back-tracking for match verification\nbetween frames.\n\'\'\'\nfrom math import sqrt\n\nimport numpy as np\n\nimport cv2\n\n\nclass Tracker:\n    """"""Lucas-Kanade sparse optical flow tracker""""""\n\n    def __init__(self):\n        self.track_len = 5\n        self.tracks = []\n        self.lk_params = dict(winSize=(15, 15),\n                              maxLevel=2,\n                              criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n        self.feature_params = dict(maxCorners=500,\n                                   qualityLevel=0.3,\n                                   minDistance=7,\n                                   blockSize=7)\n\n    def update_tracks(self, img_old, img_new):\n        """"""Update tracks.""""""\n        # Get old points, using the latest one.\n        points_old = np.float32([track[-1]\n                                 for track in self.tracks]).reshape(-1, 1, 2)\n\n        # Get new points from old points.\n        points_new, _st, _err = cv2.calcOpticalFlowPyrLK(\n            img_old, img_new, points_old, None, **self.lk_params)\n\n        # Get inferred old points from new points.\n        points_old_inferred, _st, _err = cv2.calcOpticalFlowPyrLK(\n            img_new, img_old, points_new, None, **self.lk_params)\n\n        # Compare between old points and inferred old points\n        error_term = abs(\n            points_old - points_old_inferred).reshape(-1, 2).max(-1)\n        point_valid = error_term < 1\n\n        new_tracks = []\n        for track, (x, y), good_flag in zip(self.tracks, points_new.reshape(-1, 2), point_valid):\n            # Track is good?\n            if not good_flag:\n                continue\n\n            # New point is good, add to track.\n            track.append((x, y))\n\n            # Need to drop first old point?\n            if len(track) > self.track_len:\n                del track[0]\n\n            # Track updated, add to track groups.\n            new_tracks.append(track)\n\n        # New track groups got, do update.\n        self.tracks = new_tracks\n\n    def get_new_tracks(self, frame, roi):\n        """"""Get new tracks every detect_interval frames.""""""\n        # Using mask to determine where to look for feature points.\n        mask = np.zeros_like(frame)\n        mask[roi[0]:roi[1], roi[2]:roi[3]] = 255\n\n        # Get good feature points.\n        feature_points = cv2.goodFeaturesToTrack(\n            frame, mask=mask, **self.feature_params)\n\n        if feature_points is not None:\n            for x, y in np.float32(feature_points).reshape(-1, 2):\n                self.tracks.append([(x, y)])\n\n    def get_average_track_length(self):\n        """"""Get the average track length""""""\n        length = 0\n        tracks = np.array(self.tracks)\n        def distance(track):\n            """"""Get distance between the first and last point.""""""\n            delta_x = abs(track[-1][0] - track[0][0])\n            delta_y = abs(track[-1][1] - track[0][1])\n            return sqrt(delta_x*delta_x + delta_y*delta_y)\n        for track in tracks:\n            length += distance(track)\n        return length / len(tracks)\n\n    def draw_track(self, image):\n        """"""Draw track lines on image.""""""\n        cv2.polylines(image, [np.int32(track)\n                              for track in self.tracks], False, (0, 255, 0))\n\n\ndef main():\n    """"""Test code""""""\n    import sys\n    try:\n        video_src = sys.argv[1]\n    except:\n        video_src = 0\n\n    tracker = Tracker()\n\n    cam = cv2.VideoCapture(video_src)\n    detect_interval = 5\n    frame_idx = 0\n\n    prev_gray = cam.read()\n    while True:\n        _ret, frame = cam.read()\n        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n        # Update tracks.\n        if len(tracker.tracks) > 0:\n            tracker.update_tracks(prev_gray, frame_gray)\n\n        # Get new tracks every detect_interval frames.\n        target_box = [100, 400, 100, 400]\n        if frame_idx % detect_interval == 0:\n            tracker.get_new_tracks(frame_gray, target_box)\n\n        # Draw tracks\n        tracker.draw_track(frame)\n\n        frame_idx += 1\n        prev_gray = frame_gray\n        cv2.imshow(\'lk_track\', frame)\n        ch = cv2.waitKey(1)\n        if ch == 27:\n            break\n\n\nif __name__ == \'__main__\':\n    main()\n'"
os_detector.py,0,"b'""""""Detect OS""""""\nfrom platform import system\n\n\ndef detect_os(bypass=False):\n    """"""Check OS, as multiprocessing may not work properly on Windows and macOS""""""\n    if bypass is True:\n        return\n\n    os_name = system()\n\n    if os_name in [\'Windows\']:\n        print(""It seems that you are running this code from {}, on which the Python multiprocessing may not work properly. Consider running this code on Linux."".format(os_name))\n        print(""Exiting.."")\n        exit()\n    else:\n        print(""Linux is fine! Python multiprocessing works."")\n'"
pose_estimator.py,0,"b'""""""Estimate head pose according to the facial landmarks""""""\nimport cv2\nimport numpy as np\n\n\nclass PoseEstimator:\n    """"""Estimate head pose according to the facial landmarks""""""\n\n    def __init__(self, img_size=(480, 640)):\n        self.size = img_size\n\n        # 3D model points.\n        self.model_points = np.array([\n            (0.0, 0.0, 0.0),             # Nose tip\n            (0.0, -330.0, -65.0),        # Chin\n            (-225.0, 170.0, -135.0),     # Left eye left corner\n            (225.0, 170.0, -135.0),      # Right eye right corner\n            (-150.0, -150.0, -125.0),    # Mouth left corner\n            (150.0, -150.0, -125.0)      # Mouth right corner\n        ]) / 4.5\n\n        self.model_points_68 = self._get_full_model_points()\n\n        # Camera internals\n        self.focal_length = self.size[1]\n        self.camera_center = (self.size[1] / 2, self.size[0] / 2)\n        self.camera_matrix = np.array(\n            [[self.focal_length, 0, self.camera_center[0]],\n             [0, self.focal_length, self.camera_center[1]],\n             [0, 0, 1]], dtype=""double"")\n\n        # Assuming no lens distortion\n        self.dist_coeefs = np.zeros((4, 1))\n\n        # Rotation vector and translation vector\n        self.r_vec = np.array([[0.01891013], [0.08560084], [-3.14392813]])\n        self.t_vec = np.array(\n            [[-14.97821226], [-10.62040383], [-2053.03596872]])\n        # self.r_vec = None\n        # self.t_vec = None\n\n    def _get_full_model_points(self, filename=\'assets/model.txt\'):\n        """"""Get all 68 3D model points from file""""""\n        raw_value = []\n        with open(filename) as file:\n            for line in file:\n                raw_value.append(line)\n        model_points = np.array(raw_value, dtype=np.float32)\n        model_points = np.reshape(model_points, (3, -1)).T\n\n        # Transform the model into a front view.\n        model_points[:, 2] *= -1\n\n        return model_points\n\n    def show_3d_model(self):\n        from matplotlib import pyplot\n        from mpl_toolkits.mplot3d import Axes3D\n        fig = pyplot.figure()\n        ax = Axes3D(fig)\n\n        x = self.model_points_68[:, 0]\n        y = self.model_points_68[:, 1]\n        z = self.model_points_68[:, 2]\n\n        ax.scatter(x, y, z)\n        ax.axis(\'square\')\n        pyplot.xlabel(\'x\')\n        pyplot.ylabel(\'y\')\n        pyplot.show()\n\n    def solve_pose(self, image_points):\n        """"""\n        Solve pose from image points\n        Return (rotation_vector, translation_vector) as pose.\n        """"""\n        assert image_points.shape[0] == self.model_points_68.shape[0], ""3D points and 2D points should be of same number.""\n        (_, rotation_vector, translation_vector) = cv2.solvePnP(\n            self.model_points, image_points, self.camera_matrix, self.dist_coeefs)\n\n        # (success, rotation_vector, translation_vector) = cv2.solvePnP(\n        #     self.model_points,\n        #     image_points,\n        #     self.camera_matrix,\n        #     self.dist_coeefs,\n        #     rvec=self.r_vec,\n        #     tvec=self.t_vec,\n        #     useExtrinsicGuess=True)\n        return (rotation_vector, translation_vector)\n\n    def solve_pose_by_68_points(self, image_points):\n        """"""\n        Solve pose from all the 68 image points\n        Return (rotation_vector, translation_vector) as pose.\n        """"""\n\n        if self.r_vec is None:\n            (_, rotation_vector, translation_vector) = cv2.solvePnP(\n                self.model_points_68, image_points, self.camera_matrix, self.dist_coeefs)\n            self.r_vec = rotation_vector\n            self.t_vec = translation_vector\n\n        (_, rotation_vector, translation_vector) = cv2.solvePnP(\n            self.model_points_68,\n            image_points,\n            self.camera_matrix,\n            self.dist_coeefs,\n            rvec=self.r_vec,\n            tvec=self.t_vec,\n            useExtrinsicGuess=True)\n\n        return (rotation_vector, translation_vector)\n\n    def draw_annotation_box(self, image, rotation_vector, translation_vector, color=(255, 255, 255), line_width=2):\n        """"""Draw a 3D box as annotation of pose""""""\n        point_3d = []\n        rear_size = 75\n        rear_depth = 0\n        point_3d.append((-rear_size, -rear_size, rear_depth))\n        point_3d.append((-rear_size, rear_size, rear_depth))\n        point_3d.append((rear_size, rear_size, rear_depth))\n        point_3d.append((rear_size, -rear_size, rear_depth))\n        point_3d.append((-rear_size, -rear_size, rear_depth))\n\n        front_size = 100\n        front_depth = 100\n        point_3d.append((-front_size, -front_size, front_depth))\n        point_3d.append((-front_size, front_size, front_depth))\n        point_3d.append((front_size, front_size, front_depth))\n        point_3d.append((front_size, -front_size, front_depth))\n        point_3d.append((-front_size, -front_size, front_depth))\n        point_3d = np.array(point_3d, dtype=np.float).reshape(-1, 3)\n\n        # Map to 2d image points\n        (point_2d, _) = cv2.projectPoints(point_3d,\n                                          rotation_vector,\n                                          translation_vector,\n                                          self.camera_matrix,\n                                          self.dist_coeefs)\n        point_2d = np.int32(point_2d.reshape(-1, 2))\n\n        # Draw all the lines\n        cv2.polylines(image, [point_2d], True, color, line_width, cv2.LINE_AA)\n        cv2.line(image, tuple(point_2d[1]), tuple(\n            point_2d[6]), color, line_width, cv2.LINE_AA)\n        cv2.line(image, tuple(point_2d[2]), tuple(\n            point_2d[7]), color, line_width, cv2.LINE_AA)\n        cv2.line(image, tuple(point_2d[3]), tuple(\n            point_2d[8]), color, line_width, cv2.LINE_AA)\n\n    def draw_axis(self, img, R, t):\n        points = np.float32(\n            [[30, 0, 0], [0, 30, 0], [0, 0, 30], [0, 0, 0]]).reshape(-1, 3)\n\n        axisPoints, _ = cv2.projectPoints(\n            points, R, t, self.camera_matrix, self.dist_coeefs)\n\n        img = cv2.line(img, tuple(axisPoints[3].ravel()), tuple(\n            axisPoints[0].ravel()), (255, 0, 0), 3)\n        img = cv2.line(img, tuple(axisPoints[3].ravel()), tuple(\n            axisPoints[1].ravel()), (0, 255, 0), 3)\n        img = cv2.line(img, tuple(axisPoints[3].ravel()), tuple(\n            axisPoints[2].ravel()), (0, 0, 255), 3)\n\n    def draw_axes(self, img, R, t):\n        img\t= cv2.drawFrameAxes(img, self.camera_matrix, self.dist_coeefs, R, t, 30)\n\n\n    def get_pose_marks(self, marks):\n        """"""Get marks ready for pose estimation from 68 marks""""""\n        pose_marks = []\n        pose_marks.append(marks[30])    # Nose tip\n        pose_marks.append(marks[8])     # Chin\n        pose_marks.append(marks[36])    # Left eye left corner\n        pose_marks.append(marks[45])    # Right eye right corner\n        pose_marks.append(marks[48])    # Mouth left corner\n        pose_marks.append(marks[54])    # Mouth right corner\n        return pose_marks'"
stabilizer.py,0,"b'""""""\nUsing Kalman Filter as a point stabilizer to stabiliz a 2D point.\n""""""\nimport numpy as np\n\nimport cv2\n\n\nclass Stabilizer:\n    """"""Using Kalman filter as a point stabilizer.""""""\n\n    def __init__(self,\n                 state_num=4,\n                 measure_num=2,\n                 cov_process=0.0001,\n                 cov_measure=0.1):\n        """"""Initialization""""""\n        # Currently we only support scalar and point, so check user input first.\n        assert state_num == 4 or state_num == 2, ""Only scalar and point supported, Check state_num please.""\n\n        # Store the parameters.\n        self.state_num = state_num\n        self.measure_num = measure_num\n\n        # The filter itself.\n        self.filter = cv2.KalmanFilter(state_num, measure_num, 0)\n\n        # Store the state.\n        self.state = np.zeros((state_num, 1), dtype=np.float32)\n\n        # Store the measurement result.\n        self.measurement = np.array((measure_num, 1), np.float32)\n\n        # Store the prediction.\n        self.prediction = np.zeros((state_num, 1), np.float32)\n\n        # Kalman parameters setup for scalar.\n        if self.measure_num == 1:\n            self.filter.transitionMatrix = np.array([[1, 1],\n                                                     [0, 1]], np.float32)\n\n            self.filter.measurementMatrix = np.array([[1, 1]], np.float32)\n\n            self.filter.processNoiseCov = np.array([[1, 0],\n                                                    [0, 1]], np.float32) * cov_process\n\n            self.filter.measurementNoiseCov = np.array(\n                [[1]], np.float32) * cov_measure\n\n        # Kalman parameters setup for point.\n        if self.measure_num == 2:\n            self.filter.transitionMatrix = np.array([[1, 0, 1, 0],\n                                                     [0, 1, 0, 1],\n                                                     [0, 0, 1, 0],\n                                                     [0, 0, 0, 1]], np.float32)\n\n            self.filter.measurementMatrix = np.array([[1, 0, 0, 0],\n                                                      [0, 1, 0, 0]], np.float32)\n\n            self.filter.processNoiseCov = np.array([[1, 0, 0, 0],\n                                                    [0, 1, 0, 0],\n                                                    [0, 0, 1, 0],\n                                                    [0, 0, 0, 1]], np.float32) * cov_process\n\n            self.filter.measurementNoiseCov = np.array([[1, 0],\n                                                        [0, 1]], np.float32) * cov_measure\n\n    def update(self, measurement):\n        """"""Update the filter""""""\n        # Make kalman prediction\n        self.prediction = self.filter.predict()\n\n        # Get new measurement\n        if self.measure_num == 1:\n            self.measurement = np.array([[np.float32(measurement[0])]])\n        else:\n            self.measurement = np.array([[np.float32(measurement[0])],\n                                         [np.float32(measurement[1])]])\n\n        # Correct according to mesurement\n        self.filter.correct(self.measurement)\n\n        # Update state value.\n        self.state = self.filter.statePost\n\n    def set_q_r(self, cov_process=0.1, cov_measure=0.001):\n        """"""Set new value for processNoiseCov and measurementNoiseCov.""""""\n        if self.measure_num == 1:\n            self.filter.processNoiseCov = np.array([[1, 0],\n                                                    [0, 1]], np.float32) * cov_process\n            self.filter.measurementNoiseCov = np.array(\n                [[1]], np.float32) * cov_measure\n        else:\n            self.filter.processNoiseCov = np.array([[1, 0, 0, 0],\n                                                    [0, 1, 0, 0],\n                                                    [0, 0, 1, 0],\n                                                    [0, 0, 0, 1]], np.float32) * cov_process\n            self.filter.measurementNoiseCov = np.array([[1, 0],\n                                                        [0, 1]], np.float32) * cov_measure\n\n\ndef main():\n    """"""Test code""""""\n    global mp\n    mp = np.array((2, 1), np.float32)  # measurement\n\n    def onmouse(k, x, y, s, p):\n        global mp\n        mp = np.array([[np.float32(x)], [np.float32(y)]])\n\n    cv2.namedWindow(""kalman"")\n    cv2.setMouseCallback(""kalman"", onmouse)\n    kalman = Stabilizer(4, 2)\n    frame = np.zeros((480, 640, 3), np.uint8)  # drawing canvas\n\n    while True:\n        kalman.update(mp)\n        point = kalman.prediction\n        state = kalman.filter.statePost\n        cv2.circle(frame, (state[0], state[1]), 2, (255, 0, 0), -1)\n        cv2.circle(frame, (point[0], point[1]), 2, (0, 255, 0), -1)\n        cv2.imshow(""kalman"", frame)\n        k = cv2.waitKey(30) & 0xFF\n        if k == 27:\n            break\n\n\nif __name__ == \'__main__\':\n    main()\n'"
