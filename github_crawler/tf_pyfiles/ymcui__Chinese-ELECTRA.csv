file_path,api_count,code
build_openwebtext_pretraining_dataset.py,2,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Preprocessess the Open WebText corpus for ELECTRA pre-training.""""""\n\nimport argparse\nimport multiprocessing\nimport os\nimport random\nimport tarfile\nimport time\nimport tensorflow.compat.v1 as tf\n\nimport build_pretraining_dataset\nfrom util import utils\n\n\ndef write_examples(job_id, args):\n  """"""A single process creating and writing out pre-processed examples.""""""\n  job_tmp_dir = os.path.join(args.data_dir, ""tmp"", ""job_"" + str(job_id))\n  owt_dir = os.path.join(args.data_dir, ""openwebtext"")\n\n  def log(*args):\n    msg = "" "".join(map(str, args))\n    print(""Job {}:"".format(job_id), msg)\n\n  log(""Creating example writer"")\n  example_writer = build_pretraining_dataset.ExampleWriter(\n      job_id=job_id,\n      vocab_file=os.path.join(args.data_dir, ""vocab.txt""),\n      output_dir=os.path.join(args.data_dir, ""pretrain_tfrecords""),\n      max_seq_length=args.max_seq_length,\n      num_jobs=args.num_processes,\n      blanks_separate_docs=False,\n      do_lower_case=args.do_lower_case\n  )\n  log(""Writing tf examples"")\n  fnames = sorted(tf.io.gfile.listdir(owt_dir))\n  fnames = [f for (i, f) in enumerate(fnames)\n            if i % args.num_processes == job_id]\n  random.shuffle(fnames)\n  start_time = time.time()\n  for file_no, fname in enumerate(fnames):\n    if file_no > 0 and file_no % 10 == 0:\n      elapsed = time.time() - start_time\n      log(""processed {:}/{:} files ({:.1f}%), ELAPSED: {:}s, ETA: {:}s, ""\n          ""{:} examples written"".format(\n              file_no, len(fnames), 100.0 * file_no / len(fnames), int(elapsed),\n              int((len(fnames) - file_no) / (file_no / elapsed)),\n              example_writer.n_written))\n    utils.rmkdir(job_tmp_dir)\n    with tarfile.open(os.path.join(owt_dir, fname)) as f:\n      f.extractall(job_tmp_dir)\n    extracted_files = tf.io.gfile.listdir(job_tmp_dir)\n    random.shuffle(extracted_files)\n    for txt_fname in extracted_files:\n      example_writer.write_examples(os.path.join(job_tmp_dir, txt_fname))\n  example_writer.finish()\n  log(""Done!"")\n\n\ndef main():\n  parser = argparse.ArgumentParser(description=__doc__)\n  parser.add_argument(""--data-dir"", required=True,\n                      help=""Location of data (vocab file, corpus, etc)."")\n  parser.add_argument(""--max-seq-length"", default=128, type=int,\n                      help=""Number of tokens per example."")\n  parser.add_argument(""--num-processes"", default=1, type=int,\n                      help=""Parallelize across multiple processes."")\n  parser.add_argument(""--do-lower-case"", dest=\'do_lower_case\',\n                      action=\'store_true\', help=""Lower case input text."")\n  parser.add_argument(""--no-lower-case"", dest=\'do_lower_case\',\n                      action=\'store_false\', help=""Don\'t lower case input text."")\n  parser.set_defaults(do_lower_case=True)\n  args = parser.parse_args()\n\n  utils.rmkdir(os.path.join(args.data_dir, ""pretrain_tfrecords""))\n  if args.num_processes == 1:\n    write_examples(0, args)\n  else:\n    jobs = []\n    for i in range(args.num_processes):\n      job = multiprocessing.Process(target=write_examples, args=(i, args))\n      jobs.append(job)\n      job.start()\n    for job in jobs:\n      job.join()\n\n\nif __name__ == ""__main__"":\n  main()\n'"
build_pretraining_dataset.py,6,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Writes out text data as tfrecords that ELECTRA can be pre-trained on.""""""\n\nimport argparse\nimport multiprocessing\nimport os\nimport random\nimport time\nimport tensorflow.compat.v1 as tf\n\nfrom model import tokenization\nfrom util import utils\n\n\ndef create_int_feature(values):\n  feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n  return feature\n\n\nclass ExampleBuilder(object):\n  """"""Given a stream of input text, creates pretraining examples.""""""\n\n  def __init__(self, tokenizer, max_length):\n    self._tokenizer = tokenizer\n    self._current_sentences = []\n    self._current_length = 0\n    self._max_length = max_length\n    self._target_length = max_length\n\n  def add_line(self, line):\n    """"""Adds a line of text to the current example being built.""""""\n    line = line.strip().replace(""\\n"", "" "")\n    if (not line) and self._current_length != 0:  # empty lines separate docs\n      return self._create_example()\n    bert_tokens = self._tokenizer.tokenize(line)\n    bert_tokids = self._tokenizer.convert_tokens_to_ids(bert_tokens)\n    self._current_sentences.append(bert_tokids)\n    self._current_length += len(bert_tokids)\n    if self._current_length >= self._target_length:\n      return self._create_example()\n    return None\n\n  def _create_example(self):\n    """"""Creates a pre-training example from the current list of sentences.""""""\n    # small chance to only have one segment as in classification tasks\n    if random.random() < 0.1:\n      first_segment_target_length = 100000\n    else:\n      # -3 due to not yet having [CLS]/[SEP] tokens in the input text\n      first_segment_target_length = (self._target_length - 3) // 2\n\n    first_segment = []\n    second_segment = []\n    for sentence in self._current_sentences:\n      # the sentence goes to the first segment if (1) the first segment is\n      # empty, (2) the sentence doesn\'t put the first segment over length or\n      # (3) 50% of the time when it does put the first segment over length\n      if (first_segment or\n          len(first_segment) + len(sentence) < first_segment_target_length or\n          (second_segment and\n           len(first_segment) < first_segment_target_length and\n           random.random() < 0.5)):\n        first_segment += sentence\n      else:\n        second_segment += sentence\n\n    # trim to max_length while accounting for not-yet-added [CLS]/[SEP] tokens\n    first_segment = first_segment[:self._max_length - 2]\n    second_segment = second_segment[:max(0, self._max_length -\n                                         len(first_segment) - 3)]\n\n    # prepare to start building the next example\n    self._current_sentences = []\n    self._current_length = 0\n    # small chance for random-length instead of max_length-length example\n    if random.random() < 0.05:\n      self._target_length = random.randint(5, self._max_length)\n    else:\n      self._target_length = self._max_length\n\n    return self._make_tf_example(first_segment, second_segment)\n\n  def _make_tf_example(self, first_segment, second_segment):\n    """"""Converts two ""segments"" of text into a tf.train.Example.""""""\n    vocab = self._tokenizer.vocab\n    input_ids = [vocab[""[CLS]""]] + first_segment + [vocab[""[SEP]""]]\n    segment_ids = [0] * len(input_ids)\n    if second_segment:\n      input_ids += second_segment + [vocab[""[SEP]""]]\n      segment_ids += [1] * (len(second_segment) + 1)\n    input_mask = [1] * len(input_ids)\n    input_ids += [0] * (self._max_length - len(input_ids))\n    input_mask += [0] * (self._max_length - len(input_mask))\n    segment_ids += [0] * (self._max_length - len(segment_ids))\n    tf_example = tf.train.Example(features=tf.train.Features(feature={\n        ""input_ids"": create_int_feature(input_ids),\n        ""input_mask"": create_int_feature(input_mask),\n        ""segment_ids"": create_int_feature(segment_ids)\n    }))\n    return tf_example\n\n\nclass ExampleWriter(object):\n  """"""Writes pre-training examples to disk.""""""\n\n  def __init__(self, job_id, vocab_file, output_dir, max_seq_length,\n               num_jobs, blanks_separate_docs, do_lower_case,\n               num_out_files=1000):\n    self._blanks_separate_docs = blanks_separate_docs\n    tokenizer = tokenization.FullTokenizer(\n        vocab_file=vocab_file,\n        do_lower_case=do_lower_case)\n    self._example_builder = ExampleBuilder(tokenizer, max_seq_length)\n    self._writers = []\n    for i in range(num_out_files):\n      if i % num_jobs == job_id:\n        output_fname = os.path.join(\n            output_dir, ""pretrain_data.tfrecord-{:}-of-{:}"".format(\n                i, num_out_files))\n        self._writers.append(tf.io.TFRecordWriter(output_fname))\n    self.n_written = 0\n\n  def write_examples(self, input_file):\n    """"""Writes out examples from the provided input file.""""""\n    with tf.io.gfile.GFile(input_file) as f:\n      for line in f:\n        line = line.strip()\n        if line or self._blanks_separate_docs:\n          example = self._example_builder.add_line(line)\n          if example:\n            self._writers[self.n_written % len(self._writers)].write(\n                example.SerializeToString())\n            self.n_written += 1\n      example = self._example_builder.add_line("""")\n      if example:\n        self._writers[self.n_written % len(self._writers)].write(\n            example.SerializeToString())\n        self.n_written += 1\n\n  def finish(self):\n    for writer in self._writers:\n      writer.close()\n\n\ndef write_examples(job_id, args):\n  """"""A single process creating and writing out pre-processed examples.""""""\n\n  def log(*args):\n    msg = "" "".join(map(str, args))\n    print(""Job {}:"".format(job_id), msg)\n\n  log(""Creating example writer"")\n  example_writer = ExampleWriter(\n      job_id=job_id,\n      vocab_file=args.vocab_file,\n      output_dir=args.output_dir,\n      max_seq_length=args.max_seq_length,\n      num_jobs=args.num_processes,\n      blanks_separate_docs=args.blanks_separate_docs,\n      do_lower_case=args.do_lower_case\n  )\n  log(""Writing tf examples"")\n  fnames = sorted(tf.io.gfile.listdir(args.corpus_dir))\n  fnames = [f for (i, f) in enumerate(fnames)\n            if i % args.num_processes == job_id]\n  random.shuffle(fnames)\n  start_time = time.time()\n  for file_no, fname in enumerate(fnames):\n    if file_no > 0:\n      elapsed = time.time() - start_time\n      log(""processed {:}/{:} files ({:.1f}%), ELAPSED: {:}s, ETA: {:}s, ""\n          ""{:} examples written"".format(\n              file_no, len(fnames), 100.0 * file_no / len(fnames), int(elapsed),\n              int((len(fnames) - file_no) / (file_no / elapsed)),\n              example_writer.n_written))\n    example_writer.write_examples(os.path.join(args.corpus_dir, fname))\n  example_writer.finish()\n  log(""Done!"")\n\n\ndef main():\n  parser = argparse.ArgumentParser(description=__doc__)\n  parser.add_argument(""--corpus-dir"", required=True,\n                      help=""Location of pre-training text files."")\n  parser.add_argument(""--vocab-file"", required=True,\n                      help=""Location of vocabulary file."")\n  parser.add_argument(""--output-dir"", required=True,\n                      help=""Where to write out the tfrecords."")\n  parser.add_argument(""--max-seq-length"", default=128, type=int,\n                      help=""Number of tokens per example."")\n  parser.add_argument(""--num-processes"", default=1, type=int,\n                      help=""Parallelize across multiple processes."")\n  parser.add_argument(""--blanks-separate-docs"", default=True, type=bool,\n                      help=""Whether blank lines indicate document boundaries."")\n  parser.add_argument(""--do-lower-case"", dest=\'do_lower_case\',\n                      action=\'store_true\', help=""Lower case input text."")\n  parser.add_argument(""--no-lower-case"", dest=\'do_lower_case\',\n                      action=\'store_false\', help=""Don\'t lower case input text."")\n  parser.set_defaults(do_lower_case=True)\n  args = parser.parse_args()\n\n  utils.rmkdir(args.output_dir)\n  if args.num_processes == 1:\n    write_examples(0, args)\n  else:\n    jobs = []\n    for i in range(args.num_processes):\n      job = multiprocessing.Process(target=write_examples, args=(i, args))\n      jobs.append(job)\n      job.start()\n    for job in jobs:\n      job.join()\n\n\nif __name__ == ""__main__"":\n  main()\n'"
cmrc2018_drcd_evaluate.py,0,"b'# -*- coding: utf-8 -*-\n\'\'\'\nEvaluation script for CMRC 2018\nversion: v5\nNote: \nv5 formatted output, add usage description\nv4 fixed segmentation issues\n\'\'\'\nfrom __future__ import print_function\nfrom collections import Counter, OrderedDict\nimport string\nimport re\nimport argparse\nimport json\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\nimport nltk\nimport pdb\n\n# split Chinese with English\ndef mixed_segmentation(in_str, rm_punc=False):\n\tin_str = str(in_str).decode(\'utf-8\').lower().strip()\n\tsegs_out = []\n\ttemp_str = """"\n\tsp_char = [\'-\',\':\',\'_\',\'*\',\'^\',\'/\',\'\\\\\',\'~\',\'`\',\'+\',\'=\',\n\t\t\t   \'\xef\xbc\x8c\',\'\xe3\x80\x82\',\'\xef\xbc\x9a\',\'\xef\xbc\x9f\',\'\xef\xbc\x81\',\'\xe2\x80\x9c\',\'\xe2\x80\x9d\',\'\xef\xbc\x9b\',\'\xe2\x80\x99\',\'\xe3\x80\x8a\',\'\xe3\x80\x8b\',\'\xe2\x80\xa6\xe2\x80\xa6\',\'\xc2\xb7\',\'\xe3\x80\x81\',\n\t\t\t   \'\xe3\x80\x8c\',\'\xe3\x80\x8d\',\'\xef\xbc\x88\',\'\xef\xbc\x89\',\'\xef\xbc\x8d\',\'\xef\xbd\x9e\',\'\xe3\x80\x8e\',\'\xe3\x80\x8f\']\n\tfor char in in_str:\n\t\tif rm_punc and char in sp_char:\n\t\t\tcontinue\n\t\tif re.search(ur\'[\\u4e00-\\u9fa5]\', char) or char in sp_char:\n\t\t\tif temp_str != """":\n\t\t\t\tss = nltk.word_tokenize(temp_str)\n\t\t\t\tsegs_out.extend(ss)\n\t\t\t\ttemp_str = """"\n\t\t\tsegs_out.append(char)\n\t\telse:\n\t\t\ttemp_str += char\n\n\t#handling last part\n\tif temp_str != """":\n\t\tss = nltk.word_tokenize(temp_str)\n\t\tsegs_out.extend(ss)\n\n\treturn segs_out\n\n\n# remove punctuation\ndef remove_punctuation(in_str):\n\tin_str = str(in_str).decode(\'utf-8\').lower().strip()\n\tsp_char = [\'-\',\':\',\'_\',\'*\',\'^\',\'/\',\'\\\\\',\'~\',\'`\',\'+\',\'=\',\n\t\t\t   \'\xef\xbc\x8c\',\'\xe3\x80\x82\',\'\xef\xbc\x9a\',\'\xef\xbc\x9f\',\'\xef\xbc\x81\',\'\xe2\x80\x9c\',\'\xe2\x80\x9d\',\'\xef\xbc\x9b\',\'\xe2\x80\x99\',\'\xe3\x80\x8a\',\'\xe3\x80\x8b\',\'\xe2\x80\xa6\xe2\x80\xa6\',\'\xc2\xb7\',\'\xe3\x80\x81\',\n\t\t\t   \'\xe3\x80\x8c\',\'\xe3\x80\x8d\',\'\xef\xbc\x88\',\'\xef\xbc\x89\',\'\xef\xbc\x8d\',\'\xef\xbd\x9e\',\'\xe3\x80\x8e\',\'\xe3\x80\x8f\']\n\tout_segs = []\n\tfor char in in_str:\n\t\tif char in sp_char:\n\t\t\tcontinue\n\t\telse:\n\t\t\tout_segs.append(char)\n\treturn \'\'.join(out_segs)\n\n\n# find longest common string\ndef find_lcs(s1, s2):\n\tm = [[0 for i in range(len(s2)+1)] for j in range(len(s1)+1)]\n\tmmax = 0\n\tp = 0\n\tfor i in range(len(s1)):\n\t\tfor j in range(len(s2)):\n\t\t\tif s1[i] == s2[j]:\n\t\t\t\tm[i+1][j+1] = m[i][j]+1\n\t\t\t\tif m[i+1][j+1] > mmax:\n\t\t\t\t\tmmax=m[i+1][j+1]\n\t\t\t\t\tp=i+1\n\treturn s1[p-mmax:p], mmax\n\n#\ndef evaluate(ground_truth_file, prediction_file):\n\tf1 = 0\n\tem = 0\n\ttotal_count = 0\n\tskip_count = 0\n\tfor instance in ground_truth_file[""data""]:\n\t\t#context_id   = instance[\'context_id\'].strip()\n\t\t#context_text = instance[\'context_text\'].strip()\n\t\tfor para in instance[""paragraphs""]:\n\t\t\tfor qas in para[\'qas\']:\n\t\t\t\ttotal_count += 1\n\t\t\t\tquery_id    = qas[\'id\'].strip()\n\t\t\t\tquery_text  = qas[\'question\'].strip()\n\t\t\t\tanswers \t= [x[""text""] for x in qas[\'answers\']]\n\n\t\t\t\tif query_id not in prediction_file:\n\t\t\t\t\tsys.stderr.write(\'Unanswered question: {}\\n\'.format(query_id))\n\t\t\t\t\tskip_count += 1\n\t\t\t\t\tcontinue\n\n\t\t\t\tprediction \t= str(prediction_file[query_id]).decode(\'utf-8\')\n\t\t\t\tf1 += calc_f1_score(answers, prediction)\n\t\t\t\tem += calc_em_score(answers, prediction)\n\n\tf1_score = 100.0 * f1 / total_count\n\tem_score = 100.0 * em / total_count\n\treturn f1_score, em_score, total_count, skip_count\n\n\ndef calc_f1_score(answers, prediction):\n\tf1_scores = []\n\tfor ans in answers:\n\t\tans_segs = mixed_segmentation(ans, rm_punc=True)\n\t\tprediction_segs = mixed_segmentation(prediction, rm_punc=True)\n\t\tlcs, lcs_len = find_lcs(ans_segs, prediction_segs)\n\t\tif lcs_len == 0:\n\t\t\tf1_scores.append(0)\n\t\t\tcontinue\n\t\tprecision \t= 1.0*lcs_len/len(prediction_segs)\n\t\trecall \t\t= 1.0*lcs_len/len(ans_segs)\n\t\tf1 \t\t\t= (2*precision*recall)/(precision+recall)\n\t\tf1_scores.append(f1)\n\treturn max(f1_scores)\n\n\ndef calc_em_score(answers, prediction):\n\tem = 0\n\tfor ans in answers:\n\t\tans_ = remove_punctuation(ans)\n\t\tprediction_ = remove_punctuation(prediction)\n\t\tif ans_ == prediction_:\n\t\t\tem = 1\n\t\t\tbreak\n\treturn em\n\nif __name__ == \'__main__\':\n\tparser = argparse.ArgumentParser(description=\'Evaluation Script for CMRC 2018\')\n\tparser.add_argument(\'dataset_file\', help=\'Official dataset file\')\n\tparser.add_argument(\'prediction_file\', help=\'Your prediction File\')\n\targs = parser.parse_args()\n\tground_truth_file   = json.load(open(args.dataset_file, \'rb\'))\n\tprediction_file     = json.load(open(args.prediction_file, \'rb\'))\n\tF1, EM, TOTAL, SKIP = evaluate(ground_truth_file, prediction_file)\n\tAVG = (EM+F1)*0.5\n\toutput_result = OrderedDict()\n\toutput_result[\'AVERAGE\'] = \'%.3f\' % AVG\n\toutput_result[\'F1\'] = \'%.3f\' % F1\n\toutput_result[\'EM\'] = \'%.3f\' % EM\n\toutput_result[\'TOTAL\'] = TOTAL\n\toutput_result[\'SKIP\'] = SKIP\n\toutput_result[\'FILE\'] = args.prediction_file\n\tprint(json.dumps(output_result))\n\n'"
configure_finetuning.py,1,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Config controlling hyperparameters for fine-tuning ELECTRA.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport tensorflow.compat.v1 as tf\n\n\nclass FinetuningConfig(object):\n  """"""Fine-tuning hyperparameters.""""""\n\n  def __init__(self, model_name, data_dir, **kwargs):\n    # general\n    self.model_name = model_name\n    self.debug = False  # debug mode for quickly running things\n    self.log_examples = False  # print out some train examples for debugging\n    self.num_trials = 1  # how many train+eval runs to perform\n    self.do_train = True  # train a model\n    self.do_eval = True  # evaluate the model\n    self.do_test = True # evaluate on the test set\n    self.keep_all_models = True  # if False, only keep the last trial\'s ckpt\n\n    # model\n    self.model_size = ""small""  # one of ""small"", ""base"", or ""large""\n    self.task_names = [""chunk""]  # which tasks to learn\n    # override the default transformer hparams for the provided model size; see\n    # modeling.BertConfig for the possible hparams and util.training_utils for\n    # the defaults\n    self.model_hparam_overrides = (\n        kwargs[""model_hparam_overrides""]\n        if ""model_hparam_overrides"" in kwargs else {})\n    self.embedding_size = None  # bert hidden size by default\n    self.vocab_size = 30522  # number of tokens in the vocabulary\n    self.do_lower_case = True\n\n    # training\n    self.learning_rate = 1e-4\n    self.weight_decay_rate = 0.01\n    self.layerwise_lr_decay = 0.8  # if > 0, the learning rate for a layer is\n                                   # lr * lr_decay^(depth - max_depth) i.e.,\n                                   # shallower layers have lower learning rates\n    self.num_train_epochs = 3.0  # passes over the dataset during training\n    self.warmup_proportion = 0.1  # how much of training to warm up the LR for\n    self.save_checkpoints_steps = 1000000\n    self.iterations_per_loop = 1000\n    self.use_tfrecords_if_existing = True  # don\'t make tfrecords and write them\n                                           # to disc if existing ones are found\n\n    # writing model outputs to disc\n    self.write_test_outputs = False  # whether to write test set outputs,\n                                     # currently supported for GLUE + SQuAD 2.0\n    self.n_writes_test = 5  # write test set predictions for the first n trials\n\n    # sizing\n    self.max_seq_length = 128\n    self.train_batch_size = 32\n    self.eval_batch_size = 32\n    self.predict_batch_size = 32\n    self.double_unordered = True  # for tasks like paraphrase where sentence\n                                  # order doesn\'t matter, train the model on\n                                  # on both sentence orderings for each example\n    # for qa tasks\n    self.max_query_length = 64   # max tokens in q as opposed to context\n    self.doc_stride = 128  # stride when splitting doc into multiple examples\n    self.n_best_size = 20  # number of predictions per example to save\n    self.max_answer_length = 30  # filter out answers longer than this length\n    self.answerable_classifier = True  # answerable classifier for SQuAD 2.0\n    self.answerable_uses_start_logits = True  # more advanced answerable\n                                              # classifier using predicted start\n    self.answerable_weight = 0.5  # weight for answerability loss\n    self.joint_prediction = True  # jointly predict the start and end positions\n                                  # of the answer span\n    self.beam_size = 20  # beam size when doing joint predictions\n    self.qa_na_threshold = -2.75  # threshold for ""no answer"" when writing SQuAD\n                                  # 2.0 test outputs\n\n    # TPU settings\n    self.use_tpu = False\n    self.num_tpu_cores = 1\n    self.tpu_job_name = None\n    self.tpu_name = None  # cloud TPU to use for training\n    self.tpu_zone = None  # GCE zone where the Cloud TPU is located in\n    self.gcp_project = None  # project name for the Cloud TPU-enabled project\n\n    # default locations of data files\n    self.data_dir = data_dir\n    pretrained_model_dir = os.path.join(data_dir, ""models"", model_name)\n    self.raw_data_dir = os.path.join(data_dir, ""finetuning_data"", ""{:}"").format\n    self.vocab_file = os.path.join(pretrained_model_dir, ""vocab.txt"")\n    if not tf.io.gfile.exists(self.vocab_file):\n      self.vocab_file = os.path.join(self.data_dir, ""vocab.txt"")\n    task_names_str = "","".join(\n        kwargs[""task_names""] if ""task_names"" in kwargs else self.task_names)\n    self.init_checkpoint = None if self.debug else pretrained_model_dir\n    self.model_dir = os.path.join(pretrained_model_dir, ""finetuning_models"",\n                                  task_names_str + ""_model"")\n    results_dir = os.path.join(pretrained_model_dir, ""results"")\n    self.results_txt = os.path.join(results_dir,\n                                    task_names_str + ""_results.txt"")\n    self.results_pkl = os.path.join(results_dir,\n                                    task_names_str + ""_results.pkl"")\n    qa_topdir = os.path.join(results_dir, task_names_str + ""_qa"")\n    self.qa_eval_file = os.path.join(qa_topdir, ""{:}_eval.json"").format\n    self.qa_preds_file = os.path.join(qa_topdir, ""{:}_preds.json"").format\n    self.qa_na_file = os.path.join(qa_topdir, ""{:}_null_odds.json"").format\n    self.preprocessed_data_dir = os.path.join(\n        pretrained_model_dir, ""finetuning_tfrecords"",\n        task_names_str + ""_tfrecords"" + (""-debug"" if self.debug else """"))\n    self.test_predictions = os.path.join(\n        pretrained_model_dir, ""test_predictions"",\n        ""{:}_{:}_{:}_predictions.pkl"").format\n\n    # update defaults with passed-in hyperparameters\n    self.update(kwargs)\n\n    # default hyperparameters for single-task models\n    if len(self.task_names) == 1:\n      task_name = self.task_names[0]\n      if task_name == ""rte"" or task_name == ""sts"":\n        self.num_train_epochs = 10.0\n      elif ""squad"" in task_name or ""qa"" in task_name:\n        self.max_seq_length = 512\n        self.num_train_epochs = 2.0\n        self.write_distill_outputs = False\n        self.write_test_outputs = False\n      elif task_name == ""chunk"":\n        self.max_seq_length = 256\n      else:\n        self.num_train_epochs = 3.0\n\n    # default hyperparameters for different model sizes\n    if self.model_size == ""large"":\n      self.learning_rate = 5e-5\n      self.layerwise_lr_decay = 0.9\n    elif self.model_size == ""small"":\n      self.embedding_size = 128\n\n    # debug-mode settings\n    if self.debug:\n      self.save_checkpoints_steps = 1000000\n      self.use_tfrecords_if_existing = False\n      self.num_trials = 1\n      self.iterations_per_loop = 1\n      self.train_batch_size = 32\n      self.num_train_epochs = 3.0\n      self.log_examples = True\n\n    # passed-in-arguments override (for example) debug-mode defaults\n    self.update(kwargs)\n\n  def update(self, kwargs):\n    for k, v in kwargs.items():\n      if k not in self.__dict__:\n        raise ValueError(""Unknown hparam "" + k)\n      self.__dict__[k] = v\n'"
configure_pretraining.py,0,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Config controlling hyperparameters for pre-training ELECTRA.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\n\nclass PretrainingConfig(object):\n  """"""Defines pre-training hyperparameters.""""""\n\n  def __init__(self, model_name, data_dir, **kwargs):\n    self.model_name = model_name\n    self.debug = False  # debug mode for quickly running things\n    self.do_train = True  # pre-train ELECTRA\n    self.do_eval = False  # evaluate generator/discriminator on unlabeled data\n\n    # loss functions\n    self.electra_objective = True  # if False, use the BERT objective instead\n    self.gen_weight = 1.0  # masked language modeling / generator loss\n    self.disc_weight = 50.0  # discriminator loss\n    self.mask_prob = 0.15  # percent of input tokens to mask out / replace\n\n    # optimization\n    self.learning_rate = 5e-4\n    self.lr_decay_power = 1.0  # linear weight decay by default\n    self.weight_decay_rate = 0.01\n    self.num_warmup_steps = 10000\n\n    # training settings\n    self.iterations_per_loop = 200\n    self.save_checkpoints_steps = 1000\n    self.num_train_steps = 1000000\n    self.num_eval_steps = 100\n\n    # model settings\n    self.model_size = ""small""  # one of ""small"", ""base"", or ""large""\n    # override the default transformer hparams for the provided model size; see\n    # modeling.BertConfig for the possible hparams and util.training_utils for\n    # the defaults\n    self.model_hparam_overrides = (\n        kwargs[""model_hparam_overrides""]\n        if ""model_hparam_overrides"" in kwargs else {})\n    self.embedding_size = None  # bert hidden size by default\n    self.vocab_size = 30522  # number of tokens in the vocabulary\n    self.do_lower_case = True  # lowercase the input?\n\n    # generator settings\n    self.uniform_generator = False  # generator is uniform at random\n    self.untied_generator_embeddings = False  # tie generator/discriminator\n                                              # token embeddings?\n    self.untied_generator = True  # tie all generator/discriminator weights?\n    self.generator_layers = 1.0  # frac of discriminator layers for generator\n    self.generator_hidden_size = 0.25  # frac of discrim hidden size for gen\n    self.disallow_correct = False  # force the generator to sample incorrect\n                                   # tokens (so 15% of tokens are always\n                                   # fake)\n    self.temperature = 1.0  # temperature for sampling from generator\n\n    # batch sizes\n    self.max_seq_length = 128\n    self.train_batch_size = 128\n    self.eval_batch_size = 128\n\n    # TPU settings\n    self.use_tpu = False\n    self.num_tpu_cores = 1\n    self.tpu_job_name = None\n    self.tpu_name = None  # cloud TPU to use for training\n    self.tpu_zone = None  # GCE zone where the Cloud TPU is located in\n    self.gcp_project = None  # project name for the Cloud TPU-enabled project\n\n    # default locations of data files\n    self.pretrain_tfrecords = os.path.join(\n        data_dir, ""pretrain_tfrecords/pretrain_data.tfrecord*"")\n    self.vocab_file = os.path.join(data_dir, ""vocab.txt"")\n    self.model_dir = os.path.join(data_dir, ""models"", model_name)\n    results_dir = os.path.join(self.model_dir, ""results"")\n    self.results_txt = os.path.join(results_dir, ""unsup_results.txt"")\n    self.results_pkl = os.path.join(results_dir, ""unsup_results.pkl"")\n\n    # update defaults with passed-in hyperparameters\n    self.update(kwargs)\n\n    self.max_predictions_per_seq = int((self.mask_prob + 0.005) *\n                                       self.max_seq_length)\n\n    # debug-mode settings\n    if self.debug:\n      self.train_batch_size = 8\n      self.num_train_steps = 20\n      self.eval_batch_size = 4\n      self.iterations_per_loop = 1\n      self.num_eval_steps = 2\n\n    # defaults for different-sized model\n    if self.model_size == ""small"":\n      self.embedding_size = 128\n    # Here are the hyperparameters we used for larger models; see Table 6 in the\n    # paper for the full hyperparameters\n    # else:\n    #   self.max_seq_length = 512\n    #   self.learning_rate = 2e-4\n    #   if self.model_size == ""base"":\n    #     self.embedding_size = 768\n    #     self.generator_hidden_size = 0.33333\n    #     self.train_batch_size = 256\n    #   else:\n    #     self.embedding_size = 1024\n    #     self.mask_prob = 0.25\n    #     self.train_batch_size = 2048\n\n    # passed-in-arguments override (for example) debug-mode defaults\n    self.update(kwargs)\n\n  def update(self, kwargs):\n    for k, v in kwargs.items():\n      if k not in self.__dict__:\n        raise ValueError(""Unknown hparam "" + k)\n      self.__dict__[k] = v\n'"
run_finetuning.py,23,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Fine-tunes an ELECTRA model on a downstream task.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport collections\nimport json\n\nimport tensorflow.compat.v1 as tf\n\nimport configure_finetuning\nfrom finetune import preprocessing\nfrom finetune import task_builder\nfrom model import modeling\nfrom model import optimization\nfrom util import training_utils\nfrom util import utils\nimport numpy as np\n\n\nclass FinetuningModel(object):\n  """"""Finetuning model with support for multi-task training.""""""\n\n  def __init__(self, config: configure_finetuning.FinetuningConfig, tasks,\n               is_training, features, num_train_steps):\n    # Create a shared transformer encoder\n    bert_config = training_utils.get_bert_config(config)\n    self.bert_config = bert_config\n    if config.debug:\n      bert_config.num_hidden_layers = 3\n      bert_config.hidden_size = 144\n      bert_config.intermediate_size = 144 * 4\n      bert_config.num_attention_heads = 4\n    assert config.max_seq_length <= bert_config.max_position_embeddings\n    bert_model = modeling.BertModel(\n        bert_config=bert_config,\n        is_training=is_training,\n        input_ids=features[""input_ids""],\n        input_mask=features[""input_mask""],\n        token_type_ids=features[""segment_ids""],\n        use_one_hot_embeddings=config.use_tpu,\n        embedding_size=config.embedding_size)\n    percent_done = (tf.cast(tf.train.get_or_create_global_step(), tf.float32) /\n                    tf.cast(num_train_steps, tf.float32))\n\n    # Add specific tasks\n    self.outputs = {""task_id"": features[""task_id""]}\n    losses = []\n    for task in tasks:\n      with tf.variable_scope(""task_specific/"" + task.name):\n        task_losses, task_outputs = task.get_prediction_module(\n            bert_model, features, is_training, percent_done)\n        losses.append(task_losses)\n        self.outputs[task.name] = task_outputs\n    self.loss = tf.reduce_sum(\n        tf.stack(losses, -1) *\n        tf.one_hot(features[""task_id""], len(config.task_names)))\n\n\ndef model_fn_builder(config: configure_finetuning.FinetuningConfig, tasks,\n                     num_train_steps, pretraining_config=None):\n  """"""Returns `model_fn` closure for TPUEstimator.""""""\n\n  def model_fn(features, labels, mode, params):\n    """"""The `model_fn` for TPUEstimator.""""""\n    utils.log(""Building model..."")\n    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n    model = FinetuningModel(\n        config, tasks, is_training, features, num_train_steps)\n\n    # Load pre-trained weights from checkpoint\n    init_checkpoint = config.init_checkpoint\n    if pretraining_config is not None:\n      init_checkpoint = tf.train.latest_checkpoint(pretraining_config.model_dir)\n      utils.log(""Using checkpoint"", init_checkpoint)\n    tvars = tf.trainable_variables()\n\n    # calculate total number of params\n    num_params = sum([np.prod(v.shape) for v in tvars])\n    utils.log(\'##### params: {} #####\'.format(num_params))\n\n    scaffold_fn = None\n    if init_checkpoint:\n      assignment_map, _ = modeling.get_assignment_map_from_checkpoint(\n          tvars, init_checkpoint)\n      if config.use_tpu:\n        def tpu_scaffold():\n          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n          return tf.train.Scaffold()\n        scaffold_fn = tpu_scaffold\n      else:\n        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n\n    # Build model for training or prediction\n    if mode == tf.estimator.ModeKeys.TRAIN:\n      train_op = optimization.create_optimizer(\n          model.loss, config.learning_rate, num_train_steps,\n          weight_decay_rate=config.weight_decay_rate,\n          use_tpu=config.use_tpu,\n          warmup_proportion=config.warmup_proportion,\n          layerwise_lr_decay_power=config.layerwise_lr_decay,\n          n_transformer_layers=model.bert_config.num_hidden_layers\n      )\n      output_spec = tf.estimator.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=model.loss,\n          train_op=train_op,\n          scaffold_fn=scaffold_fn,\n          training_hooks=[training_utils.ETAHook(\n              {} if config.use_tpu else dict(loss=model.loss),\n              num_train_steps, config.iterations_per_loop, config.use_tpu, 10)])\n    else:\n      assert mode == tf.estimator.ModeKeys.PREDICT\n      output_spec = tf.estimator.tpu.TPUEstimatorSpec(\n          mode=mode,\n          predictions=utils.flatten_dict(model.outputs),\n          scaffold_fn=scaffold_fn)\n\n    utils.log(""Building complete"")\n    return output_spec\n\n  return model_fn\n\n\nclass ModelRunner(object):\n  """"""Fine-tunes a model on a supervised task.""""""\n\n  def __init__(self, config: configure_finetuning.FinetuningConfig, tasks,\n               pretraining_config=None):\n    self._config = config\n    self._tasks = tasks\n    self._preprocessor = preprocessing.Preprocessor(config, self._tasks)\n\n    is_per_host = tf.estimator.tpu.InputPipelineConfig.PER_HOST_V2\n    tpu_cluster_resolver = None\n    if config.use_tpu and config.tpu_name:\n      tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n          config.tpu_name, zone=config.tpu_zone, project=config.gcp_project)\n    tpu_config = tf.estimator.tpu.TPUConfig(\n        iterations_per_loop=config.iterations_per_loop,\n        num_shards=config.num_tpu_cores,\n        per_host_input_for_training=is_per_host,\n        tpu_job_name=config.tpu_job_name)\n    run_config = tf.estimator.tpu.RunConfig(\n        cluster=tpu_cluster_resolver,\n        model_dir=config.model_dir,\n        save_checkpoints_steps=config.save_checkpoints_steps,\n        save_checkpoints_secs=None,\n        tpu_config=tpu_config)\n\n    if self._config.do_train:\n      (self._train_input_fn,\n       self.train_steps) = self._preprocessor.prepare_train()\n    else:\n      self._train_input_fn, self.train_steps = None, 0\n    model_fn = model_fn_builder(\n        config=config,\n        tasks=self._tasks,\n        num_train_steps=self.train_steps,\n        pretraining_config=pretraining_config)\n    self._estimator = tf.estimator.tpu.TPUEstimator(\n        use_tpu=config.use_tpu,\n        model_fn=model_fn,\n        config=run_config,\n        train_batch_size=config.train_batch_size,\n        eval_batch_size=config.eval_batch_size,\n        predict_batch_size=config.predict_batch_size)\n\n  def train(self):\n    utils.log(""Training for {:} steps"".format(self.train_steps))\n    self._estimator.train(\n        input_fn=self._train_input_fn, max_steps=self.train_steps)\n\n  def evaluate(self, split=""dev""):\n    return {task.name: self.evaluate_task(task, split=split) for task in self._tasks}\n\n  def evaluate_task(self, task, split=""dev"", return_results=True):\n    """"""Evaluate the current model.""""""\n    utils.log(""Evaluating"", task.name, split)\n    eval_input_fn, _ = self._preprocessor.prepare_predict([task], split)\n    results = self._estimator.predict(input_fn=eval_input_fn,\n                                      yield_single_examples=True)\n    if task.name == ""cmrc2018"" or task.name == ""drcd"":\n      scorer = task.get_scorer(split)\n    else:\n      scorer = task.get_scorer()\n    for r in results:\n      if r[""task_id""] != len(self._tasks):  # ignore padding examples\n        r = utils.nest_dict(r, self._config.task_names)\n        scorer.update(r[task.name])\n    if return_results:\n      utils.log(task.name + "": "" + scorer.results_str())\n      utils.log()\n      return dict(scorer.get_results())\n    else:\n      return scorer\n\n  def write_classification_outputs(self, tasks, trial, split):\n    """"""Write classification predictions to disk.""""""\n    utils.log(""Writing out predictions for"", tasks, split)\n    predict_input_fn, _ = self._preprocessor.prepare_predict(tasks, split)\n    results = self._estimator.predict(input_fn=predict_input_fn,\n                                      yield_single_examples=True)\n    # task name -> eid -> model-logits\n    logits = collections.defaultdict(dict)\n    for r in results:\n      if r[""task_id""] != len(self._tasks):\n        r = utils.nest_dict(r, self._config.task_names)\n        task_name = self._config.task_names[r[""task_id""]]\n        logits[task_name][r[task_name][""eid""]] = (\n            r[task_name][""logits""] if ""logits"" in r[task_name]\n            else r[task_name][""predictions""])\n    for task_name in logits:\n      utils.log(""Pickling predictions for {:} {:} examples ({:})"".format(\n          len(logits[task_name]), task_name, split))\n      if trial <= self._config.n_writes_test:\n        utils.write_pickle(logits[task_name], self._config.test_predictions(\n            task_name, split, trial))\n\n\ndef write_results(config: configure_finetuning.FinetuningConfig, results):\n  """"""Write evaluation metrics to disk.""""""\n  utils.log(""Writing results to"", config.results_txt)\n  utils.mkdir(config.results_txt.rsplit(""/"", 1)[0])\n  utils.write_pickle(results, config.results_pkl)\n  with tf.io.gfile.GFile(config.results_txt, ""w"") as f:\n    results_str = """"\n    for trial_results in results:\n      for task_name, task_results in trial_results.items():\n        if task_name == ""time"" or task_name == ""global_step"":\n          continue\n        results_str += task_name + "": "" + "" - "".join(\n            [""{:}: {:.2f}"".format(k, v)\n             for k, v in task_results.items()]) + ""\\n""\n    f.write(results_str)\n  utils.write_pickle(results, config.results_pkl)\n\n\ndef run_finetuning(config: configure_finetuning.FinetuningConfig):\n  """"""Run finetuning.""""""\n\n  # Setup for training\n  results = []\n  trial = 1\n  heading_info = ""model={:}, trial {:}/{:}"".format(\n      config.model_name, trial, config.num_trials)\n  heading = lambda msg: utils.heading(msg + "": "" + heading_info)\n  heading(""Config"")\n  utils.log_config(config)\n  generic_model_dir = config.model_dir\n  tasks = task_builder.get_tasks(config)\n\n  # Train and evaluate num_trials models with different random seeds\n  while config.num_trials < 0 or trial <= config.num_trials:\n    config.model_dir = generic_model_dir + ""_"" + str(trial)\n    if config.do_train:\n      utils.rmkdir(config.model_dir)\n\n    model_runner = ModelRunner(config, tasks)\n    if config.do_train:\n      heading_info = ""model={:}, trial {:}/{:}"".format(config.model_name, trial, config.num_trials)\n      heading(""Start training"")\n      model_runner.train()\n      utils.log()\n\n    if config.do_eval:\n      if config.write_test_outputs and trial <= config.n_writes_test:\n        heading(""Running on the dev set and writing the predictions"")\n        for task in tasks:\n          # Currently only writing preds for GLUE and SQuAD 2.0 is supported\n          if task.name in [""cola"", ""mrpc"", ""mnli"", ""sst"", ""rte"", ""qnli"", ""qqp"",\n                           ""sts""]:\n            for split in task.get_test_splits():\n              model_runner.write_classification_outputs([task], trial, split)\n          elif task.name == ""squad"":\n            scorer = model_runner.evaluate_task(task, ""dev"", False)\n            scorer.write_predictions()\n            preds = utils.load_json(config.qa_preds_file(task.name+""_dev""))\n            null_odds = utils.load_json(config.qa_na_file(task.name+""_dev""))\n            for q, _ in preds.items():\n              if null_odds[q] > config.qa_na_threshold:\n                preds[q] = """"\n            utils.write_json(preds, config.test_predictions(\n                task.name, ""dev"", trial))\n          elif task.name == ""cmrc2018"" or task.name == ""drcd"":\n            scorer = model_runner.evaluate_task(task, ""dev"", False)\n            scorer.write_predictions()\n            preds = utils.load_json(config.qa_preds_file(task.name+""_dev""))\n            #utils.write_json(preds, config.test_predictions(task.name, ""dev"", trial))\n            if config.num_trials > 1:\n              utils.write_json(preds, config.qa_preds_file(task.name+""_dev_""+str(trial)))\n          else:\n            utils.log(""Skipping task"", task.name,\n                      ""- writing predictions is not supported for this task"")\n      else:\n        heading(""Run dev set evaluation"")\n        results.append(model_runner.evaluate(split=""dev""))\n        write_results(config, results)\n\n    if config.do_test:\n      if config.write_test_outputs and trial <= config.n_writes_test:\n        heading(""Running on the test set and writing the predictions"")\n        for task in tasks:\n          # Currently only writing preds for GLUE and SQuAD 2.0 is supported\n          if task.name in [""cola"", ""mrpc"", ""mnli"", ""sst"", ""rte"", ""qnli"", ""qqp"",\n                           ""sts""]:\n            for split in task.get_test_splits():\n              model_runner.write_classification_outputs([task], trial, split)\n          elif task.name == ""squad"":\n            scorer = model_runner.evaluate_task(task, ""eval"", False)\n            scorer.write_predictions()\n            preds = utils.load_json(config.qa_preds_file(task.name+""_eval""))\n            null_odds = utils.load_json(config.qa_na_file(task.name+""_eval""))\n            for q, _ in preds.items():\n              if null_odds[q] > config.qa_na_threshold:\n                preds[q] = """"\n            utils.write_json(preds, config.test_predictions(\n                task.name, ""eval"", trial))\n          elif task.name == ""cmrc2018"" or task.name == ""drcd"":\n            scorer = model_runner.evaluate_task(task, ""eval"", False)\n            scorer.write_predictions()\n            preds = utils.load_json(config.qa_preds_file(task.name+""_eval""))\n            #utils.write_json(preds, config.test_predictions(task.name, ""eval"", trial))\n            if config.num_trials > 1:\n              utils.write_json(preds, config.qa_preds_file(task.name+""_eval_""+str(trial)))\n          else:\n            utils.log(""Skipping task"", task.name,\n                      ""- writing predictions is not supported for this task"")\n      else:\n        heading(""Run test set evaluation"")\n        results.append(model_runner.evaluate(split=""eval""))\n        write_results(config, results)\n\n    if trial != config.num_trials and (not config.keep_all_models):\n      utils.rmrf(config.model_dir)\n    trial += 1\n\n\ndef main():\n  parser = argparse.ArgumentParser(description=__doc__)\n  parser.add_argument(""--data-dir"", required=True,\n                      help=""Location of data files (model weights, etc)."")\n  parser.add_argument(""--model-name"", required=True,\n                      help=""The name of the model being fine-tuned."")\n  parser.add_argument(""--hparams"", default=""{}"",\n                      help=""JSON dict of model hyperparameters."")\n  args = parser.parse_args()\n  if args.hparams.endswith("".json""):\n    hparams = utils.load_json(args.hparams)\n  else:\n    hparams = json.loads(args.hparams)\n  tf.logging.set_verbosity(tf.logging.ERROR)\n  run_finetuning(configure_finetuning.FinetuningConfig(\n      args.model_name, args.data_dir, **hparams))\n\n\nif __name__ == ""__main__"":\n  main()\n'"
run_pretraining.py,68,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Pre-trains an ELECTRA model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport collections\nimport json\n\nimport tensorflow.compat.v1 as tf\n\nimport configure_pretraining\nfrom model import modeling\nfrom model import optimization\nfrom pretrain import pretrain_data\nfrom pretrain import pretrain_helpers\nfrom util import training_utils\nfrom util import utils\n\n\nclass PretrainingModel(object):\n  """"""Transformer pre-training using the replaced-token-detection task.""""""\n\n  def __init__(self, config: configure_pretraining.PretrainingConfig,\n               features, is_training):\n    # Set up model config\n    self._config = config\n    self._bert_config = training_utils.get_bert_config(config)\n    if config.debug:\n      self._bert_config.num_hidden_layers = 3\n      self._bert_config.hidden_size = 144\n      self._bert_config.intermediate_size = 144 * 4\n      self._bert_config.num_attention_heads = 4\n\n    # Mask the input\n    masked_inputs = pretrain_helpers.mask(\n        config, pretrain_data.features_to_inputs(features), config.mask_prob)\n\n    # Generator\n    embedding_size = (\n        self._bert_config.hidden_size if config.embedding_size is None else\n        config.embedding_size)\n    if config.uniform_generator:\n      mlm_output = self._get_masked_lm_output(masked_inputs, None)\n    elif config.electra_objective and config.untied_generator:\n      generator = self._build_transformer(\n          masked_inputs, is_training,\n          bert_config=get_generator_config(config, self._bert_config),\n          embedding_size=(None if config.untied_generator_embeddings\n                          else embedding_size),\n          untied_embeddings=config.untied_generator_embeddings,\n          name=""generator"")\n      mlm_output = self._get_masked_lm_output(masked_inputs, generator)\n    else:\n      generator = self._build_transformer(\n          masked_inputs, is_training, embedding_size=embedding_size)\n      mlm_output = self._get_masked_lm_output(masked_inputs, generator)\n    fake_data = self._get_fake_data(masked_inputs, mlm_output.logits)\n    self.mlm_output = mlm_output\n    self.total_loss = config.gen_weight * mlm_output.loss\n\n    # Discriminator\n    disc_output = None\n    if config.electra_objective:\n      discriminator = self._build_transformer(\n          fake_data.inputs, is_training, reuse=not config.untied_generator,\n          embedding_size=embedding_size)\n      disc_output = self._get_discriminator_output(\n          fake_data.inputs, discriminator, fake_data.is_fake_tokens)\n      self.total_loss += config.disc_weight * disc_output.loss\n\n    # Evaluation\n    eval_fn_inputs = {\n        ""input_ids"": masked_inputs.input_ids,\n        ""masked_lm_preds"": mlm_output.preds,\n        ""mlm_loss"": mlm_output.per_example_loss,\n        ""masked_lm_ids"": masked_inputs.masked_lm_ids,\n        ""masked_lm_weights"": masked_inputs.masked_lm_weights,\n        ""input_mask"": masked_inputs.input_mask\n    }\n    if config.electra_objective:\n      eval_fn_inputs.update({\n          ""disc_loss"": disc_output.per_example_loss,\n          ""disc_labels"": disc_output.labels,\n          ""disc_probs"": disc_output.probs,\n          ""disc_preds"": disc_output.preds,\n          ""sampled_tokids"": tf.argmax(fake_data.sampled_tokens, -1,\n                                      output_type=tf.int32)\n      })\n    eval_fn_keys = eval_fn_inputs.keys()\n    eval_fn_values = [eval_fn_inputs[k] for k in eval_fn_keys]\n\n    def metric_fn(*args):\n      """"""Computes the loss and accuracy of the model.""""""\n      d = {k: arg for k, arg in zip(eval_fn_keys, args)}\n      metrics = dict()\n      metrics[""masked_lm_accuracy""] = tf.metrics.accuracy(\n          labels=tf.reshape(d[""masked_lm_ids""], [-1]),\n          predictions=tf.reshape(d[""masked_lm_preds""], [-1]),\n          weights=tf.reshape(d[""masked_lm_weights""], [-1]))\n      metrics[""masked_lm_loss""] = tf.metrics.mean(\n          values=tf.reshape(d[""mlm_loss""], [-1]),\n          weights=tf.reshape(d[""masked_lm_weights""], [-1]))\n      if config.electra_objective:\n        metrics[""sampled_masked_lm_accuracy""] = tf.metrics.accuracy(\n            labels=tf.reshape(d[""masked_lm_ids""], [-1]),\n            predictions=tf.reshape(d[""sampled_tokids""], [-1]),\n            weights=tf.reshape(d[""masked_lm_weights""], [-1]))\n        if config.disc_weight > 0:\n          metrics[""disc_loss""] = tf.metrics.mean(d[""disc_loss""])\n          metrics[""disc_auc""] = tf.metrics.auc(\n              d[""disc_labels""] * d[""input_mask""],\n              d[""disc_probs""] * tf.cast(d[""input_mask""], tf.float32))\n          metrics[""disc_accuracy""] = tf.metrics.accuracy(\n              labels=d[""disc_labels""], predictions=d[""disc_preds""],\n              weights=d[""input_mask""])\n          metrics[""disc_precision""] = tf.metrics.accuracy(\n              labels=d[""disc_labels""], predictions=d[""disc_preds""],\n              weights=d[""disc_preds""] * d[""input_mask""])\n          metrics[""disc_recall""] = tf.metrics.accuracy(\n              labels=d[""disc_labels""], predictions=d[""disc_preds""],\n              weights=d[""disc_labels""] * d[""input_mask""])\n      return metrics\n    self.eval_metrics = (metric_fn, eval_fn_values)\n\n  def _get_masked_lm_output(self, inputs: pretrain_data.Inputs, model):\n    """"""Masked language modeling softmax layer.""""""\n    masked_lm_weights = inputs.masked_lm_weights\n    with tf.variable_scope(""generator_predictions""):\n      if self._config.uniform_generator:\n        logits = tf.zeros(self._bert_config.vocab_size)\n        logits_tiled = tf.zeros(\n            modeling.get_shape_list(inputs.masked_lm_ids) +\n            [self._bert_config.vocab_size])\n        logits_tiled += tf.reshape(logits, [1, 1, self._bert_config.vocab_size])\n        logits = logits_tiled\n      else:\n        relevant_hidden = pretrain_helpers.gather_positions(\n            model.get_sequence_output(), inputs.masked_lm_positions)\n        hidden = tf.layers.dense(\n            relevant_hidden,\n            units=modeling.get_shape_list(model.get_embedding_table())[-1],\n            activation=modeling.get_activation(self._bert_config.hidden_act),\n            kernel_initializer=modeling.create_initializer(\n                self._bert_config.initializer_range))\n        hidden = modeling.layer_norm(hidden)\n        output_bias = tf.get_variable(\n            ""output_bias"",\n            shape=[self._bert_config.vocab_size],\n            initializer=tf.zeros_initializer())\n        logits = tf.matmul(hidden, model.get_embedding_table(),\n                           transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n\n      oh_labels = tf.one_hot(\n          inputs.masked_lm_ids, depth=self._bert_config.vocab_size,\n          dtype=tf.float32)\n\n      probs = tf.nn.softmax(logits)\n      log_probs = tf.nn.log_softmax(logits)\n      label_log_probs = -tf.reduce_sum(log_probs * oh_labels, axis=-1)\n\n      numerator = tf.reduce_sum(inputs.masked_lm_weights * label_log_probs)\n      denominator = tf.reduce_sum(masked_lm_weights) + 1e-6\n      loss = numerator / denominator\n      preds = tf.argmax(log_probs, axis=-1, output_type=tf.int32)\n\n      MLMOutput = collections.namedtuple(\n          ""MLMOutput"", [""logits"", ""probs"", ""loss"", ""per_example_loss"", ""preds""])\n      return MLMOutput(\n          logits=logits, probs=probs, per_example_loss=label_log_probs,\n          loss=loss, preds=preds)\n\n  def _get_discriminator_output(self, inputs, discriminator, labels):\n    """"""Discriminator binary classifier.""""""\n    with tf.variable_scope(""discriminator_predictions""):\n      hidden = tf.layers.dense(\n          discriminator.get_sequence_output(),\n          units=self._bert_config.hidden_size,\n          activation=modeling.get_activation(self._bert_config.hidden_act),\n          kernel_initializer=modeling.create_initializer(\n              self._bert_config.initializer_range))\n      logits = tf.squeeze(tf.layers.dense(hidden, units=1), -1)\n      weights = tf.cast(inputs.input_mask, tf.float32)\n      labelsf = tf.cast(labels, tf.float32)\n      losses = tf.nn.sigmoid_cross_entropy_with_logits(\n          logits=logits, labels=labelsf) * weights\n      per_example_loss = (tf.reduce_sum(losses, axis=-1) /\n                          (1e-6 + tf.reduce_sum(weights, axis=-1)))\n      loss = tf.reduce_sum(losses) / (1e-6 + tf.reduce_sum(weights))\n      probs = tf.nn.sigmoid(logits)\n      preds = tf.cast(tf.round((tf.sign(logits) + 1) / 2), tf.int32)\n      DiscOutput = collections.namedtuple(\n          ""DiscOutput"", [""loss"", ""per_example_loss"", ""probs"", ""preds"",\n                         ""labels""])\n      return DiscOutput(\n          loss=loss, per_example_loss=per_example_loss, probs=probs,\n          preds=preds, labels=labels,\n      )\n\n  def _get_fake_data(self, inputs, mlm_logits):\n    """"""Sample from the generator to create corrupted input.""""""\n    inputs = pretrain_helpers.unmask(inputs)\n    disallow = tf.one_hot(\n        inputs.masked_lm_ids, depth=self._bert_config.vocab_size,\n        dtype=tf.float32) if self._config.disallow_correct else None\n    sampled_tokens = tf.stop_gradient(pretrain_helpers.sample_from_softmax(\n        mlm_logits / self._config.temperature, disallow=disallow))\n    sampled_tokids = tf.argmax(sampled_tokens, -1, output_type=tf.int32)\n    updated_input_ids, masked = pretrain_helpers.scatter_update(\n        inputs.input_ids, sampled_tokids, inputs.masked_lm_positions)\n    labels = masked * (1 - tf.cast(\n        tf.equal(updated_input_ids, inputs.input_ids), tf.int32))\n    updated_inputs = pretrain_data.get_updated_inputs(\n        inputs, input_ids=updated_input_ids)\n    FakedData = collections.namedtuple(""FakedData"", [\n        ""inputs"", ""is_fake_tokens"", ""sampled_tokens""])\n    return FakedData(inputs=updated_inputs, is_fake_tokens=labels,\n                     sampled_tokens=sampled_tokens)\n\n  def _build_transformer(self, inputs: pretrain_data.Inputs, is_training,\n                         bert_config=None, name=""electra"", reuse=False, **kwargs):\n    """"""Build a transformer encoder network.""""""\n    if bert_config is None:\n      bert_config = self._bert_config\n    with tf.variable_scope(tf.get_variable_scope(), reuse=reuse):\n      return modeling.BertModel(\n          bert_config=bert_config,\n          is_training=is_training,\n          input_ids=inputs.input_ids,\n          input_mask=inputs.input_mask,\n          token_type_ids=inputs.segment_ids,\n          use_one_hot_embeddings=self._config.use_tpu,\n          scope=name,\n          **kwargs)\n\n\ndef get_generator_config(config: configure_pretraining.PretrainingConfig,\n                         bert_config: modeling.BertConfig):\n  """"""Get model config for the generator network.""""""\n  gen_config = modeling.BertConfig.from_dict(bert_config.to_dict())\n  gen_config.hidden_size = int(round(\n      bert_config.hidden_size * config.generator_hidden_size))\n  gen_config.num_hidden_layers = int(round(\n      bert_config.num_hidden_layers * config.generator_layers))\n  gen_config.intermediate_size = 4 * gen_config.hidden_size\n  gen_config.num_attention_heads = max(1, gen_config.hidden_size // 64)\n  return gen_config\n\n\ndef model_fn_builder(config: configure_pretraining.PretrainingConfig):\n  """"""Build the model for training.""""""\n\n  def model_fn(features, labels, mode, params):\n    """"""Build the model for training.""""""\n    model = PretrainingModel(config, features,\n                             mode == tf.estimator.ModeKeys.TRAIN)\n    utils.log(""Model is built!"")\n    if mode == tf.estimator.ModeKeys.TRAIN:\n      train_op = optimization.create_optimizer(\n          model.total_loss, config.learning_rate, config.num_train_steps,\n          weight_decay_rate=config.weight_decay_rate,\n          use_tpu=config.use_tpu,\n          warmup_steps=config.num_warmup_steps,\n          lr_decay_power=config.lr_decay_power\n      )\n      output_spec = tf.estimator.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=model.total_loss,\n          train_op=train_op,\n          training_hooks=[training_utils.ETAHook(\n              {} if config.use_tpu else dict(loss=model.total_loss),\n              config.num_train_steps, config.iterations_per_loop,\n              config.use_tpu)]\n      )\n    elif mode == tf.estimator.ModeKeys.EVAL:\n      output_spec = tf.estimator.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=model.total_loss,\n          eval_metrics=model.eval_metrics,\n          evaluation_hooks=[training_utils.ETAHook(\n              {} if config.use_tpu else dict(loss=model.total_loss),\n              config.num_eval_steps, config.iterations_per_loop,\n              config.use_tpu, is_training=False)])\n    else:\n      raise ValueError(""Only TRAIN and EVAL modes are supported"")\n    return output_spec\n\n  return model_fn\n\n\ndef train_or_eval(config: configure_pretraining.PretrainingConfig):\n  """"""Run pre-training or evaluate the pre-trained model.""""""\n  if config.do_train == config.do_eval:\n    raise ValueError(""Exactly one of `do_train` or `do_eval` must be True."")\n  if config.debug:\n    utils.rmkdir(config.model_dir)\n  utils.heading(""Config:"")\n  utils.log_config(config)\n\n  is_per_host = tf.estimator.tpu.InputPipelineConfig.PER_HOST_V2\n  tpu_cluster_resolver = None\n  if config.use_tpu and config.tpu_name:\n    tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n        config.tpu_name, zone=config.tpu_zone, project=config.gcp_project)\n  tpu_config = tf.estimator.tpu.TPUConfig(\n      iterations_per_loop=config.iterations_per_loop,\n      num_shards=(config.num_tpu_cores if config.do_train else\n                  config.num_tpu_cores),\n      tpu_job_name=config.tpu_job_name,\n      per_host_input_for_training=is_per_host)\n  run_config = tf.estimator.tpu.RunConfig(\n      cluster=tpu_cluster_resolver,\n      model_dir=config.model_dir,\n      save_checkpoints_steps=config.save_checkpoints_steps,\n      tpu_config=tpu_config)\n  model_fn = model_fn_builder(config=config)\n  estimator = tf.estimator.tpu.TPUEstimator(\n      use_tpu=config.use_tpu,\n      model_fn=model_fn,\n      config=run_config,\n      train_batch_size=config.train_batch_size,\n      eval_batch_size=config.eval_batch_size)\n\n  if config.do_train:\n    utils.heading(""Running training"")\n    estimator.train(input_fn=pretrain_data.get_input_fn(config, True),\n                    max_steps=config.num_train_steps)\n  if config.do_eval:\n    utils.heading(""Running evaluation"")\n    result = estimator.evaluate(\n        input_fn=pretrain_data.get_input_fn(config, False),\n        steps=config.num_eval_steps)\n    for key in sorted(result.keys()):\n      utils.log(""  {:} = {:}"".format(key, str(result[key])))\n    return result\n\n\ndef train_one_step(config: configure_pretraining.PretrainingConfig):\n  """"""Builds an ELECTRA model an trains it for one step; useful for debugging.""""""\n  train_input_fn = pretrain_data.get_input_fn(config, True)\n  features = tf.data.make_one_shot_iterator(train_input_fn(dict(\n      batch_size=config.train_batch_size))).get_next()\n  model = PretrainingModel(config, features, True)\n  with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    utils.log(sess.run(model.total_loss))\n\n\ndef main():\n  parser = argparse.ArgumentParser(description=__doc__)\n  parser.add_argument(""--data-dir"", required=True,\n                      help=""Location of data files (model weights, etc)."")\n  parser.add_argument(""--model-name"", required=True,\n                      help=""The name of the model being fine-tuned."")\n  parser.add_argument(""--hparams"", default=""{}"",\n                      help=""JSON dict of model hyperparameters."")\n  args = parser.parse_args()\n  if args.hparams.endswith("".json""):\n    hparams = utils.load_json(args.hparams)\n  else:\n    hparams = json.loads(args.hparams)\n  tf.logging.set_verbosity(tf.logging.ERROR)\n  train_or_eval(configure_pretraining.PretrainingConfig(\n      args.model_name, args.data_dir, **hparams))\n\n\nif __name__ == ""__main__"":\n  main()\n'"
finetune/__init__.py,0,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'"
finetune/feature_spec.py,2,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Defines the inputs used when fine-tuning a model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nimport configure_finetuning\n\n\ndef get_shared_feature_specs(config: configure_finetuning.FinetuningConfig):\n  """"""Non-task-specific model inputs.""""""\n  return [\n      FeatureSpec(""input_ids"", [config.max_seq_length]),\n      FeatureSpec(""input_mask"", [config.max_seq_length]),\n      FeatureSpec(""segment_ids"", [config.max_seq_length]),\n      FeatureSpec(""task_id"", []),\n  ]\n\n\nclass FeatureSpec(object):\n  """"""Defines a feature passed as input to the model.""""""\n\n  def __init__(self, name, shape, default_value_fn=None, is_int_feature=True):\n    self.name = name\n    self.shape = shape\n    self.default_value_fn = default_value_fn\n    self.is_int_feature = is_int_feature\n\n  def get_parsing_spec(self):\n    return tf.io.FixedLenFeature(\n        self.shape, tf.int64 if self.is_int_feature else tf.float32)\n\n  def get_default_values(self):\n    if self.default_value_fn:\n      return self.default_value_fn(self.shape)\n    else:\n      return np.zeros(\n          self.shape, np.int64 if self.is_int_feature else np.float32)\n'"
finetune/preprocessing.py,12,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Code for serializing raw fine-tuning data into tfrecords""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport os\nimport random\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nimport configure_finetuning\nfrom finetune import feature_spec\nfrom util import utils\n\n\nclass Preprocessor(object):\n  """"""Class for loading, preprocessing, and serializing fine-tuning datasets.""""""\n\n  def __init__(self, config: configure_finetuning.FinetuningConfig, tasks):\n    self._config = config\n    self._tasks = tasks\n    self._name_to_task = {task.name: task for task in tasks}\n\n    self._feature_specs = feature_spec.get_shared_feature_specs(config)\n    for task in tasks:\n      self._feature_specs += task.get_feature_specs()\n    self._name_to_feature_config = {\n        spec.name: spec.get_parsing_spec()\n        for spec in self._feature_specs\n    }\n    assert len(self._name_to_feature_config) == len(self._feature_specs)\n\n  def prepare_train(self):\n    return self._serialize_dataset(self._tasks, True, ""train"")\n\n  def prepare_predict(self, tasks, split):\n    return self._serialize_dataset(tasks, False, split)\n\n  def _serialize_dataset(self, tasks, is_training, split):\n    """"""Write out the dataset as tfrecords.""""""\n    dataset_name = ""_"".join(sorted([task.name for task in tasks]))\n    dataset_name += ""_"" + split\n    dataset_prefix = os.path.join(\n        self._config.preprocessed_data_dir, dataset_name)\n    tfrecords_path = dataset_prefix + "".tfrecord""\n    metadata_path = dataset_prefix + "".metadata""\n    batch_size = (self._config.train_batch_size if is_training else\n                  self._config.eval_batch_size)\n\n    utils.log(""Loading dataset"", dataset_name)\n    n_examples = None\n    if (self._config.use_tfrecords_if_existing and\n        tf.io.gfile.exists(metadata_path)):\n      n_examples = utils.load_json(metadata_path)[""n_examples""]\n\n    if n_examples is None:\n      utils.log(""Existing tfrecords not found so creating"")\n      examples = []\n      for task in tasks:\n        task_examples = task.get_examples(split)\n        examples += task_examples\n      if is_training:\n        random.shuffle(examples)\n      utils.mkdir(tfrecords_path.rsplit(""/"", 1)[0])\n      n_examples = self.serialize_examples(\n          examples, is_training, tfrecords_path, batch_size)\n      utils.write_json({""n_examples"": n_examples}, metadata_path)\n\n    input_fn = self._input_fn_builder(tfrecords_path, is_training)\n    if is_training:\n      steps = int(n_examples // batch_size * self._config.num_train_epochs)\n    else:\n      steps = n_examples // batch_size\n\n    return input_fn, steps\n\n  def serialize_examples(self, examples, is_training, output_file, batch_size):\n    """"""Convert a set of `InputExample`s to a TFRecord file.""""""\n    n_examples = 0\n    with tf.io.TFRecordWriter(output_file) as writer:\n      for (ex_index, example) in enumerate(examples):\n        if ex_index % 2000 == 0:\n          utils.log(""Writing example {:} of {:}"".format(\n              ex_index, len(examples)))\n        for tf_example in self._example_to_tf_example(\n            example, is_training,\n            log=self._config.log_examples and ex_index < 1):\n          writer.write(tf_example.SerializeToString())\n          n_examples += 1\n      # add padding so the dataset is a multiple of batch_size\n      while n_examples % batch_size != 0:\n        writer.write(self._make_tf_example(task_id=len(self._config.task_names))\n                     .SerializeToString())\n        n_examples += 1\n    return n_examples\n\n  def _example_to_tf_example(self, example, is_training, log=False):\n    examples = self._name_to_task[example.task_name].featurize(\n        example, is_training, log)\n    if not isinstance(examples, list):\n      examples = [examples]\n    for example in examples:\n      yield self._make_tf_example(**example)\n\n  def _make_tf_example(self, **kwargs):\n    """"""Make a tf.train.Example from the provided features.""""""\n    for k in kwargs:\n      if k not in self._name_to_feature_config:\n        raise ValueError(""Unknown feature"", k)\n    features = collections.OrderedDict()\n    for spec in self._feature_specs:\n      if spec.name in kwargs:\n        values = kwargs[spec.name]\n      else:\n        values = spec.get_default_values()\n      if (isinstance(values, int) or isinstance(values, bool) or\n          isinstance(values, float) or isinstance(values, np.float32) or\n          (isinstance(values, np.ndarray) and values.size == 1)):\n        values = [values]\n      if spec.is_int_feature:\n        feature = tf.train.Feature(int64_list=tf.train.Int64List(\n            value=list(values)))\n      else:\n        feature = tf.train.Feature(float_list=tf.train.FloatList(\n            value=list(values)))\n      features[spec.name] = feature\n    return tf.train.Example(features=tf.train.Features(feature=features))\n\n  def _input_fn_builder(self, input_file, is_training):\n    """"""Creates an `input_fn` closure to be passed to TPUEstimator.""""""\n\n    def input_fn(params):\n      """"""The actual input function.""""""\n      d = tf.data.TFRecordDataset(input_file)\n      if is_training:\n        d = d.repeat()\n        d = d.shuffle(buffer_size=100)\n      return d.apply(\n          tf.data.experimental.map_and_batch(\n              self._decode_tfrecord,\n              batch_size=params[""batch_size""],\n              drop_remainder=True))\n\n    return input_fn\n\n  def _decode_tfrecord(self, record):\n    """"""Decodes a record to a TensorFlow example.""""""\n    example = tf.io.parse_single_example(record, self._name_to_feature_config)\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name, tensor in example.items():\n      if tensor.dtype == tf.int64:\n        example[name] = tf.cast(tensor, tf.int32)\n      else:\n        example[name] = tensor\n    return example\n'"
finetune/scorer.py,0,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Base class for evaluation metrics.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\n\n\nclass Scorer(object):\n  """"""Abstract base class for computing evaluation metrics.""""""\n\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(self):\n    self._updated = False\n    self._cached_results = {}\n\n  @abc.abstractmethod\n  def update(self, results):\n    self._updated = True\n\n  @abc.abstractmethod\n  def get_loss(self):\n    pass\n\n  @abc.abstractmethod\n  def _get_results(self):\n    return []\n\n  def get_results(self, prefix=""""):\n    results = self._get_results() if self._updated else self._cached_results\n    self._cached_results = results\n    self._updated = False\n    return [(prefix + k, v) for k, v in results]\n\n  def results_str(self):\n    return "" - "".join([""{:}: {:.2f}"".format(k, v)\n                       for k, v in self.get_results()])\n'"
finetune/task.py,0,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Defines a supervised NLP task.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nfrom typing import List, Tuple\n\nimport configure_finetuning\nfrom finetune import feature_spec\nfrom finetune import scorer\nfrom model import modeling\n\n\nclass Example(object):\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(self, task_name):\n    self.task_name = task_name\n\n\nclass Task(object):\n  """"""Override this class to add a new fine-tuning task.""""""\n\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(self, config: configure_finetuning.FinetuningConfig, name):\n    self.config = config\n    self.name = name\n\n  def get_test_splits(self):\n    return [""test""]\n\n  @abc.abstractmethod\n  def get_examples(self, split):\n    pass\n\n  @abc.abstractmethod\n  def get_scorer(self) -> scorer.Scorer:\n    pass\n\n  @abc.abstractmethod\n  def get_feature_specs(self) -> List[feature_spec.FeatureSpec]:\n    pass\n\n  @abc.abstractmethod\n  def featurize(self, example: Example, is_training: bool,\n                log: bool=False):\n    pass\n\n  @abc.abstractmethod\n  def get_prediction_module(\n      self, bert_model: modeling.BertModel, features: dict, is_training: bool,\n      percent_done: float) -> Tuple:\n    pass\n\n  def __repr__(self):\n    return ""Task("" + self.name + "")""\n'"
finetune/task_builder.py,0,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Returns task instances given the task name.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport configure_finetuning\nfrom finetune.classification import classification_tasks\nfrom finetune.qa import qa_tasks\nfrom finetune.tagging import tagging_tasks\nfrom model import tokenization\n\n\ndef get_tasks(config: configure_finetuning.FinetuningConfig):\n  tokenizer = tokenization.FullTokenizer(vocab_file=config.vocab_file,\n                                         do_lower_case=config.do_lower_case)\n  return [get_task(config, task_name, tokenizer)\n          for task_name in config.task_names]\n\n\ndef get_task(config: configure_finetuning.FinetuningConfig, task_name,\n             tokenizer):\n  """"""Get an instance of a task based on its name.""""""\n  if task_name == ""cola"":\n    return classification_tasks.CoLA(config, tokenizer)\n  elif task_name == ""mrpc"":\n    return classification_tasks.MRPC(config, tokenizer)\n  elif task_name == ""mnli"":\n    return classification_tasks.MNLI(config, tokenizer)\n  elif task_name == ""sst"":\n    return classification_tasks.SST(config, tokenizer)\n  elif task_name == ""rte"":\n    return classification_tasks.RTE(config, tokenizer)\n  elif task_name == ""qnli"":\n    return classification_tasks.QNLI(config, tokenizer)\n  elif task_name == ""qqp"":\n    return classification_tasks.QQP(config, tokenizer)\n  elif task_name == ""sts"":\n    return classification_tasks.STS(config, tokenizer)\n  elif task_name == ""squad"":\n    return qa_tasks.SQuAD(config, tokenizer)\n  elif task_name == ""squadv1"":\n    return qa_tasks.SQuADv1(config, tokenizer)\n  elif task_name == ""newsqa"":\n    return qa_tasks.NewsQA(config, tokenizer)\n  elif task_name == ""naturalqs"":\n    return qa_tasks.NaturalQuestions(config, tokenizer)\n  elif task_name == ""triviaqa"":\n    return qa_tasks.TriviaQA(config, tokenizer)\n  elif task_name == ""searchqa"":\n    return qa_tasks.SearchQA(config, tokenizer)\n  elif task_name == ""chunk"":\n    return tagging_tasks.Chunking(config, tokenizer)\n  elif task_name == ""bqcorpus"":\n    return classification_tasks.BQCorpus(config, tokenizer)\n  elif task_name == ""chnsenticorp"":\n    return classification_tasks.ChnSentiCorp(config, tokenizer)\n  elif task_name == ""xnli"":\n    return classification_tasks.XNLI(config, tokenizer)\n  elif task_name == ""lcqmc"":\n    return classification_tasks.LCQMC(config, tokenizer)\n  elif task_name == ""cmrc2018"":\n    return qa_tasks.CMRC2018(config, tokenizer)\n  elif task_name == ""drcd"":\n    return qa_tasks.DRCD(config, tokenizer)   \n  else:\n    raise ValueError(""Unknown task "" + task_name)\n'"
model/__init__.py,0,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'"
model/modeling.py,84,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""The transformer encoder used by ELECTRA. Essentially BERT\'s with a few\nadditional functionalities added.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport copy\nimport json\nimport math\nimport re\n\nimport numpy as np\nimport six\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.contrib import layers as contrib_layers\n\n\nclass BertConfig(object):\n  """"""Configuration for `BertModel` (ELECTRA uses the same model as BERT).""""""\n\n  def __init__(self,\n               vocab_size,\n               hidden_size=768,\n               num_hidden_layers=12,\n               num_attention_heads=12,\n               intermediate_size=3072,\n               hidden_act=""gelu"",\n               hidden_dropout_prob=0.1,\n               attention_probs_dropout_prob=0.1,\n               max_position_embeddings=512,\n               type_vocab_size=2,\n               initializer_range=0.02):\n    """"""Constructs BertConfig.\n\n    Args:\n      vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n      hidden_size: Size of the encoder layers and the pooler layer.\n      num_hidden_layers: Number of hidden layers in the Transformer encoder.\n      num_attention_heads: Number of attention heads for each attention layer in\n        the Transformer encoder.\n      intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)\n        layer in the Transformer encoder.\n      hidden_act: The non-linear activation function (function or string) in the\n        encoder and pooler.\n      hidden_dropout_prob: The dropout probability for all fully connected\n        layers in the embeddings, encoder, and pooler.\n      attention_probs_dropout_prob: The dropout ratio for the attention\n        probabilities.\n      max_position_embeddings: The maximum sequence length that this model might\n        ever be used with. Typically set this to something large just in case\n        (e.g., 512 or 1024 or 2048).\n      type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n        `BertModel`.\n      initializer_range: The stdev of the truncated_normal_initializer for\n        initializing all weight matrices.\n    """"""\n    self.vocab_size = vocab_size\n    self.hidden_size = hidden_size\n    self.num_hidden_layers = num_hidden_layers\n    self.num_attention_heads = num_attention_heads\n    self.hidden_act = hidden_act\n    self.intermediate_size = intermediate_size\n    self.hidden_dropout_prob = hidden_dropout_prob\n    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n    self.max_position_embeddings = max_position_embeddings\n    self.type_vocab_size = type_vocab_size\n    self.initializer_range = initializer_range\n\n  @classmethod\n  def from_dict(cls, json_object):\n    """"""Constructs a `BertConfig` from a Python dictionary of parameters.""""""\n    config = BertConfig(vocab_size=None)\n    for (key, value) in six.iteritems(json_object):\n      config.__dict__[key] = value\n    return config\n\n  @classmethod\n  def from_json_file(cls, json_file):\n    """"""Constructs a `BertConfig` from a json file of parameters.""""""\n    with tf.io.gfile.GFile(json_file, ""r"") as reader:\n      text = reader.read()\n    return cls.from_dict(json.loads(text))\n\n  def to_dict(self):\n    """"""Serializes this instance to a Python dictionary.""""""\n    output = copy.deepcopy(self.__dict__)\n    return output\n\n  def to_json_string(self):\n    """"""Serializes this instance to a JSON string.""""""\n    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n\nclass BertModel(object):\n  """"""BERT model. Although the training algorithm is different, the transformer\n  model for ELECTRA is the same as BERT\'s.\n\n  Example usage:\n\n  ```python\n  # Already been converted into WordPiece token ids\n  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n\n  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n\n  model = modeling.BertModel(config=config, is_training=True,\n    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n\n  label_embeddings = tf.get_variable(...)\n  pooled_output = model.get_pooled_output()\n  logits = tf.matmul(pooled_output, label_embeddings)\n  ...\n  ```\n  """"""\n\n  def __init__(self,\n               bert_config,\n               is_training,\n               input_ids,\n               input_mask=None,\n               token_type_ids=None,\n               use_one_hot_embeddings=True,\n               scope=None,\n               embedding_size=None,\n               input_embeddings=None,\n               input_reprs=None,\n               update_embeddings=True,\n               untied_embeddings=False):\n    """"""Constructor for BertModel.\n\n    Args:\n      bert_config: `BertConfig` instance.\n      is_training: bool. true for training model, false for eval model. Controls\n        whether dropout will be applied.\n      input_ids: int32 Tensor of shape [batch_size, seq_length].\n      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n        embeddings or tf.embedding_lookup() for the word embeddings. On the TPU,\n        it is much faster if this is True, on the CPU or GPU, it is faster if\n        this is False.\n      scope: (optional) variable scope. Defaults to ""electra"".\n\n    Raises:\n      ValueError: The config is invalid or one of the input tensor shapes\n        is invalid.\n    """"""\n    bert_config = copy.deepcopy(bert_config)\n    if not is_training:\n      bert_config.hidden_dropout_prob = 0.0\n      bert_config.attention_probs_dropout_prob = 0.0\n\n    input_shape = get_shape_list(token_type_ids, expected_rank=2)\n    batch_size = input_shape[0]\n    seq_length = input_shape[1]\n\n    if input_mask is None:\n      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    assert token_type_ids is not None\n\n    if input_reprs is None:\n      if input_embeddings is None:\n        with tf.variable_scope(\n            (scope if untied_embeddings else ""electra"") + ""/embeddings"",\n            reuse=tf.AUTO_REUSE):\n          # Perform embedding lookup on the word ids\n          if embedding_size is None:\n            embedding_size = bert_config.hidden_size\n          (self.token_embeddings, self.embedding_table) = embedding_lookup(\n              input_ids=input_ids,\n              vocab_size=bert_config.vocab_size,\n              embedding_size=embedding_size,\n              initializer_range=bert_config.initializer_range,\n              word_embedding_name=""word_embeddings"",\n              use_one_hot_embeddings=use_one_hot_embeddings)\n      else:\n        self.token_embeddings = input_embeddings\n\n      with tf.variable_scope(\n          (scope if untied_embeddings else ""electra"") + ""/embeddings"",\n          reuse=tf.AUTO_REUSE):\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.token_embeddings,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=bert_config.type_vocab_size,\n            token_type_embedding_name=""token_type_embeddings"",\n            use_position_embeddings=True,\n            position_embedding_name=""position_embeddings"",\n            initializer_range=bert_config.initializer_range,\n            max_position_embeddings=bert_config.max_position_embeddings,\n            dropout_prob=bert_config.hidden_dropout_prob)\n    else:\n      self.embedding_output = input_reprs\n    if not update_embeddings:\n      self.embedding_output = tf.stop_gradient(self.embedding_output)\n\n    with tf.variable_scope(scope, default_name=""electra""):\n      if self.embedding_output.shape[-1] != bert_config.hidden_size:\n        self.embedding_output = tf.layers.dense(\n            self.embedding_output, bert_config.hidden_size,\n            name=""embeddings_project"")\n\n      with tf.variable_scope(""encoder""):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask = create_attention_mask_from_input_mask(\n            token_type_ids, input_mask)\n\n        # Run the stacked transformer. Output shapes\n        # sequence_output: [batch_size, seq_length, hidden_size]\n        # pooled_output: [batch_size, hidden_size]\n        # all_encoder_layers: [n_layers, batch_size, seq_length, hidden_size].\n        # attn_maps: [n_layers, batch_size, n_heads, seq_length, seq_length]\n        (self.all_layer_outputs, self.attn_maps) = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=bert_config.hidden_size,\n            num_hidden_layers=bert_config.num_hidden_layers,\n            num_attention_heads=bert_config.num_attention_heads,\n            intermediate_size=bert_config.intermediate_size,\n            intermediate_act_fn=get_activation(bert_config.hidden_act),\n            hidden_dropout_prob=bert_config.hidden_dropout_prob,\n            attention_probs_dropout_prob=\n            bert_config.attention_probs_dropout_prob,\n            initializer_range=bert_config.initializer_range,\n            do_return_all_layers=True)\n        self.sequence_output = self.all_layer_outputs[-1]\n        self.pooled_output = self.sequence_output[:, 0]\n\n  def get_pooled_output(self):\n    return self.pooled_output\n\n  def get_sequence_output(self):\n    """"""Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    """"""\n    return self.sequence_output\n\n  def get_all_encoder_layers(self):\n    return self.all_layer_outputs\n\n  def get_embedding_output(self):\n    """"""Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    """"""\n    return self.embedding_output\n\n  def get_embedding_table(self):\n    return self.embedding_table\n\n\ndef gelu(input_tensor):\n  """"""Gaussian Error Linear Unit.\n\n  This is a smoother version of the RELU.\n  Original paper: https://arxiv.org/abs/1606.08415\n\n  Args:\n    input_tensor: float Tensor to perform activation.\n\n  Returns:\n    `input_tensor` with the GELU activation applied.\n  """"""\n  cdf = 0.5 * (1.0 + tf.math.erf(input_tensor / tf.sqrt(2.0)))\n  return input_tensor * cdf\n\n\ndef get_activation(activation_string):\n  """"""Maps a string to a Python function, e.g., ""relu"" => `tf.nn.relu`.\n\n  Args:\n    activation_string: String name of the activation function.\n\n  Returns:\n    A Python function corresponding to the activation function. If\n    `activation_string` is None, empty, or ""linear"", this will return None.\n    If `activation_string` is not a string, it will return `activation_string`.\n\n  Raises:\n    ValueError: The `activation_string` does not correspond to a known\n      activation.\n  """"""\n\n  # We assume that anything that""s not a string is already an activation\n  # function, so we just return it.\n  if not isinstance(activation_string, six.string_types):\n    return activation_string\n\n  if not activation_string:\n    return None\n\n  act = activation_string.lower()\n  if act == ""linear"":\n    return None\n  elif act == ""relu"":\n    return tf.nn.relu\n  elif act == ""gelu"":\n    return gelu\n  elif act == ""tanh"":\n    return tf.tanh\n  else:\n    raise ValueError(""Unsupported activation: %s"" % act)\n\n\ndef get_assignment_map_from_checkpoint(tvars, init_checkpoint, prefix=""""):\n  """"""Compute the union of the current variables and checkpoint variables.""""""\n  name_to_variable = collections.OrderedDict()\n  for var in tvars:\n    name = var.name\n    m = re.match(""^(.*):\\\\d+$"", name)\n    if m is not None:\n      name = m.group(1)\n    name_to_variable[name] = var\n\n  initialized_variable_names = {}\n  assignment_map = collections.OrderedDict()\n  for x in tf.train.list_variables(init_checkpoint):\n    (name, var) = (x[0], x[1])\n    if prefix + name not in name_to_variable:\n      continue\n    assignment_map[name] = prefix + name\n    initialized_variable_names[name] = 1\n    initialized_variable_names[name + "":0""] = 1\n\n  return assignment_map, initialized_variable_names\n\n\ndef dropout(input_tensor, dropout_prob):\n  """"""Perform dropout.\n\n  Args:\n    input_tensor: float Tensor.\n    dropout_prob: Python float. The probability of dropping out a value (NOT of\n      *keeping* a dimension as in `tf.nn.dropout`).\n\n  Returns:\n    A version of `input_tensor` with dropout applied.\n  """"""\n  if dropout_prob is None or dropout_prob == 0.0:\n    return input_tensor\n\n  output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n  return output\n\n\ndef layer_norm(input_tensor, name=None):\n  """"""Run layer normalization on the last dimension of the tensor.""""""\n  return contrib_layers.layer_norm(\n      inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)\n\n\ndef layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\n  """"""Runs layer normalization followed by dropout.""""""\n  output_tensor = layer_norm(input_tensor, name)\n  output_tensor = dropout(output_tensor, dropout_prob)\n  return output_tensor\n\n\ndef create_initializer(initializer_range=0.02):\n  """"""Creates a `truncated_normal_initializer` with the given range.""""""\n  return tf.truncated_normal_initializer(stddev=initializer_range)\n\n\ndef embedding_lookup(input_ids,\n                     vocab_size,\n                     embedding_size=128,\n                     initializer_range=0.02,\n                     word_embedding_name=""word_embeddings"",\n                     use_one_hot_embeddings=False):\n  """"""Looks up words embeddings for id tensor.\n\n  Args:\n    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n      ids.\n    vocab_size: int. Size of the embedding vocabulary.\n    embedding_size: int. Width of the word embeddings.\n    initializer_range: float. Embedding initialization range.\n    word_embedding_name: string. Name of the embedding table.\n    use_one_hot_embeddings: bool. If True, use one-hot method for word\n      embeddings. If False, use `tf.nn.embedding_lookup()`. One hot is better\n      for TPUs.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, embedding_size].\n  """"""\n  # This function assumes that the input is of shape [batch_size, seq_length,\n  # num_inputs].\n  #\n  # If the input is a 2D tensor of shape [batch_size, seq_length], we\n  # reshape to [batch_size, seq_length, 1].\n  original_dims = input_ids.shape.ndims\n  if original_dims == 2:\n    input_ids = tf.expand_dims(input_ids, axis=[-1])\n\n  embedding_table = tf.get_variable(\n      name=word_embedding_name,\n      shape=[vocab_size, embedding_size],\n      initializer=create_initializer(initializer_range))\n\n  if original_dims == 3:\n    input_shape = get_shape_list(input_ids)\n    tf.reshape(input_ids, [-1, input_shape[-1]])\n    output = tf.matmul(input_ids, embedding_table)\n    output = tf.reshape(output,\n                        [input_shape[0], input_shape[1], embedding_size])\n  else:\n    if use_one_hot_embeddings:\n      flat_input_ids = tf.reshape(input_ids, [-1])\n      one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n      output = tf.matmul(one_hot_input_ids, embedding_table)\n    else:\n      output = tf.nn.embedding_lookup(embedding_table, input_ids)\n\n    input_shape = get_shape_list(input_ids)\n\n    output = tf.reshape(output,\n                        input_shape[0:-1] + [input_shape[-1] * embedding_size])\n  return output, embedding_table\n\n\ndef embedding_postprocessor(input_tensor,\n                            use_token_type=False,\n                            token_type_ids=None,\n                            token_type_vocab_size=16,\n                            token_type_embedding_name=""token_type_embeddings"",\n                            use_position_embeddings=True,\n                            position_embedding_name=""position_embeddings"",\n                            initializer_range=0.02,\n                            max_position_embeddings=512,\n                            dropout_prob=0.1):\n  """"""Performs various post-processing on a word embedding tensor.\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length,\n      embedding_size].\n    use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      Must be specified if `use_token_type` is True.\n    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.\n    token_type_embedding_name: string. The name of the embedding table variable\n      for token type ids.\n    use_position_embeddings: bool. Whether to add position embeddings for the\n      position of each token in the sequence.\n    position_embedding_name: string. The name of the embedding table variable\n      for positional embeddings.\n    initializer_range: float. Range of the weight initialization.\n    max_position_embeddings: int. Maximum sequence length that might ever be\n      used with this model. This can be longer than the sequence length of\n      input_tensor, but cannot be shorter.\n    dropout_prob: float. Dropout probability applied to the final output tensor.\n\n  Returns:\n    float tensor with same shape as `input_tensor`.\n\n  Raises:\n    ValueError: One of the tensor shapes or input values is invalid.\n  """"""\n  input_shape = get_shape_list(input_tensor, expected_rank=3)\n  batch_size = input_shape[0]\n  seq_length = input_shape[1]\n  width = input_shape[2]\n\n  output = input_tensor\n\n  if use_token_type:\n    if token_type_ids is None:\n      raise ValueError(""`token_type_ids` must be specified if""\n                       ""`use_token_type` is True."")\n    token_type_table = tf.get_variable(\n        name=token_type_embedding_name,\n        shape=[token_type_vocab_size, width],\n        initializer=create_initializer(initializer_range))\n    # This vocab will be small so we always do one-hot here, since it is always\n    # faster for a small vocabulary.\n    flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)\n    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n    token_type_embeddings = tf.reshape(token_type_embeddings,\n                                       [batch_size, seq_length, width])\n    output += token_type_embeddings\n\n  if use_position_embeddings:\n    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)\n    with tf.control_dependencies([assert_op]):\n      full_position_embeddings = tf.get_variable(\n          name=position_embedding_name,\n          shape=[max_position_embeddings, width],\n          initializer=create_initializer(initializer_range))\n      # Since the position embedding table is a learned variable, we create it\n      # using a (long) sequence length `max_position_embeddings`. The actual\n      # sequence length might be shorter than this, for faster training of\n      # tasks that do not have long sequences.\n      #\n      # So `full_position_embeddings` is effectively an embedding table\n      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n      # perform a slice.\n      position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n                                     [seq_length, -1])\n      num_dims = len(output.shape.as_list())\n\n      # Only the last two dimensions are relevant (`seq_length` and `width`), so\n      # we broadcast among the first dimensions, which is typically just\n      # the batch size.\n      position_broadcast_shape = []\n      for _ in range(num_dims - 2):\n        position_broadcast_shape.append(1)\n      position_broadcast_shape.extend([seq_length, width])\n      position_embeddings = tf.reshape(position_embeddings,\n                                       position_broadcast_shape)\n      output += position_embeddings\n\n  output = layer_norm_and_dropout(output, dropout_prob)\n  return output\n\n\ndef create_attention_mask_from_input_mask(from_tensor, to_mask):\n  """"""Create 3D attention mask from a 2D tensor mask.\n\n  Args:\n    from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n    to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n  """"""\n  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n  batch_size = from_shape[0]\n  from_seq_length = from_shape[1]\n\n  to_shape = get_shape_list(to_mask, expected_rank=2)\n  to_seq_length = to_shape[1]\n\n  to_mask = tf.cast(\n      tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n  # We don\'t assume that `from_tensor` is a mask (although it could be). We\n  # don\'t actually care if we attend *from* padding tokens (only *to* padding)\n  # tokens so we create a tensor of all ones.\n  #\n  # `broadcast_ones` = [batch_size, from_seq_length, 1]\n  broadcast_ones = tf.ones(\n      shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n  # Here we broadcast along two dimensions to create the mask.\n  mask = broadcast_ones * to_mask\n\n  return mask\n\n\ndef attention_layer(from_tensor,\n                    to_tensor,\n                    attention_mask=None,\n                    num_attention_heads=1,\n                    size_per_head=512,\n                    query_act=None,\n                    key_act=None,\n                    value_act=None,\n                    attention_probs_dropout_prob=0.0,\n                    initializer_range=0.02,\n                    do_return_2d_tensor=False,\n                    batch_size=None,\n                    from_seq_length=None,\n                    to_seq_length=None):\n  """"""Performs multi-headed attention from `from_tensor` to `to_tensor`.\n\n  This is an implementation of multi-headed attention based on ""Attention\n  is all you Need"". If `from_tensor` and `to_tensor` are the same, then\n  this is self-attention. Each timestep in `from_tensor` attends to the\n  corresponding sequence in `to_tensor`, and returns a fixed-with vector.\n\n  This function first projects `from_tensor` into a ""query"" tensor and\n  `to_tensor` into ""key"" and ""value"" tensors. These are (effectively) a list\n  of tensors of length `num_attention_heads`, where each tensor is of shape\n  [batch_size, seq_length, size_per_head].\n\n  Then, the query and key tensors are dot-producted and scaled. These are\n  softmaxed to obtain attention probabilities. The value tensors are then\n  interpolated by these probabilities, then concatenated back to a single\n  tensor and returned.\n\n  In practice, the multi-headed attention are done with transposes and\n  reshapes rather than actual separate tensors.\n\n  Args:\n    from_tensor: float Tensor of shape [batch_size, from_seq_length,\n      from_width].\n    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n    attention_mask: (optional) int32 Tensor of shape [batch_size,\n      from_seq_length, to_seq_length]. The values should be 1 or 0. The\n      attention scores will effectively be set to -infinity for any positions in\n      the mask that are 0, and will be unchanged for positions that are 1.\n    num_attention_heads: int. Number of attention heads.\n    size_per_head: int. Size of each attention head.\n    query_act: (optional) Activation function for the query transform.\n    key_act: (optional) Activation function for the key transform.\n    value_act: (optional) Activation function for the value transform.\n    attention_probs_dropout_prob: (optional) float. Dropout probability of the\n      attention probabilities.\n    initializer_range: float. Range of the weight initializer.\n    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size\n      * from_seq_length, num_attention_heads * size_per_head]. If False, the\n      output will be of shape [batch_size, from_seq_length, num_attention_heads\n      * size_per_head].\n    batch_size: (Optional) int. If the input is 2D, this might be the batch size\n      of the 3D version of the `from_tensor` and `to_tensor`.\n    from_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `from_tensor`.\n    to_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `to_tensor`.\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length,\n      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is\n      true, this will be of shape [batch_size * from_seq_length,\n      num_attention_heads * size_per_head]).\n\n  Raises:\n    ValueError: Any of the arguments or tensor shapes are invalid.\n  """"""\n\n  def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width):\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n  to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])\n\n  if len(from_shape) != len(to_shape):\n    raise ValueError(\n        ""The rank of `from_tensor` must match the rank of `to_tensor`."")\n\n  if len(from_shape) == 3:\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n    to_seq_length = to_shape[1]\n  elif len(from_shape) == 2:\n    if batch_size is None or from_seq_length is None or to_seq_length is None:\n      raise ValueError(\n          ""When passing in rank 2 tensors to attention_layer, the values ""\n          ""for `batch_size`, `from_seq_length`, and `to_seq_length` ""\n          ""must all be specified."")\n\n  # Scalar dimensions referenced here:\n  #   B = batch size (number of sequences)\n  #   F = `from_tensor` sequence length\n  #   T = `to_tensor` sequence length\n  #   N = `num_attention_heads`\n  #   H = `size_per_head`\n\n  from_tensor_2d = reshape_to_matrix(from_tensor)\n  to_tensor_2d = reshape_to_matrix(to_tensor)\n\n  # `query_layer` = [B*F, N*H]\n  query_layer = tf.layers.dense(\n      from_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=query_act,\n      name=""query"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  # `key_layer` = [B*T, N*H]\n  key_layer = tf.layers.dense(\n      to_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=key_act,\n      name=""key"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  # `value_layer` = [B*T, N*H]\n  value_layer = tf.layers.dense(\n      to_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=value_act,\n      name=""value"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  # `query_layer` = [B, N, F, H]\n  query_layer = transpose_for_scores(query_layer, batch_size,\n                                     num_attention_heads, from_seq_length,\n                                     size_per_head)\n\n  # `key_layer` = [B, N, T, H]\n  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,\n                                   to_seq_length, size_per_head)\n\n  # Take the dot product between ""query"" and ""key"" to get the raw\n  # attention scores.\n  # `attention_scores` = [B, N, F, T]\n  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n  attention_scores = tf.multiply(attention_scores,\n                                 1.0 / math.sqrt(float(size_per_head)))\n\n  if attention_mask is not None:\n    # `attention_mask` = [B, 1, F, T]\n    attention_mask = tf.expand_dims(attention_mask, axis=[1])\n\n    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n    # masked positions, this operation will create a tensor which is 0.0 for\n    # positions we want to attend and -10000.0 for masked positions.\n    adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n\n    # Since we are adding it to the raw scores before the softmax, this is\n    # effectively the same as removing these entirely.\n    attention_scores += adder\n\n  # Normalize the attention scores to probabilities.\n  # `attention_probs` = [B, N, F, T]\n  attention_probs = tf.nn.softmax(attention_scores)\n\n  # This is actually dropping out entire tokens to attend to, which might\n  # seem a bit unusual, but is taken from the original Transformer paper.\n  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)\n\n  # `value_layer` = [B, T, N, H]\n  value_layer = tf.reshape(\n      value_layer,\n      [batch_size, to_seq_length, num_attention_heads, size_per_head])\n\n  # `value_layer` = [B, N, T, H]\n  value_layer = tf.transpose(value_layer, [0, 2, 1, 3])\n\n  # `context_layer` = [B, N, F, H]\n  context_layer = tf.matmul(attention_probs, value_layer)\n\n  # `context_layer` = [B, F, N, H]\n  context_layer = tf.transpose(context_layer, [0, 2, 1, 3])\n\n  if do_return_2d_tensor:\n    # `context_layer` = [B*F, N*H]\n    context_layer = tf.reshape(\n        context_layer,\n        [batch_size * from_seq_length, num_attention_heads * size_per_head])\n  else:\n    # `context_layer` = [B, F, N*H]\n    context_layer = tf.reshape(\n        context_layer,\n        [batch_size, from_seq_length, num_attention_heads * size_per_head])\n\n  return context_layer, attention_probs\n\n\ndef transformer_model(input_tensor,\n                      attention_mask=None,\n                      hidden_size=768,\n                      num_hidden_layers=12,\n                      num_attention_heads=12,\n                      intermediate_size=3072,\n                      intermediate_act_fn=gelu,\n                      hidden_dropout_prob=0.1,\n                      attention_probs_dropout_prob=0.1,\n                      initializer_range=0.02,\n                      do_return_all_layers=False):\n  """"""Multi-headed, multi-layer Transformer from ""Attention is All You Need"".\n\n  This is almost an exact implementation of the original Transformer encoder.\n\n  See the original paper:\n  https://arxiv.org/abs/1706.03762\n\n  Also see:\n  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n      seq_length], with 1 for positions that can be attended to and 0 in\n      positions that should not be.\n    hidden_size: int. Hidden size of the Transformer.\n    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n    num_attention_heads: int. Number of attention heads in the Transformer.\n    intermediate_size: int. The size of the ""intermediate"" (a.k.a., feed\n      forward) layer.\n    intermediate_act_fn: function. The non-linear activation function to apply\n      to the output of the intermediate/feed-forward layer.\n    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n    attention_probs_dropout_prob: float. Dropout probability of the attention\n      probabilities.\n    initializer_range: float. Range of the initializer (stddev of truncated\n      normal).\n    do_return_all_layers: Whether to also return all layers or just the final\n      layer.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n    hidden layer of the Transformer.\n\n  Raises:\n    ValueError: A Tensor shape or parameter is invalid.\n  """"""\n  if hidden_size % num_attention_heads != 0:\n    raise ValueError(\n        ""The hidden size (%d) is not a multiple of the number of attention ""\n        ""heads (%d)"" % (hidden_size, num_attention_heads))\n\n  attention_head_size = int(hidden_size / num_attention_heads)\n  input_shape = get_shape_list(input_tensor, expected_rank=3)\n  batch_size = input_shape[0]\n  seq_length = input_shape[1]\n  input_width = input_shape[2]\n\n  # The Transformer performs sum residuals on all layers so the input needs\n  # to be the same as the hidden size.\n  if input_width != hidden_size:\n    raise ValueError(""The width of the input tensor (%d) != hidden size (%d)"" %\n                     (input_width, hidden_size))\n\n  # We keep the representation as a 2D tensor to avoid re-shaping it back and\n  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on\n  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to\n  # help the optimizer.\n  prev_output = reshape_to_matrix(input_tensor)\n\n  attn_maps = []\n  all_layer_outputs = []\n  for layer_idx in range(num_hidden_layers):\n    with tf.variable_scope(""layer_%d"" % layer_idx):\n      with tf.variable_scope(""attention""):\n        attention_heads = []\n        with tf.variable_scope(""self""):\n          attention_head, probs = attention_layer(\n              from_tensor=prev_output,\n              to_tensor=prev_output,\n              attention_mask=attention_mask,\n              num_attention_heads=num_attention_heads,\n              size_per_head=attention_head_size,\n              attention_probs_dropout_prob=attention_probs_dropout_prob,\n              initializer_range=initializer_range,\n              do_return_2d_tensor=True,\n              batch_size=batch_size,\n              from_seq_length=seq_length,\n              to_seq_length=seq_length)\n          attention_heads.append(attention_head)\n          attn_maps.append(probs)\n\n        attention_output = None\n        if len(attention_heads) == 1:\n          attention_output = attention_heads[0]\n        else:\n          # In the case where we have other sequences, we just concatenate\n          # them to the self-attention head before the projection.\n          attention_output = tf.concat(attention_heads, axis=-1)\n\n        # Run a linear projection of `hidden_size` then add a residual\n        # with `layer_input`.\n        with tf.variable_scope(""output""):\n          attention_output = tf.layers.dense(\n              attention_output,\n              hidden_size,\n              kernel_initializer=create_initializer(initializer_range))\n          attention_output = dropout(attention_output, hidden_dropout_prob)\n          attention_output = layer_norm(attention_output + prev_output)\n\n      # The activation is only applied to the ""intermediate"" hidden layer.\n      with tf.variable_scope(""intermediate""):\n        intermediate_output = tf.layers.dense(\n            attention_output,\n            intermediate_size,\n            activation=intermediate_act_fn,\n            kernel_initializer=create_initializer(initializer_range))\n\n      # Down-project back to `hidden_size` then add the residual.\n      with tf.variable_scope(""output""):\n        prev_output = tf.layers.dense(\n            intermediate_output,\n            hidden_size,\n            kernel_initializer=create_initializer(initializer_range))\n        prev_output = dropout(prev_output, hidden_dropout_prob)\n        prev_output = layer_norm(prev_output + attention_output)\n        all_layer_outputs.append(prev_output)\n\n  attn_maps = tf.stack(attn_maps, 0)\n  if do_return_all_layers:\n    return tf.stack([reshape_from_matrix(layer, input_shape)\n                     for layer in all_layer_outputs], 0), attn_maps\n  else:\n    return reshape_from_matrix(prev_output, input_shape), attn_maps\n\n\ndef get_shape_list(tensor, expected_rank=None, name=None):\n  """"""Returns a list of the shape of tensor, preferring static dimensions.\n\n  Args:\n    tensor: A tf.Tensor object to find the shape of.\n    expected_rank: (optional) int. The expected rank of `tensor`. If this is\n      specified and the `tensor` has a different rank, and exception will be\n      thrown.\n    name: Optional name of the tensor for the error message.\n\n  Returns:\n    A list of dimensions of the shape of tensor. All static dimensions will\n    be returned as python integers, and dynamic dimensions will be returned\n    as tf.Tensor scalars.\n  """"""\n  if isinstance(tensor, np.ndarray) or isinstance(tensor, list):\n    shape = np.array(tensor).shape\n    if isinstance(expected_rank, six.integer_types):\n      assert len(shape) == expected_rank\n    elif expected_rank is not None:\n      assert len(shape) in expected_rank\n    return shape\n\n  if name is None:\n    name = tensor.name\n\n  if expected_rank is not None:\n    assert_rank(tensor, expected_rank, name)\n\n  shape = tensor.shape.as_list()\n\n  non_static_indexes = []\n  for (index, dim) in enumerate(shape):\n    if dim is None:\n      non_static_indexes.append(index)\n\n  if not non_static_indexes:\n    return shape\n\n  dyn_shape = tf.shape(tensor)\n  for index in non_static_indexes:\n    shape[index] = dyn_shape[index]\n  return shape\n\n\ndef reshape_to_matrix(input_tensor):\n  """"""Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).""""""\n  ndims = input_tensor.shape.ndims\n  if ndims < 2:\n    raise ValueError(""Input tensor must have at least rank 2. Shape = %s"" %\n                     (input_tensor.shape))\n  if ndims == 2:\n    return input_tensor\n\n  width = input_tensor.shape[-1]\n  output_tensor = tf.reshape(input_tensor, [-1, width])\n  return output_tensor\n\n\ndef reshape_from_matrix(output_tensor, orig_shape_list):\n  """"""Reshapes a rank 2 tensor back to its original rank >= 2 tensor.""""""\n  if len(orig_shape_list) == 2:\n    return output_tensor\n\n  output_shape = get_shape_list(output_tensor)\n\n  orig_dims = orig_shape_list[0:-1]\n  width = output_shape[-1]\n\n  return tf.reshape(output_tensor, orig_dims + [width])\n\n\ndef assert_rank(tensor, expected_rank, name=None):\n  """"""Raises an exception if the tensor rank is not of the expected rank.\n\n  Args:\n    tensor: A tf.Tensor to check the rank of.\n    expected_rank: Python integer or list of integers, expected rank.\n    name: Optional name of the tensor for the error message.\n\n  Raises:\n    ValueError: If the expected shape doesn\'t match the actual shape.\n  """"""\n  if name is None:\n    name = tensor.name\n\n  expected_rank_dict = {}\n  if isinstance(expected_rank, six.integer_types):\n    expected_rank_dict[expected_rank] = True\n  else:\n    for x in expected_rank:\n      expected_rank_dict[x] = True\n\n  actual_rank = tensor.shape.ndims\n  if actual_rank not in expected_rank_dict:\n    scope_name = tf.get_variable_scope().name\n    raise ValueError(\n        ""For the tensor `%s` in scope `%s`, the actual rank ""\n        ""`%d` (shape = %s) is not equal to the expected rank `%s`"" %\n        (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))\n'"
model/optimization.py,21,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Functions and classes related to optimization (weight updates).\nModified from the original BERT code to allow for having separate learning\nrates for different layers of the network.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport re\nimport tensorflow.compat.v1 as tf\n\n\ndef create_optimizer(\n    loss, learning_rate, num_train_steps, weight_decay_rate=0.0, use_tpu=False,\n    warmup_steps=0, warmup_proportion=0, lr_decay_power=1.0,\n    layerwise_lr_decay_power=-1, n_transformer_layers=None):\n  """"""Creates an optimizer and training op.""""""\n  global_step = tf.train.get_or_create_global_step()\n  learning_rate = tf.train.polynomial_decay(\n      learning_rate,\n      global_step,\n      num_train_steps,\n      end_learning_rate=0.0,\n      power=lr_decay_power,\n      cycle=False)\n  warmup_steps = max(num_train_steps * warmup_proportion, warmup_steps)\n  learning_rate *= tf.minimum(\n      1.0, tf.cast(global_step, tf.float32) / tf.cast(warmup_steps, tf.float32))\n\n  if layerwise_lr_decay_power > 0:\n    learning_rate = _get_layer_lrs(learning_rate, layerwise_lr_decay_power,\n                                   n_transformer_layers)\n  optimizer = AdamWeightDecayOptimizer(\n      learning_rate=learning_rate,\n      weight_decay_rate=weight_decay_rate,\n      beta_1=0.9,\n      beta_2=0.999,\n      epsilon=1e-6,\n      exclude_from_weight_decay=[""LayerNorm"", ""layer_norm"", ""bias""])\n  if use_tpu:\n    optimizer = tf.tpu.CrossShardOptimizer(optimizer)\n\n  tvars = tf.trainable_variables()\n  grads = tf.gradients(loss, tvars)\n  (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n  train_op = optimizer.apply_gradients(\n      zip(grads, tvars), global_step=global_step)\n  new_global_step = global_step + 1\n  train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n  return train_op\n\n\nclass AdamWeightDecayOptimizer(tf.train.Optimizer):\n  """"""A basic Adam optimizer that includes ""correct"" L2 weight decay.""""""\n\n  def __init__(self,\n               learning_rate,\n               weight_decay_rate=0.0,\n               beta_1=0.9,\n               beta_2=0.999,\n               epsilon=1e-6,\n               exclude_from_weight_decay=None,\n               name=""AdamWeightDecayOptimizer""):\n    """"""Constructs a AdamWeightDecayOptimizer.""""""\n    super(AdamWeightDecayOptimizer, self).__init__(False, name)\n\n    self.learning_rate = learning_rate\n    self.weight_decay_rate = weight_decay_rate\n    self.beta_1 = beta_1\n    self.beta_2 = beta_2\n    self.epsilon = epsilon\n    self.exclude_from_weight_decay = exclude_from_weight_decay\n\n  def _apply_gradients(self, grads_and_vars, learning_rate):\n    """"""See base class.""""""\n    assignments = []\n    for (grad, param) in grads_and_vars:\n      if grad is None or param is None:\n        continue\n\n      param_name = self._get_variable_name(param.name)\n\n      m = tf.get_variable(\n          name=param_name + ""/adam_m"",\n          shape=param.shape.as_list(),\n          dtype=tf.float32,\n          trainable=False,\n          initializer=tf.zeros_initializer())\n      v = tf.get_variable(\n          name=param_name + ""/adam_v"",\n          shape=param.shape.as_list(),\n          dtype=tf.float32,\n          trainable=False,\n          initializer=tf.zeros_initializer())\n\n      # Standard Adam update.\n      next_m = (\n          tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))\n      next_v = (\n          tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,\n                                                    tf.square(grad)))\n      update = next_m / (tf.sqrt(next_v) + self.epsilon)\n\n      # Just adding the square of the weights to the loss function is *not*\n      # the correct way of using L2 regularization/weight decay with Adam,\n      # since that will interact with the m and v parameters in strange ways.\n      #\n      # Instead we want ot decay the weights in a manner that doesn\'t interact\n      # with the m/v parameters. This is equivalent to adding the square\n      # of the weights to the loss with plain (non-momentum) SGD.\n      if self.weight_decay_rate > 0:\n        if self._do_use_weight_decay(param_name):\n          update += self.weight_decay_rate * param\n\n      update_with_lr = learning_rate * update\n      next_param = param - update_with_lr\n\n      assignments.extend(\n          [param.assign(next_param),\n           m.assign(next_m),\n           v.assign(next_v)])\n\n    return assignments\n\n  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n    if isinstance(self.learning_rate, dict):\n      key_to_grads_and_vars = {}\n      for grad, var in grads_and_vars:\n        update_for_var = False\n        for key in self.learning_rate:\n          if key in var.name:\n            update_for_var = True\n            if key not in key_to_grads_and_vars:\n              key_to_grads_and_vars[key] = []\n            key_to_grads_and_vars[key].append((grad, var))\n        if not update_for_var:\n          raise ValueError(""No learning rate specified for variable"", var)\n      assignments = []\n      for key, key_grads_and_vars in key_to_grads_and_vars.items():\n        assignments += self._apply_gradients(key_grads_and_vars,\n                                             self.learning_rate[key])\n    else:\n      assignments = self._apply_gradients(grads_and_vars, self.learning_rate)\n    return tf.group(*assignments, name=name)\n\n  def _do_use_weight_decay(self, param_name):\n    """"""Whether to use L2 weight decay for `param_name`.""""""\n    if not self.weight_decay_rate:\n      return False\n    if self.exclude_from_weight_decay:\n      for r in self.exclude_from_weight_decay:\n        if re.search(r, param_name) is not None:\n          return False\n    return True\n\n  def _get_variable_name(self, param_name):\n    """"""Get the variable name from the tensor name.""""""\n    m = re.match(""^(.*):\\\\d+$"", param_name)\n    if m is not None:\n      param_name = m.group(1)\n    return param_name\n\n\ndef _get_layer_lrs(learning_rate, layer_decay, n_layers):\n  """"""Have lower learning rates for layers closer to the input.""""""\n  key_to_depths = collections.OrderedDict({\n      ""/embeddings/"": 0,\n      ""/embeddings_project/"": 0,\n      ""task_specific/"": n_layers + 2,\n  })\n  for layer in range(n_layers):\n    key_to_depths[""encoder/layer_"" + str(layer) + ""/""] = layer + 1\n  return {\n      key: learning_rate * (layer_decay ** (n_layers + 2 - depth))\n      for key, depth in key_to_depths.items()\n  }\n'"
model/tokenization.py,2,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tokenization classes, the same as used for BERT.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport unicodedata\nimport six\nimport tensorflow.compat.v1 as tf\n\n\n\ndef convert_to_unicode(text):\n  """"""Converts `text` to Unicode (if it\'s not already), assuming utf-8 input.""""""\n  if six.PY3:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, bytes):\n      return text.decode(""utf-8"", ""ignore"")\n    else:\n      raise ValueError(""Unsupported string type: %s"" % (type(text)))\n  elif six.PY2:\n    if isinstance(text, str):\n      return text.decode(""utf-8"", ""ignore"")\n    elif isinstance(text, unicode):\n      return text\n    else:\n      raise ValueError(""Unsupported string type: %s"" % (type(text)))\n  else:\n    raise ValueError(""Not running on Python2 or Python 3?"")\n\n\ndef printable_text(text):\n  """"""Returns text encoded in a way suitable for print or `tf.logging`.""""""\n\n  # These functions want `str` for both Python2 and Python3, but in one case\n  # it\'s a Unicode string and in the other it\'s a byte string.\n  if six.PY3:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, bytes):\n      return text.decode(""utf-8"", ""ignore"")\n    else:\n      raise ValueError(""Unsupported string type: %s"" % (type(text)))\n  elif six.PY2:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, unicode):\n      return text.encode(""utf-8"")\n    else:\n      raise ValueError(""Unsupported string type: %s"" % (type(text)))\n  else:\n    raise ValueError(""Not running on Python2 or Python 3?"")\n\n\ndef load_vocab(vocab_file):\n  """"""Loads a vocabulary file into a dictionary.""""""\n  vocab = collections.OrderedDict()\n  index = 0\n  with tf.io.gfile.GFile(vocab_file, ""r"") as reader:\n    while True:\n      token = convert_to_unicode(reader.readline())\n      if not token:\n        break\n      token = token.strip()\n      vocab[token] = index\n      index += 1\n  return vocab\n\n\ndef convert_by_vocab(vocab, items):\n  """"""Converts a sequence of [tokens|ids] using the vocab.""""""\n  output = []\n  for item in items:\n    output.append(vocab[item])\n  return output\n\n\ndef convert_tokens_to_ids(vocab, tokens):\n  return convert_by_vocab(vocab, tokens)\n\n\ndef convert_ids_to_tokens(inv_vocab, ids):\n  return convert_by_vocab(inv_vocab, ids)\n\n\ndef whitespace_tokenize(text):\n  """"""Runs basic whitespace cleaning and splitting on a piece of text.""""""\n  text = text.strip()\n  if not text:\n    return []\n  tokens = text.split()\n  return tokens\n\n\nclass FullTokenizer(object):\n  """"""Runs end-to-end tokenziation.""""""\n\n  def __init__(self, vocab_file, do_lower_case=True):\n    self.vocab = load_vocab(vocab_file)\n    self.inv_vocab = {v: k for k, v in self.vocab.items()}\n    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n\n  def tokenize(self, text):\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n        split_tokens.append(sub_token)\n\n    return split_tokens\n\n  def convert_tokens_to_ids(self, tokens):\n    return convert_by_vocab(self.vocab, tokens)\n\n  def convert_ids_to_tokens(self, ids):\n    return convert_by_vocab(self.inv_vocab, ids)\n\n\nclass BasicTokenizer(object):\n  """"""Runs basic tokenization (punctuation splitting, lower casing, etc.).""""""\n\n  def __init__(self, do_lower_case=True):\n    """"""Constructs a BasicTokenizer.\n\n    Args:\n      do_lower_case: Whether to lower case the input.\n    """"""\n    self.do_lower_case = do_lower_case\n\n  def tokenize(self, text):\n    """"""Tokenizes a piece of text.""""""\n    text = convert_to_unicode(text)\n    text = self._clean_text(text)\n\n    # This was added on November 1st, 2018 for the multilingual and Chinese\n    # models. This is also applied to the English models now, but it doesn\'t\n    # matter since the English models were not trained on any Chinese data\n    # and generally don\'t have any Chinese data in them (there are Chinese\n    # characters in the vocabulary because Wikipedia does have some Chinese\n    # words in the English Wikipedia.).\n    text = self._tokenize_chinese_chars(text)\n\n    orig_tokens = whitespace_tokenize(text)\n    split_tokens = []\n    for token in orig_tokens:\n      if self.do_lower_case:\n        token = token.lower()\n        token = self._run_strip_accents(token)\n      split_tokens.extend(self._run_split_on_punc(token))\n\n    output_tokens = whitespace_tokenize("" "".join(split_tokens))\n    return output_tokens\n\n  def _run_strip_accents(self, text):\n    """"""Strips accents from a piece of text.""""""\n    text = unicodedata.normalize(""NFD"", text)\n    output = []\n    for char in text:\n      cat = unicodedata.category(char)\n      if cat == ""Mn"":\n        continue\n      output.append(char)\n    return """".join(output)\n\n  def _run_split_on_punc(self, text):\n    """"""Splits punctuation on a piece of text.""""""\n    chars = list(text)\n    i = 0\n    start_new_word = True\n    output = []\n    while i < len(chars):\n      char = chars[i]\n      if _is_punctuation(char):\n        output.append([char])\n        start_new_word = True\n      else:\n        if start_new_word:\n          output.append([])\n        start_new_word = False\n        output[-1].append(char)\n      i += 1\n\n    return ["""".join(x) for x in output]\n\n  def _tokenize_chinese_chars(self, text):\n    """"""Adds whitespace around any CJK character.""""""\n    output = []\n    for char in text:\n      cp = ord(char)\n      if self._is_chinese_char(cp):\n        output.append("" "")\n        output.append(char)\n        output.append("" "")\n      else:\n        output.append(char)\n    return """".join(output)\n\n  def _is_chinese_char(self, cp):\n    """"""Checks whether CP is the codepoint of a CJK character.""""""\n    # This defines a ""chinese character"" as anything in the CJK Unicode block:\n    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n    #\n    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n    # despite its name. The modern Korean Hangul alphabet is a different block,\n    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n    # space-separated words, so they are not treated specially and handled\n    # like the all of the other languages.\n    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n        (cp >= 0x3400 and cp <= 0x4DBF) or  #\n        (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n        (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n        (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n        (cp >= 0x2B820 and cp <= 0x2CEAF) or\n        (cp >= 0xF900 and cp <= 0xFAFF) or  #\n        (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n      return True\n\n    return False\n\n  def _clean_text(self, text):\n    """"""Performs invalid character removal and whitespace cleanup on text.""""""\n    output = []\n    for char in text:\n      cp = ord(char)\n      if cp == 0 or cp == 0xfffd or _is_control(char):\n        continue\n      if _is_whitespace(char):\n        output.append("" "")\n      else:\n        output.append(char)\n    return """".join(output)\n\n\nclass WordpieceTokenizer(object):\n  """"""Runs WordPiece tokenziation.""""""\n\n  def __init__(self, vocab, unk_token=""[UNK]"", max_input_chars_per_word=200):\n    self.vocab = vocab\n    self.unk_token = unk_token\n    self.max_input_chars_per_word = max_input_chars_per_word\n\n  def tokenize(self, text):\n    """"""Tokenizes a piece of text into its word pieces.\n\n    This uses a greedy longest-match-first algorithm to perform tokenization\n    using the given vocabulary.\n\n    For example:\n      input = ""unaffable""\n      output = [""un"", ""##aff"", ""##able""]\n\n    Args:\n      text: A single token or whitespace separated tokens. This should have\n        already been passed through `BasicTokenizer.\n\n    Returns:\n      A list of wordpiece tokens.\n    """"""\n\n    text = convert_to_unicode(text)\n\n    output_tokens = []\n    for token in whitespace_tokenize(text):\n      chars = list(token)\n      if len(chars) > self.max_input_chars_per_word:\n        output_tokens.append(self.unk_token)\n        continue\n\n      is_bad = False\n      start = 0\n      sub_tokens = []\n      while start < len(chars):\n        end = len(chars)\n        cur_substr = None\n        while start < end:\n          substr = """".join(chars[start:end])\n          if start > 0:\n            substr = ""##"" + substr\n          if substr in self.vocab:\n            cur_substr = substr\n            break\n          end -= 1\n        if cur_substr is None:\n          is_bad = True\n          break\n        sub_tokens.append(cur_substr)\n        start = end\n\n      if is_bad:\n        output_tokens.append(self.unk_token)\n      else:\n        output_tokens.extend(sub_tokens)\n    return output_tokens\n\n\ndef _is_whitespace(char):\n  """"""Checks whether `chars` is a whitespace character.""""""\n  # \\t, \\n, and \\r are technically contorl characters but we treat them\n  # as whitespace since they are generally considered as such.\n  if char == "" "" or char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n    return True\n  cat = unicodedata.category(char)\n  if cat == ""Zs"":\n    return True\n  return False\n\n\ndef _is_control(char):\n  """"""Checks whether `chars` is a control character.""""""\n  # These are technically control characters but we count them as whitespace\n  # characters.\n  if char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n    return False\n  cat = unicodedata.category(char)\n  if cat.startswith(""C""):\n    return True\n  return False\n\n\ndef _is_punctuation(char):\n  """"""Checks whether `chars` is a punctuation character.""""""\n  cp = ord(char)\n  # We treat all non-letter/number ASCII as punctuation.\n  # Characters such as ""^"", ""$"", and ""`"" are not in the Unicode\n  # Punctuation class but we treat them as punctuation anyways, for\n  # consistency.\n  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n    return True\n  cat = unicodedata.category(char)\n  if cat.startswith(""P""):\n    return True\n  return False\n'"
pretrain/__init__.py,0,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'"
pretrain/pretrain_data.py,12,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Helpers for preparing pre-training data and supplying them to the model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nimport configure_pretraining\nfrom model import tokenization\nfrom util import utils\n\n\ndef get_input_fn(config: configure_pretraining.PretrainingConfig, is_training,\n                 num_cpu_threads=4):\n  """"""Creates an `input_fn` closure to be passed to TPUEstimator.""""""\n\n  input_files = []\n  for input_pattern in config.pretrain_tfrecords.split("",""):\n    input_files.extend(tf.io.gfile.glob(input_pattern))\n\n  def input_fn(params):\n    """"""The actual input function.""""""\n    batch_size = params[""batch_size""]\n\n    name_to_features = {\n        ""input_ids"": tf.io.FixedLenFeature([config.max_seq_length], tf.int64),\n        ""input_mask"": tf.io.FixedLenFeature([config.max_seq_length], tf.int64),\n        ""segment_ids"": tf.io.FixedLenFeature([config.max_seq_length], tf.int64),\n    }\n\n    d = tf.data.Dataset.from_tensor_slices(tf.constant(input_files))\n    d = d.repeat()\n    d = d.shuffle(buffer_size=len(input_files))\n\n    # `cycle_length` is the number of parallel files that get read.\n    cycle_length = min(num_cpu_threads, len(input_files))\n\n    # `sloppy` mode means that the interleaving is not exact. This adds\n    # even more randomness to the training pipeline.\n    d = d.apply(\n        tf.data.experimental.parallel_interleave(\n            tf.data.TFRecordDataset,\n            sloppy=is_training,\n            cycle_length=cycle_length))\n    d = d.shuffle(buffer_size=100)\n\n    # We must `drop_remainder` on training because the TPU requires fixed\n    # size dimensions. For eval, we assume we are evaluating on the CPU or GPU\n    # and we *don""t* want to drop the remainder, otherwise we wont cover\n    # every sample.\n    d = d.apply(\n        tf.data.experimental.map_and_batch(\n            lambda record: _decode_record(record, name_to_features),\n            batch_size=batch_size,\n            num_parallel_batches=num_cpu_threads,\n            drop_remainder=True))\n    return d\n\n  return input_fn\n\n\ndef _decode_record(record, name_to_features):\n  """"""Decodes a record to a TensorFlow example.""""""\n  example = tf.io.parse_single_example(record, name_to_features)\n\n  # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n  # So cast all int64 to int32.\n  for name in list(example.keys()):\n    t = example[name]\n    if t.dtype == tf.int64:\n      t = tf.cast(t, tf.int32)\n    example[name] = t\n\n  return example\n\n\n# model inputs - it\'s a bit nicer to use a namedtuple rather than keep the\n# features as a dict\nInputs = collections.namedtuple(\n    ""Inputs"", [""input_ids"", ""input_mask"", ""segment_ids"", ""masked_lm_positions"",\n               ""masked_lm_ids"", ""masked_lm_weights""])\n\n\ndef features_to_inputs(features):\n  return Inputs(\n      input_ids=features[""input_ids""],\n      input_mask=features[""input_mask""],\n      segment_ids=features[""segment_ids""],\n      masked_lm_positions=(features[""masked_lm_positions""]\n                           if ""masked_lm_positions"" in features else None),\n      masked_lm_ids=(features[""masked_lm_ids""]\n                     if ""masked_lm_ids"" in features else None),\n      masked_lm_weights=(features[""masked_lm_weights""]\n                         if ""masked_lm_weights"" in features else None),\n  )\n\n\ndef get_updated_inputs(inputs, **kwargs):\n  features = inputs._asdict()\n  for k, v in kwargs.items():\n    features[k] = v\n  return features_to_inputs(features)\n\n\nENDC = ""\\033[0m""\nCOLORS = [""\\033["" + str(n) + ""m"" for n in list(range(91, 97)) + [90]]\nRED = COLORS[0]\nBLUE = COLORS[3]\nCYAN = COLORS[5]\nGREEN = COLORS[1]\n\n\ndef print_tokens(inputs: Inputs, inv_vocab, updates_mask=None):\n  """"""Pretty-print model inputs.""""""\n  pos_to_tokid = {}\n  for tokid, pos, weight in zip(\n      inputs.masked_lm_ids[0], inputs.masked_lm_positions[0],\n      inputs.masked_lm_weights[0]):\n    if weight == 0:\n      pass\n    else:\n      pos_to_tokid[pos] = tokid\n\n  text = """"\n  provided_update_mask = (updates_mask is not None)\n  if not provided_update_mask:\n    updates_mask = np.zeros_like(inputs.input_ids)\n  for pos, (tokid, um) in enumerate(\n      zip(inputs.input_ids[0], updates_mask[0])):\n    token = inv_vocab[tokid]\n    if token == ""[PAD]"":\n      break\n    if pos in pos_to_tokid:\n      token = RED + token + "" ("" + inv_vocab[pos_to_tokid[pos]] + "")"" + ENDC\n      if provided_update_mask:\n        assert um == 1\n    else:\n      if provided_update_mask:\n        assert um == 0\n    text += token + "" ""\n  utils.log(tokenization.printable_text(text))\n'"
pretrain/pretrain_helpers.py,54,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Helper functions for pre-training. These mainly deal with the gathering and\nscattering needed so the generator only makes predictions for the small number\nof masked tokens.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow.compat.v1 as tf\n\nimport configure_pretraining\nfrom model import modeling\nfrom model import tokenization\nfrom pretrain import pretrain_data\n\n\ndef gather_positions(sequence, positions):\n  """"""Gathers the vectors at the specific positions over a minibatch.\n\n  Args:\n    sequence: A [batch_size, seq_length] or\n        [batch_size, seq_length, depth] tensor of values\n    positions: A [batch_size, n_positions] tensor of indices\n\n  Returns: A [batch_size, n_positions] or\n    [batch_size, n_positions, depth] tensor of the values at the indices\n  """"""\n  shape = modeling.get_shape_list(sequence, expected_rank=[2, 3])\n  depth_dimension = (len(shape) == 3)\n  if depth_dimension:\n    B, L, D = shape\n  else:\n    B, L = shape\n    D = 1\n    sequence = tf.expand_dims(sequence, -1)\n  position_shift = tf.expand_dims(L * tf.range(B), -1)\n  flat_positions = tf.reshape(positions + position_shift, [-1])\n  flat_sequence = tf.reshape(sequence, [B * L, D])\n  gathered = tf.gather(flat_sequence, flat_positions)\n  if depth_dimension:\n    return tf.reshape(gathered, [B, -1, D])\n  else:\n    return tf.reshape(gathered, [B, -1])\n\n\ndef scatter_update(sequence, updates, positions):\n  """"""Scatter-update a sequence.\n\n  Args:\n    sequence: A [batch_size, seq_len] or [batch_size, seq_len, depth] tensor\n    updates: A tensor of size batch_size*seq_len(*depth)\n    positions: A [batch_size, n_positions] tensor\n\n  Returns: A tuple of two tensors. First is a [batch_size, seq_len] or\n    [batch_size, seq_len, depth] tensor of ""sequence"" with elements at\n    ""positions"" replaced by the values at ""updates."" Updates to index 0 are\n    ignored. If there are duplicated positions the update is only applied once.\n    Second is a [batch_size, seq_len] mask tensor of which inputs were updated.\n  """"""\n  shape = modeling.get_shape_list(sequence, expected_rank=[2, 3])\n  depth_dimension = (len(shape) == 3)\n  if depth_dimension:\n    B, L, D = shape\n  else:\n    B, L = shape\n    D = 1\n    sequence = tf.expand_dims(sequence, -1)\n  N = modeling.get_shape_list(positions)[1]\n\n  shift = tf.expand_dims(L * tf.range(B), -1)\n  flat_positions = tf.reshape(positions + shift, [-1, 1])\n  flat_updates = tf.reshape(updates, [-1, D])\n  updates = tf.scatter_nd(flat_positions, flat_updates, [B * L, D])\n  updates = tf.reshape(updates, [B, L, D])\n\n  flat_updates_mask = tf.ones([B * N], tf.int32)\n  updates_mask = tf.scatter_nd(flat_positions, flat_updates_mask, [B * L])\n  updates_mask = tf.reshape(updates_mask, [B, L])\n  not_first_token = tf.concat([tf.zeros((B, 1), tf.int32),\n                               tf.ones((B, L - 1), tf.int32)], -1)\n  updates_mask *= not_first_token\n  updates_mask_3d = tf.expand_dims(updates_mask, -1)\n\n  # account for duplicate positions\n  if sequence.dtype == tf.float32:\n    updates_mask_3d = tf.cast(updates_mask_3d, tf.float32)\n    updates /= tf.maximum(1.0, updates_mask_3d)\n  else:\n    assert sequence.dtype == tf.int32\n    updates = tf.math.floordiv(updates, tf.maximum(1, updates_mask_3d))\n  updates_mask = tf.minimum(updates_mask, 1)\n  updates_mask_3d = tf.minimum(updates_mask_3d, 1)\n\n  updated_sequence = (((1 - updates_mask_3d) * sequence) +\n                      (updates_mask_3d * updates))\n  if not depth_dimension:\n    updated_sequence = tf.squeeze(updated_sequence, -1)\n\n  return updated_sequence, updates_mask\n\n\ndef _get_candidates_mask(inputs: pretrain_data.Inputs, vocab,\n                         disallow_from_mask=None):\n  """"""Returns a mask tensor of positions in the input that can be masked out.""""""\n  ignore_ids = [vocab[""[SEP]""], vocab[""[CLS]""], vocab[""[MASK]""]]\n  candidates_mask = tf.ones_like(inputs.input_ids, tf.bool)\n  for ignore_id in ignore_ids:\n    candidates_mask &= tf.not_equal(inputs.input_ids, ignore_id)\n  candidates_mask &= tf.cast(inputs.input_mask, tf.bool)\n  if disallow_from_mask is not None:\n    candidates_mask &= ~disallow_from_mask\n  return candidates_mask\n\n\ndef mask(config: configure_pretraining.PretrainingConfig,\n         inputs: pretrain_data.Inputs, mask_prob, proposal_distribution=1.0,\n         disallow_from_mask=None, already_masked=None):\n  """"""Implementation of dynamic masking. The optional arguments aren\'t needed for\n  BERT/ELECTRA and are from early experiments in ""strategically"" masking out\n  tokens instead of uniformly at random.\n\n  Args:\n    config: configure_pretraining.PretrainingConfig\n    inputs: pretrain_data.Inputs containing input input_ids/input_mask\n    mask_prob: percent of tokens to mask\n    proposal_distribution: for non-uniform masking can be a [B, L] tensor\n                           of scores for masking each position.\n    disallow_from_mask: a boolean tensor of [B, L] of positions that should\n                        not be masked out\n    already_masked: a boolean tensor of [B, N] of already masked-out tokens\n                    for multiple rounds of masking\n  Returns: a pretrain_data.Inputs with masking added\n  """"""\n  # Get the batch size, sequence length, and max masked-out tokens\n  N = config.max_predictions_per_seq\n  B, L = modeling.get_shape_list(inputs.input_ids)\n\n  # Find indices where masking out a token is allowed\n  vocab = tokenization.FullTokenizer(\n      config.vocab_file, do_lower_case=config.do_lower_case).vocab\n  candidates_mask = _get_candidates_mask(inputs, vocab, disallow_from_mask)\n\n  # Set the number of tokens to mask out per example\n  num_tokens = tf.cast(tf.reduce_sum(inputs.input_mask, -1), tf.float32)\n  num_to_predict = tf.maximum(1, tf.minimum(\n      N, tf.cast(tf.round(num_tokens * mask_prob), tf.int32)))\n  masked_lm_weights = tf.cast(tf.sequence_mask(num_to_predict, N), tf.float32)\n  if already_masked is not None:\n    masked_lm_weights *= (1 - already_masked)\n\n  # Get a probability of masking each position in the sequence\n  candidate_mask_float = tf.cast(candidates_mask, tf.float32)\n  sample_prob = (proposal_distribution * candidate_mask_float)\n  sample_prob /= tf.reduce_sum(sample_prob, axis=-1, keepdims=True)\n\n  # Sample the positions to mask out\n  sample_prob = tf.stop_gradient(sample_prob)\n  sample_logits = tf.log(sample_prob)\n  masked_lm_positions = tf.random.categorical(\n      sample_logits, N, dtype=tf.int32)\n  masked_lm_positions *= tf.cast(masked_lm_weights, tf.int32)\n\n  # Get the ids of the masked-out tokens\n  shift = tf.expand_dims(L * tf.range(B), -1)\n  flat_positions = tf.reshape(masked_lm_positions + shift, [-1, 1])\n  masked_lm_ids = tf.gather_nd(tf.reshape(inputs.input_ids, [-1]),\n                               flat_positions)\n  masked_lm_ids = tf.reshape(masked_lm_ids, [B, -1])\n  masked_lm_ids *= tf.cast(masked_lm_weights, tf.int32)\n\n  # Update the input ids\n  replace_with_mask_positions = masked_lm_positions * tf.cast(\n      tf.less(tf.random.uniform([B, N]), 0.85), tf.int32)\n  inputs_ids, _ = scatter_update(\n      inputs.input_ids, tf.fill([B, N], vocab[""[MASK]""]),\n      replace_with_mask_positions)\n\n  return pretrain_data.get_updated_inputs(\n      inputs,\n      input_ids=tf.stop_gradient(inputs_ids),\n      masked_lm_positions=masked_lm_positions,\n      masked_lm_ids=masked_lm_ids,\n      masked_lm_weights=masked_lm_weights\n  )\n\n\ndef unmask(inputs: pretrain_data.Inputs):\n  unmasked_input_ids, _ = scatter_update(\n      inputs.input_ids, inputs.masked_lm_ids, inputs.masked_lm_positions)\n  return pretrain_data.get_updated_inputs(inputs, input_ids=unmasked_input_ids)\n\n\ndef sample_from_softmax(logits, disallow=None):\n  if disallow is not None:\n    logits -= 1000.0 * disallow\n  uniform_noise = tf.random.uniform(\n      modeling.get_shape_list(logits), minval=0, maxval=1)\n  gumbel_noise = -tf.log(-tf.log(uniform_noise + 1e-9) + 1e-9)\n  return tf.one_hot(tf.argmax(tf.nn.softmax(logits + gumbel_noise), -1,\n                              output_type=tf.int32), logits.shape[-1])\n'"
util/__init__.py,0,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'"
util/training_utils.py,3,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utilities for training the models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport datetime\nimport re\nimport time\nimport tensorflow.compat.v1 as tf\n\nfrom model import modeling\nfrom util import utils\n\n\nclass ETAHook(tf.estimator.SessionRunHook):\n  """"""Print out the time remaining during training/evaluation.""""""\n\n  def __init__(self, to_log, n_steps, iterations_per_loop, on_tpu,\n               log_every=1, is_training=True):\n    self._to_log = to_log\n    self._n_steps = n_steps\n    self._iterations_per_loop = iterations_per_loop\n    self._on_tpu = on_tpu\n    self._log_every = log_every\n    self._is_training = is_training\n    self._steps_run_so_far = 0\n    self._global_step = None\n    self._global_step_tensor = None\n    self._start_step = None\n    self._start_time = None\n\n  def begin(self):\n    self._global_step_tensor = tf.train.get_or_create_global_step()\n\n  def before_run(self, run_context):\n    if self._start_time is None:\n      self._start_time = time.time()\n    return tf.estimator.SessionRunArgs(self._to_log)\n\n  def after_run(self, run_context, run_values):\n    self._global_step = run_context.session.run(self._global_step_tensor)\n    self._steps_run_so_far += self._iterations_per_loop if self._on_tpu else 1\n    if self._start_step is None:\n      self._start_step = self._global_step - (self._iterations_per_loop\n                                              if self._on_tpu else 1)\n    self.log(run_values)\n\n  def end(self, session):\n    self._global_step = session.run(self._global_step_tensor)\n    self.log()\n\n  def log(self, run_values=None):\n    step = self._global_step if self._is_training else self._steps_run_so_far\n    if step % self._log_every != 0:\n      return\n    msg = ""{:}/{:} = {:.1f}%"".format(step, self._n_steps,\n                                     100.0 * step / self._n_steps)\n    time_elapsed = time.time() - self._start_time\n    time_per_step = time_elapsed / (\n        (step - self._start_step) if self._is_training else step)\n    msg += "", SPS: {:.1f}"".format(1 / time_per_step)\n    msg += "", ELAP: "" + secs_to_str(time_elapsed)\n    msg += "", ETA: "" + secs_to_str(\n        (self._n_steps - step) * time_per_step)\n    if run_values is not None:\n      for tag, value in run_values.results.items():\n        msg += "" - "" + str(tag) + ("": {:.4f}"".format(value))\n    utils.log(msg)\n\n\ndef secs_to_str(secs):\n  s = str(datetime.timedelta(seconds=int(round(secs))))\n  s = re.sub(""^0:"", """", s)\n  s = re.sub(""^0"", """", s)\n  s = re.sub(""^0:"", """", s)\n  s = re.sub(""^0"", """", s)\n  return s\n\n\ndef get_bert_config(config):\n  """"""Get model hyperparameters based on a pretraining/finetuning config""""""\n  if config.model_size == ""large"":\n    args = {""hidden_size"": 1024, ""num_hidden_layers"": 24}\n  elif config.model_size == ""base"":\n    args = {""hidden_size"": 768, ""num_hidden_layers"": 12}\n  elif config.model_size == ""small"":\n    args = {""hidden_size"": 256, ""num_hidden_layers"": 12}\n  else:\n    raise ValueError(""Unknown model size"", config.model_size)\n  args[""vocab_size""] = config.vocab_size\n  args.update(**config.model_hparam_overrides)\n  # by default the ff size and num attn heads are determined by the hidden size\n  args[""num_attention_heads""] = max(1, args[""hidden_size""] // 64)\n  args[""intermediate_size""] = 4 * args[""hidden_size""]\n  args.update(**config.model_hparam_overrides)\n  return modeling.BertConfig.from_dict(args)\n'"
util/utils.py,10,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A collection of general utility functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport pickle\nimport sys\n\nimport tensorflow.compat.v1 as tf\n\n\ndef load_json(path):\n  with tf.io.gfile.GFile(path, ""r"") as f:\n    return json.load(f)\n\n\ndef write_json(o, path):\n  if ""/"" in path:\n    tf.io.gfile.makedirs(path.rsplit(""/"", 1)[0])\n  with tf.io.gfile.GFile(path, ""w"") as f:\n    json.dump(o, f)\n\n\ndef load_pickle(path):\n  with tf.io.gfile.GFile(path, ""rb"") as f:\n    return pickle.load(f)\n\n\ndef write_pickle(o, path):\n  if ""/"" in path:\n    tf.io.gfile.makedirs(path.rsplit(""/"", 1)[0])\n  with tf.io.gfile.GFile(path, ""wb"") as f:\n    pickle.dump(o, f, -1)\n\n\ndef mkdir(path):\n  if not tf.io.gfile.exists(path):\n    tf.io.gfile.makedirs(path)\n\n\ndef rmrf(path):\n  if tf.io.gfile.exists(path):\n    tf.io.gfile.rmtree(path)\n\n\ndef rmkdir(path):\n  rmrf(path)\n  mkdir(path)\n\n\ndef log(*args):\n  msg = "" "".join(map(str, args))\n  sys.stdout.write(msg + ""\\n"")\n  sys.stdout.flush()\n\n\ndef log_config(config):\n  for key, value in sorted(config.__dict__.items()):\n    log(key, value)\n  log()\n\n\ndef heading(*args):\n  log(80 * ""="")\n  log(*args)\n  log(80 * ""="")\n\n\ndef nest_dict(d, prefixes, delim=""_""):\n  """"""Go from {prefix_key: value} to {prefix: {key: value}}.""""""\n  nested = {}\n  for k, v in d.items():\n    for prefix in prefixes:\n      if k.startswith(prefix + delim):\n        if prefix not in nested:\n          nested[prefix] = {}\n        nested[prefix][k.split(delim, 1)[1]] = v\n      else:\n        nested[k] = v\n  return nested\n\n\ndef flatten_dict(d, delim=""_""):\n  """"""Go from {prefix: {key: value}} to {prefix_key: value}.""""""\n  flattened = {}\n  for k, v in d.items():\n    if isinstance(v, dict):\n      for k2, v2 in v.items():\n        flattened[k + delim + k2] = v2\n    else:\n      flattened[k] = v\n  return flattened\n'"
finetune/classification/classification_metrics.py,0,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Evaluation metrics for classification tasks.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport numpy as np\nimport scipy\nimport sklearn\n\nfrom finetune import scorer\n\n\nclass SentenceLevelScorer(scorer.Scorer):\n  """"""Abstract scorer for classification/regression tasks.""""""\n\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(self):\n    super(SentenceLevelScorer, self).__init__()\n    self._total_loss = 0\n    self._true_labels = []\n    self._preds = []\n\n  def update(self, results):\n    super(SentenceLevelScorer, self).update(results)\n    self._total_loss += results[\'loss\']\n    self._true_labels.append(results[\'label_ids\'] if \'label_ids\' in results\n                             else results[\'targets\'])\n    self._preds.append(results[\'predictions\'])\n\n  def get_loss(self):\n    return self._total_loss / len(self._true_labels)\n\n\nclass AccuracyScorer(SentenceLevelScorer):\n\n  def _get_results(self):\n    correct, count = 0, 0\n    for y_true, pred in zip(self._true_labels, self._preds):\n      count += 1\n      correct += (1 if y_true == pred else 0)\n    return [\n        (\'accuracy\', 100.0 * correct / count),\n        (\'loss\', self.get_loss()),\n    ]\n\n\nclass F1Scorer(SentenceLevelScorer):\n  """"""Computes F1 for classification tasks.""""""\n\n  def __init__(self):\n    super(F1Scorer, self).__init__()\n    self._positive_label = 1\n\n  def _get_results(self):\n    n_correct, n_predicted, n_gold = 0, 0, 0\n    for y_true, pred in zip(self._true_labels, self._preds):\n      if pred == self._positive_label:\n        n_gold += 1\n        if pred == self._positive_label:\n          n_predicted += 1\n          if pred == y_true:\n            n_correct += 1\n    if n_correct == 0:\n      p, r, f1 = 0, 0, 0\n    else:\n      p = 100.0 * n_correct / n_predicted\n      r = 100.0 * n_correct / n_gold\n      f1 = 2 * p * r / (p + r)\n    return [\n        (\'precision\', p),\n        (\'recall\', r),\n        (\'f1\', f1),\n        (\'loss\', self.get_loss()),\n    ]\n\n\nclass MCCScorer(SentenceLevelScorer):\n\n  def _get_results(self):\n    return [\n        (\'mcc\', 100 * sklearn.metrics.matthews_corrcoef(\n            self._true_labels, self._preds)),\n        (\'loss\', self.get_loss()),\n    ]\n\n\nclass RegressionScorer(SentenceLevelScorer):\n\n  def _get_results(self):\n    preds = np.array(self._preds).flatten()\n    return [\n        (\'pearson\', 100.0 * scipy.stats.pearsonr(\n            self._true_labels, preds)[0]),\n        (\'spearman\', 100.0 * scipy.stats.spearmanr(\n            self._true_labels, preds)[0]),\n        (\'mse\', np.mean(np.square(np.array(self._true_labels) - self._preds))),\n        (\'loss\', self.get_loss()),\n    ]\n'"
finetune/classification/classification_tasks.py,11,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Text classification and regression tasks.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport csv\nimport os\nimport tensorflow.compat.v1 as tf\n\nimport configure_finetuning\nfrom finetune import feature_spec\nfrom finetune import task\nfrom finetune.classification import classification_metrics\nfrom model import tokenization\nfrom util import utils\n\n\nclass InputExample(task.Example):\n  """"""A single training/test example for simple sequence classification.""""""\n\n  def __init__(self, eid, task_name, text_a, text_b=None, label=None):\n    super(InputExample, self).__init__(task_name)\n    self.eid = eid\n    self.text_a = text_a\n    self.text_b = text_b\n    self.label = label\n\n\nclass SingleOutputTask(task.Task):\n  """"""Task with a single prediction per example (e.g., text classification).""""""\n\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(self, config: configure_finetuning.FinetuningConfig, name,\n               tokenizer):\n    super(SingleOutputTask, self).__init__(config, name)\n    self._tokenizer = tokenizer\n\n  def get_examples(self, split):\n    return self._create_examples(read_tsv(\n        os.path.join(self.config.raw_data_dir(self.name), split + "".tsv""),\n        max_lines=100 if self.config.debug else None), split)\n\n  @abc.abstractmethod\n  def _create_examples(self, lines, split):\n    pass\n\n  def featurize(self, example: InputExample, is_training, log=False):\n    """"""Turn an InputExample into a dict of features.""""""\n    tokens_a = self._tokenizer.tokenize(example.text_a)\n    tokens_b = None\n    if example.text_b:\n      tokens_b = self._tokenizer.tokenize(example.text_b)\n\n    if tokens_b:\n      # Modifies `tokens_a` and `tokens_b` in place so that the total\n      # length is less than the specified length.\n      # Account for [CLS], [SEP], [SEP] with ""- 3""\n      _truncate_seq_pair(tokens_a, tokens_b, self.config.max_seq_length - 3)\n    else:\n      # Account for [CLS] and [SEP] with ""- 2""\n      if len(tokens_a) > self.config.max_seq_length - 2:\n        tokens_a = tokens_a[0:(self.config.max_seq_length - 2)]\n\n    # The convention in BERT is:\n    # (a) For sequence pairs:\n    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n    #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n    # (b) For single sequences:\n    #  tokens:   [CLS] the dog is hairy . [SEP]\n    #  type_ids: 0     0   0   0  0     0 0\n    #\n    # Where ""type_ids"" are used to indicate whether this is the first\n    # sequence or the second sequence. The embedding vectors for `type=0` and\n    # `type=1` were learned during pre-training and are added to the wordpiece\n    # embedding vector (and position vector). This is not *strictly* necessary\n    # since the [SEP] token unambiguously separates the sequences, but it\n    # makes it easier for the model to learn the concept of sequences.\n    #\n    # For classification tasks, the first vector (corresponding to [CLS]) is\n    # used as the ""sentence vector"". Note that this only makes sense because\n    # the entire model is fine-tuned.\n    tokens = []\n    segment_ids = []\n    tokens.append(""[CLS]"")\n    segment_ids.append(0)\n    for token in tokens_a:\n      tokens.append(token)\n      segment_ids.append(0)\n    tokens.append(""[SEP]"")\n    segment_ids.append(0)\n\n    if tokens_b:\n      for token in tokens_b:\n        tokens.append(token)\n        segment_ids.append(1)\n      tokens.append(""[SEP]"")\n      segment_ids.append(1)\n\n    input_ids = self._tokenizer.convert_tokens_to_ids(tokens)\n\n    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n    # tokens are attended to.\n    input_mask = [1] * len(input_ids)\n\n    # Zero-pad up to the sequence length.\n    while len(input_ids) < self.config.max_seq_length:\n      input_ids.append(0)\n      input_mask.append(0)\n      segment_ids.append(0)\n\n    assert len(input_ids) == self.config.max_seq_length\n    assert len(input_mask) == self.config.max_seq_length\n    assert len(segment_ids) == self.config.max_seq_length\n\n    if log:\n      utils.log(""  Example {:}"".format(example.eid))\n      utils.log(""    tokens: {:}"".format("" "".join(\n          [tokenization.printable_text(x) for x in tokens])))\n      utils.log(""    input_ids: {:}"".format("" "".join(map(str, input_ids))))\n      utils.log(""    input_mask: {:}"".format("" "".join(map(str, input_mask))))\n      utils.log(""    segment_ids: {:}"".format("" "".join(map(str, segment_ids))))\n\n    eid = example.eid\n    features = {\n        ""input_ids"": input_ids,\n        ""input_mask"": input_mask,\n        ""segment_ids"": segment_ids,\n        ""task_id"": self.config.task_names.index(self.name),\n        self.name + ""_eid"": eid,\n    }\n    self._add_features(features, example, log)\n    return features\n\n  def _load_glue(self, lines, split, text_a_loc, text_b_loc, label_loc,\n                 skip_first_line=False, eid_offset=0, swap=False):\n    examples = []\n    for (i, line) in enumerate(lines):\n      try:\n        if i == 0 and skip_first_line:\n          continue\n        eid = i - (1 if skip_first_line else 0) + eid_offset\n        text_a = tokenization.convert_to_unicode(line[text_a_loc])\n        if text_b_loc is None:\n          text_b = None\n        else:\n          text_b = tokenization.convert_to_unicode(line[text_b_loc])\n        if ""test"" in split or ""diagnostic"" in split:\n          label = self._get_dummy_label()\n        else:\n          label = tokenization.convert_to_unicode(line[label_loc])\n        if swap:\n          text_a, text_b = text_b, text_a\n        examples.append(InputExample(eid=eid, task_name=self.name,\n                                     text_a=text_a, text_b=text_b, label=label))\n      except Exception as ex:\n        utils.log(""Error constructing example from line"", i,\n                  ""for task"", self.name + "":"", ex)\n        utils.log(""Input causing the error:"", line)\n    return examples\n\n  @abc.abstractmethod\n  def _get_dummy_label(self):\n    pass\n\n  @abc.abstractmethod\n  def _add_features(self, features, example, log):\n    pass\n\n\nclass RegressionTask(SingleOutputTask):\n  """"""Task where the output is a real-valued score for the input text.""""""\n\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(self, config: configure_finetuning.FinetuningConfig, name,\n               tokenizer, min_value, max_value):\n    super(RegressionTask, self).__init__(config, name, tokenizer)\n    self._tokenizer = tokenizer\n    self._min_value = min_value\n    self._max_value = max_value\n\n  def _get_dummy_label(self):\n    return 0.0\n\n  def get_feature_specs(self):\n    feature_specs = [feature_spec.FeatureSpec(self.name + ""_eid"", []),\n                     feature_spec.FeatureSpec(self.name + ""_targets"", [],\n                                              is_int_feature=False)]\n    return feature_specs\n\n  def _add_features(self, features, example, log):\n    label = float(example.label)\n    assert self._min_value <= label <= self._max_value\n    # simple normalization of the label\n    label = (label - self._min_value) / self._max_value\n    if log:\n      utils.log(""    label: {:}"".format(label))\n    features[example.task_name + ""_targets""] = label\n\n  def get_prediction_module(self, bert_model, features, is_training,\n                            percent_done):\n    reprs = bert_model.get_pooled_output()\n    if is_training:\n      reprs = tf.nn.dropout(reprs, keep_prob=0.9)\n\n    predictions = tf.layers.dense(reprs, 1)\n    predictions = tf.squeeze(predictions, -1)\n\n    targets = features[self.name + ""_targets""]\n    losses = tf.square(predictions - targets)\n    outputs = dict(\n        loss=losses,\n        predictions=predictions,\n        targets=features[self.name + ""_targets""],\n        eid=features[self.name + ""_eid""]\n    )\n    return losses, outputs\n\n  def get_scorer(self):\n    return classification_metrics.RegressionScorer()\n\n\nclass ClassificationTask(SingleOutputTask):\n  """"""Task where the output is a single categorical label for the input text.""""""\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(self, config: configure_finetuning.FinetuningConfig, name,\n               tokenizer, label_list):\n    super(ClassificationTask, self).__init__(config, name, tokenizer)\n    self._tokenizer = tokenizer\n    self._label_list = label_list\n\n  def _get_dummy_label(self):\n    return self._label_list[0]\n\n  def get_feature_specs(self):\n    return [feature_spec.FeatureSpec(self.name + ""_eid"", []),\n            feature_spec.FeatureSpec(self.name + ""_label_ids"", [])]\n\n  def _add_features(self, features, example, log):\n    label_map = {}\n    for (i, label) in enumerate(self._label_list):\n      label_map[label] = i\n    label_id = label_map[example.label]\n    if log:\n      utils.log(""    label: {:} (id = {:})"".format(example.label, label_id))\n    features[example.task_name + ""_label_ids""] = label_id\n\n  def get_prediction_module(self, bert_model, features, is_training,\n                            percent_done):\n    num_labels = len(self._label_list)\n    reprs = bert_model.get_pooled_output()\n\n    if is_training:\n      reprs = tf.nn.dropout(reprs, keep_prob=0.9)\n\n    logits = tf.layers.dense(reprs, num_labels)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    label_ids = features[self.name + ""_label_ids""]\n    labels = tf.one_hot(label_ids, depth=num_labels, dtype=tf.float32)\n\n    losses = -tf.reduce_sum(labels * log_probs, axis=-1)\n\n    outputs = dict(\n        loss=losses,\n        logits=logits,\n        predictions=tf.argmax(logits, axis=-1),\n        label_ids=label_ids,\n        eid=features[self.name + ""_eid""],\n    )\n    return losses, outputs\n\n  def get_scorer(self):\n    return classification_metrics.AccuracyScorer()\n\n\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n  """"""Truncates a sequence pair in place to the maximum length.""""""\n\n  # This is a simple heuristic which will always truncate the longer sequence\n  # one token at a time. This makes more sense than truncating an equal percent\n  # of tokens from each, since if one sequence is very short then each token\n  # that\'s truncated likely contains more information than a longer sequence.\n  while True:\n    total_length = len(tokens_a) + len(tokens_b)\n    if total_length <= max_length:\n      break\n    if len(tokens_a) > len(tokens_b):\n      tokens_a.pop()\n    else:\n      tokens_b.pop()\n\n\ndef read_tsv(input_file, quotechar=None, max_lines=None):\n  """"""Reads a tab separated value file.""""""\n  with tf.io.gfile.GFile(input_file, ""r"") as f:\n    reader = csv.reader(f, delimiter=""\\t"", quotechar=quotechar)\n    lines = []\n    for i, line in enumerate(reader):\n      if max_lines and i >= max_lines:\n        break\n      lines.append(line)\n    return lines\n\n\nclass MNLI(ClassificationTask):\n  """"""Multi-NLI.""""""\n\n  def __init__(self, config: configure_finetuning.FinetuningConfig, tokenizer):\n    super(MNLI, self).__init__(config, ""mnli"", tokenizer,\n                               [""contradiction"", ""entailment"", ""neutral""])\n\n  def get_examples(self, split):\n    if split == ""dev"":\n      split += ""_matched""\n    return self._create_examples(read_tsv(\n        os.path.join(self.config.raw_data_dir(self.name), split + "".tsv""),\n        max_lines=100 if self.config.debug else None), split)\n\n  def _create_examples(self, lines, split):\n    if split == ""diagnostic"":\n      return self._load_glue(lines, split, 1, 2, None, True)\n    else:\n      return self._load_glue(lines, split, 8, 9, -1, True)\n\n  def get_test_splits(self):\n    return [""test_matched"", ""test_mismatched"", ""diagnostic""]\n\n\nclass MRPC(ClassificationTask):\n  """"""Microsoft Research Paraphrase Corpus.""""""\n\n  def __init__(self, config: configure_finetuning.FinetuningConfig, tokenizer):\n    super(MRPC, self).__init__(config, ""mrpc"", tokenizer, [""0"", ""1""])\n\n  def _create_examples(self, lines, split):\n    examples = []\n    examples += self._load_glue(lines, split, 3, 4, 0, True)\n    if self.config.double_unordered and split == ""train"":\n      examples += self._load_glue(\n          lines, split, 3, 4, 0, True, len(examples), True)\n    return examples\n\n\nclass CoLA(ClassificationTask):\n  """"""Corpus of Linguistic Acceptability.""""""\n\n  def __init__(self, config: configure_finetuning.FinetuningConfig, tokenizer):\n    super(CoLA, self).__init__(config, ""cola"", tokenizer, [""0"", ""1""])\n\n  def _create_examples(self, lines, split):\n    return self._load_glue(lines, split, 1 if split == ""test"" else 3,\n                           None, 1, split == ""test"")\n\n  def get_scorer(self):\n    return classification_metrics.MCCScorer()\n\n\nclass SST(ClassificationTask):\n  """"""Stanford Sentiment Treebank.""""""\n\n  def __init__(self, config: configure_finetuning.FinetuningConfig, tokenizer):\n    super(SST, self).__init__(config, ""sst"", tokenizer, [""0"", ""1""])\n\n  def _create_examples(self, lines, split):\n    if ""test"" in split:\n      return self._load_glue(lines, split, 1, None, None, True)\n    else:\n      return self._load_glue(lines, split, 0, None, 1, True)\n\n\nclass QQP(ClassificationTask):\n  """"""Quora Question Pair.""""""\n\n  def __init__(self, config: configure_finetuning.FinetuningConfig, tokenizer):\n    super(QQP, self).__init__(config, ""qqp"", tokenizer, [""0"", ""1""])\n\n  def _create_examples(self, lines, split):\n    return self._load_glue(lines, split, 1 if split == ""test"" else 3,\n                           2 if split == ""test"" else 4, 5, True)\n\n\nclass RTE(ClassificationTask):\n  """"""Recognizing Textual Entailment.""""""\n\n  def __init__(self, config: configure_finetuning.FinetuningConfig, tokenizer):\n    super(RTE, self).__init__(config, ""rte"", tokenizer,\n                              [""entailment"", ""not_entailment""])\n\n  def _create_examples(self, lines, split):\n    return self._load_glue(lines, split, 1, 2, 3, True)\n\n\nclass QNLI(ClassificationTask):\n  """"""Question NLI.""""""\n\n  def __init__(self, config: configure_finetuning.FinetuningConfig, tokenizer):\n    super(QNLI, self).__init__(config, ""qnli"", tokenizer,\n                               [""entailment"", ""not_entailment""])\n\n  def _create_examples(self, lines, split):\n    return self._load_glue(lines, split, 1, 2, 3, True)\n\n\nclass STS(RegressionTask):\n  """"""Semantic Textual Similarity.""""""\n\n  def __init__(self, config: configure_finetuning.FinetuningConfig, tokenizer):\n    super(STS, self).__init__(config, ""sts"", tokenizer, 0.0, 5.0)\n\n  def _create_examples(self, lines, split):\n    examples = []\n    if split == ""test"":\n      examples += self._load_glue(lines, split, -2, -1, None, True)\n    else:\n      examples += self._load_glue(lines, split, -3, -2, -1, True)\n    if self.config.double_unordered and split == ""train"":\n      examples += self._load_glue(\n          lines, split, -3, -2, -1, True, len(examples), True)\n    return examples\n\n#\nclass BQCorpus(ClassificationTask):\n  """"""BQCorpus.""""""\n\n  def __init__(self, config: configure_finetuning.FinetuningConfig, tokenizer):\n    super(BQCorpus, self).__init__(config, ""bqcorpus"", tokenizer,\n                               [""0"", ""1""])\n\n  def get_examples(self, split):\n    return self._create_examples(read_tsv(\n        os.path.join(self.config.raw_data_dir(self.name), split + "".tsv""),\n        max_lines=100 if self.config.debug else None), split)\n\n  def _create_examples(self, lines, split):\n    return self._load_glue(lines, split, 0, 1, 2, skip_first_line=True)\n\n#\nclass ChnSentiCorp(ClassificationTask):\n  """"""ChnSentiCorp.""""""\n\n  def __init__(self, config: configure_finetuning.FinetuningConfig, tokenizer):\n    super(ChnSentiCorp, self).__init__(config, ""chnsenticorp"", tokenizer,\n                               [""0"", ""1""])\n\n  def get_examples(self, split):\n    return self._create_examples(read_tsv(\n        os.path.join(self.config.raw_data_dir(self.name), split + "".tsv""),\n        max_lines=100 if self.config.debug else None), split)\n\n  def _create_examples(self, lines, split):\n    return self._load_glue(lines, split, 1, None, 0, skip_first_line=True)\n\n\n#\nclass XNLI(ClassificationTask):\n  """"""XNLI.""""""\n\n  def __init__(self, config: configure_finetuning.FinetuningConfig, tokenizer):\n    super(XNLI, self).__init__(config, ""xnli"", tokenizer,\n                               [""contradictory"", ""entailment"", ""neutral""])\n\n  def get_examples(self, split):\n    return self._create_examples(read_tsv(\n        os.path.join(self.config.raw_data_dir(self.name), split + "".tsv""),\n        max_lines=100 if self.config.debug else None), split)\n\n  def _create_examples(self, lines, split):\n    if split != ""train"":\n      return self._load_glue(lines, split, 6, 7, 1, skip_first_line=True)\n    else:\n      return self._load_glue(lines, split, 0, 1, 2, skip_first_line=True)\n\n\n#\nclass LCQMC(ClassificationTask):\n  """"""LCQMC.""""""\n\n  def __init__(self, config: configure_finetuning.FinetuningConfig, tokenizer):\n    super(LCQMC, self).__init__(config, ""lcqmc"", tokenizer,\n                               [""0"", ""1""])\n\n  def get_examples(self, split):\n    return self._create_examples(read_tsv(\n        os.path.join(self.config.raw_data_dir(self.name), split + "".tsv""),\n        max_lines=100 if self.config.debug else None), split)\n\n  def _create_examples(self, lines, split):\n    return self._load_glue(lines, split, 0, 1, 2, skip_first_line=True)\n\n'"
finetune/qa/mrqa_official_eval.py,2,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Official evaluation script for the MRQA Workshop Shared Task.\nAdapted fromt the SQuAD v1.1 official evaluation script.\nModified slightly for the ELECTRA codebase.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport string\nimport re\nimport json\nimport tensorflow.compat.v1 as tf\nfrom collections import Counter\n\nimport configure_finetuning\n\n\ndef normalize_answer(s):\n  """"""Lower text and remove punctuation, articles and extra whitespace.""""""\n  def remove_articles(text):\n    return re.sub(r\'\\b(a|an|the)\\b\', \' \', text)\n\n  def white_space_fix(text):\n    return \' \'.join(text.split())\n\n  def remove_punc(text):\n    exclude = set(string.punctuation)\n    return \'\'.join(ch for ch in text if ch not in exclude)\n\n  def lower(text):\n    return text.lower()\n\n  return white_space_fix(remove_articles(remove_punc(lower(s))))\n\n\ndef f1_score(prediction, ground_truth):\n  prediction_tokens = normalize_answer(prediction).split()\n  ground_truth_tokens = normalize_answer(ground_truth).split()\n  common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n  num_same = sum(common.values())\n  if num_same == 0:\n    return 0\n  precision = 1.0 * num_same / len(prediction_tokens)\n  recall = 1.0 * num_same / len(ground_truth_tokens)\n  f1 = (2 * precision * recall) / (precision + recall)\n  return f1\n\n\ndef exact_match_score(prediction, ground_truth):\n  return (normalize_answer(prediction) == normalize_answer(ground_truth))\n\n\ndef metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n  scores_for_ground_truths = []\n  for ground_truth in ground_truths:\n    score = metric_fn(prediction, ground_truth)\n    scores_for_ground_truths.append(score)\n  return max(scores_for_ground_truths)\n\n\ndef read_predictions(prediction_file):\n  with tf.io.gfile.GFile(prediction_file) as f:\n    predictions = json.load(f)\n  return predictions\n\n\ndef read_answers(gold_file):\n  answers = {}\n  with tf.io.gfile.GFile(gold_file, \'r\') as f:\n    for i, line in enumerate(f):\n      example = json.loads(line)\n      if i == 0 and \'header\' in example:\n        continue\n      for qa in example[\'qas\']:\n        answers[qa[\'qid\']] = qa[\'answers\']\n  return answers\n\n\ndef evaluate(answers, predictions, skip_no_answer=False):\n  f1 = exact_match = total = 0\n  for qid, ground_truths in answers.items():\n    if qid not in predictions:\n      if not skip_no_answer:\n        message = \'Unanswered question %s will receive score 0.\' % qid\n        print(message)\n        total += 1\n      continue\n    total += 1\n    prediction = predictions[qid]\n    exact_match += metric_max_over_ground_truths(\n        exact_match_score, prediction, ground_truths)\n    f1 += metric_max_over_ground_truths(\n        f1_score, prediction, ground_truths)\n\n  exact_match = 100.0 * exact_match / total\n  f1 = 100.0 * f1 / total\n\n  return {\'exact_match\': exact_match, \'f1\': f1}\n\n\ndef main(config: configure_finetuning.FinetuningConfig, split, task_name):\n  answers = read_answers(os.path.join(config.raw_data_dir(task_name), split + "".jsonl""))\n  predictions = read_predictions(config.qa_preds_file(task_name))\n  return evaluate(answers, predictions, True)\n'"
finetune/qa/qa_metrics.py,0,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Evaluation metrics for question-answering tasks.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport numpy as np\nimport six\n\nimport configure_finetuning\nfrom finetune import scorer\nfrom finetune.qa import mrqa_official_eval\nfrom finetune.qa import squad_official_eval\nfrom finetune.qa import squad_official_eval_v1\nfrom model import tokenization\nfrom util import utils\n\n\nRawResult = collections.namedtuple(""RawResult"", [\n    ""unique_id"", ""start_logits"", ""end_logits"", ""answerable_logit"",\n    ""start_top_log_probs"", ""start_top_index"", ""end_top_log_probs"",\n    ""end_top_index""\n])\n\n\nclass SpanBasedQAScorer(scorer.Scorer):\n  """"""Runs evaluation for SQuAD 1.1, SQuAD 2.0, and MRQA tasks.""""""\n\n  def __init__(self, config: configure_finetuning.FinetuningConfig, task, split,\n               v2):\n    super(SpanBasedQAScorer, self).__init__()\n    self._config = config\n    self._task = task\n    self._name = task.name\n    self._split = split\n    self._v2 = v2\n    self._all_results = []\n    self._total_loss = 0\n    self._split = split\n    self._eval_examples = task.get_examples(split)\n\n  def update(self, results):\n    super(SpanBasedQAScorer, self).update(results)\n    self._all_results.append(\n        RawResult(\n            unique_id=results[""eid""],\n            start_logits=results[""start_logits""],\n            end_logits=results[""end_logits""],\n            answerable_logit=results[""answerable_logit""],\n            start_top_log_probs=results[""start_top_log_probs""],\n            start_top_index=results[""start_top_index""],\n            end_top_log_probs=results[""end_top_log_probs""],\n            end_top_index=results[""end_top_index""],\n        ))\n    self._total_loss += results[""loss""]\n\n  def get_loss(self):\n    return self._total_loss / len(self._all_results)\n\n  def _get_results(self):\n    self.write_predictions()\n    if self._name == ""squad"":\n      squad_official_eval.set_opts(self._config, self._split)\n      squad_official_eval.main()\n      return sorted(utils.load_json(\n          self._config.qa_eval_file(self._name)).items())\n    elif self._name == ""squadv1"" or self._name == ""cmrc2018"" or self._name == ""drcd"":\n      return sorted(squad_official_eval_v1.main(\n          self._config, self._split, self._name).items())\n    else:\n      return sorted(mrqa_official_eval.main(\n          self._config, self._split, self._name).items())\n\n  def write_predictions(self):\n    """"""Write final predictions to the json file.""""""\n    unique_id_to_result = {}\n    for result in self._all_results:\n      unique_id_to_result[result.unique_id] = result\n\n    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n        ""PrelimPrediction"",\n        [""feature_index"", ""start_index"", ""end_index"", ""start_logit"",\n         ""end_logit""])\n\n    all_predictions = collections.OrderedDict()\n    all_nbest_json = collections.OrderedDict()\n    scores_diff_json = collections.OrderedDict()\n\n    for example in self._eval_examples:\n      example_id = example.qas_id if (""squad"" in self._name or ""cmrc2018"" in self._name or ""drcd"" in self._name) else example.qid\n      features = self._task.featurize(example, False, for_eval=True)\n\n      prelim_predictions = []\n      # keep track of the minimum score of null start+end of position 0\n      score_null = 1000000  # large and positive\n      for (feature_index, feature) in enumerate(features):\n        result = unique_id_to_result[feature[self._name + ""_eid""]]\n        if self._config.joint_prediction:\n          start_indexes = result.start_top_index\n          end_indexes = result.end_top_index\n        else:\n          start_indexes = _get_best_indexes(result.start_logits,\n                                            self._config.n_best_size)\n          end_indexes = _get_best_indexes(result.end_logits,\n                                          self._config.n_best_size)\n        # if we could have irrelevant answers, get the min score of irrelevant\n        if self._v2:\n          if self._config.answerable_classifier:\n            feature_null_score = result.answerable_logit\n          else:\n            feature_null_score = result.start_logits[0] + result.end_logits[0]\n          if feature_null_score < score_null:\n            score_null = feature_null_score\n        for i, start_index in enumerate(start_indexes):\n          for j, end_index in enumerate(\n              end_indexes[i] if self._config.joint_prediction else end_indexes):\n            # We could hypothetically create invalid predictions, e.g., predict\n            # that the start of the span is in the question. We throw out all\n            # invalid predictions.\n            if start_index >= len(feature[self._name + ""_tokens""]):\n              continue\n            if end_index >= len(feature[self._name + ""_tokens""]):\n              continue\n            if start_index == 0:\n              continue\n            if start_index not in feature[self._name + ""_token_to_orig_map""]:\n              continue\n            if end_index not in feature[self._name + ""_token_to_orig_map""]:\n              continue\n            if not feature[self._name + ""_token_is_max_context""].get(\n                start_index, False):\n              continue\n            if end_index < start_index:\n              continue\n            length = end_index - start_index + 1\n            if length > self._config.max_answer_length:\n              continue\n            start_logit = (result.start_top_log_probs[i] if\n                           self._config.joint_prediction else\n                           result.start_logits[start_index])\n            end_logit = (result.end_top_log_probs[i, j] if\n                         self._config.joint_prediction else\n                         result.end_logits[end_index])\n            prelim_predictions.append(\n                _PrelimPrediction(\n                    feature_index=feature_index,\n                    start_index=start_index,\n                    end_index=end_index,\n                    start_logit=start_logit,\n                    end_logit=end_logit))\n\n      if self._v2:\n        if len(prelim_predictions) == 0 and self._config.debug:\n          tokid = sorted(feature[self._name + ""_token_to_orig_map""].keys())[0]\n          prelim_predictions.append(_PrelimPrediction(\n              feature_index=0,\n              start_index=tokid,\n              end_index=tokid + 1,\n              start_logit=1.0,\n              end_logit=1.0))\n      prelim_predictions = sorted(\n          prelim_predictions,\n          key=lambda x: (x.start_logit + x.end_logit),\n          reverse=True)\n\n      _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n          ""NbestPrediction"", [""text"", ""start_logit"", ""end_logit""])\n\n      seen_predictions = {}\n      nbest = []\n      for pred in prelim_predictions:\n        if len(nbest) >= self._config.n_best_size:\n          break\n        feature = features[pred.feature_index]\n        tok_tokens = feature[self._name + ""_tokens""][\n            pred.start_index:(pred.end_index + 1)]\n        orig_doc_start = feature[\n            self._name + ""_token_to_orig_map""][pred.start_index]\n        orig_doc_end = feature[\n            self._name + ""_token_to_orig_map""][pred.end_index]\n        orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n        tok_text = "" "".join(tok_tokens)\n\n        # De-tokenize WordPieces that have been split off.\n        tok_text = tok_text.replace("" ##"", """")\n        tok_text = tok_text.replace(""##"", """")\n\n        # Clean whitespace\n        tok_text = tok_text.strip()\n        tok_text = "" "".join(tok_text.split())\n        orig_text = "" "".join(orig_tokens)\n\n        final_text = get_final_text(self._config, tok_text, orig_text)\n        if final_text in seen_predictions:\n          continue\n\n        seen_predictions[final_text] = True\n\n        nbest.append(\n            _NbestPrediction(\n                text=final_text,\n                start_logit=pred.start_logit,\n                end_logit=pred.end_logit))\n\n      # In very rare edge cases we could have no valid predictions. So we\n      # just create a nonce prediction in this case to avoid failure.\n      if not nbest:\n        nbest.append(\n            _NbestPrediction(text=""empty"", start_logit=0.0, end_logit=0.0))\n\n      assert len(nbest) >= 1\n\n      total_scores = []\n      best_non_null_entry = None\n      for entry in nbest:\n        total_scores.append(entry.start_logit + entry.end_logit)\n        if not best_non_null_entry:\n          if entry.text:\n            best_non_null_entry = entry\n\n      probs = _compute_softmax(total_scores)\n\n      nbest_json = []\n      for (i, entry) in enumerate(nbest):\n        output = collections.OrderedDict()\n        output[""text""] = entry.text\n        output[""probability""] = probs[i]\n        output[""start_logit""] = entry.start_logit\n        output[""end_logit""] = entry.end_logit\n        nbest_json.append(dict(output))\n\n      assert len(nbest_json) >= 1\n\n      if not self._v2:\n        all_predictions[example_id] = nbest_json[0][""text""]\n      else:\n        # predict """" iff the null score - the score of best non-null > threshold\n        if self._config.answerable_classifier:\n          score_diff = score_null\n        else:\n          score_diff = score_null - best_non_null_entry.start_logit - (\n              best_non_null_entry.end_logit)\n        scores_diff_json[example_id] = score_diff\n        all_predictions[example_id] = best_non_null_entry.text\n\n      all_nbest_json[example_id] = nbest_json\n\n    utils.write_json(dict(all_predictions),\n                     self._config.qa_preds_file(self._name+""_""+self._split))\n    if self._v2:\n      utils.write_json({\n          k: float(v) for k, v in six.iteritems(scores_diff_json)},\n          self._config.qa_na_file(self._name+""_""+self._split))\n\n\ndef _get_best_indexes(logits, n_best_size):\n  """"""Get the n-best logits from a list.""""""\n  index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n\n  best_indexes = []\n  for i in range(len(index_and_score)):\n    if i >= n_best_size:\n      break\n    best_indexes.append(index_and_score[i][0])\n  return best_indexes\n\n\ndef _compute_softmax(scores):\n  """"""Compute softmax probability over raw logits.""""""\n  if not scores:\n    return []\n\n  max_score = None\n  for score in scores:\n    if max_score is None or score > max_score:\n      max_score = score\n\n  exp_scores = []\n  total_sum = 0.0\n  for score in scores:\n    x = np.exp(score - max_score)\n    exp_scores.append(x)\n    total_sum += x\n\n  probs = []\n  for score in exp_scores:\n    probs.append(score / total_sum)\n  return probs\n\n\ndef get_final_text(config: configure_finetuning.FinetuningConfig, pred_text,\n                   orig_text):\n  """"""Project the tokenized prediction back to the original text.""""""\n\n  # When we created the data, we kept track of the alignment between original\n  # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n  # now `orig_text` contains the span of our original text corresponding to the\n  # span that we predicted.\n  #\n  # However, `orig_text` may contain extra characters that we don\'t want in\n  # our prediction.\n  #\n  # For example, let\'s say:\n  #   pred_text = steve smith\n  #   orig_text = Steve Smith\'s\n  #\n  # We don\'t want to return `orig_text` because it contains the extra ""\'s"".\n  #\n  # We don\'t want to return `pred_text` because it\'s already been normalized\n  # (the SQuAD eval script also does punctuation stripping/lower casing but\n  # our tokenizer does additional normalization like stripping accent\n  # characters).\n  #\n  # What we really want to return is ""Steve Smith"".\n  #\n  # Therefore, we have to apply a semi-complicated alignment heruistic between\n  # `pred_text` and `orig_text` to get a character-to-charcter alignment. This\n  # can fail in certain cases in which case we just return `orig_text`.\n\n  def _strip_spaces(text):\n    ns_chars = []\n    ns_to_s_map = collections.OrderedDict()\n    for i, c in enumerate(text):\n      if c == "" "":\n        continue\n      ns_to_s_map[len(ns_chars)] = i\n      ns_chars.append(c)\n    ns_text = """".join(ns_chars)\n    return ns_text, dict(ns_to_s_map)\n\n  # We first tokenize `orig_text`, strip whitespace from the result\n  # and `pred_text`, and check if they are the same length. If they are\n  # NOT the same length, the heuristic has failed. If they are the same\n  # length, we assume the characters are one-to-one aligned.\n  tokenizer = tokenization.BasicTokenizer(do_lower_case=config.do_lower_case)\n\n  tok_text = "" "".join(tokenizer.tokenize(orig_text))\n\n  start_position = tok_text.find(pred_text)\n  if start_position == -1:\n    if config.debug:\n      utils.log(\n          ""Unable to find text: \'%s\' in \'%s\'"" % (pred_text, orig_text))\n    return orig_text\n  end_position = start_position + len(pred_text) - 1\n\n  (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n  (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n\n  if len(orig_ns_text) != len(tok_ns_text):\n    if config.debug:\n      utils.log(""Length not equal after stripping spaces: \'%s\' vs \'%s\'"",\n                orig_ns_text, tok_ns_text)\n    return orig_text\n\n  # We then project the characters in `pred_text` back to `orig_text` using\n  # the character-to-character alignment.\n  tok_s_to_ns_map = {}\n  for (i, tok_index) in six.iteritems(tok_ns_to_s_map):\n    tok_s_to_ns_map[tok_index] = i\n\n  orig_start_position = None\n  if start_position in tok_s_to_ns_map:\n    ns_start_position = tok_s_to_ns_map[start_position]\n    if ns_start_position in orig_ns_to_s_map:\n      orig_start_position = orig_ns_to_s_map[ns_start_position]\n\n  if orig_start_position is None:\n    if config.debug:\n      utils.log(""Couldn\'t map start position"")\n    return orig_text\n\n  orig_end_position = None\n  if end_position in tok_s_to_ns_map:\n    ns_end_position = tok_s_to_ns_map[end_position]\n    if ns_end_position in orig_ns_to_s_map:\n      orig_end_position = orig_ns_to_s_map[ns_end_position]\n\n  if orig_end_position is None:\n    if config.debug:\n      utils.log(""Couldn\'t map end position"")\n    return orig_text\n\n  output_text = orig_text[orig_start_position:(orig_end_position + 1)]\n  return output_text\n'"
finetune/qa/qa_tasks.py,44,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Question answering tasks. SQuAD 1.1/2.0 and 2019 MRQA tasks are supported.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport collections\nimport json\nimport os\nimport six\nimport tensorflow.compat.v1 as tf\n\nimport configure_finetuning\nfrom finetune import feature_spec\nfrom finetune import task\nfrom finetune.qa import qa_metrics\nfrom model import modeling\nfrom model import tokenization\nfrom util import utils\n\n\nclass QAExample(task.Example):\n  """"""Question-answering example.""""""\n\n  def __init__(self,\n               task_name,\n               eid,\n               qas_id,\n               qid,\n               question_text,\n               doc_tokens,\n               orig_answer_text=None,\n               start_position=None,\n               end_position=None,\n               is_impossible=False):\n    super(QAExample, self).__init__(task_name)\n    self.eid = eid\n    self.qas_id = qas_id\n    self.qid = qid\n    self.question_text = question_text\n    self.doc_tokens = doc_tokens\n    self.orig_answer_text = orig_answer_text\n    self.start_position = start_position\n    self.end_position = end_position\n    self.is_impossible = is_impossible\n\n  def __str__(self):\n    return self.__repr__()\n\n  def __repr__(self):\n    s = """"\n    s += ""qas_id: %s"" % (tokenization.printable_text(self.qas_id))\n    s += "", question_text: %s"" % (\n        tokenization.printable_text(self.question_text))\n    s += "", doc_tokens: [%s]"" % ("" "".join(self.doc_tokens))\n    if self.start_position:\n      s += "", start_position: %d"" % self.start_position\n    if self.start_position:\n      s += "", end_position: %d"" % self.end_position\n    if self.start_position:\n      s += "", is_impossible: %r"" % self.is_impossible\n    return s\n\n\ndef _check_is_max_context(doc_spans, cur_span_index, position):\n  """"""Check if this is the \'max context\' doc span for the token.""""""\n\n  # Because of the sliding window approach taken to scoring documents, a single\n  # token can appear in multiple documents. E.g.\n  #  Doc: the man went to the store and bought a gallon of milk\n  #  Span A: the man went to the\n  #  Span B: to the store and bought\n  #  Span C: and bought a gallon of\n  #  ...\n  #\n  # Now the word \'bought\' will have two scores from spans B and C. We only\n  # want to consider the score with ""maximum context"", which we define as\n  # the *minimum* of its left and right context (the *sum* of left and\n  # right context will always be the same, of course).\n  #\n  # In the example the maximum context for \'bought\' would be span C since\n  # it has 1 left context and 3 right context, while span B has 4 left context\n  # and 0 right context.\n  best_score = None\n  best_span_index = None\n  for (span_index, doc_span) in enumerate(doc_spans):\n    end = doc_span.start + doc_span.length - 1\n    if position < doc_span.start:\n      continue\n    if position > end:\n      continue\n    num_left_context = position - doc_span.start\n    num_right_context = end - position\n    score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n    if best_score is None or score > best_score:\n      best_score = score\n      best_span_index = span_index\n\n  return cur_span_index == best_span_index\n\n\ndef _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text):\n  """"""Returns tokenized answer spans that better match the annotated answer.""""""\n\n  # The SQuAD annotations are character based. We first project them to\n  # whitespace-tokenized words. But then after WordPiece tokenization, we can\n  # often find a ""better match"". For example:\n  #\n  #   Question: What year was John Smith born?\n  #   Context: The leader was John Smith (1895-1943).\n  #   Answer: 1895\n  #\n  # The original whitespace-tokenized answer will be ""(1895-1943)."". However\n  # after tokenization, our tokens will be ""( 1895 - 1943 ) ."". So we can match\n  # the exact answer, 1895.\n  #\n  # However, this is not always possible. Consider the following:\n  #\n  #   Question: What country is the top exporter of electornics?\n  #   Context: The Japanese electronics industry is the lagest in the world.\n  #   Answer: Japan\n  #\n  # In this case, the annotator chose ""Japan"" as a character sub-span of\n  # the word ""Japanese"". Since our WordPiece tokenizer does not split\n  # ""Japanese"", we just use ""Japanese"" as the annotation. This is fairly rare\n  # in SQuAD, but does happen.\n  tok_answer_text = "" "".join(tokenizer.tokenize(orig_answer_text))\n\n  for new_start in range(input_start, input_end + 1):\n    for new_end in range(input_end, new_start - 1, -1):\n      text_span = "" "".join(doc_tokens[new_start:(new_end + 1)])\n      if text_span == tok_answer_text:\n        return new_start, new_end\n\n  return input_start, input_end\n\n\ndef is_whitespace(c):\n  return c == "" "" or c == ""\\t"" or c == ""\\r"" or c == ""\\n"" or ord(c) == 0x202F\n\n\nclass QATask(task.Task):\n  """"""A span-based question answering tasks (e.g., SQuAD).""""""\n\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(self, config: configure_finetuning.FinetuningConfig, name,\n               tokenizer, v2=False):\n    super(QATask, self).__init__(config, name)\n    self._tokenizer = tokenizer\n    self._examples = {}\n    self.v2 = v2\n\n  def _add_examples(self, examples, example_failures, paragraph, split):\n    paragraph_text = paragraph[""context""]\n    doc_tokens = []\n    char_to_word_offset = []\n    prev_is_whitespace = True\n    for c in paragraph_text:\n      if is_whitespace(c):\n        prev_is_whitespace = True\n      else:\n        if prev_is_whitespace:\n          doc_tokens.append(c)\n        else:\n          doc_tokens[-1] += c\n        prev_is_whitespace = False\n      char_to_word_offset.append(len(doc_tokens) - 1)\n\n    for qa in paragraph[""qas""]:\n      qas_id = qa[""id""] if ""id"" in qa else None\n      qid = qa[""qid""] if ""qid"" in qa else None\n      question_text = qa[""question""]\n      start_position = None\n      end_position = None\n      orig_answer_text = None\n      is_impossible = False\n      if split == ""train"":\n        if self.v2:\n          is_impossible = qa[""is_impossible""]\n        if not is_impossible:\n          if ""detected_answers"" in qa:  # MRQA format\n            answer = qa[""detected_answers""][0]\n            answer_offset = answer[""char_spans""][0][0]\n          else:  # SQuAD format\n            answer = qa[""answers""][0]\n            answer_offset = answer[""answer_start""]\n          orig_answer_text = answer[""text""]\n          answer_length = len(orig_answer_text)\n          start_position = char_to_word_offset[answer_offset]\n          if answer_offset + answer_length - 1 >= len(char_to_word_offset):\n            utils.log(""End position is out of document!"")\n            example_failures[0] += 1\n            continue\n          end_position = char_to_word_offset[answer_offset + answer_length - 1]\n\n          # Only add answers where the text can be exactly recovered from the\n          # document. If this CAN\'T happen it\'s likely due to weird Unicode\n          # stuff so we will just skip the example.\n          #\n          # Note that this means for training mode, every example is NOT\n          # guaranteed to be preserved.\n          actual_text = "" "".join(\n              doc_tokens[start_position:(end_position + 1)])\n          cleaned_answer_text = "" "".join(\n              tokenization.whitespace_tokenize(orig_answer_text))\n          actual_text = actual_text.lower()\n          cleaned_answer_text = cleaned_answer_text.lower()\n          if actual_text.find(cleaned_answer_text) == -1:\n            utils.log(""Could not find answer: \'{:}\' in doc vs. ""\n                      ""\'{:}\' in provided answer"".format(\n                          tokenization.printable_text(actual_text),\n                          tokenization.printable_text(cleaned_answer_text)))\n            example_failures[0] += 1\n            continue\n        else:\n          start_position = -1\n          end_position = -1\n          orig_answer_text = """"\n\n      example = QAExample(\n          task_name=self.name,\n          eid=len(examples),\n          qas_id=qas_id,\n          qid=qid,\n          question_text=question_text,\n          doc_tokens=doc_tokens,\n          orig_answer_text=orig_answer_text,\n          start_position=start_position,\n          end_position=end_position,\n          is_impossible=is_impossible)\n      examples.append(example)\n\n  def get_feature_specs(self):\n    return [\n        feature_spec.FeatureSpec(self.name + ""_eid"", []),\n        feature_spec.FeatureSpec(self.name + ""_start_positions"", []),\n        feature_spec.FeatureSpec(self.name + ""_end_positions"", []),\n        feature_spec.FeatureSpec(self.name + ""_is_impossible"", []),\n    ]\n\n  def featurize(self, example: QAExample, is_training, log=False,\n                for_eval=False):\n    all_features = []\n    query_tokens = self._tokenizer.tokenize(example.question_text)\n\n    if len(query_tokens) > self.config.max_query_length:\n      query_tokens = query_tokens[0:self.config.max_query_length]\n\n    tok_to_orig_index = []\n    orig_to_tok_index = []\n    all_doc_tokens = []\n    for (i, token) in enumerate(example.doc_tokens):\n      orig_to_tok_index.append(len(all_doc_tokens))\n      sub_tokens = self._tokenizer.tokenize(token)\n      for sub_token in sub_tokens:\n        tok_to_orig_index.append(i)\n        all_doc_tokens.append(sub_token)\n\n    tok_start_position = None\n    tok_end_position = None\n    if is_training and example.is_impossible:\n      tok_start_position = -1\n      tok_end_position = -1\n    if is_training and not example.is_impossible:\n      tok_start_position = orig_to_tok_index[example.start_position]\n      if example.end_position < len(example.doc_tokens) - 1:\n        tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n      else:\n        tok_end_position = len(all_doc_tokens) - 1\n      (tok_start_position, tok_end_position) = _improve_answer_span(\n          all_doc_tokens, tok_start_position, tok_end_position, self._tokenizer,\n          example.orig_answer_text)\n\n    # The -3 accounts for [CLS], [SEP] and [SEP]\n    max_tokens_for_doc = self.config.max_seq_length - len(query_tokens) - 3\n\n    # We can have documents that are longer than the maximum sequence length.\n    # To deal with this we do a sliding window approach, where we take chunks\n    # of the up to our max length with a stride of `doc_stride`.\n    _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n        ""DocSpan"", [""start"", ""length""])\n    doc_spans = []\n    start_offset = 0\n    while start_offset < len(all_doc_tokens):\n      length = len(all_doc_tokens) - start_offset\n      if length > max_tokens_for_doc:\n        length = max_tokens_for_doc\n      doc_spans.append(_DocSpan(start=start_offset, length=length))\n      if start_offset + length == len(all_doc_tokens):\n        break\n      start_offset += min(length, self.config.doc_stride)\n\n    for (doc_span_index, doc_span) in enumerate(doc_spans):\n      tokens = []\n      token_to_orig_map = {}\n      token_is_max_context = {}\n      segment_ids = []\n      tokens.append(""[CLS]"")\n      segment_ids.append(0)\n      for token in query_tokens:\n        tokens.append(token)\n        segment_ids.append(0)\n      tokens.append(""[SEP]"")\n      segment_ids.append(0)\n\n      for i in range(doc_span.length):\n        split_token_index = doc_span.start + i\n        token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n\n        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n                                               split_token_index)\n        token_is_max_context[len(tokens)] = is_max_context\n        tokens.append(all_doc_tokens[split_token_index])\n        segment_ids.append(1)\n      tokens.append(""[SEP]"")\n      segment_ids.append(1)\n\n      input_ids = self._tokenizer.convert_tokens_to_ids(tokens)\n\n      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n      # tokens are attended to.\n      input_mask = [1] * len(input_ids)\n\n      # Zero-pad up to the sequence length.\n      while len(input_ids) < self.config.max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n      assert len(input_ids) == self.config.max_seq_length\n      assert len(input_mask) == self.config.max_seq_length\n      assert len(segment_ids) == self.config.max_seq_length\n\n      start_position = None\n      end_position = None\n      if is_training and not example.is_impossible:\n        # For training, if our document chunk does not contain an annotation\n        # we throw it out, since there is nothing to predict.\n        doc_start = doc_span.start\n        doc_end = doc_span.start + doc_span.length - 1\n        out_of_span = False\n        if not (tok_start_position >= doc_start and\n                tok_end_position <= doc_end):\n          out_of_span = True\n        if out_of_span:\n          start_position = 0\n          end_position = 0\n        else:\n          doc_offset = len(query_tokens) + 2\n          start_position = tok_start_position - doc_start + doc_offset\n          end_position = tok_end_position - doc_start + doc_offset\n\n      if is_training and example.is_impossible:\n        start_position = 0\n        end_position = 0\n\n      if log:\n        utils.log(""*** Example ***"")\n        utils.log(""doc_span_index: %s"" % doc_span_index)\n        utils.log(""tokens: %s"" % "" "".join(\n            [tokenization.printable_text(x) for x in tokens]))\n        utils.log(""token_to_orig_map: %s"" % "" "".join(\n            [""%d:%d"" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n        utils.log(""token_is_max_context: %s"" % "" "".join([\n            ""%d:%s"" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n        ]))\n        utils.log(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))\n        utils.log(""input_mask: %s"" % "" "".join([str(x) for x in input_mask]))\n        utils.log(""segment_ids: %s"" % "" "".join([str(x) for x in segment_ids]))\n        if is_training and example.is_impossible:\n          utils.log(""impossible example"")\n        if is_training and not example.is_impossible:\n          answer_text = "" "".join(tokens[start_position:(end_position + 1)])\n          utils.log(""start_position: %d"" % start_position)\n          utils.log(""end_position: %d"" % end_position)\n          utils.log(""answer: %s"" % (tokenization.printable_text(answer_text)))\n\n      features = {\n          ""task_id"": self.config.task_names.index(self.name),\n          self.name + ""_eid"": (1000 * example.eid) + doc_span_index,\n          ""input_ids"": input_ids,\n          ""input_mask"": input_mask,\n          ""segment_ids"": segment_ids,\n      }\n      if for_eval:\n        features.update({\n            self.name + ""_doc_span_index"": doc_span_index,\n            self.name + ""_tokens"": tokens,\n            self.name + ""_token_to_orig_map"": token_to_orig_map,\n            self.name + ""_token_is_max_context"": token_is_max_context,\n        })\n      if is_training:\n        features.update({\n            self.name + ""_start_positions"": start_position,\n            self.name + ""_end_positions"": end_position,\n            self.name + ""_is_impossible"": example.is_impossible\n        })\n      all_features.append(features)\n    return all_features\n\n  def get_prediction_module(self, bert_model, features, is_training,\n                            percent_done):\n    final_hidden = bert_model.get_sequence_output()\n\n    final_hidden_shape = modeling.get_shape_list(final_hidden, expected_rank=3)\n    batch_size = final_hidden_shape[0]\n    seq_length = final_hidden_shape[1]\n\n    answer_mask = tf.cast(features[""input_mask""], tf.float32)\n    answer_mask *= tf.cast(features[""segment_ids""], tf.float32)\n    answer_mask += tf.one_hot(0, seq_length)\n\n    start_logits = tf.squeeze(tf.layers.dense(final_hidden, 1), -1)\n\n    start_top_log_probs = tf.zeros([batch_size, self.config.beam_size])\n    start_top_index = tf.zeros([batch_size, self.config.beam_size], tf.int32)\n    end_top_log_probs = tf.zeros([batch_size, self.config.beam_size,\n                                  self.config.beam_size])\n    end_top_index = tf.zeros([batch_size, self.config.beam_size,\n                              self.config.beam_size], tf.int32)\n    if self.config.joint_prediction:\n      start_logits += 1000.0 * (answer_mask - 1)\n      start_log_probs = tf.nn.log_softmax(start_logits)\n      start_top_log_probs, start_top_index = tf.nn.top_k(\n          start_log_probs, k=self.config.beam_size)\n\n      if not is_training:\n        # batch, beam, length, hidden\n        end_features = tf.tile(tf.expand_dims(final_hidden, 1),\n                               [1, self.config.beam_size, 1, 1])\n        # batch, beam, length\n        start_index = tf.one_hot(start_top_index,\n                                 depth=seq_length, axis=-1, dtype=tf.float32)\n        # batch, beam, hidden\n        start_features = tf.reduce_sum(\n            tf.expand_dims(final_hidden, 1) *\n            tf.expand_dims(start_index, -1), axis=-2)\n        # batch, beam, length, hidden\n        start_features = tf.tile(tf.expand_dims(start_features, 2),\n                                 [1, 1, seq_length, 1])\n      else:\n        start_index = tf.one_hot(\n            features[self.name + ""_start_positions""], depth=seq_length,\n            axis=-1, dtype=tf.float32)\n        start_features = tf.reduce_sum(tf.expand_dims(start_index, -1) *\n                                       final_hidden, axis=1)\n        start_features = tf.tile(tf.expand_dims(start_features, 1),\n                                 [1, seq_length, 1])\n        end_features = final_hidden\n\n      final_repr = tf.concat([start_features, end_features], -1)\n      final_repr = tf.layers.dense(final_repr, 512, activation=modeling.gelu,\n                                   name=""qa_hidden"")\n      # batch, beam, length (batch, length when training)\n      end_logits = tf.squeeze(tf.layers.dense(final_repr, 1), -1,\n                              name=""qa_logits"")\n      if is_training:\n        end_logits += 1000.0 * (answer_mask - 1)\n      else:\n        end_logits += tf.expand_dims(1000.0 * (answer_mask - 1), 1)\n\n      if not is_training:\n        end_log_probs = tf.nn.log_softmax(end_logits)\n        end_top_log_probs, end_top_index = tf.nn.top_k(\n            end_log_probs, k=self.config.beam_size)\n        end_logits = tf.zeros([batch_size, seq_length])\n    else:\n      end_logits = tf.squeeze(tf.layers.dense(final_hidden, 1), -1)\n      start_logits += 1000.0 * (answer_mask - 1)\n      end_logits += 1000.0 * (answer_mask - 1)\n\n    def compute_loss(logits, positions):\n      one_hot_positions = tf.one_hot(\n          positions, depth=seq_length, dtype=tf.float32)\n      log_probs = tf.nn.log_softmax(logits, axis=-1)\n      loss = -tf.reduce_sum(one_hot_positions * log_probs, axis=-1)\n      return loss\n\n    start_positions = features[self.name + ""_start_positions""]\n    end_positions = features[self.name + ""_end_positions""]\n\n    start_loss = compute_loss(start_logits, start_positions)\n    end_loss = compute_loss(end_logits, end_positions)\n\n    losses = (start_loss + end_loss) / 2.0\n\n    answerable_logit = tf.zeros([batch_size])\n    if self.config.answerable_classifier:\n      final_repr = final_hidden[:, 0]\n      if self.config.answerable_uses_start_logits:\n        start_p = tf.nn.softmax(start_logits)\n        start_feature = tf.reduce_sum(tf.expand_dims(start_p, -1) *\n                                      final_hidden, axis=1)\n        final_repr = tf.concat([final_repr, start_feature], -1)\n        final_repr = tf.layers.dense(final_repr, 512,\n                                     activation=modeling.gelu)\n      answerable_logit = tf.squeeze(tf.layers.dense(final_repr, 1), -1)\n      answerable_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n          labels=tf.cast(features[self.name + ""_is_impossible""], tf.float32),\n          logits=answerable_logit)\n      losses += answerable_loss * self.config.answerable_weight\n\n    return losses, dict(\n        loss=losses,\n        start_logits=start_logits,\n        end_logits=end_logits,\n        answerable_logit=answerable_logit,\n        start_positions=features[self.name + ""_start_positions""],\n        end_positions=features[self.name + ""_end_positions""],\n        start_top_log_probs=start_top_log_probs,\n        start_top_index=start_top_index,\n        end_top_log_probs=end_top_log_probs,\n        end_top_index=end_top_index,\n        eid=features[self.name + ""_eid""],\n    )\n\n  def get_scorer(self, split=""dev""):\n    return qa_metrics.SpanBasedQAScorer(self.config, self, split, self.v2)\n\n\nclass MRQATask(QATask):\n  """"""Class for finetuning tasks from the 2019 MRQA shared task.""""""\n\n  def __init__(self, config: configure_finetuning.FinetuningConfig, name,\n               tokenizer):\n    super(MRQATask, self).__init__(config, name, tokenizer)\n\n  def get_examples(self, split):\n    if split in self._examples:\n      utils.log(""N EXAMPLES"", split, len(self._examples[split]))\n      return self._examples[split]\n\n    examples = []\n    example_failures = [0]\n    with tf.io.gfile.GFile(os.path.join(\n        self.config.raw_data_dir(self.name), split + "".jsonl""), ""r"") as f:\n      for i, line in enumerate(f):\n        if self.config.debug and i > 10:\n          break\n        paragraph = json.loads(line.strip())\n        if ""header"" in paragraph:\n          continue\n        self._add_examples(examples, example_failures, paragraph, split)\n    self._examples[split] = examples\n    utils.log(""{:} examples created, {:} failures"".format(\n        len(examples), example_failures[0]))\n    return examples\n\n  def get_scorer(self, split=""dev""):\n    return qa_metrics.SpanBasedQAScorer(self.config, self, split, self.v2)\n\n\nclass SQuADTask(QATask):\n  """"""Class for finetuning on SQuAD 2.0 or 1.1.""""""\n\n  def __init__(self, config: configure_finetuning.FinetuningConfig, name,\n               tokenizer, v2=False):\n    super(SQuADTask, self).__init__(config, name, tokenizer, v2=v2)\n\n  def get_examples(self, split):\n    if split in self._examples:\n      return self._examples[split]\n\n    with tf.io.gfile.GFile(os.path.join(\n        self.config.raw_data_dir(self.name),\n        split + (""-debug"" if self.config.debug else """") + "".json""), ""r"") as f:\n      input_data = json.load(f)[""data""]\n\n    examples = []\n    example_failures = [0]\n    for entry in input_data:\n      for paragraph in entry[""paragraphs""]:\n        self._add_examples(examples, example_failures, paragraph, split)\n    self._examples[split] = examples\n    utils.log(""{:} examples created, {:} failures"".format(\n        len(examples), example_failures[0]))\n    return examples\n\n  def get_scorer(self, split=""dev""):\n    return qa_metrics.SpanBasedQAScorer(self.config, self, split, self.v2)\n\n\nclass SQuAD(SQuADTask):\n  def __init__(self, config: configure_finetuning.FinetuningConfig, tokenizer):\n    super(SQuAD, self).__init__(config, ""squad"", tokenizer, v2=True)\n\n\nclass SQuADv1(SQuADTask):\n  def __init__(self, config: configure_finetuning.FinetuningConfig, tokenizer):\n    super(SQuADv1, self).__init__(config, ""squadv1"", tokenizer)\n\n\nclass NewsQA(MRQATask):\n  def __init__(self, config: configure_finetuning.FinetuningConfig, tokenizer):\n    super(NewsQA, self).__init__(config, ""newsqa"", tokenizer)\n\n\nclass NaturalQuestions(MRQATask):\n  def __init__(self, config: configure_finetuning.FinetuningConfig, tokenizer):\n    super(NaturalQuestions, self).__init__(config, ""naturalqs"", tokenizer)\n\n\nclass SearchQA(MRQATask):\n  def __init__(self, config: configure_finetuning.FinetuningConfig, tokenizer):\n    super(SearchQA, self).__init__(config, ""searchqa"", tokenizer)\n\n\nclass TriviaQA(MRQATask):\n  def __init__(self, config: configure_finetuning.FinetuningConfig, tokenizer):\n    super(TriviaQA, self).__init__(config, ""triviaqa"", tokenizer)\n\n\nclass CMRC2018(SQuADTask):\n  def __init__(self, config: configure_finetuning.FinetuningConfig, tokenizer):\n    super(CMRC2018, self).__init__(config, ""cmrc2018"", tokenizer)\n\n\nclass DRCD(SQuADTask):\n  def __init__(self, config: configure_finetuning.FinetuningConfig, tokenizer):\n    super(DRCD, self).__init__(config, ""drcd"", tokenizer)\n'"
finetune/qa/squad_official_eval.py,4,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Official evaluation script for SQuAD version 2.0.\n\nIn addition to basic functionality, we also compute additional statistics and\nplot precision-recall curves if an additional na_prob.json file is provided.\nThis file is expected to map question ID\'s to the model\'s predicted probability\nthat a question is unanswerable.\n\nModified slightly for the ELECTRA codebase.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport collections\nimport json\nimport numpy as np\nimport os\nimport re\nimport string\nimport sys\nimport tensorflow.compat.v1 as tf\n\nimport configure_finetuning\n\nOPTS = None\n\ndef parse_args():\n  parser = argparse.ArgumentParser(\'Official evaluation script for SQuAD version 2.0.\')\n  parser.add_argument(\'data_file\', metavar=\'data.json\', help=\'Input data JSON file.\')\n  parser.add_argument(\'pred_file\', metavar=\'pred.json\', help=\'Model predictions.\')\n  parser.add_argument(\'--out-file\', \'-o\', metavar=\'eval.json\',\n                      help=\'Write accuracy metrics to file (default is stdout).\')\n  parser.add_argument(\'--na-prob-file\', \'-n\', metavar=\'na_prob.json\',\n                      help=\'Model estimates of probability of no answer.\')\n  parser.add_argument(\'--na-prob-thresh\', \'-t\', type=float, default=1.0,\n                      help=\'Predict """" if no-answer probability exceeds this (default = 1.0).\')\n  parser.add_argument(\'--out-image-dir\', \'-p\', metavar=\'out_images\', default=None,\n                      help=\'Save precision-recall curves to directory.\')\n  parser.add_argument(\'--verbose\', \'-v\', action=\'store_true\')\n  if len(sys.argv) == 1:\n    parser.print_help()\n    sys.exit(1)\n  return parser.parse_args()\n\ndef set_opts(config: configure_finetuning.FinetuningConfig, split):\n  global OPTS\n  Options = collections.namedtuple(""Options"", [\n      ""data_file"", ""pred_file"", ""out_file"", ""na_prob_file"", ""na_prob_thresh"",\n      ""out_image_dir"", ""verbose""])\n  OPTS = Options(\n      data_file=os.path.join(\n          config.raw_data_dir(""squad""),\n          split + (""-debug"" if config.debug else """") + "".json""),\n      pred_file=config.qa_preds_file(""squad""),\n      out_file=config.qa_eval_file(""squad""),\n      na_prob_file=config.qa_na_file(""squad""),\n      na_prob_thresh=config.qa_na_threshold,\n      out_image_dir=None,\n      verbose=False\n  )\n\ndef make_qid_to_has_ans(dataset):\n  qid_to_has_ans = {}\n  for article in dataset:\n    for p in article[\'paragraphs\']:\n      for qa in p[\'qas\']:\n        qid_to_has_ans[qa[\'id\']] = bool(qa[\'answers\'])\n  return qid_to_has_ans\n\ndef normalize_answer(s):\n  """"""Lower text and remove punctuation, articles and extra whitespace.""""""\n  def remove_articles(text):\n    regex = re.compile(r\'\\b(a|an|the)\\b\', re.UNICODE)\n    return re.sub(regex, \' \', text)\n  def white_space_fix(text):\n    return \' \'.join(text.split())\n  def remove_punc(text):\n    exclude = set(string.punctuation)\n    return \'\'.join(ch for ch in text if ch not in exclude)\n  def lower(text):\n    return text.lower()\n  return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef get_tokens(s):\n  if not s: return []\n  return normalize_answer(s).split()\n\ndef compute_exact(a_gold, a_pred):\n  return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\ndef compute_f1(a_gold, a_pred):\n  gold_toks = get_tokens(a_gold)\n  pred_toks = get_tokens(a_pred)\n  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n  num_same = sum(common.values())\n  if len(gold_toks) == 0 or len(pred_toks) == 0:\n    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n    return int(gold_toks == pred_toks)\n  if num_same == 0:\n    return 0\n  precision = 1.0 * num_same / len(pred_toks)\n  recall = 1.0 * num_same / len(gold_toks)\n  f1 = (2 * precision * recall) / (precision + recall)\n  return f1\n\ndef get_raw_scores(dataset, preds):\n  exact_scores = {}\n  f1_scores = {}\n  for article in dataset:\n    for p in article[\'paragraphs\']:\n      for qa in p[\'qas\']:\n        qid = qa[\'id\']\n        gold_answers = [a[\'text\'] for a in qa[\'answers\']\n                        if normalize_answer(a[\'text\'])]\n        if not gold_answers:\n          # For unanswerable questions, only correct answer is empty string\n          gold_answers = [\'\']\n        if qid not in preds:\n          print(\'Missing prediction for %s\' % qid)\n          continue\n        a_pred = preds[qid]\n        # Take max over all gold answers\n        exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n        f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n  return exact_scores, f1_scores\n\ndef apply_no_ans_threshold(scores, na_probs, qid_to_has_ans, na_prob_thresh):\n  new_scores = {}\n  for qid, s in scores.items():\n    pred_na = na_probs[qid] > na_prob_thresh\n    if pred_na:\n      new_scores[qid] = float(not qid_to_has_ans[qid])\n    else:\n      new_scores[qid] = s\n  return new_scores\n\ndef make_eval_dict(exact_scores, f1_scores, qid_list=None):\n  if not qid_list:\n    total = len(exact_scores)\n    return collections.OrderedDict([\n      (\'exact\', 100.0 * sum(exact_scores.values()) / total),\n      (\'f1\', 100.0 * sum(f1_scores.values()) / total),\n      (\'total\', total),\n    ])\n  else:\n    total = len(qid_list)\n    return collections.OrderedDict([\n      (\'exact\', 100.0 * sum(exact_scores[k] for k in qid_list) / total),\n      (\'f1\', 100.0 * sum(f1_scores[k] for k in qid_list) / total),\n      (\'total\', total),\n    ])\n\ndef merge_eval(main_eval, new_eval, prefix):\n  for k in new_eval:\n    main_eval[\'%s_%s\' % (prefix, k)] = new_eval[k]\n\ndef plot_pr_curve(precisions, recalls, out_image, title):\n  plt.step(recalls, precisions, color=\'b\', alpha=0.2, where=\'post\')\n  plt.fill_between(recalls, precisions, step=\'post\', alpha=0.2, color=\'b\')\n  plt.xlabel(\'Recall\')\n  plt.ylabel(\'Precision\')\n  plt.xlim([0.0, 1.05])\n  plt.ylim([0.0, 1.05])\n  plt.title(title)\n  plt.savefig(out_image)\n  plt.clf()\n\ndef make_precision_recall_eval(scores, na_probs, num_true_pos, qid_to_has_ans,\n                               out_image=None, title=None):\n  qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n  true_pos = 0.0\n  cur_p = 1.0\n  cur_r = 0.0\n  precisions = [1.0]\n  recalls = [0.0]\n  avg_prec = 0.0\n  for i, qid in enumerate(qid_list):\n    if qid_to_has_ans[qid]:\n      true_pos += scores[qid]\n    cur_p = true_pos / float(i+1)\n    cur_r = true_pos / float(num_true_pos)\n    if i == len(qid_list) - 1 or na_probs[qid] != na_probs[qid_list[i+1]]:\n      # i.e., if we can put a threshold after this point\n      avg_prec += cur_p * (cur_r - recalls[-1])\n      precisions.append(cur_p)\n      recalls.append(cur_r)\n  if out_image:\n    plot_pr_curve(precisions, recalls, out_image, title)\n  return {\'ap\': 100.0 * avg_prec}\n\ndef run_precision_recall_analysis(main_eval, exact_raw, f1_raw, na_probs,\n                                  qid_to_has_ans, out_image_dir):\n  if out_image_dir and not os.path.exists(out_image_dir):\n    os.makedirs(out_image_dir)\n  num_true_pos = sum(1 for v in qid_to_has_ans.values() if v)\n  if num_true_pos == 0:\n    return\n  pr_exact = make_precision_recall_eval(\n    exact_raw, na_probs, num_true_pos, qid_to_has_ans,\n    out_image=os.path.join(out_image_dir, \'pr_exact.png\'),\n    title=\'Precision-Recall curve for Exact Match score\')\n  pr_f1 = make_precision_recall_eval(\n    f1_raw, na_probs, num_true_pos, qid_to_has_ans,\n    out_image=os.path.join(out_image_dir, \'pr_f1.png\'),\n    title=\'Precision-Recall curve for F1 score\')\n  oracle_scores = {k: float(v) for k, v in qid_to_has_ans.items()}\n  pr_oracle = make_precision_recall_eval(\n    oracle_scores, na_probs, num_true_pos, qid_to_has_ans,\n    out_image=os.path.join(out_image_dir, \'pr_oracle.png\'),\n    title=\'Oracle Precision-Recall curve (binary task of HasAns vs. NoAns)\')\n  merge_eval(main_eval, pr_exact, \'pr_exact\')\n  merge_eval(main_eval, pr_f1, \'pr_f1\')\n  merge_eval(main_eval, pr_oracle, \'pr_oracle\')\n\ndef histogram_na_prob(na_probs, qid_list, image_dir, name):\n  if not qid_list:\n    return\n  x = [na_probs[k] for k in qid_list]\n  weights = np.ones_like(x) / float(len(x))\n  plt.hist(x, weights=weights, bins=20, range=(0.0, 1.0))\n  plt.xlabel(\'Model probability of no-answer\')\n  plt.ylabel(\'Proportion of dataset\')\n  plt.title(\'Histogram of no-answer probability: %s\' % name)\n  plt.savefig(os.path.join(image_dir, \'na_prob_hist_%s.png\' % name))\n  plt.clf()\n\ndef find_best_thresh(preds, scores, na_probs, qid_to_has_ans):\n  num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n  cur_score = num_no_ans\n  best_score = cur_score\n  best_thresh = 0.0\n  qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n  for i, qid in enumerate(qid_list):\n    if qid not in scores: continue\n    if qid_to_has_ans[qid]:\n      diff = scores[qid]\n    else:\n      if preds[qid]:\n        diff = -1\n      else:\n        diff = 0\n    cur_score += diff\n    if cur_score > best_score:\n      best_score = cur_score\n      best_thresh = na_probs[qid]\n  return 100.0 * best_score / len(scores), best_thresh\n\ndef find_all_best_thresh(main_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans):\n  best_exact, exact_thresh = find_best_thresh(preds, exact_raw, na_probs, qid_to_has_ans)\n  best_f1, f1_thresh = find_best_thresh(preds, f1_raw, na_probs, qid_to_has_ans)\n  main_eval[\'best_exact\'] = best_exact\n  main_eval[\'best_exact_thresh\'] = exact_thresh\n  main_eval[\'best_f1\'] = best_f1\n  main_eval[\'best_f1_thresh\'] = f1_thresh\n\ndef main():\n  with tf.io.gfile.GFile(OPTS.data_file) as f:\n    dataset_json = json.load(f)\n    dataset = dataset_json[\'data\']\n  with tf.io.gfile.GFile(OPTS.pred_file) as f:\n    preds = json.load(f)\n  if OPTS.na_prob_file:\n    with tf.io.gfile.GFile(OPTS.na_prob_file) as f:\n      na_probs = json.load(f)\n  else:\n    na_probs = {k: 0.0 for k in preds}\n  qid_to_has_ans = make_qid_to_has_ans(dataset)  # maps qid to True/False\n  has_ans_qids = [k for k, v in qid_to_has_ans.items() if v]\n  no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v]\n  exact_raw, f1_raw = get_raw_scores(dataset, preds)\n  exact_thresh = apply_no_ans_threshold(exact_raw, na_probs, qid_to_has_ans,\n                                        OPTS.na_prob_thresh)\n  f1_thresh = apply_no_ans_threshold(f1_raw, na_probs, qid_to_has_ans,\n                                     OPTS.na_prob_thresh)\n  out_eval = make_eval_dict(exact_thresh, f1_thresh)\n  if has_ans_qids:\n    has_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=has_ans_qids)\n    merge_eval(out_eval, has_ans_eval, \'HasAns\')\n  if no_ans_qids:\n    no_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=no_ans_qids)\n    merge_eval(out_eval, no_ans_eval, \'NoAns\')\n  if OPTS.na_prob_file:\n    find_all_best_thresh(out_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans)\n  if OPTS.na_prob_file and OPTS.out_image_dir:\n    run_precision_recall_analysis(out_eval, exact_raw, f1_raw, na_probs,\n                                  qid_to_has_ans, OPTS.out_image_dir)\n    histogram_na_prob(na_probs, has_ans_qids, OPTS.out_image_dir, \'hasAns\')\n    histogram_na_prob(na_probs, no_ans_qids, OPTS.out_image_dir, \'noAns\')\n  if OPTS.out_file:\n    with tf.io.gfile.GFile(OPTS.out_file, \'w\') as f:\n      json.dump(out_eval, f)\n  else:\n    print(json.dumps(out_eval, indent=2))\n\nif __name__ == \'__main__\':\n  OPTS = parse_args()\n  if OPTS.out_image_dir:\n    import matplotlib\n    matplotlib.use(\'Agg\')\n    import matplotlib.pyplot as plt\n  main()\n'"
finetune/qa/squad_official_eval_v1.py,2,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""\nOfficial evaluation script for v1.1 of the SQuAD dataset.\nModified slightly for the ELECTRA codebase.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom collections import Counter\nimport string\nimport re\nimport json\nimport sys\nimport os\nimport collections\nimport tensorflow.compat.v1 as tf\n\nimport configure_finetuning\n\n\ndef normalize_answer(s):\n  """"""Lower text and remove punctuation, articles and extra whitespace.""""""\n  def remove_articles(text):\n    return re.sub(r\'\\b(a|an|the)\\b\', \' \', text)\n\n  def white_space_fix(text):\n    return \' \'.join(text.split())\n\n  def remove_punc(text):\n    exclude = set(string.punctuation)\n    return \'\'.join(ch for ch in text if ch not in exclude)\n\n  def lower(text):\n    return text.lower()\n\n  return white_space_fix(remove_articles(remove_punc(lower(s))))\n\n\ndef f1_score(prediction, ground_truth):\n  prediction_tokens = normalize_answer(prediction).split()\n  ground_truth_tokens = normalize_answer(ground_truth).split()\n  common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n  num_same = sum(common.values())\n  if num_same == 0:\n    return 0\n  precision = 1.0 * num_same / len(prediction_tokens)\n  recall = 1.0 * num_same / len(ground_truth_tokens)\n  f1 = (2 * precision * recall) / (precision + recall)\n  return f1\n\n\ndef exact_match_score(prediction, ground_truth):\n  return (normalize_answer(prediction) == normalize_answer(ground_truth))\n\n\ndef metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n  scores_for_ground_truths = []\n  for ground_truth in ground_truths:\n    score = metric_fn(prediction, ground_truth)\n    scores_for_ground_truths.append(score)\n  return max(scores_for_ground_truths)\n\n\ndef evaluate(dataset, predictions):\n  f1 = exact_match = total = 0\n  for article in dataset:\n    for paragraph in article[\'paragraphs\']:\n      for qa in paragraph[\'qas\']:\n        total += 1\n        if qa[\'id\'] not in predictions:\n          message = \'Unanswered question \' + qa[\'id\'] + \\\n                    \' will receive score 0.\'\n          print(message, file=sys.stderr)\n          continue\n        ground_truths = list(map(lambda x: x[\'text\'], qa[\'answers\']))\n        prediction = predictions[qa[\'id\']]\n        exact_match += metric_max_over_ground_truths(\n            exact_match_score, prediction, ground_truths)\n        f1 += metric_max_over_ground_truths(\n            f1_score, prediction, ground_truths)\n\n  exact_match = 100.0 * exact_match / total\n  f1 = 100.0 * f1 / total\n\n  return {\'exact_match\': exact_match, \'f1\': f1}\n\n\ndef main(config: configure_finetuning.FinetuningConfig, split, task_name=""squadv1""):\n  expected_version = \'1.1\'\n  # parser = argparse.ArgumentParser(\n  #     description=\'Evaluation for SQuAD \' + expected_version)\n  # parser.add_argument(\'dataset_file\', help=\'Dataset file\')\n  # parser.add_argument(\'prediction_file\', help=\'Prediction File\')\n  # args = parser.parse_args()\n  Args = collections.namedtuple(""Args"", [\n      ""dataset_file"", ""prediction_file""\n  ])\n  args = Args(dataset_file=os.path.join(\n      config.raw_data_dir(task_name),\n      split + (""-debug"" if config.debug else """") + "".json""),\n              prediction_file=config.qa_preds_file(task_name))\n  with tf.io.gfile.GFile(args.dataset_file) as dataset_file:\n    dataset_json = json.load(dataset_file)\n    if dataset_json[\'version\'] != expected_version:\n      print(\'Evaluation expects v-\' + expected_version +\n            \', but got dataset with v-\' + dataset_json[\'version\'],\n            file=sys.stderr)\n    dataset = dataset_json[\'data\']\n  with tf.io.gfile.GFile(args.prediction_file) as prediction_file:\n    predictions = json.load(prediction_file)\n  return evaluate(dataset, predictions)\n\n'"
finetune/tagging/tagging_metrics.py,0,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Metrics for sequence tagging tasks.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport six\n\nimport numpy as np\n\nfrom finetune import scorer\nfrom finetune.tagging import tagging_utils\n\n\nclass WordLevelScorer(scorer.Scorer):\n  """"""Base class for tagging scorers.""""""\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(self):\n    super(WordLevelScorer, self).__init__()\n    self._total_loss = 0\n    self._total_words = 0\n    self._labels = []\n    self._preds = []\n\n  def update(self, results):\n    super(WordLevelScorer, self).update(results)\n    self._total_loss += results[\'loss\']\n    n_words = int(round(np.sum(results[\'labels_mask\'])))\n    self._labels.append(results[\'labels\'][:n_words])\n    self._preds.append(results[\'predictions\'][:n_words])\n    self._total_loss += np.sum(results[\'loss\'])\n    self._total_words += n_words\n\n  def get_loss(self):\n    return self._total_loss / max(1, self._total_words)\n\n\nclass AccuracyScorer(WordLevelScorer):\n  """"""Computes accuracy scores.""""""\n\n  def __init__(self, auto_fail_label=None):\n    super(AccuracyScorer, self).__init__()\n    self._auto_fail_label = auto_fail_label\n\n  def _get_results(self):\n    correct, count = 0, 0\n    for labels, preds in zip(self._labels, self._preds):\n      for y_true, y_pred in zip(labels, preds):\n        count += 1\n        correct += (1 if y_pred == y_true and y_true != self._auto_fail_label\n                    else 0)\n    return [\n        (\'accuracy\', 100.0 * correct / count),\n        (\'loss\', self.get_loss())\n    ]\n\n\nclass F1Scorer(WordLevelScorer):\n  """"""Computes F1 scores.""""""\n\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(self):\n    super(F1Scorer, self).__init__()\n    self._n_correct, self._n_predicted, self._n_gold = 0, 0, 0\n\n  def _get_results(self):\n    if self._n_correct == 0:\n      p, r, f1 = 0, 0, 0\n    else:\n      p = 100.0 * self._n_correct / self._n_predicted\n      r = 100.0 * self._n_correct / self._n_gold\n      f1 = 2 * p * r / (p + r)\n    return [\n        (\'precision\', p),\n        (\'recall\', r),\n        (\'f1\', f1),\n        (\'loss\', self.get_loss()),\n    ]\n\n\nclass EntityLevelF1Scorer(F1Scorer):\n  """"""Computes F1 score for entity-level tasks such as NER.""""""\n\n  def __init__(self, label_mapping):\n    super(EntityLevelF1Scorer, self).__init__()\n    self._inv_label_mapping = {v: k for k, v in six.iteritems(label_mapping)}\n\n  def _get_results(self):\n    self._n_correct, self._n_predicted, self._n_gold = 0, 0, 0\n    for labels, preds in zip(self._labels, self._preds):\n      sent_spans = set(tagging_utils.get_span_labels(\n          labels, self._inv_label_mapping))\n      span_preds = set(tagging_utils.get_span_labels(\n          preds, self._inv_label_mapping))\n      self._n_correct += len(sent_spans & span_preds)\n      self._n_gold += len(sent_spans)\n      self._n_predicted += len(span_preds)\n    return super(EntityLevelF1Scorer, self)._get_results()\n'"
finetune/tagging/tagging_tasks.py,8,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Sequence tagging tasks.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport collections\nimport os\nimport tensorflow.compat.v1 as tf\n\nimport configure_finetuning\nfrom finetune import feature_spec\nfrom finetune import task\nfrom finetune.tagging import tagging_metrics\nfrom finetune.tagging import tagging_utils\nfrom model import tokenization\nfrom pretrain import pretrain_helpers\nfrom util import utils\n\n\nLABEL_ENCODING = ""BIOES""\n\n\nclass TaggingExample(task.Example):\n  """"""A single tagged input sequence.""""""\n\n  def __init__(self, eid, task_name, words, tags, is_token_level,\n               label_mapping):\n    super(TaggingExample, self).__init__(task_name)\n    self.eid = eid\n    self.words = words\n    if is_token_level:\n      labels = tags\n    else:\n      span_labels = tagging_utils.get_span_labels(tags)\n      labels = tagging_utils.get_tags(\n          span_labels, len(words), LABEL_ENCODING)\n    self.labels = [label_mapping[l] for l in labels]\n\n\nclass TaggingTask(task.Task):\n  """"""Defines a sequence tagging task (e.g., part-of-speech tagging).""""""\n\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(self, config: configure_finetuning.FinetuningConfig, name,\n               tokenizer, is_token_level):\n    super(TaggingTask, self).__init__(config, name)\n    self._tokenizer = tokenizer\n    self._label_mapping_path = os.path.join(\n        self.config.preprocessed_data_dir,\n        (""debug_"" if self.config.debug else """") + self.name +\n        ""_label_mapping.pkl"")\n    self._is_token_level = is_token_level\n    self._label_mapping = None\n\n  def get_examples(self, split):\n    sentences = self._get_labeled_sentences(split)\n    examples = []\n    label_mapping = self._get_label_mapping(split, sentences)\n    for i, (words, tags) in enumerate(sentences):\n      examples.append(TaggingExample(\n          i, self.name, words, tags, self._is_token_level, label_mapping\n      ))\n    return examples\n\n  def _get_label_mapping(self, provided_split=None, provided_sentences=None):\n    if self._label_mapping is not None:\n      return self._label_mapping\n    if tf.io.gfile.exists(self._label_mapping_path):\n      self._label_mapping = utils.load_pickle(self._label_mapping_path)\n      return self._label_mapping\n    utils.log(""Writing label mapping for task"", self.name)\n    tag_counts = collections.Counter()\n    train_tags = set()\n    for split in [""train"", ""dev"", ""test""]:\n      if not tf.io.gfile.exists(os.path.join(\n          self.config.raw_data_dir(self.name), split + "".txt"")):\n        continue\n      if split == provided_split:\n        split_sentences = provided_sentences\n      else:\n        split_sentences = self._get_labeled_sentences(split)\n      for _, tags in split_sentences:\n        if not self._is_token_level:\n          span_labels = tagging_utils.get_span_labels(tags)\n          tags = tagging_utils.get_tags(span_labels, len(tags), LABEL_ENCODING)\n        for tag in tags:\n          tag_counts[tag] += 1\n          if provided_split == ""train"":\n            train_tags.add(tag)\n    if self.name == ""ccg"":\n      infrequent_tags = []\n      for tag in tag_counts:\n        if tag not in train_tags:\n          infrequent_tags.append(tag)\n      label_mapping = {\n          label: i for i, label in enumerate(sorted(filter(\n              lambda t: t not in infrequent_tags, tag_counts.keys())))\n      }\n      n = len(label_mapping)\n      for tag in infrequent_tags:\n        label_mapping[tag] = n\n    else:\n      labels = sorted(tag_counts.keys())\n      label_mapping = {label: i for i, label in enumerate(labels)}\n    utils.write_pickle(label_mapping, self._label_mapping_path)\n    self._label_mapping = label_mapping\n    return label_mapping\n\n  def featurize(self, example: TaggingExample, is_training, log=False):\n    words_to_tokens = tokenize_and_align(self._tokenizer, example.words)\n    input_ids = []\n    tagged_positions = []\n    for word_tokens in words_to_tokens:\n      if len(words_to_tokens) + len(input_ids) + 1 > self.config.max_seq_length:\n        input_ids.append(self._tokenizer.vocab[""[SEP]""])\n        break\n      if ""[CLS]"" not in word_tokens and ""[SEP]"" not in word_tokens:\n        tagged_positions.append(len(input_ids))\n      for token in word_tokens:\n        input_ids.append(self._tokenizer.vocab[token])\n\n    pad = lambda x: x + [0] * (self.config.max_seq_length - len(x))\n    labels = pad(example.labels[:self.config.max_seq_length])\n    labeled_positions = pad(tagged_positions)\n    labels_mask = pad([1.0] * len(tagged_positions))\n    segment_ids = pad([1] * len(input_ids))\n    input_mask = pad([1] * len(input_ids))\n    input_ids = pad(input_ids)\n    assert len(input_ids) == self.config.max_seq_length\n    assert len(input_mask) == self.config.max_seq_length\n    assert len(segment_ids) == self.config.max_seq_length\n    assert len(labels) == self.config.max_seq_length\n    assert len(labels_mask) == self.config.max_seq_length\n\n    return {\n        ""input_ids"": input_ids,\n        ""input_mask"": input_mask,\n        ""segment_ids"": segment_ids,\n        ""task_id"": self.config.task_names.index(self.name),\n        self.name + ""_eid"": example.eid,\n        self.name + ""_labels"": labels,\n        self.name + ""_labels_mask"": labels_mask,\n        self.name + ""_labeled_positions"": labeled_positions\n    }\n\n  def _get_labeled_sentences(self, split):\n    sentences = []\n    with tf.io.gfile.GFile(os.path.join(self.config.raw_data_dir(self.name),\n                                        split + "".txt""), ""r"") as f:\n      sentence = []\n      for line in f:\n        line = line.strip().split()\n        if not line:\n          if sentence:\n            words, tags = zip(*sentence)\n            sentences.append((words, tags))\n            sentence = []\n            if self.config.debug and len(sentences) > 100:\n              return sentences\n          continue\n        if line[0] == ""-DOCSTART-"":\n          continue\n        word, tag = line[0], line[-1]\n        sentence.append((word, tag))\n    return sentences\n\n  def get_scorer(self):\n    return tagging_metrics.AccuracyScorer() if self._is_token_level else \\\n      tagging_metrics.EntityLevelF1Scorer(self._get_label_mapping())\n\n  def get_feature_specs(self):\n    return [\n        feature_spec.FeatureSpec(self.name + ""_eid"", []),\n        feature_spec.FeatureSpec(self.name + ""_labels"",\n                                 [self.config.max_seq_length]),\n        feature_spec.FeatureSpec(self.name + ""_labels_mask"",\n                                 [self.config.max_seq_length],\n                                 is_int_feature=False),\n        feature_spec.FeatureSpec(self.name + ""_labeled_positions"",\n                                 [self.config.max_seq_length]),\n    ]\n\n  def get_prediction_module(\n      self, bert_model, features, is_training, percent_done):\n    n_classes = len(self._get_label_mapping())\n    reprs = bert_model.get_sequence_output()\n    reprs = pretrain_helpers.gather_positions(\n        reprs, features[self.name + ""_labeled_positions""])\n    logits = tf.layers.dense(reprs, n_classes)\n    losses = tf.nn.softmax_cross_entropy_with_logits(\n        labels=tf.one_hot(features[self.name + ""_labels""], n_classes),\n        logits=logits)\n    losses *= features[self.name + ""_labels_mask""]\n    losses = tf.reduce_sum(losses, axis=-1)\n    return losses, dict(\n        loss=losses,\n        logits=logits,\n        predictions=tf.argmax(logits, axis=-1),\n        labels=features[self.name + ""_labels""],\n        labels_mask=features[self.name + ""_labels_mask""],\n        eid=features[self.name + ""_eid""],\n    )\n\n  def _create_examples(self, lines, split):\n    pass\n\n\ndef tokenize_and_align(tokenizer, words, cased=False):\n  """"""Splits up words into subword-level tokens.""""""\n  words = [""[CLS]""] + list(words) + [""[SEP]""]\n  basic_tokenizer = tokenizer.basic_tokenizer\n  tokenized_words = []\n  for word in words:\n    word = tokenization.convert_to_unicode(word)\n    word = basic_tokenizer._clean_text(word)\n    if word == ""[CLS]"" or word == ""[SEP]"":\n      word_toks = [word]\n    else:\n      if not cased:\n        word = word.lower()\n        word = basic_tokenizer._run_strip_accents(word)\n      word_toks = basic_tokenizer._run_split_on_punc(word)\n    tokenized_word = []\n    for word_tok in word_toks:\n      tokenized_word += tokenizer.wordpiece_tokenizer.tokenize(word_tok)\n    tokenized_words.append(tokenized_word)\n  assert len(tokenized_words) == len(words)\n  return tokenized_words\n\n\nclass Chunking(TaggingTask):\n  """"""Text chunking.""""""\n\n  def __init__(self, config, tokenizer):\n    super(Chunking, self).__init__(config, ""chunk"", tokenizer, False)\n'"
finetune/tagging/tagging_utils.py,0,"b'# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utilities for sequence tagging tasks.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\ndef get_span_labels(sentence_tags, inv_label_mapping=None):\n  """"""Go from token-level labels to list of entities (start, end, class).""""""\n  if inv_label_mapping:\n    sentence_tags = [inv_label_mapping[i] for i in sentence_tags]\n  span_labels = []\n  last = \'O\'\n  start = -1\n  for i, tag in enumerate(sentence_tags):\n    pos, _ = (None, \'O\') if tag == \'O\' else tag.split(\'-\')\n    if (pos == \'S\' or pos == \'B\' or tag == \'O\') and last != \'O\':\n      span_labels.append((start, i - 1, last.split(\'-\')[-1]))\n    if pos == \'B\' or pos == \'S\' or last == \'O\':\n      start = i\n    last = tag\n  if sentence_tags[-1] != \'O\':\n    span_labels.append((start, len(sentence_tags) - 1,\n                        sentence_tags[-1].split(\'-\')[-1]))\n  return span_labels\n\n\ndef get_tags(span_labels, length, encoding):\n  """"""Converts a list of entities to token-label labels based on the provided\n  encoding (e.g., BIOES).\n  """"""\n\n  tags = [\'O\' for _ in range(length)]\n  for s, e, t in span_labels:\n    for i in range(s, e + 1):\n      tags[i] = \'I-\' + t\n    if \'E\' in encoding:\n      tags[e] = \'E-\' + t\n    if \'B\' in encoding:\n      tags[s] = \'B-\' + t\n    if \'S\' in encoding and s - e == 0:\n      tags[s] = \'S-\' + t\n  return tags\n'"
