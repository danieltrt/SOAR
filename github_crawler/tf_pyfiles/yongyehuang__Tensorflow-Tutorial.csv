file_path,api_count,code
example-python/Tutorial-graph.py,22,"b'# -*- coding:utf-8 -*- \n\nimport tensorflow as tf\nimport os\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess = tf.Session(config=config)\n\n""""""TensorBoard \xe7\xae\x80\xe5\x8d\x95\xe4\xbe\x8b\xe5\xad\x90\xe3\x80\x82\ntf.summary.scalar(\'var_name\', var)     # \xe8\xae\xb0\xe5\xbd\x95\xe6\xa0\x87\xe9\x87\x8f\xe7\x9a\x84\xe5\x8f\x98\xe5\x8c\x96\ntf.summary.histogram(\'vec_name\', vec)  # \xe8\xae\xb0\xe5\xbd\x95\xe5\x90\x91\xe9\x87\x8f\xe6\x88\x96\xe8\x80\x85\xe7\x9f\xa9\xe9\x98\xb5\xef\xbc\x8ctensor\xe7\x9a\x84\xe6\x95\xb0\xe5\x80\xbc\xe5\x88\x86\xe5\xb8\x83\xe5\x8f\x98\xe5\x8c\x96\xe3\x80\x82\n\nmerged = tf.summary.merge_all()        # \xe6\x8a\x8a\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe8\xae\xb0\xe5\xbd\x95\xe5\xb9\xb6\xe6\x8a\x8a\xe4\xbb\x96\xe4\xbb\xac\xe5\x86\x99\xe5\x88\xb0 log_dir \xe4\xb8\xad\ntrain_writer = tf.summary.FileWriter(FLAGS.log_dir + \'/train\', sess.graph)  # \xe4\xbf\x9d\xe5\xad\x98\xe4\xbd\x8d\xe7\xbd\xae\n\n\xe8\xbf\x90\xe8\xa1\x8c\xe5\xae\x8c\xe5\x90\x8e\xef\xbc\x8c\xe5\x9c\xa8\xe5\x91\xbd\xe4\xbb\xa4\xe8\xa1\x8c\xe4\xb8\xad\xe8\xbe\x93\xe5\x85\xa5 tensorboard --logdir=log_dir_path\n""""""\n\ntf.app.flags.DEFINE_string(\'log_dir\', \'summary/graph/\', \'log saving path\')\nFLAGS = tf.app.flags.FLAGS\nif os.path.exists(FLAGS.log_dir):\n    os.rmdir(FLAGS.log_dir)\nos.makedirs(FLAGS.log_dir)\nprint \'created log_dir path\'\n\nwith tf.name_scope(\'add_example\'):\n    a = tf.Variable(tf.truncated_normal([100, 1], mean=0.5, stddev=0.5), name=\'var_a\')\n    tf.summary.histogram(\'a_hist\', a)\n    b = tf.Variable(tf.truncated_normal([100, 1], mean=-0.5, stddev=1.0), name=\'var_b\')\n    tf.summary.histogram(\'b_hist\', b)\n    increase_b = tf.assign(b, b + 0.05)\n    c = tf.add(a, b)\n    tf.summary.histogram(\'c_hist\', c)\n    c_mean = tf.reduce_mean(c)\n    tf.summary.scalar(\'c_mean\', c_mean)\nmerged = tf.summary.merge_all()\nwriter = tf.summary.FileWriter(FLAGS.log_dir + \'add_example\', sess.graph)\n\n\ndef main(_):\n    sess.run(tf.global_variables_initializer())\n    for step in xrange(500):\n        sess.run([merged, increase_b])  # \xe6\xaf\x8f\xe6\xad\xa5\xe6\x94\xb9\xe5\x8f\x98\xe4\xb8\x80\xe6\xac\xa1 b \xe7\x9a\x84\xe5\x80\xbc\n        summary = sess.run(merged)\n        writer.add_summary(summary, step)\n    writer.close()\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
example-python/Tutorial_10_1_numpy_data_1.py,5,"b'""""""Use tf.data.Dataset to create dataset for numpy data.\r\nWith dataset.make_one_shot_iterator(), it is easy but very slow.\r\n""""""\r\n\r\nfrom __future__ import print_function\r\nfrom __future__ import division\r\nfrom __future__ import absolute_import\r\n\r\nimport warnings\r\n\r\nwarnings.filterwarnings(\'ignore\')  # \xe4\xb8\x8d\xe6\x89\x93\xe5\x8d\xb0 warning\r\nimport tensorflow as tf\r\n\r\n# \xe8\xae\xbe\xe7\xbd\xaeGPU\xe6\x8c\x89\xe9\x9c\x80\xe5\xa2\x9e\xe9\x95\xbf\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\n\r\nimport numpy as np\r\nimport time\r\nimport sys\r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n\r\nmnist = input_data.read_data_sets(\'../data/MNIST_data\', one_hot=True)\r\n\r\nX = mnist.train.images\r\ny = mnist.train.labels\r\n\r\nbatch_size = 128\r\n# \xe5\x88\x9b\xe5\xbb\xba dataset\r\ndataset = tf.data.Dataset.from_tensor_slices((X, y))\r\nprint(\'dataset is\', dataset)\r\n\r\n# \xe5\xaf\xb9\xe6\x95\xb0\xe6\x8d\xae\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x93\x8d\xe4\xbd\x9c\r\n# def pre_func(X, y):\r\n#     X = tf.multiply(X, 2)\r\n#     return X, y\r\n# dataset = dataset.map(pre_func)\r\n\r\ndataset = dataset.shuffle(buffer_size=5000)  # \xe8\xae\xbe\xe7\xbd\xae buffle_size \xe8\xb6\x8a\xe5\xa4\xa7\xef\xbc\x8cshuffle \xe8\xb6\x8a\xe5\x9d\x87\xe5\x8c\x80\r\ndataset = dataset.repeat().batch(batch_size)\r\nprint(\'after get batch\', dataset)\r\n\r\n# \xe7\x94\x9f\xe6\x88\x90\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8\r\niterator = dataset.make_one_shot_iterator()\r\nprint(iterator)\r\n\r\n# \xe8\xbf\xad\xe4\xbb\xa3\xe5\x8f\x96\xe5\x80\xbc\r\ntime0 = time.time()\r\nfor count in range(100):  # 100batch  125 seconds\r\n    X_batch, y_batch = sess.run(iterator.get_next())\r\n    sys.stdout.write(""\\rloop {}, pass {:.2f}s"".format(count, time.time() - time0))\r\n    sys.stdout.flush()\r\n    # print(\'count = {} : y = {}\'.format(count, y_batch.reshape([-1])))\r\n\r\n'"
example-python/Tutorial_10_1_numpy_data_2.py,7,"b'""""""Use tf.data.Dataset to create dataset for numpy data.\r\nWith dataset.make_initializable_iterator(), it is more faster then dataset.make_one_shot_iterator().\r\n""""""\r\n\r\nfrom __future__ import print_function\r\nfrom __future__ import division\r\nfrom __future__ import absolute_import\r\n\r\nimport warnings\r\n\r\nwarnings.filterwarnings(\'ignore\')  # \xe4\xb8\x8d\xe6\x89\x93\xe5\x8d\xb0 warning\r\nimport tensorflow as tf\r\n\r\n# \xe8\xae\xbe\xe7\xbd\xaeGPU\xe6\x8c\x89\xe9\x9c\x80\xe5\xa2\x9e\xe9\x95\xbf\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\n\r\nimport numpy as np\r\nimport time\r\nimport sys\r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n\r\nmnist = input_data.read_data_sets(\'../data/MNIST_data\', one_hot=True)\r\n\r\nX = mnist.train.images\r\ny = mnist.train.labels\r\n\r\n# \xe4\xbd\xbf\xe7\x94\xa8 placeholder \xe6\x9d\xa5\xe5\x8f\x96\xe4\xbb\xa3 array\xef\xbc\x8c\xe5\xb9\xb6\xe4\xbd\xbf\xe7\x94\xa8 initiable iterator\xef\xbc\x8c \xe5\x9c\xa8\xe9\x9c\x80\xe8\xa6\x81\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe5\x86\x8d\xe5\xb0\x86 array \xe4\xbc\xa0\xe8\xbf\x9b\xe5\x8e\xbb\r\n# \xe8\xbf\x99\xe6\xa0\xb7\xe8\x83\xbd\xe5\xa4\x9f\xe9\x81\xbf\xe5\x85\x8d\xe6\x8a\x8a\xe5\xa4\xa7\xe6\x95\xb0\xe7\xbb\x84\xe4\xbf\x9d\xe5\xad\x98\xe5\x9c\xa8\xe5\x9b\xbe\xe4\xb8\xad\r\nX_placeholder = tf.placeholder(dtype=X.dtype, shape=X.shape)\r\ny_placeholder = tf.placeholder(dtype=y.dtype, shape=y.shape)\r\n\r\nbatch_size = 128\r\n# \xe5\x88\x9b\xe5\xbb\xba dataset\r\ndataset = tf.data.Dataset.from_tensor_slices((X_placeholder, y_placeholder))\r\nprint(\'dataset is\', dataset)\r\n\r\n# \xe5\xaf\xb9\xe6\x95\xb0\xe6\x8d\xae\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x93\x8d\xe4\xbd\x9c\r\n# def pre_func(X, y):\r\n#     X = tf.multiply(X, 2)\r\n#     return X, y\r\n# dataset = dataset.map(pre_func)\r\n\r\ndataset = dataset.shuffle(buffer_size=5000)  # \xe8\xae\xbe\xe7\xbd\xae buffle_size \xe8\xb6\x8a\xe5\xa4\xa7\xef\xbc\x8cshuffle \xe8\xb6\x8a\xe5\x9d\x87\xe5\x8c\x80\r\ndataset = dataset.repeat().batch(batch_size)\r\nprint(\'after get batch\', dataset)\r\n\r\n# \xe7\x94\x9f\xe6\x88\x90\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8\r\niterator = dataset.make_initializable_iterator()\r\nprint(iterator)\r\nsess.run(iterator.initializer, feed_dict={X_placeholder: X, y_placeholder: y})\r\n\r\n# \xe8\xbf\xad\xe4\xbb\xa3\xe5\x8f\x96\xe5\x80\xbc\r\ntime0 = time.time()\r\nfor count in range(100):  # 100batch  1.95 seconds\r\n    X_batch, y_batch = sess.run(iterator.get_next())\r\n    sys.stdout.write(""\\rloop {}, pass {:.2f}s"".format(count, time.time() - time0))\r\n    sys.stdout.flush()\r\n    # print(\'count = {} : y = {}\'.format(count, y_batch.reshape([-1])))\r\n\r\n'"
example-python/Tutorial_10_2_image_data_1.py,7,"b'""""""Use tf.data.Dataset to create dataset for image(png) data.\r\nWith one-shot\r\n""""""\r\n\r\nfrom __future__ import print_function\r\nfrom __future__ import division\r\nfrom __future__ import absolute_import\r\n\r\nimport warnings\r\n\r\nwarnings.filterwarnings(\'ignore\')  # \xe4\xb8\x8d\xe6\x89\x93\xe5\x8d\xb0 warning\r\nimport tensorflow as tf\r\n\r\n\r\n# \xe8\xae\xbe\xe7\xbd\xaeGPU\xe6\x8c\x89\xe9\x9c\x80\xe5\xa2\x9e\xe9\x95\xbf\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\n\r\nimport numpy as np\r\nimport sys\r\nimport os\r\nimport time\r\n\r\n\r\ndef get_file_path(data_path=\'../data/sketchy_000000000000/\'):\r\n    """"""\xe8\xa7\xa3\xe6\x9e\x90\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xef\xbc\x8c\xe8\x8e\xb7\xe5\x8f\x96\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xe5\x92\x8c\xe6\xa0\x87\xe7\xad\xbe\xe3\x80\x82""""""\r\n    img_paths = list()\r\n    labels = list()\r\n    class_dirs = sorted(os.listdir(data_path))\r\n    dict_class2id = dict()\r\n    for i in range(len(class_dirs)):\r\n        label = i\r\n        class_dir = class_dirs[i]\r\n        dict_class2id[class_dir] = label\r\n        class_path = os.path.join(data_path, class_dir)  # \xe6\xaf\x8f\xe7\xb1\xbb\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\r\n        file_names = sorted(os.listdir(class_path))\r\n        for file_name in file_names:\r\n            file_path = os.path.join(class_path, file_name)\r\n            img_paths.append(file_path)\r\n            labels.append(label)\r\n    return img_paths, labels\r\n\r\n\r\ndef parse_png(img_path, label, height=256, width=256, channel=3):\r\n    """"""\xe6\xa0\xb9\xe6\x8d\xae img_path \xe8\xaf\xbb\xe5\x85\xa5\xe5\x9b\xbe\xe7\x89\x87\xe5\xb9\xb6\xe5\x81\x9a\xe7\x9b\xb8\xe5\xba\x94\xe5\xa4\x84\xe7\x90\x86""""""\r\n    # \xe4\xbb\x8e\xe7\xa1\xac\xe7\x9b\x98\xe4\xb8\x8a\xe8\xaf\xbb\xe5\x8f\x96\xe5\x9b\xbe\xe7\x89\x87\r\n    img = tf.read_file(img_path)\r\n    img_decoded = tf.image.decode_png(img, channels=channel)\r\n    # resize\r\n    img_resized = tf.image.resize_images(img_decoded, [height, width])\r\n    # normalize\r\n    img_norm = img_resized * 1.0 / 127.5 - 1.0\r\n    return img_norm, label\r\n\r\n\r\nimg_paths, labels = get_file_path()\r\nbatch_size = 128\r\ndataset = tf.data.Dataset.from_tensor_slices((img_paths, labels))\r\ndataset = dataset.map(parse_png)\r\nprint(\'parsing image\', dataset)\r\ndataset = dataset.shuffle(buffer_size=5000).repeat().batch(batch_size)\r\nprint(\'batch\', dataset)\r\n\r\n# \xe7\x94\x9f\xe6\x88\x90\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8\r\niterator = dataset.make_one_shot_iterator()\r\nprint(iterator)\r\n\r\ntime0 = time.time()\r\nfor count in range(100):\r\n    X_batch, y_batch = sess.run(iterator.get_next())\r\n    sys.stdout.write(""\\rloop {}, pass {:.2f}s"".format(count, time.time() - time0))\r\n    sys.stdout.flush()\r\n'"
example-python/Tutorial_10_2_image_data_2.py,12,"b'""""""Use tf.data.Dataset to create dataset for image(png) data.\r\nWith TF Queue, shuffle data\r\n\r\nrefer: https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/5_DataManagement/build_an_image_dataset.py\r\n""""""\r\n\r\nfrom __future__ import print_function\r\nfrom __future__ import division\r\nfrom __future__ import absolute_import\r\n\r\nimport warnings\r\n\r\nwarnings.filterwarnings(\'ignore\')  # \xe4\xb8\x8d\xe6\x89\x93\xe5\x8d\xb0 warning\r\nimport tensorflow as tf\r\n\r\n# \xe8\xae\xbe\xe7\xbd\xaeGPU\xe6\x8c\x89\xe9\x9c\x80\xe5\xa2\x9e\xe9\x95\xbf\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\n\r\nimport numpy as np\r\nimport sys\r\nimport os\r\nimport time\r\n\r\n\r\ndef get_file_path(data_path=\'../data/sketchy_000000000000/\'):\r\n    """"""\xe8\xa7\xa3\xe6\x9e\x90\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xef\xbc\x8c\xe8\x8e\xb7\xe5\x8f\x96\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xe5\x92\x8c\xe6\xa0\x87\xe7\xad\xbe\xe3\x80\x82""""""\r\n    img_paths = list()\r\n    labels = list()\r\n    class_dirs = sorted(os.listdir(data_path))\r\n    dict_class2id = dict()\r\n    for i in range(len(class_dirs)):\r\n        label = i\r\n        class_dir = class_dirs[i]\r\n        dict_class2id[class_dir] = label\r\n        class_path = os.path.join(data_path, class_dir)  # \xe6\xaf\x8f\xe7\xb1\xbb\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\r\n        file_names = sorted(os.listdir(class_path))\r\n        for file_name in file_names:\r\n            file_path = os.path.join(class_path, file_name)\r\n            img_paths.append(file_path)\r\n            labels.append(label)\r\n    return img_paths, labels\r\n\r\n\r\ndef get_batch(img_paths, labels, batch_size=128, height=256, width=256, channel=3):\r\n    """"""\xe6\xa0\xb9\xe6\x8d\xae img_path \xe8\xaf\xbb\xe5\x85\xa5\xe5\x9b\xbe\xe7\x89\x87\xe5\xb9\xb6\xe5\x81\x9a\xe7\x9b\xb8\xe5\xba\x94\xe5\xa4\x84\xe7\x90\x86""""""\r\n    # \xe4\xbb\x8e\xe7\xa1\xac\xe7\x9b\x98\xe4\xb8\x8a\xe8\xaf\xbb\xe5\x8f\x96\xe5\x9b\xbe\xe7\x89\x87\r\n    img_paths = np.asarray(img_paths)\r\n    labels = np.asarray(labels)\r\n\r\n    img_paths = tf.convert_to_tensor(img_paths, dtype=tf.string)\r\n    labels = tf.convert_to_tensor(labels, dtype=tf.int32)\r\n    # Build a TF Queue, shuffle data\r\n    image, label = tf.train.slice_input_producer([img_paths, labels], shuffle=True)\r\n    # Read images from disk\r\n    image = tf.read_file(image)\r\n    image = tf.image.decode_jpeg(image, channels=channel)\r\n    # Resize images to a common size\r\n    image = tf.image.resize_images(image, [height, width])\r\n    # Normalize\r\n    image = image * 1.0 / 127.5 - 1.0\r\n    # Create batches\r\n    X_batch, y_batch = tf.train.batch([image, label], batch_size=batch_size,\r\n                                      capacity=batch_size * 8,\r\n                                      num_threads=4)\r\n    return X_batch, y_batch\r\n\r\n\r\nimg_paths, labels = get_file_path()\r\nX_batch, y_batch = get_batch(img_paths, labels)\r\n\r\nsess.run(tf.global_variables_initializer())\r\ntf.train.start_queue_runners(sess=sess)\r\n\r\ntime0 = time.time()\r\nfor count in range(100):   # 11s for 100batch\r\n    _X_batch, _y_batch = sess.run([X_batch, y_batch])\r\n    sys.stdout.write(""\\rloop {}, pass {:.2f}s"".format(count, time.time() - time0))\r\n    sys.stdout.flush()\r\n'"
utils/u03_flags.py,6,"b'# -*- coding:utf-8 -*-\n\nimport tensorflow as tf \n\n""""""tf.app.flags \xe7\x94\xa8\xe6\x9d\xa5\xe5\xae\x9e\xe7\x8e\xb0\xe5\xaf\xb9\xe5\x91\xbd\xe4\xbb\xa4\xe8\xa1\x8c\xe5\x8f\x82\xe6\x95\xb0\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xa7\xa3\xe6\x9e\x90\xe3\x80\x82\n\xe4\xb8\x80\xe8\x88\xac\xe5\x85\x88\xe4\xbd\xbf\xe7\x94\xa8 tf.app.flags.DEFINE_ ... \xe6\x9d\xa5\xe5\xae\x9a\xe4\xb9\x89\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x9b\n\xe7\x84\xb6\xe5\x90\x8e\xe9\x80\x9a\xe8\xbf\x87 tf.app.flags.FLAGS \xe5\xaf\xb9\xe8\xb1\xa1\xe5\x8f\x96\xe5\x90\x84\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe3\x80\x82\n""""""\n\nflags = tf.flags\nlogging = tf.logging\n\nFLAGS = flags.FLAGS\nflags.DEFINE_string(\'name\', \'huang\', \'str_name\')\nflags.DEFINE_integer(\'age\', 99, \'people age\')\nflags.DEFINE_float(\'weight\', 51.1, \'people weight\')\n\ndef main(_):\n    print \'FLAGS.name=\', FLAGS.name \n    print \'FLAGS.age=\', FLAGS.age\n    print \'FLAGS.weight=\', FLAGS.weight \n\nif __name__ == \'__main__\':\n    tf.app.run()      # \xe4\xbc\x9a\xe8\x87\xaa\xe5\x8a\xa8\xe8\xbf\x90\xe8\xa1\x8c main() \xe5\x87\xbd\xe6\x95\xb0\n'"
models/m01_batch_normalization/__init__.py,0,b'# -*- coding:utf-8 -*- \r\n\r\n'
models/m01_batch_normalization/mnist_cnn.py,49,"b'# -*- coding:utf-8 -*-\r\n\r\n""""""\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\xe5\xae\x9a\xe4\xb9\x89\xe3\x80\x82\r\n\xe5\x85\xb3\xe4\xba\x8e tf.layers.batch_normalization() \xe7\x9a\x84\xe7\x90\x86\xe8\xa7\xa3\xe5\x8f\x82\xe8\x80\x83\xef\xbc\x9a [tensorflow\xe4\xb8\xadbatch normalization\xe7\x9a\x84\xe7\x94\xa8\xe6\xb3\x95](https://www.cnblogs.com/hrlnw/p/7227447.html)\r\n""""""\r\n\r\nfrom __future__ import print_function, division, absolute_import\r\n\r\nimport tensorflow as tf\r\n\r\n\r\nclass Model(object):\r\n    def __init__(self, settings):\r\n        self.model_name = settings.model_name\r\n        self.img_size = settings.img_size\r\n        self.n_channel = settings.n_channel\r\n        self.n_class = settings.n_class\r\n        self.drop_rate = settings.drop_rate\r\n        self.global_step = tf.Variable(0, trainable=False, name=\'Global_Step\')\r\n        self.learning_rate = tf.train.exponential_decay(settings.learning_rate,\r\n                                                        self.global_step, settings.decay_step,\r\n                                                        settings.decay_rate, staircase=True)\r\n\r\n        self.conv_weight_initializer = tf.contrib.layers.xavier_initializer(uniform=True)\r\n        self.conv_biases_initializer = tf.zeros_initializer()\r\n        # \xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\r\n        self.fc_weight_initializer = tf.truncated_normal_initializer(0.0, 0.005)\r\n        self.fc_biases_initializer = tf.constant_initializer(0.1)\r\n\r\n        # placeholders\r\n        with tf.name_scope(\'Inputs\'):\r\n            self.X_inputs = tf.placeholder(tf.float32, [None, self.img_size, self.img_size, self.n_channel],\r\n                                           name=\'X_inputs\')\r\n            self.y_inputs = tf.placeholder(tf.int64, [None], name=\'y_input\')\r\n\r\n        self.logits_train = self.inference(is_training=True, reuse=False)\r\n        self.logits_test = self.inference(is_training=False, reuse=True)\r\n\r\n        # \xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\r\n        self.pred_lables = tf.argmax(self.logits_test, axis=1)\r\n        self.pred_probas = tf.nn.softmax(self.logits_test)\r\n        self.test_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n            labels=tf.cast(self.y_inputs, dtype=tf.int32), logits=self.logits_test))\r\n        self.test_acc = tf.reduce_mean(tf.cast(tf.equal(self.pred_lables, self.y_inputs), tf.float32))\r\n\r\n        # \xe8\xae\xad\xe7\xbb\x83\xe7\xbb\x93\xe6\x9e\x9c\r\n        self.train_lables = tf.argmax(self.logits_train, axis=1)\r\n        self.train_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n            labels=tf.cast(self.y_inputs, dtype=tf.int32), logits=self.logits_train))\r\n        self.train_acc = tf.reduce_mean(tf.cast(tf.equal(self.train_lables, self.y_inputs), tf.float32))\r\n\r\n        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\r\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n        """"""\r\n        **\xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x9a** \xe4\xb8\x8b\xe9\x9d\xa2\xe4\xb8\x80\xe5\xae\x9a\xe8\xa6\x81\xe4\xbd\xbf\xe7\x94\xa8\xe8\xbf\x99\xe6\xa0\xb7\xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xe6\x9d\xa5\xe5\x86\x99\xe3\x80\x82\r\n        with tf.control_dependencies(update_ops) \xe8\xbf\x99\xe5\x8f\xa5\xe8\xaf\x9d\xe7\x9a\x84\xe6\x84\x8f\xe6\x80\x9d\xe6\x98\xaf\xe5\xbd\x93\xe8\xbf\x90\xe8\xa1\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84\xe5\x86\x85\xe5\xae\xb9(train_op) \xe6\x97\xb6\xef\xbc\x8c\xe4\xb8\x80\xe5\xae\x9a\xe5\x85\x88\xe6\x89\xa7\xe8\xa1\x8c update_ops \xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe6\x93\x8d\xe4\xbd\x9c\xe3\x80\x82\r\n        \xe8\xbf\x99\xe9\x87\x8c\xe7\x9a\x84 update_ops \xe5\x9c\xa8\xe8\xbf\x99\xe9\x87\x8c\xe4\xb8\xbb\xe8\xa6\x81\xe6\x98\xaf\xe6\x9b\xb4\xe6\x96\xb0 BN \xe5\xb1\x82\xe7\x9a\x84\xe6\xbb\x91\xe5\x8a\xa8\xe5\xb9\xb3\xe5\x9d\x87\xe5\x80\xbc\xe5\x92\x8c\xe6\xbb\x91\xe5\x8a\xa8\xe6\x96\xb9\xe5\xb7\xae\xe3\x80\x82\r\n\r\n        \xe9\x99\xa4\xe4\xba\x86 BN \xe5\xb1\x82\xe5\xa4\x96\xef\xbc\x8c\xe8\xbf\x98\xe6\x9c\x89 center loss \xe4\xb8\xad\xe4\xb9\x9f\xe9\x87\x87\xe7\x94\xa8\xe8\xbf\x99\xe6\xa0\xb7\xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe5\x9c\xa8 center loss \xe4\xb8\xad\xef\xbc\x8cupdate_ops \xe6\x93\x8d\xe4\xbd\x9c\xe4\xb8\xbb\xe8\xa6\x81\xe6\x9b\xb4\xe6\x96\xb0\xe7\xb1\xbb\xe4\xb8\xad\xe5\xbf\x83\xe5\x90\x91\xe9\x87\x8f\xe3\x80\x82\r\n        \xe5\x9b\xa0\xe4\xb8\xba\xe4\xb9\x8b\xe5\x89\x8d\xe5\x9c\xa8 center loss \xe7\x8a\xaf\xe8\xbf\x87\xe6\xb2\xa1\xe6\x9b\xb4\xe6\x96\xb0 center \xe7\x9a\x84\xe9\x94\x99\xe8\xaf\xaf\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x8d\xb0\xe8\xb1\xa1\xe9\x9d\x9e\xe5\xb8\xb8\xe6\xb7\xb1\xe5\x88\xbb\xe3\x80\x82\r\n        """"""\r\n        with tf.control_dependencies(update_ops):  # \xe8\xbf\x99\xe5\x8f\xa5\xe8\xaf\x9d\xe7\x9a\x84\xe6\x84\x8f\xe6\x80\x9d\xe6\x98\xaf\xe5\xbd\x93\xe8\xbf\x90\xe8\xa1\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84\xe5\x86\x85\xe5\xae\xb9(train_op) \xe6\x97\xb6\xef\xbc\x8c\xe4\xb8\x80\xe5\xae\x9a\xe5\x85\x88\xe6\x89\xa7\xe8\xa1\x8c update_ops \xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe6\x93\x8d\xe4\xbd\x9c\r\n            self.train_op = self.optimizer.minimize(self.train_loss, global_step=self.global_step)\r\n\r\n    def inference(self, is_training, reuse=False):\r\n        """"""\xe5\xb8\xa6 BN \xe5\xb1\x82\xe7\x9a\x84CNN """"""\r\n        with tf.variable_scope(\'cnn\', reuse=reuse):\r\n            # \xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82 + BN  + max_pooling\r\n            conv1 = tf.layers.conv2d(self.X_inputs, filters=32, kernel_size=5, strides=1, padding=\'same\',\r\n                                     kernel_initializer=self.conv_weight_initializer, name=\'conv1\')\r\n            bn1 = tf.layers.batch_normalization(conv1, training=is_training, name=\'bn1\')\r\n            bn1 = tf.nn.relu(bn1)  # \xe4\xb8\x80\xe8\x88\xac\xe9\x83\xbd\xe6\x98\xaf\xe5\x85\x88\xe7\xbb\x8f\xe8\xbf\x87 BN \xe5\xb1\x82\xe5\x86\x8d\xe5\x8a\xa0\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\r\n            pool1 = tf.layers.max_pooling2d(bn1, pool_size=2, strides=2, padding=\'same\', name=\'pool1\')\r\n\r\n            # \xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82 + BN  + max_pooling\r\n            conv2 = tf.layers.conv2d(pool1, filters=64, kernel_size=5, strides=1, padding=\'same\',\r\n                                     kernel_initializer=self.conv_weight_initializer, name=\'conv2\')\r\n            bn2 = tf.layers.batch_normalization(conv2, training=is_training, name=\'bn2\')\r\n            bn2 = tf.nn.relu(bn2)  # \xe4\xb8\x80\xe8\x88\xac\xe9\x83\xbd\xe6\x98\xaf\xe5\x85\x88\xe7\xbb\x8f\xe8\xbf\x87 BN \xe5\xb1\x82\xe5\x86\x8d\xe5\x8a\xa0\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\r\n            pool2 = tf.layers.max_pooling2d(bn2, pool_size=2, strides=2, padding=\'same\', name=\'pool2\')\r\n\r\n            # \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5,\xe4\xbd\xbf\xe7\x94\xa8\xe5\x8d\xb7\xe7\xa7\xaf\xe6\x9d\xa5\xe5\xae\x9e\xe7\x8e\xb0\r\n            _, k_height, k_width, k_depth = pool2.get_shape().as_list()\r\n            fc1 = tf.layers.conv2d(pool2, filters=1024, kernel_size=k_height, name=\'fc1\')\r\n            bn3 = tf.layers.batch_normalization(fc1, training=is_training, name=\'bn3\')\r\n            bn3 = tf.nn.relu(bn3)\r\n\r\n            # dropout, \xe5\xa6\x82\xe6\x9e\x9c is_training = False \xe5\xb0\xb1\xe4\xb8\x8d\xe4\xbc\x9a\xe6\x89\xa7\xe8\xa1\x8c dropout\r\n            fc1_drop = tf.layers.dropout(bn3, rate=self.drop_rate, training=is_training)\r\n\r\n            # \xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\r\n            flatten_layer = tf.layers.flatten(fc1_drop)\r\n            out = tf.layers.dense(flatten_layer, units=self.n_class)\r\n        return out\r\n\r\n    def inference2(self, is_training, reuse=False):\r\n        """"""\xe4\xb8\x8d\xe5\xb8\xa6 BN \xe5\xb1\x82\xe7\x9a\x84 CNN\xe3\x80\x82""""""\r\n        with tf.variable_scope(\'cnn\', reuse=reuse):\r\n            # \xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82 + BN  + max_pooling\r\n            conv1 = tf.layers.conv2d(self.X_inputs, filters=32, kernel_size=5, strides=1, padding=\'same\',\r\n                                     activation=tf.nn.relu,\r\n                                     kernel_initializer=self.conv_weight_initializer, name=\'conv1\')\r\n            pool1 = tf.layers.max_pooling2d(conv1, pool_size=2, strides=2, padding=\'same\', name=\'pool1\')\r\n\r\n            # \xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82 + BN  + max_pooling\r\n            conv2 = tf.layers.conv2d(pool1, filters=64, kernel_size=5, strides=1, padding=\'same\',\r\n                                     activation=tf.nn.relu,\r\n                                     kernel_initializer=self.conv_weight_initializer, name=\'conv2\')\r\n            pool2 = tf.layers.max_pooling2d(conv2, pool_size=2, strides=2, padding=\'same\', name=\'pool2\')\r\n\r\n            # \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5,\xe4\xbd\xbf\xe7\x94\xa8\xe5\x8d\xb7\xe7\xa7\xaf\xe6\x9d\xa5\xe5\xae\x9e\xe7\x8e\xb0\r\n            _, k_height, k_width, k_depth = pool2.get_shape().as_list()\r\n            fc1 = tf.layers.conv2d(pool2, filters=1024, kernel_size=k_height, activation=tf.nn.relu, name=\'fc1\')\r\n\r\n            # dropout, \xe5\xa6\x82\xe6\x9e\x9c is_training = False \xe5\xb0\xb1\xe4\xb8\x8d\xe4\xbc\x9a\xe6\x89\xa7\xe8\xa1\x8c dropout\r\n            fc1_drop = tf.layers.dropout(fc1, rate=self.drop_rate, training=is_training)\r\n\r\n            # \xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\r\n            flatten_layer = tf.layers.flatten(fc1_drop)\r\n            out = tf.layers.dense(flatten_layer, units=self.n_class)\r\n        return out\r\n'"
models/m01_batch_normalization/predict.py,7,"b'# -*- coding:utf-8 -*-\r\n\r\nfrom __future__ import print_function, division, absolute_import\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\nimport time\r\n\r\nfrom mnist_cnn import Model\r\n\r\n\r\nclass Settings(object):\r\n    def __init__(self):\r\n        self.model_name = \'mnist_cnn\'\r\n        self.img_size = 28\r\n        self.n_channel = 1\r\n        self.n_class = 10\r\n        self.drop_rate = 0.5\r\n        self.learning_rate = 0.001\r\n        self.decay_step = 2000\r\n        self.decay_rate = 0.5\r\n        self.training_steps = 10000\r\n        self.batch_size = 100\r\n\r\n        self.summary_path = \'summary/\' + self.model_name + \'/\'\r\n        self.ckpt_path = \'ckpt/\' + self.model_name + \'/\'\r\n\r\n        if not os.path.exists(self.summary_path):\r\n            os.makedirs(self.summary_path)\r\n        if not os.path.exists(self.ckpt_path):\r\n            os.makedirs(self.ckpt_path)\r\n\r\n\r\ndef main():\r\n    """"""\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xae\xad\xe7\xbb\x83\xe3\x80\x82""""""\r\n    from tensorflow.examples.tutorials.mnist import input_data\r\n\r\n    mnist = input_data.read_data_sets(""../../data/MNIST_data"", one_hot=False)\r\n    print(mnist.test.labels.shape)\r\n    print(mnist.train.labels.shape)\r\n\r\n    my_setting = Settings()\r\n    with tf.variable_scope(my_setting.model_name):\r\n        model = Model(my_setting)\r\n\r\n    # \xe6\xa8\xa1\xe5\x9e\x8b\xe8\xa6\x81\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe5\x8f\x98\xe9\x87\x8f\r\n    var_list = tf.trainable_variables()\r\n    if model.global_step not in var_list:\r\n        var_list.append(model.global_step)\r\n    # \xe6\xb7\xbb\xe5\x8a\xa0 BN \xe5\xb1\x82\xe7\x9a\x84\xe5\x9d\x87\xe5\x80\xbc\xe5\x92\x8c\xe6\x96\xb9\xe5\xb7\xae\r\n    global_vars = tf.global_variables()\r\n    bn_moving_vars = [v for v in global_vars if \'moving_mean\' in v.name]\r\n    bn_moving_vars += [v for v in global_vars if \'moving_variance\' in v.name]\r\n    var_list += bn_moving_vars\r\n    # \xe5\x88\x9b\xe5\xbb\xbaSaver\r\n    saver = tf.train.Saver(var_list=var_list)\r\n\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    with tf.Session(config=config) as sess:\r\n        if not os.path.exists(my_setting.ckpt_path + \'checkpoint\'):\r\n            print(""There is no checkpoit, please check out."")\r\n            exit()\r\n        saver.restore(sess, tf.train.latest_checkpoint(my_setting.ckpt_path))\r\n        tic = time.time()\r\n        n_batch = len(mnist.test.labels) // my_setting.batch_size\r\n        predict_labels = list()\r\n        true_labels = list()\r\n        for step in range(n_batch):\r\n            X_batch, y_batch = mnist.test.next_batch(my_setting.batch_size, shuffle=False)\r\n            X_batch = X_batch.reshape([-1, 28, 28, 1])\r\n            pred_label, test_loss, test_acc = sess.run([model.pred_lables, model.test_loss, model.test_acc],\r\n                                           feed_dict={model.X_inputs: X_batch, model.y_inputs: y_batch})\r\n            predict_labels.append(pred_label)\r\n            true_labels.append(y_batch)\r\n        predict_labels = np.hstack(predict_labels)\r\n        true_labels = np.hstack(true_labels)\r\n        acc = np.sum(predict_labels == true_labels) / len(true_labels)\r\n        print(""Test sample number = {}, acc = {:.4f}, pass {:.2f}s"".format(len(true_labels), acc, time.time() - tic))\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n'"
models/m01_batch_normalization/train.py,9,"b'# -*- coding:utf-8 -*- \r\n\r\nfrom __future__ import print_function, division, absolute_import\r\n\r\nimport tensorflow as tf\r\nimport os\r\nimport time\r\n\r\nfrom mnist_cnn import Model\r\n\r\n\r\nclass Settings(object):\r\n    def __init__(self):\r\n        self.model_name = \'mnist_cnn\'\r\n        self.img_size = 28\r\n        self.n_channel = 1\r\n        self.n_class = 10\r\n        self.drop_rate = 0.5\r\n        self.learning_rate = 0.001\r\n        self.decay_step = 2000\r\n        self.decay_rate = 0.5\r\n        self.training_steps = 10000   # \xe8\x80\x97\xe6\x97\xb6 90s\r\n        self.batch_size = 100\r\n\r\n        self.summary_path = \'summary/\' + self.model_name + \'/\'\r\n        self.ckpt_path = \'ckpt/\' + self.model_name + \'/\'\r\n\r\n        if not os.path.exists(self.summary_path):\r\n            os.makedirs(self.summary_path)\r\n        if not os.path.exists(self.ckpt_path):\r\n            os.makedirs(self.ckpt_path)\r\n\r\n\r\ndef main():\r\n    """"""\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xae\xad\xe7\xbb\x83\xe3\x80\x82""""""\r\n    from tensorflow.examples.tutorials.mnist import input_data\r\n\r\n    mnist = input_data.read_data_sets(""../../data/MNIST_data"", one_hot=False)\r\n    print(mnist.test.labels.shape)\r\n    print(mnist.train.labels.shape)\r\n\r\n    my_setting = Settings()\r\n    with tf.variable_scope(my_setting.model_name):\r\n        model = Model(my_setting)\r\n\r\n    # \xe6\xa8\xa1\xe5\x9e\x8b\xe8\xa6\x81\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe5\x8f\x98\xe9\x87\x8f\r\n    var_list = tf.trainable_variables()\r\n    if model.global_step not in var_list:\r\n        var_list.append(model.global_step)\r\n    # \xe6\xb7\xbb\xe5\x8a\xa0 BN \xe5\xb1\x82\xe7\x9a\x84\xe5\x9d\x87\xe5\x80\xbc\xe5\x92\x8c\xe6\x96\xb9\xe5\xb7\xae\r\n    global_vars = tf.global_variables()\r\n    bn_moving_vars = [v for v in global_vars if \'moving_mean\' in v.name]\r\n    bn_moving_vars += [v for v in global_vars if \'moving_variance\' in v.name]\r\n    var_list += bn_moving_vars\r\n    # \xe5\x88\x9b\xe5\xbb\xbaSaver\r\n    saver = tf.train.Saver(var_list=var_list)\r\n\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    with tf.Session(config=config) as sess:\r\n        print(""initializing variables."")\r\n        sess.run(tf.global_variables_initializer())\r\n        sess.run(tf.local_variables_initializer())\r\n        if os.path.exists(my_setting.ckpt_path + \'checkpoint\'):\r\n            print(""restore checkpoint."")\r\n            saver.restore(sess, tf.train.latest_checkpoint(my_setting.ckpt_path))\r\n        tic = time.time()\r\n        for step in range(my_setting.training_steps):\r\n            if 0 == step % 100:\r\n                X_batch, y_batch = mnist.train.next_batch(my_setting.batch_size, shuffle=True)\r\n                X_batch = X_batch.reshape([-1, 28, 28, 1])\r\n                _, g_step, train_loss, train_acc = sess.run(\r\n                    [model.train_op, model.global_step, model.train_loss, model.train_acc],\r\n                    feed_dict={model.X_inputs: X_batch, model.y_inputs: y_batch})\r\n                X_batch, y_batch = mnist.test.next_batch(my_setting.batch_size, shuffle=True)\r\n                X_batch = X_batch.reshape([-1, 28, 28, 1])\r\n                test_loss, test_acc = sess.run([model.test_loss, model.test_acc],\r\n                                               feed_dict={model.X_inputs: X_batch, model.y_inputs: y_batch})\r\n                print(\r\n                    ""Global_step={:.2f}, train_loss={:.2f}, train_acc={:.2f}; test_loss={:.2f}, test_acc={:.2f}; pass {:.2f}s"".format(\r\n                        g_step, train_loss, train_acc, test_loss, test_acc, time.time() - tic\r\n                    ))\r\n            else:\r\n                X_batch, y_batch = mnist.train.next_batch(my_setting.batch_size, shuffle=True)\r\n                X_batch = X_batch.reshape([-1, 28, 28, 1])\r\n                sess.run([model.train_op], feed_dict={model.X_inputs: X_batch, model.y_inputs: y_batch})\r\n            if 0 == (step + 1) % 1000:\r\n                path = saver.save(sess, os.path.join(my_setting.ckpt_path, \'model.ckpt\'),\r\n                                  global_step=sess.run(model.global_step))\r\n                print(""Save model to {} "".format(path))\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n'"
models/m02_dcgan/__init__.py,0,b'# -*- coding:utf-8 -*- \r\n\r\n'
models/m02_dcgan/dcgan.py,55,"b'# -*- coding:utf-8 -*- \r\n\r\nfrom __future__ import print_function, division, absolute_import\r\n\r\nimport warnings\r\n\r\nwarnings.filterwarnings(\'ignore\')  # \xe4\xb8\x8d\xe6\x89\x93\xe5\x8d\xb0 warning\r\n\r\nimport matplotlib\r\nmatplotlib.use(\'Agg\')\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\n\r\nimport time\r\nimport os\r\nimport sys\r\n\r\n\r\n# Generator Network\r\n# Input: Noise, Output: Image\r\ndef generator(z, reuse=False):\r\n    """"""\xe8\xbe\x93\xe5\x85\xa5\xe5\x99\xaa\xe5\xa3\xb0\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe5\x9b\xbe\xe7\x89\x87\xe3\x80\x82\r\n       G\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xe4\xbd\xbf\xe7\x94\xa8ReLU\xe4\xbd\x9c\xe4\xb8\xba\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe4\xbd\xbf\xe7\x94\xa8tanh.\r\n       \xe6\xaf\x8f\xe4\xb8\x80\xe5\xb1\x82\xe9\x83\xbd\xe5\x8a\xa0 BN\r\n    """"""\r\n    with tf.variable_scope(\'Generator\', reuse=reuse) as scope:\r\n        # \xe6\x8c\x89\xe7\x85\xa7\xe8\xae\xba\xe6\x96\x87\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\xef\xbc\x8c\xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\xe7\x94\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\r\n        x = tf.layers.dense(z, units=4 * 4 * 1024)\r\n        x = tf.nn.relu(tf.layers.batch_normalization(x, training=True, name=\'bn1\'))\r\n        # Reshape to a 4-D array of images: (batch, height, width, channels)\r\n        # \xe7\x84\xb6\xe5\x90\x8e\xe6\x8a\x8a\xe5\x85\xa8\xe9\x93\xbe\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba reshape \xe6\x88\x90 4D \xe7\x9a\x84 tensor\r\n        x = tf.reshape(x, shape=[-1, 4, 4, 1024])\r\n        print(\'tr_conv4:\', x)\r\n        # Deconvolution, image shape: (batch, 8, 8, 512)\r\n        x = tf.layers.conv2d_transpose(x, 512, 5, strides=2, padding=\'same\')\r\n        x = tf.nn.relu(tf.layers.batch_normalization(x, training=True, name=\'bn2\'))\r\n        print(\'tr_conv3:\', x)\r\n        # Deconvolution, image shape: (batch, 16, 16, 256)\r\n        x = tf.layers.conv2d_transpose(x, 256, 5, strides=2, padding=\'same\')\r\n        x = tf.nn.relu(tf.layers.batch_normalization(x, training=True, name=\'bn3\'))\r\n        print(\'tr_conv2:\', x)\r\n        # Deconvolution, image shape: (batch, 32, 32, 128)\r\n        x = tf.layers.conv2d_transpose(x, 128, 5, strides=2, padding=\'same\')\r\n        x = tf.nn.relu(tf.layers.batch_normalization(x, training=True, name=\'bn4\'))\r\n        print(\'tr_conv1:\', x)\r\n        # Deconvolution, image shape: (batch, 64, 64, 3)\r\n        x = tf.layers.conv2d_transpose(x, 3, 5, strides=2, padding=\'same\')\r\n        x = tf.nn.tanh(tf.layers.batch_normalization(x, training=True, name=\'bn5\'))\r\n        print(\'output_image:\', x)\r\n        return x\r\n\r\n\r\ndef discriminator(x, reuse=False):\r\n    """"""\xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe3\x80\x82\r\n    D\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xe4\xbd\xbf\xe7\x94\xa8LeakyReLU\xe4\xbd\x9c\xe4\xb8\xba\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe3\x80\x82\r\n    """"""\r\n    with tf.variable_scope(\'Discriminator\', reuse=reuse):\r\n        # Typical convolutional neural network to classify images.\r\n        print(\'input_image:\', x)\r\n        x = tf.layers.conv2d(x, 128, 5, strides=2, padding=\'same\')\r\n        x = tf.nn.leaky_relu(tf.layers.batch_normalization(x, training=True, name=\'bn1\'))\r\n        print(\'conv1:\', x)\r\n        x = tf.layers.conv2d(x, 256, 5, strides=2, padding=\'same\')\r\n        x = tf.nn.leaky_relu(tf.layers.batch_normalization(x, training=True, name=\'bn2\'))\r\n        print(\'conv2:\', x)\r\n        x = tf.layers.conv2d(x, 512, 5, strides=2, padding=\'same\')\r\n        x = tf.nn.leaky_relu(tf.layers.batch_normalization(x, training=True, name=\'bn3\'))\r\n        print(\'conv3:\', x)\r\n        x = tf.layers.conv2d(x, 1024, 5, strides=2, padding=\'same\')\r\n        x = tf.nn.leaky_relu(tf.layers.batch_normalization(x, training=True, name=\'bn4\'))\r\n        print(\'conv4:\', x)\r\n        x = tf.layers.flatten(x)\r\n        x = tf.layers.dense(x, 1)\r\n        print(\'output:\', x)\r\n    return x\r\n\r\n\r\ndef get_file_path(data_path=\'../../data/anime/\'):\r\n    """"""\xe8\xa7\xa3\xe6\x9e\x90\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xef\xbc\x8c\xe8\x8e\xb7\xe5\x8f\x96\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xe5\x92\x8c\xe6\xa0\x87\xe7\xad\xbe(\xe5\x9c\xa8\xe8\xbf\x99\xe4\xb8\xaa\xe4\xbe\x8b\xe5\xad\x90\xe4\xb8\xad\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe7\x9c\x9f\xe5\xae\x9e\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe6\xa0\x87\xe7\xad\xbe\xe9\x83\xbd\xe6\x98\xaf1)\xe3\x80\x82""""""\r\n    files = os.listdir(data_path)\r\n    img_paths = [file for file in files if file.endswith(\'.jpg\')]\r\n    img_paths = list(map(lambda s: os.path.join(data_path, s), img_paths))\r\n    labels = [1] * len(img_paths)\r\n    return img_paths, labels\r\n\r\n\r\ndef get_batch(img_paths, labels, batch_size=50, height=64, width=64, channel=3):\r\n    """"""\xe6\xa0\xb9\xe6\x8d\xae img_path \xe8\xaf\xbb\xe5\x85\xa5\xe5\x9b\xbe\xe7\x89\x87\xe5\xb9\xb6\xe5\x81\x9a\xe7\x9b\xb8\xe5\xba\x94\xe5\xa4\x84\xe7\x90\x86""""""\r\n    # \xe4\xbb\x8e\xe7\xa1\xac\xe7\x9b\x98\xe4\xb8\x8a\xe8\xaf\xbb\xe5\x8f\x96\xe5\x9b\xbe\xe7\x89\x87\r\n    img_paths = np.asarray(img_paths)\r\n    labels = np.asarray(labels)\r\n\r\n    img_paths = tf.convert_to_tensor(img_paths, dtype=tf.string)\r\n    labels = tf.convert_to_tensor(labels, dtype=tf.int32)\r\n    # Build a TF Queue, shuffle data\r\n    image, label = tf.train.slice_input_producer([img_paths, labels], shuffle=True)\r\n    # Read images from disk\r\n    image = tf.read_file(image)\r\n    image = tf.image.decode_jpeg(image, channels=channel)\r\n    # Resize images to a common size\r\n    image = tf.image.resize_images(image, [height, width])\r\n    # Normalize\r\n    image = image * 1.0 / 127.5 - 1.0\r\n    # Create batches\r\n    X_batch, y_batch = tf.train.batch([image, label], batch_size=batch_size,\r\n                                      capacity=batch_size * 8,\r\n                                      num_threads=4)\r\n    return X_batch, y_batch\r\n\r\n\r\ndef generate_img(sess, seed=3, show=False, save_path=\'generated_img2\'):\r\n    """"""\xe5\x88\xa9\xe7\x94\xa8\xe9\x9a\x8f\xe6\x9c\xba\xe5\x99\xaa\xe5\xa3\xb0\xe7\x94\x9f\xe6\x88\x90\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c \xe5\xb9\xb6\xe4\xbf\x9d\xe5\xad\x98\xe5\x9b\xbe\xe7\x89\x87\xe3\x80\x82""""""\r\n    if not os.path.exists(save_path):\r\n        os.makedirs(save_path)\r\n    step = sess.run(global_step)\r\n    n_row, n_col = 5, 10\r\n    f, a = plt.subplots(n_row, n_col, figsize=(n_col, n_row))\r\n    np.random.seed(seed)  # \xe6\xaf\x8f\xe6\xac\xa1\xe9\x83\xbd\xe7\x94\xa8\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xe9\x9a\x8f\xe6\x9c\xba\xe6\x95\xb0\xe4\xbe\xbf\xe4\xba\x8e\xe5\xaf\xb9\xe6\xaf\x94\xe7\xbb\x93\xe6\x9e\x9c\r\n    z_batch = np.random.uniform(-1., 1., size=[n_row * n_col, noise_dim])\r\n    fake_imgs = sess.run(gen_image, feed_dict={noise_input: z_batch})\r\n    for i in range(n_row):\r\n        # Noise input.\r\n        for j in range(n_col):\r\n            # Generate image from noise. Extend to 3 channels for matplot figure.\r\n            img = (fake_imgs[n_col * i + j] + 1) / 2\r\n            a[i][j].imshow(img)\r\n            a[i][j].axis(\'off\')\r\n    f.savefig(os.path.join(save_path, \'{}.png\'.format(step)), dpi=100)\r\n    if show:\r\n        f.show()\r\n\r\n\r\n# \xe6\x9e\x84\xe5\xbb\xba\xe7\xbd\x91\xe7\xbb\x9c\r\n# Network Params\r\nimage_dim = 64  # \xe5\xa4\xb4\xe5\x83\x8f\xe7\x9a\x84\xe5\xb0\xba\xe5\xba\xa6\r\nn_channel = 3\r\nnoise_dim = 100  # \xe8\xbe\x93\xe5\x85\xa5\xe5\x99\xaa\xe5\xa3\xb0 z \xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\r\n\r\nnoise_input = tf.placeholder(tf.float32, shape=[None, noise_dim])\r\nreal_img_input = tf.placeholder(tf.float32, shape=[None, image_dim, image_dim, n_channel])\r\n\r\n# \xe6\x9e\x84\xe9\x80\xa0\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x9a\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\r\ngen_image = generator(noise_input)\r\n\r\n# \xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe6\x9c\x89\xe4\xb8\xa4\xe7\xa7\x8d\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8c\xe4\xb8\x80\xe7\xa7\x8d\xe7\x9c\x9f\xe5\xae\x9e\xe5\x9b\xbe\xe5\x83\x8f\xef\xbc\x8c\xe4\xb8\x80\xe7\xa7\x8d\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe5\x81\x87\xe5\x9b\xbe\xe5\x83\x8f\r\ndisc_real = discriminator(real_img_input)  # \xe7\x9c\x9f\xe5\x9b\xbe\xe5\x83\x8f\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\r\ndisc_fake = discriminator(gen_image, reuse=True)\r\ndisc_concat = tf.concat([disc_real, disc_fake], axis=0)\r\n\r\n# \xe6\x9e\x84\xe5\xbb\xba target\r\ndisc_target = tf.placeholder(tf.float32, shape=[None])\r\ngen_target = tf.placeholder(tf.float32, shape=[None])\r\n\r\n# \xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\r\ndisc_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=tf.reshape(disc_concat, [-1]),\r\n                                                                   labels=disc_target))\r\ngen_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=tf.reshape(disc_fake, [-1]),\r\n                                                                  labels=gen_target))\r\n\r\n# \xe4\xb8\xa4\xe4\xb8\xaa\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\r\n# Build Optimizers\r\nglobal_step = tf.Variable(0, trainable=False, name=\'Global_Step\')\r\noptimizer_gen = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5)\r\noptimizer_disc = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5)\r\n# G \xe5\x92\x8c D \xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\r\ngen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'Generator\')\r\n# Discriminator Network Variables\r\ndisc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'Discriminator\')\r\n\r\n# Create training operations\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):  # \xe8\xbf\x99\xe5\x8f\xa5\xe8\xaf\x9d\xe7\x9a\x84\xe6\x84\x8f\xe6\x80\x9d\xe6\x98\xaf\xe5\xbd\x93\xe8\xbf\x90\xe8\xa1\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84\xe5\x86\x85\xe5\xae\xb9(train_op) \xe6\x97\xb6\xef\xbc\x8c\xe4\xb8\x80\xe5\xae\x9a\xe5\x85\x88\xe6\x89\xa7\xe8\xa1\x8c update_ops \xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe6\x93\x8d\xe4\xbd\x9c\r\n    train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars)\r\n    train_disc = optimizer_disc.minimize(disc_loss, var_list=disc_vars, global_step=global_step)\r\n\r\n# \xe6\xa8\xa1\xe5\x9e\x8b\xe8\xa6\x81\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe5\x8f\x98\xe9\x87\x8f\r\n# var_list = tf.global_variables() + tf.local_variables()  # \xe8\xbf\x99\xe6\xa0\xb7\xe4\xbf\x9d\xe5\xad\x98\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\x98\xe9\x87\x8f\xe4\xb8\x8d\xe4\xbc\x9a\xe5\x87\xba\xe9\x94\x99\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe5\xbe\x88\xe5\xa4\x9a\xe6\xb2\xa1\xe5\xbf\x85\xe8\xa6\x81\xe7\x9a\x84\xe5\x8f\x98\xe9\x87\x8f\xe4\xb9\x9f\xe4\xbf\x9d\xe5\xad\x98\xe4\xba\x86\xe3\x80\x82414M \xe6\xaf\x8f\xe4\xb8\xaa ckpt\r\nvar_list = tf.trainable_variables()\r\nif global_step not in var_list:\r\n    var_list.append(global_step)\r\n# \xe6\xb7\xbb\xe5\x8a\xa0 BN \xe5\xb1\x82\xe7\x9a\x84\xe5\x9d\x87\xe5\x80\xbc\xe5\x92\x8c\xe6\x96\xb9\xe5\xb7\xae\r\nglobal_vars = tf.global_variables()\r\nbn_moving_vars = [v for v in global_vars if \'moving_mean\' in v.name]\r\nbn_moving_vars += [v for v in global_vars if \'moving_variance\' in v.name]\r\nvar_list += bn_moving_vars\r\n# \xe5\x88\x9b\xe5\xbb\xbaSaver\r\n\r\nsaver = tf.train.Saver(var_list=var_list)\r\n\r\n# \xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbf\x9d\xe5\xad\x98\xe8\xb7\xaf\xe5\xbe\x84\r\nckpt_path = \'ckpt/\'\r\nif not os.path.exists(ckpt_path):\r\n    os.makedirs(ckpt_path)\r\n\r\n# Training Params\r\nnum_steps = 500000\r\nbatch_size = 64\r\ng_step = 3  # \xe6\x9b\xb4\xe6\x96\xb0\xe4\xb8\x80\xe6\xac\xa1 D, \xe6\x9b\xb4\xe6\x96\xb0 g_step \xe6\xac\xa1 G\r\n\r\n# \xe6\x9e\x84\xe5\xbb\xba\xe6\x95\xb0\xe6\x8d\xae\xe8\xaf\xbb\xe5\x8f\x96\xe5\x87\xbd\xe6\x95\xb0\r\nimg_paths, labels = get_file_path()\r\nprint(""n_sample={}"".format(len(img_paths)))\r\nget_X_batch, get_y_batch = get_batch(img_paths, labels, batch_size=batch_size)\r\n\r\n# Start training\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nwith tf.Session(config=config) as sess:\r\n    print(""initializing variables."")\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(tf.local_variables_initializer())\r\n\r\n    if os.path.exists(ckpt_path + \'checkpoint\'):\r\n        print(""restore checkpoint."")\r\n        saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\r\n\r\n    # \xe5\x90\xaf\xe5\x8a\xa8\xe6\x95\xb0\xe6\x8d\xae\xe8\xaf\xbb\xe5\x8f\x96\xe9\x98\x9f\xe5\x88\x97\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n\r\n    try:\r\n        tic = time.time()\r\n        for i in range(1, num_steps + 1):\r\n\r\n            # \xe5\x87\x86\xe5\xa4\x87\xe8\xbe\x93\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x9a\xe7\x9c\x9f\xe5\xae\x9e\xe5\x9b\xbe\xe7\x89\x87\r\n            X_batch, y_batch = sess.run([get_X_batch, get_y_batch])  # \xe6\x9c\xac\xe4\xbe\x8b\xe4\xb8\xad\xe4\xb8\x8d\xe7\xae\xa1 _y_batch \xe7\x9a\x84label\r\n            # \xe5\x87\x86\xe5\xa4\x87\xe5\x99\xaa\xe5\xa3\xb0\xe8\xbe\x93\xe5\x85\xa5\r\n            z_batch = np.random.uniform(-1., 1., size=[batch_size, noise_dim])\r\n\r\n            # \xe5\xaf\xb9\xe4\xba\x8e\xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xef\xbc\x9a\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe7\x9c\x9f\xe5\xae\x9e\xe5\x9b\xbe\xe7\x89\x87\xe7\xb1\xbb\xe5\x88\xab\xe4\xb8\xba 1\xef\xbc\x8c\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe7\x94\x9f\xe6\x88\x90\xe5\x9b\xbe\xe7\x89\x87\xe7\xb1\xbb\xe5\x88\xab\xe7\xbb\x990\r\n            batch_disc_y = np.concatenate([np.ones([batch_size]), np.zeros([batch_size])], axis=0)\r\n            # \xe5\xaf\xb9\xe4\xba\x8e\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xef\xbc\x9a\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe7\x94\x9f\xe6\x88\x90\xe5\x9b\xbe\xe7\x89\x87\xe7\xb1\xbb\xe5\x88\xab\xe9\x83\xbd\xe7\xbb\x991\r\n            batch_gen_y = np.ones([batch_size])\r\n\r\n            # Training\r\n            feed_dict = {real_img_input: X_batch, noise_input: z_batch,\r\n                         disc_target: batch_disc_y, gen_target: batch_gen_y}\r\n            # update 1 \xe6\xac\xa1 D\xef\xbc\x8cupdate 2 \xe6\xac\xa1 G\r\n            _, dl = sess.run([train_disc, disc_loss], feed_dict=feed_dict)\r\n            gl = 0.0\r\n            for _ in range(g_step):\r\n                _, gl = sess.run([train_gen, gen_loss], feed_dict=feed_dict)\r\n\r\n            if i % 10 == 0:\r\n                print(\'Step {}: Generator Loss: {:.2f}, Discriminator Loss: {:.2f}. Time passed {:.2f}s\'.format(i, gl, dl,\r\n                                                                                                                time.time() - tic))\r\n                if i % 500 == 0:  # \xe6\xaf\x8f\xe8\xae\xad\xe7\xbb\x83 1000 step\xef\xbc\x8c\xe7\x94\x9f\xe6\x88\x90\xe4\xb8\x80\xe6\xac\xa1\xe5\x9b\xbe\xe7\x89\x87\r\n                    generate_img(sess)\r\n                    path = saver.save(sess, os.path.join(ckpt_path, \'model.ckpt\'), global_step=sess.run(global_step))\r\n                    print(""Save model to {} "".format(path))\r\n\r\n        generate_img(sess)  # \xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe6\xac\xa1\xe7\x94\x9f\xe6\x88\x90\xe5\x9b\xbe\xe5\x83\x8f\r\n    except Exception as e:\r\n        print(e)\r\n    finally:\r\n        coord.request_stop()\r\n        coord.join(threads)\r\n\r\n'"
models/m03_wgan/wgan.py,53,"b'# -*- coding:utf-8 -*- \r\n\r\nfrom __future__ import print_function, division, absolute_import\r\n\r\nimport warnings\r\n\r\nwarnings.filterwarnings(\'ignore\')  # \xe4\xb8\x8d\xe6\x89\x93\xe5\x8d\xb0 warning\r\n\r\nimport matplotlib\r\n\r\nmatplotlib.use(\'Agg\')\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\n\r\nimport time\r\nimport os\r\nimport sys\r\n\r\n\r\n# Generator Network\r\n# Input: Noise, Output: Image\r\ndef generator(z, reuse=False):\r\n    """"""\xe8\xbe\x93\xe5\x85\xa5\xe5\x99\xaa\xe5\xa3\xb0\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe5\x9b\xbe\xe7\x89\x87\xe3\x80\x82\r\n       G\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xe4\xbd\xbf\xe7\x94\xa8ReLU\xe4\xbd\x9c\xe4\xb8\xba\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe4\xbd\xbf\xe7\x94\xa8tanh.\r\n       \xe6\xaf\x8f\xe4\xb8\x80\xe5\xb1\x82\xe9\x83\xbd\xe5\x8a\xa0 BN\r\n    """"""\r\n    with tf.variable_scope(\'Generator\', reuse=reuse) as scope:\r\n        # \xe6\x8c\x89\xe7\x85\xa7\xe8\xae\xba\xe6\x96\x87\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\xef\xbc\x8c\xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\xe7\x94\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\r\n        x = tf.layers.dense(z, units=4 * 4 * 1024)\r\n        x = tf.nn.relu(tf.layers.batch_normalization(x, training=True, name=\'bn1\'))\r\n        # Reshape to a 4-D array of images: (batch, height, width, channels)\r\n        # \xe7\x84\xb6\xe5\x90\x8e\xe6\x8a\x8a\xe5\x85\xa8\xe9\x93\xbe\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba reshape \xe6\x88\x90 4D \xe7\x9a\x84 tensor\r\n        x = tf.reshape(x, shape=[-1, 4, 4, 1024])\r\n        print(\'tr_conv4:\', x)\r\n        # Deconvolution, image shape: (batch, 8, 8, 512)\r\n        x = tf.layers.conv2d_transpose(x, 512, 5, strides=2, padding=\'same\')\r\n        x = tf.nn.relu(tf.layers.batch_normalization(x, training=True, name=\'bn2\'))\r\n        print(\'tr_conv3:\', x)\r\n        # Deconvolution, image shape: (batch, 16, 16, 256)\r\n        x = tf.layers.conv2d_transpose(x, 256, 5, strides=2, padding=\'same\')\r\n        x = tf.nn.relu(tf.layers.batch_normalization(x, training=True, name=\'bn3\'))\r\n        print(\'tr_conv2:\', x)\r\n        # Deconvolution, image shape: (batch, 32, 32, 128)\r\n        x = tf.layers.conv2d_transpose(x, 128, 5, strides=2, padding=\'same\')\r\n        x = tf.nn.relu(tf.layers.batch_normalization(x, training=True, name=\'bn4\'))\r\n        print(\'tr_conv1:\', x)\r\n        # Deconvolution, image shape: (batch, 64, 64, 3)\r\n        x = tf.layers.conv2d_transpose(x, 3, 5, strides=2, padding=\'same\')\r\n        x = tf.nn.tanh(tf.layers.batch_normalization(x, training=True, name=\'bn5\'))\r\n        print(\'output_image:\', x)\r\n        return x\r\n\r\n\r\ndef discriminator(x, reuse=False):\r\n    """"""\xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe3\x80\x82\r\n    D\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xe4\xbd\xbf\xe7\x94\xa8LeakyReLU\xe4\xbd\x9c\xe4\xb8\xba\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe3\x80\x82\r\n    """"""\r\n    with tf.variable_scope(\'Discriminator\', reuse=reuse):\r\n        # Typical convolutional neural network to classify images.\r\n        print(\'input_image:\', x)\r\n        x = tf.layers.conv2d(x, 128, 5, strides=2, padding=\'same\')\r\n        x = tf.nn.leaky_relu(tf.layers.batch_normalization(x, training=True, name=\'bn1\'))\r\n        print(\'conv1:\', x)\r\n        x = tf.layers.conv2d(x, 256, 5, strides=2, padding=\'same\')\r\n        x = tf.nn.leaky_relu(tf.layers.batch_normalization(x, training=True, name=\'bn2\'))\r\n        print(\'conv2:\', x)\r\n        x = tf.layers.conv2d(x, 512, 5, strides=2, padding=\'same\')\r\n        x = tf.nn.leaky_relu(tf.layers.batch_normalization(x, training=True, name=\'bn3\'))\r\n        print(\'conv3:\', x)\r\n        x = tf.layers.conv2d(x, 1024, 5, strides=2, padding=\'same\')\r\n        x = tf.nn.leaky_relu(tf.layers.batch_normalization(x, training=True, name=\'bn4\'))\r\n        print(\'conv4:\', x)\r\n        x = tf.layers.flatten(x)\r\n        x = tf.layers.dense(x, 1)\r\n        print(\'output:\', x)\r\n    return x\r\n\r\n\r\ndef get_file_path(data_path=\'../../data/anime/\'):\r\n    """"""\xe8\xa7\xa3\xe6\x9e\x90\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xef\xbc\x8c\xe8\x8e\xb7\xe5\x8f\x96\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xe5\x92\x8c\xe6\xa0\x87\xe7\xad\xbe(\xe5\x9c\xa8\xe8\xbf\x99\xe4\xb8\xaa\xe4\xbe\x8b\xe5\xad\x90\xe4\xb8\xad\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe7\x9c\x9f\xe5\xae\x9e\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe6\xa0\x87\xe7\xad\xbe\xe9\x83\xbd\xe6\x98\xaf1)\xe3\x80\x82""""""\r\n    files = os.listdir(data_path)\r\n    img_paths = [file for file in files if file.endswith(\'.jpg\')]\r\n    img_paths = list(map(lambda s: os.path.join(data_path, s), img_paths))\r\n    labels = [1] * len(img_paths)\r\n    return img_paths, labels\r\n\r\n\r\ndef get_batch(img_paths, labels, batch_size=50, height=64, width=64, channel=3):\r\n    """"""\xe6\xa0\xb9\xe6\x8d\xae img_path \xe8\xaf\xbb\xe5\x85\xa5\xe5\x9b\xbe\xe7\x89\x87\xe5\xb9\xb6\xe5\x81\x9a\xe7\x9b\xb8\xe5\xba\x94\xe5\xa4\x84\xe7\x90\x86""""""\r\n    # \xe4\xbb\x8e\xe7\xa1\xac\xe7\x9b\x98\xe4\xb8\x8a\xe8\xaf\xbb\xe5\x8f\x96\xe5\x9b\xbe\xe7\x89\x87\r\n    img_paths = np.asarray(img_paths)\r\n    labels = np.asarray(labels)\r\n\r\n    img_paths = tf.convert_to_tensor(img_paths, dtype=tf.string)\r\n    labels = tf.convert_to_tensor(labels, dtype=tf.int32)\r\n    # Build a TF Queue, shuffle data\r\n    image, label = tf.train.slice_input_producer([img_paths, labels], shuffle=True)\r\n    # Read images from disk\r\n    image = tf.read_file(image)\r\n    image = tf.image.decode_jpeg(image, channels=channel)\r\n    # Resize images to a common size\r\n    image = tf.image.resize_images(image, [height, width])\r\n    # Normalize\r\n    image = image * 1.0 / 127.5 - 1.0\r\n    # Create batches\r\n    X_batch, y_batch = tf.train.batch([image, label], batch_size=batch_size,\r\n                                      capacity=batch_size * 8,\r\n                                      num_threads=4)\r\n    return X_batch, y_batch\r\n\r\n\r\ndef generate_img(sess, seed=3, show=False, save_path=\'generated_img\'):\r\n    """"""\xe5\x88\xa9\xe7\x94\xa8\xe9\x9a\x8f\xe6\x9c\xba\xe5\x99\xaa\xe5\xa3\xb0\xe7\x94\x9f\xe6\x88\x90\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c \xe5\xb9\xb6\xe4\xbf\x9d\xe5\xad\x98\xe5\x9b\xbe\xe7\x89\x87\xe3\x80\x82""""""\r\n    if not os.path.exists(save_path):\r\n        os.makedirs(save_path)\r\n    step = sess.run(global_step)\r\n    n_row, n_col = 5, 10\r\n    f, a = plt.subplots(n_row, n_col, figsize=(n_col, n_row))\r\n    np.random.seed(seed)  # \xe6\xaf\x8f\xe6\xac\xa1\xe9\x83\xbd\xe7\x94\xa8\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xe9\x9a\x8f\xe6\x9c\xba\xe6\x95\xb0\xe4\xbe\xbf\xe4\xba\x8e\xe5\xaf\xb9\xe6\xaf\x94\xe7\xbb\x93\xe6\x9e\x9c\r\n    z_batch = np.random.uniform(-1., 1., size=[n_row * n_col, noise_dim])\r\n    fake_imgs = sess.run(gen_image, feed_dict={noise_input: z_batch})\r\n    for i in range(n_row):\r\n        # Noise input.\r\n        for j in range(n_col):\r\n            # Generate image from noise. Extend to 3 channels for matplot figure.\r\n            img = (fake_imgs[n_col * i + j] + 1) / 2\r\n            a[i][j].imshow(img)\r\n            a[i][j].axis(\'off\')\r\n    f.savefig(os.path.join(save_path, \'{}.png\'.format(step)), dpi=100)\r\n    if show:\r\n        f.show()\r\n\r\n\r\n# \xe6\x9e\x84\xe5\xbb\xba\xe7\xbd\x91\xe7\xbb\x9c\r\n# Network Params\r\nimage_dim = 64  # \xe5\xa4\xb4\xe5\x83\x8f\xe7\x9a\x84\xe5\xb0\xba\xe5\xba\xa6\r\nn_channel = 3\r\nnoise_dim = 100  # \xe8\xbe\x93\xe5\x85\xa5\xe5\x99\xaa\xe5\xa3\xb0 z \xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\r\n\r\nnoise_input = tf.placeholder(tf.float32, shape=[None, noise_dim])\r\nreal_img_input = tf.placeholder(tf.float32, shape=[None, image_dim, image_dim, n_channel])\r\n\r\n# \xe6\x9e\x84\xe9\x80\xa0\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x9a\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\r\ngen_image = generator(noise_input)\r\n\r\n# \xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe6\x9c\x89\xe4\xb8\xa4\xe7\xa7\x8d\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8c\xe4\xb8\x80\xe7\xa7\x8d\xe7\x9c\x9f\xe5\xae\x9e\xe5\x9b\xbe\xe5\x83\x8f\xef\xbc\x8c\xe4\xb8\x80\xe7\xa7\x8d\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe5\x81\x87\xe5\x9b\xbe\xe5\x83\x8f\r\ndisc_real = discriminator(real_img_input)  # \xe7\x9c\x9f\xe5\x9b\xbe\xe5\x83\x8f\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\r\ndisc_fake = discriminator(gen_image, reuse=True)\r\n\r\n# \xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0 (\xe4\xb8\x8eDCGAN\xe7\xac\xac1\xe4\xb8\xaa\xe6\x94\xb9\xe5\x8f\x98)\r\ndisc_loss = tf.reduce_mean(disc_fake - disc_real)\r\ngen_loss = tf.reduce_mean(-disc_fake)\r\n\r\n# \xe4\xb8\xa4\xe4\xb8\xaa\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\r\n# Build Optimizers (\xe7\xac\xac2\xe4\xb8\xaa\xe6\x94\xb9\xe5\x8f\x98)\r\nglobal_step = tf.Variable(0, trainable=False, name=\'Global_Step\')\r\noptimizer_gen = tf.train.RMSPropOptimizer(learning_rate=5e-5)\r\noptimizer_disc = tf.train.RMSPropOptimizer(learning_rate=5e-5)\r\n\r\n# G \xe5\x92\x8c D \xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\r\ngen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'Generator\')\r\n# Discriminator Network Variables\r\ndisc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'Discriminator\')\r\n\r\n# \xe5\xaf\xb9\xe4\xba\x8e D \xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xa3\x81\xe5\x89\xaa  \xef\xbc\x88\xe7\xac\xac3\xe4\xb8\xaa\xe6\x94\xb9\xe5\x8f\x98\xef\xbc\x8c\xe6\xb3\xa8\xe6\x84\x8f\xe6\xaf\x8f\xe6\xac\xa1\xe6\x9b\xb4\xe6\x96\xb0\xe5\xae\x8c D \xe5\x90\x8e\xe9\x83\xbd\xe8\xa6\x81\xe6\x89\xa7\xe8\xa1\x8c\xe8\xa3\x81\xe5\x89\xaa\xef\xbc\x89\r\nclipped_disc = [tf.assign(v, tf.clip_by_value(v, -0.01, 0.01)) for v in disc_vars]\r\n\r\n# Create training operations\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):  # \xe8\xbf\x99\xe5\x8f\xa5\xe8\xaf\x9d\xe7\x9a\x84\xe6\x84\x8f\xe6\x80\x9d\xe6\x98\xaf\xe5\xbd\x93\xe8\xbf\x90\xe8\xa1\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84\xe5\x86\x85\xe5\xae\xb9(train_op) \xe6\x97\xb6\xef\xbc\x8c\xe4\xb8\x80\xe5\xae\x9a\xe5\x85\x88\xe6\x89\xa7\xe8\xa1\x8c update_ops \xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe6\x93\x8d\xe4\xbd\x9c\r\n    train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars, global_step=global_step)\r\n    train_disc = optimizer_disc.minimize(disc_loss, var_list=disc_vars)\r\n\r\n# \xe6\xa8\xa1\xe5\x9e\x8b\xe8\xa6\x81\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe5\x8f\x98\xe9\x87\x8f\r\n# var_list = tf.global_variables() + tf.local_variables()  # \xe8\xbf\x99\xe6\xa0\xb7\xe4\xbf\x9d\xe5\xad\x98\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\x98\xe9\x87\x8f\xe4\xb8\x8d\xe4\xbc\x9a\xe5\x87\xba\xe9\x94\x99\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe5\xbe\x88\xe5\xa4\x9a\xe6\xb2\xa1\xe5\xbf\x85\xe8\xa6\x81\xe7\x9a\x84\xe5\x8f\x98\xe9\x87\x8f\xe4\xb9\x9f\xe4\xbf\x9d\xe5\xad\x98\xe4\xba\x86\xe3\x80\x82414M \xe6\xaf\x8f\xe4\xb8\xaa ckpt\r\nvar_list = tf.trainable_variables()\r\nif global_step not in var_list:\r\n    var_list.append(global_step)\r\n# \xe6\xb7\xbb\xe5\x8a\xa0 BN \xe5\xb1\x82\xe7\x9a\x84\xe5\x9d\x87\xe5\x80\xbc\xe5\x92\x8c\xe6\x96\xb9\xe5\xb7\xae\r\nglobal_vars = tf.global_variables()\r\nbn_moving_vars = [v for v in global_vars if \'moving_mean\' in v.name]\r\nbn_moving_vars += [v for v in global_vars if \'moving_variance\' in v.name]\r\nvar_list += bn_moving_vars\r\n# \xe5\x88\x9b\xe5\xbb\xbaSaver\r\n\r\nsaver = tf.train.Saver(var_list=var_list)\r\n\r\n# \xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbf\x9d\xe5\xad\x98\xe8\xb7\xaf\xe5\xbe\x84\r\nckpt_path = \'ckpt/\'\r\nif not os.path.exists(ckpt_path):\r\n    os.makedirs(ckpt_path)\r\n\r\n# Training Params\r\nnum_steps = 500000\r\nbatch_size = 64\r\nd_iters = 5  # \xe6\x9b\xb4\xe6\x96\xb0 d_iters \xe6\xac\xa1 D, \xe6\x9b\xb4\xe6\x96\xb0 1 \xe6\xac\xa1 G\r\n\r\n# \xe6\x9e\x84\xe5\xbb\xba\xe6\x95\xb0\xe6\x8d\xae\xe8\xaf\xbb\xe5\x8f\x96\xe5\x87\xbd\xe6\x95\xb0\r\nimg_paths, labels = get_file_path()\r\nprint(""n_sample={}"".format(len(img_paths)))\r\nget_X_batch, get_y_batch = get_batch(img_paths, labels, batch_size=batch_size)\r\n\r\n# Start training\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nwith tf.Session(config=config) as sess:\r\n    print(""initializing variables."")\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(tf.local_variables_initializer())\r\n\r\n    if os.path.exists(ckpt_path + \'checkpoint\'):\r\n        print(""restore checkpoint."")\r\n        saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\r\n\r\n    # \xe5\x90\xaf\xe5\x8a\xa8\xe6\x95\xb0\xe6\x8d\xae\xe8\xaf\xbb\xe5\x8f\x96\xe9\x98\x9f\xe5\x88\x97\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n\r\n\r\n    def get_batch():\r\n        # \xe5\x87\x86\xe5\xa4\x87\xe8\xbe\x93\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x9a\xe7\x9c\x9f\xe5\xae\x9e\xe5\x9b\xbe\xe7\x89\x87\r\n        X_batch, _ = sess.run([get_X_batch, get_y_batch])  # \xe6\x9c\xac\xe4\xbe\x8b\xe4\xb8\xad\xe4\xb8\x8d\xe7\xae\xa1 _y_batch \xe7\x9a\x84label\r\n        # \xe5\x87\x86\xe5\xa4\x87\xe5\x99\xaa\xe5\xa3\xb0\xe8\xbe\x93\xe5\x85\xa5\r\n        z_batch = np.random.uniform(-1., 1., size=[batch_size, noise_dim])\r\n        # Training \xe8\xbf\x99\xe9\x87\x8c\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81 target\r\n        feed_dict = {real_img_input: X_batch, noise_input: z_batch}\r\n        return feed_dict\r\n\r\n\r\n    try:\r\n        tic = time.time()\r\n        for i in range(1, num_steps + 1):\r\n            # update 5 \xe6\xac\xa1 D\xef\xbc\x8cupdate 1 \xe6\xac\xa1 G\r\n            dl = 0.0\r\n            for _ in range(d_iters):\r\n                _, _, dl = sess.run([train_disc, clipped_disc, disc_loss], feed_dict=get_batch())\r\n            _, gl = sess.run([train_gen, gen_loss], feed_dict=get_batch())\r\n\r\n            if i % 5 == 0:\r\n                print(\r\n                    \'Step {}: Generator Loss: {:.2f}, Discriminator Loss: {:.2f}. Time passed {:.2f}s\'.format(i, gl, dl,\r\n                                                                                                              time.time() - tic))\r\n                if i % 200 == 0:  # \xe6\xaf\x8f\xe8\xae\xad\xe7\xbb\x83 1000 step\xef\xbc\x8c\xe7\x94\x9f\xe6\x88\x90\xe4\xb8\x80\xe6\xac\xa1\xe5\x9b\xbe\xe7\x89\x87\r\n                    generate_img(sess)\r\n                    path = saver.save(sess, os.path.join(ckpt_path, \'model.ckpt\'), global_step=sess.run(global_step))\r\n                    print(""Save model to {} "".format(path))\r\n\r\n        generate_img(sess)  # \xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe6\xac\xa1\xe7\x94\x9f\xe6\x88\x90\xe5\x9b\xbe\xe5\x83\x8f\r\n    except Exception as e:\r\n        print(e)\r\n    finally:\r\n        coord.request_stop()\r\n        coord.join(threads)\r\n'"
models/m03_wgan/wgan_gp.py,56,"b'# -*- coding:utf-8 -*- \r\n\r\nfrom __future__ import print_function, division, absolute_import\r\n\r\nimport warnings\r\n\r\nwarnings.filterwarnings(\'ignore\')  # \xe4\xb8\x8d\xe6\x89\x93\xe5\x8d\xb0 warning\r\n\r\nimport matplotlib\r\n\r\nmatplotlib.use(\'Agg\')\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\n\r\nimport time\r\nimport os\r\nimport sys\r\n\r\n\r\n# Generator Network\r\n# Input: Noise, Output: Image\r\ndef generator(z, reuse=False):\r\n    """"""\xe8\xbe\x93\xe5\x85\xa5\xe5\x99\xaa\xe5\xa3\xb0\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe5\x9b\xbe\xe7\x89\x87\xe3\x80\x82\r\n       G\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xe4\xbd\xbf\xe7\x94\xa8ReLU\xe4\xbd\x9c\xe4\xb8\xba\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe4\xbd\xbf\xe7\x94\xa8tanh.\r\n       \xe6\xaf\x8f\xe4\xb8\x80\xe5\xb1\x82\xe9\x83\xbd\xe5\x8a\xa0 BN\r\n    """"""\r\n    with tf.variable_scope(\'Generator\', reuse=reuse) as scope:\r\n        # \xe6\x8c\x89\xe7\x85\xa7\xe8\xae\xba\xe6\x96\x87\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\xef\xbc\x8c\xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\xe7\x94\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\r\n        x = tf.layers.dense(z, units=4 * 4 * 1024)\r\n        x = tf.nn.relu(tf.layers.batch_normalization(x, training=True, name=\'bn1\'))\r\n        # Reshape to a 4-D array of images: (batch, height, width, channels)\r\n        # \xe7\x84\xb6\xe5\x90\x8e\xe6\x8a\x8a\xe5\x85\xa8\xe9\x93\xbe\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba reshape \xe6\x88\x90 4D \xe7\x9a\x84 tensor\r\n        x = tf.reshape(x, shape=[-1, 4, 4, 1024])\r\n        print(\'tr_conv4:\', x)\r\n        # Deconvolution, image shape: (batch, 8, 8, 512)\r\n        x = tf.layers.conv2d_transpose(x, 512, 5, strides=2, padding=\'same\')\r\n        x = tf.nn.relu(tf.layers.batch_normalization(x, training=True, name=\'bn2\'))\r\n        print(\'tr_conv3:\', x)\r\n        # Deconvolution, image shape: (batch, 16, 16, 256)\r\n        x = tf.layers.conv2d_transpose(x, 256, 5, strides=2, padding=\'same\')\r\n        x = tf.nn.relu(tf.layers.batch_normalization(x, training=True, name=\'bn3\'))\r\n        print(\'tr_conv2:\', x)\r\n        # Deconvolution, image shape: (batch, 32, 32, 128)\r\n        x = tf.layers.conv2d_transpose(x, 128, 5, strides=2, padding=\'same\')\r\n        x = tf.nn.relu(tf.layers.batch_normalization(x, training=True, name=\'bn4\'))\r\n        print(\'tr_conv1:\', x)\r\n        # Deconvolution, image shape: (batch, 64, 64, 3)\r\n        x = tf.layers.conv2d_transpose(x, 3, 5, strides=2, padding=\'same\')\r\n        x = tf.nn.tanh(tf.layers.batch_normalization(x, training=True, name=\'bn5\'))\r\n        print(\'output_image:\', x)\r\n        return x\r\n\r\n\r\ndef discriminator(x, reuse=False):\r\n    """"""\xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe3\x80\x82\r\n    D\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xe4\xbd\xbf\xe7\x94\xa8LeakyReLU\xe4\xbd\x9c\xe4\xb8\xba\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe3\x80\x82\r\n    """"""\r\n    with tf.variable_scope(\'Discriminator\', reuse=reuse):\r\n        # Typical convolutional neural network to classify images.\r\n        print(\'input_image:\', x)\r\n        x = tf.layers.conv2d(x, 128, 5, strides=2, padding=\'same\')\r\n        x = tf.nn.leaky_relu(tf.layers.batch_normalization(x, training=True, name=\'bn1\'))\r\n        print(\'conv1:\', x)\r\n        x = tf.layers.conv2d(x, 256, 5, strides=2, padding=\'same\')\r\n        x = tf.nn.leaky_relu(tf.layers.batch_normalization(x, training=True, name=\'bn2\'))\r\n        print(\'conv2:\', x)\r\n        x = tf.layers.conv2d(x, 512, 5, strides=2, padding=\'same\')\r\n        x = tf.nn.leaky_relu(tf.layers.batch_normalization(x, training=True, name=\'bn3\'))\r\n        print(\'conv3:\', x)\r\n        x = tf.layers.conv2d(x, 1024, 5, strides=2, padding=\'same\')\r\n        x = tf.nn.leaky_relu(tf.layers.batch_normalization(x, training=True, name=\'bn4\'))\r\n        print(\'conv4:\', x)\r\n        x = tf.layers.flatten(x)\r\n        x = tf.layers.dense(x, 1)\r\n        print(\'output:\', x)\r\n    return x\r\n\r\n\r\ndef get_file_path(data_path=\'../../data/anime/\'):\r\n    """"""\xe8\xa7\xa3\xe6\x9e\x90\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xef\xbc\x8c\xe8\x8e\xb7\xe5\x8f\x96\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xe5\x92\x8c\xe6\xa0\x87\xe7\xad\xbe(\xe5\x9c\xa8\xe8\xbf\x99\xe4\xb8\xaa\xe4\xbe\x8b\xe5\xad\x90\xe4\xb8\xad\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe7\x9c\x9f\xe5\xae\x9e\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe6\xa0\x87\xe7\xad\xbe\xe9\x83\xbd\xe6\x98\xaf1)\xe3\x80\x82""""""\r\n    files = os.listdir(data_path)\r\n    img_paths = [file for file in files if file.endswith(\'.jpg\')]\r\n    img_paths = list(map(lambda s: os.path.join(data_path, s), img_paths))\r\n    labels = [1] * len(img_paths)\r\n    return img_paths, labels\r\n\r\n\r\ndef get_batch(img_paths, labels, batch_size=50, height=64, width=64, channel=3):\r\n    """"""\xe6\xa0\xb9\xe6\x8d\xae img_path \xe8\xaf\xbb\xe5\x85\xa5\xe5\x9b\xbe\xe7\x89\x87\xe5\xb9\xb6\xe5\x81\x9a\xe7\x9b\xb8\xe5\xba\x94\xe5\xa4\x84\xe7\x90\x86""""""\r\n    # \xe4\xbb\x8e\xe7\xa1\xac\xe7\x9b\x98\xe4\xb8\x8a\xe8\xaf\xbb\xe5\x8f\x96\xe5\x9b\xbe\xe7\x89\x87\r\n    img_paths = np.asarray(img_paths)\r\n    labels = np.asarray(labels)\r\n\r\n    img_paths = tf.convert_to_tensor(img_paths, dtype=tf.string)\r\n    labels = tf.convert_to_tensor(labels, dtype=tf.int32)\r\n    # Build a TF Queue, shuffle data\r\n    image, label = tf.train.slice_input_producer([img_paths, labels], shuffle=True)\r\n    # Read images from disk\r\n    image = tf.read_file(image)\r\n    image = tf.image.decode_jpeg(image, channels=channel)\r\n    # Resize images to a common size\r\n    image = tf.image.resize_images(image, [height, width])\r\n    # Normalize\r\n    image = image * 1.0 / 127.5 - 1.0\r\n    # Create batches\r\n    X_batch, y_batch = tf.train.batch([image, label], batch_size=batch_size,\r\n                                      capacity=batch_size * 8,\r\n                                      num_threads=4)\r\n    return X_batch, y_batch\r\n\r\n\r\ndef generate_img(sess, seed=3, show=False, save_path=\'generated_img_gp\'):\r\n    """"""\xe5\x88\xa9\xe7\x94\xa8\xe9\x9a\x8f\xe6\x9c\xba\xe5\x99\xaa\xe5\xa3\xb0\xe7\x94\x9f\xe6\x88\x90\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c \xe5\xb9\xb6\xe4\xbf\x9d\xe5\xad\x98\xe5\x9b\xbe\xe7\x89\x87\xe3\x80\x82""""""\r\n    if not os.path.exists(save_path):\r\n        os.makedirs(save_path)\r\n    step = sess.run(global_step)\r\n    n_row, n_col = 5, 10\r\n    f, a = plt.subplots(n_row, n_col, figsize=(n_col, n_row))\r\n    np.random.seed(seed)  # \xe6\xaf\x8f\xe6\xac\xa1\xe9\x83\xbd\xe7\x94\xa8\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xe9\x9a\x8f\xe6\x9c\xba\xe6\x95\xb0\xe4\xbe\xbf\xe4\xba\x8e\xe5\xaf\xb9\xe6\xaf\x94\xe7\xbb\x93\xe6\x9e\x9c\r\n    z_batch = np.random.uniform(-1., 1., size=[n_row * n_col, noise_dim])\r\n    fake_imgs = sess.run(gen_image, feed_dict={noise_input: z_batch})\r\n    for i in range(n_row):\r\n        # Noise input.\r\n        for j in range(n_col):\r\n            # Generate image from noise. Extend to 3 channels for matplot figure.\r\n            img = (fake_imgs[n_col * i + j] + 1) / 2\r\n            a[i][j].imshow(img)\r\n            a[i][j].axis(\'off\')\r\n    f.savefig(os.path.join(save_path, \'{}.png\'.format(step)), dpi=100)\r\n    if show:\r\n        f.show()\r\n\r\n\r\n# \xe6\x9e\x84\xe5\xbb\xba\xe7\xbd\x91\xe7\xbb\x9c\r\n# Network Params\r\nbatch_size = 64\r\nLAMBDA = 10\r\nimage_dim = 64  # \xe5\xa4\xb4\xe5\x83\x8f\xe7\x9a\x84\xe5\xb0\xba\xe5\xba\xa6\r\nn_channel = 3\r\nnoise_dim = 100  # \xe8\xbe\x93\xe5\x85\xa5\xe5\x99\xaa\xe5\xa3\xb0 z \xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\r\n\r\nnoise_input = tf.placeholder(tf.float32, shape=[None, noise_dim])\r\nreal_img_input = tf.placeholder(tf.float32, shape=[None, image_dim, image_dim, n_channel])\r\n\r\n# \xe6\x9e\x84\xe9\x80\xa0\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x9a\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\r\ngen_image = generator(noise_input)\r\n\r\n# \xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe6\x9c\x89\xe4\xb8\xa4\xe7\xa7\x8d\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8c\xe4\xb8\x80\xe7\xa7\x8d\xe7\x9c\x9f\xe5\xae\x9e\xe5\x9b\xbe\xe5\x83\x8f\xef\xbc\x8c\xe4\xb8\x80\xe7\xa7\x8d\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe5\x81\x87\xe5\x9b\xbe\xe5\x83\x8f\r\ndisc_real = discriminator(real_img_input)  # \xe7\x9c\x9f\xe5\x9b\xbe\xe5\x83\x8f\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\r\ndisc_fake = discriminator(gen_image, reuse=True)\r\n\r\n# \xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0 (\xe4\xb8\x8eDCGAN\xe7\xac\xac1\xe4\xb8\xaa\xe6\x94\xb9\xe5\x8f\x98)\r\ndisc_loss = tf.reduce_mean(disc_fake - disc_real)\r\ngen_loss = tf.reduce_mean(-disc_fake)\r\n\r\n# \xe9\x87\x87\xe6\xa0\xb7\xef\xbc\x9a X_ = X_real * alpha + X_fake * (1 - alpha)\r\nalpha = tf.random_uniform(shape=[batch_size, 1], minval=0., maxval=1.)\r\ndiffrences = gen_image - real_img_input\r\ninterpolates = real_img_input + (alpha * diffrences)\r\ngradients = tf.gradients(discriminator(interpolates, reuse=True), [interpolates, ])[0]\r\ngrad_l2 = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\r\ngradient_penalty = tf.reduce_mean((grad_l2 - 1.) ** 2)\r\ndisc_loss += LAMBDA * gradient_penalty\r\n\r\n# \xe4\xb8\xa4\xe4\xb8\xaa\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\r\n# Build Optimizers (\xe7\xac\xac2\xe4\xb8\xaa\xe6\x94\xb9\xe5\x8f\x98)\r\nglobal_step = tf.Variable(0, trainable=False, name=\'Global_Step\')\r\noptimizer_gen = tf.train.AdamOptimizer(learning_rate=5e-5, beta1=0.5, beta2=0.9)\r\noptimizer_disc = tf.train.AdamOptimizer(learning_rate=5e-5, beta1=0.5, beta2=0.9)\r\n\r\n# G \xe5\x92\x8c D \xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\r\ngen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'Generator\')\r\n# Discriminator Network Variables\r\ndisc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'Discriminator\')\r\n\r\n# Create training operations\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):  # \xe8\xbf\x99\xe5\x8f\xa5\xe8\xaf\x9d\xe7\x9a\x84\xe6\x84\x8f\xe6\x80\x9d\xe6\x98\xaf\xe5\xbd\x93\xe8\xbf\x90\xe8\xa1\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84\xe5\x86\x85\xe5\xae\xb9(train_op) \xe6\x97\xb6\xef\xbc\x8c\xe4\xb8\x80\xe5\xae\x9a\xe5\x85\x88\xe6\x89\xa7\xe8\xa1\x8c update_ops \xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe6\x93\x8d\xe4\xbd\x9c\r\n    train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars, global_step=global_step)\r\n    train_disc = optimizer_disc.minimize(disc_loss, var_list=disc_vars)\r\n\r\n# \xe6\xa8\xa1\xe5\x9e\x8b\xe8\xa6\x81\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe5\x8f\x98\xe9\x87\x8f\r\n# var_list = tf.global_variables() + tf.local_variables()  # \xe8\xbf\x99\xe6\xa0\xb7\xe4\xbf\x9d\xe5\xad\x98\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\x98\xe9\x87\x8f\xe4\xb8\x8d\xe4\xbc\x9a\xe5\x87\xba\xe9\x94\x99\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe5\xbe\x88\xe5\xa4\x9a\xe6\xb2\xa1\xe5\xbf\x85\xe8\xa6\x81\xe7\x9a\x84\xe5\x8f\x98\xe9\x87\x8f\xe4\xb9\x9f\xe4\xbf\x9d\xe5\xad\x98\xe4\xba\x86\xe3\x80\x82414M \xe6\xaf\x8f\xe4\xb8\xaa ckpt\r\nvar_list = tf.trainable_variables()\r\nif global_step not in var_list:\r\n    var_list.append(global_step)\r\n# \xe6\xb7\xbb\xe5\x8a\xa0 BN \xe5\xb1\x82\xe7\x9a\x84\xe5\x9d\x87\xe5\x80\xbc\xe5\x92\x8c\xe6\x96\xb9\xe5\xb7\xae\r\nglobal_vars = tf.global_variables()\r\nbn_moving_vars = [v for v in global_vars if \'moving_mean\' in v.name]\r\nbn_moving_vars += [v for v in global_vars if \'moving_variance\' in v.name]\r\nvar_list += bn_moving_vars\r\n# \xe5\x88\x9b\xe5\xbb\xbaSaver\r\n\r\nsaver = tf.train.Saver(var_list=var_list)\r\n\r\n# \xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbf\x9d\xe5\xad\x98\xe8\xb7\xaf\xe5\xbe\x84\r\nckpt_path = \'ckpt-gp/\'\r\nif not os.path.exists(ckpt_path):\r\n    os.makedirs(ckpt_path)\r\n\r\n# Training Params\r\nnum_steps = 500000\r\nbatch_size = 64\r\nd_iters = 5  # \xe6\x9b\xb4\xe6\x96\xb0 d_iters \xe6\xac\xa1 D, \xe6\x9b\xb4\xe6\x96\xb0 1 \xe6\xac\xa1 G\r\n\r\n# \xe6\x9e\x84\xe5\xbb\xba\xe6\x95\xb0\xe6\x8d\xae\xe8\xaf\xbb\xe5\x8f\x96\xe5\x87\xbd\xe6\x95\xb0\r\nimg_paths, labels = get_file_path()\r\nprint(""n_sample={}"".format(len(img_paths)))\r\nget_X_batch, get_y_batch = get_batch(img_paths, labels, batch_size=batch_size)\r\n\r\n# Start training\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nwith tf.Session(config=config) as sess:\r\n    print(""initializing variables."")\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(tf.local_variables_initializer())\r\n\r\n    if os.path.exists(ckpt_path + \'checkpoint\'):\r\n        print(""restore checkpoint."")\r\n        saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\r\n\r\n    # \xe5\x90\xaf\xe5\x8a\xa8\xe6\x95\xb0\xe6\x8d\xae\xe8\xaf\xbb\xe5\x8f\x96\xe9\x98\x9f\xe5\x88\x97\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n\r\n\r\n    def get_batch():\r\n        # \xe5\x87\x86\xe5\xa4\x87\xe8\xbe\x93\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x9a\xe7\x9c\x9f\xe5\xae\x9e\xe5\x9b\xbe\xe7\x89\x87\r\n        X_batch, _ = sess.run([get_X_batch, get_y_batch])  # \xe6\x9c\xac\xe4\xbe\x8b\xe4\xb8\xad\xe4\xb8\x8d\xe7\xae\xa1 _y_batch \xe7\x9a\x84label\r\n        # \xe5\x87\x86\xe5\xa4\x87\xe5\x99\xaa\xe5\xa3\xb0\xe8\xbe\x93\xe5\x85\xa5\r\n        z_batch = np.random.uniform(-1., 1., size=[batch_size, noise_dim])\r\n        # Training \xe8\xbf\x99\xe9\x87\x8c\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81 target\r\n        feed_dict = {real_img_input: X_batch, noise_input: z_batch}\r\n        return feed_dict\r\n\r\n\r\n    try:\r\n        tic = time.time()\r\n        for i in range(1, num_steps + 1):\r\n            # update 5 \xe6\xac\xa1 D\xef\xbc\x8cupdate 1 \xe6\xac\xa1 G\r\n            dl = 0.0\r\n            for _ in range(d_iters):\r\n                _, dl = sess.run([train_disc, disc_loss], feed_dict=get_batch())\r\n            _, gl = sess.run([train_gen, gen_loss], feed_dict=get_batch())\r\n\r\n            if i % 5 == 0:\r\n                print(\'Step {}: Generator Loss: {:.2f}, Discriminator Loss: {:.2f}. Time passed {:.2f}s\'.format(i, gl, dl,\r\n                                                                                                              time.time() - tic))\r\n                if i % 100 == 0:  # \xe6\xaf\x8f\xe8\xae\xad\xe7\xbb\x83 1000 step\xef\xbc\x8c\xe7\x94\x9f\xe6\x88\x90\xe4\xb8\x80\xe6\xac\xa1\xe5\x9b\xbe\xe7\x89\x87\r\n                    generate_img(sess)\r\n                    path = saver.save(sess, os.path.join(ckpt_path, \'model.ckpt\'), global_step=sess.run(global_step))\r\n                    print(""Save model to {} "".format(path))\r\n\r\n        generate_img(sess)  # \xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe6\xac\xa1\xe7\x94\x9f\xe6\x88\x90\xe5\x9b\xbe\xe5\x83\x8f\r\n    except Exception as e:\r\n        print(e)\r\n    finally:\r\n        coord.request_stop()\r\n        coord.join(threads)\r\n'"
models/m04_pix2pix/__init__.py,0,b'# -*- coding:utf-8 -*- \r\n\r\n'
models/m04_pix2pix/model.py,53,"b'# -*- coding:utf-8 -*-\r\n\r\n""""""\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbb\xa3\xe7\xa0\x81\xe3\x80\x82""""""\r\n\r\nfrom __future__ import print_function, division, absolute_import\r\n\r\nimport tensorflow as tf\r\nimport collections\r\n\r\nEPS = 1e-12\r\nModel = collections.namedtuple(""Model"",\r\n                               ""outputs, predict_real, predict_fake, discrim_loss, discrim_grads_and_vars, gen_loss_GAN, gen_loss_L1, gen_grads_and_vars, train"")\r\n\r\n\r\ndef discrim_conv(batch_input, out_channels, stride):\r\n    padded_input = tf.pad(batch_input, [[0, 0], [1, 1], [1, 1], [0, 0]], mode=""CONSTANT"")\r\n    return tf.layers.conv2d(padded_input, out_channels, kernel_size=4, strides=(stride, stride), padding=""valid"",\r\n                            kernel_initializer=tf.random_normal_initializer(0, 0.02))\r\n\r\n\r\ndef gen_conv(batch_input, out_channels, separable_conv=False):\r\n    """"""\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe7\x9a\x84 encoder \xe9\x83\xa8\xe5\x88\x86\xe3\x80\x82""""""\r\n    # [batch, in_height, in_width, in_channels] => [batch, out_height, out_width, out_channels]\r\n    initializer = tf.random_normal_initializer(0, 0.02)\r\n    if separable_conv:  # \xe6\xb7\xb1\xe5\xba\xa6\xe5\x8f\xaf\xe5\x88\x86\xe5\x8d\xb7\xe7\xa7\xaf\r\n        return tf.layers.separable_conv2d(batch_input, out_channels, kernel_size=4, strides=(2, 2), padding=""same"",\r\n                                          depthwise_initializer=initializer, pointwise_initializer=initializer)\r\n    else:\r\n        return tf.layers.conv2d(batch_input, out_channels, kernel_size=4, strides=(2, 2), padding=""same"",\r\n                                kernel_initializer=initializer)\r\n\r\n\r\ndef gen_deconv(batch_input, out_channels, separable_conv=False):\r\n    """"""\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe7\x9a\x84 decoder \xe9\x83\xa8\xe5\x88\x86\xe3\x80\x82""""""\r\n    # [batch, in_height, in_width, in_channels] => [batch, out_height, out_width, out_channels]\r\n    initializer = tf.random_normal_initializer(0, 0.02)\r\n    if separable_conv:\r\n        _b, h, w, _c = batch_input.shape\r\n        resized_input = tf.image.resize_images(batch_input, [h * 2, w * 2],\r\n                                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\r\n        return tf.layers.separable_conv2d(resized_input, out_channels, kernel_size=4, strides=(1, 1), padding=""same"",\r\n                                          depthwise_initializer=initializer, pointwise_initializer=initializer)\r\n    else:\r\n        return tf.layers.conv2d_transpose(batch_input, out_channels, kernel_size=4, strides=(2, 2), padding=""same"",\r\n                                          kernel_initializer=initializer)\r\n\r\n\r\ndef lrelu(x, a):\r\n    with tf.name_scope(""lrelu""):\r\n        # adding these together creates the leak part and linear part\r\n        # then cancels them out by subtracting/adding an absolute value term\r\n        # leak: a*x/2 - a*abs(x)/2\r\n        # linear: x/2 + abs(x)/2\r\n\r\n        # this block looks like it has 2 inputs on the graph unless we do this\r\n        x = tf.identity(x)\r\n        return (0.5 * (1 + a)) * x + (0.5 * (1 - a)) * tf.abs(x)\r\n\r\n\r\ndef batchnorm(inputs):\r\n    return tf.layers.batch_normalization(inputs, axis=3, epsilon=1e-5, momentum=0.1, training=True,\r\n                                         gamma_initializer=tf.random_normal_initializer(1.0, 0.02))\r\n\r\n\r\ndef create_generator(generator_inputs, generator_outputs_channels, ngf=64):\r\n    """"""\xe8\xbe\x93\xe5\x85\xa5\xe5\x92\x8c\xe8\xbe\x93\xe5\x87\xba\xe9\x83\xbd\xe6\x98\xaf [batch_size, 256, 256, 3] \xe7\x9a\x84 tensor.\r\n    ngf \xe6\x98\xaf\xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9a\x84\xe6\xb7\xb1\xe5\xba\xa6\xe3\x80\x82\r\n    """"""\r\n    layers = []\r\n\r\n    # ngf \xe8\xa1\xa8\xe7\xa4\xba\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe7\x9a\x84\xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe6\xb7\xb1\xe5\xba\xa6\r\n    # encoder_1: [batch, 256, 256, in_channels] => [batch, 128, 128, ngf]\r\n    with tf.variable_scope(""encoder_1""):\r\n        output = gen_conv(generator_inputs, ngf)\r\n        layers.append(output)\r\n\r\n    layer_specs = [\r\n        ngf * 2,  # encoder_2: [batch, 128, 128, ngf] => [batch, 64, 64, ngf * 2]\r\n        ngf * 4,  # encoder_3: [batch, 64, 64, ngf * 2] => [batch, 32, 32, ngf * 4]\r\n        ngf * 8,  # encoder_4: [batch, 32, 32, ngf * 4] => [batch, 16, 16, ngf * 8]\r\n        ngf * 8,  # encoder_5: [batch, 16, 16, ngf * 8] => [batch, 8, 8, ngf * 8]\r\n        ngf * 8,  # encoder_6: [batch, 8, 8, ngf * 8] => [batch, 4, 4, ngf * 8]\r\n        ngf * 8,  # encoder_7: [batch, 4, 4, ngf * 8] => [batch, 2, 2, ngf * 8]\r\n        ngf * 8,  # encoder_8: [batch, 2, 2, ngf * 8] => [batch, 1, 1, ngf * 8]\r\n    ]\r\n\r\n    for out_channels in layer_specs:\r\n        with tf.variable_scope(""encoder_%d"" % (len(layers) + 1)):\r\n            rectified = lrelu(layers[-1], 0.2)\r\n            # [batch, in_height, in_width, in_channels] => [batch, in_height/2, in_width/2, out_channels]\r\n            convolved = gen_conv(rectified, out_channels)\r\n            output = batchnorm(convolved)\r\n            layers.append(output)\r\n\r\n    layer_specs = [\r\n        (ngf * 8, 0.5),  # decoder_8: [batch, 1, 1, ngf * 8] => [batch, 2, 2, ngf * 8 * 2]\r\n        (ngf * 8, 0.5),  # decoder_7: [batch, 2, 2, ngf * 8 * 2] => [batch, 4, 4, ngf * 8 * 2]\r\n        (ngf * 8, 0.5),  # decoder_6: [batch, 4, 4, ngf * 8 * 2] => [batch, 8, 8, ngf * 8 * 2]\r\n        (ngf * 8, 0.0),  # decoder_5: [batch, 8, 8, ngf * 8 * 2] => [batch, 16, 16, ngf * 8 * 2]\r\n        (ngf * 4, 0.0),  # decoder_4: [batch, 16, 16, ngf * 8 * 2] => [batch, 32, 32, ngf * 4 * 2]\r\n        (ngf * 2, 0.0),  # decoder_3: [batch, 32, 32, ngf * 4 * 2] => [batch, 64, 64, ngf * 2 * 2]\r\n        (ngf, 0.0),  # decoder_2: [batch, 64, 64, ngf * 2 * 2] => [batch, 128, 128, ngf * 2]\r\n    ]\r\n\r\n    num_encoder_layers = len(layers)\r\n    for decoder_layer, (out_channels, dropout) in enumerate(layer_specs):\r\n        skip_layer = num_encoder_layers - decoder_layer - 1\r\n        with tf.variable_scope(""decoder_%d"" % (skip_layer + 1)):\r\n            if decoder_layer == 0:  # decoder \xe7\x9a\x84\xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\xe7\x94\xa8\xe7\x9a\x84\xe6\x98\xaf encoder \xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\r\n                # first decoder layer doesn\'t have skip connections\r\n                # since it is directly connected to the skip_layer\r\n                input = layers[-1]\r\n            else:  # decoder \xe7\x9a\x84\xe6\xaf\x8f\xe4\xb8\x80\xe5\xb1\x82\xe5\x92\x8c encoder \xe7\x9a\x84\xe6\xaf\x8f\xe4\xb8\x80\xe5\xb1\x82 \xe6\x8b\xbc\xe6\x8e\xa5\xe5\x9c\xa8\xe4\xb8\x80\xe8\xb5\xb7\r\n                input = tf.concat([layers[-1], layers[skip_layer]], axis=3)\r\n\r\n            rectified = tf.nn.relu(input)\r\n            # [batch, in_height, in_width, in_channels] => [batch, in_height*2, in_width*2, out_channels]\r\n            output = gen_deconv(rectified, out_channels)\r\n            output = batchnorm(output)\r\n\r\n            if dropout > 0.0:\r\n                output = tf.nn.dropout(output, keep_prob=1 - dropout)\r\n\r\n            layers.append(output)\r\n\r\n    # decoder_1: [batch, 128, 128, ngf * 2] => [batch, 256, 256, generator_outputs_channels]\r\n    with tf.variable_scope(""decoder_1""):\r\n        input = tf.concat([layers[-1], layers[0]], axis=3)\r\n        rectified = tf.nn.relu(input)\r\n        output = gen_deconv(rectified, generator_outputs_channels)\r\n        output = tf.tanh(output)\r\n        layers.append(output)\r\n\r\n    return layers[-1]\r\n\r\n\r\ndef create_discriminator(discrim_inputs, discrim_targets, ndf=64):\r\n    """"""\xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe6\x98\xaf\xe4\xb8\xa4\xe4\xb8\xaa\xe6\xa8\xa1\xe6\x80\x81\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe3\x80\x82\xe8\xbe\x93\xe5\x87\xba  [batch, 30, 30, 1]""""""\r\n    n_layers = 3\r\n    layers = []\r\n\r\n    # \xe7\xac\xac\xe4\xb8\x80\xe6\xad\xa5\xe6\x98\xaf\xe6\x8a\x8a\xe4\xb8\xa4\xe4\xb8\xaa\xe6\xa8\xa1\xe6\x80\x81\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe5\x9c\xa8\xe6\xb7\xb1\xe5\xba\xa6\xe4\xb8\x8a\xe6\x8b\xbc\xe5\x9c\xa8\xe4\xb8\x80\xe8\xb5\xb7\r\n    # 2x [batch, height, width, in_channels] => [batch, height, width, in_channels * 2]\r\n    input = tf.concat([discrim_inputs, discrim_targets], axis=3)\r\n\r\n    # layer_1: [batch, 256, 256, in_channels * 2] => [batch, 128, 128, ndf]\r\n    with tf.variable_scope(""layer_1""):\r\n        convolved = discrim_conv(input, ndf, stride=2)\r\n        rectified = lrelu(convolved, 0.2)\r\n        layers.append(rectified)\r\n\r\n    # layer_2: [batch, 128, 128, ndf] => [batch, 64, 64, ndf * 2]\r\n    # layer_3: [batch, 64, 64, ndf * 2] => [batch, 32, 32, ndf * 4]\r\n    # layer_4: [batch, 32, 32, ndf * 4] => [batch, 31, 31, ndf * 8]\r\n    for i in range(n_layers):\r\n        with tf.variable_scope(""layer_%d"" % (len(layers) + 1)):\r\n            out_channels = ndf * min(2 ** (i + 1), 8)\r\n            stride = 1 if i == n_layers - 1 else 2  # last layer here has stride 1\r\n            convolved = discrim_conv(layers[-1], out_channels, stride=stride)\r\n            normalized = batchnorm(convolved)\r\n            rectified = lrelu(normalized, 0.2)\r\n            layers.append(rectified)\r\n\r\n    # layer_5: [batch, 31, 31, ndf * 8] => [batch, 30, 30, 1]\r\n    with tf.variable_scope(""layer_%d"" % (len(layers) + 1)):\r\n        convolved = discrim_conv(rectified, out_channels=1, stride=1)\r\n        output = tf.sigmoid(convolved)\r\n        layers.append(output)\r\n    """"""\xe6\xb3\xa8\xe6\x84\x8f D \xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe4\xb8\x8d\xe6\x98\xaf 1 \xe4\xb8\xaa\xe5\x80\xbc\xef\xbc\x8c\xe8\x80\x8c\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa 30 * 30 \xe7\x9a\x84\xe7\x9f\xa9\xe9\x98\xb5\xef\xbc\x8c\xe5\xaf\xb9\xe5\xba\x94\xe6\xaf\x8f\xe4\xb8\xaa patch \xe6\x98\xaf\xe5\x90\xa6\xe6\x98\xaf\xe7\x9c\x9f\xe5\xae\x9e\xe7\x9a\x84\xe3\x80\x82\r\n    \xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe7\x94\xa8\xe7\x9a\x84\xe6\x98\xaf sigmoid \xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe3\x80\x82\r\n    """"""\r\n    return layers[-1]\r\n\r\n\r\ndef create_model(inputs, targets, gan_weight=1.0, l1_weight=100.0, lr=0.0002, beta1=0.5):\r\n    with tf.variable_scope(""generator""):\r\n        out_channels = int(targets.get_shape()[-1])\r\n        outputs = create_generator(inputs, out_channels)  # outputs \xe6\x98\xaf\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\r\n\r\n    # create two copies of discriminator, one for real pairs and one for fake pairs\r\n    # they share the same underlying variables\r\n    with tf.name_scope(""real_discriminator""):\r\n        with tf.variable_scope(""discriminator""):\r\n            # 2x [batch, height, width, channels] => [batch, 30, 30, 1]\r\n            predict_real = create_discriminator(inputs, targets)\r\n\r\n    with tf.name_scope(""fake_discriminator""):\r\n        with tf.variable_scope(""discriminator"", reuse=True):\r\n            # 2x [batch, height, width, channels] => [batch, 30, 30, 1]\r\n            predict_fake = create_discriminator(inputs, outputs)\r\n\r\n    with tf.name_scope(""discriminator_loss""):\r\n        # minimizing -tf.log will try to get inputs to 1\r\n        # predict_real => 1\r\n        # predict_fake => 0\r\n        """"""\xe6\x88\x91\xe4\xbb\xac\xe5\xaf\xb9\xe6\xaf\x8f\xe4\xb8\xaapatch\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\xa4\xe6\x96\xad\xe7\x9c\x9f\xe5\x81\x87\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe5\x8f\x96\xe5\xb9\xb3\xe5\x9d\x87\xe5\x80\xbc\xe4\xbd\x9c\xe4\xb8\xba\xe7\xbb\x93\xe6\x9e\x9c\xe3\x80\x82\r\n        predict_real \xe8\xb6\x8a\xe5\xa4\xa7\xe8\xb6\x8a\xe5\xa5\xbd\xef\xbc\x9b 1 - predict_fake \xe8\xb6\x8a\xe5\xb0\x8f\xe8\xb6\x8a\xe5\xa5\xbd""""""\r\n        discrim_loss = tf.reduce_mean(-(tf.log(predict_real + EPS) + tf.log(1 - predict_fake + EPS)))\r\n\r\n    with tf.name_scope(""generator_loss""):\r\n        # predict_fake => 1\r\n        # abs(targets - outputs) => 0\r\n        """"""\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe5\x8c\x85\xe5\x90\xab\xe4\xb8\xa4\xe9\x83\xa8\xe5\x88\x86\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x9a\r\n        \xe4\xb8\x80\xe9\x83\xa8\xe5\x88\x86\xe6\x98\xaf\xe6\x83\xb3\xe8\xae\xa9\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe8\xb6\x8a\xe7\x9c\x9f\xe5\xae\x9e\xe8\xb6\x8a\xe5\xa5\xbd\xef\xbc\x9b\xe5\x8f\xa6\xe4\xb8\x80\xe9\x83\xa8\xe5\x88\x86\xe6\x98\xaf\xe8\xb6\x8a\xe6\x8e\xa5\xe8\xbf\x91\xe7\x9b\xae\xe6\xa0\x87\xe5\x9b\xbe\xe7\x89\x87\xe8\xb6\x8a\xe5\xa5\xbd\xe3\x80\x82\r\n        \xe5\x9b\xa0\xe4\xb8\xba\xe5\x9c\xa8\xe6\x9c\xac\xe4\xbe\x8b\xe4\xb8\xad\xe6\x98\xaf\xe7\x9b\x91\xe7\x9d\xa3\xe5\xad\xa6\xe4\xb9\xa0\xe3\x80\x82\r\n        gan_weight\xef\xbc\x9a1\xef\xbc\x8c l1_weight: 100\r\n        """"""\r\n        gen_loss_GAN = tf.reduce_mean(-tf.log(predict_fake + EPS))\r\n        gen_loss_L1 = tf.reduce_mean(tf.abs(targets - outputs))\r\n        gen_loss = gen_loss_GAN * gan_weight + gen_loss_L1 * l1_weight\r\n\r\n    with tf.name_scope(""discriminator_train""):\r\n        discrim_tvars = [var for var in tf.trainable_variables() if var.name.startswith(""discriminator"")]\r\n        discrim_optim = tf.train.AdamOptimizer(lr, beta1)\r\n        discrim_grads_and_vars = discrim_optim.compute_gradients(discrim_loss, var_list=discrim_tvars)\r\n        discrim_train = discrim_optim.apply_gradients(discrim_grads_and_vars)\r\n\r\n    with tf.name_scope(""generator_train""):\r\n        with tf.control_dependencies([discrim_train]):\r\n            gen_tvars = [var for var in tf.trainable_variables() if var.name.startswith(""generator"")]\r\n            gen_optim = tf.train.AdamOptimizer(lr, beta1)\r\n            gen_grads_and_vars = gen_optim.compute_gradients(gen_loss, var_list=gen_tvars)\r\n            gen_train = gen_optim.apply_gradients(gen_grads_and_vars)\r\n\r\n    """"""\xe5\xaf\xb9\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x8c\x87\xe6\x95\xb0\xe5\xb9\xb3\xe5\x9d\x87, \xe4\xbd\x86\xe6\x98\xaf num_updates=None\xef\xbc\x8c\xe5\xae\x9e\xe9\x99\x85\xe4\xb8\x8a\xe5\xb9\xb6\xe6\xb2\xa1\xe6\x9c\x89\xe4\xbb\x80\xe4\xb9\x88\xe6\x95\x88\xe6\x9e\x9c\xef\xbc\x8c\xe5\x8f\xaa\xe6\x98\xaf\xe6\x8a\x8a\xe8\xbf\x993\xe4\xb8\xaa\xe6\x8d\x9f\xe5\xa4\xb1\xe6\x8b\xbc\xe5\x9c\xa8\xe4\xb8\x80\xe8\xb5\xb7\xe7\xbd\xa2\xe4\xba\x86\xe3\x80\x82""""""\r\n    ema = tf.train.ExponentialMovingAverage(decay=0.99)\r\n    update_losses = ema.apply([discrim_loss, gen_loss_GAN, gen_loss_L1])\r\n\r\n    global_step = tf.train.get_or_create_global_step()\r\n    incr_global_step = tf.assign(global_step, global_step + 1)\r\n\r\n    return Model(\r\n        predict_real=predict_real,\r\n        predict_fake=predict_fake,\r\n        discrim_loss=ema.average(discrim_loss),\r\n        discrim_grads_and_vars=discrim_grads_and_vars,\r\n        gen_loss_GAN=ema.average(gen_loss_GAN),\r\n        gen_loss_L1=ema.average(gen_loss_L1),\r\n        gen_grads_and_vars=gen_grads_and_vars,\r\n        outputs=outputs,\r\n        train=tf.group(update_losses, incr_global_step, gen_train),\r\n    )\r\n'"
models/m04_pix2pix/pix2pix.py,166,"b'# -*- coding:utf-8 -*-\r\n\r\n""""""pix2pix gan\xef\xbc\x8ccopy from https://github.com/affinelayer/pix2pix-tensorflow""""""\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport argparse\r\nimport os\r\nimport json\r\nimport glob\r\nimport random\r\nimport collections\r\nimport math\r\nimport time\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(""--input_dir"", help=""path to folder containing images"")\r\nparser.add_argument(""--mode"", required=True, choices=[""train"", ""test"", ""export""])\r\nparser.add_argument(""--output_dir"", required=True, help=""where to put output files"")\r\nparser.add_argument(""--seed"", type=int)\r\nparser.add_argument(""--checkpoint"", default=None,\r\n                    help=""directory with checkpoint to resume training from or use for testing"")\r\n\r\nparser.add_argument(""--max_steps"", type=int, help=""number of training steps (0 to disable)"")\r\nparser.add_argument(""--max_epochs"", type=int, help=""number of training epochs"")\r\nparser.add_argument(""--summary_freq"", type=int, default=100, help=""update summaries every summary_freq steps"")\r\nparser.add_argument(""--progress_freq"", type=int, default=50, help=""display progress every progress_freq steps"")\r\nparser.add_argument(""--trace_freq"", type=int, default=0, help=""trace execution every trace_freq steps"")\r\nparser.add_argument(""--display_freq"", type=int, default=0,\r\n                    help=""write current training images every display_freq steps"")\r\nparser.add_argument(""--save_freq"", type=int, default=5000, help=""save model every save_freq steps, 0 to disable"")\r\n\r\nparser.add_argument(""--separable_conv"", action=""store_true"", help=""use separable convolutions in the generator"")\r\nparser.add_argument(""--aspect_ratio"", type=float, default=1.0, help=""aspect ratio of output images (width/height)"")\r\nparser.add_argument(""--lab_colorization"", action=""store_true"",\r\n                    help=""split input image into brightness (A) and color (B)"")\r\nparser.add_argument(""--batch_size"", type=int, default=1, help=""number of images in batch"")\r\nparser.add_argument(""--which_direction"", type=str, default=""AtoB"", choices=[""AtoB"", ""BtoA""])\r\nparser.add_argument(""--ngf"", type=int, default=64, help=""number of generator filters in first conv layer"")\r\nparser.add_argument(""--ndf"", type=int, default=64, help=""number of discriminator filters in first conv layer"")\r\nparser.add_argument(""--scale_size"", type=int, default=286, help=""scale images to this size before cropping to 256x256"")\r\nparser.add_argument(""--flip"", dest=""flip"", action=""store_true"", help=""flip images horizontally"")\r\nparser.add_argument(""--no_flip"", dest=""flip"", action=""store_false"", help=""don\'t flip images horizontally"")\r\nparser.set_defaults(flip=True)\r\nparser.add_argument(""--lr"", type=float, default=0.0002, help=""initial learning rate for adam"")\r\nparser.add_argument(""--beta1"", type=float, default=0.5, help=""momentum term of adam"")\r\nparser.add_argument(""--l1_weight"", type=float, default=100.0, help=""weight on L1 term for generator gradient"")\r\nparser.add_argument(""--gan_weight"", type=float, default=1.0, help=""weight on GAN term for generator gradient"")\r\n\r\n# export options\r\nparser.add_argument(""--output_filetype"", default=""png"", choices=[""png"", ""jpeg""])\r\na = parser.parse_args()\r\n\r\nEPS = 1e-12\r\nCROP_SIZE = 256\r\n\r\n\r\n""""""collections.namedtuple \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x90\x86\xe8\xa7\xa3\xe4\xb8\xba c \xe4\xb8\xad\xe7\x9a\x84 struct\xef\xbc\x8c\xe9\xa6\x96\xe5\x85\x88\xe7\xbb\x99\xe6\xaf\x8f\xe4\xb8\xaa item \xe5\x91\xbd\xe5\x90\x8d\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe7\xbb\x99\xe6\xaf\x8f\xe4\xb8\xaaitem \xe8\xbf\x9b\xe8\xa1\x8c\xe8\xb5\x8b\xe5\x80\xbc""""""\r\nExamples = collections.namedtuple(""Examples"", ""paths, inputs, targets, count, steps_per_epoch"")\r\nModel = collections.namedtuple(""Model"",\r\n                               ""outputs, predict_real, predict_fake, discrim_loss, discrim_grads_and_vars, gen_loss_GAN, gen_loss_L1, gen_grads_and_vars, train"")\r\n\r\n\r\ndef preprocess(image):\r\n    with tf.name_scope(""preprocess""):\r\n        # [0, 1] => [-1, 1]\r\n        return image * 2 - 1\r\n\r\n\r\ndef deprocess(image):\r\n    with tf.name_scope(""deprocess""):\r\n        # [-1, 1] => [0, 1]\r\n        return (image + 1) / 2\r\n\r\n\r\ndef preprocess_lab(lab):\r\n    with tf.name_scope(""preprocess_lab""):\r\n        L_chan, a_chan, b_chan = tf.unstack(lab, axis=2)\r\n        # L_chan: black and white with input range [0, 100]\r\n        # a_chan/b_chan: color channels with input range ~[-110, 110], not exact\r\n        # [0, 100] => [-1, 1],  ~[-110, 110] => [-1, 1]\r\n        return [L_chan / 50 - 1, a_chan / 110, b_chan / 110]\r\n\r\n\r\ndef deprocess_lab(L_chan, a_chan, b_chan):\r\n    with tf.name_scope(""deprocess_lab""):\r\n        # this is axis=3 instead of axis=2 because we process individual images but deprocess batches\r\n        return tf.stack([(L_chan + 1) / 2 * 100, a_chan * 110, b_chan * 110], axis=3)\r\n\r\n\r\ndef augment(image, brightness):\r\n    # (a, b) color channels, combine with L channel and convert to rgb\r\n    a_chan, b_chan = tf.unstack(image, axis=3)\r\n    L_chan = tf.squeeze(brightness, axis=3)\r\n    lab = deprocess_lab(L_chan, a_chan, b_chan)\r\n    rgb = lab_to_rgb(lab)\r\n    return rgb\r\n\r\n\r\ndef discrim_conv(batch_input, out_channels, stride):\r\n    padded_input = tf.pad(batch_input, [[0, 0], [1, 1], [1, 1], [0, 0]], mode=""CONSTANT"")\r\n    return tf.layers.conv2d(padded_input, out_channels, kernel_size=4, strides=(stride, stride), padding=""valid"",\r\n                            kernel_initializer=tf.random_normal_initializer(0, 0.02))\r\n\r\n\r\ndef gen_conv(batch_input, out_channels):\r\n    """"""\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe7\x9a\x84 encoder \xe9\x83\xa8\xe5\x88\x86\xe3\x80\x82""""""\r\n    # [batch, in_height, in_width, in_channels] => [batch, out_height, out_width, out_channels]\r\n    initializer = tf.random_normal_initializer(0, 0.02)\r\n    if a.separable_conv:   # \xe6\xb7\xb1\xe5\xba\xa6\xe5\x8f\xaf\xe5\x88\x86\xe5\x8d\xb7\xe7\xa7\xaf\r\n        return tf.layers.separable_conv2d(batch_input, out_channels, kernel_size=4, strides=(2, 2), padding=""same"",\r\n                                          depthwise_initializer=initializer, pointwise_initializer=initializer)\r\n    else:\r\n        return tf.layers.conv2d(batch_input, out_channels, kernel_size=4, strides=(2, 2), padding=""same"",\r\n                                kernel_initializer=initializer)\r\n\r\n\r\ndef gen_deconv(batch_input, out_channels):\r\n    """"""\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe7\x9a\x84 decoder \xe9\x83\xa8\xe5\x88\x86\xe3\x80\x82""""""\r\n    # [batch, in_height, in_width, in_channels] => [batch, out_height, out_width, out_channels]\r\n    initializer = tf.random_normal_initializer(0, 0.02)\r\n    if a.separable_conv:\r\n        _b, h, w, _c = batch_input.shape\r\n        resized_input = tf.image.resize_images(batch_input, [h * 2, w * 2],\r\n                                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\r\n        return tf.layers.separable_conv2d(resized_input, out_channels, kernel_size=4, strides=(1, 1), padding=""same"",\r\n                                          depthwise_initializer=initializer, pointwise_initializer=initializer)\r\n    else:\r\n        return tf.layers.conv2d_transpose(batch_input, out_channels, kernel_size=4, strides=(2, 2), padding=""same"",\r\n                                          kernel_initializer=initializer)\r\n\r\n\r\ndef lrelu(x, a):\r\n    with tf.name_scope(""lrelu""):\r\n        # adding these together creates the leak part and linear part\r\n        # then cancels them out by subtracting/adding an absolute value term\r\n        # leak: a*x/2 - a*abs(x)/2\r\n        # linear: x/2 + abs(x)/2\r\n\r\n        # this block looks like it has 2 inputs on the graph unless we do this\r\n        x = tf.identity(x)\r\n        return (0.5 * (1 + a)) * x + (0.5 * (1 - a)) * tf.abs(x)\r\n\r\n\r\ndef batchnorm(inputs):\r\n    return tf.layers.batch_normalization(inputs, axis=3, epsilon=1e-5, momentum=0.1, training=True,\r\n                                         gamma_initializer=tf.random_normal_initializer(1.0, 0.02))\r\n\r\n\r\ndef check_image(image):\r\n    assertion = tf.assert_equal(tf.shape(image)[-1], 3, message=""image must have 3 color channels"")\r\n    with tf.control_dependencies([assertion]):\r\n        image = tf.identity(image)\r\n\r\n    if image.get_shape().ndims not in (3, 4):\r\n        raise ValueError(""image must be either 3 or 4 dimensions"")\r\n\r\n    # make the last dimension 3 so that you can unstack the colors\r\n    shape = list(image.get_shape())\r\n    shape[-1] = 3\r\n    image.set_shape(shape)\r\n    return image\r\n\r\n\r\n# based on https://github.com/torch/image/blob/9f65c30167b2048ecbe8b7befdc6b2d6d12baee9/generic/image.c\r\ndef rgb_to_lab(srgb):\r\n    with tf.name_scope(""rgb_to_lab""):\r\n        srgb = check_image(srgb)\r\n        srgb_pixels = tf.reshape(srgb, [-1, 3])\r\n\r\n        with tf.name_scope(""srgb_to_xyz""):\r\n            linear_mask = tf.cast(srgb_pixels <= 0.04045, dtype=tf.float32)\r\n            exponential_mask = tf.cast(srgb_pixels > 0.04045, dtype=tf.float32)\r\n            rgb_pixels = (srgb_pixels / 12.92 * linear_mask) + (((\r\n                                                                 srgb_pixels + 0.055) / 1.055) ** 2.4) * exponential_mask\r\n            rgb_to_xyz = tf.constant([\r\n                #    X        Y          Z\r\n                [0.412453, 0.212671, 0.019334],  # R\r\n                [0.357580, 0.715160, 0.119193],  # G\r\n                [0.180423, 0.072169, 0.950227],  # B\r\n            ])\r\n            xyz_pixels = tf.matmul(rgb_pixels, rgb_to_xyz)\r\n\r\n        # https://en.wikipedia.org/wiki/Lab_color_space#CIELAB-CIEXYZ_conversions\r\n        with tf.name_scope(""xyz_to_cielab""):\r\n            # convert to fx = f(X/Xn), fy = f(Y/Yn), fz = f(Z/Zn)\r\n\r\n            # normalize for D65 white point\r\n            xyz_normalized_pixels = tf.multiply(xyz_pixels, [1 / 0.950456, 1.0, 1 / 1.088754])\r\n\r\n            epsilon = 6 / 29\r\n            linear_mask = tf.cast(xyz_normalized_pixels <= (epsilon ** 3), dtype=tf.float32)\r\n            exponential_mask = tf.cast(xyz_normalized_pixels > (epsilon ** 3), dtype=tf.float32)\r\n            fxfyfz_pixels = (xyz_normalized_pixels / (3 * epsilon ** 2) + 4 / 29) * linear_mask + (\r\n                                                                                                  xyz_normalized_pixels ** (\r\n                                                                                                  1 / 3)) * exponential_mask\r\n\r\n            # convert to lab\r\n            fxfyfz_to_lab = tf.constant([\r\n                #  l       a       b\r\n                [0.0, 500.0, 0.0],  # fx\r\n                [116.0, -500.0, 200.0],  # fy\r\n                [0.0, 0.0, -200.0],  # fz\r\n            ])\r\n            lab_pixels = tf.matmul(fxfyfz_pixels, fxfyfz_to_lab) + tf.constant([-16.0, 0.0, 0.0])\r\n\r\n        return tf.reshape(lab_pixels, tf.shape(srgb))\r\n\r\n\r\ndef lab_to_rgb(lab):\r\n    with tf.name_scope(""lab_to_rgb""):\r\n        lab = check_image(lab)\r\n        lab_pixels = tf.reshape(lab, [-1, 3])\r\n\r\n        # https://en.wikipedia.org/wiki/Lab_color_space#CIELAB-CIEXYZ_conversions\r\n        with tf.name_scope(""cielab_to_xyz""):\r\n            # convert to fxfyfz\r\n            lab_to_fxfyfz = tf.constant([\r\n                #   fx      fy        fz\r\n                [1 / 116.0, 1 / 116.0, 1 / 116.0],  # l\r\n                [1 / 500.0, 0.0, 0.0],  # a\r\n                [0.0, 0.0, -1 / 200.0],  # b\r\n            ])\r\n            fxfyfz_pixels = tf.matmul(lab_pixels + tf.constant([16.0, 0.0, 0.0]), lab_to_fxfyfz)\r\n\r\n            # convert to xyz\r\n            epsilon = 6 / 29\r\n            linear_mask = tf.cast(fxfyfz_pixels <= epsilon, dtype=tf.float32)\r\n            exponential_mask = tf.cast(fxfyfz_pixels > epsilon, dtype=tf.float32)\r\n            xyz_pixels = (3 * epsilon ** 2 * (fxfyfz_pixels - 4 / 29)) * linear_mask + (\r\n                                                                                       fxfyfz_pixels ** 3) * exponential_mask\r\n\r\n            # denormalize for D65 white point\r\n            xyz_pixels = tf.multiply(xyz_pixels, [0.950456, 1.0, 1.088754])\r\n\r\n        with tf.name_scope(""xyz_to_srgb""):\r\n            xyz_to_rgb = tf.constant([\r\n                #     r           g          b\r\n                [3.2404542, -0.9692660, 0.0556434],  # x\r\n                [-1.5371385, 1.8760108, -0.2040259],  # y\r\n                [-0.4985314, 0.0415560, 1.0572252],  # z\r\n            ])\r\n            rgb_pixels = tf.matmul(xyz_pixels, xyz_to_rgb)\r\n            # avoid a slightly negative number messing up the conversion\r\n            rgb_pixels = tf.clip_by_value(rgb_pixels, 0.0, 1.0)\r\n            linear_mask = tf.cast(rgb_pixels <= 0.0031308, dtype=tf.float32)\r\n            exponential_mask = tf.cast(rgb_pixels > 0.0031308, dtype=tf.float32)\r\n            srgb_pixels = (rgb_pixels * 12.92 * linear_mask) + ((rgb_pixels ** (\r\n            1 / 2.4) * 1.055) - 0.055) * exponential_mask\r\n\r\n        return tf.reshape(srgb_pixels, tf.shape(lab))\r\n\r\n\r\ndef load_examples():\r\n    if a.input_dir is None or not os.path.exists(a.input_dir):\r\n        raise Exception(""input_dir does not exist"")\r\n\r\n    input_paths = glob.glob(os.path.join(a.input_dir, ""*.jpg""))\r\n    decode = tf.image.decode_jpeg\r\n    if len(input_paths) == 0:\r\n        input_paths = glob.glob(os.path.join(a.input_dir, ""*.png""))\r\n        decode = tf.image.decode_png\r\n\r\n    if len(input_paths) == 0:\r\n        raise Exception(""input_dir contains no image files"")\r\n\r\n    def get_name(path):\r\n        name, _ = os.path.splitext(os.path.basename(path))\r\n        return name\r\n\r\n    # if the image names are numbers, sort by the value rather than asciibetically\r\n    # having sorted inputs means that the outputs are sorted in test mode\r\n    if all(get_name(path).isdigit() for path in input_paths):\r\n        input_paths = sorted(input_paths, key=lambda path: int(get_name(path)))\r\n    else:\r\n        input_paths = sorted(input_paths)\r\n\r\n    with tf.name_scope(""load_images""):\r\n        path_queue = tf.train.string_input_producer(input_paths, shuffle=a.mode == ""train"")\r\n        reader = tf.WholeFileReader()\r\n        paths, contents = reader.read(path_queue)\r\n        raw_input = decode(contents)\r\n        raw_input = tf.image.convert_image_dtype(raw_input, dtype=tf.float32)\r\n\r\n        assertion = tf.assert_equal(tf.shape(raw_input)[2], 3, message=""image does not have 3 channels"")\r\n        with tf.control_dependencies([assertion]):\r\n            raw_input = tf.identity(raw_input)\r\n\r\n        raw_input.set_shape([None, None, 3])\r\n\r\n        if a.lab_colorization:\r\n            # load color and brightness from image, no B image exists here\r\n            lab = rgb_to_lab(raw_input)\r\n            L_chan, a_chan, b_chan = preprocess_lab(lab)\r\n            a_images = tf.expand_dims(L_chan, axis=2)\r\n            b_images = tf.stack([a_chan, b_chan], axis=2)\r\n        else:\r\n            # break apart image pair and move to range [-1, 1]\r\n            width = tf.shape(raw_input)[1]  # [height, width, channels]\r\n            a_images = preprocess(raw_input[:, :width // 2, :])\r\n            b_images = preprocess(raw_input[:, width // 2:, :])\r\n\r\n    if a.which_direction == ""AtoB"":\r\n        inputs, targets = [a_images, b_images]\r\n    elif a.which_direction == ""BtoA"":\r\n        inputs, targets = [b_images, a_images]\r\n    else:\r\n        raise Exception(""invalid direction"")\r\n\r\n    # synchronize seed for image operations so that we do the same operations to both\r\n    # input and output images\r\n    seed = random.randint(0, 2 ** 31 - 1)\r\n\r\n    def transform(image):\r\n        r = image\r\n        if a.flip:\r\n            r = tf.image.random_flip_left_right(r, seed=seed)\r\n\r\n        # area produces a nice downscaling, but does nearest neighbor for upscaling\r\n        # assume we\'re going to be doing downscaling here\r\n        r = tf.image.resize_images(r, [a.scale_size, a.scale_size], method=tf.image.ResizeMethod.AREA)\r\n\r\n        offset = tf.cast(tf.floor(tf.random_uniform([2], 0, a.scale_size - CROP_SIZE + 1, seed=seed)), dtype=tf.int32)\r\n        if a.scale_size > CROP_SIZE:\r\n            r = tf.image.crop_to_bounding_box(r, offset[0], offset[1], CROP_SIZE, CROP_SIZE)\r\n        elif a.scale_size < CROP_SIZE:\r\n            raise Exception(""scale size cannot be less than crop size"")\r\n        return r\r\n\r\n    with tf.name_scope(""input_images""):\r\n        input_images = transform(inputs)\r\n\r\n    with tf.name_scope(""target_images""):\r\n        target_images = transform(targets)\r\n\r\n    paths_batch, inputs_batch, targets_batch = tf.train.batch([paths, input_images, target_images],\r\n                                                              batch_size=a.batch_size)\r\n    steps_per_epoch = int(math.ceil(len(input_paths) / a.batch_size))\r\n\r\n    return Examples(\r\n        paths=paths_batch,\r\n        inputs=inputs_batch,\r\n        targets=targets_batch,\r\n        count=len(input_paths),\r\n        steps_per_epoch=steps_per_epoch,\r\n    )\r\n\r\n\r\ndef create_generator(generator_inputs, generator_outputs_channels):\r\n    layers = []\r\n\r\n    # encoder_1: [batch, 256, 256, in_channels] => [batch, 128, 128, ngf]\r\n    with tf.variable_scope(""encoder_1""):\r\n        output = gen_conv(generator_inputs, a.ngf)\r\n        layers.append(output)\r\n\r\n    layer_specs = [\r\n        a.ngf * 2,  # encoder_2: [batch, 128, 128, ngf] => [batch, 64, 64, ngf * 2]\r\n        a.ngf * 4,  # encoder_3: [batch, 64, 64, ngf * 2] => [batch, 32, 32, ngf * 4]\r\n        a.ngf * 8,  # encoder_4: [batch, 32, 32, ngf * 4] => [batch, 16, 16, ngf * 8]\r\n        a.ngf * 8,  # encoder_5: [batch, 16, 16, ngf * 8] => [batch, 8, 8, ngf * 8]\r\n        a.ngf * 8,  # encoder_6: [batch, 8, 8, ngf * 8] => [batch, 4, 4, ngf * 8]\r\n        a.ngf * 8,  # encoder_7: [batch, 4, 4, ngf * 8] => [batch, 2, 2, ngf * 8]\r\n        a.ngf * 8,  # encoder_8: [batch, 2, 2, ngf * 8] => [batch, 1, 1, ngf * 8]\r\n    ]\r\n\r\n    for out_channels in layer_specs:\r\n        with tf.variable_scope(""encoder_%d"" % (len(layers) + 1)):\r\n            rectified = lrelu(layers[-1], 0.2)\r\n            # [batch, in_height, in_width, in_channels] => [batch, in_height/2, in_width/2, out_channels]\r\n            convolved = gen_conv(rectified, out_channels)\r\n            output = batchnorm(convolved)\r\n            layers.append(output)\r\n\r\n    layer_specs = [\r\n        (a.ngf * 8, 0.5),  # decoder_8: [batch, 1, 1, ngf * 8] => [batch, 2, 2, ngf * 8 * 2]\r\n        (a.ngf * 8, 0.5),  # decoder_7: [batch, 2, 2, ngf * 8 * 2] => [batch, 4, 4, ngf * 8 * 2]\r\n        (a.ngf * 8, 0.5),  # decoder_6: [batch, 4, 4, ngf * 8 * 2] => [batch, 8, 8, ngf * 8 * 2]\r\n        (a.ngf * 8, 0.0),  # decoder_5: [batch, 8, 8, ngf * 8 * 2] => [batch, 16, 16, ngf * 8 * 2]\r\n        (a.ngf * 4, 0.0),  # decoder_4: [batch, 16, 16, ngf * 8 * 2] => [batch, 32, 32, ngf * 4 * 2]\r\n        (a.ngf * 2, 0.0),  # decoder_3: [batch, 32, 32, ngf * 4 * 2] => [batch, 64, 64, ngf * 2 * 2]\r\n        (a.ngf, 0.0),  # decoder_2: [batch, 64, 64, ngf * 2 * 2] => [batch, 128, 128, ngf * 2]\r\n    ]\r\n\r\n    num_encoder_layers = len(layers)\r\n    for decoder_layer, (out_channels, dropout) in enumerate(layer_specs):\r\n        skip_layer = num_encoder_layers - decoder_layer - 1\r\n        with tf.variable_scope(""decoder_%d"" % (skip_layer + 1)):\r\n            if decoder_layer == 0:\r\n                # first decoder layer doesn\'t have skip connections\r\n                # since it is directly connected to the skip_layer\r\n                input = layers[-1]\r\n            else:\r\n                input = tf.concat([layers[-1], layers[skip_layer]], axis=3)\r\n\r\n            rectified = tf.nn.relu(input)\r\n            # [batch, in_height, in_width, in_channels] => [batch, in_height*2, in_width*2, out_channels]\r\n            output = gen_deconv(rectified, out_channels)\r\n            output = batchnorm(output)\r\n\r\n            if dropout > 0.0:\r\n                output = tf.nn.dropout(output, keep_prob=1 - dropout)\r\n\r\n            layers.append(output)\r\n\r\n    # decoder_1: [batch, 128, 128, ngf * 2] => [batch, 256, 256, generator_outputs_channels]\r\n    with tf.variable_scope(""decoder_1""):\r\n        input = tf.concat([layers[-1], layers[0]], axis=3)\r\n        rectified = tf.nn.relu(input)\r\n        output = gen_deconv(rectified, generator_outputs_channels)\r\n        output = tf.tanh(output)\r\n        layers.append(output)\r\n\r\n    return layers[-1]\r\n\r\n\r\ndef create_model(inputs, targets):\r\n    def create_discriminator(discrim_inputs, discrim_targets):\r\n        n_layers = 3\r\n        layers = []\r\n\r\n        # 2x [batch, height, width, in_channels] => [batch, height, width, in_channels * 2]\r\n        input = tf.concat([discrim_inputs, discrim_targets], axis=3)\r\n\r\n        # layer_1: [batch, 256, 256, in_channels * 2] => [batch, 128, 128, ndf]\r\n        with tf.variable_scope(""layer_1""):\r\n            convolved = discrim_conv(input, a.ndf, stride=2)\r\n            rectified = lrelu(convolved, 0.2)\r\n            layers.append(rectified)\r\n\r\n        # layer_2: [batch, 128, 128, ndf] => [batch, 64, 64, ndf * 2]\r\n        # layer_3: [batch, 64, 64, ndf * 2] => [batch, 32, 32, ndf * 4]\r\n        # layer_4: [batch, 32, 32, ndf * 4] => [batch, 31, 31, ndf * 8]\r\n        for i in range(n_layers):\r\n            with tf.variable_scope(""layer_%d"" % (len(layers) + 1)):\r\n                out_channels = a.ndf * min(2 ** (i + 1), 8)\r\n                stride = 1 if i == n_layers - 1 else 2  # last layer here has stride 1\r\n                convolved = discrim_conv(layers[-1], out_channels, stride=stride)\r\n                normalized = batchnorm(convolved)\r\n                rectified = lrelu(normalized, 0.2)\r\n                layers.append(rectified)\r\n\r\n        # layer_5: [batch, 31, 31, ndf * 8] => [batch, 30, 30, 1]\r\n        with tf.variable_scope(""layer_%d"" % (len(layers) + 1)):\r\n            convolved = discrim_conv(rectified, out_channels=1, stride=1)\r\n            output = tf.sigmoid(convolved)\r\n            layers.append(output)\r\n\r\n        return layers[-1]\r\n\r\n    with tf.variable_scope(""generator""):\r\n        out_channels = int(targets.get_shape()[-1])\r\n        outputs = create_generator(inputs, out_channels)\r\n\r\n    # create two copies of discriminator, one for real pairs and one for fake pairs\r\n    # they share the same underlying variables\r\n    with tf.name_scope(""real_discriminator""):\r\n        with tf.variable_scope(""discriminator""):\r\n            # 2x [batch, height, width, channels] => [batch, 30, 30, 1]\r\n            predict_real = create_discriminator(inputs, targets)\r\n\r\n    with tf.name_scope(""fake_discriminator""):\r\n        with tf.variable_scope(""discriminator"", reuse=True):\r\n            # 2x [batch, height, width, channels] => [batch, 30, 30, 1]\r\n            predict_fake = create_discriminator(inputs, outputs)\r\n\r\n    with tf.name_scope(""discriminator_loss""):\r\n        # minimizing -tf.log will try to get inputs to 1\r\n        # predict_real => 1\r\n        # predict_fake => 0\r\n        discrim_loss = tf.reduce_mean(-(tf.log(predict_real + EPS) + tf.log(1 - predict_fake + EPS)))\r\n\r\n    with tf.name_scope(""generator_loss""):\r\n        # predict_fake => 1\r\n        # abs(targets - outputs) => 0\r\n        gen_loss_GAN = tf.reduce_mean(-tf.log(predict_fake + EPS))\r\n        gen_loss_L1 = tf.reduce_mean(tf.abs(targets - outputs))\r\n        gen_loss = gen_loss_GAN * a.gan_weight + gen_loss_L1 * a.l1_weight\r\n\r\n    with tf.name_scope(""discriminator_train""):\r\n        discrim_tvars = [var for var in tf.trainable_variables() if var.name.startswith(""discriminator"")]\r\n        discrim_optim = tf.train.AdamOptimizer(a.lr, a.beta1)\r\n        discrim_grads_and_vars = discrim_optim.compute_gradients(discrim_loss, var_list=discrim_tvars)\r\n        discrim_train = discrim_optim.apply_gradients(discrim_grads_and_vars)\r\n\r\n    with tf.name_scope(""generator_train""):\r\n        with tf.control_dependencies([discrim_train]):\r\n            gen_tvars = [var for var in tf.trainable_variables() if var.name.startswith(""generator"")]\r\n            gen_optim = tf.train.AdamOptimizer(a.lr, a.beta1)\r\n            gen_grads_and_vars = gen_optim.compute_gradients(gen_loss, var_list=gen_tvars)\r\n            gen_train = gen_optim.apply_gradients(gen_grads_and_vars)\r\n\r\n    ema = tf.train.ExponentialMovingAverage(decay=0.99)\r\n    update_losses = ema.apply([discrim_loss, gen_loss_GAN, gen_loss_L1])\r\n\r\n    global_step = tf.train.get_or_create_global_step()\r\n    incr_global_step = tf.assign(global_step, global_step + 1)\r\n\r\n    return Model(\r\n        predict_real=predict_real,\r\n        predict_fake=predict_fake,\r\n        discrim_loss=ema.average(discrim_loss),\r\n        discrim_grads_and_vars=discrim_grads_and_vars,\r\n        gen_loss_GAN=ema.average(gen_loss_GAN),\r\n        gen_loss_L1=ema.average(gen_loss_L1),\r\n        gen_grads_and_vars=gen_grads_and_vars,\r\n        outputs=outputs,\r\n        train=tf.group(update_losses, incr_global_step, gen_train),\r\n    )\r\n\r\n\r\ndef save_images(fetches, step=None):\r\n    image_dir = os.path.join(a.output_dir, ""images"")\r\n    if not os.path.exists(image_dir):\r\n        os.makedirs(image_dir)\r\n\r\n    filesets = []\r\n    for i, in_path in enumerate(fetches[""paths""]):  # batch \xe4\xb8\xad\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe5\x9b\xbe\xe7\x89\x87\r\n        name, _ = os.path.splitext(os.path.basename(in_path.decode(""utf8"")))\r\n        fileset = {""name"": name, ""step"": step}\r\n        for kind in [""inputs"", ""outputs"", ""targets""]:  # \xe4\xb8\x89\xe7\xa7\x8d\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x9a\xe8\xbe\x93\xe5\x85\xa5\xe6\xa8\xa1\xe6\x80\x811\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe7\x94\x9f\xe6\x88\x90\xe6\xa8\xa1\xe6\x80\x812\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe7\x9c\x9f\xe5\xae\x9e\xe6\xa8\xa1\xe6\x80\x812\xe5\x9b\xbe\xe7\x89\x87\r\n            filename = name + ""-"" + kind + "".png""\r\n            if step is not None:\r\n                filename = ""%08d-%s"" % (step, filename)\r\n            fileset[kind] = filename\r\n            out_path = os.path.join(image_dir, filename)\r\n            contents = fetches[kind][i]\r\n            with open(out_path, ""wb"") as f:\r\n                f.write(contents)\r\n        filesets.append(fileset)\r\n    return filesets\r\n\r\n\r\ndef append_index(filesets, step=False):\r\n    index_path = os.path.join(a.output_dir, ""index.html"")\r\n    if os.path.exists(index_path):\r\n        index = open(index_path, ""a"")\r\n    else:\r\n        index = open(index_path, ""w"")\r\n        index.write(""<html><body><table><tr>"")\r\n        if step:\r\n            index.write(""<th>step</th>"")\r\n        index.write(""<th>name</th><th>input</th><th>output</th><th>target</th></tr>"")\r\n\r\n    for fileset in filesets:\r\n        index.write(""<tr>"")\r\n\r\n        if step:\r\n            index.write(""<td>%d</td>"" % fileset[""step""])\r\n        index.write(""<td>%s</td>"" % fileset[""name""])\r\n\r\n        for kind in [""inputs"", ""outputs"", ""targets""]:\r\n            index.write(""<td><img src=\'images/%s\'></td>"" % fileset[kind])\r\n\r\n        index.write(""</tr>"")\r\n    return index_path\r\n\r\n\r\ndef main():\r\n    if a.seed is None:\r\n        a.seed = random.randint(0, 2 ** 31 - 1)\r\n\r\n    tf.set_random_seed(a.seed)\r\n    np.random.seed(a.seed)\r\n    random.seed(a.seed)\r\n\r\n    if not os.path.exists(a.output_dir):\r\n        os.makedirs(a.output_dir)\r\n\r\n    if a.mode == ""test"" or a.mode == ""export"":\r\n        if a.checkpoint is None:\r\n            raise Exception(""checkpoint required for test mode"")\r\n\r\n        # load some options from the checkpoint\r\n        options = {""which_direction"", ""ngf"", ""ndf"", ""lab_colorization""}\r\n        with open(os.path.join(a.checkpoint, ""options.json"")) as f:\r\n            for key, val in json.loads(f.read()).items():\r\n                if key in options:\r\n                    print(""loaded"", key, ""="", val)\r\n                    setattr(a, key, val)\r\n        # disable these features in test mode\r\n        a.scale_size = CROP_SIZE\r\n        a.flip = False\r\n\r\n    # \xe6\x89\x93\xe5\x8d\xb0\xe5\x8f\x82\xe6\x95\xb0\r\n    for k, v in a._get_kwargs():\r\n        print(k, ""="", v)\r\n\r\n    # \xe5\x86\x99\xe5\x85\xa5\xe5\x8f\x82\xe6\x95\xb0\r\n    with open(os.path.join(a.output_dir, ""options.json""), ""w"") as f:\r\n        f.write(json.dumps(vars(a), sort_keys=True, indent=4))\r\n\r\n    if a.mode == ""export"":\r\n        # export the generator to a meta graph that can be imported later for standalone generation\r\n        if a.lab_colorization:\r\n            raise Exception(""export not supported for lab_colorization"")\r\n\r\n        input = tf.placeholder(tf.string, shape=[1])\r\n        input_data = tf.decode_base64(input[0])\r\n        input_image = tf.image.decode_png(input_data)\r\n\r\n        # remove alpha channel if present\r\n        input_image = tf.cond(tf.equal(tf.shape(input_image)[2], 4), lambda: input_image[:, :, :3], lambda: input_image)\r\n        # convert grayscale to RGB\r\n        input_image = tf.cond(tf.equal(tf.shape(input_image)[2], 1), lambda: tf.image.grayscale_to_rgb(input_image),\r\n                              lambda: input_image)\r\n\r\n        input_image = tf.image.convert_image_dtype(input_image, dtype=tf.float32)\r\n        input_image.set_shape([CROP_SIZE, CROP_SIZE, 3])\r\n        batch_input = tf.expand_dims(input_image, axis=0)\r\n\r\n        with tf.variable_scope(""generator""):\r\n            batch_output = deprocess(create_generator(preprocess(batch_input), 3))\r\n\r\n        output_image = tf.image.convert_image_dtype(batch_output, dtype=tf.uint8)[0]\r\n        if a.output_filetype == ""png"":\r\n            output_data = tf.image.encode_png(output_image)\r\n        elif a.output_filetype == ""jpeg"":\r\n            output_data = tf.image.encode_jpeg(output_image, quality=80)\r\n        else:\r\n            raise Exception(""invalid filetype"")\r\n        output = tf.convert_to_tensor([tf.encode_base64(output_data)])\r\n\r\n        key = tf.placeholder(tf.string, shape=[1])\r\n        inputs = {\r\n            ""key"": key.name,\r\n            ""input"": input.name\r\n        }\r\n        tf.add_to_collection(""inputs"", json.dumps(inputs))\r\n        outputs = {\r\n            ""key"": tf.identity(key).name,\r\n            ""output"": output.name,\r\n        }\r\n        tf.add_to_collection(""outputs"", json.dumps(outputs))\r\n\r\n        init_op = tf.global_variables_initializer()\r\n        restore_saver = tf.train.Saver()\r\n        export_saver = tf.train.Saver()\r\n\r\n        with tf.Session() as sess:\r\n            sess.run(init_op)\r\n            print(""loading model from checkpoint"")\r\n            checkpoint = tf.train.latest_checkpoint(a.checkpoint)\r\n            restore_saver.restore(sess, checkpoint)\r\n            print(""exporting model"")\r\n            export_saver.export_meta_graph(filename=os.path.join(a.output_dir, ""export.meta""))\r\n            export_saver.save(sess, os.path.join(a.output_dir, ""export""), write_meta_graph=False)\r\n\r\n        return\r\n\r\n    # \xe5\x87\x86\xe5\xa4\x87\xe6\x95\xb0\xe6\x8d\xae\xe8\xaf\xbb\xe5\x85\xa5\r\n    examples = load_examples()\r\n    print(""examples count = %d"" % examples.count)\r\n\r\n    # \xe5\x88\x9b\xe5\xbb\xba\xe6\xa8\xa1\xe5\x9e\x8b\r\n    # inputs and targets are [batch_size, height, width, channels]\r\n    model = create_model(examples.inputs, examples.targets)\r\n\r\n    # undo colorization splitting on images that we use for display/output\r\n    if a.lab_colorization:\r\n        if a.which_direction == ""AtoB"":\r\n            # inputs is brightness, this will be handled fine as a grayscale image\r\n            # need to augment targets and outputs with brightness\r\n            targets = augment(examples.targets, examples.inputs)\r\n            outputs = augment(model.outputs, examples.inputs)\r\n            # inputs can be deprocessed normally and handled as if they are single channel\r\n            # grayscale images\r\n            inputs = deprocess(examples.inputs)\r\n        elif a.which_direction == ""BtoA"":\r\n            # inputs will be color channels only, get brightness from targets\r\n            inputs = augment(examples.inputs, examples.targets)\r\n            targets = deprocess(examples.targets)\r\n            outputs = deprocess(model.outputs)\r\n        else:\r\n            raise Exception(""invalid direction"")\r\n    else:  # \xe6\x8a\x8a\xe6\x95\xb0\xe6\x8d\xae\xe6\x98\xa0\xe5\xb0\x84\xe5\x9b\x9e\xe5\x88\xb0 (0, 1) \xe4\xb9\x8b\xe9\x97\xb4\xe4\xbe\xbf\xe4\xba\x8e\xe6\x9c\x80\xe5\x90\x8e\xe4\xbf\x9d\xe5\xad\x98\xe5\x9b\xbe\xe7\x89\x87\r\n        inputs = deprocess(examples.inputs)\r\n        targets = deprocess(examples.targets)\r\n        outputs = deprocess(model.outputs)\r\n\r\n    def convert(image):\r\n        """"""\xe6\x8a\x8a\xe5\x8e\x9f\xe5\xa7\x8b\xe7\x9a\x84img\xe6\x94\xb9\xe6\x88\x90\xe6\x95\xb4\xe5\xbd\xa2\xe7\x9a\x84\xe7\xb1\xbb\xe5\x9e\x8b\xef\xbc\x8c\xe8\x87\xaa\xe5\x8a\xa8\xe7\x9a\x84\xe6\x98\xa0\xe5\xb0\x84\xe5\x88\xb0 [0,255]\xe9\x97\xb4""""""\r\n        if a.aspect_ratio != 1.0:  # \xe5\xa6\x82\xe6\x9e\x9c aspect_ratio \xe4\xb8\x8d\xe6\x98\xaf 1 \xef\xbc\x88\xe4\xb8\x8d\xe6\x98\xaf\xe6\x96\xb9\xe5\xbd\xa2\xef\xbc\x89\xe7\x9a\x84\xe8\xaf\x9d\xe9\x9c\x80\xe8\xa6\x81resize\xe6\x81\xa2\xe5\xa4\x8d\xe5\xa4\xa7\xe5\xb0\x8f\r\n            # upscale to correct aspect ratio\r\n            size = [CROP_SIZE, int(round(CROP_SIZE * a.aspect_ratio))]\r\n            image = tf.image.resize_images(image, size=size, method=tf.image.ResizeMethod.BICUBIC)\r\n\r\n        return tf.image.convert_image_dtype(image, dtype=tf.uint8, saturate=True)\r\n\r\n    # reverse any processing on images so they can be written to disk or displayed to user\r\n    with tf.name_scope(""convert_inputs""):\r\n        converted_inputs = convert(inputs)\r\n\r\n    with tf.name_scope(""convert_targets""):\r\n        converted_targets = convert(targets)\r\n\r\n    with tf.name_scope(""convert_outputs""):\r\n        converted_outputs = convert(outputs)\r\n\r\n    with tf.name_scope(""encode_images""):\r\n        display_fetches = {\r\n            ""paths"": examples.paths,\r\n            ""inputs"": tf.map_fn(tf.image.encode_png, converted_inputs, dtype=tf.string, name=""input_pngs""),\r\n            ""targets"": tf.map_fn(tf.image.encode_png, converted_targets, dtype=tf.string, name=""target_pngs""),\r\n            ""outputs"": tf.map_fn(tf.image.encode_png, converted_outputs, dtype=tf.string, name=""output_pngs""),\r\n        }\r\n\r\n    # summaries\r\n    with tf.name_scope(""inputs_summary""):\r\n        tf.summary.image(""inputs"", converted_inputs)\r\n\r\n    with tf.name_scope(""targets_summary""):\r\n        tf.summary.image(""targets"", converted_targets)\r\n\r\n    with tf.name_scope(""outputs_summary""):\r\n        tf.summary.image(""outputs"", converted_outputs)\r\n\r\n    with tf.name_scope(""predict_real_summary""):\r\n        tf.summary.image(""predict_real"", tf.image.convert_image_dtype(model.predict_real, dtype=tf.uint8))\r\n\r\n    with tf.name_scope(""predict_fake_summary""):\r\n        tf.summary.image(""predict_fake"", tf.image.convert_image_dtype(model.predict_fake, dtype=tf.uint8))\r\n\r\n    tf.summary.scalar(""discriminator_loss"", model.discrim_loss)\r\n    tf.summary.scalar(""generator_loss_GAN"", model.gen_loss_GAN)\r\n    tf.summary.scalar(""generator_loss_L1"", model.gen_loss_L1)\r\n\r\n    for var in tf.trainable_variables():\r\n        tf.summary.histogram(var.op.name + ""/values"", var)\r\n\r\n    for grad, var in model.discrim_grads_and_vars + model.gen_grads_and_vars:\r\n        tf.summary.histogram(var.op.name + ""/gradients"", grad)\r\n\r\n    with tf.name_scope(""parameter_count""):\r\n        parameter_count = tf.reduce_sum([tf.reduce_prod(tf.shape(v)) for v in tf.trainable_variables()])\r\n\r\n    saver = tf.train.Saver(max_to_keep=1)\r\n\r\n    logdir = a.output_dir if (a.trace_freq > 0 or a.summary_freq > 0) else None\r\n    sv = tf.train.Supervisor(logdir=logdir, save_summaries_secs=0, saver=None)\r\n    with sv.managed_session() as sess:\r\n        print(""parameter_count ="", sess.run(parameter_count))\r\n\r\n        if a.checkpoint is not None:\r\n            print(""loading model from checkpoint"")\r\n            checkpoint = tf.train.latest_checkpoint(a.checkpoint)\r\n            saver.restore(sess, checkpoint)\r\n\r\n        max_steps = 2 ** 32\r\n        if a.max_epochs is not None:\r\n            max_steps = examples.steps_per_epoch * a.max_epochs\r\n        if a.max_steps is not None:\r\n            max_steps = a.max_steps\r\n\r\n        if a.mode == ""test"":\r\n            # testing\r\n            # at most, process the test data once\r\n            start = time.time()\r\n            max_steps = min(examples.steps_per_epoch, max_steps)\r\n            for step in range(max_steps):\r\n                results = sess.run(display_fetches)\r\n                filesets = save_images(results)\r\n                for i, f in enumerate(filesets):\r\n                    print(""evaluated image"", f[""name""])\r\n                index_path = append_index(filesets)\r\n            print(""wrote index at"", index_path)\r\n            print(""rate"", (time.time() - start) / max_steps)\r\n        else:\r\n            # training\r\n            start = time.time()\r\n\r\n            for step in range(max_steps):\r\n                def should(freq):\r\n                    return freq > 0 and ((step + 1) % freq == 0 or step == max_steps - 1)\r\n\r\n                options = None\r\n                run_metadata = None\r\n                if should(a.trace_freq):\r\n                    options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n                    run_metadata = tf.RunMetadata()\r\n\r\n                fetches = {\r\n                    ""train"": model.train,\r\n                    ""global_step"": sv.global_step,\r\n                }\r\n\r\n                if should(a.progress_freq):\r\n                    fetches[""discrim_loss""] = model.discrim_loss\r\n                    fetches[""gen_loss_GAN""] = model.gen_loss_GAN\r\n                    fetches[""gen_loss_L1""] = model.gen_loss_L1\r\n\r\n                if should(a.summary_freq):\r\n                    fetches[""summary""] = sv.summary_op\r\n\r\n                if should(a.display_freq):\r\n                    fetches[""display""] = display_fetches\r\n\r\n                results = sess.run(fetches, options=options, run_metadata=run_metadata)\r\n\r\n                if should(a.summary_freq):\r\n                    print(""recording summary"")\r\n                    sv.summary_writer.add_summary(results[""summary""], results[""global_step""])\r\n\r\n                if should(a.display_freq):\r\n                    print(""saving display images"")\r\n                    filesets = save_images(results[""display""], step=results[""global_step""])\r\n                    append_index(filesets, step=True)\r\n\r\n                if should(a.trace_freq):\r\n                    print(""recording trace"")\r\n                    sv.summary_writer.add_run_metadata(run_metadata, ""step_%d"" % results[""global_step""])\r\n\r\n                if should(a.progress_freq):\r\n                    # global_step will have the correct step count if we resume from a checkpoint\r\n                    train_epoch = math.ceil(results[""global_step""] / examples.steps_per_epoch)\r\n                    train_step = (results[""global_step""] - 1) % examples.steps_per_epoch + 1\r\n                    rate = (step + 1) * a.batch_size / (time.time() - start)\r\n                    remaining = (max_steps - step) * a.batch_size / rate\r\n                    print(""progress  epoch %d  step %d  image/sec %0.1f  remaining %dm"" % (\r\n                    train_epoch, train_step, rate, remaining / 60))\r\n                    print(""discrim_loss"", results[""discrim_loss""])\r\n                    print(""gen_loss_GAN"", results[""gen_loss_GAN""])\r\n                    print(""gen_loss_L1"", results[""gen_loss_L1""])\r\n\r\n                if should(a.save_freq):\r\n                    print(""saving model"")\r\n                    saver.save(sess, os.path.join(a.output_dir, ""model""), global_step=sv.global_step)\r\n\r\n                if sv.should_stop():\r\n                    break\r\n\r\n\r\nmain()\r\n'"
models/m04_pix2pix/test.py,31,"b'# -*- coding:utf-8 -*-\r\n\r\n""""""\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xae\xad\xe7\xbb\x83\xe4\xbb\xa3\xe7\xa0\x81""""""\r\nfrom __future__ import print_function, division, absolute_import\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport argparse\r\nimport os\r\nimport json\r\nimport glob\r\nimport random\r\nimport collections\r\nimport math\r\nimport time\r\nimport sys\r\n\r\nsys.path.append(\'.\')\r\nfrom utils import *\r\nfrom model import *\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(""--input_dir"", help=""path to folder containing images"")\r\nparser.add_argument(""--mode"", type=str, default=""test"", choices=[""train"", ""test"", ""export""])\r\nparser.add_argument(""--output_dir"", required=True, help=""where to put output files"")\r\nparser.add_argument(""--seed"", type=int)\r\nparser.add_argument(""--checkpoint"", default=None,\r\n                    help=""directory with checkpoint to resume training from or use for testing"")\r\n\r\nparser.add_argument(""--max_steps"", type=int, help=""number of training steps (0 to disable)"")\r\nparser.add_argument(""--max_epochs"", type=int, help=""number of training epochs"")\r\nparser.add_argument(""--summary_freq"", type=int, default=100, help=""update summaries every summary_freq steps"")\r\nparser.add_argument(""--progress_freq"", type=int, default=50, help=""display progress every progress_freq steps"")\r\nparser.add_argument(""--trace_freq"", type=int, default=0, help=""trace execution every trace_freq steps"")\r\nparser.add_argument(""--display_freq"", type=int, default=0,\r\n                    help=""write current training images every display_freq steps"")\r\nparser.add_argument(""--save_freq"", type=int, default=5000, help=""save model every save_freq steps, 0 to disable"")\r\n\r\nparser.add_argument(""--separable_conv"", action=""store_true"", help=""use separable convolutions in the generator"")\r\nparser.add_argument(""--aspect_ratio"", type=float, default=1.0, help=""aspect ratio of output images (width/height)"")\r\nparser.add_argument(""--lab_colorization"", action=""store_true"",\r\n                    help=""split input image into brightness (A) and color (B)"")\r\nparser.add_argument(""--batch_size"", type=int, default=1, help=""number of images in batch"")\r\nparser.add_argument(""--which_direction"", type=str, default=""AtoB"", choices=[""AtoB"", ""BtoA""])\r\nparser.add_argument(""--ngf"", type=int, default=64, help=""number of generator filters in first conv layer"")\r\nparser.add_argument(""--ndf"", type=int, default=64, help=""number of discriminator filters in first conv layer"")\r\nparser.add_argument(""--scale_size"", type=int, default=286, help=""scale images to this size before cropping to 256x256"")\r\nparser.add_argument(""--flip"", dest=""flip"", action=""store_true"", help=""flip images horizontally"")\r\nparser.add_argument(""--no_flip"", dest=""flip"", action=""store_false"", help=""don\'t flip images horizontally"")\r\nparser.set_defaults(flip=True)\r\nparser.add_argument(""--lr"", type=float, default=0.0002, help=""initial learning rate for adam"")\r\nparser.add_argument(""--beta1"", type=float, default=0.5, help=""momentum term of adam"")\r\nparser.add_argument(""--l1_weight"", type=float, default=100.0, help=""weight on L1 term for generator gradient"")\r\nparser.add_argument(""--gan_weight"", type=float, default=1.0, help=""weight on GAN term for generator gradient"")\r\n\r\n# export options\r\nparser.add_argument(""--output_filetype"", default=""png"", choices=[""png"", ""jpeg""])\r\na = parser.parse_args()\r\n\r\n\r\ndef main():\r\n    if a.seed is None:\r\n        a.seed = random.randint(0, 2 ** 31 - 1)\r\n\r\n    tf.set_random_seed(a.seed)\r\n    np.random.seed(a.seed)\r\n    random.seed(a.seed)\r\n\r\n    if not os.path.exists(a.output_dir):\r\n        os.makedirs(a.output_dir)\r\n\r\n    if a.checkpoint is None:\r\n        raise Exception(""checkpoint required for test mode"")\r\n\r\n    # load some options from the checkpoint\r\n    options = {""which_direction"", ""ngf"", ""ndf"", ""lab_colorization""}\r\n    with open(os.path.join(a.checkpoint, ""options.json"")) as f:\r\n        for key, val in json.loads(f.read()).items():\r\n            if key in options:\r\n                print(""loaded"", key, ""="", val)\r\n                setattr(a, key, val)\r\n    # disable these features in test mode\r\n    a.scale_size = CROP_SIZE\r\n    a.flip = False\r\n\r\n    # \xe6\x89\x93\xe5\x8d\xb0\xe5\x8f\x82\xe6\x95\xb0\r\n    for k, v in a._get_kwargs():\r\n        print(k, ""="", v)\r\n\r\n    # \xe5\x86\x99\xe5\x85\xa5\xe5\x8f\x82\xe6\x95\xb0\r\n    with open(os.path.join(a.output_dir, ""options.json""), ""w"") as f:\r\n        f.write(json.dumps(vars(a), sort_keys=True, indent=4))\r\n\r\n    # \xe5\x87\x86\xe5\xa4\x87\xe6\x95\xb0\xe6\x8d\xae\xe8\xaf\xbb\xe5\x85\xa5\r\n    examples = load_examples(input_dir=a.input_dir, batch_size=a.batch_size, mode=a.mode,\r\n                             which_direction=a.which_direction)\r\n    print(""examples count = %d"" % examples.count)\r\n\r\n    # \xe5\x88\x9b\xe5\xbb\xba\xe6\xa8\xa1\xe5\x9e\x8b\r\n    # inputs and targets are [batch_size, height, width, channels]\r\n    model = create_model(examples.inputs, examples.targets)\r\n\r\n    # undo colorization splitting on images that we use for display/output\r\n    if a.lab_colorization:\r\n        if a.which_direction == ""AtoB"":\r\n            # inputs is brightness, this will be handled fine as a grayscale image\r\n            # need to augment targets and outputs with brightness\r\n            targets = augment(examples.targets, examples.inputs)\r\n            outputs = augment(model.outputs, examples.inputs)\r\n            # inputs can be deprocessed normally and handled as if they are single channel\r\n            # grayscale images\r\n            inputs = deprocess(examples.inputs)\r\n        elif a.which_direction == ""BtoA"":\r\n            # inputs will be color channels only, get brightness from targets\r\n            inputs = augment(examples.inputs, examples.targets)\r\n            targets = deprocess(examples.targets)\r\n            outputs = deprocess(model.outputs)\r\n        else:\r\n            raise Exception(""invalid direction"")\r\n    else:  # \xe6\x8a\x8a\xe6\x95\xb0\xe6\x8d\xae\xe6\x98\xa0\xe5\xb0\x84\xe5\x9b\x9e\xe5\x88\xb0 (0, 1) \xe4\xb9\x8b\xe9\x97\xb4\xe4\xbe\xbf\xe4\xba\x8e\xe6\x9c\x80\xe5\x90\x8e\xe4\xbf\x9d\xe5\xad\x98\xe5\x9b\xbe\xe7\x89\x87\r\n        inputs = deprocess(examples.inputs)\r\n        targets = deprocess(examples.targets)\r\n        outputs = deprocess(model.outputs)\r\n\r\n    def convert(image):\r\n        """"""\xe6\x8a\x8a\xe5\x8e\x9f\xe5\xa7\x8b\xe7\x9a\x84img\xe6\x94\xb9\xe6\x88\x90\xe6\x95\xb4\xe5\xbd\xa2\xe7\x9a\x84\xe7\xb1\xbb\xe5\x9e\x8b\xef\xbc\x8c\xe8\x87\xaa\xe5\x8a\xa8\xe7\x9a\x84\xe6\x98\xa0\xe5\xb0\x84\xe5\x88\xb0 [0,255]\xe9\x97\xb4""""""\r\n        if a.aspect_ratio != 1.0:  # \xe5\xa6\x82\xe6\x9e\x9c aspect_ratio \xe4\xb8\x8d\xe6\x98\xaf 1 \xef\xbc\x88\xe4\xb8\x8d\xe6\x98\xaf\xe6\x96\xb9\xe5\xbd\xa2\xef\xbc\x89\xe7\x9a\x84\xe8\xaf\x9d\xe9\x9c\x80\xe8\xa6\x81resize\xe6\x81\xa2\xe5\xa4\x8d\xe5\xa4\xa7\xe5\xb0\x8f\r\n            # upscale to correct aspect ratio\r\n            size = [CROP_SIZE, int(round(CROP_SIZE * a.aspect_ratio))]\r\n            image = tf.image.resize_images(image, size=size, method=tf.image.ResizeMethod.BICUBIC)\r\n\r\n        return tf.image.convert_image_dtype(image, dtype=tf.uint8, saturate=True)\r\n\r\n    # reverse any processing on images so they can be written to disk or displayed to user\r\n    with tf.name_scope(""convert_inputs""):\r\n        converted_inputs = convert(inputs)\r\n\r\n    with tf.name_scope(""convert_targets""):\r\n        converted_targets = convert(targets)\r\n\r\n    with tf.name_scope(""convert_outputs""):\r\n        converted_outputs = convert(outputs)\r\n\r\n    with tf.name_scope(""encode_images""):\r\n        display_fetches = {\r\n            ""paths"": examples.paths,\r\n            ""inputs"": tf.map_fn(tf.image.encode_png, converted_inputs, dtype=tf.string, name=""input_pngs""),\r\n            ""targets"": tf.map_fn(tf.image.encode_png, converted_targets, dtype=tf.string, name=""target_pngs""),\r\n            ""outputs"": tf.map_fn(tf.image.encode_png, converted_outputs, dtype=tf.string, name=""output_pngs""),\r\n        }\r\n\r\n    # summaries\r\n    with tf.name_scope(""inputs_summary""):\r\n        tf.summary.image(""inputs"", converted_inputs)\r\n\r\n    with tf.name_scope(""targets_summary""):\r\n        tf.summary.image(""targets"", converted_targets)\r\n\r\n    with tf.name_scope(""outputs_summary""):\r\n        tf.summary.image(""outputs"", converted_outputs)\r\n\r\n    with tf.name_scope(""predict_real_summary""):\r\n        tf.summary.image(""predict_real"", tf.image.convert_image_dtype(model.predict_real, dtype=tf.uint8))\r\n\r\n    with tf.name_scope(""predict_fake_summary""):\r\n        tf.summary.image(""predict_fake"", tf.image.convert_image_dtype(model.predict_fake, dtype=tf.uint8))\r\n\r\n    tf.summary.scalar(""discriminator_loss"", model.discrim_loss)\r\n    tf.summary.scalar(""generator_loss_GAN"", model.gen_loss_GAN)\r\n    tf.summary.scalar(""generator_loss_L1"", model.gen_loss_L1)\r\n\r\n    for var in tf.trainable_variables():\r\n        tf.summary.histogram(var.op.name + ""/values"", var)\r\n\r\n    for grad, var in model.discrim_grads_and_vars + model.gen_grads_and_vars:\r\n        tf.summary.histogram(var.op.name + ""/gradients"", grad)\r\n\r\n    with tf.name_scope(""parameter_count""):\r\n        parameter_count = tf.reduce_sum([tf.reduce_prod(tf.shape(v)) for v in tf.trainable_variables()])\r\n\r\n    saver = tf.train.Saver(max_to_keep=1)\r\n\r\n    logdir = a.output_dir if (a.trace_freq > 0 or a.summary_freq > 0) else None\r\n    sv = tf.train.Supervisor(logdir=logdir, save_summaries_secs=0, saver=None)\r\n    with sv.managed_session() as sess:\r\n        print(""parameter_count ="", sess.run(parameter_count))\r\n\r\n        if a.checkpoint is not None:\r\n            print(""loading model from checkpoint"")\r\n            checkpoint = tf.train.latest_checkpoint(a.checkpoint)\r\n            saver.restore(sess, checkpoint)\r\n\r\n        max_steps = 2 ** 32\r\n        if a.max_epochs is not None:\r\n            max_steps = examples.steps_per_epoch * a.max_epochs\r\n        if a.max_steps is not None:\r\n            max_steps = a.max_steps\r\n\r\n        # testing\r\n        # at most, process the test data once\r\n        start = time.time()\r\n        max_steps = min(examples.steps_per_epoch, max_steps)\r\n        for step in range(max_steps):\r\n            results = sess.run(display_fetches)\r\n            filesets = save_images(results, output_dir=a.output_dir)\r\n            for i, f in enumerate(filesets):\r\n                print(""evaluated image"", f[""name""])\r\n            index_path = append_index(filesets, output_dir=a.output_dir)\r\n        print(""wrote index at"", index_path)\r\n        print(""rate"", (time.time() - start) / max_steps)\r\n\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()'"
models/m04_pix2pix/train.py,33,"b'# -*- coding:utf-8 -*-\r\n\r\n""""""\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xae\xad\xe7\xbb\x83\xe4\xbb\xa3\xe7\xa0\x81""""""\r\nfrom __future__ import print_function, division, absolute_import\r\n\r\nimport tensorflow as tf\r\nimport os\r\n\r\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = \'0\'\r\n\r\nimport numpy as np\r\nimport argparse\r\nimport json\r\nimport glob\r\nimport random\r\nimport collections\r\nimport math\r\nimport time\r\nimport sys\r\n\r\nsys.path.append(\'.\')\r\nfrom utils import *\r\nfrom model import *\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(""--input_dir"", help=""path to folder containing images"")\r\nparser.add_argument(""--mode"", required=True, choices=[""train"", ""test"", ""export""])\r\nparser.add_argument(""--output_dir"", required=True, help=""where to put output files"")\r\nparser.add_argument(""--seed"", type=int)\r\nparser.add_argument(""--checkpoint"", default=None,\r\n                    help=""directory with checkpoint to resume training from or use for testing"")\r\n\r\nparser.add_argument(""--max_steps"", type=int, help=""number of training steps (0 to disable)"")\r\nparser.add_argument(""--max_epochs"", type=int, help=""number of training epochs"")\r\nparser.add_argument(""--summary_freq"", type=int, default=100, help=""update summaries every summary_freq steps"")\r\nparser.add_argument(""--progress_freq"", type=int, default=50, help=""display progress every progress_freq steps"")\r\nparser.add_argument(""--trace_freq"", type=int, default=0, help=""trace execution every trace_freq steps"")\r\nparser.add_argument(""--display_freq"", type=int, default=0,\r\n                    help=""write current training images every display_freq steps"")\r\nparser.add_argument(""--save_freq"", type=int, default=5000, help=""save model every save_freq steps, 0 to disable"")\r\n\r\nparser.add_argument(""--separable_conv"", action=""store_true"", help=""use separable convolutions in the generator"")\r\nparser.add_argument(""--aspect_ratio"", type=float, default=1.0, help=""aspect ratio of output images (width/height)"")\r\nparser.add_argument(""--lab_colorization"", action=""store_true"",\r\n                    help=""split input image into brightness (A) and color (B)"")\r\nparser.add_argument(""--batch_size"", type=int, default=1, help=""number of images in batch"")\r\nparser.add_argument(""--which_direction"", type=str, default=""AtoB"", choices=[""AtoB"", ""BtoA""])\r\nparser.add_argument(""--ngf"", type=int, default=64, help=""number of generator filters in first conv layer"")\r\nparser.add_argument(""--ndf"", type=int, default=64, help=""number of discriminator filters in first conv layer"")\r\nparser.add_argument(""--scale_size"", type=int, default=286, help=""scale images to this size before cropping to 256x256"")\r\nparser.add_argument(""--flip"", dest=""flip"", action=""store_true"", help=""flip images horizontally"")\r\nparser.add_argument(""--no_flip"", dest=""flip"", action=""store_false"", help=""don\'t flip images horizontally"")\r\nparser.set_defaults(flip=True)\r\nparser.add_argument(""--lr"", type=float, default=0.0002, help=""initial learning rate for adam"")\r\nparser.add_argument(""--beta1"", type=float, default=0.5, help=""momentum term of adam"")\r\nparser.add_argument(""--l1_weight"", type=float, default=100.0, help=""weight on L1 term for generator gradient"")\r\nparser.add_argument(""--gan_weight"", type=float, default=1.0, help=""weight on GAN term for generator gradient"")\r\n\r\n# export options\r\nparser.add_argument(""--output_filetype"", default=""png"", choices=[""png"", ""jpeg""])\r\na = parser.parse_args()\r\n\r\n\r\ndef main():\r\n    if a.seed is None:\r\n        a.seed = random.randint(0, 2 ** 31 - 1)\r\n\r\n    tf.set_random_seed(a.seed)\r\n    np.random.seed(a.seed)\r\n    random.seed(a.seed)\r\n\r\n    if not os.path.exists(a.output_dir):\r\n        os.makedirs(a.output_dir)\r\n\r\n    # \xe6\x89\x93\xe5\x8d\xb0\xe5\x8f\x82\xe6\x95\xb0\r\n    for k, v in a._get_kwargs():\r\n        print(k, ""="", v)\r\n\r\n    # \xe5\x86\x99\xe5\x85\xa5\xe5\x8f\x82\xe6\x95\xb0\r\n    with open(os.path.join(a.output_dir, ""options.json""), ""w"") as f:\r\n        f.write(json.dumps(vars(a), sort_keys=True, indent=4))\r\n\r\n    # \xe5\x87\x86\xe5\xa4\x87\xe6\x95\xb0\xe6\x8d\xae\xe8\xaf\xbb\xe5\x85\xa5\r\n    examples = load_examples(input_dir=a.input_dir, batch_size=a.batch_size, mode=a.mode,\r\n                             which_direction=a.which_direction)\r\n    print(""examples count = %d"" % examples.count)\r\n\r\n    # \xe5\x88\x9b\xe5\xbb\xba\xe6\xa8\xa1\xe5\x9e\x8b\r\n    # inputs and targets are [batch_size, height, width, channels]\r\n    model = create_model(examples.inputs, examples.targets)\r\n\r\n    # undo colorization splitting on images that we use for display/output\r\n    if a.lab_colorization:\r\n        if a.which_direction == ""AtoB"":\r\n            # inputs is brightness, this will be handled fine as a grayscale image\r\n            # need to augment targets and outputs with brightness\r\n            targets = augment(examples.targets, examples.inputs)\r\n            outputs = augment(model.outputs, examples.inputs)\r\n            # inputs can be deprocessed normally and handled as if they are single channel\r\n            # grayscale images\r\n            inputs = deprocess(examples.inputs)\r\n        elif a.which_direction == ""BtoA"":\r\n            # inputs will be color channels only, get brightness from targets\r\n            inputs = augment(examples.inputs, examples.targets)\r\n            targets = deprocess(examples.targets)\r\n            outputs = deprocess(model.outputs)\r\n        else:\r\n            raise Exception(""invalid direction"")\r\n    else:  # \xe6\x8a\x8a\xe6\x95\xb0\xe6\x8d\xae\xe6\x98\xa0\xe5\xb0\x84\xe5\x9b\x9e\xe5\x88\xb0 (0, 1) \xe4\xb9\x8b\xe9\x97\xb4\xe4\xbe\xbf\xe4\xba\x8e\xe6\x9c\x80\xe5\x90\x8e\xe4\xbf\x9d\xe5\xad\x98\xe5\x9b\xbe\xe7\x89\x87\r\n        inputs = deprocess(examples.inputs)\r\n        targets = deprocess(examples.targets)\r\n        outputs = deprocess(model.outputs)\r\n\r\n    def convert(image):\r\n        """"""\xe6\x8a\x8a\xe5\x8e\x9f\xe5\xa7\x8b\xe7\x9a\x84img\xe6\x94\xb9\xe6\x88\x90\xe6\x95\xb4\xe5\xbd\xa2\xe7\x9a\x84\xe7\xb1\xbb\xe5\x9e\x8b\xef\xbc\x8c\xe8\x87\xaa\xe5\x8a\xa8\xe7\x9a\x84\xe6\x98\xa0\xe5\xb0\x84\xe5\x88\xb0 [0,255]\xe9\x97\xb4""""""\r\n        if a.aspect_ratio != 1.0:  # \xe5\xa6\x82\xe6\x9e\x9c aspect_ratio \xe4\xb8\x8d\xe6\x98\xaf 1 \xef\xbc\x88\xe4\xb8\x8d\xe6\x98\xaf\xe6\x96\xb9\xe5\xbd\xa2\xef\xbc\x89\xe7\x9a\x84\xe8\xaf\x9d\xe9\x9c\x80\xe8\xa6\x81resize\xe6\x81\xa2\xe5\xa4\x8d\xe5\xa4\xa7\xe5\xb0\x8f\r\n            # upscale to correct aspect ratio\r\n            size = [CROP_SIZE, int(round(CROP_SIZE * a.aspect_ratio))]\r\n            image = tf.image.resize_images(image, size=size, method=tf.image.ResizeMethod.BICUBIC)\r\n\r\n        return tf.image.convert_image_dtype(image, dtype=tf.uint8, saturate=True)\r\n\r\n    # reverse any processing on images so they can be written to disk or displayed to user\r\n    with tf.name_scope(""convert_inputs""):\r\n        converted_inputs = convert(inputs)\r\n\r\n    with tf.name_scope(""convert_targets""):\r\n        converted_targets = convert(targets)\r\n\r\n    with tf.name_scope(""convert_outputs""):\r\n        converted_outputs = convert(outputs)\r\n\r\n    with tf.name_scope(""encode_images""):\r\n        display_fetches = {\r\n            ""paths"": examples.paths,\r\n            ""inputs"": tf.map_fn(tf.image.encode_png, converted_inputs, dtype=tf.string, name=""input_pngs""),\r\n            ""targets"": tf.map_fn(tf.image.encode_png, converted_targets, dtype=tf.string, name=""target_pngs""),\r\n            ""outputs"": tf.map_fn(tf.image.encode_png, converted_outputs, dtype=tf.string, name=""output_pngs""),\r\n        }\r\n\r\n    # summaries\r\n    with tf.name_scope(""inputs_summary""):\r\n        tf.summary.image(""inputs"", converted_inputs)\r\n\r\n    with tf.name_scope(""targets_summary""):\r\n        tf.summary.image(""targets"", converted_targets)\r\n\r\n    with tf.name_scope(""outputs_summary""):\r\n        tf.summary.image(""outputs"", converted_outputs)\r\n\r\n    with tf.name_scope(""predict_real_summary""):\r\n        tf.summary.image(""predict_real"", tf.image.convert_image_dtype(model.predict_real, dtype=tf.uint8))\r\n\r\n    with tf.name_scope(""predict_fake_summary""):\r\n        tf.summary.image(""predict_fake"", tf.image.convert_image_dtype(model.predict_fake, dtype=tf.uint8))\r\n\r\n    tf.summary.scalar(""discriminator_loss"", model.discrim_loss)\r\n    tf.summary.scalar(""generator_loss_GAN"", model.gen_loss_GAN)\r\n    tf.summary.scalar(""generator_loss_L1"", model.gen_loss_L1)\r\n\r\n    for var in tf.trainable_variables():\r\n        tf.summary.histogram(var.op.name + ""/values"", var)\r\n\r\n    for grad, var in model.discrim_grads_and_vars + model.gen_grads_and_vars:\r\n        tf.summary.histogram(var.op.name + ""/gradients"", grad)\r\n\r\n    with tf.name_scope(""parameter_count""):\r\n        parameter_count = tf.reduce_sum([tf.reduce_prod(tf.shape(v)) for v in tf.trainable_variables()])\r\n\r\n    saver = tf.train.Saver(max_to_keep=1)\r\n\r\n    logdir = a.output_dir if (a.trace_freq > 0 or a.summary_freq > 0) else None\r\n    sv = tf.train.Supervisor(logdir=logdir, save_summaries_secs=0, saver=None)\r\n    with sv.managed_session() as sess:\r\n        print(""parameter_count ="", sess.run(parameter_count))\r\n\r\n        if a.checkpoint is not None:\r\n            print(""loading model from checkpoint"")\r\n            checkpoint = tf.train.latest_checkpoint(a.checkpoint)\r\n            saver.restore(sess, checkpoint)\r\n\r\n        max_steps = 2 ** 32\r\n        if a.max_epochs is not None:\r\n            max_steps = examples.steps_per_epoch * a.max_epochs\r\n        if a.max_steps is not None:\r\n            max_steps = a.max_steps\r\n\r\n\r\n        # training\r\n        start = time.time()\r\n\r\n        for step in range(max_steps):\r\n            def should(freq):\r\n                return freq > 0 and ((step + 1) % freq == 0 or step == max_steps - 1)\r\n\r\n            options = None\r\n            run_metadata = None\r\n            if should(a.trace_freq):\r\n                options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n                run_metadata = tf.RunMetadata()\r\n\r\n            fetches = {\r\n                ""train"": model.train,\r\n                ""global_step"": sv.global_step,\r\n            }\r\n\r\n            if should(a.progress_freq):\r\n                fetches[""discrim_loss""] = model.discrim_loss\r\n                fetches[""gen_loss_GAN""] = model.gen_loss_GAN\r\n                fetches[""gen_loss_L1""] = model.gen_loss_L1\r\n\r\n            if should(a.summary_freq):\r\n                fetches[""summary""] = sv.summary_op\r\n\r\n            if should(a.display_freq):\r\n                fetches[""display""] = display_fetches\r\n\r\n            results = sess.run(fetches, options=options, run_metadata=run_metadata)\r\n\r\n            if should(a.summary_freq):\r\n                print(""recording summary"")\r\n                sv.summary_writer.add_summary(results[""summary""], results[""global_step""])\r\n\r\n            if should(a.display_freq):\r\n                print(""saving display images"")\r\n                filesets = save_images(results[""display""], output_dir=a.output_dir, step=results[""global_step""])\r\n                append_index(filesets, output_dir=a.output_dir, step=True)\r\n\r\n            if should(a.trace_freq):\r\n                print(""recording trace"")\r\n                sv.summary_writer.add_run_metadata(run_metadata, ""step_%d"" % results[""global_step""])\r\n\r\n            if should(a.progress_freq):\r\n                # global_step will have the correct step count if we resume from a checkpoint\r\n                train_epoch = math.ceil(results[""global_step""] / examples.steps_per_epoch)\r\n                train_step = (results[""global_step""] - 1) % examples.steps_per_epoch + 1\r\n                rate = (step + 1) * a.batch_size / (time.time() - start)  # \xe6\xaf\x8f\xe7\xa7\x92\xe9\x92\x9f\xe5\xa4\x84\xe7\x90\x86\xe5\x9b\xbe\xe7\x89\x87\xe5\xbc\xa0\xe6\x95\xb0\r\n                remaining = (max_steps - step) * a.batch_size / rate  # \xe9\xa2\x84\xe4\xbc\xb0\xe8\xbf\x98\xe9\x9c\x80\xe7\x9a\x84\xe6\x97\xb6\xe9\x97\xb4\r\n                print(""progress  epoch %d  step %d  image/sec %0.1f  remaining %dm"" % (\r\n                    train_epoch, train_step, rate, remaining / 60))\r\n                print(""discrim_loss"", results[""discrim_loss""])\r\n                print(""gen_loss_GAN"", results[""gen_loss_GAN""])\r\n                print(""gen_loss_L1"", results[""gen_loss_L1""])\r\n\r\n            if should(a.save_freq):\r\n                print(""saving model"")\r\n                saver.save(sess, os.path.join(a.output_dir, ""model""), global_step=sv.global_step)\r\n\r\n            if sv.should_stop():\r\n                break\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()'"
models/m04_pix2pix/utils.py,59,"b'# -*- coding:utf-8 -*-\r\n\r\n""""""\xe6\x95\xb0\xe6\x8d\xae\xe8\xaf\xbb\xe5\x8f\x96\xe5\x92\x8c\xe6\x95\xb0\xe6\x8d\xae\xe5\xa4\x84\xe7\x90\x86\xe5\x87\xbd\xe6\x95\xb0\xe3\x80\x82""""""\r\n\r\nfrom __future__ import print_function, division, absolute_import\r\n\r\nimport tensorflow as tf\r\nimport os\r\nimport math\r\nimport glob\r\nimport random\r\nimport collections\r\n\r\nCROP_SIZE = 256\r\n\r\n""""""collections.namedtuple \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x90\x86\xe8\xa7\xa3\xe4\xb8\xba c \xe4\xb8\xad\xe7\x9a\x84 struct\xef\xbc\x8c\xe9\xa6\x96\xe5\x85\x88\xe7\xbb\x99\xe6\xaf\x8f\xe4\xb8\xaa item \xe5\x91\xbd\xe5\x90\x8d\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe7\xbb\x99\xe6\xaf\x8f\xe4\xb8\xaaitem \xe8\xbf\x9b\xe8\xa1\x8c\xe8\xb5\x8b\xe5\x80\xbc""""""\r\nExamples = collections.namedtuple(""Examples"", ""paths, inputs, targets, count, steps_per_epoch"")\r\n\r\n\r\ndef preprocess(image):\r\n    with tf.name_scope(""preprocess""):\r\n        # [0, 1] => [-1, 1]\r\n        return image * 2 - 1\r\n\r\n\r\ndef deprocess(image):\r\n    with tf.name_scope(""deprocess""):\r\n        # [-1, 1] => [0, 1]\r\n        return (image + 1) / 2\r\n\r\n\r\ndef preprocess_lab(lab):\r\n    with tf.name_scope(""preprocess_lab""):\r\n        L_chan, a_chan, b_chan = tf.unstack(lab, axis=2)\r\n        # L_chan: black and white with input range [0, 100]\r\n        # a_chan/b_chan: color channels with input range ~[-110, 110], not exact\r\n        # [0, 100] => [-1, 1],  ~[-110, 110] => [-1, 1]\r\n        return [L_chan / 50 - 1, a_chan / 110, b_chan / 110]\r\n\r\n\r\ndef deprocess_lab(L_chan, a_chan, b_chan):\r\n    with tf.name_scope(""deprocess_lab""):\r\n        # this is axis=3 instead of axis=2 because we process individual images but deprocess batches\r\n        return tf.stack([(L_chan + 1) / 2 * 100, a_chan * 110, b_chan * 110], axis=3)\r\n\r\n\r\ndef augment(image, brightness):\r\n    # (a, b) color channels, combine with L channel and convert to rgb\r\n    a_chan, b_chan = tf.unstack(image, axis=3)\r\n    L_chan = tf.squeeze(brightness, axis=3)\r\n    lab = deprocess_lab(L_chan, a_chan, b_chan)\r\n    rgb = lab_to_rgb(lab)\r\n    return rgb\r\n\r\n\r\ndef check_image(image):\r\n    assertion = tf.assert_equal(tf.shape(image)[-1], 3, message=""image must have 3 color channels"")\r\n    with tf.control_dependencies([assertion]):\r\n        image = tf.identity(image)\r\n\r\n    if image.get_shape().ndims not in (3, 4):\r\n        raise ValueError(""image must be either 3 or 4 dimensions"")\r\n\r\n    # make the last dimension 3 so that you can unstack the colors\r\n    shape = list(image.get_shape())\r\n    shape[-1] = 3\r\n    image.set_shape(shape)\r\n    return image\r\n\r\n\r\n# based on https://github.com/torch/image/blob/9f65c30167b2048ecbe8b7befdc6b2d6d12baee9/generic/image.c\r\ndef rgb_to_lab(srgb):\r\n    with tf.name_scope(""rgb_to_lab""):\r\n        srgb = check_image(srgb)\r\n        srgb_pixels = tf.reshape(srgb, [-1, 3])\r\n\r\n        with tf.name_scope(""srgb_to_xyz""):\r\n            linear_mask = tf.cast(srgb_pixels <= 0.04045, dtype=tf.float32)\r\n            exponential_mask = tf.cast(srgb_pixels > 0.04045, dtype=tf.float32)\r\n            rgb_pixels = (srgb_pixels / 12.92 * linear_mask) + (((\r\n                                                                     srgb_pixels + 0.055) / 1.055) ** 2.4) * exponential_mask\r\n            rgb_to_xyz = tf.constant([\r\n                #    X        Y          Z\r\n                [0.412453, 0.212671, 0.019334],  # R\r\n                [0.357580, 0.715160, 0.119193],  # G\r\n                [0.180423, 0.072169, 0.950227],  # B\r\n            ])\r\n            xyz_pixels = tf.matmul(rgb_pixels, rgb_to_xyz)\r\n\r\n        # https://en.wikipediorg/wiki/Lab_color_space#CIELAB-CIEXYZ_conversions\r\n        with tf.name_scope(""xyz_to_cielab""):\r\n            # convert to fx = f(X/Xn), fy = f(Y/Yn), fz = f(Z/Zn)\r\n\r\n            # normalize for D65 white point\r\n            xyz_normalized_pixels = tf.multiply(xyz_pixels, [1 / 0.950456, 1.0, 1 / 1.088754])\r\n\r\n            epsilon = 6 / 29\r\n            linear_mask = tf.cast(xyz_normalized_pixels <= (epsilon ** 3), dtype=tf.float32)\r\n            exponential_mask = tf.cast(xyz_normalized_pixels > (epsilon ** 3), dtype=tf.float32)\r\n            fxfyfz_pixels = (xyz_normalized_pixels / (3 * epsilon ** 2) + 4 / 29) * linear_mask + (\r\n                                                                                                      xyz_normalized_pixels ** (\r\n                                                                                                          1 / 3)) * exponential_mask\r\n\r\n            # convert to lab\r\n            fxfyfz_to_lab = tf.constant([\r\n                #  l       a       b\r\n                [0.0, 500.0, 0.0],  # fx\r\n                [116.0, -500.0, 200.0],  # fy\r\n                [0.0, 0.0, -200.0],  # fz\r\n            ])\r\n            lab_pixels = tf.matmul(fxfyfz_pixels, fxfyfz_to_lab) + tf.constant([-16.0, 0.0, 0.0])\r\n\r\n        return tf.reshape(lab_pixels, tf.shape(srgb))\r\n\r\n\r\ndef lab_to_rgb(lab):\r\n    with tf.name_scope(""lab_to_rgb""):\r\n        lab = check_image(lab)\r\n        lab_pixels = tf.reshape(lab, [-1, 3])\r\n\r\n        # https://en.wikipediorg/wiki/Lab_color_space#CIELAB-CIEXYZ_conversions\r\n        with tf.name_scope(""cielab_to_xyz""):\r\n            # convert to fxfyfz\r\n            lab_to_fxfyfz = tf.constant([\r\n                #   fx      fy        fz\r\n                [1 / 116.0, 1 / 116.0, 1 / 116.0],  # l\r\n                [1 / 500.0, 0.0, 0.0],  # a\r\n                [0.0, 0.0, -1 / 200.0],  # b\r\n            ])\r\n            fxfyfz_pixels = tf.matmul(lab_pixels + tf.constant([16.0, 0.0, 0.0]), lab_to_fxfyfz)\r\n\r\n            # convert to xyz\r\n            epsilon = 6 / 29\r\n            linear_mask = tf.cast(fxfyfz_pixels <= epsilon, dtype=tf.float32)\r\n            exponential_mask = tf.cast(fxfyfz_pixels > epsilon, dtype=tf.float32)\r\n            xyz_pixels = (3 * epsilon ** 2 * (fxfyfz_pixels - 4 / 29)) * linear_mask + (\r\n                                                                                           fxfyfz_pixels ** 3) * exponential_mask\r\n\r\n            # denormalize for D65 white point\r\n            xyz_pixels = tf.multiply(xyz_pixels, [0.950456, 1.0, 1.088754])\r\n\r\n        with tf.name_scope(""xyz_to_srgb""):\r\n            xyz_to_rgb = tf.constant([\r\n                #     r           g          b\r\n                [3.2404542, -0.9692660, 0.0556434],  # x\r\n                [-1.5371385, 1.8760108, -0.2040259],  # y\r\n                [-0.4985314, 0.0415560, 1.0572252],  # z\r\n            ])\r\n            rgb_pixels = tf.matmul(xyz_pixels, xyz_to_rgb)\r\n            # avoid a slightly negative number messing up the conversion\r\n            rgb_pixels = tf.clip_by_value(rgb_pixels, 0.0, 1.0)\r\n            linear_mask = tf.cast(rgb_pixels <= 0.0031308, dtype=tf.float32)\r\n            exponential_mask = tf.cast(rgb_pixels > 0.0031308, dtype=tf.float32)\r\n            srgb_pixels = (rgb_pixels * 12.92 * linear_mask) + ((rgb_pixels ** (\r\n                1 / 2.4) * 1.055) - 0.055) * exponential_mask\r\n\r\n        return tf.reshape(srgb_pixels, tf.shape(lab))\r\n\r\n\r\ndef load_examples(input_dir, batch_size, mode, which_direction, scale_size=286, lab_colorization=False, flip=True):\r\n    """"""\r\n    Args:\r\n        input_dir: \xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84.\r\n        batch_size: batch \xe5\xa4\xa7\xe5\xb0\x8f\r\n        mode: \'train\' or \'test\'\r\n        which_direction: \'AtoB\' or \'BtoA\'\r\n        scale_size: \xe5\x8e\x9f\xe6\x96\x87\xe5\x85\x88 scale \xe5\x88\xb0 286 \xe5\x86\x8d\xe8\xa3\x81\xe5\x89\xaa\xe5\x88\xb0 256\r\n        lab_colorization: \xe6\x98\xaf\xe5\x90\xa6\xe4\xbd\xbf\xe7\x94\xa8 lab \xe9\xa2\x9c\xe8\x89\xb2\xe7\xa9\xba\xe9\x97\xb4\xef\xbc\x8cif False\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8 rgb\r\n        flip: \xe6\x98\xaf\xe5\x90\xa6\xe7\xbf\xbb\xe8\xbd\xac\r\n    """"""\r\n    if input_dir is None or not os.path.exists(input_dir):\r\n        raise Exception(""input_dir does not exist"")\r\n\r\n    input_paths = glob.glob(os.path.join(input_dir, ""*.jpg""))\r\n    decode = tf.image.decode_jpeg\r\n    if len(input_paths) == 0:\r\n        input_paths = glob.glob(os.path.join(input_dir, ""*.png""))\r\n        decode = tf.image.decode_png\r\n\r\n    if len(input_paths) == 0:\r\n        raise Exception(""input_dir contains no image files"")\r\n\r\n    def get_name(path):\r\n        name, _ = os.path.splitext(os.path.basename(path))\r\n        return name\r\n\r\n    # if the image names are numbers, sort by the value rather than asciibetically\r\n    # having sorted inputs means that the outputs are sorted in test mode\r\n    if all(get_name(path).isdigit() for path in input_paths):\r\n        input_paths = sorted(input_paths, key=lambda path: int(get_name(path)))\r\n    else:\r\n        input_paths = sorted(input_paths)\r\n\r\n    with tf.name_scope(""load_images""):\r\n        path_queue = tf.train.string_input_producer(input_paths, shuffle=mode == ""train"")\r\n        reader = tf.WholeFileReader()\r\n        paths, contents = reader.read(path_queue)\r\n        raw_input = decode(contents)\r\n        raw_input = tf.image.convert_image_dtype(raw_input, dtype=tf.float32)\r\n\r\n        assertion = tf.assert_equal(tf.shape(raw_input)[2], 3, message=""image does not have 3 channels"")\r\n        with tf.control_dependencies([assertion]):\r\n            raw_input = tf.identity(raw_input)\r\n\r\n        raw_input.set_shape([None, None, 3])\r\n\r\n        if lab_colorization:\r\n            # load color and brightness from image, no B image exists here\r\n            lab = rgb_to_lab(raw_input)\r\n            L_chan, a_chan, b_chan = preprocess_lab(lab)\r\n            a_images = tf.expand_dims(L_chan, axis=2)\r\n            b_images = tf.stack([a_chan, b_chan], axis=2)\r\n        else:\r\n            # break apart image pair and move to range [-1, 1]\r\n            width = tf.shape(raw_input)[1]  # [height, width, channels]\r\n            a_images = preprocess(raw_input[:, :width // 2, :])\r\n            b_images = preprocess(raw_input[:, width // 2:, :])\r\n\r\n    if which_direction == ""AtoB"":\r\n        inputs, targets = [a_images, b_images]\r\n    elif which_direction == ""BtoA"":\r\n        inputs, targets = [b_images, a_images]\r\n    else:\r\n        raise Exception(""invalid direction"")\r\n\r\n    # synchronize seed for image operations so that we do the same operations to both\r\n    # input and output images\r\n    seed = random.randint(0, 2 ** 31 - 1)\r\n\r\n    def transform(image):\r\n        """"""""""""\r\n        r = image\r\n        if flip:\r\n            r = tf.image.random_flip_left_right(r, seed=seed)\r\n\r\n        # area produces a nice downscaling, but does nearest neighbor for upscaling\r\n        # assume we\'re going to be doing downscaling here\r\n        r = tf.image.resize_images(r, [scale_size, scale_size], method=tf.image.ResizeMethod.AREA)\r\n\r\n        offset = tf.cast(tf.floor(tf.random_uniform([2], 0, scale_size - CROP_SIZE + 1, seed=seed)), dtype=tf.int32)\r\n        if scale_size > CROP_SIZE:\r\n            r = tf.image.crop_to_bounding_box(r, offset[0], offset[1], CROP_SIZE, CROP_SIZE)\r\n        elif scale_size < CROP_SIZE:\r\n            raise Exception(""scale size cannot be less than crop size"")\r\n        return r\r\n\r\n    with tf.name_scope(""input_images""):\r\n        input_images = transform(inputs)\r\n\r\n    with tf.name_scope(""target_images""):\r\n        target_images = transform(targets)\r\n\r\n    paths_batch, inputs_batch, targets_batch = tf.train.batch([paths, input_images, target_images],\r\n                                                              batch_size=batch_size)\r\n    steps_per_epoch = int(math.ceil(len(input_paths) / batch_size))\r\n\r\n    return Examples(\r\n        paths=paths_batch,\r\n        inputs=inputs_batch,\r\n        targets=targets_batch,\r\n        count=len(input_paths),\r\n        steps_per_epoch=steps_per_epoch,\r\n    )\r\n\r\n\r\ndef save_images(fetches, output_dir, step=None):\r\n    image_dir = os.path.join(output_dir, ""images"")\r\n    if not os.path.exists(image_dir):\r\n        os.makedirs(image_dir)\r\n\r\n    filesets = []\r\n    for i, in_path in enumerate(fetches[""paths""]):  # batch \xe4\xb8\xad\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe5\x9b\xbe\xe7\x89\x87\r\n        name, _ = os.path.splitext(os.path.basename(in_path.decode(""utf8"")))\r\n        fileset = {""name"": name, ""step"": step}\r\n        for kind in [""inputs"", ""outputs"", ""targets""]:  # \xe4\xb8\x89\xe7\xa7\x8d\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x9a\xe8\xbe\x93\xe5\x85\xa5\xe6\xa8\xa1\xe6\x80\x811\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe7\x94\x9f\xe6\x88\x90\xe6\xa8\xa1\xe6\x80\x812\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe7\x9c\x9f\xe5\xae\x9e\xe6\xa8\xa1\xe6\x80\x812\xe5\x9b\xbe\xe7\x89\x87\r\n            filename = name + ""-"" + kind + "".png""\r\n            if step is not None:\r\n                filename = ""%08d-%s"" % (step, filename)\r\n            fileset[kind] = filename\r\n            out_path = os.path.join(image_dir, filename)\r\n            contents = fetches[kind][i]\r\n            with open(out_path, ""wb"") as f:\r\n                f.write(contents)\r\n        filesets.append(fileset)\r\n    return filesets\r\n\r\n\r\ndef append_index(filesets, output_dir, step=False):\r\n    index_path = os.path.join(output_dir, ""index.html"")\r\n    if os.path.exists(index_path):\r\n        index = open(index_path, ""a"")\r\n    else:\r\n        index = open(index_path, ""w"")\r\n        index.write(""<html><body><table><tr>"")\r\n        if step:\r\n            index.write(""<th>step</th>"")\r\n        index.write(""<th>name</th><th>input</th><th>output</th><th>target</th></tr>"")\r\n\r\n    for fileset in filesets:\r\n        index.write(""<tr>"")\r\n\r\n        if step:\r\n            index.write(""<td>%d</td>"" % fileset[""step""])\r\n        index.write(""<td>%s</td>"" % fileset[""name""])\r\n\r\n        for kind in [""inputs"", ""outputs"", ""targets""]:\r\n            index.write(""<td><img src=\'images/%s\'></td>"" % fileset[kind])\r\n\r\n        index.write(""</tr>"")\r\n    return index_path\r\n'"
utils/u01_logging/logging1.py,0,"b'# -*- coding:utf-8 -*- \r\n\r\nimport logging\r\nimport logging.handlers\r\n\r\n""""""the use of logging.\r\nrefer: http://blog.csdn.net/chosen0ne/article/details/7319306\r\n\r\n- handler\xef\xbc\x9a\xe5\xb0\x86\xe6\x97\xa5\xe5\xbf\x97\xe8\xae\xb0\xe5\xbd\x95\xef\xbc\x88log record\xef\xbc\x89\xe5\x8f\x91\xe9\x80\x81\xe5\x88\xb0\xe5\x90\x88\xe9\x80\x82\xe7\x9a\x84\xe7\x9b\xae\xe7\x9a\x84\xe5\x9c\xb0\xef\xbc\x88destination\xef\xbc\x89\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\xe6\x96\x87\xe4\xbb\xb6\xef\xbc\x8csocket\xe7\xad\x89\xe3\x80\x82\r\n    \xe4\xb8\x80\xe4\xb8\xaalogger\xe5\xaf\xb9\xe8\xb1\xa1\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x80\x9a\xe8\xbf\x87addHandler\xe6\x96\xb9\xe6\xb3\x95\xe6\xb7\xbb\xe5\x8a\xa00\xe5\x88\xb0\xe5\xa4\x9a\xe4\xb8\xaahandler\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaahandler\xe5\x8f\x88\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x8d\xe5\x90\x8c\xe6\x97\xa5\xe5\xbf\x97\xe7\xba\xa7\xe5\x88\xab\xef\xbc\x8c\xe4\xbb\xa5\xe5\xae\x9e\xe7\x8e\xb0\xe6\x97\xa5\xe5\xbf\x97\xe5\x88\x86\xe7\xba\xa7\xe8\xbf\x87\xe6\xbb\xa4\xe6\x98\xbe\xe7\xa4\xba\xe3\x80\x82\r\n- formatter\xef\xbc\x9a\xe6\x8c\x87\xe5\xae\x9a\xe6\x97\xa5\xe5\xbf\x97\xe8\xae\xb0\xe5\xbd\x95\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\x85\xb7\xe4\xbd\x93\xe6\xa0\xbc\xe5\xbc\x8f\xe3\x80\x82formatter\xe7\x9a\x84\xe6\x9e\x84\xe9\x80\xa0\xe6\x96\xb9\xe6\xb3\x95\xe9\x9c\x80\xe8\xa6\x81\xe4\xb8\xa4\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x9a\xe6\xb6\x88\xe6\x81\xaf\xe7\x9a\x84\xe6\xa0\xbc\xe5\xbc\x8f\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xe5\x92\x8c\xe6\x97\xa5\xe6\x9c\x9f\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xa4\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe9\x83\xbd\xe6\x98\xaf\xe5\x8f\xaf\xe9\x80\x89\xe7\x9a\x84\xe3\x80\x82\r\n""""""\r\n\r\n# 1.\xe4\xbf\x9d\xe5\xad\x98\xe8\x87\xb3\xe6\x97\xa5\xe5\xbf\x97\xe6\x96\x87\xe4\xbb\xb6\r\nLOG_FILE = \'training.log\'\r\n# 2.\xe8\xae\xbe\xe7\xbd\xae\xe6\x97\xa5\xe5\xbf\x97\xef\xbc\x8cmaxBytes\xe8\xae\xbe\xe7\xbd\xae\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xa7\xe5\xb0\x8f\xef\xbc\x8cbackpuCount\xe8\xae\xbe\xe7\xbd\xae\xe6\x96\x87\xe4\xbb\xb6\xe6\x95\xb0\xe9\x87\x8f\r\nhandler = logging.handlers.RotatingFileHandler(LOG_FILE, maxBytes=1024 * 1024, backupCount=5)  # \xe5\xae\x9e\xe4\xbe\x8b\xe5\x8c\x96handler\r\n# 3.\xe8\xae\xbe\xe7\xbd\xae\xe6\xb6\x88\xe6\x81\xaf\xe8\xbe\x93\xe5\x87\xba\xe6\xa0\xbc\xe5\xbc\x8f\r\nfmt = \'%(asctime)s - %(filename)s:%(lineno)s - %(name)s - %(message)s\'\r\n\r\nformatter = logging.Formatter(fmt)  # \xe5\xae\x9e\xe4\xbe\x8b\xe5\x8c\x96formatter\r\nhandler.setFormatter(formatter)  # \xe4\xb8\xbahandler\xe6\xb7\xbb\xe5\x8a\xa0formatter\r\n\r\nlogger = logging.getLogger(\'train\')  # \xe8\x8e\xb7\xe5\x8f\x96\xe5\x90\x8d\xe4\xb8\xbatst\xe7\x9a\x84logger\r\nlogger.addHandler(handler)  # \xe4\xb8\xbalogger\xe6\xb7\xbb\xe5\x8a\xa0handler\r\nlogger.setLevel(logging.DEBUG)\r\n\r\nfor i in range(10):\r\n    logger.info(\'first info message %d\' % i)\r\n    logger.debug(\'first debug message %d\' % i)\r\n'"
utils/u02_tfrecord/__init__.py,0,b'# -*- coding:utf-8 -*- \r\n\r\n'
utils/u02_tfrecord/sequence_example_lib.py,30,"b'# -*- coding:utf-8 -*- \r\n\r\n# Copyright 2016 Google Inc. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the ""License"");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#    http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an ""AS IS"" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n""""""Utility functions for working with tf.train.SequenceExamples.\r\nhttps://github.com/tensorflow/magenta/blob/master/magenta/common/sequence_example_lib.py\r\n""""""\r\n\r\nimport math\r\nimport tensorflow as tf\r\n\r\nQUEUE_CAPACITY = 500\r\nSHUFFLE_MIN_AFTER_DEQUEUE = QUEUE_CAPACITY // 5\r\n\r\n\r\ndef make_sequence_example(inputs, labels):\r\n    """"""Returns a SequenceExample for the given inputs and labels.\r\n\r\n    Args:\r\n      inputs: A list of input vectors. Each input vector is a list of floats.\r\n      labels: A list of ints.\r\n\r\n    Returns:\r\n      A tf.train.SequenceExample containing inputs and labels.\r\n    """"""\r\n    input_features = [\r\n        tf.train.Feature(float_list=tf.train.FloatList(value=input_))\r\n        for input_ in inputs]\r\n    label_features = [\r\n        tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\r\n        for label in labels]\r\n    feature_list = {\r\n        \'inputs\': tf.train.FeatureList(feature=input_features),\r\n        \'labels\': tf.train.FeatureList(feature=label_features)\r\n    }\r\n    feature_lists = tf.train.FeatureLists(feature_list=feature_list)\r\n    return tf.train.SequenceExample(feature_lists=feature_lists)\r\n\r\n\r\ndef _shuffle_inputs(input_tensors, capacity, min_after_dequeue, num_threads):\r\n    """"""Shuffles tensors in `input_tensors`, maintaining grouping.""""""\r\n    shuffle_queue = tf.RandomShuffleQueue(\r\n        capacity, min_after_dequeue, dtypes=[t.dtype for t in input_tensors])\r\n    enqueue_op = shuffle_queue.enqueue(input_tensors)\r\n    runner = tf.train.QueueRunner(shuffle_queue, [enqueue_op] * num_threads)\r\n    tf.train.add_queue_runner(runner)\r\n\r\n    output_tensors = shuffle_queue.dequeue()\r\n\r\n    for i in range(len(input_tensors)):\r\n        output_tensors[i].set_shape(input_tensors[i].shape)\r\n\r\n    return output_tensors\r\n\r\n\r\ndef get_padded_batch(file_list, batch_size, input_size,\r\n                     num_enqueuing_threads=4, shuffle=False):\r\n    """"""Reads batches of SequenceExamples from TFRecords and pads them.\r\n\r\n    Can deal with variable length SequenceExamples by padding each batch to the\r\n    length of the longest sequence with zeros.\r\n\r\n    Args:\r\n      file_list: A list of paths to TFRecord files containing SequenceExamples.\r\n      batch_size: The number of SequenceExamples to include in each batch.\r\n      input_size: The size of each input vector. The returned batch of inputs\r\n          will have a shape [batch_size, num_steps, input_size].\r\n      num_enqueuing_threads: The number of threads to use for enqueuing\r\n          SequenceExamples.\r\n      shuffle: Whether to shuffle the batches.\r\n\r\n    Returns:\r\n      inputs: A tensor of shape [batch_size, num_steps, input_size] of floats32s.\r\n      labels: A tensor of shape [batch_size, num_steps] of int64s.\r\n      lengths: A tensor of shape [batch_size] of int32s. The lengths of each\r\n          SequenceExample before padding.\r\n    Raises:\r\n      ValueError: If `shuffle` is True and `num_enqueuing_threads` is less than 2.\r\n    """"""\r\n    file_queue = tf.train.string_input_producer(file_list)\r\n    reader = tf.TFRecordReader()\r\n    _, serialized_example = reader.read(file_queue)\r\n\r\n    sequence_features = {\r\n        \'inputs\': tf.FixedLenSequenceFeature(shape=[input_size],\r\n                                             dtype=tf.float32),\r\n        \'labels\': tf.FixedLenSequenceFeature(shape=[],\r\n                                             dtype=tf.int64)}\r\n\r\n    _, sequence = tf.parse_single_sequence_example(\r\n        serialized_example, sequence_features=sequence_features)\r\n\r\n    length = tf.shape(sequence[\'inputs\'])[0]  # \xe5\xba\x8f\xe5\x88\x97\xe9\x95\xbf\xe5\xba\xa6\r\n    input_tensors = [sequence[\'inputs\'], sequence[\'labels\'], length]\r\n\r\n    if shuffle:\r\n        if num_enqueuing_threads < 2:\r\n            raise ValueError(\r\n                \'`num_enqueuing_threads` must be at least 2 when shuffling.\')\r\n        shuffle_threads = int(math.ceil(num_enqueuing_threads) / 2.)\r\n\r\n        # Since there may be fewer records than SHUFFLE_MIN_AFTER_DEQUEUE, take the\r\n        # minimum of that number and the number of records.\r\n        min_after_dequeue = count_records(\r\n            file_list, stop_at=SHUFFLE_MIN_AFTER_DEQUEUE)\r\n        input_tensors = _shuffle_inputs(\r\n            input_tensors, capacity=QUEUE_CAPACITY,\r\n            min_after_dequeue=min_after_dequeue,\r\n            num_threads=shuffle_threads)\r\n\r\n        num_enqueuing_threads -= shuffle_threads\r\n\r\n    tf.logging.info(input_tensors)\r\n    return tf.train.batch(\r\n        input_tensors,\r\n        batch_size=batch_size,\r\n        capacity=QUEUE_CAPACITY,\r\n        num_threads=num_enqueuing_threads,\r\n        dynamic_pad=True,\r\n        allow_smaller_final_batch=False)\r\n\r\n\r\ndef count_records(file_list, stop_at=None):\r\n    """"""Counts number of records in files from `file_list` up to `stop_at`.\r\n\r\n    Args:\r\n      file_list: List of TFRecord files to count records in.\r\n      stop_at: Optional number of records to stop counting at.\r\n\r\n    Returns:\r\n      Integer number of records in files from `file_list` up to `stop_at`.\r\n    """"""\r\n    num_records = 0\r\n    for tfrecord_file in file_list:\r\n        tf.logging.info(\'Counting records in %s.\', tfrecord_file)\r\n        for _ in tf.python_io.tf_record_iterator(tfrecord_file):\r\n            num_records += 1\r\n            if stop_at and num_records >= stop_at:\r\n                tf.logging.info(\'Number of records is at least %d.\', num_records)\r\n                return num_records\r\n    tf.logging.info(\'Total records: %d\', num_records)\r\n    return num_records\r\n\r\n\r\ndef flatten_maybe_padded_sequences(maybe_padded_sequences, lengths=None):\r\n    """"""Flattens the batch of sequences, removing padding (if applicable).\r\n\r\n    Args:\r\n      maybe_padded_sequences: A tensor of possibly padded sequences to flatten,\r\n          sized `[N, M, ...]` where M = max(lengths).\r\n      lengths: Optional length of each sequence, sized `[N]`. If None, assumes no\r\n          padding.\r\n\r\n    Returns:\r\n       flatten_maybe_padded_sequences: The flattened sequence tensor, sized\r\n           `[sum(lengths), ...]`.\r\n    """"""\r\n\r\n    def flatten_unpadded_sequences():\r\n        # The sequences are equal length, so we should just flatten over the first\r\n        # two dimensions.\r\n        return tf.reshape(maybe_padded_sequences,\r\n                          [-1] + maybe_padded_sequences.shape.as_list()[2:])\r\n\r\n    if lengths is None:\r\n        return flatten_unpadded_sequences()\r\n\r\n    def flatten_padded_sequences():\r\n        indices = tf.where(tf.sequence_mask(lengths))\r\n        return tf.gather_nd(maybe_padded_sequences, indices)\r\n\r\n    return tf.cond(\r\n        tf.equal(tf.reduce_min(lengths), tf.shape(maybe_padded_sequences)[1]),\r\n        flatten_unpadded_sequences,\r\n        flatten_padded_sequences)\r\n'"
utils/u02_tfrecord/tfrecord_1_numpy_reader.py,11,"b'# -*- coding:utf-8 -*- \r\n\r\nimport tensorflow as tf\r\nimport os\r\nimport sys\r\nimport time\r\n\r\n\'\'\'read data\r\n\xe4\xbb\x8e tfrecord \xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\xe8\xaf\xbb\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe5\xaf\xb9\xe5\xba\x94\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe6\xa0\xbc\xe5\xbc\x8f\xe4\xb8\xba\xe5\x9b\xba\xe5\xae\x9ashape\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\r\n\'\'\'\r\n\r\n# **1.\xe6\x8a\x8a\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84 tfrecord \xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe5\x88\x97\xe8\xa1\xa8\xe5\x86\x99\xe5\x85\xa5\xe9\x98\x9f\xe5\x88\x97\xe4\xb8\xad\r\ntfrecord_dir = \'tfrecord/numpy/\'\r\ntfrecord_files = os.listdir(tfrecord_dir)\r\ntfrecord_files = list(map(lambda s: os.path.join(tfrecord_dir, s), tfrecord_files))\r\n\r\nfilename_queue = tf.train.string_input_producer(tfrecord_files, num_epochs=None, shuffle=True)\r\n\r\n# **2.\xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe8\xaf\xbb\xe5\x8f\x96\xe5\x99\xa8\r\nreader = tf.TFRecordReader()\r\n_, serialized_example = reader.read(filename_queue)\r\n# **3.\xe6\xa0\xb9\xe6\x8d\xae\xe4\xbd\xa0\xe5\x86\x99\xe5\x85\xa5\xe7\x9a\x84\xe6\xa0\xbc\xe5\xbc\x8f\xe5\xaf\xb9\xe5\xba\x94\xe8\xaf\xb4\xe6\x98\x8e\xe8\xaf\xbb\xe5\x8f\x96\xe7\x9a\x84\xe6\xa0\xbc\xe5\xbc\x8f\r\nfeatures = tf.parse_single_example(serialized_example,\r\n                                   features={\r\n                                       \'X\': tf.FixedLenFeature([784], tf.float32),  # \xe6\xb3\xa8\xe6\x84\x8f\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe6\x98\xaf\xe6\xa0\x87\xe9\x87\x8f\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe8\xaf\xb4\xe6\x98\x8e\xe6\x95\xb0\xe7\xbb\x84\xe9\x95\xbf\xe5\xba\xa6\r\n                                       \'y\': tf.FixedLenFeature([], tf.int64)}     # \xe8\x80\x8c\xe6\xa0\x87\xe9\x87\x8f\xe5\xb0\xb1\xe4\xb8\x8d\xe7\x94\xa8\xe8\xaf\xb4\xe6\x98\x8e\r\n                                   )\r\nX_out = features[\'X\']\r\ny_out = features[\'y\']\r\n\r\nprint(X_out)\r\nprint(y_out)\r\n# **4.\xe9\x80\x9a\xe8\xbf\x87 tf.train.shuffle_batch \xe6\x88\x96\xe8\x80\x85 tf.train.batch \xe5\x87\xbd\xe6\x95\xb0\xe8\xaf\xbb\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\r\n""""""\r\n\xe5\x9c\xa8shuffle_batch \xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\xad\xef\xbc\x8c\xe6\x9c\x89\xe5\x87\xa0\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe4\xbd\x9c\xe7\x94\xa8\xe5\xa6\x82\xe4\xb8\x8b\xef\xbc\x9a\r\ncapacity: \xe9\x98\x9f\xe5\x88\x97\xe7\x9a\x84\xe5\xae\xb9\xe9\x87\x8f\xef\xbc\x8c\xe5\xae\xb9\xe9\x87\x8f\xe8\xb6\x8a\xe5\xa4\xa7\xe7\x9a\x84\xe8\xaf\x9d\xef\xbc\x8cshuffle \xe5\xbe\x97\xe5\xb0\xb1\xe6\x9b\xb4\xe5\x8a\xa0\xe5\x9d\x87\xe5\x8c\x80\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe5\x8d\xa0\xe7\x94\xa8\xe5\x86\x85\xe5\xad\x98\xe4\xb9\x9f\xe4\xbc\x9a\xe6\x9b\xb4\xe5\xa4\x9a\r\nnum_threads: \xe8\xaf\xbb\xe5\x8f\x96\xe8\xbf\x9b\xe7\xa8\x8b\xe6\x95\xb0\xef\xbc\x8c\xe8\xbf\x9b\xe7\xa8\x8b\xe8\xb6\x8a\xe5\xa4\x9a\xef\xbc\x8c\xe8\xaf\xbb\xe5\x8f\x96\xe9\x80\x9f\xe5\xba\xa6\xe7\x9b\xb8\xe5\xaf\xb9\xe4\xbc\x9a\xe5\xbf\xab\xe4\xba\x9b\xef\xbc\x8c\xe6\xa0\xb9\xe6\x8d\xae\xe4\xb8\xaa\xe4\xba\xba\xe9\x85\x8d\xe7\xbd\xae\xe5\x86\xb3\xe5\xae\x9a\r\nmin_after_dequeue: \xe4\xbf\x9d\xe8\xaf\x81\xe9\x98\x9f\xe5\x88\x97\xe4\xb8\xad\xe6\x9c\x80\xe5\xb0\x91\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x87\x8f\xe3\x80\x82\r\n   \xe5\x81\x87\xe8\xae\xbe\xe6\x88\x91\xe4\xbb\xac\xe8\xae\xbe\xe5\xae\x9a\xe4\xba\x86\xe9\x98\x9f\xe5\x88\x97\xe7\x9a\x84\xe5\xae\xb9\xe9\x87\x8fC\xef\xbc\x8c\xe5\x9c\xa8\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\x96\xe8\xb5\xb0\xe9\x83\xa8\xe5\x88\x86\xe6\x95\xb0\xe6\x8d\xaem\xe4\xbb\xa5\xe5\x90\x8e\xef\xbc\x8c\xe9\x98\x9f\xe5\x88\x97\xe4\xb8\xad\xe5\x8f\xaa\xe5\x89\xa9\xe4\xb8\x8b\xe4\xba\x86 (C-m) \xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\xe7\x84\xb6\xe5\x90\x8e\xe9\x98\x9f\xe5\x88\x97\xe4\xbc\x9a\xe4\xb8\x8d\xe6\x96\xad\xe8\xa1\xa5\xe5\x85\x85\xe6\x95\xb0\xe6\x8d\xae\xe8\xbf\x9b\xe6\x9d\xa5\xef\xbc\x8c\r\n   \xe5\xa6\x82\xe6\x9e\x9c\xe5\x90\x8e\xe5\x8b\xa4\xe4\xbe\x9b\xe5\xba\x94\xef\xbc\x88CPU\xe6\x80\xa7\xe8\x83\xbd,\xe7\xba\xbf\xe7\xa8\x8b\xe6\x95\xb0\xe9\x87\x8f\xef\xbc\x89\xe8\xa1\xa5\xe5\x85\x85\xe9\x80\x9f\xe5\xba\xa6\xe6\x85\xa2\xe7\x9a\x84\xe8\xaf\x9d\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe4\xb8\x8b\xe4\xb8\x80\xe6\xac\xa1\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x8c\xe5\x8f\xaf\xe8\x83\xbd\xe6\x89\x8d\xe8\xa1\xa5\xe5\x85\x85\xe4\xba\x86\xe4\xb8\x80\xe7\x82\xb9\xe7\x82\xb9\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe8\xa1\xa5\xe5\x85\x85\xe5\xae\x8c\xe5\x90\x8e\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe4\xb8\xaa\xe6\x95\xb0\xe5\xb0\x91\xe4\xba\x8e\r\n   min_after_dequeue \xe7\x9a\x84\xe8\xaf\x9d\xef\xbc\x8c\xe4\xb8\x8d\xe8\x83\xbd\xe5\x8f\x96\xe8\xb5\xb0\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe5\xbe\x97\xe7\xbb\xa7\xe7\xbb\xad\xe7\xad\x89\xe5\xae\x83\xe8\xa1\xa5\xe5\x85\x85\xe8\xb6\x85\xe8\xbf\x87 min_after_dequeue \xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe4\xbb\xa5\xe5\x90\x8e\xe6\x89\x8d\xe8\xae\xa9\xe5\x8f\x96\xe8\xb5\xb0\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\r\n   \xe8\xbf\x99\xe6\xa0\xb7\xe5\x81\x9a\xe4\xbf\x9d\xe8\xaf\x81\xe4\xba\x86\xe9\x98\x9f\xe5\x88\x97\xe4\xb8\xad\xe6\xb7\xb7\xe7\x9d\x80\xe8\xb6\xb3\xe5\xa4\x9f\xe5\xa4\x9a\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe4\xbb\x8e\xe8\x80\x8c\xe6\x89\x8d\xe8\x83\xbd\xe4\xbf\x9d\xe8\xaf\x81 shuffle \xe5\x8f\x96\xe5\x80\xbc\xe6\x9b\xb4\xe5\x8a\xa0\xe9\x9a\x8f\xe6\x9c\xba\xe3\x80\x82\r\n   \xe4\xbd\x86\xe6\x98\xaf\xef\xbc\x8cmin_after_dequeue \xe4\xb8\x8d\xe8\x83\xbd\xe8\xae\xbe\xe7\xbd\xae\xe5\xa4\xaa\xe5\xa4\xa7\xef\xbc\x8c\xe5\x90\xa6\xe5\x88\x99\xe8\xa1\xa5\xe5\x85\x85\xe6\x97\xb6\xe9\x97\xb4\xe5\xbe\x88\xe9\x95\xbf\xef\xbc\x8c\xe8\xaf\xbb\xe5\x8f\x96\xe9\x80\x9f\xe5\xba\xa6\xe4\xbc\x9a\xe5\xbe\x88\xe6\x85\xa2\xe3\x80\x82\r\n""""""\r\nX_batch, y_batch = tf.train.shuffle_batch([X_out, y_out], batch_size=128,\r\n                                          capacity=2000, min_after_dequeue=100, num_threads=4)\r\nsess = tf.Session()\r\ninit = tf.global_variables_initializer()\r\nsess.run(init)\r\n\r\n# **5.\xe5\x90\xaf\xe5\x8a\xa8\xe9\x98\x9f\xe5\x88\x97\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x95\xb0\xe6\x8d\xae\xe8\xaf\xbb\xe5\x8f\x96\r\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84 coord \xe6\x98\xaf\xe4\xb8\xaa\xe7\xba\xbf\xe7\xa8\x8b\xe5\x8d\x8f\xe8\xb0\x83\xe5\x99\xa8\xef\xbc\x8c\xe6\x8a\x8a\xe5\x90\xaf\xe5\x8a\xa8\xe9\x98\x9f\xe5\x88\x97\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe5\x8a\xa0\xe4\xb8\x8a\xe7\xba\xbf\xe7\xa8\x8b\xe5\x8d\x8f\xe8\xb0\x83\xe5\x99\xa8\xe3\x80\x82\r\n# \xe8\xbf\x99\xe6\xa0\xb7\xef\xbc\x8c\xe5\x9c\xa8\xe6\x95\xb0\xe6\x8d\xae\xe8\xaf\xbb\xe5\x8f\x96\xe5\xae\x8c\xe6\xaf\x95\xe4\xbb\xa5\xe5\x90\x8e\xef\xbc\x8c\xe8\xb0\x83\xe7\x94\xa8\xe5\x8d\x8f\xe8\xb0\x83\xe5\x99\xa8\xe6\x8a\x8a\xe7\xba\xbf\xe7\xa8\x8b\xe5\x85\xa8\xe9\x83\xa8\xe9\x83\xbd\xe5\x85\xb3\xe4\xba\x86\xe3\x80\x82\r\ncoord = tf.train.Coordinator()\r\nthreads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n# y_outputs = list()\r\n# for i in range(5):\r\n#     _X_batch, _y_batch = sess.run([X_batch, y_batch])\r\n#     print(\'** batch %d\' % i)\r\n#     print(\'_X_batch:\\n\', _X_batch)\r\n#     print(\'_y_batch:\\n\', _y_batch)\r\n#     y_outputs.extend(_y_batch.tolist())\r\n# print(\'All y_outputs: \\n\', y_outputs)\r\n\r\n# \xe8\xbf\xad\xe4\xbb\xa3\xe5\x8f\x96\xe5\x80\xbc\r\ntime0 = time.time()\r\nfor count in range(100):  # 100batch  125 seconds\r\n    _X_batch, _y_batch = sess.run([X_batch, y_batch])\r\n    sys.stdout.write(""\\rloop {}, pass {:.2f}s"".format(count, time.time() - time0))\r\n    sys.stdout.flush()\r\n\r\n# **6.\xe6\x9c\x80\xe5\x90\x8e\xe8\xae\xb0\xe5\xbe\x97\xe6\x8a\x8a\xe9\x98\x9f\xe5\x88\x97\xe5\x85\xb3\xe6\x8e\x89\r\ncoord.request_stop()\r\ncoord.join(threads)\r\n\r\n'"
utils/u02_tfrecord/tfrecord_1_numpy_reader_without_shuffle.py,16,"b""# -*- coding:utf-8 -*- \r\n\r\nimport tensorflow as tf\r\nimport os\r\nimport time\r\n\r\n'''read data.\r\nwithout shuffle, for validation or test data.\r\n\xe9\x81\x8d\xe5\x8e\x86\xe4\xb8\x80\xe6\xac\xa1\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe9\x92\x88\xe5\xaf\xb9\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe6\x88\x96\xe8\x80\x85\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe4\xbd\xbf\xe7\x94\xa8 shuffle \xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xe8\xaf\xbb\xe5\x8f\x96 batch\xef\xbc\x8c\xe8\x80\x8c\xe6\x98\xaf\xe9\x9c\x80\xe8\xa6\x81\xe4\xbf\x9d\xe8\xaf\x81\xe5\xae\x8c\xe6\x95\xb4\xe7\x9a\x84\xe9\x81\x8d\xe5\x8e\x86\xe4\xb8\x80\xe6\xac\xa1\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe3\x80\x82\r\n\xe5\x92\x8c tf.train.shuffle_batch \xe7\x9a\x84\xe8\xaf\xbb\xe5\x8f\x96\xe6\x96\xb9\xe5\xbc\x8f\xe7\xb1\xbb\xe4\xbc\xbc\xef\xbc\x8c\xe5\x8f\xaa\xe6\x98\xaf\xe6\x9c\x89\xe5\x87\xa0\xe4\xb8\xaa\xe5\x9c\xb0\xe6\x96\xb9\xe9\x9c\x80\xe8\xa6\x81\xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x9a\r\n1.\xe6\x8a\x8a\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe5\xad\x97\xe5\x86\x99\xe5\x85\xa5\xe9\x98\x9f\xe5\x88\x97\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x8c\xe8\xae\xb0\xe5\xbe\x97\xe8\xae\xbe shuffle=False\r\n2.\xe5\x90\x8c\xe6\x97\xb6\xe8\xae\xbe\xe7\xbd\xae num_epochs \xe5\x8f\x82\xe6\x95\xb0\xe4\xb8\xba 1. \xe8\xbf\x99\xe6\xa0\xb7\xe5\xbd\x93\xe8\xaf\xbb\xe5\x8f\x96\xe5\xae\x8c\xe4\xb8\x80\xe4\xb8\xaaepoch\xe4\xbb\xa5\xe5\x90\x8e\xe5\xb0\xb1\xe4\xbc\x9a\xe6\x8a\x9b\xe5\x87\xba OutOfRangeError.\r\n3.tf.train.batch() \xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84 allow_smaller_final_batch \xe5\x8f\x82\xe6\x95\xb0\xe4\xb8\x80\xe5\xae\x9a\xe8\xa6\x81\xe8\xae\xbe\xe7\xbd\xae True\xef\xbc\x8c \xe6\x89\x8d\xe4\xb8\x8d\xe4\xbc\x9a\xe6\xbc\x8f\xe6\x8e\x89\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x8d\xe6\xbb\xa1 batch_size \xe7\x9a\x84\xe4\xb8\x80\xe5\xb0\x8f\xe9\x83\xa8\xe5\x88\x86\xe6\x95\xb0\xe6\x8d\xae\r\n4.\xe5\x9c\xa8\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe5\x8a\xa0\xe4\xb8\x8a tf.initialize_local_variables()\xef\xbc\x8c \xe5\x90\xa6\xe5\x88\x99\xe4\xbc\x9a\xe6\x8a\xa5\xe9\x94\x99\xe3\x80\x82\r\n5.\xe8\xae\xb0\xe5\xbe\x97\xe5\x8a\xa0\xe4\xb8\x8a try...except... \xe6\x9d\xa5\xe6\x8d\x95\xe8\x8e\xb7 OutOfRangeError\r\n'''\r\n\r\n# **1.\xe6\x8a\x8a\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84 tfrecord \xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe5\x88\x97\xe8\xa1\xa8\xe5\x86\x99\xe5\x85\xa5\xe9\x98\x9f\xe5\x88\x97\xe4\xb8\xad\r\ntfrecord_dir = 'tfrecord/numpy/'\r\ntfrecord_files = os.listdir(tfrecord_dir)\r\ntfrecord_files = list(map(lambda s: os.path.join(tfrecord_dir, s), tfrecord_files))\r\n\r\nfilename_queue = tf.train.string_input_producer(tfrecord_files, num_epochs=None, shuffle=True)\r\n\r\n# create a reader from file queue\r\nreader = tf.TFRecordReader()\r\n_, serialized_example = reader.read(filename_queue)\r\n# get feature from serialized example\r\nfeatures = tf.parse_single_example(serialized_example,\r\n                                   features={\r\n                                       'X': tf.FixedLenFeature([784], tf.float32),\r\n                                       'y': tf.FixedLenFeature([], tf.int64)}\r\n                                   )\r\nX_out = features['X']\r\ny_out = features['y']\r\n\r\nbatch_size = 128\r\nX_batch, y_batch = tf.train.batch([X_out, y_out],\r\n                                  batch_size=2,\r\n                                  capacity=2000,\r\n                                  num_threads=4,\r\n                                  allow_smaller_final_batch=True)  # \xe4\xbf\x9d\xe8\xaf\x81\xe9\x81\x8d\xe5\x8e\x86\xe5\xae\x8c\xe6\x95\xb4\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\r\n\r\n# \xe5\xa6\x82\xe6\x9e\x9c\xe8\xae\xbe\xe5\xae\x9a\xe4\xba\x86\xe8\xaf\xbb\xe5\x8f\x96\xe8\xbd\xae\xe6\x95\xb0\xef\xbc\x88num_epochs\xef\xbc\x89\xe4\xb8\xba1\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\xb0\xb1\xe7\xae\x97\xe4\xbd\xbf\xe7\x94\xa8 shuffle_batch, \xe4\xb9\x9f\xe4\xbc\x9a\xe4\xbf\x9d\xe8\xaf\x81\xe5\x8f\xaa\xe5\x8f\x96\xe4\xb8\x80\xe4\xb8\xaaepoch\xef\xbc\x8c\xe4\xb8\x8d\xe9\x87\x8d\xe5\xa4\x8d\xef\xbc\x8c\xe4\xb9\x9f\xe4\xb8\x8d\xe4\xbc\x9a\xe6\xbc\x8f\xe5\xa4\xb1\r\n# X_batch, y_batch = tf.train.shuffle_batch([X_out, y_out],\r\n#                                           batch_size=6,\r\n#                                           capacity=20,\r\n#                                           min_after_dequeue=10,\r\n#                                           num_threads=2,\r\n#                                           allow_smaller_final_batch=True)\r\n\r\nsess = tf.Session()\r\n# \xe6\xb3\xa8\xe6\x84\x8f\xe4\xb8\x8b\xe9\x9d\xa2\xe8\xbf\x99\xe4\xb8\xaa\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe8\xae\xbe\xe4\xba\x86 num_epoch\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96 local_variables\r\ninit = tf.group(tf.global_variables_initializer(),\r\n                tf.initialize_local_variables())\r\nsess.run(init)\r\n\r\ncoord = tf.train.Coordinator()\r\nthreads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\ny_outputs = list()\r\nfor i in range(25):\r\n    try:\r\n        _X_batch, _y_batch = sess.run([X_batch, y_batch])\r\n        print('** batch %d' % i)\r\n        print('_y_batch:\\n', _y_batch)\r\n        y_outputs.extend(_y_batch.tolist())\r\n    except tf.errors.OutOfRangeError as e:\r\n        print(e.message)\r\n        break\r\nprint('all y_outputs:', y_outputs)\r\nprint('sorted y_outputs:', sorted(y_outputs))\r\n\r\ncoord.request_stop()\r\ncoord.join(threads)\r\n"""
utils/u02_tfrecord/tfrecord_1_numpy_writer.py,5,"b'""""""tfrecord \xe5\x86\x99\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae.\r\n\xe5\xb0\x86\xe5\x9b\xba\xe5\xae\x9ashape\xe7\x9a\x84\xe7\x9f\xa9\xe9\x98\xb5\xe5\x86\x99\xe5\x85\xa5 tfrecord \xe6\x96\x87\xe4\xbb\xb6\xe3\x80\x82\xe8\xbf\x99\xe7\xa7\x8d\xe5\xbd\xa2\xe5\xbc\x8f\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\x86\x99\xe5\x85\xa5 tfrecord \xe6\x98\xaf\xe6\x9c\x80\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe3\x80\x82\r\nrefer: http://blog.csdn.net/qq_16949707/article/details/53483493\r\n""""""\r\n\r\nfrom __future__ import print_function\r\nfrom __future__ import division\r\nfrom __future__ import absolute_import\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\nimport os\r\nimport sys\r\nimport time\r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n\r\nmnist = input_data.read_data_sets(\'../../data/MNIST_data\', one_hot=False)\r\nX = mnist.train.images\r\ny = mnist.train.labels\r\n\r\nNUM_SHARDS = 64  # tfrecord \xe6\x96\x87\xe4\xbb\xb6\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\xef\xbc\x8c\xe7\xa8\x8d\xe5\xbe\xae\xe5\xa4\xa7\xe4\xba\x9b\xe5\xaf\xb9 shuffle \xe4\xbc\x9a\xe5\xa5\xbd\xe4\xba\x9b\r\nn_sample = len(X)\r\nnum_per_shard = n_sample // NUM_SHARDS  # \xe6\xaf\x8f\xe4\xb8\xaa tfrecord \xe7\x9a\x84\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\xe9\x87\x8f\r\n# \xe5\x9c\xa8\xe6\x89\x93\xe5\x8c\x85\xe4\xb9\x8b\xe5\x89\x8d\xe5\x85\x88\xe6\x89\x8b\xe5\x8a\xa8\xe6\x89\x93\xe4\xb9\xb1\xe4\xb8\x80\xe6\xac\xa1\r\nnew_idxs = np.random.permutation(n_sample)\r\nX = X[new_idxs]\r\ny = y[new_idxs]\r\n\r\ntfrecord_dir = \'tfrecord/numpy/\'\r\nif not os.path.exists(tfrecord_dir):\r\n    os.makedirs(tfrecord_dir)\r\n\r\ntime0 = time.time()\r\nfor shard_id in range(NUM_SHARDS):\r\n    output_filename = \'%d-of-%d.tfrecord\' % (shard_id, NUM_SHARDS)\r\n    output_path = os.path.join(tfrecord_dir, output_filename)\r\n    with tf.python_io.TFRecordWriter(output_path) as writer:\r\n        start_ndx = shard_id * num_per_shard\r\n        end_ndx = min((shard_id + 1) * num_per_shard, n_sample)\r\n        for i in range(start_ndx, end_ndx):\r\n            sys.stdout.write(\'\\r>> Converting image %d/%d shard %d, %g s\' % (\r\n                i + 1, n_sample, shard_id, time.time() - time0))\r\n            sys.stdout.flush()\r\n\r\n            X_sample = X[i].tolist()\r\n            y_sample = y[i]\r\n            # **3.\xe5\xae\x9a\xe4\xb9\x89\xe6\x95\xb0\xe6\x8d\xae\xe7\xb1\xbb\xe5\x9e\x8b\xef\xbc\x8c\xe6\x8c\x89\xe7\x85\xa7\xe8\xbf\x99\xe9\x87\x8c\xe5\x9b\xba\xe5\xae\x9a\xe7\x9a\x84\xe5\xbd\xa2\xe5\xbc\x8f\xe5\x86\x99\xef\xbc\x8c\xe6\x9c\x89float_list(\xe5\xa5\xbd\xe5\x83\x8f\xe5\x8f\xaa\xe6\x9c\x8932\xe4\xbd\x8d), int64_list, bytes_list.\r\n            example = tf.train.Example(\r\n                features=tf.train.Features(\r\n                    feature={\'X\': tf.train.Feature(float_list=tf.train.FloatList(value=X_sample)),\r\n                             \'y\': tf.train.Feature(int64_list=tf.train.Int64List(value=[y_sample]))}))\r\n            # **4.\xe5\xba\x8f\xe5\x88\x97\xe5\x8c\x96\xe6\x95\xb0\xe6\x8d\xae\xe5\xb9\xb6\xe5\x86\x99\xe5\x85\xa5\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\r\n            serialized = example.SerializeToString()\r\n            writer.write(serialized)\r\n\r\nprint(\'Finished writing training data to tfrecord files.\')\r\n'"
utils/u02_tfrecord/tfrecord_2_seqence_reader.py,19,"b'# -*- coding:utf-8 -*-\r\n\r\nimport tensorflow as tf\r\nimport math\r\n\r\nQUEUE_CAPACITY = 100\r\nSHUFFLE_MIN_AFTER_DEQUEUE = QUEUE_CAPACITY // 5\r\n\r\n""""""\r\n\xe8\xaf\xbb\xe5\x8f\x96\xe5\x8f\x98\xe9\x95\xbf\xe5\xba\x8f\xe5\x88\x97\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\r\n\xe5\x92\x8c\xe5\x9b\xba\xe5\xae\x9ashape\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe8\xaf\xbb\xe5\x8f\x96\xe6\x96\xb9\xe5\xbc\x8f\xe4\xb8\x8d\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe5\x9c\xa8\xe8\xaf\xbb\xe5\x8f\x96\xe5\x8f\x98\xe9\x95\xbf\xe5\xba\x8f\xe5\x88\x97\xe4\xb8\xad\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe6\x97\xa0\xe6\xb3\x95\xe4\xbd\xbf\xe7\x94\xa8 tf.train.shuffle_batch() \xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe5\x8f\xaa\xe8\x83\xbd\xe4\xbd\xbf\xe7\x94\xa8\r\ntf.train.batch() \xe5\x87\xbd\xe6\x95\xb0\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xaf\xbb\xe5\x8f\x96\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x94\xef\xbc\x8c\xe5\x9c\xa8\xe8\xaf\xbb\xe5\x8f\x96\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x8c\xe5\xbf\x85\xe9\xa1\xbb\xe8\xae\xbe\xe7\xbd\xae dynamic_pad \xe5\x8f\x82\xe6\x95\xb0\xe4\xb8\xba True, \xe6\x8a\x8a\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe5\xba\x8f\xe5\x88\x97 padding\r\n\xe5\x88\xb0\xe5\x9b\xba\xe5\xae\x9a\xe9\x95\xbf\xe5\xba\xa6\xef\xbc\x88\xe8\xaf\xa5batch\xe4\xb8\xad\xe6\x9c\x80\xe9\x95\xbf\xe7\x9a\x84\xe5\xba\x8f\xe5\x88\x97\xe9\x95\xbf\xe5\xba\xa6\xef\xbc\x89\xef\xbc\x8cpadding\xe9\x83\xa8\xe5\x88\x86\xe4\xb8\xba 0\xe3\x80\x82\r\n\r\n\xe6\xad\xa4\xe5\xa4\x96\xef\xbc\x8c\xe5\x9c\xa8\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe4\xb8\xba\xe4\xba\x86\xe5\xae\x9e\xe7\x8e\xb0 shuffle \xe5\x8a\x9f\xe8\x83\xbd\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8 RandomShuffleQueue \xe9\x98\x9f\xe5\x88\x97\xe6\x9d\xa5\xe5\xae\x8c\xe6\x88\x90\xe3\x80\x82\xe8\xaf\xa6\xe8\xa7\x81\xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84 _shuffle_inputs \xe5\x87\xbd\xe6\x95\xb0\xe3\x80\x82\r\n""""""\r\n\r\n\r\ndef _shuffle_inputs(input_tensors, capacity, min_after_dequeue, num_threads):\r\n    """"""Shuffles tensors in `input_tensors`, maintaining grouping.""""""\r\n    shuffle_queue = tf.RandomShuffleQueue(\r\n        capacity, min_after_dequeue, dtypes=[t.dtype for t in input_tensors])\r\n    enqueue_op = shuffle_queue.enqueue(input_tensors)\r\n    runner = tf.train.QueueRunner(shuffle_queue, [enqueue_op] * num_threads)\r\n    tf.train.add_queue_runner(runner)\r\n\r\n    output_tensors = shuffle_queue.dequeue()\r\n\r\n    for i in range(len(input_tensors)):\r\n        output_tensors[i].set_shape(input_tensors[i].shape)\r\n\r\n    return output_tensors\r\n\r\n\r\ndef get_padded_batch(file_list, batch_size, num_enqueuing_threads=4, shuffle=False):\r\n    """"""Reads batches of SequenceExamples from TFRecords and pads them.\r\n\r\n    Can deal with variable length SequenceExamples by padding each batch to the\r\n    length of the longest sequence with zeros.\r\n\r\n    Args:\r\n      file_list: A list of paths to TFRecord files containing SequenceExamples.\r\n      batch_size: The number of SequenceExamples to include in each batch.\r\n      num_enqueuing_threads: The number of threads to use for enqueuing\r\n          SequenceExamples.\r\n      shuffle: Whether to shuffle the batches.\r\n\r\n    Returns:\r\n      labels: A tensor of shape [batch_size] of int64s.\r\n      frames: A tensor of shape [batch_size, num_steps] of floats32s. note that\r\n          num_steps is the max time_step of all the tensors.\r\n    Raises:\r\n      ValueError: If `shuffle` is True and `num_enqueuing_threads` is less than 2.\r\n    """"""\r\n    file_queue = tf.train.string_input_producer(file_list)\r\n    reader = tf.TFRecordReader()\r\n    _, serialized_example = reader.read(file_queue)\r\n\r\n    context_features = {\r\n        ""label"": tf.FixedLenFeature([], dtype=tf.int64)\r\n    }\r\n    sequence_features = {\r\n        ""frame"": tf.FixedLenSequenceFeature([], dtype=tf.int64)\r\n    }\r\n\r\n    context_parsed, sequence_parsed = tf.parse_single_sequence_example(\r\n        serialized=serialized_example,\r\n        context_features=context_features,\r\n        sequence_features=sequence_features\r\n    )\r\n\r\n    labels = context_parsed[\'label\']\r\n    frames = sequence_parsed[\'frame\']\r\n    input_tensors = [labels, frames]\r\n\r\n    if shuffle:\r\n        if num_enqueuing_threads < 2:\r\n            raise ValueError(\r\n                \'`num_enqueuing_threads` must be at least 2 when shuffling.\')\r\n        shuffle_threads = int(math.ceil(num_enqueuing_threads) / 2.)\r\n\r\n        # Since there may be fewer records than SHUFFLE_MIN_AFTER_DEQUEUE, take the\r\n        # minimum of that number and the number of records.\r\n        min_after_dequeue = count_records(\r\n            file_list, stop_at=SHUFFLE_MIN_AFTER_DEQUEUE)\r\n        input_tensors = _shuffle_inputs(\r\n            input_tensors, capacity=QUEUE_CAPACITY,\r\n            min_after_dequeue=min_after_dequeue,\r\n            num_threads=shuffle_threads)\r\n\r\n        num_enqueuing_threads -= shuffle_threads\r\n\r\n    tf.logging.info(input_tensors)\r\n    return tf.train.batch(\r\n        input_tensors,\r\n        batch_size=batch_size,\r\n        capacity=QUEUE_CAPACITY,\r\n        num_threads=num_enqueuing_threads,\r\n        dynamic_pad=True,\r\n        allow_smaller_final_batch=False)\r\n\r\n\r\ndef count_records(file_list, stop_at=None):\r\n    """"""Counts number of records in files from `file_list` up to `stop_at`.\r\n\r\n    Args:\r\n      file_list: List of TFRecord files to count records in.\r\n      stop_at: Optional number of records to stop counting at.\r\n\r\n    Returns:\r\n      Integer number of records in files from `file_list` up to `stop_at`.\r\n    """"""\r\n    num_records = 0\r\n    for tfrecord_file in file_list:\r\n        tf.logging.info(\'Counting records in %s.\', tfrecord_file)\r\n        for _ in tf.python_io.tf_record_iterator(tfrecord_file):\r\n            num_records += 1\r\n            if stop_at and num_records >= stop_at:\r\n                tf.logging.info(\'Number of records is at least %d.\', num_records)\r\n                return num_records\r\n    tf.logging.info(\'Total records: %d\', num_records)\r\n    return num_records\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    tfrecord_file_names = [\'../../data/seq_test1.tfrecord\', \'../../data/seq_test2.tfrecord\']\r\n    label_batch, frame_batch = get_padded_batch(tfrecord_file_names, 10, shuffle=True)\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    sess = tf.Session(config=config)\r\n    tf.train.start_queue_runners(sess=sess)\r\n    for i in xrange(3):\r\n        _frames_batch, _label_batch = sess.run([frame_batch, label_batch])\r\n        print(\'** batch %d\' % i)\r\n        print(_label_batch)\r\n        print(_frames_batch)\r\n'"
utils/u02_tfrecord/tfrecord_2_seqence_writer.py,10,"b'# -*- coding:utf-8 -*- \r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\n\r\n\'\'\'tfrecord \xe5\x86\x99\xe5\x85\xa5\xe5\xba\x8f\xe5\x88\x97\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\x8d\xe5\x9b\xba\xe5\xae\x9a\xe3\x80\x82\r\n\xe5\x92\x8c\xe5\x9b\xba\xe5\xae\x9a shape \xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\xa4\x84\xe7\x90\x86\xe6\x96\xb9\xe5\xbc\x8f\xe7\xb1\xbb\xe4\xbc\xbc\xef\xbc\x8c\xe5\x89\x8d\xe8\x80\x85\xe4\xbd\xbf\xe7\x94\xa8 tf.train.Example() \xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe8\x80\x8c\xe5\xaf\xb9\xe4\xba\x8e\xe5\x8f\x98\xe9\x95\xbf\xe5\xba\x8f\xe5\x88\x97\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe4\xbd\xbf\xe7\x94\xa8 \r\ntf.train.SequenceExample()\xe3\x80\x82 \xe5\x9c\xa8 tf.train.SequenceExample() \xe4\xb8\xad\xef\xbc\x8c\xe5\x8f\x88\xe5\x8c\x85\xe6\x8b\xac\xe4\xba\x86\xe4\xb8\xa4\xe9\x83\xa8\xe5\x88\x86\xef\xbc\x9a\r\ncontext \xe6\x9d\xa5\xe6\x94\xbe\xe7\xbd\xae\xe9\x9d\x9e\xe5\xba\x8f\xe5\x88\x97\xe5\x8c\x96\xe9\x83\xa8\xe5\x88\x86\xef\xbc\x9b\r\nfeature_lists \xe6\x94\xbe\xe7\xbd\xae\xe5\x8f\x98\xe9\x95\xbf\xe5\xba\x8f\xe5\x88\x97\xe3\x80\x82\r\n\r\nrefer: \r\nhttps://github.com/tensorflow/magenta/blob/master/magenta/common/sequence_example_lib.py\r\nhttps://github.com/dennybritz/tf-rnn\r\nhttp://leix.me/2017/01/09/tensorflow-practical-guides/\r\nhttps://github.com/siavash9000/im2txt_demo/blob/master/im2txt/im2txt/ops/inputs.py\r\n\'\'\'\r\n\r\n# **1.\xe5\x88\x9b\xe5\xbb\xba\xe6\x96\x87\xe4\xbb\xb6\r\nwriter1 = tf.python_io.TFRecordWriter(\'../../data/seq_test1.tfrecord\')\r\nwriter2 = tf.python_io.TFRecordWriter(\'../../data/seq_test2.tfrecord\')\r\n\r\n# \xe9\x9d\x9e\xe5\xba\x8f\xe5\x88\x97\xe6\x95\xb0\xe6\x8d\xae\r\nlabels = [1, 2, 3, 4, 5, 1, 2, 3, 4]\r\n# \xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\x8d\xe5\x9b\xba\xe5\xae\x9a\xe7\x9a\x84\xe5\xba\x8f\xe5\x88\x97\r\nframes = [[1], [2, 2], [3, 3, 3], [4, 4, 4, 4], [5, 5, 5, 5, 5],\r\n          [1], [2, 2], [3, 3, 3], [4, 4, 4, 4]]\r\n\r\n\r\nwriter = writer1\r\nfor i in tqdm(xrange(len(labels))):  # **2.\xe5\xaf\xb9\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\r\n    if i == len(labels) / 2:\r\n        writer = writer2\r\n        print(\'\\nThere are %d sample writen into writer1\' % i)\r\n    label = labels[i]\r\n    frame = frames[i]\r\n    # \xe9\x9d\x9e\xe5\xba\x8f\xe5\x88\x97\xe5\x8c\x96\r\n    label_feature = tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\r\n    # \xe5\xba\x8f\xe5\x88\x97\xe5\x8c\x96\r\n    frame_feature = [\r\n        tf.train.Feature(int64_list=tf.train.Int64List(value=[frame_])) for frame_ in frame\r\n    ]\r\n\r\n    seq_example = tf.train.SequenceExample(\r\n        # context \xe6\x9d\xa5\xe6\x94\xbe\xe7\xbd\xae\xe9\x9d\x9e\xe5\xba\x8f\xe5\x88\x97\xe5\x8c\x96\xe9\x83\xa8\xe5\x88\x86\r\n        context=tf.train.Features(feature={\r\n            ""label"": label_feature\r\n        }),\r\n        # feature_lists \xe6\x94\xbe\xe7\xbd\xae\xe5\x8f\x98\xe9\x95\xbf\xe5\xba\x8f\xe5\x88\x97\r\n        feature_lists=tf.train.FeatureLists(feature_list={\r\n            ""frame"": tf.train.FeatureList(feature=frame_feature),\r\n        })\r\n    )\r\n\r\n    serialized = seq_example.SerializeToString()\r\n    writer.write(serialized)  # **4.\xe5\x86\x99\xe5\x85\xa5\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\r\n\r\nprint(\'Finished.\')\r\nwriter1.close()\r\nwriter2.close()\r\n'"
utils/u02_tfrecord/tfrecord_3_image_reader.py,17,"b'# -*- coding:utf-8 -*- \r\n\r\nimport tensorflow as tf\r\n\r\nimport os\r\nimport time\r\nimport sys\r\n\r\n\'\'\'read data\r\n\xe4\xbb\x8e tfrecord \xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\xe8\xaf\xbb\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe5\xaf\xb9\xe5\xba\x94\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe6\xa0\xbc\xe5\xbc\x8f\xe4\xb8\xba png \xe6\xa0\xbc\xe5\xbc\x8f\xe3\x80\x82\r\n\'\'\'\r\n\r\n# **1.\xe6\x8a\x8a\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84 tfrecord \xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe5\x88\x97\xe8\xa1\xa8\xe5\x86\x99\xe5\x85\xa5\xe9\x98\x9f\xe5\x88\x97\xe4\xb8\xad\r\nTFRECORD_DIR = \'tfrecord/sketchy_image/\'\r\ntfrecord_files = os.listdir(TFRECORD_DIR)\r\ntfrecord_files = list(map(lambda s: os.path.join(TFRECORD_DIR, s), tfrecord_files))\r\n\r\nfilename_queue = tf.train.string_input_producer(tfrecord_files, num_epochs=None, shuffle=True)\r\n# **2.\xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe8\xaf\xbb\xe5\x8f\x96\xe5\x99\xa8\r\nreader = tf.TFRecordReader()\r\n_, serialized_example = reader.read(filename_queue)\r\n# **3.\xe6\xa0\xb9\xe6\x8d\xae\xe4\xbd\xa0\xe5\x86\x99\xe5\x85\xa5\xe7\x9a\x84\xe6\xa0\xbc\xe5\xbc\x8f\xe5\xaf\xb9\xe5\xba\x94\xe8\xaf\xb4\xe6\x98\x8e\xe8\xaf\xbb\xe5\x8f\x96\xe7\x9a\x84\xe6\xa0\xbc\xe5\xbc\x8f\r\nfeatures = tf.parse_single_example(serialized_example,\r\n                                   features={\r\n                                       \'image\': tf.FixedLenFeature([], tf.string),\r\n                                       \'label\': tf.FixedLenFeature([], tf.int64)\r\n                                   }\r\n                                   )\r\nimg = features[\'image\']\r\n# \xe8\xbf\x99\xe9\x87\x8c\xe9\x9c\x80\xe8\xa6\x81\xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xa7\xa3\xe7\xa0\x81\r\nimg = tf.image.decode_png(img, channels=3)  # \xe8\xbf\x99\xe9\x87\x8c\xef\xbc\x8c\xe4\xb9\x9f\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xa7\xa3\xe7\xa0\x81\xe4\xb8\xba 1 \xe9\x80\x9a\xe9\x81\x93\r\nimg = tf.image.resize_images(images=img, size=(224, 224))\r\n# img = tf.image.resize_image_with_crop_or_pad(img, 224, 224)\r\n# img = tf.reshape(img, [256, 256, 3])  # 256*256*3\r\n\r\nlabel = features[\'label\']\r\n\r\nprint(\'img is\', img)\r\nprint(\'label is\', label)\r\n# **4.\xe9\x80\x9a\xe8\xbf\x87 tf.train.shuffle_batch \xe6\x88\x96\xe8\x80\x85 tf.train.batch \xe5\x87\xbd\xe6\x95\xb0\xe8\xaf\xbb\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\r\n""""""\r\n\xe8\xbf\x99\xe9\x87\x8c\xef\xbc\x8c\xe4\xbd\xa0\xe4\xbc\x9a\xe5\x8f\x91\xe7\x8e\xb0\xe6\xaf\x8f\xe6\xac\xa1\xe5\x8f\x96\xe5\x87\xba\xe6\x9d\xa5\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x83\xbd\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xef\xbc\x8c\xe9\x99\xa4\xe9\x9d\x9e\xe4\xbd\xa0\xe6\x8a\x8a capacity \xe5\x92\x8c min_after_dequeue \xe8\xae\xbe\xe5\xbe\x97\xe5\xbe\x88\xe5\xa4\xa7\xef\xbc\x8c\xe5\xa6\x82\r\nX_batch, y_batch = tf.train.shuffle_batch([img, label], batch_size=100,\r\n                                          capacity=20000, min_after_dequeue=10000, num_threads=3)\r\n\xe8\xbf\x99\xe6\x98\xaf\xe5\x9b\xa0\xe4\xb8\xba\xe5\x9c\xa8\xe6\x89\x93\xe5\x8c\x85\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe9\x83\xbd\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe4\xb8\x80\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe9\xa1\xba\xe5\xba\x8f\xe6\x89\x93\xe5\x8c\x85\xe7\x9a\x84\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe6\xaf\x8f\xe6\xac\xa1\xe5\xa1\xab\xe6\x95\xb0\xe6\x8d\xae\xe9\x83\xbd\xe6\x98\xaf\xe6\x8c\x89\xe7\x85\xa7\xe9\x82\xa3\xe4\xb8\xaa\xe9\xa1\xba\xe5\xba\x8f\xe5\xa1\xab\xe5\x85\x85\xe8\xbf\x9b\xe6\x9d\xa5\xe3\x80\x82\r\n\xe5\x8f\xaa\xe6\x9c\x89\xe5\xbd\x93\xe6\x88\x91\xe4\xbb\xac\xe6\x8a\x8a\xe9\x98\x9f\xe5\x88\x97\xe5\xae\xb9\xe9\x87\x8f\xe8\x88\x8d\xe5\xbe\x97\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xa4\xa7\xef\xbc\x8c\xe8\xbf\x99\xe6\xa0\xb7\xe5\x9c\xa8\xe9\x98\x9f\xe5\x88\x97\xe4\xb8\xad\xe6\x89\x8d\xe4\xbc\x9a\xe6\xb7\xb7\xe6\x9d\x82\xe5\x90\x84\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\xe4\xbd\x86\xe6\x98\xaf\xe8\xbf\x99\xe6\xa0\xb7\xe9\x9d\x9e\xe5\xb8\xb8\xe4\xb8\x8d\xe5\xa5\xbd\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe8\xbf\x99\xe6\xa0\xb7\xe7\x9a\x84\xe8\xaf\x9d\xef\xbc\x8c\r\n\xe8\xaf\xbb\xe5\x8f\x96\xe9\x80\x9f\xe5\xba\xa6\xe5\xb0\xb1\xe4\xbc\x9a\xe9\x9d\x9e\xe5\xb8\xb8\xe6\x85\xa2\xe3\x80\x82\xe6\x89\x80\xe4\xbb\xa5\xe8\xa7\xa3\xe5\x86\xb3\xe6\x96\xb9\xe6\xb3\x95\xe6\x98\xaf\xef\xbc\x9a\r\n1.\xe5\x9c\xa8\xe5\x86\x99\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe5\x85\x88\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x95\xb0\xe6\x8d\xae shuffle\xe3\x80\x82\r\n2.\xe5\xa4\x9a\xe5\xad\x98\xe5\x87\xa0\xe4\xb8\xaa tfrecord \xe6\x96\x87\xe4\xbb\xb6\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82 64 \xe4\xb8\xaa\xe3\x80\x82\r\n""""""\r\n\r\nX_batch, y_batch = tf.train.shuffle_batch([img, label], batch_size=128,\r\n                                          capacity=5000, min_after_dequeue=100, num_threads=2)\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\ninit = tf.global_variables_initializer()\r\nsess.run(init)\r\n\r\n# **5.\xe5\x90\xaf\xe5\x8a\xa8\xe9\x98\x9f\xe5\x88\x97\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x95\xb0\xe6\x8d\xae\xe8\xaf\xbb\xe5\x8f\x96\r\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84 coord \xe6\x98\xaf\xe4\xb8\xaa\xe7\xba\xbf\xe7\xa8\x8b\xe5\x8d\x8f\xe8\xb0\x83\xe5\x99\xa8\xef\xbc\x8c\xe6\x8a\x8a\xe5\x90\xaf\xe5\x8a\xa8\xe9\x98\x9f\xe5\x88\x97\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe5\x8a\xa0\xe4\xb8\x8a\xe7\xba\xbf\xe7\xa8\x8b\xe5\x8d\x8f\xe8\xb0\x83\xe5\x99\xa8\xe3\x80\x82\r\n# \xe8\xbf\x99\xe6\xa0\xb7\xef\xbc\x8c\xe5\x9c\xa8\xe6\x95\xb0\xe6\x8d\xae\xe8\xaf\xbb\xe5\x8f\x96\xe5\xae\x8c\xe6\xaf\x95\xe4\xbb\xa5\xe5\x90\x8e\xef\xbc\x8c\xe8\xb0\x83\xe7\x94\xa8\xe5\x8d\x8f\xe8\xb0\x83\xe5\x99\xa8\xe6\x8a\x8a\xe7\xba\xbf\xe7\xa8\x8b\xe5\x85\xa8\xe9\x83\xa8\xe9\x83\xbd\xe5\x85\xb3\xe4\xba\x86\xe3\x80\x82\r\ncoord = tf.train.Coordinator()\r\nthreads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n# y_outputs = list()\r\n# for i in range(5):\r\n#     _X_batch, _y_batch = sess.run([X_batch, y_batch])\r\n#     print(\'** batch %d\' % i)\r\n#     print(\'_X_batch.shape:\', _X_batch.shape)\r\n#     print(\'_y_batch:\', _y_batch)\r\n#     y_outputs.extend(_y_batch.tolist())\r\n# print(y_outputs)\r\n\r\ntime0 = time.time()\r\n# \xe5\x8f\xaa\xe8\xa7\xa3\xe6\x9e\x90\xe5\x9b\xbe\xe7\x89\x87                         200batch     9.6 seconds\r\n# resize_images                     200batch     22.8 seconds\r\n# resize_image_with_crop_or_pad     200batch     22.4 seconds\r\nfor count in range(500):\r\n    _X_batch, _y_batch = sess.run([X_batch, y_batch])\r\n    sys.stdout.write(""\\rloop {}, pass {:.2f}s"".format(count, time.time() - time0))\r\n    sys.stdout.flush()\r\n\r\n# **6.\xe6\x9c\x80\xe5\x90\x8e\xe8\xae\xb0\xe5\xbe\x97\xe6\x8a\x8a\xe9\x98\x9f\xe5\x88\x97\xe5\x85\xb3\xe6\x8e\x89\r\ncoord.request_stop()\r\ncoord.join(threads)\r\n'"
utils/u02_tfrecord/tfrecord_3_image_writer.py,9,"b'# -*- coding:utf-8 -*- \r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\nimport sys\r\nimport os\r\nimport time\r\n\r\n\'\'\'tfrecord \xe5\x86\x99\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae.\r\n\xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xe6\x8d\xae\xe5\x86\x99\xe5\x85\xa5 tfrecord \xe6\x96\x87\xe4\xbb\xb6\xe3\x80\x82\xe4\xbb\xa5 png\xe6\xa0\xbc\xe5\xbc\x8f\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xb8\xba\xe4\xbe\x8b\xe3\x80\x82\r\n\r\n\xe7\x8e\xb0\xe5\x9c\xa8\xe7\xbd\x91\xe4\xb8\x8a\xe5\x85\xb3\xe4\xba\x8e\xe6\x89\x93\xe5\x8c\x85\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe4\xbe\x8b\xe5\xad\x90\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xa4\x9a\xef\xbc\x8c\xe5\xae\x9e\xe7\x8e\xb0\xe6\x96\xb9\xe5\xbc\x8f\xe5\x90\x84\xe5\xbc\x8f\xe5\x90\x84\xe6\xa0\xb7\xef\xbc\x8c\xe6\x95\x88\xe7\x8e\x87\xe4\xb9\x9f\xe7\x9b\xb8\xe5\xb7\xae\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xa4\x9a\xe3\x80\x82\r\n\xe9\x80\x89\xe6\x8b\xa9\xe5\x90\x88\xe9\x80\x82\xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xe8\x83\xbd\xe5\xa4\x9f\xe6\x9c\x89\xe6\x95\x88\xe5\x9c\xb0\xe8\x8a\x82\xe7\x9c\x81\xe6\x97\xb6\xe9\x97\xb4\xe5\x92\x8c\xe7\xa1\xac\xe7\x9b\x98\xe7\xa9\xba\xe9\x97\xb4\xe3\x80\x82\r\n\xe6\x9c\x89\xe5\x87\xa0\xe7\x82\xb9\xe9\x9c\x80\xe8\xa6\x81\xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x9a\r\n1.\xe6\x89\x93\xe5\x8c\x85 tfrecord \xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x8c\xe5\x8d\x83\xe4\xb8\x87\xe4\xb8\x8d\xe8\xa6\x81\xe4\xbd\xbf\xe7\x94\xa8 Image.open() \xe6\x88\x96\xe8\x80\x85 matplotlib.image.imread() \xe7\xad\x89\xe6\x96\xb9\xe5\xbc\x8f\xe8\xaf\xbb\xe5\x8f\x96\xe3\x80\x82\r\n 1\xe5\xbc\xa0\xe5\xb0\x8f\xe4\xba\x8e10kb\xe7\x9a\x84png\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe5\x89\x8d\xe8\x80\x85\xef\xbc\x88Image.open) \xe6\x89\x93\xe5\xbc\x80\xe5\x90\x8e\xef\xbc\x8c\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe5\xaf\xb9\xe8\xb1\xa1100+kb, \xe5\x90\x8e\xe8\x80\x85\xe7\x9b\xb4\xe6\x8e\xa5\xe7\x94\x9f\xe6\x88\x90 numpy \xe6\x95\xb0\xe7\xbb\x84\xef\xbc\x8c\xe5\xa4\xa7\xe6\xa6\x82\xe6\x98\xaf\xe5\x8e\x9f\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe5\x87\xa0\xe7\x99\xbe\xe5\x80\x8d\xe5\xa4\xa7\xe5\xb0\x8f\xe3\x80\x82\r\n \xe6\x89\x80\xe4\xbb\xa5\xe5\xba\x94\xe8\xaf\xa5\xe7\x9b\xb4\xe6\x8e\xa5\xe4\xbd\xbf\xe7\x94\xa8 tf.gfile.FastGFile() \xe6\x96\xb9\xe5\xbc\x8f\xe8\xaf\xbb\xe5\x85\xa5\xe5\x9b\xbe\xe7\x89\x87\xe3\x80\x82\r\n2.\xe4\xbb\x8e tfrecord \xe4\xb8\xad\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x8c\xe5\x86\x8d\xe7\x94\xa8 tf.image.decode_png() \xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xa7\xa3\xe7\xa0\x81\xe3\x80\x82\r\n3.\xe4\xb8\x8d\xe8\xa6\x81\xe9\x9a\x8f\xe4\xbe\xbf\xe4\xbd\xbf\xe7\x94\xa8 tf.image.resize_image_with_crop_or_pad \xe7\xad\x89\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9b\xb4\xe6\x8e\xa5\xe4\xbd\xbf\xe7\x94\xa8 tf.reshape()\xe3\x80\x82\xe5\x89\x8d\xe8\x80\x85\xe9\x80\x9f\xe5\xba\xa6\xe6\x9e\x81\xe6\x85\xa2\xe3\x80\x82\r\n\'\'\'\r\n\r\n# png \xe6\x96\x87\xe4\xbb\xb6\xe8\xb7\xaf\xe5\xbe\x84\r\nIMG_DIR = \'../../data/sketchy_000000000000/\'\r\nTFRECORD_DIR = \'tfrecord/sketchy_image/\'\r\nNUM_SHARDS = 64  # tfrecord \xe6\x96\x87\xe4\xbb\xb6\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\xef\xbc\x8c\xe7\xa8\x8d\xe5\xbe\xae\xe5\xa4\xa7\xe4\xba\x9b\xe5\xaf\xb9 shuffle \xe4\xbc\x9a\xe5\xa5\xbd\xe4\xba\x9b\r\n\r\n\r\ndef get_file_path(data_path=\'../../data/sketchy_000000000000/\'):\r\n    """"""\xe8\xa7\xa3\xe6\x9e\x90\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xef\xbc\x8c\xe8\x8e\xb7\xe5\x8f\x96\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xe5\x92\x8c\xe6\xa0\x87\xe7\xad\xbe\xe3\x80\x82""""""\r\n    img_paths = list()\r\n    labels = list()\r\n    # \xe5\xbf\x85\xe9\xa1\xbb\xe4\xbf\x9d\xe8\xaf\x81\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86 \xe5\x92\x8c \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe4\xb8\xad\xe7\xb1\xbb\xe5\x88\xab\xe6\x95\xb0\xe7\x9b\xb8\xe5\x90\x8c\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe8\xaf\x9d\xe5\xba\x94\xe8\xaf\xa5\xe4\xbd\xbf\xe7\x94\xa8 dict_class2id \xe6\x9d\xa5\xe4\xbf\x9d\xe8\xaf\x81\xe7\xb1\xbb\xe5\x88\xab\xe5\xaf\xb9\xe5\xba\x94\xe6\xad\xa3\xe7\xa1\xae\xe3\x80\x82\r\n    class_dirs = sorted(os.listdir(data_path))\r\n    dict_class2id = dict()\r\n    for i in range(len(class_dirs)):\r\n        label = i\r\n        class_dir = class_dirs[i]\r\n        dict_class2id[class_dir] = label\r\n        class_path = os.path.join(data_path, class_dir)  # \xe6\xaf\x8f\xe7\xb1\xbb\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\r\n        file_names = sorted(os.listdir(class_path))\r\n        for file_name in file_names:\r\n            file_path = os.path.join(class_path, file_name)\r\n            img_paths.append(file_path)\r\n            labels.append(label)\r\n    img_paths = np.asarray(img_paths)\r\n    labels = np.asarray(labels)\r\n    return img_paths, labels\r\n\r\n\r\ndef bytes_feature(values):\r\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))\r\n\r\n\r\ndef int64_feature(values):\r\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[values]))\r\n\r\n\r\ndef convert_tfrecord_dataset(tfrecord_dir, n_shards, img_paths, labels, shuffle=True):\r\n    """""" convert samples to tfrecord dataset.\r\n    Args:\r\n        dataset_dir: \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xe3\x80\x82\r\n        tfrecord_dir: \xe4\xbf\x9d\xe5\xad\x98 tfrecord \xe6\x96\x87\xe4\xbb\xb6\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xe3\x80\x82\r\n        n_shards\xef\xbc\x9a tfrecord \xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xaa\xe6\x95\xb0\r\n        img_paths: \xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe5\x90\x8d\xe5\xad\x97\xe3\x80\x82\r\n        labels\xef\xbc\x9a\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe6\xa0\x87\xe7\xad\xbe\xe3\x80\x82\r\n    """"""\r\n    if not os.path.exists(tfrecord_dir):\r\n        os.makedirs(tfrecord_dir)\r\n    n_sample = len(img_paths)\r\n    num_per_shard = n_sample // n_shards  # \xe6\xaf\x8f\xe4\xb8\xaa tfrecord \xe7\x9a\x84\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\xe9\x87\x8f\r\n\r\n    # \xe5\x9c\xa8\xe6\x89\x93\xe5\x8c\x85\xe4\xb9\x8b\xe5\x89\x8d\xe5\x85\x88\xe6\x89\x8b\xe5\x8a\xa8\xe6\x89\x93\xe4\xb9\xb1\xe4\xb8\x80\xe6\xac\xa1\r\n    if shuffle:\r\n        new_idxs = np.random.permutation(n_sample)\r\n        img_paths = img_paths[new_idxs]\r\n        labels = labels[new_idxs]\r\n\r\n    time0 = time.time()\r\n    for shard_id in range(n_shards):\r\n        output_filename = \'%d-of-%d.tfrecord\' % (shard_id, n_shards)\r\n        output_path = os.path.join(tfrecord_dir, output_filename)\r\n        with tf.python_io.TFRecordWriter(output_path) as writer:\r\n            start_ndx = shard_id * num_per_shard\r\n            end_ndx = min((shard_id + 1) * num_per_shard, n_sample)\r\n            for i in range(start_ndx, end_ndx):\r\n                sys.stdout.write(\'\\r>> Converting image %d/%d shard %d, %g s\' % (\r\n                    i + 1, n_sample, shard_id, time.time() - time0))\r\n                sys.stdout.flush()\r\n                png_path = img_paths[i]\r\n                label = labels[i]\r\n                img = tf.gfile.FastGFile(png_path, \'rb\').read()  # \xe8\xaf\xbb\xe5\x85\xa5\xe5\x9b\xbe\xe7\x89\x87\r\n                example = tf.train.Example(\r\n                    features=tf.train.Features(\r\n                        feature={\r\n                            \'image\': bytes_feature(img),\r\n                            \'label\': int64_feature(label)\r\n                        }))\r\n                serialized = example.SerializeToString()\r\n                writer.write(serialized)\r\n    print(\'\\nFinished writing data to tfrecord files.\')\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    img_paths, labels = get_file_path()\r\n    convert_tfrecord_dataset(TFRECORD_DIR, NUM_SHARDS, img_paths, labels, shuffle=True)\r\n'"
